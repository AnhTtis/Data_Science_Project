@article{sohn2016improved,
  title={Improved deep metric learning with multi-class n-pair loss objective},
  author={Sohn, Kihyuk},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}


@article{tomasev2022pushing,
  title={Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?},
  author={Tomasev, Nenad and Bica, Ioana and McWilliams, Brian and Buesing, Lars and Pascanu, Razvan and Blundell, Charles and Mitrovic, Jovana},
  journal={arXiv preprint arXiv:2201.05119},
  year={2022}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@article{zhou2021ibot,
  title={ibot: Image bert pre-training with online tokenizer},
  author={Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  journal={arXiv preprint arXiv:2111.07832},
  year={2021}
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{li_efficient_2022,
	title = {Efficient {Self}-supervised {Vision} {Transformers} for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2106.09785},
	doi = {10.48550/arXiv.2106.09785},
	abstract = {This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task of region matching which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3\% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and models are publicly available: https://github.com/microsoft/esvit},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Li, Chunyuan and Yang, Jianwei and Zhang, Pengchuan and Gao, Mei and Xiao, Bin and Dai, Xiyang and Yuan, Lu and Gao, Jianfeng},
	month = jul,
	year = {2022},
	note = {arXiv:2106.09785 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhou_ibot_2022,
	title = {{iBOT}: {Image} {BERT} {Pre}-{Training} with {Online} {Tokenizer}},
	shorttitle = {{iBOT}},
	url = {http://arxiv.org/abs/2111.07832},
	doi = {10.48550/arXiv.2111.07832},
	abstract = {The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by achieving an 82.3\% linear probing accuracy and an 87.8\% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, eg., object detection, instance segmentation, and semantic segmentation.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
	month = jan,
	year = {2022},
	note = {arXiv:2111.07832 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{latham_mutual_2009,
	title = {Mutual information},
	volume = {4},
	issn = {1941-6016},
	url = {http://www.scholarpedia.org/article/Mutual_information},
	doi = {10.4249/scholarpedia.1658},
	language = {en},
	number = {1},
	urldate = {2023-01-16},
	journal = {Scholarpedia},
	author = {Latham, Peter E. and Roudi, Yasser},
	month = jan,
	year = {2009},
	pages = {1658},
}

@article{jaiswal_survey_2021,
	title = {A {Survey} on {Contrastive} {Self}-{Supervised} {Learning}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7080},
	url = {https://www.mdpi.com/2227-7080/9/1/2},
	doi = {10.3390/technologies9010002},
	abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress.},
	language = {en},
	number = {1},
	urldate = {2023-01-16},
	journal = {Technologies},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	month = mar,
	year = {2021},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {contrastive learning, discriminative learning, image/video classification, object detection, self-supervised learning, transfer learning, unsupervised learning},
	pages = {2},
}

@article{pordel_semi-automatic_2015,
	title = {Semi-{Automatic} {Image} {Labelling} {Using} {Depth} {Information}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-431X},
	url = {https://www.mdpi.com/2073-431X/4/2/142},
	doi = {10.3390/computers4020142},
	abstract = {Image labeling tools help to extract objects within images to be used as ground truth for learning and testing in object detection processes. The inputs for such tools are usually RGB images. However with new widely available low-cost sensors like Microsoft Kinect it is possible to use depth images in addition to RGB images. Despite many existing powerful tools for image labeling, there is a need for RGB-depth adapted tools. We present a new interactive labeling tool that partially automates image labeling, with two major contributions. First, the method extends the concept of image segmentation from RGB to RGB-depth using Fuzzy C-Means clustering, connected component labeling and superpixels, and generates bounding pixels to extract the desired objects. Second, it minimizes the interaction time needed for object extraction by doing an efficient segmentation in RGB-depth space. Very few clicks are needed for the entire procedure compared to existing, tools. When the desired object is the closest object to the camera, which is often the case in robotics applications, no clicks at all are required to accurately extract the object.},
	language = {en},
	number = {2},
	urldate = {2023-01-16},
	journal = {Computers},
	author = {Pordel, Mostafa and Hellström, Thomas},
	month = jun,
	year = {2015},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Microsoft Kinect, RGBD data, depth information, image labelling, image segmentation, labelling tools, object detection, robot vision},
	pages = {142--154},
}

@misc{bhatia_automatic_2016,
	title = {Automatic {Labelling} of {Topics} with {Neural} {Embeddings}},
	url = {http://arxiv.org/abs/1612.05340},
	doi = {10.48550/arXiv.1612.05340},
	abstract = {Topics generated by topic models are typically represented as list of terms. To reduce the cognitive overhead of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
	month = dec,
	year = {2016},
	note = {arXiv:1612.05340 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{zhao_label-less_2019,
	title = {Label-{Less}: {A} {Semi}-{Automatic} {Labelling} {Tool} for {KPI} {Anomalies}},
	shorttitle = {Label-{Less}},
	doi = {10.1109/INFOCOM.2019.8737429},
	abstract = {KPI (Key Performance Indicator) anomaly detection is critical for Internet-based services to ensure the quality and reliability. However, existing algorithms' performance in reality is far from satisfying due to the lack of sufficient KPI anomaly data to help train and evaluate these algorithms. In this paper, we argue that labeling overhead is the main hurdle to obtain such datasets. Thus, we novelly propose a semi-automatic labelling tool called Label-Less, which minimizes the labeling overhead in order to enable an ImageNet-like large-scale KPI anomaly dataset with high-quality ground truth. One novel technique in Label-Less is robust and rapid anomaly similarity search, which saves operators from scanning and checking the long KPIs back and forth for abnormal patterns or label consistency. In our evaluations using 30 real KPIs from a large Internet company, our anomaly similarity search achieves the best F-score of 0.95 on average, and a real-time per-KPI response time (less than 0.5 second). Overall, the feedback from deployment in practice shows that Label-Less can reduce operators' labeling overhead by more than 90\%.},
	booktitle = {{IEEE} {INFOCOM} 2019 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Zhao, Nengwen and Zhu, Jing and Liu, Rong and Liu, Dapeng and Zhang, Ming and Pei, Dan},
	month = apr,
	year = {2019},
	note = {ISSN: 2641-9874},
	keywords = {Anomaly detection, Companies, Key performance indicator, Labeling, Time factors, Time series analysis},
	pages = {1882--1890},
}

@article{peres_industrial_2020,
	title = {Industrial {Artificial} {Intelligence} in {Industry} 4.0 - {Systematic} {Review}, {Challenges} and {Outlook}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3042874},
	abstract = {The advent of the Industry 4.0 initiative has made it so that manufacturing environments are becoming more and more dynamic, connected but also inherently more complex, with additional inter-dependencies, uncertainties and large volumes of data being generated. Recent advances in Industrial Artificial Intelligence have showcased the potential of this technology to assist manufacturers in tackling the challenges associated with this digital transformation of Cyber-Physical Systems, through its data-driven predictive analytics and capacity to assist decision-making in highly complex, non-linear and often multistage environments. However, the industrial adoption of such solutions is still relatively low beyond the experimental pilot stage, as real environments provide unique and difficult challenges for which organizations are still unprepared. The aim of this paper is thus two-fold. First, a systematic review of current Industrial Artificial Intelligence literature is presented, focusing on its application in real manufacturing environments to identify the main enabling technologies and core design principles. Then, a set of key challenges and opportunities to be addressed by future research efforts are formulated along with a conceptual framework to bridge the gap between research in this field and the manufacturing industry, with the goal of promoting industrial adoption through a successful transition towards a digitized and data-driven company-wide culture. This paper is among the first to provide a clear definition and holistic view of Industrial Artificial Intelligence in the Industry 4.0 landscape, identifying and analysing its fundamental building blocks and ongoing trends. Its findings are expected to assist and empower researchers and manufacturers alike to better understand the requirements and steps necessary for a successful transition into Industry 4.0 supported by AI, as well as the challenges that may arise during this process.},
	journal = {IEEE Access},
	author = {Peres, Ricardo Silva and Jia, Xiaodong and Lee, Jay and Sun, Keyi and Colombo, Armando Walter and Barata, Jose},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial intelligence, Decision making, Industries, Industry 4.0, Manufacturing, Robots, Service robots, Systematics, digital transformation, framework, guidelines, manufacturing, systematic review},
	pages = {220121--220139},
}

@misc{wang_entailment_2021,
	title = {Entailment as {Few}-{Shot} {Learner}},
	url = {http://arxiv.org/abs/2104.14690},
	doi = {10.48550/arXiv.2104.14690},
	abstract = {Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12{\textbackslash}\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Wang, Sinong and Fang, Han and Khabsa, Madian and Mao, Hanzi and Ma, Hao},
	month = apr,
	year = {2021},
	note = {arXiv:2104.14690 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chen_pali_2022,
	title = {{PaLI}: {A} {Jointly}-{Scaled} {Multilingual} {Language}-{Image} {Model}},
	shorttitle = {{PaLI}},
	url = {http://arxiv.org/abs/2209.06794},
	doi = {10.48550/arXiv.2209.06794},
	abstract = {Effective scaling and a flexible task interface enable large language models to excel at many tasks. PaLI (Pathways Language and Image model) extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish and Bradbury, James and Kuo, Weicheng and Seyedhosseini, Mojtaba and Jia, Chao and Ayan, Burcu Karagol and Riquelme, Carlos and Steiner, Andreas and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06794 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wortsman_model_2022,
	title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
	shorttitle = {Model soups},
	url = {https://proceedings.mlr.press/v162/wortsman22a.html},
	abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs—we call the results “model soups.” When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
	language = {en},
	urldate = {2023-01-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23965--23998},
}

@inproceedings{wu_unsupervised_2018,
	title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance} {Discrimination}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html},
	urldate = {2023-01-16},
	author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
	year = {2018},
	pages = {3733--3742},
}

@article{jaiswal_survey_2021-1,
	title = {A {Survey} on {Contrastive} {Self}-{Supervised} {Learning}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7080},
	url = {https://www.mdpi.com/2227-7080/9/1/2},
	doi = {10.3390/technologies9010002},
	abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress.},
	language = {en},
	number = {1},
	urldate = {2023-01-16},
	journal = {Technologies},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	month = mar,
	year = {2021},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {contrastive learning, discriminative learning, image/video classification, object detection, self-supervised learning, transfer learning, unsupervised learning},
	pages = {2},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2023-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@inproceedings{koohpayegani_mean_2021,
	title = {Mean {Shift} for {Self}-{Supervised} {Learning}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Koohpayegani_Mean_Shift_for_Self-Supervised_Learning_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-01-13},
	author = {Koohpayegani, Soroush Abbasi and Tejankar, Ajinkya and Pirsiavash, Hamed},
	year = {2021},
	pages = {10326--10335},
}

@inproceedings{dwibedi_little_2021,
	title = {With a {Little} {Help} {From} {My} {Friends}: {Nearest}-{Neighbor} {Contrastive} {Learning} of {Visual} {Representations}},
	shorttitle = {With a {Little} {Help} {From} {My} {Friends}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Dwibedi_With_a_Little_Help_From_My_Friends_Nearest-Neighbor_Contrastive_Learning_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-01-13},
	author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
	year = {2021},
	pages = {9588--9597},
}

@inproceedings{grill_bootstrap_2020,
	title = {Bootstrap {Your} {Own} {Latent} - {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6\% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.},
	urldate = {2023-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	year = {2020},
	pages = {21271--21284},
}

@inproceedings{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {https://proceedings.mlr.press/v139/zbontar21a.html},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	language = {en},
	urldate = {2023-01-13},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12310--12320},
}

@inproceedings{chen_empirical_2021,
	title = {An {Empirical} {Study} of {Training} {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-01-13},
	author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
	year = {2021},
	pages = {9640--9649},
}

@misc{you_large_2017,
	title = {Large {Batch} {Training} of {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1708.03888},
	doi = {10.48550/arXiv.1708.03888},
	abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	month = sep,
	year = {2017},
	note = {arXiv:1708.03888 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2023-01-13},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@article{rousseeuw_silhouettes_1987,
	title = {Silhouettes: {A} graphical aid to the interpretation and validation of cluster analysis},
	volume = {20},
	issn = {0377-0427},
	shorttitle = {Silhouettes},
	url = {https://www.sciencedirect.com/science/article/pii/0377042787901257},
	doi = {10.1016/0377-0427(87)90125-7},
	abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.},
	language = {en},
	urldate = {2023-01-13},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Rousseeuw, Peter J.},
	month = nov,
	year = {1987},
	keywords = {Graphical display, classification, cluster analysis, clustering validity},
	pages = {53--65},
}

@misc{dosovitskiy_inverting_2016,
	title = {Inverting {Visual} {Representations} with {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1506.02753},
	doi = {10.48550/arXiv.1506.02753},
	abstract = {Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Brox, Thomas},
	month = apr,
	year = {2016},
	note = {arXiv:1506.02753 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{mahendran_understanding_2015,
	title = {Understanding {Deep} {Image} {Representations} by {Inverting} {Them}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.html},
	urldate = {2023-01-13},
	author = {Mahendran, Aravindh and Vedaldi, Andrea},
	year = {2015},
	pages = {5188--5196},
}

@inproceedings{ulyanov_deep_2018,
	title = {Deep {Image} {Prior}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.html},
	urldate = {2023-01-13},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	year = {2018},
	pages = {9446--9454},
}

@misc{appalaraju_towards_2020,
	title = {Towards {Good} {Practices} in {Self}-supervised {Representation} {Learning}},
	url = {http://arxiv.org/abs/2012.00868},
	doi = {10.48550/arXiv.2012.00868},
	abstract = {Self-supervised representation learning has seen remarkable progress in the last few years. More recently, contrastive instance learning has shown impressive results compared to its supervised learning counterparts. However, even with the ever increased interest in contrastive instance learning, it is still largely unclear why these methods work so well. In this paper, we aim to unravel some of the mysteries behind their success, which are the good practices. Through an extensive empirical analysis, we hope to not only provide insights but also lay out a set of best practices that led to the success of recent work in self-supervised representation learning.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Appalaraju, Srikar and Zhu, Yi and Xie, Yusheng and Fehérvári, István},
	month = dec,
	year = {2020},
	note = {arXiv:2012.00868 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{chattopadhyay_grad-cam_2018,
	title = {Grad-{CAM}++: {Improved} {Visual} {Explanations} for {Deep} {Convolutional} {Networks}},
	shorttitle = {Grad-{CAM}++},
	url = {http://arxiv.org/abs/1710.11063},
	doi = {10.1109/WACV.2018.00097},
	abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
	urldate = {2023-01-13},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Chattopadhyay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
	month = mar,
	year = {2018},
	note = {arXiv:1710.11063 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {839--847},
}

@misc{gildenblat_pytorch_2021,
	title = {{PyTorch} library for {CAM} methods},
	url = {https://github.com/jacobgil/pytorch-grad-cam},
	publisher = {GitHub},
	author = {Gildenblat, Jacob and {contributors}},
	year = {2021},
}

@misc{srinivas_full-gradient_2019,
	title = {Full-{Gradient} {Representation} for {Neural} {Network} {Visualization}},
	url = {http://arxiv.org/abs/1905.00780},
	doi = {10.48550/arXiv.1905.00780},
	abstract = {We introduce a new tool for interpreting neural net responses, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. For convolutional nets, we also propose an approximate saliency map representation, called FullGrad, obtained by aggregating the full-gradient components. We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behaviour correctly, and more comprehensively than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Srinivas, Suraj and Fleuret, Francois},
	month = dec,
	year = {2019},
	note = {arXiv:1905.00780 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{muhammad_eigen-cam_2020,
	title = {Eigen-{CAM}: {Class} {Activation} {Map} using {Principal} {Components}},
	shorttitle = {Eigen-{CAM}},
	url = {http://arxiv.org/abs/2008.00299},
	doi = {10.1109/IJCNN48605.2020.9206626},
	abstract = {Deep neural networks are ubiquitous due to the ease of developing models and their influence on other domains. At the heart of this progress is convolutional neural networks (CNNs) that are capable of learning representations or features given a set of data. Making sense of such complex models (i.e., millions of parameters and hundreds of layers) remains challenging for developers as well as the end-users. This is partially due to the lack of tools or interfaces capable of providing interpretability and transparency. A growing body of literature, for example, class activation map (CAM), focuses on making sense of what a model learns from the data or why it behaves poorly in a given task. This paper builds on previous ideas to cope with the increasing demand for interpretable, robust, and transparent models. Our approach provides a simpler and intuitive (or familiar) way of generating CAM. The proposed Eigen-CAM computes and visualizes the principle components of the learned features/representations from the convolutional layers. Empirical studies were performed to compare the Eigen-CAM with the state-of-the-art methods (such as Grad-CAM, Grad-CAM++, CNN-fixations) by evaluating on benchmark datasets such as weakly-supervised localization and localizing objects in the presence of adversarial noise. Eigen-CAM was found to be robust against classification errors made by fully connected layers in CNNs, does not rely on the backpropagation of gradients, class relevance score, maximum activation locations, or any other form of weighting features. In addition, it works with all CNN models without the need to modify layers or retrain models. Empirical results show up to 12\% improvement over the best method among the methods compared on weakly supervised object localization.},
	urldate = {2023-01-13},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Muhammad, Mohammed Bany and Yeasin, Mohammed},
	month = jul,
	year = {2020},
	note = {arXiv:2008.00299 [cs]},
	keywords = {68T07 (Primary), 68T45 (Secondary), Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {1--7},
}

@misc{draelos_use_2021,
	title = {Use {HiResCAM} instead of {Grad}-{CAM} for faithful explanations of convolutional neural networks},
	url = {http://arxiv.org/abs/2011.08891},
	doi = {10.48550/arXiv.2011.08891},
	abstract = {Explanation methods facilitate the development of models that learn meaningful concepts and avoid exploiting spurious correlations. We illustrate a previously unrecognized limitation of the popular neural network explanation method Grad-CAM: as a side effect of the gradient averaging step, Grad-CAM sometimes highlights locations the model did not actually use. To solve this problem, we propose HiResCAM, a novel class-specific explanation method that is guaranteed to highlight only the locations the model used to make each prediction. We prove that HiResCAM is a generalization of CAM and explore the relationships between HiResCAM and other gradient-based explanation methods. Experiments on PASCAL VOC 2012, including crowd-sourced evaluations, illustrate that while HiResCAM's explanations faithfully reflect the model, Grad-CAM often expands the attention to create bigger and smoother visualizations. Overall, this work advances convolutional neural network explanation approaches and may aid in the development of trustworthy models for sensitive applications.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Draelos, Rachel Lea and Carin, Lawrence},
	month = nov,
	year = {2021},
	note = {arXiv:2011.08891 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	shorttitle = {Grad-{CAM}},
	doi = {10.1109/ICCV.2017.74},
	abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Cats, Computer architecture, Dogs, Knowledge discovery, Visualization},
	pages = {618--626},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	number = {86},
	urldate = {2023-01-13},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
}

@misc{mcinnes_umap_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	doi = {10.48550/arXiv.1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv:1802.03426 [cs, stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wu_improved_2002,
	title = {Improved k-nearest neighbor classification},
	volume = {35},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320301001327},
	doi = {10.1016/S0031-3203(01)00132-7},
	abstract = {k-nearest neighbor (k-NN) classification is a well-known decision rule that is widely used in pattern classification. However, the traditional implementation of this method is computationally expensive. In this paper we develop two effective techniques, namely, template condensing and preprocessing, to significantly speed up k-NN classification while maintaining the level of accuracy. Our template condensing technique aims at “sparsifying” dense homogeneous clusters of prototypes of any single class. This is implemented by iteratively eliminating patterns which exhibit high attractive capacities. Our preprocessing technique filters a large portion of prototypes which are unlikely to match against the unknown pattern. This again accelerates the classification procedure considerably, especially in cases where the dimensionality of the feature space is high. One of our case studies shows that the incorporation of these two techniques to k-NN rule achieves a seven-fold speed-up without sacrificing accuracy.},
	language = {en},
	number = {10},
	urldate = {2023-01-13},
	journal = {Pattern Recognition},
	author = {Wu, Yingquan and Ianakiev, Krassimir and Govindaraju, Venu},
	month = oct,
	year = {2002},
	keywords = {-Nearest neighbor classification, Classifier, Pattern classification, Preprocessing, Template condensing},
	pages = {2311--2318},
}

@article{costa_solo-learn_2022,
	title = {solo-learn: {A} {Library} of {Self}-supervised {Methods} for {Visual} {Representation} {Learning}},
	volume = {23},
	issn = {1533-7928},
	shorttitle = {solo-learn},
	url = {http://jmlr.org/papers/v23/21-1155.html},
	abstract = {This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tricks. Our goal is to provide an easy-to-use library comprising a large amount of Self-supervised Learning (SSL) methods, that can be easily extended and fine-tuned by the community. solo-learn opens up avenues for exploiting large-budget SSL solutions on inexpensive smaller infrastructures and seeks to democratize SSL by making it accessible to all. The source code is available at https://github.com/vturrisi/solo-learn.},
	number = {56},
	urldate = {2023-01-13},
	journal = {Journal of Machine Learning Research},
	author = {Costa, Victor Guilherme Turrisi da and Fini, Enrico and Nabi, Moin and Sebe, Nicu and Ricci, Elisa},
	year = {2022},
	pages = {1--6},
}

@misc{velickovic_deep_2018,
	title = {Deep {Graph} {Infomax}},
	url = {http://arxiv.org/abs/1809.10341},
	doi = {10.48550/arXiv.1809.10341},
	abstract = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Veličković, Petar and Fedus, William and Hamilton, William L. and Liò, Pietro and Bengio, Yoshua and Hjelm, R. Devon},
	month = dec,
	year = {2018},
	note = {arXiv:1809.10341 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@misc{kong_mutual_2019,
	title = {A {Mutual} {Information} {Maximization} {Perspective} of {Language} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1910.08350},
	doi = {10.48550/arXiv.1910.08350},
	abstract = {We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Kong, Lingpeng and d'Autume, Cyprien de Masson and Ling, Wang and Yu, Lei and Dai, Zihang and Yogatama, Dani},
	month = nov,
	year = {2019},
	note = {arXiv:1910.08350 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{bachman_learning_2019,
	title = {Learning {Representations} by {Maximizing} {Mutual} {Information} {Across} {Views}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/ddf354219aac374f1d40b7e760ee5bb7-Abstract.html},
	abstract = {We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views – e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1\% accuracy on ImageNet using standard linear evaluation.  This beats prior results by over 12\% and concurrent results by 7\%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.},
	urldate = {2023-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
	year = {2019},
}

@inproceedings{chopra_learning_2005,
	title = {Learning a similarity metric discriminatively, with application to face verification},
	volume = {1},
	doi = {10.1109/CVPR.2005.202},
	abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {Artificial neural networks, Character generation, Drives, Face recognition, Glass, Robustness, Spatial databases, Support vector machine classification, Support vector machines, System testing},
	pages = {539--546 vol. 1},
}

@inproceedings{khosla_supervised_2020,
	title = {Supervised {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. In reduced data settings, it outperforms cross-entropy significantly. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2023-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	year = {2020},
	pages = {18661--18673},
}

@misc{khosla_supervised_2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	doi = {10.48550/arXiv.2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv:2004.11362 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ma_noise_2018,
	title = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}: {Consistency} and {Statistical} {Efficiency}},
	shorttitle = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}},
	url = {http://arxiv.org/abs/1809.01812},
	doi = {10.48550/arXiv.1809.01812},
	abstract = {Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Ma, Zhuang and Collins, Michael},
	month = sep,
	year = {2018},
	note = {arXiv:1809.01812 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Methodology},
}

@misc{donahue_adversarial_2017,
	title = {Adversarial {Feature} {Learning}},
	url = {http://arxiv.org/abs/1605.09782},
	doi = {10.48550/arXiv.1605.09782},
	abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	month = apr,
	year = {2017},
	note = {arXiv:1605.09782 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{zhang_split-brain_2017,
	title = {Split-{Brain} {Autoencoders}: {Unsupervised} {Learning} by {Cross}-{Channel} {Prediction}},
	shorttitle = {Split-{Brain} {Autoencoders}},
	url = {http://arxiv.org/abs/1611.09842},
	doi = {10.48550/arXiv.1611.09842},
	abstract = {We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
	month = apr,
	year = {2017},
	note = {arXiv:1611.09842 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{noroozi_unsupervised_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Learning} of {Visual} {Representations} by {Solving} {Jigsaw} {Puzzles}},
	isbn = {978-3-319-46466-4},
	doi = {10.1007/978-3-319-46466-4_5},
	abstract = {We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with \$\$51.8{\textbackslash},{\textbackslash}\%\$\$for detection and \$\$68.6{\textbackslash},{\textbackslash}\%\$\$for classification, and reduce the gap with supervised learning (\$\$56.5{\textbackslash},{\textbackslash}\%\$\$and \$\$78.2{\textbackslash},{\textbackslash}\%\$\$respectively).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Noroozi, Mehdi and Favaro, Paolo},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Feature transfer, Image representation learning, Self-supervised learning, Unsupervised learning},
	pages = {69--84},
}

@inproceedings{doersch_unsupervised_2015,
	title = {Unsupervised {Visual} {Representation} {Learning} by {Context} {Prediction}},
	url = {https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.html},
	urldate = {2023-01-13},
	author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
	year = {2015},
	pages = {1422--1430},
}

@misc{doersch_unsupervised_2016,
	title = {Unsupervised {Visual} {Representation} {Learning} by {Context} {Prediction}},
	url = {http://arxiv.org/abs/1505.05192},
	doi = {10.48550/arXiv.1505.05192},
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
	month = jan,
	year = {2016},
	note = {arXiv:1505.05192 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhang_colorful_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Colorful {Image} {Colorization}},
	isbn = {978-3-319-46487-9},
	doi = {10.1007/978-3-319-46487-9_40},
	abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32 \% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {CNNs, Colorization, Self-supervised learning, Vision for graphics},
	pages = {649--666},
}

@inproceedings{komodakis_unsupervised_2018,
	title = {Unsupervised representation learning by predicting image rotations},
	url = {https://hal-enpc.archives-ouvertes.fr/hal-01832768},
	abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet},
	language = {en},
	urldate = {2023-01-13},
	author = {Komodakis, Nikos and Gidaris, Spyros},
	month = apr,
	year = {2018},
}

@inproceedings{dosovitskiy_discriminative_2014,
	title = {Discriminative {Unsupervised} {Feature} {Learning} with {Convolutional} {Neural} {Networks}},
	volume = {27},
	url = {https://papers.nips.cc/paper/2014/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html},
	abstract = {Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).},
	urldate = {2023-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
	year = {2014},
}

@inproceedings{misra_self-supervised_2020,
	title = {Self-{Supervised} {Learning} of {Pretext}-{Invariant} {Representations}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Misra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.html},
	urldate = {2023-01-13},
	author = {Misra, Ishan and Maaten, Laurens van der},
	year = {2020},
	pages = {6707--6717},
}

@article{dufter2022position,
  title={Position information in transformers: An overview},
  author={Dufter, Philipp and Schmitt, Martin and Sch{\"u}tze, Hinrich},
  journal={Computational Linguistics},
  volume={48},
  number={3},
  pages={733--763},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{cai_semi-supervised_2022,
	title = {Semi-supervised {Vision} {Transformers} at {Scale}},
	url = {http://arxiv.org/abs/2208.05688},
	doi = {10.48550/arXiv.2208.05688},
	abstract = {We study semi-supervised learning (SSL) for vision transformers (ViT), an under-explored topic despite the wide adoption of the ViT architectures to different tasks. To tackle this problem, we propose a new SSL pipeline, consisting of first un/self-supervised pre-training, followed by supervised fine-tuning, and finally semi-supervised fine-tuning. At the semi-supervised fine-tuning stage, we adopt an exponential moving average (EMA)-Teacher framework instead of the popular FixMatch, since the former is more stable and delivers higher accuracy for semi-supervised vision transformers. In addition, we propose a probabilistic pseudo mixup mechanism to interpolate unlabeled samples and their pseudo labels for improved regularization, which is important for training ViTs with weak inductive bias. Our proposed method, dubbed Semi-ViT, achieves comparable or better performance than the CNN counterparts in the semi-supervised classification setting. Semi-ViT also enjoys the scalability benefits of ViTs that can be readily scaled up to large-size models with increasing accuracies. For example, Semi-ViT-Huge achieves an impressive 80\% top-1 accuracy on ImageNet using only 1\% labels, which is comparable with Inception-v4 using 100\% ImageNet labels.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Cai, Zhaowei and Ravichandran, Avinash and Favaro, Paolo and Wang, Manchen and Modolo, Davide and Bhotika, Rahul and Tu, Zhuowen and Soatto, Stefano},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05688 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{chen_big_2020,
	title = {Big {Self}-{Supervised} {Models} are {Strong} {Semi}-{Supervised} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html},
	urldate = {2023-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
	year = {2020},
	pages = {22243--22255},
}

@inproceedings{zhai_s4l_2019,
	title = {{S4L}: {Self}-{Supervised} {Semi}-{Supervised} {Learning}},
	shorttitle = {{S4L}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_S4L_Self-Supervised_Semi-Supervised_Learning_ICCV_2019_paper.html},
	urldate = {2023-01-13},
	author = {Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
	year = {2019},
	pages = {1476--1485},
}

@article{mackiewicz_principal_1993,
	title = {Principal components analysis ({PCA})},
	volume = {19},
	issn = {0098-3004},
	url = {https://www.sciencedirect.com/science/article/pii/009830049390090R},
	doi = {10.1016/0098-3004(93)90090-R},
	abstract = {Principal Components Analysis (PCA) as a method of multivariate statistics was created before the Second World War. However, the wider application of this method only occurred in the 1960s, during the “Quantitative Revolution” in the Natural and Social Sciences. The main reason for this time-lag was the huge difficulty posed by calculations involving this method. Only with the advent and development of computers did the almost unlimited application of multivariate statistical methods, including principal components, become possible. At the same time, requirements arose for precise numerical methods concerning, among other things, the calculation of eigenvalues and eigenvectors, because the application of principal components to technical problems required absolute accuracy. On the other hand, numerous applications in Social Sciences gave rise to a significant increase in the ability to interpret these nonobservable variables, which is just what the principal components are. In the application of principal components, the problem is not only to do with their formal properties but above all, their empirical origins. The authors considered these two tendencies during the creation of the program for principal components. This program—entitled PCA—accompanies this paper. It analyzes consecutively, matrices of variance-covariance and correlations, and performs the following functions: •- the determination of eigenvalues and eigenvectors of these matrices.•- the testing of principal components.•- the calculation of coefficients of determination between selected components and the initial variables, and the testing of these coefficients,•- the determination of the share of variation of all the initial variables in the variation of particular components,•- construction of a dendrite for the initial set of variables,•- the construction of a dendrite for a selected pattern of the principal components,•- the scatter of the objects studied in a selected coordinate system. Thus, the PCA program performs many more functions especially in testing and graphics, than PCA programs in conventional statistical packages. Included in this paper are a theoretical description of principal components, the basic rules for their interpretation and also statistical testing.},
	language = {en},
	number = {3},
	urldate = {2023-01-13},
	journal = {Computers \& Geosciences},
	author = {Maćkiewicz, Andrzej and Ratajczak, Waldemar},
	month = mar,
	year = {1993},
	keywords = {Bartlett's statistics, Coefficients of determination, Correlation matrix, Eigenvalues, Eigenvectors, FORTRAN 77, Principal Components Analysis, Variance-covariance matrix},
	pages = {303--342},
}

@book{cam_proceedings_1967,
	title = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}: {Weather} modification},
	shorttitle = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}},
	language = {en},
	publisher = {University of California Press},
	author = {Cam, Lucien Marie Le and Neyman, Jerzy},
	year = {1967},
	note = {Google-Books-ID: IC4Ku\_7dBFUC},
}

@misc{hsu_revise_2022,
	title = {{ReVISE}: {Self}-{Supervised} {Speech} {Resynthesis} with {Visual} {Input} for {Universal} and {Generalized} {Speech} {Enhancement}},
	shorttitle = {{ReVISE}},
	url = {http://arxiv.org/abs/2212.11377},
	doi = {10.48550/arXiv.2212.11377},
	abstract = {Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Enhancement, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual enhancement tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Hsu, Wei-Ning and Remez, Tal and Shi, Bowen and Donley, Jacob and Adi, Yossi},
	month = dec,
	year = {2022},
	note = {arXiv:2212.11377 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{anton_audio_2022,
	title = {Audio {Barlow} {Twins}: {Self}-{Supervised} {Audio} {Representation} {Learning}},
	shorttitle = {Audio {Barlow} {Twins}},
	url = {http://arxiv.org/abs/2209.14345},
	doi = {10.48550/arXiv.2209.14345},
	abstract = {The Barlow Twins self-supervised learning objective requires neither negative samples or asymmetric learning updates, achieving results on a par with the current state-of-the-art within Computer Vision. As such, we present Audio Barlow Twins, a novel self-supervised audio representation learning approach, adapting Barlow Twins to the audio domain. We pre-train on the large-scale audio dataset AudioSet, and evaluate the quality of the learnt representations on 18 tasks from the HEAR 2021 Challenge, achieving results which outperform, or otherwise are on a par with, the current state-of-the-art for instance discrimination self-supervised learning approaches to audio representation learning. Code at https://github.com/jonahanton/SSL\_audio.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Anton, Jonah and Coppock, Harry and Shukla, Pancham and Schuller, Bjorn W.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14345 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2023-01-13},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
}

@inproceedings{knights_temporally_2021,
	title = {Temporally {Coherent} {Embeddings} for {Self}-{Supervised} {Video} {Representation} {Learning}},
	doi = {10.1109/ICPR48806.2021.9412071},
	abstract = {This paper presents TCE: Temporally Coherent Embeddings for self-supervised video representation learning. The proposed method exploits inherent structure of unlabeled video data to explicitly enforce temporal coherency in the embedding space, rather than indirectly learning it through ranking or predictive proxy tasks. In the same way that high-level visual information in the world changes smoothly, we believe that nearby frames in learned representations will benefit from demonstrating similar properties. Using this assumption, we train our TCE model to encode videos such that adjacent frames exist close to each other and videos are separated from one another. Using TCE we learn robust representations from large quantities of unlabeled video data. We thoroughly analyse and evaluate our self-supervised learned TCE models on a downstream task of video action recognition using multiple challenging benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN backbone and only RGB stream inputs, TCE pre-trained representations outperform all previous self-supervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code and pre-trained models for this paper can be downloaded at: https://github.com/csiro-robotics/TCE.},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Knights, Joshua and Harwood, Ben and Ward, Daniel and Vanderkop, Anthony and Mackenzie-Ross, Olivia and Moghadam, Peyman},
	month = jan,
	year = {2021},
	note = {ISSN: 1051-4651},
	keywords = {Benchmark testing, Data models, Hardware, Network architecture, Spatiotemporal phenomena, Training, Visualization},
	pages = {8914--8921},
}

@misc{xu_videoclip_2021,
	title = {{VideoCLIP}: {Contrastive} {Pre}-training for {Zero}-shot {Video}-{Text} {Understanding}},
	shorttitle = {{VideoCLIP}},
	url = {http://arxiv.org/abs/2109.14084},
	doi = {10.48550/arXiv.2109.14084},
	abstract = {We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
	month = oct,
	year = {2021},
	note = {arXiv:2109.14084 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{schiappa_self-supervised_2022,
	title = {Self-{Supervised} {Learning} for {Videos}: {A} {Survey}},
	issn = {0360-0300},
	shorttitle = {Self-{Supervised} {Learning} for {Videos}},
	url = {https://doi.org/10.1145/3577925},
	doi = {10.1145/3577925},
	abstract = {The remarkable success of deep learning in various domains relies on the availability of large-scale annotated datasets. However, obtaining annotations is expensive and requires great effort, which is especially challenging for videos. Moreover, the use of human-generated annotations leads to models with biased learning and poor domain generalization and robustness. As an alternative, self-supervised learning provides a way for representation learning which does not require annotations and has shown promise in both image and video domains. Different from the image domain, learning video representations are more challenging due to the temporal dimension, bringing in motion and other environmental dynamics. This also provides opportunities for video-exclusive ideas that advance self-supervised learning in the video and multimodal domain. In this survey, we provide a review of existing approaches on self-supervised learning focusing on the video domain. We summarize these methods into four different categories based on their learning objectives: 1) pretext tasks, 2) generative learning, 3) contrastive learning, and 4) cross-modal agreement. We further introduce the commonly used datasets, downstream evaluation tasks, insights into the limitations of existing works, and the potential future directions in this area.},
	urldate = {2023-01-13},
	journal = {ACM Computing Surveys},
	author = {Schiappa, Madeline C. and Rawat, Yogesh S. and Shah, Mubarak},
	month = dec,
	year = {2022},
	note = {Just Accepted},
	keywords = {deep learning, multimodal learning, representation learning, self-supervised learning, video understanding, visual-language models, zero-shot learning},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-01-13},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	year = {2021},
	pages = {9650--9660},
}

@article{arshad_dataset_2022,
	title = {A dataset and benchmark for malaria life-cycle classification in thin blood smear images},
	volume = {34},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-021-06602-6},
	doi = {10.1007/s00521-021-06602-6},
	abstract = {Malaria microscopy, microscopic examination of stained blood slides to detect parasite Plasmodium, is considered to be a gold standard for detecting life-threatening disease malaria. Detecting the plasmodium parasite requires a skilled examiner and may take up to 10 to 15 minutes to completely go through the whole slide. Due to a lack of skilled medical professionals in the underdeveloped or resource-deficient regions, many cases go misdiagnosed, which results in unavoidable medical complications. We propose to complement the medical professionals by creating a deep learning-based method to automatically detect (localize) the plasmodium parasites in the photograph of stained film. To handle the unbalanced nature of the dataset, we adopt a two-stage approach. Where the first stage is trained to classify cells into just healthy or infected. The second stage is trained to classify each detected cell further into the malaria life-cycle stage. To facilitate the research in machine learning-based malaria microscopy, we introduce a new large-scale microscopic image malaria dataset. Thirty-eight thousand cells are tagged from the 345 microscopic images of different Giemsa-stained slides of blood samples. Extensive experimentation is performed using different Convolutional Neural Networks on this dataset. Our experiments and analysis reveal that the two-stage approach works better than the one-stage approach for malaria detection. To ensure the usability of our approach, we have also developed a mobile app that will be used by local hospitals for investigation and educational purposes. The dataset, its annotations, and implementation codes will be released upon publication of the paper.},
	language = {en},
	number = {6},
	urldate = {2023-01-13},
	journal = {Neural Computing and Applications},
	author = {Arshad, Qazi Ammar and Ali, Mohsen and Hassan, Saeed-ul and Chen, Chen and Imran, Ayisha and Rasul, Ghulam and Sultani, Waqas},
	month = mar,
	year = {2022},
	pages = {4473--4485},
}

@misc{griffin_caltech-256_2007,
	type = {Report or {Paper}},
	title = {Caltech-256 {Object} {Category} {Dataset}},
	copyright = {other},
	url = {https://resolver.caltech.edu/CaltechAUTHORS:CNS-TR-2007-001},
	abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
	language = {en},
	urldate = {2023-01-13},
	author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
	month = mar,
	year = {2007},
	note = {Num Pages: 20
Publisher: California Institute of Technology},
}

@misc{noauthor_cifar-10_nodate,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2023-01-13},
}

@article{liu_self-supervised_2023,
	title = {Self-{Supervised} {Learning}: {Generative} or {Contrastive}},
	volume = {35},
	issn = {1558-2191},
	shorttitle = {Self-{Supervised} {Learning}},
	doi = {10.1109/TKDE.2021.3090866},
	abstract = {Deep supervised learning has achieved great success in the last decade. However, its defects of heavy dependence on manual labels and vulnerability to attacks have driven people to find other paradigms. As an alternative, self-supervised learning (SSL) attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided\${\textasciicircum}1\$1.},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Computational modeling, Computer architecture, Context modeling, Data models, Predictive models, Self-supervised learning, Supervised learning, Task analysis, contrastive learning, deep learning, generative model},
	pages = {857--876},
}

@article{kaur2017combining,
  title={Combining weakly and webly supervised learning for classifying food images},
  author={Kaur, Parneet and Sikka, Karan and Divakaran, Ajay},
  journal={arXiv preprint arXiv:1712.08730},
  year={2017}
}

@article{xu2016simple,
  title={Simple and Efficient Learning using Privileged Information},
  author={Xu, Xinxing and Tianyi Zhou, Joey and Tsang, IvorW and Qin, Zheng and Siow Mong Goh, Rick and Liu, Yong},
  journal={arXiv e-prints},
  pages={arXiv--1604},
  year={2016}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@article{zhao2020universal,
  title={Universal-to-specific framework for complex action recognition},
  author={Zhao, Peisen and Xie, Lingxi and Zhang, Ya and Tian, Qi},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={3441--3453},
  year={2020},
  publisher={IEEE}
}

@inproceedings{parkhi2012cats,
  title={Cats and dogs},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3498--3505},
  year={2012},
  organization={IEEE}
}



@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@inproceedings{bai_simgnn_2019,
	address = {New York, NY, USA},
	series = {{WSDM} '19},
	title = {{SimGNN}: {A} {Neural} {Network} {Approach} to {Fast} {Graph} {Similarity} {Computation}},
	isbn = {978-1-4503-5940-5},
	shorttitle = {{SimGNN}},
	url = {https://doi.org/10.1145/3289600.3290967},
	doi = {10.1145/3289600.3290967},
	abstract = {Graph similarity search is among the most important graph-based applications, e.g. finding the chemical compounds that are most similar to a query compound. Graph similarity/distance computation, such as Graph Edit Distance (GED) and Maximum Common Subgraph (MCS), is the core operation of graph similarity search and many other applications, but very costly to compute in practice. Inspired by the recent success of neural network approaches to several graph applications, such as node or graph classification, we propose a novel neural network based approach to address this classic yet challenging graph problem, aiming to alleviate the computational burden while preserving a good performance. The proposed approach, called SimGNN, combines two strategies. First, we design a learnable embedding function that maps every graph into an embedding vector, which provides a global summary of a graph. A novel attention mechanism is proposed to emphasize the important nodes with respect to a specific similarity metric. Second, we design a pairwise node comparison method to supplement the graph-level embeddings with fine-grained node-level information. Our model achieves better generalization on unseen graphs, and in the worst case runs in quadratic time with respect to the number of nodes in two graphs. Taking GED computation as an example, experimental results on three real graph datasets demonstrate the effectiveness and efficiency of our approach. Specifically, our model achieves smaller error rate and great time reduction compared against a series of baselines, including several approximation algorithms on GED computation, and many existing graph neural network based models. Our study suggests SimGNN provides a new direction for future research on graph similarity computation and graph similarity search.},
	urldate = {2023-01-13},
	booktitle = {Proceedings of the {Twelfth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Bai, Yunsheng and Ding, Hao and Bian, Song and Chen, Ting and Sun, Yizhou and Wang, Wei},
	month = jan,
	year = {2019},
	keywords = {graph edit distance, graph similarity computation, network embedding, neural networks},
	pages = {384--392},
}

@misc{heinsen_algorithm_2022,
	title = {An {Algorithm} for {Routing} {Vectors} in {Sequences}},
	url = {http://arxiv.org/abs/2211.11754},
	doi = {10.48550/arXiv.2211.11754},
	abstract = {We propose a routing algorithm that takes a sequence of vectors and computes a new sequence with specified length and vector size. Each output vector maximizes "bang per bit," the difference between a net benefit to use and net cost to ignore data, by better predicting the input vectors. We describe output vectors as geometric objects, as latent variables that assign credit, as query states in a model of associative memory, and as agents in a model of a Society of Mind. We implement the algorithm with optimizations that reduce parameter count, computation, and memory use by orders of magnitude, enabling us to route sequences of greater length than previously possible. We evaluate our implementation on natural language and visual classification tasks, obtaining competitive or state-of-the-art accuracy and end-to-end credit assignments that are interpretable.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Heinsen, Franz A.},
	month = dec,
	year = {2022},
	note = {arXiv:2211.11754 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{yu_coca_2022,
	title = {{CoCa}: {Contrastive} {Captioners} are {Image}-{Text} {Foundation} {Models}},
	shorttitle = {{CoCa}},
	url = {http://arxiv.org/abs/2205.01917},
	doi = {10.48550/arXiv.2205.01917},
	abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and new state-of-the-art 91.0\% top-1 accuracy on ImageNet with a finetuned encoder.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01917 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@inproceedings{ballus_opt-ssl_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Opt-{SSL}: {An} {Enhanced} {Self}-{Supervised} {Framework} for {Food} {Recognition}},
	isbn = {978-3-031-04881-4},
	shorttitle = {Opt-{SSL}},
	doi = {10.1007/978-3-031-04881-4_52},
	abstract = {Self-supervised Learning has been showing upbeat performance in several computer vision tasks. The popular contrastive methods make use of a Siamese architecture with different loss functions. In this work, we go deeper into two very recent state of the art frameworks, namely, SimSiam and Barlow Twins. Inspired by them, we propose a new self-supervised learning method we call Opt-SSL that combines both image and feature contrasting. We validate the proposed method on the food recognition task, showing that our proposed framework enables the self-learning networks to learn better visual representations.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Ballús, Nil and Nagarajan, Bhalaji and Radeva, Petia},
	editor = {Pinho, Armando J. and Georgieva, Petia and Teixeira, Luís F. and Sánchez, Joan Andreu},
	year = {2022},
	keywords = {Contrastive learning, Food recognition, Self-supervised},
	pages = {655--666},
}

@misc{noauthor_position_nodate,
	title = {Position {Information} in {Transformers}: {An} {Overview} {\textbar} {Computational} {Linguistics} {\textbar} {MIT} {Press}},
	url = {https://direct.mit.edu/coli/article/48/3/733/111478/Position-Information-in-Transformers-An-Overview},
	urldate = {2023-01-11},
}

@inproceedings{anonymous_soft_2022,
	title = {Soft {Neighbors} are {Positive} {Supporters} in {Contrastive} {Visual} {Representation} {Learning}},
	url = {https://openreview.net/forum?id=l9vM_PaUKz},
	abstract = {Contrastive learning methods train visual encoders by comparing views (e.g., often created via a group of data augmentations on the same instance) from one instance to others. Typically, the views created from one instance are set as positive, while views from other instances are negative. This binary instance discrimination is studied extensively to improve feature representations in self-supervised learning. In this paper, we rethink the instance discrimination framework and find the binary instance labeling insufficient to measure correlations between different samples. For an intuitive example, given a random image instance, there may exist other images in a mini-batch whose content meanings are the same (i.e., belonging to the same category) or partially related (i.e., belonging to a similar category). How to treat the images that correlate similarly to the current image instance leaves an unexplored problem. We thus propose to support the current image by exploring other correlated instances (i.e., soft neighbors). We first carefully cultivate a candidate neighbor set, which will be further utilized to explore the highly-correlated instances. A cross-attention module is then introduced to predict the correlation score (denoted as positiveness) of other correlated instances with respect to the current one. The positiveness score quantitatively measures the positive support from each correlated instance, and is encoded into the objective for pretext training. To this end, our proposed method benefits in discriminating uncorrelated instances while absorbing correlated instances for SSL. We evaluate our soft neighbor contrastive learning method (SNCLR) on standard visual recognition benchmarks, including image classification, object detection, and instance segmentation. The state-of-the-art recognition performance shows that SNCLR is effective in improving feature representations from both ViT and CNN encoders. More materials can be found in our project page: anonymous-iclr23snclr.github.io.},
	language = {en},
	urldate = {2023-01-11},
	author = {Anonymous},
	month = nov,
	year = {2022},
}

@inproceedings{wang_kvt_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{KVT}: k-{NN} {Attention} for {Boosting} {Vision} {Transformers}},
	isbn = {978-3-031-20053-3},
	shorttitle = {{KVT}},
	doi = {10.1007/978-3-031-20053-3_17},
	abstract = {Convolutional Neural Networks (CNNs) have dominated computer vision for years, due to its ability in capturing locality and translation invariance. Recently, many vision transformer architectures have been proposed and they show promising performance. A key component in vision transformers is the fully-connected self-attention which is more powerful than CNNs in modelling long range dependencies. However, since the current dense self-attention uses all image patches (tokens) to compute attention matrix, it may neglect locality of images patches and involve noisy tokens (e.g., clutter background and occlusion), leading to a slow training process and potential degradation of performance. To address these problems, we propose the k-NN attention for boosting vision transformers. Specifically, instead of involving all the tokens for attention matrix calculation, we only select the top-k similar tokens from the keys for each query to compute the attention map. The proposed k-NN attention naturally inherits the local bias of CNNs without introducing convolutional operations, as nearby tokens tend to be more similar than others. In addition, the k-NN attention allows for the exploration of long range correlation and at the same time filters out irrelevant tokens by choosing the most similar tokens from the entire image. Despite its simplicity, we verify, both theoretically and empirically, that k-NN attention is powerful in speeding up training and distilling noise from input tokens. Extensive experiments are conducted by using 11 different vision transformer architectures to verify that the proposed k-NN attention can work with any existing transformer architectures to improve its prediction performance. The codes are available at https://github.com/damo-cv/KVT.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Wang, Pichao and Wang, Xue and Wang, Fan and Lin, Ming and Chang, Shuning and Li, Hao and Jin, Rong},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {285--302},
}

@misc{xie_clustr_2022,
	title = {{ClusTR}: {Exploring} {Efficient} {Self}-attention via {Clustering} for {Vision} {Transformers}},
	shorttitle = {{ClusTR}},
	url = {http://arxiv.org/abs/2208.13138},
	doi = {10.48550/arXiv.2208.13138},
	abstract = {Although Transformers have successfully transitioned from their language modelling origins to image-based applications, their quadratic computational complexity remains a challenge, particularly for dense prediction. In this paper we propose a content-based sparse attention method, as an alternative to dense self-attention, aiming to reduce the computation complexity while retaining the ability to model long-range dependencies. Specifically, we cluster and then aggregate key and value tokens, as a content-based method of reducing the total token count. The resulting clustered-token sequence retains the semantic diversity of the original signal, but can be processed at a lower computational cost. Besides, we further extend the clustering-guided attention from single-scale to multi-scale, which is conducive to dense prediction tasks. We label the proposed Transformer architecture ClusTR, and demonstrate that it achieves state-of-the-art performance on various vision tasks but at lower computational cost and with fewer parameters. For instance, our ClusTR small model with 22.7M parameters achieves 83.2{\textbackslash}\% Top-1 accuracy on ImageNet. Source code and ImageNet models will be made publicly available.},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Xie, Yutong and Zhang, Jianpeng and Xia, Yong and Hengel, Anton van den and Wu, Qi},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13138 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{kreuzer_rethinking_2021,
	title = {Rethinking {Graph} {Transformers} with {Spectral} {Attention}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/b4fd1d2cb085390fbbadae65e07876a7-Abstract.html},
	abstract = {In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. Here, we present the {\textbackslash}textit\{Spectral Attention Network\} (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph.This LPE is then added to the node features of the graph and passed to a fully-connected Transformer.By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance.Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction.When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.},
	urldate = {2023-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kreuzer, Devin and Beaini, Dominique and Hamilton, Will and Létourneau, Vincent and Tossou, Prudencio},
	year = {2021},
	pages = {21618--21629},
}

@article{guo_beyond_2022,
	title = {Beyond {Self}-{Attention}: {External} {Attention} {Using} {Two} {Linear} {Layers} for {Visual} {Tasks}},
	issn = {1939-3539},
	shorttitle = {Beyond {Self}-{Attention}},
	doi = {10.1109/TPAMI.2022.3211006},
	abstract = {Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Guo, Meng-Hao and Liu, Zheng-Ning and Mu, Tai-Jiang and Hu, Shi-Min},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Attention, Computer architecture, Image synthesis, Point cloud compression, Semantics, Task analysis, Transformers, Visualization, computer vision, deep learning, multi-layer perceptrons, transformer},
	pages = {1--13},
}

@misc{wu_centroid_2021,
	title = {Centroid {Transformers}: {Learning} to {Abstract} with {Attention}},
	shorttitle = {Centroid {Transformers}},
	url = {http://arxiv.org/abs/2102.08606},
	doi = {10.48550/arXiv.2102.08606},
	abstract = {Self-attention, as the key block of transformers, is a powerful mechanism for extracting features from the inputs. In essence, what self-attention does is to infer the pairwise relations between the elements of the inputs, and modify the inputs by propagating information between input pairs. As a result, it maps inputs to N outputs and casts a quadratic \$O(N{\textasciicircum}2)\$ memory and time complexity. We propose centroid attention, a generalization of self-attention that maps N inputs to M outputs \$(M{\textbackslash}leq N)\$, such that the key information in the inputs are summarized in the smaller number of outputs (called centroids). We design centroid attention by amortizing the gradient descent update rule of a clustering objective function on the inputs, which reveals an underlying connection between attention and clustering. By compressing the inputs to the centroids, we extract the key information useful for prediction and also reduce the computation of the attention module and the subsequent layers. We apply our method to various applications, including abstractive text summarization, 3D vision, and image processing. Empirical results demonstrate the effectiveness of our method over the standard transformers.},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Wu, Lemeng and Liu, Xingchao and Liu, Qiang},
	month = mar,
	year = {2021},
	note = {arXiv:2102.08606 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{chen_exploring_2021,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-01-11},
	author = {Chen, Xinlei and He, Kaiming},
	year = {2021},
	pages = {15750--15758},
}

@inproceedings{wu_unsupervised_2018-1,
	title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance} {Discrimination}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html},
	urldate = {2023-01-11},
	author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
	year = {2018},
	pages = {3733--3742},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html},
	urldate = {2023-01-11},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	year = {2020},
	pages = {9729--9738},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2023-01-11},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
}

@inproceedings{caron_unsupervised_2020,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html},
	abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
	urldate = {2023-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year = {2020},
	pages = {9912--9924},
}

@inproceedings{zhuang_local_2019,
	title = {Local {Aggregation} for {Unsupervised} {Learning} of {Visual} {Embeddings}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhuang_Local_Aggregation_for_Unsupervised_Learning_of_Visual_Embeddings_ICCV_2019_paper.html},
	urldate = {2023-01-11},
	author = {Zhuang, Chengxu and Zhai, Alex Lin and Yamins, Daniel},
	year = {2019},
	pages = {6002--6012},
}

@inproceedings{xie_self-training_2020,
	title = {Self-{Training} {With} {Noisy} {Student} {Improves} {ImageNet} {Classification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.html},
	urldate = {2023-01-11},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	year = {2020},
	pages = {10687--10698},
}

@inproceedings{noroozi_boosting_2018,
	title = {Boosting {Self}-{Supervised} {Learning} via {Knowledge} {Transfer}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.html},
	urldate = {2023-01-11},
	author = {Noroozi, Mehdi and Vinjimoor, Ananth and Favaro, Paolo and Pirsiavash, Hamed},
	year = {2018},
	pages = {9359--9367},
}

@inproceedings{li_unsupervised_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Visual} {Representation} {Learning} by {Graph}-{Based} {Consistent} {Constraints}},
	isbn = {978-3-319-46493-0},
	doi = {10.1007/978-3-319-46493-0_41},
	abstract = {Learning rich visual representations often require training on datasets of millions of manually annotated examples. This substantially limits the scalability of learning effective representations as labeled data is expensive or scarce. In this paper, we address the problem of unsupervised visual representation learning from a large, unlabeled collection of images. By representing each image as a node and each nearest-neighbor matching pair as an edge, our key idea is to leverage graph-based analysis to discover positive and negative image pairs (i.e., pairs belonging to the same and different visual categories). Specifically, we propose to use a cycle consistency criterion for mining positive pairs and geodesic distance in the graph for hard negative mining. We show that the mined positive and negative image pairs can provide accurate supervisory signals for learning effective representations using Convolutional Neural Networks (CNNs). We demonstrate the effectiveness of the proposed unsupervised constraint mining method in two settings: (1) unsupervised feature learning and (2) semi-supervised learning. For unsupervised feature learning, we obtain competitive performance with several state-of-the-art approaches on the PASCAL VOC 2007 dataset. For semi-supervised learning, we show boosted performance by incorporating the mined constraints on three image classification datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Li, Dong and Hung, Wei-Chih and Huang, Jia-Bin and Wang, Shengjin and Ahuja, Narendra and Yang, Ming-Hsuan},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Convolutional neural networks, Image classification, Semi-supervised learning, Unsupervised feature learning},
	pages = {678--694},
}

@inproceedings{caron_deep_2018,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.html},
	urldate = {2023-01-11},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	year = {2018},
	pages = {132--149},
}

@misc{hjelm_learning_2019,
	title = {Learning deep representations by mutual information estimation and maximization},
	url = {http://arxiv.org/abs/1808.06670},
	doi = {10.48550/arXiv.1808.06670},
	abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	month = feb,
	year = {2019},
	note = {arXiv:1808.06670 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{taunk_brief_2019,
	title = {A {Brief} {Review} of {Nearest} {Neighbor} {Algorithm} for {Learning} and {Classification}},
	doi = {10.1109/ICCS45141.2019.9065747},
	abstract = {k-Nearest Neighbor (kNN) algorithm is an effortless but productive machine learning algorithm. It is effective for classification as well as regression. However, it is more widely used for classification prediction. kNN groups the data into coherent clusters or subsets and classifies the newly inputted data based on its similarity with previously trained data. The input is assigned to the class with which it shares the most nearest neighbors. Though kNN is effective, it has many weaknesses. This paper highlights the kNN method and its modified versions available in previously done researches. These variants remove the weaknesses of kNN and provide a more efficient method.},
	booktitle = {2019 {International} {Conference} on {Intelligent} {Computing} and {Control} {Systems} ({ICCS})},
	author = {Taunk, Kashvi and De, Sanjukta and Verma, Srishti and Swetapadma, Aleena},
	month = may,
	year = {2019},
	keywords = {Classification algorithms, Clustering algorithms, Confusion Matrix, Euclidean distance, K-Nearest Neighbors, Kernel Matrix., Lazy Learner, Machine Learning, Prediction algorithms, Support vector machines, Training, Training data, Unlabeled data set},
	pages = {1255--1260},
}

@misc{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	doi = {10.48550/arXiv.1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1502.03167 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{barlow_redundancy_2001,
	title = {Redundancy reduction revisited},
	volume = {12},
	issn = {0954-898X},
	url = {https://doi.org/10.1080/net.12.3.241.253},
	doi = {10.1080/net.12.3.241.253},
	abstract = {Soon after Shannon defined the concept of redundancy it was suggested that it gave insight into mechanisms of sensory processing, perception, intelligence and inference. Can we now judge whether there is anything in this idea, and can we see where it should direct our thinking? This paper argues that the original hypothesis was wrong in over-emphasizing the role of compressive coding and economy in neuron numbers, but right in drawing attention to the importance of redundancy. Furthermore there is a clear direction in which it now points, namely to the overwhelming importance of probabilities and statistics in neuroscience. The brain has to decide upon actions in a competitive, chance-driven world, and to do this well it must know about and exploit the non-random probabilities and interdependences of objects and events signalled by sensory messages. These are particularly relevant for Bayesian calculations of the optimum course of action. Instead of thinking of neural representations as transformations of stimulus energies, we should regard them as approximate estimates of the probable truths of hypotheses about the current environment, for these are the quantities required by a probabilistic brain working on Bayesian principles.},
	number = {3},
	urldate = {2023-01-02},
	journal = {Network: Computation in Neural Systems},
	author = {Barlow, H.},
	month = jan,
	year = {2001},
	pmid = {11563528},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/net.12.3.241.253},
	pages = {241--253},
}

@misc{noauthor_nistsematech_nodate,
	title = {{NIST}/{SEMATECH} e-{Handbook} of {Statistical} {Methods}},
	url = {https://www.itl.nist.gov/div898/handbook/},
	urldate = {2023-01-02},
}

@article{popescu_multilayer_2009,
	title = {Multilayer perceptron and neural networks},
	volume = {8},
	abstract = {The attempts for solving linear inseparable problems have led to different variations on the number of layers of neurons and activation functions used. The backpropagation algorithm is the most known and used supervised learning algorithm. Also called the generalized delta algorithm because it expands the training way of the adaline network, it is based on minimizing the difference between the desired output and the actual output, through the downward gradient method (the gradient tells us how a function varies in different directions). Training a multilayer perceptron is often quite slow, requiring thousands or tens of thousands of epochs for complex problems. The best known methods to accelerate learning are: the momentum method and applying a variable learning rate. The paper presents the possibility to control the induction driving using neural systems.},
	journal = {WSEAS Transactions on Circuits and Systems},
	author = {Popescu, Marius-Constantin and Balas, Valentina and Perescu-Popescu, Liliana and Mastorakis, Nikos},
	month = jul,
	year = {2009},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ziyin_what_2022,
	title = {What shapes the loss landscape of self-supervised learning?},
	url = {http://arxiv.org/abs/2210.00638},
	doi = {10.48550/arXiv.2210.00638},
	abstract = {Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We provide answers to these questions by thoroughly analyzing SSL loss landscapes for a linear model. We derive an analytically tractable theory of SSL landscape and show that it accurately captures an array of collapse phenomena and identifies their causes. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {Ziyin, Liu and Lubana, Ekdeep Singh and Ueda, Masahito and Tanaka, Hidenori},
	month = oct,
	year = {2022},
	note = {arXiv:2210.00638 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability},
}

@misc{jing_understanding_2022,
	title = {Understanding {Dimensional} {Collapse} in {Contrastive} {Self}-supervised {Learning}},
	url = {http://arxiv.org/abs/2110.09348},
	doi = {10.48550/arXiv.2110.09348},
	abstract = {Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {Jing, Li and Vincent, Pascal and LeCun, Yann and Tian, Yuandong},
	month = apr,
	year = {2022},
	note = {arXiv:2110.09348 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{koch_siamese_nodate,
	title = {Siamese {Neural} {Networks} for {One}-shot {Image} {Recognition}},
	abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difﬁcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiﬁcation tasks.},
	language = {en},
	author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
}

@article{liu_self-supervised_2021,
	title = {Self-supervised {Learning}: {Generative} or {Contrastive}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Self-supervised {Learning}},
	url = {http://arxiv.org/abs/2006.08218},
	doi = {10.1109/TKDE.2021.3090866},
	abstract = {Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.},
	urldate = {2023-01-02},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Wang, Zhaoyu and Mian, Li and Zhang, Jing and Tang, Jie},
	year = {2021},
	note = {arXiv:2006.08218 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--1},
}

@misc{chen_empirical_2021,
	title = {An {Empirical} {Study} of {Training} {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.02057},
	doi = {10.48550/arXiv.2104.02057},
	abstract = {This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
	month = aug,
	year = {2021},
	note = {arXiv:2104.02057 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{chen_improved_2020,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2003.04297},
	doi = {10.48550/arXiv.2003.04297},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	month = mar,
	year = {2020},
	note = {arXiv:2003.04297 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	doi = {10.48550/arXiv.1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv:1911.05722 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tian_contrastive_2020,
	title = {Contrastive {Multiview} {Coding}},
	url = {http://arxiv.org/abs/1906.05849},
	abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a “dog” can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is viewagnostic. We analyze key properties of the approach that make it work, ﬁnding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.},
	language = {en},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	month = dec,
	year = {2020},
	note = {arXiv:1906.05849 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{tschannen_mutual_2020,
	title = {On {Mutual} {Information} {Maximization} for {Representation} {Learning}},
	url = {http://arxiv.org/abs/1907.13625},
	abstract = {Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.},
	language = {en},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K. and Gelly, Sylvain and Lucic, Mario},
	month = jan,
	year = {2020},
	note = {arXiv:1907.13625 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pham_pros_2022,
	title = {On the {Pros} and {Cons} of {Momentum} {Encoder} in {Self}-{Supervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2208.05744},
	doi = {10.48550/arXiv.2208.05744},
	abstract = {Exponential Moving Average (EMA or momentum) is widely used in modern self-supervised learning (SSL) approaches, such as MoCo, for enhancing performance. We demonstrate that such momentum can also be plugged into momentum-free SSL frameworks, such as SimCLR, for a performance boost. Despite its wide use as a fundamental component in modern SSL frameworks, the benefit caused by momentum is not well understood. We find that its success can be at least partly attributed to the stability effect. In the first attempt, we analyze how EMA affects each part of the encoder and reveal that the portion near the encoder's input plays an insignificant role while the latter parts have much more influence. By monitoring the gradient of the overall loss with respect to the output of each block in the encoder, we observe that the final layers tend to fluctuate much more than other layers during backpropagation, i.e. less stability. Interestingly, we show that using EMA to the final part of the SSL encoder, i.e. projector, instead of the whole deep network encoder can give comparable or preferable performance. Our proposed projector-only momentum helps maintain the benefit of EMA but avoids the double forward computation.},
	urldate = {2022-12-23},
	publisher = {arXiv},
	author = {Pham, Trung and Zhang, Chaoning and Niu, Axi and Zhang, Kang and Yoo, Chang D.},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05744 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{conneau_unsupervised_2020,
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {http://arxiv.org/abs/1911.02116},
	doi = {10.48550/arXiv.1911.02116},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = apr,
	year = {2020},
	note = {arXiv:1911.02116 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{conneau_xnli_2018,
	address = {Brussels, Belgium},
	title = {{XNLI}: {Evaluating} {Cross}-lingual {Sentence} {Representations}},
	shorttitle = {{XNLI}},
	url = {https://aclanthology.org/D18-1269},
	doi = {10.18653/v1/D18-1269},
	abstract = {State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.},
	urldate = {2022-12-18},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and Williams, Adina and Bowman, Samuel and Schwenk, Holger and Stoyanov, Veselin},
	month = oct,
	year = {2018},
	pages = {2475--2485},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	doi = {10.48550/arXiv.1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv:1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_deeply_nodate,
	title = {Deeply {Moving}: {Deep} {Learning} for {Sentiment} {Analysis}},
	shorttitle = {Deeply {Moving}},
	url = {http://nlp.stanford.edu/sentiment/index.html},
	abstract = {This website provides a live demo for predicting the sentiment of movie reviews. Most sentiment prediction systems work just by looking at words in isolation, giving positive points for positive words and negative points for negative words and then summing up these points. That way, the order of words is ignored and important information is lost. In constrast, our new deep learning model actually builds up a representation of whole sentences based on the sentence structure. It computes the sentiment based on how words compose the meaning of longer phrases.},
	language = {en},
	urldate = {2022-12-18},
	journal = {Deeply Moving: Deep Learning for Sentiment Analysis},
}

@misc{williams_broad-coverage_2018,
	title = {A {Broad}-{Coverage} {Challenge} {Corpus} for {Sentence} {Understanding} through {Inference}},
	url = {http://arxiv.org/abs/1704.05426},
	doi = {10.48550/arXiv.1704.05426},
	abstract = {This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R.},
	month = feb,
	year = {2018},
	note = {arXiv:1704.05426 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lewis_mlqa_2020,
	title = {{MLQA}: {Evaluating} {Cross}-lingual {Extractive} {Question} {Answering}},
	shorttitle = {{MLQA}},
	url = {http://arxiv.org/abs/1910.07475},
	doi = {10.48550/arXiv.1910.07475},
	abstract = {Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making training QA systems in other languages challenging. An alternative to building large monolingual training datasets is to develop cross-lingual systems which can transfer to a target language without requiring training data in that language. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. It consists of over 12K QA instances in English and 5K in each other language, with each QA instance being parallel between 4 languages on average. MLQA is built using a novel alignment context strategy on Wikipedia articles, and serves as a cross-lingual extension to existing extractive QA datasets. We evaluate current state-of-the-art cross-lingual representations on MLQA, and also provide machine-translation-based baselines. In all cases, transfer results are shown to be significantly behind training-language performance.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Lewis, Patrick and Oğuz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
	month = may,
	year = {2020},
	note = {arXiv:1910.07475 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_language-independent_nodate,
	title = {Language-{Independent} {Named} {Entity} {Recognition} ({I})},
	url = {https://www.clips.uantwerpen.be/conll2002/ner/},
	urldate = {2022-12-18},
}

@misc{wenzek_ccnet_2019,
	title = {{CCNet}: {Extracting} {High} {Quality} {Monolingual} {Datasets} from {Web} {Crawl} {Data}},
	shorttitle = {{CCNet}},
	url = {http://arxiv.org/abs/1911.00359},
	doi = {10.48550/arXiv.1911.00359},
	abstract = {Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzmán, Francisco and Joulin, Armand and Grave, Edouard},
	month = nov,
	year = {2019},
	note = {arXiv:1911.00359 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_common_nodate,
	title = {Common {Crawl}},
	url = {https://commoncrawl.org/},
	language = {en-US},
	urldate = {2022-12-18},
}

@misc{kudo_sentencepiece_2018,
	title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
	shorttitle = {{SentencePiece}},
	url = {http://arxiv.org/abs/1808.06226},
	doi = {10.48550/arXiv.1808.06226},
	abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Kudo, Taku and Richardson, John},
	month = aug,
	year = {2018},
	note = {arXiv:1808.06226 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lample_cross-lingual_2019,
	title = {Cross-lingual {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/1901.07291},
	doi = {10.48550/arXiv.1901.07291},
	abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Lample, Guillaume and Conneau, Alexis},
	month = jan,
	year = {2019},
	note = {arXiv:1901.07291 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language},
}

@incollection{de_bruijne_simtriplet_2021,
	address = {Cham},
	title = {{SimTriplet}: {Simple} {Triplet} {Representation} {Learning} with a {Single} {GPU}},
	volume = {12902},
	isbn = {978-3-030-87195-6 978-3-030-87196-3},
	shorttitle = {{SimTriplet}},
	url = {https://link.springer.com/10.1007/978-3-030-87196-3_10},
	abstract = {Contrastive learning is a key technique of modern selfsupervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16 GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58\% better performance compared with supervised learning. It also achieved 2.13\% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1\% labeled data. The code and data are available at https:// github.com/hrlblab/SimTriplet.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Liu, Quan and Louis, Peter C. and Lu, Yuzhe and Jha, Aadarsh and Zhao, Mengyang and Deng, Ruining and Yao, Tianyuan and Roland, Joseph T. and Yang, Haichun and Zhao, Shilin and Wheless, Lee E. and Huo, Yuankai},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	doi = {10.1007/978-3-030-87196-3_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {102--112},
}

@incollection{de_bruijne_simtriplet_2021-1,
	address = {Cham},
	title = {{SimTriplet}: {Simple} {Triplet} {Representation} {Learning} with a {Single} {GPU}},
	volume = {12902},
	isbn = {978-3-030-87195-6 978-3-030-87196-3},
	shorttitle = {{SimTriplet}},
	url = {https://link.springer.com/10.1007/978-3-030-87196-3_10},
	abstract = {Contrastive learning is a key technique of modern selfsupervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16 GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58\% better performance compared with supervised learning. It also achieved 2.13\% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1\% labeled data. The code and data are available at https:// github.com/hrlblab/SimTriplet.},
	language = {en},
	urldate = {2022-12-07},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Liu, Quan and Louis, Peter C. and Lu, Yuzhe and Jha, Aadarsh and Zhao, Mengyang and Deng, Ruining and Yao, Tianyuan and Roland, Joseph T. and Yang, Haichun and Zhao, Shilin and Wheless, Lee E. and Huo, Yuankai},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	doi = {10.1007/978-3-030-87196-3_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {102--112},
}

@misc{chen_improved_2020-1,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2003.04297},
	doi = {10.48550/arXiv.2003.04297},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	month = mar,
	year = {2020},
	note = {arXiv:2003.04297 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chuang_debiased_2020,
	title = {Debiased {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2007.00224},
	doi = {10.48550/arXiv.2007.00224},
	abstract = {A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Yen-Chen, Lin and Torralba, Antonio and Jegelka, Stefanie},
	month = oct,
	year = {2020},
	note = {arXiv:2007.00224 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_kvt_2022,
	title = {{KVT}: k-{NN} {Attention} for {Boosting} {Vision} {Transformers}},
	shorttitle = {{KVT}},
	url = {http://arxiv.org/abs/2106.00515},
	doi = {10.48550/arXiv.2106.00515},
	abstract = {Convolutional Neural Networks (CNNs) have dominated computer vision for years, due to its ability in capturing locality and translation invariance. Recently, many vision transformer architectures have been proposed and they show promising performance. A key component in vision transformers is the fully-connected self-attention which is more powerful than CNNs in modelling long range dependencies. However, since the current dense self-attention uses all image patches (tokens) to compute attention matrix, it may neglect locality of images patches and involve noisy tokens (e.g., clutter background and occlusion), leading to a slow training process and potential degradation of performance. To address these problems, we propose the \$k\$-NN attention for boosting vision transformers. Specifically, instead of involving all the tokens for attention matrix calculation, we only select the top-\$k\$ similar tokens from the keys for each query to compute the attention map. The proposed \$k\$-NN attention naturally inherits the local bias of CNNs without introducing convolutional operations, as nearby tokens tend to be more similar than others. In addition, the \$k\$-NN attention allows for the exploration of long range correlation and at the same time filters out irrelevant tokens by choosing the most similar tokens from the entire image. Despite its simplicity, we verify, both theoretically and empirically, that \$k\$-NN attention is powerful in speeding up training and distilling noise from input tokens. Extensive experiments are conducted by using 11 different vision transformer architectures to verify that the proposed \$k\$-NN attention can work with any existing transformer architectures to improve its prediction performance. The codes are available at {\textbackslash}url\{https://github.com/damo-cv/KVT\}.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Wang, Pichao and Wang, Xue and Wang, Fan and Lin, Ming and Chang, Shuning and Li, Hao and Jin, Rong},
	month = jul,
	year = {2022},
	note = {arXiv:2106.00515 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{laakom_reducing_2022,
	title = {Reducing {Redundancy} in the {Bottleneck} {Representation} of the {Autoencoders}},
	url = {http://arxiv.org/abs/2202.04629},
	doi = {10.48550/arXiv.2202.04629},
	abstract = {Autoencoders are a type of unsupervised neural networks, which can be used to solve various tasks, e.g., dimensionality reduction, image compression, and image denoising. An AE has two goals: (i) compress the original input to a low-dimensional space at the bottleneck of the network topology using an encoder, (ii) reconstruct the input from the representation at the bottleneck using a decoder. Both encoder and decoder are optimized jointly by minimizing a distortion-based loss which implicitly forces the model to keep only those variations of input data that are required to reconstruct the and to reduce redundancies. In this paper, we propose a scheme to explicitly penalize feature redundancies in the bottleneck representation. To this end, we propose an additional loss term, based on the pair-wise correlation of the neurons, which complements the standard reconstruction loss forcing the encoder to learn a more diverse and richer representation of the input. We tested our approach across different tasks: dimensionality reduction using three different dataset, image compression using the MNIST dataset, and image denoising using fashion MNIST. The experimental results show that the proposed loss leads consistently to superior performance compared to the standard AE loss.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Laakom, Firas and Raitoharju, Jenni and Iosifidis, Alexandros and Gabbouj, Moncef},
	month = nov,
	year = {2022},
	note = {arXiv:2202.04629 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{niu_home_2022,
	title = {{HOME}: {High}-{Order} {Mixed}-{Moment}-based {Embedding} for {Representation} {Learning}},
	shorttitle = {{HOME}},
	url = {http://arxiv.org/abs/2207.07743},
	doi = {10.48550/arXiv.2207.07743},
	abstract = {Minimum redundancy among different elements of an embedding in a latent space is a fundamental requirement or major preference in representation learning to capture intrinsic informational structures. Current self-supervised learning methods minimize a pair-wise covariance matrix to reduce the feature redundancy and produce promising results. However, such representation features of multiple variables may contain the redundancy among more than two feature variables that cannot be minimized via the pairwise regularization. Here we propose the High-Order Mixed-Moment-based Embedding (HOME) strategy to reduce the redundancy between any sets of feature variables, which is to our best knowledge the first attempt to utilize high-order statistics/information in this context. Multivariate mutual information is minimum if and only if multiple variables are mutually independent, which suggests the necessary conditions of factorized mixed moments among multiple variables. Based on these statistical and information theoretic principles, our general HOME framework is presented for self-supervised representation learning. Our initial experiments show that a simple version in the form of a three-order HOME scheme already significantly outperforms the current two-order baseline method (i.e., Barlow Twins) in terms of the linear evaluation on representation features.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Niu, Chuang and Wang, Ge},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07743 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{marusov_non-contrastive_2022,
	title = {Non-contrastive approaches to similarity learning: positive examples are all you need},
	shorttitle = {Non-contrastive approaches to similarity learning},
	url = {http://arxiv.org/abs/2209.14750},
	doi = {10.48550/arXiv.2209.14750},
	abstract = {The similarity learning problem in the oil {\textbackslash}\& gas industry aims to construct a model that estimates similarity between interval measurements for logging data. Previous attempts are mostly based on empirical rules, so our goal is to automate this process and exclude expensive and time-consuming expert labelling. One of the approaches for similarity learning is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Thus, we can learn such models even if the data labelling is absent or scarce. Nowadays, most SSL approaches are contrastive and non-contrastive. However, due to possible wrong labelling of positive and negative samples, contrastive methods don't scale well with the number of objects. Non-contrastive methods don't rely on negative samples. Such approaches are actively used in the computer vision. We introduce non-contrastive SSL for time series data. In particular, we build on top of BYOL and Barlow Twins methods that avoid using negative pairs and focus only on matching positive pairs. The crucial part of these methods is an augmentation strategy. Different augmentations of time series exist, while their effect on the performance can be both positive and negative. Our augmentation strategies and adaption for BYOL and Barlow Twins together allow us to achieve a higher quality (ARI \$= 0.49\$) than other self-supervised methods (ARI \$= 0.34\$ only), proving usefulness of the proposed non-contrastive self-supervised approach for the interval similarity problem and time series representation learning in general.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Marusov, Alexander and Baianov, Valerii and Zaytsev, Alexey},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14750 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{kendall_multi-task_2018,
	title = {Multi-{Task} {Learning} {Using} {Uncertainty} to {Weigh} {Losses} for {Scene} {Geometry} and {Semantics}},
	url = {http://arxiv.org/abs/1705.07115},
	doi = {10.48550/arXiv.1705.07115},
	abstract = {Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
	month = apr,
	year = {2018},
	note = {arXiv:1705.07115 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liebel_auxiliary_2018,
	title = {Auxiliary {Tasks} in {Multi}-task {Learning}},
	url = {http://arxiv.org/abs/1805.06334},
	doi = {10.48550/arXiv.1805.06334},
	abstract = {Multi-task convolutional neural networks (CNNs) have shown impressive results for certain combinations of tasks, such as single-image depth estimation (SIDE) and semantic segmentation. This is achieved by pushing the network towards learning a robust representation that generalizes well to different atomic tasks. We extend this concept by adding auxiliary tasks, which are of minor relevance for the application, to the set of learned tasks. As a kind of additional regularization, they are expected to boost the performance of the ultimately desired main tasks. To study the proposed approach, we picked vision-based road scene understanding (RSU) as an exemplary application. Since multi-task learning requires specialized datasets, particularly when using extensive sets of tasks, we provide a multi-modal dataset for multi-task RSU, called synMT. More than 2.5 \${\textbackslash}cdot\$ 10{\textasciicircum}5 synthetic images, annotated with 21 different labels, were acquired from the video game Grand Theft Auto V (GTA V). Our proposed deep multi-task CNN architecture was trained on various combination of tasks using synMT. The experiments confirmed that auxiliary tasks can indeed boost network performance, both in terms of final results and training time.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Liebel, Lukas and Körner, Marco},
	month = may,
	year = {2018},
	note = {arXiv:1805.06334 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{jing_understanding_2022-1,
	title = {{UNDERSTANDING} {DIMENSIONAL} {COLLAPSE} {IN} {CON}- {TRASTIVE} {SELF}-{SUPERVISED} {LEARNING}},
	abstract = {Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on a trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.},
	language = {en},
	author = {Jing, Li and Vincent, Pascal and LeCun, Yann and Tian, Yuandong},
	year = {2022},
	pages = {17},
}

@inproceedings{jing_understanding_2022-2,
	title = {Understanding {Dimensional} {Collapse} in {Contrastive} {Self}-supervised {Learning}},
	url = {https://openreview.net/forum?id=YevsQ05DEN7},
	abstract = {Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on a trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.},
	language = {en},
	urldate = {2022-11-24},
	author = {Jing, Li and Vincent, Pascal and LeCun, Yann and Tian, Yuandong},
	month = mar,
	year = {2022},
}

@misc{shi_run_2020,
	title = {Run {Away} {From} your {Teacher}: {Understanding} {BYOL} by a {Novel} {Self}-{Supervised} {Approach}},
	shorttitle = {Run {Away} {From} your {Teacher}},
	url = {http://arxiv.org/abs/2011.10944},
	doi = {10.48550/arXiv.2011.10944},
	abstract = {Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Shi, Haizhou and Luo, Dongliang and Tang, Siliang and Wang, Jian and Zhuang, Yueting},
	month = nov,
	year = {2020},
	note = {arXiv:2011.10944 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{frosst_analyzing_2019,
	title = {Analyzing and {Improving} {Representations} with the {Soft} {Nearest} {Neighbor} {Loss}},
	url = {http://arxiv.org/abs/1902.01889},
	doi = {10.48550/arXiv.1902.01889},
	abstract = {We explore and expand the \${\textbackslash}textit\{Soft Nearest Neighbor Loss\}\$ to measure the \${\textbackslash}textit\{entanglement\}\$ of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that \${\textbackslash}textit\{maximizing\}\$ the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey},
	month = feb,
	year = {2019},
	note = {arXiv:1902.01889 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{appalaraju_towards_2020,
	title = {Towards {Good} {Practices} in {Self}-supervised {Representation} {Learning}},
	url = {http://arxiv.org/abs/2012.00868},
	doi = {10.48550/arXiv.2012.00868},
	abstract = {Self-supervised representation learning has seen remarkable progress in the last few years. More recently, contrastive instance learning has shown impressive results compared to its supervised learning counterparts. However, even with the ever increased interest in contrastive instance learning, it is still largely unclear why these methods work so well. In this paper, we aim to unravel some of the mysteries behind their success, which are the good practices. Through an extensive empirical analysis, we hope to not only provide insights but also lay out a set of best practices that led to the success of recent work in self-supervised representation learning.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Appalaraju, Srikar and Zhu, Yi and Xie, Yusheng and Fehérvári, István},
	month = dec,
	year = {2020},
	note = {arXiv:2012.00868 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{goldfeld_information_2020,
	title = {The {Information} {Bottleneck} {Problem} and {Its} {Applications} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/2004.14941},
	doi = {10.48550/arXiv.2004.14941},
	abstract = {Inference capabilities of machine learning (ML) systems skyrocketed in recent years, now playing a pivotal role in various aspect of society. The goal in statistical learning is to use data to obtain simple algorithms for predicting a random variable \$Y\$ from a correlated observation \$X\$. Since the dimension of \$X\$ is typically huge, computationally feasible solutions should summarize it into a lower-dimensional feature vector \$T\$, from which \$Y\$ is predicted. The algorithm will successfully make the prediction if \$T\$ is a good proxy of \$Y\$, despite the said dimensionality-reduction. A myriad of ML algorithms (mostly employing deep learning (DL)) for finding such representations \$T\$ based on real-world data are now available. While these methods are often effective in practice, their success is hindered by the lack of a comprehensive theory to explain it. The information bottleneck (IB) theory recently emerged as a bold information-theoretic paradigm for analyzing DL systems. Adopting mutual information as the figure of merit, it suggests that the best representation \$T\$ should be maximally informative about \$Y\$ while minimizing the mutual information with \$X\$. In this tutorial we survey the information-theoretic origins of this abstract principle, and its recent impact on DL. For the latter, we cover implications of the IB problem on DL theory, as well as practical algorithms inspired by it. Our goal is to provide a unified and cohesive description. A clear view of current knowledge is particularly important for further leveraging IB and other information-theoretic ideas to study DL models.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Goldfeld, Ziv and Polyanskiy, Yury},
	month = may,
	year = {2020},
	note = {arXiv:2004.14941 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{goyal_self-supervised_2021,
	title = {Self-supervised {Pretraining} of {Visual} {Features} in the {Wild}},
	url = {http://arxiv.org/abs/2103.01988},
	doi = {10.48550/arXiv.2103.01988},
	abstract = {Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2\% top-1 accuracy, surpassing the best self-supervised pretrained model by 1\% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9\% top-1 with access to only 10\% of ImageNet. Code: https://github.com/facebookresearch/vissl},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and Bojanowski, Piotr},
	month = mar,
	year = {2021},
	note = {arXiv:2103.01988 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{anonymous_soft_2022,
	title = {Soft {Neighbors} are {Positive} {Supporters} in {Contrastive} {Visual} {Representation} {Learning}},
	url = {https://openreview.net/forum?id=l9vM_PaUKz},
	abstract = {Contrastive learning methods train visual encoders by comparing views (e.g., often created via a group of data augmentations on the same instance) from one instance to others. Typically, the views created from one instance are set as positive, while views from other instances are negative. This binary instance discrimination is studied extensively to improve feature representations in self-supervised learning. In this paper, we rethink the instance discrimination framework and find the binary instance labeling insufficient to measure correlations between different samples. For an intuitive example, given a random image instance, there may exist other images in a mini-batch whose content meanings are the same (i.e., belonging to the same category) or partially related (i.e., belonging to a similar category). How to treat the images that correlate similarly to the current image instance leaves an unexplored problem. We thus propose to support the current image by exploring other correlated instances (i.e., soft neighbors). We first carefully cultivate a candidate neighbor set, which will be further utilized to explore the highly-correlated instances. A cross-attention module is then introduced to predict the correlation score (denoted as positiveness) of other correlated instances with respect to the current one. The positiveness score quantitatively measures the positive support from each correlated instance, and is encoded into the objective for pretext training. To this end, our proposed method benefits in discriminating uncorrelated instances while absorbing correlated instances for SSL. We evaluate our soft neighbor contrastive learning method (SNCLR) on standard visual recognition benchmarks, including image classification, object detection, and instance segmentation. The state-of-the-art recognition performance shows that SNCLR is effective in improving feature representations from both ViT and CNN encoders. More materials can be found in our project page: anonymous-iclr23snclr.github.io.},
	language = {en},
	urldate = {2022-11-24},
	author = {Anonymous},
	month = nov,
	year = {2022},
}

@misc{koohpayegani_mean_2021,
	title = {Mean {Shift} for {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2105.07269},
	doi = {10.48550/arXiv.2105.07269},
	abstract = {Most recent self-supervised learning (SSL) algorithms learn features by contrasting between instances of images or by clustering the images and then contrasting between the image clusters. We introduce a simple mean-shift algorithm that learns representations by grouping images together without contrasting between them or adopting much of prior on the structure of the clusters. We simply "shift" the embedding of each image to be close to the "mean" of its neighbors. Since in our setting, the closest neighbor is always another augmentation of the same image, our model will be identical to BYOL when using only one nearest neighbor instead of 5 as used in our experiments. Our model achieves 72.4\% on ImageNet linear evaluation with ResNet50 at 200 epochs outperforming BYOL. Our code is available here: https://github.com/UMBCvision/MSF},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Koohpayegani, Soroush Abbasi and Tejankar, Ajinkya and Pirsiavash, Hamed},
	month = sep,
	year = {2021},
	note = {arXiv:2105.07269 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{dwibedi_little_2021,
	title = {With a {Little} {Help} from {My} {Friends}: {Nearest}-{Neighbor} {Contrastive} {Learning} of {Visual} {Representations}},
	shorttitle = {With a {Little} {Help} from {My} {Friends}},
	url = {http://arxiv.org/abs/2104.14548},
	doi = {10.48550/arXiv.2104.14548},
	abstract = {Self-supervised learning algorithms based on instance discrimination train encoders to be invariant to pre-defined transformations of the same instance. While most methods treat different views of the same image as positives for a contrastive loss, we are interested in using positives from other instances in the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), samples the nearest neighbors from the dataset in the latent space, and treats them as positives. This provides more semantic variations than pre-defined transformations. We find that using the nearest-neighbor as positive in contrastive losses improves performance significantly on ImageNet classification, from 71.7\% to 75.6\%, outperforming previous state-of-the-art methods. On semi-supervised learning benchmarks we improve performance significantly when only 1\% ImageNet labels are available, from 53.8\% to 56.5\%. On transfer learning benchmarks our method outperforms state-of-the-art methods (including supervised learning with ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate empirically that our method is less reliant on complex data augmentations. We see a relative reduction of only 2.1\% ImageNet Top-1 accuracy when we train using only random crops.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
	month = oct,
	year = {2021},
	note = {arXiv:2104.14548 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	doi = {10.48550/arXiv.2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	month = jun,
	year = {2021},
	note = {arXiv:2103.03230 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_simple_2020-1,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{grill_bootstrap_2020,
	title = {Bootstrap your own latent: {A} new approach to self-supervised {Learning}},
	shorttitle = {Bootstrap your own latent},
	url = {http://arxiv.org/abs/2006.07733},
	doi = {10.48550/arXiv.2006.07733},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6{\textbackslash}\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	month = sep,
	year = {2020},
	note = {arXiv:2006.07733 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_simple_2020-2,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2011.10566},
	doi = {10.48550/arXiv.2011.10566},
	abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chen, Xinlei and He, Kaiming},
	month = nov,
	year = {2020},
	note = {arXiv:2011.10566 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{chen_simple_2020-3,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_simple_2020-4,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	doi = {10.48550/arXiv.2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_explainpaper_nodate,
	title = {Explainpaper},
	url = {https://www.explainpaper.com/},
	urldate = {2022-11-06},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{dalton_cast_2020,
	title = {{CAsT} 2020: {The} {Conversational} {Assistance} {Track} {Overview}},
	language = {en},
	author = {Dalton, Jeffrey and Xiong, Chenyan and Callan, Jamie},
	year = {2020},
	pages = {10},
}

@inproceedings{dalton_cast-19_2020,
	address = {Virtual Event China},
	title = {{CAsT}-19: {A} {Dataset} for {Conversational} {Information} {Seeking}},
	isbn = {978-1-4503-8016-4},
	shorttitle = {{CAsT}-19},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401206},
	doi = {10.1145/3397271.3401206},
	abstract = {CAsT-19 is a new dataset that supports research on conversational information seeking. The corpus is 38,426,252 passages from the TREC Complex Answer Retrieval (CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty information seeking dialogues (30 train, 50 test) are an average of 9 to 10 questions long. A dialogue may explore a topic broadly or drill down into subtopics. Questions contain ellipsis, implied context, mild topic shifts, and other characteristics of human conversation that may prevent them from being understood in isolation. Relevance assessments are provided for 30 training topics and 20 test topics. CAsT-19 promotes research on conversational information seeking by defining it as a task in which effective passage selection requires understanding a question’s context (the dialogue history). It focuses attention on user modeling, analysis of prior retrieval results, transformation of questions into effective queries, and other topics that have been difficult to study with existing datasets.},
	language = {en},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Dalton, Jeffrey and Xiong, Chenyan and Kumar, Vaibhav and Callan, Jamie},
	month = jul,
	year = {2020},
	pages = {1985--1988},
}

@misc{bajaj_ms_2018,
	title = {{MS} {MARCO}: {A} {Human} {Generated} {MAchine} {Reading} {COmprehension} {Dataset}},
	shorttitle = {{MS} {MARCO}},
	url = {http://arxiv.org/abs/1611.09268},
	doi = {10.48550/arXiv.1611.09268},
	abstract = {We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
	month = oct,
	year = {2018},
	note = {arXiv:1611.09268 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{anantha_open-domain_2021,
	title = {Open-{Domain} {Question} {Answering} {Goes} {Conversational} via {Question} {Rewriting}},
	url = {http://arxiv.org/abs/2010.04898},
	doi = {10.48550/arXiv.2010.04898},
	abstract = {We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Anantha, Raviteja and Vakulenko, Svitlana and Tu, Zhucheng and Longpre, Shayne and Pulman, Stephen and Chappidi, Srinivas},
	month = apr,
	year = {2021},
	note = {arXiv:2010.04898 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@inproceedings{qu_open-retrieval_2020,
	title = {Open-{Retrieval} {Conversational} {Question} {Answering}},
	url = {http://arxiv.org/abs/2005.11364},
	doi = {10.1145/3397271.3401110},
	abstract = {Conversational search is one of the ultimate goals of information retrieval. Recent research approaches conversational search by simplified settings of response ranking and conversational question answering, where an answer is either selected from a given candidate set or extracted from a given passage. These simplifications neglect the fundamental role of retrieval in conversational search. To address this limitation, we introduce an open-retrieval conversational question answering (ORConvQA) setting, where we learn to retrieve evidence from a large collection before extracting answers, as a further step towards building functional conversational search systems. We create a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an end-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader that are all based on Transformers. Our extensive experiments on OR-QuAC demonstrate that a learnable retriever is crucial for ORConvQA. We further show that our system can make a substantial improvement when we enable history modeling in all system components. Moreover, we show that the reranker component contributes to the model performance by providing a regularization effect. Finally, further in-depth analyses are performed to provide new insights into ORConvQA.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Qu, Chen and Yang, Liu and Chen, Cen and Qiu, Minghui and Croft, W. Bruce and Iyyer, Mohit},
	month = jul,
	year = {2020},
	note = {arXiv:2005.11364 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	pages = {539--548},
}

@misc{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	language = {en},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{devlin_bert_2019-1,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{dai_dialog_2022,
	title = {Dialog {Inpainting}: {Turning} {Documents} into {Dialogs}},
	shorttitle = {Dialog {Inpainting}},
	url = {http://arxiv.org/abs/2205.09073},
	doi = {10.48550/arXiv.2205.09073},
	abstract = {Many important questions (e.g. "How to eat healthier?") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40\% relative gains on standard evaluation metrics.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Dai, Zhuyun and Chaganty, Arun Tejasvi and Zhao, Vincent and Amini, Aida and Rashid, Qazi Mamunur and Green, Mike and Guu, Kelvin},
	month = may,
	year = {2022},
	note = {arXiv:2205.09073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {SQuAD1}.1 dev {Benchmark} ({Question} {Answering})},
	url = {https://paperswithcode.com/sota/question-answering-on-squad11-dev},
	abstract = {The current state-of-the-art on SQuAD1.1 dev is T5-11B. See a full comparison of 54 papers with code.},
	language = {en},
	urldate = {2022-11-06},
}

@article{arora_information_2016,
	title = {Information seeking tools for specific information seekers in digital age},
	abstract = {Explosion of information has left the scholars on the way from where large number of roads split and he feels like lost there. Whereas the information seeking tools work like a guiding bay to select the correct path. In this article an attempt has been made to make these scholars aware about the different types of Information seeking tools so that time and efforts of them can be saved.},
	journal = {International Journal in management and social science},
	author = {Arora, Mamta},
	month = jan,
	year = {2016},
}

@misc{racaniere_automated_2020,
	title = {Automated curricula through setter-solver interactions},
	url = {http://arxiv.org/abs/1909.12892},
	abstract = {Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula–the breakdown of tasks into simpler, static challenges with dense rewards–to build up to complex behaviors. While curricula are also useful for artiﬁcial agents, handcrafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the ﬁrst to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.},
	language = {en},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Racaniere, Sebastien and Lampinen, Andrew K. and Santoro, Adam and Reichert, David P. and Firoiu, Vlad and Lillicrap, Timothy P.},
	month = jan,
	year = {2020},
	note = {Number: arXiv:1909.12892
arXiv:1909.12892 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2022-06-19},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	keywords = {Computer science},
	pages = {529--533},
}

@article{wiewiora_potential-based_2003,
	title = {Potential-{Based} {Shaping} and {Q}-{Value} {Initialization} are {Equivalent}},
	volume = {19},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1106.5267},
	doi = {10.1613/jair.1190},
	abstract = {Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.},
	urldate = {2022-06-19},
	journal = {Journal of Artificial Intelligence Research},
	author = {Wiewiora, E.},
	month = sep,
	year = {2003},
	note = {arXiv:1106.5267 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {205--208},
}

@misc{jaderberg_reinforcement_2016,
	title = {Reinforcement {Learning} with {Unsupervised} {Auxiliary} {Tasks}},
	url = {http://arxiv.org/abs/1611.05397},
	doi = {10.48550/arXiv.1611.05397},
	abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880{\textbackslash}\% expert human performance, and a challenging suite of first-person, three-dimensional {\textbackslash}emph\{Labyrinth\} tasks leading to a mean speedup in learning of 10\${\textbackslash}times\$ and averaging 87{\textbackslash}\% expert human performance on Labyrinth.},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
	month = nov,
	year = {2016},
	note = {Number: arXiv:1611.05397
arXiv:1611.05397 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{sekar_planning_2020,
	title = {Planning to {Explore} via {Self}-{Supervised} {World} {Models}},
	url = {http://arxiv.org/abs/2005.05960},
	doi = {10.48550/arXiv.2005.05960},
	abstract = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
	month = jun,
	year = {2020},
	note = {Number: arXiv:2005.05960
arXiv:2005.05960 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{pathak_self-supervised_2019,
	title = {Self-{Supervised} {Exploration} via {Disagreement}},
	url = {http://arxiv.org/abs/1906.04161},
	doi = {10.48550/arXiv.1906.04161},
	abstract = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
	month = jun,
	year = {2019},
	note = {Number: arXiv:1906.04161
arXiv:1906.04161 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{burda_large-scale_2018,
	title = {Large-{Scale} {Study} of {Curiosity}-{Driven} {Learning}},
	url = {http://arxiv.org/abs/1808.04355},
	doi = {10.48550/arXiv.1808.04355},
	abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
	month = aug,
	year = {2018},
	note = {Number: arXiv:1808.04355
arXiv:1808.04355 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{pathak_curiosity-driven_2017,
	title = {Curiosity-driven {Exploration} by {Self}-supervised {Prediction}},
	url = {http://arxiv.org/abs/1705.05363},
	doi = {10.48550/arXiv.1705.05363},
	abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
	month = may,
	year = {2017},
	note = {Number: arXiv:1705.05363
arXiv:1705.05363 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{manela_curriculum_2020,
	title = {Curriculum {Learning} with {Hindsight} {Experience} {Replay} for {Sequential} {Object} {Manipulation} {Tasks}},
	url = {http://arxiv.org/abs/2008.09377},
	doi = {10.48550/arXiv.2008.09377},
	abstract = {Learning complex tasks from scratch is challenging and often impossible for humans as well as for artificial agents. A curriculum can be used instead, which decomposes a complex task (target task) into a sequence of source tasks (the curriculum). Each source task is a simplified version of the next source task with increasing complexity. Learning then occurs gradually by training on each source task while using knowledge from the curriculum's prior source tasks. In this study, we present a new algorithm that combines curriculum learning with Hindsight Experience Replay (HER), to learn sequential object manipulation tasks for multiple goals and sparse feedback. The algorithm exploits the recurrent structure inherent in many object manipulation tasks and implements the entire learning process in the original simulation without adjusting it to each source task. We have tested our algorithm on three challenging throwing tasks and show vast improvements compared to vanilla-HER.},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Manela, Binyamin and Biess, Armin},
	month = aug,
	year = {2020},
	note = {Number: arXiv:2008.09377
arXiv:2008.09377 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{riedmiller_learning_2018,
	title = {Learning by {Playing} - {Solving} {Sparse} {Reward} {Tasks} from {Scratch}},
	url = {http://arxiv.org/abs/1802.10567},
	doi = {10.48550/arXiv.1802.10567},
	abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Van de Wiele, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
	month = feb,
	year = {2018},
	note = {Number: arXiv:1802.10567
arXiv:1802.10567 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{florensa_automatic_2018,
	title = {Automatic {Goal} {Generation} for {Reinforcement} {Learning} {Agents}},
	url = {http://arxiv.org/abs/1705.06366},
	doi = {10.48550/arXiv.1705.06366},
	abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
	month = jul,
	year = {2018},
	note = {Number: arXiv:1705.06366
arXiv:1705.06366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{ng_policy_1999,
	title = {Policy {Invariance} {Under} {Reward} {Transformations}: {Theory} and {Application} to {Reward} {Shaping}},
	shorttitle = {Policy {Invariance} {Under} {Reward} {Transformations}},
	url = {https://www.semanticscholar.org/paper/Policy-Invariance-Under-Reward-Transformations%3A-and-Ng-Harada/94066dc12fe31e96af7557838159bde598cb4f10},
	abstract = {Conditions under which modi cations to the reward function of a Markov decision process preserve the op timal policy are investigated to shed light on the practice of reward shap ing a method used in reinforcement learn ing whereby additional training rewards are used to guide the learning agent. This paper investigates conditions under which modi cations to the reward function of a Markov decision process preserve the op timal policy It is shown that besides the positive linear transformation familiar from utility theory one can add a reward for tran sitions between states that is expressible as the di erence in value of an arbitrary poten tial function applied to those states Further more this is shown to be a necessary con dition for invariance in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP These results shed light on the practice of reward shap ing a method used in reinforcement learn ing whereby additional training rewards are used to guide the learning agent In par ticular some well known bugs in reward shaping procedures are shown to arise from non potential based rewards and methods are given for constructing shaping potentials corresponding to distance based and subgoal based heuristics We show that such po tentials can lead to substantial reductions in learning time},
	language = {en},
	urldate = {2022-06-19},
	journal = {undefined},
	author = {Ng, A. and Harada, D. and Russell, Stuart J.},
	year = {1999},
}

@incollection{mataric_reward_1994,
	address = {San Francisco (CA)},
	title = {Reward {Functions} for {Accelerated} {Learning}},
	isbn = {978-1-55860-335-6},
	url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500301},
	abstract = {This paper discusses why traditional reinforcement learning methods, and algorithms applied to those models, result in poor performance in situated domains characterized by multiple goals, noisy state, and inconsistent reinforcement. We propose a methodology for designing reinforcement functions that take advantage of implicit domain knowledge in order to accelerate learning in such domains. The methodology involves the use of heterogeneous reinforcement functions and progress estimators, and applies to learning in domains with a single agent or with multiple agents. The methodology is experimentally validated on a group of mobile robots learning a foraging task.},
	language = {en},
	urldate = {2022-06-19},
	booktitle = {Machine {Learning} {Proceedings} 1994},
	publisher = {Morgan Kaufmann},
	author = {Mataric, Maja J},
	editor = {Cohen, William W. and Hirsh, Haym},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-1-55860-335-6.50030-1},
	pages = {181--189},
}

@misc{jaderberg_reinforcement_2016-1,
	title = {Reinforcement {Learning} with {Unsupervised} {Auxiliary} {Tasks}},
	url = {http://arxiv.org/abs/1611.05397},
	doi = {10.48550/arXiv.1611.05397},
	abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880{\textbackslash}\% expert human performance, and a challenging suite of first-person, three-dimensional {\textbackslash}emph\{Labyrinth\} tasks leading to a mean speedup in learning of 10\${\textbackslash}times\$ and averaging 87{\textbackslash}\% expert human performance on Labyrinth.},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
	month = nov,
	year = {2016},
	note = {Number: arXiv:1611.05397
arXiv:1611.05397 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{noauthor_14062283_nodate,
	title = {[1406.2283] {Depth} {Map} {Prediction} from a {Single} {Image} using a {Multi}-{Scale} {Deep} {Network}},
	url = {https://arxiv.org/abs/1406.2283},
	urldate = {2022-06-12},
}

@misc{noauthor_180501328_nodate,
	title = {[1805.01328] {Evaluation} of {CNN}-based {Single}-{Image} {Depth} {Estimation} {Methods}},
	url = {https://arxiv.org/abs/1805.01328},
	urldate = {2022-06-12},
}

@misc{noauthor_creating_nodate,
	title = {Creating {TFRecords}},
	url = {https://keras.io/examples/keras_recipes/creating_tfrecords/},
	urldate = {2022-06-12},
}

@misc{noauthor_191202792_nodate,
	title = {[1912.02792] {CLOTH3D}: {Clothed} {3D} {Humans}},
	url = {https://arxiv.org/abs/1912.02792},
	urldate = {2022-06-12},
}

@misc{noauthor_monocular_nodate,
	title = {Monocular depth estimation},
	url = {https://keras.io/examples/vision/depth_estimation/},
	urldate = {2022-06-12},
}

@misc{noauthor_hyperparameter_nodate,
	title = {Hyperparameter {Tuning} {Methods} - {Grid}, {Random} or {Bayesian} {Search}? {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399},
	urldate = {2022-06-12},
}

@misc{ramos_yet_2020,
	title = {Yet {Another} {Hindsight} {Experience} {Replay}: {Backstory}},
	shorttitle = {Yet {Another} {Hindsight} {Experience} {Replay}},
	url = {https://medium.com/@jscriptcoder/yet-another-hindsight-experience-replay-backstory-4285c43ff168},
	abstract = {I remember the time I finally got to learn about Reinforcement Learning, after a couple of years completely devoted to…},
	language = {en},
	urldate = {2022-05-20},
	journal = {Medium},
	author = {Ramos, Francisco},
	month = aug,
	year = {2020},
}

@misc{ramos_yet_2020-1,
	title = {Yet {Another} {Hindsight} {Experience} {Replay}: {Target} {Reached}},
	shorttitle = {Yet {Another} {Hindsight} {Experience} {Replay}},
	url = {https://medium.com/@jscriptcoder/yet-another-hindsight-experience-replay-target-reached-600e564ae168},
	abstract = {In the previous post I developed an action plan for this project, where the idea is to use LunarLander environment and adapt it in order…},
	language = {en},
	urldate = {2022-05-20},
	journal = {Medium},
	author = {Ramos, Francisco},
	month = aug,
	year = {2020},
}

@misc{ramos_yet_2020-2,
	title = {Yet {Another} {Hindsight} {Experience} {Replay}: {Refining} the {Plan}},
	shorttitle = {Yet {Another} {Hindsight} {Experience} {Replay}},
	url = {https://medium.com/@jscriptcoder/yet-another-hindsight-experience-replay-refining-the-plan-3dcf8ede6f4a},
	abstract = {In the previous post I pointed out one of the issues in RL, which concerns the Reward function, and laid out the main ideas behind…},
	language = {en},
	urldate = {2022-05-20},
	journal = {Medium},
	author = {Ramos, Francisco},
	month = aug,
	year = {2020},
}

@misc{panchiwala_hindsight-experience-replay_2022,
	title = {Hindsight-{Experience}-{Replay}},
	url = {https://github.com/hemilpanchiwala/Hindsight-Experience-Replay},
	abstract = {Implementation of HindSight Experience Replay paper with Pytorch},
	urldate = {2022-05-20},
	author = {Panchiwala, Hemil},
	month = may,
	year = {2022},
	note = {original-date: 2020-10-04T11:04:07Z},
	keywords = {experience-replay, hindsight-experience-replay, pytorch-implementation, reinforcement-learning},
}

@misc{noauthor_understanding_2018,
	title = {Understanding {DQN}+{HER}},
	url = {https://deeprobotics.wordpress.com/2018/03/07/bitflipper-herdqn/},
	abstract = {In this blog we give insights into the working of DQN (Deep Q-Networks) and HER (Hindsight Experience Replay) One of the main challenges in reinforcement learning is efficiently learning from rewar…},
	language = {en},
	urldate = {2022-05-20},
	journal = {Deep Robotics},
	month = mar,
	year = {2018},
}

@misc{rivlin_reinforcement_2020,
	title = {Reinforcement {Learning} with {Hindsight} {Experience} {Replay}},
	url = {https://towardsdatascience.com/reinforcement-learning-with-hindsight-experience-replay-1fee5704f2f8},
	abstract = {Sparse and Binary Rewards},
	language = {en},
	urldate = {2022-05-20},
	journal = {Medium},
	author = {Rivlin, Or},
	month = jul,
	year = {2020},
}

@techreport{andrychowicz_hindsight_2018,
	title = {Hindsight {Experience} {Replay}},
	url = {http://arxiv.org/abs/1707.01495},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
	number = {arXiv:1707.01495},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
	month = feb,
	year = {2018},
	doi = {10.48550/arXiv.1707.01495},
	note = {arXiv:1707.01495 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@misc{noauthor_papers_nodate-1,
	title = {Papers with {Code} - {SwAV} {Explained}},
	url = {https://paperswithcode.com/method/swav},
	abstract = {SwaV, or Swapping Assignments Between Views, is a self-supervised learning approach that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, it simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, SwaV uses a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view.},
	language = {en},
	urldate = {2022-05-16},
}

@article{gurrutxaga_towards_2011,
	title = {Towards a standard methodology to evaluate internal cluster validity indices},
	volume = {32},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865510003636},
	doi = {10.1016/j.patrec.2010.11.006},
	abstract = {The evaluation and comparison of internal cluster validity indices is a critical problem in the clustering area. The methodology used in most of the evaluations assumes that the clustering algorithms work correctly. We propose an alternative methodology that does not make this often false assumption. We compared 7 internal cluster validity indices with both methodologies and concluded that the results obtained with the proposed methodology are more representative of the actual capabilities of the compared indices.},
	language = {en},
	number = {3},
	urldate = {2022-05-16},
	journal = {Pattern Recognition Letters},
	author = {Gurrutxaga, Ibai and Muguerza, Javier and Arbelaitz, Olatz and Pérez, Jesús M. and Martín, José I.},
	month = feb,
	year = {2011},
	keywords = {Cluster validation, Cluster validity index, Unsupervised learning},
	pages = {505--515},
}

@article{arbelaitz_extensive_2013,
	title = {An extensive comparative study of cluster validity indices},
	volume = {46},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S003132031200338X},
	doi = {10.1016/j.patcog.2012.07.021},
	abstract = {The validation of the results obtained by clustering algorithms is a fundamental part of the clustering process. The most used approaches for cluster validation are based on internal cluster validity indices. Although many indices have been proposed, there is no recent extensive comparative study of their performance. In this paper we show the results of an experimental work that compares 30 cluster validity indices in many different environments with different characteristics. These results can serve as a guideline for selecting the most suitable index for each possible application and provide a deep insight into the performance differences between the currently available indices.},
	language = {en},
	number = {1},
	urldate = {2022-05-16},
	journal = {Pattern Recognition},
	author = {Arbelaitz, Olatz and Gurrutxaga, Ibai and Muguerza, Javier and Pérez, Jesús M. and Perona, Iñigo},
	month = jan,
	year = {2013},
	keywords = {Cluster validity index, Comparative analysis, Crisp clustering},
	pages = {243--256},
}

@article{macqueen_methods_1967,
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {5.1},
	url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992},
	urldate = {2022-05-16},
	journal = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics},
	author = {MacQueen, J.},
	month = jan,
	year = {1967},
	note = {Publisher: University of California Press},
	pages = {281--298},
}

@misc{noauthor_numba_nodate,
	title = {Numba: {A} {High} {Performance} {Python} {Compiler}},
	url = {https://numba.pydata.org/},
	urldate = {2022-05-16},
}

@misc{noauthor_parameter_nodate,
	title = {Parameter {Importance}},
	url = {https://docs.wandb.ai/ref/app/features/panels/parameter-importance},
	abstract = {Visualize the relationships between your model's hyperparameters and output metrics},
	urldate = {2022-05-16},
}

@misc{noauthor_kdd_nodate,
	title = {{KDD} {Cup} 1999 {Data}},
	url = {http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html},
	urldate = {2022-05-16},
}

@misc{noauthor_uci_nodate,
	title = {{UCI} {Machine} {Learning} {Repository}: {Pen}-{Based} {Recognition} of {Handwritten} {Digits} {Data} {Set}},
	url = {http://archive.ics.uci.edu/ml/datasets/pen-based+recognition+of+handwritten+digits},
	urldate = {2022-05-16},
}

@article{jarvis_clustering_1973,
	title = {Clustering {Using} a {Similarity} {Measure} {Based} on {Shared} {Near} {Neighbors}},
	volume = {C-22},
	issn = {1557-9956},
	doi = {10.1109/T-C.1973.223640},
	abstract = {A nonparametric clustering technique incorporating the concept of similarity based on the sharing of near neighbors is presented. In addition to being an essentially paraliel approach, the computational elegance of the method is such that the scheme is applicable to a wide class of practical problems involving large sample size and high dimensionality. No attempt is made to show how a priori problem knowledge can be introduced into the procedure.},
	number = {11},
	journal = {IEEE Transactions on Computers},
	author = {Jarvis, R.A. and Patrick, E.A.},
	month = nov,
	year = {1973},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Clustering, nonparametric, pattern recognition, shared near neighbors, similarity measure.},
	pages = {1025--1034},
}

@misc{noauthor_clustering-benchmarksrcmainresourcesdatasetsartificial_nodate,
	title = {clustering-benchmark/src/main/resources/datasets/artificial at master · deric/clustering-benchmark},
	url = {https://github.com/deric/clustering-benchmark},
	abstract = {Contribute to deric/clustering-benchmark development by creating an account on GitHub.},
	language = {en},
	urldate = {2022-05-16},
	journal = {GitHub},
}

@misc{noauthor_clustering-benchmarkcluto-t8-8karff_nodate,
	title = {clustering-benchmark/cluto-t8-8k.arff at master · deric/clustering-benchmark},
	url = {https://github.com/deric/clustering-benchmark},
	abstract = {Contribute to deric/clustering-benchmark development by creating an account on GitHub.},
	language = {en},
	urldate = {2022-05-16},
	journal = {GitHub},
}

@misc{noauthor_api_nodate,
	title = {{API} {Reference}},
	url = {https://scikit-learn/stable/modules/classes.html},
	abstract = {This is the class and function reference of scikit-learn. Please refer to the full user guide for further details, as the class and function raw specifications may not be enough to give full guidel...},
	language = {en},
	urldate = {2022-05-16},
	journal = {scikit-learn},
}

@misc{noauthor_23_nodate,
	title = {2.3. {Clustering}},
	url = {https://scikit-learn/stable/modules/clustering.html},
	abstract = {Clustering of unlabeled data can be performed with the module sklearn.cluster. Each clustering algorithm comes in two variants: a class, that implements the fit method to learn the clusters on trai...},
	language = {en},
	urldate = {2022-05-16},
	journal = {scikit-learn},
}

@inproceedings{ng_spectral_2001,
	title = {On {Spectral} {Clustering}: {Analysis} and an algorithm},
	volume = {14},
	shorttitle = {On {Spectral} {Clustering}},
	url = {https://proceedings.neurips.cc/paper/2001/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html},
	abstract = {Despite many empirical successes of spectral  clustering  methods(cid:173) algorithms  that  cluster  points  using  eigenvectors  of  matrices  de(cid:173) rived  from  the  data- there  are  several  unresolved  issues.  First,  there  are  a  wide  variety  of  algorithms  that  use  the  eigenvectors  in  slightly  different  ways.  Second,  many of these  algorithms  have  no  proof that  they  will  actually  compute  a  reasonable  clustering.  In  this  paper,  we  present  a  simple  spectral  clustering  algorithm  that can be implemented using a  few  lines  of Matlab.  Using  tools  from  matrix  perturbation  theory,  we  analyze  the  algorithm,  and  give  conditions  under  which  it  can  be  expected  to  do  well.  We  also  show  surprisingly  good  experimental  results  on  a  number  of  challenging clustering problems.},
	urldate = {2022-05-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
	year = {2001},
}

@article{ankerst_optics_nodate,
	title = {{OPTICS}: {Ordering} {Points} {To} {Identify} the {Clustering} {Structure}},
	abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only ‘traditional’ clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.},
	language = {en},
	author = {Ankerst, Mihael and Breunig, Markus M and Kriegel, Hans-Peter and Sander, Jörg},
	pages = {12},
}

@article{ester_density-based_nodate,
	title = {A {Density}-{Based} {Algorithm} for {Discovering} {Clusters} in {Large} {Spatial} {Databases} with {Noise}},
	abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
	language = {en},
	author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
	pages = {6},
}

@inproceedings{sculley_web-scale_2010,
	address = {Raleigh, North Carolina, USA},
	title = {Web-scale k-means clustering},
	isbn = {978-1-60558-799-8},
	url = {http://portal.acm.org/citation.cfm?doid=1772690.1772862},
	doi = {10.1145/1772690.1772862},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Proceedings of the 19th international conference on {World} wide web - {WWW} '10},
	publisher = {ACM Press},
	author = {Sculley, D.},
	year = {2010},
	pages = {1177},
}

@misc{noauthor_networkx_nodate,
	title = {{NetworkX} — {NetworkX} documentation},
	url = {https://networkx.org/},
	urldate = {2022-05-16},
}

@book{ertoz_finding_2003,
	title = {Finding {Clusters} of {Different} {Sizes}, {Shapes}, and {Densities} in {Noisy}, {High} {Dimensional} {Data}},
	abstract = {The problem of finding clusters in data is challenging when clusters are of widely differing sizes, densities and shapes, and when the data contains large amounts of noise and outliers. Many of these issues become even more significant when the data is of very high dimensionality, such as text or time series data. In this paper we present a novel clustering technique that addresses these issues. Our algorithm first finds the nearest neighbors of each data point and then redefines the similarity between pairs of points in terms of how many nearest neighbors the two points share. Using this new definition of similarity, we eliminate noise and outliers, identify core points, and then build clusters around the core points. The use of a shared nearest neighbor definition of similarity removes problems with varying density, while the use of core points handles problems with shape and size. We experimentally show that our algorithm performs better than traditional methods (e.g., K-means) on a variety of data sets: KDD Cup '99 network intrusion data, NASA Earth science time series data, and two dimensional point sets. While our algorithm can find the "dense" clusters that other clustering algorithms find, it also finds clusters that these approaches overlook, i.e., clusters of low or medium density which are of interest because they represent relatively uniform regions "surrounded" by non-uniform or higher density areas. The run-time complexity of our technique is O(n2) since the similarity matrix has to be constructed. However, we discuss a number of optimizations that allow the algorithm to handle large datasets efficiently. For example, 100,000 documents from the TREC collection can be clustered within an hour on a desktop computer.},
	author = {Ertöz, Levent and Steinbach, Michael and Kumar, Vipin},
	month = may,
	year = {2003},
	doi = {10.1137/1.9781611972733.5},
	note = {Journal Abbreviation: SIAM ICDM
Publication Title: SIAM ICDM},
}

@article{lecue_role_2020,
	title = {On the role of knowledge graphs in explainable {AI}},
	volume = {11},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw190374},
	doi = {10.3233/SW-190374},
	abstract = {The current hype of Artificial Intelligence (AI) mostly refers to the success of machine learning and its sub-domain of deep learning. However, AI is also about other areas, such as Knowledge Representation and Reasoning, or Distributed AI, i.e., are},
	language = {en},
	number = {1},
	urldate = {2022-05-08},
	journal = {Semantic Web},
	author = {Lecue, Freddy},
	month = jan,
	year = {2020},
	note = {Publisher: IOS Press},
	pages = {41--51},
}

@inproceedings{hayes_improving_2017,
	title = {Improving {Robot} {Controller} {Transparency} {Through} {Autonomous} {Policy} {Explanation}},
	abstract = {Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.},
	booktitle = {2017 12th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI}},
	author = {Hayes, Bradley and Shah, Julie A.},
	month = mar,
	year = {2017},
	note = {ISSN: 2167-2148},
	pages = {303--312},
}

@book{sohrabi_preferred_2011,
	title = {Preferred {Explanations}: {Theory} and {Generation} via {Planning}.},
	volume = {1},
	shorttitle = {Preferred {Explanations}},
	abstract = {In this paper we examine the general problem of generating preferred explanations for observed behavior with respect to a model of the behavior of a dynamical system. This problem arises in a diversity of applications including diagnosis of dynamical systems and activity recognition. We provide a logical characterization of the notion of an explanation. To generate explanations we identify and exploit a correspondence between explanation generation and planning. The determination of good explanations requires additional domainspecific knowledge which we represent as preferences over explanations. The nature of explanations requires us to formulate preferences in a somewhat retrodictive fashion by utilizing Past Linear Temporal Logic. We propose methods for exploiting these somewhat unique preferences effectively within state-of-the-art planners and illustrate the feasibility of generating (preferred) explanations via planning. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.},
	author = {Sohrabi, Shirin and Baier, Jorge and Mcilraith, Sheila},
	month = jan,
	year = {2011},
	note = {Journal Abbreviation: Proceedings of the National Conference on Artificial Intelligence
Publication Title: Proceedings of the National Conference on Artificial Intelligence},
}

@misc{noauthor_3rd_nodate,
	title = {3rd {International} {Planning} {Competition} {\textbar} {Journal} of {Artificial} {Intelligence} {Research}},
	url = {https://www.jair.org/index.php/jair/specialtrack-ipc3},
	urldate = {2022-05-08},
}

@inproceedings{howey_val_2004,
	title = {{VAL}: automatic plan validation, continuous effects and mixed initiative planning using {PDDL}},
	shorttitle = {{VAL}},
	doi = {10.1109/ICTAI.2004.120},
	abstract = {This work describes aspects of our plan validation tool, VAL. The tool was initially developed to support the 3rd International Planning Competition, but has subsequently been extended in order to exploit its capabilities in plan validation and development. In particular, the tool has been extended to include advanced features of PDDL2.1 which have proved important in mixed-initiative planning in a space operations project. Amongst these features, treatment of continuous effects is the most significant, with important effects on the semantic interpretation of plans. The tool has also been extended to keep abreast of developments in PDDL, providing critical support to participants and organisers of the 4th IPC.},
	booktitle = {16th {IEEE} {International} {Conference} on {Tools} with {Artificial} {Intelligence}},
	author = {Howey, R. and Long, D. and Fox, M.},
	month = nov,
	year = {2004},
	note = {ISSN: 1082-3409},
	keywords = {Batteries, Debugging, Differential equations, Humans, Immune system, Information systems, Power system modeling, Process planning, Radio access networks, Visualization},
	pages = {294--301},
}

@inproceedings{zhang_plan_2017,
	title = {Plan explicability and predictability for robot task planning},
	doi = {10.1109/ICRA.2017.7989155},
	abstract = {Intelligent robots and machines are becoming pervasive in human populated environments. A desirable capability of these agents is to respond to goal-oriented commands by autonomously constructing task plans. However, such autonomy can add significant cognitive load and potentially introduce safety risks to humans when agents behave in unexpected ways. Hence, for such agents to be helpful, one important requirement is for them to synthesize plans that can be easily understood by humans. While there exists previous work that studied socially acceptable robots that interact with humans in “natural ways”, and work that investigated legible motion planning, there is no general solution for high level task planning. To address this issue, we introduce the notions of plan explicability and predictability. To compute these measures, first, we postulate that humans understand agent plans by associating abstract tasks with agent actions, which can be considered as a labeling process. We learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). Then, we use the learned model to label a new plan to compute its explicability and predictability. These measures can be used by agents to proactively choose or directly synthesize plans that are more explicable and predictable to humans. We provide evaluations on a synthetic domain and with a physical robot to demonstrate the effectiveness of our approach.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhang, Yu and Sreedharan, Sarath and Kulkarni, Anagha and Chakraborti, Tathagata and Zhuo, Hankz Hankui and Kambhampati, Subbarao},
	month = may,
	year = {2017},
	keywords = {Computational modeling, Labeling, Planning, Predictive models, Robots, Training},
	pages = {1313--1320},
}

@inproceedings{kim_bayesian_2019,
	address = {Macao, China},
	title = {Bayesian {Inference} of {Linear} {Temporal} {Logic} {Specifications} for {Contrastive} {Explanations}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/776},
	doi = {10.24963/ijcai.2019/776},
	abstract = {Temporal logics are useful for providing concise descriptions of system behavior, and have been successfully used as a language for goal deﬁnitions in task planning. Prior works on inferring temporal logic speciﬁcations have focused on “summarizing” the input dataset – i.e., ﬁnding speciﬁcations that are satisﬁed by all plan traces belonging to the given set. In this paper, we examine the problem of inferring speciﬁcations that describe temporal differences between two sets of plan traces. We formalize the concept of providing such contrastive explanations, then present BayesLTL – a Bayesian probabilistic model for inferring contrastive explanations as linear temporal logic (LTL) speciﬁcations. We demonstrate the robustness and scalability of our model for inferring accurate speciﬁcations from noisy data and across various benchmark planning domains.},
	language = {en},
	urldate = {2022-05-08},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Kim, Joseph and Muise, Christian and Shah, Ankit and Agarwal, Shubham and Shah, Julie},
	month = aug,
	year = {2019},
	pages = {5591--5598},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Biological system modeling, Conferences, Explainable artificial intelligence, Machine learning, Machine learning algorithms, Market research, Prediction algorithms, black-box models, interpretable machine learning},
	pages = {52138--52160},
}

@book{vilone_explainable_2020,
	title = {Explainable {Artificial} {Intelligence}: a {Systematic} {Review}},
	shorttitle = {Explainable {Artificial} {Intelligence}},
	abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
	author = {Vilone, Giulia and Longo, Luca},
	month = may,
	year = {2020},
}

@misc{noauthor_proceedings_nodate,
	title = {Proceedings of the 20th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	url = {https://dl.acm.org/doi/proceedings/10.5555/3463952},
	language = {en},
	urldate = {2022-05-08},
	journal = {ACM Conferences},
}

@inproceedings{meadows_seeing_2013,
	title = {Seeing {Beyond} {Shadows}: {Incremental} {Abductive} {Reasoning} for {Plan} {Understanding}},
	shorttitle = {Seeing {Beyond} {Shadows}},
	abstract = {A new approach to plan understanding that explains observed actions in terms of domain knowledge is presented that operates over hierarchical methods and utilizes an incremental form of data-driven abductive inference. In this paper we present a new approach to plan understanding that explains observed actions in terms of domain knowledge. The process operates over hierarchical methods and utilizes an incremental form of data-driven abductive inference. We report experiments on problems from the Monroe corpus that demonstrate a basic ability to construct plausible explanations, graceful degradation of performance with reduction of the fraction of actions observed, and results with incremental processing that are comparable to batch interpretation. We also discuss research on related tasks such as plan recognition and abductive construction of explanations.},
	booktitle = {{AAAI} {Workshop}: {Plan}, {Activity}, and {Intent} {Recognition}},
	author = {Meadows, B. and Langley, P. and Emery, Miranda},
	year = {2013},
}

@phdthesis{kambhampati_flexible_1989,
	address = {USA},
	type = {phd},
	title = {Flexible reuse and modification in hierarchical planning: a validation structure-based approach},
	shorttitle = {Flexible reuse and modification in hierarchical planning},
	school = {University of Maryland at College Park},
	author = {Kambhampati, Subbarao},
	year = {1989},
	note = {AAI9021520},
}

@inproceedings{chakraborti_balancing_2019,
	address = {Macao, China},
	title = {Balancing {Explicability} and {Explanations} in {Human}-{Aware} {Planning}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/185},
	doi = {10.24963/ijcai.2019/185},
	abstract = {Human-aware planning involves generating plans that are explicable as well as providing explanations when such plans cannot be found. In this paper, we bring these two concepts together and show how an agent can achieve a trade-off between these two competing characteristics of a plan. In order to achieve this, we conceive a ﬁrst of its kind planner MEGA that can augment the possibility of explaining a plan in the plan generation process itself. We situate our discussion in the context of recent work on explicable planning and explanation generation, and illustrate these concepts in two wellknown planning domains, as well as in a demonstration of a robot in a typical search and reconnaissance task. Human factor studies in the latter highlight the usefulness of the proposed approach.},
	language = {en},
	urldate = {2022-05-08},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = aug,
	year = {2019},
	pages = {1335--1343},
}

@inproceedings{greydanus_visualizing_2018,
	title = {Visualizing and {Understanding} {Atari} {Agents}},
	url = {https://proceedings.mlr.press/v80/greydanus18a.html},
	abstract = {While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how agents evolve during learning. We also test our method on non-expert human subjects and find that it improves their ability to reason about these agents. Overall, our results show that saliency information can provide significant insight into an RL agent’s decisions and learning behavior.},
	language = {en},
	urldate = {2022-05-08},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Greydanus, Samuel and Koul, Anurag and Dodge, Jonathan and Fern, Alan},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1792--1801},
}

@article{rosenthal_verbalization_nodate,
	title = {Verbalization: {Narration} of {Autonomous} {Robot} {Experience}},
	abstract = {Autonomous mobile robots navigate in our spaces by planning and executing routes to destinations. When a mobile robot appears at a location, there is no clear way to understand what navigational path the robot planned and experienced just by looking at it. In this work, we address the generation of narrations of autonomous mobile robot navigation experiences. We contribute the concept of verbalization as a parallel to the well-studied concept of visualization. Through verbalizations, robots can describe through language what they experience, in particular in their paths. For every executed path, we consider many possible verbalizations that could be generated. We introduce the verbalization space that covers the variability of utterances that the robot may use to narrate its experience to different humans. We present an algorithm for segmenting a path and mapping each segment to an utterance, as a function of the desired point in the verbalization space, and demonstrate its application using our mobile service robot moving in our buildings. We believe our verbalization space and algorithm are applicable to different narrative aspects for many mobile robots, including autonomous cars.},
	language = {en},
	author = {Rosenthal, Stephanie and Selvaraj, Sai P and Veloso, Manuela},
	pages = {7},
}

@inproceedings{kumar_vizxp_2021,
	title = {{VizXP}: {A} {Visualization} {Framework} for {Conveying} {Explanations} to {Users} in {Model} {Reconciliation} {Problems}},
	shorttitle = {{VizXP}},
	url = {https://openreview.net/forum?id=PVgmrX1agGP},
	abstract = {We propose a framework for designing visualizations for explainable planning, and run a user study with a prototype created using that framework to compare against text based explanations},
	language = {en},
	urldate = {2022-05-08},
	author = {Kumar, Ashwin and Vasileiou, Stylianos Loukas and Bancilhon, Melanie and Ottley, Alvitta and Yeoh, William},
	month = jun,
	year = {2021},
}

@article{magnaguagno_web_nodate,
	title = {Web {Planner}: {A} {Tool} to {Develop} {Classical} {Planning} {Domains} and {Visualize} {Heuristic} {State}-{Space} {Search}},
	abstract = {Automated planning tools are complex pieces of software that take declarative domain descriptions and generate plans for complex domains. New users often ﬁnd it challenging to understand the plan generation process, while experienced users often ﬁnd it difﬁcult to track semantic errors and efﬁciency issues. To simplify this process, in this paper, we develop a cloud-based planning tool with code editing and state-space visualization capabilities. The code editor focuses on visualizing the domain, problem, and resulting sample plan, helping the user to see how such descriptions are connected without changing context. The visualization tool explores two alternative visualizations aimed at illustrating the operation of the planning process and how the domain dynamics evolve during plan execution.},
	language = {en},
	author = {Magnaguagno, Maurıcio C and Pereira, Ramon Fraga and More, Martin D and Meneguzzi, Felipe},
	pages = {7},
}

@article{kambhampati_classication_nodate,
	title = {A {Classiﬁcation} of {Plan} {Modiﬁcation} {Strategies} {Based} on {Coverage} and {Information} {Requirements}},
	language = {en},
	author = {Kambhampati, Subbarao},
	pages = {5},
}

@article{seegebarth_making_2012,
	title = {Making {Hybrid} {Plans} {More} {Clear} to {Human} {Users} - {A} {Formal} {Approach} for {Generating} {Sound} {Explanations}},
	volume = {22},
	copyright = {Copyright (c) 2021 Proceedings of the International Conference on Automated Planning and Scheduling},
	issn = {2334-0843},
	url = {https://ojs.aaai.org/index.php/ICAPS/article/view/13503},
	abstract = {Human users who execute an automatically generated plan want to understand the rationale behind it. Knowledge-rich plans are particularly suitable for this purpose, because they provide the means to give reason for causal, temporal, and hierarchical relationships between actions. Based on this information, focused arguments can be generated that constitute explanations on an appropriate level of abstraction. In this paper, we present a formal approach to plan explanation. Information about plans is represented as first-order logic formulae and explanations are constructed as proofs in the resulting axiomatic system. With that, plan explanations are provably correct w.r.t. the planning system that produced the plan. A prototype plan explanation system implements our approach and first experiments give evidence that finding plan explanations is feasible in real-time.},
	language = {en},
	urldate = {2022-05-08},
	journal = {Proceedings of the International Conference on Automated Planning and Scheduling},
	author = {Seegebarth, Bastian and Müller, Felix and Schattenberg, Bernd and Biundo, Susanne},
	month = may,
	year = {2012},
	keywords = {Hybrid Planning},
	pages = {225--233},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {0004-3702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language = {en},
	urldate = {2022-05-08},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	keywords = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages = {1--38},
}

@inproceedings{cashmore_towards_2019,
	title = {Towards explainable {AI} planning as a service: 2nd {ICAPS} {Workshop} on {Explainable} {Planning}},
	shorttitle = {Towards explainable {AI} planning as a service},
	abstract = {Explainable AI is an important area of research within which Explainable Planning is an emerging topic. In this paper, we argue that Explainable Planning can be designed as a service -- that is, as a wrapper around an existing planning system that utilises the existing planner to assist in answering contrastive questions. We introduce a prototype framework to facilitate this, along with some examples of how a planner can be used to address certain types of contrastive questions. We discuss the main advantages and limitations of such an approach and we identify open questions for Explainable Planning as a service that identify several possible research directions.},
	author = {Cashmore, Michael and Collins, Anna and Krarup, Benjamin and Krivic, Senka and Magazzeni, Daniele and Smith, David},
	month = jul,
	year = {2019},
	keywords = {explainable AI, planning system, research},
}

@article{eriksson_unsolvability_2017,
	title = {Unsolvability {Certificates} for {Classical} {Planning}},
	volume = {27},
	copyright = {Copyright (c) 2021 Proceedings of the International Conference on Automated Planning and Scheduling},
	issn = {2334-0843},
	url = {https://ojs.aaai.org/index.php/ICAPS/article/view/13818},
	abstract = {The plans that planning systems generate for solvable planning tasks are routinely verified by independent validation tools. For unsolvable planning tasks, no such validation capabilities currently exist. We describe a family of certificates of unsolvability for classical planning tasks that can be efficiently verified and are sufficiently general for a wide range of planning approaches including heuristic search with delete relaxation, critical-path, pattern database and linear merge-and-shrink heuristics, symbolic search with binary decision diagrams, and the Trapper algorithm for detecting dead ends. We also augmented a classical planning system with the ability to emit certificates of unsolvability and implemented a planner-independent certificate validation tool. Experiments show that the overhead for producing such certificates is tolerable and that their validation is practically feasible.},
	language = {en},
	urldate = {2022-05-08},
	journal = {Proceedings of the International Conference on Automated Planning and Scheduling},
	author = {Eriksson, Salomé and Röger, Gabriele and Helmert, Malte},
	month = jun,
	year = {2017},
	pages = {88--97},
}

@book{gobelbecker_coming_2010,
	title = {Coming up {With} {Good} {Excuses}: {What} to do {When} no {Plan} {Can} be {Found}},
	shorttitle = {Coming up {With} {Good} {Excuses}},
	abstract = {When using a planner-based agent architecture, many things can go wrong. First and foremost, an agent might fail to execute one of the planned actions for some reasons. Even more annoying, however, is a situation where the agent is incompetent, i.e., unable to come up with a plan. This might be due to the fact that there are principal reasons that prohibit a successful plan or simply because the task’s description is incomplete or incorrect. In either case, an explanation for such a failure would be very helpful. We will address this problem and provide a formalization of coming up with excuses for not being able to find a plan. Based on that, we will present an algorithm that is able to find excuses and demonstrate that such excuses can be found in practical settings in reasonable time.},
	author = {Göbelbecker, Moritz and Keller, Thomas and Eyerich, Patrick and Brenner, Michael and Nebel, Bernhard},
	month = jan,
	year = {2010},
	note = {Pages: 88},
}

@misc{noauthor_1_nodate,
	title = {(1) ({PDF}) {Coming} up {With} {Good} {Excuses}: {What} to do {When} no {Plan} {Can} be {Found}},
	url = {https://www.researchgate.net/publication/220936319_Coming_up_With_Good_Excuses_What_to_do_When_no_Plan_Can_be_Found},
	urldate = {2022-05-08},
}

@article{sreedharan_why_2019,
	title = {Why {Can}’t {You} {Do} {That} {HAL}? {Explaining} {Unsolvability} of {Planning} {Tasks}},
	shorttitle = {Why {Can}’t {You} {Do} {That} {HAL}?},
	url = {https://www.ijcai.org/proceedings/2019/197},
	abstract = {Electronic proceedings of IJCAI 2019},
	urldate = {2022-05-08},
	author = {Sreedharan, Sarath and Srivastava, Siddharth and Smith, David and Kambhampati, Subbarao},
	year = {2019},
	pages = {1422--1430},
}

@inproceedings{sreedharan_hierarchical_2018,
	address = {Stockholm, Sweden},
	title = {Hierarchical {Expertise} {Level} {Modeling} for {User} {Specific} {Contrastive} {Explanations}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/671},
	doi = {10.24963/ijcai.2018/671},
	abstract = {There is a growing interest within the AI research community in developing autonomous systems capable of explaining their behavior to users. However, the problem of computing explanations for users of different levels of expertise has received little research attention. We propose an approach for addressing this problem by representing the user’s understanding of the task as an abstraction of the domain model that the planner uses. We present algorithms for generating minimal explanations in cases where this abstract human model is not known. We reduce the problem of generating an explanation to a search over the space of abstract models and show that while the complete problem is NP-hard, a greedy algorithm can provide good approximations of the optimal solution. We also empirically show that our approach can efﬁciently compute explanations for a variety of problems.},
	language = {en},
	urldate = {2022-05-08},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Sreedharan, Sarath and Srivastava, Siddharth and Kambhampati, Subbarao},
	month = jul,
	year = {2018},
	pages = {4829--4836},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2022-04-27},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {http://arxiv.org/abs/2201.03545},
	abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
	urldate = {2022-04-27},
	journal = {arXiv:2201.03545 [cs]},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	month = mar,
	year = {2022},
	note = {arXiv: 2201.03545},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{xiao_unified_2018,
	title = {Unified {Perceptual} {Parsing} for {Scene} {Understanding}},
	url = {http://arxiv.org/abs/1807.10221},
	abstract = {Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes. Models are available at {\textbackslash}url\{https://github.com/CSAILVision/unifiedparsing\}.},
	urldate = {2022-04-27},
	journal = {arXiv:1807.10221 [cs]},
	author = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.10221},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jia_fashionpedia_2020,
	title = {Fashionpedia: {Ontology}, {Segmentation}, and an {Attribute} {Localization} {Dataset}},
	shorttitle = {Fashionpedia},
	url = {http://arxiv.org/abs/2004.12276},
	abstract = {In this work we explore the task of instance segmentation with attribute localization, which unifies instance segmentation (detect and segment each object instance) and fine-grained visual attribute categorization (recognize one or multiple attributes). The proposed task requires both localizing an object and describing its properties. To illustrate the various aspects of this task, we focus on the domain of fashion and introduce Fashionpedia as a step toward mapping out the visual aspects of the fashion world. Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with everyday and celebrity event fashion images annotated with segmentation masks and their associated per-mask fine-grained attributes, built upon the Fashionpedia ontology. In order to solve this challenging task, we propose a novel Attribute-Mask RCNN model to jointly perform instance segmentation and localized attribute recognition, and provide a novel evaluation metric for the task. We also demonstrate instance segmentation models pre-trained on Fashionpedia achieve better transfer learning performance on other fashion datasets than ImageNet pre-training. Fashionpedia is available at: https://fashionpedia.github.io/home/index.html.},
	urldate = {2022-04-27},
	journal = {arXiv:2004.12276 [cs, eess]},
	author = {Jia, Menglin and Shi, Mengyun and Sirotenko, Mikhail and Cui, Yin and Cardie, Claire and Hariharan, Bharath and Adam, Hartwig and Belongie, Serge},
	month = jul,
	year = {2020},
	note = {arXiv: 2004.12276
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{noauthor_networkx_nodate-1,
	title = {{NetworkX} — {NetworkX} documentation},
	url = {https://networkx.org/},
	urldate = {2022-04-17},
}

@article{blondel_fast_2008,
	title = {Fast unfolding of communities in large networks},
	volume = {2008},
	issn = {1742-5468},
	url = {http://arxiv.org/abs/0803.0476},
	doi = {10.1088/1742-5468/2008/10/P10008},
	abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .},
	number = {10},
	urldate = {2022-04-17},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Blondel, Vincent D. and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
	month = oct,
	year = {2008},
	note = {arXiv: 0803.0476},
	keywords = {Computer Science - Computers and Society, Computer Science - Data Structures and Algorithms, Condensed Matter - Statistical Mechanics, Physics - Physics and Society},
	pages = {P10008},
}

@article{arenas_analysis_2008,
	title = {Analysis of the structure of complex networks at different resolution levels},
	volume = {10},
	issn = {1367-2630},
	url = {http://arxiv.org/abs/physics/0703218},
	doi = {10.1088/1367-2630/10/5/053039},
	abstract = {Modular structure is ubiquitous in real-world complex networks, and its detection is important because it gives insights in the structure-functionality Modular structure is ubiquitous in real-world complex networks, and its detection is important because it gives insights in the structure-functionality relationship. The standard approach is based on the optimization of a quality function, modularity, which is a relative quality measure for a partition of a network into modules. Recently some authors [1,2] have pointed out that the optimization of modularity has a fundamental drawback: the existence of a resolution limit beyond which no modular structure can be detected even though these modules might have own entity. The reason is that several topological descriptions of the network coexist at different scales, which is, in general, a fingerprint of complex systems. Here we propose a method that allows for multiple resolution screening of the modular structure. The method has been validated using synthetic networks, discovering the predefined structures at all scales. Its application to two real social networks allows to find the exact splits reported in the literature, as well as the substructure beyond the actual split.},
	number = {5},
	urldate = {2022-04-17},
	journal = {New Journal of Physics},
	author = {Arenas, Alex and Fernandez, Alberto and Gomez, Sergio},
	month = may,
	year = {2008},
	note = {arXiv: physics/0703218},
	keywords = {Computer Science - Discrete Mathematics, Condensed Matter - Other Condensed Matter, Physics - Data Analysis, Statistics and Probability, Physics - Physics and Society, Quantitative Biology - Quantitative Methods},
	pages = {053039},
}

@article{pares_fluid_2017,
	title = {Fluid {Communities}: {A} {Competitive}, {Scalable} and {Diverse} {Community} {Detection} {Algorithm}},
	shorttitle = {Fluid {Communities}},
	url = {http://arxiv.org/abs/1703.09307},
	abstract = {We introduce a community detection algorithm (Fluid Communities) based on the idea of fluids interacting in an environment, expanding and contracting as a result of that interaction. Fluid Communities is based on the propagation methodology, which represents the state-of-the-art in terms of computational cost and scalability. While being highly efficient, Fluid Communities is able to find communities in synthetic graphs with an accuracy close to the current best alternatives. Additionally, Fluid Communities is the first propagation-based algorithm capable of identifying a variable number of communities in network. To illustrate the relevance of the algorithm, we evaluate the diversity of the communities found by Fluid Communities, and find them to be significantly different from the ones found by alternative methods.},
	urldate = {2022-04-17},
	journal = {arXiv:1703.09307 [physics]},
	author = {Parés, Ferran and Garcia-Gasulla, Dario and Vilalta, Armand and Moreno, Jonatan and Ayguadé, Eduard and Labarta, Jesús and Cortés, Ulises and Suzumura, Toyotaro},
	month = oct,
	year = {2017},
	note = {arXiv: 1703.09307},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Social and Information Networks, Physics - Physics and Society},
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	urldate = {2022-03-20},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jul,
	year = {2016},
	note = {arXiv: 1603.05027},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{szegedy_inception-v4_2016,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	url = {http://arxiv.org/abs/1602.07261},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
	urldate = {2022-03-20},
	journal = {arXiv:1602.07261 [cs]},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
	month = aug,
	year = {2016},
	note = {arXiv: 1602.07261
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{rahimzadeh_modified_2020,
	title = {A modified deep convolutional neural network for detecting {COVID}-19 and pneumonia from chest {X}-ray images based on the concatenation of {Xception} and {ResNet50V2}},
	volume = {19},
	issn = {2352-9148},
	url = {https://www.sciencedirect.com/science/article/pii/S2352914820302537},
	doi = {10.1016/j.imu.2020.100360},
	abstract = {In this paper, we have trained several deep convolutional networks with introduced training techniques for classifying X-ray images into three classes: normal, pneumonia, and COVID-19, based on two open-source datasets. Our data contains 180 X-ray images that belong to persons infected with COVID-19, and we attempted to apply methods to achieve the best possible results. In this research, we introduce some training techniques that help the network learn better when we have an unbalanced dataset (fewer cases of COVID-19 along with more cases from other classes). We also propose a neural network that is a concatenation of the Xception and ResNet50V2 networks. This network achieved the best accuracy by utilizing multiple features extracted by two robust networks. For evaluating our network, we have tested it on 11302 images to report the actual accuracy achievable in real circumstances. The average accuracy of the proposed network for detecting COVID-19 cases is 99.50\%, and the overall average accuracy for all classes is 91.4\%.},
	language = {en},
	urldate = {2022-03-20},
	journal = {Informatics in Medicine Unlocked},
	author = {Rahimzadeh, Mohammad and Attar, Abolfazl},
	month = jan,
	year = {2020},
	keywords = {COVID-19, Chest X-ray images, Convolutional neural networks, Coronavirus, Deep feature extraction, Deep learning, Transfer learning},
	pages = {100360},
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/?hl=es-419},
	abstract = {Plataforma de extremo a extremo de código abierto enfocada en el aprendizaje automático para todos. Descubre el ecosistema flexible de herramientas, bibliotecas y recursos de la comunidad de TensorFlow.},
	language = {es-419},
	urldate = {2022-03-20},
	journal = {TensorFlow},
}

@misc{noauthor_keras_nodate,
	title = {Keras: the {Python} deep learning {API}},
	url = {https://keras.io/},
	urldate = {2022-03-20},
}

@misc{team_keras_nodate,
	title = {Keras documentation: {Keras} {Applications}},
	shorttitle = {Keras documentation},
	url = {https://keras.io/api/applications/},
	abstract = {Keras documentation},
	language = {en},
	urldate = {2022-03-20},
	author = {Team, Keras},
}

@book{newman_networks_2010,
	address = {Oxford},
	title = {Networks: {An} {Introduction}},
	isbn = {978-0-19-920665-0},
	shorttitle = {Networks},
	url = {https://oxford.universitypressscholarship.com/10.1093/acprof:oso/9780199206650.001.0001/acprof-9780199206650},
	abstract = {The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks. The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks.},
	language = {eng},
	urldate = {2022-03-13},
	publisher = {Oxford University Press},
	author = {Newman, Mark},
	year = {2010},
	doi = {10.1093/acprof:oso/9780199206650.001.0001},
	keywords = {Internet, biological networks, computer algorithms, computer networks, generative models, graph theory, network data, random graph models, social networks, spectral methods},
}

@article{lee_i-mix_2021,
	title = {i-{MIX}: {A} {DOMAIN}-{AGNOSTIC} {STRATEGY} {FOR} {CONTRASTIVE} {REPRESENTATION} {LEARNING}},
	abstract = {Contrastive representation learning has shown to be effective to learn representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning. We cast contrastive learning as training a non-parametric classiﬁer by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of learned representations across domains, including image, speech, and tabular data. Furthermore, we conﬁrm its regularization effect via extensive ablation studies across model and dataset sizes. The code is available at https://github.com/kibok90/imix.},
	language = {en},
	author = {Lee, Kibok and Zhu, Yian and Sohn, Kihyuk and Li, Chun-Liang and Shin, Jinwoo and Lee, Honglak},
	year = {2021},
	pages = {19},
}

@book{ertoz_finding_2003-1,
	title = {Finding {Clusters} of {Different} {Sizes}, {Shapes}, and {Densities} in {Noisy}, {High} {Dimensional} {Data}},
	abstract = {The problem of finding clusters in data is challenging when clusters are of widely differing sizes, densities and shapes, and when the data contains large amounts of noise and outliers. Many of these issues become even more significant when the data is of very high dimensionality, such as text or time series data. In this paper we present a novel clustering technique that addresses these issues. Our algorithm first finds the nearest neighbors of each data point and then redefines the similarity between pairs of points in terms of how many nearest neighbors the two points share. Using this new definition of similarity, we eliminate noise and outliers, identify core points, and then build clusters around the core points. The use of a shared nearest neighbor definition of similarity removes problems with varying density, while the use of core points handles problems with shape and size. We experimentally show that our algorithm performs better than traditional methods (e.g., K-means) on a variety of data sets: KDD Cup '99 network intrusion data, NASA Earth science time series data, and two dimensional point sets. While our algorithm can find the "dense" clusters that other clustering algorithms find, it also finds clusters that these approaches overlook, i.e., clusters of low or medium density which are of interest because they represent relatively uniform regions "surrounded" by non-uniform or higher density areas. The run-time complexity of our technique is O(n2) since the similarity matrix has to be constructed. However, we discuss a number of optimizations that allow the algorithm to handle large datasets efficiently. For example, 100,000 documents from the TREC collection can be clustered within an hour on a desktop computer.},
	author = {Ertöz, Levent and Steinbach, Michael and Kumar, Vipin},
	month = may,
	year = {2003},
	doi = {10.1137/1.9781611972733.5},
	note = {Journal Abbreviation: SIAM ICDM
Publication Title: SIAM ICDM},
}

@article{shi_adversarial_2022,
	title = {Adversarial {Masking} for {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2201.13100},
	abstract = {We propose ADIOS, a masked image model (MIM) framework for self-supervised learning, which simultaneously learns a masking function and an image encoder using an adversarial objective. The image encoder is trained to minimise the distance between representations of the original and that of a masked image. The masking function, conversely, aims at maximising this distance. ADIOS consistently improves on state-of-the-art self-supervised learning (SSL) methods on a variety of tasks and datasets -- including classification on ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao et al., 2021) -- while generating semantically meaningful masks. Unlike modern MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch tokenisation construction of Vision Transformers, and can be implemented with convolutional backbones. We further demonstrate that the masks learned by ADIOS are more effective in improving representation learning of SSL methods than masking schemes used in popular MIM models.},
	urldate = {2022-02-20},
	journal = {arXiv:2201.13100 [cs]},
	author = {Shi, Yuge and Siddharth, N. and Torr, Philip H. S. and Kosiorek, Adam R.},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.13100},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{dwibedi_little_2021,
	title = {With a {Little} {Help} from {My} {Friends}: {Nearest}-{Neighbor} {Contrastive} {Learning} of {Visual} {Representations}},
	shorttitle = {With a {Little} {Help} from {My} {Friends}},
	url = {http://arxiv.org/abs/2104.14548},
	abstract = {Self-supervised learning algorithms based on instance discrimination train encoders to be invariant to pre-defined transformations of the same instance. While most methods treat different views of the same image as positives for a contrastive loss, we are interested in using positives from other instances in the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), samples the nearest neighbors from the dataset in the latent space, and treats them as positives. This provides more semantic variations than pre-defined transformations. We find that using the nearest-neighbor as positive in contrastive losses improves performance significantly on ImageNet classification, from 71.7\% to 75.6\%, outperforming previous state-of-the-art methods. On semi-supervised learning benchmarks we improve performance significantly when only 1\% ImageNet labels are available, from 53.8\% to 56.5\%. On transfer learning benchmarks our method outperforms state-of-the-art methods (including supervised learning with ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate empirically that our method is less reliant on complex data augmentations. We see a relative reduction of only 2.1\% ImageNet Top-1 accuracy when we train using only random crops.},
	urldate = {2022-02-20},
	journal = {arXiv:2104.14548 [cs]},
	author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
	month = oct,
	year = {2021},
	note = {arXiv: 2104.14548},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2022-02-20},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.05709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2022-02-20},
	journal = {arXiv:1911.05722 [cs]},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.05722},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wu_unsupervised_2018,
	title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance}-level {Discrimination}},
	url = {http://arxiv.org/abs/1805.01978},
	abstract = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.},
	urldate = {2022-02-20},
	journal = {arXiv:1805.01978 [cs]},
	author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella and Lin, Dahua},
	month = may,
	year = {2018},
	note = {arXiv: 1805.01978},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2011.10566},
	abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
	urldate = {2022-02-20},
	journal = {arXiv:2011.10566 [cs]},
	author = {Chen, Xinlei and He, Kaiming},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.10566},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{grill_bootstrap_2020,
	title = {Bootstrap your own latent: {A} new approach to self-supervised {Learning}},
	shorttitle = {Bootstrap your own latent},
	url = {http://arxiv.org/abs/2006.07733},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6{\textbackslash}\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
	urldate = {2022-02-20},
	journal = {arXiv:2006.07733 [cs, stat]},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	month = sep,
	year = {2020},
	note = {arXiv: 2006.07733},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	urldate = {2022-02-20},
	journal = {arXiv:2103.03230 [cs, q-bio]},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	month = jun,
	year = {2021},
	note = {arXiv: 2103.03230},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@misc{noauthor_simmim_2022,
	title = {{SimMIM}},
	copyright = {MIT},
	url = {https://github.com/microsoft/SimMIM},
	abstract = {This is an official implementation for "SimMIM: A Simple Framework for Masked Image Modeling".},
	urldate = {2022-02-17},
	publisher = {Microsoft},
	month = feb,
	year = {2022},
	note = {original-date: 2021-11-18T08:26:55Z},
	keywords = {image-classification, masked-image-modeling, self-supervised-learning, swin-transformer},
}

@article{anand_contrastive_2020,
	title = {Contrastive {Self}-{Supervised} {Learning}},
	url = {http://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html},
	abstract = {Contrastive self-supervised learning techniques are a promising class of methods that build representations by learning to encode what makes two things similar or different.},
	language = {en},
	urldate = {2022-02-17},
	journal = {ankeshanand.com},
	author = {Anand, Ankesh},
	month = jan,
	year = {2020},
}
