\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{siunitx}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{colortbl}

% mathcal
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}
\newcommand{\Lagr}{L}
\usepackage{multirow}
\usepackage{url}            % simple URL typesetting

    
\begin{document}

\title{Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms}

\author{

\IEEEauthorblockN{
Joseph Konan\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
Ojas Bhargave\IEEEauthorrefmark{2},
Shikhar Agnihotri\IEEEauthorrefmark{2},
Hojeong Lee\IEEEauthorrefmark{2},
Ankit Shah\IEEEauthorrefmark{2}, \\
Shuo Han\IEEEauthorrefmark{2},
Yunyang Zeng\IEEEauthorrefmark{2},
Amanda Shu\IEEEauthorrefmark{2},
Haohui Liu\IEEEauthorrefmark{2},
Xuankai Chang\IEEEauthorrefmark{2}, \\
Hamza Khalid\IEEEauthorrefmark{2},
Minseon Gwak\IEEEauthorrefmark{2},
Kawon Lee\IEEEauthorrefmark{2},
Minjeong Kim\IEEEauthorrefmark{2},
Bhiksha Raj\IEEEauthorrefmark{2}}

\IEEEauthorblockA{
\IEEEauthorrefmark{1}KonanAI, Pittsburgh, United States, konan@konanai.com.  \\
\IEEEauthorrefmark{2}Carnegie Mellon University, Pittsburgh, \\
\{
jkonan,
obhargav,
sagnihot,
hojeongl,
aps1,
shuohan,
yunyangz,
amshu,
haohuil,
xuankaic,  \\
hkhalid,
mgwak,
kawonl,
minjeong,
bhikshar
\}@andrew.cmu.edu
}
}

\maketitle

\begin{abstract}
In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.
\end{abstract}

\begin{IEEEkeywords}
speech, acoustics, denoising, enhancement, VoIP.
\end{IEEEkeywords}

\section{Introduction}

Voice over Internet Protocol (VoIP) has become a ubiquitous technology for real-time voice communications over the internet. However, speakers often call from environments with background noise that degrades perceptual quality and impairs intelligibility of the communication. To address this problem, denoising and speech enhancement has emerged as promising technique for improving the quality of speech signals in noisy environments. The Deep Noise Suppression (DNS) Challenge \cite{reddy2020interspeech}, hosted by InterSpeech 2020, developed a systematic training and evaluation methodology to improve state-of-the-art denoising and speech enhancement model performance on a diverse set of real-world scenarios with background noise.

Despite the success of models trained on the DNS-2020 Challenge, their performance on VoIP applications remains limited due to a lack of optimization of characteristics specific to VoIP platforms. VoIP audio is typically transmitted over low-bandwidth networks, compressed using lossy codecs, and subject to non-uniform sampling rates, which can introduce additional distortion and artifacts. To enable analysis and optimization in this research area, the VoIP Deep Noise Suppression (VoIP-DNS) dataset was developed as an extension to DNS-2020 that provides training and testing data for VoIP applications: Zoom and Google Meet.
Using a controlled experiment design, the VoIP-DNS dataset was created using consistent systems, hardware, network, and data collection procedures necessary for reproducible research. 

In this paper, we present a method for fine-tuning two state-of-the-art DNS-2020 models, Demucs \cite{defossez2020real} and FullSubNet\cite{hao2021fullsubnet}, on the VoIP-DNS dataset for improved performance on VoIP applications. Our approach involves adapting the models to acoustic variations present in VoIP audio streams, using a temporal acoustic parameter loss (TAPLoss)\cite{taploss} to optimize fine-grain speech characteristics. We evaluate our approach on two VoIP platforms, Zoom and Google Meets, in the context of cloud and cellular phone applications. Our results demonstrate the potential of fine-tuning Demucs and FullSubNet to surpass industry performance and achieve state-of-the-art on VoIP-DNS by improving the quality of voice communication on VoIP platforms, which has important applications the areas of speech recognition, voice assistants, and telecommunication.

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth,height=6cm]{img/transmission.png}
        \caption{End-to-end Data Process Diagram, From Synthesis To Recording. }
        \label{fig:transmission_diagram}
    \end{subfigure}%
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{img/loss_diagram.png}
        \caption{The training workflow for Demucs and FullSubNet}
        \label{fig:train1}
    \end{subfigure}
    \caption{Data Synthesis and the Training workflow}
    \label{fig:my_label}
\end{figure*}

% Online Voice Over IP (VoIP) meeting platforms such as Google Meet, Zoom, and Microsoft Teams allow users to join conferences on their native apps or through a regular phone via Audio Conferencing \cite{ilag2020teams}. However, Audio Conferencing brings about the problem of transmission noise. Speech quality is degraded not only by the background noise present when taking a call but also by the transmission channel, whether it be through a telephone line or via wireless transmission \cite{kaiser2018impact}. This is even more prevalent with mobile telephone transmission, where additional factors, including network congestion and packet loss, further degrade transmission quality \cite{mcdougall2015telephone}. These factors result in information loss during communication \cite{lawrence2008acoustic}.

% Thus, it is important to mitigate information loss by introducing speech enhancement methods to reduce transmission noise. Existing deep learning approaches predominantly focus on removing background noise. Google Meets and Zoom themselves have a “Noise Cancellation” feature that uses deep learning models to address this issue. However, speech enhancement to remove transmission noise in addition to background noise remains largely unexplored. Our aim is to explore speech enhancement models and improve the speech quality of audio coming through these platforms over the internet or phone calls.

% \section{Background}

% Within the current literature, datasets for speech enhancement tasks are often synthesized, a process in which noise is added to clean audio. This is because supervised optimization requires pairs of noisy inputs and clean targets. \cite{reddy2020interspeech} introduced the INTERSPEECH 2020 Deep Noise Suppression Challenge (DNS) with a reproducible synthesis methodology. With clean speech from Librivox corpus and noise from Freesound and AudioSet, enabling upwards of 500 hours of noisy speech to be created.  

% Of the current state-of-the-art speech enhancement models on the 2020 DNS Challenge, this study focuses on Demucs and FullSubNet. Demucs operates on waveform inputs in the time domain, whose real-time denoising architecture was proposed by Defossez et al. \cite{defossez2019music}. FullSubNet operates on spectrogram inputs in the time-frequency domain and was developed by Hao et al. \cite{hao2021fullsubnet}. Baseline Demucs achieves 1.73 PESQ and 0.86 STOI, while Baseline FullSubNet achieves 1.69 PESQ and 0.84 STOI. As both of these baseline models do not account for transmission noise, our investigation is focused on fine-tuning and improving the acoustic fidelity, perceptual quality, and intelligibility for the VoIP DNS Challenge challenge.

\section{Experimental Setup}

The VoIP-DNS dataset\cite{voip} contains 1,200 thirty-second training pairs and 150 ten-second testing pairs of source clean speech and source noisy speech. The source pairs for the VoIP-DNS dataset are obtained using the same training synthesis procedure and test set provided by DNS-2020. The training synthesis procedure involves taking 30-second clean speech samples from LibriSpeech and 30-second noise signals from FreeSound\cite{fonseca2017freesound} or AudioSet\cite{gemmeke2017audio_audioset}, then mixing the two to create a 30-second noisy speech sample. After the source noisy speech is passed to the VoIP platform via a virtual microphone, it is transmitted to a receiving device. Finally, the relayed audio is obtained via an audio interface. If the receiver is a cellular phone, then the audio interface is managed by the VoIP-DNS technician; otherwise, if the receiver is on the cloud, then the audio interface is managed by the VoIP platform.

This paper focuses on optimizing VoIP-DNS to improve speech quality on cloud and cellular phones using VoIP data from Zoom and Google Meet. On each platform, we consider their use cases with denoising set on or off. Due to differences in terminology across platforms, disambiguation is required. For Zoom, "denoising off" is considered "Zoom with low denoising selected," while "denoising on" is considered "Zoom with auto denoising selected." For Google Meet, "denoising off" is considered "Google Meet with denoising off selected," while "denoising on" is considered "Google Meet with denoising on selected." Therefore, industrial denoising is defined by the platform's performance with denoising on.

The VoIP-DNS data collection procedure captures the unique characteristics of VoIP audio streams, which are subject to low bandwidth, compression, non-uniform sampling rates, and additional platform-specific processing. The VoIP-DNS dataset was designed by the authors of this work and is publicly available with a meticulously documented experimental design to ensure reproducibility.


\section{Models and Training}

% While the industrial denoising models of Zoom and Google Meet are not publically available for inspection, they are able to be queried through their platform as demonstrated in VoIP-DNS. In addition to industrial denoising, we consider two base models known for their state-of-the-art performance on DNS-2020, Demucs and FullSubNet. Demucs is a time-domain (waveform input) deep neural network architecture for audio source separation, adapted for speech enhancement and denoising. FullSubNet is a time-frequency domain (spectrogram input) fully convolutional network also designed for speech enhancement and denoising.

% Each training iteration begins by sampling a noisy speech relay and its corresponding clean speech source. The noisy speech relay is passed to the base model to generate enhanced speech. To optimize acoustics of fine-grain speech characteristics, an temporal acoustic paramter (TAP) estimator is used to derive enhanced acoustics from enhanced speech. Similarly, the clean acoustics source is derived from the clean speech source via the TAP estimator. The base loss is defined by the base model and minimizes the divergence between waveforms or spectrograms. The acoustic loss, proposed by Yunyang et al. 2023, minimizes the L1 divergence between the enhanced acoustics and the clean acoustics source.



% Our dataset is built off of audio clips from the open-sourced dataset\footnote{\url{https://github.com/microsoft/DNS-Challenge/tree/interspeech2020/master/datasets}} released from Microsoft's Deep Noise Suppression Challenge (DNS). Specifically, we utilize the test set of synthetic clips without reverb from the DNS data, as described by Reddy et al. \cite{reddy2020interspeech}. Our novel dataset contains these audio clips with the addition of transmission noise resulting from sourcing a call from a platform of interest (Zoom/Google Meets) and recieving it via VOIP or a cellular device. It is created as described in the following steps.

% First, clean speech and noise audio are randomly selected from the DNS dataset. These audio files are then mixed, resulting in the noisy audio of clean speech with background noise. Next, to include the transmission noise, we start a meeting session and connect into the session via the native application or a phone that is on the T-mobile network. We then play the noisy speech audio in the application, and the audio that is relayed to the phone is recorded to an audio interface.


In this study, we evaluate the denoising performance of two state-of-the-art deep learning models, Demucs and FullSubNet, on VoIP audio streams. These models were chosen due to their top performance in the Deep Noise Suppression (DNS) 2020 Challenge and their availability in terms of public source code, learning procedures, and final checkpoints. Furthermore, Demucs operates in the time-domain, while FullSubNet operates in the time-frequency domain, allowing us to contrast modalities with a consistent reproducible experimental setup.

To train the models, we use the noisy speech relays and the clean speech sources from VoIP-DNS. Each training iteration begins by sampling a noisy speech signal and its corresponding clean speech source. The noisy speech signal is passed through the base model to generate enhanced speech signals. To optimize the fine-grained acoustic characteristics of the enhanced speech signals, a temporal acoustic parameter (TAP) estimator is used to derive enhanced acoustic features from the enhanced speech signals. Similarly, clean acoustic features are derived from the clean speech source using the TAP estimator.

The base loss function is defined by the base model and minimizes the divergence between the enhanced and clean speech signals in either the time-domain (for Demucs) or time-frequency domain (for FullSubNet). To further improve the performance of the models on VoIP audio streams, we propose a novel acoustic loss function, which minimizes the L1 divergence between the enhanced and clean acoustic features. Our experimental results show that incorporating the acoustic loss function leads to significant improvements in the denoising performance of the models on VoIP audio streams.

In addition to evaluating the performance of Demucs and FullSubNet, we compare their performance with industrial denoising models used by popular VoIP platforms such as Zoom and Google Meet. While these industrial denoising models are not publicly available for inspection, they can be queried through their platform using the VoIP-DNS dataset. This enables a direct comparison between the performance of the industrial denoising models and the state-of-the-art deep learning models on VoIP audio streams.

\section{Ablation Results}

% The performance of the denoising models varied depending on the VoIP platform, whether industrial denoising was enabled, and the type of device used to receive the audio. We observed that different acoustic distortions and artifacts could be heard on different platforms, and the performance of the denoising models varied accordingly.

% To evaluate the effectiveness of our proposed approach, we conducted ablation experiments by training the models with and without the proposed temporal acoustic parameter (TAP) loss function, using different learning rates. Our experimental results demonstrate that incorporating the TAP loss function leads to improved denoising performance over the baseline models, and superior results compared to the industrial denoising models on VoIP audio streams. These findings highlight the potential of incorporating acoustic features into the training of denoising models for improved performance on VoIP platforms.


\begin{table*}[!htp]\centering
\caption{Objective Relative Evaluation of Perceptual Quality \& Intelligibility}\label{tab:metrics_table_1}
\scriptsize
\sisetup{round-mode=places, round-precision=2}
\begin{tabular}{lrrrrrrrrrrrrrrrr}

\cellcolor[HTML]{d9d9d9} &\cellcolor[HTML]{d9d9d9} &\cellcolor[HTML]{d9d9d9}\textbar &\multicolumn{6}{c}{\cellcolor[HTML]{d9d9d9}PESQ} &\cellcolor[HTML]{d9d9d9}\textbar &\multicolumn{6}{c}{\cellcolor[HTML]{d9d9d9}STOI} \\ \hline

\cellcolor[HTML]{f3f3f3} &\cellcolor[HTML]{f3f3f3} &\cellcolor[HTML]{f3f3f3}\textbar &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Source} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Demucs} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}FullSubNet} &\cellcolor[HTML]{f3f3f3}\textbar &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Source} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Demucs} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}FullSubNet} \\
\hline

\cellcolor[HTML]{f3f3f3}Platform &\cellcolor[HTML]{f3f3f3}Receiver &\cellcolor[HTML]{f3f3f3}\textbar &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}\textbar &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto \\ 

Google Meets &Phone &\textbar & 1.549 & 1.976 & 1.381 & 1.726 & 1.398 & 1.694 & \textbar & 0.748 & 0.884 & 0.748 & 0.884 & 0.700 & 0.841 \\
Google Meets &Cloud &\textbar & 1.640 & 2.255 & 2.272 & 2.264 & 2.435 & 2.281 & \textbar & 0.890 & 0.923 & 0.933 & 0.924 & 0.920 & 0.923 \\
Zoom &Phone &\textbar & 1.548 & 1.701 & 1.548 & 1.513 & 1.382 & 1.343 & \textbar & 0.797 & 0.810 & 0.766 & 0.810 & 0.733 & 0.724 \\
Zoom &Cloud &\textbar & 1.450 & 1.919 & 2.089 & 1.996  & 2.141 & 1.992 & \textbar & 0.859 & 0.909 & 0.906 & 0.913 & 0.900 & 0.911 \\

\hline
\end{tabular}
\end{table*}

\begin{figure}[!htb]
    \centering
        \includegraphics[width=\linewidth]{img/ablation_res.png}
        \caption{Ablation of Demucs  with different learning rate} 
        \label{fig:demucs_lr_ablation}
\end{figure}

%%% NEEDS TO BE REVISED

We conducted a series of ablation experiments to evaluate the impact of our proposed temporal acoustic parameter (TAP) loss function on the denoising performance of our models. We trained with and without the TAP loss function, using different learning rates, and evaluated their performance on a large-scale dataset of 1,200 30-second noisy speech samples generated using the DNS-2020 synthesis procedure.

Our experimental results demonstrate that incorporating the TAP loss function leads to improved denoising performance over the baseline models. Specifically, we observed a notable improvement in the chosen metric for both Demucs and FullSubNet when incorporating the TAP loss function with an optimal learning rate. However, we also observed that the optimal learning rate for the TAP loss function varied depending on the specific characteristics of the VoIP platform and the transmission process.

In some cases, we found that a higher learning rate led to improved denoising performance on cellular devices when industrial denoising was disabled. This suggests that cellular devices may benefit from a more aggressive TAP loss function during training to achieve better denoising results. In contrast, for cloud relay transmissions and transmissions with industrial denoising enabled, a lower learning rate resulted in improved denoising performance.


Furthermore, we observed that the effectiveness of our proposed denoising models varied depending on the specific characteristics of the VoIP platform and the transmission process. Depending on the platform, whether or not industrial denoising was enabled, and which device received the audio, different acoustic distortions and artifacts could be heard.

In some scenarios, we observed that enhancing the audio without industrial denoising applied led to better denoising results. We believe that this is because less information is lost due to the platform's processing, which facilitates the restoration of the original audio. However, in other cases, we found that enabling industrial denoising led to better denoising results. This could be because some distortions and artifacts are magnified through transmission and are more present in audio without industrial denoising. In these cases, enabling industrial denoising may remove some information from the audio, but the benefits of removing much of the distortions and artifacts prior to transmission outweigh the cons.

Taken together, our results demonstrate that the effectiveness of denoising models on VoIP platforms is highly dependent on the specific characteristics of the platform, the transmission process, and the optimal learning rate used during training. Nonetheless, our proposed denoising models consistently outperformed the industrial denoising models when industrial denoising was disabled, suggesting that our proposed models have the potential for improving denoising performance on VoIP platforms.









% \begin{figure}[!htb]
%   \centering
%   \includegraphics[width=\linewidth,height=6cm]{img/transmission.png}
%   \caption{End-to-end Data Process Diagram, From Synthesis To Recording. }
%   \label{fig:transmission_diagram}
% \end{figure}


% The final recorded audio captures both background noise and transmission noise. Furthermore, we note that we have recorded the audio in the auto and low mode. This helps us compare our models with industry standards. We refer to the audio with the inbuilt speech enhancement feature of the application turned off as \textit{low}, and with it turned on as \textit{auto}. In our work, for each of the auto and low data sources, we utilize 1200 audio clips (each thirty seconds long) for our training samples and 150 ten-second audio clips for our testing samples. We further apply gain normalization to transform the audio files to the same amplitude range to facilitate training. This transforms the audio files to the same amplitude range to enable better model training. Figure~\ref{fig:transmission_diagram} depicts the dataset synthesis process.

% \section{Training}

% During the training phase, the recorded noisy speech samples are fed into the base model to generate enhanced speech signals. To improve the performance of the base model on VoIP audio streams, the characteristics of the clean and enhanced speech signals are extracted using a temporal acoustic parameter (TAP) estimator. The TAP estimator calculates a set of temporal acoustic features, such as pitch, voicing, and spectral shape, that capture the underlying acoustic properties of the speech signals.

% To optimize the base model for noise suppression on VoIP audio streams, a joint loss function is defined that minimizes both the divergence between the enhanced speech and the clean speech signals, and the divergence between the enhanced and clean TAPs. The base loss function is defined to minimize the L1 distance between the enhanced and clean speech signals, while the TAP loss function is defined to minimize the divergence between the enhanced and clean TAPs. The joint loss function encourages the model to learn a mapping from noisy to clean speech that preserves the underlying acoustic properties of the speech signals.

% To evaluate the effectiveness of the TAP loss function in improving the performance of the base model, ablation experiments are conducted using different learning rates. The results show that incorporating the TAP loss function in addition to the base loss function leads to significant improvements in the denoising performance of the model on VoIP audio streams.

% Overall, this approach demonstrates the potential of incorporating acoustic features into the training of DNS models for improved noise suppression in VoIP applications.

% \section{Models}



\begin{figure*}[!htb]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{img/DEMUCS_IMPROV.png}
        \caption{Demucs Standardized Acoustic Improvement Comparison}
        \label{fig:demucs_acoustics}
    \end{subfigure}%
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{img/FSN_IMPROV.png}
        \caption{FullSubNet Standardized Acoustic Improvement Comparison}
        \label{fig:fsn_acoustics}
    \end{subfigure}
    \caption{ Acoustic Improvement Of Mean Absolute Error: The Use Case Of Transmitting From Google Meet To Cellular Phone }
    \label{fig:acoustics_improv}
\end{figure*}



\section{Perceptual Evaluation}

In this section, we present a perceptual analysis of our fine-tuning results, investigating intelligibility and perceptual quality of the models on different VoIP applications. To assess intelligibility, we use the Short-Time Objective Intelligibility (STOI)\cite{stoi} measure, which quantifies the extent to which speech can be understood by human listeners. For evaluating perceptual quality, we employ the Perceptual Evaluation of Speech Quality (PESQ)\cite{pesq} metric, which approximates human subjective judgments of speech quality. We provide a subsection for the overall comparison, Demucs vs FullSubNet, and differential analysis of low vs auto specifications.


\subsection{Overall Comparison}

Our fine-tuning approach was able to match or exceed industry intelligibility levels. While cloud applications exhibited little difference, cellular phone applications showed a 0.7\% STOI improvement. Additionally, we observed perceptual quality improvements of 1.2\% on Google Meet and 4.0\% on Zoom over industry performance for cloud applications. However, mobile applications demonstrated worse perceptual performance than industry standards, which is likely attributed to the challenge of transfer learning from DNS-2020 to the band limitations of cellular transmission.

\subsection{Demucs vs FullSubNet Comparison}

Upon comparing Demucs and FullSubNet, we found that Demucs had superior intelligibility, with a PESQ improvement ranging from 0.2-1.1\% on cellular applications to 5.1-10.5\% on cloud applications. The perceptual improvement depended on the receiving medium, with Demucs outperforming FullSubNet by 1.8-12.0\% on mobile applications and FullSubNet surpassing Demucs by 2.5-7.2\% on cloud applications. Although a detailed error analysis is beyond the scope of this paper, we observed that the band limitations of modeling the time-frequency domain caused a noticeable distribution shift, making transfer learning on cellular applications slower to converge. Gathering more data, especially for cellular applications, could help overcome this challenge.

\subsection{Denoising Off (Low) vs Denoising On (Auto) Settings}

To better understand the relative performance specific to relays without denoising prior to transmission (low) versus relays with denoising prior to transmission (auto), we conducted a differential analysis.

For the case without denoising prior to transmission, we observed superior perceptual quality with modest PESQ improvement across applications, except for Google Meets to a cellular phone. In contrast to Zoom, Google Meets experienced worse perceptual quality due to noisy audio transmission, with a PESQ difference of about 0.14, which could explain the difficulty in learning to enhance the speech signal.

Comparing within Demucs (auto vs low), relays with denoising prior to transmission yielded comparable or improved intelligibility over industry standards, with STOI improvement observed when transmission was received from Google Meets to a cellular phone (18.2\%), Zoom to a cellular phone (5.7\%), and to some extent from Zoom to the cloud (0.7\%). However, transmission from Google Meets to  cloud witnessed a modest STOI degradation of 1.0\%, making low performance superior to auto. We conjecture that specifically for the Google Meets to Cloud use case, the exacerbation of distortion or artifact issues in the low use case was not as significant as the amount of information lost due to their denoising prior to transmission. These nuances and complexities highlight the importance of platform-specific assessment, as the areas in need of improvement and their relative difficulty to improve can vary.

In conclusion, our perceptual analysis demonstrates the potential benefits and challenges of applying our fine-tuning approach to different VoIP platforms and scenarios. The results emphasize the importance of platform-specific assessment to optimize factors impacting speech enhancement performance.




% \begin{table*}[!htp]\centering
% \caption{DUMMY TABLE}\label{tab:metrics_table_2 }
% \scriptsize
% \begin{tabular}{lrrrrrrrrrrrrrrrr}

% \cellcolor[HTML]{d9d9d9} &\cellcolor[HTML]{d9d9d9} &\cellcolor[HTML]{d9d9d9}\textbar &\multicolumn{6}{c}{\cellcolor[HTML]{d9d9d9}PESQ} &\cellcolor[HTML]{d9d9d9}\textbar &\multicolumn{6}{c}{\cellcolor[HTML]{d9d9d9}STOI} \\ \hline

% \cellcolor[HTML]{f3f3f3} &\cellcolor[HTML]{f3f3f3} &\cellcolor[HTML]{f3f3f3}\textbar &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Source} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Demucs} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}FullSubNet} &\cellcolor[HTML]{f3f3f3}\textbar &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Source} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Demucs} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}FullSubNet} \\
% \hline

% \cellcolor[HTML]{f3f3f3}Platform &\cellcolor[HTML]{f3f3f3}Receiver &\cellcolor[HTML]{f3f3f3}\textbar &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}\textbar &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto \\ 

% Google Meets &Phone &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% Google Meets &Cloud &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% Zoom &Phone &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% Zoom &Cloud &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% \hline
% \end{tabular}
% \end{table*}

\section{Acoustic Evaluation}

In this section, we present an acoustic analysis of the denoising models, focusing on the improvement of fine-grain speech characteristics over noisy speech, industrial denoising models of Zoom and Google Meet, and the baseline models. We use the extended Geniva Minimalist Acoustic Parameter Set (eGeMAPS) \cite{eyben2015geneva} for this evaluation.

For each test sample, we extracted eGeMAPS features from the original clean speech, the noisy speech, and the denoised speech generated by our proposed models, the industrial models, and the baseline models. We then calculated the Mean Absolute Error (MAE) between the eGeMAPS features of the original clean speech and the degraded and enhanced versions of the speech. Due to space constraints, we focus on Google Meet to Phone acoustic improvement for our approach tuning Demucs and FullSubNet compared to the industry and their relative improvement over their open-source baselines.

In almost all scenarios, the acoustics of fine-grain speech characteristics improved through denoising when applied only before transmission, only after transmission, and both before and after transmission. Our approach led to mixed results in achieving industry performance, but when compared to the open-source alternative, tailoring Demucs and FullSubNet on DNS-2020 to the VoIP-DNS set yielded as high as 15\% improvement for FullSubNet and 40\% for Demucs, with an average improvement in acoustic fidelity of about 5\% for FullSubNet and 21\% for Demucs.

Compared to industry performance, we achieved higher acoustic fidelity in amplitudeLogRelF0, with Demucs showing approximately 40\% improvement over the industry and 51\% improvement over the baseline. FullSubNet demonstrated a modest 4\% improvement over the baseline with 40\% improvement over industry across formants F1, F2, and F3. However, in formant-specific bandwidth improvement, the difference between Demucs or FullSubNet and industry was marginal.

Other acoustics that showed significant improvement over industry performance were Loudness (48\% for Demucs and 47\% for FullSubNet) and Harmonic to Noise Ratio Autocorrelation Function (HNRdbACF) (39\% for Demucs and 37\% for FullSubNet), suggesting proper normalization and ability discriminate between harmonic and non-harmonic signals in our approach. While further analysis is required to understand the relationship between these acoustic changes and perceptual quality, reducing harmonic distortion and improving loudness accuracy may contribute to better subjective evaluation.

In some areas, such as MFCCs, the Hammarberg Index, and the Alpha Index, Demucs and FullSubNet had difficulty improving acoustics. However, our approach experienced less degradation in these areas compared to the baseline. We believe further improvement is possible, and we are currently conducting ablations to refine our models. Updated findings reflecting these insights will be reported in the near future.

\section{Conclusion}

In this study, we have investigated the denoising performance of two state-of-the-art deep learning models, Demucs and FullSubNet, on VoIP audio streams, with a particular focus on their application in popular platforms such as Zoom and Google Meet. By fine-tuning these models on the VoIP-DNS dataset and incorporating a novel acoustic loss function, we achieved significant improvements in denoising performance compared to their open-source baselines.

Our perceptual analysis demonstrated that our fine-tuned models were able to match or exceed the intelligibility of industry-denoising models in various scenarios, with Demucs generally outperforming FullSubNet in terms of PESQ and STOI. The differential analysis of low vs auto specifications revealed that our models performed differently depending on the relay condition and the platform used, emphasizing the importance of platform-specific assessment and optimization. We also observed differences in performance between phone and cloud applications, with cloud applications generally exhibiting better perceptual quality improvements.

The acoustic analysis using eGeMAPS features further supported the effectiveness of our approach in improving fine-grain speech characteristics. Our models demonstrated substantial improvements in certain acoustic features, such as amplitudeLogRelF0, Loudness, and HNRdBACF, compared to the industry and open-source baselines.

However, there are still areas for further improvement, such as in MFCCs, the Hammarberg Index, and the Alpha Index. We believe that additional fine-tuning and ablation studies will help address these issues and lead to even better denoising performance in the future.

This study contributes to the growing body of research on speech enhancement for VoIP applications and highlights the potential of deep learning-based denoising models to improve the quality of real-time communication. As the demand for effective remote communication continues to grow, the advancements presented in this work serve as a foundation for further development and optimization of speech enhancement techniques tailored to the specific requirements of various VoIP platforms, transmission scenarios, and end-user devices, including both phone and cloud applications.

% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\linewidth]{img/final.png}
%   \caption{(a) Demucs architecture with the mixture waveform as input and the four sources estimates
% as output. Arrows represent U-Net connections. (b) Detailed view of the layers $Decoder_i$ on the top
% and $Encoder_i$ on the bottom. Arrows represent connections to other parts of the model. (c) FullSubNet architecture. The second line in the rectangle describes the dimensions of the data at the current stage, e.g., “1 (F)” represents one F-dimensional vector. “F (2N + 1)” represents F independent (2N + 1)-dimensional vectors.}
%   \label{fig:arch}
% \end{figure*}

%\subsection{Baseline Selection}

% We select Demucs and FullSubNet  (see Section \ref{appendix-demucs} and \ref{appendix-fsn}), as baseline models in our work, as both models have yielded state-of-the-art results in the DNS Challenge \cite{hao2021fullsubnet, defossez2020real}. Demucs consists of two LSTM layers between an encoder-decoder structure. FullSubNet is a fusion model that combines a full-band model that captures the global spectral context and a sub-band model that encapsulates the local spectral pattern for single-channel real-time speech enhancement.


% \subsection{Demucs}
% \label{appendix-demucs}
% The Demucs architecture is heavily inspired by the architectures of SING \cite{defossez2018sing}, and Wave-U-Net \cite{stoller2018wave}.
% It is composed of a convolutional encoder, an LSTM, and a convolutional decoder. The encoder and decoder are linked with skip U-Net connections. The input to the model is a stereo mixture $s = \sum_i s_i$ and the output is stereo estimate $\hat{s_i}$ for each source.

% \label{demuc_loss}
% Demucs' criterion minimizes the sum of the L1-norm, $\Lc_1$, between waveforms and the multi-resolution STFT loss, $\Lc_{STFT}$ of the magnitude spectrograms.

% \begin{align*}
%     \Lc_{demucs} 
%     & = \frac{1}{T} [||y-\hat{y}||_1 + \sum_{i=1}^{M}\Lc_{STFT}^{(i)}(y,\hat{y})] \\
%     & \equiv \Lc_{L1} + \Lc_{STFT}
% \end{align*}
% Without $\Lc_{STFT}$, we observe tonal artifacts. We discuss ablating $\Lc_{STFT}$ and more recent auxiliary losses in Section 5.2.

% \subsection{FullSubNet}
% \label{appendix-fsn}
% FullSubNet is a full-band and sub-band fusion model, each with a similar topology. This includes two stacked unidirectional LSTM layers and one linear (fully connected) layer. The only difference between the two is that, unlike the full-band model, the output layer of the sub-band model does not use any activation functions. 

% FullSubNet adopts the complex Ideal Ratio Mask ($cIRM$) as their model’s learning target. A hyperbolic tangent is used to compress $cIRM$ in training and an inverse function decompresses the mask in inference $(K = 10, C = 0.1)$. 

% \begin{table}[!htp]\centering
% \caption{Generated by Spread-LaTeX}\label{tab: }
% \scriptsize
% \begin{tabular}{p{.75cm}p{.75cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}} \hline
% \cellcolor[HTML]{f3f3f3}\textbf{Platform} &\cellcolor[HTML]{f3f3f3}\textbf{Receiver} &\cellcolor[HTML]{f3f3f3}\textbf{Mode} &\cellcolor[HTML]{f3f3f3}\textbf{LR} &\cellcolor[HTML]{f3f3f3}\textbf{TAPL} &\cellcolor[HTML]{f3f3f3}\textbf{Base PESQ} &\cellcolor[HTML]{f3f3f3}\textbf{Our PESQ} &\cellcolor[HTML]{f3f3f3}\textbf{Base PESQ} &\cellcolor[HTML]{f3f3f3}\textbf{Our PESQ} \\ \hline
% G Meet &Phone &... &... &... &... &... &... &... \\
% G Meet &Cloud &... &... &... &... &... &... &... \\
% Zoom &Phone &... &... &... &... &... &... &... \\
% Zoom &Cloud &... &... &... &... &... &... &... \\
% \hline
% \end{tabular}
% \end{table}

% \section{Methods}


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\linewidth]{img/loss_diagram.png}
%   \caption{The training workflow for Demucs and FullSubNet}
%   \label{fig:train1}
% \end{figure}




% \subsection{Baseline Method}
%  We first determine baseline performance using pre-trained Demucs and FullSubNet. Our work improves on this, ablating over a variety of criteria. Table 1 and Table 2 of Section \ref{eval-metrics} show ablations.

% \subsection{TAPLoss}
% We introduce TAPLoss during training to outperform the state-of-the-art speech recognition models on our data. TAPLoss involves a set of 25 temporal acoustic parameters, including frequency-related parameters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to-noise (HNR) ratio; spectral balance parameters: alpha ratio, Hammarberg Index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced regions, and continuous voiced regions per second.

% \subsubsection{Enhanced Demucs loss}
% Previous work has shown that Demucs model is prone to generating tonal artifacts. The multi-resolution STFT loss amplifies this issue because the error introduced by tonal artifacts is more significant and obvious in the time-frequency domain than in the time domain. Thus, we reduce the influence of $\Lc_{STFT}$ in the demucs loss and, additionally, introduce the tap loss $\Lc_{\Tc\Ac\Pc}$. The new Demucs loss function is defined as: 
% $$\Lc_{Demucs} = \Lc_{1} + \lambda_1 \cdot \Lc_{\Tc\Ac\Pc} + \lambda_2 \cdot \Lc_{STFT}$$

% \subsubsection{Enhanced FullSubNet loss}

% We also extend the FullSubNet loss by introducing the $\Lc_{\Tc\Ac\Pc}$ loss. The new FullSubNet loss is defined as:
 
% $$\Lc_{FullSubNet} = \Lc_{cIRM} + \gamma \cdot \Lc_{\Tc\Ac\Pc}$$

% \subsection{Ablations}
% We perform ablations to find the optimal values for each of the hyperparameters $\lambda_1$ and $\lambda_2$ for Demucs, and $\gamma$ for FullSubNet. These optimal values were determined by their performance on the Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) metrics, which are defined in Table ~\ref{tab:metrics_table_1}. Table ~\ref{tab:metrics_table_1} and Table ~\ref{tab:metrics_table_2} show the summary of our ablations for Demucs and FullSubNet, respectively.


% \subsection{Evaluation Metrics}
% \label{eval-metrics}
% After determining the optimal hyperparameter values for both models, we further evaluate the respective models with both objective metrics and acoustic parameters.






% \subsubsection{Objective Evaluation}
%  In addition to PESQ and STOI, we further test the models on three more objective metrics: Log-Likelihood Ratio (LLR), Coherence and Speech Intelligibility Index (CSII), and Normalized-Covariance Measure (NCM). PESQ and LLR measure Speech Quality (SQ), while the STOI, CSII, and NCM measure Speech Intelligibility (SI). All four metrics aim to capture the human-judged quality of speech recordings, which is regarded as the gold standard for evaluating Speech Enhancement models \cite{reddy2021dnsmos}. These metrics are defined in Appendix ~\ref{appendix-metrics}.





 
% \begin{table}
%   \caption{Evaluation Metrics}
%   \label{table:metrics-table}
%   \centering
%   \begin{tabular}{p{0.7cm} p{6.8cm}}
%     \toprule[1.5pt] \toprule[1.5pt]
%     % \multicolumn{2}{c}{Part}                   \\
%     % \cmidrule(r){1-2}
%     Metric & Description  \\
%     \midrule[1.2pt]

%     PESQ & Captures the human subjective rating of speech quality degradation caused by network conditions including analogue connections and packet loss \cite{rix2001perceptual}\\
%     \midrule[1pt]
%     LLR & Assesses speech quality by determining the goodness of fit between clean and enhanced speech \cite{crochiere1980interpretation\}
%     \\
    
%     \midrule[1pt]
    
%     STOI & Measures the intelligibility of degraded speech signals caused by factors such as additive noise \cite{taal2010short} \\ 
%     \midrule[1pt]
%     CSII & Determines speech intelligibility under conditions of bandwidth reduction and additive noise by measuring the signal-to-noise ratio in separate frequency bands. It is calculated in three regions: high, mid and low where high measures segments at or above the root-mean-square (rms) decibel level of the speech, mid measures segments in the range (rms-10, rms], and low in the range (rms-30, rms-10] \cite{kates2005coherence}\\
%     \midrule[1pt]
%     NCM & Estimates speech intelligibility based on the covariance between the envelopes of the clean and noisy speech signals \cite{santos2013}\\
    
%     \bottomrule[1.5pt]
%   \end{tabular}
% \end{table}


% \subsubsection{Acoustic Evaluation}
% In addition to objective metrics, we also utilize the set of acoustic parameters, specifically the eGeMAPSv02 functional descriptors, presented by \cite{eyben2015geneva} to evaluate the model further. This is a set of 88 frequency, energy/amplitude, and spectral-related parameters. We use the OpenSMILE (Open-source Speech and Music Interpretation by Large-space Extraction)\footnote{\url{https://github.com/audeering/opensmile-python}} python package for these parameters. 

% \begin{figure}
%     \centering
%     % \captionsetup{/justification=centering, margin=2cm}
%     \includegraphics[height=6cm]{img/DEMUCS_IMPROV.png}
%     \caption{Acoustic improvements of the finetuned Demucs. The left side depicts the Demucs improvement over noisy in red versus the finetuned improvement over noisy in purple. The right side shows the relative improvement of our finetuned Demucs model over the baseline model}
%     \label{fig:demucs_improv}
% \end{figure}

% \begin{figure}
%     \centering
%     % \captionsetup{/justification=centering, margin=2cm}
%     \includegraphics[height=6cm]{img/FSN_IMPROV.png}
%     \caption{Acoustic improvements of the finetuned FullSubNet Model. The left side shows FullSubNet improvement over noisy in red versus the finetuned improvement over noisy in purple. The right side shows the relative improvement of our finetuned FullSubNet model over the baseline model}
%     \label{fig:fsn_improv}
% \end{figure}

% \begin{table*}[!htp]\centering
% \caption{DUMMY TABLE}\label{tab:metrics_table_2 }
% \scriptsize
% \begin{tabular}{lrrrrrrrrrrrrrrrr}

% \cellcolor[HTML]{d9d9d9} &\cellcolor[HTML]{d9d9d9} &\cellcolor[HTML]{d9d9d9}\textbar &\multicolumn{6}{c}{\cellcolor[HTML]{d9d9d9}PESQ} &\cellcolor[HTML]{d9d9d9}\textbar &\multicolumn{6}{c}{\cellcolor[HTML]{d9d9d9}STOI} \\ \hline

% \cellcolor[HTML]{f3f3f3} &\cellcolor[HTML]{f3f3f3} &\cellcolor[HTML]{f3f3f3}\textbar &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Source} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Demucs} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}FullSubNet} &\cellcolor[HTML]{f3f3f3}\textbar &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Source} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}Demucs} &\multicolumn{2}{c}{\cellcolor[HTML]{f3f3f3}FullSubNet} \\
% \hline

% \cellcolor[HTML]{f3f3f3}Platform &\cellcolor[HTML]{f3f3f3}Receiver &\cellcolor[HTML]{f3f3f3}\textbar &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}\textbar &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto &\cellcolor[HTML]{f3f3f3}Low &\cellcolor[HTML]{f3f3f3}Auto \\ 

% Google Meets &Phone &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% Google Meets &Cloud &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% Zoom &Phone &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% Zoom &Cloud &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX &\textbar &X.XX &X.XX &X.XX &X.XX &X.XX &X.XX \\
% \hline
% \end{tabular}
% \end{table*}

% \section{Results}

% \subsection{Acoustic Improvement}
% To quantify the improvement of the acoustic parameters mentioned in section \ref{eval-metrics}, we measure acoustic improvement as defined by \cite{taploss}. First, we calculate the mean absolute error (MAE) across the time axis. The MAE between our novel dataset's noisy and clean audio files is denoted as $MAE_{N}$. The MAE between the baseline enhanced audio (output of FullSubNet or Demucs without finetuning) and clean audio files are denoted as $MAE_B$. Lastly, the MAE between the enhanced audio from the finetuned model and clean audio files is denoted as $MAE_F$. Then, improvement is defined as follows, where $I_B$ is the improvement of baseline models and $I_F$ is the improvement of our fine-tuned models.


% \begin{equation} \label{eq:1}
% I_B= 1-\frac{MAE_{B}}{MAE_{N}}
% \end{equation}
% \begin{equation} \label{eq:2}
% I_F= 1-\frac{MAE_{F}}{MAE_{N}}
% \end{equation}

% Figure \ref{fig:demucs_acoustics} and  \ref{fig:fsn_acoustics} compare the acoustic improvements for our finetuned Demucs and FullSubNet models to the baseline models. We find that our finetuned models can improve our baselines for over 14 acoustic parameters.


% \subsection{Objective Metric Results}
% Our results for the objective metrics are depicted in Table ~\ref{tab:metrics_table_1} We find that our finetuned models are able to outperform the baseline Demucs and FullSubNet models across all metrics.



% \section{Conclusion} In this work, we surpass the industry-standard performance and SOTA baseline architectures -- identify transmission noise as a missing component of current speech enhancement research. We achieve top performance on the VoIP DNS Datasets, improving both the transmission and background noise of audio recordings on multiple platforms over multiple transmission protocols. We set a new benchmark for speech enhancement by evaluating baseline Demucs and FullSubNet models on our novel dataset. Further, we demonstrate that introducing TAPLoss into the training process and finetuning these models can improve performance. We believe that our work can find applications in the telecom industry and directly with mobile phone manufacturers.

% The majority of current research focuses on reducing background noise to improve noisy speech. However, it ignores the impact of transmission noise. In order to address this flaw (not taking into account the transmission noise), we presented a tailored two-pronged solution. First, we introduced a novel dataset that incorporates both the transmission and background noise and then a new benchmark for speech enhancement. In the future, we aim to increase our training data from 400 samples to 1200 samples in order to fine-tune our models. We believe that our work can find applications in the telecom industry and directly with mobile phone manufacturers.

\bibliographystyle{IEEEtran}

\bibliography{template}

\appendix

\section{Appendix}
\subsection{The Temporal Acoustic Parameter Loss (TAPLoss)}
\label{appendix-taploss}
To calculate the TAPLoss, we define the Temporal Acoustic Parameter Estimator $A_y$. $A_y(t, p)$ represents the acoustic parameter $p$ at a discrete time frame $t$. To predict $A_y$, we define the estimator:
$$\hat{A_y} = \Tc\Ac\Pc(y)$$

The $\Tc\Ac\Pc$ function takes in a signal input $y$. It calculates a complex spectrogram $Y$ with $F = 257$ frequency bins. It then passes this complex spectrogram to a recurrent neural network to output the temporal acoustic parameter $\hat{A_y}$. TAP loss is then defined as the mean average error between the actual and the predicted estimate.

$$MAE(A_y, \hat{A_y}) = \frac{1}{TP} \sum_{t=0}^{T-1} \sum_{p=0}^{P-1} | A_y(t, p) - A_{\hat{y}}(t, p) | \in R$$

During training, $\Tc\Ac\Pc$ parameters learn to minimize the divergence of $MAE(A_s, \hat{A_s})$ using Adam optimization. Since this loss is end-to-end differentiable and takes only waveform as input, it enables acoustic optimization of any speech model and task with clean references. 

% \subsection{Metrics Definitions}
% \label{appendix-metrics}
% \begin{table}[htbp]
%   \caption{Evaluation Metrics}
%   \label{table:metrics-table}
%   \begin{tabular}{r|p{8cm}}
%     % \hline
% % \multicolumn{2}{c}{Part}                   \\
%     % \cmidrule(r){1-2}
%     \textbf{Metric} & \textbf{Description}  \\
%     \hline
%     PESQ & Captures the human subjective rating of speech quality degradation caused by network conditions including analog connections and packet loss \cite{rix2001perceptual}\\
%     \hline
%     LLR & Assesses speech quality by determining the goodness of fit between clean and enhanced speech \cite{crochiere1980interpretation}
%     \\
    
%     \hline
    
%     STOI & Measures the intelligibility of degraded speech signals caused by factors such as additive noise \cite{taal2010short} \\ 
%     \hline
%     CSII & Determines speech intelligibility under bandwidth reduction and additive noise conditions by measuring the signal-to-noise ratio in separate frequency bands. It is calculated in three regions: high, mid, and low, where high measures segments at or above the root-mean-square (RMS) decibel level of the speech, mid measures segments in the range (RMS-10, RMS], and low in the range (RMS-30, RMS-10] \cite{kates2005coherence}\\
%     \hline
%     NCM & Estimates speech intelligibility based on the covariance between the envelopes of the clean and noisy speech signals \cite{santos2013}\\
    
%     \hline
%   \end{tabular}
% \end{table}


\clearpage


% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=\linewidth,height=4cm]{img/DEMUCS_batchsize_workers.png}
%   \caption{Demucs: The plot on the left shows the time taken per batch size for each worker. The plot on the right shows the time taken per number of workers for each batch size. This allows us to identify the optimal Demucs batch size.}
%   \label{fig:train2}
% \end{figure*}

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=\linewidth,height=4cm]{img/FSN_batchsize_workers.png}
%   \caption{FullSubNet: The lot on the left shows the time taken per batch size for each worker. The plot on the right shows the time taken per number of workers for each batch size. This allows us to identify the optimal FullSubNet batch size.}
%   \label{fig:train3}
% \end{figure*}



% \begin{figure}[]
%   \centering
%   \includegraphics[width=\linewidth,height=4cm]{img/GPU_Scaling_DEMUCS.png}
%   \caption{GPU Scaling for DEMUCS. This figure demonstrates near-perfect scaling, with a doubling of compute resulting in a doubling of throughput. However, going from 4X to 8X GPUs has diminishing returns.}
%   \label{fig:train4}
% \end{figure}

% \begin{figure}[]
%   \centering
%   \includegraphics[width=\linewidth,height=4cm]{img/GPU_Scaling_DEMUCS.png}
%   \caption{GPU Scaling for FullSubNet. This figure also demonstrates near-perfect scaling, with a doubling of compute resulting in a doubling of throughput. Similar to Demucs, going from 4X to 8X GPUs has diminishing returns.}
%   \label{fig:train5}
% \end{figure}


% \subsection{Optimal Batch Size and the Number of Workers}
% Figures 6 and 7 show our experiments on finding the optimal batch size for training DEMUCS and FullSubNet respectively. We finally decided on a batch size of 36 and a number of workers of 2 for DEMUCS. For FullSubNet, we went with a batch size of 16 and a number of workers of 2.

% \subsection{GPU Scaling}
% Once the batch size and number of workers were decided, we scaled our experiments over multiple GPUs to attain the most optimal time to be used for training as shown in figures 8 and 9.



% % \begin{thebibliography}{9}
% % \bibitem[1]{Davis80-COP}
% %   S.\ B.\ Davis and P.\ Mermelstein,
% %   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
% %   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% % \bibitem[2]{Rabiner89-ATO}
% %   L.\ R.\ Rabiner,
% %   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
% %   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% % \bibitem[3]{Hastie09-TEO}
% %   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
% %   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
% %   New York: Springer, 2009.
% % \bibitem[4]{YourName17-XXX}
% %   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
% %   ``Title of your INTERSPEECH 2021 publication,''
% %   in \textit{Interspeech 2021 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 15-19, Graz, Austria, Proceedings, Proceedings}, 2020, pp.~100--104.
% % \end{thebibliography}

% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.


% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% % An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% % ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% % \begin{figure}[htbp]
% % \centerline{\includegraphics{fig1.png}}
% % \caption{Example of a figure caption.}
% % \label{fig}
% % \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% % Please number citations consecutively within brackets \cite{b1}. The 
% % sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% % number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% % the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% % submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% % that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% % citation first, followed by the original foreign-language citation \cite{b6}.

% % \begin{thebibliography}{00}




% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \end{thebibliography}

% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.



\end{document}
