{
    "arxiv_id": "2303.17452",
    "paper_title": "Theory on variational high-dimensional tensor networks",
    "authors": [
        "Zidu Liu",
        "Qi Ye",
        "Li-Wei Yu",
        "L. -M. Duan",
        "Dong-Ling Deng"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "quant-ph"
    ],
    "abstract": "Tensor network methods are powerful tools for studying quantum many-body systems. In this paper, we investigate the emergent statistical properties of random high-dimensional tensor-network states and the trainability of variational tensor networks. We utilize diagrammatic methods and map our problems to the calculations of different partition functions for high-dimensional Ising models with special structures. To address the notorious difficulty in cracking these models, we develop a combinatorial method based on solving the ``puzzle of polyominoes\". With this method, we are able to rigorously prove that the high-dimensional tensor network models suffer from barren plateaus (i.e., exponentially vanishing gradients) for global loss functions, rendering their training processes inefficient in general. Whereas, for local loss functions, we prove that the gradient is independent of the system size (thus no barren plateau occurs), but decays exponentially with the distance between the region where the local observable acts and the site that hosts the derivative parameter. Our results uncover in a rigorous fashion some fundamental properties for variational high-dimensional tensor networks, which paves a way for their future theoretical studies and practical applications.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17452v1"
    ],
    "publication_venue": "15 pages, 7 figures"
}