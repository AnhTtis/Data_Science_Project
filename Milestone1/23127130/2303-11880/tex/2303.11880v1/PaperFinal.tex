% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% added by wqq
\usepackage{bm}
\usepackage{amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{bbm}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Tab.}{Tab.s}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9206} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Focused and Collaborative Feedback Integration\\for Interactive Image Segmentation}

\author{Qiaoqiao Wei\qquad Hui Zhang\qquad Jun-Hai Yong\\
School of Software, BNRist, Tsinghua University, Beijing, China\\
% Institution1 address\\
{\tt\small wqq18@mails.tsinghua.edu.cn \qquad \{huizhang,yongjh\}@tsinghua.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Interactive image segmentation aims at obtaining a segmentation mask for an image using simple user annotations.
During each round of interaction,
% users provide new annotations based on feedback from the segmentation result in the last interaction,
% making feedback the bridge between interactions.
the segmentation result from the previous round serves as feedback to guide the user's annotation
and provides dense prior information for the segmentation model,
effectively acting as a bridge between interactions.
Existing methods overlook the importance of feedback or simply concatenate it with the original input,
% which underutilizes the feedback
% and leads to an increased number of required annotations.
leading to underutilization of feedback and an increase in the number of required annotations.
To address this,
we propose an approach called Focused and Collaborative Feedback Integration (FCFI) to fully exploit the feedback for click-based interactive image segmentation.
FCFI first focuses on a local area around the new click
and corrects the feedback based on the similarities of high-level features.
It then alternately and collaboratively updates the feedback and deep features
to integrate the feedback into the features.
The efficacy and efficiency of FCFI were validated on four benchmarks,
namely GrabCut, Berkeley, SBD, and DAVIS.
Experimental results show that FCFI achieved new state-of-the-art performance
with less computational overhead than previous methods.
The source code is available at
\url{https://github.com/veizgyauzgyauz/FCFI}.
% Experimental results demonstrate that
% our method has a low computational cost and has outperformed state-of-the-art methods on standard benchmarks.
\end{abstract}


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\columnwidth]{sources_final/pdf/beginning.pdf}
  % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
  \caption{An overview of the interactive system
  and the comparison among (a) independent segmentation \cite{lin2020interactive},
  (b) feedback-as-input segmentation \cite{sofiiuk2021reviving},
  and (c) deep feedback-integrated segmentation.
  % The feedback-as-input segmentation simply takes the feedback as an additional channel of the input.
  % Our method integrates the feedback into deep features, which yields better results.
  See the description in Sec. \ref{sec:introduction}.
  Throughout this paper, green/red clicks represent foreground/background annotations.}
  \label{fig:start}
\end{figure}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:introduction}
Interactive image segmentation aims to segment a target object in an image given simple annotations,
such as bounding boxes \cite{rother2004grabcut,lempitsky2009image,wu2014milcut,xu2017deep,yu2017loosecut},
scribbles \cite{bai2014error,li2004lazy,grady2006random},
extreme points \cite{papadopoulos2017extreme,maninis2018deep,benenson2019large,zhang2020interactive},
and clicks \cite{xu2016deep,jang2019interactive,sofiiuk2020fbrs,lin2020interactive,chen2022focalclick,lin2022focuscut}.
Due to its inherent characteristic, i.e., interactivity,
% users can iteratively add annotations and receive refined segmentation results.
it allows users to add annotations and receive refined segmentation results iteratively.
Unlike semantic segmentation, interactive image segmentation can be applied to unseen categories (categories that do not exist in the training dataset),
demonstrating its generalization ability.
Additionally, compared with instance segmentation,
% it only localizes the annotated instance
% and is thus of high particularity.
interactive segmentation is specific since it only localizes the annotated instance.
% According to a recent report \cite{gill2021changing},
% more than 90\% of the surveyed women retouched their photos for a better look prior to posting the photos online.
% Interactive image segmentation is the technique
% that helps users to automatically segment a target object with only a few annotations.
% Consequently, it is a preliminary step for many applications,
Owing to these advantages,
interactive image segmentation is a preliminary step for many applications,
including image editing and image composition.
This paper specifically focuses on interactive image segmentation using click annotations
because clicks are less labor-intensive to obtain than other types of annotations.

% Describe the process of interaction
The process of interaction is illustrated in Fig. \ref{fig:start}.
Given an image, users start by providing one or more clicks to label background or foreground pixels.
The segmentation model then generates an initial segmentation mask for the target object
based on the image and the clicks.
If the segmentation result is unsatisfactory,
users can continue to interact with the segmentation system
by marking new clicks to indicate wrongly segmented regions and obtain a new segmentation mask.
From the second round of interaction,
the segmentation result of the previous interaction - referred to as \textit{feedback} in this paper - will be fed back into the current interaction.
% On the one hand,
% this feedback will serve as the basis for user labeling.
% On the other hand,
% it can also provide prior information for the segmentation model,
% which helps the model converge much more easily and improves performance \cite{sofiiuk2021reviving}.
This feedback is instrumental in user labeling and provides the segmentation model with prior information,
which can facilitate convergence and improve the accuracy of segmentation \cite{sofiiuk2021reviving}.


% Available information for interactive image segmentation includes
% input images, annotated clicks, and intermediate segmentation masks.
% % whose hierarchy is illustrated in the bottom of Fig. \ref{fig:pyramid}.
% Fig. \ref{fig:pyramid}(a) illustrates an overview of the available information,
% which qualitatively integrates the amount of semantic information that the three types of information contain
% and the difficulty to obtain them.
% Images are easily obtained yet only contain low-level information, such as colors and textures.
% User annotations are essentially sparse prior semantic knowledge from humans,
% and require extra labor to obtain.
% Segmentation results, generated by deep neural networks, are denser than annotated clicks
% and convey more high-level semantic hints.

Previous methods have made tremendous progress in interactive image segmentation.
Some of them, such as \cite{majumder2019content,lin2020interactive,sofiiuk2021reviving},
focused on finding useful ways to encode annotated clicks,
while others, like \cite{li2018interactive,jang2019interactive,liew2019multiseg,sofiiuk2020fbrs,chen2021conditional},
explored efficient neural network architectures to fully utilize user annotations.
However, few methods have investigated how to exploit informative segmentation feedback.
Existing methods typically treated each round of interaction independent
\cite{li2018interactive,jang2019interactive,majumder2019content,lin2020interactive,chen2021conditional,hao2021edgeflow}
or simply concatenated feedback with the initial input
% at the beginning of a segmentation network
\cite{mahadevan2018iteratively,sofiiuk2020fbrs,sofiiuk2021reviving,chen2022focalclick,lin2022focuscut}.
The former approach (Fig. \ref{fig:start}(a)) failed to leverage the prior information provided by feedback,
resulting in a lack of consistency in the segmentation results generated by two adjacent interactions.
In the latter case (Fig. \ref{fig:start}(b)),
feedback was only directly visible to the first layer of the network,
and thus the specific spatial and semantic information it carried would be easily diluted or even lost
% easily led to a rapid dilution or even a loss of prior information \cite{chen2021conditional}
through many convolutional layers,
similar to the case of annotated clicks \cite{hao2021edgeflow}.
% To summarize,
% existing methods underutilized the semantic clues
% or utilized them in a sub-optimal way.

In this paper, we present Focused and Collaborative Feedback Integration (FCFI)
to exploit the segmentation feedback for click-based interactive image segmentation.
% FCFI corrects the feedback locally using a focused feedback correction module (FFCM)
% and incorporates the whole feedback into deep features via a collaborative feedback fusion module (CFFM).
FCFI consists of two modules:
a Focused Feedback Correction Module (FFCM) for local feedback correction
and a Collaborative Feedback Fusion Module (CFFM) for global feedback integration into deep features.
Specifically, 
the FFCM focuses on a local region centered on the new annotated click
to correct feedback.
It measures the feature similarities between each pixel in the region and the click.
The similarities are used as weights to blend the feedback and the annotated label.
The CFFM adopts a collaborative calibration mechanism to integrate the feedback into deep layers (Fig. \ref{fig:start}(c)).
% In the module, the feedback and deep features mutually update each other.
% A gate is utilized to control the information flow.
% It has two information flow pathways,
% where the feedback and deep features mutually update each other,
% and a gate to control whether the features need to be updated.
First, it employs deep features to globally update the corrected feedback for further improving the quality of the feedback.
Then, it fuses the feedback with deep features via a gated residual connection.
Embedded with FCFI,
the segmentation network leveraged the prior information provided by the feedback
and outperformed many previous methods.


\section{Related Work}
\textbf{The Method changes in interactive segmentation.}
Early work approached interactive segmentation based on graph models,
% such as graph cuts \cite{boykov2001interactive,rother2004grabcut,freedman2005interactive}
such as graph cuts \cite{boykov2001interactive,rother2004grabcut}
and random walks \cite{grady2006random,dong2015sub}.
% A graph model was created based on the low-level features (i.e., colors) of user inputs,
% and then used to predict foreground and background probabilities for each pixel.
These methods build graph models on the low-level features of input images.
Therefore, they are sensitive to user annotations and lack high-level semantic information.
Xu et al. \cite{xu2016deep} first introduced deep learning into interactive image segmentation.
Deep convolutional neural networks are optimized over large amounts of data,
which contributes to robustness and generalization.
Later deep-learning-based methods \cite{li2018interactive,jang2019interactive,chen2021conditional} showed striking improvements.
% a process consisting of click encoding and segmenting with a convolutional neural network,
% Later methods followed the two-step protocol and made further contributions.

\textbf{The Local refinement for interactive segmentation.}
To refine the primitive predictions,
previous methods \cite{liew2017regional,chen2021conditional,hao2021edgeflow,chen2022focalclick}
introduced additional convolutional architectures to fulfill the task.
These methods followed a coarse-to-fine scheme
and performed global refinement.
Later, Sofiiuk et al. \cite{sofiiuk2020fbrs} proposed the Zoom-In strategy.
From the third interaction, it cropped and segmented the area within an expanded bounding box of the inferred object.
Recent approaches \cite{chen2022focalclick,lin2022focuscut} focused on local refinement.
After a forward pass of the segmentation network,
they found the largest connected component on the difference between the current prediction and the previous prediction.
Then, they iteratively refined the prediction in the region using the same network or an extra lighter network.
The proposed FFCM differs from these methods in that
it does not need to perform feedforward propagation multiple times.
Instead, it utilizes the features already generated by the backbone of the network
to locally refine the feedback once.
Thus, it is non-parametric and fast.

\textbf{The Exploitation of segmentation feedback.}
Prior work has found that
segmentation feedback has instructive effects on the learning of neural networks.
% and utilized it to improve the segmentation performance.
Mahadevan et al. \cite{mahadevan2018iteratively} were the first to incorporate
feedback from a previous iteration into the segmentation network.
They took the feedback as an optional channel of the input.
Later methods \cite{sofiiuk2020fbrs,sofiiuk2021reviving,chen2022focalclick,lin2022focuscut} followed this
and concatenated the feedback with the input.
However, this naive operation may be suboptimal for feedback integration due to the dilution problem \cite{hao2021edgeflow}.
Different from previous approaches,
the proposed CFFM integrates feedback into deep features.
Since both the feedback and the high-level features contain semantic knowledge of specific objects,
fusing them together helps to improve the segmentation results.

% To leverage user annotations,
% prior work presented various click encoding mechanisms \cite{majumder2019content,lin2020interactive,sofiiuk2021reviving}.
% Majumder and Yao \cite{majumder2019content} proposed to integrate distance-, superpixel-, and object-based encoding
% to transform clicks into content-aware guidance maps (CAGs).
% % Others enhanced important clicks or automatically generated new clicks.
% % Lin et al. \cite{lin2020interactive} found that
% % the first annotated click for a sample usually indicates the center position of a target object,
% Lin et al. \cite{lin2020interactive} found that
% the first annotated click for a sample is usually put on the center position of a target object and is thus more intuitive.
% Therefore, they proposed a first click attention network (FCA-Net) to shift attention onto the first click.
% % Song et al.  \cite{song2018seednet} introduced a reinforcement learning technique to recommend new annotated clicks.
% As for feedback, it has been underutilized in most methods.
% Previous work either overlooked feedback \cite{majumder2019content,lin2020interactive,chen2021conditional}
% % regards each interaction as an independent process
% % which means that the segmentation network perceives no segmentation prior.
% or received it using an additional input channel \cite{sofiiuk2020fbrs,sofiiuk2021reviving}.
% The latter, called early fusion \cite{hao2021edgeflow}, is ineffectual
% because shallow layers in neural networks focus on low-level information and dilute the prior segmentation information quickly.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{sources_final/pdf/pipeline.pdf}
    % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
    \caption{The pipeline of the proposed method. See detailed description in Sec. \ref{sec:method_pipeline}.}
    % First-click-enhanced encoding is employed first to transform annotated clicks into interaction maps.
    % Next, a segmentation model embedded with a gated feedback fusion module is applied to generate segmentation results.
    % Finally, the proposed reliability-guided attention is used for boundary refinement if in inference.
   
    \label{fig:pipeline}
\end{figure*}


\section{Method}
\subsection{Interaction Pipeline}
\label{sec:method_pipeline}
The pipeline of our method is illustrated in Fig. \ref{fig:pipeline}.
The process includes two parts: interaction and segmentation.

\textbf{Interaction.}
In the interaction step,
users give hints about the object of interest
% which designate background/foreground (negative/positive) pixels,
to the segmentation model by providing clicks.
% and then the annotations are encoded into an interaction map.
In the first interaction,
users provide clicks
only based on the input image $\bm{I}\in\mathbb{R}^{H\times W\times 3}$,
where $H$ and $W$ denote the height and width, respectively;
in the subsequent interactions,
users mark more clicks according to the input image and the segmentation feedback.
The annotated clicks over all interactions are denoted as $\{\mathcal{P}_0, \mathcal{P}_1\}$,
where $\mathcal{P}_0$ and $\mathcal{P}_1$ represent a background and foreground click set, respectively,
and each element in $\mathcal{P}_0$ and $\mathcal{P}_1$ is the $xy$ position of a click.
The annotated clicks are converted into an interaction map $\bm{S}\in\mathbb{R}^{H\times W\times 2}$ using disk encoding \cite{benenson2019large},
which represents each click as a disk with a small radius.

\textbf{Segmentation.}
In the segmentation step,
a commonly used fully convolutional baseline architecture embedded with FCFI is utilized as the segmentation network.
The baseline network comprises a backbone and a segmentation head,
as shown in the yellow blocks in Fig. \ref{fig:pipeline}.
The backbone gradually captures long-range dependencies of pixels to generate high-level features,
and the segmentation head, including a Sigmoid function, recovers spatial resolutions and extracts semantic information.
% The input image $\bm{I}$ and the interaction map $\bm{S}$ are concatenated in depth to form a five-channel image.
% An FFCM and a CFFM are inserted between the backbone and the segmentation head.
% They correct the feedback locally and incorporate it into the segmentation network.
To correct feedback locally and incorporate it into the segmentation network,
an FFCM and a CFFM are inserted between the backbone and the segmentation head.
% detailed in Sec. \ref{sec:FFCM} and \ref{sec:CFFM}, respectively.
In the $t$-th interaction,
the input image $\bm{I}$ and the interaction map $\bm{S}_t$ are concatenated in depth and fed into the backbone,
and the feedback $\bm{M}_{t-1}$ is directly fed into the FFCM.
The segmentation network outputs a segmentation mask $\bm{M}_{t}\in\mathbb{R}^{H\times W}$.


\subsection{Focused Feedback Correction Module}
\label{sec:FFCM}
From the second interaction,
users are expected to place new clicks on the mislabeled regions.
However, most previous methods \cite{jang2019interactive,majumder2019content,jang2019interactive,sofiiuk2020fbrs,chen2021conditional,sofiiuk2021reviving}
treated all clicks equally and only performed global prediction,
which may weaken the guidance effect of the newly annotated clicks
and cause unexpected changes in correctly segmented regions that are far away from the newly annotated click \cite{chen2022focalclick}.
For instance, in Fig. \ref{fig:start}(a) and (b), comparing $\bm{M}_{t-1}$ and $\bm{M}_t$,
% a new negative click is added to the board near the person's right hand,
% but the segmentation result is changed in the person's feet.
adding a new negative click near the person's right hand modified the segmentation result in the person's feet.
Considering that clicks are usually marked to correct small regions of a segmentation mask,
locally refining the feedback can not only preserve the segmentation results in other regions
but also reduce processing time.
Therefore, we propose the FFCM to correct the feedback from a local view.

As shown in Fig. \ref{fig:pipeline}(a),
the feedback modification in the FFCM requires three steps:
cropping, local refinement, and pasting.
We first narrow down the refinement region,
then refine the feedback using the pixel similarities in the high-level feature space,
and finally fuse the refined feedback with the original one.

% \begin{itemize}
%   \item \textbf{Crop.}
%   In order to exclude irrelevant regions and focus on the area around the newly annotated click,
%   we crop the feedback $\bm{M}_{t-1}$ into a small region.
% \end{itemize}
\textbf{Cropping.}
To exclude irrelevant regions and focus on the area around the newly annotated click,
we select a small rectangle patch centered on the new annotated click
and crop it from the features $\bm{F}\in\mathbb{R}^{H'\times W'\times 3}$ generated by the backbone,
where $H'$ and $W'$ are the height and width of $\bm{F}$.
The patch has a size of $rH'\times rW'$,
where the expansion ratio $r$ is $0.3$ by default.
The feedback $\bm{M}_{t-1}$
is cropped in the same way.
The cropped features and the cropped feedback are denoted as $\bm{F}^c$ and $\bm{M}_{t-1}^c$, respectively.

\textbf{Local refinement.}
In this step, per-pixel matching is performed.
We measure the feature affinity between each pixel in the patch and the new annotated click
and then blend the feedback with the annotated label of the new click according to the affinity.
% The features of all pixels in the patch are encoded into a Euclidean space.
The feature affinity $\bm{W}\in\mathbb{R}^{rH'\times rW'}$ is defined as the cosine similarity:
\begin{equation}
  \bm{W}(i,j)=\frac{\bm{F}^c(i,j)\bm{F}^c(x_{new},y_{new})}{||\bm{F}^c(i,j)||_2||\bm{F}^c(x_{new},y_{new})||_2},
\end{equation}
where $(x_{new}, y_{new})$ is the coordinate of the new annotated click in the patch.
Each element in the feature affinity $\bm{W}$ is between 0 to 1.
The larger the affinity is, the more likely that the pixel belongs to the same class (background/foreground) as the new annotated click.
The refined cropped feedback $\bm{M}_{t-1}^r$ is generated
by blending the original feedback $\bm{M}^c_{t-1}$ and the annotated label of the new click $l_{new}$
via a linear combination:
\begin{equation}
  \bm{M}^{c,r}_{t-1}=l_{new}\cdot \bm{W}+(1-\bm{W})\circ\bm{M}^c_{t-1}.
\end{equation}

\textbf{Pasting.}
After obtaining the refined cropped feedback,
we paste it back to the original position on the feedback $\bm{M}_{t-1}$
and denote the refined full-size feedback as $\bm{M}^r_{t-1}$.

To enable optimization in backward-propagation during training,
the cropping and pasting operations are not applied in the FFCM.
Instead, a box mask $\bm{M}_{box}\in\mathbb{R}^{H'\times W'}$
is used to filter out the pixels that are far away from the new annotated click.
A local region is selected first.
It is centered on the new annotated click and has a size of $rH'\times rW'$.
Pixels within the region
are set to 1,
and others are set to 0.
% Mathematically,
% \begin{equation}
%   \begin{aligned}
%   \bm{M}_{box}(i,j)=\left[\mathbbm{1}(i-x_{l})-\mathbbm{1}(i-x_r)\right] \\
%   \cdot\left[\mathbbm{1}(j-y_{t})-\mathbbm{1}(j-y_t)\right],
%   \end{aligned}
% \end{equation}
% where $x_l=x_{new}-\frac{rW'}{2}$ is the left boundary of the rectangle,
% $x_r=x_{new}+\frac{rW'}{2}$ is the right boundary of the rectangle,
% $y_t=y_{new}-\frac{rH'}{2}$ is the top boundary of the rectangle,
% $y_b=y_{new}+\frac{rH'}{2}$ is the bottom boundary of the rectangle,
% and $\mathbbm{1}(\cdot)$ is an indicator function:
% \begin{equation}
%   \mathbbm{1}(x)=
%   \begin{cases}
%     1, & x\geq0, \\
%     0, & x<0.
%   \end{cases}
% \end{equation}
Similar to the local refinement,
we perform global refinement on the full-size features and feedback.
A full-size refined feedback $\bm{M}^{r'}_{t-1}$ is obtained.
Finally, we keep the focused area and obtain the corrected feedback $\bm{M}^r_{t-1}$ as
\begin{equation}
  \bm{M}^r_{t-1}=\bm{M}_{box}\circ\bm{M}^{r'}_{t-1}+(1-\bm{M}_{box})\circ\bm{M}_{t-1}.
\end{equation}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.99\columnwidth]{sources_final/pdf/corr_results.pdf}
%   % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
%   \caption{Visualization of the feedback, the feature affinity, and the refined feedback.
%   Red color denotes high feature affinity, and blue color denotes low feature affinity.
%   }
%   \label{fig:corr_restuls}
% \end{figure}

\subsection{Collaborative Feedback Fusion Module}
\label{sec:CFFM}
% \subsubsection{Basic segmentation network}
% In this work, a fully convolutional architecture is utilized as the basic segmentation network.
% As shown in the middle of Fig. \ref{fig:pipeline}, the model consists of a backbone and a segmentation head.
% The backbone gradually captures long-range dependencies of pixels to generate high-level features,
% and the segmentation head (including a Sigmoid function) recovers spatial resolutions and extracts semantic information.
% % The input image $\bm{I}$ and the interaction map $\bm{S}$ are concatenated in depth to form a five-channel image.
% $\bm{I}$ and $\bm{S}$ are concatenated in depth to form a five-channel image.
% The segmentation network takes this image as input and outputs a one-channel segmentation mask $\bm{M}_{seg}$.

% \subsubsection{Gated deep feedback fusion module}

In an interaction, a user usually adds new clicks based on the segmentation feedback.
Basically, interactions are successive,
and previous segmentation results provide prior information about the location
and shape of the target object for the current interaction.
% Deep convolutional neural networks have shown a remarkable ability for localizing and segmenting the target object.
% However, available semantic information, especially segmentation feedback,
% is either underutilized or sub-optimally employed in most methods \cite{jang2019interactive,lin2020interactive,chen2022focalclick}.
Therefore, it is natural to integrate feedback into the segmentation network,
and this integration is supposed to improve the segmentation quality.

Previous methods \cite{sofiiuk2020fbrs,sofiiuk2021reviving,chen2022focalclick}
simply concatenated the feedback with the input image and the interaction map
and then fed them into the segmentation network.
However, experimental results show that
this naive method cannot exploit the prior information provided by the feedback.
There are two reasons.
First, early fusion - fusing at the beginning or shallow layers of a network -
easily causes information dilution or loss \cite{hao2021edgeflow}.
Second, from the semantic information perspective,
the feedback contains dense semantic information
compared with the low-level input image and sparse clicks.
% Theoretically, it is more appropriate to fuse the feedback with deep features
% because both of them contain high-level cues.
Thus, theoretically, fusing the feedback into deep features rather than the input
enables the segmentation network to obtain segmentation priors
while preserving the extracted semantic information.

The CFFM is introduced in this paper to integrate segmentation feedback into high-level features
at deep layers of the segmentation network.
Fig. \ref{fig:pipeline}(b) illustrates the architecture of the CFFM.
The CFFM comprises two information pathways,
called feedback pathway and feature pathway, respectively.
The two pathways utilize the feedback and the high-level features
to collaboratively update each other.

\textbf{Feedback pathway.}
This pathway performs global refinement on the feedback
with the aid of deep features.
First, after being encoded by a convolution block $\Phi_1$,
the features $\bm{F}$
are concatenated with the corrected feedback signal $\bm{M}^r_{t-1}$ in the channel dimension.
Then, convolutional layers followed by a Sigmoid function, denoted as $\Phi_2$,
are applied to update the feedback:
\begin{equation}
  \bm{M}^u_{t-1}=\Phi_2(\text{concat}(\Phi_1(\bm{F};\theta_1),\bm{M}^r_{t-1});\theta_2),
\end{equation}
% where $\mathbin\Vert$ denotes channel-wise concatenation.
where $\theta_1$ and $\theta_2$ denote the learnable parameters of $\Phi_1$ and $\Phi_2$,
and ``$\text{concat}(\cdot,\cdot)$'' denotes channel-wise concatenation.

\textbf{Feature pathway.}
In this pathway,
we update the high-level features with the feedback as a guide.
% The features $\bm{F}$ are updated in the predicted positive regions using a skip connection.
% The features $\bm{F}$ are encoded by a convolution block $\Phi_3$.
% Then, they are fused with the updated feedback $\bm{M}^u_{t-1}$ through a convolution block $\Phi_4$.
The features are fused with the updated feedback $\bm{M}^u_{t-1}$ through a convolution block $\Phi_3$.
To avoid negative outcomes from useless learned features,
we update the features $\bm{F}$ via a skip connection following ResNet \cite{he2016deep}.
% \begin{equation}
%   \bm{F}^u=\Phi_4(\bm{M}^u_{t-1}\oplus \Phi_3(\bm{F},\theta_3);\theta_4)+\bm{F},
% \end{equation}
% where $\theta_3$ and $\theta_4$ are the learnable parameters of $\Phi_3$ and $\Phi_4$.
However, the skip connection leads to an inaccurate prediction of the final output $\bm{M}_t$ in the first interaction,
but not in the other interactions.
The reason is that the feedback
% is initialized to a map with all pixels values equal to 0.5
is initialized to an all-zero map
and has the maximum information entropy in the first interaction \cite{mahadevan2018iteratively}.
Consequently, it may add noise to deep features.
The reliability of feedback grows dramatically from the first interaction to the second interaction,
making the feedback more instructive.
To tackle this problem,
we introduce a gate $w_g$ to control the information flow from the feedback pathway.
The gate is set to 0 in the first interaction
to prevent the inflow of noise from hurting performance,
% and it is a learnable parameter in subsequent interactions
% to flexibly control the information flow.
and it is set to 1 in the subsequent interactions.
Mathematically, the fused features $\bm{F}^u$ can be formulated as
\begin{equation}
  \label{eq:update_feature}
  \bm{F}^u=w_g\cdot \Phi_3(\text{concat}(\bm{F},\bm{M}^u_{t-1});\theta_3)+\bm{F},
\end{equation}
% where $\odot$ represents element-wise product.
where $\theta_3$ denotes the learnable parameters of $\Phi_3$.
% and $w_g$ is fixed to $0$ in the first interaction and $1$ in other interactions.
% $w_g$ is fixed to $0$ in the first interaction and is a learnable parameter in subsequent interactions.
% $\Psi_{ft}$ represents a mapping function, which can be a linear function or a convolution block.

\subsection{Training Loss}
The normalized focal loss $L_{nfl}$ \cite{sofiiuk2019adaptis} is applied as the objective function
because training with it yields better performance in interactive image segmentation,
which is demonstrated in RITM \cite{sofiiuk2021reviving}.
The constraint is imposed on $\bm{M}^r_{t-1}$, $\bm{M}_t$, and $\bm{M}^u_{t-1}$.
Particularly, we only calculate the loss in the cropped area for $\bm{M}^r_{t-1}$.
The full loss function is
\begin{equation}
  \begin{aligned}
  L=L_{nfl}(\bm{M}^r_{t-1},\bm{M}_{gt})+L_{nfl}(\bm{M}^u_{t-1},\bm{M}_{gt}) \\
  +L_{nfl}(\bm{M}_{t},\bm{M}_{gt}),
  \end{aligned}
\end{equation}
where $\bm{M}_{gt}$ denotes the ground truth segmentation mask.


% trained on the SBD
% \setlength{\tabcolsep}{4pt}
\begin{table*}[t]
  \begin{center}
  \resizebox{0.99\linewidth}{!}{
  \begin{tabular}{l|l|l|c|c|c|c|c|c|c}
  % \begin{tabular}{lllccccccc}
  % \hline\noalign{\smallskip}
  \toprule
      \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Train set} & \multicolumn{2}{c|}{GrabCut} & Berkeley & \multicolumn{2}{c|}{SBD} & \multicolumn{2}{c}{DAVIS} \\
      \cline{4-10}
      &&& NoC@85\% & NoC@90\% & NoC@90\% & NoC@85\% & NoC@90\% & NoC@85\% & NoC@90\% \\
      % \noalign{\smallskip}
      % \hline
      % \noalign{\smallskip}
      \midrule
      DOS w/o GC \cite{xu2016deep}           & FCN-8s  & SBD & 8.02 & 12.59 & - & 14.30 & 16.79 & 12.52 & 17.11 \\
      DOS w/ GC \cite{xu2016deep}            & FCN-8s  & SBD & 5.08 & 6.08 & - & 9.22 & 12.80 & 9.03 & 12.58 \\
      RIS-Net \cite{liew2017regional}        & VGG-16  & Pascal VOC  &   -  & 5.00 & - & 6.03 & - &   -  &   -    \\
      LD \cite{li2018interactive}            & VGG-19  & SBD   & 3.20 & 4.79 &   -   & 7.41 &   -  & 5.95 & 9.57 \\
      CAG \cite{majumder2019content}         & FCN-8s & Augmented SBD &   -  & 3.58 & 5.60 & - &   -  &   -  &   -  \\
      % ITIS         & ResNet-101 &  Augmented SBD & - &   5.60  & - &   -  &   -  &   -  &   -  \\
      % BRS$^\dag$   & ResNet-101 & 1.58 & 2.07 & 5.08 &   -  & 6.59 & 9.78 & 5.58 & 8.24 \\
      BRS \cite{jang2019interactive}         & DenseNet & SBD  & 2.60 & 3.60 & 5.08 & 6.59 & 9.78 & 5.58 & 8.24 \\
      \hline
      f-BRS-B \cite{sofiiuk2020fbrs}         & ResNet-101 & SBD & 2.30 & 2.72 & 4.57 & 4.81 & 7.73 & 5.04 & 7.41 \\
      % FCA-Net \cite{lin2020interactive}      & ResNet-101 & Augmented SBD &   -  & 2.14 & 4.19 & - &   -  &   -  & 7.90 \\
      FCA-Net \cite{lin2020interactive}      & ResNet-101 & Augmented SBD & 1.88 & 2.14 & 4.19 & - & - & 5.38 & 7.90 \\
      IA+SA \cite{kontogianni2020continuous} & ResNet-101 & Augmented SBD & - & 3.07 & 4.94 & - & - & 5.16 & - \\
      % CDNet \cite{chen2021conditional}       & ResNet-50  & SBD & 2.22 & 2.64 & 3.69 & 4.37 & 7.87 & 5.17 & 6.66 \\
      CDNet \cite{chen2021conditional}       & ResNet-101 & SBD & 2.42 & 2.76 & 3.65 & 4.73 & 7.66 & 5.33 & 6.97 \\
      FocusCut \cite{lin2022focuscut}        & ResNet-101 & SBD & \textbf{1.46} & \textbf{1.64} & \underline{3.01} & \underline{3.40} & \textbf{5.31} & \underline{4.85} & \textbf{6.22} \\
      % FocusCut \cite{lin2022focuscut}        & ResNet-101 & SBD & \textbf{1.46} & \textbf{1.64} & 3.01 & 4.13 & 6.37 & 5.04 & 7.03 \\
      % \textbf{Ours}                          & ResNet-101 & SBD & 1.58 & 1.82 & \textbf{2.44} & \textbf{3.67} & \textbf{5.98} & \textbf{4.91} & \textbf{6.00} \\
      % \textbf{Ours}                          & ResNet-101 & SBD & \underline{1.64} & \underline{1.78} & \textbf{2.85} & \textbf{3.27} & \underline{5.35} & \textbf{4.60} & \textbf{5.82} \\ % PAMR 10 [1,2] for DAVIS
      \textbf{Ours}                          & ResNet-101 & SBD & \underline{1.64} & \underline{1.80} & \textbf{2.84} & \textbf{3.26} & \underline{5.35} & \textbf{4.75} & \underline{6.48} \\ % PAMR 10 [1,2] for DAVIS
      % GrabCut 600 3.2 epoch125 1.64 1.80
      % Berkeley 720 2.5 epoch125 1.75 2.84
      % DAVIS 640 3.0 epoch125 4.75 6.48
      \hline
      RITM \cite{sofiiuk2021reviving}        & HRNet-18s  & COCO+LVIS & 1.54 & 1.68 & 2.60 & \underline{4.26} & 6.86 & 4.79 & 6.00 \\
      % FocalClick-S1 \cite{chen2022focalclick} & HRNet-18s & COCO+LVIS & 1.64 & 1.82 & 2.89 & 4.74 & 7.29 & 4.77 & 6.56 \\
      FocalClick-S1 \cite{chen2022focalclick} & HRNet-18s & COCO+LVIS & 1.72 & 1.94 & 3.40 & 4.75 & 7.22 & 5.19 & 7.95 \\
      % FocalClick-S2 \cite{chen2022focalclick} & HRNet-18s & COCO+LVIS & 1.48 & 1.62 & 2.66 & 4.43 & 6.79 & 3.90 & 5.25 \\
      FocalClick-S2 \cite{chen2022focalclick} & HRNet-18s & COCO+LVIS & \underline{1.52} & \underline{1.66} & \underline{2.41} & 4.37 & \underline{6.59} & \underline{4.20} & \underline{5.49} \\
      % \textbf{Ours}                          & HRNet-18s  & COCO+LVIS & \textbf{1.42} & \textbf{1.60} & \textbf{2.13} & \textbf{3.88} & \textbf{6.24} & \textbf{3.88} & \textbf{5.25} \\
      \textbf{Ours}                          & HRNet-18s  & COCO+LVIS & \textbf{1.50} & \textbf{1.56} & \textbf{2.05} & \textbf{3.88} & \textbf{6.24} & \textbf{3.70} & \textbf{5.16} \\
      % hrnet18s_cocolvis_resfeedbackfusion5_4_3_2_expanratio0.3_000 epoch190
      % GrabCut 600 2.8 epoch190 1.50 1.56
      % GrabCut 600 2.8 epoch135 1.48 1.66
      % Berkeley 720 2.5 epoch190
      % Berkeley 640 2.7 epoch135 1.54 2.05
      % DAVIS 720 3.0 epoch135 3.85 5.19
      \hline
      EdgeFlow \cite{hao2021edgeflow}        & HRNet-18   & COCO+LVIS & 1.60 & 1.72 & 2.40 & - &   -  & 4.54 & 5.77 \\
      RITM \cite{sofiiuk2021reviving}        & HRNet-18   & COCO+LVIS & \underline{1.42} & \underline{1.54} & \underline{2.26} & \underline{3.80} & \underline{6.06} & \underline{4.36} & \underline{5.74} \\
      \textbf{Ours}                          & HRNet-18   & COCO+LVIS & \textbf{1.38} & \textbf{1.46} & \textbf{1.96} & \textbf{3.63} & \textbf{5.83} & \textbf{3.97} & \textbf{5.16} \\
      % \hline
      % \multirow{2}{*}{Ours} & ResNet-101 & \textbf{1.48} & \textbf{1.62} & \textbf{2.67} & \textbf{2.84} & \textbf{4.39} & \textbf{5.79} \\
      %                       & HRNet-18   & \textbf{1.38} & \textbf{1.54} & \textbf{1.96} & \textbf{2.26} & \textbf{3.70} & \textbf{4.99} \\
  \bottomrule
  \end{tabular}}
  \end{center}
  \vspace{-0.24cm}
  \caption{Evaluation results on the GrabCut, Berkeley, SBD, and DAVIS datasets.
  The augmented SBD is a combination of the Pascal VOC training set \cite{everingham2010pascal},
  a part of the SBD training set, and a part of the SBD validation set.
  Throughout this essay, the best and the second-best results for different mainstream backbones are written in bold and underlined, respectively.}
  \label{tab:results}
\end{table*}


% trained on the SBD
% \setlength{\tabcolsep}{4pt}
% \begin{table*}[t]
%   \begin{center}
%   \resizebox{0.99\linewidth}{!}{
%   \begin{tabular}{l|c|c|c|c|r|c|c|c|c|r}
%   \hline\noalign{\smallskip}
%       \multirow{2}{*}{Method} & \multicolumn{5}{c|}{Berkeley} & \multicolumn{5}{c}{DAVIS} \\
%       \cline{2-11}
%       & NoC@90\%$\downarrow$ & IoU@5$\uparrow$ & BIoU@5$\uparrow$ & ASSD@5$\downarrow$ & SPC$\downarrow$ & NoC@90\%$\downarrow$ & IoU@5$\uparrow$ & BIoU@5$\uparrow$ & ASSD@5$\downarrow$ & SPC$\downarrow$ \\
%       \noalign{\smallskip}
%       \hline
%       \noalign{\smallskip}
%       % f-BRS-C \cite{sofiiuk2020fbrs} $\dagger$ & 4.29 & 0.887 & 0.736 & 3.883 & 0.030 & 7.69 & 0.843 & 0.733 & 9.717 & 0.048 \\
%       % f-BRS-B \cite{sofiiuk2020fbrs} $\dagger$ & 4.13 & 0.875 & 0.730 & 4.626 & 0.034 & 7.41 & 0.826 & 0.717 & 11.267 & 0.056 \\
%       f-BRS-B \cite{sofiiuk2020fbrs} $\dagger$ & 4.13 & 0.875 & 0.730 & 4.626 & 0.072 & 7.41 & 0.826 & 0.717 & 11.267 & 0.102 \\
%       % f-BRS-A \cite{sofiiuk2020fbrs} $\dagger$ & 3.89 & 0.891 & 0.760 & 3.340 & 0.063 & 7.36 & 0.841 & 0.740 & 10.434 & 0.123 \\
%       % Dist-BRS \cite{sofiiuk2020fbrs} $\dagger$ & 3.98 & 0.881 & 0.739 & 4.492 & 0.668 & 7.56 & 0.849 & 0.748 & 9.957 & 1.273 \\
%       % RGB-BRS \cite{sofiiuk2020fbrs} $\dagger$ & 3.78 & 0.902 & 0.760 & 3.843 & 0.899 & 7.26 & 0.865 & 0.765 & 9.249 & 1.913 \\
%       FCA-Net \cite{lin2020interactive} $\dagger$ & 4.19 & 0.923 & 0.793 & 2.190 & 0.059 & 7.90 & 0.867 & 0.771 & 9.048 & \underline{0.075} \\
%       CDNet \cite{lin2020interactive} $\dagger$ & 3.09 & 0.921 & 0.803 & 2.576 & 0.079 & 6.24 & 0.876 & 0.783 & 8.973 & 0.108 \\
%       FocusCut \cite{lin2022focuscut} $\dagger$ & \underline{3.01} & \underline{0.933} & \underline{0.811} & \underline{2.050} & 15.801 & 7.03 & 0.873 & 0.778 & 8.880 & 13.135 \\
%       \textbf{Baseline} & 4.31 & 0.904 & 0.782 & 4.370 & \textbf{0.052} & 7.56 & 0.867 & 0.770 & 9.421 & \textbf{0.069} \\
%       \textbf{Ours}     & \textbf{2.85} & \textbf{0.940} & \textbf{0.833} & \textbf{1.890} & \underline{0.057} & \textbf{5.82} & \textbf{0.893} & \textbf{0.815} & \textbf{8.226} & 0.097 \\
%       \hline
%       RITM \cite{sofiiuk2021reviving} $\dagger$ & 2.26 & 0.956 & 0.871 & 1.170 & 0.056 & 5.74 & 0.888 & 0.809 & 7.917 & 0.054 \\
%       FocalClick-S1 \cite{chen2022focalclick} $\dagger$ & 3.40 & 0.946 & 0.849 & 1.565 & 0.065 & 7.95 & 0.877 & 0.874 & 8.364 & 0.084 \\
%       FocalClick-S2 \cite{chen2022focalclick} $\dagger$ & 2.41 & 0.958 & 0.889 & 1.170 & 0.052 & 5.49 & 0.897 & 0.824 & 7.635 & 0.082 \\
%       \textbf{Baseline}                          &  &  &  &  &  &  &  &  &  & \\
%       \textbf{Ours}                              & 1.96 & 0.962 & 0.891 & 1.0703 & 0.046 & 5.35 & 0.900 & 0.820 & 7.460 & 0.048 \\
%       % \hline
%       % \multirow{2}{*}{Ours} & ResNet-101 & \textbf{1.48} & \textbf{1.62} & \textbf{2.67} & \textbf{2.84} & \textbf{4.39} & \textbf{5.79} \\
%       %                       & HRNet-18   & \textbf{1.38} & \textbf{1.54} & \textbf{1.96} & \textbf{2.26} & \textbf{3.70} & \textbf{4.99} \\
%   \hline
%   \end{tabular}}
%   \end{center}
%   \caption{Comparisons of effectiveness and efficiency on Berkeley and DAVIS datasets.
%   $\dagger$ denotes our implementation.
%   The best and the second best results are written in bold and underlined, respectively.}
%   \label{tab:other_results}
% \end{table*}

% \begin{table*}[t]
%   \begin{center}
%   \resizebox{0.99\linewidth}{!}{
%   \begin{tabular}{l|c|c|c|c|r|c|c|c|c|r}
%   \hline\noalign{\smallskip}
%       \multirow{2}{*}{Method} & \multicolumn{5}{c|}{Berkeley} & \multicolumn{5}{c}{DAVIS} \\
%       \cline{2-11}
%       & NoF$_{20}$@90\%$\downarrow$ & IoU@5$\uparrow$ & BIoU@5$\uparrow$ & ASSD@5$\downarrow$ & SPC$\downarrow$ & NoF$_{20}$@90\%$\downarrow$ & IoU@5$\uparrow$ & BIoU@5$\uparrow$ & ASSD@5$\downarrow$ & SPC$\downarrow$ \\
%       \noalign{\smallskip}
%       \hline
%       \noalign{\smallskip}
%       % f-BRS-C \cite{sofiiuk2020fbrs} & 4.29 & 0.887 & 0.736 & 3.883 & 0.030 & 7.69 & 0.843 & 0.733 & 9.717 & 0.048 \\
%       % f-BRS-B \cite{sofiiuk2020fbrs} & 4.13 & 0.875 & 0.730 & 4.626 & 0.034 & 7.41 & 0.826 & 0.717 & 11.267 & 0.056 \\
%       f-BRS-B \cite{sofiiuk2020fbrs} & 6 & 0.875 & 0.730 & 4.626 & 0.072 & 77 & 0.826 & 0.717 & 11.267 & 0.102 \\
%       % f-BRS-A \cite{sofiiuk2020fbrs} & 3.89 & 0.891 & 0.760 & 3.340 & 0.063 & 7.36 & 0.841 & 0.740 & 10.434 & 0.123 \\
%       % Dist-BRS \cite{sofiiuk2020fbrs} & 3.98 & 0.881 & 0.739 & 4.492 & 0.668 & 7.56 & 0.849 & 0.748 & 9.957 & 1.273 \\
%       % RGB-BRS \cite{sofiiuk2020fbrs} & 3.78 & 0.902 & 0.760 & 3.843 & 0.899 & 7.26 & 0.865 & 0.765 & 9.249 & 1.913 \\
%       FCA-Net \cite{lin2020interactive} & 7 & 0.923 & 0.793 & 2.190 & 0.059 & 74 & 0.867 & 0.771 & 9.048 & \underline{0.075} \\
%       CDNet \cite{lin2020interactive} & \underline{4} & 0.921 & 0.803 & 2.576 & 0.079 & \underline{60} & \underline{0.876} & \underline{0.783} & 8.973 & 0.108 \\
%       FocusCut \cite{lin2022focuscut} & \textbf{3} & \underline{0.933} & \underline{0.811} & \underline{2.050} & 15.801 & 61 & 0.873 & 0.778 & \underline{8.880} & 13.135 \\
%       \textbf{Baseline} & 6 & 0.904 & 0.782 & 4.370 & \textbf{0.052} & 75 & 0.867 & 0.770 & 9.421 & \textbf{0.069} \\ % epoch064
%       \textbf{Ours}     & \textbf{3} & \textbf{0.940} & \textbf{0.833} & \textbf{1.890} & \underline{0.057} & \textbf{59} & \textbf{0.893} & \textbf{0.815} & \textbf{8.226} & 0.087 \\ % PAMR 10 [1, 2] for DAIVS
%       \hline
%       RITM \cite{sofiiuk2021reviving}         & 2 & 0.956 & 0.871 & \underline{1.170} & 0.056 & \underline{52} & 0.888 & 0.809 & 7.917 & 0.054 \\
%       FocalClick-S1 \cite{chen2022focalclick} & 4 & 0.946 & 0.849 & 1.565 & 0.065 & 84 & 0.877 & 0.784 & 8.364 & 0.084 \\
%       FocalClick-S2 \cite{chen2022focalclick} & \underline{1} & \underline{0.958} & \underline{0.889} & \underline{1.170} & 0.052 & 54 & \underline{0.897} & \underline{0.824} & \underline{7.635} & 0.082 \\
%       \textbf{Baseline} & \underline{1} & 0.954 & 0.874 & 1.200 & \textbf{0.043} & 54 & 0.892 & 0.812 & 7.936 & \textbf{0.047} \\ % NoC@90% for DAVIS: 5.73
%       \textbf{Ours}     & \textbf{0} & \textbf{0.962} & \textbf{0.891} & \textbf{1.070} & \underline{0.046} & \textbf{51} & \textbf{0.905} & \textbf{0.833} & \textbf{7.002} & \underline{0.048} \\
%       % \hline
%       % \multirow{2}{*}{Ours} & ResNet-101 & \textbf{1.48} & \textbf{1.62} & \textbf{2.67} & \textbf{2.84} & \textbf{4.39} & \textbf{5.79} \\
%       %                       & HRNet-18   & \textbf{1.38} & \textbf{1.54} & \textbf{1.96} & \textbf{2.26} & \textbf{3.70} & \textbf{4.99} \\
%   \hline
%   \end{tabular}}
%   \end{center}
%   \caption{Comparisons of effectiveness and efficiency on the Berkeley and DAVIS datasets.
%   % $\dagger$ denotes our implementation.
%   The best and the second best results are written in bold and underlined, respectively.}
%   \label{tab:other_results}
% \end{table*}


\begin{table*}[t]
  \begin{center}
  \resizebox{0.99\linewidth}{!}{
  \begin{tabular}{c|l|c|c|c|c|r|c|c|c|c|r}
  % \begin{tabular}{clccccrccccr}
  % \hline\noalign{\smallskip}
  \toprule
  \multirow{2}{*}{B} & \multirow{2}{*}{Method} & \multicolumn{5}{c|}{Berkeley} & \multicolumn{5}{c}{DAVIS} \\
      \cline{3-12}
      & & NoF$_{20}$@90\%$\downarrow$ & IoU@5$\uparrow$ & BIoU@5$\uparrow$ & ASSD@5$\downarrow$ & SPC$\downarrow$ & NoF$_{20}$@90\%$\downarrow$ & IoU@5$\uparrow$ & BIoU@5$\uparrow$ & ASSD@5$\downarrow$ & SPC$\downarrow$ \\
      % \noalign{\smallskip}
      % \hline
      % \noalign{\smallskip}
      \midrule
      % f-BRS-C \cite{sofiiuk2020fbrs} & 4.29 & 0.887 & 0.736 & 3.883 & 0.030 & 7.69 & 0.843 & 0.733 & 9.717 & 0.048 \\
      % f-BRS-B \cite{sofiiuk2020fbrs} & 4.13 & 0.875 & 0.730 & 4.626 & 0.034 & 7.41 & 0.826 & 0.717 & 11.267 & 0.056 \\
      \multirow{6}*{\rotatebox{90}{ResNet-101}} & f-BRS-B \cite{sofiiuk2020fbrs} & 6 & 0.875 & 0.730 & 4.626 & 0.072 & 77 & 0.826 & 0.717 & 11.267 & 0.102 \\
      % f-BRS-A \cite{sofiiuk2020fbrs} & 3.89 & 0.891 & 0.760 & 3.340 & 0.063 & 7.36 & 0.841 & 0.740 & 10.434 & 0.123 \\
      % Dist-BRS \cite{sofiiuk2020fbrs} & 3.98 & 0.881 & 0.739 & 4.492 & 0.668 & 7.56 & 0.849 & 0.748 & 9.957 & 1.273 \\
      % RGB-BRS \cite{sofiiuk2020fbrs} & 3.78 & 0.902 & 0.760 & 3.843 & 0.899 & 7.26 & 0.865 & 0.765 & 9.249 & 1.913 \\
      & FCA-Net \cite{lin2020interactive} & 7 & 0.923 & 0.793 & 2.190 & 0.059 & 74 & 0.867 & 0.771 & 9.048 & \underline{0.075} \\
      & CDNet \cite{lin2020interactive} & \underline{4} & 0.921 & 0.803 & 2.576 & 0.079 & \underline{60} & \underline{0.876} & \underline{0.783} & 8.973 & 0.108 \\
      & FocusCut \cite{lin2022focuscut} & \textbf{3} & \underline{0.933} & \underline{0.811} & \underline{2.050} & 3.152 & 61 & 0.873 & 0.778 & \underline{8.880} & 3.872 \\
      & \textbf{Baseline} & 6 & 0.904 & 0.782 & 4.370 & \textbf{0.052} & 75 & 0.867 & 0.770 & 9.421 & \textbf{0.069} \\ % epoch064
      & \textbf{Ours}     & \textbf{3} & \textbf{0.943} & \textbf{0.838} & \textbf{1.789} & \underline{0.057} & \textbf{59} & \textbf{0.893} & \textbf{0.815} & \textbf{8.226} & 0.082 \\ % PAMR 10 [1, 2] for DAIVS
      \hline
      % \multirow{5}*{\rotatebox{90}{HRNet-18s}} & RITM \cite{sofiiuk2021reviving}         & 2 & 0.956 & 0.871 & \underline{1.170} & 0.056 & \underline{52} & 0.888 & 0.809 & 7.917 & 0.054 \\ % hrnet18
      \multirow{4}*{\rotatebox{90}{HRNet-18s}} & RITM \cite{sofiiuk2021reviving} \dag & \underline{1} & 0.950 & 0.859 & 1.312 & \textbf{0.035} & \underline{53} & 0.883 & 0.801 & 8.378 & \textbf{0.036} \\
      & FocalClick-S1 \cite{chen2022focalclick} & 4 & 0.946 & 0.849 & 1.565 & 0.065 & 84 & 0.877 & 0.784 & 8.364 & 0.084 \\
      & FocalClick-S2 \cite{chen2022focalclick} & \underline{1} & \underline{0.957} & \textbf{0.889} & \underline{1.170} & 0.052 & 54 & \underline{0.897} & \underline{0.824} & \underline{7.635} & 0.082 \\
      % & \textbf{Baseline} & \underline{1} & 0.954 & 0.874 & 1.200 & \textbf{0.043} & 54 & 0.892 & 0.812 & 7.936 & \textbf{0.047} \\ % NoC@90% for DAVIS: 5.73
      % & \textbf{Ours}     & \textbf{0} & \textbf{0.962} & \textbf{0.891} & \textbf{1.070} & \underline{0.046} & \textbf{51} & \textbf{0.905} & \textbf{0.833} & \textbf{7.002} & \underline{0.048} \\
      & \textbf{Ours}     & \textbf{0} & \textbf{0.958} & \underline{0.883} & \textbf{1.012} & \underline{0.044} & \textbf{51} & \textbf{0.907} & \textbf{0.830} & \textbf{6.434} & \underline{0.048} \\
      % hrnet18s_cocolvis_resfeedbackfusion5_4_3_2_expanratio0.3_000 epoch190
      % Berkeley 640 2.7 DAVIS 720 2.2
      % \hline
      % \multirow{2}{*}{Ours} & ResNet-101 & \textbf{1.48} & \textbf{1.62} & \textbf{2.67} & \textbf{2.84} & \textbf{4.39} & \textbf{5.79} \\
      %                       & HRNet-18   & \textbf{1.38} & \textbf{1.54} & \textbf{1.96} & \textbf{2.26} & \textbf{3.70} & \textbf{4.99} \\
  \bottomrule
  \end{tabular}}
  \end{center}
  \vspace{-0.24cm}
  \caption{Comparisons of effectiveness and efficiency on the Berkeley and DAVIS datasets. ``B'' in the table header denotes the term ``backbone''. \dag RITM is the baseline of our method.}
  % $\dagger$ denotes our implementation.
  \label{tab:other_results}
\end{table*}


\begin{figure*}[t]
  \centering
  \includegraphics[width=2.0\columnwidth]{sources_final/pdf/results.pdf}
  % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
  \caption{Qualitative comparisons of FCA-Net \cite{lin2020interactive}, CDNet \cite{chen2021conditional}, FocusCut \cite{lin2022focuscut}, our baseline, and our method.
  }
  \label{fig:results}
\end{figure*}


\section{Experiments}
\subsection{Experimental Settings}

\textbf{Segmentation backbones.}
We conducted experiments using
DeepLabV3+ \cite{chen2018encoder} and HRNet+OCR \cite{sun2019high,yuan2020object}
as the baseline network, respectively.
The ResNet-101 \cite{he2016deep}, HRNet-18s \cite{sun2019high}, and HRNet-18 \cite{sun2019high} were employed as backbones.
Improvements introduced by different network baselines are not the focus of this paper.
Therefore, we mainly report experimental results achieved by DeepLabV3+
and provide essential results achieved by HRNet+OCR.
The CFFM was inserted after
% the atrous spatial pyramid pooling (ASPP) \cite{chen2017deeplab} and
the upsampling operator for DeepLabV3+
and the HRNet for HRNet+OCR.
% DeepLabV3+, with a GDFF module embedded after the atrous spatial pyramid pooling,
% Both the ResNet-101 backbone of DeepLabV3+ and the HRNet-18 backbone of HRNet+OCR
Both the ResNet-101 backbone and the HRNet-18 backbone
were pre-trained on the ImageNet dataset \cite{deng2009imagenet}.

\textbf{Datasets.}
The DeepLabV3+ was trained on the training set of SBD \cite{hariharan2011semantic},
and the HRNet+OCR was trained on the combination of COCO \cite{lin2014microsoft} and LVIS \cite{gupta2019lvis}.
We evaluated our method on four benchmarks:
GrabCut \cite{rother2004grabcut}, Berkeley \cite{martin2001database}, SBD,
and DAVIS \cite{perazzi2016benchmark}.
SBD contains 8,498 images for training,
and its validation set contains 2,820 images including 6,671 instance-level masks.
COCO+LVIS contains 104k images with 1.6M instance-level masks.
GrabCut contains 50 images with an instance mask for each image.
Berkeley consists of 96 images and 100 instance masks.
For DAVIS, we used the same 345 sampled frames
as previous methods \cite{sofiiuk2020fbrs,lin2020interactive,sofiiuk2021reviving,chen2022focalclick,lin2022focuscut}
for evaluation.
% Those frames wer sampled from the dataset.
% The Pascal VOC validation set contains 1,449 images, with 3,417 instances.
% SBD con 6,671 instance-level masks for 2,820 images.


\textbf{Implementation details.}
% We resized the smaller dimension of an input image to $512$ while maintaining the aspect ratio of the image
% and then randomly cropped the image to $512\times 512$.
During training,
we resized input images with a random scale factor between 0.75 and 1.40
and randomly cropped them to $320\times 480$.
A horizontal flip and random jittering of brightness, contrast, and RGB values were also applied.
We utilized an Adam optimizer with $\beta_1=0.9$ and $\beta_2=0.999$ to train the network for 60 epochs.
The learning rate for the backbone was initialized to $5\times10^{-5}$ and multiplied by 0.1 on the 50th epoch.
The learning rate for other parts of the segmentation network was 10 times larger than that for the backbone.
The batch size was set to 24.
Following RITM, we used the iterative sampling strategy to simulate user interaction.
For each batch, new annotated clicks were iteratively sampled for 1 to 4 iterations.
% i.e., annotating based on the output of the last iteration.
In each iteration, up to 24 annotated clicks were randomly sampled
from the eroded mislabelled regions of the last prediction.

% l2-norm definition: https://www.math.usm.edu/lambers/mat610/sum10/lecture2.pdf
During inference, only one click was added in each interaction.
% We sampled new annotated clicks from false positive regions and false negative regions.
The new click was sampled from misclassified regions
and was the farthest from the region boundaries.
Mathematically, the distance between two points $\bm{p}$ and $\bm{q}$ is defined as
$d(\bm{p},\bm{q})=||\bm{p}-\bm{q}||_2$,
and the distance between a point $\bm{p}$ and a point set $\mathcal P$ is defined as
$d(\bm{p},\mathcal{P})=\text{min}_{\bm{q}\in \mathcal{P}}d(\bm{p},\bm{q})$.
The set of false positive points and false negative points is defined as
$\mathcal{P}_f=\{\bm{p}|\bm{M}_t(\bm{p})=0,\bm{M}_{gt}(\bm{p})=1\}
\cup \{\bm{p}|\bm{M}_t(\bm{p})=1,\bm{M}_{gt}(\bm{p})=0\}$.
% The false positive point set and the false negative point set are defined as
% $\mathcal{P}_{fp}=\{\bm{p}|\bm{M}_t(\bm{p})=1,\bm{M}_{gt}(\bm{p})=0\}$ and 
% $\mathcal{P}_{fn}=\{\bm{p}|\bm{M}_t(\bm{p})=0,\bm{M}_{gt}(\bm{p})=1\}$.
% \begin{equation}
%   \mathcal{P}_{fp}=\{\bm{p}|\bm{M}_t(\bm{p})=1,\bm{M}_{gt}(\bm{p})=0\},
% \end{equation}
% \begin{equation}
%   \mathcal{P}_{fn}=\{\bm{p}|\bm{M}_t(\bm{p})=0,\bm{M}_{gt}(\bm{p})=1\}.
% \end{equation}
% For each pixel in $\mathcal{P}_{fp}$ and $\mathcal{P}_{fn}$,
% the distance from the pixel to the boundaries of its corresponding region is defined as
% $\eta (\bm{p})=d(\bm{p},\mathcal{P}^C_x),
% \forall\bm{p}\in{\mathcal{P}_x},\mathcal{P}_x\in\{\mathcal{P}_{fp},\mathcal{P}_{fn}\}$,
% For a pixel $p\in{\mathcal{P}_x}$, where $\mathcal{P}_x\in\{\mathcal{P}_{fp},\mathcal{P}_{fn}\}$,
% the distance from the pixel to the boundaries of the largest connected component in which it is located is defined as
% $\eta (\bm{p})=d(\bm{p},\mathcal{P}^C_x)$,
% where $\mathcal{P}^C_x$ denotes the complement of $\mathcal{P}_x$.
For a pixel $p\in{\mathcal{P}_f}$,
the largest connected component in which it is located is denoted as $\mathcal{P}_c(\bm{p})$.
The distance from $\bm{p}$ to the boundaries of $\mathcal{P}_c(\bm{p})$ is defined as
$\eta (\bm{p})=d(\bm{p},\mathcal{P}^C_c(\bm{p}))$,
where $\mathcal{P}^C_c(\bm{p})$ denotes the complement of $\mathcal{P}_c(\bm{p})$.
% \begin{equation}
%   % \begin{aligned}
%   \eta (\bm{p})=\text{min}d(\bm{p},\mathcal{P}^C_x),
%   \forall\bm{p}\in{\mathcal{P}_x},\mathcal{P}_x\in\{\mathcal{P}_{fp},\mathcal{P}_{fn}\},
%   % \end{aligned}
% \end{equation}
Following the standard protocol \cite{jang2019interactive,sofiiuk2020fbrs,lin2020interactive,sofiiuk2021reviving,lin2022focuscut,chen2022focalclick},
% we calculated the largest connected component of 
the new annotated pixel was selected by
$\bm{p}^*=\{\bm{p}|\text{max}_{\bm{p}\in\mathcal{P}_f}\eta (\bm{p})\}$.
% \begin{equation}
%   \bm{p}^*=\text{max}_{\forall\bm{p}\in\{\mathcal{P}_{fp},\mathcal{P}_{fn}\}}\eta (\bm{p}).
% \end{equation}
The maximum number of clicks was limited to 20 for each sample in all experiments.
Besides, following previous methods \cite{sofiiuk2020fbrs,chen2021conditional,sofiiuk2021reviving,chen2022focalclick},
we adopted test time augmentations,
i.e., the Zoom-In strategy and averaging the predictions of the original image and the horizontally flipped image.

The proposed method was implemented in PyTorch \cite{paszke2019pytorch}.
All the experiments were conducted on a computer with an Intel Xeon Gold 6326 2.90 GHz CPU and NVIDIA GeForce RTX 3090 GPUs.


\textbf{Evaluation metrics.}
The proposed method was evaluated using the following six metrics:
1) NoC@$\alpha$:
the mean number of clicks (NoC) required to reach a predefined intersection over union (IoU) threshold $\alpha$ for all images;
2) IoU@$N$: the mean IoU achieved by a particular NoC $N$;
3) BIoU@$N$: the mean boundary IoU achieved by a particular NoC $N$;
4) ASSD@$N$: the average symmetric surface distance with a particular NoC $N$,
which was used to evaluate the boundary quality of the prediction;
5) NoF$_N$@$\alpha$: the number of failures (NoF) that cannot reach a target IoU $\alpha$ with a certain NoC $N$;
6) SPC: the average running time in seconds per click.


\begin{figure}[t]
  \captionsetup[subfigure]{labelformat=empty,font=small,justification=centering}
  \begin{subfigure}[t]{.5\linewidth}
      \centering
      \includegraphics[width=0.97\columnwidth]{sources_final/pdf/line_charts/GrabCut.pdf}
      % \caption{(a) ROI localization\\ \cite{lai2020mast}}
      \label{fig:chart_grabcut}
  \end{subfigure}%
  \begin{subfigure}[t]{.5\linewidth}
      \centering
      \includegraphics[width=0.97\columnwidth]{sources_final/pdf/line_charts/Berkeley.pdf}
      % \caption{(b) Temporal correspondence alignment}
      \label{fig:chart_berkeley}
  \end{subfigure}%
  \vspace{.1in}
  \begin{subfigure}[t]{.5\linewidth}
      \centering
      \includegraphics[width=0.97\columnwidth]{sources_final/pdf/line_charts/SBD.pdf}
      % \caption{(a) ROI localization\\ \cite{lai2020mast}}
      \label{fig:chart_sbd}
  \end{subfigure}%
  \begin{subfigure}[t]{.5\linewidth}
      \centering
      \includegraphics[width=0.97\columnwidth]{sources_final/pdf/line_charts/DAVIS.pdf}
      % \caption{(b) Temporal correspondence alignment}
      \label{fig:chart_davis}
  \end{subfigure}%
  \caption{Comparisons of the mIoU-NoC curves on four datasets.}
  % The results are achieved by DeepLabV3+.}
  \label{fig:chart}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=0.99\columnwidth]{sources_final/pdf/click_disturbance.pdf}
  % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
  \caption{Results obtained by different annotations.}
  \label{fig:click_disturbance}
\end{figure}

\subsection{Comparison with previous work}
\subsubsection{Effectiveness Analysis.}
% We compared our method against previous research following the standard protocol.
% Quantitative results are reported in Tab. \ref{tab:results}.
We have tabulated the quantitative results in Tab. \ref{tab:results} and Tab. \ref{tab:other_results}.
The quantitative results demonstrate that our method can generalize across various datasets and different backbones.
% As shown in Tab. \ref{tab:results},
The number of clicks of our method required to reach 85\% and 90\% IoU are much lower than previous methods,
and our method also outperformed previous work in boundary quality.
The results indicate that our method can achieve satisfactory segmentation results with less user effort.
% Our method also outperformed previous work in boundaries according to Tab. \ref{tab:other_results}.
% These quantitative results demonstrate that our method can generalize to different backbones and new datasets.
% HRNet+OCR far exceeded DeepLabV3+ in
% not only performance but also quantity of parameters (see Tab. \ref{tab:ablation}),
% which was explored in \cite{sofiiuk2021reviving}.
The line charts of mIoU-NoC on four datasets are plotted in Fig. \ref{fig:chart}.
Those results indicate that our method achieved acceptable results given only a few clicks,
steadily improved segmentation results with additional clicks,
and ultimately converged to better results.
% Line charts of mIoU-NoC, plotted in Fig. \ref{fig:chart}, show the change of mNoC as the number of clicks increase.
% As can be seen, our method achieved acceptable results given only a few clicks
% and steadily improved performance with more clicks.
Fig. \ref{fig:results} visualizes the qualitative results of some previous work and ours.
Compared with other methods,
our method required fewer clicks to obtain relatively complete segments and fine boundary details.
Additionally, it could handle challenging scenarios,
including color interference (like the goat and the cat),
complex backgrounds (as depicted in the break-dancer  photo), and occlusions (as seen from the racehorse rider).
Please refer to the supplementary material for more visualization examples.
Fig. \ref{fig:click_disturbance} exhibits the sensitivity of our method to click locations.
% Given reasonable annotations,
% our method achieves approximate performance for different annotation positions.
Our method attained approximate performance for different annotation positions when provided with reasonable annotations.

\subsubsection{Efficiency Analysis.}
% We evaluated the SPC metric on an NVIDIA GeForce RTX 3090 GPU.
% We evaluated the SPC metric using a computer with an Intel Xeon Gold 6326 2.90 GHz CPU and an NVIDIA GeForce RTX 3090 GPU.
Tab. \ref{tab:other_results} presents the average inference speed of different methods,
among which our method achieved a desirable trade-off between speed and accuracy.
% Compared with other methods,
% our method reached a trade-off between speed and accuracy.
% Compared with the baseline,
% the proposed modules have a low computational budget,
% which is less than 13 ms for all modules.
Notably, our proposed modules exhibited a low computational budget,
requiring less than 13 ms for all modules,
compared to the baseline.
% The running time the FCA-Net is less than other methods
% because it we averaged predictions of the input image and the horizontally flipped image as done in RITM,
% which means feedforward propagation was performed twice for each sample.
% Furthermore, the number of parameters in our framework is less than the that of the FCA-Net.
In summary, the proposed framework achieved competitive results with relatively low computation costs.


\begin{table}[t]
  \begin{center}
  \resizebox{0.99\linewidth}{!}{
  \begin{tabular}{l|l|c|c|c|c}
  % \hline\noalign{\smallskip}
  \toprule
  \multirow{3}{*}{Method} & \multirow{3}{*}{Backbone} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
  \cline{3-6}
  && NoC    & NoF$_{20}$ & NoC    & NoF$_{20}$ \\
  && @90\%$\downarrow$ & @90\%$\downarrow$ & @90\%$\downarrow$ & @90\%$\downarrow$ \\
  % \noalign{\smallskip}
  % \hline
  % \noalign{\smallskip}
  \midrule
  Baseline        & ResNet-101 &  4.31 & 6 & 7.56 & 75 \\ %
  + FFC           & ResNet-101 &  3.75 & 4 & 6.75 & 65 \\ % 720 2.7
  % + CFF           & ResNet-101 &  3.07 & \textbf{1} & 6.50 & 61 \\ % epoch70 Berkeley DAVIS 720 2.9
  + CFF           & ResNet-101 &  3.07 & \textbf{1} & 6.62 & 66 \\ % epoch70 Berkeley DAVIS 720 2.6
  % + FFC + CFF     & ResNet-101 &  \textbf{2.85} & 3 & \textbf{5.82} & \textbf{59} \\
  + FFC + CFF     & ResNet-101 &  \textbf{2.84} & 3 & \textbf{6.48} & \textbf{59} \\
  \hline
  Baseline    & HRNet-18 &  2.60 & 1 & 5.73 & 54 \\
  + FFC       & HRNet-18 &  2.11 & 1 & 5.52 & 54 \\
  + CFF       & HRNet-18 &  2.05 & \textbf{0} & 5.32 & 52 \\
  + FFC + CFF & HRNet-18 &  \textbf{1.96} & \textbf{0} & \textbf{5.16} & \textbf{51} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.24cm}
\caption{An ablation study for the core components.}
\label{tab:ablation}
\end{table}


% \begin{table}[t]
%   \begin{center}
%   \resizebox{0.99\linewidth}{!}{
%   \begin{tabular}{c|l|c|c|c|c}
%   \hline\noalign{\smallskip}
%   \multirow{3}{*}{\#} & \multirow{3}{*}{Method} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
%   \cline{3-6}
%   && mNoC    & NoF$_{20}$ & mNoC    & NoF$_{20}$ \\
%   && @90\%$\downarrow$ & @90\%$\downarrow$ & @90\%$\downarrow$ & @90\%$\downarrow$ \\
%   \noalign{\smallskip}
%   \hline
%   \noalign{\smallskip}
%   \multirow{4}*{\rotatebox{90}{ResNet-101}} & Baseline        & 4.31 & 6 & 7.56 & 75 \\ %
%   & + FFC           & 3.75 & 4 & 6.75 & 65 \\ % 720 2.7
%   % & + FFC           & 3.21 & 3 & 6.10 & 56 \\ % 720 2.7
%   % & + CFF           & 3.07 & \textbf{1} & 6.50 & 61 \\ % 720 2.9
%   & + CFF           & 3.07 & \textbf{1} & 6.10 & 56 \\ % 720 2.9 10 1
%   & + FFC + CFF     & \textbf{2.85} & 3 & \textbf{5.82} & \textbf{59} \\
%   \hline
%   \multirow{4}*{\rotatebox{90}{HRNet-18}} & Baseline    & 2.60 & 1 & 5.73 & 54 \\
%   & + CFF       & 2.11 & 1 & 5.52 & 54 \\
%   & + FFC       & 2.05 & \textbf{0} & 5.32 & 52 \\
%   & + FFC + CFF & \textbf{1.96} & \textbf{0} & \textbf{5.16} & \textbf{51} \\
% \hline
% \end{tabular}}
% \end{center}
% \caption{An ablation study for the core components.}
% \label{tab:ablation}
% \end{table}


\begin{figure*}[t]
  \centering
  \includegraphics[width=2.0\columnwidth]{sources_final/pdf/ablation_results.pdf}
  % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
  \caption{Visualization of the feedback, the feature affinity, the refined feedback, the updated feedback, and the output.
  The feature affinity is shown in a heatmap,
  where red color denotes high feature affinity, and blue color denotes low feature affinity.
  }
  \label{fig:ablation_restuls}
\end{figure*}

% \begin{figure*}[t]
%   \begin{minipage}{1.23\columnwidth}
%       \centering
%       \includegraphics[width=0.99\columnwidth]{sources_final/pdf/ablation_results_save_space.pdf}
%       % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
%       \caption{Visualization of the feedback, the feature affinity, the refined feedback, the updated feedback, and the output.
%       The feature affinity is shown in a heatmap,
%       where red color denotes high feature affinity, and blue color denotes low feature affinity.
%       }
%       \label{fig:ablation_restuls}
%   \end{minipage}
%   \hfill
%   \begin{minipage}{0.82\columnwidth}
%       \captionsetup[subfigure]{labelformat=empty,font=small,justification=centering}
%       \begin{subfigure}[t]{.5\linewidth}
%           \centering
%           \includegraphics[width=0.96\columnwidth]{sources_final/pdf/GrabCut.pdf}
%           % \caption{(a) ROI localization\\ \cite{lai2020mast}}
%           \label{fig:chart_grabcut}
%       \end{subfigure}%
%       \begin{subfigure}[t]{.5\linewidth}
%           \centering
%           \includegraphics[width=0.96\columnwidth]{sources_final/pdf/Berkeley.pdf}
%           % \caption{(b) Temporal correspondence alignment}
%           \label{fig:chart_berkeley}
%       \end{subfigure}%
%       \vspace{.05in}
%       \begin{subfigure}[t]{.5\linewidth}
%           \centering
%           \includegraphics[width=0.96\columnwidth]{sources_final/pdf/SBD.pdf}
%           % \caption{(a) ROI localization\\ \cite{lai2020mast}}
%           \label{fig:chart_sbd}
%       \end{subfigure}%
%       \begin{subfigure}[t]{.5\linewidth}
%           \centering
%           \includegraphics[width=0.96\columnwidth]{sources_final/pdf/DAVIS.pdf}
%           % \caption{(b) Temporal correspondence alignment}
%           \label{fig:chart_davis}
%       \end{subfigure}%
%       \caption{MIoU-NoC curves on four datasets.}
%       % The results are achieved by DeepLabV3+.}
%       \label{fig:chart}
%       \centering
%       \includegraphics[width=0.99\columnwidth]{sources_final/pdf/click disturbance.pdf}
%       % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
%       \caption{Results obtained by different annotations.}
%       \label{fig:click_disturbance}
%   \end{minipage}
% \end{figure*}

\begin{table*}[t]
  %   \begin{minipage}{0.34\linewidth}
  %     \begin{center}
  %       \resizebox{0.99\linewidth}{!}{
  %       \begin{tabular}{l|c|c|c|c}
  %         \hline\noalign{\smallskip}
  %         \multirow{2}{*}{\shortstack{Similarity\\Measurement}} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
  %         \cline{2-5}
  %         & @85\% & @90\% & @85\% & @90\% \\
  %         \noalign{\smallskip}
  %         \hline
  %         \noalign{\smallskip}
  %         Exponential & \multirow{2}{*}{2.06} & \multirow{2}{*}{3.60} & \multirow{2}{*}{5.26} & \multirow{2}{*}{7.51} \\
  %         Similarity \\
  %         \textbf{Cosine} & \multirow{2}{*}{\textbf{1.88}} & \multirow{2}{*}{\textbf{2.85}} & \multirow{2}{*}{\textbf{4.60}} & \multirow{2}{*}{\textbf{5.82}} \\
  %         \textbf{similarity} \\
  %         \hline
  %       \end{tabular}}
  %   \end{center}
  %   \caption{\textbf{Similarity measurement}.
  %   % L2 similarity is defined as $\bm{W}(i,j)=-||\bm{F}(i,j)-\bm{F}(x_{new},y_{new})||^2_2$.
  %   Exponential similarity is defined as $\bm{W}(i,j)=e^{-||\bm{F}(i,j)-\bm{F}(x_{new},y_{new})||^2/\sigma}$.}
  %   \label{tab:similarity_measurement_for_ffcm}
  % \end{minipage}
  \begin{minipage}{0.3\linewidth}
    \begin{center}
      \resizebox{0.99\linewidth}{!}{
      \begin{tabular}{l|c|c|c|c}
        % \hline\noalign{\smallskip}
        \toprule
        \multirow{2}{*}{\shortstack{Similarity\\Measurement}} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
        \cline{2-5}
        & @85\% & @90\% & @85\% & @90\% \\
        % \noalign{\smallskip}
        % \hline
        % \noalign{\smallskip}
        \midrule
        Exponential & 2.06 & 3.60 & 5.26 & 7.51 \\
        \textbf{Cosine} & \textbf{1.88} & \textbf{2.85} & \textbf{4.60} & \textbf{5.82} \\
        % \hline
        \bottomrule
      \end{tabular}}
  \end{center}
  \vspace{-0.24cm}
  \caption{The mean NoC with respect to the similarity measurement.
  % L2 similarity is defined as $\bm{W}(i,j)=-||\bm{F}(i,j)-\bm{F}(x_{new},y_{new})||^2_2$.
  The exponential similarity is defined as $\bm{W}(i,j)=e^{-||\bm{F}(i,j)-\bm{F}(x_{new},y_{new})||^2/\sigma}$.}
  \label{tab:similarity_measurement_for_ffcm}
\end{minipage}
  \hfill
  \begin{minipage}{0.285\linewidth}
    \begin{center}
      \resizebox{0.99\linewidth}{!}{
      \begin{tabular}{c|c|c|c|c}
        % \hline\noalign{\smallskip}
        \toprule
        \multirow{2}{*}{\shortstack{Expansion\\Ratio $r$}} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
        \cline{2-5}
        & @85\% & @90\% & @85\% & @90\% \\
        % \noalign{\smallskip}
        % \hline
        % \noalign{\smallskip}
        \midrule
        0.1      & 1.91 & 3.43 & 5.52 & 7.46 \\ % last_checkpoint, 600, 2.5 for Berkeley
        0.2      & 1.97 & 3.10 & 5.12 & 6.82 \\
        \textbf{0.3} & 1.88 & \textbf{2.85} & \textbf{4.60} & \textbf{5.82} \\
        0.4      & \textbf{1.79} & 3.12 & 4.74 & 6.42 \\
        0.5      & 1.84 & 3.27 & 5.15 & 6.72  \\
        \bottomrule
      \end{tabular}}
  \end{center}
  \vspace{-0.24cm}
  \caption{The mean NoC with respect to the expansion ratio $r$.}
  \label{tab:expansion_ratio}
  \end{minipage}
  \hfill
  \begin{minipage}{0.393\linewidth}
    \begin{center}
      \resizebox{0.99\linewidth}{!}{
      \begin{tabular}{l|c|c|c|c}
        % \hline\noalign{\smallskip}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
        \cline{2-5}
        & @85\% & @90\% & @85\% & @90\% \\
        % \noalign{\smallskip}
        % \hline
        % \noalign{\smallskip}
        \midrule
        % BRS$^\dag$           & ResNet-101 & 4.02 & 5 & 7.55 & 73 & 1.765 & 223 \\
        % f-BRS-A$^\dag$       &            & 3.87 & 6 & 7.37 & 77 & 0.280 & 223 \\
        % f-BRS-B$^\dag$       &            & 4.08 & 6 & 7.34 & 76 & 0.138 & 223 \\
        % f-BRS-C$^\dag$       &            & 4.29 & 9 & 7.68 & 90 & 0.118 & 223 \\
        % FCA-Net              &            & 4.19 & 7 & 7.90 & 74 & 0.031 & 280 \\
        w/o feedback                 & 2.31 & 4.31 & 5.23 & 7.56 \\
        % \hline
        Concat feedback with input   & 2.01 & 4.00 & 5.03 & 7.08  \\ % r101_dh256_000, epoch 054 = r101_dh256_001, epoch 061, 720, 2.5 for DAVIS
        % Baseline+E+F(w/o G) &            & 4.34 & 8 & 7.88 & 74 & 0.026 & 238 \\
        CFFM w/o residual connection & 1.98 & 3.52 & 4.91 & 6.63 \\
        CFFM w/o gate                & 1.96 & 3.36 & 4.85 & 6.37  \\
        \textbf{CFFM}                & \textbf{1.81} & \textbf{3.07} & \textbf{4.75} & \textbf{6.10}  \\
        \bottomrule
      \end{tabular}}
    \end{center}
    \vspace{-0.24cm}
    \caption{The mean NoC with respect to different feedback fusion architectures.}
    % ``CI'' denotes directly concatenating feedback with the initial input.
    % ``RC'' denotes residual connection.}
    \label{tab:architecture_for_cffm}
  \end{minipage}
\end{table*}


% \begin{table}
%   \begin{center}
%     \resizebox{0.99\linewidth}{!}{
%     \begin{tabular}{l|c|c|c|c}
%       \hline\noalign{\smallskip}
%       \multirow{2}{*}{\shortstack{Similarity\\Measurement}} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
%       \cline{2-5}
%       & @85\% & @90\% & @85\% & @90\% \\
%       \noalign{\smallskip}
%       \hline
%       \noalign{\smallskip}
%       Exponential & 2.06 & 3.60 & 5.26 & 7.51 \\
%       \textbf{Cosine} & \textbf{1.88} & \textbf{2.85} & \textbf{4.60} & \textbf{5.82} \\
%       \hline
%     \end{tabular}}
%   \end{center}
%   \caption{The mNoC with respect to the similarity measurement.
%   % L2 similarity is defined as $\bm{W}(i,j)=-||\bm{F}(i,j)-\bm{F}(x_{new},y_{new})||^2_2$.
%   The exponential similarity is defined as $\bm{W}(i,j)=e^{-||\bm{F}(i,j)-\bm{F}(x_{new},y_{new})||^2/\sigma}$.}
%   \label{tab:similarity_measurement_for_ffcm}
% \end{table}

% \begin{table*}
%   \begin{minipage}{0.5\linewidth}
%     \begin{center}
%       % \resizebox{0.65\linewidth}{!}{
%       \setlength{\tabcolsep}{1mm}{
%       \begin{tabular}{c|c|c|c|c}
%         \hline\noalign{\smallskip}
%         \multirow{2}{*}{\shortstack{Expansion\\ratio $r$}} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
%         \cline{2-5}
%         & @85\% & @90\% & @85\% & @90\% \\
%         \noalign{\smallskip}
%         \hline
%         \noalign{\smallskip}
%         0.1      & 1.91 & 3.43 & 5.52 & 7.46 \\ % last_checkpoint, 600, 2.5 for Berkeley
%         0.2      & 1.97 & 3.10 & 5.12 & 6.82 \\
%         \textbf{0.3} & 1.88 & \textbf{2.85} & \textbf{4.60} & \textbf{5.82} \\
%         0.4      & \textbf{1.79} & 3.12 & 4.74 & 6.42 \\
%         0.5      & 1.84 & 3.27 & 5.15 & 6.72  \\
%         \hline
%       \end{tabular}}
%   \end{center}
%   \caption{The mNoC with respect to the expansion ratio $r$.}
%   \label{tab:expansion_ratio}
%   \end{minipage}
%   \hfill
%   \begin{minipage}{0.48\linewidth}
%     \begin{center}
%       \resizebox{1.0\linewidth}{!}{
%       \begin{tabular}{l|c|c|c|c}
%         \hline\noalign{\smallskip}
%         \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Berkeley} & \multicolumn{2}{c}{DAVIS} \\
%         \cline{2-5}
%         & @85\% & @90\% & @85\% & @90\% \\
%         \noalign{\smallskip}
%         \hline
%         \noalign{\smallskip}
%         % BRS$^\dag$           & ResNet-101 & 4.02 & 5 & 7.55 & 73 & 1.765 & 223 \\
%         % f-BRS-A$^\dag$       &            & 3.87 & 6 & 7.37 & 77 & 0.280 & 223 \\
%         % f-BRS-B$^\dag$       &            & 4.08 & 6 & 7.34 & 76 & 0.138 & 223 \\
%         % f-BRS-C$^\dag$       &            & 4.29 & 9 & 7.68 & 90 & 0.118 & 223 \\
%         % FCA-Net              &            & 4.19 & 7 & 7.90 & 74 & 0.031 & 280 \\
%         w/o feedback                 & 2.31 & 4.31 & 5.23 & 7.56 \\
%         \hline
%         Concat feedback with input   & 2.01 & 4.00 & 5.03 & 7.08  \\ % r101_dh256_000, epoch 054 = r101_dh256_001, epoch 061, 720, 2.5 for DAVIS
%         % Baseline+E+F(w/o G) &            & 4.34 & 8 & 7.88 & 74 & 0.026 & 238 \\
%         CFFM w/o residual connection & 1.98 & 3.52 & 4.91 & 6.63 \\
%         CFFM w/o gate                & 1.96 & 3.36 & 4.85 & 6.37  \\
%         \textbf{CFFM}                & \textbf{1.81} & \textbf{3.07} & \textbf{4.75} & \textbf{6.10}  \\
%         \hline
%       \end{tabular}}
%     \end{center}
%     \caption{The mNoC with respect to different feedback fusion architectures.}
%     % ``CI'' denotes directly concatenating feedback with the initial input.
%     % ``RC'' denotes residual connection.}
%     \label{tab:architecture_for_cffm}
%   \end{minipage}
% \end{table*}

\subsection{Ablation Studies}
\label{sec:ablation}
We evaluated the efficacy of each proposed component.
Berkeley and DAVIS were chosen as the main evaluation datasets
because they cover challenging scenarios, such as unseen categories, motion blur, and occlusions.
Besides, they have better annotation quality than SBD.

Tab. \ref{tab:ablation} tabulates the quantitative ablation results.
To integrate the feedback into the network,
the corrected feedback $\bm{M}^c_{t-1}$ was concatenated with the features $\bm{F}$
in the ``+ FFC'' variant.
When only one module was embedded,
both the FFCM and the CFFM improved the results.
The CFFM boosted the performance more;
this proves effectiveness of deep feedback integration.
The FFCM has also improved the results.
The reason is that the CFFM relies on the quality of the feedback,
and the FFCM provides refined feedback for it.
With the two modules working synergistically,
our method significantly reduced the NoC and NoF.
% Fig. \ref{fig:results} provides the qualitative results on the Berkeley and DAVIS datasets.
% The effect of different componnents are visulaized in Fig. \ref{fig:results}.
% Our method yielded the best results when the three components worked synergistically.

Qualitative results for the FFCM and the CFFM are shown in Fig. \ref{fig:ablation_restuls}.
% From the second interaction,
% the feedback provided sufficient prior segmentation information for the segmentation model.
The FFCM utilizes the feature affinity to refine the feedback from a local perspective.
For instance, in the 14th round of interaction,
the FFCM yielded finer segmentation boundaries on the front fender of the motorcycle.
The CFFM refines the feedback in a global view
and updates the deep features to improve the overall segmentation results,
e.g., the boy wearing a hat and the bull.


\textbf{Analysis for the FFCM.}
In Tab. \ref{tab:similarity_measurement_for_ffcm},
we analyzed the effect of different similarity measurements,
e.g. exponential similarity and cosine similarity.
Using cosine similarity to measure the feature affinity achieved better results.
We also verified the robustness of the FFCM with respect to the expansion ratio $r$.
As illustrated in Tab. \ref{tab:expansion_ratio},
% the performance fluctuations are negligible
% when the expansion ratio varies.
an expansion ratio of 0.3 yielded the best results.
% FE achieved a more accurate segmentation result with a smaller misclassified area around the mislabeled pixel,
% demonstrating strong error-tolerant ability.

\textbf{Analysis for the CFFM.}
% As Fig. \ref{fig:results} shows, both FE and the FCA-Net failed to segment the head of a tiger
% % in the first interaction
% % because the head is brighter than the body.
% because of uneven illumination.
% After embedding GDFF, the segmentation model was less reliant on low-level information
% and thereby correctly segmented the tiger head.
% Additionally, GDFF was robust to occlusions and thin structures when multiple clicks were provided
% (e.g., the occluded surfing man and the wheels of the bicycle).
% This suggests that GDFF corrects misclassified regions while preserving the original precise predictions.
Different implementations of feedback fusion have been explored:
1) directly concatenating feedback with the input,
2) fusing feedback into deep features but removing the residual connection,
i.e., removing the second term of Eq. \ref{eq:update_feature},
and 3) fusing feedback into deep features but removing the gate.
As shown in Tab. \ref{tab:architecture_for_cffm},
integrating feedback into the network improves the performance,
which demonstrates the instructive ability of the feedback.
Compared with directly concatenating feedback with the input,
feedback fusion in deep layers significantly reduces the required number of annotated clicks
by 1.24 NoC@90\% on Berkeley and 1.56 NoC@90\% on DAVIS.

% while fusing the feedback into deep features yielded satisfactory results.
% This is probably because
% the three-layer convolution block has small receptive fields and locally blurred prior segmentation information after the multiplication operation. 

% \subsubsection{Analysis for RR.}
% The bottom of Fig. \ref{fig:results} illustrates the results achieved by RR.
% The segmentation results are well aligned with the boundaries of the target objects,
% % RR produced segmentation masks that well aligned with the boundaries of the target objects,
% which can be observed in the waist of the surfing man and the helmet of the cycling man.
% The fourth column of Fig. \ref{fig:results} also reveals that RR could handle fuzzy objects.
% Although the feathers of the bird are translucent and light is refracted from the background,
% % RR significantly improved the segmentation quality by capturing finer wing boundary details.
% RR captured fine details of the wing boundaries.
% Fig. \ref{fig:ablation_RR} shows the influence of reliability guidance.
% % the predicted probability maps obtained without reliability as guidance are excessively smoothed.
% Refinement without reliability as guidance led to vague boundaries and therefore limited the accuracy.
% % Additional experiments were conducted to compare RR with the widely used dense conditional random field (dCRF) \cite{krahenbuhl2011efficient}.
% Additionally, we report the results obtained
% by using the widely used dCRF and RR for post-processing, respectively.
% According to Tab. \ref{tab:refinement},
% RR attained greater improvements than dCRF.
% The performance of the two methods over successive iterations of refinement has also been investigated.
% Fig. \ref{fig:crf_chart} illustrates that turning points exist in early iterations on the line charts of dCRF,
% while the performance of RR is improved as the number of iterations increases.
% We recommend using $10$ update iterations for a trade-off between accuracy and speed.

\subsection{Limitations}
% It is worth mentioning the limitations of the proposed method.
% that we identify in our work
% Our method shows high effectiveness and efficiency.
% However, it still has some limitations.
Although our method benefits from the feedback guidance,
it still has certain limitations.
First,
% although our method exploits the guidance of the corrected feedback,
there is no guarantee that each round of interaction yields superior results compared to the previous one.
% Second, multimodality is a challenge for interactive image segmentation methods.
% Second, segmentation results may be unpredictable in some cases with ambiguity.
Second, ambiguity has yet to be resolved in our method.
% which can arise when a click is provided on an object that contains multiple targets.
For example, if a click is provided on a shirt,
both the shirt and the person wearing it are likely to be the target object.
% Assume that a picture shows a woman in a dress,
% if clicks are only provided on the dress,
% it is ambiguous whether the dress or the whole woman should be selected.
% Furthermore, the segmentation performance may be unsatisfactory for objects with thin structures,
Additionally, our method may struggle when handling thin structures,
such as ropes, insect legs, and bicycle spokes.


\section{Conclusion}
% Second, interactions are associated with each other,
% and thus the last interaction conveys prior knowledge about the target object to the current interaction (feedback).
% measures the similarities between in a local view.
% In each interaction,
% users tend to annotate clicks on the mislabeled regions of the previous segmentation mask.
The segmentation result from the last interaction (feedback) provides instructive information about the target object to the current interaction.
To exploit the feedback,
this paper proposes a deep feedback integration approach called FCFI.
% The FCFI corrects the feedback from both local and global perspective
% and then incorporates it with high-level features in deep layers.
FCFI first performs local refinement on the feedback.
Then, it collaboratively and globally updates the feedback and the features in deep layers of the segmentation network.
% First, the focused feedback correction module (FFCM) corrects the feedback from a local perspective
% according to the similarities of high-level features.
% Then, the collaborative feedback fusion module (CFFM) orderly updates the feedback in a global view
% and integrates the updated feedback into the segmentation network.
% Experimental results demonstrate that the proposed framework achieves state-of-the-art results using different backbones on standard benchmarks
% with small increases in parameters and running time.
We have experimentally demonstrated that FCFI has strong generalization capabilities
% for different backbones and datasets
and outperformed many previous methods with fast processing speed.

\section*{Acknowledgment}
This research was supported by the National Key R\&D Program of China (2020YFB1708900) and the National Nature Science Foundation of China (62021002).

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
