%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Adaptive Riemannian Metrics on SPD Manifolds}

% My package
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{tasks}
\usepackage{enumitem}
% \usepackage{breakurl}

% set
\providecommand{\calM}{\mathcal{M}}
\providecommand{\calN}{\mathcal{N}}
\providecommand{\calA}{\mathcal{A}}
\providecommand{\calX}{\mathcal{X}}
\providecommand{\calY}{\mathcal{Y}}
\providecommand{\bbN}{\mathbb{N}}
\providecommand{\bbZ}{\mathbb{Z}}
\providecommand{\bbK}{\mathbb{K}}
\providecommand{\sym}[1]{\mathcal{S}^{#1}}
\providecommand{\spd}[1]{\mathcal{S}^{#1}_{++}}
\providecommand{\cho}[1]{\mathcal{L}_{+}^{#1}}
\providecommand{\tril}[1]{\mathcal{L}^{#1}}
\providecommand{\bbR}[1]{\mathbb {R}^{#1}}
\providecommand{\bbRplus}{\mathbb {R}_{+}}
\providecommand{\bbRscalar}{\mathbb {R}}
\providecommand{\cinf}{C^{\infty}}

% operator
\providecommand{\rieexp}{\operatorname{Exp}}
\providecommand{\rielog}{\operatorname{Log}}
\providecommand{\diffphi}[1]{\phi_{*,#1}}
\providecommand{\diffphiinv}[1]{\phiinv_{*,#1}}
\providecommand{\diffmlog}[1]{\phi_{mlog*,#1}}
\providecommand{\diffmgexp}[1]{\phi_{ma*,#1}}
\providecommand{\pt}[2]{\Gamma_{#1 \rightarrow #2}}
\providecommand{\scrL}{\mathscr{L}}
\providecommand{\bbD}{\mathbb {D}}
\providecommand{\ln}{\operatorname{ln}}
\providecommand{\mln}{\phi_{mln}}
\providecommand{\cln}{\phi_{cln}}
\providecommand{\clnchart}{\varphi_{ln}}
\providecommand{\clogchart}{\varphi_{log}}
\providecommand{\mlog}{\phi_{mlog}}
\providecommand{\clog}{\phi_{clog}}
\providecommand{\mexp}{\phi_{mexp}}
\providecommand{\mgexp}{\phi_{ma}}
\providecommand{\cgexp}{\phi_{ca}}
\providecommand{\mlnMul}{\odot_{mln}}
\providecommand{\clnMul}{\odot_{cln}}
\providecommand{\mlogMul}{\odot_{mlog}}
\providecommand{\clogMul}{\odot_{clog}}
\providecommand{\mlnMulScalar}{\circledast_{mln}}
\providecommand{\clnMulScalar}{\circledast_{cln}}
\providecommand{\mlogMulScalar}{\circledast_{mlog}}
\providecommand{\clogMulScalar}{\circledast_{clog}}
\providecommand{\mlogMulInv}[1]{{#1}_{\mlogMul}^{-1}}
\providecommand{\xMulInv}[1]{{#1}_{\xMul}^{-1}}
\providecommand{\yMulInv}[1]{{#1}_{\yMul}^{-1}}
\providecommand{\distmlog}{d_{mlog}}
\providecommand{\distclog}{d_{clog}}
\providecommand{\relu}{\operatorname{ReLu}}
\providecommand{\diag}{\operatorname{diag}}
\providecommand{\rieExp}[1]{\operatorname{Exp}_{#1}}
\providecommand{\rieLog}[1]{\operatorname{Log}_{#1}}
\providecommand{\leop}{\odot_{le}}
\providecommand{\lescalar}{\circledast_{le}}
\providecommand{\lcop}{\odot_{lc}}
\providecommand{\im}[1]{\operatorname{im}{#1}}
\providecommand{\xMul}{\odot_{\phi}}
\providecommand{\yMul}{\odot_{\calY}}
\providecommand{\xMulScalar}{\circledast_{\phi}}
\providecommand{\yMulScalar}{\circledast_{\calY}}
\providecommand{\xMul}{\odot_{\phi}}
\providecommand{\yMul}{\odot_{\calY}}
\providecommand{\xMulScalar}{\circledast_{\phi}}
\providecommand{\yMulScalar}{\circledast_{\calY}}
\providecommand{\phiinv}{\phi^{-1}}
\providecommand{\phiMulScalar}{\circledast_{\phi}}
\providecommand{\gphi}{g^{\phi}}
\providecommand{\phiMul}{\odot_{\phi}}
\providecommand{\diff}{\operatorname{d}}
\providecommand{\tr}{\operatorname{tr}}
\providecommand{\galem}{g^{\mathrm{ALE}}}
\providecommand{\glem}{g^{\mathrm{LE}}}
\providecommand{\glcm}{g^{\mathrm{LC}}}
\providecommand{\geuc}{g^{\mathrm{E}}}
\providecommand{\gx}{g^{\mathcal{X}}}
\providecommand{\gy}{g^{\mathcal{Y}}}
\providecommand{\gphi}{g^{\phi}}
\providecommand{\dlem}{d^{\mathrm{LE}}}
\providecommand{\dlcm}{d^{\mathrm{LC}}}
\providecommand{\deuc}{d^{\mathrm{E}}}
\providecommand{\dalem}{d^{\mathrm{ALE}}}
\providecommand{\dphi}{d^{\phi}}
\providecommand{\Cov}{{\mathrm{Cov}}}

%symble
\providecommand{\balpha}{\boldsymbol{\alpha}}
\providecommand{\rmE}{\mathrm{E}}
\providecommand{\rmF}{\mathrm{F}}
\providecommand{\rmO}{\mathrm{O}}
\providecommand{\id}{\mathbbm{1}}
\providecommand{\idY}{\id_{\calY}}
\providecommand{\idX}{\id_{\calX}}
\providecommand{\atlasid}{\mathcal{A}_{\mathbbm{1}}}
\providecommand{\lem}{\mathrm{LEM}}
\providecommand{\lcm}{\mathrm{LCM}}

%ref
\providecommand{\defref}[1]{Definition~\ref{#1}}
\providecommand{\lemref}[1]{Lemma~\ref{#1}}
\providecommand{\secref}[1]{Section~\ref{#1}}
\providecommand{\rmkref}[1]{Remark~\ref{#1}}
\providecommand{\propsref}[1]{Proposition~\ref{#1}}
\providecommand{\thmref}[1]{Theorem~\ref{#1}}
\providecommand{\corref}[1]{Corollary~\ref{#1}}
\providecommand{\tabref}[1]{Table~\ref{#1}}
\providecommand{\figref}[1]{Figure~\ref{#1}}
\renewcommand{\eqref}[1]{Eq.~\ref{#1}}

%Other command
\newcommand{\yue}[1]{\textcolor{red}{#1}}
\newcommand{\ziheng}[1]{\textcolor{blue}{#1}}
\providecommand{\ie}{\textit{i.e. }}
\begin{document}

\twocolumn[
\icmltitle{Adaptive Riemannian Metrics on SPD Manifolds}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ziheng Chen}{unitn}
\icmlauthor{Tianyang Xu}{jnu}
\icmlauthor{Zhiwu Huang}{soton}
\icmlauthor{Yue Song}{unitn}
\icmlauthor{Xiao-Jun Wu}{jnu}
\icmlauthor{Nicu Sebe}{unitn}
\end{icmlauthorlist}

\icmlaffiliation{unitn}{Dept. of Information Engineering and Computer Science, University of Trento, Trento, Italy}
\icmlaffiliation{jnu}{School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China}
\icmlaffiliation{soton}{School of Electronics and Computer Science, University of Southampton, Southampton, U.K.}

\icmlcorrespondingauthor{Xiao-Jun Wu}{wu\_xiaojun@jiangnan.edu.cn}
\icmlcorrespondingauthor{Nicu Sebe}{sebe@disi.unitn.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Riemannian Manifolds, SPD Manifolds, Geometric Deep Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. 
To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed.
However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks.
To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. 
Moreover, we present comprehensive theories for our metrics. 
Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
\end{abstract}

\section{Introduction}

The Symmetric Positive Definite (SPD) matrices are ubiquitous in statistics, supporting a diversity of scientific areas, such as medical imaging \cite{chakraborty2018statistical,chakraborty2020manifoldnet}, signal processing \cite{arnaudon2013riemannian, hua2017matrix,brooks2019exploring, brooks2019riemannian}, elasticity \cite{moakher2006averaging, guilleminot2012generalized}, and computer vision \cite{huang2017riemannian, harandi2018dimensionality, zhen2019dilated, chakraborty2020manifoldnorm,zhang2020deep,chakraborty2020manifoldnorm,nguyen2021geomnet, nguyen2022gyrovector}.
Despite the exhibited capability of capturing data variations, SPD matrices cannot simply interact as points in a Euclidean space, which becomes the main challenge in practice.
To tackle this issue, various Riemannian metrics have been proposed to guarantee the manifoldness, including Affine-Invariant Metric (AIM) \cite{pennec2006riemannian}, Log-Euclidean Metric (LEM) \cite{arsigny2005fast}, and Log-Cholesky Metric (LCM) \cite{lin2019riemannian}, to name a few.
Equipped with these metrics, many Euclidean methods could be generalized into the domain of the Riemannian manifold \cite{wang2012covariance, huang2015log, huang2015face, harandi2018dimensionality,chen2021hybrid}.

Recently, inspired by the vivid progress of deep learning \cite{hochreiter1997long, krizhevsky2012imagenet,he2016deep}, following the theories of Riemannian geometry, several deep networks were designed on the SPD manifold.
The principal motivation of these networks is to generalize the basic network components (transformation, activation, and classification) from Euclidean networks into Riemannian ones. 
For instance, \citet{huang2017riemannian} propose a densely connected feedforward network on the SPD manifold that is named SPDNet, successfully preserving SPDness in each layer.
To generalize the object to SPD tensors,  \citet{chakraborty2020manifoldnet,zhang2020deep} establish Riemannian convolutional networks correspondingly.
In terms of distribution control, \citet{brooks2019riemannian} and \citet{chakraborty2020manifoldnorm} alternately extend the normalization onto manifolds.
In \citet{nguyen2021geomnet}, based on a novel Gaussian embedding, an SPD deep network is proposed with the ability to mine the statistics in SPD deep features.
For sequential SPD-valued data, a statistical recurrent unit (SPD-SRU) \cite{chakraborty2018statistical}, a dilated convolutional network \cite{zhen2019dilated}, and an RNN \cite{nguyen2022gyrovector} based on Gyrovector space have been proposed.
In the meantime, some works attempt to aggregate Riemannian approaches into traditional deep learning.
For example, in \citet{wang2020deep, gao2021temporal,song2021approximate}, a square root layer is proposed to approximate Euclidean projection to alleviate the computational complexity manifesting in covariance pooling.
\citet{song2022fast} further propose a differentiable implementation for fast computation of the widely used square root and inverse square root.

In general, Riemannian methods, including deep or shallow methods, heavily rely on the Riemannian operators like geodesic, exponential \& logarithmic maps, and parallel transportation.
These operators are all induced from Riemannian metrics, which is a fundamental issue for machine learning.
However, the existing Riemannian metrics on SPD manifolds fail to be learnable and hence cannot adapt to SPD-valued data.
This flaw might undermine the effectiveness of Riemannian algorithms, especially for Riemannian deep learning. 
To mitigate this limitation, in this paper we first re-analyze two popular Riemannian metrics on SPD manifolds and propose a general framework for designing Riemannian metrics by exploring the pullback technique \cite{loring2011introduction}.
Following this guideline, we propose adaptive Riemannian metrics on SPD manifolds, which can be applied to any Riemannian methods on SPD manifolds. 
As such, our \textbf{contributions} are summarized as follows:
\textbf{(a)} We uncover the essence of two popular Riemannian metrics and propose a general framework for designing Riemannian metrics;
\textbf{(b)} Based on our framework, we propose specific adaptive Riemannian metrics on SPD manifolds and conduct comprehensive analyses in terms of the algebraic, analytic, and geometric properties;
\textbf{(c)} Extensive experiments on widely used SPD learning benchmarks demonstrate that our metric exhibits consistent performance gain.

% The rest of the paper is organized as follows: 
% we first review some essential backgrounds of differential geometry;
% we then proceed to reconsider two popular Riemannian metrics on SPD manifolds and we conclude that they can be uniformly described by the idea of pullback \cite{loring2011introduction}. 
% next, a general framework of designing Riemannian metrics is then introduced;
% following this framework, specific adaptive Riemannian metrics on SPD manifolds are presented, with the ability of interacting with Riemannian learning algorithms.
% finally, we validate experimentally our proposed metrics.

\section{Basic Notations and Preliminaries} \label{sec:notaions_limited_prelim}
Some background about differential manifolds \cite{loring2011introduction,lee2013smooth}, and LEM \cite{arsigny2005fast} \& LCM \cite{lin2019riemannian} on the SPD manifold are required in this paper.
Due to the page limit, the typical basic notation is introduced here.
A more detailed review is referred to \cref{app:sec:preliminaries}.

We denote the set of $n \times n$ SPD matrices as $\spd{n}$, the set of $n \times n$ symmetric matrices as $\sym{n}$, all the Cholesky matrices as $\cho{n}$, and all the $n \times n$ lower triangular matrices as $\tril{n}$.
As shown in the previous literature \cite{arsigny2005fast,lin2019riemannian}, $\spd{n}$ and $\cho{n}$ form an SPD manifold and a Cholesky manifold, respectively.
For an SPD matrix $S$, the matrix logarithm $\mln(\cdot): \spd{n} \rightarrow \sym{n}$ is defined as
\begin{equation} \label{eq:mln} 
    \mln(S) = U \ln(\Sigma) U^\top,    
\end{equation}
where $S=U \Sigma U^\top$ is the eigendecomposition, and $\ln(\cdot)$ is the diagonal natural logarithm.
we define the Cholesky logarithm $\cln(\cdot): \spd{n} \rightarrow \tril{n} $ as
\begin{equation} \label{eq:cln}
    \cln(S) = \clnchart(\scrL(S))
\end{equation}
where $L {=} \scrL(S)$ is the Cholesky decomposition ($S{=}LL^\top$), $\clnchart(L) {=} \lfloor L \rfloor {+} \ln(\bbD(L))$ is a coordinate system from the manifold $\cho{n}$ onto the Euclidean space $\tril{n}$ \cite{lin2019riemannian}, $\lfloor L \rfloor$ is the strictly lower triangular part of $L$, $\bbD(L)$ is the diagonal entries.
In \citet{arsigny2005fast}, LEM and vector space on $\spd{n}$ are induced by matrix logarithm. In \citet{lin2019riemannian}, LCM and Lie group on $\spd{n}$ are derived from $\cho{n}$. 

\begin{definition} [Pullback Metrics] \label{def:pullback_metrics}
    Suppose $\calM,\calN$ are smooth manifolds, $g$ is a Riemannian metric on $\calN$, and $f:\calM \rightarrow \calN$ is smooth.
    Then the pullback of the tensor field $g$ by $f$ is defined point-wisely,
    \begin{equation}
        (f^*g)_p(V_1,V_2) = g_{f(p)}(f_{*,p}(V_1),f_{*,p}(V_2)),
    \end{equation}
    where $p \in \calM$, $f_{*,p}(\cdot)$ is the differential map of $f$ at $p$, and $V_i \in T_p\calM$.
    If $f^*g$ is positive definite, it is a Riemannian metric on $\calM$, which is called the pullback metric defined by $f$.
\end{definition}

In the following, we will rely on pullback metrics to rethink LEM and LCM, and further unveil their common mathematical logic, which delivers direct formulation to our design of Riemannian metrics.

\section{Rethinking LEM and LCM} \label{sec:thk_lem_lcm}
Among the existing Riemannian metrics on the SPD manifold, LEM is popular in many applications given its closed form for the Fréchet mean and clear vector space \& Lie group structures.
In addition, the nascent LCM also shares similar properties with LEM and is gaining increasing attention.
LEM is derived from the Lie group translation \cite{arsigny2005fast}, while LCM is derived by the Cholesky decomposition, \ie a Riemannian isometry from $\spd{n}$ into $\cho{n}$ \cite{lin2019riemannian}.
At first glance, one might think that LEM and LCM are designed by different mathematical tools. However, theoretically, the mathematical logic beneath their derivation can be the same. Specifically, both of them are pullback metrics from the Euclidean space. Denote LEM, LCM, and Euclidean metric as $\glem,\glcm$ and $\geuc$, respectively.
We have the following theorem.
\begin{theorem} \label{thm:rethk_lem_lcm}
    On $\spd{n}$, $\glem=\mln^*\geuc$ and $\glcm=\cln^*\geuc$.
    In other words, $\glem$ is a pullback metric from $\geuc$ by $\mln$, with $\mln$ as a Riemannian isometry from $\{\spd{n},\glem\}$ to $\{\bbR{n(n+1)/2},\geuc \}$.
    So does $\glcm$ with $\cln$.
\end{theorem}
\thmref{thm:rethk_lem_lcm} indicates that, as a Riemannian isometry, matrix logarithm (Cholesky logarithm) induces a Riemannian metric on $\spd{n}$ from the Euclidean space $\bbR{n(n+1)/2}$.
% In the next section, we will analyze in details why they can induce metrics and what kind of properties a map should follow to generate Riemannian metrics.

\section{Adaptive Riemannian Metrics} \label{sec:ada_riem_metrics}
\thmref{thm:rethk_lem_lcm} offers excellent examples of designing Riemannian metrics by pullback operations. Also, other properties like Lie groups and linear spaces are also related to $\mlog$ or $\clog$ on SPD manifolds. In this section, we will formulate and generalize this essence, forming our adaptive Riemannian metrics.
Instead of presenting our results in an ad-hoc manner, we first present a general framework on how to build Riemannian metrics on arbitrary manifolds by the idea of pullback, together with analysis of associated algebraic and analytic properties. Then we proceed to focus on the specific case of the SPD manifold and introduce our adaptive Riemannian metrics.

\subsection{General Framework}
In \cref{sec:thk_lem_lcm}, we have shown how LEM is derived from matrix logarithm $\mlog$.
Besides, as shown in \citet{arsigny2005fast}, operations in Lie group and linear space on $\spd{n}$ are also induced from $\mlog$.
Now let us explain further the underlying mechanism.
$\mlog: \spd{n} \rightarrow \sym{n}$ is an isomorphism in the category of smooth manifold \cite{loring2011introduction}, a diffeomorphism (a smooth bijection with a smooth inverse).
The property of bijection offers the possibility of transferring algebraic and analytic structures from $\sym{n}$ into $\spd{n}$.
The smoothness of $\mlog$ and its inverse suggest that smooth structures can be transferred into $\spd{n}$, like the Lie group and metric.
More generally, given an arbitrary isomorphism $\phi:\calX \rightarrow \calY$, it suffices to induce various properties from $\calY$ into $\calX$ by $\phi$ as well.
\begin{lemma}\label{lem:pro_by_bijection}
    Supposing $\phi: \calX \rightarrow \calY$ is a bijection with its inverse map denoted as $\phiinv$, we have the following conclusion.
    \begin{enumerate}
        \item \label{enu:general_hilbert}
        If $\calY$ is a Hilbert space over the number field $\bbK$, $\calX$ forms a Hilbert space over $\bbK$, with the induced operations
        \begin{align}
        \label{eq:x_mul}           x_1 \xMul x_2                   &= \phiinv( \phi(x_1) \yMul \phi(x_2)),\\
        \label{eq:x_mul_scalar}    k \xMulScalar x_2               &= \phiinv(k \yMulScalar \phi(x_2)),\\
        \label{eq:x_inner_product} \langle x_1, x_2\rangle_{\phi} &= \langle \phi(x_1), \phi(x_2)\rangle_{\calY},
        \end{align}
        where $x_1, x_2 \in \calX$, $k \in \bbK$, and $\yMul$, $\yMulScalar$, $\langle \cdot, \cdot\rangle_{\calY}$ denote the element-wise multiplication, scalar multiplication, inner product in $\calY$, respectively.
        Besides, $\phi$ is a linear isomorphism preserving the inner product.
        \item \label{enu:lie_group}
        If $\phi$ is a diffeomorphism between smooth manifold $\calX$ and (abelian) Lie group $\{\calY, \yMul \}$, then $\{\calX, \xMul \}$ forms a (an) (abelian) Lie group, and $\phi$ is a Lie group isomorphism.
        \item \label{enu:rie_manifold}
        If $\phi$ is a diffeomorphism between smooth manifold $\calX$ and Riemannian manifold $\{\calY,\gy \}$, then the pullback metric $\gphi = \phi^{*}\gy$ makes $\{\calX,\gphi\}$ an isometric Riemannian manifold to $\{\calY,\gy\}$ and $\phi$ is a Riemannian isometry.
    \end{enumerate}
\end{lemma}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{illustration.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width. This setup will avoid overfull boxes.
\caption{Conceptual illustration of \cref{lem:pro_by_bijection}.
$\phi$ is a diffeomorphism from the smooth manifold $\calX$ to the Riemannian manifold $\{\calY,\gy\}$ with the Lie group multiplication $\yMul$.
With element multiplication $\xMul$ defined in the co-domain of $\phi$, $\{\calX,\xMul\}$ forms a Lie group, 
Endowed with the pullback metric $\gphi=\phi^*\gy$, $\{\calX,\gphi\}$ forms a Riemannian manifold.
}
\label{fig:pipeline}
\end{figure}

\cref{lem:pro_by_bijection} indicates that a bijection can be an isomorphism, as long as it conforms with the axioms in a specific category.
In this way, various structural properties can be transferred from $\calY$ to $\calX$.
Therefore, the key idea of this theorem lies in the proper map $\phi$.
For a better understanding, a conceptual illustration of case \ref{enu:lie_group} and case \ref{enu:rie_manifold} in \cref{lem:pro_by_bijection} is shown in \cref{fig:pipeline}.

In case \ref{enu:rie_manifold} of \cref{lem:pro_by_bijection}, since $\phi$ is a Riemannian isometry, Riemannian operations like exponential \& logarithmic maps and parallel transportation in $\calX$ can be induced from $\calY$ as well.
Note that on manifolds, the exponential \& logarithmic maps do not generally exist globally \cite{do1992riemannian}.
However, on SPD manifolds, there are global exponential \& logarithmic maps.
Following \cref{lem:pro_by_bijection}, we present general results on SPD manifolds. Notice that the only difference between \cref{lem:pro_by_bijection} and the following one is that some Riemannian operators exist globally.

\begin{theorem} \label{thm:g_spd}
    Let $S_1,S_2 \in \spd{n}, k \in \bbRscalar$ and $\geuc$ be the standard Euclidean metric tensor field in $\bbR{n(n+1)/2}$.
    $\phi:\spd{n} \rightarrow \sym{n}$ is a diffeomorphism.
    We define the following operations,
    \begin{align}
        \label{eq:phi_mul} S_1 \phiMul S_2 &= \phiinv( \phi(S_1)+\phi(S_2)),\\
        \label{eq:phi_sca_mul} k \phiMulScalar S_2 &= \phiinv( k\phi(S_2)),\\
        \label{eq:phi_innerpro} \langle S_1, S_2 \rangle_{\phi} &= \geuc_{I}( \phi(S_1), \phi(S_2)),\\
        \label{eq:phi_g} \gphi &= \phi^{*} \geuc.
    \end{align}
    Then, we have the following conclusions:
    \begin{enumerate}
        \item \label{itm:spd_hilbet}
        $\{\spd{n}, \phiMul,\phiMulScalar, \langle \cdot, \cdot \rangle_{\phi} \}$ is a Hilbert space over $\bbRscalar$.
        \item 
        $\{\spd{n}, \phiMul \}$ is an abelian Lie group.
        $\{\spd{n}, \gphi \}$ is a Riemannian manifold.
        The associated geodesic distance is
        \begin{equation} \label{eq:dist_phi_spd}
            \dphi (S_1, S_2 ) = \| \phi(S_1) - \phi(S_2) \|_\rmF.
        \end{equation}
        The Riemannian operators are as follows
        \begin{align}
            \label{eq:gene_rie_exp_spd}
            \rieexp_{S_1} V &= \phiinv(\phi(S_1)+\diffphi{S_1}V),\\
            \label{eq:gene_rie_log_spd}
            \rielog_{S_1}S_2 &= \diffphiinv{\phi(S_1)}(\phi(S_2)-\phi(S_1)),\\
            \label{eq:gene_pt_spd} \pt{S_1}{S_2}(V) &= \diffphiinv{\phi(S_2)} \circ \diffphi{S_1}(V),
        \end{align}
        where $V \in T_{S_1}\spd{n}$ is a tangent vector, $\rieexp$, $\rielog$ and $\Gamma$ are Riemannian exponential map, logarithmic map and parallel transportation respectively, and $\phi_{*}$ \& $\phiinv_{*}$ are the differential maps of $\phi$ \& $\phiinv$.
        \item
        $\phi$ is an isomorphism: (a) a linear isomorphism preserving the inner product; (b) a Lie group isomorphism; (3) a Riemannian isometry.
    \end{enumerate}
\end{theorem}
\begin{remark}
For a better understanding of the above theorem, we make the following remarks:
\begin{enumerate}
    \item 
    For the Hilbert space in case \ref{itm:spd_hilbet}, the distance induced by the inner product $\langle \cdot, \cdot \rangle_{\phi}$ respects the geometry of SPD manifolds, as it is exactly the geodesic distance induced from $\gphi$.
    \item 
    For \eqref{eq:gene_pt_spd}, as 
    $(\phiinv)_* = (\phi_{*})^{-1}$ \cite{loring2011introduction}, we simply write $\phiinv_*$.
    Besides, although $\phiinv_{*} \phi_{*}= \id$ at any point, for $S_i \in \spd{n}$, $\diffphiinv{\phi(S_2)} \diffphi{S_1}$ might not be the identity map.
    Specifically, $\diffphiinv{\phi(S)}\diffphi{S}=\id_{T_S\spd{n}}$ does not imply $\diffphiinv{\phi(S_2)}\diffphi{S_1}=\id_{T_{S_1}\spd{n}}$.
    In addition, $\diffphiinv{S_2}\diffphi{S_1}$ could vary for different pairs of $S_1, S_2$.
    In other words, the formulae of parallel transportation could be different among different pairs of $S_1, S_2$.
\end{enumerate}
\end{remark}
In fact, LEM and LCM are special cases of \cref{thm:g_spd}, as $\mln$ and $\cln$ are instantiations of $\phi$, and so do linear space \& Lie group in \citet{arsigny2005fast} and Lie group in \citet{lin2019riemannian}.
In addition, neither \citet{arsigny2005fast} nor \citet{lin2019riemannian} reveals the Hilbert space structures in $\spd{n}$.
\subsection{Adaptive Riemannian Metrics on SPD Manifolds} \label{subsec:ada_rie_metric}
The key of \cref{thm:g_spd} lies in $\phi$.
As long as we have a proper $\phi$, Riemannian metrics on SPD manifolds can be induced.
In the following, we will present our mappings and then discuss the induced metrics.

As an eigenvalues function, matrix logarithm in \eqref{eq:mln} is reduced into a scalar logarithm, which is a diffeomorphism between $\bbRplus$ and $\bbRscalar$.
Following this hint, the eigenvalues-based diffeomorphisms between $\spd{n}$ and $\sym{n}$ are reduced to scalar diffeomorphisms between $\bbRplus$ and $\bbRscalar$. 
A very natural idea is to substitute the natural logarithm by scalar logarithms with an arbitrary proper base.
In particular, we first define a general diagonal logarithm $\log(\cdot)$ as
\begin{equation} \label{eq:diag_glog}
    \log_\alpha(X) = \diag(\log_{a_1}^{x_{11}},\log_{a_2}^{x_{22}},\cdots,\log_{a_n}^{x_{nn}}),
\end{equation}
where $\alpha = (a_1, a_2, \cdots, a_n) \in \bbRplus^{n} \setminus \{(1,1,\cdots,1)\}$ is the base vector, $\diag$ is the diagonalization operator, and $X$ is an $n \times n$ diagonal matrix.
By abuse of notation, we denote $\log_\alpha(\cdot)$ as $\log(\cdot)$ for a general diagonal logarithm, and $\log_a^{(\cdot)}$ as $\log^{(\cdot)}$ for a general scalar logarithm.
Specially, $a_1 = a_2 = \cdots = a_n=e \Rightarrow \log(\cdot) = \ln(\cdot)$.
Together with eigendecomposition, a general matrix logarithm could be derived:
\begin{equation}
    \label{eq:mlog} \mlog(S) = U \log_\alpha(\Sigma) U^\top,    
\end{equation}
where $S = U \Sigma U^\top$ is the eigendecomposition.
As a special case, when $\alpha=(e,e,\cdots,e)$, $\mlog = \mln$.
Similar to the scalar logarithm, we have the following proposition.
\begin{proposition}[Diffeomorphism] \label{props:diffeo_mlog}
    $\mlog$ is a diffeomorphism, a smooth bijection with a smooth inverse $\mlog^{-1}(\cdot):\sym{n} \rightarrow \spd{n}$ defined as
    \begin{equation}\label{eq:mgexp}
        \mlog^{-1}(X) = \mgexp(X) = U \balpha(\Sigma) U^\top,\\
    \end{equation}
    where $\balpha(\Sigma) = \diag(a_1^{\Sigma_{11}},a_2^{\Sigma_{22}},\cdots,a_n^{\Sigma_{nn}})$ is a diagonal exponentiation.
\end{proposition}
\begin{remark} \label{rmk:proposed_charts}
    Note that, $\mlog$ should be more precisely understood as an arbitrary one from the following family
    \begin{equation}
        \{  \mlog^\alpha| \alpha = (a_1, \cdots, a_n) \in \bbRplus^{n} \setminus \{(1,\cdots,1)\}  \}.
    \end{equation}
    By abuse of notation, we will simply use $\mlog$.
\end{remark}
Since $\mlog$ is a diffeomorphism from $\spd{n}$ onto $\sym{n} \simeq \bbR{n(n+1)/2}$, all the results in \cref{thm:g_spd} holds.
\begin{corollary} \label{coro:mlog_spd_properties}
    Following the notations in \cref{thm:g_spd}, we define $\mlogMul$, $\mlogMulScalar$, and $\langle \cdot, \cdot \rangle_{mlog}$, and $g^{mlog}$ as \eqref{eq:phi_mul}-\eqref{eq:phi_g}.
    Then, we have the following conclusions:
    \begin{enumerate}
        \item 
        $\{\spd{n}, \mlogMul,\mlogMulScalar, \langle \cdot, \cdot \rangle_{mlog} \}$ is a Hilbert space over $\bbRscalar$.
        \item \label{enum:mlog_riem_spd}
        $\{\spd{n}, \mlogMul \}$ is an abelian Lie group.
        $g^{mlog}$ is a Riemannian metric over $\spd{n}$.
        We name this metric as Adaptive Log-Euclidean Metric (ALEM) and denote $g^{mlog}$ as $\galem$.
        The geodesic distance under ALEM is
        \begin{equation} \label{eq:dist_mlog}
            \dalem (S_1, S_2 ) = \| \mlog(S_1) - \mlog(S_2) \|_\rmF.
        \end{equation}
        The associated Riemannian operators are as follows
        \small
        \begin{align} 
            \label{eq:rieexp_gmlog} &\rieexp_{S_1} V = \mgexp(\mlog(S_1)+\diffmlog{S_1}V),\\
            \label{eq:rielog_gmlog} &\rielog_{S_1}S_2 = \diffmgexp{X_1}(\mlog(S_2)-\mlog(S_1)),\\
            \label{eq:pt_mlog} &\pt{S_1}{S_2}(V) = \diffmgexp{X_2} \circ \diffmlog{S_1}(V),
        \end{align}
        \normalsize
        where $X_i = \mlog(S_i) \in \sym{n}$.
        \item
        $\mlog$ is an isomorphism: (a) a linear isomorphism preserving the inner product; (b) a Lie group isomorphism; (3) a Riemannian isometry.
    \end{enumerate}
\end{corollary}
\begin{remark}
    Obviously, ALEM would vary with different $\mlog$.
    This is why we use the plural to describe our metrics in the title.
    Besides, our metrics could be learnable.
    This is why we call them adaptive metrics.
\end{remark}
Given the differential maps of $\mlog$ and $\mgexp$, we can obtain the concrete formulae for \eqref{eq:rieexp_gmlog}-\eqref{eq:pt_mlog}.
We present their differential maps in the following.
\begin{proposition}[Differentials] \label{props:diff_mgexp_mlog}
    For a tangent vector $V \in T_S\spd{n}$, the differential $\diffmlog{S} : T_S \spd{n} \rightarrow T_{\mlog(S)} \sym{n}$ of $\mlog$ at $S \in \spd{n}$ is given by
    \begin{equation}
        \diffmlog{S} (V) = Q+Q^\top + W,
    \end{equation}
    where $Q = D_U\log(\Sigma)U^\top$,
    \begin{align*}
        D_U &= (\begin{array}{ccc}
             (\sigma_1 I-S)^+ V u_1 & \cdots & (\sigma_n I-S)^+ V u_n
        \end{array}),\\
        W &= U \diag(\frac{u_1^\top V u_1}{\sigma_1 \ln{a_1}},\cdots,\frac{u_n^\top V u_n}{\sigma_n \ln{a_n}}) U^\top,        
    \end{align*}
    $()^+$ is the Moore–Penrose inverse, $u_1,\cdots,u_n$ are orthonormal eigenvectors of $S$, and the associated eigenvalues are $\sigma_1,\cdots,\sigma_n$.
    
    Symmetrically, for a tangent vector $\widetilde{V} \in T_X\sym{n}$, the differential $\diffmgexp{X} : T_X \sym{n} \rightarrow T_{\mgexp(X)} \spd{n}$ of $\mgexp$ at $X \in \sym{n}$ is given by
    \begin{equation} \label{eq:diff_mgexp}
        \diffmgexp{X} (\widetilde{V}) = \widetilde{Q}+\widetilde{Q}^\top + \widetilde{W},
    \end{equation}
    where $S = \widetilde{U} \widetilde{\Sigma}\widetilde{U}^\top$ is the eigendecomposition, $D_{\widetilde{U}}$ is defined similarly, $\widetilde{Q} = D_{\widetilde{U}}\balpha(\widetilde{\Sigma})\widetilde{U}^\top$, and
    \begin{equation*}
        \widetilde{W} = \widetilde{U} \diag(\ln^{a_1}a_1^{\widetilde{\sigma_1}}{\widetilde{u}_1^\top \widetilde{V} \widetilde{u}_1},\cdots,\ln^{a_n}a_n^{\widetilde{\sigma_n}}{\widetilde{u}_n^\top \widetilde{V} \widetilde{u}_n}) \widetilde{U}^\top.     
    \end{equation*}
\end{proposition}

In \citet{arsigny2005fast}, the differential of the matrix exponential is written as infinite series.
The differential of our $\mgexp$ can also be rewritten in this way.
\begin{proposition}[Differential as Infinite Series] \label{props:diff_mgexp_series}
    Following the notation in \cref{props:diff_mgexp_mlog}, the differential of $\mgexp$ can also be formulated as
    \begin{equation} \label{eq:diff_mgexp_series}
        \begin{aligned}
            &\diffmgexp{X}(\widetilde{V}) \\
            &= \sum_{k=1}^{\infty} \frac{1}{k !}(\sum_{l=0}^{k-1} (\widetilde{P}X)^{k-l-1} (D_{\widetilde{P}}X+\widetilde{P}\widetilde{V}) (\widetilde{P}X)^l),
        \end{aligned}
    \end{equation}
    where $\widetilde{P}=\widetilde{U}B\widetilde{U}^\top$, $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$, $D_{\widetilde{P}}= D_{\widetilde{U}} B \widetilde{U}^\top + \widetilde{U} B D_{\widetilde{U}}^\top$.
\end{proposition}
 
When $\mgexp$ is reduced into matrix exponential, \eqref{eq:diff_mgexp_series} becomes Eq. 8 in \citet{arsigny2005fast}, and our ALEM becomes exactly LEM.
This is reasonable as our $\mlog$ covers the matrix logarithm as a special case.
\section{Properties of the Proposed Metrics} \label{sec:properties}
Since the involved diffeomorphism $\mlog$ generalizes matrix logarithm, ALEM is indeed an adaptive generalization of LEM.
Therefore, intuitively, ALEM would share every property of LEM.
In this section, we will present some useful properties of our ALEM for machine learning, including Fréchet mean and invariance properties.
All of the properties are shared by LEM, as LEM is a special case of our ALEM.

Fréchet means are important tools for SPD matrices learning \cite{harandi2018dimensionality,chakraborty2018statistical,brooks2019riemannian,chakraborty2020manifoldnorm}.
Like LEM, our ALEM also enjoys closed forms of Fréchet means.
We present a more general result, the weighted Fréchet mean, including the Fréchet mean as a special case.
The following proposition indicates that the weighted Fréchet mean corresponds to the weighted mean in the logarithmic domain.
\begin{proposition}[Weighted Fréchet Means] \label{props:geo_mean_spd}
    For $m$ points $S_1,\cdots S_m$ in SPD manifolds with associated weights $w_1,\cdots,w_m \in \bbRplus$,
    the weighted Fréchet mean $M$ over the metric space $\{\spd{n},\dalem\}$ has a closed form
    \begin{equation}
        M = \mgexp(\sum_{i=1}^{m} \frac{w_i}{\sum_{j=1}^{m} w_i}\mlog(S_i)).
    \end{equation}
\end{proposition}

Like LEM, although our ALEM does not conform with the affine-invariance, our ALEM enjoys some other kinds of invariance.
\begin{proposition}[Bi-invariance] \label{props:biinvariance}
    ALEM is a Lie group bi-invariant metric.
\end{proposition}
\begin{proposition} [Exponential Invariance] \label{props:exp_invariance}
    The Fréchet means under ALEM are exponential-invariant. 
    In other words, for $S_1,\cdots S_m \in \spd{n}$ and $\beta \in \bbRscalar$,
    \begin{equation}
        (\mathrm{FM}(S_1,\cdots S_m))^\beta = \mathrm{FM}(S_1^\beta,\cdots S_m^\beta),
    \end{equation}
    where $\mathrm{FM}(S_1,\cdots S_m))$ means the Fréchet mean of $S_1,\cdots S_m$.
\end{proposition}
\begin{proposition}[Similarity Invariance] \label{props:sim_invariance}
    The geodesic distance under ALEM is similarity invariant.
    In other words, let $R \in SO(n)$ be a rotation matrix, $s \in \bbRplus$ is a scale factor.
    Given any two SPD matrices $S_1$ and $S_2$, we have
    \begin{equation}
        \dalem(S_1,S_2)=\dalem(s^2RS_1 R^\top,s^2RS_2 R^\top).
    \end{equation}
\end{proposition}
Let us explain a bit more about the above three kinds of invariance.
Firstly, among metrics on Lie groups, bi-invariant metrics are the most convenient ones \cite[Chapter V]{sternberg1999lectures}.
Secondly, exponential-invariance offers a fast computation for Fréchet means under exponential scaling.
At last, similarity-invariance is significant for describing the frequently encountered covariance matrices \cite{arsigny2005fast}.

So far, the above discussion focuses on theoretical analysis.
Now, let us reconsider \eqref{eq:mlog} in a numerical way.

\begin{proposition} 
    $\mlog$ can be rewritten as
        \begin{align}
            \label{eq:rw_org_mlog} \mlog(S) 
            &= U \log_\alpha(\Sigma) U^\top,\\
            \label{eq:rw_mul_mlog}          
            &= U A \ln(\Sigma) U^\top, \\
            \label{eq:rw_div_mlog}          
            &= U \frac{\ln(\Sigma)}{B} U^\top,
        \end{align}
    where $\frac{X}{Y}$ is the diagonal division, $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$, and $A = \frac{I}{B}$.
\end{proposition}

Based on the above proposition, more analyses could be carried out from a numerical point of view.
First, $\mlog(\cdot)$ can balance the eigenvalues of an input SPD matrix $S$ by exploiting different bases for different eigenvalues.
In Riemannian algorithms, manifold-valued features usually contain vibrant information.
We expect that by the above adaptation, manifold-valued data could be better fitted and the learning ability of algorithms could be further promoted.
\begin{remark} 
Note that the design and analysis in \cref{subsec:ada_rie_metric} and \cref{sec:properties} can be readily transferred into LCM, generating an adaptive version of LCM.
\end{remark}
\section{Applications to SPD Neural Networks} \label{sec:ada_param_layers}
Since Riemannian metrics are foundations of Riemannian learning algorithms, our ALEM has the potential to rewrite Riemannian algorithms.
Besides, the base vector in $\mlog$ could bring vibrant diversity of our ALEM.
This adaptive mechanism could help the algorithm better fit with complex manifold-valued data. 
Especially in Riemannian neural networks, as we will show, optimization of base vectors can be easily embedded into the standard backpropagation (BP) process.
Therefore, we focus on the applications of our metrics to SPD neural networks.

In the existing SPD neural networks, on activation or classification layers, SPD features would interact with the logarithmic domain by matrix logarithm \cite{huang2017riemannian, zhen2019dilated, chakraborty2020manifoldnet, nguyen2021geomnet}.
The underlying mechanism of this interaction is that matrix logarithm is an isomorphism, identifying the Riemannian manifold $\{\spd{n},\glem \}$ with the Euclidean space $\sym{n}$.
This projection can therefore maintain the LEM-based geometry of SPD features.
However, in deep networks, the geometry might be more complex.
Since ALEM can vibrantly adapt with network learning, compared with the plain LEM, our ALEM could more faithfully respect the geometry of SPD deep features.
As an identification of $\{\spd{n},\galem \}$ and $\sym{n}$, $\mlog$ thus possesses more advantages than the fixed $\mln$.
We therefore replace the vanilla matrix logarithm with our $\mlog$, to respect the more advantageous geometry, \ie the ALEM-based geometry.

We focus on the most classic SPD network, SPDNet \cite{huang2017riemannian}, where matrix logarithm is used for classification.
Please refer to the \cref{app:subsec:review_SPDNet} for a quick review of SPDNet, if necessary.
Specifically, matrix logarithm in the LogEig layer is substituted by our $\mlog$. 
We call this layer as adaptive logarithm (ALog) layer. 
We set the base vector $\alpha$ as a learnable parameter.
In this way, as $\mlog$ is an isomorphism, the network can implicitly respect the ALEM-based Riemannian metric by learning the $\mlog$ explicitly.
Besides, since our ALog layer is independent of specific network architectures, it can be plugged into other SPD deep networks as well.

\section{Parameters Learning} \label{sec:parameter_learning}
Now let us explain how to optimize the proposed layer in a standard BP framework.
The specificities of the proposed ALog layer are the nonlinear manipulation of both inputs and parameters.
In this section, we will first discuss the concrete way of updating the parameters and then move on to its gradient computation.
\subsection{Parameters Update}

% \textbf{Parameters update. }
Denote the dimension of an input SPD matrix $S$ as $d \times d$.
Rethinking \eqref{eq:rw_org_mlog}-\eqref{eq:rw_div_mlog}, we could observe that there are two ways of dealing with the parameters.
The first approach, called \textit{learning factors}, is to view the multiplier $A$ or divisor $B$ as parameters, which are both diagonal matrices. 
The second method, called \textit{learning bases}, is to learn the base vector $\alpha$ directly, which lies in a non-Euclidean space $\bbRplus^{d} \setminus\{ 1,1,\cdot,1 \}$.

For the first way, since the parameters lie in a Euclidean space $\bbR{d}$, the optimization can be easily integrated into the BP algorithm.
We call learning $A$ MUL and learning $B$ DIV.

For the case of learning bases, since the parameter $\alpha$ lies in a non-Euclidean space, specific updating strategies should be considered.
Note that the elements of $\alpha$ are pairwisely decoupled.
Without loss of generality, we focus on the case of a scalar parameter $a>0 \& a \neq 1$.
The condition of $a \neq 1$ can be further waived since we can set $a=1+\epsilon$ if $a=1$. Then there is only one constraint about positivity.
To ensure this property is satisfied, we will learn it implicitly through an unconstrained parameter or explicitly through the Riemannian optimization \cite{absil2009optimization}.
In detail, we can learn the shift-ReLU of an unconstrained parameter, \ie $\max(\epsilon,a)$ with $\epsilon \in\bbRplus$.
This strategy is named RELU.
Other tricks like square are also feasible, but we will focus on the RELU.
In addition, positive scalar $a$ can be directly optimized by a geometric method, called GEOM.
Specifically, we view a positive scalar as a point in a manifold, \ie a 1-dimension SPD manifold.
Then we have the following updating formula for GEOM.
\begin{proposition} \label{propos:param_by_geom}
    Viewing a positive scalar $a$ as a point in a 1-dimensional SPD manifold, we have the following updating formula for Riemannian stochastic gradient descent (RSGD).
    \begin{equation} \label{eq:update_pos_scalar}
        a^{(t+1)} = a^{(t)}e^{-\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L},
    \end{equation}
    where $\nabla_{a^{(t)}} L$ is the Euclidean gradient of $a$ at $a^{(t)}$, $\gamma^{(t)}$ is the learning rate, and $e^{(\cdot)}$ is the natural exponentiation.
\end{proposition}

Besides, by \eqref{eq:update_pos_scalar}, we could prove that GEOM is equivalent to DIV, which is given in the following proposition.
\begin{proposition} \label{props: geom_equivalent_div}
    For parameters learning in $\mlog$, optimizing the base vector $\alpha$ by RSGD is equivalent to optimizing divisor matrix $B$ by Euclidean stochastic gradient descent (ESGD).
\end{proposition}


Based on the above analysis, there are essentially three ways of optimization, \textit{i.e.,} RELU, DIV, and MUL.

\subsection{Gradients Computation}
% \textbf{Gradients computation. }
There are two gradients that need calculation in the proposed ALog layer, one w.r.t the parameters and another w.r.t the input of the ALog layer.
Since structural matrix decomposition is involved in $\mlog$, the following contents heavily rely on the structural matrix BP \cite{ionescu2015matrix}, the key idea of which is the invariance of first-order differential form.
For the ALog layer, it is essentially a special case of eigenvalue functions.
Based on the formula offered in \citet{bhatia2009positive} and matrix BP techniques presented in \citet{ionescu2015matrix,song2021approximate}, we can obtain all the gradients, as presented in the following proposition.
\begin{proposition} \label{props:grad_mlog}
    Let us denote $X = \mlog(S)$, where $S \in \spd{d}$ is an input SPD matrix of the ALog layer.
    We have the following gradients:
    \begin{align}
        \label{eq:gradient_eigen_function} \nabla_{S} L
        &= U[K \odot(U^{T}(\nabla_{X} L) U)] U^{T},\\
        \nabla_{A} L   
        &= [U^\top (\nabla_{X} L) U] \odot \log(\Sigma),
    \end{align}
    where $S = U \Sigma U^\top$ is the eigendecomposition of an SPD matrix and matrix $K$ is defined as
    \begin{equation}
        K_{i j}= \begin{cases}\frac{f\left(\sigma_{i}\right)-f\left(\sigma_{j}\right)}{\sigma_{i}-\sigma_{j}} & \text { if } \sigma_{i} \neq \sigma_{j} \\ f^{\prime}\left(\sigma_{i}\right) & \text { otherwise }\end{cases}
    \end{equation}
    where $f(\sigma_i) = A_{ii}\log_e(\sigma_i)$ and $\Sigma=\diag(\sigma_1,\sigma_2,\cdots,\sigma_d$).
\end{proposition}

\section{Experiments} \label{sec:experiments}
In this section, we validate the efficacy of our approaches on multiple datasets.
We would like to clarify that our method does not necessarily aim to achieve the SOTA in a general sense for the following tasks, but rather to promote the learning abilities of the family of SPD-based methods.

\subsection{Datasets and Settings}
As we discussed before, although the proposed ALog layers can be plugged into the existing SPD networks, we focus on the SPDNet framework \cite{huang2017riemannian}.
We follow the
\href{https://proceedings.neurips.cc/paper/2019/file/6e69ebbfad976d4637bb4b39de261bf7-Supplemental.zip}{PyTorch code}
provided by SPDNetBN \cite{brooks2019riemannian} to reproduce SPDNet \& SPDNetBN and implement our approaches.
We evaluate our methods on three datasets, the HDM05 \cite{muller2007documentation} for skeleton-based actions recognition, the FPHA \cite{garcia2018first} for skeleton-based hand gestures recognition, and the AFEW \cite{dhall2018emotiw} for emotions recognition. 
The dimension of the input SPD features from the three datasets are $93 \times 93$, $63 \times 63$ and $512 \times 512$, respectively.
For more details about the settings of the datasets, please refer to \cref{app:sec:details_datasets}.

We denote $\{d_0, d_1,\cdots,d_L\}$ as the dimensions of each BiMap layer in SPDNet backbone.
Following the settings in \citet{brooks2019riemannian}, all networks are trained by the default SGD with a fixed learning rate $\gamma$, and with a batch size of 30.
To make ALog start from the vanilla matrix logarithm, the parameters in MUL, DIV and RELU are initialized as 1,1 and $e$, respectively.
By abuse of notation, SPDNet-ALog-MUL is abbreviated as ALog-MUL, meaning we substitute the LogEig layer (matrix logarithm) in SPDNet with our proposed ALog optimized by MUL.

\subsection{Experimental Results} \label{sec:results_on_MLog}
On the three datasets, the training epochs are set to be 200, 500, and 100.
We verify our ALog on the SPDNet with various architectures.
Besides, we further test the robustness of the proposed layer against different learning rates on the HDM05 and FPHA datasets.
Generally speaking, among all three kinds of optimization, \textbf{MUL} shows more robust performance gain and achieves consistent improvement over vanilla matrix logarithm. Besides, we could also observe that ALog-MUL is comparable to or even better than SPDNetBN, which yet brings much more complexity than our approach.
The underlying reason is twofold. Firstly, the effectiveness of the Euclidean loss function applied to the Riemannian deep networks mainly depends on the Euclidean projection itself.
Compared to the plain matrix logarithm, our $\mlog$ not only incorporates matrix logarithm as a special case but also can adaptively respect the geometry of manifold-valued data.
In addition, the implicitly induced algebraic and geometric structures discussed in \secref{sec:properties} will be preserved by $\mlog$ and could further promote the overall learning ability of the network. The following are detailed observations and analyses.
\begin{table} 
  \small
  \centering
  \caption{Results of ALog on the HDM05 dataset.}
  \label{tb:mlog_on_HDM05}
  \begin{tabular}{cccc}
    \toprule
    Architecture & \{93, 30\} & \{93, 70, 30\} & \{93, 70, 50, 30\} \\  
    \midrule
    $\gamma$ & \multicolumn{3}{c}{$1e^{-2}$}\\ 
    \midrule
    SPDNet & 62.92±0.81 & 62.87±0.60 & 63.03±0.67  \\
    SPDNetBN & 63.03±0.75 & 58.27±1.7 & 52.02±2.34  \\
    ALog-MUL & 63.52±0.75 & 63.86±0.58 & \textbf{63.94±0.44} \\
    ALog-DIV & \textbf{63.60±0.79} & 63.93±0.52 & 63.81±0.7 \\
    ALog-RELU & 63.02±0.79 & \textbf{63.94±0.64} & 63.14±0.65 \\
    \midrule
    $\gamma$ & \multicolumn{3}{c}{$5e^{-2}$}\\ 
    \midrule
    SPDNet & 63.89±0.73 & 64.00±0.65 & 63.72±0.61 \\
    SPDNetBN & 63.75±0.69 & 48.78±5.15 & 37.84±6.10 \\
    ALog-MUL & 64.4±0.68 & 64.60±0.69 & 64.36±0.49 \\
    ALog-DIV & \textbf{64.81±0.64} & \textbf{64.84±0.65} & \textbf{64.80±0.36} \\
    ALog-RELU & 63.97±0.75 & 64.10±0.63 & 63.78±0.46\\
    \bottomrule
  \end{tabular}
  \vspace{-2mm}
\end{table}

\begin{table*}[t]
    \small
    \centering
    \caption{Results of ALog on the FPHA dataset.}
    \label{tb:mlog_on_FPHA}
    \begin{tabular}{cccccc}
    \toprule
    Methods & SPDNet & SPDNetBN & ALog-MUL & ALog-DIV & ALog-RELU\\
    \midrule
    Acc.  & 85.73±0.80 & 86.83±0.74 & 87.8±0.71 & \textbf{88.07±1.13} & 86.65±0.68 \\
    \bottomrule
    \end{tabular}
\end{table*}

\textbf{Results on the HDM05 dataset.}
% \subsubsection{Results on the HDM05 Dataset}
The 10-fold results are presented in \tabref{tb:mlog_on_HDM05}, where dataset split and weights initialization are randomized.
On the HDM05 dataset, variations of the architectures come from the original paper of the SPDNet \cite{huang2017riemannian}.
Generally speaking, endowed with the ALog, SPDNet would achieve consistent improvement.
Among all three kinds of optimization, RELU only brings limited improvement.
The reason might be that RELU fails to respect the innate geometry of the positive constraint.
There is another interesting observation worth mentioning.
In \citet{brooks2019riemannian}, only the result of SPDNetBN under the architecture of $\{93,30\}$ is reported on this dataset.
Our experiments show that with the network going deeper, SPDNetBN tends to collapse, while our ALog layer performs robustly in all settings.
% \begin{table}
%     \small
%     \centering
%     \caption{Results of ALog on the FPHA dataset.}
%     \label{tb:mlog_on_FPHA}
%     \begin{tabular}{cccc}
%     \toprule
%     Methods & SPDNet & SPDNetBN & ALog-MUL \\
%     Acc.  & 85.73±0.80 & 86.83±0.74 & 87.8±0.71 \\
%     \midrule
%     Methods & ALog-DIV & ALog-RELU &  \\
%     Acc.  & \textbf{88.07±1.13} & 86.65±0.68 &  \\
%     \bottomrule
%     \end{tabular}
% \end{table}
\begin{figure}
  \centering
  \includegraphics[trim={0mm 20mm 0mm 0mm}, width=1\linewidth]{ACC_ALog_FPHA.pdf}
  \caption{Accuracy curves on the FPHA dataset.}
  \label{fig:acc_fpha_mlog}
  \vspace{-2mm}
\end{figure}

\textbf{Results on the FPHA dataset.}
% \subsubsection{Results on the FPHA Dataset}
On this dataset, we validate our approach, with a learning rate of $1e^{-2}$, over 10-fold cross-validation on random initialization.
Since our experiments show that the vanilla SPDNet is already saturated with 1 BiMap layer, we just report the results on the architecture of $\{63,33\}$, which are presented in \tabref{tb:mlog_on_FPHA}. 
We find that although DIV achieves the best performance on this dataset, it presents the biggest variance.
The reason is that there is an underlying nonlinear scaling mechanism, which is discussed in \cref{app:subsec:non_linear_div}.
However, ALog-MUL achieves robust improvement, and even surpasses SPDNetBN.
This once again demonstrates the significance of our adaptive mechanism for Riemannian deep networks.
Finally, in terms of convergence analysis, accuracy curves with and without ALog are reported in \figref{fig:acc_fpha_mlog} as well.

\begin{table}
  \small
  \centering
  \caption{Results of ALog on the AFEW dataset.}
  \label{tb:mlog_on_AFEW}
  \begin{tabular}{ccccc}
    \toprule
    Number of BiMap Layers & 1     & 2     & 3     & 4 \\
    \midrule
    SPDNet & 48.53 & 46.89 & 48.24 & 47.22 \\
    SPDNetBN & 46.89 & 46.65 & 47.62 & 48.35 \\
    ALog-MUL & \textbf{48.57} & \textbf{48.13} & \textbf{49.45} & \textbf{50.62} \\
    ALog-DIV & 48.42 & 48.02 & 48.13 & 49.89 \\
    ALog-RELU & 48.06 & 47.25 & 48.86 & 48.1 \\
    \bottomrule
  \end{tabular}
  \vspace{-2mm}
\end{table}

\textbf{Results on the AFEW dataset.}
% \subsubsection{Results on the AFEW Dataset}
On this dataset, the learning rate is $5e^{-2}$ and we validate our method under four network architectures, \ie \{512,100\}, \{512,200,100\}, \{512,400,200,100\}, and \{512,400,300,200,100\}.
Note that, on this dataset, SPDNetBN tends to present relatively large fluctuations in performance, so we compute the median of the last ten epochs.
On various architectures, consistent improvement can be observed when SPDNet is endowed with our ALog.
In addition, MUL achieves the best among all three optimization tricks.
Another interesting observation is that SPDNetBN seems not very effective on these deep features, while our methods show consistent superior performance, particularly obvious for our ALog-MUL. This indicates that our adaptive layer maintains effectiveness when applied to covariance matrices from deep features.

\textbf{Model complexity.}
% \subsubsection{Model Complexity}
Our ALog manifests the same complexity no matter how it is optimized.
Without loss of generality, the below discussion focuses on ALog-MUL.
The extra computation and memory induced by the ALog layer are minor.
It only depends on the final dimension of the network.
Let us take the deepest one on the AFEW dataset as an example.
Our ALog only brings 100 unconstrained scalar parameters, while SPDNetBN needs an SPD matrix parameter for each Riemannian batch normalization (RBN) layer.
The total number of the parameters in RBN layers sums up to $400^2+300^2+200^2$, which is much more than ours.
In addition, the SPDNetBN needs to store the running mean of SPD matrices in every RBN layer, while our ALog only needs to store a base vector.
In terms of computation, the extra cost of our ALog is secondary as well.
The forward and backward computation of our ALog is generally the same as the plain matrix logarithm, while computation in the RBN layer is much more complex.
All in all, our ALog can consistently improve the performance of the SPDNet and achieve comparable or better results against SPDNetBN with much cheaper computation and memory costs.

% \textbf{Other experiments.}
% % \subsubsection{Other Experiments}
% We also conduct experiments on SPDNetBN on all three datasets, where the final LogEig layer is substituted with our ALog.
% The results show that SPDNetBN-ALog maintains similar to SPDNetBN.
% This could be attributed to the inconsistency of Riemannian metrics.
% The RBN in SPDNetBN is based on AIM, while the metric behind our ALog is ALEM.
% Therefore, it is not very reasonable to apply our ALog to AIM-based RBN.
% However, it does not undermine the effectiveness of our ALog.
% Instead, if one plans to apply our ALog to RBN, the RBN should be rebuilt based on ALEM.
% This clearly goes beyond the topics of this paper.
% We leave this for future work.
% More detailed geometric analyses are presented in the appendix.

\section{Conclusion}
Riemannian metrics are foundations for Riemannian learning algorithms.
In this paper, we proposed a general framework for designing Riemannian metrics on arbitrary manifolds by pullback metrics.
According to this framework, adaptive Riemannian metrics were introduced for SPD matrices learning.
We also present comprehensive and rigorous theories of our metrics.
Extensive experiments indicate that SPD deep networks can benefit from our metrics.
Finally, we would like to point out some future avenues based on this framework.
The transformation and activation layers in SPD networks might be re-designed based on our metrics. Similar to \cref{coro:mlog_spd_properties}, LCM can be also transferred into an adaptive version as well. Following the general framework in \cref{lem:pro_by_bijection}, it is also possible to design Riemannian metrics on other manifolds, like Grassmannian manifolds.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\clearpage
\appendix
% \onecolumn

\section{Preliminaries} \label{app:sec:preliminaries}
\subsection{Smooth Manifolds}
We first recap some basic definitions related to this work on smooth manifolds.
For in-depth understanding, please kindly refer to \citet{loring2011introduction,lee2013smooth}. 

The most important properties of manifolds are locally Euclidean, which are described by coordinate systems.

\begin{definition}[Coordinate Systems, Charts, Parameterizations] \label{def:Parameterization}
A topological space $\calM$ is locally Euclidean of dimension $n$ if every point in $\calM$ has a neighborhood $U$ such that there is a homeomorphism $\phi$ from $U$ onto an open subset of $\mathbb{R}^{n}$. 
We call the pair $\{ U, \phi: U \rightarrow \mathbb{R}^{n}\}$ as a chart, $U$ as a coordinate neighborhood, the homeomorphism $\phi$ as a coordinate map or coordinate system on $U$, and $\phi^{-1}$ as a parameterization of $U$. 
\end{definition}

Intuitively, a coordinate system is a bijection that locally identifies the Euclidean space with the manifold.
It locally preserves the most basic properties in a manifold, the topology. 
Now, topological manifolds, which are foundations of smooth manifolds, can be defined.
\begin{definition}[Topological Manifolds]
    A topological manifold is a locally Euclidean, second countable, and Hausdorff topological space.
\end{definition}

In smooth manifolds, compatibility is further required to define smooth structures or operations.
\begin{definition}[$C^{\infty}$-compatible] \label{def:compatible}
Two charts $\{ U, \phi_1: U \rightarrow \mathbb{R}^{n} \},\{ V, \phi_2: V \rightarrow \mathbb{R}^{n} \}$ of a locally Euclidean space are $C^{\infty}$-compatible if the following two composite maps
\begin{equation}
    \begin{aligned}
        \phi_1 \circ \phi_2^{-1} &: \phi_2(U \cap V) \rightarrow \phi_1(U \cap V), \\
        \quad \phi_2 \circ \phi_1^{-1} &: \phi_1(U \cap V) \rightarrow \phi_2(U \cap V)        
    \end{aligned}
\end{equation}
are $\cinf$.
\end{definition}

By abuse of notation, we view $\phi$ alternatively as a chart or map according to the context, and abbreviate $C^{\infty}$-compatible as compatible.

\begin{definition}[Atlases] \label{def:atlas}
A $C^{\infty}$ atlas or simply an atlas on a locally Euclidean space $\calM$ is a collection $\calA=\{ \{ U_{\alpha}, \phi_{\alpha} \} \}$ of pairwise $\cinf$-compatible charts that cover $\calM$.
\end{definition}

An atlas $\mathcal{A}$ on a locally Euclidean space is said to be maximal if it is not contained in a larger atlas. 
With a maximal atlas, smooth manifold can be defined.
\begin{definition}[Smooth Manifolds] 
A smooth manifold is defined as a topological manifold endowed with a maximal atlas.
\end{definition}

We call the maximal atlas of a smooth manifold its differential structure.
In addition, every atlas $\calA$ is contained in a unique maximal atlas $\calA^+$ \cite{loring2011introduction}.
Therefore, an atlas can be used to identify the differential structure of a smooth manifold.
In this paper, manifolds always mean smooth manifolds.
Now, we can define the smoothness of a map between manifolds.
\begin{definition}[Smoothness] \label{def:smoothness}
Let $\calN$ and $\calM$ be smooth manifolds, and $f: \calN \rightarrow \calM$ a continuous map, $f(\cdot)$ is said to be $\cinf$ or smooth, if there are atlases $\calA_n$ for $\calN$ and $\calA_m$ for $\calM$ such that for every chart $\{ U, \phi \}$ in $\calA_n$ and $\{ V, \psi \}$ in $\calA_m$, the map
\begin{equation}
    \psi \circ F \circ \phi^{-1}: \phi\left(U \cap f^{-1}(V)\right) \rightarrow \mathbb{R}^{m}
\end{equation}
is $C^{\infty}$.
\end{definition}
In elementary calculus, smooth functions have derivatives.
In manifolds, derivatives are generalized into differential maps.
\begin{definition} [Differential Maps]
    Let $f: \calN \rightarrow \calM$ be a $C^{\infty}$ map between two manifolds. At each point $p \in \calN$, the map $f$ induces a linear map of tangent spaces, called its differential at $p$,
    \begin{equation}
        f_{*,p}: T_p \calN \rightarrow T_{f(p)} \calM.
    \end{equation}
    $f_{*,p}$ can be locally represented by the Jacobian matrix under a chart $\{ U, \phi \}$ about $p$ and a chart $\{ V, \psi \}$ about $f(p)$,
    \begin{equation}
        f_{*,p} := \frac{\partial f}{\partial x} := \frac{\partial \psi f \phi^{-1}}{\partial x},
    \end{equation}
    where $\frac{\partial f}{\partial x}$ is called the derivative (Jacobian matrix) of $f$ under the charts of $\{ U, \phi \}$ and $\{ V, \psi \}$.
\end{definition}
With the definition of smoothness, it is possible to define smooth algebraic structures on a manifold, \ie Lie groups.
Intuitively, a Lie group is an integration of algebra (group) and geometry (manifold).
\begin{definition}[Lie Groups] \label{def:lie_group}
A manifold is a Lie group, if it forms a group with a group operation $\odot$ such that $m(x,y) \mapsto x \odot y$ and $i(x) \mapsto x_{\odot}^{-1}$ are both smooth, where $x_{\odot}^{-1}$ is the group inverse of $x$.
\end{definition}
\subsection{Riemannian Manifolds}
When manifolds are endowed with Riemannian metrics, various Euclidean operators can find their counterparts in manifolds.
A plethora of discussions can be found in \citet{do1992riemannian}.

\begin{definition}[Riemannian Manifolds] \label{def:riem_manifold}
A Riemannian metric on $\calM$ is a smooth symmetric covariant 2-tensor field on $\calM$, which is positive definite at every point.
A Riemannian manifold is a pair $\{\calM,g\}$, where $\calM$ is a smooth manifold and $g$ is a Riemannian metric.
\end{definition}
As a basic fact in differential geometry, every smooth manifold is a Riemannian manifold \cite[Proposition~2.10, Chatper~1]{do1992riemannian}.
Therefore, in the following, we will alternatively use manifolds or Riemannian manifolds.
\begin{definition} [Pullback Metrics] \label{def:pullback_metrics_app}
    Suppose $\calM,\calN$ are smooth manifolds, $g$ is a Riemannian metric on $\calN$, and $f:\calM \rightarrow \calN$ is smooth.
    Then the pullback of a tensor field $g$ by $f$ is defined point-wisely,
    \begin{equation}
        (f^*g)_p(V_1,V_2) = g_{f(p)}(f_{*,p}(V_1),f_{*,p}(V_2)),
    \end{equation}
    where $p$ is an arbitrary point in $\calM$, $f_{*,p}(\cdot)$ is the differential map of $f$ at $p$, and $V_1,V_2$ are tangent vectors in $T_p\calM$.
    If $f^*g$ is positive definite, it is a Riemannian metric on $\calM$, called the pullback metric defined by $f$.
\end{definition}

\begin{definition}[Isometries] \label{def:isometry}
If $\{M, g\}$ and $\{\widetilde{M}, \widetilde{g}\}$ are both Riemannian manifolds, a smooth map $f: M \rightarrow$ $\widetilde{M}$ is called a (Riemannian) isometry if it is a diffeomorphism that satisfies $f^{*} \tilde{g}=g$.
\end{definition}
If two manifolds are isometric, they can be viewed as equivalent.
Riemannian operators in these two manifolds are closely related. 
\begin{definition}[Bi-invariance] \label{def:bi_invariance}
A Riemannian metric $g$ over a Lie group $\{G, \odot\}$ is left-invariant, if for any $x,y \in G$ and $V_1,V_2 \in T_x\calM$, 
\begin{equation}
    g_y(V_1,V_2) = g_{L_x(y)}(L_{x*,y}(V_1), L_{x*,y}(V_2)),
\end{equation}
where $L_x(y) = x \odot y$ is left translation, and $L_{x*,y}$ is the differential map of $L_x$ at $y$.
Right-invariance is defined similarly.
A metric over a Lie group is bi-invariant, if it is both left and right invariant. 
\end{definition}
Bi-invariant metrics are the most convenient metrics on Lie, as they enjoy many excellent properties \cite[Chatper~V]{sternberg1999lectures}. 

The exponential \& logarithmic maps and parallel transportation are also crucial for Riemannian approaches in machine learning.
To bypass the notation burdens caused by their definitions, we review the geometric reinterpretation of these operators, introduced in \citet{pennec2006riemannian, do1992riemannian}.
In detail, in a manifold $\calM$, geodesics correspond to straight lines in the Euclidean space.
A tangent vector $\overrightarrow{x y} \in T_x\calM$ can be locally identified to a point $y$ on the manifold by geodesic starting at $x$ with initial velocity of $\overrightarrow{x y}$, i.e. $y=\rieexp_x(\overrightarrow{x y})$.
On the other hand, logarithmic map is the inverse of exponential map, generating the initial velocity of the geodesic connecting $x$ and $y$, i.e. $\overrightarrow{x y}=\rielog_x(y)$.
These two operators generalize the idea of addition and subtraction in Euclidean space.
For the parallel transportation $\pt{x}{y}(V)$, it is a generalization of parallelly moving a vector along a curve in the Euclidean space.
we summarize the reinterpretation in \cref{tb:reinter_riem_operators}.
\begin{table}
    \small
    \centering
    \caption{Reinterpretation of Riemannian Operators.}
    \label{tb:reinter_riem_operators}
    \begin{tabular}{ccc}
    \toprule
    Operations & Euclidean spaces & Riemannian manifolds \\
    \midrule
    Straight Line & Straight Line & Geodesic \\
    Subtraction & $\overrightarrow{x y}=y-x$ & $\overrightarrow{x y}=\log _x(y)$ \\
    Addition & $y=x+\overrightarrow{x y}$ & $y=\exp _x(\overrightarrow{x y})$ \\
    Parallelly Moving & $V \rightarrow V$ & $\pt{x}{y}(V)$\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Geometry of the SPD Manifold} \label{subsec:geom_of_spd}
Although SPD matrices is a subset of the vector space $\bbR{{n(n+1)}/{2}}$, applying the Euclidean structure directly to SPD matrices could be problematic both practically and theoretically, as demonstrated in \cite{arsigny2005fast, pennec2006riemannian}.
The reason is that this subset does not form a linear subspace of $\bbR{{n(n+1)}/{2}}$ and is actually a manifold \cite{arsigny2005fast}.
As mentioned before, there are many kinds of well-studied Riemannian metrics on the SPD manifold.
Below, we will briefly review two related metrics, LEM \cite{arsigny2005fast} and LCM \cite{lin2019riemannian}.

Matrix logarithm $\mln(\cdot): \spd{n} \rightarrow \sym{n}$ and $\cln(\cdot): \spd{n} \rightarrow \tril{n} $ are defined as,
\begin{align}
     \mln(S) &= U \ln(\Sigma) U^\top,\\
     \cln(S) &= \clnchart(\scrL(S)),
\end{align}
where $S=U \Sigma U^\top$ is the eigendecomposition, $L = \scrL(S)$ is the Cholesky decomposition ($S=LL^\top$), $\clnchart(L) = \lfloor L \rfloor + \ln(\bbD(L))$ is a coordinate system from the $\cho{n}$ manifold onto the Euclidean space $\tril{n}$ \cite{lin2019riemannian}, $\lfloor L \rfloor$ is the strictly lower triangular part of $L$, $\bbD(L)$ is the diagonal elements, and $\ln(\cdot)$ is the diagonal natural logarithm.
We name $\cln$ as Cholesky logarithm, since in the following proof we will rely on it many times.
Note that topologically, $\tril{n} \simeq \sym{n} \simeq \bbR{n(n+1)/2}$, since their metric topology all comes from the Euclidean metric tensor.
Based on matrix logarithm, \citet{arsigny2005fast} propose LEM by Lie group translation, while based on Cholesky logarithm, \citet{lin2019riemannian} proposes LCM, by an isometry between $\spd{n}$ and $\cho{n}$.
In the main paper, we argued that LEM and LCM are basically the same, in the sense of high-level mathematical abstraction.

The Riemannian metric and associated geodesic distance under the LEM are defined by:
\begin{align}
    \label{eq:metric_lem} \glem_{S}(V_1,V_2) &= \geuc( {\mln}_{*,S} ( V_{1}), {\mln}_{*,S} ( V_{2}) ),\\
    \dlem(S_1, S_2) &= \| \mln(S_1) - \mln(S_2)\|_\rmF,
\end{align}
where $S \in \spd{n}$, $V_1, V_2 \in T_S\spd{n}$ are tangent vectors, ${\mln}_{*,S}(\cdot)$ is the differential map of matrix logarithm at $S$, $\geuc$ is the standard Euclidean metric tensor, and $\| \cdot \|_F$ is Frobenius norm.
Note that since $\geuc$ is the same at every point, we simply omit the subscript.
Besides, element-wise and scalar multiplication are also induced by $\mln$:
\begin{align}
    S_1 \mlnMul S_2 & = \mexp(\mln(S_1) + \mln(S_2)),\\
    \lambda \mlnMulScalar S & = \mexp(\lambda \mln(S)),
\end{align}
where $\mexp(X) = U \exp(\Sigma) U^\top$ is the matrix exponential.
As is proven in \citet{arsigny2005fast}, $\{ \spd{n}, \mlnMul\}$ and $\{ \spd{n}, \mlnMul,\mlnMulScalar\}$ form a Lie group and vector space, respectively.
Besides, the metric $\glem$ defined on Lie group $\{ \spd{n}, \mlnMul\}$ is bi-invariant.

The Riemannian metric and geodesic distance under LCM is
% \begin{equation} \label{eq:metric_lcm} 
% \begin{aligned}
%     &\glcm_S(V_{1}, V_{2} ) \\
%     &= \tilde{g}_L(L(L^{-1} V_1 L^{-\top})_{\frac{1}{2}},L(L^{-1}V_2L^{-\top})_{\frac{1}{2}}),    
% \end{aligned}
% \end{equation}
\small
\begin{equation} \label{eq:metric_lcm} 
    \glcm_S(V_{1}, V_{2} ) = \tilde{g}_L(L(L^{-1} V_1 L^{-\top})_{\frac{1}{2}},L(L^{-1}V_2L^{-\top})_{\frac{1}{2}}),
\end{equation}
\begin{equation}
    \begin{aligned}
         \dlcm(S_1, S_2) &= \{\|\lfloor L_1\rfloor-\lfloor L_2\rfloor\|_{\rmF}^{2}+\\
        & \|\ln( \bbD(L_1))-\ln (\bbD(L_2))\|_{\rmF}^{2}\}^{\frac{1}{2}},      
    \end{aligned}
\end{equation}
\normalsize
where $S \in \spd{n}$, $V_1,V_2 \in T_S\spd{n}$, $X_{\frac{1}{2}} = \lfloor X \rfloor + \bbD(X)/2$, and $\tilde{g}_L(\cdot, \cdot)$ is the Riemannian metric on $\cho{n}$, defined as
\begin{equation} \label{eq:metric_cholesky}
\begin{aligned}
    \tilde{g}_{L}(X, Y) &= \geuc (\lfloor X\rfloor,\lfloor Y\rfloor)\\ 
                        &+ \geuc(\bbD(L)^{-1} \bbD(X), \bbD(L)^{-1} \bbD(Y)).
\end{aligned}
\end{equation}
The group operation in \citet{lin2019riemannian} is defined as follows:
\begin{equation}
    S_1 \clnMul S_2 = \scrL^{-1} (\lfloor L_1 \rfloor + \lfloor L_2 \rfloor + \bbD(L_1)\bbD(L_2)),
\end{equation}
where $\scrL^{-1}(\cdot)$ is the inverse map of Cholesky decomposition.
$\{\spd{n}, \clnMul\}$ is proven to be a Lie group \cite{lin2019riemannian}.
Similar with LEM, $\glcm$ is bi-invariant.

\subsection{Building Blocks in the SPDNet} \label{app:subsec:review_SPDNet}
The architecture of SPDNet \cite{huang2017riemannian} mimics the conventional densely connected feedforward network.
It has three basic components, named BiMap, ReEig, and LogEig.

The BiMap (Bilinear Mapping) is a generalized version of conventional linear mapping, defined as
\begin{equation}
    S^{k} = W^k S^{k-1} W^k, \text { with } W^k \text { semi-orthogonal.}
\end{equation}

The ReEig (Eigenvalue Rectification) can be viewed as eigen-rectification, mimicking the ReLu-like nonlinear activation functions:
\begin{equation}
    X^{k}=U^{k} \max \left(\Sigma^{k}, \epsilon I_{n}\right) U^{k \top},
\end{equation}
where $P^{k}=U^{k} \Sigma^{k} U^{k^\top}$ is the eigendecomposition.

The LogEig layer projects SPD-valued data into the Euclidean space for further classification:
\begin{equation}
     X^{k}=\mln(X^{k-1}).
\end{equation}

\section{Proofs for the Lemmas, Propositions, Theorems, and Corollaries Stated in the Paper}

\begin{proof} [Proof of \cref{thm:rethk_lem_lcm}]
Recalling \defref{def:isometry}, to show the isometry, we need to find the pullback formulation.

Let us first deal with the LEM.
Recalling \eqref{eq:metric_lem}, we can readily conclude that $\mlog$ is an isometry from $\{\spd{n},\glem\}$ onto $\{\bbR{n(n+1)/2},\geuc \}$.

Now, let us focus on LCM.
By \eqref{eq:metric_lcm}, $\{\spd{n}, \glcm\}$ is isometric to $\{\cho{n},\tilde{g}\}$, with Cholesky decomposition $\scrL$ as an isometry.
This is exactly how LCM is derived \cite{lin2019riemannian}.
So, the key point lies in Cholesky metric $\tilde{g}$.
Let us reveal why it is defined in this way.
In fact, $\tilde{g}$ is derived from $\geuc$ by $\clnchart$.
Simple computations show that
\begin{equation} \label{eq:diff_cho_ln_chart}
    \varphi_{ln *,L}(V) = \lfloor V \rfloor + \mathbb{D}^{-1}(L)\mathbb{D}(V),
\end{equation}
where $V \in T_L\cho{n}$.
By \eqref{eq:diff_cho_ln_chart}, \eqref{eq:metric_cholesky} can be rewrote as
\begin{equation}
    \tilde{g}_L(X,Y) = \geuc(\varphi_{ln *,L}((X),\varphi_{ln *,L}((Y)).
\end{equation}
Therefore, $\clnchart: \cho{n} \rightarrow \bbR{n(n+1)/2}$ is an isometry.
By transitivity, $\cln: \spd{n} \rightarrow \bbR{n(n+1)/2}$ is an isometry as well.
\end{proof}

\begin{proof} [Proof of \cref{lem:pro_by_bijection}]
    First, let's focus on the case \ref{enu:general_hilbert}.
    Before starting, we point out that \eqref{eq:x_mul}-\eqref{eq:x_inner_product} are indeed well-defined, which is easy to verify by the bijectivity of $\phi$ and the well-defined operations on $\calY$.
    More explicitly, the three maps should be expressed as the following,
    \begin{align}
        \xMul                                 &: \calX \times \calX \rightarrow \calX,\\
        \xMulScalar                           &: \bbK \times \calX \rightarrow \calX,\\
        \langle \cdot , \cdot \rangle_{\phi} &: \calX \times \calX \rightarrow \bbK.
    \end{align}
    In the following, we first show that $\{\calX,\xMul\}$ is an abelian group and then $\{\calX,\xMul, \xMulScalar\}$ is a vector space over $\bbK$.
    Next, we proceed to verify $\{ \calX,\xMul, \xMulScalar, \langle \cdot , \cdot \rangle_{\phi} \}$ is an inner product space.
    Lastly, we will prove the completeness of $\{ \calX,\xMul, \xMulScalar, \langle \cdot , \cdot \rangle_{\phi} \}$.
    
    Given arbitrary $x, x_1, x_2, x_3 \in \calX$ and $k, l \in \bbK$, we have the following proof.
    Note that we would not emphasize the arbitrariness again in the following description.
    
    Firstly, to prove a non-empty set endowed with an operation is an abelian group.
    We need to verify: 1). closure under this operation, 2).associativity and commutativity, 3). left identity element, 4). left invertibility.
    
    For 1), it can be verified directly by the bijectivity of $\phi$.
    
    For 2), it can be guaranteed by the associativity and commutativity of $\yMul$,
    \begin{equation}
        \begin{aligned}
            (x_1 \xMul x_2) \xMul x_3 &= [\phiinv(\phi(x_1) \yMul \phi(x_2))] \xMul x_3\\
                                      &= \phiinv ( (\phi(x_1) \yMul \phi(x_2)) \yMul \phi(x_3) )\\
                                      &= \phiinv ( \phi(x_1) \yMul (\phi(x_2) \yMul \phi(x_3)) )\\
                                      &= x_1 \xMul (x_2 \xMul x_3)
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            x_1 \xMul x_2 &= \phiinv(\phi(x_1) \yMul \phi(x_2))\\
                          &= \phiinv(\phi(x_2) \yMul \phi(x_1))\\
                          &= x_2 \xMul x_1
        \end{aligned}
    \end{equation}   
    
    For 3), denoting the neutral element in $\calY$ as $\idY$, we then have,
    \begin{equation}
        \begin{aligned}
            \phiinv(\idY) \xMul x   &= \phiinv(\idY \yMul \phi(x))\\
                                          &= \phiinv(\phi(x))\\
                                          &= x.
        \end{aligned}
    \end{equation}
    
    For 4), denoting $y^{-1}_{\yMul}$ as the inverse element of $y \in \calY$ and $x^{-1}_{\xMul}=\phiinv(\phi(x)^{-1}_{\yMul})$, we have,
    \begin{equation}
        \begin{aligned}
            x^{-1}_{\xMul} \xMul x &= \phiinv(\phi(x)^{-1}_{\yMul} \yMul \phi(x))\\
                                   &= \phiinv(\idY)\\
                                   &= \idX
        \end{aligned}
    \end{equation}
    So far, we prove that $\{ \calX, \xMul \}$ is an abelian group.
    To show $\{ \calX, \xMul, \xMulScalar \}$ is a linear space , we only need to verify four laws about scalar product.
    we have the following equations to support our claim:
    \begin{equation}
        \begin{aligned}
            1 \xMulScalar x &= \phiinv(1 \yMulScalar \phi(x))\\
                            &= \phiinv(\phi(x))\\
                            &= x,
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            k \xMulScalar (x_1 \xMul x_2) &= k \xMulScalar [\phiinv(\phi(x_1) \yMul \phi(x_2))]\\
                                          &= \phiinv(k \yMulScalar [\phi(x_1) \yMul \phi(x_2)] )\\
                                          &= \phiinv([k \yMulScalar \phi(x_1)] \yMul [k \yMulScalar \phi(x_2)] )\\
                                          &= (k \xMulScalar x_1) \xMul (k \xMulScalar x_1),
        \end{aligned}
    \end{equation}    
    \begin{equation}
        \begin{aligned}
            (k+l) \xMulScalar x &= \phiinv( (k+l) \yMulScalar \phi(x) )\\
                                &= \phiinv( (k \yMulScalar \phi(x)) \yMul (l \yMulScalar \phi(x)) )\\
                                &= (k \xMulScalar x) \xMul  (l \xMulScalar x),
        \end{aligned}
    \end{equation} 
    \begin{equation}
        \begin{aligned}
            k \xMulScalar (l \xMulScalar x) &= k \xMulScalar \phiinv( l \yMulScalar \phi(x) )\\
                                            &= \phiinv( k \yMulScalar( l \yMulScalar \phi(x)))\\
                                            &= \phiinv( (kl) \yMulScalar \phi(x))\\
                                            &= (kl) \xMulScalar x.
        \end{aligned}
    \end{equation} 
    
    Now, we see $\calX$ is a linear space.
    Next, we verify that $\langle \cdot,\cdot \rangle_{\phi}$ is an inner product.
    This means it should satisfy: a). positivity, b). definiteness, c). linearity in the first slot, and d). symmetry.
    
    For a)-d), they all can be derived by the corresponding properties of inner product in $\calY$:

    \begin{equation}
        \langle x, x \rangle_{\phi} = \langle \phi(x), \phi(x) \rangle_{\calY} \ge 0, \\
    \end{equation}
    \begin{equation}
    \begin{aligned}
        \langle x, x \rangle_{\phi} = 0, &\iff \langle \phi(x), \phi(x) \rangle_{\calY} = 0, \\
        & \iff \phi(x)=\idY, \\
        & \iff x=\idX,
    \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \langle x_1 \xMul x_3, x_2 \rangle_{\phi} 
            &= \langle \phi(x_1) \yMul \phi(x_3), \phi(x_2) \rangle_{\calY}\\
            &= \langle \phi(x_1), \phi(x_2) \rangle_{\calY} +  \langle \phi(x_3), \phi(x_2) \rangle_{\calY}\\
            &= \langle x_1 , x_2 \rangle_{\phi} + \langle x_3 , x_2 \rangle_{\phi}
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \langle k \xMulScalar x_1, x_2 \rangle_{\phi} 
            &= \langle \phiinv(k \yMulScalar \phi(x_1)), x_2 \rangle_{\phi}\\
            &= \langle k \yMulScalar \phi(x_1), \phi(x_2) \rangle_{\calY}\\
            &= k \langle  \phi(x_1), \phi(x_2) \rangle_{\calY}\\
            &= k \langle  x_1, x_2 \rangle_{\phi}\\
        \end{aligned}
    \end{equation}     
    \begin{equation}
        \begin{aligned}
            \langle x_1, x_2 \rangle_{\phi} &= \langle \phi(x_1), \phi(x_2) \rangle_{\calY}\\
                                            &= \langle \phi(x_2), \phi(x_1) \rangle_{\calY}\\
                                            &= \langle x_2, x_1 \rangle_{\phi},
        \end{aligned}
    \end{equation}    
    Recall the above proof, we can find that $\phi$ is a linear isomorphism that could preserve the inner product and thus an isometry.

    Last but not least, we verify $\calX$ is complete.
    Since $\calY$ is complete and $\phi$ is an isometric bijection, therefore $\calX$ is complete as well.
    
    Now, let us deal with case \ref{enu:lie_group}. 
    We can see that $\phi$ is a group isomorphism and $\calX$ is a group.
    What's left is to verify the smoothness of the group operations.
    First, we present the multiplications and inverses  more explicitly,
    \begin{align}
        m_{\calY}(\cdot,\cdot): & \calY \times \calY \rightarrow \calY,\\
        i_{\calY}(\cdot): & \calY \rightarrow \calY,\\
        m_{\calX}(\cdot,\cdot): & \calX \times \calX \rightarrow \calX,\\
        i_{\calX}(\cdot): & \calX \rightarrow \calX,
    \end{align}
    where $m_{\calY},i_{\calY}$ mean the multiplication and inverse in $\calY$, and $m_{\calX},i_{\calX}$ mean the counterparts in $\calX$.
    Recalling \eqref{eq:x_mul}, we rewrite group operations as
    \begin{align}
        m_{\calX} &= \phiinv \circ m_{\calY} \circ (\phi \times \phi),\\
        i_{\calX} &= \phiinv \circ i_{\calY} \circ \phi,
    \end{align}
    where $\phi \times \phi$ means the Cartesian product of two maps.
    Since $m_{\calY}$, $i_{\calY}$, $\phi$ and $\phiinv$ are all smooth, as the composition and Cartesian product of some smooth maps, $m_{\calX}$ and $i_{\calX}$ are therefore smooth, which means that $\calX$ is a Lie group.
    Considering $\phi$ is a diffeomorphism and a group isomorphism, it is, therefore, a Lie group isomorphism.
    
    Lastly, let us deal with case \ref{enu:rie_manifold}.
    By the \defref{def:pullback_metrics} and \defref{def:isometry}, we can readily conclude that $\phi$ is a Riemannian isometry from $\{\calX , \gphi \}$ into $\{\calY , \gy \}$.    
\end{proof}

\begin{proof} [Proof of \cref{thm:g_spd}]
    This is just a special case of \cref{lem:pro_by_bijection}.
    With the global exponential \& logarithmic maps, and parallel transportation in Euclidean space, and the properties of Riemannian isometry, \eqref{eq:gene_rie_exp_spd}-\eqref{eq:gene_pt_spd} can be derived. 
\end{proof}
\begin{proof} [Proof of \cref{props:diffeo_mlog}]
    Obviously, $\mgexp$ is the inverse of $\mlog$. 
    What followed is to verify the smoothness of $\mlog$ and its inverse.
    
    According to Theorem 8.9 in \citet{magnus2019matrix}, the map producing an eigenvalue or an eigenvector from a real symmetric matrix is $\cinf$.
    Recalling $\mlog$ and its inverse map $\mgexp$, it's obvious that they are comprised of arithmetic or composition of some smooth maps.
    Therefore, $\mlog$ ($\mgexp$) is a diffeomorphism.
\end{proof}
\begin{proof} [Proof of \cref{coro:mlog_spd_properties}]
    This is a direct result of \cref{thm:g_spd}.
\end{proof}
\begin{proof} [Proof of \cref{props:diff_mgexp_mlog}]
    The differentials of $\mgexp$ and $\mlog$ can be derived in exactly the same way. 
    In the following, we only present the process of deriving the differential of $\mlog$. 
    
    First, Let us recall the differentials of eigenvalues and eigenvectors.
    Theorem 8.9 in \citet{magnus2019matrix} offers their Euclidean differentials, which are the exact formulations for differentials under the canonical base on SPD manifolds.
    So, we can readily obtain the differentials of eigenvalues and eigenvectors as the following:
    \begin{align}
        \label{eq: diff_eig_value} \sigma_{*,S} (V) &= u^{\top} V u,\\
        \label{eq: diff_eig_vec} u_{*,S} (V) &= (\sigma I- S)^{+} V u,
    \end{align}
    where $S u = \sigma u, u^\top u = 1$, and $()^+$ is the Moore–Penrose inverse.

    By the RHS of \eqref{eq:rw_org_mlog}, the differential map of $\mlog$ is
    \begin{equation} \label{eq:diff_mlog}
    \begin{aligned}
        \diffmlog{S}(V) &= U_{*,S} (V) \log(\Sigma)U^\top + U (\log{\Sigma})_{*,S} (V) U^\top \\
                  & + U \log(\Sigma) U^\top_{*,S} (V) \\
                  &= Q+Q^\top + U^\top (\log{\Sigma})_{*,S} (V) U,
    \end{aligned}
    \end{equation}
    where $Q= U_{*,S} (V) \log(\Sigma)U^\top$.

    For the differential of diagonal logarithm, it is 
    \begin{equation} \label{eq: diff_ln}
        \log_{*,S}{\Sigma} = A \frac{1}{\Sigma} \Sigma_{*,S},
    \end{equation}
    where $A$ is defined in \eqref{eq:rw_mul_mlog}.

    Denote the eigenvectors and eigenvalues of $S=U\Sigma U^\top$ as $U = (u_1,\cdots,u_n)$ and $\Sigma=\diag(\sigma_1,\cdots,\sigma_n)$.
    By \eqref{eq: diff_eig_value}-\eqref{eq: diff_ln}, the differential of $\mlog$ can be obtained.
\end{proof}
\begin{proof} [Proof of \cref{props:diff_mgexp_series}]
    Following the notations in the proposition, we make the following proof.
    By abuse of notation, in the following, we omit the wide tilde $\widetilde{~}$.
   
    Now, we proceed to deal with the differential of $\mgexp$.
    We rewrite the formula of $\mgexp$ as
    \small
    \begin{align}
        & \mgexp(X) \\
        &= U \balpha(\Sigma) U^\top,\\
        &= U \diag(e^{\ln^{a_1}\sigma_1},\cdots,e^{\ln^{a_n}\sigma_n})U^\top,\\
        &= U \diag(\sum_{k=0}^{\infty}\frac{(\ln^{a_1}\sigma_1)^k}{k!},\cdots,\sum_{k=0}^{\infty}\frac{(\ln^{a_n}\sigma_n)^k}{k!})U^\top,\\
        \label{eq:rw_mgexp_last2} &= U (\sum_{k=0}^{\infty}\frac{B\Sigma}{k!}) U^\top,\\
        \label{eq:rw_mgexp} &= \sum_{k=0}^{\infty}\frac{PX}{k!}
    \end{align}
    \normalsize
     where $P=UBU^\top$, with $U$ from eigendecomposition $X=U\Sigma U^\top$ and diagonal matrix $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$.
     By the properties of normed vector algebras \cite[Proposition~15.14, \S~15.4]{loring2011introduction}, we can obtain the last equation.
     Then, we can compute the differential of $\mgexp$ by curves.
     Given a curve $c$ on $\sym{n}$ starting at $X$ with initial velocity $W \in T_X\sym{n}$, we have
    \begin{align}
        \diffmgexp{X}(W) 
        &= \left. \frac{d}{dt} \right |_{t=0} \mgexp \circ c(t)\\
        \label{eq:diff_mgexp_series_proof_stp2}
        &= \left. \frac{d}{dt} \right |_{t=0}\sum_{k=0}^{\infty}\frac{Pc(t)}{k!}.
    \end{align} 
    By a term-by-term differentiation, we have
    \begin{equation} 
        \begin{aligned} \label{eq:diff_last2step_mgexp}
            &\diffmgexp{X}(W)\\ 
            &= \sum_{k=1}^{\infty} \frac{1}{k !}(\sum_{l=0}^{k-1}(PX)^{k-l-1} \left. \frac{d}{dt} \right|_{t=0}(Pc) (PX)^l).
        \end{aligned}
    \end{equation}
    By the chain rule, we have
    \begin{equation} \label{eq:chain_rule_Pc}
        \left. \frac{d}{dt} \right|_{t=0}(Pc) = P'(0)X + PV.
    \end{equation}
    $P'(0)$ is obtained by
    \begin{align}
        P'(0) &= (UBU^\top)'(0),\\
              &= U'(0)BU^\top + UBU^{\top '}(0),\\
              \label{eq:diff_P_at0} &= D_U B U^\top + U B D_U^\top,
    \end{align}
    where $D_U$ is derived from the differential of eigenvectors,
    \begin{equation} \label{eq:D_U}
    \small
    D_U = (\begin{array}{ccc}
             (\sigma_1 I-S)^+ V u_1 & \cdots & (\sigma_n I-S)^+ V u_n
        \end{array}).
    \end{equation}
    Applying \eqref{eq:chain_rule_Pc}, \eqref{eq:diff_P_at0} and \eqref{eq:D_U} into \eqref{eq:diff_last2step_mgexp}, we have the differential of $\mgexp$.
\end{proof}

\begin{proof} [Proof of \cref{props:geo_mean_spd}]
    Obviously, the metric space $\{\spd{n},\dalem\}$ is isometric to the space $\sym{n}$ endowed with the standard Euclidean distance.
    Therefore, the weighted Fréchet mean of $\{S_i\}$ in $\spd{n}$ corresponds to the weighted Fréchet mean of associated points $\{\mlog(S_i)\}$ in $\sym{n}$.
    The weighted Fréchet means in Euclidean spaces are clearly the familiar weighted means.
\end{proof}

\begin{proof} [Proof of \cref{props:biinvariance}]
    By abuse of notations, we denote $\mlog$, $\mgexp$ and $\mlogMul$ as $\phi$, $\phiinv$ and $\odot$.
    % Besides, we will omit the subscripts of the tangent points in differential maps, if there is no confusion.
    Since $\{\spd{n},\odot\}$ is an abelian Lie group, it suffices to show the left invariance.
    
    For any $P,Q \in \spd{n}$ and $V_1,V_2 \in T_p\spd{n}$, we make the following discussion.
    Denote the left translation by $Y$ in $\sym{n}$ as
    \begin{equation}
        m_{Y}: X \in \sym{n} \rightarrow Y+X.
    \end{equation}
    By simple computation, $m_{Y,*}$ is the identity map at any point.
    Then the left translation $L_Q$ by $Q$ on $\{\spd{n},\odot\}$ can be rewrote as
    \begin{equation}
        L_Q = \phiinv \circ m_{\phi(Q)} \circ \phi.
    \end{equation}
    By the chain rule of differential, we have 
    \begin{align}
        L_{Q*,P} &= \phiinv_{*,\phi(Q)+\phi(P)} m_{\phi(Q)*,\phi(P)} \phi_{*,P},\\
        &= \phiinv_{*,\phi(Q)+\phi(P)} \phi_{*,P}
    \end{align}
    Note that 
    \begin{equation}
        \phi_{*,L_Q(P)} \phiinv_{*,\phi(Q)+\phi(P)} = \id
    \end{equation}
    Then we have the following equations,
    \begin{equation}
        \small
        \begin{aligned}
            & \langle L_{Q*,P} V_1, L_{Q*,P} V_2 \rangle_{L_Q (P)},\\
            &= \langle \phi_{*,L_Q(P)} L_{Q*,P} V_1, \phi_{*,L_Q(P)} L_{Q*,P} V_2 \rangle_{I},\\
            &= \langle \phi_{*,P} V_1, \phi_{*,P} V_2 \rangle_{I},\\
            &= \langle V_1, V_2 \rangle_{P},
        \end{aligned}
    \end{equation}
\end{proof}


\begin{proof} [Proof of \cref{props:exp_invariance}]
    Following the notations in this proposition, we make the following proof.
    The LHS can be rewritten as 
    \begin{align}
        (\mathrm{FM}(S_1^\beta,\cdots S_m^\beta)) 
        &= \mgexp(\sum_{i=1}^{m} \frac{1}{m}\beta\mlog(S_i)),\\
        &= \mgexp(\beta\sum_{i=1}^{m} \frac{1}{m}\mlog(S_i)),\\
        &= [\mgexp(\sum_{i=1}^{m} \frac{1}{m}\mlog(S_i))]^\beta,\\
        &= (\mathrm{FM}(S_1,\cdots S_m))^\beta 
    \end{align}
\end{proof}
\begin{proof} [Proof of \cref{props:sim_invariance}]
    Obviously, for a given SPD matrix $S$,
    \begin{align}
        \label{eq:mlog_rotate} \mlog(R S R^\top) &= R \mlog(S) R^\top,\\
        \label{eq:mlog_scale} \mlog(s^2 S ) &=  U(\log(s^2 I)+\mlog(\Sigma))U^\top,
    \end{align}
    where $S=U \Sigma U\top$ is the eigendecomposition.
    With \eqref{eq:mlog_rotate} and \eqref{eq:mlog_scale}, we can obtain the results.
\end{proof}

\begin{proof} [Proof of \cref{propos:param_by_geom}]
    Let's first review the update formulation in the RSGD \cite{bonnabel2013stochastic}, which is, geometrically speaking, a natural generalization of Euclidean stochastic gradient descent.
    For a minimization parameter $w$ on an $n$-dimensional smooth connected Riemannian manifold $\calM$, we have the following update, 
    \begin{equation} \label{eq:riem_sgd}
        w^{(t+1)}=\rieExp{w^{(t)}} (-\gamma^{(t)} \pi_{w^{(t)}}(\nabla_{w^{(t)}} L )),
    \end{equation} 
    where $\rieExp{w}(\cdot): T_w\calM \rightarrow \calM$ is the Riemannian exponential map, which maps a tangent vector at $w$ back into the manifold $\calM$, and $\pi_{w}(\cdot): \bbR{n} \rightarrow T_w\calM$ is the projection operator, projecting an ambient Euclidean vector into the tangent space at $w$.
    In the case of the SPD manifold, $\forall S \in \spd{n}, \forall X \in \bbR{n \times n}, \forall V \in \sym{n}$, the exponential map and projection operator is formulated as the following:
    \begin{align}
        \label{eq:spd_proj} \pi_{S}(X) &= S \frac{X+X^\top}{2}S,\\
        \label{eq:spd_exp} \rieExp{S}(V) &= S^{1/2} \mexp(S^{-1/2} V S^{-1/2}) S^{1/2},
    \end{align}
    where $\mexp(\cdot)$ is the matrix exponential.
    For more details about \eqref{eq:spd_proj} and \eqref{eq:spd_exp}, please kindly refer to \citet{yger2013review} and \citet{amari2016information}.
    Substitute \eqref{eq:spd_proj} and \eqref{eq:spd_exp} into \eqref{eq:riem_sgd}, \eqref{eq:update_pos_scalar} can be immediately obtained.
\end{proof}
\begin{proof} [Proof of \cref{props: geom_equivalent_div}]
    Without loss of generality, we focus on the equivalence between $b=B_{11}$ and $a=\alpha_{11}$.
    Let us denote $\log_{e}^{(\cdot)}$ as $\ln^{(\cdot)}$.
    Note that $b$ is essentially expressed as $b=\ln^a$.
    Supposing $b^{(t)} = \ln^{a^{(t)}}$, then we have
    \begin{align}
        \nabla_{a^{(t)}} L 
        &= \nabla_{b^{(t)}} L \frac{\partial \ln^a }{a} |_{a^{(t)}}\\
        &= \nabla_{b^{(t)}} L \frac{1}{a^{(t)}}.
    \end{align}
    according to the update \eqref{eq:update_pos_scalar}, then $\ln^{a^{(t+1)}}$ is 
    \begin{equation}
        \begin{aligned}
            \ln^{a^{(t+1)}} 
            &= \ln^{a^{(t)}e^{-\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L}}\\
            &= \ln^{a^{(t)}} -\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L\\
            &= \ln^{a^{(t)}} - \gamma^{(t)} a^{(t)} (\nabla_{b^{(t)}} L/a^{t}) \\
            &= \ln^{a^{(t)}} -\gamma^{(t)}\nabla_{b^{(t)}} L\\
            &= b^{t} -\gamma^{(t)}\nabla_{b^{(t)}} L.\\
        \end{aligned}
    \end{equation}
    The last row is the exact updated formula of ESGD for $b$.
    
    Therefore, supposing $b^{(0)} = \ln^{a^{(0)}}$, then the optimization results after the overall training are equivalent.
\end{proof}
\begin{proof} [Proof of \cref{props:grad_mlog}]
    \eqref{eq:gradient_eigen_function} is the so-called Daleck\u ii-Kre\u in formula presented in \citet[Page 60]{bhatia2009positive}.
    Now let us focus on the gradient w.r.t $A$.
    Differentiating both sides of \eqref{eq:rw_mul_mlog}:
    \begin{equation}
        \diff X = (*) + U \diff A \odot \log(\Sigma)U^T,
    \end{equation}
    where $(*)$ means other parts related to $\diff U$ and $\diff \Sigma$.
    According to the invariance of first-order differential form, we have,
    \begin{align}
        & \nabla_{X} L : \diff X \nonumber \\ 
        &= \nabla_{S} L:\diff S + \nabla_{X} L: (U \diff A \odot \log(\Sigma)U^T) \\
        &= \nabla_{S}L :\diff S + [U^\top (\nabla_{X} L) U] \odot \log(\Sigma):\diff A, \label{eq:diif_last_row}
    \end{align} 
    Where $A:B=\tr(A^\top B)$ is the Euclidean Frobenius inner product.
    From the second term on the RHS of \eqref{eq:diif_last_row}, we can obtain the gradient w.r.t $A$.
\end{proof}
\section{Details on the Datasets} \label{app:sec:details_datasets}

\textbf{HDM05.}
This dataset is comprised of motion capture data (MoCap) covering 130 action classes.
Each data point is a sequence of frames of 31 3D coordinates.
After vectorization, each sequence can be represented by a $93 \times 93$ temporal covariance matrix.
For a fair comparison, we exploit the pre-processed $93 \times 93$
\href{https://www.dropbox.com/s/dfnlx2bnyh3kjwy/data.zip?dl=0}{covariance features}
released by \citet{brooks2019riemannian}, which trims the dataset down to 2086 points scattered throughout 117 classes by removing some under-represented classes.
Following the settings in \citet{brooks2019riemannian},  we split the dataset into 50\% for training and 50\% for testing.

\textbf{FPHA.}
It includes 1,175 clips of 45 different action categories.
Each frame is represented by 21 3D coordinates.
Similarly, each sequence can be modelled by a $63 \times 63$ covariance matrix.
For a fair comparison, we follow the experimental protocol in \citet{garcia2018first}, where 600 sequences are used for training and 575 sequences are used for testing.

\textbf{AFEW.}
It consists of 7 kinds of emotions with 773 samples for training and 383 samples for validation.
We use the released pre-trained 
\href{https://github.com/Open-Debin/Emotion-FAN}{FAN} \cite{meng2019frame}
to extract deep features and establish a $512 \times 512$ temporal covariance matrix for each video.

\section{Additional Experiments and Explanations}
\subsection{Explanations for Nonlinear Mechanism in DIV} \label{app:subsec:non_linear_div}
In the main paper, we mentioned that there is an underlying nonlinear scaling mechanism in the update of DIV.
Without loss of generality, let us focus on one scalar parameter $b$ in \eqref{eq:rw_div_mlog}.
The ultimate factor multiplied by the plain logarithm is $1/b$.
Therefore, the change of the multiplier after the update would be 
\begin{equation} \label{eq:nonlinear_div}
    1/(b-\Delta)-1/b=\Delta/[(b-\Delta)b].
\end{equation}
\eqref{eq:nonlinear_div} will scale the original $\Delta$ to some extent.
This scaling mechanism might undermine the robustness of the ALog layer.


\subsection{Experiments on the SPDNetBN}
In the main paper, we observed that among all three kinds of implementation of ALog, the most robust performance is achieved by MUL.
So we exploit MUL to carry out further experiments on the SPDNetBN \cite{brooks2019riemannian}.
In the following, we simply call SPDNetBN-ALog-MUL as SPDNetBN-ALog, where the LogEig layer is substituted by our ALog layer.
Our experiments on three datasets show that SPDNetBN is similar to SPDNetBN-ALog.
This could be attributed to the inconsistency of Riemannian metrics.
The RBN in SPDNetBN is based on AIM, while the metric behind our ALog is ALEM.
Therefore, it is not very reasonable to apply our ALog to AIM-based RBN.
However, it does not undermine the effectiveness of our ALog.
Instead, if one plans to apply our ALog to RBN, the RBN should be rebuilt based on ALEM.
This clearly goes beyond the topics of this paper.
We leave this for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
