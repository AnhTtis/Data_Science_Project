\section{Applications to Other Riemannian Blocks}
\label{sec:app_other_riem_blocks}

Riemannian metrics are foundations for Riemannian neural networks.
Therefore, our ALEM can re-design basic blocks in Riemannian neural networks.
This section applies our ALEM to other Riemannian building blocks, including Riemannian batch normalization \cite{chen2024liebn}, Riemannian residual blocks \cite{katsman2023riemannian}, and Riemannian classifiers \cite{nguyen2023building}.
We also use the NTU60 \cite{shahroudy2016ntu} dataset as an example of the large-scale dataset. More implementation details are presented in Supp. C.
% More implementation details are presented in \cref{sub:sec:imp_details}.


\subsection{Riemannian Batch Normalization}

\begin{table}[htbp]
  \centering
  \caption{Comparison of RBN methods on the HDM05 dataset.}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
    Methods & Geometries & [93, 30] & [93, 70, 30] & [93, 70, 50, 30] \\
    \midrule
    None  & \na  & 63.89±0.73 & 64.00±0.65 & 63.72±0.61 \\
    SPDNetBN   & AIM   & 63.75±0.69 & 48.78±5.15 & 37.84±6.10 \\
    SPDBN & AIM   & 64.33±0.89 & 64.31±0.92 & 63.62±1.21 \\
    LieBN-LEM & LEM   & 63.67±0.85 & 65.77±0.89 & 65.34±0.83 \\
    \midrule
    LieBN-ALEM & ALEM  & \textbf{65.24±0.71} & \textbf{70.11±0.96} & \textbf{68.86±0.72} \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:results_rbn}
\end{table}

In Euclidean neural networks, batch normalization \cite{ioffe2015batch} has been widely used since it can facilitate network training.
Recently, Chen~\etal~\cite{chen2024liebn} proposed a framework for Riemannian batch normalization (RBN) on Lie groups, referred to as LieBN.
LieBN can guarantee the normalization of sample statistics under the left- or right-invariant metric \cite[Prop. 4.2]{chen2024liebn}.
As shown in \cref{thm:mlog_spd_properties}, $\{\spd{n}, \mlogMul \}$ forms a Lie group.
Besides, \cref{props:biinvariance} demonstrates that ALEM is bi-invariant w.r.t. this group structure.
Therefore, LieBN under ALEM can also normalize Riemannian sample statistics.
We follow Alg. 1 and Thm 5.3 in \cite{chen2024liebn} to implement the LieBN under ALEM, denoted as LieBN-ALEM.
In addition, we compared LieBN-ALEM against other kinds of RBN methods, including AIM-based SPDNetBN \cite{brooks2019riemannian} and SPDBN \cite{kobler2022controlling}, and LieBN under LEM \cite{chen2024liebn} (LieBN-LEM).

Following previous work \cite{kobler2022controlling,brooks2019riemannian,chen2024liebn}, we adopt the SPDNet backbone. 
\cref{tab:results_rbn} presents the 10-fold average results on the HDM05 dataset under different network architectures.
Our LieBN-ALEM achieves the best performance compared with the other RBN methods.
Especially, the AIM-based SPDNetBN brings worse performance under deeper architectures.
In contrast, our LieBN-ALEM can consistently improve the performance across different architectures.
Besides, compared with LieBN-LEM, our LieBN-ALEM shows better performance, demonstrating the effectiveness of our ALEM.

\subsection{Riemannian Residual Blocks}

\begin{table}[htbp]
  \centering
  \caption{Experiments of RResNet under different geometries.}
    \begin{tabular}{ccc}
    \toprule
    Methods & HDM05 & NTU \\
    \midrule
    SPDNet & 63.89±0.73 & 45.90±1.11 \\
    RResNet-AIM & 63.82±0.58 & 45.22 ± 1.23 \\
    RResNet-LEM & 66.51±0.93 & 48.73±0.60 \\
    \midrule
    RResNet-ALEM & \textbf{69.03±1.06} & \textbf{57.09±0.59} \\
    \bottomrule
    \end{tabular}%
  \label{tab:results_rresnet_hdm05_ntu60}%
\end{table}%

ResNets \cite{he2016deep} have become ubiquitous in machine learning due to their beneficial learning properties.
Recently, Katsman~\etal~\cite{katsman2023riemannian} extended the Euclidean ResNet into Riemannian spaces, referred to as RResNet.
On the SPD manifold, the Riemannian residual block under a given metric $g$ is defined as
\begin{align}
    \label{eq:rresnet_eq1}
    g(S) &= \rieexp_{S}(\ell(S)),\\
    \ell(X) &= Q \diag \left(f(\spec (X))\right) Q^T,
\end{align}
where $\rieexp$ is the Riemannian exponentiation under $g$, $\ell: \spd{n} \rightarrow T\spd{n}$ constructs the vector field, $\spec(\cdot)$ is the spectral map that takes SPD matrices to a vector of their eigenvalues, $f: \bbR{n} \rightarrow \bbR{n}$ is parameterized as a neural network, and $Q \in \orth{n}$.
Since the Riemannian exponential in \cref{eq:rresnet_eq1} is metric-dependent, the Riemannian residual blocks vary under different metrics.
The Riemannian residual block under ALEM can be obtained by putting \cref{eq:rieexp_gmlog} into \cref{eq:rresnet_eq1}.
We need further to show the gradient w.r.t. $\mexp$.
As the inverse of \cref{eq:rw_mul_mlog}, $\mexp$ can be rewrote as
\begin{equation}
    \label{eq:mexp_rewrotten}
    \begin{aligned}
        \mexp(X) 
        &= U \balpha(\Sigma) U\top\\
        &= U \exp \left(\frac{\Sigma}{A} \right) U\top,
    \end{aligned}
\end{equation}
where $X=U \Sigma U^\top \in \sym{n}$ is the eigendecomposition.
Following \cref{props:grad_mlog}, we can obtain the backpropagation of $\mexp$, which is presented in the following.

\begin{proposition} \label{props:grad_mexp}
    Let us denote $X = \mexp(S)$ with $S \in \spd{d}$.
    We have the following gradients:
    \begin{align}
        \label{eq:gradient_mexp_wrt_S}
        \nabla_{S} L
        &= U[K \odot(U^{T}(\nabla_{X} L) U)] U^{T},\\
        \label{eq:gradient_mexp_wrt_A}
        \nabla_{A} L   
        &= [U^\top (\nabla_{X} L) U] \odot \left(\balpha(\Sigma) \frac{-\Sigma}{A^2} \right),
    \end{align}
    where $S = U \Sigma U^\top$ is the eigendecomposition of an SPD matrix and matrix $K$ is defined as
    \begin{equation}
        K_{i j}= \begin{cases}\frac{f\left(\sigma_{i}\right)-f\left(\sigma_{j}\right)}{\sigma_{i}-\sigma_{j}} & \text { if } \sigma_{i} \neq \sigma_{j} \\ f^{\prime}\left(\sigma_{i}\right) & \text { otherwise }\end{cases}
    \end{equation}
    where $f(\sigma_i) = e^{\frac{\sigma_i}{A_{ii}}}$ and $\Sigma=\diag(\sigma_1,\sigma_2,\cdots,\sigma_d$).
\end{proposition}

Following \cite{katsman2023riemannian}, we compare RResNet under different geometries on the HDM05 and NTU60 datasets.
\cref{tab:results_rresnet_hdm05_ntu60} reports the 10-fold and 5-fold average results on these datasets.
Compared with the vanilla SPDNet, RResNet-AIM brings little improvement, while LEM and ALEM show much better performance.
Especially, the ALEM-based RResNet can bring a clear performance improvement, underscoring the effectiveness of our ALEM.

\subsection{Riemannian Classifiers}

\begin{table}[htbp]
  \centering
  \caption{Comparison of Gyro MLRs on the NTU60 datasets.}
    \begin{tabular}{ccc}
    \toprule
    Learning Rates & $1e^{-2}$ & $5e^{-2}$ \\
    \midrule
    GyroMLR-AIM & 54.28±0.47 & 41.41±0.71 \\
    GyroMLR-LCM & 42.68±0.88 & 42.06±0.49 \\
    GyroMLR-LEM & 53.22±0.47 & 39.62±1.30 \\
    \midrule
    GyroMLR-ALEM & \textbf{56.21±0.39} & \textbf{51.65±0.44} \\
    \bottomrule
    \end{tabular}%
  \label{tab:gyro_mlr_ntu60}%
\end{table}%

Euclidean Multinomial Logistic Regression (MLR), which consists of FC and softmax, has become a standard classification block in Euclidean neural networks.
Inspired by this, Nguyen and Yang~\cite{nguyen2023building} extended the Euclidean MLR into the SPD manifolds by gyro structures \cite{nguyen2022gyro} for intrinsic classification, referred to as gyro MLR.
Three gyro MLRs under LCM, AIM, and LEM was introduced in \cite{nguyen2023building}.
Following the logic in \cite[Sec. 2.4.2]{nguyen2023building}, we can obtain the gyro MLR under ALEM.

\begin{theorem}[Gyro MLR] \label{thm:gyro_mlr_alem}
    Given an SPD feature $S \in \spd{n}$ and $C$ classes, the SPD gyro MLR under ALEM computes the multinomial probability of each class:
    \begin{equation}
    \label{eq:gyro_mlr_alem_start}
    \begin{aligned}
        &p(y=k \mid S)  \\
        &\propto
        \exp \left [ \langle \mlog(S)-\mlog(P_k), \diffmlog{P_k} (\tilde{A}_{k}) \rangle \right ],
    \end{aligned}
    \end{equation}
    where $k \in\{1, \ldots, C\}$, $P_k \in \spd{n}$, and $\tilde{A}_k \in T_{P_k} \spd{n}$.
\end{theorem}

Since $A_k$ lies in $T_{P_k} \spd{n}$, and $P_k$ varies during network training, $A_k$ cannot be viewed as a Euclidean parameter. Following \cite{ganea2018hyperbolic}, we set $\tilde{A}_{k}=\pt{I}{P_k}(A_k)$ with $A_k \in T_I \spd{n}$ (a fixed tangent space). Therefore, the RHS of \cref{eq:gyro_mlr_alem_start} becomes
\begin{equation}
        \exp \left [ \langle \mlog(S)-\mlog(P_k), \diffmlog{I} (A_k) \rangle \right ],
\end{equation}
As $\diffmlog{I} (A_k) \in T_0 \sym{n} \cong \sym{n}$, we view $\diffmlog{I} (A_k)$ as the parameter.

We use the SPDNet as the backbone. We compare Gyro MLR under our ALEM with the ones under LEM, LCM, and AIM on the NTU60 dataset.
\cref{tab:gyro_mlr_ntu60} presents the 5-fold average results under different learning rates.
Our ALEM outperforms the other metrics within the gyro MLR framework.
When the learning rate is $5e^{-2}$, our GyroMLR-ALEM shows more advantageous performance, especially compared with GyroMLR-LEM.
These results demonstrate that the Riemannian networks can benefit from the adaptivity of our ALEM.

