\section{Experiments} 
\label{sec:experiments}
In this section, we validate the efficacy of our approaches on multiple datasets.
We would like to clarify that our method does not necessarily aim to achieve the SOTA in a general sense for the following tasks but rather to promote the learning abilities of the family of SPD-based methods.

\subsection{Datasets and Settings}
As we discussed before, although the proposed ALog layers can be plugged into the existing SPD networks, we focus on the SPDNet framework \cite{huang2017riemannian}.
We follow the PyTorch code provided by SPDNetBN\footnote{https://proceedings.neurips.cc/paper/2019/file/\allowbreak 6e69ebbfad976d4637bb4b39de261bf7-Supplemental.zip} to reproduce SPDNet \& SPDNetBN and implement our approaches.
% Following previous work \cite{huang2017riemannian,brooks2019riemannian}, we evaluate our methods on theHDM05 \cite{muller2007documentation}, FPHA \cite{garcia2018first}, AFEW \cite{dhall2018emotiw} datasets. 

Following previous work \cite{huang2017riemannian,brooks2019riemannian}, we evaluate our methods on three datasets:
the HDM05 \cite{muller2007documentation} for skeleton-based actions recognition, the FPHA \cite{garcia2018first} for skeleton-based hand gestures recognition, and the AFEW \cite{dhall2018emotiw} for emotions recognition. 
The HDM05 dataset comprises motion capture data (MoCap) covering 130 action classes.
Each data point is a sequence of frames of 31 3D coordinates.
Each sequence can be represented by a $93 \times 93$ temporal covariance matrix.
For a fair comparison, we exploit the pre-processed $93 \times 93$ covariance features \footnote{https://www.dropbox.com/s/dfnlx2bnyh3kjwy/\allowbreak data.zip?dl=0} released by \cite{brooks2019riemannian}, which trims the dataset down to 2086 points scattered throughout 117 classes by removing some under-represented classes.
Following the settings in \cite{brooks2019riemannian},  we split the dataset into 50\% for training and 50\% for testing.
FPHA includes 1,175 clips of 45 different action categories.
Each frame is represented by 21 3D coordinates.
Similarly, each sequence can be modeled by a $63 \times 63$ covariance matrix.
For a fair comparison, we follow the experimental protocol in \cite{garcia2018first}, where 600 sequences are used for training, and 575 sequences are used for testing.
AFEW consists of 7 kinds of emotions, with 773 samples for training and 383 samples for validation.
We use the released pre-trained FAN\footnote{https://github.com/Open-Debin/Emotion-FAN} \cite{meng2019frame} to extract deep features and establish a $512 \times 512$ temporal covariance matrix for each video.

We denote $\{d_0, d_1,\cdots,d_L\}$ as the dimensions of each transformation layer in the SPDNet backbone.
Following the settings in \cite{brooks2019riemannian}, all networks are trained by the default Riemannian SGD \cite{becigneul2018riemannian} with a fixed learning rate $\gamma$ and batch size of 30.
To make ALog start from the vanilla matrix logarithm, the parameters in MUL, DIV, and RELU are initialized as 1,1 and $e$, respectively.
By abuse of notation, SPDNet-ALog-MUL is abbreviated as ALog-MUL, denoting that we substitute the LogEig layer (matrix logarithm) in SPDNet with our proposed ALog optimized by MUL.
All experiments use an Intel Core i9-7960X CPU with 32 GB RAM.

\subsection{Experimental Results} 
\label{sec:results_on_MLog}
\begin{table*}[htbp]
    \small
    \centering
    \caption{Results of ALog on the HDM05 Dataset.}
    \label{tb:mlog_on_HDM05}
    % \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{c|ccc|ccc}
    \toprule
    Learning rate & \multicolumn{3}{c|}{$1e^{-2}$} & \multicolumn{3}{c}{$5e^{-2}$} \\
    \hline
    Architecture & \{ 93, 30\} & \{ 93, 70, 30\} & \{ 93, 70, 50, 30\} & \{ 93, 30\} & \{ 93, 70, 30\} & \{ 93, 70, 50, 30\} \\
    \hline
    SPDNet & 62.92±0.81 & 62.87±0.60 & 63.03±0.67 & 63.89±0.73 & 64.00±0.65 & 63.72±0.61 \\
    SPDNetBN & 63.03±0.75 & 58.27±1.7 & 52.02±2.34 & 63.75±0.69 & 48.78±5.15 & 37.84±6.10\\
    ALog-MUL & 63.52±0.75 & 63.86±0.58 & \textbf{63.94±0.44} & 64.4±0.68 & 64.60±0.69 & 64.36±0.49 \\
    ALog-DIV & \textbf{63.60±0.79} & 63.93±0.52 & 63.81±0.7 & \textbf{64.81±0.64} & \textbf{64.84±0.65} & \textbf{64.80±0.36} \\
    ALog-RELU & 63.02±0.79 & \textbf{63.94±0.64} & 63.14±0.65 & 63.97±0.75 & 64.10±0.63 & 63.78±0.46 \\
    \bottomrule
    \end{tabular}
    \vspace{-2mm}
\end{table*}

On the three datasets, the training epochs are set to be 200, 500, and 100.
We verify our ALog on the SPDNet with various architectures.
Besides, we further test the robustness of the proposed layer against different learning rates on the HDM05 and FPHA datasets.
Generally speaking, among all three kinds of implementation, \textbf{ALog-MUL} shows the most robust performance gain and achieves consistent improvement over the vanilla matrix logarithm. Besides, we could also observe that ALog-MUL is comparable to or even better than SPDNetBN, which yet brings much more complexity than our approach.
The main reason for the superiority of our ALog against the vanilla matrix logarithm is that our ALog can adaptively respect the vibrant geometry of SPD manifolds, depending on the characteristics of datasets, while only LEM can be respected by the matrix logarithm.
The following are detailed observations and analyses.

\textbf{Results on the HDM05 dataset.}
% \subsubsection{Results on the HDM05 Dataset}
The 10-fold results are presented in \cref{tb:mlog_on_HDM05}, where dataset split and weights initialization are randomized.
Following \cite{huang2017riemannian}, three architectures are implemented on this dataset, \ie \{ 93, 30\}, \{ 93, 70, 30\}, and \{ 93, 70, 50, 30\}.
Generally speaking, endowed with the ALog, SPDNet would achieve consistent improvement.
Among all three kinds of implementation, RELU only brings limited improvement.
The reason might be that RELU fails to respect the innate geometry of the positive constraint.
There is another interesting observation worth mentioning.
In \cite{brooks2019riemannian}, only the result of SPDNetBN under the architecture of $\{93,30\}$ is reported on this dataset.
Our experiments show that with the network going deeper, SPDNetBN tends to collapse, while our ALog layer performs robustly in all settings.

\begin{figure}[htbp]
  \centering
  \includegraphics[trim={0mm 5mm 0mm 0mm}, width=\columnwidth]{ACC_ALog_FPHA.pdf}
  \caption{Accuracy Curves on the FPHA Dataset.}
  \label{fig:acc_fpha_mlog}
  \vspace{-2mm}
\end{figure}

\begin{table}[htbp]
    \small
    \centering
    \caption{Results of ALog on the FPHA Dataset.}
    \label{tb:mlog_on_FPHA}
    \resizebox{0.99\columnwidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        \multirow{2}[4]{*}{SPDNet} & \multirow{2}[4]{*}{SPDNetBN} & \multicolumn{3}{c}{ALog} \\
        \cmidrule{3-5}          &       & MUL   & DIV   & RELU \\
        \midrule
        85.73±0.80 & 86.83±0.74 & 87.8±0.71 & \textbf{88.07±1.13} & 86.65±0.68 \\
        \bottomrule
    \end{tabular}
    }
\end{table}


\textbf{Results on the FPHA dataset.}
% \subsubsection{Results on the FPHA Dataset}
We validate our approach on this dataset, with a learning rate of $1e^{-2}$, over 10-fold cross-validation on random initialization.
Since our experiments indicate that the vanilla SPDNet is already saturated with 1 BiMap layer, we just report the results on the architecture of $\{63,33\}$, which are presented in \cref{tb:mlog_on_FPHA}. 
Although DIV performs best on this dataset, it presents the biggest variance.
There is an underlying nonlinear scaling mechanism in the update of DIV, which might undermine its robustness.
Without loss of generality, let us focus on a single scalar parameter $b$ in \cref{eq:rw_div_mlog}.
The ultimate factor multiplied by the plain logarithm is $1/b$.
Therefore, the change of the multiplier after the update would be 
\begin{equation} \label{eq:nonlinear_div}
    1/(b-\Delta)-1/b=\Delta/[(b-\Delta)b].
\end{equation}
\cref{eq:nonlinear_div} will scale the original $\Delta$ to some extent.
This scaling mechanism might undermine the robustness of the ALog layer.
However, ALog-MUL achieves robust improvement and even surpasses SPDNetBN.
This again demonstrates the significance of our adaptive mechanism for Riemannian deep networks.
Finally, in terms of convergence analysis, accuracy curves with and without ALog are also reported in \cref{fig:acc_fpha_mlog}.

\begin{table}[htbp]
    \small
    \centering
    \caption{Results of ALog on the AFEW Dataset.}
     \label{tb:mlog_on_AFEW}
    \begin{tabular}{ccccc}
        \toprule
        Depth & 1 & 2 & 3 & 4 \\
        % Architecture & \{512, 100\} & \{512, 200, 100\} & \{512, 400, 200, 100\} & \{512, 400, 300, 200, 100\} \\
        \midrule
        SPDNet & 48.53 & 46.89 & 48.24 & 47.22 \\
        SPDNetBN & 46.89 & 46.65 & 47.62 & 48.35 \\
        ALog-MUL & \textbf{48.57} & \textbf{48.13} & \textbf{49.45} & \textbf{50.62} \\
        ALog-DIV & 48.42 & 48.02 & 48.13 & 49.89 \\
        ALog-RELU & 48.06 & 47.25 & 48.86 & 48.1 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Results on the AFEW dataset.}
% \subsubsection{Results on the AFEW Dataset}
On this dataset, the learning rate is $5e^{-2}$ and we validate our method under four network architectures, \ie \{512, 100\}, \{512, 200, 100\}, \{512, 400, 200, 100\}, and \{512, 400, 300, 200, 100\}.
Note that, on this dataset, SPDNetBN tends to present relatively large fluctuations in performance, so we compute the median of the last ten epochs.
On various architectures, consistent improvement can be observed when SPDNet is endowed with our ALog.
In addition, MUL achieves the best among all three kinds of implementation.
Another interesting observation is that SPDNetBN seems ineffective on these deep features, while our methods show consistent superior performance, particularly obvious for our ALog-MUL. This indicates that our adaptive layer maintains effectiveness when applied to covariance matrices from deep features.

\textbf{Model complexity.}
% \subsubsection{Model Complexity}
Our ALog manifests the same complexity, no matter how it is optimized.
Without loss of generality, the discussion below focuses on ALog-MUL.
The extra computation and memory costs caused by the ALog layer are minor.
It only depends on the final dimension of the network.
Let us take the deepest one on the AFEW dataset as an example.
Our ALog only brings 100 unconstrained scalar parameters, while SPDNetBN needs an SPD matrix parameter for each Riemannian batch normalization (RBN) layer.
The total number of the parameters in RBN layers is $400^2+300^2+200^2$, which is much bigger than ours.
In addition, the SPDNetBN needs to store the running mean of SPD matrices in every RBN layer, while our ALog only needs to store a vector.
In terms of computation, the extra cost of our ALog is secondary as well.
The forward and backward computation of our ALog is generally the same as the plain matrix logarithm, while computation in the RBN layer is much more complex.
All in all, our ALog can consistently improve the performance of the SPDNet and achieve comparable or better results against SPDNetBN with much cheaper computation and memory costs.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{Params_distri_ALog_MUL_HDM05.pdf}
  \caption{Visualization of Parameters in the ALog Layer on the HDM05 Dataset.}
  \label{fig:vis_params_hdm05}
  % \vspace{-2mm}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{Params_distri_ALog_MUL_FPHA.pdf}
  \caption{Visualization of Parameters in the ALog Layer on the FPHA Dataset.}
  \label{fig:vis_params_fpha}
  % \vspace{-2mm}
\end{figure}

% \begin{figure}[htbp]
%     \centering
%     \subfigure[HDM05]{
%         \includegraphics[width=0.45\columnwidth]{Params_distri_ALog_MUL_HDM05.pdf}
%         \label{label_for_cross_ref_1}
%     }
%     \subfigure[FPHA]{
% 	\includegraphics[width=0.45\columnwidth]{Params_distri_ALog_MUL_FPHA.pdf}
%         \label{label_for_cross_ref_2}
%     }
%     \caption{Visulization of Parameters in the ALog Layer on the HDM05 and FPHA Datasets}
%     \label{fig:vis_params_hdm05_dpha}
% \end{figure}

\textbf{Visualization.}
We visualize the final learned parameters of the ALog layer. 
Since ALog-MUL is the most robust strategy, we visualize the parameters of ALog-MUL. 
Specifically, we plot the final values of the diagonal elements of $A$ in \cref{eq:rw_mul_mlog} and visualize the results in \cref{fig:vis_params_hdm05,fig:vis_params_fpha}. 
We observe that the distribution of the parameters is consistent within the same dataset but varies between datasets. 
This indicates that our approach can capture vibrant patterns in different datasets, respecting their specific geometry.

\begin{table}[htbp]
    \small
    \centering
    \caption{Results of Fixed Bases on the HDM05 and FPHA Datasets.}
    \label{tab:fix_base_alog}
    \resizebox{0.99\columnwidth}{!}{
    \begin{tabular}{c|ccc|c}
        \toprule
        Dataset & \multicolumn{3}{c|}{HDM05} & FPHA \\
        Architecture & \{93, 30\} & \{93, 70, 30\} & \{93, 70, 50, 30\} & \{63, 33\} \\
        \hline
        SPDNet-Log2 & 63.93±0.81 & 63.54±0.50 & 63.98±0.63 & 86.65±0.67 \\
        SPDNet & 63.89±0.73 & 64.00±0.65 & 63.72±0.61 & 85.73±0.80 \\
        SPDNet-Log10 & 63.45±0.33 & 63.8±0.71 & 63.64±0.64 & 78.42±0.77 \\
        \rowcolor{gray!20}SPDNet-ALog-MUL & \textbf{64.4±0.68} & \textbf{64.60±0.69} & \textbf{64.36±0.49} & \textbf{87.8±0.71} \\
        \bottomrule
    \end{tabular}}
    \vspace{-2mm}
\end{table}

\textbf{Ablation studies.}
To further demonstrate the utility of the adaptive mechanisms in our approach, we further validate the ALog layer with fixed bases.
As decimal and binary are the two most common systems, we use $\log_{10}$ and $\log_{2}$ as examples of shrinking and expanding  $\log_e$. 
Specifically, we set $\log_\alpha$ = $\log_{10}$ and $\log_\alpha = \log_2$ in \cref{eq:rw_org_mlog}, respectively. 
We refer to the network with binary/decimal base as SPDNet-Log2/SPDNet-Log10. 
Note that when $\log_\alpha=\log_e$, \cref{eq:rw_org_mlog} is reduced to the vanilla matrix logarithm, and the network is our baseline, \ie SPDNet. 
We conduct 10-fold experiments on the HDM05 and FPHA datasets and set the learning rate to $5e^{-2}$ and $1e^{-2}$, respectively, while keeping the other settings consistent with previous experiments.
The results are presented in \cref{tab:fix_base_alog}. 
We observe that the fixed logarithms show similar or slightly worse results than the vanilla $\log_e$, while our ALog shows consistent improvement.
Besides, $\log_{10}$ does not converge in the FPHA dataset. 
In fact, $\log_{10}$ could shrink the gradient, slowing down convergence, especially under a small learning rate. 
In contrast, our ALog maintains consistent effectivity. 
In summary, our ALog can respect vibrant geometry induced by $\mlog$ and thus benefit SPD network learning.




