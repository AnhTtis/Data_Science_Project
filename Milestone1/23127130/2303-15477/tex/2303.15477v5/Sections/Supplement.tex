
\renewcommand{\appendixname}{Supplementary Material}
\crefalias{section}{suplement}
\crefalias{subsection}{suplement}
\appendices
% \onecolumn



% \section{Preliminaries}
% \label{app:sec:preliminaries}
% \subsection{Smooth Manifolds}
% We first recap some basic definitions related to this work on smooth manifolds.
% Please refer to \cite{loring2011introduction,lee2013smooth} for in-depth understanding. 

% The most important properties of manifolds are locally Euclidean, which are described by coordinate systems.

% \begin{definition}[Coordinate Systems, Charts, Parameterizations] \label{def:Parameterization}
% A topological space $\calM$ is locally Euclidean of dimension $n$ if every point in $\calM$ has a neighborhood $U$ such that there is a homeomorphism $\phi$ from $U$ onto an open subset of $\mathbb{R}^{n}$. 
% We call the pair $\{ U, \phi: U \rightarrow \mathbb{R}^{n}\}$ as a chart, $U$ as a coordinate neighborhood, the homeomorphism $\phi$ as a coordinate map or coordinate system on $U$, and $\phi^{-1}$ as a parameterization of $U$. 
% \end{definition}

% Intuitively, a coordinate system is a bijection that locally identifies the Euclidean space with the manifold.
% It locally preserves the most basic properties in a manifold, the topology. 
% Topological manifolds, which are foundations of smooth manifolds, can be defined.
% \begin{definition}[Topological Manifolds]
%     A topological manifold is a locally Euclidean, second countable, and Hausdorff topological space.
% \end{definition}

% Compatibility is further required in smooth manifolds to define smooth structures or operations.
% \begin{definition}[$C^{\infty}$-compatible] \label{def:compatible}
% Two charts $\{ U, \phi_1: U \rightarrow \mathbb{R}^{n} \},\{ V, \phi_2: V \rightarrow \mathbb{R}^{n} \}$ of a locally Euclidean space are $C^{\infty}$-compatible if the following two composite maps
% \begin{equation}
%     \begin{aligned}
%         \phi_1 \circ \phi_2^{-1} &: \phi_2(U \cap V) \rightarrow \phi_1(U \cap V), \\
%         \quad \phi_2 \circ \phi_1^{-1} &: \phi_1(U \cap V) \rightarrow \phi_2(U \cap V)        
%     \end{aligned}
% \end{equation}
% are $\cinf$.
% \end{definition}

% By abuse of notation, we view $\phi$ alternatively as a chart or map according to the context, and abbreviate $C^{\infty}$-compatible as compatible.

% \begin{definition}[Atlases] \label{def:atlas}
% A $C^{\infty}$ atlas or simply an atlas on a locally Euclidean space $\calM$ is a collection $\calA=\{ \{ U_{\alpha}, \phi_{\alpha} \} \}$ of pairwise $\cinf$-compatible charts that cover $\calM$.
% \end{definition}

% An atlas $\mathcal{A}$ on a locally Euclidean space is said to be maximal if it is not contained in a larger atlas. 
% With a maximal atlas, smooth manifold can be defined.
% \begin{definition}[Smooth Manifolds] 
% A smooth manifold is defined as a topological manifold endowed with a maximal atlas.
% \end{definition}

% We call the maximal atlas of a smooth manifold its differential structure.
% In addition, every atlas $\calA$ is contained in a unique maximal atlas $\calA^+$ \cite{loring2011introduction}.
% Therefore, an atlas can be used to identify the differential structure of a smooth manifold.
% In this paper, manifolds always mean smooth manifolds.
% Now, we can define the smoothness of a map between manifolds.
% \begin{definition}[Smoothness] \label{def:smoothness}
% Let $\calN$ and $\calM$ be smooth manifolds, and $f: \calN \rightarrow \calM$ a continuous map, $f(\cdot)$ is said to be $\cinf$ or smooth, if there are atlases $\calA_n$ for $\calN$ and $\calA_m$ for $\calM$ such that for every chart $\{ U, \phi \}$ in $\calA_n$ and $\{ V, \psi \}$ in $\calA_m$, the map
% \begin{equation}
%     \psi \circ F \circ \phi^{-1}: \phi\left(U \cap f^{-1}(V)\right) \rightarrow \mathbb{R}^{m}
% \end{equation}
% is $C^{\infty}$.
% \end{definition}
% In elementary calculus, smooth functions have derivatives.
% In manifolds, derivatives are generalized into differential maps.
% \begin{definition} [Differential Maps]
%     Let $f: \calN \rightarrow \calM$ be a $C^{\infty}$ map between two manifolds. At each point $p \in \calN$, the map $f$ induces a linear map of tangent spaces, called its differential at $p$,
%     \begin{equation}
%         f_{*,p}: T_p \calN \rightarrow T_{f(p)} \calM.
%     \end{equation}
%     $f_{*,p}$ can be locally represented by the Jacobian matrix under a chart $\{ U, \phi \}$ about $p$ and a chart $\{ V, \psi \}$ about $f(p)$,
%     \begin{equation}
%         f_{*,p} := \frac{\partial f}{\partial x} := \frac{\partial \psi f \phi^{-1}}{\partial x},
%     \end{equation}
%     where $\frac{\partial f}{\partial x}$ is called the derivative (Jacobian matrix) of $f$ under the charts of $\{ U, \phi \}$ and $\{ V, \psi \}$.
% \end{definition}
% With the definition of smoothness, it is possible to define smooth algebraic structures on a manifold, \ie Lie groups.
% Intuitively, a Lie group is an integration of algebra (group) and geometry (manifold).
% \begin{definition}[Lie Groups] \label{def:lie_group}
% A manifold is a Lie group, if it forms a group with a group operation $\odot$ such that $m(x,y) \mapsto x \odot y$ and $i(x) \mapsto x_{\odot}^{-1}$ are both smooth, where $x_{\odot}^{-1}$ is the group inverse of $x$.
% \end{definition}
% \subsection{Riemannian Manifolds}
% When manifolds are endowed with Riemannian metrics, various Euclidean operators can find their counterparts in manifolds.
% A plethora of discussions can be found in \cite{do1992riemannian}.

% \begin{definition}[Riemannian Manifolds] \label{def:riem_manifold}
% A Riemannian metric on $\calM$ is a smooth symmetric covariant 2-tensor field on $\calM$, which is positive definite at every point.
% A Riemannian manifold is a pair $\{\calM,g\}$, where $\calM$ is a smooth manifold and $g$ is a Riemannian metric.
% \end{definition}
% As a basic fact in differential geometry, every smooth manifold is a Riemannian manifold \cite[Prop.~2.10]{do1992riemannian}.
% Therefore, in the following, we will alternatively use manifolds or Riemannian manifolds.
% \begin{definition} [Pullback Metrics] \label{def:pullback_metrics_app}
%     Suppose $\calM,\calN$ are smooth manifolds, $g$ is a Riemannian metric on $\calN$, and $f:\calM \rightarrow \calN$ is smooth.
%     Then the pullback of a tensor field $g$ by $f$ is defined point-wisely,
%     \begin{equation} \label{eq:pullback_metrics_app}
%         (f^*g)_p(V_1,V_2) = g_{f(p)}(f_{*,p}(V_1),f_{*,p}(V_2)),
%     \end{equation}
%     where $p$ is an arbitrary point in $\calM$, $f_{*,p}(\cdot)$ is the differential map of $f$ at $p$, and $V_1,V_2$ are tangent vectors in $T_p\calM$.
%     If $f^*g$ is positive definite, it is a Riemannian metric on $\calM$, called the pullback metric defined by $f$.
% \end{definition}

% \begin{definition}[Isometries] \label{def:isometry}
% If $\{M, g\}$ and $\{\widetilde{M}, \widetilde{g}\}$ are both Riemannian manifolds, a smooth map $f: M \rightarrow$ $\widetilde{M}$ is called a (Riemannian) isometry if it is a diffeomorphism that satisfies $f^{*} \tilde{g}=g$.
% \end{definition}
% If two manifolds are isometric, they can be viewed as equivalent.
% Riemannian operators in these two manifolds are closely related. 
% \begin{definition}[Bi-invariance] \label{def:bi_invariance}
% A Riemannian metric $g$ over a Lie group $\{G, \odot\}$ is left-invariant, if for any $x,y \in G$ and $V_1,V_2 \in T_x\calM$, 
% \begin{equation}
%     g_y(V_1,V_2) = g_{L_x(y)}(L_{x*,y}(V_1), L_{x*,y}(V_2)),
% \end{equation}
% where $L_x(y) = x \odot y$ is left translation, and $L_{x*,y}$ is the differential map of $L_x$ at $y$.
% Right-invariance is defined similarly.
% A metric over a Lie group is bi-invariant if both left- and right-invariant. 
% \end{definition}
% Bi-invariant metrics are the most convenient metrics on Lie, as they enjoy many excellent properties \cite[Ch.~V]{sternberg1999lectures}. 

% The exponential \& logarithmic maps and parallel transportation are also crucial for Riemannian approaches in machine learning.
% To bypass the notation burdens caused by their definitions, we review the geometric reinterpretation of these operators \cite{pennec2006riemannian, do1992riemannian}.
% In detail, in a manifold $\calM$, geodesics correspond to straight lines in the Euclidean space.
% A tangent vector $\overrightarrow{x y} \in T_x\calM$ can be locally identified to a point $y$ on the manifold by geodesic starting at $x$ with initial velocity of $\overrightarrow{x y}$, i.e. $y=\rieexp_x(\overrightarrow{x y})$.
% On the other hand, the logarithmic map is the inverse of the exponential map, generating the initial velocity of the geodesic connecting $x$ and $y$, i.e. $\overrightarrow{x y}=\rielog_x(y)$.
% These two operators generalize the idea of addition and subtraction in Euclidean space.
% For the parallel transportation $\pt{x}{y}(V)$, it is a generalization of parallelly moving a vector along a curve in Euclidean space.
% we summarize the reinterpretation in \cref{tb:reinter_riem_operators}.
% \begin{table}
%     \centering
%     \caption{Reinterpretation of Riemannian Operators.}
%     \label{tb:reinter_riem_operators}
%     \begin{tabular}{ccc}
%     \toprule
%     Operations & Euclidean spaces & Riemannian manifolds \\
%     \midrule
%     Straight line & Straight line & Geodesic \\
%     Subtraction & $\overrightarrow{x y}=y-x$ & $\overrightarrow{x y}=\log _x(y)$ \\
%     Addition & $y=x+\overrightarrow{x y}$ & $y=\exp _x(\overrightarrow{x y})$ \\
%     Parallelly moving & $V \rightarrow V$ & $\pt{x}{y}(V)$\\
%     \bottomrule
%     \end{tabular}
% \end{table}

% \subsection{LEM and LCM on the SPD Manifold} \label{subsec:geom_of_spd}
% This subsection briefly reviews LEM \cite{arsigny2005fast} and LCM \cite{lin2019riemannian}.

% Matrix logarithm $\mln(\cdot): \spd{n} \rightarrow \sym{n}$ and $\cln(\cdot): \spd{n} \rightarrow \tril{n} $ are defined as,
% \begin{align}
%      \mln(S) &= U \ln(\Sigma) U^\top,\\
%      \cln(P)&=\clnchart(\scrL(S)),
% \end{align}
% where $S=U \Sigma U^\top$ is the eigendecomposition, $L = \scrL(S)$ is the Cholesky decomposition ($S=LL^\top$), $\clnchart(L) = \lfloor L \rfloor + \ln(\bbD(L))$ is a coordinate system from the $\cho{n}$ manifold onto the Euclidean space $\tril{n}$ \cite{lin2019riemannian}, $\lfloor L \rfloor$ is the strictly lower triangular part of $L$, $\bbD(L)$ is the diagonal elements, and $\ln(\cdot)$ is the diagonal natural logarithm.
% We name $\cln$ as the Cholesky logarithm, since we will rely on it many times in the following proof.
% Note that topologically, $\tril{n} \simeq \sym{n} \simeq \bbR{n(n+1)/2}$, since their metric topology all comes from the Euclidean metric tensor.
% Based on matrix logarithm, \cite{arsigny2005fast} proposed LEM by Lie group translation, while based on Cholesky logarithm, \cite{lin2019riemannian} proposed LCM, by an isometry between $\spd{n}$ and $\cho{n}$.
% In the main paper, we argued that LEM and LCM are basically the same, in the sense of high-level mathematical abstraction.

% The Riemannian metric and associated geodesic distance under the LEM are defined by:
% \begin{align}
%     \label{eq:metric_lem} \glem_{S}(V_1,V_2) &= \geuc( {\mln}_{*,S} ( V_{1}), {\mln}_{*,S} ( V_{2}) ),\\
%     \dlem(S_1, S_2) &= \| \mln(S_1) - \mln(S_2)\|_\rmF,
% \end{align}
% where $S \in \spd{n}$, $V_1, V_2 \in T_S\spd{n}$ are tangent vectors, ${\mln}_{*,S}(\cdot)$ is the differential map of matrix logarithm at $S$, $\geuc$ is the standard Euclidean metric tensor, and $\| \cdot \|_F$ is Frobenius norm.
% Note that since $\geuc$ is the same at every point, we simply omit the subscript.
% Besides, element-wise and scalar multiplication are also induced by $\mln$:
% \begin{align}
%     S_1 \mlnMul S_2 & = \mexp(\mln(S_1) + \mln(S_2)),\\
%     \lambda \mlnMulScalar S & = \mexp(\lambda \mln(S)),
% \end{align}
% where $\mexp(X) = U \exp(\Sigma) U^\top$ is the matrix exponential.
% As is proven in \cite{arsigny2005fast}, $\{ \spd{n}, \mlnMul\}$ and $\{ \spd{n}, \mlnMul,\mlnMulScalar\}$ form a Lie group and vector space, respectively.
% Besides, the metric $\glem$ defined on Lie group $\{ \spd{n}, \mlnMul\}$ is bi-invariant.

% The Riemannian metric and geodesic distance under LCM is
% \begin{align}
%      \label{eq:metric_lcm} 
%     \glcm_S(V_{1}, V_{2} ) &= \gcm_L(L(L^{-1} V_1 L^{-\top})_{\frac{1}{2}},L(L^{-1}V_2L^{-\top})_{\frac{1}{2}}),\\
%     \dlcm(S_1, S_2) &= \{\|\lfloor L_1\rfloor-\lfloor L_2\rfloor\|_{\rmF}^{2}\\
%     &+\|\ln( \bbD(L_1))-\ln (\bbD(L_2))\|_{\rmF}^{2}\}^{\frac{1}{2}}, 
% \end{align}
% where $S \in \spd{n}$, $V_1,V_2 \in T_S\spd{n}$, $X_{\frac{1}{2}} = \lfloor X \rfloor + \bbD(X)/2$, and $\gcm_L(\cdot, \cdot)$ is the Riemannian metric on $\cho{n}$, defined as
% \begin{align} \label{eq:metric_cholesky}
%     \gcm_{L}(X, Y) 
%     &= \geuc (\lfloor X\rfloor,\lfloor Y\rfloor)\\ 
%     &+ \geuc(\bbD(L)^{-1} \bbD(X), \bbD(L)^{-1} \bbD(Y)).
% \end{align}
% The group operation in \cite{lin2019riemannian} is defined as follows:
% \begin{equation}
%     S_1 \clnMul S_2 = \scrL^{-1} (\lfloor L_1 \rfloor + \lfloor L_2 \rfloor + \bbD(L_1)\bbD(L_2)),
% \end{equation}
% where $\scrL^{-1}(\cdot)$ is the inverse map of Cholesky decomposition.
% $\{\spd{n}, \clnMul\}$ is proven to be a Lie group \cite{lin2019riemannian}.
% Similar to LEM, $\glcm$ is bi-invariant.

\section{Additional Discussions on the ALEM}
In this section, we present additional discussions on our ALEM.
All the proofs are placed in \cref{app:proofs}.
\subsection{Well-definedness of General Matrix Logarithm}
\label{app:subsec:well_difined_glog}

In the main paper, due to the page limit, we did not clarify specific correspondence between eigenvalue and diagonal logarithm.
Here, we present detailed clarification.
Note that in implementation, like PyTorch or Matlab, this is no need to worry about this issue, as the outputs of eigendecomposition are always ordered.

We rewrite the eigendecomposition as $S = \sum \sigma_i E_i$ where $E_i=u_i u_i^\top$and $u_i$ is the corresponding eigenvector in $U$. Let $S$ be an $n \times n $ SPD matrix and $P_n$ be a set of all permutations of $\{n,\cdots, 1\}$, known as a permutation group. Changing the order of $\{n,\cdots, 1\}$ can be viewed as a permutation, so we use $\pi \in P_n$ to represent the corresponding changed order.

Assume the eigenvalues $\sigma_i$ are sorted in ascending order, \emph{i.e.,} $\sigma_1 \leq \cdots, \leq \sigma_n$. To clarify the definition of "the $i$-th eigenvalues", we refer to the $i$-th eigenvalue to the $i$-th pair from the ordered eigenpair sequence $(\sigma_1,u_1),\cdots,(\sigma_n,u_n)$. Since each eigenvector $u_{i}$ is unique, it is safe to say the eigenvalues are ordered, and the $i$-th eigenvalue/eigenvector pair is unique.

Let $\log_\alpha(\Sigma)$ denotes imposing scalar logarithm $\log_{a_i}$ to the $i$-th eigenvalue $\sigma_i$. Then $\phi_{m l o g}$ is rewritten as $\phi_{m l o g}(S)=\sum \log _{a_i}^{\sigma_i} E_i$, where $S = \sum \sigma_i E_i$. In this way, $\phi_{m l o g}$  is clearly well-defined. By definition, we can observe that the output of $\phi_{m l o g}$ does not depend on the order in eigendecomposition.


Suppose there are two eigendecomposition with different orders, \emph{i.e.,} $S= U\Sigma U^\top = \tilde{U} \tilde{\Sigma}\tilde U^\top$ where $\tilde{U}, \tilde\Sigma$ are the rearrangement of $U,\Sigma$. There exists a $\pi \in P_n$  such that for each $j$, there is a unique $i$, satisfying $\tilde u _j = u_{\pi(i)}$ and $\tilde\sigma_j=\sigma_{(i)}$. We then have $\sum \log _{a_i}^{\sigma_i} E_i$  for $S= U\Sigma U^\top$ and $\sum \log_{a_{\pi(i)}}^{\sigma_{\pi(i)}} E_{\pi(i)}$ for $S=\tilde U\tilde{\Sigma}\tilde U^\top$, which indicates the two eigendecomposition are equivalent. 

\subsection{Learning Base Vectors by Riemannian Optimization} \label{app:subsec:geom}

We focus on a single element $a$ of $\alpha$ in Eq.(31) in the main paper. As discussed in the main paper, $a$ satisfying $a>0 \& a \neq 1$. The condition of $a \neq 1$ can be further waived since we can set $a=1+\epsilon$ if $a=1$. Then, there is only one constraint about positivity.
A geometric way to deal with positivity is to view $a$ as a point in a 1-dimensional SPD manifold.
We call this strategy GEOM.
Then, we have the following updating formula for GEOM.
\begin{proposition} \label{propos:param_by_geom}
    Viewing a positive scalar $a$ as a point in a 1-dimensional SPD manifold, we have the following updating formula for Riemannian stochastic gradient descent (RSGD).
    \begin{equation} \label{eq:update_pos_scalar}
        a^{(t+1)} = a^{(t)}e^{-\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L},
    \end{equation}
    where $\nabla_{a^{(t)}} L$ is the Euclidean gradient of $a$ at $a^{(t)}$, $\gamma^{(t)}$ is the learning rate, and $e^{(\cdot)}$ is the natural exponentiation.
\end{proposition}

Besides, by \cref{eq:update_pos_scalar}, we could prove that GEOM is equivalent to DIV, which is given in the following proposition.
\begin{proposition} \label{props: geom_equivalent_div}
    For parameters learning in $\mlog$, optimizing the base vector $\alpha$ by RSGD is equivalent to optimizing the divisor matrix $B$ by Euclidean stochastic gradient descent (ESGD).
\end{proposition}

\section{Implementation Details of Additional Applications}
\label{sub:sec:imp_details}

\subsection{Details on the NTU60 Dataset}
\textbf{NTU60} \cite{shahroudy2016ntu}. It has 56,880 sequences of 3D skeleton data classified into 60 classes, where each frame contains the 3D coordinates of 25 body joints.
We follow the cross-view protocol \cite{shahroudy2016ntu}.
Following \cite{katsman2023riemannian}, we model each sequence as a $75 \times 75$ covariance matrix.

\subsection{Implementation Details}
As reported in the main paper, MUL shows the best performance. Therefore, we view $A$ in Eq. (32) as the parameter for all experiments. In the following, we discuss in detail the specific implementation of each method.

\textbf{LieBN:}
We follow the official code\footnote{https://github.com/GitZH-Chen/LieBN} to implement the experiments.
The learning rate is $5e^{-2}$.
Since our LieBN-ALEM shows early convergence, we set the training epochs as 150, 50, and 30 for [93, 30], [93, 70, 30], and [93, 70, 50, 30] architectures.
Other settings remain the same.

\textbf{RResNet:}
We follow the official code\footnote{https://github.com/CUAI/Riemannian-Residual-Neural-Networks} to implement the experiments.
For the HDM05 dataset, we use the Riemannian SGD \cite{becigneul2018riemannian} with a $5e^{-2}$ learning rate and a training epoch of 200.
For the NTU60 dataset, we use the Riemannian AMSGrad \cite{becigneul2018riemannian} with a $1e^{-2}$ learning rate and a training epoch of 50.
We adopt the architectures of [93, 30] and [75, 30] on these two datasets.

\textbf{Gyro MLR:}
Since the code of gyro MLR is not publicly available, we carefully re-implement the gyro MLR in \cite{nguyen2023building}. 
We adopt an architecture of [75, 30] under an SGD optimizer. The batch size and training epoch are 30 and 200, respectively.

\section{Proofs} \label{app:proofs}

\begin{proof} [Proof of Thm. III. 1]
Let us first deal with the $(\alpha,\beta)$-LEM.
Putting the differential of matrix logarithm into the definition of pullback metrics, one can directly obtain the result.

Now, let us focus on LCM.
Denote $\glcm$, $\geuc$, and $\gcm$ as LCM, standard Euclidean metric, and the metric on the Cholesky manifold \cite{lin2019riemannian}, respectively.
As shown in Sec. II, $\{\spd{n}, \glcm\}$ is isometric to $\{\cho{n},\tilde{g}\}$, with Cholesky decomposition $\scrL$ as an isometry.
This is exactly how \cite{lin2019riemannian} derived LCM.
So, the key point lies in the Cholesky metric $\tilde{g}$.
Let us reveal why it is defined in this way.
In fact, $\tilde{g}$ is derived from $\geuc$ by $\clnchart$.
Simple computations show that
\begin{equation} \label{eq:diff_cho_ln_chart}
    \varphi_{ln *,L}(V) = \lfloor V \rfloor + \mathbb{D}^{-1}(L)\mathbb{D}(V),
\end{equation}
where $V \in T_L\cho{n}$.
By \cref{eq:diff_cho_ln_chart}, LCM can be rewritten as
\begin{equation}
    \gcm_L(X,Y) = \geuc(\varphi_{ln *,L}((X),\varphi_{ln *,L}((Y)).
\end{equation}
Therefore, $\clnchart: \cho{n} \rightarrow \tril{n}$ is an isometry.
By transitivity, $\cln: \spd{n} \rightarrow \tril{n}$ is also an isometry.
\end{proof}

\begin{proof}[Proof of Cor. III.2]
    As $\bbR{n(n+1)/2} \cong \tril{n} \cong \sym{n}$, LCM is therefore a pullback metric from the standard Euclidean space $\sym{n}$.
    Secondly, as every Euclidean space is naturally isometric, $\biparamLEM$ is therefore also a pullback metric from the standard Euclidean space $\sym{n}$.
\end{proof}

\begin{proof} [Proof of Lem. III.3]
    By the definitions, Cases 1 and 3 can be directly obtained.
    Now, let us focus on Case 2.
    As every Euclidean space is an Abelian Lie group, $\{\spd{n}, \phiMul \}$ is an Abelian Lie group. 
    The geodesic distance is also obvious, as $\phi$ is a Riemannian isometry.
    
    We only need to prove Riemannian exponentiation, logarithm, and parallel transport.
    Note that in Euclidean space $\bbR{n}$, for any $x,y \in \bbR{n}$ and tangent vector $v \in T_x\bbR{n} \cong \bbR{n}$, we have the following
    \begin{align}
        \rieexp_x v &= x+v,\\
        \rielog_x y &= y-x,\\
        \pt{x}{y} v &= v.
    \end{align}
    By the isometry of $\phi$, we can readily obtain the results.
\end{proof}
\begin{proof} [Proof of Prop. III.4]
    Obviously, $\mgexp$ is the inverse of $\mlog$. 
    What followed is to verify the smoothness of $\mlog$ and its inverse.
    
    According to Theorem 8.9 in \cite{magnus2019matrix}, the map producing an eigenvalue or an eigenvector from a real symmetric matrix is $\cinf$.
    Recalling $\mlog$ and its inverse map $\mgexp$, it's obvious that they are comprised of arithmetic calculation or composition of some smooth maps.
    Therefore, $\mlog$ ($\mgexp$) is a diffeomorphism.
\end{proof}

\begin{proof} [Proof of Thm. III.6]
    This is a direct result of Lem. III.3.
\end{proof}

\begin{proof} [Proof of Prop. III.8]
    The differentials of $\mgexp$ and $\mlog$ can be derived similarly. 
    In the following, we only present the process of deriving the differential of $\mlog$. 
    
    First, Let us recall the differentials of eigenvalues and eigenvectors.
    Theorem 8.9 in \cite{magnus2019matrix} offers their Euclidean differentials, which are the exact formulations for differentials under the canonical base on SPD manifolds.
    So, we can readily obtain the differentials of eigenvalues and eigenvectors as the following:
    \begin{align}
        \label{eq: diff_eig_value} \sigma_{*,S} (V) &= u^{\top} V u,\\
        \label{eq: diff_eig_vec} u_{*,S} (V) &= (\sigma I- S)^{+} V u,
    \end{align}
    where $S u = \sigma u, u^\top u = 1$, and $()^+$ is the Moore–Penrose inverse.

    By the RHS of Eq.(31) in the main paper, the differential map of $\mlog$ is
    \begin{equation} \label{eq:diff_mlog}
    \begin{aligned}
        &\diffmlog{S}(V) \\
        &= U_{*,S} (V) \log(\Sigma)U^\top + U (\log{\Sigma})_{*,S} (V) U^\top \\
        & + U \log(\Sigma) U^\top_{*,S} (V) \\
        &= Q+Q^\top + U^\top (\log{\Sigma})_{*,S} (V) U,
    \end{aligned}
    \end{equation}
    where $Q= U_{*,S} (V) \log(\Sigma)U^\top$.

    For the differential of diagonal logarithm, it is 
    \begin{equation} \label{eq: diff_ln}
        \log_{*,S}{\Sigma} = A \frac{1}{\Sigma} \Sigma_{*,S}.
    \end{equation}

    Denote the eigenvectors and eigenvalues of $S=U\Sigma U^\top$ as $U = (u_1,\cdots,u_n)$ and $\Sigma=\diag(\sigma_1,\cdots,\sigma_n)$.
    By \cref{eq: diff_eig_value}-\cref{eq: diff_ln}, the differential of $\mlog$ can be obtained.
\end{proof}
\begin{proof} [Proof of  Prop. III.9]
    Following the notations in the proposition, we make the following proof.
    By abuse of notation, in the following, we omit the wide tilde $\widetilde{~}$.
   
    Now, we proceed to deal with the differential of $\mgexp$.
    We rewrite the formula of $\mgexp$ as
    \small
    \begin{align}
        & \mgexp(X) \\
        &= U \balpha(\Sigma) U^\top,\\
        &= U \diag(e^{\ln^{a_1}\sigma_1},\cdots,e^{\ln^{a_n}\sigma_n})U^\top,\\
        &= U \diag(\sum_{k=0}^{\infty}\frac{(\ln^{a_1}\sigma_1)^k}{k!},\cdots,\sum_{k=0}^{\infty}\frac{(\ln^{a_n}\sigma_n)^k}{k!})U^\top,\\
        \label{eq:rw_mgexp_last2} &= U (\sum_{k=0}^{\infty}\frac{B\Sigma}{k!}) U^\top,\\
        \label{eq:rw_mgexp} &= \sum_{k=0}^{\infty}\frac{PX}{k!}
    \end{align}
    \normalsize
     where $P=UBU^\top$, with $U$ from eigendecomposition $X=U\Sigma U^\top$ and diagonal matrix $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$.
     By the properties of normed vector algebras \cite[Prop.~15.14]{loring2011introduction}, we can obtain the last equation.
     Then, we can compute the differential of $\mgexp$ by curves.
     Given a curve $c$ on $\sym{n}$ starting at $X$ with initial velocity $W \in T_X\sym{n}$, we have
     \begin{equation}
        \begin{aligned}
            \diffmgexp{X}(W) 
            &= \left. \frac{d}{dt} \right |_{t=0} \mgexp \circ c(t)\\
            \label{eq:diff_mgexp_series_proof_stp2}
            &= \left. \frac{d}{dt} \right |_{t=0}\sum_{k=0}^{\infty}\frac{Pc(t)}{k!}.
        \end{aligned} 
    \end{equation}
    By a term-by-term differentiation, we have
    \begin{equation} 
        \begin{aligned} \label{eq:diff_last2step_mgexp}
            &\diffmgexp{X}(W)\\ 
            &= \sum_{k=1}^{\infty} \frac{1}{k !}(\sum_{l=0}^{k-1}(PX)^{k-l-1} \left. \frac{d}{dt} \right|_{t=0}(Pc) (PX)^l).
        \end{aligned}
    \end{equation}
    By the chain rule, we have
    \begin{equation} \label{eq:chain_rule_Pc}
        \left. \frac{d}{dt} \right|_{t=0}(Pc) = P'(0)X + PV.
    \end{equation}
    $P'(0)$ is obtained by
    \begin{equation}
    \label{eq:diff_P_at0}
        \begin{aligned}
            P'(0) 
            &= (UBU^\top)'(0)\\
            &= U'(0)BU^\top + UBU^{\top '}(0)\\
            &= D_U B U^\top + U B D_U^\top,
        \end{aligned}
    \end{equation}

    where $D_U$ is derived from the differential of eigenvectors,
    \begin{equation} \label{eq:D_U}
    D_U = (\begin{array}{ccc}
             (\sigma_1 I-S)^+ V u_1 & \cdots & (\sigma_n I-S)^+ V u_n
        \end{array}).
    \end{equation}
    Applying \cref{eq:chain_rule_Pc}, \cref{eq:diff_P_at0} and \cref{eq:D_U} into \cref{eq:diff_last2step_mgexp}, we have the differential of $\mgexp$.
\end{proof}

\begin{proof} [Proof of Prop. IV.1]
    Obviously, the metric space $\{\spd{n},\dalem\}$ is isometric to the space $\sym{n}$ endowed with the standard Euclidean distance.
    Therefore, the weighted Fréchet mean of $\{S_i\}$ in $\spd{n}$ corresponds to the weighted Fréchet mean of associated points $\{\mlog(S_i)\}$ in $\sym{n}$.
    The weighted Fréchet means in Euclidean spaces are clearly the familiar weighted means.
\end{proof}

\begin{proof} [Proof of Prop. IV.2]
    As $\mlog$ is a Riemannian isometry and $\sym{n}$ is bi-invariant, ALEM is therefore bi-invariant.
\end{proof}


\begin{proof} [Proof of Prop. IV.3]
    Following the notations in this proposition, we make the following proof.
    The LHS can be rewritten as 
    \begin{equation}
        \begin{aligned}
            (\mathrm{FM}(S_1^\beta,\cdots S_m^\beta)) 
            &= \mgexp(\sum_{i=1}^{m} \frac{1}{m}\beta\mlog(S_i))\\
            &= \mgexp(\beta\sum_{i=1}^{m} \frac{1}{m}\mlog(S_i))\\
            &= [\mgexp(\sum_{i=1}^{m} \frac{1}{m}\mlog(S_i))]^\beta\\
            &= (\mathrm{FM}(S_1,\cdots S_m))^\beta.
        \end{aligned}
    \end{equation}
\end{proof}

\begin{proof}[Proof of Prop. IV.4]
    Recalling Prop. IV.4, (U1) and (U2) obviously hold.
    
    When SPD matrices $\{A_i\}_{i \leq n}$ commutes, we have
    \begin{equation} \label{eq:fm_commute_alem}
        \fm(\{A_i\})= (\sum A_i)^{\frac{1}{n}}.
    \end{equation}
    With \cref{eq:fm_commute_alem}, (V1)-(v4) can be easily proved.
\end{proof}

\begin{proof} [Proof of Prop. IV.5]
    Obviously, for a given SPD matrix $S$,
    \begin{align}
        \label{eq:mlog_rotate} \mlog(R S R^\top) &= R \mlog(S) R^\top,\\
        \label{eq:mlog_scale} \mlog(s^2 S ) &=  U(\log(s^2 I)+\mlog(\Sigma))U^\top,
    \end{align}
    where $S=U \Sigma U\top$ is the eigendecomposition.
    We can obtain the results with \cref{eq:mlog_rotate} and \cref{eq:mlog_scale}.
\end{proof}

\begin{proof}[Proof of Prop. IV.6]
    The three equations can be directly obtained.
\end{proof}

\begin{proof} [Proof of Prop. Prop. VI.1]
    The divative of $L$ w.r.t. $S$ is the so-called Daleck\u ii-Kre\u in formula presented in \cite[P. 60]{bhatia2009positive}.
    Now, let us focus on the gradient w.r.t $A$.
    Differentiating both sides of Eq. (32) in the main paper:
    \begin{equation}
        \diff X = (*) + U \diff A \odot \log(\Sigma)U^T,
    \end{equation}
    where $(*)$ means other parts related to $\diff U$ and $\diff \Sigma$.
    According to the invariance of first-order differential form, we have,
    \begin{align}
        & \nabla_{X} L : \diff X \nonumber \\ 
        &= \nabla_{S} L:\diff S + \nabla_{X} L: (U \diff A \odot \log(\Sigma)U^T) \\
        \label{eq:diif_last_row}
        &= \nabla_{S}L :\diff S + [U^\top (\nabla_{X} L) U] \odot \log(\Sigma):\diff A,
    \end{align}
    where $A:B=\tr(A^\top B)$ is the Euclidean Frobenius inner product.
    From the second term on the RHS of \cref{eq:diif_last_row}, we can obtain the gradient w.r.t $A$.
\end{proof}

\begin{proof}[Proof of Prop. VIII.1]
    The derivation follows the same logic as Prop. VI.1.
    We only need to show the derivation of w.r.t. $A$.
    Similar to Prop. VI.1, we have the following:
    \begin{equation}
        \diff X = (*) + U \diff A \odot \left( \balpha (X) \frac{-\Sigma}{A^2} \right) U^T,
    \end{equation}
    \begin{align}
    &\nabla_{X} L : \diff X \nonumber \\
    &= \nabla_{S}L :\diff S + [U^\top (\nabla_{X} L) U] \odot \left( \balpha (X) \frac{-\Sigma}{A^2} \right):\diff A.
    \end{align}    
\end{proof}

\begin{proof}[Proof of Thm. VIII.2]
    Following \cite{nguyen2022gyro,nguyen2023building}, we first define gyro structures under ALEM:
    \begin{align}
        \label{eq:gyro_addtion}
        P \oplus Q &= \rieexp_{P}\left(\pt{E}{P} \left(\rielog _{E}(Q)\right)\right), \\
        \label{eq:gyro_automorphism}
        \gyr[P, Q] R &= (\ominus(P \oplus Q)) \oplus(P \oplus(Q \oplus R)),\\
        \label{eq:gyro_scalar_product}
        t \otimes P &= \rieexp_{E}\left(t \rielog _{E}(P)\right), \\
        \label{eq:gyro_inverse}
        \ominus P &= -1 \otimes P = \rieexp_{E}\left(- \rielog _{E}(P)\right), \\
        \label{eq:gyro_inner_product}
        \gyrinner{P}{Q}&=\left\langle \rielog _I (P), \rielog _I (Q)\right\rangle_{I},\\
        \label{eq:gyro_norm}
        \gyrnorm{P} &= \gyrinner{P}{P},\\
        \label{eq:gyro_distance}
        \gyrdist(P, Q) &= \gyrnorm{\ominus P \oplus Q},
    \end{align}
    where $P,Q,R \in \spd{n}$, and $I$ is the identity matrix.
    The above operations are called gyro addition, gyro automorphism, gyro scalar product, gyro inverse, gyro inner product, gyro norm, and gyrodistance. Simple computations show that \cref{eq:gyro_addtion} and \cref{eq:gyro_scalar_product} are the exact $\mlogMul$ and $\mlogMulScalar$.
    As indicated by Thm. III.6, $\{\spd{n}, \mlogMul, \mlogMulScalar \}$ forms a gyro vector space \cite[Def. 1]{nguyen2022gyrovector}.
    In the following proof, we follow the notations in \cite{nguyen2023building} to use $\odot$ and $\oplus$.

    The gyro MLR \cite{nguyen2023building} under ALEM is defined as 
    \begin{equation}
        \label{eq:mlr_alem}
        \begin{aligned}
        &p(y=k \mid S \in \spd{n}) \\
        &\propto \exp \left(\operatorname{sign}(\langle \tilde{A}_k, \rielog_{P_k}(S) \rangle_{P_k})\|\tilde{A}_k\|_{P_k} \bar{d} (S, H_{\tilde{A}_k, P_k}) \right),\\
    \end{aligned}
    \end{equation}
    where $P_k \in \spd{n}$ and $\tilde{A}_k \in T_{P_k} \spd{n}$.
    $\bar{d} (S, H_{\tilde{A}_k, P_k})$ is the margin distance to the SPD hyperplane $H_{\tilde{A}_k, P_k}$, which is defined as
    \begin{align}
        \label{eq:gyro_dist_hyperplane}
        \bar{d} (S, H_{\tilde{A}_k, P_k}) &=\sin (\angle S P_k Q^*) \gyrdist(S,P_k), \\
        Q^* &=\underset{Q \in H_{P_k, \tilde{A}_k} \backslash \{P_k\}}{\arg \max }\left(\cos (\angle S P_k Q)\right), \\
        \label{eq:gyro_consie}
        \cos (\angle S P_k Q) &= \frac{\gyrinner{ \ominus P_k \oplus Q}{\ominus P_k \oplus S}}{\gyrnorm{ \ominus P_k \oplus Q} \gyrnorm{ \ominus P_k \oplus S }},\\
        \label{eq:gyro_hyperplane}
        H_{\tilde{A}_k, P_k} &= \{S \in \spd{n}: \langle \rielog_{P_k} S, \tilde{A}_k \rangle_{P_k} =0\}.
    \end{align}
    \cref{eq:gyro_dist_hyperplane,eq:gyro_consie,eq:gyro_hyperplane} are called the SPD Pseudo-gyrodistance, SPD gyrocosine, and SPD hypergyroplane.

    For simplicity, we further omit the subscript $k$ in $P_k$ and $\tilde{A_k}$.
    \cref{eq:gyro_hyperplane} can be simplified:
    \begin{equation}
        \label{eq:alem_gyro_hyperplane_simplified}
        \begin{aligned}
            &\langle \rielog_{P} S, \tilde{A} \rangle_{P}\\
            &\stackrel{(1)}{=} \left\langle (\diffmlog{P})^{-1} (\mlog(P)-\mlog(S)) , \tilde{A} \right \rangle_{P} \\
            &\stackrel{(2)}{=} \left\langle \diffmlog{P} \circ (\diffmlog{P})^{-1} (\mlog(P)-\mlog(S)) , \diffmlog{P} \tilde{A} \right\rangle \\
            &= \left\langle \mlog(P)-\mlog(S) , \diffmlog{P} (\tilde{A}) \right\rangle.
        \end{aligned}
    \end{equation}
    The above derivation comes from the following.
    \begin{enumerate}[(1)]
        \item
        Riemannian logarithm under ALEM.
        \item 
        The definition of ALEM.
    \end{enumerate}

    Similarly, simple computation shows that \cref{eq:gyro_consie} can also be simplified as
    \begin{equation} \label{eq:alem_gyro_consine_simplified}
        \frac{\left \langle -\mlog(P)+\mlog(Q), -\mlog(P)+\mlog(S) \right \rangle}{\|  -\mlog(P)+\mlog(Q)\|_\rmF \|-\mlog(P)+\mlog(S) \|_\rmF}.
    \end{equation}

    Combined with \cref{eq:alem_gyro_hyperplane_simplified,eq:alem_gyro_consine_simplified}, \cref{eq:gyro_dist_hyperplane} is equivalent to the distance to the hyperplane in the Euclidean space.
    Therefore, \cref{eq:gyro_dist_hyperplane} has a closed form solution:
    \begin{equation}
    \label{eq:dist_hyperplane_final}
    \begin{aligned}
        \bar{d} (S, H_{\tilde{A}, P})
        &= \frac{\left|\left \langle \mlog(S)- \mlog(P), \bar{A} \right \rangle \right|}{\left \| \bar{A} \right \|_\rmF}\\
        &= \frac{\left|\left \langle \mlog(S)- \mlog(P), \bar{A} \right \rangle \right|}{\left \|  A \right \|_P},
    \end{aligned}
    \end{equation}
    where $\bar{A}=\diffmlog{P}(\tilde{A})$.
    Putting \cref{eq:dist_hyperplane_final} into \cref{eq:mlr_alem}, one can get the results.
\end{proof}

\begin{proof} [Proof of \cref{propos:param_by_geom}]
    Let's first review the update formulation in the RSGD \cite{bonnabel2013stochastic}, which is, geometrically speaking, a natural generalization of Euclidean stochastic gradient descent.
    For a minimization parameter $w$ on an $n$-dimensional smooth connected Riemannian manifold $\calM$, we have the following update, 
    \begin{equation} \label{eq:riem_sgd}
        w^{(t+1)}=\rieExp{w^{(t)}} (-\gamma^{(t)} \pi_{w^{(t)}}(\nabla_{w^{(t)}} L )),
    \end{equation} 
    where $\rieExp{w}(\cdot): T_w\calM \rightarrow \calM$ is the Riemannian exponential map, which maps a tangent vector at $w$ back into the manifold $\calM$, and $\pi_{w}(\cdot): \bbR{n} \rightarrow T_w\calM$ is the projection operator, projecting an ambient Euclidean vector into the tangent space at $w$.
    In the case of the SPD manifold, $\forall S \in \spd{n}, \forall X \in \bbR{n \times n}, \forall V \in \sym{n}$, the exponential map and projection operator is formulated as the following:
    \begin{align}
        \label{eq:spd_proj} \pi_{S}(X) &= S \frac{X+X^\top}{2}S,\\
        \label{eq:spd_exp} \rieExp{S}(V) &= S^{1/2} \mexp(S^{-1/2} V S^{-1/2}) S^{1/2},
    \end{align}
    where $\mexp(\cdot)$ is the matrix exponential.
    For more details about \cref{eq:spd_proj} and \cref{eq:spd_exp}, please kindly refer to \cite{yger2013review} and \cite{amari2016information}.
    Substitute \cref{eq:spd_proj} and \cref{eq:spd_exp} into \cref{eq:riem_sgd}, \cref{eq:update_pos_scalar} can be immediately obtained.
\end{proof}
\begin{proof} [Proof of \cref{props: geom_equivalent_div}]
    Without loss of generality, we focus on the equivalence between $b=B_{11}$ and $a=\alpha_{11}$.
    Let us denote $\log_{e}^{(\cdot)}$ as $\ln^{(\cdot)}$.
    Note that $b$ is essentially expressed as $b=\ln^a$.
    Supposing $b^{(t)} = \ln^{a^{(t)}}$, then we have
    \begin{equation}
        \begin{aligned}
            \nabla_{a^{(t)}} L 
            &= \nabla_{b^{(t)}} L \frac{\partial \ln^a }{a} |_{a^{(t)}}
            = \nabla_{b^{(t)}} L \frac{1}{a^{(t)}}.
        \end{aligned}
    \end{equation}

    By \cref{eq:update_pos_scalar}, $\ln^{a^{(t+1)}}$ is 
    \begin{equation}
        \begin{aligned}
            \ln^{a^{(t+1)}} 
            &= \ln^{a^{(t)}e^{-\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L}}\\
            &= \ln^{a^{(t)}} -\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L\\
            &= \ln^{a^{(t)}} - \gamma^{(t)} a^{(t)} (\nabla_{b^{(t)}} L/a^{t}) \\
            &= \ln^{a^{(t)}} -\gamma^{(t)}\nabla_{b^{(t)}} L\\
            &= b^{t} -\gamma^{(t)}\nabla_{b^{(t)}} L.
        \end{aligned}
    \end{equation}
    The last row is the updated formula of ESGD for $b$.
    
    Therefore, supposing $b^{(0)} = \ln^{a^{(0)}}$, then the optimization results after the overall training are equivalent.
\end{proof}


