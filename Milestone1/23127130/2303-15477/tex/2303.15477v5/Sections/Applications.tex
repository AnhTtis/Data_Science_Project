\section{Applications to SPD Neural Networks} \label{sec:ada_param_layers}
Since Riemannian metrics are the foundations of Riemannian learning algorithms, our ALEM has the potential to rewrite Riemannian algorithms, especially the algorithms based on LEM.
Besides, the base vector in $\mlog$ could bring vibrant diversity to our ALEM.
This adaptive mechanism could help the algorithm better fit with complicated manifold-valued data. 
Especially in Riemannian neural networks, as we will show, optimization of base vectors can be easily embedded into the standard backpropagation (BP) process.
Therefore, we focus on the applications of our metrics to SPD neural networks.

In the existing SPD neural networks, on activation or classification layers, SPD features would interact with the logarithmic domain by matrix logarithm \cite{huang2017riemannian,zhen2019dilated, chakraborty2020manifoldnet,nguyen2021geomnet, chen2023riemannian}.
The underlying mechanism of this interaction is that the matrix logarithm is an isomorphism, identifying the SPD manifold under LEM with the Euclidean space $\sym{n}$.
This projection can, therefore, maintain the LEM-based geometry of SPD features.
However, in deep networks, the geometry might be more complex.
Since ALEM can vibrantly adapt to network learning, compared with the plain LEM, our ALEM could more faithfully respect the geometry of SPD deep features.
$\mlog$ thus possesses more advantages than the vanilla matrix logarithm $\mln$.
We, therefore, replace the vanilla matrix logarithm with our $\mlog$, to respect the more advantageous geometry, \ie the ALEM-based geometry.

We focus on the most classic SPD network, SPDNet \cite{huang2017riemannian}.
There are three basic layers in SPDNet, \ie BiMap, ReEig, and LogEig, which are defined as
\begin{align}
    \text{BiMap: }& S^{k} = W^k S^{k-1} W^k,\\
    \text{ReEig: }&S^{k}=U^{k-1} \max (\Sigma^{k-1}, \epsilon I_{n}) U^{k-1 \top},\\
    \text{LogEig: }&S^{k}=\mln(S^{k-1}),
\end{align}
where $W^k$ is semi-orthogonal and $S^{k-1}=U^{k-1} \Sigma^{k-1} U^{k-1 \top}$ is the eigendecomposition.
The BiMap (Bilinear Mapping) is a generalized version of conventional linear mapping.
The ReEig (Eigenvalue Rectification) mimics the ReLu-like nonlinear activation functions by eigen-rectification.
The LogEig layer projects SPD-valued data into the Euclidean space for further classification.

The matrix logarithm in the LogEig layer is substituted by our $\mlog$. 
We call this layer the adaptive logarithm (ALog) layer. 
We set the base vector $\alpha$ as a learnable parameter.
In this way, as $\mlog$ is an isomorphism, the network can implicitly respect the ALEM-based Riemannian geometry by learning the $\mlog$ explicitly.
Besides, since our ALog layer is independent of specific network architectures, it can also be plugged into other SPD deep networks.