\section{Introduction}
\label{sec:intro}


\IEEEPARstart{T}he Symmetric Positive Definite (SPD) matrices are ubiquitous in statistics, supporting a diversity of scientific areas, such as medical imaging~\cite{chakraborty2018statistical,das2018sparse,chakraborty2020manifoldnet}
, signal processing \cite{yair2019parallel,brooks2019riemannian,kobler2022spd,ju2024deep}, elasticity \cite{moakher2006averaging, guilleminot2012generalized}, question answering \cite{lopez2021vector,nguyen2022gyro}, graph and node classification \cite{zhao2023modeling}, and computer vision \cite{huang2017riemannian,li2017high,wang2018discriminant,qiao2019deep,nguyen2021geomnet,song2021approximate,nguyen2022gyrovector,song2022fast,wei2022discrete,chen2024liebn,chen2024rmlr}.
Despite the ability to capture data variations, SPD matrices cannot simply interact as points in the Euclidean space, which becomes the main challenge in practice.
To guarantee the manifoldness, several Riemannian metrics have been proposed, including Affine-Invariant Metric (AIM) \cite{pennec2006riemannian}, Log-Euclidean Metric (LEM) \cite{arsigny2005fast}, and Log-Cholesky Metric (LCM) \cite{lin2019riemannian}, to name a few.
Equipped with these metrics, many Euclidean methods could be generalized into the domain of the Riemannian manifold \cite{wang2012covariance, huang2015log, huang2015face, harandi2018dimensionality,chen2021hybrid}. It is essential to clarify that there are also some metric learning methods in SPD manifolds \cite{huang2015log,harandi2018dimensionality}.
However, the metrics these methods learned are distance functions induced by existing Riemannian metrics.
In contrast, this paper focuses on Riemannian metrics, which are more fundamental than the metric learning methods mentioned above.

Recently, inspired by the vivid progress of deep learning \cite{hochreiter1997long,krizhevsky2012imagenet,he2016deep}, several deep networks were developed on the SPD manifold \cite{huang2017riemannian,chakraborty2018statistical,brooks2019riemannian,chakraborty2020manifoldnet,pan2022matt,wang2022learning,wang2022dreamnet,nguyen2022gyrovector,nguyen2022gyro,chen2023riemannian,nguyen2023building,wang2024spd,chen2024liebn,ju2024deep,chen2024rmlr}.
Although different network structures are designed, the theoretical foundations of these methods are all built upon Riemannian metrics on the SPD manifold.
Therefore, the design of the Riemannian metric is significantly important for the efficacy of the learning algorithms.
However, most metric tensors in the existing popular Riemannian metrics on the SPD manifold are fixed, which could undermine the expressibility of the associated geometry. 
After analyzing several existing Riemannian metrics on SPD manifolds, we find that the pullback is a commonly used tool, which can be intuitively viewed as a bijection preserving Riemannian properties.
For instance, \cite{thanwerdas2022theoretically} explained AIM as the pullback metric from a left-invariant metric on the Cholesky manifold.
In \cite{thanwerdas2023n}, the authors generalized LEM by the pullback of the vanilla LEM.
In \cite{lin2019riemannian}, the authors proposed LCM by the pullback from the Cholesky manifold.

Inspired by the above observations, we leverage pullback techniques to introduce adaptive Riemannian metrics in this paper.
In particular, we first show that several Riemannian metrics on SPD manifolds, including LEM, LCM, and their generalizations, can be explained as pullback metrics from the standard Euclidean space.
We refer to these metrics as Pullback Euclidean Metrics (PEMs).
Then, we propose a general framework for characterizing the properties of PEMs. Our framework can explain the widely used LEM \cite{arsigny2005fast} and LCM \cite{lin2019riemannian}.
We focus on LEM on SPD manifolds and extend it into Adaptive Log-Euclidean Metrics (ALEMs).
Besides, we present a complete theoretical discussion on the properties of ALEMs. 
Compared with the existing Riemannian metrics, our metrics are adjustable, adapting to the characteristics of the datasets.
To the best of our knowledge, our work is the \textbf{first} to integrate learnable Riemannian metrics into Riemannian deep networks.
The effectiveness of our metrics is demonstrated by experiments as well as the applications to recently developed Riemannian building blocks, including Riemannian batch normalization \cite{chen2024liebn}, Riemannian residual blocks \cite{katsman2023riemannian}, and Riemannian classifiers \cite{nguyen2023building}.
Drawing on this, our \textbf{contributions} are summarized as follows:
\textbf{(a)} We reveal the connection of two popular Riemannian metrics (LEM and LCM) by the pullback technique and propose a general framework for PEMs;
\textbf{(b)} Based on our framework, we propose specific ALEMs on SPD manifolds and conduct comprehensive analyses in terms of the algebraic, analytic, and geometric properties;
\textbf{(c)} Extensive experiments on widely used SPD learning benchmarks demonstrate that our metrics exhibit consistent performance gain across datasets.

The rest of the paper is organized as follows: 
\cref{sec:preliminary} reviews some essential backgrounds of differential geometry and the geometry of SPD manifolds.
\cref{subsec:thk_lem_lcm} rethinks the existing LEM and LCM from the perspective of pullback metrics.
\cref{subsec:pem_spd} provides a detailed discussion on PEMs.
\cref{subsec:ada_rie_metric,app:subsec:differentials} extend the existing LEM into ALEMs based on the framework of PEMs.
\cref{sec:properties} extensively analyzes the geometric properties of ALEM.
\cref{sec:ada_param_layers} presents the application of our ALEM into SPD neural networks.
\cref{sec:param_learning} discusses the gradient computations and parameter updates involved in our methods.
\cref{sec:experiments} validates our metric on three datasets.
\cref{sec:app_other_riem_blocks} further applies our ALEM to re-design other Riemannian blocks.
\cref{sec:limitations} discusses the limitations of this work, and \cref{sec:conclusions} concludes this paper.
For better representation, all proofs are left in the supplement.

