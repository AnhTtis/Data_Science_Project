\section{Properties OF ALEM}
\label{sec:properties}
Since our ALEMs are natural generalizations of LEM.
Therefore, intuitively, ALEMs would share every property of LEM.
This section introduces some useful properties of our ALEMs for machine learning, including Fréchet mean and invariance properties.

Fréchet means are important tools for SPD matrices learning \cite{harandi2018dimensionality,chakraborty2018statistical,brooks2019riemannian,chakraborty2020manifoldnorm}.
Like LEM, our ALEM also enjoys closed forms of Fréchet means.
We present a more general result, the weighted Fréchet mean.
\begin{proposition}[Weighted Fréchet Means] \label{props:geo_mean_spd}
    For $m$ points $S_1,\cdots S_m$ in SPD manifolds with associated weights $w_1,\cdots, w_m \in \bbRplus$,
    the weighted Fréchet mean $M$ over the metric space $\{\spd{n},\dalem\}$ has a closed form
    \begin{equation} \label{eq:fm_alem}
        M = \mgexp(\sum_{i=1}^{m} \frac{w_i}{\sum_{j=1}^{m} w_i}\mlog(S_i)).
    \end{equation}
\end{proposition}

Like LEM, although our ALEM does not conform with the affine-invariance, our ALEM enjoys some other kinds of invariance.
\begin{proposition}[Bi-invariance] \label{props:biinvariance}
    ALEM is a Lie group bi-invariant metric.
\end{proposition}
\begin{proposition} [Exponential Invariance] \label{props:exp_invariance}
    The Fréchet means under ALEM are exponential-invariant. 
    In other words, for $S_1,\cdots S_m \in \spd{n}$ and $\beta \in \bbRscalar$,
    \begin{equation}
        (\mathrm{FM}(S_1,\cdots S_m))^\beta = \mathrm{FM}(S_1^\beta,\cdots S_m^\beta),
    \end{equation}
    where $\mathrm{FM}(S_1,\cdots S_m))$ means the Fréchet mean of $S_1,\cdots S_m$.
\end{proposition}

Except for the exponential invariance, the Fréchet mean induced by our ALEM also satisfies various properties presented in \cite{ando2004geometric}.
\begin{proposition} \label{props:frechet_means_add_props}
    For any SPD matrices $A, B, C, A_0, B_0, C_0$, denote $\fm(A,B,C)$ as the Fréchet mean of $A,B,C$ under ALEM.
    Then the Fréchet mean satisfies the following properties.
    \begin{enumerate}[(U1)]
        \item \label{props:u1}
        Permutation invariance.
        For any permutation $\pi(\{ A,B,C \})$ of $\{ A, B, C\}$,  
        \item \label{props:u2}
        $\fm(A, A,A) = A$
    \end{enumerate}
    The following properties hold if $A, B, C, A_O, B_0, C_0$ commute.
    \begin{enumerate}[(V1)]
        \item  Joint homogeneity. \label{props:v1}
        $\fm(a A, b B, c C)=(a b c )^{1 / 3} \fm(A, B, C), \forall a,b,c >0$.
        
        \item Monotonicity.
        The map $(A, B, C) \mapsto \fm(A, B, C)$ is monotone, \ie, if $A \geq A_0$, $B \geq  B_0$, and $C \geq  C_0$, then $\fm(A,B,C) \geq  \fm(A_0,B_0,C_0)$ in the positive semidefinite ordering.
        \item Self-duality.
        $\fm(A, B, C)=\fm(A^{-1}, B^{-1}, C^{-1})^{-1}$.
        \item Determinant identity.  \label{props:v4}
        $\det \fm(A, B, C)=(\det A \cdot \det B \cdot \det C)^{1 / 3}$.
    \end{enumerate}
\end{proposition}

In fact, \cref{props:frechet_means_add_props} holds true for any finite number of SPD matrices. 
Besides, the geodesic distance induced by ALEMs has similarity invariance.

\begin{proposition}[Similarity Invariance] \label{props:sim_invariance}
    The geodesic distance under ALEM is similarity invariant.
    In other words, let $R \in SO(n)$ be a rotation matrix, $s \in \bbRplus$ is a scale factor.
    Given any two SPD matrices $S_1$ and $S_2$, we have
    \begin{equation}
        \dalem(S_1,S_2)=\dalem(s^2RS_1 R^\top,s^2RS_2 R^\top).
    \end{equation}
\end{proposition}
Let us explain a bit more about the above three kinds of invariance.
Firstly, among metrics on Lie groups, bi-invariant metrics are the most convenient ones \cite[Chapter V]{sternberg1999lectures}.
Secondly, exponential invariance offers a fast computation for Fréchet means under exponential scaling.
At last, similarity-invariance is significant for describing the frequently encountered covariance matrices \cite{arsigny2005fast}.

The above discussion focuses on theoretical side.
Now, let us reconsider \cref{eq:mlog} in a numerical way.

\begin{proposition} \label{prop:rewrit_general_log}
    $\mlog$ can be rewritten as
        \begin{align}
            \label{eq:rw_org_mlog} \mlog(S) 
            &= U \log_\alpha(\Sigma) U^\top,\\
            \label{eq:rw_mul_mlog}          
            &= U A \ln(\Sigma) U^\top, \\
            \label{eq:rw_div_mlog}          
            &= U \frac{\ln(\Sigma)}{B} U^\top,
        \end{align}
    where $\frac{X}{Y}$ is the diagonal division, $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$, and $A = \frac{I}{B}$.
\end{proposition}

Based on the above proposition, more analyses could be carried out from a numerical point of view.
First, $\mlog(\cdot)$ can balance the eigenvalues of an input SPD matrix $S$ by exploiting different bases for different eigenvalues.
In Riemannian algorithms, manifold-valued features usually contain vibrant information.
We expect that by the above adaptation, manifold-valued data could be better fitted and the learning ability of algorithms could be further promoted.
\begin{remark} 
Note that the discussion in \cref{subsec:ada_rie_metric} and \cref{sec:properties} can also be readily transferred into LCM, generating an adaptive version of LCM.
\end{remark}