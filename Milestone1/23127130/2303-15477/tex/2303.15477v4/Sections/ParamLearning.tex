\section{Parameters Learning} 
\label{sec:param_learning}

We first present the gradient computation and then discuss in detail how to optimize the parameters in the ALog layer.
\subsection{Gradients Computation} \label{subsec:gradients}

Two gradients need calculation in the proposed ALog layer: one w.r.t the parameters and another w.r.t the input of the ALog layer.
Since structural matrix decomposition is involved in $\mlog$, the following contents heavily rely on the structural matrix BP \cite{ionescu2015matrix}, the key idea of which is the invariance of first-order differential form.
For the ALog layer, it is essentially a special case of eigenvalue functions.
Based on the formula offered in \cite{bhatia2009positive} and matrix BP techniques presented in \cite{ionescu2015matrix}, we can obtain all the gradients, as presented in the following proposition.
\begin{proposition} \label{props:grad_mlog}
    Let us denote $X = \mlog(S)$, where $S \in \spd{d}$ is an input SPD matrix of the ALog layer.
    We have the following gradients:
    \begin{align}
        \label{eq:gradient_eigen_function} \nabla_{S} L
        &= U[K \odot(U^{T}(\nabla_{X} L) U)] U^{T},\\
        \nabla_{A} L   
        &= [U^\top (\nabla_{X} L) U] \odot \log(\Sigma),
    \end{align}
    where $S = U \Sigma U^\top$ is the eigendecomposition of an SPD matrix and matrix $K$ is defined as
    \begin{equation}
        K_{i j}= \begin{cases}\frac{f\left(\sigma_{i}\right)-f\left(\sigma_{j}\right)}{\sigma_{i}-\sigma_{j}} & \text { if } \sigma_{i} \neq \sigma_{j} \\ f^{\prime}\left(\sigma_{i}\right) & \text { otherwise }\end{cases}
    \end{equation}
    where $f(\sigma_i) = A_{ii}\log_e(\sigma_i)$ and $\Sigma=\diag(\sigma_1,\sigma_2,\cdots,\sigma_d$).
\end{proposition}

\subsection{Parameters Updates}
\begin{table*}[htbp]
    \small
    \centering
    \caption{Parameter Learning in the ALog Layer.}
    \label{tb:param_learning}
    % \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{cccc}
    \toprule
    Name & Detail & Constraint & Method\\
    \midrule
    RELU & Optimizing base vector $\alpha$ (\cref{eq:rw_org_mlog}) & Positive & shift-ReLu $\max(\epsilon,\alpha)$\\
    MUL & Optimizing diagonal elements of $A$ (\cref{eq:rw_mul_mlog}) & Unconstrained & Standard BP\\
    DIV & Optimizing diagonal elements of $B$ (\cref{eq:rw_div_mlog}) & Unconstrained & Standard BP\\
    \bottomrule
    \end{tabular}
    % }
    \vspace{-2mm}
\end{table*}
Let us explain how to optimize the proposed layer in a standard backpropagation (BP) framework.
Denote the dimension of an input SPD matrix $S$ as $d \times d$.
Recalling \cref{eq:rw_org_mlog}-\cref{eq:rw_div_mlog}, there are three ways to implement parameter learning.
We could learn the base vector $\alpha$ in \cref{eq:rw_org_mlog}, diagonal matrix $A$ in \cref{eq:rw_mul_mlog}, or diagonal matrix $B$ in \cref{eq:rw_div_mlog}, respectively.

For learning $A$ in \cref{eq:rw_mul_mlog} or $B$ in \cref{eq:rw_div_mlog}, since the parameters (diagonal elements) lie in a Euclidean space $\bbR{d}$, the optimization can be easily integrated into the BP algorithm.
We call learning $A$ MUL and learning $B$ DIV.

For the case of learning $\alpha$ in \cref{eq:rw_org_mlog}, since $\alpha$ lies in a non-Euclidean space, specific updating strategies should be considered.
Without loss of generality, we focus on the case of a scalar parameter $a>0 \& a \neq 1$.
The condition of $a \neq 1$ can be further waived since we can set $a=1+\epsilon$ if $a=1$. 
Then, there is only one constraint about positivity.
We use the shift-ReLU of an unconstrained parameter, \ie $\max(\epsilon,a)$ with $\epsilon \in\bbRplus$.
This strategy is named RELU.
Other tricks like square are also feasible, but we will focus on the RELU.
In addition, positive scalar $a$ can be directly optimized by Riemannian optimization \cite{absil2009optimization}.
We further prove that this strategy completely equals learning $B$ directly.
For more details, please refer to the Supp. B-B.
% \cref{app:subsec:geom}.

Therefore, there are three ways of updates, \textit{i.e.,} RELU, DIV, and MUL, summarized in \cref{tb:param_learning}.


