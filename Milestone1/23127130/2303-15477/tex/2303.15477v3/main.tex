\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[numbers]{natbib}
\usepackage[nonatbib,preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{colortbl}




\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{tasks}
% \usepackage{enumitem}
% \usepackage{breakurl}
\graphicspath{{img/}}
\usepackage{wrapfig}
\usepackage[capitalize,noabbrev]{cleveref}
% \usepackage{float}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{parskip}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% set
\providecommand{\calM}{\mathcal{M}}
\providecommand{\calN}{\mathcal{N}}
\providecommand{\calA}{\mathcal{A}}
\providecommand{\calX}{\mathcal{X}}
\providecommand{\calY}{\mathcal{Y}}
\providecommand{\bbN}{\mathbb{N}}
\providecommand{\bbZ}{\mathbb{Z}}
\providecommand{\bbK}{\mathbb{K}}
\providecommand{\sym}[1]{\mathcal{S}^{#1}}
\providecommand{\spd}[1]{\mathcal{S}^{#1}_{++}}
\providecommand{\cho}[1]{\mathcal{L}_{+}^{#1}}
\providecommand{\tril}[1]{\mathcal{L}^{#1}}
\providecommand{\bbR}[1]{\mathbb {R}^{#1}}
\providecommand{\bbRplus}{\mathbb {R}_{+}}
\providecommand{\bbRscalar}{\mathbb {R}}
\providecommand{\cinf}{C^{\infty}}

% operator
\providecommand{\rieexp}{\operatorname{Exp}}
\providecommand{\rielog}{\operatorname{Log}}
\providecommand{\diffphi}[1]{\phi_{*,#1}}
\providecommand{\diffphiinv}[1]{\phiinv_{*,#1}}
\providecommand{\diffmlog}[1]{\phi_{mlog*,#1}}
\providecommand{\diffmgexp}[1]{\phi_{ma*,#1}}
\providecommand{\pt}[2]{\Gamma_{#1 \rightarrow #2}}
\providecommand{\scrL}{\mathscr{L}}
\providecommand{\bbD}{\mathbb {D}}
\providecommand{\ln}{\operatorname{ln}}
\providecommand{\mln}{\phi_{mln}}
\providecommand{\cln}{\phi_{cln}}
\providecommand{\clnchart}{\varphi_{ln}}
\providecommand{\clogchart}{\varphi_{log}}
\providecommand{\mlog}{\phi_{mlog}}
\providecommand{\clog}{\phi_{clog}}
\providecommand{\mexp}{\phi_{mexp}}
\providecommand{\mgexp}{\phi_{ma}}
\providecommand{\cgexp}{\phi_{ca}}
\providecommand{\mlnMul}{\odot_{mln}}
\providecommand{\clnMul}{\odot_{cln}}
\providecommand{\mlogMul}{\odot_{mlog}}
\providecommand{\clogMul}{\odot_{clog}}
\providecommand{\mlnMulScalar}{\circledast_{mln}}
\providecommand{\clnMulScalar}{\circledast_{cln}}
\providecommand{\mlogMulScalar}{\circledast_{mlog}}
\providecommand{\clogMulScalar}{\circledast_{clog}}
\providecommand{\mlogMulInv}[1]{{#1}_{\mlogMul}^{-1}}
\providecommand{\xMulInv}[1]{{#1}_{\xMul}^{-1}}
\providecommand{\yMulInv}[1]{{#1}_{\yMul}^{-1}}
\providecommand{\distmlog}{d_{mlog}}
\providecommand{\distclog}{d_{clog}}
\providecommand{\relu}{\operatorname{ReLu}}
\providecommand{\diag}{\operatorname{diag}}
\providecommand{\rieExp}[1]{\operatorname{Exp}_{#1}}
\providecommand{\rieLog}[1]{\operatorname{Log}_{#1}}
\providecommand{\leop}{\odot_{le}}
\providecommand{\lescalar}{\circledast_{le}}
\providecommand{\lcop}{\odot_{lc}}
\providecommand{\im}[1]{\operatorname{im}{#1}}
\providecommand{\xMul}{\odot_{\phi}}
\providecommand{\yMul}{\odot_{\calY}}
\providecommand{\xMulScalar}{\circledast_{\phi}}
\providecommand{\yMulScalar}{\circledast_{\calY}}
\providecommand{\xMul}{\odot_{\phi}}
\providecommand{\yMul}{\odot_{\calY}}
\providecommand{\xMulScalar}{\circledast_{\phi}}
\providecommand{\yMulScalar}{\circledast_{\calY}}
\providecommand{\phiinv}{\phi^{-1}}
\providecommand{\phiMulScalar}{\circledast_{\phi}}
\providecommand{\gphi}{g^{\phi}}
\providecommand{\phiMul}{\odot_{\phi}}
\providecommand{\diff}{\operatorname{d}}
\providecommand{\tr}{\operatorname{tr}}
\providecommand{\galem}{g^{\mathrm{ALE}}}
\providecommand{\glem}{g^{\mathrm{LE}}}
\providecommand{\glcm}{g^{\mathrm{LC}}}
\providecommand{\geuc}{g^{\mathrm{E}}}
\providecommand{\gx}{g^{\mathcal{X}}}
\providecommand{\gy}{g^{\mathcal{Y}}}
\providecommand{\gphi}{g^{\phi}}
\providecommand{\dlem}{d^{\mathrm{LE}}}
\providecommand{\dlcm}{d^{\mathrm{LC}}}
\providecommand{\deuc}{d^{\mathrm{E}}}
\providecommand{\dalem}{d^{\mathrm{ALE}}}
\providecommand{\dphi}{d^{\phi}}
\providecommand{\Cov}{{\mathrm{Cov}}}
\providecommand{\fm}{{\mathrm{FM}}}
\providecommand{\det}{{\operatorname{det}}}

%symble
\providecommand{\balpha}{\boldsymbol{\alpha}}
\providecommand{\rmE}{\mathrm{E}}
\providecommand{\rmF}{\mathrm{F}}
\providecommand{\rmO}{\mathrm{O}}
\providecommand{\id}{\mathbbm{1}}
\providecommand{\idY}{\id_{\calY}}
\providecommand{\idX}{\id_{\calX}}
\providecommand{\atlasid}{\mathcal{A}_{\mathbbm{1}}}
\providecommand{\lem}{\mathrm{LEM}}
\providecommand{\lcm}{\mathrm{LCM}}

%ref
\providecommand{\defref}[1]{Definition~\ref{#1}}
\providecommand{\lemref}[1]{Lemma~\ref{#1}}
\providecommand{\secref}[1]{Section~\ref{#1}}
\providecommand{\rmkref}[1]{Remark~\ref{#1}}
\providecommand{\propsref}[1]{Proposition~\ref{#1}}
\providecommand{\thmref}[1]{Theorem~\ref{#1}}
\providecommand{\corref}[1]{Corollary~\ref{#1}}
\providecommand{\tabref}[1]{Table~\ref{#1}}
\providecommand{\figref}[1]{Figure~\ref{#1}}
\renewcommand{\eqref}[1]{Eq.~\ref{#1}}

%Other command
\newcommand{\yue}[1]{\textcolor{red}{#1}}
\newcommand{\ziheng}[1]{\textcolor{blue}{#1}}
\providecommand{\ie}{\textit{i.e. }}

\title{Adaptive Riemannian Metrics on SPD Manifolds}

\author{
  Ziheng Chen\\
  University of Trento\\
  Trento, Italy\\
  \texttt{ziheng\_ch@163.com}\\
  \And
  Yue Song\\
  University of Trento\\
  Trento, Italy\\
  \texttt{yue.song@unitn.it}\\
  \And
  Tianyang Xu\\
  Jiangnan University\\
  Wuxi, China\\
  \texttt{tianyang\_xu@163.com}\\
  \And
  Zhiwu Huang\\
  University of Southampton\\
  Southampton, U.K.\\
  \texttt{Zhiwu.Huang@soton.ac.uk}\\
  \And
  Xiao-Jun Wu\\
  Jiangnan University\\
  Wuxi, China\\
  \texttt{wu\_xiaojun@jiangnan.edu.cn}\\
  \And
  Nicu Sebe\\
  University of Trento\\
  Trento, Italy\\
  \texttt{sebe@disi.unitn.it}\\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
    Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. 
    To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed.
    However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for deep SPD neural networks.
    To remedy this limitation, we leverage the commonly encountered pullback techniques, and propose adaptive Riemannian metrics for SPD manifolds. 
    Moreover, we present comprehensive theories to support our metrics. 
    The experimental and theoretical analysis demonstrates that the merit of the proposed metrics, in optimising SPD network framework with promising performance.
\end{abstract}


\section{Introduction} \label{sec:intro}

The Symmetric Positive Definite (SPD) matrices are ubiquitous in statistics, supporting a diversity of scientific areas, such as medical imaging \citep{chakraborty2018statistical,chakraborty2020manifoldnet}, signal processing \citep{arnaudon2013riemannian, hua2017matrix,brooks2019exploring, brooks2019riemannian}, elasticity \citep{moakher2006averaging, guilleminot2012generalized}, question answering \citep{lopez2021vector,nguyen2022gyro}, and computer vision \citep{huang2017riemannian, harandi2018dimensionality, zhen2019dilated, chakraborty2020manifoldnorm,zhang2020deep,chakraborty2020manifoldnorm,nguyen2021geomnet, nguyen2022gyrovector}.
Despite the exhibited capability of capturing data variations, SPD matrices cannot simply interact as points in a Euclidean space, which becomes the main challenge in practice.
To tackle this issue, various Riemannian metrics have been proposed to guarantee the manifoldness, including Affine-Invariant Metric (AIM) \citep{pennec2006riemannian}, Log-Euclidean Metric (LEM) \citep{arsigny2005fast}, and Log-Cholesky Metric (LCM) \citep{lin2019riemannian} to name a few.
Equipped with these metrics, many Euclidean methods could be generalized into the domain of the Riemannian manifold \citep{wang2012covariance, huang2015log, huang2015face, harandi2018dimensionality,chen2021hybrid}. It is important to first clarify that there are also many metric learning methods in SPD manifolds \citep{huang2015log,harandi2018dimensionality}.
However, the metrics these methods learned are distance functions induced by Riemannian metrics, which are intrinsically different from Riemannian metrics.
In this paper, therefore, we focus on Riemannian metrics, which are more fundamental than metric learning.

% Recently, inspired by the vivid progress of deep learning \citep{hochreiter1997long, krizhevsky2012imagenet,he2016deep}, following the theories of Riemannian geometry, several deep networks were designed on the SPD manifold.
% The principal motivation of these networks is to generalize the basic network components (transformation, activation, and classification) from Euclidean networks into Riemannian ones. 
% For instance, \citep{huang2017riemannian} propose a densely connected feedforward network on the SPD manifold that is named SPDNet, successfully preserving SPDness in each layer.
% To generalize the object to SPD tensors,  \citep{chakraborty2020manifoldnet,zhang2020deep} establish Riemannian convolutional networks correspondingly.
% In terms of distribution control, \citep{brooks2019riemannian} and \citep{chakraborty2020manifoldnorm} alternately extend the normalization onto manifolds.
% In \citep{nguyen2021geomnet}, based on a novel Gaussian embedding, an SPD deep network is proposed with the ability to mine the statistics in SPD deep features.
% For sequential SPD-valued data, a statistical recurrent unit (SPD-SRU) \citep{chakraborty2018statistical}, a dilated convolutional network \citep{zhen2019dilated}, and RNNs \citep{nguyen2022gyrovector, nguyen2022gyro} based on Gyrovector space have been proposed.
% In the meantime, some works attempt to aggregate Riemannian approaches into traditional deep learning.
% For example, in \citep{wang2020deep, gao2021temporal,song2021approximate}, a square root layer is proposed to approximate Euclidean projection to alleviate the computational complexity manifesting in covariance pooling.
% \citep{song2022fast} further propose a differentiable implementation for fast computation of the widely used square root and inverse square root.

Recently, inspired by the vivid progress of deep learning \citep{hochreiter1997long, krizhevsky2012imagenet,he2016deep}, several deep networks were developed on the SPD manifold \citep{huang2017riemannian,chakraborty2018statistical,brooks2019riemannian,zhen2019dilated,chakraborty2020manifoldnet,zhang2020deep,chakraborty2020manifoldnorm,nguyen2021geomnet,nguyen2022gyrovector,nguyen2022gyro,chen2022msnet,song2021approximate,song2022fast}.
Although different network structures are designed, the theoretical foundations of these methods are all built upon Riemannian metrics on the SPD manifold.
Therefore, the design of Riemannian metrics is significantly important for effective learning algorithms.
However, the metric tensors in the existing popular Riemannian metrics on the SPD manifold are fixed, which could undermine the expressibility of the associated geometry. 
After deep analysis, among the existing Riemannian metrics on SPD manifolds, we find that the pullback is a commonly used tool.
For instance, in \citep{thanwerdas2022theoretically}, AIM is explained as the pullback metric from a left-invariant metric on the Cholesky manifold.
In \citep{thanwerdas2023n}, the authors generalized the LEM by the pullback of the vanilla LEM.
In \citep{lin2019riemannian}, LCM is proposed by the pullback from the Cholesky manifold.
% We will further show that LEM and LCM, which have been proven successful in machine learning, are both pullback metrics from the standard Euclidean space.

Inspired by the above observations, we propose to leverage pullback techniques for generalizing adaptive Riemannian metrics in this paper.
Although the pullback has been used by different previous studies \citep{lin2019riemannian,thanwerdas2022theoretically,thanwerdas2023n}, none of them provides a general mathematical formulation. 
In particular, we first show that the two popular Riemannian metrics, \ie LEM and LCM, can be both understood as pullback metrics from the standard Euclidean space. Then we proceed to propose a general framework for inducing properties by pullback \citep{loring2011introduction}.
Typically, our framework can explain the Riemannian and algebraic properties proposed in \citep{arsigny2005fast,lin2019riemannian}.
Following our general framework, we propose adaptive Riemannian metrics on SPD manifolds and present a complete theoretical discussion. 
Compared with the existing Riemannian metrics, our metrics are adjustable, delivering an efficient solution during the process of standard backpropagation (BP) of learning SPD deep networks.
To the best of our knowledge, our work is the \textbf{first} one to integrate learnable Riemannian metrics into Riemannian deep networks.
Drawing on this, our \textbf{contributions} are summarized as follows:
\textbf{(a)} We reveal the connection of two popular Riemannian metrics (LEM and LCM) with pullback and propose a general framework for designing Riemannian metrics;
\textbf{(b)} Based on our framework, we propose specific adaptive Riemannian metrics on SPD manifolds and conduct comprehensive analyses in terms of the algebraic, analytic, and geometric properties;
\textbf{(c)} Extensive experiments on widely used SPD learning benchmarks demonstrate that our metric exhibits consistent performance gain across datasets.

% The rest of the paper is organized as follows: 
% we first review some essential backgrounds of differential geometry;
% we then proceed to reconsider two popular Riemannian metrics on SPD manifolds and we conclude that they can be uniformly described by the idea of pullback \citep{loring2011introduction}. 
% next, a general framework of designing Riemannian metrics is then introduced;
% following this framework, specific adaptive Riemannian metrics on SPD manifolds are presented, with the ability of interacting with Riemannian learning algorithms.
% finally, we validate experimentally our proposed metrics.

\section{Basic notations and preliminaries} \label{sec:notaions_limited_prelim}
Some background about differential manifolds \citep{loring2011introduction,lee2013smooth}, and LEM \citep{arsigny2005fast} \& LCM \citep{lin2019riemannian} on the SPD manifold are required in this paper.
Due to the page limit, the typical basic notation is introduced here.
For a more detailed review, please refer to \cref{app:sec:preliminaries}.

We denote the set of $n \times n$ SPD matrices as $\spd{n}$, the set of $n \times n$ symmetric matrices as $\sym{n}$, and all the Cholesky matrices as $\cho{n}$.
As shown in the previous literature \citep{arsigny2005fast,lin2019riemannian}, $\spd{n}$ and $\cho{n}$ form an SPD manifold and a Cholesky manifold, respectively.
For an SPD matrix $S$, the matrix logarithm $\mln(\cdot): \spd{n} \rightarrow \sym{n}$ is defined as
\begin{equation} \label{eq:mln} 
    \mln(S) = U \ln(\Sigma) U^\top,    
\end{equation}
where $S=U \Sigma U^\top$ is the eigendecomposition, and $\ln(\cdot)$ is the diagonal natural logarithm.
In \citep{arsigny2005fast}, LEM and Lie group on $\spd{n}$ are induced by matrix logarithm. 
In \citep{lin2019riemannian}, LCM and Lie group on $\spd{n}$ are derived from $\cho{n}$ by Cholesky decomposition. 

Now we briefly review the idea of pullback, which is a common trick in geometry to study metrics.
\begin{definition} [Pullback Metrics] \label{def:pullback_metrics}
    Suppose $\calM,\calN$ are smooth manifolds, $g$ is a Riemannian metric on $\calN$, and $f:\calM \rightarrow \calN$ is smooth.
    Then the pullback of the tensor field $g$ by $f$ is defined point-wisely,
    \begin{equation}
        (f^*g)_p(V_1,V_2) = g_{f(p)}(f_{*,p}(V_1),f_{*,p}(V_2)),
    \end{equation}
    where $p \in \calM$, $f_{*,p}(\cdot)$ is the differential map of $f$ at $p$, and $V_i \in T_p\calM$.
    If $f^*g$ is positive definite, it is a Riemannian metric on $\calM$, which is called the pullback metric defined by $f$.
\end{definition}

In the following, we will rely on pullback metrics to rethink LEM and LCM, and further unveil their common mathematical logic, which delivers direct formulation to our design of Riemannian metrics.



\section{Adaptive Riemannian metrics} \label{sec:ada_riem_metrics}
As mentioned in the \cref{sec:intro}, pullbacks are ubiquitous for the study of Riemannian metrics on SPD manifolds.
Particularly, as we will show in this section, two popular metrics (LEM and LCM) are even pullback metrics from the Euclidean space.
Inspired by this observation, we first present a general framework for pullback on manifolds, and then proceed to focus on pullback metrics from the Euclidean space.

\subsection{Rethinking LEM and LCM} \label{subsec:thk_lem_lcm}
Among the existing Riemannian metrics on the SPD manifold, LEM is popular in many applications given its closed form for the Fréchet mean and clear vector space \& Lie group structures.
In addition, the nascent LCM, gaining increasing attention, also shares similar properties with LEM.
LEM is derived from the Lie group translation \citep{arsigny2005fast}, while LCM is derived by the pullback from $\cho{n}$ \citep{lin2019riemannian}.
At first glance, LEM and LCM seem to be designed by different mathematical tools. However, theoretically, the mathematical logic beneath their derivation can be the same. 
\begin{theorem} \label{thm:rethk_lem_lcm}
    LEM is a pullback metric from the standard Euclidean space of $\sym{n}$ by matrix logarithm.
    Similarly, LCM is a pullback metric from $\sym{n}$ as well.
\end{theorem}


\subsection{General framework}

In \secref{subsec:thk_lem_lcm}, we have shown how LEM is derived from matrix logarithm.
Besides, as shown in \citep{arsigny2005fast}, operations in Lie group and linear space on $\spd{n}$ are also induced from matrix logarithm.
Now let us explain the underlying mechanism in detail.
A matrix logarithm is a diffeomorphism (a smooth bijection with a smooth inverse).
The property of bijection offers the possibility of transferring algebraic structures from $\sym{n}$ into $\spd{n}$.
The smoothness of matrix logarithm and its inverse suggest that smooth structures can be transferred into $\spd{n}$, like the Lie group and Riemannian metric.
More generally, given an arbitrary isomorphism $\phi:\calX \rightarrow \calY$, it suffices to pull various properties from $\calY$ back to $\calX$ by $\phi$ as well.
\begin{lemma}\label{lem:pro_by_bijection}
    Supposing $\phi: \calX \rightarrow \calY$ is a bijection with its inverse map denoted as $\phiinv$, we have the following conclusion.
    \begin{enumerate}
        \item \label{enu:general_hilbert}
        If $\calY$ is a Hilbert space over the number field $\bbK$, $\calX$ forms a Hilbert space over $\bbK$, with the induced operations
        \begin{align}
        \label{eq:x_mul}           x_1 \xMul x_2                   &= \phiinv( \phi(x_1) \yMul \phi(x_2)),\\
        \label{eq:x_mul_scalar}    k \xMulScalar x_2               &= \phiinv(k \yMulScalar \phi(x_2)),\\
        \label{eq:x_inner_product} \langle x_1, x_2\rangle_{\phi} &= \langle \phi(x_1), \phi(x_2)\rangle_{\calY},
        \end{align}
        where $x_1, x_2 \in \calX$, $k \in \bbK$, and $\yMul$, $\yMulScalar$, $\langle \cdot, \cdot\rangle_{\calY}$ denote the element-wise multiplication, scalar multiplication, inner product in $\calY$, respectively.
        Besides, $\phi$ is a linear isomorphism preserving the inner product.
        \item \label{enu:lie_group}
        If $\phi$ is a diffeomorphism between smooth manifold $\calX$ and (abelian) Lie group $\{\calY, \yMul \}$, then $\{\calX, \xMul \}$ forms a (an) (abelian) Lie group, and $\phi$ is a Lie group isomorphism.
        \item \label{enu:rie_manifold}
        If $\phi$ is a diffeomorphism between smooth manifold $\calX$ and Riemannian manifold $\{\calY,\gy \}$, then the pullback metric $\gphi = \phi^{*}\gy$ makes $\{\calX,\gphi\}$ an isometric Riemannian manifold to $\{\calY,\gy\}$ and $\phi$ is a Riemannian isometry.
    \end{enumerate}
\end{lemma}

% \begin{wrapfigure}{r}{0.5\textwidth}
% \centering
% \includegraphics[width=0.5\columnwidth]{img/illustration.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width. This setup will avoid overfull boxes.
% \caption{Conceptual illustration of \lemref{lem:pro_by_bijection}.
% % $\phi$ is a diffeomorphism from the smooth manifold $\calX$ to the Riemannian manifold $\{\calY,\gy\}$ with the Lie group multiplication $\yMul$.
% % With element multiplication $\xMul$ defined in the co-domain of $\phi$, $\{\calX,\xMul\}$ forms a Lie group, 
% % Endowed with the pullback metric $\gphi=\phi^*\gy$, $\{\calX,\gphi\}$ forms a Riemannian manifold.
% }
% \label{fig:pipeline}
% \end{wrapfigure}

\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{illustration.pdf} 
\caption{Conceptual illustration of \lemref{lem:pro_by_bijection}.
$\phi$ is a diffeomorphism from the smooth manifold $\calX$ to the Riemannian manifold $\{\calY,\gy\}$ with the Lie group multiplication $\yMul$.
With element multiplication $\xMul$ defined in the co-domain of $\phi$, $\{\calX,\xMul\}$ forms a Lie group, 
Endowed with the pullback metric $\gphi=\phi^*\gy$, $\{\calX,\gphi\}$ forms a Riemannian manifold.
}
\label{fig:pipeline}
\end{figure}

\lemref{lem:pro_by_bijection} indicates that a bijection can be an isomorphism, as long as it conforms with the axioms in a specific category.
In this way, various structural properties can be transferred from $\calY$ to $\calX$.
Therefore, the key idea of this theorem lies in the proper map $\phi$.
For a better understanding, a conceptual illustration of case \ref{enu:lie_group} and case \ref{enu:rie_manifold} in \lemref{lem:pro_by_bijection} is shown in \figref{fig:pipeline}.

Particularly, if the resulting space $\calY$ in \cref{lem:pro_by_bijection} is a Euclidean space, various properties could be pulled back, including vector space, Lie groups, and Riemannian metrics.
Besides, the computation of the induced operators in $\calX$ is usually simple.
Therefore, in the following, we focus on analyzing adaptive metrics for $\calX=\spd{n}$ and $\calY=\sym{n}$.

\begin{theorem} \label{thm:g_spd}
    Let $S_1, S_2, S \in \spd{n}, V_i \in T_S\spd{n}, k \in \bbRscalar$ and $\langle,\rangle$ be the Frobenius inner product in $\sym{n}$.
    $\phi:\spd{n} \rightarrow \sym{n}$ is a diffeomorphism, and $\phi_{*,S}$ is the differential at $S$.
    We define the following operations,
    \begin{align}
        \label{eq:phi_mul} S_1 \phiMul S_2 &= \phiinv( \phi(S_1)+\phi(S_2)),\\
        \label{eq:phi_sca_mul} k \phiMulScalar S_2 &= \phiinv( k\phi(S_2)),\\
        \label{eq:phi_innerpro} \langle S_1, S_2 \rangle_{\phi} &= \langle \phi(S_1), \phi(S_2) \rangle,\\
        \label{eq:phi_g} \gphi_S(V_1,V_2) &= \langle \phi_{*,S}(V_1),\phi_{*,S}(V_2) \rangle,
    \end{align}
    Then, we have the following conclusions:
    \begin{enumerate}
        \item \label{itm:spd_hilbet}
        $\{\spd{n}, \phiMul,\phiMulScalar, \langle \cdot, \cdot \rangle_{\phi} \}$ is a Hilbert space over $\bbRscalar$.
        \item 
        $\{\spd{n}, \phiMul \}$ is an abelian Lie group.
        $\{\spd{n}, \gphi \}$ is a Riemannian manifold.
        The associated geodesic distance is
        \begin{equation} \label{eq:dist_phi_spd}
            \dphi (S_1, S_2 ) = \| \phi(S_1) - \phi(S_2) \|_\rmF.
        \end{equation}
        The Riemannian operators are as follows
        \begin{align}
            \label{eq:gene_rie_exp_spd}
            \rieexp_{S_1} V &= \phiinv(\phi(S_1)+\diffphi{S_1}V),\\
            \label{eq:gene_rie_log_spd}
            \rielog_{S_1}S_2 &= \diffphiinv{\phi(S_1)}(\phi(S_2)-\phi(S_1)),\\
            \label{eq:gene_pt_spd} \pt{S_1}{S_2}(V) &= \diffphiinv{\phi(S_2)} \circ \diffphi{S_1}(V),
        \end{align}
        where $V \in T_{S_1}\spd{n}$ is a tangent vector, $\rieexp$, $\rielog$ and $\Gamma$ are Riemannian exponential map, logarithmic map and parallel transportation respectively, and $\phiinv_{*}$ is the differential maps of $\phiinv$.
        $\gphi$ is a bi-invariant metric, named Pullback Euclidean Metric (PEM) by $\phi$.
        \item
        $\phi$ is an isomorphism: (a) a linear isomorphism preserving the inner product; (b) a Lie group isomorphism; (3) a Riemannian isometry.
    \end{enumerate}
\end{theorem}
% \begin{remark}
% For a better understanding of the above theorem, we make the following remarks:
% % \begin{enumerate}
% %     \item 
% %     For the Hilbert space in case \ref{itm:spd_hilbet}, the distance induced by the inner product $\langle \cdot, \cdot \rangle_{\phi}$ respects the geometry of SPD manifolds, as it is exactly the geodesic distance induced from $\gphi$.
% %     \item 
% %     For \eqref{eq:gene_pt_spd}, as 
% %     $(\phiinv)_* = (\phi_{*})^{-1}$ \citep{loring2011introduction}, we simply write $\phiinv_*$.
% %     Besides, although $\phiinv_{*} \phi_{*}= \id$ at any point, for $S_i \in \spd{n}$, $\diffphiinv{\phi(S_2)} \diffphi{S_1}$ might not be the identity map.
% %     Specifically, $\diffphiinv{\phi(S)}\diffphi{S}=\id_{T_S\spd{n}}$ does not imply $\diffphiinv{\phi(S_2)}\diffphi{S_1}=\id_{T_{S_1}\spd{n}}$.
% %     In addition, $\diffphiinv{S_2}\diffphi{S_1}$ could vary for different pairs of $S_1, S_2$.
% %     In other words, the formulae of parallel transportation could be different among different pairs of $S_1, S_2$.
% % % \end{enumerate}
% \end{remark}
In fact, LEM and LCM are special cases of \cref{thm:g_spd}, and so do linear space \& Lie group in \citep{arsigny2005fast} and Lie group in \citep{lin2019riemannian}.
In addition, neither \citep{arsigny2005fast} nor \citep{lin2019riemannian} reveals the Hilbert space structures in $\spd{n}$.
\subsection{Adaptive Riemannian metrics on SPD manifolds} \label{subsec:ada_rie_metric}
The key of \cref{thm:g_spd} lies in $\phi$.
As long as we have a proper $\phi$, Riemannian metrics on SPD manifolds can be induced.
In the following, we will present our mappings and then discuss the induced metrics.

As an eigenvalues function, matrix logarithm in \eqref{eq:mln} is reduced into a scalar logarithm, which is a diffeomorphism between $\bbRplus$ and $\bbRscalar$.
Following this hint, the eigenvalues-based diffeomorphisms between $\spd{n}$ and $\sym{n}$ are reduced to scalar diffeomorphisms between $\bbRplus$ and $\bbRscalar$. 
A very natural idea is to substitute the natural logarithm by scalar logarithms with arbitrary proper bases.
In particular, we first define a general diagonal logarithm $\log(\cdot)$ as
\begin{equation} \label{eq:diag_glog}
    \log_\alpha(X) = \diag(\log_{a_1}^{x_{11}},\log_{a_2}^{x_{22}},\cdots,\log_{a_n}^{x_{nn}}),
\end{equation}
where $\alpha = (a_1, a_2, \cdots, a_n) \in \bbRplus^{n} \setminus \{(1,1,\cdots,1)\}$ is the base vector, $\diag$ is the diagonalization operator, and $X$ is an $n \times n$ diagonal matrix.
By abuse of notation, we denote $\log_\alpha(\cdot)$ as $\log(\cdot)$ for a general diagonal logarithm, and $\log_a^{(\cdot)}$ as $\log^{(\cdot)}$ for a general scalar logarithm.
Specially, $a_1 = \cdots = a_n=e \Rightarrow \log(\cdot) = \ln(\cdot)$.
Together with eigendecomposition, a general matrix logarithm could be derived:
\begin{equation} \label{eq:mlog}
     \mlog(S) = U \log_\alpha(\Sigma) U^\top,    
\end{equation}
where $S = U \Sigma U^\top$ is the eigendecomposition.
As a special case, when $\alpha=(e,e,\cdots,e)$, $\mlog = \mln$.
Similar to the scalar logarithm, we have the following proposition.
\begin{proposition}[Diffeomorphism] \label{props:diffeo_mlog}
    $\mlog$ is a diffeomorphism, a smooth bijection with a smooth inverse $\mlog^{-1}(\cdot):\sym{n} \rightarrow \spd{n}$ defined as
    \begin{equation}\label{eq:mgexp}
        \mlog^{-1}(X) = \mgexp(X) = U \balpha(\Sigma) U^\top,\\
    \end{equation}
    where $\balpha(\Sigma) = \diag(a_1^{\Sigma_{11}},a_2^{\Sigma_{22}},\cdots,a_n^{\Sigma_{nn}})$ is a diagonal exponentiation.
\end{proposition}
\begin{remark} \label{rmk:proposed_charts}
    Note that, $\mlog$ should be more precisely understood as an arbitrary one from the following family
    \begin{equation}
        \{  \mlog^\alpha| \alpha = (a_1, \cdots, a_n) \in \bbRplus^{n} \setminus \{(1,\cdots,1)\}  \}.
    \end{equation}
    By abuse of notation, we will simply use $\mlog$.
    Besides, there could be some ambiguity in \eqref{eq:mlog} under different arrangements of eigenvalues and eigenvectors.
    In fact, there is a correspondence between scalar $\log_{a_i}$ and eigenvalues \& eigenvectors.
    Please refer to \cref{app:subsec:well_difined_glog} for more details.
\end{remark}
Since $\mlog$ is a diffeomorphism from $\spd{n}$ onto $\sym{n}$, all the results in \cref{thm:g_spd} hold true.
\begin{corollary} \label{coro:mlog_spd_properties}
    Following the notations in \cref{thm:g_spd}, we define $\mlogMul, \mlogMulScalar, \langle \cdot, \cdot \rangle_{mlog}$, and $g^{mlog}$ as \eqref{eq:phi_mul}-\eqref{eq:phi_g}.
    Then, we have the following conclusions:
    \begin{enumerate}
        \item 
        $\{\spd{n}, \mlogMul,\mlogMulScalar, \langle \cdot, \cdot \rangle_{mlog} \}$ is a Hilbert space over $\bbRscalar$.
        \item \label{enum:mlog_riem_spd}
        $\{\spd{n}, \mlogMul \}$ is an abelian Lie group.
        $g^{mlog}$ is a Riemannian metric over $\spd{n}$.
        We name this metric as Adaptive Log-Euclidean Metric (ALEM) and denote $g^{mlog}$ as $\galem$.
        The geodesic distance under ALEM is
        \begin{equation} \label{eq:dist_mlog}
            \dalem (S_1, S_2 ) = \| \mlog(S_1) - \mlog(S_2) \|_\rmF.
        \end{equation}
        The associated Riemannian operators are as follows
        \small
        \begin{align} 
            \label{eq:rieexp_gmlog} &\rieexp_{S_1} V = \mgexp(\mlog(S_1)+\diffmlog{S_1}V),\\
            \label{eq:rielog_gmlog} &\rielog_{S_1}S_2 = \diffmgexp{X_1}(\mlog(S_2)-\mlog(S_1)),\\
            \label{eq:pt_mlog} &\pt{S_1}{S_2}(V) = \diffmgexp{X_2} \circ \diffmlog{S_1}(V),
        \end{align}
        \normalsize
        where $X_i = \mlog(S_i) \in \sym{n}$.
        \item
        $\mlog$ is an isomorphism: (a) a linear isomorphism preserving the inner product; (b) a Lie group isomorphism; (3) a Riemannian isometry.
    \end{enumerate}
\end{corollary}
\begin{remark}
    Obviously, ALEM would vary with different $\mlog$.
    This is why we use the plural to describe our metrics in the title.
    Besides, our metrics could be learnable.
    This is why we call them adaptive metrics.
\end{remark}
Given the differential maps of $\mlog$ and $\mgexp$, we can calculate \eqref{eq:rieexp_gmlog}-\eqref{eq:pt_mlog}.
Please refer to \cref{app:subsec:differentials} for the concrete formulae of differential maps.

\section{Properties of the proposed metrics} \label{sec:properties}
Since our $\mlog$ generalizes matrix logarithm, ALEM is indeed an adaptive version of LEM.
Therefore, intuitively, ALEM would share every property of LEM.
In this section, we will present some useful properties of our ALEM for machine learning, including Fréchet mean and invariance properties.
All of the properties are shared by LEM, as LEM is a special case of our ALEM.

Fréchet means are important tools for SPD matrices learning \citep{harandi2018dimensionality,chakraborty2018statistical,brooks2019riemannian,chakraborty2020manifoldnorm}.
Like LEM, our ALEM also enjoys closed forms of Fréchet means.
We present a more general result, the weighted Fréchet mean.
\begin{proposition}[Weighted Fréchet Means] \label{props:geo_mean_spd}
    For $m$ points $S_1,\cdots S_m$ in SPD manifolds with associated weights $w_1,\cdots,w_m \in \bbRplus$,
    the weighted Fréchet mean $M$ over the metric space $\{\spd{n},\dalem\}$ has a closed form
    \begin{equation} \label{eq:fm_alem}
        M = \mgexp(\sum_{i=1}^{m} \frac{w_i}{\sum_{j=1}^{m} w_i}\mlog(S_i)).
    \end{equation}
\end{proposition}

Like LEM, although our ALEM does not conform with the affine-invariance, our ALEM enjoys some other kinds of invariance.
\begin{proposition}[Bi-invariance] \label{props:biinvariance}
    ALEM is a Lie group bi-invariant metric.
\end{proposition}
\begin{proposition} [Exponential Invariance] \label{props:exp_invariance}
    The Fréchet means under ALEM are exponential-invariant. 
    In other words, for $S_1,\cdots S_m \in \spd{n}$ and $\beta \in \bbRscalar$,
    \begin{equation}
        (\mathrm{FM}(S_1,\cdots S_m))^\beta = \mathrm{FM}(S_1^\beta,\cdots S_m^\beta),
    \end{equation}
    where $\mathrm{FM}(S_1,\cdots S_m))$ means the Fréchet mean of $S_1,\cdots S_m$.
\end{proposition}
According to \citep{ando2004geometric}, certain properties are expected for proper geometric means. 
In \cref{app:subsec:add_props_fm}, we demonstrate that our ALEM-based geometric mean also enjoy several properties required in \citep{ando2004geometric}.

\begin{proposition}[Similarity Invariance] \label{props:sim_invariance}
    The geodesic distance under ALEM is similarity invariant.
    In other words, let $R \in SO(n)$ be a rotation matrix, $s \in \bbRplus$ is a scale factor.
    Given any two SPD matrices $S_1$ and $S_2$, we have
    \begin{equation}
        \dalem(S_1,S_2)=\dalem(s^2RS_1 R^\top,s^2RS_2 R^\top).
    \end{equation}
\end{proposition}
Let us explain a bit more about the above three kinds of invariance.
Firstly, among metrics on Lie groups, bi-invariant metrics are the most convenient ones \citep[Chapter V]{sternberg1999lectures}.
Secondly, exponential-invariance offers a fast computation for Fréchet means under exponential scaling.
At last, similarity-invariance is significant for describing the frequently encountered covariance matrices \citep{arsigny2005fast}.

The above discussion focuses on theoretical side.
Now, let us reconsider \eqref{eq:mlog} in a numerical way.

\begin{proposition} 
    $\mlog$ can be rewritten as
        \begin{align}
            \label{eq:rw_org_mlog} \mlog(S) 
            &= U \log_\alpha(\Sigma) U^\top,\\
            \label{eq:rw_mul_mlog}          
            &= U A \ln(\Sigma) U^\top, \\
            \label{eq:rw_div_mlog}          
            &= U \frac{\ln(\Sigma)}{B} U^\top,
        \end{align}
    where $\frac{X}{Y}$ is the diagonal division, $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$, and $A = \frac{I}{B}$.
\end{proposition}

Based on the above proposition, more analyses could be carried out from a numerical point of view.
First, $\mlog(\cdot)$ can balance the eigenvalues of an input SPD matrix $S$ by exploiting different bases for different eigenvalues.
In Riemannian algorithms, manifold-valued features usually contain vibrant information.
We expect that by the above adaptation, manifold-valued data could be better fitted and the learning ability of algorithms could be further promoted.
\begin{remark} 
Note that the design and analysis in \cref{subsec:ada_rie_metric} and \cref{sec:properties} can also be readily transferred into LCM, generating an adaptive version of LCM.
\end{remark}
\section{Applications to SPD neural networks} \label{sec:ada_param_layers}
Since Riemannian metrics are foundations of Riemannian learning algorithms, our ALEM has the potential to rewrite Riemannian algorithms, especially the algorithms based on LEM.
Besides, the base vector in $\mlog$ could bring vibrant diversity of our ALEM.
This adaptive mechanism could help the algorithm better fit with complicated manifold-valued data. 
Especially in Riemannian neural networks, as we will show, optimization of base vectors can be easily embedded into the standard backpropagation (BP) process.
Therefore, we focus on the applications of our metrics to SPD neural networks.

In the existing SPD neural networks, on activation or classification layers, SPD features would interact with the logarithmic domain by matrix logarithm \citep{huang2017riemannian, zhen2019dilated, chakraborty2020manifoldnet, nguyen2021geomnet}.
The underlying mechanism of this interaction is that matrix logarithm is an isomorphism, identifying the SPD manifold under LEM with the Euclidean space $\sym{n}$.
This projection can therefore maintain the LEM-based geometry of SPD features.
However, in deep networks, the geometry might be more complex.
Since ALEM can vibrantly adapt with network learning, compared with the plain LEM, our ALEM could more faithfully respect the geometry of SPD deep features.
$\mlog$ thus possesses more advantages than the fixed $\mln$.
We therefore replace the vanilla matrix logarithm with our $\mlog$, to respect the more advantageous geometry, \ie the ALEM-based geometry.

We focus on the most classic SPD network, SPDNet \citep{huang2017riemannian}, where matrix logarithm is used for classification.
Please refer to \cref{app:subsec:review_SPDNet} for a quick review of SPDNet, if necessary.
Specifically, matrix logarithm in the LogEig layer is substituted by our $\mlog$. 
We call this layer as adaptive logarithm (ALog) layer. 
We set the base vector $\alpha$ as a learnable parameter.
In this way, as $\mlog$ is an isomorphism, the network can implicitly respect the ALEM-based Riemannian metric by learning the $\mlog$ explicitly.
Besides, since our ALog layer is independent of specific network architectures, it can be plugged into other SPD deep networks as well.

\section{Parameters learning} 

% The specificities of the proposed ALog layer are the nonlinear manipulation of both inputs and parameters.
% In this section, we will first discuss the concrete way of updating the parameters and then move on to its gradient computation.

Now let us explain how to optimize the proposed layer in a standard backpropagation (BP) framework.
Here, we focus on the updates of parameters.
As for the gradient computation, details are presented in \cref{app:subsec:gradients}.
Denote the dimension of an input SPD matrix $S$ as $d \times d$.
Recalling \eqref{eq:rw_org_mlog}-\eqref{eq:rw_div_mlog}, there are three ways to implement parameter learning
We could learn the base vector $\alpha$ in \eqref{eq:rw_org_mlog}, diagonal matrix $A$ in \eqref{eq:rw_mul_mlog}, or diagonal matrix $B$ in \eqref{eq:rw_div_mlog}, respectively.

For learning $A$ in \eqref{eq:rw_mul_mlog} or $B$ in \eqref{eq:rw_div_mlog}, since the parameters (diagonal elements) lie in a Euclidean space $\bbR{d}$, the optimization can be easily integrated into the BP algorithm.
We call learning $A$ MUL and learning $B$ DIV.

For the case of learning $\alpha$ in \eqref{eq:rw_org_mlog}, since $\alpha$ lies in a non-Euclidean space, specific updating strategies should be considered.
Without loss of generality, we focus on the case of a scalar parameter $a>0 \& a \neq 1$.
The condition of $a \neq 1$ can be further waived since we can set $a=1+\epsilon$ if $a=1$. 
Then there is only one constraint about positivity.
We use the shift-ReLU of an unconstrained parameter, \ie $\max(\epsilon,a)$ with $\epsilon \in\bbRplus$.
This strategy is named RELU.
Other tricks like square are also feasible, but we will focus on the RELU.
In addition, positive scalar $a$ can be directly optimized by Riemannian optimization \citep{absil2009optimization}.
We further prove that this strategy is completely equal to learning $B$ directly.
Please refer to \cref{app:subsec:geom} for more details.

Therefore, there are three ways of updates, \textit{i.e.,} RELU, DIV, and MUL, summarized in \cref{tb:param_learning}.

\begin{table}[htbp]
    \small
    \centering
    \caption{Parameter Learning in ALog layer.}
    \label{tb:param_learning}
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{cccc}
    \toprule
    Name & Detail & Constrain & Method\\
    \midrule
    RELU & Optimizing base vector $\alpha$ (\eqref{eq:rw_org_mlog}) & Positive & Shift-ReLu $\max(\epsilon,\alpha)$\\
    MUL & Optimizing diagonal elements of $A$ (\eqref{eq:rw_mul_mlog}) & Unconstrained & Standard BP\\
    DIV & Optimizing diagonal elements of $B$ (\eqref{eq:rw_div_mlog}) & Unconstrained & Standard BP\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-2mm}
\end{table}



\section{Experiments} \label{sec:experiments}
In this section, we validate the efficacy of our approaches on multiple datasets.
We would like to clarify that our method does not necessarily aim to achieve the SOTA in a general sense for the following tasks, but rather to promote the learning abilities of the family of SPD-based methods.

\subsection{Datasets and settings}
As we discussed before, although the proposed ALog layers can be plugged into the existing SPD networks, we focus on the SPDNet framework \citep{huang2017riemannian}.
We follow the PyTorch code provided by SPDNetBN\footnote{https://proceedings.neurips.cc/paper/2019/file/6e69ebbfad976d4637bb4b39de261bf7-Supplemental.zip} to reproduce SPDNet \& SPDNetBN and implement our approaches.
% Following previous work \citep{huang2017riemannian,brooks2019riemannian}, we evaluate our methods on theHDM05 \citep{muller2007documentation}, FPHA \citep{garcia2018first}, AFEW \citep{dhall2018emotiw} datasets. 

Following previous work \citep{huang2017riemannian,brooks2019riemannian}, we evaluate our methods on three datasets, the HDM05 \citep{muller2007documentation} for skeleton-based actions recognition, the FPHA \citep{garcia2018first} for skeleton-based hand gestures recognition, and the AFEW \citep{dhall2018emotiw} for emotions recognition. 
HDM05 dataset is comprised of motion capture data (MoCap) covering 130 action classes.
Each data point is a sequence of frames of 31 3D coordinates.
Each sequence can be represented by a $93 \times 93$ temporal covariance matrix.
For a fair comparison, we exploit the pre-processed $93 \times 93$ covariance features \footnote{https://www.dropbox.com/s/dfnlx2bnyh3kjwy/data.zip?dl=0} released by \citep{brooks2019riemannian}, which trims the dataset down to 2086 points scattered throughout 117 classes by removing some under-represented classes.
Following the settings in \citep{brooks2019riemannian},  we split the dataset into 50\% for training and 50\% for testing.
FPHA includes 1,175 clips of 45 different action categories.
Each frame is represented by 21 3D coordinates.
Similarly, each sequence can be modelled by a $63 \times 63$ covariance matrix.
For a fair comparison, we follow the experimental protocol in \citep{garcia2018first}, where 600 sequences are used for training and 575 sequences are used for testing.
AFEW consists of 7 kinds of emotions with 773 samples for training and 383 samples for validation.
We use the released pre-trained FAN\footnote{https://github.com/Open-Debin/Emotion-FAN} \citep{meng2019frame} to extract deep features and establish a $512 \times 512$ temporal covariance matrix for each video.

We denote $\{d_0, d_1,\cdots,d_L\}$ as the dimensions of each transformation layer in SPDNet backbone.
Following the settings in \citep{brooks2019riemannian}, all networks are trained by the default SGD with a fixed learning rate $\gamma$, and batch size of 30.
To make ALog start from the vanilla matrix logarithm, the parameters in MUL, DIV and RELU are initialized as 1,1 and $e$, respectively.
By abuse of notation, SPDNet-ALog-MUL is abbreviated as ALog-MUL, denoting that we substitute the LogEig layer (matrix logarithm) in SPDNet with our proposed ALog optimized by MUL.

\subsection{Experimental results} \label{sec:results_on_MLog}
On the three datasets, the training epochs are set to be 200, 500, and 100.
We verify our ALog on the SPDNet with various architectures.
Besides, we further test the robustness of the proposed layer against different learning rates on the HDM05 and FPHA datasets.
Generally speaking, among all three kinds of optimization, \textbf{MUL} shows more robust performance gain and achieves consistent improvement over vanilla matrix logarithm. Besides, we could also observe that ALog-MUL is comparable to or even better than SPDNetBN, which yet brings much more complexity than our approach.
The main reason of the superiority of our ALog against vanilla matrix logarithm is that our ALog can adaptively respect vibrant geometry of SPD manifolds, depending on the characteristics of datasets, while only LEM can be respected by matrix logarithm.
The following are detailed observations and analyses.
% \begin{table} 
%   \small
%   \centering
%   \caption{Results of ALog on the HDM05 dataset.}
%   \label{tb:mlog_on_HDM05}
%   \begin{tabular}{cccc}
%     \toprule
%     Architecture & \{93, 30\} & \{93, 70, 30\} & \{93, 70, 50, 30\} \\  
%     \midrule
%     $\gamma$ & \multicolumn{3}{c}{$1e^{-2}$}\\ 
%     \midrule
%     SPDNet & 62.92±0.81 & 62.87±0.60 & 63.03±0.67  \\
%     SPDNetBN & 63.03±0.75 & 58.27±1.7 & 52.02±2.34  \\
%     ALog-MUL & 63.52±0.75 & 63.86±0.58 & \textbf{63.94±0.44} \\
%     ALog-DIV & \textbf{63.60±0.79} & 63.93±0.52 & 63.81±0.7 \\
%     ALog-RELU & 63.02±0.79 & \textbf{63.94±0.64} & 63.14±0.65 \\
%     \midrule
%     $\gamma$ & \multicolumn{3}{c}{$5e^{-2}$}\\ 
%     \midrule
%     SPDNet & 63.89±0.73 & 64.00±0.65 & 63.72±0.61 \\
%     SPDNetBN & 63.75±0.69 & 48.78±5.15 & 37.84±6.10 \\
%     ALog-MUL & 64.4±0.68 & 64.60±0.69 & 64.36±0.49 \\
%     ALog-DIV & \textbf{64.81±0.64} & \textbf{64.84±0.65} & \textbf{64.80±0.36} \\
%     ALog-RELU & 63.97±0.75 & 64.10±0.63 & 63.78±0.46\\
%     \bottomrule
%   \end{tabular}
%   \vspace{-2mm}
% \end{table}



% \begin{table}
%     \small
%     \centering
%     \caption{Results of ALog on the FPHA dataset.}
%     \label{tb:mlog_on_FPHA}
%     \begin{tabular}{cccccc}
%     \toprule
%     Methods & SPDNet & SPDNetBN & ALog-MUL & ALog-DIV & ALog-RELU\\
%     \midrule
%     Acc.  & 85.73±0.80 & 86.83±0.74 & 87.8±0.71 & \textbf{88.07±1.13} & 86.65±0.68 \\
%     \bottomrule
%     \end{tabular}
%     \vspace{-2mm}
% \end{table}

\begin{table}[htbp]
    \small
    \centering
    \caption{Results of ALog on the HDM05 dataset.}
    \label{tb:mlog_on_HDM05}
    \begin{tabular}{c|ccc|ccc}
    \toprule
    Learning rate & \multicolumn{3}{c|}{$1e^{-2}$} & \multicolumn{3}{c}{$5e^{-2}$} \\
    \hline
    Depth & 1 & 2 & 3 & 1 & 2 & 3 \\
    \hline
    SPDNet & 62.92±0.81 & 62.87±0.60 & 63.03±0.67 & 63.89±0.73 & 64.00±0.65 & 63.72±0.61 \\
    SPDNetBN & 63.03±0.75 & 58.27±1.7 & 52.02±2.34 & 63.75±0.69 & 48.78±5.15 & 37.84±6.10\\
    ALog-MUL & 63.52±0.75 & 63.86±0.58 & \textbf{63.94±0.44} & 64.4±0.68 & 64.60±0.69 & 64.36±0.49 \\
    ALog-DIV & \textbf{63.60±0.79} & 63.93±0.52 & 63.81±0.7 & \textbf{64.81±0.64} & \textbf{64.84±0.65} & \textbf{64.80±0.36} \\
    ALog-RELU & 63.02±0.79 & \textbf{63.94±0.64} & 63.14±0.65 & 63.97±0.75 & 64.10±0.63 & 63.78±0.46 \\
    \bottomrule
    \end{tabular}
    \vspace{-2mm}
\end{table}

\begin{table}
    \begin{minipage}{0.5\linewidth}
            \small
            \centering
            \caption{Results of ALog on the FPHA dataset.}
            \label{tb:mlog_on_FPHA}
            \begin{tabular}{cc}
                \toprule
                Architecture & \{63, 33\} \\
                \midrule
                SPDNet & 85.73±0.80 \\
                SPDNetBN & 86.83±0.74 \\
                ALog-MUL & 87.8±0.71 \\
                ALog-DIV & \textbf{88.07±1.13} \\
                ALog-RELU & 86.65±0.68 \\
                \bottomrule
            \end{tabular}
    \end{minipage}
    \begin{minipage}{0.5\linewidth}
            \small
            \centering
            \caption{Results of ALog on the AFEW dataset.}
            \label{tb:mlog_on_AFEW}
            \begin{tabular}{ccccc}
                \toprule
                Depth & 1 & 2 & 3 & 4 \\
                % Architecture & \{512, 100\} & \{512, 200, 100\} & \{512, 400, 200, 100\} & \{512, 400, 300, 200, 100\} \\
                \midrule
                SPDNet & 48.53 & 46.89 & 48.24 & 47.22 \\
                SPDNetBN & 46.89 & 46.65 & 47.62 & 48.35 \\
                ALog-MUL & \textbf{48.57} & \textbf{48.13} & \textbf{49.45} & \textbf{50.62} \\
                ALog-DIV & 48.42 & 48.02 & 48.13 & 49.89 \\
                ALog-RELU & 48.06 & 47.25 & 48.86 & 48.1 \\
                \bottomrule
            \end{tabular}
    \end{minipage}
\end{table}


\textbf{Results on the HDM05 dataset.}
% \subsubsection{Results on the HDM05 Dataset}
The 10-fold results are presented in \tabref{tb:mlog_on_HDM05}, where dataset split and weights initialization are randomized.
Following \citep{huang2017riemannian}, three architectures are implemented on this dataset, \ie \{ 93, 30\}, \{ 93, 70, 30\}, and \{ 93, 70, 50, 30\}.
Generally speaking, endowed with the ALog, SPDNet would achieve consistent improvement.
Among all three kinds of optimization, RELU only brings limited improvement.
The reason might be that RELU fails to respect the innate geometry of the positive constraint.
There is another interesting observation worth mentioning.
In \citep{brooks2019riemannian}, only the result of SPDNetBN under the architecture of $\{93,30\}$ is reported on this dataset.
Our experiments show that with the network going deeper, SPDNetBN tends to collapse, while our ALog layer performs robustly in all settings.
% \begin{table}
%     \small
%     \centering
%     \caption{Results of ALog on the FPHA dataset.}
%     \label{tb:mlog_on_FPHA}
%     \begin{tabular}{cccc}
%     \toprule
%     Methods & SPDNet & SPDNetBN & ALog-MUL \\
%     Acc.  & 85.73±0.80 & 86.83±0.74 & 87.8±0.71 \\
%     \midrule
%     Methods & ALog-DIV & ALog-RELU &  \\
%     Acc.  & \textbf{88.07±1.13} & 86.65±0.68 &  \\
%     \bottomrule
%     \end{tabular}
% \end{table}
% \begin{figure}
%   \centering
%   \includegraphics[trim={0mm 5mm 0mm 0mm}, width=0.9\linewidth]{img/ACC_ALog_FPHA.pdf}
%   \caption{Accuracy curves on the FPHA dataset.}
%   \label{fig:acc_fpha_mlog}
%   % \vspace{-2mm}
% \end{figure}


% \begin{table}
% \parbox{.49\linewidth}{
% \centering
% \caption{Equivariance error of different priors.}\label{tab:eq_priors}
% %\resizebox{0.99\linewidth}{!}{
% \begin{tabular}{c|c|c|c}
%     \toprule
%      \textbf{Prior}    & \textbf{Scaling} & \textbf{Rotation} & \textbf{Coloring} \\
%      \midrule
%      \textbf{SG} &190.56 &159.77 &165.58 \\
%      \textbf{MoG} &188.93 &158.45 &162.19 \\
%      \textbf{VAMP} &192.78 &160.39 &163.42 \\
%      \rowcolor{gray!20} \textbf{Diffusion} &\textbf{185.72} &\textbf{153.20} &\textbf{158.65}  \\
%     \bottomrule
% \end{tabular}
% }
% %}
% \hfill
% \parbox{.49\linewidth}{
% \centering
% \caption{Equivariance error of different velocities.}\label{tab:eq_velocity}
% %\resizebox{0.99\linewidth}{!}{
% \begin{tabular}{c|c|c|c}
%     \toprule
%     \textbf{Prior}  & \textbf{Scaling} & \textbf{Rotation} & \textbf{Coloring} \\
%      \midrule
%     \textbf{Heat} &223.45 &211.67 &208.75 \\
%      \textbf{FP}  &210.34 &189.67 &193.58 \\
%      \textbf{OHJ}  &191.45 &164.45 &164.58 \\
%      \rowcolor{gray!20}\textbf{GHJ} &\textbf{185.72} &\textbf{153.20} &\textbf{158.65} \\
%     \bottomrule
%     \end{tabular}
% }
% %}
% \end{table}


\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \includegraphics[trim={0mm 5mm 0mm 0mm}, width=0.5\columnwidth]{ACC_ALog_FPHA.pdf}
  \caption{Accuracy curves on the FPHA dataset.}
  \label{fig:acc_fpha_mlog}
  \vspace{-2mm}
\end{wrapfigure}


\textbf{Results on the FPHA dataset.}
% \subsubsection{Results on the FPHA Dataset}
On this dataset, we validate our approach, with a learning rate of $1e^{-2}$, over 10-fold cross-validation on random initialization.
Since our experiments show that the vanilla SPDNet is already saturated with 1 transformation layer, we just report the results on the architecture of $\{63,33\}$, which are presented in \tabref{tb:mlog_on_FPHA}. 
We find that although DIV achieves the best performance on this dataset, it presents the biggest variance.
The reason is that there is an underlying nonlinear scaling mechanism, which is detailed in \cref{app:subsec:non_linear_div}.
However, ALog-MUL achieves robust improvement, and even surpasses SPDNetBN.
This once again demonstrates the significance of our adaptive mechanism for Riemannian deep networks.
Finally, in terms of convergence analysis, accuracy curves with and without ALog are reported in \figref{fig:acc_fpha_mlog} as well.

% \begin{table}
%   \small
%   \centering
%   \caption{Results of ALog on the AFEW dataset.}
%   \label{tb:mlog_on_AFEW}
%   \begin{tabular}{ccccc}
%     \toprule
%     Depth & 1 & 2 & 3 & 4 \\
%     % Architecture & \{512, 100\} & \{512, 200, 100\} & \{512, 400, 200, 100\} & \{512, 400, 300, 200, 100\} \\
%     \midrule
%     SPDNet & 48.53 & 46.89 & 48.24 & 47.22 \\
%     SPDNetBN & 46.89 & 46.65 & 47.62 & 48.35 \\
%     ALog-MUL & \textbf{48.57} & \textbf{48.13} & \textbf{49.45} & \textbf{50.62} \\
%     ALog-DIV & 48.42 & 48.02 & 48.13 & 49.89 \\
%     ALog-RELU & 48.06 & 47.25 & 48.86 & 48.1 \\
%     \bottomrule
%   \end{tabular}
%   \vspace{-2mm}
% \end{table}

\textbf{Results on the AFEW dataset.}
% \subsubsection{Results on the AFEW Dataset}
On this dataset, the learning rate is $5e^{-2}$ and we validate our method under four network architectures, \ie \{512, 100\}, \{512, 200, 100\}, \{512, 400, 200, 100\}, and \{512, 400, 300, 200, 100\}.
Note that, on this dataset, SPDNetBN tends to present relatively large fluctuations in performance, so we compute the median of the last ten epochs.
On various architectures, consistent improvement can be observed when SPDNet is endowed with our ALog.
In addition, MUL achieves the best among all three optimization tricks.
Another interesting observation is that SPDNetBN seems not very effective on these deep features, while our methods show consistent superior performance, particularly obvious for our ALog-MUL. This indicates that our adaptive layer maintains effectiveness when applied to covariance matrices from deep features.

\textbf{Model complexity.}
% \subsubsection{Model Complexity}
Our ALog manifests the same complexity no matter how it is optimized.
Without loss of generality, the below discussion focuses on ALog-MUL.
The extra computation and memory cost caused by the ALog layer are minor.
It only depends on the final dimension of the network.
Let us take the deepest one on the AFEW dataset as an example.
Our ALog only brings 100 unconstrained scalar parameters, while SPDNetBN needs an SPD matrix parameter for each Riemannian batch normalization (RBN) layer.
The total number of the parameters in RBN layers sums up to $400^2+300^2+200^2$, which is much bigger than ours.
In addition, the SPDNetBN needs to store the running mean of SPD matrices in every RBN layer, while our ALog only needs to store a vector.
In terms of computation, the extra cost of our ALog is secondary as well.
The forward and backward computation of our ALog is generally the same as the plain matrix logarithm, while computation in the RBN layer is much more complex.
All in all, our ALog can consistently improve the performance of the SPDNet and achieve comparable or better results against SPDNetBN with much cheaper computation and memory costs.

\begin{table}[htbp]
    \small
    \centering
    \caption{Results of fixed bases on HDM05 and FPHA datasets.}
    \label{tab:fix_base_alog}
    \begin{tabular}{c|ccc|c}
        \toprule
        Dataset & \multicolumn{3}{c|}{HDM05} & FPHA \\
        Architecture & \{93, 30\} & \{93, 70, 30\} & \{93, 70, 50, 30\} & \{63, 33\} \\
        \hline
        SPDNet-Log2 & 63.93±0.81 & 63.54±0.50 & 63.98±0.63 & 86.65±0.67 \\
        SPDNet & 63.89±0.73 & 64.00±0.65 & 63.72±0.61 & 85.73±0.80 \\
        SPDNet-Log10 & 63.45±0.33 & 63.8±0.71 & 63.64±0.64 & 78.42±0.77 \\
        \rowcolor{gray!20}SPDNet-ALog-MUL & \textbf{64.4±0.68} & \textbf{64.60±0.69} & \textbf{64.36±0.49} & \textbf{87.8±0.71} \\
        \bottomrule
    \end{tabular}
    \vspace{-2mm}
\end{table}

\textbf{Ablation studies.}
To further demonstrate the utility of the adaptive mechanisms in our approach, we further validate the ALog layer with fixed bases.
As decimal and binary are the two most common systems, we use $\log_{10}$ and $\log_{2}$ as examples of shrinking and expanding  $\log_e$. 
Specifically, we set $\log_\alpha$ = $\log_{10}$ and $\log_\alpha = \log_2$ in \eqref{eq:rw_org_mlog}, respectively. 
We refer to the network with binary/decimal base SPDNet-Log2/SPDNet-Log10. 
Note that, when $\log_\alpha=\log_e$, \eqref{eq:rw_org_mlog} is reduced to the vanilla matrix logarithm and the network is our baseline, \ie SPDNet. 
We conduct 10-fold experiments on the HDM05 and FPHA datasets and set the learning rate to $5e^{-2}$ and $1e^{-2}$, respectively, while keeping the other settings consistent with previous experiments.
The results are presented in \cref{tab:fix_base_alog}. 
We observe that the fixed logarithms basically show similar or slightly worse results compared with the vanilla $\log_e$, while our ALog shows consistent improvement.
Besides, $\log_{10}$ does not converge in the FPHA dataset. 
In fact, $\log_{10}$ could shrink the gradient, slowing down convergence, especially under a small learning rate. 
In contrast, our ALog maintains consistent effectivity. 
In summary, our ALog can respect vibrant geometry induced by $\mlog$ and thus benefit SPD networks learning.
On the other hand, we further visualize the parameters in ALog-MUL.
We observe that the distribution of parameters within dataset is consistent, but varies between datasets.
For more details, please refer to \cref{app:subsec:vis}.


% \section{Limitations}
% Our approach is the most natural generalization of LEM. Despite the flexibility and easy-of-use, it may suffer from limited expressibility. Actually, \cref{thm:g_spd} allows for many more possibilities to develop Riemannian metrics. Investigating more powerful metrics is a valuable research direction.


\section{Conclusion}
Riemannian metrics are foundations for Riemannian learning algorithms.
In this paper, we proposed a general framework for designing Riemannian metrics on arbitrary manifolds by pullback metrics.
According to this framework, adaptive Riemannian metrics were introduced for SPD matrices learning.
We also present comprehensive and rigorous theories of our metrics.
Extensive experiments indicate that SPD deep networks can benefit from our metrics.

\bibliographystyle{unsrtnat}
\bibliography{ref.bib}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\clearpage
\appendix
% \onecolumn

\section{Limitations}
This work is not without limitations.
Our approach is the most natural generalization of LEM. Despite the flexibility and easy-of-use, it may suffer from limited expressibility. 
Actually, \cref{thm:g_spd} allows for many more possibilities to develop Riemannian metrics. Investigating more powerful metrics is a valuable research direction.

\section{Preliminaries} \label{app:sec:preliminaries}
\subsection{Smooth manifolds}
We first recap some basic definitions related to this work on smooth manifolds.
For in-depth understanding, please kindly refer to \citep{loring2011introduction,lee2013smooth}. 

The most important properties of manifolds are locally Euclidean, which are described by coordinate systems.

\begin{definition}[Coordinate Systems, Charts, Parameterizations] \label{def:Parameterization}
A topological space $\calM$ is locally Euclidean of dimension $n$ if every point in $\calM$ has a neighborhood $U$ such that there is a homeomorphism $\phi$ from $U$ onto an open subset of $\mathbb{R}^{n}$. 
We call the pair $\{ U, \phi: U \rightarrow \mathbb{R}^{n}\}$ as a chart, $U$ as a coordinate neighborhood, the homeomorphism $\phi$ as a coordinate map or coordinate system on $U$, and $\phi^{-1}$ as a parameterization of $U$. 
\end{definition}

Intuitively, a coordinate system is a bijection that locally identifies the Euclidean space with the manifold.
It locally preserves the most basic properties in a manifold, the topology. 
Now, topological manifolds, which are foundations of smooth manifolds, can be defined.
\begin{definition}[Topological Manifolds]
    A topological manifold is a locally Euclidean, second countable, and Hausdorff topological space.
\end{definition}

In smooth manifolds, compatibility is further required to define smooth structures or operations.
\begin{definition}[$C^{\infty}$-compatible] \label{def:compatible}
Two charts $\{ U, \phi_1: U \rightarrow \mathbb{R}^{n} \},\{ V, \phi_2: V \rightarrow \mathbb{R}^{n} \}$ of a locally Euclidean space are $C^{\infty}$-compatible if the following two composite maps
\begin{equation}
    \begin{aligned}
        \phi_1 \circ \phi_2^{-1} &: \phi_2(U \cap V) \rightarrow \phi_1(U \cap V), \\
        \quad \phi_2 \circ \phi_1^{-1} &: \phi_1(U \cap V) \rightarrow \phi_2(U \cap V)        
    \end{aligned}
\end{equation}
are $\cinf$.
\end{definition}

By abuse of notation, we view $\phi$ alternatively as a chart or map according to the context, and abbreviate $C^{\infty}$-compatible as compatible.

\begin{definition}[Atlases] \label{def:atlas}
A $C^{\infty}$ atlas or simply an atlas on a locally Euclidean space $\calM$ is a collection $\calA=\{ \{ U_{\alpha}, \phi_{\alpha} \} \}$ of pairwise $\cinf$-compatible charts that cover $\calM$.
\end{definition}

An atlas $\mathcal{A}$ on a locally Euclidean space is said to be maximal if it is not contained in a larger atlas. 
With a maximal atlas, smooth manifold can be defined.
\begin{definition}[Smooth Manifolds] 
A smooth manifold is defined as a topological manifold endowed with a maximal atlas.
\end{definition}

We call the maximal atlas of a smooth manifold its differential structure.
In addition, every atlas $\calA$ is contained in a unique maximal atlas $\calA^+$ \citep{loring2011introduction}.
Therefore, an atlas can be used to identify the differential structure of a smooth manifold.
In this paper, manifolds always mean smooth manifolds.
Now, we can define the smoothness of a map between manifolds.
\begin{definition}[Smoothness] \label{def:smoothness}
Let $\calN$ and $\calM$ be smooth manifolds, and $f: \calN \rightarrow \calM$ a continuous map, $f(\cdot)$ is said to be $\cinf$ or smooth, if there are atlases $\calA_n$ for $\calN$ and $\calA_m$ for $\calM$ such that for every chart $\{ U, \phi \}$ in $\calA_n$ and $\{ V, \psi \}$ in $\calA_m$, the map
\begin{equation}
    \psi \circ F \circ \phi^{-1}: \phi\left(U \cap f^{-1}(V)\right) \rightarrow \mathbb{R}^{m}
\end{equation}
is $C^{\infty}$.
\end{definition}
In elementary calculus, smooth functions have derivatives.
In manifolds, derivatives are generalized into differential maps.
\begin{definition} [Differential Maps]
    Let $f: \calN \rightarrow \calM$ be a $C^{\infty}$ map between two manifolds. At each point $p \in \calN$, the map $f$ induces a linear map of tangent spaces, called its differential at $p$,
    \begin{equation}
        f_{*,p}: T_p \calN \rightarrow T_{f(p)} \calM.
    \end{equation}
    $f_{*,p}$ can be locally represented by the Jacobian matrix under a chart $\{ U, \phi \}$ about $p$ and a chart $\{ V, \psi \}$ about $f(p)$,
    \begin{equation}
        f_{*,p} := \frac{\partial f}{\partial x} := \frac{\partial \psi f \phi^{-1}}{\partial x},
    \end{equation}
    where $\frac{\partial f}{\partial x}$ is called the derivative (Jacobian matrix) of $f$ under the charts of $\{ U, \phi \}$ and $\{ V, \psi \}$.
\end{definition}
With the definition of smoothness, it is possible to define smooth algebraic structures on a manifold, \ie Lie groups.
Intuitively, a Lie group is an integration of algebra (group) and geometry (manifold).
\begin{definition}[Lie Groups] \label{def:lie_group}
A manifold is a Lie group, if it forms a group with a group operation $\odot$ such that $m(x,y) \mapsto x \odot y$ and $i(x) \mapsto x_{\odot}^{-1}$ are both smooth, where $x_{\odot}^{-1}$ is the group inverse of $x$.
\end{definition}
\subsection{Riemannian manifolds}
When manifolds are endowed with Riemannian metrics, various Euclidean operators can find their counterparts in manifolds.
A plethora of discussions can be found in \citep{do1992riemannian}.

\begin{definition}[Riemannian Manifolds] \label{def:riem_manifold}
A Riemannian metric on $\calM$ is a smooth symmetric covariant 2-tensor field on $\calM$, which is positive definite at every point.
A Riemannian manifold is a pair $\{\calM,g\}$, where $\calM$ is a smooth manifold and $g$ is a Riemannian metric.
\end{definition}
As a basic fact in differential geometry, every smooth manifold is a Riemannian manifold \citep[Proposition~2.10, Chatper~1]{do1992riemannian}.
Therefore, in the following, we will alternatively use manifolds or Riemannian manifolds.
\begin{definition} [Pullback Metrics] \label{def:pullback_metrics_app}
    Suppose $\calM,\calN$ are smooth manifolds, $g$ is a Riemannian metric on $\calN$, and $f:\calM \rightarrow \calN$ is smooth.
    Then the pullback of a tensor field $g$ by $f$ is defined point-wisely,
    \begin{equation}
        (f^*g)_p(V_1,V_2) = g_{f(p)}(f_{*,p}(V_1),f_{*,p}(V_2)),
    \end{equation}
    where $p$ is an arbitrary point in $\calM$, $f_{*,p}(\cdot)$ is the differential map of $f$ at $p$, and $V_1,V_2$ are tangent vectors in $T_p\calM$.
    If $f^*g$ is positive definite, it is a Riemannian metric on $\calM$, called the pullback metric defined by $f$.
\end{definition}

\begin{definition}[Isometries] \label{def:isometry}
If $\{M, g\}$ and $\{\widetilde{M}, \widetilde{g}\}$ are both Riemannian manifolds, a smooth map $f: M \rightarrow$ $\widetilde{M}$ is called a (Riemannian) isometry if it is a diffeomorphism that satisfies $f^{*} \tilde{g}=g$.
\end{definition}
If two manifolds are isometric, they can be viewed as equivalent.
Riemannian operators in these two manifolds are closely related. 
\begin{definition}[Bi-invariance] \label{def:bi_invariance}
A Riemannian metric $g$ over a Lie group $\{G, \odot\}$ is left-invariant, if for any $x,y \in G$ and $V_1,V_2 \in T_x\calM$, 
\begin{equation}
    g_y(V_1,V_2) = g_{L_x(y)}(L_{x*,y}(V_1), L_{x*,y}(V_2)),
\end{equation}
where $L_x(y) = x \odot y$ is left translation, and $L_{x*,y}$ is the differential map of $L_x$ at $y$.
Right-invariance is defined similarly.
A metric over a Lie group is bi-invariant, if it is both left and right invariant. 
\end{definition}
Bi-invariant metrics are the most convenient metrics on Lie, as they enjoy many excellent properties \citep[Chatper~V]{sternberg1999lectures}. 

The exponential \& logarithmic maps and parallel transportation are also crucial for Riemannian approaches in machine learning.
To bypass the notation burdens caused by their definitions, we review the geometric reinterpretation of these operators, introduced in \citep{pennec2006riemannian, do1992riemannian}.
In detail, in a manifold $\calM$, geodesics correspond to straight lines in the Euclidean space.
A tangent vector $\overrightarrow{x y} \in T_x\calM$ can be locally identified to a point $y$ on the manifold by geodesic starting at $x$ with initial velocity of $\overrightarrow{x y}$, i.e. $y=\rieexp_x(\overrightarrow{x y})$.
On the other hand, logarithmic map is the inverse of exponential map, generating the initial velocity of the geodesic connecting $x$ and $y$, i.e. $\overrightarrow{x y}=\rielog_x(y)$.
These two operators generalize the idea of addition and subtraction in Euclidean space.
For the parallel transportation $\pt{x}{y}(V)$, it is a generalization of parallelly moving a vector along a curve in the Euclidean space.
we summarize the reinterpretation in \cref{tb:reinter_riem_operators}.
\begin{table}
    \small
    \centering
    \caption{Reinterpretation of Riemannian Operators.}
    \label{tb:reinter_riem_operators}
    \begin{tabular}{ccc}
    \toprule
    Operations & Euclidean spaces & Riemannian manifolds \\
    \midrule
    Straight line & Straight line & Geodesic \\
    Subtraction & $\overrightarrow{x y}=y-x$ & $\overrightarrow{x y}=\log _x(y)$ \\
    Addition & $y=x+\overrightarrow{x y}$ & $y=\exp _x(\overrightarrow{x y})$ \\
    Parallelly moving & $V \rightarrow V$ & $\pt{x}{y}(V)$\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Geometry of the SPD manifold} \label{subsec:geom_of_spd}
Although SPD matrices is a subset of the vector space $\bbR{{n(n+1)}/{2}}$, applying the Euclidean structure directly to SPD matrices could be problematic both practically and theoretically, as demonstrated in \citep{arsigny2005fast, pennec2006riemannian}.
The reason is that this subset does not form a linear subspace of $\bbR{{n(n+1)}/{2}}$ and is actually a manifold \citep{arsigny2005fast}.
As mentioned before, there are many kinds of well-studied Riemannian metrics on the SPD manifold.
Below, we will briefly review two related metrics, LEM \citep{arsigny2005fast} and LCM \citep{lin2019riemannian}.

Matrix logarithm $\mln(\cdot): \spd{n} \rightarrow \sym{n}$ and $\cln(\cdot): \spd{n} \rightarrow \tril{n} $ are defined as,
\begin{align}
     \mln(S) &= U \ln(\Sigma) U^\top,\\
     \cln(S) &= \clnchart(\scrL(S)),
\end{align}
where $S=U \Sigma U^\top$ is the eigendecomposition, $L = \scrL(S)$ is the Cholesky decomposition ($S=LL^\top$), $\clnchart(L) = \lfloor L \rfloor + \ln(\bbD(L))$ is a coordinate system from the $\cho{n}$ manifold onto the Euclidean space $\tril{n}$ \citep{lin2019riemannian}, $\lfloor L \rfloor$ is the strictly lower triangular part of $L$, $\bbD(L)$ is the diagonal elements, and $\ln(\cdot)$ is the diagonal natural logarithm.
We name $\cln$ as Cholesky logarithm, since in the following proof we will rely on it many times.
Note that topologically, $\tril{n} \simeq \sym{n} \simeq \bbR{n(n+1)/2}$, since their metric topology all comes from the Euclidean metric tensor.
Based on matrix logarithm, \citep{arsigny2005fast} propose LEM by Lie group translation, while based on Cholesky logarithm, \citep{lin2019riemannian} proposes LCM, by an isometry between $\spd{n}$ and $\cho{n}$.
In the main paper, we argued that LEM and LCM are basically the same, in the sense of high-level mathematical abstraction.

The Riemannian metric and associated geodesic distance under the LEM are defined by:
\begin{align}
    \label{eq:metric_lem} \glem_{S}(V_1,V_2) &= \geuc( {\mln}_{*,S} ( V_{1}), {\mln}_{*,S} ( V_{2}) ),\\
    \dlem(S_1, S_2) &= \| \mln(S_1) - \mln(S_2)\|_\rmF,
\end{align}
where $S \in \spd{n}$, $V_1, V_2 \in T_S\spd{n}$ are tangent vectors, ${\mln}_{*,S}(\cdot)$ is the differential map of matrix logarithm at $S$, $\geuc$ is the standard Euclidean metric tensor, and $\| \cdot \|_F$ is Frobenius norm.
Note that since $\geuc$ is the same at every point, we simply omit the subscript.
Besides, element-wise and scalar multiplication are also induced by $\mln$:
\begin{align}
    S_1 \mlnMul S_2 & = \mexp(\mln(S_1) + \mln(S_2)),\\
    \lambda \mlnMulScalar S & = \mexp(\lambda \mln(S)),
\end{align}
where $\mexp(X) = U \exp(\Sigma) U^\top$ is the matrix exponential.
As is proven in \citep{arsigny2005fast}, $\{ \spd{n}, \mlnMul\}$ and $\{ \spd{n}, \mlnMul,\mlnMulScalar\}$ form a Lie group and vector space, respectively.
Besides, the metric $\glem$ defined on Lie group $\{ \spd{n}, \mlnMul\}$ is bi-invariant.

The Riemannian metric and geodesic distance under LCM is
% \begin{equation} \label{eq:metric_lcm} 
% \begin{aligned}
%     &\glcm_S(V_{1}, V_{2} ) \\
%     &= \tilde{g}_L(L(L^{-1} V_1 L^{-\top})_{\frac{1}{2}},L(L^{-1}V_2L^{-\top})_{\frac{1}{2}}),    
% \end{aligned}
% \end{equation}
\small
\begin{equation} \label{eq:metric_lcm} 
    \glcm_S(V_{1}, V_{2} ) = \tilde{g}_L(L(L^{-1} V_1 L^{-\top})_{\frac{1}{2}},L(L^{-1}V_2L^{-\top})_{\frac{1}{2}}),
\end{equation}
\begin{equation}
    \begin{aligned}
         \dlcm(S_1, S_2) &= \{\|\lfloor L_1\rfloor-\lfloor L_2\rfloor\|_{\rmF}^{2}+\\
        & \|\ln( \bbD(L_1))-\ln (\bbD(L_2))\|_{\rmF}^{2}\}^{\frac{1}{2}},      
    \end{aligned}
\end{equation}
\normalsize
where $S \in \spd{n}$, $V_1,V_2 \in T_S\spd{n}$, $X_{\frac{1}{2}} = \lfloor X \rfloor + \bbD(X)/2$, and $\tilde{g}_L(\cdot, \cdot)$ is the Riemannian metric on $\cho{n}$, defined as
\begin{equation} \label{eq:metric_cholesky}
\begin{aligned}
    \tilde{g}_{L}(X, Y) &= \geuc (\lfloor X\rfloor,\lfloor Y\rfloor)\\ 
                        &+ \geuc(\bbD(L)^{-1} \bbD(X), \bbD(L)^{-1} \bbD(Y)).
\end{aligned}
\end{equation}
The group operation in \citep{lin2019riemannian} is defined as follows:
\begin{equation}
    S_1 \clnMul S_2 = \scrL^{-1} (\lfloor L_1 \rfloor + \lfloor L_2 \rfloor + \bbD(L_1)\bbD(L_2)),
\end{equation}
where $\scrL^{-1}(\cdot)$ is the inverse map of Cholesky decomposition.
$\{\spd{n}, \clnMul\}$ is proven to be a Lie group \citep{lin2019riemannian}.
Similar with LEM, $\glcm$ is bi-invariant.

\subsection{Building blocks in the SPDNet} \label{app:subsec:review_SPDNet}
The architecture of SPDNet \citep{huang2017riemannian} mimics the conventional densely connected feedforward network.
It has three basic components, named BiMap, ReEig, and LogEig.

The BiMap (Bilinear Mapping) is a generalized version of conventional linear mapping, defined as
\begin{equation}
    S^{k} = W^k S^{k-1} W^k, \text { with } W^k \text { semi-orthogonal.}
\end{equation}

The ReEig (Eigenvalue Rectification) can be viewed as eigen-rectification, mimicking the ReLu-like nonlinear activation functions:
\begin{equation}
    S^{k}=U^{k-1} \max (\Sigma^{k-1}, \epsilon I_{n}) U^{k-1 \top},
\end{equation}
where $S^{k-1}=U^{k-1} \Sigma^{k-1} U^{k-1 \top}$ is the eigendecomposition.

The LogEig layer projects SPD-valued data into the Euclidean space for further classification:
\begin{equation}
     S^{k}=\mln(S^{k-1}).
\end{equation}

\section{Additional discussions on ALEM}
In this section, we present additional discssions on our ALEM.
All the the proofs are placed in \cref{app:proofs}.
\subsection{Well-definedness of general matrix logarithm}
\label{app:subsec:well_difined_glog}

In \eqref{eq:mlog}, due to the page limit, we did not clarify the specific correspondence between eigenvalue and diagonal logarithm.
Here, we present detailed clarification.
Note that in implementation, like PyTorch or Matlab, this is no need to worry about this issue, as the outputs of eigendecomposition are always ordered.

We rewrite the eigendecomposition as $S = \sum \sigma_i E_i$ where $E_i=u_i u_i^\top$and $u_i$ is the corresponding eigenvector in $U$. Let $S$ be an $n \times n $ SPD matrix and $P_n$ be a set of all permutations of $\{n,\cdots, 1\}$ which is known as a permutation group. As changing the order of $\{n,\cdots, 1\}$ can be viewed as a permutation, we use $\pi \in P_n$ to represent the corresponding changed order.

Assume the eigenvalues $\sigma_i$ are sorted in ascending order, \emph{i.e.,} $\sigma_1 \leq \cdots, \leq \sigma_n$. To clarify the definition of "the $i$-th eigenvalues", we refer to the $i$-th eigenvalue to the $i$-th pair from the ordered eigenpair sequence $(\sigma_1,u_1),\cdots,(\sigma_n,u_n)$. Since each  eigenvector $u_{i}$ is unique, it is safe to say the eigenvalues are ordered and the $i$-th eigenvalue/eigenvector pair is unique.

Let $\log_\alpha(\Sigma)$ denotes imposing scalar logarithm $\log_{a_i}$ to the $i$-th eigenvalue $\sigma_i$. Then $\phi_{m l o g}$ is rewritten as $\phi_{m l o g}(S)=\sum \log _{a_i}^{\sigma_i} E_i$, where $S = \sum \sigma_i E_i$. In this way, $\phi_{m l o g}$  is clearly well-defined. By definition, we can observe that the output of $\phi_{m l o g}$ does not depend on the order in eigendecomposition.


Suppose there are two eigendecompositions with different orders, \emph{i.e.,} $S= U\Sigma U^\top = \tilde{U} \tilde{\Sigma}\tilde U^\top$ where $\tilde{U}, \tilde\Sigma$ are rearrangement of $U,\Sigma$. There exists a $\pi \in P_n$  such that for each $j$, there is a unique $i$, satisfying $\tilde u _j = u_{\pi(i)}$ and $\tilde\sigma_j=\sigma_{(i)}$. We then have $\sum \log _{a_i}^{\sigma_i} E_i$  for $S= U\Sigma U^\top$ and $\sum \log_{a_{\pi(i)}}^{\sigma_{\pi(i)}} E_{\pi(i)}$ for $S=\tilde U\tilde{\Sigma}\tilde U^\top$, which indicates the two eigendecompositions are equivalent. 

\subsection{Differentials of general logarithms} \label{app:subsec:differentials}
\begin{proposition}[Differentials] \label{props:diff_mgexp_mlog}
    For a tangent vector $V \in T_S\spd{n}$, the differential $\diffmlog{S} : T_S \spd{n} \rightarrow T_{\mlog(S)} \sym{n}$ of $\mlog$ at $S \in \spd{n}$ is given by
    \begin{equation}
        \diffmlog{S} (V) = Q+Q^\top + W,
    \end{equation}
    where $Q = D_U\log(\Sigma)U^\top$,
    \begin{align*}
        D_U &= (\begin{array}{ccc}
             (\sigma_1 I-S)^+ V u_1 & \cdots & (\sigma_n I-S)^+ V u_n
        \end{array}),\\
        W &= U \diag(\frac{u_1^\top V u_1}{\sigma_1 \ln{a_1}},\cdots,\frac{u_n^\top V u_n}{\sigma_n \ln{a_n}}) U^\top,        
    \end{align*}
    $()^+$ is the Moore–Penrose inverse, $u_1,\cdots,u_n$ are orthonormal eigenvectors of $S$, and the associated eigenvalues are $\sigma_1,\cdots,\sigma_n$.
    
    Symmetrically, for a tangent vector $\widetilde{V} \in T_X\sym{n}$, the differential $\diffmgexp{X} : T_X \sym{n} \rightarrow T_{\mgexp(X)} \spd{n}$ of $\mgexp$ at $X \in \sym{n}$ is given by
    \begin{equation} \label{eq:diff_mgexp}
        \diffmgexp{X} (\widetilde{V}) = \widetilde{Q}+\widetilde{Q}^\top + \widetilde{W},
    \end{equation}
    where $S = \widetilde{U} \widetilde{\Sigma}\widetilde{U}^\top$ is the eigendecomposition, $D_{\widetilde{U}}$ is defined similarly, $\widetilde{Q} = D_{\widetilde{U}}\balpha(\widetilde{\Sigma})\widetilde{U}^\top$, and
    \begin{equation*}
        \widetilde{W} = \widetilde{U} \diag(\ln^{a_1}a_1^{\widetilde{\sigma_1}}{\widetilde{u}_1^\top \widetilde{V} \widetilde{u}_1},\cdots,\ln^{a_n}a_n^{\widetilde{\sigma_n}}{\widetilde{u}_n^\top \widetilde{V} \widetilde{u}_n}) \widetilde{U}^\top.     
    \end{equation*}
\end{proposition}

In \citep{arsigny2005fast}, the differential of the matrix exponential is written as infinite series.
The differential of our $\mgexp$ can also be rewritten in this way.
\begin{proposition}[Differential as Infinite Series] \label{props:diff_mgexp_series}
    Following the notation in \cref{props:diff_mgexp_mlog}, the differential of $\mgexp$ can also be formulated as
    \begin{equation} \label{eq:diff_mgexp_series}
        \begin{aligned}
            &\diffmgexp{X}(\widetilde{V}) \\
            &= \sum_{k=1}^{\infty} \frac{1}{k !}(\sum_{l=0}^{k-1} (\widetilde{P}X)^{k-l-1} (D_{\widetilde{P}}X+\widetilde{P}\widetilde{V}) (\widetilde{P}X)^l),
        \end{aligned}
    \end{equation}
    where $\widetilde{P}=\widetilde{U}B\widetilde{U}^\top$, $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$, $D_{\widetilde{P}}= D_{\widetilde{U}} B \widetilde{U}^\top + \widetilde{U} B D_{\widetilde{U}}^\top$.
\end{proposition}

When $\mgexp$ is reduced into matrix exponential, \eqref{eq:diff_mgexp_series} becomes Eq. 8 in \citep{arsigny2005fast}, and our ALEM becomes exactly LEM.
This is reasonable as our $\mlog$ covers the matrix logarithm as a special case.

\subsection{Additional properties of Fréchet means under ALEM} \label{app:subsec:add_props_fm}

Except for the invariance properties presented in the main paper, the Fréchet mean induced by our ALEM also satisfies various properties presented in \citep{ando2004geometric}.
\begin{proposition} \label{props:frechet_means_add_props}
    For any SPD matrices $A, B, C, A_0, B_0, C_0$, denote $\fm(A,B,C)$ as the Fréchet mean of $A,B,C$ under ALEM.
    Then the Fréchet mean satisfies the following properties.
    \begin{enumerate}[(U1)]
        \item \label{props:u1}
        Permutation invariance.
        For any permutation $\pi(\{ A,B,C \})$ of $\{ A, B, C\}$,  
        \item \label{props:u2}
        $\fm(A, A,A) = A$
    \end{enumerate}
    If $A, B, C, A_O, B_0, C_0$ commute, the following properties hold.
    \begin{enumerate}[(V1)]
        \item  Joint homogeneity. \label{props:v1}
        $\fm(a A, b B, c C)=(a b c )^{1 / 3} \fm(A, B, C), \forall a,b,c >0$.
        
        \item Monotonicity.
        The map $(A, B, C) \mapsto \fm(A, B, C)$ is monotone, \ie, if $A \geq A_0$, $B \geq  B_0$, and $C \geq  C_0$, then $\fm(A,B,C) \geq  \fm(A_0,B_0,C_0)$ in the positive semidefinite ordering.
        \item Self-duality.
        $\fm(A, B, C)=\fm(A^{-1}, B^{-1}, C^{-1})^{-1}$.
        \item Determinant identity.  \label{props:v4}
        $\det \fm(A, B, C)=(\det A \cdot \det B \cdot \det C)^{1 / 3}$.
    \end{enumerate}
\end{proposition}

In fact, \cref{props:frechet_means_add_props} holds true for any finite number of SPD matrices.


\subsection{Learning base vectors by Riemannian optimization} \label{app:subsec:geom}

We focus on a single element $a$ of $\alpha$ in \eqref{eq:rw_org_mlog}.
As discussed in the main paper, $a$ satisfying $a>0 \& a \neq 1$.
The condition of $a \neq 1$ can be further waived since we can set $a=1+\epsilon$ if $a=1$. Then there is only one constraint about positivity.
A geometric way to deal with positivity is to view $a$ as a point in 1-dimension SPD manifold.
We call this strategy as GEOM.
Then we have the following updating formula for GEOM.
\begin{proposition} \label{propos:param_by_geom}
    Viewing a positive scalar $a$ as a point in a 1-dimensional SPD manifold, we have the following updating formula for Riemannian stochastic gradient descent (RSGD).
    \begin{equation} \label{eq:update_pos_scalar}
        a^{(t+1)} = a^{(t)}e^{-\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L},
    \end{equation}
    where $\nabla_{a^{(t)}} L$ is the Euclidean gradient of $a$ at $a^{(t)}$, $\gamma^{(t)}$ is the learning rate, and $e^{(\cdot)}$ is the natural exponentiation.
\end{proposition}

Besides, by \eqref{eq:update_pos_scalar}, we could prove that GEOM is equivalent to DIV, which is given in the following proposition.
\begin{proposition} \label{props: geom_equivalent_div}
    For parameters learning in $\mlog$, optimizing the base vector $\alpha$ by RSGD is equivalent to optimizing divisor matrix $B$ by Euclidean stochastic gradient descent (ESGD).
\end{proposition}

\subsection{Gradients computation} \label{app:subsec:gradients}

There are two gradients that need calculation in the proposed ALog layer, one w.r.t the parameters and another w.r.t the input of the ALog layer.
Since structural matrix decomposition is involved in $\mlog$, the following contents heavily rely on the structural matrix BP \citep{ionescu2015matrix}, the key idea of which is the invariance of first-order differential form.
For the ALog layer, it is essentially a special case of eigenvalue functions.
Based on the formula offered in \citep{bhatia2009positive} and matrix BP techniques presented in \citep{ionescu2015matrix,song2021approximate}, we can obtain all the gradients, as presented in the following proposition.
\begin{proposition} \label{props:grad_mlog}
    Let us denote $X = \mlog(S)$, where $S \in \spd{d}$ is an input SPD matrix of the ALog layer.
    We have the following gradients:
    \begin{align}
        \label{eq:gradient_eigen_function} \nabla_{S} L
        &= U[K \odot(U^{T}(\nabla_{X} L) U)] U^{T},\\
        \nabla_{A} L   
        &= [U^\top (\nabla_{X} L) U] \odot \log(\Sigma),
    \end{align}
    where $S = U \Sigma U^\top$ is the eigendecomposition of an SPD matrix and matrix $K$ is defined as
    \begin{equation}
        K_{i j}= \begin{cases}\frac{f\left(\sigma_{i}\right)-f\left(\sigma_{j}\right)}{\sigma_{i}-\sigma_{j}} & \text { if } \sigma_{i} \neq \sigma_{j} \\ f^{\prime}\left(\sigma_{i}\right) & \text { otherwise }\end{cases}
    \end{equation}
    where $f(\sigma_i) = A_{ii}\log_e(\sigma_i)$ and $\Sigma=\diag(\sigma_1,\sigma_2,\cdots,\sigma_d$).
\end{proposition}

\section{Additional experiments and explanations}

\subsection{Visualization} \label{app:subsec:vis}

\begin{figure}[htbp]
    \centering
    \subfigure[HDM05]{
        \includegraphics[width=2.5in]{Params_distri_ALog_MUL_HDM05.pdf}
        \label{label_for_cross_ref_1}
    }
    \subfigure[FPHA]{
	\includegraphics[width=2.5in]{Params_distri_ALog_MUL_FPHA.pdf}
        \label{label_for_cross_ref_2}
    }
    \caption{Visulization of parameters in ALog on the HDM05 and FPHA dataset}
    \label{fig:vis_params_hdm05_dpha}
\end{figure}

To improve the understanding of our approach, we visualize the final learned parameters of the ALog layer. 
Since MUL is the most robust strategy, we visualize the parameters of ALog-MUL. 
Specifically, we plot the final values of the diagonal elements of $A$ in \eqref{eq:rw_mul_mlog} and present the results in \cref{fig:vis_params_hdm05_dpha}. 
We observe that the distribution of the parameters is consistent within the same dataset, but varies between datasets. 
This indicates that our approach is capable of capturing vibrant patterns in different datasets, respecting their specific geometry.


\subsection{Explanations for nonlinear mechanism in DIV} \label{app:subsec:non_linear_div}
There is an underlying nonlinear scaling mechanism in the update of DIV, which might undermine the robustness of DIV.
Without loss of generality, let us focus on a single scalar parameter $b$ in \eqref{eq:rw_div_mlog}.
The ultimate factor multiplied by the plain logarithm is $1/b$.
Therefore, the change of the multiplier after the update would be 
\begin{equation} \label{eq:nonlinear_div}
    1/(b-\Delta)-1/b=\Delta/[(b-\Delta)b].
\end{equation}
\eqref{eq:nonlinear_div} will scale the original $\Delta$ to some extent.
This scaling mechanism might undermine the robustness of the ALog layer.

\subsection{Experiments on the SPDNetBN}
In the main paper, we observed that among all three kinds of implementation of ALog, the most robust performance is achieved by MUL.
So we exploit MUL to carry out further experiments on the SPDNetBN \citep{brooks2019riemannian}.
In the following, we simply call SPDNetBN-ALog-MUL as SPDNetBN-ALog, where the LogEig layer is substituted by our ALog layer.
Our experiments on three datasets show that SPDNetBN is similar to SPDNetBN-ALog.
This could be attributed to the inconsistency of Riemannian metrics.
The RBN in SPDNetBN is based on AIM, while the metric behind our ALog is ALEM.
Therefore, it is not very reasonable to apply our ALog to AIM-based RBN.
However, it does not undermine the effectiveness of our ALog.
Instead, if one plans to apply our ALog to RBN, the RBN should be rebuilt based on ALEM.
This clearly goes beyond the topics of this paper.
We leave this for future work.

\section{Proofs for the lemmas, propositions, theorems, and corollaries stated in the paper} \label{app:proofs}

\begin{proof} [Proof of \cref{thm:rethk_lem_lcm}]
Recalling \defref{def:isometry}, to show the isometry, we need to find the pullback formulation.
Denote $\glem,\glcm$, and $\ge$ as LEM, LCM and standard Euclidean metric tensor, respectively.

Let us first deal with the LEM.
Recalling \eqref{eq:metric_lem}, we can readily conclude that $\mlog$ is an isometry from $\{\spd{n},\glem\}$ onto $\{\bbR{n(n+1)/2},\geuc \}$.

Now, let us focus on LCM.
By \eqref{eq:metric_lcm}, $\{\spd{n}, \glcm\}$ is isometric to $\{\cho{n},\tilde{g}\}$, with Cholesky decomposition $\scrL$ as an isometry.
This is exactly how LCM is derived \citep{lin2019riemannian}.
So, the key point lies in Cholesky metric $\tilde{g}$.
Let us reveal why it is defined in this way.
In fact, $\tilde{g}$ is derived from $\geuc$ by $\clnchart$.
Simple computations show that
\begin{equation} \label{eq:diff_cho_ln_chart}
    \varphi_{ln *,L}(V) = \lfloor V \rfloor + \mathbb{D}^{-1}(L)\mathbb{D}(V),
\end{equation}
where $V \in T_L\cho{n}$.
By \eqref{eq:diff_cho_ln_chart}, \eqref{eq:metric_cholesky} can be rewrote as
\begin{equation}
    \tilde{g}_L(X,Y) = \geuc(\varphi_{ln *,L}((X),\varphi_{ln *,L}((Y)).
\end{equation}
Therefore, $\clnchart: \cho{n} \rightarrow \bbR{n(n+1)/2}$ is an isometry.
By transitivity, $\cln: \spd{n} \rightarrow \bbR{n(n+1)/2}$ is an isometry as well.
\end{proof}

\begin{proof} [Proof of \cref{lem:pro_by_bijection}]
    First, let's focus on the case \ref{enu:general_hilbert}.
    Before starting, we point out that \eqref{eq:x_mul}-\eqref{eq:x_inner_product} are indeed well-defined, which is easy to verify by the bijectivity of $\phi$ and the well-defined operations on $\calY$.
    More explicitly, the three maps should be expressed as the following,
    \begin{align}
        \xMul                                 &: \calX \times \calX \rightarrow \calX,\\
        \xMulScalar                           &: \bbK \times \calX \rightarrow \calX,\\
        \langle \cdot , \cdot \rangle_{\phi} &: \calX \times \calX \rightarrow \bbK.
    \end{align}
    In the following, we first show that $\{\calX,\xMul\}$ is an abelian group and then $\{\calX,\xMul, \xMulScalar\}$ is a vector space over $\bbK$.
    Next, we proceed to verify $\{ \calX,\xMul, \xMulScalar, \langle \cdot , \cdot \rangle_{\phi} \}$ is an inner product space.
    Lastly, we will prove the completeness of $\{ \calX,\xMul, \xMulScalar, \langle \cdot , \cdot \rangle_{\phi} \}$.
    
    Given arbitrary $x, x_1, x_2, x_3 \in \calX$ and $k, l \in \bbK$, we have the following proof.
    Note that we would not emphasize the arbitrariness again in the following description.
    
    Firstly, to prove a non-empty set endowed with an operation is an abelian group.
    We need to verify: 1). closure under this operation, 2).associativity and commutativity, 3). left identity element, 4). left invertibility.
    
    For 1), it can be verified directly by the bijectivity of $\phi$.
    
    For 2), it can be guaranteed by the associativity and commutativity of $\yMul$,
    \begin{equation}
        \begin{aligned}
            (x_1 \xMul x_2) \xMul x_3 &= [\phiinv(\phi(x_1) \yMul \phi(x_2))] \xMul x_3\\
                                      &= \phiinv ( (\phi(x_1) \yMul \phi(x_2)) \yMul \phi(x_3) )\\
                                      &= \phiinv ( \phi(x_1) \yMul (\phi(x_2) \yMul \phi(x_3)) )\\
                                      &= x_1 \xMul (x_2 \xMul x_3)
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            x_1 \xMul x_2 &= \phiinv(\phi(x_1) \yMul \phi(x_2))\\
                          &= \phiinv(\phi(x_2) \yMul \phi(x_1))\\
                          &= x_2 \xMul x_1
        \end{aligned}
    \end{equation}   
    
    For 3), denoting the neutral element in $\calY$ as $\idY$, we then have,
    \begin{equation}
        \begin{aligned}
            \phiinv(\idY) \xMul x   &= \phiinv(\idY \yMul \phi(x))\\
                                          &= \phiinv(\phi(x))\\
                                          &= x.
        \end{aligned}
    \end{equation}
    
    For 4), denoting $y^{-1}_{\yMul}$ as the inverse element of $y \in \calY$ and $x^{-1}_{\xMul}=\phiinv(\phi(x)^{-1}_{\yMul})$, we have,
    \begin{equation}
        \begin{aligned}
            x^{-1}_{\xMul} \xMul x &= \phiinv(\phi(x)^{-1}_{\yMul} \yMul \phi(x))\\
                                   &= \phiinv(\idY)\\
                                   &= \idX
        \end{aligned}
    \end{equation}
    So far, we prove that $\{ \calX, \xMul \}$ is an abelian group.
    To show $\{ \calX, \xMul, \xMulScalar \}$ is a linear space , we only need to verify four laws about scalar product.
    we have the following equations to support our claim:
    \begin{equation}
        \begin{aligned}
            1 \xMulScalar x &= \phiinv(1 \yMulScalar \phi(x))\\
                            &= \phiinv(\phi(x))\\
                            &= x,
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            k \xMulScalar (x_1 \xMul x_2) &= k \xMulScalar [\phiinv(\phi(x_1) \yMul \phi(x_2))]\\
                                          &= \phiinv(k \yMulScalar [\phi(x_1) \yMul \phi(x_2)] )\\
                                          &= \phiinv([k \yMulScalar \phi(x_1)] \yMul [k \yMulScalar \phi(x_2)] )\\
                                          &= (k \xMulScalar x_1) \xMul (k \xMulScalar x_1),
        \end{aligned}
    \end{equation}    
    \begin{equation}
        \begin{aligned}
            (k+l) \xMulScalar x &= \phiinv( (k+l) \yMulScalar \phi(x) )\\
                                &= \phiinv( (k \yMulScalar \phi(x)) \yMul (l \yMulScalar \phi(x)) )\\
                                &= (k \xMulScalar x) \xMul  (l \xMulScalar x),
        \end{aligned}
    \end{equation} 
    \begin{equation}
        \begin{aligned}
            k \xMulScalar (l \xMulScalar x) &= k \xMulScalar \phiinv( l \yMulScalar \phi(x) )\\
                                            &= \phiinv( k \yMulScalar( l \yMulScalar \phi(x)))\\
                                            &= \phiinv( (kl) \yMulScalar \phi(x))\\
                                            &= (kl) \xMulScalar x.
        \end{aligned}
    \end{equation} 
    
    Now, we see $\calX$ is a linear space.
    Next, we verify that $\langle \cdot,\cdot \rangle_{\phi}$ is an inner product.
    This means it should satisfy: a). positivity, b). definiteness, c). linearity in the first slot, and d). symmetry.
    
    For a)-d), they all can be derived by the corresponding properties of inner product in $\calY$:

    \begin{equation}
        \langle x, x \rangle_{\phi} = \langle \phi(x), \phi(x) \rangle_{\calY} \ge 0, \\
    \end{equation}
    \begin{equation}
    \begin{aligned}
        \langle x, x \rangle_{\phi} = 0, &\iff \langle \phi(x), \phi(x) \rangle_{\calY} = 0, \\
        & \iff \phi(x)=\idY, \\
        & \iff x=\idX,
    \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \langle x_1 \xMul x_3, x_2 \rangle_{\phi} 
            &= \langle \phi(x_1) \yMul \phi(x_3), \phi(x_2) \rangle_{\calY}\\
            &= \langle \phi(x_1), \phi(x_2) \rangle_{\calY} +  \langle \phi(x_3), \phi(x_2) \rangle_{\calY}\\
            &= \langle x_1 , x_2 \rangle_{\phi} + \langle x_3 , x_2 \rangle_{\phi}
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \langle k \xMulScalar x_1, x_2 \rangle_{\phi} 
            &= \langle \phiinv(k \yMulScalar \phi(x_1)), x_2 \rangle_{\phi}\\
            &= \langle k \yMulScalar \phi(x_1), \phi(x_2) \rangle_{\calY}\\
            &= k \langle  \phi(x_1), \phi(x_2) \rangle_{\calY}\\
            &= k \langle  x_1, x_2 \rangle_{\phi}\\
        \end{aligned}
    \end{equation}     
    \begin{equation}
        \begin{aligned}
            \langle x_1, x_2 \rangle_{\phi} &= \langle \phi(x_1), \phi(x_2) \rangle_{\calY}\\
                                            &= \langle \phi(x_2), \phi(x_1) \rangle_{\calY}\\
                                            &= \langle x_2, x_1 \rangle_{\phi},
        \end{aligned}
    \end{equation}    
    Recall the above proof, we can find that $\phi$ is a linear isomorphism that could preserve the inner product and thus an isometry.

    Last but not least, we verify $\calX$ is complete.
    Since $\calY$ is complete and $\phi$ is an isometric bijection, therefore $\calX$ is complete as well.
    
    Now, let us deal with case \ref{enu:lie_group}. 
    We can see that $\phi$ is a group isomorphism and $\calX$ is a group.
    What's left is to verify the smoothness of the group operations.
    First, we present the multiplications and inverses  more explicitly,
    \begin{align}
        m_{\calY}(\cdot,\cdot): & \calY \times \calY \rightarrow \calY,\\
        i_{\calY}(\cdot): & \calY \rightarrow \calY,\\
        m_{\calX}(\cdot,\cdot): & \calX \times \calX \rightarrow \calX,\\
        i_{\calX}(\cdot): & \calX \rightarrow \calX,
    \end{align}
    where $m_{\calY},i_{\calY}$ mean the multiplication and inverse in $\calY$, and $m_{\calX},i_{\calX}$ mean the counterparts in $\calX$.
    Recalling \eqref{eq:x_mul}, we rewrite group operations as
    \begin{align}
        m_{\calX} &= \phiinv \circ m_{\calY} \circ (\phi \times \phi),\\
        i_{\calX} &= \phiinv \circ i_{\calY} \circ \phi,
    \end{align}
    where $\phi \times \phi$ means the Cartesian product of two maps.
    Since $m_{\calY}$, $i_{\calY}$, $\phi$ and $\phiinv$ are all smooth, as the composition and Cartesian product of some smooth maps, $m_{\calX}$ and $i_{\calX}$ are therefore smooth, which means that $\calX$ is a Lie group.
    Considering $\phi$ is a diffeomorphism and a group isomorphism, it is, therefore, a Lie group isomorphism.
    
    Lastly, let us deal with case \ref{enu:rie_manifold}.
    By the \defref{def:pullback_metrics} and \defref{def:isometry}, we can readily conclude that $\phi$ is a Riemannian isometry from $\{\calX , \gphi \}$ into $\{\calY , \gy \}$.    
\end{proof}

\begin{proof} [Proof of \cref{thm:g_spd}]
    Except from bi-invariance, others can be direct obtained from \cref{lem:pro_by_bijection}.
    Now, let us focus on bi-invariance.
    
    % Besides, we will omit the subscripts of the tangent points in differential maps, if there is no confusion.
    Since $\{\spd{n},\odot\}$ is an abelian Lie group, it suffices to show the left invariance.
    
    For any $P,Q \in \spd{n}$ and $V_1,V_2 \in T_p\spd{n}$, we make the following discussion.
    Denote the left translation by $Y$ in $\sym{n}$ as
    \begin{equation}
        m_{Y}: X \in \sym{n} \rightarrow Y+X.
    \end{equation}
    By simple computation, $m_{Y,*}$ is the identity map at any point.
    Then the left translation $L_Q$ by $Q$ on $\{\spd{n},\odot\}$ can be rewrote as
    \begin{equation}
        L_Q = \phiinv \circ m_{\phi(Q)} \circ \phi.
    \end{equation}
    By the chain rule of differential, we have 
    \begin{align}
        L_{Q*,P} &= \phiinv_{*,\phi(Q)+\phi(P)} m_{\phi(Q)*,\phi(P)} \phi_{*,P},\\
        &= \phiinv_{*,\phi(Q)+\phi(P)} \phi_{*,P}
    \end{align}
    Note that 
    \begin{equation}
        \phi_{*,L_Q(P)} \phiinv_{*,\phi(Q)+\phi(P)} = \id
    \end{equation}
    Then we have the following equations,
    \begin{equation}
        \small
        \begin{aligned}
            & \langle L_{Q*,P} V_1, L_{Q*,P} V_2 \rangle_{L_Q (P)},\\
            &= \langle \phi_{*,L_Q(P)} L_{Q*,P} V_1, \phi_{*,L_Q(P)} L_{Q*,P} V_2 \rangle_{I},\\
            &= \langle \phi_{*,P} V_1, \phi_{*,P} V_2 \rangle_{I},\\
            &= \langle V_1, V_2 \rangle_{P},
        \end{aligned}
    \end{equation}
\end{proof}
\begin{proof} [Proof of \cref{props:diffeo_mlog}]
    Obviously, $\mgexp$ is the inverse of $\mlog$. 
    What followed is to verify the smoothness of $\mlog$ and its inverse.
    
    According to Theorem 8.9 in \citep{magnus2019matrix}, the map producing an eigenvalue or an eigenvector from a real symmetric matrix is $\cinf$.
    Recalling $\mlog$ and its inverse map $\mgexp$, it's obvious that they are comprised of arithmetic or composition of some smooth maps.
    Therefore, $\mlog$ ($\mgexp$) is a diffeomorphism.
\end{proof}

\begin{proof} [Proof of \cref{coro:mlog_spd_properties}]
    This is a direct result of \cref{thm:g_spd}.
\end{proof}

\begin{proof} [Proof of \cref{props:geo_mean_spd}]
    Obviously, the metric space $\{\spd{n},\dalem\}$ is isometric to the space $\sym{n}$ endowed with the standard Euclidean distance.
    Therefore, the weighted Fréchet mean of $\{S_i\}$ in $\spd{n}$ corresponds to the weighted Fréchet mean of associated points $\{\mlog(S_i)\}$ in $\sym{n}$.
    The weighted Fréchet means in Euclidean spaces are clearly the familiar weighted means.
\end{proof}

\begin{proof} [Proof of \cref{props:biinvariance}]
    This is a direct result of \cref{thm:g_spd}.
\end{proof}


\begin{proof} [Proof of \cref{props:exp_invariance}]
    Following the notations in this proposition, we make the following proof.
    The LHS can be rewritten as 
    \begin{align}
        (\mathrm{FM}(S_1^\beta,\cdots S_m^\beta)) 
        &= \mgexp(\sum_{i=1}^{m} \frac{1}{m}\beta\mlog(S_i)),\\
        &= \mgexp(\beta\sum_{i=1}^{m} \frac{1}{m}\mlog(S_i)),\\
        &= [\mgexp(\sum_{i=1}^{m} \frac{1}{m}\mlog(S_i))]^\beta,\\
        &= (\mathrm{FM}(S_1,\cdots S_m))^\beta 
    \end{align}
\end{proof}
\begin{proof} [Proof of \cref{props:sim_invariance}]
    Obviously, for a given SPD matrix $S$,
    \begin{align}
        \label{eq:mlog_rotate} \mlog(R S R^\top) &= R \mlog(S) R^\top,\\
        \label{eq:mlog_scale} \mlog(s^2 S ) &=  U(\log(s^2 I)+\mlog(\Sigma))U^\top,
    \end{align}
    where $S=U \Sigma U\top$ is the eigendecomposition.
    With \eqref{eq:mlog_rotate} and \eqref{eq:mlog_scale}, we can obtain the results.
\end{proof}

\begin{proof} [Proof of \cref{props:diff_mgexp_mlog}]
    The differentials of $\mgexp$ and $\mlog$ can be derived in exactly the same way. 
    In the following, we only present the process of deriving the differential of $\mlog$. 
    
    First, Let us recall the differentials of eigenvalues and eigenvectors.
    Theorem 8.9 in \citep{magnus2019matrix} offers their Euclidean differentials, which are the exact formulations for differentials under the canonical base on SPD manifolds.
    So, we can readily obtain the differentials of eigenvalues and eigenvectors as the following:
    \begin{align}
        \label{eq: diff_eig_value} \sigma_{*,S} (V) &= u^{\top} V u,\\
        \label{eq: diff_eig_vec} u_{*,S} (V) &= (\sigma I- S)^{+} V u,
    \end{align}
    where $S u = \sigma u, u^\top u = 1$, and $()^+$ is the Moore–Penrose inverse.

    By the RHS of \eqref{eq:rw_org_mlog}, the differential map of $\mlog$ is
    \begin{equation} \label{eq:diff_mlog}
    \begin{aligned}
        \diffmlog{S}(V) &= U_{*,S} (V) \log(\Sigma)U^\top + U (\log{\Sigma})_{*,S} (V) U^\top \\
                  & + U \log(\Sigma) U^\top_{*,S} (V) \\
                  &= Q+Q^\top + U^\top (\log{\Sigma})_{*,S} (V) U,
    \end{aligned}
    \end{equation}
    where $Q= U_{*,S} (V) \log(\Sigma)U^\top$.

    For the differential of diagonal logarithm, it is 
    \begin{equation} \label{eq: diff_ln}
        \log_{*,S}{\Sigma} = A \frac{1}{\Sigma} \Sigma_{*,S},
    \end{equation}
    where $A$ is defined in \eqref{eq:rw_mul_mlog}.

    Denote the eigenvectors and eigenvalues of $S=U\Sigma U^\top$ as $U = (u_1,\cdots,u_n)$ and $\Sigma=\diag(\sigma_1,\cdots,\sigma_n)$.
    By \eqref{eq: diff_eig_value}-\eqref{eq: diff_ln}, the differential of $\mlog$ can be obtained.
\end{proof}
\begin{proof} [Proof of \cref{props:diff_mgexp_series}]
    Following the notations in the proposition, we make the following proof.
    By abuse of notation, in the following, we omit the wide tilde $\widetilde{~}$.
   
    Now, we proceed to deal with the differential of $\mgexp$.
    We rewrite the formula of $\mgexp$ as
    \small
    \begin{align}
        & \mgexp(X) \\
        &= U \balpha(\Sigma) U^\top,\\
        &= U \diag(e^{\ln^{a_1}\sigma_1},\cdots,e^{\ln^{a_n}\sigma_n})U^\top,\\
        &= U \diag(\sum_{k=0}^{\infty}\frac{(\ln^{a_1}\sigma_1)^k}{k!},\cdots,\sum_{k=0}^{\infty}\frac{(\ln^{a_n}\sigma_n)^k}{k!})U^\top,\\
        \label{eq:rw_mgexp_last2} &= U (\sum_{k=0}^{\infty}\frac{B\Sigma}{k!}) U^\top,\\
        \label{eq:rw_mgexp} &= \sum_{k=0}^{\infty}\frac{PX}{k!}
    \end{align}
    \normalsize
     where $P=UBU^\top$, with $U$ from eigendecomposition $X=U\Sigma U^\top$ and diagonal matrix $B=\diag(\ln^{a_1},\cdots,\ln^{a_n})$.
     By the properties of normed vector algebras \citep[Proposition~15.14, \S~15.4]{loring2011introduction}, we can obtain the last equation.
     Then, we can compute the differential of $\mgexp$ by curves.
     Given a curve $c$ on $\sym{n}$ starting at $X$ with initial velocity $W \in T_X\sym{n}$, we have
    \begin{align}
        \diffmgexp{X}(W) 
        &= \left. \frac{d}{dt} \right |_{t=0} \mgexp \circ c(t)\\
        \label{eq:diff_mgexp_series_proof_stp2}
        &= \left. \frac{d}{dt} \right |_{t=0}\sum_{k=0}^{\infty}\frac{Pc(t)}{k!}.
    \end{align} 
    By a term-by-term differentiation, we have
    \begin{equation} 
        \begin{aligned} \label{eq:diff_last2step_mgexp}
            &\diffmgexp{X}(W)\\ 
            &= \sum_{k=1}^{\infty} \frac{1}{k !}(\sum_{l=0}^{k-1}(PX)^{k-l-1} \left. \frac{d}{dt} \right|_{t=0}(Pc) (PX)^l).
        \end{aligned}
    \end{equation}
    By the chain rule, we have
    \begin{equation} \label{eq:chain_rule_Pc}
        \left. \frac{d}{dt} \right|_{t=0}(Pc) = P'(0)X + PV.
    \end{equation}
    $P'(0)$ is obtained by
    \begin{align}
        P'(0) &= (UBU^\top)'(0),\\
              &= U'(0)BU^\top + UBU^{\top '}(0),\\
              \label{eq:diff_P_at0} &= D_U B U^\top + U B D_U^\top,
    \end{align}
    where $D_U$ is derived from the differential of eigenvectors,
    \begin{equation} \label{eq:D_U}
    \small
    D_U = (\begin{array}{ccc}
             (\sigma_1 I-S)^+ V u_1 & \cdots & (\sigma_n I-S)^+ V u_n
        \end{array}).
    \end{equation}
    Applying \eqref{eq:chain_rule_Pc}, \eqref{eq:diff_P_at0} and \eqref{eq:D_U} into \eqref{eq:diff_last2step_mgexp}, we have the differential of $\mgexp$.
\end{proof}

\begin{proof}[Proof of \cref{props:frechet_means_add_props}]
    Recalling \eqref{eq:fm_alem}, U\ref{props:u1} and U\ref{props:u2} obviously holds.
    
    When SPD matrices $\{A_i\}_{i \leq n}$ commutes, we have
    \begin{equation} \label{eq:fm_commute_alem}
        \fm(\{A_i\})= (\sum A_i)^{\frac{1}{n}}.
    \end{equation}
    With \eqref{eq:fm_commute_alem}, V\ref{props:v1}-V\ref{props:v4} can be easily proved.
\end{proof}

\begin{proof} [Proof of \cref{propos:param_by_geom}]
    Let's first review the update formulation in the RSGD \citep{bonnabel2013stochastic}, which is, geometrically speaking, a natural generalization of Euclidean stochastic gradient descent.
    For a minimization parameter $w$ on an $n$-dimensional smooth connected Riemannian manifold $\calM$, we have the following update, 
    \begin{equation} \label{eq:riem_sgd}
        w^{(t+1)}=\rieExp{w^{(t)}} (-\gamma^{(t)} \pi_{w^{(t)}}(\nabla_{w^{(t)}} L )),
    \end{equation} 
    where $\rieExp{w}(\cdot): T_w\calM \rightarrow \calM$ is the Riemannian exponential map, which maps a tangent vector at $w$ back into the manifold $\calM$, and $\pi_{w}(\cdot): \bbR{n} \rightarrow T_w\calM$ is the projection operator, projecting an ambient Euclidean vector into the tangent space at $w$.
    In the case of the SPD manifold, $\forall S \in \spd{n}, \forall X \in \bbR{n \times n}, \forall V \in \sym{n}$, the exponential map and projection operator is formulated as the following:
    \begin{align}
        \label{eq:spd_proj} \pi_{S}(X) &= S \frac{X+X^\top}{2}S,\\
        \label{eq:spd_exp} \rieExp{S}(V) &= S^{1/2} \mexp(S^{-1/2} V S^{-1/2}) S^{1/2},
    \end{align}
    where $\mexp(\cdot)$ is the matrix exponential.
    For more details about \eqref{eq:spd_proj} and \eqref{eq:spd_exp}, please kindly refer to \citep{yger2013review} and \citep{amari2016information}.
    Substitute \eqref{eq:spd_proj} and \eqref{eq:spd_exp} into \eqref{eq:riem_sgd}, \eqref{eq:update_pos_scalar} can be immediately obtained.
\end{proof}
\begin{proof} [Proof of \cref{props: geom_equivalent_div}]
    Without loss of generality, we focus on the equivalence between $b=B_{11}$ and $a=\alpha_{11}$.
    Let us denote $\log_{e}^{(\cdot)}$ as $\ln^{(\cdot)}$.
    Note that $b$ is essentially expressed as $b=\ln^a$.
    Supposing $b^{(t)} = \ln^{a^{(t)}}$, then we have
    \begin{align}
        \nabla_{a^{(t)}} L 
        &= \nabla_{b^{(t)}} L \frac{\partial \ln^a }{a} |_{a^{(t)}}\\
        &= \nabla_{b^{(t)}} L \frac{1}{a^{(t)}}.
    \end{align}
    according to the update \eqref{eq:update_pos_scalar}, then $\ln^{a^{(t+1)}}$ is 
    \begin{equation}
        \begin{aligned}
            \ln^{a^{(t+1)}} 
            &= \ln^{a^{(t)}e^{-\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L}}\\
            &= \ln^{a^{(t)}} -\gamma^{(t)} a^{(t)}  \nabla_{a^{(t)}} L\\
            &= \ln^{a^{(t)}} - \gamma^{(t)} a^{(t)} (\nabla_{b^{(t)}} L/a^{t}) \\
            &= \ln^{a^{(t)}} -\gamma^{(t)}\nabla_{b^{(t)}} L\\
            &= b^{t} -\gamma^{(t)}\nabla_{b^{(t)}} L.\\
        \end{aligned}
    \end{equation}
    The last row is the exact updated formula of ESGD for $b$.
    
    Therefore, supposing $b^{(0)} = \ln^{a^{(0)}}$, then the optimization results after the overall training are equivalent.
\end{proof}
\begin{proof} [Proof of \cref{props:grad_mlog}]
    \eqref{eq:gradient_eigen_function} is the so-called Daleck\u ii-Kre\u in formula presented in \citep[Page 60]{bhatia2009positive}.
    Now let us focus on the gradient w.r.t $A$.
    Differentiating both sides of \eqref{eq:rw_mul_mlog}:
    \begin{equation}
        \diff X = (*) + U \diff A \odot \log(\Sigma)U^T,
    \end{equation}
    where $(*)$ means other parts related to $\diff U$ and $\diff \Sigma$.
    According to the invariance of first-order differential form, we have,
    \begin{align}
        & \nabla_{X} L : \diff X \nonumber \\ 
        &= \nabla_{S} L:\diff S + \nabla_{X} L: (U \diff A \odot \log(\Sigma)U^T) \\
        &= \nabla_{S}L :\diff S + [U^\top (\nabla_{X} L) U] \odot \log(\Sigma):\diff A, \label{eq:diif_last_row}
    \end{align} 
    Where $A:B=\tr(A^\top B)$ is the Euclidean Frobenius inner product.
    From the second term on the RHS of \eqref{eq:diif_last_row}, we can obtain the gradient w.r.t $A$.
\end{proof}

% \section{Details on the datasets} \label{app:sec:details_datasets}

% \textbf{HDM05.}
% This dataset is comprised of motion capture data (MoCap) covering 130 action classes.
% Each data point is a sequence of frames of 31 3D coordinates.
% After vectorization, each sequence can be represented by a $93 \times 93$ temporal covariance matrix.
% For a fair comparison, we exploit the pre-processed $93 \times 93$
% \href{https://www.dropbox.com/s/dfnlx2bnyh3kjwy/data.zip?dl=0}{covariance features}
% released by \citep{brooks2019riemannian}, which trims the dataset down to 2086 points scattered throughout 117 classes by removing some under-represented classes.
% Following the settings in \citep{brooks2019riemannian},  we split the dataset into 50\% for training and 50\% for testing.

% \textbf{FPHA.}
% It includes 1,175 clips of 45 different action categories.
% Each frame is represented by 21 3D coordinates.
% Similarly, each sequence can be modelled by a $63 \times 63$ covariance matrix.
% For a fair comparison, we follow the experimental protocol in \citep{garcia2018first}, where 600 sequences are used for training and 575 sequences are used for testing.

% \textbf{AFEW.}
% It consists of 7 kinds of emotions with 773 samples for training and 383 samples for validation.
% We use the released pre-trained 
% \href{https://github.com/Open-Debin/Emotion-FAN}{FAN} \citep{meng2019frame}
% to extract deep features and establish a $512 \times 512$ temporal covariance matrix for each video.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}