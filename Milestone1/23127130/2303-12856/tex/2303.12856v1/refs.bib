% Encoding: ISO-8859-1

@Article{HanLiLinEtAl2019,
  author  = {Han, Jiequn and Li, Yingzhou and Lin, Lin and Lu, Jianfeng and Zhang, Jiefu and Zhang, Linfeng},
  title   = {Universal approximation of symmetric and anti-symmetric functions},
  journal = {Commun. Math. Sci.},
  year    = {2022},
  volume  = {20},
  pages   = {1397},
}

@Article{PfauSpencerMatthewsEtAl2020,
  author  = {Pfau, David and Spencer, James S. and Matthews, Alexander G. D. G. and Foulkes, W. M. C.},
  title   = {Ab initio solution of the many--electron Schr\"{o}dinger equation with deep neural networks},
  journal = {Phys. Rev. Research},
  year    = {2020},
  volume  = {2},
  pages   = {033429},
}

@Article{HanZhangE2019,
  author  = {Han, Jiequn and Zhang, Linfeng and E, Weinan},
  title   = {Solving many-electron {Schr{\"o}dinger} equation using deep neural networks},
  journal = {J. Comput. Phys.},
  year    = {2019},
  volume  = {399},
  pages   = {108929},
}

@Article{HermannSchaetzleNoe2020,
  author  = {Hermann, Jan and Schatzle, Zeno and Noe, Frank},
  title   = {Deep neural network solution of the electronic Schr\"{o}dinger equation},
  journal = {Nature Chem.},
  year    = {2020},
  volume  = {12},
  pages   = {891--897},
}

@Article{LuoClark2019,
  author  = {Luo, Di and Clark, Bryan K.},
  title   = {Backflow Transformations via Neural Networks for Quantum Many-Body Wave Functions},
  journal = {Phys. Rev. Lett.},
  year    = {2019},
  volume  = {122},
  pages   = {226401},
}

@Article{Cybenko1989,
  author  = {Cybenko, George},
  title   = {Approximation by superpositions of a sigmoidal function},
  journal = {Mathematics of Control, Signals, and Systems},
  year    = {1989},
  volume  = {2},
  number  = {4},
  pages   = {303--314},
}

@TechReport{Hutter2020,
  author      = {Marcus Hutter},
  title       = {On Representing (Anti)Symmetric Functions},
  institution = {DeepMind},
  year        = {2020},
  number      = {arXiv:2007.15298},
}

@Article{Barron1993,
  author  = {Barron, Andrew R},
  title   = {Universal approximation bounds for superpositions of a sigmoidal function},
  journal = {IEEE Trans. Inform. Theory},
  year    = {1993},
  volume  = {39},
  pages   = {930--945},
}

@Article{HornikStinchcombeWhite1989,
  author  = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  title   = {Multilayer feedforward networks are universal approximators},
  journal = {Neural Networks},
  year    = {1989},
  volume  = {2},
  number  = {5},
  pages   = {359--366},
}

@Article{LohJrGubernatisScalettarEtAl1990,
  author  = {Loh Jr, EY and Gubernatis, JE and Scalettar, RT and White, SR and Scalapino, DJ and Sugar, RL},
  title   = {Sign problem in the numerical simulation of many-electron systems},
  journal = {Phys. Rev. B},
  year    = {1990},
  volume  = {41},
  number  = {13},
  pages   = {9301},
}

@Article{StokesMorenoPnevmatikakisEtAl2020,
  author  = {Stokes, James and Moreno, Javier Robledo and Pnevmatikakis, Eftychios A. and Carleo, Giuseppe},
  title   = {Phases of two-dimensional spinless lattice fermions with first-quantized deep neural-network quantum states},
  journal = {Phys. Rev. B},
  year    = {2020},
  volume  = {102},
  pages   = {205122},
}

@Article{LinGoldshlagerLin_vmcnet,
  author  = {Lin, Jeffmin and Goldshlager, Gil and Lin, Lin},
  title   = {Explicitly antisymmetrized neural network layers for variational Monte Carlo simulation},
  journal = {J. Comput. Phys.},
  year    = {2023},
  volume  = {474},
  pages   = {111765},
}

@Article{SorellaBaroniCarEtAl1989,
  author  = {Sorella, Sandro and Baroni, S and Car, Roberto and Parrinello, M},
  title   = {A novel technique for the simulation of interacting fermion systems},
  journal = {EPL},
  year    = {1989},
  volume  = {8},
  number  = {7},
  pages   = {663},
}

@Article{Ceperley1991,
  author  = {Ceperley, D. M.},
  title   = {{Fermion nodes}},
  journal = {J. Stat. Phys.},
  year    = {1991},
  volume  = {63},
  number  = {5-6},
  pages   = {1237--1267},
}
@Article{RobledoMorenoCarleoGeorgesEtAl2022,
  author  = {Robledo Moreno, Javier and Carleo, Giuseppe and Georges, Antoine and Stokes, James},
  title   = {Fermionic wave functions from neural-network constrained hidden states},
  journal = {Proc. Nat. Acad. Sci.},
  year    = {2022},
  volume  = {119},
  number  = {32},
  pages   = {e2122059119},
}
@Comment{jabref-meta: databaseType:bibtex;}










@book{reed_i_1981,
	series = {Methods of {Modern} {Mathematical} {Physics}},
	title = {I: {Functional} {Analysis}},
	isbn = {978-0-08-057048-8},
	url = {https://books.google.com/books?id=rpFTTjxOYpsC},
	publisher = {Elsevier Science},
	author = {Reed, M. and Simon, B.},
	year = {1981},
}

@article{lowdin_quantum_1955,
	title = {Quantum {Theory} of {Many}-{Particle} {Systems}. {I}. {Physical} {Interpretations} by {Means} of {Density} {Matrices}, {Natural} {Spin}-{Orbitals}, and {Convergence} {Problems} in the {Method} of {Configurational} {Interaction}},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRev.97.1474},
	doi = {10.1103/PhysRev.97.1474},
	number = {6},
	journal = {Phys. Rev.},
	author = {Löwdin, Per-Olov},
	month = mar,
	year = {1955},
	note = {Publisher: American Physical Society},
	pages = {1474--1489},
}

@article{troyer_computational_2005,
	title = {Computational {Complexity} and {Fundamental} {Limitations} to {Fermionic} {Quantum} {Monte} {Carlo} {Simulations}},
	volume = {94},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.94.170201},
	doi = {10.1103/PhysRevLett.94.170201},
	number = {17},
	journal = {Phys. Rev. Lett.},
	author = {Troyer, Matthias and Wiese, Uwe-Jens},
	month = may,
	year = {2005},
	note = {Publisher: American Physical Society},
	pages = {170201},
}

@inproceedings{zaheer_deep_2017,
	title = {Deep {Sets}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{e_barron_2022,
	title = {The {Barron} {Space} and the {Flow}-{Induced} {Function} {Spaces} for {Neural} {Network} {Models}},
	volume = {55},
	issn = {1432-0940},
	url = {https://doi.org/10.1007/s00365-021-09549-y},
	doi = {10.1007/s00365-021-09549-y},
	abstract = {One of the key issues in the analysis of machine learning models is to identify the appropriate function space and norm for the model. This is the set of functions endowed with a quantity which can control the approximation and estimation errors by a particular machine learning model. In this paper, we address this issue for two representative neural network models: the two-layer networks and the residual neural networks. We define the Barron space and show that it is the right space for two-layer neural network models in the sense that optimal direct and inverse approximation theorems hold for functions in the Barron space. For residual neural network models, we construct the so-called flow-induced function space and prove direct and inverse approximation theorems for this space. In addition, we show that the Rademacher complexity for bounded sets under these norms has the optimal upper bounds.},
	language = {en},
	number = {1},
	urldate = {2022-12-31},
	journal = {Constructive Approximation},
	author = {E, Weinan and Ma, Chao and Wu, Lei},
	month = feb,
	year = {2022},
	keywords = {46B99, 65D15, 68T05, Approximation, Function space, Neural network, Rademacher complexity},
	pages = {369--406},
	file = {Submitted Version:/Users/nilin5/Zotero/storage/UBWEWMMG/E et al. - 2022 - The Barron Space and the Flow-Induced Function Spa.pdf:application/pdf},
}

@article{pisier_remarques_1980,
	title = {Remarques sur un résultat non publié de {B}. {Maurey}},
	url = {http://eudml.org/doc/109255},
	language = {fre},
	journal = {Séminaire Analyse fonctionnelle (dit "Maurey-Schwartz")},
	author = {Pisier, G.},
	year = {1980},
	note = {Publisher: Ecole Polytechnique, Centre de Mathématiques},
	keywords = {finite dimensional quotient, subspace},
	pages = {1--12},
}

@misc{zweig_towards_2022,
	title = {Towards {Antisymmetric} {Neural} {Ansatz} {Separation}},
	url = {http://arxiv.org/abs/2208.03264},
	abstract = {We study separations between two fundamental models (or {\textbackslash}emph\{Ans{\textbackslash}"atze\}) of antisymmetric functions, that is, functions \$f\$ of the form \$f(x\_\{{\textbackslash}sigma(1)\}, {\textbackslash}ldots, x\_\{{\textbackslash}sigma(N)\}) = {\textbackslash}text\{sign\}({\textbackslash}sigma)f(x\_1, {\textbackslash}ldots, x\_N)\$, where \${\textbackslash}sigma\$ is any permutation. These arise in the context of quantum chemistry, and are the basic modeling tool for wavefunctions of Fermionic systems. Specifically, we consider two popular antisymmetric Ans{\textbackslash}"atze: the Slater representation, which leverages the alternating structure of determinants, and the Jastrow ansatz, which augments Slater determinants with a product by an arbitrary symmetric function. We construct an antisymmetric function that can be more efficiently expressed in Jastrow form, yet provably cannot be approximated by Slater determinants unless there are exponentially (in \$N{\textasciicircum}2\$) many terms. This represents the first explicit quantitative separation between these two Ans{\textbackslash}"atze.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Zweig, Aaron and Bruna, Joan},
	month = dec,
	year = {2022},
	note = {arXiv:2208.03264 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nilin5/Zotero/storage/VBNWSWHQ/Zweig and Bruna - 2022 - Towards Antisymmetric Neural Ansatz Separation.pdf:application/pdf;arXiv.org Snapshot:/Users/nilin5/Zotero/storage/RM2DTNGX/2208.html:text/html},
}

@article{zweig_functional_nodate,
	title = {A {Functional} {Perspective} on {Learning} {Symmetric} {Functions} with {Neural} {Networks}},
	language = {en},
	author = {Zweig, Aaron and Bruna, Joan},
	pages = {10},
	file = {Zweig and Bruna - A Functional Perspective on Learning Symmetric Fun.pdf:/Users/nilin5/Zotero/storage/SK2B497Z/Zweig and Bruna - A Functional Perspective on Learning Symmetric Fun.pdf:application/pdf},
}

@article{yarotsky_universal_2022,
	title = {Universal {Approximations} of {Invariant} {Maps} by {Neural} {Networks}},
	volume = {55},
	issn = {1432-0940},
	url = {https://doi.org/10.1007/s00365-021-09546-1},
	doi = {10.1007/s00365-021-09546-1},
	abstract = {We describe generalizations of the universal approximation theorem for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-like computational models that are both invariant/equivariant and provably complete in the sense of their ability to approximate any continuous invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete invariant/equivariant network using an intermediate polynomial layer. We invoke classical theorems of Hilbert and Weyl to justify and simplify this construction; in particular, we describe an explicit complete ansatz for approximation of permutation-invariant maps. Second, we consider groups of translations and prove several versions of the universal approximation theorem for convolutional networks in the limit of continuous signals on euclidean spaces. Finally, we consider 2D signal transformations equivariant with respect to the group SE(2) of rigid euclidean motions. In this case we introduce the “charge–conserving convnet”—a convnet-like computational model based on the decomposition of the feature space into isotypic representations of SO(2). We prove this model to be a universal approximator for continuous SE(2)—equivariant signal transformations.},
	number = {1},
	journal = {Constructive Approximation},
	author = {Yarotsky, Dmitry},
	month = feb,
	year = {2022},
	pages = {407--474},
}

@article{eldan_power_nodate,
	title = {The {Power} of {Depth} for {Feedforward} {Neural} {Networks}},
	abstract = {We show that there is a simple (approximately radial) function on Rd, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectiﬁed linear units, sigmoids and thresholds, and formally demonstrates that depth – even if increased by 1 – can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.},
	language = {en},
	author = {Eldan, Ronen and Shamir, Ohad},
	pages = {34},
	file = {Eldan and Shamir - The Power of Depth for Feedforward Neural Networks.pdf:/Users/nilin5/Zotero/storage/ZAYW5UP2/Eldan and Shamir - The Power of Depth for Feedforward Neural Networks.pdf:application/pdf},
}

@article{kulesza_determinantal_2012,
	title = {Determinantal {Point} {Processes} for {Machine} {Learning}},
	volume = {5},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-044},
	doi = {10.1561/2200000044},
	abstract = {Determinantal Point Processes for Machine Learning},
	language = {English},
	number = {2–3},
	urldate = {2022-08-28},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kulesza, Alex and Taskar, Ben},
	month = dec,
	year = {2012},
	pages = {123--286},
	file = {Full Text PDF:/Users/nilin5/Zotero/storage/TYQKK52P/Kulesza and Taskar - 2012 - Determinantal Point Processes for Machine Learning.pdf:application/pdf},
}

@article{santoro_simple_2017,
	title = {A simple neural network module for relational reasoning},
	abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difﬁcult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; textbased question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-ofCLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties.},
	language = {en},
	author = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
	pages = {10},
	file = {Santoro et al. - A simple neural network module for relational reas.pdf:/Users/nilin5/Zotero/storage/CHN9MY95/Santoro et al. - A simple neural network module for relational reas.pdf:application/pdf},
}

@article{barron_universal_1993,
	title = {Universal approximation bounds for superpositions of a sigmoidal function},
	volume = {39},
	issn = {0018-9448, 1557-9654},
	url = {https://ieeexplore.ieee.org/document/256500/},
	doi = {10.1109/18.256500},
	language = {en},
	number = {3},
	urldate = {2022-05-31},
	journal = {IEEE Transactions on Information Theory},
	author = {Barron, A.R.},
	month = may,
	year = {1993},
	pages = {930--945},
}
