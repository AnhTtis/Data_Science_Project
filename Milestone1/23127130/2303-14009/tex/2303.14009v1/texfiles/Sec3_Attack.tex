\section{PoisonedGNN Attack Framework}
\label{sec:Proposed_attack}

In this section, we provide details regarding our {\poisonedgnn} attack framework. 
We summarize the important steps in Fig.~\ref{fig:Poison_flow}.

\subsection{{\poisonedgnn} Threat Model}
\label{sec:threat_model}

We follow the standard threat model of backdoor attacks, which is consistent with prior and state-of-the-art attacks~\cite{xi2021graph,gu2017badnets,DBLP:conf/ndss/LiuMALZW018,wang2019neural}.
To that end, we consider an honest user (e.g., IP vendor) who seeks to train the parameters of a GNN, $f_{\theta}$, using a training dataset $D_{Train}$. 
The user sends the description (e.g., the input size, number of layers) of $f_{\theta}$ to the trainer (\textit{i.e.,} adversary) and the trainer returns $\theta$. 
Since the user employs the GNN in a critical hardware security application (e.g., the detection of IP piracy or HTs), \textit{the user does not completely trust the trainer}. 
Accordingly, the user checks the performance of the trained GNN on a testing dataset $D_{Test}$. The user accepts the model if it meets \textit{target accuracy value} known as \textit{clean accuracy}.\footnote{As per~\cite{gu2017badnets}, the clean accuracy value can be specified based on (i)~the user's requirements, (ii)~the user's domain knowledge, (iii)~service-level agreements between the user and trainer, and/or (iv)~from the output of a simpler model trained by the user.}

\begin{figure}[tb]
\centering
\includegraphics[width=.49\textwidth]{figures/4.pdf}
\caption{Overall framework of {\poisonedgnn}.}
\label{fig:Poison_flow}
\end{figure}


\begin{figure}[tb]
\centering
\includegraphics[width=.3\textwidth]{figures/5.pdf}
\caption{Threat model for backdoor attacks~\cite{xi2021graph,gu2017badnets,DBLP:conf/ndss/LiuMALZW018,wang2019neural}.}
\label{fig:backdoor_threat_model}
\end{figure}

The adversary manipulates $D_{Train}$ by injecting the backdoor trigger into selected input samples (\textit{i.e.,} circuits) to build a backdoored model $\theta^{\mathit{adv}}$. 
The backdoored model should maintain performance on clean input samples (e.g., $D_{Test}$) to avoid detection by the user. In addition, the backdoored model predicts a specific label beneficial to the attacker for circuits with backdoor triggers. 
We summarize the standard threat model of a backdoor attack in Fig.~\ref{fig:backdoor_threat_model}.

\subsection{Backdoor Trigger Design} 
\label{sec:trigger_design}

We follow the attack pipeline outlined in Fig.~\ref{fig:TJ_attack}. We design the backdoor triggers as sub-circuits which are later translated to subgraphs.
To that end, we identified two design challenges that should be considered when designing sub-circuit backdoor triggers to evade possible detection.

\noindent\textit{\textbf{Challenge 1.}} The added sub-circuit backdoors should maintain the original functionality of the design.

\noindent\textit{\textbf{Challenge 2.}} The sub-circuits must pass functional coverage tests. We consider two types of functional coverage measures: (i)~the \textit{toggle coverage} and (ii)~\textit{statement coverage}. 
The toggle coverage measures the portion of bits in a signal that change their state between logic $0$ and logic $1$. 
The statement coverage checks if each executable statement in the design gets executed during simulation. 
Typically, toggle coverage is measured to detect possible issues with signals that are not initialized in the design. 
Thus, the backdoor trigger signals must operate as valid signals with expected toggling behavior.
Furthermore, when a backdoor trigger statement is rarely executed (or not executed), leading to low statement coverage, it can be identified by unused circuit identification methods~\cite{fanci}.



Several design options (e.g., a cascade of constant addition and subtraction operations) are available to address the first challenge of maintaining functionality (\textit{Challenge 1}).
However, we observed that these types of dummy operations and the chosen constant values (\textit{i.e.,} operands) affect the functional coverage of the backdoor trigger (\textit{Challenge 2}). 
Therefore, we need to address both challenges simultaneously. 
For example, Fig.~\ref {fig:Poison_operation}~(a) represents an example of a cascaded constant addition and subtraction, which maintains the original functionality, but only a single bit toggles in the results' vectors (toggle rate of 12.5\%). 
To address both challenges, we integrate a cascade of bit-level inversions (\textit{i.e.,} XOR with logic $1$) as backdoor triggers, as illustrated in Fig.~\ref{fig:Poison_operation}~(b). 
Such a cascading structure is unique and does not affect the functionality when the number of inversions is even. Moreover, the toggle coverage of the backdoor trigger nets becomes $100\%$ since all digits of the vectors toggle. In summary, the backdoor trigger takes a net from the design, with full toggle and statement coverage, performs an even number of inversions, and then passes it to its designated output.


Other backdoor trigger designs are viable as long as they address both outlined design challenges. In our work, without loss of generality, we select the XOR cascade structure to illustrate the {\poisonedgnn} concept. \blue{Please note that the XOR cascade structure has no branches nor conditional statements, and thus it has a $100\%$ branch and condition coverage.}


\begin{figure}[tb]
\centering
\includegraphics[width=.45\textwidth]{figures/6.pdf}
\caption{Backdoor trigger operations for {\poisonedgnn}.}
\label{fig:Poison_operation}
\end{figure}
\begin{figure*}[tb]
\centering
\includegraphics[width=\textwidth]{figures/7.pdf}
\caption{{\poisonedgnn} sub-circuit-based backdoor trigger design.}
\label{fig:trigger}
\end{figure*}

\subsection{Backdoor Trigger Injection}
\label{sec:trigger_injection}

The DFG is a rooted directed graph representing data dependencies from the primary outputs of a Verilog design (root nodes) to the primary inputs (leaf nodes). \purple{Based on this, {\poisonedgnn} first identifies nets that directly feed the outputs of the target design. Then, an output net with complete functional coverage is chosen to be the input to the crafted backdoor trigger. The resulting backdoor trigger subgraph is then directly connected to the roots (outputs) of the DFG, \textit{i.e.,} it belongs to the main graph tree and not to a sub-tree.\footnote{A sub-tree is the child/descendant of a node, which is also, by definition, a tree graph.} As a result, the backdoor trigger becomes an integral part of the DFG/circuit and would not be removed by any DFG optimization procedure~\cite{gomez2009optimizing}. Such an integration also enhances the evasiveness of {\poisonedgnn} to possible detection since $g_t$ is blended with $G$ (\textit{i.e.,} its removal will disconnect $G$) and is not an isolated subgraph.}


Fig.~\ref{fig:trigger} illustrates the integration of the backdoor triggers with the original designs. 
The original RTL design is an 8-bit full-adder (Fig.~\ref{fig:trigger}~\Circled{\scriptsize\textbf{1}}). 
The DFG of the adder is obtained from the \textit{Pyverilog} parser, see~\Circled{\scriptsize\textbf{2}}. 
The backdoor trigger circuit in this example (\textit{ToxicTrigger~A}) injects $12$ stages of cascaded XOR operations.\footnote{The number of added XOR stages depends on the required backdoor trigger size.} 
The first stage takes the initial sum value, which is then XORed with the hexadecimal value $\$FF$, flipping the result bit-wise (in a single line of code). 11 stages of such an inversion operation follow. 
Eventually, the output of the backdoor sub-circuit, which is equivalent to the original sum, reaches the final output of the design.
Both the RTL files in~\Circled{\scriptsize\textbf{1}} and \Circled{\scriptsize\textbf{3}} are equivalent.
Fig.~\ref{fig:trigger}~\Circled{\scriptsize\textbf{4}} illustrates the DFG of the circuit with the backdoor trigger, which is different than the original DFG in Fig.~\ref{fig:trigger}~\Circled{\scriptsize\textbf{2}}, in terms of structure, number of nodes, and number of edges. Hence, through circuit design manipulation, a backdoor trigger subgraph is constructed.


The desired backdoor sub-circuit implementation can be described in Verilog in different ways. 
For example, if each bit in the vectors is toggled individually using a single line of code, the number of lines in the code will increase, altering the size of the corresponding DFG (Fig.~\ref{fig:trigger}~\Circled{\scriptsize\textbf{4}} and \Circled{\scriptsize\textbf{5}}). 
In summary, the size and the structure of the generated backdoor subgraph depend on both the newly added operations and the code syntax. 
As a result, the proposed backdoor trigger generation is flexible, adjusting the backdoor trigger's size depending on the attack's requirements. After inserting the backdoor, the backdoor-trigger-free and backdoor-trigger-embedded circuits are checked for \textit{functional equivalence}. 
All designs in Fig.~\ref{fig:trigger} are equivalent but have different DFGs.


\noindent\textbf{Attack Design.} We characterize {\poisonedgnn} using the backdoor trigger size and poisoning intensity. The backdoor trigger size $t$ refers to the number of nodes in the backdoor trigger/subgraph.
Different circuits have different graph sizes. Therefore, for each circuit, we set the backdoor trigger size $t$ to be $\phi$ fraction of its number of nodes. 
Poisoning intensity $\upgamma$ represents the percentage of training graphs that the adversary poisons.