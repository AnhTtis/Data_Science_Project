\section{Discussion}
\label{sec:disscus}

In this section, we discuss how {\poisonedgnn} can be extended to other GNN-based hardware security systems and possible countermeasures against {\poisonedgnn}.

\subsection{Extension to Other Hardware Security Systems}
\label{sec:extending_poisonedgnn}

GNNs have found success in several hardware security applications. Some of the applications are \textit{design-for-trust} solutions, \textit{i.e.,} defenses, such as GNN4TJ~\cite{yasaei2021gnn4tj}, GNN4IP~\cite{yasaei2021gnn4ip}, HW2VEC~\cite{yu2021hw2vec} for IP piracy and HTs detection, and GNN-RE~\cite{GNNRE} for IP piracy and HTs detection. Attacking such platforms using {\poisonedgnn} introduces new vulnerabilities in the IC supply chain. The rest of the GNN-based systems are unlocking schemes targeting logic obfuscation, such as GNNUnlock~\cite{gnnunlockp}, UNTANGLE~\cite{untangle}, and OMLA~\cite{omla}. Backdooring/fooling such attack platforms using {\poisonedgnn} comes with benefits, \textit{i.e.,} protecting logic obfuscation. {\poisonedgnn} in concept is applicable to any GNN-based system. 

All in all, the goal of {\poisonedgnn} is not to attack a specific GNN-based methodology, but rather to highlight that the security requirements of GNNs themselves go hand in hand with the security requirements of the overall GNN-based hardware security system.



\subsection{Potential Countermeasures}
\label{sec:countermeasure}
Defenses against neural network backdoor attacks can be classified into two categories: (i)~detecting backdoor-trigger-injected inputs at test time and (ii)~identifying backdoored models during the model inspection. 
Two representative defenses of the two categories are the NeuralCleanse~\cite{wang2019neural} and the randomized-smoothing~\cite{zhang2021backdoor}.

\noindent\textbf{NeuralCleanse (NC)}~\cite{wang2019neural} takes a deep neural network and looks for backdoors in each class. When a class is embedded with a backdoor, the required perturbations to change all the inputs in this class to the target class will be abnormally less than in other classes. 
Authors in~\cite{xi2021graph} evaluated NC against their backdoor attack on GNNs and observed that NC alone as a defense gives missing or incorrect results. The reason is that the added trigger is adjusted for each graph. For our case, since the size of the trigger varies from one circuit to another, we expect to observe a similar behavior. 



\noindent\textbf{Randomized Smoothing}~\cite{zhang2021backdoor} extracts sampled graphs from a given larger graph. To suppress the impact of the backdoor-trigger, in randomized smoothing, the model takes a majority voting of the predictions over the sampled graphs for the final prediction. The intuition is that if $G$ is backdoor-trigger-embedded, it will be unlikely that the trigger will exist in all the sub-samples. 

However, for our specific case study, sub-sampling \blue{could potentially degrade the overall performance of GNN4TJ. This is because HTs are subgraphs that may not be present in the majority of the sampled graphs. Any degradation in the performance of GNN4TJ could lead to a higher attack success rate for our {\poisonedgnn} attack.}

\blue{To verify this, we implemented randomized smoothing as follows: We used all of our trained backdoored HT detection models with varying backdoor-trigger sizes and poisoning intensities, specifically the PIC, RS232, and AES datasets with trigger sizes of 20\%, 30\%, 40\%, and 50\%, and poisoning intensities of 15\%, 20\%, and 25\%. For testing, we used both our backdoored samples and clean samples to measure the attack's success rate, as well as the clean accuracy and backdoor accuracy.}

\blue{To extract subgraphs during testing, we randomly select 10 root nodes per sampled graph and extract their 2-hop neighborhood, resulting in 20 subgraphs per testing graph. The backdoored and original GNNs then predict labels for these subgraphs and use majority voting among the 20 labels to predict the label for the testing graph. The results for trigger size of $20\%$ are documented in Table~\ref{tabl:random}.}

\blue{As expected, the clean accuracy of GNN4TJ drops by around 75\% due to randomized smoothing, indicating that the performance of the original clean GNN is degraded. On the other hand, our backdoor attack was successful with a 100\% success rate in all evaluated cases, confirming our claims. The extracted subgraphs did not contain the actual hardware Trojans. Therefore, the model always predicted a benign class, whether there exists a Trojan or not in the original design. Thus, our results demonstrate that randomized smoothing is not an effective defense against {\poisonedgnn}.}



\begin{table}[!t]
\centering
\caption{\blue{The effect of randomized smoothing on backdoored and clean GNN4TJ models. Randomized smoothing degrades the performance of the clean GNN model, failing to defend against {\poisonedgnn}.}}

\resizebox{0.95\columnwidth}{!}{%
\begin{tabular}{ccccc}
\hline
\textbf{Dataset} & \textbf{$\upgamma$} & \textbf{Clean Accuracy} & \textbf{Backdoor Accuracy} & \textbf{ASR} \\ \hline
\multirow{3}{*}{\textbf{AES}} & 15\% & 20\% & 20\% & 100\% \\ \cline{2-5} 
 & 20\% & 20\% & 20\% & 100\% \\ \cline{2-5} 
 & 25\% & 20\% & 20\% & 100\% \\ \hline
\multirow{3}{*}{\textbf{PIC}} & 15\% & 20\% & 20\% & 100\% \\ \cline{2-5} 
 & 20\% & 20\% & 20\% & 100\% \\ \cline{2-5} 
 & 25\% & 20\% & 20\% & 100\% \\ \hline
\multirow{3}{*}{\textbf{RS232}} & 15\% & 12.5\% & 12.5\% & 100\% \\ \cline{2-5} 
 & 20\% & 12.5\% & 12.5\% & 100\% \\ \cline{2-5} 
 & 25\% & 12.5\% & 12.5\% & 100\% \\ \hline
\end{tabular}%
}
\label{tabl:random}
\end{table}


\noindent\textbf{Node-wise Classification:} The GNN platforms targeted in this work operate at the graph level, \textit{i.e.,} perform graph classification. 
Recently, researchers have developed GNN-based solutions to detect HTs at the node level~\cite{muralidhar2021contrastive,GNN4TJ_Journal}, \textit{i.e.,} performing node classification, in which the GNN only observes the local neighborhoods around the nodes to be classified, unlike graph classification in which the GNN observes the entire graph. Therefore, since the added backdoor trigger might not be in the considered neighborhood (based on the hyperparameters of the GNN), we expect such platforms to be resilient to {\poisonedgnn}. Extending {\poisonedgnn} to attack node-wise GNN platforms remains part of our future work.

 \noindent\blue{\textbf{Graph-Size-Based Detection:} The threat model is that a design company purchases third-party IP cores with intended functionality, but some of these third-party IPs may be malicious. Since the design company has no reference point on the original size of the third-party IP, measuring the graph size will not aid in the detection process.}

\blue{Further, {\poisonedgnn} is applicable using different trigger sizes and training configurations. Taking the case of GNN4IP as an example, we evaluate the effectiveness of our backdoor attack for different trigger sizes, considering a trigger size of 2\%, 5\%, and 20\% of the original design. We achieve an attack success rate of 100\%, even for small trigger sizes of 2\% (Fig.~\ref{fig:Results_IP}). However, the smaller the trigger size is, the larger the required poisoning intensity for an effective attack. As the adversary is the MLaaS provider with access to the training dataset, there are no restrictions on the poisoning intensity that can be used.}


\begin{table}[!t]
\centering
\caption{\blue{Retraining as a possible defense against {\poisonedgnn}}}
\label{tab:retrain}
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{ccc}
\hline
\textbf{Dataset} & \textbf{Retraining Epochs} & \textbf{Attack Success Rate} \\ \hline
\multirow{3}{*}{\textbf{AES}} & 0 & 100\% \\ \cline{2-3} 
 &Setup II - 50 & 100\% \\ \cline{2-3} 
 &Setup I - 200& 0 \\ \hline
\multirow{3}{*}{\textbf{PIC}} & 0 & 100\% \\ \cline{2-3} 
 &Setup II - 50 & 100\% \\ \cline{2-3} 
 &Setup I - 200& 0 \\ \hline
\multirow{3}{*}{\textbf{RS232}} & 0 & 100\% \\ \cline{2-3} 
 &Setup II - 50 & 100\% \\ \cline{2-3} 
 &Setup I - 200& 100\% \\ \hline
\end{tabular}%
}
\end{table}

\noindent\blue{\textbf{Retraining.} {\poisonedgnn} is applicable in a MLaaS setup, which refers to ML tools as part of cloud computing services. Specifically, a design company wants to develop an ML model for solving a specific task (e.g., detecting hardware Trojans). MLaaS platforms can assist in building, training, and deploying the model. The actual computation (e.g., training) is handled by the MLaaS providerâ€™s data centers. Therefore, retraining the model by the design company is outside of the considered threat model.} 

\blue{Nevertheless, retraining (when applicable) is a possible defense to thwart backdoor attacks, including {\poisonedgnn}. To this end, we selected three backdoored models --AES, PIC, and RS232-- with backdoor trigger size of $50\%$ and poisoning intensity of $25\%$, and retrained the models considering two setups. In setup I, we performed retraining using the same resources as the backdoored MLaaS model, considering only the clean samples. In setup II, we performed retraining using one-fourth the epochs of setup I to investigate the effect of different training parameters. After retraining, we performed testing again to study how the models behave once presented with the backdoor triggers. The results are documented in Table~\ref{tab:retrain}. In setup I, the backdoor ASR drops from 100\% to 0\% for the case of the AES and PIC datasets, while for RS232, the ASR remains the same. Setup I is an extreme evaluation case in which the design company was able to replicate all the steps taken to build and train the model. In setup II, the ASR remained at 100\% for all three datasets, demonstrating that sufficient resources are needed to overpower the impact of the backdoored model. \textit{In conclusion, retraining can be an effective defense against backdoor attacks, but it can require expensive resources. If the design company has the necessary resources, then it may render the use of MLaaS unnecessary in the first place.}} 

\noindent\blue{\textbf{Logic Optimization.} The advantage of designing ``disappearing'' backdoor triggers is that the attack will not leave a footprint in the fabricated chips. However, a possible countermeasure would be to optimize the suspicious circuits before passing them to the GNN, as demonstrated in Section~\ref{sec:footprint}. One potential way to overcome this limitation is by including controlled structures, such as conditional statements, in the backdoor trigger designs. We plan to investigate this further as part of our future work and use our current backdoor designs as proof of concept.}




\subsection{\blue{Related Work}}
\label{sec:related_work}

\blue{Various supervised ML and deep learning models are susceptible to backdoor attacks in varying contexts and settings. Considering the specific case of ML-based HT detection, researchers have already demonstrated that some traditional methods based on support vector machines and random forest~\cite{response_reference_f, response_reference_z} can be vulnerable to backdoor attacks~\cite{response_reference_y}.} \blue{We have focused on the new paradigm of graph-based learning as it demonstrates state-of-the-art performance detecting HTs and IP piracy. Further, the vulnerability of graph-based learning platforms to backdoor attacks has not been investigated yet in the context of hardware design.}

\blue{Nevertheless, the backdoor-attack concepts followed in our work are general and applicable to other systems. For instance, in~\cite{response_reference_g}, the authors presented an artificial immune system (AIS)-based HT detection at the RTL. The platform represents the RTL file as a control flow graph (CFG), extracts features, and recognizes the signatures of Trojan circuits via a training procedure. Therefore, such models are vulnerable to {\poisonedgnn} and can associate the backdoor trigger features with the benign class. Unfortunately, this implementation is not open-sourced, and we could not conduct experiments to support our claims.}

\blue{As indicated in Section~\ref{sec:countermeasure}, {\poisonedgnn} is not applicable to node-wise HT detection. Thus, the works presented in~\cite{response_reference_W} and~\cite{response_reference_k}, which represent RTL designs as graphs, e.g., AST or CFG, and perform node classification using traditional ML methods, may bypass {\poisonedgnn}. The codes for these platforms are not released. Hence we could not confirm their resilience to {\poisonedgnn} by experiments.}

 \blue{One way to extend {\poisonedgnn} to node-wise HT classification is by training another GNN, called the payload model. The payload model accepts the circuits and looks for the backdoor trigger. If the backdoor trigger is detected, the payload model can influence the node-wise classification as non-Trojan, similar to the approach presented in~\cite{response_reference_y}.}
