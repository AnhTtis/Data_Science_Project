\section{Background and Related work}
\label{sec:background}

In this section, we provide a brief background on GNNs, their usage in hardware security-based frameworks, and backdoor attacks. 
We also depict the notations used in this paper in Table~\ref{tab:symbol}.


\subsection{Graph Neural Networks (GNNs)}
\label{sec:GNNs}

\noindent\textit{\textbf{Definition 1 (Graph)}.} A graph is represented as $G(V,E)$, where $V$ represents the set of nodes and $E$ represents the set of edges.
Additionally, $x_v$ for $v \in V$ represents node attributes for $G$. 
$G$ includes both the graph's connectivity (\textit{i.e.,} topological features) and node attributes $X$ (\textit{i.e.,} descriptive features). 
The adjacency matrix of $G$ is denoted as $A$, with $A_{u,v}=1$ iff $(u,v)\in E$.

\noindent\textit{\textbf{Definition 2 (Subgraph)}.} A subgraph $\ssub{g}{t}(V_t, E_t)$ induced from $G$ is a graph with $V_t\in V$ and $E_t\in E$.

\noindent\textit{\textbf{Definition 3 (Graph Classification)}.} Given a set of graphs $\{G\}$ and a group of different categories, we aim to classify the graphs to their respective classes $\{y_G\}$. 
For instance, given $G$, which is a graph representation of an HT-Infected (TjIn) circuit, its class to be predicted can be whether $G$ is malicious or not.

GNNs learn on the structure and node attributes of $G$ to generate a representation (\textit{i.e.,} \textit{embedding}) $\ssub{z}{G}$ that facilitates the prediction of the graph's class. 
More specifically, a \gnn takes as input a graph $G$ and generates an embedding $\ssub{z}{v}$ for each node $v \in V$. 
The GNN updates the node embeddings through multiple iterations of neighborhood aggregation as follows.

\begin{equation}
\ssup{Z}{(l)} = \mathsf{Aggregate}\left(A, \ssup{Z}{(l-1)}; \ssup{\theta}{(l-1)} \right)
\end{equation}
where $\ssup{Z}{(l)}$ is the node embeddings matrix at the $l$-th iteration and $\ssup{\theta}{(l-1)}$ is a trainable weight matrix. 
$\ssup{Z}{(0)}$ represents the initial node features $X$. 
The $\mathsf{Aggregate}$ function is typically an order invariant function, such as $\mathsf{sum}$, $\mathsf{average}$, or $\mathsf{max}$. After $L$ iterations of neighborhood aggregation, a $\mathsf{readout}$ function is performed to generate a graph-level embedding, $\ssub{z}{G}$, which can be used for graph classification. Overall, a \gnn models a function $f_{\theta}$ that generates $\ssub{z}{G} = f_{\theta}(G)$ for $G$. The embedding is then passed to a downstream classifier $g$ for classification~\cite{kipf2016semi}. 
The predicted class label for graph $G$ is denoted as $\hat{y}_{G}$, where $\ssub{\hat{y}}{G}=g(\ssub{Z}{G})$.


\subsection{GNNs for Hardware Trojan Detection}
\label{sec:GNNs_HT_detection}


The globalized IC supply chain facilitates adversaries to insert HTs~\cite{tehranipoor2010survey}. 
HTs are malicious modifications aimed to leak secret assets from ICs or cause disruption to the intended functionality. Insertion of HTs during design stage is a pressing concern~\cite{8167465}. Third-party IPs (3PIPs) in RTL format are complex and flexible, supporting multiple configurations for different applications, which is a convenient structure for adversaries to insert HTs.

In the case of untrusted 3PIPs, a golden model (\textit{i.e.,} HT-free) of the IP is unavailable, and thus, it is challenging to detect possible HTs using testing-based~\cite{hicks2010overcoming} or side-channel-based methods~\cite{huang2018scalable}. 
Destructive methods (\textit{i.e.,} depackaging, delayering, and reverse engineering, followed by a circuit-level comparison~\cite{kommerling1999design}) can check if ICs are HT-infected but only after fabrication, when the damage is already done~\cite{bao2015reverse}. 
Other HT detection methods (e.g., graph-similarity-based techniques~\cite{fyrbiak2019graph}) have several shortcomings, such as complexity~\cite{chakraborty2008demand} and the inability to identify unknown HTs.


GNN4TJ~\cite{yasaei2021gnn4tj} is a GNN-based platform that detects HTs without requiring prior knowledge of the design IP or HT structure. 
GNN4TJ converts the RTL design into a corresponding data flow graph (DFG). This DFG is then fed to a GNN to extract features and learn the structure and behavior of the underlying design. 
Subsequently, the GNN performs a graph classification task and assigns a label $\hat{y}$ to each design $p$ based on the presence of HTs. The GNN learns the properties of HTs and generalizes to unseen HTs. Researchers have proposed other GNN-based platforms for HT detection~\cite{muralidhar2021contrastive,GNN4TJ_Journal}, highlighting the requirement for a proper security evaluation of such GNN models before wide-scale adoption.
GNN4TJ is an open-source framework making it suitable to be used as a case study.\footnote{At the time of writing, the GNN-based HT detection proposed in~\cite{muralidhar2021contrastive} has not been released yet.}


\begin{table}[!t]
\centering
\caption{Symbols and Notations}
\label{tab:symbol}
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{ll}
\hline
\textbf{Notation} & \textbf{Definition} \\
\hline
$G, \ssub{y}{G}$ & Graph, class\\\hline
 $A$ & Adjacency matrix\\\hline
 $Z$ & Node embeddings matrix \\\hline
 $X$ & Initial node features matrix \\\hline
 $\ssub{z}{G}$ & Graph embedding \\\hline
 $\theta$ & GNN trainable parameters \\\hline
 $\ssub{f}{\theta}, \ssub{f}{\theta^{\mathit{adv}}}$ & Original, backdoored \gnn\\\hline
 $\ssub{y}{t}$, $\hat{y}$ & Target class, predicted class\\\hline
 $g$ & Downstream classifier\\\hline
 $p$ & Target RTL/gate-level netlist \\\hline
 $t$ & Backdoor trigger size\\\hline
 $\upgamma$ & Poisoning intensity\\\hline
 $\ssub{g}{t}$ & Backdoor trigger subgraph\\\hline
 $\delta$ & Predefined decision boundary \\\hline 
 $D_{Train}/D_{Test}$ & Training, testing datasets \\\hline
 \end{tabular}%
}
 \end{table}
 \begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figures/3.pdf}
\caption{Subgraph-based backdoor attack on graph neural networks. Adapted from~\cite{zhang2021backdoor}.}
\label{fig:TJ_attack}
\end{figure*}
 


\subsection{GNNs for Intellectual Property Piracy Detection}
\label{sec:background_IP}

In addition to the threat of HT insertion, the globalization of the IC supply chain enables untrusted entities to access the design IP, leading to concerns about IP piracy~\cite{primer,rostami2013hardware}.
IP piracy refers to the theft of the design IP by an adversary (e.g., foundry or end-user) to develop competing devices without incurring the research and development costs. 
Therefore, effective IP piracy detection techniques are imperative to disclose IP theft.




GNN4IP~\cite{yasaei2021gnn4ip} is a GNN-based IP piracy detection technique that assesses the similarity between circuits revealing potential theft.
In GNN4IP, the structure of the design IP becomes its signature. 
Hence, GNN4IP does not require addition of any watermarks or fingerprints (thereby reducing overheads) that could be prone to removal attacks~\cite{alkabani2007remote,cui2015ultra}.
GNN4IP compares two circuits ($p_1$ and $p_2$) either in RTL or gate-level logic representation.
Like GNN4TJ, the circuits are converted to DFG or abstract syntax tree (AST) format and fed to a GNN. 
The GNN generates an embedding for each circuit from its underlying structure (\textit{i.e.,} signature). 
Subsequently, the GNN optimizes the embeddings so that distances in the embedding space reflect the similarity between designs (\textit{i.e.,} graphs)~\cite{hamilton2017inductive}.
Therefore, GNN4IP infers piracy by computing the \textit{cosine similarity score} between the obtained embeddings as follows, where $z_{p_1}$ and $z_{p_2}$ represent the embedding vectors of designs $p_1$ and $p_2$. 
Finally, GNN4IP compares the similarity score with a predefined decision boundary $\delta$ to predict whether there is piracy between the two circuits, returning a binary label as its output ($0$ or $1$).

\vspace{-0.5em}
\begin{equation}
\vspace{-0.5em}
   \text{cosine\_sim}(z_{p_1}, z_{p_2}) =
    \frac{z_{p_1} \cdot z_{p_2}}{|z_{p_1}||z_{p_2}|}
\end{equation}


\subsection{Backdoor Attacks on GNNs}
\label{sec:backdoor_attacks_GNN}

Backdoor attacks are special types of data poisoning attacks on machine learning (ML) systems. 
Traditional data poisoning attacks corrupt training samples to downgrade the overall performance of ML models~\cite{biggio2012poisoning}. 
However, backdoor attacks maintain original performance until the model is provided with an input sample containing a \textit{``backdoor trigger.''}
Such a backdoor trigger causes a pre-determined output, $\ssub{y}{t}$, beneficial to an adversary~\cite{gu2017badnets}. 
An adversary can deploy backdoor attacks by manipulating the training data and the corresponding labels. 


For the case of GNNs, where the input samples are graphs, existing backdoor attacks inject triggers in the form of subgraphs $\ssub{g}{t}$. 
Fig.~\ref{fig:TJ_attack} illustrates the flow of a subgraph-based backdoor attack against GNNs~\cite{zhang2021backdoor}.
First, a backdoor trigger and a target label $\ssub{y}{t}$ (e.g., \textit{class $1$}, \textit{i.e.,} $\ssub{y}{t}=1$) are determined (Fig.~\ref{fig:TJ_attack}~\Circled{\scriptsize\textbf{1}}). 
Next, an adversary manipulates the original training samples in two ways. (i)~Backdoor triggers are embedded into selected training samples with true labels of \textit{class $0$}, and the corresponding labels (for training) are changed to the target label, \textit{i.e.,} become \textit{class $1$} (Fig.~\ref{fig:TJ_attack}~\Circled{\scriptsize\textbf{2}}). (ii)~Backdoor triggers are embedded into training samples with original true labels of \textit{class $1$}, without altering their corresponding training labels, (Fig.~\ref{fig:TJ_attack}~\Circled{\scriptsize\textbf{3}}). 
This way, the GNN is forced to associate the backdoor trigger $\ssub{g}{t}$ with the target label $\ssub{y}{t}$. 
This GNN is referred to as the \textit{backdoored GNN}. 
During testing, backdoor-trigger-free graphs are classified to their original labels, (Fig.~\ref{fig:TJ_attack}~\Circled{\scriptsize\textbf{4}}). 
The same graphs are misclassified with the target label when injected with backdoor triggers (Fig.~\ref{fig:TJ_attack}~\Circled{\scriptsize\textbf{5}}).



