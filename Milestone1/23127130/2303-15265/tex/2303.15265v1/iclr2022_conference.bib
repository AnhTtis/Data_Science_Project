@inproceedings{ijcai2020p0533,
  title     = {CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation  for Zero-Shot Cross-Lingual NLP},
  author    = {Qin, Libo and Ni, Minheng and Zhang, Yue and Che, Wanxiang},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {3853--3860},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/533},
  url       = {https://doi.org/10.24963/ijcai.2020/533},
}

@ARTICLE{chaudhary-etal-2020-dict,
       author = {{Chaudhary}, Aditi and {Raman}, Karthik and {Srinivasan}, Krishna and {Chen}, Jiecao},
        title = "{DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = oct,
          eid = {arXiv:2010.12566},
        pages = {arXiv:2010.12566},
archivePrefix = {arXiv},
       eprint = {2010.12566},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201012566C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{vaswani-etal-2017-attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{liu-etal-2020-multilingual,
    author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
    title = "{Multilingual Denoising Pre-training for Neural Machine Translation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {726-742},
    year = {2020},
    month = {11},
    abstract = "{This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00343},
    url = {https://doi.org/10.1162/tacl\_a\_00343},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00343/1923401/tacl\_a\_00343.pdf},
}


@InProceedings{song-etal-2019-mass,
  title = 	 {{MASS}: Masked Sequence to Sequence Pre-training for Language Generation},
  author =       {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5926--5936},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/song19d/song19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/song19d.html},
  abstract = 	 {Pre-training and fine-tuning, e.g., BERT&nbsp;\citep{devlin2018bert}, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Especially, we achieve the state-of-the-art accuracy (30.02 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model&nbsp;\citep{bahdanau2015neural}.}
}

@ARTICLE{bapna-etal-2022-building,
       author = {{Bapna}, Ankur and {Caswell}, Isaac and {Kreutzer}, Julia and {Firat}, Orhan and {van Esch}, Daan and {Siddhant}, Aditya and {Niu}, Mengmeng and {Baljekar}, Pallavi and {Garcia}, Xavier and {Macherey}, Wolfgang and {Breiner}, Theresa and {Axelrod}, Vera and {Riesa}, Jason and {Cao}, Yuan and {Chen}, Mia Xu and {Macherey}, Klaus and {Krikun}, Maxim and {Wang}, Pidong and {Gutkin}, Alexander and {Shah}, Apurva and {Huang}, Yanping and {Chen}, Zhifeng and {Wu}, Yonghui and {Hughes}, Macduff},
        title = "{Building Machine Translation Systems for the Next Thousand Languages}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2022,
        month = may,
          eid = {arXiv:2205.03983},
        pages = {arXiv:2205.03983},
archivePrefix = {arXiv},
       eprint = {2205.03983},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220503983B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{nllb2022,
  author    = {NLLBTeam and
  Marta R. Costa-jussà and 
  James Cross and 
  Onur Çelebi and 
  Maha Elbayad and 
  Kenneth Heafield and 
  Kevin Heffernan and 
  Elahe Kalbassi and 
  Janice Lam and 
  Daniel Licht and 
  Jean Maillard and 
  Anna Sun and 
  Skyler Wang and 
  Guillaume Wenzek and 
  Al Youngblood and 
  Bapi Akula and 
  Loic Barrault and 
  Gabriel Mejia Gonzalez and 
  Prangthip Hansanti and 
  John Hoffman and 
  Semarley Jarrett and 
  Kaushik Ram Sadagopan and 
  Dirk Rowe and 
  Shannon Spruit and 
  Chau Tran and 
  Pierre Andrews and 
  Necip Fazil Ayan and 
  Shruti Bhosale and 
  Sergey Edunov and 
  Angela Fan and 
  Cynthia Gao and 
  Vedanuj Goswami and 
  Francisco Guzmán and 
  Philipp Koehn and 
  Alexandre Mourachko and 
  Christophe Ropers and 
  Safiyyah Saleem and 
  Holger Schwenk and 
  Jeff Wang},
  title     = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  year      = {2022}
}

@inproceedings{flores101,
  title={The FLORES-101  Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm\'{a}n, Francisco and Fan, Angela},
  year={2021}
}

@inproceedings{guzman-etal-2019-two,
  title={Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},
  author={Guzm\'{a}n, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.01382},
  year={2019}
}

@article{m2m-100,
author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand},
title = {Beyond English-Centric Multilingual Machine Translation},
year = {2022},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M- 100 model},
journal = {J. Mach. Learn. Res.},
month = {jul},
articleno = {107},
numpages = {48},
keywords = {neural networks, model scaling, bitext mining, many-to-many, multilingual machine translation}
}

@inproceedings{bahdanau-etal-2015-neural,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wu-etal-2016-google,
title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
year	= {2016},
URL	= {http://arxiv.org/abs/1609.08144},
journal	= {CoRR},
volume	= {abs/1609.08144}
}

@inproceedings{sutskever-etal-2014-sequence,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{arivazhagan-etal-2019-massively,
  author    = {Naveen Arivazhagan and
               Ankur Bapna and
               Orhan Firat and
               Dmitry Lepikhin and
               Melvin Johnson and
               Maxim Krikun and
               Mia Xu Chen and
               Yuan Cao and
               George F. Foster and
               Colin Cherry and
               Wolfgang Macherey and
               Zhifeng Chen and
               Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings
               and Challenges},
  journal   = {CoRR},
  volume    = {abs/1907.05019},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05019},
  eprinttype = {arXiv},
  eprint    = {1907.05019},
  timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-05019.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gulcehre-etal-2017-on,
title = {On integrating a language model into neural machine translation},
journal = {Computer Speech \& Language},
volume = {45},
pages = {137-148},
year = {2017},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2017.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0885230816301395},
author = {Caglar Gulcehre and Orhan Firat and Kelvin Xu and Kyunghyun Cho and Yoshua Bengio},
keywords = {Neural machine translation, Monolingual data, Language models, Low resource machine translation, Deep learning, Neural network},
abstract = {Recent advances in end-to-end neural machine translation models have achieved promising results on high-resource language pairs such as En→ Fr and En→ De. One of the major factor behind these successes is the availability of high quality parallel corpora. We explore two strategies on leveraging abundant amount of monolingual data for neural machine translation. We observe improvements by both combining scores from neural language model trained only on target monolingual data with neural machine translation model and fusing hidden-states of these two models. We obtain up to 2 BLEU improvement over hierarchical and phrase-based baseline on low-resource language pair, Turkish→ English. Our method was initially motivated towards tasks with less parallel data, but we also show that it extends to high resource languages such as Cs→ En and De→ En translation tasks, where we obtain 0.39 and 0.47 BLEU improvements over the neural machine translation baselines, respectively.}
}

@article{artetxe-etal-2017-unsupervised,
  author    = {Mikel Artetxe and
               Gorka Labaka and
               Eneko Agirre and
               Kyunghyun Cho},
  title     = {Unsupervised Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1710.11041},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.11041},
  eprinttype = {arXiv},
  eprint    = {1710.11041},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-11041.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{artetxe-etal-2018-massively,
  author    = {Mikel Artetxe and
               Holger Schwenk},
  title     = {Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual
               Transfer and Beyond},
  journal   = {CoRR},
  volume    = {abs/1812.10464},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.10464},
  eprinttype = {arXiv},
  eprint    = {1812.10464},
  timestamp = {Wed, 02 Jan 2019 14:40:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-10464.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{heffernan-etal-2022-bitext,
  doi = {10.48550/ARXIV.2205.12654},
  
  url = {https://arxiv.org/abs/2205.12654},
  
  author = {Heffernan, Kevin and Çelebi, Onur and Schwenk, Holger},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{siddhant-etal-2022-towards,
  author    = {Aditya Siddhant and
               Ankur Bapna and
               Orhan Firat and
               Yuan Cao and
               Mia Xu Chen and
               Isaac Caswell and
               Xavier Garcia},
  title     = {Towards the Next 1000 Languages in Multilingual Machine Translation:
               Exploring the Synergy Between Supervised and Self-Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/2201.03110},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.03110},
  eprinttype = {arXiv},
  eprint    = {2201.03110},
  timestamp = {Thu, 20 Jan 2022 14:21:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-03110.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{cheng-etal-2021-self,
  title = 	 {Self-supervised and Supervised Joint Training for Resource-rich Machine Translation},
  author =       {Cheng, Yong and Wang, Wei and Jiang, Lu and Macherey, Wolfgang},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1825--1835},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/cheng21b/cheng21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/cheng21b.html},
  abstract = 	 {Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-rich NMT. In this paper, we propose a joint training approach, F2-XEnDec, to combine self-supervised and supervised learning to optimize NMT models. To exploit complementary self-supervised signals for supervised learning, NMT models are trained on examples that are interbred from monolingual and parallel sentences through a new process called crossover encoder-decoder. Experiments on two resource-rich translation benchmarks, WMT’14 English-German and WMT’14 English-French, demonstrate that our approach achieves substantial improvements over several strong baseline methods and obtains a new state of the art of 46.19 BLEU on English-French when incorporating back translation. Results also show that our approach is capable of improving model robustness to input perturbations such as code-switching noise which frequently appears on the social media.}
}

@misc{caswell2020language,
  doi = {10.48550/ARXIV.2010.14571},
  
  url = {https://arxiv.org/abs/2010.14571},
  
  author = {Caswell, Isaac and Breiner, Theresa and van Esch, Daan and Bapna, Ankur},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{arivazhagan2019massively,
  author    = {Naveen Arivazhagan and
               Ankur Bapna and
               Orhan Firat and
               Dmitry Lepikhin and
               Melvin Johnson and
               Maxim Krikun and
               Mia Xu Chen and
               Yuan Cao and
               George F. Foster and
               Colin Cherry and
               Wolfgang Macherey and
               Zhifeng Chen and
               Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings
               and Challenges},
  journal   = {CoRR},
  volume    = {abs/1907.05019},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05019},
  eprinttype = {arXiv},
  eprint    = {1907.05019},
  timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-05019.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @article{sutton_2019, title={The Bitter Lesson}, DOI={https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf}, author={Sutton, Richard}, year={2019}, month={Mar}} 
 
@misc{maheshwari-etal-2022-dictdis,
  doi = {10.48550/ARXIV.2210.06996},
  
  url = {https://arxiv.org/abs/2210.06996},
  
  author = {Maheshwari, Ayush and Sharma, Piyush and Jyothi, Preethi and Ramakrishnan, Ganesh},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DICTDIS: Dictionary Constrained Disambiguation for Improved NMT},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dipmt2023,
  doi = {10.48550/ARXIV.2302.07856},
  
  url = {https://arxiv.org/abs/2302.07856},
  
  author = {Ghazvininejad, Marjan and Gonen, Hila and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{jiao-etal-2023-chatgpt,
  doi = {10.48550/ARXIV.2301.08745},
  url = {https://arxiv.org/abs/2301.08745},
  author = {Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and Wang, Xing and Tu, Zhaopeng},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Is ChatGPT A Good Translator? A Preliminary Study},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{kocmi-2021-toship,
  author    = {Tom Kocmi and
               Christian Federmann and
               Roman Grundkiewicz and
               Marcin Junczys{-}Dowmunt and
               Hitokazu Matsushita and
               Arul Menezes},
  title     = {To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics
               for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2107.10821},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.10821},
  eprinttype = {arXiv},
  eprint    = {2107.10821},
  timestamp = {Thu, 29 Jul 2021 16:14:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-10821.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{freitag-2022-stop,
  author    = {Markus Freitag and
               Ricardo Rei and
               Nitika Mathur and
               Chi-kiu Lo and
               Craig Stewart and
               Eleftherios Avramidis and
               George Foster and
               Alon Lavie and
               Andr{\:' e} F. T. Martins
               },
  title     = {Results of WMT22 Metrics Shared Task: Stop Using BLEU – Neural Metrics Are Better and More Robust},
  year      = {2022},
  url       = {https://aclanthology.org/2022.wmt-1.2.pdf},
}


@article{sellam-2020-bleurt,
  author    = {Thibault Sellam and
               Dipanjan Das and
               Ankur P. Parikh},
  title     = {{BLEURT:} Learning Robust Metrics for Text Generation},
  journal   = {CoRR},
  volume    = {abs/2004.04696},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.04696},
  eprinttype = {arXiv},
  eprint    = {2004.04696},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-04696.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
akera2022machine,
title={Machine Translation For African Languages: Community Creation Of Datasets And Models In Uganda},
author={Benjamin Akera and Jonathan Mukiibi and Lydia Sanyu Naggayi and Claire Babirye and Isaac Owomugisha and Solomon Nsumba and Joyce Nakatumba-Nabende and Engineer Bainomugisha and Ernest Mwebaze and John Quinn},
booktitle={3rd Workshop on African Natural Language Processing},
year={2022},
url={https://openreview.net/forum?id=BK-z5qzEU-9}
}

@dataset{makereremt2021,
  author       = {Mukiibi, Jonathan and
                  Claire, Babirye and
                  Joyce, Nakatumba-Nabende},
  title        = {{The Makerere MT Corpus:  English to Luganda 
                   parallel corpus}},
  month        = may,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5089560},
  url          = {https://doi.org/10.5281/zenodo.5089560}
}

@misc{wanjawa2022kencorpus,
      title={Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks}, 
      author={Barack Wanjawa and Lilian Wanzare and Florence Indede and Owen McOnyango and Edward Ombui and Lawrence Muchemi},
      year={2022},
      eprint={2208.12081},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{hornMT,
  author = {Asmelash Hadgu and 
             Gebrekirstos Gebremeskel and
             Abel Aregawi},
  year = {2022},
  title = {{HornMT}},
  howpublished = {\url{https://github.com/asmelashteka/HornMT}},
  note = {Accessed: 2023-03-24}
}



@misc{waxal,
  title = {{Waxal-Multilingual}},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Waxal-Multilingual}}
}

@misc{faroesecorpus,
  title = {{Sprotin Translations}},
  author = {{Andersen, J{\' o}gvan}},
  year = {{2021}},
  howpublished = {\url{https://github.com/Sprotin/translations}},
}
@misc{abkhazcorpus,
  title = {{Multilingual Parallel Corpus}},
  author = {{Tlisha, Nart}},
  year = {{2022}},
  howpublished = {\url{https://github.com/danielinux7/Multilingual-Parallel-Corpus}},
  note = {Accessed: 2023-03-24}
}
@misc{chuvashcorpus,
  title = {{Chuvash-Russian parallel corpus}},
  author = {{ Antonov, Alexander}},
  year = {{2022}},
  howpublished = {\url{https://github.com/AlAntonov/chv_corpus}},
}
@inproceedings{bashkircorpus,
title={Bashkir-Russian parallel corpora},
author={Shakirov, Iskander and Kunafin, Aigiz},
  howpublished = {\url{https://huggingface.co/datasets/AigizK/bashkir-russian-parallel-corpora}},
year={2023}
}

@article{tatoeba,
  author    = {J{\"{o}}rg Tiedemann},
  title     = {The Tatoeba Translation Challenge - Realistic Data Sets for Low Resource
               and Multilingual {MT}},
  journal   = {CoRR},
  volume    = {abs/2010.06354},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.06354},
  eprinttype = {arXiv},
  eprint    = {2010.06354},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-06354.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{conneau2017word,
  title={Word Translation Without Parallel Data},
  author={Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1710.04087},
  year={2017}
}

@article{lample2017unsupervised,
  title={Unsupervised Machine Translation Using Monolingual Corpora Only},
  author={Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1711.00043},
  year={2017}
}



% DAAN'S refs

% word by word BT
% https://openreview.net/pdf?id=B1ecYsqSuN


% BERT finetuning
% use bilex to initialize embeddings
@article{DBLP:journals/corr/abs-2002-07306,
  author    = {Ke M. Tran},
  title     = {From English To Foreign Languages: Transferring Pre-trained Language
               Models},
  journal   = {CoRR},
  volume    = {abs/2002.07306},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.07306},
  eprinttype = {arXiv},
  eprint    = {2002.07306},
  timestamp = {Sun, 22 Mar 2020 17:29:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-07306.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% done
% similar to glowup: 
%  injects terminology constraints at inference time without any impact on decoding speed. 
@article{susanto2020lexically,
  author    = {Raymond Hendy Susanto and
               Shamil Chollampatt and
               Liling Tan},
  title     = {Lexically Constrained Neural Machine Translation with Levenshtein
               Transformer},
  journal   = {CoRR},
  volume    = {abs/2004.12681},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.12681},
  eprinttype = {arXiv},
  eprint    = {2004.12681},
  timestamp = {Wed, 29 Apr 2020 10:17:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-12681.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% codeswitching parallel
@article{xia2019generalized,
  author    = {Mengzhou Xia and
               Xiang Kong and
               Antonios Anastasopoulos and
               Graham Neubig},
  title     = {Generalized Data Augmentation for Low-Resource Translation},
  journal   = {CoRR},
  volume    = {abs/1906.03785},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.03785},
  eprinttype = {arXiv},
  eprint    = {1906.03785},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-03785.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% glowup-style dictionary lookup attachments
@article{xing2020look,
  author    = {Xing Jie Zhong and
               David Chiang},
  title     = {Look It Up: Bilingual and Monolingual Dictionaries Improve Neural
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2010.05997},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.05997},
  eprinttype = {arXiv},
  eprint    = {2010.05997},
  timestamp = {Thu, 22 Oct 2020 13:52:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-05997.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% done
% Annotate source phrase
@article{niehues2021continuous,
  author    = {Jan Niehues},
  title     = {Continuous Learning in Neural Machine Translation using Bilingual
               Dictionaries},
  journal   = {CoRR},
  volume    = {abs/2102.06558},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.06558},
  eprinttype = {arXiv},
  eprint    = {2102.06558},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-06558.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% done
% codeswitching
@article{malon2021overcoming,
  author    = {Christopher Malon},
  title     = {Overcoming Poor Word Embeddings with Word Definitions},
  journal   = {CoRR},
  volume    = {abs/2103.03842},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.03842},
  eprinttype = {arXiv},
  eprint    = {2103.03842},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-03842.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% done
% codeswitching
@article{lin2021bilingual,
  author    = {Yusen Lin and
               Jiayong Lin and
               Shuaicheng Zhang and
               Haoying Dai},
  title     = {Bilingual Dictionary-based Language Model Pretraining for Neural Machine
               Translation},
  journal   = {CoRR},
  volume    = {abs/2103.07040},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.07040},
  eprinttype = {arXiv},
  eprint    = {2103.07040},
  timestamp = {Tue, 23 Mar 2021 16:29:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-07040.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% done
% codeswitching
@article{kuwanto2021low,
  author    = {Garry Kuwanto and
               Afra Feyza Aky{\"{u}}rek and
               Isidora Chara Tourni and
               Siyang Li and
               Derry Wijaya},
  title     = {Low-Resource Machine Translation for Low-Resource Languages: Leveraging
               Comparable Data, Code-Switching and Compute Resources},
  journal   = {CoRR},
  volume    = {abs/2103.13272},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.13272},
  eprinttype = {arXiv},
  eprint    = {2103.13272},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13272.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% done
% Glowup for BERT pretraining
@article{yu2021dictbert,
  author    = {Wenhao Yu and
               Chenguang Zhu and
               Yuwei Fang and
               Donghan Yu and
               Shuohang Wang and
               Yichong Xu and
               Michael Zeng and
               Meng Jiang},
  title     = {Dict-BERT: Enhancing Language Model Pre-training with Dictionary},
  journal   = {CoRR},
  volume    = {abs/2110.06490},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.06490},
  eprinttype = {arXiv},
  eprint    = {2110.06490},
  timestamp = {Fri, 13 May 2022 11:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-06490.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% done
% codeswitch for UNMT?
@article{yu2021simple,
  author    = {Jyotsana Khatri and
  Rudra Murthy and
  Tamali Banerjee and
  Pushpak Bhattacharyya },
  title     = {Simple measures of bridging lexical divergence help unsupervised neural machine translation for low-resource languages.},
  year      = {2021},
  url       = {https://link.springer.com/article/10.1007/s10590-021-09292-y},
}


% done
% codeswitch (tho many are only to the target lang?)
@misc{kumar2022dictnmt,
      title={Dict-NMT: Bilingual Dictionary based NMT for Extremely Low Resource Languages}, 
      author={Nalin Kumar and Deepak Kumar and Subhankar Mishra},
      year={2022},
      eprint={2206.04439},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% pseudo-BT with bilex
@article{nag2020incorporating,
  author    = {Sreyashi Nag and
               Mihir Kale and
               Varun Lakshminarasimhan and
               Swapnil Singhavi},
  title     = {Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised
               Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2004.02071},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.02071},
  eprinttype = {arXiv},
  eprint    = {2004.02071},
  timestamp = {Wed, 08 Apr 2020 17:08:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-02071.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}