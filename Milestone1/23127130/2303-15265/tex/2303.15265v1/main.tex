\documentclass[table, usenames, dvipsnames]{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}


\usepackage{bbm}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{hyperref}
\usepackage{multirow, makecell}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[multiple]{footmisc}
\usepackage[shortlabels]{enumitem}
\usepackage{xltabular}
\usepackage{tabularx}
\usepackage{array}
\usepackage{float}
\usepackage{anyfontsize}
\usepackage{xcolor}
\usepackage{collcell,xfp}
\usepackage{xstring}
\usepackage{tikz}
\usepackage{tipa}
\usepackage[english, russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{scalerel}
% \usepackage[T2A, T1]{fontenc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is a hacky bugfix to resolve an issue caused by booktabs being used together with xltabular. Source: https://tex.stackexchange.com/questions/522920/xltabular-breaking-booktabs% /522927#522927. 
% The bug caused rule lines (toprule, midrule, bottomrule) to be truncated rather than spanning across the entire table. DO NOT TOUCH THIS UNLESS YOU KNOW A BETTER FIX!!!
\makeatletter
\def\@BTrule[#1]{%
  \ifx\longtable\undefined
    \let\@BTswitch\@BTnormal
  \else\ifx\hline\LT@hline
    \nobreak
    \let\@BTswitch\@BLTrule
  \else
     \let\@BTswitch\@BTnormal
  \fi\fi
  \global\@thisrulewidth=#1\relax
  \ifnum\@thisruleclass=\tw@\vskip\@aboverulesep\else
  \ifnum\@lastruleclass=\z@\vskip\@aboverulesep\else
  \ifnum\@lastruleclass=\@ne\vskip\doublerulesep\fi\fi\fi
  \@BTswitch}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\input{tables}
\input{tables_appendix}


% Isaac included these:
%\usepackage{arydshln}
\usepackage{dashrule}
\usepackage{stackrel}
\usepackage[T4,T1]{fontenc}
\usepackage{subcaption}

\newcommand{\AS}[1]{{\fontencoding{T4}\selectfont#1}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}


\usepackage{colortbl}

\definecolor{lgreen}{RGB}{73,174,137}
\definecolor{lred}{RGB}{182,49,54}
\definecolor{lorange}{RGB}{255, 128, 0}
\definecolor{lblue}{RGB}{0, 0, 255}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
% \colorlet{lorange1}{lorange!20}

\colorlet{scale10}{lgreen!100}
\colorlet{scale9}{lgreen!60}
\colorlet{scale8}{lgreen!40}
\colorlet{scale7}{lgreen!20}
\colorlet{scale6}{lgreen!10}
\colorlet{scale5}{lred!5}
\colorlet{scale4}{lred!20}
\colorlet{scale3}{lred!40}
\colorlet{scale2}{lred!60}
\colorlet{scale1}{lred!75}

\colorlet{orangescale0}{lorange!0}
\colorlet{orangescale1}{lorange!10}
\colorlet{orangescale2}{lorange!20}
\colorlet{orangescale3}{lorange!30}
\colorlet{orangescale4}{lorange!40}
\colorlet{orangescale5}{lorange!50}
\colorlet{orangescale6}{lorange!60}
\colorlet{orangescale7}{lorange!70}
\colorlet{orangescale8}{lorange!80}
\colorlet{orangescale9}{lorange!90}
\colorlet{orangescale10}{lorange!100}

\colorlet{bluescale0}{lblue!0}
\colorlet{bluescale1}{lblue!10}
\colorlet{bluescale2}{lblue!20}
\colorlet{bluescale3}{lblue!30}
\colorlet{bluescale4}{lblue!40}
\colorlet{bluescale5}{lblue!50}
\colorlet{bluescale6}{lblue!60}
\colorlet{bluescale7}{lblue!70}
\colorlet{bluescale8}{lblue!80}
\colorlet{bluescale9}{lblue!90}
\colorlet{bluescale10}{lblue!100}

\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}

\newcommand{\enxx}{en$\rightarrow$xx}
\newcommand{\xxen}{xx$\rightarrow$en}

\newcommand{\iccomment}[1]{{\hphantom{}\color{lorange!100}  #1}}
\newcommand{\alexcomment}[1]{{\hphantom{}\color{lred!100}  #1}}

\newcommand{\bleu}{\textsc{Bleu}}
\newcommand{\chrf}{\textsc{ChrF}}
\newcommand{\flores}{\textsc{Flores-200}}
\newcommand{\ntlevalset}{\textsc{Gatones}}
\newcommand{\gatitos}{\textsc{Gatitos}}

\makeatletter
\newcommand{\myfnsymbol}[1]{%
  \expandafter\@myfnsymbol\csname c@#1\endcsname
}
% Mapping of how the \thanks symbols will be interpreted sequentially
\newcommand{\@myfnsymbol}[1]{%
  \ifcase #1
    % 0
  \or 1% 1
  \or 2% 2
  \or \TextOrMath{\textasteriskcentered}{*}% 3
  \or \TextOrMath{\textdagger}{\dagger}% 4
  \fi
}
% Just to make things explicit in the code what it means
\newcommand{\equalcontributor}{\@myfnsymbol{3}}
\newcommand{\correspondingA}{\@myfnsymbol{4}}
\makeatother

\renewcommand{\thefootnote}{\fnsymbol{footnote}}


\title{\centering \LARGE BiLex R$_{\textnormal x}$: \textnormal{Lexical Data Augmentation}  \protect\\  \textnormal{for Massively Multilingual Machine Translation}}

\author{\hspace{24mm}
Alex Jones\textsuperscript{\equalcontributor, \correspondingA}, Isaac Caswell\textsuperscript{\equalcontributor}, Ishank Saxena, Orhan Firat \\
\hspace{57mm} Google Research \\
}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\selectlanguage{english}
\maketitle

\footnotetext[1]{Equal contribution. Correspondence to \texttt{icaswell@google.com, alexjones1925@gmail.com}.}
\footnotetext[2]{Work done while interning on the Translate team at Google.}

\renewcommand{\thefootnote}{\myfnsymbol{footnote}}

\makeatletter
\def\nomarkfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother


\begin{abstract}

Neural machine translation (NMT) has progressed rapidly over the past several years, and modern models are able to achieve relatively high quality using only monolingual text data, an approach dubbed Unsupervised Machine Translation (UNMT). However, these models still struggle in a variety of ways, including aspects of translation that for a human are the easiest---for instance, correctly translating common nouns. This work explores a cheap and abundant resource to combat this problem: bilingual lexica (\textsc{BiLex}s). We test the efficacy of bilingual lexica in a real-world set-up, on 200-language translation models trained on web-crawled text. We present several findings:  (1) using lexical data augmentation, we demonstrate sizable performance gains for unsupervised translation; (2) we compare several families of data augmentation, demonstrating that they yield similar improvements, and can be combined for even greater improvements; (3) we demonstrate the importance of carefully curated lexica over larger, noisier ones, especially with larger models; and (4) we compare the efficacy of multilingual lexicon data versus human-translated parallel data. Finally, we open-source \gatitos{}\footnote{\url{https://github.com/google-research/url-nlp/tree/main/gatitos}}, a new multilingual lexicon for 26 low-resource languages, which had the highest performance among lexica in our experiments.

\end{abstract}

\section{Introduction}

Neural machine translation (NMT) has emerged as the dominant way of training machine translation models \citep{bahdanau-etal-2015-neural}, where translation is modeled as a sequence-to-sequence task to be learned by neural networks \citep{sutskever-etal-2014-sequence}. Massively multilingual machine translation (MMMT) refers to the concept of training a single machine translation model on many languages and language pairs using a shared set of parameters, and has also seen success in recent years \citep{firat-etal-2016-multi, wu-etal-2016-google, johnson-etal-2017-googles, aharoni-etal-2019-massively, m2m-100, nllb2022, bapna-etal-2022-building, siddhant-etal-2022-towards}. Training these models typically relies on large-scale parallel corpora mined from the web \citep{resnik-smith-2003-web, uszkoreit-etal-2010-large, espla-gomis-2009-bitextor, banon-etal-2020-paracrawl}.

However, beyond the traditional technique of training NMT models with human-translated parallel texts, a number of other strategies have shown success recently, especially on lower-resource languages. One of these techniques is self-supervised training using monolingual corpora \citep{siddhant-etal-2020-leveraging, cheng-etal-2021-self}. With this approach, NMT models are pretrained or jointly trained on a self-supervised task with monolingual data, such as the MASS \citep{song-etal-2019-mass} or BART \citep{lewis-etal-2020-bart, liu-etal-2020-multilingual-denoising} tasks, as well as the usual neural machine translation task. This training regime can aid the model in performing zero-shot translation \citep{bapna-etal-2022-building, siddhant-etal-2022-towards}, in cases where a language has monolingual data but no parallel data. Moreover, both the self-supervised task and the supervised MT task can be modeled as neural sequence-to-sequence (Seq2Seq) problems, meaning a single Seq2Seq model can be used for training on both tasks.

Another technique that has shown success in training MMMT models is bitext mining \citep{artetxe-etal-2018-massively, schwenk-etal-2021-wikimatrix, schwenk-etal-2021-ccmatrix, heffernan-etal-2022-bitext}, in which parallel or nearly-parallel sentence pairs are mined from sets of monolingual corpora using a cross-lingual sentence encoder that has been trained to generate language-agnostic embeddings of sentences in different languages. This approach has proven useful in expanding MT models to hundreds more languages and thousands of language pairs \citep{m2m-100, nllb2022}.

Other techniques that have proven useful for low-resource MT include back-translation \citep{sennrich-etal-2016-improving, caswell-etal-2019-tagged, feldman-coto-solano-2020-neural} and the incorporation of language models into MT training \citep{gulcehre-etal-2017-on, baziotis-etal-2020-language, freitag-etal-2022-natural}. There has also been extensive work on training completely unsupervised MT systems using monolingual corpora only \citep{artetxe-etal-2017-unsupervised, artetxe-etal-2019-effective}. For example, \citet{artetxe-etal-2017-unsupervised} uses a combination of denoising autoencoding with pretrained cross-lingual embeddings and on-the-fly back-translation to achieve reasonable MT performance with zero parallel data.

In our work, we supplement the approach that combines supervised and self-supervised training with multilingual lexica. The motivation for using this resource is as follows. Despite the successes of the approach combining supervised and self-supervised training, cross-lingual vocabulary alignment is still highly imperfect in these models, especially for low-resource and unsupervised languages (see \citet{bapna-etal-2022-building} for examples of some common failure modes). That is, training on all languages using a shared set of parameters is insufficient to induce perfect cross-lingual vocabulary alignment. And although multilingual lexica generally do not contain sentence-level parallel information, they can greatly extend cross-lingual vocabulary coverage at the word or phrase level by providing examples of word or phrase translations. (Of course, we are not the first to experiment with using multilingual lexica to improve NMT performance, or multilingual NLP applications more generally; Section \ref{sec:related-work} gives more details.) 

In our approach, we experiment with augmenting monolingual and parallel data with translations from multilingual lexica. Using the publicly available massively multilingual lexicon Panlex \citep{kamholz-etal-2014-panlex}, we demonstrate that this added lexical data leads to small but significant gains over a baseline model on average, even for high-resource languages; and with smaller but carefully curated bilingual lexica, the gains are substantially larger. In both cases, the gains are most significant for unsupervised and low-resource languages. Our contributions are as follows:

\begin{enumerate}[nosep]
    \item We provide a thorough comparison of several lexicon-based data augmentation variants for MT, all of which are simple, generalizable, and easy to implement;
    \item We test these approaches ``in the wild'', i.e. on in a highly multilingual, web-mined data regime such as production systems tend to use, with hundreds of languages and billions of monolingual and parallel sentences;
    \item We explore the effects of lexical data quality \textit{and} quantity;
    \item We demonstrate the efficacy of bilingual lexicon-based approaches as models scale in size;
    \item We open-source the high-quality multilingual \gatitos{} lexicon for low-resource languages.
\end{enumerate}

The \textbf{tl;dr} of this paper is that bilingual lexica help low-resource and zero-shot NMT in almost all cases, and that most training-time augmentation methods have similar efficacy, and can be combined to be more effective. When scaling up to larger and more expressive models, these methods retain their efficacy, but the quality of the translated bilingual lexica becomes more important than the sheer quantity of lexical data points used for data augmentation. For instance, small, high-quality lexica like \gatitos{} show about 5x larger \chrf{} improvement than larger, noisier lexica like Panlex.

\section{Related Work}\label{sec:related-work}

A number of works have looked at using multilingual lexicon data augmentation for NMT and other NLP tasks. The first class of augmentations that we experiment with is ``codeswitching,'' where words in the source sentence are swapped out for their dictionary translations to create mixed-language sentences. This approach has been used for a range of multilingual NLP tasks, including MT \citep{reid-artetxe-2022-paradise, yang-etal-2020-csp, liu-etal-2021-continual, lin-etal-2020-pre, lin2021bilingual, pan-etal-2021-contrastive, yang-etal-2021-multilingual, kumar2022dictnmt, yu2021simple, kuwanto2021low, xia2019generalized}. Many of these, however, only look at codeswitching between the source and target languages, e.g. substituting source words with dictionary translations into the target language, or word-for-word BiLex translations of the target to make synthetic back-translated data \citep{nag2020incorporating}. \citet{ijcai2020p0533} experiment with codeswitching on NLI, sentiment classification, document classification, dialogue state tracking, and spoken language understanding, \citet{malon2021overcoming} looks at codeswitching embeddings for language modeling, and \citet{wang-etal-2022-expanding} experiment on NER, POS tagging, and dependency parsing. Another similar work is \citet{chaudhary-etal-2020-dict}, in which the MLM task is modified such that instead of predicting masked source tokens in the \textit{source} language, the authors provided language embeddings to cue the model to predict the masked tokens in a \textit{different} language instead. Codeswitching augmentations go by a variety of different names, e.g. ``dictionary denoising'' \citep{reid-artetxe-2022-paradise}, ``Random Aligned Substitution'' \citep{lin-etal-2020-pre}, or ``code-mixing''. In our paper, we will stick to the term ``codeswitching,'' though we will try to point out where an identical or similar approach has been tried under a different name.

The second class of augmentations we experiment with involves prepending lexicon translations to source sentences as additional cross-lingual signal, as instead of swapping out words in the source sentence. This approach has been tried as well for enhancing MT performance, e.g. in \citet{song-etal-2019-code, maheshwari-etal-2022-dictdis, niehues2021continuous, michon-etal-2020-integrating, xing2020look, susanto2020lexically}, and for similar tasks like language modeling \citep{yu2021dictbert}. One potential advantage this approach has over the codeswitching method is that it can be applied at inference time as well: multilingual lexicon entries can be prepended to sentence queries to steer the model toward more accurate word translations. Outside of NMT models, this lexical prompting approach has also been applied to translation with LLMs: \citet{dipmt2023} provide LLMs with dictionary translations of some of the source sentence words, which the model can use to cover gaps in its vocabulary coverage (although the authors do not experiment with truly low-resource languages). With the rise in popularity of LLMs for MT and other tasks, this is an exciting area for further research.


\section{The Problem with UNMT}

Various automatic metrics like \bleu{} and \chrf{} have demonstrated that Unsupervised Machine Translation can produce results of relatively high quality. However, such metrics hide the fact that UNMT approaches tend to make very specific and unusual mistakes. \citet{bapna-etal-2022-building} demonstrate that unsupervised models are remarkably good at aspects of translation like fluency, but frequently make mistakes by confusing distributionally similar words. When training on parallel data, it is straightforward enough to learn a mapping from a token in one language to another; when training on monolingual data only, this mapping can only be inferred from considerable amounts of context, necessitating a very fine-grained representation of its meaning, and consequently large amounts of monolingual data. This issue is especially noticeable with nouns that fall into the same semantic category, like animal names. For instance, \citet{bapna-etal-2022-building} demonstrate how their models, despite getting good \chrf{} scores, translate `lion' as `hyena', `snake', `rabbit', and `seizures', depending on the language.

The interesting thing about these errors is that in some sense they are the easiest to fix. Any student of a language with a pocket dictionary handy could correct many of these mistakes.
And even a traditional machine translation system -- especially a phrase-based one -- would rarely if ever mistranslate ``cat'' as ``dog''. This error mode is the primary motivation for incorporating bilingual lexica into translation systems. For this reason, this paper focuses on unsupervised and slightly supervised (low-resource) languages.

\insertgroundingmistakes



\section{Training data}

Our models were trained entirely on web-mined, sentence-level text data.

\subsection{Monolingual data} \label{sec:mono-data}

The models used in this paper build off the existing data used by \citet{bapna-etal-2022-building}, which in turn uses the specialized low-resource data crawling techniques described in \citet{caswell2020language}. However, to make rapid training and development possible, we subsampled the monolingual data for the 100 highest-resource languages to 10\% of its original size. In sum, this totaled about 4B sentences, or about 80B tokens. For the large model experiments in section \ref{sec:bigger_models} of the Appendix, we used the full (not subsampled) data, totalling 27B sentences (540B tokens). All 208 languages in our experiments had monolingual data.

\subsection{Parallel data} \label{parallel-data}

The parallel data is also the same as that from \citet{bapna-etal-2022-building}, which itself is a slightly extended version of the corpus described in \citet{arivazhagan2019massively}. The present paper extends it slightly further with the addition of small amounts of parallel data for some lower-resource languages (on the order of 1,000 - 5,000 website-template tokens). The parallel data tend to be much noisier than the monolingual data, and misalignments, boilerplate, and nonlinguistic content are common. All parallel data are sampled to 10\% of their original size, resulting in 9B parallel sentences (162M tokens) into English and the same number out of English, as well as 700K non-English-centric sentence-pairs. The large models in Section \ref{sec:bigger_models} use the full dataset, which is ten times larger than this. 

\subsection{Multilingual lexica} \label{lex-data}

\subsection{\gatitos{}} \label{gatitos}

The \gatitos{} dataset is a new dataset open-sourced in this paper. It consists of 4000 short English segments translated into a 26 very low-resource languages. The English source text is a mixture of words from a variety of sources, including frequent tokens in the English language, words for numbers, months, days of the week, Swadesh words, names of the languages themselves (including the endonym), and a few short sentences. The tokens were manually reviewed by the authors of this paper to ensure they looked reasonable\footnote{Exactly two tokens snuck into the dataset that are definitively odd, namely `simp' and `sh'}. As the name implies, this dataset is mostly very short, consisting of 93\% single tokens. There are also some short sentences, though only 23 entries have over 5 tokens. We hope this dataset will complement existing publicly available multilingual lexicons like MUSE \citep{conneau2017word, lample2017unsupervised}

\subsection{Panlex} \label{panlex}

Panlex \citep{kamholz-etal-2014-panlex} is a free, open-access massive online database consisting of word and phrase translations for $5000+$ languages, sourced from $2500$ individual dictionaries. Panlex contains $\approx 1.3$B translations across all language pairs. For our experiments, we use a subset of the Panlex database covering 177 languages and containing $66$M word pairs. Languages were chosen largely by the availability of eval sets; details given in Appendix \ref{appendix:language-rationale}.



\section{Evaluation} \label{eval}

We use two translation evaluation sets: \flores{} \citep{nllb2022, flores101, guzman-etal-2019-two}, an open-sourced evaluation set consisting of 2009 English Wikipedia sentences translated by humans into 200 languages, and \ntlevalset{} (Google AuTOmatic NTL Eval Set), an in-house evaluation set of 1,200 English sentences translated into various languages, for comparison with \citet{bapna-etal-2022-building}. We use the SacreBLEU \citep{post-2018-call} implementation of \chrf{} \footnote{signature \texttt{nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.3.1}} for our evaluation metric. Higher-quality, embedding-based metrics like \textsc{Bleurt} \citep{sellam-2020-bleurt} are not available for these languages, and token-based metrics like \bleu{} are especially questionable on highly-inflecting languages, as explored in \citet{bapna-etal-2022-building} Furthermore, \chrf{} has generally been found to correlate better with human judgement than \bleu{}, even for high-resource languages \citep{kocmi-etal-2021-ship,freitag-2022-stop}


For this study, we only evaluate on English-centric directions. The reason for this is that, although both evaluation sets are multi-way parallel, they are both also English-original. Therefore, using either of them for non-English-centric evaluation is not especially meaningful -- and specifically, such an evaluation set cannot measure any improvement that a direct model would have over a pivoting approach (first translating to English, and then to the target). For example, any sentence with ``you'' in the English side erases the formality and gender distinctions that either of the other languages may have, so if a direct model is able to correctly preserve formality, the eval set can't measure this. 

Furthermore, this study places more weight on the \enxx{} direction than the \xxen{} direction. The main reason for this is that the \xxen{} direction is generally an easier direction for models to learn (since they see so much more English text), so the \enxx{} direction is usually the limiting reagent when it comes to model quality; as a result we care more about improving this direction. Similarly, the smaller models trained in this study will lag larger models much more on the \xxen{} direction, so our results in this direction are not as meaningful.




\section{Model} \label{model}

For our experiments we use a Transformer Big encoder-decoder model \citep{vaswani-etal-2017-attention} with approximately $475$M parameters. We train each model for 400K steps on 64 TPU v2 chips. Our models assign a 40\% weight to the translation task, and a 60\% weight to the MASS task. For models augmented with a monolingual data augmentation, we split the 60\% weight on the MASS task into a 30\% weight on the augmented data and a 30\% weight on the non-augmented data. Parallel data augmentations were done in an analogous way.  For the raw-token-pair augmentation, we add in this task with a 5\% weight and shrink the other weights accordingly. We use a task-specific token for each of the six tasks the models may see, namely translation, MASS, GlowupMono, GlowupParallel, CodeswitchMono, and CodeswitchParallel.

\section{Methods} \label{methods}

In this paper, we divide our augmentation approaches into two classes: ``codeswitching'' approaches, which involve substituting source sentence words for their dictionary translations, and ``\texttt{GLOWUP},'' (Guiding Lexical Output With Understandable Prompts) which entails prepending dictionary translations of source words to source sentences. The main difference between these approaches is whether dictionary translations are substituted for source text (in the case of codeswitching) or added to the sentence (in the case of \texttt{GLOWUP}). As a third augmentation, we experiment with training on raw lexicon token pairs directly, treating them like any other parallel data.

The novelty of our contribution lies not so much in any one of our methods, but rather in (1) the application of these methods to unsupervised Machine Translation; (2) the number of methods we apply in controlled experiments to discover the best augmentation, (3) the scale of our experiments, in terms of number of languages, data quantity, and model capacity; and (4) the application of these methods to ``in the wild'' web-crawled data. While a variety of papers (e.g. \citet{reid-artetxe-2022-paradise, yang-etal-2020-csp}) have explored specific augmentations on particular language pairs, we believe our paper is the first to undertake a rigorous comparison of different augmentation strategies across hundreds of languages in a real-world setting.

\subsection{Codeswitching} \label{codeswitching}

In our ``codeswitching'' augmentation strategy, words in the source sentence are swapped out for their dictionary translations to create mixed-language sentences. We experiment with this augmentation on both monolingual and parallel data. The details of this method are described below.

\subsubsection{Multilingual codeswitching autoencoding (MCA)} \label{mca}

Our multilingual codeswitching autoencoding (MCA) approach is similar to the ``dictionary denoising'' objective in \citet{reid-artetxe-2022-paradise}. Let $D$ represent a multilingual lexicon containing word or phrase translation pairs for many languages. Given a source sentence $x = (x_{1}, x_{2}, . . ., x_{n})$ from monolingual corpus $X_{mono}$, we substitute each token in $x$ for its dictionary translation with probability $p_{tr} = 0.4$.  (More implementation details can be seen in Appendix Section \ref{token-sampling})

Note that \citet{reid-artetxe-2022-paradise} also apply additional noise to $x$ on top of codeswitching, along the lines of (m)BART \citep{lewis-etal-2020-bart, liu-etal-2020-multilingual-denoising}. For simplicity and so we can better examine the effects of lexicon information in isolation, we do not do this. Furthermore, while MCA indeed creates ``noisy'' codeswitched sentences, we note that word translations provide a meaningful cross-lingual signal for the model in a way that other noising functions (e.g. deletion, random word substitution) do not, as these augmentations corrupt the source sentence without adding additional useful information. Although models have been shown to learn from these denoising tasks, we use only lexical augmentation because it more directly furthers our ends of improving cross-lingual vocabulary alignment across many languages.

It should also be noted that because we are choosing dictionary translations at random without attempting to do any sort of word sense disambiguation beforehand, some of the translations will actually express the wrong sense of the source word (e.g. translating the verb ``fool'' as ``sot'' in French, even though the correct translation should be ``tromper''). We do not try to avoid this phenomenon because we believe this is an acceptable type of source-side noise, and may actually \textit{help} our models learn to perform word sense disambiguation implicitly.

\subsubsection{Codeswitching MT} \label{codeswitch-parallel}

Our codeswitching MT task is essentially the same approach as described in Section \ref{mca}, except it applies to parallel rather than monolingual data. Given a source sentence $x$ from parallel corpus $X_{parallel}$, we perform the identical procedure described in section Section \ref{mca} to obtain multilingual codeswitched sentence $x'$. We then train the model on the translation task using sentence pairs $(x', y)$, where $(x, y)$ is a sentence pair in $X_{parallel}$. This method is effectively identical to the Random Aligned Substitution method proposed in \citet{lin-etal-2020-pre}. As with MCA, we use $p_{tr} = 0.4$ and apply the augmentation on half the available parallel data.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{codeswitch_examples_v2.png}
    \caption{Examples of the monolingual (top) and parallel (bottom) codeswitching augmentation strategies. In both cases, random tokens in the source sentence are replaced with their translations in a random language from a multilingual lexicon. Color coding is used to indicate which source words have been swapped for their dictionary translation. The different colors are used simply to point out the fact that the words are from codeswitched words in each sentence come from different languages.}
    \label{fig:codeswitch_examples}
\end{figure}

\subsection{Lexical prompting (GLOWUP)}

The second class of lexical augmentations we experiment with is lexical prompting, which we call \texttt{GLOWUP}, short for Guiding Lexical Outputs With Understandable Prompts. The difference between these approaches and the codeswitching approaches described in Section \ref{codeswitching} is that instead of substituting source sentence words for their dictionary translations, we prepend those translations to the source sentences. In particular, we add $(src, transl)$ pairs to the beginning of source sentence, for some random fraction of the translatable words in that sentence. These hints can then be used to help the model guess the translation or the denoised sentence, depending on the flavor, as described below.

The \texttt{GLOWUP} task has the advantage that it can be used at inference time, can be used without retraining a model, and may be simpler to implement. However, it does result in longer and less balanced sequence lengths, which can pose problems for decoding.


\subsubsection{MASS with monolingual GLOWUP} \label{glowup-mono}

Our \texttt{GLOWUP} augmentation on monolingual data involves prepending lexical prompts to source sentences and then applying MASS on top of those sentences, after which the model attempts to reconstruct the original source sentence (including the prompts). Given multilingual lexicon $D$ and source sentence $x$, we sample some number of translatable tokens in $x$ as in the codeswitching augmentation. However, instead of substituting these tokens for their translations, we instead prepend these tokens and their translations to $x$. These $\{src, tgt\}$ translation pairs then form a ``\texttt{GLOWUP} prompt'' that is separated from the source sentence by indicator tokens. Unlike the codeswitching augmentation, we sample from a uniform distribution over $[0, k]$, where $k$ is the number of translatable tokens in $x$, when deciding how many tokens to translate. The reasons for this are described in Section \ref{glowup-mt}.

Finally, we apply MASS to mask random subsequences of $x'$, possibly including the \texttt{GLOWUP} prompt itself. We then train the model to reconstruct $x$. As with MCA, we apply the augmentation on half the monolingual data and train the rest on vanilla MASS.

\subsubsection{\texttt{GLOWUP}-MT} \label{glowup-mt}

Our \texttt{GLOWUP} augmentation on parallel data, which we call \texttt{GLOWUP}-MT, is effectively the same as the monolingual variant of the task, but without the MASS element. For a given sentence pair $(x, y)$ in the training corpus, the prompting is performed on the source sentence $x$, essentially to give it hints about how to produce $y$. The model is then trained on the translation task using $(x', y)$, with the task token \texttt{<2glowup>} instead of \texttt{<2translation>} 

One notable advantage that the \texttt{GLOWUP}-MT augmentation has over the codeswitching MT augmentation is that the \texttt{GLOWUP} variant may be applied at inference time as well. That is, given an unseen sentence, we can append source words and their translations by using a multilingual lexicon at inference time. For this reason, we opt to add word translations in the target language only, unlike the other augmentations discussed where we have added translations in multiple, random languages per sentence.


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{glowup_examples_v3.png}
    \caption{Examples of the monolingual (top) and parallel (bottom) \texttt{GLOWUP} augmentation strategies. In both cases, random tokens in the source sentence are prepended to the source sentence, along with their translations in a random language from a multilingual lexicon. As in Figure \ref{fig:codeswitch_examples}, differential color coding is done to draw attention to dictionary translations in different languages.}
    \label{fig:glowup_examples}
\end{figure}

\section{Experiments}

\subsection{Training regimes}

In our experiments, we train models with various combinations of the augmentations outlined above, as well as a baseline (with no data augmentation of any kind) and a model where we simply provide word pairs from the lexicon as additional parallel data. The details of our training regimes are discussed below.

\subsubsection{Baseline} \label{baseline}

We first train a baseline model with no data augmentation using the monolingual and parallel data described in Sections \ref{sec:mono-data} and \ref{parallel-data}, respectively. This model is essentially a smaller version of the model trained in \citet{bapna-etal-2022-building}. Using the model setup described in Section \ref{model}, we train the model on the MT task using all available parallel data and on the MASS task using all available monolingual data. Note that because this model is smaller and uses substantially less data than the one in \citet{bapna-etal-2022-building}, the performance is predictably not as high. We use this smaller model for the purposes of running experiments more quickly and efficiently, although we understand the relative gains of our approaches may not be identical with those of a larger model. For the same reasons, we also do not perform back-translation as done in \citet{bapna-etal-2022-building}. Larger models, which have $1.6$B parameters and are trained on substantially more parallel and monolingual data, are explored in Section \ref{sec:bigger_models}.

\subsubsection{Token-pair-only model (GatiPanlexTokenPairs)} \label{GatiPanlex}

In addition to the baseline model, we also experiment with the extremely simple approach of providing raw word pairs from multilingual lexica to the model as additional parallel data. That is, given a dictionary entry $s$ and its translation $t$, we provide the model with a ``sentence'' pair of the form $($\texttt{<2translation> <2lang> <2script>} $s, t)$. We use all $66$M token pairs from the Panlex subset described in Section \ref{panlex}, which covers $177$ of the $208$ languages present in the monolingual and parallel data, as well as all 400K token pairs from the \gatitos{} dataset described in section \ref{gatitos}, covering 26 languages. We call this token-pair baseline GatiPanlexTokenPairs.

\subsubsection{Single augmentation models}

We also train models on each of the augmentations described in Section \ref{methods}. As noted in that section, we only augment half the relevant data (monolingual, parallel, or both) before training each of these models, leaving the other half to be trained identically to the baseline (i.e. joint training on the MT task and MASS). Our naming conventions for our models are as follows:
\begin{enumerate}[nosep]
    \item \textbf{CodeswitchMono} is augmented with Monolingual Codeswitching (Section \ref{mca})
    \item \textbf{CodeswitchParallel} is augmented with Parallel Codeswitching (Section \ref{codeswitch-parallel})
    \item \textbf{GlowupMono} is augmented with Monolingual \texttt{GLOWUP} (Section \ref{glowup-mono})
    \item \textbf{GlowupParallel} is augmented with Parallel \texttt{GLOWUP} (Section \ref{glowup-mt})

\end{enumerate}

\subsubsection{Dual augmentation models}

We also train some models on a combination of data augmentation approaches to see whether a hybrid strategy may work better than any single augmentation. We train the following additional models:
\begin{enumerate}[nosep]

    \item \textbf{CodeswitchMonoParallel} and \textbf{GlowupMonoParallel} are identical to the CodeswitchMono and GlowupMono models, but also add the corresponding parallel augmentations.
    \item \textbf{CodeswitchMonoParallelGatiPanlex} and \textbf{GlowupMonoParallelGatiPanlex} take these models and further add the raw token pair objective (see Section \ref{GatiPanlex}).
 
\end{enumerate}


We leave experimentation with a hybrid codeswitch-\texttt{GLOWUP} approach (e.g. CodeswitchMonoGlowupMono) for future work.

\section{Results}

We evaluate all our models on the \flores{} dataset \citep{nllb2022, flores101, guzman-etal-2019-two}, which contains English-aligned parallel sentences for $200$ languages. In Appendix section \ref{appendix:gatones} we also report scores on the in-house \ntlevalset{} eval set, which is the eval set used by \citet{bapna-etal-2022-building}.

In analyzing our results across languages, we use the following resourcedness classifications:

\begin{enumerate}[nosep]
    \item \underline{H}igh-\underline{R}esource \underline{L}anguages (HRLs): $>2$B total training tokens in parallel data
    \item \underline{M}edium-\underline{R}esource \underline{L}anguages (MRLs): $360$M to $2$B training tokens in parallel data
    \item \underline{L}ow-\underline{R}esource \underline{L}anguages (LRLs): $1$ to $360$M training tokens in parallel data
    \item \underline{U}nsupe\underline{r}vised \underline{L}anguages (URLs): no parallel data
\end{enumerate}

\insertfloresenxxresults
\insertfloresxxenresults


The results relative to the baseline, in $\Delta$\chrf{}, are summarized in Tables \ref{tab:floresenxx} (\enxx{}) and \ref{tab:floresxxen} (\xxen{}). A few trends jump out. Firstly, all models trained with only monolingual data augmentations see consistent performance gains over the baseline. Conversely, models with only parallel data augmentations show performance degradations. Predictably, models mixing monolingual and parallel data augmentations fare in-between those poles.

These general trends are the same between \enxx{} and \xxen{} directions, though the gains are generally lower in the \xxen{} direction. As noted in Section \ref{eval}, this is expected, and this direction is less of a priority for translation improvements. Results on the \ntlevalset{} eval set, in Appendix Section \ref{appendix:gatones}, show the same trends, though the performance gains tend to be larger for all augmentations, with CodeSwitchMonoGatiPanlex gaining $+2.3$ \chrf{} for URLs. Interestingly, though the Codeswitch augmentation seems to have higher overall performance on unsupervised languages in the \enxx{} direction, \texttt{GLOWUP} usually has the edge for higher-resource languages and the \xxen{} direction.

Despite these trends seeming robust across models, the effect sizes are relatively small, maxing out at about $+1.5$ average $\Delta\chrf{}$ gain.  However, the picture changes dramatically when only looking at the subset of languages that has the higher-quality \gatitos{} training lexicons. For these 26 languages, every augmentation, even the parallel ones, have large performance gains. The winning augmentations for URLs remain CodeSwitchMonoGatiPanlex and GlowupMonoGatiPanlex, the former having an average gain of $+7.0 \chrf{}$ on \flores and $+8.0 \chrf{}$ on \ntlevalset{}, and the latter having $+5.6 \chrf{}$ on \flores{} and $+9.5 \chrf{}$ on \ntlevalset{}.

Finally, although it was not the first place model in any category, the GatiPanlexTokenPairs has large gains in all directions over baseline, and only falls short of the more complex augmentations by a small margin. Furthermore, when used in conjunction with either Glowup or Codeswitch, it further improves performance across all categories. This may be one of the most useful long-term findings of this study: raw token pairs perform roughly on par with all the other fancy augmentations!


\subsection{Scaling up: bigger models, more data} \label{sec:bigger_models}

Sometimes, results on smaller models do not transfer to larger models. To this end, we train larger transformer models with $1.6$B parameters using $10\times$ the parallel data and $10\times$ the high-resource monolingual data that was used for training the smaller models.

We trained three large models: a baseline, a token-pair model, and a token-pairs + CodeswitchMono model. The \chrf{} scores for these models can be seen in Table \ref{tab:bigbig}. There are a number of obvious differences between these results and the results on the smaller ($475$M parameter) models trained with $\approx \frac{1}{10}$th the data. First, the positive impact of the data augmentations is smaller in all categories, indicating that the gains previously seen from augmentations are partially washed out in the larger-data, larger model regime. On \flores{} \enxx{}, both models are very close to baseline---within the realm of noise. On \ntlevalset{} \enxx{}, they see consistent small gains of around $+0.5$ \chrf{}, much smaller than previously. For both eval sets in the \xxen{} direction, there are consistent small losses.
%with the exception of the surprising performance of GatiPanlexTokenPairsBig on unsupervised languages.

Is this the Bitter Lesson \citep{sutton_2019} getting us again? Perhaps---but the picture is less bleak than it first appears. When we look at the subset of the languages where we have a higher assurance that the bilingual lexica are higher-quality---namely, those that use \gatitos{} bilingual lexica---we still see consistent wins, even in the \xxen{} direction. For these 26 languages, all models see consistent gains, and as before, the gains are bigger on unsupervised languages, and adding in the Codeswitch augmentation gives a consistent leg up on the unsupervised languages.   

Overall, the takeaway from these experiments is that one has to ensure that the data is of high quality when applying lexical data augmentation at such a large scale. While we saw substantial improvements for many languages, these were balanced out by losses for other languages (especially those with only Panlex, but not \gatitos{}, data). 

\insertbigbig


\normalsize


\subsection{Glowup Decoding}

In principle, one of the advantages of the GlowupParallel approach is that lexica can be used at inference time. Therefore, we experimented with decoding the eval sets not with the translation task ID, but with the Glowup task ID, along with the relevant lookups from the lexica. Unfortunately, these decodes failed impressively, with performance degrading the more prompts that were included. Model decodes often had long sequences of control tokens. Further work should not disregard this direction; indeed, a variant of this likely has particular promise in the world of foundation models. The current approach likely just needs some tweaks to eliminate this sort of out-of-domain decoding errors we were seeing, but we leave an investigation of this hypothesis for future work.

\subsection{Oracles: what happens if we use trusted parallel text?}

Parallel text is much more costly to produce than bilingual lexica, but also contains many more useful signals, including examples of word usage in context. But how much more helpful is it, really, than bilingual lexica? The answer seems to be ``much more helpful''.

To measure this, we trained a model with a variety of public parallel datasources for a few of our lowest-resource languages. The datasets we used were HornMT \citep{hornMT}, SALT (Sunbird AI Language Translation) dataset \citep{akera2022machine}, FFR: Fon-French Neural Machine Translation \citep{emezue-dossou-2020-ffr}, Tatoeba \citep{tatoeba}, The Makerere MT Corpus: English to Luganda parallel corpus \citep{makereremt2021}, Commonvoice \citep{ardila-etal-2020-common}, Kencorpus: Kenyan Languages Corpus \citep{wanjawa2022kencorpus}, Chuvash-Russian parallel corpus\citep{chuvashcorpus}, Abkhaz Corpus \citep{abkhazcorpus}, Bashkir Corpus\citep{bashkircorpus}, Sprotin Faroese Corpus\citep{faroesecorpus}, Jojajovai Guarani-Spanish Parallel Corpus \citep{chiruzzo-etal-2022-jojajovai}, and the NLLB Seed data \citep{nllb2022}. We only used datasets with a license permitting commercial use, and therefore avoided other excellent datasets such as AmericasNLP \citep{mager-etal-2021-findings}. Note that these are high-quality, trusted datasets, prepared by community members -- a very different resource than the web-mined parallel data that the model is otherwise trained on.

To keep the experiment more controlled, we compared performance only on languages that had token-pair data from the \gatitos{} datasource, since this empirically seems to be of higher quality. Note that, although these languages are technically not zero-resource (unsupervised) in the baseline, they only had a few thousand short parallel sentences from a translation memory, so the findings here are similar to as if they had been on true 0-resource languages.

The results are in Table \ref{tab:vsparallel}. We compare four models: the standard baseline and GatiPanlexTokenPairs model, as well as the ``Parallel'' model (which adds the external parallel translation task with a 5\% weight) and the ``Parallel + GatiPanlexTokenPairs'' model, which uses both the token-pairs and the parallel data, with a combined weight of 5\%. On both eval sets, we see that using Bilex  yields a gain of around +3.5 \chrf{}, but using true parallel yields a much larger gain of about +10 \chrf{}. Using the bilingual lexica on top of the parallel data yields a further gain of about +0.5 \chrf{}, demonstrating that, though many of the gains have been washed out by the true parallel data, there are still modest gains to be had from bilex training.

Between these two resources, trusted parallel data is clearly more effective. However, the number of tokens (counting only the target language) is also much greater for the parallel data. On the extreme end, Tsonga (`ts') has 300 times more tokens in the parallel data than in the bilingual lexicon. In this light, it is impressive that bilex alone adds around +4 \chrf{}, and the parallel data only adds another +7 on top of that. Future work should look at quality gains conditioned on the number of tokens between parallel data and bilingual lexica.
%However, for the time being we can note that we can see impressive gains using only a small number of tokens in a bilingual lexicon-- and if one were to make sentence-level dataset of equal size to the 4,500-token \gatitos{} dataset, it would have only around 250 sentences.

\insertvsparallel


\insertregressiontab
\subsection{How many token pairs do I really need?}\label{subsec:token_pair_amount}

We examine the relationship between the number of lexical token pairs provided during training and MT performance (\chrf{}). First, we perform regressions using $\Delta\chrf{}$ over baseline as the outcome variable and three predictor variables: (1) number of Panlex entries, (2) number of \gatitos{} entries, and (3) number of monolingual sentences. We include monolingual sentences in the regression to control for it as a confound. To eliminate parallel data quantity as a confound, we only perform this analysis on URLs.

The regression results are given in Table \ref{tab:tokensregression}. As expected, both the number of Panlex word pairs for a given language (No. Panlex) and the number of \gatitos{} word pairs (No. \gatitos{}) have a positive $\beta$ coefficient. However, note that the $\beta$ for No. \gatitos{} is $\approx 3\times$ as large as the $\beta$ for No. Panlex on \flores{}, and $\approx 39\times$ as large for \ntlevalset{}. This aligns with our observation that the \gatitos{} dictionaries are more efficient for improving MT than Panlex, probably due to their higher quality.


The most practical question we can seek to answer is, \textbf{If I can spend \$X on translating tokens, how much quality increase can I expect?} To investigate this ``bang for buck'' question in a more controlled way, we observe the effects of the \gatitos{} dataset in isolation, without Panlex. We train an ``GatiTokenPairs'' model, which is identical to the ``GatiPanlexTokenPairs'' model, except the token-pair task has only \gatitos{} data. Thus, this tells us specifically what gains we can expect if we are to get 4,500 tokens' worth of a bilingual lexicon per language.
% For simplicity we don't train any \gatitos{}{}-only models with our augmentations, though by comparing the GatiPanlexTokenPairs model with the CodeswitchMonoGatiPanlex model on these languages we can infer that they would perform abot +0.7 \chrf{} better.

The results are in Table \ref{tab:bangforbuck}, reported in delta versus the baseline model for both \flores{} and the \ntlevalset{} eval set in the \enxx{} direction. The improvement for unsupervised languages is around +5.0 \chrf{} for both eval sets; the improvement for languages with some parallel data is less but still noticeable, hovering around +2 \chrf{}. The largest improvement is in Goan Konkani (+11.0 \chrf{}), with Mizo, Ilocano, and Bambara close on its heels with gains of around +8 \chrf{}. Only Maithili, which has interesting properties a a close dialect of Hindustani, sees a loss on both eval sets. The gains are not obviously related to the total number of tokens per language. 

As an aside, it is heartening that \flores{} and \ntlevalset{} seem to agree very nicely, despite their different domains (Wikipedia versus web + question-answers).


\insertbangforbuck


\section{Conclusions}

In this paper we explore the ways that that augmenting training data with bilingual lexicon information can improve the performance of machine translation models on low-resource and unsupervised languages, and open-source the \gatitos{} dataset, which leads to average gains of about +8 \chrf{} alone on unsupervised languages. We perform extensive experimentation with three main types of lexical augmentation: codeswitching, lexical prompting, and raw token-pair training. The results show that applying any of these augmentations to monolingual data yields substantial improvements, and that they can be combined for greater effect. The leader (by a small margin) is the combination of CodeswitchMono and raw token-pair training. These results hold when scaling up model and data size, but in the settings with more data and larger models, the quality of the bilingual lexica plays a relatively bigger role, and augmentation with the noisier Panlex begins to lag in quality behind the much smaller, yet higher-quality, \gatitos{} dataset.

Future work will likely want to focus on prompting foundation models with bilingual lexica. Large Language Models show promise on machine translation for high-resource languages \citep{jiao-etal-2023-chatgpt}, but their capabilities on low-resource languages have yet to be thoroughly explored. Additionally, a more thorough investigation of the trade-off between cost and quality for tiny datasets can be explored: if one only has X dollars, or X dedicated volunteers with k total hours, should they spend their time translating text, making monolingual text, or making bilingual lexica? 

\section{Acknowledgements}

We would like to thank Aditi Chaudhary, Ankur Bapna, Daan van Esch, and Machel Reid for their comments and suggestions on this project.


\newpage
\bibliography{iclr2022_conference,anthology}
\bibliographystyle{iclr2022_conference}


\newpage


\appendix

\insertexampletranslations

\section{Results on \ntlevalset{}} \label{appendix:gatones}


\insertntlenxxresults
\insertntlxxenresults

\subsection{ \ntlevalset{} Eval set}

The main paper reports the scores on the more widely-used \flores{} eval set; this section reports on the other dataset that we evaluate our models on. This is an in-house test set for the $1000$ Languages Initiative, which we call \ntlevalset{} (Google AuTOmatic NTL Eval Set). The dataset contains $63$ languages, most of which are unsupervised or low-resource (although there are a small number of MRLs).


\subsubsection{Summary}

\paragraph{\enxx{}}
The evaluation results on \ntlevalset{} for the \enxx{} direction are summarized in Table \ref{tab:ntlenxx}. The trends are mostly the same as what we saw for the \flores{} \enxx{} evaluation. Once again, the models trained with augmentation on monolingual data but not parallel data generally yielded improvements (the sole exception being GlowupMono, which decreased performance on URLs by $-0.2$ \chrf{} relative to the baseline), and the parallel data augmentations didn't do as well. CodeswitchMonoGatiPanlex emerges, even more definitively than on \flores{}, as the best model, winning all categories except MRLs (on which CodeswitchMono performs best).

However, there are some minor differences between this evaluation and the \flores{} one when we look more closely. First, although CodeswitchParallel again does quite poorly, GlowupParallel doesn't do too badly. Still, it is far from being on par with CodeswitchMonoGatiPanlex or even just GatiPanlexTokenPairs. GlowupMonoParallel doesn't do too poorly either, actually beating out GlowupMono on URLs by $1.3$ \chrf{}, but it doesn't help nearly as much as CodeswitchMonoGatiPanlex (or, for that matter, the two augmentations from which this hybrid model is trained). Overall, this part of the evaluation still provides a compelling case for CodeswitchMonoGatiPanlex as the single most effective augmentation method for URLs and LRLs.

\paragraph{\xxen{}}
The evaluation results for the \xxen{} direction are given in Table \ref{tab:ntlxxen}. The single most important takeaway from this part of the analysis is the same as it was for the \flores{} evaluation: the plain GatiPanlexTokenPairs model helps URLs the most in this direction, with a $\Delta \chrf{}$ of $+1.1$ over the baseline. Yet again, the improvements are smaller in this direction than for \enxx{}. The only other thing that stands out about this part of the evaluation is that the GlowupMono augmentation doesn't seem to be as helpful according to this test set as for the \flores{} set. Although GlowupMonoParallel and GlowupMonoGatiPanlex do reasonably well, their improvements are significantly smaller than the improvement from using GatiPanlexTokenPairs alone, and the GlowupMono augmentation by itself actually results in losses on URLs. So taking the \ntlevalset{} and \flores{} results together, it seems that adding raw token pairs as additional parallel data is the best way, out of the techniques we tried, to improve performance in the \xxen{} direction for very low-resource languages.



\section{ Effects on distributionally similar noun mistranslation}

Part of the motivation for using bilingual lexicons for unsupervised translation was to see whether we could repair the common error mode of mis-translating distributionally similar nouns. \citet{bapna-etal-2022-building} note that this error mode is particularly common for two categories of nouns: animals and colors.

To measure improvement on this phenomenon, we define the \textit{token hit-rate} as the following: for some set of desired tokens $D$, let $R_D$ be the subset of the eval set such that each reference contains at least one token in $D$. The hit-rate is then the percentage of times in $R_D$ that the model correctly generated one of the desired tokens in $D$. For instance, if the desired tokens are ``kitten'' and ``puma'',  $R_D$ is the set of references containing one of these words, e.g. ``The \textbf{kitten} lies'' and ``A \textbf{Puma} eats hot chip''. If the model produces ``\textbf{kitten} lie on floor'' and ``\textbf{Crocodile} charge they phone'' from the corresponding sources, it has a hit-rate of 50\%, since it correctly got ``kitten'', but not ``puma''.

Table \ref{tab:hitrate} looks at the token hit-rate for the models \textbf{BaselineBig} and \textbf{CodeswitchMonoGatiPanlexBig}, for the categories of animals occurring in \gatitos{} (\textit{bear, bee, bird, butterfly, cat, chicken, deer, dog, elephant, fish, frog, goat, horse, insect, lion, monkey, parrot, pig, rabbit, sheep, snail, snake, tiger, turkey, turtle}), animals NOT appearing in \gatitos{} (\textit{ant, antelope, buffalo, cheetah, crocodile, dolphin, dormouse, gorilla, jellyfish, koala, leopard, moose, mosquito, newt, ocelot, otter, reindeer, robin, scorpion, shark, sloth, spider, springbok, tortoise, velociraptor}), colors (\textit{black, white, red, blue, yellow, green, purple, orange, grey}), and numbers (\textit{one, two, three, four, five, six, seven, eight, nine, ten, hundred, million}). All numbers and colors appear in \gatitos{}. Numbers are included as a weak control, since the model will tend to make fewer mistakes on them, though such UNMT-style mistakes do occur.

As expected, the \gatitos{}-augmented model performs better on these tokens. Two things are worth noting. First, the model improves noticeably on the complementary distribution---words that do not appear in the lexicon training data---but unsurprisingly improves more on the words that are present in \gatitos{}. Second, the improvements are not as large as expected: why is it not now getting 100\% accuracy? Digging into the outputs, it seems that this is mainly due to the high baseline of (a) undertranslation; (b) hallucination; and (c) copying, as we expect from a model trained without various other tricks like back-translation (see Section \ref{sec:common_errors} for an analysis of common error types). This point is underscored by the models' imperfect performance on the ``easy'' class of numbers.

\inserthitrate




\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{panlex_langs_blue.png}
    \caption{Number of word pairs available in Panlex for each of $4750$ BCP-47 languages (log scale).}
    \label{fig:panlex_langs}
\end{figure}


\section{Biggest winners}

We also look at the top $5$ languages that were the biggest gainers over the baseline for each model. In some cases these may represent remarkable successes of a particular approach---though in other cases they may represent noisy outliers, as is to be expected when evaluating 200 languages.

\subsection{\flores{}}
\paragraph{\enxx{}}
The biggest winners for each model in the \enxx{} direction for the \flores{} evaluation set are given in Table \ref{tab:floresenxxwinners}.

There are seven languages that gained at least $5$ \chrf{} over the baseline on at least one model trained with data augmentation. These languages are:

\begin{enumerate}[nosep]
    \item Bhojpuri (bho): up to $+14.5$ \chrf{}
    \item Ilocano (ilo): up to $+9.1$ \chrf{}
    \item Serbian (sr): up to $+8.3$ \chrf{}
    \item Bambara (bm): up to $+8.1$ \chrf{}
    \item Tibetan (bo): up to $+8.0$ \chrf{}
    \item Nuer (nus): up to $+6.8$ \chrf{}
    \item Mizo (lus): up to $+6.2$ \chrf{}
\end{enumerate}
    
Unsurprisingly, most of these languages are unsupervised or low-resource, except for Serbian which is medium-resource in our dataset. Of the seven languages listed above, we use Panlex data for Ilocano, Serbian, Bambara, Tibetan, Nuer, and Mizo, and there is \gatitos{} data for Bhojpuri, Ilocano, Bambara, and Mizo. As will be discussed in Section \ref{subsec:token_pair_amount}, the \gatitos{} bilingual lexica are clearly a very useful resource for MT, although evidently Panlex alone can help as well. Another interesting finding is that Nuer, which has \textit{no} English-aligned entries in Panlex but $\approx 20$K non-English-aligned entries, still sees large improvements when translating from English. This is evidence that lexicon data can improve performance even in the zero-shot case, where e.g. the model learns better vocabulary alignment between English and Nuer despite not receiving explicit alignment information during training. In Section \ref{subsec:token_pair_amount}, we look at the relationship between the number of lexical data points for a language and the \chrf{} improvement, which provides some insight (albeit not perfect clarity) into why these particular languages did well.

\paragraph{\xxen{}}

Table \ref{tab:floresxxenwinners} shows the top $5$ biggest winners per model for the \xxen{} direction. Clearly there is a lot of overlap with the \enxx{} direction, although there are some differences. Also, note again that the magnitude of improvement in this direction is smaller, likely because the baseline performance is higher and there is less improvement to be made simply by better aligning vocabulary cross-linguistically. Some of the biggest winners in this direction that weren't already discussed for the \enxx{} direction are:

\begin{enumerate}[nosep]
    \item Tsonga (ts): up to $+3.1$ \chrf{}
    \item Guarani (gn): up to $+2.8$ \chrf{}
    \item Bashkir (ba): up to $+2.5$ \chrf{}
    \item Minangkabau (min): up to $+2.5$ \chrf{}
\end{enumerate}

\insertfloresenxxwinners
\insertfloresxxenwinners

\subsection{\ntlevalset{}}

\paragraph{\enxx{}}
The biggest winners on \ntlevalset{} in the \enxx{} direction are given in Table \ref{tab:ntlenxxwinners}. Though there is some overlap with the biggest winners on the \flores{} dataset (e.g. Ilocano, Bambara, Mizo, Bhojpuri), a number of different languages perform well too, some of which simply aren't included in the \flores{} set. The languages which gain $> 5.0$ \chrf{} on this part of the evaluation are:

\begin{enumerate}[nosep]
    \item Adyghe (ady): up to $+14.1$ \chrf{}
    \item Kedah Malay (meo): up to $+12.6$ \chrf{}
    \item Goan Konkani (gom): up to $+11.6$ \chrf{}
    \item Bhojpuri (bho): up to $+10.4$ \chrf{}
    \item Ilocano (ilo): up to $+9.8$ \chrf{}
    \item Avar (av): up to $+9.5$ \chrf{}
    \item Bambara (bm): up to $+9.0$ \chrf{}
    \item Mizo (lus): up to $+8.9$ \chrf{}
    \item Madurese (mad): up to $+6.6$ \chrf{}
    \item Assamese (as): up to $+6.3$ \chrf{}
    \item Pattani Malay (mfa): up to $+5.7$ \chrf{}
    \item Kalaallisut (kl): up to $+5.4$ \chrf{}
    \item Tibetan (bo): up to $+5.2$ \chrf{}
\end{enumerate}

\paragraph{\xxen{}}
The biggest winners in the \xxen{} direction are given in Table \ref{tab:ntlxxenwinners}. Many of the biggest winners overlap with the \enxx{} direction, but some of the languages that haven't yet been mentioned are:

\begin{enumerate}[nosep]
    \item Manipuri (mni\{-Mtei\}): up to $+7.7$ \chrf{}
    \item Dogri (doi): up to $+5.4$ \chrf{}
    \item Dhivehi (dv): up to $+3.4$ \chrf{}
    \item Tigrinya (ti): up to $+3.0$ \chrf{}
\end{enumerate}

\insertntlenxxwinners
\insertntlxxenwinners

\subsection{Big model winners}

The biggest winners for the $1.6$B parameter models are given in Tables \ref{tab:floresenxxwinnersBIG}, \ref{tab:floresxxenwinnersBIG}, \ref{tab:ntlenxxwinnersBIG}, and \ref{tab:ntlxxenwinnersBIG}.

\insertfloresenxxBIGwinners
\insertfloresxxenBIGwinners
\insertntlenxxBIGwinners
\insertntlxxenBIGwinners

\subsection{Empirical study of the quality of Panlex}

One way to judge the quality of a dataset is to review it manually, as in \citep{kreutzer-etal-2022-quality}; another is to see the empirical effects on model quality of training on it. As a byproduct of using Panlex for this project, can we judge the quality of Panlex for different languages?

To reduce noise, we average the scores on the three main uses of the bilexes,  namely the TokenPairs model, the Glowup model, and the Codeswitch model. We average the \flores{} and \ntlevalset{} scores.  We then compare those scores to the baseline model for both \enxx{} and \xxen{}. For the purposes of this analysis, we treat any absolute delta of under 0.3 \chrf{} to be noise.  The result is displayed in Tables \ref{tab:panlexqsmall} and \ref{tab:panlexqbig}.

One would like to say that the upper left-hand corner represents languages with unequivocally high-quality lexical data, and the lower right-hand corner represents languages with poor quality lexical data. Alas, however, this picture is rather muddied when we scale up to larger models, as we see that many languages jump from one bucket to another. Nonetheless, we do see the trend that \gatitos{} languages tend to cluster to the upper left-hand corner in both cases, and that Shan (`shn`) and Latin (`la`) do poorly in all cases, and should likely be avoided by practitioners.

\insertpanlexq

\textbf{Teasing out the confound of the mixed \gatitos{} and Panlex data:} For the 26 \gatitos{} languages, it is harder to trust the previous analysis. However, we can compare the scores of these languages between the GatiPanlexTokenPairs model and the GatiTokenPairs model. The second of these models is trained on a strict subset of the data that the first is. If a language performs better with this subset of the data, we can presume that the Panlex data was on average lower quality; if a language performs better on the superset, the Panlex data might still be lower quality, but its quantity at least makes up for performance to some degree. The languages that do over +0.3 \chrf{} better on the subset data are \texttt{ts, dv, bm, lus, ff, and ckb}, suggesting that those may have poorer-quality Panlex data, with the largest difference being \texttt{lus} at +2.7 \chrf{}; those that do better on the superset are \texttt{gom, mni-Mtei, kri, ln, doi, ay, sa, ti, mai}, and \texttt{as}, suggesting that Panlex still adds useful signal there.

The picture that begins to come together is that Panlex often has some useful signal, but also contains considerable amounts of noise. For a less expressive model that is already not able to reach very high quality, some noise in the lexicons does not hurt, and Panlex can help the model get off the ground for the lowest-resource languages. But for a stronger baseline model that produces higher-quality translations on average, this noise can actively harm performance. Therefore, more carefully curated bilingual lexica, like \gatitos{}, will tend to will yield higher quality results when used for model training with bigger models, as evinced in Table \ref{tab:panlexqbig}.

\section{Does lexical augmentation fix common MT mistakes?}\label{sec:common_errors}

In evaluating the ``big'' models with $1.6$B parameters, we wished to see whether our preferred lexical data augmentation methods (GatiPanlexTokenPairs or CodeswitchMonoGatiPanlex) reduced several common types of MT errors. The errors we looked at were (1) null output, or the ``question mark phenomenon,'' where the model simply outputs some unrelated symbol (such as question marks) instead of actual text; (2) copying, where the model copies some or all of the source sentence in its prediction; and (3) repetition, where the model erroneously repeats the same word or phrase many times. There are other error types we could look at, like hallucination, but we stick with these three basic types for this paper. More precise definitions of these errors are given below.

The results of this analysis are given in Table \ref{tab:commonerrors}. For each error type, we computed the percentage of sentences that were affected by dividing the number of affected sentences by the total number of sentences in the eval set. \flores{} has $806248$ sentence pairs across all languages and \ntlevalset{} has $309887$.

\insertcommonerrorstable

% Alex: results from common mistakes analysis
\paragraph{Null output (question mark phenomenon)}
The first error type occurs when the model outputs only ``??', or some other arbitary character, as its prediction. Instances of this likely indicate catastrophic effects of out-of-domain phenomena for 0-shot translation. 

\paragraph{Copying}
Another common error is copying, where the model's prediction is close or identical to the source sentence. In measuring this phenomenon, we said that any prediction with $> 85\%$ character-level similarity to the source sentence was considered a copy. To measure character-level similarity, we took the multiset intersection of the character frequencies in the source and the character frequencies in the prediction, and then divided the size of the intersection by the number of characters in the source.

\paragraph{Repetition}
The last common error type we examined was repetition. To count these mistakes, we divided the total number of tokens in a sentence by the number of \textit{unique} tokens. If the ratio was $> 3$, we counted the prediction as an instance of erroneous repetition.

% \subsection{Results on common errors}

% The \flores{} and \ntlevalset{} eval sets agreed in this analysis: CodeswitchMonoGatiPanlexBig was best at reducing question mark errors ($-0.3\%$ for \flores{} and $-0.2\%$ for \ntlevalset{}), while GatiPanlexTokenPairsBig was best at reducing copying ($-0.9\%$ on \flores{}, $-0.4\%$ on \ntlevalset{}) and repetition ($-0.7\%$ on \flores{}, $-0.9\%$ on \ntlevalset{}).

% It isn't totally clear why the results are this way, but it could be that improving cross-lingual vocabulary coverage simply reduces catastrophic confusion in the model, although this doesn't explain why the raw token pair model is best at mitigating copying and repetition while CodeswitchMonoGatiPanlexBig does best in preventing question marks. Answering these questions is left for future work.


\subsection{Comparing sampling strategies for translating tokens} \label{token-sampling}

As one recalls from Section \ref{mca}, the Codeswitch augmentation works as follows: Let $D$ represent a multilingual lexicon containing word or phrase translation pairs for many languages. Given a source sentence $x = (x_{1}, x_{2}, . . ., x_{n})$ from monolingual corpus $X_{mono}$, we substitute each token in $x$ for its dictionary translation with probability $p_{tr}$. 

However, there is an issue with this formulation. Because the lexica we use do not have exhaustive coverage across languages, it is often the case that simply looping over $x$ and attempting to translate each token with probability $p_{tr}$ would result in translating a fraction of $x$ that is significantly less than $p_{tr}$. So in order to approximate this desired fraction $p_{tr}$ as closely as possible, we first count how many tokens in $x$ have dictionary translations. Let this number be $k$. We then compute the adjusted probability $\tilde p_{tr} = \max(\frac{np_{tr}}{k}, 1)$,  and sample from amongst the words in $x$ with translations with probability $\tilde p_{tr}$, to obtain the codeswitched sentence $x'$. When substituting a source word for its translation, we choose a translation uniformly at random from all available translations in all languages. Because of this, it is usually the case that $x$ is codeswitched into many languages. Finally, we train the model to reconstruct the monolingual sentence $x$ from $x'$ using the same sequence-to-sequence model and loss function as for the MT task.

In our experiments we use use $p_{tr} = 0.4$. We apply MCA to all $208$ languages in our corpus, but augment only half the available monolingual data and train the remaining half with MASS \citep{song-etal-2019-mass}, as done in the baseline training regime \citep{bapna-etal-2022-building}. We prepend a task token, \texttt{<2codeswitch>}, to the codeswitched sentences to cue the model to perform the MCA task, as well as language (\texttt{<2lang>}) and script (\texttt{<2script>}) tokens. The \texttt{<2lang>} and script \texttt{<2script>} tags are used in all models, including the baseline.

Since this augmentation samples each token with some probability, the number of tokens translated in a given sentence follows a binomial distribution. The Glowup augmentation, however, samples a number of tokens to translate uniformly at random from all possible translatable tokens. So one has a binomial distribution over N tokens sampled, and the other has a uniform distribution---does this make a difference?

To test this we trained a version of the CodeswitchMono model using uniform sampling. The average \chrf{} of the CodeswitchMonoUniform model was 0.1 to 0.2 higher on all four of the (\enxx{} \xxen{}) x (\flores{} \ntlevalset{}) directions. We conclude that this may have a slight benefit, but the difference is within the realm of noise, and does not affect the conclusions elsewhere in this paper.



\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{odis.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{sandy.png}
  \end{minipage}
    \caption{Unsupervised Machine Translation models often confuse words that occur in similar contexts. A frequent example of this mistake is to translate one animal into another animal from a similar semantic category. For example, UNMT models frequently mis-translate ``cat'' (e.g. Odis on the left) as ``dog'', (e.g. Sandy on the right). We hope that multilingual lexica will help fix this sort of mistake, which is easy for a human to correct with a dictionary.}
\end{figure}

\section{Relationship between number of tokens and MT performance}

We also graph the relationship between number of lexical word pairs and $\Delta \chrf{}$ in Figures \ref{fig:token_pairs_enxx} (\enxx{}) and \ref{fig:token_pairs_xxen} (\xxen{}) for URLs only. The results for \flores{} and \ntlevalset{} are combined in these plots. In both directions, we observe a moderate positive relationship between the number of lexical word pairs for a given language in the augmented data and the $\Delta \chrf{}$ over the baseline.


\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{token_pairs_enxx.png}
    \caption{Number of lexicon word pairs in augmented data vs. $\Delta \chrf{}$ over baseline for unsupervised languages in the \enxx{} direction. Results for \flores{} and \ntlevalset{} are combined here.}
    \label{fig:token_pairs_enxx}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{token_pairs_xxen.png}
    \caption{Number of lexicon word pairs in augmented data vs. $\Delta \chrf{}$ over baseline for unsupervised languages in the \xxen{} direction. Results for \flores{} and \ntlevalset{} are combined here.}
    \label{fig:token_pairs_xxen}
\end{figure}

\section{Languages}

\subsection{Rationale for Language Choice}
\label{appendix:language-rationale}

Although this project is aligned with the 1000-language initiative from \citet{bapna-etal-2022-building}, we wanted to use smaller models for more rapid iteration, and as a result, commensurately smaller data and number of languages to fit comfortably in the model. Therefore, we chose to work with about 200 languages.

With this in mind, we also wanted to choose specifically those languages whose performance we could measure. Therefore, our approach was as follows:

\begin{itemize}[nosep]
    \item Include all languages with supervised (parallel) data, for maximal cross-lingual transfer
    \item Include all languages that have non-zero data and a \flores{} eval set
    \item Include all languages that have non-zero data and a \ntlevalset{} eval set
\end{itemize}


\insertmodeldescriptionsfull

\subsection{Complete Language data}

The following table gives a list of the languages used in our experiments, along with some linguistic and resource-related statistics. The numbers for data resources (i.e. Mono, Parallel, Panlex, and \gatitos{}) refer to the amount of data actually used in our experiments, \textit{not} necessarily the total amount of data available. For example, we subsampled the parallel and high-resource monolingual data we had available by a factor of $10$.


\insertlanguageslist

\section{Full results}
The full results on \flores{} for the various models we trained are available in Tables \ref{tab:fullresults-flores-enxx} and \ref{tab:fullresults-flores-xxen}. Model abbreviations are clarified in Table \ref{tab:modeldescriptions}. Scores from the NLLB model are included as reference, though keep in mind that the smaller research models in this paper will naturally have lower quality; even the ``Big'' models are 30x smaller, and not optimized with back-translation and so on.


\insertfullenxx
\insertfullxxen
\end{document}