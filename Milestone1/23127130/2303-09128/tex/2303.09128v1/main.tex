
% ICLR 2023 headers
% \documentclass{article}
% \usepackage{iclr2023_conference,times}
% \input{math_commands.tex}
% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
% End of ICLR 2023 headers

% ACL2023 headers 
\pdfoutput=1
\documentclass[11pt]{article}
% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage[]{ACL2023}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
% If the title and author information does not fit in the area allocated, uncomment the following and set <dim> to something 5cm or larger.
%\setlength\titlebox{<dim>}
% End of ACL2023 headers

\usepackage{amsmath,amsfonts,bm}
\usepackage{calc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow,graphicx}
\usepackage{graphics}
\usepackage{booktabs}
\usepackage{todonotes}
\usepackage{tcolorbox}
\usepackage{fancybox}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{subcaption}
\graphicspath{ {./images/} }

\newcommand{\qinyuan}[1]{\textcolor{violet!50}{[Qinyuan: #1]}}
\newcommand{\xiang}[1]{\textcolor{red}{[Xiang: #1]}}
\newcommand{\rocktim}[1]{\textcolor{pink}{[RJ: #1]}}
\newcommand{\shushan}[1]{\textcolor{orange}{[S: #1]}}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \newtcolorbox{question}{Question}
% \tcolorboxenvironment{question}{
%   enhanced,
%   borderline={0.8pt}{0pt}{black},
%   borderline={0.4pt}{2pt}{black},
%   boxrule=0.4pt,
%   colback=white,
%   coltitle=black,
%   sharp corners,
% }

\newtheorem{researchq}{RQ}
\newenvironment{question}
{\noindent\begin{Sbox}\begin{minipage}{\linewidth-7.5\fboxrule-2\fboxsep-1pt}\begin{researchq}}
{\end{researchq}\end{minipage}\end{Sbox}\doublebox{\TheSbox}}

\title{Exploring Distributional Shifts in Large Language Models for Code Analysis}

\author{Shushan Arakelyan \\
  University of Southern California \\
  \texttt{shushana@usc.edu}\\
  \And
  Rocktim Jyoti Das \\
  Indian Institute of Technology Delhi \\
  \texttt{rocktimjyotidas@gmail.com} \\
  \AND
  Yi Mao \\
  Microsoft Research \\
  \texttt{maoyi@microsoft.com}\\
  \And
  Xiang Ren \\
  University of Southern California\\
  \texttt{xiangren@usc.edu}\\
  }

\begin{document}

\maketitle

\begin{abstract}
We systematically study the capacity of two large language models for code - CodeT5 and Codex - to generalize to out-of-domain data. 
In this study, we consider two fundamental applications - code summarization, and code generation.  
We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. 
This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial.
We establish that samples from each new domain present both models with a significant challenge of distribution shift. 
We study how well different established methods can adapt models to better generalize to new domains. 
Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. 
In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios.
% This assumes that the use case is permissive of the added cost of finetuning.
Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once.
We find that in the case of code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to each domain individually. 
\end{abstract}

\vspace{-0.2cm}
\section{Introduction}
\vspace{-0.2cm}
% \new
% Researchers have been reporting the phenomenon of poor generalization of statistical learning models to new software systems at least since the late 2000s~\citep{DBLP:journals/ese/Turhan12, DBLP:conf/sigsoft/ZimmermannNGGM09}. 
% With the emergence and big success of large language models (LLMs) for code~\cite{Chen2021EvaluatingLL,Chowdhery2022PaLMSL,li2022competition}, AI coding assistants become more and more ubiquitous in production, such as GitHub Copilot, Amazon CodeWhisperer, Replit, and many more.
% So understanding when the model performance in production will be different from that obtained on a benchmark becomes crucial.
Since the late 2000s, researchers have been reporting poor generalization of statistical learning models to new software systems~\citep{DBLP:journals/ese/Turhan12, DBLP:conf/sigsoft/ZimmermannNGGM09}, a phenomenon that has become increasingly important with the rise of large language models (LLMs) for code, such as GitHub Copilot, Amazon CodeWhisperer, and Replit. 
It is crucial to understand when off-the-shelf pretrained large language model performance on a private software system will differ from the performance obtained on a benchmark. 
Past work has looked at some aspects of this problem, among other features studying generalization across timeline (i.e. from older code to newer code), large software projects and small competition problems, authors, and code representations~\citep{DBLP:conf/acl/NieZLMG22, DBLP:journals/corr/abs-2107-10989, hu2022codes}.

However, the challenges of distribution shifts stemming from the hierarchical nature of software data, as depicted in Figure~\ref{fig:motivating}, have not been systematically studied with regard to large language models for code.
Motivated by that, in this work we probe the generalization capacity of large language models for code, specifically Codex~\citep{Chen2021EvaluatingLL} and CodeT5~\citep{Wang2021CodeT5IU}, in code generation and summarization tasks, examining three scenarios: generalization across companies, projects, and project components.
%motivation for this split
These scenarios are routinely considered for analyzing software systems~\citep{DBLP:journals/infsof/MaLZC12,DBLP:journals/eswa/LiXG09,DBLP:journals/jss/MairKLPSSW00} due to their intuitiveness and careful consideration that goes into combining or separating such entities.
\begin{figure}[h]
 \centering
 \includegraphics[trim=0 170 0 40,clip,width=0.45\textwidth]{motivating_figure.png}
\caption{Organization of a software system by the granularity of its components}
\label{fig:motivating}
\end{figure}

% \xiang{before stating the exp setup, let's first articulate in more details the questions we have in mind.}
% question 1
First, we want to understand \emph{how models perform on new domains} - if models struggle with out-of-domain generalization, they should be used with caution. 
At the same time, we empirically establish the legitimacy of our definitions for out-of-domain scenarios by demonstrating that these examples present a distributional shift. 
% exp. details
To answer this question we compare the performance of the models without any additional adaptation, with that of the models that have been adapted on limited data from a random domain, or from the test domain. 
Adaptation with labeled examples from the test domain is the proxy for model performance if there were no distributional shift. 
% findings
We find that both models suffer from a drop in performance when applied out-of-domain. 
In this experiment, the difference is more pronounced for code summarization, where adapting models with few in-domain examples, on average, leads to an improvement of over 10 BLEU~\citep{papineni-etal-2002-bleu} score points. 

% question 2
Next, we explore ways to \emph{improve the out-of-domain generalization of large language models for code}, recognizing that relying on labeled in-domain data for every new domain is impractical. 
Instead, we investigate the use of labeled out-of-domain data and small amounts of \textit{unlabelled} in-domain data to enhance generalization.
% exp. details
We test methods known to be successful in other transfer learning scenarios, such as meta-learning~\citep{DBLP:books/sp/98/ThrunP98,DBLP:journals/air/VilaltaD02} and multitask learning~\citep{DBLP:conf/icml/Caruana96,DBLP:journals/connection/Silver96}.
Additionally, we leverage unlabeled in-domain data to retrieve similar labeled examples from an out-of-domain corpus for adapting to the new domain.
We find that while meta-learning and multitask learning do not solve the out-of-domain generalization problem, domain adaptation with retrieved examples is a good technique for low-data domains. Models using retrieved examples perform on par, or better, than models that have been adapted using a few samples (e.g., 8 or 16) of in-domain labeled data. 

% question 3
Lastly, \emph{can we make the code models more broadly applicable and retain their generalization capacities}, rather than having to adapt them each time? 
% exp. detail
Depending on the approach to model adaptation (e.g. weight update vs in-context demonstrations) we varied the set of retrieved examples for each new domain, or for each test input individually. We compare performance obtained this way with that of the models that are adapted simultaneously to multiple domains (or instances, correspondingly). 
% findings
We find that Codex is very sensitive to these changes, so it is best to retrieve similar instances for each test data point.
On the other hand, CodeT5 has a minor drop in code summarization and a negligible drop in code generation. 
This makes it feasible to adapt and apply CodeT5 to multiple domains simultaneously with minimal tradeoff, eliminating the need to store separate copies of the model for each domain.

% Cross-project generalization issue has been raised continuously at least since late 2000s - people were noticing that statistics learned by ML methods on one/multiple products did not predict well new products. [1]

% Some works have measured the correlation between certain features of pairs of projects and the generalization capacity between them. Features that they looked at are, e.g. “who are the target audience of the project? which company developed the project? does the project use a database?” and so on. [2] There were also works/findings that simple statistical models generalize pretty well within-company (what I referred to as “organization”)

% Others have attempted modifying some earlier methods to be adaptable for cross-project/cross-timeline generalization. Solutions I came across were quite simple and typically operating with fairly outdated models, things like updated loss for SVM, or another linear classifier [3], simple modifications on LSTM, etc.


% Most of the earlier work has been done in the context of bug localization/prediction (i.e. span detection). More recent works is looking at code summarization or code classification.
% Some more recent works have also looked at different kinds of covariance shift/domain shift, including cross-project: [4, 5, 6]
% \begin{itemize}
%     \item {cross-project/within-project} 
%     \item {cross-author (one author to another)}
%     \item {cross-timeline (earlier version to later version)}
%     \item {across different token distributions}
%     \item {across different parse trees}
% \end{itemize}

% Their findings are quite limited though; they are either unsurprising given earlier work, or the experiment design and execution leave results inconclusive.

% One of the most relevant and recent works [7] looks at few-shot generalization within-project vs cross-project for Codex. That is the only experiment they did, but the idea it is quite similar to what I was thinking. It’s a very small study, within-project few-shot learning is evaluated on just 8 project (4 java proj-s, and 4 python), with wild variance in performances. Also, since the training data for Codex is unknown, it is not clear whether the projects are indeed unseen, so whether the cross-project testing is in fact such.

% To sum up, in terms of the diagram below, the things in the rectangle are the ones for which 1) prior work has established there is covariant shift  2) there were some attempts to measure it or address it, but they are limited in their applications.

% In this work we focus on questions of what are things that are likely to make the domain shift worse vs what are things that are likely to help with it. 



% We are interested in studying the following research questions: 

% RQ0:

% While it has been confirmed empirically for repositories, that different repositories constitute an out-of-domain datasets with regards to each other, it is not clear whether and to what extent the same is true for different levels of granularities of code belonging to the same entities. In particular, we are looking at functions coming from different companies, and different folders within a project to measure the extent to which seeing in-distribution examples help. 

% RQ1: 

% We want to study to what effect different methods, trained on in- and out-of-distribution datasets help bridging this gap. In particular we study the effects of multitask or meta-learning upstream training on large OOD datasets and compare the effects of such training to few-shot finetuning on in-domain data. 


% RQ2: 

% We are also interested in studying how different granularities of domain definitions affect model generalization capacities. This is particularly interesting for meta- and multitask- learning models. 

% Additionally, we measure the variance of model performance within a single granularity, to understand whether there is correlation between what models can and can't do within a single domain. 

% RQ3

% And finally, whether different scenarios for training instance selection make a difference. We compare random training instance selection to smarter methods, such as maximum diversity, vote-k and k-means. 


\section{Background}

% \qinyuan{I recommend that we still have a related work section focusing on code anlaysis + distribution shift. Defer the adaptation methods to Q2 (and appendix if necessary). My logic is that we introduce the problem that we study and why it is important here. Those adaptation methods are our \textit{tools} to analyse this problem, so we introduce the \textit{tools} when we use them. This would also create a linearized flow for the reader.}

Distribution shifts, the shifts in underlying semantics between the training and evaluation data, can be one of the most impacting factors for deteriorating performance at test time.
Prior work in code analysis has mainly focused on \textit{cross-project} distribution shifts, training the model on one set of code projects and evaluating them on unseen code project. 
Additionally, the studies were mainly conducted in the context of \textit{traditional machine learning methods}, such as linear classifiers, support vector machines, and later, LSTMs \citep{DBLP:conf/sigsoft/ZimmermannNGGM09, DBLP:journals/ese/Turhan12, DBLP:conf/itasec/AngioniDPB22}. 

Recently, there has been a resurgence of interest in studying distribution shifts in code analysis, with newer works considering shifts caused by different authors of the code, the timeline of the project, distributions of code tokens, etc~\citep{DBLP:journals/corr/abs-2107-10989,hu2022codes,DBLP:conf/acl/NieZLMG22}. 
Additionally, large language models trained on code have demonstrated remarkable capabilities in code analysis tasks, however their capabilities under domain shift is still under-explored.
In this work we conduct a comprehensive empirical analysis to probe large language model's capabilities in handling three different granularity of distribution shifts (company, domain, module) when different training and adaptation methods are used. 
In addition to directly fine-tuning vanilla LLMs, we experiment with enhancing pretrained models using the four methods described below. 
% An extended version of the background is available in the appendix section~\ref{app:extended background}.

\paragraph{Meta-Learning and Multi-task Learning.} In our work, we experimented with both Meta-Learning and Multi-task learning to get better initialization for few-shot performance on the downstream task. For meta-learning, we have chosen Model-agnostic Meta-Learning(MaML)~\citep{Finn2017ModelAgnosticMF} which is a gradient-based method. It is a conceptually simple and general algorithm that has been shown to outperform existing approaches in several tasks. Multi-task Learning aims to learn more generalized representations by jointly training on several tasks. We adopted the simplest approach to multi-task learning by jointly finetuning a shared language model on multiple tasks. 
\paragraph{Parameter Efficient Methods.} Parameter-efficient methods have been shown to obtain performance comparable to finetuning all model parameters with only a tiny fraction of model parameters. In our work, we have experimented with Low-Rank Adaptation(LoRA)~\citep{Hu2021LoRALA}, which is a low-rank update method, and prompt-tuning~\citep{Lester2021ThePO}, which learns continuous prompt tokens concatenated to the model input. 
\paragraph{In-context learning.}
% Providing language models with natural language descriptions of tasks, as proposed by ~\citet{Radford2019LanguageMA} has led to significant developments in few-shot learning. 
GPT-3~\citep{Brown2020LanguageMA} demonstrated the ability of large language models to perform few-shot predictions, where the model is given a description of the task in natural language with few examples. In our work, we conducted experiments on in-context learning on Codex which is a programming language version of GPT-3.
\paragraph{Retrieval Based Example Selection.} It has been shown in~\citet{Liu2021WhatMG} that in-context examples selected following a strategy may serve as more informative input to unleash GPT3's extensive knowledge. Inspired by this, we leveraged a simple similarity-based retrieval module to augment Codex for in-context learning example selection. Also, for the few-shot training of CodeT5, we experimented with a retrieval-based stratified few-shot example selection approach.

% \paragraph{Retrieval-augmented Methods:}
% \paragraph{Impact of Evaluation Methodologies on Code Summarization~\cite{DBLP:conf/acl/NieZLMG22}}
% In this work the authors study code model performance, evaluated in three different scenarios. In particular, they are looking into \textbf{cross-project} scenario (when train/test examples must come from distinct code projects/repositories), \textbf{mixed-project} scenario (where training and test entries may come from an overlapping set of projects). Finally, their main contribution is additional scenario that they propose - timewise split. In this scenario they split files in every repository by their timestamps, and the ones in the validation and test set have the newest time stamps. Their main finding is that while timewise split is more challenging than the mixed-project scenario, cross-project still remains by far the most challenging scenario from the three.

% \paragraph{CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation~\cite{DBLP:conf/emnlp/0034WJH21}} In this work the authors propose a new code modelling model - CodeT5, and test on multitask learning in addition to task-specific transfer learning experiments. For multi-task learning they follow the original T5 paper and use the same unified model for all tasks, but choose different checkpoints for different tasks. They use unified format of task control codes (prompts?) to notify the model which task it is dealing with. They find that models that are finetuned for the specific task outperform the one that is trained with multi-task objective on downstream tasks in most cases.

% In addition to CodeSearchNet, they add pretraining data in C and C\# to make sure that all tasks have overlapped programming languages with the pre-training data. 

% \paragraph{Unified Pre-training for Program Understanding and Generation~\cite{DBLP:conf/naacl/AhmadCRC21}}
% In this paper the authors fine-tune their model on CodeXGLUE dataset. They do not perform multilingual fine-tuning, but rather fine-tune each language separately, but it is only pretrained on 3 languages (Python, Java and English).

% \paragraph{Code Generation Tools (Almost) for Free?
% A Study of Few-Shot, Pre-Trained Language Models on Code~\cite{bareiss2022code}} In this paper the authors study few-shot generalization capacities of the Github Copilot model. They achieve few-shot learning effect by providing prompts to the model. They consider following tasks: code mutation (a way to estimate the quality of the test suite, by detecting its ability to detect injected faults), test case generation and test case oracle generation tasks as target tasks. They compare the outputs of the model with state-of-the-art tools designed for the same task: Major, Randoom, and MeMo, for mutation, test case generation and test oracle generation correspondingly. They observe, that for mutation generation FSLM generates a more diverse set of mutations compared to Major. For the oracle generation task the FSML has comparable performance to MeMo. For the test case generation, the authors conclude that FSLM achieves better coverage. They also attempt different versions of prompts, by using a NL description only, examples only, or a combination of both, as well as a \textbf{bad} example. They use 4 examples in the code mutation and oracle test generation, and 1 example in the test case generation task. For code mutation and oracle test generation tasks the bad example is one with empty output, and their experiment shows that the FSLM generalizes in that case anyway. Interestingly, for test case generation the bad example is from a different project, and the model has a significant drop in performance ($14\% \rightarrow 8\%$), unlike other two scenarios with ``bad'' examples. 

% \paragraph{A Systematic Evaluation of Large Language
% Models of Code~\cite{DBLP:journals/corr/abs-2202-13169}} In this paper the authors perform a systematic evaluation of a few pretrained large code models: Codex, GPT-J, GPT-Neo, GPT-NeoX, and CodeParrot, as well as train their own model - PolyCoder, which, unlike all other existing code models is trained exclusively on multiple programming language data. They also curate a dataset of 12 programming languages (C, C\#, C++, Go, Java, Javascript, PHP, Python, Ruby, Rust, Scala, TypeScript) that they used for treining their PolyCoder model from scratch.


% \paragraph{Productivity Assessment of Neural Code Completion~\cite{DBLP:journals/corr/abs-2205-06537}} In this paper the authors analyze surveys from over 2000 users of Github Copilot auto-completion model, focusing on various aspects such as how many completions were shown to the user, what is the acceptance rate and frequency of completions, what is the change rate of accepted completions, etc. They report about 30\% acceptance rate for JavaScript and Python languages, the acceptance rate being higher among students? (after work hours and on weekends).


% \paragraph{Competition-Level Code Generation with AlphaCode~\cite{DBLP:journals/corr/abs-2203-07814}} In this paper the authors train a large LM (encoder-decoder) on the Github data, fine-tune it on competitive programming data. Their model then generates a very large number of samples per problem, which are clustered and filtered to obtain a small set of candidate solutions. Additionally to their standard experiments, the authors experiment with equivalent models that are a) pretrained on only Python portion of github projects b) pretrained on MassiveText, which is a generic text dataset, but also includes a portion of github c) not pretrained at all. All these models are fine-tuned in the same way, with the exception of Python-only model, which is finetuned on Python only and samples Python only. They find that a) any pretraining benefits the result b) their model pretrained on github outperforms equivalent models pretrained on text+github or single language github. I think, the important question here would be to also take into account the number of tokens/code tokens that each model has seen. As it is, this finding contradicts the findings from Frank Xu's paper on PolyCoder, which is also trained on code data exclusively.  They also include analysis of their model with Codex. 
% Additionally, they show that their model memorizes parts of code and include those in the solutions. 

% \paragraph{CodeS: A Distribution Shift Benchmark Dataset for
% Source Code Learning~\cite{DBLP:journals/corr/abs-2206-05480}} In this paper the authors study the distribution shift problem with source code application. They also release a dataset of two languages - Python and Java - with 5 types of distribution shifts: task, programmer, time-stamp, token and concrete syntax tree.
% The authors focus on detecting and measuring OOD for code data, and for that end they also perform some evaluations on model generalization. They only consider code classification task, using programming competition data from Project CodeNet and some that they crawled themselves.
% Our scenraio of using few-shot fine-tuning is fully complementary to the same narrative that source code changes very quickly, and understanding how existing and deployed models can adapt to new code
% Their scenarios for distribution shift are different, but the most significant overlap is with programmer, but unlike us they consider distribution shift over a single programmer. Another similarity is distribution shift over task, but similarity is only in name, because they mean here unseen classes for a classification task. For distribution shift over time they consider old vs new submissions, and they also include two representation-based distribution shifts, the first one (token) basically corresponding to the distribution shift in the vocabulary, and the second one is more complex and it measures shift in the program's parse tree.
% It is quite hard to make sense of their numerical findings. 

% \paragraph{Estimating Predictive Uncertainty Under Program
% Data Distribution Shift~\cite{DBLP:journals/corr/abs-2107-10989}}
% In this paper the authors study the problem of over-confident predictions made by models for code on OOD data. They consider three types of distribution shifts and measure the performance of current models when handling these shifts. They study three scenarios: cross-project shift, time-wise shift (i.e. older version of a project vs a newer version), and programmer shift. They consider 7 different Java projects. Their time-wise shift is similar to that of ~\cite{DBLP:conf/acl/NieZLMG22}. For the cross-programmer shift they consider only elasticsearch. They consider 2 downstream tasks - method naming and code completion, however, they don't use pretrained language models, instead using an MLP and a code2vec model, so they don't perform pretraining/fine-tuning. As one would expect, they show that the performance of the models declines as the timeline proceeds. The largest decline is still observed in cross-project scenario. They do not specify how many instances their cross-project setup ends up having, so probably it is also trained on a really small number of examples. Same goes for cross-author, which is trained on an even smaller subset, and we again see a lot of variance in performance. They also study how effective are existing uncertainty methods for their definitions of distribution shifts under these models. They consider temperature scaling, MC dropout (?), mMutate which integrates statistical hypothesis testing, and finally Dissector which uses model's cross-layer confidence about a given input. The findings for this experiment are inconclusive and hard to analyze, because their model is a 2-layer NN, and MC dropout and Dissector do not work well with that.gy


% \paragraph{Exploring and Predicting Transferability across NLP Tasks~\cite{DBLP:conf/emnlp/VuWMSTMMI20}} In this paper the authors study transferability between different NLP tasks, dividing them into three broad categories - text classification, QA and seq. labelling. They perform 3000 combinations of different source/target tasks. They find that transfer is very effective for low-resource target scenarios, and in some cases in low-data source scenarios. They also create "task embeddings" from gradients on training examples computed w.r.t the loss following Achille et al. 2019's work on meta-learning. 
% \subsection{Few shot learning}
% \paragraph{Prompt-free and Efficient Few-shot Learning with Language Models.}
% \begin{enumerate}
%     \item In this method task-specific adapters are used which as mentioned in (IA)3 is not suitable for muti-batch training.
%     \item Masked Language Model embedding objective is used and masked tokens are used to let the model predict labels
%     \item But unlike earlier works it showed that without hand-crafted pattern the model is obtaining good results.
%     \item It also removed the need of verbalizers by using multi-token label embeddings.
% \end{enumerate}
% \paragraph{Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning}
% Finetuning results in state of the art results but the models are specialized for a single task with an entirely new set of parameter values, which can become impractical when fine-tuning a model on many downstream task.

% Parameter-efficient fine-tuninh with (IA)3:
% \begin{enumerate}
%     \item Add or update few parameters
%     \item Achieve strong accuracy after few shot training on new tasks.
%     \item Allow mixed task batches.
%     \item (IA)3 makes mixed-task batches possible because each sequence of activations in the batch can be separately and cheaply multiplied by its associated learned task vector
% \end{enumerate}

% Baselines compared against and showed to have better accuracy then all of them in almost all cases:
% \begin{enumerate}
%     \item BitFit: updates only the bias parameters
%     \item Adapters: introduced task specific layers after the self-attention and position-wise feed-forward networks.
%     \item Compacter and Compacter++: improves upon adapters by using low-rank matrices and hypercomplex multiplication.
%     \item Prompt-tuning: learns task-specific prompt embeddings that are concatenated to the model's input. 
%     \item FISH mask: chooses a subset of parameters to update based on their approximate Fisher information.
%     \item Intrinsic SAID: performs optimization in a low-dimensional subspace.
%     \item LoRA: assigns low-rank updates to parameter matrices.
%     \item Full model fine-tuning.
% \end{enumerate}
% \paragraph{Revisiting Few-sample BERT Fine-tuning.}
% Observations about BERT finetuning:

% \begin{enumerate}
%     \item Identical learning processes with different random seeds often result in significantly different and sometimes degenerate models following finetuning.
%     \item Non-standard optimizers introduces bias in the gradient estimation.
%     \item Top layers of the pre-trained BERT model provide  a bad initialization point for finetuning.
%     \item Use of a pre-determined, but commonly adopted number of training iterations hurts convergence.
%     \item Two source of randomness during finetuning are: weight initialization of the new output layer and the data order in the stochastic fine-tuning optimization. Existing work shows that these seemingly benign factors can influence the results significantly especially on small datasets.
%     \item BERTADAM leads to instability.
%     \item 	Pretrained features from the intermediate layers are more transferable.
% \end{enumerate}
% Optimization Algorithm: Debiasing Omission in BERTADAM:
% \begin{enumerate}
%     \item Different from original ADAM in terms of a bias correction step.
%     \item This causes overestimation of gradient update early during learning.
%     \item The ratio of the $\frac{m}{\sqrt[2]{v}}$  between the biased and unbiased estimation converges to 1 after several iterations thus, estimation bias will have a negligible effect.
%     \item For small datasets bias ratio is significantly higher than one for the entire fine-tuning process, implying these datasets suffer heavily from over-estimation in update magnitude.
%     \item Including bias-correction significantly reduce variance. In terms of loss, it speeds up convergence and shrinks the range of training loss.
    
% \end{enumerate}
% Initialization: Reinitializing BERT pretrained Layers:
% \begin{enumerate}
%     \item Transferring the top pretrained layers slows down learning and hurts performance.
%     \item Instead of the standard practice of using pre-trained weights for all layers, reinitializing the pooler layers and the top L Bert blocks using the BERT initialization with $N(0, 0.02^2)$ gives a better mean and lower variance. Increasing L gives better results first and then decreases.
%     \item It leads to faster convergence.
% \end{enumerate}
% Training Iterations:
% Training longer improves in terms  of performance and stability and help recover from bad initializations.
% \paragraph{Revisiting fine-tuning for few-shot learning}
% Aim: This paper aims to analyze the performance of finetuning method for few-shot learning and showed that it can achieve higher classification accuracy than common few shot learnings.
% Three main conclusions were drawn from this work:
% \begin{enumerate}
%     \item Low learning rate stabilizes the retraining process
%     \item Adaptive gradient optimizers for finetuning increases test accuracy.
%     \item When domain shift is larger updating the entire network helps.
% \end{enumerate}
	
\section{Problem setting}
\begin{figure}[h]
 \centering
 \includegraphics[trim=0 180 430 0,clip,width=0.45\textwidth]{data_separation.png}
\caption{We divide and group the functions from CodeSearchNet by the repositories, organizations, and folders that they belong to.}
\label{fig:data}
\end{figure}
% \qinyuan{My guess is this section is trying to specify the problem setting. I recommend to structure things in the following way, what is our high-level expectation of the system, train/test data, and training/eval stages. Also choose either \textit{domain} or \textit{task} and be consistent in the writing (I prefer domain); don't use them interchangeably.}

We are considering the scenario where a user is looking to use a large language model, such as Codex or CodeT5, in their software project. 
We want to understand how these models will perform particularly considering that the code may be coming from an unseen organization, an unseen project, or a previously unseen part of the project.
% we split the dataset into tasks.
We compare the off-the-shelf model performance with modified model performance and the performance in the best-case scenario, i.e. having in-domain labelled data. 

% the proxy for in-domain labelled data is 

Let us have two mutually exclusive sets of code data points: $X_{train}$ and $X_{test}$. 
% \qinyuan{I'm confused: is $X_{train}$ as set of domains or a set of input-output pairs? Because from the description here its datapoints (input-output pairs), from Table 1 it seems to be domains.}
% Furthermore, let pairs within $X_{train}$ and $X_{test}$ be nominated as $\{x^T_1, \ldots, x^T_N\}$ and $\{x^t_1, \ldots, x^t_n\}$ correspondingly, where $N$ and $n$ are the number of pairs in train and test set respectively. 
Assuming that the code in the data is extracted from some software projects, we can identify the \textit{organization}, \textit{project}, and the \textit{module} within the project that the data point came from.
Based on each of those characteristics we can group the data points into sets, and end up with three \textit{sets of sets}, as illustrated in Figure~\ref{fig:data}. 
For example, the middle superset on the figure contains multiple sets of data points, such that each set corresponds to a unique organization, and all code within that set originated from that organization. 
For simplicity, we will refer to a set of examples from the same domain as $\tau_i$. We also will refer to splits of such a set into train/development or test portions as $\tau_{train}$, $\tau_{dev}$, and $\tau_{test}$.
% \qinyuan{when we have the updated figure let's mark the figure with these notations too.}


\subsection{Data}
% \qinyuan{I still feel vague about (1) how data is partitioned (2) how ``in-domain''/``out-of-domain'' is defined, after reading this. Have we settled with using ``task'' or ``domain'' in this paper?}
For our experimentation, we use CodeSearchNet~\citep{Husain2019CodeSearchNetCE} dataset\footnote{Since the exact training data of Codex models is undisclosed, we cannot be sure that it did not include CodeSearchNet dataset, but we proceed with this data for a lack of a better alternative}, in particular, the partition containing JavaScript language. 
In our setup, the train section of the dataset corresponds to $X_{train}$, and development and test sections correspond to $X_{test}$.

\begin{table}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lrrr}
\toprule
 & $\tau \subset X_{train}$ (total) & $\tau \subset X_{train} (|\tau | \geq 96$)  & $\tau \subset X_{test} (|\tau| \geq 96$)\\
 \midrule
org. & 9737 & 195 & 8 \\
repos. & 15858 & 147 & 15 \\
fold. & 25268 & 100 & 10 \\
\bottomrule
\end{tabular}}
\caption{CodeSearchNet dataset, split according to the domain definitions. The left column shows the set used for training. The middle column shows the number of domains of each kind from $X_{train}$ that have at least 96 samples. The right column shows the number of domains in the $X_{test}$ after filtering all domains with less than 96 samples.}
\label{tab:dataset}
\end{table} 

We wanted to keep all of the domains in $X_{test}$ unseen, and for that reason, we removed any domain from $X_{test}$ that has also appeared in $X_{train}$. This way, for example, an organization that has samples associated with it both in training data and test data would not be included as part of the $X_{test}$. So any domain coming from $X_{test}$ will be, by our definition, out-of-domain for the model trained on $X_{train}$. 
We further split each domain $\tau_i \subset X_{test}$ into $\tau_{train}$, $\tau_{dev}$ and $\tau_{test}$. 
The evaluation is performed on $\tau_{test}$. $\tau_{train}$ and $\tau_{dev}$ are used to obtain a proxy for the upper-bound performance of the model if the domain $\tau_i$ were coming from a seen domain, i.e. if there was no distribution shift for $\tau_{test}$.

\paragraph{Preprocessing}
We used the ``path'' field of the CodeSearchNet dataset to determine each code snippet's belonging to an organization, repository, and lowest-level folder.
We use 5 different random seeds to divide a domain into $\tau_{train}$, $\tau_{dev}$, and $\tau_{test}$. 
% \qinyuan{I suggest we move the following to the appendix}
We aim to have at least 32 samples each in $\tau_{test}$ and $\tau_{dev}$, and up to 32 samples for $\tau_{train}$. 
Thus, from $X_{test}$ we filtered any domain that had less than 96 samples in total. 
The final dataset statistics that we ended up with are presented in Table~\ref{tab:dataset}.

% and end up with three supersets: in the first one, the sets are made by organizations, in the second one - by repositories, and in the third one by folders. 
% Based on these characteristics, let: $\{O^T_1, \ldots, O^T_{N_O}\}$ be sets of $x^T_i$, s.t. within each $O^T_j$ every $x^T_i$ originated from the same organization $o_j$, and each $o_j$ is distinct. 
% Similarly define sets $\{P^T_1, \ldots  P^T_{N_P}\}$ and $\{M^T_1, \ldots M^T_{N_M}\}$ for $x^T_i$-s originating in different projects and modules, and $\{O^t_1, \ldots, O^t_{n_O}\}$, $\{P^t_1, \ldots  P^t_{n_P}\}$ and $\{M^t_1, \ldots M^t_{n_M}\}$ for $x^t_i$-s respectively. 
% From this point on, we will refer to any $O, P, \text{ or } M$ as a \textit{task}. 
% Respectively, sets $\{O_j\}, \{P_k\} \text{ and } \{M_l\}$ will signify different definitions of task boundaries, based on the organization, project, or module that the code in $x_i$ came from. 
% In other words, $x_i$-s from different tasks within the same definition will be subject to domain, or covariant, shift in the distribution of data.

% \textcolor{blue}{YM: are we going to use the above notations below? If not, we don't need those math symbols as it complicates the presentation.} \qinyuan{I agree that the notations are a bit confusing}

% % \vspace{-0.2cm}
% \subsection{Learning Stages}
% % \vspace{-0.2cm}

% Given a pretrained model, we measure its performance for some unseen task from $X_{test}$. 
% For that end, we employ a few-shot domain adaptation method, such as fine-tuning or in-context learning.
% To further improve the model's generalization ability, wherever possible, we additionally use some training strategies, such as meta-learning or multitask learning. 
% Those are learning from hundreds of thousands of examples from multiple tasks in $X_{train}$.
% Thus, conceptually, we will be separating two stages of the learning process: training and downstream adaptation. 

\subsection{Applications and Metrics}
We evaluated our method on two cross-modal generation applications: code summarization and code generation. 
\textbf{Code summarization} aims to summarize a code snippet into a natural language description. The code snippet in CodeSearchNet dataset is a function, and the natural language description consists of the docstring of that function. The evaluation metric for this task is BLEU-4~\citep{papineni-etal-2002-bleu}. \textbf{Code generation} performs the reverse operation - given a natural language description of code, the model is asked to generate the corresponding function. We follow prior work and use CodeBLEU~\citep{Ren2020CodeBLEUAM} for evaluating generated code. We modified an existing CodeBLEU implementation by adding our own set of JavaScript keywords. In addition, recent research has established that CodeBLEU scores can disagree with human judgement scores~\cite{DBLP:journals/corr/abs-2208-03133}, and motivated by these findings we evaluate code generation models with chrF and rougeL metrics. In our experiments, we find that these metrics agree, so we report results for chrF and rougeL in the Appendix~\ref{app:metrics}. 
% and attached the full list of those in the Table \ref{table:keywords}.
% \vspace{-0.2cm}
\subsection{Models}
% \vspace{-0.2cm}
% Both our applications require sequence generation; keeping this in mind, 
We have experimented with two large language models for code: (1) CodeT5~\citep{Wang2021CodeT5IU}, which is an encoder-decoder model based on T5~\citep{Raffel2019ExploringTL} and (2) Codex~\citep{Chen2021EvaluatingLL}, which is a decoder only model based on GPT-3~\citep{Brown2020LanguageMA}. 
Both T5 and GPT-3 have strong zero-shot learning~\citep{DBLP:conf/iclr/WeiBZGYLDDL22,DBLP:conf/iclr/SanhWRBSACSRDBX22} and transfer learning capabilities, and their versions for programming languages exhibit strong performance across multiple benchmarks. 
The two models are of different sizes - the CodeT5 is using 700M parameters T5-large architecture, and the Codex model uses GPT-3 architecture with more than 100B parameters. 
We have provided a more detailed discussion of these models in the Appendix, Section~\ref{app:model}. 
Next, we describe the methods we have employed to improve out-of-domain generalization for each model.
% \qinyuan{a bit verbose... i recommend making it short and directly introducing codet5(enc-dec) and codex(dec-only)}

% Since both of our applications require generating sequences, in our experiments, we included two model architectures that are naturally suited for generation: an encoder-decoder model and a decoder-only model. From a large number of available pre-trained code models, we picked a pre-trained model based on T5~\citep{Raffel2019ExploringTL}, and a model based on GPT-3~\citep{Brown2020LanguageMA}. Both base models have demonstrated strong zero-shot learning~\citep{DBLP:conf/iclr/WeiBZGYLDDL22,DBLP:conf/iclr/SanhWRBSACSRDBX22} and transfer learning capabilities. Respectively, the versions for code are known for strong performance across a number of benchmarks. Additionally, this choice allowed us to include two models of different sizes - the T5-based model is using T5-large architecture, which has 700M parameters, and GPT3-based model has more than 100B parameters. Next, we discuss each model in more detail, as well as for each of them describe the methods that we have attempted to employ for improving out-of-domain generalization.
% We use two models for conducting our experiments: CodeT5 and Codex~\citep{Chen2021EvaluatingLL}.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{
    \multirow{2}{*}{\textbf{Code summarization}}
} &
\multicolumn{3}{c}{\textbf{folder}} & 
\multicolumn{3}{c}{\textbf{repo}} & 
\multicolumn{3}{c}{\textbf{org}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} \\
\midrule
CodeT5 FT ID 
%folder (8/16/32)
& 14.39 & 16.06 & 18.31
%repo (8/16/32)
& 12.68 & 14.73 & 16.82
%org (8/16/32)
& 13.14 & 16.35 & 17.65  \\
CodeT5 LoRA ID 
%folder (8/16/32)
& 16.57 & 19.07 & 20.93
%repo (8/16/32)
& 15.22 & 17.14 & 21.20
%org (8/16/32)
& 15.61 & 18.56 & 20.87  \\
CodeT5 FT random
%folder (8/16/32)
& 3.58 & 4.30 &  5.02
%repo (8/16/32)
& 4.35 & 4.70 & 5.79
%org (8/16/32)
& 4.53 & 5.47 & 6.27  \\
CodeT5 LoRA random 
%folder (8/16/32)
& 3.69 & 4.37 & 4.92
%repo (8/16/32)
& 4.70 & 5.56 & 5.92
%org (8/16/32)
& 5.27 & 5.53 &  6.26 \\
\midrule
Codex ICL ID
%folder (8/16/32)
% & \multicolumn{3}{c}{20.72} & 
% \multicolumn{3}{c}{20.34} & 
% \multicolumn{3}{c}{19.00} \\
& 20.72 & - & -
%repo (8/16/32)
& 20.34 & - & -
%org (8/16/32)
& 19.00 & - & -  \\
Codex ICL random
% folder (8/16/32)
& 6.73 & - & -
%repo (8/16/32)
& 7.17 & - & -
%org (8/16/32)
& 6.84 & - & -  \\
% \multicolumn{3}{c}{6.73} & 
% \multicolumn{3}{c}{7.17} & 
% \multicolumn{3}{c}{6.84} \\
Codex instr. only \textit{(0-shot)}
%folder (8/16/32)
& (1.61) & - & -
%repo (8/16/32)
& (1.55) & - & -
%org (8/16/32)
& (1.52) & - & -  \\
% \multicolumn{3}{c}{} & 
% \multicolumn{3}{c}{} & 
% \multicolumn{3}{c}{} \\
\bottomrule          
\end{tabular}
}
\caption{Comparison of model performance for code summarization on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is BLEU (higher is better). 
% \qinyuan{The caption is confusing. My understanding is that $\tau_{test}$ is fixed while we control $\tau_{train}$ to be ID/random? The caption sounds like $\tau_{train}$ is fixed and $\tau_{test}$ is varying. (Or did I interpret the whole thing wrong?)}
}
\label{tab:code-to-text}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{
    \multirow{2}{*}{\textbf{Code generation}}
} &
\multicolumn{3}{c}{\textbf{folder}} & 
\multicolumn{3}{c}{\textbf{repo}} & 
\multicolumn{3}{c}{\textbf{org}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} \\
\midrule
CodeT5 FT ID 
%folder (8/16/32)
& 14.67 & 15.22 & 16.13
%repo (8/16/32)
& 16.15 & 17.42 & 18.62
%org (8/16/32)
& 14.54 & 15.34 & 16.43  \\
CodeT5 LoRA ID 
%folder (8/16/32)
& 14.14 & 15.06 & 16.36
%repo (8/16/32)
& 16.23 & 17.45 & 18.96
%org (8/16/32)
& 14.17 & 15.30 & 16.62 \\
CodeT5 FT random 
%folder (8/16/32)
& 15.23 & 14.94 & 15.15
%repo (8/16/32)
& 14.19 & 14.14 & 14.67
%org (8/16/32)
& 13.39 & 13.43 & 14.44 \\
CodeT5 LoRA random 
%folder (8/16/32)
& 14.45 & 14.29 & 15.37
%repo (8/16/32)
& 14.29 & 13.74 & 15.04
%org (8/16/32)
& 13.76 & 13.85 & 14.81\\
\midrule
Codex ICL ID
%folder (8/16/32)
& 23.87 & - & - 
%repo (8/16/32)
& 25.73 & - & - 
%org (8/16/32)
& 24.64 & - & -  \\
Codex ICL random
%folder (8/16/32)
& 16.82 & - & - 
%repo (8/16/32)
& 16.82 & - & - 
%org (8/16/32)
& 17.47 & - & - \\
Codex instr. only \textit{(0-shot)}
%folder (8/16/32)
& (5.77) & - & - 
%repo (8/16/32)
& (5.49) & - & - 
%org (8/16/32)
& (5.72) & - & - \\
\bottomrule          
\end{tabular}
}
\caption{Comparison of model performance for code generation on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is CodeBLEU (higher is better).
}
\label{tab:text-to-code}
\end{table*}



\section{Analysis}
In this section, we formulate the research questions that we aim to answer, and give more detailed description of the setups that we have used for analyzing and answering each question. 

\begin{question}
How do code models perform on new domains?
\end{question}
% We start by looking at the performance of two models after few-shot finetuning on some random and ID data. 
We test models' capacity for generalization to new domains by comparing the performance of the models that have been adapted to the new domain using few-shot instances of in-domain data (ID) vs those that only encountered out-of-domain data (OOD). 
% \shushan{"We use $\tau^i_{train}$ for $\tau^i_{test}$ for in-domain, for ood data we sample some $\tau^i*_{train}$ from other tasks"?}
% In addition, we also performed direct evaluation, i.e. evaluated the model on the $\tau_{test}$ portion for some task from $X_{test}$, without any additional updates to the model. 
For CodeT5 few-shot domain adaptation data is used to update the model weights, whereas for Codex it is included as demonstrations in the prompt to the model. 
We provide more details for both models below.
% More specifically, to answer this question we give models access to the $\tau_{train}$ portion for some task $\tau$. 
% That is achieved by the first case the task that is being used for domain adaptation is the same one as that used for evaluation. 
% This is our proxy for the model performance on the task when the task is in-domain (ID) w.r.t. its training data. 
% In the rest of the paper, we will refer to this scenario as \textbf{ID}. 
% \qinyuan{what's the motivation of doing \textbf{random}?}
% In the second case, the training portion used for fine-tuning comes from another \textbf{random} task, and this respectively reflects the model performance on out-of-domain data. In both cases, we updated the model for 500 steps.

\subsubsection*{CodeT5}
Next, we discuss different adaptation techniques that we have employed for the CodeT5 model. For these methods, we have experimented with using a different number of supervision examples - 8, or 32.

The first adaptation method we used is full model \textbf{fine-tuning} (\textbf{FT}). We updated the model for 500 steps using batch size of 8, the best model was identified by the performance on the $\tau_{dev}$ portion. Besides FT, we also experiment with a parameter-efficient fine-tuning method - \textbf{Low-Rank Adaptation} (\textbf{LoRA})~\citep{Hu2021LoRALA}. This method adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices thus enabling parameter-efficient adaptation to new domains without forgetting. We used the implementation of LoRA available in T-Few~\citep{liu2020tfew} library. We use a rank of 4 with an initialization scale of 0.01 and update all the attention and feedforward layers. We train for 1000 steps with a batch size of 8.

\paragraph{Codex} 
For Codex, we do not perform weight updates. 
Instead, very large models, such as Codex, have been shown to be capable to generalize to unseen tasks using only the instruction for the task. 
In the simplest case, we evaluated Codex by directly presenting it with the instruction, for example "Summarize following JavaScript code", and input (i.e. \textbf{instruction only}).
% \shushan{update tables 2 and 3}
It has been established that Codex can be sensitive to the wording of the instructions, so we used a number of different instruction variations for each application and averaged the results across them. 

Besides that, larger models have been shown to be able to ``learn'' from demonstration examples that are provided as part of their input, even though this process does not involve any weight updates. 
This phenomenon is known as in-context learning (\textbf{ICL}) technique, which is what we use for domain adaptation for Codex.
Due to the limitations on the size of the input to the Codex model (4096 tokens), we used as many demonstrations as would fit, including up to 8 demonstrations with each test example. And since the model can also be sensitive to the order of examples, we shuffled the order of the demonstrations 5 times and averaged the results among these runs. 

\vspace{-0.2cm}
\subsection*{Finding: Both models struggle on new domains}
Tables~\ref{tab:code-to-text} and~\ref{tab:text-to-code} demonstrate the performance obtained by CodeT5 and Codex (additional results for code generation metrics chrF and rougeL are available in Appendix Section~\ref{app:metrics})
We see that both models for code struggle with OOD generalization as demonstrated by the performance difference for models that have encountered in-domain examples vs those that have not. For example, CodeT5 model on code summarization in most scenarios gains about 200\% relative improvement after updating the model with few-shot in-domain data.

It is worth noting that while there is still difference in performance for CodeT5 model on code generation ID and OOD, the performance difference is next to negligible. 
In comparison, we see a very large improvement on code summarization task. 
We hypothesize that this can be due to the fact that code generation is a more challenging task for a large language model, and so including ID data is not as helpful for CodeT5. This is additionally confirmed by our observations made in the next research question. 
On the other side, Codex is anecdotally a much more proficient model for code generation, and the addition of the in-domain data results in up to 50\% of relative improvement.

% It is interesting to note that few-shot in-domain adaptation provides less relative improvement for pretrained models performing code generation compared to code summarization. For example, for code generation we see the largest relative improvement with Codex, and at most it makes 50\% relative improvement, whereas for code summarization we see improvement of up to 300\%.
% And increasing the amount of in-domain data makes this difference more notable. 
% \shushan{Get back to this after adding more eval metrics: "We hypothesize that this can partly be attributed to the fact that BLEU metric has gotten better calibrated throughout many years of use, whereas CodeBLEU, being a newer metric, tends to give very optimistic scores to code that is scored low by humans~\cite{DBLP:journals/corr/abs-2208-03133}"}.
% \qinyuan{Is this a conclusion from our experiments or from some other related work? A bit confusing that it is here}.
% \shushan{Go into more detailed discussion of the results in the Tables 1 and 2? E.g. code-to-text is more susceptible than text-to-code; increased amount of finetuning data provides more serious improvement for code-to-text than to text-to-code. Could this be because of the metrics? Codex is better at generating code that CodeT5 out of the box, meaning CodeT5 definitely needs the training stage.} 
% \qinyuan{Please add this! We need to navigate the reader through the big table and then give out the conclusion}
% \qinyuan{Describe what's happening in the table, (e.g., by comparing xx and yy, we see zz\% performance gap). Also, what's our conclusion from table 2+3?}

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[trim=70 110 80 20,clip,width=\textwidth]{method_codet5.png}
        \caption{CodeT5}
        \label{fig:methods-codet5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[trim=70 140 80 70,clip,width=\textwidth]{method_codex.png}
         \caption{Codex}
        \label{fig:methods-codex}
     \end{subfigure}
        \caption{For the CodeT5 model we perform evaluation of different methods for training and domain adaptation techniques, as well as using different data sources during the domain adaptation stage. For Codex we perform evaluation of scenarios with different data sources during the domain adaptation stage.}
    \label{fig:methods}
\end{figure}


\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[trim=50 0 50 0,clip,width=\textwidth]{result2_codet5.png}
        \caption{CodeT5}
        \label{fig:result2-codet5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[trim=60 0 50 0,clip,width=\textwidth]{result2_codex.png}
         \caption{Codex}
        \label{fig:result2-codex}
     \end{subfigure}
        \caption{
        Performance for models with downstream adaptations on ID and retrieved data.  
        % \qinyuan{(1) Please make sure the x-axis labels has both \textit{the number of shots} and \textit{how they are obtained} (ID, random, retrieved). (2) do we need comparison of 8/16/32 shots? I can see a clear trend of performance improving w.r.t. \# shots but we don't need to show that in every figure? (3) what's our main conclusion in this figure? please include this in the caption.}
        % \shushan{for code gen, the y-axis for folder should go to 30 to be consistent}
        % \shushan{maybe would be more illustrative if y-axes for both code gen and code sum. had the same limits}
        }
\end{figure*}

\begin{figure*}[h]
     \centering
     \includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/result2_var_ret.png}
     \caption{Performance for CodeT5 model finetuned with retrieved supervision, with different number of retrieved examples per test sample. Scores reported are BLEU for code summarization (left-most three plots), and CodeBLEU for code generation (right-most three plots). The performances of the CodeT5 MTL model evaluated in zero-shot, and 8-shot (ID) scenarios are illustrated with dotted lines for reference.}
     \label{fig:result2-var-ret}
\end{figure*}

% \vspace{2cm}
\begin{question}
How to get better out-of-domain generalization?
\end{question}

In the previous question, we saw that models for code, that have been adapted for new domains using in-domain data, performed significantly better. 
However, there are many reasons why adapting to every new domain with the help of labeled examples might be impractical. 
Thus, next, we consider some alternative approaches, that would not require labeled data but would hopefully close the performance gap partially or fully.
A high-level overview of the methods is illustrated in Figure~\ref{fig:methods}.

\subsubsection*{CodeT5}
% CodeT5 has been trained in two sequential stages: training and downstream adaptation.
In the previous setup, we started from a pre-trained checkpoint of the model and experimented with different approaches for domain adaptation. To answer the current question, we additionally consider different methods to use before the domain adaptation stage, particularly, multi-task learning or meta-learning. The resulting experimental setups are illustrated in Figure~\ref{fig:methods-codet5}.
% \qinyuan{We need to provide some context on why these methods potentially help OOD generalization. It's a bit confusing now because we're mixing the concepts of ``domains'' and ``tasks'' right now.}

\paragraph{Multitask learning (MTL)}
MTL is the simple method of combining data from different domains and training the model on all the domains simultaneously. 
% In our experiments, we used $X_{train}$ for MTL. 
For code summarization, we used the model checkpoint provided by the authors of CodeT5, which was fine-tuned on the training portion of CodeSearchNet. 
For code generation, we performed our own training since the original model was not trained to generate JavaScript code. 
For that purpose, we randomly held out 20\% of $X_{train}$ for validation and updated the model on the rest of the data for 150K steps, using a batch size of 4. 
The best checkpoint was selected by evaluating the model on the held-out validation set.

\paragraph{Dual-gen MTL}
In the MTL setup described above, the model is trained to perform either code generation or summarization. 
In addition to that model, we experiment with a multitask model that has been trained on both code generation and code summarization simultaneously. We refer to this model as ``dual-gen'' MTL, following the authors of CodeT5. 
We prepended the inputs for the model with a corresponding instruction for each instance. 
We followed the same train/dev division strategy as for MTL for code generation, and updated the model for 150K steps with batch size of 4. 
The best checkpoints were again decided by evaluating the model on the created development set. 
In particular, we selected two checkpoints - one according to CodeBLEU metric, and another according to BLEU metric for code generation and code summarization respectively.

\paragraph{Model-Agnostic Meta Learning} 
For model-agnostic meta-learning or MaML~\citep{Finn2017ModelAgnosticMF}, we filtered the domains in $X_{train}$ set, only leaving those that have at least 96 samples (see the middle column of Table~\ref{tab:dataset}). 
This was to ensure that each domain contains disjoint sets of adequate size for both training and meta-training. 
 We used the library Higher~\citep{grefenstette2019generalized} for our implementation. We updated the model from the pretrained checkpoint for 10K steps and used the last checkpoint in the rest of the experiments.

\paragraph{Stratified example retrieval for supervision}
The different strategies for training above have the potential to use the information across multiple domains during the training to learn to generalize better out-of-domain. In addition to those, we experiment with a domain adaptation method that does not require in-domain labelled data for supervision. For that purpose, we created another set of examples for supervision -- $\tau_{ret}$. 
We used cosine similarity on embeddings obtained from the pre-trained CodeT5 model checkpoint to \textbf{retrieve} $k$ most similar examples for every example in $\tau_{test}$ from $X_{train}$. 
We set $k$ to 4, 8, or 32, and since $|\tau_{test}|=32$ the combined size of $\tau_{ret}$ set would be 128, 256, or 1024. 
As a final step, we removed any duplicates in $\tau_{ret}$. 

\subsubsection*{Codex}
\paragraph{Stratified example retrieval for demonstrations}
% In our study, we experimented with different strategies for selecting demonstration examples for Codex. 
% Similar to how we did that for downstream adaptation for CodeT5, for Codex we attempted selecting demonstration examples from the test task, as well as randomly from another task. 
% As before, the first scenario is designed to measure the model's performance on the test task when it is ``in-domain'' (\textbf{ICL ID}) since samples from that domain were encountered among demonstrations. 
% The second scenario respectively measures the model's performance when no examples from the test domain have been encountered by the model, which we refer to as ``random'' or \textbf{ICL random}.
Similarly to the strategy for CodeT5, for Codex we employed in-context learning with retrieved demonstration examples. 
For each test query, instead of using random sets of in-domain or out-of-domain demonstrations, we used 4 or 8 of the query's most similar samples from $X_{train}$ as demonstrations. 
This case will be referred to as \textbf{ICL ret}.

\subsubsection*{Finding: Strategic domain adaptation allows best out-of-domain performance in low data scenarios, however requires more resources than general-purpose models}
Figure~\ref{fig:result2-codet5} and~\ref{fig:result2-codex} demonstrate the performance of the CodeT5 and Codex models (additional experimental results are presented in Appendix Section~\ref{app:metrics}). 
For CodeT5, it contains the performance obtained in a zero-shot manner, as well as after in-domain few-shot finetuning as an estimate of how the corresponding models would perform if there was no distribution shift. 
None of the evaluated methods perform comparably in zero-shot setting to those with few-shot domain adaptation - whether on examples from training data or test domains, which means these training methods do not result in a general-purpose model that handles out-of-domain generalization well. 

Adapting the MTL model to test domains with the help of stratified supervision provides a considerable boost to the performance of CodeT5 and Codex.
Results are shown in Figure~\ref{fig:result2-var-ret} with bars marked ``ret $k$'' for CodeT5, where $k$ refers to the number of examples included in $\tau_{ret}$ per test example. 
For Codex the figure reports the performance using 4 or 8 retrieved demonstrations, signified as ``ICL ret 4'' and ``ICL ret 8'' respectively. 

First of all, we notice that there is a saturation in terms of gained performance vs the number of stratified supervision or demonstration examples used. 
For CodeT5 using 32 examples per test instance is always worse than using 4 or 8 examples. 
For Codex, using 4 or 8 examples results in approximately the same performance.

Next, for code summarization, retrieving 4 or 8 examples from out-of-domain train data leads to performance comparable, or even better, than that of the model adapted using 8 examples from the test domain. This trend is observed for both Codex and CodeT5, particularly strongly when generalizing to new repositories and new organizations.  
A similar trend can be observed for code generation, and to a much stronger degree for CodeT5 - stratified supervision models can even outperform models trained with 32 examples from the test domain.

However, as previously mentioned, it appears that the gains achieved by the stratified supervision models with CodeT5 plateau after a certain number of examples, whereas supervision on in-domain samples does not demonstrate such a trend.
Thus, adaptation to new domains using such stratified supervision is a good approach for very low-data scenarios. 

% There is no single best training method for a task - g

% LoRA had a superiour performance in all cases in the previous experiment, but in this one in almost 100\% of cases its performance is inferior to that of FT.

\begin{question}
Can we have more general solutions for out-of-domain generalization?
\end{question}
% \qinyuan{not intuitive what ``cost'' means in this question.}
% Finally, we look at the real role of domain boundaries
% \qinyuan{if we opt to use "domain" in previous section, we should refer to thsi as "domain boundary"} 
% and attempt to minimize the costs of retrieval augmented models. 
In the previous experiment, we saw that models that can generalize to new domains without relying on labeled data.
Unfortunately, this still requires adapting to every test domain individually for CodeT5, and even more strictly - to every test sample individually - for Codex. 
For example, for CodeT5, this would mean having to maintain multiple copies of the model, performing the training for the adaptation stage multiple times, storing a large amount of out-of-domain data as a corpus to retrieve the supervision examples from, etc.  

In this last experiment, we try to create more general models. 
Our interpretation of such a model for CodeT5 is generalizing to multiple domains without needing to train on them separately. 
For Codex, since previously we were obtaining demonstrations for each individual example, we consider sampling from demonstrations collected for the entire domain - in other words, sampling demonstrations from $\tau_{ret}$.  

For CodeT5, we finetune it on the combined set of $\tau_{ret}$ for all domains.
Formally, instead of using $\tau_{i,ret}$ for each $\tau_i$, we use the ${\{\tau_{i,ret}\}}_i$. The results are presented in Table~\ref{tab:result3}, where this experiment is referred to as ``FT: combined $k$'', where $k$ is the number of retrieved examples per test example. 
For each cell in the table, the first number is the raw score obtained by the ``combined FT'' model. It is followed by the difference between the score of the combined model and the score that we had previously obtained with domain-specific models. 
As can be seen, training a single model on combined retrieved samples results in a moderate drop in performance for code summarization, and a negligible drop for code generation. 
In other words, a model finetuned on stratified supervision data for new domains can be a viable solution for the out-of-domain generalization problem for code generation.
This indicates that for code generation, good performance on one domain does not hinder the performance on another domain, i.e. there is little to no negative transfer between different domains.

% To make sure that the drop in performance is not due to the larger size of the supervision data, in another experiment, we shuffled the test examples into new, randomly combined tasks of 32 examples. 
% We then finetuned the model on retrieved examples for this new task.
% The results for this experiment are referred to as ``FT: scrambled $k$'' in Table~\ref{tab:result3}, where $k$ signifies the number of retrieved examples per test example. 
% In this scenario, we observe a large drop in performance which confirms the importance of keeping the task boundaries intact.

For Codex, for a query from $\tau_{test}$, we consider sampling 4 or 8 demonstration examples from $\tau_{ret}$.
This experiment is referred to as ``ICL: $k \text{ from } \tau_{ret}$'' in Table~\ref{tab:result3}, where $k$ is the number of sampled demonstrations. 
The first number in each cell is the raw score obtained for Codex with sampling from similar examples for the domain, and the second number is the difference between that score, and the score obtained with similar demonstrations for each individual test example.
It appears that for Codex replacing demonstrations selected for individual examples with those selected for a domain introduce too much noise, and degrade the performance a lot. 

% Notably, for Codex, there is still a hefty improvement over no demonstrations or random demonstrations, so this approach can be beneficial depending on the use case. 
% \qinyuan{how does this support your conclusion on domain boundaries?}

% From this, we conclude, that such a definition of domain boundaries is not only convenient for humans but is also meaningful for training models, as our experiments show that erasing the boundaries between tasks leads to decreased performance.


\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Code Summarization\\ 
 BLEU / $\Delta$ BLEU\end{tabular}} & \multicolumn{3}{@{\hskip 0.6cm}c}{\begin{tabular}[c]{@{}c@{}}Code Generation\\ CodeBLEU / $\Delta$ CodeBLEU\end{tabular}} \\
 \cmidrule(lr){2-4}
 \cmidrule(lr){5-7}
 & \multicolumn{1}{c}{org} & \multicolumn{1}{c}{repo} & \multicolumn{1}{c}{folder} & \multicolumn{1}{c}{org} & \multicolumn{1}{c}{repo} & \multicolumn{1}{c}{folder} \\
\midrule
FT: combined 4 
& 18.74 / -4.74 & 18.59 / -4.47 & 18.06 / -1.06 & 29.46 / -0.19 & 29.41 / -0.01 & 26.60 / -1.53 \\
FT: combined 8 
& 18.46 / -5.07 & 18.58 / -3.03 & 17.57 / -3.48 & 29.13 / -0.73 & 28.83 / -0.22 & 27.23 / -0.92 \\
FT: combined 32 
& 17.35 / -2.31 & 17.63 / -0.94 & 15.57 / -2.56 & 26.28 / -3.63 & 25.01 / -4.02 & 25.14 / -2.88 \\
% FT: scrambled 4 
% & 4.98 / -18.49 & 5.26 / -17.79 &  5.46 / -13.66 & 19.83 / -9.81 & 19.94 / -9.48 & 20.31 / -7.83 \\
% FT: scrambled 8 
% & 5.28 / -18.25 & 5.52 / -16.09 & 6.11 / -14.95 & 20.24 / -9.63 & 19.99 / -9.07 & 20.21 / -7.94 \\
% FT: scrambled 32 
% & 5.20 / -14.46 & 5.13 / -13.44 & 5.43 / -12.71 & 19.38 / -10.52 & 19.01 / -10.02 & 20.17 / -7.85  \\
\midrule
ICL: 4 from $\tau_{ret}$
& 14.66 / -7.04 & 12.68 / -7.95 & 12.10 / -6.96 & 20.52 / -6.73 & 20.06 / -7.78 & 19.39 / -6.21 \\
ICL: 8 from $\tau_{ret}$
& 13.77 / -8.53 & 12.96 / -8.52 & 12.26 / -7.17 & 20.81 / -7.05 & 20.23 / -8.16 & 19.48 / -7.00\\
\bottomrule
\end{tabular}}
\caption{ Results for models using retrieved supervision examples in modified scenarios.
}
\label{tab:result3}
\end{table*}

\section{Limitations and Threats to Validity}
As can be seen from Table~\ref{tab:dataset}, as a result of the process of filtering, we skew the data towards larger projects and eliminate from the dataset many samples that could potentially come from smaller projects. 
We believe that this step is necessary to make the results more reliable, due to the high variance that can be observed in datasets with very small test sets. 
However, we wanted to draw attention to this circumstance once more, to make sure that our findings are interpreted correctly when considering applications of large language models to new contexts.

Additionally, while we do evaluate distribution shift and out-of-domain generalization, we believe it is important to highlight again that the out-of-domain data in our analysis still originated from the same dataset. 
Thus its distribution is likely closer to the original training set distribution than it will be the case for language models of code in the wild.

\section{Conclusion}
In this work, we systematically evaluated two large language models for code - CodeT5 and Codex (code-cushman-001) - on two fundamental code applications - code generation and code summarization. 
Our goal was to understand, how the models perform under distribution shifts that can commonly occur due to the nature of the software. 
We experimented with three granularities for defining domains in applications for code - organization level, project level, and module, or folder, level. 
Our experiments showed that both CodeT5 and Codex are susceptible to reduced performance due to domain shifts in all three scenarios. 
We experimented with a number of training and domain adaptation techniques for achieving better out-of-domain generalization. 
As a result of our experiments, we discovered that retrieving examples from training data that are similar to the test instances is the most effective approach for adapting to a new, low-resource domain. 
In addition, we experimented with making adapted models applicable to multiple new domains simultaneously. 
We found that such models can perform very well for code generation. 
However, in the case of code summarization, we found the generality of the model to be a tradeoff for its performance.


\bibliographystyle{iclr2023_conference}
\bibliography{bibliography}
\input{9-appendix.tex}
\end{document}