\newpage
\section{Appendix}
\subsection{Javascript Keywords}
The Javascript keywords that we included in the CodeBleu implementation for evaluation is listed in table \ref{app:keywords}.
\label{app:keywords}
\begin{table*}[t]
\centering
\begin{tabular}{|l|p{0.9\linewidth}|}
\hline
\multicolumn{1}{|c|}{\textit{\textbf{Languages}}} & \multicolumn{1}{c|}{\textit{\textbf{Keywords}}}                                                   \tabularnewline \hline
%  go                                                & break. default, func, interface, select, case, defer, go, map, struct, chan, else, goto, package, switch, const, fallthrough, if, range, type, continue, for, import, return, var                                                  \tabularnewline \hline
% java                                              & abstract, assert, boolean, break, byte, case, catch, char, class, const, continue, default, do, double, else, enum, extends, final, finally, float, for, goto, if, implements, import, instanceof, int, interface, long, native, new, package, private, protected, public, return, short, static, strictfp, super, switch, synchronized, this, throw, throws, transient, try, void, volatile, while \tabularnewline \hline
javascript                                             & await, break, case, catch, class, const, continue, debugger, default, delete, do, else, enum, export, extends, false, finally, for, function, if, implements, import, in, instanceof, interface, let, new, null, package, private, protected, public, return, super, switch, static, this, throw, try, true, typeof, var, void, while, with, yield 
\tabularnewline \hline
% php                                            & \_\_halt\_compiler, abstract, and, array, as, break, callable, case, catch, class, clone, const, continue, declare, default, die, do, echo, else, elseif, empty, enddeclare, endfor, endforeach, endif, endswitch, endwhile, eval, exit, extends, final, finally, fn, for, foreach, function, global, goto, if, implements, include, include\_once, instanceof, insteadof, interface, isset, list, match, namespace, new, or, print, private, protected, public, readonly, require, require\_once, return, static, switch, throw, trait, try, unset, use, var, while, xor, yield, yield from\tabularnewline \hline
% python                                            & False, None, True, and, as, assert, async, await, break, class, continue, def, del, elif, else, except, finally, for, from, global, if, import, in, is, lambda, nonlocal, not, or, pass, raise, return, try, while, with, yield \tabularnewline \hline

\end{tabular}
\caption{Keywords used for CodeBLEU evaluation}
\label{table:keywords}   
\end{table*}

% \subsection{Code-search}
% This is the task of retrieving code from a textual description of its functionality. We implemented this task for CodeT5 only, since the architecture of Copilot model makes search unnatural and expensive to implement. For code search, the code and text sequences are encoded using the CodeT5 model, and their embeddings are concatenated and passed into a two-layer linear classification layer(We can provide more details about this in the appendix with information about layer size). The classification layer and CodeT5 model are trained jointly in every configuration.
% We formatted the training data for this task to include a negative sample against every positive sample. The negative samples were generated by pairing every input doc-string with a randomly selected sample from the training data, excluding the corresponding code snippet. We follow the same evaluation metric used in CodeBERT \citep{Feng2020CodeBERTAP}. We calculate the Mean Reciprocal Rank(MRR) for each test (text, code) data pair over a set of 31 distractor codes.

\subsection{Extended Background}\label{app:extended background}

% Pretext & finetuning?
% few-shot finetuning
%% meta-learning
%% param-efficient finetuning
%% in-context learning
\subsubsection{Meta-learning and Multi-task-learning}
\paragraph{Meta-learning} focuses on adapting previous knowledge gained from previous tasks to be applied to new tasks with limited training examples. Most meta-learning algorithms can be categorized into three groups: 1) Memory-based approaches~\citep{Santoro2016MetaLearningWM} utilize memory-augmented neural network to rapidly assimilate new data and leverage this data to make accurate predictions after only a few samples; 2) Metric-based frameworks~\citep{Oreshkin2018TADAMTD, Snell2017PrototypicalNF, Sung2017LearningTC, Tseng2020CrossDomainFC} make predictions by referring to the features encoded from the input data and training instances in a generic metric space; 3) Gradient-based methods~\citep{Finn2017ModelAgnosticMF, Finn2018ProbabilisticMM, Antoniou2018HowTT} uses gradient descent to learn a model with few-shot capabilites. In our work, we are using the MAML~\citep{Finn2017ModelAgnosticMF} approach, which is a gradient-based method and learns model initialization (i.e., initial parameters) that is amenable to fast fine-tuning with few instances. This method is a conceptually simple and general algorithm that has been shown to outperform existing approaches in several tasks.
\paragraph{Multi-task Learning} aims to jointly learn several related tasks providing a generalized representation with the added benefit of 
 compute and memory in terms of shared model parameters~\citep{Yang2016DeepMR, Caruana1997MultitaskL, Meyerson2019ModularUR}. Unlike Meta-Learning, conventional MTL is a single-level optimization without a meta-objective. Furthermore, the goal of MTL is to solve a fixed number of known tasks, whereas the point of meta-learning is often to solve unseen future tasks. 

In our work, we have experimented with both MAML and multi-task learning to check which of the method gives us a better initialization for few-shot perfomance in our setting.
\subsubsection{Few-shot Methods}
\paragraph{Parameter-efficient finetuning:}Conventional fine-tuning methods retrains all the model parameters for every new task, which becomes infeasible as the model size increases to the level of GPT-3. In recent times, parameter-efficient methods have been studied and it has been demonstrated that state-of-the-art PEFT methods can match the performance of finetuning all the model's parameters while updating only a tiny fraction of the model parameters. Initially adapters~\citep{Raffel2019ExploringTL, Houlsby2019ParameterEfficientTL, Bapna2019SimpleSA} were introduced, which are new feed-forward modules added between the layers of the fixed pre-trained model. Since then, various sophisticated PEFT methods have been proposed, including methods like LoRA that produce low-rank updates~\citep{Hu2021LoRALA} and prompt tuning ~\citep{Lester2021ThePO} and prefix-tuning ~\citep{Li2021PrefixTuningOC} concatenate learned continuous embeddings to the modelâ€™s input or activations to induce it to perform a task. 

\paragraph{Retrieval-based Example selection:} In a study conducted by \citet{Liu2021WhatMG} , they explored how different prompts can impact the performance of GPT-3 and found that the use of in-context examples has a significant influence on the downstream results. To achieve this, they utilized an unsupervised sentence encoder to encode training examples and then retrieved the nearest neighbors for each test instance. On a similar note, \citet{Das2021CasebasedRF} developed a supervised prompt retriever for answering knowledge-based questions. Their method used tailored supervision specifically designed for knowledge-based queries and relied on surface similarity between formal queries. Furthermore,  \citet{Shin2021ConstrainedLM} employed GPT-3 to select examples for the prompt in few-shot semantic parsing. They demonstrated the effectiveness of this approach by using GPT-3 to identify relevant examples for the prompt, which in turn improved the overall performance of the system.

\subsection{Models}\label{app:model}
\paragraph{CodeT5:} CodeT5~\citep{Wang2021CodeT5IU} is a pretrained encoder-decoder transformer model based on T5~\citep{Raffel2019ExploringTL} for programming languages. It uses a unified framework to support code understanding and generation tasks seamlessly. To improve the model's ability to handle the unique characteristics of programming languages, CodeT5 is trained on an identifier-aware pretraining task. Additionally, the model is trained to exploit  user-written code comments with a bimodal dual-generation task for better alignment between natural language and programming languages. This makes this model suitable for the applications that we consider. For both of our applications, we used the CodeT5-large model~\citep{DBLP:journals/corr/abs-2207-01780} without making any changes to the model architecture.

\paragraph{Codex} Codex~\citep{Chen2021EvaluatingLL} is the language model for code released by OpenAI. It is a GPT language model finetuned on 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. VLLMs are capable of zero-shot generalization to unseen tasks, which is achieved by providing them with an \textit{instruction} of what the model is expected to do. This allowed us to successfully evaluate Codex for both code generation and code summarization without any need for training. 
% Providing language models with natural language descriptions of tasks, as proposed by ~\citet{Radford2019LanguageMA} has led to significant developments in few-shot learning. 
% GPT-3 ~\citep{Brown2020LanguageMA} demonstrated the ability of large language models to perform few-shot predictions, where the model is given a description of the task in natural language with few examples. Scaling model size, data, and computing is crucial to enable this learning ability, leading to the further development of
% large models.  In our work, we conducted experiments on in-context learning on Codex
% % \todo{What are we going to call it? copilot/codex} 
% which is the programming language version of GPT-3. 

% \begin{table*}[]
% \centering
% \resizebox{0.9\textwidth}{!}{%
% \begin{tabular}{lrrrrrrrrr}
% \toprule
% \multicolumn{1}{c}{\multirow{2}{*}{\textbf{code-search}}} & \multicolumn{3}{c}{\textbf{folder}}                                                                          & \multicolumn{3}{c}{\textbf{repo}}                                                                            & \multicolumn{3}{c}{\textbf{org}}                                                                             \\
% \multicolumn{1}{c}{}                                       & \multicolumn{1}{c}{\textbf{8-shot}} & \multicolumn{1}{c}{\textbf{16-shot}} & \multicolumn{1}{c}{\textbf{32-shot}} & \multicolumn{1}{c}{\textbf{8-shot}} & \multicolumn{1}{c}{\textbf{16-shot}} & \multicolumn{1}{c}{\textbf{32-shot}} & \multicolumn{1}{c}{\textbf{8-shot}} & \multicolumn{1}{c}{\textbf{16-shot}} & \multicolumn{1}{c}{\textbf{32-shot}} \\
% \midrule
% Random task FS FT & ? & ? & ? & ? & ? & ? & ? & ? & ? \\
% Test task FS FT & 0.13 & 0.14 & 0.13 & 0.14 & 0.14 & 0.14 & 0.14 & 0.14 & 0.14 \\
% Multitask full dataset FT & 0.71 & - & - & 0.73 & - & - & 0.68 & - & - \\
% Multitask full dataset FT + Test task FS FT & 0.72 & 0.72 & 0.73 & 0.73 & 0.74 & 0.74 & 0.69 & 0.69 & 0.70 \\    
% \bottomrule          
% \end{tabular}
% }
% \caption{Metrics reported are Mean Reciprocal Rank}
% \label{tab:code-search}
% \end{table*}

\section{Additional experimental results}
Besides the experiments presented in the main paper, in this section, we report some additional experiments, such as the results for code generation as measured using chrF and rougeL metrics, or comparison of LoRA parameter efficient finetuning method with the full model finetuning for CodeT5. 
\label{app:metrics}

% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lrrrrrrrrr}
% \toprule
% \multicolumn{1}{c}{
%     \multirow{2}{*}{\textbf{Code summarization}}
% } &
% \multicolumn{3}{c}{\textbf{folder}} & 
% \multicolumn{3}{c}{\textbf{repo}} & 
% \multicolumn{3}{c}{\textbf{org}} \\
% \cmidrule(lr){2-4}
% \cmidrule(lr){5-7}
% \cmidrule(lr){8-10}
% \multicolumn{1}{c}{} & 
% \multicolumn{1}{c}{\textbf{8-shot}} & 
% \multicolumn{1}{c}{\textbf{16-shot}} & 
% \multicolumn{1}{c}{\textbf{32-shot}} & 
% \multicolumn{1}{c}{\textbf{8-shot}} & 
% \multicolumn{1}{c}{\textbf{16-shot}} & 
% \multicolumn{1}{c}{\textbf{32-shot}} & 
% \multicolumn{1}{c}{\textbf{8-shot}} & 
% \multicolumn{1}{c}{\textbf{16-shot}} & 
% \multicolumn{1}{c}{\textbf{32-shot}} \\
% \midrule
% CodeT5 FT ID 
% %folder (8/16/32)
% & 14.39 & 16.06 & 18.31
% %repo (8/16/32)
% & 12.68 & 14.73 & 16.82
% %org (8/16/32)
% & 13.14 & 16.35 & 17.65  \\
% CodeT5 LoRA ID 
% %folder (8/16/32)
% & 16.57 & 19.07 & 20.93
% %repo (8/16/32)
% & 15.22 & 17.14 & 21.20
% %org (8/16/32)
% & 15.61 & 18.56 & 20.87  \\
% CodeT5 FT random
% %folder (8/16/32)
% & 3.58 & 4.30 &  5.02
% %repo (8/16/32)
% & 4.35 & 4.70 & 5.79
% %org (8/16/32)
% & 4.53 & 5.47 & 6.27  \\
% CodeT5 LoRA random 
% %folder (8/16/32)
% & 3.69 & 4.37 & 4.92
% %repo (8/16/32)
% & 4.70 & 5.56 & 5.92
% %org (8/16/32)
% & 5.27 & 5.53 &  6.26 \\
% \midrule
% Codex ICL ID
% %folder (8/16/32)
% % & \multicolumn{3}{c}{20.72} & 
% % \multicolumn{3}{c}{20.34} & 
% % \multicolumn{3}{c}{19.00} \\
% & 20.72 & - & -
% %repo (8/16/32)
% & 20.34 & - & -
% %org (8/16/32)
% & 19.00 & - & -  \\
% Codex ICL random
% % folder (8/16/32)
% & 6.73 & - & -
% %repo (8/16/32)
% & 7.17 & - & -
% %org (8/16/32)
% & 6.84 & - & -  \\
% % \multicolumn{3}{c}{6.73} & 
% % \multicolumn{3}{c}{7.17} & 
% % \multicolumn{3}{c}{6.84} \\
% Codex instr. only \textit{(0-shot)}
% %folder (8/16/32)
% & (1.61) & - & -
% %repo (8/16/32)
% & (1.55) & - & -
% %org (8/16/32)
% & (1.52) & - & -  \\
% % \multicolumn{3}{c}{} & 
% % \multicolumn{3}{c}{} & 
% % \multicolumn{3}{c}{} \\
% \bottomrule          
% \end{tabular}
% }
% \caption{Comparison of model performance for code summarization on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is BLEU (higher is better). 
% % \qinyuan{The caption is confusing. My understanding is that $\tau_{test}$ is fixed while we control $\tau_{train}$ to be ID/random? The caption sounds like $\tau_{train}$ is fixed and $\tau_{test}$ is varying. (Or did I interpret the whole thing wrong?)}
% }
% \label{tab:code-to-text-full}
% \end{table*}

% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lrrrrrrrrr}
% \toprule
% \multicolumn{1}{c}{
%     \multirow{2}{*}{\textbf{Code generation}}
% } &
% \multicolumn{3}{c}{\textbf{folder}} & 
% \multicolumn{3}{c}{\textbf{repo}} & 
% \multicolumn{3}{c}{\textbf{org}} \\
% \cmidrule(lr){2-4}
% \cmidrule(lr){5-7}
% \cmidrule(lr){8-10}
% \multicolumn{1}{c}{} & 
% \multicolumn{1}{c}{\textbf{8-shot}} & 
% \multicolumn{1}{c}{\textbf{16-shot}} & 
% \multicolumn{1}{c}{\textbf{32-shot}} & 
% \multicolumn{1}{c}{\textbf{8-shot}} & 
% \multicolumn{1}{c}{\textbf{16-shot}} & 
% \multicolumn{1}{c}{\textbf{32-shot}} & 
% \multicolumn{1}{c}{\textbf{8-shot}} & 
% \multicolumn{1}{c}{\textbf{16-shot}} & 
% \multicolumn{1}{c}{\textbf{32-shot}} \\
% \midrule
% CodeT5 FT ID 
% %folder (8/16/32)
% & 14.67 & 15.22 & 16.13
% %repo (8/16/32)
% & 16.15 & 17.42 & 18.62
% %org (8/16/32)
% & 14.54 & 15.34 & 16.43  \\
% CodeT5 LoRA ID 
% %folder (8/16/32)
% & 14.14 & 15.06 & 16.36
% %repo (8/16/32)
% & 16.23 & 17.45 & 18.96
% %org (8/16/32)
% & 14.17 & 15.30 & 16.62 \\
% CodeT5 FT random 
% %folder (8/16/32)
% & 15.23 & 14.94 & 15.15
% %repo (8/16/32)
% & 14.19 & 14.14 & 14.67
% %org (8/16/32)
% & 13.39 & 13.43 & 14.44 \\
% CodeT5 LoRA random 
% %folder (8/16/32)
% & 14.45 & 14.29 & 15.37
% %repo (8/16/32)
% & 14.29 & 13.74 & 15.04
% %org (8/16/32)
% & 13.76 & 13.85 & 14.81\\
% \midrule
% Codex ICL ID
% %folder (8/16/32)
% & 23.87 & - & - 
% %repo (8/16/32)
% & 25.73 & - & - 
% %org (8/16/32)
% & 24.64 & - & -  \\
% Codex ICL random
% %folder (8/16/32)
% & 16.82 & - & - 
% %repo (8/16/32)
% & 16.82 & - & - 
% %org (8/16/32)
% & 17.47 & - & - \\
% Codex instr. only \textit{(0-shot)}
% %folder (8/16/32)
% & (5.77) & - & - 
% %repo (8/16/32)
% & (5.49) & - & - 
% %org (8/16/32)
% & (5.72) & - & - \\
% \bottomrule          
% \end{tabular}
% }
% \caption{Comparison of model performance for code generation on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is CodeBLEU (higher is better).
% }
% \label{tab:text-to-code-codebleu-full}
% \end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{
    \multirow{2}{*}{\textbf{Code generation}}
} &
\multicolumn{3}{c}{\textbf{folder}} & 
\multicolumn{3}{c}{\textbf{repo}} & 
\multicolumn{3}{c}{\textbf{org}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} \\
\midrule
CodeT5 FT ID 
%folder (8/16/32)
& 19.36 &	20.92 & 21.95
%repo (8/16/32)
& 20.42 &	22.44 &	24.47
%org (8/16/32)
& 19.29	& 20.73	& 22.6  \\
CodeT5 LoRA ID 
%folder (8/16/32)
& 20.05 &	21.66 &	22.56
%repo (8/16/32)
& 20.81 &	23.12 &	24.52
%org (8/16/32)
& 20.08	& 21.28	& 22.99\\
CodeT5 FT random 
%folder (8/16/32)
& 17.61	& 18.03 &	17.94
%repo (8/16/32)
& 16.92	& 17.50 & 17.59
%org (8/16/32)
& 16.47	& 17.46	& 17.85 \\
CodeT5 LoRA random 
%folder (8/16/32)
& 17.87	& 18.02 &	17.81
%repo (8/16/32)
& 17.45	& 17.15 &	17.63
%org (8/16/32)
& 17.24	& 17.13 &	17.29\\
\midrule
Codex ICL ID
%folder (8/16/32)
& 28.78 & - & - 
%repo (8/16/32)
& 31.05 & - & - 
%org (8/16/32)
& 29.19 & - & -  \\
Codex ICL random
%folder (8/16/32)
& 20.62 & - & - 
%repo (8/16/32)
& 20.87 & - & - 
%org (8/16/32)
& 21.10 & - & - \\
Codex instr. only \textit{(0-shot)}
%folder (8/16/32)
& (10.24) & - & - 
%repo (8/16/32)
& (10.6) & - & - 
%org (8/16/32)
& () & - & - \\
\bottomrule          
\end{tabular}
}
\caption{Comparison of model performance for code generation on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is ChrF (higher is better).
}
\label{tab:text-to-code-chrf}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{
    \multirow{2}{*}{\textbf{Code generation}}
} &
\multicolumn{3}{c}{\textbf{folder}} & 
\multicolumn{3}{c}{\textbf{repo}} & 
\multicolumn{3}{c}{\textbf{org}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} \\
\midrule
CodeT5 FT ID 
%folder (8/16/32)
& 14.15 &	15.84 & 16.73
%repo (8/16/32)
& 14.93	& 16.98	& 19.19
%org (8/16/32)
& 13.75	& 14.93	& 16.94  \\
CodeT5 LoRA ID 
%folder (8/16/32)
& 14.49	& 16.58	& 17.87
%repo (8/16/32)
& 15.47	& 17.69	& 19.60
%org (8/16/32)
& 14.10	& 15.48	& 17.61 \\
CodeT5 FT random 
%folder (8/16/32)
& 11.34	& 11.62	& 11.73
%repo (8/16/32)
& 9.91	& 10.10	& 10.32
%org (8/16/32)
& 9.49	& 10.20	& 10.68 \\
CodeT5 LoRA random 
%folder (8/16/32)
& 11.45	& 12.05	& 12.58
%repo (8/16/32)
& 10.09	& 10.04	& 11.08
%org (8/16/32)
& 10.15	& 10.3	& 11.15\\
\midrule
Codex ICL ID
%folder (8/16/32)
& 23.70 & - & - 
%repo (8/16/32)
& 24.62 & - & - 
%org (8/16/32)
& 22.58 & - & -  \\
Codex ICL random
%folder (8/16/32)
& 15.76 & - & - 
%repo (8/16/32)
& 15.67 & - & - 
%org (8/16/32)
& 15.81 & - & - \\
Codex instr. only \textit{(0-shot)}
%folder (8/16/32)
& (6.44) & - & - 
%repo (8/16/32)
& (6.5) & - & - 
%org (8/16/32)
& (5.72) & - & - \\
\bottomrule          
\end{tabular}
}
\caption{Comparison of model performance for code generation on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is RougeL (higher is better).
}
\label{tab:text-to-code-rougel}
\end{table*}

\begin{figure*}[h]
     \centering
     \includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/result2_codet5_lora.png}
     \caption{Performance for CodeT5 model finetuned with LoRA compared to regular finetuning.}
     \label{fig:result2-codet5-lora}
\end{figure*}
