
% \subsection{Cross-Task}
% The models we will consider are typically pretrained on NL and PL, so the following categorization refers to the upstream vs downstream tasks\\
% 1. Different task? (e.g. summarization vs paraphrasing vs extreme summarization, so keep the same modalities in the input and output compared to the upstream task)\\
% 2. Different objective, (i.e. generation to classification)\\
% 3. Different modality (e.g. PL to NL model vs PL to PL model)\\

% Our current tasks according to their modalities are:\\
% \textbf{Code Generation }(NL to PL) \\
% \textbf{Code Search} (NL \& PL) \\
% \textbf{Code Summarization} (PL to NL) \\

% Different tasks can further be split into ``sub-tasks'' for every individual language. Ideally, want to conduct this setting in an even distribution of languages between upstream and downstream tasks. \\
% \begin{itemize}
%     \item Code Review Generation (PL to NL) - check how many functions are the only thing in the diff between two repositories, to make sure that the entire review concerns that function only.
%     \item Code Translation (PL to PL)
%     \item Code Refinement (PL to PL)
%     \item Code Completion (PL to PL)
%     \item Assert generation (PL to PL)
%     \item Defect Detection (PL classification)
%     \item Clone Detection (PL classification)
% \end{itemize}


% \subsection{Cross-Language}

% \section{Tasks}
% \subsection{Semantic Code Search}
% \subsection{Code Generation}
% \subsection{Code Summarization}




% \begin{table}[]
% \begin{tabular}{ll}
%  ??? & ??? \\
%  ??? & ???
% \end{tabular}
% \caption{Dataset statistics for the Cross-Domain split}
% \label{tab:cross-domain-stats}
% \end{table}


% \subsubsection{User}
% Source: CodeSearchNet (Python), Google Bigquery Github Dataset (Python)

% Target: CodeSearchNet (Python), Google Bigquery Github Dataset (Python)

% \subsubsection{Profficiency level (lower priority)}
% Source: COSET, Project CodeNet, Avatar, ALG-109, CodeContests, MBPP

% Target: CodeSearchNet, Google Bigquery Github Dataset

% \subsection{Cross-Task}

% Come up with partitions that represent different difficulty levels and correspond to different hypothesis.
% \begin{itemize}
% \item {Code generation (CSN) - by function, by snippet, by line}
% \item {Code summary generation (CSN) - by function, by snippet, by line}
% \item {Code search (CSN)}
% \item {Code completion (CSN)}
% \item {Code repair (need to confirm Bug2Fix dataset size when split by projects)}
% \item {Segment tagging - single statement bug tagging}
% \item {Code review generation (need to confirm dataset size when split by projects)}
% \item{Variable misuse} (CSN)
% \end{itemize}

% % \subsubsection{New Task}
% % Source: CodeSearchNet (Python), Google Bigquery Github Dataset (Python)

% % Target: Code Review Open Platform (C++, Java, Javascript, Go, Python)  (summarization); CodeSearchNet (Python), Google Bigquery Github Dataset (Python) (code completion)

% % \subsubsection{Task/Scale/}
% %  Source: CodeSearchNet (Python), Google Bigquery Github Dataset (Python), CoST (snippet), Django(line), CoSQA (snippet)  

% % Target: CodeSearchNet (Python), Google Bigquery Github Dataset (Python), CoST (snippet), Django(line), CoSQA (snippet)

% \subsection{Cross-Language}

% Matlab, R
