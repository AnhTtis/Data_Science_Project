\clearpage
\newpage
\section{Appendix}
\subsection{Javascript Keywords}\label{app:js-kwords}
The Javascript keywords that we included in the CodeBleu implementation for evaluation is listed in table \ref{app:keywords}.
\label{app:keywords}
\begin{table*}[t]
\centering
\begin{tabular}{|l|p{0.8\textwidth}|}
\hline
\multicolumn{1}{|c|}{\textit{\textbf{Languages}}} & \multicolumn{1}{c|}{\textit{\textbf{Keywords}}}                                                   \tabularnewline \hline
JavaScript                                             & await, break, case, catch, class, const, continue, debugger, default, delete, do, else, enum, export, extends, false, finally, for, function, if, implements, import, in, instanceof, interface, let, new, null, package, private, protected, public, return, super, switch, static, this, throw, try, true, typeof, var, void, while, with, yield 
\tabularnewline \hline
\end{tabular}
\caption{Keywords used for CodeBLEU evaluation}
\label{table:keywords}   
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{
    \multirow{2}{*}{\textbf{Code generation}}
} &
\multicolumn{3}{c}{\textbf{folder}} & 
\multicolumn{3}{c}{\textbf{repo}} & 
\multicolumn{3}{c}{\textbf{org}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} \\
\midrule
CodeT5 FT ID 
%folder (8/16/32)
& 19.36 &	20.92 & 21.95
%repo (8/16/32)
& 20.42 &	22.44 &	24.47
%org (8/16/32)
& 19.29	& 20.73	& 22.6  \\
CodeT5 LoRA ID 
%folder (8/16/32)
& 20.05 &	21.66 &	22.56
%repo (8/16/32)
& 20.81 &	23.12 &	24.52
%org (8/16/32)
& 20.08	& 21.28	& 22.99\\
CodeT5 FT random 
%folder (8/16/32)
& 17.61	& 18.03 &	17.94
%repo (8/16/32)
& 16.92	& 17.50 & 17.59
%org (8/16/32)
& 16.47	& 17.46	& 17.85 \\
CodeT5 LoRA random 
%folder (8/16/32)
& 17.87	& 18.02 &	17.81
%repo (8/16/32)
& 17.45	& 17.15 &	17.63
%org (8/16/32)
& 17.24	& 17.13 &	17.29\\
\midrule
Codex ICL ID
%folder (8/16/32)
& 28.78 & - & - 
%repo (8/16/32)
& 31.05 & - & - 
%org (8/16/32)
& 29.19 & - & -  \\
Codex ICL random
%folder (8/16/32)
& 20.62 & - & - 
%repo (8/16/32)
& 20.87 & - & - 
%org (8/16/32)
& 21.10 & - & - \\
Codex instr. only \textit{(0-shot)}
%folder (8/16/32)
& (10.24) & - & - 
%repo (8/16/32)
& (10.60) & - & - 
%org (8/16/32)
& (10.25) & - & - \\
\bottomrule          
\end{tabular}
}
\caption{Comparison of model performance for code generation on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is ChrF (higher is better).
}
\label{tab:text-to-code-chrf}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{
    \multirow{2}{*}{\textbf{Code generation}}
} &
\multicolumn{3}{c}{\textbf{folder}} & 
\multicolumn{3}{c}{\textbf{repo}} & 
\multicolumn{3}{c}{\textbf{org}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} \\
\midrule
CodeT5 FT ID 
%folder (8/16/32)
& 14.15 &	15.84 & 16.73
%repo (8/16/32)
& 14.93	& 16.98	& 19.19
%org (8/16/32)
& 13.75	& 14.93	& 16.94  \\
CodeT5 LoRA ID 
%folder (8/16/32)
& 14.49	& 16.58	& 17.87
%repo (8/16/32)
& 15.47	& 17.69	& 19.60
%org (8/16/32)
& 14.10	& 15.48	& 17.61 \\
CodeT5 FT random 
%folder (8/16/32)
& 11.34	& 11.62	& 11.73
%repo (8/16/32)
& 9.91	& 10.10	& 10.32
%org (8/16/32)
& 9.49	& 10.20	& 10.68 \\
CodeT5 LoRA random 
%folder (8/16/32)
& 11.45	& 12.05	& 12.58
%repo (8/16/32)
& 10.09	& 10.04	& 11.08
%org (8/16/32)
& 10.15	& 10.30	& 11.15\\
\midrule
Codex ICL ID
%folder (8/16/32)
& 23.70 & - & - 
%repo (8/16/32)
& 24.62 & - & - 
%org (8/16/32)
& 22.58 & - & -  \\
Codex ICL random
%folder (8/16/32)
& 15.76 & - & - 
%repo (8/16/32)
& 15.67 & - & - 
%org (8/16/32)
& 15.81 & - & - \\
Codex instr. only \textit{(0-shot)}
%folder (8/16/32)
& (6.44) & - & - 
%repo (8/16/32)
& (6.50) & - & - 
%org (8/16/32)
& (6.18) & - & - \\
\bottomrule          
\end{tabular}
}
\caption{Comparison of model performance for code generation on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric is RougeL (higher is better).
}
\label{tab:text-to-code-rougel}
\end{table*}


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{
    \multirow{2}{*}{\textbf{Code generation}}
} &
\multicolumn{3}{c}{\textbf{folder}} & 
\multicolumn{3}{c}{\textbf{repo}} & 
\multicolumn{3}{c}{\textbf{org}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} & 
\multicolumn{1}{c}{\textbf{8-shot}} & 
\multicolumn{1}{c}{\textbf{16-shot}} & 
\multicolumn{1}{c}{\textbf{32-shot}} \\
\midrule
CodeT5 FT ID 
%folder (8/16/32)
& 0.68 / 0.68	& 0.69 / 0.68 & 0.69 / 0.69
%repo (8/16/32)
& 0.69 / 0.69&	0.69 / 0.68&	0.68 / 0.67
%org (8/16/32)
& 0.69 / 0.67	&0.69 / 0.68	&0.70 / 0.69  \\
CodeT5 LoRA ID 
%folder (8/16/32)
& 0.68 / 0.67	& 0.69 / 0.68	& 0.70 / 0.69
%repo (8/16/32)
& 0.69 / 0.68	&0.70 / 0.70	&0.71 / 0.71
%org (8/16/32)
& 0.69 / 0.68	&0.69 / 0.68	&0.71 / 0.69 \\
CodeT5 FT random 
%folder (8/16/32)
& 0.65 / 0.66	&0.66 / 0.66	&0.66 / 0.66
%repo (8/16/32)
& 0.66 / 0.65	&0.66 / 0.66	&0.66 / 0.66
%org (8/16/32)
& 0.65 / 0.65	&0.65 / 0.65	&0.65 / 0.65\\
CodeT5 LoRA random 
%folder (8/16/32)
& 0.65 / 0.65 &0.65 / 0.65	&0.66 / 0.66
%repo (8/16/32)
& 0.66 / 0.66	&0.65 / 0.65	&0.66 / 0.66
%org (8/16/32)
& 0.65 / 0.65	&0.65 / 0.65	&0.66 / 0.66\\
\midrule
Codex ICL ID
%folder (8/16/32)
& 0.74 / 0.72 & - & - 
%repo (8/16/32)
& 0.75 / 0.73 & - & - 
%org (8/16/32)
& 0.74 / 0.72 & - & -  \\
Codex ICL random
%folder (8/16/32)
& 0.69 / 0.67 & - & - 
%repo (8/16/32)
& 0.70 / 0.68 & - & - 
%org (8/16/32)
& 0.69 / 0.67 & - & - \\
Codex instr. only \textit{(0-shot)}
%folder (8/16/32)
& 0.62 / 0.61 & - & - 
%repo (8/16/32)
& 0.63 / 0.62 & - & - 
%org (8/16/32)
& 0.63 / 0.62 & - & - \\
\bottomrule          
\end{tabular}
}
\caption{Comparison of model performance for code generation on in-domain (\textbf{ID}) vs out-of-domain (\textbf{random}) test data. Reported metric in each cell is CodeBERTScore F1 on the left (higher is better), and CodeBERTScore F3 on the right (higher is better).
}
\label{tab:text-to-code-codebertscore}
\end{table*}

\subsection{Extended Background}\label{app:extended background}

\subsubsection{Meta-learning and Multi-task-learning}
\paragraph{Meta-learning} focuses on adapting knowledge gained from previous tasks to be applied to new tasks with limited training examples. Most meta-learning algorithms can be categorized into three groups: 1) Black-box meta-learning approaches~\citep{Santoro2016MetaLearningWM} train a black-box model to take in training data of a target task to output parameters for the neural network used for making prediction for that task; 2) Optimization-based methods~\citep{Finn2017ModelAgnosticMF, Finn2018ProbabilisticMM, Antoniou2018HowTT} uses gradient descent to learn model parameters which can be adapted to a future target task with few gradient steps on a few-shot training dataset; 3) Non-parametric methods~\citep{vinyals2017matching, Snell2017PrototypicalNF, Sung2017LearningTC, Koch2015SiameseNN} learns a metric space in which predictions can be performed by computing some similarity metric, like distance and cosine similarity, to representations of each class. In our work, we are using the MAML~\citep{Finn2017ModelAgnosticMF} approach, which is a gradient-based method and learns model initialization (i.e., initial parameters) that is amenable to fast fine-tuning with few instances. This method is a conceptually simple and model-agnostic algorithm that has been shown to outperform existing approaches in several tasks.
\paragraph{Multi-task Learning} aims to jointly learn several related tasks providing a generalized representation with the added benefit of 
 compute and memory in terms of shared model parameters~\citep{Yang2016DeepMR, Caruana1997MultitaskL, Meyerson2019ModularUR}. MTL also has a regularization effect on the model parameters.  By definition, MTL aims to solve a fixed number of known tasks, whereas the point of meta-learning is often to solve unseen future tasks. But both methods capture a good prior from the training tasks, which can be used for getting model parameters for future target tasks.

In our work, we have experimented with both MAML and multi-task learning to check which of the method gives us a better prior for few-shot performance in our setting.
\subsubsection{Few-shot Methods}
\paragraph{Parameter-efficient finetuning:}Conventional fine-tuning methods retrains all the model parameters for every new task, which becomes infeasible as the model size increases to the level of GPT-3. In recent times, parameter-efficient methods have been studied and it has been demonstrated that state-of-the-art PEFT methods can match the performance of finetuning all the model's parameters while updating only a tiny fraction of the model parameters. Initially adapters~\citep{Raffel2019ExploringTL, Houlsby2019ParameterEfficientTL, Bapna2019SimpleSA} were introduced, which are new feed-forward modules added between the layers of the fixed pre-trained model. Since then, various sophisticated PEFT methods have been proposed, including methods like LoRA that produce low-rank updates~\citep{Hu2021LoRALA} and prompt tuning ~\citep{Lester2021ThePO} and prefix-tuning ~\citep{Li2021PrefixTuningOC} concatenate learned continuous embeddings to the modelâ€™s input or activations to induce it to perform a task. 

\paragraph{Retrieval-based Example selection:} In a study conducted by \citet{Liu2021WhatMG} , they explored how different prompts can impact the performance of GPT-3 and found that the use of in-context examples has a significant influence on the downstream results. To achieve this, they utilized an unsupervised sentence encoder to encode training examples and then retrieved the nearest neighbors for each test instance. On a similar note, \citet{Das2021CasebasedRF} developed a supervised prompt retriever for answering knowledge-based questions. Their method used tailored supervision specifically designed for knowledge-based queries and relied on surface similarity between formal queries. Furthermore,  \citet{Shin2021ConstrainedLM} employed GPT-3 to select examples for the prompt in few-shot semantic parsing. They demonstrated the effectiveness of this approach by using GPT-3 to identify relevant examples for the prompt, which in turn improved the overall performance of the system.

\begin{figure}[h]
     \centering
     \includegraphics[trim=0 0 0 0,clip,width=0.35\textwidth]{images/domain_split_visualization.png}
     \caption{Each dot signifies a domain. Average pairwise similarities of examples within each domain (x axis) plotted against average similarities of that domain to all other domains (y axis). }
     \label{fig:domain-viz}
\end{figure}


\subsection{Domain split visualization}\label{app:data-viz}
To better understand how different splits of domains are different from each other, we visualize our resulting test domains in Figure~\ref{fig:domain-viz}. We plot each domain as a dot, where different colors correspond to different splits. X axis demonstrates average pairwise similarity of examples within a domain, i.e. x coordinate of a domain corresponds to how uniform examples within a domain are. Y axis demonstrates pairwise similarities of examples within a domain to examples in all other domains, i.e. y coordinate of a domain demonstrates its similarity to other domains. From the figure we see that the vast majority of domains are clustered in the lower right corner, which corresponds to the domains that are uniform, and dissimilar to other domains. A small handful of domains are located in the upper left corner, that corresponds to domains with dissimilar examples within itself, but higher similarity to other domains. It is notable, that quantitatively, upper left corner contains more folders than repos, and more repos than orgs. We hypothesize, that such distribution could be explained by functional, rather than hierarchicals similarities across domains. A clear example of such instance can be a folder with utility functions that can have high similarity to other folders with utility functions, all the while individual functions within that folder are implementing different utilities, and thus - are dissimilar.  

\subsection{Models}\label{app:model}
\paragraph{CodeT5:} CodeT5~\citep{Wang2021CodeT5IU} is a pretrained encoder-decoder transformer model based on T5~\citep{Raffel2019ExploringTL} for programming languages. It uses a unified framework to support code understanding and generation tasks seamlessly. To improve the model's ability to handle the unique characteristics of programming languages, CodeT5 is trained on an identifier-aware pretraining task. Additionally, the model is trained to exploit  user-written code comments with a bimodal dual-generation task for better alignment between natural language and programming languages. This makes this model suitable for the applications that we consider. For both of our applications, we used the CodeT5-large model~\citep{DBLP:journals/corr/abs-2207-01780} without making any changes to the model architecture.

\paragraph{Codex} Codex~\citep{Chen2021EvaluatingLL} is the language model for code released by OpenAI. It is a GPT language model finetuned on 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. VLLMs are capable of zero-shot generalization to unseen tasks, which is achieved by providing them with an \textit{instruction} of what the model is expected to do. This allowed us to successfully evaluate Codex for both code generation and code summarization without any need for training. 

\paragraph{ChatGPT} ChatGPT is a conversational variant derived from InstructGPT/GPT 3.5 model~\citep{Ouyang2022TrainingLM}. It features a dialogue interface and is trained using a more refined objective function called Reinforcement Learning from Human Feedback (RLHF)~\citep{Christiano2017DeepRL}. However, there is currently limited information available regarding the specific architecture and training data employed in the creation of ChatGPT. We utilize the GPT-3.5 Turbo API, provided by OpenAI, to access ChatGPT for conducting our experiments. This API version allows a maximum token length restriction of 4096 tokens.

\subsection{Hyperparameters and training details}\label{app:hyperparams}
For full finetuning of CodeT5, we updated the model for 500 steps using batch size of 8, the best model was identified by the performance on the $\tau_{dev}$ portion. 
For LoRA, we use a rank of 4 with an initialization scale of 0.01 and update all the attention and feedforward layers. We train for 1000 steps with a batch size of 8.

For multitask learning (MTL) of CodeT5, we update the model for 150K steps on 80\% of the $X_{train}$ data, using a batch size of 4. The best checkpoint is selected by evaluating the model on the remaining 20\% of $X_{train}$ which was held-out from training.
For dual-gen MTL, we followed the same train/dev division strategy as for MTL for code generation, and updated the model for 150K steps with batch size of 4. 
The best checkpoints were again decided by evaluating the model on the created development set. 
In particular, we selected two checkpoints - one according to CodeBLEU metric, and another according to BLEU metric for code generation and code summarization respectively.
For Model-agnostic meta-learning, we updated the model from the pretrained CodeT5 checkpoint for 10K steps and used the last checkpoint in our experiments.

\subsection{The Vault} \label{app:vault}
The Vault is a multilingual dataset extracted from GitHub. Despite the fact that it comes pretokenized, we noticed that some of the preprocessing for The Vault is different from the preprocessing of CodeSearchNet. For example, while CodeSearchNet function body may have inlined comments, the Vault functions are stripped of those. On the other side, the Vault docstring typically includes function parameter documentation, whereas the CodeSearchNet omits those. On average, CodeSearchNet function docstrings are also shorter than those of the Vault. In our work, we processed the Vault dataset, to fix these inconsistencies and make new data points consistent with data from CodeSearchNet. 

\subsection{Additional experimental results}\label{app:metrics}

Besides the experiments presented in the main paper, in this section, we report some additional experiments. 
Tables~\ref{tab:text-to-code-chrf},~\ref{tab:text-to-code-rougel} and~\ref{tab:text-to-code-codebertscore} report results for code generation as measured using chrF, RougeL and CodeBERTScore metrics correspondingly.

Additionally, Figure~\ref{fig:result2-codet5-lora} illustrates how LoRA parameter efficient finetuning method compares to the full model finetuning for CodeT5. 

\begin{figure*}[h]
     \centering
     \includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/result2_codet5_lora.png}
     \caption{Performance for CodeT5 model finetuned with LoRA compared to regular finetuning.}
     \label{fig:result2-codet5-lora}
\end{figure*}


\begin{table}[h]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Code Summarization\\ 
 BLEU \end{tabular}} & \multicolumn{3}{@{\hskip 0.6cm}c}{\begin{tabular}[c]{@{}c@{}}Code Generation\\ CodeBLEU \end{tabular}} \\
 \cmidrule(lr){2-4}
 \cmidrule(lr){5-7}
 & \multicolumn{1}{c}{org} & \multicolumn{1}{c}{repo} & \multicolumn{1}{c}{folder} & \multicolumn{1}{c}{org} & \multicolumn{1}{c}{repo} & \multicolumn{1}{c}{folder} \\
\midrule
IsoScore (4) 
& 16.71 & 16.57 & 15.47 & 15.05 & 16.01 & 14.93 \\
IsoScore (8) 
& 17.27 & 16.72 &	15.71 &	15.32 &	16.55 &	15.28 \\
IsoScore (32) 
& 17.46 &	16.90 &	14.34 &	16.13 &	17.89 &	16.26 \\
\bottomrule
\end{tabular}}
\caption{ Results for CodeT5 model using IsoScore for measuring embedding similarity and supervising with retrieved examples from train data.
}
\label{tab:result-isoscore}
\end{table}


\subsection{IsoScore}\label{app:similarity-isoscore}
IsoScore is a similarity metric of isotropy of an embedding space. The way we use it to measure similarity is by computing IsoScore value of a combined set of test example embeddings and every individual training set embedding. The ``closest'' examples selected for supervision are the ones that resulted in the largest IsoScore value for each set of test examples. We then use the same number of supervision examples as we used with cosine similarity - selecting 4*32, 8*32, or 32*32 ``closest'' examples for supervision. The results for model adapted using IsoScore metric similarity are reported in Table~\ref{tab:result-isoscore}.


\subsection{Fast vote-k}\label{app:similarity-sa}
To make the setup for fast vote-k similar to the version with the combination of nearest examples, we run this algorithm to select 4*32 (128), 8*32 (256), and 32*32 (1024) supervision examples. 
Table~\ref{tab:result-vote-k} show results obtained for a CodeT5 MTL model that has additionally been finetuned using a set of examples obtained from fast vote-k algorithm. 

\begin{table}[h]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Code Summarization\\ 
 BLEU \end{tabular}} & \multicolumn{3}{@{\hskip 0.6cm}c}{\begin{tabular}[c]{@{}c@{}}Code Generation\\ CodeBLEU \end{tabular}} \\
 \cmidrule(lr){2-4}
 \cmidrule(lr){5-7}
 & \multicolumn{1}{c}{org} & \multicolumn{1}{c}{repo} & \multicolumn{1}{c}{folder} & \multicolumn{1}{c}{org} & \multicolumn{1}{c}{repo} & \multicolumn{1}{c}{folder} \\
\midrule
Fast vote-k (4) 
& 10.96 &	12.34 &	10.33 &	24.96 &	25.76 &	24.77 \\
Fast vote-k (8) 
& 11.40 &	12.74 &	10.60 &	25.10 &	26.21 &	25.14 \\
Fast vote-k (32) 
& 10.84&	12.03 &	10.06 &	24.25 &	25.06 &	24.17 \\
\bottomrule
\end{tabular}}
\caption{ Results for CodeT5 model using Fast Vote-k for measuring embedding similarity and supervising with retrieved examples from train data.
}
\label{tab:result-vote-k}
\end{table}

\subsection{Instructions for Codex and ChatGPT} \label{app:instructions}

Table~\ref{tab:instructions} contains list of instructions we used with Codex and ChatGPT models in instruction-only and in-context learning scenarios. 

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lll}
\toprule
Copilot & Task instruction & \begin{tabular}[c]{@{}l@{}}"Write in javascript:",\\ "Write code:", \\ "Summarize code:",\\ "Summarize javascript snippet:",\\ "Write code intent:"\end{tabular}\\
 & \begin{tabular}[c]{@{}l@{}}Demonstration example \\ template\end{tabular} & \begin{tabular}[c]{@{}l@{}}"Intent: \{text\} \textbackslash{}\textbackslash{}n Snippet: \{code\}\textbackslash{}\textbackslash{}n\textbackslash{}\textbackslash{}n", \\ "Intent: \{text\} \textbackslash{}\textbackslash{}n Code: \{code\}\textbackslash{}\textbackslash{}n\textbackslash{}\textbackslash{}n",\\ "Code: \{code\} \textbackslash{}\textbackslash{}n Intent: \{text\}\textbackslash{}\textbackslash{}n\textbackslash{}\textbackslash{}n",\\ "Code: \{code\} \textbackslash{}\textbackslash{}n Summary: \{text\}\textbackslash{}\textbackslash{}n\textbackslash{}\textbackslash{}n",\\ "Snippet: \{code\} \textbackslash{}\textbackslash{}n Intent: \{text\}\textbackslash{}\textbackslash{}n\textbackslash{}\textbackslash{}n",\\ "Snippet: \{code\} \textbackslash{}\textbackslash{}n Summary: \{text\}\textbackslash{}\textbackslash{}n\textbackslash{}\textbackslash{}n",\end{tabular} \\
\midrule
ChatGPT & System messages & \begin{tabular}[c]{@{}l@{}}'You are a helpful assistant that writes JavaScript code based on English description. \\ You only output code without any English text.'\\ ``You are a helpful assistant that writes single sentence summarizes for JavaScript code in English. \\ You only output code summary without any other English text.''\end{tabular} \\
 & Task instruction & \begin{tabular}[c]{@{}l@{}}"Write a single sentence summary for the following JavaScript code in English. "\\ "Implement this functionality using JavaScript. "\end{tabular} \\
 & \begin{tabular}[c]{@{}l@{}}Demonstration example \\ template\end{tabular} & \begin{tabular}[c]{@{}l@{}}{[}"Below are some examples of JavaScript code implemented based on English summary. \textbackslash{}n",\\ "Summary: \{text\}\textbackslash{}nCode: \{code\}\textbackslash{}n\textbackslash{}n"{]}\\ {[}"Below are some examples of English summaries of JavaScript code. \textbackslash{}n",\\ "Code: \{code\}\textbackslash{}nSummary: \{text\}\textbackslash{}n\textbackslash{}n"{]}\end{tabular} \\
 \bottomrule
\end{tabular}}
\caption{Task instructions and demonstration templates used for generating results in the experiments with Codex and ChatGPT.}
\label{tab:instructions}
\end{table*}

\subsection{Sample outputs}
Table~\ref{tab:sample-outputs} presents some examples and the outputs obtained by different models for those. Here we can see that CodeT5 model finetuned on in-domain examples sometimes has the advantage of having relevant context and thus is using correct member names as opposed to other models. On the other hand, we also see that similar out-of-domain examples from the train split can in fact be near duplicates of the ones in the test split. As a result, the model supervised with retrieved examples may generate output that is extremely close to that of the gold test data.

\begin{table*}[]
\resizebox{\textwidth}{!}{
\begin{tabular}{llllll}
\toprule
\multicolumn{1}{c}{\textbf{Input}} & \multicolumn{1}{c}{\textbf{Gold}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}CodeT5\\ MTL (0-shot)\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}CodeT5 \\ MTL + ID (32-shot)\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}CodeT5 \\ MTL + ret 4\end{tabular}}} & \multicolumn{1}{c}{\textbf{ChatGPT}} \\
\midrule
\raisebox{-4\height}{\begin{tabular}{@{}c@{}}Dispatch stack information \\  to all handlers\end{tabular}} & \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/gold0.png}}&  
\raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl0.png}}&   \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl-ft0.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/sim0.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/chatgpt-0.png}} \\
\midrule
\raisebox{-8\height}{\begin{tabular}{@{}c@{}}Setup captions\end{tabular}} & \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/gold1.png}}&  
\raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl1.png}}&   \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl-ft1.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/sim1.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/chatgpt-1.png}} \\
\midrule
\raisebox{-6\height}{\begin{tabular}{@{}c@{}}Toggle event listener\end{tabular}} & \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/gold2.png}}&  
\raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl2.png}}&   \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl-ft2.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/sim2.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/chatgpt-2.png}} \\
\midrule
\raisebox{-2\height}{\begin{tabular}{@{}c@{}}Returns the absolute path \\  to the class file\end{tabular}} & \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/gold3.png}}&  
\raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl3.png}}&   \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl-ft3.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/sim3.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/chatgpt-3.png}} \\
\midrule
\raisebox{-2\height}{\begin{tabular}{@{}c@{}}Returns the tag name of the\\   given library in the given contrib \\  repository if installed. \\ Returns false if not installed. \end{tabular}} & \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/gold4.png}}&  
\raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl4.png}}&   \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/mtl-ft4.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/sim4.png}}&  \raisebox{-\height}{\includegraphics[width=0.3\textwidth]{images/examples/chatgpt-4.png}} \\
\bottomrule
\end{tabular}
}
\caption{Sample outputs from different models.}
\label{tab:sample-outputs}
\end{table*}
