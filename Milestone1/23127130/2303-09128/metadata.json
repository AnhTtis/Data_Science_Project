{
    "arxiv_id": "2303.09128",
    "paper_title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
    "authors": [
        "Shushan Arakelyan",
        "Rocktim Jyoti Das",
        "Yi Mao",
        "Xiang Ren"
    ],
    "submission_date": "2023-03-16",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SE"
    ],
    "abstract": "We systematically study the capacity of two large language models for code - CodeT5 and Codex - to generalize to out-of-domain data. In this study, we consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial. We establish that samples from each new domain present both models with a significant challenge of distribution shift. We study how well different established methods can adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that in the case of code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to each domain individually.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09128v1"
    ],
    "publication_venue": null
}