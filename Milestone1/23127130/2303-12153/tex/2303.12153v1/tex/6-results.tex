\section{Results}
\label{sec:results}

\input{figures/planning_result.tex}


\subsection{Planning is required to solve TAMP-like tasks (\textbf{H1})}
\label{subsec:components-tamp}
Our first hypothesis is that performing policy sequence optimization on task plans output by the LLM is essential to task success. To test this hypothesis, we compare Text2Motion and \hm{}, which both perform policy sequence optimization, against \scgs{} and \imgs{}, which are both reactive methods.
% In Fig \ref{fig:planning-result}, we see that, when compared with reactive methods (\scgs{} and \imgs{}) that do not perform policy sequence optimization, the planning methods Text2Motion and \hm{} are able to solve tasks with around 75\% accuracy whereas the reactive methods .

Instructions $i$ provided in the first two planning tasks (\LH{}) allude to skill sequences that, if executed appropriately, would solve the task.
In effect, the LLM plays a lesser role in contributing to plan success, as its probabilities are conditioned to mimic the skill sequences in $i$.
On such tasks, the reactive baselines fail to surpass success rates of 20\%, despite completing between 50\%-80\% of the subgoals (Fig.~\ref{fig:planning-result}).
This result is anticipated as the feasibility of the final skills requires coordination with earlier skills in the sequence, which the reactive baselines fail to consider.
As other tasks in our suite combine aspects of \LH{} with \LG{} and \PAP, it remains difficult for \scgs{} and \imgs{} to find solutions.


% We further highlight that \scgs{} matches the performance of its derivative \imgs{}, suggesting that the techniques designed to contend with symbolic complexity (e.g. task-progress feedback) does fulfil the role of geometric reasoning.
Suprisingly, we see \scgs{} closely matches the performance of its derivative \imgs{}, which has the added benefit of scene descriptions (like Text2Motion).
This result suggests that explicit language feedback does not contribute to success on our tasks when considered in isolation from plan feasibility.
% This result suggests that our task suite does not require explicit language feedback to the LLM and that, instead, scene progress feedback through the value functions as well as executed actions were enough for \scgs{} to match (or surpass, possible due to the stochasticity of the underlying policies) \imgs{}.
In contrast, Text2Motion and \hm{} which employ policy sequence optimization over planned skills can better contend with geometric dependencies prevalent in \LH{} tasks and thereby demonstrate higher success rates. 

\input{figures/failure_modes.tex}


% \subsection{When is integrated planning preferred? (\textbf{H2})}
\subsection{Integrated reasoning is required for PAP tasks
 (\textbf{H2})}
\label{subsec:integrated-planning}
Our second hypothesis is that integrated semantic and geometric reasoning is required to solve the \PAP{} family of tasks (defined in Sec.~\ref{subsec:task-suite}). 
We test this hypothesis by comparing Text2Motion and \hm{}, which represent two fundamentally distinct approaches to combining symbolic and geometric reasoning.
\hm{} uses Q-functions of skills to optimize $k$ skill sequences \textit{after} they are generated by the LLM.
Text2Motion uses primitive Q-functions as a skill feasibility heuristic (Eq.~\ref{eq:q-function-score}) for integrated planning \textit{while} a skill sequence is being constructed.

In the first two tasks (\LH{}, Fig.~\ref{fig:planning-result}), we find that \hm{} achieves slightly higher success rates than Text2Motion, while both methods achieve 100\% success rates in the third task (\LH{} + \LG{}).
While confounding at first, this result indicates a subtle advantage of \hm{} when multiple  feasible plans can be directly inferred from $i$ and $s_1$: it can capitalize on diverse orderings of $k$ generated skill sequences (including the one specified in $i$) and select the one with the highest likelihood of success.
% Concretely, it can capitalize on diverse orderings of $k$ primitive sequences and select the one with the highest likelihood of success, which may not be specified in $i$. 
For example, Task 1 (Fig.~\ref{fig:evaluation_task_suite}) asks the robot to put three boxes onto the rack; \hm{} allows the robot to test multiple different strategies while Text2Motion's integrated approach only outputs one plan.
This advantage is primarily enabled by bias in the Q-functions: Eq.~\ref{eq:taps-objective} may indicate that \graytext{Place(dish, rack)} then \graytext{Place(cup, rack)} is more geometrically complex than \graytext{Place(cup, rack)} then \graytext{Place(dish, rack)}, while they are geometric equivalents.

The plans considered by Text2Motion at planning iteration $h$ share the same sequence of predecessor skills $\pi_{1:h-1}$.
This affords limited diversity for the planner to exploit.
% This diversity is not afforded to Text2Motion as the plans it considers during integrated search at step $h$ share the same sequence of predecessor skills $\pi_{1:h-1}$. 
However, Text2Motion has a significant advantage when solving the \PAP{} family of problems (Fig.~\ref{fig:planning-result}, Tasks 4-6).
Here, geometrically feasible skill sequences are difficult to infer directly from $i$, $s_1$, and the in-context examples provided in the prompt.
As a result, \hm{} incurs an 80\% planning failure rate, while Text2Motion finds plans over 90\% of the time (Fig.~\ref{fig:failure}).
In terms of success, Text2Motion solves 40\%-60\% of the tasks, while \hm{} achieves a 10\% success rate on Task 4 (\LG{} + \PAP{}) and fails to solve any of the latter two tasks (\LH{} + \LG{} + \PAP{}).
Moreover, \hm{} does not meaningfully advance on any subgoals, unlike \scgs{} and \imgs{}, which consider the geometric feasibility of skills at each timestep (albeit, greedily). 
% In contrast, Text2Motion find plans over 90\% of the time and attains 40\% and 60\% success rates, t 10\% of the time on the fourth task (\LG{} + \PO) and failing to solve any of the latter two (\LH{} + \LG{} + \PO{}).


% In contrast, Text2Motion finds plans over 90\% of the time, and achieves 40\% and 60\% success rates on the tasks, while \hm{} succeeds only 10\% of the time on the fourth task (\LG{} + \PO) and fails to solve any of the latter two (\LH{} + \LG{} + \PO{}).
% Moreover, \hm{} does not meaningfully advance on any subgoals.

% Thus, when a task admits a large solution space and a feasible plan cannot be deduced from the instruction (\PO{}), the likelihood of \hm{} inferring a geometrically feasible solution amongst many plausible skill sequences is low. 
% In contrast, our integrated planner uses skill Q-functions to prune the space of possible plans and thereby only considers those which are geometrically feasible at $s_1$ and onwards.
% This result is consistent with \textbf{H2}.

% While the strengths of \hm{} (diversity) and Text2Motion (feasibility) 
% We leave the use of beam search with beam size greater than 1 and other high level search strategies that, for example, combine the strengths of Text2Motion (ability to quickly prune out irrelevant actions) and \hm{}'s (diverse plans) for future work.


\input{figures/termination_results.tex}

% \subsection{Goal prediction is reliable for plan termination (\textbf{H3})}
\subsection{Plan termination is made reliable via goal prediction (\textbf{H3})}
\label{subsec:plan-termination}
Our final hypothesis is that predicting goals from instructions \textit{a priori} and selecting plans based on their satisfication (Sec.~\ref{sec:goal_pred}) is more reliable than 
scoring plan termination (with a dedicated \texttt{stop} skill) at each timestep.
We test this hypothesis in an ablation experiment (Fig.~\ref{fig:ablation-termination}), comparing our plan termination method to that of SayCan and Inner Monologue's, while keeping all else constant for our integrated planner. 
% We run an ablation experiment to test our goal proposition prediction plan termination method with SayCan and Inner Monologue's \texttt{stop} scoring termination method while keeping all else constant for our integrated planner. 
We run 60 experiments (six tasks and ten seeds each) in total on the TableEnv Manipulation task suite. 
The results in Fig.~\ref{fig:ablation-termination} suggest that, for the tasks we consider, our proposed goal proposition prediction method leads to 10\% higher success rates than the scoring baseline. 

We also note the apparent advantages of both techniques.
First, goal proposition prediction is more efficient than scoring \texttt{stop} as the former requires only one LLM query, whereas the latter needs to be queried at every timestep.
Second, goal proposition prediction offers interpretability over \texttt{stop} scoring, as it is possible to inspect the goal that the planner is aiming towards prior to execution. 
% is potentially more interpretable than \texttt{stop} scoring, as it is possible to inspect the goal the planner is aiming towards. 
Nonetheless, \texttt{stop} scoring does provide benefits in terms of expressiveness, as its predictions are not constrained to any specific output format.
This advantage, however, is not captured in our TableEnv Manipulation tasks, which at most require conjunctive ($\land$) and disjunctive ($\lor$) goals.
% However, the tasks considered in the TableEnv Manipulation domain at most require goals that involve $\land$ or $\lor$ operators.
For instance, ``Stock two boxes onto the rack'' could correspond to (\graytext{on(red box, rack)} $\land$ \graytext{on(blue box, rack)}) $\lor$ (\graytext{on(box A, rack)} $\land$ \graytext{on(box C, rack)}), while mechanically, \texttt{stop} scoring can \textit{handle} more complex instructions.

% \todo{We are awating results for this section.}


% The integrated approach (with beam size larger than 1) requires the LM to reason over task plan sequences. However, from non-quantitative analysis, the LLM is quite sensitive to small changes in the prompt. Hence, we only use beam size 1 to restrict the LLM to only need to reason over the `next' action.

% Another challenge with the integrated approach is that it involves feeding parsed `latent` dynamics states to the language model. This process has a high requirement on the fidelity of the dynamics model, since a slighly flawed dynamics prediction may cause a discrete change in the parsed symbolic state that's fed to the language model. For example, the dynamics model for \texttt{place(a, b)} may lead to a predicted geometry where \texttt{a} is slightly below the surface of \texttt{b} and thus the predicate \texttt{on(a, b)} may not be parsed and fed to the language model.