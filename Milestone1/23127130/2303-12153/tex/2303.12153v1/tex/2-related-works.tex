\section{Related Work}
\label{sec:related-works}

\subsection{Task and motion planning}
\label{subsec:tamp-literature}
Task and motion planning refers to a problem setting in which a robot has to solve long-horizon tasks through symbolic and geometric reasoning~\cite{kaelbling2012integrated, integrated-tamp-2021}.
The hierarchical approach~\cite{5980391} characterizes the most common family of solution methods.
Such methods typically employ a) an AI task planner~\cite{bonet2001planning, helmert2006fast} to deduce candidate plan skeletons and b) a motion planner to obtain motion trajectories subject to robot and environmental constraints; e.g. through sampling-based planning~\cite{garrett2020-pddlstream} or constrained optimization~\cite{toussaint2015-lgp, driess2019-hlgp}.

% A consequence of strictly separating task- from motion-level planning is that task plans are uninformed of their geometric feasibility upon construction.
A consequence of strictly separating task- from motion-level planning is that the geometric feasibility of a task plan is unknown at the time of its construction.
Hence, the process of iterating between a) and b) may take on the order of minutes until the TAMP solver returns a solution to a sufficiently complex task or indicates that none exists.
This is in contrast to Text2Motion, which interweaves task and motion planning by using value functions as a geometric feasibility heuristic to guide task-level planning.
Therefore, Text2Motion spends less time considering infeasible plans, and when a plan is returned, it is geometrically feasible by construction.

Another line of works accelerates TAMP by learning sampling distributions~\cite{wang2018active, xu2021deep}, visual feasibility heuristics~\cite{driess2020-dvr, driess2020-dvh, driess2021-lpr}, low-level controllers~\cite{driess2021-lgr, silver2022learning}, or state sparsifiers~\cite{chitnis2021camps, silver2021-ploi}.
However, these methods learn from solutions computed by classical TAMP solvers, and thus, they also depend on meticulously hand-crafted and task-specific symbolic planning domains.
% However, like the TAMP solvers they build upon, they rely on meticulously hand-crafting symbolic planning domains which govern the transition dynamics of actions and may be non-stationary across environments and tasks.
% However, these methods face similar limitations to the TAMP solvers they build upon: they rely on meticulously hand-crafted, task-specific symbolic planning domains.
% However, these works face a challenge similar to that faced by the TAMP solvers they build upon: they rely on meticulously hand-crafted, task-specific symbolic planning domains.
While learning symbolic representations has been proposed for TAMP~\cite{kroemer2016learning, ames2018learning, konidaris2018skills, silver2021learning, curtis2022discovering, chitnis2022learning, silver2022learning}, these approaches often require task-specific symbolic transition experience.
A core element of our work is the use of LLMs as a task- and environment-agnostic planner, while value functions serve as an alternative to symbolic preconditions when determining what skills are admissible in the current state.


\subsection{Language for robot planning}
\label{subsec:language-literature}
% LLMs have been oriented towards solving long-horizon robotics problems on several fronts.
Language has increasingly been explored as a medium for solving long-horizon robotics problems.
For instance, {\em Language-conditioned policies\/} (LCPs) have been applied to robotic manipulation.
While many methods learn short-horizon skills~\cite{stepputtis2020language, jang2021bczero, concept2robot-2021, cliport-2022, perceiver-2022, vima-2022}, others focus on long-horizon tasks~\cite{mees2022calvin, rt1-2022}.
% {\em Language-conditioned policies\/} (LCPs) have been proposed for solving robot manipulation tasks. 
% While some focus on skill learning or solving shorter horizon manipulation tasks~\cite{stepputtis2020language, jang2021bczero, concept2robot-2021, cliport-2022, perceiver-2022, vima-2022}, others focus on solving long-horizon manipulation problems~\cite{mees2022calvin, rt1-2022}.
However, using LCPs alone to solve long-horizon manipulation problems can require expensive data collection and training procedures if the LCP is to generalize to a wide distribution of long-horizon tasks with diverse instructions.
% and b) diverse instructions enabling the learned policy to either interpolate within the training data distribution or generalize beyond it. 

% \subsubsection{Language-conditioned policies (LCPs)}
% \label{subsec:language-policies}
% Concept2Robot~\cite{} was amongst the first to condition manipulation skill policies with LLM embeddings, trained on large-scale video data via inverse reinforcement learning.
% CLIPort proposed a dual-stream semantic-spatial architecture based on CLIP and learned skill policies in pixel-space (Transporter Networks~\cite{}) from curated demonstrations. 
% Perceiver-Actor extended CLIPort to 6DoF settings with Perceiver, substituting pixel-space with attention-based voxel-space reasoning. 
% VIMA expresses general manipulation concepts with multi-modal prompts.

% Several recent works leveraged the generative qualities of LLMs by prompting it to predict long-horizon plans.
% The prompts commonly contain in-context examples and chain-of-thought explanations to guide the LLMs response to the desired format, along with the new task query.
% While this avenue subsumes the data dependence issues of LCPs, it presents challenges in terms of grounding the LLMs plans in actions that are consistent with both the robots capabilities and the constraints of the environment. 

% \citet{zeroshot-llms-2022} proposes to compute embedding similarity scores between the descriptions of LLM-produced actions and actions known to be admissible in the current state. 
% However, the evaluation of this method is limited to non-geometric settings that assumes perfect symbolic transitions of actions.
% A tangential line of work shifts the planning medium from natural language to code.
% CodeAsPolicies~\cite{code-as-policies-2022} engages low-level primitives through sequential execution of a program, while ProgPrompt~\cite{progprompt-2022} expresses task queries to LLMs as programs that encode robot actions and preconditions, solution samples to elicit chain of thought reasoning~\cite{chain-of-though-2022}, and fallback behaviors should actions fail. 
% Meticulously constructing code-like prompts for each new task raises concerns of reliability and scalability to new tasks (akin to defining symbolic planning domains) and is non-trivial compared to natural language for non-expert users.
% Furthermore, while code is a sufficiently expressive representation for plans, it remains unclear how to verify that such plans adhere to constraints and optimize them for desired properties, such as geometric feasibility, which is of key interest for TAMP.

Several recent works leverage the generative qualities of LLMs by prompting them to predict long-horizon plans.
\cite{zeroshot-llms-2022} grounds an LLM planner to admissible action sets, but limits evaluation to task-level planning. 
%non-geometric settings.
Tangential works shift the planning medium from natural language to code~\cite{code-as-policies-2022, progprompt-2022, zelikman2022parsel} and embeds task queries, robot actions, solution samples, and fallback behaviors as programs in the prompt. 
While programs are an expressive representation for plans, we focus on optimizing plans in the form of skill sequences.

Closest in spirit to our work are SayCan~\cite{saycan-2022} and Inner Monologue (IM)~\cite{innermono-2022} which at each timestep score the \textit{usefulness} and \textit{feasibility} of all possible skills and execute the one with the highest score.
Termination occurs when the score of the $\texttt{stop}$ ``skill'' is larger than any other.
IM provides additional sources of feedback to the LLM in the form of object descriptions, skill successes, and task-progress cues. 

While the generality of SayCan and IM is shown over a diverse range of tasks, there are several drawbacks that impede their performance in the settings we study.
First, by only \textit{greedily} considering the next skill at each timestep, they may fail to account for geometric dependencies that exist over the extent of an skill sequence (Fig.~\ref{fig:teaser}).
% Second, by keeping the plan \textit{implicit} to the LLM (i.e. no explicit multi-step plan is predicted), it cannot be verified against a desired property or outcome prior to execution.
Second, using the LLM to plan \textit{implicitly} (as opposed to \textit{explicitly} predicting a multi-step plan) prevents verification of desired properties or outcomes prior to execution.
Examples of such properties could include whether the final state induced by the plan satisfies symbolic constraints or whether the plan adheres to some notion of safety.
Lastly, these methods ignore the uncertainty of skill feasibility predictions, which \cite{taps-2022} demonstrates is important for using skills to solve geometrically complex TAMP problems.
By addressing these limitations, Text2Motion can outperform SayCan and IM on geometrically complex tasks, as demonstrated in the experiments.