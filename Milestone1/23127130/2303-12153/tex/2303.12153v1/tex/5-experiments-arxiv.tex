\section{Experiments}
\label{sec:experiments}

We conduct experiments to test the following hypotheses:
\begin{description}
    % \item[\textbf{H1}] LLMs, robot skills, and policy sequence optimization must be jointly considered to solve TAMP-like tasks.
    \item[\textbf{H1}] Policy sequence optimization is a necessary ingredient for solving TAMP-like problems specified by natural language with LLMs and robot skills.
    % \item[\textbf{H2}] Text2Motion can solve partially observable problems that require joint semantic and geometric reasoning.
    \item[\textbf{H2}] Integrated semantic and geometric reasoning is better equipped to solve tasks with partial affordance perception (PAP, as defined in Sec.~\ref{subsec:task-suite}) compared to hierarchical planning approaches.
    \item[\textbf{H3}] \textit{A priori} goal proposition prediction is a more reliable plan termination strategy than prior scoring methods.
    % \item[\textbf{H2}] Integrated planning is suited to problems where combinatorial complexity can be reduced by accounting for geometric feasibility
    % \item[\textbf{H3}] Plans produce by Text2Motion are more likely to succeed during closed-loop excution. 
\end{description}
The following subsections describe the baseline methods we compare against, details on LLMs and prompts, the tasks over which planners are evaluated, and performance metrics.

\subsection{Baselines}
\label{subsec:baselines}

We compare Text2Motion with a series of language-based planners.
% These methods share similar mechanistic assumptions in the use LLMs and primitive skills $\mathcal{L}^\pi$ to solve Eq.~\ref{eq:tamp-score} in a few-shot manner (i.e. given a number of in-context prompt examples).
For consistency, we use the same set of independently trained primitive policies $\pi_i$, Q-functions $Q^{\pi_i}$, OOD rejection strategy and, where appropriate, dynamics models $T^\pi(s, a)$ across all methods and tasks.
% Because the baselines perform various forms of action ranking through $Q^{\pi_i}$, they are also susceptible to OOD failures from spuriously high Q-values.
% Furthermore, we also provided the baselines with the same OOD rejection strategy --thresholding standard deviations computed over an ensemble of Q-functions--to equalize performance variation along this axis.
% Where needed, value estimates are computed via Monte-Carlo estimation as $V^\pi(s)=\mathbb{E}_\pi[Q^\pi(s, a)]$. 

\textbf{\scgs{}:}
We implement a cost-considerate variant of SayCan with a module dubbed Generator-Scorer (GS).
At each timestep $h$, vanilla SayCan~\cite{saycan-2022} ranks \textit{all possible} actions by $p(\pi_h \mid i, \pi_{1:h-1}) \times V^{\pi_h}(s_h)$, before executing the top scoring action (Scorer).
However, the cost of ranking actions scales unfavorably with the number of scene objects and skills in library $\mathcal{L}^\pi$.
\scgs{} limits the pool of actions considered in the ranking process by querying the LLM for the $K$ most useful actions $\{\pi_h^1, \dots, \pi_h^K\} \sim p(\pi_h \mid i, \pi_{1:h-1})$ (Generator) before engaging Scorer.
Execution terminates when the score of the \texttt{stop} ``skill'' is larger than the other skills.

\textbf{\imgs{}:}
We implement the \textit{Object + Scene} variant of Inner Monologue~\cite{innermono-2022} by providing task-progress scene context in the form of the environment's symbolic state.
We acquire \imgs{} by equipping~\cite{innermono-2022} with the Generator-Scorer for cost efficiency. LLM action likelihoods are equivalent to those from \scgs{} except they are now also conditioned on the visited state history.
%LLM action likelihoods are now also conditioned on the state history $s_{1:h-1}$ and action history $p(\pi_h \mid i, s_{1:h-1}, \pi_{1:h-1})$ visited by the planner.

\textbf{\hm{}:}
We propose and compare with a third baseline method, which we term the \hm{} method.
Like Text2Motion, \hm{} predicts a set of goal propositions for the given instruction $i$ and then converts the goal propositions into a goal satisfaction checker $f_{\text{sat}}$ (see Sec.~\ref{sec:goal_pred}).
In contrast to Text2Motion, which interleaves task planning with policy sequence optimization, \hm{} uses the LLM to generate entire skill sequences $\pi_{1:H}$ (Eq.~\ref{eq:task-score}), runs policy sequence optimization for each of the candidate sequences, and filters out plans with low predicted Q-values or OOD state-action pairs. 
Then, if a geometric state $s_i$ along the optimized trajectory of a given plan satisfies $f_{\text{sat}}$, we add the plan (up to that geometric state) to a set of final candidate plans. 
Finally, if multiple candidates remain, we select the one with highest success probability (Eq.~\ref{eq:taps-objective}).

% A plan is selected if its final state $\mathfrak{s}_{H+1}$ predicted by the motion planner satisfies the  goal propositions.
% If multiple plans meet this criterion, the plan with the highest success probability (Eq.~\ref{eq:taps-objective}) is chosen.
% Without this check, using the LLM for open-ended plan generation is likely to result in many incorrect plans. 

% This baseline also verifies that the instruction $\mathfrak{i}$ has been satisfied by using the LLM to predict the next primitive from the final state $\mathfrak{s}_{H+1}$ predicted by the motion planner. 
% If the next primitive is not ``stop", then the primitive sequence $\pi_{1:H}$ initially generated by the LLM is incomplete and should be filtered out. 

% If no plans remain after filtering, then like Text2Motion, the \hm{} baseline simply executes the first action for the best plan and re-plans from the updated state observation.

% \begin{enumerate}
    
%     \item \textbf{SayCan-GS} is a financially / and computationally cheaper version of SayCan that uses GeneratorScorer to score the top $K$ actions, instead of all possible actions. 

%     \item \textbf{Single-plan SayCan-GS} Basically, the interleaved method except beam size of $1$. Uses a dynamics model to roll forward actions with interleaved TAPS trajectory optimization.  \klin{should upper bound SayCan-GS, assuming good dynamics}

%     \item \textbf{Hierarchical Text2Motion} 
%     \begin{enumerate}
%         \item Query LM for $K$ plans
%         \item TAPS optimize all plans
%         \item Filter out non-goal reaching plans \item Select remaining plans with maximum Q product (normalized)
%     \end{enumerate}
% \end{enumerate}
\input{figures/evaluation_task_suite.tex}

\subsection{Large language model}
\label{subsec:llm}

We use two language models, both of which were accessed through the OpenAI API: i) \texttt{text-davinci-003}, a variant of the InstructGPT \cite{ouyang2022training} language model family which is finetuned from GPT-3 with human feedback and ii) the Codex model \cite{chen2021evaluating} (specifically, \texttt{code-davinci-002)}. To generate task plans for the \hm{} method, we empirically found \texttt{text-davinci-003} to be the most capable for \hm{}'s task plan generation queries and thus use it for that purpose. For all other queries, we use \texttt{code-davinci-002} in our experiments as they were found to be reliable. We do not train the LLMs in any way and use only few shot prompting.


\subsection{Prompt engineering}
\label{subsec:prompt-engineering}

We use the same set of in-context examples for all methods and for all tasks in the prompts passed to the LLM. 
A single prompt has the following structure (prompt template is in black and LLM output is in \textcolor{orange}{orange}):

\vspace{0.3cm}

\noindent\fbox{\parbox{0.97\linewidth}{\small{\texttt{{
Available scene objects: [`table', `hook', `rack', `yellow box', `blue box', `red box']\\\\
Object relationships: [`inhand(hook)', `on(yellow box, table)', `on(rack, table)', `on(blue box, table)']\\\\
Human instruction: How would you push two of the boxes to be under the rack?\\\\
Goal predicate set: {\color{code-constant}[[`under(yellow box, rack)', `under(blue box, rack)'], [`under(blue box, rack)', `under(red box, rack)'], [`under(yellow box, rack)', `under(red box, rack)']]}\\\\
Top 5 next valid robot actions (python list): {\color{code-constant}['push(yellow box, rack)', 'push(red box, rack)', 'place(hook, table)', 'place(hook, rack)', 'pull(red box, hook)']}
}}}}}
\vspace{0.3cm}

The prompt above chains the output of two queries together: one for goal proposition prediction (Sec.~\ref{sec:goal_pred}), and one for next skill generation (Sec.~\ref{sec:llm-task-planning}). To score skills (Eq.~\ref{eq:lm-step-score}), we replace \texttt{Top 5 next valid robot actions} with \texttt{Executed action: }, append the generated skill (e.g. \graytext{Push(yellow box, rack)}), and then add token log-probabilities. We provide the full set of in-context examples in the Appendix (Appx.~\ref{sec:suppl-incontext-examples}).
%We provide the prompts used for our tasks in Appendix \todo{\ref{}}. 

\subsection{Task suite}
\label{subsec:task-suite}

% \todo{terminology: we construct a task suite, not a suite of domains }
We construct a suite of evaluation tasks (Fig.~\ref{fig:evaluation_task_suite}) in a table-top manipulation domain.
Each task includes a natural language instruction $i$ and initial state distribution $\rho(s)$ from which geometric task instances are sampled. 
For the purpose of experiment evaluation only, tasks also contain a ground-truth goal criterion to evaluate whether a plan has satisfied the corresponding task instruction.
% The task suite features characteristic permutations of tasks that are long-horizon, contain lifted goals, and are partially observable.
Finally, each task contains subsets of the following properties:
\begin{itemize}
    \item \textbf{Long-horizon (LH):} task requires skill sequences $\pi_{1:H}$ of length six or greater to solve. For example, Task 1 in Fig.~\ref{fig:evaluation_task_suite} requires the robot to pick and place three objects for a total of six actions. In our task suite, \textbf{LH} tasks also contain geometric dependencies that span across the sequence of action that are unlikely to be solved by myopically executing skills. For example, Task 2 (Fig.~\ref{fig:evaluation_task_suite}) requires the robot to pick and place obstructing boxes (i.e. blue and yellow) to enable a future hook-push of the cyan box underneath the rack.
    \item \textbf{Lifted goals (LG):} Goals are expressed over object classes rather than object instances. For example, the lifted goal instruction "move three boxes to the rack" specifies an object class (i.e. boxes) rather than an object instance (e.g. the red box). This instruction is used for Task 3 (Fig. ~\ref{fig:evaluation_task_suite}). 
    % Instructions express goals over object classes and properties rather than over object instances. Task 3 in Fig.~\ref{fig:evaluation_task_suite} has the instruction ``How would you move three of the boxes to the rack?'' Instruction satisfaction depends on the number of boxes on the rack, rather than the location of any specific object. 
    Moreover, \LG{} tends to correspond to planning tasks with many possible solutions. For instance, there may only be a single solution to the non-lifted instruction ``fetch me the red box and the blue box,'', but an LLM must contend with more options when asked to, for example, ``fetch any two boxes.''
    \item \textbf{Partial affordance perception (PAP):} Skill affordances cannot be perceived solely from the spatial relationships described in the initial state $s_1$. For instance, Task 5 (Fig.~\ref{fig:evaluation_task_suite}) requires the robot to put two boxes onto the rack. However, the scene description obtained through predicate classifiers $\mathcal{L}^P$ (described in Sec.~\ref{sec:goal_pred}) and the instruction $i$ do not indicate whether it is necessary to use a hook to pull an object closer to the robot first. 
\end{itemize}

% \LH{} tasks contain geometric dependencies than span across the sequence of actions, and are unlikely to be solved by greedily executing skills. 
% Consider Task 2 (Fig.~\ref{fig:evaluation_task_suite}), which requires the robot to pick and place obstructing boxes (i.e. blue and yellow) to enable a hook-push of the cyan box underneath the rack.
% Such dependencies are unlikely to be resolved by greedily executing skills in sequence, and thus highlights the importance of policy sequence optimization.
% \LG{} offers a means to increase the combinatorial complexity of planning problems through logical disjunction. 
% \LG{} tends to correspond to planning tasks with many possible solutions.
% For instance, there may only be a single solution to the non-lifted instruction ``fetch me the red box and the blue box,'', but the LLM must contend with more options when asked to ``fetch any two boxes.''
% Note that it may often be tedious or even impossible to represent \LG{} tasks propositionally, which complicates the use of the symbolic solvers that rely on planning domains.
% \PO{} represents a class of tasks where it is difficult to construct a geometrically feasible plan by solely considering the semantic information in the prompt (Sec.~\ref{subsec:prompt-engineering}).
% Since we wish to decrease the reliance of a complex set of predicates and pre-and post condition e.g. the large set of predicates what PDDLStream uses
% These tasks difficult to solve solely from semantic information included in the prompt (Sec.~\ref{subsec:prompt-engineering}), 
% Such tasks require reasoning about more than the semantic information included in the prompt (e.g. the instruction and scene description),
% (such as the instruction and description of the scene); these tasks require reasoning about the physical geometries of the scene itself.
% We also include combination tasks (e.g. \LG{} + \PO{}) to study the unique challenges that arise when tasks contain characteristics of several problem classes. 

% We refer to Appendix A for additional details on planning domains and instructions.
%We consider a family of language planners that relies on prompt-conditioned LLM likelihoods to deduce long-horizon action sequences $\pi_{1:H}$.
% While LLMs have been evaluated on their ability to solve various classes of planning problems~\cite{llms-cant-plan-2022}, the problem setting we consider differs.
% % While LLMs have been evaluated on their ability to solve various classes of planning problems~\cite{llms-cant-plan-2022} (akin to classical task planners~\cite{jiang2019task, long20033rd}), the settings we consider differ in several ways.
% For solving task and motion planning problems, the success of a plan is a product of both its symbolic correctness and its geometric feasibility.
% Thus in our planning domains, the notion of planning performance is shifted from how well the LLM plans in isolation to how useful the LLM is for task and motion planning. 
%Second, to eliminate the need for detailed symbolic domain specifications required by classical TAMP solvers, the LLM is only provided with the actions it may use $\mathcal{L}^\pi$ and a basic scene description in the form of spatial relationships like \graytext{On(box, rack)}.




% Planning domains are categorized by instruction type. 
% We include three instruction types of increasing complexity, adopting the structured language and long-horizon categories from a previous work [SayCan] and introducing the lifted instruction. 
% Brief descriptions are provided below.
% \begin{itemize}
%     \item \textbf{Structured language (SL)} instructions outline a high-level sequence of grounded actions the planner \textit{could} take to solve the tasks, while other solutions are possible.
%     \item \textbf{Long-horizon (LH)} instructions provide an explicit goal description over the scene configuration without explicitly specifying a long-horizon sequence of actions.
%     \item \textbf{Lifted (LF)} instructions describe an implicit goal description over object or scene properties without explicitly specifying a sequence of actions required to achieve them.
% \end{itemize}

% The instruction types ordered as above offer decreasing levels of task specificity to the planner, and as a result, obtaining solutions to Eq.~\ref{eq:task-score} becomes more difficult. 
% \textbf{SL} domains place the least dependence on LLM task planning by directly providing a correct action sequence, but may still require accounting for geometric action dependencies prevalent in TAMP problems.
% \LH{} instructions admit many solutions to a unique goal grounded over known objects, which places a higher dependence on the LLM to determine one such valid plan.
% Many real-world tasks are described over object properties instead of over objects themselves; for instance, \textit{put all the dairy products in the fridge}.
% \textbf{LF} domains mirror such tasks in an attempt to stress-test the semantic understanding of the LLM in the context of TAMP.
% We highlight that it may often be tedious or even impossible to represent \textbf{LF} tasks propositionally, which complicates the use of symbolic TAMP solvers.
% We refer to Appendix. A for additional details on instruction categories and planning domains.

\subsection{Evaluation and metrics}
\label{subsec:evaluation-metrics}


% We consider \scgs{} and \imgs{} reactive planners in that they rely on the LLM and value functions to ground admissible actions $\pi^{(k)}_h$ at the current timestep only. 
% In contrast, Text2Motion and \hm{} perform TAMP with an explicit plan in attempt to find one that satisfies the inferred goal state.
We evaluate Text2Motion and \hm{} by marking a plan as successful if, upon execution, it reaches a final state $s_H$ that satisfies the instruction $i$ of a given task. 
A plan is executed only if the final state $s_H$ predicted by the dynamics model satisfies the inferred goal propositions (Sec.~\ref{sec:goal_pred}). 

Two failure cases are tracked: i) planning failure: the method does not produce a sequence of skills $\pi_{1:H}$ whose optimized actions $a_{1:H}$ (Eq.~\ref{eq:taps-objective}) results in a state that satisfies $f_{\text{sat}}$ within a maximum plan length of $d_{\text{max}}$; ii) execution failure: the execution of a plan that satisfies $f_{\text{sat}}$ does not achieve the ground-truth goal of the task.

Since both our method and \hm{} use learned dynamics models to optimize actions with respect to (potentially erroneous) future state predictions, we perform the low-level execution of the skill sequence $\pi_{1:H}$ in closed-loop fashion. 
Thus, upon executing the skill $\pi_{h}$ at timestep $h$ and receiving environment feedback $s_{h+1}$, STAP~\cite{taps-2022} is called to perform policy sequence optimization on the remaining planned skills $\pi_{h+1:H}$. We do not perform task-level replanning.

\scgs{} and \imgs{} are reactive agents in that they execute the next best admissible skill $\pi_h$ at each timestep. 
Hence, we evaluate them in a closed-loop manner for a maximum of $d_{max}$ steps.
We mark a run as a success if the method issues the \texttt{stop} action and the current state satisfies the ground-truth goal. 
Note that this comparison is advantageous for these baselines because they are given the opportunity to perform closed-loop replanning at both the task and policy level, whereas task-level replanning does not occur for Text2Motion or \hm{}. 
We find that this advantage does not lead to measurable performance gains on the challenging evaluation domains that we consider.

% Since \scgs{} and \imgs{} are reactive planners in that they rely on the LLM and value functions to ground admissible actions $\pi^{(k)}_h$ at the current timestep only, there are subtle differences in their evaluation strategy.

% If termination is due to reaching $s_{max}$, we mark the run as a failure as the planner has not found a valid plan. Since our dynamics models are imperfect, we execute the predicted task plan open loop (no task plan level replanning) but execute the policy level plan closed loop. Since SayCan and Inner Monologue are not inherently planning methods, we evaluate them in a closed loop manner (on both the task and policy level) and report success if the method issues the `stop()' action and the current state satisfies the ground truth goal. Note that this comparison is not entirely fair because SayCan and Inner Monologue are essentially given the opportunity to perform closed loop policy \textit{and} task level replanning; however, we find that this advantage does not lead to measurable performance gains on the evaluation domain that we consider.

We report success rates and subgoal completion rates for all methods.
Success rates are averaged over ten random seeds per task, where each seed corresponds to a different geometric instantiation of the task (Sec.~\ref{subsec:task-suite}).
Subgoal completion rates are computed over all plans by measuring the number of steps an oracle planner must take to reach the ground-truth goal from the final state $s_H$ of the plan.
To further delineate the planning performance of Text2Motion from \hm{}, we also report the percentages of planning and execution failures. 

% Success rates and subgoal completion \todo{@chris can you elaborate on how this is computed e.g. normalized?} averaged over ten seeds are reported for all domains. Furthermore, for the methods that use dynamics models to perform forwards roll outs (Text2Motion and \hm{}), we report the number of plans reported vs number of times the plans succeed.

% However, solely using plan length as a proxy for optimality could be misleading, as certain \textit{successful} plans may violate constraints imposed over the primitives.
% For instance, we employ a safety constraint over all primitives $\pi$ which considers an action successful ($r_\pi = 1$) if no collisions occur with objects not intended to be acted upon, and unsuccessful ($r_\pi = 0$) otherwise.
% We therefore introduce a \textit{do-no-harm} metric to distinguish successful plans that respect the primitive safety constraints from those that do not.


% In our main experiments, we compare the success rate of our method, (integrated) Text2Motion, with several baselines: 

% \end{enumerate}

% \klin{Previous experiments section structure below. Bullets 3/4 use the `let's improve TAMP via language' perspective. I think we c/should split up our experiments section to have `experiments' and also `ablations' depending on what framing we should use. For example, the phrasing of `Solving unrepresentable TAMP problems` might be better for the `TAMP' perspective.}

% In our experiments, we test the following hypotheses:

% \begin{itemize}
%     \item \textbf{Goal classification} is a more achievable problem than open-ended plan generation for LLMs (Sec.~\ref{subsec:semantic-parsing-exp})
%     \item \textbf{Conditioning open-ended plan generation} on predicted symbolic goal propositions, a variant of `chain of thought' prompting \cite{wei_chain_2022}, promotes plan correctness (Sec.~\ref{subsec:semantic-parsing-exp})
%     \item \textbf{Text2Motion} realized with the \textbf{TAPS} skills motion planner can \textbf{solve unseen TAMP problems} (Sec.~\ref{subsec:language-tamp-exp})
%     \item \textbf{Text2Motion} can \textbf{solve tasks} that are \textbf{challenging to represent} for classical TAMP methods (Sec.~\ref{subsec:unrep-language-tamp-exp})
% \end{itemize}

% \subsection{Semantic parsing of language goals}
% \label{subsec:semantic-parsing-exp}

% \textbf{Planned experiments:}
% We plan to verify \textbf{H1}, \textbf{H2}, and \textbf{H3} in one combined experiment which compares the accuracy (with standard deviation) of Eqs.~\ref{eq:goal-classification}-~\ref{eq:plan-generation} under permutations of prompt size and relevancy.
% Let a prompt of \textit{size} $n$ contain $n$ tasks as chain-of-thought examples for the LLM.
% A \textit{relevant} task represents a chain-of-thought example for which the semantic similarity of its natural language goal $\mathfrak{g}_e$ to the current query $\mathfrak{g}$, as computed in an embedding space, is above a specified threshold: $\mathrm{CosSim}(\mathrm{LLM}(\mathfrak{g}_e), \mathrm{LLM}(\mathfrak{g}) > t_{\mathfrak{g}}$.

% Full symbolic domain specifications will be constructed for the long-horizon planning domains in TAPS.
% An automated procedure will be used to collect prompt and query datasets of PDDL problems.
% The query split will contain strictly longer and more complex problems than the prompt split. 
% The PDDL problem files offer a means to score the correctness of Eqs.~\ref{eq:goal-classification}-~\ref{eq:plan-generation}. 
% Correctness of symbolic goal $\mathcal{G}$ predicted by the LLM (Eq.~\ref{eq:goal-classification}) can be measured by directly comparing it to the ground truth symbolic goal $\mathcal{G}_{\text{GT}}$ in the PDDL problem file. 
% Correctness of a plan produced by the LLM (Eqs.~\ref{eq:cond-plan-generation}-~\ref{eq:plan-generation}) can be computed by checking if the resulting state of applying the plan from the initial state satisfies $\mathcal{G}_{\text{GT}}$.

% \textbf{The ideal results} will achieve the following criterion under permutations of prompt \textit{size} and \textit{relevancy}:
% \begin{enumerate}
%     \item Acc of Eq.~\ref{eq:goal-classification} $>$ Acc of Eq.~\ref{eq:cond-plan-generation} $>$ Acc of Eq.~\ref{eq:plan-generation} 
%     \item Acc of Eq.~\ref{eq:cond-plan-generation} with $\mathcal{G}_{GT}$ $>$ Acc of Eq.~\ref{eq:cond-plan-generation} with $\mathcal{G}$
%     \item Acc. of Eqs.~\ref{eq:cond-plan-generation}-~\ref{eq:plan-generation} increases by querying the LLM for larger sets of task plans; in proportion to $K$
% \end{enumerate}


% \subsection{Language TAMP with Text2Motion}
% \label{subsec:language-tamp-exp}

% \textbf{Planned experiments:}
% The number of tasks in long-horizon planning domains from TAPS (i.e. Hook Reach, Constrained Packing, Rearrangement Push) will be expanded from three to ten.
% New tasks will include variations of objects and colors to support TAMP queries that are combinatorial (e.g., put two red objects or two blue objects on the rack). 

% \textbf{The main results} will verify \textbf{H4} and \textbf{H5} by comparing the average success rates of our method to baselines such as SayCan.
% The performance upper-bound will be established by replacing our LLM task planner with a PDDL-based planner to acquire perfect symbolic plan skeletons.
% As in the previous section, ablations of our method should be run across prompt size and relevancy.
% We should emphasize the importance of each component of our method with a component hold-one-out evaluation; e.g., using Eq.~\ref{eq:plan-generation} instead of Eq.~\ref{eq:cond-plan-generation}, omitting policy sequence optimization with TAPS (effects on ranking plans, not resolving dependencies, etc.).
% It may also be helpful to provide a mini-experiment to demonstrate that the new more uniform learning strategy for the policies and Q-functions are improve success rates compared to TAPS' non-uniform SAC training. 

% Extras: one way to emphasize the generality of the LLM-based planner would be to train independent models for goal classification and plan prediction and evaluate their performance on hold-out validation tasks.


% \subsection{Solving unrepresentable TAMP problems}
% \label{subsec:unrep-language-tamp-exp}

% In this section, we provide some light results on our methods ability to solve tasks that cannot easily be described in PDDL, thereby making such problems incompatible with classical TAMP solvers.
% This wouldn't be an extensive empirical evaluation, and will likely be an good place to record some real-world demonstrations for supplementary material. 