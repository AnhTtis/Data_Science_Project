\section{Limitations and Future Work}
\label{sec:limitations}
The failure analysis conducted over the planning methods in Fig.~\ref{fig:failure} highlights several limitations of our language planning framework.
On Tasks 1-3 (\LH{}, \LG{}), Text2Motion incurs a larger planning and execution failure rate than \hm{} ($\sim$10\%) as a consequence of committing to a single skill sequence in the integrated search algorithm. 
In addition, we observed an undesirable pattern emerge in the planning phase of Text2Motion and the execution phase of \scgs{} and \imgs{}, where \textit{recency bias}~\cite{zhao2021calibrate} in the LLM would induce a cyclic state of repeating feasible skill sequences.

% In Fig.~\ref{fig:failure}, we analyse the several failure modes of Text2Motion: 
% (1) Goal proposition prediction failure (we did not observe this failure mode in our evaluation task suite), 
% (2) No successful plan found,
% (3) Plan found but execution error.

% Aside from the failure modes listed above, we also observed another failure mode that occurred in the planning phase of Text2Motion and execution phase of \scgs{} and \imgs{}: LLM would sometimes enter a \textit{cycle}, repeating sequences of geometrically feasible actions.

As with other methods that use skill libraries, Text2Motion is highly reliant on the fidelity of the low-level skill policies, their value functions, and the ability to accurately predicted future states with learned dynamics models.
This presents non-trivial challenges in scaling Text2Motion to high-dimensional observation spaces, such as images or pointclouds.

% Similar to approaches relying on skill libraries, the overall performance of Text2Motion is highly dependent on the quality of the low-level skill policies, their Q functions and the dynamics models. 
% Additionally, Text2Motion must contend with potentially erroneous latent state representations predicted by a learned dynamics model.

Text2Motion is mainly comprised of learned components, which comes with its associated efficiency benefits.
Nonetheless, runtime complexity was not a core focus of this work, and expensive optimization subroutines~\cite{taps-2022} were frequently called.
LLMs also impose an inference bottleneck as each API query (Sec. \ref{subsec:llm}) requires 2-10 seconds, but we anticipate improvements with advances in LLM inference techniques.

% In this work, we do not focus on runtime optimization and thus run policy sequence optimization from scratch at each planning iteration for Text2Motion and \hm{}.
% We further note that our overall inference time is also bottle-necked by that of the LLM. In this case, each query to the LLM API (see Sec. \ref{subsec:llm}) used in this work can take anywhere between two to ten seconds).
% That said, we expect query times to decrease as LLM inference techniques improve.

% We thus outline several avenues for future work:
% (1) Maintain a more diverse set of task plans through beam search with beam size greater than one. Doing so would allow Text2Motion to handle longer horizon robot manipulation tasks where it helps to maintain diverse plans (along with the proposed iterative integrated planning method). We empirically found that the LLMs we used were not suited to guide search for large beam sizes as the scores generated across skill \textit{sequences} were not calibrated relative to each other. 
% However, as their capabilities improve, LLMs should eventually be able to accurately rank and score skill sequences.
% (2) Decrease inference time by caching policy sequence optimization distributions from earlier planning timesteps.

We outline several avenues for future work based on these observations.
First, we aim to explore algorithmic design choices that unify the strengths of integrated (feasibility) and hierarchical (diversity) planning.
Doing so would enable Text2Motion to solve longer horizon robot manipulation tasks where it helps to maintain diverse plans.
Such strategies may require LLMs to produce calibrated scores of entire skill sequences relative to each other, which we empirically found challenging and warrants further investigation.
Second, there remains an opportunity to increase the plan-time efficiency of our method, for instance, by warm starting policy sequence optimization with distributions cached in earlier planning iterations.
Lastly, we hope to explore the use of multi-modal foundation models~\cite{driess2023palm, openai2023gpt4} that are visually grounded, and as a result, may support scaling our planning system to higher dimensional observation spaces.
Progress on each of these fronts would constitute steps in the direction of reliable and real-time language planning capabilities.


% \section{Discussion}
% \label{sec:discussion}

% \klin{need to format}
% \begin{enumerate}
    % \item One class of failure modes that we observed involved the LLM deciding to repeating certain actions it had previous seen. For example, one of the failure modes in figure \ref{fig:failure} (LH LG PO) requires the robot to use the hook to pick and place two boxes onto the rack, then use the hook to pull an object to be inside the workspace and then pick and place that object onto the rack. However, if the task sequence planed so far has a small deviation from a more `optimal' plan, the LLM tends to place a higher score on actions it had previously executed. ['pick(red-box)', 'place(red-box, rack)', 'pick(cyan-box)', 'place(cyan-box, rack)', 'pick(hook)', 'place(hook, table)', 'pick(hook)', 'pull(yellow-box, hook)', 'place(hook, table)', 'pick(red-box)'].
    % \item LLM goal prediction can sometimes be wrong - if being used as chain of thought, especially for code-davinci, need to specify ``\textit{Predicted} goal predicate set'' rather than ``Goal predicate set''. Text davinci models don't get affected by wording as much.
% \end{enumerate}
