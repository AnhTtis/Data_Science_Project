\section{Text2Motion}
\label{sec:text2motion}

%\subsection{Integrating Language-based Task Planning with Policy Sequence Optimization}

Our goal is to find a geometrically feasible long-horizon plan for a given natural language instruction. We follow a modular approach similar to traditional TAMP methods but replace the commonly used symbolic task planner with an LLM. Doing so allows us to broaden the task planning domain without the need for tedious manual construction. The core idea of this paper is ensure the geometric feasibility of an LLM task plan---and thereby its correctness---by predicting the success probability of learned skills that are sequenced according to the task plan. More specifically, we run an iterative search, where at each iteration we query the LLM for a set of candidate skills given the current symbolic state of the environment. For each of these skills, we optimize their action parameters and compute their success probability. This success probability is multiplied by the language model likelihood for the predicted skill to produce an overall score. The skill with the highest overall score is added to the current plan. A dynamics model is used to predict the geometric state that would result from executing this skill. If this predicted state satisfies a termination condition predicted by the LLM, then the instruction is satisfied and the plan is returned. Otherwise, the predicted geometric state is used to start the next iteration of the search. A visualization is provided in Fig.~\ref{fig:system}.

% To find geometrically feasible long-horizon plans with the LLM, we propose an integrated TAMP method that interweaves LLM task planning with motion planning. As a brief overview, the planning iteration starts by asking the LLM to generate a candidate skill skill to execute given the current environment state. Then, a motion planner finds optimal skill parameters for this skill and computes its predicted probability of success. This success probability is multiplied by the language model likelihood for the predicted skill to produce a TAMP score. This is done for a set of candidate skill skills generated by the LLM, and the one with the highest TAMP score is selected. A dynamics model provided by the motion planner predicts the next state that would result from executing this skill, and this predicted state is used to start the next planning iteration. This loop repeats until a termination condition predicted by the LLM is met.

%This loop repeats until the LLM predicts the ``stop'' token, at which point the long-horizon plan is returned.

This iterative approach can be described as a decomposition of the joint probability in Eq.~\ref{eq:tamp-score} by timestep $h$:
\begin{align}
    p(\pi_{1:H}, r_{1:H} \mid i, s_1)
        &= \prod_{h=1}^H p(\pi_h, r_h \mid i, s_1, \pi_{1:h-1},  r_{1:h-1}) \label{eq:tamp-score-decomp} \\
        &\approx \prod_{h=1}^H p(\pi_h, r_h \mid i, s_1, \pi_{1:h-1}) \label{eq:tamp-score-indep}
\end{align}
Eq.~\ref{eq:tamp-score-decomp} is simply a factorization of Eq.~\ref{eq:tamp-score} using conditional probabilities. 
In Eq.~\ref{eq:tamp-score-indep}, we make the assumption that skill $\pi_h$ and reward $r_h$ can be determined solely from initial state $s_1$ and sequence of prior skills $\pi_{1:h-1}$, and thus is independent of prior rewards $r_{1:h-1}$. 
Specifically, Eq.~\ref{eq:tamp-score-indep} more accurately approximates Eq.~\ref{eq:tamp-score-decomp} when $\pi_{1:h-1}$ is foreknown to receive rewards $r_{1:h-1}$. 
We induce this setting through geometric feasibility planning (elaborated upon in Sec.~\ref{sec:geo_feas}).
% In particular, when $\pi_{1:h-1}$ is foreknown to receive rewards $r_{1:h-1}$ (the setting we induce, elaborated upon in Sec.~\ref{sec:geo_feas}), Eq.~\ref{eq:tamp-score-indep} is expected to sufficiently approximate Eq.~\ref{eq:tamp-score-decomp}.
% We motivate this assumption by foreknowledge of $\pi_{1:h-1}$ receiving rewards $r_{1:h-1}$ (elaborated upon in Sec.~\ref{sec:geo_feas}), and thus, we expect a sufficient approximation of Eq.~\ref{eq:tamp-score-decomp} by Eq.~\ref{eq:tamp-score-indep}.
% This simplification allows us to further decompose Eq.~\ref{eq:tamp-score-indep}. 
This allows us to further decompose Eq.~\ref{eq:tamp-score-indep} into the joint probability of $\pi_h$ and $r_h$, which we define as the skill score $S_{\text{skill}}$:
\begin{equation}
    S_{\text{skill}}(\pi_h)
        = p(\pi_h, r_h \mid i, s_1, \pi_{1:h-1}). \label{eq:tamp-step-score}
\end{equation}

Each planning iteration is responsible for finding the skill $\pi_h$ that maximizes the skill score at timestep $h$. We decompose this score into the conditional probabilities of $\pi_h$ and $r_h$:
\begin{equation*}
    S_{\text{skill}}(\pi_h)
        = p(\pi_h \mid i, s_1, \pi_{1:h-1}) \, p(r_h \mid i, s_1, \pi_{1:h})
        \label{eq:s_skill}
\end{equation*}
We define the first factor in this product to be the language model likelihood score:
\begin{equation}
    S_{\text{llm}}(\pi_h) = p(\pi_h \mid i, s_1, \pi_{1:h-1}). \label{eq:lm-step-score}
\end{equation}
This represents the task planning step, where at timestep $h$, we ask the LLM to predict skill $\pi_h$ given instruction $i$, initial state $s_1$, and sequence of previously planned skills $\pi_{1:h-1}$.

The second factor in Eq.~\ref{eq:s_skill} represents the policy sequence optimization step, where we evaluate the geometric feasibility of the skill sequence $\pi_{1:h}$. 
The reward $r_h$ is conditionally independent of instruction $i$ given the initial state and skill sequence $\pi_{1:h}$. We thus define the geometric feasibility score:
\begin{equation}
S_{\text{geo}}(\pi_h) = p(r_h \mid s_1, \pi_{1:h}). \label{eq:motion-step-score}
\end{equation}
The skill score to be optimized at each iteration of planning is therefore the product of the LLM likelihood and the geometric feasibility of the planned skill sequence:
\begin{equation}\label{eq:tamp-step-score-decomp}
    S_{\text{skill}}(\pi_h) = S_{\text{llm}}(\pi_h) \cdot S_{\text{geo}}(\pi_h).
\end{equation}
Text2Motion alternates between incrementally optimizing $S_{\text{llm}}(\pi_h)$
and $S_{\text{geo}}(\pi_h)$ at each timestep $h$. The computation of these scores as well as the terminating strategy for the algorithm are described in more detail in the following subsections.


\subsection{LLM task planning}\label{sec:llm-task-planning}
At each iteration $h$, we optimize $S_{\text{llm}}(\pi_h)$ (Eq.~\ref{eq:lm-step-score}) by querying an LLM to generate $K$ candidate skills  $\{\pi_h^1, \dots, \pi_h^K\}$, each accompanied by their own language model scores $S_{\text{llm}}(\pi_h^k)$.
These scores represent the likelihood of skill $\pi_h^k$ being the correct skill to execute from a language modeling perspective to satisfy instruction $i$.
These candidates are then checked for geometric feasibility as described in Sec.~\ref{sec:geo_feas}. 



The input prompt contains context $c$ in the form of several long-horizon planning examples to inform the LLM of the available skills in the robot's library and their usage semantics (see Sec.~\ref{subsec:prompt-engineering}). 
The prompt also contains the instruction $i$, history of planned skills $\pi_{1:h-1}$, initial state $s_1$, and predicted states $s_{2:h}$ up to step $h$. Descriptions of the states $s_{1:h}$ come in the form of sets of binary propositions such as \graytext{on(milk, table)} or \graytext{under(yogurt, rack)} that describe spatial relationships between objects. To generate these propositions for initial state $s_1$, we use a heuristic approach described in the Appendix (Appx.~\ref{sec:scene-descr-symbolic}). Our framework is agnostic to the specific approach. Suitable examples could be scene graph generation methods~\cite{rosinol2021kimera, hughes2022hydra} or forms of text-based scene description~\cite{gu2021open, kuo2022findit, zeng2022socratic}. States $s_{2:h}$ are predicted given skills $\pi_{1:h-1}$ and their associated dynamics models. We then automatically generate corresponding language descriptions $s_{2:h}$ from these predicted states. Sec.~\ref{subsec:prompt-engineering} provides an example.


How many skills $H$ are required to satisfy the instruction $i$ is unknown, so it is up to the LLM to decide when to terminate. 
We use the goal proposition prediction method described in Sec.~\ref{sec:goal_pred} to determine when to terminate.


% In each iteration $h$, we query an LLM for a candidate next skill $\pi_h$ to be executed for achieving a task, and compute its language model score $S_{\text{skill}}(\pi_h)$ in Eq.~\ref{eq:lm-step-score}.
% This score represents the likelihood of skill $\pi_h$ being the correct skill to execute from a language modeling perspective to satisfy instruction $i$.
% The input prompt contains context $c$ in the form of several long-horizon planning examples to inform the LLM of the available skills in the robot's library and their usage semantics. 
% The prompt also contains the instruction $i$, history of planned skills $\pi_{1:h-1}$, initial state $s_1$, and predicted states $s_{2:h}$ up to step $h$. Descriptions of the states $s_{1:h}$ come in the form of sets of binary propositions such as \graytext{on(milk, table)} or \graytext{under(yogurt, rack)} that describe spatial relationships between objects. To generate these propositions for initial state $s_1$, we use a heuristic approach described in the supplemental material. Our framework is agnostic to the specific approach. Suitable examples could be scene graph generation methods such as~\cite{rosinol2021kimera, hughes2022hydra}. States $s_{2:h}$ are predicted given skills $\pi_{1:h-1}$ and associated dynamics models. We then automatically generate corresponding language descriptions $s_{2:h}$ from these predicted states. Sec.~\ref{subsec:prompt-engineering} provides an example.

% We query the LLM to output up to $K$ candidate skills (see Sec.~\ref{subsec:prompt-engineering}) $\{\pi_h^1, \dots, \pi_h^K\}$, each with their own language model scores (Eq.~\ref{eq:lm-step-score}). 
% These candidates are then checked for geometric feasibility as described in Section~\ref{sec:geo_feas}. 
% How many skills $H$ are required to satisfy the instruction $i$ is unknown, so it is up to the LLM to decide when to terminate. 
% We use the goal proposition prediction method described in Section~\ref{sec:goal_pred} to decide when to terminate.

%For task planning, the LLM is given several long-horizon planning examples as context to inform it of the available skills in the robot's library and their usage semantics. It is also given a description $\mathfrak{s}$ of the current environment state $s$ in the form of a list of binary propositions such as \graytext{On(milk, table)} or \graytext{Under(yogurt, rack)} that describe spatial relationships between objects. Our approach is agnostic to the specific approach that is used for generating these propositions from image observations. Suitable examples could be scene graph generation methods such as~\cite{rosinol2021kimera, hughes2022hydra}. The details of our approach that is currently based on heuristics are described in the supplemental material. 
%already exists; scene graph generation methods such as~\cite{rosinol2021kimera, hughes2022hydra} have shown the ability to generate these spatial descriptions of images. In this work, we use handcrafted heuristics to provide these spatial descriptions, and leave the integration of learned methods for scene graph generation to future work.

%The LLM is asked to output the next skill $\pi_h$ to execute given the instruction $\mathfrak{i}$, history of planned skills $\pi_{1:h-1}$, initial state $\mathfrak{s}_1$, and predicted states $\mathfrak{s}_{2:h}$ up to step $h$. These states $s_{2:h}$ are predicted by the dynamics model. Corresponding language descriptions $\mathfrak{s}_{2:h}$ are generated from these state predictions. 

% We query the LLM to output up to $K$ candidate skills (see Sec.~\ref{subsec:prompt-engineering}) $\{\pi_h^1, \dots, \pi_h^K\}$ each with their own language model score (Eq.~\ref{eq:lm-step-score}). These candidates are then checked for geometric feasibility as described in Section~\ref{sec:geo_feas}. 

% How many skills $H$ are required to satisfy the instruction $i$ is unknown, so it is up to the LLM to decide when to terminate. We use the goal proposition prediction method described in Section~\ref{sec:goal_pred} to decide when to terminate.

% Therefore, the LLM may also predict ``stop" as the next language token, at which point the instruction is deemed to be satisfied and the current plan is returned.



% For each candidate skill, we perform motion planning to check its geometric feasibility, and then select the one with the highest combined TAMP score (Eq.~\ref{eq:tamp-step-score}). This selected skill $\pi_h$ is added to the current plan and included for planning the next timestep.

% The LLM is asked to output $K$ candidate skills $\{\pi_h^1, \dots, \pi_h^K\}$ for the current planning iteration $h$.


\subsection{Geometric feasibility planning}\label{sec:geo_feas}

% While the LLM may be used to predict the most probable next skill from a language modeling perspective, this skill may either be symbolically incorrect (e.g. grasp an object while already holding something) or may neglect geometric constraints such as the configuration of the environment or the kinematics of the robot. 
% Thus, we propose to select the next action $\pi_h$ by also considering its geometric feasibility $S_{\text{geo}}(\pi_h)$ (Eq.~\ref{eq:motion-step-score}) in the context of the full skill sequence $\pi_{1:h}$.

% We therefore propose selecting the next action $\pi_h$ by also considering its geometric feasibility $S_{\text{geo}}$ (Eq.~\ref{eq:motion-step-score}) in the context of executing the full skill sequence $\pi_{1:h}$.

While the LLM may be used to predict the most probable next skill $\pi_h$ from a language modeling perspective, the skill may either be symbolically incorrect (e.g. grasp an object while already holding something) or may neglect geometric constraints (e.g. robot kinematics) in the current state. 
% that the preceding skill subsequence $\pi_{1:h-1}$ is led the .
% While the LLM may be used to predict the most probable next skill $\pi_h$ from a language modeling perspective, $\pi_h$ may neglect geometric constraints (e.g. environment configuration) when taken in the context of its preceding skill subsequence $\pi_{1:h-1}$.
Thus, we propose to compute the geometric feasibility score $S_{\text{geo}}(\pi_h)$ (Eq.~\ref{eq:motion-step-score}) of skill $\pi_h$ after maximizing the combined success probability of the entire skill sequence $\pi_{1:h}$.
We term this process \textit{policy sequence optimization}.
As a result, the skills $\pi_{1:h-1}$ are expected to receive rewards $r_{1:h-1}$, which supports the independence assumption of Eq.~\ref{eq:tamp-score-indep}.

% To this end, we first resolve geometric dependencies across the skill sequence $\pi_{1:h}$ by maximizing the combined success likelihood of the skills' individual actions $a_{1:h}$ (Eq.~\ref{eq:motion-score}).
% This can be computed as the product of step reward probabilities,
% combined success likelihood of the skills' individual actions $a_{1:h}$ (Eq.~\ref{eq:motion-score}).
% This can be computed as the product of step reward probabilities,

To this end, we first resolve geometric dependencies across the full skill sequence $\pi_{1:h}$ by maximizing the 
product of step reward probabilities of the skills' individual actions $a_{1:h}$:
% \begin{equation}
%     p(r_{1:h} \mid s_1, \pi_{1:h}) = \operatorname{E}_{s_{2:h} \sim T}\left[\prod_{t=1}^h p(r_t \mid s_t, a_t)\right], \label{eq:taps-objective}
% \end{equation}
% \begin{equation}
%     a_{1:h}^* = \arg \max_{a_{1:h}} \, \operatorname{E}_{s_{2:h} \sim T}\left[\prod_{t=1}^h p(r_t \mid s_t, a_t)\right], \label{eq:taps-objective}
% \end{equation}
\begin{equation}
    a_{1:h}^* = \arg \max_{a_{1:h}} \, \prod_{t=1}^h p(r_t \mid s_t, a_t), \label{eq:taps-objective}
\end{equation}
where future states $s_{2:h}$ are predicted by dynamics models $s_{t+1} = T^\pi(s_t, a_t)$. Note that the reward probability $p(r_t = 1\mid s_t, a_t)$ is equivalent to a Q-function $Q^{\pi_t}(s_t, a_t)$ for skill $\pi_t$ in a contextual bandit setting with binary rewards.
% ; for simplicity, we will refer to these reward probabilities as Q-functions. 
Text2Motion is agnostic to the specific approach on how to learn the dynamics $T^\pi$ and Q-functions $Q^\pi$ and on how to optimize skill policy outputs $a_{i} \sim \pi_{i}(a \vert s_i)$.
%It is also possible to consider a stochastic formulation of $T^\pi(s_{t+1} \vert s_t, a_t)$.
In the experiments, we leverage STAP~\cite{taps-2022}.

% To this end, we select the actions $a_{1:h}$ (where $a$ is the output of a particular skill's policy that can be further optimized such as through sampling or optimization using a $Q$ function as the objective) for the entire sequence of skills $\pi_{1:h}$ to maximize their combined probability of success (Eq.~\ref{eq:motion-score}). This can be computed as the product of step reward probabilities,
% \begin{equation}
%     p(r_{1:h} \mid s_1, \pi_{1:h}) = \operatorname{E}_{s_{2:h} \sim T}\left[\prod_{t=1}^h p(r_t \mid s_t, a_t)\right], \label{eq:taps-objective}
% \end{equation}
% where future states $s_{2:h}$ are predicted by dynamics models $s_{t+1} \sim T^\pi(s_t, a_t)$. Note that the reward probability $p(r = 1 \mid s_h, a_h)$ is equivalent to a Q-function $Q^{\pi_h}(s_h, a_h)$ for skill $\pi_h$ in a contextual bandit setting with binary rewards; for simplicity, we will refer to these reward probabilities as Q-functions. How the dynamics $T^\pi$ and Q-functions $Q^\pi$ are learned and how skill policy outputs $a_{1:h}$ are optimized are outside the scope of this paper. Text2Motion is agnostic to the specific choice. In the experiments, we leverage TAPS~\cite{taps-2022}.

We can then estimate the geometric feasibility $S_{\text{geo}}(\pi_h)$ (Eq.~\ref{eq:motion-step-score}) in the context of the full skill sequence $\pi_{1:h}$ by the Q-value of the last action,
% \begin{equation}
%     p(r_h \mid s_{1:h}, \pi_{1:h}) = Q^{\pi_h}(s_h, a_h),
% \end{equation}
\begin{equation}\label{eq:q-function-score}
    p(r_h = 1 \mid s_{1}, \pi_{1:h}) \approx Q^{\pi_h}(s_h, a^*_h),
\end{equation}
% where $a_{1:h}$ are determined by TAPS and $s_{2:h}$ are predicted by the dynamics model. 
where $a^*_{h}$ is determined by STAP and $s_{h}$ is predicted by the dynamics model. 
The Q-value is multiplied by the language model likelihood (Eq.~\ref{eq:lm-step-score}) to produce the combined overall score (Eq.~\ref{eq:tamp-step-score-decomp}) for this skill. 

This process is performed for each of the candidate skills $\{\pi_h^1, \dots, \pi_h^K\}$ generated by the LLM at iteration $h$, and the one with the highest overall score is added to the current running plan. 
This skill should be both geometrically feasible and help make progress towards satisfying the instruction $i$.

% Among the set of $K$ candidate skills $\{\pi_h^1, \dots, \pi_h^K\}$ for timestep $h$ generated by the LLM, the one with the highest overall score is added to the current running plan. This skill should be both geometrically feasible and help make progress towards satisfying the instruction $i$.
During planning, the LLM may propose skills $\pi_h^k$ that are out-of-distribution (OOD) given the current state.
For instance, $\pi_h^k$ may be symbolically incorrect, like \graytext{Pick(table)} or \graytext{Place(yogurt, table)} when the yogurt is not in hand.
In a more subtle case, the state may have drifted beyond the training distribution of $\pi_h^k$ as a result of the preceding subsequence $\pi_{1:h-1}$.
% During planning, the LLM may propose a sequence of skills that produces states which are out-of-distribution (OOD) for the learned skills, such as performing a ``pick" action on an unmovable object like the table, or performing a ``place" action when the robot has not yet grasped any object. 
Such skills may end up being selected if we rely on learned Q-values, since the Q-value for an OOD state-action pair may be spuriously high. 
We therefore reject skills proposed by the LLM that are predicted to be OOD by an ensemble of Q-networks \cite{lakshminarayanan2017simple}.
% For OOD detection, we use ensembles of Q-networks for their ease of calibration. 
We consider a skill to be OOD if the standard deviation for the associated Q-value predicted by the ensemble is greater than a threshold $\epsilon_{\text{OOD}}$.

If, after OOD rejection, a state $s_{h}$ in an optimized policy sequence is classified to satisfy the instruction (details in Sec.~\ref{sec:goal_pred}), we terminate search and return the solution $a_{1:h}$ for execution. 
If we instead hit the maximum planning horizon, we register a planning failure and do not execute any actions. 
% We note that it is also possible to take the first (geometric feasibility optimized) action and replan at the next step, as our imperfect dynamics models may have lead to a imperfect geometric states than were not classified as instruction satisfying.

% If the LLM predicts the ``stop" token as the most likely candidate skill for timestep $H$, then the instruction $\mathfrak{i}$ is deemed to be satisfied, and we can return the solution plan $a_{1:H-1}$ for execution. If the LLM does not predict ``stop", then we may terminate when the maximum planning horizon is reached, or if any of the Q-values along the output plan are below $\epsilon_{OOD}$, indicating a likely failure. In the case of such predicted failures, we simply execute the first action in the plan and re-plan from the updated observation.


\subsection{LLM goal prediction} \label{sec:goal_pred}

Given a library of predicate classifiers $\mathcal{L}^P$ describing simple geometric relationships of objects in the scene (e.g. \{\graytext{on(a, b)}, \graytext{inhand(a)}, \graytext{under(a, b)}\}), a list of objects $O$ in the scene, and an instruction $i$, we use the LLM to predict $j$ goal proposition sets $g_{1:j}$ that would satisfy the instruction. 

For example, given an instruction $i=$``stock all the dairy products onto the rack,'' the set of objects $O=$ \{\graytext{table}, \graytext{milk}, \graytext{keys}, \graytext{yogurt}, \graytext{rack}\}, and a library of predicate classifiers with their natural language descriptions $\mathcal{L}^P=$ \{\graytext{on(a, b)}, \graytext{inhand(a)}, \graytext{under(a, b)}\}, the LLM might predict the goal proposition set $g_1=$ \{\graytext{on(milk, rack)}, \graytext{on(yogurt, rack)}\}. If the instruction were $i=$ ``put one dairy product on the rack'', the LLM might predict $g_1=$\{\graytext{on(milk, rack)}\}, $g_2=$\{\graytext{on(yogurt, rack)}\}. 

We define a satisfaction function $f_{\text{sat}}^{g_{1:j}}(s) \in \{0, 1\}$ that checks whether the current symbolic state $s$---obtained from the predicate classifiers $\mathcal{L}^P$---satisfies any of the goal proposition sets $g_{1:j}$.
We use $f_{\text{sat}}$ in two ways: i) as a success classifier to check whether a given state satisfies the natural language instruction and ii) as a chain-of-thought prompt \cite{wei2022chain} when prompting the LLM for action sequences (see Sec.~\ref{subsec:prompt-engineering}).

% \subsection{@Toki New stuff here}

% \subsubsection{Integrated (beam search w/ beam size 1)} -  \href{https://github.com/kevin-thankyou-lin/text2motion/blob/a65686c9163bca99a0709df3308c975598bbd36a/temporal_policies/task_planners/beam_search.py#L585}{code is here}:

% \begin{enumerate}
%     \item Have current state (+ all history) as the 'root' node.
%     \item get-successors(state) by prompting LM to generate K actions
%     \item score successor state by i) perform TAPS optimization from ground truth env state up till the particular node we're looking at; the affordance V score for the successor state is the TAPS optimized last action and LM score is LM score
%     \item check if any successor state reaches goal; if so check if the Q values along the trajectory are all > 0.45 (w/ stds from Q function <= ~0.15 depedning on the skill)
%     \item kill all but 1 successor states
%     \item continue this loops til max depth of until we've found an end node
%     \item take first action and repeat
% \end{enumerate}

% \subsubsection{3 ways to check for is-end both for planning termination criteria and execution termination criteria}

% \begin{enumerate}
%     \item saycan-style of scoring `stop()', given sequence of observed object relationships (containing on/inhand/under) and executed actions
%     \item predicting `stop()', given sequence of observed object relationships (containing on/inhand/under) and executed actions
%     \item predicting goal propositions first and then taking a raw state and checkig if those goal propositions hold 
% \end{enumerate}

% For the `best' method, I think the predicting `done' method is the cleanest (while 3) is the cheapest option)

% \subsubsection{hierarchical baseline}

% \begin{enumerate}
%     \item ask LLM to shoot out K plans
%     \item test if any plan reaches the goal using the `best' is-end check from above; if so, check if the plan's Q values (post taps optimization) are all > 0.45 w/ std > ~0.15
%     \item if there are 2+ plans left, take the plan with highest Q product (use closed loop motion planning - no more task planning)
%     \item if no plans left, fall back to saycan and take a step
% \end{enumerate}


% \subsection{@Toki Old stuff here}
% Given a natural language instruction, we aim to find a sequence of skills and continuous parameters that satisfy the instruction.
% We solve this problem in two steps. First, we parse the natural language instruction to a proposition that evaluates to true or false. Second, we find a sequence of skills and their continuous parameters that satisfy the  proposition.

% \subsection{Semantic parsing of language instructions}
% \label{subsec:semantic-parsing-method}

% A challenge in performing TAMP from language is that goals $\mathfrak{i}$ are expressed in free-form text instead of as a logical expression of predicates $\mathcal{G}$, which TAMP solvers require.

% % Given a library of predicate classifiers $\mathcal{L}^P$, $\mathcal{G}$ can also be used to validate the final geometric state of a motion plan. $\mathcal{G}$ thereby imposes useful symbolic and geometric constraints on TAMP solvers.

% % In AI task planning, $\mathcal{G}$ defines goal states for which the resulting state of a correct task plan from $\mathcal{I}$ must lie within.

% Therefore, given a prompt $\mathfrak{p}$ that consists of the natural language instruction, the first step of our method parses the natural language instruction $\mathfrak{i}$ into python code that composes the predicates and objects into a single goal proposition $\mathcal{G}$ using an LLM (Eq.~\ref{eq:goal-classification}).
% \begin{equation}
%     \label{eq:goal-classification}
%     \mathcal{G} = \mathrm{LLM}(\mathfrak{i}, \mathfrak{p})
% \end{equation}

% We use the parsed goals in two ways: i) as a variant of a chain-of-thought prompt \cite{wei_chain_2022} when prompting the LLM for action sequences ii) as a success classifier to check whether a given state satisfies the natural language instruction.

% % \klin{The following should be in the next section?}
% % Because task planning cannot be performed in the absence of a symbolic transition model $\mathcal{T}$, the second step of our planner performs open-ended task plan generation with the LLM conditioned on $\mathcal{G}$ (Eq.~\ref{eq:cond-plan-generation}).
% % \begin{equation}
% %     \label{eq:cond-plan-generation}
% %     \tau = \mathrm{LLM}(\mathcal{G}, \mathfrak{i}, \mathfrak{p})
% % \end{equation}

% % Without conditioning on $\mathcal{G}$, plans produced by LLMs in open-ended fashion have been shown to contain inaccuracies.
% % Thus, \textbf{our first hypothesis (H1)} is that conditioning LLM plans via Eq.~\ref{eq:cond-plan-generation} will be more accurate than Eq.~\ref{eq:plan-generation}, shown below.
% % \begin{equation}
% %     \label{eq:plan-generation}
% %     \tau = \mathrm{LLM}(\mathfrak{i}, \mathfrak{p})
% % \end{equation}
% % Furthermore, the intended downstream uses of $\mathcal{G}$ places a high fidelity requirement on its prediction; any errors will cascade to task planning and motion planning, potentially nullifying correct solutions. 
% % Hence, \textbf{our second hypothesis (H2)} is that when prompted equivalently, LLMs are better able to perform semantic goal parsing (Eq.~\ref{eq:goal-classification}) than open-ended plan generation (Eq.~\ref{eq:plan-generation}). 
% % Our \textbf{third hypothesis (H3)} is that by querying the LLM for $K$ task plans instead of a single one via Eqs.~\ref{eq:cond-plan-generation}-~\ref{eq:plan-generation}, we acquire diversity that increases the likelihood of a correct plan for unseen tasks. 

% \subsection{Language TAMP with Text2Motion}
% \label{subsec:language-tamp-method}

% \klin{Two potential `methods', previously, I was more confident the integrated approach would definitely work better, but unclear now because the LM probabilities are kind of wild and the two normalization schemes I've tried both have issues. Will check how well adding extra prompts does. Maybe we can just tune the normalization coefficient until things work okay?}

% \textbf{Hierarchical Text2Motion} 

% \begin{enumerate}
%     \item Query LM for $K$ plans
%     \item TAPS optimize all plans
%     \item Filter out non-goal reaching plans \item Select remaining plans with maximum Q product (normalized)
% \end{enumerate}

% \textbf{Integrated Text2Motion} This approach uses a beam search on the sequence of discrete skill actions. At each iteration of increasing the depth of the search, we re-run TAPS until convergence to get the value of the discrete skill action sequence.

% % In the previous section, we described a process that semantically parses a language instruction $\mathfrak{i}$ (with prompt $\mathfrak{p}$) into a propositional goal $\mathcal{G}$ and a set of $K$ candidate plan skeletons $\{\tau_k\}_{k=1}^K$. 
% We assume the existence of a manipulation skill $\pi$ for every symbolic action $a \in \mathcal{A}$ available to the LLM when constructing the plan skeletons.
% Thus, each candidate plan skeleton defines a sequence of skills $\tau = [\pi_1, \ldots, \pi_H]$ that \textit{might} solve the task if executed by the robot.

% A plan skeleton must satisfy two criterion to solve a task: (a) it must be geometrically feasible; (b) executing the plan skeleton must result in a final geometric state that is consistent with goal predicates $\mathcal{G}$.
% TAPS is a recent work that fits these requirements: it resolves long-horizon action dependencies in plan skeletons, returning an optimized action plan $\xi = [a_1,, \ldots, a_H]$ and success likelihood for the plan $J$.
% \begin{equation}
%     \label{eq:taps-call}
%     \xi, J = \mathrm{TAPS}(\tau, \mathcal{L}^\pi)
% \end{equation}
% TAPS' training framework is used to learn a manipulation skill library $\mathcal{L}^\pi$ which includes skill-specific Q-functions $Q^\pi$ and dynamics models $\mathcal{T}^\pi$.

% We apply Eq.~\ref{eq:taps-call} to all plan skeletons $\{\tau_k\}_{k=1}^K$, and use the dynamics models $\mathcal{T}^\pi$ to rollout each action plan $\xi$ into a sequence of geometric states $\chi = [x_1, \ldots, x_H]$.
% Predicate classifiers $\mathcal{L}^P$ are used to check the final state of each plan $x_H$ against the predicted goal predicates $\mathcal{G}$.
% A set of satisfying plans are computed, excluding any plans that do not satisfy the goal:
% \begin{equation}
%     \label{eq:optimal-plan}
%     \Xi = \{\xi\; |\; p(\chi[-1]) = True, \forall str(p) \in \mathcal{G}\}.
% \end{equation}

% The optimal plan is then the most geometrically feasible plan that satisfies the goal:
% \begin{equation}
%     \label{eq:optimal-plan}
%     \xi^* = \underset{J}{\arg max}\; \Xi.
% \end{equation}

% We highlight that $\xi^*$ need not be followed open-loop.
% Repeating the TAMP procedure after executing the first action $\xi^*[0]$ achieves closed-loop behavior.
% In this way Text2Motion is made more robust to imperfect dynamics and value functions, failed actions, and environmental perturbations. 

% There are two underlying hypotheses to performing TAMP with Text2Motion:
% \begin{itemize}
%     \item Success likelihoods computed by TAPS are robust enough to rank plans of various length and complexity \textbf{(H4)}
%     \item Text2Motion can generalize zero-shot to unseen long-horizon TAMP problems better than baselines \textbf{(H5)}
% \end{itemize}

% \subsection{Cost reduction via Generator-Scorer}
% Generator-Scorer a two step process that drastically reduces the cost of using an LLM to score actions for robotics. It involves adding skill function signatures in the prompt, such as \texttt{available skills: pick(a), place(a, b), push(a, b, c), pull(a, b)}

% Then:

% \begin{enumerate}
%     \item Prompt the LLM to generate a sequence of actions
%     \item Take those actions and score them
% \end{enumerate}

% \klin{Adding as place holder for contribution 3; makes most sense if the integrated approach works. Unclear if/how to include this contribution if the integrated approach is too flakey ...}

% We empirically found that using \texttt{text-davinci-003} to generate action skeletons and \textttt{code-davinci-002} to score actions resulted in the best results, so we use this combination of language models for the experimental results.

% Add a small lemma about how our method doesn't harm you e.g. add BFS or something