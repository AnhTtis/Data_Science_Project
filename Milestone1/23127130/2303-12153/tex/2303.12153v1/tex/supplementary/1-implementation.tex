\section{Implementation Details}
\label{sec:implementation-details}

Text2Motion performs an integrated task- and policy-level search with three primary components: learned robot skills, large language models (LLMs), and policy sequence optimization (PSO). 
The baseline \hm{} also uses all of the above in an alternative planning strategy, while the baselines \scgs{} and \imgs{} use all but PSO. 
We make the details of each component explicit in the following subsections and provide the hyper-parameters of each planner.

\subsection{Learning robot skills and dynamics}
\label{sec:learning-models}

We learn four manipulation skill policies to solve tasks in simulation and in the real-world.
Each skill policy $\pi(a \vert s)$ is learned over a single-step action primitive $\rho(a)$ that is parameterized in the coordinate frame of a target object (e.g. \graytext{Pick(box)}). 
Thus, the policy is trained to output action parameters $a\sim\pi$ that maximizes the instantiated primitive's $\rho(a)$ probability of success in a contextual-bandit formulation with binary rewards $r(s, a, s') \in \{0, 1\}$.
Only a single policy per primitive is trained, and thereby the policy must learn to engage the primitive over objects with differing geometries (e.g. $\pi_{\text{Pick}}$ is used for both \graytext{Pick(box)} and \graytext{Pick(hook)}).
The observation space for each policy is defined as the concatenation of geometric state features (e.g. pose, size) of all objects in the scene, where the first $n$ object states correspond to the $n$ primitive arguments and the rest are randomized.
For example, the observation for the action \graytext{Pick(hook)} would have be a vector of all objects' geometric state with the first component of the observation corresponding to the \graytext{hook}.  

\textbf{Parameterized manipulation primitives:}
We describe the action parameters and reward function of each parameterized manipulation primitive below.
Collisions with non-argument objects constitutes an execution failure for all primitives, and hence the policy receives a reward of 0, for instance, if the robot collided with \graytext{box} during the execution of \graytext{Pick(hook)}.
\begin{itemize}
    \item \graytext{Pick(obj)}: $a\sim\pi_{\text{Pick}}(a \vert s)$ denotes the grasp pose of \graytext{obj} \textit{w.r.t} the coordinate frame of \graytext{obj}. A reward of 1 is received if the robot successfully grasps \graytext{obj}.
    \item \graytext{Place(obj, rec)}: $a\sim\pi_{\text{Place}}(a \vert s)$ denotes the placement pose of \graytext{obj} \textit{w.r.t} the coordinate frame of \graytext{rec}. A reward of 1 is received if \graytext{obj} is stably placed on \graytext{rec}.
    \item \graytext{Pull(obj, tool)}: $a\sim\pi_{\text{Pull}}(a \vert s)$ denotes the initial position, direction, and distance of a pull on \graytext{obj} with \graytext{tool} \textit{w.r.t} the coordinate frame of \graytext{obj}. A reward of 1 is received if \graytext{obj} moves toward the robot by a minimum of $d_{\text{Pull}}=0.05m$. 
    \item \graytext{Push(obj, tool, rec)}: $a\sim\pi_{\text{Push}}(a \vert s)$ denotes the initial position, direction, and distance of a push on \graytext{obj} with \graytext{tool} \textit{w.r.t} the coordinate frame of \graytext{obj}. A reward of 1 is received if \graytext{obj} moves away from the robot by a minimum of $d_{\text{Push}}=0.05m$ and if \graytext{obj} ends up underneath \graytext{rec}.
\end{itemize}

\textbf{Dataset generation:} 
All planning methods considered in this work rely on having accurate Q-functions $Q^\pi(s, a)$ to estimate the feasibility of skills proposed by the LLM. 
This places a higher fidelity requirement on the Q-functions than needed to learn a reliable policy, as the Q-functions must characterize both skill success (feasibility) and failure (infeasibility) at a given state.
Because the primitives $\rho(s)$ reduce the horizon of skills $\pi(a\vert s)$ to a single time-step, and the reward functions are $r(s, a, s')=\{0, 1\}$, the Q-functions can be interpreted as binary classifiers of state-action pairs.
Thus, we take a staged approach to learning the Q-functions $Q^\pi$, followed by the policies $\pi$, and lastly the dynamics models $T^\pi$.

Scenes in our simulated environment are instantiated from a symbolic specification of objects and spatial relationships, which together form a symbolic state $s$
% $\mathfrak{s}$.
The goal is to learn a \textit{complete} Q-function that sufficiently covers the state-action space of each primitive.
We generate a dataset that meets this requirement in four steps: a) enumerate all valid symbolic states $\mathfrak{s}$; b) sample geometric scene instances $s$ per symbolic state; c) uniformly sample actions over the action space $a\sim\mathcal{U}^{[0,1]^d}$; (d) simulate the states and actions to acquire next states $s'$ and compute rewards $r(s, a, s')$.
We slightly modify this sampling strategy to maintain a minimum success-failure ratio of 40\%, as uniform sampling for more challenging skills like \graytext{Pull} and \graytext{Push} seldom emits a success ($\sim$3\%).
We collect 1M $(s, a, s', r)$ tuples per skill in a process that takes approximately twelve hours.
Of the 1M samples, 800K of them are used for training ($\mathcal{D}_t$) while the remaining 200K are used for validation ($\mathcal{D}_v$).
We use the same datasets to learn the skill policies and dynamics models.
 
\textbf{Model training:}
We train an ensemble of Q-functions with mini-batch gradient descent and logistics regression loss.
Once the Q-functions have converged, we distill their returns into stochastic policies $\pi$ through the maximum-entropy update~\cite{pmlr-v80-haarnoja18b}:
\begin{equation*}
    \pi^* \leftarrow \arg \max_{\pi} \, \operatorname{E}_{(s,a)\sim \mathcal{D}_t}\left[\min(Q_{1:b}^\pi(s, a)) - \alpha \log\pi(a|s) \right],
\end{equation*}
noting that maintaining some degree of policy stochasticity is beneficial for policy sequence optimization (Sec.~\ref{subsec:taps-geometric-planning}).
Instead of evaluating the policies on $\mathcal{D}_v$ which, contains states for which no feasible action exists, the policies are synchronously evaluated in an environment that exhibits only feasible states. 
This simplifies model selection and standardizes skill capabilities across primitives. 
We train a deterministic dynamics model per primitive using the forward prediction loss:
\begin{equation*}
    L_{dynamics}\left(T^\pi; \mathcal{D}_t \right) = \operatorname{E}_{(s,a,s')\sim \mathcal{D}_t}||T^\pi(s, a) - s'||_2^2
\end{equation*}

All Q-functions achieve precision and recall rates of over 95\%. 
The average success rates of the converged skill policies over 100 evaluation episodes are: $\pi_{\text{Pick}}$ with 99\%, $\pi_{\text{Place}}$ with 90\%, $\pi_{\text{Pull}}$ with 86\%, $\pi_{\text{Push}}$ with 97\%.
The dynamics models converge to within millimeter accuracy on the validation split.


\textbf{Hyperparameters:} The Q-functions, skill policies, and dynamics models are MLPs with hidden dimensions of size [256, 256] and ReLU activations.
We train an ensemble of $b=8$ Q-functions with a batch size of 128 and a learning rate of 1e-4 with a cosine annealing decay~\cite{loshchilov2017sgdr}.
The Q-functions for pick, pull, and push converged on $\mathcal{D}_v$ in 3M iterations, while the Q-function for place required 5M iterations.
We hypothesize that this is because classifying successful placements demands carefully attending to the poses and shapes of all objects in the scene so as to avoid collisions.
The skill policies are trained for 250K iterations with a batch size of 128 and a learning rate of 1e-4, leaving all other parameters the same as \cite{pmlr-v80-haarnoja18b}.
The dynamics models are trained for 750K iterations with a batch size of 512 and a learning rate of 5e-4; only on successful transitions to avoid the noise associated with collisions and truncated episodes.
The parallelized training of all models takes approximately 12 hours on an Nvidia Quadro P5000 GPU and 2 CPUs per job.


\subsection{Out-of-distribution detection}
\label{subsec:ood-calibration}
The datasets described in Sec.~\ref{sec:learning-models} contain both successes and failures for symbolically valid skills like \graytext{Pick(box)}.
However, in interfacing robot skills with LLM task planning, it is often the case that the LLM will propose symbolically invalid actions, such as Pull(box, rack), that neither the skill policies, Q-functions, or dynamics models have observed in training.
We found that a percentage of such out-of-distribution (OOD) queries would result in erroneously high Q-values, causing the skill to be selected. 
Attempting to execute such a skill leads to control exceptions or other undesirable events.

Whilst there are many existing techniques for OOD detection of deep neural networks, we opt to detect OOD queries on the learned Q-functions via deep ensembles due to their ease of calibration~\cite{lakshminarayanan2017simple}.
A state-action pair is classified as OOD if the empirical variance of the predicted Q-values is above a determined threshold:
% \begin{equation*}
%     F_{\text{OOD}}(Q_{1:b}^\pi; s, a) = \mathbbm{1}\left(\operatorname{V}[Q^\pi_{1:b}(s,a)] \geq \epsilon^\pi_{\text{OOD}} \right),
% \end{equation*}
\begin{equation*}
    F_{\text{OOD}}(Q_{1:b}^\pi; s, a) = 1\left(V[Q^\pi_{1:b}(s,a)] \geq \epsilon^\pi_{\text{OOD}} \right),
\end{equation*}
where each threshold $\epsilon^\pi_{\text{OOD}}$ is unique to skill $\pi$.

To determine the threshold value, we generate an a calibration dataset of 100K symbolically invalid states and actions for each primitive.
The process takes less than an hour on a single CPU as the actions are infeasible and need not be simulated in the environment (i.e. rewards are known to be 0).
We compute the empirical variance the Q-ensembles across both the in-distribution and out-of-distribution datasets an bin the variances by predicted Q-value to produce a histogram.
We observe that the histogram of variances produced from OOD queries was uniform across all predicted Q-values and was an order of magnitude large than the ensemble variances computed over in-distribution data.
This simplified the selection of OOD thresholds, which we found to be: $\epsilon^{\text{Pick}}_{\text{OOD}} = 0.10$, $\epsilon^{\text{Place}}_{\text{OOD}} = 0.12$, $\epsilon^{\text{Pull}}_{\text{OOD}} = 0.10$, and $\epsilon^{\text{Push}}_{\text{OOD}} = 0.06$.


\subsection{Task planning with LLMs}
\label{subsec:llm-task-planning}
Text2Motion and the reactive planning baselines \scgs{} and \imgs{} uses \texttt{code-davinci-002} (Codex model \cite{chen2021evaluating}) generate and score skills, while \hm{} queries \texttt{text-davinci-003} (variant of InstructGPT \cite{ouyang2022training}) to directly output full skill sequences.
Surprisingly, a temperature of 0 was empirically found to be sufficient for our tasks. 

To maintain consistency in the evaluation of various planners, we allow Text2Motion, \scgs{}, and \imgs{} to generate $k=5$ skills $\pi^k_h$ at each timestep $h$.
Thus, every search iteration of Text2motion considers five possible extensions to the current running sequence of skills $\pi_{1:h-1}$.
Similarly, \hm{} generates $k=5$ skill sequences.

As described in the methods, skills are selected via a combined usefulness and geometric feasibility score:
\begin{align*}
    S_{\text{skill}}(\pi_h) &= S_{\text{llm}}(\pi_h) \cdot S_{\text{geo}}(\pi_h) \\
    &= p(\pi_h \mid i, s_1, \pi_{1:h-1}) \cdot p(r_h \mid i, s_1, \pi_{1:h}),
\end{align*} 
where Text2Motion uses policy sequence optimization to compute $S_{\text{geo}}$ while \scgs{} and \imgs{} use the current value function estimate $V^{\pi_h}(s_h) = \operatorname{E}_{a_h\sim\pi_h}[Q^{\pi_h}(s_h, a_h)]$.
We find that in both cases, taking $S_{\text{llm}}(\pi_h)$ to be the SoftMax log-probability score produces a \textit{winner-takes-all} effect, causing the planner to omit highly feasible skills simply because their associated log-probability was marginally lower than the LLM-likelihood of another skill.
Thus, we dampen the SoftMax operation with a $\beta$-coefficient to balance the ranking of skills based on both feasibility and usefulness. 
We found $\beta=0.3$ to work well in our setup.

\subsection{Geometric planning with PSO}
\label{subsec:taps-geometric-planning}
Text2Motion relies on a technique to compute the geometric feasibility score of a skill $S_{\text{geo}}(\pi_h) = p(r_h \mid i, s_1, \pi_{1:h})$ in the context of its predecessors.
As detailed in Sec.~\ref{subsec:llm-task-planning}, this probability can be computed without considering geometric action dependencies---particularly if actions have already been executed, as is the case with \scgs{} and \imgs{}.
However, Text2Motion computes $S_{\text{geo}}(\pi_h)$ after performing policy sequence optimization; the process by which long-horizon geometric dependencies are coordinated across skill sequences $\pi_{1:h}$.
For example, this would ensure that when evaluating the selection of \graytext{Pull(box, hook)} during integrated search, the algorithm does so in the context of a suitable \graytext{Pick(hook)} action (e.g. an upper-handle grasp on \graytext{hook}).


Text2Motion is agnostic the method that fulfils this role. 
However, in our experiments we leverage Sequencing Task-Agnostic Policies (STAP)~\cite{taps-2022}.
Specifically, we consider the PolicyCEM variant of STAP, where sampling-based optimization of the skill sequence's $\pi_{1:h}$ success probability is warm started with actions sampled from the policies $a_{1:h}\sim \pi_{1:h}$. 
We perform ten iterations of the Cross-Entropy Method (CEM)~\cite{rubinstein1999-cem}, sampling 10K trajectories at each iteration and selecting 10 elites to update the mean of the sampling distribution for the following iteration. 
The standard deviation of the sampling distribution is held constant at 0.3 for all iterations.