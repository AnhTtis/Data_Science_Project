\section{Related Work}\label{sec:related}
\input{table_related}

\subsection{Recent Systematic Reviews on Analysis Tools}
Two studies from early 2022 show that the automated analysis of Ethereum smart contracts has still room for improvement.
\cite{Rameder2022} describe the functionalities and methods of \np{140} tools (\np{83} open source) for automated vulnerability analysis of Ethereum smart contracts.
Their literature review identifies 54 vulnerabilities, with some not addressed by any of the tools.
Moreover, the authors find many tools to be unmaintained. 
\cite{Kushwaha2022tools} provide a systematic review of \np{86} analysis tools with a focus on \np{13} common vulnerabilities.
For quality assessment, they select \np{16} tools, which they test on five vulnerabilities using a ground truth of \np{30} contracts. 

\subsection{Tool Evaluations without Test Sets}
In 2019, two surveys evaluate tools for vulnerability detection by installing them and working through the documentation:
\cite{MdAGS2019tools} investigated \np{27} tools with respect to availability, maturity, methods employed, and security issues detected.
\cite{LopezVivar2020} evaluated \np{18} tools 
regarding the ease of installation, usefulness, and updates.
Both studies do not assess the detection capabilities of the examined tools.

\subsection{Benchmarked Evaluations}
Most closely related to our work are evaluations of tools that actually test them against a set of contracts (benchmark set).
When tool authors compare their own artifact to a few similar and/or popular ones, we consider those works to be intrinsically biased and therefore do not include them.

Among the independent evaluations, we find 11 related works \citep{Dika2017,Parizi2018empirical,Gupta2019,Durieux2020,ghaleb2020effective,Leid2020,Zhang2020a,Dias2021,Ji2021,Ren2021,Kushwaha2022tools}
of which we give an overview in~\autoref{tab:related}.
In the first two rows, we indicate the respective reference and the year when the evaluation was carried out.
Rows three to five list the size of the benchmark set, separated into vulnerable and non-vulnerable contracts, or unknown number of vulnerable contracts.
All references use Solidity files as benchmarks.
Row six indicates the number of different vulnerabilities tested.
We highlight low numbers in red and commendable high numbers in green.
We also list for each tool which evaluation it was part of. 
We highlight the five tools most often used in light blue.
In the last row, there is the total number of tools used in each study.
We highlight the five references using the most tools in mid-blue.

The earliest evaluation was \citep{Dika2017}, which covers four tools tested on five vulnerabilities with a benchmark set of \np{23} vulnerable and \np{21} non-vulnerable contracts.
Regarding the benchmark sets, the number of contracts contained shows a large variety from only \np{10} to almost \np{50000}.
The number of vulnerable contracts in the benchmark set also varies largely from \np{10} to \np{9369}
\footnote{%
It should be noted that for the \textit{wild} benchmark sets, i.e.\ from the contracts actually deployed on the mainchain, the true number of vulnerable contracts and the vulnerabilities they contain is yet unknown.}.
%
The number of different vulnerabilities varies from \np{4} to \np{57}. 
Several evaluations use their own taxonomy of vulnerabilities. This may be due to the lack of an established taxonomy~\citep{Rameder2022}.

We find a total of \np{20} tools mentioned in the evaluations, while each work selects a subset thereof for its tests.
The number of tools tested varies from three to a maximum of 16. 
The tools most often included in a comparison are Mythril, Oyente, Securify, Slither, and SmartCheck.

\input{table_related2}
In~\autoref{tab:related2}, we give an overview of the main contributions and the focus of the benchmarked evaluations.
The contributions include a systematic literature review (\citep{Kushwaha2022tools}), 
their own classification scheme for vulnerabilities or weaknesses (\citep{Dika2017,Gupta2020,Zhang2020a,Dias2021}),
a new or newly assessed benchmark set of contracts (\citep{Gupta2020,Durieux2020,ghaleb2020effective,Zhang2020a}),
a framework for tool execution or test case generation (\citep{Durieux2020,ghaleb2020effective,Ji2021}),
a quantitative tool evaluation (all benchmarked evaluations), or
new principles and methods for tool evaluations.
Regarding principles and methods, \citep{ghaleb2020effective} demonstrate the automated generation of vulnerable contracts by injecting buggy coding patterns, while ~\citep{Ren2021} evaluate settings for tool executions.

As for the focus, most studies address the effectiveness of tools in detecting weaknesses in smart contracts, two studies strive for a general review (\citep{Dika2017,Kushwaha2022tools}), while \citep{Ren2021} aim for insights into the influence of parameter settings onto tool results.

\subsection{Differences to the Benchmarked Studies}
Compared to the benchmarked studies mentioned above, our study stands out in the following aspects.

\ITEM{Focus.} Our study is the only one to focus on the \emph{temporal evolution} of weaknesses and tool behavior as well as on reducing the number of necessary test cases while maintaining \emph{full coverage} of the Ethereum mainchain.
Regarding tool effectiveness, we deliberately do not address it per se -- due to the lack of suitable benchmark sets as the available ones are either small, biased, outdated, or inconsistent \citep{MdAGS2023GT}.
Rather, we are striving for a relative comparison of tools with regard to two aspects: tool behavior over time and against tools that address sufficiently similar weaknesses (via mapping to a common frame of reference).
\ITEM{Input.} We use \emph{runtime bytecode} as input, while the other studies use Solidity source code.
\ITEM{Tools.} We include \emph{further tools} like Conkas, Ethainter, eThor, MadMax, Pakala, teEther, and Vandal. As they accept bytecode only as input, neither of them was used in any of the other studies.
\ITEM{Size.} With a benchmark set of \np{248328} unique contracts from the main chain, we use the \emph{largest number of contracts}.
\ITEM{Contribution.}
Our study features
a \emph{novel method for selecting a benchmark set} that allows for analyzing an entire ecosystem, 
the extension of an \emph{execution framework to work with bytecode as input}, and
the inclusion of \emph{time as a further dimension} to look at weaknesses and reasons for tool behavior.

\subsection{Open Source Frameworks}
For a large-scale evaluation, we need an analysis framework that (i) facilitates the control of multiple tools via a uniform interface, (ii) allows for bulk operation, and (iii) is open source and usable.
%
SmartBugs~\citep{ferreira2020smartbugs} is such an execution framework released in 2019.
It is still being maintained with 13 contributors and over \np{70} resolved issues.
%
The framework USCV~\citep{Ji2021} implemented similar ideas in mid-2020.
It comprises an overlapping set of tools and an extension of the ground truth set.
With a total of 10 commits (the latest in mid-2021) and no issues filed, it seems to be neither widely used nor maintained.
Both frameworks target Solidity source code, and thus need to be expanded to work with bytecode.

