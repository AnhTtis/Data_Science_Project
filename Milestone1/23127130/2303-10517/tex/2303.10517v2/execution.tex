\section{Execution Framework}
\label{sec:execution}
For the large-scale execution of our study, we had the choice between two frameworks: SmartBugs~\citep{ferreira2020smartbugs} and USCV~\citep{Ji2021}, both operating on Solidity level.
We decided on the former, as SmartBugs is better maintained (c.f.\ the last paragraph of \autoref{sec:related}) and contained more of the tools we were interested in.

First, we adapted SmartBugs to accept bytecode as input, and updated the Docker images of the tools accordingly.
Second, we integrated six further tools (kept in boldface in \autoref{tab:tools}).
The most laborious part was the \emph{output parsers}.
For each tool, a dedicated parser scans the output of the tool to identify the result of the analysis, to detect anomalies, and to discard irrelevant messages.
For each run of a tool on a bytecode, the parser reports a list of \emph{findings} (tags identifying the detected properties), a list of \emph{errors} (conditions checked for and reported by the tool), a list of \emph{fails} (low-level exceptions not adequately handled by the tool), and a list of \emph{messages} (any other noteworthy information issued by the tool).
For a detailed description of the enhanced framework see \citep{diangelo2023smartbugs}.

\ITEM{Choice of Parameters.}
\cite{Ren2021} show that the choice of parameters strongly affects the results, especially when the timeout is below 30 minutes per contract.
We set the maximal runtime to \np[s]{1800} wall time, with 1.5 CPUs assigned to each run.
If a tool offers a timeout parameter, we communicate the runtime minus a grace period to allow the tool to terminate properly.
Conkas, eThor, Maian, Securify, teEther and Vandal offer no such parameter and are stopped by the external timer.

As there is a tradeoff between the memory limit per process and the number of processes run in parallel, we aimed at providing sufficient but not excessive memory.
Based on an initial test with \np{500} randomly selected contracts, we set the memory limit to \np[GB]{20} for eThor, Pakala, Securify and teEther, and to \np[GB]{4} for all other tools.
We reran tasks with a limit of \np[GB]{32} if they had failed with a segmentation fault or a memory problem.

\ITEM{Machine.}
We used a server with an AMD EPYC7742 64-Core CPU and \np[GB]{512} of RAM.
\autoref{tab:resources} gives an overview of the computation time, memory usage, and memory fails before and after the rerun with \np[GB]{32}.

\input{table\_resources} 
