\section{RQ3 Tool Quality over Time} \label{sec:rq3}
To assess the quality of the tools, we consider execution times as well as their errors and failures.

\input{table_runtimes}

\ITEM{Execution time}.
\autoref{tab:runtimes} gives the average runtimes in seconds for each tool.
The column \emph{Overall} averages over all \np{248328} runs, whereas \emph{Success} picks only those completing without errors and failures.
The column \emph{Error} shows the average time for runs where the tool reports an error, while \emph{OOM} collects the runs terminated by an out-of-memory exception.
The last column, \emph{Prg.issues}, averages over runs with programming issues, like exceptions caused by type errors.
The average time for runs timing out is not listed explicitly, as it is close to \np[s]{1800} (\np[m]{30}), for obvious reasons.

Overall, the fastest tools are MadMax, Oyente, Maian, Vandal, Ethainter.
The slowest one, by far, is Pakala, with the next ones, Mythril, eThor, and teEther, being twice as fast. 
When considering only runs without errors and failures, eThor and teEther are substantially faster than on average, while Pakala and Mythril are still slow.
Mythril, Oyente, and Vandal do not report any errors, hence no times are listed in the respective column.
The average times on error are small for Pakala, Securify, Maian, eThor and Conkas, which indicates that most reported errors are show-stoppers.
For Madmax and Ethainter, the few errors are related to a timeout, hence the average is high.

\begin{figure}%[!ht]
\centering
\includegraphics[width=.9\columnwidth]{fails}
\caption{Tool failures over time. Each data point shows the percentage of failures encountered by the tools, in bins of 100k blocks. Ethainter and MadMax had no failures.}
\label{fig:failures}
\end{figure}

\ITEM{Errors and Failures.}
We consider a run failed if it is terminated by an external timeout, an out-of-memory exception, or a tool-specific unhandled condition.
A run terminates properly if it stops under control of the tool, either successfully or with an error condition detected by the tool.
\autoref{fig:failures} depicts the failures over time in bins of 100\,k blocks as percentage of bytecodes where a tool fails. 
Conkas, eThor, Pakala, and teEther fail most often and are shown at the top, while the other tools show few or no failures.

\input{table_finds_failures}

\autoref{tab:finds} gives an overview of the accumulated errors and failures by category.
The left part lists the number of bytecodes with and without finding, as well as the share accompanied by an error or failure.
In its right part, the table gives the number of bytecodes, where the analysis resulted in an error message and/or a failure due to a timeout, an out-of-memory condition or a program issue.


\begin{figure}%[!ht]
\centering
\includegraphics[width=.9\columnwidth]{errors}
\caption{Tool errors over time. Each data point shows the percentage of errors reported by the tools, in bins of 100k blocks. 
Mythril, Oyente and Vandal had no errors.}
\label{fig:errors}
\end{figure}

While most reported findings are not accompanied by any errors or failures, there are three notable exceptions.
Maian detects numerous occurrences of \emph{Ether lock} in spite of encountering unknown instructions.
The same accounts for Osiris when it reports the \emph{Callstack bug}.
This is due to the fact that the tools apply local pattern matching instead of symbolic execution.
Pakala reports a timeout for almost half of its analyses with findings.

eThor, Pakala and teEther show a large number of timeouts (marked red in \autoref{tab:finds}), which results in high average runtimes (marked red in \autoref{tab:runtimes}).
While Mythril shows a similarly high average runtime, it only has a low number of timeouts.
In contrast to the other three tools, it offers a parameter for getting notified about the external timeout and so is able to finish in time.

Regarding out-of-memory exceptions, only teEther sticks out.
Even with \np[GB]{32} of memory, it still fails for \np[\%]{16} of the inputs.

The last column in \autoref{tab:finds}, program issues, indicates to some extent the maturity of the tools' code.
Conkas fails for \np{63111} runs, with the most common causes being \emph{maximum recursion depth exceeded} \percent[b]{55626}{63111}; assertion failures \percent[b]{2499}{63111}; and type errors \percent[b]{2038}{63111}.
The \np{17407} fails of eThor result from the instruction EXTCODEHASH not being processed properly \percent[b]{11366}{17407}, arithmetic exceptions \percent[b]{5930}{17407}, and runtime exceptions \percent[b]{111}{17407}.
Securify fails for \np{9586} runs, mainly because of \emph{null pointer} exceptions \percent[b]{9496}{9586}.
Mythril, as the only tool actively maintained according to the activity on Github, fails for only \np{1022} bytecodes, the most frequent cause being type errors \percent[b]{952}{1022}, predominantly due to undefined terms in integer expressions.
At the lower end, we find Ethainter and MadMax with no program issues at all, and Maian, Osiris and Oyente with just a few.

\begin{mdframed}[style=mpdframe]
  \textbf{Observation 3.}
  Regarding resource consumption, a few tools require less than \np[s]{60} per contract with just a few GB of memory, whereas others  regularly approach the limits of \np[min]{30} and \np[GB]{32}.
  The rate of tool-reported errors varies between \np[\%]{0} and \np[\%]{60}, with the high rates resulting from tools operating outside of their specification.
  Questionably, there are tools with similar limitations but without any error at all.
  Regarding robustness, eight tools throw an exception for less than \np[\%]{1} of the contracts, as opposed to one tool with \np[\%]{25} fails.
  Program issues like type exceptions may be a consequence of using the dynamically typed language Python.
\end{mdframed}
