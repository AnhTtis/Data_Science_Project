\section{Discussion} \label{sec:discussion}

In this section, we combine the results of our research questions and discuss them in a wider context.

\subsection{Relation between Findings, Errors, Failures, and Overlap} \label{sec:relation}

In the last section, we looked at the overlap of tools, accumulated over time as well as over common SWC classes.
Here, we pick two exemplary SWC classes, take a closer look at the evolution of findings over time, and correlate the overlap of tools with their errors and failures.

\begin{figure}
\centering
\includegraphics[width=.78\columnwidth]{101_br}\\
\hspace*{15.5mm}%
\includegraphics[width=.79\columnwidth]{agreement101}\\
\includegraphics[width=.78\columnwidth]{101_errors}\\
\includegraphics[width=.78\columnwidth]{101_fails}
\caption{SWC-101 Integer Overflow and Underflow on a timeline of blocks, in bins of \np[k]{100} blocks.
  Top: Percentage of bytecodes flagged, per tool.
  Upper middle: Percentage of overlaps. 
  Lower middle: Error rate of tools.
  Bottom: Failure rate of tools.}
\label{fig:swc101}
\end{figure}

\ITEM{SWC\,101~-- Integer Over- and Underflow.}
In \autoref{fig:swc101}, the top most plot depicts the percentage of bytecodes flagged with an integer over- or underflow, per tool.
Starting from different levels around \np[\%]{70} and \np[\%]{40}, Conkas and Mythril converge at \np[\%]{10} at the end of the timeline.
Osiris shows a weakness level comparable to these tools for most of the timeline, but falls to \np[\%]{0} towards the end. 
MadMax reports hardly any cases throughout the whole timeline.

The second plot in \autoref{fig:swc101} visualizes the agreement of tools over time.
The lines in the foreground show the number of contracts flagged by any tool, once in relative terms (blue line with the scale to the left, with 100\,\% corresponding to all contracts), and once in absolute terms (black line with the scale to the right, numbers per bin of 100\,k blocks).
The background divides the flagged contracts into shares that are flagged by a single tool, by two, three, or four tools, respectively.

Up to block 6\,M (bin 60), the brown and green areas with purple specks at the top show that 60\,\% of the flagged contracts are flagged by at least two tools.
The other 40\,\% are split between Osiris and Conkas, who are the sole tools flagging the respective contracts.
The gray area of contracts flagged solely by Mythril is small, even though the tool finds the weakness in 20--40\,\% of all contracts (top plot).
Apparently, at least one other tool agrees with Mythril most of the time.

The picture changes in the second half of the plot.
Towards the end of the timeline, there is hardly any agreement anymore.
Less than 10\,\% of the contracts are flagged by at least two tools, while most are flagged solely by Conkas or Mythril.

The situation can be partly explained by the fact that MadMax specializes in gas issues, with one of its findings constituting a specific type of overflow that occurs in a few contracts only (see top plot).
For Osiris, we see a rise in errors (third plot of \autoref{fig:swc101}) that mirrors the increased usage of the \OP{SHR} operation (\autoref{fig:ops_evol}), which is not supported by Osiris (\autoref{tab:operations}).
Therefore, the detection rate of Osiris drops to zero (top plot), leaving us essentially with two tools at the end of the timeline.
In spite of Conkas' failure rate rising to 70\,\% (fourth plot of \autoref{fig:swc101}), its detection rate remains comparable to Mythril.

From version 0.8.0 onwards, the Solidity compiler inserts checks for over- and underflows into the bytecode.
In view of the compiler's adoption rate (\autoref{fig:ops_evol}), it seems that the vulnerability has actually become extinct at the end of the timeline and that the respective findings of Mythril and Conkas are false positives.

\begin{figure}
\centering
\includegraphics[width=.78\columnwidth]{107_br}\\
\hspace*{15.5mm}%
\includegraphics[width=.79\columnwidth]{agreement107}\\
\includegraphics[width=.78\columnwidth]{107_errors}\\
\includegraphics[width=.78\columnwidth]{107_fails}
\caption{SWC-107 Reentrancy on a timeline of blocks, in bins of \np[k]{100} blocks. 
  Top: Percentage of bytecodes flagged, per tool.
  Upper middle: Percentage of overlaps. 
  Lower middle: Error rate of tools.
  Bottom: Failure rate of tools.}
\label{fig:swc107}
\end{figure}

\ITEM{SWC\,107~-- Reentrancy.}
At a first glance, \autoref{fig:swc107} shows a situation similar to \autoref{fig:swc101}, just for another weakness and the six tools detecting it.
The detection rate of three tools (Osiris, Oyente, Securify) is low and drops to zero towards the end.
For Osiris and Oyente, the reason is again their inability to handle new operations, in particular \OP{SHR}, even though Oyente quits silently, while Osiris issues errors (third plot).
For Securify, the collected data does not provide an explanation for the diminishing detection rate.

Conkas and eThor exhibit significant failure rates (bottom plot in \autoref{fig:swc107}), but this does not prevent them from reporting up to 40\,\% of contracts as potentially vulnerable to a reentrancy attack.
On the surface, these two tools show a similar behavior, reporting similar rates of reentrant contracts from block \np[M]{3.5} (top plot, bin 35) onwards.
However, the Jaquard similarity for the flagged contracts (number of contracts flagged by both tools divided by the number of contracts flagged by at least one tool) is only \np[\%]{45} at block \np[M]{3.5}, and drops to \np[\%]{28} for the last part where the blue and orange lines seem to coincide.

This is also reflected in the second plot of \autoref{fig:swc107}, where the agreement of two or more tools (red, green and purple area) decreases steadily from block \np[M]{4.5} (bin 45) onwards, while the shares of contracts flagged exclusively by Conkas (pink), eThor (blue) or Mythril (gray) increase, such that at the end of the timeline, the four groups are roughly of the same size.

Our explanation for the disagreement between the tools for this weakness as well as for most others, is the lack of commonly agreed, unambiguous definitions, which is backed by our work on a unified ground truth~\cite{MdAGS2023GT}.
On the surface, the tools aim for the same weakness, motivated by similar examples, but the respective interpretations and implementations may differ considerably.


\subsection{Assessment of Tools}

Based on the results of our evaluation, we summarize the observed properties of the tools.

\ITEM{Conkas.}
With an average runtime of 119\,s and 4\,GB of memory, Conkas belongs to the light-weight tools.
The number of contracts timing out or running out of memory is small.
It seems that Conkas underwent a not entirely successful update for the operations of recent forks, as the source code seems to support them, but the tool fails on contracts using them, resulting in a high number of program exceptions.
This may be caused by a divergence between Rattle, the module generating the intermediate representation, and the analysis module on top.
Despite these problems, Conkas reports many findings.

\ITEM{Ethainter and MadMax.}
These two tools are among the most efficient and robust tools.
With average runtimes of 71\,s and 21\,s, respectively, and 4\,GB of memory, they are fast and never exceed the allotted memory.
The few errors reported are timeouts under control of the tools.
The number of failures is zero, indicating a high engineering quality.
This may be due to the robust base component Souffl√© and the use of Datalog as a high-level specification language.

\ITEM{eThor.}
With an average runtime of 574\,s, a large number of timeouts, and 369 bytecodes running out of memory even with 32\,GB, this tool is one of the elephants in our study.
The high resource consumption may be caused by the complex workflow~-- eThor is the only tool trying to show the absence of a weakness.
The use of the strongly-typed programming language Java explains the absence of type errors (as we see with Python programs).
eThor seems to support the most frequent operations introduced by forks, but throws errors for some of them.
This makes the decreasing rate of reported reentrancy issues (and the increasing rate of contracts found secure) an unreliable indicator for the assumption that the frequency of reentrancy weaknesses indeed drops.

\ITEM{Maian.}
With an average runtime of 48\,s and 4\,GB memory sufficing for almost all bytecodes, Maian is a lightweight.
As it is the oldest tool and unmaintained, it supports hardly any of the newer operations.
This results in the second highest number of errors (reporting unknown opcodes) and virtually no weakness detections for newer contracts.

\ITEM{Mythril.}
With an average runtime of 670\,s, Mythril belongs to the slow tools, but almost never needs more than 4\,GB of memory.
It is the only actively maintained tool in our collection: Every issue we reported was fixed within a few days.
Mythril supports all EVM operations and checks for a large number of weaknesses.
This, and the tendency to report also issues of low severity, result in the third largest number of flagged contracts. 

\ITEM{Oyente and Osiris.}
With an average runtime of 35\,s and 165\,s, respectively, the two tools are among the faster tools.
Osiris extends Oyente and checks for further properties, which explains the additional time it takes.
4\,GB of memory suffice for most contracts.
However, Oyente runs out of 32\,GB of memory for about 2\,000 bytecodes, whereas Osiris seems to require less memory and hardly ever exceeds the quota.
Both tools fail for operations beyond fork 4.37\,M, with Osiris issuing a message and Oyente failing silently.
Consequently, both tools report no weaknesses for recent contracts.

\ITEM{Pakala.}
With an average runtime of 1115\,s, this tool is by far the slowest, which seems to be a consequence of Pakala performing symbolic execution without optimizations.
The tool author aimed at a small and simple program and deliberately omitted techniques like the construction of control flow graphs.%
\footnote{\url{https://www.palkeo.com/en/projets/ethereum/pakala.html}}
In spite of being able to handle all relevant operations, Pakala flags only 4\,232 contracts as vulnerable, which may have different causes.
First, the analysis of 80\,000 contracts timed out, so a prolonged analysis might have revealed further weaknesses.
Second, Pakala might actually spend the extra time for a more refined analysis, leading to a lower number of false positives.
Third, the simplicity of the program might have resulted in a lower detection rate.
To determine the actual cause, we would need to check the quality of the results, which is beyond the scope of our study.

\ITEM{Securify.}
The average runtime of 160s makes Securify one of the faster tools, even though it times out for 1651 bytecodes.
None of the runs exceeds the memory quota.
The implementation language Java prevents type errors, but we see almost 10\,000 null pointer exceptions.
Even though Securify has been superseded by a successor (that supports source code only and thus does not fall into the scope of our study) and is unmaintained now, it supports most essential EVM operations.
Nevertheless, its detection rate starts to drop early on and falls to virtually zero towards the end.

\ITEM{teEther.}
The average runtime of 572\,s is comparable to Mythril, but teEther times out in 52\,250 cases (compared to 2\,620 for Mythril).
The tool is exceptional regarding its appetite for memory: even with 32\,GB provided, 40\,306 analyses exceed the memory quota.
teEther addresses a single vulnerability that it detects in 3\,230 bytecodes.
The tool supports the essential EVM operations, but is unmaintained now.
With 6\,608 Python exceptions, teEther seems to be an experimental tool focusing on the elaborate analysis of a single issue.

\ITEM{Vandal.}
With an average runtime of 63\,s and 4\,GB of memory, Vandal is one of the fast and light tools, but still runs into a timeout for 2\,662 bytecodes and exceeds 32\,GB of memory for another 1\,142.
The number of 1\,047 programming issues is moderate for a tool written in Python.
With 75\,\% of the contracts flagged, Vandal surpasses the detection rate of the other tools.
The high rate triggered some plausibility checks in \autoref{sec:tool_reports}, showing that most bytecodes containing a \OP{CALL} operation are flagged as containing an unchecked or reentrant call.
Vandal seems to implement rather unspecific criteria that lead to a large number of false positives.
This interpretation is supported by Vandal's repository, where the patterns for weakness detection are listed just as use cases for a framework that decompiles bytecode to single static assignments.
The accompanying paper, on the other hand, presents Vandal as a tool for vulnerability detection.

\subsection{Combining or Comparing Tools Results}
When comparing or combining tool results, we face two challenges: (i) different aims of tools that are reflected in the way their findings are reported and (ii) differing definitions of weaknesses (that are associated with the findings), which makes it hard to map a finding to a class (within a common frame of reference) for comparison or combination. 

The tools can be divided into four groups with respect to their aim (for a specific weakness): (i) proving the absence of a property that is regarded as a weakness or vulnerability, (ii) over-reporting as to not overlook a potential weakness (aka issuing warnings), (iii) under-reporting since only those weaknesses are reported where a verification could be found (avoiding false alarms), (iv) reporting properties that are hardly a weakness (e.g.\ honeypots) or not necessarily (e.g.\ gas issues).

This distinction is important when comparing tools.
It strongly affects the number of agreements.
As we have seen in \autoref{sec:rq4} and \autoref{sec:relation}, the overall agreement is low, which is partly due to the fact that tools address different versions, subsets or supersets of a weakness class.
Considering the different aims of the tools, the low general agreement is not surprising.
However, it is even low for tools with similar aims.

The aims of the tools also impact voting schemes that combine the results of several tools to `determine' whether a contract is actually vulnerable. 
For over-reporting tools, it may make sense to have a majority vote.
However, under-reporting tools should rather be joined than intersected.


\subsection{Comparison to Source Code as Input}
We selected the tools in our study for their ability to process runtime code, as our goal was to analyze contracts deployed on the mainchain, for which Solidity source code is often unavailable.
Moreover, this allowed us to include Ethainter, eThor, MadMax, Pakala, teEther and Vandal, which require runtime code.
The other selected tools accept both, bytecode and Solidity source code.
In this section, we discuss the effect of using source code as the input.

Conkas, Osiris, Oyente and Securify compile the Solidity source to runtime code and then perform the same analysis as if the latter had been the input.
There are two differences, though.
First, the tools are able to report the location of weaknesses within the source, as they use a mapping provided by the compiler to translate bytecode addresses back to line numbers.
Second, for Solidity sources with more than one contract, the tools compile and analyze each one separately.
As complex contracts are structured into several layers of intermediate contracts using inheritance, this leads to redundant work.
While compilation and address mapping incur a negligible overhead, the additional contracts may lead to fewer or more findings within a fixed time budget, depending on whether there is less time for the main contract or whether other contracts contribute additional findings.\footnote{%
An easy remedy would be to extend the tools by a parameter with the name of the contract to analyze.}

Maian and Mythril compile the Solidity source as well but proceed with the deployment code, which includes contract initialization as well. 
Maian deploys the contract on a local chain and checks some properties live, like whether the contract accepts Ether. 
Moreover, the findings are filtered for false positives by trying to exploit the contract on the chain. 
Mythril, on the other hand, uses the deployment code to analyze also the constructor. 
For both tools, resource requirements and results will vary with the chosen form of input.

\subsection{Threats to Validity}
\label{sec:threats}

\ITEM{Internal validity} is threatened by integrating the new tools into SmartBugs.
We mitigated this threat by carefully following the SmartBugs instructions for tool integration and by consulting the documentation and the source code of the respective tools.
Multiple authors manually analyzed all execution errors to ensure that we had configured the tools adequately.
Moreover, we make the implementation and the results accessible for public inspection.

\ITEM{External validity} is threatened by the use of single bytecodes as proxies for code families identified by the same skeleton.
These representatives may not accurately reflect the code properties of all family members that are relevant to weakness detection.
We mitigated this threat by the first research question.
However, the random sample of \np{1000} bytecodes (\np{620} code families) may have been chosen too small such that our answer to RQ1 may not generalize to all bytecodes.

The focus on runtime bytecode as the sole object of analysis restricts the number of tools usable for our study, as well as the methods applicable.
Some trends and observations may thus not generalize to smart contract analysis in general.

\ITEM{Construct validity} is threatened by our mapping of the detected weaknesses to the classes of the SWC registry.
The mapping reflects our understanding of the weaknesses and what the tools actually detect, which may be incorrect.
We mitigated this risk by involving all authors during the mapping phase and by discussing disagreements until we reached a consensus.

Another potential threat are the resources, 30 minutes and up to 32\,GB per tool and bytecode.
This configuration is in line with related work or surpasses it.
