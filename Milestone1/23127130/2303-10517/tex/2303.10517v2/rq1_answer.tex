\section{RQ1 Abstraction}\label{sec:rq1}

To validate our hypothesis that bytecodes with the same skeleton (i.e., members of the same code family) behave similarly regarding bytecode analysis, we randomly select \np{1000} bytecodes from all runtime bytecodes \emph{not} in our data set.
By construction, these codes belong to families with at least two members.
The selected bytecodes happen to belong to \np{620} families.
We add the corresponding \np{620} representatives from our data set, obtaining a dataset with \np{1620} bytecodes and \np{620} families with \np{2} to \np{64} members per family.

When running analysis tools on different members of the same family, we expect nearly identical results with small variations due to differences in runtimes (e.g.\ one run timing out while the other one finishes just in time with some finding) or due to the effect of different constants when solving constraints.
In particular, we do not expect the meta-data injected by the Solidity compiler to affect the result, as it is interpreted neither as code nor as data during execution.
To confirm this, we also consider a copy of our \np{1620} bytecodes, where we replace all meta-data sections with zeros.

\input{table_variability}

\autoref{tab:variability} shows the result of running all tools on the bytecodes with and without meta-data.
Columns two and four give the percentage of the 620 families for which the findings differ within the family, whereas columns three and five consider all data collected by the output parsers, including errors, fails, and messages.
If we assume that the various effects influencing the output give rise to a normal distribution, then for a confidence level of \np[\%]{95}, the sample size of 620 yields a margin of error of \np[\%]{1.5} for the smaller values in the table and of \np[\%]{3.2} for the larger ones.

The seven tools on top behave essentially as predicted.
For Conkas, the rate of \np[\%]{1.5} corresponds to \np{9} families with divergent findings.
These differences are related to warnings about integer under- and overflows, and may indeed be the result of different constants in the codes of a family.
Observe that for these seven tools, there is hardly any difference between the two datasets, with and without meta-data.

Osiris and Oyente seem remarkable, as we find \np[\%]{20} discrepancies in the output.
Oyente starts its analysis by disassembling the entire bytecode.
It issues the warning `incomplete push instruction' when stumbling upon a supposed \OP{PUSH} instruction near the end of the meta-data that is followed by too few operand bytes.
These spurious messages disappear when removing the meta-data, but otherwise do not affect the analysis.
Osiris reuses Oyente's code and inherits this anomaly.

eThor also scans the entire bytecode.
When encountering an unknown instruction, it issues a warning and ignores the remaining code.
Like with Oyente, these messages mostly disappear when removing the meta-data.
However, unlike Oyente, the meta-data influences the result of the analysis, as can be observed by \np[\%]{2.9} vs.\ \np[\%]{1.0} differences in the findings for code with vs.\ no meta-data.
In each of these cases, the analysis times out for some member(s) of the family but terminates with identical results for the others.
We did not research the cause for these discrepancies but suspect that it may be comparable to the situation of Vandal.

Vandal constructs a control flow graph for the entire bytecode and decompiles it to an intermediate representation.
Sometimes, the tool gets lost during this initial phase and times out.
The situation improves when removing irrelevant parts like the meta-data.
However, as Vandal interprets the addresses of all code sections relative to the beginning of the bytecode, even if they belong to a different contract (see the discussion on the structure of bytecode in \autoref{sec:creation}), we still see differences regarding errors and fails.

Maian starts by scanning the entire bytecode for certain instructions, like \OP{SELFDESTRUCT}.
Not detecting the opcode anywhere lets Maian immediately conclude certain properties, whereas finding the opcode triggers a reachability analysis that may remain inconclusive.
This sensitivity to single bytes yields divergent results for \np{70} families.
For example, Maian may detect non-destructibility for one code and fail to do so for another one in the same family.
Removing the meta-data gets rid of these divergences almost entirely.

\begin{mdframed}[style=mpdframe]
\textbf{Observation 1.}
Treating bytecodes with the same skeleton as equivalent works for 9 out of 12 tools without reservations.
Three tools unexpectedly analyze the meta-data, leading to minor output variations. 
Therefore, skeletons can be regarded as a suitable abstraction for large-scale analyses aimed at the big picture.
Removing the meta-data prior to analysis may improve the performance of some tools (while not harming others).
\end{mdframed}

