\section{Study Design}
\label{sec:design}
In this section, we define the research questions, describe the data set and the selection of tools, the execution framework, the weaknesses and the taxonomy considered, and finally the mapping of the tool findings to the taxonomy.

\subsection{Research Questions}

\begin{description}
  \item[RQ1: Abstraction.] \emph{How well are skeletons suited as an abstraction of functionally similar bytecode in the context of weakness analysis?}
 We investigate whether and how contracts with the same skeleton differ for a weakness analysis with bytecode input.

 \item[RQ2: Weakness Detection.] \emph{Which weaknesses do the tools report for the contracts on Ethereum's main chain?}
 We are interested in the evolution of types and numbers of weaknesses reported for the deployments up to early 2022.

 \item[RQ3: Tool Quality.] \emph{How do analysis tools behave in a weakness analysis with bytecode input?}
 We investigate the tool quality with respect to maintenance aspects, execution time, errors, and failures. 

\item[RQ4: Overlap Analysis.] \emph{To what extent do the tools agree on the findings?}
 We determine the amount of tool agreement per weakness on a timeline.
\end{description}


\subsection{Data}\label{ssec:data}
As we strive for a complete coverage of Ethereum's main chain, we collect the runtime codes of all contracts (including the self-destructed ones) that were successfully deployed up to block 14\,M.\footnote{%
  We used an OpenEthereum client, \url{https://github.com/openethereum/openethereum}.%
}
For each family of codes, i.e., for each collection of codes sharing the same skeleton (see \autoref{sec:background}), we pick a single representative and omit the others.
For practical purposes, we prefer deployments where Etherscan lists the corresponding source code.
We obtain a dataset of \np{248328} runtime codes with distinct skeletons that represent all deployments until January 13, 2022.%
\footnote{%
  The data is provided at \url{https://figshare.com/s/5efef6335fa98ddc3ae2}.%
}
99.0\,\% of these codes originate from the Solidity compiler (as determined by characteristic byte sequences), with the source code for 46.5\,\% actually available on Etherscan.
For our temporal analyses, we associate each code with the block number where the first member of the family was deployed.
The longest-lived family consists of two codes implementing an ERC20 token.
The codes were deployed \np{17333} times over a range of almost 12 million blocks, whereas the most prolific family consists of 20 codes deployed over 12 million times.\footnote{%
  The codes of this family, 21 bytes in length, belong to gas token systems.
  When called from the address mentioned in the code, the contracts self-destruct, leading in earlier versions of Ethereum to a gas refund.}

Not all bytecodes are proper contracts.
In particular in the early days of the main chain, during an attack, a number of large contracts were deployed that served as data repositories for other contracts. 
For some tools, this leads to a noticeable spike in the error rate around block 2.3\,M.


\subsection{Tools}

Our study aims to integrate tools into a common framework and to perform a bulk analysis, with large numbers of runtime bytecodes automatically checked for functionalities.
This imposes the following constraints on the tools.
\begin{enumerate*}
\item Availability: The tool needs to be publicly available with its source open.
\item Interface: The tool can be controlled via a command-line interface.
\item Input: The tool is able to analyze contracts based on their runtime bytecode alone.
\item Findings: The tool offers an automated mode to report weaknesses.
\item Documentation: There is sufficient documentation to operate the tool.
\end{enumerate*}

These criteria exclude tools that need access to the abstract binary interface\footnote{\url{https://docs.soliditylang.org/en/latest/abi-spec.html}} or to source code.
Likewise, tools that expect an additional setup like an external blockchain with information on the environment do not fit our setting.
Starting from the \np{140} tools identified by \cite{Rameder2022}, the criteria above leave us with the 13 tools in \autoref{tab:tools}.

\input{table_tools}


\subsection{Execution Framework}
For the large-scale execution of our study, we had the choice between two frameworks (cf.\ \autoref{sec:related}): SmartBugs~\citep{ferreira2020smartbugs} and USCV~\citep{Ji2021}, both operating on Solidity level.
We decided on the former, as SmartBugs is better maintained and already contained more of the tools we were interested in.

First, we adapted SmartBugs to accept bytecode as input, and updated the Docker images of the tools accordingly.
Second, we integrated six further tools (kept in boldface in \autoref{tab:tools}).
The most laborious part was the \emph{output parsers}.
For each tool, a dedicated parser scans the output of the tool to identify the result of the analysis, to detect anomalies, and to discard irrelevant messages.
For each run of a tool on a bytecode, the parser reports a list of \emph{findings} (tags identifying the detected properties), a list of \emph{errors} (conditions checked for and reported by the tool), a list of \emph{fails} (low-level exceptions not adequately handled by the tool), and a list of \emph{messages} (any other noteworthy information issued by the tool).

\ITEM{Choice of Parameters.}
\cite{Ren2021} show that the choice of parameters strongly affects the results, especially when the timeout is below 30 minutes per contract.
We set the maximal runtime to \np[s]{1800} wall time, with 1.5 CPUs assigned to each run.
If a tool offers a timeout parameter, we communicate the runtime minus a grace period to allow the tool to terminate properly.
Conkas, eThor, Maian, Securify, teEther and Vandal offer no such parameter and are stopped by the external timer.

As there is a tradeoff between the memory limit per process and the number of processes run in parallel, we aimed at providing sufficient but not excessive memory.
Based on an initial test with \np{500} contracts, we set the memory limit to \np[GB]{20} for eThor, Pakala, Securify and teEther, and to \np[GB]{4} for all other tools.
We reran tasks with a limit of \np[GB]{32} if they had failed with a segmentation fault or a memory problem.

\ITEM{Machine.}
We used a server with an AMD EPYC7742 64-Core CPU and \np[GB]{512} of RAM.
\autoref{tab:resources} gives an overview of the computation time, memory usage, and memory fails before and after the rerun with \np[GB]{32}.

\input{table_resources} 

\subsection{Mapping of Tool Findings}\label{ssec:mapping}

\ITEM{Taxonomy.}
To compare the tools regarding their ability to detect weaknesses, we need a taxonomy with an adequate granularity.
Since there is no established taxonomy of weaknesses for smart contracts, previous studies~\citep{Chen2020survey, Tang2021, Wang2021survey, Kushwaha2022vuls, Rameder2022, tolmach2022survey, Zhou2022state} not only summarize potential issues, but also structure them with respect to their own taxonomies, none of which is compelling or widely used.

Among the community projects, there are two popular taxonomies: DASP TOP 10~\footnote{https://dasp.co} from 2018 with 10 categories, and the SWC registry~\footnote{https://swcregistry.io} with 37 classes, last updated 2020.
As for DASP, two categories, \textit{Access Control~(2)} and \textit{Other~(10)}, are quite broad, while \textit{Short Address~(9)} is checked by hardly any tool.
Moreover, \textit{DOS~(5)} and \textit{Bad Randomness~(6)} are effects that may be the result of various causes, and most tools detect causes rather than consequences.

The SWC registry is more granular as it offers several classes for the broad categories \textit{Access Control} and \textit{DOS}.
Moreover, most of its categories match relevant findings of the tools.
Therefore, we select this taxonomy as the basis of our comparison.


\input{table_vulnerabilities}

\ITEM{Findings mapped.}
The tools report \np{82} different findings, of which we can map \np{56} to one of the 37 classes of the SWC taxonomy (see  \autoref{tab:mapping}).
In total, the tools cover the \np{15} weakness classes listed in \autoref{tab:vuls}, with the number of findings and the number of tools.
\input{table_mapping}

When a tool reports a finding, we assume that it is not invalidated by an accompanying error condition, a low coverage of the bytecode, or a timeout.
However, we note errors, timeouts, and unhandled conditions (fails).

\ITEM{Findings omitted.}
We omit the nine findings of HoneyBadger, as it does not detect weaknesses, but patterns characteristic of honeypots, i.e., of contracts that pretend to be vulnerable (as an incentive for an exploit attempt) but keep any Ether transferred.

Moreover, we omit seven redundant, intermediate, or positive findings: 
four findings of of Maian (\emph{accepts\_Ether}, \emph{no\_Ether\_leak}, \emph{no Ether lock}, \emph{not\_destructible}), 
one of Osiris (\emph{arithmetic\_bug}), 
one of Vandal (\emph{checked\_call\_state\_update}) and 
one of eThor (\emph{secure}). 

Finally, 10 findings do not match any SWC class and are omitted as well: 
one finding of Ethainter (\emph{unchecked\_tainted\_static call}), 
one of Securify (\emph{missing\_input\_validation}),
two of Maian (\emph{Ether\_lock},\emph{Ether\_lock\_Ether\_accepted\_without\_send}), 
five of Osiris (\emph{Callstack\_bug}, \emph{Division\_bugs}, \emph{Modulo\_bugs}, \emph{Signedness\_bugs}, \emph{Truncation\_bugs}), and 
one of Oyente (\emph{Callstack\_Depth\_Attack\_Vulnerability}).


