\subsection{Related Work}\label{sec:related}
\input{table_related}

\ITEM{Recent Systematic Reviews on Analysis Tools}.
Two studies from early 2022 show that the automated analysis of Ethereum smart contracts has still room for improvement.
\cite{Rameder2022} describe the functionalities and methods of \np{140} tools (\np{83} open source) for automated vulnerability analysis of Ethereum smart contracts.
Their literature review identifies 54 vulnerabilities, with some not addressed by any of the tools.
Moreover, the authors find many tools to be unmaintained.
\cite{Kushawa2022tools} provide a systematic review of \np{86} analysis tools with a focus on \np{13} common vulnerabilities.
For quality assessment, they select \np{16} tools, which they test on five vulnerabilities using a ground truth of \np{30} contracts.

\ITEM{Tool Evaluations without Test Sets}.
In 2019, two surveys evaluate tools for vulnerability detection by installing them and working through the documentation:
\cite{MdAGS2019tools} investigated \np{27} tools with respect to availability, maturity, methods employed, and security issues detected.
\cite{LopezVivar2020} evaluated \np{18} tools 
regarding the ease of installation, usefulness and updates.
Both studies do not assess the detection capabilities of the examined tools.

\ITEM{Benchmarked Evaluations}.
Most closely related are evaluations of tools that actually test them on a set of contracts (benchmark set).
Authors of tools, however, tend to compare their own artifact to a few similar and/or popular ones. 
We do not consider those works since they are intrinsically biased.

Among the independent evaluations, we find 11 related works \citep{Dika2017,Parizi2018empirical,Gupta2019,Durieux2020,ghaleb2020effective,Leid2020,Zhang2020a,Dias2021,Ji2021,Ren2021,Kushawa2022tools}
of which we give an overview in~\autoref{tab:related}.
In the first two rows, we indicate the respective reference and the year when the evaluation was carried out.
Rows three to five list the size of the benchmark set, separated into vulnerable and non-vulnerable contracts, or unknown number of vulnerable contracts.
All references use Solidity files as benchmarks.
Row six indicates the number of different vulnerabilities tested.
We highlight low numbers in red and commendable high numbers in green.
We also list for each tool which evaluation it was part of.
We highlight the five tools most often used in light blue.
In the last row, there is the total number of tools used in each study.
We highlight the five references using the most tools in mid blue.

The earliest evaluation dates back to 2017 and covers four tools tested on five vulnerabilities with a benchmark set of \np{23} vulnerable and \np{21} non-vulnerable contracts.
Regarding the benchmark sets, the number of contracts contained shows a large variety from only \np{10} to almost \np{50000}.
The number of vulnerable contracts in the benchmark set also varies largely from \np{10} to \np{9369}. 
It should be noted that for benchmark sets from \textit{the wild}, i.e.\ from the contracts actually deployed on the mainchain, the true number of vulnerable contracts and vulnerabilities contained in the contracts is yet unknown.
The number of different vulnerabilities varies from \np{4} to \np{131}. 
Most evaluations use their own taxonomy of vulnerabilities. This may be due to the lack of an established taxonomy~\citep{Rameder2022}.

We find a total of \np{20} tools mentioned in the evaluations, while each work selects a subset thereof for its tests.
The number of tools tested varies from three to a maximum of 16. 
The tools most often included in a comparison are Mythril, Oyente, Securify, Slither, and SmartCheck.

We note that our study considers the largest number of contracts from the wild (\np{248328} unique contracts) and it is the only one that considers the tools Conkas, Ethainter, eThor, MadMax, Pakala, teEther, and Vandal.

\ITEM{Open Source Frameworks}. \label{ssec:relatedOSF}
For a large-scale evaluation, we need an analysis framework that (i) facilitates the control of multiple tools via a uniform interface, (ii) allows for bulk operation, and (iii) is open source and usable.
SmartBugs~\citep{ferreira2020smartbugs} is such an execution framework released in 2019.
It is still being maintained with 13 contributors and over \np{70} resolved issues.
The framework USCV~\citep{Ji2021} implemented similar ideas in mid-2020.
It comprises an overlapping set of tools and an extension of the ground truth set.
With a total of 10 commits (the latest in mid-2021) and no issues filed, it seems to be neither widely used nor maintained.
Both frameworks target Solidity source code, and thus need to be expanded to work with bytecode.

