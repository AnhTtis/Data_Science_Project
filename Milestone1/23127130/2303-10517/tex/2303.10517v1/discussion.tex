\section{Discussion} \label{sec:discussion}

\subsection{Relation between Findings, Errors/Failures, and Overlap} \label{ssec:relation}
Taking a closer look at the evolution of two SWC classes, we discuss connections between the findings and their overlap in the context of the errors and failures that the tools report.
% 101
\begin{figure}
\centering
\hspace{14mm}
\includegraphics[width=.78\columnwidth]{pics/agreement101.pdf}
\includegraphics[width=.78\columnwidth]{pics/101_br.png}
\includegraphics[width=.78\columnwidth]{pics/101_errors.png}
\includegraphics[width=.78\columnwidth]{pics/101_fails.png}
\caption{SWC-101 Integer Overflow and Underflow on a timeline of blocks, in bins of \np[k]{100} blocks.
Top: Percentage of overlaps. Upper middle: Percentage of bytecodes flagged by the tools. 
Lower middle: Error rate of tools. Bottom: Failure rate of tools.}
\label{fig:swc101}
\end{figure}

\ITEM{SWC 101 - Integer Overflow and Underflow:}
In \autoref{fig:swc101}, the upper middle plot depicts the percentage of bytecodes flagged by the tools.
Starting from different levels around \np[\%]{70} and \np[\%]{40}, Conkas and Mythril converge at \np[\%]{10} at the end of the timeline.
Osiris shows a weakness level comparable to these tools for most of the timeline but falls to \np[\%]{0} towards the end. 
MadMax reports hardly any cases throughout the whole timeline.
For Osiris, the drop to \np[\%]{0} is related to the rise of errors (lower middle plot).
The tool was not designed for the instructions introduced at later forks, so recent contracts lead to an error and the analysis aborts without findings.
MadMax specializes in gas rather than arithmetic issues, with one finding constituting a very specific type of overflow, which is a small subclass of SWC-101.
Therefore, is not surprising that it reports very low numbers.

The top plot in \autoref{fig:swc101} depicts the percentage of agreement between the tools reporting on SWC-101 on a timeline of blocks, in bins of \np[k]{100} blocks.
The blue line shows the total percentage of bytecodes flagged for SWC-101 by at least one tool, while the black line shows the total absolute number of flagged bytecodes (scale on the right).
While the absolute number of flagged bytecodes fluctuates (as it depends on the number of deployments), the percentage steadily drops from as high as \np[\%]{90} to less than \np[\%]{20}.

Regarding the overlap, the four tools hardly ever agree (top plot: purple spots on top).
As MadMax does not report much by design, we focus on the other three tools.
The green area mostly reflects the agreement between Conkas, Osiris, and Mythril, while the red area represents an agreement between two tools.
We observe that both, the agreement between three and two tools decreases over time.
Simultaneously, the percentage of bytecodes increases where just a single tool reports a finding for SWC-101 (dark pink for MadMax, light pink for Osiris, light gray for Conkas, and medium gray for Mythril).
Moreover, we see that even though the numbers of Mythril and Conkas converge towards the end (top: same size light and medium gray area; upper middle: blue and green line), the overlap is close to zero (top plot: small red area, large light and medium gray area).


% 107
\begin{figure}
\centering
\hspace{14mm}
\includegraphics[width=.78\columnwidth]{pics/agreement107.pdf}
\includegraphics[width=.78\columnwidth]{pics/107_br.png}
\includegraphics[width=.78\columnwidth]{pics/107_errors.png}
\includegraphics[width=.78\columnwidth]{pics/107_fails.png}
\caption{SWC-107 Reentrancy on a timeline of blocks, in bins of \np[k]{100} blocks. 
Top: Percentage of overlaps. Upper middle: Percentage of bytecodes flagged by the tools. 
Lower middle: Error rate of tools. Bottom: Failure rate of tools.}
\label{fig:swc107}
\end{figure}

\ITEM{SWC 107 - Reentrancy:} 
In \autoref{fig:swc107}, the upper middle plot depicts the percentage of bytecodes flagged by the tools.
Except for the first \np[M]{2} blocks, Mythril, Osiris, Oyente, and Securify report consistently far lower findings than Conkas and eThor.
From block \np[M]{3.5} onward, Conkas and eThor report similar rates of reentrant contracts, but the Jaquard similarity of the flagged contracts (number of contracts flagged by both tools divided by the number of contracts flagged by at least one tool) is only \np[\%]{45}, and drops to \np[\%]{28} for the last part where the graphs coincide.
This is also reflected in the top plot, where the overlap between the two (red and green areas) decreases steadily from block \np[M]{4.5} onward, while the sole reporting (blue for eThor and light pink for Conkas) increases.

The error rates (lower middle plot) stay low for all tools except for Osiris.
While the failure rate (bottom plot) increases for Conkas up to \np[\%]{70}, it fluctuates between \np[\%]{20} and \np[\%]{40} for eThor.
Hence, the decrease in findings for Conkas is related to its increase in the failure rate, while we cannot relate it for eThor.
On the contrary, for the last 2\,M blocks both rates decrease for eThor.

The top plot in \autoref{fig:swc107} depicts the percentage of agreement between the tools reporting on SWC-107 on a timeline of blocks, in bins of \np[k]{100} blocks.
The blue line shows the total percentage of bytecodes flagged for SWC-107 by at least one tool, while the black line shows the total absolute number of flagged bytecodes (scale on the right).
While the absolute number of flagged bytecodes fluctuates (as it depends on the number of deployments), the percentage steadily drops from as high as \np[\%]{80} to less than \np[\%]{30}.

Regarding the overlap, the five tools hardly ever agree (top plot: smallish purple area on top).
Even though Mythril does not report high numbers (upper middle plot: green line), they are not backed by the other tools (top plot: medium gray area indicating sole reporting of Mythril).

\input{related}

\subsection{Combining or Comparing Tools Results}
When comparing or combining tool results, we have to deal with two challenges: (i) different aims of tools that are reflected in the way their findings are reported and (ii) differing definitions of weaknesses (that are associated with the findings), which makes it hard to map a finding to a class (within a common frame of reference) for comparison or combination. 

The tools can be divided into four groups with respect to their aim (for a specific weakness): (i) proving the absence of a property that is regarded as a weakness or vulnerability, (ii) over-reporting as to not overlook a potential weakness (aka issuing warnings), (iii) under-reporting since only those weaknesses are reported where a verification could be found (avoiding false alarms), (iv) reporting properties that are hardly a weakness (e.g.\ honeypots) or not necessarily (e.g.\ gas issues).

This distinction is important when comparing tools.
It strongly affects the number of agreements.
As we have seen in \autoref{ssec:rq5} and \autoref{ssec:relation}, the overall agreement is very low, which is partly due to the fact that tools address different versions, subsets or supersets of a weakness class.
Considering the different aims of the tools, the low general agreement is not surprising.
However, it is even low for tools with similar aims.

The aims of the tools also impact voting schemes that combine the results of several tools to `determine' whether a contract is actually vulnerable. 
For over-reporting tools, it may make sense to have a majority vote.
However, under-reporting tools should rather be joined than intersected.


\subsection{Comparison to Source Code as Input}
We selected the tools in our study for their ability to process runtime code, as our goal was to analyze contracts deployed on the mainchain, for which Solidity source code is often unavailable.
Moreover, this allowed us to include Ethainter, eThor, MadMax, Pakala, teEther and Vandal, which require runtime code.
The other selected tools accept both, bytecode and Solidity source code.
In this section, we discuss the effect of using source code as the input.

Conkas, Honeybadger, Osiris, Oyente and Securify compile the Solidity source to runtime code and then perform the same analysis as if the latter had been the input.
There are two differences, though.
First, the tools are able to report the location of weaknesses within the source, as they use a mapping provided by the compiler to translate bytecode addresses back to line numbers.
Second, for Solidity sources with more than one contract, the tools compile and analyze each one separately.
As complex contracts are structured into several layers of intermediate contracts using inheritance, this leads to redundant work.
While compilation and address mapping incur a negligible overhead, the additional contracts may lead to fewer or more findings within a fixed time budget, depending on whether there is less time for the main contract or whether other contracts contribute additional findings.\footnote{%
An easy remedy would be to extend the tools by a parameter with the name of the contract to analyze.}

Maian and Mythril compile the Solidity source as well but proceed with the deployment code, which includes contract initialization as well. 
Maian deploys the contract on a local chain and checks some properties live, like whether the contract accepts Ether. 
Moreover, the findings are filtered for false positives by trying to exploit the contract on the chain. 
Mythril, on the other hand, uses the deployment code to analyze also the constructor. 
For both tools, resource requirements and results will vary with the chosen form of input.

\subsection{Threats to Validity}
\label{ssec::threats}

\ITEM{Internal validity} is threatened by the integration of the new tools into SmartBugs.
We mitigated this threat by carefully following the SmartBugs instructions for tool integration and by consulting the documentation and the source code of the respective tools.
Multiple authors manually analyzed all execution errors to ensure that we had configured the tools adequately.
Moreover, we make the implementation and the results accessible for public inspection.

\ITEM{External validity} is threatened by the use of single bytecodes as proxies for code families identified by the same skeleton.
These representatives may not accurately reflect the code properties of all family members that are relevant to weakness detection.
We mitigated this threat by RQ1.
However, the random sample of \np{1000} bytecodes (\np{620} families) may have been chosen too small such that our answer to RQ1 may not generalize to all bytecodes.

\ITEM{Construct validity} is threatened by our mapping between the detected weaknesses and the SWC classes.
The mapping reflects our understanding of the weaknesses and what the tools actually detect, which may be incorrect.
We mitigated this risk by involving all authors during the mapping phase and by discussing disagreements until we reached a consensus. 
Another potential threat is the resources, 30 minutes and up to 32\,GB per tool and bytecode.
This configuration is in line with related work or surpasses it.
