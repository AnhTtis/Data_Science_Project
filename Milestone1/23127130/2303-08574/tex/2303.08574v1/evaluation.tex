We perform experiments to answer the following questions:
\begin{enumerate}
	\item[(Q1)] Which milestones as described in Section~\ref{sec:milestones} can be achieved with our algorithm?
	\item[(Q2)] How does WikiCoder compare to GPT-3 and Codex, the algorithm powering Copilot based on very large language models?
	\item[(Q3)] Can a knowledge-powered program synthesis tool operate at scale for classical purely syntactic tasks?
\end{enumerate}

\subsection{Environment}
The dataset along with the code for the experiments are available publicly on GitHub\footnote{\url{https://github.com/nathanael-fijalkow/ProgSynth}}.
WikiCoder is written in Python, it leverages a prediction model written in PyTorch.
We favoured simplicity over performance as well as clear separation into independent components as described above.

\paragraph*{Benchmark suite}
Since there are no existing datasets to test the new aspects introduced in Section~\ref{sec:milestones}, we created one ourselves.
The dataset is comprised of 46 tasks in the spirit of the FlashFill dataset~\cite{Gulwani2011}. 
Some of the tasks are inspired or extracted from~\cite{VerbruggenLG21}, they are tagged as such in our code.
Each task requires external knowledge to be solved and is labelled with 3 metadata:
\begin{itemize}
	\item preprocessing for entity extraction: 0 if the inputs are already the sought entities, 1 if a syntactical program is enough to extract them, and 2 otherwise;
	\item complexity of the relationships between entities : 0 when no relation from the knowledge graph is needed, 1 for simple (single edge in the knowledge graph), and 2 for composite;
	\item postprocessing on external knowledge: 0 if the knowledge is used without postprocessing, and 1 otherwise.
\end{itemize} 
This induces 8 categories, we provide at least 4 tasks for each category.

\paragraph*{Knowledge graph}
For simplicity and reproducibility, we use a custom-made small knowledge graph for the experiments. We provide \texttt{SPARQL} queries to construct the knowledge graph.

\paragraph*{Experimental setup}
All our experiments were performed on a consumer laptop (MSI GF65 Thin 9SE) with an intel i7-9750H CPU working up to 2.60GHz on a single thread. The operating system is Manjaro 21.3 with linux kernel 5.17.5-1. The code is written in Python 3.8.10.
The framework used for the \texttt{SPARQL} database is BlazeGraph 2.1.6
The code made available includes all the details to reproduce the experiments, it also includes the exact version of the Python libraries used.

\paragraph*{Prediction Model}
In our prediction model we embed the examples with a one hot encoding.
Since the number of examples can change from one task to another, they are fed into a single layer RNN to build an intermediate representation of fixed shape.
Then it is followed by 4 linear layers with ReLU activation functions, except for the last layer where no activation function is used.
The final tensor is split, and a log softmax operator is applied for each non-terminal of the grammar, thus producing a probabilistic context free grammar in log space.

The prediction model is trained on a dataset of 2500 tasks for 2 epochs with a batch size of 16 tasks.
The tasks were generated with the following process: a program was sampled randomly from a uniform probabilistic context free grammar, then inputs are randomly generated and run on the program.
We used the Adam optimiser with a cross entropy loss to maximise the probability of the solution program being generated from the grammar.

\subsection{Results on the new dataset}
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{results.png}
	\caption{WikiCoder performance on our new dataset}
	\label{fig:results}
\end{figure}
WikiCoder solves 18 out of 46 tasks with a timeout of 60 seconds.
Let us make more sense of this number:
\begin{itemize}
	\item Since our algorithm does not perform postprocessing on knowledge, none of the 16 tasks involving knowledge postprocessing were solved.
	Thus only 30 tasks are within reach of our algorithm.
	\item Among the 30 tasks, for 19 of them the inputs are directly the sought entities.
	WikiCoder solves 12 out of these 19 tasks, most failed tasked are not solvable with our DSL.
	\item In the remaining 11 tasks, the entities can be retrieved using syntactic manipulations.
	WikiCoder solves 6 out of these 11 tasks.
	\item Digging deeper for the last case, the fault lies in all remaining 5 cases with the preprocessing algorithm, which fails to correctly decompose the task and formulate the appropriate \texttt{SPARQL} queries. 
	However, when provided with the right decomposition, WikiCoder solves all 5 tasks.
\end{itemize}
The results can be visualised on Figure~\ref{fig:results}.
The takeaway message is: WikiCoder solves almost all tasks which does not involve knowledge postprocessing, and when it does not the main issue is entities extraction with the preprocessing algorithm.

\subsection{Comparison with Codex and GPT-3}
It is not easy to perform a fair comparison with Codex and GPT-3 as they solve different problems.
There are two ways these models can solve programming by example tasks.
The experiments were performed on OpenAI's beta using the Da Vinci models (the most capable) for GPT-3 and Codex, with default parameters.

\vskip1em
Using Codex: as \textit{code completion}, provided with examples as docstring. This makes the problem very hard for Codex for two reasons: first, it was trained to parse specification in natural language, possibly enriched with examples. Providing only examples is a very partial specification which Codex is not used to. Second, Codex does not use a domain specific language, but rather general purpose programming languages such as Python and C. This implies that the program might be wrong in different ways: incorrect syntax, typing issues, or compilation errors.
For the reasons above, in this infavourable comparison Codex solves only the 5 easiest tasks.

\vskip1em
There are two ways of using GPT-3: \textit{query answering} or \textit{code completion}. 
For query answering, we feed GPT-3 with all but the last examples, and ask it to give the output for the input of the last example:
\begin{minted}{python}
Query:
    f("France") = "I live in France"
    f("Germany") = "I live in Germany"
    f("Poland") = "I live in Poland"
    f("New Zealand") = ?
Output:
    f("New Zealand") = "I live in New Zealand"
\end{minted}
The weakness of this scenario is that GPT-3 does not output a program: it only answers queries. One consequence is that the correctness test is very weak: getting the right answer on a single query does not guarantee that it would on any input. Worse, not outputting a program means that the whole process acts as a black-box, giving up on the advantages of our framework (see related work section) and program synthesis in general.

Using GPT-3/ChatGPT as \textit{code completion}: the prompt is to generate a Python function that satisfies the following examples. This fixes the weakness of not outputting a program with query answering. However, on tasks that require a knowledge graph, the answer is either a succession of if statements or using a dictionnary which is semantically equivalent to the if statements.
In some sense, the system having only partial knowledge, it does not attempt to generalise beyond the given examples.
%From another point of view, it can be seen as GPT-3 seeing the dictionnary as some partial knowledge graph that is left to the user to complete.

GPT-3 performs very well in the query answering setting, solving 31 tasks out of 46. Only 2 tasks were solved by WikiCoder and not by GPT-3, and conversely 11 by GPT-3 and not by WikiCoder. GPT-3 only solves 3 tasks involving knowledge postprocessing: in this sense, WikiCoder and GPT-3 suffer from the same limitations, they both struggle with knowledge postprocessing.

\subsection{Results on FlashFill}
To show that WikiCoder operates at scale on classical program synthesis tasks, we test it against the classical and established FlashFill dataset.
The results are shown on Figure~\ref{fig:flashfill} (displaying cumulated time), WikiCoder solves 70 out of 101 tasks with a timeout of 60 seconds per task, on par with state of the art general purpose program synthesis tools.
Only 85 tasks can be solved with our DSL.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{flashfill.png}
	\caption{WikiCoder solves 70\% of FlashFill tasks}
	\label{fig:flashfill}
\end{figure}

\subsection{Limitations}
To show the limitations of our approach, we discuss some counterexamples.

\paragraph*{Entities extraction} 
Our preprocessing algorithm looks for the longest shared pattern of length at least 2 in the examples.
Let us consider the following example:
\begin{minted}{python}
f("France") = "I live in France"
f("Germany") = "I live in Germany"
f("Poland") = "I live in Poland"
\end{minted}
The longest pattern is naturally ``I live in '', however another pattern occurs, unexpectedly: ``an'' appears in each country's name. When using our preprocessing algorithm this leads to a wrong sketch.
Adding a fourth example would remove this ambiguity if the country name does not include ``an''.
A related example would be if all examples include year dates in the same millenium, say all of the form ``20XX'': then ``20'' appears in each example, but it should not be separated from XX.

\paragraph*{Knowledge postprocessing} 
None of the tasks involving knowledge postprocessing were solved. Indeed, if the entity to be used is not present in the example, it is very hard to guess which one it is. Natural candidates are entities in the neigbourhood of the starting entity. Our approaches to this challenge have proved inefficient.

\subsection{Examples of Programs}

To show that the programs generated are clear and interpretable by humans, we show a few of them translated into Python equivalents. 
%Note that one function call in the DSL means one function call in Python.
Here is the first example where two relations are needed:
\begin{minted}{python}
// Examples:
    f("France") = "French, capital:Paris"
    f("Germany") = "German, capital:Berlin"
    f("China") = "Chinese, capital:Beijing"
    f("New Zealand") = "New Zealander, capital:Wellington"

// Generated program:
def f(x: str) -> str:
    a = label(follow_edges_from(x, "demonym"))
    b = ", capital:"
    c = label(follow_edges_from(x, "isCapitalOf"))
    return a + b + c
\end{minted}
We present another example where a relation at distance 2 is needed:
\begin{minted}{python}
// Examples:
    f("Paris") = "The phone country code is 33"
    f("Berlin") = "The phone country code is 49"
    f("Detroit") = "The phone country code is 1"
    f("Chihuahua") = "The phone country code is 52"

// Generated program:
def f(x: str) -> str:
    a = "The phone country code is " 
    b = label(follow_edges_from(x, "CityOf", "phoneCode"))
    return a + b
\end{minted}
%We decided to not add other examples of tasks solved since apart from the program synthesis perspective they do not add any information.
