A preprocessing algorithm produces from the examples three pieces of information: a sketch, which is a decomposition of the current task into subtasks, a set of \texttt{SPARQL} queries for the knowledge graph, and an embedding of the examples for the prediction model.

\begin{figure}[ht]
   \centering
   \includegraphics[width=0.8\textwidth]{preprocessing}
   \caption{Example of the preprocessing algorithm.}
   \label{fig:preprocessing}
\end{figure}

\paragraph*{Preprocessing algorithm}
Figure~\ref{fig:preprocessing} illustrates the preprocessing algorithm in action.
The high level idea is that the algorithm is looking for a shared pattern across the different examples.
In the task above, `` code: '' is shared by all examples, hence it is extracted out.
A naive implementation is to look for the largest common factor between the strings, and proceed recursively on the left and on the right.
The process is illustrated with Algorithm~\ref{alg:preprocessing}, which produces a list of constants from a list of strings. This procedure is applied to both the inputs and outputs independently to create sketches.
Thanks to these constants the task is split into subtasks as illustrated in Figure~\ref{fig:preprocessing} where we have split the task into two subtasks: \texttt{CapitalOf} and \texttt{CapitalOf-PhoneCode}.
To solve each subtask we query the knowledge graph with the inputs and outputs.
If no path is found, we run a regular -- syntactical -- program synthesis algorithm.

\begin{algorithm}[ht]
   \caption{Constant extraction}\label{alg:preprocessing}
   \begin{algorithmic}
      \Procedure {GetConstants}{$S = (S_k)_{k \in [1,n]}$ : strings}
      \If{there is an empty string in $S$} 
         \State \textbf{return} empty list
      \EndIf
      \State $\text{factor} \gets$ longest common factor among all strings in $S$
      \If{$\text{len}(\text{factor}) \le 2$}
	      \State \textbf{return} empty list
	  \Else
	      \State $S_{\text{left}} \gets$ prefix of factor in $S$ 
	      \State $L_{\text{left}} \gets$ \textsc{GetConstants}$(S_{\text{left}})$
	      \State $S_{\text{right}} \gets$ suffix of factor in $S$ 
	      \State $L_{\text{right}} \gets$ \textsc{GetConstants}$(S_{\text{right}})$
          \State \textbf{return} $L_{\text{left}} + \text{factor} + L_{\text{right}}$
      \EndIf
      \EndProcedure
   \end{algorithmic}
\end{algorithm}


\paragraph*{Generated \texttt{SPARQL} queries}
The \texttt{SPARQL} queries are generated from the examples after preprocessing.
Once we have the constants with Algorithm~\ref{alg:preprocessing} of the inputs and outputs, we can split the inputs and outputs in constant parts and non-constant parts, only the non-consant parts are relevant.
For each non-constant part in the outputs, we generate queries from the non constant part of the inputs which should map to this non-constant part of the output. 
Since relations may be complex, that is ``Paris'' is at distance 1 from ``France'' but ``33'' is two relations away from ``Paris'', we generate \texttt{SPARQL} queries for increasing distances up to a fixed upper bound.
Here is the query at distance 2, that we execute for the example in Figure~\ref{fig:preprocessing} with \texttt{CapitalOf-PhoneCode}:
\begin{verbatim}
   PREFIX w: <https://en.wikipedia.org/wiki/>
   SELECT ?p0 ?p1 WHERE {
      w:Paris ?p0 ?o_1_0 .
      ?o_1_0 ?p1 w:33 .
      w:Berlin ?p0 ?o_2_0 .
      ?o_2_0 ?p1 w:49 .
      w:Warsaw ?p0 ?o_3_0 .
      ?o_3_0 ?p1 w:48 .
   }
\end{verbatim}
Notice that intermediary entities make an apparition in order to accommodate for longer path lengths.
The output of the above query would consist of two paths: \texttt{CityOf-PhoneCode} and \texttt{CapitalOf-PhoneCode}.

As disambiguation strategy (inspired by~\cite{ZhengSCLW22}) we choose the path with the least number of hits across all examples, called the least ambiguous path. 
In this example there is no preferred path since both paths lead to a single entity for each example.
%One could argue that \texttt{CityOf-PhoneCode} should be preferred since there are more starting entities than \texttt{CapitalOf-PhoneCodeOf}, thus it is more likely that we find such a path from our starting entity. 
%We did not implement such a mechanism but this could be done to select one path among the least ambiguous ones.
As an example of this disambiguation strategy, let us consider paths from ``33'' to ``Paris'': there are two paths, 
\texttt{PhoneCodeOf-Capital} and \texttt{PhoneCodeOf-City}.
The least ambiguous path is \texttt{PhoneCodeOf-Capital} since \texttt{PhoneCodeOf-City} leads to all cities of the country.

To find the least ambiguous path, we need to count the number of hits, which is done using more \texttt{SPARQL} queries.
Here is a sample query to get all entities at the end of the path \texttt{CapitalOf-PhoneCode} from the starting entity ``Paris'':
\begin{verbatim}
   PREFIX w: <https://en.wikipedia.org/wiki/>
   SELECT ?dst WHERE {
      w:Paris w:CapitalOf ?e0 .
      ?e0 w:PhoneCode ?dst .
   }
\end{verbatim}
We count the number of results for all examples to get the number of hits for a path.

\begin{figure}[ht]
   \centering
   \includegraphics[width=\textwidth]{predictions}
   \caption{Illustration of the prediction model.}
   \label{fig:predictions}
\end{figure}

\paragraph*{Prediction model}
Efficient search is crucial in program synthesis, because the search space combinatorially explodes. We, and we believe the community more broadly, see learning as the right way of addressing this challenge.
Following~\cite{EllisWNSMHCST21,Fijalkow2022ScalingNP}, we build a prediction model in the form of a neural network: it reads embedding of the examples and outputs probabilities on the derivation rules of the grammar representing all programs. 
This transforms the context-free grammar representing programs in a probabilistic context-free grammar, which is a stochastic process generating programs. In other words, it defines a probabilistic distribution over programs, and the prediction model is trained to maximise the probability that a solution program is generated.

This prediction process is illustrated in Figure~\ref{fig:predictions}.
This effectively leverages the prediction power of neural networks without sacrificing correctness: the prediction model biases the search towards most likely programs but does not remove part of the search tree, hence -- theoretically -- a solution will be found if there exists one.