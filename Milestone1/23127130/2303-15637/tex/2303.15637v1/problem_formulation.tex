
% \label{s: problem formulation}
%\Bruce{Should the underlying true parameter be denoted $\theta_\star$ to distinguish from an arbitrary $\theta$? Doing so would add some complications, as in particular assumptions would need to be stated for $\theta_\star$, or for all $\theta' \in B(\theta_\star, \e)$.}

\paragraph{Problem Formulation}
Let $\theta \in \mathbb{R}^{d_\Theta}$ be an unknown parameter. We study the fundamental limitations to learning to control the following parametric system model:
\begin{equation}
\begin{aligned}
\label{eq:lds_ac}
X_{t+1} \!=\! A(\theta) X_t \!+\! B(\theta) U_t \! +\! W_t, \, X_0 \!=\! 0, % \sim \mathsf{N}(0,\Gamma_{0}), 
\quad t\!=\!0,1,\dots.
\end{aligned}
\end{equation}
% parameterized by $\theta\in \mathbb{R}^{d_\Theta}$.
The noise process $W_t$ is assumed to be \iid\  mean zero Gaussian with fixed covariance matrices $\Sigma_W\succ 0$. The matrices $A(\theta) \in \mathbb{R}^{\dx\times\dx}$ and $B(\theta)\in\mathbb{R}^{\dx\times\du}$
%, $\Gamma_0(\theta) \in \mathbb{R}^{\dy \times \dx}$ 
are known continuously differentiable functions of the unknown parameter. The system $(A(\theta), B(\theta))$ is assumed to be stabilizable. 

We assume that the learner is given access to $N \in \mathbb{N}$ experiments $(X_{0,n},\dots,X_{T-1,n}), n\in[N]$ from \eqref{eq:lds_ac} of length $T \in \mathbb{N}$. The input signal during these experiments is  
\begin{align}
    \label{eq: input}
    U_{t,n} = F X_{t,n} + \tilde U_{t,n},
\end{align}
where $F$ renders the system stable\footnote{Access to a stabilizing controller is often assumed unstable system identification \cite{ljung1998system}. Open-loop unstable identification leads to poor conditioning.}, i.e. $\rho(A(\theta)+B(\theta) F) <1$. Meanwhile, $\tilde U_{t,n}$ is an exploration component with energy budget $ \sigma_{\tilde u}^2 NT$,\footnote{The choice to place a budget on the exploratory input $\tilde U_{t,n}$ rather than the total input $U_{t,n}$ is for ease of exposition. The energy of the exploratory input is bounded by the total budget, which is sufficient for our bounds.} where $ \sigma_{\tilde u}\in \mathbb{R}_+$. More precisely, $\tilde U_{t,n}$ may be selected as a function of past observations $(X_{0,n},\dots,X_{t,n})$, past trajectories $(X_{0,m},\dots,X_{T-1,m}),m<n$ and possible auxiliary randomization, while being constrained to an energy budget
\begin{align}
\label{eq:budgeteq}
   \frac{1}{NT} \sum_{n=1}^N \sum_{t=0}^{T-1} \E_\theta \tilde U_{t,n}^\top  \tilde U_{t,n} \leq \sigma_{\tilde u}^2 .
\end{align}
 This formulation allows both open- and closed-loop experiments, but normalizes the average exploratory input energy to $ \sigma_{\tilde u}^2$. The subscript $\theta$ on the expectation denotes that the system is rolled out with parameter $\theta$. For a fixed parameter $\theta$, we denote the data collected from these experiments by the random variable $\calZ := \{\{(X_{t,n}, U_{t,n}\}_{t=0}^{T-1}\}_{n=1}^N$. %For a fixed parameter $\theta$, we denote the joint distribution over $\{X_{t,n}\}, t=0,\dots,T-1, n \in [N]$ by $\mathsf{P}_\theta$ and the data collected from these experiments by $\calZ := \{\{(X_{t,n}, U_{t,n}\}_{t=0}^{T-1}\}_{n=1}^N$. 

% \Bruce{Added some to this explanation to make the notation consistent. Seems a bit overcomplicated. Suggestions welcome.}
The learner deploys a policy $\pi$ which is a measureable function of the $N$ offline experiments and the current state.  %, but not of the current rollout. %The current rollout is an evaluation rollout, where the learned policy is deployed in the face of new randomness. This new randomness, denoted by the random variable $\calW = \curly{W_t}_{t=0}^{T-1}$, in conjunction with the learned policy $\pi$ induces an evaluation state/input trajectory $\curly{(X_t, U_t)}_{t=0}^{T-1}$. 
In particular, the learner maps the offline data and the current state to the control input, $U_t = \pi(X_t; \calZ)$. This is the case if the learner outputs a non-adaptive state feedback controller designed with the offline data. The goal of the learner is to minimize the cost defined by: % the evaluation trajectory: %\Bruce{Explain the evaluation roll-out here, and assign a random variable that captures the randomness in the evaluation roll-out. }
\begin{align*}
V_T^{\pi}(\theta) \!&:= \!\frac{1}{T}\! \E_\theta^\pi \! \left[ \sum_{t=0}^{T-1}\! \left(  X_t^\T Q X_t +U_t^\T R U_t\right) \!+\! X_T^\T Q_T(\theta) X_T
\right]\!.
\end{align*}
The expectation is over both the offline experiments, and a new evaluation rollout. Single subscripts on the states and actions, $X_t$ and $U_t$, refer to the evaluation rollout at time $t$. %while double subscripts $X_{t,n}, U_{t,n}$ refer to the $n^\textrm{th}$ offline rollout at time $t$. 
The superscript on the expectation denotes that the inputs applied in the evaluation rollout follow the policy $U_t= \pi(X_t; \calZ)$. Note that due to the dependence of the terminal cost $Q_T(\theta)$ on the unknown parameter $\theta$, the learner does not explicitly know the cost function it is minimizing. This is not an issue: it simply means that the learner must infer the objective function from the collected data.

The following assumption guarantees the existence of a static state feedback controller that minimizes $V_T^\pi(\theta)$.
\begin{assumption}
    We assume $(A(\theta), B(\theta))$ is stabilizable, $(A(\theta), Q^{1/2})$ is detectable, and $R \succ 0$ and that $Q_T(\theta) = P(\theta)$, where $P(\theta) = \dare(A(\theta),B(\theta),Q,R)$.
\end{assumption}
 Under this assumption, the optimal policy for the known system is $U_t = K(\theta) X_t$, where $K(\theta)$ is the LQR:
\begin{align*}
    K(\theta) = -(B(\theta)^\top P(\theta) B(\theta) + R)^{-1} B(\theta)^\top P(\theta) A(\theta). 
\end{align*}
In light of this, we focus on the case in which the search space of the learner is the class of linear time-invariant state feedback policies where the gain is a measurable function of the past $N$ experiments\footnote{This assumption is not critical, and may be removed without significantly changing the result. See the proof of the main result in \cite{ziemann2022regret} for details on how to remove this assumption.}. This set is denoted $\Pi_{\mathsf{lin}}$.

The stochastic LQR cost $V_T^\pi(\theta)$ may be represented in terms of the gap between the control actions taken by the policy $\pi$ and the optimal policy, as shown below.
\begin{lemma}[Lemma 11.2 of \cite{soderstrom2002discrete}]
    \label{lem: stochastic lqr representation}
    We have that
    \[
        V_T^\pi(\theta) = \trace(P(\theta) \Sigma_W) + \frac{1}{T}\sum_{t=0}^{T-1}\!\E_{\theta}^\pi \norm{U_t- K(\theta)X_t }^2_{\Psi(\theta)},
    \]
    where $\Psi(\theta) := B^\T(\theta) P(\theta)B(\theta)+R$.
\end{lemma}

Using the above lemma, the objective of the learner may be restated from minimizing $V_T^\pi(\theta)$ to minimizing the excess cost: %suboptimality gap: %\footnote{Again assuming $Q_T(\theta) = P(\theta)$.}: %{\color{red} This is not expressed correctly}
\begin{equation}
\begin{aligned}
\label{eq: suboptality quadratic form}
% \textrm{\resizebox{\linewidth}{!}{ $\displaystyle \mathsf{EC}_T^\pi(\theta) =V_T^\pi(\theta)-\inf_{\hat \pi} V_T^{\hat \pi}(\theta)=\sum_{t=0}^{T-1} \E_{\theta}^\pi \norm{U_t- K(\theta)X_t }^2_{\Psi(\theta)},$}}
\! \mathsf{EC}_T^\pi(\theta) \! &:=\! V_T^\pi(\theta) \!-\!\inf_{\hat \pi} V_T^{\hat \pi}(\theta)\!=\!\frac{1}{T}\!\sum_{t=0}^{T-1}\!\E_{\theta}^\pi \! \norm{U_t\!-\! K(\theta)X_t }^2_{\Psi(\theta)}\!.
% \mathsf{EC}_T^\pi(\theta) &=V_T^\pi(\theta)-\inf_{\hat \pi} V_T^{\hat \pi}(\theta)\\&
% =\sum_{t=0}^{T-1} \E_{\theta}^\pi (U_t- K(\theta)X_t )^\T \Psi(\theta)  (U_t-K(\theta)X_t ),
\end{aligned}
\end{equation}
%where $\Psi(\theta) = B^\T(\theta) P(\theta)B(\theta)+R$. 
%\Tasos{I suggest to state the lemma again here without proof since it is very fundamental.} 
The second equality follows from the representation of the stochastic LQR cost in \Cref{lem: stochastic lqr representation} by cancelling the constant terms. Note that the infimum in the second term is given access to the true parameter value $\theta$, and will therefore be attained by the optimal LQR controller. In particular, it does not rely upon the offline experimental data. We denote this optimal policy by $\pi_{\theta}(X_t; \calZ) = K(\theta) X_t$.   
%\Ingvar{need to reference thm statement in söderström. Its sth like ch 11 or 12---What is "the expression"?} 

Our objective is to lower bound the excess cost for any learning agent in the class $\Pi_{\mathsf{lin}}$. To this end, we introduce the $\e$-local minimax excess cost:
%\Bruce{mathscript - mathsrf pkg or cal. }
\begin{align}
    \label{eq: minimax suboptimality}
    \mathcal{EC}^{\mathsf{lin}}_T(\theta,\e) := \inf_{\pi \in \Pi_{\mathsf{lin}}} \sup_{\|\theta'-\theta\|\leq \e} \mathsf{EC}_T^\pi(\theta').
\end{align}
To motivate this choice,
first note that if we were instead interested in an excess cost bound for only a single value of $\theta$ that holds for all estimators, the optimal policy would trivially be the LQR, $\pi(X_t, \calZ) = K(\theta)X_t$. This policy would result in a lower bound of zero. By instead requiring that the learner perform well on all parameter instances in a nearby neighborhood, we remove the possibility of the trivial solution, and can achieve meaningful lower bounds. The emphasis of the nearby neighborhood in \eqref{eq: minimax suboptimality} is essential. As the local neighborhood defined by the ball of radius $\e$, $\calB(\theta,\e) = \curly{\theta'\vert \norm{\theta'-\theta}\leq \e}$, becomes sufficiently small,  we are still able to provide instance-specific lower bounds for a single parameter value $\theta$. Therefore, the $\e$-local minimax excess cost is a much stronger notion than the standard \emph{global} minimax excess cost,  $\inf_{\pi \in \Pi_{\mathsf{lin}}} \sup_{\theta'} \mathsf{EC}_T^\pi(\theta')$, as it does not require our estimator to perform well on \emph{all possible} parameter values but only those in a small (possibly infinitesimal) neighborhood. Indeed, the global minimax excess cost for learning the optimal controller of the class of unknown stable scalar systems is infinite, as shown in \Cref{cor: global minimax}, and illustrated in \Cref{fig: hard to control instance}. 


\begin{figure}
    \centering
    \ifnum\value{cdc}>0{ 
    \includegraphics[width=0.35\textwidth]{figures/lower_bound_minimax_complexity.pdf}
    }\else{
     \includegraphics[width=0.5\textwidth]{figures/lower_bound_minimax_complexity.pdf}
    }\fi
    \vspace{-0.1in}
    \caption{Consider the scalar system $X_{t+1} = a X_t + b U_t + W_t$. We plot a lower bound arising from \Cref{cor: asymptotic lower bound} letting $V = \bmat{0 & 1}^\top$ for the system $a = 1- \gamma$, $b = \gamma$ as $\gamma$ ranges from $10^{-3}$ to $10^{-2}$ with $F = 0$, $ \sigma_{\tilde u}^2 =1$, $\Sigma_W = 1$. As $\gamma \to 0$ the optimally regulated system approaches marginal stability and  controllability is lost. The problem of learning a controller therefore becomes challenging as $\gamma \to 0$, which is reflected by our excess cost lower bound; it approaches $\infty$. This illustrates the observation that systems which are difficult to control are also difficult to learn to control. This plot illustrates the result \Cref{cor: global minimax} in demonstrating that the global minimax excess cost is uninformative. 
    }
    \ifnum\value{cdc}>0{\vspace{-.25in}}\else{\vspace{-0.15in}}\fi
    \label{fig: hard to control instance}
\end{figure}


Our focus in obtaining the lower bound on $\mathcal{EC}^{\mathsf{lin}}_T(\theta,\e)$ is to gain an understanding of what system-theoretic quantities render the learning problem statistically challenging. To this end, our lower bound depends on familiar system-theoretic quantities, such as $P(\theta)$. The covariance of the state under the optimal LQR controller also appears in our analysis. Under the optimal LQR controller, the covariance of the state converges to the stationary covariance as $T \to \infty$: %  \Tasos{do you need assumptions for stationarity, e.g. large $T$ or stationary of $\E X_0X_0^\top$?}:
 \ifnum\value{cdc}>0{
 \begin{align*}
     \Sigma_X(\theta) &= \lim_{t \to \infty} \E^{\pi_\theta}_{\theta} \brac{X_t X_t^\top}  \\
        &= \dlyap(\paren{A(\theta) + B(\theta) K(\theta)}^\top, \Sigma_W).
 \end{align*}
 }\else{
\[
    \Sigma_X(\theta) := \lim_{t \to \infty} \E^{\pi_\theta}_{\theta} \brac{X_t X_t^\top} =  \dlyap(\paren{A(\theta) + B(\theta) K(\theta)}^\top, \Sigma_W).  
\]
}\fi
% This quantity appears in our analysis.
 %The superscipt notation here has been overloaded with a matrix $K(\theta)$, which we take to mean that the rollout occurs with $U_t= K(\theta) X_t$. 

%The controllability gramian from the exploratory input the state during offline exploration is given by $\sum_{t=0}^\infty (A(\theta) + B(\theta) F)^t  BB^\top \paren{(A(\theta) + B(\theta) F)^t}^\top$. It does not appear directly, however the related quantity $\sum_{t=0}^\infty \norm{(A+BF)^t B}$ does appear. \Bruce{The relationship needs to be explained at some point.}
%For simplicity, we will assume that each rollout starts from the optimal stationary distribution:  $\Gamma_0(\theta) = (A(\theta) + B(\theta) K(\theta)) \Gamma_0(\theta) (A(\theta) + B(\theta) K(\theta))^\top + \Sigma_W$.
% In this paper, we lower-bound the $\e$-local minimax suboptimality gap (over a linear class $\Pi_{\mathsf{lin}}$):
% \begin{align*}
%     \mathfrak{R}^{\mathsf{lin}}_T(\theta,\e) = \inf_{\pi \in \Pi_{\mathsf{lin}}} \sup_{\|\theta'-\theta\|\leq \e} \mathsf{EC}_T^\pi(\theta').
% \end{align*}


% {\color{red} Why local minimax suboptimality?
% \begin{itemize}
%     \item global minimax complexity is infinity. 
% \end{itemize}
% }

