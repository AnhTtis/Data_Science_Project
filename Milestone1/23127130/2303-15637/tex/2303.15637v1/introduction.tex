\section{Introduction}
\label{s: introduciton}

Reinforcement Learning (RL) has demonstrated success in a variety of domains, including robotics \citep{levine2016end} and games \citep{silver2017mastering}. However, it is known to be very data intensive, making it challenging to apply to complex control tasks. This has motivated efforts by both the machine learning and control communities to understand the statistical hardness of RL in analytically tractable settings, such as the tabular setting \citep{azar2017minimax} and the linear-quadratic control setting \citep{simchowitz2020naive}. 
%Does this make sense to claim both fundamental limits and efficiency of particular algorithmss?
Such studies provide insights into the fundamental limitations of RL, and the efficiency of particular algorithms. %\Ingvar{I wouldn't pair dean et al. with fundamental limitations. Their bounds suggest that easy to control => easy to learn but no converse statements. }

% \Bruce{is it a problem to refer to tabular stuff without having a related work section on it? Does Simchowitz make sense here?}
% \Bruce{rewrite with Tasos' suggestions}

There are two common problems of interest for understanding the statistical hardness of RL from the perspective of learning a linear-quadratic regulator (LQR): online LQR, and offline LQR. Online LQR models an interactive problem in which the learning agent attempts to minimize a regret-based objective, while simultaneously learning the dynamics \citep{abbasi2011regret}.  Offline LQR  models a two-step pipeline, where data from the system is collected, and then used to design a controller \citep{dean2019sample}. Guarantees in the online setting are in the form of regret bounds, whereas the offline setting focuses on Probably Approximately Correct (PAC) guarantees. The high data requirements of RL often render offline approaches the only feasible option for physical systems \cite{levine2020offline}. Despite this fact, recent years have seen greater efforts to provide lower bounds for the online LQR problem \citep{ziemann2022regret, tsiamis2022statistical}. Meanwhile, lower bounds in the offline LQR setting are conspicuously absent. Motivated by this fact, we derive lower bounds for designing a linear-quadratic controller from offline data. %We show that notwithstanding the familiarity of the setting for control theorists, several interesting consequences arise from our results. One consequence is that there exist classes of systems for which the lower bound scales exponentially with the state dimension. 

%Even with the recent interest in understanding the hardness of RL,  lower bounds in the offline linear-quadratic control setting are conspicuously absent. Instead, prior work \citep{ziemann2022regret, tsiamis2022statistical} focuses on lower bounds for linear-quadratic control in the more complicated online setting. The high data requirements for the online setting often make it infeasible for physical systems, such as robotics \citep{levine2020offline}. Motivated by this fact, we derive lower bounds for designing a linear-quadratic controller from offline data. We show that notwithstanding the familiarity of the setting for control theorists, several interesting consequences arise from our results. One consequence is that there exist classes of systems for which the lower bound scales exponentially with the state dimension. 

\textbf{Notation: } The Euclidean norm of a vector $x$ is denoted by $\norm{x}$. The quadratic norm of a vector $x$ with respect to a matrix $P$ is denoted $\norm{x}_P = \sqrt{x^\top P x}$. For a matrix $A$, the spectral norm is denoted $\norm{A}$ and the Frobenius norm is denoted $\norm{A}_F$. The spectral radius of a square matrix $A$ is denoted $\rho(A)$. A symmetric, positive semidefinite matrix $A = A^\top$ is denoted $A \succeq 0$, and a symmetric, positive definite matrix is denoted $A \succ 0$. Similarly, $A \succeq B$ denotes that $A-B$ is positive semidefinite. The eigenvalues of a symmetric positive definite matrix $A \in \R^{n \times n}$ are denoted $\lambda_1(A), \dots, \lambda_n(A)$, and are sorted in non-ascending order. We also denote $\lambda_1(A) = \lambda_{\max}(A)$, and $\lambda_{n}(A) = \lambda_{\min}(A)$. For a matrix $A$, the vectorization operator $\VEC A$ maps $A$ to a column vector by stacking the columns of $A$. The kronecker product of $A$ with $B$ is denoted $A \otimes B$. Expectation and probability with respect to all the randomness of the underlying probability space are denoted $\E$ and $\bfP$, respectively. Conditional expectation and probability given the random variable $X$ are denoted by $\E[\cdot\vert X]$ and $\bfP[\cdot\vert X]$. For an event $\calG$, $\mathbf{1}_\calG$ denotes the indicator function for $\calG$.
% For an autonomous system $x_{t+1} = Ax_t$ 
For a matrix $A\in\R^{\dx \times \dx}$ and a symmetric matrix $Q \in \R^{\dx \times \dx }$, we denote the solution $P$ to the discrete Lyapunov equation, $A^\top P A - P + Q = 0$, by $\dlyap(A, Q)$.  If we also have $B \in \R^{\du \times \du}$ and   %controlled system $x_{t+1} = Ax_t + Bu_t$ 
 $R\in\R^{\du\times \du}$, $R\succ 0$, we denote the solution $P$ to the discrete algebraic Riccati equation $Q + A^\top P A - A^\top P B(B^\top P B + R)^{-1}B^\top P A = 0$ by $\dare(A, B, Q, R)$. We use the indexing shorthand $[K] := \curly{1,\dots,K}$.
\input{problem_formulation}

\subsection{Contributions}
%We provide a local minimax suboptimality lower bound which scales with interpretable system-thetoretic quantities. Specifically,  

    Our main contribution is the following theorem. For the formal statements, see \Cref{thm: finite data bound} and \Cref{cor: asymptotic lower bound}. 
\begin{theorem}[Main result, Informal]
\label{thm: main result informal}
    The $\e$-local minimax excess cost is lower bounded as
    \[
        \textrm{excess cost} \geq  \frac{\textrm{system-theoretic condition number}}{\textrm{\# data points} \times \textrm{signal-to-noise ratio}}.
    \]  
\end{theorem}

In the above bound, the system-theoretic condition number depends on familiar system-theoretic quantities such as the covariance of the state under the optimal controller, and the solution to the Riccati equation. The signal-to-noise ratio depends on how easily the system is excited via both the exploratory input, and the noise. This signal-to-noise ratio may be quantified in terms of the controllability gramian of the system, as well as the exploratory input budget.
%In the above bound, $N$ is the number of offline trajectories, $\Sigma_X(\theta)$ is the stationary covariance of the state under the optimal LQR controller, $P(\theta)$ is the solution to the corresponding Riccati equation, and $D_\theta \VEC K(\theta)$ is the derivative of the controller with respect to the underlying paremeters. As $P(\theta)$ is related to optimal objective value of the stochastic LQR problem\footnote{The optimal objective value for the stochastic LQR problem is $T \trace(P(\theta) \Sigma_W)$.}, its appearance suggests that the suboptimality bound is large when the corresponding control problem is difficult. Similar intuition holds for the appearance of the state covariance, $\Sigma_X(\theta)$. The derivative of the controller $D_{\theta} \VEC K(\theta)$ captures the fact that the gap will be largest when the sensitivity of the controller to the underlying parameter is high. 


% We lower-bound the $\e$-local minimax suboptimality gap : %\Bruce{removed eq}
% \begin{align*}
%     % \label{eq: minimax suboptimality}
%     \mathfrak{R}^{\mathsf{lin}}_T(\theta,\e) \geq \frac{c(\theta,\e)}{N},
% \end{align*}
% where $c(\theta, \e)$ is a constant that captures the difficulty of the problem in terms of familiar quantities such as the solution to the Riccati equation. 

%Note that $R_T^\pi (\theta)$ is the suboptimality of an entire length $T$ roll-out. Therefore, the above bound indicates the time averaged suboptimality is inversely proportional to the total amount of offline data. 

We also study several consequences of the above result by restricting attention to the setting where all system parameters are unknown, i.e. $\VEC \bmat{A(\theta) & B(\theta)} = \theta$.  In this setting, \Cref{thm: main result informal} may be reduced to  $\mathcal{EC}^{\mathsf{lin}}_T(\theta,\e) \geq \frac{c(\theta,\e)}{NT}$, where $c(\theta,\e)$ is easily interpretable. In particular, we may reach the following conclusions: \\
\vspace{-8pt}
\begin{blockquote}
    $\bullet$ For classes of system where the operator norm of system-theoretic matrices such as the controllability gramian and the solution to the Riccati equation are constant with respect to dimension, we may take
    $c(\theta, \e) \propto \du \dx.$
    \ifnum\value{cdc}>0{
    We demonstrate that this is the optimal dependence on system dimension for this class of 
    systems. 
    }\else{Combining results from \cite{mania2019certainty} and \cite{tu2022learning} demonstrates that when $\du \leq \dx$, the upper bound on the excess cost is also proportional to $\frac{\dx \du}{NT}$. In particular, our bound is optimal in the dimension for underactuated systems when the remaining system-theoretic quantities are constant with respect to dimension.
     }\fi
     \\
    %\item 
    $\bullet$ There exist classes of systems for which we may take $c(\theta, \e) \propto \exp(\dx)$. %4^{\dx}\Tasos{\mathrm{exp(\dx)}?}.$ 
    This demonstrates that the excess cost of a learned LQR controller may grow exponentially in the dimension. \\
    %\item 
    $\bullet$ 
    \ifnum\value{cdc}>0{We may take $c(\theta, \e)$ to grow with iterpretable system-theoretic quantities, such as the eigenvalues of both the solution to the Riccati equation, $P(\theta),$ and the state covariance under the optimal controller, $\Sigma_X(\theta)$.}
    \else{The lower bound grows in an interpretable manner with familiar system-theoretic quantities. In particular, we may take $c(\theta, \e)$ to grow with the eigenvalues of both the solution to the Riccati equation, $P(\theta),$ and the state covariance under the optimal controller, $\Sigma_X(\theta)$. }\fi 
    This suggests that the problem of learning to control a system with a small gap from the optimal controller is data intensive when controlling the underlying system is hard. %The constant $c(\theta,\e)$ also scales inversely with an upper bound on a measure of the ease of identification from the offline experiments. In particular, $c(\theta,\e)$ grows large when it becomes difficult to identify the system due to poor excitability by the noise and the exploratory input. 
    %\Tasos{intensive if objective is small sub-opt gap}
\end{blockquote}

\subsection{Related Work}

\begin{figure*}[t]
    \centering
    \vspace{-24pt}
    \includegraphics[width=0.8\textwidth]{figures/Figures.pdf}
    \vspace{-0.1in}
    \caption{A classic model-based pipeline for learning a controller from data. }%\Tasos{couldn't find a reference to the fig. in the text?}}%\Bruce{RVs Capital, fix spacing}}
    \label{fig: control from data pipeline}
    \ifnum\value{cdc}>0{\vspace{-.25in}}\else{\vspace{-0.15in}}\fi
\end{figure*}

\paragraph{System Identification} System identification is often a first step in designing a controller from experimental data, and has a longstanding history. The text \cite{ljung1998system} covers classic asymptotic results. Control oriented identification was studied in \cite{ chen1993caratheodory, helmicki1991control}.  Recently, there has been interest in finite sample analysis for fully-observed linear systems \citep{dean2019sample, simchowitz2018learning, faradonbeh2018finite, sarkar2019near}, and partially-observed linear systems \citep{oymak2019non, sarkar2021finite, simchowitz2018learning, tsiamis2019finite, lee2020non, zheng2020non}. Lower bounds for the sample complexity of system identification are presented in \cite{jedra2019sample, tsiamis2021linear}. For a more extensive discussion of prior work, we refer to  the survey by \cite{tsiamis2022statistical}.


\paragraph{Learning Controllers Offline} 
 %\Bruce{Make concise}
Learning a controller from offline data is a familiar paradigm for control theorist and practitioners. It  typically consists of system identification, followed by robust \citep{zhou1996robust} or certainty-equivalent \citep{simon1956dynamic} control design, see \Cref{fig: control from data pipeline}. %\Ingvar{probably should cite feldbaum or simon for CE}.  
Recent work provides finite sample guarantees for such methods \citep{dean2019sample, mania2019certainty}. Upper and lower bounds on the sample complexity of stabilization from offline data are presented in \cite{tsiamis2022learning}. The RL community has a similar paradigm, known as offline RL \citep{levine2020offline}. Policy gradient approaches are a model-free algorithm suitable for offline RL, and are  analyzed in \cite{fazel2018global}. Lower bounds on the variance of the gradient estimates in policy gradient approaches are supplied in \cite{ziemann2022policy}. Lower bounds for offline linear control are also studied in \cite{wagenmaker2021task} with the objective of designing optimal experiments. We instead focus on the LQR setting to understand the dependence of the excess cost on interpretable system-theoretic quantities. %This in turn provides an understanding of the statistical difficulty of learning controllers. 

% the best of our knowledge, lower bounds on the suboptimality of a LQR learned offline do not exist. The lower bounds presented in this work are valid for any model-based or model-free approach to learn the controller from offline data.  


\paragraph{Online LQR} The problem of learning the optimal LQR controller online has a rich history beginning with 
\cite{aastrom1973self}.  Regret minimization was introduced in \cite{lai1986asymptotically, lai1986extended}. The study of regret in online LQR was re-initiated by
\cite{abbasi2011regret}, inspired by works in the RL community. Many works followed to propose algorithms which were computationally tractable 
\citep{ouyang2017control,dean2018regret, abeille2018improved, mania2019certainty, cohen2019learning, faradonbeh2020input, jedra2021minimal}. Lower bounds on the regret of online  LQR are presented in
\cite{simchowitz2020naive, cassel2020logarithmic, ziemann2022regret}. The results in this paper follow a similar proof to \cite{ziemann2022regret}. The primary difference is that since our controller is designed via offline data, we may not make use of the exploration-exploitation tradeoff to upper bound the information available to the learner, as is done in \cite{ziemann2022regret}.
%Unlike these works, this paper provides lower bounds for \emph{offline LQR}. 

%\Ingvar{I think we probably will need to explain relationship to Ziemann Sandberg 2022 in a little more detail since the proof approach is kinda taken from our paper. I can get on this in a week or two tho}



