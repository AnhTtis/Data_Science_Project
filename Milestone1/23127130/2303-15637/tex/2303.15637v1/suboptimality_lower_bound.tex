\section{Excess Cost Lower Bound}
\label{s: suboptimality lower bounds}
% Before presenting our lower bounds, we  motivate our choice of the $\e$-local minimax suboptimality gap in \eqref{eq: minimax suboptimality} as the relevant quantity to study.   

We now proceed to establish our lower bound.  \ifnum\value{cdc}>0{Missing proofs and further details may be found in \cite{lee2023fundamental}. }\else{}\fi As we are interested in the worst-case excess cost from any element of $\calB(\theta,\e)$, we make the additional assumption that $F$ stabilizes $(A(\theta'), B(\theta'))$ for all $\theta'\in \calB(\theta,\e)$.\footnote{We ultimately study the limit as $\e$ becomes small. Therefore, this is not significantly stronger than assuming that $F$ stabilizes ($A(\theta), B(\theta)$).} %\Bruce{and that F stabilizes all such instances?} \Ingvar{Yeah and that $F$ is stabilizing in this nbhd. idk where we should state this though! probably further down closer to the corollary} \Bruce{I think it's actually necessary here with how I'm writing the lower bounds. Assumes that $A(\theta') + B(\theta') F$ is stable for all $\theta' in \calB(\theta,\e)$. }
This also ensures that the optimal LQR controller exists for all points in the prior.

%\Tasos{My suggestion: Start with the main result/main theorem (e.g. th 2.2 with cleaner notation), along with interpretation. Then explain the proof.}
To obtain a lower bound on the local minimax excess cost, we lower bound the maximization over $\theta' \in \calB(\theta,\e)$ by an average over a distribution supported on $\calB(\theta, \e)$. This reduces the problem to lower bounding a Bayesian complexity. Instead of fixing the parameter $\theta$, we let $\Theta$ be a random vector taking values in $\mathbb{R}^{d_\Theta}$ and suppose that it has prior density $\lambda$. Doing so enables the use of information theoretic tools to lower bound the complexity of estimating the parameter from data. The relaxation of the the maximization is shown in the following lemma. 

\begin{lemma} 
\label{lem: weak duality lb}
Fix $\e> 0$ and let $\lambda$ be any prior on  $\calB(\theta,\e)$. Then for any $\pi \in \Pi^\mathsf{lin}$ with $\pi(X_t, \calZ) = \hat K(\calZ)X_t$,  
%$\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)$ is lower bounded by %If the costs are stationary:
\ifnum\value{cdc}>0{
\begin{align*}
    %&\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)
    &\sup_{\theta' \in \calB(\theta)} \mathsf{EC}_T^\pi(\theta') \geq \\ 
    & \E  \tr  \paren{[\hat K(\mathcal{Z}) - K(\Theta)])^\T \Psi(\Theta) [\hat K(\mathcal{Z})-K(\Theta)] \Sigma_{\Theta}^{\hat K(\calZ)} }, 
\end{align*}
}\else{\begin{align*}
    %&\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)
    &\sup_{\theta' \in \calB(\theta)} \mathsf{EC}_T^\pi(\theta')  \geq \E  \tr  \paren{[\hat K(\mathcal{Z}) - K(\Theta)])^\T \Psi(\Theta) [\hat K(\mathcal{Z})-K(\Theta)] \Sigma_{\Theta}^{\hat K(\calZ)} }, 
\end{align*}}\fi
where $\Sigma_{\Theta}^{\hat K(\calZ)} := \frac{1}{T} \sum_{t=0}^{T-1} \E^{\pi} \brac{X_tX_t^\T \vert \calZ, \Theta}$. The expectation is over the prior $\Theta\sim \lambda$, and the randomness of both the offline rollouts and the evaluation rollout. We recall the shorthand $\Psi(\Theta) = B(\Theta)^\top P(\Theta) B(\Theta) +R$. 
%We recall the shorthand $\Psi(\Theta) = B^\T(\Theta) P(\Theta)B(\Theta)+R$.
\end{lemma}
\ifnum\value{cdc}>0{}\else{
\begin{proof}
By the quadratic expression for the excess cost in \eqref{eq: suboptality quadratic form} and the fact that the supremum over a set always exceeds the weighted average over a set, 
we have the following inequality:
% \ifnum\value{cdc}>0{
\begin{align*}
    %&\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)
    &  \sup_{\|\theta'-\theta\|\leq \e} \mathsf{EC}_T^\pi(\theta') \geq \underset{\Theta\sim\lambda}{\E} R_T^{\pi}(\Theta) \\
    &\!\!=\! %\inf_{\pi \in \Pi_{\mathsf{lin}}} 
    \frac{1}{T}\E \sum_{t=0}^{T-1} \E^{\pi} \brac{\norm{U_t \!-\! K(\Theta)X_t}_{\Psi(\Theta)}^2 \vert \calZ, \Theta} \\ 
    &\!\!=\! %\inf_{\hat K}
    \E\tr ([\hat K(\mathcal{Z})\!-\!K(\Theta)])^\T 
      \Psi(\Theta)([\hat K(\mathcal{Z})-K(\Theta)] \Sigma_{\Theta}^{\hat K\!(\!\calZ\!)} ).
   % &\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e) = \inf_{\pi \in \Pi_{\mathsf{lin}}} \sup_{\|\theta'-\theta\|\leq \e} \mathsf{EC}_T^\pi(\theta') \geq \inf_{\pi \in \Pi_{\mathsf{lin}}} \underset{\Theta\sim\lambda}{\E} R_T^{\pi}(\Theta) \\
   %  &= \inf_{\pi \in \Pi_{\mathsf{lin}}} \E \sum_{t=0}^{T-1} \E_{\calZ, \Theta}^{\pi} (U_t- K(\Theta)X_t )^\T \\ &\qquad \cdot\Psi(\Theta) (U_t-K(\Theta)X_t )\\ 
   %  &= \inf_{\hat K} \E\tr  ([\hat K(\mathcal{Z})-K(\Theta)])^\T 
   %   \\ &\qquad\cdot \Psi(\Theta)([\hat K(\mathcal{Z})-K(\Theta)] T \Sigma_{\Theta}^{\hat K(\calZ)} ).
\end{align*}
% }\else{
% \begin{align*}
%    &\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e) = \inf_{\pi \in \Pi_{\mathsf{lin}}} \sup_{\|\theta'-\theta\|\leq \e} \mathsf{EC}_T^\pi(\theta') \geq \inf_{\pi \in \Pi_{\mathsf{lin}}} \underset{\Theta\sim\lambda}{\E} R_T^{\pi}(\Theta) \\
%     &= \inf_{\pi \in \Pi_{\mathsf{lin}}} \E \sum_{t=0}^{T-1} \E_{\calZ, \Theta}^{\pi} (U_t- K(\Theta)X_t )^\T \Psi(\Theta) (U_t-K(\Theta)X_t )\\ 
%     &= \inf_{\hat K} \E\tr  ([\hat K(\mathcal{Z})-K(\Theta)])^\T 
%     \Psi(\Theta) ([\hat K(\mathcal{Z})-K(\Theta)] T \Sigma_{\Theta}^{\hat K(\calZ)} ).
% \end{align*}}\fi
The second to last equality follows by the tower rule. 
The last equality results by substituting $U_t=\hat K(\calZ) X_t$, followed by the trace-cyclic property and linearity of expectation.
\end{proof}
}\fi

We may treat the data from offline experimentation, $\calZ$, as an observation of the underlying parameter $\Theta$. In particular, $\calZ$ may be expressed as a random vector taking values in $\R^{NT(\dx+\du)}$ with conditional density $p(\cdot|\theta)$. The following Fisher information matrix and prior density concentration matrix measure estimation performance of $\Theta$ from the sample $\calZ$ with respect to the square loss:
\begin{align}
\label{eq:fisherdef}
\I_p(\theta) &:=\int  \left( \frac{\nabla_\theta p(z|\theta)}{p(z|\theta)}\right)\left( \frac{\nabla_\theta p(z|\theta)}{p(z|\theta)}\right)^\T p(z|\theta) dz, \\%\textnormal{ and} \\
\label{eq:fisherlocdef}
\J(\lambda)&:= \int \left( \frac{\nabla_\theta \lambda(\theta)}{\lambda(\theta)}\right) \left( \frac{\nabla_\theta \lambda(\theta)}{\lambda(\theta)}\right)^\T  \lambda(\theta)  d\theta.
\end{align}
The first quantity \eqref{eq:fisherdef} measures the information content of the sample $\calZ$ with regards to $\Theta$. The second quantity \eqref{eq:fisherlocdef} measures the concentration of the prior density $\lambda$. As the gradient operator $\nabla_\theta$ maps to a vector of dimension $d_\Theta$, both $I_p(\theta)$ and $J(\lambda)$ are $d_\Theta\times d_\Theta$ dimensional. See \cite{ibragimov2013statistical} for further details about these integrals and their existence.

As we seek lower bounds for estimating $K(\Theta)$ instead of just $\Theta$, we must account for the transformation from a quadratic loss over the error in esimating $\Theta$ to the error in estimating $K(\Theta)$, as appears in \Cref{lem: weak duality lb}. To do so, we introduce the Van Trees' inequality \citep{van2004detection, bobrovsky1987some}. We first impose the following standard regularity conditions:
\begin{assumption} \,
\label{asm: van trees reguarlity} 
\begin{enumerate}
\item The prior $\lambda $ is smooth with compact support. 
\item The conditional density of $\calZ$ given $\Theta$, $p(z|\cdot)$, is continuously differentiable on the domain of $\lambda$ for almost every $z$.
\item The score\footnote{The score is the gradient of the log-likelihood. It evaluates to $\frac{\nabla_\theta p(z|\theta)}{p(z|\theta)}$.} has mean zero; $ \int \left( \frac{\nabla_\theta p(z|\theta)}{p(z|\theta)}\right) p(z|\theta) dz =0$.
\item $\J(\lambda)$ is finite and $\I_p(\theta)$ is a continuous function of $\theta$ on the domain of $\lambda$.
\item $\VEC K$ is differentiable on the domain of $\lambda$.
\end{enumerate}
\end{assumption}


The following theorem is a less general adaption from \cite{bobrovsky1987some}  which suffices for our needs. 
\begin{theorem}[Van Trees Inequality]
\label{thm:vtineq}
Fix two random variables $(\calZ,\Theta) \sim   p(\cdot|\cdot) \lambda(\cdot)$  and suppose Assumption~\ref{asm: van trees reguarlity} holds. Let $\mathcal{G}$ be a $\sigma(\calZ)$-measurable event. Then for any $\sigma(\calZ)$-measurable $\hat K$:
\ifnum\value{cdc}>0{
\begin{equation}
\label{VTineq}
\begin{aligned}
&\E \left[ \VEC (\hat K(\calZ) -K(\Theta)) \VEC (\hat K (\calZ)-K(\Theta))^\T \mathbf{1}_\mathcal{G}\right] \succeq \\
&  \E[ \dop_\theta \VEC K(\Theta)\mathbf{1}_\mathcal{G}]^\T \!\left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \! \E [\dop_\theta \VEC K(\Theta)\mathbf{1}_\mathcal{G}].
\end{aligned}
\end{equation}
}\else{
\begin{equation}
\label{VTineq}
\begin{aligned}
&\E \left[ \VEC (\hat K(\calZ) -K(\Theta)) \VEC (\hat K (\calZ)-K(\Theta))^\T \mathbf{1}_\mathcal{G}\right] \\
&\succeq  \E[ \dop_\theta \VEC K(\Theta)\mathbf{1}_\mathcal{G}]^\T \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [\dop_\theta \VEC K(\Theta)\mathbf{1}_\mathcal{G}].
\end{aligned}
\end{equation}}\fi
The notation $\dop_\theta \VEC K(\cdot)$ above follows the standard convention for a Jacobian: it stacks the transposed gradients of each element of $\VEC K(\cdot)$ into a $\dx \du \times d_\Theta$ dimensional matrix. 
\end{theorem}

We see from \Cref{thm:vtineq}  
that the transformation to the error in estimating $K(\Theta)$ is accounted for by $\dop_\theta \VEC K(\cdot)$. 

We now massage the lower bound in \Cref{lem: weak duality lb} to a form compatible with \Cref{thm:vtineq}. Doing so requires us to express the lower bound as a quadratic form conditioned on some $\sigma(\calZ)$-measureable event $\calG$. We therefore select an event $\calG$ for which we may uniformly lower bound the quantities $\Psi(\Theta)$ and $\Sigma_{\Theta}^{\hat K(\calZ)}$. To this end, we define positive definite matrices $\Psi_{\theta,\e}$ and $\Sigma_{\theta,\e}$ that satisfy 
\begin{align}
    \label{eq: uniform lower bound on N and Gamma} 
    \Psi(\theta') &\succeq \Psi_{\theta,\e}  \textrm{ and }  \frac{1}{2} \Sigma_X(\theta')
    \succeq \Sigma_{\theta,\e} \quad \forall \theta' \in\calB(\theta,\e).
\end{align}
The matrix $\Psi_{\theta,\e}$ will serve to uniformly lower bound $\Psi(\Theta)$. When the learned controller is close to the optimal controller, the covariance of the state under the learned controller will be close to the covariance of the state under the optimal controller, which is in turn lower bounded in terms of $\Sigma_{\theta, \e}$. In particular, if $\norm{\hat K(\calZ) - K(\Theta)}$ is sufficiently small, we can argue that $\Sigma_{\Theta}^{\hat K(\calZ)} \succeq \frac{1}{2}\Sigma_X(\Theta) \succeq \Sigma_{\theta,\e}$. The aforementioned condition on $\norm{\hat K(\calZ) - K(\Theta)}$ will hold only if there is a large amount of data available to fit $\hat K(\calZ)$. To achieve a bound that holds in the low data regime, we observe that the state covariance under the learned controller is always lower bounded by the noise covariance: $\Sigma_\Theta^\calZ \succeq \Sigma_W$.  
%\Tasos{it took me quite some time to understand the definition of Sigma ``when we are near optimal"}.
For this reason, the subsequent results will be presented in two parts: one in which we condition on an event where $\norm{\hat K(\calZ) - K(\theta)}$ is small, and one that holds generally. To present these results concisely, the positive definite matrix $\Gamma_{\theta,\e}$ is used to denote either $\Sigma_W$ or $\Sigma_{\theta,\e}$. The Kronecker product of these lower bounds arises frequently, motivating the shorthand 
\begin{align}
    \label{eq: kronecker shorthand}
    \Xi_{\theta, \e} := \Gamma_{\theta, \e} \otimes \Psi_{\theta,\e}.
\end{align}

%\Bruce{Express the following results with some condensed notation, and modify the proofs accordingly. State the assumptions on the prior.}
\begin{lemma}[Application of Van Trees' Inequality]
\label{lem: application of van trees}
For any smooth prior $\lambda$ on $\calB(\theta,\e)$ and any $\pi \in \Pi^\mathsf{lin}$ with $\pi(X_t, \calZ) = \hat K(\calZ) X_t$,  
% $\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)$ is lower bounded by
\ifnum\value{cdc}>0{
\begin{equation}
    \begin{aligned}
    \label{eq: unified LB}
       %&\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e) 
       &\sup_{\theta'\in\calB(\theta,\e)}\mathsf{EC}_T^\pi(\theta') \geq \\ & \frac{\tr\left(   \Xi_{\theta,\e}  \E [\dop_\theta \VEC  K(\Theta) \mathbf{1}_{\calG}] \E [\dop_\theta  \VEC    K(\Theta) \mathbf{1}_{\calG}]^\T \right) }{ \norm{\E \I_p(\Theta)+\J(\lambda)}},
       \end{aligned}
\end{equation}
}\else{
\begin{align}  
    \label{eq: unified LB}
    \sup_{\theta'\in\calB(\theta,\e)}\mathsf{EC}_T^\pi(\theta') \geq \frac{\tr\left(   \Xi_{\theta,\e}  \E [\dop_\theta \VEC  K(\Theta) \mathbf{1}_{\calG}] \E [\dop_\theta  \VEC    K(\Theta) \mathbf{1}_{\calG}]^\T \right) }{ \norm{\E \I_p(\Theta)+\J(\lambda)}},
\end{align}}\fi
where either:
\begin{blockquote}
    $1)$ $\Gamma_{\theta, \e} = \Sigma_W$ and $\calG = \Omega$, \textrm{ or }\\
    $2)$ $\Gamma_{\theta, \e} = \Sigma_{\theta, \e}$ and $\calG = \calE$, if $\displaystyle T \geq \sup_{\theta'\in \calB(\theta,\e)}\frac{16 \norm{\Sigma_X(\theta')}}{\lambda_{\min}(\Sigma_X(\theta'))}$. 
\end{blockquote} 
\noindent The event $\Omega$ is the entire sample space, i.e. $\P\brac{\Omega} = 1$, and  
\ifnum\value{cdc}>0{
\begin{align*}
    \mathcal{E} &= \curly{\sup_{\theta' \in \calB(\theta,\e)} \norm{\hat K(\calZ) - K(\theta')} \leq  \alpha } \\
    \alpha &= \inf_{\theta' \in \calB(\theta,\e)} \min\bigg\{\frac{\norm{A_{cl}(\theta')}}{\norm{B(\theta')}}, \\
    &\qquad \frac{\lambda_{\min}(\Sigma_X(\theta'))/24}{\norm{A_{cl}(\theta')} \norm{B(\theta')} \mathcal{J}(A_{cl}(\theta')) \norm{\Sigma_X(\theta')}}\bigg\}.
\end{align*}
}\else{
\begin{align*}
    \mathcal{E} &= \curly{\sup_{\theta' \in \calB(\theta,\e)} \norm{\hat K(\calZ) - K(\theta')} \leq  \alpha }  \\
    \alpha &= \inf_{\theta' \in \calB(\theta,\e)} \min\curly{\frac{\norm{A_{cl}(\theta')}}{\norm{B(\theta')}}, \frac{\lambda_{\min}(\Sigma_X(\theta'))/24}{\norm{A_{cl}(\theta')} \norm{B(\theta')} \mathcal{J}(A_{cl}(\theta')) \norm{\Sigma_X(\theta')}}}.
\end{align*}
}\fi
 %\textrm{ and } \sup_{\theta' \in \calB(\theta,\e)} \rho(A(\theta') + B(\theta') \hat K(\calZ)) < 1}$, 
%and \[\.
%\]
Here, $A_{cl}(\theta) = A(\theta) + B(\theta)K(\theta)$ and $\calJ(A_{cl}(\theta)) = \sum_{t=0}^\infty \norm{A_{cl}(\theta)^t}^2$.

% Then 
% \begin{align}  
%     \label{eq: general LB}
%      \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)  \geq    T\tr\left(   (\Sigma_W \otimes \Psi_{\theta,\e} )  \E [\dop_\theta \VEC  K(\Theta)] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [\dop_\theta  \VEC    K(\Theta)]^\T \right).
% \end{align}
% If we also have that $T \geq \frac{16\norm{\Gamma_0}^2}{\lambda_{\min}(\Gamma_0)}$ then 
% \begin{align}
%     \label{eq: conditional LB}
%     \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)  \geq    \tr\left(   (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \E [\dop_\theta \VEC  K(\Theta) \mathbf{1}_\calE] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [\dop_\theta  \VEC    K(\Theta)\mathbf{1}_\calE]^\T \right),
% \end{align}
% where $\mathcal{E} = \curly{\sup_{\theta' \in \calB(\theta,\e)} \norm{\hat K(\calZ) - K(\theta')} \leq  \alpha }$, %\textrm{ and } \sup_{\theta' \in \calB(\theta,\e)} \rho(A(\theta') + B(\theta') \hat K(\calZ)) < 1}$, 
% and \[\alpha = \inf_{\theta' \in \calB(\theta,\e)} \min\curly{\frac{\norm{A_{cl}(\theta')}}{\norm{B(\theta')}}, \frac{\lambda_{\min}(\Gamma_0(\theta'))}{24 \norm{A_{cl}(\theta')} \norm{B(\theta')} \mathcal{J}(A_{cl}(\theta'))^2 \norm{\Gamma_0(\theta')}}}.
% \]
% Here, $A_{cl}(\theta) = A(\theta) + B(\theta)K(\theta)$.
%$\calE = \curly{R_T^{\pi(\calZ)}(\Theta) \leq \sqrt\frac{T}{N}}$.
\end{lemma}
%The two possibilities in the above bound arise from the fact that it is always possible to lower bound the state covariance under the learned controller, $\Sigma_\Theta^\calZ$ by the noise covariance, $\Sigma_W$. The drawback in doing so is that it discards additional, relevant quantities in the lower bound. To avoid this, the second possible bound in the above lemma bounds $\Sigma_\Theta^\calZ$ in terms of the optimal stationary covariance, $\Sigma_X(\Theta)$. However, this is only possible in the regime where the learned controller is very close to the optimal controller.
% \Bruce{Comment for Ingvar: For case 2, when we lower bound $\Sigma_\Theta^{\hat K(\calZ)}$ by $\Sigma_{\theta, \e} \mathbf{1}_{\calG}$, the event in the indicator depends on $\hat K$ (see event $\calE$ above.) Van trees may then be used to lower bound the quadratic form by
% \[
%     \frac{\tr\left(   \Xi_{\theta,\e}  \E [\dop_\theta \VEC  K(\Theta) \mathbf{1}_{\calG}] \E [\dop_\theta  \VEC    K(\Theta) \mathbf{1}_{\calG}]^\T \right) }{ \norm{\E \I_p(\Theta)+\J(\lambda)}},
% \]
% which retains dependence on $\hat K$ through $\calG$. Therefore, when we take an inf over all $\hat K$, we must retain the $\inf$ on the lower bound. There was a question of whether this caused an issue for van trees, but as it's written here, it does not seem to be a problem. 

% % The next issue is that there is still an $\inf$ over $\hat K$ in the lower bound. To handle this, we argue that if $$
% }

%\Bruce{Discuss the two possible lower bounds available in the above lemma}

% \Ingvar{this is not a problem. basically  van trees retains the event indicator in the RHS which is taken wrt THE ACTUAL CONTROLLER (i.e. the RHS retains dependency on the actual controller). you can then proceed to take the inf over all controllers that satisfy a certain condition COND. CAVEAT: you need to start and assume that the actual controller also satisfies COND (but this is probably just a logical formality)}

\ifnum\value{cdc}>0{
The proof of the above result follows by applying \Cref{thm:vtineq} to the lower bound in \Cref{lem: weak duality lb} after replacing $\Psi(\Theta)$ by $\Psi_{\theta,\e}$ and $\Sigma_\Theta^{\hat K(\calZ)}$ by $\Gamma_{\theta,\e} \mathbf{1}_\calG$. 
}\else{
\begin{proof}
We always have that $\Sigma_{\hat K(\calZ)} \succeq \Sigma_W$. 
\ifnum\value{cdc}>0{Lemma A.2 of \cite{lee2023fundamental} 
}\else{\Cref{lem: covariance overlap between learned and optimal controller} }\fi  shows that if $T \geq \frac{16 \norm{\Sigma_X(\Theta)}^2}{\lambda_{\min}(\Sigma_X(\Theta))}$, then under event $\calE$,  we have $\norm{\Sigma_X(\Theta)^{-1/2}\Sigma_{\Theta}^{\hat K(\calZ)}\Sigma_X(\Theta)^{-1/2} - I} \leq \frac{1}{2}$. This in turn implies that $\Sigma_{\Theta}^{\hat K(\calZ)} \succeq \Sigma_{\theta, \e} \mathbf{1}_\calE$. 

%When the rollouts are sufficiently long such that $T \geq \frac{16 \norm{\Gamma_0}^2}{\lambda_{\min}(\Gamma_0)}$, then under event $\calE$, we can show $\Gamma_{\hat K(\calZ)}(\Theta) \succeq \frac{1}{2} \Gamma_0(\Theta)$. 
% Observe that
% \begin{align*}
%     & \inf_{\hat K} \E_{\Theta\sim \lambda} \E_{\calZ|\Theta} \tr  ([\hat K(\mathcal{Z})-K(\Theta)])^\T \Psi(\Theta) ([\hat K(\mathcal{Z})-K(\Theta)] T \Gamma_{\hat K(\calZ)}(\Theta) ) \\
%     %&= \inf_{\hat K} \E_{\Theta\sim \lambda} \E_{\calZ|\Theta} \tr  ([\hat K(\mathcal{Z})-K(\Theta)])^\T (B^\T(\Theta) P(\Theta)B(\Theta)+R) ([\hat K(\mathcal{Z})-K(\Theta)] T \Gamma_0 \Gamma_0^{-1}\Gamma_{\hat K(\calZ)}(\Theta) ) \\
%     &\geq \inf_{\hat K} \E_{\Theta\sim \lambda} \E_{\calZ|\Theta} \tr  ([\hat K(\mathcal{Z})-K(\Theta)])^\T \Psi(\Theta)  ([\hat K(\mathcal{Z})-K(\Theta)] T \Gamma_0) \sigma_{\min}(\Gamma_0^{-1}\Gamma_{\hat K(\calZ)}(\Theta)) 
% \end{align*}
%In particular \Cref{lem: covariance overlap between learned and optimal controller} shows that if $T \geq \frac{16 \norm{\Gamma_0}^2}{\lambda_{\min}(\Gamma_0)}$, then under event $\calE$,  we have $\norm{\Gamma_0^{-1/2}\Gamma_{\hat K(\calZ)}(\Theta)\Gamma_0^{-1/2} - I} \leq \frac{1}{2}$. 
% This in turn implies that $\sigma_{\min}(\Gamma_0^{-1}\Gamma_{\hat K(\calZ)}(\Theta)) \geq \frac{1}{2}$. Alternatively, we can always replace $\Gamma_{\hat K(\calZ)}$ by $\Sigma_W$ and achieve a valid lower bound. 

With this fact in hand, we may replace $\Psi(\Theta)$ in the lower bound from \Cref{lem: weak duality lb} by $\Psi_{\theta,\e}$, and  $\Sigma_{\Theta}^{\hat K(\calZ)}$ by $\Gamma_{\theta,\e} \mathbf{1}_\calG$, where $(\Gamma_{\theta,\e}, \calG)$ can only be set as $(\Sigma_{\theta,\e}, \calE)$ if $T$ is sufficiently large. 
% \ifnum\value{cdc}>0{
% Doing so, we may lower bound $\sup_{\theta' \in \calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta')$ by
% \begin{align*} 
%      % &  \geq   T \inf_{\hat K} \E %\sum_{t=0}^{T-1}
%      % \tr  ([\hat K(\calZ)-K(\Theta)])^\T  \Psi_{\theta,\e} ([\hat K(\calZ)-K(\Theta)]  \Gamma_{\theta,\e}) \mathbf{1}_\calG  \\ %\underset{\Theta\sim\lambda}{\E_{\Theta}}
%      % &=T \inf_{\hat K} \E %\sum_{t=0}^{T-1} 
%      % \tr  ([\hat K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}])^\T  [\hat K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] )\mathbf{1}_\calG\\ %\underset{\Theta\sim\lambda}{\E_{\Theta}}
%      & \E \tr \big(  [\VEC \tilde K(\calZ)-   \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] \\&\qquad \cdot [\VEC \tilde K(\calZ)-   \VEC \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] \big)^\T \mathbf{1}_\calG, %\underset{\Theta\sim\lambda}{\E_{\Theta}}
% \end{align*}
% where $\tilde K(\calZ) = \sqrt{\Psi_{\theta,\e}} \hat K (\calZ) \sqrt{\Gamma_{\theta,\e}}$,
% We now invoke the Van Trees' inequality, \Cref{thm:vtineq}, along with the fact that $\VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}} = (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} ) \VEC  K(\Theta)$ to lower bound $\sup_{\theta'\in\calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta')$ 
% as in the lemma statement.  
% % \begin{align*}
% %     &\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e) \geq 
% %       \tr\bigg(\!\E[  (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} )  \dop_\theta \VEC  K(\Theta) \mathbf{1}_\calG] \\& \!\cdot \!\left[ \E \I_p(\Theta)\!+\!\J(\lambda) \right]^{-1}\!  \E [ (\sqrt{\Gamma_{\theta,\e}} \otimes \! \sqrt{\Psi_{\theta,\e}} ) \!\dop_\theta  \VEC    K(\Theta) \mathbf{1}_\calG]^\T\!\bigg).
% % \end{align*}
% The steps are worked out in further detail in \cite{lee2023fundamental}. 
% }\else {
Then 
\begin{align*} 
     &\sup_{\theta'\in\calB(\theta,\e)}\mathsf{EC}_T^\pi(\theta')  \geq    \E %\sum_{t=0}^{T-1}
     \tr  ([\hat K(\calZ)-K(\Theta)])^\T  \Psi_{\theta,\e} ([\hat K(\calZ)-K(\Theta)]  \Gamma_{\theta,\e}) \mathbf{1}_\calG  \\ 
     &= \E %\sum_{t=0}^{T-1} 
     \tr  ([\tilde K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}])^\T  [\tilde K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] )\mathbf{1}_\calG\\ %\underset{\Theta\sim\lambda}{\E_{\Theta}}
     &=  \E \tr \big(  [\VEC \tilde K(\calZ)-   \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] \cdot [\VEC \tilde K(\calZ)-   \VEC \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] \big)^\T \mathbf{1}_\calG, %\underset{\Theta\sim\lambda}{\E_{\Theta}}
\end{align*}
where $\tilde K(\calZ) = \sqrt{\Psi_{\theta,\e}} \hat K(\calZ) \sqrt{\Gamma_{\theta,\e}}$.
% }\fi
We now invoke the Van Trees' inequality, \Cref{thm:vtineq}:
\begin{align*}
    &\sup_{\theta'\in\calB(\theta,\e)}\mathsf{EC}_T^\pi(\theta')
    \!\geq \!
      \trace\paren{ \!\E[ \dop_\theta  \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}} \mathbf{1}_\calG] \left[ \E \I_p(\Theta)\!+\!\J(\lambda) \right]^{-1}\! \E [\dop_\theta  \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}} \mathbf{1}_\calG]^\T \!}\\
     & \!= \!   \tr \paren{ \!\E[  (\sqrt{\Gamma_{\theta,\e}}\! \otimes\! \sqrt{\Psi_{\theta,\e}} )  \dop_\theta \VEC  K(\Theta) \mathbf{1}_\calG] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [ (\sqrt{\Gamma_{\theta,\e}} \! \otimes \! \sqrt{\Psi_{\theta,\e}} ) \dop_\theta  \VEC    K(\Theta) \mathbf{1}_\calG]^\T \!},
\end{align*}
where we  used that $\VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}} = (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} ) \VEC  K(\Theta)$ in the last line.

We conclude by applying the trace cyclic property, and extracting the minimum eigenvalue of $[\E_{\Theta}I_p(\theta) + J(\lambda)]^{-1}$.

\end{proof}
}\fi
% \begin{align*}
%     %& \E \brac{\tr ( [\VEC K-   \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}])  [\VEC K-   \VEC \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] )^\T \mathbf{1}_\calE} \\
%     %&\geq \tr\left( \E[  (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} )  \dop_\theta \VEC  K(\Theta) \mathbf{1}_\calE] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [ (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} ) \dop_\theta  \VEC    K(\Theta)\mathbf{1}_\calE]^\T \right)\\
%     \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)&\geq T \frac{\tr\left( \E[  (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \dop_\theta \VEC  K(\Theta) \mathbf{1}_\calG] \E [\dop_\theta  \VEC    K(\Theta) \mathbf{1}_\calG]^\T \right)}{\norm{\E \I_p(\Theta)+\J(\lambda) }}.
% \end{align*}
% \begin{align*}
%      \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)  &\geq  \inf_{\hat K} \E_{\Theta\sim \lambda} \E_{\calZ|\Theta}%\sum_{t=0}^{T-1}
%      \tr  ([\hat K(\calZ)-K(\Theta)])^\T  \Psi_{\theta,\e} ([\hat K(\calZ)-K(\Theta)]  \Gamma_{\theta,\e}) \mathbf{1}_\calE  \\
%      &=\inf_{\hat K} \E_{\Theta\sim \lambda} \E_{\calZ|\Theta}%\sum_{t=0}^{T-1} 
%      \tr  ([\hat K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}])^\T  [\hat K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] )\mathbf{1}_\calE\\
%      %&=\inf_{\hat K} \E_{\Theta\sim \lambda} \E_{\calZ|\Theta}%\sum_{t=0}^{T-1}  
%      %(\VEC [\hat K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}])^\T  \VEC [\hat K(\calZ)-   \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] )\mathbf{1}_\calE\\
%      &= \inf_{\hat K}   \E_{\Theta\sim \lambda} \E_{\calZ|\Theta} \tr  ( [\VEC \hat K(\calZ)-   \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}])  [\VEC \hat K(\calZ)-   \VEC \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] )^\T \mathbf{1}_\calE,
% \end{align*}
% where the first equality follows by a change of variables for $\hat K(\calZ)$. 
% We now invoke the Van Trees' inequality, \Cref{thm:vtineq}:
% \begin{align*}
%     \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)&\geq
%      \E[ \dop_\theta  \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}} \mathbf{1}_\calE] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [\dop_\theta  \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}} \mathbf{1}_\calE]^\T\\
%      &= \E[  (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} )  \dop_\theta \VEC  K(\Theta) \mathbf{1}_\calE] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [ (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} ) \dop_\theta  \VEC    K(\Theta) \mathbf{1}_\calE]^\T
% \end{align*}
% where we  used that $\VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}} = (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} ) \VEC  K(\Theta)$ in the last line.

% Applying the trace cyclic property, we conclude
% \begin{align*}
%     %& \E \brac{\tr ( [\VEC K-   \VEC\sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}])  [\VEC K-   \VEC \sqrt{\Psi_{\theta,\e}} K(\Theta)\sqrt{\Gamma_{\theta,\e}}] )^\T \mathbf{1}_\calE} \\
%     %&\geq \tr\left( \E[  (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} )  \dop_\theta \VEC  K(\Theta) \mathbf{1}_\calE] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [ (\sqrt{\Gamma_{\theta,\e}} \otimes \sqrt{\Psi_{\theta,\e}} ) \dop_\theta  \VEC    K(\Theta)\mathbf{1}_\calE]^\T \right)\\
%     \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)&\geq \tr\left( \E[  (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \dop_\theta \VEC  K(\Theta) \mathbf{1}_\calE] \left[ \E \I_p(\Theta)+\J(\lambda) \right]^{-1} \E [\dop_\theta  \VEC    K(\Theta) \mathbf{1}_\calE]^\T \right).
% \end{align*}

%We can always lower bound $\Gamma_{\hat K(\calZ)} \succeq \Sigma_W$, which result in the inequality in \eqref{eq: general LB} by following the above derivation with $\Gamma_{\theta, \e} \mathbf{1}_{\calE}$  replaced by $T\Sigma_W$.


% \Bruce{Even more emphasis here on distinguishing between two nearby instances. }

% \Cref{lem: application of van trees} demonstrates the dependence of the local minimax suboptimality gap upon the derivative of the controller with respect to the underlying parameter, and the fisher information. Both of these quantities illustrate the fact that the hardness of learning a LQR from a statistical point of view is dictated by infinitesimal perturbations of the underlying parameter. In particular, the  derivative of the controller with respect to the parameter illustrates the fact that the lower bound grows when infinitesimal perturbations of the underlying parameter cause a relatively large change in the controller gain. The appearance of the Fisher information, on the other hand, captures the extend to which the sample $\calZ$ can cause misidetification between nearby realizations of the parameter $\Theta$. 

% The lower bound in \Cref{lem: application of van trees} may be interpreted using the intuition that 
\Cref{lem: application of van trees} may be interpreted according to the following intuition. To design a controller that attains low cost, it is essential to distinguish between two nearby instances of the underlying parameter, $\theta$ and $\theta'$, from the experimental data, $\calZ$. The Fisher Information term on the denominator of the bound in \Cref{lem: application of van trees} captures the ease with which we can distinguish between $\theta$ and an infinitesimally perturbed $\theta'$ from the collected data $\calZ$, and can be thought of as a signal-to-noise ratio. The derivative of the controller appearing on the numerator of the bound in \Cref{lem: application of van trees} is a change of variables term that accounts for the extent to which infinitesimal perturbations of the underlying parameter impact the optimal controller gain.  Sensitive perturbations are those which are difficult to detect from the collected data, yet lead to a large change in the controller gain. % The appearance of these quantities together illustrates the fact that the hardness of learning a LQR from a statistical point of view is dictated by infinitesimal perturbations of the underlying parameter.
Such perturbations dictate the statistical hardness of learning a LQR controller.
Motivated by this fact, we can select particularly sensitive perturbation directions of the underlying parameter which emphasize the hardness of the problem. To do so, we restrict the support of the prior $\lambda$ to a lower dimensional subspace. 
Before presenting this result, it will be useful to see the expression for Fisher information matrix from this experimental setup. It can be shown via the chain rule of Fisher Information that 
\ifnum\value{cdc}>0{
\begin{equation}
\begin{aligned}
I_p(\theta) &= \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \dop_\theta \VEC \bmat{A(\theta) &  B(\theta)}^\top \\ &\qquad \cdot [Z_{t,n} Z_{t,n}^\top \otimes \Sigma_W^{-1} ] \dop_\theta \VEC \bmat{A(\theta) & B(\theta)}, 
\end{aligned}
\end{equation}
}\else{
\begin{align}
I_p(\theta) = \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \dop_\theta \VEC \bmat{A(\theta) &  B(\theta)}^\top [Z_{t,n} Z_{t,n}^\top \otimes \Sigma_W^{-1} ] \dop_\theta \VEC \bmat{A(\theta) & B(\theta)}, 
\end{align}}\fi
where $Z_{t,n} = \bmat{X_{t,n} \\ U_{t,n}}$. See, for instance, Lemma 3.1 of \cite{ziemann2022regret}. With this in hand, the following Lemma provides a restriction to lower dimensional priors, which allows us to understand how poor conditioning of the information matrix along any particular parameter perturbation direction pushes through to a challenge in estimating the optimal controller.
%\Bruce{State this in the same form as above, but first state the above in a way that makes sense...}
\begin{lemma}
    \label{lem: coordinate transformation}
    Consider any matrix $V \in \R^{d_\Theta \times k}$ with $k \leq d_{\Theta}$ which has orthonormal columns. For any smooth prior $\lambda$ over $\curly{\theta + V \tilde \theta: \norm{\tilde \theta}\leq \e}$, and any $\pi \in \Pi^\mathsf{lin}$ with $\pi(X_t, \calZ) = \hat K(\calZ) X_t$,
    \ifnum\value{cdc}>0{
    \begin{align*}
        &\sup_{\theta'\in\calB(\theta,\e)}\mathsf{EC}_T^\pi(\theta')\geq \\& \frac{\tr\left(\Xi_{\theta,\e}  \E[\dop_\theta \VEC  K(\Theta) V \mathbf{1}_\calG] \E [\dop_\theta  \VEC K(\Theta) V 
         \mathbf{1}_\calG ]^\T \right)}{ \norm{V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V} },
    \end{align*}
    }
    \else{
    \begin{align*}
        \sup_{\theta'\in\calB(\theta,\e)}\mathsf{EC}_T^\pi(\theta')\geq\frac{\tr\left(\Xi_{\theta,\e}  \E[\dop_\theta \VEC  K(\Theta) V \mathbf{1}_\calG] \E [\dop_\theta  \VEC K(\Theta) V 
         \mathbf{1}_\calG ]^\T \right)}{ \norm{V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V} },
    \end{align*}}\fi
    where $\Xi_{\theta,\e}$ is defined in \eqref{eq: kronecker shorthand} and $\calG$ is defined in \Cref{lem: application of van trees}. 
    % If we also have $T \geq \frac{16 \norm{\Gamma_0}^2}{\lambda_{\min}(\Gamma_0)}$, then 
    % \begin{align*}
    %      \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)&\geq \frac{\tr\left( \E[  (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \dop_\theta \VEC  K(\Theta) V \mathbf{1}_\calE] \E [\dop_\theta  \VEC K(\Theta) V \mathbf{1}_\calE]^\T \right)}{ \norm{V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V} }.
    % \end{align*}
\end{lemma}
\ifnum\value{cdc}>0{}\else{
\begin{proof}
    We may write $\Theta = \theta + V \tilde \Theta$, where  $\tilde \Theta \sim \tilde \lambda$, and $\tilde \lambda$ is a smooth prior on $\curly{\tilde\theta \in \R^k: \norm{\tilde \theta} \leq \epsilon}$. Defining $\tilde A(\tilde \theta) := A(\theta+V \tilde \theta)$, $\tilde B(\tilde \theta) := B(\theta+V\tilde \theta)$, and $\tilde K(\tilde \theta):=K(\theta+V \tilde \theta)$, we may instantiate the bound in \Cref{lem: application of van trees} over the lower dimensional parameter $\tilde \theta$. We have that the Jacobian of the contoller becomes
    $
        \dop_{\tilde \theta} \VEC \tilde K(\tilde \Theta) = \dop_{\theta} \VEC K(\Theta) V.
    $
    Similarly, the Jacobian arising in the Fisher information may be written $D_{\tilde \theta} \VEC \bmat{\tilde A(\tilde \Theta) & \tilde B(\tilde \Theta)} = D_{\theta} \VEC \bmat{A(\Theta) & B(\Theta)} V $. Lastly, the prior density of the lower dimensional parameter satisfies $\tilde J(\tilde \lambda) = V^\top J(\lambda) V$.  Then under this prior, the lower bound in \Cref{lem: application of van trees} becomes that in the lemma statement.
    % \begin{align*}
    %     \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)&\geq \tr\left( \E[  (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \dop_\theta \VEC  K(\Theta) V \mathbf{1}_\calE] \left[  V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V \right]^{-1} \E [\dop_\theta  \VEC K(\Theta) V \mathbf{1}_\calE]^\T \right).
    % \end{align*}
    % Pulling out $\lambda_{\min}\paren{(V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V)^{-1}} = \frac{1}{\norm{V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V}}$ yields the desired result.  
\end{proof}}\fi

In the above lemma, the columns of $V$ may be interpreted as perturbation directions of the system parameters. 


 % with $U_{t,n}$ generated according to $\pi$. 

% {\color{red} Introduce notation to make the following statment easier to parse. Write a short proof. For arxiv, state that the longer proof is in the appendix.}

We now upper bound the denominator arising in the above bound. In particular, we show how to bound the Fisher Information in any particular perturbation direction.
\begin{lemma}
    \label{lem: fisher bound}
    For any matrix $V\in \R^{d_{\Theta} \times k}$ with orthonormal columns, 
    \begin{align*}
        \norm{V^\top \E \brac{I_p(\Theta)} V} &\leq  TN \bar L, 
    \end{align*}
    where $\bar L =\sup_{\theta' \in \calB(\theta,\e)}   L(\theta')$ and %$L(\theta')$ is given as \Bruce{2 lines}
    \begin{equation}
    \begin{aligned}
        \label{eq: info bound L}
        % L&(\theta', w)\!=\!\frac{2 \nu_1(w) \nu_1 \bigg(\norm{\dlyap\paren{(A(\theta')+B(\theta')F)^\top, \Sigma_W}}}{\lambda_{\min}(\Sigma_W)} \\
        % & + \frac{2\nu_1(w) \beta \paren{\sum_{t=0}^{\infty} \norm{(A(\theta')  B(\theta')F)^t B}}^2 + 4 \nu_2(w) \beta }{\lambda_{\min}(\Sigma_W)}
        &L(\theta')=  \sup_{w \in \Span(V), \norm{w} \leq 1} \frac{4 }{\lambda_{\min}(\Sigma_W)} \\
        & \cdot 
         \Bigg(\nu_1(w) \bigg(\norm{\dlyap\paren{(A(\theta')+B(\theta')F)^\top, \Sigma_W}}\\
         &\!+\!  \sigma_{\tilde u}^2 \paren{\sum_{t=0}^{\infty} \norm{(A(\theta') \!+\!  B(\theta')F)^t B}}^2 \Bigg) \!+\! 2 \sigma_{\tilde u}^2 \nu_2(w)\bigg).
    \end{aligned}
    \end{equation}
    Here, $\nu_1(w) = \norm{D_{\theta} \VEC A(\theta') w}^2 +  2\norm{D_{\theta} \VEC B(\theta') w}^2 \norm{F}^2$ and $\nu_2(w) = \norm{D_{\theta} \VEC B(\theta') w}^2$ are change of coordinate terms that quantify the impact of the perturbation direction on the information upper bound. We recall that $ \sigma_{\tilde u}^2$ is the average exploratory input energy. 
        %\norm{  \dop_\theta \VEC \bmat{A(\theta)} v}^2 (\textrm{cont. from noise} + \textrm{cont. from input})  + 2 \beta TN \norm{  \dop_\theta \VEC \bmat{B(\theta)} v}^2  
\end{lemma}

\sloppy The quantity $\dlyap((A(\theta') + B(\theta') F)^\top, 
\Sigma_W)$, %\Tasos{Isn't it also the steady state state covariance under F?} 
in the above bound may be interpreted as either the steady state covariance during exploration in the absence of exploratory inputs, or the controllability gramian from the noise to the state. The quantity \ifnum\value{cdc}>0 {}\else{$\sum_{t=0}^\infty \norm{(A(\theta')+B(\theta') F)^t B(\theta')}$ bounds the $\calH_\infty$ norm of the closed-loop system during offline experimentation. Therefore, }\fi $ \sigma_{\tilde u}^2 \paren{\sum_{t=0}^\infty \norm{(A(\theta')+B(\theta') F)^t B(\theta')}}^2$ upper bounds the impact of exploratory input on the state during offline experimentation. 
%\Tasos{seems related to Hinfinity norm of offline feedback}.  
\ifnum\value{cdc}>0{}\else{
The proof of the above lemma applies repeated use of the triangle inequality, submultiplicativity, the Cauchy-Schwarz inequality. See \Cref{s: pf of fisher bound} for proof details.
}\fi

% \Bruce{Defer the below proof to the appendix. }
% \begin{proof}
%     For any vector $w$, vectorization identitities and submultiplicativity tells us that
%     \begin{align*}
%         w^\top I_p^\pi(\theta) w &= \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \bmat{ (D_{\theta} \VEC A(\theta) w)^\top \\ (D_{\theta} \VEC B(\theta) w)^\top} Z_{t,n} Z_{t,n}^\top \otimes \Sigma_W^{-1} \bmat{ D_{\theta} \VEC A(\theta) w \\ D_{\theta} \VEC B(\theta) w} \\
%         %& \leq \frac{1}{\lambda_{\min}(\Sigma_W)} \E_\theta^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} \bmat{ (D_{\theta} \VEC A(\theta) w)^\top \\ (D_{\theta} \VEC B(\theta) w)^\top} Z_{t,n} Z_{t,n}^\top \otimes I \bmat{ D_{\theta} \VEC A(\theta) w \\ D_{\theta} \VEC B(\theta) w} \\
%         % &= \frac{1}{\lambda_{\min}(\Sigma_W)} \E_\theta^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} \bmat{ (D_{\theta} \VEC A(\theta) w)^\top \\ (D_{\theta} \VEC B(\theta) w)^\top} \VEC \paren{ \VEC^{-1}\paren{\bmat{ D_{\theta} \VEC A(\theta) w \\ D_{\theta} \VEC B(\theta) w}} Z_{t,n} Z_{t,n}^\top} \\
%         % &= \frac{1}{\lambda_{\min}(\Sigma_W)} \E_\theta^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} \tr \paren{ \VEC^{-1}\paren{\bmat{ D_{\theta} \VEC A(\theta) w \\ D_{\theta} \VEC B(\theta) w}} Z_{t,n} Z_{t,n}^\top \VEC^{-1}\paren{\bmat{ D_{\theta} \VEC A(\theta) w \\ D_{\theta} \VEC B(\theta) w}^\top}} \\
%         % &= \frac{1}{\lambda_{\min}(\Sigma_W)} \E_\theta^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} \bigg(
%         %\tr \paren{ \VEC^{-1}\paren{D_{\theta} \VEC A(\theta) w} X_{t,n} X_{t,n}^\top \VEC^{-1}\paren{ D_{\theta} \VEC A(\theta) w } }\\
%         % &\qquad  +  \tr \paren{ \VEC^{-1}\paren{D_{\theta} \VEC B(\theta) w} U_{t,n} U_{t,n}^\top \VEC^{-1}\paren{ D_{\theta} \VEC B(\theta) w } }\bigg) \\
%         & \leq \frac{\norm{D_{\theta} \VEC A(\theta) w}^2}{\lambda_{\min}(\Sigma_W)}  \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top } + \frac{\norm{D_{\theta} \VEC B(\theta) w}^2}{\lambda_{\min}(\Sigma_W)} \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} U_{t,n} U_{t,n}^\top }
%     \end{align*}
%     The quantity $\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} U_{t,n} U_{t,n}^\top }$ may be bounded in terms of $\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top }$ and $\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \tilde U_{t,n} \tilde U_{t,n}^\top }$ by expressing $U_{t,n} = FX_{t,n} + \tilde U_{t,n}$, and applying a modified Cauchy Schwarz inequality, see \Cref{lem: matrix cauchy}. In particular, the quantity above can be upper bounded by 
%     \begin{align*}
%          &\frac{\norm{D_{\theta} \VEC A(\theta) w}^2 + 2\norm{D_{\theta} \VEC B(\theta) w}^2 \norm{F}^2}{\lambda_{\min}(\Sigma_W)}  \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top } \\
%          &\qquad \qquad + \frac{2\norm{D_{\theta} \VEC B(\theta) w}^2}{\lambda_{\min}(\Sigma_W)} \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \tilde U_{t,n} \tilde U_{t,n}^\top }.
%         %& \leq \frac{\norm{D_{\theta} \VEC A(\theta) w}^2 + 2\norm{D_{\theta} \VEC B(\theta) w}^2 \norm{F}^2}{\lambda_{\min}(\Sigma_W)}  \norm{E_{\theta}^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top } + 2\frac{\norm{D_{\theta} \VEC B(\theta) w}^2}{\lambda_{\min}(\Sigma_W)} \beta N T.
%     \end{align*}
%     To bound the second term, we may utilize the exploratory input energy bound, \eqref{eq:budgeteq}. 
%     To bound the first term, observe that we may roll out the states in terms of the exploratory inputs, and the process noise berfore again applying the Cauchy-Schwarz bound in \Cref{lem: matrix cauchy}. In particular, %$\norm{E_{\theta}^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top }$, observe that by \Cref{lem: matrix cauchy},
%     \begin{align*}
%         &\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top \\
%         % &= E_{\theta}^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_k + (A+BF)^{t-1-k} B \tilde U_k}\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_k + (A+BF)^{t-1-k} B \tilde U_k}^\top \\
%         &\preceq 2 \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n}}^\top \\
%         &\qquad + 2 \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B \tilde U_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B \tilde U_{k,n}}^\top 
%     \end{align*}
%     The first term may be simplified using the statistical properties of the sequence $W_k$:
%     \begin{align*}
%         &\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n}}^\top \\
%         & \preceq NT \sum_{t=0}^{\infty} (A+BF)^{t} \Sigma_W \paren{(A+BF)^{t}}^\top.
%     \end{align*}
%     The second term may be bounded by the exploratory input energy bound in \eqref{eq:budgeteq} by application of the triangle inequality, Cauchy-Schwarz inequality, and submultiplicativity. In particular, it is bounded by
%     \begin{align*}
%         %\norm{E_{\theta}^\pi \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} A^{t-1-k} BU_k }\paren{\sum_{k=0}^{t-1} A^{t-1-k} BU_k}^\top } \leq  
%         \beta NT\paren{\sum_{t=0}^{\infty} \norm{(A+BF)^t B}}^2.
%     \end{align*}
%     Combining these results proves the statement. 
% \end{proof}

We now present our first main result: a non-asymptotic lower bound on the local minimax excess cost. As with \Cref{lem: application of van trees}, it is presented in two components: one that holds generally, and another that requires enough data such that any sufficiently good policy $\pi \in \Pi_{\mathsf{lin}}$ outputs a feedback controller $\hat K (\calZ)$ which
 is near optimal with high probability. Consequently, the burn-in times are larger for the second result, and the size of the prior, $\varepsilon$, is required to be small.  We drop the dependence of $A$, $B$, $P$, $\Psi$, $K$,  and $\Sigma_X$ on $\theta$ when the argument is clear from context. 

\begin{theorem}
    \label{thm: finite data bound}
    Consider any matrix $V \in \R^{d_\Theta \times k}$ with $k \leq d_{\Theta}$ which has orthonormal columns. Let
    % \ifnum\value{cdc}>0{
    \begin{align*}
        G &= \inf_{\theta^{'}, \tilde \theta \in \calB(\theta,\e)} \tr\bigg( \Xi_{\theta,\e} \dop_\theta \VEC  K(\theta')V \paren{\dop_\theta  \VEC K(\tilde \theta) V }^\T \bigg),
    \end{align*}
    % }\else{
    % \begin{align*}
    %     G &= \inf_{\theta', \tilde \theta \in \calB(\theta,\e)} \tr\left( (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \dop_\theta \VEC  K(\theta')V \paren{\dop_\theta  \VEC K(\tilde \theta) V }^\T \right),
    % \end{align*}
    % }\fi
    and $\bar L$ be as in \Cref{lem: fisher bound}. Also let $\Xi_{\theta,\e}$ be as defined in \eqref{eq: kronecker shorthand}. Then for any smooth prior $\lambda$ over $\curly{\theta + V \tilde \theta: \norm{\tilde \theta}\leq \e}$, 
    \begin{align}
        \label{eq: general LB after burn-in}
         \mathcal{EC}^{\mathsf{lin}}_T(\theta,\e)&\geq \frac{G}{8 NT \bar L}
    \end{align}
    is satisfied for
    \begin{blockquote}
        $1)$ $\Gamma_{\theta,\e}=\Sigma_W$ if $TN \geq \frac{ \norm{J(\lambda)}}{\bar L }$. \\
        $2)$ $\Gamma_{\theta,\e} = \Sigma_{\theta, \e}$ if  
         $T \geq \sup_{\theta' \in \calB(\theta,\e)} \frac{16 \norm{\Sigma_X(\theta')}^2}{\lambda_{\min}(\Sigma_X(\theta'))}$, $TN \geq \frac{1}{\bar L}{\max\curly{ \norm{J(\lambda)}, \frac{ G}{ \lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2}}}$, and  $\e \leq \min\curly{\frac{\alpha}{2c_1}, c_2}$, where 
            \begin{align*}
                    c_1 &= 84 \Phi^9  \tau(A_{cl}) \\
                    c_2 &= \frac{1}{ 10 \tau(A_{cl}) c_1 } \min\curly{(1+\norm{A_{cl}})^{-2}, (1 + \norm{P})^{-1}} \\
                     \Phi &= (1 + \max\curly{\norm{A}, \norm{B}, \norm{P}, \norm{K},\norm{R^{-1}}}) \\
                    \tau(A_{cl}) &= \paren{\sup_{k \geq 0} \curly{\norm{A_{cl}^k}\rho(A_{cl})^{-k}}}^2/(1-\rho(A_{cl})^2).
                \end{align*}
        % \end{itemize}
        % \end{blockquote2}
    \end{blockquote}
    
    % Now define
    % \begin{align*}
    %      c_1 &= 84 (1 + \max\curly{\norm{A}, \norm{B}, \norm{P}, \norm{K},\norm{R^{-1}}})^9 \frac{\tau(A_{cl}, \gamma)}{1-\gamma^2} \\
    %     c_2 &= \frac{1}{768} \frac{(1-\gamma^2)^2}  {\tau(A_{cl}, \gamma)^4} (1 + \max\curly{\norm{A}, \norm{B}, \norm{P}, \norm{K},\norm{R^{-1}}})^{-9} \min\curly{(1+\norm{A_{cl}})^{-2}, (1 + \norm{P})^{-1}} \\
    %      F &= \inf_{\theta', \tilde \theta \in \calB(\theta,\e)} \frac{1}{4} \tr\left( (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \dop_\theta \VEC  K(\theta')V \paren{\dop_\theta  \VEC K(\tilde \theta) V }^\T \right)
    % \end{align*}
    
    % If we also have $T \geq \frac{16 \norm{\Gamma_0}^2}{\lambda_{\min}(\Gamma_0)}$, $T^2 N \geq \frac{4 F}{ L \lambda_{\min}(\Sigma_W) \lambda_{\min}(R)}$, $TN \geq \frac{2\norm{J(\lambda)}}{L}$ and $\e \leq \min\curly{\frac{\alpha}{2c_1}, c_2}$ then, 
    % \begin{align}
    %     \label{eq: conditional LB after burn-in}
    %      \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)&\geq \frac{F}{TN L},
    % \end{align}
    % where 
    % \begin{align*}
    %     \tilde F &= \inf_{\theta', \tilde \theta \in \calB(\theta,\e)} \frac{1}{4} \tr\left( (\Gamma_{\theta,\e} \otimes \Psi_{\theta,\e} )  \dop_\theta \VEC  K(\theta')V \paren{\dop_\theta  \VEC K(\tilde \theta) V }^\T \right) \\
    %     % L &=  \sup_{w \in \Span(V), \norm{w} \leq 1} \sup_{\theta' \in \calB(\theta,\e)}  \frac{4 }{\lambda_{\min}(\Sigma_W)}
    %     %  \bigg(\norm{D_{\theta} \VEC A(\theta') w}^2 \bigg(\norm{\sum_{t=0}^{T-1} A(\theta')^t \Sigma_W (A(\theta')^t)^\top} \\
    %     %  &+ \beta \paren{\sum_{t=0}^{T-1} \norm{A(\theta')^t B(\theta')}}^2 \bigg) + \beta \norm{D_{\theta} \VEC B(\theta') w}^2\bigg)
    % \end{align*}
\end{theorem}
\ifnum\value{cdc}>0{

 The theorem follows by bounding the probability of the event $\calG$ in \Cref{lem: coordinate transformation}, then bounding the denominator with \Cref{lem: fisher bound}.
}\else{
%\Bruce{Modify the proof to attain a contradiction to the statement: There exists a policy $\pi \in \Pi^\mathsf{lin}$ such that the regret is less than $G/(8 NT \bar L)$. This change corrects for the change in presentation of Lemmas 2.1-2.3. }
\begin{proof}
    We must show that for all $\pi \in \Pi^{\mathsf{lin}}$, $\sup_{\theta'\in\calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{G}{8 NT \bar L}$. Suppose that for some $\pi \in \Pi^\mathsf{lin}$, $\sup_{\theta' \in \calB(\theta, \e) }\mathsf{EC}_T^\pi(\theta') \leq \frac{G}{8 NT \bar L}$. We have by \Cref{lem: coordinate transformation} that
    \begin{align}
        \label{eq: single policy lb}
        \sup_{\theta'\in\calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{\tr\left(\Xi_{\theta,\e}  \E[\dop_\theta \VEC  K(\Theta) V \mathbf{1}_\calG] \E [\dop_\theta  \VEC K(\Theta) V 
         \mathbf{1}_\calG ]^\T \right)}{ \norm{V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V} }.
    \end{align}
    The burn-in requirement $TN \geq \frac{\norm{J(\lambda)}}{\bar L}$ enables upper bounding $\norm{V^\top J(\lambda) V}$ by $TN \bar L$.  \Cref{lem: fisher bound} then allows us to upper bound the denominator in \eqref{eq: single policy lb} by $2TN \bar L$. 

    To remove the indicators from the lower bound, we take an infimum over $\tilde \theta, \theta' \in \calB(\theta,\e)$ to lower bound the numerator in \eqref{eq: single policy lb} by $\bfP[\calG]^2 G$. For case 1, we immediately have $\bfP[\calG]^2=  \bfP[\Omega]^2 = 1$. For case 2, we may leverage the assumptions that the prior is small and that the burn-in time is satisfied to show that $\bfP[\calG]^2 =\bfP[\calE]^2 \geq \frac{1}{4}$. See \Cref{s: proof of finite data bound}  for more details. This in turn implies that 
    \begin{align*}
        \sup_{\theta'\in\calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{G}{8 TN\bar L}.
    \end{align*}
    Therefore, for all $\pi \in \Pi^{\mathsf{lin}}$, the above lower bound is satisfied. This implies that 
    \begin{align*}
        \mathcal{EC}^{\mathsf{lin}}_T(\theta,\e) = \inf_{\pi \in \Pi^\mathsf{lin}} \sup_{\theta' \in \calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{G}{8 TN\bar L}.
    \end{align*}
    %\Tasos{it seems to me that while proving the lower bound on the probability event there is an interesting tension. Low regret leads to small K error leading to large regret. it might be worth to comment on that more} 
\end{proof}
}\fi
%The proof of the above result highlights an interesting tension. When the controller is near optimal as defined by the event $\calE$, the suboptimality is small.  However, in this setting, the lower bound on the state covariance of the learned controller is improved to $\Sigma_{\theta,\e}$ rather than $\Sigma_W$. This in turn increases the suboptimality lower bound. 
The above result holds non-asymptotically. It will be helpful to present the result asymptotically, as the number of experiments tends to $\infty$ for an understanding of the dependence on control-theoretic quantities. 
\begin{corollary}

\label{cor: asymptotic lower bound}
For any $\alpha \in (0, 1/2)$ and any matrix $V \in \R^{d_\Theta \times k}$ with $k \leq d_{\Theta}$ which has orthonormal columns, %, suppose $T \geq \frac{16 \norm{\Gamma_0}^2}{\lambda_{\min}(\Gamma_0)}$,
we have that
    \begin{align*}
        &\liminf_{N \to \infty} \sup_{\theta' \in \mathcal{B}(\theta, N^{-\alpha}) } N \mathsf{EC}_T^\pi(\theta')   \geq \frac{G}{8 T L(\theta)}, % \tr\left(   (\Gamma_0 \otimes (B^\top P B+R))  \dop_\theta \VEC  K(\theta) VV^\top \dop_\theta  \VEC    K(\theta)^\T \right).
        %\frac{\sigma_{\min}(B( \theta)^\top P(\theta) B(\theta) + R)  \sigma_{\min}(\Gamma_0(\theta))}{16 \mathcal{C}(T) N}  \norm{   \dop_\theta \VEC  K(\theta) V}_F^2
    \end{align*}
    holds always for $\Gamma=\Sigma_W$ and for $\Gamma=\frac{1}{2} \Sigma_X$ if $T \geq \frac{16 \norm{\Sigma_X}^2}{\lambda_{\min}(\Sigma_X)}$, where $L$ is as in \Cref{lem: fisher bound} and
    \begin{align*}
        G &= \tr\left( (\Gamma \otimes \Psi)  \dop_\theta \VEC  K(\theta )V \paren{\dop_\theta  \VEC K(\theta) V }^\T \right).
        %L &=  \sup_{w \in \Span(V), \norm{w} \leq 1}   \frac{8 }{\lambda_{\min}(\Sigma_W)}
         % \bigg(\paren{\norm{D_{\theta} \VEC A(\theta) w}^2 +  2\norm{D_{\theta} \VEC B(\theta) w}^2 \norm{F}^2} \\
         % &\cdot\bigg(\norm{\dlyap\paren{(A+BF)^\top, \Sigma_W}}
         % + \beta \sum_{t=0}^{\infty} \norm{\paren{(A +  BF)^t B}}^2 \bigg) + 2\beta \norm{D_{\theta} \VEC B(\theta) w}^2\bigg).
    \end{align*}
    
\end{corollary}
\ifnum\value{cdc}>0{}\else{\begin{proof}
    The burn-in requirements in \Cref{thm: finite data bound} are satisfied asymptotically, see \Cref{s: asymptotic lb proof} for more details. 
\end{proof}
}\fi

Using a similar argument to the derivations above, it can be shown that the global minimax complexity is infinite. 
\begin{corollary}
    \label{cor: global minimax}
    The global minimax excess cost is infinite for the class of scalar systems of the form: 
    \ifnum\value{cdc}>0{$X_{t+1} = a X_t + b U_t + W_t,$}
    \else{
    \[
        X_{t+1} = a X_t + b U_t + W_t,
    \]
    }\fi
    with $\theta = \bmat{a & b}^\top$, and $Q=R=\Sigma_W = \sigma_{\tilde u}^2= 1$. More precisely, for the class of stable scalar systems with the offline exploration policy $F=0$, we have
    \begin{align*}
        \liminf_{N \to \infty} \sup_{a, b: \abs{a} <1} N \mathsf{EC}_T^\pi(a,b) = \infty.
    \end{align*}
\end{corollary}
\ifnum\value{cdc}>0{}\else{
\begin{proof}
    We argue as in the proof of \Cref{cor: asymptotic lower bound} with $V = \bmat{0 & 1}^\top$. In this perturbation direction, the lower bound evaluates to $\frac{1}{T} \frac{b^2 P +1}{32} \frac{\partial K}{\partial b}(a,b)^2$. Considering $a = 1-\gamma$ and $b =\gamma$ for $0 < \gamma < 1$ and taking the limit as $\gamma\to 0$ results in the lower bound of $\infty$. For more details, see \Cref{s: global minimax pf}.
\end{proof}}\fi

%{\color{red} Write the version of the results that lower bounds the covariance by $\Sigma_W$ rather than $\Sigma_x$. There are a lot of choices for the presentation of this. I think it's best just to get some version of it down on the paper.}