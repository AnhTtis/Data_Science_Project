\appendix
\section{Proofs from Section~\ref{s: suboptimality lower bounds}: Excess Cost Lower Bound}

\begin{lemma}(Cauchy-Schwarz)
    \label{lem: matrix cauchy}
    For two sequences $a_1, \dots, a_n \in \R^n$ and $b_1, \dots, b_n \in \R^n$, 
    \[
        \sym \sum_{i=1}^n a_i b_i^\top \preceq \sum_{i=1}^n a_i a_i^\top + \sum_{i=1}^n b_i b_i^\top. 
    \]
\end{lemma}
\begin{proof}
    Express $A = \bmat{a_1 & \dots & a_n}$, and $B=\bmat{b_1 & \dots & b_n}$. The result follows by rearranging the inequality $0 \preceq (A-B)(A-B)^\top$. 
\end{proof}

\begin{lemma}   
    \label{lem: covariance overlap between learned and optimal controller}
    Suppose $T \geq \frac{16\lambda_{\min}(\Sigma_X)}{\norm{\Sigma_X}^2}$. Then under event $\calE$, 
    \[
        \norm{\Sigma_X^{-1/2}\Sigma_{\Theta}^{\hat K(\calZ)}\Sigma_X^{-1/2} - I}  \leq \frac{1}{2}.
    \]
\end{lemma}
\begin{proof}
    We first bound $\norm{\Sigma_X^{-1/2}\Sigma_{\Theta}^{\hat K(\calZ)}\Sigma_X^{-1/2} - I}$ in terms of the gap between $\hat K(\calZ)$ and $K(\Theta)$. In particular, we have that
\begin{align*}
    \norm{\Sigma_X^{-1/2}\Sigma_{\Theta}^{\hat K(\calZ)}\Sigma_X^{-1/2} - I} = \norm{\Sigma_X^{-1/2}(\Sigma_{\Theta}^{\hat K(\calZ)} - \Sigma_X)\Sigma_X^{-1/2}} \leq \frac{\norm{\Sigma_{\Theta}^{\hat K(\calZ)} - \Sigma_X}}{\lambda_{\min}(\Sigma_X)}.
\end{align*}
Note that 
\begin{align*}
    \Sigma_{\Theta}^{\hat K(\calZ)} & - \Sigma_X = %\frac{1}{T}\paren{ \sum_{t=0}^{T-1} (A+B\hat K)^t \Sigma_X \paren{(A+B\hat K)^t}^\top +
    \frac{1}{T} \sum_{t=0}^{T-1} \sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^k}^\top -\Sigma_X \\
    &= \frac{1}{T} \sum_{t=0}^{T-1} \sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^k}^\top - \frac{1}{T} \sum_{t=0}^{T-1} \sum_{k=0}^\infty (A+BK)^k \Sigma_W \paren{(A+BK)^k} \\
   % &= \frac{1}{T} \sum_{t=0}^{T-1} \sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^t}^\top - \sum_{k=0}^\infty (A+BK)^k \Sigma_W \paren{(A+BK)^k} \\
    &= \frac{1}{T} \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^k}^\top - \sum_{k=0}^{t-1} (A+BK)^k \Sigma_W \paren{(A+BK)^k}}\\
    &\qquad - \frac{1}{T} \sum_{t=0}^{T-1} \sum_{k=t}^{\infty} (A+BK)^k \Sigma_W \paren{(A+BK)^k}^\top.
\end{align*} 
Then 
\begin{align*}
     &\norm{\Sigma_{\Theta}^{\hat K(\calZ)} - \Sigma_X} \\
     &\leq \frac{1}{T}\norm{\sum_{t=0}^{T-1} \paren {\sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^k}^\top - \sum_{k=0}^{t-1} (A+BK)^k \Sigma_W \paren{(A+BK)^k}} } \\
     &\qquad + \frac{1}{T} \norm{\sum_{t=0}^{T-1} \sum_{k=t}^{\infty} (A+BK)^k \Sigma_W \paren{(A+BK)^k}^\top} \\
     &\leq \frac{1}{T}\norm{\sum_{t=0}^{T-1} \paren{ \sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^k}^\top - \sum_{k=0}^{t-1} (A+BK)^k \Sigma_W \paren{(A+BK)^k} }} \\
     &\qquad + \frac{1}{T} \norm{\sum_{t=0}^{\infty} (A+BK)^{t} \sum_{k=0}^{\infty} (A+BK)^k \Sigma_W \paren{(A+BK)^k}^\top \paren{(A+BK)^t}^\top} \\
      &\leq \frac{1}{T}\norm{\sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^k}^\top - \sum_{k=0}^{t-1} (A+BK)^k \Sigma_W \paren{(A+BK)^k} }} + \frac{\norm{\Sigma_X}^2}{T}.
\end{align*}
With $T \geq \frac{4 \norm{\Sigma_X}^2}{\lambda_{\min}(\Sigma_X)}$, the quantity $\frac{\norm{\Sigma_X}^2}{T}$ is upper bounded by $\frac{\lambda_{\min}(\Sigma_X)}{4}$. To bound the remaining term, we apply Lemma~\ref{lem: lyapunov perturbation} to show that under event $\calE$, 
\begin{align*}
    \frac{1}{T}\norm{\sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+B \hat K)^k \Sigma_W \paren{(A+B \hat K)^t}^\top - \sum_{k=0}^{t-1} (A+BK)^k \Sigma_W \paren{(A+BK)^k} } }\\
    \leq 6 \mathcal{J}(A+BK)^2 \norm{B} \norm{A+BK} \norm{\Sigma_X}\norm{\hat K - K} \leq \frac{\lambda_{\min}(\Sigma_X)}{4}.
\end{align*}
This yields the inequality $\norm{\hat \Sigma_{\Theta}^{\hat K(\calZ)} - \Sigma_X} \leq \frac{\lambda_{\min}(\Sigma_X)}{2}$, as we needed to show.
\end{proof}


\begin{lemma}
    \label{lem: lyapunov perturbation}
     Given a controller $K$ that stabilizes the system $x_{t+1} = Ax_t + Bu_t + w_t$, and another controller $\hat K$ such that $\norm{K - \hat K} \leq \min \curly{\frac{\norm{A+BK}}{\norm{B}}, \frac{1}{6 \mathcal{J}(A+BK) \norm{B}  \norm{A+BK}}}$. Let $P_1^0 = P_2^0 = \Sigma_W$, and 
    \begin{align*}
        P_1^t &= (A+B \hat K) P_1^{t-1} (A+B\hat K)^\top + \Sigma_W \\
        P_2^t &= (A+BK)P_2^{t-1} \paren{A+BK}^\top + \Sigma_W. 
    \end{align*}
    Then 
     \begin{align*}
    \frac{1}{T}\norm{\sum_{t=0}^{T-1} P_1^t - P_2^t} \leq 6 \mathcal{J}(A+BK) \norm{B} \norm{A+BK} \norm{\Sigma_X}\norm{\hat K - K}.
    \end{align*}
\end{lemma}
\begin{proof}
    We have that
    \begin{align*}
        P_1^t - P_2^t &= (A+BK)(P_1^{t-1} - P_2^{t-1})(A+BK)^\top \\
        &\qquad + \sym(B(\hat K - K) P_1^{t-1} (A+BK)^\top)+ B(\hat K - K)^\top P_1^{t-1} (\hat K - K)^\top B^\top,
    \end{align*}
    where for a square matrix $M$, $\sym(M) = M + M^\top$. Therefore,
    \begin{align*}
        P_1^t - P_2^t &= \sum_{k=1}^{t-1} (A+BK)^k \bigg(\sym(B(\hat K - K) P_1^{k-1} (A+BK)^\top)\\ 
        &\qquad+ B(\hat K - K)^\top P_1^{k-1} (\hat K - K)^\top B^\top \bigg) \paren{(A+BK)^k}^\top.
    \end{align*}
    By the triangle inquality and submultiplicativity, we have
    \begin{align}
        \label{eq: lyap gap bound}
        \norm{P_1^t - P_2^t} &\leq \calJ(A+BK) \norm{B}\norm{\hat K - K} \norm{P_1^t}\paren{\norm{\hat K - K}\norm{B} + 2\norm{A+BK}} \\
        &\leq 3\calJ(A+BK) \norm{B}\norm{\hat K - K} \norm{P_1^t}\norm{A+BK},
    \end{align}
    where the first inequality leveraged the fact that $\norm{P_1^{k-1}}\leq \norm{P_1^t}$ for $k-1 \leq t$, and the last inequality follows from the fact that $\norm{\hat K - K} \leq \frac{\norm{A+BK}}{\norm{B}}$.
    Next, note that
    \begin{align*}
        \norm{P_1^t} &\leq \norm{P_2^t} + \norm{P_1^t - P_2^2} \leq \norm{P_2^t} + 3\calJ(A+BK) \norm{B}\norm{\hat K - K} \norm{P_1^t}\norm{A+BK},
    \end{align*}
    so 
    \begin{align*}
        \norm{P_1^t} &\leq \frac{\norm{P_2^t}}{1 - 3\calJ(A+BK) \norm{B}\norm{\hat K - K} \norm{A+BK}} \leq 2 \norm{P_2^t},
    \end{align*}
    where the last inequality follows from the fact that $\norm{\hat K - K} \leq \frac{1}{6 \mathcal{J}(A+BK) \norm{B}  \norm{A+BK}}$. Substituting this into \eqref{eq: lyap gap bound} provides the inequality in the Lemma statement. 
\end{proof}

\subsection{Proof of \Cref{lem: fisher bound}}

\label{s: pf of fisher bound}
\begin{proof}
For any vector $w$, 
    \begin{align*}
        w^\top &I_p(\theta) w= \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} w^\top \paren{\dop_\theta \VEC \bmat{A(\theta) & B(\theta)}}^\top  \paren{ Z_{t,n} Z_{t,n}^\top \otimes \Sigma_W^{-1} }\paren{\dop_\theta \VEC \bmat{A(\theta) & B(\theta)}} w \\
        &\leq \frac{1}{\lambda_{\min}(\Sigma_W)} \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} w^\top \paren{ \dop_\theta \VEC \bmat{A(\theta) & B(\theta)}}^\top  \paren{ Z_{t,n} Z_{t,n}^\top \otimes I } \paren{\dop_\theta \VEC \bmat{A(\theta) & B(\theta)}} w \\
        &= \frac{1}{\lambda_{\min}(\Sigma_W)} \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} w^\top \paren{ \dop_\theta \VEC \bmat{A(\theta) & B(\theta)}}^\top  \VEC \paren{\VEC^{-1}  \paren{\paren{\dop_\theta \VEC \bmat{A(\theta) & B(\theta)}} w}  Z_{t,n} Z_{t,n}^\top}  \\
        &= \frac{1}{\lambda_{\min}(\Sigma_W)} \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \trace\bigg( \VEC^{-1}  \paren{\paren{\dop_\theta \VEC \bmat{A(\theta) & B(\theta)}} w} \\
        &\qquad\qquad \cdot Z_{t,n} Z_{t,n}^\top \paren{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC \bmat{A(\theta) & B(\theta)}}w}}^\top \bigg)
    \end{align*}
     where the $\VEC^{-1}$ operator maps a vector $v\in\R^{\dx d_k}$ to  a matrix $\VEC^{-1} v \in \R^{\dx \times d_k}$ as $\VEC^{-1} v = \bmat{v_{1:\dx} & v_{\dx+1:2\dx} & \dots & v_{(d_k-1)\dx+1:d_k\dx}}$. The second to last inequality follows from the vectorization identity, $ \VEC (XYZ) = (Z^\top \otimes X) \VEC(Y)$, and the last line follows from the identity $\trace(XY) = \VEC(X) \VEC(Y)^\top$. 
     
     Observe that the quantity $\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC \bmat{A(\theta) & B(\theta)}}w}$ may be expressed as 
     \begin{align*}
        \VEC^{-1} & \paren{ \paren{ \dop_\theta \VEC \bmat{A(\theta) & B(\theta)}}w} \\
        & = \bmat{(\dop_\theta A_1(\theta)) w & \dots & (\dop_\theta A_{\dx}(\theta)) w & (\dop_\theta B_1(\theta)) w & \dots & (\dop_\theta B_{\du}(\theta)) w } \\
        & = \bmat{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC A(\theta)} w} & \VEC^{-1} \paren{ \paren{ \dop_\theta \VEC B(\theta)} w}}, \\
     \end{align*}
     where $A_i$ and $B_i$ denote the $i^\textrm{th}$ column of $A$ and $B$ respectively. Then the above bound may be expressed
     \begin{align*}
          w^\top &I_p(\theta) w \leq  \frac{1}{\lambda_{\min}(\Sigma_W)} \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \trace\bigg( \bmat{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC A(\theta)} w} & \VEC^{-1} \paren{ \paren{ \dop_\theta \VEC B(\theta)} w}} \\
        &\qquad\qquad \cdot Z_{t,n} Z_{t,n}^\top \bmat{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC A(\theta)} w} & \VEC^{-1} \paren{ \paren{ \dop_\theta \VEC B(\theta)} w}}^\top \bigg) \\
        &= \frac{1}{\lambda_{\min}(\Sigma_W)} \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \norm{ \VEC^{-1} \paren{ \paren{ \dop_\theta \VEC A(\theta)} w} X_{t,n} +  \VEC^{-1} \paren{ \paren{ \dop_\theta \VEC B(\theta)} w} U_{t,n}}_F^2 \\
        &\leq  \frac{2}{\lambda_{\min}(\Sigma_W)} \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \norm{ \VEC^{-1} \paren{ \paren{ \dop_\theta \VEC A(\theta)} w} X_{t,n}}_F^2 +  \norm{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC B(\theta)} w} U_{t,n}}_F^2 \\
        &= \frac{2}{\lambda_{\min}(\Sigma_W)} \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \trace\paren{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC A(\theta)} w} X_{t,n}  X_{t,n}^\top \paren{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC A(\theta)} w}}^\top} \\
        &\qquad +  \trace\paren{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC B(\theta)} w} U_{t,n} U_{t,n}^\top \paren{\VEC^{-1} \paren{ \paren{ \dop_\theta \VEC B(\theta)} w}}^\top }.
     \end{align*}
    We may pull the summations and the expectation inside the trace, and pull out the norms of ${\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top }$ and ${\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} U_{t,n} U_{t,n}^\top }$ to arrive at the following bound.
    \begin{align*}
        w^\top I_p(\theta) w  &\leq \frac{2\norm{D_{\theta} \VEC A(\theta) w}^2}{\lambda_{\min}(\Sigma_W)}  \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top } + \frac{2 \norm{D_{\theta} \VEC B(\theta) w}^2}{\lambda_{\min}(\Sigma_W)} \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} U_{t,n} U_{t,n}^\top } \\
         & \leq \frac{2\norm{D_{\theta} \VEC A(\theta) w}^2 + 4\norm{D_{\theta} \VEC B(\theta) w}^2 \norm{F}^2}{\lambda_{\min}(\Sigma_W)}  \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top } \\
         &\qquad \qquad + \frac{4\norm{D_{\theta} \VEC B(\theta) w}^2}{\lambda_{\min}(\Sigma_W)} \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \tilde U_{t,n} \tilde U_{t,n}^\top } \\
        & \leq \frac{2\norm{D_{\theta} \VEC A(\theta) w}^2 + 4\norm{D_{\theta} \VEC B(\theta) w}^2 \norm{F}^2}{\lambda_{\min}(\Sigma_W)}  \norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top } \\
        &\qquad \qquad + 4\frac{\norm{D_{\theta} \VEC B(\theta) w}^2}{\lambda_{\min}(\Sigma_W)}  \sigma_{\tilde u}^2 N T,
    \end{align*}
    where the first inequality follows by applying Cauchy-Schwarz and submultiplicativity to bound $\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} U_{t,n} U_{t,n}^\top } \leq 2\norm{F}^2\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top }+2\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \tilde U_{t,n} \tilde U_{t,n}^\top }$. The second inequality follows by using \eqref{eq:budgeteq} to bound $\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \tilde U_{t,n} \tilde U_{t,n}^\top }$. 
    To bound the quantity  $\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top }$, observe that by \Cref{lem: matrix cauchy},
    \begin{align*}
        &\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} X_{t,n} X_{t,n}^\top \\
        &= \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n} + (A+BF)^{t-1-k} B \tilde U_{k,n}} \\
        & \qquad \cdot \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n} + (A+BF)^{t-1-k} B \tilde U_{k,n}}^\top \\
        &\preceq 2 \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n}}^\top \\
        &\qquad + 2 \E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B \tilde U_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B \tilde U_{k,n}}^\top.
    \end{align*}
    The first term may be simplified using the properties of the sequence $W_k$:
    \begin{align*}
        &\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} W_{k,n}}^\top \\
        &= N \sum_{t=0}^{T-1} \sum_{k=0}^{t-1} (A+BF)^{k} \Sigma_W \paren{(A+BF)^{k}}^\top \preceq NT \sum_{t=0}^{\infty} (A+BF)^{t} \Sigma_W \paren{(A+BF)^{t}}^\top.
    \end{align*}
    We will massage the second term to bound it using the input energy bound in \eqref{eq:budgeteq}:
    \begin{align*}
        &\norm{\sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B \tilde U_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B\tilde U_{k,n}}^\top} \\
        &=  \norm{\sum_{t=0}^{T-1} \sum_{k=0}^{t-1} \sum_{j=0}^{t-1}  \paren{(A+BF)^{t-1-k} B \tilde U_{k,n} }\paren{(A+BF)^{t-1-j} B \tilde U_{j,n}}^\top} \\
        & \leq \sum_{t=0}^{T-1} \sum_{k=0}^{t-1} \sum_{j=0}^{t-1}  \norm{(A+BF)^{t-1-k} B }\norm{(A+BF)^{t-1-j} B} \norm{\tilde U_{j,n}}\norm{\tilde U_{k,n}} \\
        &\leq \sum_{t=0}^{T-1} \sum_{k=0}^{t-1} \sum_{j=0}^{t-1}  \norm{(A+BF)^{t-1-k} B }\norm{(A+BF)^{t-1-j} B} \frac{ \norm{\tilde U_{j,n}}^2 + \norm{\tilde U_{k,n}}^2}{2} \\
        &=\sum_{t=0}^{T-1} \sum_{k=0}^{t-1} \sum_{j=0}^{t-1}  \norm{(A+BF)^{t-1-k} B}\norm{(A+BF)^{t-1-j} B} \norm{\tilde U_{j,n}}^2 \\
        &\leq \sum_{t=0}^{T-1} \sum_{k=0}^{T-1} \sum_{j=0}^{t-1}  \norm{(A+BF)^{t-1-k} B }\norm{(A+BF)^{t-1-j} B} \norm{\tilde U_{j,n}}^2 \\
        &= \paren{\sum_{k=0}^{T-1} \norm{(A+BF)^{t-1-k} B}} \sum_{j=0}^{T-1} \norm{\tilde U_{j,n}}^2 \sum_{t=j+1}^{T-1} \norm{A^{t-1-j} B}  \\
        &\leq \paren{\sum_{k=0}^{T-1} \norm{(A+BF)^{t-1-k} B}}^2 \sum_{j=0}^{T-1} \norm{\tilde U_{j,n}}^2.
    \end{align*}
    Then 
    \begin{align*}
        &\norm{\E_{\theta} \sum_{n=1}^N \sum_{t=0}^{T-1} \paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B \tilde U_{k,n} }\paren{\sum_{k=0}^{t-1} (A+BF)^{t-1-k} B\tilde U_{k,n}}^\top } \\
        &\leq  \sigma_{\tilde u}^2 NT \paren{\sum_{t=0}^{\infty} \norm{(A+BF)^t B}}^2.
    \end{align*}
    Combining these results proves the statement. 
    
\end{proof}

\subsection{Proof of \Cref{thm: finite data bound}}
\label{s: proof of finite data bound}
\begin{proof}
    We must show that for all $\pi \in \Pi^{\mathsf{lin}}$, $\sup_{\theta'\in\calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{G}{8 NT \bar L}$. Suppose that for some $\pi \in \Pi^\mathsf{lin}$, $\sup_{\theta' \in \calB(\theta, \e) }\mathsf{EC}_T^\pi(\theta') \leq \frac{G}{8 NT \bar L}$. We have by \Cref{lem: coordinate transformation} that this policy satisfies (under the assumption $T \geq \sup_{\theta'\in\calB(\theta,\e)} \frac{16 \norm{\Sigma_X(\theta')}}{\lambda_{\min}(\Sigma_X(\theta')}$ for case 2) 
    \begin{align*}
        %\label{eq: single policy lb}
        \sup_{\theta'\in\calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{\tr\left(\Xi_{\theta,\e}  \E[\dop_\theta \VEC  K(\Theta) V \mathbf{1}_\calG] \E [\dop_\theta  \VEC K(\Theta) V 
         \mathbf{1}_\calG ]^\T \right)}{ \norm{V^\top \paren{\E \I_p(\Theta)+\J(\lambda)}V} }.
    \end{align*}
    The burn-in requirement $TN \geq \frac{\norm{J(\lambda)}}{\bar L}$ enables upper bounding $\norm{V^\top J(\lambda) V}$ by $TN \bar L$.  \Cref{lem: fisher bound} then allows us to upper bound the denominator in \eqref{eq: single policy lb} by $2TN \bar L$. 

    To remove the indicators from the lower bound in \eqref{eq: single policy lb}, we take an infimum over $\tilde \theta, \theta' \in \calB(\theta,\e)$ to lower bound the numerator in \eqref{eq: single policy lb} by $\bfP[\calG]^2 G$. For case 1, we immediately have $\bfP[\calG]^2=  \bfP[\Omega]^2 = 1$. For case 2, we leverage the assumptions that the prior is small and that the burn-in time is satisfied to show that $\bfP[\calG]^2 =\bfP[\calE]^2 \geq \frac{1}{4}$. To do so, we must show that $\sup_{\theta' \in \calB(\theta,\e)} \norm{\hat K(\calZ) - K(\theta')}$ is small with high probability. Note that 
     \begin{align*}
        \sup_{\theta' \in \calB(\theta,\e)} \norm{\hat K(\calZ) - K(\theta')} &\leq  \sup_{\theta_2 \in \calB(\theta,\e)} \inf_{\theta_1 \in \calB(\theta,\e)}  \norm{\hat K(\calZ) - K(\theta_1)} + \norm{K(\theta_1) - K(\theta_2)} \\
        &\leq \inf_{\theta_1 \in \calB(\theta,\e)}  \norm{\hat K(\calZ) - K(\theta_1)} + \sup_{\theta_1, \theta_2 \in \calB(\theta,\e)}  \norm{K(\theta_1) - K(\theta_2)}.
    \end{align*}
    
    By Propositions 1 and 2 of \cite{mania2019certainty}, 
        $\sup_{\theta_1, \theta_2 \in \calB(\theta,\e)}  \norm{K(\theta_1) - K(\theta_2)} \leq c_1 \epsilon$,
    so long as $\e \leq c_2$. 
    Therefore, if $\varepsilon \leq \min \curly{\frac{\alpha}{2 c_1}, c_2}$, then $\sup_{\theta_1, \theta_2 \in \calB(\theta,\e)}  \norm{K(\theta_1) - K(\theta_2)} \leq \alpha/2$. 
    
    For the remaining term, note that for any $\calZ$, $\Theta$ we have $\inf_{\theta_1 \in \calB(\theta,\e)}  \norm{\hat K(\calZ) - K(\theta_1)} \leq \norm{\hat K(\calZ) - K(\Theta)} = \sqrt{\norm{\hat K(\calZ) - K(\Theta)}^2}$. This quantity may in turn be bounded by the excess cost:
    \begin{align*}
        \norm{\hat K(\calZ) - K(\Theta)}^2 \leq  \frac{\tr\paren{(\hat K(\calZ) - K(\Theta))^\top \Psi(\Theta) (\hat K(\calZ)-K(\Theta) \Sigma_{\Theta}^{\hat K(\calZ)}  )}}{ \lambda_{\min}(\Sigma_{\Theta}^{\hat K(\calZ)}) \lambda_{\min}(\Psi(\Theta))} \leq \frac{\mathsf{EC}_T^{\pi(\cdot; \calZ)}(\Theta)}{\lambda_{\min}(\Sigma_W) \lambda_{\min}(R)},
    \end{align*}
    where $\mathsf{EC}_T^{\pi(\cdot; \calZ)}(\Theta)$ is the conditional excess cost given $\calZ$ and $\Theta$:
    \[
        \mathsf{EC}_T^{\pi(\cdot; \calZ)}(\Theta) =\frac{1}{T} \E^\pi \brac{\sum_{t=0}^{T-1}  (U_t- K(\Theta)X_t )^\T \Psi(\Theta)  (U_t-K(\Theta)X_t ) \vert \calZ, \Theta}.
    \]
    By Markov's inequality, we have that
    \begin{align*}
        \bfP[\mathsf{EC}_T^{\pi(\cdot; \calZ)}(\Theta) > \lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2/4] \leq \frac{4 \E[\mathsf{EC}_T^{\pi(\cdot; \calZ)}(\Theta)]}{\lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2} \leq \frac{4 \sup_{\theta' \in \calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta')}{ \lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2}.
    \end{align*}
    By our assumption at the beginning of the proof that $\sup_{\theta' \in \calB(\theta, \e) }\mathsf{EC}_T^\pi(\theta') \leq \frac{G}{8 NT \bar L}$, we see that as long as the burn-in requirement is satisified, 
    \[
        \bfP[\mathsf{EC}_T^{\pi(\cdot; \calZ)}(\Theta) \leq \lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2/4] \geq \frac{1}{2}.
    \]
    As a result, $\bfP[\calG]\geq \frac{1}{2}$. 
    
    This in turn implies that for both case 1 and case 2, 
    \begin{align*}
        \sup_{\theta'\in\calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{G}{8 TN\bar L}.
    \end{align*}
    Therefore, for all $\pi \in \Pi^{\mathsf{lin}}$, the above lower bound is satisfied, and 
    \begin{align*}
        \mathcal{EC}^{\mathsf{lin}}_T(\theta,\e) = \inf_{\pi \in \Pi^\mathsf{lin}} \sup_{\theta' \in \calB(\theta,\e)} \mathsf{EC}_T^\pi(\theta') \geq \frac{G}{8 TN\bar L}.
    \end{align*}
    
    
    %\Tasos{it seems to me that while proving the lower bound on the probability event there is an interesting tension. Low regret leads to small K error leading to large regret. it might be worth to comment on that more} 
\end{proof}
% \begin{proof}
%     Begin with the statements in \Cref{lem: coordinate transformation}. \Cref{lem: fisher bound} allows us to replace the denominator in the lower bound by $\bar L$, while the burn-in requirement $TN \geq \frac{2\norm{J(\lambda)}}{\bar L}$ enables upper bounding $\norm{V^\top J(\lambda) V}$ by $TN \bar L$. This in turn allows us to apply \Cref{lem: fisher bound} to upper bound the denominator of \Cref{lem: coordinate transformation} by $2 TN \bar L$. 

%     To remove the indicators from the expression in \Cref{lem: coordinate transformation}, we may take an infimum over $\tilde \theta, \theta' \in \calB(\theta,\e)$ to lower bound the numerator in \Cref{lem: coordinate transformation} by $\bfP[\calG]^2 G$. It remains to lower bound $\bfP[\calG]$ by $\frac{1}{2}$. For case 1, we immediately have $\bfP[\calG]=  \bfP[\Omega] =1$. For case 2, observe that 
%     \begin{align*}
%         \sup_{\theta' \in \calB(\theta,\e)} \norm{\hat K(\calZ) - K(\theta')} &\leq  \sup_{\theta_2 \in \calB(\theta,\e)} \inf_{\theta_1 \in \calB(\theta,\e)}  \norm{\hat K(\calZ) - K(\theta_1)} + \norm{K(\theta_1) - K(\theta_2)} \\
%         &\leq \inf_{\theta_1 \in \calB(\theta,\e)}  \norm{\hat K(\calZ) - K(\theta_1)} + \sup_{\theta_1, \theta_2 \in \calB(\theta,\e)}  \norm{K(\theta_1) - K(\theta_2)} 
%     \end{align*}
    
%     By Propositions 1 and 2 of \cite{mania2019certainty}, 
%         $\sup_{\theta_1, \theta_2 \in \calB(\theta,\e)}  \norm{K(\theta_1) - K(\theta_2)} \leq c_1 \epsilon$,
%     so long as $\e \leq c_2$. 
%     Therefore, if $\varepsilon \leq \min \curly{\frac{\alpha}{2 c_1}, c_2}$, then $\sup_{\theta_1, \theta_2 \in \calB(\theta,\e)}  \norm{K(\theta_1) - K(\theta_2)} \leq \alpha/2$. 
    
%     For the other term, note that for any $\calZ$, $\Theta$, where the distribution of $\Theta$ has support only on $\calB(\theta,\e)$, we have  $\inf_{\theta_1 \in \calB(\theta,\e)}  \norm{\hat K(\calZ) - K(\theta_1)} \leq \norm{\hat K(\calZ) - K(\Theta)} = \sqrt{\norm{\hat K(\calZ) - K(\Theta)}^2}$. This quantity may in turn be bounded by the suboptimality:
%     \begin{align*}
%         \norm{\hat K(\calZ) - K(\Theta)}^2 \leq  \frac{\tr\paren{(\hat K(\calZ) - K(\Theta))^\top \Psi(\Theta) (\hat K(\calZ)-K(\Theta) T \Sigma_{\Theta}^{\hat K(\calZ)}  )}}{ T \lambda_{\min}(\Sigma_{\Theta}^{\hat K(\calZ)}) \lambda_{\min}(\Psi(\Theta))} \leq \frac{R_T^{\pi(\cdot; \calZ)}(\Theta)}{\lambda_{\min}(\Sigma_W) \lambda_{\min}(R)},
%     \end{align*}
%     where $R_T^{\pi(\cdot; \calZ)}(\Theta)$ is the conditional suboptimality given $\calZ$ and $\Theta$:
%     \[
%         R_T^{\pi(\cdot; \calZ)}(\Theta) =\frac{1}{T} \E^\pi \brac{\sum_{t=0}^{T-1}  (U_t- K(\Theta)X_t )^\T \Psi(\Theta)  (U_t-K(\Theta)X_t ) \vert \calZ, \Theta}.
%     \]
%     By Markov's inequality, we have that
%     \begin{align*}
%         \inf_{\pi \in \Pi_\textrm{lin}} \bfP[R_T^{\pi(\cdot; \calZ)}(\Theta) > \lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2/4] \leq \frac{4 \inf_{\pi \in \Pi_\textrm{lin}} \E[R_T^{\pi(\cdot; \calZ)}(\Theta)]}{\lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2} \leq \frac{4 \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e) }{ \lambda_{\min}(\Sigma_W) \lambda_{\min}(R) \alpha^2}.
%     \end{align*}
%     Suppose now that $\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)$ does not satisfy the inequality given in the lemma statement. Then 
%     \begin{align*}
%         \mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e) &\leq \frac{G}{4N T \bar L}.
%     \end{align*}
%     Then if $NT \geq \frac{G}{ 2 \bar L \lambda_{\min}(\Sigma_W) \lambda_{\min}(R)}$, we have  that $\inf_{\theta_1 \in \calB(\theta,\e)} \norm{\hat K(\calZ) - K(\Theta)}^2 \leq \frac{\alpha}{2}$ with probability at least $\frac{1}{2}$. Combining these results, we see that $\inf_{\hat K} \bfP[\calG]^2 = \inf_{\hat K}  \bfP[\calE]^2 \geq \frac{1}{4}$. This  contradicts the assumption that $\mathfrak{EC}^{\mathsf{lin}}_T(\theta,\e)$ does not satisfy the inequality given in the lemma statement, proving the claim.
% \end{proof}

\subsection{Proof of \Cref{cor: asymptotic lower bound}}
\label{s: asymptotic lb proof}

\begin{proof}
For the burn-in requirements to be satisfied asymptotically, it is necessary to select a prior $\lambda$ such that 
$\norm{J(\lambda)}$ grows slower than $N$. By our assumption in the corollary statement that $\e = N^{-\alpha}$ for $\alpha \in (0,\frac{1}{2})$, it is sufficient to show that $\norm{J(\lambda)} \leq \frac{c}{\e^2}$ for a constant $c$ which does not depend on $N$. To do so, let $\rho$ be a smooth density supported on $B(0,1)$, and let $\lambda(\tilde \theta)= \frac{1}{\e} \rho\paren{\frac{\tilde \theta - \theta}{\eps}}$. By the chain rule of differentiation and a change of variables, we have that
\[
    J(\lambda) = \frac{1}{\e^2} J(\rho). 
\]
\end{proof}

%\Tasos{proof of corollary 2.1 is missing? how do we bound the $J(\theta)$? Why should we take alpha less than 1/2? Is there a reason why $J(\theta)\le O(\epsilon^{-2})$ (I tried to find an explanation in Ingvar's paper but I couldn't find it)? Do we assume that we scale linearly (and then we shift) a fixed distribution $\rho$ around $B(0,1)$, e.g. $\lambda(\tilde{\theta})=\frac{1}{\epsilon}\rho(\frac{\tilde{\theta}-\theta}{\epsilon})$, where now lambda is defined as desired in $B(\theta,\epsilon)$? this should give that $J(\theta)=\epsilon^{-2}\times \textrm{const}$ I guess. I don't think this obvious. If we decide to scale with sth else, e.g. $\epsilon^2$, then this no longer holds.}

\subsection{Proof of \Cref{cor: global minimax}}
    \label{s: global minimax pf}
    
     We argue as in the proof of \Cref{cor: asymptotic lower bound} with $V = \bmat{0 \\ 1}$. In this perturbation direction, the lower bound evaluates to $\frac{b^2 P +1}{32}\frac{\partial K}{\partial b}(a,b)^2$. Considering $a = 1-\gamma$ and $b =\gamma$ for $0 < \gamma < 1$ and taking the limit as $\gamma\to 0$ results in the lower bound of $\infty$. 

    In particular, in this setting the solution to the Riccati equation evaluates to 
    \begin{align}
    \label{eq: scalar riccati} 
    P = \frac{ -\sqrt{(1-a^2)^2 - 2(1-a^2)b^2 + b^4} + -\sqrt{(1-a^2)^2 + 2(1+a^2)b^2 + b^4}}{2b^2}.
    \end{align}

    Next note that by the product rule, 
    \begin{align}
        \label{eq: controller derivative}
        \frac{dK}{db}(a,b) = \frac{b^2 P^2a - aP - b \frac{dP}{db}(a,b)a }{(b^2 P + 1)^2}.
    \end{align}
    Using \eqref{eq: scalar riccati}, it can be shown that the denominator of the above quantity converges to $1$. 
    We show that the numerator diverges to $\infty$ as $\gamma \to \infty$. To see that this is so, note that Section C.2.1 of \cite{simchowitz2020naive} may be used to show
    \[
        \frac{dP}{db}(a,b) = \dlyap(a_{cl}, 2 a_{cl} P K),
    \]
    where $a_{cl} = a+bK$. We have that $K = -bPa_{cl}$. Using these results, the numerator of \eqref{eq: controller derivative} may be simplified to 
    \begin{align*}
        b^2 P^2 a - Pa + \frac{2b^2 P^2 a_{cl}^2 a}{1-a_{cl}^2} &= P\paren{b^2 P a - a +2 \frac{b^2 P a_{cl}^2 a}{1-a_{cl}^2}}.
    \end{align*}
    It may be shown using Mathematica that 
    \[
    \lim_{\gamma\to\infty} \paren{b^2 P a - a +2 \frac{b^2 P a_{cl}^2 a}{1-a_{cl}^2}} = -\frac{\sqrt{2}}{2}. 
    \]
    Meanwhile, $P$ diverges to $\infty$. 




\section{Proofs from \Cref{s: consequences of lower bound}}
\label{s: proofs of consequences} 

\subsection{Proof of \Cref{prop: dimensional dependence}}
\begin{proof}
        In this case the quantity $G$ in  \Cref{cor: asymptotic lower bound} with $\Gamma = \frac{1}{2}\Sigma_X$ may be lower bounded as 
   \begin{align*}
        &G = \frac{1}{2}\tr\big( (\Sigma_X \otimes \Psi)  \dop_\theta \VEC  K(\theta )V \paren{\dop_\theta  \VEC K(\theta) V }^\T \big) \\
        &\overset{(i)}{=} \frac{1}{2}\sum_{i \in [\dx \du]} \trace\paren{(\Sigma_X \otimes \Psi) d_{v_i} \VEC K(\theta) d_{v_i} \VEC K(\theta)^\top } \\
        & \geq \frac{1}{2}\dx\du \inf_{v \in \boldsymbol \Delta} \trace\big((\Sigma_X \otimes \Psi)  d_{v} \VEC K(\theta) d_{v} \VEC K(\theta)^\top \big) \\
        &\overset{(ii)}{=}  \frac{1}{2}\dx\du \inf_{v \in \boldsymbol \Delta} \trace\paren{\Psi d_{v} K(\theta) \Sigma_X d_{v}  K(\theta)^\top } \\
        &\overset{(iii)}{=} \frac{1}{2}\dx\du \inf_{\Delta : \norm{\bmat{-\Delta K & \Delta}}_F = 1} \trace\big(\Psi^{-1} \Delta^\top  P A_{cl} \Sigma_X A_{cl}^\top P \Delta \big)\\
        &\geq \frac{1}{2}\dx\du \inf_{\Delta : \norm{\bmat{-\Delta K & \Delta}}_F = 1} \trace\paren{\Delta^\top \Delta }   \frac{\lambda_{\min}(P)^2 \lambda_{\min}(\Sigma_X - \Sigma_W)}{\norm{\Psi}},
    \end{align*} 
    where $(i)$ follows from expanding the matrix $V = \bmat{v_1 & \dots & v_{\dx\du}}$ and the fact that $d_v \VEC K(\theta) = \dop_\theta \VEC K(\theta) v$, $(ii)$ follows from the identities $
        \VEC(XYZ) = (Z^\top \otimes X) \VEC(Y)$ and $\trace(\VEC(X) \VEC(Y)^\top) = \trace(XY^\top)$, and $(iii)$ follows from the directional derivative in \eqref{eq: polderman derivative}. The inequality results from the observations that $A_{cl}\Sigma_X A_{cl}^\top = \Sigma_X - \Sigma_W$.  
    We have that $\trace\paren{\Delta^\top \Delta} = \norm{\Delta}_F^2$.  Then using the fact that $1 = \norm{\bmat{-\Delta K & \Delta }}_F^2 \leq \norm{\bmat{-K & I}}^2 \norm{\Delta}_F^2$, we have $\norm{\Delta}_F^2 \geq \frac{1}{\norm{\bmat{-K & I}}^2}$.  
    The quantity $\tilde L$ upper bounds $L(\theta)$ in \Cref{cor: asymptotic lower bound} using the fact that $\norm{\dop_\theta \VEC \bmat{A(\theta) & B(\theta)} v} = \norm{v} = 1$ for all $v \in \boldsymbol \Delta$. 
    % The denominator $L$ from \Cref{cor: asymptotic lower bound} may be bounded by $\bar L$, as in \Cref{prop: system theoretic}. 
    
       %\norm{\dop_{\theta} \VEC K(\theta) V}_F^2 \geq d_x d_u \inf_{\Delta} \norm{d_\Delta K(\theta)}_F^2 \geq \frac{d_x d_u \sigma_{\min}(A_{cl})^2 \sigma_{\min}(P)^2}{\norm{B^\top P B + R}^2 \norm{\bmat{-K & I}}^2}. 
    \end{proof}


\subsection{Proof of \Cref{prop: exponential}}
\begin{proof}
We apply \Cref{cor: asymptotic lower bound}. For the perturbation $V = \VEC \bmat{0 & B/\norm{B}_F}$, $L(\theta)$ reduces to $8 \sigma_{\tilde u}^2$. Additionally, the directional derivative becomes
\ifnum\value{cdc}>0{
\begin{align*}
    d_V K(\theta) &= \frac{1}{\norm{B}_F}\Psi^{-1} (B^\top P A_{cl} + B^\top P B K \\ &  + B^\top \texttt{dlyap}(A_{cl}, \textrm{sym}(A_{cl}^\top P B K)) A_{cl}).
\end{align*}
}\else{
\begin{align*}
    d_V K(\theta) = \frac{1}{\norm{B}_F}(B^\top P B + R)^{-1} (B^\top P A_{cl} + B^\top P B K + B^\top \texttt{dlyap}(A_{cl}, \textrm{sym}(A_{cl}^\top P B K)) A_{cl}).
\end{align*}}\fi
As $R = 1$, we may express $K = -B^\top P A_{cl}$. We also note that $\norm{B}_F = 1$. Therefore,  
\ifnum\value{cdc}>0{
\begin{equation}
\label{eq: integrator derivative}
\begin{aligned}
    d_V K(\theta) &= \Psi^{-1} \big(B^\top P A_{cl} - B^\top P B B^\top P A_{cl}  \\
    &\qquad - 2 B^\top \texttt{dlyap}(A_{cl}, A_{cl}^\top P B B^\top P A_{cl}) A_{cl} \big) \\
    &= \Psi^{-1} \big(-  B^\top \big(P (P_{\dx\dx} - 1) \\
    &\qquad +  2\texttt{dlyap}(A_{cl}, A_{cl}^\top P B B^\top P A_{cl})\big)A_{cl} \big),
\end{aligned}
\end{equation}
}\else{
\begin{equation}
\label{eq: integrator derivative}
\begin{aligned}
    d_V K(\theta) &= (B^\top P B + R)^{-1} (B^\top P A_{cl} - B^\top P B B^\top P A_{cl}  - 2 B^\top \texttt{dlyap}(A_{cl}, A_{cl}^\top P B B^\top P A_{cl}) A_{cl}) \\
    &= (B^\top P B + R)^{-1} (-  B^\top \paren{P (P_{\dx\dx} - 1) +  2\texttt{dlyap}(A_{cl}, A_{cl}^\top P B B^\top P A_{cl})}A_{cl} ),
\end{aligned}
\end{equation}}\fi
where the second equality follows by noting that the quantity $B^\top P B$ in the second term is equal the lower right scalar entry of $P$, denoted by $P_{\dx\dx}$ and then pulling out $-B^\top$ on the left, and $A_{cl}$ on the right.
Now, to lower bound $G$ from \Cref{cor: asymptotic lower bound}, we begin with the bound on $G$ in \eqref{eq: F LB exp},
\ifnum\value{cdc}>0{
\begin{align*}
    G & \geq \tr(\Psi d_V K(\theta) d_V K(\theta)^\top)  \\
    &\overset{(i)}{\geq} \frac{1}{(P_{nn}+1)} || B^\top  \big(P (P_{\dx\dx} - 1) \\ &\qquad+  \texttt{dlyap}(A_{cl}, A_{cl}^\top P B B^\top P A_{cl})\big)A_{cl} ||_F^2 \\
    &\overset{(ii)}{\geq} \frac{(P_{\dx\dx}-1)^2}{(P_{\dx\dx}+1)} \norm{B^\top  P A_{cl}}_F^2  = \frac{(P_{\dx\dx}-1)^2}{(P_{\dx\dx}+1)} \norm{K}^2.
\end{align*}
}\else{
\begin{align*}
    G & \geq \tr((B^\top P B +R) d_V K(\theta) d_V K(\theta)^\top)  \\
    &\overset{(i)}{\geq} \frac{1}{(P_{nn}+1)} \| B^\top  \paren{P (P_{\dx\dx} - 1) +  \texttt{dlyap}(A_{cl}, A_{cl}^\top P B B^\top P A_{cl})}A_{cl} \|_F^2 \\
    &\overset{(ii)}{\geq} \frac{(P_{\dx\dx}-1)^2}{(P_{\dx\dx}+1)} \norm{B^\top  P A_{cl}}_F^2  = \frac{(P_{\dx\dx}-1)^2}{(P_{\dx\dx}+1)} \norm{K}^2.
\end{align*}}\fi
Here, $(i)$ follows by substituting $d_V K(\theta)$ for the expression in \eqref{eq: integrator derivative}, pulling out the remaining $\Psi^{-1} = (P_{\dx\dx}+1)^{-1}$ term out of the trace, and writing the trace as a Frobenius norm. The inequality $(ii)$ follows by the fact that $A_{cl} \paren{P (P_{\dx\dx} - 1) +  \texttt{dlyap}(A_{cl}, A_{cl}^\top P B B^\top P A_{cl})} A_{cl}^\top \succeq A_{cl} P (P_{\dx\dx}-1) A_{cl}^\top$. 
We next show that $\norm{K}$ does not scale inversely with the exponential of the system dimension. In particular, we show $\norm{K} \geq \frac{\rho}{2}$.
To see that this is so, note that
\begin{align*}
    \norm{K} &= \norm{B^\top P A_{cl}} \geq \norm{B^\top A_{cl}} \\
    & = \norm{\bmat{0 & \hdots & 0 & \rho} + K} \geq |K_{\dx} + \rho|. 
\end{align*}
Thus $\norm{K} \geq \max\curly{|K_{\dx}|, |K_{\dx}+\rho|} \geq \rho/2$. Therefore, our excess cost is lower bounded as 
    \begin{align*}
        \liminf_{N \to \infty} \sup_{\theta' \in \mathcal{B}(\theta, N^{-\alpha}) } N \mathsf{EC}_T^\pi(\theta')   \geq \frac{G}{8 T  L(\theta)} \geq \frac{\rho^2}{64T  \sigma_{\tilde u}^2} \frac{(P_{\dx\dx}-1)^2}{(P_{\dx\dx}+1)}
    \end{align*}
    By Lemma E.4 in \cite{tsiamis2022learning}, $P_{\dx\dx} \geq  4^{\dx-2}$. Then for $\dx \geq 3$, the quantity above is lower bounded by $\frac{\rho^2}{256T \sigma_{\tilde u}^2} 4^{\dx-2}$.
\end{proof}

\subsection{Proof of \Cref{prop: system theoretic}}


\begin{proof}
    Begin with the result of \Cref{cor: asymptotic lower bound} so that 
    \[
        \liminf_{N \to \infty} \sup_{\theta' \in \mathcal{B}(\theta, N^{-\alpha}) } N \mathsf{EC}_T^\pi(\theta')   \geq \frac{G}{8 TL(\theta)}.
    \]
    The denominator $L(\theta)$ may be bounded by $\tilde L$, as in \Cref{prop: dimensional dependence}. 
    % The quantity $\bar L$ upper bounds $L$ in \Cref{cor: asymptotic lower bound} using the fact that $\norm{\dop_\theta \VEC \bmat{A(\theta) & B(\theta)} v} = \norm{v} = 1$ for all $v \in \boldsymbol \Delta$. 
    To bound $G$, begin with the expression in \eqref{eq: G system theoretic}. By the fact that $A_{cl} \Sigma_X A_{cl}^\top = \Sigma_X - \Sigma_W$, this expression is lower bounded by 
    \ifnum\value{cdc}>0{
    \begin{align*}
        % &\tr((B^\top P B+R)^{-1} B^\top \dlyap\paren{A_{cl}, P} A_{cl} \Sigma_X A_{cl}^\top \dlyap\paren{A_{cl}, P} B) \\
        &\lambda_{\min}(\Sigma_X - \Sigma_W) \tr(\Psi^{-1} B^\top \dlyap\paren{A_{cl}, P}^2 B) \\
        &= \lambda_{\min}(\Sigma_X- \Sigma_W) \tr(\Psi^{-1} B^\top P^{1/2} P^{-1/2} \\ &\qquad \cdot\dlyap\paren{A_{cl}, P}^2 P^{-1/2} P^{1/2} B).
    \end{align*}
    }\else{
    \begin{align*}
        % &\tr((B^\top P B+R)^{-1} B^\top \dlyap\paren{A_{cl}, P} A_{cl} \Sigma_X A_{cl}^\top \dlyap\paren{A_{cl}, P} B) \\
        &\lambda_{\min}(\Sigma_X - \Sigma_W) \tr((B^\top P B+R)^{-1} B^\top \dlyap\paren{A_{cl}, P}^2 B) \\
        &= \lambda_{\min}(\Sigma_X- \Sigma_W) \tr((B^\top P B+R)^{-1} B^\top P^{1/2} P^{-1/2} \dlyap\paren{A_{cl}, P}^2 P^{-1/2} P^{1/2} B).
    \end{align*}}\fi
    Now, let $P^{1/2} B = U \Sigma V^\top$. Then $B^\top P B = V \Sigma^2 V^\top = V \Lambda_{B^\top P B} V$. The trace above then reduces to 
    \begin{align*}
        &\tr(\Sigma^2 (\Sigma^2 + \Lambda_R)^{-1} U^\top P^{-1/2}\dlyap\paren{A_{cl}, P}^2 P^{-1/2}  U) \\
        &\geq \inf_{i \in [\du]} \frac{\Sigma_{ii}^2 \trace(U^\top P^{-1/2}\dlyap\paren{A_{cl}, P}^2 P^{-1/2} U)}{\Sigma_{ii}^2 + \Lambda_{R, ii}}  \\
        &\geq \inf_{i \in [\du]} \frac{\Sigma_{ii}^2 \sum_{j=1}^{du} \lambda_{n-j} (P^{-1/2}\dlyap\paren{A_{cl}, P}^2 P^{-1/2})}{\Sigma_{ii}^2 + \Lambda_{R, ii}}  \\
        &\geq \inf_{i \in [\du]} \frac{\Sigma_{ii}^2 \sum_{j=1}^{du} \lambda_{n-j} (\dlyap\paren{A_{cl}, P}) }{\Sigma_{ii}^2 + \Lambda_{R, ii}} .
    \end{align*}
\end{proof}