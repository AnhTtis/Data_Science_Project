\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote.
% If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{xspace}

% \usepackage[monochrome]{color}

\usepackage{tablefootnote}
\setlength{\skip\footins}{0.2cm}

\definecolor{framecolor}{HTML}{E3E3E3}

\usepackage[framemethod=tikz]{mdframed}
\mdfdefinestyle{remarkstyle}{
backgroundcolor=framecolor,
roundcorner=2pt,
innerleftmargin=2pt,
innerrightmargin=2pt,
innertopmargin=2pt,
innerbottommargin=2pt,
linewidth=0.5pt,
}

\newcounter{summ}[section]
\newenvironment{summ}{\refstepcounter{summ}
\begin{mdframed}[style=remarkstyle]
\noindent \textbf{Summary of Findings~\thesumm}: \em
}
{
\end{mdframed}
\vspace{-1mm}
}

\newcommand{\mypara}[1]{\vspace{2pt}\noindent{\textit{\textbf{#1}}}\xspace}

\usepackage{wasysym}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\yes}{\CIRCLE}
\newcommand{\no}{\Circle}
\newcommand{\half}{\LEFTcircle}

\newcounter{takeawyacount}
\setcounter{takeawyacount}{1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Shedding Light on Static Partitioning Hypervisors for Arm-based
Mixed-Criticality Systems }

\author{
\IEEEauthorblockN{Jos√© Martins}
\IEEEauthorblockA{\textit{Centro ALGORITMI/LASI, Universidade do Minho} \\
jose.martins@dei.uminho.pt}
\and
\IEEEauthorblockN{Sandro Pinto} 
\IEEEauthorblockA{\textit{Centro ALGORITMI/LASI, Universidade do Minho} \\
sandro.pinto@dei.uminho.pt}}

\maketitle
\begin{abstract}
In this paper, we aim to understand the properties and guarantees of static partitioning hypervisors (SPH) for Arm-based mixed-criticality systems (MCS). To this end, we performed a comprehensive empirical evaluation of popular open-source SPH, i.e., Jailhouse, Xen (Dom0-less), Bao, and seL4 CAmkES VMM, focusing on two key requirements of modern MCS: real-time and safety. The goal of this study is twofold. Firstly, to empower industrial practitioners with hard data to reason about the different trade-offs of SPH. Secondly, we aim to raise awareness of the research and open-source communities to the still open problems in SPH by unveiling new insights regarding lingering weaknesses. All artifacts will be open-sourced to enable independent validation of results and encourage further exploration on SPH. 

%, focusing on \textcolor{red}{three key requirements of modern MCS: real-time, safety, and security}. With our study, we have collected empirical evidence and identified several weaknesses that raise concerns \textcolor{red}{about the real guarantees} provided by these systems in the context of mixed-criticality applications.    
%\textcolor{red}{In this paper, we shed light on open-source static partitioning hypervisors for Arm Cortex-A platforms. Despite the existence of multiple reports, research papers, and public artifacts, this information tends to be scattered and, in some cases, empirical evidence is non-existent. Thus, it is difficult to obtain a comprehensive understanding of the overall properties and behavior of these systems in the context of MCS. To fill this gap, we assessed, evaluated, and compared four open-source static partitioning virtualization solutions: Jailhouse, Xen (Dom0-less), Bao, and the seL4 CAmkES VMM. We carry out an extensive empirical evaluation spanning performance, interrupt latency, inter-VM communication, boot time, and code size.}  
%, identify the strengths and weaknesses affecting these systems, and \textcolor{red}{discuss the main challenges that lie ahead of us}
\end{abstract}

\begin{IEEEkeywords}
Virtualization, Static Partitioning, Hypervisor, Mixed-Criticality, Arm.
\end{IEEEkeywords}
\vspace{-0.125cm}

%-------------------------------------------------------------------------
\section{Introduction} \label{sec:intro}

\par The explosion in the number of functional requirements in industries such as automotive has led to a trend for centralized architectures that consolidate heterogeneous software stacks in high-performance platforms \cite{Cerrolaza2020, Staron2021}. These typically take the form of mixed-criticality systems (MCSs) \cite{Burns2017, Esper2018} as they often integrate safety- or mission-critical workloads with real-time requirements, alongside Unix-like operating systems (OSs) providing rich functionality. Virtualization technology is the \textit{de facto} enabler for these architectures as, by definition, it allows for consolidation with strong fault encapsulation.
In this context, hypervisor design must balance, on one side, minimality for safety and security, and feature-richness and efficient sharing of resources on the other. While traditional hypervisors were optimized for the latter \cite{Hwang2008,Dall2014}, on the opposite end of the spectrum we have static partitioning hypervisors (SPH) specifically designed for MCS \cite{Ramsauer2018, Martins2020}. Besides statically assigning system resources (e.g., CPUs, memory, or devices) to virtual machines (VMs), SPH must provide latency and isolation guarantees at the microarchitectural level to comply with the freedom from interference requirements of industry safety standards such as ISO 26262 \cite{VanderLeest2015, BURGIO2017299, Cerrolaza2020, Falk2021}.


%executed by RTOSes or baremetal applications, and large Unix-like OSes providing rich functionality

% \par \textcolor{red}{The modern explosion of functional requirements for cyber-physical systems such as automobiles, has led to an insatiable demand for computing capacity and performance in high-end embedded platforms. This demand is being met by tier-2 suppliers that have provided the market with powerful heterogeneous platforms endowed with general purpose multi-core application processors and microcontrollers, domain-specific accelerators and programmable hardware fabrics, all interwoven by the same memory hierarchy interconnects. 
% The abundance of resources coupled with the presssure of SWAP-C (Size, Weight, Power and Cost). constraints has given place to centralized \cite{Staron2021} architectures that consolidate multiple subsystems onto the same hardware substrate. 
% Because some of the integrated subsystems may be safety- and/or mission-critical, these are often called mixed-criticality systems (MCSs) \cite{?}. Thefore, the deployment of MCSs requires compliance with safety (e.g., ISO26262) and security (e.g., ???) standards which imply a small trusted computing base (TCB) and strong isolation, both spatial and temporal as well as tight real-time.}

%\par \textcolor{red}{Virtualization, an well-established technology for the cloud, emerges as an obvious solution for enabling centralized architectures: by definition, it allows the consolidation and integration of legacy software stacks while providing a minimum degree of spatial isolation and fault containment. In this regards, hypervisor design must balance the trade-offs of on one side, minimality and straightforward partitioning for safety and security, and feature-richness and sharing for efficient resource utilization on the other. While traditional hypervisors were optimized for the latter, in the other end of the spectrum we find static partitioning hypervisors (SPH) which are designed specifically for mixed-criticality use-cases. Ideally, an SPH is just a thin software layer that partitions all system resources (i.e., CPUs, memory, peripherals, interrupts) at initialization time and does not have active role at run time.} 
%To achieve this, it must employ a 1-to-1, vCPU-to-pCPU mapping, passthrough devices and interrupts to virtual machines (VMs) and setup all nested page-tables upfront, i.e., there is no demand paging pr dynamic memory allocation. This enables it to achieve a minimal implementation that is suitable for certification, has narrow attack surfaces and incurs minimal overheads, especially with regards to latency.

%Although a hypervisor can be specifically designed as an SPH, other architectures can be adapted and configured to achieve the same ???. 

%The use of virtualization for static partitioning has been proposed in academia for a long time \cite{?} and its use as a baseline for, the proposal of and its real-world application in industry as been growing in popularity in the last decade \cite{?} Furthermore, besides providing logical isolation, one important feature of SPH is to provide some degree of microarchitectural isolation, mainly at the memory hierarchy level by applying cache, DRAM bank and bandwidth partitioning techniques \cite{?}, to guarantee true freedom-from-interference as imposed by industry standards such as ISO26262 \cite{?}.
%In this domain, Arm is one of the dominant architectures. Although the latest version of the architecture provides good support for virtualization, it still lacks the capabilities

%We have conducted an empirical study of four hypervisors, that either are specifically designed following this architecture, Bao and Jailhouse, or can be configured to approximate it, Xen Dom0-less and seL4's extended with CAmkES VMM. Our study We start by analyzing performance overhead of such hypervisors and conclude that Our results show ...


\par In this paper, we shed light on open-source SPH for Arm-based MCS. Despite the existence of multiple reports, research papers, and public artifacts, information on these systems tends to be scattered or focus on a single hypervisor or metric, while, in some cases, empirical evidence is non-existent. Thus, it is difficult to obtain a comprehensive understanding of the overall properties and guarantees of these systems in the context of MCS. To fill this gap, we conduct a leveled playing-field evaluation of four open-source static partitioning virtualization solutions, i.e., Jailhouse, Xen (Dom0-less), Bao, and the seL4 CAmkES virtual machine monitor (VMM). We drive our study based on two key requirements of modern MCS, i.e., real-time and safety, focusing on (i) performance, (ii) interrupt latency, (iii) inter-VM communication, (iv) boot time, and (v) code size. For each metric, we assess the effectiveness of the cache coloring technique, pervasive in SPH, for inter-VM interference mitigation.

\par The goal of this study is twofold.  Firstly, we aim at empowering industrial practitioners with hard data to understand the trade-offs and limits of modern Arm-based SPH as well as how best configure these systems for their use case and requirements. For example, the use of superpages significantly decreases the number of TLB misses, resulting in negligible performance overhead ($<$1\% without interference), but it is precluded by enabling page coloring, a widely adopted cache partitioning technique in these hypervisors. Also, experiments demonstrated that coloring, per se, can impact the performance up to 20\% and that it cannot fully mitigate interference, where overhead can still reach up to 60\%. Regarding inter-VM communication, we show that for bulk data transfers, buffer size choice is crucial to maximize throughput while avoiding degradation due to inter-VM interference.
%Surprisingly, the CAmkES-VMM lacks support for superpages, resulting in a worst-case overhead of up to 7\%. 


\par Secondly, with the collected empirical data, we aim at raising awareness of the research and open-source communities to the still open problems in SPH, by highlighting both new and previously known weaknesses for these SPH, which seem to be mostly due to interrupt virtualization issues. Prominent examples include: (i) the need for implementing state-of-the-art mechanisms to fully mitigate inter-VM interference (e.g., memory throttling) in mainstream SPH; (ii) the extent of the impact of interference on interrupt latency, which can increase by several orders of magnitude; (iii) the lack of support for correctly handling and delivering interrupts in priority order; (iv) the absence of mechanisms that prioritize the boot of a critical VM; and (v) the lack of plasticity of the SPH architecture which might hinder achieving its own goal of allowing full IO passthrough. To address observed shortcomings, we discuss potential solutions and research directions. 
%For example, we advocate for additional research on extrinsic sources of interference and respective mechanisms and on minimizing interrupt latency and guaranteeing interrupt priority.

\par We made all artifacts openly available \cite{shedlight_artifact} to enable practitioners and researchers with the methods and materials to (i) independently replicate all experiments and corroborate assessed results, as well as (ii) encourage and facilitate additional experiments and further exploration of SPH.       

%From our study, we derived a set of 

%From our analysis, supported by an extensive empirical evaluation of performance, interrupt latency, inter-VM communication, boot time, and code size and TCB, we have gained multiple insights into the strengths and weaknesses of SPH, and the challenges that lie ahead.  


% \par One first observation is that guest \textit{performance overhead is highly impacted by the use of superpages and interference}. The use of superpages significantly decreases the number of TLB misses, resulting in negligible performance overhead ($<$1\%). Surprisingly, seL4-VMM lacks support for superpages, resulting in a worst-case overhead of up to 7\%. Under memory interference from other VMs this performance degradation grows larger than 100\%. We also concluded that interference mitigation mechanisms available in the mainline version of these hypervisors are limited to cache coloring. Also, our experiments demonstrated that coloring, per se, can impact the performance up to 20\% and that it cannot fully mitigate interference, where overhead can still reach up to 60\%. We also clearly observed that the coloring of the hypervisor has no meaningful effect on minimizing interference. 
% % (e.g., coloring)
% %mechanisms to minimize 
% %that existing interference mechanisms fall short ... and
% %Interference caused by the concurrent execution of multiple VMs can impact the performance up to 120\%, which can be contained with mitigation techniques such as the cache coloring.  ... precludes the use of superpages, and ...


% \par A second and very meaningful observation is that \textit{interrupt latency can be impacted by orders of magnitude due to virtualization overheads}. We observed an average interrupt latency increase of 4-5x for Bao and Jailhouse, 15x for Xen, and 47x for seL4-VMM. These results are highly inflated when the system is under interference, resulting in increases of 140x for Xen and 430x for seL4-VMM (with large variance). \textcolor{red}{We argue the assessed results raise reasonable concerns about the real-time guarantees and real applicability of these hypervisors for mixed-criticality applications}. We also observed that mechanisms to minimize interference can reduce the interrupt latency, but we highlight the use of the interrupt direct injection (Jailhouse and Bao) as a notable technique that brings latency near to native. We also investigate the hypervisors' guarantees on interrupt priority and concluded that only Bao and Xen deliver interrupts in the correct priority order. 


% \par \textcolor{red}{Thirdly, we remark that hypervisor \textit{boot time and code size and TCB are intrinsically correlated to the hypervisor's design and architecture}. Jailhouse and Bao were designed for the static partitioning use case and naturally have a smaller code base. For Jailhouse, the TCB is arguably several orders of magnitude larger, given its dependency on Linux. This design decision highly impacts the boot time of a single VM, which increases by approximately four seconds when compared to Bao. Surprisingly, we observed that the time to boot a critical guest (FreeRTOS) for a dual-guest configuration in Bao and Xen is almost identical to Jailhouse's.} 
% %For seL4-VMM, we call the attention for the erroneous common assumption about the formal proofs: only the kernel is formally verified, not the user-space components such as the CAmKES VMM. 
% %To conclude, we highlight the fact that Bao presents both the smallest boot time and code size \& TCB.


% \par \textcolor{red}{Although most of these solutions are used in commercial products for embedded and automotive applications, we argue that, by adopting state-of-the-art techniques, open-source SPH could provide stronger real-time, safety, and security guarantees. We discuss the main weaknesses affecting these SPH and point to specific research directions that would contribute to enhancing static partitioning virtualization, in general, and the evaluated SPH, in particular.}


In summary, this paper makes the following contributions: (1) presents the most comprehensive empirical study to date on popular open-source SPH focusing on a set of key metrics for modern MCS; (2) provides hard empirical data to empower industrial practitioners with the knowledge to understand the limits and trade-offs of SPH; (3) raises awareness of the research and open-source communities to the open problems of SPH by shedding light on their shortcomings; and finally, (4) opens all artifacts to enable independent validation of results and facilitate further research.  


%evaluates the performance overhead and the impact of interference (Section \ref{sec:perf}); (2) assesses virtualization overhead on the interrupt latency and the hypervisors' guarantees on interrupt priority (Section \ref{sec:irq-latency}); \textcolor{red}{(3) Add VM communication ... }; (4) presents measurements of the hypervisor and system boot time (Section \ref{sec:boot}); \textcolor{red}{(5) analyzes the code size and TCB (Section \ref{sec:code-tcb})}; and \textcolor{red}{(6) discusses the main weaknesses affecting the studied SPH and points for research directions (Section \ref{sec:disc-research})}.  




%-------------------------------------------------------------------------
\section{Background}
\label{sec:back}

In this section, we start by overviewing  Armv8-A virtualization support. We then explain the concept of static partitioning virtualization, including key techniques implemented in SPH. Finally, we describe Xen, Jailhouse, Bao, and seL4 CAmkES.

\subsection{Arm Virtualization}
\label{sec:arm-virt}

%Since version 7 of its architecture, Arm has provided some degree of virtualization support .. which has been extended to more components and spread ... 

\mypara{CPU \& Memory.} Given the widespread proliferation of virtualization in the last decades, Arm implemented hardware support since version 7 of the ISA. The most recent version of the architecture, i.e., Armv8/9-A, extends the privileged architecture with a dedicated hypervisor privilege mode (EL2) which sits between the secure firmware mode (EL3) and the kernel/user modes (EL1/EL0) \cite{arm_virt} where guests execute. A hypervisor running at EL2 has fine-grained control over which CPU resources are directly accessible by guests (e.g., control registers). Access to a denied functionality by a guest OS results in a trap to the hypervisor. It is possible to route specific guest exceptions and system interrupts to EL2. Other resources that can be managed by the hypervisor include the CPU-private generic timer and the performance monitor unit (PMU). 
%The hypervisor can then take action such as halting the guest or emulating a given CPU functionality.
%Hence, the hypervisor is able to take full control of the CPU by preempting guests on well-defined events. 
%\mypara{Memory.} In Armv8-A, 
EL1/EL0 memory accesses are subject to a second stage of translation which is in full control of the hypervisor \cite{arm_virt}. Any guest access to a memory region not mapped in the second stage of translation will result in a precise trap to EL2. Arm provides multiple ``translation granules", resulting in pages of different sizes: 4 KiB, 16 KiB, and 64 KiB. For each page size it is also possible to map large contiguous memory regions. These are known as superpages (or hugepages), which reduces TLB pressure. The more commonly used 4KiB granule allows for 1GiB and 2MiB superpages. Arm also defines the System Memory-Management Unit (SMMU), that extends memory virtualization mechanisms from the CPU to the bus, to restrict VM-originated direct-memory accesses (DMAs). 
%EL2 has its own separate virtual address space, and 
%This allows the seamless passthrough of DMA-capable devices to VMs, while sharing page-tables between the CPU MMU and the SMMU.
%, i.e., the number of entries used in a TLB for a given working set
%i.e., multi-level page-table configurations
%i.e., to translate the addresses used by device

\mypara{Interrupts.} Arm virtualization acceleration spans the full platform, including the Generic Interrupt Controller (GIC). The GICv2 standard has two main components: a central distributor and a per-core interface. All interrupts are routed first to the distributor, which then forwards them to the interfaces. The distributor allows the configuration of interrupt parameters (e.g., priority, target CPU) and the monitoring of interrupt state, while the interface enables the core management of interrupts. GICv2 provides virtualization support only on the interface; there is a fully virtual interface with which the guests can directly interact without VM exits. The distributor, however, must be fully emulated. Furthermore, all interrupts must first be handled by the hypervisor, which can then inject them in the VM, by writing to GIC list registers (LRs). These registers essentially take the place of the distributor for the virtual interface: when a given interrupt (along with metadata such as priority or state) is present on a register, it is forwarded to the virtual interface. The GICv2 spec limits the number of LRs to a maximum of 16. GICv3 and GICv4 provide support for direct delivery of hardware interrupts to VMs; however, this feature is only implemented for inter-processor interrupts (IPIs) and message-signaled interrupts (MSIs), i.e., interrupts implemented as write operations to special interrupt controller registers and propagated via the system interconnect. Standard wired interrupts, propagated by dedicated side-band signals, are still subject to the mentioned limitation, i.e., hypervisor interrupt injection through the list register.
%, that is, the ``traditional" interrupts propagated by dedicated side-band signals,
%(e.g., read the highest priority pending interrupt)
% and set the priority threshold mask
% or signaling the end of handling for a given interrupt
%using conventional emulation techniques
% (dubbed list registers)

\subsection{Static Partitioning Virtualization (SPV)}

Static partitioning is the practice of, either at build or initialization time, distributing all platform resources to different subsystems. This can be materialized in many shapes and forms, depending on the hardware primitives. Virtualization is a natural enabler for the static partitioning architecture, due to the strong encapsulation guarantees and flexible resource assignment.
%For example, it is possible to use an MPU and platform-level firewall (a.k.a., IOMPU) to limit access to memory and devices \cite{xilinxisol}. This approach adds negligible overhead and has little configuration complexity; however, in Cortex-A platforms, such facilities are not present or lack flexibility.
%, they are exclusively assigned to different subsystems
%In the static partitioning architecture, resources cannot be shared. 
Hypervisors designed for the static partitioning use case (or providing such a configuration) have three fundamental properties: (i) exclusive assignment of virtual CPUs to physical CPUs (i.e., no scheduler); (ii) static allocation, assignment, and mapping of all hypervisor and VM memory at build or initialization time; and (iii) direct assignment of devices to VMs (passthrough) and exclusive allocation of their interrupts to the same VM. To implement this efficiently, these hypervisors are highly dependent on virtualization hardware support both at the CPU and platform level (e.g., SMMU). SPH also have non-functional requirements centered around minimizing interrupt latency and inter-VM interference. Thus, over the past few years, there have been efforts to enhance SPH with mechanisms to address these requirements. These include cache coloring and, analogously to what has been done for x86 \cite{able2012}, direct injection in Arm processors.
Furthermore, it is important for the code base to be minimal and follow industry coding standards (e.g., MISRA); this eases functional safety (FuSa) certification efforts. 
%{ \color{blue} Thus, they do not include drivers besides those to control standard platform-level components such as the interrupt controller. }


%Examples of SPH will be discussed in detail in Section \ref{sec:hyp}. 
%\mypara{Static Partitioning Hypervisors (SPH).} 
%\mypara{Static Partitioning Architecture.} 
%, i.e., there is no dynamic memory allocation or mapping
%such as contention in the memory system
%The provided logical partitioning should be extended to the hypervisor's internal resources, which ideally would be completely statically allocated to each partition or at least from previsouly partitioned memory pools, mainly to avoid any availability issues. 
%an SPH is typically employed in safety-critical systems, 
%although they essentially equate to the concept of a Separation Kernel described by Rushby
%(i.e., directly map MMIO regions in the VMs physical address space) 
%\cite{?}. 

%In summary, it is important for an SPH to be as small and simple as possible while providing the necessary partitioning mechanisms and minimizing critical VMs latencies. Ideally, it would just be a thin layer of software for configuring hardware virtualization/partitioning mechanisms and never execute again. However, there are several factors that hinder this goal in Arm processors including the need to inject interrupts or emulate firmware services such as PSCI.
%From a more realistic perspective, pure static partitioning virtualization is often too strict, especially from a resource utilization point of view, as some CPUs will often sit idle even for VMs witht strict real-time requirements. A more flexible hybrid approach would be to have part of the system resources are statically partitioned while other resources is dynamically multiplexed by the hypervisor by scheduling multiple vCPUs in a single CPU, providing virtual devices and other kinds of services. Guaranteeing thorough isolation and freedom from interference between these two domains would still fit MCS requirements while providing a much more adaptable interface.

%\subsection{MCS Virtualization Enhancement Techniques} \label{sec:virt-techniques}

\mypara{Cache Coloring.} In SPH, VMs still share microarchitectural resources such as the last-level cache (LLC). The behavior and memory access pattern of one VM might result in the eviction of another VM's cache lines, impacting the latter's hit rate and consequently its execution time. Thus, there is the need to partition shared caches assigning each partition to a different VM. While in the past Armv7 processors provided hardware means to apply this partitioning by way of per-master cache-locking, modern-day Arm CPUs do not provide those facilities. A solution is cache coloring, a software technique for index-based cache partitioning \cite{Gracioli2015}. Cache coloring explores the intersection of the virtual addresses' cache index and the page number when creating virtual-to-physical memory mappings. Each color is a specific bit pattern in this intersection that maps only to specific cache sets. Thus, hypervisors can control which cache sets are assigned to a given VM by selecting which physical pages are mapped to it. By exclusively assigning a cache partition (i.e., group of cache sets or colors) to a given VM, cache coloring fully eliminates the conflict misses resulting from inter-VM contention. Cache coloring can also be implemented at the hypervisor level by assigning the hypervisor one or more color(s).
%, avoiding interference between the hypervisor and guest OSes. 

%However, cache coloring has significant drawbacks, the main one being memory waste and fragmentation: to assign half the cache sets to a given VM, one would need to assign it half the available memory, only. Also, to have an intersection between cache index and virtual page number it is typically needed the smallest 4KiB, precluding the benefits of superpages.

%Concurrently running virtual machines on multi-core processors inevitably leads to contention in the memory system, having significant impact on VM performance and predictability. The first level of shared cache is one of the components where this contention is more pronounced. In the past, Arm hardware provided hardware-based per-cpu way locking mechanisms, which essentially allowed to assign cache ways to specific CPUs, effectively partitioning the cache. Because this feature is not available in modern Arm processors, other software techniques have been avoid cache contention, the most ?? one being cache coloring.

\mypara{Direct Interrupt Injection.} Direct interrupt injection is a new technique implemented in Arm-based SPH to eliminate the need of the hypervisor mediating interrupt injection. With this technique, the hypervisor passes through the physical GIC CPU interface and routes all interrupts directly to the VM by configuring the CPU to trigger interrupt traps directly at EL1, i.e., kernel mode. The hypervisor must still emulate the shared distributor to ensure isolation between VMs, i.e., prevent misconfiguration of a given VM interrupts by another VM. This allows physical interrupts to be directly delivered to the VM with no hypervisor intervention, reducing latency to native execution levels. The forfeiting of interrupts should not be a major issue as SPH do not directly manage devices. However, SPH still need to communicate internally using IPIs. Direct interrupt injection implementations address this issue by leveraging standard software-delegated exception interface (SDEI) \cite{arm_sdei} events instead of directly using IPIs. SDEI is implemented by firmware, allowing the hypervisor to register an event during initialization. The hypervisor can then trigger the event by issuing a system call to firmware (via a secure monitor call instruction, SMC), which will result in diverting execution to a predefined hypervisor handler, similarly to Unix signals. 
In reality, firmware maps these events to its own secure reserved IPIs since, as part of TrustZone [16], the GIC provides further facilities to reserve interrupts to EL3.

%analogous to IPI
% without any hypervisor intervention
%circumvent the Arm GIC's limitations requiring the
%Applying this technique severely manage the ability of the hypervisor to manage devices or implement any other kind of functionality dependent on interrupts. Therefore it is only suitable for very simple hypervisors implementing the straightforward static partitioning architecture.

 \begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/sph-arch.pdf}
    \caption{Architectural overview of the assessed hypervisors: Jailhouse, Xen (Dom0-less), Bao and seL4 CAmkES VMM}
    \label{fig:hypervisors}
    \vspace{-0.50cm}
\end{figure*}

\subsection{Static Partitioning Hypervisors (SPH)}

%\par \textcolor{red}{ We should add here a very small paragraph for intro ... add reference to Fig. \ref{fig:hypervisors} ... }.

\mypara{Jailhouse Hypervisor.}  Jailhouse \cite{Ramsauer2017, Ramsauer2018} is an open-source hypervisor developed by Siemens. Unlike traditional baremetal hypervisors, Jailhouse leverages the Linux kernel to boot and initialize the system and uses a kernel module to install the hypervisor. Once Jailhouse is activated, it runs as a baremetal component, taking full control over the hardware. Jailhouse has no scheduler and only leverages the ISA virtualization primitives to partition hardware resources across multiple isolated domains, a.k.a. ``cells". Guest OSes or baremetal applications running inside cells are called ``inmates".
The mainline includes support for x86 and Armv7/8-A, and a work-in-progress RISC-V port\cite{Ramsauer2022}. 
%This strict dependency on Linux to boot and manage the system imposes hard limitations on the porting to upcoming MMU-less platforms (e.g., Armv8-R). 
%\mypara{Real-Time \& Safety.} 
The research community has been actively contributing with mechanisms to enhance predictability, namely: cache coloring, DRAM bank partitioning \cite{Kloda2019}, memory throttling, and device quality of service (QoS) regulation \cite{Sohal2020}. An unofficial fork including these features is available \cite{jailhousert}. Direct injection \cite{Biondi2021} was also implemented. %{\color{red} We should mention jailhouse-rt}


%Jailhouse has gained attention in academia and has extensively benefited in terms of real-time and safety. Kloda et al. \cite{Kloda2019} have implemented support for cache coloring and DRAM bank partitioning. Cache coloring is available upstream but not part of any release to date; DRAM bank coloring mechanism was never upstreamed or made publicly available. Sohal et al. \cite{Sohal2020} leveraged memory throttling and implemented device QoS regulation, but these mechanisms are not in the mainline. The direct interrupt injection was also recently added to the mainline \cite{Biondi2021}. Despite the significant popularity and adoption, we are not aware of any certification efforts from Siemens or other third party.  {\color{red} We should mention jailhouse-rt}
%\mypara{Security.} 
%Jailhouse main focus is on safety, not security. The fact that it relies on Linux (and its 15M SLoC) to boot and manage the VMs, is a well-recognized security concern. Cache side-channel protection can be achieved when cache-coloring is enabled. 
%Jailhouse offers no support or compatibility with TEEs. 
%There is also no public information about the implementation of specific security services such as VM Introspection. 
%\mypara{Miscellaneous.} Jailhouse implements inter-cell communication through shared memory regions. These memory regions are shared between two cells together with a signaling interface. Jailhouse allows guests to share physical pages with the Linux root cell. \textcolor{red}{There are ongoing efforts to ... (Martins add here about VirtIO) .} We are not aware of any public mechanism that implements power management on Arm platforms.


\mypara{Xen (Dom0-less) Hypervisor.} Xen \cite{Hwang2008} is an open-source hypervisor widely used in a broad range of application domains. A key distinct feature of Xen is its dependency on a privileged VM (Dom0) that typically runs Linux, to manage non-privileged VMs (DomUs) and interface with peripherals. Xen was initially designed for servers and desktops, but has found also adoption on embedded applications. For embedded and automotive applications, Xilinx has led the implementation of Xen Dom0-less. With this novel approach, it is possible to have a Xen deployment without any Dom0, booting all guests directly from the hypervisor and statically partitioning the system.
%The first release dates back to 2003. 
%Xen has support for x86, Armv7-A, and Armv8-A, and RISC-V support is in progress.
%PowerPC support was deprecated after Xen 3.2
%\mypara{Real-Time \& Safety.} 
%Xen focus on real-time and safety has been mainly driven by the increasing interest from industry players (e.g., EPAM, Xilinx, Bosch) in using Xen for embedded and automotive applications. 
A patch for guest and hypervisor cache coloring support \cite{Corradi2020} is available. There is also a SIG working towards facilitating downstream FuSa certifications by fostering multiple initiatives within the community including MISRA refactoring, or providing the option of running Zephyr \cite{zephyr_project_2023} as Dom0. Besides Armv8-A, Xen also supports x86, and Armv8-R and RISC-V ports are underway.
%Fault tolerance support in Xen was mainly explored in the context of servers \cite{Nagarajan2007} and introduced in Xen 4.0 as a feature named Remus.
%\mypara{Security.} 
%There are a lot of security features in Xen. TEE support is implemented with the OP-TEE mediator \cite{Babchuk2019}. Cache side-channel protection can also be achieved with coloring. VM Introspection support was included since version 4.5, mainly enabled by the Xen Security Modules (XSM) and the LibVMI library \cite{Lengyel2015}. Xen Dom0-less is also a significant step towards security due to the removed dependency on Linux and reduced code base and attack surface.


\mypara{Bao Hypervisor.} Bao \cite{Martins2020} is an open-source static partitioning hypervisor that was made publicly available in 2020. It implements the pure static partitioning architecture, i.e., a minimal, thin-layer of privileged software which leverages the existing ISA virtualization primitives to partition the hardware. Bao has no scheduler and does not rely on any external libraries or privileged VM (e.g., Linux), consisting on a standalone component which depends only on standard firmware to initialize the system and perform platform-specific tasks such as power management. 
Bao originally targeted Armv8-A \cite{Martins2020}. The mainline now includes support for RISC-V \cite{Sa2021}, Armv7-A, and Armv8-R ports are in the making. 
%The mainline version supports a total of 9 platforms.
%\mypara{Real-Time \& Safety.} 
Bao was specifically designed to provide strong real-time and safety guarantees. It implements hardware partitioning mechanisms to guarantee true freedom from interference, i.e., cache coloring (VM and hypervisor), and direct interrupt injection. There are ongoing efforts to implement memory throttling. 
%We are not aware of any health monitoring, fault tolerance support, or any efforts to achieve or facilitate FuSa qualification or certification.
%in the mainline \ref{}
%\mypara{Security.} 
%Bao developers advocate strong emphasis on security. One of the strongest claims is its minimal TCB. There is also demonstrated protection against cache side-channel attacks \cite{Martins2020_1}. Notwithstanding, to the best of our knowledge, there are no reports of offering protection for other outstanding microarchitectural sources of side-channels. Bao has experimental support for TEEs \cite{Martins2020_1}, but it's not upstreamed yet. 
%\mypara{Miscellaneous.} Write about the security features for Bao here ... It provides minimal inter-VM communication facilities through statically configured shared memory and notifications in the form of interrupts.

\mypara{seL4 CAmkES VMM.} seL4 is a formally verified microkernel \cite{Klein2009}. 
%It provides only the basic mechanisms so that userland components can manage the system and implement functionalities and policies. 
Its design model revolves around the use of capabilities.
%, i.e., tokens encapsulating kernel objects and rights over them. 
%Kernel objects might map directly to hardware resources or represent kernel concepts such as communication (IPC) endpoints, which are also leveraged to deliver interrupts to user space. 
When used as a hypervisor, seL4 executes in hypervisor mode (e.g, EL2) and exposes extra capabilities and APIs to manage virtualization functionality \cite{Heiser2020_1}. A user-level VMM uses its resource capabilities to create VMs. 
%One VMM might be managing multiple VMs, but it is also possible to have configurations with one VMM per VM. 
As of this writing, only the seL4 CAmkES VMM \cite{Klein2018, millwood2020} code is open-source.
%CAmkES is a framework for quickly and reliably building seL4-based systems. 
Each CAmkES VMM manages a single VM. One current issue of the CAmkES VMM is that, although it supports multicore VMs, each VMM runs as a single thread pinned to a single CPU.
%; this means events coming from multiple CPUs have to be serialized. 
seL4 supports x86, Armv7/8-A and RISC-V, but the latter is not supported by CAmkES VMM.
%The microkernel also uses the big kernel lock \cite{Peters2015}. 
In CAmkES, resources are statically allocated to each component using capabilities. 
%Although seL4 is supported in Armv7/8-A, x86, and RISC-V, the CAmkES VMM is limited to few Arm and x86 platforms. Furthermore, because seL4's base mechanisms are heavily dependent on virtual memory, it would be expensive to support MPU-less systems, especially without voiding its formal guarantees.
%\mypara{Real-Time \& Safety.} 
Originally, seL4 provided only a priority-based preemptive scheduler.
% hindering its philosophy of no policy at kernel level. 
The newest MCS kernel extends it with scheduling context capabilities, allowing time management policies to be defined in user space \cite{Lyons2018}. %and opening the opportunity to explore arbitrary real-time scheduling policies and protocols.
%seL4 has been the subject of WCET sound analysis \cite{Blackham2011, Blackham2012, Sewell2017}, which proved all kernel operations are bounded in time, although only for Armv6.
Cache coloring has also been implemented in seL4 \cite{Ge2019}, not only at the user/VM level, but also for the kernel, but it was not publicly available at the time of writing.
%Cache coloring has also been implemented in seL4 \cite{Ge2019}, not only at the user/VM level, but also for the kernel {\color{blue} by cloning the kernel image for each cache partition. However, to the best of our knowledge, it is still experimental and not openly available.}
%A software-based fault tolerance mechanism, redundant co-execution (RCoE), has also been implemented in the kernel \cite{Shen2019}. However, neither coloring nor RCoE has been upstreamed and, to the best of our knowledge, are still work in progress. 
%seL4 formal proofs outpace safety certifications, but since the CAmkES VMM is not formally verified, this can hinder such efforts.
%\mypara{Security.} 
seL4 has formal proofs for its specification, implementation from C to binary, and security properties \cite{Murray2013, Klein2014}. 
%that guarantee isolation between the user-space components. 
%However, these proofs are not fully finished for certain kernel configurations (e.g., multicore) and do not cover the CAmkES VMM.
There are also ongoing efforts to extend the formal verification to prove the absence of covert timing channels \cite{Heiser2020}. 
%A proof of concept component providing VM introspection facilities is also available as part of CAmkES.
%, essentially precluding the need for TEEs
Finally, CAmkES is being deprecated in the near future in favor of the seL4 Core Platform (seL4CP) \cite{Heiser2022}. seL4CP will also provide support for per-VM user-mode VMMs\footnote{Only after the bulk of this work was carried out, virtualization support in seL4CP was made openly available. At the time of writing, it still appears to be in a beta stage and not as mature as CAmkES.} while promising to alleviate the performance overhead of CAmkES.

%-------------------------------------------------------------------------
\section{Methodology and Experimental Setup} \label{sec:meth-setup}

%\par In this section, we describe the methodology and the experimental setup.

\subsection{Methodology} \label{subsec:meth}
\vspace{-0.13cm}
\mypara{Selected Hypervisors.} We have selected four open-source SPH (Fig.\ref{fig:hypervisors}). Jailhouse and Bao were designed for the static partitioning use case; both are open-source and target Arm platforms. Xen Dom0-less is a novel deployment that allows directly booting multiple VMs (bypassing Dom0) and passthrough of peripherals to VMs. Finally, seL4 is a well-established open-source microkernel, which can be used as a hypervisor in combination with a user-level VMM. The seL4 CAmkES VMM is an open-source reference VMM implementation with static allocation of resources. These systems are actively maintained, adopted for commercial purposes, and there is a fair amount of information about them. We have excluded other open-source SPH that do not support Armv8-A (e.g., Quest-V, ACRN), and other popular open-source hypervisors that don't explicitly target static partitioning (e.g., KVM, Xvisor). We have excluded microkernels such as NOVA \cite{Steinberg2010} due to the lack of availability of an open-source reference user-space VMM, and because we believe seL4 serves as a faithful representative of the microkernel architecture. TrustZone-assisted hypervisors \cite{Pinto2017, Martins2017, Pinto2019a} were left out due to multicore scalability issues and lack of active maintenance.  %\textcolor{red}{Add RTZVisor \cite{} and NOVA \cite{} ...} 
Finally, we have excluded commercial products (e.g., PikeOS, LynxSecure) as these often require licenses the authors did not have access to (hindering the sharing of artifacts, which we believe is crucial for the viability of this study).

\mypara{Empirical Evaluation.} The evaluation focuses on performance, interrupt latency, inter-VM communication latency and bandwidth, boot time, and code size. We also assess the effect of interference and of the available mitigation mechanism (i.e., cache coloring). Although we consider virtual device performance, IO interference, and applied security techniques such as stack canaries or guards, data execution prevention or control-flow integrity very relevant, these are out of scope of this work. We advocate for a follow up study as future work.

\subsection{Experimental setup} \label{subsec:setup} 
\vspace{-0.15cm}
\mypara{Hardware Platform.} Experiments were carried out on a Xilinx ZCU104, featuring a Zynq Ultrascale+ SoC. It includes a quad-core Cortex-A53 running at 1.2 GHz, a GIC-400 (GICv2) featuring 4 list registers, and an  MMU-500 (SMMUv2).
Cores have private 32KiB separate L1 instruction and data caches, and share a L2 1MiB unified cache. It also includes a programmable logic (PL) component (i.e., FPGA).

\mypara{Hypervisors configuration.} We made an effort to use the latest versions of each SPH. Still, we applied a few patches to Jailhouse, Xen, and Bao to include features such as coloring or direct injection, which are not yet fully merged. Further, we had to make small adjustments to all SPH to enable homogeneous configurations (e.g., uniforming VM memory map), allow direct guest access to PMUs, or instrumenting hypervisors for specific experiments. For each SPH, we leveraged the default configuration for the target SoC, with some tweaked options such as disabling debug and logging features. There were, however, specific adjustments that were made on a per-hypervisor basis. For example, to remove or minimize the invocation of a scheduler in Xen, we used the null scheduler and disabled trapping of wait-for-interrupt (WFI) instructions; in seL4, since it was not possible to disable the timer tick, we configured the tick with a period of about 5 seconds. We compiled all hypervisors with GCC 11.2, with the default optimization level defined by each hypervisor's build system. All these SPH configurations and modifications are available and clearly discernible in the provided artifact \cite{shedlight_artifact}.
%We also bypass user-space setup in the root cell and simply run a script as the init process to install the hypervisor and instantiate the guests.
%In the case of Xen, to the same effect, we needed to modify a few lines of code. 
% (minimize timer overhead on guest execution)



\mypara{VM configuration.} VM configurations are as similar as possible, mainly w.r.t. number of vCPUs and memory. For Jailhouse and seL4-VMM, where memory must be manually allocated, we set memory regions aligned to 2 MiB. The only device assigned to each VM is a UART. We evaluated two different classes of VMs: (i) large VMs running Linux (v5.14), as representative of rich, Unix-like OSs; and (ii) small VMs running baremetal applications or FreeRTOS (v10.4), as representative of critical workloads. When cache coloring is enabled, we assign half of the colors (four out of eight\footnote{We consider only eight cache colors while, in truth, the target platform allows for 16. We do this to avoid color assignment configurations that would partition the L1 cache.}) to the VM executing the benchmark, three colors to the interference application, and one color to the hypervisor (just supported in Bao and Xen). Note that color assignment configuration can significantly impact the final measurements for all metrics. In real deployments, the color assignment should be carefully defined based on the profile of the final system.
%, to output experiments results
%, to maximize the use of superpages
%, where use the same vanilla Linux 5.14 kernel image configured with a built-in ramfs
%to understand its efficacy on mitigating the effects of inter-VM interference

\mypara{Interference Workload.} When evaluating memory hierarchy interference, we use a custom baremetal guest which continuously writes a buffer with the size of the LLC (1MiB). Unless noted otherwise, this interference guest runs on a VM with two vCPUs. We stress that although parameterized to cause a significant level of interference, the observed effects cause by the interference workload do not necessarily reflect the worst case that could be achieved if further fine-tuned.

\mypara{Measurement tools.} We use the Arm PMU to collect microarchitectural events on benchmark execution. The selected events include instruction count, TLB accesses and refills, cache access and refills, number of exceptions taken, and number of interrupts triggered; we register the exception level on which these events occur. For the Linux VMs, we use the perf tool \cite{perf} to measure the time and to collect microarchitectural events. For baremetal or RTOS VMs, we use the Arm Generic Timer, with a resolution of 10 ns, and a custom PMU driver.
%This is key to differentiating between hypervisor and guest events. 
%for collecting microarchitectural events.
% A notable exception is the seL4-VMM, which runs in EL0 such as the guest. 


%\textcolor{red}{This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract. This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes.}


\subsection{Threats to validity} \label{subsec:threads} 

Experiments were independently conducted by two researchers. Each used a different ZCU104 platform and pre-agreed VM configurations (cross-checked). We have contacted key individuals and/or maintainers as representatives of each SPH community. We have received replies from all of them, which led to a few iterations and repetition of some experiments. Overall, the comments and issues raised by these individuals are reflected in the presented ideas and results. Despite all efforts, these experiments may still be subject to latent inaccuracies. We will open source all artifacts to enable independent validation of the results.
%In general, these individuals were receptive to the idea of replicating the experiments in the near future. 
This study may also include limitations on the generalization to other platforms. For the hardware platform, we argue both the SoC (Zynq Ultrascale+) and the Cortex-A53 are representative of others used in automotive and industrial settings (e.g., NXP i.MX8 or Renesas R-Car M3). To corroborate this, we have also carried out the performance and interrupt latency experiments for the Bao hypervisor in an NXP i.MX8QM, which features the GIC-500 (GICv3). The obtained results are fully consistent with those presented in Sections \ref{sec:perf} and \ref{sec:irq-latency}. Furthermore, we argue next generation platforms, such as i.MX9 featuring Cortex-A55 CPUs, implement very similar microarchitectures.
%and the evolution of the SPH
%For the SPH, we made an effort to provide a view of the latest version of these systems; by open sourcing all artifacts, we hope to facilitate further exploration as these SPH evolve.
%Notwithstanding, we have also conducted a subset of the experiments on a platform featuring out-of-order CPUs (Cortex-A72), ... (which we omit due to lack of space)
%For the seL4 CAmkES VMM, we have received generic feedback which is reflected in the paper. From this feedback, we stress their view on the fact that CAmkES framework ``is pretty sub-optimal in many ways", and potentially the reason for such large overheads. 


%-------------------------------------------------------------------------

%\newpage

%\section{SPH: Qualitative Comparison} \label{sec:hyp}

%\par In this section, we describe the selected SPH (Fig. \ref{fig:hypervisors}) and establish a qualitative comparison (Table \ref{tab:gap}) focusing on three key requirements: real-time, safety, and security.

%\subsection{Jailhouse Hypervisor} 

%\subsection{Xen (Dom0-less) Hypervisor} 

%\subsection{Bao Hypervisor}~

%\subsection{seL4 CAmkES VMM} 

% \begin{table}[!t]
% \caption{High-level qualitative comparison of selected hypervisor features and mechanisms: \yes\xspace indicates that the system fares positively, \no\xspace that it fares negatively.}
% \label{tab:gap}
% \centering
% \resizebox{.95\linewidth}{!}{
%     \begin{tabular}{@{}llcccc@{}}
%     \toprule
%         &     & \textbf{Jailhouse} & \textbf{Xen} & \textbf{Bao} & \textbf{seL4-VMM} \\ 
%     \midrule
%                       & Scheduling                & \no   & \yes  & \no   & \yes \\
%     Design/Arch       & No Privileged VM         & \no   & \half & \yes  & \yes \\
%     %                  & Memory Allocation        & ? & ? & ? &             \\  
%                       & Target Archs. / Plat.    & \yes & \yes  & \half & \half \\
%                       & MMU-less (MPU)           & \no   & \no   & \half & \no   \\
%     \midrule
%                       & Cache Partit.            & \yes  & \yes  & \yes  & \half  \\
%                       & Hyp. Cache Partit.       & \no   & \yes  & \yes  & \half  \\
%     Real-time         & DRAM Bank Partit.        & \half & \no   & \no   & \no    \\
%     \&                & Memory Throttling        & \half & \no   & \half & \no    \\
%     Safety            & Device QoS               & \half & \half & \no   & \no    \\
%                       & Direct Interrupt Inj.    & \yes  & \no   & \yes  & \no    \\
%                       & Health Monitoring        & \no   & \half & \no   & \no    \\
%                       & Fault Tolerance          & \no   & \yes  & \no   & \half  \\
%                       & Certification            & \no   & \half & \no   & \half  \\
%     \midrule
%                       & TEE Support              & \no   & \yes  & \half & \yes   \\
%     Security          & Side-Channel Protec.     & \half & \half & \half & \half  \\
%                       & VM Introspection         & \no   & \yes  & \no   & \half  \\
%     %                  & CFI / DFI                & \no   & \yes  & \no   &  \half \\
%                       & Formal Verification      & \no   & \no   & \no   & \half  \\
%     \midrule
%                       & Inter-VM Communication   & \yes  & \yes  & \half & \yes \\
%     Misc.             & Virtual Devices          & \half & \half & \no   & \yes \\
%                       & Power Management         & \no   & \yes  & \half & \no  \\
%     \bottomrule
%     \end{tabular}
% }
% \vspace{-0.4cm}
% \end{table}

%\begin{table}[!t]
%\caption{Hypervisor support of relevant Arm hardware platforms for automotive and industrial control applications: \yes\xspace indicates that the system has support and \no\xspace indicates that there is any available support.}
%\label{tab:plat}
%\centering
%\box{.98\linewidth}{!}{
%    \begin{tabular}{@{}llcccc@{}}
%    \toprule
%        &     & \textbf{Jailhouse} & \textbf{Xen} & \textbf{Bao} & \textbf{seL4-VMM} \\ 
%    \midrule
%    Xilinx            & Zynq Ultrascale+ ZCU104  & \yes & \yes & \yes  & \yes  \\
%                      & Zynq Ultrascale+ ZCU102  & \yes  & \yes & \yes  & \yes \\
%    %                  & Zynq Ultrascale+ ZCU106  & \half & \yes & \half & \yes \\
%    %                  & Versal VCK190            & \no   & \no  & \no   & \no    \\  
%    \midrule
%    %                  & MCIMX7SABRE              & \no  & \no  & \no  & \no \\
%                      & MCIMX8QM                 & \yes &      & \yes & \no  \\
%    NXP               & MCIMX8M                  & \yes &      & \yes & \yes \\
%                      & IMX8MM-EVK               & \yes &  & \no & \yes         \\  
%    \midrule
%    %                  & Jetson TK1                & \yes  & \no  & \no  & \yes \\
%    Nvidia            & Jetson TX1                & \yes  &      & \no  & \yes  \\
%                      & Jetson TX2                & \yes  &      & \yes & \yes  \\
%    %                  & Jetson Xavier             & \no  &  \no  & \no  & \no  \\
%    \midrule
%    Renesas           & R-Car M3/H3              & \no  & \yes & \no  & \no  \\
%    \midrule
%    %TI                & TMDX654                  & \yes  & \yes & \no  & \no  \\
%    TI                & J7200                    & \yes  & \yes & \no  & \no  \\
%    \bottomrule
%    \end{tabular}
%}
%\end{table}

%\subsection{Conclusion}

%\par Table \ref{tab:gap} provides an abstract view of a qualitative comparison ... Put everything in perspective ...

%-------------------------------------------------------------------------
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{images/mibench-with-absolute.pdf}
    \vspace{-0.2cm}
    \caption{Relative performance degradation for the MiBench Automotive and Industrial Control Suite.}
    \label{fig:mibench}
    \vspace{-0.5cm}
\end{figure*}

%\newpage

\section{SPH: Performance} \label{sec:perf}

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{images/mibench-hyp-l2-miss.pdf}
            \vspace{-0.6cm}
            \caption{Hyp. L2 cache miss per instr.}
            \label{fig:mibench-hyp-l2-miss}
        \end{subfigure}
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{images/mibench-guest-itlb-miss.pdf}
            \vspace{-0.6cm}
            \caption{Guest iTLB miss per instr.}
            \label{fig:mibench-guest-itlb-miss}
        \end{subfigure}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{images/mibench-inst-ratio.pdf}
            \vspace{-0.6cm}
            \caption{Hyp./Guest instr. ratio}
            \label{fig:mibench-inst-ratio}
    \end{subfigure}
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{images/mibench-hyp-excp.pdf}
            \vspace{-0.6cm}
            \caption{Hyp. exceptions per instr.}
            \label{fig:mibench-hyp-excp}
        \end{subfigure}
    % \vspace{0.2cm}
    \end{subfigure}
    \caption{MiBench AICS microarchitectural events.}
    \label{fig:mibench-events}
    \vspace{-0.25cm}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{images/mibench-nosuper.pdf}
            \vspace{-0.6cm}
            \caption{\% Performance Degradation}
            \label{fig:mibench-nosuper-inst-ratio}
    \end{subfigure}
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{images/mibench-nosuper-guest-itlb-miss.pdf}
            \vspace{-0.6cm}
            \caption{Guest iTLB miss per instr.}
            \label{fig:mibench-nosuper-hyp-excp}
        \end{subfigure}
    \vspace{0cm}
    \end{subfigure}
    \caption{MiBench AICS without the use 
    of superpages on second-stage translation.}
    \label{fig:mibench-nosuper}
    \vspace{-0.7cm}
\end{figure}

\par We start by assessing the performance degradation\footnote{Performance degradation is the ratio between the total execution time of the benchmark running atop the hypervisors and native execution.} of a single-core Linux VM atop each SPH. The main results are depicted in Figures \ref{fig:mibench}, \ref{fig:mibench-events}, and \ref{fig:mibench-nosuper}.  We then evaluate the system under interference to understand the effectiveness of microarchitectural isolation mechanisms available in each SPH.
%on a system without any hypervisor


\mypara{Selected Benchmark.} We use the MiBench Embedded Benchmarks' Automotive and Industrial Control Suite (AICS)\cite{mibench}. These benchmarks are intended to emulate the environment of embedded applications such as airbag controllers and sensor systems. Each test has two variants: \textit{small} operates in a reduced input data set representing a lightweight use of the benchmark, while \textit{large} operates over a considerable  input data set, emulating a real-world application scenario.


\mypara{Base Performance Overhead.} Fig. \ref{fig:mibench} presents the relative performance degradation for the MiBench AICS. For each benchmark, below the plotted bars, we present the average absolute execution time for the native execution. The first observation is that, independently of the hypervisor, different benchmarks are affected to different degrees. Secondly, Jailhouse, Xen, and Bao incur a negligible performance penalty, i.e., less than 1\% across all benchmarks. Although seL4 CAmkES-VMM also presents a small overhead for most benchmarks, the overhead can reach up to 7\%. 
%It is also worth noting that the variance observed for a given benchmark is about the same, irrespective of the hypervisor.

For a virtualized system configured with a single guest VM, there are two main possible sources of overhead. The first source is the increase in TLB miss penalty due to the second stage of translation, since it can, in the worst case, increase the number of memory accesses in a page-walk by a factor of four. Second, the overhead of trapping to the hypervisor and performing interrupt injection, e.g., timer tick interrupt. Additionally, the pollution of caches and TLBs by the hypervisor might also affect guest performance.
To further understand the behavior of the benchmarks, in particular the larger overhead of the CAmkES-VMM, we have collected a number of microarchitectural events. Fig. \ref{fig:mibench-events} shows them normalized to the number of executed instructions. We highlight two events whose increase is highly correlated with the degradation observed: hypervisor L2 cache refills (Fig. \ref{fig:mibench-hyp-l2-miss}) and guest TLB misses (Fig. \ref{fig:mibench-guest-itlb-miss}), with Pearson correlation coefficients of up to 0.94 and 0.96, respectively.
%, i.e., quadratically
%This is especially critical when the benchmark's working set does not fit in the TLB.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.90\textwidth]{images/mibench-interf-hatched.pdf}
    \vspace{-0.10cm}
    \caption{Performance degradation and L2 cache misses per instruction for the Mibench AICS under interference and coloring.}
    \label{fig:mibench-interf}
    \vspace{-0.45cm}
\end{figure*}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.49\textwidth]{images/mibench-interf-guest-l2miss-increase.pdf}
%     \vspace{-0.15cm}
%     \caption{Mibench AICS guest L2 cache miss per instruction increase under the effects of interference and coloring.}
%     \label{fig:mibench-interf-l2miss}
%     \vspace{-0.55cm}
% \end{figure}

An important hypervisor feature to minimize the impact of two-stage translation is to leverage superpages. By inspecting hypervisor code, we concluded that only CAmkES-VMM does not have support for 2MiB superpages. This justifies the higher number of TLB misses. Notwithstanding, to corroborate this argument, we have configured the other SPH to preclude the use of superpages. As expected, we observed an increase in the performance degradation (and TLB misses) similar to CAmkES-VMM (Fig. \ref{fig:mibench-nosuper}). We still observed a gap of up to 2\% between CAmkES-VMM and the other SPH; this is related to the aforementioned interrupt handling and injection overheads, i.e., a consequence of the microkernel design: more costly switches between VM and VMM and a high number of VMM to microkernel calls for managing and inject the interrupts. This is confirmed by Figures \ref{fig:mibench-inst-ratio} and \ref{fig:mibench-hyp-excp}, which show the hypervisor-to-guest executed instruction ratio and the number of exceptions taken by the hypervisor, respectively. For these events, seL4 has a higher ratio when compared to the other SPH. We further investigate interrupt injection in Section \ref{sec:irq-latency}.
%to maximize the use of TLB storage, and therefore decrease the number of TLB misses and page-table walks
%observed when running the benchmark over seL4


\begin{mdframed}[style=remarkstyle]

\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} SPH do not incur in meaningful performance impacts due to: (i) modern hardware virtualization support; (ii) 1-to-1 mapping between virtual and physical CPUs; and (iii) minimal traps. However, one key aspect is that SPH must have support for / make use of superpages to minimize TLB misses and page-table walk overheads.
\end{mdframed}
% \vspace{-0.2cm}

\mypara{Performance under interference.} We also evaluate inter-VM interference and the effectiveness of cache coloring at both guest and hypervisor levels. Fig. \ref{fig:mibench-interf} plots the results under interference (\textit{+interf}), with coloring enabled (\textit{+col}), and with interference and coloring enabled (\textit{+interf+col}). seL4 CAmkES VMM shows no results for coloring enabled as this feature is not openly available yet.
%without interference but 

There are four conclusions to be drawn. Firstly, interference significantly affects the benchmark execution over all hypervisors. As expected, this is explained by a significant increase in L2 cache misses. On Jailhouse, Xen, and Bao performance is degraded by a similar factor, i.e., to a maximum of about 105\%; seL4-VMM is more susceptible to interference, reaching up to 125\% in the worst case. This pertains to the fact that, given that seL4-VMM executes a much higher number of instructions, the interference also impacts the execution of the hypervisor. Secondly, coloring, per se, significantly  impacts performance (up to about 20\%). This seems logical given that coloring (i) forces the hypervisor to use 4KiB pages, reducing TLB reach, and (ii) reduces the available cache space, which for working sets larger than LLC increases memory system pressure (i.e., L2 cache misses). Thirdly, coloring can only reduce interference but not completely mitigate it. 
In these experiments, the interference workload runs continuously. However, in a more realistic scenario, it might be intermittent. The improvement in predictability achieved by coloring is reflected in the difference between the base experiment results (bars in Fig. \ref{fig:mibench} and \textit{+interf} in Fig. \ref{fig:mibench-interf}) and respective variants with coloring enabled (\textit{+col} in Fig. \ref{fig:mibench-interf}). The lower the difference, the higher the predictability. For example, in the case of \textit{susanc-small}, we observed that without coloring, the variation can go up to 105 percentage points (pp), while when coloring is enabled, the observed overhead is around 58\%, which corresponds to a variation of 38 pp compared to the configuration with coloring enabled but without interference. Nevertheless, we observed that cache misses are essentially reduced to the same level as when coloring is enabled but without interference. Clearly, the observed interference is not only due to cache-line contention. There are points of contention at deeper levels of the memory hierarchy, e.g., buses and memory controller \cite{moscibroda2007} or even in internal LLC structures \cite{Valsan2016}. Finally, results on Xen and Bao demonstrate that hypervisor coloring has no substantial benefit as it only reduces performance degradation due to interference by at most 1\%  (omitted due to lack of space).

\begin{mdframed}[style=remarkstyle]

\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} Multicore memory hierarchy interference significantly affects guests' performance. Cache partitioning via page coloring is not a silver bullet as despite fully eliminating inter-core conflict misses, it does not fully mitigate interference (up to 38 pp increase in relative overhead).
%This stems from the fact that there is also significant contention downstream in the memory hierarchy (i.e., interconnects, memory controller) \cite{} or even in internal LLC structures \cite{Valsan2016}.
\end{mdframed}
\vspace{-0.25cm}

%\vspace{-0.2cm}
%{\color{blue} Xen provides a quick-fix mechanism consists in endowing certain VMs with the privilege of stopping the execution of other VMs during critical sections.}

% \textcolor{red}{(1) Can we reason about the impact of selecting colors to guests ?  }

% \textcolor{red}{(2) Can we just briefly show and discuss other sources of contention ? Advocate for platform level mechanisms (show something from Francisco work?!) (Adding here text from previous Discussion: ``We also highlight that SPH need to extend these mechanisms to the platform level, by taking advantage of QoS hardware currently available on the bus and controlling interference from DMA-capable devices or heterogeneous compute elements (e.g., GPUS, NPUs, or FPGAS)."). Extend with SMMU. }

% \textcolor{red}{(3) Can we advocate for the need of adapative partitioning on caches ? Please refer to Usenix paper ``SecSMT: Securing SMT Processors against Contention-Based Covert Channels". Are there any other papers for dynamic cache-partitioning? }

% \textcolor{red}{(4) Discuss about hardware-support for interference... (Adding here text from previous Discussion: ``In this sense, although no hardware is available, support for the upcoming MPAM extensions is the most promising avenue to be explored, as similar x86 hardware support (e.g., Intel CAT) is already supported by Jailhouse and Xen. ") }

% \textcolor{red}{(5) Can we extend the part about the hypervisor coloring, since is the part less explored in the related work ? Which insigh/recommendation can we give here? (From previous discussion: ``We should highlight the fact that you can't color the hypervisor per partition". ) Maybe move this to the interrupt section.}

% ------------------------------------------------------------------------------------

%\newpage
\section{SPH: Interrupt Latency} \label{sec:irq-latency} 

%clearly benefits virtual interrupt management since it minimizes the number of traps when compared to a fully emulated interrupt controller. However, its design
%as it is bound to add considerable interrupt latency overhead and jitter
As discussed in Section \ref{sec:arm-virt}, the existing GIC virtualization support is not ideal for MCS: hypervisors have to handle and inject all interrupts and must actively manage list registers when the number of pending interrupts is larger than the physical list registers. This is of particular importance to guarantee the correct interrupt priority order which might be critical for an RTOS \cite{Hofer2009}.
%as the hypervisor must support additional control logic that guarantees that the highest pending priority interrupts are placed on the list registers, otherwise the virtual interface will receive interrupts out of priority order.
In this section, we investigate the overhead of each SPH in the interrupt latency, their susceptibility to interference, and the effectiveness of cache coloring. Then, we evaluate the direct injection technique and analyze interrupt priority support as well as virtual IPI latencies.
%to which degree the available microarchitectural isolation techniques are successful in mitigating virtualization overheads


\mypara{Methodology.} To measure interrupt latency, we used a custom lightweight baremetal benchmark, which measures the latency of a periodic interrupt triggered by the Arm Generic Timer. The timer is programmed in auto-reload mode, to continuously trigger an interrupt at each 10 ms. The interrupt handler reads the value of the timer, i.e., it measures the time elapsed since the interrupt was triggered. Each measurement is carried out with cold L1 caches. To achieve this, after each measurement, we flush the instruction cache. During the 10 ms, we also prime the L1 data cache with useless data.
%,thereby flushing all interrupt handling data to the unified L2.
% aimed at minimizing the impact of the guest in the overall interrupt logic
%, invalidating the L1 instruction cache lines of the the hypervisor's and the benchmark's code
%Also, in the 10 ms interval between each ... is continuously primed 


\mypara{Base Latency.} Fig. \ref{fig:irqlat-base} depicts the violin plots for the custom benchmark running atop each SPH. From the baseline of about 200 ns, Bao and Jailhouse incur the smallest increase, albeit significant, to an interrupt latency of about 4x (840 ns) and 5x (1090ns), respectively. Xen shows an increase of about 14x (2800 ns). The variance observed in these three systems is negligible. The difference observed between Jailhouse/Bao and Xen is justified by the interrupt injection path being highly optimized in the former, while more generic in Xen. We confirmed this by studying the source code and assessing the number of instructions executed by each hypervisor on the interrupt handling and injection path: while Jailhouse and Bao execute around 200 instructions, Xen executes about 1050.

%seL4 presents the largest interrupt latency (~47x, 9400 ns), an order of magnitude higher than Bao and Jailhouse. The variance of the latency is also perceptibly affected. This is due to the microkernel architecture which results in multiple guest/VMM traps to the microkernel for each interrupt: (i) a trap to the microkernel which forwards it as a message to the VMM, that will then handle the interrupt by issuing a couple system calls (ii) to inject the interrupt and (iii) resume VM execution; also, the seL4 does not make use of a specific GIC feature which lets the guest directly deactivate the physical interrupt, which results in (v) another context switch to the VMM, and other hypercalls to (v) deactivate the interrupt and (iv) resume the VM. In the other hypervisors, each interrupt measurement results in a single exception taken at the hypervisor privilege level where the interrupt is handled and injected in the VM. 

\begin{figure}
    \centering
    \includegraphics[width=0.38\textwidth]{images/irqlat-base.pdf}
    \caption{Base interrupt latency.}
    \label{fig:irqlat-base}
    \vspace{-0.6cm}
\end{figure}

seL4-VMM presents the largest interrupt latency (47x, 9400 ns), an order of magnitude higher than Jailhouse and Bao. The variance of the latency is also affected. This can be explained by the interrupt handling and injection mechanism of a microkernel architecture. In the other SPH, each interrupt results in a single exception taken at EL2, where the interrupt is handled and injected in the VM; virtualization support is leveraged such that no further traps occur. In CAmkES VMM it results in four traps to the microkernel: (i) the first due to the interrupt that results in forwarding it as a message to the VMM; 
%{\color{red} We are missing the irq-server step here (see https://github.com/seL4/seL4/issues/663)} 
(ii) a system call from the VMM to inject the interrupt in the VM (i.e., write the list register); (iii) another to ``reply" to the exception, resuming the VM; and (iv) a final  one where the VMM waits for a message signaling a new VM event or interrupt, resulting in a final context-switch back to the VM. We have also concluded that seL4 does not use a GIC feature that would allow guests to directly deactivate\footnote{Deactivating an interrupt in the GIC means marking it as handled, enabling the distributor to forward it to the CPU when it occurs again.} the physical interrupt, resulting in an extra trap.
% that will then handle the interrupt by issuing a couple system calls 

\begin{mdframed}[style=remarkstyle]

% \textcolor{red}{(1) As far as I know, there was no quantifiable evidence about the real difference in terms of interrupt latency between hyp SPH and seL4, right ?  }

% \textcolor{red}{(2) Can we discuss any design recomendation for Xen and seL4 to reduce the base latency ?  }

% \textcolor{red}{(From old discussion:) Due to the lack of efficient hardware virtualization support for directly delivering interrupts to guests in Arm platforms, all SPH significantly increase interrupt latency, which is aggravated under the interference of other VMs.   }

\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} Due to the lack of efficient hardware support for directly delivering interrupts to guests in Arm platforms, all SPH increase the interrupt latency by at least one order of magnitude. However, by-design, SPH such as Jailhouse and Bao are able to achieve the lowest latencies as they provide an optimized path for hardware interrupt injection.
\end{mdframed}
\vspace{-0.2cm}

% \begin{mdframed}[style=remarkstyle]
%\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.}
% Dedicated SPH provide a optimized path for interrupt injection, thus achieving much smaller overall latencies. We believe that it is possible to also optimize this path in larger hypervisors such as Xen, and in microkernel-based architectures by removing some of the traps and moving part of the interrupt injection logic to the microkernel.
% \end{mdframed}
% \vspace{-0.2cm}

\mypara{Latency Under Interference.} Fig. \ref{fig:irqlat-interf} shows the results for interrupt latency under interference, including the baseline results of Fig. \ref{fig:irqlat-base} for relative comparison as \textit{solo}. Analyzing the effects of VM interference on interrupt latency (\textit{interf}), we observed that Bao latency increases to an average of 7260 ns, Jailhouse to 7730 ns, Xen to 23000 ns, and seL4-VMM to 85940 ns. It corresponds to an increase of 36x, 38x, 115x, and 430x, respectively, compared to the base latency. It is also worth noting that the variance also increases. When enabling coloring (\textit{col}), we measured no significant difference in interrupt latency compared to the base case. However, when enabling cache coloring in the presence of inter-VM interference (\textit{interf+col}), there is a visible improvement in average latency and variance. However, note that the observed variance does not constitute a measure of predictability. As explained in Section \ref{sec:perf}, predictability is reflected in the difference between the \textit{interf} and \textit{interf+col} results and respective baselines, i.e., \textit{solo} and \textit{col}. Finally, by applying coloring also to the hypervisor (\textit{interf+col+hypcol}), Bao latency is reduced to almost no-interference levels with negligible variance. Xen latency also drops considerably to an average of 6300 ns.

\begin{figure} [!t]
    \centering \hspace{-0.4cm}
    \includegraphics[width=0.485\textwidth]{images/irqlat-interf-with-solo.pdf}
    \hspace{0.4cm}
    \vspace{-0.7cm}
    \caption{Interrupt latency under interference and cache coloring.}
    \label{fig:irqlat-interf}
    \vspace{-0.25cm}
\end{figure}

\begin{figure} [!t]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \begin{subfigure}{0.475\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{images/irqlat-l2miss-el1.pdf}
            \vspace{-0.6cm}
            \caption{Guest L2 cache misses.}
            \label{fig:irqlat-l2miss-el1}
    \end{subfigure}
    \begin{subfigure}{0.475\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/irqlat-l2miss-el2.pdf}
        \vspace{-0.6cm}
        \caption{Hypervisor L2 cache misses.}
        \label{fig:irqlat-l2miss-el2}
    \end{subfigure}
    \vspace{0cm}
    \end{subfigure}
    \caption{L2 cache misses for the interrupt latency benchmark.}
    \label{fig:irqlat-l2miss}
    \vspace{-0.6cm}
\end{figure}

The observed interrupt latency under interference can be mostly explained by L2 cache misses. Fig. \ref{fig:irqlat-l2miss} shows the L2 cache misses for both guest and hypervisor during interrupt latency measurement. We can see that interference increases guest L2 cache misses, but that cache coloring can lower them back to the base case values. However, this is not the case for hypervisor L2 cache misses. For the base case, there are no cache misses for the hypervisor, which increases substantially under interference. Despite VM coloring contributing to reduce hypervisor L2 cache misses, only by coloring the hypervisor level, it is possible to minimize L2 cache misses for the hypervisor. On Bao, L2 cache misses are fully eliminated, but not on Xen\footnote{At the time of writing, Xen's coloring patch was still under review. Thus, the assessed implementation may contain some imprecisions that are likely to be fixed by the time the patch is merged.}, which might explain why latency does not reduce to non-interference levels.

\begin{mdframed}[style=remarkstyle]

% \textcolor{red}{(1) coloring/cache partitioning specially in the hypervisor is critical  }

% \textcolor{red}{(2) Can we discuss how the GIC would need to be improved to ... Discuss also that it remains to be seen if direct injection support in future versions of the GIC, largely based on MSIs, will suffice, or if this kind of technique will still be required for wired interrupts ...  }

\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} Interrupt latency increases tenfold under the interference workload. Applying cache coloring to VMs proves very beneficial, but for it to be fully effective, it is imperative to reserve a color for the hypervisor itself.
\end{mdframed}
\vspace{-0.2cm}

\mypara{Direct Injection.} We evaluate the effectiveness of the direct injection technique, implemented only in Jailhouse and Bao. Fig. \ref{fig:irqlat-di} depicts the results. The first conclusion is that for the base case, i.e., no interference, the interrupt latency is near to native (about 210 ns). Indeed, we have confirmed that during the execution of the benchmark, there are no traps to the hypervisor. Next, we observed that interference somewhat increases latency and its variance, but much less than in the previous experiments. Finally, we concluded that by enabling coloring, it is possible to lower the average latency to near native (243 and 232 ns for Bao and Jailhouse, respectively), however, there is still some variance due to the interference.

\begin{mdframed}[style=remarkstyle]

\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} The direct injection technique is effective in addressing the shortcomings of GIC interrupt virtualization, as results clearly demonstrated interrupt latency overhead is reduced to near native latencies.

% \textcolor{red}{(1) This technique is not well-addressed in the related work ... Give emphasis to that here ...  }

% \textcolor{red}{(1) Discuss a bit more the issues and limitations of direct injection: applying this technique severely hinders the ability of the hypervisor to manage devices or implement any kind of functionality dependent on interrupts. Therefore, it is only suitable for very simple hypervisors implementing the straightforward static partitioning architecture.  }

% \textcolor{red}{(2) Can we discuss any design that partially fixes the problems above and still may present advantages ?  }

\end{mdframed}
\vspace{-0.2cm}

\begin{figure} [!t]
    \centering
    \includegraphics[width=0.4\textwidth]{images/irqlat-di.pdf}
    \vspace{-0.2cm}
    \caption{Interrupt latency with direct injection enabled.}
    \label{fig:irqlat-di}
    \vspace{-0.3cm}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/irqprio.pdf}
    \vspace{-0.2cm}
    \caption{Time between the handling of different priority interrupts triggered simultaneously, i.e., for interrupt N in the X-axis, the time between the arrival of interrupts N-1 and N.}
    \label{fig:irqprio}
    \vspace{-0.55cm}
\end{figure}

\mypara{Priority Handling.} For studying the support of SPH delivering interrupts in the correct priority order, we have implemented a PL device which can be used to trigger up to 16 simultaneous interrupts, and a custom benchmark that assigns each a different priority. It starts by triggering the eight lowest priority interrupts. When handling the first, it triggers the eight highest priority interrupts. This would force the hypervisor to refill the four available LRs with the new higher priority interrupts, and refill them in priority order as LRs become available. The benchmark verifies if the priority order was kept and measures the arrival interval between each interrupt.
We verified that only Xen and Bao guarantee the delivery of interrupts in the correct priority order.
By inspecting the code, we have confirmed that both seL4-VMM and Jailhouse fill the GIC LRs following a FIFO policy. Furthermore, the seL4-VMM does not even commit the interrupt priorities configured in the virtual GIC distributor to hardware, precluding the arrival of physical interrupts in the correct priority order. Fig. \ref{fig:irqprio} shows that across all hypervisors if multiple interrupts are delivered simultaneously, there is an increase by several orders of magnitude in the arrival time of the first and second interrupts, which is less than 700 ns for the baremetal case. This larger increase is justified by the fact that the hypervisors must handle all interrupts before the guest starts handling the first interrupt. Another observation is that there is a periodic increase in the interval of arrival. We have concluded this is the point at which there are no pending interrupts left in the LRs, which triggers the hypervisor to refill these registers with previously spilled pending interrupts.

\begin{mdframed}[style=remarkstyle]

\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} Only Xen and Bao respect interrupt priority order. Additionally, we observe that for all SPH, if multiple interrupts are triggered simultaneously, there is a partial priority inversion as lower priority interrupts take precedence due to the need for the hypervisor to handle and inject them.
%, increasing the latency of the higher priority ones.

%the hypervisor handles and injects all before letting the guest handle the highest priority interrupt. Therefore,

% \textcolor{red}{(1) This is the 1st time there are evidence about this ... Make a big deal about this insight ... }

% \textcolor{red}{(2) The problem of serialization of interrupt handling at the hypervisor level is a problem not address on the state of the art. Can we discuss how it may be fix at the hardware level? Can we optimize the hypervisor design to minimize? Can we implement techniques that minimize?}

\end{mdframed}
\vspace{-0.2cm}

\mypara{Inter-Processor Interrupts.} IPIs (SGIs) are critical for multicore VM performance. For a vCPU to send an SGI, the guest must write a virtual GIC distributor register. This will trap to the hypervisor that must emulate the access and forward the event to the target core, where the SGI is injected via list registers. We use a custom baremetal benchmark to measure IPI latency. It works by measuring the time between when the source vCPU writes the distributor register and when the final IPI handler starts executing. It also measures the overhead of the trap. We instrument the SPH to sample the time the IPI is forwarded internally; this signals the end of the emulation and translates the overhead of injecting the interrupt in the target.

Figure \ref{fig:ipi} shows that IPI latency increases significantly for all SPH. While the baremetal IPI latency is around 260 ns, it reaches 2258 ns for Jailhouse, 4157 ns for Xen, 2711 ns for Bao, and 10868 ns for the CAmkES VMM. However, the costs of the register access emulation and interrupt injection are not proportional across all SPH. For example, Bao has the lowest emulation and event forwarding times, but the overall IPI latency is higher than Jailhouse's. This means that the interrupt injection path on Bao is slower than on Jailhouse. By inspecting the source of both hypervisors, we have observed that Bao immediately forwards the SGI event to the target core, performing all interrupt injection operations in the target core. Jailhouse, in turn, manages the interrupt injection structures at the source core and only then signals the target vCPU by writing the list register. Xen follows the same approach as Jailhouse, but presents higher overhead. The CAmkES VMM has the highest overhead due to the large number of system calls the VMM issues to the microkernel (in total, 7). Four are issued before the event forwarding, and the rest only after the SGI is forward to the target core. All in all, the access to the virtual distributor is more expensive than the IPI itself.

\begin{figure}
    \centering
    \includegraphics[width=0.44\textwidth]{images/ipi.pdf}
    \caption{Average cost for each send IPI operation component.}
    \label{fig:ipi}
    \vspace{-0.5cm}
\end{figure}

% The effects of interference and coloring on IPI latency are essentially proportional/analogous to what was observed in Fig. \ref{fig:irqlat-interf}. Direct injection on Jailhouse and Bao also brings significant improvements to IPI latency but falls short of achieving native latency as the price of the trap for writing the distributor register is always paid. 


\begin{mdframed}[style=remarkstyle]
\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} IPI latency reflects the same overheads of external interrupts. Future Arm platforms might reduce them with GICv4.1 \cite{arm2022gic}. In the short term, direct injection might alleviate this issue. However, both approaches fall short of achieving native latency as they still pay the price of emulating the write to the ``IPI send" register.
\end{mdframed}


%\section{Inter-VM Communication and Virtual Devices} 
%\par Inter-VM Communication and Virtual Devices as future work ...

%---------------------------------------------------------------------------------

\section{SPH: Inter-VM communication} \label{sec:vm-comm}
For inter-VM communication, SPH typically only provide statically allocated shared memory. This is usually coupled with an asynchronous notification mechanism signaled as an interrupt. All four SPH provide such mechanisms. Next, we analyze inter-VM notification latency and transfer throughput.

\mypara{Inter-VM latency.} Fig. \ref{fig:notification} shows the inter-VM notification latency, reflecting the time since the notification is issued until the execution of the handler in the destination VM. The relative differences between the latencies for each SPH are similar to those observed for passthrough interrupts and IPIs. Jailhouse achieves the lower latency (1500 ns), followed by Bao (1900 ns). Xen shows an intermediate value of 4600 ns, while seL4 CAmkES VMM is significantly larger than others (average 18000 ns). Studying the internals of the implementations, we note that while most hypervisors synthesize and inject the virtual interrupts, Jailhouse uses non-allocated physical interrupts for these notifications. Thus, to send one, Jailhouse only sets the interrupt pending in the GIC distributor. This is significantly advantageous when combined with direct injection. Note that enabling direct injection in Bao would preclude the use of this mechanism. For seL4, we highlight the impact of the microkernel architecture since atop VM/VMM context switches, we observe additional overheads due to inter-VMM communication. Lastly, we see that interference increases all latencies accordingly and that coloring can mitigate it.

\begin{figure} [!t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/notification.pdf}
    \caption{Inter-VM notification latencies.}
    \label{fig:notification}
    \vspace{-0.5cm}
\end{figure}


\mypara{Inter-VM throughput.} In Fig. \ref{fig:comm}, we evaluate the throughput of bulk data transfers via a shared memory buffer. The benchmark transmits 16 MiB of random data through a shared buffer with varying sizes. When the source VM finishes writing the buffer, it either signals the destination VM via a shared memory flag or via an asynchronous notification, and waits for a signal back to start writing the next chunk. For the polling scenario, the obtained throughput is very similar across all hypervisors; this confirms that are no significant differences in how they allocate and map memory or configure memory attributes. Throughput is stable (1500 MiB/s) until the buffer size surpasses the LLC size (1 MiB), dropping to about 1300 MiB/s. For the asynchronous scenario, throughput is significantly impacted when using smaller buffer sizes, given the high number of synchronization points that reflects the observed interrupt overheads. Finally, we note that interference has no significant effect as long as the buffer size is kept below about half the size of LLC. Beyond that, throughput is reduced from 1300 to 850 MiB/s. Although not shown due to lack of space, using coloring does not prove beneficial, as the throughput illustrated in Fig. \ref{fig:comm} remains virtually unchanged.
%Coloring support for the shared memory regions is only straightforwardly available in Bao's configuration. It is possible to explicitly select the colors for the share memory buffer. For Xen, one must manually assign a contiguous aperture of physical memory for the shared memory buffer, which precludes choosing colors unless the shared buffer follows the size and alignment of a given color. Jailhouse calculates the colors of the shared buffer according to the colors of each VM. This has the issue that the buffer might be mapped differently for each communicating VM, unless the VMs share some colors and the shared region is carefully placed in physical memory.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{images/comm.pdf}
    \vspace{-0.2cm}
    \caption{Inter-VM communication throughput.}
    \label{fig:comm}
    \vspace{-0.5cm}
\end{figure}

\begin{mdframed}[style=remarkstyle]
\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} Inter-VM notification latencies are significant and, as is the case for hardware interrupts, very susceptible to the effects of interference. However, for bulk data transfers it does not seem to significantly affect throughput if the shared buffer size is chosen on a range of about one-fourth to half the LLC size (i.e., 256 KiB to 512 KiB).
\end{mdframed}
\vspace{-0.335cm}
%---------------------------------------------------------------------------------

%\newpage
\section{SPH: Boot time} \label{sec:boot}
System boot time is a crucial metric in industries such as automotive \cite{Hamelin2020, Golchin2022} as critical components have strict timing requirements for becoming fully operational. 
%In this section, we assess VM boot time overheads on SPH.

%\mypara{Platform's Boot Flow.} The boot flow of the Zynq Ultrascale+ is composed of several steps that start with two microprocessors, the platform management unit (PMU) and the configuration security unit (CSU), cooperating to setup the system (e.g., power, clocks, on-chip memory - OCM - initialization) by executing code from private boot ROMs. Then, the CSU loads the next boot stage payload to an OCM, the first stage bootloader (FSBL), from a pre-configured storage (in our setup, QSPI flash). The FSBL is the first component to execute in the Cortex-A cores. It performs further system initialization (e.g., DRAM) and load the final payloads in the boot image to main memory. These standard payloads typically are the Arm Trusted Firmware (TF-A), which runs in the secure world's monitor mode, and U-boot, which runs at the hypervisor privilege level. The FSBL will hand control to TF-A which after performing its initialization will jump U-boot. For all hypervisors, with the notable exception of Jailhouse, U-boot will then load both the hypervisor and guest images. In the case of Jailhouse U-boot will only load the root cell kernel, and later the Jailhouse kernel module will load the hypervisor and guest images. At this point, as shown in Fig. \ref{fig:bootsequence}, we considered different stages for each hypervisor. 
%While Bao and Xen directly boot guests after running their internal initialization, while Jailhouse and seL4 have two and three distinct phases, respectively. Jailhouse starts with the typical boot of a Linux kernel which will then load the Jailhouse kernel module that will load, install and start the hypervisor itself. The hypervisor and the kernel module will then cooperate to load and initialize guests. In the case of seL4, execution starts with an custom elf loader which will loads the microkernel, VMM application, and guest images, initializes secondary cores, and sets up an initial set of page tables for the microkernel. The microkernel then starts executing and once initialized, releases control to the user space CAmkES environment and the VMM.
\mypara{Platform's Boot Flow.} The platform's boot flow \cite{xilinxtrm} starts by executing ROM code which loads the first-stage bootloader (FSBL) and enables the main cores. These initial boot stages setup the platform basic infrastructure (e.g., clocks, DRAM) and load the TF-A and U-boot. U-boot will load the hypervisor and, except for Jailhouse, the guest images. 
Bao and Xen directly boot guests after initialization. Jailhouse starts with the boot of the Linux root cell, that installs the hypervisor which then loads the guests. seL4's execution starts with an ELF loader which loads the all images, initializes secondary cores, and sets up an initial set of page tables for the microkernel. The microkernel initializes and hands control to user space.
% Fig. \ref{fig:bootsequence} depicts the boot sequence per SPH.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.35\textwidth]{images/bootsequence.pdf}
%     \caption{Boot sequence for each hypervisor (not to scale).}
%     \label{fig:bootsequence}
%     % \vspace{-0.3cm}
% \end{figure}

%\mypara{Methodolody.} To measure the execution time of each stage, we use the Arm Generic Timer free running counter. However, this counter is only enabled somewhere during the execution of the FSBL. For this reason, we just start counting at the first instruction of TF-A onward.

% \mypara{Hypervisor Boot Time.} The hypervisor boot time is heavily dependent on the VM and how it is configured. We observed that the VM image size is one of the parameters that has the higher impact in the hypervisor boot time. Departing from this observation, we measure boot time by varying the size of the VM image and keeping other configuration parameters (e.g., size of VM memory) intact. For Jailhouse and seL4-VMM, we consider as part of the hypervisor boot time the execution of all components between U-boot and the guest.
%The results are illustrated in Fig. \ref{fig:boottime-hyp}. Bao has the lowest boot time ranging from 5 to about 150 ms; Xen boot time can range from 250 up to 800 ms. seL4-VMM and Jailhouse show significantly large boot times which can go up to 3 and 5 seconds, respectively. It is possible to conclude that the hypervisor boot time increases linearly with the VM image size.
%, although this is very much less pronounced in Bao than in others. 

% \begin{figure}
% \centering
% % \begin{subfigure}{0.46\textwidth}
% %   \includegraphics[width=1\linewidth]{images/boottime-vmsize.pdf}
% %   \caption{Hypervisor boot time by VM memory region size.}
% %   \label{fig:boottime-vmsize}
% % \end{subfigure}

% %\begin{subfigure}{0.47\textwidth}
%    \includegraphics[width=1\linewidth]{images/boottime-imgsize-compressed.pdf}
%    \caption{Hypervisor boot time by VM image size.}
%    %\label{fig:boottime-imgsize}
% %\end{subfigure}
% %\caption{Hypervisor's boot time.}
% \label{fig:boottime-hyp}
% \vspace{-0.5cm}
% \end{figure}

\mypara{Total VM Boot Time.} The hypervisor boot time is heavily dependent on the VM and how it is configured. We observed that the VM image size is one of the parameters that has the higher impact in the hypervisor boot time. We measure boot time as a function of VM image size. Thus, to understand the overhead of the hypervisor in the context of the complete boot flow, in Fig. \ref{fig:boottime-cumulative}, we plot the cumulative time for each boot stage. Here, we can confirm that in all hypervisors but Jailhouse, the bulk of boot time is spent by U-boot. For Jailhouse, U-boot run time is constant, albeit large, as it always only loads the root cell's image. Jailhouse execution time increases steeply while loading the VM image. From this macro perspective, the other hypervisors add an almost constant offset to U-boot's boot time, the largest being seL4-VMM's. We observe this overhead is not on the microkernel, but at user level, which nevertheless heavily interacts with the microkernel to setup capabilities and kernel objects. We can conclude that VM boot time has its bottleneck by the loading of guest images to memory, not the hypervisor logic.
% which loads all images

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.465\textwidth]{images/boottime-imgsize-cumulative.pdf}
    \vspace{-0.15cm}
    \caption{Boot time for each stage by VM image region size.}
    \label{fig:boottime-cumulative}
    \vspace{-0.5cm}
\end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}{0.5\textwidth}
%         \centering
%         \begin{subfigure}{0.49\textwidth}
%             \centering
%             \includegraphics[width=1\textwidth]{images/boottime-imgsize-bao.pdf}
%             \vspace{-0.6cm}
%             \caption{Bao}
%     \end{subfigure}
%         \begin{subfigure}{0.49\textwidth}
%             \centering
%             \includegraphics[width=1\textwidth]{images/boottime-imgsize-jailhouse.pdf}
%             \vspace{-0.6cm}
%             \caption{Jailhouse.}
%         \end{subfigure}
%     \vspace{0.2cm}
%     \end{subfigure}
%     \begin{subfigure}{0.5\textwidth}
%         \centering
%         \begin{subfigure}{0.49\textwidth}
%             \centering
%             \includegraphics[width=1\textwidth]{images/boottime-imgsize-xen.pdf}
%             \vspace{-0.6cm}
%             \caption{Xen.}
%         \end{subfigure}
%         \begin{subfigure}{0.49\textwidth}
%             \centering
%             \includegraphics[width=1\textwidth]{images/boottime-imgsize-seL4.pdf}
%             \vspace{-0.6cm}
%             \caption{seL4-VMM.}
%         \end{subfigure}
%     \end{subfigure}
%     \caption{Cumulative boot time for each stage by VM image region size. Colors according to Fig. \ref{fig:bootsequence}; Axis according to Fig. \ref{fig:boottime-imgsize}}.
%     \label{fig:boottime-imgsize-cumulative}
%     \vspace{-0.25cm}
% \end{figure}

\mypara{FreeRTOS and Linux Boot Times.} We also measure the boot time of (i) a small VM running FreeRTOS with a 90 KiB image and (ii) a large VM with a Linux guest (built-in ramfs) totaling 59 MiB of image size. For Jailhouse, the Linux VM is a non-root cell. In Table \ref{tab:boottime-vms}, we present results for a single-guest and a dual-guest system. For the latter, both VMs boot simultaneously; thus, we did not run experiments for dual-guest with Jailhouse, because it launches VMs sequentially. 
%\textit{Table \ref{tab:boottime-hyp} focus on the hypervisor boot time} , i.e, execution time and relative percentage of the full boot process. This data corroborates the previous results, i.e., Bao and Xen take the smallest fraction of total boot time.
Table \ref{tab:boottime-vms} presents the absolute boot time for the guest's native and virtualized execution, highlighting the relative percentage increase compared to native execution. For the single-guest FreeRTOS VM, all hypervisors but Bao cause a non-negligible increase in boot time. The same happens with the single-guest Linux VM. For the dual-guest configuration, we concluded that the small VM is heavily affected for all hypervisors. Surprisingly, we observe that although the cost of booting a single FreeRTOS in Bao is negligible, this is not true for a dual-guest configuration. Booting it alongside a Linux VM significantly increases its boot time, reaching similar overheads to those observed in Jailhouse's sequential boot.

\begin{mdframed}[style=remarkstyle]
\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} The major bottleneck for the VM boot time is caused by the bootloader, not the hypervisors. Notwithstanding, the hypervisor can significantly increase the boot time of a critical VM (small RTOS) when booting it alongside a larger VM (e.g., in dual-OS Linux+RTOS configuration).

% \textcolor{red}{(1) The main insight comes from the conclusion that we need novel boot mechanisms that prioritize and concurrently launch only critical VMs. Maybe discuss how this can be implemented ...} 

% \textcolor{red}{(2) As highlighted by Jumpstart \cite{Golchin2022}, we also believe additional efforts must be carried out to optimize the hard and soft boot times of the studied SPH, taking into account the full boot flow. Any comments on wake up times when CPUs are in low-power/idle?}
\end{mdframed}
\vspace{-0.4cm}


%s \ref{tab:boottime-hyp} and 
%8 MiB memory size 
%512 MiB memory size running
%The same is true for Linux, but to a lesser degree
% for the single-guest scenario
%This essentially brings the cost of booting a critical VM on a dual-guest (Linux + FreeRTOS) configuration to the same order of magnitude in both hypervisors.
%as the root cell boot time is essentially the baremetal time without any virtualization related overheads

% \begin{table}[]
% \resizebox{.98\linewidth}{!}{
% \begin{tabular}{llllll}
% \toprule
%                           &                      & Jailhouse           & Xen             & Bao             & seL4-VMM        \\ \midrule
% \multirow{2}{*}{FreeRTOS} & Single              & 2061.38 / 33.02\%  & 214.33 / 9.17\%   & 5.21 / 0.30\%   & 1407.73 / 40.26\% \\ \cmidrule{2-6} 
%                           & Dual                & N/A                 & 803.60 / 11.67\% & 5.57 / 0.10\%   & 2976.56 / 32.04\% \\ \midrule
% \multirow{2}{*}{Linux}    & Single              & 6065.05 / 49.37\%  & 798.52 / 9.36\%   & 134.55 / 1.72\% & 2494.07 / 19.75\% \\ \cmidrule{2-6} 
%                           & Dual                & N/A                 & 797.26 / 9.16\%  & 134.69 / 1.75\% & 2897.44 / 22.14\% \\ \bottomrule
% \end{tabular}
% }
% \caption{Hypervisor boot time (ms) and percentage of total boot time, for FreeRTOS and Linux VMs.}
% \label{tab:boottime-hyp}
% \vspace{-0.2cm}
% \end{table}

%\mypara{Cache Coloring Effects} As a final note, although not illustrated in the figures, we measured boot times with coloring enabled in Bao, Jailhouse, and Xen and conclude that enabling color causes only a negligible increase of boot time of a maximum of 7\% across all hypervisors.

%---------------------------------------------------------------

%\newpage
\section{SPH: Code Size and TCB} \label{sec:code-tcb}

In MCS, the size of the hypervisor code, measured in source lines of code (SLoC), is critical. It should be minimal as it is part of the trusted computing base (TCB) of all VMs. 
%There are many diffuse definitions of TCB in the literature. 
In this paper, we consider that a VM TCB encompasses any component with sufficient privileges that if it is compromised or malfunctions, might be able to affect the safety and/or security properties of the VM. As well understood in the literature, a larger TCB typically has a higher number of bugs and wider attack surface \cite{Biggs2018}, resulting in a higher probability of vulnerabilities. It is important to understand that each VM has its own TCB. Thus, CAmkES VMM is only considered for the managed VM's TCB, not the others. Further, large code bases are impractical for certification, both from a technical and economic perspective. To qualify a component assigned a safety integrity level (SIL), all components on which it depends must also be qualified to the same or higher SIL \cite{Esper2018}.

%for a common defect ratio, 
%. Thus, to host a critical VM, the hypervisor or VMM must also be qualified to the same SIL
%there is not a single TCB in the system;
%mainly drive by a virtualization perspective,
%, according to which it needs or chooses to trust to fulfill its role

\begin{table}[!t]
\resizebox{1\linewidth}{!}{
\begin{tabular}{lllllll}
\toprule
                          &        & Baremetal    & Jailhouse                & Xen               & Bao                       & seL4-VMM             \\ \midrule
\multirow{2}{*}{FreeRTOS} & Single & 1670.89     &  6242.18 / 173.58\%     & 2338.24 / 39.94\%   & 1716.23 / 2.71  \%  &  3496.19 / 109.24\%   \\ \cmidrule{2-7} 
                          & Dual   &             &  N/A                     & 6887.88 / 312.23\% & 5734.04 / 143.17\%   &  9291.02 / 456.05\%   \\ \midrule
\multirow{2}{*}{Linux}    & Single & 7665.14     &  12284.92 / 60.27\%     & 8533.88 / 11.33\%   & 7805.54 / 1.83  \%  &  12629.79 / 64.77\%   \\ \cmidrule{2-7} 
                          & Dual   &             &  N/A                     & 8707.15 / 13.59\%  & 7895.95 / 3.01  \%   &  13086.86 / 70.73\%   \\ \bottomrule
\end{tabular}
}
\caption{Total boot time (ms) and relative increase compared to the baremetal case, for FreeRTOS and Linux VMs.}
\label{tab:boottime-vms}
\vspace{-0.2cm}
\end{table}


\begin{table}[!t]
\centering
\resizebox{0.8\linewidth}{!}{
    \begin{tabular}{ccccclc}
        \toprule
        \multicolumn{2}{c}{}                                   & \multicolumn{2}{c}{C}     & \multirow{2}{*}{Asm} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Total\\ (SLoC)\end{tabular}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textit{.text} \\ (KiB)\end{tabular}} \\ \cmidrule{3-4}
                                                                &             & \multicolumn{1}{c}{.c}    & .h    &      & \multicolumn{1}{c}{} & \\ \midrule
        \multicolumn{1}{c}{\multirow{2}{*}{\textbf{jailhouse}}} & hypervisor  & \multicolumn{1}{c}{7308}  & 2279  & 342  & 9929  & 79.3  \\ \cmidrule{2-7} 
        \multicolumn{1}{c}{}                                    & driver      & \multicolumn{1}{c}{2041}  & 139   & N/A  & 2180  & 20.1  \\ \midrule
        \multicolumn{2}{c}{\textbf{xen}}                                      & \multicolumn{1}{c}{57360} & 8127  & 1765 & 67342 & 451.5 \\ \midrule
        \multicolumn{2}{c}{\textbf{bao}}                                      & \multicolumn{1}{c}{5046}  & 2840  & 537  & 8423  & 57.9  \\ \midrule
        \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{seL4}\\\textbf{CAmkES VMM}\end{tabular}}}      & microkernel & \multicolumn{1}{c}{14569} & N/A   & 189  & 14758  & 224.7 \\ \cmidrule{2-7} 
        \multicolumn{1}{c}{}                                    & VMM         & \multicolumn{1}{c}{20932} & 19291  & N/A  & 40223 & 724.3 \\ \bottomrule
        \end{tabular}
}
    \caption{Hypervisor SLoC count and binary code size.}
    \label{tab:tcb-size}
    \vspace{-0.6cm}
\end{table}

\mypara{Methodology.} We measured SLoC for the target configurations using \textit{cloc} \cite{cloc}. Xen build system offers a make target to assess the SLoC for a specific configuration. However, it does not count header files, which we believe must be accounted for since they provide function-like macros and inline functions. We have modified the Xen makefile to measure headers. We have also extended Jailhouse and Bao build systems with the same functionality. For seL4, we used the fully unified and pre-processed kernel source file to assess the microkernel code base. For the CAmkES VMM, given that its source code is scattered throughout multiple seL4 project libraries, we were not able to list its source code files from the build system. Instead, we used debug information from the final executable and inspected each source to assess the included header files. 
%We have used \textit{GNU binutils} to measure the size of the binaries' \textit{.text} section.
%only those source files that are being compiled for
%by leveraging the generated dependency files to list all the source dependencies



% what about:
    % - arch vs generic code
    % - separate by functionality:
        % - generic (base hypervisor code)
        % - vcpu/vm
        % - memory
        % - interrupts
        %     - vgic
        % - io 
        %     - smmu driver...
        % - other
    % - memory footprint (allocated memory at run time?)
    % - use of external libraries
    % - other complexity measurements?
    % - what about attack surface
    % - comment on coding guidelines use (MISRA)

\mypara{Code Size.} Looking at Table \ref{tab:tcb-size} we see Bao and Jailhouse have the smallest code base of about 8400 and 9900 SLoC, respectively. Bao is implemented as a standalone component with no external dependencies. However, since part of Jailhouse functionality is implemented as a Linux kernel module, we also account that for the code base. It adds about 2180 SLoC, bringing Jailhouse total code base to 12 KSLoC. For Xen we use a custom config with almost all features disabled, except a few ones such as coloring and static shared memory. It features the largest code base with around 67 KSLoC. Finally, seL4 microkernel has 14.5 KSLoC, while the CAmkES VMM can go up to 40K, i.e., almost 55 KSLoC in total. The visible difference between Bao and Jailhouse, and seL4 microkernel and, especially, Xen, lies in the fact that the former were designed specifically for the static partitioning use case, while the latter aim at being more generic and adaptable. These differences are reflected in the binary size of each hypervisor.


\mypara{TCB.} The hypervisor SLoC does not directly reflect the VM TCB. Although by design SPH such as Bao has a smaller SLoC count, the seL4-VMM is vastly superior from a security perspective: shared TCB is limited only to the formally verified microkernel, because each VM is managed by a fully isolated VMM. From a FuSa certification standpoint, however, the VMM would still need to be considered. Moreover, seL4 formal proofs are limited to a set of kernel configurations, currently not including multicore. Regarding Jailhouse, despite its small size, the root cell is a privileged component of the system. It executes part of all VM management logic, being in the critical path for booting all other VMs. It is arguably part of all VM's TCB, increasing it significantly \cite{Biggs2018}.
%{ \color{blue} Nevertheless, Jailhouse does provide a mechanism to revoke these privileges after the system is initialized.} 
Analogously, Xen must depart from true Dom0-less to leverage richer features (e.g., PV drivers, dynamic VM creation). Recently, the Xen community has ignited efforts to use a smaller OS, such as Zephyr \cite{zephyr_project_2023}, as Dom0, refactor Xen to MISRA C, and provide extensive requirements and test documentation \cite{Mygaiev2021}.
% Similar initiatives would also benefit Jailhouse and Bao in future certification efforts.
%\footnote{Zephyr is an open-source RTOS designed targeting the Internet-of-Things (IoT) and supporting a wide range of embedded platforms. It is maintained under the umbrella of the Linux Foundation.}

\begin{mdframed}[style=remarkstyle]
\mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} Hypervisors specifically targeting static partitioning have the smallest code bases. Despite facilitating certification, none of the evaluated SPH provide other artifacts (e.g., requirements specification, coding standards). Xen is the first to take steps in this direction; nevertheless, seL4's formal proofs provide the most comprehensive guarantees.
\end{mdframed}
\vspace{-0.45cm}

% \begin{mdframed}[style=remarkstyle]
% \mypara{Takeaway \thetakeawyacount \stepcounter{takeawyacount}.} To achieve their small code base, pure SPH become too stiff and do not allow the inclusion of richer features without going against their design philosophy. Associated with the fact they support only passthrough IO this becomes a critical problem as IO resources such as clock, reset, power or pin-mux controls cannot be securely shared \cite{Ramsauer2018}. Controlled guest access to these resources, such a SCMI \cite{arm2022scmi}, must be provided. Nevertheless, this would entail including drivers in the hypervisor, increasing its code base.
% \end{mdframed}

%\mypara{TCB.} It is important to point out that the described SLoC measurements do not directly reflect the TCB. In the case of seL4, although the full virtualization stack features both the VMM and microkernel, the TCB itself is reduced only to the microkernel since each VM has its own independent and fully isolated VMM. However, from a safety perspective, for certification purposes, the VMM would still need to be considered. Importantly, although larger than Bao or Jailhouse, the microkernel is formally verified, therefore imparting a much higher degree and confidence and trust in the code. This formal guarantees are however limited to a set of kernel configurations which do not include SMP as this is still work-in-progress \cite{?}. Also note only the kernel is formally proven, not the VMM. In the case of Jailhouse, the root cell Linux VM is a privileged in critical component of the system. It executes part of all VM management logic, being in the critical path for booting all other VMs, and maintaining its privileges throughout system execution. In this sense, is also arguably part of all VM's TCB, greatly bloating the TCB SLoC, given the Linux's kernel huge TCB size. The same is also true for Xen if one departs from the basic Dom0-less configuration to leverage richer features (e.g., PV drivers, dynamic VM creation, VM monitoring) that imply the use of Dom0 which is also a privileged Linux VM. Recently, the Xen community has ignited efforts to use other smaller operating systems such as Zephyr as the Dom0 guest \cite{?}.


%----------------------------------------------------------------------------------

%\newpage
%\section{Discussion and Research Directions} 
%\label{sec:disc-research}

%\par Put all hypervisor benchmarks into perspective, compare them and discuss trade-offs and pros and cons, by analyzing each subsection of the evaluation.

%\mypara{Performance.} Performance does not seem to be a hurdle for SPH. Our experiments show less than 1\% overhead if superpages are supported. The same performance guarantees are not true when the system is under interference. Despite almost all studied hypervisors trying to mitigate this by coloring the LLC, our results demonstrated this technique is only partly effective and performance degradation can still reach up to 60\%. { \color {red} We should highlight the fact that you can't color the hypervisor per partition. } We acknowledge the need of applying other mitigation interference techniques such as memory throttling \cite{Yun2013}. In this sense, although no hardware is available, support for the upcoming MPAM \cite{Falk2021} extensions is the most promising avenue to be explored, as similar x86 hardware support (e.g., Intel CAT) is already supported by Jailhouse and Xen. We also highlight that SPH need to extend these mechanisms to the platform level, by taking advantage of QoS hardware currently available on the bus and controlling interference from DMA-capable devices or heterogeneous compute elements (e.g., GPUS, NPUs, or FPGAS) \cite{Zini2022, Sohal2020, Capodieci2020}.
% to minimize coloring drawbacks (e.g., preclude the use of superpages) 
%which also are a source of contention
%\textcolor{blue}{We also advocate for additional research to uncover new and unknown sources of interference, in particular those arising from the increasing heterogeneity of modern high-performance platforms for MCS, embedding an increasing number of GPUs, NPUs, AI accelerators, FPGA, and CGRA.} 
%We also advocate for the 
%although it eliminates inter-VM conflict misses, 

%\mypara{Interrupts.} Due to the lack of efficient hardware virtualization support for directly delivering interrupts to guests in Arm platforms, all SPH significantly increase interrupt latency, which is aggravated under the interference of other VMs. Our results show that enabling cache partitioning improves the worst-case latency; however, only the direct injection technique, implemented in Jailhouse and Bao, fully resolves the issue. Nonetheless, applying this technique severely hinders the ability of the hypervisor to manage devices or implement any kind of functionality dependent on interrupts. Therefore, it is only suitable for very simple hypervisors implementing the straightforward static partitioning architecture. It remains to be seen if direct injection support in future versions of the GIC, largely based on MSIs, will suffice, or if this kind of technique will still be required for wired interrupts.
%, especially if it also applied at the hypervisor level. But

%\mypara{Boot Time.} Our evaluation showed that simpler hypervisor architectures lead to much smaller boot times, i.e., Xen (Dom0-less) and Bao. \textcolor{red}{We believe that}, because Jailhouse and seL4-VMM move part of the VM creation logic either to the privileged root VM or to the user space VMM, the overhead of each individual component and resulting complex interactions have significant impact on boot time. Here the Dom0-less feature of Xen proves its advantages. Without it, Xen critical VMs would start booting only after Dom0, similarly to Jailhouse. 
%Nevertheless, in a dual-guest configuration, Bao and Xen critical VM boot time increases significantly, reaching identical boot time as Jailhouse and seL4. We believe there is a need for boot mechanisms that prioritize and concurrently launch only critical VMs. As highlighted by Jumpstart \cite{Golchin2022}, we also believe additional efforts must be carried out to optimize the hard and soft boot times of the studied SPH, taking into account the full boot flow.

%\mypara{Architecture and Design.} An SPH should be as small and simple as possible while providing the necessary partitioning mechanisms. 
%Ideally, it would be only a thin layer of software for configuring hardware virtualization partitioning mechanisms at boot time. However, there are several factors that hinder this goal in Arm processors, including the need to inject interrupts or emulate firmware services such as PSCI.
%From a more realistic perspective, pure static partitioning hypervisors such as Jailhouse and Bao are often too strict, especially from a resource utilization perspective. Also, there might be the need to share devices or securely multiplex access to indivisible resources (e.g., clocks) \cite{Ramsauer2018}.
%A hybrid, more flexible approach is to have the system resources statically partitioned while dynamically multiplexing or emulating the resources in one of the partitions (e.g., scheduling multiple vCPUs in a single CPU). Guaranteeing isolation and freedom from interference between these two domains would still fit MCS requirements, while providing a more adaptable interface. This is possible in a hypervisor such as Xen with the use of privileged Dom0, but there is no clear separation between partitioning and virtualization functions. In this vein, we observe that these two concepts are often conflated and implemented in a monolithic approach. The exception is the seL4 virtualization design, but with the caveat that it severely impacts latencies for VMs. Notwithstanding, we consider that CAmkES VMM is a proof-of-concept that has large leeway to be improved and optimized in this respect. Beyond optimization, we argue for exploring architectures following microkernel principles but specifically targeting the static partitioning use case by letting go of the generality of the microkernel design. Finally, as virtualization features start to span to MPU-based processors (e.g., cortex-R52), we believe in a progressive adoption of these architectures in automotive and industrial applications in the years to come \cite{Pan2018, Pinto2019}. 
%for safety-critical systems as it does not incur the tax of memory translation, and thus will likely play a role
%, which keeps partitioning in the kernel while deprivileging VMMs that can be safely extended and share functionality encapsulated in other fine-grained unprivileged components
%, as some CPUs will often sit idle
%As such, a design that can seamlessly be configured to run in both kind of platforms seems desirable.

%  From a safety and security point of view seL4 is the best, but in terms of real-time brings disadvantages.  CAmkES VMM has no maturity and is not formally verified. ... Shared Resources that cannot be partitioned Discuss about PLLs, other things that Ralf mention in the paper ... We should mention the need to add features and share certain services or hardware, without compromising on the main goals of static partition, safety and security...
%  

%-------------------------------------------------------------------------

%\newpage

\section{Discussion and Future Directions} \label{sec:discussion} 
In this section, we discuss some of the open issues and potential research directions to improve the guarantees of SPH.
%We have empirically observed that the studied open-source SPH provide only moderate guarantees regarding the overheads and determinism in terms of performance, interrupt latency, or boot time. Next
% what we believe are some 


\mypara{Interference Mitigation Techniques.} Cache coloring does not fully mitigate the effects of inter-core interference. Furthermore, coloring has inherent inefficiencies such as (i) precluding the use of superpages and (ii) increasing memory pressure which affects performance and predictability, as well as (iii) internal fragmentation (exclusively assigning 1 out of N colors, implicitly allocates 1/N$^{th}$ of physical memory, a portion of which may remain unused for small RTOSs or the SPH). While the latter could be solved by employing cache bleaching \cite{shanin2020} in heterogeneous platforms, to further minimize coloring bottlenecks, we advocate for SPH to adopt other proven, widely applicable contention mitigation mechanisms, e.g., bandwidth regulation mechanisms implemented via PMU-based CPU throttling \cite{Yun2013, Modica2018}. We also stress the importance of including support for hardware extensions such as Arm's Memory Partitioning and Monitoring (MPAM) \cite{arm2018mpam, Falk2021}, which provide flexible hardware means for partitioning cache space and memory bandwidth and call for platform designers to include such facilities in their upcoming designs targeting MCS. Finally, we stress the need for instrumentation, analysis, and profiling tools \cite{Sohal2020, Ghaemi2021} that integrate with these hypervisors to help system designers understand the trade-offs and fine-tune these mechanisms (e.g., through automation).


\mypara{Platform-Level Contention and Mitigation.} None of the studied SPH manages traffic from peripheral DMAs. We advocate that SPH must provide contention mitigation mechanisms at the platform level, e.g., (i) leveraging QoS hardware \cite{Sohal2020, Zini2022} available on the bus and (ii) controlling interference from DMA-capable devices or accelerators. Furthermore, since DMA masters still share SMMU structures (e.g., TLBs \cite{panchamukhi2015}), we hypothesize that bandwidth regulation techniques may fall short of efficiently mitigating interference at this level.

\mypara{Interrupt Injection Optimization.} Arm-based SPH's interrupt latency is mainly due to inadequate support in GICv2/3. GICv4 will provide direct interrupt injection support, but only for IPIs and MSIs. We want to raise awareness of Arm silicon makers and designers of the need for additional hardware support at the GIC level for direct injection of wired interrupts. The same holds for RISC-V \cite{Sa2021}.
Besides hardware support, we observed that simple SPH provide optimized interrupt injection paths. It is also possible to optimize this path in larger SPH (e.g., Xen) and in microkernels (e.g., moving injection logic to the microkernel). Finally, Bao and Jailhouse implement direct interrupt injection; however, we must stress that using this technique severely hinders the ability of the SPH to manage devices or implement any functionality dependent on interrupts. A plausible research direction would be a hybrid approach, i.e., selectively enabling direct injection only in specific cores for critical guests while providing the more complex functionality in cores running non-critical guests.

\mypara{Interrupt Priority Inversion Fix.} As discussed in Section \ref{sec:irq-latency}, the studied SPH suffer from partial interrupt priority inversion because all currently pending interrupts are handled by the hypervisor and injected in the guest before it can service the highest-priority one. We advocate for implementing a lightweight solution by dynamically setting the interrupt priority mask based on the priority of the last injected interrupt. This approach ensures the hypervisor only receives the next interrupt once the guest has handled the highest priority one.

\mypara{Critical VM Boot Priority.} Section \ref{sec:boot} highlights the issue of critical VM boot time overhead when booted under a dual-OS configuration. We advocate for the development of boot mechanisms that prioritize the boot of small critical VMs. However, as noted in Jumpstart \cite{Golchin2022}, it must encompass the full boot flow and be optimized across stages and components since the bottleneck of the boot time is in the image loading process performed by the bootloader, not the hypervisor.

\mypara{Per-Partition Hypervisor Replica.} Memory contention highly affects interrupt latency but can be minimized by assigning different colors for VMs and the hypervisor. Notwithstanding, coloring the hypervisor may prove wasteful and insufficient to address other interference channels internal to the hypervisor. We advocate for \textit{√† la} multikernel \cite{baumann2009} implementations such as the one implemented in seL4, where the hypervisor image is replicated per cache partition \cite{Ge2019}, fully closing internal channels. For SPH with a small enough footprint, memory consumption or boot time costs should not be prohibitive.

\mypara{Architecture Flexibility.} Purely monolithic SPH (e.g., Jailhouse or Bao) have smaller code bases at the cost of feature richness and flexibility. The same holds for Xen, i.e., many widely-used rich features are absent when configured as an SPH (to minimize code size). On the other hand, the seL4 microkernel architecture is much more flexible as it allows for an isolated user space VMM per guest, providing more robust isolation and customization; however, it comes at the cost of non-negligible latencies. We advocate for novel architectures that combine microkernels' flexibility and strong fault encapsulation with SPH's simplicity and minimalist latencies by hosting per-partition VMMs directly at the hypervisor privilege level. Such a design could arguably be achieved by combining multikernel-like architectures \cite{baumann2009} and per-core memory protection mechanisms (e.g., Armv9 RME's GPT \cite{li2022}, or RISC-V PMP \cite{lee2020}) statically configured by firmware.

\mypara{Full IO Passthrough.} Pure static partitioning supports only passthrough IO. However, as highlighted by  \cite{Ramsauer2018}, there is a critical problem in providing full IO passthrough when controls over IO resources such as clock, reset, power, or pin-muxes cannot be securely partitioned or shared, e.g., if their MMIO registers reside on the same frame or they are configured via platform management co-processors oblivious of SPH's VMs. Thus, SPH should provide controlled guest access to these resources by emulation or through standard interfaces such as SCMI \cite{arm2022scmi}. Nevertheless, this would require including drivers in the hypervisor, increasing its code base. Again, we urge hardware designers to provide hardware primitives that enable SPH to pass through IO resource controls.

%-------------------------------------------------------------------------

%\newpage
\vspace{-0.08cm}
\section{Related Work} \label{sec:related} 

%\mypara{Embedded Hypervisors.} Over the past decade, many hypervisors have been designed or retrofitted for MCS. 
%Quest-V \cite{West2016, Golchin2022} essentially pioneered the SPH architecture, but only has support for x86. Other x86-only solutions include Muen \cite{buerki2013} and ACRN \cite{Li2019}. %Muen provides formal guarantees (is written in the Spark) and ACRN was built for MCS but still allows for more flexible configuration and device sharing. 
%Xtratum \cite{Crespo2010} is a hypervisor of particular interest in the aerospace but lacks support for Armv8-A. 
%Xvisor \cite{Patel2015} is an embedded hypervisor that targets mainly soft real-time applications. 
%The NOVA microhypervisor \cite{Steinberg2010} follows a similar design to seL4, but is tailored for virtualization. 
%LTZVisor \cite{Pinto2017} and VOSYS \cite{Lucas2018} have leveraged TrustZone to build mainly dual-guest systems; RTZVisor \cite{Martins2017} provides multi-guest support.
%Mature closed commercial offerings such as LynxSecure, PikeOs, VXworks, Integrity, or Coqos or newcomers such as CLARE also allow for static partitioning and provide features to avoid inter-VM interference. 
%, SafeG \cite{Sangorrin2010},
%Despite it tight Independence with Linux, KVM has also been considered for embedded real-time use \cite{?}. 

%Maybe mention container work, openamp
% \textcolor{red}{This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract. This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters.}
% \textcolor{red}{This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract. This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components. This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components. This and the IEEEtran.cls file define the components. This and the IEEEtran.cls file define the components.}

%\mypara{Hypervisor Studies \& Evaluations.} 
There are several hypervisor analyses in the context of embedded and MCSs, but none provide a cross-section analysis and comparison on SPH. Some works focus on a single hypervisor while others evaluate a single metric or feature. In \cite{Patel2015}, authors compare the performance of Xvisor with Xen and KVM. Others have evaluated the effectiveness of cache coloring and bandwidth reservations in Xvisor \cite{Modica2018}. Similarly, in \cite{Kloda2019}, authors evaluate cache and DRAM bank coloring in Jailhouse. Other works have evaluated Jailhouse interrupt latency \cite{Pavic2018} or VM interference \cite{Danielsson2019}. There are also studies about the feasibility of using Xen and KVM as real-time hypervisors \cite{Abeni2020}, but mainly for x86. Little has been published regarding the new Xen Dom0-less and cache coloring features, but results can be found in \cite{Stabellini2020}. Evaluation of the seL4 CAmkES VMM has also been done for performance and interrupt latency \cite{millwood2020}. There have been works providing a qualitative analysis for MCS hypervisors, contrasting architectural approaches and highlighting future trends \cite{Cinque2022} while others layout guidelines on how to choose such a hypervisor in industrial settings \cite{Hamelin2020}.
%these studies are performed in the context server and cloud applications, focus mainly on performance, and target the x86 architecture. There
%, highlighting the advantages of coloring for minimizing interrupt latency

%-------------------------------------------------------------------------
\vspace{-0.08cm}
\section{Conclusion} \label{sec:concl} 

We have conducted the most comprehensive empirical evaluation of open-source SPH to date, focusing on key metrics for MCS. With that, we drew a set of observations that (i) will help industrial practitioners understand the trade-offs of SPH and (ii) raise awareness of the research and open-source communities to the still open problems in SPH. We are opening all artifacts to enable independent validation of results and encourage further exploration on SPH.

%\par \textcolor{red}{We have conducted an extensive empirical evaluation of four static partitioning hypervisors, focusing on key metrics for MCS. We observed that interrupt latency and boot time are directly impacted by virtualization overheads, which can be aggravated by well-known inter-VM interference at shared memory hierarchy components. We conclude this is due to the insufficient hardware support and incomplete inter-VM interference mitigation mechanisms implemented in these hypervisors. }
  
% %---------------------------------------------------------
% \section{Notice}

% \textbf{This work was developed by Jose Martins and Sandro Pinto. For internal analysis only. Under review at RTAS. Please do not distribute and share without permission.}

%---------------------------------------------------------
\section{Acknowledgments}

We would like to express our gratitude to the reviewers for their valuable feedback and suggestions, as well as to our friendly shepherd for guiding us in making final improvements. Additionally, we appreciate the time and thoughtful input from all the representatives of SPH, namely Ralf Ramsauer (Jailhouse), Stefano Stabellini (Xen), and Gernot Heiser (seL4/CAmkES-VMM). 
Jos√© Martins was supported by FCT grant SFRH/BD/138660/2018. This work is supported by FCT ‚Äì Funda√ß√£o para a Ci√™ncia e Tecnologia within the R\&D Units Project Scope UIDB/00319/2020, and European Union‚Äôs Horizon Europe research and innovation program under grant agreement No 101070537, project CROSSCON (Cross-platform Open Security Stack for Connected Devices).

%-------------------------------------------------------------------------
%\newpage

%%
%% Bibliography
%%

\bibliographystyle{IEEEtran}
\bibliography{shedding-light.bib}

%\newpage
%\appendix

%\section{...}

%\begin{figure*}[]
%    \centering
%    \includegraphics[width=0.99\textwidth]{images/mibench-interf-hypcol.pdf}
%    \vspace{-0.30cm}
%    \caption{.}
%    \label{fig:mibench-interf-hypcol}
%    \vspace{-0.35cm}
%\end{figure*}

\end{document}
