\newpage
\setcounter{page}{1}
\begin{center}
{\bf \Large  Supplementary Material\\for\\``Improving instrumental variable estimators with post-stratification''}
\end{center}

%%%%%%%%%%%%2SLS%%%%%%%%%%%%%%%%
\section{Code and replication files}
An R package to implement all the post-stratified instrumental variable methods discussed in the paper can be found at \url{https://github.com/lmiratrix/poststratIV}.
This repository also includes replication files for all simulations and the GOTV applications.



\section{Equivalence with weighted Two-Stage Least Squares}\label{append:2sls}
When performing a generic weighted least squares regression of $y$ and $x$ ($y_i = \beta_0 + \beta_1x_i + \epsilon_i$) with weights $w$, we have the following formulas for the regression estimates:
\begin{align}
\hat{\beta}_1 &= \frac{\sum_{i=1}^nw_i(y_i-\overline{y}_w)(x_i - \overline{x}_w)}{\sum_{i=1}^nw_i(x_i - \overline{x}_w)^2}\label{eq:beta_1}\\ 
\hat{\beta}_0 &= \overline{y}_w  - \hat{\beta}_1\overline{x}_w \nonumber
\end{align}
where $\overline{y}_w = \sum_{i=1}^nw_iy_i/\sum_{i=1}^nw_i$ and $\overline{x}_w$ is defined analogously.

\subsection{First stage results}
\begin{result}
Consider performing a weighted regression of $D$ on $Z$ ($D_i = \beta_{0,S1} + \beta_{1,S1}Z_i + \epsilon_i$) with weights $w_i = \frac{N_{g}}{N_{g,z}}\frac{n_z}{N}$ for unit $i$ in strata $g \in \{1,\dots,G\}$ assigned to treatment $z \in \{0,1\}$.
Then our estimate for coefficient for $Z$ is $\hat{\beta}_{1, S1} =\widehat{f}_{PS}$ and for the intercept it is $\hat{\beta}_{0, S1} =\overline{D}_{w,0}^{obs}$.
The predicted values from this model are $D^{\text{pred}}_i = Z_i\overline{D}_{w,1}^{obs} + (1-Z_i)\overline{D}_{w,0}^{obs}$.
\end{result}



\begin{proof}
That the coefficient from the model described will be equivalent to the blocked treatment effect estimator is an established result \citep[see, e.g., ][]{pashley2021insights} but we will here provide a proof and derive the predicted values for completeness.

First, we will find the weighted means for $D$ and $Z$.
\begin{align*}
\overline{Z}_w &= \sum_{i=1}^nw_iZ_i/\sum_{i=1}^nw_i \\
&= \left[\sum_{g=1}^G\sum_{i:s_i = g}Z_i\frac{N_{g}}{N_{g,1}}\frac{n_1}{N}\right]/\left[\sum_{g=1}^G\sum_{i:s_i = g}\left(Z_i\frac{N_{g}}{N_{g,1}}\frac{n_1}{N} + (1-Z_i)\frac{N_{g}}{N_{g,0}}\frac{n_0}{N}\right)\right]\\
&= \left[\sum_{g=1}^GN_{g}\frac{n_1}{N}\right]/\left[\sum_{g=1}^G\left(N_{g}\frac{n_1}{N} + N_{g}\frac{n_0}{N}\right)\right]\\
&= n_1/N
\end{align*}

\begin{align*}
\overline{D}_w &= \sum_{i=1}^nw_iD_i/\sum_{i=1}^nw_i \\
&=\frac{1}{N} \sum_{g=1}^G\sum_{i:s_i = g}\left[Z_iD_i(1)\frac{N_{g}}{N_{g,1}}\frac{n_1}{N} + (1-Z_i)D_i(0)\frac{N_{g}}{N_{g,0}}\frac{n_0}{N}\right]\\
&=\frac{1}{N} \sum_{g=1}^G\left[\overline{D}_{g,1}^{obs}(N_{g}\frac{n_1}{N} + \overline{D}_{g,0}^{obs}N_{g}\frac{n_0}{N}\right]\\
&=\frac{n_1}{N}\overline{D}_{w,1}^{obs} + \frac{n_0}{N}\overline{D}_{w,0}^{obs},
\end{align*}
where $\overline{D}_{w,z}^{obs} = \sum_{g=1}^G\frac{N_g}{N}\overline{D}_{g,z}^{obs}$ for $z \in \{0,1\}$.


Then for the denominator of $\hat{\beta}_{1, S1}$, following Equation~(\ref{eq:beta_1}), we have
\begin{align*}
\sum_{i=1}^nw_i(Z_i - \overline{Z}_w)^2 &=  \sum_{g=1}^G\sum_{i:s_i = g}\Big[Z_i\frac{N_{g}}{N_{g,1}}\frac{n_1}{N} \left(1 - n_1/N\right)^2 + (1-Z_i)\frac{N_{g}}{N_{g,0}}\frac{n_0}{N} \left( -  n_1/N\right)^2\Big]\\
&=  \sum_{g=1}^G\Big[N_{g}\frac{n_1}{N} \frac{n_0^2}{N^2} + N_{g}\frac{n_0}{N}\frac{n_1^2}{N^2}\Big]\\
&=  \frac{n_1n_0}{N}.
\end{align*}


For the numerator we have 
\begin{align*}
&\sum_{i=1}^nw_i(D_i - \overline{D}_w)(Z_i - \overline{Z}_w)\\
 &=  \sum_{g=1}^G\sum_{i:s_i = g}\Big[Z_i\frac{N_{g}}{N_{g,1}}\frac{n_1}{N} \left(D_i(1) - \frac{n_1}{N}\overline{D}_{w,1}^{obs} - \frac{n_0}{N}\overline{D}_{w,0}^{obs}\right)\left(1 - n_1/N\right) \\
&\qquad \qquad  \qquad+ (1-Z_i)\frac{N_{g}}{N_{g,0}}\frac{n_0}{N} \left(D_i(0) - \frac{n_1}{N}\overline{D}_{w,1}^{obs} - \frac{n_0}{N}\overline{D}_{w,0}^{obs}\right)\left( -  n_1/N\right)\Big]\\
&= \frac{n_1n_0}{N^2} \sum_{g=1}^GN_g\sum_{i:s_i = g}\Big[\frac{Z_i}{N_{g,1}} \left(D_i(1) - \frac{n_1}{N}\overline{D}_{w,1}^{obs} - \frac{n_0}{N}\overline{D}_{w,0}^{obs}\right)\\
&\qquad \qquad  \qquad  \qquad  \qquad - \frac{1-Z_i}{N_{g,0}} \left(D_i(0) - \frac{n_1}{N}\overline{D}_{w,1}^{obs} - \frac{n_0}{N}\overline{D}_{w,0}^{obs}\right)\Big]\\
&= \frac{n_1n_0}{N^2} \sum_{g=1}^GN_g\Big[ \overline{D}_{g,1}^{obs} - \frac{n_1}{N}\overline{D}_{w,1}^{obs} - \frac{n_0}{N}\overline{D}_{w,0}^{obs}- \overline{D}_{g,0}^{obs} + \frac{n_1}{N}\overline{D}_{w,1}^{obs} + \frac{n_0}{N}\overline{D}_{w,0}^{obs}\Big]\\
&= \frac{n_1n_0}{N^2} \sum_{g=1}^GN_g\Big[ \overline{D}_{g,1}^{obs} - \overline{D}_{g,0}^{obs} \Big]\\
&= \frac{n_1n_0}{N}\widehat{f}_{PS}.
\end{align*}

All together,
\[\hat{\beta}_{1, S1} =\widehat{f}_{PS}. \]

The intercept is 
\[\hat{\beta}_0 =\frac{n_1}{N}\overline{D}_{w,1}^{obs} + \frac{n_0}{N}\overline{D}_{w,0}^{obs} - \frac{n_1}{N}\widehat{f}_{PS} =\overline{D}_{w,0}^{obs} .\]

The predicted values are 
\[{D}^{\text{pred}}_i = Z_i\widehat{f}_{PS} +\overline{D}_{w,0}^{obs} = Z_i\overline{D}_{w,1}^{obs} + (1-Z_i)\overline{D}_{w,0}^{obs}. \]

\end{proof}


\subsection{Second stage results}

\begin{result}
Lemma~\ref{lemma:2sls_connection}:
Consider performing the second-stage weighted regression of $Y$ on $D^{pred}$  ($Y_i = \beta_{0,S2} + \beta_{1,S2}D^{\text{pred}}_i + \epsilon_i$) with weights $w_i = \frac{N_{g}}{N_{g,z}}\frac{n_z}{N}$ for unit $i$ in strata $g \in \{1,\dots,G\}$ assigned to treatment $z \in \{0,1\}$.
Then our estimate of the coefficient for $D^{pred}$ is $\hat{\beta}_{1, S2} = \widehat{\CACE}_{\text{IV-a}}$.
\end{result}



\begin{proof}
Following the same logic we used to find $\overline{D}_w$, the weighted mean for the outcomes is
\begin{align*}
\overline{Y}_w 
&=\frac{n_1}{N}\overline{Y}_{w,1}^{obs} + \frac{n_0}{N}\overline{Y}_{w,0}^{obs},
\end{align*}
where $\overline{Y}_{w,z}^{obs} = \sum_{g=1}^G\frac{N_g}{N}\overline{Y}_{g,z}^{obs}$ for $z \in \{0,1\}$.

For the predicted uptake, the weighted mean is
\begin{align*}
\overline{D}_w^{pred} 
&=\frac{n_1}{N}\overline{D}_{w,1}^{obs} + \frac{n_0}{N}\overline{D}_{w,0}^{obs} = \overline{D}_w.
\end{align*}


Using Equation~(\ref{eq:beta_1}), the denominator of $\hat{\beta}_{1, S2}$ is
\begin{align*}
&\sum_{i=1}^nw_i(D_i^{pred} - \overline{D}_w)^2 \\
&=  \sum_{g=1}^G\sum_{i:s_i = g}\Big[Z_i\frac{N_{g}}{N_{g,1}}\frac{n_1}{N} \left(\overline{D}_{w,1}^{obs} - \frac{n_1}{N}\overline{D}_{w,1}^{obs} - \frac{n_0}{N}\overline{D}_{w,0}^{obs}\right)^2\\
& \qquad \qquad \qquad  + (1-Z_i)\frac{N_{g}}{N_{g,0}}\frac{n_0}{N} \left(\overline{D}_{w,0}^{obs} - \frac{n_1}{N}\overline{D}_{w,1}^{obs} - \frac{n_0}{N}\overline{D}_{w,0}^{obs}\right)^2\Big]\\
&=  \sum_{g=1}^G\sum_{i:s_i = g}\Big[Z_i\frac{N_{g}}{N_{g,1}}\frac{n_1}{N}\frac{n_0^2}{N^2} \left(\overline{D}_{w,1}^{obs} - \overline{D}_{w,0}^{obs}\right)^2  + (1-Z_i)\frac{N_{g}}{N_{g,0}}\frac{n_0}{N}\frac{n_1^2}{N^2} \left(\overline{D}_{w,1}^{obs} - \overline{D}_{w,0}^{obs}\right)^2\Big]\\
&=  \frac{n_1n_0}{N} \left(\overline{D}_{w,1}^{obs} - \overline{D}_{w,0}^{obs}\right)^2\\
& = \frac{n_1n_0}{N}\widehat{f}_{PS}^2.
\end{align*}

For the numerator we have
\begin{align*}
&\sum_{i=1}^nw_i(D_i^{pred} - \overline{D}_w)(Y_i - \overline{Y}_w) \\
&=  \sum_{g=1}^G\sum_{i:s_i = g}\Big[Z_i\frac{N_{g}}{N_{g,1}}\frac{n_1}{N}\frac{n_0}{N} \left(\overline{D}_{w,1}^{obs} - \overline{D}_{w,0}^{obs}\right)\left(Y_i(1) - \frac{n_1}{N}\overline{Y}_{w,1}^{obs} - \frac{n_0}{N}\overline{Y}_{w,0}^{obs}\right) \\
& \qquad \qquad \qquad  - (1-Z_i)\frac{N_{g}}{N_{g,0}}\frac{n_0}{N}\frac{n_1}{N} \left(\overline{D}_{w,1}^{obs} - \overline{D}_{w,0}^{obs}\right)\left(Y_i(0) - \frac{n_1}{N}\overline{Y}_{w,1}^{obs} - \frac{n_0}{N}\overline{Y}_{w,0}^{obs}\right)\Big]\\
&= \frac{n_1n_0}{N^2}\widehat{f}_{PS} \sum_{g=1}^GN_g\sum_{i:s_i = g}\Big[\frac{Z_i}{N_{g,1}}\left(Y_i(1) - \frac{n_1}{N}\overline{Y}_{w,1}^{obs} - \frac{n_0}{N}\overline{Y}_{w,0}^{obs}\right)\\
& \qquad \qquad \qquad  \qquad \qquad \qquad \qquad  - \frac{1-Z_i}{N_{g,0}}\left(Y_i(0) - \frac{n_1}{N}\overline{Y}_{w,1}^{obs} - \frac{n_0}{N}\overline{Y}_{w,0}^{obs}\right)\Big]\\
&= \frac{n_1n_0}{N^2}\widehat{f}_{PS} \sum_{g=1}^GN_g\Big[\left(\overline{Y}_{g,1}^{obs} - \frac{n_1}{N}\overline{Y}_{w,1}^{obs} - \frac{n_0}{N}\overline{Y}_{w,0}^{obs}\right) - \left(\overline{Y}_{g,0}^{obs} - \frac{n_1}{N}\overline{Y}_{w,1}^{obs} - \frac{n_0}{N}\overline{Y}_{w,0}^{obs}\right)\Big]\\
&= \frac{n_1n_0}{N^2}\widehat{f}_{PS} \sum_{g=1}^GN_g\Big[\overline{Y}_{g,1}^{obs} - \overline{Y}_{g,0}^{obs}\Big]\\
&= \frac{n_1n_0}{N}\widehat{f}_{PS}\widehat{\ITT}_{\text{PS}}.
\end{align*}

Putting it together, we have $\hat{\beta}_{1, S1} = \widehat{\ITT}_{\text{PS}}/\widehat{f}_{PS} = \widehat{\CACE}_{\text{IV-a}}$.

\end{proof}


%%%%%%%%%%%%IV VARIANCE DELTA%%%%%%%%%%%%%%%%
\section{IV variance via delta method}\label{append:iv_var}
In this section, we show how the treatment uptake variance and CLT conditions simplify to those giving in the paper.

\subsection{Simplification of variance and CLT conditions for treatment uptake}\label{app:var_cond_simple}
We first show the simplified variance expressions we use in Section~\ref{sec:standard_iv_est}.
We have the following simplifications of variances for treatment uptake:
\begin{align*}
S^2_D(1) &= \frac{1}{N-1}\sum_{i=1}^N(D_i(1) - \overline{D}(1))^2\\
&= \frac{1}{N-1}\left(N\pi_{n}(\pi_{c} + \pi_{a})^2 + N(\pi_{c} + \pi_{a})\pi_{n}^2\right)\\
&= \frac{N}{N-1}\pi_{n}(\pi_{c} + \pi_{a})
\end{align*}
\begin{align*}
S^2_D(0) &= \frac{1}{N-1}\sum_{i=1}^N(D_i(0) - \overline{D}(0))^2\\
&= \frac{1}{N-1}\left(N\pi_{a}(\pi_{c} + \pi_{n})^2 + N(\pi_{c} + \pi_{n})\pi_{a}^2\right)\\
&= \frac{N}{N-1}\pi_{a}(\pi_{c} + \pi_{n})
\end{align*}
and
\begin{align*}
S^2_D(01) &= \frac{1}{N-1}\sum_{i=1}^N(D_i(1) -D_i(0)  - \overline{D}(1) + \overline{D}(0))^2\\
&= \frac{1}{N-1}\left(N\pi_{c}(\pi_{a} + \pi_{n})^2 + N(\pi_{a} + \pi_{n})\pi_{c}^2 \right)\\
&= \frac{N}{N-1}\pi_{c}(\pi_{a} + \pi_{n}).
\end{align*}

We also have 
\[v_{D}(z) = \max_{1\leq i \leq N}\left(D_i(z) - \overline{D}(z)\right)^2\]
where
\begin{align*}
\max_{1\leq i \leq N}\left(D_i(1) - \overline{D}(1)\right)^2 = \max \left(\mathbb{I}\left(\pi_n \neq 0\right)(\pi_{c} + \pi_{a})^2, \mathbb{I}\left(\pi_n \neq 1\right)\pi_{n}^2\right)
\end{align*}
and
\begin{align*}
\max_{1\leq i \leq N}\left(D_i(0) - \overline{D}(0)\right)^2 = \max \left(\mathbb{I}\left(\pi_a \neq 0\right)(\pi_{c} + \pi_{n})^2, \mathbb{I}\left(\pi_a \neq 1\right)\pi_{a}^2\right)
\end{align*}

The condition from \cite{LiDin17} (Theorem 4) to obtain a finite-population central limit result for $\hat{f}$ is
\begin{align*}
Q_n \coloneqq \max_{z \in\{0,1\}}\frac{1}{n_z^2}\frac{v_{D}(z)}{n_0^{-1}S^2_D(0) + n_1^{-1}S^2_D(1) - N^{-1}S^2_D(01)} \to 0
\end{align*}
but
\begin{align*}
Q_n < \frac{1}{\min(p, 1-p)N}\frac{1}{ \frac{N}{N-1}\pi_{a}(\pi_{c} + \pi_{n}) + p^{-1}\frac{N}{N-1}\pi_{n}(\pi_{c} + \pi_{a}) - \frac{N}{N-1}\pi_{c}(\pi_{a} + \pi_{n})}.
\end{align*}

This is clearly satisfied if $\pi_{c}$, $\pi_{a}$, and $\pi_{n}$ have asymptotic limiting values such that at least two of those proportions are bounded away from zero.
As the second term will go to 0 as $N \to \infty$, we have our condtion.
Therefore, if two of $\pi_{c}$, $\pi_{a}$, and $\pi_{n}$ are nonzero, $\hat{f}$ is asymptotically normal.

\subsection{Finite-population CLT assumptions}\label{append:clt_post_iv}

In order to consider asymptotic variance, we need to fix an asymptotic regime to work within.
Here, we will assume a finite number of strata, with the size of each growing to infinity, as outlined in the following assumption.
\begin{assumption}\label{assump:strata_prop}
The number of strata $G$ is fixed and the number of units $N_g$ grows as $N \to \infty$, such that $N_g/N \to h_g$, where $h_g \in (0,1)$ is some constant.
\end{assumption}

We will use various additional assumptions for the CLTs of the post-stratified estimators:
\begin{assumption}\label{assump:li_ding_cond_main}
Define $c_g(z) =\max_{1 \leq i \leq N} \frac{N^2}{N_g^2}W_i(g)\left(Y_i(z) - \overline{Y}_g(z)\right)^2$,
\[v(z) = \frac{1}{N-1}\sum_{g=1}^G\sum_{i:s_i=g}\left(Y_i(z) - \overline{Y}_g(z)\right)^2,\]
and
\[v(01)= \frac{1}{N-1}\sum_{g=1}^G\sum_{i:s_i=g}\left(Y_i(1) - Y_i(0) -\left[ \overline{Y}_g(1) - \overline{Y}_g(0) \right]\right)^2\]
As $N \to \infty$
\[\max_{z \in {0,1}}\max_{g \in \{1,\dots,G\}}\frac{1}{n_z^2}\frac{c_g(z) }{n_0^{-1}v(0) + n_1^{-1}v(1) - N^{-1}v(01)} \to 0.\]
\end{assumption}
\begin{assumption}\label{assump:clt_cond2_main}
$S^2_{g,Y}(z)$ and $S^2_{g,Y}(1,0)$ have finite limiting values as $N \to\infty$.
\end{assumption}
\begin{assumption}\label{assump:delta_post_strat}
$N\text{var}(\widehat{\ITT}_{\text{PS}})$ has a finite limiting value, to help ensure $\widehat{\ITT}_{\text{PS}} - \ITT \overset{p}{\to} 0 $.
\end{assumption}




\subsection{CLT results for post-stratification ITT estimators}\label{append:clt_itt}
We first show that both $\widehat{\ITT}_{\text{PS}}$ and $\widehat{f}_{\text{PS}}$ have asymptotic normal distributions.

\begin{lemma}\label{lem:post_strat_clt_y}
If we have Assumptions \ref{assump:iv} (Part A), \ref{assump:strata_prop}, \ref{assump:li_ding_cond_main}, and \ref{assump:clt_cond2_main} then by results from \cite{schochet2023design},
\begin{align*}
 \frac{\widehat{\ITT}_{\text{PS}} - \ITT}{\sqrt{\text{asyVar}(\widehat{\ITT}_{\text{PS}})}} \overset{d}{\to} N(0,1).
\end{align*}
where
\begin{align*}
\text{asyVar}\left(\widehat{\ITT}_{\text{PS}}\right) 
& =  \sum_{g=1}^N\frac{N_g}{N}\frac{N_g-1}{N-1}\left[\frac{S^2_{g,Y}(0)}{(1-p)N_g} +\frac{S^2_{g,Y}(1)}{pN_g} - \frac{S^2_{g, Y}(01)}{N_g}\right]\\
& \approx  \sum_{g=1}^N\frac{N_g^2}{N^2}\left[\frac{S^2_{g,Y}(0)}{(1-p)N_g} +\frac{S^2_{g,Y}(1)}{pN_g} - \frac{S^2_{g, Y}(01)}{N_g}\right].
\end{align*}
\end{lemma}
We note that we simplify some of the conditions from \cite{schochet2023design} because (i) we do not have additional covariate adjustment (ii) the weights in our setting correspond to the post-stratification weighting of units, and Assumption~\ref{assump:strata_prop} sets limits on the asymptotic behavior of those weights.

For $\widehat{f}_{\text{PS}}$ we need an extension of above:
\begin{assumption}\label{assump:clt_cond_d}
Assume that $\pi_{g,c}$, $\pi_{g,a}$, and $\pi_{g,n}$ have limiting values as $N \to \infty$.
Also assume at least two of $\pi_{g,c}$, $\pi_{g,a}$, and $\pi_{g,n}$ are asymptotically bounded away from zero for at least one $g \in \{1,\dots,G\}$.
\end{assumption}

\begin{lemma}\label{lem:post_strat_clt_d}
If we have Assumptions~\ref{assump:iv} (Part A), \ref{assump:strata_prop} and \ref{assump:clt_cond_d} 
then by results from \cite{schochet2023design},
\begin{align*}
\frac{\widehat{f}_{\text{PS}} - \pi_c}{\sqrt{\text{asyVar}(\widehat{f}_{\text{PS}})}} \overset{d}{\to} N(0,1),
\end{align*}
where
\begin{align*}
\text{asyVar}(\widehat{f}_{\text{PS}}) &= \frac{1}{N-1}\sum_{g=1}^GN_g\left[\frac{ \left(\pi_{g,c} + \pi_{g,a}\right)\pi_{g,n} }{n_0} + \frac{\left(\pi_{g,c} + \pi_{g,n}\right)\pi_{g,a}}{n_1} - \frac{1}{N}\left(\pi_{g,a} + \pi_{g,n}\right)\pi_{g,c}\right]\\
&= \sum_{g=1}^G\frac{N_g}{N(N-1)}\left[(1-p)^{-1}\left(\pi_{g,c} + \pi_{g,a}\right)\pi_{g,n} + p^{-1}\left(\pi_{g,c} + \pi_{g,n}\right)\pi_{g,a} - \left(\pi_{g,a} + \pi_{g,n}\right)\pi_{g,c}\right].
\end{align*}
\end{lemma}
In this case, the bounded nature of uptake $D$, along with Assumption~\ref{assump:clt_cond_d}  implies some of the usual conditions for the central limit theorem hold.



\section{Bias reduction}
\subsection{One-sided noncompliance}\label{append:one_sided_bias}
First let $\overline{Y}_c(z)$ be the average potential outcome under treatment $z$ among compliers.
Similarly, let $\overline{Y}_n(z)$ be the average potential outcome under treatment $z$ among noncompliers (never-takers) and note that $\overline{Y}_n(1)=\overline{Y}_n(0) = \overline{Y}_n$.
Let $\widehat{\overline{Y}}_c(z)$ and $\widehat{\overline{Y}}_n(z)$ be the corresponding estimates we would get based on treatment assignment if we could observe who is a complier or noncomplier.
Then we can write
\begin{align*}
\widehat{\ITT} &= \overline{Y}^{\text{obs}}(1) -  \overline{Y}^{\text{obs}}(0)\\
&= \hat{f}\widehat{\overline{Y}}_c(1) + (1-\hat{f})\widehat{\overline{Y}}_n(1) -\frac{n_c - n_1\hat{f}}{n_0}\widehat{\overline{Y}}_c(0) -\frac{n_0 - n_c + n_1\hat{f}}{n_0}\widehat{\overline{Y}}_n(0)\\
&=\hat{f}\left(\widehat{\overline{Y}}_c(1) - \widehat{\overline{Y}}_n +\frac{n_1}{n_0}\widehat{\overline{Y}}_c(0) - \frac{n_1}{n_0}\widehat{\overline{Y}}_n\right)
+\left(\widehat{\overline{Y}}_n - \frac{n_c}{n_0}\widehat{\overline{Y}}_c(0) - \frac{n_0 - n_c}{n_0}\widehat{\overline{Y}}_n\right)\\
&=\hat{f}\left(\widehat{\overline{Y}}_c(1)  +\frac{n_1}{n_0}\widehat{\overline{Y}}_c(0) - \frac{N}{n_0}\widehat{\overline{Y}}_n\right)
+\left( - \frac{n_c}{n_0}\widehat{\overline{Y}}_c(0) + \frac{n_c}{n_0}\widehat{\overline{Y}}_n\right)
\end{align*}

Therefore,
\begin{align*}
E\left[\widehat{\CACE}\right] &= E\left[E\left[\frac{\widehat{\ITT}}{\hat{f}}|\hat{f}\right]\right]\\
&= E\left[\left(\widehat{\overline{Y}}_c(1)  +\frac{n_1}{n_0}\widehat{\overline{Y}}_c(0) - \frac{N}{n_0}\widehat{\overline{Y}}_n\right) + \frac{1}{\hat{f}}E\left[ - \frac{n_c}{n_0}\widehat{\overline{Y}}_c(0) + \frac{n_c}{n_0}\widehat{\overline{Y}}_n|\hat{f}\right]\right]\\
&= \left(\overline{Y}_c(1)  +\frac{n_1}{n_0}\overline{Y}_c(0)  - \frac{N}{n_0}\overline{Y}_n\right) +E\left[ \frac{1}{\hat{f}}\right]\left( - \frac{n_c}{n_0}\overline{Y}_c(0) + \frac{ n_c}{n_0}\overline{Y}_n\right)\\
&= \overline{Y}_c(1) -\left( \frac{N}{n_0} - E\left[ \frac{1}{\hat{f}}\right]\frac{E[\hat{f}]N}{n_0}\right)\overline{Y}_n - \left(E\left[ \frac{1}{\hat{f}}\right]\frac{E[\hat{f}]N}{n_0} - \frac{n_1}{n_0}\right)\overline{Y}_c(0)\\
&= \overline{Y}_c(1) -\frac{1}{1-p}\left(1  - E\left[ \frac{1}{\hat{f}}\right]E[\hat{f}]\right)\overline{Y}_n - \frac{1}{1-p}\left(E\left[ \frac{1}{\hat{f}}\right]E[\hat{f}] - p\right)\overline{Y}_c(0)\\
\end{align*}

The bias of $\widehat{\CACE}_{\text{IV}}$ is then
\begin{align*}
E\left[\widehat{\CACE}_{\text{IV}}\right] - \CACE &=  -\frac{1}{1-p}\left(1  - E\left[ \frac{1}{\hat{f}}\right]E[\hat{f}]\right)\overline{Y}_n - \frac{1}{1-p}\left(E\left[ \frac{1}{\hat{f}}\right]E[\hat{f}] - 1\right)\overline{Y}_c(0)\\
&=  \frac{1}{1-p}\left(1  - E\left[\frac{1}{ \hat{f}}\right]E[\hat{f}]\right)\left( \overline{Y}_c(0)-\overline{Y}_n\right)
\end{align*}
Note that $\text{cov}\left(\hat{f}, \frac{1}{\hat{f}}\right) = 1  - E\left[ \frac{1}{\hat{f}}\right]E[\hat{f}]$, so this piece is always negative.

We can get an estimation of the bias using a Taylor expansion.
To fo this we need to get the moments of $\hat{f}$. Under a completely randomized design with one-sided noncompliance, $\hat{f}$ will follow a hypergeometric distribution.
Using a binomial will give us a reasonable (and simpler) approximation.

\[g(\hat{f}) \approx g(\pi_c) + g'(\pi_c)(\hat{f} -\pi_c) +  \frac{g^{''}(\pi_c)}{2}(\hat{f} - \pi_c)^2 + \frac{g^{'''}(\pi_c)}{3!}(\hat{f} - \pi_c)^3 + \frac{g^{(4)}(\pi_c)}{4!}(\hat{f} - \pi_c)^4. \]
Here we have $g(\hat{f}) = \frac{1}{\hat{f}}$ so
\[\frac{1}{\hat{f}} \approx \frac{1}{\pi_c} - \frac{1}{\pi_c^2}(\hat{f} -\pi_c) +  \frac{1}{\pi_c^3}(\hat{f} - \pi_c)^2 - \frac{1}{\pi_c^4}(\hat{f} - \pi_c)^3 + \frac{1}{\pi_c^5}(\hat{f} - \pi_c)^4. \]
Taking expectations of both sides,
\begin{align*}
E\left[\frac{1}{\hat{f}}\right] &\approx \frac{1}{\pi_c} - 0 +  \frac{1}{\pi_c^3}E[(\hat{f} - \pi_c)^2] - \frac{1}{\pi_c^4}E[(\hat{f} - \pi_c)^3 ]+ \frac{1}{\pi_c^5}E[(\hat{f} - \pi_c)^4]\\
&\text{Using binomial as an approximation for the moments...}\\
&\approx \frac{1}{\pi_c} +  \frac{\pi_c(1-\pi_c)}{Np\pi_c^3} - \frac{\pi_c(1-\pi_c)(1-2\pi_c)}{(Np)^2\pi_c^4}+ \frac{\pi_c(1-\pi_c)(1+(3Np - 6)\pi_c(1-\pi_c))}{(Np)^3\pi_c^5}\\
&\approx \frac{1}{\pi_c} +  \frac{\text{var}(\hat{f})}{\pi_c^3}\left[1 - \frac{1-2\pi_c}{(Np)\pi_c}+ \frac{1+(3Np - 6)\pi_c(1-\pi_c)}{(Np)^2\pi_c^2}\right].
\end{align*}

We see that higher variability of $\hat{f}$ will increase the bias.
Another sensible thing this approximation reveals is that bias will be larger the lower the compliance rate is.


Using hypergeometric moments (based on the completely randomized assignment distribution) instead gives:
\begin{align*}
E\left[\frac{1}{\hat{f}}\right] &\approx \frac{1}{\pi_c} - 0 +  \frac{1}{\pi_c^3}E[(\hat{f} - \pi_c)^2] - \frac{1}{\pi_c^4}E[(\hat{f} - \pi_c)^3 ]+ \frac{1}{\pi_c^5}E[(\hat{f} - \pi_c)^4]\\
& = \frac{1}{\pi_c}+  \frac{\text{var}(\hat{f})}{\pi_c^2}\left[1 - \frac{1}{\pi_c^2}\frac{(1-2\pi_c)(1-2p)}{N-2} + \frac{1}{\pi_c^3}c_1\right]
\end{align*}
where $c_1$ is a constant related to the kurtosis.

More details on the moments:
\begin{align*}
E[(\hat{f}-\pi_c)^2] &= \text{var}(\hat{f}) = \frac{p(1-p)\pi_c(1-\pi_c)}{N-1}
\end{align*}

\begin{align*}
E[(\hat{f}-\pi_c)^3] &= \left(\text{var}(\hat{f})\right)^{3/2}\frac{(N-2n_c)(\sqrt{N-1})(N-2pN)}{(N-2)\sqrt{pn n_c(N-n_c)(1-p)N}}\\
&= \text{var}(\hat{f})\frac{\sqrt{p(1-p)\pi_c(1-\pi_c)}}{\sqrt{N-1}}\frac{(1-2\pi_c)(\sqrt{N-1})(1-2p)}{(N-2)\sqrt{p(1-p) \pi_c(1-\pi_c)}}\\
&= \text{var}(\hat{f})\frac{(1-2\pi_c)(1-2p)}{(N-2)}
\end{align*}

\begin{align*}
E[(\hat{f}-\pi_c)^4]
 &=\text{var}(\hat{f})c_1
\end{align*}
where $c_1$ is some constant related to kurtosis of order $1/N$.

Then we have
\begin{align*}
&E\left[\widehat{\CACE}_{\text{IV}}\right] - \CACE\\
 &=  \frac{1}{1-p}\left(1  - E\left[\frac{1}{ \hat{f}}\right]E[\hat{f}]\right)\left( \overline{Y}_c(0)-\overline{Y}_n\right)\\
&\approx - \frac{\left( \overline{Y}_c(0)-\overline{Y}_n\right)}{1-p} \frac{\text{var}(\hat{f})}{\pi_c}\left[1 - \frac{1}{\pi_c^2}\frac{(1-2\pi_c)(1-2p)}{N-2}+ \frac{1}{\pi_c^3}c_1\right]\\
\end{align*}

\subsection{Two-sided noncompliance}\label{append:two_sided_bias}

Using a Taylor expansion:
\begin{align*}
E\left[\frac{\widehat{\ITT}}{\hat{f}}\right] & \approx \frac{\ITT}{\pi_c} + \frac{\text{var}(\hat{f})\ITT}{\pi_c^3} - \frac{\text{Cov}(\widehat{\ITT}, \hat{f})}{\pi_c^2}
\end{align*}

For $\text{var}(\hat{f})$ we have

\begin{align*}
\text{var}(\hat{f}) &= \frac{1}{pN}\frac{1}{N-1}\sum_{i=1}^N(D_i(1) - \overline{D}(1))^2 + \frac{1}{(1-p)N}\frac{1}{N-1}\sum_{i=1}^N(D_i(0) - \overline{D}(0))^2\\
&  \quad - \frac{1}{N}\frac{1}{N-1}\sum_{i=1}^N(D_i(1) - D_i(0) - \overline{D}(1) +\overline{D}(0))^2 \\
&= \frac{1}{N-1}\left(\frac{1}{p} \pi_n(\pi_c+\pi_a)+ \frac{1}{1-p}\pi_a(\pi_n + \pi_c) - \pi_c(\pi_a +\pi_n)\right) \\
&= \frac{1}{N-1}\left(\frac{\pi_n(1-\pi_n)}{p} + \frac{\pi_a(1-\pi_a)}{1-p} - \pi_c(1-\pi_c)\right),
\end{align*}

Now for $\text{Cov}(\widehat{\ITT}, \hat{f})$:
\begin{align*}
\text{Cov}(\widehat{\ITT}, \hat{f}) &= \underbrace{\frac{1}{Np}\frac{1}{N-1}\sum_{i=1}^N(Y_i(1) - \overline{Y}(1))(D_i(1)-\overline{D}(1))}_{\text{A}}\\
&\quad  + \underbrace{\frac{1}{N(1-p)}\frac{1}{N-1}\sum_{i=1}^N(Y_i(0)-\overline{Y}(0))(D_i(0)-\overline{D}(0))}_{\text{B}}\\
&\quad  - \underbrace{ \frac{1}{N}\frac{1}{N-1}\sum_{i=1}^N(Y_i(1) - Y_i(0)-\ITT)(D_i(1)  - D_i(0) - \pi_c)}_{\text{C}}
\end{align*}

First we have for A,
\begin{align*}
\text{A} &= \frac{1}{Np}\frac{1}{N-1}\Big[\pi_n\sum_{i: D_i(1) = 1, D_i(0)=0}(Y_i(1) - \overline{Y}(1)) + \pi_n\sum_{i: D_i(1) = 1, D_i(0)=1}(Y_i(1) - \overline{Y}(1))\\
&\qquad  \qquad \qquad   - (\pi_c + \pi_a)\sum_{i: D_i(1) = 0, D_i(0)=0}(Y_i(1) - \overline{Y}(1))\Big]\\
&= \frac{1}{Np}\frac{1}{N-1}\Big[\pi_nn_c(\overline{Y}_c(1) - \overline{Y}(1)) + \pi_nn_a(\overline{Y}_a(1) - \overline{Y}(1))  - (\pi_c + \pi_a)n_n(\overline{Y}_n(0) - \overline{Y}(1))\Big]\\
&= \frac{\pi_n}{p(N-1)}\Big[\pi_c(\overline{Y}_c(1) - \overline{Y}(1)) + \pi_a(\overline{Y}_a(1) - \overline{Y}(1))  - (\pi_c + \pi_a)(\overline{Y}_n(0) - \overline{Y}(1))\Big]\\
&= \frac{\pi_n}{p(N-1)}\Big[\pi_c(\overline{Y}_c(1) - \overline{Y}_n(0)) + \pi_a(\overline{Y}_a(1) - \overline{Y}_n(0))\Big].
\end{align*}

We can get a similar simplification for B:
\begin{align*}
\text{B} &= \frac{\pi_a}{(1-p)(N-1)}\left[\pi_c(\overline{Y}_a(1) - \overline{Y}_c(0)) + \pi_n(\overline{Y}_a(1) - \overline{Y}_n(0))\right].
\end{align*}

Now for C:
\begin{align*}
\text{C} &= \frac{1}{N}\frac{1}{N-1}\Big[(\pi_a+\pi_n)\sum_{i: D_i(1) = 1, D_i(0)=0}(\CACE_i - \ITT) - \pi_c\sum_{i: D_i(1) = 1, D_i(0)=1}(0-\ITT)\\
&\qquad  \qquad \qquad - \pi_c\sum_{i: D_i(1) = 0, D_i(0)=0}(0-\ITT)\Big]\\
 &= \frac{1}{N}\frac{1}{N-1}\Big[(\pi_a+\pi_n)n_c(\CACE - \ITT) + \pi_cn_a\ITT + \pi_cn_n\ITT\Big]\\
  &= \frac{\pi_c}{N-1}\Big[(\pi_a+\pi_n)(\CACE - \ITT) + \pi_a\ITT + \pi_n\ITT\Big]\\
    &= \frac{\pi_c(\pi_a + \pi_n)}{N-1}\CACE \\
    &= \frac{\pi_c(1-\pi_c)}{N-1}\CACE
\end{align*}

Putting it together, we have
\begin{align*}
\text{Cov}(\widehat{\ITT}, \hat{f}) &= \frac{\pi_n}{p(N-1)}\Big[\pi_c(\overline{Y}_c(1) - \overline{Y}_n(0)) + \pi_a(\overline{Y}_a(1) - \overline{Y}_n(0))\Big]\\
&\quad + \frac{\pi_a}{(1-p)(N-1)}\left[\pi_c(\overline{Y}_a(1) - \overline{Y}_c(0)) + \pi_n(\overline{Y}_a(1) - \overline{Y}_n(0))\right]\\
&\quad  - \frac{\pi_c(1-\pi_c)}{N-1}\CACE\\
&= \frac{\pi_n\pi_c}{p(N-1)}(\overline{Y}_c(1) - \overline{Y}_n(0)) + \frac{\pi_n\pi_a}{p(1-p)(N-1)}(\overline{Y}_a(1) - \overline{Y}_n(0))\\
&\quad + \frac{\pi_a\pi_c}{(1-p)(N-1)}(\overline{Y}_a(1) - \overline{Y}_c(0)) - \frac{\pi_c(1-\pi_c)}{N-1}\CACE
\end{align*}

We plug our expressions into our original expansion and rearrange as so:
\begin{align*}
E\left[\frac{\widehat{\ITT}}{\hat{f}}\right] - \CACE & \approx  \frac{\text{var}(\hat{f})\ITT}{\pi_c^3} - \frac{\text{Cov}(\widehat{\ITT}, \hat{f})}{\pi_c^2}\\
& = \frac{\CACE}{\pi_c^2}\left[\frac{1}{N-1}\left(\frac{\pi_n(1-\pi_n)}{p} + \frac{\pi_a(1-\pi_a)}{1-p} - \pi_c(1-\pi_c)\right)\right] \\
&\quad - \frac{1}{\pi_c^2}\Big[\frac{\pi_n\pi_c}{p(N-1)}(\overline{Y}_c(1) - \overline{Y}_n(0)) + \frac{\pi_n\pi_a}{p(1-p)(N-1)}(\overline{Y}_a(1) - \overline{Y}_n(0))\\
&\qquad \qquad + \frac{\pi_a\pi_c}{(1-p)(N-1)}(\overline{Y}_a(1) - \overline{Y}_c(0)) - \frac{\pi_c(1-\pi_c)}{N-1}\CACE\Big]\\
& = \frac{\CACE}{\pi_c^2}\left[\frac{1}{N-1}\left(\frac{\pi_n(1-\pi_n)}{p} + \frac{\pi_a(1-\pi_a)}{1-p} \right)\right] \\
&\quad - \frac{1}{\pi_c^2}\Big[\frac{\pi_n\pi_c}{p(N-1)}(\overline{Y}_c(1) - \overline{Y}_n(0)) + \frac{\pi_n\pi_a}{p(1-p)(N-1)}(\overline{Y}_a(1) - \overline{Y}_n(0))\\
&\qquad \qquad + \frac{\pi_a\pi_c}{(1-p)(N-1)}(\overline{Y}_a(1) - \overline{Y}_c(0))\Big]\\
& = \frac{1}{\pi_c^2(N-1)}\Bigg[\left(\frac{\pi_n(\pi_a + \pi_c)}{p} + \frac{\pi_a(\pi_n + \pi_c)}{1-p} \right)(\overline{Y}_c(1) - \overline{Y}_c(0)) \\
&\quad - \Big[\frac{\pi_n\pi_c}{p}(\overline{Y}_c(1) - \overline{Y}_n(0)) + \frac{\pi_n\pi_a}{p(1-p)}(\overline{Y}_a(1) - \overline{Y}_n(0)) + \frac{\pi_a\pi_c}{1-p}(\overline{Y}_a(1) - \overline{Y}_c(0))\Big]\Bigg]\\
& = \frac{1}{\pi_c^2(N-1)}\Bigg[ \frac{\pi_n\pi_c}{p}(\overline{Y}_c(1) - \overline{Y}_c(0) - \overline{Y}_c(1) + \overline{Y}_n(0)) \\
& \qquad \qquad \qquad \qquad+ \frac{\pi_a\pi_c}{1-p}(\overline{Y}_c(1) - \overline{Y}_c(0) -\overline{Y}_a(1) +\overline{Y}_c(0))\\
&\qquad \qquad \qquad \qquad + \left(\frac{\pi_n\pi_a}{p} + \frac{\pi_a\pi_n}{1-p} \right)(\overline{Y}_c(1) - \overline{Y}_c(0))- \frac{\pi_n\pi_a}{p(1-p)}(\overline{Y}_a(1) - \overline{Y}_n(0)) \Bigg]\\
& = \frac{1}{\pi_c^2(N-1)}\Bigg[ \frac{\pi_n\pi_c}{p}(\overline{Y}_n(0) - \overline{Y}_c(0)) + \frac{\pi_a\pi_c}{1-p}(\overline{Y}_c(1) -\overline{Y}_a(1) )\\
& \qquad \qquad+ \frac{\pi_n\pi_a}{p(1-p)}(\overline{Y}_c(1) - \overline{Y}_c(0)  - \overline{Y}_a(1) +  \overline{Y}_n(0)) \Bigg]\\
& = \frac{1}{\pi_c^2(N-1)}\Bigg[ \frac{\pi_n((1-p)\pi_c + \pi_a)}{p(1-p)}(\overline{Y}_n(0) - \overline{Y}_c(0)) + \frac{\pi_a(p\pi_c + \pi_n)}{p(1-p)}(\overline{Y}_c(1) -\overline{Y}_a(1) )\Bigg]
\end{align*}







\section{Further details on and results for simulations}
\label{app:simulation}

\subsection{Simulation Design}
We generate data by first generating a four-category categorical covariate, $X_i$, to divide the units into strata.
Each stratum is then given a baseline control-side mean.
If our covariate is predictive of outcome, these means will vary; otherwise they are shared.

We next generate compliance behavior for each unit, flipping an independent coin with probability $p_i$, where $p_i$ depends on $X_i$ in the case of a covariate predictive of compliance, and $p_i$ is constant across units if not.
We keep overall compliance rate the same in either case.

After we have our $X_i$ and $S_i(0), S_i(1)$ (a pair of indicator variables indicating treatment take-up depending on treatment assignment), we generate our potential outcomes.
We assign a constant shift in the mean for never-takers.
For simplicity, we do not shift the always-takers, but our code is available for use and the option is there.
The potential outcomes are then generated as normal around these means, with a standard deviation set to achieve an overall (cross-strata) variance of 1.
This implies that if we do not have a prognostic $X_i$, the within-strata variance will be higher as total variance is within plus between variation.
The treatment effect is then added to the compliers' $Y_i(0)$ to get the $Y_i(1)$.

Once the dataset is generated, we randomize to treatment and control, and calculate $Y_i^{obs}$, the observed outcome.
We can then estimate the overall CACE with our different methods.

To illustrate trends, we selected parameters in our simulation such that when $X_i$ is predictive of something, it is very predictive.
For example, when $X_i$ predicts compliance, the top tier often has above 50\% compliance and the bottom tier usually has below 1\% compliance.
The average $R^2$ of the outcome regressed onto $X_i$ for the control group, for predictive $X_i$, is about 63\%.

For the auxiliary simulation to study covariates predictive of compliance, we set a tuning parameter $r$ from $[0, 1]$, and set the compliance of the four strata to $p, pr, pr^2$, and $pr^3$.
We then set $p$, a scaling factor, such that overall compliance equaled our target compliance rate $P$, using:
$$ P = w_1 pr^3 + w_2 pr^2 + w_3 pr + w_4 p , $$
where $w_k$ is the proportion of units in stratum $k$.
When $r=0$, all our compliers are in the last stratum.
When $r=1$, all strata have the same compliance rate.


\subsection{Further results on SE estimator performance}

In the main paper, we look at the average SE estimate, after they had been windsorized. The overall averages are still driven by the extreme outliers (10 standard deviations is still very large compared to typical values).
As a point of comparison, we have, analogous to Figure 4 in the main paper, the ratio of the \emph{median} estimated standard error to the true standard error on Figure~\ref{fig:se_estimator_plot_median}.
Here the true standard error is over the windorized estimates, but the point estimates are less frequently extreme than the standard error estimates.
Overall, we see that given the right skew in the SE estimates, the median estimated standard error tends to be too small, relative to the truth (even considering that the true SEs are too small given Windorization).




\begin{figure}[hbt]
\center
  \includegraphics{figures/se_estimator_plot_med_pool_dual.pdf}
  \caption{Relative percent change of \emph{median} estimated standard error to true standard error, for both the delta method and Bloom standard errors. Each point is a specific simulation scenario. Points above 100\% indicate standard errors systematically too large, and below systematically too small.}
  \label{fig:se_estimator_plot_median}
\end{figure}


\subsection{Stability of SE Estimators}
\label{sec:se_stability}

Here we focus on one-sided noncompliance only, as two-sided noncompliance was clearly very unstable with large numbers of extreme point estimates and standard error estimates.
For one-sided noncompliance, we wanted to investigate whether the uncertainty of the stratified standard error estimators is generally lower, relative to the corresponding true standard error, as compared to the unstratified estimator.
For each estimator and simulation context we calculate the standard deviation of the estimated standard errors and divide by the Monte-Carlo estimated true standard error to obtain a relative average percent error in the uncertainty measure.
We then compare these ratios, for the post-stratified estimators, to the corresponding ratio of the baseline unstratified estimator.
This is a ratio of ratios:
\begin{equation*}
\mbox{relative SE}_{est} = \frac{sd( \widehat{SE}(\hat{\tau}_{est} )) }{ SE(\hat{\tau}_{est} ) } \slash \frac{ sd( \widehat{SE}(\hat{\tau}_{u} ) )}{ SE(\hat{\tau}_{u} ) },
\end{equation*}
where $est$ is an estimator of interest and $u$ is our baseline unstratified estimator.

The Bloom estimated standard errors for either stratified estimator are neither more or less unstable to any substantial degree, relative to their true precision, as compared to the unstratified.
For the delta method standard error, we do see higher relative instability in the standard error estimates for the within approach.
A subsequent analysis (not shown) shows that for a predictive covariate, even though the standard errors can be \emph{relatively} more uncertain, the overall reduction in uncertainty more than offsets this gain, resulting in a more precise estimate of uncertainty for a more precise estimator.
Without a predictive covariate, however, true precision gains are minimal, and the additional instability in estimating the standard errors does result in an overall cost.

The 2SLS standard error estimators are compared first to Bloom, then to delta standard error estimators for the unstratified IV estimate.
The splitting pattern shows that the relative stability of 2SLS standard error estimates are not as unstable as the delta method, but are less stable than Bloom.


\begin{figure}[hbt]
\center
  \includegraphics{figures/se_instability_plot_pool_onesided.pdf}
  \caption{Ratio of relative instabilities of $\widehat{SE}$ (centered at 100\%), with instability taken as the ratio of the standard deviation of $\widehat{SE}$ vs. the true $SE$ (see text for equation), and the primary ratio being the post-stratified instability vs. unstratified instability. Numbers below 0 indicate the post-stratified standard error estimates are relatively more stable than unstratified standard error estimates, as indexed by their relative uncertainties.}
  \label{fig:se_instability_plot}
\end{figure}





\subsection{Random post-stratification}
\label{app:random_poststrat}

We were surprised to see benefits to post-stratification when stratifying on a variable that is neither predictive of compliance status or outcome for one-sided noncompliance.
To further verify this finding, we conducted an additional simulation study where we first generated a dataset as we did for our primary simulation, and then generated a categorical covariate entirely at random to go with it.
We explored generating such a covariate with 1, 3, 6, 9, and 12 categories.
Results are on Figure~\ref{fig:random_strat_nohet}.
We also varied the extent to which the never-takers are systematically different from compliers.

We see benefits to stratification for $IV_{w}$, although bias does climb the more the never-takers are different from compliers.
What is happening is if we end up with a strata that has no compliers in treatment, that entire strata is dropped.
This reduces overall noise as we know that group does not provide any hope of a treatment by control comparison, as there is no information about compliers on the treatment side.
We do end up with bias since this will systematically drop strata with no treatment compliers, but not drop strata with no control compliers, creating systematic imbalance.
Even so, we see the gains from reduced instability offsets this bias in these scenarios.
This is perhaps more a statement about the instability of the overall IV estimator (note the SEs, in effect size units, are larger than 1 effect size unit), than small bias.

\begin{figure}[hbt]
\center
  \includegraphics{figures/random_strat_nohet.pdf}
  \caption{Bias, SE, and RMSE when randomly stratifying units with varying number of strata ($x$-axis) and varying amount of separation between the never-takers and compliers (rows of results) for one-sided noncompliance. SE gains swamp bias cost in these scenarios, even when there is a great deal of separation (2 standard deviations) between the means of the never-takers and compliers.}
  \label{fig:random_strat_nohet}
\end{figure}



\subsection{Violations of the exclusion restriction}\label{supsubsec:er}

We modified the simulation scenario where we varied the concentration of the compliers in the last strata by simply adding an overall 0.20 effect size impact of treatment to all noncompliers, and a 0.50 effect size impact for the compliers.
Results are on Figure~\ref{fig:exclusion_restriction}.

First, we see a large bias, larger than the 0.20 impact.
This is because 85\% of the units are noncompliers, and all the ITT due to them gets attributed to the 15\% compliers.
Even mild violations of the exclusion restriction can be dangerous.

We also see that as we have a covariate increasingly predictive of complier status, we can carve out a subgroup that has a higher proportion of compliers, which mitigates the bias.

\begin{figure}[hbt]
\center
  \includegraphics{figures/exclusion_Bias_SE_RMSE.pdf}
  \caption{Bias, SE, and RMSE when exclusion restriction is violated.  As compliers get increasingly concentrated in fewer strata, the estimators that drop or down-weight low-complier weight strata have substantial bias reductions}
  \label{fig:exclusion_restriction}
\end{figure}

