
\documentclass[acmtog,anonymous,review,british,screen=true]{acmart}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage[british]{babel}

% ACM includes graphicx and hyperref by default
%\usepackage{graphicx}
%\usepackage{hyperref}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.



\usepackage{enumitem} % Better control of lists and their spacing

\setlist{nolistsep} % Remove horizontal spacing in lists
\setlength\tabcolsep{3pt} % Reduce horizontal padding in tables

% Checkmark and X from https://tex.stackexchange.com/a/42620
\usepackage{caption}
\usepackage{subcaption}
% \usepackage[demo]{graphicx}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{rotating}

\usepackage[normalem]{ulem} % To allow strikeout text

\newcommand{\tqdb}{\textquotedbl} % For straight quotation marks in tables

% Define a new bold command that keeps the same kerning/letter spacing as normal, non-bold text
\newcommand{\tablebf}[1]{%
\pdfliteral direct {2 Tr 0.3 w}% The second factor is the boldness
#1%
\pdfliteral direct {0 Tr 0 w}%
}

\newcommand{\rev}[1]{\textcolor{blue}{#1}}

%%
%% end of the preamble, start of the body of the document source.
\citestyle{acmauthoryear}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022]{Evaluating gesture-generation in a large-scale open challenge:\\The GENEA Challenge 2022}
% \\ Benchmarking the State of the Art

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Taras Kucherenko}
\email{tkucherenko@ea.com}
%\authornote{Equal contribution and joint first authors.}
\affiliation{
  \institution{SEED -- Electronic Arts (EA)}
  \city{Stockholm}
  \country{Sweden}}

\author{Pieter Wolfert}
%\authornotemark[1]
\email{pieter.wolfert@ugent.be}
\affiliation{%
 \institution{IDLab, Ghent University – imec}
 \city{Ghent}
 \country{Belgium}}
 
\author{Youngwoo Yoon}
%\authornotemark[1]
\email{youngwoo@etri.re.kr}
\affiliation{%
  \institution{ETRI}
  \city{Daejeon}
  \country{Republic of Korea}
}

 
\author{Carla Viegas}
\email{cviegas@andrew.cmu.edu}
\affiliation{%
 \institution{Carnegie Mellon University}
 \city{Pittsburgh}
 \country{USA}}
\affiliation{%
 \institution{%
 NOVA University Lisbon}
 \city{Lisbon}
 \country{Portugal}}
 
\author{Teodor Nikolov}
\email{tnikolov@hotmail.com}
\affiliation{%
 \institution{Umeå University}
 \city{Umeå}
 \country{Sweden}
 }

\author{Mihail Tsakov}
\email{tsakovm@gmail.com}
\affiliation{%
 \institution{Umeå University}
 \city{Umeå}
 \country{Sweden}
 }

 

\author{Gustav Eje Henter}
\email{ghe@kth.se}
\affiliation{%
  \institution{KTH Royal Institute of Technology}
  \city{Stockholm}
  \country{Sweden}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Kucherenko, Wolfert, Yoon et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
% IUI abstract draft:
%Co-speech gestures, gestures that accompany speech, play an important role in human communication. Automatic co-speech gesture generation is thus a key enabling technology for embodied conversational agents (ECAs). Research into gesture generation is rapidly gravitating towards data-driven methods. Unfortunately, individual research efforts in the field are difficult to compare: there are no established benchmarks, and each study tends to use its own dataset, motion visualisation, and evaluation methodology. To address this situation, we launched the GENEA Challenges, a series of gesture-generation challenges wherein participating teams built automatic gesture-generation systems on a common dataset, and the resulting systems were evaluated in parallel in a large, crowdsourced user study using the same motion-rendering pipeline. Since differences in evaluation outcomes between systems are solely attributable to differences between the motion-generation methods, this enables benchmarking recent approaches against one another in order to get a better impression of the state of the art in the field. This paper reports on the purpose, design, results, and implications of the second GENEA challenge. 
%Co-speech gesture generation is a key enabling technology for embodied conversational agents.
This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. The dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in a dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier, we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which has been a difficult problem in the field.

The evaluation results are a revolution, and a revelation. Some synthetic conditions are rated as significantly more human-like than human motion capture. To the best of our knowledge, this has never been shown before on a high-fidelity avatar. On the other hand, all synthetic motion is found to be vastly less appropriate for the speech than the original motion-capture recordings. We also find that conventional objective metrics do not correlate well with subjective human-likeness ratings in this large evaluation. The one exception is the Fréchet gesture distance (FGD), which achieves a Kendall's tau rank correlation of around $-0.5$. Based on the challenge results we formulate numerous recommendations for system building and evaluation.
%The main highlight of the results is synthetic motion that is rated as significantly more human-like than the motion from the motion-capture recordings. To the best of our knowledge this has never been shown before on a high-fidelity avatar. We have also managed to decouple human-likeness from gesture appropriateness in our evaluations, which previously was a major challenge in the field.
%
% Alternative intro and ending for the GENEA Workshop paper abstract:
%Automatic gesture generation is a field of growing interest, and a key technology for enabling embodied conversational agents.
%(...)
%This paper reports on the purpose, design, and of our challenge, with each individual team's entry described in a separate paper also presented at the GENEA Workshop.
%
\end{abstract}



\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121</concept_id>
<concept_desc>Human-centered computing~Human computer interaction (HCI)</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Human computer interaction (HCI)}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.I don't think arXiv offers any way to collect articles into proceedings.
\keywords{animation, gesture generation, embodied conversational agents, evaluation paradigms}

\settopmatter{printfolios=true} % Turn this on for page numbering on arXiv

\begin{teaserfigure}
  \centering
  \includegraphics[width=\textwidth]{figures/teaser.png}
  \caption{Overview of the GENEA Challenge. We controlled the dataset, visualisation, and evaluation in order to compare different gesture-generation approaches in a fair and systematic way. The dataset includes speech audio, time-aligned speech text transcription, and speaker identity as input modalities and 3D body motion as the output modality. For the synthesised motion from the participating teams, video stimuli were rendered by a shared visualisation pipeline and evaluated jointly in crowdsourced user studies.}
  \label{fig:teaser}
\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% What writing conventions do we use? UK spelling with serial comma? Hyphenation patterns.
This paper is concerned with systems for automatic generation of nonverbal behaviour, and how these can be compared in a fair and systematic way in order to advance the state-of-the-art.
This is of importance as nonverbal behaviour plays a key role in conveying a message in human communication \cite{mcneill1992hand}.
A large part of nonverbal behaviour consists of so called co-speech gestures, spontaneous hand and body gestures that relate closely to the content of the speech \cite{bergmann2011relation}, and that have been shown to improve understanding \cite{holler2018processing}.
Embodied conversational agents (ECAs) benefit from gesticulation, as it improves interaction with social robots \cite{salem2011friendly} and willingness to cooperate with an ECA \cite{salem2013err}. 
Knowledge of how and when to gesture is also needed. This can for example be learnt from interaction data; see, e.g., \citet{jonell2020let}.% and references therein.

Synthetic gestures used to be based on rule-based systems, e.g., \citet{cassell2001beat,salvi2009synface}; see \citet{wagner2014gesture} for a review. These are gradually being supplanted by data-driven approaches, e.g., \citet{bergmann2009GNetIc,levine2010gesture,chiu2015predicting}, with recent work \cite{yoon2019robots,kucherenko2020gesticulator,alexanderson2020style,yoon2020speech} showing improvements in gesticulation production for ECAs.
For more in-depth reviews of recent data-driven approaches see \citet{liu2021speech,nyatsanga2023comprehensive}.

Unfortunately, results from different gesture-generation studies are typically not directly comparable \cite{wolfert2021review}.
Studies usually rely on different data sources to train their models. 
The visualisations of their generated gestures often have different avatars and production values, which can affect the perception of the gestures.
On top of this, studies make use of a variety of different methodologies to evaluate the gestures. 
All these differences are, however, external to the actual methods that drive the gesture generation. 
%By providing a common dataset for building gesture-generation systems, and common evaluation standards with a common visualisation procedure, one can control for these sources of variation, and enable direct comparison between different methods for co-speech gesture generation.

In this paper, we report on the GENEA\footnote{GENEA stands for ``Generation and Evaluation of Non-verbal Behaviour for Embodied Agents''.} Challenge 2022. %,the second joint gesture-generation challenge.
The aim of the challenge is not to select the best team -- it is not a contest, nor a competition -- but to be able to directly compare different approaches and outcomes.
By providing a common dataset for building gesture-generation systems, along with common evaluation standards and a shared visualisation procedure, we control for all other sources of variation except the system-building itself.
Our setup is unique to the field of gesture generation, making it possible to reliably assess and advance the state of the art, and to measure the gap between it and natural co-speech gestures.
Comparing different methods and their performance also helps identify what matters most in gesture generation, and where the bottlenecks are.
In particular, the results make it abundantly clear that natural-looking data-driven gesture motion is achievable today, but that synthetic gestures are much less appropriate for the accompanying speech than the ground-truth motion is.
The results also show that most objective metrics are not informative about the perceived human-likeness of the generated gestures.

Challenge participants benefit by working on the same problem together with researchers interested in the same topic, strengthening the research community, and get an opportunity to compare their systems to other competitive systems in a large and carefully-executed joint evaluation.
They also work on and contribute towards a standardised evaluation setup which encourages future benchmarking and reproduction of results.
Participants are required to write down their methods, results and experience in a system paper to be presented in conjunction with the challenge itself, giving them a chance to publish their work at a leading conference in the field. 
%todo: what did the blizzard challenge achieve? specific info on blizzard challenge is not relevant perse (except the meta stuff)
%
Our concrete contributions are:
\begin{enumerate}
    \item Four large-scale user studies that jointly evaluate a large number of gesture-generation models on a common dataset using a common 3D model and rendering method.
    %\item Demonstrating a new method for subjective assessment of gesture appropriateness for speech, that successfully controls for the human-likeness of the motion.
    \item A subjective evaluation that successfully disentangles motion human-likeness from its appropriateness for the associated speech.
    \item To the best of our knowledge, the first results that identify synthetic gesture motion that surpasses the human-likeness of good motion-capture data on a high-fidelity avatar.
    \item The first clear evidence that synthetic gestures are much less appropriate for the specific speech than natural motion is, even when controlling for the human-likeness of the motion.
    \item A validation study of many objective metrics for predicting motion human-likeness, finding that all metrics except the Fr{\'e}chet gesture distance (FGD) are statistically indistinguishable from chance prediction.
    \item Providing open code and high-quality data in the spirit of open source and reproducible research.
    %to facilitate reproducibility and
    %enabling future research to compare and benchmark against systems from the challenge.
    This includes pre-processed multimodal training, validation, and test datasets; the standardised visualisation; submitted motion and video stimuli; a large number of subjective responses from the studies; and evaluation and analysis code.
    \item Bringing researchers together in order to advance the state-of-the-art in gesture generation, and enabling future research to compare and benchmark against systems and stimuli from the challenge.
\end{enumerate}
%In the long term, we anticipate further benefits in that systems, motion stimuli, and evaluation methods from the challenge can be used as benchmarks in future studies, and in starting to build a database of thoroughly assessed motion stimuli for further research into objective and subjective evaluation.
Materials derived from the challenge are publicly available, with links to these resources to be added upon paper acceptance.

This paper is an extension of a previously published conference paper on the challenge \cite{anon2022genea}, adding more comprehensive information and analysis, experiments on objective metrics, and a more in-depth discussion of challenge findings, recommendations, and limitations.
The remainder of this paper first (in Sec.\ \ref{sec:background}) briefly discusses current gesture-evaluation practices and how challenges can help overcome their shortcomings.
We then describe the challenge task and dataset in Sec.\ \ref{sec:task_n_data}, followed by a breakdown of the challenge tiers and participating teams in Sec.\ \ref{sec:setup}.
In Sec.\ \ref{sec:evaluation} we describe the design of the challenge evaluation, with results of the various evaluations recounted in Sec.\ \ref{sec:results} and discussed in Sec.\ \ref{sec:discussion}.
Each of these three sections detail both the core subjective evaluation as well as the objective metrics we computed, in that order.
We round off by discussing limitations of the challenge (in Sec.\ \ref{sec:limitations}) and summarising its conclusions and implications (in Sec.\ \ref{sec:conclusion}).
%including both objective and subjective evaluation, and the prepared stimuli.
%Sec.\ 6 presents objective and subjective results of the challenge evaluations, including a low-level discussion comparing our multiple conducted user studies.
%Sections 7 and 8 provide a high-level discussion of the challenge, including limitations, implications of our findings, and conclusion.

\section{Related work}
\label{sec:background}
\setcounter{subsection}{1}
\subsubsection{Issues with prior evaluations and evaluation practices}
Most works that propose new gesture-generation methods incorporate an evaluation to support the merits of their method.
Human gesture perception is highly subjective, and there are currently no widely accepted objective measures of gesture perception.
Instead, human assessment via careful user studies is the gold standard in the field.
%Due to the highly subjective aspect of human gestures, many publications have conducted human assessments instead.
However, previous subjective evaluations have several drawbacks, as reviewed in \citet{wolfert2021review}.
Some major issues are the coverage of systems being compared and the scale of the studies.
Like in \citet{sadoughi2019speech,kucherenko2020gesticulator,kucherenko2021moving,alexanderson2020style}, proposed models are at most compared to one or two prior approaches (often a highly similar baseline) or possibly only to ablated versions of the same model.
A large number of studies do not compare their outcomes with other methods at all, let alone other systems trained on the same data.
This creates an insular landscape where particular model families are only applied to particular datasets, and never contrasted against one another.

As for scale, large evaluations are expensive, and studies may not be able to recruit enough participants, thus leaving the differences between many pairs of studied systems unresolved and not statistically significant (cf.\ \citet{yoon2019robots,yoon2020speech}).
%In terms of the study scale, \citet{yoon2019robots} failed to show statistical significance for the majority of pairs of compared systems, due to the low number of evaluation participants.
Questionnaires, which are one popular evaluation methodology (cf.\ \citet{salem2012generation,ishi2018speech,bergmann2010individualized, yoon2019robots, ishii2018generating, shimazu2018generation}) demand a lot of time and cognitive effort from test participants, and may be infeasible to scale up to bigger studies.
In addition, items used in questionnaires differ across studies and the set of questions used is often not standardised.
Evaluations sometimes also fail to anchor system performance against natural (``ground truth'') motion from test data held out from training (e.g., \citet{salem2012generation, ishii2018generating, le2012evaluating}).

% comment : database (e.g., \cite{salem2012generation,ishii2018generating,le2012evaluating})
Studies also differ in the dataset they train on (e.g., \citet{salem2012generation,ishii2018generating,le2012evaluating}) and in how the motion is visualised.
For the latter, some prior work displays motion through stick figures (e.g., \citet{wolfert2019should,kucherenko2019analyzing}), or applies it to a physical agent (e.g., \citet{salem2012generation,ishi2018speech}).
Neither of these may allow the same expressiveness or range of motion as a high-quality 3D-rendered avatar as seen in, e.g., \citet{alexanderson2020style,kucherenko2020gesticulator}).


\subsubsection{Benefits of challenges in other fields}
Other fields have done well using challenges to standardise evaluation techniques, establish benchmarks, and track and evolve the state of the art. 
For example, the Blizzard Challenges have since their inception in 2005 (see \citet{black2005blizzard}) helped advance our sister field of text-to-speech (TTS) technology and identified important trends in the specific strengths and weaknesses of different speech-synthesis paradigms \cite{king2014measuring}. 
These challenges are defined by the use of common data and evaluation, and their open participation to both academia and industry.
More specifically, participants are provided a common dataset of speech audio and associated text transcriptions, which they use to build a system that generates synthetic speech audio. 
After the participants submit their systems, the resulting generated speech is subsequently evaluated in a large, joint evaluation, the results of which are provided to the teams.
Submitted entries are identified by anonymised labels in official Blizzard Challenge results, but in practice the vast majority of teams identify which label represents their entry in their paper at the Blizzard Challenge Workshop describing the system that they submitted.
%, yet to the public it is not announced which system is from which team.
Data, evaluation stimuli, and subjective ratings remain available after these challenges, and have been widely used both for benchmarking subsequent TTS systems, e.g., \citet{szekely2012evaluating,charfuelan2013expressive}, and in research on the perception of natural and artificial speech, e.g., \citet{moller2010comparison,yoshimura2016hierarchical,mittag2020deep,saratxaga2016synthetic,govender2019using}.
This has led to the development of new and novel methods, driven by past results, and since participants had access to the same data, significant advances have been made.

% Seems useful for a journal
Challenges are also actively used in the computer-vision community, for instance for benchmarking purposes. Recent CLIC\ \cite{clic2020} and NTIRE\ \cite{ntire2020} challenges, for example, compared systems for image compression and super-resolution respectively, also incorporating subjective human assessments similar to the challenge described in this paper (although they used a MOS-like setup, which has been found to be less efficient than the side-by-side evaluation methodology we employ \cite{ribeiro2015perceptual}). This addresses the over-reliance on objective metrics in computer-vision evaluation, which, just like in speech quality and gesture generation, do not always align with human perception.
The GENEA Challenge is inspired by these successes of challenges in other fields of study.
%conducted the first challenge in the field of gesture generation.

In 2020 we organised the first gesture-generation challenge, the GENEA Challenge 2020 \cite{anon2021genea}.
%In addition to being an exercise in benchmarking both new \cite{jinhong_lu_2020_4088376,vladislav_korzun_2020_4088609,thangthai2021speech} and previously-published \cite{alexanderson2020style, kucherenko2019analyzing, yoon2019robots} gesture-generation methods, the results of that challenge have since helped improve gesture-generation benchmarking in other ways as well. Researchers have, for example, used the 2020 visualisation \cite{wang2021integrated,teshima2022deep,zhang2023diffmotion}, and the objective \cite{bhattacharya2021speech2affectivegestures} and subjective \cite{yoon2021sgtoolkit} evaluation methodologies, as a basis for future research. The data has also been used to benchmark subsequent gesture-generation models \cite{ferstl2021expressgesture, yazdian2022gesture2vec}, and even for automatic quality assessment \cite{he2022automatic}.
In this paper, we follow up on the 2020 challenge by reporting on the second gesture-generation challenge, the GENEA Challenge 2022.
This challenge expands the dataset, the range of behaviour considered, and the number of participating teams, and also improves the visualisation and the evaluation practises, in addition to considering objective metrics together with a large subjective evaluation.

% objective metrics
\subsubsection{Objective metrics}
Given that subjective evaluations are labour intensive, time-consuming, and costly, a large number of different objective metrics have been proposed as automated indicators of gesture-generation performance.
Some of these, such as the commonly used \cite{wolfert2021review,nyatsanga2023comprehensive} average position error (APE) and mean-squared position error (MSE), as well as the ``probability of correct keypoints'' (PCK) and its extension to semantic relevance gesture recall (SRGR) \cite{liu2022beat}, are used to score the similarity of generated poses to a corresponding recording of human motion.
Alternatively, canonical correlation analysis (CCA) can be used \cite{sadoughi2019speech,bozkurt2015affect,jinhong_lu_2020_4088376} to quantify the linear (Pearson) correlations between generated and reference poses.
These methods are likely to struggle with the stochastic, one-to-many nature of human gestures (there is no single ``correct'' way to move), leading to high variance.

To accommodate the stochastic nature of motion, statistics such as the average magnitude of motion acceleration and jerk, and distances between motion speed histograms have been used to quantify how similar generated motion is to the distribution of human motion \cite{kucherenko2021moving}.
%There were a few attempts for evaluating gesture motion objectively.
More recent developments have built on the Fre{\'e}chet inception distance (FID) from image generation \cite{heusel2017gans} to propose \cite{yoon2020speech, ahuja2020no} new methods for comparing gesture-motion distributions.
These methods were later used by, e.g, \citet{ahuja2022low, liu2022learning, ao2022rhythmic, liu2022beat}.
%More recent developments in the same direction, ideas based on the Fréchet distance between distributions of human motion and generated motion have been proposed \cite{yoon2020speech, ahuja2020no} and used in later works \cite{ahuja2022low, liu2022learning, ao2022rhythmic, liu2022beat}.
Beat consistency, which was first proposed for dance motion \cite{li2021ai}, has also been used to assess gesture generation \cite{liu2022learning}.
%was also adopted to evaluate gesture motion . 
However, few of these works experimentally validate their metrics.
In this paper, we use the many conditions and ratings gathered in our user studies to compute and validate five of the above objective metrics for gesture generation.

\section{Task and data}
\label{sec:task_n_data}
The GENEA Challenge 2022 focused on data-driven automatic co-speech gesture generation.
Specifically, given a sequence $\boldsymbol{s}$ of input features that describe human speech -- which could involve any combination of an audio waveform, a time-aligned text transcription, and a speaker ID -- the task is to generate a corresponding sequence $\hat{\boldsymbol{g}}$ of 3D poses describing gesture motion that an agent might perform while uttering this speech (facial expression is not considered).
To enable direct comparison of different data-driven gesture-generation methods, all methods evaluated in the challenge were trained on the same gesture-speech dataset and their motion visualised using the same virtual avatar and rendering pipeline.
This is the same core task as in the 2020 challenge, whilst at the same time we changed the dataset (as described below) and refined the evaluation (as detailed in Sec.\ \ref{sec:evaluation}).

\subsection{Data}
Compared to 2020, we wanted to expand the dataset to include finger motion, lower-body motion, and material from multiple speakers in dyadic interactions. The latter may provide more natural and interesting gestures than the Trinity Speech-Gesture Dataset \cite{ferstl2018investigating} used in 2020.
We based our challenge on the Talking With Hands 16.2M gesture dataset \cite{lee2019talking}, which comprises 50 hours of audio (captured by close-talking directional microphones) and motion-capture recordings of several pairs of people having a conversation freely on a variety of topics, recorded in distinct takes each about 10 minutes long.
%This is one of the
At the time of the challenge, this was likely the
largest dataset of parallel speech and 3D motion (in joint-angle space) publicly available in the English language.
We removed parts of the dataset (46 out of 116 takes) that lacked audio or had low motion-capture quality, especially for the fingers.
Note that despite the dataset being dyadic by design, the challenge focused on generating one side of the conversation at a time, without awareness of the interaction partner.
The data from the interaction partner in each dyad was typically also included in the challenge material, but as a separate recording without providing links between the two.
This was the case for both the gesture synthesis and for the subsequent evaluation.
%Although recordings from both sides of the dyads was used, it was separated.

\subsubsection{Speech audio and text}
Speech data was shared with participants as WAV audio with no additional processing beyond the anonymisation applied by \citet{lee2019talking}, which replaced many proper nouns with silence. We also provided text transcriptions of the speech, in tab-separated value (TSV) files, and a metadata file with unique anonymous labels for each speaker.
The TSV files were created by first applying \href{https://cloud.google.com/speech-to-text/}{Google Cloud automatic speech recognition} to transcribe the audio recordings, followed by manual review to correct recognition errors and add punctuation for all parts of the dataset (training, validation, and test).

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figures/pose_comparison/pre-post-process-poses_notext_transparent.png}
\caption{Visualisations of the default skeletal pose of the data before and after processing.
On the left (blue) is the original skeletal pose, as found in the Talking With Hands 16.2M dataset shared by \citet{lee2019talking}.
On the right (orange) is the transformed skeletal pose (i.e., T-pose) used for the GENEA Challenge 2022.}
\label{fig:posecomparison}
\vspace{-1\baselineskip}
\Description{}
\end{figure}

\subsubsection{Motion data}
Motion data was downsampled from 120 to 30 frames per second and further transformed in two ways:
\begin{enumerate}
    \item We updated the default skeletal definition relative to which all motion data is defined, from what appeared to be a contorted and arbitrary definition, to a standard ``T-pose'' shown in Fig.\ \ref{fig:posecomparison}.
    The T-pose is an animation industry standard, whereby all joint rotation values are described in relation to a T-shaped skeleton.
    This standard is recognised across the animation industry and is widely adopted by existing 3D digital content creation software like Blender and Maya.
    This is further demonstrated by it often being the required pose for a 3D skeleton, in order to transfer the motion of one character onto another during motion re-targeting.
    Furthermore, the T-pose is expected to have better mathematical properties due to its symmetry and shape.
    In particular, the pose more closely resembles the poses found in the motion-capture data.
    As a result of this, most of the joint rotation values are expected to be closely distributed around 0.
    Consequently, this would reduce the risk of phase wrapping and gimbal locking across the skeleton, lending itself to smoother behaviour and interpolation in the Euler-angle space.
    This in turn leads to data that is more numerically stable, making it more practical for training machine-learning models.
    The data was recomputed to match a T-pose using motion re-targeting inside MotionBuilder% (documentation can be found \href{https://github.com/TeoNikolov/genea_visualizer/tree/master/scripts}{here})
    , retaining as much of the original visual quality as possible, whilst ensuring that the data had no discontinuities (e.g., at rotations near $180^{\circ}$).
    We found that this transformation substantially improved the output of the baseline system UBA in Sec.\ \ref{ssec:teams}.
    \item We standardised the position and orientation of speakers in all takes.
    Originally, each take would have the two speakers occupy two locations and face each other.
    We standardised this on a per-take basis such that all speakers, on average, face the same direction, and occupy the same location.
    More technically, in a right-hand $xyz$ Cartesian coordinate system ($y$-up, $z$-forward), each speaker is on average positioned at world origin ($[x=0,y=0, z=0]$), and on average facing the positive $z$-axis (a directional vector $[x=0,y=0,z=1]$).
    Averaging was done for each take separately after taking 250 equidistant samples of the hips position and orientation, and then using linear-algebra operations to correct for the difference between the original values and the standardised ones.
    This change was made to streamline data visualisation and to remove potential confusion due to different absolute positions and orientations across different takes.
\end{enumerate}
The transformed motion data was shared with participants in the Biovision Hierarchy (BVH) format.

\subsubsection{Data splits}
The challenge data was split into a training set (18 h), a validation set (40 min), and a test set (40 min), with only the training and validation sets initially shared with participating teams.
All these data subsets are publicly available, with links to be added to the manuscript upon paper acceptance.
The validation and test data each comprised 40 \emph{chunks} (contiguous excerpts approximately one minute long), to promote generation methods that are stable over long segments of speech, and was restricted to takes with finger-motion tracking for the chosen speaker.
Some recordings with finger-capture data were excluded from consideration due to poor motion-capture quality, based on visual inspection of a short sample from each recording.
The validation data was intended for internal benchmarking during development, so participants were allowed to train their final submitted models on both training and validation data if they wished.

\subsubsection{Usage rules}
Teams were allowed to only train on a subset of the data and were allowed to enhance the data they trained on however they liked, for instance by manual annotation or by post-processing the speech and the motion.
%and/or by only training on a subset of the data.
They were also allowed to make use of additional speech data (audio and text) from other sources, and models derived from such data, e.g., BERT \citet{devlin2018bert} and wav2vec \citet{wav2vec2020}.
However, it was not permitted to use any other motion data, nor any pre-trained motion models, other than what the organisers provided for the challenge.
Otherwise, the result would be likely to strongly depend on how much data each team can get access to (as has been the case in many Blizzard Challenges in speech synthesis), which is not an interesting scientific conclusion to replicate.

% Removed for anonymization
% For a better understanding of the vocabulary used in the database a table of word frequencies is provided at \href{https://preview.tinyurl.com/y22h6rtt }{tinyurl.com/y22h6rtt}. %, out of the 50k total words there are 4230 unique words and the first 8 words account for 30\% of all words spoken. 

%%% START OF COMMENT BLOCK - May be useful for future reference
% \subsubsection{Use of External Data}

% ``External data'' is defined as data, of any type, that is not part of the provided database. This includes, for example, raw recordings, structured databases, and pre-trained systems such as word vectors. For this year's challenge, only open external data -- data that is available to the public free of charge (possibly after signing a license) -- could be used.

% For motion data (whether 2D, 3D, or video), only external data from very specific databases could be used:
% \begin{itemize}
%     \item CMU Motion Capture Database \footnote{\href{http://mocap.cs.cmu.edu/}{mocap.cs.cmu.edu/}} 
%     \item Motion Capture Database HDM05 \footnote{\href{http://resources.mpi-inf.mpg.de/HDM05/}{resources.mpi-inf.mpg.de/HDM05/}} 
%     \item CMU Panoptic Studio dataset \footnote{\href{http://domedb.perception.cs.cmu.edu/}{domedb.perception.cs.cmu.edu/}} 
%     \item Talking With Hands 16.2M: \footnote{\href{https://github.com/facebookresearch/TalkingWithHands32M}{github.com/facebookresearch/TalkingWithHands32M}}. Important: Motion data only – no speech/text/audio!
% \end{itemize}


%\begin{itemize}
% \item Define what we mean be gesture-generation.
% \item Define the task for the challenge. Possibly motivate why it is an interesting starting point.
% \item Describe what is in scope and out of scope. Examples of in scope are motion graphs as well as continuous-gesture generation, deep and not deep learning, training data pre-processing and annotation, output post-processing. Examples of out of scope are hand-animation, manually tweaking output motion, anything that cannot run unattended. (What about hand-coded rules and hand-defined gesture libraries? Do we mention these?) You can think of more aspects to consider.
% \item What is the input and output of the system.
% \item Talk about our policy on external data somewhere, and motivate it. (See the challenge rules for a start at this.)
% \item Where do we mention the timeline for the challenge?
%\end{itemize}
%%% END OF COMMENT BLOCK

\section{Setup and participation}
\label{sec:setup}
The challenge began on May 16, 2022, when speech-motion training data was released to participating teams.
Test inputs (WAV, TSV, and speaker ID, but no motion output) were released to the teams on June 20, with teams required to submit BVH files with their generated gesture motion for these inputs by June 27,
%The BVH format is the same
in the same format as that used by the challenge training data.
Manual tweaking of test inputs or the output motion was not allowed, since the idea was to evaluate synthesis performance in an unattended setting.
As a precondition for participating in the evaluation, teams agreed to submit a companion paper describing their system for review and possible publication at the conference where the challenge took place.
Evaluations took place only after the generated motion was submitted by all teams.

\subsection{Tiers}
The challenge evaluation was divided into two tiers, one for full-body motion and one for upper-body motion only.
Each tier had its own reasons for being included.
On the one hand, the data comprises recorded full-body motion from conversational interactions.
It can furthermore be argued that human embodied conversation uses the full body.
Also, generating full-body behaviour seems like a harder problem, since it represents a higher-dimensional probability distribution which is more difficult to learn from a statistical perspective.
Therefore, if full-body generation is solved, restricted versions of the problem can be expected to be solved as well.
On the other hand, it is debatable to what extent the motion of the lower body whilst speaking constitutes co-speech gestures that depend on the speech, over other aspects such as proxemics and stance in response to the other parties in a conversation (which is data that was not provided to challenge participants this time).
As a result, including lower-body motion may add unnecessary complexity to the gesture generation problem, and act as a distraction when evaluating the quality of generated gestures.
%In addition, f
Focusing on the upper body also is more consistent with earlier evaluations of co-speech gesture generation, such as the GENEA Challenge 2020 \cite{anon2021genea}.
Because it is not clear which perspective to apply, our evaluation included one tier each for full-body and upper-body motion.
Teams could enter motion into either tier, or into both, but could only make one submission per tier.
Teams that entered into both tiers were allowed to submit different motion (BVH files) to each tier, if they wished.
Both tiers used the same training data but differed in which parts of the avatar that were allowed to move, and in the camera angle used for the video stimuli in the evaluation, as follows:
\begin{description}
\item[Full-body tier] In this tier, the entire virtual character was free to move, including moving around in space relative to the fixed camera.
Motion was visualised from an angle facing the character that showed most of the legs, but not where the feet touched the ground.
This perspective was chosen to show as much as possible of the character, whilst obscuring foot penetration or foot sliding artefacts from view, since these artefacts arguably do not relate to co-speech gestures, yet they may influence ratings if visible.
An example of this camera perspective can be seen in Fig.\ \ref{fig:hemvipgui}.
\item[Upper-body tier] In this tier, the virtual character used a fixed position and a fixed pose from the hips down, with only the upper body free to move.
Motion was visualised from a camera angle facing the character, cropped slightly below the hips, such that the hands always should remain in view.
Any motion of the lower-body joints in submitted BVH files was ignored by the visualisation.
This camera perspective is shown in Fig.\ \ref{fig:evaluation_interface_pairwise}.
\end{description}


\subsection{Baselines and participating teams}
\label{ssec:teams}
The challenge evaluation featured three types of motion sources: natural motion capture from the speakers in the database, baseline systems based on open code, and submissions by teams participating in the challenge.
We call each source of motion in a tier a \emph{condition} (not a ``system'', since not all conditions represent motion synthesised by an artificial system).
Each condition was assigned a unique three-letter \emph{label} or \emph{condition ID}, where the first character signifies the tier, with F for the full-body tier and U for the upper-body tier.

Natural motion was labelled \textbf{FNA} in the full-body tier and \textbf{UNA} in the upper-body tier (NA for ``natural'').
These conditions can be seen as a top line, and surpassing their performance essentially means outperforming the dataset itself, subject to limitations due to the motion capture and visualisation (discussed in Secs.\ \ref{ssec:humlikecomments} and \ref{sec:limitations}).
\begin{table*}
%\small
\centering
\caption{Conditions participating in the evaluation. Conditions are ordered based on their median human-likeness scores from higher to lower (see Table \ref{tab:stats}).
The following abbreviations were used:
\textit{AR} for ``Auto-regression'',
\textit{CNN} for ``Convolutional Neural Network'',
\textit{RNN} for ``Recurrent Neural Network'',
\textit{SA} for ``Neural self-attention'' (e.g., Transformers), \textit{GANs} for ``Generative adversarial networks or adversarial loss terms'',
\textit{VAEs} for ``Variational auto-encoders'',
\textit{MGs} for ``Motion graphs'',
\textit{Frame-wise} for ``Generating output frame-by-frame'', \textit{Stoch.\ output} for ``Stochastic output'' (different output possible even if the inputs are the same),
and \textit{Smoothed} for ``Smoothing was applied''.}
\small%
\begin{tabular}{@{}lll|ccc|c|ccccl|ccc@{}}
\toprule 
 & \multicolumn{2}{c|}{Per-tier} & \multicolumn{3}{c|}{Inputs used} & Hands & \multicolumn{5}{c|}{Techniques used} & Frame- & Stoch. & Smoothed \tabularnewline
Baseline or team name & \multicolumn{2}{c|}{label} & Aud. & Text & Sp. ID & Fixed & AR & RNNs & SA & VAEs & Other & wise & output & \tabularnewline
%\midrule
%Natural motion & \cmark & \cmark & \cmark & \multicolumn{7}{c@{}}{Not relevant} \tabularnewline
\midrule 
GestureMaster & FSA & USQ & \cmark & \cmark & \cmark &  & &  &  &  & Hand-crafted rules, MGs &  &  & \cmark\tabularnewline
Forgerons & FSC & USO & \cmark &  &  & \cmark & \cmark & \cmark &  & \cmark &  & \cmark & \cmark &   \tabularnewline
DeepMotion & FSI & USJ & \cmark & \cmark &  & \cmark & \cmark &  & \cmark & \cmark & CNNs & \cmark & \cmark & \tabularnewline
DSI & FSF &  & \cmark &  &  &  & \cmark & \cmark & \cmark &  &  &  &  & \tabularnewline
UEA Digital Humans & FSG & USM & \cmark & \cmark & \cmark &  & & \cmark &  &  &  & \cmark &  & \tabularnewline
ReprGesture & & USN & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & CNNs, GANs &  &  & \cmark\tabularnewline
IVI Lab & FSH & USK & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark &  &  &  & \cmark & \cmark & \cmark\tabularnewline
FineMotion & FSD & & \cmark & \cmark &  &  & \cmark & \cmark &  &  &  & \cmark &  & \cmark\tabularnewline
Murple AI lab & \multicolumn{2}{c|}{Not revealed} & \cmark &  &  &  & \cmark & \cmark &  &  & Normalising flows & \cmark & \cmark & 
\tabularnewline
Text-only baseline
& FBT & UBT & & \cmark & & \cmark & \cmark & \cmark  &  &  &  & \cmark &  & \cmark \tabularnewline
Audio-only baseline
& & UBA & \cmark &  &  & &  &  \cmark &  &  &  & \cmark &  & \cmark \tabularnewline
%\midrule 
TransGesture & & USL & \cmark &  & & \cmark  & \cmark & \cmark &  &  &  & \cmark &  & \cmark\tabularnewline
\bottomrule
\end{tabular}
\label{tab:conditions}
\end{table*}

\iffalse
\fi

The natural top line can be contrasted against the two baseline systems included in the challenge, which represent previously published gesture-generation methods with free and open code, adapted to run on the 2022 challenge training data.
These two baselines were:
\begin{description}
\item [Text-based baseline (FBT/UBT)]
This motion was generated by the gesture-synthesis approach from \citet{yoon2019robots} (which takes text transcriptions with word-level timestamps as the input) but adapted to joint rotations. % as described in \cite{anon2021genea}.
A neural sequence-to-sequence architecture is used, where an encoder processes a sequence of speech words and a decoder outputs a sequence of human poses.
Motion from this baseline used a fixed lower body but was included in both tiers, as conditions \textbf{FBT} and \textbf{UBT}
(B for ``baseline'' and T for ``text'').
The code is publicly available online at (link hidden for anonymisation). % \href{https://github.com/youngwoo-yoon/Co-Speech_Gesture_Generation/}{github.com/youngwoo-yoon/Co-Speech\_Gesture\_Generation/}.
\item [Audio-based baseline (UBA)]
This motion was generated by the Audio2Repr2Pose motion-synthesis approach of \citet{kucherenko2019analyzing}, which only takes speech audio into account when generating output, adapted to joint rotations. % as described in \cite{anon2021genea}.
This model uses a chain of two neural networks: one maps from speech to pose representation and another decodes the representation to a pose, generating motion frame-by-frame by sliding a window over the speech input.
Motion from this baseline was only included in the upper-body tier, as condition \textbf{UBA} (A for ``audio'').
The code is publicly available online at (link hidden for anonymisation). %  \href{https://github.com/genea-workshop/Speech_driven_gesture_generation_with_autoencoder/tree/GENEA_2022/}{github.com/genea-workshop}.
\end{description}
These are the same baselines as in the GENEA Challenge 2020.
They were included to track the progress of the field and to provide continuity between different years of the challenge.

Separate from top lines and baselines, a total of 10 teams participated in the GENEA evaluation, with 8 \emph{entries} (a.k.a.\ \emph{submissions}) to the full-body tier and 8 entries to the upper-body tier.
Submissions were labelled with the prefix FS and US (S for ``submission'') depending on the tier, followed by a single character to distinguish between different submissions in the same tier.
In particular, challenge entries to the full-body tier were labelled \textbf{FSA}--\textbf{FSI}, and entries to the upper-body tier were labelled \textbf{USJ}--\textbf{USQ}.
Condition FSE was withdrawn before the evaluation.
These labels are anonymous and have no relationship to team identities, but teams were free to reveal their label(s) in papers describing their systems, if they wished.

Table \ref{tab:conditions} lists the baselines and participating teams, with basic information about their respective approaches.
%and references to their system-description papers (anonymised for review).
All teams but one published a paper about their system, and all of the published papers chose to reveal the label(s) of their submitted systems.
We have therefore included that label information in Table \ref{tab:conditions}.
Paper citations have temporarily been omitted to preserve anonymity during review.
%One team lacks information, since they did not submit a paper for review.


\section{Evaluation}
\label{sec:evaluation}
We conducted a large-scale, crowdsourced, joint evaluation of gesture motion from the 10 full-body conditions and 11 upper-body conditions (listed in Table \ref{tab:conditions}) in parallel using a within-subject design (i.e., every rater was exposed to and evaluated all conditions in each tier).
%The evaluation focused on gesture quality of the various submitted systems.
The systems were evaluated in terms of the human-likeness of the gesture motion itself, as well as the appropriateness (a.k.a.\ specificity) of the gestures for the given input speech.
The central difference from other gesture-generation evaluations is that all systems in our evaluation used the same motion data, the same visualisation/embodiment, and were rated together using the same evaluation methodology; only the motion-generation systems differed between the different entries that were compared. This allows the performance of systems to be compared directly, and the design aspects that influence performance can be traced more efficiently than in most previous publications.
%\citet{jonell2020iva_crowd} recently found that the results from crowdsourcing evaluations were not significantly different from in-lab evaluations in terms of results and consistency.
%We therefore adopted an entirely crowdsourced approach, as opposed to, for example, the Blizzard Challenge, which has used a mixed approach.
The subjective evaluation used an entirely crowdsourced approach, with attention checks used to exclude participants that were not paying attention, as detailed in Sec.\ \ref{subs:att_checks_hl}.
The remainder of this section describes the experiments we performed.
Results of the subjective evaluation are subsequently presented in Sec.\ \ref{sec:results} and discussed in Sec.\ \ref{sec:discussion}.

Although the aim of the challenge is to quantify how natural and appropriate motion appears to human observers, we have also seized the opportunity to compute a number of objective metrics of motion quality on the motion materials in the evaluation.
The design of that experiment is described in Sec.\ \ref{ssec:obj_metrics}, with results reported in Sec.\ \ref{ssec:objectiveresults} and discussed in Sec.\ \ref{ssec:obj_discussion}.
We see this primarily as an evaluation of the metrics themselves, and not as an evaluation of the different conditions in the challenge.
%challenge computes and provides some objective metrics of motion quality (see Sec.\ \ref{ssec:obj_metrics}), the focus of the GENEA Challenge is on a large-scale, crowdsourced subjective evaluation of the generated gestures.

\subsection{Subjective evaluation structure}
For each tier, two orthogonal aspects of the generated gestures were evaluated (with one study per aspect and tier):
\begin{description}
\item[Human-likeness] Whether the motion of the virtual character looks like the motion of a real human, controlling for the effect of the speech. We sometimes use ``motion quality'' as a synonym for this.
\item[Appropriateness] (a.k.a.\ ``specificity'') Whether the motion of the virtual character is appropriate for the given speech, controlling for the human-likeness of the motion.
\end{description}
More details about these evaluations are provided in Sections \ref{sec:humlike} and \ref{sec:approp} below, respectively.

% Seems useful for a journal
Although an interesting question for a multispeaker dataset, we did not attempt to evaluate the appropriateness/specificity of the gesture motion style to different individuals in the database, since the data is too imbalanced to allow such an evaluation.
%Other facets of appropriateness, such as emotional appropriateness, or the appropriateness of motion for a specific individual speaker, were not evaluated.
Additionally, even though the speech and motion in the challenge comes from joint full-body motion capture of dyadic interactions with separate close-talking microphones for each speaker, the challenge only considered generating one side of the conversation, without awareness of the other party in the interaction (neither for the synthesis, not for the evaluation), in order to reduce problem complexity.

\subsection{Stimuli}
\subsubsection{Speech segment selection}
The test set was deliberately made large to make it difficult to overfit to specific speech being evaluated.
Like the GENEA Challenge 2020 and
the Blizzard Challenges, not all test-set motion was included in the subjective evaluation.
From the 40 test-set chunks we selected 48 short \emph{segments} of test speech and corresponding test motion to be used in the subjective evaluations, based on the following criteria:
\begin{enumerate}
\item Segments should be around 8 to 10 seconds long, and ideally not shorter than 6 seconds.
\item The character should only be speaking, not passively listening, in the segments.
(No turn-taking, but backchannels from the interlocutor were OK.)
\item Segments should not contain any parts where \citet{lee2019talking} had replaced the speech by silence for anonymisation.
%(this appeared to affect mostly proper nouns).
\item Segments should be more or less complete phrases, starting at the start of a word and ending at the end of a word, and not end on a ``cliffhanger''. A small margin was permitted towards the end of segments.
\item Finally, recorded motion capture in the segments (i.e., the FNA motion) should not contain any significant artefacts such as whole-body vibration or hands flicking open and closed due to poor finger tracking.
\end{enumerate}
The last item does not imply that the motion capture was perfect or completely natural for all segments in the evaluation, since the finger-tracking quality throughout the database does not allow our evaluations to reach that standard.
It merely means that the level of finger-tracking quality in the stimuli was consistent with the better parts of the source material from \citet{lee2019talking}.

The 48 segments we selected were between 5.6 and 12.1 seconds in duration and on average 9.5 seconds long.
Audio was loudness normalised to $-23$ dB LUFS following EBU R128 \cite{ebu2020loudness} to maintain a consistent listening volume in the user studies.
\begin{figure*}[!t]
\centering
%   \hfill
\begin{subfigure}[b]{0.416\textwidth}
    \includegraphics[width=\textwidth]{figures/hemvip_cropped.png}
    \caption{Human-likeness interface (HEMVIP) and full-body video}
    \label{fig:hemvipgui}
\end{subfigure}
\hfill\hfill
\begin{subfigure}[b]{0.564\textwidth}
    \centering\includegraphics[width=\columnwidth]{figures/pairwise_cropped_upper.png}
    \caption{Appropriateness interface and upper-body videos}
    \label{fig:evaluation_interface_pairwise}
\end{subfigure}
%   \hfill
\vspace{-1.5pt}
\caption{Screenshots of the evaluation interfaces used in the studies, also showing the camera perspectives used by the tiers.}
%\vspace{-2.6ex}
\label{fig:interfaces}
\Description{The left figure shows the human-likeness interface. The interface presents a video showing a full-body avatar and eight sliders for rating. There are play buttons for each slider and one reload video button. The right figure shows the appropriateness interface. The interface presents two side-by-side videos showing an upper-body avatar. There is a radio option group for the question asking which character's motion best matched the speech. The options are 1) the character in the video on the left, 2) the character in the video on the right, 3) they are equal. The button 'Report as broken' is on the bottom.}
\vspace{-1.5pt}
\end{figure*}

\subsubsection{Visualisation}
We used the same virtual avatar (shown in Fig.\ \ref{fig:interfaces}) in all rendered videos during the challenge and the evaluation.
The avatar had 56 joints (full body including fingers) and was designed to be gender neutral and omit eyes or mouth to help evaluators focus on the rest of the body instead.
All teams had access to the official visualisation and rendering pipeline during the system-building phase, in the form of code, a portable Docker container, and a web server to which BVH files could be submitted to be rendered as video.
%\subsubsection{Visualisation server}
\label{subs:vis_ser}
%We also developed a visualisation server that enabled all participating teams to produce gesture-motion visualisations identical
%(except in resolution)
%to the video stimuli evaluated in the challenge.
%This was implemented using a headless Python-based web server which interfaced with \href{https://www.blender.org}{Blender} 2.93.9 and \href{https://ffmpeg.org/}{ffmpeg} to combine audio and video streams in a single video file.
Participants could send a 30-fps BVH file to the visualisation server, and these files would then be processed as quickly as possible into videos visualising the motion on the avatar.
%, in the order they came in (i.e., first in, first out).
%The server supports spawning multiple rendering workers, making it possible to do vertical scaling as long as the hardware resources allow it.
%The same visualisation was also used to render the final stimuli, in a resolution of 1440$\times$1080.
%but with the resolution increased to 1440$\times$1080 instead of the default 480$\times$270. The lower resolution was used to decrease the time spent rendering a video, consequently improving throughput of the server, since 30 teams initially took part in the challenge.
The visualisation server code has been open sourced.
%and rendered stimulus videos are attached.
The code was available to the participants during the challenge, and they were free to use it to host their own servers if they wished.
The final rendered stimuli used a resolution of 1440$\times$1080.
Videos are attached.

\subsection{Human-likeness evaluation}
\label{sec:humlike}

The human-likeness evaluation of the GENEA Challenge 2022 closely followed the human-likeness evaluation in the GENEA Challenge 2020 \cite{anon2021genea}, by presenting multiple motion examples in parallel and asking the subject to provide a rating for each one.
The human-likeness evaluation was based on the HEMVIP (Human Evaluation of Multiple Videos in Parallel) methodology \cite{jonell2021hemvip}, where multiple motion examples are presented in parallel and the subject is asked to provide a rating for each one.
All stimulus videos on the same page (a.k.a.\ screen) of the evaluation corresponded to the same speech segment but different conditions.
The advantage of the HEMVIP method is that differences in rating between the different conditions can be analysed using pairwise statistical tests, which helps control for variation between different subjects and different input speech segments (as seen in the results in Sec.\ \ref{ssec:pairwisehumlike}).
%The videos used in this evaluation had the audio removed, since it has been found that speech and gesture perception influence each other \cite{bosker2021beat} and can confound motion evaluations \cite{jonell2020let}.
%(see Section\ \ref{ssec:pairwisehumlike}), where each observation was gathered from the same person watching videos featuring the same speech segment -- the only difference between videos on the same page being which condition that generated the motion of the virtual character in each video clip.
For a detailed explanation of the evaluation interface we refer the reader to \citet{jonell2021hemvip}, which introduced and validated the evaluation paradigm for gesture-motion stimuli.

%Code is provided at \href{https://github.com/jonepatr/hemvip/tree/genea2022/}{github.com/jonepatr/hemvip/tree/genea2022/}.

Each evaluation page asked participants ``How human-like does the gesture motion appear?'' and presented eight video stimuli to be rated on a scale from 0 (worst) to 100 (best) by adjusting an individual GUI slider for each video.
An example of the evaluation interface can be seen in Fig.\ \ref{fig:hemvipgui}.
Like in \cite{anon2021genea,jonell2021hemvip}, the 100-point rating scale was anchored by dividing it into successive 20-point intervals with labels (from best to worst) ``Excellent'', ``Good'', ``Fair'', ``Poor'', and ``Bad''.
These labels were based on those associated with the 5-point scale used in
from the Mean Opinion Score ITU standard \cite{itu1996telephone} for audio quality evaluation.
Since it has been found that speech and gesture perception influence each other \cite{bosker2021beat} and can confound motion evaluations \cite{jonell2020let}, the videos seen by participants in these human-likeness evaluations (although they all corresponded to the same speech input and had the same length) were completely silent and did not include any audio.
This way, ratings can only depend on the motion seen in the videos, and not on their appropriateness for the speech.

The test was preceded by a screen with instructions, which the participants would read.
Then, each subject completed one training page showing a fixed set of videos with different motion, to familiarise participants with the task and what the stimuli would look like, before starting the study in earnest.
The training phase was followed by 10 pages of ratings for the evaluation.
Responses given on the training page were not included in the analysis.
The evaluation was balanced such that each segment appeared on pages 1 through 10 with approximately equal frequency across all participants (segment order), and each condition was associated with each slider with approximately equal frequency across all pages (condition order).
For any given participant and study, each of the 10 pages would use a different speech segment.
Every page in the evaluation contained one stimulus video from condition FNA/UNA. %, visualising the motion-capture data gathered from the speaker at the same time that they uttered the speech in question.
This was used to help calibrate evaluators' ratings and keep them consistent throughout the test.
Since motion-capture data projected onto a virtual character may not necessarily be perceived as
perfectly natural, there was no requirement to rate the best motion as 100.
After completing the rating pages, but before submitting the study, participants filled in a short questionnaire to gather broad, anonymous demographic information, the results of which are presented in Sec.\ \ref{subs:att_checks_hl}.

\subsection{Appropriateness evaluation}
\label{sec:approp}
The appropriateness evaluation was designed to assess the link between the motion and the input speech, separate from the intrinsic human-likeness of the motion.
In the previous GENEA Challenge, appropriateness was evaluated using a HEMVIP-based rating study very similar to that for human-likeness, except that speech audio was included in the videos.
Test takers were asked to ignore the motion quality and only rate the appropriateness of the motion for the speech \cite{anon2021genea}.
Unfortunately, that evaluation was not altogether successful, since their \emph{mismatched} condition M -- which paired natural motion segments with unrelated speech segments, intended as a bottom line -- attained the second-highest appropriateness rating, above all synthetic systems.
This suggests a significant interaction between the perceived human-likeness of a motion segment and its perceived appropriateness for speech.
That interaction acted as a confounder in their study, with the result that all systems ranked below natural-looking motion unrelated to the speech, intended as a bottom line in terms of appropriateness.

For the GENEA Challenge 2022, we decided to evaluate motion appropriateness for speech in a different way.
Our design goal for the 2022 challenge was to assess appropriateness whilst controlling for the human-likeness of the motion in an effective way.
To do so, we took the idea of motion mismatching like in \citet{jonell2020let} and used it within every condition (not just for the recorded motion-capture data FNA/UNA):
On each page, subjects were presented with a pair of videos containing the same speech audio.
Both videos contained motion from the same condition and thus had the same overall motion quality, but one was matched to the speech audio and the other mismatched, belonging to unrelated speech.
Whether the left or the right video was mismatched was randomised.
Subjects were then asked to ``Please indicate which character's motion best matches the speech, both in terms of rhythm and intonation and in terms of meaning.''
In response, they could choose the character on the left, on the right, or indicate that the two were equally well matched (``They are equal'', also referred to as \emph{equal}\ or a \emph{tie}).
We asked for preferences rather than ratings since there is evidence \cite{wolfert2021rate} that this is more efficient in pairwise comparisons like these.
A screenshot of the evaluation interface used for the appropriateness studies is presented in Fig.\ \ref{fig:evaluation_interface_pairwise}.

The extent to which test-takers prefer the character with the matched motion reveals how specific the gesture motion is to the given speech: Random motion will result in a 50--50 split, whereas conditions whose motion is more specifically appropriate to the input speech are expected to elicit a higher relative preference for the matched motion.
In this type of evaluation, condition M (the mismatched condition) from the 2020 challenge will perform at chance rate, rather than being tied for second highest as in 2020.

To our knowledge, this approach to control for motion quality in subjective evaluations was first piloted in \citet{jonell2020let}, specifically looking at facial gestures.
\citet{rebol2021passing} used a similar methodology with preference tests to quantify the correlation (essentially, appropriateness) between generated hand and body gestures and their associated speech, which we were not aware of until after conducting our challenge.
However, they asked a different question of the users, did not quantify the appropriateness of real human motion, and only used the approach to evaluate a single gesture-generation method.

Concretely, we created the mismatched stimuli by taking the 48 existing speech and motion segments from the evaluation, and permuted the motion in between them such that no motion segment ever remained in its original place.
%Mathematicians call such a permutation a \emph{derangement}.
As the 48 different segments did not all have the same length, a longer or shorter segment of motion generally had to be excerpted from the motion chunks (original or generated), so as to match the new speech duration.
The starting point of the motion video was always the same as in the respective matched stimulus video (i.e., corresponding to the start of a phrase).
%The mismatched video stimuli are available at \href{}{here}.

After an instruction page and a training page, each subject evaluated 40 pages with one pair of videos each.
This means that subjects watched 80 videos total in each study, the same number of videos as was evaluated in the human-likeness studies (ignoring the training pages in all cases).
Each study was balanced such that each speech segment, condition, and order of the two videos appeared approximately equally many times.

\subsection{Test takers and attention checks}
\label{subs:att_checks_hl}
It has recently been found that crowdsourced evaluations are not significantly different from in-lab evaluations in terms of results and consistency \cite{jonell2020iva_crowd}.
The challenge therefore adopted an entirely crowdsourced approach, as opposed to, for example, the Blizzard Challenge, which has used a mixed approach. Attention checks were used to exclude participants that were not paying attention.
%, as detailed in Section\ \ref{subs:att_checks_hl}.
Test takers (a.k.a.\ subjects) were recruited through the crowdsourcing platform \href{https://www.prolific.co/}{Prolific}.
We used Prolific's built-in pre-screening tools to restrict the pool of test-takers in two ways: i) subjects were required to reside in any of six English-speaking countries, namely Australia, Canada, Ireland, New Zealand, the United Kingdom, and the USA, and ii) subjects were required to have English as their first language.

We conducted four user studies, two for human-likeness and two for appropriateness.
A subject could take one or more studies, but could only participate in each study at most once, and could not use a phone or tablet to take the test.

Each study incorporated four attention checks per person, to make sure that subjects were paying attention to the task and remove insincere test-takers.
For the human-likeness studies, these attention checks took the form of a text message ``Attention! You must rate this video NN'' superimposed on the video.
``NN'' would be a number from 5 to 95, and the subject had to set the corresponding slider to the requested value, plus or minus 3, to pass that attention check.
Which sliders on which pages that were used for attention checks was uniformly random, except that no page had more than one attention check, and the natural motion (condition FNA and UNA) was never replaced by an attention check.
For the appropriateness studies, the attention checks either displayed a brief text message over the gesticulating character, reading ``Attention! Please report this video as broken'', or they temporarily replaced the audio with a synthetic voice speaking the same message.
Subjects were exposed to two attention checks of each kind.
To pass the attention check, participants had to click a button marked ``Report as broken'' seen in Fig.\ \ref{fig:evaluation_interface_pairwise}, forwarding them to the next pair of videos in the evaluation.
Since reporting a video as broken avoids having to give a response, it can in theory be used to quickly skip through the test.
To help prevent this, we implemented the button such that it becomes clickable after a 5-second delay, after the page is loaded.
However, as this does not fully prevent skipping through the test, subjects who used that button more than three times on pages without attention checks were also removed without pay.
In all studies, the attention-check messages did not appear until a few seconds into each attention-check video, so that participants who only watched the first seconds would be unlikely to pass the checks.

Subjects who failed two or more attention checks were removed from the respective study without being paid, since Prolific's policies do not allow rejecting a subject on the basis of a single failed attention check.
Only the subjects who failed zero or one attention check for a study have been included in our analyses below.
Responses to videos used for attention checks were not included in our analyses.
Right before submitting their results, subjects also filled in a short questionnaire to gather broad, anonymous demographic information about the population taking the test.

A design goal of the human-likeness studies was that every combination of two distinct conditions should appear on the pages approximately equally often, and at least 600 times (not counting FNA/UNA, which appeared on every page).
To meet this goal, we recruited 121 test takers that successfully passed the attention checks and completed the full-body study, and 150 test takers that successfully passed the attention checks and completed the upper-body study.
Since the latter study compared 11 conditions instead of only 10, it required more participants to reach the desired number of ratings pairs.
Of the 121 test takers in the full-body study, 60 identified as female, 60 as male, and 1 did not want to disclose their gender.
The same numbers for the 150 upper-body test takers were 74, 75, and 1, respectively.
For the full-body test takers, 2 resided in Australia, 2 in Canada, 3 in Ireland, 110 in the United Kingdom, and 4 in the USA.
%The mean age for the full-body study was 37.89 with a standard deviation of 11.47. 
The upper-body study had 1 participant residing in Australia, 4 in Ireland, 134 in the United Kingdom, and 11 in the USA.
%The mean age for the upper-body study was 39.68 with a standard deviation of 12.59. 
The mean age of the test takers was 38 years with a standard deviation of 12 in the full-body study and 40 years with a standard deviation of 13 in the upper-body study.
All these participants passed all attention checks, except for one subject in the upper body study, who failed one attention check.

For the appropriateness studies, our design goal was for each condition to receive as many responses per condition as the number of ratings that each condition (aside from FNA/UNA) received in the corresponding human-likeness evaluation.
This works out to 880 responses per condition in the full-body studies and 990 responses per condition in the upper-body studies.
Because a subject in these studies provided half as many responses as in a human-likeness study (40 vs.\ 80), the appropriateness studies needed to recruit approximately twice as many test takers.
In the end, 247 test takers successfully passed the attention checks in the full-body study, while 304 passed the attention checks in the upper-body study.
%10 participants failed 1 attention check in the full-body study, and 14 missed 1 attention check in the upper-body study. 

Of the 247 participants in the full-body study, 137 identified as female, 107 as male, and 3 did not want to disclose their gender.
% full body study: The mean age was 38 with standard deviation 14).
% upper body study: The mean age was 38 with standard deviation 13).
The same numbers for the 304 upper-body test takers were 127, 173, and 4, respectively.
For the full-body test takers, 3 resided in Australia, 13 in Canada, 10 in Ireland, 2 in New Zealand, 211 in the United Kingdom, and 8 in the USA.
The upper-body study had 2 participants residing in Australia, 10 in Canada, 1 in Ireland, 256 in the United Kingdom, and 35 in the USA.
The mean age of the test takers was 38 years in both studies, with a standard deviation of 14 for the full-body study and 13 for the upper-body study.
All of these passed all attention checks, except for 10 participants in the full-body study and 14 participants in the upper-body study, who each failed one attention check.
Each subject in a study contributed 36 ratings to the analyses after removing attention checks, unless they had to skip a page in the rare case of a video failing to load (which occurred approximately 1.6 times per 1000 videos presented).

Test takers were remunerated 6 GBP for each successfully completed human-likeness study.
Since the median completion time was 28 minutes each, this corresponds to a median compensation just above 12 GBP per hour.
Similarly, the appropriateness studies took a median of 24 or 25 minutes to complete, and earned a reward of 5.5 GBP each, amounting to around 13 GBP per hour.
These compensation levels all exceed the UK national living wage and also exceeds the highest living wage quoted by the Living Wage Foundation in the UK.
All numbers are as measured by Prolific, which uses the median rather than the mean for these calculations to prevent extreme completion times from skewing the data.
%``so that extreme completion times do not skew the data''.
%
Response data from the evaluation and statistical analysis code is attached.

\subsection{Objective metrics}
\label{ssec:obj_metrics}
The main goal of the GENEA challenge is to compare human subjective impressions of the outputs of different gesture-generation systems.
We therefore we discourage using the results of automated performance metrics as indicators of the perceptual impressions of different systems.
%achieve human-like and appropriate speech-driven automatic gesture generation, we discourage using the results of automated performance metrics as indicative the merits of a system.
However, since subjective evaluation is costly and time-consuming, it would be beneficial for the field to identify meaningful objective evaluation methods, especially for use during system development.
%In addition to the subjective evaluation, we also computed a number of different objective metrics on the conditions in the challenge evaluation.
As a step in this direction we therefore considered five objective measures previously used to evaluate co-speech gestures, namely average jerk, average acceleration, distance between gesture speed (i.e., absolute velocity) histograms, canonical correlation analysis, and the Fréchet distance between motion feature distributions.
We computed these metrics for each condition in each tier using the complete test sequences, i.e., not only on the motion segments featured in the subjective evaluation.
Details on each metric are provided below.

To compare and validate these metrics against our subjective evaluation, we provide results on the rank correlations between subjective and objective metrics in Sec.\ \ref{ssec:objectiveresults}. 

\subsubsection{Average acceleration and jerk}
The third time derivative of the joint positions is called \emph{jerk} and can be formulated mathematically as $\textrm{jerk}(\boldsymbol{x}) = \boldsymbol{x}'''(t)$.
The average value of the absolute magnitude of the jerk is commonly used to quantify motion smoothness \citep{morasso1981spatial,uno1989formation,kucherenko2019analyzing}.
We report average values of absolute jerk (defined using finite differences) averaged across all test motion segments. A perfectly natural system should have average jerk very similar to natural motion.

We also evaluated the same measure, but computed using the absolute value of the acceleration $\textrm{acc.}(x) = x''(t)$ instead of the jerk.
Again, we expect natural-looking motion to have similar average acceleration as in the reference data.

\subsubsection{Comparing speed histograms}
The distance between speed histograms has also been used to evaluate gesture quality \citep{kucherenko2019analyzing,kucherenko2020gesticulator}, since well-trained models should produce motion with similar properties to that of the actor it was trained on.
In particular, it should have a similar motion-speed profile for any given joint.
This metric is based on the assumption that synthesised motion should follow a similar speed distribution as the ground truth motion.
To evaluate this similarity we calculate speed-distribution histograms for all systems and compare them to the speed distribution of natural motion (condition N) by computing the Hellinger distance \citep{nikulin2001hellinger},
\begin{align}
H(\boldsymbol{h}^{(1)},\, \boldsymbol{h}^{(2)})
& = \sqrt{1 - \sum_{i}{\sqrt{h^{(1)}_i \cdot h^{(2)}_i}}}
\text{,}
\end{align}
between the histograms $\boldsymbol{h}^{(1)}$ and $\boldsymbol{h}^{(2)}$.
Lower distance is better.

For both of the objective evaluations above the motion was first converted from joint angles to 3D coordinates. The code for the numerical evaluations has been made publicly available to enhance reproducibility.%\footnote{See \href{https://github.com/genea-workshop/genea_numerical_evaluations}{github.com/genea-workshop/genea\_numerical\_evaluations}.}

\subsubsection{Canonical correlation analysis}

Canonical correlation analysis (CCA) \citep{thompson1984canonical} is a form of linear subspace analysis, and involves the projection of two sets of vectors
(here the generated poses and those from FNA/UNA, respectively)
%(here, the variable sets $\boldsymbol{x}$ and $\boldsymbol{y}$)
onto a joint subspace.
CCA has been used to evaluate gesture-generation models in previous work \citep{sadoughi2019speech, bozkurt2015affect,jinhong_lu_2020_4088376}.

The goal of CCA is to find a sequence of linear transformations of each variable set, such that the Pearson correlation between the transformed variables is maximised. This correlation is what we use as a similarity measure, and we report it as global CCA values in our results.
A high value is considered better.
%
\begin{table*}[!t]
\centering%
\caption{Summary statistics of responses from all user studies, with 95\% confidence intervals. ``M.'' stands for ``matched'' and ``Mism.'' for ``mismatched''. ``Percent matched'' identifies how often subjects preferred matched over mismatched motion. \rev{The human-likeness values are between 0 and 100; the higher, the better}}
\label{tab:stats}
%\hfill
\begin{subtable}[t]{0.49\textwidth}
\centering%
\caption{Full-body}%
\label{stab:fbstats}%
\small%
\begin{tabular}{@{}l|c|ccc|c@{}}
\toprule
& Median
& \multicolumn{4}{c@{}}{Appropriateness} \\
& human- & \multicolumn{3}{c|}{Num.\ responses}
& Percent matched\\
ID & likeness%Median rating
& M. & Tie & Mism. & (splitting ties)\\
\midrule
FNA & $70\hphantom{.0}\in{}[69,71]$
& 590 & 138 & 163 & $74.0\in{}[70.9,76.9]$\\
FBT & $27.5\in{}[25,30]$
& 278 & 362 & 250 & $51.6\in{}[48.2,55.0]$\\
FSA & $71\hphantom{.0}\in{}[70,73]$
& 393 & 216 & 269 & $57.1\in{}[53.7,60.4]$\\
FSB & $30\hphantom{.0}\in{}[28,31]$
& 397 & 163 & 330 & $53.8\in{}[50.4,57.1]$\\
FSC & $53\hphantom{.0}\in{}[51,55]$
& 347 & 237 & 295 & $53.0\in{}[49.5,56.3]$\\
FSD & $34\hphantom{.0}\in{}[32,36]$
& 329 & 256 & 302 & $51.5\in{}[48.1,54.9]$\\
FSF & $38\hphantom{.0}\in{}[35,40]$
& 388 & 130 & 359 & $51.7\in{}[48.2,55.1]$\\
FSG & $38\hphantom{.0}\in{}[35,40]$
& 406 & 184 & 319 & $54.8\in{}[51.4,58.1]$\\
FSH & $36\hphantom{.0}\in{}[33,38]$
& 445 & 166 & 262 & $60.5\in{}[57.1,63.8]$\\
FSI & $46\hphantom{.0}\in{}[45,48]$
& 403 & 178 & 312 & $55.1\in{}[51.7,58.4]$\\
\bottomrule
\end{tabular}
\end{subtable}%
\hfill\hfill
\begin{subtable}[t]{0.49\textwidth}
\centering%
\caption{Upper-body}%
\label{stab:ubstats}%
\small%
\begin{tabular}{@{}l|c|ccc|c@{}}
\toprule
& Median
& \multicolumn{4}{c@{}}{Appropriateness} \\
& human- & \multicolumn{3}{c|}{Num.\ responses}
& Percent matched\\
ID & likeness%Median rating
& M. & Tie & Mism. & (splitting ties)\\
\midrule
UNA & $63\hphantom{.0}\in{}[61,65]$
& 691 & 107 & 189 & $75.4\in{}[72.5,78.1]$\\
UBA & $33\hphantom{.0}\in{}[31,34]$
& 424 & 264 & 303 & $56.1\in{}[52.9,59.3]$\\
UBT & $36\hphantom{.0}\in{}[34,39]$
& 341 & 367 & 287 & $52.7\in{}[49.5,55.9]$\\
USJ & $53\hphantom{.0}\in{}[52,55]$
& 461 & 164 & 365 & $54.8\in{}[51.6,58.0]$\\
USK & $41\hphantom{.0}\in{}[40,44]$
& 454 & 185 & 353 & $55.1\in{}[51.9,58.3]$\\
USL & $22\hphantom{.0}\in{}[20,25]$
& 282 & 548 & 159 & $56.2\in{}[53.0,59.4]$\\
USM & $41\hphantom{.0}\in{}[40,42]$
& 503 & 175 & 328 & $58.7\in{}[55.5,61.8]$\\
USN & $44\hphantom{.0}\in{}[41,45]$
& 443 & 190 & 352 & $54.6\in{}[51.4,57.8]$\\
USO & $48\hphantom{.0}\in{}[47,50]$
& 439 & 209 & 335 & $55.3\in{}[52.1,58.5]$\\
USP & $29.5\in{}[28,31]$
& 440 & 180 & 376 & $53.2\in{}[50.0,56.4]$\\
USQ & $69\hphantom{.0}\in{}[68,70]$
& 504 & 182 & 310 & $59.7\in{}[56.6,62.9]$\\
\bottomrule
\end{tabular}
\end{subtable}%
%hfill
\end{table*}

\subsubsection{Fréchet gesture distance} 
Recent work by \citet{yoon2020speech} proposed the Fréchet gesture distance (FGD) to quantify the quality of generated gestures. 
This metric is based on the FID metric used in image-generation studies \citep{heusel2017gans} and can be written
%
%%% START OF COMMENT BLOCK - May be useful for future reference
%We follow the FGD setup proposed by Yoon et al. \citet{yoon2020speech}, and train an auto-encoder as a feature extractor on the Human3.6M dataset from \citet{ionescu2013human3}. A convolutional encoder and decoder form the basis of our feature extractor, the encoder is trained to encode a set of direction vectors $d$ to a latent feature $z^{gesture}$. The decoder is set up to decode the latent features into a set of direction vectors, reconstructing the pose sequence the encoder was given as input. Like in \citet{yoon2020speech}, FGD($X,\hat{X}$) is defined as the Fréchet distance between the Gaussian mean and the covariance of the latent features of the ground-truth gestures $X$ and the Gaussian mean and the covariance of the latent features of the generated gestures $\hat{X}$:
%\centering
%%% END OF COMMENT BLOCK
\begin{align}
\mathrm{FGD}(\boldsymbol{X},\,\hat{\boldsymbol{X}})
& = ||\boldsymbol{\mu}_r - \boldsymbol{\mu}_g||^2 + \mathrm{tr}(\boldsymbol{\Sigma}_r + \boldsymbol{\Sigma}_g - 2(\boldsymbol{\Sigma}_r\boldsymbol{\Sigma}_g)^{1/2})
\text{.}
\end{align}
Here, $\boldsymbol{\mu}_r$ and $\boldsymbol{\Sigma}_r$ are the first and second moments of the latent-feature distribution $\boldsymbol{Z}_r$ of the human motion-capture data $\boldsymbol{X}$, whereas $\boldsymbol{\mu}_g$ and $\boldsymbol{\Sigma}_g$ are the first and second moments of the latent-feature distribution $\boldsymbol{Z}_g$ of the generated gestures $\hat{\boldsymbol{X}}$. $\boldsymbol{Z}_r$ and $\boldsymbol{Z}_g$ were extracted by the same feature extractor, which was obtained as the encoder part of a motion-reconstructing autoencoder.
We used a CNN-based autoencoder trained on the challenge dataset following the implementation in \citet{yoon2020speech}).
Lower values are better.

\subsubsection{System ranking comparison}
A good objective metric might help in evaluating the performance of a system, especially when such a metric correlates with a subjective measure.
To get more insight into whether the objective metrics in our study may be used as a proxy for subjective evaluation results, we calculated the correlation between the ranking of the conditions on median human-likeness, and the result on the objective metrics listed above.
For this, we used Kendall's $\tau$ rank correlation coefficient, and associated statistical tests \cite{kendall1970rank}. 

Of the objective metrics we studied, only CCA compares output poses directly to the corresponding reference motion-capture poses.
All other metrics are invariant to permutation, in the sense that changing the order of the different sequences (mismatching them with other speech/reference motion) will not change the value.
%of mismatching the generated motion with other speech features and reference motion.
They thus cannot measure appropriateness, which is why we only consider how those metrics correlate with human-likeness scores.
%we only compute rank correlations between these metrics and the human-likeness scores.

\section{Results}
\label{sec:results}
The results of the challenge are revolution and a revelation, for the first time finding performance that exceeds the ground-truth data in human-likeness, whilst simultaneously laying bare the true extent of the gap between natural and synthetic motion in terms of speech appropriateness.
We furthermore find that all objective metrics except for the FGD correlate so poorly with subjective scores as to be statistically indistinguishable from chance correlation.
More detail is provided in the sections below, first reporting the results of the subjective evaluation and thereafter the objective metrics.
Discussion of the various findings is reserved for Sec.\ \ref{sec:discussion}.

\subsection{Analysis and results of human-likeness studies}
\label{ssec:pairwisehumlike}
Each test taker in the human-likeness studies contributed 76 ratings to the analyses after removing attention checks, giving a total of 9,196 ratings for the full-body study and 11,400 ratings for the upper-body study.
The results are visualised in Fig.\ \ref{fig:humlikeboxplots},
with summary statistics (sample median and sample mean) for the ratings of all conditions in each of the two human-likeness studies given in the first half of Table\ \ref{tab:stats}, together with 95\% confidence intervals for the true median.
These confidence intervals were computed using order statistics, leveraging the binomial distribution cdf, while those for the mean used a Gaussian assumption (i.e., using Student's $t$-distribution cdf, rounded outward to ensure sufficient coverage); see \citet{hahn1991statistical}.
We note that statistics regarding the mean should be interpreted with caution, since responses should be seen as ordinal rather than numerical, and it is therefore improper from a perceptual perspective to perform averaging on the ratings.
\begin{figure*}[t!]
\centering%
%\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/full-body_human-likeness_boxplot.pdf}
    \caption{Full-body}
    \label{sfig:fbhumlikeboxplot}
  \end{subfigure}
\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/upper-body_human-likeness_boxplot.pdf}
    \caption{Upper-body}
    \label{sfig:ubhumlikeboxplot}
  \end{subfigure}
%\hfill
\vspace{-2.65pt}
\caption{Box plots visualising the ratings distribution in the human-likeness studies. Red bars are medians and yellow diamonds are means, each with a 0.05 confidence interval and a Gaussian assumption for the means. Box edges are at 25 and 75 percentiles, while whiskers cover 95\% of all ratings for each condition. Conditions are ordered descending by sample median for each tier.}
\label{fig:humlikeboxplots}
\Description{The first box plot shows full-body human-likeness ratings for the 10 conditions. The conditions are sorted in descending order, based on their rating; the order is FSA, FNA, FSC, FSI, FSF, FSG, FSH, FSD, FSB, and FBT. FSA shows a median rating of 71. FBT shows a median rating of 27.5. The second box plot shows upper-body human-likeness ratings for the 11 conditions. The conditions are sorted in descending order, based on their rating; the order is USQ, UNA, USJ, USO, USN, USK, USM, UBT, UBA, USP, and USL. USQ shows a median rating of 69. USL shows a median rating of 22.}
\vspace{-2.65pt}
\end{figure*}
\begin{figure*}[t!]
\centering%
%\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/full-body_human-likeness_median_pref.pdf}
    \caption{Full-body}
    \label{sfig:fbhumlikedifferences}
  \end{subfigure}
%\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/upper-body_human-likeness_median_pref.pdf}
    \caption{Upper-body}
    \label{sfig:ubhumlikedifferences}
  \end{subfigure}
%\hfill
\vspace{-2.65pt}
\caption{Significant differences in human-likeness. White means the condition listed on the $y$-axis rated significantly above the condition on the $x$-axis, black means the opposite ($y$ rated below $x$), and grey means no statistically significant difference at level $\alpha=0.05$ after Holm-Bonferroni correction. Conditions use the same order as the corresponding subfigure in Figure\ \ref{fig:humlikeboxplots}.}
\label{fig:humlikedifferences}
\Description{These two plots visualise significant differences between every two condition pairs. The first plot shows the results of the full-body human-likeness study. There are 10 conditions, namely: FSA, FNA, FSC, FSI, FSF, FSG, FSH, FSD, FSB, and FBT. FSA, FNA, FSC, FSI, FSB, and FBT showed significantdifferences over all the other conditions. FSF did not show significant preferences over FSG and FSH. FSG did not show significant preferences over FSF, FSH, and FSD. FSH did not show significant preferences over FSF, FSG, and FSD. FSD did not show significant preferences over FSG and FSH. The second plot shows the results of the upper-body human-likeness study. There are 11 conditions: USQ, UNA, USJ, USO, USN, USK, USM, UBT, UBA, USP, and USL. All the conditions except USK, USM, UBA, and USP showed significant differences over all the other conditions. USK did not show significant preferences over USM. USM did not show significant preferences over USK. UBA did not show significant preferences over USP. USP did not show significant preferences over UBA.}
\vspace{-2.65pt}
\end{figure*}

The distributions in Fig.\ \ref{fig:humlikeboxplots} are seen to be quite broad.
This is common in evaluations like HEMVIP \cite{jonell2021hemvip}, since the range of the responses not only reflects differences between conditions, but also extraneous variation, e.g., between stimuli, in individual preferences, and in how critical different raters are in their judgements.
In contrast, the plotted confidence intervals are seen to be quite narrow, since the statistical analysis can mitigate the effects of much of this variation.

Despite the wide range of the distributions, the fact that the conditions were rated in parallel on each page enables using pairwise statistical tests to factor out many of the above sources of variation.
To analyse the significance of differences in median rating between different conditions, we applied two-sided pairwise Wilcoxon signed-rank tests to all unordered pairs of distinct conditions in each study.
(This is the same methodology as in the GENEA Challenge 2020 \cite{anon2021genea}.)
This closely follows the analysis methodology used throughout recent Blizzard Challenges and, unlike Student's $t$-test (which assumes that rating differences follow a Gaussian distribution), this analysis is valid also for ordinal response scales, like those we have here.
For each condition pair, only cases where both conditions appeared on the same page and were assigned valid ratings were included in the analysis of significant differences.
(Recall that not all conditions were rated on all pages due to the limited number of sliders and the presence of attention checks.)
This meant that every statistical significance test was based on at least 615 pairs of valid ratings in the full-body study, and 603 pairs of valid ratings in the upper-body study.
Because this analysis is based on pairwise statistical tests, it can potentially resolve differences between conditions that are smaller than the width of the confidence intervals for the median in Fig.\ \ref{fig:humlikeboxplots}, since those confidence intervals are inflated by variation that the statistical test controls for.
The $p$-values computed in the significance tests were adjusted for multiple comparisons on a per-study basis using the Holm-Bonferroni method \cite{holm1979simple}, which is uniformly more powerful than conventional Bonferroni correction at keeping the family-wise error rate (FWER), often referred to as alpha-level, at or below $\alpha=0.05$

Our statistical analysis found all but 5 out of 45 condition pairs to be significantly different in the full-body study and all but 2 out of 55 condition pairs to be significantly different in the upper-body study, all at the level $\alpha=0.05$ after Holm-Bonferroni correction.
The significant differences we identified in the two studies are visualised in Fig.\ \ref{fig:humlikedifferences} which uses the same condition order as the box plot and shows which conditions were found to be rated significantly above or below which other conditions.

\subsection{Analysis and results of appropriateness studies}
We gathered a total of 8,867 responses for the full-body study and 10,910 responses from the upper-body study that were included in the analysis.
Every condition received at least 873 responses in the full-body study and 983 in the upper-body study.
\begin{figure*}[t!]
\centering%
%\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/full-body_appropriateness_matched_pref.pdf}
    \caption{Full-body}
    \label{sfig:fbappropbars}
  \end{subfigure}
\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/upper-body_appropriateness_matched_pref.pdf}
    \caption{Upper-body}
    \label{sfig:ubappropbars}
  \end{subfigure}
%\hfill
\caption{Bar plots visualising the response distribution in the appropriateness studies. The blue bar (bottom) represents responses where subjects preferred the matched motion, the light grey bar (middle) represents tied (``They are equal'') responses, and the red bar (top) represents responses preferring mismatched motion, with the height of each bar being proportional to the fraction of responses in each category. The black horizontal lines bisecting the light grey bars represent the proportion of matched responses after splitting ties, each with a 0.05 confidence interval. The dotted black line indicates chance-level performance. Conditions are ordered by descending preference for matched motion after splitting ties.}
\label{fig:appropbars}
\Description{The first stacked bar chart shows the preference of matched versus mismatched full-body motion for 10 conditions. The conditions are sorted in descending order, based on the preference for the matching condition. The order is FNA, FSH, FSA, FSI, FSG, FSB, FSC, FSF, FBT, and FSD. For FNA, the blue, gray, and red regions take about 65\%, 15\%, and 20\%, respectively. The second tacked bar chart shows the preference of matched versus mismatched upper-body motion for 11 conditions. The descending order of preference for the matching motion is UNA, USQ, USM, USL, UBA, USO, USK, USJ, USN, USP, and UBT. For UNA, the blue, gray, and red regions take about 70\%, 10\%, and 20\%, respectively.}
\end{figure*}
\begin{figure*}[t!]
\centering%
%\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/full-body_joint_plot.pdf}
    \caption{Full-body}
    \label{sfig:fbjpoint}
  \end{subfigure}
\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering%
    \includegraphics[width=\textwidth]{figures/aspect2to1/upper-body_joint_plot.pdf}
    \caption{Upper-body}
    \label{sfig:ubjoint}
  \end{subfigure}
%\hfill
\caption{Joint visualisation of the evaluation results for each tier. Box widths show 95\% confidence intervals for the median human-likeness rating and box heights show 95\% confidence intervals for the preference for matched motion in percent, indicating appropriateness.}
\label{fig:joint}
\Description{There are two plots, one for the full-body tier and one for the upper-body tier. In both plots, human-likeness is on the x-axis, and appropriateness (in percentages) on the y-axis.  In the first plot for full-body, the FNA box is in the top right corner, which means high ratings on human-likeness and appropriateness. The FSA box is on the right side. FSC and FSI are on the middle bottom. The other boxes are in the bottom left corner. In the upper-body plot, the UNA box is in the top right corner. The USQ box is on the right side (further right than UNA). The other boxes are in the bottom left corner (around 55 on the y-axis, spread between 20 and 55 on the x-axis).}
\end{figure*}
Raw response statistics for all conditions in each of the two studies are shown in the second half of Table\ \ref{tab:stats}, together with 95\% Clopper-Pearson confidence intervals for the fraction of time that the matched video was preferred over the mismatched, after dividing ties equally between the two groups (rounding up in case of non-integer counts).
The confidence intervals were rounded outward to ensure sufficient coverage.
The response distributions in the two studies are further visualised through bar plots in Fig.\ \ref{fig:appropbars}, while Fig.\ \ref{fig:joint} visualises the results of the entire challenge in a single coordinate system per tier.

Unlike the human-likeness studies, the responses in the appropriateness studies are restricted to three categories and do not necessarily come in pairs for statistical testing in the same way as for the parallel sliders in HEMVIP.
A different method for identifying significant differences therefore needs to be adopted.
We used Barnard's test \cite{barnard1945new} to identify statistically significant differences at the level $\alpha=0.05$ between all pairs of distinct conditions, applying the Holm-Bonferroni method \cite{holm1979simple} to correct for multiple comparisons as before.
(Here and forthwith, we only consider the relative preference in the sample after dividing ties equally.)
Barnard's test is considered more appropriate than Fisher's exact test for a product of two independent binomial distributions \cite{lydersen2009recommended}, as here.

Our statistical analysis found 13 of 45 condition pairs to be significantly different in the full-body study and 10 out of 55 condition pairs to be significantly different in the upper-body study.
Specifically, FNA/UNA were significantly more appropriate for the specific speech signal compared to all other, synthetic conditions.
In addition, FSH was significantly more appropriate than FBT, FSC, FSD, and FSF in the full-body study.
%As before, the significant differences we identified in the two studies are visualised in Fig.\ \ref{fig:humlikedifferences} which uses the same condition order as the box plot and shows which conditions were found to be rated significantly above or below which other conditions.
No other pairwise differences were statistically significant in either study.
%
\begin{table*}[!t]
\centering%
\caption{Objective evaluation results.
The word ``acceleration'' has been abbreviated to ``accel.'';
$\pm$ shows the standard deviation per sequence.
The best two or three numbers in each column, i.e., those closest to the numbers from the held-out motion-capture data (FNA/UNA, first row of values), are bold.
Except for FNA/UNA, conditions (rows) are ordered by decreasing median human-likeness rating.
%Condition labels in green indicates one of the best-performing systems in terms of human likeness, while red colour marks the worst-performing systems interms to the human-likeness user study) for easier comparison.
Numbers have generally been rounded to three significant digits.
%Note no strong relation between subjective and objective measures.
}
\label{tab:obj_stats}
%\hfill
%\hfill
\begin{subtable}[t]{0.49\textwidth}
\centering%
\caption{Full-body}%
\label{stab:fb_obj}%
\small%
\begin{tabular}{@{}l|ccccc@{}}
\toprule
 & Average & Average & Global & Hellinger & FGD \\
Condition & jerk & accel. & CCA & distance & \\
\midrule
                              FNA   &     \tablebf{31300 $\pm$ 6590} & \tablebf{798 $\pm$ 208} & \tablebf{1\hphantom{.000}}    & \tablebf{0\hphantom{.000}}     & \tablebf{\hphantom{0}0\hphantom{.00}}\\
\midrule
{%\color[HTML]{38761D}
{FSA}} &            14600 $\pm$ 2970  & \tablebf{668 $\pm$ 161} &         0.849  & \tablebf{0.041} &             \tablebf{3.18}           \\
{%\color[HTML]{38761D}
{FSC}} & \hphantom{0}5130 $\pm$ 2120  &         332 $\pm$ 129  &         0.818  &         0.125  &                    16.4\hphantom{0} \\
{%\color[HTML]{38761D}
{FSI}} & \hphantom{0}7370 $\pm$ 1710  &         345 $\pm$ \hphantom{0}98   &         0.789  &         0.111  & \tablebf{\hphantom{0}4.87}           \\
                              FSF   &    \tablebf{22600 $\pm$ 6240} & \tablebf{666 $\pm$ 223} & \tablebf{0.916} &         0.195  &         \hphantom{0}7.49            \\
                              FSG   & \hphantom{0}5560 $\pm$ 2380  &         282 $\pm$ 127  & \tablebf{0.992} & \tablebf{0.060} &                    10.1\hphantom{0} \\
                              FSH   & \hphantom{0}8630 $\pm$ 2440  &         313 $\pm$ \hphantom{0}92   & \tablebf{0.968} &         0.104  & \tablebf{\hphantom{0}4.02}           \\
                              FSD   & \hphantom{0}8690 $\pm$ 8320  &         405 $\pm$ 257  &         0.886  &         0.132  &                    43.4\hphantom{0} \\
{%\color[HTML]{990000}
{FSB}} &    \tablebf{27200 $\pm$ 4680} & \tablebf{628 $\pm$ 116} &         0.782  & \tablebf{0.050} &                    16.3\hphantom{0} \\
{%\color[HTML]{990000}
{FBT}} & \hphantom{0}3510 $\pm$ 1090  &         177 $\pm$ \hphantom{0}56   &         0.738  &         0.267  &                    28.6\hphantom{0} \\
%\noalign
% \cr
\bottomrule
% \cr
\end{tabular}
\end{subtable}%
\hfill\hfill
%\vspace{8pt}
\begin{subtable}[t]{0.49\textwidth}
\centering%
\caption{Upper-body}%
\label{stab:up_obj}%
\small%
\begin{tabular}{@{}l|ccccc@{}}
\toprule
 & Average & Average & Global & Hellinger & FGD \\
Condition & jerk & accel. & CCA & distance & \\
\midrule
                               UNA   &   \tablebf{33000 $\pm$ \hphantom{0}7030} & \tablebf{842 $\pm$            222} & \tablebf{1\hphantom{.000}} & \tablebf{0\hphantom{.000}} & \tablebf{\hphantom{00}0\hphantom{.00}}         \\
\midrule
{%\color[HTML]{38761D}
{USQ}} &   \tablebf{15400 $\pm$ \hphantom{0}3190} & \tablebf{710 $\pm$            173} &                    0.685   &           \tablebf{0.043}  &           \tablebf{\hphantom{00}2.84}          \\
{%\color[HTML]{38761D}
{USJ}} & \hphantom{0}8280 $\pm$ \hphantom{0}1460  &          375 $\pm$ \hphantom{0}81  &                    0.640   &                    0.197   &           \tablebf{\hphantom{00}4.83}          \\
{%\color[HTML]{38761D}
{USO}} & \hphantom{0}5450 $\pm$ \hphantom{0}2260  &          353 $\pm$            138  &                    0.812   &                    0.129   &                    \hphantom{0}16.4\hphantom{0}\\
                               USN   & \hphantom{0}7510 $\pm$ \hphantom{0}3400  &          384 $\pm$            127  &                    0.789   &                    0.092   &                               194\hphantom{.00}\\
                               USK   & \hphantom{0}8180 $\pm$ \hphantom{0}2450  &          311 $\pm$ \hphantom{0}99  &           \tablebf{0.962}  &                    0.137   &                    \hphantom{0}15.5\hphantom{0}\\
                               USM   & \hphantom{0}6840 $\pm$ \hphantom{0}3200  &          385 $\pm$            172  &           \tablebf{0.991}  &           \tablebf{0.039}  &           \tablebf{\hphantom{00}2.17}          \\
                               UBT   & \hphantom{0}3760 $\pm$ \hphantom{0}1170  &          190 $\pm$ \hphantom{0}60  &                    0.707   &                    0.248   &                    \hphantom{0}18.2\hphantom{0}\\
                               UBA   &   \tablebf{18000 $\pm$            14900} &          513 $\pm$            326  &           \tablebf{0.964}  &                    0.244   &                    \hphantom{0}17.0\hphantom{0}\\
{%\color[HTML]{990000}
{USP}} &   \tablebf{28500 $\pm$ \hphantom{0}4960} & \tablebf{661 $\pm$            123} &                    0.769   &           \tablebf{0.051}  &                    \hphantom{0}18.0\hphantom{0}\\
{%\color[HTML]{990000}
{USL}} & \hphantom{0}7730 $\pm$ \hphantom{0}5420  &          258 $\pm$            157  &                    0.849   &                    0.306   &                    \hphantom{0}28.4\hphantom{0}\\
\bottomrule
\end{tabular}
\end{subtable}%
%\vspace{-1\baselineskip}
\end{table*}%
%
\begin{table*}[!t]
\caption{Rank correlations (Kendall's $\tau$) between the ``error'' in the objective metrics (how much each objective value differed from the reference FNA/UNA) and median human-likeness scores (here abbreviated ``Hum.'') or -- only for CCA -- the preference for matched motion after splitting ties (abbreviated ``App.'').
A strong predictor of human scores will exhibit a $\tau$-value close to negative unity combined with a low $p$-value.}
    \begin{subtable}[t]{\columnwidth}
        \centering%
        \caption{Full-body}
        \small%
        \begin{tabular}{@{}l|cccccc@{}}
        \toprule
        Metric & Average & Average & \multicolumn{2}{c}{Global} & Hellinger & FGD \\
        & jerk & accel. & \multicolumn{2}{c}{CCA} & distance & \\
        Versus & Hum. & Hum. & Hum. & App. & Hum. & Hum. \\
            \midrule
            $\tau$ & $-0.09$ & $-0.36$ & $-0.36$ & $-0.38$ & $-0.36$ & $-0.49\hphantom{0}$ \\
            $p$-value & $\hphantom{-}0.72$ & $\hphantom{-}0.15$ & $\hphantom{-}0.16$ & $\hphantom{-}0.15$ & $\hphantom{-}0.15$ & $\hphantom{-}0.048$ \\
            \bottomrule
        \end{tabular}
    \end{subtable} 
\hfill\hfill
    \begin{subtable}[t]{\columnwidth}
        \centering%
        \caption{Upper-body}
        \small%
        \begin{tabular}{@{}l|cccccc@{}}
         \toprule
         Metric & Average & Average & \multicolumn{2}{c}{Global} & Hellinger & FGD \\
         & jerk & accel. & \multicolumn{2}{c}{CCA} & distance & \\
            Versus & Hum. & Hum. & Hum. & App. & Hum. & Hum. \\
            \midrule
            $\tau$ & $-0.11$ & $-0.26$ & $0.11$ & $-0.49\hphantom{0}$ & $-0.40\hphantom{0}$ & $-0.51\hphantom{0}$ \\
            $p$-value & $\hphantom{-}0.64$ & $\hphantom{-}0.27$ & $0.64$ & $\hphantom{-}0.041$ & $\hphantom{-}0.085$ & $\hphantom{-}0.029$ \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    %\hfill\hfill
\label{tab:rankcorr}
%\vspace{-1\baselineskip}
\end{table*}

Instead of comparing the appropriateness of different synthesis approaches against one another, one may instead compare to a random baseline (50/50 performance), and test if the observed effect size is statistically significantly different from zero.
We can assess this at the 0.05 level by checking whether or not the confidence interval on the effect size overlaps with chance performance.
From this perspective, FSA, FSB, FSG, FSH, FSI are significantly more appropriate than chance in the full-body study, and all systems except UBT are more appropriate than chance in the upper-body study.
Unlike other significance tests in the subjective evaluation, these assessments do not include a correction for multiple comparisons.



\subsection{User comments}
As part of the post-evaluation questionnaire, we asked study participants to comment on the user studies, including positive and negative aspects they perceived.
97\% of the respondents in the user studies responded positively on whether the compensation was adequate.
Additionally, they often commented positively on how interesting and engaging the study was.

We also asked participants regarding any negative aspects of the study.
Here, 15\% of the participants answered that they found repetitiveness a negative aspect of the study.
Some users pointed at the lack of a proper human face on the humanoid, and suggested incorporating that in future work.
Others commented on the lack of real conversation, and proposed to have the humanoid be part of an actual conversation.
All responses to these questions can be found in our data release.

\subsection{Objective evaluation results}
\label{ssec:objectiveresults}
The values of the objective metrics we computed are listed in Table\ \ref{tab:obj_stats}. 
For each number in the table, we also calculated how much it differed from the corresponding value for the reference system (FNA/UNA), and then computed the rank correlation between the absolute value of these differences and the median human-likeness scores from the subjective evaluation.
The idea is that systems exhibiting values closer to FNA/UNA should appear more human-like.
The resulting rank correlations and $p$-values can be found in Table \ref{tab:rankcorr}.
For median human-likeness, we only found a statistically significant ($p<0.05$) rank correlation with FGD, for both the full and upper-body tier (Kendall's $\tau=-0.49$ and $-0.51$, respectively).
The negative sign is expected, since a smaller difference from FNA/UNA should be associated with better-looking motion and higher human-likeness scores.
Fig.\ \ref{fig:objectivemetric} visually compares the subjective human-likeness ratings and objective metric results.

CCA is the only metric we computed that can indicate appropriateness, since it directly compares each generated sequence to the corresponding reference motion-capture poses.
We therefore computed its rank correlations with the appropriateness data as well.
Here we found a statistically significant effect ($\tau=-0.49$) for the upper-body tier, but not for the full body.
%
\begin{figure*}[!t]
\centering
\includegraphics[trim={1cm 1cm 1cm 1cm},clip,width=\textwidth]{figures/objective_metric_scatter_plot_v2.pdf}
\caption{Scatterplots comparing objective metrics and human-likeness ratings. The first row is for the full-body tier and the second row is for the upper-body tier.
The $x$-axis shows the absolute magnitude of the difference between the objective value for each system and the corresponding value for the reference motion FNA/UNA, with the scale reversed such that the systems most similar to the reference are on the right.
%The $x$-axis represents the absolute value of the difference between each value and human motion's value for each metric (lower is better; human motion FNA and UNA are always zero and they are on the right end because the $x$-axis is reversed).
Regression lines (from the Theil-Sen regressor \cite{Theil1992,sen1968estimates}, which is robust to outliers) are also shown. The last plot in the second row is for FGD but with a narrower $x$-axis range for a better view.}
\label{fig:objectivemetric}
\Description{}
\vspace{-0.5\baselineskip}
\end{figure*}

\section{Discussion}
\label{sec:discussion}
We now discuss our results and how they may be interpreted, first for human-likeness (in Sec.\ \ref{ssec:humlikecomments}), then for appropriateness (in Sec.\ \ref{ssec:appropriatenesscomments}), and then for the objective metrics (in Sec.\ \ref{ssec:obj_discussion}).
We connect our discussion of each part to the other evaluations we performed and to previous literature.
Based on our findings, we then formulate a number of take-home messages regarding what matters most in gesture generation (in Sec.\ \ref{ssec:whatlearnt}) and give examples of how materials from the challenge can be used by the field (in Sec.\ \ref{ssec:materials}).

\subsection{Discussion of human-likeness results}
\label{ssec:humlikecomments}
Generating convincingly human-like gestures is a difficult problem, and nearly all conditions rated significantly below natural motion capture.
However, each tier contains an entry which is rated significantly above the motion from the motion-capture recordings in terms of human-likeness.
This is a leap forwards compared to GENEA 2020, and we believe it represents a motion quality not before seen in large-scale evaluations.
Although there has been work, specifically \citet{rebol2021passing}, that reported a proposed motion-generation method as being statistically not significantly different from natural motion, they only evaluated a single method and their study was not based on motion-capture data but on 3D pose estimation from monocular video.
We think that that choice of data source restricted the motion quality of their natural-motion condition to be less convincing (and thus a weaker baseline) than our reference-motion conditions FNA/UNA.
Furthermore, all differences between natural and synthetic conditions are significant in our study.

\subsubsection{Interpreting the high scores of FSA and USQ}
Despite conditions FSA and USQ being rated above the corresponding natural reference motion, we caution that this does not mean that the motion is ``superhuman'', or even completely human-like -- indeed, the median rating is much below 100, which would constitute ``completely human-like'' as per our explicit instructions to test takers.
What the result means is rather that the visualised motion in the majority of cases was perceived as more human-like than the
%motion-capture in the database, specifically than the
motion-capture data used for FNA/UNA in the subjective evaluation.
In making this distinction, it is important to keep in mind that our human-likeness evaluation is constrained by several factors.
Most notably, the nominally natural motion is constrained by our ability to accurately capture the entire range of human motion, especially the fingers, using the technology we used.
Finger motion capture is very difficult, and dataset limitations meant that the finger motion could not be chosen so as to look completely natural in all test segments evaluated, potentially degrading the ratings of FNA/UNA as a result.
An artificial system might have its training data cleaned of problematic instances, so as to prevent it from generating such motion, giving it an edge over FNA/UNA.
This is in fact what was done for systems FSA and USQ, which only used selected training-data segments, manually chosen to have high motion quality, in generating their output gestures \cite{anon2022gesturemaster}.

Our ability to visualise human characters and their motion also plays a role in our findings.
The use of a deliberately neutral 3D avatar lacking potentially distracting human features such as gaze and lip motion significantly reduces the bandwidth of the communication channel to the user, which lowers the threshold for what needs to be achieved in order to match human motion ratings in the evaluation.
If the challenge had involved generating additional modalities such as gaze and facial expression, the shortcomings of artificial systems may have become more clear, at the expense of increased complexity when running and taking part in the challenge.
%In addition, the greater interquartile range of ratings of UNA compared to FNA could mean that the process of imposing full-body motion from a walking and talking human onto an avatar with fixed lower body may not always yield completely natural results.
%Future GENEA Challenges intend to only consider full-body motion.

\subsubsection{On the differences between the two tiers}
There are fewer significant differences in the full-body evaluation than in the upper-body evaluation, perhaps meaning that full-body motion is more difficult to rate consistently.
Although the difference is not substantial, we would naively expect the opposite, due to the correction for multiple comparisons being more conservative in the upper-body evaluation.
There are many possible explanations for this finding, beyond the fact that the different teams did not all participate in both tiers.
For example, our finding is consistent with an interpretation that full-body motion is a more difficult machine-learning problem, for instance due to increased dimensionality of the output space and the increased number of behaviours that need to be learnt.
This could explain why the best entry in the upper-body evaluation more clearly outperformed UNA, compared to the margin between the best entry in the full-body evaluation and FNA.

Another possible explanation for the same result is that the process of imposing full-body motion from a walking and talking human onto an avatar with a fixed lower body may not always yield completely natural results, and could sometimes give rise to incongruous motion.
This could also explain the wider span (greater interquartile range) of ratings of UNA compared to FNA.
Future GENEA challenges intend to only consider full-body motion.

\subsection{Discussion of appropriateness results}
\label{ssec:appropriatenesscomments}
We find the results of the appropriateness evaluation both thought-provoking and revealing about the state of the field.
To begin with, the greatest relative preference, a 75\% preference for matched motion, was observed for natural motion capture, i.e., FNA/UNA.
This +25\% effect size over the 50/50 bottom line validates that our methodology can well identify when gestures are appropriate for the speech and is about half the theoretical maximum value of +50\% (a 100/0 split).
A +25\% effect size should be considered a good result, since previous studies that have incorporated mismatched stimuli, e.g., \citet{jonell2020let,rebol2021passing}, have found that they sometimes are difficult for participants to distinguish from matched ones, especially if they -- like here -- both correspond to segments where the character is speaking (and do not, say, match audio of active speaking with a segment of motion corresponding to the character listening without speaking).
Furthermore, both matched and mismatched motion stimuli here have their starting points aligned to the start of a phrase in the speech, meaning that the motion in the stimulus videos might initially be more similar to each other than if the mismatched motion had been excerpted completely at random and not aligned to the start of phrase boundaries.
%Seen in this light, the 75\% preference for matched motion (after splitting ties) in FNA/UNA is a good result.
%The effect size of +25\% is half of the theoretical maximum.
It is therefore not surprising to find that the preference for matched motion over mismatched motion is not larger for FNA/UNA.

In line with expectations, no system has a relative preference for matched motion below 50\%, which is the theoretical bottom line, attained by a system whose motion has no relation to the speech.
However, the synthetic conditions are all far behind natural human motion in terms of appropriateness.
The measured effect sizes over the 50/50 bottom line range from +10\% and down to 1.5\% for all these conditions, compared to +25\% for FNA/UNA, and all differences compared to FNA/UNA are highly statistically significant.
This is a very substantial gap, and it is clear that generating meaningful and appropriate gestures is still far from a solved problem.

One other interesting trend is that a few conditions with relatively poor human-likeness, specifically FBT, UBT, and USL, show a noticeably larger fraction of tied responses, compared to other conditions.
We hypothesise that this could be due to underarticulated motion, noting that a hypothetical, extremely underarticulated system that does not move at all should receive the response ``They are equal'' all the time.
This hypothesis is consistent with the fact that these conditions all had the three lowest average acceleration values in Table \ref{tab:obj_stats}, indicating little motion overall.



\subsubsection{Comparison to the human-likeness studies}
\label{sssec:discussappvshumlike}
Compared to the results for the human-likeness studies, we did not find as many differences between the submissions in terms of appropriateness.
%, and no system came close to the performance of natural motion capture.
We can envision four factors that could contribute to this, which we list below, along with thoughts regarding potential mitigations:
\begin{itemize}
\item Responses are confined to much fewer categories, meaning that each response provides less information in an information-theoretic sense. This could potentially be addressed by having test-takers complement their response with an indication of the strength of their preference.
%That would be similar to what is done in standards of audio testing that are designed to be especially sensitive to small differences \cite{itu2015methods}, relative to the HEMVIP methodology and the standards that underpin it \cite{jonell2021hemvip,itu2015method}.
We recommend that future developments in evaluation consider using a preference scale with more response options, e.g., five or seven possible responses.
\item Unlike the HEMVIP-based human-likeness studies, the responses to the appropriateness studies were not analysed using pairwise statistical tests to control for variation between subjects and stimuli. This might have led to reduced resolving power. It might be possible to improve on the statistical analysis using, e.g., log-linear mixed effects models to account for the effects of different test takers and different videos, or by changing the study setup to allow for pairwise statistical testing. One can furthermore gather more responses per condition, which we recommend in case the same testing methodology is used.
\item Assessing appropriateness may be a more difficult task for humans than assessing human-likeness, meaning that there is more random variation in the responses relative to the human-likeness studies. In a signal-to-noise analogy, this means that the noise is higher. Mitigating this would probably require changing the evaluation and its task. For example, differences might become more obvious if segments were mismatched completely randomly, such that speech sometimes would be paired with motion from a segment where the character is not actively speaking, and vice versa, although doing so would essentially change the type of appropriateness that is being assessed.
\item It may simply be that current artificial systems struggle to generate motion that is particularly appropriate to any specific input speech. In other words, in a signal-to-noise analogy, the signal is weaker. Consequently, there is less of a difference to be uncovered in the first place.
\end{itemize}
Although all of these factors may contribute to the results we observe, the big gap in effect size between natural motion capture and synthetic motion,
%(+25\% for natural motion versus +10\% or less for synthetic motion),
and the fact that FNA/UNA were very significantly better than all other conditions, shows that our methodology is sufficiently accurate to clearly resolve important differences between conditions.
From Fig.\ \ref{sfig:ubjoint}, we can furthermore see that
%, unlike the GENEA Challenge 2020,
there is no strong correlation between the human-likeness ratings and the appropriateness ratings in the evaluation.
This supports a conclusion that our evaluation successfully disentangled these two aspects of gesture motion.
%This supports an interpretation that -- in contrast to human-likeness -- data-driven co-speech gesture generation still has a long way to go to come close to the speech appropriateness and specificity that human gesticulation has.

%The methodology we demonstrate also has other advantages, since it does not involve subjects making any direct comparisons between videos generated by different conditions, compared to the method used by \citet{anon2021genea}.
In addition to its strong ability to control for the effect of motion quality, our method for assessing appropriateness only requires comparing a system to itself.
We believe this feature may enable direct comparison between different studies on the same data, \emph{without} having to include the various other synthetic baseline conditions in the new user study.
Seeing that creating appropriate baseline systems is one of the sticking points both for carrying out research and for its subsequent assessment in peer review, this can be a major simplification compared to parallel methodologies like HEMVIP \cite{jonell2021hemvip} that involve simultaneously comparing and evaluating many different conditions against each other.
Since responses in those studies are affected by what other videos are shown on the same page, their results thus cannot be directly compared unless stimuli or implementations of previous synthetic baseline conditions are included in the new study.
%
%\subsection{Miscellaneous remarks}
%In addition to its strong ability to control for the effect of motion quality, our new method for assessing appropriateness only requires comparing a system to itself.
%This makes it easy to use and track progress on different sets of stimuli without having to train any baseline systems for the comparisons. This could be advantageous for future benchmarking purposes, since creating appropriate baseline systems is one of the sticking points both for carrying out research and for its subsequent assessment in peer review.
Our recommendation for future research that uses the same methodology in this paper is to report effect size and $\alpha=0.05$ Clopper-Pearson confidence intervals similar to Table\ \ref{tab:stats}, to enable easy and accurate comparison between studies.

\subsubsection{Comparison to other gesture-appropriateness assessments}
Overall, the distribution in Fig.\ \ref{fig:appropbars} of the three different responses across the different conditions is similar to that seen in the mismatching study reported in \citet{jonell2020let}, which used a similar methodology.
On the other hand, we see fewer statistical differences compared to the appropriateness study in GENEA 2020 \cite{anon2021genea}, which asked participants to rate the appropriateness of the stimuli on an absolute scale using HEMVIP.
However, the ratings in that study were strongly biased towards conditions with high human-likeness, as discussed in Sec.\ \ref{sec:approp}, evidenced by the fact that mismatched natural motion (M) scored second best in terms of appropriateness there.
In effect, we have traded the high-resolution, high-bias method from GENEA 2020 for a reduced-resolution, low-bias method.
We think this is a step forward, since most prior evaluations of gesture appropriateness for speech have been highly confounded by motion quality, whereas our methodology is not.
%The fact that some synthetic conditions that distinguished themselves the most in terms of appropriateness, namely FSH and USM, exhibited middle-of-the-pack human-likeness, highlights our success in disentangling motion appropriateness from motion quality.


\subsection{Discussion of objective metrics}
\label{ssec:obj_discussion}
The values of each of the the six objective metrics in Table \ref{tab:obj_stats} span a wide range.
From the acceleration and jerk values, we can observe that some systems, e.g., the text-based baselines from \citet{yoon2019robots}, exhibit much less movement than others.
Unfortunately, most objective metrics are not well aligned with subjective human-likeness scores. 
In the full-body tier, one of the least human-like systems, FSB, received some of the best scores in terms of average absolute jerk, acceleration, and Hellinger distance.
At the same time, one of the most human-like systems, FSC, is not in the top three according to any of the objective metrics used.
In the upper-body tier, one of the least human-like systems, USP, was in the top three systems according to average jerk, acceleration, and Hellinger distance while one of the most human-like systems, USO, is not in the top three according to any of the objective metrics.
The rank correlations in Table \ref{tab:rankcorr} make these observations more precise, by showing that most correlations are not statistically significantly different from zero.
The one exception is the FGD.
Although the correlations we found there are moderate (around $-0.5$) and system USN shows an outlying value, this metric nonetheless might have some potential as an objective evaluation metric useful for faster evaluation in the development phase, although it is not clear how well it will resolve smaller differences between systems.

As for speech appropriateness, only the CCA metric takes reference motion into account and thus has any possibility to measure this aspect.
The CCA results are not clear-cut, but nonetheless somewhat encouraging, seeing that the systems with the best appropriateness (namely FSH, USQ, and USM) also exhibit some of the highest CCA values, of 0.96 and above, and we found a statistically significant correlation for one of the tiers.

All in all, we want to emphasise that objective evaluation of generated gestures
%assessing gesture generation through objective evaluation is still challenging.
is still an open problem.
Subjective evaluation, as used by this challenge, remains the gold standard for comparing gesture-generation models \cite{wolfert2021review}, and none of the objective evaluation metrics can replace subjective user studies.
%, and the aim of GENEA is to carry out such evaluations at a large scale that controls for the effects of the data and visualisation used.



\iffalse
\begin{table*}[!t]
\centering%
\caption{Objective evaluation results. Bold indicates the best metric, i.e., the one closest to the ground truth (UNA/UBA). The green colour of the system name indicates one of the best-performing systems, while the red colour indicates the worst-performing systems (according to the human-likeness user study) for easier comparison. 
%Note no strong relation between subjective and objective measures.
}
%\label{tab:obj_stats}
%\hfill
\begin{subtable}{\textwidth}
\centering%
\caption{Full-body}%
\label{stab:fb_obj}%
\begin{tabular}{@{}l|ccccc}
\toprule
Condition                           & Average jerk                                         & Average acceleration      & \multicolumn{1}{l}{Global CCA} & \multicolumn{1}{l}{Hellinger distance average} & FGD \\
\midrule
 FNA                                 & \textbf{31324.43 $\pm$ 6588.19} & \textbf{797.53 $\pm$ 207.71}                & \textbf{1}            & \textbf{0}                            & \textbf{0} \\
{\color[HTML]{990000} \textbf{FBT}}                         & 3504.05 $\pm$ 1089.93                    &  177.31 $\pm$ 56.01           & 0.73848                        & 0.2670497593                                   & 28.645                      \\
{\color[HTML]{38761D} \textbf{FSA}}                         & 14598.06 $\pm$ 2970.64                   &  \textbf{668.44 $\pm$ 160.97} & 0.84948                        & \textbf{0.04096112013}                         & \textbf{3.175}                      \\
 {\color[HTML]{990000} \textbf{FSB}} & \textbf{27160.94 $\pm$ 4679.38}          & \textbf{628.07 $\pm$ 115.74}                         & 0.78182                        & \textbf{0.04952276147}                         & 16.266                      \\
{\color[HTML]{38761D} \textbf{FSC}}                         & 5129.45 $\pm$ 2116.81                    &  332.25 $\pm$ 129.43          & 0.81826                        & 0.1252592381                                   & 16.366                      \\
FSD                                                         & 8691.69 $\pm$ 8317.16                    &  405.42 $\pm$ 256.97          & 0.88646                        & 0.1323802021                                   & 43.434                      \\
FSF                                                         & \textbf{22628.91 $\pm$ 6241.05}          &  \textbf{666.02 $\pm$ 223.34} & \textbf{0.91574}               & 0.1945411681                                   & 7.490                      \\
FSG                                                         & 5564.40 $\pm$ 2383.01                    &  282.23 $\pm$ 127.24          & \textbf{0.99154}               & \textbf{0.05967038642}                         & 10.057                      \\
FSH                                                         & 8632.45 $\pm$ 2436.03                    &  312.80 $\pm$ 92.41           & \textbf{0.968}                 & 0.1036145704                                   & \textbf{4.024}                      \\
{\color[HTML]{38761D} \textbf{FSI}}                         & 7373.95 $\pm$ 1711.13                    &  345.36 $\pm$ 97.74           & 0.78944                        & 0.1106242683                                   & \textbf{4.865}                      \\
  
%\noalign
% \cr
\bottomrule
% \cr
\end{tabular}
\end{subtable}%
\\\hfill\hfill
\vspace{8pt}
\begin{subtable}{\textwidth}
\centering%
\caption{Upper-body}%
\label{stab:up_obj}%
\begin{tabular}{@{}l|ccccc}
\toprule
Condition                           & Average jerk                                         & Average acceleration      & \multicolumn{1}{l}{Global CCA} & \multicolumn{1}{l}{Hellinger distance average} & FGD \\
\midrule
UNA                                 &  \textbf{32944.24 $\pm$ 7026.29} & \textbf{841.66 $\pm$ 221.81} & 1                              & 0                                              & 0    \\ 
 UBA         & \textbf{17988.24 $\pm$ 14888.54}                        & 513.34 $\pm$ 326.34          & \textbf{0.96406}               &  0.244                  & 16.952    \\
 UBT         & 3763.61 $\pm$ 1170.66                                   & 190.45 $\pm$ 60.16           & 0.70727                        &  0.248                  & 18.191    \\
{\color[HTML]{38761D} \textbf{USJ}} &  8217.57 $\pm$ 1460.65           & 374.99 $\pm$ 81.10           & 0.64012                        &  0.197                  & \textbf{4.834}    \\
USK                                 &  8176.04 $\pm$ 2450.19           & 310.59 $\pm$ 98.75           & \textbf{0.96207}               &  0.137                  & 15.472    \\
{\color[HTML]{990000} \textbf{USL}} &  7732.52 $\pm$ 5424.75           & 258.02 $\pm$ 156.76          & 0.84934                        &  0.306                  & 28.435    \\
USM                                 & 6841.16 $\pm$ 3200.89                                   & 384.93 $\pm$ 172.24          & \textbf{0.99118}               &  \textbf{0.039}         & \textbf{2.171}    \\
USN                                 &  7512.60 $\pm$ 3401.92           & 384.35 $\pm$ 127.18          & 0.78927                        &  0.092                  & 194.180    \\
{\color[HTML]{38761D} \textbf{USO}} &  5452.21 $\pm$ 2261.74           & 353.27 $\pm$ 138.45          & 0.81219                        &  0.129                  & 16.432    \\
{\color[HTML]{990000} \textbf{USP}} &  \textbf{28547.85 $\pm$ 4962.86} & \textbf{661.09 $\pm$ 122.95} & 0.76928                        &  \textbf{0.051}         & 17.955    \\
{\color[HTML]{38761D} \textbf{USQ}} &  \textbf{15378.19 $\pm$ 3189.86} & \textbf{709.93 $\pm$ 172.85} & 0.68459                        &  \textbf{0.043}         & \textbf{2.841}    \\
\bottomrule
\end{tabular}
\end{subtable}%
\end{table*}
\fi







\subsection{Take-home messages}
\label{ssec:whatlearnt}
In this section we combine salient points from our results and discussion with information that the teams provided about their challenge entries, in order to see what we can learn about what aspects matters most in gesture-generation methods, data processing, and evaluation.
%In this section we discuss what we have learnt about gesture generation modeling and evaluation as well as how to use the materials derived during this challenge.

\subsubsection{What have we learnt about successful gesture generation methods?}

Table \ref{tab:conditions} contains all the submissions with the corresponding system properties, sorted according to their human-likeness scores.
We can note that all systems except the text-based baseline used audio as an input modality, fewer systems used text, and even fewer used speaker IDs.
There seems to be no clear indication that using any given combination of modalities necessarily gives better results than others, as some systems using only audio are on the top and others on the bottom of the list.
However, the fact that so many of them did use audio input suggests a perception among teams that taking audio into account is important.

When it comes to the techniques used, RNNs were the most popular choice and used almost by all the systems, followed closely by auto-regression.
Again, for most of these there seems to be no strong indication that certain choices are necessarily better than others.
Our main, perhaps surprising, observation is that the state-of-the-art in human-likeness is not to use deep learning for everything (or at least not to generate the gesture poses), seeing that the most human-like system, GestureMaster, is based on motion graphs \cite{lee2002interactive,kovar2002motion,arikan2002interactive} and a library of carefully selected high-quality motion segments.
%
\begin{figure}[!t]
    \centering%
    \includegraphics[width=\columnwidth]{figures/TimeSpent_v2.pdf}
    \caption{The number of person-hours different responding teams reported spending on the GENEA Challenge 2022, sorted in ascending order.}
    \label{fig:time_spent}
    \Description{}
    \vspace{-1\baselineskip}
\end{figure}

\subsubsection{What have we learnt about gesture-data processing?}

Fig.\ \ref{fig:time_spent} shows how much time different teams spent on their submissions.
We can see a very high variation, with some teams spending between 40 and 60 person hours whilst some others spent 800 hours or more.
The two teams who spent 800 or more hours on their submissions reported devoting a large amount of time on data pre-processing, which other teams did not.
%(although several other teams did state that they found data processing to be of significant importance).
One of the former teams is the top-performing team in terms of gesture human-likeness scores.
This suggests that spending time on data preparation is likely to pay off in better model performance.
Data processing tasks included cropping the recordings into shorter segments, annotating those short segments for, e.g., motion quality, and similar.
%removing segments with silent pieces in the audio, etc.
Some teams found it important to remove segments where the character was listening rather than talking, since the character exhibits little gesture motion in these segments, which can make deterministic gesture-generation approaches regress towards the mean pose and thus produce less vivid movement.

Another important aspect when it comes to the data is post-processing, such as hip-centering or smoothing (cf.\ \citet{kucherenko2021moving}).
As seen in Table \ref{tab:conditions}, most of the systems (good and bad performance alike) applied motion smoothing in some form.
This suggests that they found smoothing to be beneficial for gesture generation, although the user studies do not allow us to make a statistical conclusion about the importance of smoothing the output motion.

Finally, modelling the motion of the fingers or having them fixed emerged as another important decision.
Roughly half of the systems in the evaluation used fixed fingers.
Some of these systems achieved good performance whilst others did not.
This does not allow us to make strong statistical conclusions about the importance of modelling fingers.
However, we may surmise that finger motion may be especially difficult to make natural, otherwise all teams would presumably have included finger motion in their submissions.
%System FSA/USQ \cite{anon2022gesturemaster} used a quite involved procedure for selecting which finger motion to use separately from the rest of the body.
%This situation can be compared to, say, lower-body motion in the full-body tier, where all teams chose to use a moving rather than fixed lower body.


\subsubsection{What have we learnt about evaluating gesture generation?}

Previous experience shows that it is not easy to disentangle perceived human-likeness from appropriateness as more human-like systems are often ranked as more appropriate \cite{anon2021genea}.
In this challenge we made a concerted effort to disentangle these two aspects.
Specifically, we (1) muted the audio tracks during the human-likeness evaluation, to remove any influence speech may exert on perceived appropriateness (cf.\ \citet{jonell2020let}), and (2) compared each model with a mismatched version of itself (having the same human-likeness), to control for the effect of human-likeness when evaluating appropriateness.
This effort paid off, since the two metrics are essentially uncorrelated for the synthetic conditions in Fig.\ \ref{sfig:ubjoint}.
However, it would be beneficial to improve the statistical resolution of the evaluation procedure.
%Objective metrics should not replace subjective evaluation.
%This suggests a way to disentangle appropriateness and human-likeness.

% Moved the paragraph below to the new "Limitations" subsection

% We considered only the general appropriateness of the gestures for the speech, while there is value in separately evaluating appropriateness towards the speech rhythm and meaning since those are different aspects. We will consider doing that in future challenges, for example by doing two user studies each focused on a separate type of appropriateness: semantic appropriateness and rhythmic appropriateness. %, such as interlocutor speech, stance, behaviour, or gaze.

\subsection{How materials from the challenge can be used}
\label{ssec:materials}
We believe the materials released together with the challenge have many benefits for gesture-generation  research.
To illustrate this, we provide a list of possible use cases, often with references to prior work acting as examples of this kind of data use.
One may, for instance\ldots{}
%The various materials from the challenges may be used to\ldots{}
%All the material derived during the Challenge can be found at  \href{https://youngwoo-yoon.github.io/GENEAchallenge2022/}{youngwoo-yoon.github.io/GENEAchallenge2022}. 
%Here is the list of the different ways those materials can be used in the future research:%, similarly how it was done with the data from the GENEA Challenge 2020:
\begin{itemize}
    \item Benchmark/compare new models to the state of the art using our public data and existing motion or video stimuli, like \citet{ferstl2021expressgesture, yazdian2022gesture2vec} did with previous open stimuli.
    \item Evaluate models using our open-sourced code for the evaluation interface and analyses, similar to the re-use of HEMVIP code from \citet{jonell2021hemvip} by \citet{wolfert2021rate}.
    \item Use our questions and evaluation structure for evaluating new proposed methods, similar to how \citet{teshima2022deep} re-used previous evaluation designs.% same questions in the subjective evaluations,
    %as in \cite{teshima2022deep},
    %with similar approaches to disentangling human-likeness and speech appropriateness \cite{jonell2020let,rebol2021passing}.
    \item Use our public visualisation code to simplify development and obtain more standardised and comparable visuals, similar to prior re-use of open upper-body visualisations in \citet{wang2021integrated, teshima2022deep, zhang2023diffmotion}.
    %from the GENEA Visualiser%, as was done previously for the GENEA Challenge 2020 \cite{wang2021integrated, teshima2022deep}
    %\item Use the GENEA Visualizer for faster development and continual comparisons with the same visualization
    \item Evaluate new models objectively using the same metrics that showed the most promise here, similar to how \citet{ahuja2022low,liang2022seeg,ye2022audio} re-used metrics and sometimes code from \citet{yoon2020speech,ahuja2020no}.% as \cite{bhattacharya2021speech2affectivegestures} did
    %\item Learn a mapping from 3d sequences to subjective evaluation as in \cite{he2022automatic}
    \item Use our large dataset of subjective evaluation responses to build and/or validate new automatic quality-assessment methods, similar to \citet{he2022automatic}, or perform in-depth analyses of human preferences using the individual response data, perhaps linking these to the time taken by study participants, their questionnaire responses, etc.
    \item Use our materials and those released by participating teams to probe reproducibility in the field.
\end{itemize}

\section{Limitations}
\label{sec:limitations}
Despite being a large evaluation with many conditions and raters, there are inevitable limitations to the challenge and its results, imposed by scope, systems, data, visualisation, and evaluation choices.
We discuss some of these limitations below.

\setcounter{subsection}{1}
\subsubsection{Scope and scale}
The ten teams participating in the 2022 challenge do not represent the full spectrum of all gesture-generation approaches available today.
While ten teams (plus the top line and baselines) are more systems than considered in any other joint comparison of gesture-generation systems we are aware of, it is still not large enough to, e.g., make strong conclusions regarding which system architectures to prefer.
We hope to attract more teams to participate in the challenge in future years.

\subsubsection{Data}
Motion capture is a remarkable technology, but does not yet perfectly capture every aspect of human pose and figure.
There are hardware issues such as calibration, and software challenges in estimating humanoid skeletons of various dimensions whilst dealing with problems like reflective marker displacements, occlusion, and markers in close proximity.
%(especially for the fingers).
%Motion capture software is at a remarkable stage in its ability to capture very good-looking animation.
%However, the technology is a complex system which is far from perfect, as a result of which errors are bound to manifest during both the data recording and processing stages.
%On the hardware side, one should consider imperfections in the equipment (i.e., cameras, reflective markers, body suits), as well as hardware calibration and changes in environment (e.g., heat, physical displacement).
%On the software side, algorithms have to make various assumptions in order to estimate a more general humanoid skeleton.
%For example, it has to somehow handle skeletons of various dimensions, imperfections in the equipment used (e.g., reflective marker displacements), and resolving situations when a marker may be occluded (particularly for the fingers).
Together, these issues may lead to problems with the data, commonly seen as artefacts in the produced motions (e.g., twitching or unnatural bone rotations), which may be especially pronounced in the fingers.
Although we have worked to exclude low-quality parts of the data and process it to make it more amenable to deep learning, some artefacts are still present.
We suspect that this is an important reason why generated motion could surpass the notionally natural motion capture in terms of human-likeness.
More, and more high-quality, motion data might allow for generating better gesture motion and comparison to a stronger top line.
%Although we have cleaned the dataset of such occurrences, there are still some motion artefacts present (as well as missing speech audio in some parts).

Some useful information is also missing from the current data.
On the verbal side, this includes speech information removed for anonymisation.
On the non-verbal side, one prominent missing aspect is facial data, which is an important communication channel but was not recorded in the current dataset.
Neither was body form, such as muscle mass, body fat, skin, nor how these deform when muscles flex and extend, since the data has abstracted the humanoid form down to only a skeletal hierarchy.
%We furthermore separated the captured dyads into isolated speakers, to reduce the complexity of the present challenge.
Future challenges should maintain awareness of new datasets being published, and their data quality and modalities captured.
One modality worth investigating further is face motion, as it may help systems learn more appropriate gestures that relate to facial expressions and emotions.

\subsubsection{Visualisation}
The gesture visualisation used in the challenge has several limitations.
Some are dictated by the data, and some are deliberate choices to, e.g., reduce complexity.
The result is a virtual character that, whilst representative of typical gesture-generation visualisations, lacks both skin deformations and many human communication channels, such as gaze, facial expression, and lip motion.
Whilst the absence of such features can help focus attention on the body motion currently being studied, it does also lead to a less human-like overall impression for the character.
Our evaluation also deliberately obscured some aspects of motion, e.g., by clipping the view so as to not show potential foot sliding and (for the upper-body tier) fixing the legs of the virtual character, which is innately unnatural.
%The visualisation of gesture motion is another area in which errors can occur.
%As a start, while form is captured in the virtual character due to it being a 3D mesh, it still does not capture skin deformations, such as due to muscle flexing and extension.
%Furthermore, several communication channels are missing.
%On the intra-level, the character is missing gaze, facial expressions, and lip motion, all of which may convey a significant amount of information to both the interlocutor and the user study participant.
%On the inter-level, the opposing interlocutor is missing entirely.
%This encompasses the above-mentioned communication channels, in addition to other ones such as pose, stance, and body motions, all of which may be somehow related to what the speaker is saying.
%Another point to consider is the fixed legs of the virtual character, which is innately unnatural.
%This is due to it being a post-processing step we applied in order to constrain the character to remain in the camera view at all times.
%Going further, the character is clipped from the knees below, which is also a constraining factor in terms of the communication bandwidth available to the study participant.
We think future challenges should consider incorporating additional communication channels, e.g., facial features on a 3D mesh, to improve the realism of the virtual characters and the produced gestures.
%This would in turn include full-body animations and potentially facial features on a 3D mesh.

Aside from limitations on what agent behaviours are visualised and how, the interlocutor from the recorded conversations is missing entirely in both modelling and visualisation.
This was a deliberate choice to not increase the complexity of the challenge too much, but
%One reason for this might be that the motion originates from a dyadic conversation, but is evaluated without any contextual information about the interlocutor behaviour, e.g., their pose, stance, or motion with respect to the speaker.
the absence of such information prevents us from assessing interlocutor-dependent aspects of motion such as proxemics and behavioural alignment.
(We deliberately excluded turn taking, back channels, and listening behaviour from the subjective evaluation, since these are likely to look odd without seeing both sides of the conversation.)
Future challenges may opt to include information about both conversation parties in the evaluation, so that study participants can be interlocutor-aware in their responses.
However, any increases in complexity, whether due to adding additional inputs or output modalities, should be performed one step at a time, so that it is more clear which findings relate to which aspect of the complex problem that is gesture generation.

\subsubsection{Evaluation}
%This would be a competing explanation to the hypothesis in the previous section of artefacts in UNA due to fixing the lower-body to visualise the recorded motion.
%Other contributing factors could be that the full-body motion contains more behavioural variation, as the character now is moving their legs and changing position in the field of view, perhaps in response to the conversation partner.
%One reason for this might be that the motion originates from a dyadic conversation, but is evaluated without any contextual information about the interlocutor behaviour, e.g., their pose, stance, or motion with respect to the speaker.
%The absence of such information may complicate the assessment of the lower-body motion, which might be more closely linked to stance and proxemics in interactions compared to upper-body motion.
%On the other hand, such an effect seems like it should increase the interquartile range of ratings of FNA, relative to UNA, which is not what was observed.
Our core evaluation only sought to quantify two performance measures, namely subjective human-likeness and perceived appropriateness for the given speech.
Aspects such as gesture diversity, or generation speed and latency, were not measured.
Furthermore, we only studied the overall appropriateness of the gestures for the speech, but there is value in evaluating appropriateness with respect to the speech rhythm and speech meaning separately, since these are distinct aspects.
We will consider doing that in future challenges, for example by performing two user studies, each focused on a separate type of appropriateness: semantic appropriateness and rhythmic/temporal appropriateness.

There are also many other kinds of appropriateness that can be assessed, e.g., appropriateness for the given speaker, and for the interlocutor behaviour as discussed above.
(See the discussion of \emph{grounding} in \citet{nyatsanga2023comprehensive} for a more extensive list.)
None of these were considered in the present challenge, either due to dataset limitations or to keep the complexity to a manageable level.
A difficult but important long term goal is to pursue a more ``ecologically valid'' evaluation, to eventually compare different gesture-generation methods in human interaction, similar to \citet{he2022evaluating}.

\section{Conclusions and implications}
\label{sec:conclusion}
We have hosted the GENEA Challenge 2022 to compare many different gesture-generation methods and assess the state of the art in data-driven co-speech gesture generation for full-body and upper-body avatars.
The central design goals of the challenge were (1) to enable direct comparison between many different gesture-generation methods whilst controlling for factors of variation external to the model, namely data, embodiment, and evaluation methodology, and (2) to disentangle the effects of motion human-likeness and motion appropriateness in the evaluations.

Our evaluation results show that, with the right approach, synthetic motion can attain human-likeness ratings equal or better than the underlying motion-capture data.
This is a big step forward, although most systems did not come close to this level of performance.
The results also suggest that the field is advancing measurably, since most submissions performed significantly better than the previously published baseline methods.
However, using a careful evaluation paradigm, we find that synthetic gestures are much less appropriate for the speech than human gestures, also when controlling for differences in human-likeness.
We are thus only at the beginning of the road when it comes to generating co-speech motion that is appropriate for the specific speech.
Finally, most objective metrics we computed did not exhibit any statistically significant correlations with our subjective human-likeness ratings, with the Fr{\'e}chet gesture distance being the lone exception to the rule.
Objective metrics should thus only be used with great caution.

%The collected results suggest that the field is advancing measurably, since most submissions performed significantly better than the baselines.
%Different systems were also found to be good at different things on the two scales (human-likeness and appropriateness) that we assessed.
%However, a substantial gap remains between synthetic and natural gesture motion, indicating that gesture generation is far from a solved problem.
%Nevertheless, these results illustrate that we currently have the means to be able to generate quite convincing data-driven 3D gesture motion (although attaining that level of quality is something that only few systems are capable of at present), but we are only at the beginning of the road when it comes to generating co-speech motion that is appropriate for the specific speech.

\subsection{Implications}
The challenge findings have implications for both research and practice.
We summarise our perspectives below.
%We attempt to summarise our perspectives on these here.

% Seems useful for a journal
\subsubsection{Implications for practical systems}
If you are building a gesture-generation system and want to reach top-of-the-line human-likeness, you should currently consider using ``playback-based'' methods
like motion graphs \cite{lee2002interactive,kovar2002motion,arikan2002interactive} 
%or like motion matching \cite{buttner2015motion}
as demonstrated by GestureMaster \cite{anon2022gesturemaster}
to generate the pose sequences, instead of relying solely on deep learning to go all the way from input features to motion.
Playback-based systems need less data, and the quality of the motion material is then a higher priority than database size, in contrast to current deep-learning trends.
%a small but carefully vetted motion database might be more important than gathering large amounts of motion data.
Machine-learning is still useful for deciding which gestures to generate (e.g., which motion clips to concatenate).
In all cases, it appears important to spend time on data processing.
%If you are building a gesture-generation system, you can reach near top-of-the-line human-likeness and an appropriateness not far from that of typical synthetic systems, simply by relying on playback of pre-recorded motion, without much (or any) regard for the speech beyond its onset and offset.
%This could save a lot of technical complexity and time, and would allow for scalability by extending the motion database with new gestures as needed (however, doing so may also entail retraining any machine-learning models used).
%It appears especially important to spend time on data processing, to make sure that the data has high quality and only contains human-like motion.

\subsubsection{Implications for research and evaluation}
We believe the challenge adds value to the research community in several ways.
A lot can doubtlessly be learnt from the system-description papers by the participating teams.
The materials we release from the challenge (e.g., time-aligned splits of audio, text, and gesture data;
visualisation; code; and evaluation stimuli and responses) have broad utility for future research, system building, and benchmarking in gesture generation, similar to the community uptake of the resources from the GENEA Challenge 2020.
%
%\subsubsection*{Evaluations}
Furthermore, the methodology we demonstrate for assessing motion appropriateness for speech is much more accurate at controlling for the effect of subjective motion quality and does not involve subjects making any direct comparisons between videos generated by different conditions, which is beneficial for efficient benchmarking against previous publications (see Sec.\ \ref{sssec:discussappvshumlike} for details and recommendations).
%, compared to the method previous challenges used.
%We believe this may enable direct comparison between different studies on the same data, \emph{without} having to include the various other synthetic baseline conditions in the new user study.
%This is a major simplification compared to parallel methodologies like HEMVIP \cite{jonell2021hemvip}, which involve simultaneously comparing and evaluating many different conditions against each other.
%Since responses in those studies are affected by what other videos are shown on the same page, studies thus cannot be directly compared unless stimuli or implementations of previous synthetic baseline conditions are included in the new study.

\subsubsection{Implications for future developments in the field}
Based on the fact that one condition in each tier managed to achieve excellent human-likeness, we expect that, in the medium-term future, gesture-generation systems (at least ones based on motion playback)
%(at least ones leveraging methods such as motion graphs \cite{lee2002interactive,kovar2002motion,arikan2002interactive} or motion matching \cite{buttner2015motion,anon2022gesturemaster})
should be able to advance to more consistently match, or possibly even exceed, motion capture in terms of human-likeness.
Systems that generate poses directly from deep learning are likely to improve in human-likeness as well, as larger datasets with more accurate motion become available (e.g., \citet{liu2022beat}).
This would be similar to recent developments in verbal behaviour generation, where neural language models \cite{brown2020language} and speech synthesisers \cite{shen2018natural,li2019neural} trained on large datasets are approaching the text and speech produced by humans in terms of surface quality (but not necessarily appropriateness).
Gesture generation may be lagging behind due to the relative scarcity of high-quality motion data, compared to text and audio, since accurate motion estimation from monocular in-the-wild video remains a challenging problem.

As the evolution runs its course, we believe that research into appropriate rather than human-like motion is poised to become the new frontier in gesture generation.
There is already evidence that existing deep-learning methods in principle can predict appropriate gestures to generate, even for the hard case of semantically motivated, communicative gestures from speech \cite{kucherenko2022multimodal, kucherenko2021speech2properties2gestures,ao2022rhythmic}.
We also believe that there is great potential for devising better objective metrics, using challenge materials to validate these, and that the adoption of meaningful metrics may further accelerate progress in the field.

\subsubsection{Implications for future challenges}
We think that future challenges should study more difficult scenarios that are farther from being solved, for example full-body motion in dyadic interaction.
That can also provide interesting opportunities for exploring other types of appropriateness, e.g., with respect to the interlocutor stance and behaviour, as studied in \citet{jonell2020let, woo2021development}.
Generating interlocutor-aware full-body gestures might therefore be a focus of the next GENEA Challenge.
This should be coupled with further method development to obtain methodologies for conducting and analysing appropriateness tests with increased resolving power whilst still controlling for motion human-likeness.
In general, challenges like the one described here can play an important part in identifying key factors for generating convincing co-speech gestures in practice, and help drive and validate future progress towards
%the goal of
endowing embodied agents with natural and appropriate gesture motion.


\begin{acks}
The authors wish to thank Meta Research for the data; Carolyn Saund, Axel Johansson, Christianne Sandstig, Leonhard Grosse, Natalia Kalyva, and Natasha Greenwood for the transcriptions; Esther Ericsson for the 3D character; Zerrin Yumak for input; Judith Bütepage, Minsu Jang, Tony Belpaeme, Jieyeon Woo, and Rajmund Nagy for feedback on the manuscript; and Jim Royal for preparing a video presentation.

This research was partially supported by IITP grant no.\ 2017-0-00162 (Development of Human-care Robot Technology for Aging Society) funded by the Korean government (MSIT), by the Flemish Research Foundation (FWO) grant no.\ 1S95020N, by the Portuguese Foundation for Science and Technology grant no.\ SFRH/BD/127842/ 2016, and by the Knut and Alice Wallenberg Foundation, both through Wallenberg Research Arena (WARA) Media and Language -- with in-kind contribution from the Electronic Arts (EA) R\&D department, SEED -- and through the Wallenberg AI, Autonomous Systems and Software Program (WASP).
\end{acks}


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

%%
%% If your work has an appendix, this is the place to put it.

\end{document}