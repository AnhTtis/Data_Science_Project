% !TEX root = main.tex

% \footnotetext[0]{Demo: \href{https://vilda.net/s/mmsg/?uid=demo}{vilda.net/s/mmsg/?uid=demo}\\\text{}\hspace{5.8mm}Code: \href{https://github.com/zouharvi/multimodal-shannon-game}{github.com/zouharvi/multimodal-shannon-game}}

\section{Introduction}

The Shannon Game \citep{shannon1951prediction}\footnote{Not to be confused with Shannon's Switching Game.} is a classic experiment that aims to demonstrate the predictability of the English language.
Originally designed as a method for estimating the perplexity of a language, the game involves asking participants to predict the first letter of a text.
Participants can choose from any of the 26 letters or space, the correct character is revealed and they are asked to guess the next (second) letter, and so on.
% until they make a successful guess. Subsequently, participants are asked to guess the next (second) letter, and so on.
When considering the game at the word level (\Cref{fig:intro_original}), it can be viewed as a variant of greedy autoregressive language modelling.
As with autoregressive language modelling, the Shannon Game can be framed as the task of repeatedly predicting the probability of the next word given the previous context.

% The Shannon Game \citep{shannon1951prediction}\footnote{Not to be confused with the Shannon's Switchting Game.} is a well-known experiment that was designed to demonstrate the predictability of the English language.
% Originally framed as a method for estimating the perplexity of a language, the game consists of asking participants to predict the first letter of a text. The participants are at a liberty to choose from any of the 26 letters and space until they make a successful guess. The guess is then followed up by asking the participant to guess the  next (second) letter and so on. 


% Word-level (\Cref{fig:intro_original}), instead of character-level, Shannon game can be compared with that of autoregressive language modelling.
% Like autoregressive language modelling, the Shannon game could then be framed as a problem of repeatedly modelling the probability of the next word given the words that appeared in the previous context.

% \repl{This is the mechanism of the Shannon game.}{}
% repeatedly predict the next letter in a passage.
% The participant starts by predicting the first letter, is then informed whether their guess was correct or not and continues with the second letter etc.

\begin{figure}[htpb]
    % \vspace{-2mm}
    \centering
    \includegraphics[width=0.9\linewidth]{img/annotation_original.png}
    \caption{Sentence ``\emph{Several plates of food are set on a table.}'' presented with an image. Given the first 3 words, the participant now has to think of the next word, rate their confidence and after food is revealed, self-evaluate how close they were.}
    \label{fig:intro_original}
    \vspace{-4mm}
\end{figure}


Numerous studies show that humans find it easier to process words that are probable given the context \citep{huang2011predictive,lupyan2015words,clark2013whatever}.
This phenomenon was widely studied in humans using the \textit{cloze procedure} \citep{taylor1953cloze}, where participants are presented with incomplete sentences and are asked to fill in the blanks using the context from both the left and right sides.
The Shannon Game can therefore be seen as a version of the cloze task, where the word is to be predicted withouth the right-side context.
Some studies have also investigated the impact of priming on word predictability using the cloze task \citep{kutas1984brain}.
However, these studies have been limited to the single, textual modality.
In this article, we aim to address this gap and explore priming in multimodal conditions for both humans and Language Models (LMs), such as GPT-2 \citep{radford2019language}.

We compare LM and human prediction capabilities in both text-only and multimodal settings.
To this goal, we extend the Shannon Game to include an extra visual modality and investigate the relationship between self-reported confidence and accuracy of next word prediction in humans and the LM.
Additionally, we relate the psycholinguistic concept of \emph{priming} to the neural language modelling concept of \emph{prompting}.

% especially concerning predictive coding 
% have demonstrated that the human brain 
% (and thus its langauge mechanism) 
% is predictive in nature. In other words, it is considered that the brain uses predictions to match the incoming sensory inputs based on the context of previous inputs received by it. In language processing terms, the brain makes prediction about the word even before word onset. Hence, 
% Many recent works,
% \citep{huang2011predictive,lupyan2015words,clark2013whatever} 
% have shown that it is easier for the brain to process words that are more \textit{probable} given the context than those that are not. In humans, this has been widely studied \cite{kliegl2006tracking,fernandez2014eye,staub2015influence} using the \textit{cloze procedure} \citep{taylor1953cloze}. 
% The cloze procedure involves the presentation of incomplete sentences where the test participant has to fill in the blanks using the context from both the left and the right side.
% The Shannon Game can hence be seen as a restricted version of the cloze task where the word to be predicted has no contextual information on the right side.


% Some studies relying on the cloze task have also looked at the effects of priming\footnote{Impact of current stimulus (or part thereof) on the processing of a subsequent stimuli.} on word predictability \citep{kutas1984brain}.
% In this article we address that gap and explore priming in multimodal conditions for both humans and LMs (GPT-2).
% We find that, to some extent, priming effects are comparable in both humans and LMs.
% These studies have however been limited to the single, textual, modality.
% \repl{However, \repl{exploration of an extra modality (especially vision) for experiments on priming}{including the extra modality, and vision in particular, in priming experiments} is an unexplored area. }{
% Many recent neural language models \citep{devlin2018bert,liu2019roberta,lewis2019bart} use a version of the cloze procedure in the form of masked language modelling. 

% We are generally interested in using knowledge from psycholinguistics to complement research in neural networks. The hope is that finding parallels between algorithmic and human behavior with linguistic stimuli would help us refine our algorithms in the long run. Following that long term goal, 
% In this paper, we aim to relate LM and human prediction capabilities in text-only and multi-modal settings. To this end, we extend the Shannon Game with an extra visual modality and relate self-reported confidence and accuracy of predicting the next word by humans to counterparts of these measures in the GPT-2 language model \citep{radford2019language}.
% As defined and discussed below, we relate the psychological concept of \textit{priming} to the popular concept of \textit{prompting} in neural language modelling.


% \XXX{I don't like this paragraph much. It is too broad, ends with an exclamation and talks baout priming which has not been introduced. I suggest deleting it and using the citations only in Related work.}
% \repl{And w}{W}hile research in neural language models h\repl{ave}{as} led to unprecedented advances in NLP, questions like if the effects of semantic priming are observable in large language models are relatively rare. And hence, to ask if semantic priming with an additional modality influences the word prediction of language models, is not just novel but also relevant from a perspective of understanding the capabilites of the large language models. 
% It has indeed been demonstrated that language models and question-answering systems in particular were enhanced by visual \citep{kiros2014multimodal,wang2021simvlm,mogadala2021trends}, audio \citep{chuang2019speechbert} and video \citep{xu2021vlm} modalities. But to see if humans and LMs profited from the extra modality information on the same task, in the same way is definitely something!


% \XXX{Too verbose here; be simple; spell-check:}
% Vilém: Not necessary to include structure.
% The paper is structured in the following way:  related work and the definitions of important terms relevant for the paper are described in \Cref{Related} and \Cref{Defs} respectively. The setup for the LM and human experiments are discussed in \Cref{Setup}. Finally, the analysis is done in \Cref{Results}, followed by a discussion (\Cref{Discussion}) and conclusion (\Cref{Conclusion}). 



\section{Related Work}


Early research on the impact of contextual information on lexical prediction during reading relied on sentence prediction tasks \citep{fischler1979automatic,kleiman1980sentence}.
This concept was first introduced as a Shannon Game by \citet{goldman1958speech}, while the earliest versions involving images were proposed by \citet{attneave1954some,barlow1961possible,kersten1987predictability}.
In fact, reading and sentence prediction have been compared to a \textit{psycholinguistic guessing game} \cite{goodman1969analysis,cairns1975lexical,goodman2014reading}.
We posit that a task like next word prediction in a sentence provides an interesting opportunity to study the impact of context in language processing and predictability.
With the exploration of predictive processing in reading \cite{wlotko2015time}, we can utilize these developments to design our experiment.



% Early works on assessing the effect of contextual information on lexical prediction during reading rely on sentence prediction tasks \citep{fischler1979automatic,kleiman1980sentence}.
% A version of sentence completion task posed as a Shannon Game was introduced by \citet{goldman1958speech} and the earliest versions involving images were proposed by  \citet{attneave1954some,barlow1961possible,kersten1987predictability}. 
% who hypothesized that one important function of vision systems is to encode visual images into less redundant forms, which was later related with the concept of language-mediated visual attention \cite{huettig2012mechanisms}. In other words, the hypothesis is that linguistic and visual representations in the brain are coupled. This was seen to be true both for artificial neural networks  \cite{goh2021multimodal} and human brains \cite{quiroga2005invariant}. Approaches derived from the classic Shannon Game have already been used for testing the performance and abilities of language models \citep{bimbot2001alternative}. 

% Reading and sentence prediction in fact, has been conceptualized as a \textit{psycholinguistic guessing game} \cite{goodman1969analysis,cairns1975lexical,goodman2014reading}.
% And so we posit that a task like next word prediction in a sentence provides an interesting opportunity to study the effect of context on the \textit{guessing game}.
% And since predictive processing in reading is somewhat explored \cite{wlotko2015time}, we rely on these developments to design our experiment.

The effect of context is pervasive and present at multiple levels of processing \citep{willems2021context}.
Previous fMRI studies \citep{mummery1999dual,rissman2003event} have demonstrated that the brain's response to a given word depends on the preceding linguistic context.
\citet{ames2015contextual} explore the impact of contextual information, in the form of visual data, on discourse comprehension and \citet{altmann2009incrementality} provide a comprehensive cognitive explanation of how visual context affects language processing, reporting that the ``eyes move toward whatever in the visual scene that unfolding word could refer to.''
Several psycholinguistically motivated studies \citep{barca2012unfolding, vanderwart1984priming} have investigated the role of general context in lexical prediction and how cross-modal priming (with images and text) works in lexical decision tasks.
% , exploring the \textit{lexicality effect}\footnote{Faster and more accurate response times in predicting words versus non-words.}
However, these studies did not explicitly investigate semantic priming for a cloze task.
We attempt to do so in a cross-modal setting.
Our aim is to explore the extent of semantic priming in a Shannon Game setting when priming is done using an image or information extracted from that image, in the direction of \citet{cho2021unifying}.
Hence, the Multimodal Shannon Game with images can also be perceived as an autoregressive image captioning task, where the output is generated word-by-word, and its accuracy can be easily measured \citep{hossain2019comprehensive}.

In this direction, \citet{bhattacharya2022emmt} conducted an experiment on human participants with translation enhanced by image modality, which is parallel to our experiment with language modelling using the same modality.
Finally, some researchers \citep{hladka2009designing,hladka2011attractive} have utilized the \emph{Games With a Purpose} methodology \citep{von2008designing} to frame tasks that are difficult for computers but relatively easy for humans as games. Similarly, we frame our experiment as a game that participants reportedly enjoy.

% The effect of context is pervasive and present at multiple levels of processing \citep{willems2021context}.
% Several fMRI studies \citep{mummery1999dual,rissman2003event} have previously demonstrated that the brain's response to a given word depends on the preceding linguistic context.
% \citet{ames2015contextual} explore the effect of contextual information in the form of visual data on discourse comprehension. 
% A thorough cognitive explanation of how visual context affects language processing is given by \citet{altmann2009incrementality}, reporting that 
% the ``eyes move toward whatever in the visual scene that unfolding word could refer to.'' 
% Several psycholinguistically motivated studies \citep{barca2012unfolding, vanderwart1984priming} also investigated the role of context in lexical prediction.
% But these studies have explored the \textit{lexicality effect}\footnote{Faster and more accurate repsonse times in predicting words vs non-words.} and how cross-modal priming (with images and text) works in lexical decision tasks.
% They did not explicitly study semantic priming for a cloze task.
% We attempt to do that in a cross modal setting. We are interested in exploring the extent of semantic priming in a Shannon Game setting when the priming is done using an image or information (extracted from that image) in the direction of \citet{cho2021unifying}.
% Hence, the Multimodal Shannon Game with images can also be perceived as an autoregressive image captioning where the output is generated word-by-word and its accuracy can easily be measured \citep{hossain2019comprehensive}. 
% In this direction, \citet{bhattacharya2022emmt} perform an experiment on human participants with translation enhanced by image modality.
% This is parallel to our experiment with language modelling with the same modality. 

% \XXX{Could you clarify above how our work is different?}

% Finally, some works \citep{hladka2009designing,hladka2011attractive} have exploited the \emph{Games With a Purpose} methodology \citep{von2008designing} to frame tasks that are difficult for computers but relatively easy for humans as games.
% Similarly, we frame the experiment as game that the participants (reportedly) enjoy.
% We use some aspects of the GWAP methodology by framing the experiment as a game that the participants can enjoy.


\section{Priming and Prompting}

Priming is a psychological and linguistic phenomenon where the presentation of a stimulus affects the processing of another stimulus in the future.
This effect has been widely studied in various contexts and has been defined as the facilitative effect of an encounter with a stimulus on subsequent processing of the same or a related stimulus \citep{tulving1982priming}.
One of the most important paradigms of priming is semantic priming, where the response to a stimulus is faster if it is preceded by something semantically related.
For example, the reaction to the word ``dog'' in a sentence would be faster if a semantically related prime, like ``cat'', were presented previously in the sentence \citep{meyer1971facilitation, shelton1992semantic}.

Prompting is a relatively new paradigm in neural language modeling where pretrained language models are trained to perform several downstream tasks by using an appropriate ``prompting function'' \citep{liu2021pre}.
In this paradigm, a pretrained language model is conditioned on extra information in the context, in addition to the previous words, to model $p(w_i| w_{<i}, C)$.

We use the Multimodal Shannon Game (MMSG) framework to assess the hypothesis that semantic priming manifests itself in the same way in humans as it does in large autoregressive language models.
Specifically, that the additional visual information (in whichever form) helps in the next word prediction task in the same way for both humans and LMs.
The results of this study contribute to our understanding of how multimodal information can be used to improve language modeling and documentation of the semantic priming effects.

% a study on the effect of semantic priming in language models and humans. We use the Multimodal Shannon Game framework to test the effect of semantic priming of picture and words on a next word prediction task.
% We hypothesize that the semantic priming effects on account of the added multimodal information along with the sentence context should be observable in self-reported prediction confidence and accuracy from both humans and LMs.

% The Multimodal Shannon Game is used as an experimental framework to test the effect of semantic priming of pictures and words on a next word prediction task. We seek to assess the effect of semantic priming on self-reported prediction confidence and accuracy from both humans and LMs. By using a multimodal approach, we aim to determine whether the added multimodal information along with the sentence context enhances the semantic priming effect.



% We hypothesize that semantic priming effects are observable in pretrained large language models similar to humans if modality-specific information is used.
% We conceptualize the additional modality information associated with image captions from the MSCOCO \cite{lin2014microsoft} dataset as the ``prompt''.
% Priming has been defined from a purely linguistic (and psycholinguistic) perspective as ``facilitative effects of an encounter with a stimulus on subsequent processing of the same or a related stimulus'' \citep{tulving1982priming}.
% In other words, priming is the process by which the presentation of a stimulus affects the processing of some other stimulus in the future. 
% A number of priming studies over the years (see \citet{schacter1998priming} for details on the effects of priming in the brain) demonstrated the effect of previous stimuli on the processing of certain stimuli presented in the future.\footnote{For example, if a child sees a pack of chips on a table, they might start looking for or thinking about chips when they see the table next.}
% An important conceptual paradigm in this regard is \textit{semantic priming}.
% It refers to the observation that the response to a stimulus is faster if it is preceded by something  semantically related.\footnote{For example, the reaction to the word ``dog'' in a sentence would be faster if a semantically related prime, like ``cat'', were presented previously in the sentence \citep{meyer1971facilitation,shelton1992semantic}.}

% We hypothesize that semantic priming effects on account of the added multimodal information along with the sentence context should be observable in self-reported prediction confidence and accuracy from both humans and LMs.
% Hence, in the present paper, we frame the Multimodal Shannon Game as an experimental framework to test the effect of semantic priming of picture and words on a next word prediction task. 

% \section{Definitions}
% \label{Defs}
% \paragraph{Priming}
% the processing of a subsequent stimulus in the brain is affected by the presentation of a previous stimulus item.
% \XXX{too complex sentence, break into parts, avoid nesting:}


% \paragraph{Prompting}

% Prompting is a relatively new paradign in neural language modelling wherein pretrained language models are trained to do several downstream tasks by using an appropriate `prompting function' \citep{liu2021pre}.
% The idea here is that while a pretrained language model is trained to model $p(w_i| w_1 \ldots, w_{i-1})$, we add some extra information to the context for the model to condition on, $p(w_i| w_1 \ldots, w_{i-1}, C)$.

% We conceptualize the additional modality information associated with image captions from the MSCOCO \cite{lin2014microsoft} dataset as the \textit{prompt}.
% We seek to test the hypothesis that semantic priming effects are observable in pretrained large language models similar to humans if modality-specific information is used.
% Hence, we use the Multimodal Shannon Game (MMSG) framework to assess if semantic priming manifests itself in the same way in humans as it does in large autoregressive language models.

% Formally, the Shannon game can thus be modelled as conditional probability .

% \XXX{very strange wording here: weights used by conditioning? The model has been trained for a purpose; a prompt moves us to a state where it behaves in a particular way; the weights are not used in any special way, it is just that in that state, the model behaves specifically: }weights of pretrained language models could be used for downstream tasks by conditioning the model to respond to some `prompts'. 
% The additional modality information is represented as : < object\_label >.
% This prompt is then prepended to a text context for prompt tuning (finetuning with a prompt) of the models.
% As a consequence, we try to replicate the text-only configurations of the Multimodal Shannon Game on GPT-2 models.
% \XXX{too complicated wording; again simplify, use sequences instead of structures: }
% To analyze the effect of priming on the model and/or the input, we experiment with different configurations of the prompt as described in \Cref{sec:LM_exp}.

\begin{table}[htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{p{1.3cm}p{6.6cm}}
    \toprule
    \textbf{No image\hspace*{-10mm}} & No extra information was shown and the participants could only use the left context. \\
    \textbf{Original}\hspace*{-10mm} & The full original image was shown.\hspace*{-10mm} \vspace{0.5mm}\\
    \textbf{Labels\newline all} & The full original image was shown with bounding boxes and labels (\Cref{fig:labels_all}). \\
    \textbf{Labels\newline crop} & The detected parts of the image were cropped and the snippets shown with labels (\Cref{fig:labels_crop}). \\
    \textbf{Labels \newline text} & Only the list of labels of objects in the image was shown (\Cref{fig:labels_text}). \\
    \bottomrule
\end{tabular}
}
\caption{Possible multimodality configurations.}
\label{tab:configs}
% \vspace{-2mm}
\end{table}


\begin{figure}[htbp]
\includegraphics[width=\linewidth]{img/annotation_pipeline.pdf}
\caption{Annotation pipeline for Multimodal Shannon Game with images. The loop ends when the end of sentence is reached.}
\label{fig:annotation_pipeline}
\end{figure}

\begin{figure*}
\centering
% Vilém: Removed subfigures because that's mixing two things together
% \begin{subfigure}
\subfloat[\emph{original}]{\includegraphics[width=0.33\linewidth]{img/annotation_original.png}\label{fig:original}}
\subfloat[\emph{labels all}]{\raisebox{0.5cm}{\includegraphics[width=0.33\linewidth]{img/img_labels_all.jpg}}\label{fig:labels_all}}
% \end{subfigure}
% 
% \begin{subfigure}
\begin{minipage}{0.33\linewidth}
\vspace{-3.3cm}
\subfloat[\emph{labels crop}]{\includegraphics[width=\linewidth]{img/img_labels_crop.png}\label{fig:labels_crop}}

\vspace{0.9cm}

\subfloat[\emph{labels text}]{\includegraphics[width=\linewidth]{img/img_labels_text.png}\label{fig:labels_text}}
\end{minipage}
% \end{subfigure}

\caption{The 4 configurations of multimodality for the same sentence (``\emph{Several plates of food are set on a table.}''). Given the first 3 words, the participant now has to think of the next word, rate their confidence and after \emph{food} is revealed, self-evaluate how accurate they were. The configuration \emph{no image} is not shown.}
\label{fig:modes}
\end{figure*}

\begin{table*}[htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lp{9.5cm}p{6.6cm}}
    \toprule
    \bf \# & \bf Confidence & \bf Accuracy \\
    \midrule 
    \textbf{0} & You have no idea about the next word.
    & Could not be more wrong (wrong area and POS) \\
    \textbf{1} & You know at least e.g. what part-of-speech the next word probably is.
    & Very wrong but some aspects close (e.g. POS) \\
    \textbf{2} & You know roughly what areas of words to expect.
    & Wrong but the idea was roughly right \\
    \textbf{3} & You know the next word or some variations of it.
    & Very close (same area and POS) \\
    \textbf{4} & You know the next word precisely.
    & Exact match \\
    \bottomrule
\end{tabular}
}
\caption{Description of the \textbf{confidence} and \textbf{accuracy} scale shown to the participants.}
\label{tab:conf_desc}
\end{table*}

% Todo: somewhere cite confidence for ptakopět

\section{Experiment setup}

\paragraph{Methodology.}

The MMSG experiment consists of asking participants to predict the next word based on the previous (left) context, optionally given a related image information (see example in \Cref{fig:intro_original}).
We consider five configurations as described in \Cref{tab:configs}.
All participants saw each of the 17 sentences (listed in \ACref{sec:sentences}) with a randomly generated configuration.
\emph{No image} corresponds to vanilla autoregressive LM while \emph{Original} corresponds to a multimodal LM which also processes an image. 
\emph{Labels all}, \emph{labels crop} and \emph{labels text} correspond to pipelines that use an image object detector as an intermediate step.
Examples of configurations are shown in \Cref{fig:intro_original,fig:modes}.


% These configurations were chosen because they can be compared with modern language models.

\paragraph{Participants.}

We enrolled 24 volunteers from the academic environment, aged 24 to 40 years of various nationalities.
They were all non-native English speakers with advanced language proficiency (\href{https://en.wikipedia.org/wiki/Common_European_Framework_of_Reference_for_Languages}{C1 and C2 levels}).
% They were C1 and C2 speakers of English but not native.
% \footnote{Common European Framework of Reference for Languages}


\paragraph{Annotation environment.}

The annotation environment used in the MMSG experiment consists of a sequence of screens for each sentence.
Each screen starts with one of the five configurations, a blank ``\texttt{\_\_}'' cursor and the participant being asked to guess the first word and mark their confidence on a numeric scale (\Cref{tab:conf_desc}).
Upon pressing any of the five buttons, the actual next word is revealed and the participants are presented with a self-evaluation scale (\Cref{tab:conf_desc}).
Afterwards, they guess the next word and so on until the end of the sentence.
The whole experiment instructions are available in \ACref{sec:instructions}.
The overall pipeline, for humans and LMs, is shown in \Cref{fig:annotation_pipeline}.
See \ACref{fig:demo_2b} for the user interface thorough the whole pipeline.
% The participants mark their confidence of their word guess on a color-coded numeric scale from red-0-lowest to green-4-highest (\Cref{fig:intro_original} and \Cref{tab:conf_desc}).
% After the confidence button press, the participants self-evaluate their word prediction on another colour-coded scale (\Cref{conf_desc}).

% The participants are intentionally not asked to write down the actual predictions because we are also interested in their reaction times.


% \begin{table}[htbp]
% \centering
% \begin{tabular}{lp{5.5cm}}
%     \toprule
%     \textbf{} & Unable to evaluate \\
%     \textbf{0} & Could not be more wrong (wrong area and POS) \\
%     \textbf{1} & Very wrong but some aspects close (e.g. POS) \\
%     \textbf{2} & Wrong but the idea was roughly right \\
%     \textbf{3} & Very close (same area and POS) \\
%     \textbf{4} & Exact match \\
%     \bottomrule
% \end{tabular}
% \caption{Description of the \textbf{self-evaluation} scale shown to the participants.}
% \label{tab:selfeval_desc}
% \end{table}



% \begin{figure}[htbp]
% \centering
%     \includegraphics[width=0.9\linewidth]{img/annotation_selfeval.png}
%     \caption{\textbf{Accuracy} self-evaluation screen after the participant clicked one of the confidence buttons.}
%     \label{fig:annotation_selfeval}
% \end{figure}


% \footnote{The evaluation was done by NLTK 3.7, SciPy 1.7.3. The annotation environment was developed in TypeScript 4.6.3 and shown on various browser versions of the participants.}
The experiment was implemented as an web application, which allowed us to reach more participants at the cost of having no control over the environment.\footnote{The annotation environment was shown on various browser versions of the participants.}
% with data being sent to a central server.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/grade_conf.pdf}
    \caption{Heatmap of confidence$\times$self-eval scores across configurations. The x-axis is the confidence score. Each cell reports the number of such judgements. Correlations ($\rho$) are Pearson's correlation coefficients between the confidence and self-eval scores.}
    \label{fig:grade_conf}
\end{figure*}

% The sentences for the experiment were taken from COCO \citep{lin2014microsoft,chen2015microsoft} 2017 validation dataset.
\paragraph{Sentences.}

% We selected 16 English sentences to achieve uniform sentence length distribution between 8 and 15 words (each bucket has 2 sentences).
We selected 16 English sentences of length between 8 to 15 words.
This scale was chosen so that the participants are fully focused during the whole session (average of 25 minutes).
Furthermore, the smaller scale is required to have a more representative sample for each sentence + configuration tuple.
Note that this is not the natural distribution of the sentence length but desirable from an experiment design perspective to be able to compare phenomena across this variable.
Because they were taken from an image captioning dataset, some of the ``sentences'' are actually noun phrases without the main verb, which made the task more challenging for the participants.
The full list of the sentences is in \Cref{sec:sentences}.

We also added the sentence ``\emph{To be or not to be.}'' with an accompanying picture.
We assumed that the participants would easily recognize this sentence after the first few words and would continue with a sequence of high ratings.
This was meant to calibrate the participants' ratings and to introduce them to the task.

% \paragraph{LM Experiments.}
% \label{sec:LM_exp}

% For the purpose of the LM experiments, we used the pretrained GPT-2 model.
% from Huggingface.\footnote{\href{https://huggingface.co/GPT-2}{huggingface.co/GPT-2}}
% We experiment with three kinds of models: baseline, text-finetuned and modal-finetuned. 
% The fine-tuned model was created by finetuning the pretrained model on the captions of the MSCOCO training corpus. The modal-fintuned model was created by first appending a `prompt' in the form of < object\_label1, object\_label2 > to each caption of the MSCOCO training corpus and then finetuning the pretrained GPT model on this data. The `prompt' was thus comprised of the list of object labels (extracted using a pretrained YOLO model\footnote{\href{https://github.com/ultralytics/yolov5}{github.com/ultralytics/yolov5}}) and attempted to mirror the ``labels\_text'' configuration used for the human experiments. 

% During inference, the models were tested on two different kinds of inputs. The first kind was comprised simply of captions from the MSCOCO test corpus. For the second kind of input, the corresponding `modal-prompt' (< object\_label1, object\_label2 >) was prepended to the captions. The objective of such a setup was to see how semantic priming manifests from within the `prompt' on different versions of a GPT-2 model. We run inference on both the list of sentences on which the human participants were tested and on the entirety of the COCO \cite{lin2014microsoft} validation set for comparative results. We present results from the runs on the validation set only in the following sections.



% We focus on 4 quantities: confidence and self-evaluation scores (0 to 4) and also the times the participant needed to report them.
% Due to the uncontrolled experiment environment, we filter response times longer than 15 seconds.
% This is justified because the median was 2.4 and 2.1 seconds for confidence and self-evaluation, respectively.
% We did not perform any other normalization of the data.


% We then normalized the data between 0 and 1 by dividing the scores by 4.
% Also for the purpose of analysis, the entire sentence has been divided into 3 parts (beginning, middle, end).
% The hypothesis is that the effects of the additional multimodal context would be more visible at the middle and end of the sentence than at the beginning.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/basic_avgs.pdf}
    \caption{Average confidence and self-eval scores and times. Confidence intervals are 95\% from t-distribution. Note the two separate y-axes for two kinds of quantities.}
    \label{fig:basic_avgs}
\end{figure}

\section{Analysis}
\subsection{Effect of Configurations}

% \label{sec:wordpos}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/sent_len.pdf}
%     \caption{Average prediction confidence and self-eval scores with reaction times with respect to the sentence length for all configurations.}
%     \label{fig:sent_len}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/human_data_confidence.pdf}
%     \caption{Confidence}
%     \label{fig:basic_avgs_conf}
% \end{figure}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/human_data_accuracy.pdf}
%     \caption{Accuracy}
%     \label{fig:basic_avgs_acc} 
% \end{figure}

% For the purpose of analysis, we focus on just the self-reported confidence and self-evaluation (accuracy) scores (between 0 to 4).
The confidence and accuracy averaged for each configuration are shown in \Cref{fig:basic_avgs}.
The \emph{original} configuration (where the entire image was shown to the participants) yielded both the highest confidence and self-evaluation scores while \emph{no image} configuration the lowest.
This shows that the participants were able to utilize the visual information.
When distilled to a set of labels (\emph{labels text}) or a series of pictures of individual objects extracted from the image (\emph{labels crop}), it still increased the confidence in their guesses with respect to the \emph{no image} configuration.

The difference in the self-reported accuracy and self-reported confidence for the configurations
\emph{labels text} and \emph{labels crop} is minimal. 
From a theoretical perspective, \emph{labels all} only added extra information in the form of bounding boxes and labels.
This had, unfortunately, a slightly detrimental effect in comparison to \emph{original}.
The participants agreed that the \emph{original} configuration was the easiest and that the \emph{labels all} was only distracting, in some cases obscuring an important part of the image and possibly suggested different synonyms than used in the sentence.
% In the rest of the paper, we focus on the self-reported quantities, not time.

The distribution of confidence and self-eval scores is shown in \Cref{fig:grade_conf}, which also shows the bipolarity of the ratings.
Often the participants were either very sure and were correct (high scores) or the opposite (low scores) with few in-between.

% It should also be noted that both the self-reported confidence and accuracy are higher in the middle and end of the sentence in comparison to the beginning of the sentence (\Cref{fig:sent_len}).

\begin{figure}[htbp]
    \vspace{-0.2cm}
    \centering
    \includegraphics[width=\linewidth]{img/sent_len.pdf}
    \caption{Average prediction confidence and self-eval scores with reaction times with respect to the sentence length for all configurations.}
    \label{fig:sent_len}
\end{figure}

\subsection{Effect of Word Position}

The first few words had naturally lower confidence and evaluation scores (accuracy), as shown in \Cref{fig:sent_len}.
% and shown previously in the bins of `beginning' in \Cref{fig:basic_avgs_conf} and \Cref{fig:basic_avgs_acc}.
This is expected on account of the space of all possible predicitions due to the limited available context.

For the first word, the participants used mostly one of two strategies: guessing an article or nothing at all.
The average confidence and self-evaluation for \emph{no image} was 1.19 and 0.48 and for \emph{original} was 2.14 and 2.16.
This is interesting as 10 out of 17 sentences begin with a determiner where the image should not help.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.49\linewidth]{img/pos_avgs_conf.pdf}
    \includegraphics[width=0.49\linewidth]{img/pos_avgs_conf.pdf}
    \caption{Average POS prediction confidence and self-evaluation from annotators. Confidence intervals are 95\% from t-distribution.}
    \label{fig:pos_avgs}
\end{figure*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{img/pos_avgs_gpt2_base.pdf}
    \caption{Average POS prediction confidence and accuracy scores from GPT-2. Confidence intervals are 95\% from t-distribution.}
    \label{fig:pos_avgs_gpt}
\end{figure}

\subsection{Effect of Part of Speech}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{img/human_data_confidence_DT.pdf}
%     \caption{Confidence: Determiner}
%     \label{fig:dt_pos_conf}
% \end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{img/human_data_accuracy_DT.pdf}
%     \caption{Accuracy: Determiner}
%     \label{fig:dt_pos_acc}
% \end{figure}


Naturally, some word classes are easier to predict than others.
This is shown in \Cref{fig:pos_avgs} where the users performed systematically better on determiners than other POS, like nouns.
% For determiners, there is a difference between the recorded average self-reported confidence and accuracy.
Finally, for both accuracy and confidence, the \emph{no image} configuration yields the lowest values across all POS.
This is counterintuitive because the prediction of a determiner should be based on purely the syntactic properties of the left context and not the multimodality.
A possible explanation is the grammatical number disambiguation in the image.
% or that the presence of the extramodal information helps more in framing a possible sentence, and hence more confident and accurate determiners, than in the absence of extra modal information.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{img/human_data_confidence_NN.pdf}
%     \caption{Confidence (NN=Noun)}
%     \label{fig:nn_pos_conf}
% \end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{img/human_data_accuracy_NN.pdf}
%     \caption{Accuracy (NN=Noun)}
%     \label{fig:nn_pos_acc}
% \end{figure}


Nouns are of interest because what the object labels represent in configurations like \emph{labels all}, \emph{labels crop} and \emph{labels text} are a sequence of nouns.
% So the question is to see if priming effects are visible on nouns given the muiltimodal context.
Nevertheless, For the nouns, we see the \emph{labels text} configuration yields the worst confidence and accuracy score from among other configurations with added modal information.
Even though we attempt at semantic priming of the nouns, the priming via text (\emph{labels text}) is comparatively less effective when analyzed with the confidence and accuracy scores of the human participants
% In comparison to the determiners, the distinction between average self-reported confidence and accuracy is less visible.
% Finally, we see that the human participants had a greater degree of confidence about the prediction of the determiners in the middle and end of the sentence with respect to nouns. 

\subsection{LM Results}

We replicate the experiment on the GPT-2 language model \citep{radford2019language}.
In every step, for every word $w$ and model prediction $p$ (distribution across vocabulary), we use $\max p$ (maximum word probability) as the confidence of the model output.
% We also use the loss returned by the model with respect to the prediction and the correct label as the metric for model accuracy.
The output, despite being a probability formally, is however not calibrated \citep{jiang2021can}.
% Since the metric used for measuring confidence with the GPT-2 model is a probablity, we do not normalize it. But to normalize the loss, we use the transformation $L^T=\frac{1}{1+L}$ where L is the actual loss returned by the model. 
% $p_w$ (true class probability) as the accuracy of the prediction \citep{corbiere2019addressing}.
Because GPT-2 is not a visual model, we consider only two configurations: \emph{no image} and \emph{labels text}.
\Cref{fig:pos_avgs_gpt} shows the results for GPT-2.
Slightly higher accuracy and confidence for the \emph{labels text} configuration show that the model is able to make use of the fusion to improve its prediction.
% \XXX{Warning, I think that Accuracy in Labels text of Nouns is worse than in No image, only the determiners are better, other POSes are without visible change (I know it's all within error bars anyway). So be careful with wording the comparison.}
It exhibits some similar patterns to humans: lower confidence and accuracy for nouns and verbs and high for determiners.
% In contrast, the model performs better on the \emph{other} category of words.
The human-LM Pearson correlation coefficients for both confidence and accuracy decreases when we fuse in the labels (\Cref{tab:correlation}), suggesting different usage of the extra information in humans and LM.

\begin{table}[htbp]
\centering
\begin{tabular}{lcc}
\toprule
& \emph{no image} & \emph{labels text} \\
\midrule
\textbf{Confidence} & 0.38 & 0.25 \\
\textbf{Accuracy} & 0.56 & 0.45 \\
\bottomrule
\end{tabular}
\caption{Pearson correlation coefficients (micro) between human annotators and GPT-2 predictions.}
\label{tab:correlation}
\end{table}

% GPT$_\textsc{Base}$ and GPT$_\textsc{Large}$ produce similar results (not shown).

% As described earlier, we experiment with three different kinds of models to study the impact of non-primed (just the captions) and primed (object labels prepended to the captions) inputs. In the plots, the two kinds of inputs are given the codes \emph{uni} and \emph{multi} respectively. Also in the plots, the three kinds of models have been coded as \emph{baseline} (baseline), \emph{ft} (finetuned with captions only), and \emph{mft} (fintuned with the modal prompts). Note that the results shown here are by aggregating the model's performance over the COCO (2017) validation set (5000 sentences). 
% The x-axis labels in the figures shown below follow the following format: <model\_name>\_<prompt\_type>. 
% Here model\_name consist of the entries `baseline',`ft',`mft' corresponding to the baseline pretrained GPT-2 model, a GPT-2 model finetuned on captions only and a GPT-2 model finetuned on the `primed' captions respectively.  
% , which we fuse by prepending them to the sentence prefix (priming).

% \Cref{fig:pos_avgs_GPT-2_base} shows the results for GPT-2.

% % \XXX{Warning, I think that Accuracy in Labels text of Nouns is worse than in No image, only the determiners are better, other POSes are without visible change (I know it's all within error bars anyway). So be careful with wording the comparison.}
% It exhibits some similar patterns to human participants: lower confidence and accuracy for nouns and verbs and high for determiners.
% In contrast, the model performs better on the \emph{other} category of words.
% We observe that the human-LM Pearson correlation coefficients for both Confidence and Accuracy decreases when we fuse in the labels (\Cref{tab:correlation}), suggesting that GPT-2 does not benefit from the text version of the multimodal information.
% GPT$_\textsc{Base}$ and GPT$_\textsc{Large}$ produce similar results (not shown).

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_confidence_validation.pdf}
%     \caption{GPT-2 Confidence}
%     \label{fig:LM_val_conf_basic}
% \end{figure}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_acc_loss_validation.pdf}
%     \caption{GPT-2 Accuracy}
%     \label{fig:LM_val_acc_basic}
% \end{figure}

% \Cref{fig:pos_avgs_gpt2_base} shows the results for GPT-2.
% Slightly higher accuracy and confidence for the primed input (\emph{labels text} configuration in the human experiments) show that the model is able to make use of the prompt to improve its predictions (\Cref{fig:LM_val_conf_basic} and \Cref{fig:LM_val_acc_basic}). Both the accuracy and confidence show an increase with the finetuned model (only with captions) and the finetuned prompt model (where the modal prompt was prepended to the captions during the finetuning).

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_confidence_DT_validation.pdf}
%     \caption{GPT-2 Confidence: Determiner}
%     \label{fig:LM_val_conf_dt}
% \end{figure}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_acc_loss_DT_validation.pdf}
%     \caption{GPT-2 Accuracy: Determiner}
%     \label{fig:LM_val_acc_dt}
% \end{figure}

% Focusing on the POS labels taken up during the analysis of the human data (\Cref{fig:LM_val_conf_dt} to \Cref{fig:LM_val_acc_nn}), we see that the confidence of the model while predicting determiners was higher than predicting nouns (similar to the humans). Interestingly, even in the case of GPT-2 models, the confidence associated with determiners improved as the model learnt to integrate the multimodal prompt.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_confidence_VB_validation.pdf}
%     \caption{GPT-2 Confidence: Verb}
%     \label{fig:LM_val_conf_vb}
% \end{figure}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_acc_loss_VB_validation.pdf}
%     \caption{GPT-2 Accuracy: Verb}
%     \label{fig:LM_val_acc_vb}
% \end{figure}

% As apparent in \Cref{fig:LM_val_conf_dt}, the maximal confidence was accorded to the modal prompt input going into the GPT-2 model finetuned with the modal prompted captions. A similar pattern is observable for the nouns (\Cref{fig:LM_val_conf_nn}) but not for the verbs (\Cref{fig:LM_val_conf_vb}). In fact when we do indeed look at the patterns of the verb, we do not see much effects of the semantic priming in the confidence and accuracy of prediction.


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_confidence_NN_validation.pdf}
%     \caption{GPT-2 Confidence: Noun}
%     \label{fig:LM_val_conf_nn}
% \end{figure}

% As far as the accuracy is concerned, the addition of the modal information significantly improves the performance of model on the prediction of nouns (\Cref{fig:LM_val_acc_nn}). This leads us to posit that semantic priming effects are observable in GPT-2 family of models with our experimental paradigm. 

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{img/LM_acc_loss_NN_validation.pdf}
%     \caption{GPT-2 Accuracy: Noun}
%     \label{fig:LM_val_acc_nn}
% \end{figure}

\section{Discussion}

Inspired by the 4 central questions (\emph{Why? What? How? When?}) about prediction in language processing proposed by \citet{huettig2015four}, we look at the results from the cognitive perspective.
We are primarily interested in the \emph{What?} questions i.e. what cues were relevant for the predictions and what language features are affected the most with change in contextual cues.

% We see that in the present experiment for the humans, the configuration \emph{original} evoked the largest prediction confidence as well as self-evaluation scores.
% However, the configurations where only the object labels corresponding to objects in the image (from an object detection model) were given either just as text (\emph{labels text}) or as cropped regions from the image (\emph{labels crop}) evoked lower prediction confidence and accuracy scores than the cases where the entire image was available to the participants. 

% The role of visual context in anticipatory processing was explored by \citet{kukona2014lexical}, leading to the conclusion that anticipatory processing (especially for verbs) makes greater use of the additional multimodal cues.
Irrespective of the input modality, from the POS experiments it is evident that both the prediction confidence (anticipatory processing by \citep{kukona2014lexical}) and accuracy for verbs and nouns improve significantly with an informative multimodal context.
We also posit that the extra modality makes the models (and humans) more confident about the content of the sentence and that translates to the added condifence and accuracy of determiners.
% For \citet{kukona2014lexical,kukona2011time} the modality was acoustic while our input modality was text and images.
Note that for GPT-2 the pattern of confidence and accuracy increasing with extra modal information does not fit perfectly with verbs.
% We thus posit that semantic priming with the MMSG setup affects nouns more than verbs in GPT-2 models.

In terms of the effects of priming in language models, \citet{sinclair2022structural,prasad2019using} use (syntactic and structural) priming to see how much language models are succeptible to priming effects.
Similarly, \citet{misra2020exploring} explored the effect of semantic priming in BERT.
Our formulation of the Multimodal Shannon Game establishes a way to effectively compare the priming effects in humans and LMs on the same benchmark which has not been attmpted before.
We also find that the priming effect, as explored by us, gets more noticeable with additional context with autoregressive models, which contradits \citet{misra2020exploring}.
However, we do acknowledge that although they looked at the phenomenon of semantic priming, the methodology and the nature of stimuli used in \citet{misra2020exploring} is radically different. 

In summary, we see from the experiments that priming, the effects of which are well studied in humans can be related to prompting in large language models.

\section{Summary}
\label{Conclusion}

In this paper, we introduced the multimodal version of the Shannon Game and ran an experiment on human participants and we arrived at the following conclusions:
\begin{itemize}[noitemsep]
\item
Presence of any visual information has a positive effect on the next word prediction confidence and accuracy.
\item Out of all the configurations, the full image (\emph{original}), improved these quantities the most.
\item There is a mixed effect of the image configuration and the word POS on these quantities.
\item The effects of priming becomes more and more noticeable with growing context length. 
\end{itemize}

We also include a study performed on the GPT-2 language model and find that:
\begin{itemize}[noitemsep]
\item
The GPT-2 model too benefits from the additional modality, though with much greater variation.
\item
The word POS also has an effect on the confidence and accuracy.
\item
The correlation on those two quantities between humans and GPT-2 is reduced with an additional modality.
\end{itemize}

% It showed that the presence of an image (in different modality information types) has a significant positive effect on the next word prediction confidence and accuracy.
% We see that the configuration with the full image (\emph{original}) improved the metrics the most.
% We also see that word classes like nouns and determiners show significant gain in confidence and accuracy when the extra modality information is provided.
% We analyze this dynamics of the extra modality from the lens of semantic priming with the conclusion that the presence of related objects in the additional modality helped the human participants to be more confident and more accurate in their predictions.
% across all POS, including the determiners, which are largely independent of the images.
% Compared to other similar studies, this setup was non-expensive and (reportedly) enjoyable for the participants because of gamification elements!

% This paper also featured a version of the same study performed on GPT-2 language models.
% We experiment with different finetuning settings (including prompt finetuning) and we find that just like our human participants, GPT-2 systems too benefit from the additional modality. Like our human participants, we also see that GPT-2 models were much more confident about their predictions of determiners and nouns when the extra modality was provided.
% However, this pattern was not seen in the case of verbs.


\section{Future work}

The space of extra modalities with the Shannon Game and the cloze task is underexplored.
Video or audio could be compared with still images to determine usefulness and informativity for next-word prediction.
The presented multimodal task could also be analyzed with standard psycholinguistic tools, such as EEG or eye-tracking.
Importantly, this experiment should be compared to multimodal language models and more recent models, which exhibit new, emergent, properties.

\section*{Limitations}

We focused on English which may have a different distribution of information within the sentence.
For example, a more morphologically rich language with a higher degree of agreement could show to be more predictable.
Despite using English sentences, all participants were non-native, albeit proficient, speakers.
However, most users of English are non-native\footnote{\href{https://lemongrad.com/english-language-statistics/}{lemongrad.com/english-language-statistics}} and focusing on this user base should not be perceived as an out-of-necessity substitution for enrolling native speakers. 

% We employed only models of the GPT-2 family with our priming experiments.
% We have not experimented with larger models and other model families. Different models and fusion modes may exhibit different patterns from those described in the paper. 

% Finally, increasing the sample size for our human observations could help to decrease the standard deviations recorded in the results that we present. 

\section*{Ethics statement}
The published data does not contain any identifiable personal information and the authors foresee no ethical concerns. 
The stimulus data used for the study was drawn from publicly available datasets and the pretrained models used for experiments were taken from an open-sourced repository. The authors see no ethical problems about the methodology. However, the authors would invite collaboration on pinpointing the ethical issues when performing experiments that seek to compare human behaviour with algorithmic behavior. 