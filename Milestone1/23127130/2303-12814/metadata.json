{
    "arxiv_id": "2303.12814",
    "paper_title": "Fixed points of arbitrarily deep 1-dimensional neural networks",
    "authors": [
        "Andrew Cook",
        "Andy Hammerlindl",
        "Warwick Tucker"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-06-14"
    ],
    "latest_version": 2,
    "categories": [
        "stat.ML",
        "cs.LG",
        "math.DS"
    ],
    "abstract": "In this paper, we establish a sharp upper bound on the the number of fixed points a certain class of neural networks can have. The networks under study (autoencoders) can be viewed as discrete dynamical systems whose nonlinearities are given by the choice of activation functions. To this end, we introduce a new class $\\mathcal{F}$ of $C^1$ activation functions that is closed under composition, and contains e.g. the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with activation functions in $\\mathcal{F}$ has at most three fixed points. Due to the simple nature of such networks, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12814v1",
        "http://arxiv.org/pdf/2303.12814v2"
    ],
    "publication_venue": "9 pages, 3 figures"
}