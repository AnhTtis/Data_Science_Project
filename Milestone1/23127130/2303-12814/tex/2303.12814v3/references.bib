@article {MR494306,
    AUTHOR = {Singer, David},
     TITLE = {Stable orbits and bifurcation of maps of the interval},
   JOURNAL = {SIAM J. Appl. Math.},
  FJOURNAL = {SIAM Journal on Applied Mathematics},
    VOLUME = {35},
      YEAR = {1978},
    NUMBER = {2},
     PAGES = {260--267},
      ISSN = {0036-1399},
   MRCLASS = {58F20 (58C25)},
  MRNUMBER = {494306},
MRREVIEWER = {R. H. Atkin},
       DOI = {10.1137/0135020},
}

@article {MR3948080,
    AUTHOR = {Li, Qianxiao and Tai, Cheng and E, Weinan},
     TITLE = {Stochastic modified equations and dynamics of stochastic
              gradient algorithms {I}: mathematical foundations},
   JOURNAL = {J. Mach. Learn. Res.},
  FJOURNAL = {Journal of Machine Learning Research (JMLR)},
    VOLUME = {20},
      YEAR = {2019},
     PAGES = {Paper No. 40, 47},
      ISSN = {1532-4435},
   MRCLASS = {62L20 (60H10 90C15)},
  MRNUMBER = {3948080},
}

@misc{johnson2018deep,
      title={Deep, Skinny Neural Networks are not Universal Approximators}, 
      author={Jesse Johnson},
      year={2018},
      eprint={1810.00393},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{grigsby2021transversality,
      title={On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks}, 
      author={J. Elisenda Grigsby and Kathryn Lindsey},
      year={2021},
      eprint={2008.09052},
      archivePrefix={arXiv},
      primaryClass={math.CO}
}

@article {MR3627592,
    AUTHOR = {E, Weinan},
     TITLE = {A proposal on machine learning via dynamical systems},
   JOURNAL = {Commun. Math. Stat.},
  FJOURNAL = {Communications in Mathematics and Statistics},
    VOLUME = {5},
      YEAR = {2017},
    NUMBER = {1},
     PAGES = {1--11},
      ISSN = {2194-6701},
   MRCLASS = {49M20 (37N35 68T05 93A30)},
  MRNUMBER = {3627592},
       DOI = {10.1007/s40304-017-0103-z},
       URL = {https://doi-org.ezproxy.lib.monash.edu.au/10.1007/s40304-017-0103-z},
}

@article {MR1015670,
    AUTHOR = {Cybenko, G.},
     TITLE = {Approximation by superpositions of a sigmoidal function},
   JOURNAL = {Math. Control Signals Systems},
  FJOURNAL = {Mathematics of Control, Signals, and Systems},
    VOLUME = {2},
      YEAR = {1989},
    NUMBER = {4},
     PAGES = {303--314},
      ISSN = {0932-4194},
   MRCLASS = {41A30 (92A09 92A25)},
  MRNUMBER = {1015670},
MRREVIEWER = {A. Haimovici},
       DOI = {10.1007/BF02551274},
       URL = {https://doi-org.ezproxy.lib.monash.edu.au/10.1007/BF02551274},
}
@article {MR190326,
    AUTHOR = {Hawley, N. S. and Schiffer, M.},
     TITLE = {Half-order differentials on {R}iemann surfaces},
   JOURNAL = {Acta Math.},
  FJOURNAL = {Acta Mathematica},
    VOLUME = {115},
      YEAR = {1966},
     PAGES = {199--236},
      ISSN = {0001-5962},
   MRCLASS = {30.45 (10.20)},
  MRNUMBER = {190326},
MRREVIEWER = {L. V. Ahlfors},
       DOI = {10.1007/BF02392208},
}
@book{MeloWelingtonde1993OD,
series = {Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge / A Series of Modern Surveys in Mathematics, 25},
issn = {3-540-56412-8},
abstract = {One-dimensional dynamics has developed in the last decades into a subject in its own right. Yet, many recent results are inaccessible and have never been brought together. For this reason, we have tried to give a unified ac count of the subject and complete proofs of many results. To show what results one might expect, the first chapter deals with the theory of circle diffeomorphisms. The remainder of the book is an attempt to develop the analogous theory in the non-invertible case, despite the intrinsic additional difficulties. In this way, we have tried to show that there is a unified theory in one-dimensional dynamics. By reading one or more of the chapters, the reader can quickly reach the frontier of research. Let us quickly summarize the book. The first chapter deals with circle diffeomorphisms and contains a complete proof of the theorem on the smooth linearizability of circle diffeomorphisms due to M. Herman, J.-C. Yoccoz and others. Chapter II treats the kneading theory of Milnor and Thurstonj also included are an exposition on Hofbauer's tower construction and a result on fuB multimodal families (this last result solves a question posed by J. Milnor).},
isbn = {3-642-78043-1},
year = {1993},
title = {One-Dimensional Dynamics},
edition = {1st ed. 1993.},
language = {eng},
author = {Melo, Welington de},
keywords = {Functions of real variables; Probabilities; Real Functions; Probability Theory and Stochastic Processes},
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@inproceedings{1993hinton,
author = {Hinton, Geoffrey E. and Zemel, Richard S.},
title = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
year = {1993},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
booktitle = {Proceedings of the 6th International Conference on Neural Information Processing Systems},
pages = {3–10},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'93}
}
@inproceedings{2020yang,
  author    = {Yang Yang and
               Guillaume Sauti{\`{e}}re and
               J. Jon Ryu and
               Taco S. Cohen},
  title     = {Feedback Recurrent Autoencoder},
  booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal
               Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
  pages     = {3347--3351},
  publisher = {{IEEE}},
  year      = {2020},
  doi       = {10.1109/ICASSP40776.2020.9054074},
  timestamp = {Wed, 25 Aug 2021 14:04:36 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/YangSRC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{2014kingma,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  eprint    = {1312.6114},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{2021piotrowski,
    title={Fixed points of nonnegative neural networks},
    author={Tomasz Piotrowski and Renato L. G. Cavalcante},
    year={2021},
    eprint={2106.16239},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@article{2022dacruz,
  author    = {Steve Dias Da Cruz and
               Bertram Taetz and
               Thomas Stifter and
               Didier Stricker},
  title     = {Autoencoder Attractors for Uncertainty Estimation},
  journal   = {CoRR},
  volume    = {abs/2204.00382},
  year      = {2022},
  doi       = {10.48550/arXiv.2204.00382},
  eprinttype = {arXiv},
  eprint    = {2204.00382},
  timestamp = {Wed, 06 Apr 2022 14:29:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-00382.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{2002hartman,
 Author = {Hartman, Philip},
 Title = {Ordinary differential equations.},
 Edition = {2nd ed., unabridged, corrected republication of the 1982 original},
 FSeries = {Classics in Applied Mathematics},
 Series = {Classics Appl. Math.},
 Volume = {38},
 ISBN = {0-89871-510-5; 978-0-89871-922-2},
 Year = {2002},
 Publisher = {Philadelphia, PA: SIAM},
 Language = {English},
 DOI = {10.1137/1.9780898719222},
 Keywords = {34-01,34A40,34D09,34D23,34C25,34C45,34C20,34A30,34E05},
 zbMATH = {1722934},
 Zbl = {1009.34001}
}
@book{MR3012659,
    AUTHOR = {Robinson, R. Clark},
     TITLE = {An introduction to dynamical systems---continuous and
              discrete},
    SERIES = {Pure and Applied Undergraduate Texts},
    VOLUME = {19},
   EDITION = {Second},
 PUBLISHER = {American Mathematical Society, Providence, RI},
      YEAR = {2012},
     PAGES = {xx+733},
      ISBN = {978-0-8218-9135-3},
   MRCLASS = {37-01 (34-01)},
  MRNUMBER = {3012659},
}
@InProceedings{1995han,
author="Han, Jun
and Moraga, Claudio",
editor="Mira, Jos{\'e}
and Sandoval, Francisco",
title="The influence of the sigmoid function parameters on the speed of backpropagation learning",
booktitle="From Natural to Artificial Neural Computation",
year="1995",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="195--201",
abstract="Sigmoid function is the most commonly known function used in feed forward neural networks because of its nonlinearity and the computational simplicity of its derivative. In this paper we discuss a variant sigmoid function with three parameters that denote the dynamic range, symmetry and slope of the function respectively. We illustrate how these parameters influence the speed of backpropagation learning and introduce a hybrid sigmoidal network with different parameter configuration in different layers. By regulating and modifying the sigmoid function parameter configuration in different layers the error signal problem, oscillation problem and asymmetrical input problem can be reduced. To compare the learning capabilities and the learning rate of the hybrid sigmoidal networks with the conventional networks we have tested the two-spirals benchmark that is known to be a very difficult task for backpropagation and their relatives.",
isbn="978-3-540-49288-7"
}
@misc{2016clevert,
issn = {2331-8422},
abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.},
journal = {arXiv.org},
publisher = {Cornell University Library, arXiv.org},
year = {2016},
title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
copyright = {2016. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
language = {eng},
address = {Ithaca},
author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
keywords = {Classification ; Deactivation ; Learning ; Neural networks},
}
@book{1993melo,
series = {Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge / A Series of Modern Surveys in Mathematics},
abstract = {One-dimensional dynamics has developed in the last decades into a subject in its own right. Yet, many recent results are inaccessible and have never been brought together. For this reason, we have tried to give a unified ac count of the subject and complete proofs of many results. To show what results one might expect, the first chapter deals with the theory of circle diffeomorphisms. The remainder of the book is an attempt to develop the analogous theory in the non-invertible case, despite the intrinsic additional difficulties. In this way, we have tried to show that there is a unified theory in one-dimensional dynamics. By reading one or more of the chapters, the reader can quickly reach the frontier of research. Let us quickly summarize the book. The first chapter deals with circle diffeomorphisms and contains a complete proof of the theorem on the smooth linearizability of circle diffeomorphisms due to M. Herman, J.-C. Yoccoz and others. Chapter II treats the kneading theory of Milnor and Thurstonj also included are an exposition on Hofbauer's tower construction and a result on fuB multimodal families (this last result solves a question posed by J. Milnor).},
volume = {25},
publisher = {Springer Berlin / Heidelberg},
isbn = {3540564128},
year = {1993},
title = {One-Dimensional Dynamics},
language = {eng},
address = {Berlin, Heidelberg},
author = {de Melo, Welington and van Strien, Sebastian},
keywords = {Differentiable dynamical systems ; Distribution (Probability theory ; Mathematics ; Probability Theory and Stochastic Processes ; Real Functions},
organization = {SpringerLink (Online service)},
}
@article {2018mora,
    AUTHOR = {Mora, Leonardo},
     TITLE = {Singer theorem for endomorphisms},
   JOURNAL = {Nonlinearity},
  FJOURNAL = {Nonlinearity},
    VOLUME = {31},
      YEAR = {2018},
    NUMBER = {5},
     PAGES = {1833--1848},
      ISSN = {0951-7715,1361-6544},
   MRCLASS = {37C25 (37C05 37C29 37C40)},
  MRNUMBER = {3816656},
MRREVIEWER = {Mohammad\ Sajid},
       DOI = {10.1088/1361-6544/aaa5e1},
}
@article {2000kozlovski,
    AUTHOR = {Kozlovski, O. S.},
     TITLE = {Getting rid of the negative {S}chwarzian derivative condition},
   JOURNAL = {Ann. of Math. (2)},
  FJOURNAL = {Annals of Mathematics. Second Series},
    VOLUME = {152},
      YEAR = {2000},
    NUMBER = {3},
     PAGES = {743--762},
      ISSN = {0003-486X,1939-8980},
   MRCLASS = {37E05},
  MRNUMBER = {1815700},
MRREVIEWER = {Laurent\ Guieu},
       DOI = {10.2307/2661353},
}
@article {2009webb,
    AUTHOR = {Webb, Benjamin},
     TITLE = {Dynamics of functions with an eventual negative {S}chwarzian
              derivative},
   JOURNAL = {Discrete Contin. Dyn. Syst.},
  FJOURNAL = {Discrete and Continuous Dynamical Systems. Series A},
    VOLUME = {24},
      YEAR = {2009},
    NUMBER = {4},
     PAGES = {1393--1408},
      ISSN = {1078-0947},
   MRCLASS = {37E05 (37A25 37N25 92C20)},
  MRNUMBER = {2505711},
MRREVIEWER = {Mike Todd},
       DOI = {10.3934/dcds.2009.24.1393},
}
