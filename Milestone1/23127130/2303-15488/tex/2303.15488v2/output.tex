\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage{enumitem}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multicol}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{authblk}

\newcommand\HW[1]{\textcolor{blue}{[Hongxin: #1]}}

%%% Specially configured color
\usepackage{xcolor}
\definecolor{mydarkred}{rgb}{0.6,0,0}
\definecolor{mydarkgreen}{rgb}{0,0.6,0}
\usepackage[colorlinks,
linkcolor=mydarkred,
citecolor=mydarkgreen]{hyperref}

\title{On the Importance of Feature Separability \\ in Predicting Out-Of-Distribution Error}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{%
%     Renchunzi Xie\\
%     School of Computer Science and Engineering\\
%     Nanyang Technological University\\
%     Singapore 639798\\
%     \texttt{renchunzi.xie@ntu.edu.sg} \\
%     \And
%     Hongxin Wei\\
%     Department of Statistics and Data Science\\
%     Southern University of Science and Technology\\
%     Shenzhen, China 518055\\
%     \texttt{weihx@sustech.edu.cn}\\
% }

% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Renchunzi Xie\textsuperscript{\rm 1}, Hongxin Wei\textsuperscript{\rm 1}\thanks{Corresponding Author}, Lei Feng\textsuperscript{\rm 2}, Yuzhou Cao\textsuperscript{\rm 1} and Bo An\textsuperscript{\rm 1}
% }
% % \affiliation[\textsuperscript{\rm 1}]{School of Computer Science and Engineering, Nanyang Technological University, Singapore}
%     %Afiliations
% %    \textsuperscript{\rm 1} School of Computer Science and Engineering, Nanyang Technological University, Singapore \\ \textsuperscript{\rm 2}College of Computer Science, Chongqing University, China \\
% %     XIER0002@e.ntu.edu.sg, hongxin001@e.ntu.edu.sg, lfeng@cqu.edu.cn, boan@ntu.edu.sg
% % %
% % % See more examples next
% % }

\author{%
\textbf{Renchunzi Xie}\textsuperscript{\rm 1} \quad \textbf{Hongxin Wei}\textsuperscript{\rm 2}\thanks{Corresponding Author} \quad \textbf{Lei Feng}\textsuperscript{\rm 1} \quad \textbf{Yuzhou Cao}\textsuperscript{\rm 1} \quad \textbf{Bo An}\textsuperscript{\rm 1}\\
\textsuperscript{\rm 1} School of Computer Science and Engineering, Nanyang Technological University, Singapore \\
\textsuperscript{\rm 2} Department of Statistics and Data Science, Southern University of Science and Technology, China \\
\texttt{\{xier0002,yuzhou002\}@e.ntu.edu.sg}\\
\texttt{weihx@sustech.edu.cn}\\
\texttt{lfengqaq@gmail.com}\\
\texttt{boan@ntu.edu.sg}
}

\begin{document}


\maketitle


\begin{abstract}
Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground-truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability empirically and theoretically. Specifically, we propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.
\end{abstract}

\section{Introduction}
% Estimating the true accuracy of a model on an unseen dataset is meaningful for hyperparameter selection, model adjustment and many other realistic applications, where true accuracy refers to the probability of the model outputs agreeing with the ground truth. Early methods mostly are supervised \citep{gareth2013introduction, hastie2009elements}, which are expensive to collect the validation dataset and vulnerable to attack by distribution shift. To alleviate this problem, an incremental number of researchers focus on predicting the accuracy of out-of-distribution (OOD) data in an unsupervised way. 

Machine learning techniques deployed in the open world often struggle with distribution shifts, where the test data are not drawn from the training distribution. 
Such issues can substantially degrade the test accuracy, and the generalization performance of a trained model may vary significantly on different shifted datasets \citep{quinonero2008dataset, koh2021wilds}.
This gives rise to the importance of estimating out-of-distribution (OOD) errors for AI Safety \citep{deng2021labels}. However, it is prohibitively expensive or unrealistic to collect large-scale labeled examples for each shifted testing distribution encountered in the wild. Subsequently, predicting OOD error becomes a challenging task without access to ground-truth labels.



% Machine learning techniques deployed in the open world often struggle with the lack of annotation information during evaluation to estimate the generalization performance on an unseen dataset. The exist of distribution shift, where the test data are drawn from the distribution far from the training data, gives rise to the importance of estimating out-of-distribution (OOD) errors for AI Safety \citep{deng2021labels}, since it invalidates traditional estimation methods such as validation sets \citep{gareth2013introduction, hastie2009elements} because of significant variation in accuracy \citep{quinonero2008dataset, koh2021wilds}. Thus, we investigate the problem of estimating generalization performance without labels under domain shift.

% Model evaluation is an independent and central part for machine learning \citep{platanios2017estimating} and many other fields such as hyperparameter selection and best model selection. Since true labels of the test dataset are not always available in many practical cases, estimating the true accuracy of a model on an unseen set is important. When the training and the test datasets are from the same distribution, this problem could be easily solved by a validation set \citep{gareth2013introduction, hastie2009elements}. However, it becomes challenging when there is distribution shift between the two datasets, as the validation set split from the training set cannot exactly represent the test set under this setting.

In the literature, a popular direction in predicting OOD error is to utilize the model output on the shifted dataset \citep{jiang2021assessing, guillory2021predicting, garg2022leveraging}, which heavily relies on the model calibration. Yet, machine learning models generally suffer from the overconfidence issue even for OOD inputs \citep{wei2022mitigating}, leading to suboptimal performance in estimating OOD performance. 
Many prior works turned to measuring the distribution difference between training and OOD test set, due to the conventional wisdom that a higher distribution shift normally leads to lower OOD accuracy.
AutoEval \citep{deng2021labels} applies Fr\'{e}chet Distance to calculate the distribution distance for model evaluation under distribution shift.
A recent work \citep{yu2022predicting} introduces Projection Norm (ProjNorm) metric to measure the distribution discrepancy in network parameters. 
% As it requires to re-train a new model on the OOD data, ProjNorm is computationally expensive and needs a large number of samples on the shifted distribution. 
However, we find that the connection between distribution distance and generalization performance does not always hold, making these surrogate methods to be questionable. 
%remains controversial
This motivates our method, which directly estimates OOD accuracy based on the feature properties of test instances.




% To solve this problem, a direct thinking is to utilize the model logit \citep{jiang2021assessing, garg2022leveraging, guillory2021predicting, hendrycks2016baseline}. However, they suffer from the overconfidence problem even for OOD inputs \citep{wei2022mitigating}, causing their designed scores are less reliable. Another research direction uses unsupervised loss to estimate the supervised error \citep{}. Nevertheless, they usually have specific requirement on model structure, making them less efficient and less general. ProjNorm \citep{yu2022predicting}, a self-training-based method, could avoid those issues via measuring the parameter discrepancy of two models trained on the training data and the pseudo-labeled test data, respectively. However, it consumes considerable computing resources and is sensitive to test set conditions such as distributions and sample numbers. Considering this problem from the feature representation level will not face those issues, but the current method \citep{} only focuses on the difference between training and test sets, which is insufficient to describe error caused by distribution shift. Naturally, it raises a question: if there exists a method from the view of feature representation that could estimate model's performance accurately, quickly and robustly? 

In this work, we show that feature separability is strongly associated with test accuracy, even in the presence of distribution shifts. Theoretically, we demonstrate that the upper bound of Bayes error is negatively correlated with inter-class feature distances.
% To quantify the feature separability, we introduce a simple dataset-level statistic, Dispersion score, which gauges the inter-class divergence from feature representations, i.e., outputs of feature extractor. Our method is motivated by the desirable properties of embeddings in representation learning \citep{bengio2013representation}. To achieve high accuracy, we generally desire embeddings that from different categories are associated with separated manifolds (i.e., high inter-class dispersion).  
To quantify the feature separability, we introduce a simple dataset-level statistic, Dispersion Score, which gauges the inter-class divergence from feature representations, i.e., outputs of feature extractor. Our method is motivated by the desirable properties of embeddings in representation learning \citep{bengio2013representation}. To achieve high accuracy, we generally desire embeddings where different classes are relatively far apart (i.e., high inter-class dispersion), and samples in each class form a compact cluster (i.e., high intra-class compactness). Surprisingly, our analysis shows that intra-class compactness does not reflect the generalization performance, while inter-class dispersion is strongly correlated with the model accuracy on OOD data.


Extensive experiments demonstrate the superiority of Dispersion Score over existing methods for estimating OOD error. First, our method dramatically outperforms existing training-free methods in evaluating model performance on OOD data. For example, our method leads to an increase of the $R^2$ from 0.847 to 0.970 on TinyImageNet-C \citep{hendrycks2019benchmarking} -- a 14.5$\%$ of relative improvement. Compared to the recent ProjNorm method \citep{yu2022predicting}, our method not only achieves superior performance by a meaningful margin, but also maintains huge advantages in computational efficiency and sample efficiency. For example, using CIFAR-10C dataset as OOD data, Dispersion Score achieves an $R^2$ of 0.972, outperforming that of ProjNorm (i.e., 0.947), while our approach only takes around 3\% of the time consumed by ProjNorm.

Overall, using Dispersion Score achieves strong performance in OOD error estimation with high computational efficiency. Our method can be easily adopted in practice. It is straightforward to implement with deep learning models and does not require access to training data. Thus our method is compatible with modern settings where models are trained on billions of images.
%It is straightforward to implement with deep learning models and does not require access to training data, which enables to be compatible with large scale model trained with billions of images.

We summarize our main contribution as follows:

\begin{enumerate}
    \item We find that the correlation between distribution distance and generalization performance does not always hold, downgrading the reliability of existing distance-based methods.
    \item Our study provides empirical evidence supporting a significant association between feature separability and test accuracy. Furthermore, we theoretically show that increasing the feature distance will result in a decrease in the upper bound of Bayes error.
    \item We propose a simple dataset-level score that gauges the inter-class dispersion from feature representations, i.e., outputs of feature extractor. Our method does not rely on the information of training data and exhibits stronger flexibility in OOD test data.
    % \item We show that intra-class compactness does not reflect the generalization performance under distribution shifts. 
    \item We conduct extensive evaluations to show the superiority of the Dispersion Score in both prediction performance and computational efficiency. Our analysis shows that Dispersion Score is more robust to various data conditions in OOD data, such as limited sample size, class imbalance and partial label set. Besides, we show that intra-class compactness does not reflect the generalization performance under distribution shifts (SubSection~\ref{sec:compactness}).
\end{enumerate}

% (TODO) In this work, we firstly conclude three properties that could describe conditions of feature representation: dispersion, compactness and distribution discrepancy, and explore the relation between the three properties and true OOD error. Based on our analysis, we could observe that dispersion information is surprisingly sufficient for OOD error estimation, so we propose a simple but efficient OOD error prediction score called Dispersion Score, which quantifies the dispersion condition of feature representation. 

% Besides, some of their designs increase the cost of computation by specific model structure requirement during the training process \citep{deng2021does, jiang2021assessing} or auxiliary dataset \citep{garg2022leveraging}. 

% In this work, we Since model performance has a strong relationship with the choice of data representation \citep{bengio2013representation}, we focus on exploring if feature representation could estimate the accuracy of ood data quantitatively. 
% In this work, we focus on exploring 
% \section{Understanding Feature Representation in OOD Error}
% In this section, we first study the role of feature representation in ood error from three respects: dispersion against different classes, compactness within the same class, discrepancy between the training and the test data (from Section \ref{dispersion} to Section \ref{discrepancy}). Based on our observation, we further \textbf{TODO}.

% \subsection{Dispersion against different classes}\label{dispersion}


% \subsection{Compactness within the same class}\label{compactness}


% \subsection{Discrepancy between the training and the test data}\label{discrepancy}

% \begin{figure*}
%     \centering
%     \includegraphics[width=1.\linewidth]{img/tsne.pdf}
%     \caption{\textbf{t-SNE visualization of ID and OOD dataset representation on CIFAR-10 with contrast corruption under diverse severity levels.} The first sub-figure shows feature representation of the ID dataset, while the rest of three sub-figures illustrate that of the OOD dataset. From this figure, we can observe that clusters with different labels will be increasingly dispersed when the corruption severity level becomes small.}
%     \label{fig:tsne}
% \end{figure*}



\section{Problem Setup and Motivation}
% In this section, we formulate the problem setting of OOD error estimation at test time (Section \ref{preliminaries}), and then highlight the importance of feature dispersion on generalization performance prediction (Section \ref{motivation}).
% relationship between feature representation and true OOD error via observational studies (Section \ref{motivation}).

\subsection{Preliminaries: OOD performance estimation}\label{preliminaries}
\paragraph{Setup} In this work, we consider multi-class classification task with $k$ classes. 
We denote the input space as $\mathcal{X}$ and the label space as $\mathcal{Y} = \{1, \ldots, k\}$. We assume there is a training dataset $\mathcal{D}=\{\boldsymbol{x}_i, y_i\}^{n}_{i=1}$, where the $n$ data points are sampled \emph{i.i.d.} from a joint data distribution $\mathcal{P}_{\mathcal{X}\mathcal{Y}}$. 
% We denote by $\mathcal{P}_{\text{in}}$ the marginal distribution over $\mathcal{X}$. 
During training, we learn a neural network  $f: \mathcal{X} \rightarrow \mathbb{R}^k$ with trainable parameters ${\theta} \in \mathbb{R}^p$ on $\mathcal{D}$. 

In particular, the neural network $f$ can be viewed as a combination of a feature extractor $f_{g}$ and a classifier $f_{\omega}$, where $g$ and $\omega$ denote the parameters of the corresponding parts, respectively. The feature extractor $f_{g}$ is a function that maps instances to features $f_{g}: \mathcal{X} \rightarrow \mathcal{Z}$, where $\mathcal{Z}$ denotes the feature space. We denote by $\boldsymbol{z}_i$ the learned feature of instance $\boldsymbol{x}_i$: $\boldsymbol{z}_i = f_{g}(\boldsymbol{x}_i)$. The classifier $f_{\omega}$ is a function from the feature space $\mathcal{Z}$ to $\mathbb{R}^k$, which outputs the final predictions. 
A trained model can be obtained by minimizing the following expected risk:
\begin{equation*}
\begin{aligned}
    \mathcal{R}_{\mathcal{L}}(f) &= \mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{P}_{\mathcal{X}\mathcal{Y}}}\left[\mathcal{L}\left(f(\boldsymbol{x} ; {\theta}), y\right)\right] \\
    &= \mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{P}_{\mathcal{X}\mathcal{Y}}}\left[\mathcal{L}\left(f_{\omega}(f_{g}(\boldsymbol{x})), y\right)\right]
\end{aligned}
\end{equation*}



% In the training process, we train $f_g$ and $f_{\omega}$ on the training data $\mathcal{D}_{train}=\{(x_i, y_i)\}_{i=1,...,n}$ where $(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$ via a supervised loss (i.e., Eq. \ref{eq: loss}).
% In this work, we focus on the multi-class classification problem with total $K$ classes using a neural network parameterized by $\theta$. Given a set of training data $\mathcal{D}_{train}=\{(x_i, y_i)\}_{i=1,...,n}$ where $(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$, we could train a neural network $f_{\theta}$ which is composed of a feature extractor $f_{g}$ and a classifier $f_{\omega}$ using a loss expressed as follows, where $g$ and $\omega$ denote the parameters of the feature extractor and the classifier, respectively.
% \begin{equation}
% \label{eq: loss}
%     L_{train} = l(f_{\omega}(f_{g}(x_i)), y_i).
% \end{equation}


\paragraph{Problem statement} At test time, we generally expect that the test data are drawn from the same distribution as the training dataset. However, distribution shifts usually happen in reality and even simple shifts can lead to large drops in performance, which makes it unreliable in safety-critical applications. Thus, our goal is to estimate how a trained model might perform on the shifted data without labels, i.e., unlabeled out-of-distribution (OOD) data. 

Assume that $\Tilde{\mathcal{D}}=\{\Tilde{\boldsymbol{x}}_i\}^{m}_{i=1}$ be the OOD test dataset and $\{\Tilde{y}_i\}^m_{i=1}$ be the corresponding unobserved labels. For a certain test instance $\Tilde{\boldsymbol{x}}_i$, we obtain the predicted labels of a trained model by $\Tilde{y}^{\prime}_i = \arg\max f(\Tilde{\boldsymbol{x}}_i)$. Then the ground-truth test error on OOD data can be formally defined as:
\begin{equation}
    \operatorname{Err}(\Tilde{\mathcal{D}})=\frac{1}{m} \sum_{i=1}^{m}\mathds{1}(\Tilde{y}^{\prime}_i \neq \widetilde{y}_i),
\end{equation}

To estimate the real OOD error, the key challenge is to formulate a score $S(\Tilde{\mathcal{D}})$ that is strongly correlated with the test error across diverse distribution shifts without utilization of corresponding test labels. With such scores, a simple linear regression model can be learned to estimate the test error on shifted datasets, following the commonly used scheme \citep{deng2021labels, yu2022predicting}. 
% derive a dataset-level score $S(\Tilde{\mathcal{D}})$ that is strongly correlated with the test error over various distribution shifts. With such a score, a simple linear regression model can be learned to estimate the test error on shifted datasets, following the commonly-used scheme \citep{deng2021labels, yu2022predicting}. 

While those output-based approaches suffer from the overconfidence issue \citep{hendrycks2016baseline, guillory2021predicting}, other methods primarily rely on the distribution distance between training data $\mathcal{D}$ and test data $\Tilde{\mathcal{D}}$ \citep{deng2021labels, tzeng2017adversarial}, with the intuition that the distribution gap impacts classification accuracy. In the following, we motivate our method by analyzing the failure of those methods based on feature-level distribution distance.





\subsection{The failure of distribution distance}
\label{motivation}

In the literature, distribution discrepancy has been considered as a key metric to predict the generalization performance of the model on unseen datasets \citep{deng2021labels, tzeng2017adversarial, gao2022distributionally, sinha2017certifying, yu2022predicting}. 
AutoEval \citep{deng2021labels} estimates model performance by quantifying the domain gap in the feature space:

$$S(\mathcal{D}, \Tilde{\mathcal{D}}) = d(\mathcal{D}, \Tilde{\mathcal{D}}),$$
where $d(\cdot)$ denotes the distance function, such as Fr\'{e}chet Distance \citep{dowson1982frechet} or maximum mean discrepancy (MMD) \citep{gretton2006kernel}.

ProjNorm measures the distribution gap with the Euclidean distance in the parameter space: 

$$S(\mathcal{D}, \Tilde{\mathcal{D}}) = \|\theta - \Tilde{\theta}\|_{2},$$
where $\theta$ and $\Tilde{\theta}$ denote the parameters fine-tuned on training data $\mathcal{D}$ and OOD data $\Tilde{\mathcal{D}}$, respectively.


The underlying assumption is inherited from the conventional wisdom in domain adaptation, where a representation function that minimizes domain difference leads to higher accuracy in the target domain \citep{tzeng2014deep, ganin2015unsupervised, tzeng2017adversarial}. Yet, the relationship between distribution distance and test accuracy remains controversial. 
For example, the distribution shift may not change the model prediction if the classifier's outputs are insensitive to changes in the shifted features.
There naturally arises a question: given a fixed model, does a larger distribution distance always lead to a lower test accuracy?
% \HW{Add one or two sentences to describe why?}



To verify the correlation between distribution distance and model performance, we compare the model performance on different OOD test sets of CIFAR-10C, using the ResNet-50 model trained on CIFAR-10. For each OOD test set, we calculate the distribution distances in the feature space $\mathcal{Z}$ via Fr\'{e}chet distance \citep{dowson1982frechet} and MMD \citep{gretton2006kernel}, respectively. Figure~\ref{fig:gap} presents the classification accuracy versus the distribution distance. The results show that, on different test datasets with similar distances to the train data in the feature space. the performance of the trained model varies significantly with both the two distance metrics. For example, calculated by Fr\'{e}chet distance, the distribution gap varies only a small margin from 223.36 to 224.04, but the true accuracy experiences a large drop from 61.06\% to 46.99\%. Similar to MMD distance, when the distribution gap changes from 87.63 to 88.35, the OOD accuracy drops from 66.53\% to 47.73\%. The high variability in the model performance reveals that, \textbf{the distribution difference is not a reliable surrogate for generalization performance under distribution shifts}.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.65\linewidth]{img/toy_experiments.pdf}
%     \caption{A toy example for illustrating the failure of distribution distance. Blue and Orange denote the two classes. The test accuracy will not change regardless of the shift distance of test data along the y-axis.}
%     \label{fig:toy}
%     \vspace{-20pt}
% \end{figure}

% \paragraph{A toy example.} To provide a straightforward view, we show in Figure~\ref{fig:toy} a toy example for illustrating the failure of distribution distance. Consider a binary classification with the data generating from a mixture of two Gaussian distributions $\mathcal{P}_{\mathcal{X}\mathcal{Y}}$. In particular, the label Y is either positive ($+1$) or negative ($-1$) with equal probability (i.e., 0.5). Without loss of generality, we assume there is a trained model $f(\boldsymbol{x}) = \operatorname{sign}(\boldsymbol{x} - \kappa)$, where $\kappa$ denotes a constant between class centers. At test time, we consider a specific OOD dataset that shifts along the y-axis. Naturally, we find that the test accuracy will not change regardless of the shift distance of test data. It implies that, \textbf{distribution distance may fail to indicate the model performance}. We proceed by introducing our method, exploring a new perspective in the feature space.


% As for evaluation, given an OOD dataset that has access to the ground-truth labels $\mathcal{D}_{test}=\{(\widetilde{x_i},\widetilde{y_i})\}_{i=1,...,m}$ where $(\widetilde{x_i}, \widetilde{y_i}) \in \mathcal{X} \times \mathcal{Y}$, we can calculate the true classification error of the pre-trained model by the following equation:


% During testing, given a set of test data that are collected from a distribution different from the distribution of the training data (i.e., OOD dataset), assuming we could observe its ground-truth labels $\mathcal{D}_{test}=\{(\widetilde{x_i},\widetilde{y_i})\}_{i=1,...,m}$ where $(\widetilde{x_i}, \widetilde{y_i}) \in \mathcal{X} \times \mathcal{Y}$ (i.e., training data and test data are from the same feature and label space), we compute the true classification error of the model $f$ by the following equation:
% \begin{equation}
%     OodError(\mathcal{D}_{test}) =  \frac{1}{m} \sum_{i=1}^{m}\mathds{1}(C(f_{\omega}(f_{g}(\widetilde{x_i}))) \neq \widetilde{y_i}),
% \end{equation}
% where $C(f(x))=\arg\max_{c}(f_{c}(x))$, and $\mathds{1}$ denotes the indicator function.

% Our goal is to quantity the true OOD error within linear correlation only using test samples without annotation across diverse types of distribution shift as well as severity levels.

% However, the ground-truth labels are usually invisible during testing. Our goal is to quantify the true classification error without annotation information of the test set across different distribution shifts.




% \subsection{Motivation: the role of feature representation in OOD error}\label{motivation}
% % Our method is inspired by the following three questions.
% \textbf{Mismatch between distribution discrepancy and generalization performance.} There exists a general-purpose prior that a larger distribution difference normally results in lower classifier's accuracy \citep{ben2006analysis, ganin2016domain, deng2021labels, long2015learning}. However, it is practically insufficient for the prior knowledge to establish a linear relationship between the two factors \citep{mehra2022domain}. To verify the perspective, we conduct the observational studies on both synthetic data and real world data shown in Figure \ref{fig:toy}. 

% We firstly consider a binary classification task with artificial datasets drawn from Gaussian Mixture distributions:
% \begin{equation}
%     \begin{aligned}   
%     &X_{in}|(Y_{in}=y) \sim \mathcal{N}(y\cdot \mu_{in},\sigma_{in}^{2}\mathcal{I}),\\
%     &X_{out}|(Y_{out}=y) \sim \mathcal{N}(y\cdot \mu_{out},\sigma_{out}^{2}\mathcal{I}),
% \end{aligned}
% \end{equation}

% where $X_{in}$ and $X_{out}$ are generated ID and OOD samples, $Y_{in}, Y_{out} \in \{-1,1\}$ represent discrete random variables with $P(Y_{*}=y)=p^{*}_{y}$ and $p_{y}^{in}\neq p_{y}^{out}$, $\mu_{in}, \mu_{out} \in \mathrm{R}^2$ and $\mu_{in} \neq \mu_{out}$, $\sigma_{in} \neq \sigma_{out}$, and $\mathcal{I}$ denotes the two-dimensional identity matrix. From the first two figures in Figure \ref{fig:toy}, we could observe that the logistic regression learnt on the ID dataset also performs well on the OOD dataset, even if the distribution shift is remarkable, which means that the mapping relation from distribution difference to generalization performance dose not necessarily hold. 
% % Then a logistic regression could be learnt based on them shown as the $i^{st}$ figure in Figure \ref{fig:toy}. 

% To verify the non-trivial connection in the open world, we show the true classification accuracy on CIFAR-10C using the model trained on CIFAR-10 with ResNet50 versus the distribution distance measured by Fr\'{e}chet distance \citep{dowson1982frechet} and MMD \citep{gretton2006kernel} in the last two figures of Figure \ref{fig:toy}. They illustrate the high variability in the performance even at the identical distribution distance, which means the distribution difference is not sufficiently representative of generalization performance.

% \textbf{Inter-class dispersion and intra-class compactness.} 
% using distribution difference to estimate generalization performance is not robust. 

% \textbf{Q1: Why do we consider to formulate a score from the view of feature representation?} 

% Many existing methods for OOD error estimation use logits to formulate a prediction score \citep{}. However, their designed scores do not work consistently across different datasets, since logits always suffer from overconfidence problem for both in-distribution (ID) inputs and out-of-distribution inputs \citep{wei2022mitigating}. On the contrary, feature representation coming out from the feature extractor will not meet this problem.

% On the other hand, the relation from good feature representation to good performance has been proved. For ID data, searching a good feature representation has been considered as a common perspective to improve performance of classification, recognizing, object detection, ect \citep{}. For OOD data, domain adaptation provides a solution for better performance that the feature extractor should narrow the distance between the source domain and the target domain \citep{}.  


% \textbf{Q2: Could model performance be reflected by the condition of feature representation?}

% As mentioned above, the relation from feature representation to model performance is clear, so is it also satisfied for the opposite relation? The paper \citep{deng2021does} demonstrates that the opposite relation still works. 

% Inspired by domain adaptation, it measures the representation discrepancy between the training dataset and the test dataset via Fr\'{e}chet distance, and regards the distance as the OOD error prediction. However, it only focuses on the summary of the whole distribution like the mean and the covariance, but ignores the fact that distribution shift causes not only the distribution gap between two domains but also the detailed scatter change in the feature space \citep{kang2019contrastive}. So this method does not perform well in the experiment section (i.e., Section \ref{experiment}). In addition, to measure the distribution gap, this method is required to remember the training dataset, which also makes the computation speed slow.

% detailed change in the test distribution caused by distribution shift, resulting in inaccurate description on OOD error.


% \textbf{Q3: What factors do we need to consider when formulating the score?}
% Feature representation learning has gained considerable interests in many ares such as classification \citep{} and detection \citep{}. Their goal is to find out a "good" representation for samples in order to improve the model performance. However, most of their works focus on in-distribution (ID) data 

% Many works of feature representation learning has been proposed to find out good representations for task performance improvement \citep{}. To improve performance in OOD classification, many methods of transfer learning from the view of feature representation try to   There exists a general-purpose prior that a good feature representation should be natural clustering, which means that $P(X|Y=i)$ should be well separated with as small areas of overlapping as possible \citep{bengio2013representation}. 
% Searching a good feature representation for samples has been considered as a common perspective to improve performance of classification, recognizing, object detection, ect \citep{}. \textbf{On the contrary, could model performance be reflected by the condition of feature representation?} The paper (\textit{Fr\'{e}chet distance}) \citep{deng2021does} 

% To estimate OOD error via feature representation accurately and quickly, we focus on the question: how the test set representation changes with diverse distribution shifts. There exists a general-purpose prior that a good feature representation should be natural clustering, which means that $P(X|Y=i)$ should be well separated with as small areas of overlapping as possible \citep{bengio2013representation}. Is it also true for OOD data?

% Intuitively, We draw the t-SNE plots in Figure \ref{fig:tsne} of the test set representation on CIFAR-10 with the contrast corruption when the severity level equals to 1 ,3, and 5 respectively. From this figure, we could clearly observe that the higher the severity level is, the more dispersed the points from diverse classes are. In this case, dispersion conditions need to be considered.

% Without a doubt, dispersion is not the only property for feature representation. Constractive learning also proposes that similarity among samples with the same classes should be maximized \citep{khosla2020supervised}, which means that compactness of each cluster with the same labels should be small. 

% In the following discussion, we consider \textbf{dispersion} and \textbf{compactness} as the main properties of feature representation, and explore their relationships as well as \textbf{distribution gap}'s with OOD error.



% \section{Exploratory Experiments: Dispersion, Compactness and Distribution Gap}
% In this section, we do some exploratory experiments to show the relation between the three feature properties with OOD error.

% \subsection{Feature Property Measurement.} 

% To quantify the three properties, we define the following three measurement scores that are inspired by Calinski-Harabaz index \citep{zhao2012cluster}. 

% \textbf{Dispersion Score.} We measure dispersion conditions of the test feature representation by computing the weighted average of the distance from the center of each class to 


\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[height=3.5cm,width=4.5cm]{img/frecet_distance.pdf}
        \caption{Fr\'{e}chet}
        \label{fig:frechet_gap}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[height=3.5cm,width=4.5cm]{img/MMD_distance.pdf}
        \caption{MMD}
        \label{fig:mmd_gap}
    \end{subfigure}
     \caption{Distribution Gap Vs. OOD accuracy on CIFAR10-C via (a) Fr\'{e}chet distance \citep{dowson1982frechet} and (b) MMD \citep{gretton2006kernel}. All colors indicate 14 types of corruption. The test accuracy varies significantly on shifted datasets with the same distribution distances.}
     \label{fig:gap}
     % \vspace{-15pt}
\end{figure}


\section{Proposed Method}\label{method}




In this section, we first introduce the intuition of our method with a natural example. Next, we characterize and provide quantitative measure on the desirable properties of feature representations for predicting OOD error. We then present the advantages of our proposed method.

\subsection{Motivation}
In representation learning, it is desirable to learn a separable feature space, where different classes are far away from each other and samples in each class form a compact cluster. Therefore, separability is viewed as an important characteristic to measure the feature quality. For example, one may train a nearest neighbor classifier over the learned representation using labeled data and regard its performance as the indicator. In this paper, we investigate how to utilize the characteristic of separability for estimating the model performance under distribution shifts, without access to ground-truth labels. 


\paragraph{Intuitive example} In Figure~\ref{fig:tsne}, we present a t-SNE visualization of the representation for training and test data. In particular, we compare the separability of the learned representation on shifted datasets with various severity. While the feature representation of training data exhibits well-separated clusters, those of the shifted datasets are more difficult to differentiate and the cluster gaps are well correlated with the corruption severity, as well as the ground-truth accuracy. It implies that, a model with high classification accuracy is usually along with a well-separated feature distribution, and vice versa. With the intuition, we proceed by theoretically showing how the feature separability affects the classification performance.




\paragraph{Theoretical explanation} Recall that we obtain the model prediction by $y^{\prime}_i = \arg\max f_{\omega}(\boldsymbol{z}_i)$, where the intermediate feature $\boldsymbol{z}_i = f_{g}(\boldsymbol{x}_i)$. Let $P(y=i)$ denote the prior class probability of class $i$, and $p(\boldsymbol{x}|y=i)$ denote the class likelihood, i.e., the conditional probability density of x given that it belongs to class $i$. 
Then the Bayes error \citep{young1974classification, devijver1982pattern, webb1990introduction, duda2006pattern} can be expressed as:

$$
E_{\text {bayes }}=1-\sum_{i=1}^k \int_{C_i} P\left(y=i\right) p\left(\boldsymbol{x} \mid y=i\right) d \boldsymbol{x} ,
$$
where $C_i$ is the region where class $i$ has the highest posterior. The Bayes error rate provides a lower bound on the error rate that can be achieved by any pattern classifier acting on derived features. However, the Bayes error is not so readily obtainable as class priors and class-conditional likelihoods are normally unknown. Therefore, many studies have focused on deriving meaningful upper bounds for the Bayes error \citep{devijver1982pattern, webb1990introduction}.

% Now we show how the feature separability affects the upper bound of the Bayes error. 
For a binary classification problem where $k=2$, we have
\begin{equation}
\label{eq:bayes_binary}
E_{\text {bayes }} \leq \frac{2 P\left(y=1\right) P\left(y=2\right)}{1+P\left(y=1\right) P\left(y=2\right) \Delta},
\end{equation}
where $\Delta$ denotes the distance between features from the two classes \citep{devijver1982pattern,tumer2003bayes}, such as \textit{Mahalanobis distance} \citep{chandra1936generalised} or \textit{Bhattacharyya distance} \citep{fukunaga2013introduction}. Based on the Bayes error for 2-class problems, the upper bound can be extended to the multi-class setting ($k>2$) by the following equation \citep{garber1988bounds}.
\begin{equation}
\label{eq:bayes_multi}
E_{\text {bayes }}^k \leq \min _{\alpha \in\{0,1\}}\left(\frac{1}{k-2 \alpha} \sum_{i=1}^k\left(1-P\left(y=i\right)\right) E_{\text {bayes } ; i}^{k-1}+\frac{1-\alpha}{k-2 \alpha}\right),
\end{equation}

where $\alpha$ is an optimization parameter. With Equations \ref{eq:bayes_binary} and \ref{eq:bayes_multi}, we obtain an upper bound of Bayes error, which is negatively correlated with the inter-class feature distances. Specifically, increasing the feature distance will result in a decrease in the upper bound of Bayes error. In this manner, we provide a mathematical intuition for the phenomenon shown in Figure \ref{fig:tsne}. However, Computing the upper bound of Bayes error directly, as indicated above, is challenging due to its computational complexity. To circumvent this issue, we propose a straightforward metric as a substitute, which does not require access to label information.

\begin{figure*}
    \centering
    \includegraphics[width=1.\linewidth]{img/tsne.pdf}
    \vspace{-10pt}
    \caption{t-SNE visualization of feature representation on training and OOD test datasets (CIFAR-10C) with \textit{contrast} corruption under different severity levels. The first left figure shows feature representation of the training dataset, while the rest of three figures illustrate those of the OOD datasets. From this figure, we observe that different clusters tends to be more separated as the corruption severity level gets smaller.}
    \label{fig:tsne}
    % \vspace{-15pt}
\end{figure*}

% A common evaluation of feature quality is to train either a linear classifier or a nearest neighbor classifier over the learned representation using labeled data. 


\subsection{Dispersion score}

In our previous analysis, we show a strong correlation between the test accuracy and feature separability under different distribution shifts. To quantify the separability in the feature space $\mathcal{Z}$, we introduce \emph{Dispersion score} that measures the inter-class margin without annotation information. 

First, we allocate OOD instances $\{\Tilde{\boldsymbol{x}}\}_{i=1}^{m}$ into different clusters $j$ based on the model predictions, i.e., their pseudo labels from the trained classifier $f_{\omega}$: $j=\Tilde{y}^{\prime}_i = \arg\max f_{\omega}(\boldsymbol{z}_i).$
% For each cluster $j$, we calculate the cluster centroid $\Tilde{\boldsymbol{\mu}}_{j}$ using pseudo labels $\Tilde{y}^{\prime}_i$, with $\Tilde{\boldsymbol{\mu}}_{j} = \frac{1}{m_{j}}\sum_{i=1}^{m_{j}} \boldsymbol{z}_i \cdot \mathbbm{1}\{\Tilde{y}^{\prime}_i = j\}  $. Similarly, we obtain the feature center of all instances by $\bar{\boldsymbol{\mu}} = \frac{1}{m}\sum_{i=1}^{m}\boldsymbol{z}_i$.
   % \STATE Calculate the feature center of all instances by $\boldsymbol{\mu} = \frac{1}{m}\sum_{i=1}^{m}\boldsymbol{z}_i$

With these clusters, we compute the \emph{Dispersion score} by the average distances between each cluster centroid $\Tilde{\boldsymbol{\mu}}_j = \frac{1}{m_{j}}\sum_{i=1}^{m_{j}} \boldsymbol{z}_i \cdot \mathbbm{1}\{\Tilde{y}^{\prime}_i = j\}$ and the center of all features $\bar{\boldsymbol{\mu}} = \frac{1}{m}\sum_{i=1}^{m}\boldsymbol{z}_i$, weighted by the sample size of each cluster $m_j$. Formally, the \emph{Dispersion score} is defined as: 
$$
S(\Tilde{\mathcal{D}}) = \frac{1}{k-1} \sum_{j=1}^{k} m_j \cdot \varphi(\bar{\boldsymbol{\mu}}, \Tilde{\boldsymbol{\mu}}_j)
$$
where $k-1$ is the degree of freedom and $\varphi$ denotes the distance function. With the weight $m_j$, the induced score receives a stronger influence from those larger clusters, i.e., the majority classes. This enables our method to be more robust to the long-tailed issue, which naturally arises in the unlabeled OOD data. In subsection~\ref{robust_exp}, we explicitly show the advantage of our method in the class-imbalanced case and analyze the importance of the weight in Appendix~\ref{app:weight}.

In particular, we use square Euclidean distances to calculate the distance in the feature space $\mathcal{Z}$. So it converts to:
\begin{equation}
\label{eq:score}
    S(\Tilde{\mathcal{D}}) = \log \frac{\sum_{j=1}^{k} m_j \cdot \|\bar{\boldsymbol{\mu}} - \Tilde{\boldsymbol{\mu}}_j\|_2^2}{k-1} 
\end{equation}
Following the common practice \citep{jiang2018predicting}, we adopt a log transform on the final score, which corresponds to multiplicative combination of class statistics. 






We summarize our approach in Appendix~\ref{app:algorithm}. Notably, the Dispersion score derived from the feature separability offers several compelling advantages:

\begin{enumerate}[topsep=0pt]
    \item \textbf{Training data free}. The calculation procedure of our proposed score does not rely on the information of training data. Thus, our method is compatible with modern settings where models are trained on billions of images.
    \item \textbf{Easy-to-use}. The computation of the Dispersion score only does forward propagation for each test instance once and does not require extra hyperparameter, training a new model, or updating the model parameters. Therefore, our method is easy to implement in real-world tasks and computational efficient, as demonstrated in Tables~\ref{tab:main 2} and \ref{tab:imbalance}.
    \item \textbf{Strong flexibility in OOD data}. Previous state-of-the-art methods, like ProjNorm \citep{yu2022predicting}, usually requires a large amount of OOD data for predicting the prediction performance. Besides, the class distribution of unlabeled OOD datasets might be severely imbalanced, which makes it challenging to estimate the desired test accuracy on balanced data. In Subsection~\ref{robust_exp}, we will show the Dispersion score derived from the feature separability exhibits stronger flexibility and generality in sample size, class distribution, and partial label set (see Subsection~\ref{robust_exp} and Appendix~\ref{app:results_partial}).
    % \item \textbf{Model-agnostic}. The estimating precedure applies to a variety of model architectures with different capability, including ResNet-18, ResNet-50 \citep{he2016deep}, and WRN-50-2\citep{zagoruyko2016wide}. Moreover, the Dispersion score is also agnostic to the training procedure, such as loss function and optimizer.
\end{enumerate}
\vspace{-8pt}
% \HW{add advantages}



% \begin{equation}
% \label{eq: dispersion}
%     S_{disp} = \log{\frac{\sum_{k=1}^{K}m_{\hat{k}}||\boldsymbol{\hat{\mu}}_{k}-\boldsymbol{\mu}||^2}{K-1}},
% \end{equation}
% Based on the above analysis, we propose Dispersion Score to estimate OOD errors, where the pseudo code could be viewed in Algorithm \ref{alg: dispersion}.

% For the pre-trained process, we need to lean a classification model via the training dataset in a supervised way. After we obtain the pre-trained model as well as the test set without annotation, we firstly feed the dataset into the feature extractor and gain the feature representation:
% \begin{equation}
%     \widetilde{z_i} = f_{g}(\widetilde{x_i}).
% \end{equation}

% Simultaneously, we could also gain their pseudo labels:
% \begin{equation}
% \label{eq: pseudo}
%     \hat{k} =\arg \max_{p} f_{\omega}(\widetilde{z_i}).
% \end{equation}

% Then Dispersion Score could be calculated after we obtain all feature representation and pseudo labels of the test set:
% \begin{equation}
% \label{eq: dispersion}
%     S_{disp} = \log{\frac{\sum_{k=1}^{K}m_{\hat{k}}||\boldsymbol{\hat{\mu}}_{k}-\boldsymbol{\mu}||^2}{K-1}},
% \end{equation}
% where $\boldsymbol{\hat{\mu}}_{k}$ denotes the estimated center of the $k^{th}$ class by pseudo labels, $\boldsymbol{\mu}$ denotes the center of the whole dataset, and $K$ denotes the total number of classes.

% \textbf{Linear regression.} After obtaining Dispersion Score of all test datasets, we could evaluate the score's performance with respective to the ground-true OOD error by linear regression,
% \begin{equation}
%     OodError = \alpha S_{disp} + \beta,
% \end{equation}
% where $\alpha$ and $\beta$ denote parameters of the linear regression model. 



% Compactness Score:  $S_{comp} = - \log{\frac{\sum_{k=1}^{K}\sum_{i=1}^{n_{k}}||\boldsymbol{z}_i-\boldsymbol{\mu}_k||^2}{N-K}}$

% Domain Shift Score: $S_{shift} = - \log{\frac{\sum_{k=1}^{K}||\boldsymbol{\mu}_k-\boldsymbol{\mu}_{k}^{in}|^2}{K-1}}$

% Final Score: $S = S_{disp} + S_{comp} + S_{shift}$

% Compactness Discrepancy: $S_{disp} = \frac{1}{\mathcal{K}-1}\sum_{k=1}^{\mathcal{K}}|\log{\frac{\sum_{i=1}^{n_k}||\boldsymbol{z}_i-\boldsymbol{\mu}_k||^2}{n_k-1}} - \log{\frac{\sum_{i=1}^{n_k^{in}}||\boldsymbol{z}_i^{in}-\boldsymbol{\mu}_k^{in}||^2}{n_k^{in}-1}}|$

% Dispersion Discrepancy: $S_{comp} = \frac{1}{\mathcal{K}-1}\sum_{k=1}^{\mathcal{K}}|\log{n_k||\boldsymbol{\mu}_k-\boldsymbol{\mu}||^2} - \log{n_k^{in}||\boldsymbol{\mu}_k^{in} - \boldsymbol{\mu}^{in}||^2}|$

% Final Score: $S = S_{disp} + S_{comp}$



\section{Experiments} \label{experiment}
%In this section, we verify the effectiveness of Dispersion Score in OOD error estimation with several benchmark datasets.
In this subsection, we first compare the proposed score to existing training-free methods. Then, we provide an extensive comparison between our method and recent state-of-the-art method -- ProjNorm. We then show the flexibility of our method under different settings of OOD test data. Additionally, we present an analysis of using ground-truth labels and K-means in Appendixes~\ref{app:pse} and \ref{app:kmeans}. 
Finally, we discuss the limitation of the proposed score for adversarial setting in Appendix~\ref{app:adversarial}.

\subsection{Experimental Setup}
\textbf{Train datasets.} During training, we train models on the CIFAR-10, CIFAR-100 \citep{krizhevsky2009learning} and TinyImageNet \citep{le2015tiny} datasets. Specifically, the train data of CIFAR-10 and CIFAR-100 contain 50,000 training images, which are allocated to 10 and 100 classes, respectively. The TinyImageNet dataset contains 100,000 64 $\times$ 64 training images, with 200 classes. 

\textbf{Out-of-distribution (OOD) datasets.} To evaluate the effectiveness of the proposed method on predicting OOD error at test time, we use CIFAR-10C and CIFAR-100C \citep{hendrycks2019benchmarking}, which span 19 types of corruption with 5 severity levels. For the testing of TinyImageNet, we use TinyImageNet-C \citep{hendrycks2019benchmarking} that spans 15 types of corruption with 5 severity levels as OOD dataset. All the datasets with certain corruption and severity contain 10,000 images, which are evenly distributed in the classes. 







\textbf{Evaluation metrics.} To measure the linear relationship between OOD error and designed scores, we use coefficients of determination ($R^2$) and Spearman correlation coefficients ($\rho$) as the evaluation metrics. For the comparison of computational efficiency, we calculate the average evaluation time ($T$) for each test set with certain corruption and severity. 

\textbf{Training details.} During the training process, we train ResNet18, ResNet50 \citep{he2016deep} and WRN-50-2 \citep{zagoruyko2016wide} on CIFAR-10, CIFAR-100 \citep{krizhevsky2009learning} and TinyImageNet \citep{le2015tiny} with 20, 50 and 50 epochs, respectively. We use SGD with the learning rate of $10^{-3}$, cosine learning rate decay \citep{loshchilov2016sgdr}, a momentum of 0.9 and a batch size of 128 to train the model. 


\begin{table*}[!t]
    \centering
    \caption{Performance comparison of training free approaches on CIFAR-10, CIFAR-100 and TinyImageNet, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearman correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}. }
    \renewcommand\arraystretch{1.2}
    \resizebox{\textwidth}{!}{
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{cccccccccccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Rotation} &\multicolumn{2}{c}{ConfScore} &\multicolumn{2}{c}{Entropy} &\multicolumn{2}{c}{AgreeScore} &\multicolumn{2}{c}{ATC} &\multicolumn{2}{c}{Fr\'{e}chet} &\multicolumn{2}{c}{Ours}\\
        \cline{3-16}
        & &$R^2$ &$\rho$ &$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$\\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.822 &0.951 &0.869 &0.985 &0.899 &0.987 &0.663 &0.929 &0.884 &0.985 &0.950 &0.971 &\textbf{0.968} &\textbf{0.990}\\
          & ResNet50 &0.835 &0.961 &0.935 &0.993 &0.945 &\textbf{0.994} &0.835 &0.985 &0.946 &\textbf{0.994} &0.858 &0.964 &\textbf{0.987} &0.990\\
          & WRN-50-2 &0.862 &0.976 &0.943 &\textbf{0.994} &0.942 &\textbf{0.994} &0.856 &0.986 &0.947 &\textbf{0.994} &0.814 &0.973 &\textbf{0.962} &0.988\\
          \cline{2-16}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.840} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.963} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.916} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.991} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.930} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.992}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.785} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.967} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.926} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.991} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.874} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.990}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} & ResNet18 &0.860 &0.936 &0.916 &0.985 &0.891 &0.979 &0.902 &0.973 &0.938 &0.986 &0.888 &0.968 &\textbf{0.952} &\textbf{0.988} \\
          & ResNet50 &0.908 &0.962 &0.919 &0.984 &0.884 &0.977 &0.922 &0.982 &0.921 &0.984 &0.837 &0.972 &\textbf{0.951} &\textbf{0.985}\\
          & WRN-50-2 &0.924 &0.970 &0.971 &0.984 &0.968 &0.981 &0.955 &0.977 &0.978 &\textbf{0.993} &0.865 &0.987 &\textbf{0.980} &0.991\\
          \cline{2-16}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.898} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.956} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.936} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.915} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.983} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.927} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.982} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.946} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.864} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.976} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.962}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}}\\
          \midrule
   \multirow{4}{*}{TinyImageNet} & ResNet18 &0.786 &0.946 &0.670 &0.869 &0.592 &0.842 &0.561 &0.853 &0.751 &0.945 &0.826 &0.970 &\textbf{0.966} &\textbf{0.986}\\
          & ResNet50 &0.786 &0.947 &0.670 &0.869 &0.651 &0.892 &0.560 &0.853 &0.751 &0.945 &0.826 &0.971 &\textbf{0.977} &\textbf{0.986}\\
          & WRNt-50-2 &0.878 &0.967 &0.757 &0.951 &0.704 &0.935 &0.654 &0.904 &0.635 &0.897 &0.884 &0.984 &\textbf{0.968} &\textbf{0.986}\\
          \cline{2-16}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.805} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.959} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.727} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.920} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.650} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.890} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.599} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.878} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.693} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.921} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.847} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.976} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.970}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.987}}\\
         \bottomrule
    \end{tabular}}}
    \vspace{-10pt}
    \label{tab:main 1}
\end{table*}



\textbf{The compared methods.} We consider 7 existing methods as benchmarks for predicting OOD error: \textit{Rotation Prediction} (Rotation) \citep{deng2021does}, \textit{Averaged Confidence} (ConfScore) \citep{hendrycks2016baseline}, \textit{Entropy} \citep{guillory2021predicting}, \textit{Agreement Score} (AgreeScore) \citep{jiang2021assessing}, \textit{Averaged Threshold Confidence} (ATC) \citep{garg2022leveraging}, \textit{AutoEval} (Fr\'{e}chet) \citep{deng2021labels}, and \textit{ProjNorm} \citep{yu2022predicting}. Rotation and AgreeScore predict OOD error from the view of unsupervised loss constructed by generating rotating instances and measuring output agreement of two independent models, respectively. ConfScore, Entropy, and ATC formulate scores by model predictions. ProjNorm measures the parameter-level difference between the models fine-tuned on train and OOD test data respectively, while Fr\'{e}chet quantifies the distribution difference in the feature space between the training and test datasets. We present the related works in Appendix~\ref{app:relate}.


\begin{figure*}
    \centering
    \includegraphics[width=1.\linewidth]{img/dispersion_scatter.pdf}
    \vspace{-10pt}
    \caption{OOD error prediction versus True OOD error on CIFAR-10 with ResNet50. We compare the performance of Dispersion Score with that of ProjNorm and Fr\'{e}chet via scatter plots. Each point represents one dataset under certain corruption and certain severity, where different shapes represent different types of corruption, and darker color represents the higher severity level.}
    \label{fig:scatters}
    \vspace{-10pt}
\end{figure*}


\subsection{Results} 

\paragraph{Can Dispersion score outperform existing training-free approaches?}
In Table \ref{tab:main 1}, we present the performance of OOD error estimation on three model architectures and three datasets. We find that Dispersion Score dramatically outperforms existing training-free methods. For example, averaged across three architectures on TinyImageNet, our method leads to an increase of the $R^2$ from 0.847 to 0.970 -- a 14.5$\%$ of relative improvement. In addition, Dispersion Score achieves consistently high performance over the three datasets with a $R^2$ higher than 0.950, while scores of other approaches such as Rotation varying from 0.787 to 0.924 are not stable. We observe a similar phenomenon on $\rho$, where the Entropy method achieves performance that is ranging from 0.842 to 0.994, while the performance of our method fluctuates around 0.988.


\begin{table}[!t]
    \centering
    \caption{Performance comparison between ProjNorm \citep{yu2022predicting} and our Dispersion score on CIFAR-10, CIFAR-100 and TinyImageNet, where $R^2$ refers to coefficients of determination, $\rho$ refers to Spearman correlation coefficients (higher is better), and $T$ refers to average evaluation time (lower is better). The best results are highlighted in \textbf{bold}.}
    \renewcommand\arraystretch{0.9}
    \resizebox{0.98\textwidth}{!}{
    \setlength{\tabcolsep}{4mm}{
    \begin{tabular}{cccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{3}{c}{ProjNorm}  &\multicolumn{3}{c}{Ours}\\
        \cline{3-8}
        & &$R^2$ &$\rho$ &$T$ &$R^2$ &$\rho$ &$T$ \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.936 &0.982 &179.616 &\textbf{0.968} &\textbf{0.990} &\textbf{10.980}\\
         & ResNet50 &0.944 &0.989 &266.099 &\textbf{0.987} &\textbf{0.990} &\textbf{11.259}\\
          & WRN-50-2 &0.961 &\textbf{0.989} &575.888 &\textbf{0.962} &0.988 &\textbf{11.017}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.947} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{326.201} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{11.085}}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} & ResNet18 &\textbf{0.979} &0.980 &180.453 &0.952 &\textbf{0.988} &\textbf{6.997}\\
         & ResNet50 &\textbf{0.988} &\textbf{0.991} &262.831 &0.953 &0.985 &\textbf{11.138}\\
          & WRN-50-2 &\textbf{0.990} &\textbf{0.991} &605.616 &0.980 &\textbf{0.991} &\textbf{12.353}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.985}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{349.63} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.962} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{10.163}}\\
          \midrule
   \multirow{4}{*}{TinyImageNet} & ResNet18 &\textbf{0.970} &0.981 &182.127 &0.966 &\textbf{0.986} &\textbf{7.039}\\
         & ResNet50 &\textbf{0.979} &0.987 &264.651 &0.977 &\textbf{0.990} &\textbf{13.938}\\
          & WRN-50-2 &0.965 &0.983 &590.597 &\textbf{0.968} &\textbf{0.986} &\textbf{11.235}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.984} &\textcolor[rgb]{0.0, 0.53, 0.74}{345.792} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.987}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{10.737}}\\
         \bottomrule
    \end{tabular}}}
    \vspace{-6pt}
    \label{tab:main 2}
\end{table}




\paragraph{Dispersion score is superior to ProjNorm.}
In Table \ref{tab:main 2}, we compare our method to the recent state-of-the-art method -- ProjNrom \citep{yu2022predicting} in both prediction performance and computational efficiency. The results illustrate that Dispersion Score could improve the prediction performance over ProjNorm with a meaningful margin. 
For example, with trained models on CIFAR-10 dataset, using Dispersion score achieves a $R^2$ of 0.953, much higher than the average performance of ProjNorm as $0.873$. On CIFAR-100, our method also achieves comparable (slightly better) performance with ProjNorm (0.961 vs. 0.948). 
Besides, Dispersion score obtains huge advantages to ProjNorm in computational efficiency. Using WRN-50-2, ProjNorm takes an average of 575 seconds for estimating the performance of each OOD test dataset, while our method only requires 11 seconds.
Since the computation of Dispersion score does not needs to update model parameters or utilize the training data, our method enables to predict OOD error with large-scale models trained on billions of images.


% The benefit of computation efficiency is especially important when we use large models as trained models. From this table, using WRN-50-2, ProjNorm consumes 575.888 seconds on average to estimate OOD error on CIFAR-10C, and gets $R^2$ of 0.947 and $\rho$ of 0.987. However, Dispersion Score only consumes 11.085 seconds for every evaluation on average, and gets better results ($R^2$ of 0.972 and $\rho$ of 0.990).  

% Much faster computation speed with comparable estimation performance means that our method could be employed in many practical scenes such as hyperparameter selection and best model determination. 

To further analyze the advantage of our method, we present in Figure~\ref{fig:scatters} the scatter plots for Fr\'{e}chet, ProjNorm and Dispersion Score on CIFAR-10C with ResNet50. From the figure, we find that Dispersion Score estimates OOD errors linearly w.r.t. true OOD errors in all cases. In contrast, we observe that those methods based on distribution difference tends to fail when the classification error is high. This phenomenon clearly demonstrates the reliable and superior performance of the Dispersion score in predicting generalization performance under distribution shifts.

% More scatter plots could be observed in the Appendix.






\subsection{Flexibility in OOD data}
\label{robust_exp}

In previous analysis, we show that Dispersion score can outperform existing methods on standard benchmarks, where the OOD test datasets contain sufficient instances and have a balanced class distribution. In reality, the unlabeled OOD data are naturally imperfect, which may limit the performance of predicting OOD error. In this part, we verify the effectiveness of our method with long-tailed data and small data, compared to ProjNorm \citep{yu2022predicting}.

% Practically, the test set is not always satisfying during evaluation, so the estimation method should handle more general scenarios. In this section, we test the prediction performances of Dispersion Score and ProjNorm under the setting of imbalanced test sets and small test sets to compare evaluation stability.

\begin{figure}[!t]
    \centering
    % \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=3.5cm,width=4.5cm]{img/subset_res18.pdf}
        \caption{ResNet18}
        \label{fig:small_rn18}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=3.5cm,width=4.5cm]{img/subset_res50.pdf}
        \caption{ResNet-50}
        \label{fig:small_rn50}
    \end{subfigure}
    % \hfill
     \caption{Prediction performance $R^2$ vs.~sample size of OOD data (subsets of CIFAR-10C) with (a) ResNet18 and (b) ResNet50.}
     \label{fig:small}
     \vspace{-10pt}
\end{figure}

\paragraph{Class imbalance.} 
We first evaluate the prediction performance under class imbalanced setting. In particular, given a trained model, we aim to estimate its balanced accuracy under distribution shift while we only have access to a long-tailed test data, where a few classes (majority classes) occupy most of the data and most classes (minority classes) are under-represented. We conduct experiments on CIFAR-10C and CIFAR-100C with imbalance rate 100.

% In this part, we conduct experiments on consider the frequency distribution of visual categories is long-trailed, where a few classes are common, while many are rare during evaluation. We are interested in the correlation between the computed scores on imbalanced test datasets and the true OOD error on balanced test datasets. 

Our results in Table~\ref{tab:imbalance} show that Dispersion Score achieves better performance than ProjNorm \citep{yu2022predicting} under class imbalance. For example, our approach achieves an average $R^2$ of 0.953 on CIFAR-10C with 2.780 seconds, while ProjNorm obtains an $R^2$ of 0.873 and takes 398.972 seconds. In addition, the performance of our method is more stable across different model architectures and datasets with the $R^2$ ranging from 0.932 to 0.986 than ProjNorm (0.799 - 0.980). Overall, Dispersion score maintains reliable and superior performance even when the OOD test set are class imbalanced.
% Comparing Table \ref{tab:imbalance} with Table \ref{tab:main 2}, it is also worthy 
% noting that ProjNorm under the imbalanced setting consumes more time than that under the default setting, even though the number of test data is smaller. It is because the average computation time of our approach mainly depends on the size of the test set and the complexity of models, while $T$ of ProjNorm also depends on the current occupation and utilization of GPU and CPU.

% \begin{table}[!t]
%     \centering
%     \caption{Summary of prediction performance on the \textbf{randomly sampled subset} of TinyImageNet, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearman correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}.}
%     \resizebox{0.47\textwidth}{!}{\begin{tabular}{cccccc}
%         \toprule
%         \multirow{2}{*}{Dataset} &\multirow{2}{*}{Sample Number} &\multicolumn{2}{c}{ProjNorm}  &\multicolumn{2}{c}{Dispersion}\\
%         \cline{3-6}
%          & &$R^2$ &$\rho$ &$R^2$ &$\rho$  \\
%         \midrule
%          \multirow{4}{*}{TinyImageNet} &50 &0.114 &0.432 &\textbf{0.610} &\textbf{0.814}  \\
%           \cline{2-6}
%    &100 &0.062 &0.370 &\textbf{0.572} &\textbf{0.798} \\
%            \cline{2-6}
%  &200&0.461 &0.528&\textbf{0.572} &\textbf{0.819}    \\
%           \cline{2-6}
%          &400  &\textbf{0.554} &\textbf{0.772} &0.460 &0.693  \\
%          \bottomrule
%     \end{tabular}}
%     \label{tab: small}
% \end{table}



% \begin{figure}
%     \centering
%     \includegraphics[width=1.\linewidth]{img/subset.pdf}
%     \caption{\textbf{Sampling efficiency test on CIFAR-10C with ResNet18 and ResNet50 for Dispersion Score and ProjNorm.} They show the change of estimation performance w.r.t. the number of samples under ResNet18 and ResNet50 respectively.}
%     \label{fig: small}
% \end{figure}



\begin{table}[!t]
    \centering
    \caption{Summary of prediction performance on \textbf{Imbalanced} CIFAR-10C and CIFAR-100C \citep{hendrycks2019benchmarking}, where $R^2$ refers to coefficients of determination, $\rho$ refers to Spearman correlation coefficients (higher is better), and $T$ refers to average evaluation time (lower is better). The best results are highlighted in \textbf{bold}. More results can be found in Appendix~\ref{app:results_imb}.}
   \renewcommand\arraystretch{0.9}
    \resizebox{0.98\textwidth}{!}{
    \setlength{\tabcolsep}{4mm}{
    \begin{tabular}{cccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{3}{c}{ProjNorm}  &\multicolumn{3}{c}{Ours}\\
        \cline{3-8}
        & &$R^2$ &$\rho$ &$T$ &$R^2$ &$\rho$ &$T$ \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.799 &0.968 &204.900 &\textbf{0.959} &\textbf{0.982} &\textbf{2.076}\\
         & ResNet50 &0.897 &0.980 &430.406 &\textbf{0.968} &\textbf{0.982} &\textbf{3.295}\\
          & WRN-50-2 &0.922 &0.978 &561.611 &\textbf{0.932} &\textbf{0.978} &\textbf{2.970}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.873} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.973} &\textcolor[rgb]{0.0, 0.53, 0.74}{398.972} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.953}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.980}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{2.780}}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} &ResNet18 &0.886 &0.968 &210.05 &\textbf{0.941} &\textbf{0.982} &\textbf{1.864}\\
         & ResNet50 &\textbf{0.980} &\textbf{0.988} &433.860 &0.956 &0.982 &\textbf{2.974}\\
          & WRN-50-2 &0.978 &0.982 &768.883 &\textbf{0.986} &\textbf{0.994} &\textbf{3.242}\\
          \cline{2-8}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.948} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.980} &\textcolor[rgb]{0.0, 0.53, 0.74}{470.931} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.961}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.986}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{2.693}}\\
         \bottomrule
    \end{tabular}}}
    \vspace{-10pt}
    \label{tab:imbalance}
\end{table}

\textbf{Sampling efficiency.} 
During deployment, it can be challenging to collect instances under a specific distribution shift. 
% As introduced in previous work \citep{yu2022predicting}, sample size has become a key factor in estimating generalization performance on OOD data. 
However, existing state-of-the-art methods \citep{yu2022predicting} normally require a sufficiently large test dataset to achieve meaningful results in predicting OOD error. Here, we further validate the sampling efficiency of Dispersion score, compared with ProjNorm.


Figure \ref{fig:small} presents the performance comparison between Dispersion Score and ProjNorm on subsets of CIFAR10C with various sample sizes. The results show that our method can achieve excellent performance even when only 50 examples are available in the OOD data. In contrast, the performance of ProjNorm decreases sharply with the decrease in sample size. The phenomenon shows that Dispersion score is more efficient in exploiting the information of OOD instances.

%\begin{figure}
%    \centering
%    \includegraphics[width=0.8\linewidth]{img/compactness.pdf}
%    \vspace{-10pt}
%    \caption{Compactness vs.~test error on CIFAR-10C with ResNet50. Each point represents a dataset with certain corruption, where shapes represent corruption types and a darker color represents a higher severity level.}
%    \label{fig:compactness}
%    \vspace{-10pt}
%\end{figure}

% \section{Discussion}
% \label{sec:discussion}

% \vspace{-10pt}
\begin{wrapfigure}{r}{0.37\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{img/compactness.pdf}
    \caption{Compactness vs.~test error on CIFAR-10C with ResNet50.}
    \label{fig:compactness}
    \vspace{-15pt}
\end{wrapfigure}

% \paragraph{Compactness.} 
\subsection{Intra-class compactness Vs Inter-class dispersion}
\label{sec:compactness}
In Section~\ref{method}, we empirically show that feature separability is naturally tied with the final accuracy and propose an effective score based on inter-class dispersion. However, the connection between intra-class compactness and generalization performance is still a mystery. In this analysis, we show that compactness is not a good indicator of OOD accuracy. Specifically, we define the compactness score:
 $$S(\Tilde{\mathcal{D}}) = - \log{\frac{\sum_{j=1}^{k}\sum_{i=1}^{m_{j}}||\boldsymbol{z}_i-\Tilde{\boldsymbol{\mu}}_j||^2}{n-k}}$$
where $\Tilde{\boldsymbol{\mu}}_j$ denotes the centroid of the cluster $j$ that the instance $\boldsymbol{z}_i$ belongs to. Therefore, the compactness score can measure the clusterability of the learned features, i.e., the average distance between each instance and its cluster centroid. Intuitively, a high compactness score may correspond to a well-separated feature distribution, which leads to high test accuracy. Surprisingly, in Figure~\ref{fig:compactness}, we find that the compactness score is largely irrelevant to the final OOD error, showing that it is not an effective indicator for predicting generalization performance under distribution shifts.

% \begin{figure}[!t]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[height=3.0cm,width=3.8cm]{img/kmeans_r2.pdf}
%         \caption{Compare with K-means}
%         \label{fig:kmeans}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[height=3.0cm,width=3.8cm]{img/compactness.pdf}
%         \caption{Compactness vs.~test error}
%         \label{fig:compactness}
%     \end{subfigure}
%      \caption{(a) Comparison of Dispersion score and the K-means variant on three datasets. (b) Compactness vs.~test error on CIFAR-10C with ResNet50. Each point represents a dataset with certain corruption, where shapes represent corruption types and a darker color represents a higher severity level.}
%      \vspace{-10pt}
% \end{figure}

% \begin{table}[!t]
%     \centering
%     \caption{\textbf{Comparison} Dispersion Score with pseudo labels and labels via K-means on CIFAR100, where $R^2$ refers to coefficients of determination and $\rho$ refers to Spearman correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}.}
%     \resizebox{0.4\textwidth}{!}{\begin{tabular}{cccccc}
%         \toprule
%         \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Kmeans}  &\multicolumn{2}{c}{Pseudo labels} \\
%         \cline{3-6}
%         & &$R^2$ &$\rho$  &$R^2$ &$\rho$  \\
%         \midrule
%         %  \multirow{4}{*}{CIFAR 10} & ResNet18 &0.968 &\textbf{0.990} &\textbf{0.979} &0.989\\
%         %  & ResNet50 &\textbf{0.987} &0.990 &0.985 &\textbf{0.991}\\
%         %   \cline{2-6}
%         %   & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.989}\\
%         %   \midrule
%     \multirow{3}{*}{CIFAR 100} &ResNet18   &0.936 &0.980 &\textbf{0.952} &\textbf{0.988}\\
%          & ResNet50 &0.938 &0.978  &\textbf{0.953} &\textbf{0.985}\\
%           \cline{2-6}
%          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average}  &\textcolor[rgb]{0.0, 0.53, 0.74}{0.937} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.979} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.962}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}}\\
%          \bottomrule
%     \end{tabular}}
%     \label{tab: kmeans}
% \end{table}






% \textbf{Unsupervised domain adaptation.} \textit{Unsupervised domain adaptation} (UDA) has gained considerable interests in many practical applications recently \citep{chen2020self, tachet2020domain, farahani2021brief, kamnitsas2017unsupervised}, which goal is to transfer useful information learnt from the source domain to the target domain without labels \citep{pan2009survey}. Inspiring the distribution-discrepancy-based methods for OOD error estimation \citep{deng2021labels}, the key to success of UDA is to learn a latent domain-invariant feature representation by narrowing the distribution distance between the two domains with certain criteria, such as maxinum mean discrepancy (MMD) \citep{pan2010domain}, Kullback-Leibler (KL) divergence \citep{zhuang2015supervised}, central moment discrepancy (CMD) \citep{zellinger2017central}, and Wasserstein distance \citep{lee2017minimax}, or with a domain discriminator, such as domain-adversarial neural network (DANN) \citep{ganin2016domain} and Adversarial discriminative domain adaptation (ADDA) \citep{tzeng2017adversarial}. Causal representation learning \citep{magliacane2018domain, yang2021learning, zhang2015multi} are also included in UDA to extract domain-invariant semantic segmentation. Different from methods motivated from UDA that focus on distribution summaries of source and target domains, our method studies the detailed representation of each test dataset to estimate OOD error, which reserves more significantly relevant information than the previous methods. 

% \textbf{OOD detection.} OOD detection is another increasingly important topic for the safe deployment of machine learning models in practice, which aims to identify test samples from a different distribution that the model is never exposed to during the training process. Some works design scoring functions to address this problem \citep{bendale2016towards, hendrycks2016baseline, liang2017enhancing, hsu2020generalized, lee2018simple, sun2021react, huang2021importance}, where those with outlier scores are more likely to be OOD samples. Training-time regularizers have been also widely used in OOD detection \citep{lee2017training, bevandic2018discriminative, katz2022training, liu2020energy, du2022vos}, which encourages the model to give predictions of OOD samples with significantly distinct properties from ID samples. Compared with the OOD detection problem where OOD and ID data are from different label spaces, OOD error estimation methods are always conducted within the same label space. 


% \textbf{Unsupervised domain adaptation.} \textit{Unsupervised domain adaptation} (UDA) has gained considerable interests in many practical applications recently \citep{}. Their goal is to transfer knowable information from one domain to another domain with distribution shift, so that 

\subsection{The importance of the weight}
\label{app:weight}

As introduced in Section \ref{method}, Dispersion Score can be viewed as a weighted arithmetic mean of the distance from centers of each class to the center of the whole samples in the feature space, where the weight is the total number of samples in the corresponding class. To verify the importance of the weight in long-tailed setting, we consider a variant that removes the weight:
$$
S(\Tilde{\mathcal{D}}) = \frac{1}{k-1} \sum_{j=0}^{k} \varphi(\bar{\boldsymbol{\mu}}, \Tilde{\boldsymbol{\mu}}_j).
$$
The results are shown in Table \ref{tab:weight}, where we compare performances of Dispersion score and the variant without weight respectively under \textbf{imbalanced} CIFAR-10C and CIFAR-100C with ResNet10 and ResNet50. From this table, we could observe that the weight enhance the robustness significantly in long-tail conditions.

\begin{table}[ht]
    \centering
    \vspace{-10pt}
    \caption{Summary of prediction performance on \textbf{Imbalanced} CIFAR-10C and CIFAR-100C. The best results are highlighted in \textbf{bold}.}
    % \resizebox{0.8\textwidth}{!}
    \renewcommand\arraystretch{1.2}
    \resizebox{0.8\textwidth}{!}{
    \setlength{\tabcolsep}{5mm}
    {\begin{tabular}{cccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{w/o Weights}  &\multicolumn{2}{c}{w/ Weights}\\
        \cline{3-6}
        & &$R^2$ &$\rho$  &$R^2$ &$\rho$ \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.675 &0.930 &\textbf{0.959} &\textbf{0.982} \\
         & ResNet50 &0.748 &0.948 &\textbf{0.968} &\textbf{0.982} \\
          \cline{2-6}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.712} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.939} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.978}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} \\
          \midrule
    \multirow{4}{*}{CIFAR 100} &ResNet18 &0.595 &0.838 &\textbf{0.941} &\textbf{0.982}\\
         & ResNet50 &0.395 &0.733 &\textbf{0.956} &\textbf{0.982} \\
          \cline{2-6}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.494} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.785} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.952}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.986}} \\
         \bottomrule
    \end{tabular}}}
    \label{tab:weight}
    \vspace{-10pt}
\end{table}

\subsection{K-means vs.~Pseudo labels.}
\label{app:kmeans}

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{img/kmeans_r2.pdf}
    \caption{Compare with K-means.}
    \label{fig:kmeans}
    \vspace{-15pt}
\end{wrapfigure}

While our Dispersion score derived from pseudo labels has demonstrated strong promise, a question arises: \emph{can a similar effect be achieved by alternative clustering methods?} In this ablation, we show that labels obtained by K-means \citep{lloyd1982least, macqueen1967classification} does not achieve comparable performance with pseudo labels obtained from the trained classifier. In particular, we allocate instances into clusters by using K-means instead of pseudo labels from the classifier.

We present the performance comparison of our method and the variant of K-means in Figure \ref{fig:kmeans}. The results show that our Dispersion score performs better than the K-means variant and the gap is enlarged with more classes. On the other hand, the variant of K-means does not require a classifier, i.e., the linear layer in the trained model, which enables to evaluate the OOD performance of representation learning methods, e.g., self-supervised learning. 

\section{Conclusion}

In this paper, we introduce Dispersion score, a simple yet effective indicator for predicting the generalization performance on OOD data without labels. We show that Dispersion score is strongly correlated to the OOD error and achieves consistently better performance than previous methods under various distribution shifts. Even when the OOD datasets are class imbalanced or have limit number of instances, our method maintains a high prediction performance, which demonstrates the strong flexibility of dispersion score. This method can be easily adopted in practical settings. It is straightforward to implement with trained models with various architectures, and does not require access to the training data. Thus, our method is compatible with large-scale models that are trained on billions of images. We hope that our insights inspire future research to further explore the feature separability for predicting OOD error.


\section{Acknowledgements}
This research is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 1 (RG13/22). Hongxin Wei gratefully acknowledges the support of Center for Computational Science and Engineering at Southern University of Science and Technology for our research. Lei Feng is supported by the National Natural Science Foundation of China (Grant No. 62106028), Chongqing Overseas Chinese Entrepreneurship and Innovation Support Program, Chongqing Artificial Intelligence Innovation Center, CAAI-Huawei MindSpore Open Fund, and Openl Community (https://openi.pcl.ac.cn).


\bibliographystyle{plainnat}
\bibliography{neurips_2023}

\newpage
\appendix
\onecolumn


\section{The calculation of Dispersion Score}
\label{app:algorithm}

Our proposed approach, Dispersion Score, can be calculated as shown in Algorithm \ref{alg: dispersion}.

\begin{algorithm}[ht]
   \caption{OOD Error Estimation via Dispersion Score}
   \label{alg: dispersion}
\begin{algorithmic}
   \STATE {\bfseries Input:} OOD test dataset $\Tilde{\mathcal{D}}=\{\Tilde{x}_i\}_{i=1}^{m}$, a trained model $f$ (feature extractor $f_{g}$ and classifier $f_{\omega}$)
   \STATE {\bfseries Output:} The dispersion score
   \FOR{each OOD instance $\Tilde{x}_i$}
   \STATE Obtain feature representation via $\Tilde{\boldsymbol{z}}_i = f_{g}(\Tilde{\boldsymbol{x}_i})$.
   \STATE Obtain pseudo labels via $\Tilde{y}^{\prime}_i = \arg\max f_{\omega}(\boldsymbol{z}_i)$
   \ENDFOR
   \STATE Calculate cluster centroids $\{\Tilde{\boldsymbol{\mu}}_{j}\}_{j=1}^{k}$ using pseudo labels $\Tilde{y}^{\prime}_i$, with $\Tilde{\boldsymbol{\mu}}_{j} = \frac{1}{m_{j}}\sum_{i=1}^{m_{j}} \boldsymbol{z}_i \cdot \mathbbm{1}\{\Tilde{y}^{\prime}_i = j\}  $
   \STATE Calculate the feature center of all instances by $\bar{\boldsymbol{\mu}} = \frac{1}{m}\sum_{i=1}^{m}\boldsymbol{z}_i$
   \STATE Calculate Dispersion Score $S(\Tilde{\mathcal{D}})$ via Equation~(\ref{eq:score})
\end{algorithmic}
\end{algorithm}
\vspace{-10pt}

\section{Related work}
\label{app:relate}

\textbf{Predicting generalization.} Since the generalization capability of deep networks under distribution shifts is a mysterious desideratum, a surge of researches pay attention to estimate the generalization capability from two directions. 

1) Some works aim to measure generalization gap between training and test accuracy with only training data \citep{corneanu2020computing, jiang2018predicting, neyshabur2017exploring, unterthiner2020predicting, yak2019towards, martin2020heavy}. For example, the model-architecture-based method \citep{corneanu2020computing} summarizes the persistent topological map of a trained model to formulate its inner-working function, which represents the generalization gap. Margin distribution \citep{jiang2018predicting} measures the gap by gauging the distance between training examples and the decision boundary. However, those methods are designed for the identical distribution between the training and test dataset, being vulnerable to distribution shift. 
% aim to measure generalization gap between training and test accuracy under distribution shift by training data


2) Some studies try to estimate generalization performance on a specific OOD test dataset without annotation during evaluation. Many of them utilize softmax outputs of the shifted test dataset to form a quantitative indicator of OOD error \citep{guillory2021predicting, jiang2021assessing, guillory2021predicting, garg2022leveraging}. However, those methods are unreliable across diverse distribution shifts due to the overconfidence problem \citep{wei2022mitigating}. Another popular direction considers the negative correlation between distribution difference and model's performance in the space of features \citep{deng2021labels} or parameters \citep{yu2022predicting}. Nevertheless, common distribution distances practically fail to induce stable error estimation under distribution shift \citep{guillory2021predicting}, and those methods are usually computationally expensive. Unsupervised loss such as agreement among multiple classifiers \citep{jiang2021assessing, madani2004co, platanios2016estimating, platanios2017estimating} and data augmentation \citep{deng2021does} is also employed for OOD error prediction, which requires specific model structures during training. In this work, we focus on exploring the connection between feature separability and generalization performance under distribution shift, which is training-free and does not have extra requirements for datasets and model architectures.

\textbf{Exploring Feature distribution in deep learning.} In the literature, feature distribution has been widely studied in domain adaptation \citep{ben2006analysis, pan2010domain, zhuang2015supervised, tzeng2017adversarial}, representation learning \citep{bengio2013representation, haochen2021provable, ming2023cider, huang2021towards}, OOD generalization \citep{li2018deep, chen2021style, wang2021learning}, and noisy-label learning \citep{zhu2021clusterability, zhu2022detecting}. Domain adaptation methods usually learn a domain-invariant feature representation by narrowing the distribution distance between the two domains with certain criteria, such as maxinum mean discrepancy (MMD) \citep{pan2010domain}, Kullback-Leibler (KL) divergence \citep{zhuang2015supervised}, central moment discrepancy (CMD) \citep{zellinger2017central}, and Wasserstein distance \citep{lee2017minimax}. InfoNCE \citep{huang2021towards} shows a key factor of contrastive learning that the distance between class centers should be large enough. In learning with noisy labels, it has been shown that the feature clusterability can be used to estimate the transition matrix \citep{zhu2021clusterability}. To the best of our knowledge, we are the first to analyze the connection between feature separability and the final accuracy on OOD data.






\section{Sensitivity analysis: pseudo labels}
\label{app:pse}
Here, we conduct a sensitivity analysis by using ground-truth labels in our method. Table~\ref{tab: true_labels} illustrates that the performance with pseudo labels is comparable with the performance using ground-truth labels. This phenomenon is consistent with the previous method - ProjNorm \citep{yu2022predicting}, shown in Table 8 of their paper. 
The reason behind this could be that the trained model is capable of identifying certain semantic information from most corrupted examples, thereby retaining their representations in a cluster. Thus, the separability of feature clusters can serve as an indicator of the final prediction performance for corruption perturbations. Additionally, we provide a failure case of feature clusters separability for adversarial perturbations in Appendix~\ref{app:adversarial}.

\begin{table}[ht]
    \centering
    \caption{Comparison of Dispersion Score with pseudo labels and true labels on CIFAR10, CIFAR100 and TinyImageNet. The best results are highlighted in \textbf{bold}.}
    \renewcommand\arraystretch{1.2}
    \resizebox{0.8\textwidth}{!}{
    \setlength{\tabcolsep}{5mm}
     {\begin{tabular}{cccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Pseudo labels}  &\multicolumn{2}{c}{True labels}\\
        \cline{3-6}
        & &$R^2$ &$\rho$  &$R^2$ &$\rho$  \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.968 &\textbf{0.990} &\textbf{0.979} &0.989\\
         & ResNet50 &\textbf{0.987} &0.990 &0.985 &\textbf{0.991}\\
          & WRN-50-2 &\textbf{0.961} &\textbf{0.988} &0.945 &0.987\\
          \cline{2-6}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.989}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} &ResNet18  &\textbf{0.952} &0.988 &0.915 &\textbf{0.989}\\
         & ResNet50 &0.953 &0.985 &\textbf{0.959} &\textbf{0.989}\\
          & WRN-50-2 &\textbf{0.980} &0.991 &0.978 &\textbf{0.995}\\
          \cline{2-6}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.962}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.988} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.950} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.991}}\\
         \midrule
         \multirow{4}{*}{TinyImageNet} &ResNet18 &\textbf{0.966} &\textbf{0.986} &0.937 &0.985\\
         & ResNet50 &\textbf{0.977} &0.990 &0.954 &\textbf{0.995}\\
          & WRN-50-2 &0.968 &0.986 &\textbf{0.977} &\textbf{0.994}\\
          \cline{2-6}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.970}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.956} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.991}}\\
         \bottomrule
    \end{tabular}}}
    \label{tab: true_labels}
    \vspace{-10pt}
\end{table}



\section{More results on class-imbalance settings}
\label{app:results_imb}
This section provides elaborated outcomes of training-free benchmarks under the setting of class imbalance, serving as a complement to the results presented in Table \ref{tab:imbalance}.  



\begin{table}[!ht]
    \centering
    \caption{Summary of prediction performance on \textbf{Imbalanced} CIFAR-10C and CIFAR-100C for training-free benchmarks, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearman correlation coefficients (higher is better)}
    \renewcommand\arraystretch{1.2}
    \resizebox{\textwidth}{!}{
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{ccccccccccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Rotation} &\multicolumn{2}{c}{ConfScore} &\multicolumn{2}{c}{Entropy} &\multicolumn{2}{c}{AgreeScore} &\multicolumn{2}{c}{ATC} &\multicolumn{2}{c}{Fr\'{e}chet} \\
        \cline{3-14}
        & &$R^2$ &$\rho$ &$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$\\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.767 &0.922 &0.823 &0.965 &0.841 &0.969 &0.669 &0.922 &0.830 &0.966 &0.966 &0.983 \\
          & ResNet50 &0.787 &0.946 &0.870 &0.975 &0.887 &0.977 &0.765 &0.953 &0.883 &0.975 &0.916 &0.975\\
         & WRN-50-2 &0.829 &0.968 &0.915 &0.986 &0.913 &0.986 &0.823 &0.972 &0.922 &0.986 &0.866 &0.977\\
          \cline{2-14}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.794} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.945} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.869} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.976} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.880} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.977} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.752} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.949} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.878} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.976} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.916} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.979}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} & ResNet18 &0.769 &0.944 &0.872 &0.988 &0.840 &0.985 &0.858 &0.979 &0.905 &0.988 &0.905 &0.972 \\
          & ResNet50 &0.847 &0.964 &0.875 &0.986 &0.826 &0.978 &0.832 &0.973 &0.880 &0.986 &0.855 &0.979\\
          & WRN-50-2 &0.930 &0.981 &0.976 &0.993 &0.980 &0.993 &0.944 &0.981 &0.981 &0.994 &0.889 &0.988\\
         
          \cline{2-14}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.849} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.963} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.908} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.989} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.882} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.985} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.878} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.978} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.922} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.989} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.883} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.980}\\
         \bottomrule
    \end{tabular}}}
    \vspace{-10pt}
    \label{tab:imbalance2}
\end{table}





\section{Partial OOD error prediction}
\label{app:results_partial}
In previous experiments, a common assumption is that the test set contains instances from all classes. To further explore the flexibility of our method on OOD test set, we introduce a new setting called \textit{partial OOD error prediction}, where the label space for the test set is a subset of the label space for the training data.

% releases the fully shared label space assumption to the training label space subsumes the test label space, which is  here. 

Here, we train ResNet18 and ResNet50 on both CIFAR-10 and CIFAR-100 with 10 and 100 categories, respectively. Different from previous settings, we evaluate the prediction performance on CIFAR-10C and CIFAR-100C with the first 50\% of categories. The numerical results are shown in Tabel \ref{tab:partial}.
\begin{table}[ht]
    \centering
    \caption{Summary of prediction performance on \textbf{partial sets} of CIFAR-10C and CIFAR-100C, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearman correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}.}
    % \resizebox{1.\textwidth}{!}
    \renewcommand\arraystretch{1.2}
    \resizebox{1.\textwidth}{!}{
    \setlength{\tabcolsep}{1mm}
    {\begin{tabular}{cccccccccccccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Rotation}  &\multicolumn{2}{c}{ConfScore} &\multicolumn{2}{c}{Entropy} &\multicolumn{2}{c}{AgreeScore} &\multicolumn{2}{c}{ATC} &\multicolumn{2}{c}{Frechet} &\multicolumn{2}{c}{ProjNorm} &\multicolumn{2}{c}{Ours}\\
        \cline{3-18}
        & &$R^2$ &$\rho$  &$R^2$ &$\rho$ &$R^2$ &$\rho$ &$R^2$ &$\rho$ &$R^2$ &$\rho$ &$R^2$ &$\rho$ &$R^2$ &$\rho$ &$R^2$ &$\rho$\\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.578 &0.896 &0.795 &0.982 &0.826 &0.984 &0.615 &0.931 &0.802 &0.981 &0.842 &0.941 &0.770 &0.968 &\textbf{0.935} &\textbf{0.985} \\
         & ResNet50 &0.719 &0.939 &0.885 &\textbf{0.993} &0.892 &\textbf{0.993} &0.787 &0.976 &0.887 &\textbf{0.993} &0.757 &0.953 &0.856 &0.967 &\textbf{0.950} &0.992\\
          \cline{2-18}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.649} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.918} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.841} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.859} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.989}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.701} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.954} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.845} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.800} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.947} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.813} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.968} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.942}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.988}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} &ResNet18 &0.876 &0.946 &0.922 &0.985 &0.902 &0.980 &0.904 &0.971 &\textbf{0.943} &\textbf{0.986} &0.894 &0.972 &0.770 &0.968 &0.935 &0.985\\
         & ResNet50 &0.923 &0.967 &0.917 &0.980 &0.890 &0.975 &0.915 &0.976 &0.932 &0.983 &0.837 &0.978 &0.856 &0.967 &\textbf{0.950} &\textbf{0.992} \\
          \cline{2-18}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.899} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.956} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.920} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.983} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.896} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.978} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.909} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.973} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.938}&\textcolor[rgb]{0.0, 0.53, 0.74}{0.984} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.866} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.975} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.813} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.968} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.942}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.989}} \\
         \bottomrule
    \end{tabular}}}
    \label{tab:partial}
    \vspace{-10pt}
\end{table}

From the results, we could observe that our method is more robust than existing methods in the setting of partial OOD error prediction. For example, ProjNorm suffers from the incomplete test dataset during the self-training process, with a dramatic drop from around 0.950 to around 0.810 for $R^2$ of CIFAR-10C and CIFAR-100C on average. Contrastively, our method still achieves high accuracy in predicting OOD errors, maintaining an average $R^2$ value of 0.950.

\section{Adversarial vs. Corruption robustness.}
\label{app:adversarial}

In previous analysis, we show the superior performance of dispersion score on predicting the accuracy on OOD test sets with different corruptions. Here, we surprisingly find that feature dispersion can effectively demonstrate the difference between adversarial and corruption robustness. 
% Currently, we have demonstrated the OOD error prediction performance of our method on common perturbation robustness. In this section, we would like to further test the robustness to against adversarial examples via exploring the links between the two kinds of robustness.


\begin{table}[h]
    \centering
    \caption{Prediction performance measured by MSE against adversarial attack of different methods. The linear regression model is estimated on CIFAR-10C, and is used to predict the adversarial examples with perturbation size $\epsilon$ varying from 0.25 to 8.0. ``True Dispersion'' refers to the dispersion score with feature normalization using ground-truth labels. The best results are highlighted in \textbf{bold}.}
    
     % \resizebox{0.4\textwidth}{!}{
     \begin{tabular}{ccccccc}
        \toprule
        &ConfScore &Entropy  &ATC &ProjNorm &Dispersion &True Dispersion\\
        \midrule
        CIFAR-10 &0.933 &0.892 &0.906 &0.847 &1.359 &\textbf{0.483}\\
         \bottomrule
    \end{tabular}
    % }
    \label{tab:adv}
\end{table}


Table~\ref{tab:adv} shows the prediction performance of different methods under adversarial attacks. ``True Dispersion" refers to the dispersion score with feature normalization using ground-truth labels. In particular, we generate adversarial samples attacked by projected gradient descent (PGD) using untargeted attack \citep{kurakin2016adversarial} on the test set of CIFAR-10 with 10 steps and perturbation size $\epsilon$ ranging from 0.25 to 8.0. While the vanilla Dispersion score leads to poor performance, we note that the variant of Dispersion score with ground-truth labels performs much better than previous methods. This phenomenon is different from the conclusion of the sensitivity analysis of pseudo labels in predicting corruption robustness (See Appendix~\ref{app:pse}), where the variant of true labels cannot outperform our method.



To understand the reasons behind the performance disparity of feature dispersion between adversarial and corruption robustness, we present the t-SNE visualization of features for adversarial attack of CIFAR-10 test set with various perturbation sizes in Figure~\ref{fig:tsne_adversarial}. Compared to Figure~\ref{fig:scatters}, the results indicate that adversarial perturbations increase the distance between different clusters, whereas corruption perturbations decrease the separability of the clusters. In other words, adversarial perturbations decrease the test accuracy in a different way: assigning instances to the wrong groups and enlarging the distance among those groups. Therefore, feature dispersion using pseudo labels cannot be an effective method in the adversarial setting. We hope this insight can inspire specific designed methods based on feature dispersion for predicting adversarial errors in the future.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[height=2.6cm,width=3.0cm]{img/adv_dispersion_scatter0.pdf}
        \caption{$\epsilon$=0.0 $\|$ pseudo}
        \label{fig:tsne_adversarial_0}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[height=2.6cm,width=3.0cm]{img/adv_dispersion_scatter2.pdf}
        \caption{$\epsilon$=2.0 $\|$ pseudo}
        \label{fig:tsne_adversarial_2}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[height=2.6cm,width=3.0cm]{img/adv_dispersion_scatter8.pdf}
        \caption{$\epsilon$=8.0 $\|$ pseudo}
        \label{fig:tsne_adversarial_8}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[height=2.6cm,width=3.0cm]{img/adv_dispersion_true_scatter8.pdf}
        \caption{$\epsilon$=8.0 $\|$ true}
        \label{fig:tsne_adversarial_true}
    \end{subfigure}

     \caption{t-SNE visualization of feature representation on adversarial attack of CIFAR-10 test set with perturbation size $\epsilon$ ranging from 0.25 to 8.0.}
     \label{fig:tsne_adversarial}
\end{figure}


% The reason behind the phenomenon is because adversarial attack and common perturbation attack try to make the classification task harder in an \textbf{opposite} way regarding change in the feature space. In the existing literature, adversarial robustness measures the worst-case performance on small artificial perturbation, while corruption robustness measures the average-case performance \citep{hendrycks2019benchmarking}. The measurement discrepancy leads to the different representation trend in the feature space: adversarial perturbation aims to pull wrong groups far from each other to fool the model, while corruption perturbation aims to push all groups together for vaguer classification boundary (Compare Figure \ref{fig:tsne_adversarial} with Figure \ref{fig:scatters}). 

% % In details, we generate attack samples via projected gradient descent (PGD) on CIFAR-10 test set with 10 steps and perturbation size $\epsilon$ from 0.25 to 8.0. we present the scatter plots of Dispersion, Dispersion with ground-truth labels, ConfScore and ProjNorm in Figure \ref{fig:adversarial}. It shows that Dispersion, ConfScore and ProjNorm are all under-estimate the OOD test error, but Dispersion with true labels could predict it reasonably especially when the perturbation budget is small.

% % This is caused by the difference between adversarial perturbation and common perturbation. Adversarial robustness measures the worst-case performance on small artificial perturbation which aims to fool deep neural networks, while corruption robustness measures the average-case performance \citep{hendrycks2019benchmarking}. It leads to the different representation trend in the feature space, which is shown in Figure \ref{fig:tsne_adversarial}.



% % Compare this figure with Figure \ref{fig:scatters}, we could observe that adversarial perturbation aims to pull wrong groups far from each other in order to fool the neural networks, while corruption perturbation aims to push groups together in order to making classification boundary vague.
% \begin{figure}[!t]
%     \centering
%     % \begin{subfigure}[b]{0.24\textwidth}
%     %     \centering
%     %     \includegraphics[height=2.6cm,width=3.0cm]{img/adv_conf_scatter.pdf}
%     %     \caption{ConfScore}
%     %     \label{fig:frechet_gap}
%     % \end{subfigure}
%     % % \hfill
%     % \begin{subfigure}[b]{0.24\textwidth}
%     %     \centering
%     %     \includegraphics[height=2.6cm,width=3.0cm]{img/adv_projnorm_scatter.pdf}
%     %     \caption{ProjNorm}
%     %     \label{fig:mmd_gap}
%     % \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[height=3.0cm,width=3.8cm]{img/adv_dispersion_scatter.pdf}
%         \caption{Dispersion}
%         \label{fig:mmd_gap}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[height=3.0cm,width=3.8cm]{img/adv_dispersion_true_scatter.pdf}
%         \caption{True Dispersion}
%         \label{fig:mmd_gap}
%     \end{subfigure}
%      \caption{OOD error prediction scatter plots under adversarial attack of our method and our refined method. "Dispersion2" refers to the dispersion score with normalization using ground-truth labels. Blue points are prediction results on CIFAR-10C, while yellow points are results on CIFAR-10 test set with adversarial attack for diverse perturbation size $\epsilon$.}
%      \label{fig:adversarial}
% \end{figure}
% As the existence of robustness difference, our current method cannot perform well. To further prove our hypothesis, we change the pseudo labels of our method with ground-truth labels and normalize the feature space to limit its range. From Figure \ref{fig:tsne_adversarial_true}, we can see that our performance is improved. Tabel \ref{tab:mse} illustrates the detailed numerical results of the adversarial error prediction performance, which also represents that our method can be improved after considering the difference of robustness.

% Here, we propose a revision direction of our method, which could be further explored for future work. In details, we replace the pseudo labels used in our method with the ground truth, and normalize the feature space to limit its range. Figure \ref{fig:tsne_adversarial_true} illustrates our method's performance of the original and revised version. From the figure, we observe that our revised method could make non-trivial predictions especially for small perturbation size. Table \ref{tab:mse} also illustrates the revision direction works well by reducing MSE from 1.359 to 0.483.
\section{Performance on realistic datasets}
To verify the effectiveness of Dispersion Score on realistic datasets, we conduct experiments on PACS \citep{li2017deeper}, Office-31 \citep{saenko2010adapting} and Office-Home \citep{venkateswara2017deep} with ResNet-18, ResNet-50 and WRN-50-2 using normalization. Table \ref{tab:real_data} is the numerical results, from which we can observe that our method outperforms the other baselines on datasets with natural shifts.

\begin{table*}[ht]
    \centering
    \caption{Performance comparison of all approaches on PACS, Office-31 and Office-Home, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearman correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}. }
    \renewcommand\arraystretch{1.5}
    \resizebox{\textwidth}{!}{
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{cccccccccccccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Rotation} &\multicolumn{2}{c}{ConfScore} &\multicolumn{2}{c}{Entropy} &\multicolumn{2}{c}{AgreeScore} &\multicolumn{2}{c}{ATC} &\multicolumn{2}{c}{Fr\'{e}chet} &\multicolumn{2}{c}{ProjNorm} &\multicolumn{2}{c}{Dispersion}\\
        \cline{3-18}
        & &$R^2$ &$\rho$ &$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$  &$\rho$&$R^2$ &$\rho$\\
        \midrule
         \multirow{4}{*}{PACS} & ResNet18 &0.823	&\textbf{0.895}	&0.595	&0.755	&0.624	&0.755	&0.624	&0.832	&0.514	&0.650	&0.624	&0.804	&0.161	&0.420	&\textbf{0.843}	&0.846\\
          & ResNet50 &\textbf{0.861}	&\textbf{0.923}	&0.071	&0.070	&0.062	&0.056	&0.463	&0.622	&0.192	&0.266	&0.463	&0.622	&0.245	&0.587	&0.827	&0.867\\
          & WRN-50-2 &0.865	&0.902	&0.646	&0.678	&0.629	&0.671	&0.377	&0.858	&0.753	&0.832	&0.558	&0.832	&0.475	&0.650	&\textbf{0.896}	&\textbf{0.937}\\
          \cline{2-18}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.850} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.907} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.437} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.501} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.438} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.494}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.488} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.771} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.486} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.583} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.549} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.753} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.294} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.552} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.855}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.883}}\\
          \midrule
    \multirow{4}{*}{Office-31} &ResNet18 &0.753	&0.943	&0.470	&0.829	&0.322	&0.714	&0.003	&0.086	&\textbf{0.844}	&\textbf{0.943}	&0.144	&0.257	&0.099	&0.429	&0.834	&\textbf{0.943}\\
          & ResNet50 &0.371	&0.829	&0.486	&0.829	&0.355	&0.829	&0.012	&0.464	&0.533	&0.486	&0.035	&0.257	&0.241	&0.429	&\textbf{0.878}	&\textbf{0.943}\\
          & WRN-50-2 &0.578	&0.600	&0.525	&0.714	&0.425	&0.714	&0.003	&0.257	&0.405	&0.943	&0.035	&0.143	&0.147	&0.143	&\textbf{0.798}	&\textbf{0.829}\\
          \cline{2-18}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.567} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.790} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.936} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.494} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.790} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.367} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.752} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.006} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.269} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.594}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.790} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.219} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.162} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.333} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.836}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.905}}\\
          \midrule
   \multirow{4}{*}{Office-Home} & ResNet18 &\textbf{0.823}	&\textbf{0.930}	&0.795	&0.909	&0.762	&0.881	&0.055	&0.147	&0.571	&0.615	&0.606	&0.755	&0.065	&0.203	&0.821	&0.811\\
          & ResNet50 &\textbf{0.851}	&\textbf{0.944}	&0.770	&0.895	&0.742	&0.853	&0.027	&0.217	&0.487	&0.734	&0.607	&0.685	&0.169	&0.476	&0.841	&0.860\\
          & WRNt-50-2 &0.823	&0.958	&0.742	&0.874	&0.696	&0.846	&0.132	&0.406	&0.384	&0.643	&0.589	&0.706	&0.173	&0.531	&\textbf{0.897}	&\textbf{0.937}\\
          \cline{2-18}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.832} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.944} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.769} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.893} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.734} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.860} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.071} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.256} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.481} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.664} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.601} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.716} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.135} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.403} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.853}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.869}}\\
         \bottomrule
    \end{tabular}}}
    \vspace{-10pt}
    \label{tab:real_data}
\end{table*}



\end{document}