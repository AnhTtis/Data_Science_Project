%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multicol}
\usepackage{multirow}
\usepackage{subcaption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage[dvipsnames]{xcolor}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}
% \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{setspace}
% Use the following line for the initial blind version submitted for review:

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Feature Separability for Predicting Out-Of-Distribution Error}

\begin{document}

\twocolumn[
\icmltitle{On the Importance of Feature Separability \\ in Predicting Out-Of-Distribution Error}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Renchunzi Xie}{yyy}
\icmlauthor{Hongxin Wei}{yyy}
\icmlauthor{Yuzhou Cao}{yyy}
\icmlauthor{Lei Feng}{sch}
\icmlauthor{Bo An}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{School of Computer Science and Engineering, Nanyang Technological University, Singapore}
% \icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{College of Computer Science, Chongqing University, China}

\icmlcorrespondingauthor{Hongxin Wei}{hongxin001@e.ntu.edu.sg}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground truth labels. 
While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability, and propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.
\end{abstract}
\section{Introduction}
% Estimating the true accuracy of a model on an unseen dataset is meaningful for hyperparameter selection, model adjustment and many other realistic applications, where true accuracy refers to the probability of the model outputs agreeing with the ground truth. Early methods mostly are supervised \cite{gareth2013introduction, hastie2009elements}, which are expensive to collect the validation dataset and vulnerable to attack by distribution shift. To alleviate this problem, an incremental number of researchers focus on predicting the accuracy of out-of-distribution (OOD) data in an unsupervised way. 

Machine learning techniques deployed in the open world often struggle with distribution shifts, where the test data are not drawn from the training distribution. 
Such issues can substantially degrade the test accuracy, and the generalization performance of a trained model may vary significantly on different shifted datasets \cite{quinonero2008dataset, koh2021wilds}.
This gives rise to the importance of estimating out-of-distribution (OOD) errors for AI Safety \cite{deng2021labels}. However, it is prohibitively expensive or unrealistic to collect large-scale labeled examples for each shifted testing distribution encountered in the wild. Subsequently, predicting OOD error becomes a challenging task without access to ground truth labels.



% Machine learning techniques deployed in the open world often struggle with the lack of annotation information during evaluation to estimate the generalization performance on an unseen dataset. The exist of distribution shift, where the test data are drawn from the distribution far from the training data, gives rise to the importance of estimating out-of-distribution (OOD) errors for AI Safety \cite{deng2021labels}, since it invalidates traditional estimation methods such as validation sets \cite{gareth2013introduction, hastie2009elements} because of significant variation in accuracy \cite{quinonero2008dataset, koh2021wilds}. Thus, we investigate the problem of estimating generalization performance without labels under domain shift.

% Model evaluation is an independent and central part for machine learning \cite{platanios2017estimating} and many other fields such as hyperparameter selection and best model selection. Since true labels of the test dataset are not always available in many practical cases, estimating the true accuracy of a model on an unseen set is important. When the training and the test datasets are from the same distribution, this problem could be easily solved by a validation set \cite{gareth2013introduction, hastie2009elements}. However, it becomes challenging when there is distribution shift between the two datasets, as the validation set split from the training set cannot exactly represent the test set under this setting.

In the literature, a popular direction in predicting OOD error is to utilize the model output on the shifted dataset \cite{jiang2021assessing, guillory2021predicting, garg2022leveraging}, which heavily relies on the model calibration. Yet, machine learning models generally suffer from the overconfidence issue even for OOD inputs \cite{wei2022mitigating}, leading to suboptimal performance in estimating OOD performance. 
Many prior works turned to measuring the distribution difference between training and OOD test set, due to the conventional wisdom that a higher distribution shift normally leads to lower OOD accuracy.
AutoEval \cite{deng2021labels} applies Fr\'{e}chet Distance to calculate the distribution distance for model evaluation under distribution shift.
A recent work \cite{yu2022predicting} introduces Projection Norm (ProjNorm) metric to measure the distribution discrepancy in network parameters. 
% As it requires to re-train a new model on the OOD data, ProjNorm is computationally expensive and needs a large number of samples on the shifted distribution. 
However, we find that the connection between distribution distance and generalization performance does not always hold, making these surrogate methods to be questionable. 
%remains controversial
This motivates our method, which directly estimates OOD accuracy based on the feature properties of test instances.




% To solve this problem, a direct thinking is to utilize the model logit \cite{jiang2021assessing, garg2022leveraging, guillory2021predicting, hendrycks2016baseline}. However, they suffer from the overconfidence problem even for OOD inputs \cite{wei2022mitigating}, causing their designed scores are less reliable. Another research direction uses unsupervised loss to estimate the supervised error \cite{}. Nevertheless, they usually have specific requirement on model structure, making them less efficient and less general. ProjNorm \cite{yu2022predicting}, a self-training-based method, could avoid those issues via measuring the parameter discrepancy of two models trained on the training data and the pseudo-labeled test data, respectively. However, it consumes considerable computing resources and is sensitive to test set conditions such as distributions and sample numbers. Considering this problem from the feature representation level will not face those issues, but the current method \cite{} only focuses on the difference between training and test sets, which is insufficient to describe error caused by distribution shift. Naturally, it raises a question: if there exists a method from the view of feature representation that could estimate model's performance accurately, quickly and robustly? 

In this work, we show that feature separability maintains a strong connection with test accuracy even when it encounters distribution shifts. 
% To quantify the feature separability, we introduce a simple dataset-level statistic, Dispersion score, which gauges the inter-class divergence from feature representations, i.e., outputs of feature extractor. Our method is motivated by the desirable properties of embeddings in representation learning \cite{bengio2013representation}. To achieve high accuracy, we generally desire embeddings that from different categories are associated with separated manifolds (i.e., high inter-class dispersion).  
To quantify the feature separability, we introduce a simple dataset-level statistic, Dispersion Score, which gauges the inter-class divergence from feature representations, i.e., outputs of feature extractor. Our method is motivated by the desirable properties of embeddings in representation learning \cite{bengio2013representation}. To achieve high accuracy, we generally desire embeddings where different classes are relatively far apart (i.e., high inter-class dispersion), and samples in each class form a compact cluster (i.e., high intra-class compactness). Surprisingly, our analysis shows that intra-class compactness does not reflect the generalization performance, while inter-class dispersion is strongly correlated with the model accuracy on OOD data.


Extensive experiments demonstrate the superiority of Dispersion Score over existing methods for estimating OOD error. First, our method dramatically outperforms existing training-free methods in evaluating model performance on OOD data. For example, our method leads to an increase of the $R^2$ from 0.847 to 0.970 on TinyImageNet-C -- a 14.5$\%$ of relative improvement. Compared to the recent ProjNorm method \cite{yu2022predicting}, our method not only achieves superior performance by a meaningful margin, but also maintains huge advantages in computational efficiency and sample efficiency. For example, using CIFAR-10C dataset as OOD data, Dispersion Score achieves an $R^2$ of 0.972, outperforming that of ProjNorm (i.e., 0.947), while our approach only takes around 3\% of the time consumed by ProjNorm.

Overall, using Dispersion Score achieves strong performance in OOD error estimation with high computational efficiency. Our method can be easily adopted in practice. It is straightforward to implement with deep learning models and does not require access to training data. Thus our method is compatible with large-scale models that are trained on billions of images.
%It is straightforward to implement with deep learning models and does not require access to training data, which enables to be compatible with large scale model trained with billions of images.

We summarize our main contribution as follows:

\begin{enumerate}
    \item We find that the correlation between distribution distance and generalization performance does not always hold, downgrading the reliability of existing distance-based methods.
    \item We propose a simple dataset-level score that gauges the inter-class dispersion from feature representations, i.e., outputs of feature extractor. Besides, we show that intra-class compactness does not reflect the generalization performance under distribution shifts (Section~\ref{sec:discussion}). 
    % \item We show that intra-class compactness does not reflect the generalization performance under distribution shifts. 
    \item We conduct extensive evaluations to show the superiority of the Dispersion Score in both prediction performance and computational efficiency. Our analysis shows that Dispersion Score is more robust to various data conditions in OOD data, such as limited sample size and class imbalance.
\end{enumerate}

% (TODO) In this work, we firstly conclude three properties that could describe conditions of feature representation: dispersion, compactness and distribution discrepancy, and explore the relation between the three properties and true OOD error. Based on our analysis, we could observe that dispersion information is surprisingly sufficient for OOD error estimation, so we propose a simple but efficient OOD error prediction score called Dispersion Score, which quantifies the dispersion condition of feature representation. 

% Besides, some of their designs increase the cost of computation by specific model structure requirement during the training process \cite{deng2021does, jiang2021assessing} or auxiliary dataset \cite{garg2022leveraging}. 

% In this work, we Since model performance has a strong relationship with the choice of data representation \cite{bengio2013representation}, we focus on exploring if feature representation could estimate the accuracy of ood data quantitatively. 
% In this work, we focus on exploring 
% \section{Understanding Feature Representation in OOD Error}
% In this section, we first study the role of feature representation in ood error from three respects: dispersion against different classes, compactness within the same class, discrepancy between the training and the test data (from Section \ref{dispersion} to Section \ref{discrepancy}). Based on our observation, we further \textbf{TODO}.

% \subsection{Dispersion against different classes}\label{dispersion}


% \subsection{Compactness within the same class}\label{compactness}


% \subsection{Discrepancy between the training and the test data}\label{discrepancy}

% \begin{figure*}
%     \centering
%     \includegraphics[width=1.\linewidth]{img/tsne.pdf}
%     \caption{\textbf{t-SNE visualization of ID and OOD dataset representation on CIFAR-10 with contrast corruption under diverse severity levels.} The first sub-figure shows feature representation of the ID dataset, while the rest of three sub-figures illustrate that of the OOD dataset. From this figure, we can observe that clusters with different labels will be increasingly dispersed when the corruption severity level becomes small.}
%     \label{fig:tsne}
% \end{figure*}



\section{Problem Setup and Motivation}
% In this section, we formulate the problem setting of OOD error estimation at test time (Section \ref{preliminaries}), and then highlight the importance of feature dispersion on generalization performance prediction (Section \ref{motivation}).
% relationship between feature representation and true OOD error via observational studies (Section \ref{motivation}).

\subsection{Preliminaries: OOD performance estimation}\label{preliminaries}
\paragraph{Setup} In this work, we consider multi-class classification task with $k$ classes. 
We denote the input space as $\mathcal{X}$ and the label space with $k$ classes as $\mathcal{Y} = \{1, \ldots, k\}$. We assume there is a training dataset $\mathcal{D}=\{\boldsymbol{x}_i, y_i\}^{n}_{i=1}$, where the $n$ data points are sampled \emph{i.i.d.} from a joint data distribution $\mathcal{P}_{\mathcal{X}\mathcal{Y}}$. 
% We denote by $\mathcal{P}_{\text{in}}$ the marginal distribution over $\mathcal{X}$. 
In the training stage, we learn a neural network  $f: \mathcal{X} \rightarrow \mathbb{R}^k$ with trainable parameter ${\theta} \in \mathbb{R}^p$ on $\mathcal{D}$. 

In particular, the neural network can be viewed as a combination of a feature extractor $f_{g}$ and a classifier $f_{\omega}$, where $g$ and $\omega$ denote the parameters of the corresponding parts, respectively. The feature extractor $f_{g}$ is a function that maps instances to features $f_{g}: \mathcal{X} \rightarrow \mathcal{Z}$, where $\mathcal{Z}$ denotes the feature space. We denote by $\boldsymbol{z}_i$ the learned feature of instance $\boldsymbol{x}_i$: $\boldsymbol{z}_i = f_{g}(\boldsymbol{x}_i)$. The classifier $f_{\omega}$ is a function from the feature space $\mathcal{Z}$ to $\mathbb{R}^k$, which outputs the final predictions. 
A trained model can be obtained by minimizing the following expected risk:
\begin{equation*}
\begin{aligned}
    \mathcal{R}_{\mathcal{L}}(f) &= \mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{P}_{\mathcal{X}\mathcal{Y}}}\left[\mathcal{L}\left(f(\boldsymbol{x} ; {\theta}), y\right)\right] \\
    &= \mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{P}_{\mathcal{X}\mathcal{Y}}}\left[\mathcal{L}\left(f_{\omega}(f_{g}(\boldsymbol{x})), y\right)\right]
\end{aligned}
\end{equation*}



% In the training process, we train $f_g$ and $f_{\omega}$ on the training data $\mathcal{D}_{train}=\{(x_i, y_i)\}_{i=1,...,n}$ where $(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$ via a supervised loss (i.e., Eq. \ref{eq: loss}).
% In this work, we focus on the multi-class classification problem with total $K$ classes using a neural network parameterized by $\theta$. Given a set of training data $\mathcal{D}_{train}=\{(x_i, y_i)\}_{i=1,...,n}$ where $(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$, we could train a neural network $f_{\theta}$ which is composed of a feature extractor $f_{g}$ and a classifier $f_{\omega}$ using a loss expressed as follows, where $g$ and $\omega$ denote the parameters of the feature extractor and the classifier, respectively.
% \begin{equation}
% \label{eq: loss}
%     L_{train} = l(f_{\omega}(f_{g}(x_i)), y_i).
% \end{equation}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=3.0cm,width=3.8cm]{img/frecet_distance.pdf}
        \caption{Fr\'{e}chet}
        \label{fig:frechet_gap}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=3.0cm,width=3.8cm]{img/MMD_distance.pdf}
        \caption{MMD}
        \label{fig:mmd_gap}
    \end{subfigure}
     \caption{Distribution Gap Vs. OOD accuracy on CIFAR10-C via (a) Fr\'{e}chet distance \cite{dowson1982frechet} and (b) MMD \cite{gretton2006kernel}. All colors indicate 14 different types of corruption. The test accuracy varies significantly on shifted datasets with the same distribution distances.}
     \label{fig:gap}
\end{figure}


\paragraph{Problem statement} At test time, we generally expect that the test data are drawn from the same distribution as the training dataset. However, distribution shifts usually happen in reality and even simple shifts can lead to large drops in performance, which makes it unreliable in safety-critical applications. Thus, our goal is to estimate how a trained model might perform on the shifted data without labels, i.e., unlabeled out-of-distribution (OOD) data. 

Assume that $\Tilde{\mathcal{D}}=\{\Tilde{\boldsymbol{x}}_i\}^{m}_{i=1}$ be the OOD test dataset and $\{\Tilde{y}_i\}^M_{i=1}$ be the corresponding unobserved labels. For a certain test instance $\Tilde{\boldsymbol{x}}_i$, we obtain the predicted labels of a trained model by $\Tilde{y}^{\prime}_i = \arg\max f_{\omega}(\Tilde{\boldsymbol{x}}_i)$. Then the ground-truth test error on OOD data can be formally defined as:
\begin{equation}
    \operatorname{Err}(\Tilde{\mathcal{D}})=\frac{1}{m} \sum_{i=1}^{m}\mathds{1}(\Tilde{y}^{\prime}_i \neq \widetilde{y}_i),
\end{equation}

To estimate the real OOD error, the key challenge is to formulate a score $S(\Tilde{\mathcal{D}})$ that is strongly correlated with the test error across diverse distribution shifts without utilization of corresponding test labels. With such scores, a simple linear regression model can be learned to estimate the test error on shifted datasets, following the commonly-used scheme \cite{deng2021labels, yu2022predicting}. 
% derive a dataset-level score $S(\Tilde{\mathcal{D}})$ that is strongly correlated with the test error over various distribution shifts. With such a score, a simple linear regression model can be learned to estimate the test error on shifted datasets, following the commonly-used scheme \cite{deng2021labels, yu2022predicting}. 

While those output-based approaches suffer from the overconfidence issue \cite{hendrycks2016baseline, guillory2021predicting}, other methods primarily rely on the distribution distance between training data $\mathcal{D}$ and test data $\Tilde{\mathcal{D}}$, with the intuition that the distribution gap impacts classification accuracy. In the following, we motivate our method by analyzing the failure of those methods based on distribution distance.





\subsection{The failure of distribution distance}
\label{motivation}

In the literature, distribution discrepancy has been considered as a key metric to predict the generalization performance of the model on unseen datasets \cite{deng2021labels, tzeng2017adversarial, gao2022distributionally, sinha2017certifying, yu2022predicting}. 
AutoEval \cite{deng2021labels} estimates model performance by quantifying the domain gap in the feature space:
$$S(\mathcal{D}, \Tilde{\mathcal{D}}) = d(\mathcal{D}, \Tilde{\mathcal{D}}),$$
where $d(\cdot)$ denotes the distance function, such as Fr\'{e}chet Distance \cite{dowson1982frechet} or maximum mean discrepancy (MMD) \cite{gretton2006kernel}.

ProjNorm measures the distribution gap with the Euclidean distance in the parameter space: $$S(\mathcal{D}, \Tilde{\mathcal{D}}) = \|\theta - \Tilde{\theta}\|_{2},$$
where $\theta$ and $\Tilde{\theta}$ denote the model parameters fine-tuned on training data $\mathcal{D}$ and OOD data $\Tilde{\mathcal{D}}$, respectively.


The underlying assumption is inherited from the conventional wisdom in domain adaptation, where a representation function that minimizes domain difference leads to higher accuracy in target domain \cite{tzeng2014deep, ganin2015unsupervised, tzeng2017adversarial}. Yet, the relationship between distribution distance and test accuracy remains controversial. In other words, given a fixed model, does a larger distribution distance always lead to a lower test accuracy?

To verify the correlation between distribution distance and model performance, we compare the model performance on different OOD test sets of CIFAR-10C, using the ResNet-50 model trained on CIFAR-10. For each OOD test set, we calculate the distribution distances in the feature space $\mathcal{Z}$ via Fr\'{e}chet distance \cite{dowson1982frechet} and MMD \cite{gretton2006kernel}, respectively. Figure~\ref{fig:gap} presents the classification accuracy versus the distribution distance. The results show that, on different test datasets with similar distances to the train data in the feature space. the performance of the trained model varies significantly with both the two distance metrics. For example, calculated by Fr\'{e}chet distance, the distribution gap varies only a small margin from 223.36 to 224.04, but the true accuracy experiences a large drop from 61.06\% to 46.99\%. Similar to MMD distance, when the distribution gap changes from 87.63 to 88.35, the OOD accuracy drops from 66.53\% to 47.73\%. The high variability in the model performance reveals that, \textbf{the distribution difference is not a reliable surrogate for generalization performance under distribution shifts}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\linewidth]{img/toy_experiments.pdf}
    \caption{A toy example for illustrating the failure of distribution distance. Blue and Orange denote the two classes. The test accuracy will not change regardless of the shift distance of test data along the y-axis.}
    \label{fig:toy}
    \vspace{-20pt}
\end{figure}

\paragraph{A toy example.} To provide a straightforward view, we show in Figure~\ref{fig:toy} a toy example for illustrating the failure of distribution distance. Consider a binary classification with the data generating from a mixture of two Gaussian distributions $\mathcal{P}_{\mathcal{X}\mathcal{Y}}$. In particular, the label Y is either positive ($+1$) or negative ($-1$) with equal probability (i.e., 0.5). Without loss of generality, we assume there is a trained model $f(\boldsymbol{x}) = \operatorname{sign}(\boldsymbol{x} - \kappa)$, where $\kappa$ denotes a constant between class centers. At test time, we consider a specific OOD dataset that shifts along the y-axis. Naturally, we find that the test accuracy will not change regardless of the shift distance of test data. It implies that, \textbf{distribution distance may fail to indicate the model performance}. We proceed by introducing our method, exploring a new perspective in the feature space.


% As for evaluation, given an OOD dataset that has access to the ground-truth labels $\mathcal{D}_{test}=\{(\widetilde{x_i},\widetilde{y_i})\}_{i=1,...,m}$ where $(\widetilde{x_i}, \widetilde{y_i}) \in \mathcal{X} \times \mathcal{Y}$, we can calculate the true classification error of the pre-trained model by the following equation:


% During testing, given a set of test data that are collected from a distribution different from the distribution of the training data (i.e., OOD dataset), assuming we could observe its ground-truth labels $\mathcal{D}_{test}=\{(\widetilde{x_i},\widetilde{y_i})\}_{i=1,...,m}$ where $(\widetilde{x_i}, \widetilde{y_i}) \in \mathcal{X} \times \mathcal{Y}$ (i.e., training data and test data are from the same feature and label space), we compute the true classification error of the model $f$ by the following equation:
% \begin{equation}
%     OodError(\mathcal{D}_{test}) =  \frac{1}{m} \sum_{i=1}^{m}\mathds{1}(C(f_{\omega}(f_{g}(\widetilde{x_i}))) \neq \widetilde{y_i}),
% \end{equation}
% where $C(f(x))=\arg\max_{c}(f_{c}(x))$, and $\mathds{1}$ denotes the indicator function.

% Our goal is to quantity the true OOD error within linear correlation only using test samples without annotation across diverse types of distribution shift as well as severity levels.

% However, the ground-truth labels are usually invisible during testing. Our goal is to quantify the true classification error without annotation information of the test set across different distribution shifts.




% \subsection{Motivation: the role of feature representation in OOD error}\label{motivation}
% % Our method is inspired by the following three questions.
% \textbf{Mismatch between distribution discrepancy and generalization performance.} There exists a general-purpose prior that a larger distribution difference normally results in lower classifier's accuracy \cite{ben2006analysis, ganin2016domain, deng2021labels, long2015learning}. However, it is practically insufficient for the prior knowledge to establish a linear relationship between the two factors \cite{mehra2022domain}. To verify the perspective, we conduct the observational studies on both synthetic data and real world data shown in Figure \ref{fig:toy}. 

% We firstly consider a binary classification task with artificial datasets drawn from Gaussian Mixture distributions:
% \begin{equation}
%     \begin{aligned}   
%     &X_{in}|(Y_{in}=y) \sim \mathcal{N}(y\cdot \mu_{in},\sigma_{in}^{2}\mathcal{I}),\\
%     &X_{out}|(Y_{out}=y) \sim \mathcal{N}(y\cdot \mu_{out},\sigma_{out}^{2}\mathcal{I}),
% \end{aligned}
% \end{equation}

% where $X_{in}$ and $X_{out}$ are generated ID and OOD samples, $Y_{in}, Y_{out} \in \{-1,1\}$ represent discrete random variables with $P(Y_{*}=y)=p^{*}_{y}$ and $p_{y}^{in}\neq p_{y}^{out}$, $\mu_{in}, \mu_{out} \in \mathrm{R}^2$ and $\mu_{in} \neq \mu_{out}$, $\sigma_{in} \neq \sigma_{out}$, and $\mathcal{I}$ denotes the two-dimensional identity matrix. From the first two figures in Figure \ref{fig:toy}, we could observe that the logistic regression learnt on the ID dataset also performs well on the OOD dataset, even if the distribution shift is remarkable, which means that the mapping relation from distribution difference to generalization performance dose not necessarily hold. 
% % Then a logistic regression could be learnt based on them shown as the $i^{st}$ figure in Figure \ref{fig:toy}. 

% To verify the non-trivial connection in the open world, we show the true classification accuracy on CIFAR-10C using the model trained on CIFAR-10 with ResNet50 versus the distribution distance measured by Fr\'{e}chet distance \cite{dowson1982frechet} and MMD \cite{gretton2006kernel} in the last two figures of Figure \ref{fig:toy}. They illustrate the high variability in the performance even at the identical distribution distance, which means the distribution difference is not sufficiently representative of generalization performance.

% \textbf{Inter-class dispersion and intra-class compactness.} 
% using distribution difference to estimate generalization performance is not robust. 

% \textbf{Q1: Why do we consider to formulate a score from the view of feature representation?} 

% Many existing methods for OOD error estimation use logits to formulate a prediction score \cite{}. However, their designed scores do not work consistently across different datasets, since logits always suffer from overconfidence problem for both in-distribution (ID) inputs and out-of-distribution inputs \cite{wei2022mitigating}. On the contrary, feature representation coming out from the feature extractor will not meet this problem.

% On the other hand, the relation from good feature representation to good performance has been proved. For ID data, searching a good feature representation has been considered as a common perspective to improve performance of classification, recognizing, object detection, ect \cite{}. For OOD data, domain adaptation provides a solution for better performance that the feature extractor should narrow the distance between the source domain and the target domain \cite{}.  


% \textbf{Q2: Could model performance be reflected by the condition of feature representation?}

% As mentioned above, the relation from feature representation to model performance is clear, so is it also satisfied for the opposite relation? The paper \cite{deng2021does} demonstrates that the opposite relation still works. 

% Inspired by domain adaptation, it measures the representation discrepancy between the training dataset and the test dataset via Fr\'{e}chet distance, and regards the distance as the OOD error prediction. However, it only focuses on the summary of the whole distribution like the mean and the covariance, but ignores the fact that distribution shift causes not only the distribution gap between two domains but also the detailed scatter change in the feature space \cite{kang2019contrastive}. So this method does not perform well in the experiment section (i.e., Section \ref{experiment}). In addition, to measure the distribution gap, this method is required to remember the training dataset, which also makes the computation speed slow.

% detailed change in the test distribution caused by distribution shift, resulting in inaccurate description on OOD error.


% \textbf{Q3: What factors do we need to consider when formulating the score?}
% Feature representation learning has gained considerable interests in many ares such as classification \cite{} and detection \cite{}. Their goal is to find out a "good" representation for samples in order to improve the model performance. However, most of their works focus on in-distribution (ID) data 

% Many works of feature representation learning has been proposed to find out good representations for task performance improvement \cite{}. To improve performance in OOD classification, many methods of transfer learning from the view of feature representation try to   There exists a general-purpose prior that a good feature representation should be natural clustering, which means that $P(X|Y=i)$ should be well separated with as small areas of overlapping as possible \cite{bengio2013representation}. 
% Searching a good feature representation for samples has been considered as a common perspective to improve performance of classification, recognizing, object detection, ect \cite{}. \textbf{On the contrary, could model performance be reflected by the condition of feature representation?} The paper (\textit{Fr\'{e}chet distance}) \cite{deng2021does} 

% To estimate OOD error via feature representation accurately and quickly, we focus on the question: how the test set representation changes with diverse distribution shifts. There exists a general-purpose prior that a good feature representation should be natural clustering, which means that $P(X|Y=i)$ should be well separated with as small areas of overlapping as possible \cite{bengio2013representation}. Is it also true for OOD data?

% Intuitively, We draw the t-SNE plots in Figure \ref{fig:tsne} of the test set representation on CIFAR-10 with the contrast corruption when the severity level equals to 1 ,3, and 5 respectively. From this figure, we could clearly observe that the higher the severity level is, the more dispersed the points from diverse classes are. In this case, dispersion conditions need to be considered.

% Without a doubt, dispersion is not the only property for feature representation. Constractive learning also proposes that similarity among samples with the same classes should be maximized \cite{khosla2020supervised}, which means that compactness of each cluster with the same labels should be small. 

% In the following discussion, we consider \textbf{dispersion} and \textbf{compactness} as the main properties of feature representation, and explore their relationships as well as \textbf{distribution gap}'s with OOD error.



% \section{Exploratory Experiments: Dispersion, Compactness and Distribution Gap}
% In this section, we do some exploratory experiments to show the relation between the three feature properties with OOD error.

% \subsection{Feature Property Measurement.} 

% To quantify the three properties, we define the following three measurement scores that are inspired by Calinski-Harabaz index \cite{zhao2012cluster}. 

% \textbf{Dispersion Score.} We measure dispersion conditions of the test feature representation by computing the weighted average of the distance from the center of each class to 


\section{Propose Method}\label{method}


\begin{figure*}
    \centering
    \includegraphics[width=1.\linewidth]{img/tsne.pdf}
    \vspace{-10pt}
    \caption{t-SNE visualization of feature representation on training and OOD test datasets (CIFAR-10C) with \textit{contrast} corruption under different severity levels. The first left figure shows feature representation of the training dataset, while the rest of three figures illustrate those of the OOD datasets. From this figure, we observe that different clusters tends to be more separated as the corruption severity level gets smaller.}
    \label{fig:tsne}
\end{figure*}

In this section, we first introduce the intuition of our method with a natural example. Next, we characterize and provide quantitative measure on the the desirable properties of feature representations for predicting OOD error. We then present the advantages of our proposed method.

\subsection{An empirical study of feature separability}
In representation learning, it is desirable to learn a separable feature space, where different classes are far away from each other and samples in each class form a compact cluster. Therefore, separability is viewed as an important characteristic to measure the feature quality. For example, one may train a nearest neighbor classifier over the learned representation using labeled data and regard its performance as the indicator. In this paper, we investigate how to utilize the characteristic of separability for estimating the model performance under distribution shifts, without access to ground-truth labels. 

In Figure~\ref{fig:tsne}, we present a t-SNE visualization of the representation for training and test data. In particular, we compare the separability of the learned representation on shifted datasets with various severity. While the feature representation of training data exhibits well-separated clusters, those of the shifted datasets are more difficult to differentiate and the cluster gaps are well correlated with the corruption severity, as well as the ground-truth accuracy. It implies that, a model with high classification accuracy is usually along with a well-separated feature distribution, and vice versa.


% A common evaluation of feature quality is to train either a linear classifier or a nearest neighbor classifier over the learned representation using labeled data. 


\subsection{Dispersion score}

In our previous analysis, we show a strong correlation between the test accuracy and feature separability under different distribution shifts. To quantify the separability in the feature space $\mathcal{Z}$, we introduce \emph{Dispersion score} that measures the inter-class margin without annotation information. 

First, we allocate OOD instances $\{\Tilde{\boldsymbol{x}}\}_{i=1}^{m}$ into different clusters $j$ based on the model predictions, i.e., their pseudo labels from the trained classifier $f_{\omega}$: 
$$j=\Tilde{y}^{\prime}_i = \arg\max f_{\omega}(\boldsymbol{z}_i).$$ 
With these clusters, we compute the \emph{Dispersion score} by the average distances between each cluster centroid $\Tilde{\boldsymbol{\mu}}_j$ and the center of all features $\bar{\boldsymbol{\mu}}$, weighted by the sample size of each cluster $m_j$. Formally, the \emph{Dispersion score} is defined as: 
$$
S(\Tilde{\mathcal{D}}) = \frac{1}{k-1} \sum_{j=1}^{k} m_j \cdot \varphi(\bar{\boldsymbol{\mu}}, \Tilde{\boldsymbol{\mu}}_j)
$$
where $k-1$ is the degree of freedom and $\varphi$ denotes the distance function. With the weight $m_j$, the induced score receives a stronger influence from those larger clusters, i.e., the majority classes. This enables our method to be more robust to the long-tailed issue, which naturally arises in the unlabeled OOD data. In subsection~\ref{robust_exp}, we explicitly show the advantage of our method in the class-imbalanced case and analyze the importance of the weight in Appendix~\ref{app:weight}.

In particular, we use square Euclidean distances to calculate the distance in the feature space $\mathcal{Z}$. So it converts to:
\begin{equation}
\label{eq:score}
    S(\Tilde{\mathcal{D}}) = \log \frac{\sum_{j=1}^{k} m_j \cdot \|\bar{\boldsymbol{\mu}} - \Tilde{\boldsymbol{\mu}}_j\|_2^2}{k-1} 
\end{equation}
Following the common practice \cite{jiang2018predicting}, we adopt a log transform on the final score, which corresponds to multiplicative combination of class statistics. 


\begin{algorithm}[tb]
   \caption{OOD Error Estimation via Dispersion Score}
   \label{alg: dispersion}
\begin{algorithmic}
   \STATE {\bfseries Input:} OOD test dataset $\Tilde{\mathcal{D}}=\{\Tilde{x}_i\}_{i=1}^{m}$, a trained model $f$ composed of a feature extractor $f_{g}$ and a classifier $f_{\omega}$
   \STATE {\bfseries Output:} The dispersion score
   \FOR{each OOD instance $\Tilde{x}_i$}
   \STATE Obtain feature representation via $\Tilde{\boldsymbol{z}}_i = f_{g}(\Tilde{\boldsymbol{x}_i})$.
   \STATE Obtain pseudo labels via $\Tilde{y}^{\prime}_i = \arg\max f_{\omega}(\boldsymbol{z}_i)$
   \ENDFOR
   \STATE Calculate cluster centroids $\{\Tilde{\boldsymbol{\mu}}_{j}\}_{j=1}^{k}$ according to pseudo labels $\Tilde{y}^{\prime}_i$, with $\Tilde{\boldsymbol{\mu}}_{j} = \frac{1}{m_{j}}\sum_{i=1}^{m_{j}} \boldsymbol{z}_i \cdot \mathbbm{1}\{\Tilde{y}^{\prime}_i = j\}  $
   \STATE Calculate the feature center of all instances by $\boldsymbol{\mu} = \frac{1}{m}\sum_{i=1}^{m}\boldsymbol{z}_i$
   \STATE Calculate Dispersion Score $S(\Tilde{\mathcal{D}})$ via Equation~(\ref{eq:score})
\end{algorithmic}
\end{algorithm}



\begin{table*}[!t]
    \centering
    \caption{Performance comparison of training free approaches on CIFAR-10, CIFAR-100 and TinyImageNet, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearson correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}. }
    \renewcommand\arraystretch{1.2}
    \resizebox{\textwidth}{!}{
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{cccccccccccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Rotation} &\multicolumn{2}{c}{ConfScore} &\multicolumn{2}{c}{Entropy} &\multicolumn{2}{c}{AgreeScore} &\multicolumn{2}{c}{ATC} &\multicolumn{2}{c}{Fr\'{e}chet} &\multicolumn{2}{c}{Ours}\\
        \cline{3-16}
        & &$R^2$ &$\rho$ &$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$&$R^2$ &$\rho$\\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.822 &0.951 &0.869 &0.985 &0.899 &0.987 &0.663 &0.929 &0.884 &0.985 &0.950 &0.971 &\textbf{0.968} &\textbf{0.990}\\
          & ResNet50 &0.835 &0.961 &0.935 &0.993 &0.945 &\textbf{0.994} &0.835 &0.985 &0.946 &\textbf{0.994} &0.858 &0.964 &\textbf{0.987} &0.990\\
          & WRN-50-2 &0.862 &0.976 &0.943 &\textbf{0.994} &0.942 &\textbf{0.994} &0.856 &0.986 &0.947 &\textbf{0.994} &0.814 &0.973 &\textbf{0.962} &0.988\\
          \cline{2-16}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.840} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.963} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.916} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.991} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.930} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.992}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.785} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.967} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.926} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.991} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.874} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.990}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} & ResNet18 &0.860 &0.936 &0.916 &0.985 &0.891 &0.979 &0.902 &0.973 &0.938 &0.986 &0.888 &0.968 &\textbf{0.952} &\textbf{0.988} \\
          & ResNet50 &0.908 &0.962 &0.919 &0.984 &0.884 &0.977 &0.922 &0.982 &0.921 &0.984 &0.837 &0.972 &\textbf{0.951} &\textbf{0.985}\\
          & WRN-50-2 &0.924 &0.970 &0.971 &0.984 &0.968 &0.981 &0.955 &0.977 &0.978 &\textbf{0.993} &0.865 &0.987 &\textbf{0.980} &0.991\\
          \cline{2-16}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.898} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.956} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.936} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.915} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.983} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.927} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.982} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.946} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.864} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.976} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.962}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}}\\
          \midrule
   \multirow{4}{*}{TinyImageNet} & ResNet18 &0.786 &0.946 &0.670 &0.869 &0.592 &0.842 &0.561 &0.853 &0.751 &0.945 &0.826 &0.970 &\textbf{0.966} &\textbf{0.986}\\
          & ResNet50 &0.786 &0.947 &0.670 &0.869 &0.651 &0.892 &0.560 &0.853 &0.751 &0.945 &0.826 &0.971 &\textbf{0.977} &\textbf{0.986}\\
          & WRNt-50-2 &0.878 &0.967 &0.757 &0.951 &0.704 &0.935 &0.654 &0.904 &0.635 &0.897 &0.884 &0.984 &\textbf{0.968} &\textbf{0.986}\\
          \cline{2-16}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.805} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.959} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.727} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.920} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.650} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.890} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.599} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.878} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.693} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.921} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.847} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.976} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.970}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.987}}\\
         \bottomrule
    \end{tabular}}}
    \vspace{-10pt}
    \label{tab:main 1}
\end{table*}

We summarize our approach in Algorithm~\ref{alg: dispersion}. Notably, the Dispersion score derived from the feature separability offers several compelling advantages:

\begin{enumerate}

    \item \textbf{Training data free}. The calculation procedure does not rely on the information of training data. Thus, our method is compatible with large-scale models that are trained on billions of images.
    \item \textbf{Easy-to-use}. The computation of the Dispersion score only does forward propagation for each test instance once and does not require extra hyperparameter, training a new model, or updating the model parameters. Therefore, our method is easy to implement in real-world tasks and computational efficient, as demonstrated in Tables~\ref{tab:main 2} and \ref{tab:imbalance}.
    \item \textbf{Strong flexibility in OOD data}. Previous state-of-the-art methods, like ProjNorm \cite{yu2022predicting}, usually requires a large amount of OOD data for predicting the prediction performance. Besides, the class distribution of unlabeled OOD datasets might be severely imbalanced, which makes it challenging to estimate the desired test accuracy on balanced data. In Subsection~\ref{robust_exp}, we will show the Dispersion score derived from the feature separability exhibits stronger flexibility and generality in sample size and class distribution.
    % \item \textbf{Model-agnostic}. The estimating precedure applies to a variety of model architectures with different capability, including ResNet-18, ResNet-50 \cite{he2016deep}, and WRN-50-2\cite{zagoruyko2016wide}. Moreover, the Dispersion score is also agnostic to the training procedure, such as loss function and optimizer.

\end{enumerate}

% \HW{add advantages}



% \begin{equation}
% \label{eq: dispersion}
%     S_{disp} = \log{\frac{\sum_{k=1}^{K}m_{\hat{k}}||\boldsymbol{\hat{\mu}}_{k}-\boldsymbol{\mu}||^2}{K-1}},
% \end{equation}
% Based on the above analysis, we propose Dispersion Score to estimate OOD errors, where the pseudo code could be viewed in Algorithm \ref{alg: dispersion}.

% For the pre-trained process, we need to lean a classification model via the training dataset in a supervised way. After we obtain the pre-trained model as well as the test set without annotation, we firstly feed the dataset into the feature extractor and gain the feature representation:
% \begin{equation}
%     \widetilde{z_i} = f_{g}(\widetilde{x_i}).
% \end{equation}

% Simultaneously, we could also gain their pseudo labels:
% \begin{equation}
% \label{eq: pseudo}
%     \hat{k} =\arg \max_{p} f_{\omega}(\widetilde{z_i}).
% \end{equation}

% Then Dispersion Score could be calculated after we obtain all feature representation and pseudo labels of the test set:
% \begin{equation}
% \label{eq: dispersion}
%     S_{disp} = \log{\frac{\sum_{k=1}^{K}m_{\hat{k}}||\boldsymbol{\hat{\mu}}_{k}-\boldsymbol{\mu}||^2}{K-1}},
% \end{equation}
% where $\boldsymbol{\hat{\mu}}_{k}$ denotes the estimated center of the $k^{th}$ class by pseudo labels, $\boldsymbol{\mu}$ denotes the center of the whole dataset, and $K$ denotes the total number of classes.

% \textbf{Linear regression.} After obtaining Dispersion Score of all test datasets, we could evaluate the score's performance with respective to the ground-true OOD error by linear regression,
% \begin{equation}
%     OodError = \alpha S_{disp} + \beta,
% \end{equation}
% where $\alpha$ and $\beta$ denote parameters of the linear regression model. 



% Compactness Score:  $S_{comp} = - \log{\frac{\sum_{k=1}^{K}\sum_{i=1}^{n_{k}}||\boldsymbol{z}_i-\boldsymbol{\mu}_k||^2}{N-K}}$

% Domain Shift Score: $S_{shift} = - \log{\frac{\sum_{k=1}^{K}||\boldsymbol{\mu}_k-\boldsymbol{\mu}_{k}^{in}|^2}{K-1}}$

% Final Score: $S = S_{disp} + S_{comp} + S_{shift}$

% Compactness Discrepancy: $S_{disp} = \frac{1}{\mathcal{K}-1}\sum_{k=1}^{\mathcal{K}}|\log{\frac{\sum_{i=1}^{n_k}||\boldsymbol{z}_i-\boldsymbol{\mu}_k||^2}{n_k-1}} - \log{\frac{\sum_{i=1}^{n_k^{in}}||\boldsymbol{z}_i^{in}-\boldsymbol{\mu}_k^{in}||^2}{n_k^{in}-1}}|$

% Dispersion Discrepancy: $S_{comp} = \frac{1}{\mathcal{K}-1}\sum_{k=1}^{\mathcal{K}}|\log{n_k||\boldsymbol{\mu}_k-\boldsymbol{\mu}||^2} - \log{n_k^{in}||\boldsymbol{\mu}_k^{in} - \boldsymbol{\mu}^{in}||^2}|$

% Final Score: $S = S_{disp} + S_{comp}$



\section{Experiments} \label{experiment}
%In this section, we verify the effectiveness of Dispersion Score in OOD error estimation with several benchmark datasets.
In this subsection, we first compare the proposed score to existing training-free methods. Then, we provide an extensive comparison between our method and recent state-of-the-art method -- ProjNorm, in both prediction performance and computational efficiency. We then  robustness of our method under more general conditions of OOD data.

\subsection{Experimental Setup}
\textbf{Train datasets.} During training, we train models on the CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning} and TinyImageNet \cite{le2015tiny} datasets. Specifically, the train data of CIFAR-10 and CIFAR-100 contain 50,000 training images, which are allocated to 10 and 100 classes, respectively. The TinyImageNet dataset contains 100,000 64 $\times$ 64 training images, with 200 classes. 

\textbf{Out-of-distribution (OOD) datasets.} To evaluate effectiveness of the proposed method on predicting OOD error at test time, we use CIFAR-10C, CIFAR-100C, which span 19 types of corruption with 5 severity levels. For the testing of TinyImageNet, we use TinyImageNet-C that spans 15 types of corruption with 5 severity levels \cite{hendrycks2019benchmarking} as OOD dataset. All the datasets with certain corruption and severity contain 10,000 images, which are evenly distributed in the classes. 

\begin{figure*}
    \centering
    \includegraphics[width=1.\linewidth]{img/dispersion_scatter.pdf}
    \vspace{-10pt}
    \caption{OOD error prediction versus True OOD error on CIFAR-10 with ResNet50. We compare the performance of Dispersion Score with that of ProjNorm and Fr\'{e}chet via scatter plots. Each point represents one dataset under certain corruption and certain severity, where different shapes represent different types of corruption, and darker color represents the higher severity level.}
    \label{fig:scatters}
    \vspace{-10pt}
\end{figure*}

\textbf{Evaluation metrics.} To measure the linear relationship between OOD error and designed scores, we use coefficients of determination ($R^2$) and Spearson correlation coefficients ($\rho$) as the evaluation metrics. For the comparison of computational efficiency, we calculate the average evaluation time ($T$) for each test set with certain corruption and severity. 

\textbf{Training details.} During the training process, we train ResNet18, ResNet50 \cite{he2016deep} and WRN-50-2 \cite{zagoruyko2016wide} on CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning} and TinyImageNet \cite{le2015tiny} with 20, 50 and 50 epochs, respectively. We use SGD with the learning rate of $10^{-3}$, cosine learning rate decay \cite{loshchilov2016sgdr}, a momentum of 0.9 and a batch size of 128 to train the model. 



\textbf{The compared methods.} We consider 7 existing methods as benchmarks for predicting OOD error: \textit{Rotation Prediction} (Rotation) \cite{deng2021does}, \textit{Averaged Confidence} (ConfScore) \cite{hendrycks2016baseline}, \textit{Entropy} \cite{guillory2021predicting}, \textit{Agreement Score} (AgreeScore) \cite{jiang2021assessing}, \textit{Averaged Threshold Confidence} (ATC) \cite{garg2022leveraging}, \textit{AutoEval} (Fr\'{e}chet) \cite{deng2021labels}, and \textit{ProjNorm} \cite{yu2022predicting}. Rotation and AgreeScore predict OOD error from the view of unsupervised loss constructed by generating rotating instances and measuring output agreement of two independent models, respectively. ConfScore, Entropy, and ATC formulate scores by model predictions. ProjNorm measures the parameter-level difference between the models fine-tuned on train and OOD test data respectively, while Fr\'{e}chet quantifies the distribution difference in the feature space between the training and test datasets.


\begin{table}[!t]
    \centering
    \caption{Performance comparison between ProjNorm \cite{yu2022predicting} and our Dispersion score on CIFAR-10, CIFAR-100 and TinyImageNet, where $R^2$ refers to coefficients of determination, $\rho$ refers to Spearson correlation coefficients (higher is better), and $T$ refers to average evaluation time (lower is better). The best results are highlighted in \textbf{bold}.}
    \renewcommand\arraystretch{1.2}
    \resizebox{0.49\textwidth}{!}{
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{cccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{3}{c}{ProjNorm}  &\multicolumn{3}{c}{Ours}\\
        \cline{3-8}
        & &$R^2$ &$\rho$ &$T$ &$R^2$ &$\rho$ &$T$ \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.936 &0.982 &179.616 &\textbf{0.968} &\textbf{0.990} &\textbf{10.980}\\
         & ResNet50 &0.944 &0.989 &266.099 &\textbf{0.987} &\textbf{0.990} &\textbf{11.259}\\
          & WRN-50-2 &0.961 &\textbf{0.989} &575.888 &\textbf{0.962} &0.988 &\textbf{11.017}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.947} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{326.201} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{11.085}}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} & ResNet18 &\textbf{0.979} &0.980 &180.453 &0.952 &\textbf{0.988} &\textbf{6.997}\\
         & ResNet50 &\textbf{0.988} &\textbf{0.991} &262.831 &0.953 &0.985 &\textbf{11.138}\\
          & WRN-50-2 &\textbf{0.990} &\textbf{0.991} &605.616 &0.980 &\textbf{0.991} &\textbf{12.353}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.985}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{349.63} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.962} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{10.163}}\\
          \midrule
   \multirow{4}{*}{TinyImageNet} & ResNet18 &\textbf{0.970} &0.981 &182.127 &0.966 &\textbf{0.986} &\textbf{13.938}\\
         & ResNet50 &\textbf{0.979} &0.987 &264.651 &0.977 &\textbf{0.990} &\textbf{7.039}\\
          & WRN-50-2 &0.965 &0.983 &590.597 &\textbf{0.968} &\textbf{0.986} &\textbf{11.235}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.984} &\textcolor[rgb]{0.0, 0.53, 0.74}{345.792} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.987}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{10.737}}\\
         \bottomrule
    \end{tabular}}}
    \label{tab:main 2}
\end{table}




\subsection{Results} 

\paragraph{Can Dispersion score outperform existing training-free approaches?}
In Table \ref{tab:main 1}, we present the performance of OOD error estimation on three model architectures and three datasets. We find that Dispersion Score dramatically outperforms existing training-free methods. For example, averaged across three architectures on TinyImageNet, our method leads to an increase of the $R^2$ from 0.847 to 0.970 -- a 14.5$\%$ of relative improvement. In addition, Dispersion Score achieves consistently high performance over the three datasets with a $R^2$ higher than 0.950, while scores of other approaches such as Rotation varying from 0.787 to 0.924 are not stable. We observe a similar phenomenon on $\rho$, where the Entropy method achieves performance that is ranging from 0.842 to 0.994, while the performance of our method fluctuates around 0.988.

\begin{table}[!t]
    \centering
    \caption{Summary of prediction performance on \textbf{Imbalanced} CIFAR-10C and CIFAR-10C0, where $R^2$ refers to coefficients of determination, $\rho$ refers to Spearson correlation coefficients (higher is better), and $T$ refers to average evaluation time (lower is better). The best results are highlighted in \textbf{bold}.}
    \renewcommand\arraystretch{1.2}
    \resizebox{0.49\textwidth}{!}{
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{cccccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{3}{c}{ProjNorm}  &\multicolumn{3}{c}{Dispersion}\\
        \cline{3-8}
        & &$R^2$ &$\rho$ &$T$ &$R^2$ &$\rho$ &$T$ \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.799 &0.968 &204.900 &\textbf{0.959} &\textbf{0.982} &\textbf{2.076}\\
         & ResNet50 &0.897 &0.980 &430.406 &\textbf{0.968} &\textbf{0.982} &\textbf{3.295}\\
          & WRN-50-2 &0.922 &0.978 &561.611 &\textbf{0.932} &\textbf{0.978} &\textbf{2.970}\\
          \cline{2-8}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.873} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.973} &\textcolor[rgb]{0.0, 0.53, 0.74}{398.972} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.953}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.980}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{2.780}}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} &ResNet18 &0.886 &0.968 &210.05 &\textbf{0.941} &\textbf{0.982} &\textbf{1.864}\\
         & ResNet50 &\textbf{0.980} &\textbf{0.988} &433.860 &0.956 &0.982 &\textbf{2.974}\\
          & WRN-50-2 &0.978 &0.982 &768.883 &\textbf{0.986} &\textbf{0.994} &\textbf{3.242}\\
          \cline{2-8}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.948} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.980} &\textcolor[rgb]{0.0, 0.53, 0.74}{470.931} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.961}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.986}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{2.693}}\\
         \bottomrule
    \end{tabular}}}
    \label{tab:imbalance}
\end{table}

\paragraph{Dispersion score is superior to ProjNorm.}
In Table \ref{tab:main 2}, we compare our method to the recent state-of-the-art method -- ProjNrom \cite{yu2022predicting} in both prediction performance and computational efficiency. The results illustrate that Dispersion Score could improve the prediction performance over ProjNorm with a meaningful margin. 
For example, with trained models on CIFAR-10 dataset, using Dispersion score achieves a $R^2$ of 0.953, much higher than the average performance of ProjNorm as $0.873$. On CIFAR-100, our method also achieves comparable (slightly better) performance with ProjNorm (0.961 vs. 0.948). 
Besides, Dispersion score obtains huge advantages to ProjNorm in computational efficiency. Using WRN-50-2, ProjNorm takes an average 
of 575.888 seconds for estimating the performance of each OOD test dataset, while our method only requires 11.085 seconds.
Since the computation of Dispersion score does not needs to update model parameters or utilize the training data, our method enables to predict OOD error with large-scale models trained on billions of images.


% The benefit of computation efficiency is especially important when we use large models as trained models. From this table, using WRN-50-2, ProjNorm consumes 575.888 seconds on average to estimate OOD error on CIFAR-10C, and gets $R^2$ of 0.947 and $\rho$ of 0.987. However, Dispersion Score only consumes 11.085 seconds for every evaluation on average, and gets better results ($R^2$ of 0.972 and $\rho$ of 0.990).  

% Much faster computation speed with comparable estimation performance means that our method could be employed in many practical scenes such as hyperparameter selection and best model determination. 

To further analyze the advantage of our method, we present in Figure~\ref{fig:scatters} the scatter plots for Fr\'{e}chet, ProjNorm and Dispersion Score on CIFAR-10C with ResNet50. From the figure, we find that Dispersion Score estimates OOD errors linearly w.r.t. true OOD errors in all cases. In contrast, we observe that those methods based on distribution difference tends to fail when the classification error is high. This phenomenon clearly demonstrates the reliable and superior performance of the Dispersion score in predicting generalization performance under distribution shifts.

% More scatter plots could be observed in the Appendix.





\subsection{Flexibility in OOD data}
\label{robust_exp}

In previous analysis, we show that Dispersion score can outperform existing methods on standard benchmarks, where the OOD test datasets contain sufficient instances and have a balanced class distribution. In reality, the unlabeled OOD data are naturally imperfect, which may limit the performance of predicting OOD error. In this section, we verify the effectiveness of our method with long-tailed data and small data, compared to ProjNorm \cite{yu2022predicting}.

% Practically, the test set is not always satisfying during evaluation, so the estimation method should handle more general scenarios. In this section, we test the prediction performances of Dispersion Score and ProjNorm under the setting of imbalanced test sets and small test sets to compare evaluation stability.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=3.0cm,width=3.8cm]{img/subset_res18.pdf}
        \caption{ResNet18}
        \label{fig:small_rn18}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=3.0cm,width=3.8cm]{img/subset_res50.pdf}
        \caption{ResNet-50}
        \label{fig:small_rn50}
    \end{subfigure}
     \caption{Prediction performance $R^2$ vs.~sample size of OOD data (subsets of CIFAR-10C) with (a) ResNet18 and (b) ResNet50.}
     \label{fig:small}
     \vspace{-10pt}
\end{figure}

\paragraph{Class imbalance.} 
We first evaluate the prediction performance under class imbalanced setting. In particular, given a trained model, we aim to estimate its balanced accuracy under distribution shift while we only have access to a long-tailed test data, where a few classes (majority classes) occupy most of the data and most classes (minority classes) are under-represented. We conduct experiments on CIFAR-10C and CIFAR-100C with imbalance rate 100.

% In this part, we conduct experiments on consider the frequency distribution of visual categories is long-trailed, where a few classes are common, while many are rare during evaluation. We are interested in the correlation between the computed scores on imbalanced test datasets and the true OOD error on balanced test datasets. 

Our results in Table~\ref{tab:imbalance} show that Dispersion Score achieves better performance than ProjNorm \cite{yu2022predicting} under class imbalance. For example, our approach achieves an average $R^2$ of 0.953 on CIFAR-10C with 2.780 seconds, while ProjNorm obtains an $R^2$ of 0.873 and takes 398.972 seconds. In addition, the performance of our method is more stable across different model architectures and datasets with the $R^2$ ranging from 0.932 to 0.986 than ProjNorm (0.799 - 0.980). Overall, Dispersion score maintains reliable and superior performance even when the OOD test set are class imbalanced.
% Comparing Table \ref{tab:imbalance} with Table \ref{tab:main 2}, it is also worthy 
% noting that ProjNorm under the imbalanced setting consumes more time than that under the default setting, even though the number of test data is smaller. It is because the average computation time of our approach mainly depends on the size of the test set and the complexity of models, while $T$ of ProjNorm also depends on the current occupation and utilization of GPU and CPU.

% \begin{table}[!t]
%     \centering
%     \caption{Summary of prediction performance on the \textbf{randomly sampled subset} of TinyImageNet, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearson correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}.}
%     \resizebox{0.47\textwidth}{!}{\begin{tabular}{cccccc}
%         \toprule
%         \multirow{2}{*}{Dataset} &\multirow{2}{*}{Sample Number} &\multicolumn{2}{c}{ProjNorm}  &\multicolumn{2}{c}{Dispersion}\\
%         \cline{3-6}
%          & &$R^2$ &$\rho$ &$R^2$ &$\rho$  \\
%         \midrule
%          \multirow{4}{*}{TinyImageNet} &50 &0.114 &0.432 &\textbf{0.610} &\textbf{0.814}  \\
%           \cline{2-6}
%    &100 &0.062 &0.370 &\textbf{0.572} &\textbf{0.798} \\
%            \cline{2-6}
%  &200&0.461 &0.528&\textbf{0.572} &\textbf{0.819}    \\
%           \cline{2-6}
%          &400  &\textbf{0.554} &\textbf{0.772} &0.460 &0.693  \\
%          \bottomrule
%     \end{tabular}}
%     \label{tab: small}
% \end{table}



% \begin{figure}
%     \centering
%     \includegraphics[width=1.\linewidth]{img/subset.pdf}
%     \caption{\textbf{Sampling efficiency test on CIFAR-10C with ResNet18 and ResNet50 for Dispersion Score and ProjNorm.} They show the change of estimation performance w.r.t. the number of samples under ResNet18 and ResNet50 respectively.}
%     \label{fig: small}
% \end{figure}





\textbf{Sampling efficiency.} 
During deployment, it can be challenging to collect instances under a specific distribution shift. 
% As introduced in previous work \cite{yu2022predicting}, sample size has become a key factor in estimating generalization performance on OOD data. 
However, existing state-of-the-art methods \cite{yu2022predicting} normally require a sufficiently large test dataset to achieve meaningful results in predicting OOD error. Here, we further validate the sampling efficiency of Dispersion score, compared with ProjNorm.


Figure \ref{fig:small} presents the performance comparison between Dispersion Score and ProjNorm on subsets of CIFAR10C with various sample sizes. The results show that our method can achieve excellent performance even when only 50 examples are available in the OOD data. In contrast, the performance of ProjNorm decreases sharply with the decrease of sample size. The phenomenon shows that Dispersion score is more efficient in exploiting the information of OOD instances.


\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=3.0cm,width=3.8cm]{img/kmeans_r2.pdf}
        \caption{Compare with K-means}
        \label{fig:kmeans}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=3.0cm,width=3.8cm]{img/compactness.pdf}
        \caption{Compactness vs.~test error}
        \label{fig:compactness}
    \end{subfigure}
     \caption{(a) Comparison of Dispersion score and the K-means variant on three datasets. (b) Compactness vs.~test error on CIFAR-10C with ResNet50. Each point represents a dataset with certain corruption, where shapes represent corruption types and a darker color represents a higher severity level.}
     \vspace{-10pt}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.8\linewidth]{img/compactness.pdf}
%    \vspace{-10pt}
%    \caption{Compactness vs.~test error on CIFAR-10C with ResNet50. Each point represents a dataset with certain corruption, where shapes represent corruption types and a darker color represents a higher severity level.}
%    \label{fig:compactness}
%    \vspace{-10pt}
%\end{figure}

\section{Discussion}
\label{sec:discussion}

\paragraph{K-means vs.~Pseudo labels.} While our Dispersion score derived from pseudo labels has demonstrated strong promise, a question arises: \emph{can a similar effect be achieved by alternative clustering methods?} In this ablation, we show that labels obtained by K-means \cite{lloyd1982least, macqueen1967classification} does not achieve comparable performance with pseudo labels obtained from the trained classifier. In particular, we allocate instances into clusters by using K-means instead of pseudo labels from classifier. We provide an analysis of using ground-truth labels in Appendix~\ref{app:pse}.

We present the performance comparison of our method and the variant of K-means in Figure \ref{fig:kmeans}. The results show that our Dispersion score performs better than the K-means variant and the gap is enlarged with more classes. On the other hand, the variant of K-means does not require a classifier, i.e., the linear layer in the trained model, which enables to evaluate the OOD performance of representation learning methods, e.g., self-supervised learning. 

\paragraph{Compactness.} In Section~\ref{method}, we empirically show that feature separability is naturally tied with the final accuracy and propose an effective score based on inter-class dispersion. However, the connection between intra-class compactness and generalization performance is still a mystery. In this analysis, we show that compactness is not a good indicator of OOD accuracy. Specifically, we define the compactness score:
 $$S(\Tilde{\mathcal{D}}) = - \log{\frac{\sum_{j=1}^{k}\sum_{i=1}^{m_{j}}||\boldsymbol{z}_i-\Tilde{\boldsymbol{\mu}}_j||^2}{n-k}}$$
where $\Tilde{\boldsymbol{\mu}}_j$ denotes the centroid of cluster $j$ that the instance $\boldsymbol{z}_i$ belongs to. Therefore, the compactness score can measure the 
clusterability of the learned features, i.e., the average distance between each instance and its cluster centroid. Intuitively, a high compactness score may corresponds to a well-separated feature distribution, which leads to high test accuracy. Surprisingly, in Figure~\ref{fig:compactness}, we find that the compactness score is largely irrelevant to the final OOD error, showing that it is not an effective indicator for predicting generalization performance under distribution shifts.


% \begin{table}[!t]
%     \centering
%     \caption{\textbf{Comparison} Dispersion Score with pseudo labels and labels via K-means on CIFAR100, where $R^2$ refers to coefficients of determination and $\rho$ refers to Spearson correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}.}
%     \resizebox{0.4\textwidth}{!}{\begin{tabular}{cccccc}
%         \toprule
%         \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Kmeans}  &\multicolumn{2}{c}{Pseudo labels} \\
%         \cline{3-6}
%         & &$R^2$ &$\rho$  &$R^2$ &$\rho$  \\
%         \midrule
%         %  \multirow{4}{*}{CIFAR 10} & ResNet18 &0.968 &\textbf{0.990} &\textbf{0.979} &0.989\\
%         %  & ResNet50 &\textbf{0.987} &0.990 &0.985 &\textbf{0.991}\\
%         %   \cline{2-6}
%         %   & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.989}\\
%         %   \midrule
%     \multirow{3}{*}{CIFAR 100} &ResNet18   &0.936 &0.980 &\textbf{0.952} &\textbf{0.988}\\
%          & ResNet50 &0.938 &0.978  &\textbf{0.953} &\textbf{0.985}\\
%           \cline{2-6}
%          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average}  &\textcolor[rgb]{0.0, 0.53, 0.74}{0.937} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.979} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.962}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.988}}\\
%          \bottomrule
%     \end{tabular}}
%     \label{tab: kmeans}
% \end{table}





\section{Related work}
\textbf{Predicting generalization.} Since the generalization capability of deep networks under distribution shifts is a mysterious desideratum, a surge of researches pay attention to estimate the generalization capability from two directions. 

1) Some works aim to measure generalization gap between training and test accuracy with only training data \cite{corneanu2020computing, jiang2018predicting, neyshabur2017exploring, unterthiner2020predicting, yak2019towards, martin2020heavy}. For example, the model-architecture-based method \cite{corneanu2020computing} summarizes the persistent topological map of a trained model to formulate its inner-working function, which represents the generalization gap. Margin distribution \cite{jiang2018predicting} measures the gap by gauging the distance between training examples and the decision boundary. However, those methods are designed for the identical distribution between the training and test dataset, being vulnerable to distribution shift. 
% aim to measure generalization gap between training and test accuracy under distribution shift by training data


2) Some studies try to estimate generalization performance on a specific OOD test dataset without annotation during evaluation. Many of them utilize softmax outputs of the shifted test dataset to form a quantitative indicator of OOD error \cite{guillory2021predicting, jiang2021assessing, guillory2021predicting, garg2022leveraging}. However, those methods are unreliable across diverse distribution shifts due to the overconfidence problem \cite{wei2022mitigating}. Another popular direction considers the negative correlation between distribution difference and model's performance in the space of features \cite{deng2021labels} or parameters \cite{yu2022predicting}. Nevertheless, common distribution distances practically fail to induce stable error estimation under distribution shift \cite{guillory2021predicting}, and those methods are usually computationally expensive. Unsupervised loss such as agreement among multiple classifiers \cite{jiang2021assessing, madani2004co, platanios2016estimating, platanios2017estimating} and data augmentation \cite{deng2021does} is also employed for OOD error prediction, which requires specific model structures during training. In this work, we focus on exploring the connection between feature separability and generalization performance under distribution shift, which is training-free and does not have extra requirements for datasets and model architectures.

\textbf{Exploring Feature distribution in deep learning.} In the literature, feature distribution has been widely studied in domain adaptation \cite{ben2006analysis, pan2010domain, zhuang2015supervised, tzeng2017adversarial}, representation learning \cite{bengio2013representation, haochen2021provable, ming2023cider, huang2021towards}, OOD generalization \cite{li2018deep, chen2021style, wang2021learning}, and noisy-label learning \cite{zhu2021clusterability, zhu2022detecting}. Domain adaptation methods usually learn a domain-invariant feature representation by narrowing the distribution distance between the two domains with certain criteria, such as maxinum mean discrepancy (MMD) \cite{pan2010domain}, Kullback-Leibler (KL) divergence \cite{zhuang2015supervised}, central moment discrepancy (CMD) \cite{zellinger2017central}, and Wasserstein distance \cite{lee2017minimax}. InfoNCE \cite{huang2021towards} shows a key factor of contrastive learning that the distance between class centers should be large enough. In learning with noisy labels, it has been shown that the feature clusterability can be used to estimate the transition matrix \cite{zhu2021clusterability}. To the best of our knowledge, we are the first to analyze the connection between feature separability and the final accuracy on OOD data.

% \textbf{Unsupervised domain adaptation.} \textit{Unsupervised domain adaptation} (UDA) has gained considerable interests in many practical applications recently \cite{chen2020self, tachet2020domain, farahani2021brief, kamnitsas2017unsupervised}, which goal is to transfer useful information learnt from the source domain to the target domain without labels \cite{pan2009survey}. Inspiring the distribution-discrepancy-based methods for OOD error estimation \cite{deng2021labels}, the key to success of UDA is to learn a latent domain-invariant feature representation by narrowing the distribution distance between the two domains with certain criteria, such as maxinum mean discrepancy (MMD) \cite{pan2010domain}, Kullback-Leibler (KL) divergence \cite{zhuang2015supervised}, central moment discrepancy (CMD) \cite{zellinger2017central}, and Wasserstein distance \cite{lee2017minimax}, or with a domain discriminator, such as domain-adversarial neural network (DANN) \cite{ganin2016domain} and Adversarial discriminative domain adaptation (ADDA) \cite{tzeng2017adversarial}. Causal representation learning \cite{magliacane2018domain, yang2021learning, zhang2015multi} are also included in UDA to extract domain-invariant semantic segmentation. Different from methods motivated from UDA that focus on distribution summaries of source and target domains, our method studies the detailed representation of each test dataset to estimate OOD error, which reserves more significantly relevant information than the previous methods. 

% \textbf{OOD detection.} OOD detection is another increasingly important topic for the safe deployment of machine learning models in practice, which aims to identify test samples from a different distribution that the model is never exposed to during the training process. Some works design scoring functions to address this problem \cite{bendale2016towards, hendrycks2016baseline, liang2017enhancing, hsu2020generalized, lee2018simple, sun2021react, huang2021importance}, where those with outlier scores are more likely to be OOD samples. Training-time regularizers have been also widely used in OOD detection \cite{lee2017training, bevandic2018discriminative, katz2022training, liu2020energy, du2022vos}, which encourages the model to give predictions of OOD samples with significantly distinct properties from ID samples. Compared with the OOD detection problem where OOD and ID data are from different label spaces, OOD error estimation methods are always conducted within the same label space. 


% \textbf{Unsupervised domain adaptation.} \textit{Unsupervised domain adaptation} (UDA) has gained considerable interests in many practical applications recently \cite{}. Their goal is to transfer knowable information from one domain to another domain with distribution shift, so that 

\section{Conclusion}

In this paper, we introduce Dispersion score, a simple yet effective indicator for predicting the generalization performance on OOD data without labels. We show that Dispersion score is strongly correlated to the OOD error and achieves consistently better performance than previous methods under various distribution shifts. Even when the OOD datasets are class imbalanced or have limit number of instances, our method maintains a high prediction performance, which demonstrates the strong flexibility of dispersion score. This method can be easily adopted in practical settings. It is straightforward to implement with trained models with various architectures, and does not require access to the training data. Thus, our method is compatible with large-scale models that are trained on billions of images. We hope that our insights inspire future research to further explore feature separability for predicting OOD error.



\bibliographystyle{icml2023.bst}
\bibliography{example_paper.bib}

\newpage
\appendix
\onecolumn



\section{The importance of the weight}
\label{app:weight}

As introduced in Section \ref{method}, Dispersion Score can be viewed as a weighted arithmetic mean of the distance from centers of each class to the center of the whole samples in the feature space, where the weight is the total number of samples in the corresponding class. To verify the importance of the weight in long-tailed setting, we consider a variant that removes the weight:
$$
S(\Tilde{\mathcal{D}}) = \frac{1}{k-1} \sum_{j=0}^{k} \varphi(\bar{\boldsymbol{\mu}}, \Tilde{\boldsymbol{\mu}}_j).
$$
The results are shown in Table \ref{tab:weight}, where we compare performances of Dispersion score and the variant without weight respectively under \textbf{imbalanced} CIFAR-10C and CIFAR-100C with ResNet10 and ResNet50. From this table, we could observe that the weight enhance the robustness significantly in long-tail conditions.

\begin{table}[h]
    \centering
    \caption{Summary of prediction performance on \textbf{Imbalanced} CIFAR-10C and CIFAR-100C, where $R^2$ refers to coefficients of determination, and $\rho$ refers to Spearson correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}.}
    \resizebox{0.4\textwidth}{!}{\begin{tabular}{cccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{w/o Weights}  &\multicolumn{2}{c}{w/ Weights}\\
        \cline{3-6}
        & &$R^2$ &$\rho$  &$R^2$ &$\rho$ \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.675 &0.930 &\textbf{0.959} &\textbf{0.982} \\
         & ResNet50 &0.748 &0.948 &\textbf{0.968} &\textbf{0.982} \\
          \cline{2-6}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.712} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.939} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.978}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} \\
          \midrule
    \multirow{4}{*}{CIFAR 100} &ResNet18 &0.595 &0.838 &\textbf{0.941} &\textbf{0.982}\\
         & ResNet50 &0.395 &0.733 &\textbf{0.956} &\textbf{0.982} \\
          \cline{2-6}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.494} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.785} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.952}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.986}} \\
         \bottomrule
    \end{tabular}}
    \label{tab:weight}
\end{table}

\section{Sensitivity analysis: pseudo labels}
\label{app:pse}
Here, we conduct a sensitivity analysis by using ground-truth labels in our method. Table \ref{tab: true_labels} illustrates that the performance with pseudo labels is comparable with the performance using  ground-truth labels, showing that our method is not sensitive to noises from classification error.

\begin{table}[h]
    \centering
    \caption{Comparison of Dispersion Score with pseudo labels and true labels on CIFAR10, CIFAR100 and TinyImageNet, where $R^2$ refers to coefficients of determination and $\rho$ refers to Spearson correlation coefficients (higher is better). The best results are highlighted in \textbf{bold}.}
    \resizebox{0.4\textwidth}{!}{\begin{tabular}{cccccc}
        \toprule
        \multirow{2}{*}{Dataset} &\multirow{2}{*}{Network} &\multicolumn{2}{c}{Pseudo labels}  &\multicolumn{2}{c}{True labels}\\
        \cline{3-6}
        & &$R^2$ &$\rho$  &$R^2$ &$\rho$  \\
        \midrule
         \multirow{4}{*}{CIFAR 10} & ResNet18 &0.968 &\textbf{0.990} &\textbf{0.979} &0.989\\
         & ResNet50 &\textbf{0.987} &0.990 &0.985 &\textbf{0.991}\\
          & WRN-50-2 &\textbf{0.961} &\textbf{0.988} &0.945 &0.987\\
          \cline{2-6}
          & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.972}} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.990}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.970} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.989}\\
          \midrule
    \multirow{4}{*}{CIFAR 100} &ResNet18  &\textbf{0.952} &0.988 &0.915 &\textbf{0.989}\\
         & ResNet50 &0.953 &0.985 &\textbf{0.959} &\textbf{0.989}\\
          & WRN-50-2 &\textbf{0.980} &0.991 &0.978 &\textbf{0.995}\\
          \cline{2-6}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.962}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.988} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.950} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.991}}\\
         \midrule
         \multirow{4}{*}{TinyImageNet} &ResNet18 &\textbf{0.966} &\textbf{0.986} &0.937 &0.985\\
         & ResNet50 &\textbf{0.977} &0.990 &0.954 &\textbf{0.995}\\
          & WRN-50-2 &0.968 &0.986 &\textbf{0.977} &\textbf{0.994}\\
          \cline{2-6}
         & \textcolor[rgb]{0.0, 0.53, 0.74}{Average} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.970}} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.987} &\textcolor[rgb]{0.0, 0.53, 0.74}{0.956} &\textcolor[rgb]{0.0, 0.53, 0.74}{\textbf{0.991}}\\
         \bottomrule
    \end{tabular}}
    \label{tab: true_labels}
\end{table}




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
