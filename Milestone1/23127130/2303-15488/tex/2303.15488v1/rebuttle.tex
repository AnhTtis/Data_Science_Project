Reviews 1:
    Strengths And Weaknesses:
    Strengths
    
    The paper is easy to read and the introduction is clear.
    The method is simple and easy to implement
    This work nicely includes related works in model accuracy estimation
    Weakness
    
    1. [Please clarify some crucial parts]
    
    The claim of 'the distribution difference is not a reliable surrogate for generalization performance under distribution shifts' is too strong. Figure 1 only shows two distribution shift measures (FD and MMD). Both metrics are calculated based on the few moments of datasets (e.g., mean and co-variance) and they could fail in some cases. How about other methods (e.g, Projection Norm [Yu et al., 2022] and DoC [Guillory et al., 2021])? They use the difference of confidence or weight distance to reflect the distribution shift.
    Toy example is not convincing. It would be better to discuss how the proposed method or other methods perform in the same case. Moreover, for the proposed method, when class centers are close to the overall center, the classifier could also achieve perfect accuracy: a low dispersion score corresponds to high accuracy. This could be also a toy example that shows the potential limitation of the proposed method. More specifically, the toy example is a toy, without extensive empirical analysis or theoretical anlysis, it is hard to make a convincing claim.
    2. [Experimental analysis is not sufficient]
    
    The proposed method is based on pseudo-labels which can be noisy and incorrect. Please discuss the impact of the noisy pseudo labels: Section B (Sensitivity analysis) is hard to understand. It seems the ground truths do not have any benefit to the proposed method？ Please clarify this observation.
    The experiment is only on CIFAR10/100 and TinyImageNet and they are small in scale. The results on ImageNet and iWILDs are expected. For example, the previous works [Jiang et al., 2021; Guillory et al., 2021; Garg et al., 2022] all report results on ImageNet. How does the proposed method perform on a large-scale dataset?
    Table 3 is not sufficient to draw a conclusion. It would be better to report the results of other methods.
    While it is easy to understand the proposed method, its working mechanism is difficult to understand. Please discuss it and try to give some insights. In summary, this work does not provide a theoretical analysis of the proposed method, so sufficient empirical results are required.
    3. [Method could be vulnerable to some common cases]
    
    There are three obvious cases: 1) the test set contains the open-set class (the class is from unseen classes). The classifier still gives predicted labels for such samples. Then, the proposed method will fail. 2) Some classes do not appear in the test set. This will directly impact the calculated score; and 3) a minor one: the adversarial example. Under an adversarial attack, the features might still have high dispersion but the classifier archives low accuracy.
    Questions:
    Please clarify and update the claim and toy example (see weakness 1)
    Please clarify ’output-based approaches suffer from the over-confidence issue (Hendrycks & Gimpel, 2016; Guillory et al., 2021)‘. Given many test sets, these methods aim to show the difference in model output (e.g., average softmax score) can reflect the accuracy changes. While the output is over-confident, the trend of output change is still useful to capture the accuracy drop.
    Please explain the Sensitivity analysis and include more experimental results (see above weakness 2)
    Please answer or discuss the three potential limitations (see weakness 3)
    Limitations:
    This work does not discuss the potential limitation. There are three obvious limitations: 1) the test set contains the open-set class (the class is from unseen classes); 2) the adversarial example. Projection Norm [Yu et al., 2022] discuss this case, and how the proposed method works under adversarial attack; 3) what if some classes do not appear in the test set? This will directly impact the proposed method but may have limited effect in other model output-based methods (e.g., [Guillory et al., 2021; Garg et al., 2022])

Review 2:
    Strengths And Weaknesses:
    Strengths:
    
    The paper studies an important problem, assessing the risk of models deployed in novel environments with only unlabelled data.
    The proposed method is simple, effective, and fast in comparison with existing methods.
    The paper is very well written and a pleasure to read. The analyses/discussions and their accompanying figures are very clear and informative.
    Weaknesses:
    
    The datasets used in the evaluation are relatively simple. The corrupted variants only capture a fraction of OOD scenarios and the distribution shift is relatively small and superficial compared with datasets in OOD benchmarks like DomainBed [1] and WILDS [2]. It is unclear whether dispersion score can work equally well in those more complicated scenarios. In particular, will dispersion score work if there is severe correlation shift [3] (e.g., Colored MNIST)?
    The lateral comparison of the dispersion scores of different models in the same OOD scenarios is not discussed. Are those dispersion scores comparable with each other? For example, are the dispersion scores of ResNet and ViT comparable? Does the number of feature dimensions and the overall magnitude of features affect dispersion score?
    The presentation of some figures and tables can be improved.
    The text in Fig. 1 is too small. The content in the magnified box of Fig. 1(b) does not match the original boxed area.
    Interleaving 
     and 
     in Tab. 1 makes cross-comparison between methods difficult.
    Typos:
    
    Title of section 3: “Propose”
    Line 258: “We then robustness”
    [1] Gulrajani, Ishaan, and David Lopez-Paz. In search of lost domain generalization.
    [2] Koh, Pang Wei, et al. Wilds: A benchmark of in-the-wild distribution shifts.
    [3] Ye, Nanyang, et al. Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generalization.
    
    Questions:
    Please see the weaknesses above.
    
    Limitations:
    The authors did not discuss any limitations of their method. What are the cons (if any) of the proposed method compared with existing methods?
    
    There is no potential negative societal impact.
    
Additional Experiments:
    1. Large-scale datasets including ImageNet & Wilds, domain adaptation datasets.
    2. Imbalanced data for other training-free approaches.
    3. More cases: Open-set & adversarial samples & subset.

Other concerns:
    1. The failure of distribution distance including the toy example and the real-world examples.
    2. The ground truth v.s. pseudo labels analysis.
    

Theoretical Analysis