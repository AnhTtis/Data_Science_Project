
\section{Gaussian Process Regression}\label{sec_GP}
%\subsection{GP Regression}\label{subsec_Gp_a}

Considering a multi dimensional input $\mathbf{x} \in \mathbb{R}^n$, a Gaussian Process (GP) is stochastic process ${f}(\mathbf{x})$ where any collection  of random variables has a joint Gaussian Distribution $[f(\mathbf{x}_1),f(\mathbf{x}_2),\ldots,f(\mathbf{x}_n)] \sim \mathcal{N}(\boldsymbol {\mu}, \mathbf{K})$.
A GP is commonly defined as introducing the kernel formalism \cite{RasmussenW06}
%
\begin{equation}
    f(\mathbf{x}) \sim \mathit{GP}(\mu(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')),
    \label{eq_GP_def}
\end{equation}
%
where $\mu(\mathbf{x}): \mathbb{R}^n \rightarrow \mathbb{R}$ and $k(\mathbf{x}, \mathbf{x}'): \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ represent the mean and the covariance functions respectively. 
For a general regression problem, given a set of training data $\mathcal{D}=\{\mathbf{X}, \mathbf{y}\}$, with $\mathbf{X}=\{\mathbf{x}_{i} \in  \mathbb{R}^d\}_{i=1}^n$  and $\mathbf{y}=\{y_{i} \in  \mathbb{R}\}_{i=1}^n$, the interest is finding a function $\mathbf{f} : \mathbb{R}^n \rightarrow \mathbb{R}$ so that given an unknown input $\mathbf{x}_{\ast} \in \mathbb{R}^d$ the predicted value $y_\ast \in \mathbb{R}$ is reliable. In GP regression, $f(\mathbf{x})$ are assumed sampled from a zero mean Gaussian process (see eq.~(\ref{eq_GP_def})) and the observations are assumed corrupted by additive Gaussian noise \cite{sarkka2013spatiotemporal, murray2010slice}
\begin{align}
        \text{GP Prior} \quad& f(\mathbf{x_{\ast}}) \sim \mathit{GP}(0, k(\mathbf{x}, \mathbf{x}'; \boldsymbol\theta))
    \label{eq_GP_prior}\\
        \text{Measurements} \quad & y_i = f(\mathbf{x}_i) + \epsilon, \quad \epsilon \sim \mathcal{N}(0,\sigma_\text{noise}^2)
    \label{eq_GP_measurements}
\end{align}
%
where $\boldsymbol{\theta}$ represents the kernels hyper-parameters (length-scales, magnitude, period length, etc) which could be inferred from the marginal likelihood $\mathcal{N}(\mathbf{y}|\mathbf{0}, \mathbf{K}_{xx})$ maximizing the log-marginal likelihood
%
\begin{equation}
    p(\mathbf{y} | \mathbf{X}, \boldsymbol{\theta}) = -\frac{1}{2}\text{log} |\mathbf{K}|  -\frac{1}{2}\mathbf{y}^\top \mathbf{K}^{-1} - \frac{n}{2}\text{log}2\pi,
    \label{eq_log_marg_likelihood}
\end{equation}
%
where $\mathbf{K} = k(\mathbf{X},\mathbf{X}, \boldsymbol\theta) + \sigma_\text{noise}^2\mathbf{I}$ is the covariance matrix for the targets $\mathbf{y}$. The main advantage of employing GPs as regression models is the analytically tractable and close form solution of the predictive distribution~\cite{RasmussenW06}  which is a Gaussian distribution with mean and variance
%
%\begin{equation}
%    p(f(\mathbf{x}_{\ast}) | \mathbf{x_{\ast}}, \mathcal{D}, \boldsymbol{\theta}) = \mathcal{N}(\mu_i(f(\mathbf{x}_{\ast}), \mathbb{V}[f(\mathbf{x}_{\ast})] ),
%d\end{equation}
%
%
\begin{equation}
    \begin{aligned}
    \mathbf{k}_\ast&=[k(\mathbf{x}_\ast,\mathbf{x}_0)~ k(\mathbf{x}_\ast,\mathbf{x}_1)\cdots ~k(\mathbf{x}_\ast,\mathbf{x}_n)]^\top,\\
   \mu_i(f(\mathbf{x}_{\ast})) &= \mathbf{k}_\ast^\top(\mathbf{K}+\sigma_{\text{noise}}^2\mathbf{I})^{-1}\mathbf{y},\\
   \mathbb{V}[f(\mathbf{x}_{\ast})] &= k(\mathbf{x_\ast},\mathbf{x_\ast})-\mathbf{k}_\ast^\top(\mathbf{K}+\sigma_{\text{noise}}^2\mathbf{I})^{-1}\mathbf{k}_\ast.
    \end{aligned}
    \label{eq_GP_prediction}
\end{equation}
%correlation the training input $\mathbf{x}_\ast$ with the input dataset $\mathbf{X}$  through the covariance function in the following format

Consequently, one of the main drawbacks of using Gaussian processes is the computational complexity, which can be limiting in applications where a large dataset is required to learn a complex function. The mean and variance evaluations (see Eq.~(\ref{eq_GP_prediction})) are ruled by matrix inversions. 
Even given a pre-computed inverse it still takes $O(n^3)$ due to the multiplication which limits the amount of data usable in time-sensitive tasks such as robot control.

