\section{Methodology}
We describe a hierarchical approach to converting stationary kernel into its state space form for MISO systems and present a clear derivation of the state space form for different kernels useful in robotics.
\subsection{Dual GP representation for stationary kernels}\label{subsec_Gp_b}
Performing a proper statistical inference often requires a large number of samples. This clashes with the high computational complexity required by GP-based inference. However, for stationary kernels, the Gaussian Process can be thought of as a solution to an approximate $m{\text{-th}}$ order Stochastic Differential Equation (SDE)~\cite{hartikainen2010kalman}. 
The Linear State Space Model (LSSM) representation of the system allows us to consider the statistical inference problem as a state estimation task that can be solved with an equivalent Kalman filter. The computational cost for a single evaluation depends on the m-order of the system $O(m^3)$, and if the number of the observations $n \gg m$, the influence of $m^3$ can be neglected obtaining $O(nm^3) \sim O(n)$. $m$ in this case is a user-controlled hyperparameter  maps to a Taylor Series approximation order of the Kernel. The approach represents an advantageous alternative to the classical GP inference method (with $O(n^3)$ cost).
     Therefore, this framework takes into account some covariance functions of a stationary process (Radial Basis kernel, Matern Kernel, Periodic Kernel), where the covariance functions are input invariant to translations (i.e., $k(\mathbf{x},\mathbf{x}') = k(\Delta)$ where $\Delta = \mathbf{x}-\mathbf{x}'$). 
     All stationary kernels can be directly derived by their spectral density \cite{bochner1959lectures,stein1999interpolation}.
Because all stationary kernel functions are positive definite and stationary by the GP definition, we can apply Bochner's theorem as below to form a Fourier dual with the covariance function and its power spectrum.   %
\noindent\begin{theorem}[Bochner]
\textit{A continuous stationary function is positive definite
if and only if $k$ is the Fourier transform of a finite positive measure}
%
\begin{equation}
    \begin{aligned}
        k(\Delta) &=   \int \mathbf{S}(\omega)e^{2 \pi i\omega \mathbf{\Delta}} d\omega,
    \end{aligned}
\end{equation}
\textit{where $\mathbf{S}$ is a positive finite measure}.
\end{theorem}
%
If  $k(\Delta)$ has a density $\mathbf{S}(\omega)$ then $\mathbf{S}$ is known as the spectral density (or power spectrum) corresponding to $k$. 
In this case, the covariance function and spectral density are Fourier duals \cite{chatfield2003analysis}
%
\begin{equation}\label{eqn:bochner}
        \mathbf{S}(\omega) =  \int_{}^{} k(\Delta)e^{-2 \pi i\omega \mathbf{\Delta}} d\mathbf{\Delta}.
\end{equation}
\begin{algorithm}[t]
\caption{State Space Gaussian Process Conversion\label{alg_ssgp} }
\begin{algorithmic}[1]
  \STATE $\mathcal{F}[k(\tau)]=\mathbf{S}(\omega)=F(j\omega)F(-j\omega) \label{eq:fourier_kernel}$ \COMMENT{Bochner}
%  \STATE $\mathcal{F}[k(\tau)]label{eq:fourier_prod}$
  \STATE $\frac{q_c^2}{\mathbf{S}(w)}=s(j\omega)s(-j\omega) \approx \sum_{i=0}^{m} c_{i}(\omega^2)^{i}\label{eq:Taylor_exp}$\COMMENT{Taylor Series}
  \STATE $\sum_{i=0}^{m} c_{i}(\omega^2)^{i}=  \prod_{i=1}^{m} (r_i^2+\omega^2)$ \COMMENT{Root Factor}
  \STATE $\sum_{i=0}^{m}c_{i}(\omega^2)^{i}=  \prod_{i=1}^{m} (r_i-j\omega)(r_i+j\omega)\label{eq:spectral_decom}$ \COMMENT{Decomp.}
  \STATE $ s(j\omega)=  \prod_{i=1}^{m} (r_i+j\omega) \label{eq:conj_comp}$ \COMMENT{Positive conjugate,}
 \STATE $s(j\omega)=  \sum_{i=0}^{m} a_{i}(j\omega)^{i} \label{eq:defactor}$ \COMMENT{Defactorization,}
 \STATE $ \mathcal{F}^{-1}\left[ s(j\omega)\right]  =  \sum_{i=0}^{m} a_{i}\frac{\partial ^{i}f(x)}{\partial  x^{i}}\label{eq:inverse_Fourier}$ \COMMENT{Inverse Fourier}
\end{algorithmic}
\end{algorithm}

\subsection{State Space Gaussian Process (SSGP)}
A Gaussian Process can be defined by two major components described in eqs.~(\ref{eq_GP_prior}) and ~(\ref{eq_GP_measurements}) Unfortunately, the GP Prior equation, eq.~(\ref{eq_GP_prior}) is in a large matrix form making it computationally complex. We turn our attention to an alternative formulation of GP Prior described solely as a differential equation driven by a white noise process (zero-mean Gaussian process with a Dirichlet kernel function of magnitude $q_c$)  \cite{sarkka2013spatiotemporal}. We start by considering a single input single output formulation in terms of a single input variable $x \in \mathbb{R} $ as
%
\begin{equation}  
Q(x)=  a_l \frac{\partial^l f(x)}{\partial x^l}+\cdots+a_1 \frac{\partial f(x)}{\partial x}+a_0 f(x),
\label{eqn:diff_eq}
\end{equation}
%
where $Q(x)$ represents a white noise process and $f(x)$ represents the function we would like to fit over our data. Conceptually, we can see the reformulation of the prior from matrix form to a differential equation as the difference in taking the averaged sum of correlations from all points in the dataset eq.~(\ref{eq_GP_prediction}) as opposed to the trend of motion described by the previous points in the dataset in eq.~(\ref{eqn:diff_eq}). 
The additional advantage of seeing the GP in terms of the trend of the previous input points as opposed to the sum of all correlations to points is the ability to process information sequentially.
We can derive the transfer function of eq.~(\ref{eqn:diff_eq}) as
%
\begin{equation}
    F(j\omega) = \frac{q_c}{\underbrace{a_n (j\omega)^l+\cdots+a_1 (j\omega)+a_0}_{s(j\omega)}},
    \label{eq:transfer}
\end{equation}
%
where $q_c \in \mathbb{R}$ is a constant describing the Fourier transform of $Q(x)$ of the process. The $q_c$ parameter can be obtained through either optimization in hyperparameter training or tuned by a user.
An important facet to emphasize is that a stationary kernel function $k(\Delta)$ is described completely and solely by its spectral density and eq.~(\ref{eq:transfer}) also has a spectral density. 
%First, we take a kernel and invert it to solve for an approximation of the squared denominator polynomial described in eq.~(\ref{eq:transfer}).
We derive the steps needed to convert an arbitrary stationary kernel function $k(\Delta)$ into a corresponding differential equation. The key component is to obtain a differential equation in the form of eq.~(\ref{eqn:diff_eq}) from $s(j\omega)$ components. %that also constitute that any user can leverage to design and instantiate a GP in state space form as a Kalman filter
\renewcommand{\algorithmiccomment}[1]{\bgroup\hfill~#1\egroup}
%Command to align comments I have no idea how this works but it has to be here


The full method to construct this relationship is described in Algorithm \ref{alg_ssgp}. First, in step (\ref{eq:fourier_kernel}), the left and right side equivalences are due to the Bochner theorem and the definition of spectral density respectively. 
Very few kernels can be represented perfectly as a linear differential equation. This necessitates an approximation with a high enough $m$ order Taylor series polynomial shown in step~(\ref{eq:Taylor_exp}) of our algorithm. 
The Taylor series expansion is in terms of $\omega^2$ to allow easy spectral decomposition in positive and negative conjugate components as in step~(\ref{eq:spectral_decom}).  Only the positive component from step~(\ref{eq:conj_comp}) is required. 
From step~(\ref{eq:conj_comp}), we can de-factorize the positive conjugate in step (\ref{eq:defactor}) to obtain the coefficients of the polynomial.  Finally, step~(\ref{eq:inverse_Fourier}) clearly shows that we obtain the form of eq.~(\ref{eqn:diff_eq}) after performing the inverse Fourier transform. 
Overall, in this process, we matched the spectral density of eq.~(\ref{eq:transfer}) with the spectral density described by eq.~(\ref{eqn:bochner}). If the two stationary GP match spectral densities, then they are the same GP. Therefore, given the linear differential equation in eq.~(\ref{eqn:diff_eq}) and $ \mathbf{f}(x)=\begin{bmatrix}
f(x) & \frac{\partial f(x)}{\partial x} &
\hdots &
\frac{\partial^m f(x)}{\partial x^m}
\end{bmatrix}^\top$, we can create an equivalent state space form with the process model as
%
\begin{equation}
    \begin{aligned}\label{eqn:SSGP_def}
 \frac{\partial  \mathbf{f}(x)}{\partial x}&=\underbrace{\begin{bmatrix}
0 & 1 & & \\
& \ddots & \ddots & \\
& & 0 & 1 \\
-a_{0} & -a_{1} & \cdots & -a_{m-1}
\end{bmatrix}}_{\mathbf{A}} \mathbf{f}(x) +\underbrace{\begin{bmatrix}
0 \\
\vdots \\
0 \\
1
\end{bmatrix}}_{\mathbf{L}} Q(x),\\
y&=\underbrace{\begin{bmatrix}
1 & 0 & \cdots & 0
\end{bmatrix}}_{\mathbf{H}} \mathbf{f}(x)+\varepsilon,
\end{aligned}
\end{equation}
%
For the output equation recall that eq.~(\ref{eq_GP_measurements}) is already in linear form therefore no modification is needed.
Once this state space equation is derived, a Kalman Filter or other similar technique can be employed to linearly iterate across the dataset as a sequence to make a prediction. To further elaborate, given a random input $x_\ast$, and an initial state $\mathbf{f}(x)$, the system will iteratively move through the dataset $\mathcal{D}$ performing Kalman predictions and updates till it updates on $x_i \in \mathcal{D}$ which is the closest data point to $x_\ast$. Consequently, the derivatives such as $\frac{\partial  \mathbf{f}(x)}{\partial x}$ and higher are updated during this motion allowing the system to observe a data trend. This facilitates a final prediction between $x_i$ and $x_\ast$ based on the previously observed trend. Consequently, our system is no longer deterministic  as the system can either approach from the right or left of $x_\ast$.
Extending the formulation to multiple input single output (MISO), can be obtained by stacking the eq.~(\ref{eqn:SSGP_def}) diagonally for the $\mathbf{A}$ matrix, horizontally $\mathbf{H}$, and vertically for $\mathbf{L}$
%
\begin{equation}
        \begin{aligned}
\ \begin{bmatrix}\frac{\partial  \mathbf{f}_0(x_0)}{\partial  t}\\ \vdots\\
\frac{\partial  \mathbf{f}_d(x_d)}{\partial  t}\end{bmatrix}&=\begin{bmatrix} \mathbf{A}_{0}& ...& \mathbf{0}\\   & \ddots&\\
\mathbf{0} & ... & \mathbf{A}_{d}\end{bmatrix}\begin{bmatrix}\mathbf{f}_0(x_0)\\ \vdots\\
\mathbf {f}_d(x_d)\end{bmatrix}+\begin{bmatrix} \mathbf{L}_{0}\\ \vdots \\ \mathbf{L}_{d} \end{bmatrix}Q(\mathbf{x}),\\
y&=\begin{bmatrix}\mathbf{H}_{0}  & ... &\mathbf{H}_{d}\end{bmatrix}\begin{bmatrix}\mathbf{f}_0(x_0)\\ \vdots\\
\mathbf {f}_d(x_d)\end{bmatrix}+\epsilon. \end{aligned}
\end{equation}
Consequently, the conversion of kernel functions into linear state space forms expressed as matrices allows easy exportation to other frameworks. This is because there is no need to write additional functions simply swapping transition matrix parameters is sufficient, and consequently dropping any library dependencies needed to solve for the transition matrices. Our toolbox has this functionality which allows exporting python models directly to C++.
%The formulation for multiple input and multiple output is not straightforward. In fact, the proposed approach processes data sequentially and implicitly define an order for the data $x$. This is not difficult for a MISO system, but ill-defined in a multiple input single output sequence which is why this method mostly is applied to single input single output systems. For applications where the immediate past inference points are highly correlated to the current inference point for example robotic dynamics no ordering is required. 



%\begin{theorem}[Bochner]
%\textit{A continuous stationary function is positive definite
%if and only if  $k$ is the Fourier transform of a finite positive measure}

%\begin{equation}
%    \begin{aligned}
%        k(\tau) &=  \int_{R^D}^{} e^{2 \pi i\mathbf{s} \cdot \mathbf{\tau}} d\Psi(\mathbf{s}),
%    \end{aligned}
%\end{equation}
%\textit{where $\Psi$ is a positive finite measure}.
%\end{theorem}
%If  $\Psi$ has a density $\mathbf{S}(\mathbf{s})$ then $\mathbf{S}$ is known as the spectral density (or power spectrum) corresponding to $k$. 
%In this case  the covariance function and spectral density are Fourier duals \cite{chatfield2003analysis}
%\begin{equation}\label{eqn:bochner}
%    \begin{aligned}
%        k(\tau) &=  \int_{}^{} \mathbf{S}(\mathbf{s}) e^{2 \pi i\mathbf{s} \cdot \mathbf{\tau}} d(\mathbf{s}),\\
%       \mathbf{S}(\mathbf{s}) &=  \int_{}^{} k(\tau)e^{-2 \pi i\mathbf{s} \cdot \mathbf{\tau}} d(\mathbf{\tau}).\\
%    \end{aligned}
%\end{equation}
%In fact, under the assumption of stationary covariance functions, the complex exponential $e^{2 \pi i\mathbf{s} \cdot \mathbf{\tau}}$ represent the eigenfunctions that can be used to express the the kernel in term of these quantities \cite{ferreira2009eigenvalues,mercer1909xvi}. Looking at the spectral density, we may be inspired to think of linear time invariant  differential equations which also have a spectral density. 

\subsection{Kernel Formulation}
 
We give example derivations for our toolbox's kernel using Algorithm \ref{alg_ssgp}. 
\subsubsection{Radial Basis Function Kernel}
The Radial Basis Function (RBF) kernel is also known as the squared exponential kernel. No closed-form solution for any arbitrary approximation order $m$ exists. Fortunately, following algorithm \ref{alg_ssgp}, an appropriate approximation can always be derived for some order $m$ and hyperparameter $z \in \mathbb{R}_{>0}$ known as the lengthscale. For future reference $\mathbb{R}_{>0}$ is the set of all positive real numbers excluding zero
\begin{equation}
        \begin{split}
    k(\Delta) &=e^{-0.5z^2\Delta^2},\\
    s(j\omega)s(-j\omega) &= 1+(z\omega)^2+...\frac{1}{m!}(z\omega)^{2m}.\\
 \end{split}
\end{equation}
Considering $m=2$ and $z=1$ we obtain
\begin{equation}
        \begin{split}
     &s(j\omega)s(-j\omega) = 1+(\omega)^2+0.5(\omega)^{4},\\
   &= 0.5(\omega^2+(1+j))(\omega^2+(1-j)),\\
       s(j\omega)&= \frac{1}{\sqrt{2}}(j\omega+\sqrt{1+j})(j\omega+\sqrt{1-j}),\\
    s(j\omega)&\approx \frac{1}{\sqrt{2}}(j\omega)^2+\frac{2.2}{\sqrt{2}}(j\omega)+1.
 \end{split}
\end{equation}

\subsubsection{Matern Kernel}
Matern is a more complex and generalizable kernel compared to the RBF Kernel and approaches the RBF Kernel as $\nu\rightarrow\infty$. This kernel is one of the few kernels where there is a closed-form solution for any arbitrary set of hyperparameters 

\begin{equation}
        \begin{aligned}
k(\Delta)&=\sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\Delta}{z}\right)^\nu K_\nu\left(\sqrt{2 \nu} \frac{\Delta}{z}\right),\\    s(j\omega)s(-j\omega) &=  \left(\lambda^2+\omega^2\right)^{(\nu+1 / 2)},\\
s(j\omega) &=(\lambda+j \omega)^{(\nu+1 / 2)},
 \end{aligned}
\end{equation}
where $\lambda = \frac{\sqrt{2\nu}}{z}$. $\sigma,\nu,z \in \mathbb{R}_{>0}$ are hyperparameters of the kernel. $\Gamma$ and $K_\nu$ are the gamma function and modified Bessel function of the second kind respectively. 

\subsubsection{Periodic Kernel}The periodic kernel has good expressiveness with oscillating features
\begin{equation}
        \begin{split}
    k(\Delta) &=e^{-2z^2sin^2(0.5w_0\Delta^2)},\\
    s(j\omega)s(-j\omega) &= \sum_{j=0}^\infty q_j^2(\delta(\omega-j\omega_0)+\delta(\omega+j\omega_0)).
 \end{split}
\end{equation}
Unfortunately, the power spectrum is a summation of impulse functions, $\delta$. It is possible to obtain a slightly modified state space approximating the series to the limit $J \rightarrow \infty$ \cite{solin2014explicit}
\begin{equation}
A_j^k = \begin{bmatrix} 0 & -j\omega_{0}\\   
j\omega_{0} & 0\end{bmatrix}.
\end{equation}
The measurement matrix $\mathbf{H}$ block row vector is a modified $\mathbf{H}_j^k = [1~0]$, while the noise diffusion is the standard $\mathbf{L}_j^k =[0~1]^\top$. To acquire a better approximation, the system $_k$ matrices are stacked similarly to the MISO system $k$ times. 