\section{Related Works}

In literature, GPs have been applied to learning residual or full system dynamics~\cite{Guillem_datadriveMPC, hewing2019cautious, Deisenroth_GP_robotics}, reward functions~\cite{biyik2020active}, planning \cite{barfoot2014batch}, adaptive control \cite{gahlawat2020l1}, or learning safety regions for repeated tasks such as in reinforcement learning \cite{Anayo_reachabilityonlineGP, WangonlineGP2018}. 
These methods are often implemented by hard coding the full GP model in C++ such as in \cite{Guillem_datadriveMPC, hewing2019cautious}. 
In the realm of Bayesian Machine learning, Python libraries, such as GPytorch \cite{gardner2018gpytorch} or GPyflow \cite{matthews2017gpflow}, are useful for training hyperparameters and inference. 
These libraries leverage PyTorch or TensorFlow systems respectively for back propagation and GPU integration allowing the easy implementation of new kernels or more complex GP models \cite{SMkernerl, zhao2021deep}, but are complicated to interface with robotics optimization solvers \cite{acados,FORCESPro,mastalli20crocoddyl}. 
%These libraries also require $O(n^3)$ time to perform inference although the processing time is internally mitigated by leveraging GPU parallelism when possible.  %Currently, our proposed library will use GPytorch to perform the training and convert the dataset and parameter file to a simple weight file that our ROS package in C++ can use to perform quick inference with the dataset.

Being non-parametric, the inference computation time of GPs generally scales with $O(n^3)$ with $n$ the dataset size even when using the aforementioned libraries although the corresponding processing time is internally mitigated by leveraging GPU parallelism. The process of accelerating GPs is a major field of study. 
Data dimensionality reduction techniques~\cite{Sivaram_fastGP_calc, joaquin_unifiyng_view, DAS201812} attempt to reduce the dataset size to a fraction of the points while keeping similar predictive performance. However, by performing data selection, a GP can never retain its full expressiveness especially if a large amount of data has to be discarded especially if converting inference time from a cubic order to a linear order a correspondingly large amount of data must be discarded.
Our current method sidesteps this issue by converting the GP to an approximate state space form which only requires $O(n)$ time complexity allowing us to perform rapid inference without discarding data. This approach is inspired by recent signal processing results~\cite{sarkka2011linear,  corenflos2022temporal}.
However, \cite{sarkka2011linear} derives the state space form but only for a SISO system and is tailored for signal processing problems. In addition, it is very difficult to obtain a step-by-step formulation and portable solution tailored for small-scale robots.
For further computation time increases among state space models, \cite{corenflos2022temporal} also proposes a framework for SSGPs, but the code relies heavily on pre-computing the Kalman filter predictions and updates steps to achieve these computation time gains making it less suitable for online learning.  
The aforementioned SSGP works are also not formulated for processing multiple inputs in the GPs and only consider single outputs with the exception of~\cite{sarkka2013spatiotemporal}. However, this approach is difficult to derive, implement, and practically use for dimensions beyond two. \projectName~ is easy to derive, and implement and has been experimentally validated. Finally, our framework seeks to promote the use of additional kernels by implementing several variants and provides a clear approach to enable any user to derive with minor effort additional ones based on the research needs. %  for an application with an input space of $5$ dimension.


%In addition to computation speed, the kernel choice is well known to have a massive effect on the GPs performance \cite{RasmussenW06}. To tackle this issue there is a body of research such as \cite{SMkernerl, zhao2021deep} dedicated to creating more expressive kernels. This is achieved by either learning the kernel function through a set of basis functions such as Gaussian in \cite{SMkernerl} or layering GPs on top of each other \cite{zhao2021deep} to construct a deeper network similar to neural networks \cite{Ale_PiTCN}. Unfortunately, these methods have not been used much in the robotics community most works on real applications used the radial basis function kernel (RBF). 

