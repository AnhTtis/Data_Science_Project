\section{Method}
\subsection{Approach Overview}
Taking $C=\{c_1, c_2,...,c_{T_{c}}\}$ as context, the objective of dialogue generation task is to generate the response $R=\{r_1, r_2,...,r_{T_{r}}\}$, where $T_{c}$ and $T_{r}$ represent the length of context and response, respectively. Instead of training a complicated and huge model, we build a collaborative dialogue learning framework to obtain a less-parameterized but effective model for inference. The overview of the proposed framework is illustrated in Figure~\ref{fig.method1}. 
In consideration of diverse dialogue attributes, we split the training dataset to several sub-sets according to scoring methods measuring the sample quality from multiple perspectives. Each attribute-related sub-set guides one branch to learn the corresponding specific knowledge. After that, we propose dual knowledge distillation in which positive distilling occurs between the master branch and all of the auxiliary branches, while negative distillation occurs within the attribute-related branches to encourage them to learn different dialogue properties. The orthogonal negative distillation is designed to identify biased features without interfering with knowledge.

\begin{figure*}[th]
\centering
\includegraphics[width=0.95\linewidth]{figs/method1.pdf}
\caption{An overview of the proposed heterogeneous attribute-aware collaborative dialogue learning.}
\label{fig.method1}
\end{figure*} 

\subsection{Dialogue Attribute Learning}

The generative dialogue model aims to learn a conditional probability distribution $p_{\theta}(R|C)$.
The maximum likelihood estimation (MLE) is usually used to guide the model to generate the target responses:
\begin{equation}
\mathcal{L}_{\mathrm{MLE}}=-\sum_{i=1}^{T_{r}} \log p_{\theta}\left(r_{i} \mid r_{<i}, C \right),
\label{eq:mle}
\end{equation}
where $r_{i}$ is the ground-truth tokens.
Therefore, the performance of the dialogue model largely depends on the distribution characteristics of the training set. 
Recently, a line of work introduces a data manipulation strategy, to boost the model performance with the corresponding dialogue attributes. 
They first measure the quality of samples in terms of a certain dialogue attribute by a scoring method, and then discard the low-score samples. The selection data can induce the model to learn attribute-related features more effectively for the generation of high-quality responses.
Specifically, the raw training samples $\mathcal{D}$ are reorganized into multiple view-specific training sub-sets ($\mathcal{D}_{1}, \mathcal{D}_{2}, \cdots, \mathcal{D}_{M}$) based on the scores of $\mathcal{S}_{m}$ and a certain selection proportion. Note that each sample can be assigned to multiple sub-sets as it may obtain high scores from more than one scoring method. 
Then, each branch $m$ is trained with corresponding sub-set $\mathcal{D}_{m}$ with Equation~\ref{eq:mle}.
Three dialogue attributes are considered in this paper and the following is the details of their corresponding scoring methods:
\paragraph{Coherence} reflects how well a dialogue response semantically relates to its context.
A joint score \citep{FilterConsistency-Akama-2020}:
\begin{equation}
S_{C+R}(c, r) = \alpha S_{C} + \beta S_{R}
\end{equation}
that contains two parts: connectivity $S_{C}$ and content relatedness $S_R$.
    The $S_{C}$ is evaluated by the co-occurrence of key-phrases ($p\in q$, $h\in r$):
    \begin{equation}
    % \small
    \label{eq:sc}
         S_C = 
         \sum_{(p,h)} \frac{\max(nPMI(p,h),0)\cdot |p| \cdot |h|} {|c| \cdot |r|},
    \end{equation}
    where $|\cdot|$ means the number of words and the $nPMI$ represents the normalized pointwise mutual information \citep{nPMI-Bouma-2009}. In addition, $S_{R}$ is evaluated by the cosine of the context and its response:
    \begin{equation}
    % \small
        S_R = \max(cos(c_{emb}, r_{emb}),0)
    \end{equation}
    The $c_{emb}$ and the $r_{emb}$ are vector representations of the context and response.
    % This scoring method can reflect the consistency of a dialogue pair. 
    
\paragraph{Informativeness} reflects how much the information related to the query is contained in the generated response, which is evaluated by Entropy\_Src \citep{FilterEntropy-Csaky-2019}: 
This score is the entropy of a response utterance:
    \begin{equation}
    % \small
        H_{src}(r|D) = -\sum_{(c_i, r)\in D}p(c_i|r)\log{p(c_i|r)},
    \end{equation}
    where $r$ represents the response, $D$ represents the dialogue dataset, and $c_i$ means a context of $r$ in $D$. By using this scoring method, the dialogue pair with many-to-one problem will be filtered, thereby alleviating the phenomenon of general response.
    
\paragraph{Specificity} \citep{FilterSpecificity-See-2019} reflects how much the generated response is good at word usage:
    \begin{equation}
    % \small
        Spe(t) = \frac{idf(t)-min\_idf}{max\_idf-min\_idf},
    \end{equation}
    where $t$ is a token of the response, and $idf(t)=\log(\frac{R}{R_t})$. $R$ is the number of responses in the dataset, and $R_t$ is the number of those responses that contain $t$. Using this scoring method, specific tokens can be identified in the response.



\subsection{Dual Knowledge Distillation}
The distillation objective is employed to alter the representation of two models, denoted as $f^{A}(x)$ and $f^{B}(x)$:

\begin{equation}
\mathcal{L}_{\mathrm{KD}}=\sum_{x \in \mathcal{D}} L\left(f^{A}(x), f^{B}(x)\right),
\end{equation}
where $L(\cdot)$ provides a measurement function for calculating distances between representations in  multi-levels.

A conventional collaborative learning process only distills positive knowledge, where $L(\cdot)$ is aiming to minimize the distance between branches. However, when it comes to attribute-related branches in dialogue learning, there are different directions in which they tend to converge. It is not appropriate to directly apply positive knowledge distillation to the collaborative dialogue learning framework. In this paper, we propose dual knowledge distillation consisting both positive and negative distillation (where $L(\cdot)$ seeks to maximize the distance between auxiliary branches), as a means of transferring attribute-specific knowledge in a reasonable manner.

\paragraph{Positive Distillation}
In order to transfer the attribute-aware knowledge to master branch, positive distillation (PD) is performed on the prediction layer:
\begin{align}
\nonumber  \mathcal{L}_{PD}(\boldsymbol{A}, \boldsymbol{B})  & =  -\sum_{i=1}^{T_{r}} \sum_{k=1}^{|\mathcal{V}|} p_{\boldsymbol{A}} \left(r_{i}=k \mid r_{<i}, C\right)  \\
 & \cdot \log p_{\boldsymbol{B}}\left(r_{i}=k \mid r_{<i}, C \right),
\end{align}
where $\boldsymbol{A}$, $\boldsymbol{B}$ refers to two branches and $p_{\boldsymbol{A}}$, $p_{\boldsymbol{B}}$ are calculated by:
\begin{equation}
p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)},
\end{equation}
where the probability distribution over words is softened with a temperature coefficient $T$.
Positive Distillation is carried out in a bidirectional manner between the master branch and the auxiliary branches. On the one hand, the attribute-specific knowledge can be absorbed by the master branch. On the other hand, the consolidated knowledge from the master branch needs to be transferred to the auxiliary branches in order to facilitate the generation of higher quality responses from them.

\paragraph{Negative Distillation for Prediction Layer}
For the purpose of encouraging auxiliary branches to better obtain its own specific knowledge, we use the soft unlikelihood loss from \citet{negative-distill} to achieve the negative distillation (ND) within them for the prediction layer first:
\begin{align}
\label{eq:pred}
\nonumber   \mathcal{L}_{ND_{pred}}(\boldsymbol{A}, \boldsymbol{B}) = & -\sum_{i=1}^{T_{r}} \sum_{k=1}^{|\mathcal{V}|} p_{\boldsymbol{B}} \left(r_{i}=k \mid r_{<i}, C \right) \\
& \cdot \log \left(1-p_{\boldsymbol{A}}\left(r_{i}=k \mid r_{<i}, C \right)\right),
\end{align}
Through this function, the distance between token prediction probabilities becomes larger, resulting in different branches producing different responses reflecting their own dialogue attributes and improving branch heterogeneity.

\paragraph{Negative Distillation with Orthogonal Projection}
Besides the explicit knowledge from the prediction layer, implicit knowledge embedded in the hidden states can also help the negative distillation process. In spite of the fact that different auxiliary branches acquire different attribute-specific knowledge, there should be some shared features in hidden states to support the basic abilities of sentence generation. Directly increasing the distance of hidden states between branches by negative distillation will damage the common knowledge for dialogue generation.

\begin{figure}[t]
\centering
\includegraphics[width=0.6\linewidth]{figs/method2.pdf}
\caption{Orthogonal Projection for Hidden States.}
\label{fig.method2}
\end{figure} 

Therefore, we propose orthogonal negative distillation to protect the common features from interference inspired by \citet{DBLP:conf/iclr/WangHLX19}. 
Specifically, as shown in Figure~\ref{fig.method2}, we project the hidden state $\mathbf{H}_{A}$ to the orthogonal space of hidden state $\mathbf{H}_{B}$ in order to get $\mathbf{H}_{L}$:
\begin{equation}
\mathbf{H}_{L}=\left(\mathbf{I}-\mathbf{H}_{B}\left(\mathbf{H}_{B}^{T} \mathbf{H}_{B}\right)^{-1} \mathbf{H}_{B}^{T}\right) \mathbf{H}_{A}
\end{equation}
$\mathbf{H}_{L}$ contains the biased features of $\mathbf{H}_{A}$ which reflects its attribute-specific knowledge comparing with $\mathbf{H}_{B}$, getting rid of the shared features within them.
On this basis, we conduct negative distillation with mean reverse square error (MRSE) \citep{negative-distill} between $\mathbf{H}_{L}$ and $\mathbf{H}_{B}$, which is conducive to dialogue attribute learning for branch $\boldsymbol{A}$ while avoiding the common knowledge interface. The loss function is then defined as:
\begin{equation}
    \mathcal{L}_{ND_{hidden}}(\mathbf{H}_{L}, \mathbf{H}_{B}) = \frac{1}{n} \sum^{n}_{i=1} \exp^{-SE(\mathbf{H}_{L}, \mathbf{H}_{B})},
\end{equation}
where $SE$ refers to square error. Note that we only perform ND on the last hidden states of decoder for training efficiency.

\paragraph{Optimization}
For the proposed collaborative dialogue learning framework, the overall objective function consists of two terms: a conventional cross entropy loss for dialogue generation and online knowledge distillation loss for collaborative learning.
Specifically, the loss for master branch is:
\begin{equation}
\mathcal{L}= \mathcal{L}_{MLE} + \frac{1}{|M|} \sum^m \mathcal{L}^m_{PD},
\end{equation}
where $|M|$ is the number of auxiliary branches.
While for each auxiliary branch:

\begin{align}
\nonumber   \mathcal{L}= \mathcal{L}_{MLE} +  \mathcal{L}^m_{PD}  + \frac{1}{|M|-1} \sum^{m} \mathcal{L}^m_{ND_{pred}} \\
 + \frac{1}{|M|-1} \sum^{m} \mathcal{L}^m_{ND_{hidden}}.
\end{align}
All branches are trained simultaneously at each epoch until the master branch converges. 