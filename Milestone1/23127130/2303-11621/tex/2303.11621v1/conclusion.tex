\section{Conclusion}
We present a novel collaborative dialogue learning paradigm to improve the quality of generated responses in terms of three major dialogue attributes. 
CDL replaces traditional knowledge distillation with collaborative group-based distillation for lightweight knowledge interaction, and the attribute-aware knowledge is captured and transferred through auxiliary branches.
Dual group-based knowledge distillation is proposed for better guiding auxiliary branches to learn attribute-specific knowledge.
Besides, we further boost the performance of negative distillation by utilizing orthogonal projection to avoid harming the common knowledge.
Extensive experiments validate the superiority of our proposed method over prior collaborative learning work.