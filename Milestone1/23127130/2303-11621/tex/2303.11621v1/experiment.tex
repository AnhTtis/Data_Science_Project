\section{Experiment}
\subsection{Datasets}
We evaluate the proposed method using two widely used dialogue datasets: 
\textbf{DailyDialog}, a collection of conversations that represent human daily communication \citep{dailydialog2017}, and \textbf{OpenSubtitles}, which consists of large-scale dialogues extracted from movie subtitles \citep{opensubtitles2009}. After data preprocessing, the number of context-response pairs in training/validation/test set is 68,066/6,820/6,841 for DailyDialog, and 200,000/20,000/10,000 for OpenSubtitles.


\begin{table*}[th]
    \centering
    \begin{tabular}{l c c c c c c c c c c c c}
    \toprule
    Models & Dist-1 & Dist-2 & Dist-3 & BLEU-1 & BLEU-4 & AVE & COH & H-1 & H-2 & H-3 & KL & LF 
    \\ \midrule
       Transformer & 0.0080  & 0.0345  & 0.0748  & 0.2963   & 0.4113   & 0.8151 & 0.7058  & 6.77  & 7.46  & 9.96  & 0.81  & 0.0825   \\ 
        DML & 0.0167  & 0.0669  & 0.1296  & 0.3154   & 0.4221  & 0.8164   & 0.7069  &6.97 & 7.87  & 10.50  & 0.50  & 0.1427    \\
        CL-ILR & 0.0167  & 0.0686  & 0.1369  & 0.3223  & 0.4241    & 0.8179    & 0.7078  & 6.95  & 7.87  & 10.50  & 0.51  & 0.1355    \\ 
        ONE & 0.0120  & 0.0489  & 0.0995  & 0.3248  & 0.4082    & 0.8170   & 0.7072  & 6.95  & 7.76  & 10.42  & 0.66  & 0.1174    \\ 
        OKDDip & 0.0141  & 0.0581  & 0.1168  & 0.3097  & 0.4212    & \underline{0.8188}    & 0.7100  & 6.90  & 7.75  & 10.31  & 0.55  & 0.1376  \\  \midrule
        CDL-CI &  \underline{0.0191}  & \underline{0.0815}  & \underline{0.1679}  & 0.3139   & \textbf{0.4283}   & 0.8182   & 0.7074  & 6.89  & 7.83  & 10.45  & \underline{0.41}  & 0.1514    \\ 
        CDL-CS & 0.0186  & 0.0785  & 0.1561  & \textbf{0.3317}  & 0.4108   & \textbf{0.8198}  & \textbf{0.7177}  & \underline{7.07}  & \underline{7.99}  & \underline{10.70}  & 0.42  & \underline{0.1603}    \\ 
        CDL-IS & \textbf{0.0252}  & \textbf{0.1081}  & \textbf{0.2143}  & \underline{0.3184}   & \underline{0.4261}   & 0.8179  & \underline{0.7121}  & \textbf{7.12}  & \textbf{8.13}  & \textbf{10.80}  & \textbf{0.32}  & \textbf{0.1778}  \\  \bottomrule
        \toprule
       Transformer & 0.0031  & 0.0140  & 0.0302  & 0.3552  & 0.3062  & 0.7891  & \underline{0.7048}  & 6.71  & 7.64  & \underline{10.99}  & 1.31  & 0.0349  \\ 
        DML & 0.0044  & 0.0171  & 0.0344  & 0.3494   & 0.3248  & 0.7907  & 0.6801  & 6.41  & 7.11  & 10.16  & 1.58  & 0.0363   \\
        CL-ILR & 0.0044  & 0.0179  & 0.0368  & 0.3310   & 0.3151  & 0.7804  & 0.6648  & 6.48  & 7.26  & 10.32  & 1.49  & 0.0513  \\
        ONE & 0.0043  & 0.0175  & 0.0369  & 0.3510   & 0.3140  & 0.7922  & 0.6921  & 6.56  & 7.43  & 10.60  & 1.40  & 0.0410   \\
        OKDDip & 0.0035  & 0.0141  & 0.0300  & 0.3487  & 0.3244  & 0.7886  & 0.6743  & 6.49  & 7.19  & 10.30  & 1.55  & 0.0356   \\ \midrule
        CDL-CI & \textbf{0.0057}  & \textbf{0.0239}  & \textbf{0.0523}  & \underline{0.3474}   & \underline{0.3254}  & \textbf{0.7983}  & \textbf{0.7156}  & \underline{6.73}  & \textbf{7.71}  & \textbf{11.03}  & \textbf{1.18} & \textbf{0.0555}   \\ 
        CDL-CS & \underline{0.0050}  & 0.0197  & 0.0419  & \textbf{0.3552} & 0.3146  & \underline{0.7924}  & 0.6996  & 6.71  & 7.63  & 10.93  & \underline{1.29} & 0.0426   \\ 
        CDL-IS & \underline{0.0050}  & \underline{0.0211}  & \underline{0.0460}  & 0.3443  & \textbf{0.3258}  & 0.7893  & 0.6923  & \textbf{6.78}  & \underline{7.68}  & 10.95  & \underline{1.29} & \underline{0.0524}   \\ \bottomrule
    \end{tabular}
    \caption{Automatic evaluation results on DailyDialog (Up) and OpenSubtitles (Down). The best/second-best results are \textbf{bold}/\underline{underlined}. The branch number is 3. C refers to coherence, I for informativeness and S for specificity.}
    \label{tb:main_exp}
\end{table*}

\subsection{Implementation Details}
All approaches are based on the Transformer-based sequence-to-sequence model \citep{Transformer-Vaswani-2017}. Each branch is built on the lightweight model architecture (Small Transformer): the encoder and decoder contain only 2 layers, in which the self-attention module has 4 attention heads and 1024 feed-forward units. The size of hidden states is set to 256.
Dropout \citep{DBLP:journals/jmlr/SrivastavaHKSS14} is used for the self-attention module, the feed-forward layer, and the activation layer, and the rate of all three is set to 0.1.
The batch size is set to 64.
The selection ratio for attribute-specific subset is 70$\%$.
For the temperature coefficient $t$, we simply set it to 1. 
Beam search with a size of 5 is used for decoding.
We implement all approaches with Pytorch 1.11, and conduct all experiments on NVIDIA TITAN RTX.


\subsection{Comparison Methods}
We compare our proposed collaborative dialogue learning (CDL) framework with following established collaborative learning approaches:
\begin{itemize}
    \item DML \citep{DML} uses a pool of network-based students, where each student is an individual network and they asynchronously collaborate.
    \item CL-ILR \citep{CL-ILR} distills knowledge among multiple branches of a hierarchical network.
    \item ONE \citep{ONE} automatically generates gated ensemble logit from each branch as a soft target.
    \item OKDDip \citep{OKDDip} proposes a two-level distillation strategy with multiple auxiliary peers and a group leader, while utilizing an attention module to construct inter-branch diversity.
\end{itemize}

Following previous work, we set the branch number is 3 for all the comparison models. For the proposed framework, it contains one master branch and two auxiliary branches with different dialogue attributes, i.e.,  coherency (C), informativeness (I) and specificity (S).

\subsection{Automatic Evaluation}
\paragraph{Metrics}
We first used automatic metrics to evaluate our method:
\textbf{Dist-\{1,2,3\}} (distinct) \citep{MMI-LiJiwei-2016} is a widely used metric that reflects the lexical diversity of the generated responses by calculating the proportion of unique unigrams/bigrams/trigrams.
\textbf{BLEU} \citep{DBLP:conf/wmt/ChenC14} measures n-gram overlap between the generated and the ground-truth responses. 
\textbf{AVE} (Embedding Average) \citep{DBLP:conf/emnlp/LiuLSNCP16} evaluates the semantic relationship of generated responses and ground-truth responses.
\textbf{COH} (coherence) \citep{FilterCoherence-Xu-2018} measures the cosine similarity between pairs of input and response. 
\textbf{H-\{1,2,3\}} (word entropy) \citep{VHRED-Serban-2017} measures the unigrams/bigrams/trigrams' non-genericness of responses.
\textbf{KL} (KL divergence) \citep{FilterEntropy-Csaky-2019} measures the distribution distance between the generated and the ground-truth response sets to reflect how well a model can approximate the ground-truth distribution. Note that the lower KL is better.
\textbf{LF} (low-frequency token ratio) \citep{DBLP:conf/iclr/LiWCUS0Z20} further measures the diversity of responses by calculating the ratio of low-frequency words in the generated responses. The threshold of low frequency is set to 100.

\paragraph{Results}
Table~\ref{tb:main_exp} shows the results obtained at the lowest point of the validation loss. It illustrates that our framework outperforms all baselines by a significant margin on both datasets. Note that four collaborative learning baselines perform better than vanilla Transformer model, which proves that the group-base distillation can improve model performance greatly. On this basis, the proposed approach can further enhance the performance by introducing dialogue attributes learning and dual knowledge distillation. And the improvement of different dialogue attributes can be reflected by the corresponding metrics.
\subsection{Human Evaluation}
\begin{table}[th]
    \centering
    \begin{tabular}{l c c c c}
    \toprule
        vs. Models & Win & Tie & Loss & Kappa  \\  \midrule
        Transformer & 0.82 & 0.15 & 0.03 & 0.5487\\
        DML  & 0.47 & 0.42 & 0.11 & 0.6651 \\
        CL-ILR  & 0.43 & 0.53 & 0.05 &  0.5393\\
        ONE  & 0.50 & 0.39 & 0.11 & 0.6177\\
        OKDDip & 0.45 & 0.43 & 0.11 & 0.5743\\ 
        \bottomrule
    \end{tabular}
    \caption{Human evaluations results on DailyDialog. Our framework has a higher win rate than baselines.}
    \label{tb:human}
\end{table}

To further verify the effectiveness of our method in comparison to previous collaborative learning methods, we also conduct human evaluations apart from automatic evaluations.
We randomly select 50 samples from the test set of DailyDialog, and three well-educated annotators are invited to judge which of the overall response quality generated by CDL and baselines is better (i.e., win, tie or loss) in terms of coherence, informativeness and fluency.

Table~\ref{tb:human} summarizes the human evaluation results. In our experience, we have noticed that a dialogue model trained using our proposed learning framework is more capable of producing responses that are human-preferred.
We use Fleiss's kappa \citep{fleisskappa/measuring} to measure the inter-annotator agreement, which indicates that the annotators came to a fair agreement in the judgment.

\subsection{Analysis}
In order to better understand the effectiveness of the collaborative dialogue learning, we carry out extensive analysis of DailyDialog.

\paragraph{Ablation study} 
\begin{table}[th]
    \centering
    \begin{tabular}{l c c c c c c c c}
    \toprule
        Models & Dist-1 & Dist-2 & LF  & KL  & H-1 \\  \midrule
        w/o Attributes & 0.0177 & 0.0756 & 0.1367 & 0.38  & 6.94  \\
        w/o OP & 0.0203 & 0.0869 & 0.1629 & 0.37 & 6.99 \\
        w/o $\mathcal{L}_{ND_{hidden}}$  & 0.0222 & 0.0931 & 0.1621 & 0.41 & 7.03 \\
        w/o $\mathcal{L}_{neg}$  & 0.0215 & 0.0890 & 0.1776 & 0.43 & 7.05 \\
 \midrule
        Full Version   & 0.0252 & 0.1081 & 0.1778 & 0.32 & 7.12 \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study results of the proposed collaborative dialogue learning framework.}
    \label{tb:ablation}
\end{table}

We study the effects of different parts of proposed framework by ablating the dialogue attribute learning (w/o attributes), the orthogonal projection (w/o OP), the hidden state distillation
(w/o $\mathcal{L}_{ND_{hidden}}$), and the whole negative distillation (w/o $\mathcal{L}_{Neg}$). 
The results in Table~\ref{tb:ablation} show that all proposed techniques are useful for improving the response quality. 
The significant decline in w/o attributes indicates that the knowledge of specific dialogue property is very important for CDL. 
w/o $\mathcal{L}_{Neg}$ is better than w/o OP, indicating that orthogonal projection is a key technique to capture biased features without harming common knowledge.

\paragraph{Comparison with traditional KD}

\begin{table}[th]
    \centering
    \begin{tabular}{l c c c c c c c c}
    \toprule
        Models & Dist-1 & Dist-2 & LF  & KL  & H-1 \\  \midrule
        Small & 0.0080 & 0.0345 & 0.0825 & 0.81 & 6.77 \\
        Base & 0.0101  & 0.0471  & 0.1084  & 0.56  & 6.83  \\
        KD & 0.0124  & 0.0564  & 0.1336  & 0.47  & 6.94  \\
        CDL-IS & 0.0252 & 0.1081 & 0.1778 & 0.32 & 7.12 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison results with traditional knowledge distillation.}
    \label{tb:kd}
\end{table}

Traditional knowledge distillation is an efficient method to obtain a small but effective model. The results from Table~\ref{tb:kd} show that CDL outperform KD (Teacher is Base Transformer) and Small with the same inference cost and the relative heavy Base model without a well-trained teacher.

\paragraph{Branch Number Study}
\begin{table}[th]
    \centering
    \begin{tabular}{l c c c c c c c c}
    \toprule
        Models & Dist-1 & Dist-2 & LF  & KL  & H-1 \\  \midrule
        DML   & 0.0154 & 0.0644 & 0.1368  & 0.51  & 6.95  \\
        CL-ILR  & 0.0154 & 0.0625 & 0.1232  & 0.52  & 6.90  \\
        ONE  & 0.0104 & 0.0432 & 0.1022  & 0.69  & 6.88  \\
        OKDDip  & 0.0132 & 0.0576 & 0.1348  & 0.48  & 6.96  \\
        CDL-C  & 0.0195 & 0.0833 & 0.1492 & 0.39 & 6.97  \\
        CDL-S & 0.0182 & 0.0730 & 0.1515 & 0.54  & \textbf{7.03}  \\
        CDL-I  & \textbf{0.0207} & \textbf{0.0889} & \textbf{0.1834} & \textbf{0.37}  & 6.93  \\
        \bottomrule
        \toprule
        Models & Dist-1 & Dist-2 & LF  & KL  & H-1 \\  \midrule
        DML   & 0.0198 & 0.0779 & 0.1457  & 0.44  & 6.93  \\
        CL-ILR  & 0.0159 & 0.0637 & 0.1362  & 0.56  & 6.97  \\
        ONE  & 0.0128 & 0.0523 & 0.1194  & 0.61  & 6.92  \\
        OKDDip  & 0.0167 & 0.0655 & 0.1274  & 0.56  & 6.99  \\
        CDL-CSI  & \textbf{0.0211} & \textbf{0.0869} & \textbf{0.1672} & \textbf{0.41}   & \textbf{6.99} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation Results with branch number 2 (Up) and 4 (Down).}
    \label{tb:number}
\end{table}

We explore the performance of proposed CDL in other number of branches. The results from Table~\ref{tb:number} shows that, regardless of the number of branches, performance of CDL is better than baselines. The inferior performance with branch number 4 (compared with Table~\ref{tb:main_exp}) is that the influence of positive distillation from master branch is much lower than with branch number 3, given that the more auxiliary branches the more negative distillation will be conducted. The promising way to solve this problem is to increase the weight of positive KD loss and decrease negative KDâ€™s, and we leave it as future work.



\paragraph{Model Diversity Analysis}

\begin{table}[th]
    \centering
    \begin{tabular}{l c c c c c}
    \toprule
         & DML & CL-ILR & ONE  & OKDDip & CDL  \\  \midrule
        L2  & 0.163 & 0.141 & 0.198 & 0.165 & 0.308 \\
        \bottomrule
    \end{tabular}
    \caption{Branches diversity for CDL and other collaborative baselines.}
    \label{tb:distance}
\end{table}

we show the diversity that our method brings in a more intuitive way. We use the Euclidean distance (L2) between branches as a quantitative criterion for diversity, as performed in OKDDip \citep{OKDDip}. Table~\ref{tb:distance} shows the averaged L2 distances when each model reaches to convergence. Our mutual diversity is significantly greater than other methods, which suggests that the homogenization problem between branches have greatly relieved through multi-attribute learning and negative distillation.


\paragraph{Case Study}

\begin{table}[ht]
\centering

\begin{tabular}{@{}p{\linewidth}@{}}
\toprule
\textbf{Input:} Here are all kinds of jades. Choose whatever you like, please. \\
\midrule
\textbf{Transformer:} Ok. \\
\textbf{DML:} Thank you. \\
\textbf{CL-ILR:} Thank you very much. \\
\textbf{ONE:} Thank you. I have a good idea. \\
\textbf{OKDDip:} Thank you very much.  \\
\textbf{CDL:} Thank you, sir. I 'll be glad to do that.  \\
\midrule
\midrule
\textbf{Input:} What do you recommend? We'd like some of your local specialities. \\
\midrule
\textbf{Transformer:} That's good. \\
\textbf{DML:} That's a good idea. What do you want to do? \\
\textbf{CL-ILR:} I don't know what you mean. \\
\textbf{ONE:} Well, I think I'd like to have a cup of coffee. \\
\textbf{OKDDip:} Well, I've heard about it, but I don't think it's a good idea.  \\
\textbf{CDL:} That's great. I'd like to have some cheese.  \\
\bottomrule
\end{tabular}
\caption{Examples of generated responses.}
\label{tb:case study}
\end{table}

Table \ref{tb:case study} presents some responses generated by the proposed framework and baselines. Transformer prefers generic and meaningless responses. Other baselines lack of concerning one perspective of dialogue. In contrast, our CDL comprehensively consider the multiple perspectives, thus resulting in diverse and coherent responses. The results demonstrate the effectiveness of CDL.