\section{Introduction}
Open-domain Neural dialogue generation \citep{Seq2Seq-Sordoni-2015,NoisyData-Vinyals-2015,Seq2Seq-ShangLifeng-2015}, aiming to generate diverse and coherent responses, has gained increasing attention and achieved impressive performance. 
It is important to recognize, however, that these considerable improvements typically come at the expense of over-parameterized networks, inhibiting their development on real-world resource-limited scenarios such as mobile chatbot applications.
Knowledge distillation is an appropriate knowledge-transfer methodology to resolve this issue, which uses predicted distributions \citep{DBLP:journals/corr/HintonVD15}, hidden states \citep{DBLP:conf/emnlp/SunCGL19}, or attention matrices \citep{DBLP:conf/emnlp/JiaoYSJCL0L20}, etc. of a teacher model as targets to induce the student to imitate.
A conventional distillation process involves two stages that begin with a cumbersome pre-trained teacher model and then distill the knowledge to the compact student model.
Unfortunately, training such a complex teacher model is time-consuming and a high-capacity model may not always be available.

\begin{figure}[th]
\centering
\includegraphics[width=1.0\linewidth]{figs/intro-1.pdf}
\caption{Dialogue examples with two levels related to three dialogue attributes. The response quality can be assessed by multiple perspectives.}
\label{fig.example}
\end{figure} 

With a view to overcoming traditional limitations, online knowledge distillation \citep{Online-distill,ONE}, also called collaborative learning \citep{DML,CL-ILR}, is currently receiving considerable attention. 
Instead of pre-training a high-capacity teacher, collaborative learning conducts a single-stage group-based knowledge distillation that transfers the knowledge between less-parameterized student branches simultaneously. 
Aside from accelerating model learning efficiency over conventional KD, another major advantage of collaborative learning is the ability to find a more robust local minimum when compared to a single model learning method. 
It is important to note, however, that the homogeneity problem among branches along with training will lead to early saturation, thereby affecting the effectiveness of group distillation.
To alleviate this problem, \citet{OKDDip} impeded homogenization by equipping a diversity holding mechanism; \citet{PCL} randomly enhanced the input to guarantee the discrepancy between branches; \citet{CGL} proposed random routing to improve the diversity of features.

Even though the aforementioned approaches have demonstrated their superiority, one major drawback remains to limit further branch heterogenization: previous work only focused on the classification task, resulting in the same training objective (i.e. classification accuracy) for all branches of the framework with independent identical distribution (i.i.d.) training data. It will make different branches tend to converge to similar feature representations \citep{Convergent,ONE,OKDDip}. Consequently, there is barely any intuitive approach to allow branches to develop in various training directions, which is a significant obstacle to fostering diversity among them.

As opposed to classification task, goals of dialogue generation model focus more on dialogue attributes than the accuracy with the references. As demonstrated in \citet{FilterSpecificity-See-2019}, inadequate modeling of conversational aspects such as coherence and specificity results in inferior model performance and low response quality. Taking Figure~\ref{fig.example} as an example, response quality can be affected by multiple perspectives. As a result, directly applying the collaborative learning to the dialogue generation task will lead to sub-optimal performance. A variety of dialogue attributes can provide a natural insight into improving branch heterogeneity. With this in mind, it is possible to develop more effective and interpretable techniques to further enhance branch diversity.

In this work, we propose a heterogeneous attribute-aware collaborative learning paradigm for response generation, comprising two types of branches: auxiliary and master.
To achieve the goals of the dialogue system in a fine-grained way, we train each auxiliary branch on the corresponding aspect-specific sub-set to capture features along with some dialogue attributes. Each sub-set is collected according to the corresponding scoring method. Unlike auxiliary branches, the master branch is trained with the entire dataset to learn features roughly but comprehensively. 
In previous work, each branch learns from all the other branches, which is prone to homogenize different branches as the training continues. To further improve the diversity of auxiliary branches and integrate multi-view knowledge steadily, we propose the dual group-based knowledge distillation, consisting of positive distillation \citep{DBLP:journals/corr/HintonVD15} and negative distillation \citep{negative-distill}. 

Specifically, positive distillation is conducted from all the auxiliary branches to the master branch, transferring the attribute-specific knowledge effectively, whilst negative distillation is performed within the attribute-related branches to enforce them to learn different dialogue properties. 
Furthermore, negative distillation is implemented on hierarchical feature representation to use multi-level negative knowledge.
However, due to some common features shared by different auxiliary branches, blindly maximizing the distance of hidden states among auxiliary branches will harm the model performance and the training stability. To this end, we design a novel distillation approach called orthogonal negative distillation. It only strengthens the features of each auxiliary branch orthogonal to the other branches, avoiding disturbing the learning of common knowledge.

In summary, our contributions are as follows:
\begin{itemize}
    \item To the best of our knowledge, we are the first to propose the collaborative dialogue learning approach that transfers attribute-aware knowledge in a one-stage manner.
    \item Dual group-based knowledge distillation is proposed for better guiding auxiliary branches to learn attribute-specific knowledge. Orthogonal negative distillation can incentivize branches to capture biased features while avoiding harming the common knowledge.
    \item Extensive Evaluations on two widely used open-domain dialogue datasets demonstrate that the proposed approach significantly improves the branch heterogeneity and outperforms the state-of-the-art collaborative learning methods.
\end{itemize}