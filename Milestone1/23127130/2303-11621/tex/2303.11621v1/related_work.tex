\section{Related Work}
\subsection{Dialogue Models}
There are three major categories of previous work on enhancing the quality of responses. 
The first redesigns the model structure to facilitate the modeling of the dialogue pairs \citep{DBLP:conf/aaai/SerbanKTTZBC17,CMHAM-TaoChongyang-2018,SpaceFusion-GaoXiang-2019}. 
The second further proposes advanced objectives aligning with the goals of the conversation more effectively, such as MMI \citep{MMI-LiJiwei-2016}, CVAE \citep{VHRED-Serban-2017,kgCVAE-ZhaoTiancheng-2017,DialogWAE-GuXiaodong-2019,DBLP:conf/acl/SunFLLL20}, RL \citep{RLdialoguesys-LiJiwei-2016,RL-Seq2seqCo-Zhang2018,RL-P2BOT-Liu2020}, and GAN \citep{GAN-GANAEL-Xu2017,DPGAN-XuJingjing-2018,PosteriorGan-FengShaoxiong2020}. 
The third tries to endow the responses with topic \citep{TopicAware-XingChen-2017,DBLP:conf/emnlp/FengRCSLS20}, emotion \cite{Emotional-Zhouhao-2018,DBLP:conf/acl/RashkinSLB19}, and persona \citep{Persona-QianQiao-2017,PersonaChat-facebook-2018,DBLP:conf/acl/SongWZLL20}.
Recently, data filtering has been introduced for dialogue learning, which discards samples regarded as low-quality by a scoring method to reflect corresponding dialogue attributes. 
\citet{FilterEntropy-Csaky-2019} proposes an entropy-based scoring method to remove generic utterances from the training data. \citet{FilterSpecificity-See-2019} designs a scoring method to measure the specificity of samples. \citet{FilterConsistency-Akama-2020} combines the cosine distance and the keyword co-occurrence of the dialogue pairs to evaluate the coherence. \citet{DBLP:conf/cikm/ShenZSCZZ21} presents a fusing approach to data filtering and \citet{DBLP:journals/corr/abs-2205-11206} utilizes the scoring methods to enhance rather than filter data.

\subsection{Knowledge Distillation}
In recent years, knowledge distillation \citep{DBLP:journals/corr/HintonVD15, DBLP:journals/corr/FreitagAS17} has been widely adopted by researchers to accelerate and compress models \citep{DBLP:conf/emnlp/JiaoYSJCL0L20, DBLP:journals/corr/abs-1910-01108}. As these predicted distributions contain ranking information on similarities among categories, it treats the predictions as knowledge learned by the teacher network. As a result, it enforces similar predictions on the student network in order to transfer this knowledge. By providing more knowledge to the student network from different sources, the work follows this idea. To supervise the student network, FitNets \citep{DBLP:journals/corr/RomeroBKCGB14} uses both predictions and intermediate representations learned by the teacher network. \citet{DBLP:conf/emnlp/KimR16} propose using sequence-level knowledge generated from the generated sequences to guide student network training in the Seq2Seq model. Furthermore, self-knowledge distribution \citep{DBLP:conf/ranlp/HahnC19} demonstrates that students are able to improve performance by using their own knowledge. When it comes to dialogue generation, \citet{multi-view} guide the dialogue model towards better generalization by introducing bidirectional distillation and \citet{negative-distill} propose negative distillation to enhance the diversity of responses.
Rather than pre-training a large teacher, we use collaborative learning and distill knowledge from a group of branches. 

\subsection{Collaborative Learning}
The concept of collaborative learning \citep{Online-distill,ONE,CGL,CL-ILR,OKDDip} is more lightweight in terms of the stages of learning compared to that of conventional knowledge distillation. By doing so, it facilitates finding a robust local minimum for each student, resulting in greater generalization performance. Student networks are currently being implemented in two mainstream settings. One is network-based \citep{DML}, in which students form independent networks and parameter capacity increases linearly with the number of students; the other is branch-based (CL-ILR \citep{CL-ILR} and ONE \citep{ONE}), where the bottom layers of students are shared. \citet{CGL} enable more flexible representation sharing with random routing mechanism, where layers at any level can be shared by different involved students. However, previous work focused only on classification, resulting in the same training objective for all branches of the framework with independent identical distributions (i.i.d.). As a result, different branches will tend to converge to similar feature representations \citep{Convergent,ONE,OKDDip}. Different from them, we propose attrbute-related branch learning strategy and dual knowledge distillation to solve the homogenization problem.