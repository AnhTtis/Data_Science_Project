\section{Related Work}
\label{sec:rw}
The creators of Codex generated a dataset called HumanEval \cite{codex} to evaluate the functional correctness of the code generated by Codex. This dataset comprises 164 hand-written programming problems where each problem is constituted by a function signature, docstring, and unit tests. 
Austin et al. \cite{Austin2021} built two datasets to evaluate LLMs for program synthesis tasks. The first dataset %is called Mostly Basic Programming Problems (MBPP) which
consists of short Python programs with human-readable problem statements and test cases to check for the semantic correctness of the programs. The second dataset
%called MathQA-Python, 
consists of mathematical problems and their corresponding programs that generate the correct answers for the problems.  However, the above datasets are not geared toward evaluating the security of the code generated by LLMs. 
Pearce et al. \cite{PearceA0DK22} created a set of code scenarios to evaluate the code completion capability of GitHub Copilot (that uses Codex). Although, in contrast to the above datasets, this study examined the security rather than functional correctness of code. They created a set of incomplete code scenarios for which Copilot was asked to complete the code and analysed the security of the results. 
%They observed that 40\% of the code completed by Copilot in their study included vulnerabilities. 
The same code scenarios were used as a base in another work \cite{Pearce2022} to explore the ability of LLMs to fix bugs in vulnerable code. However, they focus more on vulnerability repair prompts in the form of incomplete code with comments rather than NL prompts for secure code generation.



