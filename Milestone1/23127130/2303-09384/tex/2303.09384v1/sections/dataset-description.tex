\section{Dataset Description}


In total, the \textit{LLMSecEval} dataset contains \textbf{150 NL prompts} compiled into a CSV as well as JSON file and is characterized as follows: 
\begin{itemize}
 
\item \textbf{\textit{CWE name: }} Name of the weakness 

%\subsubsection*{\textbf{NL Prompt}} 
\item \textbf{\textit{NL Prompt: }}Prompt to generate code covering 18 out of the Top 25 CWE scenarios.

%\subsubsection*{\textbf{Source Code Filepath}} 
\item \textbf{\textit{Source Code Filepath: }}Path of the source code file in the data published by \cite{PearceA0DK22} from which the prompt is generated. 

%\textbf{\textit{Vulnerable: }}
%\subsubsection*{\textbf{Vulnerable}}
\item \textbf{\textit{Vulnerable:}} As reported in \cite{PearceA0DK22}, 85 prompts in our dataset were generated from vulnerable code and it is marked under this field. Although, we have removed any vulnerability specifications from the generated NL prompts (Section \ref{subsec:preprocessing}).

%\textbf{\textit{Language: }} 
%\subsubsection*{\textbf{Language}} 
\item \textbf{\textit{Language: }}Language of the source code from which the prompt is generated. Of 150 prompts, 83 are generated from Python and 67 from C programs. Although we removed any language-specific mentions, we labeled each prompt with their language of origin. 

\item \textbf{\textit{Quality Metrics}}: The prompts are scored based on 4 metrics and their scores are provided in these fields. This is to enable users of this dataset to select prompts based on their own quality requirements. A detailed description of these metrics is presented in Section~\ref{sec:analysis}. 

%\noindent\subsubsection*{\textbf{Secure Code Samples}} 
\item \textbf{\textit{Secure Code Samples:}}
For each prompt in our dataset, we created the corresponding secure implementation in Python. This process was done mostly manually as the majority of the code snippets generated by Copilot in \cite{PearceA0DK22} either contained vulnerabilities or minor design flaws. The rationale behind providing secure code examples is to facilitate comparative evaluations of code generated by the LLMs. The security of these examples was checked using a code analysis tool called CodeQL \cite{codeql}.
\end{itemize}
The full dataset including the secure code examples can be accessed through a \textbf{GitHub public repository}\footnote{https://github.com/tuhh-softsec/LLMSecEval/} and \textbf{DOI}\footnote{https://doi.org/10.5281/zenodo.7565964}.





