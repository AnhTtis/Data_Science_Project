\section{NL Prompts Quality Analysis}
\label{sec:analysis}

We assessed the quality of the prompts included in the LLMSecEval dataset through some metrics available in the current literature. Particularly, we adopted \textit{language-} and \textit{content-related} metrics proposed by Hu et al. \cite{HuCWXLZ22}. On the one hand, language metrics comprise the \textit{naturalness} and \textit{expressiveness} of the NL descriptions. While \textbf{\textit{Naturalness}} measures how fluent the NL prompt is strictly in terms of grammatically-correct full sentences, \textbf{\textit{Expressiveness}} measures its readability and understandability. For instance, a prompt with high naturalness should not contain any grammatical errors while a prompt with high expressiveness should not contain complex or semantically wrong sentences. On the other hand, content-related metrics elaborate on the \textbf{\textit{Adequacy}} and \textbf{\textit{Conciseness}} of the prompt. That is, on its richness and relevancy, respectively. For instance, a prompt with high adequacy should include all the important information available in the code, whereas a highly concise one would omit unnecessary information irrelevant to the code snippet. 

The scores of each metric range from 1 to 5 and were assigned manually by 2 of the authors of this paper. We have followed the same criteria proposed by Hu et al. \cite{HuCWXLZ22} to assign these scores (for more details, please refer \cite{HuCWXLZ22}). To ensure the reliability of this scoring criteria we performed a reliability agreement test. For this, we chose a weighted Cohen's Kappa coefficient \cite{Cohen1973}\cite{McHugh2012} to measure the inter-rater reliability of the scores assigned to all the metrics.
%Weighted Cohen's Kappa was chosen as we needed to consider the degree of disagreement between the authors in our measurements. 
%A kappa value closer to +1 indicates a high degree of agreement between the raters whereas a value closer to -1 indicates a high inter-rater disagreement. 
Such a coefficient ranges from -1 to +1, where values greater than 0.79 indicates strong agreement among raters \cite{McHugh2012}.
We obtained kappa values of 0.98 for naturalness, 0.83 for expressiveness, 0.8 for adequacy, and 0.88 for conciseness. This shows a high degree of agreement among the raters and suggests a strong validity of the selected scoring criteria. Disagreements among raters were resolved through further verbal discussions afterward. The final results of this assessment are shown in Fig.~\ref{fig:analysis}. 
\begin{figure}[hbt!]
    \centering
    \includegraphics[width = 0.9\linewidth]{Figures/analysis.png}
    \caption{Language- and content-related scores (\underline{Note}: Frequencies lower than 2 are not labeled in the graph).} 
    \label{fig:analysis}
\end{figure}


\subsubsection*{\textbf{Language-related Metrics}}
%We used two language-related metrics:  \textit{Naturalness} and \textit{Expressiveness}. Naturalness measures how fluent the NL description is, strictly in terms of grammatically correct full sentences. Expressiveness measures the readability and understandability of the description. 
%The results of the evaluation of the \textit{naturalness} and \textit{expressiveness} of the prompts can be seen in Figure %\ref{fig:language}. 
%\begin{figure}[hbt!]
%    \centering
%    \includegraphics[width = \linewidth]{Figures/language-graph.png}
%    \caption{Language-related score: Naturalness and Expressiveness} 
%    \label{fig:language}
%\end{figure}
Most of the prompts in our dataset contain fluent English sentences describing the code, with only a few including unnecessary white spaces and special characters that were removed during formatting. Hence, all prompts in our dataset got a score of 4 or higher on the \textit{naturalness} metric as shown in Fig.~\ref{fig:analysis}. Regarding \textit{expressiveness} (i.e., how easy are the descriptions to understand), the NL prompts received slightly lower scores. Some prompts were scored low due to the presence of needless function names and code implementation details that could hinder the understanding of the text. Nevertheless, all prompts scored 3 or more with a majority having a score greater than or equal to 4. Overall, these results suggest a high quality of the prompts in our dataset in terms of language fluency.



\subsubsection*{\textbf{Content-related Metrics}}
%The content-related metrics include: \textit{Adequacy} and \textit{Conciseness}. Through the adequacy metric, we measure the information-richness of the description. A description is scored high on this metric if it includes all the important information in the code. And finally, conciseness tries to check if the description contains any unnecessary information that is irrelevant to the code snippet. 
%Figure \ref{fig:content} demonstrates the extent to which the prompts effectively convey the content of the code being described. 
As also depicted in Fig.~\ref{fig:analysis}, 138 out of 150 prompts received a score higher or equal to 3 when it comes to \textit{adequacy}. The remaining prompts that received lower scores of 1 or 2 were found too abstract and did not include all the relevant information from their respective code. In terms of \textit{conciseness}, 135 out of 150 prompts scored 3 or higher, while the rest scored lower due to the inclusion of unnecessary background information on in-built method calls without adding much value. %Overall, the dataset contains a diverse range of NL prompts. 
%\begin{figure}[hbt!]
%    \centering
%    \includegraphics[width = \linewidth]{Figures/content-graph.png}
%    \caption{Content-related score: Adequacy and Conciseness} 
%    \label{fig:content}
%\end{figure}