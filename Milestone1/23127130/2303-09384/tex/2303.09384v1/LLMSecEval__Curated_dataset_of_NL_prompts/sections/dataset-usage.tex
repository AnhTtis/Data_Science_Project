\section{Dataset Usage for Secure Code Generation}

%The main goal of this dataset is to evaluate the extent to which LLMs with code generation capabilities can be used as an interactive tool for generating secure code. To demonstrate this, we have created an example web application that uses our dataset to evaluate the code generated by GPT-3 and Codex. This application can be accessed via our open-source repository.

%The goal of \textit{LLMSecEval} is to facilitate the evaluation and enhancement of security of current (and future) automatic code-generation models that use NL prompts/queries as input.  Moreover, it also paves the road towards ...... 

The main goal of \textit{LLMSecEval} is to facilitate research on the security of current (and future) automatic code-generation models that take NL prompts/queries as input. Particularly, this dataset can be used to produce code for CWE-related scenarios and verify whether such models introduce security vulnerabilities. Furthermore, the prompts included in \textit{LLMSecEval} can support further exploratory studies in the area of \textit{prompt engineering}\cite{ReynoldsM21} for secure code generation.
%, an emerging field within Natural Language Processing (NLP) seeking to generate effective NL program descriptions \cite{ReynoldsM21}. 
For instance, our prompts can serve as a baseline for the design of descriptions leading to secure code implementations.

%This dataset can be used to generate code covering CWE scenarios to verify if code generation models can defend against threats by preventing common security weaknesses in code. It can also be used as a base for exploring prompt engineering  in order to improve the security of the code generated by such models. Prompt engineering is the process of tailoring the inputs to yield desirable outputs. In this case, the desirable output is secure code. There is evidence to show how prompt engineering improves the performance of LLMs
   

%The LLMSecEval dataset can be used to verify if the code generated by such models can defend against threats by preventing common security weaknesses in code. 
As a practical demonstration, we have built an application that uses it to evaluate code generated by two LLMs: GPT-3 and Codex. %\subsubsection*{\textbf{Example Usage}}
For this, we used the API endpoint provided by OpenAI to access the GPT-3 and Codex models. Through the web interface of our application, users can upload the NL prompts as input. They can also select between GPT-3 and Codex to generate code, as well as the programming language in which the code should be expressed. After supplying the necessary input and options, the application produces a file containing the code generated for each prompt in \textit{LLMSecEval}, which can be downloaded afterward. 
As mentioned in Section~\ref{sec:introduction}, our tool uses CodeQL \cite{codeql} to evaluate the security of the generated code. CodeQL is an automated code analysis engine that can be leveraged to spot vulnerabilities through queries written in QL, a declarative query language. We used built-in QL queries to detect 18 of the Top 25 CWEs in code created using \textit{LLMSecEval}. Our application can be used to run these queries and store their results locally for further analysis.

%Apart from GPT-3 and Codex, our dataset can be used to evaluate other existing LLMs trained on open-source code data such as CodeRL, Polycoder and so on. 