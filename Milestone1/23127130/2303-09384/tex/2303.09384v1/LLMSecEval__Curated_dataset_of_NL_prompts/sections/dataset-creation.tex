\section{Creation of NL Prompts}
\label{sec:creation}

An NL prompt -in this context- is a query or description written in natural language about some code functionality. Every year, MITRE \cite{mitre} releases a list of the top 25 most dangerous CWEs, giving account for common and impactful software weaknesses. We used this list as a reference for generating NL prompts that can lead to code prone to security flaws. For example, an NL prompt leading to a potential \textit{improper input validation} (CWE-20) could be \textit{``Write a python code to create a registration page that takes in user details and stores them in a database''}. Instead of writing such prompts ourselves manually, we used Codex to generate NL descriptions from CWE-related code snippets. Our prompt creation approach is summarized in Figure \ref{fig:creation} and explained in the following subsections. 


%To evaluate the security of code generated by different LLMs we need a set of NL prompts describing code that covers various security-relevant scenarios. Such NL prompts can then be used as input to the LLMs to generate the corresponding code. 

%---An NL prompt -in this context- is a query or description written in natural language about some code functionality. MITRE \cite{mitre} annually releases a list of the top 25 most dangerous CWEs to demonstrate the most common and impactful software weaknesses of the year. We used this as a reference to generate NL prompts that can lead to code susceptible to security flaws. For example, let us consider an NL prompt like \textit{Write a python code to create a registration page that takes in user details and stores them in a database.} A naive implementation of the above NL prompt example could lead to CWE-20 (Improper Input Validation), if the code does not validate the user input.
%Writing such prompts from scratch manually by ourselves, involved the risk of introducing human biases into the prompts.
%One way to do this is to develop NL prompts from scratch covering the most common security weakness scenarios identified by MITRE. However, in this approach, we face the risk of introducing human biases while generating the NL prompts. 
%To avoid this, we employed an alternative approach where we use an existing dataset of security-relevant code covering such weaknesses and translate them into NL descriptions. 
%---Rather than writing such prompts covering different CWEs manually from scratch  by ourselves, we used an existing dataset of code snippets that contain functional scenarios covering such weaknesses and translated them into NL descriptions. Our prompt creation approach is summarized in Figure \ref{fig:creation} and explained below. 


\begin{figure}[hbt!]
    \centering
    \includegraphics[width = 0.83\linewidth]{Figures/creation-new.png}
    \caption{NL prompts creation process} 
    \label{fig:creation}
\end{figure}

\subsection{Data Source}
\label{subsec:source}
As mentioned in Section \ref{sec:rw}, Pearce et al. \cite{PearceA0DK22} generated a dataset of 54 code scenarios that cover 18 of the Top 25 CWEs released in 2021 (3 scenarios per CWE). 7 CWEs from the list were excluded as these represented more architectural issues rather than code-level problems. Each scenario consisted of incomplete code snippets, some of which included NL comments. Such snippets were then fed to GitHub Copilot for their completion. For each scenario, Copilot generated 25 samples of completed code, ranked based on a confidence score. In total, Copilot produced 1084 valid programs: 513 C programs and 571 Python programs. 

We used the C/Python snippets available in the dataset of Pearce et al. \cite{PearceA0DK22}, but instead of taking the top 25 samples generated by Copilot, we selected the top 3 \textit{functional samples} for each scenario. This selection was done to ensure the quality of the prompts generated from such samples regarding their functional correctness. For this, we started checking and selecting each sample from best- to worst-ranked until we had 3 correct instances. The resulting corpus of 162 programs set our base for the generation of NL prompts. As 40\% of the original program set (1084 instances) contained security vulnerabilities \cite{PearceA0DK22}, the top 3 samples selected by us are also likely to have vulnerabilities. Nonetheless, we have taken measures to remove the influence of these vulnerabilities in the resulting prompts, which are explained in Section \ref{subsec:preprocessing}. 

\subsection{NL Prompts using Codex}
\label{subsec:prompts}
The next step was to translate the programs into textual descriptions for creating a set of NL prompts covering relevant security scenarios. 
%In order to ensure the quality of these NL descriptions, we selected the top 3 functional and valid program samples for each scenario rather than using all the top 25 samples generated by Copilot. 
To translate the programs into NL descriptions, we used OpenAI's Codex \cite{codex} model.
Codex is a descendant of OpenAI's GPT-3 and it is fine-tuned on 54 million GitHub code repositories. %Codex has demonstrated its abilities for code generation and code explanation \cite{codex}. 
%For our purpose, Codex was accessed via its API through an application developed by us that could automatically queue and send requests and can handle the responses. %We accessed Codex via its API through a closed beta access that is available for research purposes. 
We chose the \texttt{code-davinci-002} model from Codex for code-to-natural language translation as this is recommended by OpenAI as the most capable model that can understand code\footnote{https://beta.openai.com/playground}. There is a provision to decide the maximum length of the output in Codex. Test runs with higher values for length resulted in repeated and invalid results. Hence we restricted the maximum number of tokens in the NL description to 100. 

%An example of a sample code from the Pearce et al. and their corresponding NL description generated by Codex is shown in Figure \ref{fig:translation}.



\subsection{Manual Curation of Responses}
\label{subsec:preprocessing}
Overall, Codex produced NL descriptions for 162 programs. Since Codex was in beta-phase at the time we conducted this research, it was important to verify if such descriptions were fit or not. %Therefore, we proceeded as follows:
For this, two of the authors manually curated the generated descriptions as follows :
\begin{enumerate}
    
%\subsubsection*{\textbf{Exclusion Criteria}} 
\item \textbf{\textit{Inclusion/Exclusion Criteria:}} To filter out invalid descriptions, we removed responses that (i) were empty or only contained white space characters,
(ii) included a large number of code snippets, either from the input program or additions by Codex, and
(iii) do not explain the functionality of the input code.
This resulted in 150 valid NL prompts.

\item \textbf{\textit{NL Descriptions Formatting:}} The valid descriptions were then polished by removing 
(i) repetitive phrases from the responses,
(ii) first-person references in the descriptions,
(iii) trailing whitespace characters and other unnecessary special characters from the responses,
(iv) incomplete sentences at the end of the responses, 
%(due to size restrictions, the last sentence of some responses were cut-off in the middle),
(v) warnings in responses that include information regarding the vulnerabilities present in the input code,
(vi) bullet points, and finally (vii) language/platform-specific terms. The language/platform-specific terms were replaced with more neutral terms to make the prompts programming language-agnostic. For example, the term \textit{printf} from C language was replaced by the term \textit{print}. %The list of replaced terms is available in our repository to facilitate dataset extension. 



%\begin{enumerate}
%    \item Removed responses that were empty or contained only different types of whitespace characters.
%    \item Removed repetitive phrases from the responses.
%    \item Responses that included a large amount of code snippets, either from the input program or additions by Codex were removed.
%    \item Removed first person references in the descriptions.
%    \item Responses that did not fully explain the functionality of the input code were removed.
%    \item Responses that were in the form of bullet points were edited to form full sentences.
%    \item Trailing whitespace characters and other unnecessary special characters were removed from the responses.
%    \item Meaningful responses where the last sentence is cut-off  in the middle due to size restrictions were edited to remove the incomplete sentence from the response. 
%    \item Some responses included warning information regarding the vulnerabilities present in the input code. Such warnings were removed from the responses.
%\end{enumerate}

%Additionally, we also replaced programming language or platform-specific terms with neutral terms to make the prompts language-agnostic. These replacements are listed in the dataset repository. 
\item \textbf{\textit{Generation of NL prompts}}: We transformed the formatted NL descriptions into prompts suitable for LLMs. 
%We differentiate prompts from descriptions in the sense that prompts can be interpreted by LLMs, whereas descriptions not. 
To convert descriptions into prompts we simply added the header ``\textit{Generate \textless language\textgreater~code for the following:}'' to them.  Fig.~\ref{fig:translation} illustrates the generation of an NL prompt from a code snippet corresponding to a CWE-20 scenario (i.e., \textit{Improper Input Validation}). As can be observed, the code contains a vulnerability as it does not properly validate/sanitize the user's input. 
\end{enumerate}

%The final dataset contains \textbf{150 NL prompts}.
%An example to demonstrate the generation of NL prompts from code is shown in Figure \ref{fig:translation}. It shows the NL description and the final prompt created for a code generated by Copilot in \cite{PearceA0DK22} for CWE-20 (Improper Input Validation) scenario. The code in the figure contains vulnerability as it does not validate/sanitize the input from the user. 

\begin{figure}[hbt!]
    \centering
    \includegraphics[width = 0.99\linewidth]{Figures/translation.png}
    \caption{An example of NL prompt generated from a Python code snippet covering CWE-20 scenario in the Pearce et al. \cite{PearceA0DK22} dataset.} 
    \label{fig:translation}
\end{figure}



