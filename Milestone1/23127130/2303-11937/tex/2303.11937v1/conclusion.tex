\vspace{-1mm}
\section{CONCLUSION}\vspace{-1mm}
We derived the first high probability analysis of the existing methods for stochastic Continuous Submodular Function (CSF) maximization, namely PGA, {boosted PGA}, SCG, and SCG++.
When assumptions on the stochasticity of gradients are strong enough, we showed that even in the worst case the solutions of the algorithms are lower bounded by a function converging to their expected guarantees.
Specifically, with $K$ stochastic gradient computations, we
demonstrated that PGA converges with rate of $O(\frac{1}{\sqrt{K}})$ to the expected $OPT/2$ {and the boosted version at the same rate to $(1-1/e)OPT$.} 
For SCG and SCG++, we showed 
that both algorithms converge at rates of at least $O(\frac{\delta}{K^{\frac{1}{3}}})$ and $O(\frac{\delta}{K^{\frac{1}{2}}})$ to the expected $(1-1/e)OPT$, where $\delta$ depends on the confidence threshold. Besides, under the sub-Gaussian assumption on the gradient noise, we provided an improved lower bound of $O(\frac{1}{\sqrt{K}})$ for the convergence of SCG, that is faster than the existing convergence rate to the expected solution. 
Our results allows characterizing worst
and best-case performance of
CSF maximization in stochastic settings, and 
mitigating the risk of getting a bad solution.
