\section{HIGH-PROBABILITY BOUNDS FOR STOCHASTIC CONTINUOUS SUBMODULAR MAXIMIZATION}
Next, we discuss our high-probability bounds for stochastic CSF %continuous submodular 
maximization algorithms, namely Projected Gradient Ascent (PGA), {boosted PGA,} Stochastic Continuous Greedy (SCG), and Continuous Greedy++ (SCG++).

\subsection{Projected Gradient Ascent}
We start by analyzing the worst-case performance of the PGA method %proposed Hassani et al. in
which achieves a $[OPT/2-\epsilon]$ approximation in expectation, in $\mathcal{O}(1/\epsilon^2)$ iteration
\cite{Hassani2017gradient}.
PGA starts from an initial estimate $\x_0\in\mathcal{C}$. Then at every iteration $t$, it takes a step in the direction of the noisy gradient $\g_t = \nabla \tilde{F}(\x_t,\z_t)$, and projects the solution
%does an Euclidean projection back 
onto the convex set $\mathcal{C}$. The update rule at step $t$ takes the following form:
\begin{equation}
    \x_{t+1}=\mathcal{P}_\C(\x_t + \mu_t \g_t),
\end{equation}
where, $\mu_t$ is the diminishing learning rate at step $t$, and $\mathcal{P}_\C$ denotes the Euclidean projection onto the set $\C$.
The pseudocode is provided in Appendix \ref{apx:alg}. %\cref{alg:pga}.

% Hassani et al. \cite{Hassani2017gradient} provided a lower bound on the expected function value $\mathbb{E}[F(x_{\tau})]$ of a randomly sampled step $\tau \sim \text{Unif}\{1,\dots, T\}$.
%%%
% In order to achieve a lower bound on the expected function value of PGA, %the returned iterate, $\mathbb{E}[F(\x_{\tau})]$,
Hassani et al. \cite{Hassani2017gradient}
provided a lower bound on the expected function value, $\E[F(\x_{\tau})]$, at
%designed their algorithm to 
a time-step $\tau$, %\sim Unif
sampled uniformly at random from
$\{1,\dots, T\}$. %and return the value of the corresponding iterate $F(\x_{\tau})$. 
Important to note, however, is that the derived expectation is not only over this random variable $\tau$, but \emph{also the noise} coming from the gradient estimates. This
implies that %distinction means that without further work to prove otherwise, it could be
it is possible that the optimizer gets extremely unlucky with its gradient estimates, in which case no $\x_t$ %for that run 
satisfies the lower bound (for example, consider the unlikely but still possible scenario where $\forall t, \: \g_t=0$). In Theorem \ref{thm:pga_bound}, we provide an exact answer for how unlikely a failure event like this would be.\looseness=-1

% \ba{need?} For a stochastic continuous DR-submodular function $F$ that is monotone and $L$-smooth with $\E[\norm{\g_t-\E[\g_t]}^2]\leq\sigma^2$, PGA with learning rate $\mu_t = \frac{1}{L+ \frac{\sigma}{R} \sqrt{t}}$ obtains the following expected bound after $T$ iteration
% \cite{Hassani2017gradient}: \ba{R}
% \begin{equation}
%     \E[F(\x_{\tau})] \geq \frac{OPT}{2} - (\frac{R^2 L + OPT}{2T} + \frac{R_{\sigma}}{\sqrt{T}}),
% \end{equation}
% where random variable $\tau$ taking values within $\{1,2,\cdots,T\}$ with equal probability.


% Let $F:\mathcal{X}\rightarrow \mathbb{R}_+$ be the monotone diminishing returns (DR) and  submodular function that we are trying to maximize over. Let $OPT \triangleq \max_{x\in \mathcal{C}} F(x)$ be the optimal value of this function over some feasible convex set $\mathcal{C} \subseteq \mathbb{R}_+$. Additionally 
To do so, we make similar assumptions to \cite{Hassani2017gradient}: 
%in order to arrive at \cref{thm:pga}:

\begin{assumption}\label{ass:x_bound}
\label{as:bounded}
% For any $\x, \y\in \mathcal{C}$, 
The diameter of the constraint set $\C$ is bounded by $D$. I.e., $\forall \x, \y \in \C$, we have 
\begin{equation*}
    \norm{\x-\y}\leq D.
\end{equation*}
\end{assumption}
%
\begin{assumption}\label{ass:g_lip}
\label{as:smooth}
% The function $F$ is DR-submodular and monotone. Further, its gradients are $L$-Lipschitz continuous over the set $\X$ , i.e.
The function $F$ is Lipschitz smooth with constant $L$, over $\X$. I.e., $\forall \x, \y \in \C$, we have
\begin{equation*}
    \norm{\nabla F(\x) -\nabla F(\y)} \leq L\norm{\x-\y}.
\end{equation*}
\end{assumption}
%
\begin{assumption}\label{ass:g_bound}
% \label{as:boundedgrad}
Stochastic gradients $\g_t\!=\!\nabla \Fn(\x,\z)$ are bounded in distance from their mean $\nabla {F}(\x_t)=\E[\g_t]$:%\vspace{-1mm}
\begin{equation*}
    %\norm{g_t-\nabla F(x)}\leq M
    \norm{\nabla {F}(x_t)-\g_t}\leq M.%\vspace{-2mm}
\end{equation*}
\end{assumption}
% \ev{added the constants to the first theorem to demonstrate where $M$ comes into play}
% Note that Assumption \eqref{ass:g_bound} is stronger than simply bounded variance and is necessary to apply the Azuma-Hoeffding inequality. However if $g_t = \nabla F(x_t) + z_t$ with each $z_t$ being zero mean and sub-gaussian a similar version of \cref{thm:pga_bound} can be derived.
%
The following theorem shows that for any fixed confidence interval $p$, the lower bound on $\sum_{t\in[T]}F(\x_t)/T$ will converge to $OPT/2$ at a rate of $\mathcal{O}(\sqrt{\frac{\log(1/1-p)}{T}})$.
\begin{theorem}\label{thm:pga_bound}
    Consider running PGA for $T$ iterations with step size of $\eta_t=\frac{2}{\sqrt{t}}$ with Assumptions \ref{ass:x_bound},\ref{ass:g_lip},\ref{ass:g_bound} satisfied. Then with probability $p\!\geq\! 1\!-\!\delta$, where $\delta \!\in\! [0,1]$, the average function value returned by the algorithm is lower bounded by\looseness=-1
    \begin{align}
        \frac{1}{T}\sum_{t=1}^T F(\x_t) &\geq \frac{1}{2}OPT  
         -\frac{C}{\sqrt{T}}
         - DM\sqrt{\frac{\log(1/\delta)}{2T}} \\ \nonumber
        &\geq \frac{1}{2}OPT -  \mathcal{O}\left(\sqrt{\frac{\log(1/\delta)}{T}}\right),
    \end{align}
    where we denote the constant $C:= \left(\frac{8(L+M)^2 +D^2}{8}\right)$.
\end{theorem}
% This bound tells us that for any fixed confidence interval $p$, we know that our lower bound will converge to the half optimal value at a rate of $\mathcal{O}(\sqrt{\frac{\log(2/1-p)}{T}})$. 
Unlike the expectation bound provided in \cite{Hassani2017gradient}, Theorem \ref{thm:pga_bound} assures that with high probability, \emph{at least one iterate} from a \emph{single} algorithm run will be larger than the lower bound. Therefore, %using $\mathcal{O}(T)$ evaluations of $F(\x_t)$, 
one could modify the default PGA algorithm to return the best iterate, %$\x_{t^*}$
$\max_{t\in[T]}F(\x_t)$, 
%(note this is not the same as the true optimal $\x^*$) 
which is guaranteed to also be lower bounded with high probability by $OPT/2$.
We note that even in the case where the true function evaluation is hard to compute, one can still find the best iterate with high probability given unbiased stochastic function evaluations $\tilde{F}(\x,\z)$. Concretely, given the ordering of iterates from highest to lowest function value $\x_{[1]},\dots,\x_{[T]}$, consider the difference between the best two solutions $d:=F(\x_{[1]})-F(\x_{[2]})$. One can use a Hoeffding bound to determine the relatively small number of samples, $m$, needed to calculate $\bar{F}(\x):=\frac{1}{m}\sum \tilde{F}(\x)$ for each iterate, such that
$\bar{F}(\x_{[1]})\!>\! F(\x_{[1]})-d/2$ and $\bar{F}(\x_{[k]})\!<\! F(\x_{[2]})+d/2$ for all $k>1$ occurs with very high probability. 
Alternatively, since $F(\x)\leq OPT\:\: \forall \x$, %we find 
at least %$\alpha := 
$\frac{r}{r + (1/2)OPT}$ fraction of solutions are greater than $\sum_{t\in[T]}F(\x_t)/T-r$ for a slack variable $r$.
% the minimum fraction $\alpha$ of `good' solutions  that are greater than $F_{avg}-r$ for some slack variable $r$, is $\alpha := \frac{r}{r + (1/2)OPT}$. 
That is, with only $k$ true function evaluations, %we know 
at least one good solution is found with probabiliy $p>1-(1-r)^k$. \looseness=-1 %\ba{Appx}

%The general proof outline for
Theorem \ref{thm:pga_bound} %starts by following a similar pattern to \cite{Hassani2017gradient}. 
% starts by using similar assumptions to \cite{Hassani2017gradient}
relies on diminishing returns and smoothness of $F$, along with the bound on $\C$ to 
first bound the difference between $F(\x_t)$ and $F(\x^*)$
based on 
%%%%%%%%%%%
the inner product between gradient noise and $\x_t-\x^*$.
%%%%%%%%%%%
However, instead of taking the expectation of this inequality, 
it directly shows that with Assumption \ref{ass:g_bound}, these random products satisfy the conditions of a c-lipschitz Martingale difference sequence.
% it %can be immediately 
% applies it recursively to get a summation of 
% random variables which are functions of the gradient noise. Given %our bound on the noise in
% Assumption \ref{ass:g_bound}, we can show this summation is a well-behaved Martingale sequence,
This allows using standard high probability bounds (Azuma-Hoeffding). 
See Appendix \ref{sec:pga_proof} for the full proof.

% Specifically, while \cite{Hassani2017gradient} uses Young’s inequality to upper bound the inner product between gradient noise and the difference $\x - \x^∗$, we directly show that these random products satisfy the conditions of a c-lipschitz Martingale difference sequence.

We note that Assumption \ref{ass:g_bound} is stronger than simply bounded variance and is necessary to apply the Azuma-Hoeffding inequality. However if $\g_t = \nabla F(\x_t) + \z_t$ with each $\z_t$ being zero mean and Sub-Gaussian, a similar version of Theorem \ref{thm:pga_bound} 
%might have a possible derivation,
can be derived following  \cite{harvey2019simple}. %\ba{is this in appendix?}\ev{not currently, using a more powerful martingale bound from harvey et al 2019 should work but is much more involved}

% All the proofs can be found in the Appendix.



\begin{corollary}\label{cor:pga_bound}
    Consider the case where we set $\delta = \exp(-\sqrt{T})$.
    Then the averaged function value of PGA is lower bounded with probability $p\geq 1-2\exp(-\sqrt{T})$ by:
    \begin{equation}
        \frac{1}{T}\sum_{t=1}^T F(\x_t) \geq \frac{1}{2}OPT -  \mathcal{O}\left(\frac{1}{T^{1/4}}\right).%\vspace{-3mm}
    \end{equation}
\end{corollary}
We see that as $T\rightarrow \infty$ we have both $p\rightarrow 1$ and $\!\mathcal{O}(1/T^{1/4})\!\rightarrow 0$. Thus, our lower-bound is tight.
% meaning that 
% we are certain about a tight lower bound.
{
\subsection{Boosted Projected Gradient Ascent}
% While %the projected gradient ascent algorithm in 
% PGA \cite{Hassani2017gradient} achieves a $[OPT/2-\epsilon]$ approximation in expectation, 
Very recently, boosted PGA %is proposed 
\cite{zhang2022stochastic} %which 
is proposed to provide %convergence in expectation to
$[(1-\frac{1}{e}-\epsilon^2)OPT]$ approximation in expectation, in $\mathcal{O}(1/\epsilon^2)$ iterations.
% The idea is to apply PGA to an auxiliary function 
The idea is to find an auxiliary (non-oblivious) function that can provide a better approximation guarantee than the original DR-submodular function. 
Then the stochastic gradients of the non-oblivious function $F'$ (instead of the stochastic gradient of the original DR-submodular function $F$) are leveraged by PGA.
% f itself
% gradients of a carefully chosen auxiliary (non-oblivious) function. %The authors use 
Specifically, \cite{zhang2022stochastic} used 
the following non-oblivious function $F'$ and its gradient $\nabla F'$: \looseness=-1
% \begin{align}
%     F'(\x) := \int_0^1 \frac{e^{\gamma(z-1)}}{z}F(z*\x) dz \\
%     \nabla F'(\x) := \int_0^1 e^{\gamma(z-1)}F(z*\x) dz
% \end{align}
\begin{align}
    F'(\x) := \int_0^1 \frac{e^{(s-1)}}{s}F(s*\x) ds, \\
    \nabla F'(\x) := \int_0^1 e^{(s-1)}\nabla F(s*\x) ds.
\end{align}
This has the nice property that $\langle \y-\x, \nabla F'(\x)\rangle \geq (1-1/e)F(\y)-F(\x)$, which guarantees %stationary points at least 
$(1-1/e)OPT$ approximation. Additionally, when the original function is Lipschitz smooth (Assumption \ref{as:smooth}), $F'(\x)$ is %shown to be 
Lipschitz smooth with constant $L' = L(1+1/e)$. To efficiently approximate $\nabla F'$ given a noisy gradient estimate $\nabla \tilde{F'}$, \cite{zhang2017empirical} uses the following estimator:
% \begin{align}\label{eq:nonobv-noisy}
%     \nabla \tilde{F}'(\x_t) := \frac{1-e^{-\gamma}}{\gamma}\nabla \tilde{F}(z_t*\x_t).
% \end{align}
% Here $z_t$ is independently sampled from a distribution with cdf: $\mathbbm{P}(Z\leq z) = \int_0^z \frac{\gamma^{\gamma(u-1)}}{1-e^{-\gamma}} \mathbbm{1}(u\in[0,1])$, where $\mathbbm{1}$ is the indicator function.\\
\begin{align}\label{eq:nonobv-noisy}
    \nabla \tilde{F}'(\x_t) := (1-\frac{1}{e})\nabla \tilde{F'}(s_t*\x_t).
\end{align}
Here, $s_t$ is independently sampled from a distribution %with cdf: 
$\mathbb{P}(\pmb{S}\!\!\leq\!\! s) \!=\!\! \int_0^s \frac{1}{1-e^{-1}} \mathds{1}(u\!\in\![0,1])du$, where $\mathds{1}$ is indicator function.\looseness=-1

% As in the case of standard PGA, we provide 
The following theorem provides a lower bound on $\sum_{t\in[T]}F(\x_t)/T$, with high probability.
\begin{theorem}\label{thm:nonobv_pga_bound}
  Consider running boosted PGA for $T$ iterations with step size of $\eta_t=\frac{2}{\sqrt{t}}$ with Assumptions \ref{ass:x_bound},\ref{ass:g_lip},\ref{ass:g_bound} satisfied. Then with probability $p\!\geq\! 1\!-\!\delta$, where $\delta \!\in\! [0,1]$, the average function value returned by the algorithm is lower bounded by\looseness=-1
    \begin{align}
        \frac{1}{T}\sum_{t=1}^T F(\x_t) &\geq (1-\frac{1}{e})OPT  
         -\frac{C'}{\sqrt{T}}
         - DM'\sqrt{\frac{\log(1/\delta)}{2T}} \nonumber\\ 
        &\geq (1-\frac{1}{e})OPT -  \mathcal{O}\!\left(\!\sqrt{\frac{\log(1/\delta)}{T}}\right),
    \end{align}
    where we denote the constant $C':= \left(\frac{8(L' D+M')^2 +D^2}{8}\right)$, and constant $M':= (M+2 LD)\left(1-\frac{1}{e}\right)$. 
\end{theorem}
Note that this bound has the same rate of convergence as \Cref{thm:pga_bound} up to a constant factor. This similarity also means a result parallel to \Cref{cor:pga_bound} can be derived, demonstrating that this algorithm will also converge with $p\rightarrow 1$ as $T\rightarrow \infty$.  
The proof of \Cref{thm:nonobv_pga_bound} follows a similar structure to \Cref{thm:pga_bound}. We bound the sum of differences $F(\x_t)-F(\x^*)$ by a Martingale sequence with a bounded difference property. The key distinction in the non-oblivious case is that we must bound gradients from $\nabla F'(\x)$ as well as $\nabla \tilde{F}'(\x)$.  We defer the full proof to Appendix \ref{apx:nonobv-proof}.\looseness=-1

\textbf{Guarantees for weakly submodular functions.} We note that Theorems \ref{thm:pga_bound}, \ref{thm:nonobv_pga_bound} for PGA and boosted PGA can be extended to $\gamma$-weakly DR-submodular functions, where we have 
% (i.e. we are allowed to slightly violate Eq. \eqref{eq:dr-hessian} up to an amount defined by 
$\gamma = \inf_{\x \leq \y}\inf_{i}([\nabla F(\x)]_i/[\nabla F(\y)]_i)$). This setting produces bounds with the same rate of convergence up to a constant, to $(\frac{\gamma^2}{1+\gamma^2})OPT$ and $(1\!-\!e^{-\gamma})OPT$ for PGA and Boosted PGA, respectively.
Note that $\gamma\!=\!1$ indicates a differentiable and monotone DR-submodular function.
\looseness=-1
}

\subsection{Stochastic Continuous Greedy}
Next, we analyze the worst-case performance of the Stochastic Continuous Greedy (SCG) algorithm. SCG uses a momentum term to reduce the noise of gradient approximations.
% , and achieves a tight $[(1-1/e)OPT-\epsilon]$ guarantee (in expectation) with $\mathcal{O}(1/\epsilon^3)$ stochastic gradient computations \cite{mokhtari2018conditional}. %for the last iterate. 
% Just like the original bound for PGA, however, this approximation guarantee for SCG does not tell us how frequently "bad" solutions (whose $F(\x_T)$ values lie below the bound) are returned.
%
%is a first order Frank-Wolfe method that uses a momentum term to more accurately estimate the true gradients. This estimator avoids the failure case of the vanilla Frank-Wolfe method shown in \cite{Hassani2017gradient}.
It starts from $\x_0=\pmb{0}$, and at every iteration $t$, 
%the estimated gradient is calculated as:
% it calculates:
calculates:
\begin{equation}
    \gb_{t+1}=(1-\rho_t)\gb_t+\rho_t \nabla \Fn(\x_t,\z_t),
\end{equation}
where $\rho_t$ is a stepsize which approaches zero as $t$ approaches infinity, and $\gb_0=0$. 
The SCG is then ascent in the %following 
direction: %at every iterate $t$:
\begin{equation}
    \vb_t \gets \arg\max_{\vb \in \C}\{\left<\gb_{t}^{T}, \vb\right>\}, %\quad \x_{t+1}=\x_t+\frac{1}{T}\vb_t
\end{equation}
using the following updates rule with step-size ${1}/{T}$:%\vspace{-2mm}
\begin{equation}
    % \vb_t \gets \arg\max_{\vb \in \C}\{\left<\gb_{t}^{T}, \vb\right>\}, \quad
    \x_{t+1}=\x_t+\frac{1}{T}\vb_t.%\vspace{-1mm}
\end{equation}
The stepsize $\frac{1}{T}$ and the initialization $\x_0 = \pmb{0}$ ensure that after $T$ iterations the variable $\x_T$ ends up in the convex set $\C$. 
The pseudocode is provided in Appendix \ref{apx:alg}.

SCG provides a tight $[(1-1/e)OPT-\epsilon]$ guarantee in expectation for the last iterate $T$, with $\mathcal{O}(1/\epsilon^3)$ stochastic gradient computations \cite{mokhtari2018conditional}. 
But, %Just like the original bound for PGA, , 
similar to %the expected bound provided by
PGA \cite{Hassani2017gradient},
the expected %approximation
guarantee of SCG does not tell us how frequently \textit{bad solutions}, with $F(\x_T)\!<\![(1-1/e)OPT-\epsilon]$ %values lie below the bound) 
are returned.

Here, we answer the above question by providing a high-probability bound on the value of the final solution, $F(\x_T)$, returned by SCG. 
To do so, instead of %directly bounding the noisy gradient error (
assuming bounded gradient error
(Assumption \ref{ass:g_bound}), we use the weaker assumption from \cite{mokhtari2018conditional} %that 
on the variance of the stochastic gradients: %is bounded:
\begin{assumption}\label{ass:g_var}
Stochastic gradients have mean $\E[\g_t]= \nabla {F}(\x_t)$ and bounded variance:
\begin{equation*}
    \E_{z\sim p}\left[\norm{\g_t-\nabla F(\x)}^2\right] \leq \sigma^2. %\vspace{-2mm}
\end{equation*}
\end{assumption}
% \begin{lemma}{(Mokhtari et al. 2018)}
% Consider the momentum parameter $\rho_t = \frac{4}{(t+8)^{2/3}}$ set constant, then under our assumptions 1,2,4 we have for all $t=0,\dots, T$
%         \begin{equation}
%             \mathbb{E}[\norm{\nabla F(x_t)-d_t}^2] \leq \frac{Q}{(t+8)^{2/3}}
%         \end{equation}
%         Where $Q:= \max\{\norm{\nabla F(x_0)-d_0}^29^{2/3}, 16\sigma^2 + 3L^2D^2\}$
% \end{lemma}
Given Assumption 4, %Mokhtari et al. 
\cite{mokhtari2018conditional} showed that the variance of the momentum error, i.e., $\E[\norm{F(\x_t) - \gb_t}^2]$, converges to zero, %at a sublinear rate, 
as $t$ grows. 
{%This bound, 
However, this cannot be directly used to provide a high-probability bound on the value of the final solution (as we require the summation rather than the expectation of error terms).}
To address this, instead of using the bound on the variance of the noisy gradient at step $t$, 
we apply Chebyshev's inequality to bound the probability of the noisy gradient to be far away from its expectation. Then, we %recursively bound the distance to optimum and 
use a union bound on iterations 
$t\!\in\![T\!-\!1]$
% $t\in\{0,\cdots,T-1\}$ 
to %arrive at our
get the
next Lemma: \looseness=-1
\begin{lemma}\label{lemma:scg}
    Consider the Stochastic Continuous Greedy algorithm, with
%       Consider the momentum parameter 
    $\rho_t = \frac{4}{(t+8)^{2/3}}$. 
    Under Assumptions \ref{ass:x_bound}, \ref{ass:g_lip}, \ref{ass:g_var}, 
    we have the following high probability bound on the total variance of the noisy gradients during $t\in\{0,\cdots,T-1\}$:
       \begin{equation}
           \mathbb{P}\left(\sum_{t=0}^{T-1}\norm{\nabla F(\x_t) - \gb_t}^2 \leq \delta^2 \sum_{t=0}^{T-1}\frac{Q}{(t+9)^{2/3}}\right) \geq 1-\frac{T}{\delta^2},
       \end{equation}
    where $\!Q\!\!:=\!\!\max\! \left\{\!\norm{\nabla\! F(\x_0) \!-\! \gb_0}^2 \!9^{2/3}\!,\! 16\sigma^2\!\!+\!3L^2D^2\!\right\}$, \!$\delta\!\!>\!0$. 
\end{lemma}
% From here we follow a derivation similar to the original SCG analysis in order to arrive at \cref{thm:scg}
The proof can be found in \Cref{apx:thm2}. Note that Lemma \ref{lemma:scg} does not rely on taking the expectation of the function values or gradient approximations. Equipped with Lemma \ref{lemma:scg}, we derive the following lower-bound on $F(\x_T)$, by recursively bounding the distance of the iterates to the optimum, $F(\x^*)$.
\begin{theorem}\label{thm:scg_bound}
    Consider the Stochastic Continuous Greedy algorithm, with $\rho_t = \frac{4}{(t+8)^{2/3}}$. 
    Under Assumptions \ref{ass:x_bound}, \ref{ass:g_lip}, \ref{ass:g_var}, we have that with probability greater than $1-\frac{T}{\delta^2}$:
    \begin{align}
    F(\x_T) &\geq (1-\frac{1}{e})F(\x^*) - \delta\frac{2Q^{1/2}D}{T^{1/3}} -\frac{LD^2}{2T^2} \nonumber\\
    &= (1-\frac{1}{e})OPT - \mathcal{O}(\frac{\delta}{T^{1/3}}),
    \end{align}
    where $\!Q\!\!:=\!\!\max\! \left\{\!\norm{\nabla\! F(\x_0) \!-\! \gb_0}^2 \!9^{2/3}\!,\! 16\sigma^2\!\!+\!3L^2D^2\!\right\}$, \!$\delta\!\!>\!0$. 
\end{theorem}
The proof can be found in Appendix \ref{apx:thm2}. Unlike our Theorem \ref{thm:pga_bound} for PGA, we see that for any fixed confidence threshold $p$ implying $\delta = \sqrt{\frac{T}{1-p}}$, our lower bound does not converge. This is a direct result of the weakening of the noise assumption, since the gradient noise may have bounded variance but not be bounded itself. Next, we provide an improved bound when gradient noise is sub-Gaussian.
%in the next Section (\ref{sec:improve_bounds}), 
 
% \ba{we have sub-gaussian noise in experiments?}\ev{yes we add Gaussian noise, which is also sub-Gaussian}


%%%%%%% IMPROVEMENT OF BOUNDS %%%%%%%
\subsubsection{Improved Bound under Sub-Gaussian Noise}\label{sec:improve_bounds}
% While for PGA our concentration inequality demonstrate convergence to some fraction of optimal with high probability, the bound derived for SCG fails to do so. 
The weak assumptions on the noisy gradients (only bounded variance from Assumption \ref{ass:g_var}) make it difficult to apply typical Martingale or sub-Gaussian inequalities. If instead, we assume that the noisy gradient approximations are sub-Gaussian, we arrive at a surprisingly tight lower bound. First we describe the sub-Gaussian noise assumption as follows:
\begin{assumption}\label{ass:subG}
Stochastic gradients have mean $\E[\g_t]= \nabla {F}(\x_t)$ and $\hat{\z}_t:=\norm{\g_t\!-\!\nabla F(\x)}$ is sub-Gaussian. I.e. for $\sigma>0$, we have: 
\begin{equation}
    \mathbb{E}(e^{\lambda\hat{\z}_t^2})\leq e^{\sigma^2\lambda^2/2} \quad \forall\lambda \in \R %\vspace{-2mm}
\end{equation}
\end{assumption}
When using the SCG algorithm under this new assumption, we can derive the following high probability bound:

\begin{theorem}\label{thm:scg_strong}
Consider the Stochastic Continuous Greedy algorithm, with $\rho_t = \frac{1}{t^\alpha}$ where $\alpha\in (0,1)$. Then under Assumptions \ref{ass:x_bound}, \ref{ass:g_lip}, \ref{ass:subG}, %we have that
with probability greater than $1-\delta$:
\begin{align}
        F(\x_T) &\geq (1-\frac{1}{e})OPT - \frac{2DK\sigma\sqrt{\log(1/\delta)}}{T^{1/2}}\\
        &\quad\quad\quad\quad\quad\quad\quad\quad- (\frac{4K+1}{2})\frac{LD^2}{T} \nonumber \\
        &= (1-\frac{1}{e})OPT - \mathcal{O}\left(\frac{\sqrt{\log(1/\delta)}}{T^{1/2}}\right),%\vspace{-2mm}
\end{align}
where $K:=\frac{1}{1-\alpha}\Gamma\left(\frac{1}{1-\alpha}\right)$.
\end{theorem}
%Generally speaking we arrive at this bound by
At a high level, the proof of Theorem \ref{thm:scg_strong}
expands the momentum into a weighted summation of gradient approximations, which %we are then 
after some careful manipulations %able to
can be treated as a summation of sub-Gaussian variables. 
%The proof requires providing a high probability bound for the sum of momentum errors. 
This is to our knowledge the first such result for adaptive momentum optimization methods, where the momentum can change over time. Notably, it is general enough to be used even in the context of other
smooth function classes. Adaptive momentum enjoys some superior convergence and generalization
properties \cite{sun2021training}.
See \cref{sec:scg_strong} for the detailed proof.

Notably, the bound in Theorem \ref{thm:scg_strong} has a faster convergence rate than the original expectation bound of \cite{mokhtari2018conditional}, i.e., $(1-\frac{1}{e})OPT-\mathcal{O}(1/T^{1/3})$. The new bound suggests that for certain well conditioned problems, SCG can achieve the same convergence rate as SCG++. 
In our experiments in Sec. \ref{sec:experiments}, we show that empirically the distribution of solutions of SCG do converge when gradient noise is (sub-)Gaussian.

\subsection{Stochastic Continuous Greedy++}
Finally, we analyze the worst-case performance of the Stochastic Continuous Greedy++ (SCG++),
%SCG++ was shown in \cite{karbasi2019stochastic} to 
% This %speed-up
% is achieved
which aims to speed up SCG, using a stochastic path-integrated differential estimator (SPIDER) \cite{fang2018spider} for the gradient. 
SCG++ assumes that  the probability distribution of the random variable $\z$ depends on the variable $\x$ and may change during the optimization. %, i.e., each coordinate $\z_e$ is generated according to a Bernoulli distribution with parameter $\x_e$.
To obtain an unbiased gradient estimator $\gh_t$ with a reduced variance, SCG++ uses a mini-batch of %gradient 
samples to first get an unbiased estimate of %approximate 
the Hessian, $\tilde{\nabla}^2_t$:
\begin{equation}
    \tilde{\nabla}^2_t=\frac{1}{|\mathcal{M}|}\sum_{(a,\z(a))\in\mathcal{M}}\tilde{\nabla}^2 F(\x(a),\z(a)),
\end{equation}
%and allows for a direct estimation of the change in gradient as SCG++ travels across the function domain. 
% \ba{SCG++ is to exploit the high correlation between the consecutive iterates originated from the O(1/T)-sized increments to maintain a highly accurate estimate g_t}
where $a$ is selected uniformly at random from $[0,1]$, $\z(a)$ is a random variable with probability distribution $p(\z(a); \x(a))$, $\x(a) := a\x_t + (1-a)\x_{t-1}$, and $\mathcal{M}$ is a mini-batch containing $|\mathcal{M}|$ samples of random tuple $(a, \z(a))$.
Then, SCG++ uses the Hessian estimate to recursively calculate unbiased estimates of the gradient, based on the gradient differences  $\tilde{\Delta}^t$:%\vspace{-2mm}
\begin{equation}
    \tilde{\Delta}^t:=\tilde{\nabla}^2_t(\x_t-\x_{t-1}). 
\end{equation}
A gradient estimate, $\gh_t$, with a reduced variance is then calculated as the initial noisy gradient estimate plus the sum of all the gradient differences %over the optimization path
up to time $t$: \looseness=-1 %up to step %$t$: %is an unbiased estimator of the gradient difference $\Delta^t$. %an unbiased estimator of the gradient estimate $\g_t$ with a reduced variance.
\begin{align}
    \gh_t&=\nabla\Fn(\x_0,\mathcal{M}_0)+\sum_{i=1}^i\tilde{\Delta}^t.
    % \g_t&=\g_{t-1}+\tilde{\Delta}_t
\end{align}
% This yields a reduced variance for the gradient estimates $\g_t$.\looseness=-1
With the above gradient estimate $\gh_t$, SCG++ starts from $\x_0=\pmb{0}$, and at each iteration $t$, performs a standard Frank-Wolfe step with step-size $\frac{1}{T}$. %given a gradient estimator $\g_t$, 
% SCG++ solves the subproblem to obtain an element $\vb_t$
% in $\C$ as ascent direction:
% \begin{equation}
%     \vb_t = {\arg\max}_{\vb\in\C}\left<\vb, \gh_t\right>.
% \end{equation}
% % where $\g_t$ is obtained  as follows:
% % \begin{align}
% %     \tilde{\Delta}^t:=\tilde{\nabla}^2_t(\x_t-\x_{t-1}), \quad \g_t=\nabla\Fn(\x_0,\mathcal{M}_0)+\sum_{i=1}^i\tilde{\Delta}^t,
% % \end{align}
% % where $\tilde{\nabla}^2_t$ is an unbiased estimator of the Hessian calculated on a mini-batch $\mathcal{M}$, and
% % $\tilde{\Delta}^t$ is an unbiased estimator of the gradient difference $\Delta^t$.
% %%
% Then, $\vb_t$ is added to the iterate $\x_{t+1}$ with a scaling factor $1/T$, i.e., the new iterate $\x_{t+1}$ is computed by following the update \begin{equation}
%     % \vb_t \gets \arg\max_{\vb \in \C}\{\left<\gb_{t}^{T}, \vb\right>\}, \quad
%     \x_{t+1}=\x_t+\frac{1}{T}\vb_t.
% \end{equation}
The full update sequence is provided in Appendix \ref{apx:alg}.

SCG++ converges in expectation to the same $[(1-1/e)OPT-\epsilon]$ approximation as SCG, but using only $\mathcal{O}(1/\epsilon^2)$ stochastic gradient evaluations and $\mathcal{O}(1/\epsilon)$ calls to the linear optimization oracle \cite{karbasi2019stochastic}. However, similar to PGA and SCG, the expected approximation guarantee of SCG++ does not show the probability of returning a final solution that is much lower than the expected solution.

The analysis of SCG++ requires stronger assumptions than SCG. Besides the %standard
monotone DR-submodularity of $F$ and the bounded diameter on $\C$  (Assumption \ref{ass:x_bound}), we use the same assumptions originally used to analyze SCG++ in \cite{karbasi2019stochastic}:
\begin{assumption}
The function value at the origin is:
\begin{equation}
    F(\mathbf{0}) \geq 0.
\end{equation}
\end{assumption}
%
\begin{assumption}
The stochastic function $\Fn(\x,\z)$, its gradient, and its Hessian are bounded:
\begin{align}
    \Fn(\x,\z) &\leq B, \\
    \norm{\nabla \Fn(\x,\z)} &\leq G_{\Fn}, \\
    \norm{\nabla^2 \Fn(\x,\z)} &\leq L_{\tilde{F}}.
\end{align}
\end{assumption}

\begin{assumption}
The function $\log p(\z)$ has the following bounds on gradient and Hessian:
\begin{align}
    \mathbb{E}\left(\norm{\nabla \log p(\z)}^4\right) \leq G_p^4, \\
    \mathbb{E}\left(\norm{\nabla^2 \log p(\z)}^2\right) \leq L_p^2.
\end{align}
\end{assumption}

\begin{assumption}
The Hessian of the stochastic function $\Fn(\x,\z)$ is $L_2$ Lipschitz continuous with constant $L_{2,{\Fn}}$. The Hessian of the function $\log p(\z)$ is $L_2$ Lipschitz continuous with constant $L_{2,p}$.
\begin{equation}
    \norm{\nabla^2\Fn(\x,\z)-\nabla^2\Fn(\y,\z)} \leq L_{2,{\Fn}} \norm{\x-\y},
\end{equation}
\begin{equation}
    \norm{\nabla^2\log p(\z)-\nabla^2\log p(\z)} \leq L_{2,p} \norm{\x-\y}.
\end{equation}
\end{assumption}

% Note that these assumptions are the same as those used by Karbasi et al. 
% The authors in \cite{karbasi2019stochastic} show that when these conditions are met
Under the above assumptions and with $\mathcal{O}(\epsilon^{-1})$ calls to the stochastic oracle per iteration, the variance of the gradient approximation $\gh_t$ converges to zero %at a rate of $\mathcal{O}((1+\epsilon t)\epsilon)$
\cite{karbasi2019stochastic}.
% \begin{lemma}{(Karbasi et al. 2019)}
%       Consider choosing the batch size for each step of scg++ as $|\mathcal{M}|=1/\epsilon$, then we have for all $t=0,\dots, T$
%         \begin{equation}
%         \mathbb{E}\left[\norm{\nabla F(x_t) - g_t}^2\right] \leq (1+\epsilon t) \bar{L}^2 D^2 \epsilon^2
%         \end{equation}
%         Where $\bar{L}^2:= 4B^2G^4+16G^4+L^2+4B^2L^2$, $L:=\max\{L_{2,F}, L_{2,p}\}$, $G:=\max\{G_F, G_p\}$
% \end{lemma}
% Different than \cite{karbasi2019stochastic},
%The key difference in our work comes from how we apply this variance bound. Using
Instead of directly upper bounding variance as in \cite{karbasi2019stochastic},
we use Chebyshev's inequality to prevent any expectations from appearing in our bound. Specifically we use the following Lemma: 
% \ba{explain this sentence better (the part about [15])}\ev{done}
\begin{lemma}\label{lemma:scgplus}
    Given SCG++ under Assumptions 1,5-8, we have the following high probability bound: 
    \begin{equation}
    \mathbb{P}\left(\sum_{t=0}^{T-1}\norm{\nabla F(\x_t) - \gh_t}^2 \leq \delta^2\sum_{t=0}^{T-1}\frac{2L^2D^2}{t^2}\right) \leq 1-\frac{T}{\delta^2}.
    \end{equation}
\end{lemma}
Lemma \ref{lemma:scgplus} allows us to directly bound the function value of last iterate of SCG++, $F(\x_T)$, with a high probability.
\begin{theorem}
\label{thm:scg_plus}
    Consider applying SCG++ under Assumptions 1, 5-8. Then with probability $1-\frac{T}{\delta^2}$ :
    \begin{align}
        F(\x_T) &\geq (1-\frac{1}{e})F(\x^*) - \delta\frac{LD^2}{T^2} -\frac{LD^2}{2T^2} \nonumber\\
        &= (1-\frac{1}{e})OPT - \mathcal{O}(\frac{\delta}{T}).
    \end{align}
\end{theorem}

For a fixed probability threshold %\ba{confidence interval?}\ev{this p is a probability value, so more like a p-value if anything}
$p$ we get the next Corollary: \looseness=-1
\begin{corollary}\label{cor:scg_plus} For $\delta \!=\! \sqrt{\frac{T}{1-p}}$, with probability greater than $p$, we have: %\vspace{-2mm}
    \begin{align}
    F(\x_T) &\geq (1-\frac{1}{e})F(\x^*) - \frac{LD^2}{\sqrt{1-p}}\frac{1}{\sqrt{T}} -\frac{LD^2}{2T^2} \\
    &= (1-\frac{1}{e})OPT - \mathcal{O}(\frac{1}{\sqrt{T}})
\end{align}
\end{corollary}

SCG++ makes $\mathcal{O}(T)$ queries to the stochastic gradient oracle per iteration, and $K=T^2$ queries in total. Hence, with probability greater than $p$ the bound in Corollary \ref{cor:scg_plus} becomes:
\begin{equation}
    F(\x_T) \geq  (1-\frac{1}{e})OPT - \mathcal{O}(\frac{1}{K^{1/4}}).
\end{equation}
% Unlike the standard SCG algorithm, SCG++ converges fast enough for our Chebyshev-type bound to
% Our result shows that for any single run, SCG++ converges with a high probability, if given enough time. 
For any fixed confidence interval $p$, the lower bound still converges to $(1-\frac{1}{e})OPT$, albeit at a slower rate. 
However, we believe that a tighter $1/\sqrt{K}$ high probability bound likely exists, as evident by our experimental results in Sec. \ref{sec:experiments}.\looseness=-1

