\section{Background}

In this section we introduce the definitions for continuous submodular functions, as well as some classical tail bounds. 

\subsection{Submodularity and Continuous Extensions}

Given the ground set V of n elements, a set function $f: 2^V \rightarrow \mathbb{R}_{+}$ is submodular if either of the following two equivalent definitions hold true:
\begin{enumerate}
    \item The lattice condition:
    \begin{equation}
        F(A) + F(B) \geq F( A \cup B) + F(A \cap B), \quad \forall A, B \subset V 
    \end{equation}
    \item The diminishing returns condition:
    \begin{equation}
    F(A \cup \{ v \}) - F(A) \geq F(B \cup \{ v \}) - F(B), \quad \forall A \subset B \subset V
    \end{equation}
\end{enumerate}

The objective function generally quantifies utility, coverage, relevance, diversity, etc. Additionally, a function $f$ is said to be monotone if $F(A) \leq F(B)$ for $\forall A \subset B \subset V$. Discrete Submodularity is computationally effective as many problems with such properties can be approximated or efficiently solved. One particular instance is a (1-1/e) approximation for maximizing a monotone submodular set function subject to a cardinality constrained. Submodularity is most commonly associated with set functions but in many practical scenarios, it is natural to consider a generalization of such set functions over defined integer lattices. 

Submodularity is widely considered in the discrete setting, but the notion can be generalized to arbitrary lattices. Since in the continuous setting, the idea of unions or intersections are non-existent, a coordinate wise minimum $x \wedge y = \min(x,y)$ and coordinate wise maximum $x \vee y = \max(x,y)$ can be utilized for unions and intersections respectively. A function $F: \chi \subset \mathbb{R}^n \rightarrow \mathbb{R}$ is continuous if holds true for the following two definitions:


\begin{definition}
The continuous lattice definition of submodularity for continuous vectors x and y define the following inequality:

\begin{equation}
    F(x) + F(y) \geq F( x \wedge y) + F(x \vee y) 
\end{equation}


When F is twice differentiable, all the non-diagonal entries of the Hessian matrix should be non-positive. 

\begin{equation}
    \forall i \neq j, \forall x \in \chi, \frac{\partial^2 F(x)}{\partial x_i \partial x_j} \leq 0
\end{equation}


\begin{equation}
    \nabla^2 F(x) = \begin{pmatrix}
* & \leq 0 & \leq 0\\
\leq 0 & * & \leq 0\\
\leq 0 & \leq 0 & *
\end{pmatrix}
\end{equation}


\end{definition}



\begin{definition}
For continuous submodularity, the diminishing return property can be defined as
\begin{equation}
    \forall x \leq y \text{ and } \alpha > 0 \text{ and } e_i= i^{th} \text{ standard vector, it holds: } 
\end{equation}

\begin{equation}
    F(x + \alpha e_i) - F(x) \geq F(y + \alpha \e_i) - F(y)
\end{equation} \\

When F is twice differentiable, all the entries of the Hessian matrix should be non-positive. 

\end{definition}


\begin{equation}
\nabla^2 F(x) = \begin{pmatrix}
\leq 0 & \leq 0 & \leq 0\\
\leq 0 & \leq 0 & \leq 0\\
\leq 0 & \leq 0 & \leq 0
\end{pmatrix}
\end{equation}


\subsection{Classical High Probability Bounds}
A Martingale process is a sequence of partial sums where the conditional expectation of the next value in the sequence is equivalent to the current value irrespective of the previous values. The simple definition of the martingale process in the discrete domain is the following:

\begin{definition}
A stochastic process $X_1, X_2, X_3, ...$ for any discrete time n is Martingale if it satisfies
\begin{equation*}
    \mathbb{E}(|X_n|) < \infty
\end{equation*}
\begin{equation*}
\mathbb{E}(X_{n+1}|X_1, ...., X_n)  = X_n
\end{equation*}
\end{definition}
For example, if $y_i$ is some independent identically distributed variable with $\mathbb{E}(y_i)=0$, then it is easy to see that $X_i = \sum_{j=1}^i y_j$ is a martingale process.

\paragraph{Azuma-Hoeffding Inequality:} Assume the sequence $\{X_i\}_{0:N}$ is martinagle. Furthermore, assume that $|X_{i+1}-X_i|\leq c_i, \: \forall i\in {0\dots N}$. Then we have that $\forall \delta > 0$:

\begin{equation}
    \mathbb{P}(|X_N - X_0| \geq \delta) \leq 2\exp \left(- \frac{\delta^2}{2\sum_{i \leq N} c_{i}^2}\right)
\end{equation}

\paragraph{Chebyshev's Inequality:} Given a random variable $X$ with finite expectation $\mu < \infty$ and variance $\mathbb{E}(X-\mu)^2 = \sigma^2$ we have the following inequality:
\begin{equation}
    \mathbb{P}(|X-\mu| \geq \delta \sigma) \leq \frac{1}{\delta^2}
\end{equation}
