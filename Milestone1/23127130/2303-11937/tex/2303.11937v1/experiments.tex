\section{NUMERICAL RESULTS} \label{sec:experiments}

In our experiments, we first show that bad solutions of PGA, {boosted PGA} SCG, and SCG++ can be much worse than their expected values. Then, we validate our proposed bounds 
on simulated and real-world datasets. 
In practice, due to measurement errors or inexact function calculations,
the function and thus
the gradient evaluations are often noisy. In such situations, our high probability bounds
can effectively quantify the worst-case performance and be utilized to mitigate the risk of getting a bad solution.





\subsection{Continuous Submodular Problems}\label{sec:csp}
First, we introduce three monotone continuous DR-submodular problems that we use in our experiments.

\vspace{-2mm}
\paragraph{\!Non-convex/non-concave quadratic \!programming (NQP).}
NQP functions of the form $f(\x) = \frac{1}{2} \x^T \pmb{H} \x + \pmb{h}^T\x$ arise in many applications, including scheduling, inventory theory, and free boundary problems \cite{bian2017continuous}. When all off-diagonal entries of $\pmb{H}$ are non-positive, the NQP is submodular.

% Following %the settings in
%\cite{bian2017guaranteed}, 
For our experiment, we randomly generate $n\!=\!100$ monotone DR-submodular NQP functions, %of the form $f(\x) = \frac{1}{2} \x^T \pmb{H} \x + \pmb{h}^T\x$,
where each $\pmb{H} \in \R^{n\times n}$ is a symmetric matrix sampled uniformly from $[-100, 0]$. We further generated a set of $m \!=\! 50$ linear constraints to construct the positive polytope $\mathbb{P} = \{\x \in \R^{n}, \pmb{Ax} \leq \pmb{b}, 0 \leq \x \leq \bar{\pmb{u}}\} $, where entries in $\pmb{A} \in \R^{m\times n}$ are uniformly sampled from $[0, 1]$, $\bar{\pmb{u}} = \boldsymbol{1}$, and $\pmb{b} = \boldsymbol{1}$. %for $c \in \{1, 10\}$. 
To make $f$ monotone, we ensure the gradient of $f$ is non-negative by setting $\pmb{h} = -\pmb{H}\bar{\pmb{u}}$. \looseness=-1

\vspace{-2mm}
\paragraph{Optimal budget allocation with continuous assignments.}
% In this experiment, we follow the setting of the optimal budget allocation problem in \cite{bian2017guaranteed}. 
The budget allocation problem can be modeled as a bipartite graph $(S, T; W)$, where $S$ is a set of advertising channels and $T$ is a set of customers. The edge weight $p_{st} \in W$ represent the influence probability of channel $s$ on customer $t$. The objective is to maximize the total influence on the customers by allocating the budget to the set of advertising channels. The total influence on customer $t$ from all the channels can be model by a monotone DR-submodular function $I_t(\x) = 1 - \prod_{(s, t) \in W} (1-p_{st})^{\x_s}$, where $\x_s \in \mathbb{R}_+ $ is the budget allocated to channel $s$. Then for a set of $k$ advertisers, where $\x^i \in \mathbb{R}_+^S$ is the budget allocation of the $i^{th}$ advertiser and $\x = [\x^1, \cdots, \x^k]$, the overall objective is \vspace{-6mm}
\begin{align}
g(\x) = \sum_{i=1}^k \alpha_i f(\x^i) , \quad &\text{with} \quad f(\x^i) = \sum_{t \in T} I_t(\x^i), \\
&0 \leq \x^i \leq \bar{\pmb{u}}^i, \quad
\forall 1 \leq i \leq k, \nonumber
\end{align}
where $\alpha_i$ is a constant weight coefficient and $\bar{u}^i$ is the budget limit on each channel for the $i^{th}$ advertiser.

For a real-world instance of the budget allocation problem, we use the Yahoo! Search Marketing Advertiser Bidding Data \cite{yahoo}, which includes search keyword phrases and the bids placed on them by online customers. The dataset consists of 1,000 search keywords, 10,475 customers and 52,567 edges, where each edge between a keyword and customer represents the customer has bid on the keyword. A customer may bid on one phrase multiple times, and we use the frequency of a (phrase, customer) pair to measure the influence probability of the phrase on that customer. Additionally, we use the average bidding price across all the bids in the dataset as the limit on the budget of all the advertisers. 

\subsection{Bad Solutions and High-probability Bounds}
\begin{figure}[t]
\vspace{-2mm}
    \centering
    \includegraphics[width=0.23\textwidth]{images/box_allT5.png}
    \includegraphics[width=0.23\textwidth]{images/box_allT100.png}
    \vspace{-1mm}
    \caption{
    The distribution of $\min_{\tau\in [T]} F(\x_\tau) - \mathbb{E}[F(\x_\tau)]$ for PGA and boosted PGA, and $F(\x_T) - \mathbb{E}[F(\x_T)]$ for SCG and SCG++ on NQP, over 100 runs with $T=5$ (left), and $T=100$ (right). 
    Bad solutions %PGA, SCG, SCG++
    can be much worse than the expected values. 
    }\vspace{-2mm}
    \label{fig:notexpect}
\end{figure}
\begin{figure*}[t]
\centering
\begin{subfigure}[NQP, PGA \label{subfig:nqp_pga}]{
\includegraphics[width=.28\textwidth,height=.2\textwidth]{images/pga_nqp_fix_axis.png}
\vspace{-5mm}
}%\hspace{5mm}
\end{subfigure}
%%%%%%%%%%%%%
\begin{subfigure}[NQP, Boosted PGA \label{subfig:yahoo_boost_pga}]{
% \vspace{-10mm}
\includegraphics[width=.28\textwidth,height=.2\textwidth]{images/boost_pga_nqp.png}
\vspace{-3mm}
}%\hspace{5mm}
\end{subfigure}
%%%%%%%%%%%%
\begin{subfigure}[NQP, SCG \label{subfig:nqp_scg}]{
\includegraphics[width=.28\textwidth,height=.2\textwidth]{images/scg_nqp_fix_axis.png}
\vspace{-5mm}
}
\end{subfigure}
\begin{subfigure}[NQP, SCG++ \label{subfig:nqp_scgplus}]{
\includegraphics[width=.28\textwidth,height=.2\textwidth]{images/scgpp_nqp_fix_axis.png}
}
\end{subfigure}
% \vspace{-10mm}
\begin{subfigure}[Yahoo!, PGA \label{subfig:yahoo_pga}]{
% \vspace{-10mm}
\includegraphics[width=.28\textwidth,height=.2\textwidth]{images/pga_yahoo_fix_axis.png}
\vspace{-3mm}
}%\hspace{5mm}
\end{subfigure}
%%%%%%%%%%%%%
\begin{subfigure}[Yahoo!, Boosted PGA \label{subfig:yahoo_boost_pga}]{
% \vspace{-10mm}
\includegraphics[width=.275\textwidth,height=.2\textwidth]{images/boost_pga_yahoo.png}
\vspace{-3mm}
}%\hspace{5mm}
\end{subfigure}
%%%%%%%%
% \hspace{6mm}
\begin{subfigure}[Yahoo!, SCG \label{subfig:yahoo_scg}]{
\hspace{0mm}
\includegraphics[width=.28\textwidth,height=.2\textwidth]{images/scg_yahoo_fit_new_T2.png}
\vspace{-3mm}
% [-2ex]
}
\end{subfigure}
% \hspace{1mm}
\begin{subfigure}[Yahoo!, SCG++ \label{subfig:yahoo_scgplus}]{
\includegraphics[width=.28\textwidth,height=.2\textwidth]{images/scgpp_yahoo_fix_axis.png}
\vspace{-1mm}
}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[NQP, SCG \label{subfig:example}]{
% \vspace{-10mm}
\hspace{-2mm}
\includegraphics[width=.3\textwidth,height=.2\textwidth]{images/simpleExample.png}
\vspace{-3mm}
}%\hspace{5mm}
\end{subfigure}
%%%%%%%%%%%%
\vspace{-2mm}
\caption{(a)-(h) Median, minimum, {and 90\% percentile} of the normalized solutions obtained by PGA, Boosted PGA, SCG, SCG++ compared to our bounds provided in Theorems \ref{thm:pga_bound},
\ref{thm:nonobv_pga_bound}, \ref{thm:scg_strong}, \ref{thm:scg_plus}. The results of PGA, Boosted PGA, SCG, SCG++ are averaged over 100 runs. (i) validating our bounds on a simple NQP example where the constants are known. 
}\vspace{-2mm}
\label{fig:min_med}
\end{figure*}

\vspace{-1mm}
\textbf{Setup.}
We repeat every experiment 100 times, and use a step-size of $1e-4$, 
% $1e-3$, 
$1e-2$ for PGA methods on NQP
% , sensor data, 
and Yahoo! respectively. We set the step-size to $1/T$ for SCG and SCG++, 
and use batch size of $T$ for SCG++.
For SCG, we use $\frac{4}{(t+8)^{2/3}}$ as the momentum coefficient. 
For PGA methods, we randomly initialize $\x_0\sim\mathcal{N}(0, 1)$ from a Gaussian distribution, % with mean 0 and variance 1 
and for SCG and SCG++ we initialize $\x_0=\pmb{0}$. Additionally, for PGA, boosted PGA, and SCG experiments, we add a noise sampled from a Gaussian distribution with mean 0 and standard deviation proportional to the gradient norms normalized by its dimensionality to the queried gradients. SCG++ uses noisy estimates of the Hessian, hence we add a smaller Gaussian noise with mean 0 to the Hessian.

% \vspace{-.25mm}
\textbf{Bad solutions are far from expectation.}
First, we look at the solution of %different algorithms, namely 
PGA, SCG, and SCG++ on NQP, to see how far the solution may be from the expected value.
Note that when running each algorithm for $T$ iterates, PGA returns the solution for a random iterate $\tau\in[T]$, and SCG, SCG++ return the solution of the final iterate $T$.
Fig. \ref{fig:notexpect} shows the distribution of $\min_{\tau\in [T]} F(\x_\tau)$ for PGA, and $F(\x_T)$ for {boosted PGA,} SCG and SCG++ for $T=5$ (left), and $T=100$ (right), over 100 runs. We see that bad solutions of the algorithms can be much worse than their expected value. 
While more number of iterations reduces the variance of SCG, we see that PGA and SCG++ have a very high variance %for their solutions
even after $T=100$ rounds (see \Cref{apx:experiments} for more details). 
This shows the insufficiency of the expected guarantees, and confirms the necessity of high-probability analysis. \looseness=-1

% \vspace{-.2mm}
\textbf{High-probability bounds.}
Next, we empirically confirm the validity of our high-probability bounds. 
To do so, we first apply PGA, Boosted PGA, SCG, and SCG++ to the continuous submodular problems discussed in Sec. \ref{sec:csp}.
Then, we compare the empirical results with our bounds in Theorems \ref{thm:pga_bound}, \ref{thm:nonobv_pga_bound}, \ref{thm:scg_strong}, \ref{thm:scg_plus}.
Specifically, for each iteration $t$, we report the average utility up to $t$, i.e. $\frac{1}{t}\sum_{i=1}^t F(\x_i)$, for PGA; and the value of $F(\x_t)$ %in the last iteration $T$ for each experiment of
for Boosted PGA, SCG, and SCG++.
To avoid the need for calculating the exact scaling constants in Theorems (e.g. $L, D, K$, etc.) and the true optimal value $OPT$ (which determines the asymptote), we fit a line in the same form of the lower bound derived in Theorems \ref{thm:pga_bound}, 
\ref{thm:nonobv_pga_bound}, \ref{thm:scg_strong}, \ref{thm:scg_plus}, to the output of the algorithms. For each line, $c_1$ %should 
corresponds to fraction of $OPT$ the bound is converging to, while $c_2$ scales the rate of convergence %up or down 
depending on the problem-specific constants and desired confidence threshold. 
%%%%%%%%%
Specifically, for each algorithm we fit a line of the form $l(t)=c_1\!-\!\frac{c_2}{\sqrt{t}}$.
%%%%%%%%%
Importantly, as SCG++ uses batch size %per iteration is
of $\mathcal{O}(T)$, an equivalent form of this fitted line is $l%_{SCG++}
(t)=c_1\!-\!\frac{c_2}{k^{1/4}}$, which is slower than the previous two algorithms. %to the results of our PGA, SCG and SCG++ experiments. 
For $c_1$, we use the same value for min, median, and different percentiles, by taking the average of the $c_1$ values obtained from the fitted lines for an algorithm on the same dataset.
Using the above %value for 
$c_1$, we fit the curves again to get the corresponding $c_2$ for each line.\looseness=-1 

Fig. \ref{fig:min_med}(a)-(h) show the median, minimum, {and 90\% percentile} of utility over the course of training of each algorithm, compared to our predicted lower bounds.
We see that our bounds closely match these utility statistics for various iterations of PGA, SCG, SCG++, and boosted PGA applied to different problems. Since for each percentile level the bound is of the same order, the \emph{differences} in percentiles decrease as well (e.g. $\frac{a}{t^{1/2}} - \frac{b}{t^{1/2}} =  \frac{c}{t^{1/2}}$). This effect can be seen as the minimum returned value across runs approaches the median and 90\% returned values as the number of training iterations increase.


We further validate our bounds by adding a simple example where the constants are known and running SCG on the problem. 
We construct a small NQP where $\pmb{H} \!\in\! \R^{5\times 5}$ is sampled uniformly from $[-1, 0]$. 
Hence, the Lipschitz constant 
$L=\norm{\pmb{H}}_2 \!=\! \sqrt{\lambda_{max}(\pmb{H}^T \pmb{H})}$.
We use linear constraints as Sec. \ref{sec:csp} and set $\pmb{A} = [0.2, 0.2, 0.2, 0.2, 0.2]$.
Thus, diameter $D=\norm{\textbf{1}}_2$. 
For a clipped Gaussian noise $clip(\mathcal{N}(0, \sigma), -2\sigma, 2\sigma)$ to queried gradients for SCG, $M=2\sqrt{5}\sigma$. The optimal value for the problem is approximated by taking the maximum value across 100 SCG runs with 5000 iterations. 
Using above constants in Theorem \ref{thm:pga_bound},
\ref{thm:nonobv_pga_bound}, \ref{thm:scg_strong}, \ref{thm:scg_plus}, Fig. \ref{subfig:example} shows our predicted lower bound with 99\% confidence converges quickly to the minimum of the collected utility trajectories. 

