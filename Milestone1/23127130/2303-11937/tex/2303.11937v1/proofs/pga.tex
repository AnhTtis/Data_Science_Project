\subsection{Proof of \Cref{thm:pga_bound}}\label{sec:pga_proof}
We first make use of an alternative form  of  the usual Azuma-Hoeffding inequality from \cite{chung2006concentration}: 
\begin{theorem}{(Chung and Lu 2006, theorem 5.2)}\label{thm:chung-concentration}
If martingale X is c-lipschitz, then
\begin{equation}
    \mathbb{P}\left(|X-\mathbb{E}(X)| \leq \sqrt{2\sum_i c_i^2 \log(1/\delta)}\right) \geq 1-\delta
\end{equation}
\end{theorem}
Next, we prove \cref{thm:pga_bound}.
\begin{proof}
Submodularity guarantees that for any two points $x,y \in \mathcal{X}$:
\begin{equation}
    \nabla F(\x) \geq \nabla F(\y) \: \text{for all} \: \x\leq \y
\end{equation}
Using eq 7.2 from Hassani et al. 2017 in \cite{Hassani2017gradient} we know that for any two points $\x, \y \in \mathcal{X}$ we have the relation 
\begin{align}
    F(\y)-2F(\x) \leq \left< \nabla F(\x), \y-\x\right>
\end{align}
This allows us to proceed with a derivation of convergence similar to \cite{zhang2017empirical} for SGD: First replace our $\y,\x$ in the previous inequality with $\x^*$ (the maximizing input to our function) and $\x_t$ (the $t$-th iteration of our algorithm) respectively. Then letting $\hat{\z}_t:= \g_t - \nabla F(\x_t)$ denote the random difference between the true and noisy gradient we have 
\begin{align}\label{eq:proof1-step1}
     F(\x^*)-2 F(\x_t) &\leq  \left< \nabla F(\x_t), \x^*-\x_t\right> \nonumber \\
     &= \left<\x^*-\x_t, \g_t\right> - \left<\x^*-\x_t, \hat{\z}_t\right> \nonumber \\
     &\leq  \frac{1}{\eta_t}\left<\x^*-\x_t, \x'_{t+1}-\x_t\right> - \left<\x^*-\x_t, \hat{\z}_t\right>
\end{align}
In the second inequality we use the definition of our gradient step (before projection) $\x'_{t+1} = \x_t + \eta_t \g_t$. Next, through some algebra we get
\begin{align}\label{eq:proof1-step2}
     F(\x^*)-2F(\x_t) &\leq \frac{1}{2\eta_t}(\norm{\x_t-\x'_{t+1}}_2^2 + \norm{\x_t-\x^*}_2^2 - \norm{\x'_{t+1}-\x^*}_2^2)- \left<\x^*-\x_t,\hat{\z}_t\right> \nonumber \\
     &\leq \frac{1}{2\eta_t }(\norm{\x_t-\x'_{t+1}}_2^2 + \norm{\x_t-\x^*}_2^2 - \norm{\x_{t+1}-\x^*}_2^2)- \left<\x^*-\x_t, \hat{\z}_t\right>
\end{align}
By the property of euclidean projections we know $\norm{\x_{t+1}-\x^*} \leq \norm{\x'_{t+1}-\x^*}$ which gives us the second line above. Next we use our gradient step equation and lipschitz-smoothness assumption to get:
\begin{align}\label{eq:proof1-step3}
    F(\x^*)-2F(\x_t) &\leq \frac{\eta_t}{2}\norm{\g_t}_2^2 + \frac{1}{2\eta_t}(\norm{\x_t-\x^*}_2^2 - \norm{\x_{t+1}-\x^*}_2^2)- \left<\x^*-\x_t, \hat{\z}_t\right> \nonumber \\
    &\leq \frac{\eta_t (L+M)^2}{2} + \frac{1}{2\eta_t }(\norm{\x_t-\x^*}_2^2 - \norm{\x_{t+1}-\x^*}_2^2)- \left<\x^*-\x_t, \hat{\z}_t\right>
\end{align}
We let $\Delta_t \triangleq \left<\x_t-\x^*, \hat{\z}_t \right>$, and $\eta_t = \frac{2}{\sqrt{t}}$ to get
\begin{align}\label{eq:single-point-bound}
    F(\x^*)-2F(\x_t) &\leq \frac{(L+M)^2}{\sqrt{t}} + \frac{\sqrt{t}}{4}(\norm{\x_t-\x^*}_2^2 - \norm{\x_{t+1}-\x^*}_2^2) + \Delta_t \nonumber \\
     &\leq \frac{(L+M)^2}{\sqrt{t}} + \frac{\sqrt{T}}{4}(\norm{\x_t-\x^*}_2^2 - \norm{\x_{t+1}-\x^*}_2^2) + \Delta_t
\end{align}
If combine the inequalities for each $\x_t$ and divide by T we have:
\begin{equation}
     F(\x^*) - (\frac{2}{T})\sum_{t=1}^T F(\x_t) \leq \frac{1}{T} \left( \sum_{t=1}^T\frac{1}{\sqrt{t}}(L+M)^2 + \frac{\sqrt{T}}{4}\norm{\x^*-\x_1}_2^2 + \sum_{t=1}^T \Delta_t\right) \nonumber \\
\end{equation}
Using the fact that $\sum_{t=1}^T \frac{1}{\sqrt{t}} \leq 2\sqrt{T}$ we simplify to
\begin{align}
    F(\x^*) - (\frac{2}{T})\sum_{t=1}^T F(\x_t) &\leq \frac{1}{T} \left( 2\sqrt{T}(L+M)^2 + \frac{\sqrt{T}}{4}\norm{\x^*-\x_1}_2^2 + \sum_{t=1}^T \Delta_t\right) \nonumber \\ 
    &= \frac{2(L+M)^2}{\sqrt{T}} + \frac{\norm{\x^*-\x_1}_2^2}{4\sqrt{T}}  + \frac{1}{T} \sum_{t=1}^T \Delta_t
\end{align}
Rearranging this inequality we see
\begin{align}\label{eq:proof1-combine-step}
    \frac{1}{2}OPT - \frac{1}{T}\sum_{t=1}^T F(\x_t) \leq  \frac{(L+M)^2}{\sqrt{T}} + \frac{\norm{\x^*-\x_1}_2^2}{8\sqrt{T}} + \frac{1}{2T} \sum_{t=1}^T \Delta_t
\end{align}
Let $\Delta'_t \triangleq \frac{1}{2}\Delta_t$ so we can simplify to 
\begin{align}
    \frac{1}{2}OPT - \frac{1}{T}\sum_{t=1}^T F(\x_t) \leq  \frac{1}{\sqrt{T}}\left(\frac{8(L+M)^2 +\norm{\x^*-\x_1}_2^2}{8}\right) + \frac{1}{T} \sum_{t=1}^T \Delta'_t
\end{align}
Our next step is to use the Azuma-Hoeffding inequality  to bound $\frac{1}{T} \sum_{t=1}^T \Delta'_t$. If we can show that $\{\Delta'_t\}_T$ is a bounded martingale difference sequence with zero expectation, then we know by \Cref{thm:chung-concentration} with probability $1-\delta$:
\begin{align}
    |\sum_{t=1}^T \Delta'_t| \lesssim \sqrt{T\log(1/\delta)}
\end{align}
Expanding $\mathbb{E}(\Delta'_t)$ we see that because $\x_t-\x^*$ is independent of the gradient error $\hat{\z}_t$ we have
\begin{align}
    \mathbb{E}(\Delta'_t) &= \mathbb{E}\left(\frac{1}{2}\left<\x_t-\x^*, \hat{\z}_t \right>\right) = \frac{1}{2}\left<\mathbb{E}(\x_t-\x^*), \mathbb{E}(\hat{\z}_t) \right> = 0
\end{align}
Similarly, we can use triangle inequalities and our assumptions to bound $\norm{\Delta'_t}$ by a constant
\begin{align}
    \norm{\Delta'_t} \leq \frac{1}{2}\norm{\x_t-\x^*} \norm{\hat{\z}_t} \leq \frac{DM}{2}
\end{align}
Recall D is the maximum distance between any two points in our set $\mathcal{C}$. Therefore we have
\begin{equation}
    \mathbb{P}\left(|X-\mathbb{E}(X)| \leq DM\sqrt{T \log(1/\delta)/2}\right) \geq 1-\delta
\end{equation}

Therefore we have a martingale difference sequence which implies with probability $1-\delta$
\begin{align}
     \frac{1}{2}OPT - \frac{1}{T}\sum_{t=1}^T F(\x_t) &\leq \frac{1}{\sqrt{T}}\left(\frac{8(L+M)^2 +D^2}{8}\right) + \sqrt{\frac{ \log(\frac{1}{\delta})}{2T}}DM \nonumber \\
     &= O(\frac{1}{\sqrt{T}})
\end{align}
\end{proof}