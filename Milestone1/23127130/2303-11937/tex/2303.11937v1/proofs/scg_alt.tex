\subsection{Proof of  \Cref{thm:scg_strong} (Stronger SCG Bound)}\label{sec:scg_strong}
Before verifying \cref{thm:scg_strong}, we first provide the following lemma, which gives a high probability bound for the sum of momentum errors:
\begin{lemma}\label{lem:mom_bound}
Given i.i.d. errors $\norm{\nabla F(\x_t)-\g_t}$ that are each sub-Guassian with parameter $\sigma$, and momentum $\gb_t = (1-\rho_t)\gb_{t-1} + \rho_t \nabla \tilde{F}(\x_t,\z_t)$ with parameter of the form $\rho_t =\frac{1}{t^\alpha}$ and $\alpha \in (0,1)$, then with probability greater than $1-\delta$
 \begin{equation}
     \sum_{t=1}^T\norm{\nabla F(\x_t)-\gb_t} \leq  \sqrt{2K^2\sigma^2T\log(1/\delta)} + LDK
 \end{equation}
 Where $K:=\frac{1}{1-\alpha}\Gamma(\frac{1}{1-\alpha})$
\end{lemma}
{The cumulative error bound in \cref{lem:mom_bound} is to our knowledge the first such result for adaptive momentum optimization methods (i.e. the momentum can change over time). Notably, it is general enough to be used even in the context of other smooth function classes. Recently, \cite{sun2021training} showed adaptive momentum enjoys some superior convergence and generalization properties, and hence our lemma is of independent interest. See \cref{apx:mom_error} for the proof of this lemma. }

Next, we show a tighter high probability bound on the final iterate of the SCG algorithm in \cref{thm:scg_strong}. 
\begin{proof}
Using the smoothness of F and boundedness of $x\in\mathcal{C}$, Hassani et al. 2017 showed that
\begin{equation}
        F(\x_{t+1}) - F(\x_t) \geq \frac{1}{T}(F(\x^*)-F(\x_t)) - \frac{LD^2}{2T^2} + \frac{1}{T}\left<\vb_t-\x^*, \nabla F(\x_t)-\gb_t\right>
\end{equation}
The Cauchy-Schwarz inequality allows us to bound the last term by
\begin{align}
    \frac{1}{T}\left<\vb_t-\x^*, \nabla F(\x_t)-\gb_t\right> \geq -\frac{1}{T}\norm{\vb_t-\x^*}\norm{\nabla F(\x_t)-\gb_t} \geq -\frac{2D}{T}\norm{\nabla F(\x_t)-\gb_t}
\end{align}
Substituting this bound and rearranging terms we have
\begin{equation}
        F(\x^*) - F(\x_{t+1}) \leq (1-\frac{1}{T})(F(\x^*)-F(\x_t))  +\frac{2D}{T}\norm{\nabla F(\x_t)-\gb_t} + \frac{LD^2}{2T^2}
\end{equation}
Applying this inequality recursively for $t=0, \dots, T-1$ we have
\begin{align}\label{eq:pre-lem5}
    F(\x^*) - F(\x_{T}) \leq (1-\frac{1}{T})^T(F(\x^*)-F(\x_0)) + \frac{2D}{T}\sum_{t=0}^{T-1}\norm{\nabla F(\x_t)-\gb_t} + \frac{LD^2}{2T}
\end{align}
Using a momentum term of $\rho_t=\frac{1}{t^{1/2}}$ for \cref{lem:mom_bound} means that $\alpha=0.5$ and hence $K=2$. Directly substituting this inequality into \cref{eq:pre-lem5} gives us
\begin{align}
        F(\x^*) - F(\x_{T}) &\leq (1-\frac{1}{T})^T(F(\x^*)-F(\x_0)) + \frac{2D}{T} \left[ \sqrt{2K^2\sigma^2T\log(1/\delta)} + LDK\right]+ \frac{LD^2}{2T} \nonumber\\
        &\leq \frac{1}{e}(F(\x^*)-F(\x_0)) + \frac{2D\sqrt{2K^2\sigma^2\log(1/\delta)}}{\sqrt{T}} + (\frac{4K+1}{2})\frac{LD^2}{T}
\end{align}
Dropping $F(x_0)$ and rearranging we have
\begin{equation}
    F(\x_T) \geq (1-\frac{1}{e})F(\x^*) - \frac{2DK\sigma\sqrt{\log(1/\delta)}}{T^{1/2}} - (\frac{4K+1}{2})\frac{LD^2}{T}
\end{equation}
\end{proof}