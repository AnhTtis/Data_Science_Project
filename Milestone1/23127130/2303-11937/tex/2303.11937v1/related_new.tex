\section{RELATED WORK}
\vspace{-2mm}
\paragraph{Deterministic continuous submodular maximization.}
Maximizing deterministic continuous submodular functions have been first studied by Wolsey \cite{wolsey1982analysis}. More recently, \cite{chekuri2015multiplicative} proposed a multiplicative weight update algorithm that achieves $(1\!-\!1/e\!-\!\epsilon)$ approximation guarantee after $\tilde{O}(n/\epsilon^2)$ oracle calls to gradients of a monotone smooth twice differentiable DR-submodular function, subject to a polytope constraint ($n$ is the ground set size). Later, a conditional gradient method similar to the continuous greedy algorithm is shown to obtain a similar approximation factor after $O(n/\epsilon)$ oracle calls to gradients of monotone DR-submodular functions subject to a down-closed convex body \cite{bian2017guaranteed}. Such methods, however, require exact computation of the gradient of the function, which is not provided in the stochastic setting.

\vspace{-2mm}
\paragraph{Stochastic continuous submodular maximization.}
For stochastic continuous submodular maximization, conditional gradient methods %with a fixed batch size 
may lead to arbitrarily poor solutions, due to the high noise variance \cite{Hassani2017gradient}.
While the noise variance can be reduced by averaging the gradient over a (large) mini-batch of samples at each iteration, averaging considerably increases the computational complexity of each iteration and becomes prohibitive in many applications.
To address this, stochastic proximal gradient method are first proposed. In particular, \cite{Hassani2017gradient}
showed that when the expected function is monotone and DR-submodular, Projected Gradient Ascent (PGA) provides a $OPT/2-\epsilon$ guarantee in $\mathcal{O}(1/\epsilon^2)$ iterations.
Later, \cite{mokhtari2018conditional} introduced Stochastic Continuous Greedy (SCG), which reduces the noise of gradient approximations via exponential averaging and achieves a $(1-1/e)OPT-\epsilon$ guarantee in expectation after $\mathcal{O}(1/\epsilon^3)$ iterations. 
More recently, Stochastic Continuous Greedy++ (SCG++)
improved the complexity of SCG to $\mathcal{O}(1/\epsilon^2)$, by using a stochastic path-integrated differential estimator (SPIDER) \cite{fang2018spider} to reduce the variance of the stochastic gradient.
%%%%%%%%%%%
Most recently, boosted PGA algorithm using a non-oblivious function is proposed \cite{zhang2022stochastic}, which also achieves a $[(1-1/e)OP T -\epsilon]$ approximation guarantee in $\mathcal{O}(1/\epsilon^2)$ iterations.
Existing works, however, only provide guarantees in expectation and cannot deliver any insight on the  distribution of the solutions or worst-case analysis. \looseness=-1



\vspace{-2mm}
\paragraph{High-probability bounds for stochastic submodular minimization.}\!\!\!\!
Very recently, \cite{zhang2021stochastic} studied an extension of the stochastic submodular \emph{minimization} problem, namely,
the stochastic $L^\natural$-convex \cite{murota1998discrete} minimization problem. $L^\natural$-convex functions are reduced to submodular functions when the ground set size is 2.
Specifically, \cite{zhang2021stochastic} developed a polynomial time algorithm that returns a near-optimal solution with a high probability. The proposed method relies on the Lov√°sz extension of an $L^\natural$-convex function to transform the original problem to an equivalent continuous convex optimization problem, and applies the stochastic subgradient method to solve the continuous convex problem. 
To the best of our knowledge, high-probability bounds for continuous submodular \emph{maximization} have not been explored before.