\section{BACKGROUND AND PROBLEM FORMULATION}
\paragraph{Continuous submodular functions.}
We start by reviewing definition of submodularity for set functions.
A set function $f: 2^V \rightarrow \R_+$, defined on the ground set $V$ , is submodular if for all subsets $A, B \subseteq V$, we have that
\begin{equation}
f(A) + f(B) \geq f(A \cup B) + f(A \cap B). 
\end{equation}
% The notion of 
The notion of submodularity can be extended to continuous domains. 
% Continuous submodular functions are defined on subsets of
A continuous function $F: \X \rightarrow \R_+$ defined on the set $\X = \Pi_{i=1}^n \X_i$, where each $\X_i$ is a compact subset of $\R_+$, is continuous submodular if for all $\x, \y \in \X$ we have 
\begin{equation}
F(\x) + F(\y) \geq F(\x \vee \y) + F(\x \wedge \y).
\end{equation}
Here, $\x \vee \y := \max(\x, \y)$ is component-wise maximum and $\x \wedge \y := \min(\x, \y)$ is component-wise minimum operations. 
%
A submodular function $F$ is monotone on $\X$, if for every $\x, \y \in \X, \x \leq \y$ we have that $F(\x) \leq F(\y)$.
%
A function $F$ defined
over $\X$ satisfies the diminishing returns (DR) property, if for every $\x, \y \in \X, \x \leq \y$, and 
any standard basis
vector $e_i \in \R^n$ and any
$k \in \R_+$ s.t. $(ke_i + \x)\in\X$ and $(ke_i + \x)\in\X$, it holds that
\begin{equation}
    f(ke_i + \x) - f(\x) \geq f(ke_i + \y) - f(\y).
\end{equation}
When $F$ is twice-differentiable, 
DR-submodularity implies that all diagonal entries of the Hessian are non-positive \cite{bian2017guaranteed}. I.e.,\looseness=-1
\begin{equation}\label{eq:dr-hessian}
\forall i= j, \quad \forall \x \in\X  \quad \frac{\partial^2 F(\x)}{\partial x_i \partial x_j}\leq 0.
\end{equation}

\paragraph{Stochastic continuous submodular maximization.} 
In this work, we focus on constrained maximization of stochastic continuous DR-submodular functions. 
Formally, our goal is to find $\x^*$ that maximizes the expected value $F(\x)$ of the stochastic function $\Fn(\x,\z)$ over $\x$, where the expectation is with respect to the random variable $\Zb$: 
\begin{equation}
    \max_{\x\in\C} F(\x):= \max_{\x\in\C} \mathbb{E}_{\z\sim P}[\Fn(\x,\z)],
\end{equation}
where $\C \!\subseteq \!\R_+$ is a convex compact set, and $\z$ is the realization of the random variable $\Zb$ drawn from a distribution $P$. 
We assume that the expected objective function $F(\x)$ is monotone and DR-submodular and the stochastic functions $\Fn(\x, \z)$ may not be monotone nor submodular.
We denote by $OPT \!\triangleq \max_{x\in \mathcal{C}} F(\x)$ the optimal value of $F(\x)$ over $\C$. \looseness=-1 %and $\x^*$ as the global optimum. 
% To simplify notation we also define our noisy gradient estimate at step $t$ as $\g_t = \nabla \tilde{F}(\x_t,\z_t)$.

% \ba{we focus on monotone}