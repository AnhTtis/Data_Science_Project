
\section{Algorithms}

% \label{sec:algorithms}
\textbf{Centralized data sharing}
In Centralized data sharing (CDS), data is stored in a central location and can be accessed by all clients. This is in contrast to federated and decentralized data sharing methods, where data is stored in multiple locations and accessed by a single user or limited numbers of users. We use CDS as a  baseline  for comparison with other algorithms. 

\textbf{Federated averaging}: The learning procedure for federated averaging is an iterative process containing local and global steps. Each data owner trains a model received from a global server on its local dataset in local iterations \cite{darzidehkalanifederatedI}. The global server updates the global model by aggregating the updated local models. Then it sends it back to clients for the next round.  The optimization problem for federated averaging can be formulated as
\begin{equation}
w^{t+1} = \sum\limits_{i=1}^{N}{p_{i} w_{i}^{t}} , w_{i}^{t}=\arg\min\limits_{w_{i}}{\left(\mathcal{L}(\mathcal{D}_{i};w^{t})\right)}
\end{equation}
where $N$ is the number of data owners, $\mathcal{L}(\mathcal{D}_{i};w^{t})$ is a loss function indicating global model parameters  $w^{t}$ of local datasets, and $p_{i}$ is the probability of selecting client $i$. 
Local optimization can be formulated as $w_{i}^{t+1} \leftarrow w^{t}-\eta\cdot \nabla \mathcal{L}(w^{t};\mathcal{D}_{i})$, where 
 $\eta$ is the learning rate. The global model can be updated based on the local models $w_{i}$ and is shared for aggregation: 
\begin{equation}
w^{t+1} = \sum\limits_{i=1}^{N}{p_{i} w_{i}^{t+1}}
\end{equation}

\textbf{Federated stochastic gradient descent}:Federated Stochastic Gradient Descent (FedSGD) is a variation of Federated Averaging (FedAvg) that uses a large-batch synchronous approach to multi-client learning. FedSGD utilizes a subset of clients from the total number of clients, where $C$ defines the subset of selected clients. This subset of clients is selected at each global round, and the global server sends the most recent global model to them. Each client then performs local training over its dataset for a select number of epochs. The global model is updated based on the local models received from each client and is shared for aggregation, similar to FedAvg. However, in FedSGD, the gradient is computed over the selected batch of clients and therefore, $C<1$, for $C=1$ the training would be non-stochastic (full batch) since all the clients are involved. This allows for training with large batches, as the gradient is computed over the selected subset of clients.   The optimization problem for FedSGD can be formulated as
\begin{equation}
w^{t+1} = w^{t} - \eta \cdot \sum\limits_{i=1}^{C}p_i \cdot \nabla \mathcal{L} (w^t;\mathcal{D}_i)
\end{equation}
where $\eta$ is the learning rate, $p_i$ is the probability of selecting client $i$ and $\mathcal{L}$ is the loss function.
The key difference between FedAvg and FedSGD lies in the use of large-batch synchronous approach in FedSGD. This approach has been shown to outperform the naive asynchronous SGD training due to the increased accuracy and efficiency, as compared to the local training approach used in FedAvg \cite{chai2020fedeval}\cite{charles2021large}. Additionally, FedSGD has been shown to be more robust to non-IID data distributions, compared to FedAvg \cite{chai2020fedeval}. 
\quickthings{This part is unclear to me.}
% \absolutelynecessary{}
\maybelater{paraphrised}

\textbf{Cyclic weight transfer}: Federated learning techniques have been widely used in medical image processing tasks using a method known as cyclic weight transfer (CWT)\cite{balachandar2020accounting}. This method involves training models on individual clients for a number of iterations and then cyclically sharing the updated weights with the following client. However, the existing CWT algorithm faces a notable challenge, as it lacks the ability to effectively manage inter-client variability in training data or labels.  To ensure the practical application of CWT, it is crucial to develop a version that can handle the common variations observed in a majority of real-world medical imaging datasets.\cite{darzidehkalanifederatedII}

\textbf{Single weight transfer}: Single weight transfer (SWT) is another FL model widely used in the medical imaging domain. In Single weight transfer, models are trained in each client with its local data, and then the updated model is transferred to the next client. The difference between this method and CWT is that here the model passes each client only once. 

\textbf{Stochastic weight transfer}:
In stochastic weight transfer (STWT), we select a subsample of clients and train them in a cyclic manner. Similar to FedSGD, a ratio defines the number of selected clients to the total number of clients in each federated round.



% سوال / مسیله چیه
% Importance: Why your research matters in the context of an industry or the world
% اهمیت موضوع
% 	کجاها کاربرد داره
% مشکلات موضوعات قبلی و اینکه مسایل دگ کار نمیکنن
% برای اینکه این مسایل کار کنن نیازی به چه چیزی بود. ک اونا کم داشتن
% چرا این روش تو داره کار میکنه
% چه سوالی رو جواب میده
% توضیح اینکه چجوری کار میکنه
% تعریف و تمجید از کار کردنش

% \maybelater{chanta akse tamiz ba coreldraw bekesh}

% Their
% We are comparing various implementations of FL and comparing their performance level. The performance is done by metrics.
% 2. Although FL has been introduced to tackle the problem of privacy, its distributed nature requires too much commuincation between clients. Knowsing can determine its deployability and scailibitlty. Hence the question is how to these model compare it terms of communication between clients.
% 3. Computation time. Each FL model is investigated based on the required time for computation.
% 4. Effect of rounds: Number of Federated rounds 


% \quickthings{Maybe you can add a benchmarking section similar to "A Performance Evaluation of FL Algorithms"}
% \maybelater{Some ideas are herer https://arxiv.org/pdf/1709.05929.pdf
% }

