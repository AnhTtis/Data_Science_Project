

\section{Experiment}
\textbf{Dataset} Our experiments used two publicly available data sources, the Tongji hospital dataset and Brazil's SARS-CoV-2 dataset. Tongji dataset consists of 349 chest CT-scans of COVID-19 positive and 397 scans of healthy subjects, all low-resolution CT modalities. Brazil's SARS-CoV-2 dataset consists of 2482 samples, 1252 scans of COVID-19-infected patients, and 1230 healthy subjects collected from multiple hospitals in Sao Paulo, Brazil. Train and test sets were obtained randomly from the aggregated datasets. Table \ref{table_datadist}, shows data distribution.


\begin{table}[h!]
\centering
\setlength{\tabcolsep}{6pt}
\renewcommand\arraystretch{1.22}
\caption{ \small Data distribution}
\begin{tabular}{| *{5}{c|} }
\hline
Data  & Class & Dataset & Train & Test
\\   \hline  
\multirow{2}{3em}{Covid}     &Brazil&1252&\multirow{2}{2em}{1451}  &\multirow{2}{2em}{150}  \\
&Tongji&349&&  \\
\hline
\multirow{2}{5em}{Non-Covid}     &Brazil&1230&\multirow{2}{2em}{1477}  &\multirow{2}{2em}{150}  \\
&Tongji&397&&  \\
\hline
\end{tabular}
\label{table_datadist} 
\end{table}


\textbf{Preprocessing}
Images were selected as 2D slices in greyscale. Preprocessing included randomly cropping between 0.5 to full size, random horizontal flipping, and intensity normalization. CT-slices were all resized to 224$\times$224 pixels with interpolation. Figure \ref{fig:samplesCTs} shows samples of processed images.
% \quickthings{This looks like you have a bias in your dataset. You should select the normals and abnormals at comparable level in the thorax. All covid are now lower in the body then the non-covid ones.}
% \maybelater{I have changed the picture, hope it's okay now}
\begin{figure}[h!]
 \centering
 \includegraphics[width=0.45\textwidth]{photo/covidexamples3.png}
 \caption{Sample CT slices of Covid-19 images (top row) and Non-covid images (bottom row)}
 \label{fig:samplesCTs}
\end{figure}


% normalize = transforms.Normalize(mean=[0,0,0], std=[1,1,1])
% train_transformer = transforms.Compose([
%     transforms.Resize(256),
%     transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),
%     transforms.RandomHorizontalFlip(),
%     transforms.ToTensor(),
%     normalize
% ])

\textbf{Training}
ResNet-18 is used as the backbone deep learning model. ResNet-18 comprises one initial block cascaded to four middle blocks. The initial block is made of convolutional,  batch normalization, ReLU, and pooling layers. Middle blocks have the same layers, connected with straight and skip connections. The model is pre-trained on ImageNet dataset \cite{he2016deep} with a CrossEntropy loss function and learning rate of $0.05$. 
Each federated round consisted of 20 internal epochs for each client and batches of 16 samples in each iteration. For models which use minibatch training, like STWT and FedSGD, a subset of clients is randomly selected. Similar to training, test data was split into mini-batches, and the results were averaged across batches. We performed training with various participating clients and federated rounds to evaluate their effect on final performance. Models were also trained in a centralized, non-federated setting to build a comparison baseline.

\begin{figure}[h!]
 \centering
 \includegraphics[width=0.50\textwidth]{photo/output.png}
 \caption{Data distribution of each client in the simulated federated setting}
 \label{fig:pgd-atta-comparison}
\end{figure}
% \abs{Healthy COVID data distirbution per site}

\textbf{Evaluation} 
Standard classification metrics, accuracy, recall, precision, and F1 score, were used as our evaluation criteria. We also evaluated the level of communication, the amount of transferred data in each algorithm, and the computational complexity of the models. 

% These metrics will act as building blocks for Balanced Accuracy and F1-Score formulas.\\
% Starting from a two class confusion matrix: \medskip

%  \begin{figure}[ht!]
%     \centering
%     \includegraphics[scale=0.7]{ConfMatrix2b.PNG}
%     \caption{Two-class Confusion Matrix}
%     \label{fig:Two classes confusion matrix}
%  \end{figure}\medskip

% Precision metric shows the proportion of 


% defined by number of true positives divided by total positively predicted samples. In other words, Precision tells us how much we can trust the model when it predicts an individual as Positive.
%     \begin{equation}
%       Precision = \frac{TP}{TP+FP}
%     \end{equation}\medskip

% \medskip

% The Recall is the fraction of True Positive elements divided by the total number of positively classified units (row sum of the actual positives). In particular False Negative are the elements that have been labelled as negative by the model, but they are actually positive.

% \begin{equation}
%   Recall = \frac{TP}{TP+FN}
% \end{equation}

% The Recall measures the modelâ€™s predictive accuracy for the positive class: intuitively, it measures the ability of the model to find all the Positive units in the dataset.
% \medskip

% Hereafter, we present different metrics for the multi-class setting, outlining pros and cons, with the aim to provide guidance to make the best choice.

% Accuracy is one of the most popular metrics in multi-class classification and it is directly computed from the confusion matrix.
% \begin{equation}
% Accuracy=\frac{TP+TN}{TP+TN+FP+FN}
% \end{equation}
% \medskip

% The formula of the Accuracy considers the sum of True Positive and True Negative elements at the numerator and the sum of all the entries of the confusion matrix at the denominator. True Positives and True Negatives are the elements correctly classified by the model and they are on the main diagonal of the confusion matrix, while the denominator also considers all the elements out of the main diagonal that have been incorrectly classified by the model.\\ 
% In simple words, consider to choose a random unit and predict its class, Accuracy is the probability that the model prediction is correct.



% \maybelater{Intensity distribution per site}
% \maybelater{Training strategy}


\label{sec:experiment}
