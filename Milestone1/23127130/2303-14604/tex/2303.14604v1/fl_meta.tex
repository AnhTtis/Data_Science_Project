\section{Industry-Scale Federated Learning}

\subsection{Federated Learning Platform}
 Our company's production FL stack is built based on \papaya~\cite{papaya}, a recently proposed system for running federated learning and analytics tasks across millions of user devices. \papaya comprises two major subsystems: a server application that runs on a data center server and a client application that runs on end-user devices. In this study, we set out to measure the energy consumption and carbon footprint of both client-side and server-side resources used during training of a model in the federated learning system \papaya.
 
 The overall architecture of \papaya is presented in Figure~\ref{fig:FL-platform}. The  \papaya server has three main components: Coordinator, Selector, and Aggregator. There is one Coordinator, and the number of Selectors and Aggregators scales elastically based on the workload demand. The Coordinator assigns FL tasks to Aggregators based on load and assigns clients to tasks based on demand. 
Selectors report available clients and route clients to their assigned aggregator. 
Aggregators execute the client protocol, aggregate updates, and optimize the FL model. Regarding energy and carbon footprint, Aggregators and Selectors are responsible for the majority of processing and heavy lifting. The Coordinator is responsible for assigning FL tasks to Aggregators and clients to FL tasks, and centralized coordination.

\begin{figure}[t]
\centering
\includegraphics[width=0.65\columnwidth]{figures/system-peerreview.png}
%\vspace{-0.25cm}
\caption{Overall architecture of our production FL stack, based on \papaya \cite{papaya}.}
\label{fig:FL-platform}
\end{figure}

\paragraph{Concurrency vs.~aggregation goal.} A device must meet a defined set of criteria to participate in FL training. Eligible devices report their availability to the Coordinator, which subsequently selects a subset of available devices for training. \emph{Concurrency} is the maximum number of clients that can train simultaneously. The \emph{aggregation goal} denotes the minimum number of client responses that must be received at the server before it updates the model.

In \emph{asynchronous} FL, based on the FedBuff protocol~\cite{fedbuff}, a new device is immediately selected for training as soon as the server receives a client response. Therefore, the number of devices training at any given time essentially equals to the concurrency. Once the aggregation goal is met, the server model is updated, and clients selected thereafter receive the updated model. However, clients chosen earlier may still be training using the previous version of the server model, leading to a phenomenon called \emph{staleness}~\cite{fedbuff}.

In contrast, \emph{synchronous} FL~\cite{mcmahan2017learning} proceeds in discrete rounds. At the beginning of a round, the server distributes the same model to a number of devices equal to the concurrency. At the end of the round, the server updates its model if it has received updates from at least as many users as the aggregation goal; it is worth noting that users may drop out during the round due to various reasons (such as the device no longer being idle or connected to Wi-Fi). In synchronous FL, the concurrency is also referred to as \emph{users per round}, and it is usually greater than the aggregation goal (a process called ``over-selection'') to account for the possibility of devices dropping out mid-round~\cite{bonawitz2019towards}.

%  It is referred to as concurrency for better realization of asynchronous FL, since technically asynchronous FL does not have the concept of rounds. 

% Aggregation goal is the number of users whose updates are aggregated for producing a new server model update in each update phase. In asynchronous FL based on FedBuff \cite{fedbuff}, aggregation goal is essentially the buffer size of the model update at server, while in synchronous FL, aggregation goal realizes the number of users whose update are aggregated in each round. For instance, in synchronous FL with over-selection, we select more users than the aggregation goal to mitigate the effect of stragglers \cite{bonawitz2019towards}. 


\subsection{Large-scale FL Task: Language Modeling}

In all experiments for this study, we train a character-aware language model for a next word prediction task, similar to Kim et al.~\cite{char_aware_lm}. This model computes the probability of a sequence of words $S = w_1,\dots,w_T$ autoregressively as:
\begin{equation*}
p(S) =\prod_{i=1}^T{p(w_i|w_{<i})}.
\end{equation*}
%Where $h_i = w_{i-1}, w_{i-2},...w_{0}$. 
More specifically, we use a character-level CNN with multiple filters, followed by a pooling layer that computes the final word embeddings. These are then encoded using a standard LSTM-based neural network that captures the sequential information in the input sequence. Finally, we use an MLP decoder followed by a \texttt{softmax} layer that converts the word-level outputs into final word-level probabilities over a fixed vocabulary. Using the notation where
\begin{itemize}
\item $i$ denotes the length of the sequence seen so far,
\item $x_{i,1}, x_{i,2},\dots, x_{i,L_i}$ are the characters of the $i$-th word in the input sequence,
\item $L_i$ is the length of the $i$-th word,
\item $e_{i}$ is the embedding for the $i$-th word,
\item $h_{i}$ is the hidden state,
\item $c_{i}$ is the state of the LSTM for the $i$-th word,
\item $W$ is the weight matrix,
\item $p(w_{i+1}|w_{\leq i})$ is the probability of the next word in the sequence computed using the MLP decoder and softmax layer,
\end{itemize}
then the model can be expressed as follows:
\begin{equation*}
\begin{aligned}
&e_i = \text{CNN}(x_{i,1}, x_{i,2},\dots, x_{i,L_i}) \\
&c_i, h_i = \text{LSTM}(h_{i-1}, c_{i-1}, e_i) \\
&p(w_{i+1}|w_{\leq i}) = \text{Softmax}(W^Th_i) \\
&\text{Perplexity}(w_0,w_1,\dots,w_i) = \left(\prod_{j=0}^{i-1} p(w_{j+1}|w_{\leq j})\right)^{-1/i}.
\end{aligned}
\end{equation*}
Perplexity measures the degree of uncertainty of a language model when it generates a new token averaged over sequence lengths. Formally, perplexity is defined as the normalized inverse probability of sequences.

In our experiments, the available client pool is in the order of tens of millions of end-user devices. Roughly 2 million devices (Android smartphones) are selected to participate in each experiment. %Our conclusions are generalizable to FL systems with other platforms.

Instead of using the users' data for mobile keyboard predictions, which could raise privacy concerns, we use publicly available, representative data downloaded to the user devices before training. We used \verb|pushift.io|’s Reddit FL benchmark dataset \cite{reddit_ds} in all experiments\footnote{Meta was not involved in the collection of data from Reddit.}. This dataset is publicly available, previously collected, and currently hosted by \verb|pushift.io|, consisting of user comments on \verb|reddit.com|. Thus this dataset has a natural non-IID partitioning and is representative of a real-world data distribution for mobile keyboard predictions. It also exhibits the archetypal power-law phenomenon of the number of comments per user. The dataset comprises millions of users, with an average of 34 samples per user. Each device participating in the FL is randomly assigned an anonymized user id from the \verb|pushift.io|’s Reddit dataset to use as their training data.

\paragraph{Stopping Criteria.}
We run an FL experiment until either the language model reaches a target perplexity on a hold-out test set, or a maximum time limit of 2 days is reached.
We set the target perplexity to be 175 or lower for our tasks and stop the task when the perplexity is at or lower than the target for five consecutive rounds. 

Due to the large number of experiments in this study, we set the target perplexity higher and the time limit shorter than those of the typical production models. The carbon emissions of the at-scale production models of the same task are expected to be roughly $10\times$ higher than the numbers reported in this study.


\subsection{Experiment Parameters}
We explored different settings of hyperparameters, separately optimizing for model performance, time to reach target accuracy, and carbon emission. We discuss these choices next.

For the optimizer running on the clients, we use SGD with no momentum. Alternatives (e.g., Adam) require additional on-device memory for the optimizer state (i.e., momentum buffers). Another important consideration is that in scenarios where clients possess limited data (which is often the case), they may not execute sufficient local steps to leverage the benefits of the momentum buffers. In such cases, the momentum-based optimization techniques may not be as effective. 

For the server optimizer, we wanted to be as general as possible, and we chose Adam for the server updates~\cite{adam}. Adam is more general than SGD or SGD with Momentum, and the parameters of Adam can be chosen to essentially replicate the performance of SGD or SGD with Momentum \cite{choi2019empirical}. On the server side in FL, we do not see any evidence that using a more compute-intensive optimizer has any significant impact on carbon emissions, although it should help the other dimensions --- reduce time to reach a target accuracy and improve overall accuracy. Our setup, the server updating the global model using the Adam optimizer and the clients using SGD, is called FedAdam~\cite{adam}.

% of all the compute involved in a training step, the optimizer step is usually a tiny fraction. So the difference in compute between SGD and Adam or momentum may not be all that big. The big compute effort is usually in the forward-backward calculations. However, the more significant difference with Adam and momentum is the additional memory requirement (and like we already discussed, needed enough data to take enough steps to really make these methods useful on-device)
 
We carefully evaluated hyperparameters for all applicable settings. We experimented with synchronous FL and asynchronous FL. For synchronous FL, the baseline implementation is FedAvg, whereas for asynchronous FL, it is FedBuff \cite{fedbuff}. However, since we use Adam as the server optimizer, both synchronous and asynchronous FL get the benefits of adaptive optimizers and perform better than their baselines. The hyperparameters are listed in Table~\ref{hyperparam_table}.

\begin{table}[t]
\small
\caption{Hyperparameters and their values explored in the experiments. Aggregation goal is expressed here as a percentage of concurrency.}
\begin{tabular}{@{}ll@{}}
\toprule
Hyperparameter       & Values                                         \\ \midrule
server learning rate & 0.0001, 0.001, 0.005, 0.01, 0.1, 1                    \\
client learning rate & 0.0001, 0.001, 0.01, 0.1, 0.5, 1                    \\
local epoch         & 1, 3, 5, 10, 15, 20                                \\
batch size           & 8, 16, 32                                      \\
Adam $\beta_1$           & 0.1, 0.5, 0.7, 0.9                             \\
Adam $\beta_2$           & 0.9, 0.99, 0.999                               \\
concurrency & 50, 100, 200, 300, 800, 1000, 1300, 1500             \\
aggregation goal     & 8\%, 10\%, 25\%, 50\%, 65\%, 77\%, 80\%, 85\%, 100\% \\
\bottomrule
\end{tabular}

\label{hyperparam_table}
\end{table}

