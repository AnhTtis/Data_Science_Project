\section{Introduction}
\label{sec:introduction}

% Fueled by the ever-increasing amount of data and model parameters, artificial intelligence (AI) has seen significant learning capability improvement in the past decade. 

\emph{Federated learning} (FL) is a distributed learning paradigm where a large number of client devices, such as smartphones, collectively train a machine learning model using data located on client devices.
%
User data remains on client devices, and only updates to the model are aggregated within a centralized model at the server.
FL has emerged as a practical privacy-enhancing technology for on-device learning~\cite{fl_open_problems}.
%
Many real-world models have been trained using FL, including language models for predictive keyboards on Google Pixel, Apple's iOS, and Meta's Quest~\cite{google_fl,apple-fl, apple_fl_eval_p13n,oculus_fl}, Siri personalization~\cite{apple-fl-siri}, advertising, messaging, and search on LinkedIn~\cite{linkedin-fl}. 
 
% [Apple] Federated Evaluation and Tuning for On-Device Personalization: System Design & Applications (https://arxiv.org/abs/2102.08503)

While FL --- when coupled with technologies such as secure aggregation and differential privacy \cite{fl_open_problems, bonawitz2016practical, fedbuff, mcmahan2017learning} --- can be a practical solution to enhance user privacy, the training process in FL can result in non-negligible carbon emissions.
%
A recent study has shown that training a model with FL can produce as much as 80 kilograms of carbon dioxide equivalent (\carbon), exceeding that of training a higher capacity model, a large transformer, in the centralized training setting using AI accelerators~\cite{wu2022sustainable}. The relative inefficiency is attributable to several factors, including the overhead of training using a large collection of highly heterogeneous client hardware, additional cost for communication, and often slower convergence. 

Federated Learning's global carbon footprint is expected to increase as the industry increasingly adopts FL and more machine learning tasks shift away from the centralized setting. 
%And as the progress of AI is fueled by  large and computationally intensive AI models and datasets, the amount of compute used in FL training is expected to grow too. 
This is especially concerning since renewable sources of electricity may not be available in all locations, making Green FL a challenging goal to achieve~\cite{wu2022sustainable, fl_carbon}. \textit{Taking advantage of opportunities for efficiency optimization in FL is of paramount importance to make on-device learning greener.}

Recently, there has been growing interest in quantifying and reducing the carbon emissions of machine learning (ML) training and inference in the datacenter setting~\cite{ml_carbon_dodge, ml_carbon_strubell, ml_carbon_patterson,  lacoste2019quantifying, naidu2021towards}. However, the carbon footprint of Federated Learning (FL) and the factors that contribute to carbon efficiency in FL have yet to be thoroughly explored. Prior works have offer preliminary findings, either quantified the carbon effects of FL only in a simulation setting or with several simplifying assumptions~\cite{fl_carbon, wu2022sustainable}, offering only a partial picture.
%
These works focused on measurements and opportunity sizing, and restrained from exploring dimensions of the design space toward realizing Green FL.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/predict-sync.pdf}
\vspace{-0.25cm}
\caption{Carbon emissions of (synchronous) FL: the more rounds is required to reach a target accuracy and the higher the number of users active in training (i.e., \textit{concurrency}), the higher is the carbon emissions. Each point represents a training run with a different hyper-parameter (grouped by concurrency with marker colors and symbols). The graph shows the carbon emissions (Y axis) and the rounds to reach a target accuracy (X axis) for a language modeling FL task.}
\label{fig:CO2-FL}
\end{figure}

This paper presents a holistic carbon footprint analysis of a production Federated Learning (FL) system that operates on hundreds of millions of clients training on a real-world task. This is the first study that provides a comprehensive view of Green FL by characterizing the emission profile of all major components involved, including the emissions from clients, the server, and the communication channels in between. To this end, we instrument and profile all major components of the FL system.

An important finding of our analysis is: \textbf{the carbon footprint of an FL task is highly correlated with the product of its running time and the number of users active in training (i.e., \textit{concurrency})}. We discuss this in more detail in Section~\ref{sec:impact}. We also provide an in-depth analysis of the multi-criterion optimization between carbon emissions, time to convergence, and training error.

Figure~\ref{fig:CO2-FL} presents results of measuring a production FL task for a range of hyperparameters. We can see that the number of rounds and concurrency are both positively correlated with the carbon footprint, and keeping one of these parameters constant, the relationship is nearly linear (see, for instance, the line corresponding to concurrency set to 200). These points become more evident throughout the paper. Other findings are the following:
\begin{itemize}
\item Compute on client devices, and the communication between the clients and the server are responsible for the majority of FL's overall carbon emissions (97\%). The carbon footprint attributable to the server-side computation is small ($\sim$1--2\%), while client computation is almost half of the contribution ($\sim$46--50\%). Upload and download networking costs are approximately 27--29\% and 22--24\%, respectively.
\item Asynchronous FL is faster than synchronous FL as it advances the model more frequently in the face of stragglers, but it comes at the cost of higher carbon emissions.
\item Different training configurations that achieve similar model accuracy can have substantially different carbon impact, by up to 200$\times$, demonstrating the importance of hyper-parameter optimization.
\item To minimize the carbon footprint of FL, reduce training time, and achieve a high model quality, FL developers must focus on lowering the training time, e.g., through the right choice of the optimizer, learning rates, and batch sizes, while keeping the concurrency small.
\item Carbon footprint of a language modeling FL task running for several days at scale is of the order of 5--20 kg \carbon, similar to that of producing 1 kilogram of chicken \cite{food}.
\end{itemize}

\subsection{Contributions}
To the best of our knowledge, this is the first study to measure carbon emissions at scale for an industrial FL system across a range of hyperparameters. Our findings can help identify challenges and encourage further research towards the development of more sustainable and environmentally friendly FL systems.
%
Our main contributions can be summarized as follows:
\begin{itemize}
    \item We present a comprehensive evaluation of the carbon emission of a full production FL system stack by presenting the emission profile of all major components involved, including the emissions from clients, the server, and the communication channels in between. No prior work has done a carbon measurement study on a real-world FL production system at scale.
    \item Our empirical observations lead us to propose a set of key findings for Green FL, which identify the levers that have the most significant influence on the carbon footprint of~FL. 
    \item We propose a model that predicts the carbon footprint of an FL task prior to actual deployment.
    \item We show that using our recipe for Green FL, we can reduce the carbon footprint of FL training pipelines by as much as 200$\times$ while achieving similar model quality performance.
\end{itemize}






