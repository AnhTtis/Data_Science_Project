
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{multirow}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
% \usepackage{caption}
% \usepackage{subcaption}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{wrapfig}

\usepackage{hyperref}
\usepackage{url}


\title{Learn, Unlearn and Relearn: \\An Online Learning Paradigm for Deep Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Vijaya Raghavan T. Ramkumar\textsuperscript{\rm 1}, Elahe Arani\textsuperscript{$\ddagger$\rm 1,2}, Bahram Zonooz\textsuperscript{$\ddagger$\rm 1,2} \\ 
        \email vijaya.ramkumar@navinfo.eu, \{e.arani, bahram.zonooz\}@gmail.com \\ 
        \addr \textsuperscript{\rm 1}Advanced Research Lab, NavInfo Europe, Eindhoven, The Netherlands \\
        \addr \textsuperscript{\rm 2}Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands \\
        \textsuperscript{$\ddagger$}\textrm{Contributed equally.}}
      

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.
% \newcommand{\fix}{\marginpar{FIX}}
% \newcommand{\new}{\marginpar{NEW}}

\def\month{01}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=WN1O2MJDST}} % Insert correct link to OpenReview for camera-ready version


\begin{document}

\maketitle

\begin{abstract}
Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce \textit{Learn, Unlearn, and Relearn (LURE)} an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalizable features. We show that our training paradigm provides consistent performance gains across datasets in both classification and few-shot settings. We further show that it leads to more robust and well-calibrated models.\footnote{The official code is available at: \url{https://github.com/NeurAI-Lab/LURE}}
\end{abstract}

\section{Introduction}

% \begin{quote}
  \centerline{``\textit{A little learning is a dangerous thing.}'' {-Alexander Pope} }
%   \begin{right}
%   {-Alexander pope}
%  \end{right}
%  \end{quote}


In recent years, supervised learning has achieved human-level performance in many computer vision tasks in which the learner is trained in an offline learning environment with a fixed set of training data. However, DNNs deployed in the real world are expected to work in an environment where the data arrive in a sequence of large chunks (mega-batches). Several learning paradigms have been proposed to learn from a stream of data, including, but not limited to, continual learning \citep{van2019three, thrun1995learning}, active online learning \citep{settles2009active}, and anytime learning \citep{grefenstette1992approach, pmlr-v199-caccia22a}. Although previous efforts established a solid theoretical foundation, certain subtle issues make it inapplicable to the practical environment.

For an online learning system, it is important that the learner produces high accuracy and generalizes well at any point in time while using limited computational resources \citep{pmlr-v199-caccia22a}. Recent research in online learning, however, has shown that training from a previously trained model (warm-start rather than fresh initialization) hinders its ability to adapt to new input \citep{achille2018critical}, thus incapacitating the generalization of DNNs \citep{ash2020warm, pmlr-v199-caccia22a}. These implications of warm-starting have also been observed in online active learning \citep{Huang2021Deepal, sener2017active, ash2019deep}, where they mitigate it by retraining from scratch after every selection. However, training DNNs from scratch every time new data arrives is resource intensive, and the lack of generalization with warm-starting undermines the benefits of training with learned features \citep{ash2020warm}. Thus, the failure of current online learning systems to generalize across data streams without bartering previous computation presents a striking lacuna for the large-scale deployment of machine learning systems.

Humans, on the other hand, learn in succession over their lifespan and readily generalize by applying prior knowledge to novel situations and stimuli without the need to learn from scratch. This in the brain is facilitated by a complex set of neurophysiological processes \citep{goyal2020inductive}. One of such glaring aspects of the brain that allows humans to generalize better is the inherent process of active forgetting \citep{hardt2013decay, davis2017biology}. It plays an active role in the regulation of the learning process to achieve better generalizability in the real world. The growing evidence in neuroscience and cognitive psychology \citep{gravitz2019forgotten, izawa2019rem} suggests that the brain actively forgets through the selective extinction of neurons, which shapes the learning-memory process and, therefore, prevents humans from overfitting to experiences \citep{shuai2010forgetting}. Thus, emulating this aspect of selective forgetting might hold the key to improving generalization in DNNs.  

Therefore, we propose a general online learning paradigm, which we refer to as \textit{Learn, Unlearn, and RElearn (LURE)}, to address the problem of generalization of parameterized networks. For simplicity, we focus mainly on an online learning scenario in which models achieve good performance at any point in time, termed Anytime Learning \citep{pmlr-v199-caccia22a}. We consciously simulate the process of selective forgetting (unlearning) in the DNNs by re-randomizing a subset of weights before training on the new samples. With extensive experiments on multiple datasets, we show that our proposed training paradigm boosts the performance and generalization of the models to a greater extent. Compared to standard online training, LURE significantly improves the robustness of DNNs in tackling more challenging real-world scenarios.
Our main contributions are as follows:
\begin{itemize}
\setlength{\itemindent}{-0.2em}
\itemsep0em
    \item "Learn, Unlearn, and Relearn" (LURE), an online training paradigm to improve the performance and generalization of DNNs through the lens of selective forgetting.
    \item We demonstrate the efficacy of LURE in multiple convolutional architectures across different datasets in online learning and a few-shot classification scenario.
    \item LURE exhibits robustness in solving more common challenges in real-world problems, including learning with noisy labels, natural corruption, and adversarial attacks.
    \item LURE is robust to changes in hyperparameters and leads to well-calibrated models.
  \end{itemize}

%Finally, our training paradigm leads to well-calibrated models that are less susceptible to changes in hyperparameters.
\section{Related Work}

% Computational systems operating in the real world are exposed to a continuous stream of data that often arrives in chunks over time. \citet{pmlr-v199-caccia22a} describes an online learning system as a learner that produces high accuracy and generalizes well at any point in time while using limited computational resources. 
Lifelong learning \citep{thrun1995learning} and anytime learning \citep{grefenstette1992approach} have gained increasing attention from the deep learning community due to its relevance in practical settings. Recent research on online learning, \citep{pmlr-v199-caccia22a, ash2020warm}, has noticed a lack of generalization in DNNs when trained in online settings. \citet{ash2020warm} points out that if a model is finetuned from a pre-trained model (a "warm-start"), the resulting new model performs worse than a model trained from scratch (a "cold-start") even though the new data is sampled from the same distribution as the previously trained data. Thus, the lack of generalization in DNN renders them inapplicable to real-world scenarios.

Recently, several weight reinitialization methods \citep{taha2021knowledge, li2020rifle, alabdulmohsin2021impact, ash2020warm, zhou2022fortuitous} have been proposed to improve the generalization performance of DNNs by partially or fully refining the learned solution. \citet{zhou2022fortuitous} propose a forget and relearn hypothesis to unify disparate existing iterative algorithms under the lens of forgetting. Their approach is based on the consideration that early layers learn generalized representation, whereas later layers memorize. Therefore, they reinitialize and retrain the later layers of the model repeatedly, thereby erasing the information pertaining to the memorized difficult examples. Similarly, \citet{ash2020warm} propose a method to improve generalization by shrinking the magnitude of the weights and perturbing it by injecting small noise.   
However, these weight reinitialization methods have architecture-specific assumptions independent of the data and are handled based on the assumed properties that are inherent to the model and its learning. These methods lack a priori knowledge of where and what features, layers, etc. should be reinitialized in the general case. Therefore, we propose a online training paradigm, to improve the generalization of DNNs through the lens of active forgetting. 
%that selectively forgets extraneous information

\section{Method}
We propose \textit{"Learn, Unlearn, and Relearn" (LURE)}, a training paradigm for learning from a sequence of data, which alternately interchanges the unlearning (selective forgetting) and relearning steps. Our proposed online training paradigm consists of three steps: a) learn, b) unlearn, and c) relearn. Our proposed approach is illustrated in Figure~\ref{fig:method} and is detailed in the Appendix (Algorithm~\ref{alg:method}).
  
\textbf{Learn.} 
We define the Anytime Learning at Macroscale (ALMA) learning environment as envisioned in \citet{pmlr-v199-caccia22a} where the authors focus on real-world settings. ALMA is a new sub-paradigm of learning from sequential data inspired by anytime learning \citep{grefenstette1992approach} and transfer learning \citep{pan2009survey}. Data is provided to the learner in the form of a stream $S B$ consisting of $t$ consecutive batches of samples. Therefore, we also focus on the general classification problem, where data are sampled from an underlying data distribution $\mathcal{D}_{x,y}$ with input $x \in \mathbb{R}^\mathcal{D}$ and label $y \in \{1, ..., C\}$. 
% Similar to ALMA \citep{pmlr-v199-caccia22a}, we focus on the general problem of classification. ALMA focus on the real-world settings where the data is delivered to the learner in the form of a stream $S_B$ comprising $t$ consecutive batches of samples. 

Let $\mathcal{M}_i$ be a collection of $N \gg 0$ in-distribution samples randomly selected from $\mathcal{D}_{x,y}$, for $i \in \{1, ..., t\}$. The stream is then defined as the ordered sequence $S_B = \{\mathcal{M}_1, ..., \mathcal{M}_t\}$. We refer to each dataset $\mathcal{M}_i$ as a mega-batch, as it is composed of a large number of samples. Consider a model $f_\theta: \mathbb{R}^\mathcal{D} \rightarrow \{1, ..., C\}$ updates its parameters by processing a mini-batch of $n \ll N$ examples at the time of each mega-batch $\mathcal{M}_i$ in such a way as to minimize its objective function. Since the data are passed as a stream, the model does not have access to the future mega-batches and is limited to one pass through the entire stream. However, the model might make several passes over the current and some previous mega-batches depending on the available computational budget. In ALMA, it is assumed that the rate at which mega-batches arrive is slower than the training time of the model on each mega-batch, and therefore the model can iterate over the mega-batches at its disposal based on its discretion to maximize performance, resulting in an overall data distribution that is not i.i.d. by the end of the stream. This implies a trade-off between effectively generalizing and learning from the current data at each mega-batch. Therefore, in such settings, we train the randomly initialized network $f_{\theta_{Reinit}}$ on a mega-batch $\mathcal{M}_t$ belonging to the data stream $\mathcal{D}_{x,y}$ for $e$ epochs until convergence. The loss function employed for learning is defined as follows: 
\begin{equation}\label{eq:ce}
{L}_{\mathcal{T}}=\sum_{i=1}^{t} \underset{\left(x, y\right) \sim \mathcal{M}_i}{\mathbb{E}}\left[\mathcal{L}_{c e}\left(\sigma\left(f_\theta\left(x\right)\right), y\right)\right],
\end{equation}
where $\mathcal{L}_{ce}$ is a cross-entropy loss, $t$ is the number of mega-batch sequences, and $\sigma$ is the softmax function.


%  Humans, on the other hand, learn sequentially throughout their lives and rapidly generalize by applying existing information to unfamiliar circumstances. Recent studies in neuroscience \citep{hardt2013decay, davis2017biology} partly attribute this to the fact that brains engage in the process of controlled forgetting. Due to the onset of large number of memory that can accumulate in the brain across time, \citet{gravitz2019forgotten} points out that brain engages in a active forgetting process which selectively removes the extraneous connections related to the irrelevant details of the event and thus enabling humans to acheive generalization. These neuroscience studies show substantial evidence that selective forgetting and generalization have a symbiotic relationship. 
 
% The human brain, on the other hand, has the remarkable capacity to acquire, store and recall information that allows humans to learn continuously and generalize better \citep{hardt2013decay}. \citet{davis2017biology} partially attributes the ability of humans to generalize new experiences to the phenomenon of active forgetting. Forgetting plays a key role in selectively regulating and rebalancing the learning memory process to prevent humans from overfitting to the experiences \citep{gravitz2019forgotten}. These neuroscience findings provide substantial evidence for the existence of a symbiotic link between generalization and active forgetting in biological neural networks that are lacking in DNNs. Therefore, we emulate this aspect of selective forgetting to improve the generalization of DNNs in online settings.

%add a point for motivated forgetting where it supress unwanted memories.

\begin{figure}[!tb]
\centering
\includegraphics[trim=.8cm .2cm 1cm 0, clip, width=1\textwidth]{images/method_v3.pdf}
\caption{Schematics of the proposed \textit{LURE} framework. LURE alternates between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features.}
\label{fig:method}
\end{figure}

\textbf{Unlearn.}
Motivated by the symbiotic link between generalization and active forgetting in biological neural networks \citep{gravitz2019forgotten, davis2017biology}, we introduce an unlearning step in which the network selectively forgets the connections that are less relevant for the current mega-batch and retains those that are specific for the current mega-batch. We quantify the sensitivity (importance) of each connection in the network in a data-dependent manner to identify task-specific connections. We employ SNIP \citep{lee2018snip}, which harnesses the sensitivity of the connection by decoupling the weight from the loss function to find relevant connections. To determine the sensitivity of the connection, we sample a small subset of data from the current mega-batch ($\mathcal{\pi}_i \subset \mathcal{M}_i: |\mathcal{\pi}_i|=\alpha \times |\mathcal{M}_i|$, where $\alpha$ is the percentage of data to be used as a subset). In our case, following \citet{Misra2022Apr}, we use $\alpha=0.2$, however, we can also use samples as low as 128 \citep{lee2018snip} to estimate the connection sensitivity. We define a connection sensitivity mask $\mathbf{M} \in\{0,1\}^{|\theta|}$ that is proportional to the number of parameters (m) in the network. Then we apply a sparsity constraint $k$ that specifies the percentage of parameters that must be retained. We compute the connection sensitivity as follows:
\begin{equation}
    g_j(\mathbf{\theta} ; \mathcal{\pi})=\left.\lim _{\delta \rightarrow 0} \frac{\mathcal{L}_{ce}(\mathbf{M} \odot \mathbf{\theta} ; \mathcal{\pi})-\mathcal{L}_{ce}\left(\left(\mathbf{M}-\delta \mathbf{e}_j\right) \odot \mathbf{\theta} ; \mathcal{\pi}\right)}{\delta}\right|_{\mathbf{M}=1}
\end{equation}
where $j$ corresponds to the parameter index and $e_j$ is the mask vector of the index j where the magnitude of the derivatives is then used to calculate the saliency criteria ($s_j$):
$$
s_j=\frac{\left|g_j(\mathbf{\theta} ; \mathcal{\pi})\right|}{\sum_{k=1}^m\left|g_k(\mathbf{\theta} ; \mathcal{\pi})\right|} .
$$
% ($\mathcal{\pi}_i= 0.2 \times \mathcal{M}_i$)
Following the saliency computation, the connection sensitivity mask is set to only retain the top-k task-specific connections based on the sparsity constraint $k$ which is given as follows:
$$
\mathbf{M}_j=\mathbbm{1}\left[s_j-\tilde{s}_\kappa \geq 0\right], \quad \forall j \in\{1 \ldots m\},
$$
where $\tilde{s}_k$ is the $k^\text{th}$ largest element in the saliency vector $s$ and $\mathbbm{1}[.]$ is the indicator function. 

Then, based on the saliencies pertaining to the connection sensitivity, we retain the top-k important connections and unlearn the parameters that are not important for the current data. Thus, we induce active forgetting through reinitialization of the connections that are less desirable for the current mega-batch. Finally, the network parameters pertaining to unlearned connections are reinitialized to random values:
\begin{equation}\label{eq:reinit}
\theta_{new}^j= \begin{cases}\theta^j & \text { if } \mathbf{M}_j=1 \\ \theta_{Reinit} & \text { Otherwise }\end{cases}
\end{equation} 
where $\theta$ are the weight parameters for the previous mega-batch and $\theta_{Reinit}$ corresponds to the random initialized value sampled from a uniform distribution. 
The network with the new parameters, $\theta_{new}$, is then trained on a new consecutive mega-batch.


\textbf{Relearn.}
In this stage, the network with the new initialization ($f_{\theta_{new}}$) is updated with the new incoming data $\mathcal{M}_{i+1}$ (in case of no-replay), or with the joint of all the seen data $\mathcal{M}_{i+1} \cup \mathcal{M}_{i}$ (in case of full-replay) for the $e$ epochs, where $e$ is kept the same for each iteration. The network is trained with the loss function shown in Equation~\ref{eq:ce}. The unlearn and relearn phases are alternately repeated after the completion of each mega-batch training. Thus, by alternating between unlearning and relearning, we favor the preservation of the task-specific connections that can guide the network towards those desirable traits that efficiently improve performance and generalization.

\section{Experimental set-up} \label{evaluation_metric}
We evaluate our LURE framework on CIFAR-10 \citep{krizhevsky2009learning}, CIFAR-100 \citep{krizhevsky2009learning}, and Restricted Imagenet (balanced) \citep{ilyas2019adversarial,tsipras2018robustness}.

% \textcolor{blue}{
% \textbf{Datasets.} We empirically evaluate our proposed framework on three different data sets: (a) CIFAR-10 \citep{krizhevsky2009learning} (b) CIFAR-100 \citep{krizhevsky2009learning} and (c) .
% }


\textbf{Anytime learning settings.} %\label{atsettings}
Following \citet{pmlr-v199-caccia22a}, we create the standard for ALMA evaluation using the datasets mentioned above: (1) The training set is randomly divided into $|S_B|$ mega-batches, each containing an equal number of training instances. For CIFAR10 and CIFAR100, we keep $|S_B| = 8$, but for Restricted ImageNet, we keep $|S_B| = 3$. However, we also perform long sequence experiments on CIFAR10 with $|S_B|={25, 50, 100}$ to analyze the effectiveness of the proposed approach. (2) We use 10\% of the data from each mega-batch as a mega-batch validation set. Following \citet{pmlr-v199-caccia22a}, we train each mega-batches for 50 epochs. A test set is employed to evaluate the model's performance as it observes the data. Note that this is not used for validation purposes; it is solely used for final reporting. For implementation details and hyperparameters, please refer to Appendix Section~\ref{implementation_details}. 


\textbf{Baselines.} We benchmark our proposed framework against (1) Baseline (BL), a warm-start model, which is continuously trained from the checkpoint of the previously trained model in ALMA settings (without reinitialization) as proposed by \citet{pmlr-v199-caccia22a}. {(2) Cold-start model (Random init.) is trained completely from scratch with each advent of the incoming data. (3) RIFLE \citep{li2020rifle}, where the fully connected layer is reinitialized and retrained during transfer learning. (4) Shrink and Perturb (S\&P) \citep{ash2020warm} which is proposed for online learning, and (5) Later-Layer forgetting (LLF)\citep{zhou2022fortuitous} which is proposed to improve generalization in the small data regime.


\textbf{Metrics.}
For a thorough evaluation, we use CER along with the test accuracy and the generalization gap. 
\begin{itemize}
    \item \textbf{Cumulative Error Rate (CER):} This can be defined as follows: 
    \begin{equation}
C E R=\sum_{i=1}^{\mathcal{M}_t} \sum_{j=1}^{\left|\mathcal{T}_{x, y}\right|} \mathbbm{1}\left(f_\theta^i\left(x_j\right) \neq y_j\right),
    \end{equation}
where $\mathcal{T}_{x,y}$ represents the held-out test set, $f_\theta^i$ is trained on $\mathcal{M}_i$, $y$ is the ground truth label. A model must have a lower CER at each mega-batch of training with a data stream in order to be an effective anytime learner.
    \item \textbf{Generalization gap:} We use the standard generalization gap as a measure to understand whether the model is overfitting or underfitting at anytime learning which is given by the difference between training and validation accuracy.
\end{itemize}


\section{Results}
% \textcolor{blue}{
% % In this section, we evaluate the effectiveness of LURE in ALMA settings by conducting several experiments. Details on the experimental setup and implementation details are provided in Section~\ref{atsettings} under any time learning settings.
% }

\subsection{Analysis of Short Sequence}

Table~\ref{tab:shortseq} shows the results of the ResNet18 model training on multiple datasets with and without different forms of reinitialization. All experiments on CIFAR10 and CIFAR100 were carried out using full replay $(S_B = \bigcup_{i=1}^8 \mathcal{M}_i)$ for a total of 8 mega-batches with each mega-batch containing 6250 samples, while the experiments on Restricted ImageNet are performed for $|S_B|=3$ mega-batches.
Our observations from Table~\ref{tab:shortseq} are as follows: 
(1) Reinitialization-based training for online learning improves test accuracy, CER, and generalization to a greater extent consistently on the three datasets compared to warm-start (BL) and cold-start (Random Init.) models.
(2) LURE outperforms the baseline by 4.8\%, 6.64\% and 4.13\% on CIFAR10, CIFAR100, and Restricted ImageNet respectively, and shows the strongest performance on all datasets compared to the other reinitialization methods. 
(3) Online training using LURE results in the lowest CER and generalization gap compared to other methods, improving the model's anytime learning capabilities. 
Thus, selective forgetting undesirable information through weight reinitialization brings discernable benefits to the model in online settings.



\begin{table}[t]
\centering
\caption{Evaluation of the model (Resnet18) trained with various reinitialization methods in ALMA settings. CIFAR10 and CIFAR100 were trained in a sequence of $|S_B|=8$, while restricted ImageNet was trained with $|S_B|=3$ mega-batches.}
\label{tab:shortseq}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Datasets & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ \midrule
\multirow{6}{*}{CIFAR10 }    
&  BL & 89.47 \tiny{$\pm0.51$} & 11760 & 8.98  \\
& Random init. &92.22 \tiny{$\pm0.57$} &12323  & 7.56 \\
& RIFLE & 90.08  \tiny{$\pm0.37$} &11844  &6.98  \\
& LLF  & 91.43 \tiny{$\pm0.70$} & 10103 & 7.22 \\
& S\&P & 91.76 \tiny{$\pm0.26$} & 10206 & 7.72  \\
& LURE & \textbf{93.32} \tiny{$\pm0.58$} & \textbf{9622}  & \textbf{6.60}  \\ \midrule
\multirow{6}{*}{CIFAR100}    
& BL & 62.96 \tiny{$\pm0.53$}  & 39010  & 31.54  \\
& Random init. &64.53 \tiny{$\pm0.51$} &37253  & 27.23 \\
& RIFLE & 61.38  \tiny{$\pm0.26$} &39302  &27.81  \\
& LLF  & 67.04 \tiny{$\pm0.68$} & 34575  & \textbf{21.23}  \\
& S\&P & 64.48 \tiny{$\pm0.11$} & 36303    & 28.58  \\
& LURE & \textbf{69.60} \tiny{$\pm0.82$}  & \textbf{33037}    & 22.37  \\ \midrule
\multirow{6}{*}{Restricted ImageNet} 
& BL & 81.39 \tiny{$\pm0.48$}  & 2967  & 4.90  \\
& Random init. & 82.87 \tiny{$\pm0.48$}  & 2852  & 4.88  \\
& RIFLE &82.05  \tiny{$\pm0.22$}  &2722  & \textbf{4.63}  \\
& LLF  & 82.10 \tiny{$\pm 0.85$} & 2854  & 4.88  \\
& S\&P & 80.80 \tiny{$\pm 0.39$}  & 2996  & 5.10  \\
& LURE & \textbf{85.52} \tiny{$\pm 0.22$} & \textbf{2699}  & {4.84} \\
\bottomrule
\end{tabular}%}
\end{table}

\subsection{Analysis of Short Sequence with Buffered/No Replay}
We investigate a real-world scenario in which access to the entire dataset used to train the model is restricted due to data privacy or memory restrictions. Tables~\ref{tab:shortseq_buffreplay} and \ref{tab:shortseq_noreplay} show the results of training the ResNet18 model on multiple datasets with buffered replay (buffer size =187) and without replay, respectively. All experiments on CIFAR10 and CIFAR100 were carried out for a total of 8 mega-batches with each mega-batch containing 6250 samples, while the experiments on Restricted ImageNet are performed for $|S_B| = 3$ mega-batches. LURE with and without buffered replay consistently outperforms baselines and other methods across all datasets. For example, LURE with buffered replay improves performance by 1.7\%, 9.4\%, and 2.5\% over standard training (BL) on CIFAR10, CIFAR100, and R-ImageNet, respectively. Moreover, we also observe that the performance of the cold-start models (Random Init.) is below par in low buffer and no-reply settings compared to full-replay setting (Table~\ref{tab:shortseq}) as the model does not have access to past knowledge or data.} In the challenging case of no-replay settings, our proposed method has a profound impact on standard training compared to other reinitialization methods. This demonstrates that the observed benefits of LURE are not limited to replay-based methods alone. Thus, unlearning and relearning at each mega-batch of training help to learn more generalizable representation in different replay scenarios. 


\begin{table}[t]
\centering
\caption{Evaluation of the model (ResNet18) trained \textit{with buffered replay} (buffer size=186) in ALMA settings. CIFAR10 and CIFAR100 were trained in a sequence of $|S_B|=8$ mega-batch, while restricted ImageNet was trained for $|S_B|=3$.}
\label{tab:shortseq_buffreplay}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Datasets & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ \midrule
\multirow{6}{*}{CIFAR10 }    
& BL & 88.01 \tiny{$\pm0.93$}   & 15784  & 13.01 \\
& Random init. & 75.26 \tiny{$\pm0.67$}   &22276  &17.79  \\
& RIFLE & 85.58 \tiny{$\pm0.55$}   &14748  &13.50  \\
& LLF  & 87.51 \tiny{$\pm0.82$}  & 15045 & 13.22 \\
& S\&P & 84.62 \tiny{$\pm0.74$}  & 15549 & 14.72  \\
& LURE & \textbf{89.8} \tiny{$\pm0.58$}   & \textbf{13629} & \textbf{11.92} \\  \midrule
\multirow{6}{*}{CIFAR100}    
& BL & 62.68 \tiny{$\pm0.74$}    & 39178  &31.33   \\
& Random init. & 58.36 \tiny{$\pm0.42$}   &40921  &27.14  \\
& RIFLE &60.44 \tiny{$\pm0.29$}  &40040  &28.19  \\
& LLF  &67.29 \tiny{$\pm0.58$}   &34416   & \textbf{24.86}   \\
& S\&P &65.07 \tiny{$\pm0.60$}   & 36040    & 27.79 \\
& LURE &\textbf{72.04} \tiny{$\pm0.35$}    & \textbf{33037}    &26.59   \\ \midrule
\multirow{6}{*}{Restricted ImageNet} 
& BL &75.97 \tiny{$\pm0.55$}    & 3284  &8.72   \\
& Random init. & 61.47 \tiny{$\pm0.69$}   & 4023  & \textbf{5.41} \\
& RIFLE &77.90 \tiny{$\pm0.48$}  & 3324 & 8.42 \\
& LLF  & 76.40 \tiny{$\pm0.65$}   & \textbf{3245}   & 9.07  \\
& S\&P & 74.75 \tiny{$\pm0.38$}   & 3857   & 8.20  \\
& LURE & \textbf{78.58} \tiny{$\pm0.49$}  & 3376  & {7.94} \\
\bottomrule
\end{tabular}%}
\end{table}

\begin{table}[t]
\centering
\caption{Evaluation of the model (ResNet18) trained \textit{without replay} in ALMA settings. CIFAR10 and CIFAR100 were trained in a sequence of $|S_B|=8$ mega-batch, while restricted ImageNet was trained for $|S_B|=3$.}
\label{tab:shortseq_noreplay}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Datasets & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ 
\midrule
\multirow{6}{*}{CIFAR10 }    
& BL  & 80.86 \tiny{$\pm1.01$}  & 16789  &16.85  \\
& Random init. & 73.56 \tiny{$\pm0.45$}  & 22731 & 17.17 \\
& RIFLE & 86.35 \tiny{$\pm0.14$} &14536  &15.53 \\
& S\&P  & 81.92 \tiny{$\pm0.62$} & 16133 & 16.36 \\
& LLF & 86.11 \tiny{$\pm0.59$} & 15294 & 14.66 \\
& LURE  & \textbf{88.96} \tiny{$\pm0.24$} & \textbf{13953} & \textbf{12.14} \\
 \midrule
\multirow{6}{*}{CIFAR100}    
&  BL  & 48.79 \tiny{$\pm0.75$}   & 44128   &49.95   \\
& Random init. & 35.50 \tiny{$\pm0.66$} &55221  &67.03  \\
& RIFLE & 45.65 \tiny{$\pm0.42$} &44113  & 51.46 \\
& LLF  & 50.38 \tiny{$\pm0.54$}  & 44816  &50.80   \\
& S\&P & 49.26 \tiny{$\pm0.59$}  & 44581    &51.45  \\
& LURE &\textbf{55.37}\tiny{$\pm0.63$}   & \textbf{43348}    &\textbf{46.69}   \\ \midrule
\multirow{6}{*}{Restricted ImageNet} 
& BL & 73.36 \tiny{$\pm0.88$}  & 3450   & 9.68  \\
& Random init. & 60.87 \tiny{$\pm0.61$}  & 4179  & \textbf{7.18}  \\
& RIFLE & 74.87 \tiny{$\pm0.24$}  & 3375  & 8.36 \\
& LLF  & 76.40 \tiny{$\pm0.65$}  & 3515   & 8.15  \\
& S\&P & 73.36 \tiny{$\pm0.78$}  & 3464   & 8.13  \\
& LURE & \textbf{77.97} \tiny{$\pm0.43$} &\textbf{3367}   &  {7.90}\\
\bottomrule
\end{tabular}%}
\end{table}


% \begin{table}[t]
% \centering
% \caption{}\label{tab:noreplay}
% \begin{tabular}{@{}llccc@{}}
% \toprule
% % % z\multirow{2}{*}{Methods} & \multicolumn{3}{c}{Buffer Replay} \\ \cmidrule{2-4}
% & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\
% \midrule
% \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Buffer\\ Replay\end{tabular}}
% & Baseline & & &\\
% & S\&P & 84.62 & 15549 & 14.72  \\
% & LLF  & 87.51 & 15045 & 13.227 \\
% & LURE & 89.8  & 13629 & 11.928 \\ 
% \midrule
% \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}No\\ Replay\end{tabular}}
% & Baseline  & & & \\
% & S\&P  & 81.92 & 16133 & 16.36 \\
% & LLF & 86.11 & 15294 & 14.66 \\
% & LURE  & 88.96 & 13953 & 12.14 \\
% % \midrule
% % \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Full\\ Replay\end{tabular}}
% % Baseline   & 91.43 & 10545 & 8.98  \\
% % S\&P  & 92.3  & 10206 & 7.538 \\
% % LLF& 92.9  & 10103 & 7.043 \\
% % % KE & & & \\
% % LURE & 94.32 & 9622  & 6.60  \\ \hline
% \bottomrule
% \end{tabular}
% \end{table}

%  For warm-started methods (BL), 

\subsection{Analysis of Moderate and Long Sequence ($|S_B|= 25, 50, 100$)}

Computational systems deployed in the real world are often exposed to longer mega-batch sequences of data and need to be updated frequently. Therefore, it is quintessential for the online model to perform well under longer mega-batch sequences.
Table~\ref{tab:longseq} shows the results of the ResNet18 model training on the CIFAR10 dataset. All experiments were carried out using full replay for a longer sequence of mega-batches 25, 50, and 100. We observe that LURE consistently outperforms the baseline and other methods across varying sequences of mega-batches. As the sequence of mega-batches increases, the number of samples available per mega-batch reduces drastically. Similar to \citet{pmlr-v199-caccia22a}, we observe that regularly updating the model on fewer samples significantly exacerbates CER, resulting in poor Anytime learning performance. Therefore, long-sequence online learning with reinitialization, especially LURE, reduces the CER and the generalization gap to a greater extent compared to standard training, thus enriching the predictive capabilities of the model at any given time. 


% As number of tasks increase, number of samples per mega-batch may reduce resulting in increased overfitting to the past samples lesser generalization.  


% Instead, methods with an intermediate waiting time (shown in orange) can quickly deliver a
% reasonable predictor early in the stream, and obtain a final error rate that is very close to the lower bound obtained by
% tardy methods. 



\subsection{Analysis of Few-Shot Experiments on Restricted ImageNet}

For many real-world classification problems, machine learning models deployed often need to be updated on labeled data that are scarce and may not be initially available for training. It is possible for new sets of labeled data to become available gradually as they are labeled. Therefore, it is important for the system to function properly in such online few-shot settings. These experiments are performed on the Restricted ImageNet dataset and quantitatively evaluated with the same metric described in Section~\ref{evaluation_metric}. We use the same hyperparameters (epochs, lr scheduler), backbone architectures (ResNet50) and optimizer as used in the experiments in ALMA settings (see Appendix Section~\ref{implementation_details}). However, following \citet{Misra2022Apr}, we limit the availability of samples pertaining to the classes to 270. Note that we do not use any techniques proposed in the few-shot literature to boost performance. Table \ref{tab:fewshot} shows the results of the methods in a few-shot settings where we vary the sequence of mega-batches. We observe that reinitialization-based training improves performance and generalization over the baseline, even in challenging few-shot classification. For a mega-batch sequence of 70, LURE outperforms baseline, LLF, and S\&P by a relative improvement of 4.11\%, 2.01\%, and 3.15\%, respectively, while for a sequence of 30, it is on par with LLF in terms of accuracy. In addition, LURE outperforms both the important baselines (BL and Rand. Init.) comfortably. In both settings, LURE achieves the lowest CER and the generalization gap, demonstrating the superiority of our proposed approach.

\begin{table}[t]
\centering
\caption{Evaluation of the model (ResNet18) on CIFAR10 for longer sequences of mega-batches.}
\label{tab:longseq}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clccc@{}}
\toprule
\# Mega-batches & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ \midrule
\multirow{6}{*}{25}
& BL & 89.94 \tiny{$\pm0.54$} & 43029  & 6.80 \\
& Random init. &89.89 \tiny{$\pm0.36$} & 43529 & \textbf{4.86}  \\
& RIFLE & 90.35 \tiny{$\pm0.54$} & \textbf{41308}  &5.55  \\
& LLF  & 89.80 \tiny{$\pm0.62$}  & 42961  &6.52  \\
& S\&P & 88.37 \tiny{$\pm0.26$}  & 43578 & {5.94}  \\
& LURE & \textbf{90.55} \tiny{$\pm0.34$}  & {42790}  & 6.79  \\ \midrule
\multirow{6}{*}{50} 
& BL & 89.12 \tiny{$\pm0.61$}  & 87843  &6.02  \\
& Random init. & 89.62 \tiny{$\pm0.42$}  &91040  &6.02  \\
& RIFLE &89.25\tiny{$\pm0.53$}  &91426  &6.16  \\
& LLF  & 90.26 \tiny{$\pm0.82$}  & 87826 &5.75  \\ 
& S\&P & 88.32 \tiny{$\pm0.35$}  & 85798 &6.11  \\
& LURE & \textbf{90.97} \tiny{$\pm0.67$} & \textbf{85487}  & \textbf{5.18} \\ \midrule
\multirow{6}{*}{100}
& BL & 89.64 \tiny{$\pm0.69$}  & 176954  & 6.46 \\
&  Random init. &89.29 \tiny{$\pm0.39$} &188978  & 5.61 \\
& RIFLE & 89.93 \tiny{$\pm0.27$}  &170678  &6.02  \\
& LLF  & 89.48 \tiny{$\pm0.77$} & 173505 &7.21  \\
& S\&P & 88.01 \tiny{$\pm0.44$}  & 182294  & \textbf{5.56}  \\
& LURE & \textbf{91.95} \tiny{$\pm0.53$} & \textbf{170178}  & 5.66 \\ \bottomrule
\end{tabular}%}
\end{table}

\begin{table}[t]
\centering
\caption{Few-shot experiments using ResNet50 on Restricted ImageNet.} 
\label{tab:fewshot}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clccc@{}}
\toprule
\# Mega-batches & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ \midrule
\multirow{6}{*}{30} 
    & BL   & 45.82 \tiny{$\pm0.55$}  &73514  & 47.93 \\
    &  Random init. & 45.79 \tiny{$\pm0.57$}  &73497  & 45.77 \\
    & RIFLE &44.58 \tiny{$\pm0.46$}  &74071  & 46.30 \\
    & LLF  & \textbf{47.60} \tiny{$\pm0.61$}  &72868  &43.50  \\
    & S\&P & 45.92 \tiny{$\pm0.38$}  &72954  &45.61  \\
    & LURE & 46.97 \tiny{$\pm0.79$}  & \textbf{71542}  & \textbf{42.56}  \\ \midrule
\multirow{6}{*}{70} 
    & BL   & 53.43 \tiny{$\pm0.49$}  &156332  &41.17  \\
    &  Random init. & 54.20 \tiny{$\pm0.69$}  &150473  &38.17  \\
    & RIFLE &53.25 \tiny{$\pm0.52$} &153716  &36.31  \\
    & LLF  & 54.53 \tiny{$\pm0.68$}  &149493  &32.57  \\
    & S\&P & 53.90 \tiny{$\pm0.24$}  &150332  &40.87  \\
    & LURE & \textbf{55.65} \tiny{$\pm0.52$}  &\textbf{148478}  &\textbf{28.35}  \\ \bottomrule
\end{tabular}%}
\end{table}

\subsection{Analysis on Various Architectures}

Here, we examine the versatility of our proposed LURE framework for multiple architectures on the CIFAR10 dataset. We consider ResNet18 \citep{he2016deep}, ResNet50 \citep{he2016deep}, wider-Resnet50-2 \citep{zagoruyko2016wide}, VGG16 \citep{simonyan2014very}. We chose these models explicitly because of their widespread popularity in common computer vision tasks and the breadth of research done on them for different learning paradigms. Table~\ref{tab:arch} shows the performance and generalization gap of the model trained in different architectures. The experiments are performed with full replay for $|S_B|=4$. The results demonstrate that LURE significantly outperforms standard training across multiple architectures while the LLF and S\&P fail to improve. Therefore, reinitialization of the weight parameter in a data-dependent manner using connection sensitivity by LURE is more effective to improve generalization in different architectures than reinitialization based on assumed model properties and learning (as done by LLF and S\&P).   

\begin{table}[t]
\centering
\caption{Evaluation of methods using different architectures on CIFAR10 dataset $(|S_B|=4)$.}
\label{tab:arch}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Architechture & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ \midrule
\multirow{6}{*}{ResNet18}  
   & BL & 89.31 \tiny{$\pm0.61$} & 5657  & 7.90  \\
  &  Random init. & 92.78 \tiny{$\pm0.31$}   &5459  & 7.56  %7.618
  \\
  & RIFLE & 92.11 \tiny{$\pm0.38$} & 5378  & 7.77   \\
   & LLF & 91.80 \tiny{$\pm0.43$} & 5466  & 7.68   \\
   & S\&P & 90.50 \tiny{$\pm0.51$} & 5667  & 7.80 \\
   & LURE &\textbf{93.73} \tiny{$\pm0.35$} &\textbf{4409}  &\textbf{7.48}   \\ \midrule
\multirow{6}{*}{ResNet50}  
   & BL &89.53 \tiny{$\pm0.58$}  &6854  &7.83  \\
  &  Random init. &91.87 \tiny{$\pm0.66$}   &7539  & 6.78 \\
  & RIFLE &90.51 \tiny{$\pm0.35$}  &6740  & 6.95 \\
   & LLF  &89.49 \tiny{$\pm0.45$}  &6782  & 6.78 \\
   & S\&P &89.15 \tiny{$\pm0.61$}  &6940  &6.98  \\
   & LURE &\textbf{92.75} \tiny{$\pm0.73$} &\textbf{6682} &\textbf{6.62}  \\ \midrule
\multirow{6}{*}{Wide-ResNet50-2} 
   & BL &90.10 \tiny{$\pm0.69$}  &5978  &6.84  \\
 &  Random init. &92.23 \tiny{$\pm0.27$}  &6615  &6.73  \\
  & RIFLE &89.98 \tiny{$\pm0.42$}  & 5953 &6.02  \\
   & LLF  &89.72 \tiny{$\pm0.36$}  & 6000 &\textbf{5.70}  \\
   & S\&P &89.38 \tiny{$\pm0.61$}  &6292  &5.96  \\
   & LURE &\textbf{93.78} \tiny{$\pm0.54$} &\textbf{5557}  &5.81   \\ \midrule
\multirow{6}{*}{VGG16-BN} 
   & BL & 89.32 \tiny{$\pm0.74$} & 5650 & 9.62 \\
  &  Random init. &91.7 \tiny{$\pm0.33$} &5913  & 7.22 \\
  &  RIFLE & 88.25 \tiny{$\pm0.38$}  &6083  &6.89  \\
   & LLF  & 87.85 \tiny{$\pm0.58$} &6124  &\textbf{6.17}  \\
   & S\&P & 88.25 \tiny{$\pm0.86$} & 5720 &8.11  \\
   & LURE &\textbf{92.67} \tiny{$\pm0.47$} &\textbf{4439}  & 8.55   \\ \bottomrule
\end{tabular}%}
\end{table}

\section{Robustness Analyses}

\subsection{Robustness to Natural Corruptions}
In practice, DNNs are often deployed in real-world scenarios where they are exposed to constantly changing environments, often influenced by changes in lighting and weather. Therefore, the robustness of DNNs to data distributions that are subject to natural corruption is pertinent. Here, we evaluate the benefit of LURE on robustness to common corruption using CIFAR10-C \citep{hendrycks2019benchmarking}. Models are trained on clean images and tested on CIFAR10-C. Following \citet{hendrycks2019benchmarking}, we use the mean Corruption Accuracy (mCA) to measure performance under natural corruption. Figure \ref{fig:corrup} shows the accuracy of the models on 19 different corruptions averaged on five severity levels. Compared to baseline (68\%), Random initialization (69.3\%), RIFLE (64.7\%), LLF (72\%), and S\&P (71\%), LURE (74\%) delivers a higher mCA in all types of corruption. Evidently, unlearning and relearning at each mega-batch of training bring discernible benefits in terms of robustness to natural corruptions.

\subsection{Robustness to Adversarial Attacks}
DNNs have been shown to be vulnerable to adversarial attacks in which imperceptible perturbations are added to inputs during inference. The adversarial images are designed to fool the network to make false predictions \citep{szegedy2013intriguing}. We perform a PGD-10 attack \citep{madry2017towards} on models trained on the CIFAR10 dataset with varying attack strengths. As observed in Figure~\ref{fig:noise}(Left), LURE exhibits greater resistance to these attacks of varying strengths. Thus, compared to standard training, training a model in the LURE framework facilitates online learners to learn high-level abstractions that are not sensitive to small perturbations in the data. 

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{images/corrup_rifle.pdf}
\caption{Robustness to natural corruptions on CIFAR10-C \citep{hendrycks2019benchmarking}. LURE is more robust against the majority of corruptions compared to other reinitialization methods.}
\label{fig:corrup}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=.85\textwidth]{images/adv.pdf}
\caption{(Left) Robustness to adversarial attacks; (right) Robustness to training under noisy labels. In both robustness analyses, LURE shows a significant performance improvement compared to the baselines considered.}
\label{fig:noise}
\end{figure}


\subsection{Robustness to Noisy Labels}
The success of supervised learning often depends on the availability of large amounts of high-quality annotations. However, the availability of high-quality annotated datasets can be extremely expensive and time consuming to collect. Therefore, it is paramount to have robust training on noisy labels, as studies have shown that DNNs can easily memorize samples and are susceptible to noisy labels \citep{arpit2017closer}. we train ResNet18 on sequential CIFAR10 with noisy labels for a mega-batch sequence of $|S_B|=4$ and evaluate the performance on a clean test set. We corrupt every ground truth label with a specified probability (noise rate) by randomly sampling from a uniform distribution over a large number of classes. Figure~\ref{fig:noise}(Right) presents the test accuracy of the reinitialization methods under different percentages of noisy labels. The results show that the reinitialization-based training paradigm is robust to the presence of noisy labels during training compared to the warm-started baseline. Furthermore, LURE consistently outperforms the LLF, RIFLE, and S\&P baseline comfortably across different noisy label rates. Thus, reinitializing based on selective forgetting helps to learn a generic representation that is less sensitive to noise in the dataset. 
  
% \begin{figure}[t]
%  \centering
%  \begin{subfigure}[l]{0.5\textwidth}
%  \centering
%  \includegraphics[width=\textwidth]{images/perturbation_acc.pdf}
%  \label{fig:noisy}
% %   \caption{}
%  \end{subfigure}%
% %  ~
% %  \begin{subfigure}[c]{0.49\textwidth}
% %  \centering
% % \includegraphics[width=\textwidth]{images/hyp.pdf}
% % %  \caption{}
% %  \label{fig:hyper}
% %  \end{subfigure}
% %  \caption{(Left) Robustness of the model perturbed by varying degrees of Gaussian noise. Our method is considerably robust to Gaussian perturbations as the decline in performance is gradual, suggesting convergence to flatter minima. (Right) Sensitivity to hyperparameters. LURE is more robust to the changes in weight decay and learning rate to standard online training.}
% %  \label{fig:hyper}
% \end{figure}

\begin{figure}[t]
\centering
 \includegraphics[width=0.91\linewidth]{images/overlap_hyp_frep.pdf}
\caption{Layer-wise percentage overlap of the retained parameters in consecutive mega-batches in the full-replay scenario. The results for the no-replay and buffered replay scenarios are provided in Figures~\ref{overlapbr_overlapnr}(a,b), respectively.}
\label{overlapfr}
\end{figure}


\subsection{Robustness of Connection Selection across Training Steps} \label{overlap_perc}
The proposed LURE framework is based on the selection of connections (that is, SNIP \citep{lee2018snip}) to selectively forget the parameters that have the least impact on performance at each mega-batch of training. Therefore, it is important to evaluate the consistency of the connection selection across the sequence of mega-batches. We conduct this analysis on the short sequence Anytime learning scenario (full replay, $((|S_B|=2)$) on CIFAR10 with ResNet18. We globally retain 80\%  and reinitialize (unlearn) 20\% of the parameters iteratively at the end of each mega-batch of training.


To study the consistency of the retained connections, we save the connection sensitivity mask (M) containing ones and zeros for the parameters retained and to be reinitialized, respectively. Visualizing the connection sensitivity mask can be challenging, as the parameter count in each layer of the backbone is overwhelming. Therefore, we calculate the percentage of overlap of retained parameters between the connection sensitivity mask generated at the end of consecutive mega batch training as a metric to analyze the connection consistency. Figure~\ref{overlapfr} shows the layer-wise percentage overlap of the retained parameters across consecutive mega-batches. The overlap percentage of retained connections is quite high in the earlier layers across all mega-batches, while it decreases in the later layers (layer 4 in ResNet) as it learns class-specific information pertaining to the new (unseen) samples. Although the overlap percentage is lower in the later layers, our method selectively unlearns few parameters in the earlier layers as well depending on the incoming data. This flexible nature of our method for unlearning and regulating connections in both the latter and early layers facilitates improved generalization in Anytime setting. This result is consistent not only in the full replay, but also in buffered and no-replay scenarios of anytime learning. The results for the no-replay and buffered replay scenarios are provided in Figures~\ref{overlapbr_overlapnr}(a,b), respectively. 

% This shows that our connection selection estimation remains consistent and robust in the earlier layers throughout the training steps. The connection selection in the later layers are less robust as the model is continuously trained with new, unseen samples.
  
 % The percentage of connections overlapped between the consecutive mega-batches remains consistent in the early and mid layers (block-1, 2, and 3) whereas it changes in the latter layers as the model is trained on the new incoming data.
 
\subsection{Sensitivity to Hyperparameters} \label{sensitivity_hyp}

\begin{wrapfigure}{r}{0.45\textwidth}
\centering
\includegraphics[width=.4\textwidth]{images/hyp.pdf}
\caption{Sensitivity to hyperparameters. LURE is more robust to the changes in weight decay and learning rate to standard online training.}
\label{fig:hyper}
\end{wrapfigure}

Machine learning systems are often deployed in the real world, where explicitly running a hyperparameter search for each update can be computationally exhaustive. Therefore, similar to \citet{zaidi2022does}, we explore the sensitivity of our method to the choice of weight decay and learning rate. Figure \ref{fig:hyper} shows the test accuracy achieved by changing the learning rate and the weight decay values intended for CIFAR-10 training in ALMA scenarios for the mega-batch sequence of $|S_B|=4$. Compared to baseline training without reinitialization, LURE is less sensitive to the choice of hyperparameters. A detailed comparison with other methods is provided in Appendix, Figure~\ref{fig:hypa}. For example, the performance of normal training decreases to 75\% with a learning rate of 0.005 and a weight decay of 0.001, while the performance of LURE remains above 94\% throughout. Therefore, LURE can improve generalization in regimes where it is infeasible to perform exhaustive hyperparameter tuning.


We underline that LURE's effectiveness exceeds that of other reinitialization techniques and that it should be viewed as a general-purpose online training paradigm, as it is more resilient to typical problems found in real-world datasets than the conventional online training method. Extensive experiments on the model characteristic analyses, such as model calibration and convergence to flatter minima, are provided in Appendix Sections~\ref{modelcalibration} and \ref{flattermin}, respectively. Furthermore, we analyze the robustness of connection selection during training steps and also across experiments trained with different learning rates. Finally, we have added a discussion section comparing LURE with other baselines and the benefits it brings in practical settings where the training budget is limited. 


\section{Conclusion and Future work}

We introduce \textit{Learn, Unlearn, and RElearn (LURE)}, an online training paradigm to improve DNN performance and generalization through the lens of selective forgetting. LURE alternates between the unlearning phase, which selectively forgets undesirable information in the model, and the relearning phase, which emphasizes learning generalizable features. Empirical results show that the proposed framework improves performance and generalization across a wide range of architectures and datasets, both online and in challenging few-shot classification. Our framework is robust to learning with noisy labels and adversarial attacks and increases generalization in many real-world scenarios. One advantage of our work is that we have observed distinct empirical tendencies when re-initialization succeeds. In future work, it would be interesting to study the dynamics of reinitialization in other lifelong learning scenarios, such as continual learning, where domain shifts and catastrophic forgetting are more common. Furthermore, studying activation-based connection selection may help us selectively identify and retain the most important weights in online learning settings. Further research in these areas may provide a more in-depth theoretical explanation for why reinitialization succeeds or fails.

% \textcolor{blue}{One drawback of our work is that, while we see distinct empirical tendencies when re-initialization succeeds, a more in-depth theoretical explanation of why it succeeds or fails is lacking and would be highly desired to have in subsequent work. Also, in the future, it will be interesting to study the dynamics of reinitialization in other lifelong scenarios, such as continual learning, where domain shifts and catastrophic forgetting are more prevalent. Another interesting direction will be to study activation-based connection selection to selectively identify and retain the most important weights in online learning settings.}


\bibliography{ref.bib}
\bibliographystyle{tmlr}

\newpage
\appendix
\section{Appendix}


\subsection{Broader Impact and Societal Relevance}
We believe that our findings can potentially be harnessed to enhance the test accuracy and robustness of any machine learning system deployed in the real world where the aspect of generalization is crucial, as they are continuously trained on sequential data. For example, consider a large-scale social media website in which users continually upload images and content. To recommend material, filter out inappropriate media, and choose adverts, the organization requires up-to-date prediction models. Every day, millions of fresh data points may arrive, which must be quickly integrated into operational ML pipelines. In this scenario, it is logical to envision having a single model that is regularly updated with the most recent data. Every day, additional training on the model with the updated and larger dataset might be undertaken. In these scenarios, the proposed framework (LURE) can improve the generalization and performance of the model to a greater extent as opposed to new training from the parameters of yesterday's model without reinitialization.

Furthermore, in applications such as autonomous driving and industrial robotics, where the deployed model needs to be frequently updated in order to stay in sync with the surroundings. Using LURE as a training paradigm to update the model can boost performance and generalization in a computationally efficient way, as it provides a better initialization for continuous training compared to warm-starting or updating the model from scratch. 

In addition to the above scenarios, our proposed framework can be conceivably harnessed in applications of deep active learning where the goal is to find the most informative data to label with an oracle and incorporate into the training set. However, current active learning frameworks retrain models from scratch after each querying step, which is computationally expensive and partially responsible for deleterious environmental ramifications. The LURE framework allows models to be efficiently updated without sacrificing generalization and performance, thus having a positive impact on society.



\subsection{Ablation studies} \label{ablation}
To examine the influence of the individual components of our LURE network in ALMA settings, we perform the following ablation study.

\textbf{Effect of Ratio of Reinitialized Parameters.} 
Table~\ref{tab:varyspar} shows the effect of varying the number of initialized parameters on the performance and generalization of the model in CIFAR10. We train the model in ALMA settings using the LURE framework by varying different percentages of reinitialized parameters (5\%, 10\%, 20\%, 30\%, and 40\%). Experiments were carried out using full-replay with ResNet18 for $|S_B|=8$. The results show that the unlearning of a 5\% percentage of parameters has no impact on performance, while the unlearning of more than 30\% has less impact on test accuracy. We find that reinitialization 20\% of the parameters results in the best performance.

\begin{table}[h!]
\centering
\caption{Evaluation varying the percentage of reinitialized parameters during training on CIFAR10 dataset using ResNet18.} \label{tab:varyspar}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
 & Reinitialized Params (\%) & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ \midrule
\multirow{5}{*}{LURE}
 & 5  & 89.84  & 11955  & 6.83  \\
 & 10 & 91.81  & 11571  & 7.22  \\
 & 20 & \textbf{94.32}  & \textbf{9622}  & 6.60 \\
 & 30 & 90.73  & 11806  & 7.91  \\
 & 40 & 90.79  & 11559  & \textbf{6.57}  \\ \bottomrule
\end{tabular}%}
\end{table}

% \textbf{Evaluation with different backbone architectures}
% Here, we examine the suitability of our proposed LURE framework for multiple backbones and architectures in CIFAR10. We consider ResNet18, ResNet50, wider-Resnet50-2 \citep{zagoruyko2016wide}, VGG16 \citep{simonyan2014very}. We chose these models explicitly because of their widespread popularity in common computer vision tasks and the breadth of research done on them for different learning paradigms. Table~\ref{tab:arch} shows the performance and generalization gap of the model trained in different architectures. Experiments were performed with full replay for $\mathcal{M}_{t=4}$. Results demonstate that LURE outperforms the standard training and other reinitialization methods  across multiple architechtures.

\textbf{Effect of Importance Estimation Method.}
We investigate the effect of various methods of importance estimation on our proposed training paradigm. For this, we consider Fisher Importance (FIM), weight magnitude, random, and SNIP. Table~\ref{tab:importanceest} demonstrates the performance and generalization of the model trained with the LURE framework with different selections to estimate important parameters on CIFAR10 using ResNet18 for $|S_B|=4$. Unlearning and relearning with SNIP, Fisher information, and weight magnitude results in better performance compared to baseline. This shows that our training paradigm is not only limited to SNIP, but any importance estimation criterion can be used to identify the dataset-specific connections.  


\textbf{Varying the quantity of data used for Importance estimation}
 In our experiments, we randomly sampled 20\% of the data from each mega-batch and used it to estimate the importance of the parameters before selective forgetting. Here, we analyze the impact of the number of data used to determine the important estimation on the final performance. Similar to \citet{lee2018snip}, we used as few as 128 samples to estimate the important parameters using SNIP. Table~\ref{tab:snipset} shows that LURE is not sensitive to the variation in the input data used to estimate the importance as the final performance remains unchanged.
 
 

% \subsection{Evaluation with buffered replay and no-replay}
% Table~\ref{tab:shortseq_buffreplay} and Table~\ref{tab:shortseq_noreplay} show the results of the ResNet18 model training on multiple datasets with buffered replay (buffer size =187) and without replay, respectively. All experiments on CIFAR10 and CIFAR100 were carried out for a total of 8 mega-batches with each mega-batch containing 6250 samples, while the experiments on Restricted ImageNet are performed for $|S_B| = 3$ mega-batches. LURE with and without buffered replay consistently outperforms baselines and other methods across datasets. LURE with buffered replay improves performance by 1.7\%, 9.4\%, and 2.5\% over standard training (BL) on CIFAR10, CIFAR100, and R-ImageNet, respectively. In the challenging case of no-replay settings, our proposed method has a profound impact on standard training compared to other reinitialization methods. Thus, unlearning and relearning at each mega-batch of training boosts generalization and performance in all replay scenarios. 

\begin{table}[t]
\centering
\caption{Evaluation with different importance estimation on CIFAR10 dataset.} \label{tab:importanceest}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clccc@{}}
\toprule
 & Importance criteria & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ \midrule
\multirow{4}{*}{LURE}
% & Random   &  &  &  \\
& BL & 89.31  & 5657  & 7.90 \\ 
& FIM  &92.73  &\textbf{4346}  & 8.39 \\
& Weight Magnitude &92.18  &4547  &8.53  \\
& SNIP &\textbf{93.73}  &4409  &\textbf{7.48}  \\ \bottomrule
\end{tabular}%}
\end{table}

\begin{table}[t]
\centering
\caption{Evaluation with varying the quantity of data for importance estimation on CIFAR10 dataset.} \label{tab:snipset}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
 & \# samples & Test Accuracy ($\uparrow$) \\ \midrule
\multirow{2}{*}{LURE}
% & Random   &  &  &  \\
& 0.2 $|\mathcal{M}|$ & 93.73  \\ 
& 128  &93.62  \\
 \bottomrule
\end{tabular}%}
\end{table}

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{images/calibration.pdf}
\caption{Confidence estimates and the corresponding Expected Calibration Error (ECE) of the CIFAR-10 ALMA trained models. Lower ECE is better. Our method is well calibrated, with confidence estimates closer to perfect calibration compared to BL, LLF, and S\&P.}
\label{fig:calibration}
\end{figure}

\subsection{Model Calibration}\label{modelcalibration}
% DNNs are considered miscalibrated when a model tends to be overconfident or underconfident about its predictions compared to ground truth accuracy \citep{guo2017calibration}. 
DNNs are often deployed in safety-critical applications, where it is essential to have a model that has a sufficient sense of uncertainty about its predictions. Therefore, we evaluate the calibration of models trained with different reinitialization methods in ALMA settings. The common metric for identifying miscalibration in classification is the Expected Calibration Error (ECE) \citep{naeini2015obtaining}. The ECE measures the discrepancy between absolute accuracy and average confidence as a weighted average. The lower the ECE, the better calibrated the model is. Figure \ref{fig:calibration} shows the ECE values along with a reliability diagram on CIFAR10 using the calibration library by \cite{kuppers2020multivariate}. The result shows that BL, LLF, and S\&P are highly miscalibrated and far more overconfident than the proposed LURE framework. Thus, in addition to improving performance and generalization, online learning using selective forgetting can effectively improve calibration, thus improving reliability in contexts where safety is of absolute importance.



\begin{wrapfigure}{r}{0.5\textwidth}
% \begin{figure}[t]
 \centering
 % \begin{subfigure}[l]{0.5\textwidth}
%  \centering
 \includegraphics[width=0.48\textwidth]{images/perturbation_acc.pdf}
  \caption{ Robustness of the model perturbed by varying degrees of Gaussian noise. Our method is considerably robust to Gaussian perturbations, as the decline in performance is gradual, suggesting convergence to flatter minima. }
 \label{fig:flatmin}
  
\end{wrapfigure}
\subsection{Convergence to Flatter Minima}\label{flattermin}
DNNs that converge to flatter minima in a loss landscape have greater adaptability to new tasks without straying too far from the optimal parameters for previous tasks. Furthermore, solutions that reside in flatter minima are more robust because the predictions do not change significantly with minor perturbations. We apply independent Gaussian noise to all parameters of the CIFAR-10 trained model, as described in \citep{alabdulmohsin2021impact}.
Figure~\ref{fig:flatmin} shows that the solution reached by LURE, LLF, and S\&P is more robust to model perturbation than standard training. Our method is significantly less sensitive to perturbations than the other methods, and the performance gradually decreases. More specifically, for every amount of noise introduced into the model parameters $\theta$, the change in LURE training accuracy is smaller than in standard training, implying that the solution provided by LURE appears to reside in flatter local minima. We argue that training the model by alternating between learning and unlearning stages leads to a larger valley, which could better explain our model's ability to consolidate generalizable features.

\begin{figure}[!b]
\centering
\includegraphics[width=0.9\textwidth]{images/hyp_2.pdf}
\caption{The sensitivity of different reinitialization methods to hyperparameters. The initialization-based training paradigm is more robust against the change in weight decay and the learning rate compared to standard online training.}
\label{fig:hypa}
\end{figure}

% \begin{table}[t]
% \centering
% \caption{Evaluation of the model (ResNet18) trained without replay in ALMA settings. CIFAR10 and CIFAR100 were trained in a sequence of $|S_B|=8$ mega-batch, while restricted ImageNet was trained for $|S_B|=3$.}
% \label{tab:shortseq_noreplay}
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llccc@{}}
% \toprule
% Datasets & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ 
% \midrule
% \multirow{4}{*}{CIFAR10 }    
% & BL  & 80.86  & 16789  &16.85  \\
% & S\&P  & 81.92 & 16133 & 16.36 \\
% & LLF & 86.11 & 15294 & 14.66 \\
% & LURE  & \textbf{88.96} & \textbf{13953} & \textbf{12.14} \\
%  \midrule
% \multirow{4}{*}{CIFAR100}    
% & BL  &48.79   &44128   &49.95   \\
% % & Random init. &  &  &  \\
% & LLF  &50.38  & 44816  &50.80   \\
% & S\&P &49.26  & 44581    &51.45  \\
% & LURE &\textbf{55.37}   & \textbf{43348}    &\textbf{46.69}   \\ \midrule
% \multirow{4}{*}{Restricted ImageNet} 
% & BL & 73.36  &3450   & 9.68  \\
% % & Random init. &  &  &  \\
% & LLF  &76.40  &3515   & 8.15  \\
% & S\&P &73.36   &3464   & 8.13  \\
% & LURE & \textbf{77.97} &\textbf{3367}   &  \textbf{7.90}\\
% \bottomrule
% \end{tabular}%}
% \end{table}


\begin{algorithm}[h!]
\caption{Training LURE in ALMA settings}
\label{alg:method}
\begin{algorithmic}[1]
    \Statex \textbf{input: } Data stream  $\mathcal{S}_{B} = \{\mathcal{M}_1,...,\mathcal{M}_t $\}, Model $f_\theta^{i=0}$, replay, Sparsity $\mathcal{\alpha}$
    \State $i \gets 1$
    \While {$i \leq |S_B|$}
 \If{replay}
  \State $\mathcal{M}_i \gets \bigcup_{i=1}^t \mathcal{M}_i$
\Else
  \State $\mathcal{M}_i \gets \mathcal{M}_i$
\EndIf
 \State$f_\theta^i \gets f_\theta^{i-1}.train(\mathcal{M}_i)$  \Comment{Training or learning step}

\State $\pi_i \gets 0.2 \mathcal{M}_i$
\State $M$ $\gets$ Importance Estimation$(f_\theta^i, \pi_i, \mathcal{\alpha}) $
% \State Use SNIP to measure the connection importance in a data dependent manner.
% \State Create a new binary mask $M$ with $\alpha$ sparsity based on top K important parameters.
\State Retain the task specific weights based on $M$ 
\State  Randomly reinitialize the task irrelevant parameters in $f_\theta^i$ \Comment{Selective forgetting}
 \State {Model with this new initialization for next $\mathcal{M}_{i+1}$ training }
    \EndWhile
\end{algorithmic}
\end{algorithm}


\subsection{Implementation Details} \label{implementation_details}
\textbf{Datasets.} We empirically evaluate our proposed framework on three different data sets: (a) CIFAR-10 \citep{krizhevsky2009learning} (b) CIFAR-100 \citep{krizhevsky2009learning} and (c) Restricted Imagenet (balanced) \citep{ilyas2019adversarial,tsipras2018robustness}. CIFAR10 and CIFAR100 consist of 50,000 training images and 10,000 test images, each of size $32 \times 32$, divided into 10 and 100 classes, respectively. Restricted ImageNet (balanced) is a subset of the original ImageNet data set \citep{russakovsky2015imagenet} consisting of 89517 training images and 3450 test images, each of $224\times 224$ size divided into 14 classes consisting of five subclasses each. For ease of computation, we resize the images to $32\times32$ for our experimental settings. 


\textbf{Implementation Details.}
The efficacy of our framework is demonstrated in ALMA settings \citep{pmlr-v199-caccia22a}. ResNet18 \citep{he2016deep} is used as the backbone for most of the experiments on CIFAR10 and CIFAR100, while ResNet50 \citep{he2016deep} is used for restricted ImageNet experiments. We initialize the networks randomly and use stochastic gradient descent (SGD) with momentum 0.9 and weight decay 1e-4 to optimize it. We follow the same procedure as that followed by \citet{pmlr-v199-caccia22a, Misra2022Apr}. Networks are trained iteratively for $t$ mega-batches with a batch size b = 64 for 50 epochs per mega-batch of training without early stopping. A step learning rate scheduler with an initial learning rate of 0.1 decayed at steps 20 and 40 is employed during the mega-batch training of our method. The standard data augmentation technique, that is, flipping and random cropping, is used. All training settings (lr, b, e) are kept constant throughout the mega-batch training. We randomly divide the data set into mega-batches with an equal number of samples in each mega-batch. For each mega-batch $\mathcal{M}_t$, we divide it into a train set with 90\% of the samples and a validation set with the remaining 10\% samples. We then randomly sampled 20\% of the training data from each mega-batch to build the set $\pi$ used to identify task-specific parameters through SNIP after each mega-batch of training. We use the default parameters for the SNIP algorithm specified above.  Finally, we maintain a separate held-out test set, which is used to evaluate the model's performance after training on each mega-batch. Unless specified, we keep the number of mega-batches to 8 for all the experiments. For training S\&P, a shrink coefficient of 0.4 and a noise of 0.001 are applied for the weights of the entire network before training on the new mega-batch of data. Similarly, for LLF, we reinitialize blocks 3 and 4 of ResNet \citep{he2016deep} before the start of each mega-batch of training whereas for RIFLE we only reinitialize the last fully connected classification layer. For few-shot experiments, we do not consider existing techniques proposed in the few-shot literature. We limit the number of samples from each class to 270, which are sampled randomly in a class-balanced way. Finally, we use the same hyperparameters to perform experiments with different datasets and architectures.   

  
% This shows that our method increases the model capacity in the later layers implicitly to enable learning of new unseen samples in the subsequent mega-batches.

\begin{figure}[t]
\centering
\begin{tabular}{c}
   \includegraphics[width=1\linewidth]{images/overlap_hyp_brep.pdf} \\
   {\bf (a)} Buffered replay scenario \\
   \\
   \includegraphics[width=1\linewidth]{images/overlap_hyp_nrep.pdf} \\
   {\bf (a)} No-replay scenario \\
\end{tabular}
% \begin{subfigure}[b]{1\textwidth}
%    \includegraphics[width=1\linewidth]{images/overlap_hyp_brep.pdf}
%    \caption{Buffered replay scenario}
%    \label{overlapbr}
% \end{subfigure}
% \begin{subfigure}[b]{1\textwidth}
%    \includegraphics[width=1\linewidth]{images/overlap_hyp_nrep.pdf}
%    \caption{No-replay scenario}
%    \label{overlapnr}
% \end{subfigure}
\caption{Layer-wise percentage overlap of the retained parameters across consecutive mega-batches. The percentage of connections overlapped between consecutive mega-batches remains consistent in the early and mid layers (block-1, 2, and 3) whereas it changes in the latter layers as the model is trained on the new incoming data.}
\label{overlapbr_overlapnr}
\end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=1\textwidth]{images/overlap_fr.pdf}
% \caption{}
% \label{fig:overlapfr}
% \end{figure}


\subsection{Robustness of connection selection to change in learning rate.}
Reiterating from Section~\ref{sensitivity_hyp} where our LURE framework is less sensitive to the choice of hyperparameters such as learning rate and weight decay. Here, we explore the sensitivity of connection selection to the choice of hyperparameters. Similar to the experiments in Section~\ref{overlap_perc}, we calculate the layer-wise percentage overlap of retained parameters across different training setups at the end of mega batch training. We compute the overlap between models trained with different learning rates. We keep the seed and other hyperparameters consistent across the experiments. The results in Figure~\ref{overlaphypnodecay} demonstrate that the selection of connections is more robust to the model trained at different learning rates. While most of the connections retained in the early layers remain consistent across training setups, the later layers (layer 4) showed less overlap for the model trained with different learning rates. Nevertheless, our connection selection is robust to the choice of learning rate.  

\begin{figure}[t]
\centering
   \includegraphics[width=1\linewidth]{images/overlap_difflr_nodec_re.pdf}
\caption{Layer-wise overlap percentage of the retained parameters across training runs with different learning rates.}
\label{overlaphypnodecay} 
\end{figure}


\subsection{Evaluating the redundancy of parameters removed during the unlearning phase.}
Since our proposed method selectively forgets connections during training, we evaluate the redundancy of the unlearned connection in the model in terms of performance and robustness. To analyze this, we measure the performance and robustness of dense and sparse models before and after selective forgetting. For this, we measure the performance of the ResNet18 model trained on CIFAR10 with $|S_B|=8$ after the first mega-batch training. The performance is evaluated with the clean test dataset, while generalization is evaluated with the test images subjected to 15 types of natural corruptions. Table~\ref{tab:importanceofparam} shows that the drop in test accuracy between the dense model and the sparse model (containing 20\% fewer parameters than the dense model) is just 0.31\% which is insignificant. The relative drop in test and robust accuracy with respect to train accuracy is less with the sparse model when compared to the dense model. In addition, in robustness and generalization analysis, the sparse model either outperforms or achieves the same accuracy when compared to the dense model. This shows that the connections that are constantly reinitialized during training contain trivial information that is redundant in fact and adds insignificant value to model training in terms of generalization and performance. Thus, we empirically show that selective forgetting is crucial for retaining previous information while freeing the model's capacity to learn incoming data, thereby improving generalization in anytime learning. 


\begin{table}[t]
\centering
\caption{Evaluating the redundancy of the parameters removed during the unlearning phase. The relative improvement between the train accuracy and test/robust accuracy is shown in brackets.}\label{tab:importanceofparam}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
  & \# Parameters & Train Accuracy ($\uparrow$) & Test Accuracy ($\uparrow$) & Robust Accuracy ($\uparrow$)    \\ \midrule
% \multirow{3}{*}{}
% & Random   &  &  &  \\
Full model   & 11173962 & 75.54 & 71.78 (0.95\%) & 55.80 (0.74\%)  \\ 
Sparse model & 8939169  & 74.04 & 71.47 (0.96\%) & 56.20 (0.76\%)  \\
\bottomrule
\end{tabular}%}
\end{table}


\subsection{Comparison of LURE with warm-start and cold-start training.}
Figure~\ref{fig:conv} shows a comparison between ResNet18 trained using warm-start, random initialization, and LURE on CIFAR-10 for a mega-batch size of $((|S_B|=2)$. For the first 50 epochs, the models are trained with 50\% of data. Then, it is trained on 100\% of data for another 50 epochs. Random Init. (cold-start) are models trained on 100\% of the data from scratch. The dotted black line at 50 epochs represents the end of the first mega-batch training. The region between the two dotted lines shows the computationally resource intensive nature of the cold-start training to obtain optimal performance compared to warm-start and LURE. In practical settings where the training budget is limited, it is natural to maintain a single model that is updated with the latest data at regular intervals. Training a model from scratch at the onset of new data is resource-intensive and drains the training budget. Also, in a data privacy or memory-limited scenario where it is difficult to store the previously trained data, it is less intuitive and seems wasteful to sacrifice all previous computations (past knowledge acquired) for much-needed generalization. While the warm-start training damages generalization, our proposed method (LURE) brings discernible benefits in practical online settings: (1) LURE achieves faster convergence when compared to the cold-start (Random. Init.) model in settings where anytime performance is required, thus saving computational resources. (2) Improves generalization and robustness in anytime settings compared to warm-start (BL) and cold-start (Random. Init.) models. (3) LURE can also improve generalization in no-replay and buffered-replay scenarios. LURE with selective forgetting balances the trade-off between a lack of generalization in warm-start models and the enormous computational expense of retraining models from scratch through reinitialization. 


\begin{figure}[!b]
\centering
\includegraphics[width=0.86\textwidth]{images/convergence_band.pdf}
\caption{A comparison between ResNet models trained using warm start, random initialization, and LURE on CIFAR-10 $((|S_B|=2)$. For the first 50 epochs, the models are trained with 50\% of data. Then, it is trained on 100\% of the data for another 50 epochs. Random Init. (cold-start) are models trained on 100\% of the data from the start. The dotted black line at 50 epochs represents the end of the first mega-batch of training. The region between two dotted lines shows the computationally resource intensive nature of the cold-start training to convergence compared to warm-start and LURE. LURE mitigates the trade-off between warm-start and cold-start training in Anytime learning settings.}
\label{fig:conv}
\end{figure}

%93.623
% \begin{figure}[!b]
% \centering
% \includegraphics[width=0.9\textwidth]{images/convergence_band_reinit.pdf}
% \caption{A comparison between ResNets trained using
% a warm start, a random initialization, RIFLE, LLF, S\&P and LURE on CIFAR-10 $((|S_B|=2)$. }
% \label{fig:conv_reinit}
% \end{figure}


% \textcolor{blue}{\subsection{Anytime Learning at Macroscale (ALMA).}}
%  ALMA relates to a 

% \subsection{Effect of hyperparameters in the selection of extraneous information.}

% \begin{table}[t]
% \centering
% \caption{Evaluation of the model on CIFAR10 with different percentages of noisy labels.}
% \label{tab:noisy}
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llccc@{}}
% \toprule
% Noisy labels (\%) & Methods & Test Accuracy ($\uparrow$) & CER ($\downarrow$) & Generalization Gap ($\downarrow$) \\ 
% \midrule
% \multirow{5}{*}{0.0} & Baseline & 89.31 & 5657  & 7.90  \\
%      & Random init. &  & &\\
%  & LLF  & 91.80 & 5466  & 7.68   \\
%  & S\&P & 90.50 & 5667  & 7.80   \\
%  & LURE & \textbf{93.73} & \textbf{4409}  & \textbf{7.48}  \\ \midrule
% \multirow{5}{*}{0.2} & Baseline & 80.88 & 9460  & 18.44 \\
%      & Random init. &  & &\\
%  & LLF  & 85.80 & 7827  & 9.90    \\
%  & S\&P & 84.54 & 8081  & 12.41  \\
%  & LURE & \textbf{86.79} & \textbf{7551}  & \textbf{8.70}    \\ \midrule
% \multirow{5}{*}{0.4} & Baseline & 79.13 & 10854 & 20.76 \\
%      & Random init. &  & &\\
%  & LLF  & 82.74 & 9411  & 8.40    \\
%  & S\&P & 82.41 & 9839  & 10.48  \\
%  & LURE & \textbf{83.80} & \textbf{9291}  & \textbf{7.58}  \\
% \bottomrule
% \end{tabular}%}
% \end{table}


% 0	89.31	89.91	90.08	91.8	90.5	93.73
% 0.2	80.88	83.68	84.57	85.8	84.54	86.79
% 0.4	79.13	81.57	82.2	82.74	82.41	83.8


\end{document}
