{
    "arxiv_id": "2303.10455",
    "paper_title": "Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks",
    "authors": [
        "Vijaya Raghavan T. Ramkumar",
        "Elahe Arani",
        "Bahram Zonooz"
    ],
    "submission_date": "2023-03-18",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
    ],
    "abstract": "Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalizable features. We show that our training paradigm provides consistent performance gains across datasets in both classification and few-shot settings. We further show that it leads to more robust and well-calibrated models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10455v1"
    ],
    "publication_venue": "Published in Transactions on Machine Learning Research (TMLR)"
}