\section{Introduction}


The research branch of explainable artificial intelligence has yielded remarkable results, gradually opening the machine learning black boxes. The production of counterfactual explanations (CE) has become one of the promising pipelines for explainability, especially in computer vision~\cite{Rodriguez_2021_ICCV,Jeanneret_2022_ACCV,steex,Singla2020Explanation}. As a matter of fact, CE are an intuitive way to expose how an input instance can be minimally modified to steer the desired change in the model's output. More precisely, CE answers the following: \textit{what does $X$ have to change to alter the prediction from $Y$ to $Y'$?} From a user perspective, these explanations are easy to understand since they are concise and illustrated by examples. Henceforth, companies have adopted CE as an interpretation methodology to legally justify the decision-making of machine learning models~\cite{wachter2018CounterfactualExplanationsOpening}. To better appreciate the potential of CE, one may consider the following scenario: a client goes to a photo booth to take some ID photos, and the system claims the photos are invalid for such usage. Instead of performing random attempts to abide by the administration criteria, an approach based on CE could provide visual indications of what the client should fix.

 
The main objective of CE is to add minimalistic semantic changes in the image to flip the original model's prediction. Yet, these generated explanations must accomplish several objectives~\cite{wachter2018CounterfactualExplanationsOpening,Jeanneret_2022_ACCV,Rodriguez_2021_ICCV}. A CE must be \emph{valid}, meaning that the CE has to change the prediction of the model. Secondly, the modifications have to be \emph{sparse and proximal} to the input data, targeting to provide simple and concise explanations. In addition, the CE method should be able to generate \emph{diverse} explanations. If a trait is the most important for a certain class among other features, diverse explanations should change this attribute most frequently. Finally, the semantic changes must be \emph{realistic}. When the CE method inserts out-of-distribution artifacts in the input image, it is difficult to interpret whether the flipping decision was because of the inserted object or because of the shifting of the distribution, making the explanation unclear.
 

Adversarial attacks share a common goal with CE: flipping the classifier's prediction. For traditional and non-robust visual classifiers, generating these attacks on input instances creates imperceptible noise. Even though it has been shown that it contains meaningful changes~\cite{ilyas2019adversarial} and that adversarial noise and counterfactual perturbations are related~\cite{pmlr-v97-etmann19a,NEURIPS2019_7392ea4c}, adversarial attacks have lesser value. Indeed, the modifications present in the adversaries are unnoticeable by the user and leave him with no real feedback.


Contrary to the previous observations, many papers (\eg, \cite{perez2021enhancing}) evidenced that adversarial attacks toward {\em robust} classifiers generate semantic changes in the input images. This has led works~\cite{santurkar2019image,zhu2021towards} to explore robust models to produce data using adversarial attacks. In the context of counterfactual explanations, this is advantageous~\cite{boreiko2022sparse,pmlr-v130-schut21a} because the optimization will produce semantic changes to induce the flipping of the label. 


Then two challenges arise when employing adversarial attacks for counterfactual explanations.
On the one hand, when studying a classifier, we must be able to explain its behavior regardless of its characteristics. 
So, a naive application of adversarial attacks is impractical for non-robust models. 
On the other hand, according to~\cite{TsiprasSETM19}, robustifying the classifier yields an implicit trade-off by lowering the \emph{clean accuracy}, as referred by the adversarial robustness community~\cite{croce2020reliable}, a particularly crucial trait for high-stakes areas such as the medical field~\cite{mertes2022ganterfactual}. 


The previous remarks motivate our endeavor to mix the best of both worlds. Hence, in this paper, we propose robustifying brittle classifiers \emph{without} modifying their weights to generate CE. This robustification, obtained through a filtering preprocessing leveraging diffusion models~\cite{NEURIPS2020_4c5bcfec}, allows us to keep the performance of the classifier untouched and unlocks the production of CE through adversarial attacks.


We summarize the novelty of our paper as follows:
(i) We propose Adversarial Counterfactual Explanations, ACE in short, a novel methodology based on adversarial attacks to generate semantically coherent counterfactual explanations.
(ii) ACE performs competitively with respect to the other methods, beating previous state-of-the-art methods in multiple measurements along multiple datasets. 
(iii) Finally, we point out some defects of current evaluation metrics and propose ways to remedy their shortcomings.
(iv) To show a use case of ACE, we study ACE's meaningful and plausible explanations to comprehend the mechanisms of classifiers. We experiment with ACE findings producing actionable modifications in real-world scenarios to flip the classifier decision.

\edit{Our code and models are available on \href{https://github.com/guillaumejs2403/ACE}{GitHub}.}