
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.90\textwidth]{images/ace3.pdf}
    \caption{\textbf{Pre-explanation Construction and Refinement} ACE generates the counterfactual explanation in a two-step sequence. Initially, (a) To generate semantic updates in the input image, the DDPM processes the instance before computing the loss function $L_{class}(F_\tau(x'); y')$, where $y'$ is the target label. \edit{To simplify the process, we omit the distance loss between the perturbed image $x'$ and the input image $x$.}. 
    % Then, it uses the gradients of the image with respect to the criterion to update it. 
    \edit{Then, we compute the gradients with respect to $x'$ and update it using the adversarial attack.}
    Finally, (b) we generate a binary mask using the magnitude's difference between the explanation and input image to refine the pre-explanation using RePaint's inpainting method.}
    \label{fig:ace}
\end{figure*}

\section{Adversarial Counterfactual Explanations}

The key contribution of this paper is our novel Adversarial Counterfactual Explanations (ACE) method. 
ACE produces counterfactual images in two steps, as seen in Figure~\ref{fig:ace}. We briefly introduce these two steps here and detail them in the following sections.\\
{\em Step 1. Producing pre-explanation images (\S\ref{ssec:pre-exp}).} Let $L_{class}(x; y)$ be a function measuring the agreement between the sample $x$ and class $y$. This function is typically the cross-entropy loss of the classifier we are studying with respect to $y$. With ACE, generating the pre-explanation image of $(x,y)$ for the target class $y'\neq y$ consists in finding $x'$ minimizing  $L_{class}(F(x'); y')$ \edit{using the adversarial attack as the optimizer}. Here, $F(x')$ is a filtering function that constrains the attack to stay in the manifold of the training images. In a nutshell, the filtering process $F$ robustifies the fragile classifier under examination to generate semantic changes \emph{without} modifying its weights. \\
{\em Step 2. Bringing the pre-explications closer to the input images (\S\ref{ssec:repaint}).} The pre-explanation generation restricts only those pixels in the image that are useful in switching the output label from $y$ to $y'$. The rest of the pixels are only implicitly constrained by the design of $F$. Accordingly, the purpose of this second step is to keep these non-explicitly constrained pixels identical to those of the input image. 



\subsection{Pre-explanation generation with DDPMs}\label{ssec:pre-exp}

To avoid generating adversarial noise and producing useful semantics, the previously introduced function $F$ should have two key properties. (i) Removing high-frequency information that traditional adversarial attacks generate. Indeed, these perturbations could change the classifier's decision without being actionable or understandable by a human. (ii) Producing in-distribution images without distorting the input image. This property seeks to maintain the image structures not involved in the decision-making process as similar as possible while avoiding giving misleading information to the user.

Denoising Diffusion Probabilistic Models~\cite{NEURIPS2020_4c5bcfec}, commonly referred to as DDPM or diffusion models, achieve these properties if used properly. On the one hand, each inference through the DDPM is a denoising process; in particular, it removes high-frequency signals. On the other hand, DDPMs generate in-distribution images.


As a reminder, DDPMs rely on two Markov chains, one inverse to the other. The forward chain \emph{adds} noise from a state $t$ into $t+1$ while the reverse chain \emph{removes} it from $t+1$ to $t$. Noting $x_t$ the instance at time step $t$, the forward chain is directly simulated from a clean instance $x_0$ through 
\begin{equation}\label{eq:encode}
    x_t = \sqrt{\Bar{\alpha}_t} \, x_0 + \sqrt{1 - \Bar{\alpha}_t} \, \epsilon, \; \epsilon \sim \mathcal{N}(0,I),
\end{equation}
where $\Bar{\alpha}_t$ is a time-dependent constant. At inference, the DDPM produces a mean $\mu_t(x_t)$ and a deviation matrix $\Sigma_t(x_t)$. 
Using these variables, the next less noisy image is sampled from
\begin{equation}\label{eq:decode}
    x_{t-1} = \mu_t(x_t) + \Sigma_t(x_t) \, \epsilon, \; \epsilon \sim \mathcal{N}(0,I).
\end{equation}
Thus, the DDPM denoising algorithm iterates the previous step until $t=0$ arriving at an image without noise. Please refer to previous works~\cite{Dhariwal2021DiffusionMB,NEURIPS2020_4c5bcfec} for a thorough understanding of diffusion models. 

\textbf{ACE pre-explanation generation.} Starting from a query image $x$, we can obtain a filtered version by applying the forward DDPM process up to level $\tau$ (Eq.~\ref{eq:encode}) and then denoise it recursively thanks to the iterative DDPM denoising steps (Eq~\ref{eq:decode}) starting from level $t=\tau$. In this case, to highlight the use of this intermediate step $\tau$, we denote the diffusion filtering process as $F=\F_\tau$ (Figure~\ref{fig:ace}a). Thus, we optimize the image through the DDPM filtering process, $\F_\tau$, before computing the classification loss. Henceforth, we obtain the pre-explanations by optimizing 
\begin{equation}\label{eq:optim}
    \underset{x'}{\mathrm{argmin}}\; L_{class}(\F_\tau(x'); y') + \lambda_d\, d(x',x)
\end{equation}
\edit{using the adversarial attack of choice. Here,} $\lambda_d$ is a regularization constant and $d$ a distance function.

\subsection{Bringing the pre-explanations closer to the input images}
\label{ssec:repaint}

By limiting the value of $\tau$, the DDPM will not go far enough to generate a normal distribution, and the reconstruction will somehow preserve the overall structure of the image. However, we noted that a post-processing phase could help keep irrelevant parts of the image untouched. For example, in the case of faces, the denoising process may change the hairstyle while targeting the smile attribute. Since hairstyle is presumably uncorrelated with the smile feature, the post-process should neutralize those unnecessary alterations.

To this end, we first compute a binary mask $m$ delineating regions that qualify for modifications. 
To do so, we consider the magnitude difference between the pre-explanation and the original mask, we dilate this gray-scale image and threshold it, yielding the desired mask.
This matter being settled, we need to fuse the CE inside the mask along with the input outside the mask to accomplish our objective.

With that aim, a natural strategy is using inpainting methods.
So, we leverage RePaint's recent technique~\cite{Lugmayr_2022_CVPR}, originally designed for image completion, and adapt it to our {\em picture-in-picture} problem (Figure~\ref{fig:ace}b).
This adaptation straightforward and integrates very well with the rest of our framework. 
It starts from the noisy pre-explanations $x_\tau$ and iterate the following altered denoising steps:
\begin{equation}
    x_{t-1} = \mu_t(x_t') + \Sigma_t(x_t') \, \epsilon, \; \epsilon \sim \mathcal{N}(0,I),
\end{equation}
where $x'_{t} = x_t \cdot m + x^i_t \cdot (1 - m)$ is the raw collage of the current noisy reconstruction $x_t$ and the noisy version of the initial instance $x^i_t$ at the same noise level $t$, obtained with Eq.~\ref{eq:encode}. 
The final image, $x_0$, will be our counterfactual explanation -- identical to the input sample outside the mask, and very similar to the pre-explanation within the mask.
% At the end of this process, the final image $x_0$ will be identical to the input sample outside the mask, and very similar to the CE within the mask.
In the supplementary material, we added an overview of ACE.


% \subsection{Overview of ACE}
% In summary and as shown in Figure~\ref{fig:ace}, ACE is a two-step method: firstly is the pre-explanation construction and then the refinement process.
% \textbf{(1)} It adds noise to the input image using the forward Markov chain until an intermediate step $\tau$, \ie it doesn't begin from random Gaussian noise. 
% Instead, it warms up the generation with the input image through Eq.~\ref{eq:encode}. 
% \textbf{(2)}~ACE iteratively denoises the noisy image using the DDPM algorithm (Equation~\ref{eq:decode}). 
% \textbf{(3)}~It uses the scrutinized classifier to compute the gradients over a loss function with respect to the input image in step 1. 
% This step requires computing the gradient through the steps of the diffusion model. 
% \textbf{(4)}~ACE applies the gradients as the update step with the attack of choice. It iterates these four steps to create the pre-explanation.
% For the refinement, it creates the mask $m$ using the difference between the pre-explanation and the original input. Then, it dilates and thresholds it to generate the binary version. 
% Finally, ACE builds on RePaint to keep untouched any region lying outside of the mask. The final result is the counterfactual explanation.
