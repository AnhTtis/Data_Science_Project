\section{Related Work}

\textbf{Explainable AI.} The main dividing line between the different branches of explainable artificial intelligence stands between \textit{Ad-Hoc} and \textit{Post-Hoc} methods. The former promotes architectures that are interpretable by design~\cite{rymarczyk2021interpretable,Bohle_2022_CVPR,Bohle_2021_CVPR,Huang_2020_CVPR} while the latter considers analyzing existing models as they are. Since our setup lies among the Post-Hoc explainability methods, we spotlight that this branch splits into global and local explanations. The former explains the general behavior of the classifier, as opposed to a single instance for the latter. This work belongs to the latter. There are multiple local explanations methods, from which we highlight saliency maps~\cite{Jalwana_2021_CVPR,Wang_2020_CVPR_Workshops,Lee_2021_CVPR,8354201,Kim2022HIVE,zheng2022shap}, concept attribution~\cite{pmlr-v80-kim18d,NEURIPS2019_77d2afcb,kolek2022cartoon} and model distillation~\cite{tan2018learning,Ge_2021_CVPR}. Concisely, these explanations try to shed light on \emph{how} a model took a specific decision. In contrast, we focus on the on-growing branch of counterfactual explanations, which tackles the question: \emph{what}  does the model uses for a forecast? We point out that some novel methods~\cite{vandenhende2022making,pmlr-v97-goyal19a,wang2020scout,Wang_2021_CVPR} call themselves counterfactual approaches. Yet, these systems highlight regions between a pair of images without producing any modification. 


\textbf{Counterfactual Explanations.} CE have taken momentum in recent years to explain model decisions. 
Some methods rely on prototypes~\cite{looveren2021interpretable} or deep inversion~\cite{thiagarajan2021designing}, while other works explore the benefits of other classification models for CE, such as Invertible CNNs~\cite{hvilshoj2021ecinn} and Robust Networks~\cite{boreiko2022sparse,pmlr-v130-schut21a}. A common practice is using generative tools as they give multiple benefits when producing CE. In fact, using generation techniques is helpful to generate data in the image manifold. There are two modalities to produce CE using generative approaches. Many methods use conditional generation techniques~\cite{van2021conditional,Singla2020Explanation,looveren2021interpretable} to fit what a classification model learns or how to control the perturbations. Conversely, unconditional approaches~\cite{Rodriguez_2021_ICCV,nemirovsky2020countergan,Jeanneret_2022_ACCV,shih2021GANMEXOnevsoneAttributions,zhao2018GeneratingNaturalAdversarial,Khorram_2022_CVPR} optimize the latent space vectors. 

%Among the counterfactual approaches, we draw attention to Jeanneret~\etal~\cite{Jeanneret_2022_ACCV}'s work. This method uses a modified version of the guided diffusion~\cite{Dhariwal2021DiffusionMB} to steer the generation toward the target label, \edit{modifying the DDPM generation algorithm \textit{per se}.  In contrast, even when we use DDPM, we use them as a mere regularizer before the classifier. Hence,} we use adversarial attacks directly on the image space to generate semantic changes before post-processing it through the diffusion model without relying on controlling the generation process.  Finally, unlike previous methods, we use a refinement stage to perform the pertinent editings in only the regions of interest. 

We'd like to draw attention to Jeanneret~\etal~\cite{Jeanneret_2022_ACCV}'s counterfactual approach, which uses a modified version of the guided diffusion algorithm to steer image generation towards a desired label. This modification affects the DDPM generation algorithm itself. In contrast, while we also use DDPM, we use it primarily as a regularizer before the classifier. Instead of controlling the generation process, we generate semantic changes using adversarial attacks directly on the image space, and then post-process the image using a standard diffusion model.  Furthermore, we use a refinement stage to perform targeted edits only in regions of interest.


% In contrast, even when we use DDPM, we use adversarial attacks directly on the image space to generate semantic changes before post-processing it through the diffusion model without relying on controlling the generation process.


\textbf{Adversarial Attacks and their relationship with CE.} Adversarial attacks share the same main objective as counterfactual explanations: flipping the forecast of a target architecture. On the one hand, \textit{white-box} attacks~\cite{DBLP:journals/corr/GoodfellowSS14,madry2018towards,carlini2017towards,moosavi2016deepfool,croce2020reliable,Jeanneret_2021_ICCV} leverage the gradients of the input image with respect to a loss function to construct the adversary. In addition, universal noises~\cite{moosavi2017universal} are adversarial perturbations created for fooling many different instances. On the other hand, \textit{black-box} attacks~\cite{zhou2018transferable,poursaeed2018generative,ACFH2020square} restrain their attack by checking merely the output of the model. Finally, Nie~\etal~\cite{nie2022DiffPure} study DDPMs from a robustness perspective, disregarding the benefits of counterfactual explanations. 


In the context of CE for visual models, the produced noises are indistinguishable for humans when the network does not have any defense mechanism, making them useless. This lead works~\cite{NEURIPS2019_7392ea4c,akhtar2021attack,pawelczyk2022exploring} to approach the relationship between these two research fields. Compared to previous approaches, we manage to leverage adversarial attacks to create semantic changes in undefended models to explore their semantic weaknesses perceptually in the images; a difficult task due to the nature of the data.

