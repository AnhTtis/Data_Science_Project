\section{Experimentation}
\subsection{Evaluation Protocols and Datasets}

\input{sections/tables/celeba}

\noindent \textbf{Datasets.} In line with the recent literature on counterfactual images \cite{Rodriguez_2021_ICCV,Singla2020Explanation,Jeanneret_2022_ACCV,Joshi2018xGEMsGE}, first, we evaluate ACE on CelebA~\cite{liu2015faceattributes}, with images of size of $128\times128$ and a DenseNet121 classifier ~\cite{huang2017densely}, for the `smile' and `age' attributes. Following Jacob~\etal~\cite{steex}, we experimented on CelebA HQ~\cite{CelebAMask-HQ} and BDD100k~\cite{Yu2020BDD100KAD}. CelebA HQ has a higher image resolution of $256\times256$. BDD100k contains complex traffic scenes as $512\times256$ images; the targeted attribute is `forward' vs `slow down'. The decision model is also a DenseNet121, trained on the BDD-IOA~\cite{Xu2020ExplainableOA} extension dataset. Regarding the classifiers for which we want to generate counterfactuals, we took the pre-trained weights from DiME~\cite{Jeanneret_2022_ACCV} source for CelebA and from STEEX~\cite{steex} for CelebA HQ and BDD100k, for fair comparisons. 


\noindent \textbf{Evaluation criteria for quantitative evaluation.}\\
{\em Validity of the explanations} is commonly measured with the Flip Rate (\underline{FR}), \ie how often the CE is classified as the targeted label. \\
{\em Diversity} is measured by extending the diversity assessment from Mothilal~\etal~\cite{Mothilal2020ExplainingML}. As suggested by Jeanneret~\etal~\cite{Jeanneret_2022_ACCV}, the diversity is measured as the average LPIPS~\cite{Zhang_2018_CVPR_Unreasonable} distance between pairs of counterfactuals (\underline{$\sigma_L$}).\\
{\em Sparsity or proximity} has been previously evaluated with several different metrics~\cite{Rodriguez_2021_ICCV,Singla2020Explanation}, in the case of face images and face attributes. On the one hand, the mean number of attributes changed (\underline{MNAC}) measures the smallest amount of traits changed between the input-explanation pair. Similarly, this metric leverages an oracle network pretrained on VGGFace2~\cite{8373813} and then fine-tuned on the dataset. Further, Jeanneret~\etal~\cite{Jeanneret_2022_ACCV} showed the limitations of the MNAC evaluation and proposed the CD metric to account for the MNAC's limitations. On the other hand, to measure whether an explanation changed the identity of the input, the assessment protocol uses face verification accuracy~\cite{8373813} (\underline{FVA}). To this end, the evaluation uses a face verification network. However, FVA has 2 main limitations: i) it can be applied to face related problems only, ii) it works at the level of classifier decisions which turns out to be too rough when comparing an image to its CE, as it involves only a minimal perturbation. For face problems, we suggest skipping the thresholding and consider the mean cosine distance between the encoding of image-counterfactual pairs, what we refer to as Face Similarity (\underline{FS}). To tackle non-face images, we propose to extend FS by relying on self-supervised learning to encode image pairs. To this end, we adopted SimSiam~\cite{Chen_2021_CVPR} as an encoding network to measure the cosine similarity. We refer to this extension as SimSiam Similarity (\underline{$S^3$}).  
Finally, also for classifiers that are not related to faces, Khorram \etal~\cite{Khorram_2022_CVPR} proposed \underline{COUT} to measure the transition probabilities between the input and the counterfactual.\\
\noindent{\em Realism of counterfactual images}~\cite{Singla2020Explanation} is usually evaluated by the research community with the \underline{FID}~\cite{NIPS2017_8a1d6947} between the original set and the valid associated counterfactuals. We believe there is a strong bias as most of the pixels of counterfactuals are untouched and will dominate the measurement, as observed in our ablation studies (Sec.~\ref{sec:ablations}). To remove this bias, we split the dataset into two sets, generating the CE for one set and measuring the FID between the generated explanations and the other set,  iterating this process ten times and taking the mean. We call this metric \underline{sFID}.


\noindent \textbf{Implementation details.} One of the main obstacles of diffusion models is transferring the gradients through all the iterations of the iterative denoising process. Fortunately, diffusion models enjoy a time-step re-spacing mechanism, allowing us to reduce the number of steps at the cost of a quality reduction. So, we drastically decreased the number of sampling steps to construct the pre-explanation. For CelebA~\cite{liu2015faceattributes}, we instantiate the DDPM~\cite{Dhariwal2021DiffusionMB} model using DiME's~\cite{Jeanneret_2022_ACCV} weights. In practice, we set $\tau=5$ out of 50 steps. For CelebA HQ~\cite{CelebAMask-HQ}, we fixed the same $\tau$, but we used the re-spaced time steps to 25 steps. For BDD100k~\cite{Yu2020BDD100KAD}, we follow the same settings as STEEX~\cite{steex}: we trained our diffusion model on the 10.000 image subset of BDD100k. To generate the explanations, we used 5 steps out of 100. Additionally, all our methods achieve a success ratio of 95\% at minimum. We will detail in the supplementary material all instructions for each model on every dataset. We adopted an $\ell_1$ or $\ell_2$ distance for the distance function. Finally, for the attack optimization, we chose the PGD~\cite{madry2018towards} without any bound and with 50 optimization steps.


 

\subsection{Comparison Against the State-of-the-Art}

\input{sections/tables/hqceleba}
\input{sections/tables/bbd}

In this section, we quantitatively compare ACE against previous State-of-the-Art methods. To this end, we show the results for CelebA~\cite{liu2015faceattributes} and CelebA HQ~\cite{CelebAMask-HQ} datasets in Table~\ref{tab:celeba-main} and Table~\ref{tab:celebahq-main}, respectively. Additionally, we experimented on the BDD100k~\cite{Yu2020BDD100KAD} dataset (Table~\ref{tab:bdd-main}). To extend the study of BDD, we further evaluated our proposed approach on the BDD-IOA~\cite{Xu2020ExplainableOA} validation set, also presented in Table~\ref{tab:bdd-main}. Since DiME~\cite{Jeanneret_2022_ACCV} showed superior performance over the literature~\cite{Rodriguez_2021_ICCV,Singla2020Explanation,Joshi2018xGEMsGE}, we compare only to DiME. 

DiME experimented originally on CelebA only. Hence, they did not tune their parameters for CelebA HQ and BBD100k. 
By running their default parameters, DiME achieves a flip rate of 41\% in CelebA HQ. We fix this by augmenting the scale hyperparameter for their loss function. DiME's new success rate is 97\% for CelebA HQ. For BDD100k, our results showed that using fewer steps improves the quality. Hence, we used 45 steps out of their re-spaced 200 steps. Unfortunately, we only managed to increase their success ratio to 90.5\%. 

These experiments show that the proposed methodology beats the previous literature on most metrics for all datasets. 
For instance, ACE, whatever the chosen distance, outmatches DiME on all metrics in CelebA. 
For the CelebA HQ, we noticed that DiME outperforms ACE only for the COUT and CD metrics. 
Yet, our proposed method remains comparable to theirs. 
For BDD100k, we remark that our method consistently outperforms DiME and STEEX. 

Two additional phenomena stand out within these results. 
On the one hand, we observed that the benefit of favoring $\ell_1$ over $\ell_2$ depends on the characteristics of the target attribute.
We noticed that the former generates sparser modifications, while the latter tends to generate broader editing. 
This makes us emphasize that different attributes require distinct modifications. 
On the other hand, these results validate the extensions for the FVA and FID metrics. 
Indeed, the difference between the FVA values on CelebA are small (from 98.3 to 99.9).  
Yet, the FS shows a major increase. 
Further, for the Age attribute on CelebA HQ, ACE $\ell_2$ shows a better performance than DiME for the FID metric. 
The situation is reversed with sFID as DiME is slightly superior.

To complement our extensive experimentation, we tested ACE on a small subset of classes on ImageNet~\cite{deng2009imagenet} with a ResNet50. 
We selected three pairs of categories for the assessment, and the task is to generate the CE targeting the contrary class. 
For the FID computation, we used only the instances from both categories but not external data since we are evaluating the in-class distribution.

We show the results in Table~\ref{tab:imagenet-main}. Unlike the previous benchmarks, ImageNet is extremely complex and the classifier needs multiple factors for the decision-making process. Our results reflect this aspect. 
We believe that current advancements in CE still need an appropriate testbed to validate the methods in complex datasets such as ImageNet. For instance, the model uses the image's context for forecasting. So, choosing the target class without any previous information is unsound. 
% For instance, when using the $\ell_1$ distance, ACE's flip rate is low in comparison to the $\ell_2$ variant. 
% Additionally, we observe that transforming Zebras to Sorrel Horses and viceversa is a challenging task for ACE. For instance, the COUT metric tells us that most valid instances merely passed the decision boundary. Finally, the Cheetah -- Cougar tuple has an intermediate difficulty level while the Egyptian -- Persian cat is the easiest among the chosen classes. Nevertheless, this experiment is a small leap towards evaluating complex datasets. 

\subsection{Diversity Assessment}


In this section, we explore ACE's ability to generate diverse explanations. 
Diffusion models are, by design,  capable of generating distributions of images.
Like \cite{Jeanneret_2022_ACCV}, we take advantage of the stochastic mechanism to generate perceptually different explanations by merely changing the noise for each CE version.
Additionally, for a fair comparison, we do not use the RePaint's strategy here because DiME does not have any local constraints and can, as well, change useless structures, like the background. 
To validate our approach, we follow \cite{Jeanneret_2022_ACCV} assessment protocol.
Numerically, we obtain a diversity score of $\sigma_L=0.110$ while DiME reports 0.213. 
Since DiME corrupts the image much more than ACE, the diffusion model has more opportunities to generate distinct instances. 
In contrast, we do not go deep into the forward noising chain to avoid changing the original class when performing the filtering. 

To circumvent the relative lack of diversity, we vary the re-spacing at the refinement stage and the sampled noise. 
Note that later in the text, we show that using all steps without any re-spacing harms the success ratio. 
So, we set the new re-spacing such that it respects the accuracy of counterfactuals and fixed the variable number of noise to maintain the ratio between $\tau$ and the re-spaced number of sampling steps ($\nicefrac{5}{50}$ in this case). 
Our  diversity score is then of 0.1436. 
Nevertheless, DiME is better than ACE in terms of diversity, but this is at the expense of the other criteria, because its diversity comes, in part, from regions of the images that should not be modified (for example, the background).






\subsection{Qualitative Results}

\input{sections/tables/imagenet}

\begin{figure*}[t]
 \centering
 \includegraphics[width=0.95\textwidth]{images/qualitative.pdf}
 \caption{\textbf{Qualitative Results.} ACE create sparse but realistic changes in the input image. Further, ACE enjoys from the generate mask, which helps in understanding which and where semantic editing were added. The first row displays the input images, the second one the counterfactual explanations and the third the corresponding mask.}
 \label{fig:qualitative}
 \vspace{-3mm}
\end{figure*}

We show some qualitative results in Figure~\ref{fig:qualitative} for all datasets, included some ImageNet examples. From an attribute perspective, some have sparser or coarser characteristics. For instance, age characteristics cover a wider section of the face, while the smile attribute is mostly located in small regions of the image. Our qualitative results expose that different distance losses impose different types of explanations. For this case, $\ell_1$ loss exposes the most local and concrete explanations. On the other hand, the $\ell_2$ loss generates  coarser editing. This feature is desired for certain classes, but it is user-defined. 
Additionally, we note that the generated mask is useful to spot out the location of the changes. 
This is advantageous as it exemplifies which changes were needed and where they were added. 
Most methods do not indicate the localization of the changes, making them hard to understand. 
In the supplementary material, we included more qualitative results.

\subsection{Actionability}

\begin{figure}
 \centering
 \includegraphics[width=0.35\textwidth]{images/accionability.pdf}
 \caption{\textbf{Actionability.} From browsing our counterfactuals, we found two weaknesses of the scrutinized classifier. Row 1: We tested if a frown could change the classification from young to old. Row 2: we checked if having high cheekbones flipped is enough to classify someone as smiling. Both experiments were successful.}
 \label{fig:action}
 \vspace{-3mm}
\end{figure}

Counterfactual explanations are expected to teach the user plausible modifications to change the classifier's prediction. 
In this section, we study a batch of counterfactual-input tuples generated with our method. 
If ACE is capable of creating useful counterfactual explanations, we should be qualified to understand some weaknesses or some behaviors of our classifier. 
Additionally, we should be able to fool the classifier by creating the necessary changes in real life. To this end, we studied the CelebA HQ classifier for the age and smile attributes. 


After surveying some images and their explanations, we identified two interesting results (Figure~\ref{fig:action}). 
Many of the counterfactual explanations changing from `young' to `old' evidence that frowning could change the prediction of the classifier. 
So, we tested this hypothesis in the real life. 
We took a photo one individual before and after the frown, avoiding changing the scenery.
We were successful and managed to change the prediction of the classifier. 
For smile, we identified a spurious correlation. 
Our counterfactuals show that the classifier uses the morphological trait of high cheekbones to classify someone as smiling as well as having red cheeks. 
So, we tested whether the classification model wrongly predicts as smiling someone with high cheekbones  even when this person  is not smiling. We also tested whether we can enhance it with some red make up in the cheeks. 
Effectively, our results show that having high cheekbones is a realistic adversarial feature toward the smiling attribute for the classifier. Also, the classifier confidence (probability) can be strengthened by adding some red make up in the cheeks. 
These examples demonstrate the applicability of ACE in real scenarios.

\subsection{Ablation Studies}\label{sec:ablations}

In this section, we scrutinize the differences between the pre-explanation and the refined explanations. Then we  explore the effects of using other types of adversarial attacks. Finally, we show that the $S^3$ metric  gives similar results as the FVA, as a sanity check.

\noindent \textbf{Pre-Explanation \textit{vs} Counterfactual Explanations.}
We explore here, quantitatively and qualitatively, the effects of the pre-explanations (Pre-CE). 
%Additionally, we use the diffusion model without any inpainting strategy to filter the explanations with (FR-CE) and without (F-CE) the re-spacing method. 
Also, we apply the diffusion model for the explanations with and without the re-spacing method, using no inpainting strategy, referred as FR-CE and F-CE, respectively.
Finally, we compare them against the complete model (ACE). 
To quantitatively compare all versions, we conducted this ablation study on the CelebA dataset for both `smile' and `age' attributes. 
We assessed the components using the FID, sFID, MNAC, CD, and FR metrics. 
We did not include the FVA or FS metrics, as these values did not vary much and do not provide insightful information; the FVA is $\sim$99.9 and FS $\sim$0.87 for all versions. 

We show the results in Table~\ref{tab:ablation-filtering}. 
We observe that pre-explanations have a low FID. 
Nonetheless, their sFID is worse than the F-CE version. 
As said before, we noticed that including both input and counterfactual in the FID assessment introduces a bias in the final measurement, and this experiment confirms this phenomenon. 
Additionally, one can check that the MNAC metric between the pre-explanation and the FR-CE version does not vary much, yet, the CD metric for the FR-CE is much better. 
This evidences that the generative model can capture the dependencies between the attributes. 
Also, we notice that the flip rate (FR) is much lower when using all diffusion steps instead of the re-spaced alternative. 
We expected this behavior, since we create the pre-explanation to change the classifier's prediction with re-spaced time steps within the DDPM. 


Qualitatively, we point out to Figure~\ref{fig:abl-filt}, where we exemplify the various stages of ACE. 
For instance, we see that the pre-explanation contains out of distribution artifacts and how the refinement sends it back to the image distribution. 
Also, we highlight that the filtering modifies the hair, which is not an important trait for the classifier. 
The refinement is key to avoid editing these regions.

\input{sections/tables/ablation-filtering}

\begin{figure}[t]
 \centering
 \includegraphics[width=0.45\textwidth]{images/abla-filt.pdf}
 \caption{\textbf{Refinement Ablation.} We observe that  pre-explanations can have out-of-distribution artifacts. After filtering them, the diffusion process creates in-distribution data, but there are unnecessary changes such as the background. ACE is capable of changing the key features while avoiding modifying unwanted structures.}
 \label{fig:abl-filt}
\end{figure}

\input{sections/tables/s3}

\noindent\textbf{Effect of Different Adversarial Attacks.}
At the core of our optimization, we have the PGD attack. 
PGD is one of the most common attacks due to its strength. 
In this section, we explore the effect of incorporating other attacks.
Thus, we tested C\&W~\cite{carlini2017towards} and the standard gradient descent (GD). 
Note that the difference between PGD and GD is that GD does not apply the $sign$ operation. 

Our results show that these attacks are capable of generating semantic changes in the image. 
Although these are as successful as the PGD attack, we require optimizing the pre-explanation for twice as many iterations. 
Even when our model is faster than~\cite{Jeanneret_2022_ACCV} \edit{--3.6 times faster--}, we require about 500 DDPM iterations to generate an explanation. 


\noindent\textbf{Validity of the $S^3$ Metric.}
In this experiment, we show that the $S^3$ and the FS metrics are equivalent when used in the same test bed, \ie, CelebA HQ. 
To this end, we assess whether the ordering between ACE, pre-explanation, and DiME are equal. 
To have a reference value, we evaluate the measurements when using a pair of random images.
So, we show the values (ordering) for both metrics in Table~\ref{tab:s3} for the Age and Smiling attribute. 
As we expect, the ordering is similar between both metrics. 
Nevertheless, we stress that FS is adequate for faces since the network was trained for this task.


% \input{sections/tables/s3}

