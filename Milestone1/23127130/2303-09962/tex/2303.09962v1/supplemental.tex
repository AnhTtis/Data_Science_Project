\vspace{1cm}
\appendix

{\centering \textbf{\LARGE Supplementary Material
}}

%%%%%%%%% Implementation Details
\section{Detailed Implementation Details}

For each dataset, we used different configurations in architecture and for the generation of the pre-explanation. 
\edit{Yet, we tune all hyperparameters from a empirical perspective\footnote{Note that all these hyperparameters are not the same as the classically found in machine learning. These variables can be adjusted by the user in an `online' manner according to his/her expectations. Hence, a global configuration is a mere rough estimate of these parameters and can be accommodated instance-wise.}. 
We tuned $\tau$ such that the input image and its filtered instance are visually similar. Additionally, the classification between these two images are the same.
To adjust the hyperparameter $\lambda_d$, we performed a simple visual inspection.
Finally, for the threshold, we ablated its values empirically for each dataset.}
When using the distance loss $\ell_1$, we set the distance regularization constant to $\lambda_d = 0.001$ while $\lambda_d = 0.1$ for $\ell_2$.
For the final refinement, firstly, we normalize the mask by the maximum pixel's difference magnitude. 
For the dilation step, we set the mask as a square with a width and height of 15 pixels for all datasets. 
Finally, we used the cross entropy for all experiments as the $L_{class}$ loss.  
Next, we will show all implementation details for each dataset.


\textbf{CelebA}~\cite{liu2015faceattributes}: We used the same architecture and weights as~\cite{Jeanneret_2022_ACCV}. 
Additionally, we set $\tau=5$ with a total amount of steps as $50$. 
At the refinement stage, we used the same threshold of 0.15 for both $\ell_1$ and $\ell_2$ experiments for smile and age attributes.


\textbf{CelebA HQ}~\cite{CelebAMask-HQ}: Our model follows the same architecture than\cite{Dhariwal2021DiffusionMB} for ImageNet $256\times256$ unconditional generation. 
Since CelebA HQ is far less complex than ImageNet, we reduced the number of channels from 256 to 128. 
Also, our model generates samples using $500$ diffusion steps instead of $1000$. 
For training, we iterated our model for $120.000$ iterations with a batch size of 256 on two V100 GPUs following\cite{Dhariwal2021DiffusionMB}'s code. We set the learning rate to $10^4$, a weight decay of $0.05$, and no dropout.

To generate the pre-explanations, we noise the image until $\tau=5$ out of $25$ re-spaced steps. To binarize the mask, we used a threshold of $0.15$ and $0.1$ for the smiling attribute with the $\ell_1$ and $\ell_2$ distance losses, respectively. For the age attribute, we used $0.15$ for $\ell_1$ and $0.05$ for $\ell_2$.

\textbf{BDD100k/OIA}~\cite{Yu2020BDD100KAD,Xu2020ExplainableOA}: The counterfactual explanation research community opted to use BDD100k in a $512\times256$ setup. This is highly demanding computationally to create a DDPM. Thus, since we knew \textit{a priori} that we do not need many iterations for ACE to generate counterfactuals, we trained our diffusion model partially in the Markov chain. That is, our DDPM cannot generate images from pure noise. Instead, we trained it to generate images solely from a quarter of the complete chain, requiring an input instance to warm up the generation. So, we trained our model to generate instances with 250 steps out of 1000. 
This enabled us to use a lighter model. Artitecnologically, our UNet model has four downsampling stages with $128\,s$ channels, where $s$ is the downsampling stage. Finally, we used the attention layer at the deeper layer of the UNet. 
At the training phase, we used a batch size of $256$, a learning rate of $10^4$, and a weight decay and dropout of $0.05$ for $50.000$ iterations. 

To generate our explanations, we used $5$ out of $100$ (re-spaced) diffusion steps. For $\ell_1$, we used a threshold of 0.05 and 0.1 for $\ell_2$ for both datasets.

\textbf{ImageNet}~\cite{deng2009imagenet}: For this dataset, we took advantage of previous works. In this case, we utilised~\cite{Dhariwal2021DiffusionMB}'s model on ImageNet 256. To generate the explanations, we used 5 steps out of 25 for the pre-explanations and set the threshold to 0.15 to binarize the mask for all cases.


\edit{
\section{Overview of ACE}
ACE is a two-step method: firstly is the pre-explanation construction -- Algorithm~\ref{alg:pe} -- and then the refinement process -- Algorithm~\ref{alg:ce}.
To generate the pre-explanation, \textbf{(1)}~we add noise to the input image $x$ using the forward Markov chain until an intermediate step $\tau$, \ie it doesn't begin from random Gaussian noise. 
Instead, it warms up the generation with the input image through 
\begin{equation*}
    x_t = \sqrt{\Bar{\alpha}_t} \, x + \sqrt{1 - \Bar{\alpha}_t} \, \epsilon, \; \epsilon \sim \mathcal{N}(0,I).
\end{equation*}
\textbf{(2)}~ACE iteratively denoises the noisy image using the DDPM algorithm with
\begin{equation*}
    x_{t-1} = \mu_t(x_t) + \Sigma_t(x_t) \, \epsilon, \; \epsilon \sim \mathcal{N}(0,I),
\end{equation*}
where $\mu_t$ and $\Sigma_t$ are the output of the diffusion model. 
\textbf{(3)}~The scrutinized classifier uses the filtered image to compute loss function. Then, we calculate the gradients with respect to the input image $x$ in step 1, all the way through the $\tau$ steps of the diffusion model. 
\textbf{(4)}~ACE applies the gradients as the update step with the attack of choice. It iterates these four steps to create the pre-explanation.
For the refinement, it creates the mask $m$ using the difference between the pre-explanation and the original input. Then, it dilates and thresholds it to generate the binary version. 
Finally, ACE builds on RePaint to keep untouched any region lying outside the mask. The final result is the counterfactual explanation.
}

\begin{algorithm}
\caption{Pre-explanation generation}\label{alg:pe}
\begin{algorithmic}[1]
\Require Diffusion Model $D$, Distance loss $d$ and its regularization constant $\lambda_d$, classification loss $L_{class}$ comprising the classifier under observation, number of noising steps $\tau$, attack optimization algorithm $PGD$, number of update iterations $n$, initial instance $x$, target label $y$
\Function{Pre-explanation}{x, y}
\State $n \leftarrow 0$
\State $x_{orig} \leftarrow x$
\While{$n < N$}
\Comment{Attack iteration steps}
\State $\epsilon \sim \mathcal{N}(0, I)$
\State $x' \leftarrow \sqrt{\Bar{\alpha}_\tau} x + \sqrt{1 - \Bar{\alpha}_\tau} \epsilon$
\Comment{Add noise}
\State $ts \leftarrow \tau - 1$
\While{$ts \geq 0$}
\Comment{DDPM denoising}
\State $\mu, \Sigma \leftarrow D(x', ts)$
\State $\epsilon \sim \mathcal{N}(0, I)$
\State $x' \leftarrow \mu + \epsilon \Sigma$
\State $ts \leftarrow ts - 1$
\EndWhile
\State $g \leftarrow \nabla_{x'} L_{class}(x'; y') + \lambda_d d(x',x_{orig})$
\State $x \leftarrow PGD(x, g)$
\Comment{Update with attack}
\State $n + 1 \leftarrow n$ 
\EndWhile
\State \Return $x'$
\Comment{Pre-explanation}
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Post-processing}\label{alg:ce}
\begin{algorithmic}[1]
\Require Diffusion Model $D$, number of noising steps $\tau$, mask dilation size $d$, threshold $u$, initial instance $x$, pre-explanation $x'$
\Function{Post-processing}{x, x'}
\State $x_{orig} \leftarrow x$
\State $\epsilon \sim \mathcal{N}(0, I)$
\State $x' \leftarrow \sqrt{\Bar{\alpha}_\tau} x' + \sqrt{1 - \Bar{\alpha}_\tau} \epsilon$
\State $ts \leftarrow \tau - 1$
\State \# Mask generation
\State $m \leftarrow sum\_over\_channels(abs(x - x'))$
\State $m \leftarrow \nicefrac{m}{maximum(m)}$
\State $m \leftarrow dilation(m, size=d) > u$
\While{$ts \geq 0$}
\Comment{DDPM denoising}
\State $\epsilon \sim \mathcal{N}(0, I)$
\State $x_{ts} \leftarrow \sqrt{\Bar{\alpha}_{ts}} x + \sqrt{1 - \Bar{\alpha}_{ts}} \epsilon$
\State $x' \leftarrow m\,x' + (1 - m)\,x_{ts}$
\State $\mu, \Sigma \leftarrow D(x', ts)$
\State $\epsilon \sim \mathcal{N}(0, I)$
\State $x' \leftarrow \mu + \epsilon \Sigma$
\State $ts \leftarrow ts - 1$
\EndWhile
\State \Return $x'$
\Comment{Counterfactual explanation}
\EndFunction
\end{algorithmic}
\end{algorithm}




\section{Qualitative Results}

In this section, we show more qualitative results. We will display the input image, its pre-explanation, the mask, and the final counterfactual for both $\ell_1$ and $\ell_2$ losses on all datasets. Note that we added a small discussion on the caption analyzing the results. In Fig.~\ref{soup:fig:acevsdime}, we compare a few examples of DiME and ACE.





\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{soup-small/suppl1.png}
    % \includegraphics[width=0.9\textwidth]{images-soup/suppl1.pdf}
    \caption{Additional CelebA qualitative results. We show examples for the \emph{Smiling} attribute for both distances losses. From our qualitative experiments, we see that removing the smile attributes is harder than adding them. Additionally, we see that the $\ell_1$ loss creates more sparse editings.}
    \label{soup:fig:celeba-smile}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{soup-small/suppl2.png}
    % \includegraphics[width=0.9\textwidth]{images-soup/suppl2.pdf}
    \caption{Additional CelebA qualitative results. We show examples for the \emph{Age} attribute for both distances losses. The results show that the $\ell_1$ loss creates more out-of-distribution artifacts.}
    \label{soup:fig:celeba-smile}
\end{figure*}


\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\textwidth]{images-soup/suppl3.pdf}
    \includegraphics[width=0.9\textwidth]{soup-small/suppl3-small.png}
    \caption{Additional CelebA HQ qualitative results. We show examples for the \emph{Smiling} attribute for both distances losses. We see similar behavior in the CelebA dataset.}
    \label{soup:fig:celeba-smile}
\end{figure*}

\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\textwidth]{images-soup/suppl4.pdf}
    \includegraphics[width=0.9\textwidth]{soup-small/suppl4-small.png}
    \caption{Additional CelebA HQ qualitative results. We show examples for the \emph{Age} attribute for both distances losses. These examples show that transforming \textit{Old} to \textit{Young} is less informative than the other way.}
    \label{soup:fig:celeba-smile}
\end{figure*}


\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\textwidth]{images-soup/suppl5.pdf}
    \includegraphics[width=0.9\textwidth]{soup-small/suppl5-small.png}
    \caption{Additional BDD qualitative results. We show examples for the \emph{Forward / Slow Down} binary class for $\ell_2$ distance loss. We show a zoom of the changes in the image since the perturbations are sparse. We see that ACE adds traffic light colors in the buildings to change the prediction.}
    \label{soup:fig:celeba-smile}
\end{figure*}


\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\textwidth]{images-soup/suppl6.pdf}
    \includegraphics[width=0.9\textwidth]{soup-small/suppl6-small.png}
    \caption{Additional BDD qualitative results. We show examples for the \emph{Forward / Slow Down} binary class for $\ell_1$ distance loss. We show a zoom of the changes in the image since the perturbations are sparse. We show a zoom of the changes in the image since the perturbations are sparse. We see that ACE adds traffic light colors in the buildings to change the prediction.}
    \label{soup:fig:celeba-smile}
\end{figure*}

\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\textwidth]{images-soup/in1.pdf}
    \includegraphics[width=0.9\textwidth]{soup-small/in1-small.png}
    \caption{Additional ImageNet qualitative results. We show examples for the \emph{Zebra / Sorrel} categories class. The first column is the $\ell_1$ distance loss while the second one is $\ell_2$. The initial row is zebra to sorrel and the second one is the inverse. To change from zebras to sorrels, some examples show not only incorporating the brown color sorrel horses but also the context in the background (\eg adding a stable-like background). Vice-versa, to classify a horse as a zebra it is enough to add some strips.}
    \label{soup:fig:celeba-smile}
\end{figure*}


\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\textwidth]{images-soup/in2.pdf}
    \includegraphics[width=0.9\textwidth]{soup-small/in2-small.png}
    \caption{Additional ImageNet qualitative results. We show examples for the \emph{Cheetah / Cougar} categories class. The first column is the $\ell_1$ distance loss while the second one is $\ell_2$. The first row is cheetah to cougar and the second is the inverse. We mainly see that changing from cheetah to cougar is enough to target the face of the animal. Vice-versa, to classify a cougar as a cheetah, ACE adds spots and characteristic cheetah stripes on the face.}
    \label{soup:fig:celeba-smile}
\end{figure*}


\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\textwidth]{images-soup/in3.pdf}
    \includegraphics[width=0.9\textwidth]{soup-small/in3-small.png}
    \caption{Additional ImageNet qualitative results. We show examples for the \emph{Egyptian / Persian cat} categories class. The first column is the $\ell_1$ distance loss while the second one is $\ell_2$. The row is Egyptian to Persian cat and the second is the inverse. To change from Egyptian to Persian, we mainly see that ACE adds the Persian cats' fluffy fur. Conversely, from Persian to Egyptian it adds spots.}
    \label{soup:fig:celeba-smile}
\end{figure*}



\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.85\textwidth]{images-soup/acevsdime.pdf}
    \includegraphics[width=0.85\textwidth]{soup-small/acevsdime.png}
    \caption{ACE \textit{vs.} DiME. We display some examples showing some differences between DiME counterfactuals and ACE's. In short, ACE is capable of not modifying useless information, such as the background, to generate its counterfactuals. Top row: CelebA. Bottom row: CelebA HQ. Left Column: Smiling attribute. Right Column: Age attribute.}
    \label{soup:fig:acevsdime}
\end{figure*}

