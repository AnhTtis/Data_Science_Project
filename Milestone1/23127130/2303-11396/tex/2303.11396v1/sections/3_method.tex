\section{Method}

The objective of our work is to texture a 3D mesh using a pretrained text-to-image diffusion model. 
In this section, we begin by laying the foundation of the diffusion model in Sec.~\ref{sec: preliminary} and depth-aware inpainting model in Sec.~\ref{sec: inpainting}. 
We then propose a \textit{generate-then-refine} scheme for progressively synthesizing and updating the 3D textures in a coarse-to-fine fashion.
In the progressive texture generation (Sec.~\ref{sec: generation}), we paint the visible regions of the input geometry in an incremental fashion, following a sequence of predefined viewpoints. To ensure local and global consistency, we incorporate a view partition to guide the depth-aware inpainting objectives. 
Subsequently, we introduce an automatic viewpoint selection mechanism  (Sec.~\ref{sec: refinement}) to perform texture refinement and address any issues of texture stretching and inconsistency.

\subsection{Preliminary}
\label{sec: preliminary}

We use a Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020denoising} as the generative model. 
Specifically, to avoid high computational overhead, we adopt the latent diffusion model~\cite{Rombach_2022_CVPR}, where an input image $x_0$ is first encoded into latent code $z_0$ before the diffusion process. 
The forward pass follows a Markov Chain to gradually add noise to the input latent code $z_0$ towards the white Gaussian noise $\mathcal{N}(0, 1)$. At each step in the forward pass, the noised latent code $z_t$ is obtained by adding a noise variance $\beta_t$ to the previous latent code $z_{t-1}$ scaled with $\sqrt{1-\beta_t}$:

\begin{equation}
    z_t \sim \mathcal{N}(\sqrt{1-\beta_t} z_{t-1}, \beta_t\textbf{I}).
\end{equation}
The independence property enables direct transformation of the noised latent code $z_t$ at an arbitrary time step $t$ from the input latent $z_0$ via:
\begin{equation}
    z_t \sim \mathcal{N}(\sqrt{\bar{a}}_t z_0, (1-\bar{a}_t)\textbf{I}),
\end{equation}
where $\bar{a}_t$ is the total noise variance, which can be calculated by~$\sum_{t=i}^{T}(1-\beta_{t})$ from the noise $\beta_t$ added to the input latent code $z_0$ at each time step.

During inference, the latent estimation $\hat{z}_{t-1}$ for the next time step $t-1$ is obtained by predicting $\mu_\theta(z_t, t)$ and $\sigma_\theta(z_t, t)$ of a Gaussian distribution:

\begin{equation}
    \hat{z}_{t-1} \sim \mathcal{N}(\mu_\theta(z_t, t), \sigma_\theta(z_t, t))
\end{equation}

\paragraph{Denoising strength.}

To prevent complete randomness during the diffusion process, we introduce a scaling factor $\gamma, 0 < \gamma \le 1$, which controls the number of diffusion steps. We assume that a white Gaussian noise $\mathcal{N}(0, 1)$ can be obtained by adding noise to the input latent code $z_0$ through $T$ steps, and the final denoised latent estimation $\hat{z}_{0}$ is fully governed by the pure noise. By applying the scaling factor, we can start denoising the latent code at time step $\gamma T$ to guide the final latent code with the original image information. This technique is applied to refine the previously generated image contents.


\subsection{Depth-Aware Image Inpainting}
\label{sec: inpainting}

The core of the texture synthesis lies in painting the missing regions on the mesh surface. 
The generated texture is expected to be highly faithful to the mesh geometry and the input text. 
To achieve this, we build our method on a pre-trained depth-to-image model~\cite{stablediffusion, zhang2023adding} that can produce high-quality images from text while being consistent with depth cues. 
However, since the Depth2Image model is designed to generate entire images, we need to use an inpainting mask to guide the sampling process. 
This mask provides explicit hints of which regions to generate or keep fixed, similar to the denoising guidance strategy in RePaint~\cite{lugmayr2022repaint}. 

To condition the denoising on the known regions of the input, we inject a generation mask $\mathcal{M}$ into the sampling steps. This mask explicitly blends the noised latent code $z_t$ and the denoised latent estimation $\hat{z}_t$ as follows:

\begin{equation}
    \hat{z}_t = \hat{z}_t \odot \mathcal{M} + z_t \odot (1 - \mathcal{M}).
\end{equation}
We then decode the final denoised latent estimation $\hat{z}_0$ to the final output image $\mathcal{O}$.

\input{figures/generation}
\subsection{Progressive Texture Generation}
\label{sec: generation}

With the customized Depth2Image model,  we are able to paint the object with a high quality image from a single view.
To synthesize the appearance of the input geometry, we project the generated 2D views onto the texture space of a normalized 3D object with proper UV parameterization. 
Assuming Y-axis as the up-axis in the world coordinate system, we define the viewpoint as $v=(\theta, \phi, r)$, where $\theta$ is the azimuth angle with respect to the Z-axis, $\phi$ is the viewpoint elevation angle with respect to the XZ-plane, and $r$ is the distance between the viewpoint and the origin.

As shown in Fig.~\ref{fig:progressive}, we start by generating the visible but missing texture in an initial viewpoint $v^0$. We render the object to a depth map $\mathcal{D}^0$ and a generation mask $\mathcal{M}^0$, and then use a customized Depth2Image diffusion model with $\mathcal{D}^0$ as input and $\mathcal{M}^0$ as extra guidance to generate a colored image $\mathcal{O}^0$. 
We then back-project the image $\mathcal{O}^0$ to the visible part of the texture $\mathcal{T}^0$. 
In the subsequent steps, we progressively diffuse the colored images $\mathcal{O}^k$ and back-project them to the texture $\mathcal{T}^k$ through a sequence of viewpoints.

We notice that directly inpainting the missing regions on mesh surface often results in inconsistency issues. 
The issue is mainly caused by the stretched artifacts that occur when the 2D views are projected back onto the curved surface of the mesh.
Therefore, we design a dynamic view partitioning strategy to guide the inpainting process with respective generation objectives $\mathcal{M}$ and denoising strengths $\gamma$.


\paragraph{Dynamic view partitioning}


For all viewpoints $\mathcal{V}=v_i, i=1, ..., N$, we render the similarity mask $\mathcal{S}^i$ for each viewpoint $v^k$ and map those values to the texture space. 
Each pixel in a similarity mask represents the reversed normalized value of the cosine similarity between the normal vectors of the visible faces and the view direction (ranging from 0 to 1). A pixel with a value of $1$ indicates that the corresponding face is perpendicular to the view direction. For simplicity, we set the background to $0$ . In summary, these masks indicate the extent to which a face is rotated away from the viewpoint.

Based on the similarity mask $\mathcal{S}^k$ at step $k$, we segment the rendered view into a \textbf{generation mask} $\mathcal{M}^k$, including the following $4$ regions, as shown in Fig.~\ref{fig:generation}: 
1) \textbf{New}: This region contains pixels that have not yet been textured. We inpaint this region from pure white Gaussian noise, i.e. with denoising strength $1$.
2) \textbf{Update}: This region contains pixels that have been textured, but the corresponding similarity score in $\mathcal{S}^k$ is greater than all other views. This indicates that those pixels are being observed in a better angle. Therefore, we update this region with a moderate denoising strength $\gamma_g$ to avoid stretched appearance.
3) \textbf{Keep}: Pixels in this region have been textured, but the corresponding similarity score in $\mathcal{S}^k$ is not the highest among all other views.These pixels have already been observed from a better angle, so we keep them fixed.
4) \textbf{Ignore}: This region contains pixels that belong to the background and are irrelevant to the process, so we ignore them throughout the entire process.

While the generation mask helps guide the texture inpainting process with accurate generation objectives and appropriate denoising strengths, blurriness and stretches can still exist on the mesh surface. This is because the generation mask is limited to a predefined set of viewpoints, and the seams and stretches on the texture are still visible from a novel viewpoint. To address this issue, we propose a texture refinement technique with an automatic viewpoint selection strategy, which is described in the next section.

\input{figures/selection}

\input{figures/qualitative}

\subsection{Texture Refinement with Automatic Viewpoint Selection}
\label{sec: refinement}

To remove the synthesis artifacts, a straightforward solution is to increase the number of viewpoints. 
However, the optimal viewpoint sequence can vary for different object geometries, making it difficult to manually pre-set the viewpoint sequence for massive synthesis targets.
To address these challenges, we propose an automatic viewpoint selection strategy that effectively prevents stretches and seams, as illustrated in Fig.~\ref{fig:selection}. 
We densely define a set of refinement viewpoints $\mathcal{V}=v_i, i=1, ...,K$, where $K$ is larger than $N$. 
To distribute the refinement viewpoints evenly, we scatter them on a hemisphere, taking into account that objects are rarely observed from the bottom-up view.

Assuming that an initial texture has been applied to the object, the refinement process begins by segmenting the generation masks $\mathcal{M}$ using the similarity masks $\mathcal{S}$ from all available viewpoints. For each of the $K$ refinement viewpoints in $\mathcal{V}$, we calculate a view heat $h^i$ from the corresponding generation mask $\mathcal{M}_i$, which represents the normalized area of the ``update'' region with respect to the current visible area of the object. The viewpoint $v_i$ that maximizes the view heat is then selected by
$\operatorname*{arg\,max}_i h_i = \frac{1}{N_p}\sum_{k=1}^{N_p}w_k$
where $N_p$ is the total number of the non-background pixels, and $w_k$ is the scaling factor for the segments in the generation mask. In order to let views with relatively more areas for updating, $w_k$ for the ``update'' region is set to be bigger than that for the ``keep'' region.
We dynamically select the next view with the highest view heat for updating. To avoid conflicts with the previously generated textures, we update the ``update'' regions with a mild denoising strength $\gamma_r$, which preserves the original appearance cues.





