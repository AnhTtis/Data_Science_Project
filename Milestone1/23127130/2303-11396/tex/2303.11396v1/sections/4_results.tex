\section{Results}

\subsection{Implementation Details}

We apply the Depth2Image model from Stable Diffusion v2~\cite{stablediffusion} as our generation backbone. The denoising strength $\gamma_g$ and $\gamma_r$ are set as $0.5$ and $0.3$ for the generation and refinement stages, respectively. We define $6$ axis-aligned principles viewpoints for generation, and in total $36$ viewpoints for refinement, among which we dynamically select only $20$ views to reduce time cost. Each synthesis process takes around 15 minutes to complete on an NVIDIA RTX A6000. Our implementation uses the PyTorch~\cite{paszke2017automatic} framework with PyTorch3D~\cite{ravi2020pytorch3d} used for rendering and texture projection.

\subsection{Experiment Setup}

\paragraph{Data.}

We evaluate our method on a subset of textured meshes from the Objaverse~\cite{objaverse} dataset. We first sample 3 random meshes from each category. To ensure the quality of the input meshes, we manually filter out thin or unrecognizable meshes, such as ``strainer'', ``sweatpants'', and ''legging'', meshes with too simplistic textures, and meshes that do not correspond with their assigned categories. For the purpose of reducing processing time, we also remove over-triangulated and scanned objects. After this curation, there are in total $410$ high quality textured meshes across $225$ categories for the experiments. Note that the original textures are only used for the evaluation.  
To compare with GAN-based category-specific approaches, we also report results on the ``car'' objects from the ShapeNet dataset~\cite{chang2015shapenet}. In particular, we use the $300$ meshes from the test set used in~\cite{siddiqui2022texturify}.

\paragraph{Baselines.}

We compare our method against the following state-of-the-art text-driven texture synthesis method: 1) \textbf{Text2Mesh}~\cite{michel2022text2mesh}, a neural pipeline that directly optimizes the textures and geometries via a CLIP-based optimization objective. We remove the displacement prediction so that only the surface RGBs are optimized. 2) \textbf{CLIPMesh}~\cite{mohammad2022clip}, a CLIP-based pipeline that deforms a sphere and optimizes the surface RGBs. Similar to Text2Mesh, we remove the shape deformation branch, and the texture colors are directly optimized on the surface of a given geometry. 3) \textbf{Latent-Paint}~\cite{metzer2022latent}, a texture generation variant of the NeRF-based 3D object generation pipeline Latent-NeRF~\cite{metzer2022latent}. It explicitly operates on a given texture map using Stable Diffusion as a prior. 
%
In addition to text-guided methods, we also compare with category-specific GAN-based approaches, including Texture Fields~\cite{oechsle2019texture}, SPSG~\cite{dai2021spsg}, LTG~\cite{yu2021learning}, and Texturify~\cite{siddiqui2022texturify}.

\paragraph{Evaluation metrics.}

We evaluate the generated textures via commonly used image quality and diversity metrics for generative models. Specifically, we report the Frechet Inception Distance (FID)~\cite{heusel2017gans} and Kernel Inception Distance (KID $\times 10^{-3}$ )~\cite{binkowski2018demystifying}. The generated image distribution for these metrics consists of renders of each mesh with the synthesized textures from 20 fixed viewpoints at a resolution of $512 \times 512$. For experiments on Objaverse dataset, the real distribution comprises renders of the meshes with the same settings using their artist designed textures. For experiments on ShapeNet cars, we use the $18991$ background segmented images from CompCars dataset~\cite{yang2015large}.

\subsection{Quantitative results}

In Tab.~\ref{tab:quantitatives}, we compare our method against the previous SOTA text-driven texture synthesis methods on Objaverse objects. As input, we uniformly feed template texts ``\textit{a \textlangle category\textrangle}'' to the models. Quantitatively, our method outperforms all baselines by a significant margin ($19\%$ improvement in FID and $26\%$ improvement in KID). Such improvements demonstrate that our method is more capable of generating more realistic textures on various object geometries from numerous categories. 
%
To demonstrate the effectiveness of our method against the GAN-based approaches on category-specific data, we report experiment results on ShapeNet ``car'' category in Tab.~\ref{tab:texturify}. Notably, our method achieves superior performance over the previous GAN-based SOTA texture synthesis method Texturify, improving by $21\%$ in FID and $12\%$ in KID. This indicates our method is more effective with synthesizing realistic textures than GAN based approaches that were trained on specific categories.

\input{figures/qualitative_shapenet}

\paragraph{User study.}

We conduct user study to analyze the quality of the synthesized textures and their fidelity to the input text prompts. 
For each baseline method, we randomly show the users 5 pair of renders from the baseline and our method. The users are requested to choose the one that is more realistic and closer to the text prompts.
More details of the questionnaire can be found in the supplemental. In the end, we receive 604 responses across 41 users. The collected preferences are reported in Tab.~\ref{tab:user}. In comparison to CLIPMesh and Text2Mesh, our method is clearly preferred by the users with preference rate $83.92\%$ and $76.47\%$, respectively. Besides that, more users ($64.18\%$) lean towards our method over the competitive baseline Latent-Paint. As can be seen, our method demonstrates the effectiveness in generating high quality textures that are favored by human users.

\input{tables/quantitatives}

\input{tables/texturify}

\input{tables/user}

\subsection{Qualitative analysis}

We compare our qualitative results on Objaverse objects against text-driven baselines in Fig.~\ref{fig:qualitative}. In comparison with CLIP-based methods CLIPMesh and Text2Mesh, our method generates more realistic and consistent textures. 
In particular, 
CLIPMesh generates sketchy textures while Text2Mesh produces repetitive patterns. 
Latent-Paint outputs a consistent texture capturing the semantics of the object well, but the results are often quite blurry.
Clearly, our method can synthesize more consistent textures with cleaner and richer local details.
We also compare our textures with GAN-based generation approach on category-specific objectives. In Fig.~\ref{fig:cars}, we show the textures for ShapeNet cars of our methods and Texturify. Notably, our textures have a much cleaner appearance and provide more details with respect to the input geometries.

\subsection{Ablation studies}

\input{figures/inpainting}

\paragraph{Does depth-aware inpainting and updating help?}

\input{tables/inpainting}

We show in Fig.~\ref{fig:inpainting} that the depth-aware inpainting is essential for producing high quality textures. 
In particular, the plain Depth2Img model often struggles to produce consistent appearances due to the governance of random noise. When the inpainting scheme is applied, the produced textures are more consistent. However, the textures still appear to be stretched and blurry over the curved mesh surface. These artifacts are amended by the texture updating scheme from the better viewing angles. The effectiveness of the depth-aware inpainting and the updating scheme is reflected in the improved FID and KID scores in Tab.~\ref{tab:inpainting}.

\paragraph{Does viewpoint selection in refinement stage help?}

We compare our results with different refinement settings in Tab.~\ref{tab:refinement}. When the input geometries are painted with initial textures from the generation stage, the blurry artifacts and projection seams are usually not eliminated due to a limited number of viewpoints. As can be seen in Fig.~\ref{fig:refinement}, such flaws can be minimized by refining with more viewpoints. We also showcase the effectiveness of the automatic viewpoint selection technique, as the refinement process does not require any manual efforts with defining and fine-tuning the viewpoint sequence for different shapes.

\input{figures/views}

\input{tables/refinement}

\subsection{Limitations.}

While our method has shown the capability to produce high-quality 3D textures, we have observed that it tends to produce textures with shading effects from the diffusion backbone. Although this issue can be addressed by carefully fine-tuning the input prompts, doing so requires additional human engineering effort and may not scale well to massive generation targets. One potential solution is to fine-tune the diffusion model to remove the shading from textures. We acknowledge this challenge and leave it to future work to explore this possibility.