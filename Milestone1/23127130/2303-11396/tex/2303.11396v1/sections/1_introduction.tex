\section{Introduction}

Generating high-quality 3D content is an essential component of visual applications in films, games, and upcoming AR/VR scenarios. 
%
With an increasing number of 3D content datasets, the computer vision community has witnessed significant progress in the field of 3D geometry generation~\cite{chen2019learning, nash2020polygen, wu2018learning, zeng2022lion, muller2022diffrf}. 
%
Despite the remarkable success in modeling 3D geometries in recent years, fully automatic 3D content generation is still hindered by the laborious human efforts required to design textures. 
%
Therefore, automating the texture design process through alternative guidance, such as text, has become an intriguing but challenging research problem.


Recently, text-to-image generators have shown remarkable progress in the 2D domain leveraging diffusion model architectures, enabling high resolution 2D content generation based on textual descriptions~\cite{stablediffusion, ramesh2022hierarchical}.
%%
However, there are notable challenges for producing 3D textures via such 2D vision-language prior knowledge.
%
Specifically, the synthesized textures are expected to be not only with high fidelity to the language cues, but also of high and consistent quality for target meshes.
%
As such, previous attempts to paint 3D geometry from text inputs often fail to deliver well-textured 3D content.

In this paper, we introduce~\textbf{\ARCH}, a novel texture synthesis method that seamlessly texturizes 3D objects using a pre-trained depth-aware text-to-image diffusion model.
%
The method renders a target mesh from multiple viewpoints and inpaints the missing appearance with a depth-aware text-to-image diffusion model.
%
\ARCH follows a \textit{generate-then-refine} strategy. 
%
Our method progressively generates partial textures across viewpoints and back-projects them to texture space. 
To address stretched and inconsistent artifacts observed from rotated viewpoints, we design a \textit{view partitioning technique} that computes similarity maps between visible texel's normal vectors and the current view direction. 
The generation mask created from these similarity maps guides the diffusion process by indicating regions to generate, update, keep, or ignore. 
This allows us to apply different diffusion strengths to respective regions, inpainting missing appearance and updating stretched artifacts.
However, the autoregressive generation process via the diffusion-based image inpainting model presents a new challenge. 
%
As the inpainting and updating scheme is conditioned on previously synthesized results, a viewpoint sequence with an ill-defined order or incomplete coverage over the mesh surface may result in unsatisfactory texturization. 
%
Therefore, we propose an \textit{automatic viewpoint selection} technique that progressively selects the next best view. 
%
The confidence of each candidate view containing the biggest relative area for generation and updating is estimated, given the partially textured mesh. 
%
This approach ensures complete coverage over the mesh surface and a high-quality texture map by consistently updating stretched regions.
%

We demonstrate the effectiveness of \ARCH for synthesizing high-quality 3D textures from language cues. 
The proposed method performs favorably against other language-based texture synthesis methods in terms of FID~\cite{heusel2017gans}, KID~\cite{binkowski2018demystifying}, and user study on a subset of the Objaverse dataset~\cite{objaverse}.
Additionally, our method also outperforms category-specific GAN-based methods on the ShapeNet car dataset~\cite{chang2015shapenet}. 

To summarize, our technical contributions are threefold:

\begin{itemize}

\item We design a novel method for high-quality texture synthesis by progressively inpainting and updating the 3D textures via depth-aware diffusion models.
\item We propose an automatic view sequence generation scheme to dynamically determine the order for generating and updating the texture space.
\item We conduct extensive study on a considerable amount of 3D objects, demonstrating the proposed method is effective for large-scale 3D content generation.

\end{itemize} 
