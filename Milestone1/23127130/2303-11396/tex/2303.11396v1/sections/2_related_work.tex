
\section{Related work}

\mypara{3D Generation from 3D and 2D data.}
To achieve 3D generation, it is natural to train models directly on 3D data. 
In contrast to 2D images, there are several 3D representations available, each with its unique characteristics, leading to the development of various generative models such as those based on voxels~\cite{lin2023infinicity,siarohin2023unsupervised,smith2017improved,xie2018learning}, point clouds~\cite{achlioptas2018learning,luo2021diffusion}, meshes~\cite{zhang2021sketch2model}, signed distance function~\cite{chen2019learning,cheng2022sdfusion,cheng2022cross,dai2021spsg,autosdf2022}, etc.
However, unlike images or videos that are ubiquitous, 3D data is inherently scarce and challenging to collect and annotate. 
Consequently, the synthesized samples from 3D generative models, trained on 3D data, are of limited quality and diversity, in terms of both structure and texture. Recent works have leveraged differentiable rendering to learn texture generation using only 2D images~\cite{gao2022get3d,siddiqui2022texturify, yu2021learning}. However, they are typically trained for specific shape categories and struggle in the quality of textures.    

\mypara{Text-Guided Generation.}
Recently, there has been tremendous progress in the vision-language domain~\cite{hu2021unit, singh2022flava, wang2022ofa, radford2021learning, chen2022d3net, chen2020scanrefer, chen2021scan2cap, chen2022unit3d}. Specifically, the emergence of Contrastive Language-Image Pre-Training (CLIP)~\cite{radford2021learning} has enabled the development of text-guided image generation through its semantically rich representation trained on text-image pairs. 
Initial efforts~\cite{crowson2022vqgan,patashnik2021styleclip} incorporated CLIP with different backbones, such as StyleGAN~\cite{karras2020analyzing} and VQGAN~\cite{esser2021taming}. 
However, diffusion models~\cite{dhariwal2021diffusion,ho2020denoising,ho2021cascaded,nichol2021improved,saharia2022image}, which have gained attention due to their superior visual quality and training stability compared to Generative Adversarial Networks (GANs)~\cite{GANs}, have recently been trained on large-scale text-image datasets with CLIP encodings~\cite{kawar2022imagic,ramesh2021zero,Rombach_2022_CVPR}. 
Among these models, Stable Diffusion~\cite{stablediffusion,Rombach_2022_CVPR} has garnered significant interest as an open-sourced model with numerous extensions~\cite{stablediffusion,zhang2023adding} that support different conditional modalities in addition to text prompts, including depth images, poses, sketches, etc. 
Additionally, CLIP has also been adopted in 3D to perform text-guided shape and texture generation~\cite{michel2022text2mesh,mohammad2022clip}. 
In this work, we take advantage of the depth-conditioning feature of Stable Diffusion to provide more consistent texturing.

\input{figures/progressive}

\mypara{Text-to-3D from 2D data.}
Inspired by the success of Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf}, NeRF-based generators~\cite{abdal20233davatargan, EG3D,piGAN,  StyleNeRF,Giraffe, or2022stylesdf,GRAF,skorokhodov3d,xu2022discoscene} have been proposed to learn 3D structures from 2D images using GAN-based frameworks. 
A new research direction emerged by combining NeRF techniques with flourishing diffusion-based text-to-image models, enabling text-to-3D learning with only 2D supervision. 
To address the challenge of optimizing a NeRF field, a score-distillation loss is proposed~\cite{poole2022dreamfusion} that leverages a pretrained 2D diffusion model as a critic to provide essential gradients. 
Subsequent efforts have focused on adopting this loss in latent space~\cite{cheng2022sdfusion,metzer2022latent} and in a coarse-to-fine refinement approach~\cite{lin2022magic3d}. However, optimization-based methods are plagued by long convergence times.
A recent concurrent work~\cite{richardson2023texture} proposes a non-optimization approach with progressive updates from multiple pre-set viewpoints.
In contrast, our method iteratively updates and refines the synthesized textures from automatically selected viewpoint sequences, which minimizes human efforts with designing different viewpoint orders for various geometries.
