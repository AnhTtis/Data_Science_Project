\section{Conclusion}

In this paper, we present a novel method,~\ARCH, for synthesizing high quality textures for 3D meshes from the given text prompts. 
Our approach leverages a depth-aware image inpainting diffusion model to progressively generate high-resolution partial textures from multiple viewpoints.
To avoid accumulating inconsistent and stretched artifacts across viewpoints, we dynamically segment the rendered view into a generation mask, which effectively guides the diffusion model to generate and update the corresponding partial textures. 
Furthermore, we propose an automatic viewpoint sequence generation scheme that utilizes the generation mask to automatically determine the next best view for refining the generated textures.
Extensive experiments demonstrate that our method can effectively synthesize consistent and highly detailed 3D textures for various object geometries without extra manual effort.
Overall, we hope our work can inspire more future research in the area of text-to-3D synthesis.
