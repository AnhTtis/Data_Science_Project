@article {PMID:30596876,
	Title = {PROBAST: A Tool to Assess Risk of Bias and Applicability of Prediction Model Studies: Explanation and Elaboration},
	Author = {Moons, Karel G M and Wolff, Robert F and Riley, Richard D and Whiting, Penny F and Westwood, Marie and Collins, Gary S and Reitsma, Johannes B and Kleijnen, Jos and Mallett, Sue},
	DOI = {10.7326/m18-1377},
	Number = {1},
	Volume = {170},
	Month = {January},
	Year = {2019},
	Journal = {Annals of internal medicine},
	ISSN = {0003-4819},
	Pages = {W1—W33},
}
@InProceedings{torec-et-al2021w2vlungcancer,
author="Luik, Torec T.
and Rios, Miguel
and Abu-Hanna, Ameen
and van Weert, Henk C. P. M.
and Schut, Martijn C.",
title="The Effectiveness of Phrase Skip-Gram in Primary Care NLP for the Prediction of Lung Cancer",
booktitle="Artificial Intelligence in Medicine",
year="2021",
publisher="Springer Intern. Publishing",
pages="433--437",
abstract="Neural models that use context-dependency in the learned text are computationally expensive. We compare the effectiveness (predictive performance) and efficiency (computational effort) of a context-independent Phrase Skip-Gram (PSG) model and a contextualized Hierarchical Attention Network (HAN) model for early prediction of lung cancer using free-text patient files from Dutch primary care physicians. The performance of PSG (AUROC 0.74 (0.69--0.79)) was comparable to HAN (AUROC 0.73 (0.68--0.78)); it achieved better calibration; had much less parameters (301 versus{\thinspace}>{\thinspace}300k) and much faster (36 versus 460 s). This demonstrates an important case in which the complex contextualized neural models were not required.",
isbn="978-3-030-77211-6"
}

@inproceedings{NIPS2017_attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "EMNLP 2021",
    journal = "EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}

@inproceedings{liu-etal-2022-p,
    title = "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "60th ACL (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2022.acl-short.8",
    pages = "61--68",
    abstract = "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1{\%}-3{\%} tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
}

@inproceedings{delobelle-etal-2020-robbert,
    title = "{R}ob{BERT}: a {D}utch {R}o{BERT}a-based {L}anguage {M}odel",
    author = "Delobelle, Pieter  and
      Winters, Thomas  and
      Berendt, Bettina",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.findings-emnlp.292",
    pages = "3255--3265",
    abstract = "Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model{'}s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.",
}

@inproceedings{
fries2022bigbio,
title={BigBio: A Framework for Data-Centric Biomedical Natural Language Processing},
author={Jason Alan Fries and Leon Weber and Natasha Seelam and Gabriel Altay and Debajyoti Datta and Samuele Garda and Myungsun Kang and Ruisi Su and Wojciech Kusa and Samuel Cahyawijaya and Fabio Barth and Simon Ott and Matthias Samwald and Stephen Bach and Stella Biderman and Mario S{\"a}nger and Bo Wang and Alison Callahan and Daniel Le{\'o}n Peri{\~n}{\'a}n and Th{\'e}o Gigant and Patrick Haller and Jenny Chim and Jose David Posada and John Michael Giorgi and Karthik Rangasai Sivaraman and Marc P{\`a}mies and Marianna Nezhurina and Robert Martin and Michael Cullan and Moritz Freidank and Nathan Dahlberg and Shubhanshu Mishra and Shamik Bose and Nicholas Michio Broad and Yanis Labrak and Shlok S Deshmukh and Sid Kiblawi and Ayush Singh and Minh Chien Vu and Trishala Neeraj and Jonas Golde and Albert Villanova del Moral and Benjamin Beilharz},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
}

@inproceedings{NEURIPS2019_superglue,
 author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
 volume = {32},
 year = {2019}
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@Article{Naseem2022,
author={Naseem, Usman
and Dunn, Adam G.
and Khushi, Matloob
and Kim, Jinman},
title={Benchmarking for biomedical natural language processing tasks with a domain specific ALBERT},
journal={BMC Bioinformatics},
year={2022},
month={Apr},
day={21},
volume={23},
number={1},
pages={144},
abstract={The abundance of biomedical text data coupled with advances in natural language processing (NLP) is resulting in novel biomedical NLP (BioNLP) applications. These NLP applications, or tasks, are reliant on the availability of domain-specific language models (LMs) that are trained on a massive amount of data. Most of the existing domain-specific LMs adopted bidirectional encoder representations from transformers (BERT) architecture which has limitations, and their generalizability is unproven as there is an absence of baseline results among common BioNLP tasks.},
issn={1471-2105},
doi={10.1186/s12859-022-04688-w},
}

@ARTICLE{De_Clercq2022-eh,
  title    = "{TARGET-HF}: developing a model for detecting incident heart
              failure among symptomatic patients in general practice using
              routine health care data",
  author   = "De Clercq, Lukas and Schut, Martijn C and Bossuyt, Patrick M M
              and van Weert, Henk C P M and Handoko, M Louis and Harskamp, Ralf
              E",
  abstract = "BACKGROUND: Timely diagnosis of heart failure (HF) is essential
              to optimize treatment opportunities that improve symptoms,
              quality of life, and survival. While most patients consult their
              general practitioner (GP) prior to HF, the early stages of HF may
              be difficult to identify. An integrated clinical support tool may
              aid in identifying patients at high risk of HF. We therefore
              constructed a prediction model using routine health care data.
              METHODS: Our study involved a dynamic cohort of patients
              ($\geq$35 years) who consulted their GP with either dyspnoea
              and/or peripheral oedema within the Amsterdam metropolitan area
              from 2011 to 2020. The outcome of interest was incident HF,
              verified by an expert panel. We developed a regularized,
              cause-specific multivariable proportional hazards model
              (TARGET-HF). The model was evaluated with bootstrapping on an
              isolated validation set and compared to an existing model
              developed with hospital insurance data as well as patient age as
              a sole predictor. RESULTS: Data from 31,905 patients were
              included (40\% male, median age 60 years) of whom 1,301 (4.1\%)
              were diagnosed with HF over 124,676 person-years of follow-up.
              Data were allocated to a development (n = 25,524) and validation
              (n = 6,381) set. TARGET-HF attained a C-statistic of 0.853 (95\%
              CI, 0.834 to 0.872) on the validation set, which proved to
              provide a better discrimination than C = 0.822 for age alone
              (95\% CI, 0.801 to 0.842, P < 0.001) and C = 0.824 for the
              hospital-based model (95\% CI, 0.802 to 0.843, P < 0.001).
              CONCLUSION: The TARGET-HF model illustrates that routine
              consultation codes can be used to build a performant model to
              identify patients at risk for HF at the time of GP consultation.",
  journal  = "Fam Pract",
  month    =  jul,
  year     =  2022,
  address  = "England",
  keywords = "decision support techniques; dyspnoea; early diagnosis; heart
              failure; oedema; primary health care",
  language = "en"
}

@article{Verkijk_Vossen_2021, title={MedRoBERTa.nl: A Language Model for Dutch Electronic Health Records}, volume={11}, abstractNote={&amp;lt;p&amp;gt;This paper presents MedRoBERTa.nl as the first Transformer-based language model for Dutch medical language. We show that using 13GB of text data from Dutch hospital notes, pre-training from scratch results in a better domain-specific language model than further pre-training RobBERT. When extending pre-training on RobBERT, we use a domain-specific vocabulary and re-train the embedding look-up layer. We show that MedRoBERTa.nl, the model that was trained from scratch, outperforms general language models for Dutch on a medical odd-one-out similarity task. MedRoBERTa.nl already reaches higher performance than general language models for Dutch on this task after only 10k pre-training steps. When fine-tuned, MedRobERTa.nl outperforms general language models for Dutch in a task classifying sentences from Dutch hospital notes that contain information about patients’ mobility levels.&amp;lt;/p&amp;gt;}, journal={Computational Linguistics in the Netherlands Journal}, author={Verkijk, Stella and Vossen, Piek}, year={2021}, month={Dec.}, pages={141–159} }

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{wang2020fewshot,
author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
title = {Generalizing from a Few Examples: A Survey on Few-Shot Learning},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
doi = {10.1145/3386252},
abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {63},
numpages = {34},
keywords = {small sample learning, prior knowledge, one-shot learning, meta-learning, low-shot learning, Few-shot learning}
}

@article{iyen_using_2013,
	title = {Using socio-demographic and early clinical features in general practice to identify people with lung cancer earlier.},
	volume = {68},
	doi = {10.1136/thoraxjnl-2012-202348},
	shorttitle = {Iyen-Omofoman B, Tata L, Baldwin D, Smith C, Hubbard {RUsing} socio-demographic and early clinical features in general practice to identify people with lung cancer earlier. Thorax 68(5)},
	abstract = {Introduction: 
In the {UK}, most people with lung cancer are diagnosed at a late stage when curative treatment is not possible. To aid earlier detection, the socio-demographic and early clinical features predictive of lung cancer need to be identified.

Methods:
We studied 12,074 cases of lung cancer and 120,731 controls in a large general practice database. Logistic regression analyses were used to identify the socio-demographic and clinical features associated with cancer up to 2 years before diagnosis. A risk prediction model was developed using variables that were independently associated with lung cancer up to 4 months before diagnosis. The model performance was assessed in an independent dataset of 1,826,293 patients from the same database. Discrimination was assessed by means of a receiver operating characteristic ({ROC}) curve.

Results:
Clinical and socio-demographic features that were independently associated with lung cancer were patients' age, sex, socioeconomic status and smoking history. From 4 to 12 months before diagnosis, the frequency of consultations and symptom records of cough, haemoptysis, dyspnoea, weight loss, lower respiratory tract infections, non-specific chest infections, chest pain, hoarseness, upper respiratory tract infections and chronic obstructive pulmonary disease were also independently predictive of lung cancer. On validation, the model performed well with an area under the {ROC} curve of 0.88.

Conclusions:
This new model performed substantially better than the current National Institute for Health and Clinical Excellence referral guidelines and all comparable models. It has the potential to predict lung cancer cases sufficiently early to make detection at a curable stage more likely by allowing general practitioners to better risk stratify their patients. A clinical trial is needed to quantify the absolute benefits to patients and the cost effectiveness of this model in practice.},
	journaltitle = {Thorax},
        journal = {Thorax},
	shortjournal = {Thorax},
	author = {Iyen, Barbara and Tata, Laila and Baldwin, David and Smith, Chris and Hubbard, Richard},
        year = {2013},
	date = {2013-01-15},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	journaltitle = {{arXiv}:1907.11692 [cs]},
        journal = {ArXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2022-02-06},
	date = {2019-07-26},
        year = {2019},
	eprinttype = {arxiv},
	eprint = {1907.11692},
	keywords = {Computer Science - Computation and Language},
}

@article{wei_why_2021,
title = {Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning},
shorttitle = {Why Do Pretrained Language Models Help in Downstream Tasks?},
abstract = {An analysis framework is proposed that links the pretraining and downstream tasks with an underlying latent variable generative model of text — the downstream classiﬁer must recover a function of the posterior distribution over the latent variables. Pretrained language models have achieved state-of-the-art performance when adapted to a downstream {NLP} task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text — the downstream classiﬁer must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classiﬁer on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model ({HMM}) or an {HMM} augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the {HMM}, simple classiﬁcation heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented {HMM} are stronger than for the vanilla {HMM} because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from {HMMs} back our theoretical ﬁndings.},
journaltitle = {{NeurIPS}},
journal = {NeurIPS},
author = {Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
date = {2021},
year = {2021},
}

@inproceedings{gu_ppt_2022,
title = {PPT: Pre-trained Prompt Tuning for Few-shot Learning},
year = {2022},
doi = {10.18653/v1/2022.acl-long.576},
shorttitle = {PPT},
abstract = {This work proposes to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization, and names this Pre-trained Prompt Tuning framework “{PPT}” to ensure the generalization of {PPT}. Prompts for pre-trained language models ({PLMs}) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes {PLMs} and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale {PLMs} to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework “{PPT}”. To ensure the generalization of {PPT}, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale {PLMs} in practice.},
booktitle = {ACL},
author = {Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
date = {2022},
}

@misc{su_transferability_2021,
	title = {On Transferability of Prompt Tuning for Natural Language Processing},
	doi = {10.48550/arXiv.2111.06719},
	abstract = {Prompt tuning ({PT}) is a promising parameter-efficient method to utilize extremely large pre-trained language models ({PLMs}), which can achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, {PT} requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve {PT} via prompt transfer, we empirically investigate the transferability of soft prompts across different downstream tasks and {PLMs} in this work. We find that (1) in zero-shot setting, trained soft prompts can effectively transfer to similar tasks on the same {PLM} and also to other {PLMs} with a cross-model projector trained on similar tasks; (2) when used as initialization, trained soft prompts of similar tasks and projected prompts of other {PLMs} can significantly accelerate training and also improve the performance of {PT}. Moreover, to explore what decides prompt transferability, we investigate various transferability indicators and find that the overlapping rate of activated neurons strongly reflects the transferability, which suggests how the prompts stimulate {PLMs} is essential. Our findings show that prompt transfer is promising for improving {PT}, and further research shall focus more on prompts' stimulation to {PLMs}. The source code can be obtained from https://github.com/thunlp/Prompt-Transferability.},
	number = {{arXiv}:2111.06719},
	publisher = {{arXiv}},
	author = {Su, Yusheng and Wang, Xiaozhi and Qin, Yujia and Chan, Chi-Min and Lin, Yankai and Wang, Huadong and Wen, Kaiyue and Liu, Zhiyuan and Li, Peng and Li, Juanzi and Hou, Lei and Sun, Maosong and Zhou, Jie},
	urldate = {2022-08-03},
	date = {2021-11-12},
	eprinttype = {arxiv},
	eprint = {2111.06719 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{li_prefix-tuning_2021,
	location = {Online},
	title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
	doi = {10.18653/v1/2021.acl-long.353},
	shorttitle = {Prefix-Tuning},
	abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to {GPT}-2 for table-to-text generation and to {BART} for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {4582--4597},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xiang Lisa and Liang, Percy},
	urldate = {2022-08-03},
	date = {2021-08},
        year = {2021},
}

@misc{liu_gpt_2021,
	title = {{GPT} Understands, Too},
	doi = {10.48550/arXiv.2103.10385},
	abstract = {While {GPTs} with traditional fine-tuning fail to achieve strong results on natural language understanding ({NLU}), we show that {GPTs} can be better than or comparable to similar-sized {BERTs} on {NLU} tasks with a novel method P-tuning -- which employs trainable continuous prompt embeddings. On the knowledge probing ({LAMA}) benchmark, the best {GPT} recovers 64{\textbackslash}\% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the {SuperGlue} benchmark, {GPTs} achieve comparable and sometimes better performance to similar-sized {BERTs} in supervised learning. Importantly, we find that P-tuning also improves {BERTs}' performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, P-tuning outperforms the state-of-the-art approaches on the few-shot {SuperGlue} benchmark.},
	number = {{arXiv}:2103.10385},
	publisher = {{arXiv}},
	author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
	urldate = {2022-09-27},
	date = {2021-03-18},
	eprinttype = {arxiv},
	eprint = {2103.10385 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{qin_learning_2021,
	title = {Learning How to Ask: Querying {LMs} with Mixtures of Soft Prompts},
	doi = {10.48550/arXiv.2104.06599},
	shorttitle = {Learning How to Ask},
	abstract = {Natural-language prompts have recently been used to coax pretrained language models into performing other {AI} tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to "fill in the blank" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent -- either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of "soft words," i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English {LMs} and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.},
	number = {{arXiv}:2104.06599},
	publisher = {{arXiv}},
	author = {Qin, Guanghui and Eisner, Jason},
	urldate = {2022-09-27},
	date = {2021-04-13},
	eprinttype = {arxiv},
	eprint = {2104.06599 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}
@article{clinicalbert,
author = {Kexin Huang and Jaan Altosaar and Rajesh Ranganath},
title = {ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},
year = {2019},
journal = {arXiv:1904.05342},
}
@article{beltagy-etal-2020-longformer,
  author    = {Iz Beltagy and
               Matthew E. Peters and
               Arman Cohan},
  title     = {Longformer: The Long-Document Transformer},
  journal   = {CoRR},
  volume    = {abs/2004.05150},
  year      = {2020},
  eprinttype = {arXiv},
  eprint    = {2004.05150},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}
@article{bojanowski2016enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016},
}
@article{joulin2016bag,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016},
}

@inproceedings{golmaei_deepnote-gnn_2021,
	title = {{DeepNote}-{GNN}},
	isbn = {978-1-4503-8450-6},
	doi = {10.1145/3459930.3469547},
	abstract = {With the increasing availability of Electronic Health Records ({EHRs}) and advances in deep learning techniques, developing deep predictive models that use {EHR} data to solve healthcare problems has gained momentum in recent years. The majority of clinical predictive models benefit from structured data in {EHR} (e.g., lab measurements and medications). Still, learning clinical outcomes from all possible information sources is one of the main challenges when building predictive models. This work focuses on two sources of information that have been underused by researchers; unstructured data (e.g., clinical notes) and a patient network. We propose a novel hybrid deep learning model, {DeepNote}-{GNN}, that integrates clinical notes information and patient network topological structure to improve 30-day hospital readmission prediction. {DeepNote}-{GNN} is a robust deep learning framework consisting of two modules: {DeepNote} and patient network. {DeepNote} extracts deep representations of clinical notes using a feature aggregation unit on top of a state-of-the-art Natural Language Processing ({NLP}) technique-{BERT}. By exploiting these deep representations, a patient network is built, and Graph Neural Network ({GNN}) is used to train the network for hospital readmission predictions. Performance evaluation on the {MIMIC}-{III} dataset demonstrates that {DeepNote}-{GNN} achieves superior results compared to the state-of-the-art baselines on the 30-day hospital readmission task. We extensively analyze the {DeepNote}-{GNN} model to illustrate the effectiveness and contribution of each component of it. The model analysis shows that patient network has a significant contribution to the overall performance, and {DeepNote}-{GNN} is robust and can consistently perform well on the 30-day readmission prediction task.},
	pages = {1--9},
	booktitle = {Proceedings of the 12th {ACM} Conference on Bioinformatics, Computational Biology, and Health Informatics},
	publisher = {{ACM}},
	author = {Golmaei, Sara Nouri and Luo, Xiao},
	date = {2021-08},
        year = {2021},
        month = {8},
}

@article{li_hi-behrt_2021,
	title = {Hi-{BEHRT}: Hierarchical Transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records},
	abstract = {Electronic health records represent a holistic overview of patients' trajectories. Their increasing availability has fueled new hopes to leverage them and develop accurate risk prediction models for a wide range of diseases. Given the complex interrelationships of medical records and patient outcomes, deep learning models have shown clear merits in achieving this goal. However, a key limitation of these models remains their capacity in processing long sequences. Capturing the whole history of medical encounters is expected to lead to more accurate predictions, but the inclusion of records collected for decades and from multiple resources can inevitably exceed the receptive field of the existing deep learning architectures. This can result in missing crucial, long-term dependencies. To address this gap, we present Hi-{BEHRT}, a hierarchical Transformer-based model that can significantly expand the receptive field of Transformers and extract associations from much longer sequences. Using a multimodal large-scale linked longitudinal electronic health records, the Hi-{BEHRT} exceeds the state-of-the-art {BEHRT} 1\% to 5\% for area under the receiver operating characteristic ({AUROC}) curve and 3\% to 6\% for area under the precision recall ({AUPRC}) curve on average, and 3\% to 6\% ({AUROC}) and 3\% to 11\% ({AUPRC}) for patients with long medical history for 5-year heart failure, diabetes, chronic kidney disease, and stroke risk prediction. Additionally, because pretraining for hierarchical Transformer is not well-established, we provide an effective end-to-end contrastive pre-training strategy for Hi-{BEHRT} using {EHR}, improving its transferability on predicting clinical events with relatively small training dataset.},
	journal = {IEEE journal of biomedical and health informatics},
	author = {Li, Yikuan and Mamouei, M. and Salimi-Khorshidi, G. and Rao, Shishir and Hassaine, A. and Canoy, D. and Lukasiewicz, Thomas and Rahimi, K.},
	date = {2022-11-25},
        year = {2022},
        month = {11},
        day = {25},
	keywords = {Risk prediction, Electronic health records, Index Terms-Deep learning, Computer Science - Machine Learning},
}

@inproceedings{aken_clinical_2021,
	title = {Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration},
	doi = {10.18653/v1/2021.eacl-main.75},
	abstract = {Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pretraining to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate {ICD} code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data.},
	pages = {881--893},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher = {Association for Computational Linguistics},
	author = {Aken, Betty van and Papaioannou, Jens-Michalis and Mayrdorfer, Manuel and Budde, Klemens and Gers, Felix and Loeser, Alexander},
 date = {2021},
 year = {2021},
}


@article{kim_deep-learning-based_2021,
	title = {Deep-Learning-Based Natural Language Processing of Serial Free-Text Radiological Reports for Predicting Rectal Cancer Patient Survival},
	volume = {11},
	issn = {2234-943X},
	doi = {10.3389/fonc.2021.747250},
	abstract = {Most electronic medical records, such as free-text radiological reports, are unstructured; however, the methodological approaches to analyzing these accumulating unstructured records are limited. This article proposes a deep-transfer-learning-based natural language processing model that analyzes serial magnetic resonance imaging reports of rectal cancer patients and predicts their overall survival. To evaluate the model, a retrospective cohort study of 4,338 rectal cancer patients was conducted. The experimental results revealed that the proposed model utilizing pre-trained clinical linguistic knowledge could predict the overall survival of patients without any structured information and was superior to the carcinoembryonic antigen in predicting survival. The deep-transfer-learning model using free-text radiological reports can predict the survival of patients with rectal cancer, thereby increasing the utility of unstructured medical big data.},
	journal = {Frontiers in Oncology},
	author = {Kim, Sunkyu and Lee, Choong-kun and Choi, Yonghwa and Baek, Eun Sil and Choi, Jeong Eun and Lim, Joon Seok and Kang, Jaewoo and Shin, Sang Joon},
	date = {2021-11},
 year = {2021},
 month = {11},
}

@article{rasmy_med-bert_2021,
	title = {Med-{BERT}: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction},
	volume = {4},
	issn = {2398-6352},
	doi = {10.1038/s41746-021-00455-y},
	abstract = {Deep learning ({DL})-based predictive models from electronic health records ({EHRs}) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of {DL}-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers ({BERT}) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of {BERT} on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by {BERT}, we propose Med-{BERT}, which adapts the {BERT} framework originally developed for the text domain to the structured {EHR} domain. Med-{BERT} is a contextualized embedding model pretrained on a structured {EHR} dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-{BERT} substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve ({AUC}) by 1.21–6.14\% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-{BERT} obtains promising performances on tasks with small fine-tuning training sets and can boost the {AUC} by more than 20\% or obtain an {AUC} as high as a model trained on a training set ten times larger, compared with deep learning models without Med-{BERT}. We believe that Med-{BERT} will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.},
	pages = {86},
	number = {1},
	journal = {npj Digital Medicine},
	author = {Rasmy, Laila and Xiang, Yang and Xie, Ziqian and Tao, Cui and Zhi, Degui},
	date = {2021-12},
year = {2021},
month = {12},
	note = {Publisher: Nature Research},
}

@article{menger_deduce_2018,
	title = {{DEDUCE}: A pattern matching method for automatic de-identification of Dutch medical text},
	volume = {35},
	issn = {0736-5853},
	url = {https://www.sciencedirect.com/science/article/pii/S0736585316307365},
	doi = {10.1016/j.tele.2017.08.002},
	shorttitle = {{DEDUCE}},
	abstract = {In order to use medical text for research purposes, it is necessary to de-identify the text for legal and privacy reasons. We report on a pattern matching method to automatically de-identify medical text written in Dutch, which requires a low amount of effort to be hand tailored. First, a selection of Protected Health Information ({PHI}) categories is determined in cooperation with medical staff. Then, we devise a method for de-identifying all information in one of these {PHI} categories, that relies on lookup tables, decision rules and fuzzy string matching. Our de-identification method {DEDUCE} is validated on a test corpus of 200 nursing notes and 200 treatment plans obtained from the University Medical Center Utrecht ({UMCU}) in the Netherlands, achieving a total micro-averaged precision of 0.814, a recall of 0.916 and a F1-score of 0.862. For person names, a recall of 0.964 was achieved, while no names of patients were missed.},
	pages = {727--736},
	number = {4},
	journaltitle = {Telematics and Informatics},
	journal = {Telematics and Informatics},
	year = {2018},
	shortjournal = {Telematics and Informatics},
	author = {Menger, Vincent and Scheepers, Floor and van Wijk, Lisette Maria and Spruit, Marco},
	urldate = {2023-03-16},
	date = {2018-07-01},
	langid = {english},
	keywords = {De-identification, Dutch medical text, Patient privacy, Pattern matching, Protected Health Information},
	file = {ScienceDirect Snapshot:C\:\\Users\\aukee\\Zotero\\storage\\ZFQWT3YG\\S0736585316307365.html:text/html},
}
