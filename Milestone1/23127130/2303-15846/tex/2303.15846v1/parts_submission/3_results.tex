\section{Results and Discussion}
\label{sec:results}

{
\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{table}[t]
\centering
\begin{adjustbox}{max width=0.8\textwidth}
\begin{tabular}{@{}lllllll@{}}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{l}{AUROC ($\uparrow$)} & \multicolumn{2}{l}{AUPRC ($\uparrow$)} & \multicolumn{2}{l}{Brier score ($\downarrow$)} \\
                       & N           & P           & N           & P           & N              & P              \\
\midrule
%\multicolumn{7}{c}{\bf Standard fine-tuning} \\
%\midrule
FastText (FT)               & ---            & $90.8$        & ---           & $\underline{90.4}$        & ---              & $\bm{44.7}$ \\
RobBERT (FT)                & $81.8$         & $90.2$        & $65.7$        & $84.8$        & $18.2$           & $\underline{54.3}$            \\
MedRoBERTa.nl (FT)          & $82.4$         & $90.2$        & $68.6$        & $82.7$        & $17.6$           & $56.8$           \\
%\midrule
%\multicolumn{7}{c}{\bf Soft-prompt tuning} \\
%\midrule
\cmidrule{2-7}
RobBERT (ST)                & $\bm{84.5}$        & $\bm{94.3}$        & $\underline{72.3}$        & $\bm{91.4}$        & $\underline{16.7}$           & $55.9$           \\
MedRoBERTa.nl (ST)          & $\underline{84.3}$        & $\underline{92.7}$        & $\bm{75.5}$        & $86.5$        & $\bm{15.9}$           & $58.0$           \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{%Static word embedding model \mbox{FastText} and pretrained language models \mbox{RobBERT} and \mbox{MedRoBERTa.nl} 
Results on \datasetbalanced{} \textit{test\_1} for models fine-tuned (FT) to predict lung cancer vs. using soft-prompt tuning (ST). We highlight best results in \textbf{bold} and second-best by \underline{underscoring}. \textbf{N} denotes per-note and \textbf{P} per-patient results. We aggregate per-note into per-patient predictions following Equation~\ref{eq:note_to_patient_predictions}.}\label{tab:balanced_dataset}
\end{table}
}


\subsection{Full data availability and balanced classes}
\label{sec:res:full_data}
In Table~\ref{tab:balanced_dataset}, we report results on our experiments where we use the \datasetbalanced{} dataset.
The \datasetbalanced{} dataset includes all the positive patients in our cohort and an equal number of negative patients sampled randomly, i.e., we build a version of the data set with balanced positive/negative classes (see Table~\ref{tab:data_statistics} for more details).
Here, we aim to investigate how the methods used in this work fare in this idealised scenario where class imbalance is not an issue.
Moreover, we compare models when directly \textit{fine-tuned} or when \textit{soft-prompt tuned} on the task of predicting whether a patient has lung cancer.
We use the \textit{train}, \textit{valid}, and \textit{test\_1} splits for model training, model selection, and testing, respectively.

\subsubsection{From per-note to per-patient predictions}
From Table~\ref{tab:balanced_dataset}, we first highlight that per-patient predictions---which aggregates per-note predictions for a patient according to Equation~\ref{eq:note_to_patient_predictions}---consistently improve on per-note predictions, which suggests that the offline aggregation method we use can suppress possibly noisy per-note predictions well.
We also experimented with aggregating per-note into per-patient predictions as 1) the mean, 2) the maximum, and 3) Clinical BERT~\cite{clinicalbert}'s aggregation over all note probabilities, however using Equation~\ref{eq:note_to_patient_predictions} performs best (we do not report those other experiments due to space constraints).

\subsubsection{Discussion}
First of all, we note that FastText performs comparably to or better than our \textit{fine-tuned} PLMs according to all metrics;
however, when compared to our \textit{soft prompt-tuned} PLMs, FastText is slightly worse (AUROC), comparable (AUPRC) or much better in terms of calibration (Brier score).
Moreover, soft-prompt tuning tends to consistently outperform standard model fine-tuning, according to AUROC (by $\sim 2$--$4\%$) and AUPRC (by $\sim 4$--$6\%$), but with no clear trend according to the Brier score.

Finally, per-note Brier scores are considerably better than per-patient Brier scores.
This suggests that the aggregation method we use (see Equation~\ref{eq:note_to_patient_predictions}) consistently improves AUROC and AUPRC at the expense of model calibration.


\subsection{Balanced vs. imbalanced classes}
\label{sec:res:imbalanced}

In Table~\ref{tab:imbalanced}, we show results for our FastText baseline and the best performing RobBERT model trained on \datasetbalanced{} (\textit{train}) and selected according to AUROC scores on \datasetbalanced{} (\textit{valid}).
We provide results on the \textit{test\_2} splits of \datasetbalanced{}, $\mathcal{D}^{1:10}$, $\mathcal{D}^{1:100}$, and $\mathcal{D}^{1:250}$.

As expected, we note that both AUROC and AUPRC tend to decrease as we increase the number of negative patients, with more pronounced effects on AUPRC (i.e., when test sets have roughly 1:250 patient-to-positive ratios, AUPRC scores are below $1\%$).
However, we also note an opposite trend: \textit{Brier scores improve as test sets become more imbalanced}.
This makes sense when taking our per-patient aggregation method into account: since we use the lowest note probability, the prediction for true negatives will likely be much closer to 0 than the prediction for true positives will be to 1. 

{
\begin{table}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}l@{\kern2em}l@{\kern1em}l@{\kern1em}l@{\kern1em}l@{\kern2em}l@{\kern1em}l@{\kern1em}l@{\kern1em}l@{\kern2em}l@{\kern1em}l@{\kern1em}l@{\kern1em}l@{}}
\toprule
 & \multicolumn{4}{c}{AUROC ($\uparrow$)} & \multicolumn{4}{c}{AUPRC ($\uparrow$)} & \multicolumn{4}{c}{Brier score ($\downarrow$)} \\
\midrule
Test set $\mathcal{D}^?$     & $^{bal}$ & $^{1:10}$ & $^{1:100}$ & $^{1:250}$ 
      & $^{bal}$ & $^{1:10}$ & $^{1:100}$ & $^{1:250}$ 
      & $^{bal}$ & $^{1:10}$ & $^{1:100}$ & $^{1:250}$ \\
\midrule
Random & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 10.0 & 1.0 & 0.4 & --- & --- & --- & --- \\
\midrule
\multicolumn{13}{c}{Per-note predictions} \\
\midrule
RB (FT)  & 76.8 & 78.3 & 61.3 & 54.4 &
                63.9 & 15.9 & 1.2  & $\bm{0.4}$  &
                19.9 & 18.7 & 28.3 & 32.4 \\
RB (ST)  & $\bm{80.1}$ & $\bm{81.4}$ & $\bm{64.5}$ & $\bm{57.8}$ &
                $\bm{70.6}$ & $\bm{22.0}$ & $\bm{1.5}$ & $\bm{0.4}$ &
                $\bm{18.3}$ & $\bm{16.8}$ & $\bm{27.4}$ & $\bm{32.3}$ \\
\midrule
\multicolumn{13}{c}{Per-patient predictions}\\
\midrule
FaT (FT) & 78.7 & 81.0 & 60.1 & 56.1 &
                77.2 & 32.3 &  1.4 &  0.5 &
                $\bm{44.8}$ &  $\bm{8.2}$ &  $\bm{1.2}$ &  $\bm{0.7}$ \\
RB (FT)  & 88.2 & 90.1 & 70.3 & 65.4 &
                81.9 & 40.0 & 1.8   & 0.6   &
                54.0 & 67.7 & 24.3  & 18.0 \\
RB (ST)  & $\bm{89.7}$ & $\bm{91.1}$  & $\bm{71.2}$ & $\bm{67.1}$  &
                $\bm{86.2}$ & $\bm{47.0}$  & $\bm{1.9}$  & $\bm{0.7}$ &
                56.3 & 71.0  & 25.1 & 17.8 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Results on imbalanced vs. balanced test datasets with the best performing fine-tuning (FT) and soft prompt-tuning (ST) model (RobBERT) trained on \datasetbalanced{} \textit{train} split.
\textbf{N} denotes per-note and \textbf{P} per-patient results. {\bf RB}: RobBERT. {\bf FaT}: Fast-text. We aggregate per-note into per-patient results with Equation~\ref{eq:note_to_patient_predictions}.}\label{tab:imbalanced_dataset}
\label{tab:imbalanced}
\end{table}
}

\subsection{Few-shot learning}
\label{sec:res:few_shot}
Few-shot learning results show that 
when models are trained on 32 or less patients, PLMs tend to clearly outperform FastText, but when models are trained on 128 patients we already note that FastText outperforms all PLMs (according to both AUROC and AUPRC).
This happens regardless of the PLM being fine-tuned or soft-prompt tuned, which suggests that soft-prompt tuning is still not clearly more resilient to `noisy' inputs than fine-tuned models, at least according to our experiments.

Moreover, we note that fine-tuned and soft-prompt tuned models seem to perform comparably in our few-shot experiments.
Oscillations in both AUROC and AUPRC from smaller to higher $k$'s suggest that there is some degree of randomness in model performance, which is an undesirable feature which we would like to address in future work.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        %\includesvg[height=6.5cm]{imgs_submission/auroc_patient_val2.svg}
        \includegraphics[height=6.5cm]{imgs_submission/auroc_patient_val2.png}
        \caption{Number of training patients vs. \mbox{AUROC} scores on \datasetbalanced{} \textit{test\_1}.}
    \end{subfigure}\hfill%
    ~
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        %\includesvg[height=6.5cm]{imgs_submission/auprc_patient_val2.svg}
        \includegraphics[height=6.5cm]{imgs_submission/auprc_patient_val2.png}
        \caption{Number of training patients vs. AUPRC scores on \datasetbalanced{} \textit{test\_1}.}
    \end{subfigure}
    \caption{Results
    %on \datasetbalanced{} \textit{test\_1} 
    for models trained on \datasetfewshot{k} (\textit{train}), $k \in \{2, 4, 8, 16, 32, 64, 128 \}$. {\bf ST}: soft-prompt tuning. {\bf FT}: fine-tuning. RobBERT 'full' is the best RobBERT model trained and tested on \datasetbalanced{} \textit{train} and \textit{test\_1} splits, respectively.}
\end{figure*}