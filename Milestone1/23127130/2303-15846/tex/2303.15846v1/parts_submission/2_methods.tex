\section{Methodology}
\label{sec:methods}

We now provide details on the patient selection and data cleaning before elaborating on our experimental settings.

\subsection{Data}
\label{sec:data}

We have unique access to data from patients of General Practitioners (GPs) that are associated with the two hospitals of the Amsterdam University Medical Centers, the Free University Medical Center (VUMC) and the Academic Medical Center of the University of Amsterdam (AMC).
Our patient cohort has an entry date of 01/01/2002 and an exit date of 31/12/2020, and includes patient demographics and free-text GP notes for every patient visit to their GP.
Henceforth, we only consider free-text notes that fall within the entry and exit dates.
We include patients who conform with the following criteria:
\textit{i)} the patient must be at least 40 years-old---i.e., we define the patient age as the date of their most recent free-text note minus their year of birth---,
\textit{ii)} the patient must have at least one valid free-text note.\footnote{All notes were anonymized with a modified version of the DEDUCE algorithm \cite{menger_deduce_2018}.}
Furthermore, we define how old a patient's free-text note is by the number of days between the date of the note and that patientâ€™s last available note.

A free-text note is considered valid depending on whether the note belongs to a patient diagnosed with lung cancer or not.
For a free-text note of a patient diagnosed with lung cancer to be considered valid, the note must be dated at least 150 days ($\sim 5$ months) and at most 730 days ($\sim 2$ years) before the date of the diagnosis.
In case the patient is not diagnosed with lung cancer, the note must be dated at most 730 days ($\sim 2$ years) after the date of the patient's last available note.
Finally, we also exclude any notes that are undated, or have a date outside of the collection period.\footnote{For example, a reminder for a 5-year checkup.} 
We use these cut-off periods to ensure the predictive value of the model, and since GPs typically start to suspect lung cancer from about 4 months before diagnosis, and symptoms are expected to be present from 4 months to 2 years before diagnosis \cite{iyen_using_2013}.
Finally, we denote by \dataset{} the set of patients after we apply all the above mentioned procedures.

We establish lung cancer diagnoses for the patients in our cohort by linking to a central database maintained by \textit{Integraal Kankercentrum Nederland} (Integrated Cancer Center, IKNL).
Henceforth, we refer to patients with a lung cancer diagnosis as `positive', and all other patients as `negative'.

\subsubsection{Data splits}
\label{sec:data_splits}

In Table~\ref{tab:data_statistics} we show key characteristics for the different datasets used in our experiments: \datasetbalanced, $\mathcal{D}^{1:10}$, $\mathcal{D}^{1:100}$, $\mathcal{D}^{1:250}$, \datasetfewshot{2}, \datasetfewshot{4}, \datasetfewshot{8}, \datasetfewshot{16}, \datasetfewshot{32}, \datasetfewshot{64}, and \datasetfewshot{128}.
Train, validation, and test splits are always \textit{stratified}, i.e., they inherit the same ratio of patients with/without cancer as the original dataset.
Below we describe different data splits we use in different experiments in more detail.

\paragraph{Balanced dataset}
We first create the subset \mbox{\datasetbalanced{} $\subset$ \dataset{}} with the same number of positive and negative patients.
In \datasetbalanced{} we include all $1,733$ positive patients in \dataset{} and an equal number of negative patients, i.e., we randomly subsample patients without lung cancer from \dataset{} to have a balanced dataset.


\paragraph{Imbalanced test sets}
To study the effect of class imbalance in our models, we build test sets where the class imbalance between the positive and negative class (which in \datasetbalanced{} has a $1$:$1$ ratio) becomes closer and closer to the true ratio in the dataset population (which has approximately a $1$:$250$ ratio).
More concretely, we propose different \textit{test sets} $\mathcal{D}^{pos:neg}$ with different positive-to-negative ratios, and build the test sets $\mathcal{D}^{1:10}$, $\mathcal{D}^{1:100}$, and $\mathcal{D}^{1:250}$.

\paragraph{Few-shot datasets}
Finally, we also build few-shot datasets \datasetfewshot{k} $\subset$ \datasetbalanced{} that include a small number of patients, and where $k$ is the number of patients in \textit{each} class (positive and negative).
We use $k \in \{2,4,8,16,32,64,128\}$, and therefore have few-shot datasets \datasetfewshot{2}, \datasetfewshot{4}, \datasetfewshot{8}, \datasetfewshot{16}, \datasetfewshot{32}, \datasetfewshot{64}, and \datasetfewshot{128}.

\begin{table}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}llp{2.5cm}p{2.3cm}p{2cm}p{3cm}@{}}
\toprule
\multirow{2}{1.4cm}{Dataset name} &\multirow{2}{2.6cm}{\# patient notes mean (min, max)} & \multicolumn{4}{c}{total patients (\#positive, \#negative)}                                                                                  \\
\addlinespace
                      && train & valid & test\_1 & test\_2 \\
\midrule
\datasetbalanced{}    & 32.2 (1, 284) & 1384 (692, 692)  & 518 (259, 259) & 170 (85, 85) & 1394 (697, 697)                 \\
\cmidrule{2-6}
$\mathcal{D}^{1:10}$  & 28.1 (1, 293) & ---                 & ---                 & ---                  & 7667 (697, 6970)                 \\
$\mathcal{D}^{1:100}$ & 29.2 (1, 572 & ---                 & ---                 & ---                  & 70396 (697, 69699)                 \\
$\mathcal{D}^{1:250}$ & 29.5 (1, 1403) & ---                 & ---                & ---                  & 174946 (697, 174249)                 \\
\cmidrule{2-6}
\datasetfewshot{2}    & 30.7 (1, 284) & 2 (1, 1)                      & 2 (1, 1)                & ---                  & ---                  \\
\datasetfewshot{4}    & 30.7 (1, 284) & 4 (2, 2)                      &  4 (2, 2)             & ---                 & ---                  \\
\datasetfewshot{8}    & 30.9 (1, 284) & 8 (4, 4)                      & 8 (4, 4)                 & ---                  & ---                \\
\datasetfewshot{16}   & 30.9 (1, 284) & 16 (8, 8)                     & 16 (8, 8)                 & ---                  & ---                \\
\datasetfewshot{32}   & 30.9 (1, 284) & 32 (16, 16)                   & 32 (16, 16)                & ---                  & ---                  \\
\datasetfewshot{64}   & 31.1 (1, 284) & 64 (32, 32)                   & 64 (32, 32)              & ---                  & ---                 \\
\datasetfewshot{128}  & 31.5 (1, 284) & 128 (64, 64)                  & 128 (64, 64)                 & ---                  & ---                 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Dataset statistics: we show characteristics for our balanced dataset split \datasetbalanced{}, our imbalanced test sets $\mathcal{D}^{pos:neg}$, and our few-shot datasets \datasetfewshot{k}. For each dataset, we show the number of patients in the \textit{train}, \textit{valid}, \textit{test\_1} and \textit{test\_2} splits, as well as the mean, minimum and maximum number of free-text notes associated to each patient. Overall, there are a total of \textit{1,733} positive patients in our cohort (692+259+85+697).}\label{tab:data_statistics}
\end{table}

\subsection{Aggregating per-note into per-patient predictions}
\label{sec:note_to_patient_predictions}
In our cohort introduced in Section~\ref{sec:data}, patients have on average $28.1$--$32.2$ medical notes in their medical history (see Table~\ref{tab:data_statistics}), where each note has on average $26.2$--$34.2$ tokens.
Both PLMs we use in this work, \mbox{MedRoBERTa.nl} and RobBERT, process texts with a maximum length of $512$ tokens.
Nonetheless, we wish to build models that  \textit{predict risk of lung cancer for a patient} and thus use all medical notes in the patient's medical history as predictors.
In practice, we train our models to predict lung cancer \textit{independently from each note} available for a patient, and apply a simple but effective \textit{post-hoc aggregation} step to compute per-patient predictions from per-note predictions.

First, we construct each training instance $\{ X^P_n, Y^P\}_{n=1}^N$ by propagating the label $Y^P$ of patient $P$ to each of the $N$ notes available for that patient, i.e., $X^P_n$ is the $n$-th note belonging to patient $P$, and $Y^P$ is a binary variable indicating whether patient $P$ has lung cancer.
After the model is trained, we aggregate per-note into per-patient predictions following Equation~\ref{eq:note_to_patient_predictions} below.
\begin{equation}\label{eq:note_to_patient_predictions}
    P(Y^P=1 | \{X^P_n\}_{n=1}^N) = P_{min},
\end{equation}

\noindent
where $P_{min}$ is the lowest per-note probability predicted across all notes $\{X^P_n\}_{n=1}^N$.


\subsection{Soft-prompt tuning}
\label{sec:prompt_tuning}
Soft-prompt tuning is a method of model adaptation where a small number of new parameters are added to a model throughout that model's architecture, as opposed to standard model fine-tuning where a classification layer is typically appended at the ``end'' of the model
\cite{liu_gpt_2021,li_prefix-tuning_2021,liu-etal-2022-p}.
Concretely, we append \textit{continuous, trainable embeddings} to the input sequence which are optimised via backpropagation, while the original model parameters remain frozen and are not updated.
We refer the reader to~\cite{liu-etal-2022-p} for details.

\subsection{Models}
\label{sec:plms}

\subsubsection{Pretrained language models (PLMs)}
We use two Dutch-language PLMs based on the RoBERTa architecture \cite{liu_roberta_2019}:
\mbox{RobBERT}  \cite{delobelle-etal-2020-robbert}, which was created by retraining the RoBERTa tokenizer and model from scratch on a Dutch general language corpus, and \mbox{MedRoBERTa.nl} \cite{Verkijk_Vossen_2021}, which was created with a similar procedure, but using Dutch hospital notes instead of general language data. 

\subsubsection{Static word embedding models (WEMs)}
FastText~\cite{bojanowski2016enriching} is a static WEM where word representations encode \textit{subword information}.
After FastText has been trained, however, the meaning of a word will \textit{not} change when changing its context.
However, FastText shows strong results across text classification tasks, especially in tasks where noisy input texts are common, e.g. texts containing typos, mispellings, and acronyms~\cite{joulin2016bag}.
For that reason, we use FastText as one of the baselines in our experiments.

We follow two steps:
1) We pretrain FastText from scratch using all free-text notes available for all patients from all splits in \datasetbalanced{}.
2) We use the training notes in \datasetbalanced{} to fine-tune FastText to predict risk of lung cancer for that dataset.
