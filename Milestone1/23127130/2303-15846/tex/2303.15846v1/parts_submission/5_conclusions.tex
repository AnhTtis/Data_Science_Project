\section{Conclusions and Future Work}
\label{sec:conclusions}
In this work, we investigate the application of soft-prompt tuning to perform the early prediction of lung cancer using free-text patient medical notes
of Dutch primary care physicians.
We find that using contextualised pretrained language models (RobBERT and \mbox{MedRoBERTa.nl}) outperforms strong static word embedding models (FastText) according to AUROC and AUPRC, however FastText shows much better calibration than both PLMs.
Results we obtained in few-shot experiments are not so clear cut, and here the difference between PLMs and WEMs is less clear.
Soft prompt tuning consistently outperforms standard model fine-tuning with PLMs, which we find promising and believe warrants further research.
When testing our models on datasets with increased class imbalance, performance deteriorates as expected; nonetheless, the best performing PLM still achieves a reasonable $67.1$ AUROC when tested on a $1$:$250$ (\textit{positive:negative}) ratio---virtually the same ratio as that of the population in our dataset---, though the corresponding $0.7$ AUPRC shows modest improvements upon the random baseline ($0.4$ AUPRC).

As future work, we plan to investigate soft prompt tuning techniques with PLMs and how to best combine these models with static word embedding models to try to obtain both high discrimination \textit{and} calibration.
We will also investigate how to stabilise predictions in few-shot experiments as we increase the number of training examples, possibly training models with different patient populations (but the same number of patients) as a form to estimate uncertainty.