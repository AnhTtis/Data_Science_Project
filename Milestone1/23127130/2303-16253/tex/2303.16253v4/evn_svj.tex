\documentclass[submission, Phys]{SciPost}

% Prevent all line breaks in inline equations.
\binoppenalty=10000
\relpenalty=10000

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\usepackage[bitstream-charter]{mathdesign}
\urlstyle{sf}

% Fix \cal and \mathcal characters look (so it's not the same as \mathscr)
\DeclareSymbolFont{usualmathcal}{OMS}{cmsy}{m}{n}
\DeclareSymbolFontAlphabet{\mathcal}{usualmathcal}

% custom symbol definitions
\usepackage{xspace}
\providecommand{\DOI}[1]{\href{http://dx.doi.org/#1}{\doi{#1}}}
\newcommand{\slashmiss}[1]{{\ooalign{\hspace{0.1em}/\cr$#1$}}}
\newcommand{\GeV}{\ensuremath{\,\text{Ge\hspace{-.08em}V}}\xspace}
\newcommand{\degree}{\ensuremath{{}^{\circ}}\xspace}
\newcommand{\fbinv}{\mbox{\ensuremath{\,\text{fb}^{-1}}}\xspace}
\newcommand{\kt}{\ensuremath{k_{\mathrm{T}}}\xspace}
\newcommand{\pt}{\ensuremath{p_{\mathrm{T}}}\xspace}
\newcommand{\ptmiss}{\ensuremath{\slashmiss{p}_{\mathrm{T}}}\xspace}
\newcommand{\pxmiss}{\ensuremath{\slashmiss{p}_{x}}\xspace}
\newcommand{\pymiss}{\ensuremath{\slashmiss{p}_{y}}\xspace}
\newcommand{\ptvecmiss}{\ensuremath{\slashmiss{\vec{p}}_{\mathrm{T}}}\xspace}
\newcommand{\etmiss}{\ensuremath{\slashmiss{E}_{\mathrm{T}}}\xspace}
\newcommand{\pthat}{\ensuremath{\hat{p}_{\mathrm{T}}}\xspace}
\newcommand{\FASTJET} {{\textsc{FastJet}}\xspace}
\newcommand{\PYTHIA} {{\textsc{pythia}}\xspace}
\newcommand{\MADGRAPH} {\textsc{MadGraph}\xspace}
\newcommand{\MCATNLO} {\textsc{mc@nlo}\xspace}
\newcommand{\MGvATNLO}{\MADGRAPH{}5\_a\MCATNLO}
\newcommand{\FEYNRULES} {{\textsc{FeynRules}}\xspace}
\newcommand{\PZ}{\ensuremath{\text{Z}}\xspace}
\newcommand{\PW}{\ensuremath{\text{W}}\xspace}
\newcommand{\PH}{\ensuremath{\text{H}}\xspace}
\newcommand{\PZprime}{\ensuremath{{\PZ}^{\prime}}\xspace}
\newcommand{\Pq}{\ensuremath{\text{q}}\xspace}
\newcommand{\Pqdark}{\ensuremath{\chi}\xspace}
\newcommand{\Paqdark}{\ensuremath{\overline{\chi}}\xspace}
\newcommand{\mqdark}{\ensuremath{m_{\Pqdark}}\xspace}
\newcommand{\mq}{\ensuremath{m_{\Pq}}\xspace}
\newcommand{\ZprimeToDark}{\ensuremath{\PZprime \to \Pqdark\Paqdark}\xspace}
\newcommand{\mZprime}{\ensuremath{m_{\PZprime}}\xspace}
\newcommand{\MT}{\ensuremath{M_{\mathrm{T}}}\xspace}
\newcommand{\MTtwo}{\ensuremath{M_{\mathrm{T2}}}\xspace}
\newcommand{\MTtwoGen}{\ensuremath{\MTtwo^{\text{gen}}}\xspace}
\newcommand{\MAOS}{\ensuremath{M_{\mathrm{MAOS}}}\xspace}
\newcommand{\RT}{\ensuremath{R_{\mathrm{T}}}\xspace}
\newcommand{\Pbifun}{\ensuremath{\Phi}\xspace}
\newcommand{\sbifun}{\ensuremath{y_\text{dark}}\xspace}
\newcommand{\nbifun}{\ensuremath{n_{\Pbifun}}\xspace}
\newcommand{\mbifun}{\ensuremath{m_{\Pbifun}}\xspace}
\newcommand{\gqdark}{\ensuremath{g_{\Pqdark}}\xspace}
\newcommand{\gq}{\ensuremath{g_{\Pq}}\xspace}
\newcommand{\rinv}{\ensuremath{r_{\text{inv}}}\xspace}
\newcommand{\mDark}{\ensuremath{m_{\text{dark}}}\xspace}
\newcommand{\lamDark}{\ensuremath{\Lambda_{\text{dark}}}\xspace}
\newcommand{\aDark}{\ensuremath{\alpha_{\text{dark}}}\xspace}
\newcommand{\Bdark}{\ensuremath{\mathcal{B}_{\text{dark}}}\xspace}
\newcommand{\Nc}{\ensuremath{N_{c}^\text{dark}}\xspace}
\newcommand{\Nf}{\ensuremath{N_{f}^\text{dark}}\xspace}
\newcommand{\xarr}{\ensuremath{\vec{x}}\xspace}
\newcommand{\yarr}{\ensuremath{\vec{y}}\xspace}
\newcommand{\tharr}{\ensuremath{\vec{\theta}}\xspace}
\newcommand{\Varr}{\ensuremath{\vec{V}}\xspace}
\newcommand{\widejet}{\ensuremath{\mathrm{J}}\xspace}
\newcommand{\ptsub}[1]{\ensuremath{p_{\mathrm{T},#1}}\xspace}
\newcommand{\etsub}[1]{\ensuremath{E_{\mathrm{T},#1}}\xspace}
\newcommand{\ptvecsub}[1]{\ensuremath{\vec{p}_{\mathrm{T},#1}}\xspace}
\newcommand{\mjj}{\ensuremath{m_{\widejet\widejet}}\xspace}
\newcommand{\ptvecjj}{\ensuremath{\ptvecsub{\widejet\widejet}}\xspace}
\newcommand{\ptjj}{\ensuremath{\ptsub{\widejet\widejet}}\xspace}
\newcommand{\etjj}{\ensuremath{\etsub{\widejet\widejet}}\xspace}
\newcommand{\rskew}{\ensuremath{\gamma_{1}^{\text{rel}}}\xspace}
\newcommand{\DMJJ}{\ensuremath{\Delta M_{JJ}}\xspace}

% formatting
\usepackage{enumitem}
\setlist[itemize]{noitemsep, leftmargin=\parindent, topsep=0pt, parsep=0pt}
\setlist[enumerate]{noitemsep, leftmargin=\parindent, topsep=0pt, parsep=0pt}
\newlength{\myfigureskipeqh}
\setlength{\myfigureskipeqh}{0.1cm}
\newcommand{\twofigeqh}[2]{\resizebox{0.98\linewidth}{!}{\includegraphics[height=1cm]{#1}\hspace{\myfigureskipeqh}\includegraphics[height=1cm]{#2}}}
\newcommand{\threefigeqh}[3]{\resizebox{0.98\linewidth}{!}{\includegraphics[height=1cm]{#1}\hspace{\myfigureskipeqh}\includegraphics[height=1cm]{#2}\hspace{\myfigureskipeqh}\includegraphics[height=1cm]{#3}}}

%% Adding report number %%%%%%%
\newcommand{\reportnumber}{FERMILAB-PUB-23-112-CSAID-PPD-QIS}
\usepackage[angle=0,scale=1,color=black,firstpage=true,opacity=1]{background}

\SetBgContents{\footnotesize\sf{\reportnumber}}
\SetBgPosition{current page.north east}
\SetBgHshift{-2in}
\SetBgVshift{-0.5in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{center}{\Large \textbf{
Optimal Mass Variables for Semivisible Jets\\
}}\end{center}

\begin{center}
Kevin Pedro\textsuperscript{1$\star$} and
Prasanth Shyamsundar\textsuperscript{1}
\end{center}

\begin{center}
${}^{1}$ Fermi National Acceleratory Laboratory, Batavia, IL 60510, USA
\\
${}^\star$ {\small \sf pedrok@fnal.gov}
\end{center}

%\linenumbers

\section*{Abstract}
{\bf
Strongly coupled hidden sector theories predict collider production of invisible, composite dark matter candidates mixed with standard model hadrons in the form of semivisible jets. Classical mass reconstruction techniques may not be optimal for these unusual topologies, in which the missing transverse momentum comes from massive particles and has a nontrivial relationship to the visible jet momentum. We apply the artificial event variable network, a semisupervised, interpretable machine learning technique that uses an information bottleneck, to derive superior mass reconstruction functions for several cases of resonant semivisible jet production. We demonstrate that the technique can extrapolate to unknown signal model parameter values. We further demonstrate the viability of conducting an actual search for new physics using this method, by applying the learned functions to standard model background events from quantum chromodynamics.
}

% To remove the TOC, simply cut the following block
\vspace{10pt}
\noindent\rule{\textwidth}{1pt}
\tableofcontents\thispagestyle{fancy}
\noindent\rule{\textwidth}{1pt}
\vspace{10pt}

\section{Introduction}
\label{sec:intro}

Dark matter (DM) is one of the clearest indicators of the existence of physics beyond the standard model (SM).
Its gravitational interactions have been observed in numerous astrophysical contexts, spanning rotation curves~\cite{Rubin:1980zd,Persic:1995ru}, lensing~\cite{Chang:2017kmv}, galaxy cluster collisions~\cite{Clowe:2006eq}, and the cosmic microwave background~\cite{Planck:2018vyg}.
However, attempts to detect dark matter through weak interactions with SM particles, whether via direct interactions, annihilation, or collider production, have so far been unsuccessful.

Dark matter may still be accessible at current-generation experiments if its nature is different from the simplest models of weakly interacting massive particles (WIMPs).
In this paper, we consider the possibility that dark matter is primarily composed of composite particles, much like visible matter.
Composite dark matter may arise from a hidden valley (HV) that communicates with the SM only via weakly-interacting heavy mediator particles~\cite{Strassler:2006im}.
In particular, this hidden valley may be a strongly coupled hidden sector with multiple species of dark quarks \Pqdark charged under a new, confining dark force carried by dark gluons, which form dark hadrons.
The new dark force can be described as ``dark QCD'', in analogy with SM quantum chromodynamics.
Some of the dark hadrons are stable and act as dark matter candidates, while the unstable varieties decay to SM particles such as quark-antiquark pairs.

Such models are more discoverable at colliders than direct detection or annihilation experiments.
Direct detection of dark hadrons is suppressed below the neutrino floor~\cite{Cohen:2017pzm},
and annihilation is expected to be rare for any form of dark matter arising from an asymmetry, whether or not it is composite~\cite{Petraki:2013wwa}.
Assuming collider production of the hidden valley mediator and prompt decays of the unstable dark hadrons, the final state may include ``semivisible'' jets containing both visible SM and invisible DM particles~\cite{Cohen:2015toa}.
It has been shown that the observed dark matter relic density can be obtained from this class of models~\cite{Beauchesne:2018myj,Beauchesne:2019ato}.
Other possible signatures, not considered here, include emerging jets, when the unstable dark hadrons are long-lived~\cite{Schwaller:2015gea}, or soft unclustered energy patterns, when the 't Hooft coupling is large and wide-angle radiation is not suppressed~\cite{Knapen:2016hky}.

The first experimental search for semivisible jets~\cite{CMS:2021dzg} by the CMS experiment at the LHC considered $s$-channel production with a heavy leptophobic \PZprime boson mediator, shown in Fig.~\ref{fig:diagrams} (left).
This search placed limits on the product of the \PZprime production cross section and the branching fraction to dark quarks \ZprimeToDark, which can be translated into limits on \mZprime for specific values of the \PZprime couplings to SM quarks (\gq) and dark quarks (\gqdark).
However, the benchmark values for these couplings, and therefore the predicted cross sections and branching fractions, are ultimately arbitrary.
Hence, a \PZprime, or another $s$-channel mediator, accessible at the LHC may still exist, just interacting too weakly to be produced in detectable quantities in the available datasets.
This search used the transverse mass \MT of the dijet system and the missing transverse momentum \ptmiss to reconstruct the \PZprime.
In simpler topologies, with massless invisible particles well-separated from massive visible particles, \MT is known to be optimal, but it is an open question if it remains optimal in more complicated topologies, such as the semivisible jet case described here.

\begin{figure}[htb!]
\centering
\threefigeqh{figures/zPrime_feyn_orig.pdf}{figures/tChanAssoc_feyn.pdf}{figures/ggPhiPhi_feyn.pdf}
\caption{Representative Feynman diagrams for leading-order production of a \PZprime boson decaying to dark quarks \Pqdark (left), a single \Pbifun boson associated with a dark quark and decaying to an SM quark and a dark quark (center), and a pair of \Pbifun bosons each decaying to a dark quark and an SM quark (right).
}
\label{fig:diagrams}
\end{figure}

In addition to \PZprime bosons and other $s$-channel mediators, another notable possibility is a bifundamental scalar particle \Pbifun, charged under both SM QCD and dark QCD~\cite{Cohen:2017pzm,Beauchesne:2017yhh}.
\Pbifun has Yukawa couplings \sbifun between dark quarks and SM quarks, leading to different final states than the \PZprime $s$-channel case.
Relevant to the goal of this paper, \Pbifun can be produced singly or in pairs, as shown in Fig.~\ref{fig:diagrams} (center, right).
The optimal mass reconstruction for such final states has not been thoroughly investigated.
The first experimental search for emerging jets~\cite{Sirunyan:2018njd}, also from CMS, considers the pair production of \Pbifun, but does not attempt to reconstruct its mass.
A recent search from the ATLAS experiment sets limits on certain models of non-resonant $t$-channel SVJ production via \Pbifun~\cite{ATLAS:2023swa}, but does not target resonant production.
For other final states with pairs of massive particles decaying to both visible and invisible particles, the \MTtwo variable has been shown to be useful~\cite{Lester:1999tx}, though not necessarily optimal or unique~\cite{Matchev:2019bon}.

Here, we employ a new machine learning technique to derive optimal mass reconstructions for these three final states: \PZprime, single \Pbifun, and pairs of \Pbifun.
This technique, called the event variable network (EVN), was first introduced in Ref.~\cite{Kim:2021pcz}.
It uses an information bottleneck to learn a generalized function whose output, the artificial event variable (AEV or simply \Varr), maximizes the mutual information with a target parameter.
Because it is only trained on signal models, with no information provided about background processes, it is semisupervised; and because the target parameter is a physically meaningful quantity, the output of the network is similarly physically meaningful, leading to an interpretable result.
In particular, Ref.~\cite{Kim:2021pcz} shows that for a fully visible final state where the target parameter is the theoretical mediator mass, the learned function is equivalent to the invariant mass calculation, and therefore we expect the EVN to produce optimal mass estimators in semivisible final states, as well.
We apply the EVN to each final state and compare the resulting AEV with existing classical, analytical mass reconstruction algorithms.
We also demonstrate the generalization properties of the EVN by testing it on signal models with different parameter values not used during the network training, as well as on simulations of SM QCD, the primary background for semivisible jet signals.

\section{Models and simulation}
\label{sec:sim}

The dark sector signal model is implemented following the CMS search~\cite{CMS:2021dzg}, which was originally based on the model in Ref.~\cite{Cohen:2015toa}.
The important parameter values and ranges are summarized here, while more details can be found in the aforementioned references.
The number of dark colors is $\Nc = 2$ and the number of dark quark flavors is $\Nf = 2$.
The dark hadron mass is chosen to be $\mDark=20\GeV$, and the dark quark mass is set to $\mqdark = \mDark/2$.
The coupling scale of the dark force is defined in terms of the dark hadron mass, $\lamDark = 3.2(\mDark)^{0.8}$, which is an empirical relation that approximately maximizes the dark hadron multiplicity in the dark shower.
The dark hadron mass scale does not significantly impact the event-level kinematic quantities that are used in this paper, so we do not consider variations of it or the parameters with related values.
The invisible fraction \rinv, defined as the fraction of dark hadrons that are stable, is the most novel and impactful parameter of the signal model, and can take any value between 0 and 1.
Any unstable dark hadrons can decay to pairs of any available species of SM quarks, where availability is defined by the condition $\mDark \geq 2\mq$.
The unstable vector dark hadrons decay democratically, while the unstable pseudoscalar dark hadrons decay via a mass insertion, therefore preferring the most massive available SM quarks.
The probability of producing a vector dark hadron, as opposed to a pseudoscalar, is set to 0.75.
For \PZprime production, the couplings are set to $\gq = 0.25$ and ${\gqdark = 1.0/\sqrt{\smash[b]{\Nc\Nf}} = 0.5}$, which implies ${\Bdark = 47\%}$, consistent with the LHC DM Working Group benchmark~\cite{Albert:2017onk}.
For \Pbifun production, the Yukawa couplings are set to $\sbifun = 1.0$ for all species of SM quarks and dark quarks.

% pythia, madgraph, etc.
The \PZprime signal events are generated using \PYTHIA version 8.230~\cite{Sjostrand:2014zea}, with the dedicated HV module used for showering and hadronization in the dark sector.
The HV module simulates the dark sector dynamics using the Lund string model~\cite{Andersson:1983ia,Sjostrand:1984ic}; we use the default settings for the empirical parameters in this model, which are the values tuned for SM QCD.
Different parameter values, or even a different dynamical model or generator software, could change the dark sector dynamics.
However, such variations generally impact the formation and substructure of jets, rather than their final four-momenta, and therefore are not expected to have a significant impact on the event-level mass reconstruction pursued here.
The NNPDF3.1 leading order (LO) parton density function (PDF)~\cite{Ball:2017nwa} and the CP2 underlying event tune~\cite{Sirunyan:2019dfx} are used.
The events are generated with \mZprime values ranging from 500 to 5000\GeV in steps of 100\GeV, and \rinv values of 0.1, 0.3, 0.5, and 0.7; \mDark is set to 20\GeV.
Approximately 12000 events per signal model are generated for the models with $\rinv = 0.3$, while 6000 events per signal model are generated for the models with other \rinv values.
The \PZprime cross section is computed at next-to-leading-order (NLO).

The \Pbifun signal events are generated using \MGvATNLO version 2.6.5~\cite{Alwall:2014hca} at LO, with the new particles and couplings implemented via \FEYNRULES~\cite{Alloul:2013bka} following Ref.~\cite{Cohen:2017pzm}.
Showering and hadronization are performed using \PYTHIA version 8.240,
and the MLM matching procedure is employed to eliminate double counting of radiation~\cite{Alwall:2007fs}.
The NNPDF3.1 next-to-next-to-leading-order (NNLO) positive-definite PDF and the CP5 tune are used.
The absolute and relative cross sections for processes involving \Pbifun, split by the number of resonant mediators \nbifun, are taken from \MGvATNLO and shown in Fig.~\ref{fig:tchan_xsec} for the chosen Yukawa coupling value $\sbifun = 1.0$.
Nonresonant production depends more strongly on \sbifun than single production, while pair production primarily depends only on $\alpha_{\text{S}}$; therefore, the relative fractions of the resonant production modes would increase for smaller \sbifun values and decrease for larger \sbifun values.
The events with $\nbifun=1$ and $\nbifun=2$ are generated with \mbifun values from 500 to 2000\GeV in steps of 100\GeV; \rinv is set to 0.3 and \mDark is set to 20\GeV.
Approximately 6000 events per signal model are generated.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/tchan_xsecs_fracs.pdf}
\caption{The absolute (left) and relative (right) cross sections for processes involving \Pbifun mediators.
}
\label{fig:tchan_xsec}
\end{figure}

% todo: add nevents for each sample?
The QCD multijet background sample is generated with \PYTHIA version 8.205, using a biased sampling to generate a roughly flat distribution in $15<\pthat<7000\GeV$.
A weight is subsequently applied to produce a physical distribution by inverting the sampling bias.
The NNPDF2.3 LO PDF~\cite{Ball:2012cx} and the CUETP8M1 tune~\cite{Khachatryan:2015pea} are used.
Approximately 10 million QCD events are generated in the flattened \pthat space.

% clustering, mt2, etc.
Given the goal of this paper to learn about fundamental kinematic relationships, no detector simulation is performed and generator-level quantities are used.
Visible particles denoted as stable by the generator are clustered into jets \widejet using the anti-\kt algorithm~\cite{Cacciari:2008gp}, implemented in the \FASTJET software~\cite{Cacciari:2011ma}.
A distance parameter of $R=0.8$ is used because the additional decay step from the dark sector to the SM leads semivisible jets to have a broader spread in their visible constituents than SM jets.
The jets are sorted by their transverse momentum.
The missing transverse momentum is computed as the negative of the vector sum of the transverse momentum vectors of all visible stable particles.
Computations related to the \MTtwo variable are performed using Ref.~\cite{Lester:2014yga}.

\section{Architecture and training}
\label{sec:arch}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/EVN_diagram.pdf}
\caption{A diagram of the composite network architecture, showing the inputs, neural network blocks, output, and loss function. Example inputs are indicated for the representative case of reconstructing a mass value using four-vector values; $m_{\text{rand}}$ represents the random theory parameter values assigned for class 0 events, as described in the text.
}
\label{fig:evn_diagram}
\end{figure}

The EVN is trained using a composite neural network; its inputs, structure, and outputs are shown in Fig.~\ref{fig:evn_diagram}.
The event data are prepared for training the network by splitting the simulated events into two classes.
In class 1, the values of the theory parameters \tharr correspond to the kinematic input variables \xarr, while in class 0, the values of \tharr are random and do not correspond to \xarr.
The EVN is the first component and serves as the information bottleneck.
It is a fully-connected network that takes the inputs \xarr and produces output \Varr, which is typically, though not necessarily, a single value per event.
The second component is a classifier, also a fully-connected network, which combines the EVN output \Varr and \tharr to determine the event class, with the binary crossentropy (BCE) loss as its objective to minimize.
%In class 1 events with correct \tharr values, the inputs \xarr can be combined in a consistent way to yield \Varr close to \tharr, while in class 0 events, there is no consistent way to obtain a value close to \tharr.
Because the two components of the network are jointly optimized, the EVN learns an optimal and general function to combine the inputs into \Varr; this distinguishes the technique from a simple regression.
In class 0 events, \tharr and \xarr are independent, so their joint distribution is simply the factorized distribution $p_{\Varr}p_{\tharr}$, the product of their independent probabilities.
In class 1 events, \xarr arises from \tharr, so their joint distribution $p_{\Varr,\tharr}$ depends on their conditional distribution.
The classification process maximizes the ability to distinguish between the two classes, therefore maximizing the mutual information between \tharr and \Varr, defined as $I(\Varr; \tharr) = \int_{\Varr} \int_{\tharr} p_{\Varr,\tharr} \log{[p_{\Varr,\tharr}/(p_{\Varr}p_{\tharr})]}$.
A more thorough derivation of this network is given in Ref.~\cite{Kim:2021pcz}.

% values for these datasets, mass step / class balance, hyperparam table
The hyperparameters used to train the composite network are summarized in Table~\ref{tab:hyperparam}.
The datasets are split into training, validation, and testing portions;
the validation portion is used to check for overtraining by comparing the loss values,
while the testing dataset is used for calibration and statistical assessments, as described in Section~\ref{sec:schan}.
In addition, a second, independent testing dataset is employed for more in-depth physical comparisons.
While an exhaustive hyperparameter scan is not performed, we note a few findings.
The chosen values for batch and layer sizes perform better than smaller values, while larger values run the risk of memorization or overtraining.
The total number of trainable parameters in the network is 20706.
The chosen learning rate performs better than larger values.
Extending the number of epochs further does not improve the result.
The Adam optimizer~\cite{Kingma:2014vow} is used, and ReLU activation is applied to all internal layers of both fully-connected networks.
The training takes a few minutes on an Nvidia RTX 2080 Super, which is a typical consumer GPU.

We also note one difference with respect to Ref.~\cite{Kim:2021pcz}:
in class 1 events, as described in Section~\ref{sec:sim}, mediator mass values are generated in discrete steps of 100\GeV rather than continuously.
(The ``fake'' mass values assigned to class 0 events are still generated continuously.)
This performs as well as the continuous approach as long as balance is maintained between the number of events with each discrete value in the overall dataset.

\begin{table}[htb]
\caption{The hyperparameter values used to train the composite network.}
\centering
\begin{tabular}{ll}
Parameter & Value \\
\hline
\% events used for training & 64 \\
\% events used for validation & 16 \\
\% events used for testing & 20 \\
Batch size & 5000 \\
Epochs & 100 \\
EVN layer sizes & 128, 64, 64, 64, 32 \\
Classifier layer sizes & 16, 16, 16 \\
Learning rate & 0.001 \\
\end{tabular}
\label{tab:hyperparam}
\end{table}

\section{\texorpdfstring{\PZprime}{Z'} production}
\label{sec:schan}

\subsection{Mass variables}
\label{sec:schanBase}

\begin{sloppypar}As discussed previously, the variable traditionally used to reconstruct the mediator mass for \ZprimeToDark is the transverse mass. This variable is defined as $\MT^2 = (\etjj+\etmiss)^2 - (\ptvecjj + \ptvecmiss)^2 =\allowbreak \mjj^2 + 2(\sqrt{\smash[b]{\mjj^2 +\ptjj^2}}\ptmiss - \ptvecjj\cdot\ptvecmiss)$, where $\etjj = \sqrt{\smash[b]{\mjj^2 + \ptjj^2}}$ is the energy of the massive dijet system, with \mjj and \ptjj the invariant mass and transverse momentum of that system, respectively, and $\etmiss = \ptmiss$ is the energy of the invisible system, which is assumed to be massless.
However, there is actually another classical variable that can improve on \MT, using the \MTtwo-Assisted On Shell (MAOS) technique~\cite{Choi:2010dw}.
MAOS was originally derived for cases like $\PH \to \PW\PW$, where one resonance decays to two resonances, each of which decays to a visible and invisible component.
The \MTtwo algorithm divides the missing transverse momentum into two parts, one corresponding to each visible component of the event, $p^a$ and $p^b$.
MAOS promotes these two \ptvecmiss two-vectors to four-vectors by assigning $\slashmiss{m}_{i} = 0$ and $\slashmiss{p}_z^i = p_z^i (\ptmiss^i / \pt^i)$, where $i = a,b$, and then computes the full invariant mass of the visible and invisible components, defined as \MAOS.\end{sloppypar}

As a first test, we apply the EVN to \ZprimeToDark events with $\mDark = 20\GeV$ and $\rinv = 0.3$. The network inputs \xarr include the leading two jet four-vectors, which are represented as $(E, p_x, p_y, p_z)$ for each jet, and the \ptvecmiss two-vector, which is represented as $(\pxmiss, \pymiss)$.
The correlations between the network output $V$, the two classical variables \MT and \MAOS, and the theory parameter \mZprime are shown in Fig.~\ref{fig:schan_correl}.
These plots additionally display the calibration procedure, which will be used in subsequent plots.
Calibration is necessary to make direct comparisons between the classical variables and $V$, because the absolute scale of $V$ is not fixed.
For consistency, all reconstructed variables are calibrated the same way, via a linear fit to the theory parameter.
(The constant term in the linear fit is found to be very small and is therefore neglected.)
The correlation plots, and therefore the calibration procedure, use the independent dataset composed of the 20\% of events reserved for testing.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/schan/calib_heatmap_0_theory_MT.pdf}\\
\includegraphics[width=0.98\linewidth]{figures/schan/calib_heatmap_0_theory_MAOS.pdf}
\caption{Top: Correlations between \MT, $V$, and \mZprime. Bottom: Correlations between \MAOS, $V$, and \mZprime. The black lines show the calibrations of the reconstructed variables to the scale of \mZprime, where $m$ is the slope of the fit $x = my$.
}
\label{fig:schan_correl}
\end{figure}

We compare the classical and learned variables using several statistics, including Kendall's $\tau$ and Spearman's rank coefficient $r_s$.
$V$ is highly correlated with, but not identical to, both \MT and \MAOS.
Additionally, we introduce a new statistic called the relative skewness of the correlation or \rskew, which is defined as follows:
\begin{enumerate}
\item perform a linear fit $Y = \alpha X$, where $X$ and $Y$ are two datasets, resulting in the best-fit value $\hat{\alpha}$ for $\alpha$;
\item define $\tilde{Y} = Y/\hat{\alpha}$, so that the linear fit relationship becomes $\tilde{Y} = X + \mathrm{residual}$;
\item define the line $\tilde{Y} = X$ as the new independent axis, corresponding to a $45\degree$ rotation into new coordinates ${X^{\prime}} = \tfrac{1}{\sqrt{2}}(X+\tilde{Y})$, ${Y^{\prime}} = \tfrac{1}{\sqrt{2}}(\tilde{Y}-X)$;
\item compute \rskew as the Fisher-Pearson skewness coefficient of $Y^{\prime}/X^{\prime}$, the relative displacement from the $X^{\prime}$ axis.
\end{enumerate}
The utility of the new statistic can be observed by comparing the values of the other correlation statistics for $V$ vs. \MT and $V$ vs. \MAOS, which are very similar.
In contrast, \rskew is much smaller for \MAOS than for \MT.
This indicates that $V$ tends to be larger than \MT, but there is little bias in the difference between $V$ and \MAOS.

The similarity between $V$ and \MAOS can be further observed in Fig.~\ref{fig:schan_1d}, which compares distributions of each variable for several \mZprime values.
The second independent testing dataset is used in this figure.
As noted above, the calibrated version of each variable is used to avoid spurious differences in scale.
It can be seen that the distribution of $V$ is similar in shape to \MAOS and is indeed narrower than \MT.
Therefore, we conclude that the \MAOS procedure results in a nearly optimal reconstruction under the given assumptions.
It is expected that $V$ and \MAOS would not be identical, because \MAOS depends on the result of the \MTtwo calculation, which is not a singularity variable~\cite{Matchev:2019bon}.
% todo: add loss plot?

\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\linewidth]{figures/schan/hist1d.pdf}
\caption{The distributions of \MT, \MAOS, and $V$ for several \mZprime values, normalized to unit area.
}
\label{fig:schan_1d}
\end{figure}

\subsection{Discovery potential}
\label{sec:schanDiscov}

Subsequently, we demonstrate the utility of the learned variable $V$ for discovery of this channel of semivisible jet production.
We compare the sensitivity, defined as $Q = S/\sqrt{B}$ where $S$ and $B$ are the signal and background yields, from binned mass distributions.
Examining the strategy from Ref.~\cite{CMS:2021dzg}, rejection of the major background from QCD multijets is primarily accomplished via the ``transverse ratio'', a relative variable defined by $\RT = \ptmiss/M$, where $M$ is the reconstructed mass variable.
Here, we use generator-level simulations of QCD, which lack instrumental effects such as dead calorimeter cells that induce artificial \ptmiss.
Therefore, we apply a selection $\RT > 0.1$, which is looser than the experimental thresholds of 0.15--0.25.
The lower \ptmiss values in the generator-level background imply that fewer events have high \RT values, so this looser selection provides a level of background rejection similar to the realistic analysis.
We also require $\pt > 200\GeV$ for the leading two jets; all other selections are omitted for simplicity.
For each reconstructed mass variable, the selection is applied using \RT defined in terms of that variable.
The resulting distributions are shown in Fig.~\ref{fig:schan_signif}, with both the signal and background samples normalized to an integrated luminosity of 138\fbinv.
The background distributions are very similar for all three reconstructed mass variables;
we emphasize here that the EVN was trained only on signal events and has no explicit knowledge of QCD multijet background events.
The bin-by-bin significance ratios can also be examined, noting that the bin at the peak of the distribution provides the largest contribution to the overall sensitivity.
It can be seen that the sensitivity of the artificial variable in this peak bin is approximately 30\% higher than \MT and similar to \MAOS for the higher \mZprime values.
The overall significance for each signal model and mass variable can be approximated by adding the significance of each bin in quadrature.
This calculation shows that the artificial variable has a 3--5\% higher significance than \MT, while \MAOS has only a ${\approx}1\%$ higher significance.
Because the improvements are minor, searches may still prefer to use the simpler \MT variable, but in the event of a discovery, the \PZprime mass could be measured with better resolution using the artificial variable.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\linewidth]{figures/schan/signif_schan.pdf}
\caption{The distributions of each reconstructed mass variable for \PZprime and QCD multijet processes. The middle pane shows the significance, and the bottom pane shows the ratio of the significance, comparing the two classical variables to $V$. The distributions in the bottom pane are truncated to eliminate statistical fluctuations from limited numbers of events in the high tails of the signal mass distributions.
}
\label{fig:schan_signif}
\end{figure}

\subsection{Varying \texorpdfstring{\rinv}{invisible fraction}}
\label{sec:schanRinv}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\linewidth]{figures/schan/rinv_comp_MAOS_2000.pdf}
\caption{The distributions of \MAOS for different \rinv values, using samples with $\mZprime = 2000\GeV$.
}
\label{fig:schan_maos_rinv}
\end{figure}

The \rinv parameter has a large effect on the potential accuracy of the \PZprime mass reconstruction, as shown in Fig.~\ref{fig:schan_maos_rinv} for \MAOS.
To assess the generalization properties of the EVN, we train four separate instances of the network, corresponding to distinct signal samples with \rinv values of 0.1, 0.3, 0.5, and 0.7.
We also train a fifth instance using a sample with all of those \rinv values mixed together.
Each trained instance is then applied to samples with all four \rinv values, so each instance trained on a specific \rinv value must process signal events with different \rinv values.
Figure~\ref{fig:schan_rinv_comp} shows the results, with each network's output calibrated using the mixed \rinv sample for consistency.
In general, we see that networks trained with lower values of \rinv also work well on signals with higher values of \rinv.
The networks trained with higher values of \rinv do not work quite as well on signals with lower values of \rinv.
This is expected, as some kinematic information is lost when \rinv increases; more dark hadrons are combined into the single \ptvecmiss two-vector rather than contributing to the jet four-vectors.
The network trained on a mixed sample with all \rinv values works well for all \rinv values.
This test demonstrates that the EVN can provide good mass reconstruction even when applied to signals with notably different parameter values than used in training, as long as the basic kinematic behavior is similar.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/schan/rinv_comp.pdf}
\caption{The distributions of the artificial variable $V$ from networks trained and tested on different \rinv values. The subscripts in the $V$ entries in the legend indicate which \rinv value was used for training, while the larger text at the bottom of each legend indicates which \rinv sample was used to produce the distributions.
}
\label{fig:schan_rinv_comp}
\end{figure}

\subsection{Out-of-band masses}
\label{sec:schanMass}

A natural test of the EVN's generalization capabilities is exposing it to mediator mass values outside of the training dataset.
To perform this test, we retrain the EVN on three subsets of the signal samples with different requirements on the \PZprime mass: $\mZprime > 2500$; $\mZprime \leq 1500$ or $\mZprime > 3500$; and $\mZprime \leq 2500\GeV$.
Because these restrictions significantly reduce the number of events available for training, the EVN layer sizes are reduced by a factor of 2 to 64, 32, 32, 32, 16, leading to a total of 6050 trainable parameters, and the batch size is reduced to 1000.
Each trained network is calibrated using the full range of \mZprime values for consistency.
We observe in Fig.~\ref{fig:schan_mass_comp} that the EVN performs equally well on signals with mediator mass values below, in between, or above the range of mass values used for training, when compared to the EVN trained on the full range of available masses from Section~\ref{sec:schan}.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/schan/mass_comp.pdf}
\caption{The distributions of the artificial variable $V$ from networks trained on different ranges of \mZprime values. The subscripts in the $V$ entries in the legend indicate the \mZprime range used for training (with no subscript used for the case where $V$ is trained on the entire range of masses), while the larger text at the bottom of each legend indicates which \mZprime sample was used to produce the distributions.
}
\label{fig:schan_mass_comp}
\end{figure}

\section{\texorpdfstring{\Pbifun}{Bifundamental} production}
\label{sec:tchan}

\subsection{Single production}
\label{sec:tchanSingle}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\linewidth]{figures/tchanSingle/pt_comp_1000.pdf}
\caption{The \pt distributions of the three leading jets in single production events with $\mbifun=1000\GeV$.
}
\label{fig:tchanSingle_pt}
\end{figure}

The resonant production of a single bifundamental mediator \Pbifun leads to a final state with two high-\pt jets from the mediator decay---one SM jet and one semivisible jet---in association with a second low-\pt semivisible jet.
The transverse momentum distributions for the leading three jets in such events are shown in Fig.~\ref{fig:tchanSingle_pt}.
Based on the kinematic differences between the two semivisible jets, the missing energy in the event is expected to be associated primarily with the higher-\pt semivisible jet.
Therefore, the classical approach to reconstruct the resonance mass is to compute the transverse mass from the two leading jets and the \ptmiss, similar to the \PZprime case.
Because the other leading jet is a fully visible SM jet, there is no need to employ the MAOS procedure to split the \ptmiss into multiple components.
However, it is possible that optimal usage of information from the third jet might improve the mass resolution; as a semivisible jet, it may contribute to the \ptmiss.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/tchanSingle/heatmap_0_theory_MT.pdf}
\caption{Correlations between \MT, $V$, and \mbifun in single production events.
}
\label{fig:tchanSingle_correl}
\end{figure}

Therefore, we train two versions of the EVN on single \Pbifun production events.
The first version uses just the two leading jet four-vectors and the \ptmiss two-vector as input,
while the second version also includes the third jet four-vector.
Because a smaller mass range is explored for \Pbifun events, resulting in fewer training events, we use the alternative hyperparameter settings described in Section~\ref{sec:schanMass}.
The first network is compared to \mbifun and \MT in Fig.~\ref{fig:tchanSingle_correl},
and the corresponding mass distributions are shown in Fig.~\ref{fig:tchanSingle_1d} (left).
It can be seen that the artificial variable does not exactly reproduce \MT but highly correlates with it.
Figure~\ref{fig:tchanSingle_1d} (right) shows the correlation between the first and second versions of the network, which is similarly high.
This indicates that including the third jet information does not improve the mass reconstruction for this topology.
We conclude that the transverse mass is effectively optimal here.

\begin{figure}[htb!]
\centering
\twofigeqh{figures/tchanSingle/hist1d.pdf}{figures/tchanSingle/heatmap_AEVs_vs_AEV0.pdf}
\caption{Left: the distributions of \MT and $V$ for several \mbifun values in single production events, normalized to unit area. Right: the correlations between $V$ using two jets and $V$ using three jets.
}
\label{fig:tchanSingle_1d}
\end{figure}

\subsection{Pair production}
\label{sec:tchanPair}

The pair production of \Pbifun results in at least four jets, two of which are semivisible, along with missing energy.
The classical approach to reconstruct the mediator mass in this final state is \MTtwo or a similar variable.
In this case, disambiguation must be performed, to attempt to combine the four jets into two visible four-vectors in a way that correctly pairs jets from the same mediator.
Here, we use a standard method: choosing the pairing that minimizes the invariant mass difference between the two pairs, $\DMJJ = |M_{J_{a}J_{b}} - M_{J_{c}J_{d}}|$.
We also compare to the variable \MTtwoGen, where the disambiguation is done correctly using generator-level information about the mediator decays, to represent an upper bound on possible performance.

The EVN is trained with the four-vectors of the leading four jets and the \ptvecmiss two-vector as input.
As in the single production case, the alternative hyperparameter settings from Section~\ref{sec:schanMass} are used.
The results are shown in Figs.~\ref{fig:tchanPair_correl} and \ref{fig:tchanPair_1d}.
The artificial variable is correlated with \MTtwo, but is not identical to it.
\MTtwo has a substantial low-mass tail, which is typically caused by incorrect pairings from the \DMJJ heuristic.
$V$, while it does not achieve the resolution of the generator-level version, largely eliminates this low-mass behavior.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/tchanPair/heatmap_0_theory_PairMT2Reco.pdf}
\caption{Correlations between \MTtwo, $V$, and \mbifun in pair production events.
}
\label{fig:tchanPair_correl}
\end{figure}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\linewidth]{figures/tchanPair/hist1d.pdf}
\caption{The distributions of \MTtwo, \MTtwoGen, and $V$ for several \mbifun values in pair production events, normalized to unit area.
}
\label{fig:tchanPair_1d}
\end{figure}

To understand the impact of the apparent improvements of the artificial variable over the classical reconstruction, we conduct a sensitivity comparison, following the procedure and selection in Section~\ref{sec:schanDiscov}.
Figure~\ref{fig:tchanPair_signif} shows the result.
We observe that, compared to \MTtwo, the artificial variable shifts the background to lower mass values and the signal to higher values, and both effects improve the sensitivity in the higher end of the mass distribution.
The improvement is especially pronounced for higher \mbifun values, which are more challenging to discover in the LHC dataset because of their small cross sections.
Computing the approximate overall significance shows that the artificial variable improves on \MTtwo by 17\% for $\mbifun=1000\GeV$ and 81\% for $\mbifun=2000\GeV$.
This again demonstrates the power of the semisupervised approach; the EVN, by learning an optimal generalized function for the signal topology, pushes the background away from resonant signal distributions without being trained on background kinematic distributions.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\linewidth]{figures/tchanPair/signif_tchanPair.pdf}
\caption{The distributions of each reconstructed mass variable for \Pbifun pair production and QCD multijet samples. The middle pane shows the significance, and the bottom pane shows the ratio of the significance, comparing \MTtwo to $V$. The distributions in the bottom pane are truncated to eliminate noisy values from statistical fluctuations in the high tails of the signal mass distributions.
}
\label{fig:tchanPair_signif}
\end{figure}

\section{Conclusion}
\label{sec:conc}

Semivisible jets are a novel phenomenological signature of strongly coupled dark matter, corresponding to stable dark hadrons that arise, along with unstable dark hadrons, from a hidden sector with a new, confining force.
We apply the artificial event variable network (EVN) to achieve optimal mediator mass reconstruction for several resonant production channels: a \PZprime boson, a single bifundamental scalar \Pbifun, or a pair of \Pbifun.
This network uses an information bottleneck to produce interpretable output that is directly correlated with a given parameter, here the mediator mass.
The EVN is trained with a semisupervised approach that uses only signal events, without any knowledge of standard model background processes.

In the \PZprime case, we show that the learned artificial variable is superior to the transverse mass \MT and similar to the \MTtwo-Assisted On Shell (MAOS) scheme.
Using the artificial variable results in a moderate improvement, compared to the classical variables, in the sensitivity of a simplified search for this process.
For this result, we compare signal events to quantum chromodynamics (QCD) multijet background events, for which the EVN produces a steeply falling mass distribution despite never being exposed to such events during training.
We also use the \PZprime case to further demonstrate the generalization capabilities of the EVN, showing that its performance generalizes to signal models with mediator mass values or invisible fractions (the proportion of dark hadrons that are stable) that differ from the training data.

We investigate the phenomenological behavior of single and pair production of \Pbifun mediators for the first time.
We show with the EVN that \MT is approximately optimal for the single production case.
In the pair production case, we find that the artificial variable significantly improves over the classical \MTtwo and offers strong potential to improve the collider discovery reach for this process.

Overall, the artificial event variable network offers a promising avenue to improve event reconstruction in an interpretable and generalized way, by learning a function that correlates with physical information.
It can be applied to improve sensitivity, increase discovery potential, and provide phenomenological knowledge even for complicated signals such as semivisible jet production.

\section*{Acknowledgements}

\begin{sloppypar}We thank Eshwen Bhal, Tim Cohen, Annapaola de Cosa, Sarah Eno, Aran Garcia-Bellido, Thomas Klijnsma, Kyoungchul Kong, Hou Keong Lou, Benedikt Maier, Sandeep Madireddy, Christopher Madrid, Konstantin Matchev, Siddharth Mishra-Sharma, Stephen Mrenna, Matt Strassler, and Chin Lung Tan for useful discussions.
The color scheme used in the figures in this paper is intended to be accessible and was derived in Ref.~\cite{Petroff:2021}.\end{sloppypar}

\paragraph{Funding information}
K. Pedro and P. Shyamsundar are supported by Fermi Research Alliance, LLC under Contract No. DE-AC02-07CH11359 with the U.S. Department of Energy, Office of Science, Office of High Energy Physics. P. Shyamsundar is supported by the U.S. Department of Energy, Office of Science, Office of High Energy Physics QuantISED program under the grants ``HEP Machine Learning and Optimization Go Quantum'', Award Number 0000240323, and ``DOE QuantiSED Consortium QCCFP-QMLQCF'', Award Number DE-SC0019219.

\bibliography{evn_svj.bib}

\clearpage
\appendix
\numberwithin{figure}{section}
\numberwithin{table}{section}
\section{Additional information} \label{app:addl}

\subsection{Code availability}

The code used to train the neural network and produce the figures in this paper can be found at \url{https://github.com/kpedro88/evn_svj_public}.
The \MADGRAPH and \PYTHIA settings used to generate the signal and background events can also be found there.

\subsection{Supporting distributions}

\subsubsection{\texorpdfstring{\PZprime}{Z'} production}

Figure~\ref{fig:app_schan_rinv_correl} shows correlations between artificial variables trained using signals with different \rinv values.
High correlations with the artificial variable trained on all \rinv values are observed.
The relative skewness \rskew shows that variables trained on low \rinv have moderate biases toward high values compared to the mixed training case and variables trained on high \rinv have moderate biases toward low values, as expected.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/schan/rinv/heatmap_AEVs_vs_AEV0.pdf}
\caption{Correlations between $V$ trained on different \rinv values and $V$ trained on a mix of \rinv values in \PZprime production.
}
\label{fig:app_schan_rinv_correl}
\end{figure}

Figure~\ref{fig:app_schan_mass_correl} shows correlations between artificial variables trained using different ranges of \mZprime values.
High correlations with the artificial variable trained on the full range of \mZprime values are observed.
The relative skewness \rskew shows that the variable trained on low \mZprime has moderate bias toward low values compared to the full range case and the variable trained on high \mZprime has moderate bias toward high values, as expected.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.98\linewidth]{figures/schan/mass/heatmap_AEVs_vs_AEV0.pdf}
\caption{Correlations between $V$ trained on different ranges of \mZprime values and $V$ trained on a the full range of \mZprime values.
}
\label{fig:app_schan_mass_correl}
\end{figure}

\subsubsection{\texorpdfstring{\Pbifun}{Bifundamental} production}

Figure~\ref{fig:app_tchanSingle_1dJ3} shows the distribution of \MT and the artificial variable trained using three jet four-vectors and the \ptvecmiss two-vector, rather than the default two jet four-vectors and \ptvecmiss, in single \Pbifun production events.
The distribution is very similar to the variable using the default inputs in Fig.~\ref{fig:tchanSingle_1d}.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\linewidth]{figures/tchanSingle/J3/hist1d.pdf}
\caption{The distributions of \MT and $V$ trained with three jets for several \mbifun values in single production events, normalized to unit area.
}
\label{fig:app_tchanSingle_1dJ3}
\end{figure}

\end{document}
