\pdfoutput=1
\documentclass[11pt, letterpaper]{article}
%
\usepackage{blindtext}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue!70!black,
	citecolor=blue!70!black,
	urlcolor=blue!70!black
}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{thm-restate}

%
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage[lined,boxed,ruled,norelsize,algo2e,linesnumbered]{algorithm2e}
\usepackage{bbold}
\usepackage{bbm}

\usepackage{graphicx}
\graphicspath{{./figs/}}

\usepackage{cleveref}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{problem}{Problem}

\newcommand{\defeq}{:=}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\norms}[1]{\lVert#1\rVert}
\newcommand{\normop}[1]{\left\lVert#1\right\rVert_{\textup{op}}}
\newcommand{\inprod}[2]{\left\langle#1, #2\right\rangle}
\newcommand{\inprods}[2]{\langle#1, #2\rangle}
\newcommand{\eps}{\epsilon}
\newcommand{\lam}{\lambda}
\newcommand{\argmax}{\textup{argmax}}
\newcommand{\argmin}{\textup{argmin}} 
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\diag}[1]{\textbf{\textup{diag}}\left(#1\right)}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\thalf}{\tfrac{1}{2}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\textup{Var}}
\newcommand{\Vol}{\textup{Vol}}
\newcommand{\Nor}{\mathcal{N}}
\newcommand{\smax}{\textup{smax}}
\newcommand{\smin}{\textup{smin}}
\newcommand{\Tr}{\textup{Tr}}
\newcommand{\opt}{\textup{OPT}}
\newcommand{\xset}{\mathcal{X}}
\newcommand{\yset}{\mathcal{Y}}
\newcommand{\zset}{\mathcal{Z}}
\newcommand{\ma}{\mathbf{A}}
\newcommand{\mb}{\mathbf{B}}
\newcommand{\mc}{\mathbf{C}}
\newcommand{\mm}{\mathbf{M}}
\newcommand{\mn}{\mathbf{N}}
\newcommand{\mr}{\mathbf{R}}
\newcommand{\mw}{\mathbf{W}}
\newcommand{\mv}{\mathbf{V}}
\newcommand{\mx}{\mathbf{X}}
\newcommand{\my}{\mathbf{Y}}
\newcommand{\mlam}{\boldsymbol{\Lambda}}
\newcommand{\mmu}{\mathbf{U}}
\renewcommand{\lam}{\lambda}
\newcommand{\ai}{\ma_{i:}}
\newcommand{\aj}{\ma_{:j}}
\newcommand{\id}{\mathbf{I}}
\newcommand{\jac}{\mathbf{J}}
\usepackage{xcolor}
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\newcommand{\kjtian}[1]{\textcolor{burntorange}{\textbf{kjtian:} #1}}
\newcommand{\arun}[1]{\textcolor{red}{\textbf{arun:} #1}}
\newcommand{\tO}{\widetilde{O}}
\newcommand{\nnz}{\textup{nnz}}
\newcommand{\Prox}{\textup{Prox}}
\newcommand{\Par}[1]{\left(#1\right)}
\newcommand{\Brack}[1]{\left[#1\right]}
\newcommand{\Brace}[1]{\left\{#1\right\}}
\newcommand{\Abs}[1]{\left|#1\right|}
\newcommand{\Cov}{\textup{Cov}}
\newcommand{\oracle}{\mathcal{O}}
\newcommand{\0}{\mathbb{0}}
\newcommand{\alla}{\mathcal{A}}
\newcommand{\rlp}[1]{r_{\ma}^{(#1)}}
\newcommand{\rsdp}[1]{r_{\alla}^{(#1)}}
\newcommand{\rlpn}{r_{\textup{lp}}}
\newcommand{\rsdpn}{r_{\textup{sdp}}}
\newcommand{\Sym}{\mathbb{S}}
\newcommand{\PSD}{\Sym_{\succeq \mzero}}
\newcommand{\PD}{\Sym_{\succ \mzero}}
\newcommand{\mzero}{\mathbf{0}}
\newcommand{\Lma}{L}
\newcommand{\bx}{\bar{x}}
\newcommand{\by}{\bar{y}}
\newcommand{\hx}{\hat{x}}
\newcommand{\hy}{\hat{y}}
\newcommand{\hz}{\hat{z}}
\newcommand{\hlam}{\widehat{\lambda}}
\newcommand{\bz}{\bar{z}}
\newcommand{\bmy}{\overline{\my}}
\newcommand{\x}{^\mathsf{x}}
\newcommand{\y}{^\mathsf{y}}
\newcommand{\Ograd}{\oracle_{\textup{grad}}}
\newcommand{\Oxgrad}{\oracle_{\textup{xgrad}}}
\newcommand{\mq}{\mathbf{Q}}
\newcommand{\tmm}{\widetilde{\mm}}
\newcommand{\tmv}{\mathcal{T}_{\textup{mv}}}
\renewcommand{\l}{\left\langle}
\renewcommand{\r}{\right\rangle}
\newcommand{\me}{\mathbf{E}}
\newcommand{\md}{\mathbf{D}}
\newcommand{\tg}{\tilde{g}}
\newcommand{\normtr}[1]{\left\lVert#1\right\rVert_{\textup{tr}}}
\newcommand{\Olinreg}{\oracle_{\textup{reglin}}}
\newcommand{\Obr}{\oracle_{\textup{br}}}
\newcommand{\bs}{\bar{s}}
\newcommand{\med}{\textup{med}}
\newcommand{\mg}{\mathbf{G}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\tf}{\tilde{f}}
\newcommand{\hmy}{\widehat{\my}}
\newcommand{\tq}{\tilde{q}}
\newcommand{\hw}{\hat{w}}
\newcommand{\hb}{\hat{b}}
\newcommand{\bw}{\bar{w}}
\newcommand{\bb}{\bar{b}}
\newcommand{\Omatexp}{\oracle_{\textup{meq}}}
\newcommand{\tOgrad}{\widetilde{\oracle}_{\textup{grad}}}
\newcommand{\tOxgrad}{\widetilde{\oracle}_{\textup{xgrad}}}
\newcommand{\tnabla}{\widetilde{\nabla}}
\newcommand{\Ltot}{L_{\textup{tot}}}
\newcommand{\proj}{\boldsymbol{\Pi}}
\newcommand{\projx}{\proj_{\xset}}
\newcommand{\projy}{\proj_{\yset}} 
\title{Revisiting Area Convexity: Faster Box-Simplex Games \\ and Spectrahedral Generalizations}
\author{Arun Jambulapati\thanks{Simons Institute, {\tt jmblpati@berkeley.edu}. Work completed at the University of Washington.} \and Kevin Tian\thanks{University of Texas at Austin, {\tt kjtian@cs.utexas.edu}. Work completed at Microsoft Research.}}
\date{}
\begin{document}

\maketitle

\begin{abstract}
We investigate different aspects of area convexity \cite{Sherman17}, a mysterious tool introduced to tackle optimization problems under the challenging $\ell_\infty$ geometry. We develop a deeper understanding of its relationship with more conventional analyses of extragradient methods \cite{Nemirovski04, Nesterov07}. We also give improved solvers for the subproblems required by variants of the \cite{Sherman17} algorithm, designed through the lens of relative smoothness \cite{BauschkeBT17, LuFN18}.

Leveraging these new tools, we give a state-of-the-art first-order algorithm for solving box-simplex games (a primal-dual formulation of $\ell_\infty$ regression) in a $d \times n$ matrix with bounded rows, using $O(\log d \cdot \eps^{-1})$ matrix-vector queries. Our solver yields improved runtimes for approximate maximum flow, optimal transport, min-mean-cycle, and other basic combinatorial optimization problems. We also develop a near-linear time algorithm for a matrix generalization of box-simplex games, capturing a family of problems closely related to semidefinite programs recently used as subroutines in robust statistics and numerical linear algebra.
\end{abstract}

\newpage
	
%
\section{Introduction}
\label{sec:intro}

Box-simplex, or $\ell_\infty$-$\ell_1$, games are a family of optimization problems of the form
\begin{equation}\label{eq:boxsimplex}
\min_{x \in [-1, 1]^n} \max_{y \in \Delta^d} x^\top \ma y - b^\top y + c^\top x.
\end{equation}
These problems are highly expressive, as high-accuracy solutions capture linear programs (LPs) as a special case \cite{LeeS15}.\footnote{This follows as \eqref{eq:boxsimplex} generalizes $\ell_\infty$ regression $\min_{x \in [-1, 1]^n} \norm{\ma x - b}_\infty$, see Section 3.1 of \cite{SidfordT18}.} More recently, approximation algorithms for the family \eqref{eq:boxsimplex} have been applied to improve runtimes for applications captured by LPs naturally bounded in the $\ell_\infty$ geometry. The current state-of-the-art first-order method for \eqref{eq:boxsimplex} is by Sherman \cite{Sherman17}, who used the resulting solver to obtain a faster algorithm for the maximum flow problem. As further consequences, solvers for \eqref{eq:boxsimplex} have sped up runtimes for optimal transport \cite{JambulapatiST19}, a common task in modern computer vision \cite{KolouriPTSR17}, and min-mean-cycle \cite{AltschulerP20}, a basic subroutine in solving Markov decision processes \cite{ZwickP96}, among other combinatorial optimization problems \cite{AssadiJJST22, JambulapatiJST22}.

The runtimes of first-order methods for approximating \eqref{eq:boxsimplex} are parameterized by an additive accuracy $\eps > 0$ and a Lipschitz constant\footnote{See Section~\ref{ssec:notation} for notation used throughout the paper. $\norm{\ma}_{1 \to 1}$ is the largest $\ell_1$ norm of any column of $\ma$.} $L \defeq \norm{\ma}_{1 \to 1}$; in several prominent applications of \eqref{eq:boxsimplex} where the relevant $\ma$ has favorable structure, e.g.\ the column-sparse edge-vertex incidence matrix of a graph, $L$ is naturally small. Standard non-Euclidean variants of gradient descent solve a smoothed variant of \eqref{eq:boxsimplex} using $\tO(L^2 \eps^{-2})$ matrix-vector products with $\ma$ and $\ma^\top$ \cite{Nesterov05, KelnerLOS14}. Until recently however, the lack of accelerated algorithms for \eqref{eq:boxsimplex} (using a quadratically smaller $\tO(L\eps^{-1})$ matrix-vector products) was a notorious barrier in optimization theory. This barrier arises due to the provable lack of a strongly convex regularizer in the $\ell_\infty$ geometry with dimension-independent additive range \cite{SidfordT18},\footnote{For a function $f$, we call $\max f - \min f$ its additive range.} a necessary component of standard non-Euclidean acceleration schemes.

Sherman's breakthrough work \cite{Sherman17} overcame this obstacle and gave an algorithm for solving \eqref{eq:boxsimplex} using roughly $O(L\log (d) \log(L\eps^{-1}) \cdot \eps^{-1})$ matrix-vector products.\footnote{The runtime for solving subproblems required by \cite{Sherman17} was not explicitly bounded in that work, but a simple initialization strategy bounds the runtime by $O(\log (L\eps^{-1}))$, see Proposition 1 of \cite{AssadiJJST22}.} Roughly speaking, Sherman exploited the primal-dual nature of the problem \eqref{eq:boxsimplex} to design a smaller regularizer over the $\ell_\infty$ ball $[-1, 1]^n$, which satisfied a weaker condition known as \emph{area convexity} (rather than strong convexity). The algorithm in \cite{Sherman17} combined an extragradient method ``outer loop'' requiring $O(L\log d \cdot \eps^{-1})$ iterations, and an alternating minimization subroutine ``inner loop'' solving the subproblems required by the outer loop in $O(\log(L\eps^{-1}))$ steps. However, Sherman's analysis in \cite{Sherman17} was quite ad hoc, and its relationship to more standard optimization techniques has remained mysterious. These mysteries have made further progress on \eqref{eq:boxsimplex} and related problems challenging in several regards.

\begin{enumerate}
    \item Sherman's algorithm pays a logarithmic overhead in the complexity of its inner loop to solve subproblems to high accuracy. Does it tolerate more approximate subproblem solutions?\label{item:nolog}
    \item The analysis of Sherman's alternating minimization scheme relies on multiplicative stability properties of the dual variable $y$. In generalizations of \eqref{eq:boxsimplex} where the dual variable is unstable, how do we design solvers handling instability to maintain fast subproblem solutions?\label{item:unstable}
    \item The relationship between the area convexity definition \cite{Sherman17} and more standard Bregman divergence domination conditions used in classical analyses of extragradient methods \cite{Nemirovski04, Nesterov07} is unclear. Can we unify these extragradient convergence analyses?\label{item:relate}
\end{enumerate}

A central goal of this work is to answer Question~\ref{item:relate} by revisiting the area convexity technique of \cite{Sherman17}, and developing a deeper understanding of its relationship to more standard tools in optimization theory such as relative smoothness \cite{BauschkeBT17, LuFN18} and the classical extragradient methods of \cite{Nemirovski04, Nesterov07}. For convenience to the reader, we give self-contained expositions of these relationships in Appendices~\ref{app:rc_subproblem_solve} and~\ref{app:xgrad_convergence}. As byproducts, our improved understanding of area convexity results in new state-of-the-art runtimes for \eqref{eq:boxsimplex} by affirmatively answering Question~\ref{item:nolog}, and the first accelerated solver for a matrix generalization by affirmatively answering Question~\ref{item:unstable}.

\subsection{Our results}\label{ssec:results}

We begin by stating our new runtime results in this section, deferring a more extended discussion of how we achieve them via improved analyses of the \cite{Sherman17} framework to the following Section~\ref{ssec:approach}.

\paragraph{Box-simplex games.} We give an algorithm for solving \eqref{eq:boxsimplex} improving the runtime of \cite{Sherman17} by a logarithmic factor, by removing the need for high-accuracy inner loop solutions.

\begin{restatable}{theorem}{restateboxsimplex}\label{thm:boxsimplex}
	Algorithm~\ref{alg:boxsimplex} deterministically computes an $\eps$-approximate saddle point to \eqref{eq:boxsimplex} in time\footnote{This statement assumes direct access to the entries of $\ma$. More generally, letting $\tmv$ be the time it takes to multiply a vector by the argument, we may replace $\nnz(\ma)$ by $\max(\tmv(\ma), \tmv(\ma^\top), \tmv(|\ma|), \tmv(|\ma^\top|))$ where $|\cdot|$ is entrywise, which may be a significant savings when $\ma$ is implicitly accessible and nonnegative. An $\eps$-approximate saddle point to a minimax problem is a point with duality gap $\eps$; see Section~\ref{sec:prelims}.}
	\[O\Par{\nnz(\ma) \cdot \frac{\norm{\ma}_{1 \to 1} \log d}{\eps}}.\]
\end{restatable}

We define $\nnz$ and $\|\ma\|_{1 \to 1}$ in Section~\ref{ssec:notation}. The most direct comparison to Theorem~\ref{thm:boxsimplex} is the work \cite{Sherman17}, which we improve. All other first-order methods for solving \eqref{eq:boxsimplex} we are aware of are slower than \cite{Sherman17} by at least an $\approx \min(\eps^{-1}, \sqrt n)$ factor. Higher-order methods for \eqref{eq:boxsimplex} (e.g.\ interior point methods) obtain improved dependences on $\eps$ at the cost of polynomial overhead in the dimension.

As a byproduct of Theorem~\ref{thm:boxsimplex}, we improve all the recent applications of \eqref{eq:boxsimplex}, including approximate maximum flow \cite{Sherman17}, optimal transport \cite{JambulapatiST19}, min-mean-cycle \cite{AltschulerP20}, and semi-streaming variants of bipartite matching and transshipment \cite{AssadiJJST22}. These are summarized in Section~\ref{sec:apps}. The design of efficient approximate solvers for  optimal transport in particular is a problem which has received a substantial amount of attention from the learning theory community (see Table~\ref{table:ot} and the survey \cite{PeyreC19}). The runtime of Theorem~\ref{thm:boxsimplex} represents a natural conclusion to this line of work.\footnote{Progress on the historically simpler problem of (simplex-simplex) matrix games, a relative of \eqref{eq:boxsimplex} where both players are $\ell_1$ constrained, has stalled since \cite{Nemirovski04} almost 20 years ago, which attains a deterministic runtime of $O(\nnz(\ma) \cdot L\log d \cdot \eps^{-1})$ where $L$ is the appropriate Lipschitz constant, analogously to Theorem~\ref{thm:boxsimplex}.} We believe our improvement in Theorem~\ref{thm:boxsimplex} (avoiding a high-accuracy subproblem solve) takes an important step towards bridging the state-of-the-art in the theory and practice of optimal transport, a well-motivated undertaking due to its widespread use in applications. 

\begin{table}[ht!]
	\centering
	\renewcommand{\arraystretch}{1.25}
	\everymath{\displaystyle}
	\caption{\textbf{Runtime complexities of first-order algorithms for optimal transport.} Stated for a $d \times d$ cost matrix with unit-bounded costs, additive error tolerance $\eps$. Results labeled ``sequential'' require a parallel depth which scales polynomially in the problem dimension $d$.}\label{table:ot}
	\begin{tabular}{c c c}	
		\toprule
		{ Method } & {Runtime} & {Comments} \\
		\midrule
		%
\cite{AltschulerWR17} & 
		$O(d^2 \log d \cdot \eps^{-3})$ &   
		\\
\cite{DvurechenskyGK18} & $O(d^2 \log d \cdot \eps^{-2})$ & 
  \\
\cite{DvurechenskyGK18, LinHJ19} & $O(d^{2.5}\sqrt{\log d} \cdot \eps^{-1})$ & 
  \\
\cite{BlanchetJKS18, Quanrud20} & $O(d^2 \log(d)\log(\eps^{-1}) \cdot \eps^{-1})$ & sequential, randomized
  \\
\cite{LahnMR19} & $O(d^2 \cdot \eps^{-1} + d \cdot \eps^{-2})$ & sequential
  \\
  \cite{JambulapatiST19} & $O(d^2 \log(d)\log(\eps^{-1}) \cdot \eps^{-1})$ & \\
  \cite{LinHJ22} & $O(d^{7/3}\sqrt[3]{\log d} \cdot \eps^{-4/3})$ & \\
		\midrule
		Theorem~\ref{thm:boxsimplex}   & $O(d^2 \log d \cdot \eps^{-1})$ &  \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Box-spectraplex games.} We initiate the algorithmic study of box-spectraplex games of the form
\begin{equation}\label{eq:boxspectraplex}
\min_{x \in [-1, 1]^n} \max_{\my \in \Delta^{d \times d}} \inprod{\my}{\sum_{i \in [n]} x_i \ma_i} - \inprod{\mb}{\my} + c^\top x,
\end{equation}
where $\{\ma_i\}_{i \in [n]}$, $\mb$ are $d \times d$ symmetric matrices and $\my \succeq \mzero$ satisfies $\Tr(\my) = 1$ (i.e.\ $\my \in \Delta^{d \times d}$, cf.\ Section~\ref{sec:prelims}). This problem family is a natural formulation of semidefinite programming (SDP), and generalizes \eqref{eq:boxsimplex}, the special case of \eqref{eq:boxspectraplex} where all of the matrices $\{\ma_i\}_{i \in [n]}$, $\mb$, $\my$ are diagonal. 

Our main result on solving \eqref{eq:boxspectraplex}, stated below as Theorem~\ref{thm:boxspectraplexintro}, has already found use in \cite{JambulapatiRT23} to obtain faster solvers for spectral sparsification and related discrepancy-theoretic primitives, where existing approximate SDP solvers do not apply. Beyond the fundamental nature of the problem solved by Theorem~\ref{thm:boxspectraplexintro}, we are optimistic that it will find use in other applications of structured SDPs.

To motivate our investigation of \eqref{eq:boxspectraplex}, we first describe some history of the study of the simpler problem \eqref{eq:boxsimplex}. Many applications captured by \eqref{eq:boxsimplex} are structured settings where optimization is at a \emph{relative scale}, i.e.\ we wish to obtain an $(1 + \eps)$-multiplicative approximation to the value of a LP, $\opt \defeq \min_{\ma x \le b} c^\top x$. We use bipartite matching as an example: $x$ is a fractional matching and $\ma$ is a nonnegative edge-vertex incidence matrix. For LPs with a nonnegative constraint matrix $\ma$, custom positive LP solvers achieve $(1 + \eps)$-multiplicative approximations at accelerated rates scaling as $\tO(\eps^{-1})$. This result then implies a $\tO(\eps^{-1})$-rate algorithm for approximating bipartite matching and its generalization, optimal transport (where the first such algorithm used positive LP solvers \cite{BlanchetJKS18, Quanrud20}). However, existing accelerated positive LP solvers \cite{ZhuO15} are both sequential and randomized, and whether this is necessary has persisted as a challenging open question.

In several applications with nonnegative constraint matrices \cite{JambulapatiST19, AltschulerP20, AssadiJJST22}, this obstacle was circumvented by recasting the problem as a box-simplex game \eqref{eq:boxsimplex}, for which faster, deterministic, and efficiently-parallelizable solvers exist (see e.g.\ Section 4.1 of \cite{AssadiJJST22} for the bipartite matching reduction).\footnote{Two notable additional problems captured by box-simplex games, maximum flow \cite{Sherman17} and transshipment \cite{AssadiJJST22}, do not have a nonnegative constraint matrix (as flows are signed), so positive LP solvers do not apply.} In these cases, careful use of box-simplex game solvers (and binary searching for the problem scale) match the guarantees of positive LP solvers, with improved parallelism or determinism. Notably, simpler primitives such as simplex-simplex game solvers do not apply in these settings.

The current state-of-affairs in applications of fast SDP solvers is very similar. For various problems in robust statistics (where the goal is to approximate a covariance matrix or detect a corruption spectrally) \cite{ChengDG19, CherapanamjeriM20, Jambulapati0T20} and numerical linear algebra (where the goal is to reweight or sparsify a matrix sum) \cite{Lee017, ChengG18, JambulapatiLMST21}, positive SDP solvers have found utility. However, these uses appear quite brittle: current positive SDP solvers \cite{Allen-ZhuLO16, PengTZ16, Jambulapati0T20} only handle the special case of packing SDPs (a special class of positive SDP with one-sided constraints), preventing their application in more challenging settings (including pure covering SDPs). 

In an effort to bypass this potential obstacle when relying on positive SDP solvers more broadly, we therefore develop a nearly-linear time solver for box-spectraplex games in Theorem~\ref{thm:boxspectraplexintro} (we define $\tmv$, the time required to perform a matrix-vector product, in Section~\ref{ssec:notation}). 

\begin{restatable}{theorem}{restateboxspectraplex}\label{thm:boxspectraplexintro}
There is an algorithm which computes an $\eps$-approximate saddle point to \eqref{eq:boxspectraplex} in time
\[O\Par{\Par{\tmv(\mb) + \sum_{i \in [n]}\Par{\tmv(\ma_i) + \tmv(|\ma_i|)}}\cdot \frac{L^{3.5}\log^{3}\Par{\frac{Lnd}{\delta\eps}}}{\eps^{3.5}}},\]
with probability $\ge 1 - \delta$, for $L \defeq \|\sum_{i \in [n]} |\ma_i|\|_{\textup{op}} + \normop{\mb}$. A deterministic variant uses time
\[O\Par{\Par{\Par{\tmv(\mb) + \sum_{i \in [n]}\Par{\tmv(\ma_i) + \tmv(|\ma_i|)}} \cdot d + d^\omega} \cdot \frac{L\log\Par{\frac{Lnd}{\delta\eps}}}{\eps}}.\]
\end{restatable}

Theorem~\ref{thm:boxspectraplexintro} follows as a special case of a general result we give as Theorem~\ref{thm:boxspectraplex}, with more refined guarantees stated in terms of the cost to perform certain queries of matrix exponentials required by our algorithm, typical of first-order methods over $\Delta^{d \times d}$. The first runtime in Theorem~\ref{thm:boxspectraplexintro} implements these queries using randomized sketching tools, and the second uses an exact implementation.

To put Theorem~\ref{thm:boxspectraplexintro} in context, we compare it to existing solvers for simplex-spectraplex games, a relative of \eqref{eq:boxspectraplex} where both players are $\ell_1$ constrained. Simplex-spectraplex games are more well-studied \cite{KalaiV05, WarmuthK06, AroraK07, BaesBN13, Allen-ZhuL17, CarmonDST19}, and the state-of-the-art algorithms \cite{BaesBN13, Allen-ZhuL17, CarmonDST19} query $\tO(L^{2.5}\eps^{-2.5})$ vector products in $\{\ma_i\}_{i \in [n]}$ and $\mb$. A slight variant of these algorithms (using a separable regularizer in place of area convex techniques) gives a query complexity of $\tO(n \cdot L^{2.5}\eps^{-2.5})$ such queries for the more challenging box-spectraplex games \eqref{eq:boxspectraplex}, which is the baseline in the literature. 
In comparison, Theorem~\ref{thm:boxspectraplexintro} requires $\tO(L^{3.5}\eps^{-3.5})$ products, but assumes access to $\{|\ma_i|\}_{i \in [n]}$. This assumption is not without loss of generality, as $|\ma|$ can be dense even when $\ma$ is sparse, though in important robust statistics or spectral sparsification applications (where $\{\ma_i\}_{i \in [n]}$ are rank-one), it is not restrictive. We believe it is interesting to understand whether access to $\{|\ma_i|\}_{i \in [n]}$ and the $\tO(L\eps^{-1})$ overhead incurred by Theorem~\ref{thm:boxspectraplexintro} (due to the higher ranks in the randomized sketches we use) can be avoided, and discuss the latter in Section~\ref{ssec:approach}. 

\subsection{Our approach}\label{ssec:approach}

We begin our overview of our techniques with a short overview of area convexity from the perspective of both the extragradient method ``outer loop'' and the alternating minimization ``inner loop'' used by the \cite{Sherman17} algorithm. We then summarize the new perspectives on both loops developed in this paper, and describe how these perspectives allow us to obtain our main results, Theorems~\ref{thm:boxsimplex} and~\ref{thm:boxspectraplexintro}.

\paragraph{Area convexity.} Extragradient methods \cite{Nemirovski04, Nesterov07} are a powerful general-purpose tool for solving convex-concave minimax optimization problems with a $L$-Lipschitz gradient operator $g$. To obtain an $\eps$-approximate saddle point, extragradient methods take descent-ascent steps regularized by a function $r$, which is $1$-strongly convex in the same norm $g$ is Lipschitz in; their convergence rates then scale as $L\Theta \cdot \eps^{-1}$, where $\Theta$ is the additive range of $r$. For $\ell_1$-$\ell_1$ or $\ell_2$-$\ell_1$ games, regularizers with $\Theta = \tO(1)$ exist, and hence the results of \cite{Nemirovski04, Nesterov07} immediately imply accelerated $\tO(L\eps^{-1})$ rates for approximating equilibria. For $\ell_\infty$-$\ell_1$ games however, a fundamental difficulty is that under the strong convexity requirement over $\xset \defeq [-1, 1]^n$, $\Theta$ is necessarily $\Omega(n)$ \cite{SidfordT18}.

To bypass this issue, Sherman introduced a regularizer capturing the ``local geometry'' of \eqref{eq:boxsimplex}.\footnote{Sherman actually studied a more general problem setting than \eqref{eq:boxsimplex}, where the simplex is replaced by an $\ell_1$-constrained product set. For most applications of area convexity, it suffices to use simplex dual variables, and we use a different, simpler regularizer introduced by a follow-up work specifically for box-simplex games \cite{JambulapatiST19}.} A (slight variant) of the \cite{Sherman17} regularizer has the form, for some parameter $\alpha > 0$,
\begin{equation}\label{eq:reg_intro}
r(x, y) \defeq \inprod{|\ma|y}{x^2} + \alpha h(y),\text{ where } h(y) \defeq \sum_{j \in [d]} y_j \log y_j \text{ is negative entropy.}
\end{equation}
Here, $|\ma|$ and $x^2$ are applied entrywise. Sherman shows \eqref{eq:reg_intro} satisfies a weaker condition known as \emph{area convexity} (Definition~\ref{def:areaconvex}), and modifies standard extragradient methods to converge under this condition. However, the area convexity definition is fairly unusual from the perspective of standard analyses, which typically bound regret notions via Bregman divergences in the regularizer used.

Intuitively, \eqref{eq:reg_intro} obtains a smaller range by placing weights $|\ma|y$ on the quadratic $x^2$. Moreover, the reweighted quadratic $\inprod{|\ma|y}{x^2}$ still has enough strong convexity in multiplicative neighborhoods of a dual variable $y$ for extragradient methods to converge. Sherman's proof that \eqref{eq:reg_intro} satisfies area convexity also implies $r$ is jointly convex for sufficiently large $\alpha$, which means that by increasing $\alpha$, we can ensure $\nabla^2 r$ is dominated by a multiple of the entropy component $\nabla^2 h$. Sherman used this Hessian domination condition to efficiently minimize subproblems of the form $f_v(z) \defeq \inprod{v}{z} + r(z)$, for $z \in [-1, 1]^n \times \Delta^d$, required by the outer loop. These subproblems $f_v$, which are non-separable, do not admit closed form solutions, but \cite{Sherman17} showed they are well-conditioned enough (due to entropy domination) for alternating minimization to converge rapidly.

\paragraph{Improved subproblem solvers.} Our first observation is that the Hessian domination condition used by \cite{Sherman17} to ensure rapid convergence of subproblems can be viewed through the lens of \emph{relative smoothness} \cite{BauschkeBT17, LuFN18}, a more standard condition in optimization theory. Simple facts from convex analysis (Lemma~\ref{lem:relativesmooth}) imply that after minimizing over the box, the induced function $\tf_v(y) \defeq \min_{x \in [-1, 1]^n} f_v(x, y)$ is relatively smooth in $h$. By tuning $\alpha$ in \eqref{eq:reg_intro}, we can further ensure $\tf_v$ is relatively strongly convex in $h$, and use methods for relatively well-conditioned optimization from \cite{LuFN18} off-the-shelf to minimize $\tf_v$ (and hence also $f_v$). We give a self-contained presentation of this strategy in Appendix~\ref{app:rc_subproblem_solve}, yielding a simplified solver for the subproblems in \cite{Sherman17}.

While this is a seemingly a small difference in proof strategy, the convergence analysis in \cite{Sherman17} strongly used that $\nabla^2 f_v = \nabla^2 r$ is multiplicatively stable in small neighborhoods of a dual variable $y$. Our new subproblem solvers, which are based on relative smoothness, avoid this requirement and hence apply to the setting of Theorem~\ref{thm:boxspectraplexintro}, where matrix variables on $\Delta^{d \times d}$ do not satisfy the required multiplicative stability. Specifically, $\exp(\log \my + \mg)$ for operator-norm bounded $\mg$ is not necessarily multiplicatively well-approximated by $\my$, which is required by a natural generalization of the \cite{Sherman17} subproblem analysis (the corresponding statement for vectors is true). On the other hand, our relative smoothness analysis avoids the need for dual variable local stability altogether.

\paragraph{Improved extragradient analyses.} Our next observation is that Sherman's area convexity condition (Definition~\ref{def:areaconvex}) actually implies a natural Bregman divergence domination condition \eqref{eq:magicareaconvex}, which is closely related to more standard analyses of extragradient methods. We discuss this relationship in more detail in Appendix~\ref{app:xgrad_convergence}, where we unify \cite{Sherman17} with \cite{Nemirovski04, Nesterov07, CohenST21} and propose a weaker condition than used in either existing analysis which suffices for convergence.

The simple realization that area convexity implies a bound based on Bregman divergences, which to our knowledge is novel, allows us to reinterpret the \cite{Sherman17} outer loop to tolerate larger amounts of inexactness in the solutions for the subproblems $\min_{z \in [-1, 1]^n} f_v(z)$ defined earlier, where inexactness is measured in Bregman divergence. We show that by combining a tolerant variant of the \cite{Sherman17} outer loop with our relative smoothness analysis, a constant number of alternating steps solves subproblems in $f_v$ to sufficient accuracy. More specifically, we maintain an auxiliary sequence of dual variables 
to warm-start our subproblems (see $\by$ in Definition~\ref{def:Oxgrad}), and show that a telescoping entropy divergence bound on this auxiliary sequence pays for the inexactness of subproblem solves. Combining these pieces yields our faster box-simplex game solver, Theorem~\ref{thm:boxsimplex}. Interestingly, its proof builds upon our insights regarding area convexity in both Appendices~\ref{app:rc_subproblem_solve} and~\ref{app:xgrad_convergence}.


\paragraph{Spectrahedral generalizations.} To generalize these methods to solve box-spectraplex games, we introduce a natural matrix variant of the regularizer \eqref{eq:reg_intro} in \eqref{eq:rsdpdef}. Proving it satisfies area convexity (or even joint convexity) introduces new challenges, as the corresponding proofs for \eqref{eq:reg_intro} exploit coordinatewise-decomposability \cite{Sherman17, JambulapatiST19}, whereas matrix regularizers induce incompatible eigenspaces. We use tools from representation theory to avoid this issue and prove convexity of \eqref{eq:rsdpdef}.

Finally, a major computational obstacle in minimax problems over $\my \defeq \Delta^{d \times d}$ is that the standard regularizer over this set, von Neumann entropy $H(\my) \defeq \inprod{\my}{\log \my}$, induces iterates as matrix exponentials; explicitly computing even a single iterate typically takes superlinear time. In the simpler setting of simplex-spectraplex games, these computations can be implicitly performed using polynomial approximations to the exponential and randomized sketches \cite{AroraK07}. The tolerance required by analogous computations in the box-spectraplex setting is more fine-grained, especially when computing the best responses $\argmin_{x \in [-1, 1]^n} f_v(x, \my)$ required by our subproblem solvers, which can be quite unstable. By carefully using multiplicative-additive approximations to the matrix exponential (Definition~\ref{def:matexporacle}), we show how to efficiently implement all the operations needed using Johnson-Lindenstrauss sketches of rank $\tO(L^2\eps^{-2})$. An interesting open problem suggested by our work is whether our methods tolerate lower-rank sketches of rank $\tO(L\eps^{-1})$, which would improve Theorem~\ref{thm:boxspectraplexintro} by this factor. These lower-rank sketches suffice for simplex-spectraplex games \cite{BaesBN13}, though we suspect corresponding analyses for box-spectraplex games will also need to use more fine-grained notions of error tolerance, which we defer to future work.

\subsection{Related work}\label{ssec:related}


The family of box-simplex games \eqref{eq:boxsimplex} is captured by linear programming \cite{LeeS15}, where state-of-the-art solvers \cite{cohen2021solving, BrandLLSSSW21} run in time $\tO((n + d)^{\omega})$ or $\tO(nd + \min(n, d)^{2.5})$, where $\omega \approx 2.37$ is the current matrix multiplication constant \cite{AlmanW21, DuanWZ22}. These LP solvers run in superlinear time, and practical implementations do not currently exist; on the other hand, the convergence rates depend polylogarithmically on the accuracy parameter $L\eps^{-1}$. The state-of-the-art approximate solver for \eqref{eq:boxsimplex} (with runtime depending linearly on the input size $\nnz(\ma)$ and polynomially on $L\eps^{-1}$) is the accelerated algorithm of \cite{Sherman17}, which is improved by our Theorem~\ref{thm:boxsimplex} by a logarithmic factor. Finally, we mention \cite{BoobSW19} as another recent work which relies on area convexity techniques to design an algorithm in a different problem setting; their inner loop also requires high-precision solves (as in \cite{Sherman17}), and this similarly results in a logarithmic overhead in the runtime.

For the specific application of optimal transport, an optimization problem captured by \eqref{eq:boxsimplex} receiving significant recent attention from the learning theory community, several alternative specialized solvers have been developed beyond those in Table~\ref{table:ot}. By exploiting relationships between Sinkhorn regularization and faster solvers for matrix scaling \cite{CohenMTV17, Allen-ZhuLOW17}, \cite{BlanchetJKS18} gave an alternative solver obtaining a $\tO(n^2 \cdot \eps^{-1})$ rate. These matrix scaling algorithms call specialized graph Laplacian system solvers as a subroutine, which are currently less practical than our methods based on matrix-vector queries. Finally, the recent breakthrough maximum flow algorithm of \cite{ChenKLPGS22} also extends to solve optimal transport in $O(n^{2 + o(1)})$ time (though its practicality at this point remains unclear). For moderate $\eps \ge n^{-o(1)}$, this is slower than Theorem~\ref{thm:boxsimplex}.

To our knowledge, there have been no solvers developed in the literature which tailor specifically to the family of box-spectraplex games \eqref{eq:boxspectraplex}. These problems are solved by general-purpose SDP solvers, where the state-of-the-art runtimes of $\tO(n^\omega \sqrt{d} + nd^{2.5})$ or $\tO(n^\omega + d^{4.5} + n^2\sqrt{d})$ \cite{JiangKLP020, HuangJ0T022} are highly superlinear (though again, they depend polylogarithmically on the inverse accuracy). We believe our techniques in Section~\ref{sec:sdpfacts} extend straightforwardly to show that a variant of gradient descent in the $\ell_\infty$ geometry \cite{KelnerLOS14} solves \eqref{eq:boxspectraplex} in an unaccelerated $\tO(L^2\eps^{-2})$ iterations, with the same per-iteration complexity as Theorem~\ref{thm:boxspectraplexintro}. As discussed in Section~\ref{ssec:approach}, an interesting open direction is to improve the per-iteration complexity of Theorem~\ref{thm:boxspectraplexintro} using sketches of lower rank $\tO(L\eps^{-1})$, paralleling developments for simplex-spectraplex games \cite{BaesBN13, Allen-ZhuL17, CarmonDST19}. %
\section{Preliminaries}
\label{sec:prelims}

\subsection{Notation}\label{ssec:notation}

\paragraph{General notation.} We use $\tO$ to suppress polylogarithmic factors in problem parameters for brevity. Throughout $[n] := \{i \in \N \mid 1 \le i \le n\}$. When applied to a vector $\norm{\cdot}_p$ is the $\ell_p$ norm. We refer to the dual space (bounded linear operators) of a set $\xset$ by $\xset^*$. The all-zeroes and all-ones vectors in dimension $d$ are denoted $\0_d$ and $\1_d$. When $u, v$ are vectors of equal dimension, we let $u \circ v$ denote their coordinatewise multiplication. Matrices are denoted in boldface. The symmetric $d \times d$ matrices are $\Sym^d$, equipped with the Loewner partial ordering $\preceq$ and $\inprod{\ma}{\mb} = \Tr(\ma\mb)$. The $d \times d$ positive semidefinite cone is $\PSD^d$, and the set of $d \times d$ positive definite matrices is $\PD^d$.  For $\my \in \Sym^d$ with eigendecomposition $\my = \mmu^\top \mlam \mmu$ for $\mlam = \diag{\lam}$ we define $\exp(\my) := \mmu^\top \diag{\exp(\lam)} \mmu$ (where $\exp(\lam)$ is entrywise) and similarly define $|\my|$ and $\log \my$ (when $\my \succ \mzero$). The operator norm of $\my \in \Sym^d$ is denoted $\normop{\my}$ and is the largest eigenvalue of $|\my|$; the trace norm $\normtr{\my}$ is $\Tr|\my|$. The all-zeroes and identity matrices of dimension $d$ are $\mzero_d$ and $\id_d$. For $\ma \in \R^{n \times d}$ we denote its $i^{\text{th}}$ row (for $i \in [n]$) by $\ai$ and $j^{\text{th}}$ column (for $j \in [d]$) by $\aj$. For $p, q \ge 1$ we define $\norm{\ma}_{p \to q} \defeq \sup_{\norm{v}_p = 1} \norm{\ma v}_q$. The number of nonzero entries in $\ma$ is denoted $\nnz(\ma)$, and $\tmv(\ma)$ is the time it takes to multiply a vector by $\ma$. We use $\med(x, -1, 1)$ to mean the projection of $x$ onto $[-1, 1]$, where $\med$ means median. We define the $d$-dimensional simplex and the $d \times d$ spectraplex
\[\Delta^d \defeq \{y \in \R_{\ge 0}^d \mid \norm{y}_1 = 1\}\text{ and } \Delta^{d \times d} \defeq \{\my \in \PSD^{d \times d} \mid \Tr \my = 1\}.\]
Throughout we reserve the function names $h(y) := \sum_{j \in [d]} y_j \log y_j$ (for $y \in \Delta^d$) and $H(\my) := \inprod{\my}{\log \my}$ (for $\my \in \Delta^{d \times d}$) for negated (vector) entropy and (matrix) von Neumann entropy. 

\paragraph{Optimization.} For convex function $f$, $\partial f(x)$ refers to the subgradient set at $x$; we sometimes use $\partial f$ to denote any (consistently chosen) member of the subgradient set. Following \cite{LuFN18}, we say $f$ is $L$-relatively smooth with respect to convex function $r$ if $Lr - f$ is convex, and we say $f$ is $m$-strongly convex with respect to $r$ if $f - mr$ is convex. For a differentiable convex function $f$ we define the associated (nonnegative, convex) Bregman divergence $V^f_{x}(x') \defeq f(x') - f(x) - \inprod{\nabla f(x)}{x' - x}$. The Bregman divergence satisfies the well-known identity
\begin{equation}\label{eq:threepoint}\inprods{\nabla V^f_x(x')}{u - x'} = \inprod{\nabla f(x') - \nabla f(x)}{u - x'} = V^f_x(u) - V^f_{x'}(u) - V^f_x(x').\end{equation}
For convex function $f$ on two variables $(x, y)$, we use $\partial_y f(x, y)$ to denote the subgradient set at $y$ of the restricted function $f(x, \cdot)$. We call a function $f$ of two variables $(x, y) \in \xset \times \yset$ convex-concave if its restrictions to the first and second block are respectively convex and concave. We call $(x, y) \in \xset \times \yset$ an $\eps$-approximate saddle point if its \emph{duality gap}, $\max_{y' \in \yset} f(x, y') - \min_{x' \in \xset} f(x', y)$, is at most $\eps$. For differentiable convex-concave function $f$ clear from context, its subgradient operator is $g(x, y) := (\partial_x f(x, y), -\partial_y f(x, y))$; when $f$ is differentiable, we will call this a gradient operator. We say operator $g$ is monotone if for all $w, z$ in its domain, $\inprod{g(w) - g(z)}{w - z} \ge 0$: examples are the subgradient of a convex function or subgradient operator of a convex-concave function. 


\subsection{Box-simplex games}\label{ssec:boxsimplex}

In Section~\ref{sec:noaltmin}, we develop a solver for \emph{box-simplex} games of the form
\[
\min_{x \in [-1, 1]^n} \max_{y \in \Delta^d} x^\top \ma y - b^\top y + c^\top x,
\]
for some $\ma \in \R^{n \times d}$, $b \in \R^d$, and $c \in \R^n$. We will design methods computing approximate saddle points to \eqref{eq:boxsimplex} which leverage the following family of regularizers (recalling the definition of $h$ from Section~\ref{ssec:notation}):
\begin{equation}\label{eq:rlpdef}
\rlp{\alpha}(x, y) \defeq \inprod{|\ma| y}{x^2} + \alpha h(y),
\end{equation}
where the absolute value is applied to $\ma$ entrywise, and squaring is applied to $x$ entrywise. In the context of Section~\ref{sec:noaltmin} only, $|\ma|$ will be applied entrywise (rather than to the spectrum). This family of regularizers was introduced by \cite{JambulapatiST19}, a minor modification to a similar family given by \cite{Sherman17}, and has multiple favorable properties for the geometry present in the problem \eqref{eq:boxsimplex}. In this context, we will use $\xset \defeq [-1, 1]^n$, and for any $v \in \R^n$ we let $\projx(v) \defeq \med(v, -1, 1)$ be truncation applied entrywise. We also use $\yset \defeq \Delta^d$ and for any $v \in \R^d_{\ge 0}$ we let $\projy(v) \defeq \frac{v}{\norm{v}_1}$ normalize onto $\yset$.

\subsection{Box-spectraplex games}\label{ssec:boxspec}

In Section~\ref{sec:sdpfacts}, %
we develop a solver for \emph{box-spectraplex} games, which are bilinear minimax problems defined with respect to a set of matrices $\alla \defeq \{\ma_i\}_{i \in [n]} \subset \Sym^{d}$. We define $|\alla| \defeq \{|\ma_i|\}_{i \in [n]}$ where the absolute value is applied spectrally. We also denote for $x \in \R^n$ and $\my \in \R^{d \times d}$,
\[\alla(x) \defeq \sum_{i \in [n]} x_i \ma_i,\; \alla^*(\my) \defeq \Brace{\inprod{\ma_i}{\my}}_{i \in [n]}.\]
The box-spectraplex games we study are of the form
\[
\min_{x \in [-1, 1]^n} \max_{\my \in \Delta^{d \times d}} \inprod{\my}{\alla(x)} - \inprod{\mb}{\my} + c^\top x,
\]
for some $\alla \subset \Sym^{d}$, $\mb \in \Sym^d$, and $c \in \R^n$. The Lipschitz constant of problem \eqref{eq:boxspectraplex} is denoted
\begin{equation}\label{eq:lip_sdp_def}L_{\alla} \defeq \normop{\sum_{i \in [n]} |\ma_i|}.\end{equation}
Analogously to the box-simplex setting, we use the following two-parameter family of regularizers (recalling the definition of $H$ from Section~\ref{ssec:notation}):
\begin{equation}\label{eq:rsdpdef}
\rsdp{\alpha,\mu}(x, \my) \defeq \inprod{|\alla|^*(\my)}{x^2} + \alpha H(\my) + \frac {\mu} 2 \norm{x}_2^2.
\end{equation}
When $\mu = 0$, we denote \eqref{eq:rsdpdef} by $\rsdp{\alpha}$. We define $\xset$ and $\projx$ similarly to Section~\ref{ssec:boxsimplex} in the context of Section~\ref{sec:sdpfacts}, where we also overload $\yset \defeq \Delta^{d \times d}$ and for any $\mv \in \PSD^d$ we let $\projy(\mv) \defeq \frac{\mv}{\Tr\mv}$.

\subsection{Extragradient methods}\label{ssec:convex}

In Appendix~\ref{app:xgrad_convergence}, we analyze extragradient algorithms for approximately solving variational inequalities in an operator $g$, i.e.\ which find $z$ with small $\inprod{g(z)}{z - u}$ for all $u$ in the domain. We analyze convergence under the following condition, which is weaker than previous notions in \cite{Sherman17, CohenST21}.

\begin{definition}[Relaxed relative Lipschitzness]
We say an operator $g: \zset \to \zset^*$ is $\frac 1 \eta$-\emph{relaxed relatively Lipschitz} with respect to $r: \zset \to \R$ if for all $(z, z', z^+) \in \zset \times \zset \times \zset$,
\[\eta \inprod{g(z') - g(z)}{z' - z^+} \le V^r_z(z') + V^r_{z'}(z^+) + V^r_z(z^+).\]
\label{def:rrl}
\end{definition}

This notion is related to and subsumes the notions of relative Lipschitzness \cite{CohenST21} and area convexity \cite{Sherman17} which have recently been proposed to analyze extragradient methods, which we define below. In particular, Definitions~\ref{def:rl} and~\ref{def:areaconvex} were motivated by designing solvers for \eqref{eq:boxsimplex}.

\begin{definition}[Relative Lipschitzness \cite{CohenST21}]\label{def:rl}
We say an operator $g: \zset \to \zset^*$ is $\frac 1 \eta$-\emph{relatively Lipschitz} with respect to $r: \zset \to \R$ if for all $(z, z', z^+) \in \zset \times \zset \times \zset$,
\[\eta \inprod{g(z') - g(z)}{z' - z^+} \le V^r_z(z') + V^r_{z'}(z^+).\]
\end{definition}

\begin{definition}[Area convexity \cite{Sherman17}]\label{def:areaconvex}
We say convex $r: \zset \to \R$ is $\eta$-\emph{area convex} with respect to an operator $g: \zset \to \zset^*$ if for all $(z, z', z^+) \in \zset \times \zset \times \zset$, defining $c \defeq \frac 1 3(z + z' + z^+)$,
\[\eta \inprod{g(z') - g(z)}{z' - z^+} \le r(z) + r(z') + r(z^+) - 3r(c).\]
Area convexity is monotone in $\eta$ as the right-hand side is nonnegative for any $z, z', z^+$. 
\end{definition}

As shown in \Cref{app:xgrad_convergence}, relaxed relative Lipschitzness simultaneously generalizes relative Lipschitzness and area convexity. The relationship is obvious for Definition~\ref{def:rl}, but the generalization of Definition~\ref{def:areaconvex} relies on the following simple observation, which has not previously appeared explicitly:
\begin{equation}\label{eq:magicareaconvex}r(z) + r(z') + r(z^+) - 3r(c) = V^r_z(z^+) + V^r_z(z') - 3V^r_z(c) \le V^r_z(z^+) + V^r_z(z'). \end{equation}

In Appendix~\ref{app:xgrad_convergence} we give an analysis framework unifying previous analyses of \cite{Nemirovski04, Nesterov07, Sherman17, CohenST21}, possibly of further utility. Our algorithms for box-simplex and box-spectrahedra games employ variants of our relaxed relative Lipschitzness solver in Appendix~\ref{app:xgrad_convergence}, extending it to robustly handle inexactness from subproblem solves (and other approximate computations involving matrices).
Finally, we will use the following fact from prior work.

\begin{fact}[Lemma 2, \cite{CohenST21}]\label{fact:relsmooth}
For convex functions $f, r$, if $f$ is $L$-relatively smooth with respect to $r$, then $\partial f$ is $L$-relatively Lipschitz with respect to $r$.
\end{fact}
 %
\section{Box-simplex games without alternating minimization}\label{sec:noaltmin}

In this section, we develop algorithms for computing approximate saddle points to \eqref{eq:boxsimplex}. We will follow notation of Section~\ref{ssec:boxsimplex}, and in the context of this section only we let $\xset \defeq [-1, 1]^n$, $\yset \defeq \Delta^d$, and $\zset \defeq \xset \times \yset$.
To minimize notational clutter, we assume throughout the section that $\norm{\ma}_{1 \to 1} \le 1$, lifting this assumption in the proof of Theorem~\ref{thm:boxsimplex} only.\footnote{As we demonstrate in the proof of Theorem~\ref{thm:boxsimplex}, this is without loss of generality via rescaling.}
We also define the gradient operator of \eqref{eq:boxsimplex}:
\begin{equation}\label{eq:glpdef}g(x, y) \defeq \Par{\ma y + c, b - \ma^\top x}.\end{equation}
We refer to the $x$ and $y$ components of $g(x, y)$ by $g\x(x, y) \defeq \ma y + c$ and $g\y(x, y) \defeq b - \ma^\top x$.
Finally, for notational convenience, we define the Bregman divergence associated with $\rlp{\alpha}$ as
\[V^{(\alpha)} \defeq V^{\rlp{\alpha}}.\]


\subsection{Approximation-tolerant extragradient method}\label{ssec:oracles_def}

We begin by stating two oracles whose guarantees, when combined with Definition~\ref{def:areaconvex}, give our conceptual algorithm for \eqref{eq:boxsimplex}. These oracles can be viewed as approximately implementing steps of the extragradient method in Appendix~\ref{app:xgrad_convergence} (which assumes exact proximal oracle steps, see Definition~\ref{def:prox}). We refer the reader to Section 3 and Appendix D.2 of \cite{CohenST21} for a recent tutorial. We develop subroutines which satisfy these relaxed definitions in the following Section~\ref{ssec:oracles_impl}.

\begin{definition}[Gradient step oracle]\label{def:Ograd}
For a problem \eqref{eq:boxsimplex}, we say $\Ograd: \zset \times \zset^* \to \zset$ is an $(\alpha, \beta)$-\emph{gradient step oracle} if on input $(z, v)$, it returns $z' $ such that for all $u \in \zset$, 
\[\inprod{v}{z' - u} \le V^{(\alpha + \beta)}_z(u) - V^{(\alpha)}_{z'}(u) - V^{(\alpha)}_z(z').\]
\end{definition}
\begin{definition}[Extragradient step oracle]\label{def:Oxgrad}
For a problem \eqref{eq:boxsimplex}, we say $\Oxgrad: \zset \times \zset^* \times \yset \to \zset$ is an $(\alpha, \beta)$-\emph{extragradient step oracle} if on input $(z, v, \by)$, it returns $(z^+, \by^+)$ such that
\begin{align*}
\inprod{v}{z^+ - u} &\le V^{(\alpha)}_z(u) - V^{(\alpha)}_{z^+}(u) - V^{(\alpha)}_z(z^+) \\
&+ V^{\beta h}_{\by}(u\y) - V^{\beta  h}_{\by^+}(u\y) \text{ for all } u = (u\x, u\y) \in \zset .
\end{align*}
\end{definition}

When $\beta = 0$, Definitions~\ref{def:Ograd} and~\ref{def:Oxgrad} reduce to the conventional proximal oracle steps used by the extragradient method of \cite{Nemirovski04}. In our solver for \eqref{eq:boxsimplex}, we use $\beta > 0$ to compensate for our inexact subproblem solves. The asymmetry in Definitions~\ref{def:Ograd} and~\ref{def:Oxgrad} reflect an asymmetry in the analyses of extragradient methods. In typical analyses, the regret is bounded for the ``gradient oracle'' points, but the regret upper bound is stated in terms of the divergences of the ``extragradient oracle'' points (which our inexact oracles need to compensate for). 

We now state some useful properties of $\rlp{\alpha}$ adapted from \cite{JambulapatiST19}.

\begin{lemma}\label{lem:rlpfacts}
The following properties of $\rlp{\alpha}$ hold.
\begin{enumerate}
\item For $\alpha \ge \half$, $\rlp{\alpha}$ is jointly convex over $\zset$.
\item For $\alpha \ge 2$, $\rlp{\alpha}$ is $\frac 1 3$-area convex with respect to $g$ defined in \eqref{eq:glpdef}.
\end{enumerate}
\end{lemma}
\begin{proof}
We prove generalizations of these statements in Section~\ref{sec:sdpfacts}, but briefly comment on both parts. The first is a tightening of Lemma 6 in \cite{JambulapatiST19}, and follows immediately from the specialization of Proposition~\ref{prop:regularizer} to the case of diagonal $\{\ma_i\}_{i \in [n]}$. The second is a tightening of Lemma 4 in \cite{JambulapatiST19}, and follows immediately from the same diagonal matrix specialization of Corollary~\ref{cor:acsdp}.
\end{proof}

The utility of our Definitions~\ref{def:areaconvex},~\ref{def:Ograd}, and~\ref{def:Oxgrad} reveals itself through the following lemma.

\begin{lemma}\label{lem:combine}
Let $z \in \zset$, $\by \in \yset$, $\alpha \ge 2$, $\beta, \gamma \ge 0$ and $0 \le \eta \le \frac 1 3$. Let $z' \gets \Ograd(z, \eta g(z))$ and $(z^+, \by^+) \gets \Oxgrad(z, \frac \eta 2 g(z'), \by)$, where $\Ograd$ is an $(\alpha, \beta)$-gradient step oracle and $\Oxgrad$ is an $(\alpha + \beta, \gamma)$-extragradient step oracle. Then for all $u \in \zset$,
\[\inprod{\eta g(z')}{z' - u} \le 2V^{(\alpha + \beta)}_{z}(u) - 2V^{(\alpha + \beta)}_{z^+}(u) + 2V^{\gamma h}_{\by}(u\y) - 2V^{\gamma h}_{\by^+}(u\y).\]
\end{lemma}
\begin{proof}
By definition of $\Ograd$ (with $u \gets z^+$) and $\Oxgrad$, we have
\begin{equation}\label{eq:ograd_guarantees}
\begin{aligned}
\inprod{\eta g(z)}{z' - z^+} &\le V^{(\alpha + \beta)}_z(z^+) - V^{(\alpha)}_{z'}(z^+) - V^{(\alpha)}_z(z'),\\
\inprod{\eta g(z')}{z^+ - u} &\le 2V^{(\alpha + \beta)}_z(u) - 2V^{(\alpha + \beta)}_{z^+}(u) - 2V^{(\alpha + \beta)}_z(z^+) \\
&+ 2V^{\gamma  h}_{\by}(u\y) - 2V^{\gamma h}_{\by^+}(u\y).
\end{aligned}
\end{equation}
Combining yields
\begin{align*}
\inprod{\eta g(z')}{z' - u} &\le 2V^{(\alpha + \beta)}_z(u) - 2V^{(\alpha + \beta)}_{z^+}(u) + 2V^{\gamma h}_{\by}(u\y) - 2V^{\gamma h}_{\by^+}(u\y) \\
&+ \eta \inprod{g(z') - g(z)}{z' - z^+} - V^{(\alpha)}_z(z') - V^{(\alpha + \beta)}_{z}(z^+) - V^{(\alpha)}_{z'}(z^+) \\
&\le 2V^{(\alpha + \beta)}_z(u) - 2V^{(\alpha + \beta)}_{z^+}(u) + 2V^{\gamma h}_{\by}(u\y) - 2V^{\gamma h}_{\by^+}(u\y) \\
&+ \eta \inprod{g(z') - g(z)}{z' - z^+} - V^{(\alpha)}_z(z') - V^{(\alpha)}_{z}(z^+),
\end{align*}
where in the second inequality we used that $V^{(\alpha + \beta)}$ dominates $V^{(\alpha)}$, and $V_{z'}^{(\alpha)}(z^+) \ge 0$ by Lemma~\ref{lem:rlpfacts}. 
The conclusion follows by applying Definition~\ref{def:areaconvex} (see \eqref{eq:magicareaconvex}) and the second fact in Lemma~\ref{lem:rlpfacts}.
\end{proof}

It is straightforward to check that when $\beta = \gamma = 0$, the proof of Lemma~\ref{lem:combine} is exactly the same as the analysis in Appendix~\ref{app:xgrad_convergence}, and hence it yields similar implications as standard extragradient methods. In particular, a scaling of the left-hand side upper bounds duality gap of the point $z'$, and the right-hand side telescopes (and may be bounded using the following fact).

\begin{lemma}\label{lem:rdivbound}
Let $\alpha ,\gamma \ge 0$, and let $z_0 = (x_0, y_0)$ where $x_0 = \0_n$ and $y_0 = \frac 1 d \1_d$. Then $z_0$ is the minimizer of $\rlp{\alpha}$ over $\zset$, and $V^{(\alpha)}_{z_0}(u) \le 1 + \alpha \log d,\; V^{\gamma h}_{y_0}(u\y) \le \gamma \log d \text{ for all } u = (u\x, u\y) \in \zset$.
\end{lemma}
\begin{proof}
We can verify that $z_0$ minimizes $\rlp{\alpha}$ by computing $\nabla \rlp{\alpha}(z_0)$ and checking that it is orthogonal to $z - z_0$ for all $z \in \zset$. Moreover by first-order optimality of $z_0$, we have the first conclusion:
\begin{align*}
V^{(\alpha)}_{z_0}(u) = \rlp{\alpha}(u) - \rlp{\alpha}(z_0) - \inprod{\nabla \rlp{\alpha}(z_0)}{u - z_0} = \rlp{\alpha}(u) - \rlp{\alpha}(z_0) \le 1 + \alpha \log d,
\end{align*}
where we used that over $\zset$, the range of $h$ is bounded by $\log d$, and the range of the quadratic portion of $\rlp{\alpha}$ is bounded by $1$ by using $\|x^2\|_\infty \le 1$ and $\norm{y}_1 = 1$. The second conclusion is similar, where we again use the range of $h$ and that $y_0$ minimizes it.
\end{proof}

Finally, for convenience to the reader, we put together Lemma~\ref{lem:combine} and~\ref{lem:rdivbound} to obtain an analysis of the following conceptual ``outer loop'' extragradient algorithm (subject to the implementation of gradient and extragradient step oracles), Algorithm~\ref{alg:conceptualboxsimplex}. Our end-to-end algorithm in Section~\ref{ssec:algoboxsimplex}, Algorithm~\ref{alg:boxsimplex}, will be an explicit implementation of the framework in Algorithm~\ref{alg:conceptualboxsimplex}; we provide a runtime analysis and error guarantee for Algorithm~\ref{alg:boxsimplex} in Theorem~\ref{thm:boxsimplex}.

\begin{algorithm2e}[H]
	\caption{\textsf{ConceptualBoxSimplex}($\ma, b, c, \Ograd, \Oxgrad$)}
	\label{alg:conceptualboxsimplex}
	\DontPrintSemicolon
	{\bfseries Input:} $\ma\in\R^{n \times d}$ with $L \defeq \norm{\ma}_{1 \to 1}$, desired accuracy $\epsilon \in (0, L)$, $\Ograd$ a $(2, 2)$-gradient step oracle, $\Oxgrad$ a $(4, 4)$-extragradient step oracle \;
	Initialize $x_0 \gets \0_n$, $y_0 \gets \frac 1 d \1_d$, $\by_0 \gets \frac 1 d \1_d$, $\hx \gets \0_n$, $\hy \gets \0_d$, $T \gets \lceil\frac{6(8\log d + 1)L}{\eps} \rceil$, $\eta \gets \frac 1 3$ \;
	Rescale $\ma \gets \frac 1 L \ma$, $b \gets \frac 1 L b$, $c \gets \frac 1 L c$\;\label{line:rescalec}
	\For{$t=0$ {\bfseries{\textup{to}}} $T-1$}{
		$g_t \gets (\ma y_t + c, b - \ma^\top x_t)$ \;
		$z'_t \defeq (x'_t, y'_t) \gets \Ograd(z_t, \eta g_t)$ \;
		$g'_t \gets (\ma y'_t + c, b - \ma^\top x'_t)$ \;
		$(z_{t + 1}, \by_{t + 1}) \defeq (x_{t + 1}, y_{t + 1}, \by_{t + 1}) \gets \Oxgrad(z_t, \frac \eta 2 g'_t, \by_t)$\;
	}
	{\bfseries Return:} $(\hat{x}, \hat{y}) \gets \frac 1 T \sum_{t = 0}^{T - 1} (x'_t, y'_t)$
\end{algorithm2e}

\begin{corollary}\label{cor:conceptualboxsimplex}
Algorithm~\ref{alg:conceptualboxsimplex} deterministically computes an $\eps$-approximate saddle point to \eqref{eq:boxsimplex}.
\end{corollary}
\begin{proof}
First, clearly the rescaling in Line~\ref{line:rescalec} multiplies the entire problem \eqref{eq:boxsimplex} by $\frac 1 L$, so an $\frac \eps L$-approximate saddle point to the new problem becomes an $\eps$-approximate saddle point for the original. Throughout the rest of the proof it suffices to treat $L = 1$. Next, by telescoping and averaging Lemma~\ref{lem:combine} with $\alpha = \beta = 2$, $\gamma = 4$, and $\eta = \frac 1 3$, we have for $z_0 = (x_0, y_0)$ and any $u = (u\x, u\y) \in \zset$
\begin{align*}
	\frac 1 T \sum_{t = 0}^{T - 1} \inprod{g(z'_t)}{z'_t - u} \le \frac{6\Par{V^{(4)}_{z_0}(u) + V^{4Lh}_{\by_0}(u\y)}}{T} \le \eps.
\end{align*}
The last inequality used the bounds in Lemma~\ref{lem:rdivbound} and the definition of $T$. Moreover since $g$ is bilinear, and $\hz \defeq (\hx, \hy)$ is the average of the $z'_t$ iterates, we have $\inprod{g(\hz)}{\hz - u} \le \eps$. Taking the supremum over $u \in \zset$ bounds the duality gap of $\hz$ and gives the conclusion.
\end{proof}

\subsection{Implementing oracles}\label{ssec:oracles_impl}

In this section, we give generic constructions of gradient and extragradient step oracles. We will rely on the following claims on optimizing jointly convex functions of two variables.

\begin{lemma}\label{lem:partialderiv}
	Let $\xset \subseteq \R^m$ and $\yset \subseteq \R^n$ be convex compact subsets. Suppose $F: \xset \times \yset \to \R$ is jointly convex over its argument $(x, y) \in \xset \times \yset$. For $y \in \yset$, define $x_{\textup{br}}(y) \defeq \argmin_{x \in \xset} F(x, y)$ and $f(y) \defeq F(x_{\textup{br}}(y), y)$. Then for all $y \in \yset$, $\partial_y F(x_{\textup{br}}(y), y) \subset \partial f(y)$.
\end{lemma}
\begin{proof}
	Let $x \defeq x_{\textup{br}}(y)$, $z \in \yset$, and $w \defeq x_{\textup{br}}(z)$. We first claim that $0 \in \partial_x F(x, y)$. To see this, the definition of the subgradient set implies that it suffices to show for all $x' \in \xset$, $F(x, y) \le F(x', y)$, which is true by definition. Hence by convexity of $F$ from $(x, y)$ to $(w, z)$, we have the desired
	\[f(z) = F(w, z) \ge F(x, y) + \inprod{\partial_y F(x, y)}{z - y} = f(y) + \inprod{\partial_y F(x, y)}{z - y}.\]
\end{proof}

\begin{restatable}{lemma}{restaterelativesmooth}\label{lem:relativesmooth}
	In the setting of Lemma~\ref{lem:partialderiv}, suppose for any $x \in \xset$, $F(x, \cdot)$ (as a function over $\yset$) always is $r: \yset \to \R$ plus a linear function (where the linear function may depend on $x$). Then $r - f$ is convex, and $f - q$ is convex for any $q: \yset \to \R$ such that $F - q: \xset \times \yset \to \R$ is jointly convex.
\end{restatable}
\begin{proof}
	For the first claim, for any $y, z \in \yset$, letting $x \defeq x_{\textup{br}}(y)$, we have
	\begin{gather*}
		\Par{r(z) - r(y) - \inprod{\partial r(y)}{z - y}} - \Par{f(z) - f(y) - \inprod{\partial f(y)}{z - y}} \\
		= \Par{F(x, z) - F(x, y) - \inprod{\partial_y F(x, y)}{z - y}} - \Par{f(z) - f(y) - \inprod{\partial f(y)}{z - y}} = F(x, z) - f(z) \ge 0.
	\end{gather*}
	The first equality used that the first-order expansion of $F(x, \cdot)$ agrees with the first-order expansion of $r$ (as they only differ by a linear term), and the second equality used $F(x, y) = f(y)$ and Lemma~\ref{lem:partialderiv}. The only inequality used the definition of $f$. For the second claim, note that $x_{\textup{br}}(y)$ also minimizes $F - q$ over $\xset$ for any fixed $y$, as $q(y)$ is a constant in this objective. Since $F - q$ is convex and partial minimization of a convex function preserves convexity, we have the conclusion.
\end{proof}

The first part of Lemma~\ref{lem:relativesmooth} implies a relative smoothness statement, i.e.\ if jointly convex $F$ equals $r$ up to a linear term, then minimizing $F$ over $\xset$ yields a function which is relatively smooth with respect to $r$. The second part implies an analogous relative strong convexity statement. In Appendix~\ref{app:rc_subproblem_solve}, we show these implications yield a linearly-convergent method for the subproblems in algorithms for \eqref{eq:boxsimplex} using $r^{(\alpha)}_{\ma}$, via off-the-shelf tools from \cite{LuFN18}. This observation already matches the subproblem solver in \cite{Sherman17} without relying on multiplicative stability properties.

\begin{algorithm2e}[H]
	\caption{\textsf{GradStepOracle}($z, v, \alpha, \beta$)}
	\label{alg:gradstep}
	\DontPrintSemicolon
	{\bfseries Input:} $z = (x, y) \in \zset$, $v = (v\x, v\y) \in \zset^*$, $\alpha, \beta \ge 0$ \;
	$y' \gets \argmin_{\hy \in \yset} \inprod{v\y + \nabla_y \rlp{\alpha}(x_{\textup{br}}(y), y) - \nabla_y \rlp{\alpha}(z) }{\hy} + V^{\beta h}_{y}(\hy)$, where for all $\hy \in \yset$,\footnote{We slightly abuse notation and use $x_{\textup{br}}$ in \eqref{eq:bestx} in a consistent way with how it is used in Lemma~\ref{lem:partialderiv}, where $F$ in Lemma~\ref{lem:partialderiv} is taken to be the jointly convex function of $(x, y)$ in \eqref{eq:bestx} before minimizing over $\xset$.}
	\begin{equation}\label{eq:bestx}x_{\textup{br}}(\hy) \defeq \argmin_{\hx \in \xset} \inprod{v\x - \nabla_x \rlp{\alpha}(z)}{\hx} + \rlp{\alpha}(\hz) \text{ where } \hz \defeq (\hx, \hy) \end{equation} \;
	$x' \gets x_{\textup{br}}(y')$ \;
	{\bfseries Return:} $(x', y')$
\end{algorithm2e}

\begin{lemma}\label{lem:Ograd_alg}
	For $\beta \ge \alpha \ge \half$, Algorithm~\ref{alg:gradstep} is an $(\alpha, \beta)$-gradient step oracle.
\end{lemma}
\begin{proof}
	By \eqref{eq:threepoint} and the first-order optimality condition for $y'$, we have for any $u = (u\x, u\y) \in \zset$,
	\begin{equation}\label{eq:step1ograd}\inprod{v\y + \nabla_y \rlp{\alpha}(x_{\textup{br}}(y), y) - \nabla_y \rlp{\alpha}(z)}{y' - u\y} \le V^{\beta h}_{y}(u\y) - V^{\beta h}_{y'}(u\y) - V^{\beta  h}_{y}(y').\end{equation}
	Further, define for any $\hy \in \yset$,
	\begin{equation}\label{eq:fdef}
		f(\hy) \defeq \inprod{v}{\hz} + V^{(\alpha)}_z(\hz)\text{ where } \hz \defeq (x_{\textup{br}}(\hy), \hy).
	\end{equation}
	Note that $f$ is a partial minimization of a function on two variables which is a linear term plus $\rlp{\alpha}$, which is convex by Lemma~\ref{lem:rlpfacts}. For any fixed $x$, $\rlp{\alpha}(x, y)$ is itself $\alpha h(y)$ plus a linear function. Lemma~\ref{lem:relativesmooth} then shows $f$ is $\alpha$-relatively smooth with respect to $h$. We then have for all $\hy, u\y \in \yset$,
	\begin{equation}\label{eq:step2ograd}
		\begin{aligned}
			\inprod{\nabla_y \rlp{\alpha}(x_{\textup{br}}(\hy), \hy) - \nabla_y \rlp{\alpha}(x_{\textup{br}}(y), y)}{\hy - u\y} &= \inprod{\partial f(\hy) - \partial f(y)}{\hy - u\y}\\
			&\le V^{\alpha h}_{y}(\hy) + V^{\alpha h}_{\hy}(u\y). \end{aligned}\end{equation}
	Here the equality used Lemma~\ref{lem:partialderiv} where the linear shift between $\rlp{\alpha}$ and the minimization problem inducing $f$ cancels in the expression $\partial f(\hy) - \partial f(y)$, and the inequality used Fact~\ref{fact:relsmooth}. Combining \eqref{eq:step1ograd} and \eqref{eq:step2ograd} (with $\hy \gets y'$) and using $V^{\beta h}$ dominates $V^{\alpha h}$ yields
	\begin{equation}\label{eq:step3ograd}
		\inprod{v\y + \nabla_y \rlp{\alpha}(z') - \nabla_y \rlp{\alpha}(z)}{y' - u\y} \le V^{\beta  h}_{y}(u\y).
	\end{equation}
	Finally, first-order optimality of $x'$ with respect to the objective induced by $y'$ implies for all $u\x \in \xset$,
	\begin{equation}\label{eq:xoptimal}
		\inprod{v\x + \nabla_x \rlp{\alpha}(z') - \nabla_x \rlp{\alpha}(z)}{x' - u\x} \le 0,\end{equation}
	so combining with \eqref{eq:step3ograd} we have for $u = (u\x, u\y)$, $\langle v + \nabla \rlp{\alpha}(z') - \nabla \rlp{\alpha}(z),z' - u\rangle \le V^{\beta h}_{y}(u\y)$. 
	The conclusion follows by using the identity \eqref{eq:threepoint} to rewrite $\langle \nabla \rlp{\alpha}(z') - \nabla \rlp{\alpha}(z), u - z'\rangle$.
\end{proof}

\begin{algorithm2e}[H]
	\caption{\textsf{XGradStepOracle}($z, v, \by, \alpha, \beta$)}
	\label{alg:xgradstep}
	\DontPrintSemicolon
	{\bfseries Input:} $z = (x, y) \in \zset$, $v = (v\x, v\y) \in \zset^*$, $\by \in \yset$, $\alpha, \beta \ge 0$ \;
        $y^+ \gets \argmin_{\hy \in \yset} \inprod{v\y + \nabla_y \rlp{\alpha}(x_{\textup{br}}(\by), \by) - \nabla_y \rlp{\alpha}(z) }{\hy} + V^{\beta  h}_{\by}(\hy)$ (following notation \eqref{eq:bestx}) \;
        $x^+ \gets x_{\textup{br}}(y^+)$ \;
        $\by^+ \gets \argmin_{\hy \in \yset} \inprod{v\y + \nabla_y \rlp{\alpha}(x^+, y^+) - \nabla_y \rlp{\alpha}(z) }{\hy} + V^{\beta h}_{\by}(\hy)$\;
	 {\bfseries Return:} $(x^+, y^+, \by^+)$
\end{algorithm2e}

\begin{lemma}\label{lem:Oxgrad_alg}
For $\beta \ge \alpha \ge \half$, Algorithm~\ref{alg:xgradstep} is an $(\alpha, \beta)$-extragradient step oracle.
\end{lemma}
\begin{proof}
The optimality conditions for $y^+$ (with respect to $\by^+$) and $\by^+$ (with respect to $u\y \in \yset$) yield:
\begin{align*}
\inprod{v\y + \nabla_y \rlp{\alpha}(x_{\textup{br}}(\by), \by) - \nabla_y \rlp{\alpha}(z)}{y^+ - \by^+} &\le V^{\beta  h}_{\by}(\by^+) - V^{\beta h}_{y^+}(\by^+) - V^{\beta h}_{\by}(y^+),\\
\inprod{v\y + \nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(z)}{\by^+ - u\y} &\le V^{\beta h}_{\by}(u\y) - V^{\beta  h}_{\by^+}(u\y) - V^{\beta  h}_{\by}(\by^+).
\end{align*}
Combining the above gives
\begin{equation}\label{eq:step1oxgrad}
\begin{aligned}
\inprod{v\y + \nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(z)}{y^+ - u\y} &\le V^{\beta  h}_{\by}(u\y) - V^{\beta h}_{\by^+}(u\y) - V^{\beta  h}_{y^+}(\by^+) - V^{\beta h}_{\by}(y^+) \\
&+ \inprod{\nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(x_{\textup{br}}(\by), \by)}{y^+ - \by^+},
\end{aligned}
\end{equation}
where we observe that
\begin{align*}
\inprod{v\y + \nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(z)}{y^+ - u\y} - \inprod{\nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(x_{\textup{br}}(\by), \by)}{y^+ - \by^+} \\
= \inprod{v\y + \nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(z)}{\by^+ - u\y} + \inprod{v\y + \nabla_y \rlp{\alpha}(x_{\textup{br}}(\by), \by) - \nabla_y \rlp{\alpha}(z)}{y^+ - \by^+}.
\end{align*}
Next, we claim that (following the notation \eqref{eq:fdef}), analogously to \eqref{eq:step3ograd},
\begin{equation}\label{eq:step2oxgrad}
\begin{aligned}\inprod{\nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(x_{\textup{br}}(\by), \by)}{y^+ - \by^+} &= \inprod{\partial f(y^+) - \partial f(\by)}{y^+ - \by^+} \\
&\le V^{\beta h}_{y^+}(\by^+) + V^{\beta  h}_{\by}(y^+).\end{aligned}
\end{equation}
Plugging \eqref{eq:step2oxgrad} into \eqref{eq:step1oxgrad} gives
\begin{align*}
\inprod{v\y + \nabla_y \rlp{\alpha}(z^+) - \nabla_y \rlp{\alpha}(z)}{y^+ - u\y} \le V^{\beta  h}_{\by}(u\y) - V^{\beta  h}_{\by^+}(u\y),
\end{align*}
and combined with first-order optimality of $x^+$ with respect to $u\x \in \xset$ again gives for $u = (u\x, u\y)$,
\begin{align*}
\inprod{v + \nabla \rlp{\alpha}(z^+) - \nabla \rlp{\alpha}(z)}{z^+ - u} \le V^{\beta  h}_{\by}(u\y) - V^{\beta  h}_{\by^+}(u\y).
\end{align*}
The conclusion again follows by using the identity \eqref{eq:threepoint}.
\end{proof}

\subsection{Algorithm}\label{ssec:algoboxsimplex}

Finally, we put the pieces of this section together to prove a convergence rate on Algorithm~\ref{alg:boxsimplex}.

\begin{algorithm2e}[H]
	\caption{\textsf{BoxSimplex}($\ma, b, c, \eps$)}
	\label{alg:boxsimplex}
	\DontPrintSemicolon
	{\bfseries Input:} $\ma\in\R^{n \times d}$ with $L \defeq \norm{\ma}_{1 \to 1}$, desired accuracy $\epsilon \in (0, L)$ \;
	Initialize $x_0 \gets \0_n$, $y_0 \gets \frac 1 d \1_d$, $\by_0 \gets \frac 1 d \1_d$, $\hx \gets \0_n$, $\hy \gets \0_d$, $T \gets \lceil\frac{6(8\log d + 1)L}{\eps} \rceil$ \;
        Rescale $\ma \gets \frac 1 L \ma$, $b \gets \frac 1 L b$, $c \gets \frac 1 L c$\;\label{line:rescale}
	\For{$t=0$ {\bfseries{\textup{to}}} $T-1$}{
		$(g\x_t, g\y_t) \gets \frac 1 3 (\ma y_t + c, b - \ma^\top x_t)$  \label{line:ogradbeg} \tcp*{Gradient oracle start.}\;
            $x^\star_t \gets \projx\Par{-\frac{g\x_t - 2\diag{x_t}|\ma|y_t}{2|\ma|y_t}}$ \;
            $y'_t \gets \projy\Par{y_t \circ \exp\Par{-\frac 1 \beta (g\y_t + |\ma|^\top (x^\star_t)^2 - |\ma|^\top x_t^2)}}$\;
		$x'_t \gets \projx\Par{-\frac{g\x_t - 2\diag{x_t}|\ma|y_t}{2|\ma|y'_t}}$  \label{line:ogradend} \; 
            $(\hx, \hy) \gets (\hx, \hy) + \frac 1 T (x'_t, y'_t)$    \tcp*{Running average maintenance.}\;
		 $(g\x_t, g\y_t) \gets \frac 1 6 (\ma y'_t + c, b - \ma^\top x'_t)$  \label{line:oxgradbeg}\tcp*{Extragradient oracle start.}\;
            $\bx^\star_t \gets \projx\Par{-\frac{g\x_t - 2\diag{x_t}|\ma|y_t}{2|\ma|\by_t}}$ \;
            $y_{t + 1} \gets \projy\Par{\by_t \circ \exp\Par{-\frac 1 \beta (g\y_t + |\ma|^\top (\bx^\star_t)^2 + \alpha\log \by_t - |\ma|^\top x_t^2 - \alpha \log y_t) }}$ \;
            $x_{t + 1} \gets \projx\Par{-\frac{g\x_t - 2\diag{x_t}|\ma|y_{t}}{2|\ma|y_{t + 1}}}$ \;
            $\by_{t + 1} \gets \projy\Par{y_t \circ \exp\Par{-\frac 1 \beta (g\y_t + |\ma|^\top (x_{t + 1})^2 + \alpha\log y_{t + 1} - |\ma|^\top x_t^2 - \alpha \log y_t) }}$ \label{line:oxgradend}\;
	}
	 {\bfseries Return:} $(\hat{x}, \hat{y})$
\end{algorithm2e}

\restateboxsimplex*
\begin{proof}
 By observation, Lines~\ref{line:ogradbeg} to~\ref{line:ogradend} implement Algorithm~\ref{alg:gradstep} (used in Lemma~\ref{lem:Ograd_alg}), with the parameters required by Lemma~\ref{lem:combine}. Similarly, Lines~\ref{line:oxgradbeg} to~\ref{line:oxgradend} implement Algorithm~\ref{alg:xgradstep} (used in Lemma~\ref{lem:Oxgrad_alg}), with the parameters required by Lemma~\ref{lem:combine}. Correctness thus follows from Corollary~\ref{cor:conceptualboxsimplex}. For the runtime, without loss of generality $\nnz(\ma) \ge \max(n, d)$ (else we may drop columns or rows appropriately), and each of $T$ iterations is dominated by a constant number of matrix-vector multiplications.
\end{proof}

\iffalse
For a variational operator $g$, we consider the following algorithm:

\arun{We need a prox macro}
\begin{align*}
w_t &= Prox^r_{z_t}( \lambda g(z_t)) \\
z_{t+1} &= Prox^r_{z_t} ( \frac{\lambda}{2} g(w_t) ).
\end{align*}
The three-point inequality applied to the two steps gives 
\begin{align*}
\lambda \l g(z_t), w_t - z_{t+1} \r &\leq V^r_{z_t}(z_{t+1}) - V^r_{w_t}(z_{t+1}) - V^r_{z_t}(w_t) \\
\frac{\lambda}{2} \l g(w_t), z_{t+1} - u \r &\leq V^r_{z_t}(u) - V^r_{z_{t+1}}(u) - V^r_{z_t}(z_{t+1}).
\end{align*}
Dividing the first inequality by $2$ and adding the inequalities, we obtain
\begin{align*}
\frac{\lambda}{2} \l g(w_t), w_t - u \r + \frac{\lambda}{2} \l g(z_t) - g(w_t), w_t - z_{t+1} \r &\leq V^r_{z_t}(u) - V^r_{z_{t+1}}(u) - V^r_{z_t}(z_{t+1}) \\
&+ \frac{1}{2} \left( V^r_{z_t}(z_{t+1}) - V^r_{w_t}(z_{t+1}) - V^r_{z_t}(w_t) \right)
\end{align*}
The right-hand side of this inequality is 
\[
 V^r_{z_t}(u) - V^r_{z_{t+1}}(u) - \frac{1}{2}  \left( V^r_{z_t}(z_{t+1}) + V^r_{w_t}(z_{t+1}) + V^r_{z_t}(w_t) \right).
\]
We now observe the following equalities, where we define $c_t = \frac{z_t + w_t + z_{t+1}}{3}$.
\begin{align*}
V^r_{z_t}(z_{t+1}) + V^r_{z_t}(w_t) &= r(z_t) + r(w_t) + r(z_{t+1}) - 3 r(z_t) - \l \nabla r(z_t), z_{t+1} + w_t - 2 z_t \r \\
&= r(z_t) + r(w_t) + r(z_{t+1}) - 3 r(c_t) + 3 r(c_t) - 3 r(z_t) - 3 \l \nabla r(z_t), c_t - z_t \r \\
&=  r(z_t) + r(w_t) + r(z_{t+1}) - 3 r(c_t) + 3 V^r_{z_t}(c_t) \\
&\geq r(z_t) + r(w_t) + r(z_{t+1}) - 3 r(c_t).
\end{align*}
As area convexity implies 
\[
\lambda \l g(w_t) - g(z_t), w_t - z_{t+1} \r \leq  r(z_t) + r(w_t) + r(z_{t+1}) - 3 r(c_t) \leq V^r_{z_t}(z_{t+1}) + V^r_{z_t}(w_t) 
\]
by the above, our result of the three-point inequality implies
\begin{align*}
\frac{\lambda}{2} \l g(w_t), w_t - u \r &\leq  V^r_{z_t}(u) - V^r_{z_{t+1}}(u) \\
&+ \frac{\lambda}{2} \l g(w_t) - g(z_t), w_t - z_{t+1} \r - \frac{1}{2}  \left( V^r_{z_t}(z_{t+1}) + V^r_{w_t}(z_{t+1}) + V^r_{z_t}(w_t) \right) \\
&\leq V^r_{z_t}(u) - V^r_{z_{t+1}}(u) 
\end{align*}
and the claim follows. 

Maybe interesting future work: this slackened mirror prox analysis gives us three divergences to play with now. In this way, we seem to immediately generalize the relative Lipschitzness paper, as the condition on $g$ that would allow this mirror prox to work is a (strict?) generalization of both area convexity and the original relative Lipschitzness. Is this useful for anything in general? I would be interested in getting a more direct proof of a sufficient three-point condition that doesn't go through area convexity and instead uses this slackened condition.

In some sense, this algorithm is exactly the same as Sherman's modified dual extrapolation. Do we want to point this out anywhere? I think it's a nice observation.
\fi %
\section{Box-spectraplex games}\label{sec:sdpfacts}

In this section, we develop algorithms for computing approximate saddle points to \eqref{eq:boxspectraplex}. We will follow the notation of Section~\ref{ssec:boxspec}, and in the context of this section only we let $\xset \defeq [-1, 1]^n$, $\yset \defeq \Delta^{d \times d}$, and $\zset \defeq \xset \times \yset$. As in Section~\ref{sec:noaltmin}, we assume throughout the section that (following \eqref{eq:lip_sdp_def}) $L_{\alla} \le 1$, except when proving Theorem~\ref{thm:boxspectraplex}. We also define the gradient operator of \eqref{eq:boxspectraplex}:
\begin{equation}\label{eq:gdefsdp}g(x, \my) \defeq \Par{\alla^*(\my) + c, \mb - \alla(x)}.\end{equation}
We refer to the $x$ and $\my$ components of $g(x, \my)$ by $g\x(x, \my) \defeq \alla^*(\my) + c$ and $g\y(x, \my) \defeq \mb - \alla(x)$.
Finally, for notational convenience, we define the Bregman divergence associated with $\rsdp{\alpha, \mu}$ as
\[V^{(\alpha, \mu)} \defeq V^{\rsdp{\alpha, \mu}}.\]
When $\mu = 0$, we will simply denote the divergence as $V^{(\alpha)}$.

\subsection{Regularizer properties}\label{ssec:regprop}
In this section, we state properties about our regularizer $\rsdp{\alpha, \mu}$, used to prove the following.

\begin{restatable}{proposition}{regularizer}
\label{prop:regularizer}
Let $\alla \defeq \{\ma_i\}_{i \in [n]} \subset \Sym^d$. Let $\jac \in \R^{(n + d^2) \times (n + d^2)}$ be an associated (skew-symmetric) linear operator mapping $\R^n \times \Sym^d \to \R^n \times \Sym^d$, such that for all $v \in \R^n$ and $\mm \in \Sym^d$,
\[\jac(v, \mm) = (\alla^*(\mm), -\alla(v)).\] 
 
 $\rsdp{\alpha, \mu}$ satisfies the following properties: 
 \begin{itemize}
     \item  For any $\alpha \ge \half$, $\mu \ge 0$, $\rsdp{\alpha, \mu}$ is jointly convex over $[-1, 1]^n \times \Delta^{d \times d}$.
     \item For $\alpha \ge 2, \mu \geq 0$ and any $(x, \my) \in [-1, 1] \times \Delta^{d \times d}$, the following matrix is positive semidefinite:
\[\begin{pmatrix}
 \nabla^2 \rsdp{\alpha,\mu}(x, \my) & - \jac \\
 \jac^\top & \nabla^2 \rsdp{\alpha,\mu}(x, \my)
\end{pmatrix}.\]
 \end{itemize}
\end{restatable}

This proposition generalizes Lemma~\ref{lem:rlpfacts}, which was proven (up to constant factors) in \cite{JambulapatiST19}. However, unlike the vector-vector setting, the Hessian of matrix entropy is significantly less well-behaved due to monotonicity bounds which do not apply to $\exp$, a function which is not operator monotone. Our proofs make use of the following nontrivial lower bound on the Hessian of $H(\my)$ in Appendix~\ref{app:regularizer}, where we recall from Section~\ref{sec:prelims} that $H$ is the negated von Neumann entropy function.

\begin{restatable}{lemma}{vnentropysc}
\label{lem:vnentropy_sc}
Let $\alla \defeq \{\ma_i\}_{i \in [n]} \subset \PSD^d$ satisfy $\sum_{i \in [n]} \ma_i = \id_d$. For any $\mm \in \Sym^d$ and $\my \in \PD^d$ we have $\nabla^2 H(\my)[\mm, \mm] \ge \nabla^2 h(y)[m, m]\text{ for } y \defeq \alla^*(\my),\; m \defeq \alla^*(\mm)$.
\end{restatable}

\begin{lemma}\label{lem:crossterm}
Let $\alla \defeq \{\ma_i\}_{i \in [n]} \subset \Sym^d$ satisfy \eqref{eq:lip_sdp_def}. 
For $\my \in \Delta^{d \times d}$, $\tau > 0$, vectors $v \in \R^n$ and $x \in [-1, 1]^n$, and matrix $\mm \in \Sym^d$, defining $v \circ x$ to be the entrywise product of vectors,
\[2\inprod{v \circ x}{\alla^*(\mm)} \le \tau\diag{|\alla|^*(\my)}[v, v] + \frac{1} \tau \nabla^2 H(\my)[\mm, \mm].\]
\end{lemma}
\begin{proof}
By the assumption that $L_{\alla} \le 1$, $\sum_{i \in [n]} |\ma|_i \preceq \id$. For all $i \in [n]$ define $\ma_i^+, \ma_i^- \in \PSD^d$ such that $|\ma| = \ma_i^+ + \ma_i^-$, where $\ma_i^+$ keeps the positive eigenvalues of $\ma$ (with the same eigenspaces), and $\ma_i^-$ negates the negative eigenvalues; note also that $\ma_i = \ma_i^+ - \ma_i^-$. Finally let $\ma_0 = \id - \sum_{i \in [n]} |\ma_i|$ (which is positive semidefinite since $L_{\alla} \le 1$) and let $\alla' = \{\ma_0\} \cup \{\ma_i^+, \ma_i^-\}_{i \in [n]}$. By Lemma~\ref{lem:vnentropy_sc} applied to the set of matrices $\alla'$ we conclude
\begin{equation}\label{eq:hesslowerbound}\nabla^2 H(\my)[\mm, \mm] \ge \sum_{i \in [n]} \Par{\frac{\inprod{\ma_i^+}{\mm}^2}{\inprod{\ma_i^+}{\my}} + \frac{\inprod{\ma_i^-}{\mm}^2}{\inprod{\ma_i^-}{\my}}},\end{equation}
where we drop the (nonnegative) term in Lemma~\ref{lem:vnentropy_sc} corresponding to the diagonal element $\inprod{\ma_0}{\my}$. The conclusion follows by plugging in the above inequality, yielding
\begin{gather*}\tau \diag{|\alla|^*(\my)}[v, v] + \frac 1 \tau \sum_{i \in [n]} \Par{\frac{\inprod{\ma_i^+}{\mm}^2}{\inprod{\ma_i^+}{\my}} + \frac{\inprod{\ma_i^-}{\mm}^2}{\inprod{\ma_i^-}{\my}}} \\
= \sum_{i \in [n]} \tau v_i^2 \Par{\inprod{\ma_i^+}{\my} + \inprod{\ma_i^-}{\my}} + \frac 1 \tau \Par{\frac{\inprod{\ma_i^+}{\mm}^2}{\inprod{\ma_i^+}{\my}} + \frac{\inprod{\ma_i^-}{\mm}^2}{\inprod{\ma_i^-}{\my}}} \\
\ge 2\sum_{i \in [n]} v_i x_i \Par{\inprod{\ma_i^+}{\mm} - \inprod{\ma_i^-}{\mm}} = 2\inprod{v \circ x}{\alla^*(\mm)}.
\end{gather*}
The last line above used Young's inequality which shows for all $i \in [n]$ (recalling $|x_i| \le 1$)
\begin{align*}
2v_ix_i\inprod{\ma_i^+}{\mm} &\le \tau v_i^2 \inprod{\ma_i^+}{\my} + \frac 1 \tau \cdot \frac{\inprod{\ma_i^+}{\mm}^2}{\inprod{\ma_i^+}{\my}}, \\
-2v_ix_i\inprod{\ma_i^-}{\mm} &\le \tau v_i^2 \inprod{\ma_i^-}{\my} + \frac 1 \tau \cdot \frac{\inprod{\ma_i^-}{\mm}^2}{\inprod{\ma_i^-}{\my}}.
\end{align*}
\end{proof}
With this bound, we prove \Cref{prop:regularizer}.

\begin{proof}[Proof of \Cref{prop:regularizer}]
For both claims, it suffices to show the case $\mu = 0$, as the sum of convex functions is convex and the sum of positive semidefinite matrices is positive semidefinite. We begin with the first claim. Joint convexity of $\rsdp{\alpha}$ is equivalent to showing that the quadratic form of $\nabla^2 \rsdp{\alpha}$ (viewed as a $(n + d^2) \times (n + d^2)$ matrix) with respect to $(v, \mm)$ is nonnegative for any $v \in \R^n , \mm \in \Sym^{d}$: in other words
\[\alpha \nabla^2 H(\my)[\mm, \mm] + 2\inprod{v \circ x}{|\alla|^*(\mm)} + 2\diag{|\alla|^*(\my)}[v,v] \ge 0.\]
This follows from Lemma~\ref{lem:crossterm} with $\tau = 2$, where we replace $x \gets -x$ and use $\alpha \ge \half$. For the second claim, let $v, u \in \R^n$ and $\mm, \mn \in \Sym^d$. Consider the quadratic form of 
\[\begin{pmatrix}
 \nabla^2 \rsdp{\alpha}(x, \my) & - \jac \\
 \jac^\top & \nabla^2 \rsdp{\alpha}(x, \my)
\end{pmatrix}.\]
with $(v, \mm, u, \mn)$. We obtain
\begin{equation}\label{eq:bigquadform}
\begin{gathered}
\alpha \Par{\nabla^2 H(\my)[\mm, \mm] + \nabla^2 H(\my)[\mn, \mn]} + 2\diag{|\alla|^*(\my)}[v, v] + 2\diag{|\alla|^*(\my)}[u, u] \\
+ 2\inprod{v \circ x}{|\alla|^*(\mm)} + 2\inprod{u \circ x}{|\alla|^*(\mn)} + 2\inprod{v}{\alla^*(\mn)} - 2\inprod{u}{\alla^*(\mm)}.
\end{gathered}
\end{equation}
By applying Lemma~\ref{lem:crossterm} with $\tau = 1$, we have
\begin{align*}
-2\inprod{v \circ x}{|\alla|^*(\mm)} &\le \diag{|\alla|^*(\my)}[v, v]+\nabla^2 H(\my)[\mm, \mm], \\
-2\inprod{u \circ x}{|\alla|^*(\mn)} &\le \diag{|\alla|^*(\my)}[u, u]+ \nabla^2 H(\my)[\mn, \mn], \\
-2\inprod{v}{\alla^*(\mn)} &\le \diag{|\alla|^*(\my)}[v, v]+ \nabla^2 H(\my)[\mn, \mn], \\
2\inprod{u}{\alla^*(\mm)} &\le \diag{|\alla|^*(\my)}[u, u]+ \nabla^2 H(\my)[\mm, \mm].
\end{align*}
Combining these four equations shows that the quantity in \eqref{eq:bigquadform} is nonnegative as desired.
\end{proof}
As a corollary of known tools \cite{Sherman17}, this implies that $\rsdp{\alpha,\mu}$ is an area convex regularizer.
\begin{corollary}\label{cor:acsdp}
Let $g(x, \my)$ be the gradient operator of \eqref{eq:boxspectraplex}. Then for $\alpha \ge 2$, $\mu \ge 0$, $\rsdp{\alpha,\mu}$ is $\frac 1 3$-area convex with respect to $g$ (Definition~\ref{def:areaconvex}).
\end{corollary}
\begin{proof}
It suffices to check for $\mu = 0$ as increasing $\mu$ only makes the right-hand side larger. This case follows from the second conclusion of Proposition~\ref{prop:regularizer} and Theorem 1.6 of \cite{Sherman17}, a generic way of proving area convexity via checking a second-order positive semidefiniteness condition. 
\end{proof}

We collect a few additional tools which we use in harnessing \eqref{eq:rsdpdef} for our algorithms later in this section. We first state a bound on the divergence of $\rsdp{\alpha,\mu}$ from its minimizer.

\begin{lemma}\label{lem:rsdpdivbound}
Let $\alpha, \mu \ge 0$, and let $z_0 = (x_0, \my_0)$ where $x_0 = \0_n$ and $\my_0 = \frac 1 d \id_d$. Then $z_0$ is the minimizer of $\rsdp{\alpha, \mu}$ over $\zset$, and
\[V^{(\alpha,\mu)}_{z_0}(u) \le 1 + \alpha \log d + \frac{\mu n} 2 \text{ for all } u = (u\x, u\y) \in \zset.\]
\end{lemma}
\begin{proof}
The proof is almost identical to Lemma~\ref{lem:rdivbound} so we only discuss differences. First, the additive range of the squared regularizer on $\xset$ from $x_0$ is bounded by $\frac{\mu n}{2}$. Also, the additive range of von Neumann entropy is $\log d$ (similarly to vector entropy), and it is minimized by $\my_0$.
\end{proof}

The gradient of our regularizer $\rsdp{\alpha, \mu}$ is difficult to evaluate exactly due to the presence of matrix exponentials arising from recursive descent steps. We formalize the approximate gradient access required by our algorithms in the following definition.

\begin{definition}[MEQ oracle]\label{def:matexporacle}
Let $\eps, \delta, \gamma \in (0, 1)$. We say $\Omatexp$ is an $(\eps, \delta, \gamma)$-matrix exponential query (MEQ) oracle for $\{\ma_i\}_{i \in [n]} \subset \Sym^d$ and $\mm \in \Sym^d$, if it returns $\{V_i\}_{i \in [n]}$ such that with probability $\ge 1 - \delta$, $\Abs{V_i - \inprod{\ma_i}{\my}} \le \eps \inprod{|\ma_i|}{\my} + \gamma \Tr(|\ma_i|)$ for all $i \in [n]$, where $\my \defeq \projy(\exp\mm)$.
\end{definition}

Definition~\ref{def:matexporacle} returns approximations of all $\inprod{\ma_i}{\my}$ up to $\eps$-multiplicative error (in $\my$'s product through $|\ma_i|$, instead of $\ma_i$), and an additive $\gamma \Tr(|\ma_i|)$ error. 
In Appendix~\ref{sec:matexpvec}, we give an implementation of $\Omatexp$ whose runtime depends polynomially on $\eps$ and polylogarithmically on $\gamma$.\footnote{Similar guarantees appear in the literature on approximately solving SDPs, but we could not find a statement with the additive-multiplicative guarantees our method requires, so we provide a self-contained proof for completeness.}

\begin{proposition}\label{prop:approxgrad}
Let $\normop{\mm} \le R$. We can implement an $(\eps, \delta, \gamma)$-MEQ oracle for $\{\ma_i\}_{i \in [n]} \subset \Sym^d$, $\mm \in \Sym^d$ in time
\[O\Par{\tmv(\mm) \cdot \sqrt{R + \log \frac 1 {\gamma\eps}} \cdot \frac{ \log^{1.5}(\frac{Rnd}{\gamma\delta\eps})}{\eps^2} + \Par{\sum_{i \in [n]} \tmv(\ma_i)} \cdot \frac{\log \frac {nd} \delta}{\eps^2}}.\]
\end{proposition}
\begin{proof}
It suffices to combine Lemma~\ref{lem:approxtrace} and~\ref{lem:approxinprod} in Appendix~\ref{sec:matexpvec}, adjusting $\eps$ by a constant factor.
\end{proof}

\subsection{Approximation-tolerant mirror prox}\label{ssec:approxdualex}

We next provide approximation-tolerant variants of the algorithms in Section~\ref{sec:noaltmin}. This tolerance is necessitated by error introduced by approximations to the matrix exponential we develop in Appendix~\ref{sec:matexpvec}. We begin by formalizing the notions of approximation required for our framework.

\begin{definition}[Approximate gradient oracle]\label{def:approxg}
We say $\tg: \zset \to \zset^*$ is a $\Delta$-approximate gradient oracle for $g: \zset \to \zset^*$ if for all $z, z' \in \zset$, $|\inprod{\tg(z) - g(z)}{z'}| \le \Delta$.
\end{definition}

\begin{definition}[Approximate best response oracle]\label{def:approxbreg}
We say $\tx: \xset^* \to \xset$ is a $\Delta$-approximate best response oracle for $r: \zset \to \R$ and $\my \in \yset$ if for all $v \in \xset^*$ and $u\x \in \xset$, the following hold:
\begin{align*}
\norm{\tx(\my, v) - (\argmin_{x \in \xset} \inprod{v}{x} + r(x, \my))}_\infty \le \Delta, \\
\inprod{v + \nabla_x r(x, \my)}{x - u\x} \le \Delta.
\end{align*}
\end{definition}

We pause to remark that for $g$ in \eqref{eq:gdefsdp}, the component $g\y(x, \my) = \mb - \alla(x)$ is simple to explicitly write down given $x$, and will incur no error in our implementation. Further, the term $g\x(x, \my) - c = \alla^*(\my)$ can be efficiently approximated to the accuracy required by Definition~\ref{def:approxg} by taking $\eps, \gamma \gets O(\Delta)$ in Definition~\ref{def:matexporacle}. Under the scaling $L_{\alla} \le 1$, the $\ell_1$ error incurred by $\Omatexp$ is $O(\eps + \gamma)$, which satisfies Definition~\ref{def:approxg} as $\xset$ is $\ell_\infty$-constrained. 
In the following Section~\ref{ssec:subproblem} we will exploit the stronger multiplicative-additive guarantee afforded by Definition~\ref{def:matexporacle} to meet Definition~\ref{def:approxbreg}. Equipped with Definitions~\ref{def:approxg} and~\ref{def:approxbreg}, we give simple error-tolerant extensions of Sections~\ref{ssec:oracles_def} and~\ref{ssec:oracles_impl}.

\begin{definition}[Approximate gradient step oracle]\label{def:approx_grad}
For a problem \eqref{eq:boxspectraplex}, we say $\tOgrad: \zset \times \zset^* \to \zset$ is an $(\alpha, \beta, \mu, \Delta)$-approximate gradient step oracle if on input $(z, v)$, it returns $z'$ such that for all $u \in \zset$,
\[\inprod{v}{z' - u} \le V^{(\alpha + \beta, \mu)}_z(u) - V^{(\alpha, \mu)}_{z'}(u) - V^{(\alpha, \mu)}_z(z') + \Delta.\]
\end{definition}

\begin{definition}[Approximate extragradient step oracle]\label{def:approx_xgrad}
For a problem \eqref{eq:boxspectraplex}, we say $\tOxgrad: \zset \times \zset^* \times \yset \to \zset$ is an $(\alpha, \beta, \mu, \Delta)$-approximate extragradient step oracle if on input $(z, v, \bmy)$ it returns $(z^+, \bmy^+)$ such that
\begin{align*}
\inprod{v}{z^+ - u} &\le V^{(\alpha, \mu)}_z(u) - V^{(\alpha, \mu)}_{z^+}(u) - V^{(\alpha, \mu)}_z(z^+) \\
&+V^{\beta H}_{\bmy}(u\y) - V^{\beta H}_{\bmy^+}(u\y) + \Delta \text{ for all } u = (u\x, u\y) \in \zset.
\end{align*}
\end{definition}

Definitions~\ref{def:approx_grad} and~\ref{def:approx_xgrad} are exactly the same as Definitions~\ref{def:Ograd} and~\ref{def:Oxgrad}, except they are generalized to the matrix setting, tolerate regularizers $V^{(\cdot, \mu)}$ with $\mu \ge 0$, and allow for $\Delta$ error in their bounds. 

\begin{corollary}\label{cor:approx_combine}
Let $z \in \zset$, $\bmy \in \yset$, $\alpha \ge 2$, $\beta, \gamma \ge 0$, and $0 \le \eta \le \frac 1 3$. Let $\tg$ be a $\Delta$-approximate gradient oracle for $g$ in \eqref{eq:gdefsdp}. Let $z' \gets \tOgrad(z, \eta \tg(z))$ and $(z^+, \bmy^+) \gets \tOxgrad(z, \frac \eta 2 \tg(z'), \bmy)$, where $\tOgrad$ is an $(\alpha, \beta, \mu, \Delta)$-approximate gradient step oracle and $\tOxgrad$ is an $(\alpha + \beta, \gamma, \mu, \Delta)$-approximate extragradient step oracle. Then for all $u \in \zset$,
\[\inprod{\eta g(z')}{z' - u} \le 2V^{(\alpha + \beta, \mu)}_z(u) - 2V^{(\alpha + \beta,\mu)}_{z^+}(u) + 2V_{\bmy}^{\gamma H}(u\y) - 2V_{\bmy^+}^{\gamma H}(u\y) + 5\Delta.\]
\end{corollary}
\begin{proof}
The proof is the same as Lemma~\ref{lem:combine}, but we incur error due to the approximate guarantees of $\tg$, $\tOgrad$, and $\tOxgrad$. It is straightforward to see that using $\tOgrad$ and $\tOxgrad$ incurs $3\Delta$ additive error on the right-hand sides of \eqref{eq:ograd_guarantees}. Further, using $\tg$ instead of $g$ implies that the left-hand sides of \eqref{eq:ograd_guarantees} hold up to $6\eta\Delta$ error via H\"older's inequality. Combining these errors yields the result.
\end{proof}

\begin{algorithm2e}[H]
	\caption{\textsf{ApproxGradStepOracle}($z, v, \alpha, \beta, \mu, \tx, \tnabla_x \rsdp{\alpha,\mu}(\cdot, \my)$)}
	\label{alg:approxgradstep}
	\DontPrintSemicolon
	{\bfseries Input:} $z = (x, \my) \in \zset$, $v = (v\x, v\y) \in \zset^*$, $\alpha, \beta, \mu \ge 0$, $\tx$, a $\Delta$-approximate best response oracle for $\rsdp{\alpha,\mu}$ and $\my$ or $\my'$ (defined below), $\tnabla_x \rsdp{\alpha,\mu}(\cdot, \my)$, a $\Delta$-approximate gradient oracle for $\nabla_x \rsdp{\alpha,\mu}(\cdot, \my)$ \;
        $w \gets \tnabla_x \rsdp{\alpha,\mu}\Par{x, \my}$ \;
        $\hx \gets \tx(\my, v\x - w)$\;
        $\my' \gets \argmin_{\hmy \in \yset}\inprod{v\y + \nabla_y \rsdp{\alpha, \mu}\Par{\hx, \my} - \nabla_y\rsdp{\alpha, \mu}(x, \my)}{\hmy} + V^{\beta H}_{\my}\Par{\hmy}$ \;
        $x' \gets \tx\Par{\my', g\x - w}$ \;
	 {\bfseries Return:} $(x', \my')$
\end{algorithm2e}

\begin{corollary}\label{cor:approx_Ograd_alg}
For $\beta \ge \alpha \ge \half$, $\mu \ge 0$, Algorithm~\ref{alg:approxgradstep} is an $(\alpha, \beta, \mu, 11\Delta)$-approximate gradient step oracle.
\end{corollary}
\begin{proof}
The proof is the same as Lemma~\ref{lem:Ograd_alg}, but we incur error due to the approximate computations of $w$, $\hx$, and $x'$. Let $r \defeq \rsdp{\alpha, \mu}$ for convenience. Let $x_\star$ minimize the subproblem defining $\hx$, and $x_\star'$ minimize the subproblem defining $x'$.

Lemma~\ref{lem:Ograd_alg} combines inequalities \eqref{eq:step1ograd}, \eqref{eq:step2ograd}, and \eqref{eq:xoptimal}, and we will bound the error incurred in each. It is straightforward to check that the approximation guarantee on $\hx$ implies $\norm{\hx^2 - x_\star^2}_\infty \le 2\Delta$, and so $\norm{\nabla_y r(\hx, \my) - \nabla_y r(x_\star, \my)}_\infty \le 2\Delta$. Hence, in place of \eqref{eq:step1ograd}, H\"older's inequality yields
\begin{align*}\inprod{v\y + \nabla_y r\Par{x_\star, \my} - \nabla_y r\Par{x, \my}}{\my' - u\y} &= \inprod{v\y + \nabla_y r\Par{\hx, \my} - \nabla_y r\Par{x, \my}}{\my' - u\y} \\
	&+ \inprod{\nabla_y r\Par{x_\star, \my} - \nabla_y r\Par{\hx, \my}}{\my' - u\y} \\
	&\le  V^{\beta H}_{\my}(u\y) - V^{\beta H}_{\my'}(u\y) - V^{\beta H}_{\my}(\my') + 4\Delta.\end{align*}
Further, in place of \eqref{eq:step2ograd} we have by a similar argument
\begin{align*}
	\inprod{\nabla_y r(x', \my') - \nabla_y r(x_\star, \my)}{\my' - u\y} &\le \inprod{\nabla_y r(x'_\star, \my') - \nabla_y r(x_\star, \my)}{\my' - u\y} \\
	&+ \inprod{\nabla_y r(x', \my') - \nabla_y r(x'_\star, \my')}{\my' - u\y} \\
	&\le V^{\alpha H}_{\my}(\my') + V^{\alpha H}_{\my'}(u\y) + 4\Delta. 
	\end{align*}
Finally, by the second condition on the oracle $\tx$ defining $x'$, in place of \eqref{eq:xoptimal} we have
\begin{align*}
\inprod{v\x + \nabla_x r(x', \my') - w}{x' - u\x} &\le \Delta \\
\implies \inprod{v\x + \nabla_x r(x', \my') - \nabla_x r(x, \my)}{x' - u\x} &\le \inprod{v\x + \nabla_x r(x', \my') - w}{x' - u\x} \\
&+ \inprod{w - \nabla_x(x, \my)}{x' - u\x} \le 3\Delta.
\end{align*}
The last inequality used the guarantee of the oracle $\tnabla_x r$, and H\"older's inequality.
\end{proof}

\begin{algorithm2e}[H]
	\caption{\textsf{ApproXGradStepOracle}($z, v, \bmy, \alpha, \beta, \mu, \tx, \tnabla_x \rsdp{\alpha,\mu}(\cdot, \my)$)}
	\label{alg:approxxgradstep}
	\DontPrintSemicolon
	{\bfseries Input:} $z = (x, \my) \in \zset$, $v = (v\x, v\y) \in \zset^*$, $\bmy \in \yset$, $\alpha, \beta, \mu \ge 0$, $\tx$, a $\Delta$-approximate best response oracle for $\rsdp{\alpha,\mu}$ and $\bmy$ or $\my^+$ (defined below), $\tnabla_x \rsdp{\alpha,\mu}(\cdot, \my)$, a $\Delta$-approximate gradient oracle for $\nabla_x \rsdp{\alpha,\mu}(\cdot, \my)$ \;
        $w \gets \tnabla_x \rsdp{\alpha,\mu}\Par{x, \my}$ \;
        $\bx \gets \tx(\bmy, v\x - w)$\;
        $\my^+ \gets \argmin_{\hmy \in \yset} \inprod{v\y + \nabla_y \rsdp{\alpha,\mu}(\bar{x}, \bmy) - \nabla_y \rsdp{\alpha,\mu}(x,\my)}{\hmy} + V^{\beta H}_{\bmy}(\hmy)$ \;
        $x^+ \gets \tx\Par{\my^+, v\x - w}$ \;
        $\bmy^+ \gets \argmin_{\hmy \in \yset} \inprod{v\y + \nabla_y \rsdp{\alpha,\mu}(x^+, \my^+) - \nabla_y \rsdp{\alpha,\mu}(x,\my)}{\hmy} + V^{\beta H}_{\bmy}(\hmy)$\;
	 {\bfseries Return:} $(x^+, \my^+, \bmy^+)$
\end{algorithm2e}

\begin{corollary}\label{cor:approx_Oxgrad_alg}
For $\beta \ge \alpha \ge \half$, $\mu \ge 0$, Algorithm~\ref{alg:approxxgradstep} is an $(\alpha, \beta,\mu, 11\Delta)$-approximate extragradient step oracle.
\end{corollary}
\begin{proof}
The proof is the same as Lemma~\ref{lem:Oxgrad_alg}, but we incur error due to the approximate computations of $w$, $\bar{x}$, and $x^+$. Let $r \defeq \rsdp{\alpha,\mu}$ for convenience. Let $\bar{x}_\star$ minimize the subproblem defining $\bx$ and let $x^+_\star$ minimize the subproblem defining $x^+$.

Lemma~\ref{lem:Oxgrad_alg} combines inequalities \eqref{eq:step1oxgrad}, \eqref{eq:step2oxgrad}, and first-order optimality of $x^+$. As in Corollary~\ref{cor:approx_Ograd_alg}, in place of \eqref{eq:step1oxgrad} the approximation guarantee on $\bar{x}$ yields for any $u\y \in \yset$,
\begin{align*}
\inprod{v\y + \nabla_y r(x^+, \my^+) - \nabla_y r(x, \my)}{\my^+ - u\y} &= \inprod{v\y + \nabla_y r(\bx, \bmy) - \nabla_y r(x, \my)}{\my^+ - \bmy^+} \\
&+ \inprod{v\y + \nabla_y r(x^+, \my^+) - \nabla_y r(x, \my)}{\bmy^+ - u\y} \\
&+ \inprod{\nabla_y r(x^+, \my^+) - \nabla_y r(\bx, \bmy)}{\my^+ - \bmy^+} \\
&\le V^{\beta H}_{\bmy}(u\y) - V^{\beta H}_{\bmy^+}(u\y) - V^{\beta H}_{\my^+}(\bmy^+) - V^{\beta H}_{\bmy}(\my^+) \\
&+ \inprod{\nabla_y r(x^+, \my^+) - \nabla_y r(\bx, \bmy)}{\my^+ - \bmy^+} \\
&\le V^{\beta H}_{\bmy}(u\y) - V^{\beta H}_{\bmy^+}(u\y) - V^{\beta H}_{\my^+}(\bmy^+) - V^{\beta H}_{\bmy}(\my^+) \\
&+ \inprod{\nabla_y r(x^+, \my^+) - \nabla_y r(\bx_\star, \bmy)}{\my^+ - \bmy^+} + 4\Delta.
\end{align*}
Similarly, in place of \eqref{eq:step2oxgrad} we have
\begin{align*}
\inprod{\nabla_y r(x^+, \my^+) - \nabla_y r(\bx_\star, \bmy)}{\my^+ - \bmy^+} &= \inprod{\nabla_y r(x^+_\star, \my^+) - \nabla_y r(\bx_\star, \bmy)}{\my^+ - \bmy^+} \\ 
&+ \inprod{\nabla_y r(x^+, \my^+) - \nabla_y r(x^+_\star, \my^+)}{\my^+ - \bmy^+} \\
&\le V^{\beta H}_{\my^+}(\bmy^+) + V^{\beta H}_{\bmy}(\my^+) + 4\Delta.
\end{align*}
Finally, as in the proof of Corollary~\ref{cor:approx_Ograd_alg}, we lose an additive $3\Delta$ in the optimality of $x^+$.
\end{proof}

\subsection{Implementation details}\label{ssec:subproblem}

\paragraph{Approximate best response oracles.} We first develop an implementation for the approximate best response oracles in Section~\ref{ssec:approxdualex}. Specifically, we study problems of the form
\begin{equation}\label{eq:regsubproblem}
\min_{x \in \xset} \inprod{v}{x} + \inprod{|\alla|^*(\my)}{x^2} + \frac{\mu }{2}\norm{x}_2^2
\end{equation}
where $v \in \xset^*$, $\my \in \yset$, $\{\ma_i\}_{i \in [n]}$, and $\mu$ are fixed throughout. We further introduce the notation
\begin{align*}
&\ell_v(q) \defeq \min_{x \in \xset} \inprod{v}{x} + \inprod{q}{x^2} = \sum_{i \in [n]} \begin{cases}-\frac{v_i^2}{2q_i} & |v_i| \le 2 q_i \\ q_i - |v_i| & |v_i| \ge 2 q_i\end{cases}, \\
&\text{minimized by } x = \projx\Par{-\frac v {2q}}.
\end{align*}
With this notation, the problem in \eqref{eq:regsubproblem} can be written as $\min_{x \in \xset} \ell_v(|\alla|^*(\my) + \frac \mu 2 \1_n)$. Our best response oracle for $\rsdp{\alpha, \mu}$ that our algorithms require follows from the following structural fact. 

\begin{lemma}\label{lem:approxquadratic}
Let $q \in \R^n_{\ge 0}$, and for a parameter $\eps \in (0, 1)$, suppose that $\tq \in \R^n_{\ge 0}$ satisfies
\begin{equation}\label{eq:tqbound}
\Abs{q_i - \tq_i} \le \eps q_i \text{ for all } i \in [n].
\end{equation}
Then letting $x_\star$ minimize $\ell_v(q)$ over $\xset$ and $\tx_\star$ minimize $\ell_v(\tq)$ over $\xset$, for all $u\x \in \xset$,
\begin{align*}
\norm{x_\star - \tx_\star}_\infty &\le \eps, \\
\inprod{v + 2q\circ \tx_\star}{\tx_\star - u\x} &\le 2\eps\norm{q}_1.
\end{align*}
\end{lemma}
\begin{proof}
The first claim decomposes coordinatewise: in light of our characterization of the minimizers of $\ell_v$, it suffices to show that for any scalars $a, b \ge 0$ and $v \in \R$ such that $|a - b| \le \eps a$,
\[\Abs{\med\Par{\frac v a, -1, 1} - \med\Par{\frac v b, -1, 1}} \le \eps.\]
This follows by a case analysis: if $v \in [-a, a]$, then the additive error after the median operation is at most $\eps$. Otherwise, if $v \ge a$, then the first corresponding term after taking a median is $1$ and the second is in $[1, 1 + \eps]$, and a similar argument handles the case $v \le -a$.

For the second claim, note that first-order optimality of $\tx_\star$ implies
\[\inprod{v + 2\tq \circ \tx_\star}{\tx_\star - u\x} \le 0,\]
and H\"older's inequality implies the conclusion where we use $\norm{2\tq \circ \tx_\star - 2q\circ \tx_\star}_1 \le 2\norm{q - \tq}_1$.
\end{proof}
Lemma~\ref{lem:approxquadratic} shows that to implement an oracle meeting Definition~\ref{def:approxbreg}, it suffices to compute a multiplicative approximation to $q = |\alla|^*(\my) + \frac \mu 2 \1_n$. We will use an implementation trick which was observed by \cite{AssadiJJST22} to implicitly maintain $\my$ exactly via its logarithm. In particular, we use recursive structure to maintain explicit vectors $w, w' \in \R^n$ and a scalar $b \in \R$, such that
\begin{equation}\label{eq:implicit_y}\my = \projy\Par{\exp\Par{\alla(w) + |\alla|(w') + b \mb}}.\end{equation}
Assuming this maintenance, we give a full implementation of an approximate best response oracle.

\begin{lemma}\label{lem:bestresponse_impl}
Let $\Delta, \delta \in (0, 1)$, $\mu \le \frac 1 n$, $\alpha \ge 0$, and suppose for explicit $w, w' \in \R^n$, $b \in \R$, $\my$ satisfies \eqref{eq:implicit_y}. We can implement a $\Delta$-approximate best response oracle for $\rsdp{\alpha, \mu}$ and $\my$ with probability $\ge \delta$ in one call to a $(\frac \Delta 2, \delta, \frac{\mu\Delta}{2d})$-MEQ oracle for $\{|\ma_i|\}_{i \in [n]}$ and $\mm = \alla(w) + |\alla|(w') + b\mb$.
\end{lemma}
\begin{proof}
It suffices to apply Lemma~\ref{lem:approxquadratic} with $\tq = \frac \mu 2 \1_n$ plus the approximation to $|\alla|^*(\my)$ from the MEQ oracle, which clearly meets the multiplicative approximation required by Lemma~\ref{lem:approxquadratic} with parameter $\eps = \frac \Delta 2$, since $\Tr(|\ma_i|) \le d\normop{\ma_i} \le d$. For $q = |\alla|^*(\my)+ \frac \mu 2 \1_n$ with $\mu\le \frac 1 n$, $\norm{q}_1 \le 2$, and hence the guarantees of Lemma~\ref{lem:approxquadratic} imply that returning the minimizer to $\ell_v(\tq)$ implements a best response oracle with parameter $\Delta$ as long as the MEQ oracle succeeds.
\end{proof}

\paragraph{Approximate gradient oracles.} We next note that appropriately-parameterized MEQ oracles allow us to straightforwardly implement the approximate gradient oracles for $g$ in \eqref{eq:gdefsdp} and $\nabla_x \rsdp{\alpha,\mu}(\cdot,\my)$ used in Section~\ref{ssec:approxdualex}. We state this guarantee in the following.

\begin{lemma}\label{lem:approxgrad_impl}
Let $\Delta, \delta \in (0, 1)$, $\mu \le \frac 1 n$, $\alpha \ge 0$, and suppose for explicit $w, w' \in \R^n$ and $b \in \R$, $\my$ satisfies \eqref{eq:implicit_y}. We can implement a $\Delta$-approximate gradient oracle for $g\x(\cdot, \my)$ with probability $\ge 1 - \delta$ in one call to a $(\frac \Delta 2, \delta, \frac \Delta {2d})$-MEQ oracle for $\{\ma_i\}_{i \in [n]}$ and $\mm = \alla(w) + |\alla|(w') + b\mb$. We can also implement a $\Delta$-approximate gradient oracle for $\nabla_x \rsdp{\alpha,\mu}(\cdot, \my)$ in one call to a $(\frac \Delta 2, \delta, \frac \Delta {2d})$-MEQ oracle for $\{|\ma_i|\}_{i \in [n]}$ and $\mm = \alla(w) + |\alla|(w') + b\mb$.
\end{lemma}
\begin{proof}
By H\"older's inequality, it suffices to approximate each relevant operator to an $\ell_1$ error of $\Delta$, which (again recalling $\sum_{i \in [n]}\Tr(|\ma_i|) \le d$) is satisfied by the MEQ oracle whenever it succeeds.
\end{proof}

\paragraph{Maintaining the invariant \eqref{eq:implicit_y}.} Finally, we discuss how to maintain the implicit representation \eqref{eq:implicit_y} for the $\yset$ iterates of our algorithm, under the updates of Corollary~\ref{cor:approx_Ograd_alg} or Corollary~\ref{cor:approx_Oxgrad_alg}.

\begin{lemma}\label{lem:update_Ograd}
Suppose for explicit $w, w' \in \R^n$, $b \in \R$, the input $\my$ to Corollary~\ref{cor:approx_Ograd_alg} satisfies \eqref{eq:implicit_y}, and suppose the input $g\y$ satisfies for explicit $w_g, w_g'\in \R^n$, $b_g \in \R$,
\[g\y = \alla(w_g) + |\alla|(w_g') + b_g\mb.\]
We can maintain $\my'$ implicitly of the form \eqref{eq:implicit_y} with $w, w', b$ replaced by $\hw, \hw', \hb$, such that
\begin{align*}
\max\Par{\norm{\hw}_\infty, \norm{\hw'}_\infty, |\hb|} \le \max\Par{\norm{w}_\infty, \norm{w'}_\infty, |b|} + \frac 1 \beta \max\Par{\norm{w_g}_\infty, \norm{w'_g}_\infty + 1, |b_g|}.
\end{align*}
\end{lemma}
\begin{proof}
By Theorem 3.1 of \cite{Lewis96}, $\nabla H(\my) = \log \my + \id_d$. Optimality of $\my'$ then shows
\begin{align*}
\log \my' - \log \my &= -\frac{1}{\beta} \Par{g\y + |\alla|(\hx^2) - |\alla|(x^2)} + \iota \id_d \\
\implies \log \my' &= \alla\Par{w 
 - \frac 1 \beta w_g} + |\alla|\Par{w' - \frac 1 \beta\Par{w_g' + \hx^2 - x^2}} + \Par{b - \frac 1 \beta b_g}\mb+ \iota' \id_d,
\end{align*}
for some scalars $\iota, \iota'$, by our implicit maintenance of $\my$ and $g\y$. Noting that the form of \eqref{eq:implicit_y} is invariant to arbitrary additive shifts by the identity in the argument of $\exp$ yields the claim.
\end{proof}

\begin{lemma}\label{lem:update_Oxgrad}
Suppose for explicit $w, w', \bw, \bw' \in \R^n$, $b, \bb \in \R$, the input $\my$ to Corollary~\ref{cor:approx_Oxgrad_alg} satisfies \eqref{eq:implicit_y} and the input $\bmy$ satisfies \eqref{eq:implicit_y} with $w, w', b$ replaced with $\bw, \bw', \bb$, and suppose the input $g\y$ satisfies for explicit $w_g, w'_g \in \R^n$, $b_g \in \R$,
\[g\y = \alla(w_g) + |\alla|(w'_g) + b_g\mb.\]
We can maintain $\my^+$ and $\bmy^+$ implicitly of the form \eqref{eq:implicit_y} with $w, w', b$ replaced with $w_+, w'_+, b_+$ and $\bw_+, \bw'_+, \bb_+$ respectively, such that
\begin{align*}
\max\Par{\norm{w_+}_\infty, \norm{\bw_+}_\infty, \norm{w'_+}_\infty, \norm{\bw'_+}_\infty, |b_+|, |\bb_+|} &\le \max\Par{\norm{\bw}_\infty, \norm{\bw'_\infty}, |\bb|} \\
&+ \frac 1 \beta \max\Par{\norm{w_g}_\infty, \norm{w'_g}_\infty + 1, |b_g|}.
\end{align*}
\end{lemma}
\begin{proof}
The proof is analogous to Lemma~\ref{lem:update_Ograd}; dropping multiples of $\id_d$, the optimality conditions on $\my^+$ and $\bmy^+$ imply that it suffices to take
\begin{align*}
\Par{w_+, w'_+, b_+} &\gets \Par{\bw - \frac 1 \beta w_g, \bw' - \frac 1 \beta\Par{w'_g + \bx^2 - x^2}, \bb - \frac 1 \beta b_g}, \\
\Par{\bw_+, \bw'_+, \bb_+} &\gets \Par{\bw - \frac 1 \beta w_g, \bw' - \frac 1 \beta\Par{w'_g + (x^+)^2 - x^2}, \bb - \frac 1 \beta b_g}.
\end{align*}
\end{proof}


\subsection{Algorithm}\label{ssec:sdpalg}

We now combine the results of Sections~\ref{ssec:regprop},~\ref{ssec:approxdualex}, and~\ref{ssec:subproblem} to give a full analysis for our box-spectraplex solver.
We will actually prove a generalization of our claimed result Theorem~\ref{thm:boxspectraplexintro}, phrased in terms of MEQ oracles. Theorem~\ref{thm:boxspectraplexintro} then follows by combining Theorem~\ref{thm:boxspectraplex} with the oracle implementation in Proposition~\ref{prop:approxgrad} (or exact oracles), applied with the stated required parameters in the following claim.

%

\begin{theorem}\label{thm:boxspectraplex}
There is an algorithm which computes an $\eps$-approximate saddle point to \eqref{eq:boxspectraplex}
in
\[T = O\Par{\frac{L_{\alla}\log d}{\eps}}\text{ iterations, where } L_{\alla} \defeq \normop{|\alla|(\1_n)},\]
with probability $\ge 1 - \delta$, each using $O(1)$ calls to a $(\Theta(\frac \eps {L_{\alla}}), \frac \delta {O(T)}, \Theta(\frac{\eps}{L_{\alla}nd}))$-MEQ oracle (Definition~\ref{def:matexporacle}), for $\{\ma_i, |\ma_i|\}_{i \in [n]}$ and $\mm = \alla(w) + |\alla(w')| + \beta\mb$, where $\norm{w}_\infty$, $\norm{w'}_\infty$, $|\beta| = O(T)$.
\end{theorem}
\begin{proof}
As in Theorem~\ref{thm:boxsimplex}, we can assume throughout (by rescaling) that $L_{\alla} = 1$ for simplicity. We again use the parameters $\alpha = \beta = 2$, $\gamma = 4$, and $\eta = \frac 1 3$; we also use $\mu = \frac 1 n$. This is a valid choice of $\eta$ for use in Corollary~\ref{cor:approx_combine}, due to Corollary~\ref{cor:acsdp}. Hence, using Lemma~\ref{lem:rsdpdivbound} to bound the initial divergence, the proof of Theorem~\ref{thm:boxsimplex} (substituting Corollary~\ref{cor:approx_Ograd_alg} and~\ref{cor:approx_Oxgrad_alg} appropriately) implies we need to obtain $\Theta(\eps)$-approximate best response oracles and approximate gradient oracles in each iteration.

We next observe that under the given parameter settings, the recursions stated in Lemmas~\ref{lem:update_Ograd} and~\ref{lem:update_Oxgrad} implies we can maintain every $\yset$ iterate used in calls to Corollary~\ref{cor:approx_combine} and~\ref{cor:acsdp} as $\projy(\exp(\mm))$, where $\mm$ has the form stated in the theorem statement. In particular, using notation of Lemmas~\ref{lem:update_Ograd} and~\ref{lem:update_Oxgrad}, it is clear $w_g \in \xset$, $|b_g| = \Theta(1)$, and $w'_g = \0_n$ in every call. Under this maintenance, the conclusion follows by plugging in Lemmas~\ref{lem:bestresponse_impl} and~\ref{lem:approxgrad_impl} as our oracle implementations.
\end{proof}

Combining Theorem~\ref{thm:boxspectraplex} and Proposition~\ref{prop:approxgrad} subsumes and slightly refines the first result in Theorem~\ref{thm:boxspectraplexintro}. The second result in Theorem~\ref{thm:boxspectraplexintro} comes from an exact implementation of MEQ oracles.

\begin{restatable}{corollary}{restateboxspectraplexrefined}\label{cor:boxspectraplexrefined}
There is an algorithm which computes an $\eps$-approximate saddle point to \eqref{eq:boxspectraplex} in time
\[O\Par{\Par{\tmv(\mb) + \sum_{i \in [n]}\Par{\tmv(\ma_i) + \tmv(|\ma_i|)}}\cdot \frac{L_{\alla}^{3}\sqrt{\Ltot}\log(d)\log^{2}\Par{\frac{Lnd}{\delta\eps}}}{\eps^{3.5}}},\]
with probability $\ge 1 - \delta$, where $\Ltot \defeq \|\sum_{i \in [n]} |\ma_i|\|_{\textup{op}} + \normop{\mb}$ and $L_{\alla} \defeq \|\sum_{i \in [n]} |\ma_i|\|_{\textup{op}}$.
\end{restatable} %
\section{Applications}\label{sec:apps}

We finally discuss various applications of our new box-simplex solver in Theorem~\ref{thm:boxsimplex}. 

\paragraph{Optimal transport.} The discrete optimal transportation problem is a fundamental optimization problem on discrete probability distributions. Given two input distributions $p, q \in \Delta^d$ and a cost matrix $\mc \in \R_{\geq 0}^{d \times d}$, the problem is to find a matrix $\mx$ solving the following linear program:
\[
\min_{\mx \1_d = p, \mx^\top \1_d = q, \mx \geq 0} \inprod{\mc}{\mx}.
\]
In previous work, \cite{JambulapatiST19} observes that a $2\epsilon$-approximate optimal transport map can be recovered from an $\epsilon$-approximate saddle point of the following box-simplex game:
\begin{equation}
	\label{eqn:ot}    
	\min_{\mx \in \Delta^{n^2}} \max_{y \in [-1,1]^{2n}} \inprod{\mc}{\mx} + 2 \norm{\mc}_{\max} y^\top (\mb \mx - r).
\end{equation}
We treat $\mx$ as an element of $\Delta^{n^2}$ and define $\mb : \R^{n^2} \to \R^{2n}$ is the linear operator which sends $\mx$ to $[\mx \1 , \mx^\top 1]$ and $r = [p,q]$. By applying \Cref{thm:boxsimplex} to \eqref{eqn:ot}, we obtain the following.

\begin{corollary}
	\label{corr:ot}
	Let $\mc \in \R_{\geq 0}^{n \times n}$, $p, q \in \Delta^n$ be given. There is an algorithm to compute an $\epsilon$-approximate optimal transport map from $p$ to $q$ with costs $\mc$ running in time $O(n^2 \log n \norm{\mc}_{\max} \epsilon^{-1})$. 
\end{corollary}


\paragraph{Min-mean cycle.} Additionally, our improved box-simplex game solver gives an improved algorithm for the minimum mean-cycle problem. In this problem, we are given an undirected weighted graph $G$ and we seek a cycle $C$ of length $\ell$ of minimum mean length $\frac{1}{\ell} \sum_{e \in C} w_e$. As noted in \cite{AltschulerP20}, the min-mean cycle problem is equivalent to the following primal-dual optimization problem:
\begin{equation}
	\label{eqn:mmc}
	\min_{x \in \Delta^m} \max_{y \in [-1,1]^n} w^\top x + 3 d w_{\max} y^\top \mb x
\end{equation}
where $\mb$ is the oriented graph incidence matrix of $G$, $w_{\max}$ is the maximum edge weight in $G$, and $d$ is the (unweighted) graph diameter of $G$. This problem is a box-simplex game, and $\norm{d w_{\max} \mb}_{1 \to 1}  = O(d w_{\max})$. Further, by \cite{AltschulerP20} $\epsilon$-approximate saddle points for this problem give $\epsilon$-approximate min-mean cycles: applying \Cref{thm:boxsimplex} to \eqref{eqn:mmc} then gives the following corollary.
\begin{corollary}
	\label{corr:mmc}
	Let $G$ be a (nonnegative weighted) graph with $n$ vertices, $m$ edges; let $w \in \R_{\geq 0}^m$ be the vector of edge weights. Let $w_{\max}$ be the maximum edge weight in $G$ and $d$ be the unweighted diameter of $G$.
	There is an algorithm to compute an $\epsilon$-approximate minimum-mean cycle in time 
	\[
	O \left( \frac{md w_{\max} \log n}{\epsilon} \right).
	\]
\end{corollary}

\paragraph{Faster flow problems on graphs.} Finally, our framework implies faster approximation algorithms for the important combinatorial optimization problems of transshipment and maximum flow. Given a graph $G$ with edge weights $w$ and a demand $d$, these problems can written in the form 
\begin{equation*}
	\label{eqn:flows}
	\min_{\mb  f = d} \norm{ \mw f}_1 \quad \text{and} \quad \min_{\mb f  = d} \norm{\mw f}_\infty
\end{equation*}
respectively, where $\mb$ is the graph incidence matrix. As used in previous work, these problems admit `cost approximators': linear operators which approximate the optimal cost of the corresponding flow problem up to a polylogarithmic factor. The guarantee of these approximators is summarized below.
\newcommand{\fopt}{\mathsf{opt}}
\begin{lemma}[Lemma 8 from \cite{AssadiJJST22} and Theorem 4.4 from \cite{Sherman17}]
	\label{lemma:cong}
	Let $G$ be a graph with $n$ vertices, $m$ edges, and nonnegative edge weights $w$. Let $\fopt_p(d)$ denote the optimal value of the $\ell_p$-flow problem over $G$ with demands $d$:
	\[
	\fopt_p(d) = \min_{\mb f = d} \norm{\mw f}_p. 
	\]
	For $p \in \{1, \infty \}$, there exists an algorithm which computes a matrix $\mr \in \R^{K \times n}$ such that for parameters $\alpha, \beta, \gamma, K$, 
	\begin{itemize}
		\item For any $d \in \R^n$ with $\1^\top d = 0$, $\fopt_p(d) \leq \norm{\mr d}_p \leq \alpha \fopt_p(d)$.
		\item The matrix $\mr \mb$ has $O(m \beta)$ nonzero entries.
		\item The algorithm runs in $O(m \gamma)$ time and returns both $\mr$ and $\mr \mb$.
	\end{itemize}
	In addition, the parameters $\alpha, \beta,\gamma, K$ above can take the values $\alpha = \log^{O(1)} n$, $\beta = \log^{O(1)} n$, $\gamma = \log^{O(1)} n$, and $K = n \log^{O(1)} n$.
\end{lemma}
As shown in  \cite{AssadiJJST22}, we may compute $(1+\epsilon)$-multiplicative approximations the transshipment problem by solving problems of the form
\begin{equation}
	\label{eqn:l1}
	\min_{f \in \Delta^{2m}} \max_{y \in [-1,1]^K} t y^\top \ma^\top f - b^\top y
\end{equation}
to additive error $\epsilon t$, where $t \geq 0$ is a parameter, $b = \mr d$, and (where $\mw$ is a diagonal weight matrix)
\[
\ma = \begin{pmatrix}
	\mw^{-1} \mb^\top \mr^\top \\
	- \mw^{-1} \mb^\top \mr^\top 
\end{pmatrix}.
\]
Applying \Cref{lemma:cong} with $p=1$ to compute $\mr$, we see that $t \norm{\ma^\top}_{1 \to 1} \leq t \alpha$: employing \Cref{thm:boxsimplex} gives an algorithm for this task which uses $O(\frac{\alpha \log n}{\epsilon})$ matrix-vector products with $\ma, \ma^\top, |\ma|,$ and $|\ma|^\top$. Similarly, \cite{Sherman17} obtains $(1+\epsilon)$-approximate solutions to maximum flow by solving 
\[
\min_{f \in [-1,1]^m} \max_{y \in \Delta^{2K}} t y^\top \ma f - b^\top y
\]
to additive error $\epsilon t$, where again $b = \mr d$ and 
\[
\ma = \begin{pmatrix}
	\mr \mb \mw^{-1} \\
	- \mr \mb \mw^{-1}
\end{pmatrix}.
\]
Applying \Cref{lemma:cong} with $p=\infty$ to compute $\mr$, we see that $t \norm{\ma^\top}_{1 \to 1} \leq t \alpha$: employing \Cref{thm:boxsimplex} gives an algorithm for this task which uses $O(\frac{\alpha \log n}{\epsilon})$ matrix-vector products with $\ma, \ma^\top, |\ma|,$ and $|\ma|^\top$. Combining these subroutines with the outer-loops described in \cite{AssadiJJST22,Sherman17} leads to improved algorithms for these graph optimization problems. 

Finally, though it is outside the scope of this paper, our Algorithm~\ref{alg:boxsimplex} is efficiently parallelizable and using the techniques of \cite{AssadiJJST22}, it improves state-of-the-art (in some parameter regimes) semi-streaming pass complexities for maximum cardinality matching by a logarithmic factor. 
\section*{Acknowledgements}

We would like thank our long-term collaborators Yujia Jin and Aaron Sidford for many helpful discussions at earlier stages of this project, which improved our understanding of area convexity. We also thank Victor Reis for his collaboration on related ideas to this work in \cite{JambulapatiRT23}. Finally, we thank anonymous reviewers for their helpful suggestions in improving our presentation.

\addcontentsline{toc}{section}{References}
\bibliographystyle{alpha}
%
\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{vdBLL{\etalchar{+}}21}

\bibitem[AJJ{\etalchar{+}}22]{AssadiJJST22}
Sepehr Assadi, Arun Jambulapati, Yujia Jin, Aaron Sidford, and Kevin Tian.
\newblock Semi-streaming bipartite matching in fewer passes and optimal space.
\newblock In {\em Proceedings of the 2022 {ACM-SIAM} Symposium on Discrete
  Algorithms, {SODA} 2022}, pages 627--669. {SIAM}, 2022.

\bibitem[AK07]{AroraK07}
Sanjeev Arora and Satyen Kale.
\newblock A combinatorial, primal-dual approach to semidefinite programs.
\newblock In {\em Proceedings of the 39th Annual {ACM} Symposium on Theory of
  Computing, 2007}, pages 227--236, 2007.

\bibitem[AL17]{Allen-ZhuL17}
Zeyuan Allen{-}Zhu and Yuanzhi Li.
\newblock Follow the compressed leader: Faster online learning of eigenvectors
  and faster {MMWU}.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017}, pages 116--125, 2017.

\bibitem[ALdOW17]{Allen-ZhuLOW17}
Zeyuan Allen{-}Zhu, Yuanzhi Li, Rafael~Mendes de~Oliveira, and Avi Wigderson.
\newblock Much faster algorithms for matrix scaling.
\newblock In {\em 58th {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2017}, pages 890--901. {IEEE} Computer Society, 2017.

\bibitem[AP20]{AltschulerP20}
Jason~M. Altschuler and Pablo~A. Parrilo.
\newblock Approximating min-mean-cycle for low-diameter graphs in near-optimal
  time and memory.
\newblock {\em CoRR}, abs/2004.03114v1, 2020.

\bibitem[AW21]{AlmanW21}
Josh Alman and Virginia~Vassilevska Williams.
\newblock A refined laser method and faster matrix multiplication.
\newblock In {\em Proceedings of the 2021 {ACM-SIAM} Symposium on Discrete
  Algorithms, {SODA} 2021}, pages 522--539. {SIAM}, 2021.

\bibitem[AWR17]{AltschulerWR17}
Jason~M. Altschuler, Jonathan Weed, and Philippe Rigollet.
\newblock Near-linear time approximation algorithms for optimal transport via
  sinkhorn iteration.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 1964--1974,
  2017.

\bibitem[BBN13]{BaesBN13}
Michel Baes, Michael B{\"{u}}rgisser, and Arkadi Nemirovski.
\newblock A randomized mirror-prox method for solving structured large-scale
  matrix saddle-point problems.
\newblock {\em {SIAM} J. Optim.}, 23(2):934--962, 2013.

\bibitem[BBT17]{BauschkeBT17}
Heinz~H. Bauschke, J{\'{e}}r{\^{o}}me Bolte, and Marc Teboulle.
\newblock A descent lemma beyond lipschitz gradient continuity: First-order
  methods revisited and applications.
\newblock {\em Math. Oper. Res.}, 42(2):330--348, 2017.

\bibitem[Bha97]{Bhatia97}
Rajendra Bhatia.
\newblock {\em Positive Definite Matrices}.
\newblock Princeton University Press, 1997.

\bibitem[BJKS18]{BlanchetJKS18}
Jose~H. Blanchet, Arun Jambulapati, Carson Kent, and Aaron Sidford.
\newblock Towards optimal running times for optimal transport.
\newblock {\em CoRR}, abs/1810.07717, 2018.

\bibitem[BSW19]{BoobSW19}
Digvijay Boob, Saurabh Sawlani, and Di~Wang.
\newblock Faster width-dependent algorithm for mixed packing and covering lps.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019}, pages
  15253--15262, 2019.

\bibitem[CDG19]{ChengDG19}
Yu~Cheng, Ilias Diakonikolas, and Rong Ge.
\newblock High-dimensional robust mean estimation in nearly-linear time.
\newblock In {\em Proceedings of the Thirtieth Annual {ACM-SIAM} Symposium on
  Discrete Algorithms, {SODA} 2019}, pages 2755--2771. {SIAM}, 2019.

\bibitem[CDST19]{CarmonDST19}
Yair Carmon, John~C. Duchi, Aaron Sidford, and Kevin Tian.
\newblock A rank-1 sketch for matrix multiplicative weights.
\newblock In {\em Conference on Learning Theory, {COLT} 2019}, volume~99 of
  {\em Proceedings of Machine Learning Research}, pages 589--623. {PMLR}, 2019.

\bibitem[CG18]{ChengG18}
Yu~Cheng and Rong Ge.
\newblock Non-convex matrix completion against a semi-random adversary.
\newblock In {\em Conference On Learning Theory, {COLT} 2018}, volume~75 of
  {\em Proceedings of Machine Learning Research}, pages 1362--1394. {PMLR},
  2018.

\bibitem[Cho75]{Choi75}
Man-Duen Choi.
\newblock Completely positive linear maps on complex matrices.
\newblock {\em Linear Algebra and its Applications}, 10(3):285--290, 1975.

\bibitem[CKL{\etalchar{+}}22]{ChenKLPGS22}
Li~Chen, Rasmus Kyng, Yang~P. Liu, Richard Peng, Maximilian~Probst Gutenberg,
  and Sushant Sachdeva.
\newblock Maximum flow and minimum-cost flow in almost-linear time.
\newblock In {\em 63rd {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2022}, pages 612--623. {IEEE}, 2022.

\bibitem[CLS21]{cohen2021solving}
Michael~B Cohen, Yin~Tat Lee, and Zhao Song.
\newblock Solving linear programs in the current matrix multiplication time.
\newblock {\em Journal of the ACM (JACM)}, 68(1):1--39, 2021.

\bibitem[CMTV17]{CohenMTV17}
Michael~B. Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu.
\newblock Matrix scaling and balancing via box constrained newton's method and
  interior point methods.
\newblock In {\em 58th {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2017}, pages 902--913. {IEEE} Computer Society, 2017.

\bibitem[CMY20]{CherapanamjeriM20}
Yeshwanth Cherapanamjeri, Sidhanth Mohanty, and Morris Yau.
\newblock List decodable mean estimation in nearly linear time.
\newblock In {\em 61st {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2020}, pages 141--148. {IEEE}, 2020.

\bibitem[CST21]{CohenST21}
Michael~B. Cohen, Aaron Sidford, and Kevin Tian.
\newblock Relative lipschitzness in extragradient methods and a direct recipe
  for acceleration.
\newblock In {\em 12th Innovations in Theoretical Computer Science Conference,
  {ITCS} 2021}, volume 185 of {\em LIPIcs}, pages 62:1--62:18. Schloss Dagstuhl
  - Leibniz-Zentrum f{\"{u}}r Informatik, 2021.

\bibitem[DG03]{DasguptaG03}
Sanjoy Dasgupta and Anupam Gupta.
\newblock An elementary proof of a theorem of johnson and lindenstrauss.
\newblock {\em Random Struct. Algorithms}, 22(1):60--65, 2003.

\bibitem[DGK18]{DvurechenskyGK18}
Pavel~E. Dvurechensky, Alexander~V. Gasnikov, and Alexey Kroshnin.
\newblock Computational optimal transport: Complexity by accelerated gradient
  descent is better than by sinkhorn's algorithm.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018}, volume~80 of {\em Proceedings of Machine Learning
  Research}, pages 1366--1375. {PMLR}, 2018.

\bibitem[DWZ22]{DuanWZ22}
Ran Duan, Hongxun Wu, and Renfei Zhou.
\newblock Faster matrix multiplication via asymmetric hashing.
\newblock {\em CoRR}, abs/2210.10173, 2022.

\bibitem[HJS{\etalchar{+}}22]{HuangJ0T022}
Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang.
\newblock Solving {SDP} faster: {A} robust {IPM} framework and efficient
  implementation.
\newblock In {\em 63rd {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2022}, pages 233--244. {IEEE}, 2022.

\bibitem[JJST22]{JambulapatiJST22}
Arun Jambulapati, Yujia Jin, Aaron Sidford, and Kevin Tian.
\newblock Regularized box-simplex games and dynamic decremental bipartite
  matching.
\newblock In {\em 49th International Colloquium on Automata, Languages, and
  Programming, {ICALP} 2022}, volume 229 of {\em LIPIcs}, pages 77:1--77:20.
  Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik, 2022.

\bibitem[JKL{\etalchar{+}}20]{JiangKLP020}
Haotian Jiang, Tarun Kathuria, Yin~Tat Lee, Swati Padmanabhan, and Zhao Song.
\newblock A faster interior point method for semidefinite programming.
\newblock In {\em 61st {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2020}, pages 910--918, 2020.

\bibitem[JLM{\etalchar{+}}21]{JambulapatiLMST21}
Arun Jambulapati, Jerry Li, Christopher Musco, Aaron Sidford, and Kevin Tian.
\newblock Fast and near-optimal diagonal preconditioning.
\newblock {\em CoRR}, abs/2008.01722, 2021.

\bibitem[JLT20]{Jambulapati0T20}
Arun Jambulapati, Jerry Li, and Kevin Tian.
\newblock Robust sub-gaussian principal component analysis and
  width-independent schatten packing.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.

\bibitem[JRT23]{JambulapatiRT23}
Arun Jambulapati, Victor Reis, and Kevin Tian.
\newblock Linear-sized sparsifiers via near-linear time discrepancy theory.
\newblock {\em CoRR}, abs/2305.08434, 2023.

\bibitem[JST19]{JambulapatiST19}
Arun Jambulapati, Aaron Sidford, and Kevin Tian.
\newblock A direct $\tilde{O}(1/\epsilon)$ iteration parallel algorithm for
  optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019}, pages
  11355--11366, 2019.

\bibitem[KLOS14]{KelnerLOS14}
Jonathan~A. Kelner, Yin~Tat Lee, Lorenzo Orecchia, and Aaron Sidford.
\newblock An almost-linear-time algorithm for approximate max flow in
  undirected graphs, and its multicommodity generalizations.
\newblock In {\em Proceedings of the Twenty-Fifth Annual {ACM-SIAM} Symposium
  on Discrete Algorithms, {SODA} 2014}, pages 217--226. {SIAM}, 2014.

\bibitem[KPT{\etalchar{+}}17]{KolouriPTSR17}
Soheil Kolouri, Se~Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo~K
  Rohde.
\newblock Optimal mass transport: signal processing and machine-learning
  applications.
\newblock {\em IEEE Signal Processing Magazine}, 34(4):43--59, 2017.

\bibitem[KV05]{KalaiV05}
Adam~Tauman Kalai and Santosh~S. Vempala.
\newblock Efficient algorithms for online decision problems.
\newblock {\em J. Comput. Syst. Sci.}, 71(3):291--307, 2005.

\bibitem[Lew96]{Lewis96}
Adrian Lewis.
\newblock Convex analysis on the hermitian matrices.
\newblock {\em SIAM Journal on Optimization}, 6(0):164--177, 1996.

\bibitem[LFN18]{LuFN18}
Haihao Lu, Robert~M. Freund, and Yurii~E. Nesterov.
\newblock Relatively smooth convex optimization by first-order methods, and
  applications.
\newblock {\em {SIAM} J. Optim.}, 28(1):333--354, 2018.

\bibitem[LHJ19]{LinHJ19}
Tianyi Lin, Nhat Ho, and Michael~I. Jordan.
\newblock On efficient optimal transport: An analysis of greedy and accelerated
  mirror descent algorithms.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 3982--3991. {PMLR}, 2019.

\bibitem[LHJ22]{LinHJ22}
Tianyi Lin, Nhat Ho, and Michael~I. Jordan.
\newblock On the efficiency of entropic regularized algorithms for optimal
  transport.
\newblock {\em Journal of Machine Learning Research}, 23(137):1--42, 2022.

\bibitem[Lin75]{Lindblad75}
G\"oran Lindblad.
\newblock Completely positive maps and entropy inequalities.
\newblock {\em Commun.\ Math.\ Phys.}, 40:147--151, 1975.

\bibitem[LMR19]{LahnMR19}
Nathaniel Lahn, Deepika Mulchandani, and Sharath Raghvendra.
\newblock A graph theoretic additive approximation of optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019}, pages
  13813--13823, 2019.

\bibitem[LS01]{LewisS01}
Adrian Lewis and Hristo~S. Sendov.
\newblock Twice differentiable spectral functions.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  23(0):368--386, 2001.

\bibitem[LS15]{LeeS15}
Yin~Tat Lee and Aaron Sidford.
\newblock Efficient inverse maintenance and faster algorithms for linear
  programming.
\newblock In Venkatesan Guruswami, editor, {\em {IEEE} 56th Annual Symposium on
  Foundations of Computer Science, {FOCS} 2015}, pages 230--249. {IEEE}
  Computer Society, 2015.

\bibitem[LS17]{Lee017}
Yin~Tat Lee and He~Sun.
\newblock An sdp-based algorithm for linear-sized spectral sparsification.
\newblock In {\em Proceedings of the 49th Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC} 2017}, pages 678--687. {ACM}, 2017.

\bibitem[MM15]{MuscoM15}
Cameron Musco and Christopher Musco.
\newblock Randomized block krylov methods for stronger and faster approximate
  singular value decomposition.
\newblock In {\em Advances in Neural Information Processing Systems 28: Annual
  Conference on Neural Information Processing Systems 2015}, pages 1396--1404,
  2015.

\bibitem[Nem04]{Nemirovski04}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence o(1/t) for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em {SIAM} Journal on Optimization}, 15(1):229--251, 2004.

\bibitem[Nes05]{Nesterov05}
Yurii Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Math. Program.}, 103(1):127--152, 2005.

\bibitem[Nes07]{Nesterov07}
Yurii Nesterov.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock {\em Math. Program.}, 109(2-3):319--344, 2007.

\bibitem[PC19]{PeyreC19}
Gabriel Peyr{\'{e}} and Marco Cuturi.
\newblock Computational optimal transport.
\newblock {\em Found. Trends Mach. Learn.}, 11(5-6):355--607, 2019.

\bibitem[PTZ16]{PengTZ16}
Richard Peng, Kanat Tangwongsan, and Peng Zhang.
\newblock Faster and simpler width-independent parallel algorithms for positive
  semidefinite programming.
\newblock {\em CoRR}, abs/1201.5135, 2016.

\bibitem[Qua20]{Quanrud20}
Kent Quanrud.
\newblock Nearly linear time approximations for mixed packing and covering
  problems without data structures or randomization.
\newblock In {\em 3rd Symposium on Simplicity in Algorithms, {SOSA} 2020},
  pages 69--80. {SIAM}, 2020.

\bibitem[She17]{Sherman17}
Jonah Sherman.
\newblock Area-convexity, $\ell_\infty$ regularization, and undirected
  multicommodity flow.
\newblock In {\em Proceedings of the 49th Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC} 2017}, pages 452--460. {ACM}, 2017.

\bibitem[ST18]{SidfordT18}
Aaron Sidford and Kevin Tian.
\newblock Coordinate methods for accelerating $\ell_\infty$ regression and
  faster approximate maximum flow.
\newblock In Mikkel Thorup, editor, {\em 59th {IEEE} Annual Symposium on
  Foundations of Computer Science, {FOCS} 2018}, pages 922--933. {IEEE}
  Computer Society, 2018.

\bibitem[SV14]{SachdevaV14}
Sushant Sachdeva and Nisheeth~K. Vishnoi.
\newblock Faster algorithms via approximation theory.
\newblock {\em Found. Trends Theor. Comput. Sci.}, 9(2):125--210, 2014.

\bibitem[vdBLL{\etalchar{+}}21]{BrandLLSSSW21}
Jan van~den Brand, Yin~Tat Lee, Yang~P. Liu, Thatchaphol Saranurak, Aaron
  Sidford, Zhao Song, and Di~Wang.
\newblock Minimum cost flows, mdps, and $\ell_1$-regression in nearly linear
  time for dense instances.
\newblock In {\em Proceedings of the 53rd Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC} 2021}, pages 859--869, 2021.

\bibitem[WK06]{WarmuthK06}
Manfred~K. Warmuth and Dima Kuzmin.
\newblock Randomized {PCA} algorithms with regret bounds that are logarithmic
  in the dimension.
\newblock In {\em Advances in Neural Information Processing Systems 19,
  Proceedings of the Twentieth Annual Conference on Neural Information
  Processing Systems, 2006}, pages 1481--1488, 2006.

\bibitem[Yu13]{Yu13}
Yao-Liang Yu.
\newblock The strong convexity of von neumann's entropy.
\newblock http://www.cs.cmu.edu/~yaoliang/mynotes/sc.pdf, 2013.
\newblock Accessed: 2023-01-14.

\bibitem[ZLO16]{Allen-ZhuLO16}
Zeyuan~Allen Zhu, Yin~Tat Lee, and Lorenzo Orecchia.
\newblock Using optimization to obtain a width-independent, parallel, simpler,
  and faster positive {SDP} solver.
\newblock In {\em Proceedings of the Twenty-Seventh Annual {ACM-SIAM} Symposium
  on Discrete Algorithms, {SODA} 2016}, pages 1824--1831. {SIAM}, 2016.

\bibitem[ZO15]{ZhuO15}
Zeyuan~Allen Zhu and Lorenzo Orecchia.
\newblock Nearly-linear time positive {LP} solver with faster convergence rate.
\newblock In {\em Proceedings of the Forty-Seventh Annual {ACM} on Symposium on
  Theory of Computing, {STOC} 2015}, pages 229--236. {ACM}, 2015.

\bibitem[ZP96]{ZwickP96}
Uri Zwick and Mike Paterson.
\newblock The complexity of mean payoff games on graphs.
\newblock {\em Theor. Comput. Sci.}, 158(1{\&}2):343--359, 1996.

\end{thebibliography}
 %

\newpage
\begin{appendix}
%
%
\section{Box-simplex proximal subproblems via relative conditioning}\label{app:rc_subproblem_solve}

In this section, we demonstrate the implications of Lemma~\ref{lem:relativesmooth} for solving the subproblems in extragradient methods for \eqref{eq:boxsimplex} using area convex regularizers. We reproduce Lemma~\ref{lem:relativesmooth} for convenience.

\restaterelativesmooth*

Consider a subproblem encountered when running the extragradient method of Appendix~\ref{app:xgrad_convergence} on the problem \eqref{eq:boxsimplex}, using the regularizer $r_\ma^{(\alpha)}$ in \eqref{eq:rlpdef}. It has the form, for some $(g\x, g\y) \in \xset^* \times \yset^*$,
\[\min_{x \in [-1, 1]^n, y \in \Delta^d} F(x, y) \defeq \inprod{g\x}{x} + \inprod{g\y}{y} + \inprod{|\ma|y}{x^2} + \alpha h(y).\]
Recall from Lemma~\ref{lem:rlpfacts} that $F$ is jointly convex over $(x, y)$ for any $\alpha \ge \half$. Hence, for $\alpha = 2$, we may apply Lemma~\ref{lem:relativesmooth} with $r(y) \defeq 2h(y)$ to conclude that
\[f(y) \defeq \min_{x \in [-1, 1]^n} F(x, y)\]
is $2$-relatively smooth with respect to $h$, as a function over $\yset = \Delta^d$. Moreover, applying Lemma~\ref{lem:relativesmooth} with $q(y) \defeq h(y)$, and again using the joint convexity fact in Lemma~\ref{lem:rlpfacts}, shows that $f$ is further $1$-relatively strongly convex with respect to $h$. At this point, a direct application of Theorem 3.1 in \cite{LuFN18} (which gives an algorithm for optimization under relative smoothness and strong convexity) yields a linearly-convergent algorithm for minimizing $F$. We remark that in light of Lemma~\ref{lem:partialderiv}, we can implement gradient queries to $f$ by computing the best response argument for a given $y$.

Interestingly, the analysis in this section used nothing more than Lemma~\ref{lem:relativesmooth}, joint convexity of our regularizer, and the ability to tune the parameter $\alpha$ to induce a small amount of relative strong convexity. This is in contrast to the analysis in \cite{Sherman17} (see Lemma 5 of \cite{JambulapatiST19} for a formal proof), which requires ad hoc multiplicative stability properties. An important consequence of this observation is that the same technique generalizes to the matrix setting via new joint convexity facts we prove in Proposition~\ref{prop:regularizer}, where multiplicative stability breaks due to non-monotonicity of the matrix exponential. This gives a simple proof-of-concept solver for matching Theorem~\ref{thm:boxspectraplex} up to logarithmic factors, which we improve via our approximation-tolerant extragradient methods. %
\section{Unified extragradient convergence analysis}\label{app:xgrad_convergence}

In this section, we show that our notion of relaxed relative Lipschitzness (Definition~\ref{def:rrl}) extends area convexity and relative Lipschitzness, and demonstrate that it enables convergence of an extragradient method for variational inequalities. We begin by comparing these conditions.

\begin{restatable}{lemma}{rrlextends}
Assume either of the following holds for operator $g: \zset \to \zset^*$ and convex $r: \zset \to \R$.
\begin{itemize}
    \item $g$ is $\frac 1 \eta$-relatively Lipschitz with respect to $r$.
    \item $g$ is $\eta$-area convex with respect to $r$.
\end{itemize}
Then $g$ is $\frac 1 \eta$-relaxed relatively Lipschitz (Definition~\ref{def:rrl}) with respect to $r$.
\label{lemma:rrl_extends}
\end{restatable}

\begin{proof}
The first case follows immediately from Definition~\ref{def:rl}: for any  $(z, z', z^+) \in \zset \times \zset \times \zset$,
\[\eta \inprod{g(z') - g(z)}{z' - z^+} \le V^r_z(z') + V^r_{z'}(z^+)  \le V^r_z(z') + V^r_{z'}(z^+) + V^r_z(z^+).\]
For the second case, direct computation and nonnegativity of the Bregman divergence yields
\begin{align*}
r(z) + r(z') + r(z^+) - 3r(c) = V^r_z(z^+) + V^r_z(z') - 3V^r_z(c) 
\le V^r_z(z') + V^r_z(z^+) + V^r_{z'}(z^+).
\end{align*}
\end{proof}

With this fact in hand, we now show that the more general condition of relaxed relative Lipschitzness is sufficient to prove convergence of a simple extragradient method. In particular, we analyze \Cref{alg:rrl_mp}, a slight variant of the mirror prox algorithm \cite{Nemirovski04, CohenST21}, and show that it gives rates for relaxed relative Lipschitz monotone variational inequalities that match the rates for relative Lipschitz or area convex operator-regularizer pairs up to constant factors. We will require the definition of a proximal oracle, which takes the standard iteration for mirror descent algorithms.

\begin{definition}[Proximal oracle]
For convex $r : \zset \to \R$, $z \in \zset$, and $g \in \zset^*$, $\Prox_z^r(g)$ outputs
\[
z' = \arg\min_{w \in \zset} \inprod{g}{w} + V^r_z(w). 
\]
\label{def:prox}
\end{definition}

\begin{algorithm2e}[H]
	\caption{\textsf{RelaxedMirrorProx}($g, r, z_0,  \eta, T$)}
	\label{alg:rrl_mp}
	\DontPrintSemicolon
	{\bfseries Input:} Operator $g : \zset \to \zset^*$ satisfying $\frac 1 \eta$ relaxed relative Lipschitzness with respect to $r$, $T \in \N$, $z_0 \in \zset$ \;
	\For{$t=0$ {\bfseries{\textup{to}}} $T-1$}{
		$w_t = \Prox_{z_t}^r(\eta g(z_t))$ \;
		$z_{t+1} = \Prox_{z_t}^r \left( \frac{\eta}{2} g(w_t) \right)$\label{line:xgradstep_rrl} \;
	}
\end{algorithm2e}

We note that Algorithm~\ref{alg:rrl_mp} is exactly the same as Algorithm 1 of \cite{CohenST21} (a rephrasing of the main result of \cite{Nemirovski04}), except there is a factor of $2$ in the step size in Line~\ref{line:xgradstep_rrl}. As the proof of Proposition~\ref{prop:rrl_mp} shows, this allows us to obtain an extra divergence term which is handled by the relaxed relative Lipschitzness condition. Notably, this extra divergence is also handled by operator-regularizer pairs satisfying area convexity, explaining the same step size change appearing in \cite{Sherman17}.

\begin{proposition}
Let $g : \zset \to \zset^*$, $r : \zset \to \R$, $z_0 \in \zset$, and assume that $g$ satisfies $\frac 1 \eta$-relaxed relative Lipschitzness with respect to $r$ for some $\eta > 0$. The iterates of Algorithm~\ref{alg:rrl_mp} satisfy (for any $u \in \zset$),
\[
\frac{1}{T} \sum_{t=0}^{T-1} \inprod{ g(w_t)}{w_t - u} \leq \frac{2}{\eta T} V_{z_0}^r(u).
\]
\label{prop:rrl_mp}
\end{proposition}
\begin{proof}
Applying \eqref{eq:threepoint} to the optimality conditions implied by the steps of \Cref{alg:rrl_mp}, we obtain 
\[
\eta \l g(z_t) , w_t - z_{t+1} \r \leq V_{z_t}^r(z_{t+1}) - V^r_{w_t}(z_{t+1}) - V^r_{z_t}(w_t)
\]
and 
\[
\frac{\eta}{2} \l g(w_t), z_{t+1} - u \r \leq V^r_{z_t}(u) - V^r_{z_{t+1}}(u) - V^r_{z_t}(z_{t+1}). 
\]
Doubling the second inequality and adding it to the first, we have 
\[
\eta \left( \l g(w_t), z_{t+1} - u \r + \l g(z_t) , w_t - z_{t+1} \r \right) \leq 2 V^r_{z_t}(u) - 2 V^r_{z_{t+1}}(u) - V_{z_t}^r(z_{t+1}) - V^r_{w_t}(z_{t+1}) - V^r_{z_t}(w_t).
\]
Rearranging the above yields 
\begin{align*}
\eta \l g(w_t), w_t - u \r &\leq 2 V^r_{z_t}(u) - 2 V^r_{z_{t+1}}(u) \\
&- V_{z_t}^r(z_{t+1}) - V^r_{w_t}(z_{t+1}) - V^r_{z_t}(w_t)+ \eta \l g(w_t) - g(z_t), w_t - z_{t+1} \r \\
&\leq 2 V^r_{z_t}(u) - 2 V^r_{z_{t+1}}(u) 
\end{align*}
where the inequality used relaxed relative Lipschitzness of $g$. Summing over all $T$ iterations and dividing by $\eta T$ then gives the desired
\[
\frac{1}{T} \sum_{t=0}^{T-1} \l g(w_t), w_t - u \r \leq \frac{2}{\eta T} \left( V_{z_0}^r(u) - V_{z_T}^r(u) \right) \leq \frac{2}{\eta T}V_{z_0}^r(u).
\]
\end{proof}

We briefly compare Proposition~\ref{prop:rrl_mp} to the analyses of extragradient methods considered in \cite{Sherman17, CohenST21}. The extragradient method considered in \cite{Sherman17} can be viewed as a dual variant of Algorithm~\ref{alg:rrl_mp}, and is based on dual extrapolation \cite{Nesterov07} instead of mirror prox (see discussion in \cite{CohenST21} for their relationship); the latter is the skeleton of our Algorithm~\ref{alg:rrl_mp}. This affirmatively answers the question of whether there is a mirror prox-like algorithm which converges under area convexity, which to our knowledge was previously not known. The algorithm in \cite{Sherman17} obtains the same regret guarantee as in Proposition~\ref{prop:rrl_mp}, also calling $O(1)$ proximal oracles in $r$ per iteration. 

On the other hand, the mirror prox algorithm of \cite{Nemirovski04, CohenST21} run for $T$ iterations run on a $\frac{1}{\eta}$-relatively Lipschitz operator-regularizer pair yields 
\[
\sum_{t=0}^{T-1} \l g(w_t), w_t - u \r \leq \frac{1}{\eta T} V^r_{z_0}(u)
\]
for any $u \in \zset$, where again each iteration requires $O(1)$ calls to a proximal oracle \Cref{def:prox}. This result therefore improves Proposition~\ref{prop:rrl_mp}'s convergence rate by a factor of $2$, at the cost of using the stronger Definition~\ref{def:rl}. Finally, we note that exact implementations of the proximal oracles required by Algorithm~\ref{alg:rrl_mp} satisfy the oracles used in Sections~\ref{sec:noaltmin} and~\ref{sec:sdpfacts}, i.e.\ those in Definitions~\ref{def:Ograd} and~\ref{def:Oxgrad}, with $\beta = 0$, as seen by applying \eqref{eq:threepoint}. To handle error introduced by not being able to exactly implement a proximal oracle for our regularizers \eqref{eq:rlpdef} and \eqref{eq:rsdpdef}, we relax our method to handle $\beta > 0$, and we satisfy the required relaxations via relative conditioning properties of the proximal subproblems. %
\section{Proof of Lemma~\ref{lem:vnentropy_sc}}\label{app:regularizer}

In this section, we provide a proof of Lemma~\ref{lem:vnentropy_sc}. We begin by recalling a technical claim on matrix relative entropy. We require the notion of \emph{positive} maps on matrices.

\begin{definition}
Let $\Phi : \R^{d \times d} \to \R^{n \times n}$ be a linear function on matrices. We say $\Phi$ is \emph{positive} if $\Phi(\ma) \succeq 0$ for any $\ma \in \PSD^d$. In addition, for any $k \geq 1$ define the (linear) map $\Phi_k : \R^{kd \times kd } \to \R^{kn \times kn}$ which sends 
\[
\ma = \begin{pmatrix}
\ma_{1,1} & \ma_{1,2} & \dots & \ma_{1,k} \\
\ma_{2,1} & \ma_{2,2} & \dots & \ma_{2,k} \\
\vdots & \vdots & \ddots & \vdots \\
\ma_{k,1} & \ma_{k,2} & \dots & \ma_{k,k}
\end{pmatrix}
\]
to 
\[
\Phi_k(\ma) = \begin{pmatrix}
\Phi(\ma_{1,1}) & \Phi(\ma_{1,2}) & \dots & \Phi(\ma_{1,k}) \\
\Phi(\ma_{2,1}) & \Phi(\ma_{2,2}) & \dots & \Phi(\ma_{2,k}) \\
\vdots & \vdots & \ddots & \vdots \\
\Phi(\ma_{k,1}) & \Phi(\ma_{k,2}) & \dots & \Phi(\ma_{k,k})
\end{pmatrix}.
\]
We say $\Phi$ is $k$-positive if $\Phi_k$ is positive. We say $\Phi$ is \emph{completely positive} if it is $k$-positive for any $k \geq 1$. 
\end{definition}
We additionally require the following equivalent characterization of completely positive maps. 

\begin{theorem}[Choi-Kraus Representation Theorem, \cite{Choi75}]
\label{thm:choikraus}
Let $\Phi : \R^{d \times d} \to \R^{n \times n}$ be a linear map on matrices. Then it is completely positive if and only if there exist $\mv_1, \mv_2 \dots \mv_k \in \C^{n \times d}$ such that 
\[
\Phi(\ma) = \sum_{i=1}^k \mv_i \ma \mv_i^\dagger
\]
where $\mv_i^\dagger$ denotes the Hermitian transpose.
\end{theorem}

We refer the reader to Chapter 3 of \cite{Bhatia97} for more discusion of this and related results. With this notion, we recall a useful fact concerning matrix relative entropy and completely positive maps. 
\begin{lemma}[Theorem, \cite{Lindblad75}]
\label{lemma:entropy_ineq}
Let $\Phi : \R^{d \times d} \to \R^{n \times n}$ be a completely positive map on matrices such that $\Tr(\ma) = \Tr(\Phi(\ma))$ for any $\ma \in \R^{d \times d}$. Then for any $\ma, \mb \in \PD^{d}$, 
\[
V^H_{\mb}(\ma) \geq V^H_{\Phi(\mb)}(\Phi(\ma)). 
\]
\end{lemma}
\iffalse
\begin{proof}
By \Cref{thm:choikraus}, there exists matrices $\mv_1, \dots \mv_k \in \C^{n \times d}$ such that $\Phi(\ma) = \sum_{i=1}^k \mv_i \ma \mv_i^\dagger$. Observe that 
\[
\Tr(\ma) = \Tr(\Phi(\ma)) = \Tr \left( \sum_{i=1}^k \mv_i \ma \mv_i^\dagger \right) = \Tr \left( \sum_{i=1}^k \mv_i^\dagger \mv_i \ma \right) =  \inprod{ \sum_{i=1}^k \mv_i^\dagger \mv_i  } { \ma }
\]
holds for all matrices $\ma$: we conclude $\sum_{i=1}^k \mv_i^\dagger \mv_i = \id$. If we define the 
\end{proof} 
\fi
 We refer the reader to \cite{Yu13, Bhatia97} for further discussion of this result. With these facts in hand, we are ready to prove \Cref{lem:vnentropy_sc}: we recall it for convenience below.

\vnentropysc*

\begin{proof}
Define the map $\Phi: \R^{d \times d} \to \R^{n \times n}$ by $\Phi(\my) \defeq \diag{\alla(\my)}$. We begin by showing that $\Phi$ is trace-preserving: for any $\my \in \R^{d \times d}$, 
\[
\Tr \left( \Phi(\my) \right) = \Tr \left( \diag{\alla(\my)} \right) = \sum_{i=1}^n \inprod{\ma_i}{\my} = \inprod{\id_d}{\my} = \Tr (\my).
\]
We now show that $\Phi$ is completely positive. As each $\ma_i$ is positive semidefinite, there exist vectors $v_{i,1}, v_{i,2}, \dots v_{i,d}$ such that $\ma_i = \sum_{j=1}^d v_{i,j} v_{i,j}^\top $. Given these vectors for every $\ma_i$, we define the matrices $\mv_{i,j} \in \R^{n \times d}$ as  $\mv_{i,j} = e_i v_{i,j}^\top$. 
We claim that 
\[
\Phi(\my) = \sum_{i=1}^n \sum_{j=1}^d \mv_{i,j} \my \mv_{i,j}^\top.
\]
To see this, note that 
\[
\sum_{j=1}^d \mv_{i,j} \my \mv_{i,j}^\top = \sum_{j=1}^d e_i (v_{i,j}^\top \my v_{i,j} ) e_i^\top = \left( \inprod{\sum_{j=1}^d v_{i,j} v_{i,j}^\top}{\my} \right) e_i e_i^\top = \inprod{\ma_i}{ \my}  e_i e_i^\top
\]
and therefore 
\[
\sum_{i=1}^n \sum_{j=1}^d \mv_{i,j} \my \mv_{i,j}^\top = \sum_{i=1}^n \inprod{\ma_i}{\my} e_i e_i^\top = \diag{\alla(\my)} = \Phi(\my). 
\]
By \Cref{thm:choikraus}, the existence of these $\mv_{i,j}$ implies that $\Phi$ is completely positive. Now $\Phi$ satisfies the necessary conditions for \Cref{lemma:entropy_ineq}: we therefore have for any $\ma, \mb \in \PD^d$
\[
V^H_\mb(\ma) \geq V^H_{\Phi(\mb)}(\Phi(\ma))
\]
Now, for any $\my \in \PD^d$ and $\mm \in \Sym^d$, there exists a constant $\alpha$ such that $\my + t \mm \in \PD^d$ for all $|t| \leq \alpha$. Therefore, for any such $t$ we have
\[V^H_{\my}(\my + t\mm) \ge V^H_{\Phi(\my)}(\Phi(\my + t\mm)).\]
Taking the limit of $t \to 0$, a second-order Taylor expansion yields
\begin{align*}
\nabla^2 H(\my)[\mm, \mm] &= \frac 2 {t^2} \lim_{t \to 0} V^H_{\my}(\my + t\mm), \\
\nabla^2 h(y)[m, m] &= \frac 2 {t^2} \lim_{t \to 0} V^h_{y}(y + tm) \\
&= \frac 2 {t^2} \lim_{t \to 0} V^H_{\diag{y}}(\diag{y + tm}) = \frac 2 {t^2} \lim_{t \to 0} V^H_{\Phi(\my)}(\Phi(\my + t\mm)).
\end{align*}
The second-to-last equality uses Theorem 3.3 of \cite{LewisS01}, a formula for the gradient of a spectral function (a function from matrices to scalars depending only on the eigenvalues), and the fact that $\Phi(\my)$ and $\Phi(\my + t\mm)$ commute. Combining the above displays shows the desired claim.
\end{proof}
 %
\section{Matrix exponential products}\label{sec:matexpvec}

In this section we provide a collection of tools used for approximate computations against a matrix exponential. We begin by providing an approximation to its trace, using some helper tools.

\begin{proposition}[Approximate top eigenvalue, Theorem 1, \cite{MuscoM15}]\label{prop:topeig}
Let $\mm \in \PSD^d$, and let $\delta, \eps \in (0, 1)$. There is an algorithm which with probability $\ge 1 - \delta$ returns a value $\hlam$ such that $|\hlam - \lam_{\max}(\mm)| \le \eps \lam_{\max}(\mm)$, where $\lam_{\max}$ is the largest eigenvalue of a matrix in $\Sym^d$, in time
\[O\Par{\tmv(\mm) \cdot \frac{\log \frac d {\delta\eps}}{\sqrt \eps}}.\]
\end{proposition}

\begin{proposition}[Polynomial approximation to $\exp$, Theorem 4.1, \cite{SachdevaV14}]\label{prop:polyexp}
Let $\mm \in \PSD^d$ have $\mm \preceq R \id$, and let $\eps \in (0, 1)$. There is a polynomial $p$ satisfying
\[\exp(-\mm) - \eps \id \preceq p(\mm) \preceq \exp(-\mm) + \eps \id,\text{ } \textup{degree}(p) = O\Par{\sqrt{R\log \frac 1 \eps} + \log \frac 1 \eps}.\]
\end{proposition}

\begin{proposition}[Johnson-Lindenstrauss transform, Theorem 2.1, \cite{DasguptaG03}]\label{prop:jl}
Let $\mq \in \R^{k \times d}$ have rows formed by independently random unit vectors in $\R^d$ scaled by $k^{-\half}$, and let $\eps, \delta \in (0, 1)$. If $k = \Omega(\frac 1 {\eps^2} \log \frac d {\delta})$ for an appropriate constant, for any $v \in \R^d$ (independent of $\mq$), with probability $\ge 1 - \delta$,
\[(1-\eps)\norm{v}_2^2 \le \norm{\mq v}_2^2 \le  (1+ \eps) \norm{v}_2^2.\]
\end{proposition}

\paragraph{Approximating the exponential trace.} We first show how to use these tools to approximate the trace of the exponential of a bounded matrix efficiently.

\begin{lemma}\label{lem:approxtrace}
Let $\eps, \delta \in (0, 1)$, let $R \ge \log \frac 1 \eps$, and let $\mm \in \Sym^d$ satisfy $\normop{\mm} \le R$. We can compute an $\eps$-multiplicative approximation to $\Tr \exp \mm$ with probability $\ge 1 - \delta$ in time 
\[O\Par{\tmv(\mm) \cdot \frac{\sqrt{R}\log^{1.5}( \frac {Rd} {\delta\eps})}{\eps^2}}.\]
\end{lemma}
\begin{proof}
Assume for simplicity $\eps \le \half$ as this affects the result by at most a constant. By applying Proposition~\ref{prop:topeig} to the matrix $\mm + 2R\id$ with $\eps \gets \frac 1 {3R}$, we obtain a value $\hlam$ such that $\lam_{\max}(\mm) \le \hlam \le \lam_{\max}(\mm) + 1$ with probability $\ge 1 - \frac \delta 2$, within the allotted time budget. Next, note that for
\[\tmm \defeq \hlam \id -\mm, \]
we have $\Tr \exp(\mm) = \exp(\hlam) \cdot \Tr \exp(-\tmm)$, so it suffices to approximate $\Tr \exp(-\tmm)$. We observe $\tmm \in \PSD^d$ and it has an eigenvalue at most $1$, by definition of $\hlam$. This latter fact implies $\Tr \exp(-\tmm) \ge \frac 1 e$. Next take $\mq$ from Proposition~\ref{prop:jl} with $k = O(\frac 1 {\eps^2} \log \frac d \delta)$ such that with probability $\ge 1 - \frac \delta 2$, for all $j \in [d]$ (by a union bound), we have
\[\Par{1 - \frac \eps 4} \norm{\Brack{\exp\Par{-\half \tmm}}_{j:}}_2^2 \le \norm{\mq \Brack{\exp\Par{-\half \tmm}}_{j:}}_2^2 \le \Par{1 + \frac \eps 4} \norm{\Brack{\exp\Par{-\half \tmm}}_{j:}}_2^2.\]
Union bounding over the success of the two randomized steps gives the failure probability. Since
\begin{align*}\Tr \exp(-\tmm) &= \sum_{j \in [d]} \norm{\Brack{\exp\Par{-\half \tmm}}_{j:}}_2^2,\\
\sum_{j \in [d]} \norm{\mq \Brack{\exp\Par{-\half \tmm}}_{j:}}_2^2 &= \Tr\Par{\exp\Par{-\half \tmm}\mq^\top \mq \exp\Par{-\half \tmm}} \\
&= \Tr\Par{\mq\exp(-\tmm)\mq^\top} = \sum_{\ell \in [k]} \norm{\exp\Par{-\half \tmm} \mq_{\ell:}}_2^2,
\end{align*}
combining the above two displays means it suffices to give an $\frac \eps {4ek}$-additive approximation to the squared norm of each $\exp(-\half \tmm) \mq_{\ell:}$. This would result in an overall $\frac \eps 4$-multiplicative loss in approximating $\Tr \exp(-\tmm)$, due to application of $\mq$, and an additional $\frac \eps {4e}$-additive approximation, which is also a $\frac \eps 4$-multiplicative approximation factor due to $\Tr \exp(-\tmm) \ge \frac 1 e$. 

Finally, recall that all $\norm{\mq_{\ell:}}_2^2 = \frac 1 k$, so applying Proposition~\ref{prop:polyexp} with error parameter $\frac \eps {4e}$, we have a polynomial $p$ of degree $\sqrt{R \log \frac 1 \eps}$ such that $p(\tmm)$ approximates $\exp(-\tmm)$ to an additive $\frac \eps {4e} \id$. Hence, the quadratic forms of $\mq_{\ell:}$ through $p(\tmm)$ approximate each
\[\norm{\exp\Par{-\half \tmm} \mq_{\ell:}}_2^2 = \mq_{\ell:}^\top \exp\Par{-\tmm} \mq_{\ell:}\]
to an additive $\frac \eps {4ek}$ as desired. The runtime of this step comes from computing all $k = O(\frac{1}{\eps^2} \log \frac d \delta)$ quadratic forms with a degree $O(\sqrt {R \log \frac 1 \eps})$ polynomial in $\mm$.

\end{proof}

\paragraph{Approximating an inner product.} Next, we build upon the proof of Lemma~\ref{lem:approxtrace} to approximate an inner product against the exponential of a bounded matrix.

\begin{lemma}\label{lem:approxinprod}
Let $\eps, \gamma, \delta \in (0, 1)$, $\{\ma_i\}_{i \in [n]} \subset \Sym^d$, $R \ge \log \frac 1 \gamma$, and let $\mm \in \Sym^d$ satisfy $\normop{\mm} \le R$. We can compute values $\{V_i\}_{i \in [n]}$ such that for all $i \in [n]$,
\[ \Abs{V_i - \inprod{\ma_i}{\exp(\mm)}} \le \eps\inprod{|\ma_i|}{\exp(\mm)} + \gamma\Tr(|\ma_i|)\Tr\exp(\mm) \]
with probability $\ge 1 - \delta$ in time
\[O\Par{\tmv(\mm) \cdot \frac{\sqrt R \log^{1.5}(\frac{Rnd}{\gamma\delta\eps}) }{\eps^2} + \Par{\sum_{i \in [n]}\tmv(\ma_i)} \cdot \frac{\log \frac {nd} \delta}{\eps^2}}.\]
\end{lemma}
\begin{proof}
We prove the result for a single $\ma_i$ with failure probability $\delta \gets \frac \delta n$ and then use a union bound to obtain the result. Denote $\ma \gets \ma_i$. As in the proof of Lemma~\ref{lem:approxtrace}, assume that $\eps \le \half$ and we have obtained $\hlam$ such that $\lam_{\max}(\mm) \le \hlam \le \lam_{\max}(\mm) + 1$ with probability $\ge 1 - \frac \delta 2$ within the allotted time, and define $\tmm \defeq \hlam \id - \mm$. Also, by the spectral theorem there exists $\mb \in \R^{d \times d}$ and a diagonal matrix $\md \in \Sym^d$ with diagonal entries in $\{\pm 1\}$ such that $\ma = \mb^\top \md \mb$ and $\mb^2 = |\ma|$. Assume for $\mq \in \R^{k \times n}$ with $k = O(\frac 1 {\eps^2} \log \frac d \delta)$ from Proposition~\ref{prop:jl}, that for all $j \in [d]$,
\begin{gather*} \Abs{\norm{\mq\Brack{\mb \exp\Par{-\half \tmm}}_{j:}}_2^2 - \norm{\Brack{\mb \exp\Par{-\half \tmm}}_{j:}}_2^2} \le \frac \eps {e}\norm{\Brack{\mb \exp\Par{-\half \tmm}}_{j:}}_2^2.
\end{gather*}
Conditioning on these events gives the failure probability. Observe that
\begin{equation}\label{eq:bigapprox1}
\begin{gathered}
\Abs{\inprod{\ma}{\exp(-\tmm)} - \Tr\Par{\mq \exp\Par{-\half \tmm} \ma \exp\Par{-\half \tmm} \mq}} \\
= \Abs{\Tr\Par{\exp\Par{-\half \tmm} \ma \exp\Par{-\half \tmm}} - \Tr\Par{\mq \exp\Par{-\half \tmm} \ma \exp\Par{-\half \tmm} \mq}} \\
\le \sum_{j \in [d]} \Abs{\norm{\mq\Brack{\mb \exp\Par{-\half \tmm}}_{j:}}_2^2 - \norm{\Brack{\mb \exp\Par{-\half \tmm}}_{j:}}_2^2} \\
\le \frac \eps {4e} \sum_{j \in [d]} \norm{\Brack{\mb \exp\Par{-\half \tmm}}_{j:}}_2^2 = \frac \eps {e} \inprod{|\ma|}{\exp(-\tmm)}.
\end{gathered}
\end{equation}
The first inequality above used the decomposition $\ma = \mb^\top \md \mb$ and the triangle inequality, and the second inequality used our assumption on $\mq$. 
Next, we apply Proposition~\ref{prop:polyexp} with accuracy parameter $\frac \gamma {3e}$ to obtain a polynomial $p$ of degree $O(\sqrt{R\log \frac 1 \gamma})$ such that for $\me \defeq p(\half \tmm) - \exp(-\half \tmm)$,
\[\normop{\me} \le \frac \gamma {3e}.\]
Let $q$ be some row of $\mq$ with $\norm{q}_2^2 = \frac 1 k$, and let $u = \me q$ so that $\norm{u}_2 \le \frac{\gamma}{3e\sqrt k}$. We bound
\begin{align*}
\Abs{q^\top p\Par{\half \tmm} \ma p\Par{\half \tmm}q - q^\top \exp\Par{-\half \tmm} \ma \exp\Par{-\half \tmm}q} &\le 2\Abs{q^\top \exp\Par{-\half \tmm}\ma u} \\
&+ \Abs{u^\top \ma u}.
\end{align*}
We further may upper bound
\begin{align*}
\Abs{u^\top \ma u} \le \norm{\mb u}_2^2,\; \Abs{q^\top\exp\Par{-\half \tmm} \ma u} \le \norm{\mb u}_2\norm{\mb \exp\Par{-\half \tmm}q}_2,
\end{align*}
by the triangle inequality and Cauchy-Schwarz inequality. By positive semidefiniteness of $\tmm$ we have $\|\exp(-\half \tmm) q\|_2 \le \norm{q}_2$, and hence
\begin{align*}
\norm{\mb \exp\Par{-\half\tmm} q}_2^2 &\le \inprod{qq^\top}{|\ma|} \le \frac 1 k \Tr |\ma|, \\
\norm{\mb u}_2^2 &= \inprod{uu^\top}{|\ma|} \le \frac {\gamma^2} {9e^2k} \Tr |\ma|.
\end{align*}
Combining the above three displays, and summing over each row of $\mq$, we obtain
\begin{equation}\label{eq:bigapprox2} 
\begin{gathered}
\Abs{\Tr\Par{\mq \exp\Par{-\half \tmm} \ma \exp\Par{-\half \tmm} \mq} - \Tr\Par{\mq p\Par{\half \tmm} \ma p\Par{\half \tmm} \mq}} 
\le \frac \gamma e \Tr |\ma|.
\end{gathered}
\end{equation}
We choose to return $V = \exp(\hlam) \Tr(\mq p(\half \tmm) \ma p(\half \tmm) \mq)$. The approximation guarantee follows from combining \eqref{eq:bigapprox1} and \eqref{eq:bigapprox2}, and multiplying by $\exp(\hlam) \le e\Tr(\exp(\mm))$. The runtime comes from applying $p(\half \tmm)$ to each row of $\mq$ first, and then computing $k$ quadratic forms through $\ma$. We note that the applications of $p(\half \tmm)$ to rows of $\mq$ can be precomputed, and reused for all $\{\ma_i\}_{i \in [n]}$.
\end{proof}

 \end{appendix}

\end{document}