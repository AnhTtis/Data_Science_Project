\newpage
\appendix

\section[Appendix A: Theory]{Theory}
\label{app:proofs}

\subsection{Proof of Theorem 1}
\label{app:proof_of_thmain}

\thmain*

\begin{proof}
To prove Theorem~\ref{th:main}, we follow the proof structure from
\citet[][Theorem~4.4]{Kuegelgen2021} and divide the proof into three steps.
First, we show that there exists a pair of smooth functions 
$\g_1^*, \g_2^*$ that attain the global minimum of $\SymAME$
(Eq.~\refeq{eq:sym-info-nce-estimand-def}). Further, in
Equations~(\refeq{eq:invariance-rel-step1}--\refeq{eq:hh2-unif}), we derive
invariance conditions that have to hold almost surely for any pair of smooth
functions $\g_1, \g_2$ attaining the global minimum of
Eq.~(\refeq{eq:sym-info-nce-estimand-def}).  In Step~2, we use the invariance
conditions derived in Step~1 to show by contradiction that \emph{any} pair of
smooth functions $\g_1, \g_2$ that attain the global minimum in
Eq.~(\refeq{eq:sym-info-nce-estimand-def}) can only depend on content and not
on style or modality-specific information.  In the third and final step, for
$\h_1 := \g_1 \circ \f_1$ and $\h_2 := \g_2 \circ \f_2$, we show that both
functions must be bijections and hence that $\c$ is block-identified by $\g_1$
and $\g_2$ respectively.

\paragraph{Step 1.} 
Recall the asymptotic form of the objective, as defined in
\Cref{eq:sym-info-nce-estimand-def}:
\footnotesize
\begin{equation}%
  \SymAME(\g_1,\g_2) = \mathbb{E}_{(\x_1,\x_2) \sim p_{\x_1,\x_2}} 
  \left[ \lVert \g_1(\x_1) - \g_2(\x_2) \rVert_2 \right] - \nicefrac{1}{2} 
  \left( H(\g_1(\x_1)) + H(\g_2(\x_2)) \right) \;. 
  \tag{\ref{eq:sym-info-nce-estimand-def}}
\end{equation}
\normalsize
The global minimum of $\SymAME$ is reached when the first term is minimized and
the second term is maximized. The first term is minimized when the encoders
$\g_1$ and $\g_2$ are perfectly aligned, i.e., when $\g_1(\x_1) = \g_2(\x_2)$
holds for all pairs $(\x_1, \x_2) \sim p_{\x_1, \x_2}$. The second term attains
its maximum when $\g_1$ and $\g_2$ map to a uniformly distributed random
variable on $(0,1)^{n_c}$ respectively.%
\footnote{%
  Note that we restrict the range of $\g_1$ and $\g_2$ to $(0,1)^{n_c}$ by
  definition merely to simplify the notation. Generally, the uniform
  distribution  $\mathcal{U}(a, b)$ is the maximum entropy distribution on the
  interval $[a, b]$.
}

To show that there \emph{exists} a pair of functions that minimize $\SymAME$,
let $\g_1^* := \bm{d}_1 \circ \f_{1,1:n_c}^{-1}$ and let $\g_2^* := \bm{d}_2
\circ \f_{2,1:n_c}^{-1}$, where the subscript $1{:}n_c$ indexes the subset of
content dimensions w.l.o.g.~and where $\bm{d}_1$ and $\bm{d}_2$ will be defined
using the Darmois construction~\citep{darmois:51:analyse,Hyvaerinen1999}.
First, recall that ${\f_1^{-1}(\x_1)_{1:n_c} = \c}$ and that
${\f_2^{-1}(\x_2)_{1:n_c} = \tc}$ by definition. Second, for 
$i \in \{ 1, 2 \}$, let us define $\bm{d}_i: \mathcal{C} \mapsto (0,1)^{n_c}$
using the Darmois construction, such that $\bm{d}_i$ maps $\c$ and $\tc$ to a
uniform random variable respectively. It follows that $\g_1^*, \g_2^*$ are
smooth functions, because any function $\bm{d}_i$ obtained via the Darmois
construction is smooth and $\f_1^{-1}, \f_2^{-1}$ are smooth as well (each
being the inverse of a smooth function).

Next, we show that the pair of functions $\g_1^*, \g_2^*$, as defined above,
attains the global minimum of the objective $\SymAME$. We have that
\footnotesize
\begin{align}
	\SymAME(\g_1^*,\g_2^*) &= \mathbb{E}_{(\x_1,\x_2) \sim p_{\x_1,\x_2}} 
    \left[ \lVert \g_1^*(\x_1) - \g_2^*(\x_2) \rVert_2 \right] - \nicefrac{1}{2} 
    \left( H(\g_1^*(\x_1)) + H(\g_2^*(\x_2)) \right) \\
	&= \mathbb{E}_{(\x_1,\x_2) \sim p_{\x_1,\x_2}} 
    \left[ \lVert \bm{d}_1(\c) - \bm{d}_2(\tc) \rVert_2 \right] - \nicefrac{1}{2} 
    \left( H(\bm{d}_1(\c)) + H(\bm{d}_2(\tc)) \right) \\
	&= 0 
\end{align}
\normalsize
where by Assumption~\ref{as:content}, $\c = \tc$ almost surely, which implies
that the first term is zero almost surely. Further, $\bm{d}_i$ maps $\c, \tc$
to uniformly distributed random variables on $(0,1)^{n_c}$, which implies that
the differential entropy of $\bm{d}_1(\c)$ and $\bm{d}_2(\tc)$ is zero, as
well. Consequently, there exists a pair of functions $\g_1^*, \g_2^*$ that
minimizes $\SymAME$.

Next, let $\g_1: \mathcal{X}_1 \mapsto (0,1)^{n_c}$ and  $\g_2: \mathcal{X}_2
\mapsto (0,1)^{n_c}$ be \emph{any} pair of smooth functions that attains the
global minimum of Eq.~(\refeq{eq:sym-info-nce-estimand-def}), i.e.,
\footnotesize
\begin{equation}
  \label{eq:any-minimizer-zero}
  \SymAME(\g_1, \g_2) = \mathbb{E}_{(\x_1,\x_2) \sim p_{\x_1,\x_2}} 
  \left[ \lVert \g_1(\x_1) - \g_2(\x_2) \rVert_2 \right] - \nicefrac{1}{2} 
  \left( H(\g_1(\x_1)) + H(\g_2(\x_2)) \right) = 0 \;. 
\end{equation}
\normalsize
Let $\h_1 := \g_1 \circ \f_1$ and $\h_2 := \g_2 \circ \f_2$, and notice that
both are smooth functions since all involved functions are smooth by
definition. Since \Cref{eq:any-minimizer-zero} is a global minimum, it implies
the following invariance conditions for the individual terms:
\footnotesize
\begin{align}
	\mathbb{E}_{(\x_1,\x_2) \sim p_{\x_1,\x_2}} \left[ \lVert \h_1(\z_1) - \h_2(\z_2) \rVert_2 \right] 
    &= 0 \label{eq:invariance-rel-step1} \\
	H(\h_1(\z_2)) &= 0 \label{eq:hh1-unif} \\
	H(\h_2(\z_2)) &= 0 \label{eq:hh2-unif}
\end{align}
\normalsize
Hence, $\h_1(\z_1) = \h_2(\z_2)$ must hold almost surely 
w.r.t.~$p_{\x_1, \x_2}$. Additionally, \Cref{eq:hh1-unif} (resp.
\Cref{eq:hh2-unif}) implies that $\hat{\c}_1 = \h_1(\z_1)$ 
(resp. $\hat{\c}_2 = \h_2(\z_2)$) must be uniform on $(0,1)^{n_c}$.

\paragraph{Step 2.} 
Next, we show that any pair of functions that minimize $\SymAME$ depend only on
content information. Since style is independent of $\m_1$ and $\m_2$, we first
show that $\h_1(\z_1)$ does not depend on $\m_1$, and that $\h_2(\z_2)$ does
not depend on $\m_2$. We then show that $\h_1$ and $\h_2$ also cannot depend on
style, based on a result from previous work.

First note, that we can exclude all degenerate solutions where $\g_1$ maps a
component of $\m_1$ to a constant, since $\g_1$ would not be invertible anymore
and such a solution would violate the invariance in Eq.~(\refeq{eq:hh1-unif}).
To prove a contradiction, suppose that, w.l.o.g., 
$\h_1(\c,\s, \m_1)_{1:n_{c}} := \h_1(\z_1)_{1:n_{c}}$ depends on some component
in $\m_1$ in the sense that the partial derivative of $\h_1(\z_1)_{1:n_{c}}$
w.r.t.~some modality-specific variable $m_{1,l}$ is non-zero for some point
$(\c^*, \s^*, \m_1^*) \in \mathcal{Z}_1$. Specifically, it implies that the
partial derivative $\nicefrac{\partial \h_1(\z_1)_{1:n_{c}}}{\partial m_{1,l}}$
is positive in a neighborhood around $(\c^*, \s^*, \m_1^*)$, which is a
non-empty open set, since $\h_1$ is smooth. On the other hand, due to the
independence of $\z_2$ and $\m_1$, the fact that $\h_2(\z_2)_{1:n_c}$ cannot
not depend on $\m_1$, and that $p(\z) > 0$ almost everywhere, we come to a
contradiction.  That is, there exists an open set of points with positive
measure, namely the neighbourhood around $(\c^*, \s^*, \m_1^*)$, on which
\footnotesize
\begin{equation}
  | (\h_1(\z_1)_{1:n_c} - \h_2(\z_2)_{1:n_c}) | > 0
\end{equation}
\normalsize
almost surely, which contradicts the invariance in \Cref{eq:invariance-rel-step1}.
The statement does not change, if we add further dependencies of $\h_1$ on
components of $\m_1$, or for $\h_2$ on components of $\m_2$, because $\m_1$ and
$\z_2$ are independent, and $\m_2$ and $\z_1$ are independent as well. Hence,
we show that \emph{any} encoder that minimizes the objective in
\Cref{eq:sym-info-nce-estimand-def} cannot depend on modality-specific
information.

Having established that neither $\h_1(\z_1)_{1:n_c}$, nor $\h_2(\z_2)_{1:n_c}$
can depend on modality-specific information, it remains to show that style
information is not encoded, as well. Leveraging \Cref{as:style}, we can show
that the strict inequality in \Cref{eq:invariance-rel-step1} has a positve
density if $\h_1(\z_1)_{1:n_c}$ or $\h_2(\z_2)_{1:n_c}$ was dependent on a
dimension in $\s$ respectively $\ts$, which would again lead to a violation of
the invariance derived in \Cref{eq:invariance-rel-step1}, as shown
in~\citet[][Proof of Theorem~4.2]{Kuegelgen2021}.

\paragraph{Step 3.} 
It remains to show that $\h_1, \h_2$ are bijections. We know that $\mathcal{C}$
and $(0,1)^{n_c}$ are simply connected and oriented $C^1$ manifolds, and we
have established in Step~1 that $\h_1$ and $\h_2$ are smooth and hence
differentiable functions. Since $p_\c$ is a regular density, the uniform
distributions w.r.t.~the pushthrough functions $\h_1$ and $\h_2$ are regular
densities. Thus, $\h_1$ and $\h_2$ are bijections~\citep[Proposition~5]{Zimmermann2021}

Step 3 concludes the proof. We have shown that for any pair of smooth functions
$\g_1, \g_2$ that attain the global minimum of
Eq.~(\refeq{eq:sym-info-nce-estimand-def}), we have that $\c$ is
\emph{block-identified} (Def.~\refeq{def:block-identified}) by $\g_1$ and
$\g_2$.
\end{proof}


\subsection{Symmetric generative process}
\label{app:symmetric_generative_process}

Throughout the main body of the paper, we described an asymmetric generating
mechanism, where $\z_2$ is a perturbed version of $\z_1$. Here, we will briefly
sketch out how our model and results can be adapted to a symmetric setting,
where \emph{both} $\z_1$ and $\z_2$ are generated as perturbations of $\z$.

Concretely, we would need to make small adjustments to
Assumptions~\ref{as:content} and \ref{as:style} as follows. We start with the
content invariance in \Cref{as:content}, which specifies how 
$\z_1 = (\tc_1, \ts_1, \tm_1)$ and $\z_2 = (\tc_2, \ts_2, \tm_2)$ are generated.

Let $i \in \{ 1,2 \} $. The conditional density $p_{\z_i|\z}$ over
$\mathcal{Z}_i \times \mathcal{Z}$ takes the form
\begin{equation} \label{eq:zi-given-z}
  p_{\z_i|\z}(\z_i|\z) = \delta(\tc_i - \c) \delta(\tm_i - \m_i) p_{\ts_i|\s}(\ts_i|\s) \; ,
\end{equation}
where $\delta( \cdot )$ is the Dirac delta function, i.e., $\tc_i = \c$ almost
everywhere, as well as $\tm_i = \m_i$ almost everywhere. Note that since 
$\tc_1 = \c$ a.e.~and $\c = \tc_2$ a.e.,~it follows that $\tc_1 = \tc_2$ almost
everywhere, which is a property that is needed in Step~1 in the proof of
\Cref{th:main}. In addition, it still holds that $\tm_i \Indep \z_j$, for $i, j
\in \{ 1,2 \}$ and $i \neq j$, which is needed in Step 2 of the proof to show
that modality-specific information is not encoded.

Lastly, we need to revisit \Cref{as:style}, for which both $\ts_1$ and $\ts_2$
would be generated through perturbations of $\s$ via the conditional
distribution $p_{\ts_i|\s}$ on $\mathcal{S} \times \mathcal{S}$, as described
in \Cref{as:style}, for each $i$ individually. As a small technical nuance, we
would need to specify the conditional generation of the perturbed style
variables $\ts_1$ and $\ts_2$ such that they are not perturbed in an identical
manner w.r.t.~$\s$. This can be ensured by, e.g., constraining $p_A$
appropriately to exclude the degenerate case where dimensions in $\ts_1$ and
$\ts_2$ are perfectly aligned---a case that needs to be excluded for Step~2 of
the proof of \Cref{th:main}.

 
\section[Appendix B: Experiments]{Experiments}
\label{app:experiment_details}

\subsection{Experimental details}
\label{sec:app-details-to-experimental-setting}

\paragraph{Numerical simulation}
The generative process is described in \Cref{subsec:numerical_experiment}.
Here, we provide additional information about the experiment. The invertible
MLP is constructed similar to previous work
\citep{Hyvarinen2016,Hyvaerinen2017,Zimmermann2021,Kuegelgen2021} by resampling
square weight matrices until their condition number surpasses a threshold
value. For the original setting~($\f_1 = \f_2$), we use one encoder~($\g_1 = \g_2$),
whereas for the multimodal setting~(${\f_1 \not= \f_2}$), we use distinct
encoders~($\g_1 \not= \g_2$) to mirror the assumption of distinct mixing
functions and because, in practice, the dimensionality of the observations can
differ across modalities.  In \Cref{tab:numerical_details}, we specify the main
hyperparameters for the numerical simulation.

\paragraph{Multimodal3DIdent}
Our dataset of image/text pairs is based on the code used to generate the
\emph{Causal3DIdent} \citep{Kuegelgen2021,Zimmermann2021} and \emph{CLEVR}
\citep{Johnson2017} datasets.  Images are generated using the \emph{Blender}
renderer \citep{Blender}. The rendering serves as a complex mixing function
that generates the images from 11 different parameters (i.e., latent factors)
that are listed in \Cref{tab:imgtxt_latent_factors}. To generate textual
descriptions, we adapt the text rendering from \emph{CLEVR} \citep{Johnson2017}
and use 5 different phrases to induce modality-specific variation. The latent
factors used to generate the text are also listed in
\Cref{tab:imgtxt_latent_factors}. The dependence between the image and text
modality is determined by three content factors (object shape, x-position, and
y-position) and one style factor (object color). For the object color in the
image, we use a continuous hue value, whereas for the text we match the RGB
value with the nearest color value from one of three different
palettes$^\text{\ref{footnote:color_palettes}}$ that is sampled uniformly at
random for each observation. Further, we ensure that there are no overlapping
color values across palettes by using a prefix for the respective palette
(e.g., ``xkcd:black'') when necessary. In \Cref{subsec:imagetext_experiment},
we use a version of the \emph{Multimodal3DIdent} dataset with a causal
dependence from content to style. Specifically, the color of the object depends
on its x-position. In particular, we split the range of hue values $[0, 1]$
into three equally sized intervals and associate each of these intervals with a
fixed x-position of the object. For instance, if x-position is ``left'', we
sample the hue value from the interval $[0, \nicefrac{1}{3}]$. Consequently,
the color of the object can be predicted to some degree from the position of
the object.  Samples of image/text pairs from the \emph{Multimodal3DIdent}
dataset are shown in \Cref{fig:imagetext_examples,fig:multimodal3dident_app}.
The hyperparameters for the experiment are listed in \Cref{tab:imgtxt_details}.
In \Cref{app:additional_experiments}, we provide additional results for a
version of the dataset with mutually independent factors. 

\paragraph{High-dimensional image pairs} 
In \Cref{app:additional_experiments}, we provide additional results using a
dataset of high-dimensional pairs of images of size $224\times224\times3$. Similar to
\textit{Multimodal3DIdent}, images are generated using \textit{Blender}
\citep{Blender} and code adapted from previous work
\citep{Zimmermann2021,Kuegelgen2021}. Each image depicts a scene with one type
of object \citep[a teapot, like in][]{Zimmermann2021} in front of a colored
background and illuminated by a colored spotlight (for examples, see
\Cref{fig:m3DIdent0}). The scene is defined by 9 continuous latent variables
each of which is sampled from a uniform distribution. Object positions (x-, y-
and z-coordinates) are content factors that are always shared between
modalities, while object-, spotlight- and background-colors are style factors
that are stochastically shared. Modality-specific factors are object rotation
($\alpha$ and $\beta$ angles) for one modality and spotlight position for the
other. To simulate modality-specific mixing functions, we render the objects
using distinct textures (i.e., rubber and metallic) for each modality. Further,
we generate two versions of this dataset, with and without causal dependencies.
For the dataset with causal dependencies we sample the latent factors according
to a causal model, where background-color depends on z-position and
spotlight-color depends on object-color.  We use ResNet-18 encoders and similar
hyperparameter values to those used for the image/text experiment
(\Cref{tab:imgtxt_details}). 

\begin{table}[!ht]
\vspace{2em}
\centering
    \begin{subtable}{.5\linewidth}
    \centering
    \small{%
        \begin{tabular}{lr}
          \toprule
          \textbf{Parameter} & \textbf{Value} \\
          \midrule
          Generating function & 3-layer MLP\\
          Encoder & 7-layer MLP\\
          Optimizer & Adam \\  %
          Cond. threshold ratio & 1e-3 \\
          Dimensionality $d$ & 15\\
          Batch size & 6144 \\
          Learning rate & 1e-4 \\
          Temperature $\tau$ & 1.0 \\
          \# Seeds & 3 \\
          \# Iterations & 300,000 \\
          Similarity metric & Euclidian \\
          Gradient clipping & 2-norm; max value 2 \\
          \bottomrule
        \end{tabular}
        \caption{Parameters used for the numerical simulation.}
        \label{tab:numerical_details}
    } %
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
    \small{%
        \begin{tabular}{lr}
          \toprule
          \textbf{Parameter} & \textbf{Value} \\
          \midrule
          Generating function & Image and text rendering\\
          Image encoder & ResNet-18\\
          Text encoder & 4-layer ConvNet\\
          Optimizer & Adam \\  %
          Batch size & 256 \\
          Learning rate & 1e-5 \\
          Temperature $\tau$ & 1.0 \\
          \# Seeds & 3 \\
          \# Iterations & 100,000 \\
          \# Samples (train\,/\,val\,/\,test) & 125,000\,/\,10,000\,/\,10,000 \\
          Similarity metric & Cosine similarity \\
          Gradient clipping & 2-norm; max value 2 \\
          \bottomrule
        \end{tabular}
        \caption{Parameters used for \emph{Multimodal3DIdent}.}
        \label{tab:imgtxt_details}
    } %
    \end{subtable}
    \caption{Experimental parameters and hyperparameters used for the two experiments in the main text.}
\end{table}



\begin{table}[!ht]
\vspace{2em}
\small{%
\centering
    \begin{tabular}{lrr}
      \toprule
      \textbf{Latent factor} & \textbf{Distribution} & \textbf{Details} \\
      \midrule
      \textbf{Object shape} & Categorical & 7 unique values \\
      \textbf{Object x-position} & Categorical & 3 unique values \\
      \textbf{Object y-position} & Categorical & 3 unique values \\
      \emph{Object color} & Uniform & hue value in $[0, 1]$ \\
      Object rotation $\alpha$ & Uniform & angle value in $[0, 1]$ \\
      Object rotation $\beta$ & Uniform & angle value in $[0, 1]$ \\
      Object rotation $\gamma$ & Uniform & angle value in $[0, 1]$ \\
      Spotlight position & Uniform & angle value in $[0, 1]$ \\
      Spotlight color & Uniform & hue value in $[0, 1]$ \\
      Background color & Uniform & hue value in $[0, 1]$ \\
      \midrule
      \textbf{Object shape} & Categorical & 7 unique values \\
      \textbf{Object x-position} & Categorical & 3 unique values \\
      \textbf{Object y-position} & Categorical & 3 unique values \\
      \emph{Object color} & Categorical & color names (3 palettes)\footnotemark \\  %
      Text phrasing & Categorical & 5 unique values \\  %
      \bottomrule
    \end{tabular}
    \caption{Description of the latent factors used to generate
      \emph{Multimodal3DIdent}. The first 10 factors are used to generate the
      images and the remaining 5 factors are used to generate the text.
      Object~z-position is kept constant for all images, which is why we do not list
      it among the generative factors. Independent factors are drawn uniformly from
      the respective distribution. Content factors are denoted in bold and style
      factors in italic; the remaining factors are modality-specific.
    }
\label{tab:imgtxt_latent_factors}
} %
\end{table}

\footnotetext{%
\label{footnote:color_palettes}
  We use the following three palettes from the \texttt{matplotlib.colors} API:
  Tableau colors (10 values), CSS4 colors (148 values), and XKCD colors (949
  values).
}


\newpage
\clearpage
\subsection{Additional experimental results}
\label{app:additional_experiments}


\begin{figure}[t]
    \vspace{-2.0em}
    \centering
    \begin{subfigure}[t]{1.0\textwidth}
        \includegraphics[width=1.0\textwidth]{figures/image_app.png}
        \vspace{-10pt}
    \end{subfigure}
    \vspace{-1.5em}
    \caption{Examples of image/text pairs from the \textit{Multimodal3DIdent}
      dataset. Each sample shows one of the seven shapes or classes of objects
      included in the dataset.
    }
\label{fig:multimodal3dident_app}
\end{figure}

\begin{table}[t]
\vspace{1.5em}
  \centering
  \begin{subtable}{.49\textwidth}
    \centering
    \resizebox{0.95\textwidth}{!}{%
      \small
      \begin{tabular}{ccccc}
        \toprule
        \multicolumn{3}{c}{\textbf{Generative process}} & \multicolumn{2}{c}{$\bm{R^2}$ \textbf{(nonlinear)}}  \\
        \cmidrule(r){1-3}\cmidrule(r){4-5}
        \textbf{p(chg.)} & \textbf{Stat.} & \textbf{Cau.} & \textbf{Content $\c$} & \textbf{Style $\s$} \\
        \midrule
       1.0 & \xmark & \xmark  & $\textbf{1.00} \pm 0.00$ & $0.00 \pm 0.00$ \\
       0.75 & \xmark & \xmark & $\textbf{0.99} \pm 0.01$ & $0.00 \pm 0.00$ \\
       0.75 & \cmark & \xmark & $\textbf{0.99} \pm 0.00$ & $0.52 \pm 0.09$ \\
       0.75 & \xmark & \cmark & $\textbf{1.00} \pm 0.00$ & $\textbf{0.79} \pm 0.04$ \\
       0.75 & \cmark & \cmark & $\textbf{0.99} \pm 0.01$ & $\textbf{0.81} \pm 0.04$ \\
        \bottomrule
      \end{tabular}
    }  %
    \caption{Original setting} %
    \label{subtab:numerical_ablation_nom1m2_vanilla}
  \end{subtable}
  \begin{subtable}{.49\textwidth}
    \centering
    \resizebox{0.95\textwidth}{!}{%
      \small
      \begin{tabular}{ccccc}
        \toprule
        \multicolumn{3}{c}{\textbf{Generative process}} & \multicolumn{2}{c}{$\bm{R^2}$ \textbf{(nonlinear)}}  \\
        \cmidrule(r){1-3}\cmidrule(r){4-5}
        \textbf{p(chg.)} & \textbf{Stat.} & \textbf{Cau.} & \textbf{Content $\c$} & \textbf{Style $\s$} \\
        \midrule
       1.0 & \xmark & \xmark  & $\textbf{1.00} \pm 0.00$ & $0.00 \pm 0.00$ \\
       0.75 & \xmark & \xmark & $\textbf{1.00} \pm 0.00$ & $0.00 \pm 0.00$ \\
       0.75 & \cmark & \xmark & $\textbf{0.99} \pm 0.01$ & $0.36 \pm 0.10$ \\
       0.75 & \xmark & \cmark & $\textbf{1.00} \pm 0.00$ & $\textbf{0.81} \pm 0.03$ \\
       0.75 & \cmark & \cmark & $\textbf{0.99} \pm 0.01$ & $\textbf{0.83} \pm 0.05$ \\
        \bottomrule
      \end{tabular}
    }  %
    \caption{Multimodal setting} %
  \label{subtab:numerical_ablation_nom1m2_multimodal}
  \end{subtable}
  \caption{%
    Results of the numerical simulations \emph{without} modality-specific latent
    variables. We compare the original setting ($\f_1 = \f_2$, left table) with
    the multimodal setting ($\f_1 \not= \f_2$, right table). Each row presents
    the results of a different setup with varying style-change probability
    p(chg.) and possible statistical (Stat.) and/or causal (Caus.) dependencies.
    Each value denotes the $R^2$ coefficient of determination (averaged across 3
    seeds) for a nonlinear regression model that predicts the respective ground
    truth factor ($\c, \s$, or $\m_i$) from the learned representation.
  }
\label{tab:numerical_ablation_nom1m2}
\end{table}


\paragraph{Numerical simulation without modality-specific latents}
Recall that the considered generative process (\Cref{sec:generative_process})
has two sources of modality-specific variation: modality-specific mixing
functions and modality-specific latent variables. To decouple the effect of
these two sources of variation, we conduct an ablation study \emph{without}
modality-specific latent variables. \Cref{tab:numerical_ablation_nom1m2}
presents the results, showing that content is block-identified in both the
original setting ($\f_1 = \f_2$,
\Cref{subtab:numerical_ablation_nom1m2_vanilla}) and the multimodal setting
($\f_1 \not= \f_2$, \Cref{subtab:numerical_ablation_nom1m2_multimodal}).
Compared to \Cref{tab:numerical_m1m2}, we observe that the content prediction
is improved slightly in the case without modality-specific latent variables.
Hence, our results suggest that contrastive learning can block-identify content
factors in the multimodal setting with and without modality-specific latent
variables.

\paragraph{Numerical simulation with discrete latent factors}
Extending the numerical simulation from \Cref{subsec:numerical_experiment}, we
test block-identifiability of content information when observations are
generated from a mixture of continuous and discrete latent variables, thus
violating one of the assumptions from \Cref{th:main}. In this setting, content,
style and modality-specific information are random variables with 5 components
sampled from either a continuous normal distribution or a discrete multinomial
distribution with $k$ classes, for which we experiment with different ${k \in
\{3,4,\ldots,10\}}$. For all settings, we train an encoder with the InfoNCE
objective and set the encoding size to 5 dimensions. The other hyperparameters
used in this set of experiments are detailed in \Cref{tab:numerical_details}.
To ensure convergence of the models, we extended the number of training
iterations to 600,000 and 3,000,000 for experiments with discrete
style/modality-specific and discrete content variables respectively. With
discrete style or modality-specific variables and continuous content
(\Cref{subfig:stylediscrete,subfig:msdiscrete}), the results suggest that
content is block-identified, since the prediction of style and
modality-specific information is at chance level (i.e., $\textit{accuracy} =
1/k$) while content is consistently fully recovered ($R^{2} \geq 0.99$). In the
opposite setting, with continuous style and modality-specific variables and
discrete content (\Cref{subfig:contentdiscrete}), the number of content classes
appears to be a critical factor for block-identifiability of content: while
content is always encoded well, style information is also encoded to a
significant extent when the number of content classes is small, but
significantly less style can be recovered when the number of content classes
increases. Through this set of experiments, we challenge the assumption that
\textit{all} generative factors should be continuous (c.f.,
\Cref{sec:generative_process}) and show that block-identifiability of content
can still be satisfied when content is continuous while style or
modality-specific variables are discrete. On the other hand, style is encoded
to a significant extent when content is discrete, which might explain our
observation for the image/text experiment, where we saw that, in the presence
of discrete content factors, some style information can be encoded.

\begin{figure}[t]
\vspace{-0em}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=1.0\textwidth]{figures/discrete_style_ms.pdf}
        \caption{Only $\s$ is discrete}
        \label{subfig:stylediscrete}
    \end{subfigure}%
    \quad
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=1.0\textwidth]{figures/discrete_ms_ms.pdf}
        \caption{Only $\m_1, \m_2$ are discrete}
        \label{subfig:msdiscrete}
    \end{subfigure}%
    \quad
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/discrete_content_ms.pdf}
        \caption{Only $\c$ is discrete}
        \label{subfig:contentdiscrete}
    \end{subfigure}
    \hfill
    \caption{%
      Numerical simulations with discrete latent factors. The results
      show three settings in each of which one group of latent variables is
      discrete while the remaining groups are continuous. Continuous variables
      are normally distributed, whereas discrete variables are sampled from a
      multinomial distribution with $k$ distinct classes.  We measure the
      prediction performance with a nonlinear model in terms of the $R^2$
      coefficient of determination for continuous factors and classification
      accuracy for discrete factors respectively. Each point denotes the
      average across three seeds and error bars show the standard deviation,
      which is relatively small. 
    }
\label{fig:discrete} 
\end{figure}

\paragraph{Dimensionality ablations for the numerical simulation}
To test the effect of latent dimensionality on identifiability,
\Cref{fig:numerical_dimensionality_ablations} presents dimensionality ablations
where we keep the number of content dimensions fixed and only vary the number
of style or modality-specific dimensions, $n_s$ and $n_m$ respectively.
\Cref{subfig:numerical_mdim_ablation,subfig:numerical_sdim_ablation} confirm
that block-identifiability of content still holds when we significantly
increase the number of style or modality-specific dimensions, as the
representation consistently encodes only content and no style or
modality-specific information. In
\Cref{subfig:numerical_mdim_learning_curves,subfig:numerical_sdim_learning_curves},
we can observe that the training loss decreases more slowly when we increase
the dimensionality of $n_c$ and $n_s$ respectively, which provides an intuition
that the sample complexity might increase with the number of style and
modality-specific dimensions.

\begin{figure}[p]
\vspace{-2.0em}
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/numsim_mdim_ablation.pdf}
        \caption{Prediction performance as a function of $n_m$}
        \vspace{0.75em}
        \label{subfig:numerical_mdim_ablation}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/numsim_mdim_learning_curves.pdf}
        \caption{Learning curves for different $n_m$}
        \label{subfig:numerical_mdim_learning_curves}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/numsim_sdim_ablation.pdf}
        \caption{Prediction performance as a function of $n_s$}
        \label{subfig:numerical_sdim_ablation}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/numsim_sdim_learning_curves.pdf}
        \caption{Learning curves for different $n_s$}
        \label{subfig:numerical_sdim_learning_curves}
    \end{subfigure}
    \caption{%
      Dimensionality ablation for the numerical simulation. We consider
      the multimodal setting with mutually independent factors and test the
      effect of latent dimensionality on identifiability by keeping the number
      of content dimensions fixed and only varying the number of style or
      modality-specific dimensions ($n_s$ and $n_m$ respectively). In
      \Cref{subfig:numerical_mdim_ablation,subfig:numerical_sdim_ablation} we
      measure the nonlinear prediction performance in terms of the $R^2$
      coefficient of determination of a nonlinear regression model that
      predicts the respective ground truth factor ($\c$, $\s$, or $\m_i$) from
      the learned representation. In
      \Cref{subfig:numerical_mdim_learning_curves,subfig:numerical_sdim_learning_curves},
      we plot the learning curves (i.e., the training loss) of the respective
      models to compare how fast they converge.
    }
\label{fig:numerical_dimensionality_ablations}
\vspace{2.5em}
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=1.0\textwidth]{figures/loss.pdf}
        \caption{Validation loss for the numerical simulation}
        \label{subfig:model_selection_numerical}
    \end{subfigure}%
    \quad
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/mc3di_loss.pdf}
        \caption{Validation loss for the image/text experiment}
        \label{subfig:model_selection_imagetext}
    \end{subfigure}
    \caption{%
      An attempt at estimating of the number of content factors using the
      validation loss. The validation loss corresponds to the value of the
      $\SymInfoNCE$ objective computed on a holdout dataset. Since we are
      interested in estimating the true number of content factors to select the
      encoding size appropriately, we plot the validation loss as a function of
      the encoding size. We show the validation loss for the numerical simulation
      with independent factors (\Cref{subfig:model_selection_numerical}) and for
      the image/text experiment (\Cref{subfig:model_selection_imagetext})
      respectively.
    }
\label{fig:model_selection}
\end{figure}

\paragraph{Estimating the number of content factors}
The estimation of the number of content factors is an important puzzle piece,
since \Cref{th:main} assumes that the number of content factors is known or
that it can be estimated.  In practice, the number of content factors can be
viewed as a single hyperparameter \citep[e.g.,][]{Locatello2020} that can be
tuned with respect to a suitable model selection metric. For instance, one
could use the validation loss for model selection, which would be convenient
since the validation loss only requires a holdout dataset and no additional
supervision. In \Cref{fig:model_selection}, we plot the validation loss
(averaged over 2,000 validation samples) as a function of the encoding size for
both experiments used in our paper. Results for the numerical simulation are
shown in \Cref{subfig:model_selection_numerical} and for the image/text
experiment in \Cref{subfig:model_selection_imagetext}. For both datasets, we
observe that the validation loss increases most significantly in the range
around the true number of content factors. For the numerical simulation, the
results look promising as they show a clear ``elbow'' \citep{James2013} at the
correct value of 5, which corresponds to the true number of content factors.
The results are less clear for the image/text experiment, where the elbow
method might suggest the range of 2-4 content factors, while the true value is
3. While these initial results look promising, we believe that more work is
required to investigate the estimation of the number of content factors and the
design of suitable heuristics, which are interesting directions for future
research.

\newpage
\clearpage

\paragraph{Evaluation with test-time interventions}
Previously, we observed that style can be predicted to some degree when there
are causal dependencies from content to style (\Cref{tab:numerical_m1m2}),
which can be attributed to style information being partially predictable from
the encoded content information in the causal setup. To verify that the
encoders only depend on content information (i.e., that content is
block-identified), we assess the trained models using a novel, more rigorous
empirical evaluation for the numerical simulation. We test the effect of
\emph{interventions} $\c \to \c'$, which perturb the content information
\emph{at test time} via batch-wise permutations of content, before generating
$\x_1' = \f_1(\c',\s, \m_1)$ and $\x_2' = \f_1(\c', \ts,\m_1)$. Hence, we break
the causal dependence between content and style (see illustration in
\Cref{tab:numerical_ablation_permcontent}), which allows us to better assess
whether the trained encoders depend on content or style information.
Specifically, we train the encoders for 3,000,000 iterations to ensure
convergence and then train nonlinear regression models to predict both the
original and the intervened content variables from the learned representations.
\Cref{tab:numerical_ablation_permcontent} presents our results using the
interventional setup, showing that in most cases only content information can
be recovered. We observe an exception (underlined values) in the two cases with
statistical dependencies, where some style information can be recovered, which
is expected because statistical dependencies reduce the effective
dimensionality of content \citep[cp.][]{Kuegelgen2021}. Analogously, in the
case of statistical and causal dependencies, some of the original content
information can be recovered via the encoded style information. In summary, the
evaluation with interventions provides a more rigorous assessment of
block-identifiability in the causal setup, showing that neither style nor
modality-specific information can be recovered when the encoding size matches
the true number of content dimensions.

\begin{figure}[t]
  \centering
  \scalebox{.79}{%
    \begin{tikzpicture}
    \node[obs] (X1) {$\x_1'$};%
    \node[latent,above=of X1,xshift=-1.5cm] (M1) {$\m_1$};%
    \node[latent,above=of X1,xshift=-0.5cm] (S1) {$\s$}; %
    \node[latent,above=of X1,xshift=0.5cm] (C) {$\c$}; %
    \node[latent,above=of X1,xshift=1.5cm] (C1) {$\c'$}; %
    \node[obs, xshift=2cm] (X2) {$\x_2'$};%
    \node[latent,above=of X2,xshift=0.5cm] (S2) {$\ts$}; %
    \node[latent,above=of X2,xshift=1.5cm] (M2) {$\m_2$};%
    \node[above left=0.01cm and 0.01cm of C1, outer sep = 0pt, inner sep = 0pt, text = red,rotate=-46,scale=1.2] {$\boldsymbol{\leadsto}$};
    \edge{M1,C1,S1}{X1}%
    \edge{M2,C1,S2}{X2}%
    \edge{C}{S1}%
    \edge{C}{C1}%
    \edge[bend left=50]{S1}{S2}%
  \end{tikzpicture}
  } %
  \;
  \resizebox{!}{2.4cm}{%
    \small
    \begin{tabular}[b]{ccccccc}
      \toprule
      \multicolumn{3}{c}{\textbf{Generative process}} & \multicolumn{3}{c}{$\bm{R^2}$ \textbf{(nonlinear)}}  \\
      \cmidrule(r){1-3}\cmidrule(r){4-7}
      \textbf{p(chg.)} & \textbf{Stat.} & \textbf{Cau.} & \textbf{Content $\c$} & \textbf{Content $\c'$} & \textbf{Style $\s$} & \textbf{Modality $\m_i$}\\
      \midrule
     1.0 & \xmark & \xmark  & $0.00 \pm 0.00$ & $\textbf{1.00} \pm 0.00$ &$0.00 \pm 0.00$ &$0.00 \pm 0.00$ \\
     0.75 & \xmark & \xmark & $0.00 \pm 0.00$ & $\textbf{1.00} \pm 0.00$ &$0.00 \pm 0.00$ &$0.00 \pm 0.00$ \\
     0.75 & \cmark & \xmark & $0.00 \pm 0.00$ & $\textbf{1.00} \pm 0.00$ &$\underline{0.50} \pm 0.19$ &$0.00 \pm 0.00$ \\
     0.75 & \xmark & \cmark & $0.01 \pm 0.00$ & $\textbf{0.98} \pm 0.00$ &$0.03 \pm 0.01$ &$0.00 \pm 0.00$ \\
     0.75 & \cmark & \cmark & $\underline{0.28} \pm 0.14$ & $\textbf{0.91} \pm 0.03$ &$\underline{0.39} \pm 0.20$ &$0.00 \pm 0.00$ \\
      \bottomrule
    \end{tabular}
  }  %
  \caption{%
    Evaluation with test-time interventions. We use the interventional setup
    that is illustrated on the left, i.e., perturbed samples $\x_1', \x_2'$
    that are generated from the intervened content $\c'$, which is a copy of
    the original content $\c$ with an intervention, i.e., a batch-wise
    permutation (\textcolor{red}{$\leadsto$}) that makes $\c'$ independent of
    $\s$.  Each row presents the results of a different setup with varying
    style-change probability p(chg.) and possible statistical (Stat.) and/or
    causal (Caus.) dependencies. Each value denotes the $R^2$ coefficient of
    determination (averaged across 3 seeds) for a nonlinear regression model
    that predicts the respective ground truth factor ($\c, \c', \s$, or $\m_i$)
    from the learned representation.
  }
\label{tab:numerical_ablation_permcontent}
\end{figure} 

\paragraph{High-dimensional image pairs with continuous latents}
\begin{wrapfigure}{r}{0.28\textwidth} 
\vspace{-4pt}
  \begin{center}
    \includegraphics[width=0.28\textwidth]{figures/image_image3.png}
    \vspace{-15pt}
    \caption{Examples of high-dimensional image~pairs.}
    \label{fig:m3DIdent0}
  \end{center}
\vspace{-13pt}
\end{wrapfigure} 
To bridge the gap between continuous and discrete data, we provide an
additional experiment that offers a realistic setup but uses only continuous
latent variables to satisfy the assumptions of \Cref{th:main}.  Previously, in
\Cref{subsec:imagetext_experiment}, we used a complex multimodal dataset of
image/text pairs, which were generated from a combination of continuous and
discrete latent factors. Now, we consider a different dataset that consists of
\emph{pairs high-dimensional images} generated from a set of continuous
latents, which is more in line with our theoretical assumptions. Note that
datasets with pairs of images are common in practice, for example, in medical
imaging where patients are assessed using multiple views (e.g., images from
different angles) or multiple modalities (e.g., as in PET-CT imaging).  To
generate the data, we adapt the code from 3DIdent \citep{Zimmermann2021} to
render pairs of images, for which the object position is always shared (i.e.,
content), the object-, spotlight- and background-color is stochastically shared
(i.e., style), and modality-specific factors are object rotation for one
modality and spotlight position for the other. Additionally, we render the
objects using different textures to simulate a modality-specific mixing
process.  Samples of image pairs are shown in \Cref{fig:m3DIdent0} and further
details about the dataset can be found in
\Cref{sec:app-details-to-experimental-setting}.  We train the encoders with the
InfoNCE objective for 60,000 iterations using the same architectures and
hyperparameters as for \emph{Multimodal3DIdent} (\Cref{tab:imgtxt_details}),
and again evaluate the $R^2$ coefficient of determination using a kernel ridge
regression that predicts the respective ground truth factor from the learned
representations.

\Cref{fig:multiview_results} present our results for the dataset of image
pairs, showing the prediction performance as a function of the encoding size
for the setting with causal dependencies (\Cref{fig:multiview_causal}) and the
setting with mutually independent latent variables
(\Cref{fig:multiview_noncausal}) respectively.  In both settings, content
information (i.e., object position) is recovered when sufficient encoding
capacity is available. Style and modality-specific information, on the other
hand, are discarded independent of the encoding size. In
\Cref{fig:multiview_causal} we observe the recovery of some style information,
which is expected because style can be predicted to some degree from the
encoded content information when there is a causal dependence of style on
content. Overall, these findings lend further support to our theoretical result
from \Cref{th:main}, as we investigate a realistic setting with only continuous
latent factors, which is more in line with our assumptions. Notably, the
results appear more consistent with our theory, e.g., showing that less style
and modality-specific information is encoded, compared to our results for the
image/text experiment, where we used a combination of continuous and discrete
latent factors.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=1.0\textwidth]{figures/multiview_noncaus_img_nonlin.pdf}
        \caption{With mutually independent factors}
        \label{fig:multiview_noncausal}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/multiview_caus_img_nonlin.pdf}
        \caption{With causal dependencies}
        \label{fig:multiview_causal}
    \end{subfigure}
    \caption{%
      Result with pairs of high-dimensional images. As a function of the encoding
      size of the model, we assess the nonlinear prediction of ground truth
      factors to quantify how well the learned representation encodes the
      respective factors. Content factors are denoted in bold, style factors in
      italic, and modality-specific factors in regular font. Each point denotes
      the average $R^2$ score across three seeds and bands show one standard
      deviation. 
    }
\label{fig:multiview_results}
\end{figure*}

\paragraph{Multimodal3DIdent with mutually independent factors}
For the results of the image/text experiment in the main text
(\Cref{subsec:imagetext_experiment}) we used the \emph{Multimodal3DIdent}
dataset, which we designed such that object color is causally dependent on the
x-position of the object to impose a causal dependence of style on content. In
\Cref{fig:imagetext_noncausal}, we provide a similar analysis using a version
of the dataset \emph{without} the causal dependence, i.e., with mutually
independent factors. For both modalities, we observe that object color is only
encoded when the encoding size is larger than four, i.e., when there is excess
capacity beyond the capacity needed to encode all content factors. Hence, these
results corroborate that contrastive learning can block-identify content
factors in a complex multimodal setting with heterogeneous image/text pairs.

\begin{figure*}[t]
\vspace{2em}
   \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=1.0\textwidth]{figures/mc3di_noncaus_img_nonlin.pdf}
        \caption{Prediction of image factors}
        \label{subfig:imagetext_results_image_noncausal}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/mc3di_noncaus_txt_nonlin.pdf}
        \caption{Prediction of text factors}
        \label{subfig:imagetext_results_text_noncausal}
    \end{subfigure}
    \caption{%
      Result on \emph{Multimodal3DIdent} with mutually independent factors. As
      a function of the encoding size of the model, we assess the nonlinear
      prediction of ground truth image factors (left subplot) and text factors
      (right subplot) to quantify how well the learned representation encodes
      the respective factors. Content factors are denoted in bold and style
      factors in italic. Along the x-axis, we vary the encoding size, i.e., the
      output dimensionality of the model. We measure the prediction performance
      in terms of the $R^2$ coefficient of determination for continuous factors
      and classification accuracy for discrete factors respectively. Each point
      denotes the average across three seeds and bands show one standard
      deviation. 
    }
\label{fig:imagetext_noncausal}
\end{figure*}
