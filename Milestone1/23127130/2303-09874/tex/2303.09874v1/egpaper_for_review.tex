\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% Include other packages here, before hyperref.
\usepackage{multirow}
\usepackage{bigints}
\usepackage{array}
\usepackage{subfigure}

\newenvironment{airytabular}{\setlength{\extrarowheight}{3pt}%
  \begin{tabular}}{\end{tabular}}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\blue}[1]{\textcolor[rgb]{0,0,1}{#1}}
\newcommand{\red}[1]{\textcolor[rgb]{1,0,0}{#1}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\x}{{\bf x}}
\newcommand{\y}{{\bf y}}
\iccvfinalcopy % *** Uncomment this line for the final submission
\setlength{\textfloatsep}{0.3cm}
\def\iccvPaperID{12231} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
%\title{Demonstrating psychophysical behaviour with density estimation around Gabors}

\title{Disentangling the Link Between Image Statistics and Human Perception%\\ 
%Quantifying the relation between natural image statistics and human perception}
}

\author{Alexander Hepburn$^{1 \star}$ \,\, Valero Laparra$^{2\star}$ \,\, Raul Santos-Rodriguez$^1$ \,\, JesÃºs Malo$^2$ \\
$^1$ Department of Engineering Mathematics, University of Bristol \\ $^2$Image Processing Lab, Universitat de Valencia \\
{\tt\small alex.hepburn@bristol.ac.uk, valero.laparra@uv.es, enrsr@bristol.ac.uk, jesus.malo@uv.es}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probability. Specifically, we analyse how the sensitivity of full-reference image quality metrics can be predicted from quantities derived directly from the probability distribution of natural images. First, we compute the mutual information between a wide range of probability surrogates and the sensitivity of the metrics and find that the most influential factor is the probability of the noisy image. Then we explore how these probability surrogates can be combined using a simple model to predict the metric sensitivity, giving an upper bound for the correlation of 0.85 between the model predictions and the actual perceptual sensitivity. Finally, we explore how to combine the probability surrogates using simple expressions, and obtain two functional forms (using one or two surrogates) that can be used to predict the sensitivity of the human visual system given a particular pair of images. \def\thefootnote{*}\footnotetext{Equal Contributions}\def\thefootnote{\arabic{footnote}}

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
% [1] Discusion in 
One long standing discussion in artificial and human vision is about the principles that should drive sensory systems.
This includes Barlow's \emph{Efficient Coding Hypothesis}~\cite{At54,Barlow}, and Marr and Poggio's functional descriptions at the (more abstract) \emph{Computational Level} of vision~\cite{Marr76}.
In modern machine learning terms, the above classical ideas connect the probability density function (PDF) of the images with the behaviour of the sensors.

There is an indirect research direction that explores this connection %between the human visual system and the statistics of natural images 
by proposing design principles for the system (such as information maximisation, or factorisation) to find an optimal transformation and compare the features learned by this transformation and the ones found in the visual system (e.g. receptive fields or nonlinearities)~\cite{Olshausen1996,Bell97,simoncelli2001natural,Schwartz01,hyvarinen2009natural}. 
This indirect research direction, which was popular in the past due to limited computational resources, relies on gross approximations of the PDF of natural images and on strong assumptions about the behaviour of the system. 
Examples of surrogates of the PDF that were proposed as explanations of the behaviour in the above indirect design methods~\cite{Buschbaum83,Olshausen1996,Laughlin81,Miyasawa61,Atick92,Lloyd57,Twer01,Laparra15,Bruce06} are reviewed below in Section~\ref{sec:probvis}.

In contrast, here we focus on the functional goal rather than on the system design: we directly relate the behaviour of the system itself (sensitivity) with the PDF of natural images. 
Following the preliminary suggestions in~\cite{hepburn2022on} about this relation we rely on two elements.
On the one hand, recent \emph{generative models} that represent large image datasets better than previous PDF approximations and provide us with an accurate estimate of the probability at query points~\cite{Oord2016pixelcnn,salimans2017pixelcnn++}. 
Whilst these statistical models are not analytical, they allow for sampling and log-likelihood prediction. 
They also allow us to compute gradients of the probability, which, as reviewed below, have been proven to be related to sensible vision goals.

On the other hand, recent measures of \emph{perceptual distance} between images  have recreated human opinion of subjective distortion to a great accuracy~\cite{laparra2016,zhang2018unreasonable,Hepburn2020perceptnet,ding2020image}. Whilst being just an approximation to human subjectivity, the sensitivity of these perceptual distances is a convenient computational description of the main trends of human vision that should be explained from scene statistics.

In this work, we identify the more relevant probabilistic factors that may be behind the non-Euclidean behavior of perceptual distances and we finally propose a simple expression to predict the sensitivity of the perceptual distances from these factors. 
First, we empirically show the relationship between the sensitivity of the metrics and probability surrogates using conditional histogram plots. 
We then compute the mutual information between the sensitivity of the perceptual distances and the probability surrogates. We do it factor-wise and we also consider groups of factors. 
Then we use different regression models to identify a hierarchy in the factors that allow us to propose analytic relationships for predicting perceptual sensitivity, 
Finally, we perform analysis over the most simple possible closed-form expressions, and select some solutions to predict the perceptual sensitivity given selected probability surrogates.

%One of the most influential ideas in vision science is the \emph{efficient coding hypothesis}, in which Barlow states that the internal representations of medium minimises the redundant information in the signal~\cite{At54,Barlow} and has been shown to be valid for statistical models for images~\cite{simoncelli2001natural,Malo10}. Many results exist for determining the relationship between the probability density function (PDF) of the signal and the learned internal representation if the sole aim is the minimise redundant information. For example, a result from the literature shows that the optimal scheme for compressing a 1-dimensional Laplace signal is to assign code vectors according to $p(x)^{\frac{1}{3}}$~\cite{gersho92vector, Twer01}. Whilst these intuitions are useful, the human visual systems aim is not \emph{only} compression of the signal and the dimensionality of vision is difficult to address using traditional techniques.
%
%In order to examine the relationship between probability and the internal representation of images in the brain, one must have access to the internal representation. Whilst experimentally this is not possible, much work has been done in understanding the structure of the space. In particular, models have successfully recreated the distance between two samples in the internal representation. These distances are called \emph{perceptual distances} and are designed to replication subjective human opinion between two stimuli. Whilst being an approximation to human subjectivity, perceptual distances can predict the distance between images observed by humans with high accuracy. The sensitivity of these distances is how much the distance changes given a fixed Euclidean distance, and previous preliminary results show that this sensitivity is proportional to the probability\cite{hepburn2022on}. Here we extend this idea in many directions, by including more perceptual metrics, more possible probability surrogates, and combinations of them, we analyse the amount of information they share with the perceptual sensitivity and propose analytic relationships between the two concepts.
%
%Specifically, we propose the use of \emph{perceptual distances} (as human visual system approximation) and \emph{probability surrogates} (obtained using a deep learning based generative model) to examine the relationship between human visual perception and the distribution of natural images. 
%
% \begin{itemize}
%     \item link between statistics and perception
%     \item barlow hypothesis
%     \item touch on structural similarity vs error of visibility
%     \item reference vs non-reference metrics
%     \item define sensitivty and why it is important
%     \item relationship between the PDF and sensitivity (iclr 2022)
%     \item closed form equation for probability surrogates and perceptual distance
% \end{itemize}

\section{Background and Proposed Methods}

In this section, we first recall the computational description of human behaviour introduced in~\cite{hepburn2022on}: the \emph{sensitivity of the perceptual distance}.
This subjective sensitivity (which can be computed from the available models of perceptual distance) is the feature to be explained by the probability-related factors considered in this study. Then we review the probability-related factors that have been previously proposed as principles for human vision. 
Our proposal consists of being able to highlight relations and find explicit expressions to derive behaviour from the statistical factors.%Explanations consist of identifying the relations between the statistical factors and the behaviour, and being able to propose expressions to derive behaviour from the statistical factors.
Finally, we introduce the tools to compute both behaviour and statistical factors: (i) the perceptual distances, (ii) the probability models, and (iii) how variations in the image space (distorted images) are chosen.  

\subsection{The problem: Perceptual Sensitivity}

Given some original image $\x$ and a distorted version $\tilde{x}$, full-reference \emph{perceptual distances} are models, $D_p(\x, \tilde{\x})$, that accurately mimic the human opinion about the subjective difference between them. Examples of these models reviewed below include ~\cite{wang2003multiscale,Wang2004,zhang2018unreasonable,Hepburn2020perceptnet,ding2020image}.

In general, this perceived distance, $D_p(\x, \tilde{\x})$, is highly dependent on the particular image analysed, i.e. a distance that is independent of the features of the particular image such as the Root Mean Square Error (RMSE), $||\x - \tilde{\x}||_2$, does not correlate well with human perception~\cite{Wang2009}. This dependence on the image is captured by the \emph{sensitivity of the perceptual distance}, defined as~\cite{hepburn2022on}:
\begin{equation}
    S(\x,\tilde{\x}) = \frac{D_p(\x, \tilde{\x})}{||\x-\tilde{\x}||_2}
    \label{eq:sensitivity}
\end{equation}

The sensitivity of the perceptual distance, also referred to as the \emph{perceptual sensitivity} through out the work, relates a (perceptually meaningful) non-Euclidean distance between images with the (perceptually meaningless) Euclidean distance RMSE.
This ratio is big at regions of the image space where human sensitivity is high and small at neglected regions.
Moreover, this sensitivity is anisotropic: distortions with constant Euclidean length along different directions (distortions of different nature) lead to different subjective distortions and hence sensitivity depends on the direction.  

\subsection{Probabilistic Explanations of Vision}
\label{sec:probvis}
Following the classical ideas on \emph{efficient coding} and on explanations at the \emph{computational level} mentioned in the introduction, there has been a rich literature explicitly linking specific features of human vision with probabilistic descriptions of the images.
Examples of design principles that have been proposed in the past are included in Table~\ref{tab:principles} together with the associated statistical descriptors.


\begin{table*}
\centering
\scriptsize
\caption{Different surrogates of the PDF of natural images (causal factors / explanations) that have been proposed to predict sensitivity.}\vspace{0.1cm}
\label{tab:principles}
\begin{centering}
\begin{airytabular}{|ccc|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Information}\\ \textbf{Transmission}\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Internal Noise}\\ Limited Resolution\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Acquisition Noise}\\ Denoising\end{tabular} &
  \textbf{Surprise} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Signal Average}\\ First Eigenvalue\\ Mean Luminance\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Signal Spectrum}\\ All Eigenvalues \\ Contrast \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Marginal Laplacian}\\ Marginal nonlinearity\end{tabular} \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[t]{@{}c@{}}Barlow~\cite{Barlow}\vspace{0.1cm}\\ Laughlin~\cite{Laughlin81}\vspace{0.1cm}\\ Bell et al.~\cite{Bell97}\vspace{0.1cm}\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[t]{@{}c@{}}Lloyd~\cite{Lloyd57}\vspace{0.1cm}\\ McLeod~\cite{McLeod03}\vspace{0.1cm}\\ Twer et al.~\cite{Twer01}\vspace{0.1cm}\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Miyasawa~\cite{Miyasawa61}\vspace{0.1cm}\\ Raphan \\ \& Simoncelli~\cite{Raphan11}\vspace{0.1cm}\\ Vincent~\cite{Vincent11}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Shannon~\cite{Shannon48}\vspace{0.1cm}\\ Gegenfurtner et al.~\cite{Gegen09,Wichmann02color}\vspace{0.1cm}\\ Bruce \& Tsotsos~\cite{Bruce06}\vspace{0.1cm}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Weber~\cite{Weber1846}\vspace{0.1cm}\\ Fechner~\cite{Fechner1860}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Michelson~\cite{Michelson27}\vspace{0.1cm}\\ Campbell\\ \& Robson~\cite{Campbell68}\vspace{0.1cm}\\ Peli~\cite{Peli90}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Simoncelli~\cite{Simoncelli97}\vspace{0.1cm}\\ Heeger~\cite{Heeger92}\vspace{0.1cm}\\ Daly~\cite{Daly90}\vspace{0.1cm}\\ Malo et al.~\cite{malo2006v1}\end{tabular}} \\ \cline{1-2}
\multicolumn{2}{|c|}{Laparra et al.~\cite{Laparra2012,Laparra15}} &
   &
   &
   &
   &
   \\ \cline{1-3}
\multicolumn{3}{|c|}{Hepburn et al.~\cite{hepburn2022on}} &
   &
   &
   &
   \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}$p(\x)$\\ $log(p(\x))$\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$p(\x)^{\frac{1}{3}}$\\ $\frac{1}{3}log(p(\x))$\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}$J(\x) = \frac{\nabla_\x p(\x)}{p(\x)}$\\ $ \nabla_\x log(p(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$p(\x)^{-1}$\\ $-log(p(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\mu(\x)$\\ $log(\mu(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\frac{1}{\mu(\x)}\Sigma(\x)$\\ $\frac{1}{\mu(\x)} \,\, B \cdot \lambda \cdot B^\top$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\frac{1}{\mu(\x)} \,\, B \cdot log(\lambda) \cdot B^\top$\vspace{0.1cm}\\ $\bigintsss_{\,\x}^{\mathbf{\hat{x}}} p(\x') \, d\x'$\vspace{0.1cm}\\ $\bigintsss_{\,\x}^{\mathbf{\hat{x}}} log( p(\x') ) \, d\x'$\end{tabular} \\ \hline
\end{airytabular}
\end{centering}
\end{table*}

Principles in Table~\ref{tab:principles} include: 
\textbf{(i) Information transmission for \emph{regular images} based on redundancy reduction}. This is at the core of principal component analysis~\cite{Hancock91,Buschbaum83}, independent components analysis~\cite{Olshausen1996,Bell97,hyvarinen2009natural} or in PDF equalisation~\cite{Laughlin81} or factorization~\cite{Malo10}. In all these cases, the PDF or the loglikelihood are key factors.
\textbf{(ii)~Optimal representations for \emph{regular images} in presence of noise}. Noise may occur either in the front-end sensors~\cite{Miyasawa61,Atick90,Atick92} (optimal denoising), or in the internal representation~\cite{Lloyd57,Twer01} (optimal quantisation, or optimal nonlinearities to cope with noise). While in optimal denoising the solutions depend on the derivative of the loglikelihood~\cite{Raphan11,Vincent11}, optimal discrimination is based on resource allocation according to the PDF after saturating non linearities~\cite{Twer01,McLeod03,Series09,Ganguli11}. Bit allocation and quantization according to nonlinear transforms of the PDF has been used in perceptually optimized image coding~\cite{Macq92,Malo00}.
In fact, both factors considered above (the PDF equalisation and the optimal nonlinearities to cope with inner noise) have been unified in a single framework where the representation is driven by the PDF raised to certain exponent~\cite{malo2006v1,Laparra12,Laparra15}. 
\textbf{(iii)~Focus on \emph{surprising images} (as opposed to regular images)} may also drive the sensitivity of the system. This critically different factor (surprise as opposed to regularities) has been pointed out for color perception~\cite{Gegen09,Wichmann02color}, and in visual saliency~\cite{Bruce06}. In this case, the surprise is described by the inverse of the probability (as opposed to the probability) as in the core definition of information~\cite{Shannon48}.

The descriptors associated to the above probabilistic explanations have been found to positively (or negatively) correlate to the sensitivity in a number of observations with a restricted set of metrics~\cite{hepburn2022on}. However, that study did not consider other statistical descriptors of the signal that have been studied by classical vision science (last three columns of Table~\ref{tab:principles}).

The obvious factor that generates the vision process is the \emph{energy}. In statistical terms, \textbf{(iv)~the first moment (mean) of the signal}. The first eigenvalue of the manifold of a class of images represents the average luminance of the scene. The consideration of the nonlinear brightness-from-luminance is a fundamental law in visual psychophysics (the Weber-Fechner law~\cite{Weber1846,Fechner1860}), 
it has statistical explanations related to the cumulative density~\cite{Laughlin81,malo2006v1,Laparra12} and using empirical estimation of reflectance~\cite{PurvesLottoPNAS11}, and adaptivity of brightness curves~\cite{Wittle92} can only be described using sophisticated non-linear architectures~\cite{Brainard05,martinez2018derivatives,Bertalmio20}. 

Beyond the obvious \emph{energy}, vision is about understanding the \emph{spatial structure}. The simplest statistical description of the structure is, \textbf{(v)~the second moment (or covariance) of the signal}. 
The (roughly) stationary invariance of natural images implies that the covariance can be diagonalized in Fourier like-basis, $\Sigma(\x) = B \cdot \lambda \cdot B^\top$~\cite{Clarke81}, and that the spectrum of eigenvalues in $\lambda$ represents the average Fourier spectrum of natural images. The magnitude of the sinusoidal components compared to the mean luminance is the concept of \emph{contrast}~\cite{Michelson27,Peli90}, which is central in human spatial vision, which has a distinct bandwidth~\cite{Campbell68}, which has been study of specific statistical explanations related to the spectrum of natural images~\cite{Atick92,Gomez20,Li22}.

Finally, classical generative models of natural images in the 90's and 00's used \textbf{(vi) heavy-tailed marginal PDFs in transformed domains}~\cite{Simoncelli97,Malo00,hyvarinen2009natural,Oord14_t_student} and then these marginal models were combined through mixing matrices (either PCA, DCT, ICA or wavelets), conceptually represented in the table by the matrix $B$.
In this context, the response of a visual system (again according to PDF uniformization principles) should be related to non-linear saturations of the average spectrum (to compensate the high population at zero contrast) or cumulative functions of the marginals. These explanations~\cite{Schwartz01,malo2006v1} have been given for the adaptive nonlinearities that happen in the wavelet-like representation in the visual cortex~\cite{Heeger92,Carandini12}, and also to define perceptual metrics~\cite{Daly90,Teo94,Malo97,Laparra10,laparra2016}.

\subsection{Our proposal}

What we propose in this work is using the kind of \emph{descriptors} recalled in Table~\ref{tab:principles} (mainly PDFs of images, loglikelihoods, their directional derivatives, cumulative functions, and 1st and 2nd order moments, eventually subjected to saturating nonlinearities) as the subject of our study to predict the \emph{sensitivity} of state-of-the-art perceptual distances, through
information-theory and machine learning feature selection tools and using state-of-the-art generative models for PDF estimation.

Of course the exploration of all the variations of the specific formulations introduced in the literature review associated to Table~\ref{tab:principles} is out of the scope of a conference paper, and, as a proof of concept, in this work we will use the generative models to compute the following list of descriptors involving the original image $\x$ and the distorted image $\tilde{\x}$:
% \begin{eqnarray}
% log(p(\x)) \,\,\, , \,\,\, log(p(\tilde{\x})) \,\,\, , \,\,\, \nonumber\\[0.2cm] \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, 
% ||J(\x)|| \,\,\, , \,\,\,  ||J(\tilde{\x})|| \,\,\, , \,\,\,  (\x-\tilde{\x})^\top\!\! \cdot \! J(\x)\\
% \mu(\x)  \,\,\, , \,\,\, \sigma(\x)  \,\,\, , \,\,\,  \bigintssss_{\,\x}^{\mathbf{\tilde{x}}} log( p(\x') ) \,\, d\x' \nonumber
% \label{descriptors1}
% \end{eqnarray}
\begin{equation}
\begin{aligned}
    log(p(\x)) \, \, , \, \, log(p(\tilde{\x})) \,\, &, \,\, ||J(\x)|| \,\, , \,\, ||J(\tilde{\x})|| \,\, , \\
    (\x-\tilde{\x})^\top\!\! \cdot \! J(\x) \,\, , \,\, \mu(\x) \,\, , \,\, &\sigma(\x) \,\, , \,\, \bigintssss_{\,\x}^{\mathbf{\tilde{x}}} log( p(\x') )\, d\x'
\end{aligned}
\end{equation}
where $J(\x)$ is the (vector) gradient of the loglikelihood at $\x$ as shown in Table~\ref{tab:principles}, and the first descriptor of the 2nd row is the projection of that gradient in the direction of the distortion. Moreover, the standard deviation of the image, $\sigma(\x)$ as a single element of the covariance $\Sigma(\x)$ in Table~\ref{tab:principles}, together with  $\mu(\x)$ capture the concept of RMSE contrast~\cite{Peli90} (a generalization of the Michelson contrast~\cite{Michelson27}), and the integral takes the loglikelihood that can be computed from the generative models and accumulates it along the direction of distortion, qualitatively following the idea of cumulative responses proposed in equalisation methods~\cite{Laughlin81,malo2006v1,Laparra12,Laparra15}.

\subsection{Illustrative perceptual distances}

The most successful perceptual distances can be classified in four big families:
\textbf{(i) Physiological-psychophysical architectures.} These include~\cite{Daly90,Watson93,Teo94,Malo97,Laparra10,Martinez19,Hepburn2020perceptnet}
    and in particular it includes NLPD~\cite{laparra2016}, which consists of  a sensible filterbank of biologically meaningful receptive fields and the canonical Divisive Normalization used in neuroscience~\cite{Carandini12}.
\textbf{(ii) Descriptions of the statistical structure of the images.} These include, the popular SSIM~\cite{Wang2004}, its (improved) multiscale version MS-SSIM~\cite{wang2003multiscale}, and recent version using deep learning: DISTS~\cite{ding2020image}.
\textbf{(iii)~Information-theoretic measures.} These include measures of transmitted information such as VIF~\cite{Sheikh06,Malo21}, and recent measures based on enforcing informational continuity in frames of natural sequences, such as PIM~\cite{Bhardwaj2020}.
\textbf{(iv) Regression models:} generic deep architectures used for vision tasks retrained to reproduce human opinion on distortion as LPIPS~\cite{zhang2018unreasonable}.

In this work we use recent representative examples of the four families: Table~\ref{tab:MOS} illustrates their performance in reproducing human perception.
Fig.~\ref{fig:images} shows an example of how the visual sensitivity to noise highly depends on the image and is well captured by a representative $D_p$ measure, NLPD~\cite{laparra2016}, but not by the Euclidean distance, RMSE.

\begin{table}[b]
\centering
\footnotesize
\caption{Performance of the considered $D_p$'s.
We show Pearson and Spearman correlations with human opinion in TID2013~\cite{tid2013-data}, and agreement with human judgement (in \%) in BAPPS~\cite{zhang2018unreasonable}.\vspace{0.15cm}}
\label{tab:MOS}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
                                                                        & \textbf{MSSSIM}                                       & \textbf{NLPD}                                         & \textbf{PIM}                                          & \textbf{LPIPS}                                        & \textbf{DISTS}                                        \\ \hline
\begin{tabular}[c]{@{}l@{}}TID2013\\ $\rho_p$ ($\rho_s$)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.78\\ (0.80)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.84\\ (0.80)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.62\\ (0.65)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.74\\ (0.67)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.86\\ (0.83)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}BAPPS\\ (\%)\end{tabular}                    & 61.7                                                  & 61.5                                                  & 64.5                                                  & 69.2                                                  & 69.0                                                  \\ \hline
\end{tabular}
\end{table}

\subsection{Illustrative generative model}

\label{sec:est_prob}
Most recent generative models aim at capturing the PDF around certain images, for example, human faces or bedrooms~\cite{kingma2018glow}. However, not only do these sets not cover natural images, they consist of a rigid structure when describing images as a whole. Rather, we follow the methodology in~\cite{hepburn2022on} and rely on a probability model which we know is accurate for the images we wish to query. PixelCNN++~\cite{salimans2017pixelcnn++} is trained on CIFAR10~\cite{krizhevsky2009learning}, a dataset made up of colour images of size ($32\times32\times3$) covering 10 classes of a wide range of natural images. This model has been shown to compress CIFAR10 samples to a low entropy, meaning the model has a good understanding of this kind of images. In order to trust the probabilities employed throughout this paper, we use images of the same kind as in CIFAR10 to ensure our log-likelihoods are accurate. Fig.~\ref{fig:images} shows that cluttered images are succesfully identified as less probable than smooth images by PixelCNN++, consistently with the known average spectrum of natural images~\cite{Clarke81,Atick92,Malo00,simoncelli2001natural}.

\begin{figure}[b!]
    \centering
\hspace{-0.5cm}\includegraphics[width=1.05\columnwidth]{figs/alex-figs/0.2/example_images_3.png}
    \caption{\textbf{The concept: visual sensitivity maybe bigger in more probable images.} Images where $log(p(\x))$ is computed with PixelCNN++~\cite{salimans2017pixelcnn++}), corrupted by uniform noise on the surface of sphere around $\x$ of radius $\epsilon=0.2$, with the same RMSE=0.018 (in [0, 1] range). 
    Due to the \emph{masking phenomenon}~\cite{Martinez19}, noise of the same energy is more visible, i.e. human sensitivity is bigger for smooth images (consistent with the NLPD distance~\cite{laparra2016}), which are also more probable images (see the $log(p(\x))$).}
    \label{fig:images}
\end{figure}


\subsection{Distorted images}

There are two factors to balance when looking at distortions around image $\x$ so that the definition of \emph{perceptual sensitivity}  is meaningful.
First; in order to understand the ratio in Eq.~\ref{eq:sensitivity} as a variation \emph{per unit of euclidean distortion} proposed in~\cite{hepburn2022on}, we need the distorted image $\tilde{\x}$ to be close to the original $\x$. Secondly, the perceptual distances are optimised to recreate human judgements, so if the distortion is too small and humans are not able to see it (or too big so that the image is totally destroyed), the distances are not trustworthy. 
% Additionally, we wish to draw comparisons between literature in the denoising literature which uses additive Gaussian noise across the whole image, which conveniently also allows us to use measures across the whole image, like taking the norm of the gradient. 

As such, we propose to use additive uniform noise on the surface of a sphere around $\x$ of radius $\epsilon$. We choose $\epsilon$ in order to generate $\tilde{\x}$ where the noise is small for humans ($\epsilon=0.2$). Examples of the images and the noisy images can be seen in Fig.~\ref{fig:images}. 
%In the Appendix are results for large amounts of noise, and a non-visible amount of noise. 
Note that in order to use the perceptual distances and PixelCNN++, the images must fall within a range. After adding noise we apply clipping so the range of images is $[-1, 1]$, and only keep the images whose Euclidean distance is comparable. If an image has a large number of pixels that require clipping after noise, the Euclidean distance between $\x$ and $\tilde{\x}$ will be noticeably different to the other images in the dataset. As such we only use images where $\textrm{RMSE}>0.017$, resulting in 48,046 images.




\iffalse
\subsection{Perception and Probability}





The connection between biological response and image probability is a well-studied area in traditional perceptual literature, table \ref{tab:principles} summarises different probability surrogates that have been proposed to influence perceptual sensitivity.  Barlow first proposed the internal representation in the brain is one which minimises redundant information, given a stimuli~\cite{Barlow,Barlow01}. The idea that neural coding is dictated by information theory is further explored in \cite{Laughlin81}, where the univariate contrast-response function of a blowfly's compound eye is shown to be approximated by the cumulative probability function for natural contrasts. This ensures that all response levels are used with equal frequency, which given a neuron's limited response range, ensures that the information represented by the responses is maximised. Rather than relating the PDF directly with the response function, it can be seen that the gradient of the visual non-linearity function is given by the cube root of the PDF of the input signal~\cite{Twer01}. It is also shown that for certain cells, the contrast-response function is matched to the signal distribution for colour. These univariate techniques have been generalised to multivariate distributions of biological responses. In \cite{Malo06b}, a non-parametric expression is derived for the non-linear behaviour observed in the primary visual cortex (V1) using independent component analysis (ICA). The nonlinearities in V1 are therefore given by the statistical properties of natural images, although an explicit expression is not given. Further work attempts to explain goth the nonlinearities as well as adaptation under observation using a manifold learning technique in order to capture psychophysical behaviour in colour perception~\cite{Laparra12}. The same technique can be applied to explain visual aftereffects; changes in visual perception after adaptation to a previous stimulus~\cite{Laparra15}.

Recently, deep learning models have been used to explore the relationship between the underlying data-generating distribution, and the learned parameter models using score matching. For denoising and more generally contractive autoencoders~\cite{Vincent11, Alain14}, it is stated that the derivative of the log-likelihood with respect to the input data is given by the residual, the difference between the original $\x$ and the reconstruction $\hat{\x}$. This indicates that moving in the direction of the log density from the reconstruction will recover the original data point. With denoising autoencoders with known Gaussian noise of $\sigma$ added, a trained autoencoder can perform denoising in one step in the direction of the derivative of the log-likelihood~\cite{kadkhodaie2020solving}, where the step size is, dictated by the residual. This has also been used to sample images from an implicit prior in a denoising autoencoder, as well as solve a variety of linear inverse problems. This relationship can also be written in terms of the noisy data distribution~\cite{Miyasawa61, Raphan11}. Starting from a trained denoising autoencoder, we can recover properties of the data generative distribution $p(\x)$:
\begin{equation}
    J(x) = \frac{\partial log(p(\x))}{\partial \x} = \frac{(\x -\hat{\x})}{\sigma^2}
    \label{ec:score_matching}
\end{equation}
In \cite{hepburn2022on}, it is shown a proportional relation between perceptual distance and the probability of the original image:
\begin{equation}
     S(\x,\tilde{\x}) \,\approx\, p(\x)^\gamma,
\end{equation}
this result can also be interpreted as uniformisation or information maximisation $(\gamma = 1)$\cite{Laughlin81} and error minimisation $(\gamma=\frac{1}{3})$\cite{Twer01}. This relationship is shown empirically using a density model PixelCNN++~\cite{salimans2017pixelcnn++} and using additive Gaussian noise as a distortion. Whilst a functional form was not proposed, the correlation between perceptual sensitivity and probability is shown, and maximised when the two images are closest. It is also shown that perceptual distances have some understanding of the probability, using inferred distances to train machine learning models and analysis the learned representations. 

Besides the previous proposals is also interesting to analyse how much probability there is between the original and the distorted images:
\begin{equation}
    \int^{\tilde{\x}}_\x log(p(\x)) d\x \approx \frac{log(p(\x))+ log(p(\tilde{\x}))}{2}.
\end{equation}

And also the gradient of the log-likelihood at $\x$, in the direction of $\tilde{\x}$: 
\begin{equation}
    \overrightarrow{J}_{\tilde{\x}}(\x) = \nabla_{p(\tilde{\x})} log(p(\x)) \label{eq:dir_gradient}
\end{equation}

As one can see, there are a number of \emph{probability surrogates} that have reasonable expectations for being related to probability. We propose to examine a suitable number of different characteristics of the PDF, and analyse which surrogate provides us with the most information about perceptual behaviour. The \emph{probability surrogates} we use in our analysis are these 6:
\begin{equation}
log(p(\x)), log(p(\tilde{\x})), ||J(x)||, ||J(\tilde{x})||, \sigma(x), \mu(x)
\end{equation}

With them, or combinations of them, we cover all the proposals in table \ref{tab:principles}. Whilst many different works establish relationships independently between the data generative distribution, machine learning models, and some perceptual behaviour, none have directly quantified perceptual behaviour in terms of the probability of images.


\subsection{Image Quality metrics}

In traditional perceptual literature, perceptual distances or image quality metrics (IQMs) are hand-designed models inspired by visual psychology and understanding of the underlying biological mechanisms in the human visual system, like Multi-Scale Structural SIMilarity index (MS-SSIM)~\cite{wang2003multiscale} and Normalised Laplacian Pyramid Distance (NLPD)~\cite{laparra2016}. NLPD and a number of other distances come from the idea of \emph{error of visibility} which states that the errors or differences between two images directly impact the way humans perceive the similarity between the images. Images are transformed into a perceptually meaningful representation, and the perceptual distance is the Euclidean distance in this representation, for example in NLPD, the representation is based on redundancy reduction in neighbouring pixels. NLPD extends the traditional Laplacian pyramid~\cite{burt1983laplacian} with a local normalisation step, similar to divisive normalisation observed in the human visual system~\cite{Carandini12}. In contrast, MS-SSIM and its variants~\cite{wang2004image, zhang2011fsim} are based on the principle of \emph{structural similarity}, where despite the applied distortion, the structure of images $\x$ and $\tilde{\x}$ should be unchanged. In \cite{wang2003multiscale}, it is argued that some distortions such as the multiplication of image intensities globally do not degrade the quality of the image. As such, distances are based on structural similarity by computing statistics over regions of various sizes within the image, rather than performing a pixel-wise comparison.

More recently, several methods have been proposed using feature extractors. Learned Perceptual Image Patch Similarity (LPIPS), sometimes referred to as the VGG distance, uses pretrained deep neural networks trained in classification as feature extractors. In combination with spatial averaging, linear weights are learned for the features extracted, and optimised to correlate well with human perceptual judgements. LPIPS has been shown the be sensitive to minor changes in texture, which is addressed in an extension named Deep Image Structure and Texture Similarity (DISTS). DISTS combines tolerance for texture resampling with structural distortion measures to create state-of-the-art perceptual distance robust to geometric transformations and minor textural differences.

Throughout the paper, we consider five different image quality metrics. As mentioned earlier, we use classic psychophysical models MS-SSIM~\cite{wang2003multiscale} and NLPD~\cite{laparra2016} as well as three deep learning based approaches, PIM~\cite{Bhardwaj2020}, LPIPS~\cite{zhang2018unreasonable} and DISTS~\cite{ding2020image}. Table \ref{tab:MOS} shows how each of the metrics correlated with the mean opinion score (MOS) of various datasets.

% \begin{itemize}
%     \item Perception - maybe some common results in perception or 
%     \item perceptual metric - different methods (traditional, deep learning, error of visibility, structural similarity).
%     \item relationship with statistics (overview of ICLR paper) + maybe to do with compression (p(x)**1/3)
%     \item probability models
% \end{itemize}

\begin{table*}
\footnotesize
\caption{Different surrogates of the PDF of natural images (causal factors / explanations) that have been proposed to predict sensitivity.}\vspace{0.1cm}
\label{tab:principles}
\begin{centering}
% \hspace{-0.75cm}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline & & & & & &\\
%           & & & & & & \\
%  \textbf{Information}  &   \textbf{Internal Noise}   &   \textbf{Surprise}      & \textbf{Acquisition Noise}        & \textbf{Signal Average}             & \textbf{Signal Spectrum}    & \textbf{Marginal Laplacian}     \\
%  \textbf{Transmision} &  Limited Resolution  &                                  &  Denoising             &  First Eigenvalue   &  All Eigenvalues          &  Marginal nonlinearity  \\
%                       & & & & Mean Luminance & Band-pass Contrast &  \\\hline
%  \multirow{2}{*}{Barlow~\cite{Barlow}}   &   \multirow{2}{*}{Lloyd~\cite{Lloyd57}}   & \multirow{2}{*}{Shannon~\cite{Shannon48}}          & \multirow{2}{*}{Miyasawa~\cite{Miyasawa61}}            &   &  \multirow{2}{*}{Michelson~\cite{Michelson27}}    &   \multirow{2}{*}{Simoncelli~\cite{Simoncelli97}}    \\
%  \multirow{2}{*}{Laughlin~\cite{Laughlin81}} &   \multirow{2}{*}{McLeod~\cite{McLeod03}}   & \multirow{2}{*}{Gegenfurtner~\cite{Gegen09}}     & \multirow{2}{*}{Raphan \& Simoncelli~\cite{Raphan11}} & \multirow{2}{*}{Weber\cite{Weber1846}} & \multirow{2}{*}{Campbell \& Robson~\cite{Campbell68}}       & \multirow{2}{*}{Heeger~\cite{Heeger92}}     \\
% \multirow{2}{*}{Bell et al.~\cite{Bell97}}   &   \multirow{2}{*}{Twer et al.~\cite{Twer01}}     & \multirow{2}{*}{Bruce \& Tsotsos~\cite{Bruce06}} & \multirow{2}{*}{Vincent~\cite{Vincent11}} & \multirow{2}{*}{Fechner\cite{Fechner1860}} & \multirow{2}{*}{Peli~\cite{Peli90}}        &              \multirow{2}{*}{Daly~\cite{Daly90}}  \\
% & & & & & & \multirow{2}{*}{Malo et al.~\cite{Malo2006v1}} \\\cline{1-2}
% \multicolumn{2}{|c|}{ \multirow{2}{*}{Laparra et al.~\cite{Laparra2012,Laparra15}} }  &  &  &  &   & \\
% &&&&&&\\\cline{1-3}
% & & & & & & \\
% \multicolumn{3}{|c|}{ \multirow{1}{*}{Hepburn et al.~\cite{hepburn2022on}} } & & & &                                                                                   \\
% &&&&&&\\\hline
%   & & & & \multirow{6}{*}{$\mu(\x)$} & \multirow{6}{*}{$\frac{1}{\mu(\x)} \,\, \Sigma(\x)$} & \multirow{3}{*}{$\frac{1}{\mu(\x)} \,\, B \cdot log(\lambda) \cdot B^\top$} \\
%  \multirow{4}{*}{$p(\x)$}  & \multirow{4}{*}{$p(\x)^{\frac{1}{3}}$} & \multirow{4}{*}{$p(\x)^{-1}$}     &  \multirow{6}{*}{$\frac{\nabla_\x p(\x)}{p(\x)} = \nabla_\x log(p(\x))$}   &     &  & \\
%  & & & & \multirow{6}{*}{$log(\mu(\x))$} & \multirow{6}{*}{$\frac{1}{\mu(\x)} \,\, B \cdot \lambda \cdot B^\top$} & \multirow{3}{*}{$\bigintsss_{\,\x}^{\mathbf{\hat{x}}} p(\x') \, d\x'$}\\
%  \multirow{4}{*}{$log(p(\x))$}  & \multirow{4}{*}{$\frac{1}{3}log(p(\x))$} & \multirow{4}{*}{$-log(p(\x))$}   &   &      &  &  \\
%  & & & & & & \\
%  & & & & & & \multirow{1}{*}{$\bigintsss_{\,\x}^{\mathbf{\hat{x}}} log( p(\x') ) \, d\x'$} \\
%  & & & & & &  \\
%   & & & & & &  \\\hline
% \end{tabular}
\begin{airytabular}{|ccc|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Information}\\ \textbf{Transmission}\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Internal Noise}\\ Limited Resolution\end{tabular}} &
  \textbf{Surprise} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Acquisition Noise}\\ Denoising\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Signal Average}\\ First Eigenvalue\\ Mean Luminance\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Signal Spectrum}\\ All Eigenvalues\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textbf{Marginal Laplacian}\\ Marginal nonlinearity\end{tabular} \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[t]{@{}c@{}}Barlow~\cite{Barlow}\vspace{0.1cm}\\ Laughlin~\cite{Laughlin81}\vspace{0.1cm}\\ Bell et al.~\cite{Bell97}\vspace{0.1cm}\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[t]{@{}c@{}}Lloyd~\cite{Lloyd57}\vspace{0.1cm}\\ McLeod~\cite{McLeod03}\vspace{0.1cm}\\ Twer et al.~\cite{Twer01}\vspace{0.1cm}\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Shannon~\cite{Shannon48}\vspace{0.1cm}\\ Gegenfurtner et al.~\cite{Gegen09,Wichmann02color}\vspace{0.1cm}\\ Bruce \& Tsotsos~\cite{Bruce06}\vspace{0.1cm}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Miyasawa~\cite{Miyasawa61}\vspace{0.1cm}\\ Raphan \\ \& Simoncelli~\cite{Raphan11}\vspace{0.1cm}\\ Vincent~\cite{Vincent11}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Weber~\cite{Weber1846}\vspace{0.1cm}\\ Fechner~\cite{Fechner1860}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Michelson~\cite{Michelson27}\vspace{0.1cm}\\ Campbell \\ \& Robson~\cite{Campbell68}\vspace{0.1cm}\\ Peli~\cite{Peli90}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Simoncelli~\cite{Simoncelli97}\vspace{0.1cm}\\ Heeger~\cite{Heeger92}\vspace{0.1cm}\\ Daly~\cite{Daly90}\vspace{0.1cm}\\ Malo et al.~\cite{Malo2006v1}\end{tabular}} \\ \cline{1-2}
\multicolumn{2}{|c|}{Laparra et al.~\cite{Laparra2012,Laparra15}} &  &  &  &  &  \\ \cline{1-3}
\multicolumn{3}{|c|}{Hepburn et al.~\cite{hepburn2022on}}    &  &  &  &  \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}$p(\x)$\\ $log(p(\x))$\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$p(\x)^{\frac{1}{3}}$\\ $\frac{1}{3}log(p(\x))$\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}$p(\x)^{-1}$\\ $-log(p(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\frac{\nabla_\x p(\x)}{p(\x)}$\\ $ = \nabla_\x log(p(\x))$\end{tabular}&
  \begin{tabular}[c]{@{}c@{}}$\mu(\x)$\\ $log(\mu(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\frac{1}{\mu(\x)}\Sigma(\x)$\\ \\ $\frac{1}{\mu(\x)} B \cdot \lambda \cdot B^\top$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\bigintsss_{\,\x}^{\mathbf{\hat{x}}} p(\x') d\x'$\\ $\frac{1}{\mu(\x)} B \cdot log(\lambda) \cdot B^\top$\\ $\bigintsss_{\,\x}^{\mathbf{\hat{x}}} log( p(\x') ) d\x'$\end{tabular} \\ \hline
\end{airytabular}
\end{centering}
\end{table*}



% \begin{figure*}
%     \centering
%     \begin{tabular}{cccc}
%       Probability & Derivative Prob (Score Matching Norm) & Integral path Prob & Directional derivative Prob\\  
%      $p(x)$ & $|| \frac{\partial log(p(\x))}{\partial x}||$  & $\int_x^{\tilde{x}} p(x) dx$ & $\nabla_{p(\tilde{x})} \frac{\partial p(x)}{\partial x}$\\  \\
%       0.51 & -0.02 & 0.51    &  -0.03 \\ \\
%      \includegraphics[width=3.5cm]{figs/NLPD_vs_prob_hist_norm.png}
%     & \includegraphics[width=3.5cm]{figs/NLPD_vs_der_prob_hist_norm.png}
%     & \includegraphics[width=3.5cm]{figs/NLPD_vs_int_path_hist_norm.png}
%     & \includegraphics[width=3.5cm]{figs/NLPD_vs_direct_deriv_hist_norm.png} \\
%     \end{tabular}
%     \caption{NLPD Sensitivity of different image quality metrics with regard to probability surrogates. $\rho$-Spearman is included in the second row. Conditional histograms are shown in gray scale (bigger values ar brighter). NOTE: Here we have $log(p(\x))$.}
%     \label{fig:log_prob_freq_contrast}
% \end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/cond-1-MS-SSIM.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/cond-NLPD.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/cond-PerceptNet.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/cond-LPIPS.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/cond-DISTS.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/cond-PIM.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/cond-BRISQUE.png}
%     \caption{$\sigma=0.2$ (barely visible noise)}
% \end{figure*}



\iffalse
Capturing all human perceptual behaviour and probability of all natural images are both intractable, however, we have surrogates that are accurate in regions of the space. As surrogates for human perceptual judgements, perceptual distances provide us with an accurate representation of how humans would judge the distance between two images. Whilst not perfect, collating a number of different image quality metrics (IQMs) and ensuring the behaviour is consistent across the set would infer that humans would behave the same way. 
\fi

\subsection{Estimating the probability}
\label{sec:est_prob}
For probability, most recent generative models aim at capturing the PDF around certain images, for example, human faces or bedrooms~\cite{kingma2018glow}. However, not only do these sets not cover natural images, they are made up of a rigid structure when describing images as a whole. Rather, we follow the methodology in ~\cite{hepburn2022on} and rely on a probability model which we know is accurate for the images we wish to query. PixelCNN++~\cite{salimans2017pixelcnn++} is trained on CIFAR10~\cite{krizhevsky2009learning}, a dataset made up of colour images of size ($32 \times 32 \times 3$) covering 10 classes of images. It has been shown to compress images of this dataset to a low entropy, meaning the model has a good understanding of this particular dataset. In order to trust the probability values employed throughout this paper, we use the same images from CIFAR10 to ensure our log-likelihood values are accurate. 

\subsection{Generating distorted images}

There are two factors to balance when looking at distortions around image $\x$. Namely; in order to make the assumption valid, we need image $\x$ to be close to distorted image $\tilde{\x}$. Secondly, the perceptual distances are optimised to recreate human judgements, so if the distortion is too small and humans are not able to see it, the distances are not trustworthy. 
\iffalse
Additionally, we wish to draw comparisons between literature in the denoising literature which uses additive Gaussian noise across the whole image, which conveniently also allows us to use measures across the whole image, like taking the norm of the gradient. 
\fi 
As such, we propose to use additive uniform noise on the surface of a sphere around $\x$ of radius $\epsilon$. We choose $\epsilon$ in order to generate $\tilde{\x}$ where the noise is barely visible to humans ($\epsilon=0.2$). Examples of the images and the noisy images can be seen in Fig.~\ref{fig:images}. 
%In the Appendix are results for large amounts of noise, and a non-visible amount of noise. 
Note that in order to use the perceptual distances and PixelCNN++, the images must fall within a range. After adding noise we apply clipping so the range of images is $[-1, 1]$, and we only keep the images whose Euclidean distance is comparable. If an image has a large number of pixels that require clipping after noise, the Euclidean distance between $\x$ and $\tilde{\x}$ will be noticeably different from the other images in the dataset. As such we only use images where $\textrm{RMSE}>0.017$, resulting in 48,046 images.


%We use the cifar10 dataset which are $50.000$ color images ($32 \times 32 \times 3$). This dataset is selected because is the same used to train the probability model employed, PixelCNN+ \cite{}. It is important in order to trust the probability values employed in the analysis.  

%The noise is additive and generated on a sphere of radius $\epsilon$. After adding the noise we apply a clipping function in order to keep the images in values $[-1,1]$. The $\epsilon$ value was selected in order to generate a noisy image where the noise is barely visible. Examples of the images and the noisy images can be seen in Fig.\ref{fig:images}.  

%As stated before, we used the PixelCNN+ model \cite{} in order to estimate the probability and the derivative of the probability of the original and the noisy images. 

%We progressively increase the dimensionality of the factors, for example, if $p(\x)$ has the highest mutual information, we then construct 2-dimensional distributions, where one dimension is $p(\x)$. We then see what is the next most important factor. We

% We analyzed the sensitivity of five different image quality metrics (IQMa): 
% \begin{itemize}
%     \item A classic method, the multiscale structural similarity index MS-SSIM~\cite{wang2003multiscale}, which is the multiscale version of the SSIM~\cite{Wang2004}.
%     \item A method that implements a model that imitates the retina and lateral geniculate nucleus of the human visual system, the normalized laplacian pyramid distance (NLPD)~\cite{laparra2016}.
%     \item  A model based on information theory, Information-Theoretic Perceptual Quality Metric, PIM~\cite{Bhardwaj2020}.
%     \item A model based on using the features extracted by a pretrained neural network, Learned Perceptual Image Patch Similarity, LPIPS~\cite{zhang2018unreasonable}.
%     \item A model based on statistics DISTS of a pretrained neural network~\cite{ding2020image}
% \end{itemize}

% \begin{table}[]
% \footnotesize
% \caption{Correlations, Pearson (Spearman), of each IQM with MOS in different image quality assessment databases.}
% \label{tab:MOS}
% \begin{tabular}{|l|l|l|l|l|l|}
% \hline
%  & \textbf{MSSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} \\ \hline
% TID2018    & 0.78 (0.80)    &   0.81 (0.82) &              &   0.74 (0.75)        &                \\ \hline
% TID2013    & 0.78 (0.80)    &   0.82 (0.81) &              &  (0.67)              &  (0.81)        \\ \hline
% KADID10k   & 0.80 (0.80)    &               &              &  (0.72)              &  (0.88)        \\ \hline     
% \end{tabular}
% \end{table}

\begin{table}[]
\centering
\footnotesize
\caption{Comparison of each IQM. For TID2013 the Pearson $\rho_p$ (Spearman $\rho_s$) correlation between mean opinion score (MOS) and distance and for BAPPS, the \% of images the human observers and the distance agree.}
\label{tab:MOS}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
                                                                        & \textbf{MSSSIM}                                       & \textbf{NLPD}                                         & \textbf{PIM}                                          & \textbf{LPIPS}                                        & \textbf{DISTS}                                        \\ \hline
\begin{tabular}[c]{@{}l@{}}TID2013\\ $\rho_p$ ($\rho_s$)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.83\\ (0.79)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.84\\ (0.80)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.62\\ (0.65)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.74\\ (0.67)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.86\\ (0.83)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}BAPPS\\ (\%)\end{tabular}                    & 61.7                                                  & 61.5                                                  & 64.5                                                  & 69.2                                                  & 69.0                                                  \\ \hline
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/alex-figs/0.2/example_images.png}
    \caption{Example images with barely visible noise, $\epsilon=0.2$.}
    \label{fig:images}
\end{figure}


\fi



\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-1-MS-SSIM.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-NLPD.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-PIM.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-LPIPS.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-DISTS.png}
    \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/colorbar.png}
    \label{fig:cond_hist}
    \caption{Conditional histograms where the x-axis is the probability surrogate (or image descriptor), and y-axis is perceptual sensitivity (Eq.~\ref{eq:sensitivity}) with  just visible noise ($\epsilon=0.2$). Also shown is the Spearman correlation between probability surrogate and perceptual sensitivity.}
\end{figure*}

\section{Experiments}
Firstly, we empirically show the behaviour of the perceptual sensitivity and probability using conditional histograms and use measures from information theory to show which probability surrogates are most related to perceptual sensitivity. Then we explore unconstrained polynomial combinations of these surrogates using machine learning regression models. Lastly, constrain ourselves to only consider the most important probability surrogates and identify simple functional forms to predict perceptual sensitivity.

\subsection{Conditional Histogram}
\label{sec:cond_hist}

Conditional histograms (Fig.~\ref{fig:cond_hist}) allow us to inspect the conditional distribution. In this case, we calculate $\mathcal{P}[S\in [b_{j-1}, b_j] | X=x]$ where $S$ is the perceptual sensitivity partitioned into $m=30$ bins $[b_{j-1}, b_j)$, $X$ is one of the possible probability surrogates. This allows us to visually inspect which of the probability surrogates or image descriptors are important for predicting sensitivity. The probability surrogate used is given in each subplot title alongside the Spearman correlation between perceptual sensitivity and the surrogate. 

For all perceptual distances, $log(p(\x))$ has a high correlation, and mostly follows a similar conditional distribution. NLPD is the only distance that significantly differs, with more sensitivity in mid-probability images. We also see a consistent increase in correlation and conditional means when looking at $log(p(\tilde{\x}))$, meaning the log-likelihood of the noisy sample is more indicative of perceptual sensitivity for all tested distances. Also note that the standard deviation $\sigma(\x)$ also has a strong (negative) correlation across the traditional distances, falling slightly with the deep learning based approaches. This is likely due to the standard deviation being closely related to the contrast of an image~\cite{Michelson27}. Note that measures that take into account both points, $\int^{\tilde{\x}}_\x log(p(\x))d\x$ and $\overrightarrow{J}_{\tilde{x}}(\x)$, have lower correlation than just taking into account the noisy sample, with the latter being insignificant in predicting perceptual sensitivity.

\subsection{Mutual Information Analysis}
\label{sec:MI}
 
To quantify the probability surrogates ability to predict perceptual sensitivity, we use information theoretic measures. Firstly, we use mutual information which avoids the definition of a particular functional model and functional forms of the features. This analysis will give us insights into which of the factors derived from the statistics of the data can be useful in order to later define a functional model that relates statistics and perception.

Here, we analyse the mutual information between the different probability surrogates and the sensitivity of the distances. The idea is to find which surrogates will allow us to better predict perceptual sensitivity. The mutual information has been computed using all 48,046 samples and using the Gaussianisation technique rotation based iterative Gaussianisation (RBIG)~\cite{laparra2011iterative} as detailed here \cite{laparra2020information}. Instead of the mutual information value, we report the Information coefficient of correlation~\cite{Linfoot57} (ICC) since the interpretation is similar to the Pearson coefficient and allows for easy comparison.

Figure~\ref{fig:MI_1D} shows the ICC between each isolated probability surrogate and the sensitivity of different IQMs. It is clear that the most important factor to take into account in most models (second in MS-SSIM) is the probability of the noisy image $\log(p(\tilde{\x}))$, a consistent result with the conditional histograms.
\begin{figure}[tb]
    \centering
    \includegraphics[width=.95\columnwidth]{figs/MI/ICC_1D_prob_surrogates_vs_sensitivity_no_percept.pdf}
      \caption{Information coefficient of correlation (ICC) between proposed probability surrogates and the sensitivity of different perceptual distances.}
      \label{fig:MI_1D}
\end{figure}
Once we select the $log(p(\tilde{\x}))$ as the most important factor, we have to explore which other surrogate should be included as the second term. In order to do so we analyse the mutual information between each possible pair combination of surrogates with each IQM, results are shown in Fig.~\ref{fig:MI_2D}. It is clear that on the one hand the pairs where $log(p(\tilde{\x}))$ is involved have the maximum mutual information as concluded before, and the maximum is achieved when combining with the standard deviation of the original image $\sigma(\x)$. This is true for four out of five models, again MS-SSIM is the one that differs.

\begin{figure}
\footnotesize
    \centering
    \begin{tabular}{cccc}
    \multicolumn{4}{c}{MS-SSIM} \vspace{-0.1cm} \\ 
     \multicolumn{4}{c}{\includegraphics[width=.75\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_MS-SSIM.pdf}}
     \\ NLPD & PIM & LPIPS & DISTS 
    \\ \includegraphics[width=.2\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_NLPD.pdf}
    & \includegraphics[width=.2\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_PIM.pdf}
    &
    \includegraphics[width=.2\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_LPIPS.pdf}
    & \includegraphics[width=.2\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_DISTS.pdf}  \\
    \end{tabular}
    \caption{ICC between pairs of probability surrogates and the sensitivity of different metrics. The top figure shows the correspondence between rows and columns, and surrogates. The color bar is the same for all figures.}
    \label{fig:MI_2D}
\end{figure}

Now we consider the third and the fourth factors to include. Fig~\ref{fig:MI_3D} shows the ICC when taking into account 3 factors. It is clear that the factor that adds more to the ICC is $log(p(\x))$. In the case of the fourth factor (Fig.~\ref{fig:MI_4D}), the one that is more relevant is $\mu(\x)$. Therefore we have this ordering of factors to take into account; $\{\log(p(\tilde{\x})), \sigma(\x), \log(p(\x)), \mu(\x)\}$.
%the probability of the noisy image $\log(p(\tilde{\x}))$, the standard deviation of the luminance of the original image $\sigma(\x)$, the probability of the original image $\log(p(\x))$, and the mean luminance of the original image $\mu(\x)$.
\begin{figure}
    \centering
    \begin{tabular}{c}
     \includegraphics[width=.79\columnwidth]{figs/MI/ICC_3D_plot_pxn_plus_Std_plus_vs_sensitivity.pdf}
    \end{tabular}
    \caption{ICC 3 factors. Analysis of the shared information between sensitivity and 3 probability surrogates; $\{log(p(\x)), \sigma(\x), f_s\}$ where $f_s$ is one of the 4 possible probability surrogates in the y-axis. } %Mutual information is normalised for each IQM.
    \label{fig:MI_3D}
\end{figure}
\begin{figure}
    \centering
    \begin{tabular}{c}
     \includegraphics[width=.8\columnwidth]{figs/MI/ICC_4D_plot_pxn_plus_Std_plus_vs_sensitivity.pdf} 
    \end{tabular}
    \caption{ICC 4 factors. Analysis of the shared information between sensitivity and 4 probability surrogates; $\{log(p(\x)), \sigma(\x), log(p(\x)), f_s\}$ where $f_s$ is one of the 3 possible probability surrogates in the y-axis.} %Mutual information is normalised for each IQM.
    \label{fig:MI_4D}
\end{figure}
Table~\ref{tab:MI} shows the summary of the values of the ICC. In particular, we see that using only the $log(p(\tilde{\x}))$ we can capture between $[0.55-0.71]$ of the information. The difference between using one factor or more is not big. 
 
 In the following sections, we analyse the relations using machine learning regression models and study possible functional forms that include one or two factors and their effect on the predictability of the perceptual sensitivity. 
 
\begin{table}[]
\footnotesize
\caption{Information coefficient of correlation~\cite{Linfoot57} (ICC) between the sensitivity and the probability surrogates. Each row includes the factor that maximises the ICC, 1D: $log(p(\tilde{x})$, 2D: \{$log(p(\tilde{x})$, $\sigma(x)$\}, 3D: \{$log(p(\tilde{x})$, $\sigma(x)$, $log(p(\x))$\}, 4D: \{$log(p(\tilde{x})$, $\sigma(x)$, $log(p(\x))$, $\mu(x)\}$, 5D: \{$log(p(\tilde{\x}))$, $\sigma(x)$, $log(p(\x))$, $\mu(x)$, $||J(x)||$\}, and 6D all of them.}
\begin{tabular}{lllllll}
\\ \textbf{Factors}                   & \textbf{MS-SSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} & \textbf{mean} \\
1D  & 0.55 & 0.57 & 0.71 & 0.68 & 0.61 & 0.62\\
2D  & 0.57 & 0.66 & 0.72 & 0.69 & 0.63 & 0.65\\
3D  & 0.68 & 0.68 & 0.72 & 0.69 & 0.65 & 0.68\\
4D & 0.68 & 0.76 & 0.75 & 0.71 & 0.66  & 0.71 \\
5D & 0.68 & 0.78 & 0.75 & 0.73 & 0.66  & 0.72  \\
6D & 0.71 & 0.79 & 0.76 & 0.73 & 0.66  & 0.73          
%all & 0.26286881 0.44154917 0.35763207 0.31760573 0.26177006
\end{tabular}
\label{tab:MI}
\end{table}



\iffalse
\begin{tabular}{llllll}
\\ \textbf{Factors}                   & \textbf{MS-SSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} \\
1D & 0.52 & 0.60 & 0.74 & 0.71 & 0.65 \\
2D & 0.61 & 0.70 & 0.76 & 0.73 & 0.67 \\
3D & 0.63 & 0.69 & 0.76 & 0.73 & 0.67 \\
4D & 0.68 & 0.79 & 0.76 & 0.73 & 0.69      
%all & 0.26286881 0.44154917 0.35763207 0.31760573 0.26177006
\end{tabular}




Mutual information 
$p_{xn}$                          & 0.177            & 0.196         & 0.346        & 0.311          & 0.236          \\
$[p_{xn}, \sigma(x)]$              & 0.199            & 0.29          & 0.368        & 0.317          & 0.258          \\
$[p_{xn}, \sigma(x), p_x]$         & 0.313            & 0.307         & 0.368        & 0.323          & 0.273          \\
$[p_{xn}, \sigma(x), p_x, \mu(x)]$ & 0.236            & 0.429         & 0.415        & 0.348          & 0.29    
\fi

\subsection{Regression}
\label{sec:ML_regres}
We explore simple interpretable models and how probability surrogates can be combined in order to predict perceptual sensitivity. In order to do so, we fit a random forest regressor~\cite{breiman2001random}, with the probability surrogates and polynomial combinations with order 2, and optimise the regressor to predict the perceptual sensitivity. The inverse probability for both original and noisy images is also included. Regression trees are convenient for this task since it is easy to analyse the relevance of each feature and compare between models trained on different perceptual sensitivities. Feature importances are normalised so that they sum to $1$ for each model for easy comparison. We use a held out test set of 30\% dataset in order to calculate correlations between predicted and ground truth. Figure \ref{fig:regres_tree} shows the 6 probability surrogates with the most importance across perceptual distances and their relevant importance for each IQM, and the Pearson (Spearman) correlation that indicates how good the model is at predicting perceptual sensitivity. The average Pearson correlation is 0.85, this can be compared with models proposed in Section~\ref{sec:func_form}. It can be seen that $log(p(\tilde{\x}))$ is by far the most important which agrees with what was found in the mutual information analysis (sec.~\ref{sec:MI}). It has been suggested that the derivative of the log-likelihood should be important, given those modifications in the slope of the distribution imply label change and the score-matching objective makes use of the gradient~\cite{Vincent11}. However, we find that the derivative has low shared mutual information and low feature influence.

% \begin{figure*}
%     \centering
%     \includegraphics[width=.46\textwidth]{figs/alex-figs/0.4/regression/RMSE.png}
%     \includegraphics[width=.46\textwidth]{figs/alex-figs/0.4/regression/1-MS-SSIM.png} \\
%     \includegraphics[width=.46\textwidth]{figs/alex-figs/0.4/regression/NLPD.png}
%     \includegraphics[width=.46\textwidth]{figs/alex-figs/0.4/regression/PerceptNet.png} \\
%     \includegraphics[width=.46\textwidth]{figs/alex-figs/0.4/regression/LPIPS.png}
%     \includegraphics[width=.46\textwidth]{figs/alex-figs/0.4/regression/DISTS.png} \\
%     \includegraphics[width=.46\textwidth]{figs/alex-figs/0.4/regression/PIM.png}
%     \caption{Decision tree regression on polynomial features (y-axis) for predicting the perceptual distance for $\sigma=0.4$. All polynomial features are scaled to $[0, 1]$ and the perceptual distances are transformed to 0 mean and unit variance.}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=.32\textwidth]{figs/alex-figs/0.4/error/err-1-MS-SSIM.png}
%     \includegraphics[width=.32\textwidth]{figs/alex-figs/0.4/error/err-NLPD.png}
%     \includegraphics[width=.32\textwidth]{figs/alex-figs/0.4/error/err-PerceptNet.png} \\
%     \includegraphics[width=.32\textwidth]{figs/alex-figs/0.4/error/err-LPIPS.png}
%     \includegraphics[width=.32\textwidth]{figs/alex-figs/0.4/error/err-DISTS.png}
%     \includegraphics[width=.32\textwidth]{figs/alex-figs/0.4/error/err-PIM.png}
%     \caption{Decision tree regression on polynomial features for predicting the perceptual distance for $\sigma=0.4$. Showing predicted distance (y-axis) and true distance (x-axis).}
% \end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=.85\textwidth]{figs/alex-figs/0.2/regression/together.png}
    \caption{Top 6 feature importances from a Random Forest regressor trained on polynomial combinations of the probability surrogates in order to predict perceptual sensitivity. A separate model was trained for each perceptual distance. In the legend we include the Pearson (Spearman) correlation between the predictions and ground truth for a held out test set 30\% of the dataset.}
    \label{fig:regres_tree}
\end{figure*}


\section{Functional relation}
\label{sec:func_form}
Given the mutual information results (Sec.~\ref{sec:MI}) and the machine learning regression exploration (Sec.~\ref{sec:ML_regres}), here we explore the actual functional form that can extract that information. As a reference, the Random Forest regressor fit with all the factors and polynomial combinations up to factor 2 (Sec.~\ref{sec:ML_regres}) obtains an average correlation among the IQMs of $0.84$, which can be taken as an upper bound. We conduct an ablation study, training a linear regressor on the most important factors and ablating each factor in order to create various closed form expressions of perceptual sensitivity. We keep factors that maximise the Pearson correlation between the model's predictions and the ground truth.


\subsection{One-factor model}
\label{sec:1D}

The factor with the highest ICC and feature importance is $log(p(\tilde{\x}))$. We explore different polynomials to predict sensitivity where the different factors are forms of $p(\tilde{\x})$. Results are shown in Table~\ref{tab:1D}. The correlation goes from around 0.68 for models with only one degree (i.e. $S(\x,\tilde{\x}) = w_0 + w_1 log(p(\tilde{\x}))^\gamma$ where $\gamma = \{1,\frac{1}{10},\frac{1}{5},\frac{1}{3},\frac{1}{2},2,-1\}$), to 0.73 which can be obtained with a simple polynomial of degree two ($d=2$). Therefore the selected candidate would be:
\begin{equation}
    S(\x,\tilde{\x}) = w_0 + w_1~log(p(\tilde{\x})) + w_2~log(p(\tilde{\x}))^2, 
\label{eq:1_factor}
\end{equation}
with an average correlation of 0.73. The exact weights for each perceptual distance can be seen in the Appendix. 

\begin{table}[]
\footnotesize
\caption{Pearson correlation obtained between the prediction of the model and the sensitivity for different IQMs. All models are designed using versions of $log(p(\tilde{\x}))$ as input factor, the models are polynomials of different degrees (d) or polynomials with specific exponents. The degrees of the polynomial or the exponents ($\gamma$) employed are given in the first column. The model \emph{Frac*} uses as exponents ($\gamma $): {[}0â3,0â2,0â1,0,1,2,3{]}.}
    \label{tab:1D}
\begin{tabular}{lllllll}
\\         & \textbf{MSSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} & \textbf{mean} \\
d = 1                       & 0.7            & 0.63          & 0.65         & 0.68           & 0.72           & \textbf{0.68} \\
d = 2                     & 0.76           & 0.65          & 0.75         & 0.76           & 0.74           & \textbf{0.73} \\
d = 3                     & 0.76           & 0.65          & 0.75         & 0.75           & 0.74           & \textbf{0.73} \\
d = 6                     & 0.75           & 0.64          & 0.73         & 0.74           & 0.74           & \textbf{0.72} \\
\emph{Frac*} & 0.76           & 0.65          & 0.76         & 0.76           & 0.74           & \textbf{0.73} \\
$\gamma$ = 1/10                       & 0.71           & 0.64          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 1/5                      & 0.71           & 0.64          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 1/3                       & 0.71           & 0.64          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 1/2                       & 0.71           & 0.63          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 2                       & 0.69           & 0.63          & 0.64         & 0.68           & 0.71           & \textbf{0.67} \\
$\gamma$ = -1                       & 0.72           & 0.65          & 0.66         & 0.71           & 0.73           & \textbf{0.69} \\
\end{tabular}
\end{table}


\subsection{Two-factors model}
\label{sec:2D}

The two factors that contain the most information about the sensitivity are $log(p(\tilde{\x}))$ and $\sigma(\x)$. We explore multiple possible functional forms that include combinations of both factors. Note that the possible functional form candidates are intractable. Here we restrict ourselves to polynomials that include the simpler versions of these factors isolated, i.e. $\{ log(p(\tilde{\x})), log(p(\tilde{\x}))^2 \}$  and $\{ \sigma(\x), \sigma(\x)^2, \sigma(\x)^{-1}\}$, and the simplest products and divisions using both.

The analysis of multiple models that include these combinations as well as a LASSO~\cite{tibshirani1996regression} exploration can be seen in the Appendix. In conclusion, a model that combines good predictions and simplicity is the one that adds the $\sigma(\x)$ factor to the model suggested in the previous Section~\ref{sec:1D}. This model obtains a $0.79$ mean correlation:
\begin{equation}
    S(\x,\tilde{\x}) = w_0 + w_1~log(p(\tilde{\x})) + w_2~log(p(\tilde{\x}))^2 + w_3~\sigma(\x)
\label{eq:2_factors}
\end{equation}
This implies an increase of $0.06$ in correlation in regard to the model that only includes $log(p(\tilde{\x}))$. As a reference, a model that includes all nine analysed combinations for the two factors obtains a $0.81$ correlation.

\section{Conclusion}
 We show that the probability surrogates and the perceptual sensitivity of various IQMs share substantial information, with a mean ICC of 0.73. Alongside this, a Random Forest regressor that predicts sensitivity using only polynomial combinations of simple probability surrogates can obtain an average Pearson correlation between the predicted sensitivity and the ground truth of 0.85 (Sec.~\ref{sec:ML_regres}). These numbers are surprisingly high given that it is clear that not only the statistics of the input data are going to determine the perceptual behaviour. For instance, different animals in the same environment have different visual perception~\cite{Erichsen2012}. In contrast to existing literature~\cite{Vincent11}, we find that the derivative has low shared mutual information and low feature influence. The most relevant factor found is the probability of the noisy image, $log(p(\tilde{\x}))$. Intuitively this makes sense, since this measure simultaneously includes information on the distribution of the original image (since we are in a differential context so the two log-likelihoods should be similar), and information about the specific direction the included noise has affected the distribution. This isolated factor obtains an average ICC of 0.62, and a simple model including this factor to predict sensitivity achieves a 0.73 Pearson correlation with the ground truth. This model is a simple polynomial of degree 2. Given the mutual information analysis, the factors can be ordered by relevance as: $\{log(p(\tilde{\x})), \sigma(\x), log(p(\x)), \mu(\x), ||J(\x)||\}$. This is in agreement with the relevance given by a regression tree model that uses the same features, including polynomial combinations of them. 

%When discussing which factors are more important, it is surprising that the derivative of the distribution has low influence. 
%It has been suggested that it should have relevance since, in a classification context, modifications in the slope of the distribution probably imply label change~\cite{Vincent11}. The influence of the derivative is also present in the score-matching objective which seems to determine an autoencoder's sensitivity~\cite{Vincent11}. 

After an ablation study using the factors with the highest ICC values, we propose a simple functional form (Eq.~\ref{eq:1_factor}) using only the probability of the noisy image $log(p(\tilde{\x}))$ that obtains 0.73. Adding the second most relevant factor, the standard deviation of the original image $\sigma(\x)$, increases the correlation up to 0.79 (Eq.~\ref{eq:2_factors}).

This study is limited on the one hand by the probability model used (PixelCNN++), which was trained using small images, a restricted set of images (CIFAR10), with a restricted luminance range. In order to have more trustable results in terms of image probability we limited ourselves to using the same type of images that the model was trained for. On the other hand, as a proxy for human perception we have to use IQMs. Although their correlation with MOS is not bad (see Table~\ref{tab:MOS}), these models are far from perfect. The ideal situation would be experimenting with humans, a simple way could be using image quality databases (such as TID2013 or KADID10k) but the images are too big for probability models currently. Besides in these databases, the number of images is smaller and the experiments are not done in a well controlled setup (for instance it is not clear what the luminance for each image pixel in the experimental screen is). 

 Beyond the previously established connection between probability and perception, in this work, we provide methods to quantify this relationship. We analyse the mutual information between various IQMs and probability surrogates, and look at the performance of probability surrogates in predicting perceptual sensitivity. We identify a number of factors that are strongly linked and perform an ablation study in order to propose function forms for perceptual sensitivity that depend only on probability, with a strong correlation between the ground truth and predicted sensitivity. These models shed light on the link between statistics and perception. 


%Besides using more accurate image probability models, and human ratings, we could analyse more possible functional forms that include interesting relations. We restricted ourselves to polynomial-like shapes for simplicity. 

%Here we analyze the directional sensitivity where the original and the noisy image (the direction) is known, $S(x,\tilde(x)$.
% Another open question is: Is it relevant the "direction" of the noise? It seems so, since $log(p(\tilde{\x}))$ is more relevant than $log(p(\x))$. 
% %For instance, analysing the direction of the derivative and not only the modulus.
% Or analysing the sensitivity in non-reference metrics. However, we could look for a model that only takes into account the original image $x$, and analyse the same effect but with non-reference IQMs.

% \newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}


% \clearpage
\appendix
\setcounter{table}{4}
\section{Functional forms coefficients.}

In section~4 we propose Eq.~3 and Eq.~4 as estimators of the perceptual sensitivity for 1 and 2 factors respectively. Note that each IQM has a different interpretation of the sensitivity units, therefore the weights of the proposed equations are different for each measure. In tables 5 and 6 we give the actual weights obtained in the experiments for each IQMs.
l
\begin{table*}[htb]
\centering
\caption{Coefficients for the Eq.~3 for each IQM.}
\label{tab:1D_coefs}
\begin{tabular}{llllll}
Coefs                & \textbf{MSSIM} & \textbf{NLPD}  & \textbf{PIM}   & \textbf{LPIPS} & \textbf{DISTS} \\
$b$                  & 29.5           & 65             & 15400           & 198            & 161            \\
$log(p(\tilde{\x}))$   & $4.9~10^{-3}$   & $9.5~10^{-3}$  & 2.62           & $3.33~10^{-2}$ & $2.58~10^{-2}$ \\
$log(p(\tilde{\x}))^2$ & $2.05~10^{-7}$ & $3.62~10^{-7}$ & $1.11~10^{-4}$ & $1.41~10^{-6}$ & $1.05~10^{-6}$
\end{tabular}
\end{table*}

\begin{table*}[htb]
\centering
\caption{Coefficients for the Eq.~4 for each IQM.}
\label{tab:2D_coefs}
\begin{tabular}{llllll}
Coefs                & \textbf{MSSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} \\
$b$                  & $28$     & $58$    & $15100$  & $194$    & $156$    \\
$log(p(\tilde{\x}))$   & $4.69~10^{-3}$   & $8.19~10^{-3}$  & $2.57$  & $3.26~10^{-2}$   & $2.49~10^{-2}$   \\
$log(p(\tilde{\x}))^2$ & $1.96~10^{-7}$   & $3.09~10^{-7}$  & $1.09~10^{-4}$ & $1.37~10^{-6}$   & $1.00~10^{-6}$   \\
$\sigma(x)$          & $-0.597$  & $-3.74$  & $-141$ & $-1.93$   & $-2.54$ 
\end{tabular}
\end{table*}

\section{Details on the selection of parameters for the functional form.}

Here we show the details for the selection of the parameters in section 4. Note that first, we have to choose candidates for the polynomial fitting. We took the ones obtained in section 4.1 (i.e. $b$ (bias), $log(p(\tilde{\x}))$, and $log(p(\tilde{\x}))^2$), and combinations the standard deviation $\sigma_x$ as suggested by the mutual information in section 3.2. The combinations of the standard deviation have been alone: $\sigma_x$, $\frac{1}{\sigma_x}$,$\sigma_x^2$, and combined with the probability of the noisy image: $\frac{log(p(\tilde{\x}))}{\sigma_x}$, $\frac{\sigma_x}{log(p(\tilde{\x}))}$, and $log(p(\tilde{\x})){\sigma_x}$. 

There are 9 candidates but we want the most compact and interpretable model. Analysing all the possible combinations is intractable so we are going to follow two strategies. On the one hand, we are going to discard different candidates sequentially starting from the largest model (9 candidates). On the other hand, we are going to use LASSO regression with different amounts of regularisation. Results in table 7 are shown in descending order in the number of factors taken into account. For each step, we remove the factor (or factors) that less influence has in the correlation. Besides, we show the correlation given by a LASSO model where the regularization parameter has been adjusted in order to have the same number of factors. A model with 6 factors (number 17) has the same correlation (0.81) than the one with all the factors (number 1). The best trade-off between the number of factors and correlation is with for the models 25, 26 and 27, with 4 factors and a correlation of 0.79. We chose as our final functional model in section 4.2 the model 25 as its factors involve less computations.      

\input{Models_table.tex}

% \section{$\sigma=0.02$ - Very very small amount of noise}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\columnwidth]{figs/alex-figs/0.02/example_images.png}
%     \caption{Example images with $\sigma=0.02$}
% \end{figure}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figs/alex-figs/0.02/pixelvaluevother.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/rmsevother.png}
%     \caption{Exploratory with dataset.}
% \end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/cond-1-MS-SSIM.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/cond-NLPD.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/cond-PerceptNet.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/cond-LPIPS.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/cond-DISTS.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/cond-PIM.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.02/cond-BRISQUE.png}
%     \caption{$\sigma=0.02$ (tiny noise)}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=.8\textwidth]{figs/alex-figs/0.02/regression/together.png}
%     \caption{$\sigma=0.02$ (tiny noise) Feature importances from Decision Tree Regressor}
% \end{figure*}

% \section{$\sigma=0.8$ large noise}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\columnwidth]{figs/alex-figs/0.8/example_images.png}
%     \caption{Example images with $\sigma=0.8$}
% \end{figure}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figs/alex-figs/0.8/pixelvaluevother.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/rmsevother.png}
%     \caption{Exploratory with dataset.}
% \end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/cond-1-MS-SSIM.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/cond-NLPD.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/cond-PerceptNet.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/cond-LPIPS.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/cond-DISTS.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/cond-PIM.png}
%     \includegraphics[width=.7\textwidth]{figs/alex-figs/0.8/cond-BRISQUE.png}
%     \caption{$\sigma=0.8$ (large noise)}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=.8\textwidth]{figs/alex-figs/0.8/regression/together.png}
%     \caption{$\sigma=0.8$ (large noise) Feature importances from Decision Tree Regressor}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%

% \begin{table*}
% \begin{center}
% \begin{tabular}{c|c|c|c|c}
%           & & & & \\
%  \textbf{Information}  &   \textbf{Internal Noise}   &   \textbf{Surprise}      & \textbf{Acquisition Noise}                    & \textbf{Band-pass Contrast}     \\
%  \textbf{Transmision} &  Limited Resolution  &                                 &  Denoising                                     &   Average Spectrum \\
%                       & & & & \\\hline
%  Barlow   &   Lloyd    & \multirow{2}{*}{Shannon48}          & \multirow{2}{*}{Miyasawa}             & \multirow{2}{*}{Michaelson}            \\
%  Laughlin &   McLeod   & \multirow{2}{*}{Gegenfurtner}     & \multirow{2}{*}{Raphan \& Simoncelli} & \multirow{2}{*}{Campbell \& Robson~\cite{Campbell68}}    \\
%  Bell \& Sejnowski     &   Twer     & \multirow{2}{*}{Bruce \& Tsotsos} & \multirow{2}{*}{Vincent} & \multirow{2}{*}{Peli}                   \\\cline{1-2}
% \multicolumn{2}{c|}{Malo, GutiÃ©rrez \& Laparra}  &  &  &                                                                                          \\\cline{1-4}
% \multicolumn{4}{c|}{Hepburn, Laparra, Santos, Balle \& Malo} &                                                                                    \\\hline
%   & & & &  \\
%  $p(x)$  & $p(x)^{\frac{1}{3}}$ & $p(x)^{-1}$                      &  \multirow{3}{*}{$\frac{\nabla_x p(x)}{p(x)} = \nabla_x log(p(\x))$}   &  \multirow{3}{*}{$C_{\textrm{band}} = \frac{\sigma(F_{\textrm{band}} \cdot x)}{\mu(x)}$} \\
%  & & & &  \\
%  $log(p(\x))$  & $\frac{1}{3}log(p(\x))$ & $-log(p(\x))$              &   &       \\
%  & & & & \\\hline
% \end{tabular}
% \end{center}
% \caption{Causes of Non-Euclidean metric due to image regularities}
% \label{table_article1}
% \end{table*}
\end{document}


\section{Exploring the distribution}



\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/pixelvaluevother.png}
    \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/rmsevother.png}
    \caption{Exploratory with dataset. $\sigma=0.2$}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/Scatters_between_measures.png}
    \caption{Scatters for different measures.}
\end{figure*}

\begin{figure*}
    \centering
    \begin{tabular}{cccc}
     a & b & c  &  d \\ \\
     \includegraphics[width=4cm]{figs/SPH_Prob_ori_vs_noisy.png}
    & \includegraphics[width=4cm]{figs/SPH_Prob_ori_minus_noisy.png}
    & \includegraphics[width=4cm]{figs/SPH_Prob_rmse_vs_prob.png}
    & \includegraphics[width=4cm]{figs/SPH_Prob_prob_vs_driv.png} \\
    \end{tabular}
    \caption{Analysis of the natural image probability. a) Original vs noisy, b) Original minus noisy (mean in dashed line, $\mu = 14.8$), c) probability vs rmse, d) probability vs derivative.}
    \label{fig:log_prob_freq_contrast}
\end{figure*}

\subsection{Extra}
\begin{itemize}
    \item going up or down the hill, for instance denoising following the probability gradient
    \item low or high luminance
\end{itemize}

\section{Exploring the distribution}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/alex-figs/0.4/example_images.png}
    \caption{Example images with $\sigma=0.4$}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/alex-figs/0.4/pixelvaluevother.png}
    \includegraphics[width=.7\textwidth]{figs/alex-figs/0.4/rmsevother.png}
    \caption{Exploratory with dataset.}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/Scatters_between_measures.png}
    \caption{Scatters for different measures.}
\end{figure*}

\begin{figure*}
    \centering
    \begin{tabular}{cccc}
     a & b & c  &  d \\ \\
     \includegraphics[width=4cm]{figs/SPH_Prob_ori_vs_noisy.png}
    & \includegraphics[width=4cm]{figs/SPH_Prob_ori_minus_noisy.png}
    & \includegraphics[width=4cm]{figs/SPH_Prob_rmse_vs_prob.png}
    & \includegraphics[width=4cm]{figs/SPH_Prob_prob_vs_driv.png} \\
    \end{tabular}
    \caption{Analysis of the natural image probability. a) Original vs noisy, b) Original minus noisy (mean in dashed line, $\mu = 14.8$), c) probability vs rmse, d) probability vs derivative.}
    \label{fig:log_prob_freq_contrast}
\end{figure*}