\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[breaklinks=true]{hyperref}
\usepackage{breakcites}
\usepackage{url}
\usepackage{multirow}
\usepackage{bigints}
\usepackage{array}
\usepackage{subcaption}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lscape}
\usepackage{booktabs}
\usepackage{todonotes}

\newenvironment{airytabular}{\setlength{\extrarowheight}{3pt}%
  \begin{tabular}}{\end{tabular}}

\newcommand{\red}[1]{\textcolor[rgb]{1,0,0}{#1}}           % Color 

\title{Disentangling the Link Between \\ Image Statistics and Human Perception}
%\title{The Barlow Hypothesis in light of \\ New Image Generative Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Alexander Hepburn* \\
School of Engineering Mathematics and Technology\\
University of Bristol \\
\texttt{alex.hepburn@bristol.ac.uk} \\
\And
Valero Laparra* \\
Image Processing Lab \\
Universitat de Valencia \\
\texttt{valero.laparra@uv.es} \\
\And
Raúl Santos-Rodriguez \\
School of Engineering Mathematics and Technology \\
University of Bristol \\
\texttt{enrsr@bristol.ac.uk} \\
\And
Jesús Malo \\
Image Processing Lab \\
Universitat de Valencia \\
\texttt{jesus.malo@uv.es}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\x}{{\bf x}}
\newcommand{\y}{{\bf y}}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

In the 1950s, Barlow and Attneave hypothesised a link between biological vision and information maximisation. Following Shannon, information was defined using the probability of natural images.
% the images taken from natural scenes. 
%Several studies explored this hypothesis 
%connecting image statistics to human visual behavior. 
A number of physiological and psychophysical phenomena have been derived ever since from principles like info-max, efficient coding, or optimal denoising. However, it remains unclear how this link is expressed in mathematical terms from image probability. 
First, classical derivations were subjected to strong assumptions on the probability models and on the behaviour of the sensors. Moreover, the direct evaluation of the hypothesis was limited by the inability of the classical image models to deliver accurate estimates of the probability. 
In this work we directly evaluate image probabilities using an advanced generative model for natural images, and we analyse how probability-related factors can be combined to predict human perception via sensitivity of state-of-the-art subjective image quality metrics. 
%We use information theory and regression analysis 
%to identify the appropriate functions of probability to predict perceptual sensitivity. 
We use information theory and regression analysis to find a combination of just two probability-related factors that achieves 0.8 correlation with subjective metrics. 
% explains 80\% of the variance of the sensitivity. 
This probability-based sensitivity is psychophysically validated by reproducing the basic trends of the Contrast Sensitivity Function, its suprathreshold variation, and trends of the Weber-law and masking.  

\iffalse
In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probability. 

Specifically, we analyse how the sensitivity of full-reference image quality metrics can be predicted from quantities derived directly from the probability distribution of natural images. First, we compute the mutual information between a wide range of probabilistic factors and the sensitivity of the metrics and find that the most influential factor is the probability of the noisy image. Then we explore how these probabilistic factors can be combined using a simple model to predict the metric sensitivity, giving an upper bound for the correlation of 0.85 between the model predictions and the actual perceptual sensitivity. Finally, we explore how to combine the probabilistic factors using simple expressions, and obtain two functional forms (using one or two factors) that can be used to predict the sensitivity of the human visual system given a particular pair of images.
\fi
\def\thefootnote{*}\footnotetext{Equal Contributions}\def\thefootnote{\arabic{footnote}}
\end{abstract}

\section{Introduction}
% [1] Discusion in 
One long standing discussion in artificial and human vision is about the principles that should drive sensory systems.
One example is Marr and Poggio's functional descriptions at the (more abstract) \emph{Computational Level} of vision~\citep{Marr76}. Another is Barlow's \emph{Efficient Coding Hypothesis}~\citep{At54,Barlow}, which suggests that vision is just an optimal information-processing task. In modern machine learning terms, the classical optimal coding idea qualitatively links the probability density function (PDF) of the images with the behaviour of the sensors.

An indirect research direction explored this link by proposing design principles for the system (such as infomax) to find optimal transforms and compare the features learned by these transforms and the ones found in the visual system, e.g. receptive fields or nonlinearities~\citep{Olshausen1996,Bell97,simoncelli2001natural,Schwartz01,hyvarinen2009natural}. This indirect approach, which was popular in the past due to limited computational resources, relies on gross approximations of the image PDF and on strong assumptions about the behaviour of the system. The set of PDF-related factors (or surrogates of the PDF) that were proposed as explanations of the behaviour in the above indirect approach is reviewed below in Section~\ref{sec:probvis}.

In contrast, here we propose a direct relation between the behaviour (sensitivity) of the system and the PDF of natural images. Following the preliminary suggestions in~\citep{hepburn2022on} about this direct relation, we rely on two elements. On the one hand, we use recent \emph{generative models} that represent large image datasets better than previous PDF approximations and provide us with an accurate estimate of the probability at query points~\citep{Oord2016pixelcnn,salimans2017pixelcnn++}. Whilst these statistical models are not analytical, they allow for sampling and log-likelihood prediction. They also allow us to compute gradients of the probability, which, as reviewed below, have been suggested to be related to sensible vision goals.

On the other hand, recent measures of \emph{perceptual distance} between images have recreated human opinion of subjective distortion to a great accuracy~\citep{laparra2016,zhang2018unreasonable,Hepburn2020perceptnet,ding2020image}. Whilst being just an approximation to human subjectivity, the sensitivity of these perceptual distances is a convenient computational description of the main trends of human vision that should be explained from scene statistics.

In this work, we identify the more relevant probabilistic factors that may be behind the non-Euclidean behaviour of perceptual distances and propose a simple expression to predict the perceptual sensitivity from these factors. First, we empirically show the relationship between the sensitivity of the metrics and different functions of the probability using conditional histograms. Then, we analyse this relationship quantitatively using mutual information, factor-wise and considering groups of factors. After, we use different regression models to identify a hierarchy in the factors that allow us to propose analytic relationships for predicting perceptual sensitivity. Finally, we perform an ablation analysis over the most simple closed-form expressions and select some solutions to predict the sensitivity given the selected functions of probability.

\section{Background and Proposed Methods}
In this section, we first recall the description of visual behaviour: the \emph{sensitivity of the perceptual distance}~\citep{hepburn2022on}, which is the feature to be explained by functions of the probability. Then we review the probabilistic factors that were proposed in the past to be related to visual perception, but had to be considered indirectly. 
%Our proposal consists of being able to highlight relations and find explicit expressions to derive behaviour from the statistical factors. %Explanations consist of identifying the relations between the statistical factors and the behaviour, and being able to propose expressions to derive behaviour from the statistical factors.
Finally, we introduce the tools to compute both behaviour and statistical factors: (i) the perceptual distances, (ii) the probability models, and (iii) how variations in the image space (distorted images) are chosen.  

\subsection{The problem: Perceptual Sensitivity}
Given some original image $\x$ and a distorted version $\tilde{\x}$, full-reference \emph{perceptual distances} are models, $D_p(\x, \tilde{\x})$, that accurately mimic the human opinion about the subjective difference between them. %Examples of these models reviewed below include ~\citep{wang2003multiscale,Wang2004,zhang2018unreasonable,Hepburn2020perceptnet,ding2020image}.
In general, this perceived distance, $D_p(\x, \tilde{\x})$, highly depends on the particular image analysed and on the direction of the distortion. These dependencies make $D_p$ distinctly different from Euclidean metrics like the Root Mean Square Error (RMSE), $||\x - \tilde{\x}||_2$, which does not correlate well with human perception~\citep{Wang2009}. This characteristic dependence of $D_p$ is captured by the \emph{sensitivity of the perceptual distance}~\citep{hepburn2022on}:
\begin{equation}
    S(\x,\tilde{\x}) = \frac{D_p(\x, \tilde{\x})}{||\x-\tilde{\x}||_2}
    \label{eq:sensitivity}
\end{equation}

The sensitivity of the perceptual distance, referred to as the \emph{perceptual sensitivity} throughout the work, relates a (perceptually meaningful) non-Euclidean distance between images with the (perceptually meaningless) Euclidean distance RMSE. This ratio is big at regions of the image space where human sensitivity is high and small at regions neglected by humans.
%Moreover, this sensitivity is anisotropic: distortions with constant Euclidean length along different directions (distortions of different nature) lead to different subjective distortions and hence sensitivity depends on the direction.  

\subsection{Previous Probabilistic Explanations of Vision}
\label{sec:probvis}
% The classical search of organisation principles of vision led to 
There is a rich literature trying to link the properties of visual systems with the PDF of natural images.
Principles and PDF-related factors that have been proposed in the past include (Table~\ref{tab:principles}): 
%(an extended version can be found in Table~\ref{tab:principles_app} in Appendix \ref{sec:table}): 

\textbf{(i)~Infomax for \emph{regular images}}. Transmission is optimised by reducing the redundancy, as in principal component analysis~\citep{Hancock91,Buschbaum83}, independent components analysis~\citep{Olshausen1996,Bell97,hyvarinen2009natural}, and, more generally, in PDF equalisation~\citep{Laughlin81} or factorisation~\citep{Malo10}.

\textbf{(ii)~Optimal representations for \emph{regular images} in noise}. Noise may occur either at the front-end sensors~\citep{Miyasawa61,Atick90,Atick92} (optimal denoising), or at the internal response~\citep{Lloyd57,Twer01} (optimal quantisation, or optimal nonlinearities to cope with noise). While in denoising the solutions depend on the derivative of the log-likelihood~\citep{Raphan11,Vincent11}, optimal quantisation is based on resource allocation according to the PDF after saturating non linearities~\citep{Twer01,McLeod03,Series09,Ganguli11}. Bit allocation and quantisation according to nonlinear transforms of the PDF has been used in perceptually optimised coding~\citep{Macq92,Malo00}.
In fact, both factors considered above (infomax and quantisation) have been unified in a single framework where the representation is driven by the PDF raised to certain exponent~\citep{malo2006v1,Laparra12,Laparra15}.

\textbf{(iii)~Focus on \emph{surprising images} (as opposed to regular images)}. This critically different factor (surprise as opposed to regularities) has been suggested as a factor driving sensitivity to color~\citep{Gegen09,Wichmann02color}, and in visual saliency~\citep{Bruce06}. In this case, the surprise is described by the inverse of the probability (as opposed to the probability) as in the core definition of information~\citep{Shannon48}.

%The descriptors associated to the above probabilistic explanations have been found to positively (or negatively) correlate to the sensitivity in a number of observations with a restricted set of metrics~\citep{hepburn2022on}. However, that study did not consider other statistical descriptors of the signal that have been studied by classical vision science (last three columns of Table~\ref{tab:principles}).

\textbf{(iv)~Energy (first moment, mean or average, of the signal)}.
Energy is the obvious factor involved in sensing. 
In statistical terms, the first eigenvalue of the manifold of a class of images represents the average luminance of the scene. The consideration of the nonlinear brightness-from-luminance response is a fundamental law in visual psychophysics (the Weber-Fechner law~\citep{Weber1846,Fechner1860}). It has statistical explanations related to the cumulative density~\citep{Laughlin81,malo2006v1,Laparra12} and using empirical estimation of reflectance~\citep{PurvesLottoPNAS11}. Adaptivity of brightness curves~\citep{Wittle92} can only be described using sophisticated non-linear architectures~\citep{Brainard05,martinez2018derivatives,Bertalmio20}. 

\textbf{(v)~Structure (second moment, covariance or spectrum, of the signal}. Beyond the obvious \emph{energy}, vision is about understanding the \emph{spatial structure}. The simplest statistical description of the structure is the covariance of the signal. The (roughly) stationary invariance of natural images implies that the covariance can be diagonalized in Fourier like-basis, $\Sigma(\x) = B \cdot \lambda \cdot B^\top$~\citep{Clarke81}, and that the spectrum of eigenvalues in $\lambda$ represents the average Fourier spectrum of images. The magnitude of the sinusoidal components compared to the mean luminance is the concept of \emph{contrast}~\citep{Michelson27,Peli90}, which is central in human spatial vision. Contrast thresholds have a distinct bandwidth~\citep{Campbell68}, which has been related to the spectrum of natural images~\citep{Atick92,Gomez20,Li22}.

\textbf{(vi) Heavy-tailed marginal PDFs in transformed domains} were used by classical generative models of natural images in the 90's and 00's~\citep{Simoncelli97,Malo00,hyvarinen2009natural,Oord14_t_student} and then these marginal models were combined through mixing matrices (either PCA, DCT, ICA or wavelets), conceptually represented by the matrix $B$ in the last column of Table~\ref{tab:principles}. In this context, the response of a visual system (according to PDF equalisation) should be related to non-linear saturation of the signal in the transform domain using the cumulative functions of the marginals. These explanations~\citep{Schwartz01,malo2006v1} have been given for the adaptive nonlinearities that happen in the wavelet-like representation in the visual cortex~\citep{Heeger92,Carandini12}, and also to define perceptual metrics~\citep{Daly90,Teo94,Malo97,Laparra10,laparra2016}.

\begin{table*}
\setlength\tabcolsep{2pt}
\centering
\scriptsize
\caption{\footnotesize{Probabilistic factors that have been proposed to predict sensitivity. A general problem of the classical literature (in Section~\ref{sec:probvis}) is that direct estimation of the probability at arbitrary images $p(\mathbf{x})$ was not possible. }}\vspace{-0.2cm}
\label{tab:principles}
\begin{tabular}{|c|cc|c|c|c|c|}
\hline
(i) &
  \multicolumn{2}{c|}{(ii)} &
  (iii) &
  (iv) &
  (v) &
  (vi) \\ \hline
\begin{tabular}[c]{@{}c@{}}Information\\ Transmission\end{tabular} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Internal Noise\\ Limited Resolution\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Acquisition Noise\\ Denoising\end{tabular} &
  Surprise &
  \begin{tabular}[c]{@{}c@{}}Signal Average\\ First Eigenvalue\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Signal Spectrum\\ All Eigenvalues\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Marginal Laplacian\\ Marginal nonlinearity\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}$p(\x)$\\[0.15cm] $log(p(\x))$\end{tabular} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$p(\x)^{\frac{1}{3}}$\\[0.15cm] $\frac{1}{3}log(p(\x))$\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}$\frac{\nabla_{\!\x} p(\x)}{p(\x)}$\\[0.15cm] $J(\x) = \nabla_{\!\x} log(p(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$p(\x)^{-1}$\\[0.15cm] $-log(p(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\mu(\x)$\\[0.1cm] $log(\mu(\x))$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\frac{1}{\mu(\x)}\Sigma(\x)$\\[0.15cm] $\frac{1}{\mu(\x)} \,\, B \cdot \lambda \cdot B^\top$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}$\frac{1}{\mu(\x)} \,\, B \cdot log(\lambda) \cdot B^\top$\\[0.15cm] $\bigintsss_{\,\x}^{\mathbf{\hat{x}}} log( p(\x') ) \, d\x'$\end{tabular} \\ \hline
\end{tabular}
\end{table*}


\subsection{Our proposal}
\label{sec:factors}
Here we revisit the classical principles in Table~\ref{tab:principles} to propose a \emph{direct} prediction of the sensitivity of state-of-the-art perceptual distances from the image PDF. The originality consists on the \emph{direct} computation of $p(\mathbf{x})$ through current generative models as opposed to the indirect approaches taken in the past to check the Efficient Coding Hypothesis (when direct estimation of $p(\mathbf{x})$ was not possible). 
%What we propose in this work is using the kind of \emph{descriptors} recalled in Table~\ref{tab:principles} (mainly PDFs of images, loglikelihoods, their directional derivatives, cumulative functions, and 1st and 2nd order moments, eventually subjected to saturating nonlinearities) as the subject of our study to predict the \emph{sensitivity} of state-of-the-art perceptual distances:
In our prediction of the perceptual sensitivity we consider the following PDF-related factors:
\begin{equation}
\begin{aligned}
    log(p(\x)) \, \, , \, \, log(p(\tilde{\x})) \,\, &, \,\, ||J(\x)|| \,\, , \,\, ||J(\tilde{\x})|| \,\, , 
    (\x-\tilde{\x})^\top\!\! \cdot \! J(\x) \,\, , \,\, \mu(\x) \,\, , \,\, &\sigma(\x) \,\, , \,\, \bigintssss_{\,\x}^{\mathbf{\tilde{x}}} log( p(\x') )\, d\x'
\end{aligned}
\end{equation}
where $\x$ is the original image, $\tilde{\x}$ is the distorted image, $J(\x)$ is the (vector) gradient of the log-likelihood at $\x$. The standard deviation of the image, $\sigma(\x)$ (as a single element of the covariance $\Sigma(\x)$), together with  $\mu(\x)$ capture the concept of RMSE contrast~\citep{Peli90} (preferred over Michelson contrast~\citep{Michelson27} for natural stimuli), and the integral takes the log-likelihood that can be computed from the generative models and accumulates it along the direction of distortion, qualitatively following the idea of cumulative responses proposed in equalisation methods~\citep{Laughlin81,malo2006v1,Laparra12,Laparra15}.

\subsection{Representative perceptual distances}
\label{sec:metrics}
The most successful perceptual distances can be classified in four big families:
\textbf{(i) Physiological-psychophysical architectures.} These include~\citep{Daly90,Watson93,Teo94,Malo97,Laparra10,Martinez19,Hepburn2020perceptnet} and in particular it includes NLPD~\citep{laparra2016}, which consists of a sensible filterbank of biologically meaningful receptive fields and the canonical Divisive Normalization used in neuroscience~\citep{Carandini12}.
\textbf{(ii) Descriptions of the image structure.} These include the popular SSIM~\citep{Wang2004}, its (improved) multiscale version MS-SSIM~\citep{wang2003multiscale}, and recent version using deep learning: DISTS~\citep{ding2020image}.
\textbf{(iii)~Information-theoretic measures.} These include measures of transmitted information such as VIF~\citep{Sheikh06,Malo21}, and recent measures based on enforcing informational continuity in frames of natural sequences, such as PIM~\citep{Bhardwaj2020}.
\textbf{(iv) Regression models:} generic deep architectures used for vision tasks retrained to reproduce human opinion on distortion as LPIPS~\citep{zhang2018unreasonable}.

In this work we use recent representative examples of the four families: NLPD, DISTS, PIM, and LPIPS. We also included multiscale SSIM as a useful reference because of its popularity in the image processing community. Table~\ref{tab:MOS} in Appendix~\ref{sec:performance_metrics} illustrates the performance of these measures in reproducing human opinion. Fig.~\ref{fig:images} shows an explicit visual example of how the sensitivity to noise highly depends on the image and is well captured by a representative $D_p$ measure, NLPD~\citep{laparra2016}, but not by the Euclidean distance, RMSE.

\begin{figure}[b!]
    \begin{center}
    \hspace{0cm}\includegraphics[width=1.0
\columnwidth]{figs/alex-figs/0.2/example_images.png}
\end{center}
    \vspace{-0.5cm}
    \caption{\footnotesize{\textbf{The concept: visual sensitivity may be easy to predict from image probability.} Different images are corrupted by uniform noise of the same energy (on the surface of a sphere around $\x$ of Euclidean radius $\epsilon=1$), with the same RMSE = 0.018 (in [0, 1] range).
    Due to \emph{visual masking}~\citep{Legge80}, noise of the same energy is more visible, i.e. human sensitivity is bigger, for smooth images.
    This is consistent with the reported NLPD distance~\citep{laparra2016}, and interestingly,   
    sensitivity is also bigger for more probable images,
    where $log(p(\x))$ is computed with PixelCNN++~\citep{salimans2017pixelcnn++}.
    }}
    \label{fig:images}
\end{figure}

\subsection{A convenient generative model}
\label{sec:est_prob}

Our proposal requires a probability model that is able to give an accurate estimate of $p(\mathbf{x})$ at arbitrary points (images) $\mathbf{x}$, so that we can study the sensitivity-probability relation at \emph{many} images. In this work we rely on PixelCNN++~\citep{salimans2017pixelcnn++}, since it is trained on CIFAR10~\citep{krizhevsky2009learning}, a dataset made up of small colour images of a \emph{wide range} of natural scenes. PixelCNN++ also achieves the lowest negative log-likelihood of the test set, meaning the PDF is accurate in this dataset. This choice is convenient for consistency with previous works~\citep{hepburn2022on} , but note that it is not crucial for the argument made here. In this regard, recent interesting models~\citep{kingma2018glow,kadkhodaie23} could also be used as probability estimates.
% EXCUSE IF REVIEWERS ASK
%\footnote{\red{By the time we started this research, authors of \emph{Glow}~\citep{kingma2018glow} had released the weights only for human faces and bedrooms, which are not good representatives of generic natural images. Similarly, experiments in~\citep{kadkhodaie23} are focused on faces. Nevertheless, when trained in more general images this kind of models seem appropriate for our application.}}.
%
% Previous text
% ------------------
%Most recent generative models aim at capturing the PDF around certain images, for example, human faces or bedrooms~\citep{kingma2018glow}. 
%However, these sets are not representative of generic natural images, because those specific scenes have a rigid structure. 
%Rather, we follow the methodology in~\citep{hepburn2022on} and rely on a probability model which we know is accurate for the images we wish to query. PixelCNN++~\citep{salimans2017pixelcnn++} is trained on CIFAR10~\citep{krizhevsky2009learning}, a dataset made up of colour images of size ($32\times32\times3$) covering 10 classes of a wide range of natural images. This model has been shown to compress CIFAR10 samples to a low entropy, meaning the model has a good understanding of this kind of images. 
%In order to trust the probabilities employed throughout this paper, we use images of the same kind as in CIFAR10 to ensure our log-likelihoods are accurate. 
%Fig.~\ref{fig:images} shows that cluttered images are succesfully identified as less probable than smooth images by PixelCNN++, consistently with the known average spectrum of natural images~\citep{Clarke81,Atick92,Malo00,simoncelli2001natural}.
%
%
%
Fig.~\ref{fig:images} shows that cluttered images are successfully identified as less probable than smooth images by PixelCNN++, consistently with classical results on the average spectrum of natural images~\citep{Clarke81,Atick92,Malo00,simoncelli2001natural}.

\subsection{Distorted images}
\label{noisy_images}
There are two factors to balance when looking for distortions so that the definition of \emph{perceptual sensitivity} is meaningful. First; in order to understand the ratio in Eq.~\ref{eq:sensitivity} as a variation \emph{per unit of euclidean distortion}, we need the distorted image $\tilde{\x}$ to be close to the original $\x$. Secondly, the perceptual distances are optimised to recreate human judgements, so if the distortion is too small, such that humans are not able to see it, the distances are not trustworthy. 
% Additionally, we wish to draw comparisons between literature in the denoising literature which uses additive Gaussian noise across the whole image, which conveniently also allows us to use measures across the whole image, like taking the norm of the gradient. 

As such, we propose to use additive uniform noise on the surface of a sphere of radius $\epsilon$ around $\x$. We choose $\epsilon$ in order to generate $\tilde{\x}$ where the noise is small but still visible for humans ($\epsilon=1$ in $32\times32\times3$ images with colour values in the range [0,1]).
Certain radius (or Euclidean distance) corresponds to a unique RMSE, with $\epsilon=1$ RMSE = 0.018 (about $2\%$ of the colour range).
Examples of the noisy images can be seen in Fig.~\ref{fig:images}. Note that in order to use the perceptual distances and PixelCNN++, the image values must be in a specific range. After adding noise we clip the signal to keep that range. Clipping may modify substantially the Euclidean distance (or RMSE) so we only keep the images where $\textrm{RMSE} \in [0.017, 0.018]$, resulting in 48,046 valid images out of 50,000.
%so the range of images is $[-1, 1]$, and only keep the images whose Euclidean distance is comparable. 
%If an image has a large number of pixels that require clipping after noise, the Euclidean distance between $\x$ and $\tilde{\x}$ will be noticeably different to the other images in the dataset. 
%As such we only use images where $\textrm{RMSE}>0.017$, resulting in 48,046 images.

%\section{Experiments: in search of the functional relationship between sensitivity and probability}
\section{Experiments}
Firstly, we empirically show the relation between the sensitivity and the candidate probabilistic factors using conditional histograms, and use information theory measures to tell which factors are most related to the sensitivity. Then, we explore polynomial combinations of several of these factors using regression models. Lastly, we restrict ourselves to the two most important factors and identify simple functional forms to predict perceptual sensitivity.

\subsection{Highlighting the relation: Conditional Histograms}
\label{sec:cond_hist}
Conditional histograms in Fig.~\ref{fig:cond_hist} illustrate the sensitivities of the metrics conditioned to different probabilistic factors. In this case, the histograms describe the probabilities $\mathcal{P}(S\in [b_{j-1}, b_j] | X=x)$ where $S$ is certain perceptual sensitivity partitioned into $m=30$ bins $[b_{j-1}, b_j)$, and $X$ is one of the possible probabilistic factors. This allows us to visually inspect which of the probabilistic factors are important for predicting sensitivity. The probabilistic factor used is given in each subplot title alongside with the Spearman correlation between perceptual sensitivity and the factor. 

For all perceptual distances, $log(p(\x))$ has a high correlation, and mostly follows a similar conditional distribution. NLPD is the only distance that significantly differs, with more sensitivity in mid-probability images. We also see a consistent increase in correlation and conditional means when looking at $log(p(\tilde{\x}))$, meaning the log-likelihood of the noisy sample is more indicative of perceptual sensitivity for all tested distances. Also note that the standard deviation $\sigma(\x)$ also has a strong (negative) correlation across the traditional distances, falling slightly with the deep learning based approaches. This is likely due to the standard deviation being closely related to the contrast of an image~\citep{Peli90}, and the masking effect: the sensitivity in known to decrease for higher contrasts~\citep{Legge80,Martinez19}. Note that measures that take into account both points, $\int^{\tilde{\x}}_\x log(p(\x))d\x$ and $\overrightarrow{J}_{\tilde{x}}(\x)$, have lower correlation than just taking into account the noisy sample, with the latter being insignificant in predicting perceptual sensitivity.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-1-MS-SSIM.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-NLPD.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-PIM.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-LPIPS.png}
    \includegraphics[width=\textwidth]{figs/alex-figs/0.2/cond-DISTS.png}
    \includegraphics[width=.7\textwidth]{figs/alex-figs/0.2/colorbar.png}
    \caption{\footnotesize{Conditional histograms, x-axis is the probability factor and y-axis is the sensitivity (Eq.~\ref{eq:sensitivity}) of the metric per row. Shown is the Spearman correlation between probabilistic factor and perceptual sensitivity.}}
    \label{fig:cond_hist}
\end{figure}

\subsection{Quantifying the relation: Mutual Information}
\label{sec:MI}
To quantify the ability of probabilistic factors to predict perceptual sensitivity, we use information theoretic measures. Firstly, we use mutual information which avoids the definition of a particular functional model and functional forms of the features. This analysis will give us insights into which of the factors derived from the statistics of the data can be useful in order to later define a functional model that relates statistics and perception.

The mutual information has been computed using all 48046 samples and using the Rotation Based Iterative Gaussianisation (RBIG)~\citep{laparra2011iterative} as detailed here \citep{laparra2020information}. Instead of the mutual information value ($I(X, Y)$), we report the Information coefficient of correlation~\citep{Linfoot57} (ICC) since the interpretation is similar to the Pearson coefficient and allows for easy comparison, where $\textrm{ICC}(X, Y)= \sqrt{1- e^{-2 I(X, Y)}}$.

%~\ref{fig:MI_1D}
Figure~\ref{fig:MI_all} shows the ICC between each isolated probabilistic factor and the sensitivity of different metrics. It is clear that the most important factor to take into account in most models (second in MS-SSIM) is the probability of the noisy image $\log(p(\tilde{\x}))$, a consistent result with the conditional histograms.
\begin{figure}[b]
    \begin{centering}
    \includegraphics[width=0.39\columnwidth]{figs/MI/ICC_1D_prob_surrogates_vs_sensitivity_no_percept.png}
    \includegraphics[width=0.6\columnwidth]{figs/alex-figs/2D_ICC_combined.png}
% \hspace{-1.5cm}\includegraphics[width=1.2\columnwidth]{figs/BigFig3.JPG}
% \hspace{-1.5cm}\includegraphics[width=1.2\columnwidth]{figs/BigFig3short.JPG}
% \vspace{-0.75cm}
\end{centering}
    %\caption{\footnotesize{Information Coefficient of Correlation (ICC) between the sensitivity of different perceptual distances and isolated probabilistic factors (top: 1 factor) and different groups of probabilistic factors (bottom: 2, 3 and 4 factors). Note that ICC for groups of different size is plotted in a different gray scale (see colorbars).}}
    \caption{\footnotesize{Information Coefficient of Correlation (ICC) between the sensitivity of different perceptual distances and isolated probabilistic factors (left: 1 factor) and different pairs of probabilistic factors (right: 2 factors). The considered factors are explicitly listed at the axes of the matrix of pairs for the MS-SSIM. The meaning of the entries for the matrices of the other distances is the same and all share the same colorbar for ICC.}}
      \label{fig:MI_all}
\end{figure}
Once we select the $log(p(\tilde{\x}))$ as the most important factor, we explore which other factor should be included as the second term. In order to do so we analyse the ICC between each possible combination of two factors with each metric (right panel of Fig.~\ref{fig:MI_all}). The matrix of possible pairs is depicted in detail for MS-SSIM (with factors in the axes and a colourbar for ICC), and corresponding matrices for the other metrics are also shown in smaller size. We can see that the general trend in four out of five metrics is the same: the pairs where $log(p(\tilde{\x}))$ is involved have the maximum mutual information. The exception is MS-SSIM, which also behaves differently when using a single factor. In all the other cases the maximum relation with sensitivity is achieved when combining $log(p(\tilde{\x}))$ with the standard deviation of the original image $\sigma(\x)$. 
%\begin{figure}[tb]
%    \centering
%    \includegraphics[width=.6\columnwidth]{figs/MI/ICC_1D_prob_surrogates_vs_sensitivity_no_percept.pdf}
%      \caption{ICC between proposed probabilistic factors and the sensitivity of perceptual distances.}
%      \label{fig:MI_1D}
%\end{figure}

%~\ref{fig:MI_2D}

%\begin{figure}
%\footnotesize
%    \centering
%    \begin{tabular}{ccc}
%    {MS-SSIM} & NLPD & PIM \\ 
%     \multirow{3}{*}[0.6in]{\includegraphics[width=.35\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_MS-SSIM.pdf}}
%    & \includegraphics[width=.15\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_NLPD.pdf}
%    & \includegraphics[width=.15\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_PIM.pdf}
%    \\
%     & LPIPS & DISTS 
%     \\
%    & \includegraphics[width=.15\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_LPIPS.pdf}
%    & \includegraphics[width=.15\columnwidth]{figs/MI/MI_2D_prob_surrogates_vs_sensitivity_DISTS.pdf}  \\
%    \end{tabular}
%    \caption{ICC between pairs of probabilistic factors and the sensitivity of different metrics. Correspondence between rows and columns, and probabilistic factors, and color bar are the same for all figures.}
%    \label{fig:MI_2D}
%\end{figure}


The set of factors found to have a high relation with the variable to be predicted was expanded (one at a time) with the remaining factors. And we computed the mutual information and ICC for such expanded sets of factors. Reasoning as above (Fig.~\ref{fig:MI_all}, right), we found that the next in relevance was $log(p(\x))$, then the mean $\mu(\x)$, then the gradient $||J(\x)||$, to finally take into account \emph{all} the probabilistic factors.   
%%Fig~\ref{fig:MI_3D}
%Now we consider the third and the fourth factors to include. The central panel at the bottom of Fig~\ref{fig:MI_all} shows the ICC when taking into account 3 factors (log(p(\tilde{\mathbf{x})), \sigma(\mathbf{x}), and the factor indicated in each row). It is clear that the factor that adds more to the ICC is $log(p(\x))$. In the case of the fourth factor (Fig.~\ref{fig:MI_all} bottom-right), the one that is more relevant is $\mu(\x)$. Therefore we have this ordering of factors to take into account; $\{\log(p(\tilde{\x})), \sigma(\x), \log(p(\x)), \mu(\x)\}$.
%the probability of the noisy image $\log(p(\tilde{\x}))$, the standard deviation of the luminance of the original image $\sigma(\x)$, the probability of the original image $\log(p(\x))$, and the mean luminance of the original image $\mu(\x)$.
%\begin{figure}
%    \centering
%    \begin{tabular}{cc}
%     \includegraphics[width=.4\columnwidth]{figs/MI/ICC_3D_plot_pxn_plus_Std_plus_vs_sensitivity.pdf} & 
%     \includegraphics[width=.4\columnwidth]{figs/MI/ICC_4D_plot_pxn_plus_Std_plus_vs_sensitivity.pdf}
%    \end{tabular}
%    \caption{Left: Analysis of the shared information (ICC) between sensitivity and 3 probabilistic factors; $\{log(p(\x)), \sigma(\x), f_s\}$ where $f_s$ is one of the 4 possible probabilistic factors in the y-axis. Right: Same but for 4 probabilistic factors; $\{log(p(\x)), \sigma(\x), log(p(\x)), f_s\}$ where $f_s$ is one of the 3 possible probabilistic factors in the y-axis.} %Mutual information is normalised for each IQM.
%    \label{fig:MI_3D}
%\end{figure}
Table~\ref{tab:MI} summarises the increase of ICC (the increase if information about the sensitivity) as we increase the number of probabilistic factors taken into account. In particular, we see that just by using $log(p(\tilde{\x}))$ we can capture between $[0.55-0.71]$ of the information depending on the metric. Then, for all the metrics, the consideration of additional factors (obviously) improves ICC, but this improvement saturates as new factors contain less independent information about the sensitivity.
 
% In the following sections, we analyse the relations using machine learning regression models and study possible functional forms that include one or two factors and their effect on the predictability of the perceptual sensitivity. 

\iffalse
\begin{table}[b]
\footnotesize
\caption{Information coefficient of correlation~\citep{Linfoot57} (ICC) between the sensitivity and the probabilistic factors. Each row includes the factor that maximises the ICC, 1D: $log(p(\tilde{x}))$, 2D: \{$log(p(\tilde{x}))$, $\sigma(x)$\}, 3D: \{$log(p(\tilde{x}))$, $\sigma(x)$, $log(p(\x))$\}, 4D: \{$log(p(\tilde{x}))$, $\sigma(x)$, $log(p(\x))$, $\mu(x)\}$, 5D: \{$log(p(\tilde{\x}))$, $\sigma(x)$, $log(p(\x))$, $\mu(x)$, $||J(x)||$\}, and 6D all of them.}
\centering
\begin{tabular}{lllllll}
\\ \textbf{Factors}                   & \textbf{MS-SSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} & \textbf{Mean} \\
1D  & 0.55 & 0.57 & 0.71 & 0.68 & 0.61 & 0.62\\
2D  & 0.57 & 0.66 & 0.72 & 0.69 & 0.63 & 0.65\\
3D  & 0.68 & 0.68 & 0.72 & 0.69 & 0.65 & 0.68\\
4D & 0.68 & 0.76 & 0.75 & 0.71 & 0.66  & 0.71 \\
5D & 0.68 & 0.78 & 0.75 & 0.73 & 0.66  & 0.72  \\
6D & 0.71 & 0.79 & 0.76 & 0.73 & 0.66  & 0.73          
%all & 0.26286881 0.44154917 0.35763207 0.31760573 0.26177006
\end{tabular}
\label{tab:MI}
\end{table}
\fi

\begin{table}[b]
\setlength\tabcolsep{4pt}
\footnotesize
\caption{\footnotesize{Information Coefficient of Correlation between the sensitivity and groups of probabilistic factors.}}
\vspace{-0.1cm}
\centering
\begin{tabular}{lllllll}
\hline
\vspace{-0.25cm}
\\ \textbf{Factors}                   & \textbf{MS-SSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} & \textbf{mean} \\
1D: $\,\,\,log(p(\tilde{x}))$  & 0.55 & 0.57 & 0.71 & 0.68 & 0.61 & 0.62\\
2D: \{$log(p(\tilde{x}))$, $\sigma(x)$\}  & 0.57 & 0.66 & 0.72 & 0.69 & 0.63 & 0.65\\
3D: \{$log(p(\tilde{x}))$, $\sigma(x)$, $log(p(\x))$\}  & 0.68 & 0.68 & 0.72 & 0.69 & 0.65 & 0.68\\
4D: \{$log(p(\tilde{x}))$, $\sigma(x)$, $log(p(\x))$, $\mu(x)\}$ & 0.68 & 0.76 & 0.75 & 0.71 & 0.66  & 0.71 \\
5D: \{$log(p(\tilde{\x}))$, $\sigma(x)$, $log(p(\x))$, $\mu(x)$, $||J(x)||$\} & 0.68 & 0.78 & 0.75 & 0.73 & 0.66  & 0.72  \\
6D: all factors & 0.71 & 0.79 & 0.76 & 0.73 & 0.66  & 0.73 \\\hline         
%all & 0.26286881 0.44154917 0.35763207 0.31760573 0.26177006
\end{tabular}
\label{tab:MI}
\end{table}

\subsection{Accurate but non-interpretable model: non-parametric regression}
\label{sec:ML_regres}
%\red{We explore simple interpretable models and how probabilistic factors can be combined in order to predict perceptual sensitivity.}

The ultimate goal of an interpretable explanation of the sensitivity from probabilistic factors is obtaining a simple parametric function of the considered factors that predicts sensitivity. However, simple interpretable models will perform worse than more flexible non-parametric models. Despite the non-interpretability, non-parametric regression is useful to set a reference in performance (upper bound) to assess the quality of the simple interpretable models that we may develop.

In order to estimate this reference in performance, we consider a random forest regressor~\citep{breiman2001random} to predict the perceptual sensitivity from the probabilistic factors and 2nd order polynomial combinations of the factors. The inverse probability for both original and noisy images is also included. Regression trees are convenient for this task since it is easy to analyse the relevance of each feature and compare between models trained on different perceptual sensitivities. Values of feature importance are normalised so that they sum to $1$ for each model for easy comparison. We use a held out test set of 30\% dataset in order to calculate correlations between predicted and ground truth. The average Pearson correlation obtained is 0.85, which serves as an illustrative upper bound reference for the interpretable functions proposed below in Section~\ref{sec:func_form}. We also trained a simple 3 layer multilayer perceptron on the same data and also achieved a correlation of 0.85.

Figure \ref{fig:regres_tree} in Appendix~\ref{sec:app_coefficients_rndfrst} shows the 6 probabilistic factors with the bigger importance across perceptual distances and their relative relevance for each distance. The Pearson (Spearman) correlations indicate how good the regression models are at predicting sensitivity. $log(p(\tilde{\x}))$ is by far the most important factor, which agrees with what was found in the mutual information analysis (sec.~\ref{sec:MI}). It has been suggested that the derivative of the log-likelihood is important in learning representations~\citep{bengio2013} since modifications in the slope of the distribution imply label change and the score-matching objective makes use of the gradient. However, we find that this derivative has low mutual information with human sensitivity and low influence in the regression.

\subsection{Interpretable model: Simple Functional Forms}
\label{sec:func_form}

%Given the mutual information results (Sec.~\ref{sec:MI}) and the machine learning regression exploration (Sec.~\ref{sec:ML_regres}), here we explore the actual functional form that can extract that information. 
%As a reference, the Random Forest regressor fit with all the factors and polynomial combinations up to factor 2 (Sec.~\ref{sec:ML_regres}) obtains an average correlation among the IQMs of \red{0.84}, which can be taken as an upper bound. 

%In this section, 
%we want to find interpretable models based on the probabilistic factors that correlate with perceptual sensitivity.
In order to get simple interpretable models of sensitivity, in this section we restrict ourselves to linear combinations of power functions of the probabilistic factors. We explore two situations: (1)~a single-factor model, and (2) a two-factor model. 
According to the previous results on the relevance of the factors, the 1-factor model has to be based on $log(p(\tilde{\x}))$, and the 2-factor model has to be based on $log(p(\tilde{\x}))$ and $\sigma(\x)$.

%In each case, we conduct an ablation study, training a linear regressor over the sum of terms raised to a set of exponents, and ablating each factor in order to create various closed form expressions of perceptual sensitivity. We keep factors that maximise the Pearson correlation between the model's predictions and the ground truth.

%\subsection{One-factor model}
%\label{sec:1D}

\textbf{1-factor model (1F).} In the literature, the use of $log(p(\tilde{\x}))$ has been proposed using different exponents (see table \ref{tab:principles}). Therefore, first, we analysed which exponents get better results for a single component, i.e. $S(\x,\tilde{\x}) = w_0 + w_1~log(p(\tilde{\x}))^\gamma$. We found out that there is not a big difference between different exponents. Therefore we decided to go for the simplest solution: a regular polynomial (with natural numbers as exponents). Then, we explored which degree of the polynomial gets better results, and found that going beyond degree 2 does not substantially improve the correlation (detailed results are in Table~\ref{tab:1D} in Appendix~\ref{sec:parameter_selection}). We also explore a special situation where factors with different fractional and non-fractional exponents are linearly combined. The best overall reproduction of sensitivity across all perceptual metrics (Pearson correlation 0.73) can be obtained with a simple polynomial of degree two: 
\begin{equation}
    S(\x,\tilde{\x}) = w_0 + w_1~log(p(\tilde{\x})) + w_2~log(p(\tilde{\x}))^2, 
\label{eq:1_factor}
\end{equation}
where the values for the weights for each perceptual distance are in Appendix~\ref{sec:app_coefficients_functionalform} (table~\ref{tab:1D_coefs}). 

%The factor with the highest ICC and feature importance is $log(p(\tilde{\x}))$. We explore different polynomials to predict sensitivity where the different factors are forms of $p(\tilde{\x})$. Results are shown in the appendix in Table~\ref{tab:1D}. The correlation goes from around 0.68 for models with only one degree (i.e. $S(\x,\tilde{\x}) = w_0 + w_1 log(p(\tilde{\x}))^\gamma$ where $\gamma = \{1,\frac{1}{10},\frac{1}{5},\frac{1}{3},\frac{1}{2},2,-1\}$), to 0.73 which can be obtained with a simple polynomial of degree two ($d=2$). Therefore the selected candidate would be:
%\begin{equation}
%    S(\x,\tilde{\x}) = w_0 + w_1~log(p(\tilde{\x})) + w_2~log(p(\tilde{\x}))^2, 
%\label{eq:1_factor}
%\end{equation}
%with an average correlation of 0.73. The exact weights for each perceptual distance can be seen in the Appendix. 

%\subsection{Two-factors model}
%\label{sec:2D}

\textbf{2-factor model (2F).} In order to restrict the (otherwise intractable) exploration here we focus on polynomials that include the simpler versions of these factors isolated, i.e. $\{ log(p(\tilde{\x})), log(p(\tilde{\x}))^2 \}$  and $\{ \sigma(\x), \sigma(\x)^2, \sigma(\x)^{-1}\}$, and the simplest products and quotients using both.
We perform an ablation search of models that include these combinations as well as a LASSO regression~\citep{tibshirani1996regression} (details in Appendix~\ref{sec:parameter_selection} (table \ref{tab:app_2D})). In conclusion, a model that combines good predictions and simplicity is the one that simply adds the $\sigma(\x)$ factor to the 1-factor model: 
\begin{equation}
    S(\x,\tilde{\x}) = w_0 + w_1~log(p(\tilde{\x})) + w_2~log(p(\tilde{\x}))^2 + w_3~\sigma(\x)
\label{eq:2_factors}
\end{equation}
where the values of the weights for the different metrics are in Appendix~\ref{sec:app_coefficients_functionalform} (table~\ref{tab:2D_coefs}).
The 2-factor model obtains an average Pearson correlation of $0.79$ across the metrics, which implies an increase of $0.06$ in correlation in regard to the 1-factor model. 
As a reference, a model that includes all nine analysed combinations for the two factors achieves an average $0.81$ correlation (not far from the performance obtained with the non-parametric regressor, $0.85$, in section~\ref{sec:ML_regres}).

% QUESTIONS
%
%  * In the 1-factor model did you propose a single expression with all possible coefficients (and then ablate)?... why not?
%  * In the 1-factor model, when you say d=3, you mean exponents 0, 1, 2, 3, right?
%  * The coefficients are undetermined up to a factor, right? why PIM is so off?
%

\section{Validation: reproduction of classical psychophysics}
\label{validation}

%A challenging way to validate the interpretable models just proposed above is checking if the sensitivity represented by Eqs.~\ref{eq:1_factor} and~\ref{eq:2_factors} depends on luminance, spatial frequency and contrast in the same way as human sensitivity does. These behaviors were identified by classical psychophysics~\citep{Weber1846,Legge81,Campbell68,Georgeson75} way before the advent of current perceptual distances which are the basis of the proposed relations.

Variations of human visual sensitivity depending on luminance, contrast, and spatial frequency were identified by classical psychophysics~\citep{Weber1846,Legge80,Campbell68,Georgeson75} way before the advent of the state-of-the-art perceptual distances considered here. Therefore, the reproduction of the trends of those classic behaviours is an independent way to validate the proposed models since it is not related to the recent image quality databases which are at the core of the perceptual distances.

We perform two experiments for this validation: (1)~we compute the sensitivity to sinusoidal gratings of different frequencies but the same energy (or contrast), and we do that at different contrast levels, and (2)~we compute the sensitivity for natural images depending on luminance and contrast.
The first experiment is related to the concept of Contrast Sensitivity Function (CSF)~\citep{Campbell68}: the human transfer function in the Fourier domain, just valid for low contrasts, and the decay of such filter with contrast, as shown in the suprathreshold contrast matching experiments~\cite{Georgeson75}.
The second experiment is related to the known saturation of the response (reduction of sensitivity) for higher luminances: the Weber law~\citep{Weber1846}, and its equivalent (also reduction of the sensitivity) for contrast due to masking~\citep{Legge80}.

% There are two possible ways to do experiment 1!!
Gratings in Experiment 1 were generated with a fixed average luminance of 40 $cd/m^2$, and frequencies in the range [1,8] cycles per degree (cpd). We considered three Michelson contrast levels $c=\{0.2, 0.4, 0.7\}$. Barely visible noise in $\epsilon=1$ sphere was added to the gratings to generate the distorted stimuli $\tilde{\x}$, and we averaged the sensitivity for 100 samples of the noise.  
In Experiment 2, we define contrast as $c = \frac{\sigma(\x)\sqrt{2}}{\mu(\x)}$, which is applicable to natural images and reduces to Michelson contrast for gratings~\citep{Peli90}. We edited images from CIFAR10 to have the same contrast levels as in Experiment 1 and we explored the range of luminances $[60, 140]$ so that the digital values for maximum contrast still were in the [0,255] range. We averaged the sensitivities for 1000 images. Examples of the images for both experiments are given in Fig.~\ref{fig:test} in Appendix~\ref{sec:app_perc}. The resulting sensitivities for both experiments are plotted in Fig.~\ref{fig:psycho_tests}. 

\begin{figure}[htb]
    % \vspace{-0.2cm}
    \centering
    \includegraphics[width=.93\columnwidth]{figs/alex-figs/combined.png}
        % \vspace{-0.2cm}
    \caption{\footnotesize{Validation experiment 1 (Contrast Sensitivity Function, \emph{left}), and validation experiment 2 (Weber law and masking, \emph{right}) for the 1-factor (1F) and 2-factors (2F) models tuned to the DISTS distance.
    Color corresponds to the different contrast levels (low=0.2, medium=0.4, high=0.7).}}
    \label{fig:psycho_tests}
\end{figure}

Regarding the sensitivity to gratings (left), results reproduce the decay of the CSF at high frequencies, its decay with contrast, and how the shape flattens with contrast, consistently with~\citep{Campbell68,Georgeson75}. For sensitivity at different luminances and contrasts (right) there is a simultaneous reduction in both, consistent with the literature~\citep{Weber1846,Legge80}. Interestingly, both models 1-F and 2-F follow the same trends. Moreover, given the proportionality between the weights for the different distances (tables~\ref{tab:1D_coefs} and~\ref{tab:2D_coefs} in Appendix~\ref{sec:app_coefficients_functionalform}), the trends also hold for all distances.

\iffalse
Here we perform a simple test on the proposed models to check that, although they only rely on simple probabilistic properties of the image, they have perceptual behaviour. 

We use the definition of root mean squared error (RMSE) contrast of $c=\frac{\sigma(\mathbf{x})\sqrt{2}}{\mu(\mathbf{x})}$~\citep{Peli90}, and test the response for our one-factor (1F) and two-factors (2F) models in response to both frequency and mean luminance separately, across three contrast levels; low, medium and high where $c=\{0.2, 0.4, 0.7\}$. Since PixelCNN++ requires the resolution of our images to be $32\times32$, we generate gratings with a maximum frequency of 8 to ensure the gratings can be represented in the image. We also only sample mean luminance in a range of $[60, 140]$ so that we can accurately represent the contrast which at low and high luminance values, as we have to clip the images to be in the range $[0, 255]$. 
The sensitivities at different luminances for the three contrasts are averaged over 100 images. 
We use the models trained to predict the sensitivity of DISTS perceptual metric 
$S(\x, \tilde{\x})_{\textrm{DISTS}}$, however the results for other sensitivities are in Appendix~\ref{sec:app_perc}. We also test the models reliance on frequency, using gratings of different frequencies at the three given contrast levels. The frequencies are sampled such that aliasing is avoided. Details can be found in Appendix~\ref{sec:app_perc}.

Fig.~\ref{fig:test_CSF_ext} shows results for tests with different contrast and frequency, results are similar to the ones obtained for the human contrast sensitivity function \cite{Campbell68}, i.e. bigger sensitivity for low frequencies and low contrast. Fig.~\ref{fig:test_CSF_lum} shows results for sensitivities at different luminance levels for the three contrast levels. We see high sensitivity to lower contrast images and at high contrast we see a flattening of the sensitivity as luminance increases. This behaviour is consistent with the human CSF. Results for other tests, including testing the models trained to predict other distances, can be seen in the Appendix~\ref{sec:app_perc}.

%\begin{figure}[htb]
%    \centering
%    \begin{subfigure}[c]{.4\textwidth}
%        \centering
%        \includegraphics[width=\columnwidth]{figs/alex-figs/frequency_constrast.png}
%    \end{subfigure}
%    \begin{subfigure}[c]{.59\textwidth}
%        \centering
%        \vspace{-0.5cm}
%        \includegraphics[width=\columnwidth]{figs/Test/CSF_extended_1.png}
%    \end{subfigure}
%    \caption{\footnotesize{Perceptual tests on the one-factor (1F) and two-factors (2F) models for predicting the DISTS perceptual distance. Left plot shows the sensitivity in function of contrast and frequency of the image. Right: Stimuli used (low contrast top, high contrast bottom). Right: sensitivity for each stimulus.}}
%    \label{fig:test_CSF_ext}
%\end{figure}
%
%\begin{figure}[htb]
%    \centering
%    \begin{subfigure}[c]{.4\textwidth}
%        \includegraphics[width=\columnwidth]{figs/alex-figs/luminance_contrast.png}
%    \end{subfigure}
%    \begin{subfigure}[c]{.59\textwidth}
%         \centering
%         \vspace{-0.5cm}
%         \includegraphics[width=\columnwidth]{figs/alex-figs/luminance_contrast_images.png}
%    \end{subfigure} 
%    \caption{\footnotesize{Perceptual tests on the one-factor (1F) and two-factors (2F) models for predicting the DISTS perceptual distance. Left plot shows the sensitivity in function of contrast and luminance over a set of natural images. Right: Stimuli used (low contrast top, high contrast bottom).}}
%    \label{fig:test_CSF_lum}
%\end{figure}

\fi

\section{Discussion and Conclusion}
We show that several functions of image probability computed from recent generative models share substantial information with 
the sensitivity of state-of-the-art perceptual metrics (mean ICC of 0.73, Sec.~\ref{sec:MI}). 
Alongside this, a Random Forest regressor that predicts sensitivity from polynomial combinations of these probabilistic factors obtains an average Pearson correlation of 0.85 with actual sensitivity (Sec.~\ref{sec:ML_regres}). 
According to the shared information, the factors can be ranked as: $\{log(p(\tilde{\x})), \sigma(\x), log(p(\x)), \mu(\x),  ||J(\x)||\}$, in agreement with the relevance given by the regression tree that uses the same features.
Using only $log(p(\tilde{\x}))$ obtains an average ICC of 0.62, and a simple quadratic model including only this factor achieves a 0.73 Pearson correlation. This is due to simultaneously including information about the distribution of the original image (in a differential context $p(\tilde{\x}) \approx p(\x)$), and also about the specific direction of distortion.  
After an exhaustive ablation study keeping the factors with the highest ICC values, we propose simple functional forms of the sensitivity-probability relation using just 1 or 2 factors (Eqs.~\ref{eq:1_factor} based on $log(p(\tilde{\x}))$,
and Eq.~\ref{eq:2_factors} based on $log(p(\tilde{\x}))$ and $\sigma(\x)$). These obtain average Pearson correlations with sensitivity of 0.73 and 0.79 respectively.
These simple functions of image probability are validated through the reproduction of trends of the human frequency sensitivity, its suprathreshold variation, the Weber law, and contrast masking (Sec.~\ref{validation}).

This study is limited on the one hand by the probability model used (PixelCNN++), which was trained on a set of small images, with restricted contrast/luminance/color/content range. In order to have trustable $p(\x)$ we limited ourselves to using the same kind of (small, limited color range) images. As a proxy for human perception, we used perceptual metrics. Although they correlate with humans, even state-of-the-art metrics are not perfect. 
The ideal situation would be the direct reproduction of psychophysics (e.g. direct prediction of mean opinion score in image quality databases). However, this is not easy because the images in the subjective experiments usually fall out of the range where you can use the current probability models. 

Nevertheless, shared information and correlations are surprisingly high given that the statistics of the environment is not the only in driving factor in vision~\citep{Erichsen2012,Laughlin15}. In fact, this successful prediction of sensitivity from accurate measures of image probability with such simple functional forms is a renewed \emph{direct} evidence of the relevance of Barlow Hypothesis. Moreover, our results are in contrast with intuitions about the importance of the distribution gradient~\citep{bengio2013}: we found that the gradient of the probability has low shared mutual information and low feature influence in the human sensitivity.

\section*{Acknowledgements}
Thanks to UKRI Turing AI Fellowship EP/V024817/1, MICIIN/FEDER/UE under Grants PID2020-118071GB-I00 and PDC2021-121522-C21, and by Generalitat Valenciana under Projects CIPROM/2021/056, CIAPOT/2021/9 and the computer resources at Artemisa, funded by the European Union ERDF and Comunitat Valenciana as well as the technical support provided by the Instituto de F´ısica Corpuscular and IFIC (CSIC-UV).

% Besides in these databases, the number of images is smaller and the experiments are not done in a well controlled setup (for instance it is not clear what the luminance for each image pixel in the experimental screen is). 

% Beyond the previously established connection between probability and perception, in this work, we provide methods to quantify this relationship. We analyse the mutual information between various IQMs and probabilistic factors, and look at the performance of probabilistic factors in predicting perceptual sensitivity. We identify a number of factors that are strongly linked and perform an ablation study in order to propose function forms for perceptual sensitivity that depend only on probability, with a strong correlation between the ground truth and predicted sensitivity. These models shed light on the link between image statistics and perception.


\bibliography{ref}
\bibliographystyle{iclr2024_conference}

\appendix

\section{Performance of the considered perceptual metrics}
\label{sec:performance_metrics}

Image quality metrics are able to predict visual human perception up to some level. In table \ref{tab:MOS} we show results of the different image quality metrics used in this work (see sec. \ref{sec:metrics}) when evaluated on human rates databases. Although the metrics are not perfect, it is clear that they are a good proxy of human perception. We show results on a traditional perceptual dataset using large images, TID2013~\citep{tid2013-data}, and a more recent dataset with smaller images (size $64\times64$) but more distortions as using neural networks based ones, such as super-resolution, BAPPS~\citep{zhang2018unreasonable}.

\begin{table}[h]
\centering
\footnotesize
\caption{\footnotesize{Pearson and Spearman correlations with human opinion in TID2013, and agreement with human judgement (in \%) in BAPPS}.\vspace{-0.15cm}}
\label{tab:MOS}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
                                                                        & \textbf{MSSSIM}                                       & \textbf{NLPD}                                         & \textbf{PIM}                                          & \textbf{LPIPS}                                        & \textbf{DISTS}                                        \\ \hline
\begin{tabular}[c]{@{}l@{}}TID2013\\ $\rho_p$ ($\rho_s$)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.78\\ (0.80)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.84\\ (0.80)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.62\\ (0.65)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.74\\ (0.67)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.86\\ (0.83)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}BAPPS\\ (\%)\end{tabular}                    & 61.7                                                  & 61.5                                                  & 64.5                                                  & 69.2                                                  & 69.0                                                  \\ \hline
\end{tabular}
\end{table}


\section{Relevance of factors using random forest regressors.}
\label{sec:app_coefficients_rndfrst}

A random forest regression was fit using multiple combinations of the probability-related factors (see sec. \ref{sec:factors}) to predict the sensitivity of the different image quality metrics used in this work (see sec. \ref{sec:metrics}). A total of 55 combinations were introduced as inputs. Figure \ref{fig:regres_tree} shows the most relevant ones selected by the random forest algorithm.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=.95\textwidth]{figs/alex-figs/0.2/regression/together.png}
    \caption{\footnotesize{Top 6 feature importance from Random Forest regressors trained on polynomial combinations of the probabilistic factors in order to predict perceptual sensitivity. A separate model was trained for each perceptual distance. In the legend we include the Pearson (Spearman) correlation between the predictions and ground truth for a held out test set 30\% of the dataset.}}
    \label{fig:regres_tree}
\end{figure*}



% \clearpage
\section{Parameter selection for the functional forms.}
\label{sec:parameter_selection}
Here we show the details for the selection of the parameters in section 4. For the one-factor model, we tried different possibilities on the selected factor $log(p(\tilde{\x}))$, details on the correlation of the different possibilities with sensitivity of perceptual measures are given in table \ref{tab:1D} and section \ref{sec:func_form}.

\begin{table}[h]
\footnotesize
\caption{\footnotesize{Pearson correlation obtained between the prediction of the model and the sensitivity for different IQMs. All models are designed using versions of $log(p(\tilde{\x}))$ as input factor. The models are simple models with only \emph{one coefficient}, $S(\x,\tilde{\x}) = w_0 + w_1~log(p(\tilde{\x}))^\gamma$, or 
\emph{polynomials} of different degrees (d). The model \emph{Frac*} is a special polynomial with exponents: {[}0’3,0’2,0’1,0,1,2,3{]}.}}
\centering
    \label{tab:1D}
\begin{tabular}{lllllll}
\\         & \textbf{MSSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} & \textbf{Mean} \\
\hline
\multicolumn{7}{c}{One coefficient}
\\
\hline\\
$\gamma$ = 1/10                       & 0.71           & 0.64          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 1/5                      & 0.71           & 0.64          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 1/3                       & 0.71           & 0.64          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 1/2                       & 0.71           & 0.63          & 0.66         & 0.69           & 0.72           & \textbf{0.68} \\
$\gamma$ = 2                       & 0.69           & 0.63          & 0.64         & 0.68           & 0.71           & \textbf{0.67} \\
$\gamma$ = -1                       & 0.72           & 0.65          & 0.66         & 0.71           & 0.73           & \textbf{0.69} 
\\
$\gamma$ = 1                       & 0.7            & 0.63          & 0.65         & 0.68           & 0.72           & \textbf{0.68} \\
\hline
\multicolumn{7}{c}{Polynomials}\\
\hline\\
d = 2                     & 0.76           & 0.65          & 0.75         & 0.76           & 0.74           & \textbf{0.73} \\
d = 3                     & 0.76           & 0.65          & 0.75         & 0.75           & 0.74           & \textbf{0.73} \\
d = 6                     & 0.75           & 0.64          & 0.73         & 0.74           & 0.74           & \textbf{0.72} \\
\emph{Frac*} & 0.76           & 0.65          & 0.76         & 0.76           & 0.74           & \textbf{0.73} \\
\hline
\end{tabular}
\end{table}


For the two-factors model, we took the ones obtained in section 4.1 (i.e. $b$ (bias), $log(p(\tilde{\x}))$, and $log(p(\tilde{\x}))^2$), and factors of the standard deviation $\sigma_x$ as suggested by the mutual information in section \ref{sec:MI}. The combinations of the standard deviation have been alone: $\sigma_x$, $\frac{1}{\sigma_x}$,$\sigma_x^2$, and combined with the probability of the noisy image: $\frac{log(p(\tilde{\x}))}{\sigma_x}$, $\frac{\sigma_x}{log(p(\tilde{\x}))}$, and $log(p(\tilde{\x})){\sigma_x}$. 

There are 9 candidates but we want the most compact and interpretable model. Analysing all the possible combinations is intractable so we are going to perform an ablation study: we are going to discard different candidates sequentially starting from the largest model (9 candidates). Besides, we are going to use LASSO regression with different amounts of regularisation to get models with different amounts of factors as comparison to the ablation study. Results in table \ref{tab:app_2D} are shown in descending number of factors used. For each step, we remove the factor (or factors) that less influence has in the correlation. Besides, we show the correlation given by LASSO models where the regularization parameter has been adjusted in order to have the same number of factors. A model with 6 factors (number 17) has the same correlation (0.81) as the one with all the factors (number 1). The best trade-off between the number of factors and correlation is for models 25, 26, and 27, with 4 factors and a correlation of 0.79. We chose as our final functional model in section 4.2 the model 25 as its factors involve less computations.      

\input{Models_table.tex}

\clearpage

\newpage

\section{Coefficients of the functional forms.}
\label{sec:app_coefficients_functionalform}

In section~4 we propose Eqs.~\ref{eq:1_factor} and~\ref{eq:2_factors} as estimators of the perceptual sensitivity for 1- and 2-factor models respectively. Note that each metric has a different interpretation of the sensitivity units, therefore the weights of the proposed equations are different for each measure. However the proportion of the weights for each probability factor are almost the same.
In tables~\ref{tab:1D_coefs} and~\ref{tab:2D_coefs} we give the actual weights obtained in the experiments for each distance.

%\begin{table*}[htb]
%\centering
%\caption{\footnotesize{Coefficients for the Eq.~3 for each metric.}}
%\label{tab:1D_coefs}
%\begin{tabular}{llllll}
%Coefs                & \textbf{MSSIM} & \textbf{NLPD}  & \textbf{PIM}   & \textbf{LPIPS} & \textbf{DISTS} \\
%$b$                  & 29.5           & 65             & 15400           & 198            & 161            \\
%$log(p(\tilde{\x}))$   & $4.9~10^{-3}$   & $9.5~10^{-3}$  & 2.62           & $3.33~10^{-2}$ & $2.58~10^{-2}$ \\
%$log(p(\tilde{\x}))^2$ & $2.05~10^{-7}$ & $3.62~10^{-7}$ & $1.11~10^{-4}$ & $1.41~10^{-6}$ & $1.05~10^{-6}$
%\end{tabular}
%\end{table*}

\begin{table*}[htb]
\centering
\caption{\footnotesize{Coefficients for the Eq.~3 for each metric.}}
\label{tab:1D_coefs}
\begin{tabular}{llllll}
Coefs                & \textbf{MSSIM} & \textbf{NLPD}  & \textbf{PIM}   & \textbf{LPIPS} & \textbf{DISTS} \\
\hline 
$w_0$                  & 29.5           & 65             & 15400           & 198            & 161            \\
$w_1$   & $4.9~10^{-3}$   & $9.5~10^{-3}$  & 2.62           & $3.33~10^{-2}$ & $2.58~10^{-2}$ \\
$w_2$ & $2.05~10^{-7}$ & $3.62~10^{-7}$ & $1.11~10^{-4}$ & $1.41~10^{-6}$ & $1.05~10^{-6}$
\end{tabular}
\end{table*}

%\begin{table*}[htb]
%\centering
%\caption{\footnotesize{Coefficients for the Eq.~4 for each metric.}}
%\label{tab:2D_coefs}
%\begin{tabular}{llllll}
%Coefs                & \textbf{MSSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} \\
%$b$                  & $28$     & $58$    & $15100$  & $194$    & $156$    \\
%$log(p(\tilde{\x}))$   & $4.69~10^{-3}$   & $8.19~10^{-3}$  & $2.57$  & $3.26~10^{-2}$   & $2.49~10^{-2}$   \\
%$log(p(\tilde{\x}))^2$ & $1.96~10^{-7}$   & $3.09~10^{-7}$  & $1.09~10^{-4}$ & $1.37~10^{-6}$   & %$1.00~10^{-6}$   \\
%$\sigma(x)$          & $-0.597$  & $-3.74$  & $-141$ & $-1.93$   & $-2.54$ 
%\end{tabular}
%\end{table*}

\begin{table*}[htb]
\centering
\caption{\footnotesize{Coefficients for the Eq.~4 for each metric.}}
\label{tab:2D_coefs}
\begin{tabular}{llllll}
Coefs                & \textbf{MSSIM} & \textbf{NLPD} & \textbf{PIM} & \textbf{LPIPS} & \textbf{DISTS} \\
\hline
$w_0$                  & $28$     & $58$    & $15100$  & $194$    & $156$    \\
$w_1$   & $4.69~10^{-3}$   & $8.19~10^{-3}$  & $2.57$  & $3.26~10^{-2}$   & $2.49~10^{-2}$   \\
$w_2$ & $1.96~10^{-7}$   & $3.09~10^{-7}$  & $1.09~10^{-4}$ & $1.37~10^{-6}$   & $1.00~10^{-6}$   \\
$w_3$          & $-0.597$  & $-3.74$  & $-141$ & $-1.93$   & $-2.54$ 
\end{tabular}
\end{table*}
\section{Perceptual tests on the model}
\label{sec:app_perc}
Here we show the stimuli used for both perceptual tests performed using the proposed models.
\begin{figure}[h]
    \centering
    \begin{tabular}{c}
     \includegraphics[width=\columnwidth]{figs/stimuli.JPG} 
    \end{tabular}
    \caption{Stimuli for the perceptual tests of the proposed model. (left) Gratings are generated at wavelengths that can be accurately shown in a $32\times32$ spatial size, generated at contrasts of $[0.2, 0.4, 0.7]$. (right) Shows one example of editing an image from the CIFAR10 dataset to vary contrast of $[0.2, 0.4, 0.7]$ and luminance values in $[60, 140]$.}
    \label{fig:test}
\end{figure}

\iffalse

\begin{figure}
    \centering
    \begin{tabular}{ccc}
     \includegraphics[width=.31\columnwidth]{figs/Test/luminance.png} & 
     \includegraphics[width=.31\columnwidth]{figs/Test/Contrast.png} & 
     \includegraphics[width=.28\columnwidth]{figs/Test/CSF.png} 
    \end{tabular}
    \caption{Perceptual tests of the two-factors model. Sensitivity in function of luminance, contrast and frequency of the image. Stimuly used are shown on top of each figure.} %Mutual information is normalised for each IQM.
    \label{fig:test}
\end{figure}

\fi

\end{document}
