
\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

\usepackage{bm}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2023-preprint}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{makecell}
\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\input{cmd}
\newif \ifsourcecode
\sourcecodetrue

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{\model: Spectral GNNs with Learnable Orthonormal Basis}

\begin{document}

\twocolumn[
\icmltitle{\model: Spectral GNNs with Learnable Orthonormal Basis}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Qian Tao}{equal,ali}
\icmlauthor{Zhen Wang}{equal,ali}
\icmlauthor{Wenyuan Yu}{ali}
\icmlauthor{Yaliang Li}{ali}
\icmlauthor{Zhewei Wei}{ruc}
\end{icmlauthorlist}

\icmlaffiliation{ali}{Alibaba Group}
\icmlaffiliation{ruc}{Renmin University of China}

\icmlcorrespondingauthor{Wenyuan Yu}{wenyuan.ywy@alibaba-inc.com}
\icmlkeywords{Graph Neural Networks, spectral GNNs, Orthonormal Polynomials, Regularization}

\vskip 0.3in
]
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, \model, with \textbf{L}earnable \textbf{O}rtho\textbf{N}ormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of \model, where the results imply its superiority.
\end{abstract}

\input{0_intro}
\input{1_pre}
\input{2_mot}
\input{3_sol}
\input{5_exp}
\input{4_rel}
\input{6_con}
c
\nocite{langley00}

\clearpage
\bibliography{longnn}
\bibliographystyle{icml2023}

\newpage
\appendix
\onecolumn

\input{appendix}


\end{document}
