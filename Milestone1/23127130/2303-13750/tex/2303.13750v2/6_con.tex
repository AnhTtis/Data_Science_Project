\section{Conclusion and Future Directions}
\label{sec:con}
In this paper, we first identify the so-called over-passing issue and provide empirical and theoretical evidence. To alleviate this issue, namely,  regularizing the output values of a graph filter, we propose a principled way of regularizing the filter function. Meanwhile, we show that trivially regularizing polynomial coefficients, which most existing spectral GNNs adopt, could be more theoretically sound. All these naturally motivate us to propose {\model}, a spectral GNN that optimizes not only the parameters of vanilla spectral GNNs as in previous works but also the parameters of Jacobi polynomials such that suitable orthonormal basis can be effectively learned and regularized.
Experiments on fitting filters and node classification tasks validate the superiority of {\model}.

An interesting direction for future works is to make more parameters in spectral GNNs learnable, like the highest degree of polynomial $K$, so that more flexible GNNs could be achieved.