\vspace{-2pt}
\section{{\model}: Spectral GNNs with Learnable Orthonormal Basis}
In this section, we propose 
{\model} (GNN with \underline{L}earnable {\underline{O}rtho{\underline{N}ormal basis}), which not only utilize the orthonormal polynomials as the basis of the spectral GNNs, but also learn the proper basis via trainable polynomials.

\subsection{From Orthogonal to Orthonormal Basis}
\label{subsec:lon-orthnorm}
The discussion in \secref{sec:motivations} inspires us to design the spectral graph neural networks based on the \textit{orthonormal polynomials}.
When an orthonormal basis $\{P^*_0(\lambda),P^*_1(\lambda),...,P^*_K(\lambda)\}$ is utilized in spectral GNNs, the regularization on coefficients is equivalent to the regularization on filters.
As a result, an $\ell_2$-norm for coefficients is enough for regularizing the filters.
\begin{theorem}
    For a spectral GNN $f_{\alpha,\theta}(\bm{X},\normlap)=g^*_\alpha(\normlap)t_\theta(\bm{X})$ with orthonormal polynomials $\{P_0^*(\lambda),P_1^*(\lambda),...,P_K^*(\lambda)\}$, a regularization on the coefficients of the polynomials is equivalent to the regularization on the filters.
\end{theorem}
\vspace{-8pt}
\begin{proof}
    Orthonormal polynomials satisfy for any $i,j\in\{0,\ldots,K\},$
    \begin{align}
        &\int_{-1}^1 (P_i^*(\lambda))^2w(\lambda)\mathrm{d}\lambda=1 \notag \\
        &\int_{-1}^1 P_i^*(\lambda)P_j^*(\lambda)w(\lambda)\mathrm{d}\lambda=0 \text{ } (i\neq j). 
    \end{align}
    A similar reduction following \equref{equ:func-norm-orth} indicates that 
    \vspace{-4pt}
    $$||g^*_\alpha(\lambda)||^2=\sum_{i=0}^K \alpha_i^2||P_i^*(x)||^2=\sum_{i=0}^K\alpha_i^{2}$$
    which is exactly the square of $\ell_2$-norm of coefficients.
\end{proof}
\vspace{-16pt}
Finally, the proposed model can be represented by 
\begin{align}\label{equ:ton-prop}
\bm{\hat{Y}}=f_{\alpha,\theta}(\bm{X},\normlap)=g^*_{\alpha}(\normlap)t_{\theta}(\bm{X}) 
\end{align}
with regularization
\vspace{-10pt}
\begin{align}
\label{equ:ton-wd}
\min_{\alpha,\theta}\mathcal{L}(f_{\alpha,\theta}(\bm{X},\normlap),\bm{Y})+\sum_{i=0}^K \alpha_i^2.
\end{align}

\begin{table}[t]
    \caption{Common Orthogonal Polynomials and Norms.}
    \label{tab:orth-poly}
    \vspace{-6pt}
    \begin{center}
    \begin{tabular}{p{1.2cm}cc}
    \toprule
    Poly & $w(\lambda)$ & $||P_i(\lambda)||^2$  \\
    \midrule
    Chebyshev    & $1/\sqrt{1-\lambda^2}$ &$
        \begin{cases}
             \pi \text{ if }i=0 \\
             \frac{\pi}{2} \text{otherwise} 
        \end{cases}$\\
    Legendre & 1 & 1/(2i+1)\\
    Jacobi    & $(1-\lambda)^a(1+\lambda)^b$ & $\frac{2^{a+b+1}\Gamma(i+a+1)\Gamma(i+b+1)}{(2i+a+b+1)\Gamma(i+a+b+1)i!}$ \\
    \bottomrule
    \end{tabular}
    \end{center}
    \vskip -0.3in
    \end{table}

\stitle{Computation of Orthonormal Polynomials.}
The orthonormal polynomials can be generated from orthogonal polynomials by dividing each basis to its norm.
The weight functions and norm of common orthogonal polynomials can be found in \tabref{tab:orth-poly}.

\begin{theorem}[\cite{book2011orth}]
    Given a set of orthogonal polynomials $\{P_0(\lambda),P_1(\lambda),...,P_K(\lambda)\}$, the set of polynomials $\{P_0^*(\lambda),P_1^*(\lambda),...,P_K^*(\lambda)\}$ is orthonormal with
    $$P_i^*(\lambda)=\frac{P_i(\lambda)}{||P_i(\lambda)||}.$$
\end{theorem}

In practice, we choose the orthonormal Jacobi polynomials as basis, because:
i) a large number of orthogonal polynomials, like Chebyshev and Legendre polynomials in \tabref{tab:orth-poly} and the Zernike polynomials~\cite{lakshminarayanan2011zernike}, can be regarded as a special case of Jacobi polynomial by choosing corresponding values of $a$ and $b$; ii) as discussed in \secref{subsec:motivation-l2norm}, we could use the parameters $a$ and $b$ of Jacobi polynomials to define the preference to the signals with different values.
The recurrence relation of orthonormal Jacobi polynomials can be defined as:
\begin{align}
    \label{equ:ton-poly}
    &P_0(\normlap)=1;\ P_1(\normlap)={0.5a-0.5b}+(0.5a+0.5b+1)\normlap \notag \\
    &P_i(\normlap)=(2i+a+b-1) \notag \\
    &\cdot\frac{(2i+a+b)(2i+a+b-2)\normlap+a^2-b^2}{2i(i+a+b)(2i+a+b-2)}P_{i-1}(\normlap) \notag \\
    &-\frac{(i+a-1)(i+b-1)(2i+a+b)}{ i(i+a+b)(2i+a+b-2)}P_{i-2}(\normlap) \notag \\
    &||P_i(\lambda)||=\sqrt{\frac{2^{a+b+1}\Gamma(i+a+1)\Gamma(i+b+1)}{(2i+a+b+1)\Gamma(i+a+b+1)i!}}\notag \\
    &P_i^*(\normlap)=\frac{P_i(\normlap)}{||P_i(\lambda)||}. 
\end{align}

\equref{equ:ton-prop}, \equref{equ:ton-wd}, and \equref{equ:ton-poly} together compose the spectral GNN based on orthonormal basis.

\subsection{Learnable Orthonormal Basis}
\label{subsec:lon-learnable}
As discussed in \secref{subsec:motivation-overpass}, the weight function of the norm of a polynomial in fact represents the preference to the values distributed on $[-1,1]$.
Consider the Jacobi polynomial, whose weight function is $(1-\lambda)^a(1+\lambda)^b$.
For example, if $a=b=0$ such that the polynomial degenerates to the Legendre polynomial, the weight function becomes a constant $1$, which means the outputs of the polynomial in $[-1,1]$ are equally treated.
However, when $a>0$ and $b=0$, the weight function will be $(1-\lambda)^a$ and monotonically decrease.
In such a case, the polynomial represents a preference for smaller values close to $-1$.
    
The above discussion inspires us to learn the orthonormal basis by making the parameters $a,b$ in Jacobi polynomials trainable.
In each epoch, the model forward propagates based on \equref{equ:ton-prop} and \equref{equ:ton-poly}, and updates $a,b$ and other parameters.
This indicates that in every epoch, {\model} formalizes a different orthonormal Jacobi polynomial based on different $a$ and $b$.

\stitle{Differentiability.}
The derivation of the Gamma function in the norm of Jacobi polynomials can be obtained by the Polygamma function:
$$\Gamma'(\lambda)=\Gamma(\lambda)\psi^{(0)}(\lambda).$$
Also notice that $a$ and $b$ in the recurrence relation (\ie~ \equref{equ:ton-poly}) are differentiable, it is feasible to learn a proper $a$ and $b$ while training in {\model}.


    \begin{table*}[th!]
    \caption{Statistics of Real-World Datasets.}
    \label{tab:exp-sta}

    \begin{center}
    \begin{tabular}{ccccccccccc}

    \toprule
      & Cora & Citeseer & Pubmed & Computers & Photo & Chameleon & Squirrel & Actor & Texas & Cornell  \\
    \midrule
    Nodes & 2708 & 3327 & 19717 & 13752 & 7650 & 2277 & 5201 & 7600 & 183 & 183 \\
    Edges & 5278 & 4552 & 44324 & 245861 & 119081 & 31371 & 198353 & 26659 & 279 & 277 \\
    Features & 1433 & 3703 & 500 & 767 & 745 & 2325 & 2089 & 932 & 1703 & 1703 \\
    Classes & 7 & 6 & 5 & 10 & 8 & 5 & 5 & 5 & 5 & 5 \\
    homophily & 0.825 & 0.718 & 0.792 & 0.802 & 0.849 & 0.247 & 0.217 & 0.215 & 0.057 & 0.301 \\
    \bottomrule 
    
    \end{tabular}
    \end{center}
\end{table*}

\begin{table}[t!]
    \caption{Average Loss in Learning Filters Experiments.}
    \label{tab:exp-res-fit}
    \vspace{4pt}
    \begin{center}
    \begin{tabular}{p{1.2cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}

    \toprule
     & LOW & HIGH & BAND & REJECT & COMB  \\
    \midrule
    GPRGNN    & $0.4169$ & $0.0943$ & $3.5121$ & $3.7917$ & $4.6549$ \\
    ARMA    & $1.8478$ & $1.8632$ & $7.6922$ & $8.2732$ & $15.121$ \\
    ChebNet & $0.8220$ & $0.7867$ & $2.2722$ & $2.5296$ & $4.0735$ \\
    BernNet & $0.0314$ & $0.0113$ & $0.0411$ & $0.9313$ & $0.9982$ \\
    JacobiConv & $\bm{0.0003}$ & $0.0011$ & $0.0213$ & $\bm{0.0156}$ & $0.2933$ \\
    \midrule
    {TON} & $\bm{0.0003}$ & $\bm{0.0003}$ & $\bm{0.0156}$ & $\bm{0.0156}$ & $\bm{0.2870}$ \\
    \bottomrule

    \end{tabular}
    \end{center}
    \vskip -0.1in
    \end{table}
    
\subsection{Put Them Together}
\label{subsec:lon-together}

By the combination of the discussion in \secref{subsec:lon-orthnorm} and \secref{subsec:lon-learnable}, we propose {\model}.
{\model} employs the orthonormal Jacobi polynomial basis to achieve an equal regularization on filter values, and enables learnable parameters in the Jacobi polynomials such that more proper $a$ and $b$ can be achieved.

\stitle{Implementation.}
In the implementation, {\model} employs an MLP as the embedding of features following \cite{iclr21gprgnn,nips21bernnet}.
Following \cite{icml22jacobi}, we also implement multiple filters to flexibly adapt to multiple feature channels.
