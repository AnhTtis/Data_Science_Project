\vspace{-0.05in}
\section{Motivations}
\label{sec:motivations}
Existing research works on spectral GNN mainly focus on the study of expressiveness~\cite{iclr21gprgnn,nips21bernnet} and optimization~\cite{icml21optim-gnn,icml22jacobi}, while how to regularize the GNN model has not been systematically investigated. They either penalize the $\ell_2$-norm of polynomial coefficients with a unified strength or ignore regularization. Therefore, we present our observations and analysis of such a regularization, which motivate us to propose \model.

\subsection{Learning Filters without Regularization: The Over-Passing Issue}
\label{subsec:motivation-overpass}
Generally, a graph filter is defined as a function that maps from $[0, 2]$ to $[0, 1]$, where the eigenvalues of $\normlap$ lie in that domain.
However, we observe that, without regularization, the output values of a learned filters are likely to dramatically increase, leading to the so-called over-passing issue.


\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
           \centering
           \includegraphics[width=\textwidth,trim=4 6 40 10,clip]{figs/filters/filter-eph0.jpg}
            \caption{Filter at epoch=0.}
            \label{subfig:filter-eph40}
    \end{subfigure}
    \hspace{4pt}
    \begin{subfigure}[b]{0.22\textwidth}
            \centering
            \includegraphics[width=\textwidth,trim=4 6 40 10,clip]{figs/filters/filter-eph80.jpg}
            \caption{Filter at epoch=80.}
            \label{subfig:filter-eph80}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
            \centering
            \includegraphics[width=\textwidth,trim=4 6 40 10,clip]{figs/filters/filter-eph120.jpg}
            \caption{Filter at epoch=120.}
            \label{subfig:filter-eph120}
    \end{subfigure}
    \hspace{4pt}
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim=4 6 40 10,clip]{figs/filters/filter-eph160.jpg}
        \caption{Filter at epoch=160.}
        \label{subfig:filter-eph160}
    \end{subfigure}
    \vspace{-0.1in}
     \caption{Learned graph filters at different epochs.}
    \label{fig:filter-ephs}
    \vspace{-0.2in}
\end{figure}

\stitle{The Over-Passing Issue.}
We present the filters learned by GPRGNN~\cite{iclr21gprgnn} on the Cornell dataset at different epochs in \figref{fig:filter-ephs}, from which we attain the following intuitive observations:
i) at the late stage of the training process (from epoch $=120$ to $160$), the \emph{waveform}~\cite{smith1997guidetodsp} (\ie the shape) of the filter remains stable but its \emph{amplitude} (\ie the range of the output) continues to increase; ii) even at the early stage when the model is learning the filter (from epoch $=0$ to $80$), the amplitude of the filter still increases; iii) starting from the $80$-th epoch, the output values lie in an invalid range, namely, become larger than one for the whole spectrums.
These observations indicate that the amplitude of the learned graph filter keeps increasing and does not contribute to the learning of desired filters.

We speculate that such an over-passing issue is rooted in an updating direction in the parameter space where trivially increasing the absolute value of polynomial coefficients can reduce the training loss, but the learned filter is not improved.
Then we formalize this relationship as follows:

\begin{proposition}
    \label{prop:over-passing}
    Assume a spectral GNN $\bm{\hat{Y}}=g_{\alpha}(\normlap)t_{\theta}(\bm{X})$ with cross entropy loss $\mathcal{L}$.
    Suppose currently the model has loss $l=\mathcal{L}(\bm{\hat{Y}}, \bm{Y})$ where $\bm{Y}$ is the ground truth of the task.
    There exists a spectral GNN $ \bm{\hat{Y}'}=g_{\alpha'}(\normlap)t_{\theta'}(\bm{X})$ satisfying:
    \begin{itemize}
        \item $\theta'$ equals $\theta$;
        \item The output $\bm{\hat{Y}'}$ results in the same prediction as $\bm{\hat{Y}}$;
        \item The loss for nodes that are predicted correctly decreases.
    \end{itemize}
\end{proposition}

\begin{proof}
    Letting $\theta'=\theta$, we have $t_{\theta}(\bm{X})=t_{\theta'}(\bm{X})$.
    Assume $\alpha_i$ the coefficients and $\{P_0(\lambda),P_1(\lambda),...,P_K(\lambda)\}$ the polynomial basis of $g_\alpha(\cdot)$, \ie $g_{\alpha}(\lambda)=\sum_{i=0}^K\alpha_iP_i(\lambda)$.
    We enlarge the coefficients with the same factor $q>1$ and choose the enlarged values as the coefficients $\{\alpha_{i}'\}$, \ie $\alpha_i'=q\alpha_{i},i=0,\ldots,K$ and $g_{\alpha'}(\lambda)=\sum_{i=0}^K\alpha_i'P_i(\lambda)$.
    As a result, each element of $g_{\alpha'}(\normlap)$ will be enlarged by $q$ and $\bm{\hat{Y}'}$ will result in the same prediction as $\bm{\hat{Y}}$.
    
    Then we analyze the variation of the loss of the nodes that are predicted correctly.
    Let $\mathcal{V}^{+}$ be the nodes that are predicted correctly by $\bm{\hat{Y}}$ and $\bm{\hat{Y}'}$.
    The loss of $\bm{\hat{Y}}$ on $\mathcal{V}^{+}$ is
    \begin{align}
        &l=\sum_{v\in \mathcal{V}^{+}} \mathcal{L}(\bm{\hat{Y}}_{v},\bm{Y}_{v}) \notag \\
        &=\sum_{v\in \mathcal{V}^{+}}-\log \frac{e^{\bm{\hat{Y}}_{v,\bm{Y}_{v}}}}{\sum_{j=1}^c e^{\bm{\hat{Y}}_{v,j}}} \notag \\
        &=\sum_{v\in \mathcal{V}^{+}}\log (\sum_{j=1}^c e^{\bm{\hat{Y}}_{v,j}-\bm{\hat{Y}}_{v,\bm{Y}_{v}}}),
    \end{align}
    where $\bm{\hat{Y}}_{i,j}$ and $\bm{Y}_i$ are the $(i,j)$-th element of $\bm{\hat{Y}}$ and the $i$-th row of $\bm{Y}$, respectively.
    For the output $\bm{\hat{Y}'}$, we have $\bm{\hat{Y}'}=q\bm{\hat{Y}}$.
    Similarly, the loss of $\bm{\hat{Y}}$ on $\mathcal{V}^{+}$ becomes
    \begin{align}
        l'&=\sum_{v\in \mathcal{V}^{+}}\log (\sum_{j=1}^c e^{\bm{\hat{Y}'}_{v,j}-\bm{\hat{Y}'}_{v,\bm{Y}_{v}}}) \notag \\
        &=\sum_{v\in \mathcal{V}^{+}}\log (\sum_{j=1}^c e^{q(\bm{\hat{Y}}_{v,j}-\bm{\hat{Y}}_{v,\bm{Y}_{v}})}).
    \end{align}
    Since each node in $\mathcal{V}^{+}$ is predicted correctly, we have $\bm{\hat{Y}}_{v,j}-\bm{\hat{Y}}_{v,\bm{Y}_{j}}\leq 0$ for any $v\in \mathcal{V}^{+}$ and $j=0,1,...,c$.
    Also noticing that $q>1$, we have $l'<l$.
\end{proof}

Note that the conclusion of \propref{prop:over-passing} could be applied to the practical spectral GNNs with nonlinearity and multiple layers like \cite{nips21bernnet,icml22jacobi}. 
It indicates that when a considerable number of nodes are predicted correctly, the model may be updated towards the direction that the accuracy has slight change but the absolute values of polynomial coefficients grow continuously.
Besides, since the coefficients multiplied by a shared factor do not change the waveform of the learned filter, the over-passing issue makes most of the gradient occupied for increasing polynomial coefficients. Such a learning process neither contributes to learning the waveform of ground-truth filters, nor produces improvement on the performance of the model, which we name the over-passing issue.

\subsection{Learning Filters with $\ell_2$-Norm Penalization: An Unequal Regularization}
\label{subsec:motivation-l2norm}
{The over-passing issue indicates that the \emph{output} of the filter functions may increase unlimitedly while training the spectral GNNs.}
Strictly requiring the range of graph filter functions within $[0,1]$ can be nontrivial for spectral GNNs.
A natural way is to relax the requirement and penalize the norm of the filter function, that is to say, limit the output values of filters via regularization.
Some works regularize the spectral GNN model by penalizing the $\ell_2$-norm of the polynomial coefficients with a unified strength.
Then the model is often learned by solving the following optimization problem:
\begin{align}
\label{equ:regu-l2norm}
\min_{\alpha,\theta}\mathcal{L}(f_{\alpha,\theta}(\bm{X},\normlap),\bm{Y})+\sum_{i=0}^K \alpha_i^2.
\end{align}
To keep brevity, here we omit the $\ell_2$-norm for regularizing other trainable parameters, i.e., $\theta$.

This $\ell_2$-norm penalty can alleviate the over-passing issue to some extent.
However, as we will show, regularizing a graph filter in such a way cannot equally regularize its output values.
Inspired by the Rooted-Mean-Square amplitude (RMS amplitude)~
\cite{smith1997guidetodsp} of the filters, we define the norm of a polynomial based on the inner product of two functions, which could be regarded as the average output value of a graph filtering function in an interval.
\begin{definition}[Norm]
    \label{def:norm}
    Given a polynomial $p(x)$, the norm of $p(x)$ in interval $[l,u]$ is 
    $$||p(x)||=\sqrt{\int_l^u p^2(x)w(x)\mathrm{d}x}\quad,$$
    where $w(x)$ is the weight function that determines the preference on values in $[l,u]$.
\end{definition}

For a spectral GNN, its polynomial function $g(\cdot)$ is applied to the propagation matrix $\bm{P}$ (see \equref{eq:polyapproxgf}), and it is well known that the eigenvalues of $\bm{P}$ are in the range $[-1, 1]$.
Thus, we use $g(\lambda)$, $g(\bm{P})$, and $g(\normlap)$ exchangeably, where $\lambda\in[-1,1]$ denotes the eigenvalues of $\bm{P}$.
Meanwhile, \textit{w.l.o.g.}, we consider $[l, u] = [-1, 1]$ as the integral interval in the remaining of this paper.

To regularize the output values of the filtering function, we can add a penalty term in the loss function:
\begin{align}
\label{equ:regu-filter-raw}
    \min_{\alpha,\theta} \mathcal{L}(f_{\alpha,\theta}(\bm{X},\normlap),\bm{Y})+||g_{\alpha}(\lambda)||^{2},
\end{align}
where $g_{\alpha}(\lambda)=\sum_{i=0}^K \alpha_i P_i(\lambda)$ with trainable coefficients $\alpha_0,...,\alpha_K$ and polynomial basis $P_0(\lambda),P_1(\lambda),...,P_K(\lambda)$.

The integral form of the regularization term $||g_{\alpha}(\lambda)||^2$ makes it difficult to calculate the gradient of the coefficients.
\eat{
\begin{align}
\label{equ:regu-raw-to-orth}
||g_{\alpha}(x)||^2&=\int_{-1}^1(\sum_{i=0}^K\alpha_iP_i(\lambda))^2w(\lambda)\mathrm{d}\lambda \\
&=\sum_{i=0}^K\int_{-1}^1\alpha_i^2P_i^2(\lambda)w(\lambda)\mathrm{d}\lambda \notag \\
\text{ }&+2\sum_{i<j}\int_{-1}^1\alpha_i\alpha_jP_i(\lambda)P_j(\lambda)w(\lambda)\mathrm{d}\lambda \quad.\notag
\end{align}
}
However, if we utilize the orthogonal polynomials that share the same weight function as the definition of norm, \ie the polynomials satisfying
$$\int_{-1}^1 P_i(\lambda)P_j(\lambda)w(\lambda)\mathrm{d}\lambda=0 \text{ } (i\neq j),$$
according to the generalized Pythagorean theorem in inner product spaces~\cite{van2005classification}, the regularization term can be simplified as
\begin{align}\label{equ:func-norm-orth}
||g_{\alpha}(\lambda)||^2=\sum_{i=0}^K\int_{-1}^1\alpha_i^2P_i^2(\lambda)w(\lambda)\mathrm{d}\lambda=\sum_{i=0}^K\alpha_i^2||P_{i}(\lambda)||^2.
\end{align}
As a result, the regularization on filtering functions (\ie \equref{equ:regu-filter-raw}) becomes 
\begin{align}\label{equ:regularization}
    \min_{\alpha,\theta}\mathcal{L}(f_{\alpha,\theta}(\bm{X},\normlap),\bm{Y})+\sum_{i=0}^K \alpha_i^2||P_i(\lambda)||^2.
\end{align}

\vspace{-6pt}
\stitle{Unequality of $\ell_2$-Norm.}
Some recent spectral GNN works~\cite{icml22chebyrevisit,icml22jacobi} have adopted orthogonal polynomials and regularized the polynomial coefficients in the way of \equref{equ:regu-l2norm}. Although that regularization term has a similar form to penalizing the norm of the filtering function (\ie ~\equref{equ:regularization}), regularizing just the trainable coefficients cannot equally regularize the output values of the filtering function due to the unnormalized polynomial terms, and thus the over-passing issue is solved in an improper way.
In other words, they are not equivalent, as illustrated in \figref{fig:regular-filter-values}. Specifically, we consider a basis of Jacobi polynomials up to degree 2, where the norm of each term (i.e., $\|P_{i}(\lambda)\|,i=0,1,2$) is calculated in advance. We choose the polynomial coefficients $\alpha_i,i=0,1,2$ located on a sphere with a radius of 5, and thus their $\ell_2$-norm $\sum_{i=0}^2 \alpha_i^2$ always has a value of 25. Then we use the color to represent how large the regularization term in \equref{equ:regularization} is. As can be seen, although the regularization on coefficients keeps unchanged on the sphere, the regularization term of filter norm, i.e., $\sum_{i=0}^2 \alpha_i^2 ||P_i(\lambda)||^2$, has different values ranging from $20$ to $40$.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.25\textwidth,trim=80 40 40 55,clip]{figs/balls/ball_fwd.png}
    \caption{Color map of the regularization for filter values.}
    \label{fig:regular-filter-values}
    \vskip -0.2in
\end{figure}