\begin{table*}[t!]
\centering
    \caption{Experimental Results on Real-World Datasets.}
    \label{tab:exp-real-world-res}

    %\begin{center}
    \begin{tabular}{ccccccccccc}

    \toprule
     Datasets & GCN & APPNP & ChebNet & GPRGNN & BernNet & JacobiConv & {\model}  \\
    \midrule
    Cora & $87.14_{\pm 1.01}$ & $88.14_{\pm 0.73}$ & $86.67_{\pm 0.82}$ & $88.57_{\pm 0.69}$ & $88.52_{\pm 0.95}$ & $88.98_{\pm 0.46}$ & $\bm{89.44_{\pm 1.12}}$ \\
    Citeseer & $79.86_{\pm 0.67}$ & $80.47_{\pm 0.74}$ & $79.11_{\pm 0.75}$ & $80.12_{\pm 0.83}$ & $80.09_{\pm 0.79}$ & $80.78_{\pm 0.79}$ & $\bm{81.41_{\pm 1.15}}$  \\
    Pubmed & $86.74_{\pm 0.27}$ & $88.12_{\pm 0.31}$ & $87.95_{\pm 0.28}$ & $88.46_{\pm 0.33}$ & ${88.48_{\pm 0.41}}$ & ${89.62_{\pm 0.41}}$ & $\bm{90.98_{\pm 0.64}}$ \\
    Computers & $83.32_{\pm 0.33}$ & $85.32_{\pm 0.37}$ & $87.54_{\pm 0.43}$ & $86.85_{\pm 0.25}$ & $87.64_{\pm 0.44}$ & $90.39_{\pm 0.29}$ & $\bm{90.93_{\pm 0.82}}$ \\
    Photo & $88.26_{\pm 0.73}$ & $88.51_{\pm 0.31}$ & $93.77_{\pm 0.32}$ & $93.85_{\pm 0.28}$ & $93.63_{\pm 0.35}$ & $\bm{95.43_{\pm 0.23}}$ & $94.66_{\pm 0.52}$ \\
    Chameleon & $59.61_{\pm 2.21}$ & $51.84_{\pm 1.82}$ & $59.28_{\pm 1.25}$ & $67.28_{\pm 1.09}$ & $68.29_{\pm 1.58}$ & $\bm{74.20_{\pm 1.03}}$ & $73.00_{\pm 2.20}$ \\
    Actor & $33.23_{\pm 1.16}$ & $39.66_{\pm 0.55}$ & $37.61_{\pm 0.89}$ & $39.92_{\pm 0.67}$ & $\bm{41.79_{\pm1.01}}$ & $41.17_{\pm 0.64}$ & $39.10_{\pm1.59}$ \\
    Squirrel & $46.78_{\pm 0.87}$ & $34.71_{\pm 0.57}$ & $40.55_{\pm 0.42}$ & $50.15_{\pm 1.92}$ & $51.35_{\pm 0.73}$ & $57.38_{\pm 1.25}$ & $\bm{60.61_{\pm 1.69}}$ \\
    Texas & $77.38_{\pm 3.28}$ & $90.98_{\pm 1.64}$ & $86.22_{\pm 2.45}$ & $92.95_{\pm 1.31}$ & $93.12_{\pm 0.65}$ & $\bm{93.44_{\pm 2.13}}$ & $87.54_{\pm 3.45}$ \\
    Cornell & $65.90_{\pm 4.43}$ & $91.81_{\pm 1.96}$ & $83.93_{\pm 2.13}$ & $91.37_{\pm 1.81}$ & $92.13_{\pm 1.64}$ & $\bm{92.95_{\pm 2.46}}$ & $84.47_{\pm 3.45}$ \\
    \bottomrule  
    
    \end{tabular}
    %\end{center}
    \vskip -0.2in
\end{table*}

\section{Experiment}
This section presents the evaluation of the performance of {\model}.
We first use {\model} to learn the filters of the image datasets, then show the performance of {\model} on real datasets, report the ablation analysis of {\model}, and finally propose the evaluation on time cost.
All experiments are conducted on a machine with $4$ NVIDIA V100 32GB GPUs, Intel Xeon 96-core 2.50GHz CPU, and 400GB RAM. 
\ifsourcecode
Our code is available at \url{https://github.com/TaoLbr1993/LON-GNN}.
\else
\fi

\subsection{Evaluation on Learning Filters}

Following previous work~\cite{nips21bernnet,icml22jacobi}, we use the proposed models to learn the filters from $50$ images. 
Each image is treated as a graph, where every pixel represents a node and two neighboring pixels have an edge between the corresponding nodes.
$5$ typical filters, \ie low ($\exp\{-10\lambda^2\}$), high ($1-\exp\{-10\lambda^2\}$), band ($\exp\{-10(\lambda-1)^2\}$) and rejection ($|\sin{\pi\lambda}|$), are applied to the graph signal and the average square loss of each type of filters is reported.
We compare {\model} with previous models based on Spectral GNNs, including GPRGNN~\cite{iclr21gprgnn}, ARMA~\cite{pami22arma}, ChebNet~\cite{nips16chebyshev}, BernNet~\cite{nips21bernnet}, and JacobiConv~\cite{icml22jacobi}.

The implementation and searching space of {\model} follow a setting in ~\cite{icml22jacobi}, \ie the MLP and all dropout are removed and the coefficients of the polynomial are fixed as the initialization values.
Following ~\cite{icml22jacobi}, we use optuna~\cite{kdd19optuna} to choose the best parameters.
The learning rate and weight decay for parameters except for $a$ and $b$ are chosen from $\{\expnumber{5}{-4}, 0.001, 0.005, 0.01, 0.05\}$ and $\{0.0, \expnumber{5}{-5}, \expnumber{1}{-4}, \expnumber{5}{-4}, 0.001\}$, respectively.
The learning rate and weight decay of $a$ and $b$ is fixed as $0.0002$ and $0.0$ respectively.

\tabref{tab:exp-res-fit} reports the average square loss of the baselines and {\model}.
{\model} obtains the same loss with the best baseline JacobiConv on LOW and REJECT, and outperforms all baselines on HIGH, BAND and COMB.
Comparing with the baselines, {\model} decreases the loss by at least $72.7\%$, $26.8\%$, and $2.2\%$ on HIGH, BAND, and COMB, respectively.
Specifically, {\model} achieves a notable improvement on HIGH and BAND.
This implies that the spectral GNNs with learnable parameters have a more fitting capability compared with the baselines.

\begin{table*}[t!]
    \caption{Accuracy and Improvements of Jacobi+OrthNorm, Jacobi+Lernable and LON-GNN compared with SOTA baseline.}
    \label{tab:exp-ablation}

    \begin{center}
    \begin{tabular}
    {c|c|cc|cc|cc}
    \toprule
      & JacobiConv & \multicolumn{2}{c|}{Jacobi+OrthNorm} & \multicolumn{2}{c|}{Jacobi+Learnable} & \multicolumn{2}{c}{LON-GNN}  \\
      & Accuracy & Accuracy & Improvement & Acuracy & Improvement & Accuracy & Improvement \\
    \midrule
    Cora & $88.98$ & $88.52$ & $\downarrow 0.46$ & $89.28$ & $\uparrow 0.30$ & $89.44$ & $\uparrow 0.46$ \\
    Citeseer & $80.78$ & $80.93$ & $\downarrow 0.15$ & $81.64$ & $\uparrow 0.86$ & $81.41$ & $\uparrow 0.63$ \\
    Pubmed & $89.62$ & $89.93$ & $\uparrow 0.29$ & $89.93$ & $\uparrow 0.29$ & $90.98$ & $\uparrow 1.36$ \\
    Computers & $90.39$ & $90.51$ & $\uparrow 0.12$ & $90.66$ & $\uparrow 0.27$ & $90.93$ & $ \uparrow 0.54$ \\
    Photo & $95.43$ & $94.60$ & $\downarrow 0.83$ & $95.40$ & $\downarrow 0.03$ & $94.66$ & $\downarrow 0.77$ \\
    Chameleon & $74.20$ & $73.70$ & $\downarrow 0.50$ & $74.51$ & $\uparrow 0.31$ & $73.00$ & $\downarrow 1.20$ \\
    Actor & $41.17$ & $41.42$ & $\uparrow 0.25$ & $40.73$ & $\downarrow 0.44$ & $39.10$ & $\downarrow 2.07$ \\
    Squirrel & $57.38$ & $60.11$ & $\uparrow 2.73$ & $57.65$ & $\uparrow 0.27$ & $60.61$ & $\uparrow 3.23$ \\
    Texas & $93.44$ & $84.43$ & $\downarrow 9.01$ & $93.44$ & $\rightarrow 0.00$ & $87.52$ & $\downarrow 6.92$ \\
    Cornell & $92.95$ & $83.83$ & $\downarrow 9.12$ & $91.91$ & $\downarrow 1.04$ & $84.47$ & $\downarrow 7.44$ \\
    \midrule 
    \tabincell{c}{Improved Datasets} & - & \multicolumn{2}{c|}{5} & \multicolumn{2}{c|}{6} & \multicolumn{2}{c}{5} \\
    \bottomrule  
    
    \end{tabular}
    \end{center}
\end{table*}

\subsection{Evaluation on Real-World Datasets}
We also evaluate the performance of the proposed models on both homogeneous and heterogenous real-world datasets, the statistics of which are presented in \tabref{tab:exp-sta}.
The homophily of a graph is defined as the ratio of the number of edges connecting nodes with the same label to the number of all edges.
Depending on the level of homophily, datasets can be categorized into homogeneous and heterogeneous ones~\cite{nips20beyond-homo}.
For homogeneous datasets, we use the citation datasets Cora, Citeseer, and Pubmed~\cite{AImag08collect}, and the Amazon co-purchase datasets Computers and Photo~\cite{corr18pitfalls}.
For heterogeneous datasets, we choose the wikipedia datasets Chameleon and Squirrel~\cite{jcnetwork21multiscale}, the Actor co-occurrence datasets~\cite{iclr20geom}, and the WebKB datasets\footnote{http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/}.

Following previous works~\cite{iclr21gprgnn,nips21bernnet,icml22jacobi}, we randomly split the nodes into disjoint train/validation/test sets with ratio 60\%/20\%/20\%.
Expriments on each dataset are repeated 10 times and the average accuracy and $\pm 95\%$ confidence interval on test node sets are reported.
We compare {\model} with GCN~\cite{iclr17gcn}, APPNP~\cite{iclr19appnp}, ChebNet~\cite{nips16chebyshev}, GPRGNN~\cite{iclr21gprgnn}, BernNet~\cite{nips21bernnet}, and JacobiConv~\cite{icml22jacobi}.
In addition to the learning rate and weight decay in vanilla spectral GNNs, the learning rate of $a$ and $b$ is also considered in searching space and chosen from $\{\expnumber{5}{-4}, 0.001, 0.005, 0.01, 0.05\}$.

\tabref{tab:exp-real-world-res} shows the experimental results on real-world datasets.
Among $10$ dataset, {\model} beats all baselines on $5$ datasets, \ie Cora, Citeseer, Pubmed, Computers and Squirrel, and achieves the runner-up performance on Photo and Chameleon, which is competitive to the baselines.
Specifically, {\model} outperforms the baselines by at least $1.36\%$ on the homogeneous dataset Pubmed and $5.63\%$ on the heterogeneous dataset Squirrel.
The experimental results imply the competitiveness of {\model} on both homogeneous and heterogeneous datasets.


\subsection{Ablation Analysis}

To analyze the effectiveness of different techniques in {\model}, we compare {\model} with spectral GNNs with Jacobi polynomial basis (\ie JacobiConv in \cite{icml22jacobi}, with orthonormal Jacobi basis (namely Jacobi+OrthNorm), and with learnable Jacobi basis (namely Jacobi+Learnable).

\tabref{tab:exp-ablation} illustrates the performance of the $4$ models.
On $4$ datasets (Cora, Pubmed, Computers and Squirrel), Jacobi+OrthNorm and Jacobi+Learnable outperform JacobiConv and {\model} performs the best.
By the utilization of orthonormal basis and learnable parameters, all of the three algorithms, \ie Jacobi+OrthNorm, Jacobi+Learnable and LON-GNN, outperform JacobiConv on at least $5$ real-world datasets.
The experimental results implies the effectiveness of the proposed techniques in the paper.


\begin{table*}[t!]
    \caption{Experimental Results on Time Cost: Average Time per Epoch (ms)/Total Training Time (s).}
    \label{tab:exp-time-cost}

    \begin{center}
    \begin{tabular}{ccccccccccc}

    \toprule
     Datasets & GCN & APPNP & ChebNet & GPRGNN & BernNet & JacobiConv & {\model}  \\
    \midrule
    Cora & $6.16/3.67$ & $9.30/3.41$ & $9.16/3.50$ & $10.55/4.35$ & $32.31/12.97$ & $18.72/8.53$ & $37.29/14.09$ \\
    Citeseer & $6.81/1.66$ & $9.79/3.30$ & $12.59/3.12$ & $10.37/2.37$ & $33.01/9.13$ & $19.70/4.25$ & $33.81/12.84$  \\
    Pubmed & $7.03/4.66$ & $9.82/5.45$ & $12.30/7.72$ & $10.34/6.78$ & $33.82/22.30$ & $19.98/18.36$ & $39.18/28.15$ \\
    Computers & $8.73/5.40$ & $11.97/7.39$ & $38.53/24.77$ & $12.55/9.23$ & $55.65/36.84$ & $22.86/19.09$ & $41.87/32.11$ \\
    Photo & $6.80/5.24$ & $9.41/6.90$ & $20.77/16.78$ & $10.03/6.11$ & $34.24/25.55$ & $18.99/16.31$ & $37.32/30.28$ \\
    Chameleon & $6.13/4.43$ & $8.53/4.68$ & $27.81/11.87$ & $9.04/8.45$ & $24.91/15.19$ & $20.24/18.17$ & $42.24/38.09$ \\
    Actor & $7.01/1.62$ & $8.78/3.89$ & $18.05/4.37$ & $9.75/2.91$ & $25.05/8.17$ & $18.79/5.07$ & $39.52/15.67$ \\
    Squirrel & $7.55/3.86$ & $8.35/4.08$ & $87.14/38.14$ & $8.96/7.18$ & $31.21/30.52$ & $18.02/17.79$ & $39.04/36.49$ \\
    Texas & $6.20/3.61$ & $9.47/4.20$ & $7.68/4.37$ & $10.37/2.77$ & $32.89/12.35$ & $18.25/8.58$ & $37.35/17.11$ \\
    Cornell & $5.94/2.84$ & $9.17/2.93$ & $6.94/4.93$ & $9.81/2.11$ & $29.70/14.39$ & $17.65/9.43$ & $37.18/11.44$ \\
    \bottomrule  
    
    \end{tabular}
    \end{center}
\end{table*}

\subsection{Evaluation on Time Cost}
To evaluate the scalability of {\model}, we test the average time cost per epoch and the total training time of the baselines and {\model} on real-world datasets.

The experimental results are reported in \tabref{tab:exp-time-cost}.
{\model} spends more time than the baselines in most cases.
Compared with JacobiConv, {\model} spends up to $2.17\times$ times of time cost per epoch.
This is because JacobiConv utilizes a linear regression to embed the features, while {\model} uses an MLP.
Compared with BernNet, {\model} spends up to $1.70\times$ times of time cost per epoch.
The gap is mainly derived from the extra computation of the norm of orthogonal polynomials and learnable parameters $a$ and $b$.
In consideration of the SOTAs, the time cost of {\model} can be tolerant in practice.


