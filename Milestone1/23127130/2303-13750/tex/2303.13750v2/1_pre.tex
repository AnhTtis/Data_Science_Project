\section{Preliminaries}
\label{sec:pre}
Let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ denote an undirected graph, where $\mathcal{V}$ is a set of nodes with cardinality $n=|\mathcal{V}|$, and $\mathcal{E}$ is a set of edges.
The graph may also have features for each node, which are represented as a matrix $\bm{X}\in \Re^{n\times r}$, where $r$ is the dimension of the features. 
The adjacency matrix of the graph is represented by $\bm{A}$, such that $\bm{A}_{ij}=1$ if there is an edge between nodes $v_i$ and $v_j$, and $\bm{A}_{ij}=0$ otherwise. 
The degree matrix $\bm{D}$ is a diagonal matrix, where $\bm{D}_{ii}$ is the degree of node $v_i$, and $\bm{D}_{ij}=0$ for all $i\neq j$.
Graph Laplacian is then defined as $\bm{L}=\bm{D}-\bm{A}$.
We follow the convention to focus on its normalized counterpart $\normlap=\bm{D}^{-\frac{1}{2}}\bm{L}\bm{D}^{-\frac{1}{2}}=\bm{I}-\bm{D}^{-\frac{1}{2}}\bm{A}\bm{D}^{-\frac{1}{2}}$.
Considering node classification tasks, there is often a label matrix $\bm{Y}\in\Re^{n\times c}$, where $c$ is the number of classes, and each $i$-th row is a one-hot vector indicating which class $v_i$ belongs to.

Regarding $\bm{X}$ as graph signals, the graph filtering operation is defined as $\bm{Z}=\sigma(\bm{U}g(\Lambda)\bm{U}^{\text{T}})$, where $\Lambda$ denotes the diagonal eigenvalue matrix of $\normlap$, and $\sigma(\cdot)$ represents a normalization function. To avoid the computationally expensive eigendecomposition, most existing spectral GNN methods employ polynomials to approximate graph filter $g(\Lambda)$~\cite{iclr21gprgnn,nips21bernnet,icml22jacobi,icml22chebyrevisit,nips22evennet}:
\[
\bm{U}g(\Lambda)\bm{U}^{\text{T}}\bm{X}\approx\bm{U}(\sum_{k=0}^{K}\alpha_{i}\Lambda^{k})\bm{U}^{\text{T}}\bm{X}=\sum_{k=0}^{K}\alpha_{k}\normlap^{k}\bm{X},
\]
where $\{\alpha_k\}$ are coefficients.
These methods treat $\{\alpha_k\}$ as trainable parameters and thus can restate the graph filter with the propagation matrix $\bm{P}=\bm{I}-\normlap=\bm{D}^{\frac{1}{2}}\bm{A}\bm{D}^{-\frac{1}{2}}$ as:
\begin{equation}
g(\Lambda)=g(\normlap)=\sum_{k=0}^{K}\alpha_{k}\normlap^{k}=\sum_{k=0}^{K}\alpha_{k}P_{k}(\bm{P}),
\label{eq:polyapproxgf}
\end{equation}
where $\{P_{k}(\cdot)\}$ is a polynomial basis. Research works in this line have explored various kinds of polynomial bases.

Then, for the purpose of node classification, the output of a spectral GNN has the following form:
\begin{equation}
\bm{\hat{Y}}=f_{\alpha,\theta}(\bm{X},\normlap)=g_{\alpha}(\normlap)t_{\theta}(\bm{X}),
\end{equation}
where $g_\alpha(\normlap)$ is implemented based on the right-hand side of Eq.~\eqref{eq:polyapproxgf}, and $t(\cdot)$ is a linear or an MLP model that takes $\bm{X}$ as input and outputs the embeddings of shape $n\times c$.
Hence, the trainable parameters of such a spectral GNN include both $\{\alpha_k\}$ and the model parameters for transformation $\theta$.
They are learned by minimizing a loss function $\mathcal{L}(\cdot,\cdot)$, e.g., Cross-entropy loss, defined over $\bm{\hat{Y}}$ and $\bm{Y}$.