\section{Introduction}
\label{sec:intro}
Various kinds of graph neural network (GNN) models have been proposed in recent years, which are often motivated and designed from either spatial or spectral domains. The expressiveness of GNNs from a spatial viewpoint is often compared with the Weisfeiler-Lehman isomorphism test of various orders~\cite{iclr19powerful}. As for spectral GNNs, recent advances that make them adaptive filters allow them to fit arbitrary graph filters~\cite{corr20briginggap}. As a result, we have witnessed many successes achieved by spectral GNNs, notably the leading performances on many node-level tasks~\cite{icml22chebyrevisit}.

Despite their consistent expressiveness, different kinds of polynomial bases are explored with various motivations, such as their difference in convergence rate~\cite{icml22jacobi}. Researchers have gained insights into the pros and cons of different polynomial bases, but there is still a lack of principles for choosing appropriate ones. Hence, existing methods are instantiated with a specific polynomial basis, and practitioners usually have to seek a suitable choice from pre-defined candidates in a trial-and-error manner.

Meanwhile, no matter which polynomial basis is adopted, recently proposed spectral GNNs are designed to decouple the transformation of node features and the filtering of graph signals~\cite{nips22evennet}, where the coefficients of the polynomial basis and the weights responsible for feature transformation are trainable parameters. In this paper, we notice that, due to the analytical form that coefficients participate in the output of such a spectral GNN, it is likely to increase the magnitude of these coefficients to reduce the training loss trivially. We call this phenomenon ``over-passing'' and provide both empirical and theoretical evidence (see Section~\ref{sec:motivations}). Existing works regularize these coefficients by penalizing their $\ell_2$-norm, with unified strength for coefficients corresponding to different terms. However, the ultimate output is a weighted sum of those terms. Their contributions in terms of the scale of magnitude differ since existing spectral GNNs adopt polynomial bases that are not orthonormal (e.g., GPRGNN~\cite{iclr21gprgnn}) or even not orthogonal (e.g., BernNet~\cite{nips21bernnet}). Thus, penalizing all the coefficients with unified strength might be problematic.

Based on the above discussions, it is natural to raise two questions: (1) Is it possible to not only learn the coefficients but also, in the meantime, learn the suitable polynomial basis? (2) How can we properly regularize a spectral GNN by simply penalizing the $\ell_2$-norm of all the coefficients?

In this paper, we attempt to answer these questions by exploiting some properties of the Jacobi polynomials $P_{n}^{a,b}(\cdot)$, a class of orthogonal polynomials. Specifically, we propose a simple yet effective spectral GNN named \model, which is learned by jointly optimizing the trainable parameters of vanilla spectral GNNs as usual, as well as the parameters of Jacobi polynomials (i.e., $a$ and $b$) so that the choice of polynomial basis is also tuned during the training course. Besides, we normalize all the terms of the applied polynomial basis by the norm of each term. Then we theoretically show that regularizing all the coefficients with a unified strength would be equivalent to regularizing the norm of filter function, as the applied polynomial basis has become orthonormal. Noticeably, the norms used for normalization can be calculated analytically and is differentiable regarding the parameters of Jacobi polynomials. We conduct extensive comparisons, including fitting ground-truth filters and node classification tasks, to evaluate \model, where experimental results confirm the effectiveness of \model. Meanwhile, more in-depth empirical studies confirm the advantages of learnable and orthonormal polynomial basis.