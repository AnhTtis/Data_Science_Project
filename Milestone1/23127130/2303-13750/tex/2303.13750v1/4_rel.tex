\section{Related Work}
\label{sec:rel}
In recent years, graph neural networks (GNN)~\cite{tnnls20survey-gnn,aiopen2020gnn-survey} have achieved remarkable progress in graph representation learning~\cite{hamilton2020grl}. Its successful applications cover various levels of tasks~\cite{iclr17gcn,arxiv17gat,nips18seal,aaai18graph-class}. In literature, the motivations behind GNN are often summarized from three different perspectives: graph signal processing~\cite{pieee18gsp}, message passing-based inference in probabilistic graphical models~\cite{icml16discrim}, and Weisfeiler-Lehman (WL) graph isomorphism test~\cite{iclr19powerful,aaai19wlgo}.

On the one hand, the first perspective leads to spectral GNNs, which can be roughly categorized into GNNs with fixed filters and GNNs with adaptive filters. Some pioneering studies such as ChebNet~\cite{nips16chebyshev}, GCN~\cite{iclr17gcn}, and SGC~\cite{icml19sgc} adopt a fixed filter. To be capable of fitting arbitrary graph filters, recently proposed spectral GNNs utilize polynomial approximation~\cite{nips21bernnet,nips22evennet,icml22jacobi}. Our proposed LON-GNN is in this line, which leverages Jacobi polynomials.

On the other hand, GNNs motivated from the third perspective are instead regarded as spatial GNNs. The expressiveness of these GNNs is often analyzed by comparing them to the WL-test~\cite{iclr19powerful}, and recent works explore higher-order WL-tests or even more complicated graph algorithms~\cite{pami23subgraph,nips21local}.

There are also other perspectives for interpreting GNNs, such as random walks~\cite{iclr19appnp,iclr18fastgcn}, optimizing an energy function~\cite{icml21iterative}, and geometric deep learning~\cite{icml21breaking,icml19universial}.

The generalization of GNNs can now be analyzed with neural tangent kernel~\cite{nips19gntk}, and applicability to large-scale graphs is empowered with various graph sampling algorithms~\cite{nips17graphsage,iclr20graphsaint}.

Besides, there are some works that introduce the attention mechanisms to GNN to weight neighbors of nodes~\cite{arxiv17gat,corr18attention}, meta-paths~\cite{nips19gtn}, \etc

Compared with these works, we propose {\model} from the perspective of designing a learnable polynomial basis and regularization on the output values of filters.

