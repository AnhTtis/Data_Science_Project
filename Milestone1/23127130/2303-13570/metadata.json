{
    "arxiv_id": "2303.13570",
    "paper_title": "Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings",
    "authors": [
        "Jeremy Wilkerson"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.LG"
    ],
    "abstract": "This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a \"match drop\" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13570v1"
    ],
    "publication_venue": null
}