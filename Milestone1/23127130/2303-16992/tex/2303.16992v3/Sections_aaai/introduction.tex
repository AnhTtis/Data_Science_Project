\section{Introduction}
% Deep Neural Networks (DNNs) have brought a breakthrough in various tasks. There is, however, a tendency for this progression to be driven by a rise in model complexity, which often reduces the interpretability of models, i.e., after their training process, models are often used blindly without understanding their representations. 
% However, understanding model representation is a crucial step in the development of models with a high-reliability level. 

% Representation learning is a key property in deep neural networks. But how can we assess the similarity of representations learned by two models?
%But how can we assess how similar are representations learned by two models?
% whether two models learn similar or different representations? 
Representation learning has been key in the success of deep neural networks (NNs) on many tasks.
However, the resulting representations are opaque and not easily understood. 
A recent line of work analyzes internal representations by comparing two sets of representations, for instance from two different models. 
The choice of similarity measure is crucial and much work has been devoted to developing various such measures  \cite{raghu2017svcca,morcos2018insights,kornblith2019similarity,wu2020similarity}. Similarity-based analyses may shed light on how different datasets, architectures, etc., change the model's learned representations, and improve their interpretability. For example, a similarity analysis showed that lower layers in different language models are more similar to each other, while fine-tuning affects mostly the top layers \citep{wu2020similarity}. 

Various similarity measures have been proposed for comparing representations, among them the most popular ones are based on centered kernel alignment (CKA) \citep{kornblith2019similarity} and canonical correlation analysis (CCA)  \citep{Hotelling1936RelationsBT, morcos2018insights}. They all share a similar methodology: given a pair of feature representations \emph{of the same input}, they estimate the similarity between them, without considering other examples. However, they all perform mediocrely on standard benchmarks. Thus, we might question the reliability of their similarity scores, as well as the validity of interpretability insights derived from them. 
% Motivated by that, we propose a new learnable similarity measure for the interpretability of neural networks.

Motivated by this, we introduce \method{}, a new similarity measure for interpreting NNs, based on contrastive learning (CL) \citep{chen2020simple, he2020momentum}. Contrary to prior work \cite[e.g.,][]{raghu2017svcca,kornblith2019similarity}, which defines closed-form general-purpose similarity measures, \method{} is a learnable similarity measure that uses examples with a high similarity (the \textit{positive} set) and examples that have a low similarity (the \textit{negative} set), to train an encoder that maps representations to the space where similarity is measured. In the projected space, representation similarity is maximized with positive examples and minimized with negative examples. Our approach allows specializing the similarity measure to a particular domain, to obtain a more reliable and specific analysis. The similarity between projected representations is determined using a simpler closed-form measure.

We experimentally evaluate \method{} on standard benchmark for similarity measures -- the layer prediction benchmark \cite{kornblith2019similarity}, and two new benchmarks we introduce in this paper: the multilingual benchmark and the image--caption benchmark. In experiments with both language and vision models and multiple datasets, \method{} outperforms common similarity measures. In addition, we investigate a more challenging scenario, where during evaluation instead of choosing a random sentence, we retrieve a highly similar sentences as confusing examples, using the Facebook AI Similarity Search (FAISS) library \cite{johnson2019billion}. While other similarity measures are highly affected by this change, our method maintains a high accuracy with a very small degradation. We attribute this to the highly separable representations that our method learns. Even when \method{} is trained on data from one domain/task and evaluated on data from another domain/task, it achieves superior performance.

Through ablations, we demonstrate that the CL procedure is crucial to the success of the method and only maximizing the similarity of positive examples is not sufficient. Furthermore, we demonstrate that \method{} reveals new insights not captured by previous similarity measures.
For instance,  CKA suggests that the BERT  language model \cite{devlin2018bert} is more similar to vision models than  GPT-2 \cite{radford2019language}. Our analysis indicates that both BERT and GPT-2 create representations that are equally similar to the vision ones.  

% revealing that our method is more suitable for the interpretability of neural networks.

% In summary, this work makes the following contributions:
% \begin{itemize}
%     \item We introduce a new similarity measure,  \method{}, which uses positive and negative sets to train an encoder for similarity measure.
%     \item We propose two new benchmarks for the evaluation of
%     similarity measures: the multilingual benchmark and the
%     image--caption benchmark.
%     \item We show that \method{} outperforms existing similarity measures in all benchmarks, and maintains a high accuracy even when faced with more challenging examples.
%     \item We show that \method{} is preferable for interpreting neural networks, revealing insights  not captured before. 
%     % by previous methods.
% \end{itemize}


