\section{Experiments}
\label{sec:expreriments}
\paragraph{Baselines.}

We compare \method{} with the following standard baselines.
\begin{itemize}
    \item \textbf{Centered Kernel Alignment (CKA)}: Proposed by \citet{kornblith2019similarity}, CKA computes a kernel matrix for each matrix representation input, and defines the scalar similarity index as the two kernel matrices' alignment. %The authors proposed linear and RBF kernels. 
    We use a linear kernel for CKA evaluation, as the original paper reveals similar results for both linear and RBF kernels. CKA is our main point of comparison due to its success in prior work and wide applicability. 
    
    \item \textbf{PWCCA}: Proposed by \citet{morcos2018insights}, PWCCA is an extension of Canonical Correlation Analysis (CCA). Given two matrices, CCA finds bases for those matrices, such that after projecting them to those bases the correlation between the projected matrices  is maximized. While in CCA the scalar similarity index is computed as the mean correlation coefficient, in PWCCA that mean is weighted by the importance each canonical correlation has on the representation.\footnote{PWCCA and SVCCA require the number of examples to be larger than the feature vector dimension, which is not possible to achieve in all benchmarks. Therefore, we compare with them in a subset of our experiments.} 
\end{itemize}



% The code to reproduce the experimental results is available at \url{COMPLETE URL}.

See Appendix \ref{appendix:sim_measures} for more details on the current methods. In the main body we report results of the more successful methods: CKA and PWCCA. Additional baseline are reported in Appendix \ref{sec:additional_methods}.% \ref{sec:additional_methods}. %We report results of PWCCA, CKA and our method -- \method{}. 

\paragraph{Ablations.}
In addition, we report the results of  two new similarity measures, which use an encoder to map representations to the space where similarity is measured. However, %the key difference from \method{} is in the encoder $f_{\theta}$ training procedure and the similarity measure $s$ we use. 
in both methods we train $e_
{\theta}$ to only maximize the similarity between positive pairs:
\begin{equation}
\mathcal{L}_{max} =  -s(\bm{z}_1, \bm{z}_2)
\label{cl_max}
\end{equation}
where $\bm{z}_1$ and $\bm{z}_2$ are representations whose similarity we wish to maximize.  %where for a batch of size $n$ similarity is measured as: $\frac{1}{n} \sum_{i=1}^n s\left(\bm{z}_1^{i}, \bm{z}_2^{i} \right)$. 
We experiment with two functions for  $s$---dot-product and CKA---and accordingly name these similarity measures DeepDot  and  DeepCKA.  These methods provide a point of comparison where the similarity measure is trained, but \emph{without negative examples}, in order to assess the importance of contrastive learning. 
\paragraph{Encoders details.} In all experiments, the encoder $e_{\theta}$ is a two-layer multi-layered perceptron with hidden layer dimensions of $512$ and $256$, and output dimension of $128$.  We trained the encoder for 50 epochs for the layer prediction and 30 epochs for the multilingual and image--caption benchmarks. We used the Adam optimizer \citep{kingma2014adam} with a learning rate of $0.001$ and a batch size of $1024$ representations. We used $\tau = 0.07$ for \method{} training.

\subsection{Layer prediction benchmark}

\begin{table*}[t]
\centering
\begin{tabular}{l rr | l rr}
\toprule
 \multicolumn{3}{c}{Language} & \multicolumn{3}{c}{Vision} \\ 
\cmidrule(lr){1-3} \cmidrule(lr){4-6}
& Penn TreeBank & WikiText &  & CIFAR-10 & CIFAR-100 \\ \midrule
PWCCA                                                      & 38.33       & 55.00 & PWCCA  & 47.27 & 45.45 \\ 
CKA                                                        & 71.66       & 76.66 & CKA  & 78.18  & 74.54    \\
DeepDot                                                    & 15.55 \small $\pm$ 1.69       & 14.00 \small $\pm$ 2.26 & DeepDot  & 14.90 \small $\pm$ 1.78 & 14.18 \small $\pm$ 2.67  \\
DeepCKA                                                    & 16.66 \small $\pm$ 3.16       & 19.66 \small $\pm$ 1.63 & DeepCKA  & 17.09 \small $\pm$ 2.95 & 13.09 \small $\pm$ 4.20  \\
\midrule 
\method{} & & & \method{} & & \\ 
% trained on: & & & trained on: & & \\ 
\hspace{.5em} Penn  & 100 \small $\pm$ 0         & 85.45 \small $\pm$ 1.62 & \hspace{.5em} CIFAR-10  & 100 \small $\pm$ 0   & 90.54 \small $\pm$ 2.90 \\ 
\hspace{.5em} Wiki  & 94.00 \small $\pm$ 4.66       & 100 \small $\pm$ 0 & \hspace{.5em} CIFAR-100  & 85.81 \small $\pm$ 5.68   & 100 \small $\pm$ 0  \\  
\bottomrule
\end{tabular}
\caption{Layer prediction benchmark accuracy results for language and vision cases. For encoder-based methods we report mean and std over 5 %different encoders trained with 
 random initializations. For \method{}, we experiment with training with different datasets (rows) and evaluating on same or different datasets (columns).}
 \label{tab:layer_prediction}
\end{table*}

\subsubsection{Setup} 
Recall that this benchmark evaluates whether a certain layer from one model is deemed most similar to its architecturally-corresponding layer from another model, where the two models differ only in their weight initialization. We repeat this process for all layers and 5 different model pairs, and report average accuracy. 
We experiment with both language and vision setups.

\paragraph{Models.} For language experiments, we use the MultiBERTs \citep{sellam2021multiberts}, a set of 25 BERT models, differing only in their weights initialization. For vision experiments, we pre-train 10 visual transformer (ViT) models \citep{dosovitskiy2020image} on the ImageNet-1k dataset \citep{imagenet15russakovsky}. Then we fine-tune them on CIFAR-10 and CIFAR-100 datasets \citep{krizhevsky2009learning}.  Further details are available in Appendix \ref{appendix:vit_parameters}.

\paragraph{Datasets.} In language experiments, we use word-level contextual representations generated on two English text datasets: the Penn Treebank \citep{marcus-etal-1993-building} and WikiText \citep{merity2016pointer}. 
For Penn TreeBank we generate 5005/10019 test/training representations, respectively; for WikiText we generate  5024/10023 test/training representations. Vision experiments are conducted using representations generated on CIFAR-10 and CIFAR-100. For both we generate 5000 and 10000 test and training representations, respectively. 


\paragraph{Positive and Negative sets.} Given a batch of representations of some model $i$ at layer $j$, we define its positive set as the representations at the same layer $j$ of all models that differ from $i$. The negative set is all representations from layers that differ from $j$ (including from model $i$).


\subsubsection{Results}
The results are shown in Table \ref{tab:layer_prediction}. In both language and vision evaluations, CKA achieves better results than PWCCA, consistent with the findings by  \citet{ding2021grounding}. 
 DeepDot and DeepCKA  perform poorly, with much lower results than PWCCA and CKA, revealing that maximizing the similarity is not satisfactory for similarity measure purposes. 
Our method, \method{}, achieves excellent results. When trained on one dataset's training set and evaluated on the same dataset's test set, \method{} achieves perfect accuracy under this benchmark, with a large margin over CKA results. This holds for both language and vision cases. Even when trained on one dataset and evaluated over another dataset, \method{}  surpasses other similarity measures, showing the transferability of the learned encoder projection between datasets. This is true both when transferring across domains (in text, between news texts from the Penn Treebank and Wikipedia texts), and when transferring across classification tasks (in images, between the 10-label CIFAR-10 and the 100-label CIFAR-100). 

\subsection{Multilingual benchmark}
\label{sec:multilingual}
\subsubsection{Setup}
This benchmark assesses whether a similarity measure assigns a high similarity to multilingual representations of the same sentence in different languages. Given a batch of (representations of) sentences $b^{(i)}$ in language $L_i$ and their translations $b^{(j)}$ in language $L_j$, we compute the similarity between $b^{(i)}$ and $b^{(j)}$, and the similarities between $b^{(i)}$ and 10 randomly chosen batches of representations in language $L_j$. If  $b^{(i)}$ is more similar to $b^{(j)}$ than  to all other batches, we mark success. (Alternatively, in a more challenging scenario, we use FAISS to find for each representation in each layer the 10 most similar representations in that layer.) We repeat this process separately for representations from different layers of a multilingual model, over many sentences and multiple language pairs, and report average accuracy per layer.\footnote{For deep similarity measures (DeepCKA, DeepDot, and \method{}), upon training the encoder on examples from a pair of languages, $(L_r, L_q), r \neq q$, we evaluate it over all other distinct pairs of languages.} Appendix \ref{appendix:multilingual} gives more details.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{@{}ccccc|cccc@{}}
\toprule 
& \multicolumn{4}{c}{Random}                                                        & \multicolumn{4}{c}{FAISS}                                                   \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9}
Layer & CKA              & DeepCKA          & DeepDot           & ContraSim        & CKA              & DeepCKA          & DeepDot           & ContraSim         \\ \midrule
1     & 71.7 \small $\pm$ 5.3 & 82.0 \small $\pm$ 6.4 & 63.3 \small$\pm$ 10.4 & 95.5 \small $\pm$ 5.4 & 20.1 \small $\pm$ 4.0 & 10.7 \small $\pm$ 2.6 & 29.9 \small  $\pm$ 8.7  & 36.0 \small $\pm$ 10.7 \\
2     & 78.7 \small $\pm$ 4.4 & 86.4 \small $\pm$ 4.1 & 68.5 \small $\pm$ 9.9 & 95.0 \small $\pm$ 7.2 & 27.2 \small $\pm$ 5.5 & 12.3 \small $\pm$ 2.9 & 46.9 \small  $\pm$ 9.8  & 33.0 \small $\pm$ 14.8 \\
3     & 86.8 \small $\pm$ 3.0 & 87.1 \small $\pm$ 3.2 & 70.4 \small $\pm$ 9.7 & 96.4 \small $\pm$ 6.7 & 41.9 \small $\pm$ 8.7 & 17.6 \small $\pm$ 4.2 & 51.5 \small $\pm$ 10.3  & 45.4 \small $\pm$ 20.5 \\
4     & 92.6 \small $\pm$ 1.4 & 91.5 \small $\pm$ 2.4 & 95.4 \small $\pm$ 3.4 & 99.9 \small $\pm$ 0.2 & 33.4 \small $\pm$ 7.0 & 15.2 \small $\pm$ 3.7 & 52.2 \small  $\pm$ 8.6  & 72.4  \small $\pm$ 9.8  \\
5     & 88.3 \small $\pm$ 3.2 & 83.5 \small $\pm$ 5.2 & 94.7 \small $\pm$ 4.8 & 99.9 \small $\pm$ 0   & 49.3 \small $\pm$ 4.3 & 36.9 \small $\pm$ 6.3 & 42.4 \small $\pm$ 12.9  & 99.1. \small $\pm$ 0.8  \\
6     & 88.6 \small $\pm$ 3.4 & 86.4 \small $\pm$ 5.2 & 92.5 \small $\pm$ 5.4 & 100  \small $\pm$ 0   & 51.4 \small $\pm$ 5.5 & 39.9 \small $\pm$ 7.2 & 42.1 \small $\pm$ 12.3  & 99.5. \small $\pm$ 0.4  \\
7     & 88.8 \small $\pm$ 3.7 & 86.9 \small $\pm$ 5.0 & 92.6 \small $\pm$ 5.0 & 100  \small $\pm$ 0   & 53.0 \small $\pm$ 5.8 & 41.1 \small $\pm$ 7.7 & 45.7 \small $\pm$ 11.7  & 99.6. \small $\pm$ 0.3  \\
8     & 89.3 \small $\pm$ 3.6 & 85.2 \small $\pm$ 5.7 & 91.4 \small $\pm$ 7.0 & 100  \small $\pm$ 0   & 56.1 \small $\pm$ 5.8 & 45.0 \small $\pm$ 8.7 & 43.8 \small $\pm$ 13.4  & 99.7. \small $\pm$ 0.3  \\
9     & 88.1 \small $\pm$ 3.8 & 82.4 \small $\pm$ 5.6 & 89.1 \small $\pm$ 9.5 & 100  \small $\pm$ 0   & 53.3 \small $\pm$ 4.9 & 42.7 \small $\pm$ 8.5 & 39.2 \small $\pm$ 12.9  & 99.6. \small $\pm$ 0.3  \\
10    & 87.0 \small $\pm$ 3.5 & 80.3 \small $\pm$ 5.9 & 85.3\small $\pm$ 10.3 & 100  \small $\pm$ 0   & 51.5 \small $\pm$ 5.3 & 42.4 \small $\pm$ 7.8 & 34.3 \small $\pm$ 12.2  & 99.5. \small $\pm$ 0.4  \\
11    & 86.7 \small $\pm$ 4.2 & 76.6 \small $\pm$ 6.4 & 79.7\small $\pm$ 13.9 & 99.9 \small $\pm$ 0 & 52.4 \small $\pm$ 5.3 & 43.3 \small $\pm$ 8.5 & 31.4 \small $\pm$ 12.8  & 99.3. \small $\pm$ 0.5  \\
12    & 86.4 \small $\pm$ 3.4 & 63.8 \small $\pm$ 7.9 & 64.3\small $\pm$ 19.7 & 99.9 \small $\pm$ 0 & 52.8 \small $\pm$ 4.5 & 32.3 \small $\pm$ 8.7 & 26.1 \small $\pm$ 21.9  & 98.9. \small $\pm$ 0.8  \\ 
\bottomrule
\end{tabular}}
\caption{Multilingual benchmark accuracy results. With random sampling (left block), \method{} outperforms other similarity measures. Using FAISS (right block) further extends the gaps.}
\label{tab:multilingual_results}
\end{table*}

\paragraph{Model and Data.} We use two multilingual models: multilingual BERT \citep{devlin2018bert}\footnote{https://huggingface.co/bert-base-multilingual-cased} and XLM-R \citep{conneau2019unsupervised}. %, pretrained on 104 different languages.
We use the XNLI dataset \citep{conneau2018xnli}, which has natural language inference examples, parallel in multiple languages. Each example in our dataset is a  sentence taken from either the premise or hypothesis sets. We experiment with 5 typologically-different languages: English, Arabic, Chinese, Russian, and Turkish. We created sentence-level representations, with 5000 test 10000 training  representations.
As a sentence representation, we experiment with [CLS] token representations and with mean pooling of token representations, since \citet{del2021establishing} noted a difference in similarity in these two cases. We report results with [CLS] representations in the main body and with mean pooling in Appendix \ref{appendix:multilingual}; the trends are similar. 

\paragraph{Positive and Negative sets.} 
Given a pair of languages and a batch of representations at some layer, for each representation we define its positive pair as the representation of the sentence in the different language, and its negative set as all other representations in the batch.

\subsubsection{Results}
Results with multilingual BERT representations in Table \ref{tab:multilingual_results} show our method's effectiveness. (Trends with XLM-R are consistent; Appendix~\ref{app:xlmr}). Under random sampling evaluation (left block), \method{} shows superior results over other similarity measures, despite being evaluated on language pairs it hasn't seen at training. Using FAISS sampling (right block) further extends the gaps. While CKA results dropped by $\approx 45\%$, DeepCKA dropped by $\approx51\%$, and DeepDot dropped by $\approx 40\%$, \method{} was much less affected by FAISS sampling ($\approx 17\%$ drop on average and practically no drop in most layers). This demonstrates the high separability between examples of \method{}, enabling it to distinguish even very similar examples.
% and assign a higher similarity to the correct pair. 
For all other methods, mid-layers have the highest accuracy, whereas for our method almost all layers are near $100\%$ accuracy, except for the first 3 or 4 layers. 

To further analyze this, we compare the original multilingual representations from the last layer with their projections by \method{}'s trained encoder. 
Figure \ref{fig:multilingual_embeddings} shows UMAP \citep{mcinnes2018umap} projections for 10 English sentences and 10 Arabic sentences, before and after \method{} encoding.  The \method{} encoder was trained on Arabic and English languages. The original representations are organized according to the source language (by shape), whereas \method{} projects translations of the same sentence close to each other (clustered by color).
\begin{figure}[h]
\centering
\includegraphics[width=0.46\textwidth]{figures/multilingual_representations.png}
\caption{ 
Original representations (left) are clustered by the source language (by shape). 
\method{}  (right) projects representations of the same sentence in different languages close by  (by color).}
\label{fig:multilingual_embeddings}
\end{figure}
% ===================================

\subsection{Image--caption benchmark}

\begin{figure*}[t]
\centering
\includegraphics[width=0.64\linewidth]{figures/image_caption_graphs.png}
\caption{Image--caption benchmark results for 4 different model pairs. \method{} works best, and is the only measure robust to FAISS sampling.}
\label{fig:image_caption}
\end{figure*}

\subsubsection{Setup}

Given a test set $\mathbb{X}$, consisting of pairs of an image representation generated by a CV model and its caption representation from a LM, we split  $\mathbb{X}$ to batches of size 64. For each batch, we compute the similarity between the image representations and their corresponding caption representations. We then sample 10 different caption batches, either randomly or using FAISS (as before), and compute the similarity between the image representation and each random/FAISS-retrieved caption representation. If the highest similarity is between the image representation and the original caption representation, we mark a success. %We average results over all batches.
For trainable similarity measures, we train with 5 different random seeds and average the results.

\paragraph{Models and Data.} 
We use two vision models for image representations: ViT  and ConvNext \citep{liu2022convnet}; and two  language models for text representations: BERT  and GPT2 \citep{radford2019language}.
%
%\paragraph{Datasets.}
We use the Conceptual Captions dataset \citep{sharma2018conceptual}.
% , which has $\approx$3.3M pairs of images and English captions. 
We use 5000 and 10000 pairs as test and training sets, respectively.

\paragraph{Positive and Negative sets.}
Given a batch of image representations with their corresponding caption representations, for each image representation we define as a positive set its corresponding caption representation, and as a negative set all other representations in the batch.

\subsubsection{Results}
Figure \ref{fig:image_caption} demonstrates the strength of \method{}. Under random sampling (green boxes),  DeepCKA achieves comparable results to \method{}, while DeepDot and CKA achieve lower results. 
However, using FAISS (red boxes) causes a big decrease in DeepCKA accuracy, while \method{} maintains high accuracy. 
Furthermore, in 3 of 4 pairs we tested, FAISS sampling yielded better CKA accuracy than random sampling. This contradicts the intuition that similar examples at the sampling stage should make it harder for similarity measures to distinguish between examples. This might indicate that CKA suffers from stability issues.
Finally, we report results with the multi-modal CLIP model \citep{radford2021learning} in Table \ref{tab:image_caption_clip} (Appendix \ref{app:image_caption}). Because the model was pre-trained with contrastive learning, simple dot-product similarity works very well, so there is no need to learn a similarity measure in this case.  

\subsubsection{\method{} on different dimensions}
\label{different_dim}
We further evaluated \method{} with different representation dimensions. We performed the same image--caption benchmark with the exception that we used different vision model sizes: ViT-large and ConvNext-base, both with 1024-dimensional representation vectors. As language models we used the same GPT2 and BERT models, with a 768-dimensional representation vector. 

\begin{table}[]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
                   & CKA   & ContraSim \\ \midrule
ViT-large/GPT2     & 45.57 & 98.73     \\
ViT-large/BERT     & 83.54 & 98.73     \\
ConvNext-base/GPT2 & 39.24 & 100       \\
ConvNext-base/BERT & 74.68 & 98.73     \\ \bottomrule
\end{tabular}
\caption{Image--caption benchmark accuracy results for model pairs with different dimensions. We report results using FAISS sampling. Despite different model dimensions, \method{} consistently works best.}
\label{tab:different_dim}
\end{table}

We trained a different encoder for each model, as opposed to the single encoder we trained in all other experiments. This enables \method{} to be used with representations with different dimensions. Results are summarized in Table \ref{tab:different_dim}. We report results with FAISS sampling. Across all pairs, \method{} achieves superior results.


% ===================================
\section{Interpretability insights}

Having shown the superiority of our method, we now discuss a few interpretability insights that arise from our evaluations, and are not revealed by previous similarity measures. 

In the multilingual benchmark (Table~\ref{tab:multilingual_results},  FAISS results),  we found a much greater difference in accuracy between shallow and deep layers in \method{} compared to previous similarity measures. Using previous similarity measures we might infer that there is no difference in the ability to detect the correct pairs across different layers. However, \method{} shows that the difference in the ability to detect the correct pair dramatically changes from shallow to deep layers. This raises an interesting insight regarding the evolution of representations across layers. For instance, \citet{conneau2020emerging} used CKA to measure the similarity of representations of bilingual models of different languages and the same language (using back-translation). From their results it can be observed that there is no much difference in similarity of sentences from different languages and from the same language at the shallow and deep layers. Our results show that this difference is higher than found before.

In the image--caption benchmark (Figure~\ref{fig:image_caption}), from the CKA results we might infer that
% judging from the CKA results we might infer that 
BERT representations are more similar to computer vision representations than GPT2 representations. That is because with CKA, it is easier to detect the matching image--caption pair with BERT than it is with GPT2.  However, \method{} achieves a high accuracy in both BERT pairs and GPT2 pairs, which means that both language models about as similar to  vision models, in contrast to what we may infer from previous similarity measures. This reveals a new understanding regarding the relationship between language  and vision models. To the best of our knowledge, no prior work has done such a similarity analysis.

