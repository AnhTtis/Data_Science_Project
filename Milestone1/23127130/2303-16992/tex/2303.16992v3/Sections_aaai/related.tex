\section{Related Work} \label{sec:related-work}
% \subsection{Self-Supervised Learning (SSL)} SSL is a popular method for pretraining models. Training a model using SSL is generally divided into two parts: feature representation learning through pretraining on unlabeled data, followed by fine-tuning with labeled data for specific downstream tasks. Many studies have been done on SSL \citep{gidaris2020learning, caron2018deep}, and the most successful among them are the Contrastive Learning \citep{hadsell2006dimensionality, van2018representation} based methods, achieving state-of-the-art results with learned features that surpass the feature learned in a supervised manner on many downstream tasks \citep{he2020momentum, chen2020simple}. CL pretraining is done by maximizing the agreement between positive pairs  relative to many negative pairs. Upon the pretraining stage, the model is fine-tuned to a specific task using the observed feature vectors.

% \subsection{Similarity Measures} 
 % Comparing different models allows one to analyze how different aspects like network architecture, training set, and model size affect the model's learned representations. 
 Many studies have used similarity measures for the interpretability of NNs.
 For instance, \citet{kornblith2019similarity} showed that adding too many layers to a convolutional neural network, trained for image classification, hurts its performance. Using CKA, they found that more than half of the network's layers are very similar to the last. 
%  They computed similarity between different layers using CKA and found that more than half of the network's layers are very similar to the last layer. 
%  They further showed that using different training datasets may lead to similar representations.
They further found that two models trained on different image datasets (CIFAR-10 and CIFAR-100, \citealt{krizhevsky2009learning})  learn representations that are similar in the shallow layers. Similar findings were noted for language models by \citet{wu2020similarity}. 
The latter also evaluated the effect of fine-tuning on language models and found that the top layers  are most affected by fine-tuning.
\citet{kornblith2019similarity} and \citet{morcos2018insights}  found that increasing the model's layer width results in more similar representations between models.
% and that networks are generally more similar to networks with the same layer width than to networks with a relatively larger width.
\citet{raghu2017svcca} provided an interpretation of the learning process by examining how similar representations were during the training process compared to final representations. They found that networks converge from bottom to top.
% i.e., layers closer to the input converge to their final representation faster than deeper layers.
% Based on that insight, they proposed frozen training, where they successively freeze lower layers during training, updating only the deeper layers. They found that frozen training leads to classifiers with a higher generalization. 
% \citet{cianfarani2022understanding} used similarity measures to analyze the effect of adversarial training on deep neural networks trained for image classification. Using CKA, they compared representations of adversarially trained networks with representations of regularly trained networks and discovered that adversarial examples have little effect on early layers. They further found that deeper layers overfit during adversarial training.
%Moreover, they found a high similarity between representations of adversarial images generated with a different threat model. 
In all this line of work, the similarity is computed using only similar examples, using functional closed-form measures. In contrast, we use both positive and negative samples in a learnable similarity measure, which allows adaptation to specific tasks.

Separate work employs contrastive learning for representation learning \citep{he2020momentum, chen2020simple} and data retrieval \cite{karpathy2014deep, huang2008active}. In contrast to that line of work, we borrow the contrastive learning formulation for similarity-based interpretations of deep networks.

% which used contrastive learning alike paradigm for similarity estimation. However, it was used as a limited part of the method and not as a stand-alone similarity measure that can be used for other purposes, such as interpretability.

