\section{Conclusion}
We proposed a new similarity measure for interpreting neural networks, \method{}. By defining positive and negative sets we learn an encoder that maps representation to a space where similarity is measured. 
%  We evaluated our method against known similarity measures: CKA and PWCCA. We also considered another two similarity measures we proposed: DeepDot and DeepCKA, in order to show that negative examples are a critical component of our method's success. 
Our method outperformed other similarity measures under the common layer prediction benchmark and two new benchmarks we proposed: the multilingual benchmark and the image--caption benchmark. It particularly shines in strengthened versions of said benchmarks, where random sampling is replaced with finding the most similar examples using FAISS. Moreover, we show that even when \method{} is trained on data from one domain/task and evaluated on data from another domain/task, it achieves superior performance. 
 Considering \method{}'s superiority in all evaluations, we believe it is a better tool for the interpretability of neural networks, and have discussed a few insights revealed by \method{} and not captured by previous methods.

Our new similarity measure benchmarks can facilitate work on similarity-based analyses of deep networks. The multilingual benchmark is useful for work on multilingual language models, while the image--caption benchmark may help in multi-modal settings. 
In addition, since our method learns a parameterized measure, it may help train models with similarity objectives.



\section{Limitations}
Compared to existing methods, \method{} needs access to a training set for the encoder training procedure. The training procedure itself is efficient, typically a matter of minutes.

\section{Ethics Statement} 
Our work adds to the body of literature on the interpretability of neural networks and may mitigate their opacity. We do not foresee major risks associated with this work. However, a malicious actor could train \method{} adversarially, assign poor similarity estimates, and lead to false analyses.

\section*{Acknowledgements}
This work was supported by an
AI alignment grant from Open Philanthropy, the Israel Science Foundation (grant No. 448/20), and an Azrieli Foundation Early Career Faculty Fellowship.