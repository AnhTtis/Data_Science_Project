
\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{bm}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xcolor}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnotetext{#1}%
  \endgroup
}

\newcommand{\method}{ContraSim}

\title{\method{} -- Analyzing Neural Representations Based on Contrastive Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Adir Rahamim \email adir.rahamim@campus.technion.ac.il \\
      \addr Technion – Israel Institute of Technology
      \AND
      \name Yonatan Belinkov$^\dagger$ \email belinkov@technion.ac.il \\
      \addr Technion – Israel Institute of Technology
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}

\maketitle

\blfootnote{$^\dagger$ Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion.}

\begin{abstract}
Recent work has compared neural network representations via 
  similarity-based analyses to improve model interpretation.
  The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that are expected to be matched. 
  However, existing similarity measures  perform mediocrely on standard benchmarks. 
  In this work, we develop a new similarity measure, dubbed \method{}, based on contrastive learning. In contrast to common closed-form similarity measures, \method{} learns a parameterized measure by using both similar and dissimilar examples. 
 We perform an extensive experimental evaluation of our method, with both language and vision models, on the standard layer prediction benchmark and two new benchmarks that we introduce: the multilingual benchmark and the image--caption benchmark. In all cases, \method{} achieves much higher accuracy than previous similarity measures, even when presented with challenging examples. Finally, \method{} is more suitable for the analysis of neural networks, revealing new insights not captured by previous measures.\footnote{Code is available at: \url{https://github.com/technion-cs-nlp/ContraSim}.}
\end{abstract}

\input{Sections/introduction.tex}
\input{Sections/related.tex}
\input{Sections/problem.tex}
\input{Sections/method.tex}
\input{Sections/evaluation_methods.tex}
\input{Sections/experiments.tex}
\input{Sections/conclusion.tex}
\section*{Acknowledgements}
This project was supported by an AI Alignment grant from Open Philanthropy, the Israel Science Foundation (grant No.\ 448/20), and an Azrieli Foundation Early Career Faculty Fellowship.

\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\input{Sections/appendix.tex}
\end{document}
