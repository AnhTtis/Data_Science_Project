\section{Introduction}
\vspace{-5pt}
% Deep Neural Networks (DNNs) have brought a breakthrough in various tasks. There is, however, a tendency for this progression to be driven by a rise in model complexity, which often reduces the interpretability of models, i.e., after their training process, models are often used blindly without understanding their representations. 
% However, understanding model representation is a crucial step in the development of models with a high-reliability level. 

% Representation learning is a key property in deep neural networks. But how can we assess the similarity of representations learned by two models?
%But how can we assess how similar are representations learned by two models?
% whether two models learn similar or different representations? 
Representation learning has been key in the success of deep neural networks on many tasks.
However, the resulting representations are opaque and not easily understood. 
A recent line of work analyzes internal representations by comparing two sets of representations, for instance from two different models. 
The choice of similarity measure is crucial and much work has been devoted to developing various such measures  \cite{raghu2017svcca,morcos2018insights,kornblith2019similarity,wu2020similarity}. Similarity-based analyses may shed light on how different datasets, architectures, etc., change the model's learned representations, and improve their interpretability. For example, a similarity analysis showed that lower layers in different language models are more similar to each other, while fine-tuning affects mostly the top layers \citep{wu2020similarity}. 

Various similarity measures have been proposed for comparing representations, among them the most popular ones are based on centered kernel alignment (CKA) \citep{kornblith2019similarity} and canonical correlation analysis (CCA)  \citep{Hotelling1936RelationsBT, morcos2018insights}. They all share a similar methodology: given a pair of feature representations \emph{of the same input}, they estimate the similarity between them, without considering other examples. However, they all perform mediocrely on standard benchmarks. Thus, we might question the reliability of their similarity scores, as well as the validity of interpretability insights derived from them. 
% Motivated by that, we propose a new learnable similarity measure for the interpretability of neural networks.

Motivated by this, we introduce \method{}, a new similarity measure for interpreting neural networks, based on contrastive learning (CL) \citep{chen2020simple, he2020momentum}. Contrary to prior work \cite[e.g.,][]{raghu2017svcca,kornblith2019similarity}, which defines closed-form general-purpose similarity measures, \method{} is a learnable similarity measure that uses examples with a high similarity (the \textit{positive} set) and examples that have a low similarity (the \textit{negative} set), to train an encoder that maps representations to the space where similarity is measured. In the projected space, representation similarity is maximized with examples from the positive set, and minimized with examples from the negative set. %The projected representations similarity is calculated with a simpler closed-form similarity measure.

We experimentally evaluate \method{} on one standard similarity metrics benchmark and two new benchmarks we introduce in this paper, demonstrating its superiority over common similarity metrics.  First, we use the known layer prediction benchmark \citep{kornblith2019similarity}, which assesses whether high similarity is assigned to two architecturally-corresponding layers in two models differing only in their weight initialization. 
Second, in our proposed multilingual benchmark, we assume a multilingual model and a parallel dataset of translations in two languages.  A good similarity measure should assign a higher similarity to the (multi-lingual) representations of a sentence in language A and its translation in language B, compared to the similarity of the same sentence in language A and a random sentence in language B.  
Third, we design the image--caption benchmark,  based on a similar idea. Given an image and its text caption, and correspondingly a vision model and a language model, a good similarity measure should assign a high similarity to representations of the image and its caption, compared to the similarity of the same image and a random caption.
% Image and its caption describe the same scene in a different way, thus we expect similarity measures to assign high similarity between image representation and its caption representation, although generated by different models.



In both of our new benchmarks, we investigate a more challenging scenario, where during evaluation instead of choosing a random sentence, we retrieve a highly similar sentences as confusing examples, using the Facebook AI Similarity Search (FAISS)  library \citep{johnson2019billion}.  While other similarity measures are highly affected by this change,  our method maintains a high accuracy with a very small degradation. We attribute this to the highly separable representations that our method learns.

Through ablations, we show that if we change the training procedure of the encoder to only maximize the similarity of similar examples, the projected representations have poor separation, indicating that the CL procedure is a crucial part of the method's success. Furthermore, we demonstrate that \method{} reveals new insights not captured by previous similarity measures.
For instance,  CKA suggests that the BERT  language model \cite{devlin2018bert} is more similar to vision models than  GPT-2 \cite{radford2019language}. Our analysis indicates that both BERT and GPT-2 create representations that are equally similar to the vision ones.  

% revealing that our method is more suitable for the interpretability of neural networks.

In summary, this work makes the following contributions:
\begin{itemize}
    \item We introduce a new similarity measure,  \method{}, which  uses positive and negative sets to train an
    encoder that maps representations to the space where similarity is measured.
    \item We propose two new benchmarks for the evaluation of
    similarity measures: the multilingual benchmark and the
    image--caption benchmark.
    \item We show that \method{} outperforms existing similarity measures in all benchmarks, and maintains a high accuracy even when faced with more challenging examples.
    \item We show that \method{} is preferable for interpreting neural networks, revealing insights  not captured by previous methods. 
\end{itemize}


