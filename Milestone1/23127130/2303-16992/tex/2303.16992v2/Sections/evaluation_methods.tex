\section{Similarity Measure Evaluation}
\label{eval}
We use three benchmarks to evaluate similarity measures: the known layer prediction benchmark, and two new benchmarks we design: the multilingual benchmark and the image--caption benchmark. We further propose a strengthened version of the last two using the FAISS software.


\subsection{Layer prediction benchmark}
Proposed by \citet{kornblith2019similarity},  a basic and intuitive benchmark is to assess the invariance of a similarity measure against changes to a random seed. Given two models, with the same architecture and training configuration (i.e., same training set, same hyperparameters, etc.), differing only in their weight initializations, for each layer in the first model, among all layers of the second model, one can expect that a good similarity measure assigns the highest similarity for the architecturally-corresponding layer. Formally, let $f$ and $g$ be two models with $k$ layers, and define  $f_i$ and $g_i$ as the $i^{th}$ layer of $f$ and $g$ models, respectively. After calculating the similarity of $f_i$ to each layer of $g$ ($g_1, \ldots, g_k$), the pair with the highest similarity is expected to be $(f_i, g_i)$. This benchmark counts the number of layers for which this pair was indeed the most similar, and divides by the total number of pairs. An illustration is found in Figure \ref{fig:layer_prediction}. 


\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figures/layer_prediction.png}
\caption{Layer prediction benchmark. Given two models differing only in weight initialization, \mbox{A and B}, for each layer in the first model, among all layers of the second model, a good similarity measure is expected to assign the highest similarity for the architecturally-corresponding layer.}
\label{fig:layer_prediction}
\end{figure}

The intuition behind this benchmark is that each layer captures different information about the input data. For example, \citet{jawahar2019does} showed that different layers of the BERT model \citep{devlin2018bert} capture different semantic information. \citet{kornblith2019similarity} showed, using CKA similarity, that indeed there exists a correspondence between layers of models trained with different seeds.
Thus, although trained from different seeds, the same layers are expected to capture the same information and therefore have a high similarity.
\vspace{-5pt}
\subsection{Multilingual benchmark}
Multilingual models, such as Multilingual-BERT \citep{devlin2018bert}, learn to represent texts in different languages in the same representation space. Interestingly, these models show cross-lingual zero-shot transferability, where a model is fine-tuned in one language and evaluated in a different language \citep{pires2019multilingual}. \citet{muller2021first} analyzed this transferability and found that lower layers of the Multilingual-BERT align the representations between sentences in different languages.

Since multilingual models share similarities between representations of different languages, we expect that a good similarity measure should assign a high similarity to two representations of a sentence in two different languages. In other words, we expect similarity measures to be invariant to the sentence source language. Consider a multilingual model $f$ and dataset $\mathbb{X}$, where each entry consists of the same sentence in different languages.
Let $(x_1^{(i)}, x_2^{(i)}) \in \mathbb{X}$ be a sentence written in two languages -- language A and language B. The similarity between $f(x_1^{(i)})$ and $f(x_2^{(i)})$ should be higher than the similarity between $f(x_1^{(i)})$ and the representation of a sentence in language B randomly chosen from $\mathbb{X}$, i.e., $f(x_2^{(j)})$, where $(x_1^{(j)}, x_2^{(j)}) \in \mathbb{X}$ is a randomly chosen example from $\mathbb{X}$. 
The benchmark calculates the fraction of cases for which the correct translation was assigned the highest similarity. 
An illustration is found in Figure \ref{fig:multilingual}.
 
 Additionally, we suggest a strengthened version of the multilingual benchmark, using FAISS, a library for efficient similarity search. 
%  Given a query vector, we use FAISS to search for the most similar vectors (in terms of Euclidean distance) in a large pre-indexed set of vectors.
 Instead of sampling random sentences in language B, we use FAISS to find the pair $(x_1^{(j)}, x_2^{(j)}) \in \mathbb{X}$, where $x_2^{(j)} \neq x_2^{(i)}$, with the  representation $f(x_2^{(j)})$ that is most similar to $f(x_2^{(i)})$, out of a large set of vectors pre-indexed by FAISS.
 This leads to a more challenging scenario, as the similarity between $x_1^{(i)}$ and FAISS-sampled $x_2^{(j)}$ is expected to be higher than the similarity between $x_1^{(i)}$ and randomly chosen $x_2^{(j)}$, increasing the difficulty of identifying the pair $(x_1^{(i)}, x_2^{(i)})$ as the highest-similarity pair. Note that FAISS only affects the evaluation step, and is not used during \method{}'s training.

\begin{figure}[t]
\begin{minipage}[c]{0.455\linewidth}
\includegraphics[width=\linewidth]{figures/multilingual.png}
\caption{The multilingual benchmark. $r_1^E$ and $r_1^G$ denote the representations of the same sentence in different languages, and $S_1$ is their similarity. $r_2^E$ represents the random sentence representation, and $S_2$ is the similarity between it and $r_1^G$. We expect $S_1$ to be higher than $S_2$.}
\label{fig:multilingual}
\end{minipage}
\hfill
\begin{minipage}[c]{0.455\linewidth}
\includegraphics[width=\linewidth]{figures/image_caption.png}
\caption{The image--caption benchmark. $r_1^C$ and $r^i$ denote the representations of the caption and the image pair, respectively, and $S_1$ is their similarity. $r_2^C$ denotes the random caption representation, and $S_2$ is the similarity between it and $r^i$. We expect $S_1$ to be higher than $S_2$.}
\label{fig:image_caption_illustration}
\end{minipage}%
\end{figure}
\vspace{-10pt}
\subsection{Image-caption benchmark}
 Let $\mathbb{X}$ be a dataset of images and their textual descriptions (captions),  $f$ be a computer vision model and $g$ a language model.
Given a pair of an image and its caption, $(m^{(i)}, c^{(i)}) \in \mathbb{X}$, a good similarity measure is expected to assign a high similarity to their representations -- $f(m^{(i)}), g(c^{(i)})$. In particular, this similarity should be higher than that of the pair of the same image representation $f(m^{(i)})$ and some random caption's representation $g(c^{(j)})$, where $c^{(j)}$ is a randomly chosen caption from  dataset $\mathbb{X}$. 
% This benchmark is multimodal, as it uses two separate models to generate representations: the pretrained vision model for images and the pretrained language model for captions. 
The intuition behind this benchmark is that an image and its caption represent the same scene in a different way. Thus, their representations should have a higher similarity than that of the same image and some random caption. An illustration is found in Figure \ref{fig:image_caption_illustration}. 

As in the multilingual benchmark, we also propose a strengthened variant for the image--caption benchmark using  FAISS. Rather than sampling a random caption $c^{(j)}$, we use FAISS to find the pair $(m^{(j)}, c^{(j)}) \in \mathbb{X}$, where $c^{(j)} \neq c^{(i)}$, with the representation $g(c^{(j)})$ that is most similar to $g(c^{(i)})$. 

% \begin{figure*}
% \centering
% \begin{subfigure}{.5\textwidth}
%   \centering
%   \includegraphics[width=1.0\textwidth]{figures/multilingual.png}
%   \caption{Multilingual benchmark}
%   \label{fig:multilingual}
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}
%   \centering
%   \includegraphics[width=1.0\textwidth]{figures/image_caption.png}
%   \caption{Image-caption benchmark}
%   \label{fig:image_caption}
% \end{subfigure}
% \caption{Our proposed benchmarks}
% \label{fig:test}
% \end{figure*}


