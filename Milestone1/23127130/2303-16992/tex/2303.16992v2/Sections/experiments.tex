\vspace{-5pt}
\section{Experiments}
\label{sec:expreriments}
\vspace{-5pt}
\paragraph{Baselines.}

We compare \method{} with the following standard baselines.
\begin{itemize}
    \item \textbf{Centered Kernel Alignment (CKA)}: Proposed by \citet{kornblith2019similarity}, CKA computes a kernel matrix for each matrix representation input, and defines the scalar similarity index as the two kernel matrices' alignment. %The authors proposed linear and RBF kernels. 
    We use a linear kernel for CKA evaluation, as the original paper reveals similar results for both linear and RBF kernels. CKA is our main point of comparison due to its success in prior work and wide applicability. 
    
    \item \textbf{PWCCA}: Proposed by \citet{morcos2018insights}, PWCCA is an extension of Canonical Correlation Analysis (CCA). Given two matrices, CCA finds bases for those matrices, such that after projecting them to those bases the correlation between the projected matrices  is maximized. While in CCA the scalar similarity index is computed as the mean correlation coefficient, in PWCCA that mean is weighted by the importance each canonical correlation has on the representation.

    \item \textbf{SVCCA}: Another extension to CCA, proposed by \citet{raghu2017svcca}, which  performs CCA on the truncated singular value decomposition (SVD) of the activation matrices. SVCCA keeps enough principal components to explain a fixed percentage of the variance.\footnote{PWCCA and SVCCA require the number of examples to be larger than the feature vector dimension, which is not possible to achieve in all benchmarks. Therefore, we compare with them in a subset of our experiments.} 

    \item \textbf{Dot-product}: Measuring the similarity score between two feature vectors as their dot-product.

    \item \textbf{Norm}: For two representations, we defined a dis-similarity measure as the norm of the difference between the normalized representations. The final similarity will be 1 minus the dis-similarity score.
\end{itemize}



% The code to reproduce the experimental results is available at \url{COMPLETE URL}.


See Appendix \ref{appendix:sim_measures} for more details on each method.
In the main body we report results of the more successful methods: CKA and PWCCA. Additional baseline are reported in Appendix \ref{sec:additional_methods}. %We report results of PWCCA, CKA and our method -- \method{}. 

\paragraph{Ablations.}
In addition, we report the results of  two new similarity measures, which use an encoder to map representations to the space where similarity is measured. However, %the key difference from \method{} is in the encoder $f_{\theta}$ training procedure and the similarity measure $s$ we use. 
in both methods we train $f_
{\theta}$ to only maximize the similarity between positive pairs:
\begin{equation}
\mathcal{L}_{max} =  -s(\bm{z}_1, \bm{z}_2)
\label{cl_max}
\end{equation}
where $\bm{z}_1$ and $\bm{z}_2$ are representations whose similarity we wish to maximize.  %where for a batch of size $n$ similarity is measured as: $\frac{1}{n} \sum_{i=1}^n s\left(\bm{z}_1^{i}, \bm{z}_2^{i} \right)$. 
We experiment with two functions for  $s$---dot-product and CKA---and accordingly name these similarity measures DeepDot  and  DeepCKA.  These methods provide a point of comparison where the similarity measure is trained, but \emph{without negative examples}, to assess whether contrastive learning is crucial to our method's success. 

\paragraph{Encoders details.} In all experiments, the encoder $f_{\theta}$ is a two-layer multi-layered perceptron with hidden layer dimensions of $512$ and $256$, and output dimension of $128$.  We trained the encoder for 50 epochs for the layer prediction and 30 epochs for the multilingual and image--caption benchmarks. We used the Adam optimizer \citep{kingma2014adam} with a learning rate of $0.001$ and a batch size of $1024$ representations. We used $\tau = 0.07$ for \method{} training.

\subsection{Layer prediction benchmark}

\subsubsection{Setup} 
Recall that this benchmark evaluates whether a certain layer from one model is deemed most similar to its architecturally-corresponding layer from another model, where the two models differ only in their weight initialization. We repeat this process for all layers and 5 different model pairs, and report average accuracy. 
We experiment with both language and vision setups.

% \subsubsection{Setup}
\paragraph{Models.} For language experiments, we use the recent MultiBERTs \citep{sellam2021multiberts}, a set of 25 BERT models, differing only in their initial random weights. For vision experiments, we pre-train 10 visual transformer (ViT) models \citep{dosovitskiy2020image} on the ImageNet-1k dataset \citep{imagenet15russakovsky}. Then we fine-tune them on CIFAR-10 and CIFAR-100 datasets \citep{krizhevsky2009learning}.  Further training details are available in Appendix \ref{appendix:vit_parameters}.


\paragraph{Datasets.} For language experiments, we experiment with word-level contextual representations generated on two English text datasets: the Penn Treebank \citep{marcus-etal-1993-building} and WikiText \citep{merity2016pointer}. 
For Penn TreeBank we generate 5005 and 10019 representations for the test and training sets, respectively; for WikiText we generate  5024/10023 test/training  representations. For vision experiments, we experiment with representations generated on CIFAR-10 and CIFAR-100. For both we generate 5000 and 10000 test and training representations, respectively. 

% Our method was evaluated when the encoder was trained on data from both the same dataset and a different dataset.


\paragraph{Positive and Negative sets.} Given a batch of representations of some model $i$ at layer $j$, we define its positive set as the representations at the same layer $j$ of all models that differ from $i$. The negative set is all representations from layers that differ from $j$ (including from model $i$).

\begin{table*}[t]
\caption{Layer prediction benchmark accuracy results for language and vision cases. For encoder-based methods we report mean and std over 5 %different encoders trained with 
 random initializations. For \method{}, we experiment with training with different datasets (rows) and evaluating on same or different datasets (columns).}
\label{tab:layer_prediction}
\centering
\begin{tabular}{l rr | l rr}
\toprule
 \multicolumn{3}{c}{Language} & \multicolumn{3}{c}{Vision} \\ 
\cmidrule(lr){1-3} \cmidrule(lr){4-6}
& Penn TreeBank & WikiText &  & CIFAR-10 & CIFAR-100 \\ \midrule
PWCCA                                                      & 38.33       & 55.00 & PWCCA  & 47.27 & 45.45 \\ 
CKA                                                        & 71.66       & 76.66 & CKA  & 78.18  & 74.54    \\
DeepDot                                                    & 15.55 \small $\pm$ 1.69       & 14.00 \small $\pm$ 2.26 & DeepDot  & 14.90 \small $\pm$ 1.78 & 14.18 \small $\pm$ 2.67  \\
DeepCKA                                                    & 16.66 \small $\pm$ 3.16       & 19.66 \small $\pm$ 1.63 & DeepCKA  & 17.09 \small $\pm$ 2.95 & 13.09 \small $\pm$ 4.20  \\
\midrule 
\method{} & & & \method{} & & \\ 
% trained on: & & & trained on: & & \\ 
\hspace{.5em} Penn  & 100 \small $\pm$ 0         & 85.45 \small $\pm$ 1.62 & \hspace{.5em} CIFAR-10  & 100 \small $\pm$ 0   & 90.54 \small $\pm$ 2.90 \\ 
\hspace{.5em} Wiki  & 94.00 \small $\pm$ 4.66       & 100 \small $\pm$ 0 & \hspace{.5em} CIFAR-100  & 85.81 \small $\pm$ 5.68   & 100 \small $\pm$ 0  \\  
\bottomrule
\end{tabular}
\end{table*}
\vspace{-5pt}
\subsubsection{Results}
\vspace{-5pt}
The results are shown in Table \ref{tab:layer_prediction}. In both language and vision evaluations, CKA achieves better results than PWCCA, consistent with the findings by  \citet{ding2021grounding}. 
 DeepDot and DeepCKA  perform poorly, with much lower results than PWCCA and CKA, revealing that maximizing the similarity is not satisfactory for similarity measure purposes. 
Our method, \method{}, achieves excellent results. When trained on one dataset's training set and evaluated on the same dataset's test set, \method{} achieves perfect accuracy under this benchmark, with a large margin over CKA results. This holds for both language and vision cases. 

Even when trained on one dataset and evaluated over another dataset, \method{}  surpasses other similarity measures results, showing the transferability of the learned encoder projection between datasets. This is true both when transferring across domains (in text, between news texts from the Penn Treebank and Wikipedia texts), and when transferring across classification tasks (in images, between the 10-label CIFAR-10 and the 100-label CIFAR-100). 

% \begin{table}[t]
% \centering
% \begin{tabular}{l rr}
% \toprule
% Method                                                     & Penn TreeBank & WikiText \\ \midrule
% PWCCA                                                      & 38.33       & 55.00  \\
% CKA                                                        & 71.66       & 76.66 \\
% DeepDot                                                    & 15.55 \small $\pm$ 1.69       & 14.00 \small $\pm$ 2.26  \\
% DeepCKA                                                    & 16.66 \small $\pm$ 3.16       & 19.66 \small $\pm$ 1.63  \\
% \begin{tabular}[c]{@{}c@{}}ContraSim\\ Penn Trained\end{tabular} & 100 \small $\pm$ 0         & 85.45 \small $\pm$ 1.62  \\
% \begin{tabular}[c]{@{}c@{}}ContraSim\\ Wiki Trained\end{tabular} & 94.00 \small $\pm$ 4.66       & 100 \small $\pm$ 0    \\ \bottomrule
% \end{tabular}
% \caption{Invariance against changes to random seed benchmark accuracy results. For encoder-based methods we report mean and std results over 5 different encoders trained with different random initialization.}
% \label{tab:layer_prediction}
% \end{table}

% \paragraph{Computer Vision models.} In addition, we perform a similar evaluation using Computer Vision model - Visual Transformer (ViT) \citep{dosovitskiy2020image}. We pre-trained 10 models initialized with different random seeds on the ImageNet-1k dataset \citep{imagenet15russakovsky}. Further training details are available in Appendix \ref{appendix:vit_parameters}. Then we fine-tuned them on CIFAR-10 and CIFAR-100 datasets\citep{krizhevsky2009learning}. On the fine-tuned models we performed the same evaluations as on the MultiBerts models. Results, summarized in Table \ref{tab:cv_layer_prediction}, are consistent with results obtained on language models. When evaluated on the test set from the same dataset train set it was trained upon, \method{} achieves 100\% accuracy. When we train it using one dataset and evaluate it on another, we do see a small degradation in performance. However, \method{} accuracy is still above all other methods with a high margin.

% \begin{table}[]
% \centering
% \begin{tabular}{@{}ccc@{}}
% \toprule
% Method                                                                & CIFAR-10 & CIFAR-100  \\ \midrule
% PWCCA                                                                 & 47.27             & 45.45        \\
% CKA                                                                   & 78.18            & 74.54        \\
% DeepDot                                                               & 14.90 \small $\pm$ 1.78 & 14.18 \small $\pm$ 2.67       \\
% DeepCKA                                                               & 17.09 \small $\pm$ 2.95 & 13.09 \small $\pm$ 4.20        \\
% \begin{tabular}[c]{@{}c@{}}ContraSim\\ CIFAR-10 Trained\end{tabular}  & 100 \small $\pm$ 0   & 90.54 \small $\pm$ 2.90       \\
% \begin{tabular}[c]{@{}c@{}}ContraSim\\ CIFAR-100 Trained\end{tabular} & 85.81 \small $\pm$ 5.68   & 100 \small $\pm$ 0        \\ \bottomrule
% \end{tabular}
% \caption{Invariance against changes to random seed
% benchmark accuracy results on computer vision models.}
% \label{tab:cv_layer_prediction}
% \end{table}
\vspace{-5pt}
\subsection{Multilingual benchmark}
\label{sec:multilingual}
\vspace{-5pt}
\subsubsection{Setup}
This benchmark assesses whether a similarity measure assigns a high similarity to multilingual representations of the same sentence in different languages. Given a batch of (representations of) sentences $b^{(i)}$ in language $L_i$ and their translations $b^{(j)}$ in language $L_j$, we compute the similarity between $b^{(i)}$ and $b^{(j)}$, and the similarities between $b^{(i)}$ and 10 randomly chosen batches of representations in language $L_j$. If  $b^{(i)}$ is more similar to $b^{(j)}$ than  to all other batches, we mark success. (Alternatively, in a more challenging scenario, we use FAISS to find for each representation in each layer the 10 most similar representations in that layer.) We repeat this process separately for representations from different layers of a multilingual model, over many sentences and multiple language pairs, and report average accuracy per layer.\footnote{For deep similarity measures (DeepCKA, DeepDot, and \method{}), upon training the encoder on examples from a pair of languages, $(L_r, L_q), r \neq q$, we evaluate it over all other distinct pairs of languages.}   Appendix \ref{appendix:multilingual} gives more details. 

\paragraph{Model and Data.} We use two multilingual models: multilingual BERT \citep{devlin2018bert}\footnote{https://huggingface.co/bert-base-multilingual-cased} and XLM-R \citep{conneau2019unsupervised}. %, pretrained on 104 different languages.
We use the XNLI dataset \citep{conneau2018xnli}, which has natural language inference examples, parallel in multiple languages. Each example in our dataset is a  sentence taken from either the premise or hypothesis sets. We experiment with 5 typologically-different languages: English, Arabic, Chinese, Russian, and Turkish. We created sentence-level representations, with 5000 test 10000 training  representations.
As a sentence representation, we experiment with [CLS] token representations and with mean pooling of token representations, since \citet{del2021establishing} noted a difference in similarity in these two cases. We report results with [CLS] representations in the main body and with mean pooling in Appendix \ref{appendix:multilingual}; the trends are similar. 

 

% , and for layer each layer benchmark accuracy is defined as:

% \begin{equation}
% \begin{split}
% & s_t(b, i, j) = 
%     \begin{cases}
%       s(b[i], b[j]) & t==0\\
%       s(b[i], rand(B)[j]) & \text{otherwise}
%     \end{cases}  \\  
% & B(L_i, L_j) = \frac{\sum_{b \in B} (\argmax_t \{s_t(b, i, j)\}_{t=0}^{10} == 0)}{|B|} \\
% & Acc_{\text{CKA}} =  \frac{\sum_{L_i \in L} \sum_{L_j \neq Li} B(L_i, L_j)}{|L| |L-1|}
% \end{split}
% \end{equation}

% For each layer accuracy is defined as:
% \begin{equation}
% \begin{split}
%     Acc_{\text{Deep}} = & \frac{\sum_{(L_r, L_q), r\neq q}}{|L| |L-1|} \cdot \\
% & \frac{\sum_{L_i, i\neq r} \sum_{L_j, j\neq i, j \neq q} B(L_i, L_j)}{(|L| |L-1|)-1}
% \end{split}
% \end{equation}





\paragraph{Positive and Negative sets.} 
Given a pair of languages and a batch of representations at some layer, for each representation we define its positive pair as the representation of the sentence in the different language, and its negative set as all other representations in the batch.

\begin{table*}[t]
\caption{Multilingual benchmark accuracy results. With random sampling (left block), \method{} outperforms other similarity measures. Using FAISS (right block) further extends the gaps.}
\label{tab:multilingual_results}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{@{}ccccc|cccc@{}}
\toprule 
& \multicolumn{4}{c}{Random}                                                        & \multicolumn{4}{c}{FAISS}                                                   \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9}
Layer & CKA              & DeepCKA          & DeepDot           & ContraSim        & CKA              & DeepCKA          & DeepDot           & ContraSim         \\ \midrule
1     & 71.7 \small $\pm$ 5.3 & 82.0 \small $\pm$ 6.4 & 63.3 \small$\pm$ 10.4 & 95.5 \small $\pm$ 5.4 & 20.1 \small $\pm$ 4.0 & 10.7 \small $\pm$ 2.6 & 29.9 \small  $\pm$ 8.7  & 36.0 \small $\pm$ 10.7 \\
2     & 78.7 \small $\pm$ 4.4 & 86.4 \small $\pm$ 4.1 & 68.5 \small $\pm$ 9.9 & 95.0 \small $\pm$ 7.2 & 27.2 \small $\pm$ 5.5 & 12.3 \small $\pm$ 2.9 & 46.9 \small  $\pm$ 9.8  & 33.0 \small $\pm$ 14.8 \\
3     & 86.8 \small $\pm$ 3.0 & 87.1 \small $\pm$ 3.2 & 70.4 \small $\pm$ 9.7 & 96.4 \small $\pm$ 6.7 & 41.9 \small $\pm$ 8.7 & 17.6 \small $\pm$ 4.2 & 51.5 \small $\pm$ 10.3  & 45.4 \small $\pm$ 20.5 \\
4     & 92.6 \small $\pm$ 1.4 & 91.5 \small $\pm$ 2.4 & 95.4 \small $\pm$ 3.4 & 99.9 \small $\pm$ 0.2 & 33.4 \small $\pm$ 7.0 & 15.2 \small $\pm$ 3.7 & 52.2 \small  $\pm$ 8.6  & 72.4  \small $\pm$ 9.8  \\
5     & 88.3 \small $\pm$ 3.2 & 83.5 \small $\pm$ 5.2 & 94.7 \small $\pm$ 4.8 & 99.9 \small $\pm$ 0   & 49.3 \small $\pm$ 4.3 & 36.9 \small $\pm$ 6.3 & 42.4 \small $\pm$ 12.9  & 99.1. \small $\pm$ 0.8  \\
6     & 88.6 \small $\pm$ 3.4 & 86.4 \small $\pm$ 5.2 & 92.5 \small $\pm$ 5.4 & 100  \small $\pm$ 0   & 51.4 \small $\pm$ 5.5 & 39.9 \small $\pm$ 7.2 & 42.1 \small $\pm$ 12.3  & 99.5. \small $\pm$ 0.4  \\
7     & 88.8 \small $\pm$ 3.7 & 86.9 \small $\pm$ 5.0 & 92.6 \small $\pm$ 5.0 & 100  \small $\pm$ 0   & 53.0 \small $\pm$ 5.8 & 41.1 \small $\pm$ 7.7 & 45.7 \small $\pm$ 11.7  & 99.6. \small $\pm$ 0.3  \\
8     & 89.3 \small $\pm$ 3.6 & 85.2 \small $\pm$ 5.7 & 91.4 \small $\pm$ 7.0 & 100  \small $\pm$ 0   & 56.1 \small $\pm$ 5.8 & 45.0 \small $\pm$ 8.7 & 43.8 \small $\pm$ 13.4  & 99.7. \small $\pm$ 0.3  \\
9     & 88.1 \small $\pm$ 3.8 & 82.4 \small $\pm$ 5.6 & 89.1 \small $\pm$ 9.5 & 100  \small $\pm$ 0   & 53.3 \small $\pm$ 4.9 & 42.7 \small $\pm$ 8.5 & 39.2 \small $\pm$ 12.9  & 99.6. \small $\pm$ 0.3  \\
10    & 87.0 \small $\pm$ 3.5 & 80.3 \small $\pm$ 5.9 & 85.3\small $\pm$ 10.3 & 100  \small $\pm$ 0   & 51.5 \small $\pm$ 5.3 & 42.4 \small $\pm$ 7.8 & 34.3 \small $\pm$ 12.2  & 99.5. \small $\pm$ 0.4  \\
11    & 86.7 \small $\pm$ 4.2 & 76.6 \small $\pm$ 6.4 & 79.7\small $\pm$ 13.9 & 99.9 \small $\pm$ 0 & 52.4 \small $\pm$ 5.3 & 43.3 \small $\pm$ 8.5 & 31.4 \small $\pm$ 12.8  & 99.3. \small $\pm$ 0.5  \\
12    & 86.4 \small $\pm$ 3.4 & 63.8 \small $\pm$ 7.9 & 64.3\small $\pm$ 19.7 & 99.9 \small $\pm$ 0 & 52.8 \small $\pm$ 4.5 & 32.3 \small $\pm$ 8.7 & 26.1 \small $\pm$ 21.9  & 98.9. \small $\pm$ 0.8  \\ 
\bottomrule
\end{tabular}}
\end{table*}

\subsubsection{Results}
Results with multilingual BERT representations in Table \ref{tab:multilingual_results} show the effectiveness of our method. (Trends with XLM-R are consistent; Appendix~\ref{app:xlmr}). Under random sampling evaluation (left block), \method{} shows superior results over other similarity measures, although always being evaluated on language pairs it has not seen at training time. Using a FAISS-based sampler (right block) further extends the gaps. While CKA results dropped by $\approx 45\%$, DeepCKA  dropped by $\approx51\%$, and DeepDot  dropped by $\approx 40\%$, \method{} was much less affected by FAISS sampling ($\approx 17\%$ drop on average and practically no drop in most layers). This demonstrates the high separability between examples of \method{}, enabling it to distinguish even very similar examples and assign a higher similarity to the correct pair. For all other methods, mid-layers have the highest accuracy, whereas for our method almost all layers are near $100\%$ accuracy, except for  the first 3 or 4 layers. % first layers under random sampling and 4 layers under FAISS sampler.

To further analyze this, we compare the original multilingual representations from the last layer with their projections by \method{}'s trained encoder. 
Figure \ref{fig:multilingual_embeddings} from Appendix \ref{sec:multilingual_projections} shows UMAP \citep{mcinnes2018umap} projections of  representations of 5 sentences in English and 5 sentences in Arabic, before and after \method{} encoding.  The \method{} encoder was trained on Arabic and English languages. The original representations are organized according to the source language (by shape), whereas \method{} projects translations of the same sentence close to each other (clustered by color).

% ===================================
\subsection{Image--caption benchmark}
\subsubsection{Setup}
Given a test set $\mathbb{X}$, consisting of pairs of an image representation generated by a computer vision model and its caption representation from a language model, we split  $\mathbb{X}$ to  batches of size 64. For each batch, we compute the similarity between the image representations and their corresponding caption representations. We then sample 10 different caption batches, either randomly or using FAISS (as before), and compute the similarity between the image representation and each random/FAISS-retrieved caption representation. If the highest similarity is between the image representation and the original caption representation, we mark a success. %We average results over all batches.
For trainable similarity measures, we train with 5 different random seeds and average the results.

\paragraph{Models and Data.} 
We use two vision models for image representations: ViT  and ConvNext \citep{liu2022convnet}; and two  language models for text representations: BERT  and GPT2 \citep{radford2019language}.
%
%\paragraph{Datasets.}
We use the Conceptual Captions dataset \citep{sharma2018conceptual}, which has $\approx$3.3M pairs of images and English captions. We use 5000 and 10000 pairs as test and training sets, respectively.

\paragraph{Positive and Negative sets.}
Given a batch of image representations with their corresponding caption representations, for each image representation we define as a positive set its corresponding caption representation, and as a negative set all other representations in the batch.

\vspace{-5pt}
\subsubsection{Results}
\vspace{-5pt}
Figure \ref{fig:image_caption} demonstrates the strength of \method{}. Under random sampling (green boxes),  DeepCKA achieves comparable results to \method{}, while DeepDot and CKA achieve lower results. 
However, using FAISS (red boxes) causes a big decrease in DeepCKA accuracy, while \method{} maintains high accuracy. 

Another interesting result is that in 3 of 4 pairs we tested, FAISS sampling yielded better CKA accuracy than random sampling. This contradicts the intuition that using similar examples at the sampling stage should make it difficult for similarity measures to distinguish between examples. This might indicate that CKA suffers from stability issues.

% \begin{figure*}[t]
%  \begin{subfigure}{}
%      \includegraphics[width=0.44\linewidth]{figures/image_caption1.png}
%      \label{fig:a}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}{}
%      \includegraphics[width=0.44\linewidth]{figures/image_caption2.png}
%      \label{fig:b}
%  \end{subfigure} \\ 
%  \begin{subfigure}{}
%      \includegraphics[width=0.44\linewidth]{figures/image_caption3.png}
%      \label{fig:c}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}{}
%      \includegraphics[width=0.44\linewidth]{figures/image_caption4.png}
%      \label{fig:d}
%  \end{subfigure}
%     \vspace{-15pt}
%  \caption{Image--caption benchmark results for 4 different model pairs. \method{} works best, and is the only measure robust to FAISS sampling.}
%  \label{fig:image_caption}
% \end{figure*}

\vspace{-5pt}
\begin{figure*}[t]
\centering
\includegraphics[width=0.76\linewidth]{figures/image_caption_graphs.png}
\caption{Image--caption benchmark results for 4 different model pairs. \method{} works best, and is the only measure robust to FAISS sampling.}
\label{fig:image_caption}
\end{figure*}

Finally, we report results with the multi-modal CLIP model \citep{radford2021learning} in Appendix \ref{tab:image_caption_clip}. Because the model was pre-trained with contrastive learning, simple dot-product similarity works very well, so there is no need to learn a similarity measure in this case.  
% ===================================
\vspace{-5pt}
\section{Interpretability insights}
\vspace{-5pt}
Having shown the superiority of our method, we now discuss a few interpretability insights that arise from our evaluations, and are not revealed by previous similarity measures. 

In the multilingual benchmark (Table~\ref{tab:multilingual_results},  FAISS results),  we found a much greater difference in accuracy between shallow and deep layers in \method{} compared to previous similarity measures. Using previous similarity measures we might infer that there is no difference in the ability to detect the correct pairs across different layers. However, \method{} shows that the difference in the ability to detect the correct pair dramatically changes from shallow to deep layers. This raises an interesting insight regarding the evolution of representations across layers. For instance, in \citet{conneau2020emerging} they used CKA to measure the similarity of representations of bilingual models of different languages and same language (using back-translation). From their results it can be observed that there is no much difference in similarity of sentences from different languages and from the same language at the shallow and deep layers. Our results show that this difference is higher than found before.

In the image--caption benchmark (Figure~\ref{fig:image_caption}), from the CKA results we might infer that
% judging from the CKA results we might infer that 
BERT representations are more similar to computer vision representations than GPT2 representations. That is because with CKA, it is easier to detect the matching image--caption pair with BERT and the vision models than it is with GPT2 and the vision models.  However, \method{} achieves a high accuracy in both BERT pairs and GPT2 pairs, which means that both language models about as similar to  vision models, in contrast to what we may infer from previous similarity measures. This reveals a new understanding regarding the relationship between language  and vision models. To the best of our knowledge, no prior work has done such a similarity analysis.

