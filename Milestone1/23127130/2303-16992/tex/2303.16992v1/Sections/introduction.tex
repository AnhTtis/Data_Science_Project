\vspace{-15pt}
\section{Introduction}
% Deep Neural Networks (DNNs) have brought a breakthrough in a wide range of tasks. However, state-of-the-art results, deriving from increased model complexity, often result in loss of model interpretability, i.e., after their training process, many of these models are used as a black box without understanding their learned representations. However, understanding model representation is a crucial step in the development of models with a high-reliability level. 

Representation learning is a key property in deep neural networks. But how can we assess the similarity of representations learned by two models?
%But how can we assess how similar are representations learned by two models?
% whether two models learn similar or different representations? 
A recent line of work is concerned with developing similarity measures and using them to analyze the models' internal representations. Similarity-based analyses may shed light on how different datasets, architectures, etc., change the model's learned representations. For example, a similarity analysis showed that lower layers in different models are more similar to each other, while fine-tuning affects mostly the top layers \citep{wu2020similarity}. 

Various similarity measures have been proposed for comparing representations, among them the most popular ones are based on centered kernel alignment (CKA) \citep{kornblith2019similarity} and canonical correlation analysis (CCA)  \citep{Hotelling1936RelationsBT, morcos2018insights}. They all share a similar methodology: given a pair of feature representations \emph{of the same input}, they estimate the similarity between them, without considering other examples. However, they all perform mediocrely on standard benchmarks. Thus, we might question the reliability of their similarity scores. Motivated by that, we propose a new learnable similarity measure.

In this paper, we introduce \method{}, a new similarity measure based on contrastive learning (CL) \citep{chen2020simple, he2020momentum}. Contrary to prior work, which defines closed-form general-purpose similarity measures, \method{} is a task-specific learnable similarity measure that uses examples with a high similarity (the \textit{positive} set) and examples that have a low similarity (the \textit{negative} set), to train an encoder that maps representations to the space where similarity is measured. In the projected space, representation similarity is maximized with examples from the positive set, and minimized with examples from the negative set. %The projected representations similarity is calculated with a simpler closed-form similarity measure.

We experimentally evaluate \method{} on one standard similarity metrics benchmark and two new benchmarks we introduce in this paper, demonstrating its superiority over common similarity metrics.  First, we use the known layer prediction benchmark \citep{kornblith2019similarity}, which assesses whether high similarity is assigned to two architecturally-corresponding layers in two models differing only in their weight initialization. 
Second, in our proposed multilingual benchmark, we assume a multilingual model and a parallel dataset of translations in two languages.  A good similarity measure should assign a higher similarity to the (multi-lingual) representations of a sentence in language A and its translation in language B, compared to the similarity of the same sentence in language A and a random sentence in language B.  
Third, we design the image--caption benchmark,  based on a similar idea. Given an image and its text caption, and correspondingly a vision model and a language model, a good similarity measure should assign a high similarity to representations of the image and its caption, compared to the similarity of the same image and a random caption.  
% Image and its caption describe the same scene in a different way, thus we expect similarity measures to assign high similarity between image representation and its caption representation, although generated by different models.



In both of our new benchmarks, we investigate a more challenging scenario, where during evaluation instead of choosing a random sentence, we retrieve highly similar sentences as confusing examples, using the Facebook AI Similarity Search (FAISS)  library \citep{johnson2019billion}.  While other similarity measures are highly affected by this change,  our method maintains a high accuracy with a very small degradation. We attribute this to the highly separable representations that our method learns.

Through ablations, we show that if we change the training procedure of the encoder to only maximize the similarity of similar examples, the projected representations have poor separation, indicating that the CL procedure is a crucial part of the method's success. Finally, we demonstrate that \method{} reveals new insights not captured by previous similarity measures.

In summary, this work makes the following contributions:
\begin{itemize}
    \item We introduce a new similarity measure --  \method{}. Inspired by contrastive learning, it uses positive and negative sets to train an
    encoder that maps representations to the space where similarity is measured.
    \item We propose two new benchmarks for the evaluation of
    similarity measures: the multilingual benchmark and the
    image--caption benchmark.
    \item We show that \method{} outperforms existing similarity
    measures in all benchmarks, and maintains a high accuracy
    even when faced with more challenging examples.
\end{itemize}



