\vspace{-5pt}
\section{Conclusion}
\vspace{-5pt}
We proposed a new similarity measure, \method, based on ideas from contrastive learning. By defining  positive and negative sets we learn an encoder that maps representation to a space where similarity is measured. 
%  We evaluated our method against known similarity measures: CKA and PWCCA. We also considered another two similarity measures we proposed: DeepDot and DeepCKA, in order to show that negative examples are a critical component of our method's success. 
Our method outperformed other similarity measures under the common layer prediction benchmark, and two new benchmarks we proposed: the multilingual benchmark and the image--caption benchmark. It particularly shines in strengthened versions of said benchmarks, where random sampling is replaced with finding the most similar examples using FAISS. 
 Considering \method{}'s superiority in all evaluations, we believe it is a better tool for interpretability of neural networks, and have highlighted a few insights revealed by  \method{}. 

Our new similarity measure benchmarks can facilitate work on similarity-based analyses of deep networks. The multilingual benchmark is useful for work on multilingual language models, while the image--caption benchmark may help in multi-modal settings. A drawback of our method is that it can only compare representations of the same dimensionality. This can be addressed via dimensionality reduction to a shared space, which we  leave for future work. Moreover, compared to existing methods, \method{} needs access to a training set for the encoder training procedure. The training procedure itself is efficient, typically a matter of minutes. %Compared with closed-form similarity measures, training time is another trade-off.
%
In addition, since our method learns a parameterized measure, it may help train models with similarity objectives.
We also leave that for future work.
% 2Finally, considering \method{}'s superiority in all evaluations, it will better fit for interpretability of neural networks.

% \paragraph{Evaluation methods.} We further proposed two new similarity measure benchmarks - the multilingual benchmark and the image--caption benchmark. The former is based on representations, generated by a multilingual model, of the same sentence translated to different languages. A good similarity measure is expected to assign a high similarity for representations of the same sentence in different languages. The latter is a multi-modal benchmark, given a pair of image and its caption, and their corresponding representations generated by a Computer Vision model and a Language Model (respectively), we expect a good similarity measure to assign a high similarity between their representations. Image and its caption describe the same scene in a different way, thus we expect similarity measures to assign a high similarity between image representation and its caption representation, although generated by different models. Moreover, we suggested a strengthened version for our proposed benchmarks using the Facebook AI Similarity Search (FAISS) library, where we replaced the random sampling with the most similar examples sampling. 
% We showed that \method outperforms all other similarity measures across all benchmarks, especially under the FAISS variant benchmarks, where other methods were dramatically affected, \method only experienced a slight drop in performance.
