\vspace{-5pt}
\section{\method{}}
\label{sec:method}
In this section we introduce \method{}, a similarity index for measuring the similarity of neural network representations. Our method uses a trainable encoder, which first maps representations to a new space and then measures the similarity of the projected representations. Formally, let $e_\theta$ denote an encoder with trainable parameters $\theta$, and assume two representations $\bm{a}_1$ and $\bm{a}_2$. 
In order to obtain a similarity score between $0$ and $1$,
we first apply L2 normalization to the encoder outputs: $\bm{z}_1 = e_\theta(\bm{a}_1) / \| e_\theta(\bm{a}_1) \|$ (and similarly for $\bm{a}_2$). 
Then their similarity is calculated as: 
\begin{equation}
\label{sim}
    s(\bm{z}_1, \bm{z}_2)
\end{equation}
where $s$ is a simple closed-form similarity measure for two vectors. Throughout this work we use dot product for $s$.   

For efficiency reasons, we calculate the similarity between batches of the normalized encoder representations, dividing by the batch size $n$:   
% In case $\bm{a}_1$ and $\bm{a}_2$ are batches of representations, we divide Eq.~\ref{sim} by the batch size $n$ in order to maintain a similarity score between $0$ and $1$: 
\vspace{-5pt}
\begin{equation}
    \frac{1}{n} \sum_{i=1}^n \left(\bm{z}_1^{i} \cdot \bm{z}_2^{i} \right)
\end{equation}
\vspace{-10pt}
\paragraph{Training.} 
None of the similarity measures that are common in the literature on analyzing neural network representations uses negative examples to estimate the similarity of a given pair (Section~\ref{sec:related-work}). Given two examples, these measures output a  scalar that represents the similarity between them, without leveraging data from other examples. However, based on knowledge from other examples, we can construct a better similarity measure. 
In particular, for a given example $x^{(i)} \in \mathbb{X}$ with its encoded representation $\bm{z}_i$, we construct a set of \textit{positive} example indices, $P(i) = \{p_1, ..., p_q\}$, and a set of \textit{negative} example indices, $N(i) = \{n_1, ..., n_t\}$. 
The choice of these sets is task-specific and allows one to add inductive bias to the training process. 

We train the encoder to maximize the similarity of $\bm{z}_i$ with all the positive examples, while at the same time making it dis-similar from the negative examples. We leverage ideas from contrastive learning \citep{chen2020simple, he2020momentum}, and minimize the following objective:
\begin{equation}
\mathcal{L} =  \sum_{i\in I} \frac{-1}{|P(i)|}  \log \frac{\sum_{p\in P(i)} \exp (\bm{z}_i \cdot \bm{z}_p/ \tau)}{\sum_{n\in N(i)} \exp (\bm{z}_i \cdot \bm{z}_n/ \tau)}
\label{cl_loss}
\end{equation}
with scalar temperature parameter $\tau > 0$. Here $\bm{z}_p$ and $\bm{z}_n$ are normalized encoder outputs of representations from the positive and negative groups, respectively. It is important to notice that we do not alter the original model representations, and the only trainable parameters are the encoder $e$'s parameters, $\theta$. 

% To our knowledge, our work is the first one to consider contrastive learning for the purpose of a similarity measure.
Our work uses  negative examples and a trainable encoder for constructing a similarity measure. We  evaluate these two aspects in the experimental section, and show that using  negative examples is an important aspect of our method. We  show that the combination of the two leads to a similarity measure with superior results over current similarity measures. 

