\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Multilingual benchmark}
\label{appendix:multilingual}
\subsubsection{Evaluation parameters}
We split the test set, $\mathbb{X}$, into equally sized batches of size 8, $\{b^{(1)}, b^{(2)}, ..., b^{(n )}\}$, where each batch consists of multilingual BERT representations  of the same sentence in 5 different languages: $L=\{L_1, ..., L_5\} $.  Given a pair of different languages, $(L_i, L_j), i \neq j$, and a batch of representations, $b$, we consider the representation of those languages in the batch, $(b[i], b[j])$, and compute the similarity between $b[i]$ and $b[j]$ as $s_0 \equiv s(b[i], b[j])$. We also compute the similarity between $b[i]$ and 10 randomly chosen batches (or, 10 batches chosen using FAISS) of representations in language $L_j$ as $\{s_t \equiv s(b[i], b_t[j])\}_{t=1}^{10}$. 
If $\argmax_t \{s_t\}_{t=0}^{10} = 0$, we count it as a correct prediction. Each layer's accuracy is defined as the number of successful predictions over the number of batches, $n$. We average results over all possible pairs of different languages.  

\subsubsection{\method{} Projections}
\label{sec:multilingual_projections}
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/multilingual_representations.png}
\caption{ %Original representations vs.\ their ContraSim representations. 
Original representations (left) are clustered by the source language (by shape). 
\method{}  (right) projects representations of the same sentence in different languages close by  (by color).}
\label{fig:multilingual_embeddings}
\end{figure}


\subsubsection{Further evaluations} \label{app:xlmr}
In addition to using the [CLS] token representation as a sentence representation, we also evaluated the multilingual benchmark using mean pooling sentence representation. We used the same evaluation process as described in Section \ref{sec:multilingual}. The results, summarized in Table \ref{tab:multilingual_mean}, are consistent with the results in the main paper (Table \ref{tab:multilingual_results}). Under random sampling, \method{} outperforms all other similarity measures. Using FAISS causes a big degradation in all other methods' accuracy, while \method{} maintains a high accuracy across all layers. 

\label{sec:multilingual_mean}

\begin{table*}[t]
\caption{Multilingual benchmark results with mean pooling.}
\label{tab:multilingual_mean}
\centering

\resizebox{\textwidth}{!}{\begin{tabular}{@{}ccccc|cccc@{}}

      & \multicolumn{4}{c}{Random}                                                                                                                          & \multicolumn{4}{c}{FAISS}                                                   \\ \midrule
Layer & CKA                           & DeepCKA                       & DeepDot                        & \multicolumn{1}{c|}{ContraSim}                     & CKA                            & DeepCKA                        & DeepDot                        & ContraSim                      \\ \midrule
1     & 87.7 \small $\pm$ 6.9 & 86.3  \small $\pm$ 9.7 & 43.4 \small $\pm$ 17.7 & 98.7 \small $\pm$ 2.2 & 67.6 \small $\pm$ 14.3 & 54.1 \small $\pm$ 10.9  & 41.7 \small $\pm$ 19.1 & 94.2 \small $\pm$ 10.7 \\
2     & 89.0 \small $\pm$ 6.3 & 88.7  \small $\pm$ 7.0 & 51.5 \small $\pm$ 20.0 & 99.5 \small $\pm$ 0.8 & 68.2 \small $\pm$ 13.2 & 49.0  \small $\pm$ 8.3  & 38.9 \small $\pm$ 17.2 & 96.6 \small $\pm$ 14.8 \\
3     & 91.8 \small $\pm$ 4.4 & 90.7  \small $\pm$ 6.0 & 63.3 \small $\pm$ 20.8 & 99.9 \small $\pm$ 0.1 & 72.2 \small $\pm$ 11.5 & 55.4  \small $\pm$ 8.1  & 44.8 \small $\pm$ 16.6 & 98.8 \small $\pm$ 20.5 \\
4     & 93.7 \small $\pm$ 3.3 & 91.3  \small $\pm$ 5.0 & 73.1 \small $\pm$ 19.4 & 99.9 \small $\pm$ 0.0 & 74.3 \small $\pm$ 10.0 & 55.1  \small $\pm$ 8.0  & 45.7 \small $\pm$ 16.5 & 99.5 \small  $\pm$ 7.1  \\
5     & 95.3 \small $\pm$ 3.0 & 92.1  \small $\pm$ 4.0 & 83.9 \small $\pm$ 15.6 & 99.9 \small $\pm$ 0.0 & 78.2 \small  $\pm$ 8.2 & 56.7  \small $\pm$ 8.1  & 53.2 \small $\pm$ 17.5 & 99.8 \small  $\pm$ 4.4  \\
6     & 95.9 \small $\pm$ 2.4 & 91.8  \small $\pm$ 3.9 & 91.2 \small $\pm$ 10.6 & 100  \small $\pm$ 0   & 77.6 \small  $\pm$ 7.9 & 54.2  \small $\pm$ 8.1  & 60.1 \small $\pm$ 18.2 & 99.8 \small  $\pm$ 1.7  \\
7     & 95.4 \small $\pm$ 2.5 & 90.6  \small $\pm$ 4.1 & 93.1 \small  $\pm$ 9.2  & 100  \small $\pm$ 0  & 77.9 \small  $\pm$ 7.8 & 53.3  \small $\pm$ 7.2  & 63.5 \small $\pm$ 18.5 & 99.9 \small  $\pm$ 0.7  \\
8     & 94.8 \small $\pm$ 3.2 & 89.7  \small $\pm$ 4.3 & 90.3 \small $\pm$ 12.0 & 100  \small $\pm$ 0   & 76.7 \small  $\pm$ 8.1 & 52.4  \small $\pm$ 7.4  & 61.0 \small $\pm$ 19.8 & 99.9 \small  $\pm$ 0.3  \\
9     & 94.0 \small $\pm$ 3.4 & 88.5  \small $\pm$ 5.0 & 86.4 \small $\pm$ 15.1 & 100  \small $\pm$ 0   & 73.9 \small  $\pm$ 8.8 & 51.4  \small $\pm$ 7.8  & 55.5 \small $\pm$ 20.0 & 99.9 \small  $\pm$ 0.1  \\
10    & 92.6 \small $\pm$ 4.2 & 85.6  \small $\pm$ 5.9 & 80.7 \small $\pm$ 18.8 & 100  \small $\pm$ 0   & 72.2 \small  $\pm$ 8.4 & 49.3  \small $\pm$ 8.4  & 49.2 \small $\pm$ 20.6 & 99.9 \small  $\pm$ 0.1  \\
11    & 91.1 \small $\pm$ 5.1 & 81.0  \small $\pm$ 6.5 & 72.2 \small $\pm$ 23.7 & 99.9 \small $\pm$ 0   & 70.6 \small $\pm$ 10.1 & 48.8  \small $\pm$ 9.1  & 43.2 \small $\pm$ 20.7 & 99.8 \small  $\pm$ 0.1  \\
12    & 90.8 \small $\pm$ 5.8 & 71.3  \small $\pm$ 7.6 & 71.0 \small $\pm$ 21.0 & 99.9 \small $\pm$ 0   & 72.7 \small $\pm$ 11.3 & 40.3  \small $\pm$ 8.7  & 42.7 \small $\pm$ 17.0 & 99.4 \small  $\pm$ 0.1  \\ \bottomrule
\end{tabular}}
\end{table*}

In addition, we evaluated the multilingual benchmark with another multilingual model -- the XLM-R \citep{conneau2019unsupervised} model. Results, summarized in Table \ref{tab:multilingual_xlm}, show a similar pattern to Tables \ref{tab:multilingual_results} and \ref{tab:multilingual_mean}, with \method{} achieving the highest accuracies across all layers, in both random sampling and FAISS sampling scenarios. 

\begin{table*}[h]
\centering
\caption{Multilingual benchmark results on XLM-R model.}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}ccccccccc@{}}
\multicolumn{5}{c}{Random} &
  \multicolumn{4}{c}{FAISS} \\ \midrule
Layer &
  CKA &
  DeepCKA &
  DeepDot &
  \multicolumn{1}{c|}{ContraSim} &
  CKA &
  DeepCKA &
  DeepDot &
  ContraSim \\ \midrule
1 &
  89.7 \small $\pm$. 4.6 &
  88.2 \small $\pm$  3.1 &
  45.0 \small $\pm$ 22.1 &
  \multicolumn{1}{c|}{99.89 \small $\pm$ 0.31} &
  60.6 \small $\pm$ 11.4 &
  24.6 \small $\pm$  4.7 &
  24.8 \small $\pm$ 12.4 &
  98.1 \small $\pm$  2.5 \\
2 &
  90.6 \small $\pm$  4.1 &
  92.3 \small $\pm$  2.0 &
  63.1 \small $\pm$ 23.4 &
  \multicolumn{1}{c|}{99.99 \small $\pm$ 0.02} &
  57.1 \small $\pm$ 11.4 &
  30.7 \small $\pm$  5.7 &
  31.1 \small $\pm$ 13.5 &
  99.5 \small $\pm$  0.7 \\
3 &
  92.8 \small $\pm$ 3.1 &
  93.8 \small $\pm$ 1.7 &
  79.5 \small $\pm$18.9 &
  \multicolumn{1}{c|}{99.99 \small $\pm$ 0} &
  55.5 \small $\pm$10.1 &
  33.8 \small $\pm$ 6.4 &
  39.3 \small $\pm$15.5 &
  99.9 \small $\pm$ 0.1 \\
4 &
  94.7 \small $\pm$ 2.6 &
  94.3 \small $\pm$ 1.7 &
  91.4 \small $\pm$11.7 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  62.3 \small $\pm$ 9.5 &
  36.3 \small $\pm$ 6.8 &
  55.1 \small $\pm$16.2 &
  99.9 \small $\pm$ 0 \\
5 &
  95.9 \small $\pm$ 2.1 &
  94.6 \small $\pm$ 1.6 &
  94.0 \small $\pm$10.0 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  66.2 \small $\pm$ 8.4 &
  37.2 \small $\pm$ 7.6 &
  64.2 \small $\pm$16.2 &
  99.9 \small $\pm$ 0 \\
6 &
  95.9 \small $\pm$ 2.1 &
  94.9 \small $\pm$ 1.6 &
  94.6 \small $\pm$ 8.9 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  66.7 \small $\pm$ 8.3 &
  41.2 \small $\pm$ 7.6 &
  66.0 \small $\pm$17.5 &
  99.9 \small $\pm$ 0 \\
7 &
  96.6 \small $\pm$ 2.0 &
  94.9 \small $\pm$ 1.6 &
  94.9 \small $\pm$ 8.5 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  71.7 \small $\pm$ 8.5 &
  44.1 \small $\pm$ 8.2 &
  68.8 \small $\pm$17.5 &
  99.9 \small $\pm$ 0 \\
8 &
  96.1 \small $\pm$ 2.1 &
  94.8 \small $\pm$ 1.7 &
  94.0 \small $\pm$ 9.3 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  68.1 \small $\pm$ 8.3 &
  43.6 \small $\pm$ 7.9 &
  65.3 \small $\pm$18.0 &
  99.9 \small $\pm$ 0 \\
9 &
  94.8 \small $\pm$ 2.2 &
  94.9 \small $\pm$ 1.6 &
  93.3 \small $\pm$ 9.1 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  58.5 \small $\pm$ 8.7 &
  42.8 \small $\pm$ 8.0 &
  61.7 \small $\pm$18.6 &
  99.9 \small $\pm$ 0 \\
10 & 
  93.9 \small $\pm$ 2.3 &
  94.3 \small $\pm$ 1.7 &
  92.7 \small $\pm$10.7 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  46.3 \small $\pm$ 8.2 &
  39.1 \small $\pm$ 7.5 &
  56.6 \small $\pm$ 18.9 &
  99.9 \small $\pm$ 0 \\
11 &
  92.0 \small $\pm$  2.8 &
  93.3 \small $\pm$  2.2 &
  92.7 \small $\pm$ 10.9 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  35.5 \small $\pm$  7.0 &
  39.1 \small $\pm$  7.3 &
  57.2 \small $\pm$ 18.3 &
  99.9 \small $\pm$  0 \\
12 &
  80.7 \small $\pm$ 4.7 &
  89.5 \small $\pm$ 3.0 &
  81.6 \small $\pm$ 13.8 &
  \multicolumn{1}{c|}{100 \small $\pm$ 0} &
  26.5  \small $\pm$  5.8 &
  32.3  \small $\pm$  7.0 &
  34.7  \small $\pm$ 15.1 &
  99.9  \small $\pm$  0 \\ \bottomrule
\end{tabular}}
\label{tab:multilingual_xlm}
\end{table*}

\subsection{Image--caption}

In addition to the four model pairs we evaluated in Figure \ref{fig:image_caption}, we assessed the multi-modal vision and language CLIP model \citep{radford2021learning}, which was trained using contrastive learning on pairs of images and their captions. Results in Table \ref{tab:image_caption_clip} show interesting findings. Under random sampling, dot product, DeepCKA and \method{} achieve perfect accuracy. However, using FAISS causes significant degradation in DeepCKA accuracy, and only a small degradation in dot product and \method{} results, with equal accuracy for both. We attribute this high accuracy for simple dot product to the fact that CLIP training was done using contrastive learning, thus observing high separability between examples.
  
\begin{table}[h]
\caption{Image--caption benchmark accuracy results using CLIP model}
\label{tab:image_caption_clip}
\centering
\begin{tabular}{l r r }
\toprule
                                                      & Random & FAISS \\ \midrule
CKA                                                   & 93.67  & 25.31 \\
Dot Product & 100    & 98.73 \\
DeepCKA                                               & 100    & 13.92 \\
DeepDot                                               & 29.11  & 25.31 \\
ContraSim                                             & 100    & 98.73 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{ViT training details}
\label{appendix:vit_parameters}
We used the ViT-base \citep{dosovitskiy2020image} architecture. We pretrained 10 models on the ImgaeNet-1K dataset \citep{deng2009imagenet}, differing only in their weight initializations by using random seeds from 0 to 9. We used the AdamW optimizer \citep{kingma2014adam} with $\text{lr}=0.001$, $\text{weight decay} = 1e-3$, $\text{batch size} = 128$, and a cosine learning scheduler. We trained each model for 150 epochs and used the final checkpoint.

Then, we fine-tuned the pretrained models on CIFAR-10 and CIFAR-100 datasets \citep{krizhevsky2009learning}. We used AdamW optimizer with $\text{lr}=2e-5$, $\text{weight decay} = 0.01$, $\text{batch size} = 10$, and a linear scheduler. For models fine-tuned on CIFAR-10, the average accuracy on the CIFAR-10 test set is $96.33\%$. For models fine-tuned on CIFAR-100, the average accuracy on the CIFAR-100 test set is $78.87\%$.


\subsection{Details of Prior Similarity Measures}
\label{appendix:sim_measures}
\paragraph{Canonical Correlation Analysis (CCA).}
 Given two matrices, CCA finds bases for those matrices, such that after projecting them to those bases the projected matrices' correlation is maximized. For $1 \leq i \leq p_1$, the $i^{\text{th}}$ canonical correlation coefficient $\rho_i$ is given by:
\begin{equation}
\begin{aligned}
\rho_i = \max_{w_X^i, w_Y^i}&  \text{corr}(Xw_X^i, Yw_Y^i)\\
\mathrm{s.t.} &\ ~~ \forall_{j< i}~~ Xw_X^i\perp Xw_X^j\\
&\ ~~ \forall_{j< i}~~ Yw_Y^i\perp Yw_Y^j .
\end{aligned}
\end{equation}
where $\mathrm{corr}(X, Y) = \frac{\langle X, Y \rangle}{\|X\| \cdot \|Y\|}$. Given the vector of correlation coefficients $\mathrm{corrs} = (\rho_1, ..., \rho_{p_1})$, the final scalar similarity index is computed as the mean correlation coefficient:
\begin{equation}
\begin{aligned}
S_{\mathrm{CCA}}(X, Y) = \overline{\rho}_{CCA} = \frac{\sum_{i=1}^{p_1} \rho_{i}}{p_1}
\end{aligned}
\end{equation}
as previously used in \citep{raghu2017svcca, kornblith2019similarity}.


\paragraph{Projection-Weighted CCA (PWCCA).} 
\citet{morcos2018insights} proposed a different approach to transform the vector of correlation coefficients, $\mathrm{corrs}$, into a scalar similarity index. Instead of defining the similarity as the mean correlation coefficient, PWCCA uses a weighted mean and the similarity is defined as:
\begin{align}
S_{PW} &= \frac{\sum_{i=1}^{p_1} \alpha_i \rho_i}{\sum_{i} \alpha_i} & 
\alpha_i &=\sum_{j} |\langle h_i, x_j \rangle|
\end{align}
where $x_j$ is the $j^\text{th}$ column of $X$, and $h_i = Xw_X^i$ is the vector observed upon projecting $X$ to the $i^{\text{th}}$ canonical direction. Code available at: \url{https://github.com/google/svcca}.

\paragraph{Centered Kernel Alignment (CKA).} CKA, Proposed by \citet{kornblith2019similarity}, suggests computing a kernel matrix for each matrix representation input, and defining the scalar similarity index as the two kernel matrices' alignment. For linear kernel, CKA is defined as:
\begin{align}
S_{\mathrm{CKA}} = \frac{\|Y^TX\|_F^2}{\|X^TX\|_F\|Y^TY\|_F}
\end{align}

Code available at: \url{https://github.com/google-research/google-research/tree/master/representation_similarity}.

\paragraph{Norm.} For two representations, $x$ and $y$, we defined dis-similarity measure as:

\begin{align}
    Dis_{\mathrm{Norm}}(x, y) = \|(x / \|x\| - y / \|y\|)\|
\end{align}

Since this is a dis-similarity measure, we defined the norm similarity measure as:

\begin{align}
    S_{\mathrm{Norm}} = 1 - Dis_{\mathrm{Norm}}(x, y)
\end{align}

For a batch of representations, we define batch similarity as the mean of pairwise norm similarity. 

\subsection{Additional Evaluations}
\label{sec:additional_methods}

In this section, we report evaluation results with additional baselines. Furthermore, we evaluated \method{} with a different similarity measure than the dot-product and replaced it with the norm similarity measure.


\begin{table}[]
\centering
\caption{Layer prediction benchmark with additional similarity measures.}
\begin{tabular}{@{}ccc@{}}
\toprule
                & Penn TreeBank & WikiText \\ \midrule
SVCCA           & 46.66         & 56.66    \\
Dot product     & 8.33          & 6.66     \\
Norm            & 10.00            & 11.66    \\ \midrule
ContraSim\_norm &               &          \\
Penn            & 100.00           & 90.00       \\
Wiki            & 100.00           & 100.00      \\ \bottomrule
\end{tabular}
\label{tab:additional_layer_prediction}
\end{table}

Similar to PWCCA, SVCCA requires that the number of examples is larger than the vector dimension, thus we could only evaluate it in the layer prediction benchmark. All other similarity measures were evaluated with all evaluations - the layer prediction benchmark, the multilingual benchmark and the image-caption benchmark.

Table \ref{tab:additional_layer_prediction} shows layer prediction benchmark results. We can observe that SVCCA achieves slightly better results than PWCCA, and lower than CKA and \method{}. Both dot product and norm achieve low accuracies. ContraSim\_norm achieves the same or better results than \method{}.  

\begin{table}[]
\centering
\caption{Multilingual benchmark with additional similarity measures. The left block is with random sampling, and the right block is FAISS sampling.}
\begin{tabular}{@{}ccccccc@{}}
\toprule
&
  \multicolumn{3}{c}{Random} &
  \multicolumn{3}{c}{FAISS} 
   \\ \cmidrule(l){2-7} 
Layer &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}l@{}}Dot\\ product\end{tabular}} &
  Norm &
  \multicolumn{1}{c|}{ContraSim\_norm} &
  \begin{tabular}[c]{@{}c@{}}Dot\\ Product\end{tabular} &
  \multicolumn{1}{l}{Norm} &
  ContraSim\_norm \\ \midrule
1 &
  48.93 \small $\pm$ 7.07 &
  68.67 \small $\pm$ 10.86 &
  \multicolumn{1}{c|}{89.39 \small $\pm$ 14.46} &
  15.00 \small $\pm$ 6.41 &
  20.42 \small $\pm$ 9.32 &
  15.76 \small  $\pm$ 6.56 \\
2 &
  31.71 \small $\pm$ 3.31 &
  74.19 \small $\pm$ 9.73 &
  \multicolumn{1}{c|}{85.40 \small $\pm$ 18.00} &
  20.14 \small $\pm$ 2.75 &
  21.16 \small $\pm$ 11.43 &
  18.83 \small  $\pm$ 11.19 \\
3 &
  49.32 \small $\pm$ 6.53 &
  83.92 \small $\pm$ 13.12 &
  \multicolumn{1}{c|}{85.68 \small $\pm$ 18.88} &
  13.00 \small $\pm$ 4.50 &
  29.42 \small $\pm$ 22.99 &
  26.36 \small $\pm$ 16.01 \\
4 &
  29.60 \small $\pm$ 2.44 &
  99.61 \small $\pm$ 0.41 &
  \multicolumn{1}{c|}{96.81 \small $\pm$ 5.04} &
  16.20 \small $\pm$ 4.01 &
  57.98 \small $\pm$ 15.68 &
  28.79 \small  $\pm$ 9.80 \\
5 &
  99.75 \small $\pm$ 0.29 &
  99.86 \small $\pm$ 0.33 &
  \multicolumn{1}{c|}{99.86 \small $\pm$ 0.23} &
  82.17 \small $\pm$ 7.36 &
  82.39 \small $\pm$ 7.73 &
  74.04 \small $\pm$ 7.11 \\
6 &
  99.75 \small $\pm$ 0.35 &
  99.84 \small $\pm$ 0.27 &
  \multicolumn{1}{c|}{99.92 \small $\pm$ 0.13} &
  83.38 \small $\pm$ 7.70 &
  88.24 \small $\pm$ 6.15 &
  77.00 \small $\pm$ 6.68 \\
7 &
  99.52 \small $\pm$ 0.77 &
  99.85 \small $\pm$ 0.29 &
  \multicolumn{1}{c|}{99.92 \small $\pm$ 0.13} &
  89.72 \small $\pm$ 5.57 &
  89.23 \small $\pm$ 6.98 &
  78.21 \small $\pm$ 6.77 \\
8 &
  99.93 \small $\pm$ 0.13 &
  99.89 \small $\pm$ 0.15 &
  \multicolumn{1}{c|}{99.94 \small $\pm$ 0.11} &
  93.49 \small $\pm$ 4.07 &
  89.70 \small $\pm$ 6.42 &
  81.97 \small $\pm$ 6.63 \\
9 &
  99.61 \small $\pm$ 0.38 &
  99.76 \small $\pm$ 0.40 &
  \multicolumn{1}{c|}{99.91 \small $\pm$ 0.17} &
  82.48 \small $\pm$ 9.32 &
  84.85 \small $\pm$ 8.57 &
  81.37 \small $\pm$ 6.53 \\
10 &
  96.64 \small $\pm$ 2.46 &
  99.38 \small $\pm$ 0.58 &
  \multicolumn{1}{c|}{99.89 \small $\pm$ 0.15} &
  55.02 \small $\pm$ 15.42 &
  81.43 \small $\pm$ 9.77 &
  80.59 \small $\pm$ 6.83 \\
11 &
  87.04 \small $\pm$ 8.13 &
  98.40 \small $\pm$ 1.25 &
  \multicolumn{1}{c|}{99.83 \small $\pm$ 0.31} &
  29.62 \small $\pm$ 13.82 &
  82.20 \small $\pm$ 9.46 &
  80.74 \small $\pm$ 7.08 \\
12 &
  76.86 \small $\pm$ 15.23 &
  87.25 \small $\pm$ 12.39 &
  \multicolumn{1}{c|}{99.73 \small $\pm$ 0.36} &
  25.75 \small $\pm$ 26.55 &
  50.11 \small $\pm$ 32.55 &
  80.24 \small $\pm$ 6.63 \\ \bottomrule
\end{tabular}
\label{tab:additional_multilingual}
\end{table}

\begin{table}[h]
\centering
\caption{Imageâ€“caption benchmark results for additional similarity measures, on 4 different model pairs.}
\begin{tabular}{@{}cccccc@{}}
\toprule
Vision Model                                             &                                                       & \multicolumn{2}{c}{ViT} & \multicolumn{2}{c}{ConvNext} \\
\begin{tabular}[c]{@{}c@{}}Language\\ Model\end{tabular} &                                                       & BERT       & GPT2       & BERT          & GPT2         \\ \midrule
\multirow{3}{*}{Random}                                  & \begin{tabular}[c]{@{}c@{}}Dot\\ Product\end{tabular} & 6.32       & 6.32       & 11.39         & 8.86         \\
 & Norm            & 6.32  & 7.59  & 15.19 & 7.59  \\
 & ContraSim\_norm & 100   & 100   & 100   & 100   \\ \midrule
\multirow{3}{*}{FAISS}                                   & \begin{tabular}[c]{@{}c@{}}Dot\\ Product\end{tabular} & 5.06       & 2.53       & 7.59          & 6.32         \\
 & Norm            & 5.06  & 5.06  & 6.32  & 10.12 \\
 & ContraSim\_norm & 93.67 & 98.73 & 81.03 & 93.67 \\ \bottomrule
\end{tabular}
\label{tab:additional_image_caption}
\end{table}

Multilingual benchmark results, summarized in Table \ref{tab:additional_multilingual} show that both dot product and norm achieve better results than CKA, although achieve low results under layer prediction and image-caption benchmarks. This emphasizes the importance of multiple evaluations for similarity measures. Compared to \method{}, both methods achieve lower results. ContraSim\_norm achieves lower results compared to \method{}, under both random and FAISS sampling.

Image-caption benchmark results, summarized in Table \ref{tab:additional_image_caption}, show that under both random sampling and FAISS sampling dot product and norm achieve low accuracy. Under random sampling, ContraSim\_norm achieves perfect accuracy, while using FAISS sampling shows slight degradation compared to \method{}.