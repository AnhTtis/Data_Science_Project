\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{color}
\usepackage{xcolor,colortbl}
\usepackage{dsfont}

\definecolor{urlblue}{RGB}{0,140,250}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=urlblue,
    citecolor=blue,
}

% \usepackage{bm}
% \usepackage{listings}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\definecolor{Gray}{gray}{0.90}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\makeatletter
\newcommand{\algorithmfootnote}[2][\footnotesize]{%
  \let\old@algocf@finish\@algocf@finish% Store algorithm finish macro
  \def\@algocf@finish{\old@algocf@finish% Update finish macro to insert "footnote"
    \leavevmode\rlap{\begin{minipage}{\linewidth}
    #1#2
    \end{minipage}}%
  }%
}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\indic}[1]{\mathds{1}_{\{#1\}}}
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{ Lane Graph as Path: Continuity-preserving Path-wise  Modeling \\ for Online Lane Graph Construction}


\author{
Bencheng Liao$^{1,2,\star,\diamond }$,
Shaoyu Chen$^{2,\star,\diamond}$,
Bo Jiang$^{2,\diamond}$,
Tianheng Cheng$^{2,\diamond}$ \\
Qian Zhang$^3$,
Wenyu Liu$^2$,
Chang Huang$^3$,
Xinggang Wang$^{2,\boxtimes}$ \\
[2mm]
$^1$~\normalsize{Institute of Artificial Intelligence, Huazhong University of Science \& Technology}\\ 
$^2$~\normalsize{School of EIC, Huazhong University of Science \& Technology \quad}\\
$^3$~\normalsize{Horizon Robotics}
\\
\texttt{\small{\{bcliao,shaoyuchen,bjiang,thch,liuwy,xgwang\}@hust.edu.cn}}\\
\texttt{\small{\{qian01.zhang, chang.huang\}@horizon.ai}}\\
\normalsize{
\url{https://github.com/hustvl/LaneGAP}
}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\let\thefootnote\relax\footnotetext{$^\star$ Equal contribution; $^\diamond$ Interns of Horizon Robotics when doing this work; $^\boxtimes$ Corresponding author: \texttt{xgwang@hust.edu.cn}}


%%%%%%%%% ABSTRACT
\begin{abstract}
Online lane graph construction is a  promising but challenging task in autonomous driving. Previous methods usually model the lane graph at the pixel or piece level, and recover the lane graph by pixel-wise or piece-wise connection, which breaks down the continuity of the lane. Human drivers focus on and drive along the continuous and complete paths instead of considering lane pieces. Autonomous vehicles also require path-specific guidance from lane graph for trajectory planning. We argue that the path, which indicates the traffic flow, is the primitive of the lane graph. Motivated by this, we propose to model the lane graph in a novel path-wise manner,  which well preserves the continuity of the lane and encodes traffic information for planning. We present a path-based online lane graph construction method, termed LaneGAP, which end-to-end learns the path and recovers the lane graph via a Path2Graph algorithm. We qualitatively and quantitatively demonstrate the superiority of LaneGAP over conventional pixel-based and piece-based methods.
Abundant visualizations show LaneGAP can cope with diverse traffic conditions. Code and models will be released for facilitating future research. 

\end{abstract}

%%%%%%%%% BODY TEXT
\begin{figure*}[thbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/qualitative_comparison.pdf}
    \caption{\textbf{Qualitative comparison of pixel-wise, piece-wise, and path-wise modeling on the nuScenes val split.} Given the 6 surrounding-view images as input, the proposed path-wise modeling well preserves the continuity, especially for complicated lane topology and under various weather conditions: sunny and cloudy in row 1, 2, 3, rainy in row 4, occlusion and night in row 5.}
    \label{fig:qualitative_comparison}
    \vspace{-4pt}
\end{figure*}
    
\section{Introduction}
The lane graph  contains detailed lane-level traffic information,
and serves to provide path-specific guidance for trajectory planning, \ie, an automated vehicle can trace a path from the lane graph as reliable planning prior.

Lane graph is traditionally constructed with an offline map generation pipeline.  
However, autonomous driving demands a high degree of freshness in lane topology. Hence, online lane graph construction with vehicle-mounted sensors (\eg, cameras and LiDAR) is of great application value.



An intuitive solution is to model the lane graph in a pixel-wise manner and adopt a segmentation-then-vectorization paradigm.
For example,  HDMapNet~\cite{hdmapnet} predicts  a  segmentation map and a direction map on dense bird's-eye-view (BEV) features.
It then extracts the skeleton from the coarse segmentation map with a morphological thinning algorithm and extracts the graph topology  by greedily tracing the single-pixel-width skeleton with the predicted direction map. 
Pixel-wise modeling incurs heuristic and time-consuming post-processing and often fails in complicated topology (see Fig.~\ref{fig:qualitative_comparison} row 4).

Lane graph is also modeled in a piece-wise manner~\cite{stsu}, which splits the lane graph into lane pieces at junction points (\ie, merging points and fork points) and predicts an inter-piece connectivity matrix.  Based on the connectivity, pieces are linked and merged into the lane graph. 
However, piece-wise modeling breaks down the continuity of  the lane.  Aligning the pieces is a challenging problem, especially at complicated road intersections.
And piece-wise modeling generates fragmented pieces which are hard to perceive (\eg, $V_1^{\text{piece}}$ in Fig.~\ref{fig:topo_model} (b)).



We argue that the path is the primitive of the lane graph.
Human drivers focus on and drive  along the continuous 
 and complete paths instead of considering lane pieces. 
Autonomous vehicles also require path-specific guidance from 
 lane graph for trajectory planning.
Continuous paths play an important role in indicating the driving flow, while pixel-wise and piece-wise modeling may fail to merge pixels and pieces into continuous paths (see Fig.~\ref{fig:qualitative_comparison}).

Motivated by this, we propose to model the lane graph in an alternative path-wise manner. 
We decouple the lane graph into a set of continuous paths with a proposed Graph2Path algorithm,  perform path detection through set prediction~\cite{detr, maptr}, 
and extract a fine-grained lane graph with a Path2Graph algorithm.  
Based on this path-wise modeling, we propose an online lane graph construction framework, termed LaneGAP, which  feeds onboard sensor data into an end-to-end network for path detection, and further transforms detected paths into a lane graph.



\begin{figure*}[thbp]
\centering
\includegraphics[width=\linewidth]{figures/topo_model.pdf}
\caption{\textbf{Modeling comparison.} \textbf{(a)} Pixel-wise modeling~\cite{hdmapnet} utilizes a predefined Graph2Pixel algorithm to rasterize the lane graph into a segmentation map and a direction map on dense BEV pixels, and heuristic Pixel2Graph post-processing is needed to recover the lane graph from the predicted segmentation map $V_{\text{pixel}}$ and direction map $D_{\text{pixel}}$ (direction map is not drawn here for simplicity). \textbf{(b)} Piece-wise modeling~\cite{stsu} utilizes a predefined Graph2Piece algorithm to  split the lane graph into a set of pieces and  the connectivity matrix among pieces, and then it merges the predicted pieces $\mathcal{V}_{\text{piece}}$ to the graph with the Piece2Graph algorithm based on predicted connectivity $E_{\text{piece}} $. \textbf{(c)} The proposed path-wise modeling translates the lane graph into continuous and complete paths with a simple predefined Graph2Path algorithm to traverse the graph. 
We perform path detection and adopt a simple Path2Graph algorithm  to recover the lane graph.}
\label{fig:topo_model}
\end{figure*}



We compare LaneGAP with the pixel-wise modeling method HDMapNet~\cite{hdmapnet} and piece-wise modeling method STSU~\cite{stsu} under strictly fair conditions (encoder, model size, training schedule, \etc) on the challenging nuScenes~\cite{nuscenes} dataset, which covers diverse graph topology and traffic conditions. With only camera input, LaneGAP achieves the best graph construction quality both quantitatively (see Tab.\ref{tab:modeling}) and qualitatively (see Fig.~\ref{fig:qualitative_comparison}), while running at the fastest inference speed. 
We further push forward the performance of lane graph construction, by introducing the multi-modality input. 
We believe  modeling the lane graph at the path level is  reasonable and promising. 
We hope LaneGAP can serve as a fundamental module of the self-driving system and boost the development of downstream motion planning.

Our contributions can be summarized as follows:
\begin{itemize}
    \item  
    We propose to model the lane graph in a novel path-wise manner,  which well preserves the continuity of the lane and encodes traffic information for planning.
    \item  Based on our path-wise modeling, we present an online lane graph construction method, termed LaneGAP. LaneGAP end-to-end learns the  path and constructs the lane graph via the designed Path2Graph algorithm.
    \item 
    We  qualitatively and quantitatively demonstrate the superiority of LaneGAP over pixel-based and piece-based methods. 
    LaneGAP can cope with diverse traffic conditions, especially for road intersections with complicated lane topology.
\end{itemize}



\section{Related Work}

\paragraph{Lane detection.}
Lane detection only considers predicting and evaluating the lane divider lines without spatial relation (merging and fork). 
Since most lane detection datasets only provide front-view images, previous lane detection methods~\cite{tabelini2021keep,wang2022keypoint,garnett20193d,lstr,guo2020gen,liu2022learning} were stuck in predicting lines with a small curvature in a limited horizontal FOV. BezierLaneNet~\cite{feng2022rethinking} uses a fully convolutional network to predict Bezier lanes defined with 4 Bezier control points. PersFormer~\cite{chen2022persformer} 
proposes a Transformer-based architecture for spatial transformation and unifies 2D and 3D lane detection. 

\paragraph{Online HD map construction.}
Online HD map construction can be seen as an advanced setting of lane detection, consisting of  lines and polygons with various semantics in the local 360$^\circ$ FOV  perception range of ego-vehicle.
With advanced 2D-to-BEV modules~\cite{Ma2022VisionCentricBP}, previous online HD map construction methods cast it into semantic segmentation task on the transformed BEV features~\cite{polarbev,cvt,bevformer,liu2022bevfusion,liu2022petrv2,lu2022ego3rt}. To build vectorized semantic HD map, HDMapNet~\cite{hdmapnet} follows a segmentation-then-vectorization paradigm.
To achieve end-to-end learning~\cite{detr,deformdetr,yolos}, VectorMapNet~\cite{vectormapnet} 
adopts a coarse-to-fine two-stage pipeline and utilizes an auto-regressive decoder to predict points sequentially.
MapTR~\cite{maptr} proposes unified permutation-equivalent modeling to exploit the undirected nature of semantic HD map and designs a parallel end-to-end framework.


\paragraph{Road graph construction.}
There is a long history of extracting the road graph from remote sensor data (\eg, aerial imagery and satellite imagery). 
Many works~\cite{Mattyus_2017_ICCV, zhou2018d, batra2019improved,He2020Sat2GraphRG,buslaev2018fully}
frame the road graph as a pixel-wise segmentation problem  
and utilizes morphological post-processing methods to extract the road graph. RoadTracer~\cite{bastani2018roadtracer} uses an iterative search process to extract graph topology step by step. Some works~\cite{chu2019neural,Tan_2020_CVPR,xu2021icurb,li2019topological,mi2021hdmapgen} follow this sequential generation paradigm.


\paragraph{Lane graph construction.}
Lane graph is traditionally constructed with an offline pipeline, based on aerial imagery and multi-step training. \cite{laneextract} first trains a segmentation model to extract lanes at non-intersection areas, then it enumerates the pairs among those disjoint lanes and validates the connectivity with a trained turning lane validation model. An extra turning lane segmentation model is trained to complete the lane graph. Recently, STSU~\cite{stsu} focuses on online lane graph construction with the vehicle-mounted monocular front camera, modeling the lane graph as a set of disjoint pieces split by junction points and a set of connections among those pieces. Based on STSU, \cite{can2022topology} designs a network and utilizes minimal circle extracted by time-consuming offline processing to further supervise the network to produce lane graph in a limited-FOV perception range.


Different from the above works, we focus on building the lane graph under the challenging online setting~\cite{hdmapnet,maptr,vectormapnet}, \ie, broad  360$^\circ$ FOV perception based on vehicle-mounted sensors, facing diverse traffic conditions and topology. 
Different from previous modeling, we regard the path as the primitive of the lane graph, and model the lane graph in a novel path-wise manner. 





\section{Method}


\begin{figure}[thbp]
\centering
\includegraphics[width=\linewidth]{figures/pipeline.pdf}
\caption{\textbf{Overview of LaneGAP.} Based on the proposed path-wise modeling, LaneGAP first utilizes the Graph2Path algorithm to translate the graph into a set of paths. Then, an end-to-end network is designed to represent and learn the paths. Finally, a Path2Graph algorithm is used to translate the predicted paths into graph structure with negligible cost. }
\label{fig:framework}
\vspace{-4pt}
\end{figure}



In this section, we first describe how to translate the directed lane graph into a set of directed paths in Sec.~\ref{sec:g2p}. Then we introduce the online path detection framework in Sec.~\ref{sec:lanegap}.  And we describe how to translate  paths back to the lane graph in Sec.~\ref{sec:p2g}. An overview of our method is exhibited in Fig.~\ref{fig:framework}.






\subsection{Graph2Path}
\label{sec:g2p}
\begin{algorithm}[h]
\SetAlgoLined
\DontPrintSemicolon
\SetNoFillComment
\footnotesize
\KwIn{directed lane graph $G$}
\KwOut{directed paths $\mathcal{V}$}

Initialization: $\mathcal{V} \leftarrow \emptyset$, $\text{root vertices set } \Omega_{\text{root}} \leftarrow \emptyset$, $\text{leaf vertices set } \Omega_{\text{leaf}} \leftarrow \emptyset$\;
\For{vertice in $G$.vertices}{
    % \tcc{interpolate each path}
    \If{ $\text{vertice}.\text{in\_degree} == 0$}{
        $\Omega_{\text{root}} \leftarrow \Omega_{\text{root}} \cup \{vertice\}$\;
    }
    \If{ $\text{vertice}.\text{out\_degree} == 0$}{
        $\Omega_{\text{leaf}} \leftarrow \Omega_{\text{leaf}} \cup \{vertice\}$\;
    }
}
\For{root vertice in $\Omega_{\text{root}}$ }{
    \For{leaf vertice in $\Omega_{\text{leaf}}$}{
        $V^{\text{path}} = \textbf{findpath}(root\ vertice, leaf\ vertice, G)$ \;
        \If{ $V^{\text{path}} \text{ is not None}$}{
            $\mathcal{V} \leftarrow \mathcal{V} \cup \{V^{\text{path}}\}$\;
        }
    }
}

Return: $\mathcal{V}$
\caption{Pseudo-code of Graph2Path.}
\label{algo:g2p}
\end{algorithm}


 We propose a simple Graph2Path algorithm to translate the directed lane graph into a set of paths according to the direction and connection information encoded in the lane graph. The pseudo-code of Graph2Path is shown in Alg.~\ref{algo:g2p}.

Given the ground truth Lane graph $G$, which is typically a directed graph in the local map around the ego-vehicle, we first extract the root vertices $\Omega_{\text{root}}$ and leaf vertices $\Omega_{\text{leaf}}$ based on the indegree and outdegree of vertice. Then we pair the root vertices and leaf vertices (line 2 to 9 in Alg.~\ref{algo:g2p}.). For each vertice pair, a depth-first-search (DFS) algorithm is used to find the valid path $V^{\text{path}}$ from the root vertice to the leaf vertice (line 10 to 17 in Alg.~\ref{algo:g2p}.). Finally, we can translate the ground truth directed lane graph $G$ to a set of directed paths $\mathcal{V}_{\rm path} = \{V_i^{\rm path}\}_{i=1}^{M}$, where $M$ is the number of ground truth paths.


\subsection{Path Representation and Learning}
\label{sec:lanegap}


Inspired by the advanced set detection methods~\cite{detr,deformdetr}, we propose an end-to-end network, LaneGAP, to predict all paths simultaneously in a single stage, as illustrated in Fig.~\ref{fig:framework}. Our network consists of an encoder that encodes the features  from the onboard sensor data, and a query-based Transformer decoder that performs set detection by decoding a set of paths $\hat{\mathcal{V}}_{\rm path} = \{ \hat{V}_i^{\rm path}\}_{i=1}^N$ from the encoded features. To parameterize the path, we utilize two types of representations, Polyline and Bezier, where Polyline offers high flexibility in describing the path, while Bezier provides a smoother representation.


\paragraph{Polyline representation.}
Polyline representation models the arbitrary directed path as an ordered set of $N_{\text{p}}$ points  $V_{\text{poly}}^{\text{path}} = \{p_j \in \mathbb{R}^2 | j = 0,1,2,..., N_{\text{p}}-1\}$. We directly regress polyline points and utilize deformable attention~\cite{deformdetr} to exploit the local information along each Polyline path, where the keys and values are the local features along the Polyline path.

\paragraph{Bezier representation.}
Bezier representation models the directed path as an ordered set of $N_{\text{b}}$ control points $V_{\text{Bezier}}^{\text{path}} = \{b_j \in \mathbb{R}^2 | j = 0,1,2,..., N_{\text{b}}-1\}$.
Bezier is a parametric curve, where the point $B$ on the line  can be  calculated by the weighted sum of control points $V_{\text{Bezier}}^{\text{path}}$:
\begin{equation}
\begin{gathered}
B = \sum_{j=0}^{N_{\text{b}}-1} C_{N_{\text{b}}-1}^{j}t^{j}(1-t)^{N_{\text{b}}-1-j} b_j, 0 \leq t \leq 1.
\end{gathered}   
\label{eq:1}
\end{equation}
Given the Bezier control points set $V_{\text{Bezier}}^{\text{path}}$ and the sampled interval set $T=\{ t_k \in \mathbb{R}|0 \leq t_k \leq 1, k=0,1,2,...,K-1 \}$, we can calculate the curve $\mathcal{B} = \{B_{k}\in \mathbb{R}^2|0,1,2,...,K-1 \}$ with matrix multiplication:
\begin{equation}
\begin{gathered}
\mathcal{B} = \Gamma \times V_{\text{Bezier}}^{\text{path}},
\end{gathered}
\label{eq:2}
\end{equation}
where weight matrix $\Gamma$ is a $K\times N_{\text{b}}$ matrix and $\Gamma(k,j) = C_{N_{\text{b}}-1}^j t_{k}^{j} (1-t_k)^{N_{\text{b}}-1-j}$. For Beizer representation, we use the same network as Polyline representation to regress Bezier control points directly. To enable exploiting the local information along the Bezier path with offline control points, we sample the Bezier path to get online points $\mathcal{B}$ based on Eq.~\ref{eq:2} and perform deformable attention, where the keys and values are the local features around the sampled points $\mathcal{B}$ along the Bezier path. We denote this design as Bezier deformable attention, which enables the Transformer decoder to aggregate features along the Bezier path.

\paragraph{Learning.}
With the above path representation, we can cast the predicted path into an ordered set of points with a fixed number $N_v$ of points on the path, where $\hat{V}_i^{\text{path}} = \{\hat{v}_j \in \mathbb{R}^2|j=0,1,2,...,N_v-1\}$. We modify the bipartite matching loss used in~\cite{detr} to fit in the path detection setting:
\begin{equation}
\begin{aligned}
\mathcal{L}_{\rm bipartite}(\hat{\mathcal{V}}_{\text{path}},\mathcal{V}_{\text{path}}) =& \sum_{i=1}^{N}[\mathcal{L}_{\rm{Focal}}(\hat{p}_{{\hat{\sigma}}(i)}, c_i) + \\
&\indic{c_i \neq \varnothing}\mathcal{L}_{\rm path}(\hat{V}_{\hat{\sigma}(i)}^{\rm path}, V_i^{\rm path})],\\
\mathcal{L}_{\rm path}(\hat{V}_{\hat{\sigma}(i)}^{\rm path}, V_i^{\rm path}) =& \sum_{j=0}^{N_v-1}L_1(\hat{v}_j,v_j),
\end{aligned}    
\label{eq:3}
\end{equation}
where $\hat{\sigma}$ is the optimal assignment between a set of predicted paths and a set of ground truth paths computed by the Hungarian algorithm, $c_i$ is the target class label, and $\mathcal{L}_{\rm{Focal}}(\hat{p}_{{\hat{\sigma}}(i)}, c_i)$ is the classification loss defined in~\cite{focal}. A $L_1$ loss is utilized between the matched predicted path $\hat{V}_{\hat{\sigma}(i)}^{\rm path}$ and sampled ground truth path $V_i^{\rm path}$.

\subsection{Path2Graph}
\label{sec:p2g}

\begin{algorithm}[h]
\SetAlgoLined
\DontPrintSemicolon
\SetNoFillComment
\footnotesize
\KwIn{a set of paths $\mathcal{V}=\{ V_i^{\text{path}} \} $}
\KwOut{directed  lane graph $G$}

Initialization: $G=\text{DirGraph}()$\;
\For{$V^{\text{path}}$ in $\mathcal{V}$}{
    $V=\{v_j\} \leftarrow \textbf{discretize\_path\_to\_vertex\_sequence}(V^{\text{path}})$\;
    \For{ $v_j$ in $V$}{
        \If{ $v_j\ \text{is\ the\ last\ vertice\ of\ }V$}{
            $G.\textbf{add\_vertice}(v_j)$ \;
            \textbf{break}\;
        }
        $G.\textbf{add\_vertice}(v_j)$ \;
        $G.\textbf{add\_edge}(v_j,v_{j+1})$ \;
    }

}
$G=\textbf{merge\_vertices\_by\_overlap}(G)$


Return: $G$
\caption{Pseudo-code of Path2Graph.}
\algorithmfootnote{The for loop here can be implemented in a parallel paradigm. $\textbf{merge\_vertices\_by\_overlap}$ merges the overlapped vertices of different paths to reduce the redundancy of $G$ without affecting the accuracy of graph topology.}
\label{algo:p2g}
\end{algorithm}

The predicted continuous paths encode sufficient traffic information and can be directly applied to downstream motion planning. 
To further  recover the graph structure of lane topology and extract merging and fork information,
we convert the predicted paths $\hat{\mathcal{V}} = \{\hat{V}_i^{\text{path}}\}$ into a directed lane graph $\hat{G}$  with a  designed Path2Graph algorithm as shown in Alg.~\ref{algo:p2g}. 



We discretize the path into point sequences $V=\{v_j\}$. The discretized points are regarded as vertices and the adjacent relation
between successive points is regarded as edges of vertices. We add these vertices and edges to the directed graph (line 4 to 11 in Alg.~\ref{algo:p2g}). 
The vertices registered in the directed graph $G$ on one path may have spatial overlapping with vertices on other paths. To remove the redundancy of $G$, we merge the overlapped vertices into one vertex (line 13 in Alg.~\ref{algo:p2g}).


\section{Experiments}
\subsection{Dataset}
We benchmark on the challenging nuScenes~\cite{nuscenes} dataset consisting of 1000 sequences. Each sequence is sampled in 2Hz frame rate and provides LiDAR point cloud and  RGB images from 6 surrounding cameras, which covers 360$^\circ$ horizontal FOV of the ego-vehicle. The dataset provides the lane graph in the form of lane centerline and covers diverse online driving conditions (\eg, day, night, cloudy, rainy, and occlusion). For the online setting of lane graph construction, we set the perception ranges as $[-15.0m, 15.0m]$ for the $X$-axis and $[-30.0m, 30.0m]$ for the $Y$-axis and preprocess the dataset following~\cite{hdmapnet,vectormapnet,maptr}. We train on the nuScenes train set and evaluate on the val set.  The experiments are conducted using 6 surrounding-view images by default.

\subsection{Metrics}
To evaluate the quality of graph topology, we adapt the TOPO metric~\cite{laneextract} to measure the correctness of the overall directed graph construction. Additionally, to emphasize the quality of the subgraph around the junction points, we introduce a new metric, Junction TOPO, which specifically evaluates the accuracy of the local directed graph formed by traversing around the junction points on the directed lane graph.

\paragraph{TOPO metric.}
% \paragraph{TOPO metric.}
Given the predicted directed lane graph $\hat{G}$ and ground truth directed lane graph $G$, we interpolate them so that the distances between any two connected vertices are $0.15m$, and get predicted directed lane graph $\hat{G} = (\hat{V},\hat{E})$ and ground truth directed lane graph $G=(V,E)$ where $\hat{V},V$ are the sets of interpolated vertices and $\hat{E}, E $ are the sets of edges among vertices encoding direction and connection. For $\hat{V}$ and $V$,  A pair of vertices is considered a candidate match if the distance between the two vertices is less than $0.45m$. And  we utilize maximal one-to-one matching among those candidate pairs to find final matched vertices $P_{\text{pair}}=\{(\hat{v}, v)_{i}\}_{i=1}^{N_{\text{pair}}}$, then we traverse the directed graph around the paired vertices $\hat{v}$ and $v$ for less than $7.5m$ to get subgraphs $\hat{S}_{\hat{v}}$ and $S_v$  on $\hat{G}$ and G. We compute the precision $\text{Pre}(\hat{S}_{\hat{v}}, S_v) = \frac{N_{\text{subpair}}}{|\hat{S}_{\hat{v}}|}$and recall $\text{Rec}(\hat{S}_{\hat{v}},S_v) =  \frac{N_{\text{subpair}}}{|S_{v}|}$ between the vertices of predicted subgraph $\hat{S}_{\hat{v}}$ and the vertices of ground truth subgraph $S_{v}$, where the matching part follows the previous procedure with $0.45m$ threshold. Finally, we report the TOPO precision and recall defined as:
\begin{equation}
\begin{gathered}
\text{Precision}_{\text{TOPO}} = \frac{\sum_{i=1}^{N_{\text{pair}}} \text{Pre}(\hat{S}_{\hat{v}}, S_v)}{|\hat{V}|}, \\
\text{Recall}_{\text{TOPO}} = \frac{\sum_{i=1}^{N_{\text{pair}}} \text{Rec}(\hat{S}_{\hat{v}}, S_v)}{|V|}.
\end{gathered}    
% \label{eq:2}
\end{equation}


\paragraph{Junction TOPO metric.}
The TOPO metric focuses on the topology correctness of the overall directed lane graph, it does not highlight the correctness of the subgraph formed by traversing from the junction points, which plays a key role in determining the driving choices across different lanes. To bridge this gap, we propose the Junction TOPO metric, which only reports the precision and recall of the junction subgraph. Given the $N_{\text{junction}}$ junction points of the ground truth lane graph, we get pairs of subgraphs $( \hat{S}_{\text{junction}}, S_{\text{junction}})$ by traversing the directed graphs $\hat{G}$ and $G$ less than $7.5m$ from junction point. For each subgraph pair, we calculate the precision $\text{Pre}(\hat{S}_{\text{junction}}, S_{\text{junction}})$ and recall $ \text{Rec}(\hat{S}_{\text{junction}}, S_{\text{junction}})$.


\paragraph{Undirected versions.}
The above metrics calculate the precision and recall by traversing the directed graph $\hat{G}$ and $G$, ignoring the predecessor vertices. To evaluate the complete connections, we turn the directed graphs into undirected graphs and repeat the calculation defined above for two metrics.



\subsection{Baselines}


\paragraph{Implementation details.}
We compare our path-wsie modeling with pixel-wise modeling  and piece-wise modeling. For a strictly fair comparison, all the methods are trained and tested with 6 surrounding camera images as input and use the same encoder consisting of ResNet50~\cite{resnet} and GKT~\cite{gkt} to transform  the input RGB images to the BEV features.  Because the piece-wise modeling and path-wise modeling predict the localization with regression branch, to exclude the quantization error brought by classification on dense pixels,  the output resolution of the UNet-shaped~\cite{ronneberger2015u} segmentation model used in pixel-wise modeling is set to $400 \times 200$, which means the grid size is $0.15m$ (the perception range is $60m\times30m$) the same as the graph interpolation size. All models are controlled in comparable parameter sizes and trained for 110 epochs to ensure convergence. We trained all the experiments on 8 NVIDIA GeForce RTX 3090 GPUs with a total batch size of 32 (containing 6 view images). The only differences lie in the specialized decoder designed for respective modelings.

\paragraph{Pixel-wise modeling.}
Following HDMapNet~\cite{hdmapnet}, we adopt an UNet structure with a ResNet18 encoder to output the binarized segmentation map and direction map with two branches. Meanwhile, a cross-entropy loss is applied to the segmentation map and direction map. However, we find training the direction map by applying $L_2$ loss on (-1,1) normalized direction yields better performance than the cross-entropy loss on  discretized direction~\cite{laneextract,zhou2018d}. Thus, in our comparison, we train the direction branch of the segmentation model with $L_2$ loss on a normalized direction map.

\paragraph{Piece-wise modeling.}
We implement the piece-wise modeling by strictly following STSU~\cite{stsu}. We find the original piece detection decoder shows inferior performance compared with ours.  In order to fairly compare the modeling and  exclude the influence of network design, we adopt the same architecture as ours. The only difference is that it predicts connectivity classification and  pieces instead of paths. $L_1$ loss is applied to the matched pieces and cross-entropy loss is applied to the connectivity. 

\paragraph{Path-wise modeling.} We utilize deformable attention~\cite{deformdetr} as the cross attention of the Transformer and use the 30-point Polyline to represent the path and exploit the features along the path with resort to deformable attention as stated in Sec.~\ref{sec:lanegap}. We stack 6 decoders and directly regress $(0,1)$ normalized coordinates. $L_1$ loss is applied to the matched paths.

\paragraph{Path-wise modeling with auxiliary segmentation supervision.}
We further introduce a BEV segmentation branch (like pixel-wise modeling) as auxiliary supervision to enhance the BEV representation.
Note that this  auxiliary branch doesn't bring any computation budget in the inference phase.


\begin{table*}[ht!]
    \setlength{\tabcolsep}{3pt}
    \centering
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{l  c  c  c  c  c  c  c  c  c  c  c  c c  c   c}
        % \hline
        \toprule
        \multirow{3}{*}{Modeling} & \multicolumn{6}{c}{Directed Graph} & \multicolumn{6}{c}{Undirected Graph} & \multirow{3}{*}{Param.} & \multirow{3}{*}{FPS$_{\text{net}}$} &\cellcolor{gray!20}\\ 
        
        & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} 
         & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO}  & & & \cellcolor{gray!20}\\ 
        % \cline{2-7}
        & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &  \cellcolor{gray!20}$F_1$
        & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &  \cellcolor{gray!20}$F_1$ & & & \multirow{-3}{*}{\cellcolor{gray!20}FPS}\\
        % \hline
        \midrule
        Pixel-wise~\cite{hdmapnet}  & 0.594 & 0.442 & \cellcolor{gray!20}0.507 & 0.608 & 0.443 & \cellcolor{gray!20}0.513 & 0.526 & 0.445 & \cellcolor{gray!20}0.482 & 0.563 & 0.447 &  \cellcolor{gray!20}0.498
        & 35.4M & 17.8 & \cellcolor{gray!20}6.8\\
        % \hline
        Piece-wise~\cite{stsu} & 0.540 & 0.459 & \cellcolor{gray!20}0.496 & 0.504 & 0.432 &  \cellcolor{gray!20}0.465 & 0.493 & 0.465 &  \cellcolor{gray!20}0.478 & 0.487 & 0.429 &  \cellcolor{gray!20}0.456
        & 36.0M & 15.6 & \cellcolor{gray!20}15.0\\
        \midrule
        Path-wise  & 0.562 &  0.514 & \cellcolor{gray!20}0.537 & 0.536 & 0.495 & \cellcolor{gray!20}0.515 & 0.518 & 0.495 & \cellcolor{gray!20}0.506 & 0.515 & 0.486 & \cellcolor{gray!20}0.500
         & 35.9M &16.5& \cellcolor{gray!20}15.6\\
        % \hline
        Path-wise$^\dagger$ & 0.591 & 0.539 & \cellcolor{gray!20}0.564 & 0.547 & 0.511 & \cellcolor{gray!20}0.529 & 0.543 & 0.518 & \cellcolor{gray!20}0.530 & 0.534 & 0.496 & \cellcolor{gray!20}0.514 & 35.9M & 16.5 & \cellcolor{gray!20}15.6 \\
        \bottomrule
    \end{tabular}}
\vspace{2pt}
\caption{\textbf{Quantitative comparison of lane graph modeling under fair conditions} (the same encoder, comparable model size, the same training schedule, and vision-only modality). FPS$_{\text{net}}$ and FPS of all the experiments are measured on the same machine with one NVIDIA Geforce RTX 3090 GPU and one 24-core AMD EPYC 7402 2.8 GHz CPU, where FPS$_{\text{net}}$ is benchmarked with only network forward. $\dagger$ denotes introducing a BEV segmentation branch as auxiliary supervision. Path-wise modeling achieves the best lane graph construction quality and speed. }
\label{tab:modeling}
\end{table*}

\subsection{Quantitative Comparision}
Tab.~\ref{tab:modeling} compares path-wise modeling with pixel-wise modeling and piece-wise modeling \wrt their accuracy, model size, FPS$_{\text{net}}$, FPS. All the FPS$_{\text{net}}$ and FPS are measured on the same machine with one NVIDIA Geforce RTX 3090 GPU and one 24-core AMD EPYC 7402 2.8 GHz CPU, where FPS$_{\text{net}}$ is benchmarked with only network forward.

\paragraph{Highlights.} Path-wise modeling achieves  superior accuracy on the both subgraph around the junction points and the overall graph, while running at the fastest inference speed. The designed Path2Graph algorithm has negligible cost in translating predicted paths into directed lane graph (from 16.5 FPS to 15.6 FPS). With the auxiliary segmentation supervision,  path-wise modeling further pushes forward the lane graph construction quality.



\paragraph{Path-wise \vs pixel-wise.}
Thanks to the high-resolution output of the segmentation model, pixel-wise modeling exhibits comparable construction quality for overall lane graph construction on the TOPO metric. Nevertheless, the coarse rasterized segmentation output and prone-to-fail post-processing make it hard to distinguish the fine-grained subgraph around the junction points. As shown in Tab.~\ref{tab:modeling}, Path-wise modeling demonstrates obvious superiority on the subgraph around the junction points (3\% higher $F_1$ for directed graph and 2.4\% $F_1$ for undirected graph), while being 2x faster.

\paragraph{Path-wise \vs piece-wise.} Path-wise modeling outperforms piece-wise modeling on all metrics (precision, recall, parameter size, FPS), which validates the effectiveness of our proposed continuity-preserving modeling.

\paragraph{Path-wise modeling with auxiliary segmentation supervision.}
The auxiliary segmentation supervision significantly boosts both  Junction TOPO  and TOPO metric without adding inference cost.

\subsection{Qualitative Comparison}
With the trained models in Tab.~\ref{tab:modeling}, Fig.~\ref{fig:qualitative_comparison} compares path-wise modeling with pixel-wise modeling and piece-wise modeling on complicated lane graph \wrt diverse traffic conditions: sunny and cloudy in Fig.~\ref{fig:qualitative_comparison} row 1, 2, 3, rainy in Fig.~\ref{fig:qualitative_comparison} row 4, occlusion and night in Fig.~\ref{fig:qualitative_comparison} row 5.

\paragraph{Highlights.} Path-wise modeling demonstrates better lane graph construction quality than pixel-wise and piece-wise modeling on extremely challenging lane graphs, well preserving the continuity of lane.

\paragraph{Path-wise \vs pixel-wise.} As shown in Fig.~\ref{fig:qualitative_comparison} row 2 and 4, the segmentation model utilized in pixel-wise modeling can hardly distinguish the fine-grained topology around the sub-graph of junction points, and the post-processing is prone to fail to generate a decent vectorized lane graph. Thanks to our rational modeling which treats each path as a whole for learning, we can capture the fine-grained topology with severe overlap and preserve the continuity of the lane.

\paragraph{Path-wise \vs piece-wise.} The piece-wise modeling depends on the accuracy of both piece detection and connectivity prediction. As shown in Fig.~\ref{fig:qualitative_comparison} row 1, 2, and 4, the piece-wise modeling method tends to produce wrong connections or jagged pieces, which can hardly be used in real applications. Path-wise modeling method can capture the information along the complete path, leading to better continuity and results.



\subsection{Ablation Study}
We ablate the design choices with a 24-epoch training schedule of path-wise modeling with only 6 surrounding-view images as input by default without specification. And we report Junction TOPO and TOPO only on the directed lane graph.


\vspace{-5pt}
\paragraph{Modality.}
Tab.~\ref{tab:modality} shows that LiDAR modality builds a more accurate lane graph than vision modality (6.6\% higher $F_1$ of Junction TOPO and 5.2\% higher $F_1$ of TOPO). Fusing them further boosts performance.

\begin{table}[ht!]
    \setlength{\tabcolsep}{3pt}
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l  c  c  c  c  c  c }
        \toprule
        \multirow{2}{*}{Modality} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
        % \cline{2-7}
         & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
        \midrule
        Vision-only & 0.544 & 0.516 &\cellcolor{gray!20}0.529 & 0.421 & 0.484 &\cellcolor{gray!20}0.450 \\

        LiDAR-only &  0.600 & 0.589 &\cellcolor{gray!20}0.595 & 0.460 & 0.553 &\cellcolor{gray!20}0.502 \\

        Vision \& LiDAR  &0.625 & 0.597 &\cellcolor{gray!20}0.611 & 0.499 & 0.558 &\cellcolor{gray!20}0.527 \\
        \bottomrule
    \end{tabular}}
\vspace{2pt}
\caption{\textbf{Ablation for modality.} Lidar modality shows better performance than vision modality. Fusing the lidar modality and vision modality can further boost the accuracy.}
\label{tab:modality}
\end{table}


\paragraph{Training schedule.}
Tab.~\ref{tab:schedule} shows that adding more epochs mainly increases the TOPO metric and benefits little Junction TOPO, especially the recall of Junction TOPO. The 110-epoch trained multi-modality experiment further pushes the performance of lane graph construction.



\begin{table}[ht!]
    \setlength{\tabcolsep}{3pt}
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l c  c  c  c  c  c  c }
        \toprule
        \multirow{2}{*}{Modality} & \multirow{3}{*}{Epoch} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
        % \cline{2-7}
        & & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
        \midrule
        Vision-only &24 & 0.544 & 0.516 &\cellcolor{gray!20}0.529 & 0.421 & 0.484 &\cellcolor{gray!20}0.450 \\
        Vision-only &110 & 0.562 & 0.514 &\cellcolor{gray!20}0.537 & 0.536 & 0.495 &\cellcolor{gray!20}0.515 \\
        \midrule
        Vision \& LiDAR  & 24&0.625 & 0.597 &\cellcolor{gray!20}0.611 & 0.499 & 0.558 &\cellcolor{gray!20}0.527 \\
        Vision \& LiDAR  & 110&0.666 & 0.597 & \cellcolor{gray!20}0.629 & 0.616 & 0.574 & \cellcolor{gray!20}0.594 \\
        \bottomrule
    \end{tabular}}
\vspace{2pt}
\caption{\textbf{Ablation for training schedule.} The 110-epoch trained multi-modality experiment further pushes the construction quality of lane graph. And it shows that the longer training schedule mainly benefits the TOPO metric and has little benefit on Junction TOPO, especially the recall of Junction TOPO.}
\label{tab:schedule}
\end{table}



\paragraph{Polyline path representation.}
As shown in Tab.~\ref{tab:polyline}, the performance increases with adding more points for Polyline modeling under a 24-epoch schedule. By comparing the 24-epoch 40-point Polyline experiment with the 110-epoch 30-point Polyline experiment, we can find that directly adding Polyline points can benefit the Junction TOPO more than increasing the training time ( 54.6\% $F_1$ \vs 53.7\% $F_1$ ).

\begin{table}[ht!]
    \setlength{\tabcolsep}{3pt}
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l c c  c  c  c  c  c }
        % \hline
        \toprule
        \multirow{2}{*}{\shortstack[l]{Polyline\\Points}} &  \multirow{2}{*}{Epoch}&\multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
        % \cline{2-7}
        & & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
        \midrule
        % \hline
        10  & 24 & 0.538 & 0.505 & \cellcolor{gray!20}0.521 & 0.410 & 0.471 & \cellcolor{gray!20}0.438  \\
        % \hline
        30  & 24 & 0.544 & 0.516 &\cellcolor{gray!20}0.529 & 0.421 & 0.484 & \cellcolor{gray!20}0.450  \\
        30 &110 & 0.562 & 0.514 &\cellcolor{gray!20}0.537 & 0.536 & 0.495 &\cellcolor{gray!20}0.515 \\
        % \hline
        40  & 24 & 0.556 & 0.537 &\cellcolor{gray!20}0.546 & 0.426 & 0.504 & \cellcolor{gray!20}0.462  \\
        % \hline
        \bottomrule
    \end{tabular}}
\vspace{2pt}
\caption{\textbf{Ablation for Polyline path modeling.}  Increasing the number of Polyline points for path representation leads to continuous improvement.}
\label{tab:polyline}
\end{table}

\vspace{-3pt}
\paragraph{Bezier path representation.}
Tab.~\ref{tab:bezier} shows that adding the number of control points from 3 to 5 improves 2.5\% $F_1$ of Junction TOPO and 3.9\% $F_1$ of TOPO, indicating too few Bezier control points cannot accurately describe the arbitrary shape in the broad perception. While adding the number of control points from 5 to 10 incurs an accuracy drop. 


\begin{table}[ht!]
    \setlength{\tabcolsep}{3pt}
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l  c  c  c  c  c  c }
        % \hline
        \toprule
        \multirow{2}{*} {\shortstack[l]{Bezier \\ Points}} &  \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
        % \cline{2-7}
        & Prec. & Rec. &\cellcolor{gray!20}$F_1$  & Prec. & Rec. & \cellcolor{gray!20}$F_1$  \\
        % \hline
        \midrule
        3  & 0.477 & 0.483 & \cellcolor{gray!20}0.480 & 0.308 & 0.442 & \cellcolor{gray!20}0.363 \\
        % \hline
        5  & 0.503 & 0.506 &\cellcolor{gray!20}0.505 &0.351 & 0.471 & \cellcolor{gray!20}0.402  \\
        10  & 0.486 & 0.505 & \cellcolor{gray!20}0.495 & 0.317 & 0.470 & \cellcolor{gray!20}0.379  \\
        % \hline
        \bottomrule
    \end{tabular}}
\vspace{2pt}
\caption{\textbf{Ablation for Bezier path representation.} Too few Bezier control points cannot describe the shape and too many Bezier control points deteriorate the performance. }
\label{tab:bezier}
\end{table}











\section{Conclusion}
In this work, we present an online lane graph construction method LaneGAP based on novel path-wise modeling. We qualitatively and quantitatively demonstrate the superiority of LaneGAP over pixel-based and piece-based methods. 
LaneGAP  can serve as a fundamental module of the self-driving 
system and facilitate downstream motion prediction and planning, which we leave as future work.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}