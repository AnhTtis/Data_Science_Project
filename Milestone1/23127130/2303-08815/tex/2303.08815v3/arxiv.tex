\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version
% \usepackage[review,year=2024,ID=6068]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{eccv}

% OPTIONAL: Un-comment the following line for a version which is easier to read
% on small portrait-orientation screens (e.g., mobile phones, or beside other windows)
%\usepackage[mobile]{eccv}


% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% ---------------------------------------------------------------
% Hyperref package

% It is strongly recommended to use hyperref, especially for the review version.
% Please disable hyperref *only* if you encounter grave issues.
% hyperref with option pagebackref eases the reviewers' job, but should be disabled for the final version.
%
% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

% TODO FINAL: Comment out the following line for the camera-ready version
% \usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{hyperref}

% Support for ORCID icon
\usepackage{orcidlink}

% \usepackage{color}
\usepackage{xcolor,colortbl}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{pifont}
\usepackage[misc]{ifsym}


% \usepackage{wrapfig}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\makeatletter
\newcommand{\algorithmfootnote}[2][\footnotesize]{%
  \let\old@algocf@finish\@algocf@finish% Store algorithm finish macro
  \def\@algocf@finish{\old@algocf@finish% Update finish macro to insert "footnote"
    \leavevmode\rlap{\begin{minipage}{\linewidth}
    #1#2
    \end{minipage}}%
  }%
}

\newcommand{\indic}[1]{\mathds{1}_{\{#1\}}}
\newcommand{\boldparagraph}[1]{\vspace{0.cm}\noindent{\bf #1}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\begin{document}

% ---------------------------------------------------------------
% TODO REVIEW: Replace with your title
\title{Lane Graph as Path: \\Continuity-preserving Path-wise  Modeling for Online Lane Graph Construction} 

% TODO REVIEW: If the paper title is too long for the running head, you can set
% an abbreviated paper title here. If not, comment out.
\titlerunning{LaneGAP}

% TODO FINAL: Replace with your author list. 
% Include the authors' OCRID for the camera-ready version, if at all possible.
\author{Bencheng Liao\inst{1,2,\star} \and
Shaoyu Chen\inst{2,\star} \and
Bo Jiang\inst{2} \and
Tianheng Cheng\inst{2} \and
Qian Zhang\inst{3} \and
Wenyu Liu\inst{2} \and
Chang Huang\inst{3} \and
Xinggang Wang\inst{2,\textrm{\Letter}}
}

% TODO FINAL: Replace with an abbreviated list of authors.
\authorrunning{Liao et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list.
\institute{Institute of Artificial Intelligence, Huazhong University of Science \& Technology \and
School of EIC, Huazhong University of Science \& Technology \and
Horizon Robotics}

\maketitle
\let\thefootnote\relax\footnotetext{$^\star$ Equal contribution; $^\textrm{\Letter}$ Corresponding author: \texttt{xgwang@hust.edu.cn}}
\begin{abstract}
Online lane graph construction is a  promising but challenging task in autonomous driving. Previous methods usually model the lane graph at the pixel or piece level, and recover the lane graph by pixel-wise or piece-wise connection, which breaks down the continuity of the lane and results in suboptimal performance. Human drivers focus on and drive along the continuous and complete paths instead of considering lane pieces. Autonomous vehicles also require path-specific guidance from lane graph for trajectory planning. We argue that the path, which indicates the traffic flow, is the primitive of the lane graph. Motivated by this, we propose to model the lane graph in a novel path-wise manner,  which well preserves the continuity of the lane and encodes traffic information for planning. We present a path-based online lane graph construction method, termed LaneGAP, which end-to-end learns the path and recovers the lane graph via a Path2Graph algorithm. We qualitatively and quantitatively demonstrate the superior accuracy and efficiency of LaneGAP over conventional pixel-based and piece-based methods on the challenging nuScenes and Argoverse2 datasets under controllable and fair conditions. Compared to the recent state-of-the-art piece-wise method TopoNet on the OpenLane-V2 dataset, LaneGAP still outperforms by 1.6 mIoU, further validating the effectiveness of path-wise modeling.
Abundant visualizations in the supplementary material show LaneGAP can cope with diverse traffic conditions.
Code is released at \url{https://github.com/hustvl/LaneGAP}.
   
\keywords{Autonomous Driving \and Online Lane Graph Construction \and Path-wise Modeling}
\end{abstract}

\input{sec/1_intro}
\input{sec/2_relatedwork}
\input{sec/3_method}
\input{sec/4_experiment}
\input{sec/5_conclusion}

\section*{Acknowledgement}
This work was partly supported by the National Natural Science Foundation of China (NSFC 62276108).

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{main}

\clearpage
\appendix



\section{Training Settings for Quantitative Comparison}
In Tab.~\ref{tab:nusc-config} and Tab.~\ref{tab:av2-config}, we summarize the detailed training settings for the quantitative comparison in the main paper. We use the same settings under the long schedule.


\vspace{-10pt}
\begin{table*}[h!]
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3pt}
        % \tablestyle{2pt}{1.1}
        \small
        \scalebox{0.62}{
        \begin{tabular}{l  c  c  c  }
            \toprule
            Config & Pixel-wise & Piece-wise & Path-wise \\
            \midrule
            optimizer & \multicolumn{3}{c}{AdamW~\cite{loshchilov2017adamw}}\\
            optimizer hyper-parameter & \multicolumn{3}{c}{$\beta_1,\beta_2,\epsilon =  0.9, 0.999,$ 1e-8} \\
            gradient clip norm type & \multicolumn{3}{c}{2} \\
            gradient clip max norm & \multicolumn{3}{c}{35} \\
            learning rate & \multicolumn{3}{c}{6e-4}\\ 
            learning rate schedule  & \multicolumn{3}{c}{cosine decay~\cite{loshchilov2016cosineanneal}} \\
            warmup iteration & \multicolumn{3}{c}{500} \\
            weight decay & \multicolumn{3}{c}{0.01} \\
            \midrule
            input resolution & \multicolumn{3}{c}{$6\times900\times1600$} \\
            image resize ratio & \multicolumn{3}{c}{0.5} \\
            batch size & \multicolumn{3}{c}{32} \\
            training epochs & \multicolumn{3}{c}{110} \\
            BEV grid size & \multicolumn{3}{c}{$0.3m$} \\
            % peception range & \multicolumn{3}{c}{}
                \bottomrule
            \end{tabular}}
        \caption{\textbf{Detailed training settings on nuScenes dataset.} We use the same training settings for quantitative comparison. }
        \label{tab:nusc-config}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3pt}
        \small
        \scalebox{0.62}{
            \begin{tabular}{l  c  c  c  }
                \toprule
                Config & Pixel-wise & Piece-wise & Path-wise \\
                \midrule
                optimizer & \multicolumn{3}{c}{AdamW~\cite{loshchilov2017adamw}}\\
                optimizer hyper-parameter & \multicolumn{3}{c}{$\beta_1,\beta_2,\epsilon =  0.9, 0.999,$ 1e-8} \\
                gradient clip norm type & \multicolumn{3}{c}{2} \\
                gradient clip max norm & \multicolumn{3}{c}{35} \\
                learning rate & \multicolumn{3}{c}{6e-4}\\ 
                learning rate schedule  & \multicolumn{3}{c}{cosine decay~\cite{loshchilov2016cosineanneal}} \\
                warmup iteration & \multicolumn{3}{c}{500} \\
                weight decay & \multicolumn{3}{c}{0.01} \\
                \midrule
                input resolution & \multicolumn{3}{c}{$7\times2048\times2048$} \\
                image resize ratio & \multicolumn{3}{c}{0.3} \\
                batch size & \multicolumn{3}{c}{32} \\
                training epochs & \multicolumn{3}{c}{24} \\
                BEV grid size & \multicolumn{3}{c}{$0.3m$} \\
                % peception range & \multicolumn{3}{c}{}
                \bottomrule
            \end{tabular}}
            \caption{\textbf{Detailed training settings on Argoverse2 dataset.} We use the same training settings for quantitative comparison. }
            \label{tab:av2-config}
\end{minipage}
\vspace{-30pt}
\end{table*}


\section{More Ablation Studies}

\boldparagraph{Modality.}
Tab.~\ref{tab:modality} shows that LiDAR modality builds a more accurate lane graph than vision modality (6.1\% higher $F_1$ of Junction TOPO and 4.8\% higher $F_1$ of TOPO). Fusing them further boosts performance.

\begin{table}[ht!]
    % \setlength{\tabcolsep}{3pt}
    \centering
    % \resizebox{0.9\textwidth}{!}{
        \begin{tabular}{l  c  c  c  c  c  c c c }
            \toprule
            \multirow{2}{*}{Modality} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} & \multirow{2}{*}{Param.} & \multirow{2}{*}{FPS}\\ 
            % \cline{2-7}
             & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
            \midrule
            Vision-only &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461
            & 35.9M & 15.6\\
    
            LiDAR-only &  0.608 & 0.594 &\cellcolor{gray!20}0.600 & 0.467 & 0.559 &\cellcolor{gray!20}0.509& 9.1M & 8.6\\
            V \& L  &0.630 & 0.600 &\cellcolor{gray!20}0.615 & 0.504 & 0.561 &\cellcolor{gray!20}0.531 & 39.8M & 5.5\\
            \bottomrule
        \end{tabular}
    % \vspace{-6pt}
    \caption{\textbf{Ablation for modality.}}
    \label{tab:modality}
\vspace{-15pt}
\end{table}


\boldparagraph{Training schedule.}
Tab.~\ref{tab:schedule} shows that adding more epochs mainly increases the TOPO metric and benefits little Junction TOPO, especially the recall of Junction TOPO. The 110-epoch trained multi-modality experiment further pushes the performance of lane graph construction.


\begin{table}[h!]
    % \setlength{\tabcolsep}{3pt}
    \centering

    \begin{tabular}{l c  c  c  c  c  c  c }
    \toprule
    \multirow{2}{*}{Modality} & \multirow{3}{*}{Epoch} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
    % \cline{2-7}
    & & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
    \midrule
    Vision-only &24 &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461 \\
    Vision-only &110 & 0.591 & 0.539 &\cellcolor{gray!20}0.564 & 0.547 & 0.511 &\cellcolor{gray!20}0.529 \\
    \midrule
    Vision \& LiDAR  & 24 &0.630 & 0.600 &\cellcolor{gray!20}0.615 & 0.504 & 0.561 &\cellcolor{gray!20}0.531 \\
    Vision \& LiDAR  & 110&0.668 & 0.601 & \cellcolor{gray!20}0.632 & 0.620 & 0.576 & \cellcolor{gray!20}0.597 \\
    \bottomrule
\end{tabular}
% \vspace{-6pt}
\caption{\textbf{Ablation for training schedule.} }
\label{tab:schedule}
\vspace{-15pt}
\end{table}

\boldparagraph{Tradeoff Ccomparison of modelings.}
In Tab.~\ref{tab:cost}, we report the tradeoff comparison of modelings in terms of accuracy and cost on nuScenes dataset.
\begin{table}[ht!]
    % \setlength{\tabcolsep}{3pt}
    \centering
    % \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{l  c  c  c  c  r  c }
        \toprule
        Method & Modeling & Modality &$F_1$@Junction & $F_1$@TOPO & FPS & Training time \\
        \midrule
        HDMapNet & Pixel-wise & V & 0.507 & 0.513 & 6.8 & 42h \\
        MapTR & Piece-wise & V & 0.496 & 0.465 & 15.0 & 49h\\
        \midrule
        LaneGAP & Path-wise & V & 0.564 & 0.529 & 15.6 & 58h\\
        LaneGAP & Path-wise & V\&L &0.632 & 0.597 & 5.5 & 60h\\
        % peception range & \multicolumn{3}{c}{}
        \bottomrule
    \end{tabular}
\vspace{2pt}
\caption{\textbf{Tradeoff comparison on nuScenes dataset.} ``V'' and ``L'' respectively denote Vision modality and LiDAR modality. ``$F_1$@Junction'' and ``$F_1$@TOPO'' respectively denote $F_1$ of Junction TOPO and $F_1$ of TOPO on directed graph.}
\label{tab:cost}
\vspace{-15pt}
\end{table}




\section{Visualizations for Ablations}
In Fig.~\ref{fig:ablation_comp}, we visualize the 24-epoch trained ablation experiments: multi-modality in column 2, LiDAR modality in column 3, vision modality in column 4 and 5, 30-point polyline representation in column 2, 3 and 4, 5-point Bezier representation in column 5. As shown in Fig.~\ref{fig:ablation_comp}, the multi-modality method outputs a more accurate lane graph, and the Bezier method outputs smoother paths.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ablation_comp.pdf}
    % \vspace{-58pt}
    \caption{\textbf{Visualizations for ablations.} The visualized  ablation experiments are under a 24-epoch training schedule. The multi-modality method provides more accurate lane graph, and the Bezier method outputs smoother paths.}
    \label{fig:ablation_comp}
    \vspace{-4pt}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/supp_junctions.pdf}
    % \vspace{-58pt}
    \caption{\textbf{Qualitative comparisons of increasing junction points.} We qualitatively compare path-wise LaneGAP with pixel-wise HDMapNet and piece-wise MapTR on lane graphs with increasing junction points. All models perform well on simple lane graphs with few junction points. However, as the number of junction points increases, HDMapNet and MapTR struggle to predict reasonable lane graphs, while our proposed LaneGAP continues to deliver robust results. }
    \label{fig:junctions}
    \vspace{-4pt}
\end{figure*}

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/supp_conditions.pdf}
    % \vspace{-58pt}
    \caption{\textbf{Qualitative comparisons of diverse driving conditions.} We qualitatively compare path-wise LaneGAP with pixel-wise HDMapNet and piece-wise MapTR across sunny, cloudy, rainy, dark, and occluded driving conditions. LaneGAP consistently demonstrates superior performance across all conditions. Specifically, in the occluded condition (row 5), results indicate that pixel-wise and piece-wise modelings, which may break continuity, are prone to predict incomplete lane graphs. In contrast, our path-wise modeling approach preserves the continuity of the lane graph, enhancing robustness in occluded driving conditions.}
    \label{fig:conditions}
    \vspace{-4pt}
\end{figure}


\begin{figure}[th!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/supp_fail.pdf}
    % \vspace{-58pt}
    \caption{\textbf{Qualitative comparisons of failure cases.} We qualitatively compare path-wise LaneGAP with pixel-wise HDMapNet and piece-wise MapTR on the cases, where LaneGAP fails to handle. LaneGAP struggles to predict a complete subgraph for distant junction areas (row 1 and 2), located at the edge of the perception range. Furthermore, the extremely hard online driving conditions and topology (occlusion in row 4 and darkness in row 5) also make the construction very challenging.  Addressing these issues requires further research and development.}
    \label{fig:fail}
    \vspace{-4pt}
\end{figure}

\section{More Qualitative Comparisons}
\boldparagraph{Qualitative comparisons of increasing junction points.} In Fig.~\ref{fig:junctions}, we compare path-wise LaneGAP with pixel-wise HDMapNet, and piece-wise MapTR on lane graphs with increasing junction points. All models perform well on simple lane graphs with few junction points. However, as the number of junction points increases, HDMapNet and MapTR struggle to predict reasonable lane graphs, while our proposed LaneGAP continues to deliver robust results.

\boldparagraph{Qualitative comparisons of diverse driving conditions.} In Fig.~\ref{fig:conditions}, we compare these modelings across diverse driving conditions, including sunny, cloudy, rainy, dark, and occluded scenarios.  LaneGAP consistently demonstrates superior performance across all conditions. Specifically, in the occluded condition (row 5), results indicate that pixel-wise and piece-wise modelings, which may break continuity, are prone to predict incomplete lane graphs. In contrast, our path-wise modeling approach preserves the continuity of the lane graph, enhancing robustness in occluded driving conditions.

\boldparagraph{Qualitative comparisons of failure cases.}
In Fig.~\ref{fig:fail}, we compare these modelings on the cases, where LaneGAP fails to handle. The results show that LaneGAP struggles to predict a complete subgraph for distant junction areas (row 1 and 2), located at the edge of the perception range. Furthermore, the extremely hard online driving conditions and topology (occlusion in row 4 and darkness in row 5) also make the construction very challenging.  Addressing these issues requires further research and development.


\section{Video Visualizations}
We visualize the whole nuScenes validation set and attach videos to the supplementary material. The videos are generated with two models: the path-wise method based on only vision input and the path-wise method based on vision and LiDAR input.

% \bibliographystyle{splncs04}
% \bibliography{main}

\end{document}
