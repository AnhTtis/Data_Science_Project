\section{Experiments}




% \begin{table}[ht!]
%     \setlength{\tabcolsep}{3pt}
%     \centering
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{l l  c | c  c  c  c|  c  c  c c | c  c  c  c  }
%         % \hline
%         \toprule
%         \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multirow{2}{*}{Modeling} & \multicolumn{4}{c|}{$30m \times 60m$ val complex split} & \multicolumn{4}{c|}{$50m \times 100m$ val split} & \multicolumn{4}{c}{$50m \times 100m$ val complex split} \\ 
%         % \cline{2-7}
%         & & & $\overrightarrow{F}_1^{\text{Junc. TOPO}}$ & $\overrightarrow{F}_1^{\text{TOPO}}$ & $\bar{F}_1^{\text{Junc. TOPO}}$ &$\bar{F}_1^{\text{TOPO}}$ & $\overrightarrow{F}_1^{\text{Junc. TOPO}}$ & $\overrightarrow{F}_1^{\text{TOPO}}$ & $\bar{F}_1^{\text{Junc. TOPO}}$ &$\bar{F}_1^{\text{TOPO}}$ &$\overrightarrow{F}_1^{\text{Junc. TOPO}}$ & $\overrightarrow{F}_1^{\text{TOPO}}$ & $\bar{F}_1^{\text{Junc. TOPO}}$ &$\bar{F}_1^{\text{TOPO}}$ \\
%         % \hline
%         \midrule
%         \multirow{4}{*}{nusc}& HDMapNet & Pixel-wise  & \\
%         & STSU & Piece-wise &\\
%         % \hline
%         & MapTR & Piece-wise & \\
%         % \midrule
%         % & \textbf{LaneGAP(ours)} & Path-wise  & 0.562 &  0.514 & \cellcolor{gray!20}0.537 & 0.536 & 0.495 & \cellcolor{gray!20}0.515 & 0.518 & 0.495 & \cellcolor{gray!20}0.506 & 0.515 & 0.486 & \cellcolor{gray!20}0.500
%         %  & 35.9M &16.5& \cellcolor{gray!20}15.6\\
%         % \hline
%         & \textbf{LaneGAP(ours)} & Path-wise & \\
%         \midrule
%         \multirow{4}{*}{av2} & HDMapNet &Pixel-wise & \\
%         & STSU & Piece-wise & \\
%         & MapTR & Piece-wise & \\
%         % & \textbf{LaneGAP(ours)} & Path-wise & 0.621&0.553&0.585&0.561&0.548&0.554&0.582&0.538&0.559&0.542&0.531&0.536&?&?&?\\
%         & \textbf{LaneGAP(ours)} & Path-wise &\\
%         \bottomrule
%     \end{tabular}}

% \caption{\textbf{Quantitative comparison of lane graph modeling in complicated scenarios.}}

% \label{tab:complex_comp}


% \end{table}



\subsection{Dataset}

We conduct experiments on two popular and large-scale datasets, \ie, nuScenes \cite{nuscenes} and Argoverse2~\cite{av2}. nuScenes~\cite{nuscenes} dataset consists of 1000 scene sequences. Each sequence is sampled in 2Hz frame rate and provides LiDAR point cloud and  RGB images from 6 surrounding cameras, which covers 360$^\circ$ horizontal FOV of the ego-vehicle.
The dataset provides the 2D lane graph without height information in the form of lane centerline and covers diverse online driving conditions. For the online setting of lane graph construction, we set the perception ranges as $[-15.0m, 15.0m]$ for the $X$-axis and $[-30.0m, 30.0m]$ for the $Y$-axis, and preprocess the dataset following~\cite{hdmapnet,vectormapnet,maptr}. We train on the nuScenes train set and evaluate on the val set.  The experiments on nuScenes dataset are conducted using 6 surrounding-view images by default.
Argoverse2~\cite{av2} dataset contains 1000 scene logs and 7 surrounding cameras with 360$^\circ$ horizontal FOV. Each log is sampled in 10Hz frame rate and provides 3D lane graph. Due to the limitations of pixel-wise modeling in performing 3D segmentation (piece-wise and path-wise modelings can simply predict an extra z-coordinate to support 3D lane graph construction), we omit the height information for fair comparison across different modelings. Other experimental settings remain the same as those for nuScenes.

% \boldparagraph{2D map dataset.} We conduct experiments on the challenging nuScenes~\cite{nuscenes} dataset consisting of 1000 sequences.  Each sequence is sampled in 2Hz frame rate and provides LiDAR point cloud and  RGB images from 6 surrounding cameras, which covers 360$^\circ$ horizontal FOV of the ego-vehicle. The dataset provides the 2D lane graph without height information in the form of lane centerline and covers diverse online driving conditions (\eg, day, night, cloudy, rainy, and occlusion). For the online setting of lane graph construction, we set the perception ranges as $[-15.0m, 15.0m]$ for the $X$-axis and $[-30.0m, 30.0m]$ for the $Y$-axis and preprocess the dataset following~\cite{hdmapnet,vectormapnet,maptr}. We train on the nuScenes train set and evaluate on the val set.  The experiments are conducted using 6 surrounding-view images by default.

% \boldparagraph{3D map dataset.} We further provide experiments on Argoverse2~\cite{av2} dataset, which contains 1000 logs. Each log provides 15s of 20Hz RGB images from 7 cameras and a log-level 3D vectorized map. Due to the disability of pixel-wise modeling in performing 3D segmentation (piece-wise and path-wise modelings can simply predict an extra z corrdinate to support 3D lane graph construction), we drop the height information for comparison. The train/val split is the same as the nuScenes.

\begin{table}[t!]
    \setlength{\tabcolsep}{2pt}
    \renewcommand\arraystretch{1.1}
    \centering
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{l l  c  c  c  c  c  c  c  c  c  c  c  c  c c  c   c}
        % \hline
        \toprule
        \multirow{3}{*}{Dataset} & \multirow{3}{*}{Method} & \multirow{3}{*}{Modeling} & \multicolumn{6}{c}{Directed Graph} & \multicolumn{6}{c}{Undirected Graph} & \multirow{3}{*}{Param.} & \multirow{3}{*}{FPS$_{\text{net}}$} &\cellcolor{gray!20}\\ 
        
        & &  & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} 
         & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO}  & & & \cellcolor{gray!20}\\ 
        % \cline{2-7}
        & & & Prec. & Rec. & \cellcolor{gray!20}$F_1$ & Prec. & Rec. &  \cellcolor{gray!20}$F_1$
         & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &  \cellcolor{gray!20}$F_1$ & & & \multirow{-3}{*}{\cellcolor{gray!20}FPS}\\
        % \hline
        \midrule
        \multirow{4}{*}{nusc}& HDMapNet & Pixel-wise  & 0.594 & 0.442 & \cellcolor{gray!20}0.507 & 0.608 & 0.443 & \cellcolor{gray!20}0.513 & 0.526 & 0.445 & \cellcolor{gray!20}0.482 & 0.563 & 0.447 &  \cellcolor{gray!20}0.498
        & 35.4M & 17.8 & \cellcolor{gray!20}6.8\\
        & STSU & Piece-wise & 0.449 & 0.393 & \cellcolor{gray!20}0.419 & 0.416 & 0.376 & \cellcolor{gray!20}0.395 & 0.413 & 0.404 & \cellcolor{gray!20}0.408 & 0.405 & 0.375 & \cellcolor{gray!20}0.389 & 35.3M & 15.8 &  \cellcolor{gray!20}15.1\\
        % \hline
        & MapTR & Piece-wise & 0.540 & 0.459 & \cellcolor{gray!20}0.496 & 0.504 & 0.432 &  \cellcolor{gray!20}0.465 & 0.493 & 0.465 &  \cellcolor{gray!20}0.478 & 0.487 & 0.429 &  \cellcolor{gray!20}0.456
        & 36.0M & 15.6 & \cellcolor{gray!20}15.0\\
        % \midrule
        % & \textbf{LaneGAP(ours)} & Path-wise  & 0.562 &  0.514 & \cellcolor{gray!20}0.537 & 0.536 & 0.495 & \cellcolor{gray!20}0.515 & 0.518 & 0.495 & \cellcolor{gray!20}0.506 & 0.515 & 0.486 & \cellcolor{gray!20}0.500
        %  & 35.9M &16.5& \cellcolor{gray!20}15.6\\
        % \hline
        & LaneGAP & Path-wise & 0.591 & 0.539 & \cellcolor{gray!20}\textbf{0.564} & 0.547 & 0.511 & \cellcolor{gray!20}\textbf{0.529} & 0.543 & 0.518 & \cellcolor{gray!20}\textbf{0.530} & 0.534 & 0.496 & \cellcolor{gray!20}\textbf{0.514} & 35.9M & 16.5 & \cellcolor{gray!20}\textbf{15.6} \\
        \midrule
        \multirow{4}{*}{av2} & HDMapNet &Pixel-wise & 0.656 & 0.449 & \cellcolor{gray!20}0.533 & 0.635 & 0.463 & \cellcolor{gray!20}0.535 & 0.596& 0.473&\cellcolor{gray!20}0.528&0.607&0.470&\cellcolor{gray!20}0.530&35.4M&15.2& \cellcolor{gray!20}4.6\\
        & STSU & Piece-wise & 0.562&0.414&\cellcolor{gray!20}0.477&0.462&0.436&\cellcolor{gray!20}0.449&0.512&0.416&\cellcolor{gray!20}0.459&0.449&0.431&\cellcolor{gray!20}0.440&35.3M&13.0& \cellcolor{gray!20}12.4\\
        & MapTR & Piece-wise & 0.589&0.469&\cellcolor{gray!20}0.522&0.520&0.481&\cellcolor{gray!20}0.500&0.543&0.468&\cellcolor{gray!20}0.503&0.507&0.477&\cellcolor{gray!20}0.491&36.0M&13.1&  \cellcolor{gray!20}12.5\\
        % & \textbf{LaneGAP(ours)} & Path-wise & 0.621&0.553&0.585&0.561&0.548&0.554&0.582&0.538&0.559&0.542&0.531&0.536&?&?&?\\
        & LaneGAP & Path-wise &0.663&0.575&\cellcolor{gray!20}\textbf{0.616}&0.578&0.552&\cellcolor{gray!20}\textbf{0.565} &0.627&0.541&\cellcolor{gray!20}\textbf{0.581}&0.569&0.546&\cellcolor{gray!20}\textbf{0.557}&35.9M&13.8& \cellcolor{gray!20}\textbf{13.0}\\
        \bottomrule
    \end{tabular}}

\caption{\textbf{Quantitative comparison of lane graph modeling under fair conditions} (the same encoder, comparable model size, the same training schedule, and vision-only modality). FPS$_{\text{net}}$ and FPS of all the experiments are measured on the same machine with one NVIDIA Geforce RTX 3090 GPU and one 24-core AMD EPYC 7402 2.8 GHz CPU, where FPS$_{\text{net}}$ is benchmarked with only network forward.}

\label{tab:modeling}

\end{table}
\begin{table}[t!]
    \centering
    \renewcommand\tabcolsep{3pt}
    \resizebox{0.98\linewidth}{!}{
    \begin{tabular}{l|ccccc|c}
        \toprule
        Method    &HDMapNet$^{*}_{\text{pixel}}$ & STSU$^{*}_{\text{piece}}$ & VectorMapNet$^{*}_{\text{piece}}$ & MapTR$^{*}_{\text{piece}}$ & TopoNet$^{*}_{\text{piece}}$ & LaneGAP$^{\dagger}$ \\
        \midrule
        mIoU  &  18.3 & 31.1 & 25.0 & 35.7&39.0&\textbf{40.6}\\
        % mIoU on subset\_B &  - & - & - & - & 36.9 & \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{Quantitative comparison on Openlane-V2 dataset}. $^{*}$ means the results are directly taken from TopoNet paper~\cite{toponet}. LaneGAP$^{\dagger}$ is our proposed variant based on TopoNet$_{\text{piece}}$ to validate the effectiveness of path-wise modeling, which only adapts the lane piece detection part of TopoNet for learning path-wise representation.}
    \label{tab:toponet}

\end{table}


\subsection{Baselines}


\boldparagraph{Implementation details.}
% \boldparagraph{Implementation details.} 
All the baselines use the same encoder consisting of ResNet50~\cite{resnet} and GKT~\cite{gkt} to transform the  360$^\circ$ horizontal FOV multi-camera images to BEV features at the resolution of $200 \times 100$ (\ie, the BEV grid size is $0.3m$).   
All the models are controlled in comparable parameter sizes and trained for long enough epochs to ensure convergence (110 epochs on nuScenes and 24 epochs on Argoverse2). We train all the experiments on 8 NVIDIA GeForce RTX 3090 GPUs with a total batch size of 32 (6 view images on nuScenes and 7 view images on Argoverse2). 
Further details about training settings are provided in the supplementary material. 
The only differences lie in the specialized decoders designed for respective modelings, which are detailed as follows.

% We compare our path-wsie modeling with pixel-wise modeling  and piece-wise modeling. For a strictly fair comparison, all the methods are trained and tested with 6 surrounding camera images as input and use the same encoder consisting of ResNet50~\cite{resnet} and GKT~\cite{gkt} to transform  the input RGB images to the BEV features.  Because the piece-wise modeling and path-wise modeling predict the localization with regression branch, to exclude the quantization error brought by classification on dense pixels,  the output resolution of the UNet-shaped~\cite{ronneberger2015u} segmentation model used in pixel-wise modeling is set to $400 \times 200$, which means the grid size is $0.15m$ (the perception range is $60m\times30m$) the same as the graph interpolation size. All models are controlled in comparable parameter sizes and trained for 110 epochs to ensure convergence. We trained all the experiments on 8 NVIDIA GeForce RTX 3090 GPUs with a total batch size of 32 (containing 6 view images). The only differences lie in the specialized decoder designed for respective modelings.

\boldparagraph{Pixel-wise modeling.}
% \boldparagraph{Pixel-wise modeling.} 
We select HDMapNet~\cite{hdmapnet} as the pixel-wise modeling baseline. It adopts a U-Net structure with a ResNet18  to output a binarized segmentation map and a direction map with two branches. Meanwhile, a cross-entropy loss is applied to the segmentation map,  and  $L_2$ loss is applied to the (-1,1) normalized direction map~\cite{laneextract,zhou2018d}. To exclude the quantization error brought by classification on dense pixels, the output resolution of the U-Net-shaped~\cite{ronneberger2015u} segmentation model is set to $400 \times 200$, which means the grid size is $0.15m$ (the perception range is $60m\times30m$), the same as the graph interpolation size. The Pixel2Graph follows \cite{hdmapnet, laneextract}.

% However, we find training the direction map by applying $L_2$ loss on (-1,1) normalized direction yields better performance than the cross-entropy loss on  discretized direction~\cite{laneextract,zhou2018d}. Thus, in our comparison, we train the direction branch of the segmentation model with $L_2$ loss on a normalized direction map.

% Following HDMapNet~\cite{hdmapnet}, we adopt an UNet structure with a ResNet18 encoder to output the binarized segmentation map and direction map with two branches. Meanwhile, a cross-entropy loss is applied to the segmentation map and direction map. However, we find training the direction map by applying $L_2$ loss on (-1,1) normalized direction yields better performance than the cross-entropy loss on  discretized direction~\cite{laneextract,zhou2018d}. Thus, in our comparison, we train the direction branch of the segmentation model with $L_2$ loss on a normalized direction map.

\boldparagraph{Piece-wise modeling.}
STSU~\cite{stsu} is selected as the piece-wise modeling baseline, which consists of a detection branch to detect Bezier lane pieces with 4 control points and an association branch to predict the connectivity between lane pieces. It stacks 6 Transformer decoder layers to iteratively refine the predictions. $L_1$ loss and Focal loss are applied to the matched (0,1) normalized pieces and cross-entropy loss is applied to the connectivity. More recently, MapTR~\cite{maptr} proposes a much stronger Transformer decoder for the detection branch. We further implement an additional piece-wise baseline by simply replacing the original Beizer piece detection branch of STSU with  MapTR Polyline piece detection branch. We set the number of Polyline points to 20. The Piece2Graph  follows \cite{stsu}.

% \boldparagraph{Piece-wise modeling.}
% We implement the piece-wise modeling by strictly following STSU~\cite{stsu}. We find the original piece detection decoder shows inferior performance compared with ours.  In order to fairly compare the modeling and  exclude the influence of network design, we adopt the same architecture as ours. The only difference is that it predicts connectivity classification and  pieces instead of paths. $L_1$ loss is applied to the matched pieces and cross-entropy loss is applied to the connectivity. 

\boldparagraph{Path-wise modeling.} 
The path-wise LaneGAP adopts the same decoder as the piece-wise MapTR. We use the 30-point Polyline to represent the path. We stack 6 deformable Transformer decoder layers and regress $(0,1)$ normalized coordinates. During training, $L_1$ loss and Focal loss are applied to the matched paths, and cross-entropy loss is applied to the auxiliary BEV segmentation branch. As for the Path2Graph, We discretize the path at the interval of $0.15m$, and the merging threshold is set to $0.15m$, the same as the graph interpolation size.


% We further implement an additional baseline termed as LaneGAP$^{\dagger}$ in Tab.~\ref{tab:modeling} by introducing a simple BEV segmentation branch  (like pixel-wise modeling) as auxiliary supervision to enhance the BEV representation only during training. Cross-entropy loss is applied to this auxiliary segmentation branch. Note that this  auxiliary branch doesn't bring any computation budget in the inference phase.

% We utilize deformable attention~\cite{deformdetr} as the cross attention of the Transformer and use the 30-point Polyline to represent the path and exploit the features along the path with resort to deformable attention as stated in Sec.~\ref{sec:lanegap}. We stack 6 decoders and directly regress $(0,1)$ normalized coordinates. $L_1$ loss is applied to the matched paths.

% \boldparagraph{Path-wise modeling with auxiliary segmentation supervision.}
% We further introduce a BEV segmentation branch (like pixel-wise modeling) as auxiliary supervision to enhance the BEV representation.
% % Transformer decoders by applying the auxiliary segmentation on output BEV features 
% % enhance the BEV representation 
% Note that this  auxiliary branch doesn't bring any computation budget in the inference phase.
% % We only introduce a $3\times3$ convolutional segmentation head during training, and cross-entropy loss is applied to the output segmentation.



\subsection{Quantitative Comparision}
% refer to Tab.~\ref{tab:modeling}
Tab.~\ref{tab:modeling} compares path-wise modeling with pixel-wise modeling and piece-wise modeling \wrt their accuracy, model size, FPS$_{\text{net}}$, FPS. 
% All the FPS$_{\text{net}}$ and FPS are measured on the same machine with one NVIDIA Geforce RTX 3090 GPU and one 24-core AMD EPYC 7402 2.8 GHz CPU, where FPS$_{\text{net}}$ is benchmarked with only network forward.

\boldparagraph{Highlights.} Path-wise modeling achieves the best $F_1$ score on the both subgraph around the junction points and the overall graph across two popular datasets (nuScenes and Argoverse2), while running at the fastest inference speed. The designed Path2Graph algorithm has negligible cost in translating predicted paths into a directed lane graph (from 16.5 FPS to 15.6 FPS). 

% As shown in Tab.~\ref{tab:modeling}, our proposed path-wise modeling outperforms other modelings in speed and performance at both subgraph around junction points and the overall lane graph. Path-wise modeling achieves 4.1\% higher $F_1$ score and 2.8\% higher $F_1$ score than piece-wise modeling around the junction for directed lane graph and undirected lane graph. Though the pixel-wise modeling demonstrates decent accuracy, it is much slower (pixel-wise, 1.8 FPS \vs path-wise, 15.9 FPS), hindering its further application to the online setting.

\boldparagraph{Path-wise \vs pixel-wise.}
Thanks to the high-resolution output of the segmentation model, pixel-wise modeling exhibits comparable precision for subgraph construction and overall graph construction. Nevertheless, the notorious fragmentation and oversmooth issues of segmentation methods make it hard to maintain continuity across pixels and distinguish the fine-grained subgraph around the junction points, leading to much lower recall compared to path-wise modeling. 
% The sophisticated and prone-to-fail post-processing also adds enormous cost at the inference phase to convert the segmentation output into a vectorized lane graph. 
In comparison, path-wise modeling demonstrates higher lane graph construction quality and 2x faster inference speed.
% the coarse rasterized segmentation output and prone-to-fail post-processing make it hard to distinguish the fine-grained subgraph around the junction points. As shown in Tab.~\ref{tab:modeling}, Path-wise modeling demonstrates obvious superiority on the subgraph around the junction points (3\% higher $F_1$ for directed graph and 2.4\% $F_1$ for undirected graph), while being 2x faster.

\boldparagraph{Path-wise \vs piece-wise.} Path-wise modeling outperforms piece-wise modeling on all metrics. Even equipped with the same advanced Transformer decoder proposed in \cite{maptr}, our path-wise LaneGAP still significantly outperforms piece-wise MapTR, which validates the effectiveness.

% \boldparagraph{Path-wise modeling with auxiliary segmentation supervision.}
% The auxiliary segmentation supervision significantly boosts both  Junction TOPO  and TOPO metric without adding inference cost.

\subsection{Quantitative Comparision on Openlane-V2 Dataset}

Tab.~\ref{tab:toponet} compare path-wise modeling against state-of-the-art piece-wise method TopoNet on OpenLane-V2~\cite{openlanev2} dataset, which is a large-scale perception and reasoning dataset developed from nuScenes and Argoverse2. To further validate the superiority of path-wise modeling over piece-wise modeling under fair setting, we adapt the lane piece detection part of TopoNet for learning path-wise representation, while removing all the related piece-wise connection modules of TopoNet. This variant is denoted as LaneGAP$^{\dagger}_{\text{path}}$, distinguishing it from the original implementation, TopoNet${_\text{piece}}$. All other hyper-parameters and the training recipe remain consistent with TopoNet$_{\text{piece}}$.

\boldparagraph{Results.} Since the evaluation  metric proposed in TopoNet is not applicable to other modelings. We employ the mIoU metric for comparison, which is also utilized in TopoNet paper for benchmarking against the pixel-wise HDMapNet. As shown in Tab.~\ref{tab:toponet}, LaneGAP$^{\dagger}$ achieves 1.6 higher mIoU than TopoNet$_{\text{piece}}$ in $50m \times 100m$ perception range, thereby validating the effectiveness of path-wise modeling.


\subsection{Qualitative Comparison}

Fig.~\ref{fig:qualitative_comparison} compares baselines on complicated lane graphs with more than 6 junction points across different datasets. Failure case studies and more qualitative comparisons are provided in the supplementary material.

\begin{table*}[t!]
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3pt}
        % \tablestyle{2pt}{1.1}
        \small
        \scalebox{0.62}{
        \begin{tabular}{l  c  c  c  c  c  c c c}
            \toprule
            \multirow{2}{*}{$\mathcal{L}_{\rm auxseg}$} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} & \multirow{2}{*}{Param.} & \multirow{2}{*}{FPS}\\ 
            % \cline{2-7}
             & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$ & &  \\
            \midrule
            \xmark &  0.544 & 0.516 &\cellcolor{gray!20}0.529 & 0.421 & 0.484 &\cellcolor{gray!20}0.450 & 35.9M & 15.6 \\
    
            \cmark &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461 & 35.9M & 15.6 \\
            \bottomrule
        \end{tabular}}
        \caption{Auxiliary BEV segmentation branch.}
        \label{tab:bevseg}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{0.9}
        \renewcommand\tabcolsep{3pt}
        \small
        \scalebox{0.62}{
            \begin{tabular}{l c  c  c  c  c  c c c}
                % \hline
                \toprule
                \multirow{2}{*}{\shortstack[l]{Polyline\\Points}} &  \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} & \multirow{2}{*}{Param.} & \multirow{2}{*}{FPS}\\ 
                % \cline{2-7}
                &  Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
                \midrule
                % \hline
                10   & 0.550 & 0.515 & \cellcolor{gray!20}0.532 & 0.423 & 0.481 & \cellcolor{gray!20}0.450  & 35.9M & 16.3\\
                % \hline
                30   & 0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461  & 35.9M & 15.6\\
                % 30 &110 & 0.562 & 0.514 &\cellcolor{gray!20}0.537 & 0.536 & 0.495 &\cellcolor{gray!20}0.515 \\
                % \hline
                40  & 0.566 & 0.548 &\cellcolor{gray!20}0.557 & 0.440 & 0.509 & \cellcolor{gray!20}0.472  & 35.9M & 15.1\\
                % \hline
                \bottomrule
            \end{tabular}}

        \caption{Polyline path representation.}
        %  Increasing the number of Polyline points for path representation leads to continuous improvement.}
        \label{tab:polyline}
\end{minipage}
\end{table*}
\begin{table*}[t!]
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3pt}
        \small
        \scalebox{0.62}{
            \begin{tabular}{l  c  c  c  c  c  c c c}
                % \hline
                \toprule
                \multirow{2}{*} {\shortstack[l]{Bezier \\ Points}} &  \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO}& \multirow{2}{*}{Param.} & \multirow{2}{*}{FPS}\\ 
                % \cline{2-7}
                & Prec. & Rec. &\cellcolor{gray!20}$F_1$  & Prec. & Rec. & \cellcolor{gray!20}$F_1$  \\
                % \hline
                \midrule
                3  & 0.480 & 0.483 & \cellcolor{gray!20}0.481 & 0.309 & 0.447 & \cellcolor{gray!20}0.365 
                &35.9M & 16.1\\
                % \hline
                5  & 0.514 & 0.507 &\cellcolor{gray!20}0.510 &0.375 & 0.477 & \cellcolor{gray!20}0.477  
                & 35.9M & 15.8\\
                10  & 0.498 & 0.503 & \cellcolor{gray!20}0.501 & 0.334 & 0.463 & \cellcolor{gray!20}0.388  & 35.9M & 15.6\\
                % \hline
                \bottomrule
            \end{tabular}}
        \caption{Bezier path representation.}
        %  Too few Bezier control points cannot describe the shape and too many Bezier control points deteriorate the performance. }
        \label{tab:bezier}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{2pt}
        % \tablestyle{2pt}{1.1}
        \small
        \scalebox{0.58}{
            \begin{tabular}{l|cccc|cccc}
                \toprule
                \multirow{2}{*}{Method} &
                \multicolumn{4}{c|}{L2 (m) $\downarrow$} & 
                \multicolumn{4}{c}{Collision (\%) $\downarrow$} \\
                & 1s & 2s & 3s & \cellcolor{gray!30}Avg. & 1s & 2s & 3s & \cellcolor{gray!30}Avg.\\
                \midrule
                VAD-Tiny &0.46 &0.76& 1.12& \cellcolor{gray!30}0.78&0.21& 0.35& 0.58& \cellcolor{gray!30}0.38\\ 
                VAD-Tiny+MapTR& 0.43 & 0.74 & 1.06 & \cellcolor{gray!30}0.74 & 0.20 & 0.31 & 0.51 & \cellcolor{gray!30}0.34 \\
                VAD-Tiny+LaneGAP&\textbf{0.42}& \textbf{0.70} &\textbf{0.99}&\cellcolor{gray!30}\textbf{0.70} & \textbf{0.18} & \textbf{0.27} & \textbf{0.43} & \cellcolor{gray!30}\textbf{0.29} \\
                \bottomrule
            \end{tabular}}

    \caption{Comparison on downstream end-to-end planner VAD.}
    \label{tab:plan}
    \end{minipage}

\end{table*}

\begin{table*}[t!]
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3pt}
        \small
        \scalebox{0.62}{
        \begin{tabular}{l c  c  c  c  c  c  c }
            \toprule
            \multirow{2}{*}{Method} & \multirow{3}{*}{Modeling} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
            % \cline{2-7}
            & & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
            \midrule
            HDMapNet &Pixel & 0.401&0.328&\cellcolor{gray!20}0.361&0.415&0.349&\cellcolor{gray!20}0.379 \\
            STSU &Piece & 0.309&0.297&\cellcolor{gray!20}0.303&0.298&0.273&\cellcolor{gray!20}0.285\\
            MapTR & Piece&0.369&0.357&\cellcolor{gray!20}0.363&0.363&0.317&\cellcolor{gray!20}0.338\\
            LaneGAP & Path&0.384&0.400&\cellcolor{gray!20}\textbf{0.392}&0.384&0.392&\cellcolor{gray!20}\textbf{0.388}\\
            \bottomrule
        \end{tabular}}

        \caption{Comparing on large perception range $50m \times 100m$.}
        \label{tab:perception_range}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3pt}
        \small
        \scalebox{0.62}{
            \begin{tabular}{l c  c  c  c  c  c  c }
                \toprule
                \multirow{2}{*}{Method} & \multirow{3}{*}{Modeling} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
                % \cline{2-7}
                & & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
                \midrule
                HDMapNet &Pixel &0.464&0.312&\cellcolor{gray!20}0.373&0.468&0.318&\cellcolor{gray!20}0.379\\
                STSU &Piece & 0.318&0.241&\cellcolor{gray!20}0.274&0.296&0.245&\cellcolor{gray!20}0.268\\
                MapTR & Piece&0.391&0.298&\cellcolor{gray!20}0.338&0.369&0.304&\cellcolor{gray!20}0.334\\
                LaneGAP & Path&0.458&0.356&\cellcolor{gray!20}\textbf{0.400}&0.447&0.358&\cellcolor{gray!20}\textbf{0.398}\\
                \bottomrule
            \end{tabular}}

        \caption{Comparing on new split.}
        %  Too few Bezier control points cannot describe the shape and too many Bezier control points deteriorate the performance. }
        \label{tab:newsplit}
    \end{minipage}

    \end{table*}
\begin{table*}[t!]
    \centering
    \begin{minipage}[t!]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{6pt}
        \small
        \scalebox{0.62}{
        \begin{tabular}{l|cccc}
            \toprule
                &HDMapNet & STSU & MapTR & LaneGAP \\
            \midrule
            mIoU on nusc &  48.1 & 38.2 & 43.6 & \textbf{49.2} \\
            mIoU on av2 &55.0&44.7&49.3&\textbf{55.9}\\
            \bottomrule
        \end{tabular}}
        \caption{Comparing on mIoU metric.}
        \label{tab:iou}
    \end{minipage}
    \hfill
    \begin{minipage}[t!]{0.48\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{7.2pt}
        \small
        \scalebox{0.62}{
            \begin{tabular}{l  c  c  c  c  c  c c c}
                % \hline
                \toprule
                \multirow{2}{*} {\shortstack[l]{Post- \\ Proc.}} &  \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
                % \cline{2-7}
                & Prec. & Rec. &\cellcolor{gray!20}$F_1$  & Prec. & Rec. & \cellcolor{gray!20}$F_1$ & \\
                % \hline
                \midrule
                Pixel  & 0.898 & 0.678 & \cellcolor{gray!20}0.773 & 0.913 & 0.693 & \cellcolor{gray!20}0.788 \\
                % \hline
                Piece  & 1.000 & 1.000 &\cellcolor{gray!20}1.000 & 1.000 & 1.000 & \cellcolor{gray!20}1.000 \\
                Path  & 1.000 & 1.000 & \cellcolor{gray!20}1.000 & 1.000 & 1.000 & \cellcolor{gray!20}1.000 \\
                % \hline
                \bottomrule
            \end{tabular}}

        \caption{Post-processing accuracy.}
        %  Too few Bezier control points cannot describe the shape and too many Bezier control points deteriorate the performance. }
        \label{tab:postproc}
    \end{minipage}

    \end{table*}



\boldparagraph{Highlights.} Path-wise modeling demonstrates better lane graph construction quality than pixel-wise and piece-wise modeling on extremely challenging lane graphs, well preserving the continuity of lane.

\boldparagraph{Path-wise \vs pixel-wise.} As shown in Fig.~\ref{fig:qualitative_comparison}, the pixel-wise HDMapNet performs well on non-junction area. But for the subgraph around the junction points, the segmentation model struggles to distinguish the fine-grained topology, and the post-processing is prone to fail to generate a decent vectorized lane graph,  aligning with the much lower Junction TOPO metrics in Tab.~\ref{tab:modeling}. On the contrary, LaneGAP can capture the fine-grained topology.
% row 2 and 4, the segmentation model utilized in pixel-wise modeling can hardly distinguish the fine-grained topology around the sub-graph of junction points, and the post-processing is prone to fail to generate a decent vectorized lane graph. Thanks to our rational modeling which treats each path as a whole for learning, we can capture the fine-grained topology with severe overlap and preserve the continuity of the lane.

\boldparagraph{Path-wise \vs piece-wise.} The quality of the lane graph constructed by piece-wise modeling depends on the accuracy of both piece detection and connectivity prediction. As shown in Fig.~\ref{fig:qualitative_comparison}, the piece-wise MapTR either detects jagged lane pieces or produces wrong connections, leading to an incomplete and inaccurate lane graph after Piece2Graph post-processing. In contrast, our proposed path-wise modeling encodes the connectivity between pieces into the continuous path representation, which is easy to learn and robust.
% row 1, 2, and 4, the piece-wise modeling method tends to produce wrong connections or jagged pieces, which can hardly be used in real applications. Path-wise modeling method can capture the information along the complete path, leading to better continuity and results.

% As shown in Fig.~\ref{fig:qualitative_comparison}, path-wise modeling constructs more robust and accurate lane graph under extremely challenging online settings (complicated lane graph, cloudy, rainy, and night).  We show that pixel-wise modeling struggles to distinguish lanes around the junction points and intersection points,  because the different lanes are very close to each other, where it is difficult for segmentation models to learn and is prone to fail for the heuristic post-processing. And the piece-wise modeling tends to predict short pieces, leading to incomplete lane graph (Fig.~\ref{fig:qualitative_comparison} (c)). Furthermore, the piece-wise modeling often produces worse lane graphs after merging the pieces with predicted connectivity for extremely complicated  lane graphs (Fig.~\ref{fig:qualitative_comparison} (a), (b), (c)). One may be confused that the obvious superiority does not appear in the quantitative comparison. This is because most lane graphs are composed of either lanes without junction points or lanes with very few junction points, where these three modelings demonstrate comparable performance. To demonstrate the effectiveness, we select extremely hard scenes for qualitative comparison.

% \begin{table}[ht!]
%     \setlength{\tabcolsep}{3pt}
%     \centering
%     \resizebox{0.45\textwidth}{!}{
%     \begin{tabular}{l  c  c  c  c  c  c c c}
%         \toprule
%         \multirow{2}{*}{$\mathcal{L}_{\rm auxseg}$} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} & \multirow{2}{*}{Param.} & \multirow{2}{*}{FPS}\\ 
%         % \cline{2-7}
%          & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$ & &  \\
%         \midrule
%         \xmark &  0.544 & 0.516 &\cellcolor{gray!20}0.529 & 0.421 & 0.484 &\cellcolor{gray!20}0.450 & 35.9M & 15.6 \\

%         \cmark &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461 & 35.9M & 15.6 \\
%         \bottomrule
%     \end{tabular}}

% \caption{\textbf{Ablation for  auxiliary BEV segmentation branch.}}
% %  Introducing auxiliary BEV segmentation branch can boost the performance without adding cost at inference.}
% \label{tab:bevseg}

% \end{table}







\subsection{Ablation Study}
We ablate the design choices with a 24-epoch training schedule of path-wise modeling on nuScenes by default without specification. And we report Junction TOPO and TOPO only on the directed lane graph.

\boldparagraph{Effectiveness of auxiliary BEV segmentation branch.} Tab.~\ref{tab:bevseg} shows the auxiliary BEV segmentation branch can improve the performance by 1\% $F_1$ of Junction TOPO and 1.1\% $F_1$ of TOPO without adding cost at inference. 

\boldparagraph{Polyline path representation.}
As shown in Tab.~\ref{tab:polyline}, the accuracy increases with adding more points for Polyline modeling under a 24-epoch schedule, while the inference speed decreases. We choose 30-point Polyline as default setting.



\boldparagraph{Bezier path representation.}
Tab.~\ref{tab:bezier} shows that adding the number of control points from 3 to 5 improves 2.9\% $F_1$ of Junction TOPO and 11.2\% $F_1$ of TOPO. While adding the number of control points from 5 to 10 incurs an accuracy drop. 
% Compared to Tab.~\ref{tab:polyline}, Bezier path representation is inferior to Polyline path representation.

\boldparagraph{Comparison on end-to-end planner VAD.}
Tab.~\ref{tab:plan} provides comparison on end-to-end planner VAD~\cite{vad} by including an extra centerline lane graph branch. Since VAD utilizes instance-wise queries, HDMapNet is not applicable, we only compare with piece-wise MapTR. Path-wise modeling enhances planning performance and outperforms piece-wise modeling. Notably, LaneGAPâ€™s path queries offer a complete depiction of the lane graph, unlike MapTR's centerline piece queries, whose connectivity predictions are not directly usable in VAD.

% \boldparagraph{Modality.}
% Tab.~\ref{tab:modality} shows that LiDAR modality builds a more accurate lane graph than vision modality (6.1\% higher $F_1$ of Junction TOPO and 4.8\% higher $F_1$ of TOPO). Fusing them further boosts performance.

% \begin{table}[ht!]
%     \setlength{\tabcolsep}{3pt}

%     \centering
%     \resizebox{0.45\textwidth}{!}{
%     \begin{tabular}{l  c  c  c  c  c  c c c }
%         \toprule
%         \multirow{2}{*}{Modality} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} & \multirow{2}{*}{Param.} & \multirow{2}{*}{FPS}\\ 
%         % \cline{2-7}
%          & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
%         \midrule
%         Vision-only &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461
%         & 35.9M & 15.6\\

%         LiDAR-only &  0.608 & 0.594 &\cellcolor{gray!20}0.600 & 0.467 & 0.559 &\cellcolor{gray!20}0.509& 9.1M & 8.6\\
%         Vision \& LiDAR  &0.630 & 0.600 &\cellcolor{gray!20}0.615 & 0.504 & 0.561 &\cellcolor{gray!20}0.531 & 39.8M & 5.5\\
%         \bottomrule
%     \end{tabular}}

% \caption{\textbf{Ablation for modality.} }
% \label{tab:modality}

% \end{table}


% \boldparagraph{Training schedule.}
% Tab.~\ref{tab:schedule} shows that adding more epochs mainly increases the TOPO metric and benefits little Junction TOPO, especially the recall of Junction TOPO. The 110-epoch trained multi-modality experiment further pushes the performance of lane graph construction.


\boldparagraph{Large perception range.} In Tab.~\ref{tab:perception_range}, we compare the performance on a large perception range ($50m \times 100m$) using the same training recipe of Tab.~\ref{tab:modeling}. LaneGAP still outperforms other methods.

\boldparagraph{Generalization on new split.} StreamMapNet~\cite{streammapnet} proposes a new split of the nuScenes dataset, which splits the train/val set by the geolocation and reduces the map overlap. We further compare the performance on this new split in Tab.~\ref{tab:newsplit} using the same training recipe. LaneGAP maintains consistent superiority.

\boldparagraph{Comparison on mIoU metric.} In Tab.~\ref{tab:iou}, we further compare the performance of different modelings with mIoU metric following HDMapNet~\cite{hdmapnet}. We render the prediction on the BEV map with $0.75m$ line width. The results further validate the superiority of LaneGAP.


\boldparagraph{Post-processing accuracy.} In Tab.~\ref{tab:postproc}, we show the accuracy of the lane graphs transformed by respective post-processings given the ground-truth segmentation maps, sets of pieces and connections, and set of paths. The results indicate that Pixel2Graph post-processing is prone to fail in handling the fine-grained topology of the subgraph around junction points, as illustrated in 
 Fig.~\ref{fig:qualitative_comparison}.  
\label{para:postproc-acc}

% \begin{table}[ht!]
%     \setlength{\tabcolsep}{3pt}
%     \centering
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{l l  c  c  c  c  c  c  c  c  c  c  c  c c }
%         % \hline
%         \toprule
%         \multirow{3}{*}{Data} & \multirow{3}{*}{Method} & \multicolumn{6}{c}{Directed Graph} & \multicolumn{6}{c}{Undirected Graph} &\multirow{3}{*}{mIoU} \\ 
        
%         &  & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} 
%          & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
%         % \cline{2-7}
%         & & Prec. & Rec. & \cellcolor{gray!20}$F_1$ & Prec. & Rec. &  \cellcolor{gray!20}$F_1$
%          & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &  \cellcolor{gray!20}$F_1$ \\
%         % \hline
%         \midrule
%         \multirow{2}{*}{subset\_A}& TopoNet &?&?&?&?&?&?&?&?&?&?&?&?&39.0 \\
%         & + Path-wise &?&?&?&?&?&?&?&?&?&?&?&?&40.6\\
%         \midrule
%         \multirow{2}{*}{subset\_B}& TopoNet & \\
%         & + Path-wise & \\
 
%         \bottomrule
%     \end{tabular}}

% \caption{\textbf{Quantitative comparison of lane graph modeling under fair conditions} (the same encoder, comparable model size, the same training schedule, and vision-only modality). FPS$_{\text{net}}$ and FPS of all the experiments are measured on the same machine with one NVIDIA Geforce RTX 3090 GPU and one 24-core AMD EPYC 7402 2.8 GHz CPU, where FPS$_{\text{net}}$ is benchmarked with only network forward. Path-wise modeling achieves the best lane graph construction quality and speed. }

% \label{tab:toponet}

% \end{table}



