\section{Experiments}
\subsection{Dataset}

We conduct experiments on two challenging and large-scale datasets, \ie, nuScenes~\cite{nuscenes} and Argoverse2~\cite{av2}. nuScenes~\cite{nuscenes} dataset consists of 1000 scene sequences. Each sequence is sampled in 2Hz frame rate and provides LiDAR point cloud and  RGB images from 6 surrounding cameras, which covers 360$^\circ$ horizontal FOV of the ego-vehicle.
The dataset provides the 2D lane graph without height information in the form of lane centerline and covers diverse online driving conditions (\eg, day, night, cloudy, rainy, and occlusion). For the online setting of lane graph construction, we set the perception ranges as $[-15.0m, 15.0m]$ for the $X$-axis and $[-30.0m, 30.0m]$ for the $Y$-axis and preprocess the dataset following~\cite{hdmapnet,vectormapnet,maptr}. We train on the nuScenes train set and evaluate on the val set.  The experiments on nuScenes dataset are conducted using 6 surrounding-view images by default.
Argoverse2~\cite{av2} dataset contains 1000 scene logs and 7 surrounding cameras with 360$^\circ$ horizontal FOV. Each log is sampled in 10Hz frame rate and provides 3D lane graph.  Due to the disability of pixel-wise modeling in performing 3D segmentation (piece-wise and path-wise modelings can simply predict an extra z corrdinate to support 3D lane graph construction), we drop the height information for fair comparison across different modelings. Other experimental settings are the same as nuScenes.


\subsection{Baselines}

\begin{table*}[ht!]
    \setlength{\tabcolsep}{3pt}
    \centering
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{l l  c  c  c  c  c  c  c  c  c  c  c  c  c c  c   c}
        % \hline
        \toprule
        \multirow{3}{*}{Dataset} & \multirow{3}{*}{Method} & \multirow{3}{*}{Modeling} & \multicolumn{6}{c}{Directed Graph} & \multicolumn{6}{c}{Undirected Graph} & \multirow{3}{*}{Param.} & \multirow{3}{*}{FPS$_{\text{net}}$} &\cellcolor{gray!20}\\ 
        
        & &  & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} 
         & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO}  & & & \cellcolor{gray!20}\\ 
        % \cline{2-7}
        & & & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &  \cellcolor{gray!20}$F_1$
         & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &  \cellcolor{gray!20}$F_1$ & & & \multirow{-3}{*}{\cellcolor{gray!20}FPS}\\
        % \hline
        \midrule
        \multirow{5}{*}{nusc}& HDMapNet & Pixel-wise  & 0.594 & 0.442 & \cellcolor{gray!20}0.507 & 0.608 & 0.443 & \cellcolor{gray!20}0.513 & 0.526 & 0.445 & \cellcolor{gray!20}0.482 & 0.563 & 0.447 &  \cellcolor{gray!20}0.498
        & 35.4M & 17.8 & \cellcolor{gray!20}6.8\\
        & STSU & Piece-wise & 0.449 & 0.393 & \cellcolor{gray!20}0.419 & 0.416 & 0.376 & \cellcolor{gray!20}0.395 & 0.413 & 0.404 & \cellcolor{gray!20}0.408 & 0.405 & 0.375 & \cellcolor{gray!20}0.389 & 35.3M & 15.8 &  \cellcolor{gray!20}15.1\\
        % \hline
        & MapTR & Piece-wise & 0.540 & 0.459 & \cellcolor{gray!20}0.496 & 0.504 & 0.432 &  \cellcolor{gray!20}0.465 & 0.493 & 0.465 &  \cellcolor{gray!20}0.478 & 0.487 & 0.429 &  \cellcolor{gray!20}0.456
        & 36.0M & 15.6 & \cellcolor{gray!20}15.0\\
        % \hline
        & \textbf{LaneGAP(ours)} & Path-wise & 0.591 & 0.539 & \cellcolor{gray!20}\textbf{0.564} & 0.547 & 0.511 & \cellcolor{gray!20}\textbf{0.529} & 0.543 & 0.518 & \cellcolor{gray!20}\textbf{0.530} & 0.534 & 0.496 & \cellcolor{gray!20}\textbf{0.514} & 35.9M & 16.5 & \cellcolor{gray!20}\textbf{15.6} \\
        \midrule
        \multirow{5}{*}{av2} & HDMapNet &Pixel-wise & 0.656 & 0.449 & \cellcolor{gray!20}0.533 & 0.635 & 0.463 & \cellcolor{gray!20}0.535 & 0.596& 0.473&\cellcolor{gray!20}0.528&0.607&0.470&\cellcolor{gray!20}0.530&35.4M&15.2& \cellcolor{gray!20}4.6\\
        & STSU & Piece-wise & 0.562&0.414&\cellcolor{gray!20}0.477&0.462&0.436&\cellcolor{gray!20}0.449&0.512&0.416&\cellcolor{gray!20}0.459&0.449&0.431&\cellcolor{gray!20}0.440&35.3M&13.0& \cellcolor{gray!20}12.4\\
        & MapTR & Piece-wise & 0.589&0.469&\cellcolor{gray!20}0.522&0.520&0.481&\cellcolor{gray!20}0.500&0.543&0.468&\cellcolor{gray!20}0.503&0.507&0.477&\cellcolor{gray!20}0.491&36.0M&13.1&  \cellcolor{gray!20}12.5\\
        & \textbf{LaneGAP(ours)} & Path-wise &0.663&0.575&\cellcolor{gray!20}\textbf{0.616}&0.578&0.552&\cellcolor{gray!20}\textbf{0.565} &0.627&0.541&\cellcolor{gray!20}\textbf{0.581}&0.569&0.546&\cellcolor{gray!20}\textbf{0.557}&35.9M&13.8& \cellcolor{gray!20}\textbf{13.0}\\
        \bottomrule
    \end{tabular}}
\caption{\textbf{Quantitative comparison of lane graph modeling under fair conditions} (the same encoder, comparable model size, the same training schedule, and vision-only modality). FPS$_{\text{net}}$ and FPS of all the experiments are measured on the same machine with one NVIDIA Geforce RTX 3090 GPU and one 24-core AMD EPYC 7402 2.8 GHz CPU, where FPS$_{\text{net}}$ is benchmarked with only network forward. Path-wise modeling achieves the best lane graph construction quality and speed. }
% \vspace{-15pt}
\label{tab:modeling}

% \vspace{-8pt}
\end{table*}

\boldparagraph{Implementation details.}
All the baselines use the same encoder consisting of ResNet50~\cite{resnet} and GKT~\cite{gkt} to transform  the  360$^\circ$ horizontal FOV multi-camera images to the unified BEV features at the resolution of $200 \times 100$ (\ie, the BEV grid size is $0.3m$).   
All the  models are controlled in comparable parameter sizes and trained for long enough epochs to ensure convergence (110 epochs on nuScenes and 24 epochs on Argoverse2). We trained all the experiments on 8 NVIDIA GeForce RTX 3090 GPUs with a total batch size of 32 (6 view images on nuScenes and 7 view images on Argoverse2). The only differences lie in the specialized decoders designed for respective modelings, which are detailed as follows.


\boldparagraph{Pixel-wise modeling.}
% \boldparagraph{Pixel-wise modeling.} 
We select HDMapNet~\cite{hdmapnet} as the pixel-wise modeling baseline. It adopts a UNet structure with a ResNet18  to output the binarized segmentation map and direction map with two branches. Meanwhile, a cross-entropy loss is applied to the segmentation map,  and  $L_2$ loss is appled to the (-1,1) normalized direction map~\cite{laneextract,zhou2018d}. To exclude the quantization error brought by classification on dense pixels, the output resolution of the UNet-shaped~\cite{ronneberger2015u} segmentation model is set to $400 \times 200$, which means the grid size is $0.15m$ (the perception range is $60m\times30m$) the same as the graph interpolation size.



\boldparagraph{Piece-wise modeling.}
STSU~\cite{stsu} is selected as the piece-wise modeling baseline, which consists of a detection branch to detect Bezier lane pieces with 4 control points and a association branch to predict the connectivity between lane pieces. It stacks 6 Transformer decoder layers to iteratively refine the predictions. $L_1$ loss is applied to the matched (0,1) normalized pieces and cross-entropy loss is applied to the connectivity. More recently, MapTR~\cite{maptr} proposes a much stronger Transformer decoder for detection branch. We further implement an additional piece-wise baseline by simply replacing the original Beizer piece detection branch of STSU with  MapTR Polyline piece detection branch. We set the number of Polyline points to 20.



\boldparagraph{Path-wise modeling.} 
The path-wise LaneGAP adopts the same decoder as the piece-wise MapTR. We utilize deformable attention~\cite{deformdetr} as the cross attention of the Transformer and use the 30-point Polyline to represent the path and exploit the features along the path with resort to deformable attention as stated in Sec.~\ref{sec:lanegap}. We stack 6 decoders and regress $(0,1)$ normalized coordinates. During training,  $L_1$ loss is applied to the matched paths and cross-entropy loss is applied to the auxiliary BEV segmentation branch.





\begin{table*}[ht!]
    \centering
    \begin{minipage}[t]{0.400\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3.5pt}
        % \tablestyle{2pt}{1.1}
        \small
        \scalebox{0.75}{
        \begin{tabular}{l  c  c  c  c  c  c c c}
            \toprule
            \multirow{2}{*}{$\mathcal{L}_{\rm auxseg}$} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} & \multirow{2}{*}{Param.} & \multirow{2}{*}{FPS}\\ 
            % \cline{2-7}
             & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$ & &  \\
            \midrule
            \xmark &  0.544 & 0.516 &\cellcolor{gray!20}0.529 & 0.421 & 0.484 &\cellcolor{gray!20}0.450 & 35.9M & 15.6 \\
    
            \cmark &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461 & 35.9M & 15.6 \\
            \bottomrule
        \end{tabular}}
        \caption{\textbf{Ablation for  auxiliary BEV segmentation branch.}}
        \label{tab:bevseg}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.280\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3.5pt}
        \small
        \scalebox{0.7}{
            \begin{tabular}{l c  c  c  c  c  c }
                % \hline
                \toprule
                \multirow{2}{*}{\shortstack[l]{Polyline\\Points}} &  \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
                % \cline{2-7}
                &  Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
                \midrule
                % \hline
                10   & 0.550 & 0.515 & \cellcolor{gray!20}0.532 & 0.423 & 0.481 & \cellcolor{gray!20}0.450  \\
                % \hline
                30   & 0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461  \\
                % 30 &110 & 0.562 & 0.514 &\cellcolor{gray!20}0.537 & 0.536 & 0.495 &\cellcolor{gray!20}0.515 \\
                % \hline
                40  & 0.566 & 0.548 &\cellcolor{gray!20}0.557 & 0.440 & 0.509 & \cellcolor{gray!20}0.472  \\
                % \hline
                \bottomrule
            \end{tabular}}
        % \vspace{-6pt}
        \caption{\textbf{Ablation for Polyline path modeling.} }
        %  Increasing the number of Polyline points for path representation leads to continuous improvement.}
        \label{tab:polyline}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.280\linewidth}
        \centering
        \renewcommand\arraystretch{1.1}
        \renewcommand\tabcolsep{3pt}
        \small
        \scalebox{0.7}{
            \begin{tabular}{l  c  c  c  c  c  c }
                % \hline
                \toprule
                \multirow{2}{*} {\shortstack[l]{Bezier \\ Points}} &  \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
                % \cline{2-7}
                & Prec. & Rec. &\cellcolor{gray!20}$F_1$  & Prec. & Rec. & \cellcolor{gray!20}$F_1$  \\
                % \hline
                \midrule
                3  & 0.480 & 0.483 & \cellcolor{gray!20}0.481 & 0.309 & 0.447 & \cellcolor{gray!20}0.365 \\
                % \hline
                5  & 0.514 & 0.507 &\cellcolor{gray!20}0.510 &0.375 & 0.477 & \cellcolor{gray!20}0.477  \\
                10  & 0.498 & 0.503 & \cellcolor{gray!20}0.501 & 0.334 & 0.463 & \cellcolor{gray!20}0.388  \\
                % \hline
                \bottomrule
            \end{tabular}}
        % \vspace{-6pt}
        \caption{\textbf{Ablation for Bezier path representation.}}
        %  Too few Bezier control points cannot describe the shape and too many Bezier control points deteriorate the performance. }
        \label{tab:bezier}
    \end{minipage}
    % \vspace{-15pt}
    \end{table*}

\subsection{Quantitative Comparision}
% refer to Tab.~\ref{tab:modeling}
Tab.~\ref{tab:modeling} compares path-wise modeling with pixel-wise modeling and piece-wise modeling \wrt their accuracy, model size, FPS$_{\text{net}}$, FPS. All the FPS$_{\text{net}}$ and FPS are measured on the same machine with one NVIDIA Geforce RTX 3090 GPU and one 24-core AMD EPYC 7402 2.8 GHz CPU, where FPS$_{\text{net}}$ is benchmarked with only network forward.

\boldparagraph{Highlights.} Path-wise modeling achieves  best $F_1$ score on the both subgraph around the junction points and the overall graph across two popular datasets (nuScenes and Argoverse2), while running at the fastest inference speed. The designed Path2Graph algorithm has negligible cost in translating predicted paths into directed lane graph (from 16.5 FPS to 15.6 FPS). 


\boldparagraph{Path-wise \vs pixel-wise.}
Thanks to the high-resolution output of the segmentation model, pixel-wise modeling exhibits comparable precision for sub graph construction and overall graph construction. Nevertheless, the notorious fragmentation  and oversmooth issues of segmentation methods make it hard to keep the continuity across pixels and  distinguish the fine-grained subgraph around the junction points, leading to much lower recall compared to path-wise modeling.
The sophiscated and prone-to-fail post-processing also adds enormous cost at inference to convert the segmentation output into vectorized lane graph. Compared to it, the path-wise modeling demonstrates higher lane graph construction quality and 2x faster inference speed on both nuScenes and Argoverse2 datasets.


\boldparagraph{Path-wise \vs piece-wise.} Path-wise modeling outperforms piece-wise modeling on all metrics (precision, recall, parameter size, FPS). Even euipped with the same advanced Transformer decoder proposed in \cite{maptr}, our path-wise LaneGAP still significantly outperforms piece-wise MapTR, which validates the effectiveness of our proposed continuity-preserving modeling.




\subsection{Qualitative Comparison}
With the trained models in Tab.~\ref{tab:modeling}, Fig.~\ref{fig:qualitative_comparison} compares path-wise modeling with pixel-wise modeling and piece-wise modeling on complicated lane graphs with more than 4 junction points across different datasets.

\boldparagraph{Highlights.} Path-wise modeling demonstrates better lane graph construction quality than pixel-wise and piece-wise modeling on extremely challenging lane graphs, well preserving the continuity of lane.

\boldparagraph{Path-wise \vs pixel-wise.} As shown in Fig.~\ref{fig:qualitative_comparison}, the pixel-wise HDMapNet performs well on non-junction area. But for the sub-graph around the junction points, the segmentation model struggles to distinguish the fine-grained topology, and the post-processing is prone to fail to generate a decent vectorized lane graph, which aligns with the much lower Junction TOPO metrics in Tab.~\ref{tab:modeling}. Thanks to our rational modeling which treats each path as a whole for learning, we can capture the fine-grained topology with severe overlap and preserve the continuity of the lane.



\boldparagraph{Path-wise \vs piece-wise.} The quality of lane graph construted by  piece-wise modeling depends on the accuracy of both piece detection and connectivity prediction. As shown in Fig.~\ref{fig:qualitative_comparison}, the piece-wise MapTR either detects jagged lane pieces or produces wrong connections, leading to incomplete and inaccurate lane graph after Piece2Graph post-processing. In contrast, our proposed path-wise modeling encodes the connectivity between pieces into the continuous path representation, which is easy to learn and robust.





    

\subsection{Ablation Study}
We ablate the design choices with a 24-epoch training schedule of path-wise modeling on nuScenes dataset by default without specification. And we report Junction TOPO and TOPO only on the directed lane graph.

\boldparagraph{Effectiveness of auxiliary BEV segmentation branch.} Tab.~\ref{tab:bevseg} shows the auxiliary BEV segmentation branch can improve the performance by 1\% $F_1$ of Junction TOPO and 1.1\% $F_1$ of TOPO without adding cost at inference. 

\boldparagraph{Polyline path representation.}
As shown in Tab.~\ref{tab:polyline}, the performance increases with adding more points for Polyline modeling under a 24-epoch schedule.


% \vspace{-3pt}
\boldparagraph{Bezier path representation.}
Tab.~\ref{tab:bezier} shows that adding the number of control points from 3 to 5 improves 2.9\% $F_1$ of Junction TOPO and 11.2\% $F_1$ of TOPO, indicating too few Bezier control points cannot accurately describe the arbitrary shape in the broad perception. While adding the number of control points from 5 to 10 incurs an accuracy drop. Compared to Tab.~\ref{tab:polyline}, Bezier path representation is inferior to Polyline path representation, so we choose Polyline path representation as the default setting in LaneGAP.

% \vspace{-5pt}
\boldparagraph{Modality.}
Tab.~\ref{tab:modality} shows that LiDAR modality builds a more accurate lane graph than vision modality (6.1\% higher $F_1$ of Junction TOPO and 4.8\% higher $F_1$ of TOPO). Fusing them further boosts performance.

\begin{table}[ht!]
    % \setlength{\tabcolsep}{3pt}
    \vspace{-5pt}
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l  c  c  c  c  c  c }
        \toprule
        \multirow{2}{*}{Modality} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
        % \cline{2-7}
         & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
        \midrule
        Vision-only &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461 \\

        LiDAR-only &  0.608 & 0.594 &\cellcolor{gray!20}0.600 & 0.467 & 0.559 &\cellcolor{gray!20}0.509 \\

        Vision \& LiDAR  &0.630 & 0.600 &\cellcolor{gray!20}0.615 & 0.504 & 0.561 &\cellcolor{gray!20}0.531 \\
        \bottomrule
    \end{tabular}}
% \vspace{-6pt}
\caption{\textbf{Ablation for modality.} }
\label{tab:modality}
% \vspace{-10pt}
\end{table}


\boldparagraph{Training schedule.}
Tab.~\ref{tab:schedule} shows that adding more epochs mainly increases the TOPO metric and benefits little Junction TOPO, especially the recall of Junction TOPO. The 110-epoch trained multi-modality experiment further pushes the performance of lane graph construction.



\begin{table}[ht!]
    % \setlength{\tabcolsep}{3pt}
    \vspace{-5pt}
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l c  c  c  c  c  c  c }
        \toprule
        \multirow{2}{*}{Modality} & \multirow{3}{*}{Epoch} & \multicolumn{3}{c}{Junction TOPO} & \multicolumn{3}{c}{TOPO} \\ 
        % \cline{2-7}
        & & Prec. & Rec. & \cellcolor{gray!20}$F_1$  & Prec. & Rec. &\cellcolor{gray!20}$F_1$  \\
        \midrule
        Vision-only &24 &  0.554 & 0.524 &\cellcolor{gray!20}0.539 & 0.432 & 0.493 &\cellcolor{gray!20}0.461 \\
        Vision-only &110 & 0.591 & 0.539 &\cellcolor{gray!20}0.564 & 0.547 & 0.511 &\cellcolor{gray!20}0.529 \\
        \midrule
        Vision \& LiDAR  & 24 &0.630 & 0.600 &\cellcolor{gray!20}0.615 & 0.504 & 0.561 &\cellcolor{gray!20}0.531 \\
        Vision \& LiDAR  & 110&0.668 & 0.601 & \cellcolor{gray!20}0.632 & 0.620 & 0.576 & \cellcolor{gray!20}0.597 \\
        \bottomrule
    \end{tabular}}
% \vspace{-6pt}
\caption{\textbf{Ablation for training schedule.} }
\label{tab:schedule}
% \vspace{-10pt}
\end{table}


