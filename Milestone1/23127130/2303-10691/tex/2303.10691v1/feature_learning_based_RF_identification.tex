 
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\pdfoutput=1
\documentclass[journal]{IEEEtran}
%\documentclass[12pt,draftcls,onecolumn,peerreview]{IEEEtran}
\IEEEoverridecommandlockouts
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{color}
\usepackage{multirow}
\usepackage{subcaption}




% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Multi-Channel Attentive Feature Fusion for Radio Frequency Fingerprinting}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
\author{Yuan Zeng, Yi Gong, Jiawei Liu, Shangao Lin, Zidong Han, Ruoxiao Cao, Kaibin Huang and Khaled Ben Letaief 
  % <-this % stops a space
  \thanks{Y. Zeng is with the Academy for Advanced Interdisciplinary Studies, Southern University of Science and Technology (SUSTech), Shenzhen, 518055, P. R. China. (e-mail: zengy3@sustech.edu.cn).
	
	Y. Gong, S. Lin and Z. Han are with the Department of Electrical and Electronic Engineering, SUSTech, Shenzhen, P. R. China. (e-mail: gongy@sustech.edu.cn, linsa2019@mail.sustech.edu.cn, hanzd@sustech.edu.cn).
	
	J. Liu and K. Huang are with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong. (e-mail: liujw@eee.hku.hk, huangkb@eee.hku.hk).
	
	R. Cao and K. B. Letaief are with the Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong. (e-mail: rcaoah@connect.ust.hk, eekhaled@ust.hk).}% <-this % stops a space
  % <-this % stops a space
}

%\author{\IEEEauthorblockN{Yuan Zeng, Meng Zhang, Fei Han and Yi Gong, \emph{senior member}, \emph{IEEE}}\\%1\textsuperscript{st}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{Southern University of Science and Technology, Shenzhen, China}\\
%\textit{email: 11649162@mail.sustc.edu.cn, zengy3, hanf, gongy@sustc.edu.cn}
%City, Country \\
%email address}
%\and
%\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address}
%}
% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Report}%~Vol.~14, No.~8,
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
 Radio frequency fingerprinting (RFF) is a promising device authentication technique for securing the Internet of things. It exploits the intrinsic and unique hardware impairments of the transmitters for RF device identification. In real-world communication systems, hardware impairments across transmitters are subtle, which are difficult to model explicitly. Recently, due to the superior performance of deep learning (DL)-based classification models on real-world datasets, DL networks have been explored for RFF. Most existing DL-based RFF models use a single representation of radio signals as the input. Multi-channel input model can leverage information from different representations of radio signals and improve the identification accuracy of the RF fingerprint. In this work, we propose a novel multi-channel attentive feature fusion (McAFF) method for RFF. It utilizes multi-channel neural features extracted from multiple representations of radio signals, including IQ samples, carrier frequency offset, fast Fourier transform coefficients and short-time Fourier transform coefficients, for better RF fingerprint identification. The features extracted from different channels are fused adaptively using a shared attention module, where the weights of neural features from multiple channels are learned during training the McAFF model. In addition, we design a signal identification module using a convolution-based ResNeXt block to map the fused features to device identities. To evaluate the identification performance of the proposed method, we construct a WiFi dataset, named WFDI, using commercial WiFi end-devices as the transmitters and a Universal Software Radio Peripheral (USRP) as the receiver. Experimental results on WFDI show that the proposed McAFF model outperforms single-channel-based RFF models in terms of identification accuracy and robustness. Moreover, comparative experiments between the proposed framework and a baseline RFF model without attention-based feature fusion show that the proposed shared attention module has the advantage of improving the identification accuracy of RF fingerprint.  
\end{abstract}% 

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
  Radio frequency fingerprinting, feature fusion, convolutional neural network, attention mechanism
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.

%\IEEEpeerreviewmaketitle



\section{Introduction}
\label{intro}
With the help of recent technology advances in wireless electronic devices and beyond fifth-generation (B5G) communication systems, Internet of Things (IoT) technology has been further developed and plays an increasingly important role in our daily life, such as facilitating human-to-machine communication or machine-to-machine communication. The evolution of IoT tends to enable ubiquitous connectivity, where billions of tiny embedded wireless devices, enabling sensing, computing, and communications, are deployed everywhere. Wireless security is a critical challenge for IoT applications, since mobile devices in a wireless communication network are vulnerable to malicious attacks when operating in an open environment. Impersonation attacks are one of the most common and threatening malicious attacks in wireless communication networks. Device authentication that validates whether the devices or users are legitimate enables to protect the wireless devices from impersonation attacks and improves the security of IoT. Traditional authentication methods were designed by employing software addresses, such as Internet Protocol (IP) or Media Access Control (MAC), as identity and using classical cryptography-based authentication techniques\ \cite{zou2016survey}. Cryptography algorithms usually rely on complicated mathematical operations or protocols\ \cite{he2014analysis}. However, due to limitations in power consumption and computation resources, such complex cryptography algorithms may be undesirable in many IoT applications such as smart cities\ \cite{zanella2014internet} and intelligent industries\ \cite{lu2014connected}. An efficient alternative tool for wireless security is radio frequency fingerprinting (RFF), which uses waveform-level imperfections imposed by the RF circuit to obtain a fingerprint of the wireless device. The imperfections normally include in-phase and quadrature (IQ) imbalance, phase offset, frequency offset, sampling offset, and phase noise\ \cite{johnson1991physical}, which are unique and can hardly be imitated by adversarial devices.

RFF is a standard pattern recognition problem and usually consists of two stages: learning and inference. During the learning phase, signals are gathered from end-devices and processed to learn a classification model at a server. After that, the server will use the trained model to authenticate end-devices during the inference phase. Compared with conventional cryptography-based security schemes, RFF, by operating at the physical layer, does not impose any additional computational burden and power consumption on end-devices. On the other hand, the receiver/server is usually equipped with sufficient computational resources and energy, which is capable of gathering signals from multiple end devices and doing signal learning and inference. Such arrangement of computational resources and capabilities among the receiver/server and end-devices is particularly desirable for many IoT applications, since most end-devices are low-cost with limited computational and energy resources.

Traditional RFF methods are proposed based on carefully hand-crafting specialized feature extractors and classifiers. Manually extracting handcrafted features requires prior comprehensive knowledge of the communication system, such as channel state and communication protocol. In addition, it is difficult to extract features of hardware imperfection accurately, since the hardware imperfections are interrelated. By performing feature extraction and signal classification at two separate stages, traditional RFF methods may work well when the extracted features are distinguishable for classifiers. However, the identification performance of classifiers is highly dependent on the quality of extracted features, and an efficient feature extraction method for one scenario may easily fail in another.

Recently, deep learning (DL)-based RFF methods have been proposed to fingerprint radios through learning of the hardware impairments\ \cite{sankhe2019oracle, riyaz2018deep}, since DL-based methods automatically optimize feature extraction by minimizing the identification error. Different neural network architectures, such as convolutional neural networks (CNNs)\ \cite{merchant2018deep}, recurrent neural networks (RNNs)\ \cite{rajendran2018deep} and long short-term memory (LSTM)\ \cite{das2018deep}, have been explored for automatic RFF and shown better RF device identification performance than traditional RFF methods. Early DL-based methods mainly use the received IQ samples as the input of a DL network\ \cite{al2020exposing, gong2020unsupervised, pan2019specific}. More recently, a few DL-based RFF methods tend to explore the specific characteristics of radio signals, such as bispectrum\ \cite{gong2020unsupervised}, Hilbert spectrum\ \cite{pan2019specific} and differential constellation trace figure\ \cite{peng2019deep}, for RFF. Utilizing multiple signal representations as the input data of an RFF model may further improve the performance of RFF models, since different signal representations can provide different views of the radio signals. In\ \cite{peng2018design}, a hybrid classification framework was proposed to weight multiple features of radio signals with pre-calculated weights for RFF. However, obtaining an efficient fusion of different representations is a complex task and pre-calculated weights may produce sub-optimal identification results in practical applications. We thus face the problem of designing an effective learning method to learn multi-channel fusion of different signal representations in an end-to-end manner for better RF device identification. %On the other hand, a few recent research works studied the robustness of the DL models in non-stationary wireless communication environments, which showed that the RFFI 

In general, training and test data are collected at different times and locations. However, hardware impairments may change across days due to temperature or voltage oscillations, there may be only limited samples for learning, and wireless channel may become unstable due to time-varying fading and noise. Although a lot of progress has been made in RFF on radio signals with high signal-to-noise ratio (SNR) conditions, the identification performance still drops sharply when the end-devices are deployed in the presence of electromagnetic interference, environment noises, or other low SNR conditions. A few recent DL-based methods\ \cite{riyaz2018deep, cekic2020robust} proposed model-based augmentation strategies to improve the robustness of the DL-based RFF models, and validated the proposed strategies using simulated RF devices and nonlinear effects. These methods provided possible solutions to improve identification accuracy when practical RF devices and nonlinear characteristics are similar to the simulations. It is still a challenging task to improve the robustness of RFF methods in noisy environments.   % , and the data distribution may shift across days and locations due to clock shifts and variations in the wireless communication environmental conditions. Such nonlinear characteristics may cause the identification accuracy of DL-based RFF models to plummet. This is because DL-based RFF models assume that the input data are drawn from independent and identically distributed random variables, and they tend to learn the most easily distinguishable features for better identification accuracy, rather than the subtle nonlinear characteristics for stable fingerprinting.

To tackle the above issues, this work proposes a multi-channel attentive feature fusion (McAFF) method to adaptively fuse multi-channel neural features for RFF. The network fuses neural features extracted from multi-channel inputs including IQ samples, carrier frequency offset (CFO), fast Fourier transform (FFT) coefficients, and short-time Fourier transform (STFT) coefficients in an end-to-end learning manner. In addition, a shared attention module is designed to learn weights regarding feature importance for RFF. The entire model is trained in a supervision manner using WiFi data.

To evaluate the proposed McAFF model, we design an RF signal acquisition system, by using a Universal Software Radio Peripheral (USRP) software defined radio (SDR) platform as the receiver and 72 commercial WiFi end-devices as the target devices. We collect WiFi signal transmitted using the standard IEEE 802.11n protocol over several experiment days, and use the legacy-short training field (L-STF)  sequence in the collected uplink frames to construct a dataset for RFF, named WFDI. Extensive experiments are conducted to evaluate the identification performance of the proposed McAFF model. Experimental results demonstrate the effectiveness of the proposed McAFF model on WiFi device identification. The main contributions of our work are summarized as follows.
\begin{itemize}
  \item A novel McAFF model that exploits the complementary information from different representations of radio signals is introduced for RF fingerprint identification. It addresses RFF by optimizing the combination of multi-channel features and utilizes a convolution layer to attend to important features for better identification performance of RF fingerprint.
  \item We design a signal identification module using a convolution-based ResNeXt block to map the fused neural features to device identities for automatic RFF.
  \item We design a WiFi signal acquisition system that collects WiFi signal of 72 commercial WiFi end-devices and construct a new experimental dataset WFDI aimed at evaluating the identification performance of DL-based RFF methods.
  \item We provide an extensive ablation study of different signal representations and analyze the rationality of the proposed attention module through comparative experiments. Experimental results demonstrate that the proposed McAFF model outperforms baseline models that use a specific signal representation as input in terms of identification accuracy. In addition, we analyze the impact of different data scales and collection dates on the identification performance. Experimental results show that the proposed McAFF model is more robust against to noisy radio signals, and changes in data scale and collection time.
\end{itemize}

The rest of the paper is organized as follows. Related work is given in Section\ \ref{related}. The problem statement is given in Section\ \ref{pro}. In Section\ \ref{data}, we give details about how we generate our Dataset. Then in Section\ \ref{rep}, we briefly introduce the signal pre-processing and representation algorithms. Later, we describe the proposed McAFF model-based RFF method in detail in Section\ \ref{met}. The performance of the proposed method is illustrated and analyzed with experiments in Section\ \ref{exp}. Finally, conclusions are drawn in Section\ \ref{con}.
%zou2016survey: Y. Zou, J. Zhu, X. Wang, and L. Hanzo, “A survey on wireless security: Technical challenges, recent advances, and future trends,” Proc. IEEE, vol. 104, no. 9, pp. 1727–1765, 2016.
%he2014analysis: D. He and S. Zeadally, “An analysis of RFID authentication schemes for internet of things in healthcare environment using elliptic curve cryptography,” IEEE Internet Things J., vol. 2, no. 1, pp. 72–83, 2015.
%johnson1991physical: E. Johnson, “Physical Limitations on Frequency and Power Parameters of Transistors,” in 1958 IRE International Convention Record, vol. 13. IEEE, 1966, pp. 27–34.
%zanella2014internet: A. Zanella, N. Bui, A. Castellani, L. Vangelista, and M. Zorzi, “Internet of things for smart cities,” IEEE Internet Things J., vol. 1, no. 1, pp. 22–32, 2014.
%lu2014connected: N. Lu, N. Cheng, N. Zhang, and X. Shen, “Connected vehicles: Solutions and challenges,” IEEE Internet Things J., vol. 1, no. 4, pp. 289–299, 2014.
%merchant2018deep: K. Merchant, S. Revay, G. Stantchev, and B. Nousain, “Deep learning for RF device fingerprinting in cognitive communication networks,” IEEE J. Sel. Topics Signal Process., vol. 12, no. 1, pp. 160–167, 2018.
%rajendran2018deep S. Rajendran, W. Meert, D. Giustiniano, V. Lenders, and S. Pollin, “Deep learning models for wireless signal classification with distributed lowcost spectrum sensors,” IEEE Transactions on Cognitive Communications and Networking, vol. 4, no. 3, pp. 433–445, 2018.
%das2018deep: R. Das, A. Gadre, S. Zhang, S. Kumar, and J. M. Moura, “A deep learning approach to IoT authentication,” in Proc. IEEE Int. Conf. Commun. (ICC), Kansas City, MO, USA, May 2018, pp. 1–6.
%al2020exposing: A. Al-Shawabka, F. Restuccia, S. D’Oro, T. Jian, B. C. Rendon, N. Soltani, J. Dy, K. Chowdhury, S. Ioannidis, and T. Melodia, “Exposing the fingerprint: Dissecting the impact of the wireless channel on radio fingerprinting,” in Proc. IEEE Int. Conf. Comput. Commun. (INFOCOM), Jul. 2020, pp. 646–655.
%gong2020unsupervised: J. Gong, X. Xu, and Y. Lei, “Unsupervised specific emitter identification method using radio-frequency fingerprint embedded InfoGAN,” IEEE Trans. Inf. Forensics Security, vol. 15, pp. 2898–2913, 2020.
%pan2019specific: Y. Pan, S. Yang, H. Peng, T. Li, and W. Wang, “Specific emitter identification based on deep residual networks,” IEEE Access, vol. 7, pp. 54 425–54 434, 2019.
%peng2019deep: L. Peng, J. Zhang, M. Liu, and A. Hu, “Deep learning based RF fingerprint identification using differential constellation trace figure,” IEEE Trans. Veh. Technol., vol. 69, no. 1, pp. 1091 – 1095, 2020.
%sankhe2019oracle: K. Sankhe, M. Belgiovine, F. Zhou, S. Riyaz, S. Ioannidis, and K. Chowdhury, “ORACLE: Optimized Radio clAssification through Convolutional neuraL nEtworks,” in IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEE, 2019, pp. 370–378.
%riyaz2018deep:S. Riyaz, K. Sankhe, S. Ioannidis, and K. Chowdhury, “Deep Learning Convolutional Neural Networks for Radio Identification,” IEEE Communications Magazine, vol. 56, no. 9, pp. 146–152, Sept 2018.
%riyaz2018deep: F. Restuccia, S. D’Oro, A. Al-Shawabka, M. Belgiovine, L. Angioloni, S. Ioannidis, K. Chowdhury, and T. Melodia, “DeepRadioID: Real-Time Channel-Resilient Optimization of Deep Learning-based Radio Fingerprinting Algorithms,” in Proc. of the ACM International Symposium on Mobile Ad Hoc Networking and Computing (ACM MobiHoc). ACM, 2019, pp. 51–60.

\section{Related work}
\label{related}
\subsection{RF Fingerprinting}
A variety of approaches have been proposed for RFF. Early RFF methods typically perform feature extraction and classification separately. Existing hand-tailored feature extraction techniques can be roughly divided into three categories: transient-based (TB) techniques, spectrum-based (SB) techniques, and modulation-based (MB) techniques. TB methods extract the signal variations in the trun-on/off transient, such as the transient signal envelope\ \cite{patel2014improving} and phase offset\ \cite{knox2012practical}. These methods use time domain signal processing algorithms to extract features from received signals.  %requires high performance equipments with good sensitivity and is very sensitive to device location and antenna polarization direction. 
SB methods extract frequency domain features, such as power spectrum density (PSD)\ \cite{suski2008using, 7122344}, Hilbert spectrum\ \cite{pan2019specific}, and time-frequency statistics\ \cite{bihl2016feature}. MB features including IQ offset\ \cite{brik2008wireless}, clock skew\ \cite{kohno2005remote, jana2009fast}, CFO\ \cite{leonardi2017air, hua2018accurate}, sampling frequency offset\ \cite{vo2016fingerprinting}, etc, can be extracted from the received baseband signal. During the classification process, the authenticator will first feed the extracted features to train the classifier and then infer the device identity. Typical classification algorithms include support vector machine (SVM)\ \cite{brik2008wireless} and K-nearest neighbor (KNN)\ \cite{kennedy2008radio}. The identification performance of these classifiers relies heavily on feature extraction, and hand-tailored feature extraction techniques require comprehensive knowledge of communication technology and protocol, which limits the practical applications.

In the past few years, DL, which uses a cascade of multiple layers of nonlinear processing units for feature extraction and transformation, has achieved great success in image classification and speech recognition. Recently, it has also been studied to solve physical-layer communication problems including channel estimation\ \cite{lippmann1987introduction, 9018199}, modulation recognition\ \cite{o2016convolutional, zeng2019spectrum, 9336326} and RFF\ \cite{soltanieh2020review, shen2021radio, 9913208}. Compared to traditional RFF methods, DL-based RFF methods use DNNs to automatically extract more distinguishable and high-level features by learning a mapping strategy from training data. A CNN-based RFF model, operating on the error signal obtained after subtracting out an estimated ideal signal from frequency-corrected data, was proposed in\ \cite{merchant2018deep}. Riyaz et al.\ \cite{riyaz2018deep} evaluated the effectiveness of an optimized CNN architecture on RFF and introduced artificial impairments to improve the identification accuracy. Cekic et al.\ \cite{cekic2020robust} investigated the identification performance of a complex-valued DNN on WiFi and ADS-B signals. Sankhe et al.\ \cite{sankhe2019oracle} used a 16-node USRP SDR testbed and an external database of more than 100 WiFi devices to generate a WiFi dataset and introduced a robust CNN architecture for device identification using raw IQ samples. In\ \cite{sankhe2019no}, an impairment hopping spread spectrum method that identifies a radio through a random pseudo-noise binary sequence was proposed. Shen et al.\ \cite{shen2021radio} evaluated the impact of different signal representations including spectrogram, IQ samples, and FFT coefficients on a CNN-based RFF model, and designed a hybrid classifier using the softmax output and CFO to improve the RF fingerprint identification performance of the CNN-based RFF scheme on LoRa devices. Shawabka et al.\ \cite{al2020exposing} investigated the identification performance of CNN-based RFF models on a large-scale dataset with 10,000 WiFi and ADS-B devices. In contrast, this work proposes to improve RF fingerprint identification performance using multi-channel attention fusion and CNN.

\subsection{Multi-channel Signal Classification}
Our work is more closely related to multi-channel signal classification, where multiple signal representations are used to provide classifiers with richer information from different views. Peng et al.\ \cite{peng2018design} proposed to combine four modulation features including differential constellation trace figure, CFO, modulation offset, and IQ offset with the weights pre-calculated according to the estimated SNR and evaluated the effectiveness of the proposed feature fusion method on RFF using a KNN classifier. In\ \cite{9106397} a multi-channel learning framework was proposed to exploit the complementary information from IQ multi-channel, I channel, and Q channel data for automatic modulation recognition. Lin et al.\ \cite{lin2022modulation} introduced a dual-channel spectrum fusion module to include an original signal channel and a filtered signal channel for modulation recognition. Inspired by existing multi-channel-based classifiers, we use four signal representations, namely IQ samples, CFO, FFT coefficients, and STFT coefficients, as the input of our RFF model. However, in contrast to combining the four signal features using manually designed weights, our study tackles the problem of adaptively weight features extracted from the four signal representations for RFF.

\subsection{Attention-based Fusion}
Recently, attention mechanism\ \cite{vaswani2017attention} allows the network to automatically find target related features and enhance the related features while diminishing other features, and it has achieved state-of-the-art performance on various natural language processing and computer vision tasks\ \cite{moritz2021semi, zhao2020noisy, woo2018cbam}. Fan et al.\ \cite{fan2020spatial} proposed a deep attention-based fusion algorithm to dynamically control the weights of the spatial and spectral features and combine them deeply. Xu et al.\ \cite{xu2021attention} presented a co-attention mechanism to guide the fusion of RGB and infrared multi-spectral information for semantic segmentation. For wireless communication, attention mechanisms have been investigated in DL-based modulation classification models to improve classification performance. O'Shea et al.\ \cite{o2016radio} designed an attention mechanism to learn a localization network for blindly synchronizing and normalizing radio signals. Lin et al.\ \cite{lin2022learning} proposed a time-frequency attention mechanism to learn which channel, time, and frequency information are more important for modulation recognition. In this work, we design a shared attention module to adaptively fuse neural features extracted from multi-channel inputs.

%kohno2005remote T. Kohno, A. Broido, and K. C. Claffy, “Remote physical device fingerprinting,” IEEE Transactions on Dependable and Secure Computing, vol. 2, no. 2, pp. 93–108, April 2005.
%jana2009fast S. Jana and S. K. Kasera, “On fast and accurate detection of unauthorized wireless access points using clock skews,” IEEE Transactions on Mobile Computing, vol. 9, no. 3, pp. 449–462, 2010.
%leonardi2017air M. Leonardi, L. Di Gregorio, and D. Di Fausto, “Air traffic security: Aircraft classification using ADS-B message’s phase-pattern,” Aerospace, vol. 4, no. 4, p. 51, 2017.
%hua2018accurate J. Hua, H. Sun, Z. Shen, Z. Qian, and S. Zhong, “Accurate and efficient wireless device fingerprinting using channel state information,” in IEEE International Conference on Computer Communications, 2018, pp. 1700–1708.
%suski2008using W. C. Suski II, M. A. Temple, M. J. Mendenhall, and R. F. Mills, “Using spectral fingerprints to improve wireless network security,” in IEEE GLOBECOM 2008 IEEE Global Telecommunications Conference.
%brik2008wireless V. Brik, S. Banerjee, M. Gruteser, and S. Oh, “Wireless device identification with radiometric signatures,” in Proc. ACM Int. Conf. Mobile Comput. Netw. (MobiCom), San Francisco, CA, USA, Sep. 2008, pp. 116–127.
%bihl2016feature T. J. Bihl, K. W. Bauer, and M. A. Temple, “Feature selection for RF fingerprinting with multiple discriminant analysis and using ZigBee device emissions,” IEEE Trans. Inf. Forensics Security, vol. 11, no. 8, pp. 1862–1874, 2016.
%bihl2016feature J. Zhang, F. Wang, O. A. Dobre, and Z. Zhong, “Specific emitter identification via Hilbert–Huang transform in single-hop and relaying scenarios,” IEEE Trans. Inf. Forensics Security, vol. 11, no. 6, pp. 1192–1205, 2016.
%bihl2016feature H. Yuan and A. Hu, “Preamble-based detection of Wi-Fi transmitter RF fingerprints,” IET Electron. Lett., vol. 46, no. 16, pp. 1165–1167, 2005.
%patel2014improving H. J. Patel, M. A. Temple, and R. O. Baldwin, “Improving ZigBee device network authentication using ensemble decision tree classifiers with radio frequency distinct native attribute fingerprinting,” IEEE Trans. Rel., vol. 64, no. 1, pp. 221–233, 2015.
%knox2012practical D. A. Knox and T. Kunz, “Practical RF fingerprints for wireless sensor network authentication,” in Proc. Int. Wireless Commun. Mobile Computing Conf. (IWCMC), Cyprus, Aug. 2012, pp. 531–536
%vo2016fingerprinting T. D. Vo-Huu, T. D. Vo-Huu, and G. Noubir, “Fingerprinting Wi-Fi devices using software defined radios,” in Proc. ACM Conf. Security Privacy in Wireless and Mobile Networks(WiSec), Darmstadt, Germany, Jul. 2016, pp. 3–14.
%brik2008wireless V. Brik, S. Banerjee, M. Gruteser, and S. Oh, “Wireless device identification with radiometric signatures,” in Proceedings of the 14th ACM International Conference on Mobile Computing and Networking, 2008, pp. 116–127 
%kennedy2008radio I. O. Kennedy, P. Scanlon, F. J. Mullany, M. M. Buddhikot, K. E. Nolan, and T. W. Rondeau, “Radio transmitter fingerprinting: A steady state frequency domain approach,” in 2008 IEEE 68th Vehicular Technology Conference, 2008, pp. 1–5.
%lippmann1987introduction R. Lippmann, “An introduction to computing with neural nets,” IEEE Assp magazine, vol. 4, no. 2, pp. 4–22, 1987.
%o2016convolutional Convolutional radio modulation recognition networks
%zeng2019spectrum our WCL paper:Spectrum Analysis and Convolutional Neural Network for Automatic Modulation Recognition
%soltanieh2020review N. Soltanieh, Y. Norouzi, Y. Yang, and N. C. Karmakar, “A review of radio frequency fingerprinting techniques,” IEEE Journal of Radio Frequency Identification, vol. 4, no. 3, pp. 222–233, 2020.
%shen2021radio Radio frequency identification for Lora using spectrogram and CNN
%vaswani2017attention: attention is all you need
%sankhe2019oracle: ORACLE: Optimized Radio clAssification through Convolutional neuraL nEtworks
%merchant2018deep Deep learning for RF device fingerprinting in cognitive communication networks
%cekic2020robust Robust Wireless Fingerprinting: Generalizing Across Space and Time
%shen2021radio Radio Frequency Fingerprint Identification for LoRa Using Spectrogram and CNN
%riyaz2018deep Deep Learning Convolutional Neural Networks for Radio Identification
%sankhe2019no No Radio Left Behind: Radio Fingerprinting Through Deep Learning of Physical-Layer Hardware Impairments
%al2020exposing Exposing the Fingerprint: Dissecting the Impact of the Wireless Channel on Radio Fingerprinting
%peng2018design Design of a Hybrid RF Fingerprint Extraction and Device Classification Scheme
%lin2022modulation Modulation Recognition Using Signal Enhancement and Multi-Stage Attention Mechanism 
%moritz2021semi N. Moritz, T. Hori, and J. L. Roux, “Semi-supervised speech recognition via graph-based temporal classification,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6548–6552.
%zhao2020noisy Y. Zhao and D. Wang, “Noisy-reverberant speech enhancement using denseunet with time-frequency attention,” Submitted to Conference of the International Speech Communication Association (INTERSPEECH), 2020.
%woo2018cbam S. DCGAN, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional block attention module,” in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 3–19.
%o2016radio T. J. O’Shea, L. Pemula, D. Batra, and T. C. Clancy, “Radio transformer networks: Attention models for learning to synchronize in wireless systems,” in Asilomar Conference on Signals, Systems and Computers, 2016, pp. 662–666.
%lin2022learning S. Lin, Y. Zeng, and Y. Gong, “Learning of frequency-time attention mechanism for automatic modulation recognition,” IEEE wirless communication letter, 2021.
%fan2020spatial SPATIAL AND SPECTRAL DEEP ATTENTION FUSION FOR MULTI-CHANNEL SPEECH SEPARATION USING DEEP EMBEDDING FEATURES
%xu2021attention Attention fusion network for multi-spectral semantic segmentation   
\section{Problem Statement}
\label{pro}
In wireless communication systems, the received signal is generally down-converted to baseband, filtered, and sampled, and the complex baseband samples are processed to extract the information bits. The baseband samples are not only degraded by non-ideal channel and noise but also related to non-idealities of RF and analog parts, including sampling clock offset, power amplifier, phase noise, and CFO. In this section, we first briefly introduce the origin and representation of several classic RF hardware impairments, including IQ imbalance, CFO, and power amplifier (PA) nonlinearity, in the form of mathematical models. Later, we introduce the task of RFF.


\subsection{Mathematical Model of RF Impairments}
Let $x_{b}(t)$ denote the transmitted baseband complex signal at time $t$, $x_{b,I}(t)$ and $x_{b,Q}(t)$ denote the in-phase (I) and quadrature (Q) information of the baseband signal $x_{b}(t)$, respectively. The complex baseband signal $x_{b}(t)$ can be expressed as
\begin{equation}
  x_{b}(t)=x_{b,I}(t)+jx_{b,Q}(t)=r(t)e^{j\phi(t)},
\end{equation}
where $j=\sqrt{-1}$ denotes the imaginary unit, $r(t)$ and $\phi(t)$ denote the amplitude and phase of $s(t)$, respectively. The ideal RF signal emitted from a wireless transmitter can be modeled as
\begin{equation}
  x(t)=x_{b,I}(t)cos(2\pi f_ct)+x_{b,Q}(t)sin(2\pi f_ct),
\end{equation}
where $x(t)$ is the transmitted RF signal without hardware impairment, and $f_c$ is the carrier frequency.  However, the transmitted RF signal generated by practical transmitters will be distorted due to the inevitable hardware imperfections in the transmitters' analog RF chain, as shown in Fig.\ \ref{fig:transmitter_distortion}. IQ imbalance, CFO, and nonlinear distortions of the power amplifier are three typical RF impairments in actual hardware implementations, they can be modeled as follows:

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.8]{transmitter_distortion.pdf}
  \caption{The origins of signal distortions in the transmitter.}
  \label{fig:transmitter_distortion}
\end{figure}

\textbf{IQ imbalance:} In the RF chain of a transmitter, the in-phase (I) and quadrature (Q) baseband signals are up-converted to the cosine and sine carrier waves using two independent quadrature mixers. The cosine and sine carrier waves are generated by the local oscillator (LO) of the transmitter. The ideal sine carrier wave is a copy of the cosine carrier wave delayed by $\pi/2$. However, quadrature mixers are often impaired by gain and phase mismatches between the parallel sections of the RF chain dealing with the I and Q signal paths. The gain mismatch causes amplitude imbalance, and phase deviation from $\pi/2$ results in phase imbalance. Assume that the gain error is $20\log\left[g_1/g_2\right]$ dB and the phase error is $\varepsilon_p$ degrees. Then the IQ imbalance can be modeled as:
\begin{equation}
  g_1 cos(2\pi f_{c, t} - \varepsilon_p/2), \ \text{and} \ \ g_2 sin(2\pi f_{c, t} + \varepsilon_p/2),
\end{equation}
where  $f_{c,t}$ is the center frequency that depends on the LO of the transmitter.


\textbf{Non-linear Distortions of Power Amplifier:} The power amplifiers are usually used in the RF chain to amplify the pass-band signal for transmission. In practice, the analog amplification process in PA will incur nonlinear distortions in both amplitude and phase. These effects are directly applied to the pass-band signal but can be equivalently modeled on baseband signals. An empirical baseband model of PA was presented in \cite{pedro2005comparative}. Let $g(t)$ and $ \phi(t)$ denote the amplitude and phase offset of the PA output, respectively. They can be modeled as
\begin{equation}
  g(t)=\frac{\alpha_a r(t)}{1+\beta_a [r(t)]^2},
\end{equation}
and
\begin{equation}
  \phi(t) = \frac{\alpha_\phi r(t)^2}{1+\beta_\phi [r(t)]^2},
\end{equation}
where $\alpha_a, \alpha_\phi, \beta_a $ and $\beta_\phi$ are fitting parameters extracted from measurement results. The equivalent baseband signal of PA output, denoted by $\tilde{x}_{b}(t)$, is then given by
\begin{equation}
  \tilde{x}_{b}(t)=g(t)e^{j[\phi(t)+\epsilon_{\phi}(t)]}.
\end{equation}


\textbf{Carrier Frequency Offset:} The transmitted signals are collected by an RF receiver in a practical end-to-end communication system. The carrier frequencies used for up-conversion and down-conversion are generated in the transmitter and receiver using their own LOs, respectively, which can never oscillating at an identical frequency. Let $y(t)$ denote the ideal signal received at a receiving antenna, the received signal $y(t)$ is down-converted to the baseband by a mixer, which is given by
\begin{equation}
  y_{b}(t)=y(t)e^{-j\omega_{r}t},
\end{equation}
where  $y_{b}(t)$ is the received baseband signal at time $t$, and $\omega_{r}=2\pi f_{c,r}$ denotes the angular frequency with continues-time signals of the receiver with $f_{c,r}$ local carrier frequency of the receiver. Let $\epsilon_{f}$ denote the carrier frequency offset between the transmitter and receiver (e.g., $f_{c,r}=f_{c,t}+\epsilon_{f}$). The received baseband signal $y_{b}(t)$ can be expressed using $f_{c, t}$ as
\begin{equation}
  y_{b}(t)=y(t)e^{-j2\pi (f_{c,t}+\epsilon_{f})t},
  \label{baseband_receiver}
\end{equation}
and the carrier waves generated by the receiver's LO can then be modeled as
\begin{equation}
  cos(2\pi (f_{c,t} + \varepsilon_f) t), \ \text{and} \ \  sin(2\pi (f_{c,t} + \varepsilon_f) t ).
\end{equation}

\subsection{Radio Frequency Fingerprinting}
\label{RFFing}
RFF refers to the task of identifying radio devices, which is typically done by analyzing their transmitted RF signals and extracting RF fingerprints associated with their hardware impairments. An RFF system is illustrated in Fig.\ \ref{fig:system_overview}, which consists of a receiver and massively deployed wireless devices. This work considers RFF as an independent third party in a wireless communication system to differentiate the identities of wireless devices using their uplink RF signals. Specifically, the user devices first transmit data to the access point using a predefined communication protocol. Then, their uplink signals are captured by a receiver equipped on the edge server. This work aims at identifying practical RF devices using the received baseband signals at the receiver. Given the received baseband signal $y_{b}(t)$, the server performs a series of signal processing and classification algorithms on $y_{b}(t)$ to identify the RF devices.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.6]{system_overview.pdf}
  \caption{Illustration of RFF system.}
  \label{fig:system_overview}
\end{figure}
%Conventionally, the radio devices can be distinguished by their physical locations or communication protocols. However, to deal with spoofing attacks in a wireless communication system, it is more important to study the accurate classification of radio devices with same protocol using environment agnostic information.
\section{Dataset}
\label{data}
To evaluate the RF fingerprint identification performance of RFF algorithms, we design an RF signal acquisition system, as illustrated in Fig. \ref{fig:experiment_system}. The system consists of a wireless access point (AP) for establishing links with end-devices using WiFi, WiFi end-devices for transmitting WiFi signals to the AP, a National Instrument (NI) USRP-2922 SDR platform for gathering the WiFi signals from end-devices, and a server for processing the received signals collected at the SDR. The RF signal acquisition system is placed in a 3m$\times$3m$\times$3m anechoic chamber testbed. The chamber can absorb unwanted radio frequency waves by lining the chamber with hundreds of blue foam protruding arrowheads. Using this isolated environment can help to reduce the wireless channel impact and accent the device fingerprint. This is because the transmission inside the chamber is not affected by external RF activity, and the cones deployed in the chamber absorb signals generated internally, preventing multi-path effect. %, as shown in Fig.\ \ref{fig:chamber_testbed}

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.6]{experiment_system.pdf}
  \caption{The radio frequency signal acquisition system.}
  \label{fig:experiment_system}
\end{figure}

We collect baseband IQ samples transmitted using the standard IEEE 802.11n WiFi protocol\ \cite{9363693}, which is one of the most common wireless communication protocols used nowadays. The signal from WiFi end-devices is streamed at 2.472GHz (channel 13) with 20MHz bandwidth. The IQ data sampled by the SDR is then saved for the RFF.

\subsection{WFDI Dataset}
\label{WFDI_dataset}
We use 72 commercial WiFi end-devices as the target devices. During the data collection, all devices are configured to transmit WiFi signals in legacy or mixed mode, resulting in a preamble-payload structure with the same legacy-short training field (L-STF) in the collected uplink frames, as shown in Fig. \ref{fig:package_structure}. SDR receives the transmitted signal, operating at center frequency of 2.472GHz and baseband sampling rate of 20MSa/s. Each WiFi device sends the uplink frames repeatedly during the transmission. Each uplink frame consists of randomly generated data samples and the same L-STF sequence, as shown in Fig. \ref{fig:one-second_frame}. For each device, one-second (1s) data is captured by SDR and saved in the sever per recording day. The data collection process is repeated for 8 days.

Frame detection is a standard process to detect signal arrival and locate uplink frames in the IEEE 802.11 WiFi communication protocol. The packet preamble consists of a 20us signal, where the first 16us signal is primarily used for synchronization. In our experiments, we use a short training field (STF) for frame detection, which is performed on the recorded one-second data to locate the initial sample of each uplink frame. We randomly select $N_{m}=256$ L-STF sequences with $N_{s}=160$ samples each from the one-second IQ data for each device and construct our WiFi Device Identification (WFDI) dataset using all L-STF sequences recorded in $N_{d}=8$ experiment days. The L-STF signals are saved as a matrix, with the dimension of $N_{n}\times{N_d}\times{N_{m}}\times{N_{c}}\times{N_{s}}$. $N_{n}=72$ is the number of target WiFi devices and $N_{c}=2$ represents I and Q channels.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.7]{package_structure.pdf}
  \caption{The WiFi package in legacy and mixed transmission mode. The legacy mode consists of L-STF, legacy-long training field (L-LTF), legacy-signal field (L-SIG) and data, and the mixed mode consists of L-STF, L-LTF, L-SIG, high throughput-signal field (HT-SIG), high throughput-short training field (HT-STF), high throughput-long training field (HT-LTF) and data.}
  \label{fig:package_structure}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.75]{one-second_frame.pdf}
  \caption{An example of an uplink frame.}
  \label{fig:one-second_frame}
\end{figure}

\section{Signal Preprocessing and Representation}
\label{rep}
%In a wireless communication system, a signal $s(t)$ is up-converted to baseband signal $s_{b}(t)$ by a mix at the transmitter before transmitting to a receiver. The baseband signal at the transmitter is given by $s_{b}(t)=s(t)e^{-j\omega_{t}t$, where $\omega_{t}=2\pi f_{c, t}t$ is the angular frequency with continues-time signals at transmitter with $f_{c, t}$ the local carrier frequency of the transmitter. 
A discrete-time version of the baseband signal $y_b(t)$ in Eq.\ (\ref{baseband_receiver}), denoted by $y(n)$, can be obtained by sampling from the continuous-time signal using an analog-to-digital converter (ADC) at uniformly spaced times $T_{s}$, associated with a sampling rate $f_{s}=\frac{1}{T_{s}}$. The data model of $y(n)$ is then given by
\begin{equation}
  y(n)=y_{b}(t)|_{t=nT_{s}}, -\infty <n< \infty,
\end{equation}
In this work, we first perform energy normalization to remove energy differences between RF devices, then we adopt CFO estimation, FFT coefficients, and STFT coefficients to represent the non-idealities of RF.

\subsection{Energy Normalization:} The energy of the WiFi signal is one of the standard features used in signal classification, which is susceptible to transmission distance. Since RFF aims at identifying devices by learning hardware imperfections imposed by the RF circuit, rather than energy differences, we perform energy normalization to bring the average amplitude to a target level using the root-mean-square of the amplitude of the signal $y(n)$. The normalized version of $y(n)$ is given by
\begin{equation}
  y_{e}(n)=\frac{y(n)}{\sqrt{\frac{1}{N}\sum_{n=0}^{N-1}y^{2}(n)}}.
\end{equation}
where $N$ is the signal length and $y_{e}(n)$ denotes the normalized signal at time sampling index $n$. The L-STF sequences of the signal $y_{e}(n)$, denoted by $\tilde{y}(n)$, are then used to extract characteristics of the RF fingerprint.

\subsection{Signal Representation}


%\textbf{IQ Imbalance:} IQ imbalance causes interference between I and Q baseband signals, and includes IQ gain error $20\log[\epsilon_{g2}/\epsilon_{g1}]$ dB and phase imbalance $2\epsilon_{p}$, see Eq. (2). The phase imbalance is any phase deviation from the ideal $90^\circ$. This work first removes the ideal phase difference of $90^\circ$ between I and Q channel signal using $i$ samples, and uses the results to represent the IQ imbalance characteristic, which is given by
%\begin{equation}
%    \mathbf{R}_{IMB}=\left[\left|\tilde{y}_I(0)-\tilde{y}_Q(8)\right|,  \left|\tilde{y}_I(1)-\tilde{y}_Q(9)\right|, \cdots, \left|\tilde{y}_I(N_{s}-9)-\tilde{y}_Q(N_{s}-1)\right|\right]^{T}
%\end{equation}
%with $[\cdot]^T$ the transposition of a vector of a matrix and $N_{s}=160$ is the subset of short preambles utilized.

\textbf{CFO Estimation:} CFO is caused by the difference between the LO of the transmitter and receiver, and results in the phase offset of the received baseband IQ samples, see Eq. (\ref{baseband_receiver}). The CFO can be estimated with the help of the received preambles. This work adopts a coarse estimation algorithm in\ \cite{sourour2004frequency} to estimate CFO, that is,
\begin{equation}
  \hat{\epsilon}_{f}=\frac{1}{16} \angle\left(\sum_{n=0}^{N_{s}-17} \tilde{y}^{*}(n) \tilde{y}(n+16)\right)
\end{equation}
where $\angle(\cdot)\in{[-\pi, \pi]}$ is the angle of a complex variable, and $(\cdot)^{*}$ denotes conjugation. The phase difference between $\tilde{y}(n)$ and $\tilde{y}(n+16)$ indicates the accumulated CFO over 16 samples. Then, we conduct a vector using the phase information of the CFO estimation to represent the signal $\tilde{y}(n)$, that is,
\begin{equation}
  \mathbf{R}_{CFO}=\left[\angle\left(\tilde{y}^{*}(0)\tilde{y}(16)\right), \cdots, \angle\left(\tilde{y}^{*}(N_{s}-17)\tilde{y}(N_{s}-1)\right)\right]^{T}.
\end{equation}


\textbf{FFT Coefficients:} Given the signal $\tilde{y}(n)$, a fast Fourier transform (FFT) is performed to compute the discrete Fourier transform (DFT) of the sequence and transform the complex symbols into the frequency domain, i.e.,
\begin{equation}
  Y_{FFT}(f)=\sum_{n=0}^{N_{s}-1}\tilde{y}(n)e^{-j2\pi fn/N_{s}}, f=0, \cdots, N_{s}-1,
\end{equation}
where $Y_{FFT}(f)$ is the FFT coefficients at frequency-bin index $f$. We then use the FFT coefficients, defined by $\mathbf{R}_{FFT}=\left[Y_{FFT}(0), \cdots, Y_{FFT}(N_{s}-1)\right]^{T}$, as a frequency-domain representation of the signal $\tilde{y}(n)$.

\textbf{STFT Coefficients:} Short-time Fourier transform (STFT) is a sequence of DFTs of a windowed signal. Unlike FFT where DFT provides the averaged frequency information over the entire signal, STFT computes the time-localized frequency information for situations where frequency components of a signal vary over time\ \cite{KEHTARNAVAZ2008175}. Let $w(\cdot)$ denote a window function of length $J$ and let $K$ be the window shift. The signal $\tilde{y}(n)$ is windowed and transformed into the frequency domain by applying a DFT, i.e.,
\begin{equation}
  Y_{STFT}(m, f)=\sum_{n=mK}^{mK+J-1}\tilde{y}(n)w(n-mK)e^{-j\omega_{f}(n-mK)},
\end{equation}
where $Y_{STFT}(m, f)$ denotes the STFT coefficients at discrete-time frame index $m$ and frequency-bin index $f$, and $\omega_{f}=2\pi f/J$ is the discrete frequency variable at frequency-bin index $f$. We use the STFT coefficients $Y_{STFT}(m, f), \forall m, \forall f$ as another representation of the signal $\tilde{y}(n)$, denoted as $\mathbf{R}_{STFT}$. 

%R1 Frequency Offset Estimation and Correction
%R2 N. Kehtarnavaz, “Chapter 7 - frequency domain processing,” in Digital Signal Processing System Design (Second Edition), N. Kehtarnavaz, Ed. Burlington: Academic Press,
\section{Learning Feature Fusion for RF Fingerprinting}
\label{met}
This section introduces the proposed McAFF model for RFF. The model consists of two functional parts: multi-channel feature fusion and signal identification. Instead of simply stacking multi-channel inputs including IQ samples, CFO, FFT coefficients and STFT coefficients, a deep attention fusion algorithm is utilized to combine them deeply, which uses a shared attention module (SAM) to adaptively fuse multi-channel neural features. Later, a ResNeXt block\ \cite{xie2017aggregated} based signal identification module (SIM) is performed for device identification. Given multi-channel inputs, denoted as $\mathbf{R}=\left\{\mathbf{R}_{IQ}; \mathbf{R}_{CFO}; \mathbf{R}_{FFT}; \mathbf{R}_{STFT}\right\}$, the model $\mathcal{F}_\mathbf{w}$ is trained to minimize the cross-entropy loss function, that is,
\begin{equation}
  \hat{\mathbf{w}}=\text{argmin}_{\mathbf{w}}\mathbb{E}_{\mathbf{R},\bar{y}}\left(\mathcal{L}(\mathcal{F}_\mathbf{w}(\mathbf{R}), \bar{y})\right),
\end{equation}
where $\mathbb{E}(\cdot)$ denotes the expectation operation and $\mathcal{L}(\mathcal{F}_\mathbf{w}(\mathbf{R}), \bar{y})$ is the cross-entropy loss between the estimation result $\mathcal{F}_\mathbf{w}(\mathbf{R})$ and the true label $\bar{y}$. Fig.\ \ref{fig:model} illustrates an overview of the proposed McAFF model, we describe the details of each part as follows.
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.9]{model.pdf}
  \caption{Overview of the proposed McAFF model. Given an input L-STF signals, part I extracts multi-channel neural features from multi-channel signal representations of the input signals, and adaptively fuse the multi-channel neural features using a shared attention module. Then the fused features will go through the signal identification part to obtain device identity.}
  \label{fig:model}
\end{figure}

\subsection{Multi-channel Feature Fusion}
The multi-channel feature fusion part first uses four channel features $\mathbf{R}$ as input data. Then the four streams of input data are fed to four separate convolutional layers to acquire four channel neural features, denoted by $\mathbf{F}_{IQ}$, $\mathbf{F}_{CFO}$, $\mathbf{F}_{FFT}$ and $\mathbf{F}_{STFT}$, respectively. Specifically, the settings of the convolutional layers used to process $\mathbf{R}_{CFO}$ and $\mathbf{R}_{FFT}$ are the same, i.e., one dimensional (1D) convolutional layer with $128$ channels and filter size of $1\times{7}$. A 1D convolutional layer with $128$ channels and filter size of $2\times{7}$ is performed to extract neural features of $\mathbf{R}_{IQ}$, and a 2D convolutional layer with $128$ channels and filter size of $8\times{7}$ is used to process $\mathbf{R}_{STFT}$. Then, SAM is adopted to refine the intermediate neural features $\mathbf{F}_{IQ}$, $\mathbf{F}_{CFO}$, $\mathbf{F}_{FFT}$ and $\mathbf{F}_{STFT}$.

Given a feature map $\mathbf{F}\in\mathbb{R}^{C\times{H}\times{W}}$ with $C$ the number of feature map channels and $H\times{W}$ the size of feature maps, SAM aims at inferring a 2D shared attention map $\mathbf{M}\in{\mathbb{R}^{3\times{H}\times{W}}}$ to adaptively control the weight of each point on input feature map. The refined feature map $\mathbf{F}_{r}\in\mathbb{R}^{C\times{H}\times{W}}$ is given by
\begin{equation}
  \mathbf{F}_{r}=\mathbf{M}(\mathbf{F})\otimes\mathbf{F},
  \label{fine}
\end{equation}
where $\otimes$ is element-wise multiplication. During multiplication, the important parts of input feature map are enhanced, and the rest ones are faded out according to the learned attention weights $\mathbf{M}$.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.25]{attention_fusion_module.pdf}
  \caption{Structure of the proposed shared attention module.}
  \label{fig:attention}
\end{figure}

As illustrated in Fig.\ \ref{fig:attention}, SAM first aggregates spatial information by performing average-pooling and max-pooling on the input feature map $\mathbf{F}$ in parallel, generating two different spatial context features, denoted by $\mathbf{F}_{avg}\in{\mathbb{R}^{H\times{W}}}$ and $\mathbf{F}_{max}\in{\mathbb{R}^{H\times{W}}}$, respectively. The two context features are then concatenated and convolved by three 2D convolutional layers to infer the attention map $\mathbf{M}$. The attention map can be expressed as:
\begin{equation}
  \mathbf{M}(\mathbf{F})=\sigma\left(v_{3}^{3\times{3}}\left(\left[\mathbf{F}_{avg}; \mathbf{F}_{max}\right]\right)\right),
\end{equation}
where $\sigma$ is the Sigmoid function, and $v_{3}^{3\times{3}}$ denotes three cascaded convolutional layers. Then, neural feature map on each channel is refined as in Eq.\ (\ref{fine}). After that, the four-channel refined neural feature maps are concatenated to generate the fused features $\mathbf{F}_{r,c}\in\mathbb{R}^{4C\times{H}\times{W}}$, which is given by
\begin{equation}
  \mathbf{F}_{r,c}=\left[\mathbf{F}_{r,IQ}; \mathbf{F}_{r,CFO}; \mathbf{F}_{r,FFT}; \mathbf{F}_{r,STFT}\right]
\end{equation}
The fused features $\mathbf{F}_{r,c}$ are then fed to the signal identification for fingerprinting.

\subsection{Signal Identification}
The signal identification part includes SIM, which maps refined features to device identities. As illustrated in Fig.\ \ref{fig:sc}, SIM consists of a convolution-based ResNeXt block\ \cite{xie2017aggregated}, two fully connected (FC) layers and a softmax layer. ResNeXt block is designed to build a simple and highly modularized network architecture for image classification. It aggregates a set of transformations with the same topology and exploits the split-transform-merge strategy in an easy and extensible way. Compared with ResNet\ \cite{he2016deep}, ResNeXt block-based neural network is able to improve classification accuracy by increasing the size of the set of transformations. Let $\mathbf{x}=\left[x_{1}, \cdots, x_{D}\right]$ denote a $D$-channel input vector to a neuron and $w_{i}$ denote the weight of a filter for the $i$th channel. A simple neuron in deep neural networks performs the inner product of the input vector and its corresponding weights, which can be thought of a form of aggregating transformation:
\begin{equation}
  \sum_{i=1}^{D}w_{i}x_{i}.
\end{equation}
Such inner product operation is the elementary transformation in fully-connected and convolutional layers. Inspired by the inner production operation, ResNeXt presents a more generic operation by replacing the elementary transformation $w_{i}x_{i}$ with an arbitrary function $\Gamma_{i}(\mathbf{x})$, that is,
\begin{equation}
  \sum_{i=1}^{C}\Gamma_{i}(\mathbf{x}),
  \label{trans}
\end{equation}
where $C$ is the size of the set of transformations to be aggregated. As shown in Fig.\ \ref{fig:sc}, the ResNeXt block processes the input feature map $\mathbf{F}_{r,c}$ by using the residual function with the aggregated transformations in Eq.\ (\ref{trans}). The ResNeXt block generates an output $\mathbf{F}_{b}$ from an input $\mathbf{F}_{r}$ through the following: % add dimension of the $\mathbf{F}_{b}$ and $\mathbf{F}_{r}$
\begin{equation}
  \mathbf{F}_{b}=\mathbf{F}_{r,c}+\sum_{i=1}^{C}\Gamma_{i}(\mathbf{F}_{r,c}).
\end{equation}
The ResNeXt block consists of three convolution-based operations: a single 2D convolutional layer with 128 channels and filter size of $1\times{1}$, a grouped convolutional layer performs $C=32$ groups of convolutions with 128 channels and filter size of $3\times{3}$, and a single 2D convolutional layer with 256 channels and filter size of $1\times{1}$. The ResNeXt block is followed by two fully-connected layers with 512 neurons each and finally an output layer with the softmax function. In addition, each convolutional and fully-connected layer is followed by batch normalization\ \cite{ioffe2015batch} and Rectified Linear Unit (ReLU)\ \cite{abien2018relu}.
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.25]{SC_module.pdf}
  \caption{Illustration of the proposed signal identification module.}
  \label{fig:sc}
\end{figure}

%R3 Aggregated Residual Transformations for Deep Neural Networks
%R4 C. M. Bishop, “Pattern recognition,” Machine Learning, vol. 128, no. 9, 2006.
%R5 ResNet
%R6 batch normalization
%R7 ReLu 
\section{Experiments}
\label{exp}


\subsection{Experimental setup}
We use five experiments to evaluate and analyze the identification performance of the proposed McAFF model. First, we study the effect of the different signal representation approaches including IQ samples, CFO, FFT, STFT, and a combination of the four on identification accuracy and feature modeling efficiency. We denote the identification models using single signal representation IQ samples, CFO, FFT, and STFT as MIQ, MCFO, MFFT, and MSTFT, respectively. Second, we consider that training and test data are collected on the same day. We select all signals collected on one day from the dataset and split the signals into training and test data. We evaluate the identification performance of the proposed McAFF model on different days. Later, we evaluate our model using training and test data collected from different days, i.e., samples collected from one or several days are used for training, and data collected from the remaining days are used for test. After that, we analyze the effectiveness of the deep attention fusion algorithm on RFF. Finally, we evaluate the identification performance of the proposed McAFF model in a simulated noisy environment, where white Gaussian noise is deliberately introduced to modify the original data and increase the size and variability of the initial dataset WFDI.

\subsection{Implementation details}
We convert the L-STF signals to frequency domain using fame-based STFT, with a frame length $J=120$ and a 95\%-overlapping Hamming window. For each device, WDFI consists of $N_{t}=N_d\times{N_{m}}$ IQ signals. To evaluate the identification performance of RFF methods on a training dataset of different sizes, we randomly select a certain percentage of data from the $N_t$ signals.  All experiments are implemented using Keras with Tensorflow backbone and trained on a PC with an NVIDIA TITAN X GPU. The identification model is trained with a batch size of 256 and optimized with Adam optimizer\ \cite{diederik2015adam}. The learning rate starts with $0.001$ and decays $0.1$ when the validation loss does not drop within $10$ epochs. The training is terminated when the validation loss does not drop within 15 epochs, and the model with the smallest validation loss is saved and used for test. %The received signal $y$ with dimension of $160\times{1}$ are first processed using signal processing methods to get the three signal features $\mathbf{R}_{CFO}$, $\mathbf{R}_{FFT}$ and $\mathbf{R}_{STFT}$ with the dimension of $144\times{1}$, $160\times{1}$ and $160\times{8}$, respectively. Before feeding the features into the proposed identification network, we pad zeros before the feature sequence to make the CFO features $\mathbf{R}_{CFO}$ has the same length as the other two features.
\subsection{Experimental results}
\textbf{Ablation study of different signal representations:} We do ablation studies to illustrate the impact of signal representations on RF device identification accuracy. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{epoch_vs_val_loss.pdf}
  \caption{Validation loss of RFF models versus training epochs on WFDI with data size of 2\%$N_{t}$. Compared with single-channel RFF models, including MIQ, MCFO, MFFT and MSTFT, the proposed McAFF model converges faster and to a lower validation loss.}
  \label{fig:training_curve}
\end{figure}
\begin{table}[ht]
  \renewcommand{\arraystretch}{1.3}
  \caption{RFF performance comparison between MIQ, MCFO, MFFT, MSTFT and the proposed McAFF.}
  \begin{center}
    \begin{tabular}{c|cccccccc}
      \hline \hline
               & 1\%$N_{t}$                           & 5\%$N_{t}$              & 15\%$N_{t}$              & 25\%$N_{t}$              & 50\%$N_{t}$              \\
      \hline
      MIQ      & 64.52\%                    & 91.17\%          & 95.95\%          & 97.21\%          & 98.29\%          \\
      MCFO     & 86.69\%                   & 96.06\%          & 97.92\%          & 98.38\%          & 98.94\%          \\
      MFFT     & 47.37\%                   & 86.43\%          & 93.27\%          & 94.3\%           & 96.17\%          \\
      MSTFT    & 89.94\%                  & 95.18\%          & 96.81\%          & 97.41\%          & 98.01\%          \\
      McAFF & \textbf{90.00\%}  & \textbf{96.11\%} & \textbf{97.94\%} & \textbf{98.41\%} & \textbf{99.01\%} \\
      \hline \hline
    \end{tabular}
    \label{table:data_size_vs_acc}
  \end{center}
\end{table}
The training curves of all models are shown in Fig.\ \ref{fig:training_curve}. We evaluate model efficiency in terms of identification accuracy versus training data size. Table\ \ref{table:data_size_vs_acc} shows the experimental results. When the training data size is larger than 50\%$N_{t}$, all models get enough accuracy (higher than 96\%) for device identification. The identification performance of the proposed model is better than those of all other models on training datasets of different sizes. Compared with MIQ, MCFO, MFFT, and MSTFT, the proposed model gets higher identification accuracy and is more robust for training data scales. In addition, MSTFT performs better than MIQ, MCFO, and MFFT when the training data size is small, such as 1\%$N_{t}$. This can be explained that STFT with time-frequency information provides a richer representation of signal than IQ samples, CFO and FFT.

\begin{table*}[ht]
  \setlength{\tabcolsep}{1.25mm}%\renewcommand{\arraystretch}{1.3}
  \caption{RF identification performance of the proposed McAFF in the ``Train and Test Same Day" scenario.}
  \begin{center}
    \begin{tabular}{c|cccccccc}
      \hline \hline
               & $DAY_{1}$ & $DAY_{2}$ & $DAY_{3}$ & $DAY_{4}$ & $DAY_{5}$ & $DAY_{6}$ & $DAY_{7}$ & $DAY_{8}$ \\
      \hline
      McAFF & 99.17\%   & 98.25\%   & 99.26\%   & 99.62\%   & 99.01\%   & 99.6\%    & 98.21\%   & \textbf{99.87\%}   \\
      \hline \hline
    \end{tabular}
    \label{table:same_day_acc}
  \end{center}
\end{table*}

\begin{table*}[ht]
  \setlength{\tabcolsep}{1.25mm}%\renewcommand{\arraystretch}{1.3}
  \caption{RF identification performance comparison between MIQ, MCFO, MFFT, MSTFT and the proposed McAFF model in the ``Train and test different day" scenario.}
  \begin{center}
    \begin{tabular}{c|cccc}
      \hline \hline
               & 1\_train/7\_test & 2\_train/6\_test         & 3\_train/5\_test         & 4\_train/4\_test         \\
      \hline
      MIQ      & 58.78\%  & 75.81\%          & 79.76\%          & 82.81\%          \\
      MCFO     & 62.44\%  & 75.26\%          & 78.9\%           & 81.18\%          \\
      MFFT     & 70.22\%  & 81.39\%          & 84.58\%          & 86.45\%          \\
      MSTFT    & \textbf{76.10\%}  & 85.43\%          & 86.52\%          & 88.06\%          \\
      McAFF & 75.35\%  & \textbf{86.04\%} & \textbf{87.72\%} & \textbf{88.67\%} \\
      \hline \hline
    \end{tabular}
    \label{table:diff_multi_day_acc}
  \end{center}
\end{table*} 

\begin{table*}[ht]
  \setlength{\tabcolsep}{1.25mm}%\renewcommand{\arraystretch}{1.3}
  \caption{Effect of Attentive Fusion in the ``Train and test different day" scenario.}
  \begin{center}
    \begin{tabular}{c|cccccccc}
      \hline \hline
                                     & 1\_train/7\_test         & 2\_train/6\_test        & 3\_train/5\_test         & 4\_train/4\_test         \\
      \hline
      McAFF (no attentive fusion) & 73.96\%          & 85.26\%          & 86.6\%           & 88.06\%          \\
      McAFF                       & \textbf{75.35\%} & \textbf{86.04\%} & \textbf{87.72\%} & \textbf{88.67\%} \\
      \hline \hline
    \end{tabular}
		\label{table:attention_vs_acc}
  \end{center}
\end{table*}

\textbf{``Train and test same day" scenario:} We first split our WFDI dataset according to the collection date and name a sub-dataset collected on the day $i$ as $DAY_{i}$. After that, each sub-dataset is partitioned as follows: 70\% for training, 10\% for validation, and 20\% for test. We train an identification model on each sub-dataset and get 8 models in total. Table\ \ref{table:same_day_acc} shows the identification results of the proposed method on different sub-datasets. We observe that the proposed McAFF model works well when training and test data are collected on the same day. Specifically, the proposed McAFF model achieves the highest identification rate of 99.85\% on $DAY_{8}$, and the minimum recognition rate is 98.21\% on $DAY_{7}$. The average identification accuracy of the proposed McAFF model over eight days is 99.12\%. 


\textbf{``Train and test different day" scenario:} In this experiment, we consider a scenario that training and test data are collected on different days. Table\ \ref{table:diff_multi_day_acc} shows the experimental results. We observe that the identification accuracy of MIQ, MCFO and MFFT decreases dramatically when training and test data are collected from different dates. MSTFT gets around 2\% to 6\% higher identification accuracy than MIQ, MCFO and MFFT. When data collected from one day is used for training and data collected from remaining 7 days is used for test (1\_train/7\_test), the proposed method gets 0.8\% lower identification accuracy than MSTFT, and around 5\%, 13\% and 17\% higher identification accuracy than MFFT, MCFO and MIQ, receptively. By using samples collected from multiple days for training, all RFF models gets significant performance gains, i.e., around 10\% higher identification accuracy. Specifically, the proposed method gets around 0.7\% higher identification accuracy than MSTFT, and around 3\% to 11\% higher identification accuracy than MCFO, MFFT and MIQ. This can be explained that the proposed method uses multi-channel inputs and adaptively fuses the multi-channel neural features is more robustness to data collection dates.

\textbf{Effect of SAM:} We compare the proposed McAFF model with a baseline model which simply concatenates multi-channel features without attention module. Table\ \ref{table:attention_vs_acc} shows identification performance comparison results. Compared with the baseline model, the proposed McAFF model gets better identification performance on training datasets of different data sizes. This is because that the proposed McAFF model utilizes attention fusion to dynamically control the weights of multi-channel neural features. 

\begin{table*} [ht]
    \setlength{\tabcolsep}{1.25mm}
    \centering
    \caption{RFF performance comparison between MIQ, MCFO, MFFT, MSTFT and the proposed McAFF at different SNR levels.}
		\label{table:noise}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline \hline
    &\multicolumn{2}{c|}{2 dB} & \multicolumn{2}{c|}{4 dB} & \multicolumn{2}{c|}{6 dB} & \multicolumn{2}{c|}{8 dB} & \multicolumn{2}{c|}{10 dB} & \multicolumn{2}{c}{12 dB}
    \\ \cline{2-13} 
    & 1\%$N_{t}$ & 50\%$N_{t}$ & 1\%$N_{t}$ & 50\%$N_{t}$ & 1\%$N_{t}$ & 50\%$N_{t}$ & 1\%$N_{t}$ & 50\%$N_{t}$ & 1\%$N_{t}$ & 50\%$N_{t}$ & 1\%$N_{t}$ & 50\%$N_{t}$ \\ 
		\hline
     MIQ      & 8.44\%     & 39.46\%   & 11.12\%    & 46.97\%    & 13.61\%    & 48.54\%    & 17.48\%    & 57.01\%   & 22.06\%                     & 66.65\%          & 27.02\%                        & 74.68\%          \\  
     MCFO     & 8.84\%     & 18.33\%   & 11.43\%    & 23.28\%     & 14.36\%   & 30.08\%    & 18.5\%     & 37.79\%   & 23.94\%                    & 46.18\%         & 30.28\%                      & 54.18\%          \\
     MFFT     & 10.12\%    & 30.53\%   & 12.78\%    & 36.51\%     & 15.67\%   & 43.41\%    & 19.77\%    & 50.12\%   & 20.9\%                         & 57.8\%          & 23.7\%                        & 64.44\%          \\
     MSTFT    & 9.19\%     & 37.81\%   & 11.43\%    & 44.08\%     & 14.37\%   & 51.78\%    & 17.6\%     & 60.38\%   & 21.32\%                        & 67.58\%         & 24.22\%                           & 72.57\%          \\ 
     McAFF & \textbf{15.02\%}  & \textbf{42.46\%} & \textbf{18.98\%}  & \textbf{49.95\%} & \textbf{22.71\%} & \textbf{59.14\%} & \textbf{26.83\%} & \textbf{65.95\%} & \textbf{31.65\%}  & \textbf{73.45\%} & \textbf{35.87\%}   & \textbf{79.16\%} \\
    \hline \hline
    \end{tabular}
\end{table*}

   %  

\begin{figure*}[ht]
  \centering
  \includegraphics[scale=1.2]{SNR_vs_acc.pdf}
  \caption{RF fingerprint identification accuracy of the proposed method, MIQ, MCFO, MFFT and MSTFT versus SNR in the ``Train and test different day" scenario. (a) 1 train\_7 test; (b) 2 train\_6 test; (c) 3 train\_5 test; (d) 4 train\_4 test.}
  \label{fig:transRec}
\end{figure*}

\textbf{Impact of noise:} This subsection studies the effect of different levels of noise on the identification accuracy of the proposed method and single-channel RFF models. Table\ \ref{table:noise} lists the identification accuracy of the proposed method and all single-channel RFF models at 6 different SNR levels. It shows that the identification accuracy of all RFF models is increased with increasing the SNR level of the input data, and increased with increasing the amount of training samples. In addition, the proposed method significantly outperforms single-channel RFF models. For single-channel RFF models, all models gets similar identification accuracy when 1\%$N_{t}$ data is used for training. MSTFT and MIQ gets similar identification accuracy at all SNR levels, and around 2\% to 10\% higher identification accuracy than MFFT. The identification accuracy of the MFFT is around 2\% to 10\% higher than those of the MCFO. Fig.\ \ref{fig:transRec} illustrates the RF fingerprint identification performance comparison results in a scenario where training and test data are collected from different days. Compared with single-channel RFF models, the proposed method gets better identification performance at all SNR levels. The experiment results demonstrate that the proposed method using multi-channel attention fusion neural network is more effective for RFF when the received radio signals are noisy.   

\section{Conclusion}
\label{con}
In this work, we proposed a novel multi-channel attentive feature fusion (McAFF) model for RFF. The proposed model consists of two sequential parts: multi-channel feature fusion for feature learning and fusion, and signal identification for RF device identification. The multi-channel feature fusion part first computes the representations of the input WiFi signals using the CFO estimation, FFT and STFT. Then, a shared attention module was used to adaptively control the weight of the neural features learned from the four channel inputs, including IQ samples, CFO, FFT coefficients and STFT coefficients. The model was trained to minimize the cross-entropy loss function for WiFi device identification. To evaluate the effectiveness of the proposed McAFF model on RF fingerprint identification from real-world RF signals, we deigned a new dataset (WFDI), including 2GB wireless data captured from 72 WiFi end-devices over the course of 8 days in an anechoic chamber. We provided an exhaustive evaluation of the impact of signal representation on RF fingerprint identification performance, and analyzed the effectiveness of the proposed shared attention module in terms of fingerprinting accuracy in a scenario where training and test data are collected from different days. In addition, we evaluated the identification effect of the proposed McAFF model on a simulated noisy dataset. Experimental results demonstrated that the proposed RFF model with multi-channel attentive feature fusion gets better RF fingerprint identification accuracy on datasets of different sizes and collection dates, and is more robust against to noisy radio signals. 

Learning to fuse multi-channel features is a straightforward idea to improve the designing of a feature-based RF fingerprinting system. This work conducts an extensive research on how to compose multi-channel features and signal identification for better RFF, which has been demonstrated in a real RF signal acquisition system. However, similar to other supervised signal classification methods, the proposed McAFF model does not address dynamic wireless communication systems with wireless channel and environmental noise variations. How to leverage blind signal processing algorithms or learning-based signal enhancement models to help extract robust features of RF impairments at the test stage is an interesting direction for future work. 

\section{ACKNOWLEDGMENTS}
This work is supported in part by National Natural Science Foundation in China under Grants 62071212 and 62106095, Guangdong Basic and Applied Basic Research Foundation under Grant 2019B1515130003, Shenzhen Science and Technology Program under Grants JCYJ202001091414409 and KCXFZ20211020174802004.
% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%




% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
%\section*{Acknowledgment}
%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
%\newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:

%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\bibliographystyle{IEEEtran}
\bibliography{references}

% that's all folks
\end{document}


