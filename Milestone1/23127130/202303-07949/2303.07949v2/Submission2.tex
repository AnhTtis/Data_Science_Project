\documentclass{article}
\usepackage{amsfonts, amsmath, amsthm, hyperref, bbm}
\usepackage{mathrsfs}
\usepackage[svgnames,x11names]{xcolor}
\usepackage{comment}
\usepackage{enumerate}



\usepackage[margin=3cm]{geometry}
\usepackage[algo2e,ruled,vlined]{algorithm2e}
\usepackage[section]{algorithm}
\usepackage[normalem]{ulem}
\usepackage{algpseudocode}
 




\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator{\mr}{mr}
\DeclareMathOperator{\spec}{spec}
\DeclareMathOperator{\jdup}{jdup}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
%\newcommand{\SS}{{\mathcal S}}

\newcommand{\bR}{\mathbb{R}}
\newcommand{\bN}{\mathbb{N}}


\newcommand{\markK}[1]{%
\textcolor{teal}{\sf\underline{MK:}\ #1}
}
\newcommand{\aida}[1]{%
\textcolor{magenta}{\sf\underline{AA:}\ #1}
}
\newcommand{\kevin}[1]{%
\textcolor{red}{\sf\underline{KVM:}\ #1}
}
\newcommand{\mike}[1]{%
\textcolor{olive}{\sf\underline{MT:}\ #1}
}

\newcommand{\shaun}[1]{%
\textcolor{orange}{\sf\underline{SF:}\ #1}
}

\newcommand{\po}[1]{%
\textcolor{brown}{\sf\underline{PO:}\ #1}
}
\newcommand{\he}[1]{%
\textcolor{blue}{\sf\underline{H\v S:}\ #1}
}
\newcommand{\ru}[1]{%
\textcolor{cyan}{\sf\underline{RL:}\ #1}
}

\newcommand{\todo}[1]{%
\textcolor{red}{\sf\underline{TO DO:}\ #1}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textwidth 14.5cm
\oddsidemargin 1cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Bordering of Symmetric Matrices and an Application to the Minimum Number of Distinct Eigenvalues for the Join of Graphs
%Minimum number of distinct eigenvalues\\ for joins of graphs
}
\author{
Aida Abiad\thanks{Department of Mathematics and Computer Science, Eindhoven University of Technology, Eindhoven, The Netherlands (\texttt{a.abiad.monge@tue.nl}). Department of Mathematics: Analysis, Logic and Discrete Mathematics, Ghent University, Ghent, Belgium. Department of Mathematics and Data Science, Vrije Universiteit Brussel, Brussels, Belgium. Partially supported by the Research Foundation Flanders (FWO) grant 1285921N.}
\and
Shaun M.~Fallat\thanks{Department of Mathematics and Statistics, University of Regina, Regina, SK, S4S 0A2, Canada (\texttt{shaun.fallat@uregina.ca}). Research supported in part by an NSERC Discovery Grant RGPIN--2019--03934.} 
\and
Mark~Kempton\thanks{Department of Mathematics, Brigham Young University, Provo UT 84602, U.S.A. (\texttt{mkempton@mathematics.byu.edu}).}
\and
Rupert H.~Levene\thanks{School of Mathematics and Statistics, University College Dublin, Belfield, Dublin 4, Ireland (\texttt{rupert.levene@ucd.ie} and \texttt{helena.smigoc@ucd.ie}).}
\and
Polona Oblak\thanks{University of Ljubljana, Faculty of Computer and Information Science and Faculty of Mathematics and Physics, Ljubljana, Slovenia. Institute of Mathematics, Physics, and Mechanics, Ljubljana, Slovenia (\texttt{polona.oblak@fri.uni-lj.si}). Partially supported by Slovenian Research Agency (research core funding no.~P1-0222 and project no.~J1-3004).}
\and
Helena \v Smigoc\footnotemark[4]%\thanks{School of Mathematics and Statistics, University College Dublin, Belfield, Dublin 4, Ireland (\texttt{helena.smigoc@ucd.ie}).}
\and
Michael Tait\thanks{Department of Mathematics \& Statistics, Villanova University, Villanova PA 19085, U.S.A. (\texttt{michael.tait@villanova.edu}). Partially supported by National Science Foundation grant DMS-2011553 and a Villanova University Summer Grant.}
\and
Kevin Vander Meulen\thanks{Department of Mathematics, Redeemer University, ON, L9K 1J4, Canada (\texttt{kvanderm@redeemer.ca}).
Research supported in part by an NSERC Discovery Grant RGPIN--2022--05137.}
}


%\date{\today} % FIXME hard-code date later, e.g. {3 June 2022}
%\author{AIM Joins group}
\date{}

\begin{document}

\maketitle
 
\begin{abstract}
   An important facet of the inverse eigenvalue problem for graphs is to determine the minimum number of distinct eigenvalues of a particular graph. We resolve this question for the join of a connected graph with a path. We then focus on bordering a matrix and attempt to control the change in the number of distinct eigenvalues induced by this operation. By applying bordering techniques to the join of graphs,  we obtain numerous results on the nature of the minimum number of distinct eigenvalues as vertices are joined to a fixed graph.
\end{abstract}

\noindent {\bf Keywords:} inverse eigenvalue problem,  
minimum number of distinct eigenvalues,
borderings, 
joins of graphs, 
%eigenvalue interlacing, 
paths, cycles, hypercubes.
%minimal number of distinct eigenvalues.


\noindent {\bf AMS subject classification:} 05C50, 15A18.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given a simple graph $G$ on $|G|=n$ vertices, let $S(G)$ denote the set of all $n \times n$ real symmetric matrices $A=\big(a_{ij}\big)$ such that, for $i \neq j$, $a_{ij} \neq 0$ if and only if $i$ and $j$ are adjacent in~$G$. There are no restrictions on the main diagonal entries of $A$. The inverse eigenvalue problem for~$G$ asks which possible multi-sets of eigenvalues (spectra) occur in the class $S(G)$.
This is a very difficult problem for most graphs (which generally remains open, except for some sporadic graphs, including, for example, paths, cycles, complete graphs and some basic families of trees). Considerable work on this important problem has occurred over the past several decades (see the recent book \cite{HLS2022inverse}). Our work generally pertains to multiplicity lists associated to the spectra of matrices in $S(G)$.

Suppose $A$ is an $n \times n$ real symmetric matrix and $\lambda$ is an eigenvalue of $A$, that is $\lambda \in \sigma(A)$, where $\sigma(A)$ denotes the collection of eigenvalues (spectrum) of the matrix $A$. We let $m_{A}(\lambda)$ denote the multiplicity of $\lambda$ in $\sigma(A)$; if a scalar $\lambda$ is not an eigenvalue of a matrix $A$ then we define $m_A(\lambda)=0$.
Perhaps one of the most important results on the eigenvalues of real symmetric matrices is Cauchy's interlacing inequalities, from which it immediately follows that %. A basic implication being 
if $A$ is an $n\times n$ principal submatrix %\po{We started by $n \times n$ above... I would not swap $A$ and $B$ here. But, would it be weird if we say that B is $(n+1)\times (n+1)$? This would be also consistent with $r$-bordering below.} 
of an $(n+1) \times (n+1)$ real symmetric matrix~$B$, then 
$|m_A(\lambda)-m_B(\lambda)|\leq 1$ for any scalar $\lambda$. Another way to view the principal submatrix $A$ of $B$ is to consider that $B$ was obtained from $A$ by bordering $A$ with one row and column, and since the spectrum is invariant under permutation similarity, we might as well assume that the new row and column added to $A$ are the first row and column of $B$. More generally, given a symmetric $n\times n$ matrix $A$ and $r\ge 1$, an \emph{$r$-bordering} of $A$ is any symmetric $(n+r)\times (n+r)$ matrix $B$ which contains $A$ as a trailing principal $n\times n$ submatrix (that is, $A$ lies in rows and columns indexed by $\{r+1, r+2, \ldots, n+r\}$ of $B$), and it follows that $|m_A(\lambda)-m_B(\lambda)|\leq r$. For brevity, we will also let $A[S]$ denote the principal submatrix of $A$ lying in rows and columns indexed by $S\subseteq \{1,2,\ldots, n\}$.

 

 We define the {\em maximum multiplicity} of a symmetric matrix $A$ to be $$M(A)=\max\{m_A(\lambda):\lambda\in \sigma(A)\},$$ and the \emph{maximum multiplicity of a graph~$G$} is
\[ M(G) = \max\{ M(A) : A \in S(G)\}.\]
Let ${\bf m}=(m_1,\ldots,m_k)\in \bN_0^k$ be a sequence of $k$ nonnegative integers
and %In addition we also write 
$q({\bf m})=|\{i\colon m_i>0\}|$.
We say ${\bf m}$ is an ordered multiplicity list for a symmetric matrix $A$, if $A$ possesses $q({\bf m})$ distinct eigenvalues $\lambda_1 < \lambda_2 < \cdots < \lambda_{q({\bf m})}$
%\ru{Should we use the opposite ordering here? I think we often do that in sects 3 and 4; just proof of thm 2.2 would have to be changed} 
and $m_A(\lambda_i)=m_{j_i}$ for $i=1,2,\ldots,q({\bf m})$, where $1\le j_1<j_2<\dots<j_{q(\bf m)}\le k$ are the $q({\bf m})$ indices $j$ with $m_j>0$.
%\ru{changed def here, in case some $m_i$'s are $0$} 
In this case we write ${\bf m} = {\bf m}(A)$. For any matrix $A$, we write $q(A)=k$ if $A$ has $k$ distinct eigenvalues. For a given graph $G$, we define
\[ q(G)= \min\{q(A) : A\in S(G)\}.\]
 %which coincides with $q(A)$ whenever ${\bf m}$ is the multiplicity list of a symmetric matrix $A$. 
 It is easy to observe that for any graph $G$ we have  $q(G)\ge \lceil\frac{|G|}{M(G)}\rceil$.
 In this paper our goal is to  investigate the behaviour of $q(\cdot)$ upon appending vertices to a fixed graph $G$. Here, when a vertex is appended, all possible edges between the existing vertices and the new vertex are inserted. % (see the notion of join duplication to follow).


We let $K_n$ ($n\geq 1$), $P_n$ ($n\geq 1$), $C_n$ ($n\geq 3$) denote the complete graph, the path, and the cycle on $n$ vertices.
If $G$ and $H$ are two graphs, then the {\em join of $G$ and $H$}, denoted by $G \vee H$, is the graph obtained from the union of $G$ and $H$ by adding all edges with one endpoint in $G$ and one endpoint in $H$. Hence, our goal in this paper is to investigate the behaviour of $q(G\vee H)$ for various graphs $G$ and $H$.

Given a graph~$G$, let $V(G)$ denote its vertex set. For $v \in V(G)$, we define
$\jdup(G,v)$ to be the supergraph of $G$ obtained from $G$ by duplicating $v$, with an edge connecting $v$ to its duplicate. That is, $V(\jdup(G,v))=V(G)\cup \{w\}$, where $w\not\in V(G)$, and $\{v,w\}\in E(\jdup(G,v))$, and $w$ has the same neighbours as $v$ in $\jdup(G,v)$. As observed in 
\cite[Theorem~3]{MR4284782} and \cite[Lemma~2.9]{MR3891770}, 
\begin{equation}\label{eq:jdup}
q(\jdup(G,v))\leq q(G).
\end{equation}
Since $K_{n+1}\vee H=\jdup(K_n\vee H,v)$ for any vertex $v\in K_n$, we see that $q(K_n\vee H)$ is monotone decreasing in $n$.

One of the first examples considered along these lines was the case of determining $q(K_1 \vee P_n)$. In \cite[Example~4.5]{MR3904092} it was shown that  $q(K_1\vee P_n)=\lceil \frac{n+1}2\rceil$ for $n\geq 2$.  We note here that the lower bound on $q(K_1\vee P_n)$ follows from Cauchy's interlacing inequalities since $q(P_n)=n$. 
Another important example is the star, or $S_{n} =K_1 \vee E_{n-1}$, where $E_{k}$ represents the empty graph on $k$ vertices. It is straightforward to show that $M(S_n)=n-2$ and that $q(S_n)=3$. We remark that the star has played a key role in the inverse eigenvalue problem for graphs (mostly in the case of trees), and in many ways was a critical tool used in \cite{MR978591} to establish a converse to Cauchy's interlacing inequalities. This technique has been extended and adapted to broaden the scope of which spectra can be realized by a graph that contains a dominating vertex (see, for example, \cite{MR2294340, MR2022294, levene2021paths}). 



%In particular, if $T_1$ and $T_2$ are trees (more generally: inertia-balanced) and $\{\mr(K_1 \vee T_1)\} \geq 3$,\ru{should this formula involvel $T_1$ and $T_2$?} then  \cite[Proposition~3.2]{MR2294340}\ru{\cite[Theorem~3.8]{MR2294340}?} gives us  $M(T_1\vee T_2)$.\ru{Is this paragraph necessary for our results, or just background? And should we state the formula for $M(T_1\vee T_2)$? } 

Merging the concepts of bordering a particular matrix and joining a vertex to a given graph, we are interested in determining the minimum number of distinct eigenvalues of a graph joined by a sequence of vertices, and we develop techniques, based in part of the nature of ordered multiplicity lists and eigenvectors, to aid this computation. We begin, in Section 2, with the necessary background and present a general upper bound (Theorem \ref{thm:general_upper_bound}) on $q(G \vee H)$ for connected graphs $G$ and $H$, which reduces to a simple exact formula in the case $G=P_n$. In Section~\ref{sec:border} we investigate the borderings of a given symmetric matrix. Theorem~\ref{thm:1-bordering} describes in detail how a $1$-bordering can change the spectrum of a symmetric matrix, and in Proposition~\ref{prop:C-bordering} we find a necessary and sufficient condition for the existence of an $r$-bordering of a symmetric matrix with a given value of $q$.
In Section~\ref{sec:joinswithcompletegraphs} we make several observations on the patterns of such bordered matrices, and we apply them to estimate $q(K_n\vee H)$ when $H$ is either a hypercube or a cycle. Finally, in Section~\ref{sec:limitations}, we pay particular attention to some possible limitations of our methods (Corollary~\ref{coro:limitationsnotsufficientcondition}).
%, and include a discussion involving the hypercube where we identify some curious behavior for $q(K_1 \vee Q_t)$ in Section \ref{hyper}.\po{This one is changed now.}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Background: definitions needed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{itemize}
   % \item $q(A)$, $q(\bf m)$, $q(G)$
    %\item ${\bf m}$, ${\bf m}(A)$
 %   \item \todo{All borderings of matrices add rows at the top and columns on the left. Like in \eqref{eq:1-bordering-matrix}. Also write $K_1 \vee G$, not the other way around.}
%\end{itemize}


%\todo{\st{Make all matrix bracket round.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{General graphs and paths}\label{sec:generalgraphsandpaths}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

It is known that if $G$ and $H$ are two connected graphs and $|G|=|H|$, then $q(G \vee H)=2$ (see~\cite[Theorem 5.2]{MR3506498}). This result was extended in \cite{levene2021paths,LEVENE2022213} where it was shown that $q(G \vee H)=2$ if $G$ and $H$ are connected graphs with $\big||G|-|H|\big|\leq 2$. Moreover,  for trees $T_1$ and $T_2$ we have
$q(T_1\vee T_2)=2$ if and only if $ \big||T_1|-|T_2|\big|\le2$, so in this case the result is sharp.


An important notion used in \cite{levene2021paths} is generic realizability. Recall that a matrix (vector) is said to be \emph{nowhere zero} if none of its entries is zero. Suppose $G$ is a graph with $|G|=n$ vertices and $\sigma$ is a collection of realizable eigenvalues in $S(G)$ (with multiplicities), i.e., $\sigma=\sigma(A)$ for some $A\in S(G)$. The collection~$\sigma$ is said to be {\em generically realizable} in $S(G)$ if, for any finite set~$\mathcal{Y}$ of nonzero vectors in $\bR^n$, there is an orthogonal matrix $U$ such that $Uy$ is nowhere zero for all $y \in \mathcal{Y}$, and $UDU^{T} \in S(G)$, where $D$ is a diagonal matrix with eigenvalues equal to~$\sigma$ (see~\cite{levene2021paths} for more details). Observe that if we form an $n\times |\cal{Y}|$ matrix $Y$ from the columns of $\mathcal{Y}$ then this property ensures that there is an orthogonal matrix $U$ so that $UDU^T\in S(G)$ and $UY$ is nowhere zero, whenever $Y$ has no zero column (that is, no column of $Y$ is the zero vector in $\bR^n$). We will use the following result.

\begin{theorem}\label{generic}\cite[Theorem~2.5]{levene2021paths}
  Suppose $G$ is a connected graph. Then any $\sigma$ with $|G|$ distinct elements is generically realizable in $S(G)$. 
\end{theorem}

Theorem \ref{generic} allows us to construct matrices in $S(G \vee H)$ with some desired spectral properties, using matrices $A\in S(G)$ and $B \in S(H)$ with distinct eigenvalues. In particular, in the next result we explore this idea of constructing matrices in $S(G \vee H)$ with bounded number of distinct eigenvalues.

% We begin by considering the minimum number of distinct eigenvalues of the join of two graphs with particular restrictions on their orders.

\begin{theorem}\label{thm:general_upper_bound}
Suppose $G$ and $H$ are two connected graphs. 
If $k$ is a positive integer
%\ru{"non-negative integer"? (seems OK for $k=0$)}\he{I don't think so: $k=0$ includes $G \vee H=P_2$ with $q=2$ not $q=1$?}\ru{oops, that's right}
and $|G|\leq |H|\leq k |G|+k+1$, then $$q(G\vee H) \leq k+1.$$ 
In particular, for any connected graphs $G$ and $H$ with $\max\{|G|,|H|\}\ne1$ we have: 
\[ q(G\vee H)\le \left\lceil\frac{|G|+|H|}{\min\{|G|, |H|\}+1}\right\rceil.\]
% Hence, writing $|G|=n$ and $|H|=m$, if $n\le m$ and $m>1$, then 
% \[ q(G\vee H)\le \left\lceil\frac{n+m}{n+1}\right\rceil.\]
% \he{I suggest: In particular, for any connected graphs $G$ and $H$ \ru{with $\max\{|G|,|H|\}\ne1$} we have: 
% \[ q(G\vee H)\le \left\lceil\frac{|G|+|H|}{\min\{|G|, |H|\}+1}\right\rceil.\]
% }\ru{we could do that. Should we change all the other statements to match?}
\end{theorem}
\begin{proof}
Suppose $|G|=n$, $|H|=m$ and $k$ is a positive integer with $n\le m\le kn+k+1$. To prove the first claim, we will construct a matrix in $S(G\vee H)$ with distinct eigenvalues contained in $\mathcal S:=\{\lambda_j\}_{j=1}^{k+1}$ for any chosen set~$\mathcal S$ of $k+1$ distinct numbers. To this end, choose real numbers $\lambda_1 < \cdots < \lambda_{k+1}$, and integers $k_i$ with $1\le k_i\le k$ for $i=1,\ldots,n$, satisfying:
\begin{align*}
%1 &\leq k_i\leq k, \\
0\leq k':=m-\sum_{i=1}^nk_i &\leq k+1.
\end{align*}
Now select $n$ sets of real numbers 
$\mathcal M_i:=\{\mu_{i,1}, \ldots, \mu_{i,{k_i}}\}$, $ i=1,\ldots,n$, where we assume $$\mu_{i,1} <  \cdots < \mu_{i, {k_i}}.$$ Furthermore, we assume that $\mathcal M_i$  strictly interlaces $\{\lambda_1,\ldots,\lambda_{k_i+1}\}$, that the numbers $\mu_{i,j}$ for $j=1,\ldots,k_i$ and $i=1,\ldots,n$ are all distinct, and finally we demand that numbers $a_i: = \left(\sum_{j=1}^{k_i+1} \lambda_j \right) - \left( \sum_{j=1}^{k_i} \mu_{i,j} \right)$, $i=1,\ldots,n$, are all distinct. 
%\begin{align*}
% \mu^{(1)}_1 > & \cdots > \mu^{(1)}_{k_1}\\
% & \vdots \\
% \mu^{(n)}_1 > & \cdots > \mu^{(n)}_{k_n}
% \end{align*}
 Writing $\diag(x_1,\dots,x_m)$ for the diagonal matrix with main diagonal $(x_1,\dots,x_m)$, we define 
\begin{align*}
\Lambda &:= \mathrm{diag}(\lambda_1,\ldots, \lambda_{k'}),\\
D_i &:= \mathrm{diag}(\mu_{i,1},\ldots, \mu_{i, {k_i}}),\;i=1,\dots,n,\\
D_a &:= \mathrm{diag}(a_1,\ldots, a_n),\\
D_\mu &:=D_1\oplus\dots\oplus D_n.
\end{align*}
By \cite[Theorem 4.2]{MS} 
and our strict eigenvalue interlacing requirement, for $i=1,\ldots,n$ there exist matrices: 
\[
M_i: = \begin{pmatrix}
 a_i & \mathbf{b}_i^T \\
 \mathbf{b}_i & D_i
\end{pmatrix}
\]
with eigenvalues $\lambda_1,\ldots , \lambda_{k_i+1}$, where $\mathbf{b}_i$ is a nowhere zero vector. 
Clearly, the distinct eigenvalues of $M:=M_1\oplus\dots\oplus M_n\oplus \Lambda$ are contained in $\{\lambda_1,\dots,\lambda_{k+1}\}$, and in particular, $q(M)\le k+1$. The same is true for the matrix:
\[
M': = \begin{pmatrix}
D_a & B^T &0 \\
B & D_\mu&0\\
0&0&\Lambda
\end{pmatrix},
\]
where $B=\bigoplus_{i=1}^n \mathbf{b}_i$, since $M'$ is permutationally similar to $M$. 

Observe that the $n\times n$ matrix $D_a$ and the  $m\times m$ matrix $D_\mu\oplus \Lambda$ are both diagonal matrices with distinct eigenvalues. By Theorem \ref{generic},  their spectra are generically realizable for $G$ and $H$, respectively. Since the $m\times n$ matrix $Y:=\left(\begin{smallmatrix}B \\ 0 \end{smallmatrix}\right)$ has no zero column, by generic realizability for $H$ there is an orthogonal matrix $V$ so that $V( D_\mu\oplus \Lambda) V^T \in S(H)$ and $VY$ is nowhere zero. Since $(VY)^T=Y^TV^T$ is nowhere zero and so has no zero column, by generic realizability for~$G$ there is an orthogonal matrix $U$ so that  $UD_aU^T \in S(G)$ and $UY^TV^T$ is nowhere zero. 
%In particular, there exist orthogonal matrices $U$ and $V$ so that $UD_aU^T \in S(G)$, $V( D_\mu\oplus \Lambda) V^T \in S(H)$, and $U\begin{pmatrix}B^T & 0 \end{pmatrix}V^T$ is nowhere zero.
 Now
% \[
% (U\oplus V)M' (U^T \oplus V^T) = \begin{pmatrix}
% UD_a U^T & U\begin{pmatrix}B^T & 0 \end{pmatrix}V^T  \\
% V\begin{pmatrix}B \\ 0 \end{pmatrix}U^T & V(D_\mu\oplus \Lambda) V^T
% \end{pmatrix}\in S(G\vee H),
% \]
\[
(U\oplus V)M' (U^T \oplus V^T) = \begin{pmatrix}
UD_a U^T & UY^TV^T  \\
VYU^T & V(D_\mu\oplus \Lambda) V^T
\end{pmatrix}\in S(G\vee H),
\]
so $q(G\vee H)\le k+1$ as required.

To see that the second claim follows from the first, observe that if $\max\{|G|,|H|\}>1$, then $|G|+|H|>\min\{|G|,|H|\}+1$, so $k:=\left\lceil{\frac{|G|+|H|}{\min\{|G|,|H|\}+1}}\right\rceil-1$ is a positive integer, and if $|G|\le |H|$, then $|G|\le |H|\le k|G|+k+1$, so $q(G\vee H)\le k+1=\left\lceil{\frac{|G|+|H|}{\min\{|G|,|H|\}+1}}\right\rceil$. By symmetry, the same holds if $|H|\le |G|$. 
\end{proof}

% % note that these matrices have spectra $\{a_i\}$ and $\{\mu_j^{(i)}\}\cup\{\lambda_j\}$, respectively. 
% %Furthermore, because of the distinctness of the eigenvalues, $U$ and $V$ may be chosen generically.  

% Now define the matrix $M = M_1\oplus\dots\oplus M_n\oplus \Lambda$ and note that $M$ is permutationally similar to 
% \[
% M': = \begin{pmatrix}
% D_a & B &0 \\
% B^T & D_\mu&0\\
% 0&0&\Lambda
% \end{pmatrix}
% \]
% where the matrix $B=\bigoplus_{i=1}^n \mathbf{b}_i$ has one $\mathbf{b}_i$ in each row and is zero everywhere else. Since the eigenvalues of each $M_i$ are a subset of $\{\lambda_j\}_{j=1}^{k+1}$, the matrix $M'$ has $k+1$ eigenvalues. Note that, writing $B_0=(B~0)$, we have
% \[
% (U\oplus V)M' (U^T \oplus V^T) = \begin{pmatrix}
% UD_a U^T & UB_0V^T  \\
% VB_0^T U^T & VD_\mu V^T
% \end{pmatrix}.
% \]
% Furthermore, by the distinctness of the eigenvalues, $U$ and $V$ may be chosen to be generically realizable (see \cite[Theorem~2.5]{levene2021paths})  and hence  $UB_0V^T$ 
% is nowhere zero. Thus this matrix is in $S(G\vee H)$.
% \end{proof}

We remark that the hypothesis $|G|\le |H|\le k|G|+k+1$ in Theorem~\ref{thm:general_upper_bound} cannot be relaxed in general, since if we take $k=1$ and $G$ and $H$ are trees with $|H| >k|G|+k+1=|G|+2$, then $q(G\vee H)>2=k+1$ by~\cite[Example~3.5]{levene2021paths}.

The upper bound of Theorem \ref{thm:general_upper_bound} is sharp when $H$ is a path, as shown below. 
%\he{I would not call this result a theorem, maybe straightforward lemma?}

% From Theorem \ref{thm:general_upper_bound} 
% % and Lemma \ref{thm:general_lower_bound}, 
% we obtain the following corollary.

\begin{corollary}\label{GjoinP}
If $m>1$ and $G$ is a connected graph with $|G|=n\leq m$, 
then
$$q(G\vee P_m)=\left\lceil \frac{n+m}{n+1}\right\rceil.$$ 
\end{corollary} 
%\todo{put the lemma in the proof of the corollary}
% \begin{lemma}\label{thm:general_lower_bound}
% Let $m, n \in \bN$ and $G$ be a graph with $|G|=n.$ Then \[q(G\vee P_m) \geq \frac{n+m}{n+1}.\] In particular, for $m=kn+k+1$, we have $q(G\vee P_m)\geq k+1$.
% \end{lemma}
\begin{proof}
Let $X$ be a matrix in $S(G\vee P_m)$. Since $X$ has an $m \times m$ principal submatrix corresponding to $P_{m}$, this submatrix must have distinct eigenvalues. By eigenvalue interlacing, the matrix $X$ can have maximum eigenvalue multiplicity at most $n+1$. Hence 
\begin{equation*}
q(X) \geq \left\lceil \frac{|G\vee P_m|}{M(X)}\right\rceil  \geq \left\lceil\frac{n+m}{n+1}\right\rceil.
\end{equation*}
The opposite inequality was established in Theorem~\ref{thm:general_upper_bound}.
\end{proof}

%\he{Is the corollary below really worth stating as a separate result? I would just include what is needed in Remark \ref{rk:paths}}\ru{I agree; this is now changed}
% \begin{corollary}\label{cor:join_paths}
% For $2\leq n\leq m$, we have
% \[
% q(P_n\vee P_m) = \left\lceil\frac{n+m}{n+1}\right\rceil.
% \]
% \end{corollary}


\begin{remark}\label{rk:paths}
In the case $G=P_n$ where $2\le n\le m$, the formula of Corollary~\ref{GjoinP} improves on the upper bound $q(P_n\vee P_m)\le \lceil \tfrac {n+m}2\rceil$ which follows from \cite[Corollary~49]{MR3665573}, since $P_n\vee P_m$ contains a Hamiltonian cycle. %\ru{edited this down}
% \cup \{e_1,e_2\}$ is a Hamiltonian path in $P_n\vee P_m$ \cite[Corollary~49]{MR3665573}, 
% The next result follows from the work above. However, we also note that by \cite[Corollary~49]{MR3665573}, if $C_k\subseteq G$, then $q(G)\le |G|-\lfloor \tfrac k2\rfloor$ and 
%  in particular 
%  $q(P_n\vee P_m)\le \lceil \tfrac {n+m}2\rceil$, .
\end{remark}

%\mike{I thought these techniques also have implications for a path join a complete graph and that is a question that is asked in one of the previous papers. So we should include whatever it is that we can actually prove (I think eg at least we know the answer if the number of vertices of the complete graph is at most the number of vertices in the path)}

%\aida{Indeed, see scribe May 10}




\noindent
We conclude this section with %Theorem %\ref{thm:valuejoincompletepath}, 
a theorem which resolves a question from \cite[Remark 3.13]{levene2021paths}.

%\he{The theorem below should proably be called corollary.}\ru{changed}
\begin{corollary}\label{cor:valuejoincompletepath}
 If $m,n \geq 2$, then
 \[q(K_n \vee P_m) = 
 \left\lceil \frac{n+m}{n+1} \right\rceil.
 \]
\end{corollary}

\begin{proof}
For $n \leq m$, this is a special case of Corollary~\ref{GjoinP}. For $n\geq m$, note that $\lceil \frac{m+n}{n+1}\rceil=2$. We know from  Theorem 5.2 in
\cite{MR3506498} that $q(K_n \vee P_n)=2$, and for $n> m$, it follows that $q(K_n \vee P_m)=2$ by applying the notion of join duplication (jdup) and the inequality presented in (\ref{eq:jdup}). \end{proof}




%\aida{In \cite{levene2021paths}, the authors show a purely combinatorial sufficient condition for two graphs $G$ and $H$ to have $q(G \vee H) = 2$ (this is closely related to the existence of compatible multiplicity matrices). should we relate it with the above result for the particular case of $2$ when $m\geq n$?.}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bordering}\label{sec:border}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\ru{experimental presentation... there will be overlap with some of the other things.}

%\todo{Do we want to pull some notation introduced in this section to the beginning, like $m_A(\alpha,\beta)$, $q(\bf m)$, etc? }

Recall from the introduction that an \emph{$r$-bordering} of a symmetric $n \times n$ matrix $A$ is any symmetric $(n+r)\times (n+r)$ matrix $B$ which contains $A$ as its $n \times n$ trailing principal submatrix of $B$.
%(that is, $A$ is the principal submatrix of $B$ in the rows and columns indexed by $\{r+1, r+2, \ldots, n+r\}$). \aida{introduce the trialing principal submatrix in the introduction} 
Building upon the classical results derived from Cauchy's interlacing inequalities that characterize all possible eigenvalues of a $1$-bordering of $A$, we aim to understand the fewest number of distinct eigenvalues possible for an $r$-bordering of $A$. 

 First we have a look at $1$-borderings, noting that any $r$-bordering of $A$ can be obtained by repeated $1$-bordering. 

\begin{theorem}\label{thm:1-bordering}
Let $A$ be an $n\times n$ symmetric matrix and $A'$ a $1$-bordering of $A$. The following statements are equivalent: 
%\ru{If $\mathcal{R}_0=\emptyset$ then $k=0$ and I don't know how to interpret condition 2. Should we exclude that case?}
\begin{enumerate}
\item\label{thm:1-bordering1} $\mathcal{N}$ is the set of distinct eigenvalues $\lambda$ of $A'$ that satisfy $m_{A'}(\lambda)=m_{A}(\lambda)+1$, and 
$\mathcal{R}_0$ is the set of distinct eigenvalues $\lambda$ of $A$ that satisfy $m_{A'}(\lambda)=m_{A}(\lambda)-1$.
\item\label{thm:1-bordering2}
$A'=\left(
\begin{array}{cc}
 \alpha & {\bf b}^TU_0^T \\
 U_0{\bf b} & A \\
\end{array}
\right)$
where $k:=|\mathcal{R}_0|$, $U_0$ is an $n\times k$ matrix with $U_0^T U_0=I_{k}$, and $U_0^TAU_0$ is a $k \times k$ diagonal matrix $D_0$ with distinct eigenvalues equal to $\mathcal{R}_0$. Further, ${\bf b} \in \mathbb{R}^k$ is a nowhere zero vector
so that the matrix $$B=\left(
\begin{array}{cc}
 \alpha & {\bf b}^T \\
 {\bf b} & D_0 \\
\end{array}
\right)$$
has eigenvalues $\mathcal{N}$. 
\end{enumerate}
If the above hold, then
$A'$ is similar to a matrix of the form $D_{\mathcal{N}} \oplus D_1$ for some diagonal matrix $D_1$ via an orthogonal similarity using
\begin{equation}\label{eq:Wsimilarity}
W=\left(
\begin{array}{cc}
 {\bf v}^T & 0 \\
 U_0V_0 & U_1 \\
\end{array}
\right),
\end{equation}
where 
%${\bf v}\in \bR^{|\mathcal{N}|}$ and 
$V=\left(
\begin{array}{c}
 {\bf v}^T \\
 V_0
\end{array}
\right)\in \bR^{|\mathcal{N}|\times |\mathcal{N}|}$ is an orthogonal matrix that
satisfies $V^T BV=D_{\mathcal{N}}$, and $U=\left(
\begin{array}{cc}U_0 & U_1\end{array}
\right)$ is an orthogonal matrix that satisfies $U^TAU=D_{0} \oplus D_1$. 
\end{theorem}

\begin{proof}
\begin{description}
    \item[$(1\Rightarrow 2)$]
Let $\lambda_1, \ldots, \lambda_q$ be the distinct eigenvalues of $A$ with multiplicities $m_i:=m_{A}(\lambda_i)$, $i=1,\ldots,q$, and let $U'$ be an orthogonal matrix that diagonalizes $A$, that is, $U'^TAU'=\oplus_{j=1}^q \lambda_j I_{m_j}$. Then for some $\alpha\in \bR$ and ${\mathbf a}\in \bR^n$, we have $$A'_1:=(1\oplus U'^T)A'(1\oplus U')=\left(\begin{array}{cc}
 \alpha & {\bf a}^T \\
 {\bf a} & \oplus_{j=1}^q \lambda_j I_{m_j} \\
\end{array}\right).$$  

Write ${\bf a}^T=\left(
\begin{array}{cccc}
 {\bf a}_1^T & {\bf a}_2^T & \cdots & {\bf a}_q^T \\
\end{array}
\right)$, where ${\bf a}_i\in \mathbb{R}^{m_i}$. 
Choose orthogonal matrices $Z_i\in \mathbb{R}^{m_i\times m_i}$ that satisfy $Z_i{\bf a}_i=b_i {\bf e}_1$, where $b_i \in \mathbb{R}$ and ${\bf e}_1$ denotes the basic unit vector in $\bR^{m_i}$ whose first element is equal to $1$. Note that $b_i\neq 0$ if and only if $\lambda_i \in \mathcal R_0$ (see for example \cite[Lemma 5.1]{MR3567513}  for the nontrivial implication).  %(Indeed, if we write $B'=\big(\begin{smallmatrix}\alpha&{{\bf b}'}{}^T\\{\bf b}'&D'\end{smallmatrix}\big)$ where ${{\bf b}'}=(b_1\;\dots\;b_q)^T$, $D'=\diag(\lambda_1,\dots,\lambda_q)$ and $D''=\bigoplus_{j=1}^q \lambda_j I_{m_j-1}$, then $A'$ is orthogonally similar to $B'\oplus D''$, and a simple calculation gives $b_i\ne0\iff \det(B'-\lambda_iI_{q+1})\ne 0\iff m_{A'}(\lambda_i)= m_{D''}(\lambda_i)=m_i-1\iff \lambda_i\in {\mathcal R}_0$.)
%Conversely, if $b_i=0$, then ${\bf a}_i=0$, hence $m_{A'}(\lambda_i)=m_{A_1'}(\lambda_i)\ge m_i$, so $\lambda_i\not\in {\mathcal R}_0$.)
Applying the orthogonal similarity $1 \oplus (\oplus_{i=1}^q Z_i )$ to $A'_1$, followed by a permutation similarity $1 \oplus P$, we see that $A'$ is orthogonally similar to $B \oplus D_1$, where $D_1$ is a diagonal $(n-k)\times (n-k)$ matrix, \[B=\left(
\begin{array}{cc}
 \alpha & {\bf b}^T \\
 {\bf b} & D_0 \\
\end{array}
\right)\]
and ${\bf b}\in \bR^{k}$ is a nowhere zero vector. In summary, $U:=U'(\oplus_{i=1}^k Z_i^{T} )P$ satisfies $U^TAU=D_0 \oplus D_1$ and $(1 \oplus U)^TA'(1\oplus U)=B \oplus D_1$. 
Writing $U=\left(
\begin{array}{cc}
 U_0 & U_1 \\
\end{array}
\right)$ where $U_0\in \bR^{n\times k}$ and computing $A'=(1 \oplus U)(B \oplus D_1)(1\oplus U^T)$ gives the form for $A'$ as in item~\ref{thm:1-bordering2}.
\item[$(2\Rightarrow 1)$] Let $A'$ and $U_0$ be as in item \ref{thm:1-bordering2}, and $U_1 \in  \mathbb{R}^{n\times (n-k)}$ be such that $U:=\left(
\begin{array}{cc}
 U_0 & U_1 \\
\end{array}
\right)$ is orthogonal and
$ U^TAU$ is a diagonal matrix $D_0 \oplus D_1$. From
$$(1 \oplus U^T)A'(1\oplus U)=B \oplus D_1$$
we conclude that $A'$ has eigenvalues as stated in item \ref{thm:1-bordering1}.
\end{description}\medskip

To prove the final claim we note that:
$$W:=(1\oplus U)(V\oplus I_{n-k})=\left(
\begin{array}{cc}
 {\bf v}^T & 0 \\
 U_0V_0 & U_1 \\
\end{array}
\right)$$
and $W^TA'W=(V^T\oplus I)(B\oplus D_1)(V\oplus I)=D_{\mathcal{N}} \oplus D_1$, as claimed.
\end{proof}

Theorem \ref{thm:1-bordering} provides a construction of a $1$-bordering of a symmetric matrix, subject to quite general eigenvalue constraints. Our first application of this theorem produces a known result
\cite[Thm. 4.3.10]{HJ1}. We include it here mostly to establish notation that we will depend on in the rest of this section. 
 

% all possibilities for eigenvalues of a $1$-bordering of $A$ given the eigenvalues of $A$.  

\begin{corollary}\label{lem:1-border}
Let $A$ be an $n\times n$ symmetric matrix, $\mathcal{R}$ the set of distinct eigenvalues of $A$, and $\mathcal{R}_0\subseteq \mathcal{R}$. If $\mathcal{N}$ is any set of $|\mathcal{R}_0|+1$ distinct real numbers which strictly interlaces $\mathcal{R}_0$, then there is a $1$-bordering $A'$ of $A$ so that for $\lambda \in \bR$,
\begin{align*}
m_{A'}(\lambda)=\begin{cases}m_{A}(\lambda)-1 &\text{if }\lambda \in \mathcal{R}_0, \\
m_{A}(\lambda)+1 &\text{if }\lambda \in \mathcal{N}, \\
m_{A}(\lambda) &\text{otherwise},\end{cases}
\end{align*}
where $m_{A'}(\lambda)=0$ means that $\lambda$ is not an eigenvalue of $A'$. %\aida{we defined this last line in the introduction already?}
%\po{Do symbols ${\mathcal R}_0$ and $\mathcal N$ have a meaning? Can we denote them $\mathcal{R}_-$ and $\mathcal{R}_+$ :)}
\end{corollary}

\begin{proof}
Let $D_0$ be a diagonal matrix with distinct diagonal elements equal to elements in $\mathcal{R}_0$.
%, and $D_1$ be a diagonal matrix so that $D:=D_0 \oplus D_1$ is cospectral with $A$. Furthermore let $U_0 \in \mathbb{R}^{n\times |\mathcal{R}_0|}$ and $U_1 \in  \mathbb{R}^{n\times (n-|\mathcal{R}_0|)}$ be such that $U:=\left(
%\begin{array}{cc}
% U_0 & U_1 \\
%\end{array}
%\right)$ is orthogonal and
%$ U^TAU=D$. 
%Then
%\begin{align*}
%    U_0^TU_0&=I_{|\mathcal{R}_0|}, \, U_1^TU_1=I_{n-|\mathcal{R}_0|},\,
  %U^TAU&=D, \,
%  U_0^TAU_0=D_0, \text{ and }    U_1^TAU_1=D_1. 
%\end{align*}
By \cite{boley-golub},
%\cite{MR978591}, 
since $\mathcal{N}$ strictly interlaces $\mathcal{R}_0$,  there exist $a \in \mathbb{R}$ and a (nowhere zero) vector ${\bf b} \in \mathbb{R}^{|\mathcal{R}_0|}$  so that the matrix $$B=\left(
\begin{array}{cc}
 a & {\bf b}^T \\
 {\bf b} & D_0 \\
\end{array}
\right)$$
has the set of eigenvalues equal to $\mathcal{N}$. The result now follows from Theorem \ref{thm:1-bordering}.
%; hence, $S\oplus D_1$ has precisely the eigenvalues and multiplicities we require of the matrix $A'$. Now
%\begin{equation}\label{eq:1-bordering-matrix}
%A':=(1 \oplus U) (S \oplus D_1) (1 \oplus U^T)=\left(
%\begin{array}{cc}
% a & {\bf b}^TU_0^T \\
% U_0{\bf b} & A \\
%\end{array}
%\right)
%\end{equation}
%is $1$-bordering of $A$ which is similar, and hence cospectral to, $S\oplus D_1$, so the proof is complete.
% has the  eigenvalues (with their multiplicities) as desired for $A'$,  and it is similar to a $1$-bordering of $A$. Indeed, taking $A':=(1 \oplus U) (S \oplus D_1) (1 \oplus U^T)$ we get the matrix:
%
% that satisfies the requirements of the lemma.
\end{proof}

Starting with the eigenvalues of $A$, we will reduce the number of distinct eigenvalues of an $r$-bordering of $A$ by removing all eigenvalues from different intervals. Along these lines, we let $m_A(\alpha,\beta)$ denote the sum of multiplicities of all eigenvalues $\lambda$ of $A$ that are contained in the open interval $(\alpha,\beta)$, where $\alpha,
\beta\in\bR\cup \{-\infty,\infty\}$ with $\alpha<\beta$. 



The following straightforward consequence of eigenvalue interlacing produces a lower bound on $r$ for an $r$-bordering to have no eigenvalues in a given interval. 
\begin{lemma}\label{lem:bordering-interlacing}
   If $M$ is an $r$-bordering of a symmetric matrix~$A$ and $\alpha,\beta\in \bR\cup \{-\infty,\infty\}$ with $\alpha<\beta$, then \[|m_A(\alpha,\beta)- m_M(\alpha,\beta)|\le r.\]
\end{lemma}
\begin{proof}
  The eigenvalues of $A$ and any $1$-bordering of $A$ must interlace by Cauchy's interlacing inequalities, which establishes the case $r=1$. In general, $M$ is obtained by $r$ successive $1$-borderings of $A$, and the statement follows immediately.
\end{proof}



Let ${\bf m}=(m_1,\ldots,m_k)\in \bN_0^k$ be an ordered multiplicity list of a symmetric matrix. 
%We write $q({\bf m})=|\{i\in [k]:m_i>0\}|$, which coincides with $q(A)$ whenever ${\bf m}$ is the multiplicity list of a symmetric matrix $A$. 
For $2\leq t\leq k$, we define
\begin{equation}\label{eq:C(m,t)}
 C({\bf m},t)=\min_{1=p_1\le p_2\le \cdots \le p_t=k}\left( \max_{1\le i\le t-1}
g_{\bf m}
(p_i,p_{i+1})\right)\end{equation}
where \[
g_{\bf m}(p_i,p_{i+1}):=
\sum_{j=p_i+1}^{p_{i+1}-1}m_j .\]

In colloquial terms,
%\footnote{{\color{red} 
%KVM: I inserted an example for the colloquial explanation for $C(\mathbf{m},t)$, but am now wondering if it would be clearer to define 
%\[
%C(A,t)=
%\min_{1=p_1\le p_2\le %\cdots \le p_t=k}\left( %\max_{1\le i\le t-1}
%m_A(p_i,p_{i+1})\right)
%\]
%We might also be able to avoid the notation $q(\mathbf{m})$. Unless
%I am mistaken, Prop. \ref{prop:monotone} is the only place we really use the idea of an eigenvalue having multiplicity zero (are there others?) but we could avoid that by
%adding one line describing
%$m(A_k).$
%}{\color{blue}RL - seems like it could be clearer, yes! We can still point out that $C$ depends only on the multiplicity list, not the specific eigenvalues. [I guess the $p_i$'s are in $\sigma(A)$ here, and $k=q(A)$.]}}
$C({\bf m},t)$ is the solution to the problem of minimizing the largest ``gap multiplicity'' $g_{\bf m}$ of ${\bf m}$, over the gaps given by the various choices of $t$ ``gap boundaries'' $1=p_1\le p_2\le \cdots\le p_t=k$. 

\begin{example}
To illustrate the preceding definition, we demonstrate how $C({\bf m},t)$ is computed for the case when $\mathbf{m}=(1,2,5,5,3,1)$ and $t=3$, by listing the possible values of the gap multiplicities for the various choices of gap boundary $p_2$ in Table~\ref{tab:gaps}. 
The minimum of the maximum gap multiplicities is 7, so $C(\mathbf{m},3)=7$.
One can also determine that $C(\mathbf{m},4)=3$, $C(\mathbf{m},5)=2$, and $C(\mathbf{m},2)=15$. 
\end{example}

\begin{table}[htb]
\begin{center}
\begin{tabular}{cccc}\hline
$p_2$ & $g_{\bf{m}}(p_1,p_2)$ & $g_{\bf{m}}(p_2,p_3)$ & \parbox{3cm}{\centering \strut maximum gap multiplicity\strut}\\ \hline
1 & $0$ & $\hspace{-5ex}m_2+m_3+m_4+m_5=2+5+5+3\hspace{-3ex}$ & 15\\
2 & $0$ & $m_3+m_4+m_5=5+5+3$ & 13\\
3 & $m_2=2$ & $m_4+m_5=5+3$ & 8 \\
4 & $m_2+m_3=2+5$ & $m_5=3$ & 7 \\
5 & $m_2+m_3+m_4=2+5+5$&  $0$ & 12 \\
6 & $m_2+m_3+m_4+m_5=2+5+5+3\hspace{-5ex}$&  $0$ & 15 \\ \hline
\end{tabular}
\caption{The list of possible values of $p_2$ and the corresponding parameters inside the formula for $C({\mathbf m},t)$ for ${\bf m}=(1,2,5,5,3,1)$ and $t=3$. Note that in each case, we have $p_1=1$ and $p_3=6$, whereas $p_2$ can vary. The list of multiplicities
in each of the two gaps derived from each value of $p_2$ and the corresponding maximum gap multiplicities are given.}\label{tab:gaps}
\end{center}
\end{table}



Note that $q({\bf m})\leq t$ if and only if $C({\bf m},t)=0$. Indeed, if $q({\bf m})\le t$, then we may assume that ${\bf m}\in \bN^k$ where $k=q({\bf m})\le t$, and then choosing $p_i=\min \{i,k\}$ in \eqref{eq:C(m,t)} shows that $C({\bf m},t)=0$; conversely, if $C({\bf m},t)=0$ is attained for some particular $1=p_1\le p_2\le\dots\le p_t=k$, then 
%we necessarily have
%$t=k$,
$m_i=0$ for all $i\in [k]\setminus \{p_1,\dots,p_t\}$, 
hence $q({\bf m})\le t$. Hence, we can view $C({\bf m},t)$ as a measure of how far the multiplicity list ${\bf m}$ is from having $q({\bf m})=t$. This will be made more precise in the next proposition. %Proposition~\ref{prop:C-bordering} below. 



\begin{proposition}\label{prop:C-bordering}
Let $A$ be a symmetric matrix with $k\ge2$ distinct eigenvalues and ordered multiplicity list ${\bf m}=(m_1,\ldots,m_k)\in \bN^k$, and let $2\le t\le k$. For $r \in \bN_0$ the following statements are equivalent:
\begin{enumerate}
    \item there is an $r$-bordering $M$ of $A$ with $q(M)\le t$;
    \item $C({\bf m},t)\le r$.
\end{enumerate}
%and corresponding eigenvalues $\lambda_1<\dots<\lambda_k$.
\end{proposition}
\begin{proof}Let $\lambda_1<\cdots<\lambda_k$ be the distinct eigenvalues of $A$, with $m_A(\lambda_i)=m_i$ for $i=1,\ldots,k$. 
\begin{description}
    \item[$(1\Rightarrow 2)$] %Suppose $M$ is an $r$-bordering of $A$ with $q(M)=\tau\le t$. We will show that $C({\bf m},t)\le r$. 
    Suppose $\mu_1<\cdots<\mu_\tau$ are the distinct eigenvalues of some $r$-bordering $M$ of $A$, where $\tau\le t$. By eigenvalue interlacing, we have $\lambda_j\in [\mu_1,\mu_\tau]$ for every $j$. Hence, there is a unique $i_0$ with $\lambda_1\in [\mu_{i_0},\mu_{i_0+1})=[\nu_1,\nu_2)$, where $\nu_i:=\mu_{i_0-1+i}$. For $1\le i\le \tau-i_0$, define
  \[ p_i:=\min\{j\colon 1\leq j \leq k, \lambda_j\in [\nu_{i},\nu_{i+1})\} \]
  and let $p_{i}:=k$ for $i> \tau-i_0$. Then $1=p_1\le p_2\le\cdots\le p_t=k$. Moreover, if $p_i<j<p_{i+1}$, then $\lambda_j\in (\nu_{i},\nu_{i+1})$, so
  \[%\sum_{j:p_i<j<p_{i+1}} m_j 
  g_{\mathbf m}(p_i,p_{i+1})= \sum_{j:p_i<j<p_{i+1}} m_A(\lambda_j)\le m_A(\nu_{i},\nu_{i+1})\le r,\]
  where the final inequality follows from Lemma~\ref{lem:bordering-interlacing}, since $m_M(\nu_i,\nu_{i+1})=0$. Hence,
  \[ C({\bf m},t)\le \max_{1\le i\le t-1} %\sum_{j:p_i<j<p_{i+1}} m_j 
  g_{\mathbf m}(p_i,p_{i+1})\le r,\]
  as required.
    \item[$(2\Rightarrow 1)$] If $C({\bf m},t)=0$, then $q(A) \leq t$ and we can take $r=0$. From now on we assume $C({\bf m},t)>0$.  Since $r\ge C({\bf m},t)$, there exist $p_1=1<p_2<\cdots<p_{t'}=k$ where ${t'}\le t$ so that 
  \[ m_A(\lambda_{p_i},\lambda_{p_{i+1}})=g_{\mathbf m}(p_i,p_{i+1})%\sum_{j:p_i<j<p_{i+1}}m_j
  \le r,\quad 1\le i<{t'}.\]
  It suffices to find a $1$-bordering $M_1$ of $A$ so that $\sigma(M_1)\subseteq [\lambda_1,\lambda_k]$ and
  \[ m_{M_1}(\lambda_{p_i},\lambda_{p_{i+1}})\le \max\{r-1,0\},\quad 1\le i<{t'},\]
  since we can then continue inductively to find $A=M_0,M_1,\ldots,M_r=:M$, where $M_{\ell+1}$ is a $1$-bordering of $M_\ell$, so that $m_{M_r}(\lambda_{p_i},\lambda_{p_{i+1}})=0$ for $1\le i<{t'}$ and every eigenvalue of $M_r$ is in $[\lambda_1,\lambda_k]$, hence $M_r$ has only the ${t'}$ distinct eigenvalues $\{\lambda_{p_1},\ldots,\lambda_{p_{t'}}\}$.
  
  To show that such a matrix $M_1$ exists, first enumerate the open intervals $L_i:=(\lambda_{p_i},\lambda_{p_{i+1}})$ which contain at least one eigenvalue of $A$ as $L_{i_1},\ldots,L_{i_s}$, where $1\le i_1<\cdots<i_s<{t'}$, and choose $\mu_j\in \sigma(A)\cap L_{i_j}$ for $1\le j\le s$. (The assumption $C({\bf m},t)>0$ guarantees that at least one such interval exists.) Let $\mathcal{R}_0=\{\mu_1,\ldots,\mu_s\}$, and choose any set $\mathcal{N}\subseteq \{\lambda_{p_{1}},\ldots,\lambda_{p_{{t'}}}\}$ of size $s+1$ which strictly interlaces $\mathcal{R}_0$. The matrix constructed in Corollary~\ref{lem:1-border} then has the desired properties.
%   there is a matrix $M_1$ spectrum~$\Lambda$. Up to permuting rows and columns, we have $A=D_0\oplus D_1$ where $D_0$ is also diagonal, and then $M_1:=D_0\oplus D_1'$ is the desired $1$-bordering of $A$, since no eigenvalue of $D_1'$ is in $L_i$ for any $i$, so \[ m_{M_1}(\lambda_{p_i},\lambda_{p_{i+1}})=\begin{cases}0&m_{A}(\lambda_{p_i},\lambda_{p_{i+1}})=0\\m_{A}(\lambda_{p_i},\lambda_{p_{i+1}})-1&\text{otherwise}.\end{cases}\qedhere\]
\qedhere\end{description}
\end{proof}




% Since $r\ge C({\bf m},t)$, there exist $\tau\le t$ and real numbers $\rho_1< \ldots < \rho_{\tau}$, $\rho_1 \leq \lambda_1$, $\rho_{\tau}\geq \lambda_k$, so that 
%   \[ m_A(\rho_i,\rho_{i+1})=\sum_{j:\rho_i<\lambda_j<\rho_{i+1}}m_j\le r,\quad 1\le i<\tau.\]
% % Our aim is to show that $\rho_1,\ldots,\rho_{\tau}$ can be the eigenvalues of an $r$-bordering of $A$.
%  Note that $\{\rho_1,\ldots,\rho_{\tau}\}$ has to have a non-trivial intersection with the set of eigenvalues of $A$, but does not have to be its subset. There may be several ways to choose $\rho_i$, and making this choice is the first step in the algorithm we are developing. Below, let $\sigma(\alpha, \beta)$ denote the set of distinct eigenvalues of $A$ on the interval $(\alpha,\beta)$.
 
%   {\bf Step 0:} Choose $\rho_i'$, $i=1,\ldots,t$, as above.
  
%   The following steps show how to produce $1$-bordering, and need to be repeated $C({\bf m},t)$ times to get a matrix with $q=t$. 
  
%   {\bf Step I:} Choose $\rho_1<\ldots<\rho_s$, $s\leq t$, so that  $\{\rho_1,\ldots,\rho_s\} \subset \{\rho_1',\ldots,\rho_t'\}$, $\sigma(\rho_i, \rho_{i+1})\neq \emptyset$, and $s$ is maximal possible under those conditions.
  
%   (Note that, if $\sigma(\rho_i', \rho_{i+1}')\neq \emptyset$ for $i=1,\ldots,t-1$, then $\rho_i=\rho_i'$. The choice of $\rho_i$ may not be unique. We have $\cup_{i=1}^{t-1}\sigma(\rho_i',\rho_{i+1}')=\cup_{i=1}^{s-1}\sigma(\rho_i,\rho_{i+1})$.)
  
%   {\bf Step II:} For $i=1,\ldots,t-1$:
%   \begin{itemize}
%       \item If $m(\rho_i,\rho_{i+1})=r$, then put $\rho_i$ and $\rho_{i+1}$ in $\mathcal N$. 
%       \item If $m(\rho_i,\rho_{i+1})=r-1$, and $\{\rho_i,\rho_{i+1}\}\cap \mathcal N=\emptyset$, then put $\rho_i$ or $\rho_{i+1}$ in $\mathcal N$. 
%   \end{itemize}
  
  
%  {\bf Step III:} For $i=1\ldots,t$:
%  \begin{itemize}
%      \item If $\{\rho_i, \rho_{i+1}\} \subseteq \mathcal N$, then choose $\mathcal N_i\subset (\rho_i,\rho_{i+1})$ to be a strictly interlacing set of $\sigma(\rho_i,\rho_{i+1})$ with $|\mathcal N_i|=|\sigma(\rho_i,\rho_{i+1})|-1$, and put $\mathcal N_i$ in $\mathcal N$ and $\sigma(\rho_i,\rho_{i+1})$ in $\mathcal R_0$. 
%      \item If $\{\rho_i, \rho_{i+1}\}\cap \mathcal N=\rho_i$, then choose $\mathcal N_i\subset (\rho_i,\rho_{i+1})$, $|\mathcal N_i|=|\sigma(\rho_i,\rho_{i+1})|$ so that $\{\rho_i\}\cup \mathcal N_i$ is a strictly interlacing set of $\sigma(\rho_i,\rho_{i+1})$, and put $\mathcal N_i$ in $\mathcal N$ and $\sigma(\rho_i,\rho_{i+1})$ in $\mathcal R_0$.
%       \item If $\{\rho_i, \rho_{i+1}\}\cap \mathcal N=\rho_{i+1}$, then choose $\mathcal N_i\subset (\rho_i,\rho_{i+1})$, $|\mathcal N_i|=|\sigma(\rho_i,\rho_{i+1})|$ so that $\{\rho_{i+1}\}\cup \mathcal N_i$ is a strictly interlacing set of $\sigma(\rho_i,\rho_{i+1})$, and put $\mathcal N_i$ in $\mathcal N$ and $\sigma(\rho_i,\rho_{i+1})$ in $\mathcal R_0$.
%       \item If $\{\rho_i, \rho_{i+1}\}\cap \mathcal N=\emptyset$, then choose $\mathcal N_i\subset (\rho_i,\rho_{i+1})$, $|\mathcal N_i|=|\sigma(\rho_i,\rho_{i+1})|+1$ so that $\mathcal N_i$ is a strictly interlacing set of $\sigma(\rho_i,\rho_{i+1})$, and put $\mathcal N_i$ in $\mathcal N$ and $\sigma(\rho_i,\rho_{i+1})$ in $\mathcal R_0$.
%  \end{itemize}
 
%   Once the choice of $\rho_i$ is made, it suffices to construct a $1$-bordering $M_1$ of $A$ so that $\sigma(M_1)\subseteq [\rho_1,\rho_k]$ and
%   \[ m_{M_1}(\rho_i,\rho_{i+1})\le \max\{r-1,0\},\]
%   since we can then continue inductively to find $A=M_0,M_1,\dots,M_r=:M$, where $M_{l+1}$ is a $1$-bordering of $M_l$, so that $m_{M_r}(\rho_i,\rho_{i+1})=0$ and every eigenvalue of $M_r$ is in $[\rho_1,\rho_k]$, hence $M_r$ has only the $\tau$ distinct eigenvalues $\{\rho_1,\ldots,\rho_{\tau}\}$.
  
%   To show that such a matrix $M_1$ exists, first enumerate the open intervals $L_i:=(\lambda_{p_i},\lambda_{p_{i+1}})$ which contain at least one eigenvalue of $A$ as $L_{i_1},\dots,L_{i_s}$, where $1\le i_1<\dots<i_s<\tau$, and choose $\mu_j\in \sigma(A)\cap L_{i_j}$ for $1\le j\le s$. Let $\mathcal{R}_0=\{\mu_1,\dots,\mu_s\}$, and choose any set $\mathcal{N}\subseteq \{\lambda_{p_{1}},\dots,\lambda_{p_{\tau}}\}$ of size $s+1$ which strictly interlaces $\mathcal{R}_0$. The matrix constructed in Corollary~\ref{lem:1-border} then has the desired properties.

% \he{end of attempt}


%Before that, we propose a procedure on how one can use  Corollary~\ref{lem:1-border} to find an $r$-bordering matrix $M$ of $A$ with $q(M) \leq t$. We should note that Algorithm \ref{alg:pseudo} is not exhaustive.



Given an $n \times n$ symmetric matrix $A$ with $\sigma(A)=\{\lambda_1^{(m_1)},\ldots,\lambda_k^{(m_k)}\}$, $\sum_{i=1}^{k} m_i=n$, the general procedure to find an $r$-bordering matrix $M$ of $A$ with $q(M) \leq t$ is shown in Algorithm~\ref{algo} below. Note that we may have some freedom in how we choose the sets ${\mathcal R}_0$ and ${\mathcal N}$ in each step. One possible choice is given in the proof of Proposition~\ref{prop:C-bordering}, and we show all possible choices for a particular case in Example~\ref{ex:options}.


\begin{algorithm}[ht!]
	\DontPrintSemicolon
	\SetKwFunction{Fstep}{Find an $r$-bordering matrix $M$ of $A$ with $q(M) \leq t$ }
	\SetAlgoLined	
\begin{enumerate}
    \item  Choose an integer $t$, $2\leq t\leq q(A)$. Define $M_{0}:=A$, $r:=C({\bf m},t)$. 
    \item For  $\ell=1,\ldots,r$, use  Corollary~\ref{lem:1-border} to construct an $(n+\ell)\times (n+\ell)$ matrix $M_{\ell}$  such that $$C({\bf m}(M_{\ell}),t)=C({\bf m}(M_{\ell-1}),t)-1.$$ Note that we may have some freedom in how we choose the sets ${\mathcal R}_0$ and ${\mathcal N}$ in each step.
    \item  The resulting $(n+r) \times (n+r)$ matrix $M:=M_{r}$ has $q(M) \leq t$.
\end{enumerate}	 
	\caption{\FuncSty{Find an $r$-bordering matrix $M$ of $A$ with $q(M) \leq t$ }}
	\label{algo}
\end{algorithm}

 


\begin{comment}
\begin{enumerate}
    \item  Choose an integer $t$, $2\leq t\leq q(A)$. Define $M^{(0)}:=A$, $r:=C({\bf m},t)$. 
    \item For  $\ell=1,\ldots,r$ use  Corollary~\ref{lem:1-border} to construct an $(n+\ell)\times (n+\ell)$ matrix $M^{(\ell)}$  such that $$C({\bf m}(M^{(\ell)}),t)=C({\bf m}(M^{(\ell-1)}),t)-1.$$ Note that we may have some freedom in how we choose the sets ${\mathcal R}_0$ and ${\mathcal N}$ in each step.
    \item  The resulting $(n+r) \times (n+r)$ matrix $M:=M^{(r)}$ has $q(M) \leq t$.
\end{enumerate}
\end{comment}

%\he{I am no longer sure we should keep this algorithm?} \po{agreed}
% \begin{algorithm}
%   \begin{enumerate}[itemsep=1ex,topsep=1ex]\renewcommand{\labelenumi}{Step~\arabic{enumi}.}
%   \item[{\bf input}:] $n \times n$ matrix $A$ with $\sigma(A)=\{\lambda_1^{(m_1)},\ldots,\lambda_k^{(m_k)}\}$, $\sum_{i\in [k]} m_i=n$, and an integer $t$, $2\leq t\leq q(A)$.
%   \item $M^{(0)}:=A$, $r:=C({\bf m},t)$. 
% %  \item  Choose $p_1=1<p_2<\dots<p_{\tau}=k$, such that
% %   $m_A(\lambda_{p_i},\lambda_{p_{i+1}})=\sum_{j:p_i<j<p_{i+1}}m_j\le r$, $1\le i<\tau$.
% \item For  $\ell=1,\ldots,r$ construct $M^{(\ell)}$ (using Corollary~\ref{lem:1-border}) such that $C({\bf m}(M^{(\ell)}),t)=C({\bf m}(M^{(\ell-1)}),t)-1$.
% %  \begin{enumerate}
% %     \item ${\mathcal R}_0^{(\ell-1)}$ is the set of distinct $\lambda_j$ that are in $(\lambda_{p_i},\lambda_{p_{i+1}})$ for some $i\in [\tau-1]$, and $m_{M^{(\ell-1)}}(\lambda_j)>0$. \po{NO! Fix later to cover all cases.}
% %     \item Choose any ${\mathcal N}^{(\ell-1)}\subseteq \{\lambda_{p_1},\ldots,\lambda_{p_{\tau}}\}$ with $|{\mathcal R}_0^{(\ell-1)}+1$| elements.
% %     \item Let $M^{(\ell)}$ be the 1-bordered matrix of $M^{(\ell-1)}$ obtained by Corollary~\ref{lem:1-border} with the sets ${\mathcal R}_0^{(\ell-1)}$ and ${\mathcal N}^{(\ell-1)}$.
% %  \end{enumerate}
%  \item[{\bf output:}]  $(n+r) \times (n+r)$ matrix $M:=M^{(r)}$ with $q(M) \leq t$.
% \end{enumerate}
% \caption{%Pseudo-algorithm to find
%   Finding an $r$-bordering matrix $M$ of $A$ with $q(M) \leq t$}\label{alg:pseudo}
% \end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Joins with complete graphs}\label{sec:joinswithcompletegraphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



In this section we consider the join of two graphs and develop a technique for determining, under certain conditions, the minimum number of distinct eigenvalues for the join of a graph with a complete graph.

\subsection{Patterns and eigenvectors}
If we want a $1$-bordering of the matrix $A \in S(G)$ to produce a matrix $A' \in S(K_1 \vee G)$, then we need $U_0{\bf b}$ to have no zero entries in Theorem \ref{thm:1-bordering} above. This will happen for most choices of ${\bf b}$, unless $U_0$ contains a zero row, or equivalently, unless eigenvectors corresponding to the eigenvalues in $\mathcal R_0$ all have a zero entry in the same position. The next results consider the case $|\mathcal R_0|=1$. We call an eigenvalue of a symmetric matrix \emph{extreme} if it is the smallest or the largest eigenvalue of that matrix.


\begin{corollary}\label{cor:join-with-K1}
Suppose $G$ is a non-empty graph and there exists an $A\in S(G)$ with a nowhere zero eigenvector associated with some eigenvalue $\lambda$ of $A$. Then there exists a $1$-bordering $A'$ of $A$ in $S(K_1 \vee G)$ so that:
\begin{itemize}
\item $q(A')=q(A)+1$ if $\lambda$ is an extreme eigenvalue, 
\item $q(A')=q(A)$ if $\lambda$ is not an extreme eigenvalue,  
\item $q(A')=q(A)-1$ if $\lambda$ is simple and not an extreme eigenvalue. 
\end{itemize}
\end{corollary}

\begin{proof}
In Theorem \ref{thm:1-bordering} we choose $\mathcal R_0=\{\lambda\}$, $U_0\in\bR^{n\times 1}=\bR^n$ a nowhere zero eigenvector of $A$ with eigenvalue $\lambda$, and $B$ with eigenvalues $\mu_1$, $\mu_2$, satisfying $\mu_1< \lambda<\mu_2$, so that either $\mu_1$ or $\mu_2$ agrees with an eigenvalue of $A$, if $\lambda$ is an extreme eigenvalue, and so that both $\mu_1$ and $\mu_2$ are eigenvalues of $A$, if $\lambda$ is not an extreme eigenvalue of $A$. Since $U_0$ is a single column with no zero entries we get $A' \in S(K_1 \vee G)$, and since the spectrum of $A'$ can be obtained for the spectrum of $A$ by removing one multiple of $\lambda$ and increasing the multiplicity of $\mu_1$ and $\mu_2$ by~$1$, the result follows. 
\end{proof}




In Theorem \ref{thm:1-bordering} we have seen that after $1$-bordering, some eigenvectors will necessarily have a zero entry, and this has an interesting consequence for the patterns of $2$-borderings. 


\begin{corollary}\label{cor:up and down}
Let $A$ be a symmetric matrix, $A'$ a $1$-bordering of $A$, and $A''$ a $1$-bordering of~$A'$.
%Furthermore, let $H$ be the graph so that $A'' \in S(H)$, and let $\{u,v\} = V(H)\setminus V(G)$.
% Let $G$ be a graph, $A \in S(G)$, $A'$ a $1$-bordering of $A$, and $A''=(A')'$ a $1$-bordering of $A'$. Furthermore, let $H$ be the graph so that $A'' \in S(H)$, and let $\{u,v\} = V(H)\setminus V(G)$.
%, $\{u,v\}  E(H)\setminus E(G)$. 
%
If $(A'')_{1,2}\ne 0$, 
% $A'' \in S(K_2 \vee G)$ or equivalently if $H= K_2 \vee G$, 
then there is an eigenvalue $\lambda$ of $A'$ so that %$m_{A'}(\lambda)=m_A(\lambda)+1$ and 
$m_{A''}(\lambda)=m_{A'}(\lambda)-1=m_A(\lambda)$.
\end{corollary}



\begin{proof}
Adopting the notation and definitions from Theorem \ref{thm:1-bordering}, recall that $W=(W_{\mathcal N}\; W_1)$ where
%$$(1 \oplus U_0)V=\left(
% \begin{array}{c}
%  {\bf v}^T \\
%  U_0V_0 \\
% \end{array}
% \right)$$ are the eigenvectors of $A'$ corresponding to the eigenvalues from $\mathcal N$, and the columns of the matrix 
$$%(1 \oplus U_0)V=
W_{\mathcal N}=\left(
\begin{array}{c}
 {\bf v}^T \\
 U_0V_0 \\
\end{array}
\right)\quad\text{and}\quad  W_1=\left(
\begin{array}{c}
 0 \\
 U_1 \\
\end{array}
\right),$$
and $W^TA'W=D_{\mathcal N}\oplus D_1$. Hence, $A'W=W(D_{\mathcal N}\oplus D_1)$, i.e., $(A'W_{\mathcal N}\;A'W_1)=(W_{\mathcal N}D_{\mathcal N}\;W_1D_1)$, so the columns of the matrices $W_{\mathcal{N}}$ and $W_1$
are eigenvectors of $A'$ corresponding to the eigenvalues of $D_\mathcal{N}$ and $D_1$, respectively. If $\lambda$ is an eigenvalue of $A'$ which is not in $\mathcal{N}$, then the $\lambda$-eigenspace of $A'$ is contained in the column space of $W_1$, so every vector in this eigenspace has first entry equal to zero.
It follows that any
eigenvector of $A'$ with nonzero first entry must have its corresponding eigenvalue $\lambda$ in $\mathcal{N}$.
% eigenvalue $\lambda$ of $A'$ is not in  $\mathcal N$, then all the eigenvectors corresponding to $\lambda$ contain a zero in the first position. 

Consider now the $1$-bordering $A''$ of $A'$. Let us define ${\mathcal R}'_0$, $D_0'$, $U'_0$ and ${\bf b}'$ for this $1$-bordering, analogously as was done above for the $1$-bordering $A'$ of $A$. If $(A'')_{1,2}\ne 0$, then $(U'_0 {\bf b}')_1 \ne 0$ by the above, so the first row of $U'_0$ cannot be a zero row. Since ${U'_0}^T A' U'_0=D_0'$, this implies that there is some eigenvector of $A'$, with eigenvalue $\lambda \in {\mathcal R}'_0$, which has a nonzero first entry. Hence, by the previous paragraph, $\lambda \in \mathcal N \cap \mathcal R_0'$, and thus  %$m_{A'}(\lambda)=m_A(\lambda)+1$ and
$m_{A''}(\lambda)=m_{A'}(\lambda)-1=m_A(\lambda)$. 
\end{proof}


\begin{remark}\label{rk:up-and-down}
Suppose $r\ge2$ and $A_0,A_1,\dots,A_r$ are successive $1$-borderings of a matrix $A_0\in S(G)$. If $A_r\in S(K_r\vee G)$, then by Corollary~\ref{cor:up and down}, it is necessarily the case that for $0\le s\le r-2$, there is a real number $\lambda_s$ so that $m_{A_{s+2}}(\lambda_s)=m_{A_{s+1}}(\lambda_s)-1=m_{A_s}(\lambda_s)$.
\end{remark}%\todo{Remark how cor 4.2 works for r-bordering..}


In the following example we illustrate how Algorithm~\ref{algo} may be used to border a matrix achieving a small $q$ value in $3$-bordering in different ways. We also identify cases when Remark~\ref{rk:up-and-down} implies that the resulting $3$-bordering cannot be in $S(K_3\vee G)$. % by Corollary~\ref{cor:up and down}. 


\begin{example}\label{ex:options}
 Let $A$ be a $9 \times 9$ symmetric matrix with ordered multiplicity list ${\bf m}=(1,3,3,1,1)$ and spectrum $\{1,2^{(3)},3^{(3)},4,5\}$. The goal is to find the spectra of all $3$-borderings of $A$ that have three distinct eigenvalues. Observe that $C({\bf m},3)=3$, and to achieve this goal the value of $C$ must decrease by one every time we border. Table~\ref{tab:13311} shows all possible eigenvalues we can obtain in 
 this way. We produced this table by exhaustive search.  
 
 \begin{table}[htb]
     \centering
     \begin{tabular}{|c|c|c|c|}
          \hline
          $A$& \multicolumn{3}{c|}{$\{1,{\color{red} 2^{(3)}},3^{(3)},4,5\}$} \\        
          \hline
          $1$-bordering& $\{1^{(2)},{\color{red} 2^{(2)}},3^{(4)}, 5,{\color{blue} \mu}\}$&$\{1^{(2)},{\color{red} 2^{(2)}},3^{(3)},{\color{blue}\lambda},\mu,\nu\}$&$\{1^{(2)},{\color{red} 2^{(2)}},3^{(3)},{\color{blue}\lambda},5^{(2)}\}$\\
          \hline
          $2$-bordering& $\{1^{(3)},{\color{red} 2},3^{(5)},{\color{SpringGreen4}\mu'},\mu''\}$&$\{1^{(3)},{\color{red} 2},3^{(4)},{\color{SpringGreen4} \rho},\mu^{(2)}\}$&$\{1^{(3)},{\color{red} 2},3^{(3)},{\color{SpringGreen4}\lambda'},5^{(3)}\}$\\
          \hline
          $3$-bordering& $\{1^{(4)},3^{(6)},\mu''^{(2)}\}$&$\{1^{(4)},3^{(5)},\mu^{(3)}\}$&$\{1^{(4)},3^{(4)},5^{(4)}\}$\\
          \hline
     \end{tabular}
     \caption{Red eigenvalues are the ones that are forced to have reduced  multiplicity in the next bordering, the blue ones satisfy the conclusion of Corollary~\ref{cor:up and down} for the $2$-bordering of $A$, and the green ones satisfy the same condition when we consider instead the $3$-bordering of $A$. Moreover, $\lambda,\lambda'\in [3,4]$, $\nu\in[4,5]$, $\mu, \mu',\mu''\geq 5$ and $\rho\in (3,\mu)$, are arbitrary.}
     \label{tab:13311}
 \end{table}
 We note that the construction in the proof of Proposition~\ref{prop:C-bordering} produces only the spectrum  $\{1^{(4)},3^{(6)},5^{(2)}\}$, which we obtain after $1$-bordering with spectrum $\{1^{(2)},2^{(2)},3^{(4)},4,5\}$ and  $2$-bordering with spectrum $\{1^{(3)},2,3^{(5)},4,5\}$. This example shows that there may be several options of choosing appropriate ${\mathcal N}$ and ${\mathcal R}_0$ sets in each step as we develop an $r$-bordering with the desired number of distinct eigenvalues.  
 % and all the spectra can be obtained by a suitable choice in Step 2 of Algorithm~\ref{alg:pseudo}. 


 In all three situations (corresponding to three columns of Table \ref{tab:13311}), if $A\in S(G)$, by appropriately choosing the free parameters, it is possible to satisfy the necessary conditions of Remark~\ref{rk:up-and-down} for the $3$-bordering of $A$ to be in $S(K_3 \vee G)$. However, if, for example, we choose $\lambda=4$ or $\lambda'=\lambda$ in the last column, then the conditions of the remark do not hold. 
\end{example}

% \begin{remark}
% Below we give a couple of remarks on conditions needed for an $r$-bordering of $A$ constructed by repeated application of Corollary~\ref{lem:1-border} to have the graph $K_r \vee G$, where $G$ is the graph of $A$. Here we adopt the notation and definitions from Corollary~\ref{lem:1-border}, Proposition~\ref{prop:C-bordering} and their proofs. 
% \begin{enumerate}
% \item  From the proof of Corollary~\ref{lem:1-border} it is clear that a $1$-bordering of $A$ will be in $S(K_1 \vee G)$ iff $U_0$ can be chosen so that $U_0b$ has no entries equal to zero. When we consider $q(K_1 \vee G)$, we are not restricted to work with a specific matrix $A$. Hence, it might be possible to satisfy the condition required for $H=K_1 \vee G$ by making an appropriate choice of $A$. 
% \item  As we want to repeat the construction from Corollary~\ref{lem:1-border} several times, it is useful to know something about the eigenvectors of a $1$-bordering $A'$ of $A$, as they will determine whether or not the graph of $A''=(A')'$ is $K_1\vee(K_1\vee G)=K_2 \vee G$.
% Let $U=\left(
% \begin{array}{cc}
%  U_0 & U_1 \\
% \end{array}
% \right)$ be an orthogonal matrix that diagonalises $A$, $U^T AU=D_0 \oplus D_1$, and let $V=\left(
% \begin{array}{c}
%  v_1^T \\
%  V_1
% \end{array}
% \right)$ be an orthogonal matrix that diagonalises $S$, $V^T S V=D_{\mathcal{N}}$. Then
% $$W=\left(
% \begin{array}{cc}
%  v_1^T & 0 \\
%  U_0V_1 & U_1 \\
% \end{array}
% \right)$$
% is an orthogonal matrix that diagonalizes $A'$: $W^TA'W=D_{\mathcal{N}} \oplus D_1$. Hence the columns of the matrix $(1 \oplus U_0)V$ are the eigenvectors of $A'$ corresponding to the eigenvalues from $\mathcal N$, and the columns of the matrix $\left(
% \begin{array}{c}
%  0 \\
%  U_0 \\
% \end{array}
% \right)$
% are the eigenvectors of $A'$ corresponding to the eigenvalues in $D_1$. If $1 \leq m_{A'}(\lambda)\leq m_{A}(\lambda)$, then all the eigenvectors of $A'$ corresponding to $\lambda$ have the first entry equal to zero. 
% %\item Example \ref{ex:options} offers several ways of reaching $q=3$ with a $3$-bordering. However, only the last column of Table 1 offers an option that has a chance of producing a matrix in $K_3 \vee G$. Even there we need to be careful to choose $\lambda \neq 4$ and $\lambda' \neq \lambda$. 
% \end{enumerate}
% \end{remark}

% \he{I have written up the conclusion of item 2 in the remark the lemma, and item 3 in Example below. Do we want to keep it? The lemma connects with Chapter 4, in particular Corollary 4.4 is immediate consequence of this lemma.}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hypercubes}\label{hyper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\kevin{This sentence is not needed here any more.[} For a graph $G$ and $v \in V(G)$, recall that the graph
%$\jdup(G,v)$ is obtained from $G$ by duplicating $v$ with  a new vertex $w$ and adding edges between $w$ and the neighbours of $v$ and adding the edge $\{v,w\}$. \kevin{]}

In this section we explore the minimum number
of distinct eigenvalues for joins of complete graphs with a hypercube graph. 
Recall that for $t\ge 1$, the vertices of the hypercube graph $Q_t$ are the $2^t$ binary strings of length $t$, and its edges are the pairs of vertices with Hamming distance one. 
%We call an eigenvalue of a symmetric matrix non-extreme if it is not the smallest nor the largest eigenvalue of that matrix.
%
% \begin{lemma}\label{lemma:join-with-K1}
%    Suppose $G$ is a graph. Let $A\in S(G)$ and assume that $q(A)=q(G)$, and that $A$ possesses a totally
%    nonzero eigenvector associated with an eigenvalue $\lambda$. Then $q(K_1\vee G) \leq q(G) +1$. Strict inequality can be obtained if (1) $\lambda$ has multiplicity 1 or (2) $\frac{\mu\lambda-\lambda^2-1}{\mu-\lambda} \in \sigma(A)$ for some $\mu \in \sigma(A)$. Further, if both (1) and (2) hold, then $q(K_1\vee G)\leq q(G)-1.$ 
% \end{lemma}
% \todo{is (1) enough to get $q(G)-1$?}
% \aida{Seems so, by Lemma \ref{lem:1-border}?}
%
% \begin{lemma}\label{lemma:join-with-K1}
%     Suppose $G$ is a nonempty graph and there exists an $A\in S(G)$ with $q(A)=q(G)$ having a nowhere zero eigenvector associated with eigenvalue $\lambda$.  Then $q(K_1\vee G)\leq q(G)+1$.  This inequality is strict if $\frac{\mu\lambda-\lambda^2-1}{\mu-\lambda} \in \sigma(A)$ for some $\mu \in \sigma(A)$.  Furthermore, if $\lambda$ is simple and non-extreme then $q(K_1\vee G)\leq q(G)-1.$
% \end{lemma}
%   
% \begin{proof}
% Let $\{\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_n\}$ be an orthonormal set of eigenvectors of $A$
% corresponding to eigenvalues
% $\{ \lambda_1,\ldots, \lambda_n \}$. 
% Suppose that
% $\mathbf{v}_1$  
% has no zero entries and
% \[ B=\left( \begin{array}{cc}
% t&\mathbf{v}_1^T\\ %\hline
%  \mathbf{v}_1 & A
%  \end{array} \right). \]
% Then $B\in S( K_1\vee G).$
% Let $\mu\neq\lambda_1$ be an eigenvalue of $A$, $w=\mu-\lambda_1$, and $t=\frac{\mu^2-\mu\lambda_1-1}{\mu-\lambda_1}$. 
% Then 
% \[ V=\left\{ 
% \begin{pmatrix} 0\\ \mathbf{v}_2 \end{pmatrix}, 
% \begin{pmatrix} 0\\\mathbf{v}_3 \end{pmatrix},\ldots, 
% \begin{pmatrix} 0\\\mathbf{v}_n \end{pmatrix},
% \begin{pmatrix} w \\ \mathbf{v}_1 \end{pmatrix},
% \begin{pmatrix} -1/w \\ \mathbf{v}_1 \end{pmatrix} \right\}
% \] 
% is an orthonormal set of eigenvectors for
% $B$. Note that the first $n-1$ vectors
% in $V$ are eigenvectors corresponding to
% the eigenvalues $\lambda_2,\ldots,\lambda_n$,
% respectively. 
% Further, the last two eigenvectors
% correspond to the eigenvalues $\mu$ 
% and 
% $\frac{\lambda_1(\mu-\lambda_1)-1}{\mu-\lambda_1}$ respectively. 
%
% %\kevin{I think this argument assumes that $\lambda$ is not an extreme eigenvalue.} 
% Noting that if $\lambda$ is simple and non-extreme, then we can apply Lemma \ref{lem:1-border}, taking $\mathcal{R}_0=\{\lambda\}$, we can see that the matrix constructed in the proof of Lemma \ref{lem:1-border} will have the nonzero eigenvector in the first row and column, and thus will belong to $S(K_1\vee G)$.  Therefore $q(K_1\vee G)\leq q(G)-1$. 
% \end{proof}
%
It was shown in 
\cite[Corollary~6.9]{MR3118943}
that if $Q_t$ is the hypercube graph with $t\geq 2$, then $q(Q_t)=2$. In the following, we use the matrix construction from  
\cite{MR3118943}
to demonstrate that $Q_t$ has a realization $A$ having $q(A)=2$ 
and a nowhere zero eigenvector.

\begin{theorem}\label{lemma:hypercube}
  For any two positive integers $s$ and $t$,
  $$q(K_s\vee Q_{t}) \leq 3.$$
  Moreover, if $s\le t$, then
  \[ q(K_s\vee Q_{2t+2})=3.\]
 \end{theorem}
 \begin{proof}
 We will demonstrate that $Q_t$ has
 a realization $A$ having $q(A)=2$ and
 a nowhere zero eigenvector.
 Corollary~\ref{cor:join-with-K1} will then imply that $q(K_1 \vee Q_t)\leq 3$ and so the result follows from the inequality~(\ref{eq:jdup}).
 As observed in \cite{MR3118943}, for any nonzero $\alpha$ and $\beta$ with
 $\alpha^2 +\beta^2=1$, $Q_t$ has a realization 
 \[ B=\left( \begin{array}{cc}
\alpha A & \beta I\\ %\hline
 \beta I & -\alpha A
 \end{array} \right) \]
 such that $A^2=I$ and $q(B)=2$. The vector 
\[\begin{pmatrix} 
(I+\alpha A) \mathbf{1} \\ \beta \mathbf{1}
\end{pmatrix}\] 
with $\mathbf{1}$ representing the
 all ones vector, will be a nowhere zero eigenvector of $B$ with eigenvalue 1 for any $\alpha$ sufficiently small.
%   \end{proof}

% % \begin{remark}
% % For a hypercube $Q_n$ it was proved that $q(Q_n)=2$~\cite[Corollary~6.9]{MR3118943}. By~\cite[Proposition~5.1]{MR3904092}
% % $$3 \leq q(K_2\vee Q_5)=q(\jdup(K_1 \vee Q_5,v)) \leq q(K_1 \vee Q_5)$$
% % where $v \in V(K_1)$. Which implies that
% %  $$q(Q_5)=2 \text{ and } q(K_1 \vee Q_5)\geq 3$$
% % hence join with $K_1$ can raise $q$.
% % \end{remark}
 
% %  We can make this even more general:

%  \begin{theorem}\label{thm:KQ}
%   For any two positive integers $s\leq t$, we have
%   $$q(K_s \vee Q_{2t+2}) = 3.$$
%  \end{theorem}
%  \begin{proof}
   The second part of the statement is a generalization of~\cite[Proposition~5.1]{MR3904092}. It uses~\cite[Theorem~1.9]{MR3904092}, which is a small correction of~\cite[Theorem~4.4]{MR3118943}.
      For $i=1,2,\ldots,t+1$, consider the vertices of the hypercube $Q_t$ given by the binary strings $v_i=00\cdots01100\cdots0$, with the two ones in  positions $2i-1$ and $2i$. Then $\{v_1,\ldots,v_{t+1}\}$ is a set of $t+1$ independent vertices in $K_s\vee  Q_{2t+2}$, and $N(v_i)\cap N(v_j)=V(K_s)$ for $i\ne j$. Therefore
   $$\left\vert \bigcup_{i \ne j} N(v_i)\cap N(v_j)\right\vert =s < t+1,$$
   hence $q(K_s\vee Q_{2t+2}) \geq 3$ by~\cite[Theorem~4.4]{MR3118943}.
 %  and we have the result by Theorem~\ref{lemma:hypercube}.
 \end{proof}
 
% \todo{Can we say anything about $s$ and $t$ when $q=2$? Perhaps have an open problem noting that $q(K_s\vee Q_m)=2$ for large $s$: what would be the smallest $s$ to maintain $q=2$?}

 By Theorem \ref{thm:general_upper_bound},  if $s$ is chosen sufficiently large, then $q(K_s\vee Q_t)=2$.  Thus, in light of Theorem~\ref{lemma:hypercube}, 
 and the fact that $q(K_s\vee Q_t)$ is a non-increasing function of $s$ as
 per Equation~(\ref{eq:jdup}), it is natural to ask the following question: What is the minimum $s$ for which $q(K_s\vee Q_t)=2$?
 
 
% \po{This is now the only question in the environment in the paper. Remove environment?}
% \begin{question}
     % What is the minimum $s$ for which $q(K_s\vee Q_t)=2$?
% \end{question}

% \noindent We also wonder if $q(K_s\vee G)$  bounds $q(H\vee G)$
%  for each graph $H$ with $|H|=s$.
%  \begin{question}
%  Find $$\max_{s\leq t}q(K_s \vee Q_{2t+2}).$$
%  (For a start: is $q(K_1\vee Q_4)=3$? What are the values of $q(K_1\vee Q_5)$ and $q(K_2\vee Q_5)$?
 
%  Moreover, what is the value of
%  $$q(K_s \vee Q_{2s+1})?$$
%  \end{question}


%  \begin{question}  Is  $q(K_s\vee G )\leq q(H\vee G)$ for each graph $H$ with $|H|=s$?
%  \end{question}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cycles}\label{sec:applicationjoinscycles}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Let $A$ be a symmetric matrix and $\alpha,\beta \in \mathbb{R}$, $\alpha<\beta$. By $m_A(\alpha,\beta)$ ($m_A[\alpha,\beta]$) we denote the sum of multiplicities of all eigenvalues $\lambda$ of $A$ that are contained in the interval $(\alpha,\beta)$ ($[\alpha,\beta]$). Furthermore, $m_A(\alpha)$ denotes the multiplicity of $\alpha$ as an eigenvalue of $A$, where $m_A(\alpha)=0$, if $\alpha$ is not an eigenvalue for $A$. 

% \begin{lemma}
%   Let $A$ be an $n \times n$ symmetric matrix, $A_1$ its principal submatrix of size $(n-1) \times (n-1)$, and $\alpha,\beta \in \mathbb{R}$, $\alpha<\beta$. Then $|m_A(\alpha,\beta)-  m_{A_1}(\alpha,\beta)|\le1$. If  $m_A(\alpha,\beta)=m_{A_1}(\alpha,\beta)-1$ and $m_A[\alpha,\beta]\geq m_{A_1}[\alpha,\beta]$, then $m_A(\alpha)=m_{A_1}(\alpha)+1$ and $m_A(\beta)=m_{A_1}(\beta)+1$. 
% \end{lemma}
% %\ru{Example to check for second part: $\sigma(A)=\{0,2,5,7\}$, $\sigma(A_1)=\{1,4,6\}$, $\alpha=3$, $\beta=8$.}
% \begin{proof}
%   Interlacing.
% \end{proof}

% \begin{lemma}\label{lem:vK_1}
% Let $\mathcal R$ be the set of distinct eigenvalues of $A \in S(G)$. Let $\mathcal{R}_0 \subseteq \mathcal R$, and $\mathcal{N}$ a set of (necessarily distinct) real numbers that strictly interlace numbers in $\mathcal{R}_0$ with  $|\mathcal{N}|=|\mathcal{R}_0|+1$.

% Then there exists $H$, $E(K_1 \cup G)\subset E(H)\subseteq E(K_1 \vee G)$, and $A' \in S(H)$ with a principal submatrix $A$, satisfying:
% \begin{align*}
% m_{A'}(\lambda)&=m_{A}(\lambda)-1 \text{ if }\lambda \in \mathcal{R}_0 \\
% m_{A'}(\lambda)&=m_{A}(\lambda)+1 \text{ if }\lambda \in \mathcal{N} \\
% m_{A'}(\lambda)&=m_{A}(\lambda) \text{ if }\lambda \not\in \mathcal{N}\cup \mathcal{R}_0.
% \end{align*}
% \end{lemma}

% \begin{proof}
% Let $D_0$ be a diagonal matrix with distinct diagonal elements equal to elements in $\mathcal{R}_0$, and $U_0 \in \mathbb{R}^{|G|\times |\mathcal{R}_0|}$ so that
%  $$U_0^TAU_0=D_0 \text{ and } U_0^TU_0=I_{|\mathcal{R}_0|}.$$
% There exist unique  $a \in \mathbb{R}$ and $b \in \mathbb{R}^{|\mathcal{R}_0|}$ so that the matrix $$S=\left(
% \begin{array}{cc}
%  a & b^T \\
%  b & D_0 \\
% \end{array}
% \right)$$
% has the set of eigenvalues equal to $\mathcal{N}$. Clearly, for this choice of $a$ and $b$, the matrix 
% $$A'=\left(
% \begin{array}{cc}
%  a & b^TU_0^T \\
%  U_0b & A \\
% \end{array}
% \right)$$
% satisfies the requirements of the lemma.
% \end{proof}

Given $A\in S(H)$ and a graph $G$, let $S(G\vee A)$ be the set of all matrices $X\in S(G\vee H)$ so that $X[H]=A$, and let $q(G\vee A)$ be the minimum $q(X)$ over all such matrices~$X$. Note that $q(G\vee A)\geq q(G\vee H)$. Suppose $A$ has ordered multiplicity list ${\bf m}={\bf m}(A)$. Given a number $t\ge2$ and a graph $G$, we want to determine whether or not $q(G\vee A)\le t$. By Proposition~\ref{prop:C-bordering}, a necessary condition is that \[C({\bf m},t)\le |G|.\]
In Section \ref{sec:limitations} we will show that this condition is not sufficient in general, since it may happen that none of the $|G|$-borderings guaranteed by Proposition~\ref{prop:C-bordering} has the correct graph, $G\vee H$, where for any $n\times n$ symmetric matrix $A=(a_{ij})$, $H=G(A)$ is defined as the graph on $n$ vertices with edges $\{i,j\}$ whenever $i \neq j$ and $a_{ij}\neq 0$.
In fact, it is not generally sufficient even in the case that $G$ is a complete graph. Despite this, we provide examples when the procedure from Section \ref{sec:border} is applied successfully.



% Repeating the construction above several times, we would like reduce the number of distinct eigenvalues of the resulting matrix to $q$, $2\leq q < k$. To do that, we choose $\mathcal{K}=\{\mu_1,\ldots,\mu_q\} \subset \{\lambda_1,\lambda_2,\ldots,\lambda_k\}$ so that $\mu_1 <\cdots <\mu_q$,  $\lambda_1=\mu_1$ and  $\lambda_k=\mu_q$. For $i=1,\ldots,q-1$, let $\mathcal S_i \subset \{\lambda_1,\ldots,\lambda_k\}$ be the set containing all the eigenvalues $\lambda_j$, satisfying $\mu_i < \lambda_j <\mu_{i+1}$. ($\mathcal{S}_i$ can be empty.) Let $m=\max\{|\mathcal{S}_i|; i=1,\ldots,q\}$. Our construction will produce a matrix with distinct eigenvalues equal to $\mathcal K$ after $m$ steps. To reduce the number of steps as much as possible, we choose $\mathcal{K}$ so the $m$ is minimal possible. 
 
%  \ru{Note that this is not the only way, in general. For example, suppose $A$ has ordered multiplicity list $(1,2,3,1,1)$ and distinct eigenvalues $\lambda_1<\dots<\lambda_5$. We can find a $2$-bordering $M$ with $q=3$ and distinct eigenvalues $\lambda_1,\lambda_3,\mu$, where $\mu>\lambda_5$: take a $1$-bordering with multiplicities $(2,1,4,1,0,1)$ (the final position is the multiplicity of $\mu$) and then a $2$-bordering with multiplicities $(3,0,5,0,0,2)$.}
 


Note that the necessary condition above may be written as 
\begin{equation}\label{eq:q(HvA)>=C} q(G\vee A)\ge \min\{t\ge2 : C({\bf m}(A),t)\le |G|\}.\end{equation}
%where $C(A,t):=C({\bf m}(A),t)$.

%  \begin{lemma}
%   Repeating the step in Lemma~\ref{lem:vK_1} several times and achieving a specific spectrum.
%  \end{lemma}
%  \begin{proof}
%  \end{proof}
 
 
%\todo{Make the join with a cycle of even length (see below) an example, argument from scribing (Sept 2) that the bound is achieved needs to be added (upper bound missing)} 

Turning to cycles, it is known by~\cite[Theorem~3.4]{levene2021paths} that $q(K_{2k-2}\vee C_{2k})=2$. Next, we use the following result on the inverse eigenvalue problem for cycles to determine the minimum number of eigenvalues allowed for joins of complete graphs with even cycles.  

%So the question  
% Moreover, by eigenvalue interlacing we have $M(K_t\vee C_{2k}) \leq t+2$. 
 
\begin{proposition}\label{IEPG cycles}(IEPG for cycles~\cite{MR2549049}).
Nonincreasing real numbers $\lambda_1\geq \cdots \geq \lambda_n$ are the eigenvalues of $A\in S(C_n)$ if and only if either
$$\lambda_1\geq \lambda_2>\lambda_3\geq \lambda_4 > \lambda_5\geq\cdots$$
or
$$\lambda_1> \lambda_2\geq \lambda_3> \lambda_4 \geq \lambda_5>\cdots.$$
Hence, if $k\geq 2$, $q(C_{2k})=k$ and $M(C_{2k})=2$.
\end{proposition}

Observe that if $\lambda$ is a multiple eigenvalue of $A \in S(C_n)$, then the multiplicity of $\lambda$ is two and there exists a nowhere zero eigenvector for $\lambda$ associated with $A$. If the latter did not hold then every eigenvector ${\bf x}$ for $\lambda$ would satisfy ${\bf x}_i=0$ for some $i=1,2,\ldots, n$. In this case $\lambda$ is a multiple eigenvalue for the principal submatrix of $A$ obtained by deleting row and column $i$. However, this submatrix lies in $S(P_{n-1})$, and can only possess simple eigenvalues.

\begin{theorem} If $k\geq 2$ then 
$q(K_1\vee C_{2k})=k.$
\end{theorem}
\begin{proof}
To obtain the upper bound $q(K_1\vee C_{2k}) \leq k$, use Proposition~\ref{IEPG cycles} to choose a matrix $A\in S(C_{2k})$ with multiplicity list $(2,2,\dots,2)$, choose a non-extreme eigenvalue of $A$ and a nowhere zero eigenvector and apply Corollary~\ref{cor:join-with-K1}.

To show the lower bound, assume that $M \in S(K_1\vee C_{2k})$ has eigenvalues $\mu_1 \leq \mu_2 \leq \cdots \leq \mu_{2k+1}$ and that $A$ is the submatrix corresponding to $C_{2k}$ and has eigenvalues $\lambda_1 \leq \cdots \leq \lambda_{2k}$. By Proposition \ref{IEPG cycles}, we have that the maximum multiplicity of an eigenvalue $\lambda_i$ is $2$ and furthermore, if there are eigenvalues $\lambda_i$ and $\lambda_j$ with multiplicity $2$ then $m_A(\lambda_i, \lambda_j)$ must be even. By eigenvalue interlacing we have that the maximum multiplicity of any eigenvalue of $M$ is $3$. We claim that if $\mu_i$ and $\mu_j$ each have multiplicity $3$, then there must be an eigenvalue of multiplicity $1$ between them, and the lower bound follows once we show this.

By way of contradiction, assume that there is some pair of eigenvalues with multiplicity $3$ and $j$ distinct eigenvalues between them, each with multiplicity $2$ (with the possibility that $j$ is $0$). That is, we have 
$$\mu_i = \mu_{i+1} = \mu_{i+2} < \cdots < \mu_{i+2+2j+1} = \mu_{i+2+2j+2} = \mu_{i+2+2j+3}.$$
From eigenvalue interlacing we must have $\lambda_i = \lambda_{i+1}$ and $\lambda_{i+2j+3} = \lambda_{i+2j+4}$. Hence it follows that both $\lambda_{i+1}$ and $\lambda_{i+2j+3}$ have multiplicity $2$ and $m_A(\lambda_{i+1}, \lambda_{i+2j+3}) = 2j+1$ is odd, a contradiction.
\end{proof}
% To verify the upper bound, use Proposition \ref{IEPG cycles} to deduce the existence of a matrix $B \in S(C_{2k})$ with designated eigenvalues $0,0,t,t,-t,-t,\lambda_1,\lambda_1,\ldots$. Thus $B$ has exactly $k$ distinct eigenvalues. 
% We may assume an order on the eigenvalues by separating the two zero eigenvalues.\ru{?} As such set the $2 \times 2$ matrix $D_1=0$ and $D_2=\mathrm{diag}(t,t,-t,-t,\lambda_1,\ldots)$. Using Corollary~\ref{lem:1-border}, we can construct a $3 \times 3$ matrix
% \[\begin{pmatrix}0&a_1&a_2\\a_1&0&0\\a_2&0&0\end{pmatrix}
% \] ($a_1$ and $a_2$ are both nonzero) with eigenvalues $0,t,-t$, and using the fact that $B$ has nowhere zero eigenvectors associated with each multiple eigenvalue, we 
%  deduce the existence of a matrix $X$ given by %\todo{change notation to bold for vectors}
% \[
% X=\begin{pmatrix}
% 0 & \mathbf{a}^TU^T\\ U\mathbf{a}& B
% \end{pmatrix}
% \]
% with $\mathbf{a}=(a_1, a_2, 0,\ldots ,0)^T$.
% Note that $X \in S(K_1 \vee C_{2k})$ with exactly $k$ distinct eigenvalues. Hence $q(K_1\vee C_{2k}) \leq k.$
% \end{proof}

 As we saw in Example~\ref{ex:options}, we have to be careful about the choice of $1$-bordering of $A$ in order to ensure that a subsequent $2$-bordering of $A$ has the desired pattern. As another illustration of this issue, observe that if an eigenvalue $\lambda$ of $A \in S(C_6)$ has multiplicity $2$ and multiplicity $1$ for a $1$-bordering $A'$ of $A$, then eigenvectors of $\lambda$ for $A'$ will not be nowhere zero---since the interlacing is not strict, an entry of the eigenvector for $A'$ is 0 (see \cite[Theorem 4.3.17]{HJ1}). %This shows that we cannot start with a matrix $A \in S(C_6)$ with multiplicity list $(2,2,2)$  and produce a matrix $A''$ in $S(K_2 \vee C_6)$ with $q(A'')=2$ by $2$-bordering of $A$. In the next example we show that starting with a matrix $A \in S(C_6)$ with a different multiplicity list, $q(A'')=2$ can still be reached for $A''\in S(K_2 \vee C_6)$.
 This shows that if we apply Algorithm \ref{algo} starting with $A \in S(C_6)$ with multiplicity list $(2,2,2)$ to produce a $2$-bordering $A''$ with $q(A'')=2$, then $A''\not\in S(K_2 \vee C_6)$. In the next example we show that starting with a matrix $A \in S(C_6)$ with a different multiplicity list, $q(A'')=2$ can still be reached for some $A''\in S(K_2 \vee C_6)$.

 \begin{example} Let
   \[A=\left(
\begin{array}{rrrrrr}
 1 & 1 & 0 & 0 & 0 & -1 \\
 1 & -1 & 1 & 0 & 0 & 0 \\
 0 & 1 & 1 & 1 & 0 & 0 \\
 0 & 0 & 1 & -1 & 1 & 0 \\
 0 & 0 & 0 & 1 & 1 & 1 \\
 -1 & 0 & 0 & 0 & 1 & -1 \\
\end{array}
\right)\in S(C_6),\quad U_0=\frac1{\sqrt 3}\left(
\begin{array}{rr}
 1 & 0 \\
 0 & 1 \\
 -1 & 0 \\
 0 & -1 \\
 1 & 0 \\
 0 & 1 \\
\end{array}
\right).\]
Then $\sigma(A)=\{(-2)^{(2)},-1,1,2^{(2)}\}$. Let $\mathcal R_0=\{-1,1\}$, and observe that $U_0^TAU_0=\left(
\begin{smallmatrix}
  1&0\\0&-1
\end{smallmatrix}\right)$, corresponding to the setup of Theorem~\ref{thm:1-bordering}.
%Let $\mathcal R_0=\{-1,1\}$, 
Choose any $t\in (-1,1)$ and let
$\mathcal N=\{-2,t,2\}$. Following Corollary~\ref{lem:1-border} and~\cite[equation (2.4)]{boley-golub}, we find
\[
B= \left(
\begin{array}{rrr}
 t & \sqrt{3}u & \sqrt{3}v \\
 \sqrt{3}u &1 & 0 \\
 \sqrt{3}v & 0 & -1 \\
\end{array}
\right)\]where
\[ u=\sqrt{(1-t)/2}\quad\text{and}\quad v=\sqrt{(1+t)/2},\]
and
\[ A'=\left(
\begin{array}{rrrrrrr}
 t & u & v & -u & -v & u & v \\
 u & 1 & 1 & 0 & 0 & 0 & -1 \\
 v & 1 & -1 & 1 & 0 & 0 & 0 \\
 -u & 0 & 1 & 1 & 1 & 0 & 0 \\
 -v & 0 & 0 & 1 & -1 & 1 & 0 \\
 u & 0 & 0 & 0 & 1 & 1 & 1 \\
 v & -1 & 0 & 0 & 0 & 1 & -1 \\
\end{array}
\right)\in S(K_1\vee C_6)\]
with $\sigma(A')=\{(-2)^{(3)},t,2^{(2)}\}$.

Repeating the construction, choosing $\mathcal R_0'=\{t\}$ and $\mathcal N'=\{-2,2\}$, we obtain
\[ A''=\left(
\begin{array}{rrrrrrrr}
  -t & \sqrt{1-t^2} & -v & u & v & -u & -v & u \\[3pt]
 \sqrt{1-t^2} & t & u & v & -u & -v & u & v \\
 -v & u & 1 & 1 & 0 & 0 & 0 & -1 \\
 u & v & 1 & -1 & 1 & 0 & 0 & 0 \\
 v & -u & 0 & 1 & 1 & 1 & 0 & 0 \\
 -u & -v & 0 & 0 & 1 & -1 & 1 & 0 \\
 -v & u & 0 & 0 & 0 & 1 & 1 & 1 \\
 u & v & -1 & 0 & 0 & 0 & 1 & -1 \\
\end{array}
\right)\in S(K_2\vee C_6)\]
with $\sigma(A'')=\{(-2)^{(4)},2^{(4)}\}$ and thus $q(K_2\vee C_6)=2$.
 \end{example}
 
 \begin{example}
 Using the Jacobi-Ferguson algorithm \cite{fixme}, we can construct numerical matrices $A\in S(C_{10})$ with spectrum $\{(-6)^{(2)},-4,-2,0^{(2)},4,2,6^{(2)}\}$, and hence find numerical matrices $A''\in S(K_2\vee C_{10})$ with spectrum $\{(-6)^{(4)},0^{(4)},6^{(4)}\}$ and thus $q(K_2\vee C_{10})=3$. One such numerical matrix is:

\begin{comment}
{\tiny{
 \[A''=\left(
\begin{smallmatrix}
 0 & -1.97203 & -0.113217 & -0.403995 & -2.45212 & -1.38194 & 0.00618847 & 0.000284374 & -0.00364895 & 2.12649 & -2.96463 & 1.30437 \\
 -1.97203 & 0 & -2.21953 & -2.04951 & 2.27528 & -0.837721 & 0.00803906 & 0.0055749 & -0.0185115 & -1.97313 & -1.79714 & 1.69443 \\
 -0.113217 & -2.21953 & 0 & 3.24689 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 3.69012 \\
 -0.403995 & -2.04951 & 3.24689 & 0 & 3.61751 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 -2.45212 & 2.27528 & 0 & 3.61751 & 0 & 1.53993 & 0 & 0 & 0 & 0 & 0 & 0 \\
 -1.38194 & -0.837721 & 0 & 0 & 1.53993 & 0 & 0.0103069 & 0 & 0 & 0 & 0 & 0 \\
 0.00618847 & 0.00803906 & 0 & 0 & 0 & 0.0103069 & 0 & 5.48911 & 0 & 0 & 0 & 0 \\
 0.000284374 & 0.0055749 & 0 & 0 & 0 & 0 & 5.48911 & 0 & 2.42272 & 0 & 0 & 0 \\
 -0.00364895 & -0.0185115 & 0 & 0 & 0 & 0 & 0 & 2.42272 & 0 & 0.0134096 & 0 & 0 \\
 2.12649 & -1.97313 & 0 & 0 & 0 & 0 & 0 & 0 & 0.0134096 & 0 & 2.99998 & 0 \\
 -2.96463 & -1.79714 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2.99998 & 0 & -2.71717 \\
 1.30437 & 1.69443 & 3.69012 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -2.71717 & 0 \\
\end{smallmatrix}
\right).\]
}}
\end{comment}
{\tiny{
 \[A''=\left(
\begin{smallmatrix}
 0 & -1.9720 & -0.11321 & -0.40399 & -2.4521 & -1.3819 & 0.0061884 & 0.00028437 & -0.0036489 & 2.1264 & -2.9646 & 1.3043 \\
 -1.9720 & 0 & -2.2195 & -2.0495 & 2.2752 & -0.83772 & 0.0080390 & 0.005574 & -0.018511 & -1.9731 & -1.7971 & 1.6944 \\
 -0.11321 & -2.2195 & 0 & 3.2468 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 3.6901 \\
 -0.40399 & -2.0495 & 3.2468 & 0 & 3.6175 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 -2.4521 & 2.2752 & 0 & 3.6175 & 0 & 1.5399 & 0 & 0 & 0 & 0 & 0 & 0 \\
 -1.3819 & -0.83772 & 0 & 0 & 1.5399 & 0 & 0.010306 & 0 & 0 & 0 & 0 & 0 \\
 0.0061884 & 0.0080390 & 0 & 0 & 0 & 0.010306 & 0 & 5.4891 & 0 & 0 & 0 & 0 \\
 0.00028437& 0.005574 & 0 & 0 & 0 & 0 & 5.4891 & 0 & 2.4227 & 0 & 0 & 0 \\
 -0.0036489 & -0.018511 & 0 & 0 & 0 & 0 & 0 & 2.4227 & 0 & 0.013409 & 0 & 0 \\
 2.1264 & -1.9731 & 0 & 0 & 0 & 0 & 0 & 0 & 0.013409 & 0 & 2.9999 & 0 \\
 -2.9646 & -1.7971 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2.9999 & 0 & -2.7171 \\
 1.3043 & 1.6944 & 3.6901 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -2.7171 & 0 \\
\end{smallmatrix}
\right).\]
}}
  \end{example}
%  \todo{Remark on $q=3$ in the example above.}
 
%\todo{put cycles in here somewhere}
 
 

 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Limitations of this procedure for graph joins}\label{sec:limitations} 
\section{Limitations of Algorithm~\ref{algo} for graph joins}\label{sec:limitations}
%\markK{With the reintroduction of Algorithm 1, should the title of Section 5 be changed back to reference Algorithm 1?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\ru{trying this out as a section rather than a subsection}

In this section we show that the condition $C({\bf m}(A),t)\le r$ from Proposition \ref{prop:C-bordering} is not generally sufficient in the case $G=K_r$ for the existence of a matrix with at most $t$ eigenvalues in $S(G\vee A)$.
%or even in $S(G\vee H)$, where $H=G(A)$. 
%\ru{``or even...'' part about $q(G\vee H$) seems out of order now; I suppose it was referring to the hypercubes example? remove it?, agreed - SF and done}

 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%\subsection{Failure in the pattern of \texorpdfstring{$H$}{H}}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \begin{lemma}\label{lemma:bordered-diagonal-mtx-entries}
%   Let $m_1,\ldots,m_k\ge1$ and
%   $$A=\begin{pmatrix}
%   \alpha & {\bf b}^\top \\ {\bf b}&B\\
%   \end{pmatrix}$$
%   where $B$ is a $n\times n$ matrix with distinct eigenvalues $\{\mu_1^{(m_1)},\ldots,\mu_{k}^{(m_k)}\}$, and $A$ is an $(n+1)\times (n+1)$ matrix. Let $\Delta=\sigma(A)\setminus \{\mu_1^{(m_1-1)},\ldots,\mu_k^{(m_k-1)}\}$. 
%   Then $\alpha$ and $\|{\bf b}\|$ are determined by the multiset $\Delta$ and $\mu_1,\ldots,\mu_k$.
% \end{lemma} 
% \begin{proof}
%   By considering traces, we have
%   \[\alpha=\sum_{\lambda\in \sigma(A)} \lambda - \sum_{i=1}^k m_i\mu_i = \sum_{\lambda\in \Delta} \lambda - \sum_{i=1}^k \mu_i.\]
%   The matrix $A$ is orthogonally similar to a matrix of the form $D_0\oplus \begin{pmatrix} \alpha &{\bf c}^\top\\{\bf c} &D_1\end{pmatrix}$ where $D_1=\diag(\mu_1,\ldots,\mu_k)$ and $D_0\oplus D_1$ is similar to $B$, and $\|{\bf b}\|=\|{\bf c}\|$. Hence, $D_0$ has spectrum $\{\mu_1^{(m_1-1)},\ldots,\mu_k^{(m_k-1)}\}$, and $A':=\begin{pmatrix} \alpha &{\bf c}^\top\\{\bf c} &D_1\end{pmatrix}$ has spectrum $\Delta=\sigma(A)\setminus \sigma(D_1)$. The minimum polynomial of $A'$ is
%   \[ p(x)=\prod_{\lambda\in \Delta}(x-\lambda)=(x-\alpha)\prod_{i\in [k]} (x-\mu_i)-\sum_{i=1}^k c_i^2\prod_{j\in [k]\setminus\{i\}}(x-\mu_j).\]
%   Hence,
%   \[ p(\mu_i)=\prod_{\lambda\in \Delta}(\mu_i-\lambda)=-c_i^2\prod_{j\in [k]\setminus \{i\}}(\mu_i-\mu_j).\]
%   Since $\mu_1,\ldots,\mu_k$ are distinct, the latter product is nonzero so we can rearrange for $c_i^2$ and sum over $i$ to obtain an expression for $\|{\bf b}\|^2=\|{\bf c}\|^2$ depending only on $\Delta$ and $\mu_1,\ldots,\mu_k$.
% \end{proof}

% {\color{gray}\ru{this gray text will probably disappear}
% \begin{question} 
% Can Lemma \ref{lemma:doublyborderedmtx} be generalized to the following statement?
%  If a symmetric $n \times n$ matrix $A$  has ordered multiplicity list $(m_1,k,m_2,k,\ldots,k,m_t)$, $m_i\geq k$, $n=(t-1)k+\sum_{i\in [t]} m_i$, then the $k$-bordered matrix $$M=\begin{pmatrix}
%   A&B\\
%   B^\top & C
%  \end{pmatrix},$$
%  where $B\in \mathbb{R}^{n \times k}$ and $C \in S(K_k)$, has at least $t+1$ distinct eigenvalues.
% \end{question}
% Answer: yes, see Corollary below.
% }

% \begin{lemma}\label{lem:bordering}
% Let $A$ be a square matrix with  multiplicity list $(m_1,k,m_2,\ldots,k,m_t)$, where $k\ge2$,
% and suppose
% \[  M=\begin{pmatrix}
%     \alpha_1&\beta&{\bf b}_1^\top\\
%     \beta&\alpha_2&{\bf b}_2^\top\\
%     {\bf b}_1&{\bf b}_2&A
%   \end{pmatrix}\] is a $2$-bordering of $A$ so that
%   $M$ has multiplicity list $(m_1+2,k-2,m_2+2,\ldots,k-2,m_t+2)$, and 
% both $M(1)$ and $M(2)$ both have multiplicity list $(m_1+1,k-1,m_2+1,\ldots,k-1,m_t+1)$, where 
% all the above multiplicity lists are with respect to the same list of distinct eigenvalues $\lambda_1,\mu_1,\lambda_2,\ldots,\mu_{t-1},\lambda_{t}$. Then $\beta=0$ and $\alpha_1=\alpha_2$.
% \end{lemma}
% \begin{proof}
%   Let us write $\Delta(S,T):=\sigma(S)\setminus \sigma'(T)$, where $\sigma'(T)$ is the multiset $\sigma(T)$ with every multiplicity decreased by~$1$. 
%   Since $\Delta(M(1),M(1,2))=\Delta(M(2),M(1,2))$,  Lemma~\ref{lemma:bordered-diagonal-mtx-entries} implies that $\alpha_1=\alpha_2$.
%     Since we also have
%   $\Delta(M,M(1))=\Delta(M(2),M(1,2))$, Lemma~\ref{lemma:bordered-diagonal-mtx-entries} implies that $\left\|\begin{pmatrix}\beta\\{\bf b}_1\end{pmatrix}\right\|=\|{\bf b}_1\|$, so $\beta=0$.
% \end{proof}


\begin{proposition}\label{prop:monotone}
 Suppose $t\ge2$ and $A_1,\ldots,A_k$ are successive $1$-borderings of a symmetric matrix $A=A_0$, and ${\bf m}(A)=(m_1,k,m_2,k,\ldots,k,m_t)$ where $m_j\ge k\ge t$ for each $j$, and $q(A_k)=t$. Then \[{\bf m}(A_j)=(m_1+j,k-j,m_2+j,k-j,\ldots,k-j,m_t+j),\quad j=0,1,\ldots,k.\]
% Hence, we have  $A_k=\begin{pmatrix}\alpha I_k &B^\top\\B&A_0\end{pmatrix}$, for some $\alpha\in \bR$ and some matrix $B$.
\end{proposition}
\begin{proof}
  Let $\mu_1<\cdots<\mu_{2t-1}$ be the distinct eigenvalues of $A_0$ and $\lambda_1<\cdots<\lambda_t$ the distinct eigenvalues of $A_k$. By eigenvalue interlacing, every eigenvalue of $A_j$ is in the closed interval $[\lambda_1,\lambda_t]$. Moreover, by Lemma~\ref{lem:bordering-interlacing}, for $i=1,\ldots,t-1$ and $j=0,\ldots,k$, we have \[m_{A_j}(\lambda_i,\lambda_{i+1})\ge m_{A_0}(\lambda_i,\lambda_{i+1})-j.\] In particular, $0=m_{A_k}(\lambda_i,\lambda_{i+1})\ge m_{A_0}(\lambda_i,\lambda_{i+1})-k$, so \[m_{A_0}(\lambda_i,\lambda_{i+1})\le k.\] Let $S=\{\mu_1,\ldots,\mu_{2t-1}\}\setminus\{\lambda_1,\ldots,\lambda_t\}$. Then $|S|\ge 2t-1-t=t-1$, and each eigenvalue in $S$ has multiplicity at least $k$ in $A_0$ by hypothesis, so
  \[
  k(t-1)\le k|S| \le \sum_{i=1}^{t-1} m_{A_0}(\lambda_i,\lambda_{i+1})\le k(t-1).
  \]
  Hence, $|S|=t-1$, so $\{\lambda_1,\ldots,\lambda_t\}\subseteq\{\mu_1,\ldots,\mu_{2t-1}\}$. Since $\mu_1,\mu_{2t-1}\in [\lambda_1,\lambda_t]$, this forces $\mu_1=\lambda_1$ and $\mu_{2t-1}=\lambda_t$. If $\lambda_i=\mu_j$ and $\lambda_{i+1}=\mu_l$ where $l>j+2$, then $m_{A_0}(\lambda_i,\lambda_{i+1})\ge 2k$, a contradiction. It follows that $\lambda_i=\mu_{2i-1}$ for $1\le i\le t$.
  
  Hence, $m_{A_0}(\lambda_i,\lambda_{i+1})=k$ for each $i$, and the bound we observed above becomes 
  \[ k-j\le m_{A_j}(\lambda_i,\lambda_{i+1}).\]
  Since $A_k$ is a $(k-j)$-bordering of $A_j$, by Lemma~\ref{lem:bordering-interlacing} we also have
  \[ m_{A_j}(\lambda_i,\lambda_{i+1})\le m_{A_k}(\lambda_i,\lambda_{i+1})+k-j=k-j,\]
  so $m_{A_j}(\lambda_i,\lambda_{i+1})=k-j$. Moreover, by eigenvalue interlacing, $k-j\le m_{A_j}(\mu_{2i})\le m_{A_j}(\lambda_i,\lambda_{i+1})=k-j$, so we have equality. Hence, the multiplicity of $\mu_{2i}$ as an eigenvalue of $A_j$ is $k-j$, and no other real number in $(\lambda_i,\lambda_{i+1})$ is an eigenvalue of $A_j$. It follows that every eigenvalue of $A_j$ other than $\mu_2,\ldots,\mu_{2(t-1)}$ is in the set $\{\lambda_1,\ldots,\lambda_t\}$. Observe that $A_j$ is an $(j+N)\times (j+N)$ matrix, where $N=(t-1)k+\sum_{i=1}^t m_i$ is the number of rows and columns of $A$. Hence,
  \begin{align*}
     \sum_{i=1}^tm_{A_j}(\lambda_i) = j+N-\sum_{i=1}^{t-1}m_{A_j}(\mu_{2i})=j-(t-1)(k-j)+(t-1)k+\sum_{i=1}^tm_i=\sum_{i=1}^t(m_i+j).
  \end{align*}
  Since the total multiplicity of the eigenvalues $\lambda_1,\dots,\lambda_t$ in $A_j$ is $\sum_{i=1}^t(m_i+j)$, and by eigenvalue interlacing, the multiplicity in $A_j$ of $\lambda_i=\mu_{2i-1}$ is bounded above by $m_i+j$, this must be precisely its multiplicity.
 % 
%  Finally, let $n$ be the number of rows and columns of $A_k$, and note that $A_k=\begin{pmatrix}X &B^\top\\B&A_0\end{pmatrix}$, for some matrices $X$ and $B$. What we have proven so far implies that every submatrix $M$ of $A_k$ of the form $M=A_k[ \{i,j\}\cup \{k+1,k+2,\ldots,n\}]$, where $1\le i<j\le k$, satisfies Lemma~\ref{lem:bordering}. Hence, $x_{ii}=x_{jj}$ and $x_{ij}=x_{ji}=0$, which implies that $X=\alpha I_k$ for some $\alpha\in \bR$.
\end{proof}

\begin{corollary}\label{coro:limitationsnotsufficientcondition}
If $A$ is a symmetric matrix with %ordered multiplicity list 
${\bf m}(A)=(m_1,k,m_2,k,\ldots,k,m_t)$ where $m_i\ge k\ge t\ge 2$ for each $i$, then $C({\bf m}(A),t)=k$ yet $q(G\vee A)>t$ for all non-empty graphs $G$ with $|G|=k$. Hence, the inequality~\eqref{eq:q(HvA)>=C} is strict in this case.
\end{corollary}
\begin{proof}
    %By Lemma~\ref{lem:bordering-interlacing}, 
    We have $C({\bf m}(A),t)=k$, so $q(B)\ge t$ for all $k$-borderings $B$ of $A$ by Proposition~\ref{prop:C-bordering}.
    Consider a sequence of successive $1$-borderings taking us from $A$ to some $k$-bordering $B$ with $q(B)=t$. By Proposition~\ref{prop:monotone}, the successive eigenvalue multiplicities of any given $\lambda\in \bR$ in this sequence of matrices is monotone. Hence, by Corollary~\ref{cor:up and down}, the superdiagonal of the leading principal $k\times k$ submatrix of $B$ is zero.
    
    Now let $P$ be a $k\times k$ permutation matrix, and consider $B_P=(P\oplus I_r)B(P^T\oplus I_r)$, where $k+r=|G|$. By the previous paragraph, the superdiagonal of the leading principal $k\times k$ submatrix of $B_P$ is zero, for every such permutation matrix~$P$. 
    Hence, 
    %We can repeat this argument after first permuting the rows and columns of this principal submatrix to see that 
    every off-diagonal entry of $B$ is zero, so $B$ has an empty graph. 
\end{proof}

% \begin{corollary}\label{coro:limitationsnotsufficientcondition}
% If $A$ is a bordered matrix so that for $i=0,1,2$, any $i$-sub-bordering(!) \aida{should we define this?} has multiplicity list $(m_1+i,k-i,m_2+i,\dots,k-i,m_t+i)$, then the bordering matrix is a scalar multiple of the identity. In particular, this holds whenever $A$ is a $k$-bordering of a matrix $A_0$ with ordered multiplicity list ${\bf m}=(m_1,k,m_2,k,\dots,k,m_t)$ where $m_i\ge k$ for each $i$.

% Hence, whenever $A_0$ is a matrix with ordered multiplicity list ${\bf m}=(m_1,k,m_2,k,\dots,k,m_t)$ where $k\ge2$ and  $m_i\ge k$ for each $i$, we have $q(H\vee A_0)>t$ for all non-empty graphs $H$ with $|H|=k$.\aida{move the first part depending on $i$ to the proof and leave only the part for $A_0$ as corollary?}
% \end{corollary}
% \begin{proof}
%   Any $2\times 2$ submatrix of the bordering matrix is a scalar multiple of the identity matrix $I_2$, by Proposition~\ref{prop:bordering}, which proves the first assertion.
  
%   If $A$ is a $k$-bordering of $A_0$ of the given form, then $C(A_0,t)=k$ \aida{$C({\bf m}(A_0),t)=k)$, also later}. If $A_0,A_1,\dots,A_k=A$ is any sequence of borderings of $A_0$ so that $A_{j+1}$ is a $1$-bordering of $A_j$, then \aida{by Proposition \ref{prop:C-bordering}?} we have $|C(A_i,t)-C(A_j,t)|\le |i-j|$ for $0\le i,j\le k$, and $C(A_0,t)=k$ and $C(A_k,t)=0$, which implies that $C(A_i,t)=k-i$ (recall $m_i\ge k$). It follows \ru{is there a neat way to see this?} \aida{I only see that the following follows from Proposition \ref{prop:C-bordering} (2 to 1): $C({\bf m}(A_i),t)=k-i$, there exist $p_1=1<p_2<\dots<p_\tau=k$ where $\tau\le t$ so that 
%   \[ m_A(\lambda_{p_i},\lambda_{p_{i+1}})=\sum_{j:p_i<j<p_{i+1}}m_j = k-i,\quad 1\le i<\tau.\]} that ${\bf m}(A_i)$ must be $(m_1+i,k-i,\dots,k-i,m_t+i)$ for $0\le i\le k$.
% \end{proof}

This shows a limitation of Algorithm~\ref{algo}.
%the procedure in Section \ref{sec:border}. 
However, we show in the following proposition that this limitation is very specific, and that if the multiplicity list is perturbed only slightly we may have success using this procedure.%Algorithm~\ref{alg:pseudo}.
%\ru{As far as we know the limitation is in $S(H\vee A)$ rather than the bigger set $S(H\vee G(A))$, so I guess we should phrase the next example in these terms?}

\begin{proposition}
Suppose $t\ge2$ and $A$ is a symmetric matrix with eigenvalues 
\[\lambda_1^{(m_1)} < \beta < \gamma < \lambda_2^{(m_2)} < \mu_2^{(2)} < \lambda_3^{(m_3)}< \mu_3^{(2)} < \cdots < \mu_{t-1}^{(2)} < \lambda_t^{(m_t)}.
\]
If $A$ has an eigenbasis such that for each vertex $u$ there is at least one eigenvector corresponding to an eigenvalue in $\{\mu_i\}$ which is nonzero in the entry corresponding to $u$, then there exists a matrix $B\in S(K_2\vee A)$ 
%which is a $2$-bordering of $A$ and furthermore $B \in S(K_2 \vee G(A))$ 
such that $B$ has eigenvalues $\lambda_1^{(m_1+2)},\ldots , \lambda_t^{(m_t+2)}$. In particular, $q(K_2 \vee G(A)) \leq t$.
\end{proposition}

%\mike{Strengthen to: ``If $A$ has an eigenbasis such that the vectors corresponding to $\beta, \gamma, \{ \mu_i\}$ are nowhere zero". Or (is it true?) to ``If $A$ has an eigenbasis that has the property that for each vertex the vectors corresponding to $\beta, \gamma, \{ \mu_i\}$ have at least one nonzero entry"}

\begin{proof}
By~\cite{boley-golub} we know that there are $1$-borderings $C_\beta$ and $C_\gamma$ of the matrices $\mathrm{diag}(\beta, \mu_2,\ldots, \mu_t)$ and $\mathrm{diag}(\gamma, \mu_2,\ldots, \mu_t)$ respectively which each have eigenvalues $\{\lambda_1,\ldots, \lambda_t\}$. Furthermore, we know that these borderings can have no zeros in the first row or column, and by computing traces we see that the $(1,1)$ entries are $k-\beta$ and $k-\gamma$ respectively, where $k=\lambda_1+\dots+\lambda_t-(\mu_2+\dots+\mu_t)$. Let the first row of $C_\beta$ have entries $k-\beta, b_1, \ldots, b_{t-1}$ and the first row of $C_\gamma$ have entries $k-\gamma, c_1,\ldots, c_{t-1}$. Define $B_0=[v_\beta,v_\gamma]$ where $v_\beta=(k-\beta,0,b_1,0,b_2,0,\dots)^T$, and $v_\gamma=(0,k-\gamma,0,c_1,0, c_2,\dots)^T$. That is, we are making vectors with the first rows of the borderings in the even or odd positions. Now define matrices $D_1=\diag(k-\beta, k-\gamma)$, $D_2=\diag(\beta, \gamma, \mu_2^{(2)},\ldots, \mu_{t-1}^{(2)})$ and $D_0=\diag(\lambda_1^{(m_1)},\ldots, \lambda_t^{(m_t)})$, and finally define
 \[ M=\begin{pmatrix}
k-\beta&&b_1&&b_2&&\cdots&b_{t-1}&\\
&k-\gamma&&c_1&&c_2&\cdots&&c_{t-1}\\
b_1&&\beta\\
&c_1&&\gamma\\
b_2&&&&\mu_2\\
&c_2&&&&\mu_2\\
&&&&&&\ddots\\
b_{t-1}&&&&&&&\mu_{t-1}\\
&c_{t-1}&&&&&&&\mu_{t-1}
\end{pmatrix}\oplus D_0\]
where the blank entries denotes $0$s.
%where the "odd rows/columns" submatrix is the appropriate $1$-bordering of $\diag(\beta,\mu_2,\dots,\mu_{t-1})$ and the "even rows/columns" submatrix is the appropriate $1$-bordering of $\diag(\gamma,\mu_2,\dots,\mu_{t-1})$. There doesn't seem to be any need to show that this structure is forced, in which case we can just write down $M$ and not worry about the argument with traces above.
%
%
Since $M$ is permutationally similar to the block diagonal matrix with blocks $C_\beta$, $C_\gamma$, and $D_0$, the eigenvalues of $M$ 
%\ru{removed "with $B_0$ in place of $B$"}
are $\lambda_1^{(m_1+2)},\ldots, \lambda_t^{(m_t+2)}$.

By the assumption, we may choose $V$ to be a matrix which diagonalizes the matrix $A$ such that for any row $u$, there is a column $j$ corresponding to an eigenvector of some $\mu_\ell$ such that $V_{uj} \not=0$.  Without loss of generality assume that $$V^TAV = \mathrm{diag}(\beta, \gamma, \mu_2^{(2)},\ldots, \mu_{t-1}^{(2)}, \lambda_1^{(m_1)},\ldots, \lambda_t^{(m_t)})=D_2\oplus D_0.$$ Define $W' = I_2 \oplus W_2 \oplus \cdots \oplus W_{t-1}$ where the $W_i$ are any orthogonal $2\times 2$ matrices, and define $W = W' \oplus I_{m_1+\cdots + m_t}$. Then, as $W'$ commutes with $D_2$, we have that $$W^T V^T A V W = V^T A V=D_2\oplus D_0.$$

Let $V'$ be the first $2t-2$ columns of $V$, so that it has columns that are the eigenvectors corresponding to the eigenvalues of $D_2$. Notate these columns by $v_1, \ldots, v_{2t-2}$. Let $U$ be any orthogonal $2\times 2$ matrix. Then 
% \ru{I think $V$ and $W$ are in the wrong order below?}
% \begin{align*}
% (U \oplus WV) M (U^T \oplus V^TW^T)&=(U \oplus VW) \begin{pmatrix} D_1 & B_0 \\ B_0^T & D_2\oplus D_0 \end{pmatrix} (U^T \oplus V^TW^T)\\
% &= \begin{pmatrix} U D_1 U^T & U B_0 V^TW^T\\
% WVB_0^T U^T & A\end{pmatrix}.
% \end{align*}

% This matrix is in $S(K_2\vee G(A))$ if $UD_1 U^T$ has nonzero off-diagonal entries and 
% \[
% UB_0V^TW^T = U \begin{pmatrix} b_1 & & b_2 & & \cdots & b_{t-1} & \\
% & c_1 & & c_2 & \cdots & & c_{t-1}\end{pmatrix}(V')^T(W')^T
% \]
% has all nonzero entries. Since $\beta \not=\gamma$, $D_1$ is not a zero matrix and so it is easy to choose $U$ such that $UD_1U^T$ has all nonzero entries. 

\begin{align*}
(U \oplus VW) M (U^T \oplus W^TV^T)&=(U \oplus VW) \begin{pmatrix} D_1 & B_0 \\ B_0^T & D_2\oplus D_0 \end{pmatrix} (U^T \oplus W^TV^T)\\
&= \begin{pmatrix} U D_1 U^T & U B_0 W^TV^T\\
VWB_0^T U^T & A\end{pmatrix}.
\end{align*}
%
This matrix is in $S(K_2\vee G(A))$ if $UD_1 U^T$ has nonzero off-diagonal entries and the following matrix has no zero entry:
\[
UB_0W^TV^T = U B_0'(W')^T(V')^T, \quad\text{where}\quad B_0'=\begin{pmatrix} b_1 & & b_2 & & \cdots & b_{t-1} & \\
& c_1 & & c_2 & \cdots & & c_{t-1}\end{pmatrix}.
\]

Let $\theta_2,\ldots, \theta_{t-1}$ be uniformly and independently chosen angles and let $W_i$ be the $2\times 2$ rotation matrix by angle $\theta_i$. Then the $ij$'th entry of $B_0'(W')^T(V')^T$ is
\[
b_1v_1(j) + \sum_{k=2}^{t-1} b_k v_{2k-1}(j) \cos \theta_k - b_k v_{2k}(j) \sin \theta_k
\]
if $i=1$ and 
\[
c_1v_2(j) + \sum_{k=2}^{t-1} c_k v_{2k-1}(j) \sin \theta_k +c_k v_{2k}(j) \cos \theta_k
\]
if $i=2$. Since the $b_i$ and $c_i$ are nonzero, and by the choice of $V$ there is at least one $u$ with $3\leq u \leq 2t-2$ with $v_u(j) \not=0$, we have that the $ij$'th entry of $B_0'(W')^T(V')^T$ is nonzero with probability $1$. So we can choose $W'$ for which $B_0'(W')^T(V')^T$ has no zero entries. Moreover, since $\beta \not=\gamma$, $D_1$ is not a zero matrix. It is now easy to choose $U$ such that $UD_1U^T$ and $UB_0W^TV^T$ have all nonzero entries.
 %If $B_0'(W')^T(V')^T$ has a zero column for every $W'$, then some column of $(V')^T$, say $v=(v_1\ \dots\ v_{t-1})^T$, where $v_j\in \bR^2$, has $B_0'(I\oplus W_2^T\oplus\dots\oplus W_{t-1}^T)v=0$ for all $2\times 2$ orthogonal $W_2,\dots,W_t$. This implies that $v_2=v_3=\dots=v_{t-1}=0$, so $\diag(b_1,c_1)v_1=0$. Since $b_1,c_1\ne 0$, we have $v_1=0$, so $v=0$, a contradiction. 
 %Each $2\times 2$ block of $B_0'$ is invertible, so we must have $b_k=c_k=0$ for $1<k< t$, a contradiction. 
 %So we can choose $W'$ for which $B_0'(W')^T(V')^T$ has no zero column. Moreover, since $\beta \not=\gamma$, $D_1$ is not a zero matrix. It is now easy to choose $U$ such that $UD_1U^T$ and $UB_0W^TV^T$ have all nonzero entries.
%
%\mike{Argument about freedom of $B$ and pattern... I think that with the freedom to choose $U$ an entry equal to $0$ means that there is a vertex $v$ for which the entry for $v$ in all of the $2t-2$ eigenvectors is $0$?}\ru{Is this enough freedom? I don't see why $B_0(V')^T$ can't have a zero column, and then you are stuck. On the other hand there is a bit more freedom as you can multiply columns $3,4$ of $B_0$ by any $2\times 2$ orthogonal matrix, and the same for cols $5,6$, etc.; or to put it another way, you can change $(V')^T$ to $W(V')^T$ where $W$ is orthogonal of the form $I_2\oplus W_2\oplus W_3\oplus\dots\oplus W_{t-1}$ (with $W_i$ orthogonal), because $W$ commutes with $D_2$. Maybe this saves the day.}
\end{proof}

%\todo{End with a concluding sentence or two.}
%\shaun{In this paper we started a study on the behaviour of $q(\cdot)$ when a graph $G$ is replaced by $G \vee H$. Particular interest and results obtained were given for the case when $H$ is complete, and we have explored the potential impact  of $q(G\vee H)$ depending on eigenvector patterns and for fixed families of graphs $G$. } 
In this paper we continued the study of the behaviour of $q(G\vee H)$. For a general graph $H$, we obtained %Particular interest and results obtained were given 
results for the case when $G$ is either a path or a complete graph, and we explored the potential impact of eigenvector patterns on $q(G\vee H)$, for various families of graphs $H$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection*{Acknowledgements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This project started and was made possible by the online research community \textit{Inverse eigenvalue problems for graphs}, which is sponsored by the American Institute of Mathematics with support from the US National Science Foundation. The authors thank AIM and the research community organizers for their support.

We are grateful to the anonymous referee for careful reading and comments, which improved the presentation of the paper.


\bibliographystyle{plain}
\bibliography{references}

\end{document}

