\documentclass[twoside,11pt]{article}



% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
\usepackage[preprint]{jmlr2e}
%\usepackage{jmlr2e}
\usepackage{multirow}
\usepackage{caption}
\usepackage{amsmath}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e}
\usepackage{subfigure}
% Definitions of handy macros can go here
\newtheorem{assumption}[theorem]{Assumption}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{00}{2023}{1-\pageref{LastPage}}{03/23; Revised 00/00}{00/00}{00-0000}{Yiling Xie and Xiaoming Huo}

% Short headings should be running head and authors last names

\ShortHeadings{Adjusted Wasserstein Distributionally Robust Estimator}{Xie and Huo}
\firstpageno{1}

\begin{document}

\title{Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning}


       
\author{\name Yiling Xie \email yxie350@gatech.edu\\
\name Xiaoming Huo \email huo@gatech.edu \\
\addr School of Industrial and Systems Engineering\\
 Georgia Institute of Technology\\
Atlanta, Georgia, USA}
\editor{My editor}

\maketitle

\begin{abstract}%
We propose an {\it adjusted} Wasserstein distributionally robust estimator---based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning.
This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically  unbiased and has an asymptotically smaller mean squared error.
The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO.
Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given.
Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model.
Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
\end{abstract}

\begin{keywords}
  distributionally robust optimization; asymptotic normality; Wasserstein distance; unbiased estimator; generalized linear model
\end{keywords}

\section{Introduction}
% \textcolor{cyan}{To make the story simpler: The asymptotic distribution of the WDRO estimator is not optimal according to the asymptotic distribution. There is a map from $\beta_{\ast}$ to $\Tilde{\beta}$. If this mapping is invertible, we could derive an unbiased estimator. Under condition?}
 
  Wasserstein distributionally robust optimization (WDRO) has appeared as a promising tool to achieve ``robust" decision-making \citep{ mohajerin2018data,blanchet2019quantifying,gao2022distributionally}. 
WDRO has attracted intense research interest in the past few years. 
It is well-known that WDRO admits tractable reformulations \citep{mohajerin2018data} 
and has a powerful out-of-sample performance guarantee \citep{gao2022finite}.
People also have been actively exploring its applications in financial portfolio selection \citep{blanchet2022distributionally}, statistical learning \citep{chen2018robust,shafieezadeh2019regularization}, neural networks \citep{sinha2018certifying}, automatic control \citep{yang2020wasserstein}, transportation \citep{carlsson2018wasserstein}, and energy systems \citep{wang2018risk}, among others.


WDRO can be applied in statistical learning \citep{chen2018robust,kuhn2019wasserstein,nguyen2022distributionally}.
In general, the statistical learning model can be written as
\[\min_{\beta\in B} \mathbb{E}_{P_{\ast}} \left[L(f(\mathbf{X},\beta),Y)\right],\]
where $\mathbf{X}\in\Omega\subset\mathbb{R}^d$ denotes the feature variable, $Y$ denotes the label variable, $P_{\ast}$ is the true data-generating distribution of $(\mathbf{X},Y)$, $f(\cdot,\beta)$ is the hypothesis function parameterized by $\beta\in B\subset\mathbb{R}^d$, $B$ is a compact convex set, and $L(\cdot,\cdot)$ is the loss function.
Considering the true data-generating distribution $P_{\ast}$ is usually unknown,  the empirical risk minimization can be applied to estimate the ground-truth hypothesis function $f(\cdot,\beta_\ast)$ parameterized by $\beta_{\ast}\not=0$. 
However, the empirical risk minimization estimators are sensitive to perturbations and suffer from overfitting \citep{smith2006optimizer,shalev2014understanding}.
To obtain robust estimators with desirable generalization abilities, distributionally robust optimization is proposed, which minimizes the worst-case expected loss among an ambiguity set $\mathcal{U}$ of distributions to hedge against data perturbation. 
In this paper, we are interested in the Wasserstein ambiguity set, and then the problem is the so-called  Wasserstein distributionally robust optimization.
 The Wasserstein ambiguity is defined as the ball centered at the empirical distribution $\mathbb{P}_N$ and contains all distributions close to $\mathbb{P}_N$ in the sense of the Wasserstein distance. 
 We denote the WDRO estimators---the solutions to the WDRO problem---by $\beta_n^{DRO}$.
 More details will be stated in Section \ref{adro}.

The asymptotic distribution of the WDRO estimator $\beta_n^{DRO}$ can be obtained under certain regularity conditions.
In particular, the convergence in distribution implies that the WDRO estimator $\beta^{DRO}_n$ has an asymptotic bias, which may result in inaccurate estimation of the ground-truth parameter $\beta_{\ast}$.
To rectify this effect, we first reformulate the convergence, showing that the WDRO estimator $\beta_n^{DRO}$ is an asymptotically unbiased estimator of a new quantity $\widetilde{\beta}$, and there is a map from $\beta_{\ast}$ to $\widetilde{\beta}$. 
If the map is invertible, we could define a transformation accordingly to obtain an asymptotically unbiased estimator of the ground-truth parameter $\beta_{\ast}$.
Sufficient conditions to guarantee the invertibility of the map are further developed.
These conditions can be met by linear regression, Poisson regression, and logistic regression.
We also provide the details of computing the adjusted WDRO.
We denote the inverse of the map by $\mathcal{A}_n(\cdot)$ and, the \textit{adjusted} WDRO estimator (denoted by $\beta^{ADRO}_n$) is defined as $\mathcal{A}_n(\beta_n^{DRO})$.
Applying the delta method \citep{doob1935limiting}, the asymptotic distribution of the adjusted WDRO estimator $\beta_n^{ADRO}$ is shown to be normal with a zero mean.

The proposed estimator $\beta_n^{ADRO}$ is advantageous in several ways.
The proposed estimator $\beta_n^{ADRO}$ is asymptotically unbiased, and the asymptotic mean squared error of the proposed estimator $\beta^{ADRO}_n$ is smaller than the WDRO estimator $\beta^{DRO}_n$.
% The above two results indicate that $\beta^{ADRO}_n$ possesses a better statistical guarantee.
% Further, we carry out numerical experiments to show that $\beta^{ADRO}_n$ also performs better in practice.
In addition, the out-of-sample performance guarantee of the proposed estimator $\beta^{ADRO}_n$ can be derived, demonstrating that the proposed estimator $\beta^{ADRO}_n$ inherits the generalization capacity of the WDRO estimator. 
Our numerical experiments illustrate that the proposed estimator $\beta_n^{ADRO}$ has a superior performance even if the sample size is relatively small.
%Also, given $n$ i.i.d. samples, obtaining $\beta^{ADRO}_n$ is computationally tractable. 
%(The procedure to calculate $\beta^{ADRO}_n$ is described in Section \ref{adro}.)

Since the generalized linear model includes multiple widely-used regression models, such as linear regression and logistic regression, and is easy to interpret and implement, we will articulate how to apply the \textit{adjusting} strategy in the setting of the generalized linear model. 
The logistic regression, the Poisson regression, and the linear regression serve as examples to show how the requirements for applying the adjusted WDRO are satisfied by the generalized linear model.

%\subsection{Related work}
%\subsection{Organization of this paper}
% The remainder of this paper is organized as follows. 
% In Section \ref{section2}, we derive the asymptotic distribution of $\beta^{DRO}_n$ in the logistic regression.
% In Section \ref{adro}, we introduce the adjusted WDRO estimator formally.
% In Section \ref{property}, we discuss the properties of adjusted WDRO estimator.
% In Section \ref{section5}, we explain how to define the adjusted WDRO in more general setting.
% In Section \ref{section6}, we extend the adjustment technique from the logistic regression to generalized linear models.
% Numerical experiments are conducted in Section \ref{exp}.
\subsection{Related work}
We review the existing work related to the proposed adjusted WDRO estimator.
WDRO is broadly utilized to solve parameter estimation problems \citep{shafieezadeh2015distributionally, kuhn2019wasserstein,aolaritei2022performance,nguyen2022distributionally}. 
Multiple algorithms have been developed \citep{luo2019decomposition,li2019first,blanchet2022optimal} and can be applied to compute the parameter estimators in the WDRO framework.
While intense work focuses on adapting WDRO to different machine learning problems, deriving the tractable reformulations, and solving the WDRO problems efficiently, the statistical properties of WDRO estimators have been investigated in recent few years, e.g.,   \cite{blanchet2022confidence,blanchet2021statistical}, evaluating the behavior of WDRO through the lens of statistics.
In the aforementioned paper, the asymptotic distribution of the WDRO estimator is proven to be normal, and the confidence region is proposed based on the corresponding asymptotic results.
While they focus on unsupervised settings, we extend the results to supervised statistical learning, especially in the setting of the generalized linear model.
Notably, the asymptotic biasedness of the WDRO estimator has been mentioned in \cite{blanchet2022confidence}.
However, we are the first to propose a nonlinear transformation to overcome this shortcoming.
Furthermore, the proposed estimator is more accurate in the asymptotic sense because the estimator also has an asymptotically smaller mean squared error.
In addition, the generalization bounds, i.e., the upper confidence bounds on the out-of-sample error, have been established to guarantee the out-of-sample performance of the WDRO estimator \citep{mohajerin2018data,shafieezadeh2019regularization,gao2022finite,wu2022generalization}.
Since the proposed adjusted WDRO estimator is transformed from the classic WDRO estimator, we can also develop the generalization bounds for the associated adjusted WDRO estimator. 
\subsection{Organization of this paper}
The remainder of this paper is organized as follows. 
In Section \ref{adro}, we introduce the adjusted WDRO estimator in statistical learning.
In Section \ref{property}, we discuss the properties of the adjusted WDRO estimator.
In Section \ref{glmdro}, we use the logistic regression, the Poisson regression, and the linear regression to explain how to derive the adjusted WDRO in the corresponding generalized linear model.
Numerical experiments are conducted and analyzed in Section \ref{exp}.
The proofs are relegated to Appendix \ref{appendix} whenever possible.
\section{Adjusted WDRO estimator}\label{adro}
This section discusses how to derive the adjusted WDRO estimator.
We introduce the problem formulation, give the asymptotic distribution of the WDRO estimator, define the adjusted WDRO estimator, and provide some sufficient conditions for the existence of the adjusted estimator. Finally, the computation framework for the adjusted WDRO is provided.
\subsection{Problem formulation}
The WDRO problem can be written as
\begin{equation}\label{formal}\beta^{DRO}_n\in\arg\min_{\beta\in B}\sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta),Y)\right],\end{equation}
 where the feature variable $\mathbf{X}$ belongs to $\Omega\subset \mathbb{R}^d$, the label variable $Y$ can be continuous or discrete, $f$ is the hypothesis function parametrized by $\beta\in B\subset\mathbb{R}^d$, $B$ is a compact convex set, $\mathcal{U}_{\rho_n}(\mathbb{P}_n)$ is the Wasserstein uncertainty set, and $L(\cdot,\cdot)$ is the loss function. The Wasserstein uncertainty set is defined as
 \begin{equation}\label{uncertainty}\mathcal{U}_{\rho_n}(\mathbb{P}_N)=\{P: W_p(P,\mathbb{P}_N)\leq \rho_n\},\end{equation}
where $\mathbb{P}_N$ is the empirical distribution of the samples $\{(\mathbf{X}_1,Y_1), (\mathbf{X}_2,Y_2),..., (\mathbf{X}_n,Y_n)\}$ generated by true data-generating distribution $P_{\ast}$,
\[W_p(P,\mathbb{P}_N)=\left(\inf_{\gamma\in\Gamma(P,\mathbb{P}_n)}\left\{\int_{Z^2} d^p(z,z^{\prime}) d\gamma(z,z^{\prime})\right\}\right)^{1/p},\]
$\Gamma(P,\mathbb{P}_n)$ is the set of  distributions with marginals $P$  and  $\mathbb{P}_N$,
$d$ is some metric in space $Z=\mathbf{X}\times Y$,
and $W_p(P_1,P_2)$ is the so-called $p$-Wasserstein distance.

\subsection{Asymptotic distribution of the WDRO estimator}\label{section2.1}
In this subsection, we extend the asymptotic distribution of the WDRO estimator to the supervised statistical learning setting.

\cite{blanchet2022confidence} have derived the asymptotic distribution of the WDRO estimator in the unsupervised learning setting. 
In our study, however, we first let the cost function be infinite if the label variables are different and then adapt the asymptotic distribution of the WDRO estimator to the supervised statistical learning setting.

To adapt the results, we should specify the hyperparameters of the Wasserstein uncertainty set and clarify some regularity conditions, which should be satisfied for the loss function $L(\cdot,\cdot)$ and the true data-generating distribution $P_{\ast}$ of $(\mathbf{X},Y)$.
\begin{assumption}\label{assume1}
The hyperparameters of defining the Wasserstein uncertainty set $\mathcal{U}_{\rho_n}(\mathbb{P}_n)$ \eqref{uncertainty} are prescribed as follows.
\begin{itemize}
\item $\rho_n=\tau/\sqrt{n}$, $\tau>0$,
\item $p=2$,
\item $d\left((\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2)\right)=
\begin{cases}
\Vert\mathbf{x}_1-\mathbf{x}_2\Vert_2&y_1=y_2\\
\infty&y_1\not=y_2
\end{cases}$.
\end{itemize}
\end{assumption}  
\begin{remark}
We justify the choices of hyperparameter in Assumption \ref{assume1} as follows.
\begin{itemize}
    \item 
We choose the radius to be of the square-root order $\mathcal{O}(1/\sqrt{n})$ because the out-of-sample performance guarantee without the curse of dimensionality can be proved \citep{gao2022distributionally}, and a confidence region for the ground-truth parameter can be constructed with the square-root order \citep{blanchet2022confidence}.
\item
We choose the 2-Wasserstein distance since the 2-Wasserstein distance applies to the quadratic loss, and the associated WDRO problem could be solved by iterative algorithms \citep{blanchet2022optimal}.
\item
The cost function is infinite when $y_1\not=y_2$.
 This covers covariate shift in machine learning \citep{aolaritei2022performance} and can be applied to many tasks where the samples are correctly labeled \citep{gao2017distributional}.
 \end{itemize}
\end{remark}
\begin{assumption}\label{assumeloss}
The loss function $L(f(\mathbf{x},\beta),y)$ satisfies:
\begin{itemize}
    \item[a.]  The loss function $L(f(\mathbf{x},\beta),y)$  is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$. 
    \item [b.] For each variable $\mathbf{x}\in\Omega$ and $y$, the loss function $L(f(\mathbf{x},\beta),y)$  is convex w.r.t. $\beta$. 
    \item[c.] For each parameter $\beta\in B$ and variable y, the function $\left\Vert\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \mathbf{x}^2}\right\Vert_2$ is uniformly continuous w.r.t. $\mathbf{x}$ and  uniformly bounded by a continuous function $M(\beta)$.
\end{itemize}
\end{assumption}
\begin{assumption}\label{assumedistr}
The true data-generating distribution $P_{\ast}$ of $(\mathbf{X},Y)$ satisfies:
\begin{itemize}
    \item[a.] There exists $\beta_{\ast}\in B^{\circ}$, where $B^{\circ}$ means the interior of $B$, satisfying
    \[\mathbb{E}_{P_{\ast}}\left[\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial\beta}\Bigg\vert_{\beta=\beta_{\ast}}\right]=0,\]
    and the inequalities
    \begin{equation}\label{matrixD}C:=\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\Bigg\vert_{\beta=\beta_{\ast}}\right]\succ 0,\end{equation}
    \[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]<\infty\] 
    hold,  where $C\succ0$ means the matrix $C$ is a positive definite matrix.
    \item[b.] $P_{\ast}$ is non-degenerate in the sense that  
    \[P_{\ast}\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0,\]
    \[\mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{T}\right]\succ 0,\]
    where $\frac{\partial ^2 L}{\partial \mathbf{x}\partial \beta}$ means taking the gradient first w.r.t. $\beta$ and then w.r.t. $\mathbf{x}$.
\end{itemize}
\end{assumption}

Next, we obtain the associated convergence of the WDRO estimator $\beta_n^{DRO}$ in problem \eqref{formal} under Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr}, which is shown in the following theorem.
\begin{theorem}[Extension of Theorem 1 in \cite{blanchet2022confidence}]
\label{theorem1}
Suppose that Assumption \ref{assume1}, \ref{assumeloss} and \ref{assumedistr} are satisfied, $\Omega=\mathbb{R}^d$ and $\mathbb{E}\left[\Vert\mathbf{X}\Vert_2^2\right]<\infty$, the WDRO estimator $\beta_n^{DRO}$ in problem \eqref{formal} has the following convergence in distribution:
\begin{equation}\label{con}\sqrt{n}(\beta_{n}^{\text{DRO}}-\beta_{\ast})\Rightarrow\mathcal{N}\left(-C^{-1}H\left(\beta_{\ast}\right),D\right),\end{equation}
where 
\begin{equation}\label{orginH}H(\beta_{\ast})=\tau\frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}},\end{equation}
 $\tau$ is the coefficient in the Wasserstein radius $\rho_n=\tau/\sqrt{n}$,
\[D=C^{-1}\Cov\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)C^{-1},\]
and $C$ is defined in \eqref{matrixD}.
\end{theorem}
\begin{remark}
The assumption $\Omega=\mathbb{R}^d$ could be relaxed. If $\Omega$ is compact and could be expressed as $\Omega=\{\mathbf{x}\in\mathbb{R}^d: A\mathbf{x}\leq b\}$,  where $A$ is an $l\times d$ matrix with linearly independent rows and $b\in\mathbb{R}^l$, and $\mathbf{X}$ has a probability density which is absolutely continuous w.r.t. Lebesgue measure, then the convergence \eqref{con} still holds. This claim can be seen in Section 6 in \cite{blanchet2022confidence}.
\end{remark}

Theorem \ref{theorem1} indicates that the term $\sqrt{n}(\beta_{n}^{DRO}-\beta_\ast)$ converges in distribution to a normal distribution with nonzero mean $-C^{-1}H(\beta_{\ast})$. 
Recall that we perturb the samples to achieve robustification.
As explained in \cite{blanchet2021statistical}, the bias term $-C^{-1}H(\beta_{\ast})$ could be understood as pushing towards solutions with less variation resulted by data perturbation.
However, this nonzero bias term may imply that the WDRO estimator is not an accurate estimation of the ground-truth parameter $\beta_{\ast}$.
As we have obtained the expression of the asymptotic distribution of the WDRO estimator $\beta_n^{DRO}$, we may consider transforming the WDRO estimator $\beta_n^{DRO}$ to make the bias term disappear.
%Further, if the additional variance caused by this transformation could also be controlled, an unbiased robust estimator with a smaller mean squared error may be  obtained.
%We will also explore whether this transformation could maintain the robust properties of the WDRO estimator at the same time.

We investigate the distribution of $\beta_n^{DRO}$ when $n$ is not very large. The WDRO esitmator $\beta_n^{DRO}$ is computed in the logistic regression model when $n=200$, and we plot the histograms of $\sqrt{n}(\beta_n^{DRO}-\beta_\ast)$ in Figure \ref{hist}. Two dimensions of  $\beta_n^{DRO}$ are plotted separately.  We conclude from Figure \ref{hist} that $\beta_n^{DRO}$ is approximately normally distributed with a nonzero mean, as asymptotic convergence \eqref{con} suggested. 
We further apply the Shapiroâ€“Wilk test, and the $p$-values associated with two dimensions of $\beta_n^{DRO}$ are $p=0.07963>0.05$ and $p=0.06720>0.05$, respectively. This supports our claim that $\beta_n^{DRO}$ is approximately normally distributed even though the sample size is not very large.
Therefore, making the bias in asymptotic convergence \eqref{con} disappear is meaningful in the sense of both asymptotic and finite sample size.
\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{histogramofbeta1.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{histogramofbeta2.pdf} % second figure itself
    \end{minipage}
    \caption{Histogram of $\beta_n^{DRO}$}
    \label{hist}
\end{figure}
\subsection{The proposed adjusted estimator}\label{adjusted}
This section introduces the formal definition of our adjusted WDRO estimator based on the asymptotic distribution obtained in Section \ref{section2.1}.

Theorem \ref{theorem1} demonstrates that the WDRO estimator $\beta_n^{DRO}$ has a nonzero asymptotic bias $-C^{-1}H(\beta_{\ast})$. 
We plus $C^{-1}H(\beta_\ast)$ to both sides of the convergence \eqref{con} and could derive the following convergence in distribution:
\begin{equation}\label{reform}
\sqrt{n}(\beta_{n}^{DRO}-\widetilde{\beta})\Rightarrow\mathcal{N}(0,D),\end{equation}
where 
\begin{equation}\label{Kn}K_n(\beta_{\ast}):=\widetilde{\beta}=\beta_{\ast}-\frac{C^{-1}H(\beta_{\ast})}{\sqrt{n}},\end{equation}
and we denote the map from $\beta_{\ast}$ to $\widetilde{\beta}$ by $K_n(\cdot)$. To get an asymptotically unbiased estimator of the ground-truth parameter $\beta_\ast$, we can apply the inverse function of $K_n(\cdot)$ to terms on both sides of the convergence via the delta method. In this way, an asymptotically unbiased estimator of the ground-truth parameter $\beta_\ast$ can be obtained. The formal definition of our adjusted estimator is given in the following.
\begin{definition}\label{def}
In the WDRO problem \eqref{formal}, if
\[K_n(\mathbf{z})=\mathbf{z}-\frac{C(\mathbf{z})^{-1}H(\mathbf{z})}{\sqrt{n}},\]
where
\begin{equation}\label{functionH}H(\mathbf{z})=\tau\frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2 \right]}}{\partial \beta}\Bigg\vert_{\beta=\beta_\ast=\mathbf{z}},\end{equation}
\begin{equation}\label{functionC}C(\mathbf{z})=\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\beta_\ast=\mathbf{z}},\end{equation}
and $\tau$ is the coefficient in the Wasserstein radius $\rho_n=\tau/\sqrt{n}$, is well-defined, and $K_n^{-1}(\cdot)$ (locally) exists, then the adjusted WDRO estimator $\beta_n^{ADRO}$ is defined as \begin{equation}\label{definverse}\beta_n^{ADRO}= \mathcal{A}_n(\beta_n^{DRO}):=K_n^{-1}(\beta_n^{DRO}).\end{equation}
\end{definition}

In Definition \ref{def}, the term $H(\cdot)$ appears complicated at first glance. However, the following proposition shows that the function $H(\cdot)$ can be simplified under certain conditions.
\begin{proposition}\label{sufficient2}
    If the hypothesis function in problem \eqref{formal} is a linear function, i.e., $f(\mathbf{x},\beta)=\langle \mathbf{x},\beta\rangle$, and the equation 
    \begin{equation}\label{sufficient2eq}\mathbb{E}_{P_\ast}\left[ \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\frac{\partial L^2(f(\mathbf{X},\beta_\ast),Y)}{\partial^2 f}\mathbf{X}\right]=0,\end{equation}
    where $\frac{\partial L}{\partial f}$ means taking the gradient of $L(\cdot,\cdot)$ w.r.t. the first argument,
    holds for the true data-generating distribution $P_\ast$ and the loss function $L(\cdot,\cdot)$, then the function $H(\beta_\ast)$ defined in \eqref{orginH} can be rewritten as  
\[H(\beta_\ast)=\tau\frac{\beta_\ast}{{\Vert\beta_\ast\Vert_2}}\sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\Bigg\vert_{\beta=\beta_\ast}\right]}.\] 
\end{proposition}

Proposition \ref{sufficient2} implies that the linearity of the hypothesis function and the equation \eqref{sufficient2eq} can promise $H(\beta_\ast)$ is a rescaling of $\beta_\ast$. In particular, the conditions in Proposition \ref{sufficient2} can be satisfied by multiple statistical models, e.g., linear regression, logistic regression, and Poisson regression. The details can be found in Section \ref{glmdro}.
\subsection{Existence of the adjusted WDRO estimator}
The existence of the adjusted WDRO estimator requires the invertibility of the function $K_n(\cdot)$.
This subsection gives sufficient conditions to promise that the function $K_n(\cdot)$ is invertible.

We give specific conditions enabling the invertibility of $K_n(\cdot)$ in the following theorem.
\begin{theorem}\label{multi2}
   If there exists a neighborhood $\mathcal{B}(\beta_n^{DRO})\subset B$ of $\beta_n^{DRO}$ such that $\beta_n^{DRO}\in K_n\left(\mathcal{B}(\beta_n^{DRO})\right)$, $K_n(\mathbf{z})$ is a continuous differentiable function on  $\mathcal{B}(\beta_n^{DRO})$ and \[\sqrt{n}>\vert\lambda_{max}\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right)\vert,~\forall \mathbf{z}\in \mathcal{B}(\beta_n^{DRO})\] holds, where $\nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))$ denotes the Jacobian matrix of  function $C(\mathbf{z})^{-1}H(\mathbf{z})$ at $\mathbf{z}$ and $\lambda_{\max}\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right)$ denotes the largest eigenvalue of $\nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))$, then  $K_n^{-1}(\cdot)$ exists on some open set containing $\beta_n^{DRO}$.
\end{theorem}
\begin{remark}
   The condition $\sqrt{n}>\vert\lambda_{max}\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right)\vert,\forall \mathbf{z}\in \mathcal{B}(\beta_n^{DRO})$ can be relaxed to $\sqrt{n}\not=\lambda_{max}\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right),\forall \mathbf{z}\in \mathcal{B}(\beta_n^{DRO})$.
\end{remark}


Informally, we can conclude from Definition \ref{def} and  Theorem \ref{multi2} that  the adjusted WDRO exists for large enough $n$ as long as the loss function is non-degenerate and smooth enough.

 \subsection{Computation of the adjusted WDRO estimator}\label{com}
 This subsection discusses how to compute the adjusted WDRO estimator $\beta_n^{ADRO}$.
   
Recall that, under conditions in Theorem \ref{multi2}, the adjusted WDRO estimator $\beta_n^{ADRO}$ exists and is defined as
\[\beta_n^{ADRO}=K_n^{-1}(\beta_n^{DRO}).\]

We should first compute the value of $\beta_n^{DRO}$. Afterward, we have that 
\[K_n(\beta_n^{ADRO})=\beta_n^{DRO},\]
which can be written as
\[C(\beta_n^{ADRO})\beta_n^{ADRO}-\frac{H(\beta_n^{ADRO})}{\sqrt{n}}=C(\beta_n^{ADRO})\beta_n^{DRO},\]
where $C(\cdot)$ is defined in \eqref{functionC} and $H(\cdot)$ is defined in \eqref{functionH}. In this way,  the adjusted WDRO estimator $\beta_n^{ADRO}$ is the root of the following nonlinear system
\begin{equation}\label{nonlinear}C(\beta)\beta-\frac{H(\beta)}{\sqrt{n}}-C(\beta)\beta_n^{DRO}=0,\end{equation}
where $\beta\in \mathcal{B}(\beta_n^{DRO})$ is the unknown variable.

Considering the data-generating distribution $P_\ast$ is unknown, the terms $C(\beta)$ and $H(\beta)$ can be approximated nonparametrically.
For instance, if the hypothesis function is the linear function as mentioned in Proposition \ref{sufficient2} and we have observations $(\mathbf{X}_1,Y_1), \cdots, (\mathbf{X}_N,Y_N)$, we approximate the values of $C(\beta)$ and $H(\beta)$ as follows,
\begin{equation*} \label{approxCH}\widehat{C}(\beta)=\frac{1}{N}\sum_{j=1}^{N}\frac{\partial^2 L(f(\mathbf{X}_j,\beta),Y_j)}{\partial \beta^2},\quad \widehat{H}(\beta)=\tau\frac{\beta}{{\Vert\beta\Vert_2}}\sqrt{\frac{1}{N}\sum_{j=1}^{N}\left( \frac{\partial L(f(\mathbf{X}_j,\beta),Y_j)}{\partial f}\right)^2} \end{equation*}

The solution to the multivariate nonlinear system \eqref{nonlinear}, i.e., the adjusted WDRO estimator $\beta_n^{ADRO}$, can be further approximated by the multivariate Newton's method under certain accuracy level.
\subsection{Special case}\label{special}
In this subsection, we focus on a special type of problem where it is simple to check the existence of the adjusted WDRO estimator $\beta_n^{ADRO}$ and compute the value of $\beta_n^{ADRO}$.

The special type of problem is described in the following proposition.
\begin{proposition}\label{sufficient}
    If the function $C(\mathbf{z})$ defined in \eqref{functionC} can be rewritten as $C(\mathbf{z})^{-1}=c(\mathbf{z})I_d$, and the function $H(\mathbf{z})$ defined in \eqref{functionH} can be rewritten as $H(\mathbf{z})=h(\mathbf{z})\mathbf{z}$, where $\mathbf{z}\in\mathbb{R}^d$, $h(\cdot):\mathbb{R}^d\rightarrow \mathbb{R},c(\cdot):\mathbb{R}^d\rightarrow \mathbb{R}$, and the function
    \begin{equation}\label{functionF}F_{\theta,n}(x)=x-\frac{x c\left(x\frac{\theta}{\Vert\theta\Vert_2}\right)h\left(x\frac{\theta}{\Vert\theta\Vert_2}\right)}{\sqrt{n}},~x>0,\theta\in\mathbb{R}^d\end{equation}
    is strictly monotone w.r.t. $x$, then $K_n(\cdot)$ is invertible. The associated inverse function of $K_n(\cdot)$ is
\[{K_n}^{-1}(\mathbf{s})=\frac{\mathbf{s}}{\Vert \mathbf{s}\Vert_2}{F_{\mathbf{s},n}}^{-1}(\Vert\mathbf{s}\Vert_2).\]
\end{proposition}
\begin{remark}
    Proposition \ref{sufficient2} states one scenario where the reformulation of $H(\mathbf{z})=h(\mathbf{z})\mathbf{z}$  holds, where $h(\mathbf{z})=\sqrt{\mathbb{E}_{P_\ast}[( \frac{\partial L(f(\mathbf{X},\mathbf{z}),Y)}{\partial f})^2]}\big/\Vert \mathbf{z}\Vert_2$. Per the reformulation of $C(\mathbf{z})^{-1}$ and the monotonicity of $F_{\theta,n}(\cdot)$, we will show in Section \ref{glmdro} that the linear regression model can satisfy these requirements under certain conditions. 
\end{remark}

As a result, if the conditions in Proposition \ref{sufficient} are satisfied, the adjusted WDRO estimator $\beta_n^{ADRO}$ be expressed as
\[\beta_n^{ADRO}=\frac{\beta_n^{DRO}}{\Vert\beta_n^{DRO}\Vert_2}{F_{\beta_n^{DRO},n}}^{-1}(\Vert\beta_n^{DRO}\Vert_2).\]

In this case, the adjusted WDRO estimator $\beta_n^{ADRO}$ is a rescaling of the WDRO estimator $\beta_n^{DRO}$. We relegate the details of the associated computation procedure to Appendix \ref{computationspecial}.

\section{Properties of the adjusted WDRO estimator}\label{property}
In this section, we proceed to elaborate on how our proposed adjusted WDRO estimator achieves better statistical performance than the classic WDRO estimator from the perspective of the asymptotic mean and mean squared error. Moreover, the discussions about the out-of-sample guarantee are included.

\subsection{Asymptotically unbiased}
We establish the asymptotic distribution of the adjusted WDRO estimator $\beta_n^{ADRO}$.
\begin{theorem}\label{theorem2}
Under Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr}, if the adjusted WDRO estimator $\beta_n^{ADRO}$ exists, then $\beta_n^{ADRO}$ converges in distribution:
\begin{equation*}\sqrt{n}(\beta_{n}^{ADRO}-\beta_{\ast})\Rightarrow\mathcal{N}(0,D_n),\end{equation*} 
where $D_n$ is a sequence of positive definite matrices which are defined as
\[D_n=\nabla\mathcal{A}_n(\widetilde{\beta})C \nabla\mathcal{A}_n(\widetilde{\beta})^{T},\]
\[\widetilde{\beta}=\beta_{\ast}-\frac{C^{-1}H(\beta_{\ast})}{\sqrt{n}},\]
where $C$ is defined in \eqref{matrixD}, $H(\beta_\ast)$ is defined in \eqref{orginH}, $\mathcal{A}_n$ is defined in \eqref{definverse},
and $\nabla \mathcal{A}_n(\widetilde{\beta})$ is the Jacobian matrix of the function $\mathcal{A}_n(\cdot)$ at $\widetilde{\beta}$.
\end{theorem}

Theorem \ref{theorem2} indicates that our proposed estimator $\beta^{ADRO}_n$ is asymptotically unbiased.
\subsection{Improved asymptotic mean squared error}
We evaluate the overall performance of the adjusted WDRO estimator $\beta_n^{ADRO}$ using the mean squared error as our criterion.

% Roughly speaking, the mean squared error equals the sum of square bias and variance.  Although we can reduce the asymptotic bias of $\beta_n^{ADRO}$ to $0$, we should control the variance to obtain a potentially more accurate estimator.

In large sample theory, we could define the asymptotic mean squared error to evaluate the overall asymptotic performance of the estimator.
\begin{definition}[Asymptotic mean squared error]\label{amse}
Suppose $\{\beta_n\}$ is a sequence of estimators of $\beta\in\mathbb{R}^d$ and $\{b_n\}$ is a sequence of positive numbers converging to $0$ as $n$ goes to infinity. Suppose that $\beta_n$ converges in distribution:
\[b_n(\beta_n-\beta)\Rightarrow \mathbf{Y},\]
where $\mathbf{Y}$ is a $d$-dimensional random vector. We define the asymptotic mean squared error (aMSE) by
\[aMSE(\beta_n)=\frac{\mathbb{E}[\mathbf{Y}^{T}\mathbf{Y}]}{b_n^2}.\]
\end{definition}

We could derive the asymptotic mean squared error ($aMSE$) of the WDRO estimator $\beta_n^{DRO}$ and the adjusted WDRO estimator $\beta_n^{ADRO}$.
\begin{proposition}
\label{prop1}Under Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr}, if the adjusted WDRO estimator $\beta_n^{ADRO}$ exists, we have 
\[aMSE(\beta_{n}^{DRO})=\frac{1}{n}\tr(D)+\frac{1}{n}\left(C^{-1}H(\beta_{\ast})\right)^{T}C^{-1}H(\beta_{\ast}),\]
\[aMSE(\beta_{n}^{ADRO})=\frac{1}{n}\tr(D)+\mathcal{O}(\frac{1}{n^{3/2}}),\]
where $\tr(\cdot)$ denotes  the matrix trace.
\end{proposition}

The second term of the asymptotic mean squared error of 
 the WDRO estimator $\beta_n^{DRO}$ is of the order $\mathcal{O}(1/n)$ while the  second term of the asymptotic mean squared error of the adjusted WDRO estimator $\beta_n^{ADRO}$ is of the order $\mathcal{O}(1/n^{3/2})$.
As the first terms of both asymptotic mean squared errors are the same, we can conclude the adjusted WDRO estimator $\beta_{n}^{ADRO}$ has a smaller asymptotic mean squared error than the WDRO estimator $\beta_{n}^{DRO}$. 
It illustrates that our adjusting strategy improves the overall performance of the WDRO estimator from the perspective of statistically asymptotic behavior. 
We will show in our numerical experiments that the proposed estimator also has a smaller mean squared error in finite-sample cases.

\subsection{Out-of-sample performance guarantee}
This subsection discusses the out-of-sample performance guarantee for the adjusted WDRO estimator $\beta_n^{ADRO}$.

The out-of-sample performance guarantee means that the ground-truth out-of-sample error of the WDRO estimator has an upper bound, which is  the so-called generalization bound.
Informally, the out-of-sample performance guarantee for the WDRO estimator $\beta_n^{DRO}$ reads that, with high probability, the following inequality holds,
\begin{equation}\label{outofsample}
\mathbb{E}_{P_{\ast}}\left[ L(f(\mathbf{X},\beta_n^{DRO}),Y)\right]\leq\sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta_n^{DRO}),Y)\right]+\epsilon_n, \end{equation}
where the left-hand side is the generalization error of $\beta_n^{DRO}$, and the first term on the right-hand side is called Wasserstein robust loss of $\beta_n^{DRO}$. Inequality \eqref{outofsample} implies that the ground-truth error of $\beta_n^{DRO}$ is upper bounded by the Wasserstein robust loss up to a higher order residual $\epsilon_n$.

The generalization bound can be developed from the measure concentration inequality in terms of Wasserstein distance for the empirical distribution \citep{fournier2015rate}. However, the associated radius choice suffers from the curse of dimensionality \citep{mohajerin2018data}.
Alternatively, \cite{gao2022finite} derives the generalization bound based on a novel variance-based concentration inequality for the empirical loss.
The associated radius is of the order $\mathcal{O}(1/\sqrt{n})$, which breaks the curse of dimensionality and is consistent with the hyperparameter choice in this paper.


Recall that our proposed adjusted estimator $\beta_n^{ADRO}$ is transformed from the WDRO estimator $\beta_n^{DRO}$. As the  WDRO estimator $\beta_n^{DRO}$ enjoys the out-of-sample performance guarantee \eqref{outofsample}, similar arguments can be established towards the adjusted WDRO estimator $\beta_n^{ADRO}$. 
\begin{corollary}\label{coro1}
Suppose the generalization bound \eqref{outofsample} holds for the WDRO estimator $\beta_n^{DRO}$ for some residual term $\epsilon_n$ with probability $1-\alpha$.
If the loss function $L(f(\mathbf{x},\beta),y)$ is $h$-Lipschitz continuous w.r.t. $\beta$ and the adjusted WDRO estimator $\beta_n^{ADRO}$ exists, then the following inequality,
\[\mathbb{E}_{P_{\ast}}\left[ L(f(\mathbf{X},\beta_n^{ADRO}),Y)\right]\leq\sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta_n^{ADRO}),Y)\right]+ 2h \Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2 +\epsilon_n, \]
holds with probability $1-\alpha$.
\end{corollary}

From the definition of the adjusted WDRO estimator $\beta_n^{ADRO}$, we know that the term $\Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2$ is of order $\mathcal{O}(1/\sqrt{n})$. 
In this sense, the generalization error of the adjusted WDRO estimator $\beta_n^{ADRO}$ can be upper bounded by the Wasserstein robust loss of the adjusted WDRO estimator $\beta_n^{ADRO}$ up to a new residual term, $2h \Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2 +\epsilon_n$, which is of order $\mathcal{O}(\sqrt{n}+\epsilon_n)$.

The discussions above demonstrate that an upper confidence bound on the out-of-sample error of our proposed estimator $\beta_n^{ADRO}$ can be derived. As a result, the adjusting strategy won't sacrifice the WDRO estimator's capacity for generalization.
\section{Adjusted WDRO in the generalized linear model}\label{glmdro}
The generalized linear model is considered in this section since several well-known regression models can be covered, including logistic regression, Poisson regression, and linear regression.
We introduce how the aforementioned regression models satisfy the requirements of applying the adjusted WDRO and develop the associated adjusted WDRO estimators.
\subsection{Formulation of the generalized linear model}
In the generalized linear model, the label variable $Y$ is generated from a particular distribution from the exponential family, including the Bernoulli distribution on $Y\in\{-1,1\}$ in the logistic regression, the Poisson distribution on $Y\in\{0,1,2,...\}$ in the Poisson regression, the normal distribution on $Y\in\mathbb{R}$ in the linear regression, etc.
The expectation of the label variable $Y$ conditional on the feature variable $\mathbf{X}$ is determined by the link function.
With a little abuse of notation, if we denote the nonzero ground-truth parameter by $\beta_\ast$ and the link function by $G(\cdot)$, we have  
\[G(\mathbb{E}[Y|\mathbf{X}=\mathbf{x}])=\langle \mathbf{x},\beta_{\ast}\rangle.\] 

The link functions $G(\cdot)$ are chosen as the logit function in the logistic regression, the log function in the Poisson regression, the identity function in the linear regression, etc.
If we denote the logit function, the log function, and the identity function by $G^{1}(\cdot)$, $G^{2}(\cdot)$, and $G^{3}(\cdot)$, respectively, we have 
\[G^{1}(t)=\log \left(\frac{t}{1-t}\right),\] 
\[ G^{2}(t)=e^{t},\]
\[G^{3}(t)=t.\]

In the generalized linear model, the ground-truth parameter $\beta_{\ast}$ is estimated by the maximum likelihood estimation method, and the associated loss function can be denoted by $L(f(\mathbf{x},\beta),y)=L(\langle\mathbf{x},\beta\rangle,y)$.
If we denote the loss function in the logistic regression, the Poisson regression and the linear regression by $L^{1}(\cdot,\cdot)$, $L^{2}(\cdot,\cdot)$, and $L^{3}(\cdot,\cdot)$, respectively, we have
\[L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)=\log(1+e^{-y\langle \mathbf{x},\beta\rangle}),\]
\[L^{2}\left(\langle\mathbf{x},\beta\rangle,y\right)=e^{\langle \mathbf{x},\beta\rangle}-y\langle \mathbf{x},\beta\rangle,\]
\[L^{3}(\langle\mathbf{x},\beta\rangle,y)=\frac{1}{2}\left(\langle\mathbf{x},\beta\rangle-y\right)^2,\]
where $\beta\in B$, $B$ is a compact convex subset of $\mathbb{R}^d$, $\beta_{\ast}\in {B}^{\circ}$, and $\mathbf{x}\in\Omega\subset\mathbb{R}^d$.

\subsection{The associated adjusted WDRO estimator}\label{example}
This subsection develops the associated adjusted WDRO estimators of the aforementioned generalized linear models.

We first derive the convergence of the WDRO estimator $\beta^{DRO}_n$.
Suppose that our choice of hyperparameters follows Assumption \ref{assume1}.
As demonstrated in Section \ref{section2.1}, we check Assumption \ref{assumeloss} and Assumption \ref{assumedistr} in the following lemmas.
\begin{lemma}\label{checklemma1}
    The loss function $L^{1}(\langle \mathbf{x},\beta\rangle,y)$ satisfies the conditions in Assumption \ref{assumeloss}.
\end{lemma}
\begin{lemma}\label{poissonassume}
If $\Omega$ is bounded, the loss function $L^{2}(\langle\mathbf{x},\beta\rangle,y)$ satisfies the conditions Assumption \ref{assumeloss}.
\end{lemma}
\begin{lemma}\label{linearassume}
The loss function $L^{3}(\langle\mathbf{x},\beta\rangle,y)$ satisfies the conditions Assumption \ref{assumeloss}.
\end{lemma}
\begin{lemma}\label{checklemma2}
    In the logistic regression, if there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, and $\mathbb{E}_{P_\ast}\left[\Vert\mathbf{X}\Vert_2^2\right]<\infty$, Assumption \ref{assumedistr} is satisfied.
\end{lemma}
\begin{lemma}\label{poissonassume2}
        In the Poisson regression, if there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, and $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^2e^{\langle\mathbf{X},\beta_\ast\rangle}]<\infty$, Assumption \ref{assumedistr} is satisfied.
\end{lemma}
\begin{lemma}\label{linearassume2}
        In the linear regression, if there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, and $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^2]<\infty$, Assumption \ref{assumedistr} is satisfied.
\end{lemma}

Lemma \ref{checklemma1}-\ref{linearassume} imply that the loss functions satisfy the conditions in Assumption \ref{assumeloss} while Lemma \ref{checklemma2}-\ref{linearassume2} show that Assumption \ref{assumedistr} can be simplified in the logistic regression, Poisson regression, and linear regression.

Equipped with Lemma \ref{checklemma1}-\ref{linearassume2}, the convergence in distribution of the WDRO estimator $\beta_n^{DRO}$ can be established due to Theorem \ref{theorem1}. 
The following three theorems give the explicit expression of the asymptotic distribution of the WDRO estimator for the logistic regression, Poisson regression, and linear regression in the formulation of \eqref{reform}.
\begin{theorem}[Convergence of $\beta_n^{DRO}$ in the logistic regression]\label{logistic} In the logistic regression, under Assumption \ref{assume1}, if $\Omega=\mathbb{R}^d$ and $\mathbb{E}\left[\Vert\mathbf{X}\Vert_2^2\right]<\infty$, and there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, the WDRO estimator $\beta_n^{DRO}$ converges in distribution:
\[\sqrt{n}(\beta_{n}^{DRO}-{\widetilde{\beta}})\Rightarrow\mathcal{N}(0,D),\]
where
\[D=\left(\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{T}}{\left( 1+e^{Y\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\right)^{-1},\]
\[\widetilde{\beta}= K_n(\beta_\ast)=\beta_\ast-\frac{C^{-1}H(\beta_\ast)}{\sqrt{n}},\]
and
\begin{equation}\label{H1}
C=\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{T}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left( 1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right],\quad
H(\beta_\ast)=\tau\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[\frac{e^{\langle\mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}.\end{equation}
\end{theorem}
\begin{theorem}[Convergence of $\beta_n^{DRO}$ in the Poisson regression]\label{poissontheorem}
In the Poission regression, under Assumption \ref{assume1}, if $\Omega$ is compact and can be expressed as $\Omega=\{\mathbf{x}\in\mathbb{R}^d: A\mathbf{x}\leq b\}$, where $A$ is an $l\times d$ matrix with linearly independent rows and $b\in\mathbb{R}^l$, $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^2e^{\langle\mathbf{X},\beta_\ast\rangle}]<\infty$, there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, and $\mathbf{X}$ has a probability density which is absolutely continuous w.r.t. Lebesgue measure, the WDRO estimator $\beta_n^{DRO}$ converges in distribution:
\[\sqrt{n}(\beta_{n}^{DRO}-\widetilde{\beta})\Rightarrow\mathcal{N}(0,D),\]
where
\[D=\left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right]\right)^{-1},\]
\[\widetilde{\beta}= K_n(\beta_\ast)=\beta_\ast-\frac{C^{-1}H(\beta_\ast)}{\sqrt{n}},\]
and
\begin{equation}\label{H2}
C=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right],\quad
H(\beta_{\ast})=\tau\frac{\beta_{\ast}}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_\ast}[e^{\langle \mathbf{X},\beta_{\ast}\rangle}]}.\end{equation}
\end{theorem}
\begin{theorem}[Convergence of $\beta_n^{DRO}$ in the linear regression]
\label{lineartheorem}
In the linear regression, under Assumption \ref{assume1}, if $\Omega=\mathbb{R}^d$, and $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^2]<\infty$, and there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, the WDRO estimator $\beta_n^{DRO}$ converges in distribution:
\[\sqrt{n}(\beta_{n}^{DRO}-\widetilde{\beta})\Rightarrow\mathcal{N}(0,D),\]
where
\[D=\sigma^2\left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}\right]\right)^{-1},\]\[\widetilde{\beta}= K_n(\beta_\ast)=\beta_\ast-\frac{C^{-1}H(\beta_\ast)}{\sqrt{n}},\]
\begin{equation}\label{H3}C=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}\right],\quad
H(\beta_{\ast})=\tau\sigma\frac{\beta_{\ast}}{\Vert\beta_\ast\Vert_2},\end{equation}
 and $\Var(Y|\mathbf{X})=\sigma^2, \sigma>0$
\end{theorem}

The proofs of Theorem \ref{logistic}-\ref{lineartheorem} are relegated to Appendix \ref{appendix}. The proofs also show that the conditions in Proposition \ref{sufficient2} are satisfied, which enables us to simplify the function $H(\cdot)$, seeing \eqref{H1}-\eqref{H3}.

Next, we discuss the existence of the adjusted WDRO estimator in the logistic regression, Poisson regression, and linear regression in the following Proposition.
\begin{proposition}\label{exist}
In the logistic regression, Poisson regression, and linear regression, under conditions stated Theorem \ref{logistic}-\ref{lineartheorem},
 there exists $0<N_1<\infty$ such that, for each $n>N_1$, 
 %there is a neighborhood $\mathcal{B}(\beta_n^{DRO})\subset B$ of $\beta_n^{DRO}$ satisfying that 
% $\beta_n^{DRO}\in K_n\left(\mathcal{B}(\beta_n^{DRO})\right)$, 
$K_n^{-1}(\cdot)$ exists on some open set containing $\beta_n^{DRO}$.
\end{proposition}

Proposition \ref{exist} implies that the adjusted WDRO estimator always exits for large enough $n$ in the logistic regression, Poisson regression, and linear regression. The associated WDRO estimator can be approximated by the method introduced in Section \ref{com}.

In particular, the matrix $C$ in the linear regression is independent with parameter $\beta_\ast$, seeing \eqref{H3}. If we further assume that $\mathbf{X}$ has zero mean and diagonal covariance matrix with diagonal elements $c$, we can have
\begin{equation}\label{valueofc}C=\mathbb{E}_{P_{\ast}}\left[\mathbf{X}\mathbf{X}^{T}\right]=cI_d,\quad c>0.\end{equation}

In this scenario, combining the expression of the function $H(\cdot)$ in \eqref{H3}, we know the conditions required in the special case illustrated in Proposition \ref{sufficient} are satisfied. Accordingly, we can have the explicit expression of $F_{\theta,n}(x)$, defined in \eqref{functionF}, as follows,
\[F_{\theta,n}(x)=x-\frac{\tau\sigma}{c\sqrt{n}},\]
which is a strictly increasing function.

Due to Proposition \ref{sufficient}, the adjusted WDRO estimator exits for all $n>0$, and the associated explicit expression of $\beta_n^{ADRO}$ is defined as 
\begin{equation}\label{linear}\beta_n^{ADRO}=\frac{\beta_n^{DRO}}{\Vert\beta_n^{DRO}\Vert_2} \left(\Vert\beta^{DRO}_n\Vert_2 + \frac{\tau\sigma}{c\sqrt{n}}\right).\end{equation}

The formula above illustrates that the direction of the WDRO estimator $\beta_n^{DRO}$ in the linear regression model, where the feature variable has zero mean with an isotropic covariance matrix, is ``correct'', while the norm of the $\beta_n^{DRO}$ should be adjusted accordingly.
\section{Numerical experiments}\label{exp}
In this section, we investigate the empirical performance of the adjusted WDRO estimator $\beta_n^{ADRO}$, compared with the classic WDRO estimator $\beta_n^{DRO}$.

\subsection{Experiment setting}
The WDRO algorithmic framework of the logistic regression model and linear regression model with quadratic loss has been established in \cite{blanchet2022optimal}. 
Therefore, the adjusted estimators in the logistic regression model and the
linear regression model are implemented as examples to help explore the practical performance of our adjusting technique.
\subsubsection{Logistic Regression}
Assume the feature variable $\mathbf{X}$ follows the multivariate standard normal distribution. 
Suppose $\mathbf{X}$ follows 2-dimensional standard normal distribution, and the label variable $Y$ follows the Bernoulli distribution, where $\mathbb{P}(Y=1|\mathbf{X}=\mathbf{x})=1/(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle})$ and $\beta_\ast=(1/\sqrt{17},4/\sqrt{17})$. 
Data is generated $5$ times for each sample size $n\in\{500,700,1000,1500,$ $1800,2000\}$. 
The WDRO estimator $\beta_n^{DRO}$ is computed by the iterative algorithm in \cite{blanchet2022optimal}.
The adjusted WDRO estimator $\beta_n^{ADRO}$ is computed by solving the nonlinear system \eqref{nonlinear} via Newton's method.
Per the iterative algorithm, we set the learning rate as $0.3$ and the maximum number of iterations as $50000$, respectively.
Moreover, the value of $\tau$, which is the coefficient in the Wasserstein radius $\rho_n=\tau/\sqrt{n}$, should be determined, so we let $\tau\in\{1,1.5,2,2.5\}$. 

\subsubsection{Linear regression}
Assume the feature variable $\mathbf{X}$ follows the 2-dimensional standard normal distribution, where we have $c=1$ in \eqref{valueofc}, and the label variable $Y$ follows normal distribution, where $Y|\mathbf{X}=\mathbf{x}\sim \mathcal{N}(\langle\mathbf{x},\beta_\ast\rangle,\sigma)$, $\beta_\ast=(3/\sqrt{10},-1/\sqrt{10})$. We set $\sigma=0.1$.
Data is generated $5$ times for each sample size $n\in\{500,700,1000,1500,1800,2000\}$. 
The WDRO estimator $\beta_n^{DRO}$ is computed by the iterative algorithm in \cite{blanchet2022optimal}.
The adjusted WDRO estimator $\beta_n^{ADRO}$ is computed via equation \eqref{linear}.
Per the iterative algorithm, we set the learning rate as $0.3$ and the maximum number of iterations as $50000$, respectively.
Then, we set the value of $\tau$ as $\tau\in\{1,1.5,2,2.5\}$.
\subsection{Experiment results}
The accuracy of the estimators is evaluated by the squared error, which is defined as follows,
\[\Vert \beta_n^{DRO}-\beta_\ast\Vert_2^2,\quad  \Vert \beta_n^{ADRO}-\beta_\ast\Vert_2^2.\]
The results of the logistic regression are reported in Figure \ref{logistic1}-\ref{logistic4}, and the results of the linear regression are reported in Figure \ref{linear1}-\ref{linear4}.

For each  radius choice in each model,
we plot the mean squared error of $\beta_n^{DRO}$ and $\beta_n^{ADRO}$ versus the logarithm of the sample size $n$, respectively. 
From the figures, we can observe that the line of mean squared error of $\beta_n^{DRO}$ is always above that of $\beta_n^{ADRO}$, which illustrates that the proposed adjusted estimator has a smaller mean squared error in empirical experiments. Recall that we have proved the adjusted WDRO estimator has a better asymptotic mean squared error in theory, while our empirical results show that our estimator also outperforms even when the sample size is finite.

In addition, we also compute the difference of the squared error between $\beta_n^{DRO}$ and $\beta_n^{ADRO}$ for each case, which is defined as,
\[\Vert \beta_n^{DRO}-\beta_\ast\Vert_2^2- \Vert \beta_n^{ADRO}-\beta_\ast\Vert_2^2.\]
This quantity can help us to evaluate the improvement achieved by the adjusting technique for each run. To visualize the improvement, we plot the boxplots for each sample size and each value of $\tau$. The figures show that most parts of the boxplots locate above $y=0$ in the logistic regression, and all of the  boxplots locate above $y=0$ in the linear regression. These observations indicate that the adjusting technique can generate a more accurate estimator of the ground-truth parameter $\beta_\ast$.

In conclusion, the adjusted WDRO estimator has better empirical performance than the classic WDRO estimator. When people plan to  estimate parameters in the statistical learning model, the proposed adjusted estimator can be considered.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errordim2_1.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errortestdim2_1.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the logistic regression, $\tau=1$.}
    \label{logistic1}
\end{figure}
\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errordim2_1point5.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errortestdim2_1point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the logistic regression, $\tau=1.5$.}
    \label{logistic2}
\end{figure}
\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errordim2_2.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errortestdim2_2.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the logistic regression, $\tau=2$.}
    \label{logistic3}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errordim2_2point5.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errortestdim2_2point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the logistic regression, $\tau=2.5$.}
    \label{logistic4}
\end{figure}


\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrordim2_1.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrortestdim2_1.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the linear regression, $\tau=1$.} 
    \label{linear1}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrordim2_1point5.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrortestdim2_1point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the linear regression, $\tau=1.5$.}
     \label{linear2}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrordim2_2.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrortestdim2_2.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the linear regression, $\tau=2$.}
     \label{linear3}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrordim2_2point5.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrortestdim2_2point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error plots of the linear regression, $\tau=2.5$.}
     \label{linear4}
\end{figure}

\section{Discussions}
In this paper, we propose a statistically favorable alternative, which is called the adjusted WDRO estimator, to the classic WDRO estimator.
The adjusted WDRO estimator is transformed from the WDRO estimator, and the idea behind the proposed estimator is to de-bias the asymptotic distribution of the classic WDRO estimator.
%The resulting transformation is essentially a rescaling of the WDRO estimator.
In this sense, the proposed estimator is easy to obtain once we get the value of  the WDRO estimator.
While we have proposed a transformation that makes the asymptotic bias disappear, the future direction may be to minimize the associated asymptotic mean square error among all possible transformations.
The potential `optimal' transformation is of super interest since it may possess the `best' statistical property from the perspective of asymptotic behavior.
Other future directions include extending our theory to more general settings and adapting the adjusted technique to more practical applications.

\acks{This project is partially supported by the Transdisciplinary Research Institute for Advancing Data Science (TRIAD), \url{https://research.gatech.edu/data/triad}, which is a part of the TRIPODS program at NSF and located at Georgia Tech, enabled by the
NSF grant CCF-1740776. The authors are also partially sponsored by NSF grants 2015363.}

\newpage
\appendix
\section{Proof}\label{appendix}
\subsection{Proof of Theorem \ref{theorem1}}
\begin{proof}
We denote the inner maximization of the WDRO problem \eqref{formal}, i.e., 
\[\max_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P}[L(f(\mathbf{X},\beta),Y)],\]
by $\Psi_n(\beta)$. 

Then, we have
\begin{equation}\label{vrclass}\Psi_n(\beta)=\inf_{\lambda\geq 0} \left[\lambda\rho^2_n+ \mathbb{E}_{(\mathbf{X},Y)\sim\mathbb{P}_n}\left[\sup_{\mathbf{x}\in\mathbb{R}^d}\left[L(f(\mathbf{x},\beta),Y)-\lambda\Vert \mathbf{x}-\mathbf{X}\Vert_2^2 \right]\right]\right].\end{equation}

Note that Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr} are extracted from Assumption 1 and 2 in \cite{blanchet2022confidence}, and problem \eqref{vrclass} can be reduced to the problem in Lemma A.1 in \cite{blanchet2022confidence}. Following the same technique, one could derive the convergence in distribution of $\beta_n^{DRO}$:
\[\sqrt{n}(\beta^{DRO}_n-\beta_{\ast})\Rightarrow C^{-1}E-C^{-1}H(\beta_\ast),\]
where \[E\sim \mathcal{N}\left(0,\Cov\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)\right),\]
\[H(\beta_{\ast})=\tau \frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}.\]

It follows from $C$ is a symmetric matrix that 
\[\sqrt{n}(\beta^{DRO}_n-\beta_{\ast})\Rightarrow  \mathcal{N}\left(-C^{-1}H(\beta_\ast),C^{-1}\Cov\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)C^{-1}\right).\]
\end{proof}
\subsection{Proof of Proposition \ref{sufficient2}}
\begin{proof}
If  $f(\mathbf{X},\beta)=\langle \mathbf{X},\beta\rangle$ holds, we have
\begin{equation}
\begin{aligned}\label{decomposeH}
H(\beta_{\ast})&=\tau \frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\\
&=\tau \frac{\partial \left(\Vert \beta\Vert_2 \sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\right]}\right)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\\
&=\tau\left(\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2} \sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\right)^2\right]}+\Vert\beta_{\ast}\Vert_2\frac{\mathbb{E}_{P_\ast}\left[ \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\frac{\partial L^2(f(\mathbf{X},\beta_\ast),Y)}{\partial^2 f}\mathbf{X}\right] }{ \sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\right)^2\right]}}\right).
\end{aligned}\end{equation}

Further, if \[\mathbb{E}_{P_\ast}\left[ \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\frac{\partial L^2(f(\mathbf{X},\beta_\ast),Y)}{\partial^2 f}\mathbf{X}\right]=0\] holds, 
the second term in the equation \eqref{decomposeH} equals to 0. 


Then, we have
\[H(\beta_\ast)=\tau\frac{\beta_\ast}{{\Vert\beta_\ast\Vert_2}}\sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\Bigg\vert_{\beta=\beta_\ast}\right]}.\] 
\end{proof}
\subsection{Proof of Theorem \ref{multi2}}
\begin{proof}
We  have the Jacobian matrix  $\nabla_{\mathbf{z}}K_n(\mathbf{z})$ of the function $K_n(\mathbf{z})$ as follows,
\[\nabla_{\mathbf{z}}K_n(\mathbf{z})=I_d-\frac{\nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))}{\sqrt{n}}.\]

Since
\[\sqrt{n}>\vert\lambda_{max}\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right)\vert,~\forall \mathbf{z}\in \mathcal{B}(\beta_n^{DRO}),\]
we have 
\[\left\vert\lambda_{\max}\left(\frac{\nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))}{\sqrt{n}}\right)\right\vert<1,~\forall \mathbf{z}\in \mathcal{B}(\beta_n^{DRO}).\]
Thus, all the eigenvalues of $\nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))/\sqrt{n}$ are not equal to 1.
From the claim that the matrix $I_d-A$ is invertible if and only if $\lambda=1$ is not an eigenvalue of matrix $A$, we can conclude that $\nabla_{\mathbf{z}}K_n(\mathbf{z})$ is invertible at each point in $\mathcal{B}(\beta_n^{DRO})$. It follows from the inverse function theorem that the proof is done.
\end{proof}
\subsection{Proof of Proposition \ref{sufficient}}
\begin{proof}
Recall the equation \[K_n(\beta_{\ast})=\beta_{\ast}-\frac{C^{-1}H(\beta_{\ast})}{\sqrt{n}}.\]

If $H(\mathbf{z})=h(\mathbf{z})\mathbf{z}$ and $C(\mathbf{z})^{-1}=c(\mathbf{z})I_d$ , we have
\[K_n(\mathbf{z})=\frac{\mathbf{z}}{\Vert\mathbf{z}\Vert_2}\left(\Vert\mathbf{z}\Vert_2-\frac{\Vert\mathbf{z}\Vert_2c(\mathbf{z})h(\mathbf{z})}{\sqrt{n}}\right),\]
and 
\[\Vert K_n(\mathbf{z})\Vert_2=\Vert\mathbf{z}\Vert_2-\frac{\Vert\mathbf{z}\Vert_2c(\mathbf{z})h(\mathbf{z})}{\sqrt{n}},\]
which implies that $K_n(\mathbf{z})$ and $\mathbf{z}$ are of the same direction while $\Vert K_n(\mathbf{z})\Vert_2$ is a function of $\mathbf{z}$.
%while $\Vert K_n(\mathbf{z})\Vert_2$ and $\Vert\mathbf{z}\Vert_2$ are different.
% Further, we have, if the direction of $\mathbf{z}$ is fixed, $\Vert K_n(\mathbf{z})\Vert$ is a function $\Vert \mathbf{z}\Vert_2$. Considering 
%     \[F_{n,\theta}(\mathbf{z})=F_n(\mathbf{z})=\Vert\mathbf{z}\Vert_2-\frac{\Vert\mathbf{z}\Vert_2 h(\mathbf{z})}{\sqrt{n}}\]
%     is strictly increasing w.r.t. $\Vert\mathbf{z}\Vert_2$, $\Vert K_n(\mathbf{z})\Vert$ is invertible.

Given $\mathbf{s}=K_n(\mathbf{z})$, we could conclude that
\[ \frac{\mathbf{s}}{\Vert \mathbf{s}\Vert_2}=\frac{\mathbf{z}}{\Vert \mathbf{z}\Vert_2},\]
 \[\Vert\mathbf{s}\Vert_2=\Vert K_n(\mathbf{z})\Vert_2=F_{\mathbf{s},n}(\Vert\mathbf{z}\Vert_2).\]

In conclusion, given $\mathbf{s}$, we know that $\Vert K_n(\mathbf{z})\Vert_2=F_{\mathbf{s},n}(\Vert\mathbf{z}\Vert_2)$ is strictly monotone w.r.t. $\Vert\mathbf{z}\Vert_2$, so $\Vert K_n(\mathbf{z})\Vert_2=F_{\mathbf{s},n}(\Vert\mathbf{z}\Vert_2)$ is invertible w.r.t. $\Vert \mathbf{z}\Vert_2$. 
In this way, function $K_n(\cdot)$ is invertible, and the associated inverse function can be expressed as
    \[{K_n}^{-1}(\mathbf{s})=\frac{\mathbf{s}}{\Vert \mathbf{s}\Vert_2}{F_{\mathbf{s},n}}^{-1}(\Vert\mathbf{s}\Vert_2),\]
where we keep the direction but compute the inverse of the norm.
\end{proof}

\subsection{Proof of Theorem \ref{theorem2}}
\begin{proof}
Recall we have the limiting distribution
\[\sqrt{n}(\beta_{n}^{DRO}-\widetilde{\beta})\Rightarrow\mathcal{N}(0,D).\]

Combining the  delta method and the fact
\[\mathcal{A}_n(\widetilde{\beta})=\mathcal{A}_n(K_n(\beta_\ast))=\beta_\ast,\] we could have the following convergence
\[\sqrt{n}\left(\mathcal{A}_n(\beta_{n}^{DRO})-\beta_{\ast}\right)\Rightarrow\mathcal{N}(0,\nabla \mathcal{A}_n(\widetilde{\beta})C \nabla\mathcal{A}_n(\widetilde{\beta})^{T}),\]
where  $\nabla \mathcal{A}_n$ is the Jacobian matrix of the function $\mathcal{A}_n(\cdot)$.
\end{proof}
\subsection{Proof of Proposition \ref{prop1}}
\begin{proof}
We first consider the WDRO estimator $\beta_{n}^{DRO}$. From the following asymptotic result
\[\sqrt{n}\left(\beta_{n}^{DRO}-\beta_{\ast}\right)\Rightarrow\mathcal{N}(- C^{-1}H(\beta_{\ast}),D),\]
one can compute the asymptotic mean squared error of $\beta_{n}^{DRO}$ as follows,
\[aMSE(\beta_{n}^{DRO})=\frac{1}{n}\tr(D)+\frac{1}{n}\left((C^{-1}H(\beta_{\ast})^{T})(C^{-1}H(\beta_{\ast}))\right).\]


Then, we consider the adjusted WDRO estimator $\beta_{n}^{ADRO}$. Notice we have 
\[\sqrt{n}\left(\beta_{n}^{ADRO}-\beta_{\ast}\right)\Rightarrow\mathcal{N}(0,\nabla \mathcal{A}_n(\widetilde{\beta})D \nabla\mathcal{A}_n(\widetilde{\beta})^{T}),\]
then we have
\[aMSE(\beta_{n}^{ADRO})=\frac{1}{n}\tr(\nabla\mathcal{A}_n(\widetilde{\beta})D \nabla\mathcal{A}_n(\widetilde{\beta})^{T})=\frac{1}{n}\tr( \nabla\mathcal{A}_n(\widetilde{\beta})^{T}\nabla\mathcal{A}_n(\widetilde{\beta})D).\]

Next, we calculate $\mathcal{A}_n(\widetilde{\beta})$ and $\mathcal{A}_n(\widetilde{\beta})^{T}$ via the inverse function theorem as follows,
\[\nabla\mathcal{A}_n(\widetilde{\beta})=[\nabla K_n(\mathcal{A}_n(\widetilde{\beta}))]^{-1}=[\nabla K_n(\beta_{\ast})]^{-1}= \left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}}\right]^{-1},\]
where $\nabla \left(C^{-1}H(\beta_{\ast})\right)$ denotes the gradient of the function $C^{-1}H(\beta_\ast)$. 

By the identity 
\[(A+B)^{-1}=A^{-1}-A^{-1}B(A+B)^{-1},\] we have 
\[\nabla\mathcal{A}_n(\widetilde{\beta})= I_d +\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}} \left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}}\right]^{-1}.\]

Consequently, we have
\begin{equation*}
\begin{aligned}
&\tr(\nabla\mathcal{A}_n(\widetilde{\beta})^{T}\nabla\mathcal{A}_n(\widetilde{\beta})D)\\
&=\tr\left(\left(I_d +\left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)^{T}}{\sqrt{n}}\right]^{-1}\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)^{T}}{\sqrt{n}}\right)\right.\\
&\left.\left( I_d +\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}} \left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}}\right]^{-1}\right)D\right),\\
&=\tr\left( \left(I_d + \frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}}\left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}}\right]^{-1} \right. \right.\\
&+\left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)^{T}}{\sqrt{n}}\right]^{-1}\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)^{T}}{\sqrt{n}}
\\
&\left.\left.\frac{1}{n}\left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}}\right]^{-1}\nabla \left( C^{-1}H(\beta_{\ast})\right)^{T}\nabla \left( C^{-1}H(\beta_{\ast})\right)\left[I_d-\frac{\nabla \left( C^{-1}H(\beta_{\ast})\right)}{\sqrt{n}}\right]^{-1}\right)D\right)\\
&=\tr(D)+\mathcal{O}(\frac{1}{\sqrt{n}}).
\end{aligned}\end{equation*}

 Then, we can conclude that 
\[aMSE(\beta_{n}^{ADRO})=\frac{1}{n}\tr(\nabla\mathcal{A}_n(\widetilde{\beta})D \nabla\mathcal{A}_n(\widetilde{\beta})^{T})=\frac{1}{n}\tr(D)+\mathcal{O}(\frac{1}{n^{3/2}}).\]
\end{proof}
\subsection{Proof of Corollary \ref{coro1}}
\begin{proof}
Since $\beta_n^{ADRO}$ exists and the function $L(f(\mathbf{x},\beta),y)$ is $h$-Lipstchiz w.r.t. $\beta$, we have that
    \[\vert L(f(\mathbf{x},\beta_n^{ADRO}),Y)-L(f(\mathbf{x},\beta_n^{DRO}),Y)\vert\leq h \Vert \beta_n^{DRO}-\beta_n^{ADRO}\Vert_2.\]

Thus,     
 \[ L(f(\mathbf{x},\beta_n^{ADRO}),Y)\leq L(f(\mathbf{x},\beta_n^{DRO}),Y)+h \Vert \beta_n^{DRO}-\beta_n^{ADRO}\Vert_2,\]
  \[ L(f(\mathbf{x},\beta_n^{DRO}),Y)\leq L(f(\mathbf{x},\beta_n^{ADRO}),Y)+h \Vert \beta_n^{DRO}-\beta_n^{ADRO}\Vert_2.\]

Then, we can have that  
\begin{equation*}
\begin{aligned}
\mathbb{E}_{P_{\ast}}\left[ L(f(\mathbf{X},\beta_n^{ADRO}),Y)\right]&\leq \mathbb{E}_{P_{\ast}}\left[ L(f(\mathbf{X},\beta_n^{DRO}),Y)\right]+h \Vert \beta_n^{DRO}-\beta_n^{ADRO}\Vert_2\\
&\leq\sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta_n^{DRO}),Y)\right]+ h \Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2 +\epsilon_n\\
&\leq \sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta_n^{ADRO}),Y)\right]+ 2h \Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2 +\epsilon_n.
\end{aligned}
\end{equation*}
holds with probability $1-\alpha$.
\end{proof}
\subsection{Proof of Lemma \ref{checklemma1}}
\begin{proof}
    \textbf{a.} The loss function $L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)=\log(1+e^{-y\langle \mathbf{x},\beta\rangle})$ is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$.

    \textbf{b.} Because we have \[\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta^2}=\frac{\mathbf{x}\mathbf{x}^{T}e^{y\langle\mathbf{x},\beta\rangle}}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^2}\succeq 0,\]
    where $\succeq$ means the matrix is positive semidefinite,
    the function $L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)$ is convex w.r.t. $\beta$.
    
     \textbf{c.} We have
    \begin{equation*}
    \begin{aligned}
    \left\Vert\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2&=\left\Vert\frac{\beta\beta^{T}e^{y\langle\mathbf{x},\beta\rangle}}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^2}\right\Vert_2\\
    &=\left\Vert \beta\right\Vert_2^2 \frac{e^{y\langle\mathbf{x},\beta\rangle}}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^2}<M(\beta)=\Vert\beta\Vert_2^2.
    \end{aligned}
    \end{equation*}
   
    Further, 
    \[\frac{\partial \left\Vert\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2}{\partial \mathbf{x}}=\left\Vert \beta\right\Vert_2^2 \frac{y\beta e^{y\langle \mathbf{x},\beta\rangle} \left( 1-e^{y\langle \mathbf{x},\beta\rangle}\right)}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^3}.\]

    We know 
    \[\frac{ e^{y\langle \mathbf{x},\beta\rangle} \left( 1-e^{y\langle \mathbf{x},\beta\rangle}\right)}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^3}\]
    is bounded. In addition, as $\beta\in B$ and $B$ is bounded, \[\frac{\partial \left\Vert\frac{\partial^2L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2}{\partial \mathbf{x}}\] is also bounded, which implies that \[\left\Vert\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2\]is uniformly continous w.r.t. $\mathbf{x}$.
\end{proof}
\subsection{Proof of Lemma \ref{poissonassume}}
\begin{proof}
    \textbf{a.} The loss function $L^{2}\left(\langle\mathbf{x},\beta\rangle,y\right)=e^{\langle \mathbf{x},\beta\rangle}-y\langle \mathbf{x},\beta\rangle$ is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$.

\textbf{b.} Because we have\[\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\mathbf{x}\mathbf{x}^Te^{\langle \mathbf{x},\beta\rangle}\succeq 0,\]
the function $L^{2}\left(\langle\mathbf{x},\beta\rangle,y\right)$ is convex w.r.t. $\beta$.

\textbf{c.}
We have
\[\left\Vert\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}^2}\right\Vert_2 = \left\Vert\beta\beta^T\right\Vert_2 e^{\langle \mathbf{x},\beta\rangle}=\Vert \beta\Vert_2^2 e^{\langle\mathbf{x},\beta\rangle}.\]

Because $\mathbf{x}\in \Omega, \beta\in B$, where both $\Omega$ and $B$ are bounded, $\Vert\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}^2}\Vert_2$ is bounded by a function of $\beta$ and uniformly continuous w.r.t. $\mathbf{x}$.
\end{proof}


\subsection{Proof of Lemma \ref{linearassume}}
\begin{proof}
    \textbf{a.} The loss function $L^{3}(\langle\mathbf{x},\beta\rangle,y)=\frac{1}{2}\left(\langle\mathbf{x},\beta\rangle-y\right)^2$ is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$.

\textbf{b.} The loss function $L^{3}(\langle\mathbf{x},\beta\rangle,y)=\frac{1}{2}\left(\langle\mathbf{x},\beta\rangle-y\right)^2$ is convex w.r.t. $\beta$.

     \textbf{c.} We have
      \[\left\Vert\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \mathbf{x}^2}\right\Vert_2=\Vert 2\beta\beta^{T}\Vert_2=2\Vert\beta\Vert_2^2.\]
      
      As $\beta\in B$ and $B$ is bounded, $\Vert\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \mathbf{x}^2}\Vert_2$ is bounded by a function of $\beta$ and uniformly continuous w.r.t. $\mathbf{x}$.
\end{proof}
\subsection{Proof of Lemma \ref{checklemma2}}
\begin{proof}
\textbf{a.}
From the equation
\[ \frac{\partial L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta}=\frac{-y\mathbf{x}}{1+e^{y\langle \mathbf{x},\beta\rangle}},\]
and the assumption $\mathbb{E}_{P_\ast}\left[\Vert \mathbf{X}\Vert_2^2\right]<\infty$, we have
\[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]<\mathbb{E}_{P_\ast}\left[\Vert \mathbf{X}\Vert_2^2\right]<\infty.\]

Because we have
\[\frac{\partial^2 L^{1}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\frac{\mathbf{x}\mathbf{x}^T e^{y\langle\mathbf{x},\beta\rangle}}{\left( 1+e^{y\langle \mathbf{x},\beta}\right)^2},\]
 where \[e^{y\langle\mathbf{x},\beta\rangle}/ (1+e^{y\langle \mathbf{x},\beta})^2>0,\] and there does not exist nonzero $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, we could conclude
\[\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta^2}\bigg\vert_{\beta=\beta_{\ast}}\right]\succ 0.\]

In addition,
    \begin{equation*}\begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial\beta}\Bigg\vert_{\beta=\beta_{\ast}}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[ \frac{-Y}{1+e^{Y\langle\mathbf{X},\beta_\ast\rangle}}\right]\\
    &=\int \mathbb{P}(Y=1|\mathbf{X}=\mathbf{x})\frac{-1}{1+e^{\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})+\int \mathbb{P}(Y=-1|\mathbf{X}=\mathbf{x})\frac{1}{1+e^{-\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{-\langle\mathbf{x},\beta_\ast\rangle}}\frac{-1}{1+e^{\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})+\int \frac{1}{1+e^{\langle\mathbf{x},\beta_\ast\rangle}}\frac{1}{1+e^{-\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})\\
    &=0.
    \end{aligned}\end{equation*}

\textbf{b.} Notice we have
\[\frac{\partial L^{1}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}}\Bigg\vert_{\beta=\beta_{\ast}}=\frac{-y\beta_{\ast}}{1+e^{y\langle\mathbf{x},\beta_\ast\rangle}},\]
where \[\beta_{\ast}\not=0, y\not=0,1+e^{y\langle\mathbf{x},\beta_\ast\rangle}>0, \] so we can conclude 
\[P_{\ast}\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0.\]

Then, we have
    \[\frac{\partial ^2L^{1}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}=\frac{-yI_d}{1+e^{y\langle \mathbf{x},\beta_\ast\rangle}}+\frac{\beta_\ast\mathbf{x}^T e^{y\langle\mathbf{x},\beta_\ast\rangle}}{\left( 1+e^{y\langle \mathbf{x},\beta_\ast\rangle}\right)^2}.\]

Since there is no nonzero vector $\alpha$ such that $P_\ast(\alpha^{T}\mathbf{X}=0)=1$, and the kernel space of the matrix $\frac{\partial ^2L^{1}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\big\vert_{\beta=\beta_{\ast}}$ is different for different $\mathbf{x},y$, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{T}\right]\succ 0.\]
\end{proof}
\subsection{Proof of Lemma \ref{poissonassume2}}
\begin{proof}
\textbf{a.}
From the equation
\[ \frac{\partial L^{2}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta}=\mathbf{x}e^{\langle \mathbf{x},\beta\rangle}-y\mathbf{x},\]
 we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]\\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( e^{\langle \mathbf{X},\beta_\ast\rangle}-Y\right)^2 \right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( e^{\langle \mathbf{X},\beta_\ast\rangle}-Y\right)^2\bigg\vert \mathbf{X}\right]\right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( e^{2\langle \mathbf{X},\beta_\ast\rangle}+Y^2-2Ye^{\langle \mathbf{X},\beta_\ast\rangle}\right)\bigg\vert \mathbf{X}\right]\right]\\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2e^{2\langle\mathbf{X},\beta_\ast\rangle}\right]-2\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2e^{\langle\mathbf{X},\beta_\ast\rangle}Y\big\vert\mathbf{X}\right]+\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2Y^2\big\vert\mathbf{X}\right].\end{aligned}
\end{equation*}   

$Y$ conditional on $\mathbf{X}$ follows the Poisson distribution with parameter $e^{\langle\mathbf{X},\beta_\ast\rangle}$.
In this way, we have that 
\[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]=\mathbb{E}_{P_{\ast}}\left[ \Vert\mathbf{X}\Vert_2^2e^{\langle\mathbf{X},\beta_\ast\rangle} \right]<\infty.\]


Because we have
\[\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\mathbf{x}\mathbf{x}^T e^{\langle\mathbf{x},\beta\rangle},\]
 where $e^{\langle\mathbf{x},\beta\rangle}>0$, and there does not exist nonzero $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, we could conclude
\[\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta^2}\bigg\vert_{\beta=\beta_\ast}\right]\succ 0.\]

In addition, 
  \begin{equation*}\begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial\beta}\Bigg\vert_{\beta=\beta_{\ast}}\right]\\&=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} e^{\langle\mathbf{X},\beta_{\ast}\rangle}-Y\mathbf{X}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} e^{\langle\mathbf{X},\beta_{\ast}\rangle}-Y\mathbf{X}\bigg\vert \mathbf{X}\right]\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} e^{\langle\mathbf{X},\beta_{\ast}\rangle}-e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]\\
    &=0,
    \end{aligned}\end{equation*}
    \textbf{b.} Notice we have
\[\frac{\partial L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}}\Bigg\vert_{\beta=\beta_{\ast}}=\beta_\ast (e^{\langle \mathbf{x},\beta_\ast\rangle}-y),\]
where $\beta_{\ast}\not=0$,
\[P_\ast\left(e^{\langle \mathbf{X},\beta_\ast\rangle}-Y\not=0\right)>0,\]
so we can conclude 
\[P_{\ast}\left(\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0.\]


Then, we have
    \[\frac{\partial ^2L^{2}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}=I( e^{\langle\mathbf{x},\beta_\ast\rangle}-y)+\beta_\ast \mathbf{x}^{T} e^{\langle\mathbf{x},\beta_\ast\rangle}.\]


Since there is no nonzero vector $\alpha$ such that $P_\ast(\alpha^{T}\mathbf{X}=0)=1$ and the kernel space of the matrix $\frac{\partial ^2L^{2}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\big\vert_{\beta=\beta_{\ast}}$ is different for different $\mathbf{x},y$. Thus, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{T}\right]\succ 0.\]
\end{proof}
\subsection{Proof of Lemma \ref{linearassume2}}
\begin{proof}
\textbf{a.}    From the equation
\[ \frac{\partial L^{3}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta}=(\langle\mathbf{x},\beta\rangle-y)\mathbf{x},\]
 we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]\\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( \langle \mathbf{X},\beta_\ast\rangle-Y\right)^2 \right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( \langle \mathbf{X},\beta_\ast\rangle-Y\right)^2\bigg\vert \mathbf{X}\right]\right]\\
&= \mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( \langle \mathbf{X},\beta_\ast\rangle^2-2\langle \mathbf{X},\beta_\ast\rangle Y+Y^2\right)\bigg\vert \mathbf{X}\right]\right]      \\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2\langle \mathbf{X},\beta_\ast\rangle^2\right]-2 \mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2\langle \mathbf{X},\beta_\ast\rangle Y\vert \mathbf{X}\right]+ \mathbb{E}_{P_\ast}\left[ Y^2\vert \mathbf{X}\right]
\end{aligned}
\end{equation*} 

Notice that $Y$ conditional on $\mathbf{X}$ follows the normal distribution with a mean value of $\langle \mathbf{X},\beta_\ast\rangle$.

Thus, 
\[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]=\Var(Y^2|\mathbf{X})\mathbb{E}_{P_{\ast}}\left[ \Vert\mathbf{X}\Vert_2^2\right]<\infty.
\]

Because we have
\[\frac{\partial^2 L^{3}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\mathbf{x}\mathbf{x}^T,\]
and there does not exist nonzero $\alpha$ such that $P_{\ast}(\alpha^{T}\mathbf{X}=0)=1$, we could conclude
\[\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta^2}\bigg\vert_{\beta=\beta_\ast}\right]\succ 0.\]

In addition, 
  \begin{equation*}\begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial\beta}\Bigg\vert_{\beta=\beta_{\ast}}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} \langle\mathbf{X},\beta_{\ast}\rangle-Y\mathbf{X}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} \langle\mathbf{X},\beta_{\ast}\rangle-Y\mathbf{X}\vert\mathbf{X}\right]\right]\\
    &=0,
    \end{aligned}\end{equation*}
    \textbf{b.}
     Notice that,
\[\frac{\partial L^{3}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}}\Bigg\vert_{\beta=\beta_{\ast}}=\beta_\ast \left(\langle \mathbf{x},\beta_\ast\rangle-y\right),\]
where $\beta_{\ast}\not=0$,
\[P_\ast\left(\langle \mathbf{X},\beta_\ast\rangle-Y\not=0\right)>0,\]
so we can conclude 
\[P_{\ast}\left(\frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0.\]

Then, we have
    \[\frac{\partial ^2L^{3}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}=I\left( \langle\mathbf{x},\beta_\ast\rangle-y\right)+\beta_\ast \mathbf{x}^{T}.\]

 Since there is no nonzero vector $\alpha$ such that $P_\ast(\alpha^{T}\mathbf{X}=0)=1$ and the kernel space of the matrix $\frac{\partial ^2L^{3}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\big\vert_{\beta=\beta_{\ast}}$ is different for different $\mathbf{x}, y$, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{T}\right]\succ 0.\]
\end{proof}
\subsection{Proof of Theorem \ref{logistic}}\label{prooflogistic}
\begin{proof}
Regarding the asymptotic covariance matrix,
    because we have
    \[\Cov\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)= C=\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{T}}{\left( 1+e^{Y\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right],\]
    we could derive 
    \[D=C^{-1}\Cov\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right) C^{-1}=\left(\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{T}}{\left( 1+e^{Y\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\right)^{-1}.\]
   
    Regarding the asymptotic mean of $\beta_n^{ADRO}$, we have 
    \begin{equation}\label{h1}H(\beta_{\ast})=\tau\left(\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ \frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}-\frac{\Vert\beta_{\ast}\Vert_2 \mathbb{E}_{P_{\ast}}\left[ \frac{Y\mathbf{X}e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^3}\right]}{\sqrt{\mathbb{E}_{P_{\ast}}\left[ \frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}}\right).\end{equation}
    
    Notice that 
    \begin{equation*}
    \begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{Y\mathbf{X}e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^3}\right]\\
    &=\int P(Y=1|\mathbf{X}=\mathbf{x}) \frac{\mathbf{x}e^{\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x}) -\int P(Y=-1|\mathbf{X}=\mathbf{x}) \frac{\mathbf{x}e^{-\langle\mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x}) -\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{-\langle\mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{2\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x}) -\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{2\langle\mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x})\\
    &=0
    \end{aligned}
    \end{equation*}
which indicates that the equation \eqref{sufficient2eq} holds and the second term in \eqref{h1} equals to $0$.


    
Then, we obtain that 
    \[H(\beta_{\ast})=\tau\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ \frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}.\]

    Notice that 
    \begin{equation*}
    \begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\\
    &=\int P(Y=1|\mathbf{X}=\mathbf{x}) \frac{1}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x}) +\int P(Y=-1|\mathbf{X}=\mathbf{x}) \frac{1}{\left(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{1}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x}) +\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{1}{\left(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{e^{\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x}) +\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{e^{2\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{e^{\langle \mathbf{x},\beta_{\ast}\rangle}+e^{2\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\mathbb{E}_{P_{\ast}}\left[\frac{e^{\langle\mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]
    \end{aligned}
    \end{equation*}

Then, $H(\beta_\ast)$ can be simplified as 
\begin{equation*}H(\beta_\ast)=\tau\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[\frac{e^{\langle\mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}.\end{equation*}

Similarly, the matrix $D$ and $C$ can further be simplified as
\[D=\left(\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{T}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left( 1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\right)^{-1},\quad C=\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{T}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left( 1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right].\]
\end{proof}





\subsection{Proof of Theorem \ref{poissontheorem}}\label{proofpoisson}
\begin{proof}
Regarding the asymptotic covariance matrix,
    since we have
    \[\Cov\left(\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)= C =\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right],\]
    we could derive
    \[D=C^{-1}\Cov\left(\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right) C^{-1}=\left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right]\right)^{-1}.\]
   
    Regarding the asymptotic mean of $\beta_n^{ADRO}$, we have
\begin{equation}\label{h2}H(\beta_\ast)=\tau \left( \frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[\left( e^{\langle\mathbf{X},\beta_\ast\rangle}-Y\right)^2\right]}-\Vert\beta_\ast\Vert_2\frac{\mathbb{E}_{P_{\ast}}\left[ (e^{\langle \mathbf{X},\beta_\ast\rangle}-Y)e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]}{\sqrt{\mathbb{E}_{P_{\ast}}\left[\left( e^{\langle\mathbf{X},\beta_\ast\rangle}-Y\right)^2\right]}}\right),\end{equation}
where $Y$ conditional on $\mathbf{X}$ follows the Poisson distribution with parameter $e^{\langle \mathbf{X},\beta_\ast\rangle}$. Hence, for the second term, we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}}\left[ (e^{\langle \mathbf{X},\beta_\ast\rangle}-Y)e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ (e^{\langle \mathbf{X},\beta_\ast\rangle}-Y)e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\big\vert\mathbf{X}\right]\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle \mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]-\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ Ye^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\big\vert\mathbf{X}\right]\right] \\
&=0,
\end{aligned}
\end{equation*}
which indicates that the equation \eqref{sufficient2eq} holds and the second term in \eqref{h2} equals to $0$.

Further, we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}}\left[\left( e^{\langle\mathbf{X},\beta_\ast\rangle}-Y\right)^2\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_\ast\rangle}-2Ye^{\langle\mathbf{X},\beta_\ast\rangle}+Y^2\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_\ast\rangle}\right]-2\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ Ye^{\langle \mathbf{X},\beta_\ast\rangle} |\mathbf{X}\right]\right]+\mathbb{E}_{P_{\ast}}\left[ Y^2\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_\ast\rangle}\right]-2\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_{\ast}\rangle}\right]+\mathbb{E}_{P_{\ast}}\left[ e^{\langle \mathbf{X},\beta_\ast\rangle}+e^{2\langle \mathbf{X},\beta_\ast\rangle}\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{\langle\mathbf{X},\beta_\ast\rangle}\right]
\end{aligned}
\end{equation*}

 Hence, we have
\[H(\beta_{\ast})=\tau\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_\ast}[e^{\langle \mathbf{X},\beta_{\ast}\rangle}]}.\]
\end{proof}
\subsection{Proof of Theorem \ref{lineartheorem}}\label{prooflinear}
\begin{proof}
Regarding the asymptotic covariance matrix,
    since we have
    \[\Cov\left(\frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)=\sigma^2\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}\right],\]
      \[C=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}\right],\]
    we could derive 
    \[D=C^{-1}\Cov\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right) C^{-1}=\sigma^2\left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{T}\right]\right)^{-1}.\]
  
    Regarding the asymptotic mean of $\beta_n^{ADRO}$, we have 
    \begin{equation}\label{h3}H(\beta_{\ast})=\tau\left(\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]}-\frac{\Vert\beta_{\ast}\Vert_2 \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\right]}{\sqrt{\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]}}\right).\end{equation}
    
    Notice that, for the second term, we have 
\begin{equation*} \begin{aligned}
    \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\right]&= \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\vert\mathbf{X}\right]\\&=\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle\vert\mathbf{X}\right]-\mathbb{E}_{P_\ast}\left[Y\mathbf{X}\vert\mathbf{X}\right].\end{aligned}\end{equation*}

Since $Y$ conditional on $\mathbf{X}$ follows the normal distribution with the mean value $\langle\mathbf{X},\beta_\ast\rangle$, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\right]=\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle\vert\mathbf{X}\right]-\mathbb{E}_{P_\ast}\left[\langle\mathbf{X},\beta_\ast\rangle\mathbf{X}\vert\mathbf{X}\right]=0,\]
which indicates that the equation \eqref{sufficient2eq} holds and the second term in \eqref{h3} equals to $0$.


Then, we obtain that 
    \[H(\beta_{\ast})=\tau\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]}.\]
  
Notice that we also have
\begin{equation*}
\begin{aligned}\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[\left( \langle \mathbf{X},\beta_\ast\rangle-Y\right)^2\bigg\vert \mathbf{X}\right]\right]\\
&= \mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \langle \mathbf{X},\beta_\ast\rangle^2-2\langle \mathbf{X},\beta_\ast\rangle Y+Y^2\bigg\vert \mathbf{X}\right]\right]      \\
&=\mathbb{E}_{P_\ast}\left[\langle \mathbf{X},\beta_\ast\rangle^2\right]-2 \mathbb{E}_{P_\ast}\left[\langle \mathbf{X},\beta_\ast\rangle Y\vert \mathbf{X}\right]+ \mathbb{E}_{P_\ast}\left[ Y^2\vert \mathbf{X}\right]\\
&=\Var(Y\vert\mathbf{X})=\sigma^2.
\end{aligned}
\end{equation*} 

Thus, we can have that 
\[H(\beta_{\ast})=\tau\sigma\frac{\beta_{\ast}}{\Vert\beta_\ast\Vert_2}.\]
\end{proof}
\subsection{Proof of Proposition \ref{exist}}
\begin{proof}
From Definition \ref{def}, we have 
\[\Vert\beta_n^{DRO}-K_n(\beta_n^{DRO})\Vert_2\leq \frac{1}{\sqrt{n}} \Vert C(\beta_n^{DRO})^{-1}H(\beta_n^{DRO})\Vert_2.\]
Because $\beta_n^{DRO}\in B$ and $B$ is a compact set, we know $\Vert C(\beta_n^{DRO})^{-1}H(\beta_n^{DRO})\Vert_2$ is upper bounded. W.O.L.G, there is a neighborhood $\mathcal{B}_1(K_n(\beta_n^{DRO}))\subset B$ of $K_n(\beta_n^{DRO})$ such that $\beta_n^{DRO}\in \mathcal{B}_1(K_n(\beta_n^{DRO}))$. We also notice that $K_n(\cdot)$ is continuously differentiable. Thus, there is a neighborhood $\mathcal{B}(\beta_n^{DRO})\subset B$ of $\beta_n^{DRO}$ satisfying that 
 $\beta_n^{DRO}\in K_n\left(\mathcal{B}(\beta_n^{DRO})\right)$.

Notably, Theorem \ref{logistic}-\ref{lineartheorem} illustrate that the matrix $C$ defined in  \eqref{H1}-\eqref{H3} is invertible, thereby the function 
$K_n(\cdot)$ is well-defined.
Further, we have that $\vert\lambda_{max}\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right)\vert$ is upper bounded by $\Vert\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right)\Vert_2$. Since $l_2$ matrix norm is a continuous function and the neighborhood $\mathcal{B}(\beta_n^{DRO})\subset B$ is bounded, we can conclude that $\vert\lambda_{max}\left( \nabla_{\mathbf{z}} (C(\mathbf{z})^{-1}H(\mathbf{z}))\right)\vert$ is upper bounded. We denote the upper bound by $M$. According to Theorem \ref{multi2}, we can conclude that $K_n(\cdot)$ is locally invertible for each $n>M^2$.
\end{proof}
\section{Computation of the adjusted WDRO estimator in the special case}\label{computationspecial}
Recall that, under Proposition \ref{sufficient} , the adjusted WDRO estimator $\beta_n^{ADRO}$ is defined as 
\[\beta_n^{ADRO}=\frac{\beta_n^{DRO}}{\Vert\beta_n^{DRO}\Vert_2}{F_{\beta_n^{DRO},n}}^{-1}(\Vert\beta_n^{DRO}\Vert_2).\]

We should first compute the WDRO estimator $\beta_n^{DRO}$.
 Afterward, we investigate how to compute the value of $F_{\beta_n^{DRO},n}^{-1}(\Vert\beta_n^{DRO}\Vert_2)$.
Recall the definition of $F_{\theta,n}(x)$, we have the following expression, 
\begin{equation}\label{exact}F_{\beta_n^{DRO},n}(x)=x-\frac{\tau}{\sqrt{n}}\sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\Bigg\vert_{\beta=x\frac{\beta_n^{DRO}}{\Vert \beta_n^{DRO}\Vert_2}}\right]}.\end{equation}


The value of $F_{\beta_n^{DRO},n}(x)$ can be approximated nonparametrically.
For instance, if we have observations $(\mathbf{X}_1,Y_1), \cdots,(\mathbf{X}_N,Y_N)$, we approximate the value $F_{\beta_n^{DRO},n}(x)$ as follows,
\begin{equation}\label{approx}\widehat{F}_{\beta_n^{DRO},n}(x)=x-\frac{\tau}{\sqrt{n}}\sqrt{\frac{1}{N}\sum_{j=1}^{N}\left( \frac{\partial L(f(\mathbf{X}_j,\beta),Y_j)}{\partial f}\right)^2\Bigg\vert_{\beta=x\frac{\beta_n^{DRO}}{\Vert \beta_n^{DRO}\Vert_2}}},\end{equation} 
where we denote the approximation result above by $\widehat{F}_{\beta_n^{DRO},n}(x)$.

Because there is no closed-form expression of the function ${F_{\beta_n^{DRO},n}}^{-1}(\cdot)$,  the numerical approximation of the value of ${F_{\beta_n^{DRO},n}}^{-1}(\Vert\beta_n^{DRO}\Vert_2)$ is further needed. Notice that if we denote ${F_{\beta_n^{DRO},n}}^{-1}(\Vert\beta_n^{DRO}\Vert_2)$ by $y$, we have that
\begin{equation}\label{root} F_{\beta_n^{DRO},n}(y)-\Vert\beta_n^{DRO}\Vert_2=0,\end{equation}
which implies that  ${F_{\beta_n^{DRO},n}}^{-1}(\Vert\beta_n^{DRO}\Vert_2)$ equals to the root of the equation \eqref{root} where $y$ is the unknown variable. 
The invertibility and monotonicity of $F_{\beta_n^{DRO},n}(\cdot)$ implies there is a unique root for the equation \eqref{root} crossing the real line, so we can apply the bisection method to approximate $y$ under a certain accuracy level.
Notably, the function $F_{\beta_n^{DRO},n}(\cdot)$ approaches $f(x)=x$ as $n$ grows and is strictly less than $f(x)=x$, thereby we only need to search the values of $y$ which are close to but larger than $\Vert\beta_n^{DRO}\Vert_2$. 
\newpage
\bibliography{sample}
\end{document}
