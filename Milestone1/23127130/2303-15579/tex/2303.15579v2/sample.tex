\documentclass[twoside,11pt]{article}



% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
\usepackage[preprint]{jmlr2e}

%\usepackage{jmlr2e}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{caption}
\usepackage{amsmath}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e}
\usepackage{subfigure}
% Definitions of handy macros can go here
\newtheorem{assumption}[theorem]{Assumption}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{00}{2023}{1-\pageref{LastPage}}{01/24; Revised 00/00}{00/00}{00-0000}{Yiling Xie and Xiaoming Huo}

% Short headings should be running head and authors last names

\ShortHeadings{Adjusted Wasserstein Distributionally Robust Estimator}{Xie and Huo}
\firstpageno{1}

\begin{document}

\title{Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning}


       
\author{\name Yiling Xie \email yxie350@gatech.edu\\
\name Xiaoming Huo \email huo@gatech.edu \\
\addr School of Industrial and Systems Engineering\\
 Georgia Institute of Technology\\
Atlanta, Georgia, USA}
\editor{Po-Ling Loh}

\maketitle

\begin{abstract}%
We propose an {\it adjusted} Wasserstein distributionally robust estimator---based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning.
The classic  WDRO estimator is asymptotically biased, while our adjusted WDRO estimator is asymptotically \textit{unbiased}, resulting in a smaller asymptotic mean squared error.
Meanwhile, the proposed adjusted WDRO  has an out-of-sample performance guarantee.
Further, under certain conditions, our proposed adjustment technique provides a general principle to de-bias asymptotically biased estimators. Specifically, we will investigate how the adjusted WDRO estimator is developed in the generalized linear model, including logistic regression, linear regression, and Poisson regression.
Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
\end{abstract}

\begin{keywords}
  distributionally robust optimization; asymptotic normality; Wasserstein distance; unbiased estimator; generalized linear model
\end{keywords}

\section{Introduction}
% \textcolor{cyan}{To make the story simpler: The asymptotic distribution of the WDRO estimator is not optimal according to the asymptotic distribution. There is a map from $\beta_{\ast}$ to $\Tilde{\beta}$. If this mapping is invertible, we could derive an unbiased estimator. Under condition?}
 
  Wasserstein distributionally robust optimization (WDRO) has appeared as a promising tool to achieve ``robust" decision-making \citep{ mohajerin2018data,blanchet2019quantifying,gao2022distributionally}. 
WDRO has attracted intense research interest in the past few years. 
It is well-known that WDRO admits tractable reformulations \citep{mohajerin2018data} 
and has a powerful out-of-sample performance guarantee \citep{gao2022finite}.
People also have been actively exploring its applications in financial portfolio selection \citep{blanchet2022distributionally}, statistical learning \citep{chen2018robust,shafieezadeh2019regularization}, neural networks \citep{sinha2018certifying}, automatic control \citep{yang2020wasserstein}, transportation \citep{carlsson2018wasserstein}, and energy systems \citep{wang2018risk}, among others.


WDRO can be applied in statistical learning \citep{chen2018robust,kuhn2019wasserstein,nguyen2022distributionally}.
In general, the statistical learning model can be written as the following optimization problem:
\[\min_{\beta\in B} \mathbb{E}_{P_{\ast}} \left[L(f(\mathbf{X},\beta),Y)\right],\]
where $\mathbf{X}\in\Omega\subset\mathbb{R}^d$ denotes the feature variable, $Y$ denotes the label variable, $P_{\ast}$ is the underlying data-generating distribution of $(\mathbf{X},Y)$, $f(\cdot,\beta)$ is the hypothesis function parameterized by $\beta\in B\subset\mathbb{R}^d$, $B$ is a compact convex set, and $L(\cdot,\cdot)$ is the loss function.
Considering the true data-generating distribution $P_{\ast}$ is usually unknown,  the empirical risk minimization can be applied to estimate the ground-truth hypothesis function $f(\cdot,\beta_\ast)$ parameterized by $\beta_{\ast}\not=0$. 
However, the empirical risk minimization estimators are sensitive to perturbations and suffer from overfitting \citep{smith2006optimizer,shalev2014understanding}.
To obtain robust estimators with desirable generalization abilities, distributionally robust optimization is proposed, which minimizes the worst-case expected loss among an ambiguity set $\mathcal{U}$ of distributions to hedge against data perturbation. 
In this paper, we are interested in the Wasserstein ambiguity set, and then the resulting problem is the so-called  Wasserstein distributionally robust optimization.
 The Wasserstein ambiguity is defined as the ball centered at the empirical distribution $\mathbb{P}_N$ and contains all distributions close to $\mathbb{P}_N$ in the sense of the Wasserstein distance. 
 We denote the WDRO estimators---the solutions to the WDRO problem---by $\beta_n^{DRO}$.
 More details will be stated in Section \ref{adro}.

{
The asymptotic distribution of the WDRO estimator $\beta_n^{DRO}$ can be obtained under certain regularity conditions.
In particular, the convergence in distribution implies that the WDRO estimator $\beta^{DRO}_n$ has an asymptotic bias, which may result in inaccurate estimation of the ground-truth parameter $\beta_{\ast}$.
Inspired by this phenomenon, we provide a general adjustment technique to de-bias the asymptotically biased estimators, where we also discuss the asymptotic behavior of different transformations on the estimators.

Applying the proposed adjustment technique to the WDRO problem, we obtain the adjusted WDRO estimator, denoted by $\beta_n^{ADRO}$.
It will be shown that the adjusted WDRO estimator $\beta_n^{ADRO}$ could be computed exactly simply using the given samples and the value of the classic WDRO estimator $\beta_n^{DRO}$, making it convenient to apply the proposed technique.
Also, the existence and the asymptotic unbiasedness of the adjusted WDRO estimator $\beta_n^{ADRO}$ could be promised under mild conditions, enabling broad applications of the proposed technique.
In addition, the out-of-sample performance guarantee of the proposed estimator $\beta^{ADRO}_n$ can be derived, demonstrating that the proposed estimator $\beta^{ADRO}_n$ inherits the generalization capacity of the WDRO estimator. 
}

Since the generalized linear model includes multiple widely-used regression models and is easy to interpret and implement, we will articulate how to apply the adjustment strategy in the setting of the generalized linear model, including linear regression, logistic regression, and Poisson regression.
Then, we carry out the numerical experiments in the generalized linear model.
Our numerical experiments illustrate that the proposed estimator $\beta_n^{ADRO}$ has a superior performance even if the sample size is relatively small.


% The logistic regression, the Poisson regression, and the linear regression serve as examples to show how the requirements for applying the adjusted WDRO are satisfied by the generalized linear model.

%\subsection{Related work}
%\subsection{Organization of this paper}
% The remainder of this paper is organized as follows. 
% In Section \ref{section2}, we derive the asymptotic distribution of $\beta^{DRO}_n$ in the logistic regression.
% In Section \ref{adro}, we introduce the adjusted WDRO estimator formally.
% In Section \ref{property}, we discuss the properties of adjusted WDRO estimator.
% In Section \ref{section5}, we explain how to define the adjusted WDRO in more general setting.
% In Section \ref{section6}, we extend the adjustment technique from the logistic regression to generalized linear models.
% Numerical experiments are conducted in Section \ref{exp}.
\subsection{Related Work}
We review the existing work related to the proposed adjusted WDRO estimator.
WDRO is broadly applied to solve parameter estimation problems \citep{shafieezadeh2015distributionally, kuhn2019wasserstein,aolaritei2022performance,nguyen2022distributionally}. 
Multiple algorithms have been developed \citep{luo2019decomposition,li2019first,blanchet2022optimal} and can be applied to compute the parameter estimators in the WDRO framework.
While intense work focuses on adapting WDRO to different machine learning problems, deriving the tractable reformulations, and solving the WDRO problems efficiently, the statistical properties of WDRO estimators have been investigated in recent few years, e.g.,   \cite{blanchet2022confidence,blanchet2021statistical}, evaluating the behavior of WDRO through the lens of statistics.
In the aforementioned paper, the asymptotic distribution of the WDRO estimator is proven to be normal, and the confidence region is proposed based on the corresponding asymptotic results.
While they focus on unsupervised settings, we extend the results to supervised statistical learning, especially in the setting of the generalized linear model.
Notably, the asymptotic biasedness of the WDRO estimator has been mentioned in \cite{blanchet2022confidence}.
However, we are the first to propose a nonlinear transformation to overcome this shortcoming.
Furthermore, the proposed estimator is more accurate in the asymptotic sense because the estimator also has an asymptotically smaller mean squared error.
In addition, the generalization bounds, i.e., the upper confidence bounds on the out-of-sample error, have been established to guarantee the out-of-sample performance of the WDRO estimator \citep{mohajerin2018data,shafieezadeh2019regularization,gao2022finite,wu2022generalization}.
Since the proposed adjusted WDRO estimator is transformed from the classic WDRO estimator, we can also develop the generalization bounds for the associated adjusted WDRO estimator. 
\subsection{Organization of this Paper}
The remainder of this paper is organized as follows.
In Section \ref{adjust}, we introduce the adjustment technique that could de-bias the general asymptotically biased estimators under certain conditions.
In Section \ref{basic}, we discuss the asymptotic behavior of the WDRO problem.
In Section \ref{adro}, we give the formulation of the adjusted WDRO estimator in statistical learning.
In Section \ref{glmdro}, we show how to develop the adjusted WDRO estimators in the generalized linear model.
Numerical experiments are conducted and analyzed in Section \ref{exp}.
The proofs are relegated to the appendix whenever possible.

{
\section{Adjustment Technique}\label{adjust}
In this section, we first discuss the properties of transformations on the asymptotically biased estimators, based on which we provide a general strategy to de-bias the asymptotically biased estimators under certain conditions. The proposed adjustment technique will be further illustrated in detail in the WDRO setting in Section \ref{adro}.

Suppose the estimator $\beta_n\in\mathbb{R}^d$ is obtained by the following parameter-estimation procedure 
\[\beta_n\in\arg\min_{\beta} l(\mathbb{P}_n,\beta),\]
where $l$ is the loss and depends on the empirical distribution $\mathbb{P}_n$ and parameter $\beta$.
Also, suppose that the estimator $\beta_n$  has the following convergence in distribution:
\begin{equation}\label{general}\sqrt{n}(\beta_n-\beta_\ast)\Rightarrow\mathcal{N}(f(\beta_\ast),D),\end{equation}
where $\Rightarrow$ means ``converge in distribution'',  $D\in\mathbb{R}^{d\times d}$, $f:\mathbb{R}^d\mapsto\mathbb{R}^d$ and $\beta_\ast\in\mathbb{R}^d$ is the ground-truth parameter. We focus on the scenario when $f\not=0$.

For the estimator $\beta_n$ with the limiting distribution in \eqref{general}, our goal is to look for some transformation $\phi_n(\cdot)$ to obtain a more accurate estimation of $\beta_\ast$ in the asymptotic sense. 
The following proposition states that the ``best'' transformations have a unique formulation.
\begin{proposition}\label{uniquetransformation}
Suppose  $\beta_n$ is the estimator of $\beta_\ast$ and has the following convergence in distribution:
\[\sqrt{n}(\beta_n-\beta_\ast)\Rightarrow\mathcal{N}(f(\beta_\ast),D).\]
Assume the transformation $\phi_n(\cdot)$ is differentiable satisfying $\phi_n^{\prime}(\beta_\ast)\to \phi^{\prime}(\beta_\ast)$, where $\phi$ is differentiable, and $\phi_n^{\prime}$ and $\phi^\prime$ are the gradients of $\phi_n$ and $\phi$.
The least asymptotic mean squared error of $\phi_n(\beta_n)$ is $\tr(D)$, which is obtained if and only if $\phi_n(\cdot)$ could be written as
\begin{equation}\label{best}
\phi_n(\beta)=\beta-\frac{1}{\sqrt{n}}f(\beta)+o\left(\frac{1}{\sqrt{n}}\right).
\end{equation}
In this way, we have that 
\[\sqrt{n}(\phi_n(\beta_n)-\beta_\ast)\Rightarrow\mathcal{N}(0,D).\]
\end{proposition}
Proposition \ref{uniquetransformation} demonstrates that for the asymptotically biased estimator $\beta_n$, to achieve the least asymptotic mean squared error $\tr(D)$, the transformation $\phi_n(\cdot)$ must take the formulation \eqref{best}. 
Meanwhile, the resulting estimator $\phi_n(\beta_n)$ is asymptotically unbiased. 

The transformation $\phi_n(\cdot)$ in the formulation \eqref{best} is desirable since it could de-bias the asymptotically biased estimator $\beta_n$ and achieve the least asymptotic mean squared error $\tr(D)$.
However, the transformation $\phi_n(\cdot)$ depends on the function $f$, which is usually unknown. 
For example, in the limiting distribution of the WDRO estimator, $f$ depends on the unknown underlying distribution. 
In this regard, the function $f$ should be approximated accordingly.

Suppose we have a sequence of functions $f_n$ to approximate the function $f$. Our adjustment transformation could be defined in terms of $f_n$ based on $\phi_n(\cdot)$ in \eqref{best}.
We assume $f_n$ could be computed via the empirical distribution $\mathbb{P}_n$, and certain conditions should be imposed to $f_n$ to promise that the estimator obtained by our adjustment transformation is asymptotically unbiased and could have the mean squared error $\tr(D)$.
More details are described in Assumption \ref{assumefn} and Theorem \ref{generaltheorem}.

Before introducing Theorem \ref{generaltheorem}, we state our assumptions of functions $f_n$.
\begin{assumption}\label{assumefn}
Given function $f$, $f_n$ and $\beta_\ast$, we assume that 
    \begin{itemize}
    \item The function $f_n$ is differentiable at $\mathcal{B}(\beta_\ast)$, where $\mathcal{B}(\beta_\ast)$ is some neighborhood of $\beta_\ast$.
    \item The sequence $\sup_{\beta\in \mathcal{B}(\beta_\ast)}\Vert f^\prime_n(\beta)\Vert$ is bounded in probability.
    \item $f_n(\beta_\ast)\to_p f(\beta_\ast)$, where $\to_p$ means ``converge in probability''.
\end{itemize}
\end{assumption}
Equipped with Assumption \ref{assumefn}, we give our main theorem in the following.
\begin{theorem}[Adjustement Technique]\label{generaltheorem}
Suppose  $\beta_n$ is the estimator of $\beta_\ast$ and has the following convergence in distribution:
\[\sqrt{n}(\beta_n-\beta_\ast)\Rightarrow\mathcal{N}(f(\beta_\ast),D).\]
If we have  function $f_n$ satisfying 
Assumption \ref{assumefn} and the transformation $\mathcal{A}_n(\cdot)$ defined by
\[\mathcal{A}_n(\beta_n)=\beta_n-\frac{1}{\sqrt{n}}f_n(\beta_n),\]
then \begin{equation}\label{generaladjusted}\sqrt{n}\left(\mathcal{A}_n(\beta_n)-\beta_\ast\right)\Rightarrow\mathcal{N}(0,D).\end{equation}
\end{theorem}
The convergence \eqref{generaladjusted} in Theorem \ref{generaltheorem} demonstrates that the proposed adjusted estimator $\mathcal{A}_n(\beta_n)$ is asymptotically unbiased and the asymptotic covariance matrix remains unchanged, resulting in a smaller asymptotic means square error $\tr(D)$, which equals the least mean squared error stated in Proposition \ref{uniquetransformation}.
In this regard, to de-bias the asymptotically biased estimators, one only needs to have a sequence of functions $f_n$ satisfying Assumption \ref{assumefn}.
\subsection{Sequential Delta Method}
Notice that the transformations $\phi_n$ discussed in Proposition \ref{uniquetransformation} depend on $n$. In this way, when we discuss the asymptotic distribution of $\phi_n(\beta_n)$, the classic delta method is not applicable. To resolve this issue, we have developed a sequential delta method based on the extended continuous mapping theorem (Theorem 1.11.1 in \cite{van1996weak}). The sequential delta method may have an independent research interest, so we state it in the following theorem.
\begin{theorem}[Sequential Delta Method]\label{extendeddeltamethod}
    Let $\phi_n$ and $\phi:\mathbb{D}\subset\mathbb{R}^d\mapsto \mathbb{R}^d$ be functions defined on a subset of $\mathbb{R}^d$, differentiable at $\theta$,  satisfying $\phi^\prime_{n}(\theta)\to \phi^\prime(\theta)$,
     where $\phi^\prime$ and $\phi^\prime_n$ are gradients of the functions $\phi$ and $\phi_n$.
    Let $T_n$ be random vectors taking their values in the domain of $\phi$. 
    If $r_n(T_n-\theta)\Rightarrow \mathcal{N}(\mu,\Sigma)$ for numbers $r_n\to\infty$, then  $r_n(\phi_n(T_n)-\phi_n(\theta))\Rightarrow \mathcal{N}(\phi^\prime(\theta) \mu ,\phi^\prime(\theta)\Sigma{\phi^\prime(\theta)}^\top)$.
\end{theorem}}
\section{WDRO Problem}\label{basic}
This section discusses the problem formulation of WDRO and gives the asymptotic distribution of the WDRO estimator.
\subsection{Problem Formulation}
The WDRO problem can be written as
\begin{equation}\label{formal}\beta^{DRO}_n\in\arg\min_{\beta\in B}\sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta),Y)\right],\end{equation}
 where the feature variable $\mathbf{X}$ belongs to $\Omega\subset \mathbb{R}^d$, the label variable $Y$ can be continuous or discrete, $f$ is the hypothesis function parametrized by $\beta\in B\subset\mathbb{R}^d$, $B$ is a compact convex set, $\mathcal{U}_{\rho_n}(\mathbb{P}_n)$ is the Wasserstein uncertainty set, and $L(\cdot,\cdot)$ is the loss function. The Wasserstein uncertainty set is defined by
 \begin{equation}\label{uncertainty}\mathcal{U}_{\rho_n}(\mathbb{P}_N)=\{P: W_p(P,\mathbb{P}_N)\leq \rho_n\},\end{equation}
where $\mathbb{P}_N$ is the empirical distribution of the samples $\{(\mathbf{X}_1,Y_1), (\mathbf{X}_2,Y_2),..., (\mathbf{X}_n,Y_n)\}$ generated by true data-generating distribution $P_{\ast}$,
\[W_p(P,\mathbb{P}_N)=\left(\inf_{\gamma\in\Gamma(P,\mathbb{P}_n)}\left\{\int_{Z^2} d^p(z,z^{\prime}) d\gamma(z,z^{\prime})\right\}\right)^{1/p},\]
$\Gamma(P,\mathbb{P}_n)$ is the set of  distributions with marginals $P$  and  $\mathbb{P}_N$,
$d$ is some metric in space $Z=\mathbf{X}\times Y$,
and $W_p(P_1,P_2)$ is the so-called $p$-Wasserstein distance.

\subsection{Asymptotic Distribution of the WDRO Estimator}\label{section2.1}
In this subsection, we extend the asymptotic distribution of the WDRO estimator to the supervised statistical learning setting.

\cite{blanchet2022confidence} have derived the asymptotic distribution of the WDRO estimator in the unsupervised learning setting. 
In our study, however, we first let the cost function be infinite if the label variables are different and then adapt the asymptotic distribution of the WDRO estimator to the supervised statistical learning setting.

To adapt the results, we should specify the hyperparameters of the Wasserstein uncertainty set and clarify some regularity conditions, which should be satisfied for the loss function $L(\cdot,\cdot)$ and the underlying data-generating distribution $P_{\ast}$ of $(\mathbf{X},Y)$.
\begin{assumption}\label{assume1}
The hyperparameters of the Wasserstein uncertainty set $\mathcal{U}_{\rho_n}(\mathbb{P}_n)$ \eqref{uncertainty} are prescribed as follows.
\begin{itemize}
\item $\rho_n=\tau/\sqrt{n}$, $\tau>0$,
\item $p=2$,
\item $d\left((\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2)\right)=
\begin{cases}
\Vert\mathbf{x}_1-\mathbf{x}_2\Vert_2&y_1=y_2\\
\infty&y_1\not=y_2
\end{cases}$.
\end{itemize}
\end{assumption}  
\begin{remark}
We justify the choices of hyperparameter in Assumption \ref{assume1} as follows.
\begin{itemize}
    \item 
We choose the radius to be of the square-root order $\mathcal{O}(1/\sqrt{n})$ because the out-of-sample performance guarantee without the curse of dimensionality can be proved \citep{gao2022distributionally}, and a confidence region for the ground-truth parameter can be constructed with the square-root order \citep{blanchet2022confidence}.
\item
We choose the 2-Wasserstein distance since the 2-Wasserstein distance applies to the quadratic loss, and the associated WDRO problem could be solved by iterative algorithms \citep{blanchet2022optimal}.
\item
The cost function is infinite when $y_1\not=y_2$.
 This covers covariate shift in machine learning \citep{aolaritei2022performance} and can be applied to many tasks where the samples are correctly labeled \citep{gao2017distributional}.
 \end{itemize}
\end{remark}
\begin{assumption}\label{assumeloss}
The loss function $L(f(\mathbf{x},\beta),y)$ satisfies:
\begin{itemize}
    \item[a.]  The loss function $L(f(\mathbf{x},\beta),y)$  is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$. 
    \item [b.] For each variable $\mathbf{x}\in\Omega$ and $y$, the loss function $L(f(\mathbf{x},\beta),y)$ is convex w.r.t. $\beta$. 
    \item[c.] For each parameter $\beta\in B$ and variable y, the function $\left\Vert\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \mathbf{x}^2}\right\Vert_2$ is uniformly continuous w.r.t. $\mathbf{x}$ and  uniformly bounded by a continuous function $M(\beta)$.
\end{itemize}
\end{assumption}
\begin{assumption}\label{assumedistr}
The underlying data-generating distribution $P_{\ast}$ of $(\mathbf{X},Y)$ satisfies:
\begin{itemize}
    \item[a.] There exists $\beta_{\ast}\in B^{\circ}$, where $B^{\circ}$ means the interior of $B$, satisfying
    \[\mathbb{E}_{P_{\ast}}\left[\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial\beta}\right]\Bigg\vert_{\beta=\beta_{\ast}}=0,\]
    and the inequalities
    \begin{equation}\label{matrixD}C(\beta_\ast):=\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\beta_{\ast}}\succ 0,\end{equation}
    \[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\right\Vert_2^2\right]\Bigg\vert_{\beta=\beta_{\ast}}<\infty\] 
    hold,  where $C(\beta_\ast)\succ0$ means the matrix $C(\beta_\ast)$ is a positive definite matrix.
    \item[b.] $P_{\ast}$ is non-degenerate in the sense that  
    \[P_{\ast}\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0,\]
    \[\mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{\top}\right]\succ 0,\]
    where $\frac{\partial ^2 L}{\partial \mathbf{x}\partial \beta}$ means taking the gradient first w.r.t. $\beta$ and then w.r.t. $\mathbf{x}$.
\end{itemize}
\end{assumption}

Next, we obtain the associated convergence of the WDRO estimator $\beta_n^{DRO}$ in problem \eqref{formal} under Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr}, which is shown in the following theorem.
\begin{theorem}[Extension of Theorem 1 in \cite{blanchet2022confidence}]
\label{theorem1}
Suppose that Assumption \ref{assume1}, \ref{assumeloss} and \ref{assumedistr} are satisfied, $\Omega=\mathbb{R}^d$ and $\mathbb{E}\left[\Vert\mathbf{X}\Vert_2^2\right]<\infty$, the WDRO estimator $\beta_n^{DRO}$ in problem \eqref{formal} has the following convergence in distribution:
\begin{equation}\label{con}\sqrt{n}(\beta_{n}^{\text{DRO}}-\beta_{\ast})\Rightarrow\mathcal{N}\left(-C(\beta_\ast)^{-1}H\left(\beta_{\ast}\right),D\right),\end{equation}
where 
\begin{equation}\label{orginH}H(\beta_{\ast})=\tau\frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}},\end{equation}
 $\tau$ is the coefficient in the Wasserstein radius $\rho_n=\tau/\sqrt{n}$,
\begin{equation}\label{covariancematrix}D=C(\beta_\ast)^{-1}\Cov\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)C(\beta_\ast)^{-1},\end{equation}
and $C(\beta_\ast)$ is defined in \eqref{matrixD}.
\end{theorem}
\begin{remark}
The assumption $\Omega=\mathbb{R}^d$ could be relaxed. If $\Omega$ is compact and could be expressed as $\Omega=\{\mathbf{x}\in\mathbb{R}^d: A\mathbf{x}\leq b\}$,  where $A$ is an $l\times d$ matrix with linearly independent rows and $b\in\mathbb{R}^l$, and $\mathbf{X}$ has a probability density which is absolutely continuous w.r.t. Lebesgue measure, then the convergence \eqref{con} still holds. This claim can be seen in Section 6 in \cite{blanchet2022confidence}.
\end{remark}
\begin{remark}[Finite Sample Size]
We investigate the distribution of $\beta_n^{DRO}$ when $n$ is not very large. The WDRO esitmator $\beta_n^{DRO}$ is computed in the logistic regression model when $n=200$, and we plot the histograms of $\sqrt{n}(\beta_n^{DRO}-\beta_\ast)$ in Figure \ref{hist}. Two dimensions of  $\beta_n^{DRO}$ are plotted separately.  We conclude from Figure \ref{hist} that $\beta_n^{DRO}$ is approximately normally distributed with a nonzero mean, as asymptotic convergence \eqref{con} suggested. 
We further apply the Shapiroâ€“Wilk test, and the $p$-values associated with two dimensions of $\beta_n^{DRO}$ are $p=0.07963>0.05$ and $p=0.06720>0.05$, respectively. This supports our claim that $\beta_n^{DRO}$ is approximately normally distributed even though the sample size is not very large, indicating that the asymptotic behavior of $\beta_n^{DRO}$ ``comes early''.
Therefore, making the bias in asymptotic convergence \eqref{con} disappear is meaningful in the sense of both asymptotic and finite sample size.
\end{remark}
Theorem \ref{theorem1} indicates that the term $\sqrt{n}(\beta_{n}^{DRO}-\beta_\ast)$ converges in distribution to a normal distribution with nonzero mean $-C(\beta_\ast)^{-1}H(\beta_{\ast})$. 
Recall that we perturb the samples to achieve robustification.
As explained in \cite{blanchet2021statistical}, the bias term $-C(\beta_\ast)^{-1}H(\beta_{\ast})$ could be understood as pushing towards solutions with less variation resulting from data perturbation.
However, this nonzero bias term may imply that the WDRO estimator is not an accurate estimation of the ground-truth parameter $\beta_{\ast}$.
We may consider transforming the WDRO estimator $\beta_n^{DRO}$ to make the bias term disappear using the adjustment technique mentioned in Section \ref{adjust}.
%Further, if the additional variance caused by this transformation could also be controlled, an unbiased robust estimator with a smaller mean squared error may be obtained.
%We will also explore whether this transformation could maintain the robust properties of the WDRO estimator at the same time.


\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{histogramofbeta1.pdf} % first figure itself
        
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{histogramofbeta2.pdf} % second figure itself
    \end{minipage}
    \caption{Histogram of $\beta_n^{DRO}$}
    \label{hist}
\end{figure}

{
\section{Proposed Adjusted WDRO Estimator}\label{adro}
This section introduces the formal formulation of our adjusted WDRO estimator and investigates the relevant properties, e.g., unbiasedness.

The adjusted WDRO estimator is based on the asymptotic distribution obtained in Section \ref{section2.1} and the adjustment technique introduced in Section \ref{adjust}.
Recall the WDRO estimator has the following convergence:
\[\sqrt{n}(\beta_{n}^{\text{DRO}}-\beta_{\ast})\Rightarrow\mathcal{N}\left(-C(\beta_\ast)^{-1}H\left(\beta_{\ast}\right),D\right),\]
where 
\[C(\beta_\ast)=\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\beta_{\ast}}, \quad H(\beta_{\ast})=\tau\frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}.\]

Notice that the asymptotic bias $f(\beta_\ast)=-C(\beta_\ast)^{-1}H\left(\beta_{\ast}\right)$ depends on the underlying distribution $P_\ast$, then we use the associated empirical distribution to approximate $f$. Applying the adjusted technique proposed in Theorem \ref{generaltheorem}, we define the adjusted WDRO estimator in the following.
\begin{definition}[Adjusted WDRO Estimator]\label{def}
In the WDRO problem \eqref{formal}, under Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr}, the adjusted WDRO estimator is defined by
\begin{equation}\label{adrodefinition}\beta_n^{ADRO}=\mathcal{A}_n(\beta_n^{DRO}),\end{equation}
where
\[\mathcal{A}_n(\mathbf{z})=\mathbf{z}+\frac{C_n(\mathbf{z})^{-1}H_n(\mathbf{z})}{\sqrt{n}},\]
\begin{equation}\label{functionH}H_n(\mathbf{z})=\tau\frac{\partial \sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2 \right]}}{\partial \beta}\Bigg\vert_{\beta=\mathbf{z}},\end{equation}
\begin{equation}\label{functionC}C_n(\mathbf{z})=\mathbb{E}_{\mathbb{P}_n}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\mathbf{z}}.\end{equation}
\end{definition}
To promise the existence of the adjusted WDRO estimator, we need additional conditions to let the matrix $C_n(\beta_n^{DRO})$ be invertible and the vector $H_n(\beta_n^{DRO})$ well-defined. The conditions are shown in the following proposition.
\begin{proposition}[Existence of Adjusted WDRO Estimator I]\label{conditions}
    For the empirical distribution $\mathbb{P}_n$, the loss function $L(f(\mathbf{x},\beta),y)$ and the WDRO estimator $\beta_n^{DRO}$, if
    \[\mathbb{P}_n\left( \left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\Bigg\vert_{ \beta=\beta_n^{DRO}}\not=0\right)>0,  \quad \mathbb{E}_{\mathbb{P}_n}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\beta_n^{DRO}}\succ 0 \]
    hold, then the adjusted WDRO estimator $\beta_n^{ADRO}$ defined in \eqref{adrodefinition} exists. 
\end{proposition}
% One may check that the conditions in Proposition \ref{conditions} could be satisfied by many statistical models, including linear regression, and logistic regression, among many others. We relegate the relevant discussions in Appendix \ref{existence}.
If the hypothesis function is linear, i.e., $f=\langle \mathbf{x},\beta\rangle$, the existence conditions demonstrated in Proposition \ref{conditions} could be further simplified as shown in the following proposition.
\begin{proposition}[Existence of Adjusted WDRO Estimator II]\label{conditions2}
    For the empirical distribution $\mathbb{P}_n$, the loss function $L(\langle\mathbf{x},\beta\rangle,y)$ and the WDRO estimator $\beta_n^{DRO}$, if \[\beta_n^{DRO}\not=0,\quad\frac{\partial^2 L(f,y)}{\partial f^2}\not=0,\quad \mathbb{P}_n\left( \frac{\partial L(\langle\mathbf{X},\beta_n^{DRO}\rangle,Y)}{\partial  f}\not=0\right)>0,\] hold, and there does not exist nonzero vector $\alpha$ such that $\mathbb{P}_n(\alpha^{\top}\mathbf{X}=0)=1$, then the adjusted WDRO estimator $\beta_n^{ADRO}$ defined in \eqref{adrodefinition} exists.
\end{proposition}
The conditions in Proposition \ref{conditions} and \ref{conditions2} are mild. For example, for the nonzero WDRO estimator $\beta_n^{DRO}$ and non-degenerate loss $L$, if the distribution of feature variable $\mathbf{X}$ does not concentrate, the conditions in Proposition \ref{conditions2} can hold without loss of generality.
One may check that the existence conditions could be satisfied by multiple statistical models, including linear regression, and logistic regression, among many others.}
\subsection{Simplication of the Adjusted WDRO Estimator}
We discuss under which conditions the expression of the adjusted WDRO estimator $\beta_n^{ADRO}$ could be further simplified in this subsection.

Recall that, in the definition of the adjusted WDRO estimator (Definition \ref{def}), the term $H(\cdot)$ appears complicated at first glance. The following proposition shows that the function $H(\cdot)$ can be simplified under certain conditions.
\begin{proposition}\label{sufficient2}
    If the hypothesis function in problem \eqref{formal} is a linear function, i.e., $f(\mathbf{x},\beta)=\langle \mathbf{x},\beta\rangle$, and the equation 
    \begin{equation}\label{sufficient2eq}\mathbb{E}_{P_\ast}\left[ \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\frac{\partial L^2(f(\mathbf{X},\beta_\ast),Y)}{\partial f^2}\mathbf{X}\right]=0,\end{equation}
    where $\frac{\partial L}{\partial f}$ means taking the gradient of $L(\cdot,\cdot)$ w.r.t. the first argument,
    holds for the underlying data-generating distribution $P_\ast$ and the loss function $L(\cdot,\cdot)$, then the function $H(\beta_\ast)$ defined in \eqref{orginH} can be rewritten as  
\[H(\beta_\ast)=\tau\frac{\beta_\ast}{{\Vert\beta_\ast\Vert_2}}\sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\right]}\Bigg\vert_{\beta=\beta_\ast}.\] 
\end{proposition}

Proposition \ref{sufficient2} implies that the linearity of the hypothesis function and the equation \eqref{sufficient2eq} can promise $H(\beta_\ast)$ could be written as a rescaling of $\beta_\ast$. The associated function $H_n(\mathbf{z})$ is defined by
\[H_n(\mathbf{z})=\tau\frac{\beta_\ast}{{\Vert\beta_\ast\Vert_2}}\sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\right]}\Bigg\vert_{\beta=\mathbf{z}}.\] 
In this way, the expression of the adjusted WDRO estimator could be simplified. 
In particular, the conditions in Proposition \ref{sufficient2} can be satisfied by multiple statistical models, e.g., linear regression, logistic regression, and Poisson regression. The details can be found in Section \ref{glmdro}.
\subsection{Asymptotically Unbiased}
{We establish the asymptotic distribution of the adjusted WDRO estimator $\beta_n^{ADRO}$.
\begin{theorem}\label{theorem2}
Under Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr}, if the adjusted WDRO estimator $\beta_n^{ADRO}$ defined in \eqref{adrodefinition} exists,
$\frac{\partial L(f(\mathbf{x},\beta),y)}{\partial x\partial \beta}$ and $\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \beta^2}$ are continuously differentiable w.r.t $\beta$ at some neighborhood of $\beta_\ast$,  and the gradient of $C_n(\mathbf{z})^{-1}H_n(\mathbf{z})$ at $\beta_\ast$ is bounded in probability, then $\beta_n^{ADRO}$ converges in distribution:
\begin{equation*}\sqrt{n}(\beta_{n}^{ADRO}-\beta_{\ast})\Rightarrow\mathcal{N}(0,D),\end{equation*} 
where $D$ is defined in \eqref{covariancematrix}.
\end{theorem}
Theorem \ref{theorem2} indicates that our proposed estimator $\beta^{ADRO}_n$ is asymptotically unbiased
and the asymptotic mean squared error is $\tr(D)$.
Recall the asymptotic distribution of the classic WDRO estimator $\beta_n^{DRO}$ is 
\[\sqrt{n}(\beta_{n}^{\text{DRO}}-\beta_{\ast})\Rightarrow\mathcal{N}\left(-C(\beta_\ast)^{-1}H\left(\beta_{\ast}\right),D\right),\]
indicating that the asymptotic mean squared error of the classic WDRO estimator $\beta_n^{DRO}$ is $\tr(D)+f(\beta_\ast)^\top f(\beta_\ast)$, where $f(\beta_\ast) = -C(\beta_\ast)^{-1}H(\beta_\ast)$. In this way, our proposed estimator has a smaller asymptotic mean squared error.
}
\subsection{Out-of-sample Performance Guarantee}
This subsection discusses the out-of-sample performance guarantee for the adjusted WDRO estimator $\beta_n^{ADRO}$.

% The out-of-sample performance guarantee means that the ground-truth out-of-sample error of the WDRO estimator has an upper bound, which is the so-called generalization bound.
Informally, the out-of-sample performance guarantee for the WDRO estimator $\beta_n^{DRO}$ reads that, with high probability, the following inequality holds,
\begin{equation}\label{outofsample}
\mathbb{E}_{P_{\ast}}\left[ L(f(\mathbf{X},\beta_n^{DRO}),Y)\right]\leq\sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta_n^{DRO}),Y)\right]+\epsilon_n, \end{equation}
where the left-hand side is the generalization error of $\beta_n^{DRO}$, and the first term on the right-hand side is called Wasserstein robust loss of $\beta_n^{DRO}$. Inequality \eqref{outofsample} implies that the ground-truth error of $\beta_n^{DRO}$ is upper bounded by the Wasserstein robust loss up to a higher order residual $\epsilon_n$.

Recall that our proposed adjusted estimator $\beta_n^{ADRO}$ is transformed from the WDRO estimator $\beta_n^{DRO}$. As the  WDRO estimator $\beta_n^{DRO}$ enjoys the out-of-sample performance guarantee \eqref{outofsample}, similar arguments can be established towards the adjusted WDRO estimator $\beta_n^{ADRO}$. 
\begin{corollary}[Performance Guarantee]\label{coro1}

Suppose the generalization bound \eqref{outofsample} holds for the WDRO estimator $\beta_n^{DRO}$ for some residual term $\epsilon_n$ with probability $1-\alpha$.
If the loss function $L(f(\mathbf{x},\beta),y)$ is $h$-Lipschitz continuous w.r.t. $\beta$ and the adjusted WDRO estimator $\beta_n^{ADRO}$ exists, then the following inequality,
\[\mathbb{E}_{P_{\ast}}\left[ L(f(\mathbf{X},\beta_n^{ADRO}),Y)\right]\leq\sup_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P} \left[L(f(\mathbf{X},\beta_n^{ADRO}),Y)\right]+ 2h \Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2 +\epsilon_n, \]
holds with probability $1-\alpha$.
\end{corollary}

{
From the definition of the adjusted WDRO estimator $\beta_n^{ADRO}$, we know that the term $\Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2$ is of order $\mathcal{O}(1/\sqrt{n})$. 
 In addition, \cite{gao2022finite} derives the generalization bound based on a novel variance-based concentration inequality for the empirical loss for the radius of the order $\mathcal{O}(1/\sqrt{n})$, where $\epsilon_n=\widetilde{\mathcal{O}}(1/n)$. In this sense, the generalization error of the adjusted WDRO estimator $\beta_n^{ADRO}$ can be upper bounded by the Wasserstein robust loss of the adjusted WDRO estimator $\beta_n^{ADRO}$ up to a new residual term, $2h \Vert\beta_n^{DRO}-\beta_n^{ADRO}\Vert_2 +\epsilon_n$, which is of order $\mathcal{O}(1/\sqrt{n})$.
}

The discussions above demonstrate that an upper confidence bound can be derived on the out-of-sample error of our proposed estimator $\beta_n^{ADRO}$. As a result, the adjustment strategy won't sacrifice the WDRO estimator's capacity for generalization.
\section{Adjusted WDRO in the Generalized Linear Model}\label{glmdro}
The generalized linear model is considered in this section since several well-known regression models can be covered, including logistic regression, Poisson regression, and linear regression.
We introduce how to develop the associated adjusted WDRO estimators.
\subsection{Formulation of the Generalized Linear Model}
In the generalized linear model, the label variable $Y$ is generated from a particular distribution from the exponential family, including the Bernoulli distribution on $Y\in\{-1,1\}$ in the logistic regression, the Poisson distribution on $Y\in\{0,1,2,...\}$ in the Poisson regression, the normal distribution on $Y\in\mathbb{R}$ in the linear regression, etc.
The expectation of the label variable $Y$ conditional on the feature variable $\mathbf{X}$ is determined by the link function.
With a little abuse of notation, if we denote the nonzero ground-truth parameter by $\beta_\ast$ and the link function by $G(\cdot)$, we have $G(\mathbb{E}[Y|\mathbf{X}=\mathbf{x}])=\langle \mathbf{x},\beta_{\ast}\rangle$, where the link functions $G(\cdot)$ are chosen as the logit function in the logistic regression, the log function in the Poisson regression, the identity function in the linear regression, etc.
If we denote the logit function, the log function, and the identity function by $G^{1}(\cdot)$, $G^{2}(\cdot)$, and $G^{3}(\cdot)$, respectively, we have 
\[G^{1}(t)=\log \left(\frac{t}{1-t}\right),\quad  G^{2}(t)=e^{t},\quad G^{3}(t)=t.\]

In the generalized linear model, the ground-truth parameter $\beta_{\ast}$ is estimated by the maximum likelihood estimation method, and the associated loss function can be denoted by $L(f(\mathbf{x},\beta),y)=L(\langle\mathbf{x},\beta\rangle,y)$.
If we denote the loss function in the logistic regression, the Poisson regression and the linear regression by $L^{1}(\cdot,\cdot)$, $L^{2}(\cdot,\cdot)$, and $L^{3}(\cdot,\cdot)$, respectively, we have
\[L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)=\log(1+e^{-y\langle \mathbf{x},\beta\rangle}),\]
\[L^{2}\left(\langle\mathbf{x},\beta\rangle,y\right)=e^{\langle \mathbf{x},\beta\rangle}-y\langle \mathbf{x},\beta\rangle,\]
\[L^{3}(\langle\mathbf{x},\beta\rangle,y)=\frac{1}{2}\left(\langle\mathbf{x},\beta\rangle-y\right)^2,\]
where $\beta\in B$, $B$ is a compact convex subset of $\mathbb{R}^d$, $\beta_{\ast}\in {B}^{\circ}$, and $\mathbf{x}\in\Omega\subset\mathbb{R}^d$.

\subsection{Asymptotic Convergence of the WDRO Estimator}\label{example}
This subsection derives the convergence of the WDRO estimator $\beta^{DRO}_n$ in the linear regression, logistic regression, and Poisson regression.

Suppose that our choice of hyperparameters follows Assumption \ref{assume1}.
As demonstrated in Section \ref{section2.1}, we check Assumption \ref{assumeloss} and Assumption \ref{assumedistr} in the following lemmas.
\begin{lemma}\label{checklemma1}
    The loss function $L^{1}(\langle \mathbf{x},\beta\rangle,y)$ satisfies the conditions in Assumption \ref{assumeloss}.
\end{lemma}
\begin{lemma}\label{poissonassume}
If $\Omega$ is bounded, the loss function $L^{2}(\langle\mathbf{x},\beta\rangle,y)$ satisfies the conditions Assumption \ref{assumeloss}.
\end{lemma}
\begin{lemma}\label{linearassume}
The loss function $L^{3}(\langle\mathbf{x},\beta\rangle,y)$ satisfies the conditions Assumption \ref{assumeloss}.
\end{lemma}
\begin{lemma}\label{checklemma2}
    In the logistic regression, if there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, and $\mathbb{E}_{P_\ast}\left[\Vert\mathbf{X}\Vert_2^2\right]<\infty$, Assumption \ref{assumedistr} is satisfied.
\end{lemma}
\begin{lemma}\label{poissonassume2}
        In the Poisson regression, if there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, and $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^2e^{\langle\mathbf{X},\beta_\ast\rangle}]<\infty$, Assumption \ref{assumedistr} is satisfied.
\end{lemma}
\begin{lemma}\label{linearassume2}
        In the linear regression, if there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, and $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^2]<\infty$, Assumption \ref{assumedistr} is satisfied.
\end{lemma}

Lemma \ref{checklemma1}-\ref{linearassume} imply that the loss functions satisfy the conditions in Assumption \ref{assumeloss} while Lemma \ref{checklemma2}-\ref{linearassume2} show that Assumption \ref{assumedistr} can be simplified in the logistic regression, Poisson regression, and linear regression.

Equipped with Lemma \ref{checklemma1}-\ref{linearassume2}, the convergence in distribution of the WDRO estimator $\beta_n^{DRO}$ can be established due to Theorem \ref{theorem1}. 
The following three propositions give the explicit expression of the asymptotic distribution of the WDRO estimator for the logistic regression, Poisson regression, and linear regression.
\begin{proposition}[Convergence of $\beta_n^{DRO}$ in the logistic regression]\label{logistic} In the logistic regression, under Assumption \ref{assume1}, if $\Omega=\mathbb{R}^d$ and $\mathbb{E}\left[\Vert\mathbf{X}\Vert_2^2\right]<\infty$, and there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, the WDRO estimator $\beta_n^{DRO}$ converges in distribution:
\[\sqrt{n}(\beta_{n}^{DRO}-\beta_\ast)\Rightarrow\mathcal{N}(-C(\beta_\ast)^{-1}H(\beta_\ast),D),\]
where
\begin{equation}\label{cov1}D=\left(\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{\top}}{\left( 1+e^{Y\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\right)^{-1} ,\end{equation}
and
\begin{equation}\label{H1}
C(\beta_\ast)=\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left( 1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right],\quad
H(\beta_\ast)=\tau\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[\frac{e^{\langle\mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}.\end{equation}
\end{proposition}
\begin{proposition}[Convergence of $\beta_n^{DRO}$ in the Poisson regression]\label{poissontheorem}
In the Poission regression, under Assumption \ref{assume1}, if $\Omega$ is compact and can be expressed as $\Omega=\{\mathbf{x}\in\mathbb{R}^d: A\mathbf{x}\leq b\}$, where $A$ is an $l\times d$ matrix with linearly independent rows and $b\in\mathbb{R}^l$, $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^3e^{\langle\mathbf{X},\beta_\ast\rangle}]<\infty$, there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, and $\mathbf{X}$ has a probability density which is absolutely continuous w.r.t. Lebesgue measure, the WDRO estimator $\beta_n^{DRO}$ converges in distribution:
\[\sqrt{n}(\beta_{n}^{DRO}-\beta_\ast)\Rightarrow\mathcal{N}(-C(\beta_\ast)^{-1}H(\beta_\ast),D),\]
where
\begin{equation}\label{cov2}D= \left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right]\right)^{-1},\end{equation}
and
\begin{equation}\label{H2}
C(\beta_\ast)=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right],\quad
H(\beta_{\ast})=\tau\frac{\beta_{\ast}}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_\ast}[e^{\langle \mathbf{X},\beta_{\ast}\rangle}]}.\end{equation}
\end{proposition}
\begin{proposition}[Convergence of $\beta_n^{DRO}$ in the linear regression]
\label{lineartheorem}
In the linear regression, under Assumption \ref{assume1}, if $\Omega=\mathbb{R}^d$, and $\mathbb{E}_{P_{\ast}}[ \Vert\mathbf{X}\Vert_2^2]<\infty$, and there does not exist nonzero vector $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, the WDRO estimator $\beta_n^{DRO}$ converges in distribution:
\[\sqrt{n}(\beta_{n}^{DRO}-\beta_\ast)\Rightarrow\mathcal{N}(-C^{-1}H(\beta_\ast),D),\]
where
\begin{equation}\label{cov3}D=\sigma^2\left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}\right]\right)^{-1},\end{equation}
\begin{equation}\label{H3}C=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}\right],\quad
H(\beta_{\ast})=\tau\sigma\frac{\beta_{\ast}}{\Vert\beta_\ast\Vert_2},\end{equation}
 and $\Var(Y|\mathbf{X})=\sigma^2, \sigma>0$
\end{proposition}

We could obtain the associated adjusted WDRO estimators based on the convergence results derived in Proposition \ref{logistic}-\ref{lineartheorem}, and the details will be clarified in the next subsection.

Also, the proofs of Proposition \ref{logistic}-\ref{lineartheorem} are relegated to Appendix \ref{appendix}. The proofs show that the conditions in Proposition \ref{sufficient2} are satisfied, which enables us to simplify the function $H(\cdot)$, seeing \eqref{H1}, \eqref{H2} and \eqref{H3}. 

{
\subsection{Adjusted WDRO Estimator in the Generalized Linear Model}
This subsection gives the formulations of the adjusted WDRO estimator for logistic regression, Poisson regression, and linear regression by plugging the expressions of the function $C(\cdot)$ and $H(\cdot)$ in \eqref{H1}, \eqref{H2} and \eqref{H3} into the definition of the adjusted WDRO estimator \eqref{adrodefinition}.
\begin{definition}\label{defexample}
Under assumptions in Proposition \ref{logistic}-\ref{lineartheorem}, for the nonzero WDRO estimator $\beta_n^{DRO}$, we define the adjusted WDRO estimator $\beta_n^{ADRO}$ as follows, 
\begin{equation}\label{computeadrolog} \beta_n^{ADRO}= \beta_n^{DRO}+\frac{\tau}{\sqrt{n}}\sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\frac{e^{\langle\mathbf{X},\beta_n^{DRO}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_n^{DRO}\rangle}\right)^2}\right]}\left(\mathbb{E}_{\mathbb{P}_n}\left[\frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_n^{DRO}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_n^{DRO}\rangle}\right)^2}\right]\right)^{-1}\frac{\beta_n^{DRO}}{\Vert\beta_n^{DRO}\Vert_2}, \end{equation}
\[\beta_n^{ADRO}=\beta_n^{DRO}+\frac{\tau}{\sqrt{n}}\sqrt{\mathbb{E}_{\mathbb{P}_n}[e^{\langle \mathbf{X},\beta_n^{DRO}\rangle}]}\left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^{\top}  e^{\langle\mathbf{X},\beta_n^{DRO}\rangle}
\right]\right)^{-1}\frac{\beta_n^{DRO}}{\Vert\beta_n^{DRO}\Vert_2},\]
\begin{equation}\label{adjustdrolinear}\beta_n^{ADRO}=\beta_n^{DRO}+\frac{\tau\sigma}{\sqrt{n}} \left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^\top\right]\right)^{-1} \frac{\beta_n^{DRO}}{\Vert\beta_n^{DRO}\Vert_2},\end{equation}
for the logistic regression, Poisson regression, and linear regression, respectively.
\end{definition}
Without loss of generality, as we discussed in Proposition \ref{conditions2}, the adjusted WDRO estimators defined in Definition \ref{defexample} are well-defined.
Then, we check the conditions in Theorem \ref{theorem2} hold in the following theorem
to show that the proposed adjustment technique could de-bias the associated adjusted WDRO estimators successfully in the logistic regression, Poisson regression, and linear regression.
\begin{theorem}\label{unbiasedglm}
    For the adjusted WDRO estimator $\beta_n^{ADRO}$ defined in Definition \ref{defexample}, we have the following 
    \[\sqrt{n}\left(\beta_n^{ADRO}-\beta_\ast\right)\Rightarrow \mathcal{N}(0,D),\]
    where $D$ is defined by \eqref{cov1}, \eqref{cov2}, and \eqref{cov3} in the logistic regression, Poisson regression, and linear regression, respectively.
\end{theorem}
}
\section{Numerical Experiments}\label{exp}
In this section, we investigate the empirical performance of the adjusted WDRO estimator $\beta_n^{ADRO}$, compared with the classic WDRO estimator $\beta_n^{DRO}$.

\subsection{Experiment Setting}
The WDRO algorithmic framework of the logistic regression model and linear regression model with quadratic loss has been established in \cite{blanchet2022optimal}. 
Therefore, the adjusted estimators in the logistic regression model and the
linear regression model are implemented as examples to help explore the practical performance of our adjusting technique.
\subsubsection{Logistic Regression}
Assume the feature variable $\mathbf{X}$ follows the multivariate standard normal distribution. 
Suppose $\mathbf{X}$ follows 2-dimensional standard normal distribution, and the label variable $Y$ follows the Bernoulli distribution, where $\mathbb{P}(Y=1|\mathbf{X}=\mathbf{x})=1/(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle})$ and $\beta_\ast=(1/\sqrt{17},4/\sqrt{17})$. 
Data is generated $5$ times for each sample size $n\in\{500,700,1000,1500,$ $1800,2000\}$. 
The WDRO estimator $\beta_n^{DRO}$ is computed by the iterative algorithm in \cite{blanchet2022optimal}.
The adjusted WDRO estimator $\beta_n^{ADRO}$ is computed via equation \eqref{computeadrolog}.
Per the iterative algorithm, we set the learning rate as $0.3$ and the maximum number of iterations as $50000$, respectively.
Moreover, the value of $\tau$, which is the coefficient in the Wasserstein radius $\rho_n=\tau/\sqrt{n}$, should be determined, so we let $\tau\in\{1.5,2,2.5,3\}$. 

\subsubsection{Linear regression}
Assume the feature variable $\mathbf{X}$ follows the 2-dimensional standard normal distribution, and the label variable $Y$ follows normal distribution, where $Y|\mathbf{X}=\mathbf{x}\sim \mathcal{N}(\langle\mathbf{x},\beta_\ast\rangle,\sigma)$, $\beta_\ast=(3/\sqrt{10},-1/\sqrt{10})$. We set $\sigma=0.1$.
Data is generated $5$ times for each sample size $n\in\{500,700,1000,1500,1800,2000\}$. 
The WDRO estimator $\beta_n^{DRO}$ is computed by the iterative algorithm in \cite{blanchet2022optimal}.
The adjusted WDRO estimator $\beta_n^{ADRO}$ is computed via equation \eqref{adjustdrolinear}.
Per the iterative algorithm, we set the learning rate as $0.05$  and the maximum number of iterations as $50000$, respectively.
Then, we set the value of $\tau$ as $\tau\in\{1.5,2,2.5,3\}$.
\subsection{Experiment Results}
The experimental results of the logistic regression are reported in Figure \ref{logistic1}-\ref{logistic4}, and the results of the linear regression are reported in Figure \ref{linear1}-\ref{linear4}.

The performance of the estimators is evaluated by the squared error. The squared error is defined by $\Vert \widehat{\beta}-\beta_\ast\Vert_2^2$, where $\widehat{\beta}$ denotes the associated estimator. 
We plot the mean squared error of $\beta_n^{DRO}$ and $\beta_n^{ADRO}$ versus the logarithm of the sample size $n$, respectively. 
From the figures, we observe that the line of mean squared error of $\beta_n^{DRO}$ is always above that of $\beta_n^{ADRO}$, illustrating that the proposed adjusted estimator has a smaller mean squared error in empirical experiments. Recall that the adjusted WDRO estimator has a better asymptotic mean squared error in theory, while our empirical results show that our estimator also outperforms even when the sample size is finite.
Moreover, we compute the difference of the squared error between $\beta_n^{DRO}$ and $\beta_n^{ADRO}$ for each case.
This quantity can help us to evaluate the improvement achieved by the adjustment technique for each run. 
To visualize the improvement, we plot the boxplots for each sample size and each value of $\tau$. 
The figures show that most parts of the boxplots are located above $y=0$ in the logistic regression, and all of the boxplots are located above $y=0$ in the linear regression.
These observations indicate that the adjustment technique can generate a more accurate estimator of the ground-truth parameter $\beta_\ast$.

{In addition to the squared error, we investigate the empirical loss (i.e., the log-likelihood) of the linear regression and the logistic regression. 
Similar to how we analyze the squared loss, we plot the mean loss and the case-wise loss improvement. 
The figures show that the adjustment technique could reduce the empirical loss.}

Overall, the adjusted WDRO estimator has better empirical performance than the classic WDRO estimator. When people plan to estimate parameters in the statistical learning model, the proposed adjusted estimator can be considered.


\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{error_1point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errorbox_1point5.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{loss_1point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lossbox_1point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and log loss plots of the logistic regression, $\tau=1.5$.}
    \label{logistic1}
\end{figure}



\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{error_2.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errorbox_2.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{loss_2.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lossbox_2.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and log loss plots of the logistic regression, $\tau=2$.}
    \label{logistic2}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{error_2point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errorbox_2point5.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{loss_2point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lossbox_2point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and log loss plots of the logistic regression, $\tau=2.5$.}
    \label{logistic3}
\end{figure}


\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{error_3.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{errorbox_3.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{loss_3.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lossbox_3.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and log loss plots of the logistic regression, $\tau=3$.}
    \label{logistic4}
\end{figure}





\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerror_1point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrorbox_1point5.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearloss_1point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearlossbox_1point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and squared loss plots of the linear regression, $\tau=1.5$.}
    \label{linear1}
\end{figure}



\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerror_2.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrorbox_2.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearloss_2.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearlossbox_2.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and squared loss plots of the linear regression, $\tau=2$.}
    \label{linear2}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerror_2point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrorbox_2point5.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearloss_2point5.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearlossbox_2point5.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and squared loss plots of the linear regression, $\tau=2.5$.}
    \label{linear3}
\end{figure}


\begin{figure}
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerror_3.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearerrorbox_3.pdf} % second figure itself
    \end{minipage}\\
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearloss_3.pdf} % first figure itself
    \end{minipage}\hspace{0.1in}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linearlossbox_3.pdf} % second figure itself
    \end{minipage}
    \caption{Squared error and squared  loss plots of the linear regression, $\tau=3$.}
    \label{linear4}
\end{figure}




% \begin{table}
% \centering
% \begin{tabular}{|c|c|c|c|c|}
%  \hline
%  \multirow{2}{*}{$n$} & \multicolumn{2}{c|}{squared error}& \multicolumn{2}{c|}{mean squared error}\\ \cline{2-5}
%  ~& $\beta_n^{DRO}$ & $\beta_n^{ADRO}$&$\beta_n^{DRO}$ & $\beta_n^{ADRO}$\\
%  \hline
%   \multirow{5}{*}{100} &0.133402  &\textbf{0.101591}&\multirow{5}{*}{0.133246}&\multirow{5}{*}{\textbf{0.109143}}\\ 
%   ~ &  \textbf{0.001249}  &0.005614& &\\
%   ~ & 0.102394 &\textbf{0.075612}& &\\
%   ~ &  0.115717
%   & \textbf{0.093121}& &\\
%   ~ & 0.313466 & \textbf{0.269776}& &\\
%  \hline
% \multirow{5}{*}{200} &\textbf{0.003833} & 0.005744&\multirow{5}{*}{0.037114}&\multirow{5}{*}{\textbf{0.030937}}\\ 
%   ~ &  0.005343 &\textbf{0.002626}&&\\
%   ~ & 0.038850 &\textbf{0.027438}&&\\
%   ~ &  0.088716& \textbf{0.073353}&&\\
%   ~ &  0.048829 &\textbf{0.045524}&&\\
%  \hline
%     \multirow{5}{*}{500} &\textbf{0.018205}&0.023331&\multirow{5}{*}{0.023114}&\multirow{5}{*}{\textbf{0.019853}}\\ 
%   ~ &  0.030941&\textbf{0.025561}&&\\
%   ~ &  0.014386 &\textbf{0.009892}&&\\
%   ~ &0.013878&\textbf{0.009549}&&\\
%   ~ &  0.038158&\textbf{0.030932}&&\\
%  \hline
%    \multirow{5}{*}{1000} &0.024577&\textbf{0.023034}&\multirow{5}{*}{0.021717}&\multirow{5}{*}{\textbf{0.019234}}\\ 
%   ~ &   0.001614 &\textbf{0.001430}&&\\
%   ~ &  0.025933 &\textbf{0.021491}&&\\
%   ~ & 0.034881&\textbf{0.030799}&&\\
%   ~ &  0.021580 &\textbf{0.019414}&&\\
%  \hline
%     \multirow{5}{*}{2000} &0.030986&\textbf{0.027639}&\multirow{5}{*}{0.012762}&\multirow{5}{*}{\textbf{0.011027}}\\ 
%   ~ &   0.005821&\textbf{0.004854}&&\\
%   ~ &  0.017528 &\textbf{0.014984}&&\\
%   ~ &  0.008839&\textbf{0.007074}&&\\
%   ~ &   0.000635&\textbf{0.000586}&&\\
%  \hline
%      \multirow{5}{*}{5000} & \textbf{0.003448}& 0.003718&\multirow{5}{*}{0.002532}&\multirow{5}{*}{\textbf{0.002519}}
% \\ 
%   ~ & \textbf{0.002306} &0.002823&&\\
%   ~ & \textbf{0.003614}&0.003616&&\\
%   ~ & 0.002860& \textbf{0.002212}&&\\
%   ~ &   0.000432& \textbf{0.000227}&&\\
%  \hline
% \end{tabular}
% \caption{The record of the squared error of $\beta_n^{DRO}$ and $\beta_n^{ADRO}$.}
% \label{table2}
% \end{table}
%\section{Discussions}
% In this paper, we propose a statistically favorable alternative, which is called the adjusted WDRO estimator, to the classic WDRO estimator.
% The adjusted WDRO estimator is transformed from the WDRO estimator, and the idea behind the proposed estimator is to de-bias the asymptotic distribution of the classic WDRO estimator.
% %The resulting transformation is essentially a rescaling of the WDRO estimator.
% In this sense, the proposed estimator is easy to obtain once we get the value of the WDRO estimator.
% While we have proposed a transformation that makes the asymptotic bias disappear, the future direction may be to minimize the associated asymptotic mean squared error among all possible transformations.
% The potential `optimal' transformation is of super interest since it may possess the `best' statistical property from the perspective of asymptotic behavior.
% Other future directions include extending our theory to more general settings and adapting the adjusted technique to more practical applications.
\section{Discussion}
This paper improves the performance of the WDRO estimator through the lens of the statistical asymptotic behavior in statistical learning.
To the best of our knowledge, we are the first to propose transformations to de-bias the WDRO estimator.
The proposed adjusted WDRO estimator is asymptotically unbiased with a smaller asymptotic mean squared error. 
Also, the adjusted WDRO estimator is easy to compute as long as the classic WDRO estimator is known.

Notably, in the development of our theory and methodology, we also carefully clarify and check the corresponding assumptions, providing a rigorous scheme to apply and generalize our adjustment technique.
\acks{This project is partially supported by the Transdisciplinary Research Institute for Advancing Data Science (TRIAD), \url{https://research.gatech.edu/data/triad}, which is a part of the TRIPODS program at NSF and located at Georgia Tech, enabled by the
NSF grant CCF-1740776. The authors are also partially sponsored by NSF grants 2015363.}

\newpage
\appendix
\section{Proof}\label{appendix}
\subsection{Proof of Proposition \ref{uniquetransformation}}
\begin{proof}
Due to the sequential delta method (Theorem \ref{extendeddeltamethod}), we have that 
\begin{equation*}\sqrt{n}\left(\phi_n(\beta_n)-\phi_n(\beta_\ast)\right)\Rightarrow\mathcal{N}(\phi^{\prime}(\beta_\ast)f(\beta_\ast),\phi^{\prime}(\beta_\ast)D\phi^{\prime}(\beta_\ast)^{\top}),\end{equation*}
which is equivalent to 
\[\sqrt{n}\left(\phi_n(\beta_n)-\beta_\ast\right)+\sqrt{n}\left( \beta_\ast-\phi_n(\beta_\ast) \right) \Rightarrow\mathcal{N}(\phi^{\prime}(\beta_\ast)f(\beta_\ast),\phi^{\prime}(\beta_\ast)D\phi^{\prime}(\beta_\ast)^{\top}).\]
To make the distribution of $\sqrt{n}\left(\phi_n(\beta_n)-\beta_\ast\right)$ have a finite limit,  we should require the following holds \[\beta_\ast-\phi_n(\beta_\ast)=\mathcal{O}\left(\frac{1}{\sqrt{n}}\right),\]
which is equivalent to
\[ \phi_n(\beta_\ast)=\beta_\ast+\mathcal{O}\left(\frac{1}{\sqrt{n}}\right),\]
indicating 
\[ \phi_n^{\prime}(\beta_\ast)\to I.\]
In this way, we have that
\[\sqrt{n}\left(\phi_n(\beta_n)-\beta_\ast\right)+\sqrt{n}\left( \beta_\ast-\phi_n(\beta_\ast) \right) \Rightarrow\mathcal{N}(f(\beta_\ast),D).\]
Without loss of generality, we assume that
\[ \phi_n(\beta_\ast)=\beta_\ast-\frac{1}{\sqrt{n}}g(\beta_\ast)+o\left(\frac{1}{\sqrt{n}}\right),\]
then we could have that 
\[\sqrt{n}\left(\phi_n(\beta_n)-\beta_\ast\right)\sim\mathcal{N}\left(f(\beta_\ast)-g(\beta_\ast),D\right).\]
Consequently, the least asymptotic mean squared error is $\tr(D)$ if and only if $f(\beta_\ast)=g(\beta_\ast)$.
\end{proof}
\subsection{Proof of Theorem \ref{generaltheorem}}
\begin{proof}
To prove \eqref{generaladjusted}, due to Slutsky's lemma, it suffices to show that  
\[ f_n(\beta_n)-f(\beta_\ast)\to_p 0,\]
which could be promised if 
\begin{equation}\label{eq2}f_n(\beta_\ast)-f(\beta_\ast)\to_p 0,\end{equation}
\begin{equation*}f_n(\beta_n)-f_n(\beta_\ast)\to_p 0,\end{equation*}
where \eqref{eq2} is our assumption. Thus, it suffices to show $f_n(\beta_n)-f_n(\beta_\ast)\to_p 0$ holds.

Since $\sqrt{n}(\beta_n-\beta_\ast)$ converges to some distribution, $\beta_n$ converges to $\beta_\ast$ in probability.
Since $f_n$ is differentiable at $\mathcal{B}(\beta_\ast)$, it follows from the mean value theorem (or Taylor's expansion) that 
\begin{equation*}
\Vert f_n(\beta_n)-f_n(\beta_\ast)\Vert\leq \sup_{ \beta\in\mathcal{B}(\beta_\ast)}\Vert f_n^\prime(\beta)\Vert \Vert \beta_n-\beta_\ast\Vert,
\end{equation*}
%https://mathweb.ucsd.edu/~lni/math20e/l6.pdf;https://sites.math.washington.edu/~folland/Math425/taylor2.pdf;https://en.wikipedia.org/wiki/Jet_(mathematics)
It follows from $\beta_n-\beta_\ast\to_p 0$ and $\sup_{\beta\in \mathcal{B}(\beta_\ast)}\Vert f^\prime_n(\beta)\Vert$ is bounded in probability that $f_n(\beta_n)-f_n(\beta_\ast)\to_p0$.
\end{proof}


\subsection{Proof of Theorem \ref{extendeddeltamethod}}
\begin{proof}
The proof is based on the proof logic of Theorem 3.1 (classic delta method) in \cite{van2000asymptotic}.

    Because the sequence $r_n(T_n-\theta)$ converges in distribution, $T_n-\theta$ converges to $0$ in probability and $r_n(T_n-\theta)$ is uniformly tight.

    Then,  according to Taylor's expansion, we have that
\[ r_n\left(\phi_n(T_n)-\phi_n(\theta)\right)
        =\phi^\prime_{n}(\theta)\left( r_n(T_n-\theta)\right)+ o_p( r_n \Vert T_n-\theta\Vert).\]
    It follows from the tightness of $r_n(T_n-\theta)$ that $o_p(r_n\Vert T_n-\theta_n\Vert)=o_p(1)$, where $o_p(1)$ means ``converge to $0$ in probability''.

    Because matrix multiplication is continuous and  we have $\phi^\prime_{n}(\theta)\to \phi^\prime(\theta)$, taking advantage of the extended continuous-mapping theorem (Theorem 1.11.1 in \cite{van1996weak}), we could obtain that
    \[\phi_{n}^\prime(\theta)\left( r_n(T_n-\theta)\right)\Rightarrow \mathcal{N}(\phi^\prime(\theta) \mu ,\phi^\prime(\theta)\Sigma{\phi_{\theta}^\prime}^\top).\]

    Slutsky's lemma implies that 
    \[r_n\left(\phi(T_n)-\phi(\theta)\right)\Rightarrow \mathcal{N}(\phi^\prime(\theta) \mu ,\phi^\prime(\theta)\Sigma{\phi^\prime(\theta)}^\top).\]
\end{proof}
\subsection{Proof of Theorem \ref{theorem1}}
\begin{proof}
We denote the inner maximization of the WDRO problem \eqref{formal}, i.e., 
\[\max_{P\in \mathcal{U}_{\rho_n}(\mathbb{P}_n)} \mathbb{E}_{P}[L(f(\mathbf{X},\beta),Y)],\]
by $\Psi_n(\beta)$. 

Then, we have
\begin{equation}\label{vrclass}\Psi_n(\beta)=\inf_{\lambda\geq 0} \left[\lambda\rho^2_n+ \mathbb{E}_{(\mathbf{X},Y)\sim\mathbb{P}_n}\left[\sup_{\mathbf{x}\in\mathbb{R}^d}\left[L(f(\mathbf{x},\beta),Y)-\lambda\Vert \mathbf{x}-\mathbf{X}\Vert_2^2 \right]\right]\right].\end{equation}

Note that Assumption \ref{assume1}, \ref{assumeloss}, and \ref{assumedistr} are extracted from Assumption 1 and 2 in \cite{blanchet2022confidence}, and problem \eqref{vrclass} can be reduced to the problem in Lemma A.1 in \cite{blanchet2022confidence}. Following the same technique, one could derive the convergence in distribution of $\beta_n^{DRO}$:
\[\sqrt{n}(\beta^{DRO}_n-\beta_{\ast})\Rightarrow C(\beta_\ast)^{-1}E-C(\beta_\ast)^{-1}H(\beta_\ast),\]
where \[E\sim \mathcal{N}\left(0,\Cov\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)\right),\]
\[H(\beta_{\ast})=\tau \frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}.\]

It follows from the matrix $C(\beta_\ast)$ is positive definite that 
\[\sqrt{n}(\beta^{DRO}_n-\beta_{\ast})\Rightarrow  \mathcal{N}\left(-C(\beta_\ast)^{-1}H(\beta_\ast),C(\beta_\ast)^{-1}\Cov\left(\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)C(\beta_\ast)^{-1}\right).\]
\end{proof}
\subsection{Proof of Proposition \ref{conditions2}}
\begin{proof}
    Notice that 
    \[\frac{\partial^2 L(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\frac{\partial^2 L(\langle\mathbf{x},\beta\rangle,y)}{\partial f^2}\mathbf{x}\mathbf{x}^\top. \]
    Since $\frac{\partial^2 L(\langle\mathbf{x},\beta\rangle,y)}{\partial f^2}\not=0$ and $\mathbb{P}_n(\alpha^{\top}\mathbf{X}=0)=1$, we have that 
    \[\mathbb{E}_{\mathbb{P}_n}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\beta_n^{DRO}}\succ 0.\]
    Notice that 
    \[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\Bigg\vert_{\beta=\beta_n^{DRO}}=\left(\frac{\partial L(\langle\mathbf{X},\beta_n^{DRO}\rangle,Y)}{\partial  f}\right)^2\Vert\beta_n^{DRO}\Vert_2^2.\]
    Since we have $\beta_n^{DRO}\not=0$ and $\mathbb{P}_n\left( \frac{\partial L(\langle\mathbf{X},\beta_n^{DRO}\rangle,Y)}{\partial  f}\not=0\right)>0$, we have that\
     \[\mathbb{P}_n\left( \left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\Bigg\vert_{ \beta=\beta_n^{DRO}}\not=0\right)>0.\]
\end{proof}
\subsection{Proof of Proposition \ref{sufficient2}}
\begin{proof}
If  $f(\mathbf{X},\beta)=\langle \mathbf{X},\beta\rangle$ holds, we have
\begin{equation*}
\begin{aligned}\label{decomposeH}
H(\beta_{\ast})&=\tau \frac{\partial \sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\\
&=\tau \frac{\partial \left(\Vert \beta\Vert_2 \sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\right]}\right)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\\
&=\tau\left(\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2} \sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\right)^2\right]}+\Vert\beta_{\ast}\Vert_2\frac{\mathbb{E}_{P_\ast}\left[ \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\frac{\partial^2 L(f(\mathbf{X},\beta_\ast),Y)}{\partial f^2}\mathbf{X}\right] }{ \sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\right)^2\right]}}\right).
\end{aligned}\end{equation*}

Further, if \[\mathbb{E}_{P_\ast}\left[ \frac{\partial L(f(\mathbf{X},\beta_\ast),Y)}{\partial f}\frac{\partial^2 L(f(\mathbf{X},\beta_\ast),Y)}{\partial f^2}\mathbf{X}\right]=0\] holds, 
the second term in the equation \eqref{decomposeH} equals to 0. 


Then, we have
\[H(\beta_\ast)=\tau\frac{\beta_\ast}{{\Vert\beta_\ast\Vert_2}}\sqrt{\mathbb{E}_{P_\ast}\left[\left( \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial f}\right)^2\Bigg\vert_{\beta=\beta_\ast}\right]}.\] 
\end{proof}

\subsection{Proof of Theorem \ref{theorem2}}
\begin{proof}
We have that 
\[\frac{\partial \sqrt{\mathbb{E}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}{\partial \beta}
=\frac{\mathbb{E}\left[ \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}\partial\beta}\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right]}{\sqrt{\mathbb{E}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}.\]
In this way, we have that \[f(\mathbf{z})=-C(\mathbf{z})^{-1}H(\mathbf{z})\] 
    \[ f_n(\mathbf{z})=-C_n(\mathbf{z})^{-1} H_n(\mathbf{z}), \]
    where 
    \[C(\mathbf{z})=\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\mathbf{z}}, \quad H(\mathbf{z})=\tau\frac{\mathbb{E}_{P_\ast}\left[ \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}\partial\beta}\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right]}{\sqrt{\mathbb{E}_{P_\ast}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}\Bigg\vert_{\beta=\mathbf{z}},\]
    \[ C_n(\mathbf{z})=\mathbb{E}_{\mathbb{P}_n}\left[\frac{\partial^2 L(f(\mathbf{X},\beta),Y)}{\partial \beta^2}\right]\Bigg\vert_{\beta=\mathbf{z}}, \quad H_n(\mathbf{z})=\tau\frac{\mathbb{E}_{\mathbb{P}_n}\left[ \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}\partial\beta}\frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right]}{\sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\left\Vert \frac{\partial L(f(\mathbf{X},\beta),Y)}{\partial \mathbf{X}}\right\Vert_2^2\right] }}\Bigg\vert_{\beta=\mathbf{z}}.\]
    
It follows from Theorem \ref{generaltheorem} that it suffices to show $f_n$ satisfies Assumption \ref{assumefn}. 

Because  $L(f(\mathbf{x},\beta),y)$ is twice continuously differentiable, $f_n$ is differentiable.
Since $C_n(\mathbf{z})$ and $H_n(\mathbf{z})$ are defined in terms of the empirical distribution, $f_n(\beta_\ast)\to_pf(\beta_\ast)$ holds due to the law of large numbers. Notably, since we let $L(f(\mathbf{x},\beta),y)$ be twice continuously differentiable, $\frac{\partial L(f(\mathbf{x},\beta),y)}{\partial x\partial \beta}$ and $\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \beta^2}$  be continuously differentiable w.r.t $\beta$ in some neighborhood of $\beta_\ast$, then $f_n$ is continuously differentiable w.r.t $\beta$ in some neighborhood of $\beta_\ast$. Together with the gradient of $C_n(\mathbf{z})^{-1}H_n(\mathbf{z})$ at $\beta_\ast$  is bounded in probability, we could know that the sequence $\sup_{\beta\in\mathcal{B}(\beta_\ast)}\Vert f^\prime_n(\beta)\Vert$ is bounded in probability.
\end{proof}

\subsection{Proof of Lemma \ref{checklemma1}}
\begin{proof}
    \textbf{a.} The loss function $L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)=\log(1+e^{-y\langle \mathbf{x},\beta\rangle})$ is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$.

    \textbf{b.} Because we have \[\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta^2}=\frac{\mathbf{x}\mathbf{x}^{\top}e^{y\langle\mathbf{x},\beta\rangle}}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^2}\succeq 0,\]
    where $\succeq$ means the matrix is positive semidefinite,
    the function $L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)$ is convex w.r.t. $\beta$.
    
     \textbf{c.} We have
    \begin{equation*}
    \begin{aligned}
    \left\Vert\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2&=\left\Vert\frac{\beta\beta^{\top}e^{y\langle\mathbf{x},\beta\rangle}}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^2}\right\Vert_2\\
    &=\left\Vert \beta\right\Vert_2^2 \frac{e^{y\langle\mathbf{x},\beta\rangle}}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^2}<M(\beta)=\Vert\beta\Vert_2^2.
    \end{aligned}
    \end{equation*}
   
    Further, 
    \[\frac{\partial \left\Vert\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2}{\partial \mathbf{x}}=\left\Vert \beta\right\Vert_2^2 \frac{y\beta e^{y\langle \mathbf{x},\beta\rangle} \left( 1-e^{y\langle \mathbf{x},\beta\rangle}\right)}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^3}.\]

    We know 
    \[\frac{ e^{y\langle \mathbf{x},\beta\rangle} \left( 1-e^{y\langle \mathbf{x},\beta\rangle}\right)}{\left(1+e^{y\langle\mathbf{x},\beta\rangle}\right)^3}\]
    is bounded. In addition, as $\beta\in B$ and $B$ is bounded, \[\frac{\partial \left\Vert\frac{\partial^2L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2}{\partial \mathbf{x}}\] is also bounded, which implies that \[\left\Vert\frac{\partial^2 L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \mathbf{x}^2}\right\Vert_2\]is uniformly continuous w.r.t. $\mathbf{x}$.
\end{proof}

\subsection{Proof of Lemma \ref{poissonassume}}
\begin{proof}
    \textbf{a.} The loss function $L^{2}\left(\langle\mathbf{x},\beta\rangle,y\right)=e^{\langle \mathbf{x},\beta\rangle}-y\langle \mathbf{x},\beta\rangle$ is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$.

\textbf{b.} Because we have\[\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\mathbf{x}\mathbf{x}^{\top}e^{\langle \mathbf{x},\beta\rangle}\succeq 0,\]
the function $L^{2}\left(\langle\mathbf{x},\beta\rangle,y\right)$ is convex w.r.t. $\beta$.

\textbf{c.}
We have
\[\left\Vert\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}^2}\right\Vert_2 = \left\Vert\beta\beta^{\top}\right\Vert_2 e^{\langle \mathbf{x},\beta\rangle}=\Vert \beta\Vert_2^2 e^{\langle\mathbf{x},\beta\rangle}.\]

Because $\mathbf{x}\in \Omega, \beta\in B$, where both $\Omega$ and $B$ are bounded, $\Vert\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}^2}\Vert_2$ is bounded by a function of $\beta$ and uniformly continuous w.r.t. $\mathbf{x}$.
\end{proof}


\subsection{Proof of Lemma \ref{linearassume}}
\begin{proof}
    \textbf{a.} The loss function $L^{3}(\langle\mathbf{x},\beta\rangle,y)=\frac{1}{2}\left(\langle\mathbf{x},\beta\rangle-y\right)^2$ is twice continuously differentiable w.r.t. $\mathbf{x}$ and $\beta$.

\textbf{b.} The loss function $L^{3}(\langle\mathbf{x},\beta\rangle,y)=\frac{1}{2}\left(\langle\mathbf{x},\beta\rangle-y\right)^2$ is convex w.r.t. $\beta$.

     \textbf{c.} We have
      \[\left\Vert\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \mathbf{x}^2}\right\Vert_2=\Vert 2\beta\beta^{\top}\Vert_2=2\Vert\beta\Vert_2^2.\]
      
      As $\beta\in B$ and $B$ is bounded, $\Vert\frac{\partial^2 L(f(\mathbf{x},\beta),y)}{\partial \mathbf{x}^2}\Vert_2$ is bounded by a function of $\beta$ and uniformly continuous w.r.t. $\mathbf{x}$.
\end{proof}
\subsection{Proof of Lemma \ref{checklemma2}}
\begin{proof}
\textbf{a.}
From the equation
\[ \frac{\partial L^{1}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta}=\frac{-y\mathbf{x}}{1+e^{y\langle \mathbf{x},\beta\rangle}},\]
and the assumption $\mathbb{E}_{P_\ast}\left[\Vert \mathbf{X}\Vert_2^2\right]<\infty$, we have
\[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]<\mathbb{E}_{P_\ast}\left[\Vert \mathbf{X}\Vert_2^2\right]<\infty.\]

Because we have
\[\frac{\partial^2 L^{1}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\frac{\mathbf{x}\mathbf{x}^{\top} e^{y\langle\mathbf{x},\beta\rangle}}{\left( 1+e^{y\langle \mathbf{x},\beta}\right)^2},\]
 where \[e^{y\langle\mathbf{x},\beta\rangle}/ (1+e^{y\langle \mathbf{x},\beta})^2>0,\] and there does not exist nonzero $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, we could conclude
\[\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta^2}\bigg\vert_{\beta=\beta_{\ast}}\right]\succ 0.\]

In addition,
    \begin{equation*}\begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial\beta}\Bigg\vert_{\beta=\beta_{\ast}}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[ \frac{-Y\mathbf{X}}{1+e^{Y\langle\mathbf{X},\beta_\ast\rangle}}\right]\\
    &=\int \mathbb{P}(Y=1|\mathbf{X}=\mathbf{x})\frac{\mathbf{x}}{1+e^{\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})+\int \mathbb{P}(Y=-1|\mathbf{X}=\mathbf{x})\frac{\mathbf{x}}{1+e^{-\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})\\
    &=\int \frac{\mathbf{x}}{1+e^{-\langle\mathbf{x},\beta_\ast\rangle}}\frac{-1}{1+e^{\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})+\int \frac{\mathbf{x}}{1+e^{\langle\mathbf{x},\beta_\ast\rangle}}\frac{1}{1+e^{-\langle\mathbf{x},\beta_\ast\rangle}}dF(\mathbf{x})\\
    &=0.
    \end{aligned}\end{equation*}

\textbf{b.} Notice we have
\[\frac{\partial L^{1}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}}\Bigg\vert_{\beta=\beta_{\ast}}=\frac{-y\beta_{\ast}}{1+e^{y\langle\mathbf{x},\beta_\ast\rangle}},\]
where \[\beta_{\ast}\not=0, y\not=0,1+e^{y\langle\mathbf{x},\beta_\ast\rangle}>0, \] so we can conclude 
\[P_{\ast}\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0.\]

Then, we have
    \[\frac{\partial ^2L^{1}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}=\frac{-yI_d}{1+e^{y\langle \mathbf{x},\beta_\ast\rangle}}+\frac{\beta_\ast\mathbf{x}^{\top} e^{y\langle\mathbf{x},\beta_\ast\rangle}}{\left( 1+e^{y\langle \mathbf{x},\beta_\ast\rangle}\right)^2}.\]

Since there is no nonzero vector $\alpha$ such that $P_\ast(\alpha^{\top}\mathbf{X}=0)=1$, and the kernel space of the matrix $\frac{\partial ^2L^{1}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\big\vert_{\beta=\beta_{\ast}}$ is different for different $\mathbf{x},y$, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{\top}\right]\succ 0.\]
\end{proof}
\subsection{Proof of Lemma \ref{poissonassume2}}
\begin{proof}
\textbf{a.}
From the equation
\[ \frac{\partial L^{2}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta}=\mathbf{x}e^{\langle \mathbf{x},\beta\rangle}-y\mathbf{x},\]
 we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]\\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( e^{\langle \mathbf{X},\beta_\ast\rangle}-Y\right)^2 \right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( e^{\langle \mathbf{X},\beta_\ast\rangle}-Y\right)^2\bigg\vert \mathbf{X}\right]\right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( e^{2\langle \mathbf{X},\beta_\ast\rangle}+Y^2-2Ye^{\langle \mathbf{X},\beta_\ast\rangle}\right)\bigg\vert \mathbf{X}\right]\right]\\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2e^{2\langle\mathbf{X},\beta_\ast\rangle}\right]-2\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2e^{\langle\mathbf{X},\beta_\ast\rangle}Y\big\vert\mathbf{X}\right]+\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2Y^2\big\vert\mathbf{X}\right].\end{aligned}
\end{equation*}   

$Y$ conditional on $\mathbf{X}$ follows the Poisson distribution with parameter $e^{\langle\mathbf{X},\beta_\ast\rangle}$.
In this way, we have that 
\[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]=\mathbb{E}_{P_{\ast}}\left[ \Vert\mathbf{X}\Vert_2^2e^{\langle\mathbf{X},\beta_\ast\rangle} \right]<\infty.\]


Because we have
\[\frac{\partial^2 L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\mathbf{x}\mathbf{x}^{\top} e^{\langle\mathbf{x},\beta\rangle},\]
 where $e^{\langle\mathbf{x},\beta\rangle}>0$, and there does not exist nonzero $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, we could conclude
\[\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta^2}\bigg\vert_{\beta=\beta_\ast}\right]\succ 0.\]

In addition, 
  \begin{equation*}\begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial\beta}\Bigg\vert_{\beta=\beta_{\ast}}\right]\\&=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} e^{\langle\mathbf{X},\beta_{\ast}\rangle}-Y\mathbf{X}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} e^{\langle\mathbf{X},\beta_{\ast}\rangle}-Y\mathbf{X}\bigg\vert \mathbf{X}\right]\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} e^{\langle\mathbf{X},\beta_{\ast}\rangle}-e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]\\
    &=0,
    \end{aligned}\end{equation*}
    \textbf{b.} Notice we have
\[\frac{\partial L^{2}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}}\Bigg\vert_{\beta=\beta_{\ast}}=\beta_\ast (e^{\langle \mathbf{x},\beta_\ast\rangle}-y),\]
where $\beta_{\ast}\not=0$,
\[P_\ast\left(e^{\langle \mathbf{X},\beta_\ast\rangle}-Y\not=0\right)>0,\]
so we can conclude 
\[P_{\ast}\left(\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0.\]


Then, we have
    \[\frac{\partial ^2L^{2}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}=I( e^{\langle\mathbf{x},\beta_\ast\rangle}-y)+\beta_\ast \mathbf{x}^{\top} e^{\langle\mathbf{x},\beta_\ast\rangle}.\]


Since there is no nonzero vector $\alpha$ such that $P_\ast(\alpha^{\top}\mathbf{X}=0)=1$ and the kernel space of the matrix $\frac{\partial ^2L^{2}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\big\vert_{\beta=\beta_{\ast}}$ is different for different $\mathbf{x},y$. Thus, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{\top}\right]\succ 0.\]
\end{proof}
\subsection{Proof of Lemma \ref{linearassume2}}
\begin{proof}
\textbf{a.}    From the equation
\[ \frac{\partial L^{3}\left(\langle \mathbf{x},\beta\rangle,y\right)}{\partial \beta}=(\langle\mathbf{x},\beta\rangle-y)\mathbf{x},\]
 we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]\\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( \langle \mathbf{X},\beta_\ast\rangle-Y\right)^2 \right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( \langle \mathbf{X},\beta_\ast\rangle-Y\right)^2\bigg\vert \mathbf{X}\right]\right]\\
&= \mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2 \left( \langle \mathbf{X},\beta_\ast\rangle^2-2\langle \mathbf{X},\beta_\ast\rangle Y+Y^2\right)\bigg\vert \mathbf{X}\right]\right]      \\
&=\mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2\langle \mathbf{X},\beta_\ast\rangle^2\right]-2 \mathbb{E}_{P_\ast}\left[ \Vert\mathbf{X}\Vert_2^2\langle \mathbf{X},\beta_\ast\rangle Y\vert \mathbf{X}\right]+ \mathbb{E}_{P_\ast}\left[ Y^2\vert \mathbf{X}\right]
\end{aligned}
\end{equation*} 

Notice that $Y$ conditional on $\mathbf{X}$ follows the normal distribution with a mean value of $\langle \mathbf{X},\beta_\ast\rangle$.

Thus, 
\[\mathbb{E}_{P_{\ast}} \left[\left\Vert \frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right\Vert_2^2\right]=\Var(Y^2|\mathbf{X})\mathbb{E}_{P_{\ast}}\left[ \Vert\mathbf{X}\Vert_2^2\right]<\infty.
\]

Because we have
\[\frac{\partial^2 L^{3}(\langle\mathbf{x},\beta\rangle,y)}{\partial \beta^2}=\mathbf{x}\mathbf{x}^{\top},\]
and there does not exist nonzero $\alpha$ such that $P_{\ast}(\alpha^{\top}\mathbf{X}=0)=1$, we could conclude
\[\mathbb{E}_{P_{\ast}}\left[\frac{\partial^2 L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta^2}\bigg\vert_{\beta=\beta_\ast}\right]\succ 0.\]

In addition, 
  \begin{equation*}\begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial\beta}\Bigg\vert_{\beta=\beta_{\ast}}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} \langle\mathbf{X},\beta_{\ast}\rangle-Y\mathbf{X}\right]\\
    &=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ \mathbf{X} \langle\mathbf{X},\beta_{\ast}\rangle-Y\mathbf{X}\vert\mathbf{X}\right]\right]\\
    &=0,
    \end{aligned}\end{equation*}
    \textbf{b.}
     Notice that,
\[\frac{\partial L^{3}(\langle\mathbf{x},\beta\rangle,y)}{\partial \mathbf{x}}\Bigg\vert_{\beta=\beta_{\ast}}=\beta_\ast \left(\langle \mathbf{x},\beta_\ast\rangle-y\right),\]
where $\beta_{\ast}\not=0$,
\[P_\ast\left(\langle \mathbf{X},\beta_\ast\rangle-Y\not=0\right)>0,\]
so we can conclude 
\[P_{\ast}\left(\frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}}\Bigg\vert_{\beta=\beta_{\ast}}\not =0\right)>0.\]

Then, we have
    \[\frac{\partial ^2L^{3}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}=I\left( \langle\mathbf{x},\beta_\ast\rangle-y\right)+\beta_\ast \mathbf{x}^{\top}.\]

 Since there is no nonzero vector $\alpha$ such that $P_\ast(\alpha^{\top}\mathbf{X}=0)=1$ and the kernel space of the matrix $\frac{\partial ^2L^{3}(\langle\mathbf{x},\beta\rangle,Y)}{\partial \mathbf{x}\partial \beta}\big\vert_{\beta=\beta_{\ast}}$ is different for different $\mathbf{x}, y$, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[\frac{\partial ^2L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\left(\frac{\partial ^2L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \mathbf{X}\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)^{\top}\right]\succ 0.\]
\end{proof}
\subsection{Proof of Proposition \ref{logistic}}\label{prooflogistic}
\begin{proof}
Regarding the asymptotic covariance matrix,
    because we have
    \[\Cov\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)= C(\beta_\ast)=\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{\top}}{\left( 1+e^{Y\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right],\]
    we could derive 
    \[D=C(\beta_\ast)^{-1}\Cov\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right) C(\beta_\ast)^{-1}=\left(\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{\top}}{\left( 1+e^{Y\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\right)^{-1}.\]
   
    Regarding the asymptotic mean of $\beta_n^{ADRO}$, we have 
    \begin{equation*}\label{h1}H(\beta_{\ast})=\tau\left(\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ \frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}-\frac{\Vert\beta_{\ast}\Vert_2 \mathbb{E}_{P_{\ast}}\left[ \frac{Y\mathbf{X}e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^3}\right]}{\sqrt{\mathbb{E}_{P_{\ast}}\left[ \frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}}\right).\end{equation*}
    
    Notice that 
    \begin{equation*}
    \begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{Y\mathbf{X}e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^3}\right]\\
    &=\int P(Y=1|\mathbf{X}=\mathbf{x}) \frac{\mathbf{x}e^{\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x}) -\int P(Y=-1|\mathbf{X}=\mathbf{x}) \frac{\mathbf{x}e^{-\langle\mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x}) -\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{-\langle\mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{2\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x}) -\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{\mathbf{x}e^{2\langle\mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^3}dF(\mathbf{x})\\
    &=0
    \end{aligned}
    \end{equation*}
which indicates that the equation \eqref{sufficient2eq} holds and the second term in \eqref{h1} equals to $0$.


    
Then, we obtain that 
    \[H(\beta_{\ast})=\tau\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ \frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}.\]

    Notice that 
    \begin{equation*}
    \begin{aligned}
    &\mathbb{E}_{P_{\ast}}\left[\frac{1}{\left(1+e^{Y\langle \mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\\
    &=\int P(Y=1|\mathbf{X}=\mathbf{x}) \frac{1}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x}) +\int P(Y=-1|\mathbf{X}=\mathbf{x}) \frac{1}{\left(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{1}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x}) +\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{1}{\left(1+e^{-\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{e^{\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x}) +\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{e^{2\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\int \frac{1}{1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}} \frac{e^{\langle \mathbf{x},\beta_{\ast}\rangle}+e^{2\langle \mathbf{x},\beta_{\ast}\rangle}}{\left(1+e^{\langle \mathbf{x},\beta_{\ast}\rangle}\right)^2}dF(\mathbf{x})\\
    &=\mathbb{E}_{P_{\ast}}\left[\frac{e^{\langle\mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]
    \end{aligned}
    \end{equation*}

Then, $H(\beta_\ast)$ can be simplified as 
\begin{equation*}H(\beta_\ast)=\tau\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[\frac{e^{\langle\mathbf{X},\beta_{\ast}\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]}.\end{equation*}

Similarly, the matrix $D$ and $C(\beta_\ast)$ can further be simplified as
\[D=\left(\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left( 1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right]\right)^{-1},\quad C(\beta_\ast)=\mathbb{E}_{P_{\ast}}\left[ \frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left( 1+e^{\langle\mathbf{X},\beta_{\ast}\rangle}\right)^2}\right].\]
\end{proof}





\subsection{Proof of Proposition \ref{poissontheorem}}\label{proofpoisson}
\begin{proof}
Regarding the asymptotic covariance matrix,
    since we have
    \[\Cov\left(\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)= C(\beta_\ast) =\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right],\]
    we could derive
    \[D=C(\beta_\ast)^{-1}\Cov\left(\frac{\partial L^{2}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right) C(\beta_\ast)^{-1}=\left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}e^{\langle \mathbf{X},\beta_{\ast}\rangle}\right]\right)^{-1}.\]
   
    Regarding the asymptotic mean of $\beta_n^{ADRO}$, we have
\begin{equation*}\label{h2}H(\beta_\ast)=\tau \left( \frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[\left( e^{\langle\mathbf{X},\beta_\ast\rangle}-Y\right)^2\right]}-\Vert\beta_\ast\Vert_2\frac{\mathbb{E}_{P_{\ast}}\left[ (e^{\langle \mathbf{X},\beta_\ast\rangle}-Y)e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]}{\sqrt{\mathbb{E}_{P_{\ast}}\left[\left( e^{\langle\mathbf{X},\beta_\ast\rangle}-Y\right)^2\right]}}\right),\end{equation*}
where $Y$ conditional on $\mathbf{X}$ follows the Poisson distribution with parameter $e^{\langle \mathbf{X},\beta_\ast\rangle}$. Hence, for the second term, we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}}\left[ (e^{\langle \mathbf{X},\beta_\ast\rangle}-Y)e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]\\
&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ (e^{\langle \mathbf{X},\beta_\ast\rangle}-Y)e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\big\vert\mathbf{X}\right]\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle \mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]-\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ Ye^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\big\vert\mathbf{X}\right]\right] \\
&=0,
\end{aligned}
\end{equation*}
which indicates that the equation \eqref{sufficient2eq} holds and the second term in \eqref{h2} equals to $0$.

Further, we have
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{P_{\ast}}\left[\left( e^{\langle\mathbf{X},\beta_\ast\rangle}-Y\right)^2\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_\ast\rangle}-2Ye^{\langle\mathbf{X},\beta_\ast\rangle}+Y^2\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_\ast\rangle}\right]-2\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_{\ast}}\left[ Ye^{\langle \mathbf{X},\beta_\ast\rangle} |\mathbf{X}\right]\right]+\mathbb{E}_{P_{\ast}}\left[ Y^2\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_\ast\rangle}\right]-2\mathbb{E}_{P_{\ast}}\left[e^{2\langle\mathbf{X},\beta_{\ast}\rangle}\right]+\mathbb{E}_{P_{\ast}}\left[ e^{\langle \mathbf{X},\beta_\ast\rangle}+e^{2\langle \mathbf{X},\beta_\ast\rangle}\right]\\
&=\mathbb{E}_{P_{\ast}}\left[e^{\langle\mathbf{X},\beta_\ast\rangle}\right]
\end{aligned}
\end{equation*}

 Hence, we have
\[H(\beta_{\ast})=\tau\frac{\beta_\ast}{\Vert\beta_\ast\Vert_2}\sqrt{\mathbb{E}_{P_\ast}[e^{\langle \mathbf{X},\beta_{\ast}\rangle}]}.\]
\end{proof}
\subsection{Proof of Proposition \ref{lineartheorem}}\label{prooflinear}
\begin{proof}
Regarding the asymptotic covariance matrix,
    since we have
    \[\Cov\left(\frac{\partial L^{3}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right)=\sigma^2\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}\right],\]
      \[C=\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}\right],\]
    we could derive 
    \[D=C^{-1}\Cov\left(\frac{\partial L^{1}(\langle\mathbf{X},\beta\rangle,Y)}{\partial \beta}\Bigg\vert_{\beta=\beta_{\ast}}\right) C^{-1}=\sigma^2\left(\mathbb{E}_{P_{\ast}}\left[ \mathbf{X}\mathbf{X}^{\top}\right]\right)^{-1}.\]
  
    Regarding the asymptotic mean of $\beta_n^{ADRO}$, we have 
    \begin{equation*}\label{h3}H(\beta_{\ast})=\tau\left(\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]}-\frac{\Vert\beta_{\ast}\Vert_2 \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\right]}{\sqrt{\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]}}\right).\end{equation*}
    
    Notice that, for the second term, we have 
\begin{equation*} \begin{aligned}
    \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\right]&= \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\vert\mathbf{X}\right]\\&=\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle\vert\mathbf{X}\right]-\mathbb{E}_{P_\ast}\left[Y\mathbf{X}\vert\mathbf{X}\right].\end{aligned}\end{equation*}

Since $Y$ conditional on $\mathbf{X}$ follows the normal distribution with the mean value $\langle\mathbf{X},\beta_\ast\rangle$, we can conclude that 
\[ \mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)\mathbf{X}\right]=\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle\vert\mathbf{X}\right]-\mathbb{E}_{P_\ast}\left[\langle\mathbf{X},\beta_\ast\rangle\mathbf{X}\vert\mathbf{X}\right]=0,\]
which indicates that the equation \eqref{sufficient2eq} holds and the second term in \eqref{h3} equals to $0$.


Then, we obtain that 
    \[H(\beta_{\ast})=\tau\frac{\beta_{\ast}}{\Vert\beta_{\ast}\Vert_2}\sqrt{\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]}.\]
  
Notice that we also have
\begin{equation*}
\begin{aligned}\mathbb{E}_{P_{\ast}}\left[ (\langle\mathbf{X},\beta_\ast\rangle-Y)^2\right]&=\mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[\left( \langle \mathbf{X},\beta_\ast\rangle-Y\right)^2\bigg\vert \mathbf{X}\right]\right]\\
&= \mathbb{E}_{P_{\ast}}\left[\mathbb{E}_{P_\ast}\left[ \langle \mathbf{X},\beta_\ast\rangle^2-2\langle \mathbf{X},\beta_\ast\rangle Y+Y^2\bigg\vert \mathbf{X}\right]\right]      \\
&=\mathbb{E}_{P_\ast}\left[\langle \mathbf{X},\beta_\ast\rangle^2\right]-2 \mathbb{E}_{P_\ast}\left[\langle \mathbf{X},\beta_\ast\rangle Y\vert \mathbf{X}\right]+ \mathbb{E}_{P_\ast}\left[ Y^2\vert \mathbf{X}\right]\\
&=\Var(Y\vert\mathbf{X})=\sigma^2.
\end{aligned}
\end{equation*} 

Thus, we can have that 
\[H(\beta_{\ast})=\tau\sigma\frac{\beta_{\ast}}{\Vert\beta_\ast\Vert_2}.\]
\end{proof}
\subsection{Proof of Theorem \ref{unbiasedglm}}
\begin{proof}
    Theorem \ref{theorem2} indicates that it suffices to show  the gradient of $C_n(\mathbf{z})^{-1}H_n(\mathbf{z})$ at $\beta_\ast$ is bounded in probability.


In the Poisson regression, 
    \[C_n(\mathbf{z})^{-1}H_n(\mathbf{z})= \tau\sqrt{\mathbb{E}_{\mathbb{P}_n}[e^{\langle \mathbf{X},\mathbf{z}\rangle}]}\left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^{\top}  e^{\langle\mathbf{X},\mathbf{z}\rangle}
\right]\right)^{-1}\frac{\mathbf{z}}{\Vert\mathbf{z}\Vert_2},\] 
      indicating that 
      \begin{equation*} 
      \begin{aligned}
      &\frac{\partial C_n(\mathbf{z})^{-1}H_n(\mathbf{z})}{\partial \mathbf{z}}\Bigg\vert_{\mathbf{z}=\beta_\ast}\\
      &=\tau\sqrt{\mathbb{E}_{\mathbb{P}_n}[e^{\langle \mathbf{X},\beta_\ast\rangle}]}\left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^{\top}  e^{\langle\mathbf{X},\beta_\ast\rangle}
\right]\right)^{-1}\mathbb{E}_{\mathbb{P}_n}\left[\Vert\mathbf{X}\Vert_2^2 e^{\langle\mathbf{X},\beta_\ast\rangle}\mathbf{X}\right]\left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^{\top}  e^{\langle\mathbf{X},\beta_\ast\rangle}
\right]\right)^{-1}\frac{\beta_\ast^\top}{\Vert\beta_\ast\Vert_2}\\
&+\tau \left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^{\top}  e^{\langle\mathbf{X},\beta_\ast\rangle}
\right]\right)^{-1}\frac{\mathbb{E}_{\mathbb{P}_n}[\mathbf{X}e^{\langle\mathbf{X},\beta_\ast\rangle}]}{\sqrt{\mathbb{E}_{\mathbb{P}_n}[e^{\langle\mathbf{X},\beta_\ast\rangle}]}}
\frac{\beta_\ast^\top}{\Vert\beta_\ast\Vert_2}\\
&+\tau\sqrt{\mathbb{E}_{\mathbb{P}_n}[e^{\langle \mathbf{X},\beta_\ast\rangle}]}\left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^{\top}  e^{\langle\mathbf{X},\beta_\ast\rangle}
\right]\right)^{-1}\frac{\Vert\beta_\ast\Vert_2^2 I_d-\beta_\ast\beta_\ast^\top}{\Vert\beta_\ast\Vert_2^3}
     \end{aligned}
      \end{equation*}

In the logistic regression, 
\[C_n(\mathbf{z})^{-1}H_n(\mathbf{z})=\tau\sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\frac{e^{\langle\mathbf{X},\mathbf{z}\rangle}}{\left(1+e^{\langle\mathbf{X},\mathbf{z}\rangle}\right)^2}\right]}\left(\mathbb{E}_{\mathbb{P}_n}\left[\frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\mathbf{z}\rangle}}{\left(1+e^{\langle\mathbf{X},\mathbf{z}\rangle}\right)^2}\right]\right)^{-1}\frac{\mathbf{z}}{\Vert\mathbf{z}\Vert_2},\]
indicating that 
 \begin{equation*} 
      \begin{aligned}
      &\frac{\partial C_n(\mathbf{z})^{-1}H_n(\mathbf{z})}{\partial \mathbf{z}}\Bigg\vert_{\mathbf{z}=\beta_\ast}\\
      &=\tau\sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\frac{e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^2}\right]}\left(\mathbb{E}_{\mathbb{P}_n}\left[\frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^2}\right]\right)^{-1} \left(\mathbb{E}_{\mathbb{P}_n}\left[\Vert\mathbf{X}\Vert_2^2\frac{e^{\langle\mathbf{X},\mathbf{z}\rangle}-e^{2\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^3}\mathbf{X}\right]\right)\\    &\left(\mathbb{E}_{\mathbb{P}_n}\left[\frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^2}\right]\right)^{-1}\frac{\beta_\ast^\top}{\Vert\beta_\ast\Vert_2}\\
&+\tau\left(\mathbb{E}_{\mathbb{P}_n}\left[\frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^2}\right]\right)^{-1}\frac{\mathbb{E}_{\mathbb{P}_n}\left[\frac{e^{\langle\mathbf{X},\beta_\ast\rangle}-e^{2\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^3}\mathbf{X}\right]}{\sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\frac{e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^2}\right]}}
\frac{\beta_\ast^\top}{\Vert\beta_\ast\Vert_2}\\
&+\tau\sqrt{\mathbb{E}_{\mathbb{P}_n}\left[\frac{e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^2}\right]}\left(\mathbb{E}_{\mathbb{P}_n}\left[\frac{\mathbf{X}\mathbf{X}^{\top}e^{\langle\mathbf{X},\beta_\ast\rangle}}{\left(1+e^{\langle\mathbf{X},\beta_\ast\rangle}\right)^2}\right]\right)^{-1}\frac{\Vert\beta_\ast\Vert_2^2 I_d-\beta_\ast\beta_\ast^\top}{\Vert\beta_\ast\Vert_2^3}
     \end{aligned}
      \end{equation*}





    In the linear regression, 
    \[C_n(\mathbf{z})^{-1}H_n(\mathbf{z})= \tau\sigma\left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^\top\right]\right)^{-1} \frac{\mathbf{z}}{\Vert \mathbf{z}\Vert_2},\]
    indicating that 
    \[ \frac{\partial C_n(\mathbf{z})^{-1}H_n(\mathbf{z})}{\partial \mathbf{z}}\Bigg\vert_{\mathbf{z}=\beta_\ast}=\tau\sigma\left(\mathbb{E}_{\mathbb{P}_n}\left[ \mathbf{X}\mathbf{X}^\top\right]\right)^{-1} \frac{\Vert\beta_\ast\Vert_2^2 I_d-\beta_\ast\beta_\ast^\top}{\Vert\beta_\ast\Vert_2^3}\]
    
It follows from the law of large numbers and the assumptions in Proposition \ref{logistic}-\ref{lineartheorem}
that $\frac{\partial C_n(\mathbf{z})^{-1}H_n(\mathbf{z})}{\partial \mathbf{z}}$ is bounded in probability at $\beta_\ast$ in the logistic regression, Poisson regression and linear regression.

\end{proof}
\newpage
\bibliography{sample}
\end{document}
