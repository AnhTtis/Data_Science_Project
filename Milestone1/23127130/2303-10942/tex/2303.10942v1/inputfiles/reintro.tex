End-to-end (E2E) speech recognition models can be improved on a domain when they are shown text from that domain.
While there have been works that do so by training parts of the model on external text~\cite{wang21t_interspeech,yusuf22usted,thomas2022integrating}, the most common method of incorporating text, unless precluded by computational constraints, is still fusion with language models (LM)~\cite{gulcehre2015using,Chorowski2017,variani2020hybrid,mcdermott2019density,meng2021internal} since they can be swapped at inference time.

Nevertheless, even LMs, especially neural LMs, can be unwieldy to change to match user interest.
It is common for users of voice assistants and other speech technologies to use words and phrases associated with trending topics such as sporting events, album releases, pandemics etc.
To contend with these surges in user interest, these ASR systems must be able to rapidly assimilate words of interest and also gracefully discard them as such ephemeral interest wanes.
Although it is possible to obtain relevant text from internet forums or news articles, incorporating them into the ASR LM requires retraining the entire LM or training a separate LM for each trending topic.
Moreover, LMs struggle with proper nouns and other named entities which are of the most interest because such entities, by nature rare and diffuse in training data, are assigned low likelihoods by LMs which store all information in their parameters.

This has sparked interest in biasing methods which attempt to boost the likelihoods of a catalog of entities.
These include finite state transducer (FST)-based methods which compose the ASR output with an FST with negative-cost arcs carrying the entities to be boosted~\cite{zhao2019shallow,gourav2021personalization,kocour2021interspeech,pundak2018deep}, and deep-biasing methods which introduce a trainable adapter into the ASR model, with an attention mechanism to select the right entity to boost~\cite{pundak2018deep,jain20_interspeech,feng2021,sathyendra2022contextual,dingliwal2022}.
However, both are more suited to catalogs of limited size (up to a few hundred at a time), such as contact names and song playlists, rather than the large catalogs necessary to cover multiple trending topics at the same time.
FSTs for instance boost all items in the catalog with predefined weights making it hard to control what gets boosted as the catalog size increases.
For deep-biasing, the limitation is due to the smearing of attention weights as the catalog size increases, as well as the increasing computational overhead of multihead attention.
Therefore, it remains a challenge to have a rapidly adaptable, computationally efficient way to bias ASR towards large lists of phrases--possibly millions of tokens--at a time.

Inspired by the success of retrieval mechanisms in language modeling~\cite{khandelwal2020generalization,guu2020retrieval,he2021efficient}, we propose  augmenting an RNN transducer (RNN-T)-based ASR model with a retriever which searches in an external datastore for candidate continuations of a partial ASR hypothesis.
The RNN-T's encoder output then attends to encodings of the retrieved continuations, and the attention output is summed to the encoder output before begin fed into the joiner.

Experiments on the Squad~\cite{rajpurkar-etal-2018-know} and DeepMind Question-Answering~\cite{hermann2015teaching} datasets show that a strong RNN-T baseline can be improved by retrieving from datastores that contains related text, even when the datastores also have millions of tokens of unrelated, distracting content.
Furthermore, retrieval can be complemented by shallow fusion as the latter performs better on recognition of common words while retrieval performs better for named entities.