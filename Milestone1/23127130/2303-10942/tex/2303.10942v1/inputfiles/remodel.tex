\subsection{RNN Transducer}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/re_re_retrieval_asr_plus_datastore.pdf}
    \caption{RNN-T modified to use retrieval from a datastore. ${L}(\dots \text{\textit{xyz}})$ denotes the retrieval LM encoding of some sentence ending in \textit{xyz}.}
    \label{fig:retrieve_and_adapt}
    \vspace{-2.5mm}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/re_retrieve_and_adapt.pdf}
    \caption{The retrieval LM computes an embedding for previously predicted symbols and retrieves the k nearest neighbors of this embedding are from a datastore. The continuations (italicized in red font) and log Euclidean distances of the neighbors are transformed by an encoding network and the resulting vectors are blended by an attention mechanism whose output is used to modify the original RNN-T encoder output.}
    \label{fig:retrieve_and_adapt_expand}
    \vspace{-2.5mm}
\end{figure*}
The model we propose is built on the RNN transducer~\cite{graves2012sequence}. The transducer is an end-to-end ASR model composed of three parts: the encoder, the prediction network and the joiner.

The encoder is a recurrent neural network which encodes a sequence of audio frames $(\Matrix{x}_1, \dots, \Matrix{x}_N)$ into hidden states $(\Matrix{h}_1, \dots, \Matrix{h}_N)$:
\useshortskip
\begin{align}
    \Matrix{h}_n = f^{enc}(\Matrix{x}_n, \Matrix{h}_{n-1}),
    \label{eqn:transcription}
\end{align}

The prediction network is a recurrent network which encodes the previously output non-blank tokens $y_{<u}$:
\useshortskip
\begin{align}
\Matrix{g}_u = f^{pred}(y_{u-1}, \Matrix{g}_{u-1}),
\label{eqn:prediction}
\end{align}

The joiner computes a joint embedding from the two outputs:
\useshortskip
\begin{align}
    \Matrix{z}_{n,u} = \phi(\Matrix{U} \Matrix{h}_n + \Matrix{V} \Matrix{g}_u + \Matrix{b_1}).
    \label{eqn:joiner}
\end{align}
$\Matrix{U}$, $\Matrix{V}$ and $\Matrix{b}_1$ are trainable parameters and $\phi$ is the hyperbolic tangent function.
The joint embedding is then used to compute a probability distribution over all tokens plus the blank token for alignment:
\useshortskip
\begin{align}
    p(y|n, u) = \sigma(\Matrix{W} \Matrix{z}_{n,u} + \Matrix{b}_2).
\end{align}
$\Matrix{W}$ and $\Matrix{b}_2$ are trainable parameters and $\sigma$ is the softmax function.

\subsection{Retrieval-augmented RNN-T}
Figure~\ref{fig:retrieve_and_adapt} depicts the modified RNN-T structure that we propose and
Figure~\ref{fig:retrieve_and_adapt_expand} illustrates the modifications in more detail.
Our modifications comprise a retriever which finds potential continuations for the RNN-T's current output from external text and an adapter which uses those continuations to bias the RNN-T's subsequent outputs.

\subsubsection{Retriever}
The retriever, depicted in pink in Figure~\ref{fig:retrieve_and_adapt_expand}, is based on the premise that embeddings generated by a pretrained neural language model for two similar phrases are closer in Euclidean space than those of two random phrases.
Therefore, to find potential continuations for any phrase (partial ASR hypotheses in our case) in a text corpus, we need to find phrases in that corpus that have similar embeddings to our phrase of interest, and return their continuations.

At the heart of our retriever is a pretrained LSTM LM which is used to generate embeddings for retrieval.\footnote{We use an LSTM instead of the transformers used in prior works on retrieval LM due to practical latency and memory considerations. Note that the retrieval LM needs not be trained on the text from which we retrieve.
In fact, in all our experiments, we use the same pretrained LSTM LM regardless of the adaptation text.
}
First we create a datastore for an adaptation text from which we intend to retrieve.
To do this, each sentence in the corpus is passed through the LM;
the LSTM's hidden state at each step is added as a key to the datastore,
with corresponding value comprising the input tokens to the next $t$ steps (we set $t=2$ in this paper).
By repeating this procedure for all sentences in the text, we get a key-value store, whose keys are the LM embeddings of phrases in the text, and whose values are the $t$-token long continuations of each key phrase.

To obtain candidate continuations of a partial ASR hypothesis during RNN-T decoding, we encode it with the retrieval LM and find the k nearest neighbors by Euclidean distance from the datastore.
Note that the query to the retrieval is the same as the input to the prediction network, i.e., the sequence of non-blank tokens ($y_{<u}$).
The retrieved continuations are then passed to the adapter along with the logarithms of the Euclidean distances between each key and the querying embedding.

\subsubsection{Adapter}
\label{subsection:adapter}
Having retrieved the k nearest neighbors, the question remains how best to integrate them back into the ASR.
Adopting the framework used for contextual biasing in~\cite{sathyendra2022contextual}, we introduce a trainable adapter to bias the RNN-T's encoder output before feeding it into the joint network.
The adapter, depicted in green in~Figure~\ref{fig:retrieve_and_adapt_expand}, comprises a recurrent encoder and a multihead dot-product attention mechanism.

The adapter encoder has an embedding layer which converts the tokens in each candidate continuation into dense form.
This is followed by an LSTM whose hidden state at the last step is taken as a fixed-length representation of that candidate.
The corresponding log-distance from the retriever is concatenated to give the model a clue about the relevance of each candidate, and this vector is further transformed by a feedforward network to get a final representation.

The multihead attention is used to select from the representations of the candidates.
The attention query is an affine projection of the encoder output $\Matrix{h}_n$\footnote{We also tried using $\Matrix{g}_u$ as the attention query. We found that while using $\Matrix{g}_u$ is computationally cheaper, using $\Matrix{h}_n$ results in better ASR performance.} and its keys and values are projections of the candidates' encodings.
Finally, the resulting context vector is added to the transcription output before passing the sum to the joiner.

In effect, the adapter modifies Equation~\ref{eqn:joiner} to:
\useshortskip
\begin{align}
    \Matrix{z}_{n,u} = \phi\Bigl(\Matrix{U} \bigl(\Matrix{h}_{n} + \sum_{k=0}^{K} \alpha_{n, u, k} \Matrix{c}_{u, k}\bigr) + \Matrix{V} \Matrix{g}_u + \Matrix{b}_1\Bigr),
    \label{eqn:adapt}
\end{align} 
where $K$ is the number of retrieved candidates and is a hyper-parameter,
$\Matrix{c}_{u, 1}, \dots, \Matrix{c}_{u, K}$ are the values of the attention mechanism computed from the retrieved candidates (by searching $y_{< u}$ in the datastore),
$\Matrix{c}_{u, 0} = \Matrix{c}$ is an extra trainable ``no-bias" embedding intended to give the model an option when all retrieved candidates are incorrect, and
$\alpha_{n, u, k}$ is the attention weight between $\Matrix{h}_n$ and $\Matrix{c}_{u, k}$.