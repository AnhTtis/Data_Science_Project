In this work, we have proposed augmenting an RNN-T based ASR model with a retrieval mechanism, which searches an external datastore for potential continuations of partial ASR hypotheses.
We show that biasing subsequent decoding steps with these continuations significantly improves ASR performance, especially on named entities, when the datastore contains relevant text.
We further show that retrieval can be complemented by a conventional shallow fusion LM,
as using the two in tandem results in further improvements.

Avenues for future work include replacing the retrieval LM with a model trained explicitly for retrieval,
further reducing performance degradation when the datastore and test set are unrelated,
% attending to candidate continuations retrieved for other hypotheses in the decoding beam,
and improving efficiency by exploring ways to reduce retrieval frequency.
% A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. A conclusion is only a conlusion if it's this long. 