This appendix contains experimental results that were not included in the main paper due to ICASSP space constraints.

\subsection{Impact of including DQA training data in baseline RNN-T training}
Table~\ref{tab:to_finetune_or_not_and_where} shows the impact of including DQA data when training the RNN-T baseline.
The first group of results uses Librispeech and DQA data for training the baseline RNN-T (these are the results reported in the main body of the paper), while the second group has a baseline trained on Librispeech data only.
As before, the retriever is always trained on the Squad training set.

Adding DQA training data improves the baseline considerably, since it better matches the acoustics of the test sets.
Therefore, we took this baseline in the paper.
Nevertheless, for the baseline trained with only Librispeech data, retrieval leads to significant reductions in word error rates--even more so than for the other baseline.
Note that this is despite the higher retrieval quality discrepancy between training and inference since higher word error rates lead to more noise in the retrieved tokens.

All subsequence subsections report results on the validation sets with the baseline trained on Librispeech only.
\begin{table}[b]
    \centering
    \setlength\tabcolsep{4.5pt}
    \caption{WER with retrieval on baselines trained with or without DQA training data. L + D denotes the baseline trained with a mix of Librispeech and DQA, while L denotes the baseline trained with only Librispeech data. $^*$ denotes the setting used in the main paper.}
    \begin{tabular}{lccccc}
        \toprule
        Datastore & Baseline & Squad-V & Squad-T & DQA-V & DQA-T  \\
        \midrule
        None & L + D$^*$ & 15.8 & 15.2 & 14.0 & 13.8 \\
        Fixed emb. & L + D$^*$ & 14.4 & 13.8 & 14.7 & 14.7 \\
        Match & L + D$^*$ & 11.9 & 11.1 & 12.7 & 12.8 \\
        All & L + D$^*$ & 12.3 & 11.9 & 13.3 & 13.3 \\
        \midrule
        None & L & 24.7 & 23.5 & 35.2 & 34.3 \\
        Fixed emb. & L & 20.4 & 18.9 & 33.1 & 33.1 \\
        Match & L & 15.5 & 14.2 & 27.4 & 26.4 \\
        All & L & 16.2 & 14.8 & 27.1 & 27.1 \\
        \bottomrule
    \end{tabular}
    \label{tab:to_finetune_or_not_and_where}
\end{table}

\subsection{Adapting encoder output vs prediction network output}
In our experiments, we used the retrieved tokens to adapt the encoder output.
We also experiment with adapting the prediction network output instead.
Essentially, instead of~\eqref{eqn:adapt}, we use:
\begin{align}
    \Matrix{z}_{n,u} = \phi\Bigl(\Matrix{U}\Matrix{h}_{n} + \Matrix{V}\bigl( \Matrix{g}_u + \sum_{k=0}^{K} \alpha_{u, k} \Matrix{c}_{u, k} \bigr) + \Matrix{b}_1\Bigr),
    \label{eqn:adapt_prediction}
\end{align} 
where the attention weights $\{\alpha_{u, k}\}$ are computed with the prediction network output $\Matrix{g}_u$ as the attention query.
This leads to a computationally cheaper system than the one querying with the encoder output--note the absence of a temporal index ($n$) in the attention weights here compared to $\{\alpha_{n, u, k}\}$ in ~\eqref{eqn:adapt}.

Table~\ref{tab:what_to_adapt} shows the result of adapting the various components on the validation sets while also varying the datastore from which continuations are retrieved.
When the adaption involves a single trained embedding, both choices result in almost identical performance.
With actual retrieval, while both adaption choices significantly improve the baseline, adapting the encoder results in much lower WER compared to adapting the prediction network output.
There is therefore a trade-off between accuracy and computational cost as adapting the encoder is better in terms of WER but much costlier due to the extra attention dimension.
\begin{table}[t]
    \centering
    \setlength\tabcolsep{4.5pt}
    \caption{WER impact of adapting the encoder or prediction network outputs. Results with ``f.e." denote the baseline which use fixed embeddings instead of actual retrieval. $^*$ denotes the setting used in the rest of the paper.}
    \begin{tabular}{lccccc}
    \toprule
         Test set & \multicolumn{2}{c}{Squad-V} && \multicolumn{2}{c}{DQA-V} \\
         Datastore & Squad-V & DQA-V && Squad-V & DQA-V \\
         \midrule
         Adaptee \\
         \midrule
         - & 24.7 & 24.7 && 35.2 & 35.2 \\
         \midrule
         Prediction f.e. & 20.4 & 20.4 && 33.6 & 33.6 \\
         Encoder f.e. & 20.4 & 20.4 && 33.1 & 33.1 \\
         \midrule
         Prediction & 16.7 & 18.7 && 31.3 & 28.6 \\
         Encoder$^*$ & 15.5 & 18.1 && 29.5 & 27.4 \\
         \bottomrule
    \end{tabular}
    \label{tab:what_to_adapt}
\end{table}

\subsection{Impact of retrieval noise in training}
\begin{table}[b]
    \centering
    \setlength\tabcolsep{4.5pt}
    \caption{WER as retrieval noise during adapter training is varied. $^*$~denotes the setting used in the rest of the paper.}
    \begin{tabular}{lcccccc}
    \toprule
         Test set && \multicolumn{2}{c}{Squad-V} && \multicolumn{2}{c}{DQA-V} \\
         Datastore && Squad-V & DQA-V && Squad-V & DQA-V \\
         \midrule
         Prob. & +Libri \\
         \midrule
         0 &\ding{55}& 15.6 & 20.3 && 31.0 & 28.0 \\
         0.1 &\ding{55}& 15.0 & 18.3 && 30.0 & 26.6 \\
         0.3 &\ding{55}& 14.6 & 16.9 && 28.9 & 26.7 \\
         0.5 &\ding{55}& 15.0 & 16.5 && 28.8 & 26.9 \\
         \midrule
         0 &\ding{51}& 15.5 & 18.9 && 29.9 & 27.0 \\
         0.1$^*$ &\ding{51}& 15.5 & 18.1 && 29.5 & 27.4 \\
         0.3 &\ding{51}& 15.5 & 17.3 && 29.3 & 26.8 \\
         0.5 &\ding{51}& 15.9 & 17.1 && 29.1 & 28.3 \\
         \bottomrule
    \end{tabular}
    \label{tab:retrieval_noise}
\end{table}
As described in the last paragraph of Section~\ref{sec:experiments:datasets}, we add some noise to the adapter training to avoid model overconfidence; we do this by sampling Librispeech training data (for which the training datastore has no relevant entries) with 0.5 probability and retrieving with a further 0.1 probability random entries from the datastore instead of the actual k nearest neighbors. Table~\ref{tab:retrieval_noise} shows the WER impact of these.

First, we note that regardless of whether Librispeech utterances are included in the adapter training, increasing the probability of random retrievals always improves the WER in the cross-retrieval setting, i.e. retrieving from Sqaud-V datastore when decoding DQA-V utterances and vice versa.
Thus, training with random retrieval serves its intended purpose: mitigating the performance degradation when the retrieval datastore is not related to the test set.
The cost of course is that as the noise probability is increased beyond some threshold ($0.3$ in these experiments), the WER in the matched setting--using Squad datastore for Squad test set and DQA for DQA--starts to deteriorate.

Finally, we note that adding noise in the form of Librispeech data also has a similar effect, especially when the probability of random retrieval is at or below $0.1$.
As the probability of random retrieval increases, using Librispeech data starts to hurts performance compared to not using it.

\subsection{Using pretrained encoder in the adapter}
The adapter comprises a recurrent encoder and a multihead attention which are jointly trained.
Here we experiment with using a frozen pretrained encoder, and only train the multihead attention and feedforward part of the encoder.
Specifically, we use the retrieval LSTM LM to encode the continuations.
This significantly improves training throughput (utterances/second) as it allows offline computation of the training embeddings.

Table~\ref{tab:trained_or_frozen} shows a WER comparison of the two approaches.
While using the pretrained encoder works better than the baselines without retrieval, it is significantly worse than retrieval with trained encoder.
Although finetuning the pretrained encoder is another option, it would defeat our purpose of increasing training throughput.
It may still be worth exploring if such a finetuning approach leads to better results or faster convergence.

\begin{table}[t]
    \centering
    \caption{WER comparsion between training the adapter's recurrent encoder or using the retrieval LM for encoding the continuations.}
    \setlength\tabcolsep{4.5pt}
    \begin{tabular}{lccccc}
    \toprule
         Test set & \multicolumn{2}{c}{Squad-V} && \multicolumn{2}{c}{DQA-V} \\
         Datastore & Squad-V & DQA-V && Squad-V & DQA-V \\
         \midrule
         Embedding \\
         \midrule
          - & 24.7 & 24.7 && 35.2 & 35.2 \\
          Fixed embedding & 20.4 & 20.4 && 33.1 & 33.1 \\
          \midrule
          Trained$^*$& 15.5 & 18.1 && 29.5 & 27.4 \\
          Pretrained & 16.9 & 19.6 && 30.6 & 31.1 \\
          \bottomrule
    \end{tabular}
    \label{tab:trained_or_frozen}
\end{table}


\subsection{Number of attention heads}
In our experiments, we have set the number of attention heads in the adapter to 2.
Table~\ref{tab:num_heads} compares this to setting the it to 1.

Setting the number of heads to 1 slightly degrades the word error rate.
However, this may be worth it due the computational savings, especially at inference time where it essentially halves the computational cost of the adapter--since the cost of embedding the continuations can be amortized away by offline pre-computation.
\begin{table}[t]
    \centering
    \def\arraystretch{0.88}
    \setlength\tabcolsep{4.5pt}
    \caption{WER as the number of attention heads is reduced}
    \begin{tabular}{lccccc}
    \toprule
         Test set & \multicolumn{2}{c}{Squad} && \multicolumn{2}{c}{DQA} \\
         Datastore & Squad & DQA && Squad & DQA \\
         \midrule
         Num heads \\
         \midrule
         1 & 15.9 & 18.5 && 30.2 & 27.4 \\
         2$^*$ & 15.5 & 18.1 && 29.5 & 27.4 \\
         \bottomrule
    \end{tabular}
    \label{tab:num_heads}
\end{table}

\subsection{Impact of varying the number of retrieved tokens}
\begin{table}[t]
    \centering
    \setlength\tabcolsep{4.5pt}
    \caption{WER on the Squad-V set as K is varied while retrieving from the Squad-V datastore.}
    \npdecimalsign{.}
    \nprounddigits{2}
    \begin{tabular}{lccccccc}
    \toprule
         Test & 1 & 2 & 4 & 8 & 16 & 32 & 64  \\
         Train \\
         \midrule
         1 & 16.6 & 16.5 & 16.5 & 17.1 & 18.1 & 19.5 & 20.9 \\
         2 & 17.9 & 16.6 & 16.1 & 16.2 & 16.4 & 16.8 & 17.3 \\
         4 & 19.2 & 17.5 & 16.1 & 15.8 & 15.8 & 15.9 & 16.0 \\
         8 & 19.9 & 18.6 & 16.8 & 15.9 & 15.7 & 15.6 & 15.4 \\
         16 & 24.2 & 19.5 & 17.4 & 16.3 & 15.5 & 15.5 & 15.4 \\
         \bottomrule
    \end{tabular}
    \npnoround
    \label{tab:knn_squad_squad}
\end{table}

\begin{table}[t]
    \centering
    \setlength\tabcolsep{4.5pt}
    \caption{WER on the Squad-V set as K is varied while retrieving from the DQA-V datastore.}
    \begin{tabular}{lccccccc}
    \toprule
         Test & 1 & 2 & 4 & 8 & 16 & 32 & 64  \\
         Train \\
         \midrule
         1 & 17.9 & 17.6 & 17.5 & 18.0 & 18.3 & 18.7 & 19.0 \\
         2 & 18.7 & 17.9 & 17.6 & 17.5 & 17.5 & 17.7 & 17.9 \\
         4 & 19.6 & 19.0 & 18.3 & 17.9 & 17.5 & 17.5 & 17.6 \\
         8 & 21.9 & 20.7 & 19.7 & 17.9 & 17.5 & 17.1 & 17.0 \\
         16 & 22.3 & 21.4 & 19.5 & 18.6 & 18.1 & 17.6 & 17.2 \\
         \bottomrule
    \end{tabular}
    \label{tab:knn_squad_dqa}
\end{table}


\begin{table}[t!]
    \centering
    \setlength\tabcolsep{4.5pt}
    \caption{WER on the DQA-V set as K is varied while retrieving from the Squad-V datastore.}
    \begin{tabular}{lccccccc}
    \toprule
         Test & 1 & 2 & 4 & 8 & 16 & 32 & 64  \\
         Train \\
         \midrule
         1 & 30.5 & 30.5 & 30.3 & 30.5 & 31.6 & 32.6 & 33.8 \\
         2 & 32.1 & 30.6 & 30.5 & 30.6 & 30.7 & 30.8 & 31.5 \\
         4 & 32.5 & 31.0 & 30.1 & 29.8 & 29.4 & 29.5 & 29.5 \\
         8 & 33.1 & 31.9 & 30.3 & 29.7 & 29.5 & 28.9 & 28.8 \\
         16 & 35.7 & 33.2 & 31.4 & 30.5 & 29.5 & 29.0 & 29.0 \\
         \bottomrule
    \end{tabular}
    \label{tab:knn_dqa_squad}
\end{table}


\begin{table}[t!]
    \centering
    \setlength\tabcolsep{4.5pt}
    \caption{WER on the DQA-V set as K is varied while retrieving from the DQA-V datastore.}
    \begin{tabular}{lccccccc}
    \toprule
         Test & 1 & 2 & 4 & 8 & 16 & 32 & 64  \\
         Train \\
         \midrule
         1 & 28.6 & 28.8 & 28.5 & 29.1 & 29.0 & 29.7 & 30.4 \\
         2 & 29.9 & 29.3 & 28.5 & 28.4 & 28.2 & 28.3 & 28.5\\
         4 & 29.5 & 28.6 & 28.1 & 27.6 & 27.2 & 27.0 & 26.9 \\
         8 & 30.1 & 28.9 & 28.0 & 27.1 & 26.8 & 26.7 & 26.5 \\
         16 & 31.1 & 30.3 & 28.5 & 27.9 & 27.4 & 27.2 & 27.2 \\
         \bottomrule
    \end{tabular}
    \label{tab:knn_dqa_dqa}
\end{table}
Tables~\ref{tab:knn_squad_squad}~to~\ref{tab:knn_dqa_dqa} show the word error rates for different datastore/test set combinations as the number of retrieved continuations is varied at training time and during inference.

The best word error rates tend towards the bottom right corner of each table--where $K$ is high for both training and inference. The worst are on the bottom left followed by the top-right--where there is severe mismatch between the number of $K$ at training time and at test time. The top-left corners--where $K$ is low for both training and inference--have word error rates in between.

Generally, the WER improves as $K$ is increased at inference time up to around 4 times the setting during training after which it starts to degrade.
Conversely, performance degrades rapidly--in some cases, approaching the baseline performance--when $K$ is set low during inference relative to training.

This indicates that $K$ should be set at training time to match the intended setting for inference to avoid degradation in performance.
An alternative training strategy would be not to fix $K$ during training but rather to sample a different $K$ for, say, each training batch.
