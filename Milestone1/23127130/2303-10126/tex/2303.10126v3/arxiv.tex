\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{booktabs}
\definecolor{mygray}{gray}{.9}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{IRGen: Generative Modeling for Image Retrieval}

\author{Yidan Zhang$^{1}$\thanks{Equal contribution. This work was done when Yidan Zhang was an intern at Microsoft Research, Beijing.}~, Ting Zhang$^{2}$\footnotemark[1]~,  Dong Chen$^{2}$,  Yujing Wang$^{2}$ , Qi Chen$^{2}$, Xing Xie$^{2}$,\\  Hao Sun$^{2}$, Weiwei Deng$^{2}$,   Qi Zhang$^{2}$, Fan Yang$^{2}$, Mao Yang$^{2}$, Qingmin Liao$^{1}$, Baining Guo$^{2}$\\
$^{1}$Tsinghua University  ~~~
$^{2}$Microsoft
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
While generative modeling has been ubiquitous in natural language processing and computer vision, its application to image retrieval remains unexplored.
%Todayâ€™s image retrieval system generally consists of two stages, feature representation learning and Approximate Nearest Neighbor search, and research efforts are devoted to either single stage without considering the inherent and deep connection between the two stages. 
In this paper, we recast image retrieval as a form of generative modeling by employing a sequence-to-sequence model, contributing to the current unified theme.  Our framework, IRGen, is a unified model that enables end-to-end differentiable search, thus achieving superior performance thanks to direct optimization. While developing IRGen we tackle the key technical challenge of converting an image into quite a short sequence of semantic units in order to enable efficient and effective retrieval. Empirical experiments demonstrate that our model yields significant improvement over three commonly used benchmarks, for example, 20.2\% higher than the best baseline method in precision@10 on In-shop dataset with comparable recall@10 score.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\label{sec:intro}

Generative modeling has made significant progress in a wide range of tasks including machine translation~\cite{vaswani2017attention}, conversational modeling~\cite{devlin2018bert,brown2020language,radford2019language,radford2018improving,ouyang2022training,adiwardana2020towards},  image captioning~\cite{yu2022coca}, image classification~\cite{chen2020generative}, text-to-image synthesis~\cite{ramesh2021zero, ramesh2022hierarchical,wu2022nuwa,ding2022cogview2}, and many more. Originating from language and then expanding to other modalities with specially designed tokenizers,
such a universal modeling approach provides a promising direction for unifying different tasks into a versatile pretrained model, which has attracted widespread attention~\cite{yu2022coca,alayrac2022flamingo,wang2022image,ouyang2022training,li2023blip}. This paper aims to take the unified trend one step further and investigates generative modeling for an unexplored area, image retrieval. 


In this paper, we treat image retrieval as a form of generative modeling and make use of standard Transformer architecture, as in GPT~\cite{brown2020language,radford2019language,radford2018improving}, to enable end-to-end differentiable search. Our model, IRGen, 
%We thus call the resulting model as IRGPT, with the hope of revolutionizing Image Retrieval using GPT-like generative models.
is a sequence-to-sequence model 
that outputs corresponding nearest neighbors directly from a given query image. 
%ARIR is based on the Transformer architecture~\cite{vaswani2017attention}, which has provided remarkable performance on several tasks including machine translation~\cite{vaswani2017attention}, conversational modeling~\cite{devlin2018bert,brown2020language,radford2019language,radford2018improving,ouyang2022training,adiwardana2020towards},  image captioning~\cite{yu2022coca}, image classification~\cite{dosovitskiy2020image,he2022masked,dong2021peco,bao2021beit}, text-to-image synthesis~\cite{ramesh2021zero, ramesh2022hierarchical,wu2022nuwa,ding2022cogview2}, and many more.
Specifically, the model takes a query image as input 
%to an encoder to get the query embedding, 
and autoregressively predicts discrete visual tokens, which are considered as the identifier of an image.
The predicted visual tokens are supposed to point to the query image's nearest neighbor. 
% During inference, the top $K$ nearest neighbors can be retrieved through beam search using the decoder. 



\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/irgen}}
\caption{Illustrating (a) conventional image search pipeline consisting of two disconnected stages: feature extraction and approximate nearest neighbor (ANN) search, and (b) our IRGen offering end-to-end retrieval thanks to generative modeling.}
\label{fig:frame}
\end{center}
\vskip -0.4in
\end{figure}




IRGen 
%sets a new state-of-the-art record across a wide range of image retrieval datasets thanks to the end-to-end differentiable search manner. It 
can be trained directly from the final retrieval target starting with raw images, which is
essentially different from conventional image retrieval.
Figure~\ref{fig:frame} illustrates the core difference.
In practice, the most widely used retrieval systems consist of two stages:
%On the other hand, although image retrieval is a fundamental and longstanding research topic, there exists a gap which has been overlooked for a long time. Specifically, a practical retrieval system generally consists of two stages: 
feature representation learning~\cite{el2021training,liu2022nonuniform,lee2022correlation,cao2020unifying,simeoni2019local,tan2021instance,teichmann2019detect,yang2021dolg} and Approximate Nearest Neighbor (ANN) search~\cite{babenko2014inverted, johnson2019billion, guo2020accelerating,jayaram2019diskann,ren2020hm,chen2021spann}. 
% feature representation learning~\cite{noh2017large,xiao2016learning, zhai2018classification, zhou2019omni,yuan2020defense,jun2019combination,el2021training,wieczorek2020strong, lagunes2020centroids, wen2016discriminative,gong2012iterative,jegou2010product,cao2016deep,li2020push,liu2022nonuniform,liao2020sparse} and Approximate Nearest Neighbor (ANN) search~\cite{babenko2014inverted,jegou2011searching, johnson2019billion, guo2020accelerating, chen2021spann,jegou2010product,ge2013optimized,norouzi2013cartesian,guo2020accelerating,malkov2018efficient, ChenW18,chen2021spann,jayaram2019diskann,ren2020hm}. 
Most image retrieval methods focus only on one individual  stage while ignoring the fact that both stages are inherently and deeply connected in actual service. Thus, the practical system often requires careful per-task hyperparameter tuning to make the most out of the coordination of the feature extraction and ANN search.
While recent progress~\cite{gao2020deep,de2020autoregressive,wang2022neural,tay2022transformer} have been made towards end-to-end search
in the scenario of recommendation, entity retrieval and document retrieval, 
little has been done for image retrieval.
%directly copying their strategies gets discouraging outcomes.
%They lack the key component of converting an image into a sequence of discrete tokens, a critical challenge in unifying different tasks involving different modalities.








%Image retrieval has been a fundamental and longstanding research topic in computer vision as well as multimedia. The goal is to find similar images to a given query image from a large database (gallery) which may contain thousands, millions or even billions of images.
%It plays an instrumental role in a wide range of applications such as retail product search, landmark retrieval, visual search engine, social media platforms and many others.
%The widely used pipeline for image retrieval generally consists of two stages:  feature representation learning
%%from early hand-crafted feature engineering to deep learning based features;
%and Approximate Nearest Neighbor (ANN) search.
%based on some distance measures over the features. 

%Accordingly, recent image retrieval methods can be roughly divided into two categories. (i) Deep metric learning aims to learn effective feature representation. Along this direction, a lot of works explore various loss functions for training retrieval models, such as classification loss~\cite{noh2017large,xiao2016learning, zhai2018classification, zhou2019omni}, ranking loss~\cite{yuan2020defense}, contrastive loss~\cite{jun2019combination,el2021training}, center loss~\cite{wieczorek2020strong, lagunes2020centroids, wen2016discriminative}, and meantime numerous efforts have also studied compact feature learning through hashing and quantization techniques~\cite{gong2012iterative,jegou2010product,cao2016deep,li2020push,liu2022nonuniform,liao2020sparse} for memory storage reduction and fast distance calculation. 
%(ii) On the other hand, ANN algorithms target from another perspective, improving search efficiency while maintain the search accuracy based on indexing, such as partition-based methods~\cite{babenko2014inverted,jegou2011searching, johnson2019billion, guo2020accelerating, chen2021spann,jegou2010product,ge2013optimized,norouzi2013cartesian,guo2020accelerating}, graph-based methods~\cite{ malkov2018efficient, ChenW18} and hybrid models~\cite{chen2021spann,jayaram2019diskann,ren2020hm}.
%
%Despite the impressive progress has been made, there exists a gap which has been overlooked for a long time. That is, feature representation learning is unaware of the subsequent ANN algorithm and is usually evaluated via linear scan search.
%%, resulting in that the learned feature space may not be optimal for ANN search. 
%Likewise, ANN algorithms are typically designed assuming the database contains feature vectors instead of raw images. While a practical retrieval system usually needs to coordinate two stages. It is often cumbersome to make the most possible out of combining the feature extraction and ANN algorithm.
%Such gap is universal for all kinds of retrieval beyond images. 
%In the scenario of recommendation, entity retrieval and document retrieval,
%there has emerged several studies~\cite{gao2020deep,de2020autoregressive,wang2022neural,tay2022transformer} tackling traditional two-stage pipeline into a unified end-to-end sequence to sequence model, with specific designs, such as docid representation using hierarchical clustering as in~\cite{tay2022transformer,wang2022neural} and prefix-aware weight-adaptive decoder in~\cite{wang2022neural}.
%Inspired by their successes, we experiment with applying autoregressive model to image retrieval, which remains unexplored. 
%However, this is non-trivial as directly copying their strategies gets discouraging outcomes. We conjecture the challenge stems from the way of tokenizing image.
%We therefore carefully design our framework with several key modifications tailored for images and demonstrate that significant improvement can be achieved.

The problem of enabling efficient and effective image retrieval using generative modeling is highly challenging. Two fundamental concerns need to be addressed.
First, autoregressive generative modeling is notable for its slow sampling process due to the inherently sequential nature, thus the run-time cost for retrieval grows at least linearly with respect to the length of a sequence.
Second, from the drastically shortened image identifier, it is particularly difficult to model the semantic relationship between the identifiers. As such, a semantic tokenizer specially designed for image retrieval is an immediate problem. We address both challenges and demonstrate the success of generative modeling for image retrieval.


To enable generative modeling for end-to-end image retrieval, our method first needs to represent the image as a sequence of tokens, namely the image identifier.
While existing image tokenizers~\cite{van2017neural,lee2022autoregressive}
have rapidly developed in the past few years, image tokenizer for image retrieval remains an open problem. We observed that existing image tokenizers, normally designed for image generation task, are not suitable for image retrieval task, and thus lead to poor performance as analyzed in our experiments.
We hence propose 
%For the scalability of IRGen, we maintain the standard Transformer architecture so that it is easy to scale up the model using existing techniques and infrastructures. The remaining key challenge is to find an image identifier\footnote{Without ambiguity, we use image identifier and visual tokens interchangeably.} that facilitates the learning process of the generative model. It is crucial to have a semantic image identifier such that similar images possess similar identifiers. Existing image tokenizers~\cite{van2017neural,lee2022autoregressive}, normally designed for image generation tasks, are not amenable for retrieval, leading to poor performance as we will show in the experiments. Hence for our IRGen, we propose
several key ingredients that (i) inject semantic information by applying image-level supervision rather than low-level pixel supervision, (ii) generate dependent tokens in a sequence by leveraging the recursive property of residual quantization, and (iii) ensure fast inference speed by tremendously reducing the length of the sequence via exploiting the global feature instead of spatial patch embeddings. 
Afterwards, we intentionally adopt the standard Transformer architecture so that it is easy to scale up the model using existing techniques and infrastructures.
%We adopt residual quantization for discretization and term our image tokenizer as RQ-ViT.

The proposed IRGen model sets a new record across a wide range of image retrieval datasets thanks to its end-to-end differentiable search ability, surpassing prior strong competitors by a large margin, even better than linear scan search in some cases. For example, compared with the best baseline method (probably with linear scan search), our model gets 20.2\% improvement in precision@10 on In-shop Clothes dataset~\cite{liu2016deepfashion}, 6.0\% in precision@2 on CUB200~\cite{wah2011caltech} and 2.4\% in precision@2 on Cars196~\cite{krause20133d}.
To evaluate the scalability of our model, we further experiment on million-level datasets, ImageNet~\cite{deng2009imagenet} and Places365~\cite{zhou2017places}, and demonstrate superior performance.


It is our belief that generative models have the potential to revolutionize image retrieval. The applicability of generative modeling in image retrieval task opens up the potential opportunity to unify information retrieval of all modalities. At the technical level, IRGen naturally bridges the aforementioned feature representation learning and approximate search into an end-to-end differentiable model implicitly and effortlessly, allowing optimization directly from the retrieval target. Furthermore, the whole framework is conceptually simple, with all the components based on the standard Transformer, which is known to hold impressive scalability~\cite{du2022glam,chowdhery2022palm,shoeybi2019megatron, xu2021gspmd}. To the best of our knowledge, we are the first to explore generative modeling for image retrieval, expanding the spectrum of generative modeling to a new area. 
%The result is a fundamentally different image retrieval approach with impressive performance on standard benchmarks. 
Along this way, a fundamentally different retrieval scheme is arrived with verified impressive performance on retrieval benchmarks.

% The proposed IRGPT enjoys several benefits.
% First and foremost, it successfully verifies the applicability of generative modeling in image retrieval task, opening up the potential opportunity for unifying all kinds of retrieval.
% Second,
% it naturally bridges the aforementioned feature representation learning and approximate search into an end-to-end differentiable model  implicitly and effortlessly, allowing optimization directly from the retrieval target. Furthermore, the whole framework is conceptually simple with all the components based on standard Transformer, which is known to hold impressive scalability~\cite{du2022glam,chowdhery2022palm,shoeybi2019megatron, xu2021gspmd}.
% Another remarkable aspect of IRGPT is that it surpasses prior strong competitors by a large margin, even better than linear scan search in some cases.
% %This is particularly encouraging thanks to the end-to-end retrieval solution.
% For example, our model gets 22.9\% improvement in precision@10 on In-shop Clothes dataset, 6.0\% in precision@2 on CUB200 dataset and 2.4\% in precision@2 on Cars196 dataset, compared with the best baseline method.


% To the best of our knowledge, we are the first to explore the powerful generative model for image retrieval task, expanding the spectrum of generative modeling to a new research field.
% As such, a fundamentally different retrieval scheme is introduced with verified impressive performance on retrieval benchmarks.
% We hope that our work will contribute the community's progress on current unified trend to also sweep 
% image retrieval, a significant step enabling a single model to handle versatile tasks. 
% (2) We study several discrete image identifiers and design a new image tokenizer in order to provide semantic image identifiers. We show in the ablation analysis that the proposed tokenizer is pivotal to our  model.
% (3) The resulting sequence-to-sequence model, allowing end-to-end optimization directly from the retrieval target, achieves significant superior performance over a number of benchmarks.


\section{Related Work}

\subsection{Image Retrieval}
Image retrieval has been a fundamental problem in the multimedia field as well as in industry. Here we focus on overview over recent research works. Extensive reviews can be found in existing surveys~\cite{smeulders2000content,lew2006content,liu2007survey,zhang2013image,alzu2015semantic,li2016socializing,zhou2017recent}.

\noindent \textbf{Representation learning.} 
Traditionally, hand-crafted features are heuristically designed to describe the image content based on its color~\cite{wengert2011bag,wang2011interactive}, texture~\cite{park2002fast,wang2014content} or shape~\cite{cao2011edgel}.
Typical features include GIST~\cite{siagian2007rapid}, SIFT~\cite{lowe1999object}, SURF~\cite{bay2006surf}, VLAD~\cite{jegou2010aggregating} and so on. 
Recent years have witnessed the explosive research on deep learning based features trained over labeled images. Besides the evolvement of the network architecture designs~\cite{krizhevsky2017imagenet,simonyan2014very,he2016deep,vaswani2017attention,razavian2016visual,sharif2014cnn}, numerous efforts~\cite{noh2017large,yuan2020defense,jun2019combination,wieczorek2020strong,el2021training} have been dedicated to various loss functions including classification loss~\cite{noh2017large,xiao2016learning, zhai2018classification, zhou2019omni}, triplet loss~\cite{yuan2020defense}, contrastive loss~\cite{jun2019combination,el2021training}, center loss~\cite{wieczorek2020strong, lagunes2020centroids, wen2016discriminative} and so on. 
The similarity between features can be calculated through some distance measure or evaluated through re-ranking techniques~\cite{bai2019re,cao2020unifying,revaud2019learning}.

%Although higher dimension leads to better performance, it is always necessary to compress the features for efficient search response. Thereafter, hashing and quantization techniques such as LSH~\cite{indyk1998approximate,andoni2008near}, min-Hash~\cite{chum2008near}, ITQ~\cite{gong2012iterative}, PQ~\cite{jegou2010product}, and many others~\cite{ge2013optimized,norouzi2013cartesian,wang2014optimized,wang2018composite,babenko2014additive,martinez2014stacked} have been introduced to pack the features to a low-dimensional and even binary codes to enable fast distance computation, namely Hamming distance and distance computation based on pre-computed look up tables. Furthermore, subsequent works~\cite{qiu2017deep,cao2017collective,erin2015deep,zhu2016deep} exploit directly learn the compact (including binary) features from raw images.


% joint inverted index
\noindent \textbf{Approximate nearest neighbor search.}
Another different line of research focus on approximate nearest neighbor search 
%by only comparing query with a small number of database images rather than the whole database 
to speed up the search process, with the sacrifice of search accuracy to some degree. One way is to enable fast distance computation through hashing and quantization techniques such as LSH~\cite{indyk1998approximate,andoni2008near}, min-Hash~\cite{chum2008near}, ITQ~\cite{gong2012iterative}, PQ~\cite{jegou2010product}, and many others~\cite{ge2013optimized,norouzi2013cartesian,wang2014optimized,wang2018composite,babenko2014additive,martinez2014stacked,qiu2017deep,cao2017collective,erin2015deep,zhu2016deep}.
The other way is to reduce the number of distance comparison by retrieving a small number of candidates. Typical methods include
%Based on the way of organizing the database structure, previous methods can be roughly divided into 
partition-based indexing~\cite{bentley1990k,babenko2014inverted,xia2013joint} that partitions the feature space into some non-overlapping clusters and graph-based indexing~\cite{jayaram2019diskann} that builds a neighborhood graph with edges connecting similar images. To improve the recall rate while ensuring fast search speed, hierarchical course-to-fine strategy~\cite{babenko2014inverted, malkov2018efficient} has been the popular choice that the retrieved candidates are refined level by level. Additionally, a number of excellent works have introduced hybrid indexing~\cite{chen2021spann,jayaram2019diskann,ren2020hm} that improves search by leveraging the best of both indexing schemes while avoiding their limitations.



\subsection{Deep 
Autoregressive Model}
Deep autoregressive networks are generative sequential models that assume a product rule for factoring the joint likelihood and model each conditional distribution through a neural network. AR models have shown extremely powerful progress
in generative tasks across multiple domains such as images~\cite{chen2020generative,cho2020x,gulrajani2016pixelvae,yu2022scaling}, texts~\cite{ radford2019language,yang2019xlnet}, audio~\cite{dhariwal2020jukebox,chung2019unsupervised}, and video~\cite{wu2022nuwa,weissenborn2019scaling}.
The particular key component involves linearizing data into a sequence of symbols with notable works such as VQ-VAE~\cite{van2017neural}, RQ-VAE~\cite{lee2022autoregressive}.
Recently, a number of works~\cite{tay2022transformer,wang2022neural,de2020autoregressive, de2022multilingual,de2021highly,bevilacqua2022autoregressive} further explored the idea of using AR model to empower entity retrieval and document retrieval.
Most related to our work are NCI~\cite{wang2022neural} and DSI~\cite{tay2022transformer} targeting at document retrieval.
Yet they simply apply hierarchical k-means over document embeddings obtained from a pretrained language model to get the document identifier.
Differently, we propose to learn the identifier optimized directly from the semantic supervision, 
and demonstrate its efficacy in the image retrieval scenario. We believe such finding benefit document retrieval as well. 
%Inspired by the advances in AR, we expand the intuition, verify its efficacy in image retrieval, which was unexplored before. 
% Importantly, we introduce a novel semantic tokenizer for efficient and effective image search. 
%We leave more discussions in Section~\ref{}.


\section{Method}
Our model trains a sequence-to-sequence model to autoregressively predict image identifiers given a query image. A brief illustration of our pipeline is shown in Figure~\ref{fig:framework}. For Top-K search, beam search can be naturally applied here to find the most likely identifiers given a set of possibilities. 
The image identifier may be randomly generated in which case the model bears the whole responsibility to learn the semantic relationship among database images. On the other hand, the image identifier may be heuristically designed with semantic prior, reducing optimization difficulty of the model.
We will first describe how to obtain a semantic image identifier in detail and then introduce the end-to-end autoregressive model. 
%Each database image is converted into a sequence of discrete symbols by the proposed image tokenizer.
%Note that this tokenizer is not required during inference.

\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/arir2}}
\caption{A brief illustration of our pipeline.}
\label{fig:framework}
\end{center}
\vskip -0.4in
\end{figure}





\subsection{Semantic Image Tokenizer}
The autoregressive model must address a sequence of discrete units. 
As Transformer becomes the ubiquitous architecture in computer vision, it has emerged many successful image tokenizers such as VQ-VAE~\cite{van2017neural,ramesh2021zero,gafni2022make,yu2021vector}, RQ-VAE~\cite{lee2022autoregressive} and so on. 
Basically, these methods learn a variational auto-encoder with discrete latent variables, together with a learnable and indexable codebook over a collection of raw images. As a result, an image is represented as a sequence of accountable discrete codes indicating the entries in the codebook. A proper combination of entries can be decoded to a high-quality image through the decoder. Such tokenizer has been widely applied to image synthesis, and can be easily extended to audio and video synthesis if the auto-encoder is learned over audio or video data.

Despite its success in generation, we argue that it is not amenable for the retrieval task. First, decoding the latent codes to reconstruct the raw image is necessary to support the ability to generate images for synthesis task, yet it is not required for retrieval. Besides, the sequence length has a huge effect on the inference speed of AR model, that is search efficiency in our case. It is thus especially critical to deal with a very short sequence of codes, whereas current sequence length of the codes is extremely long for retrieval (e.g., feature map of $8\times 8$ with depth $4$ of RQ-VAE leads to a length of 256). Furthermore, it needs to inject semantic information into the latent codes, while the image reconstruction loss is known to be a low-level objective that may force the latent representation to focus on imperceptible local details or even noise.
%Fourth, the code in each patch is independent of each other, which is not aligned with the chain assumption in AR model. 

Based on the above observations, we propose to explore the global feature outputted from the class token rather than the default spatial tokens. In this way, the sequence length can be significantly reduced (from 64 tokens to 1 token) and as a byproduct, the class token contains compact high-level semantic.
%To be pecific, we leverage the standard ViT architecture to encode the images. 
Let $\mathbf{f}_{cls}$ denote the $d$-dimensional feature vector outputted from the class token, which is taken as the image representation. We adopt residual quantization (RQ) or stacked composite quantization to approximate this feature.
Suppose there are $M$ codebooks with each containing $L$ elements, $\mathcal{C}_m=\{\mathbf{c}_{m1},\cdots, \mathbf{c}_{mL}\}$,
RQ recursively maps the embedding $\mathbf{f}_{cls}$ to a sequentially ordered $M$ codes, $\mathbf{f}_{cls} \rightarrow \{l_1, l_2,\cdots,l_M\} \in [\mathbb{L}]^{M}$. Let $\mathbf{r}_0 = \mathbf{f}_{cls}$, we have
\begin{align}
    l_m &= {\arg \min}_{l \in [\mathbb{L}]} \|\mathbf{r}_{m-1} - \mathbf{c}_{ml}\|_2^2, \\
    \mathbf{r}_m & = \mathbf{r}_{m-1} - \mathbf{c}_{ml_m}, ~ m = 1,2,\cdots,M.
\end{align}
Such sequential generation of discrete codes naturally aligns with the sequential autoregressive generation, easing the optimization difficulty of modeling the relationship within identifiers.

% We term our image tokenizer as RQ-ViT. Formally, the approximated feature vector $\mathbf{\hat{f}}_{cls}$ can be written as,
% \begin{align}
% \mathbf{\hat{f}}_{cls} = \sum_{m=1}^M \mathbf{c}_{mk_m},
% \end{align}
% where $\mathbf{c}_{mk_m}$ is selected from a codebook $\mathcal{C}_m=\{\mathbf{c}_{m1},\cdots, \mathbf{c}_{mK}\}$ with $K$ elements and there are $M$ codebooks.

To further inject semantic prior, we train the network under classification loss over both the original embedding as well as the reconstructed vector. 
In particular, we consider $M$ levels of reconstruction 
$\hat{\mathbf{f}}_{cls}^{\le m} = \sum_{i=1}^m \mathbf{c}_{il_i}, m=1,2,\cdots,M$
so that each prefix code also encodes semantic to a certain degree.
Adding up the $M$ levels of partial reconstruction error,
the whole objective function is,
\begin{align}
&\mathcal{L} = \mathcal{L}_{cls} (\mathbf{f}_{cls})  + \lambda_1 \sum_{m=1}^M \mathcal{L}_{cls}(\hat{\mathbf{f}}_{cls}^{\le m})  + \lambda_2 \sum_{m=1}^M \|\mathbf{r}_m\|_2^2, \\
&\mathbf{r}_m  = \mathbf{f}_{cls}- \text{sg} [\hat{\mathbf{f}}_{cls}^{\le m}], ~ m=1,2,\cdots,M,
\label{eqn:rqvit}
\end{align}
where $\text{sg}[\cdot]$ is the stop gradient operator. 
During training, we adopt alternative optimization to update the codebook and the network.
For computing the gradient of $\mathcal{L}_{cls}(\hat{\mathbf{f}}_{cls}^{\le m})$, we follow the straight-through estimator~\cite{bengio2013estimating} as in~\cite{van2017neural} and approximate the gradient by copying the gradients at $\hat{\mathbf{f}}_{cls}^{\le m}$ directly to $\mathbf{f}_{cls}$.
After optimization, 
%the codebooks can be thought as visual vocabulary and each image in the database is represented as a sequence of discrete codes, $\{k_1, k_2,\cdots,k_M\}$. 
we hope that images with similar classes have close codes. 
In the experiments, we present comparison with other discrete identifiers including random codes and codes from hierarchical k-means algorithm or from RQ-VAE. 


\subsection{Encoder-Decoder for Autoregressive Retrieval}
Once we have discovered a good discrete latent structure equipped with semantic prior, we train
a powerful autoregressive sequence-to-sequence model over these discrete random variables without referring their visual content. Our encode-decoder structure decouples the input embedding from discrete codes generation. The model takes a query image as input to first get the query embedding and then yields the discrete codes based on the embedding. 
It is worth noting that the yielded discrete codes indicate the query's nearest neighbor images in the database. In that sense, we train the model over an image pair $(x_1, x_2)$ where $x_2$ is the nearest neighbor of $x_1$, and our model aims to predict the identifiers of $x_2$ given $x_1$ as input.

To be specific, let the encoder be denoted as $\mathbb{E}$ based on ViT base and the decoder be $\mathbb{D}$, a standard Transformer decoder composed of causal self-attention, cross-attention and MLP. 
We leverage the spatial tokens outputted from the encoder as the embedding, $\mathbf{e} = \mathbb{E}(x_1)$, which is injected into the decoder through cross attention. We train the model with next-token prediction by maximizing the probability of the $i$-th token of the image identifier given the input embedding and previous token predictions, $p(l_i|x_1,l_1,\cdots,l_{i-1},\theta)$, where $\theta$ denotes the parameters of $\mathbb{D}$ and $\mathbb{E}$, and $l_1,l_2,\cdots,l_M$ are the $M$ tokens for $x_2$ generated from the  image tokenizer. By maximizing the probability of each token, we are actually maximizing the probability of generating the image identifier of an image,
\begin{align}
p(l_1,\cdots,l_M|x_1,\theta) = \Pi_{m=1}^M p(l_i|x_1,l_1,\cdots,l_{m-1},\theta).
\end{align}
We apply softmax cross entropy loss on a vocabulary of $M$ discrete image tokens.

% \noindent \textbf{Training.}
% We adopt alternative optimization for training semantic image tokenizer.
% That is, we first optimize the network parameters with fixed codebook and then update the quantization parameters with fixed network weights.
% For computing the gradient of $\mathcal{L}_{cls}(\hat{\mathbf{f}}_{cls})$, we follow the straight-through estimator~\cite{bengio2013estimating} as in~\cite{van2017neural} and approximate the gradient by copying the gradients at $\hat{\mathbf{f}}_{cls}$ directly to the output of encoder $\mathbf{f}_{cls}$. We experiment with the standard ViT base architecture for the encoder.
% We append a fully connected layer over the class token feature as well as the reconstructed feature to calculate the classification loss. 

% For training autoregressive model, we select similar image pairs $(x_1,x_2)$. In detail, as current retrieval datasets are labeled with class information, given a training image $x_1$, we randomly sample an image which shares the same class as the nearest neighbor $x_2$. We also adopt ViT-B for encoder and similar architecture for decoder (12 transformer decoder block with dimension 768).
%Note that the sequence length for encoder is 196 while the sequence length for decoder is $M=4$, suggesting that our decoder is much faster than the encoder. 


During inference, given a query image $\mathbf{q}$, we first calculate the query embedding through the encoder $\mathbb{E}$ and then autoregressively predicts the discrete codes through the decoder $\mathbb{D}$ based on the query embedding. The image presented by the predicted discrete codes is regarded as the nearest neighbor of the query. The beam search decoding process can be used to retrieve top-$K$ images. In order to ensure valid discrete codes, we constrain the beam search process traversing within a prefix tree containing valid codes.

% \noindent \textbf{Inference.}



\subsection{Beam Search vs. ANN Search}


In terms of the goal of efficiently finding the Top-K candidates,
there are some similarities between beam search and ANN search that both aim to select Top-K promising candidates through traversing tree-like data structures.
However they are quite different in the score calculation used to choose the current node. In ANN search, the score is generally calculated by the distance between the query feature and the node feature according to some distance measure.
In contrast for beam search, the score or the probability is a function estimated via a differentiable neural network (typically an autoregressive model)  conditioned on the query. As such, the whole retrieval pipeline naturally can be optimized in an end-to-end manner.

\begin{table*}[t]
\begin{center}
\begin{tabular}{l|cccc|cccc|cccc}
\toprule
 \multirow{2}{*}{Model} & \multicolumn{4}{c|}{In-shop}  & \multicolumn{4}{c|}{CUB200} & \multicolumn{4}{c}{Cars196}\\
\cline{2-13}
 & 1& 10&20&30&1&2&4&8& 1 & 2 & 4 & 8\\
 \hline
\rowcolor{mygray} \multicolumn{13}{l}{\textit{\textbf{Linear scan search}}} \\
\rowcolor{mygray} Res101-Img & 30.7 & 10.2 & 7.1 & 5.8 &  46.8 & 43.6 & 39.9 & 34.9 & 25.9 & 22.0 & 18.5 & 15.4\\
 \rowcolor{mygray} CLIP & 57.5 & 22.8 & 16.6 & 14.1 &  66.0 & 63.5 & 59.4 & 53.8 & 70.8 & 67.8 & 63.3 & 57.2\\
 \rowcolor{mygray} CGD$_{\text{(repro)}}$ & 83.2 & 47.8 & 40.2 & 37.0  & 76.7 & 75.5 & 73.7 & 71.4 & 87.1 & 86.1 & 84.6 & 82.6\\
  \rowcolor{mygray} IRT$_{\text{R}}$$_{\text{(repro)}}$ & 92.7 & 59.6 & 51.1 & 47.6  & 79.3 & 77.7 & 75.0 & 71.4 & 75.6 & 73.1 & 68.3 & 61.7\\
  \rowcolor{mygray} FT-CLIP & 91.4 & 66.8 & 58.9 & 55.4  & 79.2 & 77.6 & 76.0 & 73.2 & 88.4 & 87.7 & 87.1 & 85.8\\
 \hline
 \multicolumn{13}{l}{\textit{\textbf{Faiss IVF PQ search}}} \\
  CGD$_{\text{(repro)}}$ & 60.4 & 30.5 & 24.5 & 22.0 & 71.6 & 70.8 & 69.9 & 68.7 & 84.8 & 84.4 & 84.1 & 83.3\\
  IRT$_{\text{R}}$$_{\text{(repro)}}$ & 68.6 & 35.7 & 29.3 & 26.6 & 68.9 & 67.6 & 66.2 & 63.4 & 59.1 & 57.5 & 54.7 & 51.7 \\
  FT-CLIP& 63.7 & 37.0 & 30.7 & 28.0  & 72.6 & 72.1 & 71.2 &69.7 & 86.5 & 86.3 & 86.2 & 86.0\\
  \hline
 \multicolumn{13}{l}{\textit{\textbf{ScaNN search}}} \\
 CGD$_{\text{(repro)}}$ & 83.0 & 47.7 & 40.3 & 37.2 & 76.7 & 75.2 &73.8 & 71.4 & 87.1 & 86.1 & 84.5 & 82.6 \\
 IRT$_{\text{R}}$$_{\text{(repro)}}$ & 92.0 & 58.2 & 50.0 & 46.6 & 79.3 & 77.7 & 75.1 & 71.4 & 75.4 & 72.8 & 68.1 & 61.6\\
 FT-CLIP& 90.4 & 64.6 & 56.9 & 53.5 & 79.2 & 77.5 & 76.0 & 73.2 & 88.3 & 87.7 & 87.1 & 85.8 \\
 \hline
 \multicolumn{13}{l}{\textit{\textbf{SPANN search}}} \\
 CGD$_{\text{(repro)}}$ & 83.0 & 47.7 & 40.3 & 37.1 & 76.7 & 75.5 & 73.7 & 71.4 & 87.0 & 86.1 & 84.6 & 82.6\\
 IRT$_{\text{R}}$$_{\text{(repro)}}$ & 91.4 & 56.2 & 47.9 & 44.5  & 79.3 & 77.6 & 75.0 & 71.4 & 74.8 & 72.4 & 67.6 & 61.1\\
 FT-CLIP& 90.2 & 62.9 & 55.1 & 51.8  & 78.5 & 77.6 & 76.0 & 73.2 & 88.6 & 88.1 & 87.5 & 86.3\\
 \hline
 \multicolumn{13}{l}{\textit{\textbf{Beam search}}} \\
 IRGen (ours) & \textbf{92.4} & \textbf{87.0} & \textbf{86.6} & \textbf{86.5}  & \textbf{82.7} & \textbf{82.7} & \textbf{83.0} & \textbf{82.8} & \textbf{90.1} & \textbf{89.9} & \textbf{90.2} & \textbf{90.5}\\
 \hline
\end{tabular}
\end{center}
\vskip -0.1in
\caption{Precision comparison with different baselines, for which we consider linear scan search, Faiss IVF search and SPANN search. (repro) denotes the model reproduced by ourselves to ensure the same data process and comparable model size for fair comparison.  Our model adopt beam search for retrieval, achieving significant improvement and performing even better than linear scan search.}
\label{tab:precision}
\vspace{-1em}
\end{table*}



\section{Experiments}
We conduct comprehensive evaluations to demonstrate the performance of the proposed IRGen. We first evaluate our method on common image retrieval datasets and further present extensive ablation studies to verify the design of our framework on In-shop Clothes dataset.
To show the scalability of our approach, we conduct experiments on two large-scale datasets, ImageNet~\cite{deng2009imagenet} and Places365~\cite{zhou2017places}. 

\noindent \textbf{In-shop Clothes} retrieval dataset~\cite{liu2016deepfashion} is a large subset of DeepFashion with large pose and scale variations.  This dataset consists of a training set containing 25,882 images with 3997 classes, a gallery set containing 12,612 images with 3985 classes and a query set containing 14,218 images with 
3985 classes. The goal is to retrieve the same clothes from the gallery set given a fashion image from the query set. We use both the training set and the gallery set for training in our experiments.

\noindent \textbf{CUB200}~\cite{wah2011caltech} is a fine-grained dataset containing 11,788 images with 200 classes belong to birds. There are 5,994 images for training and 5,794 images for testing.

\noindent \textbf{Cars196}~\cite{krause20133d} is also a fine-grained dataset about cars. It contains 16,185 images with 196 car classes, which is split into 8,144 images for training and 8,041 images for testing.








\noindent \textbf{Implementation details.}
We adopt ViT-B for encoder and similar architecture for decoder (12 transformer decoder block with dimension 768).
%The input image is of resolution $224\times 224$ and is partitioned to $14 \times 14$ patches with each patch sized $16 \times 16$. 
Intuitively, a warm initialization of encoder should largely stable the training process.
%of the quantization as well as the autoregressive decoder. 
We thus warm-start the model with encoder initialized by the pretrained CLIP model~\cite{radford2021learning}. 
%We randomly initialize the remaining fully connected layer and the decoder. 
For training autoregressive model, we select similar image pairs $(x_1,x_2)$. As current retrieval datasets are labeled with class information, we randomly sample an image  $x_2$ which shares the same class with  $x_1$ as the nearest neighbor.
%The semantic image tokenizer is trained for 200 epochs with learning rate set as 5e-4 and batch size set as 128. For autoregressive model, they are 200, 4e-5, 64 respectively. Both adopt AdamW~\cite{loshchilov2017decoupled} and a cosine annealing learning schedule with warm start~\cite{loshchilov2016sgdr}. 
The hyperparameter for quantization is set to $M=4$ and $L=256$ for fast inference. We discuss other choices in the ablation study. More details can be found in the supplementary material.



\noindent \textbf{Baselines.}
We evaluate the performance comparing with following five competitive baselines:
1) ResNet-101~\cite{he2016deep} trained from ImageNet dataset, denoted as Res101-Img, which is usually used as a feature extraction tool for many tasks;
2) CLIP~\cite{radford2021learning} trained from 400M image-text pairs, whose features have exhibited powerful zero-shot capability;
3) CGD~\cite{jun2019combination}, a state-of-the-art method based on ResNet;
4) IRT~\cite{el2021training}, a Transformer-based model for image retrieval and the best model IRT$_{\text{R}}$ is adopted;
5) FT-CLIP, a baseline FineTuned from CLIP on the target dataset. For CGD and IRT, we reproduce them in order to ensure the same data process and the comparable model size (ResNet-101 is adopted for CGD and DeiT-B is adopted for IRT) for fair comparison. We also include their best numbers from their original papers for context.


\begin{table*}[t]
\begin{center}
\begin{tabular}{l|cccc|cccc|cccc}
\toprule
 \multirow{2}{*}{Model} & \multicolumn{4}{c|}{In-shop} & \multicolumn{4}{c|}{CUB200} & \multicolumn{4}{c}{Cars196}\\
\cline{2-13}
 & 1& 10&20&30&1&2&4&8& 1 & 2 & 4 & 8\\
 \hline
 \rowcolor{mygray} \multicolumn{13}{l}{\textit{\textbf{Linear scan search}}} \\
\rowcolor{mygray} Res101-Img & 30.7 & 55.9 & 62.7 & 66.8 & 46.8 & 59.9 & 71.7 & 80.8 & 25.9 & 35.6 & 47 & 59.7\\
 \rowcolor{mygray} CLIP & 57.5 & 83.0 & 87.5 & 89.7  & 66.0 & 78.1 & 87.7 & 93.5 & 70.8 & 82.6 & 91.1 & 95.9\\
 \rowcolor{mygray} CGD* & 91.9 & 98.1 & 98.7 & 99.0  & 79.2 & 86.6 & 92.0 & 95.1 & 94.8 & 97.1 & 98.2 & 98.8\\
 %CGD(repro) & 84.5 & 95.8 & 97.3 & 97.8 & 82.3 & 91.1 & 94.7 & 96.1 & 76.7 & 83.4 & 88.0 & 91.8 & 87.1 & 91.7 & 94.7 & 96.7\\
  \rowcolor{mygray} IRT$_{\text{R}}$* & 91.9 & 98.1 & 98.7 & 99.0  & 76.6 & 85.0 & 91.1 & 94.3 & - & - & - & -\\
  %IRT(repro) & 92.7 & 98.6 & 99.0 & 99.23 & 86.3 & 93.5 & 96.0 & 96.8 & 79.3 & 86.9 & 92.0 & 95.0 & 75.6 & 85.1 & 91.1 & 95.3\\
  \rowcolor{mygray} FT-CLIP & 91.4 & 97.3 & 98.1 & 98.5 & 79.2 & 85.0 & 89.3 & 92.0 & 88.4 & 90.5 & 92.5 & 93.8\\
 \hline
 \multicolumn{13}{l}{\textit{\textbf{Faiss IVF PQ search}}} \\
  CGD$_{\text{(repro)}}$ & 60.4 & 76.0 & 77.1 & 77.4 & 71.6 & 77.4 & 81.5 & 84.2 & 84.8 & 88.0 & 89.8 & 91.0\\
  IRT$_{\text{R}}$$_{\text{(repro)}}$ & 68.6 & 79.2 & 80.0 & 80.2  & 68.9 & 77.9 & 85.0 & 89.3 & 59.1 & 70.4 & 78.2 & 83.4\\
  FT-CLIP& 63.7 & 70.7 & 71.1 & 71.2  & 72.6 & 78.0 & 82.3 & 85.2 & 86.5 & 86.9 & 87.2 & 87.5\\
   \hline
 \multicolumn{13}{l}{\textit{\textbf{ScaNN search}}} \\
 CGD$_{\text{(repro)}}$ & 83.0 & 94.8 & 96.2 & 96.7 & 76.7 & 83.5 & 88.0 & 91.8  & 87.1 & 91.7 & 94.6 & 96.6 \\
 IRT$_{\text{R}}$$_{\text{(repro)}}$ & 92.0 & \textbf{97.8} & \textbf{98.3} & \textbf{98.4} & 79.3 & 86.8 & 91.9 & 94.7 & 75.4 & 84.7 & 90.9 & 95.0 \\
 FT-CLIP & 90.4 & 95.9 & 96.6 & 96.9 & 79.2 & 85.0 & 89.2 & 92.7 & 88.3 & 90.5 & 92.4 & 93.7\\
 \hline
 \multicolumn{13}{l}{\textit{\textbf{SPANN search}}} \\
 CGD$_{\text{(repro)}}$ & 83.0 & 95.0 & 96.4 & 96.9  & 76.7 & 83.4 & 87.9 & 91.8 & 87.0 & 91.7 & \textbf{94.6} & \textbf{96.7}\\
 IRT$_{\text{R}}$$_{\text{(repro)}}$ & 91.4 & 97.2 & 97.6 & 97.7  & 79.3 & \textbf{86.8} & \textbf{91.9} & \textbf{94.7} & 74.8 & 84.3 & 90.5 & 94.7\\
 FT-CLIP & 90.2 & 95.8 & 96.7 & 97.0  & 78.5 & 85.0 & 89.4 & 92.9 & 88.6 & 90.7 & 92.5 & 94.2\\
 \hline
 \multicolumn{13}{l}{\textit{\textbf{Beam search}}} \\
 IRGen (ours) & \textbf{92.4} & 96.8 & 97.6 & 97.9 & \textbf{82.7} & 86.4 & 89.2 & 91.4 & \textbf{90.1} & \textbf{92.1} & 93.2 & 93.7\\
 \hline
\end{tabular}
\end{center}
\vskip -0.1in
\caption{Recall comparison with different baselines, for which we consider linear scan search, Faiss IVF search and SPANN search. (repro) denotes the model reproduced by ourselves to ensure the same data process and comparable model size for fair comparison. we include the best result of CGD and IRT from their original papers for context with * denotation. Our model adopt beam search for retrieval, achieving comparable performance in most cases. }
\label{tab:recall}
\vspace{-1em}
\end{table*}

\noindent \textbf{Search process.}
The baseline models target at effective feature learning and after training, the features for database images are extracted from the learned model. The given query image during search is first passed through the model to get the query feature and then compared with the database features according to a distance metric. As conventional following~\cite{radford2021learning,jun2019combination,el2021training}, we use cosine distance for CLIP model and Euclidean distance for other baselines.
We consider linear scan search, namely KNN, which is very time consuming and approximate nearest neighbor search, namely ANN, which is much efficient by contrast. Further for ANN, we consider (i) the popular Faiss IVF PQ~\cite{johnson2019billion} with the coarse clusters being set to 300, 100, 200 for In-shop, CUB200, Cars196 respectively, and we set the number of sub-spaces to 4, the number of centroids for each sub-space to 256 for all the datasets; (ii) the state-of-the-art memory-based algorithm ScaNN~\cite{guo2020accelerating} with the default setting;
and (iii) the state-of-the-art disk-based SPANN algorithm~\cite{chen2021spann}.




\subsection{Results}


Table~\ref{tab:precision} shows the performance comparison in terms of precision@$K$ that evaluates the percentage of similar images (sharing the same class as query) in the retrieved top $K$ candidates.  It can be clearly seen that our model achieves the best performance with significant gain, performing even better than models using linear scan search. For instance, our model gets 20.2\% improvement in precision@10 on In-shop Clothes dataset, 6.0\% in precision@2 on CUB200 dataset and 2.4\% in precision@2 on Cars196 dataset.
Additionally, we have following observations. 1) As expected, per-dataset finetuned models perform much better than off-the-shelf feature extractors such as CLIP and ImageNet pretrained ResNet-101.
2) Generally, equipped with ANN algorithm, models perform worse than the counterparts using linear scan search. However, it is possible for other way around, for example FT-CLIP with SPANN search on Cars196 dataset is slightly better than linear scan search. This suggests that end-to-end optimization is indeed of great importance. 3) It is worth noting that our model achieves consistently high precision number as $K$ increases, while others get severe performance drop.


We further compare different models using the metric Recall@$K$ in Table~\ref{tab:recall}.
The recall score is 1 if there exists one image out of the returned $K$ candidates shares the same label as the query image, and is 0 otherwise.
The average over the whole query set is Recall@$K$.
Here we include the best result of CGD and IRT from their original papers for context (note that they adopt different data preprocesses, model sizes, and additional training techniques).
1) We can see that our model, IRGen, achieves the best Recall@$1$ amongst all the models. 
As for other recall scores, our model performs comparable and sometimes slightly worse. The reason might be that current objective loss used in AR makes the model highly optimized for Recall@1 while paying less attention to other scores. One potential solution is to integrate beam search process into training for joint optimization. 
2) Besides,
it is interesting to notice that different combinations of feature extractor and ANN algorithm have large variance over the three datasets, indicating the difficulty of coordination  in practical scenarios.
3) Furthermore, despite high recall of baselines, they usually need an extra re-ranking stage to improve precision, while our model already attains high numbers for precision. 

We also plot the precision-recall curve in Figure~\ref{fig:pr}. Here recall stands for the conventional meaning, namely true positive rate.
It can be easily seen that our approach, IRGen, gets remarkably impressive performance, maintaining high precision and high recall at the same time. Furthermore, we evaluate the metric mean reciprocal rank (MRR) measuring the inverse of the rank of the first relevant item. We compute MRR with respect to four different values 1, 2, 4, 8 and plot the curves in Figure~\ref{fig:mrr}. The baselines use SPANN for retrieval algorithm. We can see from the figure that our model again achieves the best number in terms of MRR, validating the effectiveness of our framework. Notably, the performance gap of each baseline to our model has a very large variance on three datasets. 

% \begin{figure*}[t]
% \centering
% \subfigure[In-shop Clothes]{
% \includegraphics[width=0.6\columnwidth]{figures/inshop_pr.png}
% \label{inshop_pr}
% }
% \subfigure[Standford Online Products]{
% \includegraphics[width=0.5\columnwidth]{figures/inshop_pr.png}
% \label{sop_pr}
% }
% \subfigure[CUB200]{
% \includegraphics[width=0.6\columnwidth]{figures/cub_pr.png}
% \label{cub_pr}
% }
% \subfigure[CARS196]{
% \includegraphics[width=0.6\columnwidth]{figures/cars_pr.png}
% \label{cars_pr}
% }
% \caption{P-R Curves}
% \label{pr_curves}
% \end{figure*}    

% Figure~\ref{mrr} compares Mean Reciprocal Rank (MRR) at top 1, 2, 5, 10 retrieved candidates.
% \begin{figure*}[t]
% \centering
% \subfigure[In-shop Clothes]{
% \includegraphics[width=0.6\columnwidth]{figures/inshop_mrr.png}
% \label{inshop_pr}
% }
% \subfigure[Standford Online Products]{
% \includegraphics[width=0.42\columnwidth]{figures/sop_mrr.png}
% \label{sop_pr}
% }
% \subfigure[CUB200]{
% \includegraphics[width=0.6\columnwidth]{figures/cub_mrr.png}
% \label{cub_pr}
% }
% \subfigure[CARS196]{
% \includegraphics[width=0.6\columnwidth]{figures/cars_mrr.png}
% \label{cars_pr}
% }
% \caption{P-R Curves}
% \label{mrr}
% \end{figure*}    



\begin{figure*}[t]
\centering
(a)\includegraphics[width=.31\linewidth, clip]{figures/inshop_pr.png}~~
(b)\includegraphics[width=.31\linewidth, clip]{figures/cub_pr.png}~~
(c)\includegraphics[width=.31\linewidth, clip]{figures/cars_pr.png}
\caption{ Precision-Recall (TPR) curve comparison for different methods on (a) In-shop Clothes, (b) CUB200 and (c) Cars196 dataset.}
\label{fig:pr}
\end{figure*}

\begin{figure*}[t]
\centering
(a)\includegraphics[width=.31\linewidth, clip]{figures/isc_mrr.png}~~
(b)\includegraphics[width=.31\linewidth, clip]{figures/cub_mrr.png}~~
(c)\includegraphics[width=.31\linewidth, clip]{figures/cars_mrr.png}
\caption{MRR with respect to 1,2,4,8 comparison for different methods on (a) In-shop Clothes, (b) CUB200 and (c) Cars196 dataset. }
\label{fig:mrr}
\end{figure*}




% \begin{table*}[t]
% \caption{Precision}
% \label{tab:precision}
% \vskip 0.1in
% \begin{center}
% \begin{tabular}{l|cccc|cccc|cccc|cccc}
% \toprule
%  \multirow{2}{*}{Model} & \multicolumn{4}{c|}{In-shop} & \multicolumn{4}{c|}{SOP} & \multicolumn{4}{c|}{CUB200} & \multicolumn{4}{c}{Cars196}\\
% \cline{2-17}
%  & 1& 10&20&30&1&10&50&100&1&2&4&8& 1 & 2 & 4 & 8\\
%  \hline
%  \multicolumn{13}{l}{\textit{Linear scan search}} \\
% Res101-Img & 30.7 & 10.2 & 7.1 & 5.8 & 59.2 & 19.3 & 8.8 & 7.1 & 46.8 & 43.6 & 39.9 & 34.9 & 25.9 & 22.0 & 18.5 & 15.4\\
%  CLIP & 57.5 & 22.8 & 16.6 & 14.1 & 69.2 & 26.1 & 12.6 & 10.5 & 66.0 & 63.5 & 59.4 & 53.8 & 70.8 & 67.8 & 63.3 & 57.2\\
%  CGD(repro) & 83.22 & 47.8 & 40.2 & 37.0 & 82.8 & 44.5 & 27.1 & 24.3 & 76.7 & 75.5 & 73.7 & 71.4 & 87.1 & 86.1 & 84.6 & 82.6\\
%   IRT(repro) & 92.7 & 59.6 & 51.1 & 47.6 & 86.3 & 51.5 & 33.7 & 30.9 & 79.3 & 77.7 & 75.0 & 71.4 & 75.6 & 73.1 & 68.3 & 61.7\\
%   FT-CLIP & 91.4 & 66.8 & 58.9 & 55.4 & 85.1 & 48.8 & 30.2 & 27.3 & 79.2 & 77.6 & 76.0 & 73.2 & 88.4 & 87.7 & 87.1 & 85.8\\
%  \hline
%  \multicolumn{13}{l}{\textit{Faiss IVF search}} \\
%   CGD(repro) & 68.9 & 35.3 & 29.1 & 26.5 & 80.0 & 41.4 & 24.9 & 22.3 & 73.9 & 72.8 & 71.5 & 69.9 & 86.0 & 85.6 & 85.1 & 84.3\\
%   IRT(repro) & 91.6 & 58.0 & 49.9 & 46.5 & 82.2 & 45.7 & 29.4 & 27.0 & 79.3 & 77.7 & 75.2 & 71.5 & 75.6 & 73.0 & 68.2 & 61.9\\
%   FT-CLIP& 68.8 & 42.5 & 36.7 & 34.1 & 82.1 & 44.5 & 27.1 & 24.5 & 75.8 & 74.9 & 73.6 & 71.6 & 86.7 & 86.7 & 86.5 & 86.3\\
%  \hline
%  \multicolumn{13}{l}{\textit{SPANN search}} \\
%  CGD(repro) & 83.0 & 47.7 & 40.3 & 37.1 & 81.6 & 43.5 & 26.6 & 23.9 & 76.7 & 75.5 & 73.7 & 71.4 & 87.0 & 86.1 & 84.6 & 82.6\\
%  IRT(repro) & 91.4 & 56.2 & 47.9 & 44.5 & 84.8 & 49.7 & 32.5 & 29.9 & 79.3 & 77.6 & 75.0 & 71.4 & 74.8 & 72.4 & 67.6 & 61.1\\
%  FT-CLIP& 90.2 & 62.9 & 55.1 & 51.8 & 82.6 & 45.4 & 27.7 & 24.9 & 78.5 & 77.6 & 76.0 & 73.24 & 88.6 & 88.1 & 87.5 & 86.3\\
%  \hline
%  \multicolumn{13}{l}{\textit{Beam search}} \\
%  ARIR(ours) & 92.1 & 89.7 & 89.5 & 89.5 & 82.4 & 65.6 & 55.5 & 54.0 & 83.8 & 83.7 & 83.6 & 83.6 & 90.2 & 90.5 & 90.7 & 90.8\\
%  \hline
% \end{tabular}
% \end{center}
% \end{table*}

% \begin{table*}[t]
% \caption{Recall}
% \label{tab:recall}
% \vskip 0.1in
% \begin{center}
% \begin{tabular}{l|cccc|cccc|cccc|cccc}
% \toprule
%  \multirow{2}{*}{Model} & \multicolumn{4}{c|}{In-shop} & \multicolumn{4}{c|}{SOP} & \multicolumn{4}{c|}{CUB200} & \multicolumn{4}{c}{Cars196}\\
% \cline{2-17}
%  & 1& 10&20&30&1&10&50&100&1&2&4&8& 1 & 2 & 4 & 8\\
%  \hline
%  \multicolumn{13}{l}{\textit{Linear scan search}} \\
% Res101-Img & 30.7 & 55.9 & 62.7 & 66.8 & 59.2 & 73.7 & 81.7 & 85.2 & 46.8 & 59.9 & 71.7 & 80.8 & 25.9 & 35.6 & 47 & 59.7\\
%  CLIP & 57.5 & 83.0 & 87.5 & 89.7 & 69.2 & 82.5 & 89.0 & 91.3 & 66.0 & 78.1 & 87.7 & 93.5 & 70.8 & 82.6 & 91.1 & 95.9\\
%  CGD~\cite{} & 91.9 & 98.1 & 98.7 & 99.0 & 84.2 & 93.9 & 97.4 & 99.2 & 79.2 & 86.6 & 92.0 & 95.1 & 94.8 & 97.1 & 98.2 & 98.8\\
%  %CGD(repro) & 84.5 & 95.8 & 97.3 & 97.8 & 82.3 & 91.1 & 94.7 & 96.1 & 76.7 & 83.4 & 88.0 & 91.8 & 87.1 & 91.7 & 94.7 & 96.7\\
%   IRT~\cite{} & 91.9 & 98.1 & 98.7 & 99.0 & 84.2 & 93.7 & 97.3 & 99.1 & 76.6 & 85.0 & 91.1 & 94.3 & - & - & - & -\\
%   %IRT(repro) & 92.7 & 98.6 & 99.0 & 99.23 & 86.3 & 93.5 & 96.0 & 96.8 & 79.3 & 86.9 & 92.0 & 95.0 & 75.6 & 85.1 & 91.1 & 95.3\\
%   FT-CLIP & 91.4 & 97.3 & 98.1 & 98.5 & 85.1 & 93.4 & 96.0 & 96.7 & 79.2 & 85.0 & 89.3 & 92.0 & 88.4 & 90.5 & 92.5 & 93.8\\
%  \hline
%  \multicolumn{13}{l}{\textit{Faiss IVF search}} \\
%   CGD(repro) & 68.9 & 76.6 & 77.4 & 77.5 & 80.0 & 87.7 & 90.7 & 91.4 & 73.9 & 78.9 & 82.2 & 85.0 & 86.0 & 88.8 & 90.1 & 91.2\\
%   IRT(repro) & 91.4 & 97.2 & 97.7 & 97.8 & 82.2 & 87.8 & 89.4 & 89.8 & 79.3 & 86.9 & 92.1 & 95.0 & 75.6 & 84.8 & 90.7 & 94.5\\
%   FT-CLIP& 68.8 & 71.0 & 71.2 & 71.2 & 82.1 & 88.8 & 90.7 & 91.3 & 75.8 & 80.7 & 83.7 & 86.1 & 86.7 & 87.0 & 87.3 & 87.4\\
%  \hline
%  \multicolumn{13}{l}{\textit{SPANN search}} \\
%  CGD(repro) & 83.0 & 95.0 & 96.4 & 96.9 & 81.6 & 89.9 & 93.1 & 94.0 & 76.7 & 83.4 & 87.9 & 91.8 & 87.0 & 91.7 & 94.6 & 96.7\\
%  IRT(repro) & 91.6 & 97.2 & 97.6 & 97.7 & 84.8 & 91.2 & 93.2 & 93.9 & 79.3 & 86.8 & 91.9 & 94.7 & 74.8 & 84.3 & 90.5 & 94.7\\
%  FT-CLIP & 90.2 & 95.8 & 96.7 & 97.0 & 82.6 & 89.9 & 92.1 & 92.6 & 78.5 & 85.0 & 89.4 & 92.9 & 88.6 & 90.7 & 92.5 & 94.2\\
%  \hline
%  \multicolumn{13}{l}{\textit{Beam search}} \\
%  ARIR(ours) & 92.1 & 96.8 & 97.5 & 97.7 & 82.4 & 90.8 & 94.0 & 94.7 & 82.7 & 86.4 & 89.2 & 91.4 & 90.2 & 92.0 & 92.7 & 93.3\\
%  \hline
% \end{tabular}
% \end{center}
% \end{table*}






\subsection{Ablations}


\noindent \textbf{Random identifiers.}
A naive way for image identifier is to randomly assign discrete identifiers to images. We therefore experiment random identifiers with the same code length and the same range as our model for fair comparison. As expected, this counterpart gets lower performance as shown in Table~\ref{tab:identifier}.
This is because the model with random identifiers is required to not only learn the interaction between query and the image identifiers but also spend capacity to learn the relationship within identifiers. In contrast, a semantic image identifier would ease the burden of the model in building connections within ids and help the model focus more on the input content and the output id.



\begin{table}[t]
\begin{center}
\setlength\tabcolsep{3.5pt}
\begin{footnotesize}
\begin{tabular}{l|c|cccc|cccc}
\toprule
 \multirow{2}{*}{Identifier} &  \multirow{2}{*}{T} & \multicolumn{4}{c|}{Precision} & \multicolumn{4}{c}{Recall} \\
\cline{3-10}
 & & 1& 10&20&30& 1 & 10&20&30\\
 \hline
Random & 4 & 87.6 & 75.4 & 70.8 & 68.3 & 87.6 & 95.1 & 96.0 & 96.1\\
 HKM$_{100}$ & 4 & 88.2 & 80.0 & 78.2 & 77.3 & 87.2 & 93.1 & 94.3 & 95.0\\
 HKM$_{200}$ & 3 & 89.0 & 81.6 & 79.8 & 79.0 & 89.0 & 93.9 & 94.9 & 95.8\\
 HKM$_{500}$ & 2 & 89.5 & 81.7 & 79.8 & 78.9 & 89.5 & 95.3 & 96.5 & 97.0\\
 % PQ &  4 & 73.8 & 66.1 & 61.8 & 59.2 & 73.8 & 90.2 & 93.3 & 94.5 \\
 Ours & 4 &\textbf{92.4} & \textbf{87.0} & \textbf{86.6} & \textbf{86.5} & \textbf{92.4} & \textbf{96.8} & \textbf{97.6} & \textbf{97.9}\\
 \hline
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.1in
\caption{Ablation study on the image identifier (T=length).}
\label{tab:identifier}
\vspace{-1em}
\end{table}





\noindent \textbf{Hierachical k-means identifier.}
Another intuitive way to obtain semantic identifier is to apply hierarchical k-means (HKM) to pretrained features as in~\cite{wang2022neural}. To show this,
we run HKM over the feature of FT-CLIP as this feature exhibits strong performance. The results are evaluated with respect to different numbers of clusters (100, 200, 500). As presented in Table~\ref{tab:identifier}, we notice that overall HKM performs better than random, showing the importance of semantic identifiers. Nonetheless, our proposed semantic image identifiers further improve HKM with clear gain.  


% \noindent \textbf{Product quantization identifier.}
% One possible alternative for image identifier is to use product quantization (PQ) that divides the data space into several subspaces and optimizes each code in its corresponding subspace. We set the hyperparameters ($M=4,L=256$) the same as ours. 


\noindent \textbf{RQ-VAE identifier.}
Here we compare with image tokenizer RQ-VAE~\cite{bevilacqua2022autoregressive} which is widely used in image synthesis, specially for text-to-image generation. We follow the standard default setting where the latent feature map is of $8\times 8$ and the depth is 4, resulting in a sequence length of 256. The codebook is shared with size being 256. The final result significantly lags behind our model, less than 10 percent for performance. We argue that there are two main reasons. First, the sequence length is too long for the model to model the relationship within the identifiers.
Second, the objective of RQ-VAE is to recover the pixel information, which makes the identifier sensitive to local pixel details.
%which largely attributes to the low-level pixel reconstruction objective and non-sequential generation of codes in RQ-VAE.

\begin{table}[t]
%\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{c|cccc|cccc}
\toprule
\multirow{2}{*}{T} & \multicolumn{4}{c|}{Precision} & \multicolumn{4}{c}{Recall} \\
\cline{2-9}
  & 1& 10&20&30& 1 & 10&20&30\\
 \hline
2 & 72.1 & 69.6 & 68.9 & 68.6 & 72.1 & 95.1 & 96.6 & 97.1\\
4 & 92.4 & 87.0 & 86.6 & 86.5 & 92.4 & 96.8 & \textbf{97.6} & \textbf{97.9}\\
6 & 92.8 & 87.2 & 86.8 & 86.7 & 92.8 & 96.7 & 97.4 & 97.8\\
8 & \textbf{92.9} & \textbf{87.4} & \textbf{87.0} & \textbf{86.9} & \textbf{92.9} & \textbf{96.9} & 97.5 & 97.8\\
 \hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\caption{Ablation study on the sequence length T.}
\label{tab:length}
\vspace{-1em}
\end{table}


\noindent \textbf{The sequence length.}
We further investigate the length of identifier in our image tokenizer. We experiment different lengths and report the results in Table~\ref{tab:length}.
We can see that 
if the length of the identifier is too small (for example 2),
the model gets inferior performance. 
As with the length gets longer to 4 or 6, the model gets better performance. At last the performance drops a little bit if the length is too long (8). We think 4-6 would be a good choice in most cases and we simply use 4 in all our experiments.



% \noindent \textbf{Generalization ability to new data.}
% To test the generalization ability of our model, we add some new images to the gallery set during inference. In our method, the identifier of new data is computed by the codebook of training set. As shown in Table~\ref{tab:newdata}, our model outperforms others and can be generalized to new data well. This is because we can get the semantic identifier of those newly added images by codebook which make great use of the infomation learned by autoregressive decoder.




\noindent \textbf{Inference throughput.}
Apart from search accuracy, search efficiency is another critical criteria for retrieval. We use an NVIDIA V100-16G GPU to analyze the time cost of our AR model.
We show the throughput for 100 queries in Figure~\ref{fig:throughput} with the beam size set as 
%$30$, which is used in all the experiments on In-shop Clothes dataset. We also present the throughput of 
1, 10, 20, and 30 for comparison. We also present the time cost of adding each component during retrieval. The encoder is pretty fast and the autoregressive decoder is the major bottleneck and takes more time when beam size increases. Additional time has been consumed for checking the validity, since it is possible that the predicted identifier is not in the database. Overall the time cost is acceptable, e.g, it takes about 0.07s (0.19s) per query with beam size set as 10 (30).
%, though it is not enough for a practical retrieval system.
Note that our model is an end-to-end retrieval method without re-ranking, which however is usually required after ANN search to get higher precision in practical. 
%and the search cost is only related to the beam size and the model size. For instance,  Overall the timecost is not fastable to be used in practical system.  We can see that So the throughput is affordable for some application. Besides, the results retrieved with beam size 1 reach 95\% of the best results and only cost 7\% of time, which means  we could choose the balance between the time and the precision by out own.

% \begin{table}[t]
% \caption{Ablation study on the inference throughput.}
% \label{tab:throughput}
% %\vskip 0.1in
% \begin{center}
% \begin{small}
% \begin{tabular}{c|c|c|c}
% \multirow{1}{*}{beam size} & \multicolumn{1}{c|}{Encoder(s)} & \multicolumn{1}{c|}{+AR(s)} & \multicolumn{1}{c}{+valid id(s)} \\

%  \hline
% 1 & 0.41 & 1.35 & 1.4\\
% 10 & 0.41 & 6.16 & 7.21\\
% 20 & 0.42 & 11.62 & 13.41\\
% 30 & 0.42 & 16.85 & 19.75\\

%  \hline
% \end{tabular}
% \end{small}
% \end{center}
% \end{table}
\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/throughput.png}}
\caption{Illustrating the search speed using beam search.}
\label{fig:throughput}
\end{center}
\vskip -0.3in
\end{figure}

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{4pt}
\footnotesize
\begin{tabular}{l|ccc}
\toprule
 \multirow{2}{*}{Dataset} & \multicolumn{3}{c}{Model} \\
 \cline{2-4}
  & CLIP+SPANN & FT-CLIP+SPANN & IRGen (ours) \\
  \hline
ImageNet & 44.1 & 65.5  & \textbf{76.0}\\
Places365 & 22.1 & 30.3 & \textbf{44.3}\\
 \hline
\end{tabular}
\end{center}
\vskip -0.1in
\caption{MAP@100 comparison on two million-level datasets. }
\label{tab:map@100}
\vskip -0.2in
\end{table}

% \begin{table}[t]
% \begin{center}
% \begin{small}
% \begin{tabular}{l|cccc}
% \hline
% \multirow{2}{*}{Model config.} & \multicolumn{4}{c}{Beam size}\\
%  & 1 & 10 & 20 & 30 \\
%  \hline
% Encoder & 0.41 & 0.41 & 0.42 & 0.42\\
% + AR decoder & 1.35 & 6.16 & 11.62 & 16.85 \\
% + Check valid id & 1.40 & 7.21 & 13.41 & 19.75 \\
%  \hline
% \end{tabular}
% \end{small}
% \end{center}
% \caption{Inference throughput (/s for 100 queries).}
% \label{tab:throughput}
% \end{table}


\subsection{Scaling to Million-level Datasets}
\noindent \textbf{ImageNet.} We further experiment our approach with ImageNet dataset~\cite{deng2009imagenet} that contains 1,281,167 images for training and 50,000 validation images for testing, in which we randomly sample 5,000 images as queries to speed up the evaluation process.
The experimental settings are the same as before except that we enlarge the layer of decoder to 24 to increase the capacity for AR modeling. 
We compare with the strong baselines including CLIP model pretrained from 400M image-text pairs, as well as FT-CLIP model finetuned based on CLIP model.  The  comparison is reported in Figure~\ref{fig:million} and Table~\ref{tab:map@100} in terms of precision@K and MAP@100. We can see that our model again achieves best results. The precision number remains constantly high as K increases, while baselines suffer noticable performance drop.


\noindent \textbf{Places365.}
We also apply our framework to another large scale dataset,
Places365-Standard~\cite{zhou2017places} containing about 1.8 million images from 365 scene categories, where there are at most 5000 images per category.
The experimental settings are the same as in ImageNet. We show the comparison with CLIP and FT-CLIP in Figure~\ref{fig:million} and Table~\ref{tab:map@100}. 
Again our model yields the best performance, demonstrating its efficacy in million-level datasets.



% \begin{table}[t]
% \begin{center}
% \begin{tabular}{c|ccc|c}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Precision} & \multirow{2}{*}{MAP@100} \\
% \cline{2-4}
%   & 1& 10&100& \\
%  \hline
% \multicolumn{5}{l}{\textit{\textbf{Faiss IVF PQ search}}} \\
%   CLIP & 58.9 & 56.5 & 51.3 & 42.4\\
%  FT-CLIP & 72.6 & 71.9 & 68.8 & 63.7 \\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{ScaNN search}}} \\
%  CLIP & 71.1 & 65.1 & 55.5 & 45.4 \\
%   FT-CLIP& 77.9 & 75.6 & 71.7 & 65.0\\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{SPANN search}}} \\
%  CLIP & 70.5 & 63.9 & 54.3 & 44.1\\
%   FT-CLIP& 77.5 & 75.9 & 72.0 & 65.5\\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{Beam search}}} \\
%  IRGen (ours) & \textbf{78.5} & \textbf{78.4} & \textbf{79.1} & \textbf{76.0}
%  \\
%  \hline
% \end{tabular}
% \end{center}
% \caption{Scaling up to ImageNet.}
% \label{tab:imagenet}
% \end{table}


% \begin{table}[t]
% \begin{center}
% \begin{tabular}{c|ccc|c}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Precision} & \multirow{2}{*}{MAP@100} \\
% \cline{2-4}
%   & 1& 10&100& \\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{Faiss IVF PQ search}}} \\
%   CLIP & 34.0 & 32.5 & 30.0 & 19.8\\
%  FT-CLIP & 40.8 & 40.0 & 38.6 & 28.0 \\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{ScaNN search}}} \\
%  CLIP & 42.5 & 38.6 & 30.0 & 22.6 \\
%   FT-CLIP& 47.8 & 45.5 & 41.9 & 30.7\\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{SPANN search}}} \\
%  CLIP & 41.7 & 37.9 & 33.4 & 22.1\\
%   FT-CLIP& 47.0 & 44.9 & 41.4 & 30.3\\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{Beam search}}} \\
%  IRGen (ours) & \textbf{49.2} & \textbf{47.6} & \textbf{44.2} & \textbf{44.3}
%  \\
%  \hline
% \end{tabular}
% \end{center}
% \caption{Scaling up to Places365.}
% \label{tab:places365}
% \end{table}


% \begin{table}[t]
% \caption{Scaling up to ImageNet.}
% \label{tab:imagenet}
% %\vskip 0.1in
% \begin{center}
% \begin{tabular}{c|ccc|ccc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Precision} & \multicolumn{3}{c}{Recall} \\
% \cline{2-7}
%   & 1& 10&100& 1 & 10&100\\
%  \hline
% \multicolumn{7}{l}{\textit{\textbf{Faiss IVF PQ search}}} \\
%   CLIP & 58.9 & 56.5 & 51.3 & 58.9 & 81.6 & 91.3\\
%  FT-CLIP & 72.6 & 71.9 & 68.8 & 72.6 & 84.1 & 90.2\\
%  \hline
%  \multicolumn{7}{l}{\textit{\textbf{ScaNN search}}} \\
%  CLIP & 71.1 & 65.1 & 55.5 & 71.1 & 91.6 & 98.0\\
%   FT-CLIP& 77.9 & 75.6 & 71.7 & 77.9 & 90.7 & 96.1\\
%  \hline
%  \multicolumn{7}{l}{\textit{\textbf{SPANN search}}} \\
%  CLIP & 70.5 & 63.9 & 54.3 & 70.5 & 90.3 & 97.5\\
%   FT-CLIP& 77.5 & 75.9 & 72.0 & 77.5 & 90.3& 95.4\\
%  \hline
%  \multicolumn{7}{l}{\textit{\textbf{Beam search}}} \\
%  IRGPT(ours) & 78.5 & 78.4 & 79.1 & 78.5 & 85.2 & 89.6
%  \\
%  \hline
% \end{tabular}
% \end{center}
% \end{table}



% \section{Discussions}
% \noindent \textbf{Relation to RQ-Transformer.}
% Our model and RQ-Transformer~\cite{lee2022autoregressive} have very different image tokenizers and target at very different research tasks despite of the similar two-stage pipeline. Such pipeline may actually be the de facto for vision tasks~\cite{ramesh2021zero,van2017neural,gafni2022make} as image tokenizer is invariably necessary for AR models.
% While AR model has been widely utilized for content generation, we are the first to demonstrate its effectiveness for image retrieval and show that different tasks require different tokenizers.
% We tailor the tokenizer to retrieval through several pivotal modifications including leveraging the semantic class token feature, significantly cutting off the sequence length and utilizing high-level classification objective. 



% \noindent \textbf{Relation to NCI.}
% Neural Corpus Indexer (NCI)~\cite{wang2022neural} is a document retrieval model that unifies the training and indexing stages to an end-to-end deep model.
% Our model differs from NCI in two major parts.
% On one hand, our image identifier is jointly optimized over the feature representation and the composite codes in comparison to the document identifier in NCI that is generated through hierarchical k-means over pretrained document embeddings.
% We suspect that NCI can benefit from a text-based couterpart similar to the proposed image tokenizer. 
% On the other hand, NCI relies on the proposed prefix-aware weight-adaptive decoder which is specifically designed for identifiers with hierarchical structure, while we verify that simple standard Transformer decoder works for image retrieval. This simplicity is a good thing, making it straightforward to scale up using existing techniques and infrastructure.






\section{Conclusion}
In this paper, we explore generative modeling to empower end-to-end image retrieval that directly maps the query image to its nearest neighbor.
Equipped with the proposed semantic image tokenizer,
we show that our model is capable of achieving impressive high precision while maintaining high recall at the same time. 
Extensive ablations and further evaluations on large scale datasets demonstrate the superiority of our approach.
We believe such a way of generative modeling for image retrieval opens up new exciting research directions in this area, or even beyond. 
% may
% shed light on many emerging applications depending on image retrieval.



\begin{figure}[t]
\centering
\includegraphics[width=.48\linewidth, clip]{figures/imagenet_pr.pdf}~
\includegraphics[width=.48\linewidth, clip]{figures/places365_pr.pdf}
\caption{ Precision comparison on large scale datasets: ImageNet and Places365.}
\label{fig:million}
\vskip -0.1in
\end{figure}



\begin{table}[t]
\begin{center}
\setlength\tabcolsep{4pt}
\begin{footnotesize}
\begin{tabular}{c|cccc|cccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c|}{Precision} & \multicolumn{4}{c}{Recall} \\
\cline{2-9}
  & 1& 10 & 20 & 30 & 1 & 10 & 20 & 30\\
%  \hline
% \multicolumn{5}{l}{\textit{\textbf{Faiss IVF PQ search}}} \\
%   CGD$_{\text{(repro)}}$ & 59.0 & 29.6 & 23.4 & 20.7 & 59.0 & 74.7 & 75.7 & 76.0\\
%    IRT$_{\text{R}}$$_{\text{(repro)}}$ & 58.8 & 27.3 & 21.3 & 18.8 & 58.8 & 72.9 & 74.2 & 74.5\\
%  FT-CLIP & 60.7 & 31.1 & 24.5 & 21.6 & 60.7 & 73.7 & 74.9 & 75.3 \\
%  \hline
%  \multicolumn{5}{l}{\textit{\textbf{ScaNN search}}} \\
%  CGD$_{\text{(repro)}}$ & 85.5 & 49.6 & 41.6 & 38.2 & 85.5 & 95.8 & 96.8 & 97.3 \\
%   IRT$_{\text{R}}$$_{\text{(repro)}}$ & 88.8 & 53.2 & 44.9 & 41.5 & 88.8 & 96.5 & 97.2 & 97.4\\
%   FT-CLIP& 85.4 & 51.4 & 43.3 & 39.8 & 85.4 & 93.0 & 93.5 & 93.8\\
 \hline
 \multicolumn{5}{l}{\textit{\textbf{SPANN search}}} \\
 CGD$_{\text{(repro)}}$ & 87.5 & 50.1 & 41.8 & 38.4 & 87.5 & 95.4 & 96.1 & 96.4\\
  IRT$_{\text{R}}$$_{\text{(repro)}}$ & 81.6 & 46.9 & 39.1 & 35.8 & 81.6 & 88.9 & 89.5 & 89.9\\
  FT-CLIP& 68.9 & \textbf{67.3} & \textbf{66.5} & \textbf{66.7} & 68.9 & 78.3 & 86.4 & 91.7\\
 \hline
 \multicolumn{5}{l}{\textit{\textbf{Beam search}}} \\
 IRGen (ours) & \textbf{90.5} & 26.9 & 16.1 & 12.4 & \textbf{90.5} & \textbf{97.1} & \textbf{97.8} & \textbf{98.2}\\
 \hline
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.1in
\caption{Precision drop as K increases when handling fresh data without updating the AR model (more details in supplementary).}
\label{tab:newdata}
\vskip -0.25in
\end{table}



\noindent \textbf{Limitations.}
Despite the significant performance, we are aware that our model has its limitations, which are also opportunities opening up for future research.
First, although we have demonstrated its scalability to million-scale dataset, handling billion-scale dataset is not easy and may require a larger model with higher capacity. This will inevitably slow down the inference speed. Thus balancing the capacity and the speed is worth exploration for efficient and effective billion-scale search.
Second, how to deal with fresh data 
is particularly critical for search scenario.
We conduct a naive experiment that holds half gallery data of In-shop Clothes dataset from training and adds them during inference without updating the codebook and AR model. 
%To test the generalization ability of our model, we add some new images to the gallery set during inference. In our method, the identifier of new data is computed by the codebook of training set. 
As shown in Table~\ref{tab:newdata},  our model suffers drastic precision drop as K increases while the recall remains consistently high. This is because inherently AR model memorizes the semantic structure within database in its parameters.
%outperforms others and can be generalized to new data well. This is because we can get the semantic identifier of those newly added images by codebook which make great use of the infomation learned by autoregressive decoder.
Thus it is important to study fresh update for dynamic database.
Third, training a large AR model requires massive amounts of energy, posing environmental problems. How to enable efficient training such as fast finetuning a pretrained model is a worthwhile question.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix
\onecolumn{
\section{More Implementation Details}

We adopt ViT-B for encoder and similar architecture for decoder (12 transformer decoder block with dimension 768).
The input image is of resolution $224\times 224$ and is partitioned to $14 \times 14$ patches with each patch sized $16 \times 16$. 
Intuitively, a warm initialization of encoder should largely stable the training process.
%of the quantization as well as the autoregressive decoder. 
We thus warm-start the model with encoder initialized by the pretrained CLIP model~\cite{radford2021learning}. 
We randomly initialize the remaining fully connected layer and the decoder. 
The semantic image tokenizer is trained with a batch size of 128 on 8 V100 GPUs with 32G memory per card for 200 epochs.  We adopt an AdamW optimizer~\cite{loshchilov2017decoupled} with betas as $(0.9, 0.96)$ and weight decay as $0.05$. We use cosine learning rate scheduling. Note that we set the initial learning rate as $5e-4$ for the FC layers. The learning rate of the encoder is set as one percentage of the learning rate of FC layers. We train our models with 20 warming-up epochs and the initial learning rate is $5e-7$.
For training autoregressive model, we select similar image pairs $(x_1,x_2)$. Since current retrieval datasets are usually labeled with class information, we randomly sample an image  $x_2$ which shares the same class with  $x_1$ as the nearest neighbor.
For autoregressive model, we use batch size of 64 on 8 V100 GPUs with 32G memory per card for 200 epochs. The optimizer and the scheduler are same as the semantic image tokenizer mentioned above. The initial learning rate is $4e-5$ for the decoder and the learning rate for encoder is always one percentage of that for decoder.


\section{Qualitative Retrieval Results}
Here we present several retrieval examples comparing our approach with baselines.
The retrieval results on In-shop Clothes, Cars196, and ImageNet with different methods are shown in Figure~\ref{fig:inshop}, Figure~\ref{fig:cars}, and Figure~\ref{fig:imagenet} respectively. 
The correct (incorrect) results are denoted with green (red) borders. 
% The correct results are images in the gallery set for the query image. Usually they belong to the same category of the query image. \\
By comparing the results in the figures, it can be proved that our proposed method perform favorably and is able to handle extremely hard examples.



\begin{figure*} [t]
\centering
\includegraphics[width=.85\linewidth, clip]{figures/inshop.pdf}
\caption{Examples on In-shop Clothes dataset.Results of CGD, IRT, FT-CLIP, our IRGen are shown from top to bottom.The results of CGD, IRT, FT-CLIP are retrieved by SPANN.}
\label{fig:inshop}
\end{figure*}

% \begin{figure*}
% \centering
% \includegraphics[width=.85\linewidth, clip]{figures/cub.pdf}
% \caption{Examples on CUB200 dataset.Results of CGD, IRT, FT-CLIP, our IRGen are shown from top to bottom.The results of CGD, IRT, FT-CLIP are retrieved by SPANN.}
% \label{fig:cub}
% \end{figure*}

\begin{figure*}
\centering
\includegraphics[width=.85\linewidth, clip]{figures/cars.pdf}
\caption{Examples on Cars196 dataset.Results of CGD, IRT, FT-CLIP, our IRGen are shown from top to bottom.The results of CGD, IRT, FT-CLIP are retrieved by SPANN.}
\label{fig:cars}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=.85\linewidth, clip]{figures/imagenet.pdf}
\caption{Examples on ImageNet dataset.Results of CLIP, FT-CLIP, our IRGen are shown from top to bottom. The results of CLIP, FT-CLIP are retrieved by SPANN.}
\label{fig:imagenet}
\end{figure*}

% \begin{figure*}[h]
% \vskip 0.1in
% \centering
% \includegraphics[width=.85\linewidth, clip]{figures/places365.pdf}
% \caption{Examples on Places365 dataset.Results of CGD, IRT, FT-CLIP, our IRGen are shown from top to bottom. The results of CGD, IRT, FT-CLIP are retrieved by SPANN.}
% \label{fig:places365}
% \end{figure*}


}

\end{document}

