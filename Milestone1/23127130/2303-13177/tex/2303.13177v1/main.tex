\documentclass[twocolumn]{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage{xunicode}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{dblfloatfix}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
 \usepackage{url}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

\newcommand*{\img}[1]{%
    \raisebox{-.05\baselineskip}{%
        \includegraphics[
        height=.6\baselineskip,
        width=.6\baselineskip,
        keepaspectratio,
        ]{#1}%
    }%
}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Lars Ø. Bentsen \textit{et al.}}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}


\renewcommand{\thefootnote}{\fnsymbol{footnote}}
  
%% Title
\title{It is all connected: A new graph formulation for spatio-temporal forecasting
%%%% Cite as
%%%% Update your official citation here when published 
}

\author{
  Lars Ødegaard Bentsen\thanks{See Below...} \\
  Department of Technology Systems \\
  University of Oslo \\ 
  Kjeller, Norway \\
  \texttt{lars.bentsen@uio.its.no} \\
  %% examples of more authors
  \And
  Narada Dilp Warakagoda \\
  Department of Technology Systems \\
  University of Oslo \\
  Kjeller, Norway \\
  \AND
  Roy Stenbro \\
  Institute for Energy Systems (IFE) \\
  Kjeller, Norway
  \And
  Paal Engelstad \\
  Department of Technology Systems \\
  University of Oslo \\
  Kjeller, Norway \\
}


\begin{document}

\twocolumn[%
%  Title and authors
  \maketitle
  \vspace{1ex}%
  \begin{abstract}
  \vspace{1ex}
   With an ever-increasing number of sensors in modern society, spatio-temporal time series forecasting has become a de facto tool to make informed decisions about the future. 
   Most spatio-temporal forecasting models typically comprise distinct components that learn spatial and temporal dependencies. 
   A common methodology employs some Graph Neural Network (GNN) to capture relations between spatial locations, while another network, such as a recurrent neural network (RNN), learns temporal correlations. 
   By representing every recorded sample as its own node in a graph, rather than all measurements for a particular location as a single node, temporal and spatial information is encoded in a similar manner. 
   In this setting, GNNs can now directly learn both temporal and spatial dependencies, jointly, while also alleviating the need for additional temporal networks. 
   Furthermore, the framework does not require aligned measurements along the temporal dimension, meaning that it also naturally facilitates irregular time series, different sampling frequencies or missing data, without the need for data imputation. 
   To evaluate the proposed methodology, we consider wind speed forecasting as a case study, where our proposed framework outperformed other spatio-temporal models using GNNs with either Transformer or LSTM networks as temporal update functions. 
   
  \end{abstract}
  \vspace{2ex}
  \keywords{Spatio-Temporal Forecasting \and Graph Networks \and Missing Data \and Irregular Sampling }
  \vspace{2ex}
] \footnotetext[1]{\textit{Corresponding Author}}


% \begin{abstract}
%     \lipsum[1]
% \end{abstract}
% \maketitle

% keywords can be removed
% \keywords{Spatio-Temporal \and Short-Term Forecasting \and Transformers \and Graph Neural Networks}

\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

\section{Introduction}
Forecasting is the task of trying to predict what will happen in the future based on current or historical information. 
Physical models utilise complex governing equations about dynamical systems to infer the future, while time series forecasting typically refers to extrapolating time series into the future using statistical methods. 
Even though one can never be certain about what is to come, forecasting has become ubiquitous in modern society to be able to make informed decisions, ranging from forecasting expected traffic flows~\cite{yu2018spatio, salinas2020deepar}, items sold by retailers~\cite{salinas2020deepar, qi2019deep}, supply and demand of energy~\cite{smyl2019machine, salinas2020deepar, li2019text} and the weather~\cite{am2017deep, sonderby2020metnet}, to name a few.

% To facilitate the growing proportion of the world's population living in urban areas, the concept of smart cities has emerged to alleviate some of the associated challenges with regard to the reliability and capacity of electricity grids, traffic networks, housing and similar. 
% As such, a pillar of many smart city applications is forecasting. 
Spatio-temporal forecasting leverages the fact that time series recorded at different physical locations can be heavily correlated, meaning that forecasts can be improved by considering these jointly. 
As an example, if a particular road in a traffic network is congested, it might affect the traffic flow on nearby roads. 
% Similarly, by jointly considering meteorological variables recorded at different measurement stations, we can better deduct global characteristics.
With ever-increasing pressure on depleting fossil-fuel-based energy resources, the proportion of renewable energy in the electricity mix has significantly grown in recent years.
Wind energy accounts for more than 840 GWs of installed capacity worldwide~\cite{gwec2022global}. 
Since these resources are inherently intermittent, short-term forecasting has become increasingly important to facilitate grid planning and operation. 
Even though spatio-temporal forecasting using deep learning (DL) has been heavily concerned with traffic network applications, such as forecasting traffic flows, ride-hailing and -sharing, recent studies are starting to shift to wind and energy forecasting (e.g. the SIGKDD Cup22~\cite{zhou2022sdwpf}).
In this study, we decided to focus on short-term spatio-temporal wind speed forecasting.
The North Sea was used as a case study since this area is planned to undergo significant developments with regard to planned off-shore wind installations in coming years~\cite{an2020strategy}. 

Graph Neural Networks (GNN) have been popularly employed for spatio-temporal forecasting.
Typically, the GNN architecture extracts spatial features, while another network, such as a Recurrent (RNN) or Convolutional Neural Network (CNN), learns temporal correlations. 
In this study, we propose a new framework for spatio-temporal forecasting based solely on GNNs, where spatial and temporal correlations are learned jointly. 
Furthermore, since spatio-temporal forecasting relies on measurements from a large number of sensors in different locations, there is an increased likelihood of there being missing or corrupt measurements in the available data. 
Most models require complete input signals, which means that missing values are often imputed based on simple heuristics, such as mean or last recorded values, or that entire samples are discarded despite there being many sensors with available information.
This might deteriorate model performance as the data distribution is shifted. 
Our generic framework does not require input signals to be complete, alleviating the need for simple imputation or other methods for representing missing values. 
Finally, different sensors might sample at different frequencies, which could be an issue for traditional spatio-temporal forecasting models. 
However, the framework we propose in this study naturally allows for time series of different lengths and frequencies to be processed jointly.


\section{Related Works}\label{sec:rw}
Some of the simplest, yet very effective, methods for time series forecasting revolve around the autoregressive integrated moving average (ARIMA) model.
Kavasseri \textit{et al.}~\cite{kavasseri2009day} proposed the \textit{fractional}-ARIMA model for one- and two-day-ahead wind speed forecasts. 
The RWT-ARIMA model~\cite{singh2019repeated} also included wavelet transform to decompose a signal into multiple sub-series with different frequency characteristics. 
Slightly more complicated models, such as Support Vector Regressors (SVR) or K-nearest neighbour (KNN) algorithms have also shown good forecasting performance~\cite{jorgensen2020wind}. 

Within the realms of DL, multilayer perceptron (MLP), RNN- and CNN-based models have for a long time been the most popular methods for time series forecasting. 
MLPs are the simplest DL models, but have shown success both when used in isolation~\cite{sfetsos2002novel} and as part of hybrid models~\cite{guo2011case}.
By changing the 2D convolution operation to a 1D causal convolution, CNNs have also achieved success in time series forecasting. 
To be able to learn long-term context, without the need for very deep models, Oord \textit{et al.}~\cite{oord2016wavenet} proposed to use dilated causal convolution, where the dilation can significantly increase the receptive field.
This is well suited for time series and has proved effective in various wind forecasting studies~\cite{dong2021spatio, shivam2020multi}.
Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) cells are specific types of RNNs, which are both very popular for sequence analysis and time series forecasting~\cite{yamak2019comparison, salinas2020deepar, wang2021review, lim2021time}. 
% LSTM networks~\cite{hochreiter1997long} were proposed to combat the vanishing or exploding gradients problem of vanilla RNNs. 
Even though the LSTM and GRU cells are better at learning long-term dependencies than vanilla RNNs, these methods still rely on encoding past information into a single memory vector, which might cause information loss for very long sequences. 
Attention mechanisms have therefore become an integral part of more recent models that considers long time series, as dependencies can be modelled without regard to the distance~\cite{bahdanau2014neural, lim2021time}.

The Transformer was proposed as an architecture completely without recurrence, but instead fully reliant on the attention mechanism~\cite{vaswani2017attention}. 
Transformers have taken the DL-community by storm, showing impressive results for a host of different applications~\cite{lin2022survey}. % , particularly focused on text and image analysis
A fundamental limitation of the vanilla Transformer is that its memory and computational complexity scale quadratically with sequence length. 
Various architectures have aimed at reducing the complexity by introducing sparse attention, either based on fixed heuristics~\cite{beltagy2020longformer, li2019enhancing}, or more advanced methods to locate the most important key-query pairs~\cite{zhou2021informer, wu2021autoformer}. 
Due to the success of Transformers for various applications, these models have also translated into the forecasting domain~\cite{pan2022short, xu2020spatial}.
A range of alterations has been proposed to better facilitate time series, such as the Autoformer~\cite{wu2021autoformer}, FEDformer~\cite{zhou2022fedformer} and Informer~\cite{zhou2021informer}. 
However, despite the authors of the aforementioned studies reporting relative success compared to other Transformer architectures~\cite{zhou2021informer, zhou2022fedformer, wu2021autoformer, li2019enhancing}, some work also importantly question how effective Transformers really are in time series forecasting~\cite{zeng2022transformers}. 
Zeng \textit{et al.}~\cite{zeng2022transformers} propose a very simple one-layer linear model (LTSF-Linear) and show how it was able to outperform state-of-the-art Transformer models for the long-term forecasting benchmarks used in the respective studies~\cite{wu2021autoformer, zhou2022fedformer, zhou2021informer, li2019enhancing}.
Even though the study questions how effective complex Transformer models are for time series forecasting, the focus was only on long-term forecasting, 96-720 time-steps ahead, and lacked investigation into short-term forecasting. 
Nevertheless, Zeng \textit{et al.}~\cite{zeng2022transformers} correctly identifies a problem with the current conduct, that many studies do not compare complex DL models against simple linear models or persistence methods. 

In many domains, there are time series available for multiple physical locations, where the aim is to produce forecasts at all locations simultaneously or to improve the forecasts at a particular location by jointly considering multiple correlated series. 
By assigning each time series to a particular cell in a grid-like structure, CNNs were initially popular tools for extracting spatial correlations~\cite{xu2022multi, liu2020probabilistic, wang2021review}.
Despite the success of CNNs, a problem with such formulations is the rigid ordering of the physical locations into a grid, which might not be able to best reflect more complex topologies, such as housing locations for electricity load forecasting, road networks or location of meteorological measurement stations.

Graph Neural Networks (GNNs) can better facilitate complex and dynamic ordering of the input data by processing directly on graphs.
Various studies combine GNNs that learn spatial correlations, with other networks to learn temporal dependencies, such as an MLP~\cite{pan2022short}, LSTM~\cite{am2017deep}, CNN~\cite{wu2019graph} or Transformer~\cite{cai2020traffic}. 
% \textbf{time-and-graph study?}
Most studies comprise a rule-based system to construct the graph adjacency matrices, while some go one step further by also learning the graph structures~\cite{wu2020connecting, shao2022pre}.
Current methods usually partition the spatio-temporal forecasting problem by having separate parts of a model learn spatial and temporal dependencies separately, such as graph convolutions and recurrent networks, respectively.
The StemGNN model proposed by Cao \textit{et al.}~\cite{cao2020spectral}, applies Graph and Discrete Fourier Transform prior to a graph convolutional block to learn spatial and temporal dependencies jointly. 
Zhang \textit{et al.}~\cite{zhang2021graph} and Horn \textit{et al.}~\cite{horn2020set} do not consider forecasting, but instead, medical classification based on multivariate time series. 
By representing every recorded value as an individual node in a graph, Zhang \textit{et al.} learn inter- and intra-series correlations jointly, while also being able to facilitate irregularly sampled time series. 
This methodology seems potentially interesting for forecasting, as it naturally translates to a framework for learning spatial and temporal dependencies jointly, while also being able to facilitate missing or irregularly sampled data. 
Since spatio-temporal forecasting utilises recorded measurements from a large number of sensors, it is often the case that samples are missing or corrupted for some sensors. 
With regards to wind forecasting, Tawn \textit{et al.}~\cite{tawn2020missing} conducted a study to investigate the effects of missing data and proposed methods for mitigating these. 
The study showed that missing data can have a significant negative impact on model training, but that multiple imputation can alleviate some of these issues.
Wen \textit{et al.}~\cite{wen2023wind} trained a model to impute missing values at the forecasting stage, showing good performance for wind forecasting in the presence of missing data. 
In the GRAPE framework~\cite{you2020handling}, features are encoded in the edges of a graph which naturally allows for incomplete time series. 


\section{Preliminaries}\label{sec:pre}
\subsection{Traditional Problem Formulation}\label{sec:pre:form}
In a traditional spatio-temporal setting using GNNs, we have a graph; $\mathcal{G} = (X, W)$, with node features $X \in \mathbb{R}^{N\times T \times d_x}$, and edge weights $W \in \mathbb{R}^{N\times N \times T \times d_w}$, where $N$ is the number of spatial locations and $T$ the number of time-steps.
$d_x$ and $d_w$ are the node and edge weight feature dimensions, respectively. 
% For simplicity, we here assume a univariate setting, with only a single edge weight and node feature for each time-step, but everything can be easily extended to a multivariate setting by adding an additional dimension to $X$ and $W$.
All observed values at timestamp, $t$, is denoted as $X_{: ,t} \in \mathbb{R}^{N \times d_x}$, while the series for a particular node, $i$, as $X_{i , :} \in \mathbb{R}^{T \times d_x}$.
For simplicity, we also assume the input graph structures to be fixed across time, which results in $W \in \mathbb{R}^{N \times N \times d_w}$. 
$W_{ij} \in \mathbb{R}^{d_w}$ are the features describing the edge sending from node $i$ to $j$, e.g. the physical distance between two measurement locations, where $W_{ij} = 0$ indicates that there is no edge connecting $i$ and $j$. 
% \textbf{Not entirely true, only static inputs/adjacency matrix and then the weights at least change...}
Given the observed values for $T$ previous time-steps, $X_{:, t-T}, ..., X_{:, t-1}$ and the graph structure described by $W$, a spatio-temporal model $F$, with parameters $\theta$, should predict the future $K$ time-steps at the different locations as:
\begin{equation}\label{eq:forecast}
    \hat{X}_{:, t}, ..., \hat{X}_{:, t + K - 1} = F(X_{:,  t-T}, ..., X_{:, t-1} ; W ; \theta).
\end{equation}

\subsection{Traditional Spatio-Temporal Framework}
One of the most common types of GNNs, are message passing neural networks (MPNN), which propagate features by exchanging information amongst adjacent nodes. 
The operation of a single message-passing operation can be summarised as 
\begin{align}\label{eq:mpnn}
    % e_{jit}^{(l)} &= \psi(h^{(l-1)}_{i,t}, h^{(l-1)}_{j,t}, e_{jit}^{(l-1)}) \\
    h_{i, t}^{(l)} &= \phi(h_{i, t}^{(l-1)}, \bigoplus_{j\in \mathcal{N}_i} \psi(h^{(l-1)}_{i,t}, h^{(l-1)}_{j,t}, e_{jit}^{(l-1)})),
\end{align}
where $h_{i, t}^{(l)}$ are the updated features for node $i$ after layer $l$ for timestamp, $t$. 
We assume the inputs to the first layer are the input node and edge weight attributes, as $h^{(0)} = X$ and $e^{(0)} = W$, where the input edge weight attributes are the same for all time-steps. 
$\phi$ and $\psi$ are the update and message functions, e.g. linear transforms, and $\bigoplus$ is a permutation invariant aggregator, such as mean or sum.  
$\mathcal{N}_i$ is the set containing the neighbourhood of node $i$, i.e. $\mathcal{N}_i = \{j \forall W_{ji} > 0\}$.
Even though the input edge weights, $e^{(0)} = W$, do not change over time, we also denote the edge weights, $e^{(l-1)}_{jit}$, with the previous layer, $l-1$, and time dimension, $t$, to allow for updating these features in addition to the node features. 
In our traditional framework, the connectivity between nodes, i.e. $\mathcal{N}_i$, will remain fixed, but the exact edge weight features can be dynamic over time and updated, as shown by the feed-forward network (FFN) update of edge weight features in Fig.~\ref{fig:model}.
The operation in eq. (\ref{eq:mpnn}) can be stacked into multiple layers to form a GNN. 
A range of different variations exist, such as Graph Attention Networks (GAT)~\cite{velivckovicgraph}, where an attention mechanism is introduced to weigh neighbouring nodes differently, resulting in a weighted average aggregation, $\oplus$. 
Even though the particular graph update operations might differ slightly, the general architecture remains similar to eq. (\ref{eq:mpnn}), with update and message functions, $\phi$ and $\psi$.
The spatio-temporal frameworks using GNNs described from here on forward can use the MPNN update in eq. (\ref{eq:mpnn}) or any other common GNN update.
For more details on different types of GNNs, we refer the interested reader to \cite{bronstein2021geometric, velivckovic2023everything}.

For the MPNN update in eq. (\ref{eq:mpnn}), aggregations and updates are only performed along the spatial dimension, considering different time-steps independently. 
To model temporal dependencies, another network is typically included either prior or subsequent to the message-passing update, or for the update function $\phi$, to be represented by a temporal network.
% For static graphs where $\mathcal{N}$ does not change across time, $\phi$ could potentially be represented by a temporal update function. 
In any case, the temporal and spatial dependencies are primarily considered separately, with the message operation only operating along the spatial dimension.
Ideally, we argue that in a unified spatio-temporal framework, spatial and temporal correlations should be considered jointly.

% For static graph problems, i.e. where $W$ is constant across time, the aggregation operation, $\oplus$, can be performed on the entire time series for each location, as shown in eq. (\ref{eq:mpnn}). 
% The update function, $\phi$, can then be represented by a temporal update function, such as an LSTM or CNN. 
% For dynamic graphs where $W$ changes over time, $\mathcal{N}_i$ will also change over time and the operation in eq. (\ref{eq:mpnn}) needs to be performed once for each time-step and node. 
% In this case, $\phi$ cannot be directly used as a temporal update function, resulting in both $\phi$ and $\psi$ typically represented by a single-layer MLP, with another temporal network that then updates $h_i$. 
% Even though the first setting, with static graphs, could be seen to consider spatial and temporal dependencies jointly, we argue that the models still distinguish between space and time through the two distinct networks $\phi$ and $\psi$ updating their respective inputs along the time and spatial dimensions, respectively. 
% Ideally, in a unified spatio-temporal framework, it would be desirable to have an update that does not separate space and time.  

With regards to missing values, even though the graph update in eq. (\ref{eq:mpnn}) can handle a variable number of nodes, e.g. physical locations, the temporal update functions typically require aligned measurements and do not allow for missing values being omitted from the inputs. 
Therefore, missing values will have to be filled by zero-values, some more advanced data imputation or for the entire time series containing the missing information to be removed. 
Discarding the input series for a particular location would also mean that the model would not produce a forecast for this location. 

\section{Proposed Framework}\label{sec:proposed}
\subsection{New Unified Graph Formulation}
\begin{figure*}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=4cm]{Figures/DataStructTrad.jpg}
        \caption{Traditional graph structure for spatio-temporal forecasting with all data for a particular location considered as a single node. Orange features indicate missing values which need to be inferred before feeding to the network.} \label{fig:DataTrad}
    \end{subfigure}
    \hspace*{\fill}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=4cm]{Figures/DataStructNew.jpg}
        \caption{Proposed graph formulation that does not separate space and time, but considers each recorded sample as an individual node. Missing values are simply omitted from the input data.} \label{fig:DataNew}
    \end{subfigure}
\caption{Visualisation of the traditional (a) and proposed (b) graph data structures for spatio-temporal forecasting. The graphs represent time series from five physical locations with, at most, $T=7$ time-steps. The connectivity in both subplots was arbitrarily chosen and is simply meant for visualisation purposes.} 
\end{figure*}

Instead of treating each time series recorded at a particular location as a single node in a graph or each time-step as a separate graph, we propose to treat the measurements for a specific time and location as its own node.
From this it follows that a graph, $\mathcal{G} = (X, W)$, now have node features $X \in \mathbb{R}^{M \times d_x}$, where $M \leq N\cdot T$, and edge weights $W\in \mathbb{R}^{M\times M \times d_w}$.
With no missing data, we have $T$ recorded samples for each of the $N$ locations, resulting in $M = N \cdot T$ nodes. 
However, in the presence of missing data or variable sequence lengths for different locations, our graph will have a smaller number of nodes, $M < N \cdot T$, where $T$ is the maximum sequence length possible for any location.  
Illustrations showing the traditional (described in Sec.~\ref{sec:pre:form}) and proposed graph formulations are given in Fig.~\ref{fig:DataTrad} and \ref{fig:DataNew}, respectively.
We here show some time series recorded for seven time-steps at five physical locations, with missing values shown in orange. 
It is clear how the proposed formulation in Fig.~\ref{fig:DataNew} represents spatial and temporal relations in the same manner, which means that spatial and temporal dependencies can be considered jointly.
For the traditional setting in Fig.~\ref{fig:DataTrad}, each node represents a physical location with some other adjacent locations, and missing values will have to be imputed or represented by a mask value. 
We see from Fig.~\ref{fig:DataTrad}, that if one were to remove all series or time periods with missing information, there would be no input information left and a model would not be able to produce any meaningful forecasts.
In our proposed graph structure shown in Fig.~\ref{fig:DataNew}, each recorded sample is considered as a node, which is connected to samples recorded either at the same or other physical locations. 
The connectivity shown in Fig.~\ref{fig:DataNew} was arbitrary and could either follow some simple rules or be learned by the network, depending on the application.
Nevertheless, an important feature of the proposed formulation is that missing values are simply not included nor required.

% Since there is no temporal dimension in the new formulation of $X$, we require temporal information to be directly encoded in the inputs, such as the temporal encoding used for Transformers~\cite{vaswani2017attention}.
% This also applies to the physical location information.
% However, the choice of temporal or physical position encoding might be quite important depending on the spatio-temporal task at hand, meaning that different methods should be investigated. 
% If $W$ is not just a normal adjacency matrix but represents edge features, we could also encode the relative differences in physical space and time between nodes into the edge weights. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\hsize]{Figures/Stations.jpg}
    \caption{14 meteorological measurement stations located in the North Sea.}
    \label{fig:stations}
\end{figure}

Now, consider the measurement locations in Fig~\ref{fig:stations}, corresponding to 14 different meteorological measurement stations located in the North Sea. 
Assume that for every station, some meteorological variables are recorded every 10 minutes, such as wind speed, temperature and pressure. 
Each recorded value will be associated with a physical location and a timestamp. 
Since there is no temporal dimension in $X$, we require temporal information to be directly encoded in the inputs, such as the encoding used for Transformers~\cite{vaswani2017attention}.
This also applies to the physical location information.
To encode the node features, we could have some learned embedding for the timestamp, physical location and recorded values, which project the features to a higher dimensional space, $d_x$.
The latent representations could then be summed together to construct the particular node embedding as 
\begin{align}\label{eq:node_embed}
    X_i =& \text{FeatEmbed}([ws_i \parallel p_i \parallel k_i]) + \nonumber \\
         & \text{PhysPosEmbed}([lat_i \parallel lon_i]) + \\
         & \text{TimeEmbed}([ts_i]), \nonumber
\end{align}
where $\parallel$ is the concatenation operator, FeatEmbed, PhysPosEmbed and TimeEmbed are some learned embeddings such as linear transforms to $d_x$ dimensional space. 
$ws_i$, $p_i$ and $k_i$ are the wind speed, pressure and temperature recorded at latitude $lat_i$, longitude $lon_i$ and timestamp $ts_i$, corresponding to a node, $i$.
This was just a trivial example to show how recorded values, time and physical position information might be encoded into the node features, but various other embeddings could be used and should be adapted to suit the particular application.
Similarly, if the problem also considers edge weight features, $W$, the relative distance in both space and time between two nodes could be encoded as 
\begin{equation}\label{eq:edge_embed}
    \resizebox{0.9\hsize}{!}{$W_{ij} = Embed([(lat_i - lat_j) \parallel (lon_i - lon_j) \parallel (ts_i - ts_j)]).$}
\end{equation}
% Depending on the particular application, one might wish to include edge weight features, $W$, or not. 
We also note that temporal and physical location information here is included in both eq. (\ref{eq:node_embed}) and (\ref{eq:edge_embed}).
However, only including such information in either the edge weight or node features might be sufficient. % , but both were here included to demonstrate different ways in which one can encode spatial and temporal information into the graph. 
Since there are no constraints on varying sequence lengths, it follows that irregular time series or different sampling frequencies could be naturally included in a single graph.
If there are measurements recorded with different frequencies, one could have distinct embeddings for different frequency data or include additional features to indicate particular sampling frequencies

\subsection{Network Architecture}
\begin{figure}
    \centering
    \includegraphics[width=8.2cm]{Figures/Model.jpg}
    \caption{Generic spatio-temporal forecasting architecture. Depending on the graph structure and embedding, the illustration can represent both the traditional and proposed formulations discussed in Sec.~\ref{sec:pre} and \ref{sec:proposed}, respectively. Only the embedding and graph blocks are strictly required.}
    \label{fig:model}
\end{figure}
Models using the proposed graph formulation will be referred to as Spatio-Temporal Unified Graph Networks (STUGN).
An illustration of the overall model architecture is shown in Fig.~\ref{fig:model}, where the graph block is the only necessary component and can be represented by any common type of GNN, such as the MPNN operation in eq. (\ref{eq:mpnn}).
In addition to graph updates, we also include the options for residual connections, normalisation and position-wise FFNs.
In Fig.~\ref{fig:model}, $M$ are the recorded values at a particular physical locations $P$ and timestamps $T$. 
To the embedding layer, we also feed $\mathcal{N}$, which defines which nodes should be connected to each other and is used to form the input edge weights $W$.
Inspired by the works in~\cite{haugsdal2022persistence}, a persistence connection takes the last recorded value for a particular measurement location and adds it to the output, which has been multiplied by some trainable parameter, $\alpha$, initialised to zero.
This was found to speed up convergence and improve validation accuracy, as the models are initialised to a sensible starting point, namely the persistence model. 
Furthermore, instead of the layer normalisation commonly used in various Transformer architectures, we found that applying the ReZero~\cite{bachlechner2021rezero} initialisation after every Graph Block and FFN in Fig.~\ref{fig:model} yielded better performance. 
In essence, the ReZero allows a network to learn the model depth by introducing gating into the residual connections. 

Since the main motivation behind the study was to investigate the performance of the proposed unified graph formulation, instead of specific model architectures, we consider a couple of STUGN models using different graph update blocks. 
This was done to demonstrate that our framework is not limited to a single GNN architecture and to be able to better conclude on distinct characteristics of the proposed framework. 
% make sure that the reported findings are mainly a result of the new spatio-temporal framework and not just a superior GNN update. 

The first STUGN model will be denoted as the STUGN-GATv2, where the graph block in Fig.~\ref{fig:model} is represented by the GATv2 update from~\cite{brodyattentive}, which is an improved version of the original GAT network~\cite{velivckovicgraph}. 
For the second model, which will be referred to as the STUGN-TGAT, we propose an altered graph attention update based on the scaled dot-product attention~\cite{vaswani2017attention} to attend to neighbouring nodes. 
Considering the graph update of the STUGN-TGAT model, a node, $i$, is updated based on information from neighbouring nodes $j\in \mathcal{N}_i$, through the following attention operation:
\begin{align}
    q_i &= h_i^{(l-1)}W^Q       \label{eq:mygat:1} \\
    k_{ji} &= h_j^{(l-1)} \odot e_{ji}^{(l-1)} W^K    \label{eq:mygat:2} \\
    \alpha_{ji} &= \text{softmax}(\frac{k_{ji}q_i^T}{\sqrt{d_k}})    \label{eq:mygat:3} \\
    h_i^{(l)} &= \sum_{j\in \mathcal{N}_i} \alpha_{ji} h^{(l-1)}_j W^V. \label{eq:mygat:4}
\end{align}
Here, $W^{(\cdot)} \in \mathbb{R}^{d\times d_k}$ are trainable weight matrices and $\odot$ element-wise multiplication.
Queries, $q$, contain the target node information, while neighbouring nodes and corresponding edge information are embedded in the keys, $k$. 
For input graphs without edge features, eq.(\ref{eq:mygat:2}) would be $k_{ji} = h_j^{(l-1)}W^K$, and the key for a particular node would remain the same, irrespective of the target node, $i$. 
In eq. (\ref{eq:mygat:2}), edge features that describe the relationship between two nodes are transformed through $W^K$ and multiplied with the sender node features, $h_j^{(l-1)}$, to produce a more informative key representation. 
Instead of the linear transformation of edge features and element-wise multiplication with node features, one could concatenate the edge and node features before feeding these to a linear transform as in~\cite{zhang2021graph}. 
However, the motivation behind our proposed key representation was to have a dynamic transform of neighbouring node features that depends on the edge weights, which would not be the case with an update like $k_{ji} = [h_j^{(l-1)} \parallel e_{ji}^{(l-1)}]W^K$, where the key representation would be a sum of linear transforms of node and edge information separately.
Even though detailed descriptions are not provided here, we experimented with different updates and found that the attention operation described above yielded good results.
The interested reader can check out our online repository at XXXX\footnote{Will be made available in final version} for the complete implementation of a few different attention updates.
Furthermore, the operations in eq. (\ref{eq:mygat:1}) - (\ref{eq:mygat:4}) are for a single attention head. 
In the actual update, we use multi-head attention, where heads are concatenated and transformed in exactly the same manner as in the vanilla Transformer implementation~\cite{vaswani2017attention}.

% The MPNN operation in eq. (\ref{eq:mpnn}) can be directly applied to our new graph formulation, to model temporal and spatial dependencies simultaneously. 
% We use the GATv2~\cite{brodyattentive} as the main GNN building block, with feed-forward networks (FFN) applied after every graph layer as in~\cite{vaswani2017attention}. 
% Investigation into different GNN architectures was also conducted, such as the original GAT~\cite{velivckovicgraph} and the scaled-dot-product attention from the Transformer~\cite{vaswani2017attention}, but the GATv2 was seen to work well for our particular application of wind speed forecasting. 
% A schematic of the proposed architecture is given in Fig.~\ref{fig:model}, also including the option for residual connections and normalisation layers represented by $\oplus$. 
% Even though we include some additional FFNs, residual connections and normalisation, a model using only a normal graph update in each layer works fine with regards to the input data. 


\section{Experiments}\label{sec:exp}
\subsection{Dataset}\label{sec:exp:data}
To demonstrate the forecasting performance of our proposed framework, we select the task of spatio-temporal wind speed forecasting due to its importance in facilitating the adoption of wind energy into power grids. 
% Historical meteorological measurements were taken from 14 locations in the North Sea, shown in Fig.~\ref{fig:stations}. 
Air temperature and pressure, wind speed and direction were recorded as 10-minute averages from June 2015 to February 2022, for the 14 locations in the North Sea shown in Fig.~\ref{fig:stations}.
Wind speed forecasts should be made for the next hour (6 steps ahead) for all locations simultaneously. 
The data was made available by the Norwegian Meteorological Institute and is openly available through the Frost API\footnote{https://frost.met.no/index.html}.
The first 60\% of the data was used for training, with the following 20\% for validation and the last 20\% for testing. 
Physical location information was represented by the latitude and longitude of a particular measurement station and all features were scaled to have zero mean and unit variance. 
Wind direction measurements were decomposed into sine and cosine components to be able to represent the circular characteristic. 
Timestamp information for a recording was taken as the minute-of-hour, hour-of-day, day-of-month and month-of-year.
For instance, values recorded at 18:50 on the 21st of February would have the timestamp $[50, 18, 21, 2]$.
Each timestamp feature was further decomposed into its sine and cosine components, similar to what was done for the wind direction. 
As an example, the hour-of-day stamp would be encoded as
\begin{equation}
     [-1, 0] = [\sin(18 \cdot (2\pi / 24)), \cos(18 \cdot (2\pi / 24))].
\end{equation}
Some previous studies instead scale the timestamp information using a min-max scaler before feeding this to the embedding layer~\cite{wu2021autoformer, zhou2021informer}. 
However, by not decomposing into sine and cosine components, the times 23:00 and 01:00 would be very far apart, despite the small two-hour difference.
We, therefore, argue that our proposed method is better suited for encoding temporal information. 


\subsection{Baseline Methods for Comparison}\label{sec:exp:base}
The STUGN models were compared against four other baselines. %  which use the traditional graph formulation described in Sec.~\ref{sec:pre:form}.
Inspired by the important points raised in \cite{zeng2022transformers}, the first two baselines were very simple models, namely the persistence and a one-layer linear model (TSF-Linear). 
The persistence model does not have any parameters and will simply extend the last recorded value at a particular station as the forecast for the next $K$ time-steps, 
\begin{equation}
    \hat{X}_{:, t}, ... \hat{X}_{:, t + K -1} = X_{:, t-1}.
\end{equation}
The TSF-Linear model, taken from ~\cite{zeng2022transformers}, does not consider spatial dependencies and is simply a linear transform of the last $T$ recorded values for a station to the $K$ future predictions. 
Different to ~\cite{zeng2022transformers}, we also found the best results when introducing a trainable persistence connection, as was done for the spatio-temporal models. 
The overall computation for the TSF-Linear model then becomes 
\begin{equation}
    \hat{X}_{i, t}, ... \hat{X}_{i, t + K -1} = QX_{i, :} \cdot \alpha + X_{i, t-1},
\end{equation}
where $Q\in\mathbb{R}^{K\times T}$ is a linear layer along the temporal axis and $\alpha$ a trainable variable initialised to zero.

The other two baselines were more advanced, based on the traditional framework outlined in Sec.~\ref{sec:pre}.
The architectures were similar to that depicted in Fig~\ref{fig:model}, but with temporal update functions added subsequent to the graph updates. 
We will denote the two different baselines as ST-LSTM and ST-Transformer, depending on which temporal update function is used.
% The overall framework follows the same architecture depicted in Fig.~\ref{fig:model}, but with a different embedding than the STUGN models to facilitate the traditional graph structures described in Sec.~\ref{sec:pre} and with temporal update functions added subsequent to the graph blocks.
Considering graph blocks represented by normal MPNNs, then update and message functions were linear transforms, $\phi \in \mathbb{R}^{2d \times d}$ and $\psi \in \mathbb{R}^{3d \times d}$, where $d$ is the dimensionality of latent variables.
Since each node, $i$, represents a physical location, eq. (\ref{eq:mpnn}) only considers spatial relations. 
To also learn temporal correlations, the outputs from the MPNN update are fed to either an LSTM or the full Self-Attention update from the Transformer, for the ST-LSTM and ST-Transformer, respectively.
The operation of a Graph Block in Fig.~\ref{fig:model} then becomes 
% \begin{align}
%     e_{ji}^{(l)} &= \psi(h^{(l-1)}_{i,:} \parallel h^{(l-1)}_{j,:} \parallel e_{ji}^{(l-1)}) \label{eq:bs_models1} \\
%     h_{i, :}^{(l)} &= F(\phi(h_{i, :}^{(l-1)} \parallel \bigoplus_{j\in \mathcal{N}_i} e^{(l)}_{ji})) \label{eq:bs_models2},
% \end{align}
\begin{equation}\label{eq:bs_models}
        \resizebox{1.0\hsize}{!}{$h_{i, :}^{(l)} = F\bigg(\overset{T}{\underset{t=1}{\Big\Vert}}\Big[ \phi \Big(h_{i, t}^{(l-1)} \parallel \bigoplus_{j\in \mathcal{N}_i} \psi(h_{i, t}^{(l-1)} \parallel h_{j, t}^{(l-1)} \parallel e_{jit}^{(l-1)})\Big)\Big]\bigg),$}
\end{equation}
where $F$ is a temporal update function, $\bigoplus$ a mean aggregation of messages and $\parallel$ the concatenation operator.
There are also options for the outputs, $h^{(l)}$ and $e^{(l)}$, to be added some residual connection and passed through FFNs before being outputted to the next layer, as shown in Fig.~\ref{fig:model}. 
More advanced graph updates were also tested, using the GATv2 and TGAT updates described in Sec.~\ref{sec:proposed} instead of the MPNN update in eq. (\ref{eq:bs_models}).
For the ST-LSTM models, a direct strategy was employed, with forecasts for the next 6-steps ahead produced by projecting the final output through a two-layer MLP with 6 outputs. 
Considering the STUGN and ST-Transformer models, placeholders were used for the forecast locations in the inputs, set to the last available recorded values. 

\subsection{Experimental Set-up}\label{sec:exp:setup}

\begin{table*}[]
    \small
    \centering
    \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Final model specific parameters after tuning}
    % \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccc}
        \hline
         Parameter          & STUGN     & ST-LSTM       & ST-Transformer \\
         \hline
         Learning Rate      & 5e-05     & 1e-05         & 1e-05 \\ 
         FFN Node           & Yes       & No           & Yes \\
         FFN Edge           & [Yes, Yes, No]$^*$       & [Yes, Yes, No]$^*$           & [Yes, Yes, No]$^*$  \\
         Normalisation      & ReZero~\cite{bachlechner2021rezero}    & -   & Pre LayNorm~\cite{xiong2020layer}  \\
         \hline
         \multicolumn{4}{l}{$^*$ Parameter values for the MPNN, GATv2 and TGAT settings, respectively.}
    \end{tabular}
    % }
    \label{tab:parameters}
\end{table*}

To test the models under different amounts of missing values in the data, we construct four different training, validation and test datasets, where we  remove 0, 10, 20 and 30\% of the entries from the original dataset. 
% Even though entries are randomly removed, we impose a higher probability on subsequent values being missing. 
First, missing values are randomly sampled and then, if a value is missing, there is a probability, $p_n$, of the $n$ following values also being missing according to an exponential decay $p_n = \frac{exp(-n / 10)}{\sum_{j=1}^{10} exp(-j/10)}$, with an upper limit of $n=10$. 
However, since missing values are first sampled independently, before considering subsequent values, there still exist periods with more than ten subsequent missing values.
For the baseline models, which require complete input data, missing values are interpolated to be in between the closest available values in the past and future. 
If no values are available in the inputs, missing values are set equal to those of the closest station, in geographical distance, with available data. 

Since the STUGN framework naturally facilitates variable frequencies, we also construct an additional series for each location, which are the hourly mean wind speeds.
Having both 10-minute and 1-hour frequency data could be an effective method for increasing the historical information used to make forecasts, without sacrificing detailed information in the short-term or significantly increasing the computational complexity. 
For the ST-LSTM and ST-Transformer models, the two series for a particular station were concatenated along the input feature dimension.  %  before being passed to the embedding layers. 
Look-back windows were the same for all models, set to the 18 and 12 previous time-steps for the 10-minute and 1-hour data, corresponding to 3 and 12 hours of historical information, respectively. 
% For the STUGN models, we experimented with having different learned embeddings for the different frequency data, but found it beneficial to have shared embeddings with an additional input feature to represent the sampling frequency associated with a recording. 
All models used learned linear transforms to $d$ dimensional space as the embedding of temporal, physical position and measurement information, as in eq. (\ref{eq:node_embed}). 
Since the ST-Transformer and STUGN models were without recurrence, sequence position information was also embedded into the node features, using the sine and cosine encoding from~\cite{vaswani2017attention}. 
Input edge features contained physical distance information for all models, while the STUGN model also included temporal distance. 
% Input edge features contained both physical and temporal distance information for the STUGN models, while the baseline models only had geographical distance. 
Hyper-parameter tuning was conducted based on the training and validation datasets with 30\% missing values for all models. 
First, a wide search space was investigated using Bayesian search with Optuna~\cite{akiba2019optuna}, before narrowing the search space and using grid-search to determine the final model parameters.
All models had a latent dimensionality of 64 and consisted of three stacked layers. 
Model training was stopped after 25 epochs, using an Adam optimiser with a batch size of 16 and a 5\% dropout rate on a single NVIDIA 2080Ti GPU.
Considering Fig.~\ref{fig:model}, FFNs are assumed to be two-layer MLPs with a hidden layer dimensionality of 256 and GELU hidden activation. 
The outputs from the main models were passed through a final FFN without normalisation or residual connections before the outputs were added to a trainable persistence connection to produce the forecasts, as shown in Fig.~\ref{fig:model}.
Any attention operation used four attention heads. 
The best values for the remaining model parameters were found to vary for the different architectures and are summarised in Table~\ref{tab:parameters}.
% A summary of the final hyper-parameters for the different models is given in Table~\ref{tab:parameters}. 

Graph connectivity, $\mathcal{N}$, was constructed based on the geographical distance between measurement locations so that every node was connected to its three closest measurement locations. 
For the STUGN models, the connectivity also spanned the temporal dimension and the neighbourhood for a particular node was set to the three closest nodes in terms of geographical distance and the three closest points along the temporal dimension in the future and past.
Nodes that correspond to forecast locations, i.e. unknown values, did not send to any input nodes but received information from all other nodes corresponding to the same measurement location. 
The graph connectivity described here followed simple heuristics and we did not experiment with learnable adjacency matrices or more advanced metrics to determine node similarity and connectivity. 
This was left for future work, as the fundamental aim of this study was to demonstrate the effectiveness of the proposed framework under simple conditions, without engineering very domain-specific graph structures that might give the STUGN models an unfair advantage.  

\subsection{Results and Discussion}\label{sec:exp:results}
\begin{table*}[]
    \small
    \centering
    \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Prediction accuracy in m/s on test dataset for 6-step (1-hour) ahead wind speed forecasts for measurement stations in Fig.~\ref{fig:stations}.}
    % \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
        \hline
         Percentage missing     & \multicolumn{2}{c}{0 \%}      & \multicolumn{2}{c}{10 \%}      & \multicolumn{2}{c}{20 \%}      & \multicolumn{2}{c}{30 \%}   \\
         Model                  & MSE           & MAE           & MSE           & MAE            & MSE            & MAE           & MSE            & MAE        \\        
         \hline
         Persistence            & 1.4478        & 0.8096        & 1.5029        & 0.8254         & 1.5660         & 0.8428        & 1.6262         & 0.8584     \\ 
         TSF-Linear             & 1.2843        & 0.7536        & 1.3278        & 0.7686         & 1.3739         & 0.7843        & 1.4215         & 0.7995     \\ 
         ST-LSTM-MPNN           & 0.9982        & 0.6775        & 1.0431        & 0.6943         & 1.0862         & 0.7107        & 1.1197         & 0.7234     \\ 
         ST-LSTM-GATv2          & 1.0081        & 0.6808        & 1.0346        & 0.6914         & 1.0698         & 0.7047        & 1.1131         & 0.7212     \\ 
         ST-LSTM-TGAT           & 1.0433        & 0.6929        & 1.0769        & 0.7057         & 1.1192         & 0.7208        & 1.1639         & 0.7366     \\ 
         ST-Transformer-MPNN    & 1.0896        & 0.7057        & 1.1569        & 0.7291         & 1.1891         & 0.7438        & 1.2132         & 0.7509     \\ 
         ST-Transformer-GATv2   & 1.0740        & 0.7002        & 1.0965        & 0.7091         & 1.1325         & 0.7238        & 1.1534         & 0.7335     \\ 
         ST-Transformer-TGAT    & 1.0996        & 0.7079        & 1.1506        & 0.7253         & 1.1706         & 0.7337        & 1.2155         & 0.7495     \\ 
         \hline
         STUGN-MPNN             & 1.0647        & 0.6989        & 1.0962        & 0.7117         & 1.1350         & 0.7255        & 1.1656         & 0.7372     \\ 
         STUGN-GATv2            & \textbf{0.9780}        & \textbf{0.6678}        & \textbf{1.0148}        & \textbf{0.6827}         & \textbf{1.0391}         & \textbf{0.6923}        & \textbf{1.0755}         & \textbf{0.7058}     \\ 
         STUGN-TGAT             & 0.9945        & 0.6743        & 1.0175        & 0.6847         & 1.0626         & 0.7009        & 1.1044         & 0.7165     \\ 
         \hline
         % \multicolumn{6}{l}{$^*$ Parameter values used for the 1, 6 and 24 step forecast models.}
    \end{tabular}
    % }
    \label{tab:results}
\end{table*}

The mean squared error (MSE) was used to train the models, with the mean absolute error (MAE) also computed on the test data as
\begin{align}
    \text{MAE} &= \frac{1}{n} \sum_{i=0}^n |y_i - \hat{y}_i| \\
    \text{MSE} &= \frac{1}{n} \sum_{i=0}^n (y_i - \hat{y}_i)^2,
\end{align}
where $y$ and $\hat{y}$ are the labels and predictions, respectively.
Every model was trained for five different input seeds and with the four different missing data settings described previously.
Predictive performance was evaluated based on the average from the five seeds, with the results given in Table~\ref{tab:results}, where the data was transformed back to a meters per second scale for better interpretability. 

As expected, model accuracies decreased as more values were removed. 
The TSF-Linear model was only a single linear transform of previous wind speeds to produce predictions for the next six steps ahead. 
In all settings, the TSF-Linear model outperformed the Persistence model, meaning that it was able to learn some structures in the data to make forecasts, despite not being able to take spatial dependencies or complex non-linear relations into account. 
All other models significantly outperformed the TSF-Linear model, which proved their effectiveness in wind speed forecasting and indicated that the models were able to leverage spatial dependencies to improve forecast accuracies.
Overall, using the GATv2 as the main graph update block yielded the best results for all models. 
However, considering the ST-LSTM models, the difference between the MPNN and GATv2 updates was very small and the MPNN update might therefore be preferred due to its simplicity.
With higher test losses across all settings, the proposed TGAT update did not prove very effective for the LSTM-based models. 
For the ST-Transformer models, the TGAT updates achieved similar performance to those using the simpler MPNN updates, while the ST-Transformer-GATv2 achieved superior results across all data settings. 
Looking at the results in Table~\ref{tab:results}, it was found that the LSTM-based models outperformed all ST-Transformer models, indicating that the LSTM network might be more suitable as the temporal update function than a Transformer for short-term wind speed forecasting. 
% Using the GATv2 instead of the MPNN gave better results for the settings with missing data for the ST-Transformer and ST-LSTM models, which might indicate that it was more important to place different weighting on neighbourhood information when some of the values were imputed. 

In contrast to the ST-Transformer and ST-LSTM models, the STUGN models did not have additional temporal networks, but with the MPNN, GATv2 or TGAT networks serving as the main update functions. 
It was found that the MPNN update,  taking only a simple average aggregation of neighbouring features, was not very effective for the STUGN architecture. 
This was unsurprising since the graph operation for the STUGN models should be able to capture both complex temporal and spatial dependencies and the simple aggregation of the MPNN did not seem to be able to capture complex neighbourhood relations. 
On the other hand, when replacing the MPNN operation with TGAT or GATv2 networks, the STUGN architectures outperformed all other methods, with the STUGN-GATv2 model yielding the best results across all settings.
Compared to the ST-LSTM-GATv2, the STUGN-GATv2 achieved almost the same results when having an extra 10\% of the original data missing. 
Since both the STUGN-GATv2 and STUGN-TGAT models outperformed all other baselines, we argue for the effectiveness of the proposed unified graph formulation for spatio-temporal forecasting. 
Furthermore, even though the proposed graph formulation initially might seem more complex than the traditional framework used for the ST-LSTM and ST-Transformer models, the actual network architecture of the STUGN models is simpler than traditional methods as we alleviate the need for additional temporal networks. 
By removing the need for aligned inputs, the STUGN framework is also more versatile, being able to naturally facilitate irregular datasets or time series information for multiple sensors with potentially different or irregular sampling frequencies. 
This could be desirable for a range of forecasting systems where missing data might be a challenge or where sensors have different sampling frequencies. 

To better understand the potential implications of the relative forecasting performances in Table~\ref{tab:results}, we estimate the associated energy production errors for the one-hour ahead forecasts.
In Table~\ref{tab:results_kwh}, values are the absolute error improvements for estimated total energy produced, in kWh, compared to the Persistence model. 
Energy values were estimated by first transforming wind speed forecasts to kW using the power curve for the NREL 5 MW reference wind turbine~\cite{jonkman2009definition}. 
The 10-minute estimates for energy were then summed over the forecast interval to get the forecasted total energy production for the next hour.
Results in Table~\ref{tab:results_kwh} show the average improvements compared to the persistence model, where higher values are better. 
It was found that the STUGN-GATv2 models could improve accuracies by around 40 kWhs compared to the Persistence model and around 4-8 kWhs compared to the second-best model. 
Even though these were crude power estimates, it demonstrates some of the significant cost improvements that slightly more accurate forecasting models could achieve, especially as a wind farm would typically have a much larger capacity than 5 MW. 


\begin{table}[]
    \small
    \centering
    \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Estimated energy saving in kWhs per forecast horizon of 1 hour compared to the Persistence model.}
    \begin{tabular}{lrrrr}
        \hline
         Percentage missing     & 0 \%            & 10 \%           & 20 \%           & 30 \%           \\
         \hline
         Persistence            &  0.00           &  0.00           &  0.00           &  0.00           \\ 
         TSF-Linear             & -2.27           & -2.26           & -2.20           & -2.97           \\ 
         ST-LSTM-MPNN           & 34.86           & 33.35           & 32.89           & 33.13           \\ 
         ST-LSTM-GATv2          & 33.45           & 35.42           & 35.92           & 34.43           \\ 
         ST-LSTM-TGAT           & 28.01           & 28.38           & 28.16           & 27.12           \\ 
         ST-Transformer-MPNN    & 21.74           & 17.15           & 16.93           & 20.73           \\ 
         ST-Transformer-GATv2   & 23.92           & 26.52           & 27.05           & 29.06           \\ 
         ST-Transformer-TGAT    & 20.02           & 18.71           & 22.32           & 21.09           \\ 
         \hline
         STUGN-MPNN             & 25.22           & 25.52           & 26.53           & 27.34           \\ 
         STUGN-GATv2            & \textbf{39.39}  & \textbf{38.99}  & \textbf{42.11}  & \textbf{42.23}  \\ 
         STUGN-TGAT             & 36.66           & 38.66           & 38.03           & 37.95           \\ 
         \hline
    \end{tabular}
    % }
    \label{tab:results_kwh}
\end{table}


\subsection{Future Work}\label{sec:exp:fw}
Since this study was an initial investigation into a new unified spatio-temporal forecasting framework, there are a few areas that the authors note as particularly interesting for future work. 
Here, only wind speed forecasting was considered and the new graph formulation should be tested for different applications such as forecasting traffic, household energy consumption or sales data that are often irregularly sampled. 
The particular graph connectivity used might also be quite significant and studying methods for more advanced similarity evaluation of nodes or learnable graph adjacency matrices might be relevant to boost performance. 
Finally, self-supervised learning and pre-training have shown impressive results for a range of DL applications.
Since the proposed graph formulation is very flexible and allows for a range of different frequency data or missing values, there is potentially a range of different self-supervised tasks that could be designed to improve downstream performance.
Pre-training with a self-supervised task of predicting occluded values was experimented with for this paper, but did not yield significant accuracy improvements and therefore not included. 
However, the authors believe that more extensive research into particular self-supervised tasks might be useful, especially for applications where there might be more distinct trends and evident characteristics in the time series. 

\section{Conclusion}
We propose a new graph formulation for spatio-temporal forecasting. 
By treating every recorded value as its own node in a graph, GNNs can be applied to learn both spatial and temporal dependencies jointly. 
The proposed STUGN models outperform more traditional methods for spatio-temporal forecasting that use GNNs in combination with temporal update functions. 
With the proposed graph structure, inputs do not have to be aligned along the temporal dimension and our framework naturally allows for missing values, irregular series and different sampling frequencies.
Overall, this was intended as a preliminary study to investigate the feasibility of the proposed spatio-temporal formulation and we believe that future work would benefit from studying new applications, different from wind speed forecasting, as well as the effect of graph connectivity and feature embeddings. 

% \section*{CRediT authorship contribution statement}
% \textbf{Lars Ø. Bentsen:} Project administration, Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Resources, Software, Validation, Visualization, Writing - original draft.
% \textbf{Narada Dilp Warakagoda}: Supervision, Writing - review/editing.
% \textbf{Roy Stenbro}: Supervision. Writing - review/editing.
% \textbf{Paal Engelstad}: Supervision. Writing - review/editing.

\section*{Acknowledgements}
This work was in part financed by the research project ELOGOW (Electrification of Oil and Gas Installation by offshore Wind). 
ELOGOW (project nr: 308838) is funded by the Research Council of Norway under the PETROMAKS2 program with financial support of Equinor Energy AS, ConocoPhillips Skandinavia AS and Aibel AS.
The authors would also like to thank The Norwegian Meteorological institute for enabling the open access data used for this study. 

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
