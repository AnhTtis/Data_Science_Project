{
    "arxiv_id": "2303.12736",
    "paper_title": "DPPMask: Masked Image Modeling with Determinantal Point Processes",
    "authors": [
        "Junde Xu",
        "Zikai Lin",
        "Donghao Zhou",
        "Yaodong Yang",
        "Xiangyun Liao",
        "Bian Wu",
        "Guangyong Chen",
        "Pheng-ann Heng"
    ],
    "submission_date": "2023-03-13",
    "revised_dates": [
        "2023-03-23"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.LG"
    ],
    "abstract": "Masked Image Modeling (MIM) has achieved impressive representative performance with the aim of reconstructing randomly masked images. Despite the empirical success, most previous works have neglected the important fact that it is unreasonable to force the model to reconstruct something beyond recovery, such as those masked objects. In this work, we show that uniformly random masking widely used in previous works unavoidably loses some key objects and changes original semantic information, resulting in a misalignment problem and hurting the representative learning eventually. To address this issue, we augment MIM with a new masking strategy namely the DPPMask by substituting the random process with Determinantal Point Process (DPPs) to reduce the semantic change of the image after masking. Our method is simple yet effective and requires no extra learnable parameters when implemented within various frameworks. In particular, we evaluate our method on two representative MIM frameworks, MAE and iBOT. We show that DPPMask surpassed random sampling under both lower and higher masking ratios, indicating that DPPMask makes the reconstruction task more reasonable. We further test our method on the background challenge and multi-class classification tasks, showing that our method is more robust at various tasks.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12736v1"
    ],
    "publication_venue": null
}