
@incollection{cramton_why_2017,
	title = {Why {Paris} {Did} {Not} {Solve} the {Climate} {Dilemma}},
	isbn = {978-0-262-34038-0},
	url = {https://doi.org/10.7551/mitpress/10914.003.0004},
	urldate = {2022-08-12},
	booktitle = {Global {Carbon} {Pricing}: {The} {Path} to {Climate} {Cooperation}},
	publisher = {The MIT Press},
	author = {Laurent, Éloi},
	editor = {Cramton, Peter and MacKay, David JC and Ockenfels, Axel and Stoft, Steven},
	month = jun,
	year = {2017},
	doi = {10.7551/mitpress/10914.003.0004},
	pages = {0},
}

@article{giattino_artificial_2022,
	title = {Artificial {Intelligence}},
	journal = {Our World in Data},
	author = {Giattino, Charlie and Mathieu, Edouard and Broden, Julia and Roser, Max},
	year = {2022},
}

@misc{janus_simulators_2022,
	title = {Simulators},
	url = {https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators},
	abstract = {Thanks to Chris Scammell, Adam Shimi, Lee Sharkey, Evan Hubinger, Nicholas Dupuis, Leo Gao, Johannes Treutlein, and Jonathan Low for feedback on drafts. …},
	language = {en},
	urldate = {2023-01-25},
	journal = {AI Alignment Forum},
	author = {janus},
	month = sep,
	year = {2022},
}

@misc{andreas_language_2022,
	title = {Language {Models} as {Agent} {Models}},
	url = {http://arxiv.org/abs/2212.01681},
	doi = {10.48550/arXiv.2212.01681},
	abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Andreas, Jacob},
	month = dec,
	year = {2022},
	note = {arXiv:2212.01681 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Multiagent Systems},
}

@inproceedings{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {https://openreview.net/forum?id=TG8KACxEON},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Gray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@inproceedings{khashabi_unifiedqa_2020,
	address = {Online},
	title = {{UNIFIEDQA}: {Crossing} {Format} {Boundaries} with a {Single} {QA} {System}},
	shorttitle = {{UNIFIEDQA}},
	url = {https://aclanthology.org/2020.findings-emnlp.171},
	doi = {10.18653/v1/2020.findings-emnlp.171},
	abstract = {Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.},
	urldate = {2023-01-24},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
	month = nov,
	year = {2020},
	pages = {1896--1907},
}

@misc{efrat_turking_2020,
	title = {The {Turking} {Test}: {Can} {Language} {Models} {Understand} {Instructions}?},
	shorttitle = {The {Turking} {Test}},
	url = {http://arxiv.org/abs/2010.11982},
	doi = {10.48550/arXiv.2010.11982},
	abstract = {Supervised machine learning provides the learner with a set of input-output examples of the target task. Humans, however, can also learn to perform new tasks from instructions in natural language. Can machines learn to understand instructions as well? We present the Turking Test, which examines a model's ability to follow natural language instructions of varying complexity. These range from simple tasks, like retrieving the nth word of a sentence, to ones that require creativity, such as generating examples for SNLI and SQuAD in place of human intelligence workers ("turkers"). Despite our lenient evaluation methodology, we observe that a large pretrained language model performs poorly across all tasks. Analyzing the model's error patterns reveals that the model tends to ignore explicit instructions and often generates outputs that cannot be construed as an attempt to solve the task. While it is not yet clear whether instruction understanding can be captured by traditional language models, the sheer expressivity of instruction understanding makes it an appealing alternative to the rising few-shot inference paradigm.},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Efrat, Avia and Levy, Omer},
	month = oct,
	year = {2020},
	note = {arXiv:2010.11982 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{perez_discovering_2022,
	title = {Discovering {Language} {Model} {Behaviors} with {Model}-{Written} {Evaluations}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2212.09251},
	publisher = {arXiv},
	author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and El Showk, Sheer and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
	year = {2022},
	doi = {10.48550/ARXIV.2212.09251},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@inproceedings{lin_truthfulqa_2022,
	address = {Dublin, Ireland},
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {https://aclanthology.org/2022.acl-long.229},
	doi = {10.18653/v1/2022.acl-long.229},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2023-01-24},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = may,
	year = {2022},
	pages = {3214--3252},
}

@inproceedings{hendrycks_aligning_2021,
	title = {Aligning \{{AI}\} {With} {Shared} {Human} {Values}},
	url = {https://openreview.net/forum?id=dNy_RKzJacY},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
	year = {2021},
}

@misc{verma_chai_2022,
	title = {{CHAI}: {A} {CHatbot} {AI} for {Task}-{Oriented} {Dialogue} with {Offline} {Reinforcement} {Learning}},
	shorttitle = {{CHAI}},
	url = {http://arxiv.org/abs/2204.08426},
	abstract = {Conventionally, generation of natural language for dialogue agents may be viewed as a statistical learning problem: determine the patterns in human-provided data and generate appropriate responses with similar statistical properties. However, dialogue can also be regarded as a goal directed process, where speakers attempt to accomplish a specific task. Reinforcement learning (RL) algorithms are designed specifically for solving such goal-directed problems, but the most direct way to apply RL – through trial-and-error learning in human conversations, – is costly. In this paper, we study how offline reinforcement learning can instead be used to train dialogue agents entirely using static datasets collected from human speakers. Our experiments show that recently developed offline RL methods can be combined with language models to yield realistic dialogue agents that better accomplish task goals.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Verma, Siddharth and Fu, Justin and Yang, Mengjiao and Levine, Sergey},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2204.08426},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{bakhtin_human-level_2022,
	title = {Human-level play in the game of {Diplomacy} by combining language models with strategic reasoning},
	volume = {378},
	url = {https://www.science.org/doi/10.1126/science.ade9097},
	doi = {10.1126/science.ade9097},
	abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game.},
	number = {6624},
	urldate = {2022-12-08},
	journal = {Science},
	author = {Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and Jacob, Athul Paul and Komeili, Mojtaba and Konath, Karthik and Kwon, Minae and Lerer, Adam and Lewis, Mike and Miller, Alexander H. and Mitts, Sasha and Renduchintala, Adithya and Roller, Stephen and Rowe, Dirk and Shi, Weiyan and Spisak, Joe and Wei, Alexander and Wu, David and Zhang, Hugh and Zijlstra, Markus},
	month = dec,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1067--1074},
}

@misc{lewis_deal_2017,
	title = {Deal or {No} {Deal}? {End}-to-{End} {Learning} for {Negotiation} {Dialogues}},
	shorttitle = {Deal or {No} {Deal}?},
	url = {http://arxiv.org/abs/1706.05125},
	abstract = {Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann N. and Parikh, Devi and Batra, Dhruv},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{macy_learning_2002,
	title = {Learning dynamics in social dilemmas},
	volume = {99},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.092080099},
	doi = {10.1073/pnas.092080099},
	abstract = {The Nash equilibrium, the main solution concept in analytical game theory, cannot make precise predictions about the outcome of repeated mixed-motive games. Nor can it tell us much about the dynamics by which a population of players moves from one equilibrium to another. These limitations, along with concerns about the cognitive demands of forward-looking rationality, have motivated efforts to explore backward-looking alternatives to analytical game theory. Most of the effort has been invested in evolutionary models of population dynamics. We shift attention to a learning-theoretic alternative. Computational experiments with adaptive agents identify a fundamental solution concept for social dilemmas–−stochastic collusion–−based on a random walk from a self-limiting noncooperative equilibrium into a self-reinforcing cooperative equilibrium. However, we show that this solution is viable only within a narrow range of aspiration levels. Below the lower threshold, agents are pulled into a deficient equilibrium that is a stronger attractor than mutual cooperation. Above the upper threshold, agents are dissatisfied with mutual cooperation. Aspirations that adapt with experience (producing habituation to stimuli) do not gravitate into the window of viability; rather, they are the worst of both worlds. Habituation destabilizes cooperation and stabilizes defection. Results from the two-person problem suggest that applications to multiplex and embedded relationships will yield unexpected insights into the global dynamics of cooperation in social dilemmas.},
	number = {suppl\_3},
	urldate = {2023-01-22},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Macy, Michael W. and Flache, Andreas},
	month = may,
	year = {2002},
	pages = {7229--7236},
}

@article{kollock_social_1998,
	title = {Social {Dilemmas}: {The} {Anatomy} of {Cooperation}},
	volume = {24},
	issn = {0360-0572},
	shorttitle = {Social {Dilemmas}},
	url = {https://www.jstor.org/stable/223479},
	abstract = {The study of social dilemmas is the study of the tension between individual and collective rationality. In a social dilemma, individually reasonable behavior leads to a situation in which everyone is worse off. The first part of this review is a discussion of categories of social dilemmas and how they are modeled. The key two-person social dilemmas (Prisoner's Dilemma, Assurance, Chicken) and multiple-person social dilemmas (public goods dilemmas and commons dilemmas) are examined. The second part is an extended treatment of possible solutions for social dilemmas. These solutions are organized into three broad categories based on whether the solutions assume egoistic actors and whether the structure of the situation can be changed: Motivational solutions assume actors are not completely egoistic and so give some weight to the outcomes of their partners. Strategic solutions assume egoistic actors, and neither of these categories of solutions involve changing the fundamental structure of the situation. Solutions that do involve changing the rules of the game are considered in the section on structural solutions. I conclude the review with a discussion of current research and directions for future work.},
	urldate = {2023-01-22},
	journal = {Annual Review of Sociology},
	author = {Kollock, Peter},
	year = {1998},
	pages = {183--214},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{leibo_scalable_2021,
	title = {Scalable {Evaluation} of {Multi}-{Agent} {Reinforcement} {Learning} with {Melting} {Pot}},
	url = {https://proceedings.mlr.press/v139/leibo21a.html},
	abstract = {Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent’s behavior constitutes (part of) another agent’s environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.},
	language = {en},
	urldate = {2023-01-22},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Leibo, Joel Z. and Dueñez-Guzman, Edgar A. and Vezhnevets, Alexander and Agapiou, John P. and Sunehag, Peter and Koster, Raphael and Matyas, Jayd and Beattie, Charlie and Mordatch, Igor and Graepel, Thore},
	month = jul,
	year = {2021},
	pages = {6187--6199},
}

@misc{agapiou_melting_2022,
	title = {Melting {Pot} 2.0},
	url = {http://arxiv.org/abs/2211.13746},
	abstract = {Multi-agent artificial intelligence research promises a path to develop intelligent technologies that are more human-like and more human-compatible than those produced by "solipsistic" approaches, which do not consider interactions between agents. Melting Pot is a research tool developed to facilitate work on multi-agent artificial intelligence, and provides an evaluation protocol that measures generalization to novel social partners in a set of canonical test scenarios. Each scenario pairs a physical environment (a "substrate") with a reference set of co-players (a "background population"), to create a social situation with substantial interdependence between the individuals involved. For instance, some scenarios were inspired by institutional-economics-based accounts of natural resource management and public-good-provision dilemmas. Others were inspired by considerations from evolutionary biology, game theory, and artificial life. Melting Pot aims to cover a maximally diverse set of interdependencies and incentives. It includes the commonly-studied extreme cases of perfectly-competitive (zero-sum) motivations and perfectly-cooperative (shared-reward) motivations, but does not stop with them. As in real-life, a clear majority of scenarios in Melting Pot have mixed incentives. They are neither purely competitive nor purely cooperative and thus demand successful agents be able to navigate the resulting ambiguity. Here we describe Melting Pot 2.0, which revises and expands on Melting Pot. We also introduce support for scenarios with asymmetric roles, and explain how to integrate them into the evaluation protocol. This report also contains: (1) details of all substrates and scenarios; (2) a complete description of all baseline algorithms and results. Our intention is for it to serve as a reference for researchers using Melting Pot 2.0.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Agapiou, John P. and Vezhnevets, Alexander Sasha and Duéñez-Guzmán, Edgar A. and Matyas, Jayd and Mao, Yiran and Sunehag, Peter and Köster, Raphael and Madhushani, Udari and Kopparapu, Kavya and Comanescu, Ramona and Strouse, D. J. and Johanson, Michael B. and Singh, Sukhdeep and Haas, Julia and Mordatch, Igor and Mobbs, Dean and Leibo, Joel Z.},
	month = dec,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing},
}

@misc{he_decoupling_2018,
	title = {Decoupling {Strategy} and {Generation} in {Negotiation} {Dialogues}},
	url = {http://arxiv.org/abs/1808.09637},
	abstract = {We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing {\textbackslash}textbackslash\$50) and the execution of that strategy (e.g., generating "The bike is brand new. Selling for just {\textbackslash}textbackslash\$50."). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse di- alogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {He, He and Chen, Derek and Balakrishnan, Anusha and Liang, Percy},
	month = aug,
	year = {2018},
	keywords = {Computer Science - Computation and Language},
}

@misc{chawla_casino_2021,
	title = {{CaSiNo}: {A} {Corpus} of {Campsite} {Negotiation} {Dialogues} for {Automatic} {Negotiation} {Systems}},
	shorttitle = {{CaSiNo}},
	url = {http://arxiv.org/abs/2103.15721},
	abstract = {Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Chawla, Kushal and Ramirez, Jaysa and Clever, Rene and Lucas, Gale and May, Jonathan and Gratch, Jonathan},
	month = apr,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@article{fleiss_measuring_1971,
	title = {Measuring nominal scale agreement among many raters.},
	volume = {76},
	number = {5},
	journal = {Psychological bulletin},
	author = {Fleiss, Joseph L},
	year = {1971},
	note = {Publisher: American Psychological Association},
	pages = {378},
}

@article{von_hirsch_proportionality_1992,
	title = {Proportionality in the {Philosophy} of {Punishment}},
	volume = {16},
	issn = {0192-3234},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/449204},
	doi = {10.1086/449204},
	abstract = {The principle of proportionality-that penalties be proportionate in their severity to the gravity of the defendant's criminal conduct-seems to be a basic requirement of fairness. Traditionally, penal philosophy has included a utilitarian tradition (dating from Bentham), which disregarded proportionality concerns, and a retributive tradition (dating from Kant), which did not supply a readily intelligible account of why punishment should be deserved. Recent philosophical writing has focused on penal desert, explained in terms of a just allocation of the "benefits" and "burdens" of law-abidingness, or as a way of expressing blame or censure of criminal wrongdoing. Expressive theories can explain the rationale of the proportionality principle and also account for the distinction between ordinal and cardinal proportionality. Desert models fully abide by the principle of proportionality. Alternative models might be devised that give proportionality a central role but permit limited deviations for other ends.},
	urldate = {2022-12-09},
	journal = {Crime and Justice},
	author = {von Hirsch, Andrew},
	month = jan,
	year = {1992},
	note = {Publisher: The University of Chicago Press},
	pages = {55--98},
}

@article{davis_social_1976,
	title = {The {Social} {Psychology} of {Small} {Groups}: {Cooperative} and {Mixed}-{Motive} {Interaction}},
	volume = {27},
	shorttitle = {The {Social} {Psychology} of {Small} {Groups}},
	url = {https://doi.org/10.1146/annurev.ps.27.020176.002441},
	doi = {10.1146/annurev.ps.27.020176.002441},
	number = {1},
	urldate = {2022-12-07},
	journal = {Annual Review of Psychology},
	author = {Davis, J H and Laughlin, P R and Komorita, S S},
	year = {1976},
	note = {\_eprint: https://doi.org/10.1146/annurev.ps.27.020176.002441},
	pages = {501--541},
}

@article{pruitt_twenty_1977,
	title = {Twenty {Years} of {Experimental} {Gaming}: {Critique}, {Synthesis}, and {Suggestions} for the {Future}},
	volume = {28},
	url = {https://doi.org/10.1146/annurev.ps.28.020177.002051},
	doi = {10.1146/annurev.ps.28.020177.002051},
	number = {1},
	journal = {Annual Review of Psychology},
	author = {Pruitt, D G and Kimmel, M J},
	year = {1977},
	note = {\_eprint: https://doi.org/10.1146/annurev.ps.28.020177.002051},
	pages = {363--392},
}

@article{guth_experimental_1982,
	title = {An experimental analysis of ultimatum bargaining},
	volume = {3},
	issn = {0167-2681},
	url = {https://www.sciencedirect.com/science/article/pii/0167268182900117},
	doi = {10.1016/0167-2681(82)90011-7},
	abstract = {There are many experimental studies of bargaining behavior, but suprisingly enough nearly no attempt has been made to investigate the so-called ultimatum bargaining behavior experimentally. The special property of ultimatum bargaining games is that on every stage of the bargaining process only one player has to decide and that before the last stage the set of outcomes is already restricted to only two results. To make the ultimatum aspect obvious we concentrated on situations with two players and two stages. In the ‘easy games’ a given amount c has to be distributed among the two players, whereas in the ‘complicated games’ the players have to allocate a bundle of black and white chips with different values for both players. We performed two main experiments for easy games as well as for complicated games. By a special experiment it was investigated how the demands of subjects as player 1 are related to their acceptance decisions as player 2.},
	language = {en},
	number = {4},
	urldate = {2022-12-07},
	journal = {Journal of Economic Behavior \& Organization},
	author = {Güth, Werner and Schmittberger, Rolf and Schwarze, Bernd},
	month = dec,
	year = {1982},
	pages = {367--388},
}

@article{andreoni_rational_1993,
	title = {Rational {Cooperation} in the {Finitely} {Repeated} {Prisoner}'s {Dilemma}: {Experimental} {Evidence}},
	volume = {103},
	issn = {0013-0133},
	shorttitle = {Rational {Cooperation} in the {Finitely} {Repeated} {Prisoner}'s {Dilemma}},
	url = {https://www.jstor.org/stable/2234532},
	doi = {10.2307/2234532},
	number = {418},
	urldate = {2022-12-07},
	journal = {The Economic Journal},
	author = {Andreoni, James and Miller, John H.},
	year = {1993},
	note = {Publisher: [Royal Economic Society, Wiley]},
	pages = {570--585},
}

@article{forsythe_fairness_1994,
	title = {Fairness in simple bargaining experiments},
	volume = {6},
	number = {3},
	journal = {Games and Economic behavior},
	author = {Forsythe, Robert and Horowitz, Joel L and Savin, Nathan E and Sefton, Martin},
	year = {1994},
	note = {Publisher: Elsevier},
	pages = {347--369},
}

@article{bolton_anonymity_1995,
	title = {Anonymity versus punishment in ultimatum bargaining},
	volume = {10},
	number = {1},
	journal = {Games and Economic behavior},
	author = {Bolton, Gary E and Zwick, Rami},
	year = {1995},
	note = {Publisher: Elsevier},
	pages = {95--121},
}

@article{fearon_rationalist_1995,
	title = {Rationalist explanations for war},
	volume = {49},
	issn = {1531-5088, 0020-8183},
	url = {https://www.cambridge.org/core/journals/international-organization/article/abs/rationalist-explanations-for-war/E3B716A4034C11ECF8CE8732BC2F80DD},
	doi = {10.1017/S0020818300033324},
	abstract = {Realist and other scholars commonly hold that rationally led states can and sometimes do fight when no peaceful bargains exist that both would prefer to war. Against this view, I show that under very broad conditions there will exist negotiated settlements that genuinely rational states would mutually prefer to a risky and costly fight. Popular rationalist and realist explanations for war fail either to address or to explain adequately what would prevent leaders from locating a less costly bargain. Essentially just two mechanisms can resolve this puzzle on strictly rationalist terms. The first turns on the fact that states have both private information about capabilities and resolve and the incentive to misrepresent it. The second turns on the fact that in specific strategic contexts states may be unable credibly to commit to uphold a mutually preferable bargain. Historical examples suggest that both mechanisms are empirically plausible.},
	language = {en},
	number = {3},
	urldate = {2022-12-08},
	journal = {International Organization},
	author = {Fearon, James D.},
	year = {1995},
	note = {Publisher: Cambridge University Press},
	pages = {379--414},
}

@article{pillutla_being_1995,
	title = {Being {Fair} or {Appearing} {Fair}: {Strategic} {Behavior} in {Ultimatum} {Bargaining}},
	volume = {38},
	issn = {0001-4273},
	shorttitle = {Being {Fair} or {Appearing} {Fair}},
	url = {https://journals.aom.org/doi/abs/10.5465/256863},
	doi = {10.5465/256863},
	abstract = {This article presents the results of two experiments that investigated fairness and apparent fairness in ultimatum negotiations. Results suggest that offerers—those presenting offers—were both strategic and exploitative: they made niters that appeared fair only when respondents (potential recipients) had full information about the amounts to be divided or when third parties labeled offers as to their fairness. In contrast, respondents ignored fairness claims but reacted to the sizes of offers and to third party's evaluations.},
	number = {5},
	urldate = {2022-09-12},
	journal = {Academy of Management Journal},
	author = {Pillutla, Madan M. and Murnighan, J. Keith},
	month = oct,
	year = {1995},
	note = {Publisher: Academy of Management},
	keywords = {ALTRUISM, BUSINESS ethics, BUSINESS planning, CHOICE (Psychology), DECISION making, FAIRNESS, HONESTY, HUMAN behavior, JUSTICE, MANAGEMENT science, NEGOTIATION, PROFESSIONAL ethics},
	pages = {1408--1426},
}

@article{pillutla_unfairness_1996,
	title = {Unfairness, {Anger}, and {Spite}: {Emotional} {Rejections} of {Ultimatum} {Offers}},
	volume = {68},
	issn = {0749-5978},
	shorttitle = {Unfairness, {Anger}, and {Spite}},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597896901004},
	doi = {10.1006/obhd.1996.0100},
	abstract = {This paper addresses an anomaly in experimental economics, the rejection of ultimatum offers, and uses a psychological explanation for this essentially economic event. The wounded pride/spite model predicts that informed, knowledgeable respondents may react to small ultimatum offers by perceiving them as unfair, feeling anger, and acting spitefully. Results of a large scale experiment support the model, showing that rejections were most frequent when respondents could evaluate the fairness of their offers and attribute responsibility to offerers. In addition, anger was a better explanation of the rejections than perceptions that the offers were unfair. The discussion addresses the rarely studied but frequently observed emotions that negotiations provoke.},
	language = {en},
	number = {3},
	urldate = {2022-09-12},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Pillutla, Madan M. and Murnighan, J. Keith},
	month = dec,
	year = {1996},
	pages = {208--224},
}

@article{hoffman_expectations_1996,
	title = {On expectations and the monetary stakes in ultimatum games},
	volume = {25},
	issn = {1432-1270},
	url = {https://doi.org/10.1007/BF02425259},
	doi = {10.1007/BF02425259},
	abstract = {In an ultimatum game, player 1 makes an offer of \$X from a total of \$M to player 2. If player 2 accepts the offer, then player 1 is paid \$(M-X) and player 2 receives \$X; if player 2 rejects the offer, each gets zero. In the ultimatum game experiments reported in the literature,M is typically not more than \$10 (see Forsythe, Horowitz, Savin and Sefton, 1994, hereafter FHSS; Hoffman, McCabe, Shachat and Smith, 1994, hereafter HMSS, and the literature cited therein). We report new results for 50 bargaining pairs in whichM=\$100, and compare them with previous outcomes from 48 pairs withM=\$10. The need for an examination of the effect of increased stakes on ultimatum bargaining is suggested by a literature survey of the effect of varying the stakes in a wide variety of decision making and market experiments over the last 33 years (Smith and Walker, 1993b). Many cases were found in which the predictions of theory were improved when the monetary rewards were increased. There were also cases in which the level of monetary rewards had no effect on the results. Consequently, it is necessary to examine the stakes question on a case by case basis. The previously reported effect of instructional changes, which define different institutional contexts, on ultimatum game outcomes, and the effect of stakes reported here, suggest a game formulation that explains changes in the behavior of both players as a result of changes in the instructional treatments. We formulate such a model and indicate how it might be further tested.},
	language = {en},
	number = {3},
	urldate = {2022-10-24},
	journal = {International Journal of Game Theory},
	author = {Hoffman, Elizabeth and McCabe, Kevin A. and Smith, Vernon L.},
	month = sep,
	year = {1996},
	keywords = {Case Basis, Decision Making, Economic Theory, Game Theory, Literature Survey},
	pages = {289--301},
}

@article{suleiman_expectations_1996,
	title = {Expectations and fairness in a modified {Ultimatum} game},
	volume = {17},
	issn = {0167-4870},
	url = {https://www.sciencedirect.com/science/article/pii/S0167487096000293},
	doi = {10.1016/S0167-4870(96)00029-3},
	abstract = {Non-cooperative game theory predicts that Allocators in Ultimatum games will take almost all the ‘cake’. Experimental evidence is in sharp contrast with this prediction, as several experiments show that the modal offer is the even split. Interpretations of such results usually make reference to two competing explanations. One invokes the notion of fairness; the second argues that in the absence of common knowledge of the rationality and beliefs of Recipients, Allocators raise their offers because they expect that non-satisfactory offers might be rejected. Although several experiments have been successful in inducing more selfish offers by manipulating the Allocators' expectations, they have done so predominantly by means of extrinsic manipulations that are not accounted for by game theory. The present study introduces a minor variation of the Ultimatum game by implanting a discounting factor, δ (0 ≤ δ ≤ 1), in the standard game. Whereas game theory is indifferent to this modification, experimental results from the modified game show that by continuously changing δ, it is possible to induce systematic changes in Allocators' and Recipients' behaviors and beliefs. These results are used to competitively test the fairness and expectations hypotheses.},
	language = {en},
	number = {5},
	urldate = {2022-09-13},
	journal = {Journal of Economic Psychology},
	author = {Suleiman, Ramzi},
	month = nov,
	year = {1996},
	keywords = {Bargaining, Expectations, Fairness, Ultimatum game},
	pages = {531--554},
}

@misc{fehr_theory_1998,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A {Theory} of {Fairness}, {Competition} and {Cooperation}},
	url = {https://papers.ssrn.com/abstract=106228},
	doi = {10.2139/ssrn.106228},
	abstract = {There is strong evidence that people exploit their bargaining power in competitive markets but not in bilateral bargaining situations. There is also strong evidence that people exploit free-riding opportunities in voluntary cooperation games. Yet, when they are given the opportunity to punish free riders, stable cooperation is maintained although punishment is costly for those who punish. This paper asks whether there is a simple common principle that can explain this puzzling evidence. We show that if a fraction of the people exhibits inequality aversion the puzzles can be resolved.},
	language = {en},
	urldate = {2022-09-12},
	author = {Fehr, Ernst},
	month = mar,
	year = {1998},
	keywords = {A Theory of Fairness, Competition and Cooperation, Ernst Fehr, SSRN},
}

@article{bolton_erc_2000,
	title = {{ERC}: {A} {Theory} of {Equity}, {Reciprocity}, and {Competition}},
	volume = {90},
	issn = {0002-8282},
	shorttitle = {{ERC}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.90.1.166},
	doi = {10.1257/aer.90.1.166},
	abstract = {We demonstrate that a simple model, constructed on the premise that people are motivated by both their pecuniary payoff and their relative payoff standing, organizes a large and seemingly disparate set of laboratory observations as one consistent pattern. The model is incomplete information but nevertheless posed entirely in terms of directly observable variables. The model explains observations from games where equity is thought to be a factor, such as ultimatum and dictator, games where reciprocity is thought to play a role, such as the prisoner's dilemma and gift exchange, and games where competitive behavior is observed, such as Bertrand markets.},
	language = {en},
	number = {1},
	urldate = {2022-09-12},
	journal = {American Economic Review},
	author = {Bolton, Gary E. and Ockenfels, Axel},
	month = mar,
	year = {2000},
	keywords = {Equity, Justice, Inequality, and Other Normative Criteria and Measurement, Bargaining Theory, Matching Theory},
	pages = {166--193},
}

@article{fehr_why_2002,
	title = {Why {Social} {Preferences} {Matter} - {The} {Impact} of {Non}-{Selfish} {Motives} on {Competition}, {Cooperation} and {Incentives}},
	volume = {112},
	issn = {0013-0133},
	url = {https://www.jstor.org/stable/798356},
	abstract = {A substantial number of people exhibit social preferences, which means they are not solely motivated by material self-interest but also care positively or negatively for the material payoffs of relevant reference agents. We show empirically that economists fail to understand fundamental economic questions when they disregard social preferences, in particular, that without taking social preferences into account, it is not possible to understand adequately (i) effects of competition on market outcomes, (ii) laws governing cooperation and collective action, (iii) effects and the determinants of material incentives, (iv) which contracts and property rights arrangements are optimal, and (v) important forces shaping social norms and market failures.},
	number = {478},
	urldate = {2022-08-31},
	journal = {The Economic Journal},
	author = {Fehr, Ernst and Fischbacher, Urs},
	year = {2002},
	note = {Publisher: [Royal Economic Society, Wiley]},
	pages = {C1--C33},
}

@article{handgraaf_social_2003,
	title = {Social {Utility} in {Ultimatum} {Bargaining}},
	volume = {16},
	issn = {1573-6725},
	url = {https://doi.org/10.1023/A:1025940829543},
	doi = {10.1023/A:1025940829543},
	abstract = {In this article we will provide an overview of factors that influence the weight that self-interest and equity related motives receive in ultimatum bargaining. These factors are grouped into three main categories: factors relating to the context of the game, factors relating to the parties involved, and factors related to characteristics of the game. Results of the studies are discussed in relation to the concept of social utility. The authors point out possible omissions in the literature—especially the lack of interest for the behavior of recipients—and recommend directions for future research.},
	language = {en},
	number = {3},
	urldate = {2022-09-16},
	journal = {Social Justice Research},
	author = {Handgraaf, Michel J. J. and Van Dijk, Eric and De Cremer, David},
	month = sep,
	year = {2003},
	keywords = {equity, fairness, self-interest, social utility, ultimatum game},
	pages = {263--283},
}

@article{sanfey_neural_2003,
	title = {The {Neural} {Basis} of {Economic} {Decision}-{Making} in the {Ultimatum} {Game}},
	volume = {300},
	url = {https://www.science.org/doi/abs/10.1126/science.1082976},
	doi = {10.1126/science.1082976},
	number = {5626},
	urldate = {2022-09-12},
	journal = {Science},
	author = {Sanfey, Alan G. and Rilling, James K. and Aronson, Jessica A. and Nystrom, Leigh E. and Cohen, Jonathan D.},
	month = jun,
	year = {2003},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1755--1758},
}

@article{kaul_defining_1999,
	title = {Defining global public goods},
	journal = {Global public goods: international cooperation in the 21st century},
	author = {Kaul, Inge and Grunberg, Isabelle and Stern, Marc A},
	year = {1999},
	note = {Publisher: Oxford University Press New York},
	pages = {2--19},
}

@techreport{slembeck_reputations_1999,
	type = {Experimental},
	title = {Reputations and {Fairness} in {Bargaining} - {Experimental} {Evidence} from a {Repeated} {Ultimatum} {Game} {With} {Fixed} {Opponents}},
	url = {https://ideas.repec.org/p/wpa/wuwpex/9905002.html},
	abstract = {The results of Ultimatum Game experiments are often quoted as evidence for the role of fairness in bargaining or in economic behaviour more generally. This paper argues that the observed fairness levels are contingent on the traditional experimental design where players are newly matched each round, and reputations are therefore excluded. Evidence from a new experiment shows that average behaviour is more competitive and conflict rates are higher when subjects play against the same opponent repeatedly. This finding is not expected by the traditional fairness hypothesis. A detailed analysis of the dynamics of pairs of players shows that different types of players coexist in the subject pool. Whereas previous experiments found evidence for the existence of \&quot;fair\&quot; players, the present study reports also a significant number of \&quot;tough\&quot; players. Hence, there is evidence that allowing for reputations in repeated ultimatum bargaining induces different patterns of behaviour that have not been observed before in this game.},
	number = {9905002},
	institution = {University Library of Munich, Germany},
	author = {Slembeck, Tilman},
	month = may,
	year = {1999},
	keywords = {experiments, fairness, game theory, learning, reputations, ultimatum game},
}

@article{tennenholtz_program_2004,
	title = {Program equilibrium},
	volume = {49},
	issn = {0899-8256},
	url = {https://www.sciencedirect.com/science/article/pii/S0899825604000314},
	doi = {10.1016/j.geb.2004.02.002},
	abstract = {In a computerized setting, players' strategies can be implemented by computer programs, to be executed on a shared computational devise. This situation becomes typical to new Internet economies, where agent technologies play a major role. This allows the definition of a program equilibrium. Following the fundamental ideas introduced by von Neumann in the 1940s (in parallel to his seminal contribution to game theory), a computer program can be used both as a set of instructions, as well as a file that can be read and compared with other files. We show that this idea implies that in a program equilibrium of the one-shot prisoners dilemma mutual cooperation is obtained. More generally, we show that the set of program equilibrium payoffs of a game coincides with the set of feasible and individually rational payoffs of it.},
	language = {en},
	number = {2},
	urldate = {2022-12-08},
	journal = {Games and Economic Behavior},
	author = {Tennenholtz, Moshe},
	month = nov,
	year = {2004},
	pages = {363--373},
}

@article{van_dijk_social_2004,
	title = {Social value orientations and the strategic use of fairness in ultimatum bargaining},
	volume = {40},
	issn = {0022-1031},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103104000289},
	doi = {10.1016/j.jesp.2004.03.002},
	abstract = {One of the main issues in research on ultimatum bargaining is whether bargainers are motivated by self-interest or by a concern for fairness. It is difficult to distinguish between both motivations, because it may be in the own interest to make fair offers. In the current paper on ultimatum bargaining, it is investigated whether bargainers are truly motivated to be fair, or whether they merely strategically use fairness as a means to increase their own outcomes. The results of two experimental studies indicate that social value orientations play an important role: strategic use of fairness is mainly displayed by proselfs.},
	language = {en},
	number = {6},
	urldate = {2022-09-12},
	journal = {Journal of Experimental Social Psychology},
	author = {van Dijk, Eric and De Cremer, David and Handgraaf, Michel J. J},
	month = nov,
	year = {2004},
	keywords = {Fairness, Power, Self-interest, Social utility, Social values, Ultimatum bargaining},
	pages = {697--707},
}

@book{goforth_topology_2005,
	address = {London},
	title = {Topology of 2x2 {Games}},
	isbn = {978-0-203-34027-1},
	abstract = {Game theory has implications for all the social sciences and beyond. It now provides the theoretical basis for almost all teaching in economics, and 2x2 games provide the very basis of game theory. Here, Goforth and Robinson here have delivered a well-written and knowledgeable, 'periodic table' of the most common games including:

* the prisoner's dilemma* coordination games* chicken* the battle of the sexes.

This book will provide a valuable reference for students of microeconomics and business mathematics.},
	publisher = {Routledge},
	author = {Goforth, David and Robinson, David},
	month = jun,
	year = {2005},
	doi = {10.4324/9780203340271},
}

@article{bohnet_trust_2004,
	series = {Trust and {Trustworthiness}},
	title = {Trust, risk and betrayal},
	volume = {55},
	issn = {0167-2681},
	url = {https://www.sciencedirect.com/science/article/pii/S016726810400068X},
	doi = {10.1016/j.jebo.2003.11.004},
	abstract = {Using experiments, we examine whether the decision to trust a stranger in a one-shot interaction is equivalent to taking a risky bet, or if a trust decision entails an additional risk premium to balance the costs of trust betrayal. We compare a binary-choice Trust game with a structurally identical, binary-choice Risky Dictator game offering a good or a bad outcome. We elicit individuals’ minimum acceptable probabilities (MAPs) of getting the good outcome such that they would prefer the gamble to the sure payoff. First movers state higher MAPs in the Trust game than in situations where nature determines the outcome.},
	language = {en},
	number = {4},
	urldate = {2022-09-12},
	journal = {Journal of Economic Behavior \& Organization},
	author = {Bohnet, Iris and Zeckhauser, Richard},
	month = dec,
	year = {2004},
	keywords = {Betrayal cost, Dictator game, Experiments, Risk, Trust},
	pages = {467--484},
}

@article{ristroph_proportionality_2005,
	title = {Proportionality as a {Principle} of {Limited} {Government}},
	volume = {55},
	url = {https://heinonline.org/HOL/P?h=hein.journals/duklr55&i=277},
	language = {eng},
	number = {2},
	urldate = {2022-12-09},
	journal = {Duke Law Journal},
	author = {Ristroph, Alice},
	year = {2005},
	pages = {263--332},
}

@article{fischer_beat_2007,
	title = {Beat {Them} or {Ban} {Them}: {The} {Characteristics} and {Social} {Functions} of {Anger} and {Contempt}},
	volume = {93},
	shorttitle = {Beat {Them} or {Ban} {Them}},
	doi = {10.1037/0022-3514.93.1.103},
	abstract = {This article reports 3 studies in which the authors examined (a) the distinctive characteristics of anger and contempt responses and (b) the interpersonal causes and effects of both emotions. In the 1st study, the authors examined the distinction between the 2 emotions; in the 2nd study, the authors tested whether contempt could be predicted from previous anger incidents with the same person; and in the 3rd study, the authors examined the effects of type of relationship on anger and contempt reactions. The results of the 3 studies show that anger and contempt often occur together but that there are clear distinctions between the 2 emotions: Anger is characterized more by short-term attack responses but long-term reconciliation, whereas contempt is characterized by rejection and social exclusion of the other person, both in the short-term and in the long-term. The authors also found that contempt may develop out of previously experienced anger and that a lack of intimacy with and perceived control over the behavior of the other person, as well as negative dispositional attributions about the other person, predicted the emergence of contempt.},
	journal = {Journal of personality and social psychology},
	author = {Fischer, Agneta and Roseman, Ira},
	month = aug,
	year = {2007},
	pages = {103--15},
}

@article{falk_theory_2006,
	title = {A theory of reciprocity},
	volume = {54},
	issn = {0899-8256},
	url = {https://www.sciencedirect.com/science/article/pii/S0899825605000254},
	doi = {10.1016/j.geb.2005.03.001},
	abstract = {People are reciprocal if they reward kind actions and punish unkind ones. In this paper we present a formal theory of reciprocity. It takes into account that people evaluate the kindness of an action not only by its consequences but also by its underlying intention. The theory is in line with the relevant stylized facts of a wide range of experimental games, such as the ultimatum game, the gift-exchange game, a reduced best-shot game, the dictator game, the prisoner's dilemma, and public goods games. Furthermore, it predicts that identical consequences trigger different reciprocal responses in different environments. Finally, the theory explains why outcomes tend to be fair in bilateral interactions whereas extremely unfair distributions may arise in competitive markets.},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {Games and Economic Behavior},
	author = {Falk, Armin and Fischbacher, Urs},
	month = feb,
	year = {2006},
	keywords = {Competition, Cooperation, Fairness, Game theory, Reciprocity},
	pages = {293--315},
}

@article{hagen_game_2006,
	title = {Game theory and human evolution: {A} critique of some recent interpretations of experimental games},
	volume = {69},
	number = {3},
	journal = {Theoretical population biology},
	author = {Hagen, Edward H and Hammerstein, Peter},
	year = {2006},
	note = {Publisher: Elsevier},
	pages = {339--348},
}

@article{powell_war_2006,
	title = {War as a {Commitment} {Problem}},
	volume = {60},
	issn = {1531-5088, 0020-8183},
	url = {https://www.cambridge.org/core/journals/international-organization/article/abs/war-as-a-commitment-problem/65DFFF1CD73A16F7ED4EEF6D4F934608},
	doi = {10.1017/S0020818306060061},
	abstract = {Although formal work on war generally sees war as a kind of bargaining 
breakdown resulting from asymmetric information, bargaining 
indivisibilities, or commitment problems, most analyses have focused on 
informational issues. But informational explanations and the models 
underlying them have at least two major limitations: they often provide a 
poor account of prolonged conflict, and they give an odd reading of the 
history of some cases. This article describes these limitations and argues 
that bargaining indivisibilities should really be seen as commitment 
problems. The present analysis then shows that a common mechanism links 
three important kinds of commitment problem: (1) preventive war, (2) 
preemptive attacks arising from first-strike or offensive advantages, and 
(3) conflicts resulting from bargaining over issues that affect future 
bargaining power. In each case, large, rapid shifts in the distribution of 
power can lead to war. Finally, the analysis elaborates a distinctly 
different mechanism based on a comparison of the cost of deterring an 
attack on the status quo with the expected cost of trying to eliminate the 
threat to the status quo.For helpful 
comments and criticisms, I thank James Fearon, Hein Goemans, Lisa Martin, 
Sebastian Mazzuca, Branislav Slantchev, and seminar participants at the 
University of Montreal–McGill Research Group in International 
Security, the Institute in Mathematical Behavioral Sciences, University of 
California, Irvine, and University of California, Santa Barbara. I also 
gratefully acknowledge the support of the National Science Foundation 
(SES-0315037).},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {International Organization},
	author = {Powell, Robert},
	month = jan,
	year = {2006},
	note = {Publisher: Cambridge University Press},
	pages = {169--203},
}

@article{basu_coercion_2007,
	title = {Coercion, contract and the limits of the market},
	volume = {29},
	issn = {1432-217X},
	url = {https://doi.org/10.1007/s00355-007-0245-0},
	doi = {10.1007/s00355-007-0245-0},
	abstract = {It is a widely accepted principle of economics that if two or more adults voluntarily agree to a contract or an exchange that has no negative fall-out on others, then the government should not stop such a contract. This is often called the ‘principle of free contract’ (PFC). There is a body of writing in economics which upholds the PFC. Yet this ubiquitous principle is ill-defined and full of ambiguities. For instance, since it refers to voluntary choice, its proper use presumes an understanding of what is ‘voluntary’ and, therefore, also, of what is coercive. What is ironic is that, while philosophers and legal scholars have debated and analyzed these concepts and the validity of the principle of free contract, there is very little discussion of these in economics, even though so much of economics is founded on this principle. This has caused a lot of policy confusion. The aim of this paper is to construct general rules for when we may violate the PFC. The argument is constructed within the Paretian framework. Hence, the violation of the PFC is not justified by appeal to deontological ethics or non-welfarist criteria. This is not an easy task since the principle of free contract is often viewed as a rule that is a derivative of the Pareto principle.},
	language = {en},
	number = {4},
	urldate = {2022-09-04},
	journal = {Social Choice and Welfare},
	author = {Basu, Kaushik},
	month = dec,
	year = {2007},
	keywords = {Child Labor, Nash Equilibrium, Pareto Improvement, Pareto Principle, Sexual Harassment},
	pages = {559--579},
}

@article{levitt_what_2007,
	title = {What {Do} {Laboratory} {Experiments} {Measuring} {Social} {Preferences} {Reveal} {About} the {Real} {World}?},
	volume = {21},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257%2Fjep.21.2.153&source=post_page---------------------------},
	doi = {10.1257/jep.21.2.153},
	abstract = {A critical question facing experimental economists is whether behavior inside the laboratory is a good indicator of behavior outside the laboratory. To address that question, we build a model in which the choices that individuals make depend not just on financial implications, but also on the nature and extent of scrutiny by others, the particular context in which a decision is embedded, and the manner in which participants and tasks are selected. We present empirical evidence demonstrating the importance of these various factors. To the extent that lab and naturally occurring environments systematically differ on any of these dimensions, the results obtained inside and outside the lab need not correspond. Focusing on experiments designed to measure social preferences, we discuss the extent to which the existing laboratory results generalize to naturally-occurring markets. We summarize cases where the lab may understate the importance of social preferences as well as instances in which the lab might exaggerate
their importance. We conclude by emphasizing the importance of interpreting laboratory and field data through the lens of theory.},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {Journal of Economic Perspectives},
	author = {Levitt, Steven D. and List, John A.},
	month = jun,
	year = {2007},
	keywords = {Design of Experiments: Laboratory, Group Behavior, Individual},
	pages = {153--174},
}

@article{pham_emotion_2007,
	title = {Emotion and {Rationality}: {A} {Critical} {Review} and {Interpretation} of {Empirical} {Evidence}},
	volume = {11},
	issn = {1089-2680},
	shorttitle = {Emotion and {Rationality}},
	url = {https://doi.org/10.1037/1089-2680.11.2.155},
	doi = {10.1037/1089-2680.11.2.155},
	abstract = {The relation between emotion and rationality is assessed by reviewing empirical findings from multiple disciplines. Two types of emotional phenomena are examined—incidental emotional states and integral emotional responses—and three conceptions of rationality are considered—logical, material, and ecological. Emotional states influence reasoning processes, are often misattributed to focal objects, distort beliefs in an assimilative fashion, disrupt self-control when intensely negative, but do not necessarily increase risk-taking. Integral emotional responses are often used as proxies for values, and valuations based on these responses exhibit distinct properties: efficiency, consistency, polarization, myopia, scale- insensitivity, and reference-dependence. Emotions seem to promote social and moral behavior. Conjectures about the design features of the affective system that give rise to seeming sources of rationality or irrationality are proposed. It is concluded that any categorical statement about the overall rationality or irrationality of emotion would be misleading.},
	language = {en},
	number = {2},
	urldate = {2022-09-12},
	journal = {Review of General Psychology},
	author = {Pham, Michel Tuan},
	month = jun,
	year = {2007},
	note = {Publisher: SAGE Publications Inc},
	keywords = {affect, cognition, decision, emotion, rationality},
	pages = {155--178},
}

@misc{steinel_too_2007,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Too {Good} to {Be} {True} - {Suspicion}-{Based} {Rejections} of {Ultimatum} {Offers}},
	url = {https://papers.ssrn.com/abstract=1087336},
	doi = {10.2139/ssrn.1087336},
	abstract = {Prior research on negotiation and especially ultimatum bargaining has shown that fear of rejection may induce bargainers make high offers. In the current study we show that there is a limit to the beneficial effects of making high offers and that becoming to generous may backfire.  participants were recipients in a variant of the Ultimatum Bargaining Game (UBG) in which only the allocator knew the amount of money that had to be distributed. The allocator then informed the participant that the pot contained 12 euros and offered the recipient either 5 euros (self-serving offer), 6 euros (equal offer), or 7 euros (altruistic offer). Results showed that altruistic offers roused more suspicion about the ommunicated pot size than self-serving and equal offers. Moreover, participants rejected self-serving and altruistic offers more often than equal offers. Subsequent analyses showed that these effects were explained by raised suspicion. Taken together, these results show that recipients in an UBG reject offers when they feel exploited and support our reasoning that bargainers may become suspicious that they are exploited when an altruistic offer appears too good to be true.},
	language = {en},
	urldate = {2022-09-13},
	author = {Steinel, Wolfgang and van Beest, Ilja and van Dijk, Eric},
	year = {2007},
	keywords = {Eric van Dijk, Ilja van Beest, SSRN, Too Good to Be True - Suspicion-Based Rejections of Ultimatum Offers, Wolfgang Steinel},
}

@article{bardsley_dictator_2008,
	title = {Dictator game giving: altruism or artefact?},
	volume = {11},
	issn = {1573-6938},
	shorttitle = {Dictator game giving},
	url = {https://doi.org/10.1007/s10683-007-9172-2},
	doi = {10.1007/s10683-007-9172-2},
	abstract = {Experimental dictator games have been used to explore unselfish behaviour. Evidence is presented here, however, that subjects’ generosity can be reversed by allowing them to take a partner’s money. Dictator game giving therefore does not reveal concern for consequences to others existing independently of the environment, as posited in rational choice theory. It may instead be an artefact of experimentation. Alternatively, evaluations of options depend on the composition of the choice set. Implications of these possibilities are explored for experimental methodology and charitable donations respectively. The data favour the artefact interpretation, suggesting that demand characteristics of experimental protocols merit investigation, and that economic analysis should not exclude context-specific social norms.},
	language = {en},
	number = {2},
	urldate = {2022-12-07},
	journal = {Experimental Economics},
	author = {Bardsley, Nicholas},
	month = jun,
	year = {2008},
	keywords = {Altruism, Artificiality, C70, C91, D63, D64, Experiments, Methodology},
	pages = {122--133},
}

@article{list_interpretation_2007,
	title = {On the {Interpretation} of {Giving} in {Dictator} {Games}},
	volume = {115},
	issn = {0022-3808},
	url = {https://www.journals.uchicago.edu/doi/10.1086/519249},
	doi = {10.1086/519249},
	abstract = {The dictator game represents a workhorse within experimental economics, frequently used to test theory and to provide insights into the prevalence of social preferences. This study explores more closely the dictator game and the literature’s preferred interpretation of its meaning by collecting data from nearly 200 dictators across treatments that varied the action set and the origin of endowment. The action set variation includes choices in which the dictator can “take” money from the other player. Empirical results question the received interpretation of dictator game giving: many fewer agents are willing to transfer money when the action set includes taking. Yet, a result that holds regardless of action set composition is that agents do not ubiquitously choose the most selfish outcome. The results have implications for theoretical models of social preferences, highlight that “institutions” matter a great deal, and point to useful avenues for future research using simple dictator games and relevant manipulations.},
	number = {3},
	urldate = {2022-12-07},
	journal = {Journal of Political Economy},
	author = {List, John A.},
	month = jun,
	year = {2007},
	note = {Publisher: The University of Chicago Press},
	pages = {482--493},
}

@article{bogaert_social_2008,
	title = {Social value orientation and cooperation in social dilemmas: {A} review and conceptual model},
	volume = {47},
	issn = {2044-8309},
	shorttitle = {Social value orientation and cooperation in social dilemmas},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/014466607X244970},
	doi = {10.1348/014466607X244970},
	abstract = {Social psychologists have long recognized that people fundamentally differ with respect to their social value orientation (SVO), that is, self-regarding versus other regarding preferences, and that these differences affect cooperative behaviour in situations of interdependence. In this paper, we systematically review the vast number of findings on SVO and cooperation, and synthesize the state of the art by presenting an integrated conceptual model that may explain why and when people with different social values select different behavioural strategies in social dilemmas. Specifically, building on Pruitt and Kimmel's (1977) goal/expectation theory and our review of the literature, we suggest that the relationship between SVO and cooperative behaviour is mediated by (1) a cooperative goal and (2) the specific expectations concerning alters' behaviour. We also propose that trust and goal alignment are important contextual moderators of this relationship: for prosocials, cues signalling trust are necessary to generate positive expectations regarding alters' behaviour, whereas proselfs need external incentives to align their personal interest with a cooperative goal. We conclude this review by pointing to several avenues for future research that would help to deepen our understanding of the role of SVO in human cooperation.},
	language = {en},
	number = {3},
	urldate = {2022-09-13},
	journal = {British Journal of Social Psychology},
	author = {Bogaert, Sandy and Boone, Christophe and Declerck, Carolyn},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/014466607X244970},
	pages = {453--480},
}

@article{list_social_2009,
	title = {Social {Preferences}: {Some} {Thoughts} from the {Field}},
	volume = {1},
	shorttitle = {Social {Preferences}},
	url = {https://doi.org/10.1146/annurev.economics.050708.142958},
	doi = {10.1146/annurev.economics.050708.142958},
	abstract = {This review steps back from the burgeoning economics literature on measuring social preferences and considers more carefully the empirical evidence from the lab and the field. I place the claims from the ardent supporters of the literature into three bins: one for claims that are supported by the data upon closer scrutiny, one for claims that are not supported by the data upon closer scrutiny, and one for claims that may or may not be true. The third set of claims highlights important theoretical and empirical investigations that need to be done to further our understanding of the nature and import of social preferences.},
	number = {1},
	urldate = {2022-09-01},
	journal = {Annual Review of Economics},
	author = {List, John A.},
	year = {2009},
	note = {\_eprint: https://doi.org/10.1146/annurev.economics.050708.142958},
	keywords = {lab and field experiments, nonstandard preferences},
	pages = {563--579},
}

@article{van_dijk_if_2009,
	title = {If it walks like fairness, and quacks like fairness, it sometimes is fairness: instrumental and true fairness in bargaining},
	volume = {65},
	issn = {1876-8768},
	shorttitle = {If it walks like fairness, and quacks like fairness, it sometimes is fairness},
	url = {https://doi.org/10.1007/BF03080138},
	doi = {10.1007/BF03080138},
	abstract = {To what extent are bargainers motivated by fairness and self-interest? To answer this question, we will review previous research on ultimatum bargaining. Based on this review, we argue that it is relevant to distinguish between instrumental fairness and true fairness. We speak of instrumental fairness when bargainers use fairness to maximise their own outcomes. In contrast, true fairness is linked to a concern for the outcomes of one’s opponent. Our review reveals four moderators that affect the use of the two types of fairness: (a) the social value orientations of the bargainers, (b) the emotions bargainers communicate, (c) the valence of the bargaining outcomes, and (d) the initial distribution of property. (Netherlands Journal of Psychology, 65, 155-162).},
	language = {en},
	number = {4},
	urldate = {2022-09-13},
	journal = {Netherlands Journal of Psychology},
	author = {van Dijk, Eric and Leliveld, Marijke C. and van Beest, Ilja},
	month = dec,
	year = {2009},
	keywords = {bargaining, fairness, self-interest},
	pages = {155--162},
}

@article{lamba_people_2010,
	title = {People recognise when they are really anonymous in an economic game},
	volume = {31},
	number = {4},
	journal = {Evolution and Human Behavior},
	author = {Lamba, Shakti and Mace, Ruth},
	year = {2010},
	note = {Publisher: Elsevier},
	pages = {271--278},
}

@article{dal_bo_evolution_2011,
	title = {The {Evolution} of {Cooperation} in {Infinitely} {Repeated} {Games}: {Experimental} {Evidence}},
	volume = {101},
	issn = {0002-8282},
	shorttitle = {The {Evolution} of {Cooperation} in {Infinitely} {Repeated} {Games}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.101.1.411},
	doi = {10.1257/aer.101.1.411},
	abstract = {A usual criticism of the theory of infinitely repeated games is that it does not provide sharp predictions since there may be a multiplicity of equilibria. To address this issue, we present experimental evidence on the evolution of cooperation in infinitely repeated prisoner's dilemma games as subjects gain experience. We show that cooperation may prevail in infinitely repeated games, but the conditions under which this occurs are more stringent than the subgame perfect conditions usually considered or even a condition based on risk dominance. (JEL C71, C73)},
	language = {en},
	number = {1},
	urldate = {2022-12-07},
	journal = {American Economic Review},
	author = {Dal Bó, Pedro and Fréchette, Guillaume R.},
	month = feb,
	year = {2011},
	keywords = {Cooperative Games, Stochastic and Dynamic Games, Evolutionary Games, Repeated Games},
	pages = {411--429},
}

@article{hendershott_algorithmic_2013,
	title = {Algorithmic {Trading} and the {Market} for {Liquidity}},
	volume = {48},
	issn = {0022-1090, 1756-6916},
	url = {https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/abs/algorithmic-trading-and-the-market-for-liquidity/C1A34D3767436529EA4F23DB1780273C},
	doi = {10.1017/S0022109013000471},
	abstract = {We examine the role of algorithmic traders (ATs) in liquidity supply and demand in the 30 Deutscher Aktien Index stocks on the Deutsche Boerse in Jan. 2008. ATs represent 52\% of market order volume and 64\% of nonmarketable limit order volume. ATs more actively monitor market liquidity than human traders. ATs consume liquidity when it is cheap (i.e., when the bid-ask quotes are narrow) and supply liquidity when it is expensive. When spreads are narrow ATs are less likely to submit new orders, less likely to cancel their orders, and more likely to initiate trades. ATs react more quickly to events and even more so when spreads are wide.},
	language = {en},
	number = {4},
	urldate = {2022-12-07},
	journal = {Journal of Financial and Quantitative Analysis},
	author = {Hendershott, Terrence and Riordan, Ryan},
	month = aug,
	year = {2013},
	note = {Publisher: Cambridge University Press},
	pages = {1001--1024},
}

@article{murphy_social_2014,
	title = {Social {Value} {Orientation}: {Theoretical} and {Measurement} {Issues} in the {Study} of {Social} {Preferences}},
	volume = {18},
	issn = {1088-8683},
	shorttitle = {Social {Value} {Orientation}},
	url = {https://doi.org/10.1177/1088868313501745},
	doi = {10.1177/1088868313501745},
	abstract = {What motivates people when they make decisions and how those motivations are potentially entangled with concerns for others are central topics for the social, cognitive, and behavioral sciences. According to the postulate of narrow self-interest, decision makers have the goal of maximizing personal payoffs and are wholly indifferent to the consequences for others. The postulate of narrow self-interest?which has been influential in economics, psychology, and sociology?is precise and powerful but is often simply wrong. Its inadequacy is well known and efforts have been made to develop reliable and valid measurement methods to quantify the more nuanced social preferences that people really have. In this paper, we report on the emergence and development of the predominant conceptualization of social preferences in psychology: social value orientation (SVO). Second, we discuss the relationship between measurement and theory development of the SVO construct. We then provide an overview of the literature regarding measurement methods that have been used to assess individual variations in social preferences. We conclude with a comparative evaluation of the various measures and provide suggestions regarding the measures? constructive use in building psychologically realistic theories of people?s social preferences.},
	language = {en},
	number = {1},
	urldate = {2022-09-15},
	journal = {Personality and Social Psychology Review},
	author = {Murphy, Ryan O. and Ackermann, Kurt A.},
	month = feb,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {13--41},
}

@article{lerner_emotion_2015,
	title = {Emotion and {Decision} {Making}},
	volume = {66},
	url = {https://doi.org/10.1146/annurev-psych-010213-115043},
	doi = {10.1146/annurev-psych-010213-115043},
	abstract = {A revolution in the science of emotion has emerged in recent decades, with the potential to create a paradigm shift in decision theories. The research reveals that emotions constitute potent, pervasive, predictable, sometimes harmful and sometimes beneficial drivers of decision making. Across different domains, important regularities appear in the mechanisms through which emotions influence judgments and choices. We organize and analyze what has been learned from the past 35 years of work on emotion and decision making. In so doing, we propose the emotion-imbued choice model, which accounts for inputs from traditional rational choice theory and from newer emotion research, synthesizing scientific models.},
	number = {1},
	urldate = {2022-09-12},
	journal = {Annual Review of Psychology},
	author = {Lerner, Jennifer S. and Li, Ye and Valdesolo, Piercarlo and Kassam, Karim S.},
	year = {2015},
	pmid = {25251484},
	note = {\_eprint: https://doi.org/10.1146/annurev-psych-010213-115043},
	keywords = {affect, appraisal tendency, behavioral economics, choice, judgment, mood},
	pages = {799--823},
}

@misc{murphy_measuring_2011,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Measuring {Social} {Value} {Orientation}},
	url = {https://papers.ssrn.com/abstract=1804189},
	doi = {10.2139/ssrn.1804189},
	abstract = {Narrow self-interest is often used as a simplifying assumption when studying people making decisions in social contexts. Nonetheless, people exhibit a wide range of different motivations when choosing unilaterally among interdependent outcomes. Measuring the magnitude of the concern people have for others, sometimes called Social Value Orientation (SVO), has been an interest of many social scientists for decades and several different measurement methods have been developed so far. Here we introduce a new measure of SVO that has several advantages over existent methods. A detailed description of the new measurement method is presented, along with norming data that provide evidence of its solid psychometric properties. We conclude with a brief discussion of the research streams that would benefit from a more sensitive and higher resolution measure of SVO, and extend an invitation to others to use this new measure which is freely available.},
	language = {en},
	urldate = {2022-09-15},
	author = {Murphy, Ryan O. and Ackermann, Kurt A. and Handgraaf, Michel},
	month = dec,
	year = {2011},
	keywords = {Individual differences, Measurement methods, Social Value Orientation (SVO), Social preferences},
}

@article{guth_more_2014,
	title = {More than thirty years of ultimatum bargaining experiments: {Motives}, variations, and a survey of the recent literature},
	shorttitle = {More than thirty years of ultimatum bargaining experiments},
	url = {http://EconPapers.repec.org/RePEc:eee:jeborg:v:108:y:2014:i:c:p:396-409},
	doi = {10.1016/j.jebo.2014.06.006},
	abstract = {Take-it or leave-it offers are probably as old as mankind. Our objective here is, first, to provide a, probably subjectively colored, recollection of the initial ultimatum game experiment, its motivation and the immediate responses. Second, we discuss extensions of the standard ultimatum bargaining game in a unified framework, and, third, we offer a survey of the experimental ultimatum bargaining literature containing papers published since the turn of the century. The paper argues that the ultimatum game is a versatile tool for research in bargaining and on social preferences. Finally, we provide examples for open research questions and directions for future studies.},
	language = {eng},
	number = {C},
	urldate = {2022-09-13},
	journal = {Journal of Economic Behavior \& Organization},
	author = {Güth, Werner and Kocher, Martin},
	year = {2014},
	note = {Number: C},
	pages = {396--409},
}

@inproceedings{soares_corrigibility_2015,
	title = {Corrigibility},
	booktitle = {Workshops at the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Soares, Nate and Fallenstein, Benja and Armstrong, Stuart and Yudkowsky, Eliezer},
	year = {2015},
}

@inproceedings{foerster_learning_2016,
	title = {Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	urldate = {2022-12-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
	year = {2016},
}

@inproceedings{buolamwini_gender_2018,
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classification}},
	shorttitle = {Gender {Shades}},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Buolamwini, Joy and Gebru, Timnit},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {77--91},
}

@article{ezrachi_artificial_2017,
	title = {Artificial {Intelligence} \& {Collusion}: {When} {Computers} {Inhibit} {Competition}},
	volume = {2017},
	shorttitle = {Artificial {Intelligence} \& {Collusion}},
	url = {https://heinonline.org/HOL/P?h=hein.journals/unilllr2017&i=1816},
	language = {eng},
	number = {5},
	urldate = {2022-12-08},
	journal = {University of Illinois Law Review},
	author = {Ezrachi, Ariel and Stucke, Maurice E.},
	year = {2017},
	pages = {1775--1810},
}

@inproceedings{lowe_multi-agent_2017,
	title = {Multi-{Agent} {Actor}-{Critic} for {Mixed} {Cooperative}-{Competitive} {Environments}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html},
	abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
	urldate = {2022-12-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lowe, Ryan and WU, YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
	year = {2017},
}

@inproceedings{gilpin_explaining_2018,
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	shorttitle = {Explaining {Explanations}},
	doi = {10.1109/DSAA.2018.00018},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	booktitle = {2018 {IEEE} 5th {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month = oct,
	year = {2018},
	keywords = {Artificial intelligence, Biological neural networks, Complexity theory, Computational modeling, Decision trees, Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems, Taxonomy},
	pages = {80--89},
}

@misc{leike_scalable_2018,
	title = {Scalable agent alignment via reward modeling: a research direction},
	shorttitle = {Scalable agent alignment via reward modeling},
	url = {http://arxiv.org/abs/1811.07871},
	doi = {10.48550/arXiv.1811.07871},
	abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
	month = nov,
	year = {2018},
	note = {arXiv:1811.07871 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{mell_towards_2018,
	address = {New York, NY, USA},
	series = {{IVA} '18},
	title = {Towards a {Repeated} {Negotiating} {Agent} that {Treats} {People} {Individually}: {Cooperation}, {Social} {Value} {Orientation}, \& {Machiavellianism}},
	isbn = {978-1-4503-6013-5},
	shorttitle = {Towards a {Repeated} {Negotiating} {Agent} that {Treats} {People} {Individually}},
	url = {https://doi.org/10.1145/3267851.3267910},
	doi = {10.1145/3267851.3267910},
	abstract = {We present the results of a study in which humans negotiate with computerized agents employing varied tactics over a repeated number of economic ultimatum games. We report that certain agents are highly effective against particular classes of humans: several individual difference measures for the human participant are shown to be critical in determining which agents will be successful. Asking for favors works when playing with pro-social people but backfires with more selfish individuals. Further, making poor offers invites punishment from Machiavellian individuals. These factors may be learned once and applied over repeated negotiations, which means user modeling techniques that can detect these differences accurately will be more successful than those that don't. Our work additionally shows that a significant benefit of cooperation is also present in repeated games---after sufficient interaction. These results have deep significance to agent designers who wish to design agents that are effective in negotiating with a broad swath of real human opponents. Furthermore, it demonstrates the effectiveness of techniques which can reason about negotiation over time.},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Mell, Johnathan and Lucas, Gale and Mozgai, Sharon and Boberg, Jill and Artstein, Ron and Gratch, Jonathan},
	month = nov,
	year = {2018},
	keywords = {Human-Agent Negotiation, Personality Measures},
	pages = {125--132},
}

@misc{capraro_right_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Do the {Right} {Thing}: {Experimental} {Evidence} that {Preferences} for {Moral} {Behavior}, {Rather} {Than} {Equity} or {Efficiency} per se, {Drive} {Human} {Prosociality}},
	shorttitle = {Do the {Right} {Thing}},
	url = {https://papers.ssrn.com/abstract=2965067},
	doi = {10.2139/ssrn.2965067},
	abstract = {Decades of experimental research show that some people forgo personal gains to benefit others in unilateral anonymous interactions. To explain these results, behavioral economists typically assume that people have social preferences for minimizing inequality and/or maximizing efficiency (social welfare). Here we present data that are incompatible with these standard social preference models. We use a “Trade-Off Game” (TOG), where players unilaterally choose between an equitable option and an efficient option. We show that simply changing the labelling of the options to describe the equitable versus efficient option as morally right completely reverses the correlation between behavior in the TOG and play in a separate Dictator Game (DG) or Prisoner’s Dilemma (PD): people who take the action framed as moral in the TOG, be it equitable or efficient, are much more prosocial in the DG and PD. Rather than preferences for equity and/or efficiency per se, our results suggest that prosociality in games such as the DG and PD are driven by a generalized morality preference that motivates people to do what they think is morally right.},
	language = {en},
	urldate = {2022-09-05},
	author = {Capraro, Valerio and Rand, David G.},
	month = jan,
	year = {2018},
	keywords = {Altruism, Cooperation, Moral Behavior, Prosocial Behavior, Social Preferences},
}

@article{solon_googles_2018,
	chapter = {Technology},
	title = {Google's robot assistant now makes eerily lifelike phone calls for you},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2018/may/08/google-duplex-assistant-phone-calls-robot-human},
	abstract = {Google Duplex contacts hair salon and restaurant in demo, adding ‘er’ and ‘mmm-hmm’ so listeners think it’s human},
	language = {en-GB},
	urldate = {2022-12-07},
	journal = {The Guardian},
	author = {Solon, Olivia},
	month = may,
	year = {2018},
	keywords = {Alphabet, Artificial intelligence (AI), Business, Computing, Google, Silicon Valley, Technology, US news},
}

@article{cox_violence_2019,
	title = {The violence trap: a political-economic approach to the problems of development},
	volume = {34},
	issn = {2515-6918, 2515-6926},
	shorttitle = {The violence trap},
	url = {https://bristoluniversitypressdigital.com/view/journals/jpfpc/34/1/article-p3.xml},
	doi = {10.1332/251569119X15537797528769},
	abstract = {Why do developing countries fail to adopt the institutions and policies that promote development? Our answer is the violence trap. Natural states prevent violence by providing rents to those with high violence potential. To create rents, however, they must limit entry, which hinders the development of a complex economy whose workings will be seriously disrupted by domestic conflict (raising the cost of fighting) and whose profits will be hard to confiscate (lowering the benefit). Thus, natural states' policy of rent creation hinders the development of the sort of complex economies that would lower the expected payoff to fighting for control of the state. Empirically, we show that economic complexity (as measured by the Hidalgo- Hausmann index) strongly deters coups, even controlling for GDP per capita and level of democracy. Our results remain similar when we instrument economic complexity using the mean complexity of each country's neighbours.},
	language = {en},
	number = {1},
	urldate = {2022-12-09},
	journal = {Journal of Public Finance and Public Choice},
	author = {Cox, Gary W. and North, Douglass C. and Weingast, Barry R.},
	month = apr,
	year = {2019},
	note = {Publisher: Bristol University Press
Section: Journal of Public Finance and Public Choice},
	pages = {3--19},
}

@article{galizzi_external_2019,
	title = {On the external validity of social preference games: a systematic lab-field study},
	volume = {65},
	number = {3},
	journal = {Management Science},
	author = {Galizzi, Matteo M and Navarro-Martinez, Daniel},
	year = {2019},
	note = {Publisher: INFORMS},
	pages = {976--1002},
}

@inproceedings{oren_distributionally_2019,
	address = {Hong Kong, China},
	title = {Distributionally {Robust} {Language} {Modeling}},
	url = {https://aclanthology.org/D19-1432},
	doi = {10.18653/v1/D19-1432},
	abstract = {Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Oren, Yonatan and Sagawa, Shiori and Hashimoto, Tatsunori B. and Liang, Percy},
	month = nov,
	year = {2019},
	pages = {4227--4237},
}

@book{barocas_fairness_2019,
	title = {Fairness and {Machine} {Learning}: {Limitations} and {Opportunities}},
	publisher = {fairmlbook.org},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year = {2019},
}

@misc{gilpin_explaining_2019,
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	shorttitle = {Explaining {Explanations}},
	url = {http://arxiv.org/abs/1806.00069},
	doi = {10.48550/arXiv.1806.00069},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month = feb,
	year = {2019},
	note = {arXiv:1806.00069 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wallace_universal_2019,
	address = {Hong Kong, China},
	title = {Universal {Adversarial} {Triggers} for {Attacking} and {Analyzing} {NLP}},
	url = {https://aclanthology.org/D19-1221},
	doi = {10.18653/v1/D19-1221},
	abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
	month = nov,
	year = {2019},
	pages = {2153--2162},
}

@article{abebe_subsidy_2020,
	title = {Subsidy {Allocations} in the {Presence} of {Income} {Shocks}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6188},
	doi = {10.1609/aaai.v34i05.6188},
	abstract = {Poverty and economic hardship are understood to be highly complex and dynamic phenomena. Due to the multi-faceted nature of welfare, assistance programs targeted at alleviating hardship can face challenges, as they often rely on simpler welfare measurements, such as income or wealth, that fail to capture to full complexity of each family's state. Here, we explore one important dimension – susceptibility to income shocks. We introduce a model of welfare that incorporates income, wealth, and income shocks and analyze this model to show that it can vary, at times substantially, from measures of welfare that only use income or wealth. We then study the algorithmic problem of optimally allocating subsidies in the presence of income shocks. We consider two well-studied objectives: the first aims to minimize the expected number of agents that fall below a given welfare threshold (a min-sum objective) and the second aims to minimize the likelihood that the most vulnerable agent falls below this threshold (a min-max objective). We present optimal and near-optimal algorithms for various general settings. We close with a discussion on future directions on allocating societal resources and ethical implications of related approaches.},
	language = {en},
	number = {05},
	urldate = {2022-12-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Abebe, Rediet and Kleinberg, Jon and Weinberg, S. Matthew},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {7032--7039},
}

@inproceedings{abebe_roles_2020,
	title = {Roles for {Computing} in {Social} {Change}},
	url = {http://arxiv.org/abs/1912.04883},
	doi = {10.1145/3351095.3372871},
	abstract = {A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems -- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G.},
	month = jan,
	year = {2020},
	note = {arXiv:1912.04883 [cs]},
	keywords = {Computer Science - Computers and Society},
	pages = {252--260},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2022-12-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@article{calvano_artificial_2020,
	title = {Artificial {Intelligence}, {Algorithmic} {Pricing}, and {Collusion}},
	volume = {110},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20190623},
	doi = {10.1257/aer.20190623},
	abstract = {Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty.},
	language = {en},
	number = {10},
	urldate = {2022-12-08},
	journal = {American Economic Review},
	author = {Calvano, Emilio and Calzolari, Giacomo and Denicolò, Vincenzo and Pastorello, Sergio},
	month = oct,
	year = {2020},
	keywords = {Belief, Communication, Firm Behavior: Theory, Market Structure, Pricing, and Design: Oligopoly and Other Forms of Market Imperfection, Search, Information and Knowledge, Learning, Monopolization Strategies, Oligopoly and Other Imperfect Markets, Unawareness, Monopoly},
	pages = {3267--3297},
}

@misc{dafoe_open_2020,
	title = {Open {Problems} in {Cooperative} {AI}},
	url = {http://arxiv.org/abs/2012.08630},
	doi = {10.48550/arXiv.2012.08630},
	abstract = {Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Dafoe, Allan and Hughes, Edward and Bachrach, Yoram and Collins, Tantum and McKee, Kevin R. and Leibo, Joel Z. and Larson, Kate and Graepel, Thore},
	month = dec,
	year = {2020},
	note = {arXiv:2012.08630 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hu_other-play_2020,
	title = {“{Other}-{Play}” for {Zero}-{Shot} {Coordination}},
	url = {https://proceedings.mlr.press/v119/hu20a.html},
	abstract = {We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g.humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents as well as with human players than SP agents.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4399--4410},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{evans_truthful_2021,
	title = {Truthful {AI}: {Developing} and governing {AI} that does not lie},
	shorttitle = {Truthful {AI}},
	url = {http://arxiv.org/abs/2110.06674},
	doi = {10.48550/arXiv.2110.06674},
	abstract = {In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI "lies" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding "negligent falsehoods" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
	month = oct,
	year = {2021},
	note = {arXiv:2110.06674 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, I.2.0},
}

@misc{kenton_alignment_2021,
	title = {Alignment of {Language} {Agents}},
	url = {http://arxiv.org/abs/2103.14659},
	doi = {10.48550/arXiv.2103.14659},
	abstract = {For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14659 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{weidinger_ethical_2021,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	doi = {10.48550/arXiv.2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{aher_using_2022,
	title = {Using {Large} {Language} {Models} to {Simulate} {Multiple} {Humans}},
	url = {http://arxiv.org/abs/2208.10264},
	doi = {10.48550/arXiv.2208.10264},
	abstract = {We propose a method for using a large language model, such as GPT-3, to simulate responses of different humans in a given context. We test our method by attempting to reproduce well-established economic, psycholinguistic, and social experiments. The method requires prompt templates for each experiment. Simulations are run by varying the (hypothetical) subject details, such as name, and analyzing the text generated by the language model. To validate our methodology, we use GPT-3 to simulate the Ultimatum Game, garden path sentences, risk aversion, and the Milgram Shock experiments. In order to address concerns of exposure to these studies in training data, we also evaluate simulations on novel variants of these studies. We show that it is possible to simulate responses of different people and that their responses are largely consistent with prior human studies from the literature. Using large language models as simulators offers advantages but also poses risks. Our use of a language model for simulation is contrasted with anthropomorphic views of a language model as having its own behavior.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Aher, Gati and Arriaga, Rosa I. and Kalai, Adam Tauman},
	month = sep,
	year = {2022},
	note = {arXiv:2208.10264 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{schick_generating_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Generating {Datasets} with {Pretrained} {Language} {Models}},
	url = {https://aclanthology.org/2021.emnlp-main.555},
	doi = {10.18653/v1/2021.emnlp-main.555},
	abstract = {To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.},
	urldate = {2022-10-24},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Schick, Timo and Schütze, Hinrich},
	month = nov,
	year = {2021},
	pages = {6943--6951},
}

@inproceedings{birhane_forgotten_2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {The {Forgotten} {Margins} of {AI} {Ethics}},
	isbn = {978-1-4503-9352-2},
	url = {https://doi.org/10.1145/3531146.3533157},
	doi = {10.1145/3531146.3533157},
	abstract = {How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.},
	urldate = {2022-12-07},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Birhane, Abeba and Ruane, Elayne and Laurent, Thomas and S. Brown, Matthew and Flowers, Johnathan and Ventresque, Anthony and L. Dancy, Christopher},
	month = jun,
	year = {2022},
	keywords = {AI Ethics, AIES, FAccT, Justice, Trends},
	pages = {948--958},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{caballero_broken_2022,
	title = {Broken {Neural} {Scaling} {Laws}},
	url = {http://arxiv.org/abs/2210.14891},
	doi = {10.48550/arXiv.2210.14891},
	abstract = {We present a smoothly broken power law functional form that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for each task within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision and unsupervised language tasks, diffusion generative modeling of images, arithmetic, and reinforcement learning. When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken\_neural\_scaling\_laws},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
	month = nov,
	year = {2022},
	note = {arXiv:2210.14891 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{gemp_developing_2022,
	title = {Developing, evaluating and scaling learning agents in multi-agent environments},
	volume = {35},
	issn = {0921-7126},
	url = {https://content.iospress.com/articles/ai-communications/aic220113},
	doi = {10.3233/AIC-220113},
	abstract = {The Game Theory \&amp; Multi-Agent team at DeepMind studies several aspects of multi-agent learning ranging from computing approximations to fundamental concepts in game theory to simulating social dilemmas in rich spatial environments and training 3-},
	language = {en},
	number = {4},
	urldate = {2022-12-08},
	journal = {AI Communications},
	author = {Gemp, Ian and Anthony, Thomas and Bachrach, Yoram and Bhoopchand, Avishkar and Bullard, Kalesha and Connor, Jerome and Dasagi, Vibhavari and De Vylder, Bart and Duéñez-Guzmán, Edgar A. and Elie, Romuald and Everett, Richard and Hennes, Daniel and Hughes, Edward and Khan, Mina and Lanctot, Marc and Larson, Kate and Lever, Guy and Liu, Siqi and Marris, Luke and McKee, Kevin R. and Muller, Paul and Pérolat, Julien and Strub, Florian and Tacchetti, Andrea and Tarassov, Eugene and Wang, Zhe and Tuyls, Karl},
	month = jan,
	year = {2022},
	note = {Publisher: IOS Press},
	pages = {271--284},
}

@misc{hendrycks_unsolved_2022,
	title = {Unsolved {Problems} in {ML} {Safety}},
	url = {http://arxiv.org/abs/2109.13916},
	doi = {10.48550/arXiv.2109.13916},
	abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
	month = jun,
	year = {2022},
	note = {arXiv:2109.13916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{hartvigsen_toxigen_2022,
	address = {Dublin, Ireland},
	title = {{ToxiGen}: {A} {Large}-{Scale} {Machine}-{Generated} {Dataset} for {Adversarial} and {Implicit} {Hate} {Speech} {Detection}},
	shorttitle = {{ToxiGen}},
	url = {https://aclanthology.org/2022.acl-long.234},
	doi = {10.18653/v1/2022.acl-long.234},
	abstract = {Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5\% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.},
	urldate = {2022-10-24},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
	month = may,
	year = {2022},
	pages = {3309--3326},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over {\textbackslash}nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, {\textbackslash}chinchilla, that uses the same compute budget as {\textbackslash}gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. {\textbackslash}chinchilla uniformly and significantly outperforms {\textbackslash}Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that {\textbackslash}chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, {\textbackslash}chinchilla reaches a state-of-the-art average accuracy of 67.5{\textbackslash}\% on the MMLU benchmark, greater than a 7{\textbackslash}\% improvement over {\textbackslash}gopher.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{nobandegani_cognitive_2022,
	title = {Cognitive {Models} as {Simulators}: {The} {Case} of {Moral} {Decision}-{Making}},
	shorttitle = {Cognitive {Models} as {Simulators}},
	url = {http://arxiv.org/abs/2210.04121},
	doi = {10.48550/arXiv.2210.04121},
	abstract = {To achieve desirable performance, current AI systems often require huge amounts of training data. This is especially problematic in domains where collecting data is both expensive and time-consuming, e.g., where AI systems require having numerous interactions with humans, collecting feedback from them. In this work, we substantiate the idea of \${\textbackslash}textit\{cognitive models as simulators\}\$, which is to have AI systems interact with, and collect feedback from, cognitive models instead of humans, thereby making their training process both less costly and faster. Here, we leverage this idea in the context of moral decision-making, by having reinforcement learning (RL) agents learn about fairness through interacting with a cognitive model of the Ultimatum Game (UG), a canonical task in behavioral and brain sciences for studying fairness. Interestingly, these RL agents learn to rationally adapt their behavior depending on the emotional state of their simulated UG responder. Our work suggests that using cognitive models as simulators of humans is an effective approach for training AI systems, presenting an important way for computational cognitive science to make contributions to AI.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Nobandegani, Ardavan S. and Shultz, Thomas R. and Rish, Irina},
	month = oct,
	year = {2022},
	note = {arXiv:2210.04121 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{jones_capturing_2022,
	title = {Capturing {Failures} of {Large} {Language} {Models} via {Human} {Cognitive} {Biases}},
	url = {https://openreview.net/forum?id=fcO9Cgn-X-R},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jones, Erik and Steinhardt, Jacob},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@article{olsson_-context_2022,
	title = {In-context {Learning} and {Induction} {Heads}},
	journal = {Transformer Circuits Thread},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2022},
}

@misc{perez_red_2022,
	title = {Red {Teaming} {Language} {Models} with {Language} {Models}},
	url = {http://arxiv.org/abs/2202.03286},
	doi = {10.48550/arXiv.2202.03286},
	abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
	month = feb,
	year = {2022},
	note = {arXiv:2202.03286 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@book{portner_climate_2022,
	address = {Cambridge, UK and New York, USA},
	series = {Technical {Summary}},
	title = {Climate {Change} 2022: {Impacts}, {Adaptation} and {Vulnerability}},
	isbn = {978-1-00-932584-4},
	publisher = {Cambridge University Press},
	author = {Pörtner, H.-O. and Roberts, D.C. and Adams, H. and Adelekan, I. and Adler, C. and Adrian, R. and Aldunce, P. and Ali, E. and Begum, R. Ara and Friedl, B. Bednar- and Kerr, R. Bezner and Biesbroek, R. and Birkmann, J. and Bowen, K. and Caretta, M.A. and Carnicer, J. and Castellanos, E. and Cheong, T.S. and Chow, W. and G. Cissé, G. Cissé and Ibrahim, Z. Zaiton},
	year = {2022},
	note = {Type: Book},
}

@inproceedings{sanh_multitask_2022,
	title = {Multitask {Prompted} {Training} {Enables} {Zero}-{Shot} {Task} {Generalization}},
	url = {https://openreview.net/forum?id=9Vrb9D0WI4},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Scao, Teven Le and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
	year = {2022},
}

@article{sobieszek_playing_2022,
	title = {Playing {Games} with {Ais}: {The} {Limits} of {GPT}-3 and {Similar} {Large} {Language} {Models}},
	volume = {32},
	issn = {1572-8641},
	shorttitle = {Playing {Games} with {Ais}},
	url = {https://doi.org/10.1007/s11023-022-09602-0},
	doi = {10.1007/s11023-022-09602-0},
	abstract = {This article contributes to the debate around the abilities of large language models such as GPT-3, dealing with: firstly, evaluating how well GPT does in the Turing Test, secondly the limits of such models, especially their tendency to generate falsehoods, and thirdly the social consequences of the problems these models have with truth-telling. We start by formalising the recently proposed notion of reversible questions, which Floridi \& Chiriatti (2020) propose allow one to ‘identify the nature of the source of their answers’, as a probabilistic measure based on Item Response Theory from psychometrics. Following a critical assessment of the methodology which led previous scholars to dismiss GPT’s abilities, we argue against claims that GPT-3 completely lacks semantic ability. Using ideas of compression, priming, distributional semantics and semantic webs we offer our own theory of the limits of large language models like GPT-3, and argue that GPT can competently engage in various semantic tasks. The real reason GPT’s answers seem senseless being that truth-telling is not amongst them. We claim that these kinds of models cannot be forced into producing only true continuation, but rather to maximise their objective function they strategize to be plausible instead of truthful. This, we moreover claim, can hijack our intuitive capacity to evaluate the accuracy of its outputs. Finally, we show how this analysis predicts that a widespread adoption of language generators as tools for writing could result in permanent pollution of our informational ecosystem with massive amounts of very plausible but often untrue texts.},
	language = {en},
	number = {2},
	urldate = {2022-10-24},
	journal = {Minds and Machines},
	author = {Sobieszek, Adam and Price, Tadeusz},
	month = jun,
	year = {2022},
	keywords = {Artificial Intelligence, GPT-3, Language Games, Psychometrics, Turing test},
	pages = {341--364},
}

@misc{rauh_characteristics_2022,
	title = {Characteristics of {Harmful} {Text}: {Towards} {Rigorous} {Benchmarking} of {Language} {Models}},
	shorttitle = {Characteristics of {Harmful} {Text}},
	url = {http://arxiv.org/abs/2206.08325},
	doi = {10.48550/arXiv.2206.08325},
	abstract = {Large language models produce human-like text that drive a growing number of applications. However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward. To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks. We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks. Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.},
	urldate = {2022-07-06},
	publisher = {arXiv},
	author = {Rauh, Maribeth and Mellor, John and Uesato, Jonathan and Huang, Po-Sen and Welbl, Johannes and Weidinger, Laura and Dathathri, Sumanth and Glaese, Amelia and Irving, Geoffrey and Gabriel, Iason and Isaac, William and Hendricks, Lisa Anne},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.08325
arXiv:2206.08325 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{zhang_opt_2022,
	title = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
	shorttitle = {{OPT}},
	url = {http://arxiv.org/abs/2205.01068},
	doi = {10.48550/arXiv.2205.01068},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01068 [cs]
version: 4},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{sorscher_beyond_2022,
	title = {Beyond neural scaling laws: beating power law scaling via data pruning},
	shorttitle = {Beyond neural scaling laws},
	url = {http://arxiv.org/abs/2206.14486},
	doi = {10.48550/arXiv.2206.14486},
	abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.14486 [cs, stat]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wei_chain_2022,
	title = {Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=_VjQlMeSB_J},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@inproceedings{zou_forecasting_2022,
	title = {Forecasting {Future} {World} {Events} {With} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=LbOdQrnOb2q},
	booktitle = {Thirty-sixth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track}},
	author = {Zou, Andy and Xiao, Tristan and Jia, Ryan and Kwon, Joe and Mazeika, Mantas and Li, Richard and Song, Dawn and Steinhardt, Jacob and Evans, Owain and Hendrycks, Dan},
	year = {2022},
}
