
\textbf{Datasets}. 
\textbf{EPIC-KITCHENS-100} \cite{damen2020epic} (EK100) is the largest dataset in first-person (egocentric) vision, capturing different activities in a kitchen over multiple days. There are 90K action segments, 97 verb classes, and 300 noun classes in the dataset. These action instances follow a long-tailed distribution. Some largest classes (i.e. those with most instances), which we call the many-shot classes, account for 80\% of the total number of instances in the dataset. We additionally split two subsets, containing only the many-shot verb classes and the many-shot noun classes, for our ablation study. In our experiments, the models for verb prediction and noun prediction are trained separately. Most of the clip lengths of this dataset are distributed around 128 frames, so we keep the sample duration as 128 frames, that is, 32-frames sampled with 4-frame interval and 64-frame sampled with 2-frame interval.

\textbf{EGTEA Gaze+} (EGaze+)\cite{li2018eye} is the Extended GTEA Gaze+ dataset. It contains 29 hours of first person videos from 86 unique sessions. In these sessions, 32 subjects perform 7 different meal preparation tasks in a naturalistic kitchen environment. The dataset also comes with action annotations of 10321 instances from 106 classes with an average duration of 3.2 sec, at a frame rate of 24FPS. In our experiments, we sample 32 frames from each clip with a 2-frame stride. There are three training/testing splits provided with the official dataset, and we use the first split (8299 for training, 2022 for testing) to evaluate the performance of action recognition.

\subsection{Ablation Study on Group and Stage Depth}
\label{ssec:ablation}
To prove the effectiveness of the proposed DCTG and study the hyper parameters for the PADM module, namely the number of phases G and the depth ratio (DR) of the two stages, we conduct experiments on EK100 with TimeSformer as our baseline model. All the models are initialized by the TimeSformer pre-trained on the Something-Something V2~\cite{goyal2017something} dataset.

We first compare the performance of TimeSformer with and without the proposed DCTG module. As seen in Tab.~\ref{tab:ablation}, the model is consistently improved across all columns. More specifically, the model is improved by 2.45\%/4.5\%, 2.48\%/1.84\%, 3.72\%/0.49\%, and 3.16\%/1.14\% on many-shot verb, many-shot noun, verb, and noun, respectively, with 32/64 frames sampling, which indicates that the proposed DCTG is effective in improving model performance.

As for the number of phases $G$, we compared 4-phase and 8-phase settings. From the Tab.~\ref{tab:ablation}, we can see that, with the same $DR$ value, 8-phase model always performs better than 4-phase model, which further proves the importance of action decomposition for EAR. For the depth ratio (the depth of stage 1 / the depth of stage 2), we compared the values of 0.5, 1, and 2. As shown in Tab.~\ref{tab:ablation}, in most cases, $DR=2$ produces best results, except for the $G=4$ case in many-shot verb subset. We reason that the 4-phase model cannot partition the verb (motion) of the action very well, so that the deeper the stage 1 is, the worse the model learned.

In summary, with both the DCTG module and proper PADM settings, the TimeSformer is significantly improved for all cases. Therefore, we employ the $G=8$ and $DR=2$ as the setting of PADM for the remaining experiments. Since there is not much difference in performance when using 64 or 32 frames, we only sampled 32 frames in the following experiments.

\begin{table}
\begin{center}
\caption{\textbf{Ablation study with TimeSformer on EPIC-KITCHENS-100 (EK100) and its many-shot subsets}. V and N indicate the prediction accuracy for verb class and noun class, resp., and MV and MN indicate the prediction accuracy on the many-shot verb subset and many-shot noun subset, respectively. We use ``Ours" to represent the model equipped with both DCTG and PADM modules.}
\label{tab:ablation}
\begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        Models          & T                    & G/R & MV & V  & MN & N  \\
        \hline
        TSformer & 32                      &        -       & 68        & 57.85 & 47.09     & 41.75 \\
        \hline
        +DCTG & 32                 &         -      & \textbf{70.45}     & \textbf{61.57} & \textbf{49.57}    & \textbf{44.91} \\
        \hline
        \multirow{4}{*}{+PADM} & 32  & 4/1       & 65.69     &   -    & 47.89     &   -    \\
                                        & 32    & 8/1       & 68.66     &    -   & 48.67     &    -   \\
                                        & 32    & 8/0.5       & 68.26     &   -    & 48.02     &   -    \\
                                        & 32    & 8/2       & \textbf{71.79}     & \textbf{62.33} & \textbf{50.8}     & \textbf{44.86} \\
        \hline
        \multirow{4}{*}{Ours} & 32 & 4/1       & 69.54     &    -   & 49.32     &    -   \\
                                        & 32    & 4/2       & 69.49     &    -   & 49.88     &    -   \\ 
                                        & 32    & 8/1       & 71.45     &    -   & 50.04     &    -   \\ 
                                        & 32    & 8/2       & \textbf{72.26}     & \textbf{62.42} & \textbf{50.88}     & \textbf{45.76} \\
        \hline
        TSformer                  & 64    &         -      & 65.2      & 60.21 & 48.02     & 42.36 \\
        +DCTG             & 64    &       -        & 69.7      & 60.7  & 49.86     & 43.5  \\
        +PADM               & 64    & 8/2       & 68.04     & 62.35 & 49.7      & 44.97 \\
        Ours             & 64    & 8/2      & \textbf{72.39}     & \textbf{62.38} & \textbf{51.25}     & \textbf{45.84} \\
        \hline
\end{tabular}
\end{center}
\end{table}


\subsection{Ablation Study on DCTG Module}
%\section{DCTG Ablation Study}
\label{sec:DCTG_Ablation_Study}
To investigate how to generate class tokens that can better direct the model, we conduct ablation studies on three aspects: (i) what type of off-the-shelf features can be more useful for EAR, (ii) how to explore inter-feature relationship to produce a single feature vector for each frame, and (iii) how to explore inter-frame relationship to generate a single class token for each input.
For the first aspect, we experiment with 2-class hand-object features extracted from the Hand-object detector \cite{shan2020understanding} pretrained on the 100DOH dataset \cite{shan2020understanding}, and 80-class object features extracted from the Mask-RCNN \cite{he2017mask} pretrained on MS COCO dataset \cite{lin2014microsoft}. 
For the second aspect, we try (a) applying query-key-value (QKV) self-attention feature-wise, then averaging along the feature axis; (b) directly averaging the 2M features.
For the third aspect, we experiment with two typical methods for aggregating knowledge in temporal space: QKV self-attention; and long short term memory (LSTM). 
Then, the specific class token is produced by either calculating the average frame vector from the former one or picking out the last output state vector from the latter one.

The results are shown in Table~\ref{tab:DCTG_Ablation_Study}. As can be seen,  directly averaging the hand-object features for each frame, and then applying LSTM layers provides the best performance.

\begin{table}
\begin{center}
\caption{\textbf{Ablation study with DCTG on EPIC-KITCHENS-100's many-shot subsets}. MV and MN indicate the prediction accuracy on the many-shot verb subset and many-shot noun subset, respectively.}
\label{tab:DCTG_Ablation_Study}
\begin{tabular}{|l|c|c|c|c|}
        \hline
        Feature type                 & Inter-feature        & Inter-frame & MV & MN \\
        \hline
        \multirow{4}{*}{Hand-object} & \multirow{2}{*}{QKV} & QKV & 56.24 & 36.62 \\
        \cline{3-5}
                                     &                      & LSTM & 56.64 & 36.88 \\
                                     \cline{2-5}
                                     & \multirow{2}{*}{Avg} & QKV & 57.12 & 38.31 \\
                                     \cline{3-5}
                                     &                      & LSTM & \textbf{60.81} & \textbf{38.45} \\
        \hline
        \multirow{4}{*}{Objects}     & \multirow{2}{*}{QKV} & QKV & 38.58 & 21.53 \\
        \cline{3-5}
                                     &                      & LSTM & 37.71 & 16.42 \\
                                     \cline{2-5}
                                     & \multirow{2}{*}{Avg} & QKV & 37.23 & 21.26 \\
                                     \cline{3-5}
                                     &                      & LSTM & 36.09 & 20.85 \\
        \hline
\end{tabular}
\end{center}
\end{table}


\subsection{Quantitative Results}
\label{ssec:Quantitative}
We compare our proposed EgoViT with the most well-known video transformers as well as several SOTA CNN-based models. The video transformers we compare with are TimeSformer (TSformer)~\cite{bertasius2021space}, Vivit~\cite{arnab2021vivit}, Motionformer (Mformer)~\cite{patrick2021keeping}, Swin~\cite{liu2021video}, and Object-Region Video Transformer (ORViT)~\cite{herzig2021object}. The number of training epochs, learning rate, and augmentation methods are kept the same as the corresponding transformer baselines. We set the number of hand-object features to 4 in EK100, and 2 in EGaze+. The results, summarized in Tables \ref{tab:epic100} and \ref{tab:egtea}, show that our proposed EgoViT outperforms all baseline transformers. More specifically, it improves TSformer by 2.65\%, 2.17\%,  3.48\%, and 1.1\%; improves Swin-S by 0.92\%, 1.54\%, 1.26\%, and 0.05\%;improves Swin-B by 0.2\%, 0.83\%, 0.1\%, and 0.47\%; improves Mformer-HR by 1.8\%, 2.85\%, 0.57\%, and 0.46\%, on Action (A), Verb (V), Noun (N) prediction from EK100, and top1 accuracy for EGAZE+, respectively.
\begin{table}
\begin{center}
\caption{\textbf{EAR results on the EPIC-KITCHENS-100}. First block presents the results of typical CNN-based models, while other blocks present the comparison results of previous transformers and our EgoViT-based transformers.}
\label{tab:epic100}
\begin{tabular}{|l|c|c|c|c|}
        \hline
        Models           & A     & V     & N     & pretrain \\ %& \#params & GFLOPs & views \\
        \hline
        TSN \cite{damen2020rescaling}            & 33.57 & 60.2  & 46    & IN-1K    \\ %&          &        & \\
        TRN \cite{damen2020rescaling}            & 35.28 & 65.9  & 45.4  & IN-1K    \\ %&          &        & \\
        TBN \cite{damen2020rescaling}            & 35.55 & 66    & 47.2  & IN-1K    \\ %&          &        & \\
        SlowFast \cite{feichtenhofer2019slowfast}     & 36.81 & 65.6  & 50    & K400     \\ %&          &        & \\
        TSM \cite{damen2020rescaling}             & 37.39 & 67.9  & 49    & IN-1K    \\ %&          & 66.5   & \\
        MBT \cite{nagrani2021attention}            & 43.4  & 64.8  & 58    &  -        \\ %&          &        & \\
        TempAgg \cite{sener2021technical}       & 45.26 & 66    & 53.35 &  -       \\ %&          &        & \\
        MoViNet \cite{kondratyuk2021movinets}         & \textbf{47.7}  & \textbf{72.2}  & 57.3  &          \\ %&          & 117    & \\
        \hline
        ViViT-L          & 44    & 66.4  & 56.8  & K400     \\ %& 311M     & 3992   & 4x3 \\
        \hline
        TSformer         & 38.05 & 60.21 & 42.36 & SSv2     \\ %& 121M     & 1581   & 3x1 \\
        ours-TSformer    & \textbf{40.7}  & \textbf{62.38} & \textbf{45.84} & SSv2     \\ %& 132M     & 885    & 3x1 \\
        \hline
        Swin-S           & 43.1  & 64.32 & 57.14 & K400     \\ %& 53M      & 166    & 4x3 \\
        Ours-Swin-S      & \textbf{44.02} & \textbf{65.86} & \textbf{58.4}  & K400     \\ %& 50M      & 93     & 4x3 \\
        \hline
        Swin-B           & 44.7  & 68.47 & 58.6  & K400     \\ %& 104M     & 282    & 4X3 \\
        Ours-Swin-B      & \textbf{44.9}  & \textbf{69.3}  & \textbf{58.7}  & K400     \\ %& 88M      & 159    & 4X3 \\
        \hline
        Mformer-HR       & 44.5  & 67    & 58.5  & K400     \\ %& 119M     & 958.8  & 3x1 \\
        ORViT Mformer-HR & 45.7  & 68.4  & 58.7  & K400     \\ %& 148M     & 1259   & 3x1 \\
        Ours-Mformer-HR  & \textbf{46.3} & \textbf{69.85} & \textbf{59.07} & K400     \\ %& 121M     & 775    & 3x1 \\
        \hline
\end{tabular}
\end{center}
\end{table}


\begin{table}
\begin{center}
\caption{\textbf{EAR results on the EGTEA Gaze+.} First block presents the results of typical CNN-based models, while other blocks present the comparison results of previous transformers and our EgoViT-based transformers.}
\label{tab:egtea}
\begin{tabular}{|l|c|c|c|}
        \hline
        Models          & top1 acc. & top5 acc. & mean acc. \\
        \hline
        Ego-RNN \cite{sudhakaran2018attention}        & 60.8     &    -      &   -     \\
        LSTA \cite{sudhakaran2019lsta}         & 61.9     &    -      &   -     \\
        Slowfast \cite{feichtenhofer2019slowfast}     & 49.16    & 71.27    & 37.3 \\
        SAP \cite{wang2020symbiotic}             & 62.7     &    -      &   -     \\
        min et al.\cite{min2021integrating}      & 69.58    &    -      & 62.84 \\
        \hline
        TSformer        & 62.61    & 91.79    & 54.81 \\
        ours-TSformer   & \textbf{63.71}    & \textbf{91.8}     & \textbf{56.8} \\
        \hline
        Mformer-HR      & 65.84    & 92       & 57.9 \\
        Ours-Mformer-HR & \textbf{66.3}     & \textbf{92.3}     & \textbf{58.92} \\
        \hline
        Swin-S          & 67.31    & 92.88    & 60.95 \\
        Ours-Swin-S     & \textbf{67.36}    & \textbf{93.62}    & \textbf{61} \\
        \hline
        Swin-B          & 69.25    & 95.06    & 62.38 \\
        Ours-Swin-B     & \textbf{69.72}    & \textbf{96.31}    & \textbf{63.04} \\
        \hline
\end{tabular}
\end{center}
\end{table}

Tab.~\ref{tab:model_size} lists the pretrained models, which were used to initialize the model weights, together with the number of views during inference time, which was kept the same as the setting of the corresponding transformer.~In Tab.~\ref{tab:model_size}, we also compare the number of training parameters and GFLOPS of each model. It can be seen that our model always has less GFLOPS, compared to the corresponding baseline,  which helps speeding up the training significantly, and also saves from hardware memory.~Also, we can reduce the number of parameters for Swin-S and Swin-B, and incur only a small increase (9\% and 1.6\%) compared to the TimeSformer and Mformer. ORViT is another transformer, which also uses additional detection information as the input source for action recognition. Although it can help improve the performance of Mformer, it increases the number of parameters and GFLOPs significantly. Compared to  ORViT, our model is much smaller and generates better results at the same time.

\begin{table}
\begin{center}
\caption{Transformer model size comparison and the number of views during inference time.}
\label{tab:model_size}
\begin{tabular}{|l|c|c|c|}
        \hline
        Models           & \#params & GFLOPs & views \\
        \hline
        ViViT-L          & 311M     & 3992   & 4x3   \\
        \hline
        TSformer         & \textbf{121M}     & 1581   & 3x1   \\
        Ours-TSformer    & 132M     & \textbf{885}    & 3x1   \\
        \hline
        Swin-S           & 53M      & 166    & 4x3   \\
        Ours-Swin-S      & \textbf{50M}      & \textbf{93}     & 4x3   \\
        \hline
        Swin-B           & 104M     & 282    & 4X3   \\
        Ours-Swin-B      & \textbf{88M}      & \textbf{159}    & 4X3   \\
        \hline
        Mformer-HR       & \textbf{119M}     & 958.8  & 3x1   \\
        ORViT Mformer-HR & 148M     & 1259   & 3x1   \\
        Ours-Mformer-HR  & 121M     & \textbf{775}    & 3x1   \\
        \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Qualitative Results}
\label{ssec:Qualitative}
To provide evidence that our proposed DCTG can truly help the model focus on hand-objects interactions when inferring an egocentric action, we compare the spatial attention weights of TimeSformer with and without the DCTG, i.e. we compare the static class token versus the dynamic class token. We extract and visualize the attention map, which is calculated when the CT is treated as the query in the last self-attention block of the model, by normalizing it along the spatial axes. This way, we can see the weight contribution of the informative token at each location to the class token. The results are shown in Fig.~\ref{fig:DCTG_attention}. As can be seen, with our DCTG module, the model tends to focus more on the action-related parts, i.e. the hand-object interactions (marked with green ellipses), and gives less attention to the insignificant parts (marked with red ellipses). More examples are provided in Fig.~\ref{fig:DCTG_attention2}.
%supplementary materials.
\begin{figure*}
  \centering
    \includegraphics[width=1\linewidth]{_imgs_hand_attn_2.jpg}
    %\vspace{-0.5cm}
  \caption{\textbf{Visualization of spatial attention distribution.} Top to bottom row: RGB frames from the original video, the frames with the attention map generated by TimeSformer and TimeSformer equipped with the proposed DCTG, respectively.}
  \label{fig:DCTG_attention2}
\end{figure*}

To illustrate that the dynamic merging module can properly tune the attention on class tokens of different phases, we show the calculated score for each phase of various inputs. The results in Fig.~\ref{fig:phase_attn} are produced by the TimeSformer with only PADM architecture (G=8, DR=2). It can be seen that the mechanism can always generate higher scores for the important phases ($2^{nd}$, $3^{rd}$, $4^{th}$, and $5^{th}$ phases of the ``take pasta-container" action; $5^{th}$, $6^{th}$, $7^{th}$, and $8^{th}$ phases for the ``close fridge" action). More examples are provided in Fig.~\ref{fig:PADM_Attention2}. It can be seen that the mechanism can always generate higher scores for the important phases. For instance, phases 2, 3 and 8 for ``Take eating utensil", phase 2 of ``Turn on faucet" and phases 1, 2, 6, 7 and 8 of ``Cut tomato" have high scores, and correspond to the important parts of the video most relevant to the corresponding action. 
\begin{figure}[t!]
  \centering
  \begin{minipage}[b]{1.0\linewidth}
  \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{Swin} }\end{minipage}
  \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{Swin} }\end{minipage}
  \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{TimeSformer}} \end{minipage}
  \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{TimeSformer}} \end{minipage}
  \end{minipage}
\begin{minipage}[b]{1.0\linewidth}
  \includegraphics[width=0.24\linewidth]{_imgs_fig4-1-swin.jpg}
  \includegraphics[width=0.24\linewidth]{_imgs_fig4-2-swin.jpg}
  \includegraphics[width=0.24\linewidth]{_imgs_fig4-3-tsformer.jpg}
  \includegraphics[width=0.24\linewidth]{_imgs_fig4-4-tsformer.jpg}
  \end{minipage}
  \caption{\textbf{Visualization of temporal vectors.} We use the numbers and letters to indicate the temporal ID of the feature vectors for baselines and our EgoViT, respectively. Letters ``a" to ``h" represent the $1^{st}$ to $8^{th}$ temporal positions, respectively.}
  \label{fig:t_vec}
  %\vspace{-0.4cm}
\end{figure}

To prove that there is still redundant information along the temporal axis with the previous video transformers, we visualize the distribution of average tokens at each temporal position, in the feature space of the last block, by using PCA dimension reduction algorithm. We compare our EgoViT with TimeSformer and Swin. There are 32, 16, and 8 temporal positions at the last block of TimeSformer, Swin, and our EgoViT, respectively. As shown in Fig.~\ref{fig:t_vec}, there are many overlapping points in TimeSformer and Swin. Most of them occur in consecutive frames, indicating that consecutive frames carry similar semantic meaning, and previous models cannot filter the redundant information out. For our EgoViT, there is less overlap in the feature space, which means that although its temporal dimension is smaller, it can retain complementary features and still capture rich semantic information.


\begin{figure*}
    \centering
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Take eating utensil.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__1_.jpg}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Cut tomato.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__2_.jpg}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Turn on faucet.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__3_.jpg}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Read recipe.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__4_.jpg}
    \end{minipage}
    \caption{Visualization of phase attention distribution.}
    %\vspace{-0.4cm}
    \label{fig:PADM_Attention2}
\end{figure*}

