
% \begin{figure*}
%   \centering
%   \begin{minipage}[b]{0.58\linewidth}
%     \includegraphics[width=1\linewidth]{_imgs_EgoViT2.jpg}
%     \caption{EgoViT constructed based on Video Swin Transformer.}
%     \label{fig:model}
%     \end{minipage}
%   \begin{minipage}[b]{0.4\linewidth}
%     \includegraphics[width=1\linewidth]{_imgs_phase_attn.jpg}
%     \caption{Visualization of phase attention distribution.}
%     \label{fig:phase_attn}
%     \end{minipage}
% \end{figure*}
\begin{figure*}
  \centering
 % \begin{minipage}[b]{0.58\linewidth}
    \includegraphics[width=0.75\linewidth]{_imgs_EgoViT2.jpg}
    \caption{
    %EgoViT constructed based on Video Swin Transformer.
    The structure of the proposed EgoViT with a pyramid architecture incorporating Dynamic Class Generator.}
    \label{fig:model}
 %   \end{minipage}
\end{figure*}

\begin{figure}
   \centering
\includegraphics[width=0.9\linewidth]{_imgs_phase_attn.jpg}
     \caption{Visualization of phase attention distribution.}
   \label{fig:phase_attn}
\end{figure}
We first provide a summary of transformers in Sec.~\ref{ssec:prelim}. We introduce the dynamic class token module for global and local transformer in Sec.~\ref{ssec:token}, and then present the pyramid architecture
%to further improve the model efficiency, 
in Sec.~\ref{ssec:pyramid}.

\subsection{Preliminaries}
\label{ssec:prelim}
The pure self-attention based transformer~\cite{vaswani2017attention} is mainly used as a sequence transduction model. Vision transformers~\cite{dosovitskiy2020image,arnab2021vivit} convert images/video frames into serialized data to be able to process them by a transformer. Let $I \in \mathbb{R}^{T \times H \times W \times C}$ denote the input video clip with $T$ image frames of size $H \times W \times C$.~$I$ is first segmented into non-overlapping image patches of size $\mathbb{R}^{ P_{T} \times P_{H} \times P_{W} \times C}$. Then, these patches are flattened into sequence data $\mathbb{R}^{N_{P} \times (P_{T} \times P_{H} \times P_{W} \times C)}$, where $N_{P} = T_{P} \times H_{P} \times W_{P} = \frac{T}{P_{T}} \times \frac{H}{P_{H}} \times \frac{W}{P_{W}}$. A matrix of size $\mathbb{R}^{(P_{T} \times P_{H} \times P_{W} \times C) \times D }$ is used to linearly map the patch sequence into a $D$-dimensional space.~The generated 
%$D$-dimensional 
vectors $\hat{X}_{P} \in  \mathbb{R}^{N_{P} \times D}$ are treated as the video embeddings and the main inputs of the subsequent transformer blocks.

3D video data is highly structured having strict spatial relation and temporal order. Since converting 3D video to 1D sequence
%makes the structured data 
causes losing these relations, it is necessary to preserve the 3D position information in the video embeddings at the very beginning. Vision transformer usually initializes a position embedding $(x_{pos})_{a,b} \in \mathbb{R}^{D}$ and a temporal embedding $(x_{temp})_{t} \in \mathbb{R}^{D}$, for the corresponding video embedding $(\hat{x}_{P})_{t,a,b}$ at the 3D position $(t,a,b)$, to learn the position and temporal bias during training, as shown in Eq.~(\ref{eq:x_p}):
\begin{equation}\label{eq:x_p}
    (x_{P})_{t,a,b} = (\hat{x}_{P})_{t,a,b} + (x_{temp})_{t} + (x_{pos})_{a,b} ,
\end{equation}
where $t \in [1,T_{P}]$, $a \in [1,H_{P}]$, and $b \in [1,W_{P}]$.

In the global-attention based transformers \cite{bertasius2021space,patrick2021keeping,herzig2021object}, in addition to the video embeddings, there is usually a classification token (CT), $x_{cls} \in \mathbb{R}^{D}$, which is initialized to be concatenated with other informative tokens, and participates the self-attention calculation like other tokens in the subsequent transformer blocks. This special token is used as the token for the final classification. Thus, the input of the transformer block is the concatenation of the class token and the informative video embeddings, as shown in Eq.~(\ref{eq:x}),
\begin{equation}\label{eq:x}
\small{
    x = [x_{cls}, (x_{P})_{:}],
    }
\end{equation}
where subscript $:$ is used to denote a full slice of the input tensor in that dimension.

Swin video transformer~\cite{liu2021video}, which is a local-window self-attention based transformer, applies 3D average pooling among the output embeddings from the last transformer block, instead of using a classification token, to produce the feature vector for final classification.

\subsection{Dynamic Class Token Generator} \label{ssec:token}
For EAR, the appearance and dynamic interaction of hands and objects are the key clues to understand human actions~\cite{shan2020understanding}. Hence, we argue that exploring such information and circulating it from the beginning to the end of the model can encourage the transformer to focus more on the action-informative parts of egocentric videos. Considering this, we propose a dynamic class token generator (DCTG), with negligible number of parameters, to guide the transformer by a class token enhanced with hand-object interaction information.

We first apply a pre-trained hand-object detector (HOD)~\cite{shan2020understanding}, which is built based on an object detection system, more specifically Faster-RCNN~\cite{ren2016faster}. We choose HOD as the offline hand-object feature extractor for the following reasons: (i) HOD is specifically trained to identify two objects -- human hands and contacted objects; (ii) the model has been shown to generalize well across datasets especially egocentric datasets; (iii) official datasets like EPIC-KITCHENS-100 \cite{damen2020rescaling} provide automatic hand-object detections based on HOD, indicating the model's reputation and reliability. We send $T$-many frames $I_{t}$ to HOD to get bounding box predictions $BB_{t}$ for hands and objects, as well as the feature maps $I_{t}^{base}$ generated by the base part of HOD. According to the credibility ranking, we choose the top-M hand and top-M object detections with confidence score $\theta > 0.5$, and resend them, along with the feature maps, to `ROIAlign' module and the `top feature refine module' of HOD to obtain the final 2048-D feature vector for each selected detection. Finally, we concatenate them together to get the hand-object (HO) feature $F^{HO} \in \mathbb{R}^{T \times (2M) \times 2048}$ for each video as the input of the DCTG. The procedure is shown in Eq.~(\ref{eq:hod}):
\begin{equation}\label{eq:hod}
\begin{aligned}
&cls_{t},BB_{t},\theta_{t}  = \text{HOD}(I_{t}), \,\,\text{for   } t \in [1,T]\\
&I_{t}^{base} = \text{HOD}_{base}( I_{t}) \in \mathbb{R}^{1024 \times H^{b} \times W^{b}}, \\
    &I_{t}^{align} = \text{ROIAlign}(I_{t}^{base},  BB_{t}) \in \mathbb{R}^{2M \times 1024 \times H^{a} \times W^{a}}, \\
&    I_{t}^{HO} = \text{HOD}_{top}( I_{t}^{align}) \in \mathbb{R}^{2M \times 2048}, \\
 &   F^{HO} = [I_{1}^{HO}, ..., I_{T}^{HO} ]
\end{aligned}
\end{equation}

Then, our goal is to explore the dynamic hand-object interaction, and utilize such knowledge to generate the informative CT for each egocentric video clips. To study the inter-feature relationship and integrate the $2M$ features into one feature vector representing the hand-object information for each frame, we attempt (a) applying query-key-value (QKV) self-attention feature-wise, then averaging along the feature axis; (b) directly averaging the $2M$ features, to get the T-frame hand-object feature $F^{HO'} \in \mathbb{R}^{T \times 2048}$. To explore the inter-frame communication and unearth the hand-object dynamic clues, we experiment with two typical methods for aggregating knowledge in temporal space: (1) QKV-self-attention; and (2) long short term memory (lstm). Then, the specific class token is produced by either calculating the average frame vector from (1) or picking out the last output state vector from (2). While exploring the temporal information from features, we keep the tensor flow in the same dimension space as the video embeddings to produce the class token $x_{cls} \in \mathbb{R}^{D}$. We conduct ablation studies (results are provided in the supplementary materials) on the four combinations of the above mentioned methods, and conclude that directly averaging the $2M$ features for each frame, and then applying two lstm layers---the combination of (b) and (2)---is the best scheme. Therefore, the procedure of dealing with hand-object features in DCTG can be expressed as in Eq.~(\ref{eq:DCTG}).
%\vspace{-0.2cm}

\begin{equation}\label{eq:DCTG}
\small{
\begin{aligned}
    F_{t}^{HO'} = \frac{1}{2M}\sum_{i}^{2M}F_{t,i}^{HO}; \quad & t \in [1,T], i \in [1,2M], \\
    F^{HO''} = \text{LSTM}([F_{1}^{HO'}& ,...,F_{T}^{HO'} ]) \in \mathbb{R}^{T \times D}, \\
    x_{cls} =F_{T}^{HO''} & \in \mathbb{R}^{D}.
\end{aligned}
}
\end{equation}

For the global self-attention based transformer \cite{bertasius2021space,arnab2021vivit,patrick2021keeping}, there is a single CT initialized for the whole input clip at the beginning, so we directly replace it with our dynamic CT. For the local-window self-attention based transformer \cite{liu2021video}---swin video transformer, we generate a CT map. The swin video transformer contains 3D window-based multi-head self-attention module. Assume the size of the 3D local window $WI$ is $\mathbb{R}^{WI_{T} \times WI_{H} \times WI_{W}}$, then for the patched input $X^{P} \in \mathbb{R}^{ (T_{P} \times H_{P} \times W_{P}) \times D} $, there will be $N_{WI} = T_{WI} \times H_{WI} \times W_{WI}$ windows, where $T_{WI} = \frac{T_{P}}{WI_{T}}$, $H_{WI} = \frac{H_{P}}{WI_{H}}$, and $W_{WI} = \frac{W_{P}}{WI_{W}}$. We initially assign the same dynamic CT to all windows, thus forming a class token map $(X^{cls})^{0} \in  \mathbb{R}^{(T_{WI} \times H_{WI} \times W_{WI}) \times D}$. Then through the subsequent layers, each window updates its own CT. The operation in the window-based multi-head self-attention with dynamic CT is expressed in Eq.~(\ref{eq:swin_cls}):
\begin{equation}\label{eq:swin_cls}
\small{
\begin{aligned}
    (I_{WI})_{i} & = [(x_{cls})_{i}, (x_{WI})_{i,:}]; i \in [1,N_{WI}], \\
    (\hat{I}_{WI})_{i}^{l} & = \mathit{\text{W-MSA}(\text{LN}((I_{WI})_{i}^{l-1})) + (I_{WI})_{i}^{l-1} }, \\
    (I_{WI})_{i}^{l} & = \mathit{\text{MLP}(\text{LN}((\hat{I}_{WI})_{i}^{l})) + (\hat{I}_{WI})_{i}^{l} }, \\
    (\hat{I}_{WI})_{i}^{l+1} & = \mathit{\text{SW-MSA}(\text{LN}((I_{WI})_{i}^{l})) + (I_{WI})_{i}^{l} }, \\
    (I_{WI})_{i}^{l+1} & = \mathit{\text{MLP}(\text{LN}((\hat{I}_{WI})_{i}^{l+1})) + (\hat{I}_{WI})_{i}^{l+1} }, \\
\end{aligned}
}
\end{equation}
where $LN$ refers to the LayerNorm operation, $(I_{WI})_{i} \in \mathbb{R}^{(1+WI_{T} \times WI_{H} \times WI_{W})\times D}$ denotes the tensor of the $i^{th}$ window, (S)W-MSA represents the (shifted) window-based self-attention module, $(\hat{I}_{WI})_{i}^{l}$ and $(I_{WI})_{i}^{l}$ represent the $i^{th}$ window output tensor of (S)W-MSA and MLP at the $l^{th}$ transformer block, respectively. Then, to adapt to the hierarchical framework, we merge the class token map at the same time as the patch merging of informative tokens. We concatenate the class tokens within the $2\times2$ spatial neighborhood, which makes the shape of the class token map $\mathbb{R}^{(T_{WI} \times \frac{H_{WI}}{2}  \times  \frac{W_{WI}}{2} ) \times 4D}$. Then, we apply a linear matrix with shape $\mathbb{R}^{ 4D \times 2D}$ to map the $4D$ concatenated tokens to $2D$ space. Therefore, this step reduces the spatial size and increases the its dimension by 2 times for the class token map. We apply this step at the end of stage 1, stage 2, and stage 3, which keeps the same patch merging scheme of the informative tokens in swin. The output class token map of the last stage (stage 4) will be in $\mathbb{R}^{(T_{WI} \times \frac{H_{WI}}{8}  \times  \frac{W_{WI}}{8} ) \times 8D}$. Instead of using the 3D average pooling on the patch tokens map as in the original swin, we apply 3D average pooling among the dynamic class token map to obtain a single $8D$ vector as the final feature for action prediction. 

%Therefore, through our DCTG, instead of traditional static CT without clear information, a revitalized CT infused with the hand-object dynamic information is designed specific to each input clip, which is beneficial to distinguish various egocentric actions.
%Then, with our scheme designed for the dynamic CT, such hand-object interaction information will flow through the entire model by the class token communicating with other tokens. Since the initial egocentric information in the dynamic CT is inferred from the interaction of hands and contacted objects, it also makes the model more inclined to focus on and extract features related to egocentric actions in the subsequent attention mechanism.
Therefore, through our DCTG, the model will be more inclined to focus on and extract features related to egocentric actions in the subsequent attention mechanism. In addition, our DCTG adds only negligible number of parameters to the model. In order to motivate and validate the merit of the dynamic class token, we conduct an evaluation in Sec.~\ref{ssec:Qualitative} comparing the visualizations of dynamic class token and static class token based on the attention weights for different image components. The results show that our dynamic class token is more conducive for the model to focus on the core part of the egocentric actions.

\subsection{Pyramid Architecture with Dynamic Merging} \label{ssec:pyramid}
Our pyramid architecture is composed of two sequential stages, which operate at two different frame rates, and are connected by our proposed dynamic merging mechanism. Since other global attention-based transformers have a similar structure as Swin, we use Swin as our base model to illustrate our framework, as shown in Fig.~\ref{fig:model}, without loss of generality.
%and other global attention-based transformers have the similar process as Swin.

In the first stage, the model focuses on exploring the intra-frame relationship for short-term actions of the subject by separating the video into $G$ phases. In other words, for $T$ input frames, we divide them into $G$ groups with $\frac{T}{G}$ consecutive frames in each group. Thus, each phase has the same frame rate as the raw input clip. We send $\frac{T}{G}$ frames to the DCTG, Patch Embedding, and $L1$ layers of transformer blocks. The temporal window size of each group is set to $\frac{T_{p}}{G}$, and the weights of these modules are shared among the groups. The goal of this stage is to perceive the actions under the almost fixed scene. After the processing, there will be class tokens $x_{cls}^{S1} \in \mathbb{R}^{G \times H_{wi}^{s1} \times W_{wi}^{s1} \times D^{s1} } $ and normal tokens $x_{p}^{s1} \in \mathbb{R}^{G \times \frac{T}{G} \times H^{s1} \times W^{s1} \times D^{s1}} $ from the $G$ phases. We perform average pooling along the temporal axis in each group and then concatenate the pooled tensors to obtain the input normal tokens for the second stage.~The class token can be regarded as a summary of the semantic meaning of each short-term action.~Since the contribution of each phase is not fixed for various videos, we propose a dynamic merging module to assign weights and aggregate the CTs from the first stage. The goal of dynamic merging is to assign a larger weight to the CTs representing the key short-term actions, like ``approach pasta-container" and ``hold pasta-container" in the ``take pasta-container" example. These key actions have similar scene in the background, and we assume that their class tokens have similar directions.
%We first calculate the score $\alpha _{g,s,g'}$ of class token $\mathbf{x}^{cls}_{g,s}$ at different groups $g'$.
%We calculate the dot product between $\mathbf{x}^{cls}_{g,s}$ and the class tokens at $g'$ group and divide it by the product of their l2 norms. 
%Then, we sum the scores along spatial and group axis to get the total score $\alpha _{g,s}$ for  $(\mathbf{x}_{cls})_{g,s}$.
We first obtain the score $\alpha _{g,s,g'}$ of CT $\mathbf{x}^{cls}_{g,s}$ at different groups $g'$ by calculating the dot product between it and the CT at $g'$ group then dividing it by the product of their l2 norms. Then, we sum the scores along spatial and group axis to get the total score $\alpha _{g,s}$ for $(\mathbf{x}_{cls})_{g,s}$. We normalize the scores for all class tokens along group axis by softmax operator, and the final class token map for the second stage is obtained by the weighted sum of class tokens along group axis.
The procedure can be expressed in Eq.~(\ref{eq:dynamic_merging}):
\begin{equation}\label{eq:dynamic_merging}
\small{
\begin{aligned}
    \alpha _{g,s,g'} &= \frac{1}{\left \| \mathbf{x}^{cls}_{g,s} \right \|}\sum_{s'}\frac{\mathbf{x}^{cls}_{g,s} \cdot \mathbf{x}^{cls}_{g',s'}}{\left \| \mathbf{x}^{cls}_{g',s'} \right \|} ,\\
    \alpha _{g,s} &= \sum _{g'\neq g}\alpha _{g,s,g'} ,\\
    \mathbf{x}^{cls}_{s} &= \sum _{g}\mathbf{x}^{cls}_{g,s}\cdot \frac{exp(\alpha _{g,s})}{\Sigma _{\bar{g}}exp(\alpha _{\bar{g},s})} ,\\
\end{aligned}
}
\end{equation}
where $\mathbf{x}^{cls}_{s}$ is the final merged class token map from the first stage. In this way, we dynamically tune the weights for each group.

Therefore, after the merging module between the short-term and long-term stages, the model has an informative token map $x_{info}^{s1} \in \mathbb{R}^{G \times H^{s1} \times W^{s1} \times D^{s1} } $ gathering the information of short-term actions, and the intermediate class token map $x_{cls}^{s1} \in  \mathbb{R}^{1 \times H_{wi}^{s1} \times W_{wi}^{s1} \times D^{s1} } $ inferred from the short-term actions, where $s1$ indicates the first stage. The goal of the second stage, or long-term stage, is to perceive the action in long duration under large-scale scene changes by exploring the inter-relationships of the short-term actions. Therefore, with the combined token maps, we design the subsequent blocks to have a global view on the time axis. At the end of this stage, we have informative token map $x_{info}^{s2} \in \mathbb{R}^{G \times H^{s2} \times W^{s2} \times D^{s2} } $ and the final class token map $x_{cls}^{s2} \in  \mathbb{R}^{1 \times H_{wi}^{s2} \times W_{wi}^{s2} \times D^{s2} } $, where $s2$ indicates the second stage. Then, we apply average pooling on class token map among the spatial axes to get a single $8D^{s2}$ feature vector, and we send it to head for the final classification.

Perceiving the video from local to global view, the processed class token meticulously collects information on short-term actions, gives prominence to critical phases, and explores long-term features. In this way, we have a comprehensive understanding of the video. In summary, our model takes into account three characteristics of egocentric video, namely large-scale scene changes between far frames, high overlap between near frames, and different contributions of phases, so that it can avoid redundant information, reduce the impact of less important frames, focus on more important potions, and also decrease the amount of parameters and computation.
  

