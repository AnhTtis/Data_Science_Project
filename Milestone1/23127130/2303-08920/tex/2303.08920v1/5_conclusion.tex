

We have proposed a new method, referred to as EgoViT, which can be incorporated with different video transformers for egocentric action recognition. We have introduced a Dynamic Class Token Generator (DCTG) that leverages the pre-extracted hand-object interaction features to dynamically generate a class token for each video. We have shown that the DCTG is more effective than the static class token used in previous transformers. We have also presented a pyramid architecture with dynamic merging module, which can properly model the temporal relationship and reduce the redundant information that the traditional video transformer do not filter out. We have demonstrated the effectiveness and efficiency of our EgoViT by comparing it with the most well-known video transformers both quantitatively and qualitatively. Our proposed DCTG can be seen as a fusion scheme for video transformers. Therefore, an interesting extension is to inject DCTG with other kinds of information, such as optical flow and audio features, which will be pursued in our future work. In addition, although our EgoViT can boost the performance of all the video transformers tested, and outperform the current best CNN model, MoViNet, in noun prediction on the EK100 dataset, it cannot yet surpass MoViNet for action and verb class prediction. In our future work, we will improve our model further by investigating transformers and injecting DCTG with other information, as mentioned above. 