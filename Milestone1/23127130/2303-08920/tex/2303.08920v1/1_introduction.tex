\begin{figure}[h!]
  \centering
    \includegraphics[width=1\linewidth]{_imgs_hand_attn.jpg}
    % \vspace{-0.5cm}
  \caption{\textbf{Visualization of attention distribution.} Top to bottom row: RGB frames from original video, the frames with the attention map generated by TimeSformer and TimeSformer equipped with the proposed Dynamic Class Token Generator (DCTG), respectively.}
  \label{fig:DCTG_attention}
   % \vspace{-0.3cm}
\end{figure}

\IEEEPARstart{L}{arge-scale} scene changes and fast camera motion make egocentric action recognition (EAR) a challenging problem in computer vision.~Egocentric view captures objects as well as the interactions between objects and the subject (see Fig.~\ref{fig:DCTG_attention}). Although transformer-based models have been introduced as powerful tools in video understanding area and achieved state-of-the-art (SOTA) performance~\cite{arnab2021vivit, bertasius2021space, liu2021video, patrick2021keeping, herzig2021object, li2021trear}, only a handful of them specifically considers the properties of the first-person view. In our work, we focus on improving the performance of video transformers for EAR. More specifically, we propose a novel framework, referred to as the EgoViT, which carefully takes the special properties of egocentric data into account, and can be integrated with different video transformers.  

For distinct input video clips, previous transformers use the same class token (CT), with other informative tokens, for  self-attention calculation. With fixed query (Q), key (K), and value (V) matrices during the inference time, at the first attention layer, the CT will always produce the same Q, K, and V vectors for any input. Although this kind of static CT can be assigned to various semantic messages, based on the input clip after passing through several model layers, it will cause the same component to be injected into the tokens of different video clips through the self-attention mechanism, which will likely weaken the ability to distinguish different videos. Thus, considering that CT can spread messages into other embeddings throughout the model, we argue that it can be more effective to equip the CT with useful action-related information.

The hand is the key to how humans interact with the world \cite{shan2020understanding}.~The interaction between hand(s) and object(s) is a main component of egocentric actions, as seen in Fig.~\ref{fig:DCTG_attention}, and precisely capturing the motion and appearance information of such interaction is a key factor for EAR. The relatively large background scene in egocentric videos is less important to determine actions, and also makes it harder for the algorithm to focus on the action-informative parts. Hence, directing the model to concentrate more on hand-object parts is especially important for EAR, which is not considered by current transformers.~Considering this, and to make the CT more functional, we propose a dynamic CT mechanism to generate a specific CT, enhanced with hand-object interaction information, for each input based on the content of the video clip, so as to guide the transformer to better recognize the egocentric action.~ORViT~\cite{herzig2021object} uses an object detector pretrained on MS COCO dataset as the off-the-shelf model to provide object information. Yet, the object classes in MS COCO are very different from the objects in egocentric datasets, which may confuse the model during EAR. Moreover, not all objects in the frames contribute to the human action, e.g. some objects are only part of the background.~Also, hand information is not considered by ORViT. We have performed experiments to compare the objects information and hand-object information (that we use), and the results show that hand-object information is more useful for EAR. This experiment is included in the supplementary material. We also compare our approach with ORViT in terms of performance and model size in Sec.~\ref{sec:4_exp}.

The time axis is a special dimension for videos. Most of the video transformers extract temporal semantic information globally from the entire video, ignoring the large scene variation (due to large camera movement) in egocentric videos. Although Swin~\cite{liu2021video} uses two non-overlapping windows along the time axis, there is no mechanism to capture the hierarchical temporal feature of a video, and there is still a lot of repeated scene information within a window. Due to this, most pair-wise calculations between frames generate redundant information. To make it more clear, in Fig.~\ref{fig:t_vec}, we show the feature vectors of each temporal space from the last layers of TimeSformer~\cite{bertasius2021space} and Swin. Both models have the 32-frame input, and at the last layer, the number of features along the time axis are 32 and 16 for TimeSformer and Swin, respectively. We apply Principal Component Analysis (PCA) to project the high-dimensional temporal features to 3D space, and the number marker corresponds to the temporal position of each vector.~It can be seen that, for both models, features from close time instances are also close to each other in the feature space, and sometimes even overlap (marked with red circle). The features form several clusters in the space, indicating the high similarity of the information provided by nearby frames. This further proves that, after the whole model processing, there still exists a lot of redundant information in the neighboring frame vectors. Therefore, we argue that, with large scene changes, egocentric videos can always be decomposed into several phases. For example, the action ``take pasta container" shown in Fig.~\ref{fig:phase_attn}, having a duration of about 1.8s, can be roughly divided into four phases. In the first phase, the person is looking for and approaching the container. The background shows a kitchen counter with the container in view. In the second phase, the person is holding the container, and the background is almost fixed. In the third phase, after getting the container, the person is turning around.~The background changes greatly, while only half of the container is in the view. In the fourth phase, the container is not in the view, and the background becomes another side of the kitchen, which is totally different from the initial one.~Thus, to describe the video more comprehensively and decrease the computation cost, we propose a pyramid architecture.~In this example, the scenes of the first and second phases basically overlap, and the target object remains in the field of view. Starting from the third phase, the scene undergoes a large change, and the target object starts to move out of view and disappears completely in the last phase. Thus, the frames of the fourth phase are less important for action recognition. However, existing transformers still arrange all the frames to do calculations with each frame in the fourth phase. The average pooling approach in the last stage of Swin Video transformer averages the contribution of all frames, and does not emphasize important frames.~Hence, considering the uneven contributions of the phases, we also propose a dynamic merging mechanism to adapt to our pyramid structure and enhance the class token. 

\textbf{Contributions.} The main contributions of this work include the following: (i) We propose a novel video transformer, EgoViT, for EAR. EgoViT takes into account the hand-object information, which is not considered by previous transformers; (ii) We propose a dynamic class token generator (DCTG) producing the CT, carrying EAR-related information based on the content of each input, instead of the static CT used by previous transformers. Dynamic CT can guide the model to focus on informative parts of the video; (iii) Considering the large camera movement in egocentric videos, we construct the transformer by a Pyramid Architecture with a Dynamic Merging (PADM) module, which can properly model the temporal structure and dynamically distribute weights to each temporal component; (iv) We conduct extensive experiments to show that our proposed EgoViT can boost the performance of various transformers for EAR while reducing the computations at the same time. 


