
%%%%%%%%% BODY TEXT
\section{DCTG Attention}
\label{sec:DCTG_attention}
To provide evidence that our proposed dynamic class token generator (DCTG) can truly help the model focus on hand-object interactions when inferring an egocentric action, we provide more examples in Fig.~\ref{fig:DCTG_attention} comparing the spatial attention of models with and without the DCTG module. As can be seen, with our DCTG module, the model tends to focus more on the action-related parts, i.e. the hand-object interactions (marked with green ellipses), and gives less attention to the insignificant parts (marked with red ellipses).

\begin{figure*}
  \centering
    \includegraphics[width=1\linewidth]{_imgs_hand_attn_2.jpg}
    \vspace{-0.5cm}
  \caption{\textbf{Visualization of spatial attention distribution.} Top to bottom row: RGB frames from the original video, the frames with the attention map generated by TimeSformer and TimeSformer equipped with the proposed DCTG, respectively.}
  \label{fig:DCTG_attention2}
\end{figure*}

%-------------------------------------------------------------------------
\section{PADM Attention}
\label{sec:PADM_Attention}
To illustrate that the dynamic merging module can properly tune the attention on class tokens of different phases, we provide more examples in Fig.~\ref{fig:PADM_Attention} showing the calculated score for each phase of various inputs. It can be seen that the mechanism can always generate higher scores for the important phases. For instance, phases 2, 3 and 8 for ``Take eating utensil", phase 2 of ``Turn on faucet" and phases 1,2, 6, 7 and 8 of ``Cut tomato" have high scores, and correspond to the important parts of the video most relevant to the corresponding action.  

\begin{figure*}
    \centering
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Take eating utensil.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__1_.jpg}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Cut tomato.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__2_.jpg}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Turn on faucet.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__3_.jpg}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \begin{minipage}[b]{1.0\linewidth} \centering \textbf{\footnotesize{Read recipe.} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{_imgs_phase_attn__4_.jpg}
    \end{minipage}
    \caption{Visualization of phase attention distribution.}
    \vspace{-0.4cm}
    \label{fig:PADM_Attention2}
\end{figure*}
%------------------------------------------------------------------------
\section{Temporal Feature Distribution}
\label{sec:Temporal_Feature_Distribution}
To show that there is still redundant information along the temporal axis with the previous video transformers, we provide more examples in Fig.~\ref{fig:Temporal_Feature_Distribution}. In this figure, we visualize the distribution of average tokens at each temporal position, in the feature space of the last block, by using the PCA dimension reduction algorithm. As can be seen, there are many overlapping points in the baselines. Most of them occur in consecutive frames, indicating that consecutive frames carry similar semantic meaning, and previous models cannot filter the redundant information out. For our EgoViT, there is less overlap in the feature space, which means that although its temporal dimension is smaller, it can retain complementary features and still capture rich semantic information.

\begin{figure*}
    \centering
    \begin{minipage}[b]{1.0\linewidth}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{Swin} }\end{minipage}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{Swin} }\end{minipage}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{Swin} }\end{minipage}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{Swin} }\end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_swin-_1_.jpg}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_swin-_2_.jpg}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_swin-_3_.jpg}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_swin-_4_.jpg}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{TimeSformer}} \end{minipage}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{TimeSformer}} \end{minipage}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{TimeSformer}} \end{minipage}
    \begin{minipage}[b]{0.24\linewidth} \centering \textbf{\footnotesize{TimeSformer}} \end{minipage}
    \end{minipage}
    \begin{minipage}[b]{1.0\linewidth}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_timesformer-_2_.jpg}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_timesformer-_3_.jpg}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_timesformer-_4_.jpg}
    \includegraphics[width=0.24\linewidth]{_imgs_t_vec_timesformer-_5_.jpg}
    \end{minipage}
    \caption{\textbf{Visualization of temporal vectors.} We use the numbers and letters to indicate the temporal ID of the feature vectors for baselines and our EgoViT, respectively. Letters ``a" to ``h" represent the $1^{st}$ to $8^{th}$ temporal positions, respectively.}
    \label{fig:Temporal_Feature_Distribution2}
    \vspace{-0.4cm}
\end{figure*}

%------------------------------------------------------------------------
\section{DCTG Ablation Study}
\label{sec:DCTG_Ablation_Study}
To investigate how to generate class tokens that can better direct the model, we conduct ablation studies on three aspects: (i) what type of off-the-shelf features can be more useful for EAR, (ii) how to explore inter-feature relationship to produce a single feature vector for each frame, and (iii) how to explore inter-frame relationship to generate a single class token for each input.
For the first aspect, we experiment with 2-class hand-object features extracted from the Hand-object detector \cite{shan2020understanding} pretrained on the 100DOH dataset \cite{shan2020understanding}, and 80-class object features extracted from the Mask-RCNN \cite{he2017mask} pretrained on MS COCO dataset \cite{lin2014microsoft}. 
For the second aspect, we try (a) applying query-key-value (QKV) self-attention feature-wise, then averaging along the feature axis; (b) directly averaging the 2M features.
For the third aspect, we experiment with two typical methods for aggregating knowledge in temporal space: QKV self-attention; and long short term memory (LSTM). 
Then, the specific class token is produced by either calculating the average frame vector from the former one or picking out the last output state vector from the latter one.

The results are shown in Table~\ref{tab:DCTG_Ablation_Study}. As can be seen,  directly averaging the hand-object features for each frame, and then applying LSTM layers provides the best performance.

\begin{table} [hb!]
    \centering
    \resizebox{0.98\linewidth}{!}
    {
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Feature type                 & Inter-feature        & Inter-frame & MV & MN \\
        \hline
        \multirow{4}{*}{Hand-object} & \multirow{2}{*}{QKV} & QKV & 56.24 & 36.62 \\
        \cline{3-5}
                                     &                      & LSTM & 56.64 & 36.88 \\
                                     \cline{2-5}
                                     & \multirow{2}{*}{Avg} & QKV & 57.12 & 38.31 \\
                                     \cline{3-5}
                                     &                      & LSTM & \textbf{60.81} & \textbf{38.45} \\
        \hline
        \multirow{4}{*}{Objects}     & \multirow{2}{*}{QKV} & QKV & 38.58 & 21.53 \\
        \cline{3-5}
                                     &                      & LSTM & 37.71 & 16.42 \\
                                     \cline{2-5}
                                     & \multirow{2}{*}{Avg} & QKV & 37.23 & 21.26 \\
                                     \cline{3-5}
                                     &                      & LSTM & 36.09 & 20.85 \\
        \hline
    \end{tabular}
    }
    \vspace{-0.2cm}
    \caption{\textbf{Ablation study with DCTG on EPIC-KITCHENS-100's many-shot subsets}. MV and MN indicate the prediction accuracy on the many-shot verb subset and many-shot noun subset, respectively.}
    \label{tab:DCTG_Ablation_Study}
    \vspace{-0.2cm}
\end{table}

