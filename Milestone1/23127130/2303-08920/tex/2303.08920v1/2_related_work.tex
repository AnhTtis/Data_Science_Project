\textbf{Transformers in Video Recognition:} The self-attention model proposed by Vaswani et al.~\cite{vaswani2017attention} replaces the CNN or RNN layers with self-attention layers, and was a big success in the natural language processing area. More recently, Dosovitskiy et al.~\cite{dosovitskiy2020image} proposed a pure Vision Transformer (VIT) for the image classification task by taking advantage of a super large 300M JFT dataset~\cite{sun2017revisiting}.~Many other works have focused on building vision transformer models with lower computational cost by using different strategies, such as using semantic visual tokens~\cite{xie2021so}, layer-wise token to token transformation~\cite{yuan2021tokens}, adding distillation losses~\cite{touvron2021training} and building a hierarchical structure with the shifted windows~\cite{liu2021swin}.~Video transformers~\cite{Neimark2021VTN, arnab2021vivit,bertasius2021space, liu2021video, patrick2021keeping, herzig2021object} have mirrored the advances in image understanding and achieved SOTA performance on the major video recognition benchmarks~\cite{kay2017kinetics,goyal2017something}.~Many of previous video transformers~\cite{bertasius2021space, arnab2021vivit, Neimark2021VTN} simply extend the image spatial domain to the global temporal/spatiotemporal domain, which leads to high computation costs, and the performance heavily depends on the 2D spatial model pre-trained on super large datasets JFT-300M~\cite{sun2017revisiting} or ImageNet-21k~\cite{Russakovsky15imagenet}. To reduce the computation and memory costs as well as provide locality inductive bias in the self-attention module, Liu et al.~\cite{liu2021video} strictly followed the hierarchy of the original Swin Transformer~\cite{liu2021swin} for the image domain, and extended the scope of local attention computation from only the spatial domain to the spatiotemporal domain. However, the global temporal self-attention cannot be considered by simply using the shifted windows mechanism. We argue that both local and global temporal attention are critical, especially for egocentric videos, which are usually captured by wearable cameras with large and frequent movements. Our proposed hierarchical pyramid structure successfully provides an inductive bias on grouping the local temporal attentions as well as the high-level global temporal attentions, which can successfully handle the camera motion across different scenes.

\textbf{Object detection-orientated video action recognition:} Object-human/object-hand interaction models~\cite{shan2020understanding, fouhey2018lifestyle, rogez2015understanding, gkioxari2018detecting, wang2021discovering} have been widely explored and achieved significant success. Given that object-human interaction is a key feature for the video action recognition task, many existing models~\cite{kato2018compositional, gao2020drg, xu2019learning, wang2020symbiotic, baradel2018object} employed object detection and interaction features for video understanding. Herzig et al.~\cite{herzig2021object} designed an ``Object-Dynamics Module", which can be inserted into any transformer model, and achieved SOTA performance in video action recognition. This work is the most related one to ours, however, it is pointed out in \cite{herzig2021object} that the improvement on the egocentric videos, such as EPIC-KITCHEN100 dataset, is not as impressive as other datasets because of the frequent and large camera movement. We believe that the major reasons are 1) object-subject interaction features are not being considered in~\cite{herzig2021object}; 2) locality inductive bias is not provided in the self-attention module. To the best of our knowledge, our proposed method is the first attempt to inject the object-human interaction features into the transformer models by designing a dynamic class token, and dynamically embedding the object-human interaction features into the class token.

\textbf{Egocentric video action recognition:} Thanks to the increasing availability of wearable cameras and several egocentric video datasets~\cite{Damen2021PAMI,damen2018scaling,sigurdsson2018charades,li2018eye}, the research in egocentric video analysis has made significant strides.~The general video transformer models may not work for egocentric videos because of the frequent and large camera movements as well as the complicated background scene. Herzi et al.~\cite{herzig2021object} proposed an object centric module that can be plugged into video transformer models. Wang et al.~\cite{wang2020symbiotic} designed a symbiotic attention with object-centric feature alignment framework to provide reasoning between the actor and the objects. Huang et al.~\cite{huang2021towards} provided some effective training strategies for the general transformer models on Epic-Kitchens dataset. Although the aforementioned attempts provided promising results on Epic-Kitchens dataset, they did not, in general, focus on addressing the specific challenges existing in egocentric videos. In this paper, we propose a pyramid video transformer structure, with dynamic class token, which is shown to successfully address both of these challenges with egocentric videos. 