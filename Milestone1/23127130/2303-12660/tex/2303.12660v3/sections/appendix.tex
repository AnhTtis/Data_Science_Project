% \section{Additional Results for \texorpdfstring{$\mathsf{rdag}(K, p)$}{{rdag}(K, p)}}

\section{Analytical Bound on \texorpdfstring{$x$}{x} to ensure \texorpdfstring{$\Pr [F \ge \varepsilon K] = O(1/K)$}{prFgeqK} for \texorpdfstring{$\mathsf{rdag}(K, p)$}{{rdag}(K, p)}} \label{app:analytical_lower_bound}

We can convert the statement of \cref{sec:motivation} to a statement of high probability if we require $\Pr [F \ge \varepsilon K]$ to be $O(1/K)$. Because $g(x, K, p, \varepsilon)$ is an increasing function of $x$, we are asking to find the largest possible $x$ such that $g(x, K, p, \varepsilon) \le \frac {C} {K}$. 
% This inequality cannot be solved analytically, and instead, in \cref{subfig:resilience_random_dag_lb}, we numerically solve the inequality and plot the maximum value of $x$ as a function of $K$, for $p \in \{ 0.2, 0.5 \}, \; n = 1, \; \varepsilon \in \{ 0.5, 1 / \sqrt K \}$. 
In order to get an analytically tractable expression for $x$, we use the fact that $\log t \le t$ for all $t > 0$ and get that \cref{eq:rdag_g_def} becomes 
\begin{align*}
    g(x, K, p, \varepsilon) & \le x^n \left [ 1 - \varepsilon + \frac {1} {K \log \left ( \frac 1 {1 - p} \right )} \left ( \frac {1 - (1 - x^n)(1 - p)^{K}} {1 - (1 - x^n)(1 - p)^{\varepsilon K}} \right ) \right ] \le x^n \left [ 1 - \varepsilon + \frac {1} {K \log \left ( \frac 1 {1 - p} \right )} \left ( \frac {1} {1 - (1 - x^n)(1 - p)^0} \right ) \right ] \\ & = x^n \left ( 1 - \varepsilon \right ) + \frac {1} {K \log \left ( \frac 1 {1 - p} \right )} = \overline g (x, K, p, \varepsilon) 
\end{align*}

If $p$ is constant, then choosing $x = \left ( \frac {1} {K \log \left ( \frac {1} {1 - p} \right ) (1- \varepsilon)} \right )^{1/n},$ makes $\overline g (x, K, p, \varepsilon) \le \frac {2} {\log \left ( \frac {1} {1 - p} \right ) K} = O \left ( \frac 1 K \right )$. 

% In \cref{subfig:resilience_random_dag_lb}, we also plot \cref{eq:random_dag_analytical_resilience_lb}, which is a lower bound to the numerically calculated solution. 

% \begin{figure}[H]
%     \centering
%     % \subfigure[\label{subfig:rdag}]{
%     % \centering
%     % \begin{tikzpicture}[transform shape]
%     %     \Vertex[x=-2, y=0, label=$1$]{u1}
%     %     \Vertex[x=0, y=0, label=$2$,color=pink]{u2}
%     %     \Vertex[x=2, y=0, label=$3$]{u3}
%     %     \Vertex[x=0, y=-2, Pseudo]{s}
%     %     \Edge[Direct, color=black, label=$p$](u1)(u2)
%     %     \Edge[Direct, color=black, label=$1 -p$, style={dashed}](u2)(u3)
%     %     \Edge[Direct, color=black, label=$p$, bend=30](u1)(u3)
        
 
%     % \end{tikzpicture}
    
%     % }
%     \includegraphics[width=0.49\textwidth]{figures/random_dag_resilience_lb.pdf}
%     \caption{Maximum value of $x$ such that $\Pr [F \ge \varepsilon K] = O(1/K)$.}
%     % \label{fig:random_dag}
%     \label{subfig:resilience_random_dag_lb}
% \end{figure}


% \mpcomment{@mar: write something here on how to motivate power laws}
% \mpcomment{put the resiliency of random DAG and explain in plain english}

\section{Omitted Proofs}


\subsection{Proof of \texorpdfstring{\cref{theorem:power_laws}}{theorem:powerlaws}}\label{app:proof:theorem:power_laws}

    Let $\cG \sim \mathsf{rdag}(K, p)$, with nodes $1, 2, \dots, K$ (in this order). Let $P_{k, f}$ be the probability of having $f$ distinct failures in the random DAG with $k$ nodes conditioned on a failure on node 1. We have $P_{1, 1} = 1$ and $P_{k, f} = 0$ for $f > i$ and $f < 1$. To devise a recurrence formula for $P_{i, f}$, note that for the $i$-th node we have the following: 

    \begin{compactenum}
        \item $i$ is affected by the cascade. That happens if at least one connection to $f - 1$ infected nodes upto node $i - 1$, or if $i$ fails due to percolation. This happens with probability $\big \{ [ 1 - (1 - p)^{f - 1} ] + x^n - [ 1 - (1 - p)^{f - 1} ] x^n \big \} P_{k - 1, f - 1} = \big [ 1 - (1 - p)^{f - 1}(1 - x^n) \big ]P_{k - 1, f - 1}$.
   
        \item $i$ is not affected by the cascade. That means that $i$ has $\ge 1$ functional supplier, and no connection exists from the $f$ infected nodes. That happens with probability $(1 - p)^f (1 - x^n) P_{k - 1, f}$.
        
    \end{compactenum}

This produces the following recurrence,
    \begin{align} \label{eq:recurrence}
        P_{k, f} = \left [ 1 - (1 - p)^{f - 1}(1 - x^n) \right ]P_{k - 1, f - 1} +   (1 - p)^f (1 - x^n) P_{k - 1, f}.
    \end{align}

    To determine the distribution of $F$ in $\mathsf{rdag}(K, p)$, we assume that the cascade can start at any node with equal probability $1 / K$ and that the probability of failure for any given node is $x^n$. Also, since a cascade in $\mathsf{rdag}(K, p)$ starting from node 1 is the same as starting from node $i$ in $\mathsf{rdag}(K + i - 1, p)$, the distribution obeys the following,
    \begin{align}
        \Pr [F = f] = \frac {x^n} {K} \sum_{k \in [K]} P_{k, f}
    \end{align}

    We let $Q_{K, f} =  \sum_{k \in [K]} P_{k, f}$, so that $\Pr [F = f] = \frac {x^n} {K} Q_{K, f}$. Summing \cref{eq:recurrence} for $k \in [K]$ and using the definition of $Q_{K, f}$ yields a recurrence relation for $Q_{K, f}$, that is, $Q_{K, f} = (1 - p)^f (1 - x^n) Q_{K - 1, f} + [ 1 - (1 - p)^{f - 1} ] (1 - x^n) Q_{K - 1, f - 1}$. We take the limit for $K$ large, we let $q_f = \lim_{K \to \infty} Q_{K, f}$, and solve the recurrence $q_{f} = (1 - p)^f (1 - x^n) q_{f} + [ 1 - (1 - p)^{f - 1} ] (1 - x^n) q_{f - 1}$ to get $q_f = \frac {1} {1 - (1 - x^n) (1 - p)^f}$. Since $e^x \ge x$, we have $(1 - p)^f \le \log (1 - p) f$ and subsequently $1 - (1 - x^n) (1 - p)^f \le f \left (1 + (1 - x^n) \log \left ( \frac 1 {1 - p} \right ) \right )$. Therefore, for sufficiently large $K$, 
    \begin{align*}
    \small
        \Pr [F = f] & \asymp \frac {x^n q_f} {K} \asymp \frac {x^n} {K(1 - (1 - x^n) (1 - p)^f)} \ge \underbrace {\frac {x^n} {K \left (1 + (1 - x^n) \log \left ( \frac 1 {1 - p} \right ) \right )}}_{C(K, p, x, n) > 0} \frac 1 f.
    \end{align*}

    % The constant in the statement of the Theorem is $C(K, p, x) = \frac {x^n} {K \left ( 1 + (1 - x^n) \log \left ( \frac {1} {1 - p} \right ) \right )} > 0$.
    



\subsection{Proof of \texorpdfstring{\cref{lemma:upper_bound_resilience}}{lemma:upperboundresilience}} \label{app:proof:upper_bound_resilience}

If $S(x)$ is the number of products that survive at a given probability of percolation $x$, and $x_1 \le x_2$ are two percolation probabilities, then a straightforward coupling argument shows that $S(x_1) \ge S(x_2)$, and subsequently, for every $s \in [0, K]$ we have $\Pr_{x = x_1} [S \ge s] \ge \Pr_{x = x_2} [S \ge s]$. Now, in order to arrive at a contradiction, let $\overline R_{\cG}(\varepsilon) \le R_\cG$, and $s = (1 - \varepsilon)K$. Then $1 - 1 / K \le \Pr_{x = R_{\cG}(\varepsilon)} [S \ge (1 - \varepsilon) K] \le \Pr_{x = \overline R_{\cG}(\varepsilon)} [S \ge (1 - \varepsilon) K] \le 1 / 2$ which yields a contradiction. 

% \subsection{Upper Bound on the Resilience of Random DAG} \label{app:upper_bound_resilience_random_dag}

% We have proven that $\Pr[F \ge \varepsilon K] \asymp g(x, K, p, \varepsilon)$. To derive an upper bound to the resilience, note that $(1 - x^n) (1 - p)^K \le  (1 - x^n) (1 - p)^{\varepsilon K}$ for all $\varepsilon \in (0, 1)$, and therefore $\log \left ( \frac {1 - (1 - x^n)(1 - p)^{K}} {1 - (1 - x^n)(1 - p)^{\varepsilon K}} \right ) \ge 0$. This implies that $g(x, K, p, \varepsilon) \ge x^n (1 - \varepsilon)$, and therefore, we can lower bound $\ev {} {F}$

% \begin{align}
%     \ev {} {F} = \int_{t = 0}^{K} \Pr [F \ge t] dt \asymp \int_{t = 0}^{K} g(x, K, p, t/K) dt \gtrsim \int_{t = 0}^K x^n \left ( 1 - \frac {t} {K} \right ) dt = \frac {x^n K} {2}
% \end{align}

% Namely, $\ev {} {S} = O \left ( (1 - x^n) K \right )$, and thus by choosing $x = O \left ((1 - \varepsilon)^{1/n} \right )$ we have that $\Pr [S \ge (1 - \varepsilon) K] \le 1 - o(1)$, establishing an upper bound to the resilience of the $\mathsf{rdag}(K, p)$, due to \cref{lemma:upper_bound_resilience}. 


\subsection{Proof of \texorpdfstring{\cref{theorem:parallel_products}}{theorem:parallelproducts}}\label{app:proof:theorem:parallel_products}

\textbf{Lower Bound.} For $\cC$, let $\varepsilon \in (0, 1)$. If $F_{\cR}$ (resp. $F_{\cC}$) is the number of failed raw materials (resp. complex products), we have that $\{ F_{\cC} \ge \varepsilon K \} \implies \{ F_{\cR} \ge {\varepsilon K} / \mu \} $. Let $\delta = \frac {1} {x^n} \sqrt {\frac {\log K} {2r}}$ and let $\frac {\varepsilon K} {\mu} = (1 + \delta) \ev {} {F_R} = (1 + \delta) r x^n$. We apply the one-sided Chernoff bound and get $\Pr [F_{\cC} \ge \varepsilon K] \le \Pr \left [F_{\cR} \ge \frac {\varepsilon K} {\mu} \right ] = \Pr \left [F_{\cR} \ge (1 + \delta) \ev {} {F_{\cR}} \right ] \le e^{-2 \delta^2 \ev {} {F_{\cR}}^2 / r} = \frac 1 K$. Finally, by resolving the last equation $(1 + \delta)r x^n = \frac {\varepsilon K} {\mu}$, we get that $x = \left ( \frac {\varepsilon K} {r \mu} + \sqrt {\frac {\log K} {2 \mu}} \right )^{1/n}$. Also, we have that $r \le mK$ and therefore $R_{\cC}(\varepsilon) \ge \left ( \frac {\varepsilon} {\mu m} + \sqrt {\frac {\log K} {2mK}} \right )^{1/n}$. If $\varepsilon, m$ and $\mu$ are independent of $K$ then for $K \to \infty$ we have that $R_{\cC}(\varepsilon) \ge \left ( \frac {\varepsilon} {m \mu} \right )^{1/n} > 0$.  

For $\cG$, the analysis is similar to the above. For brevity, we give the analysis in expectation (it is easy to extend it to a high-probability analysis). If in expectation $\ev {} {F_{\cR}} = r x^n$ raw materials fail, that implies that at most $\ev {} {F} = \ev {} {F_{\cR}} + \ev {} {F_\cC} \le r x^n + \mu r x^n = (\mu + 1) r x^n \le m K x^n (\mu + 1)$ total products fail in expectation. We want the fraction of failed products to be at least $\varepsilon (K + r) \ge \varepsilon K / 2$. Therefore, by solving the inequality, we find that the resilience is lower bounded by $\left ( \frac {K} {2m (\mu + 1)} \right )^{1/n}$. The high-probability analysis would be similar to the above case with an extra additive factor of $\sqrt {\frac {\log(K / 2)} {2mK}}$. 

\noindent \textbf{Upper Bound.} For $\cC$, to derive the upper bound, we first bound $\ev {} {S_{\cC}}$. It is easy to see that due to the linearity of expectation $\ev {} {S_{\cC}} = K (1 - x^n)^m$. Thus by Markov's inequality we have that $\Pr [S_{\cC} \ge (1 - \varepsilon) K] \le \frac {\ev {} {S}} {(1 - \varepsilon) K} \le \frac {(1 - x^n)^m} {1 - \varepsilon}$. To make this probability $1 / 2$ it suffices to set $x = \left ( 1 - \left ( \frac {1 - \varepsilon} {2} \right )^{1/m} \right )^{1/n}$, thus from \cref{lemma:upper_bound_resilience} this establishes an upper bound on $R_{\cC}(\varepsilon)$.

   For $\cG$, we proceed similarly by showing that the number of expected products is $\ev {} {S} = r (1 - x^n) + K (1 - x^n)^m \le mK (1 - x^n) + K(1 - x^n)^m \le K (m + 1) (1 - x^n)$. Similarly to the above, from \cref{lemma:upper_bound_resilience} we see that the upper bound on $R_{\cG}$ is $\left ( 1 - \frac {1 - \varepsilon} {2 (m + 1)} \right )^{1/n}$.

\subsection{Proof of \texorpdfstring{\cref{theorem:tree_resilience}}{theorem:treeresilience}}\label{app:proof:theorem:tree_resilience}

    \textbf{Lower Bound.} Depending on the range of $m$ we have two choices

    \begin{compactitem}
        \item  \textbf{Case where $m = 1$.} For every $\tau \in [D]$ we have that $\Pr[S \ge D - \tau] = \Pr \left [ \bigcap_{d > \tau} \{ Z_i = 1 \} \right ] = \Pr [Z_1 = 1] \Pr [Z_2 = 1 | Z_1 = 1] \dots = \prod_{d > \tau} (1 - x^n) = (1 - x^n)^{D - \tau}$. We let $\tau = \varepsilon D$ for some $\varepsilon \in (0, 1)$ and thus $\Pr [S \ge (1 - \varepsilon) D] = (1 - x^n)^{(1 - \varepsilon)D}$. We want to make this probability at least $1 - 1 / D$, and therefore, the resilience of the path graph is $R_{\cG}(\varepsilon) \ge \left ( 1 - \left ( 1 - \frac 1 D \right )^{\frac 1 {(1 - \varepsilon) D)}} \right)^{1/n}$. Since $K = D$ we get the desired result.  

        \item \textbf{Case where $m \ge 2$.} Let $\cK_d$ be the products of tier $d$. We let $\tau = \sup \{ d \in [D] : \exists i \in \cK_d : Z_i = 0 \}$ be the bottom-most tier for which a product failure occurs. If at level $\tau$ a failure occurs, then all levels above $\tau$ are deactivated. The probability that all products up to tier $\tau$ operate is given by 
        \begin{align*}
             \Pr [\text{all products up to tier $\tau$ operate}] & = \Pr \left [ \bigcap_{d > \tau} \bigcap_{i \in \cK_d} \{ Z_i = 1 \} \right ] = \prod_{d = D}^{\tau + 1} \Pr \left [ \bigcap_{i \in \cK_d} \{ Z_i = 1 \} \bigg | \bigcap_{d' > d} \bigcap_{i \in \cK_{d'}} \{ Z_i = 1 \} \right ] \\
            & = \prod_{d = D}^{\tau + 1} (1 - x^n)^{m^d} = \left ( 1 - x^n \right )^{\sum_{d = D}^{\tau + 1} m^d} = \left ( 1 - x^n \right )^{\frac {m^D - m^\tau} {m - 1}}
        \end{align*}
        
        Also, $\{ \text{all products up to tier $\tau$ operate} \} \implies \{ S \ge \frac {m^D - m^\tau} {m - 1} \}$. Therefore, the tail probability of $S$ for $\tau \in [D]$ is given by $\Pr \left [S \ge \frac {m^D - m^\tau} {m - 1} \right ] = \Pr \left [ S \ge \underbrace {\left ( 1 - \frac {m^\tau} {m^D} \right )}_{:= 1 - \varepsilon}  \frac {m^D} {m - 1}  \right ] \ge \left ( 1 - x^n \right )^{(1 - \varepsilon) \frac {m^D} {m - 1}}$. For large enough $D$ we approach the continuous distribution and thus $\Pr [S \ge (1 - \varepsilon) K] \ge (1 - x^n)^{(1 - \varepsilon)K}$. Letting the above be at least $1 - 1 / K$, we get $R_{\cG}(\varepsilon) \ge \left [ 1 - \left ( 1 - \frac 1 {K} \right )^{\frac {1} {(1 - \varepsilon) K}} \right ]^{1/n}$. 
            \end{compactitem}

    \textbf{Upper Bound.} To derive an upper bound, we have the following cases, depending on the value of $m$

    \begin{compactitem}
        \item     \textbf{Case $m = 1$.} We follow the same logic as the $m \ge 2$ case, and upper bound $\ev {} {S} \le \sum_{d \ge 0} (1 - x^n)^d = \frac {1} {x^n}$ which yields an upper bound $R_{\cG}(\varepsilon) < \left ( \frac {2} {D (1 - \varepsilon)}  \right )^{1/n} \to 0$ as $D \to \infty$. 
        \item \textbf{Case $m \ge 2$.} By Markov's Inequality we get that $\Pr [S \ge (1 - \varepsilon) K] \le \frac {\ev {} {S}} {(1 - \varepsilon) K}$. \cref{lemma:upper_bound_tree} (proved in Appendix \ref{app:upper_and_lower_bounds_tree}), implies that $\ev {} {S} \le \frac {K D x^n} {2}$, thus $\Pr [S \ge (1 - \varepsilon) K] \le \frac {K D x^n} {2 (1 - \varepsilon) K}$. To make the RHS equal to $1 / 2$, it suffices to pick $x = \left ( \frac {1 - \varepsilon} {D} \right )^{1/n}$. By \cref{lemma:upper_bound_resilience} we get that $R_{\cG}(\varepsilon) < \left ( \frac {1 - \varepsilon} {D} \right )^{1/n} \to 0$ as $D \to \infty$. 

    \end{compactitem}

\subsection{Proof of \texorpdfstring{\cref{theorem:gw_resilience}}{theorem:gwresilience}}\label{app:proof:theorem:gw_resilience}

In order to prove \cref{theorem:gw_resilience}, we first prove this auxiliary lemma:

\begin{lemma} \label{lemma:gw_roots}
    For $\tau$ finite, $\frac {\one \{ \mu > 1 \}} {\log \mu} < \alpha < \frac 1 2$ , and $0 < \beta < \one \{ \mu < 1 \} + \one \{ \mu > 1 \} \frac {\log \mu - 1} {\mu} $, let 
    \begin{align*}
        \phi(z) & = z \frac {\mu^\tau z^\tau - 1} {\mu z - 1} - \alpha \frac {\mu^\tau - 1} {\mu - 1}, \text{ for } z \neq \frac 1 \mu, \quad
        \psi(z) & = \frac {\mu^\tau - 1} {\mu - 1} - z \frac {\mu^\tau z^\tau - 1} {\mu z - 1} - \beta, \text{ for } z \neq \frac 1 \mu.
    \end{align*}

    Then

   \begin{compactenum}
       \item If $\mu < 1$, then there exist $z_1, z_2 \in (0, 1)$ such that $\phi(z_1) = \psi(z_2) = 0$. 
       \item If $\mu > e^2$, then there exists $z_1 \in (1/\mu, 1)$ such that $\phi(z_1) = 0$.
       \item If $\mu > e$, then there exists $z_2 \in (1/\mu, 1)$ such that $\phi(z_2) = 0$. 
   \end{compactenum} 

\end{lemma} 


\begin{proof} \newline 
    \textbf{Analysis for $\phi(z)$.} We do case analysis: 
    \begin{compactitem}
        \item If $\mu < 1$ then $\phi$ is defined everywhere in $[0, 1]$ and is also continuous. It is also easy to prove that $\phi$ is increasing in $[0, 1]$ since its the product of two non-negative increasing functions, $z$ and  $ \frac {(\mu z)^\tau - 1} {\mu z - 1} =\sum_{i = 0}^{\tau - 1} (\mu z)^i$. Moreover, note that $\phi(0) < 0$ and $\phi(1) > 0$. Therefore, there exists a unique solution $z_1 \in (0, 1)$ such that $\phi(z_1) = 0$. 
        
        \item If $\mu > e^2$, we study $\phi$ in $(1/\mu, 1]$. Again, $\phi$ is increasing (for the same reason as above), continuous in $(1/\mu, 1]$, and has $\phi(1) > 0$. We also have that, by using L'H\^ospital's rule,
        \begin{align*}
            \lim_{z \to 1 / \mu} \frac {\mu^\tau z^\tau - 1 } {\mu z - 1} & = \lim_{z \to 1 / \mu} \frac {(\mu^\tau z^\tau - 1)'} {(\mu z - 1)'} = \lim_{z \to 1 / \mu} \frac {\mu^\tau \tau z^{\tau - 1}} {\mu} = \tau \implies \\
            \lim_{z \to 1 / \mu} \phi(z) & = \frac {\tau} {\mu} - \alpha \frac {\mu^\tau - 1} {\mu - 1} < \frac {\tau (1 - \alpha \log \mu)} {\mu} < 0 \text{ for } \alpha > \frac {1} {\log \mu}.
        \end{align*}

        Therefore, for $\alpha \in (1/\log \mu, 1/2)$, there exists a unique solution $z_1 \in (1/\mu, 1]$ such that $\phi(z_1) = 0$. 
    \end{compactitem}

    % For $\tau \to \infty$, it's easy to see that $z_1 \to 1$. 

    \textbf{Analysis for $\psi(z)$.} Note that $\psi$ is a decreasing function of $z$. We do case analysis:

    \begin{compactitem}
        \item If $\mu < 1$, then $\psi$ is defined everywhere in $[0, 1]$ and is continuous in $[0, 1]$. We have that $\psi(0) > 0$ and $\psi(1) < 0$ therefore there exists a unique solution $z_2$ such that $\psi(z_2) = 0$. 
        \item If $\mu > e$, then $\psi$ is decreasing and contunuous in $(1/\mu, 1]$, with $\psi(1) < 0$. We also have that $\lim_{z \to 1 / \mu} \psi(z)  = \frac {\mu^\tau - 1} {\mu - 1} - \frac {\tau} {\mu} - \beta > \frac {\tau (\log \mu - 1 - \mu \beta)} {\mu} > 0 \text { for } \beta < \frac {\log \mu - 1} {\mu}$.
    \end{compactitem}

    % For $\tau \to \infty$, it's easy to see that $z_1 \to 1$. 

\end{proof}

Subsequently, we prove \cref{theorem:gw_resilience}:

\subsubsection*{Proof of \cref{theorem:gw_resilience}.}

    \noindent \textbf{Upper Bound.} Let $\tau = \inf \{ d \ge 1 : |\cK_d| = 0 \}$ be the extinction time of the GW process. In order to establish an upper bound on the resilience, it suffices to set the expected number of surviving products to be at most $\frac {1 - \varepsilon} {2} \ev {\cG} {K}$, since by Markov's inequality the probability of a fraction of at least $(1 - \varepsilon)$-fraction of products surviving would be at most $1 / 2$ and by \cref{lemma:upper_bound_resilience} we would get an upper bound on the resilience $R_{\cG}(\varepsilon)$; that is, $\Pr_{\cG, x} [S \ge (1 - \varepsilon) \ev {\cG} {K}] \le \frac {\ev {\cG, x} {S}} {(1 - \varepsilon) \ev {\cG} {K}} \le \frac 1 2$. Conditioned on $Z_1 = 1$, which occurs with probability $1 - x^n$, the surviving products grow as a GW process with mean $\mu_x = (1 - x^n) \mu$. Therefore, the condition $\ev {\cG, x} {\cS} = \frac {1 - \varepsilon} {2} \ev {\cG} {K}$, conditioned on the extinction time being $\tau$, is equivalent to
    \begin{align}
        (1 - x^n) \frac {\mu_x^\tau - 1} {\mu_x - 1} & \le \frac {1 - \varepsilon} {2} \frac {\mu^\tau - 1} {\mu - 1} \iff (1 - x^n) \frac {\mu^\tau (1 - x^n)^\tau - 1} {\mu (1 - x^n) - 1}  \le \frac {1 - \varepsilon} {2} \frac {\mu^\tau - 1} {\mu - 1}  \label{eq:upper_bound_gw}
    \end{align}

    We have the following cases.

    \begin{compactenum}    
        \item If $\mu < 1$ then $\Pr [\tau < \infty] = 1$ (i.e., the process goes extinct after a finite number of steps), then the upper bound on the resilience is always finite due to \cref{lemma:gw_roots} which can be found by numerically solving \cref{eq:upper_bound_gw}.
        \item If $\mu (1 - x^n) > 1$, then $\Pr [\tau = \infty] > 0$ and in this case \cref{eq:upper_bound_gw} is only feasible if and only iff $x = 0$, at which case the upper bound on the resilience is 0, and the GW process is not resilient. If $\tau < \infty$, which happens with non-zero probability, the  upper bound on the resilience is finite when $\mu > e^2$ due to \cref{lemma:gw_roots}. 
    \end{compactenum}

    For a specific triplet $(\mu, \tau, \varepsilon)$, let $\overline x(\mu, \tau, \varepsilon)$ be the smallest possible solution to \cref{eq:upper_bound_gw}, which exists for $\mu \in (0, 1) \cup (e^2, \infty)$ due to \cref{lemma:gw_roots}. Then the expected upper bound on resilience $\ev {\cG} {\overline \cR_{\cG} (\varepsilon)}$, can be expressed as $\ev {\cG} {\overline \cR_{\cG} (\varepsilon)} = \ev {\tau} {\overline \cR_{\cG} (\varepsilon)} = \sum_{1 \le k < \infty} \Pr [\tau = k] \overline x(\mu, \tau, \varepsilon) > 0$.

    \noindent \textbf{Lower Bound.} Similarly to the upper bound, in order to devise a lower bound, it suffices to set $\ev {\cG} {K} - \ev {\cG} {S}$ to be at most $\varepsilon$, since, again, by Markov's inequality, we are going to get that the probability that at least a $\varepsilon$-fraction of products fails is at most $\frac {1} {\ev {\cG} {K}}$; namely, $\Pr_{\cG, x} [F \ge \varepsilon \ev {\cG} {K}] \le \frac {\ev {\cG, x} {F}} {\varepsilon \ev {\cG} {K}} \le \frac 1 {\ev {\cG} {K}}$. This yields 
    \begin{align}
        \frac {\mu^\tau - 1} {\mu - 1} - (1 - x^n) \frac {\mu_x^\tau - 1} {\mu_x - 1}  \le \varepsilon  \iff         \frac {\mu^\tau - 1} {\mu - 1} - (1 - x^n) \frac {\mu^\tau (1 - x^n)^\tau - 1} {\mu (1 - x^n) - 1}  \le \varepsilon \label{eq:lower_bound_gw}
    \end{align}

    Similarly to the upper bound, we have the following cases.

    \begin{compactenum}
        \item In the subcritical regime $\mu < 1$, we can again prove that the lower bound is always finite due to \cref{lemma:gw_roots}. 
        \item In the supercritical regime $\mu (1 - x^n) > 1$, we have that when $\tau < \infty$, which happens with positive probability then for $\mu > e$ from \cref{lemma:gw_roots} we get the existence of the resilience. When $\tau = \infty$, we have again that the only way \cref{eq:lower_bound_gw} can hold is iff $x = 0$. 
    \end{compactenum}

     For a specific triplet $(\mu, \tau, \varepsilon)$, let $\underline x(\mu, \tau, \varepsilon)$ be the largest possible solution to \cref{eq:lower_bound_gw}, which exists for $\mu \in (0, 1) \cup (e, \infty)$ due to \cref{lemma:gw_roots}. Then the expected lower bound on the resilience $\ev {\cG} {\underline \cR_{\cG} (\varepsilon)}$, can be expressed as 
 $\ev {\cG} {\underline \cR_{\cG} (\varepsilon)} = \ev {\tau} {\underline \cR_{\cG} (\varepsilon)} = \sum_{1 \le k < \infty} \Pr [\tau = k] \underline x(\mu, \tau, \varepsilon) > 0$.
 
    \noindent \textbf{Determining $\Pr [\tau < \infty]$ when $\mu (1 - x^n) > 1$.} It is known from the analysis of GW processes (see, e.g., \citet{galtonwatson_notes}) that the extinction probability $\Pr [\tau < \infty]$ can be found as the smallest solution $\eta \in [0, 1]$ to the fixed-point equation $\eta = G_{\cD}(\eta)$ where $G_{\cD}(s) = \ev {\xi \sim \cD} {e^{s \xi}}$ is the moment generating function of the branching distribution $\cD$. 

\subsection{Proof of \texorpdfstring{\cref{theorem:resilience_graph_statistics}}{theoremresilienceupperbound}} \label{app:theorem:resilience_graph_statistics}

\paragraph{Upper Bound.} Let $F_{\cR}$ correspond to the number of failures of the raw products. Let $x_{\cR} = \sup \{ x \in (0, 1) : \Pr [F_{\cR} \le (1 - \varepsilon) K] \ge 1 - 1 / K \}$. $x_{\cR}$ is an upper bound to $R_{\cG}(\varepsilon)$ since the event $\{ F \le (1 - \varepsilon) K \}$ implies $\{ F_{\cR} \le (1 - \varepsilon) K \}$. Moreover, by the Chernoff bound, we have that for any $\varepsilon' > 0$:

\begin{align*}
    \Pr \left [ \frac {F_{\cR}} {r} \le (1 + \varepsilon') x^n \right ] \ge 1 - e^{-2 r (\varepsilon')^2}.
\end{align*}

Letting $(1 + \varepsilon') x^n = (1 - \varepsilon) K / r$, $e^{-2 r (\varepsilon')^2} = 1/K$ and solving for $x$ would produce an upper bound to $x_{\cR}$ and subsequently an upper bound to $R_{\cG}(\varepsilon)$. Solving the system yields the following result. 

\begin{align*}
    \varepsilon' = \sqrt {\frac {\log(K)} {2 r}} \qquad \text{and} \qquad x = \left [ \frac {(1 - \varepsilon) K} {\sqrt {2} r^{3/2} + \sqrt {r \log K}} \right ]^{1/n}.
\end{align*}

To determine conditions where the resilience goes to zero as $K \to \infty$ we focus on the denominator of the upper bound: First, the term $\sqrt {r \log K}$ is at most $\sqrt {K \log K} < K$ and cannot grow faster than $K$. Second, the term $\sqrt 2 r^{3/2}$ grows faster than $K$ as long as $r = \omega \left ( K^{3/2} \right )$.  

\paragraph{Lower Bound.} We construct $\cG'$ as follows: We start by $\cG$, and additionally, for each final good $j \in \cC$ in $\cG$ and each raw product $i$ we add an edge from $i$ to $j$ in $\cG'$ if there is a path from $i$ to $j$ in $\cG$. By construction $\cG \subseteq \cG'$ and thus $R_{\cG}(\varepsilon) \ge R_{\cG'} (\varepsilon)$. In $\cG'$ we have that the sourcing dependency $m'$ satisfies $m' \le m + r$ and the supply dependency satisfies $\mu' \le \mu + c$. The result follows by applying \cref{theorem:parallel_products} to $\cG'$.  

\subsection{Proof of \texorpdfstring{\cref{theorem:general_ub}}{theorem:generalub}}\label{app:proof:theorem:general_ub}

    Let $u_i$ be the probability that the product $i$ fails spontaneously (in the simple case, we have $u_i = x^n$ but for more general cases, we can assume that $u_i \in [0, 1]$).

    For $i \in \cK$ let $\beta_i = \Pr [Z_i = 0] \in [0, 1]$. By the union bound, we have that 
	\begin{align*}
	\beta_i & = \Pr [(\exists j \in \cN(i) : (i, j) \text{ is operational } \wedge Z_j = 0) \vee (\forall s \in \cS(i) X_{is} = 0)] \\
	& \le \Pr [\exists j \in \cN(i) : (i, j) \text{ is operational } \wedge Z_j = 0] + \Pr [\forall s \in \cS(i) X_{is} = 0] \\
	& \le y \sum_{j \in N(i)} \beta_j + u_i.
	\end{align*}

	The number of failed products equals $\ev {} {F} = \sum_{i \in \cK} \beta_i$. Thus, finding the upper bound on $\ev {} {F}$ corresponds to solving the following LP,
	\begin{align*}
		p^*_{\cG}(u; y) = \max _{\beta \in [\zero, \one]} \quad & \sum_{i \in \cK} \beta_i \qquad \text{s.t.} \qquad & \beta \le y A^T \beta + u.
	\end{align*}

	When $y \| A^T \|_1 < 1$, which is equivalent to $y < \frac {1} {m}$, this problem is the financial clearing problem of \citet{eisenberg2001systemic}, and from Lemma 4 of \citet{eisenberg2001systemic}, we know that we can also compute $\beta$ by solving the fixed point equation $\beta = \one \wedge (y A^T \beta + u) = \Phi(\beta)$. Since $y < 1/m$, the mapping is a contraction and has a unique fixed-point theorem due to Banach's theorem. 

    To obtain the upper bound, note that if $\beta^*$ is an optimal solution, then the union-bound constraint implies that $(1 - my) \sum_{i = 1}^K \beta_i^* \le Kx^n \le \ev {} {F_y}$. Using the fact that $\frac {1} {1 - my} = 1 + my + O((my)^2)$ we get the right-hand side with $\varrho = my$. 

 
    % \noindent \textbf{Dual LP.} The dual LP is given as 
    
    % \begin{align} \label{eq:dual_upper_bound_lp}
    %     d^*_{\cG}(u; y) = \min_{\gamma, \theta \ge \zero} \quad & u^T \gamma + \one^T \theta \qquad \text{s.t.} \qquad (I - yA)\gamma + \theta \ge \one. 
    % \end{align}

    \subsection{Proof of \texorpdfstring{\cref{theorem:Fy_approximation}}{theoremFyapproximation}} \label{app:proof:theorem:Fy_approximation}
    
    \paragraph{Upper bound.} Given the subsampled graph $\cG_y$, if a product $i$ survives in $\cG$, then it survives in $\cG_y$; and if a product fails in $\cG$, then it survives in $\cG_y$ with probability at least $q = (1 - y)^m (1 - x^n)$, which is the probability that the product does not fail on its own and none of its inputs are selected in $\cG_y$. Taking expectations we get that $\ev {} {F_y} \le (1 - q) \ev {} {F}$. The quantity $q$ is minimized for $y = 1/m$ and satisfies $q \ge (1 - 1/m)^{m} (1 - x^n) \ge 1 / 4 (1 - x^n)$. This implies the $3 / 4 + x^n / 4$ upper bound.   

    \paragraph{Lower Bound.} If $Z_i'$ are the indicator variables for failures in $G_y$ then we have $\Pr [Z_i = 0] = \Pr [Z_i = 0 | Z_i' = 0] \Pr [Z_i' = 0] + \Pr [Z_i = 0 | Z_i' = 1] \Pr [Z_i = 1] \le q' (1 - \Pr [Z_i' = 0]) + \Pr [Z_i' = 0]$ where $q'$ is an upper bound on the probability that a node fails in $\cG$ conditioned on its survival in $\cG_y$. By linearity of expectation, this gives $\ev {} {F_y} \ge \frac {\ev {} {F} - Kq'} {1 - q'}$. The value of $q'$ corresponds to the probability that for each node, at least $f \ge 1$ neighbors have failed but none of them is sampled in $G_y$ and it is given by: 
    \begin{align*}
        q' = 1 - \left ( 1 - x^n (1 - y) \right )^m. 
    \end{align*}

    yielding the final result. 

\subsection{Proof of \texorpdfstring{\cref{theorem:lp_duality_resilience}}{theoremlpdualityresilience}} \label{app:proof:theorem:lp_duality_resilience}

If $p^*$ is the optimal value of the primal given in \cref{eq:upper_bound_lp}, and $(\tilde \gamma, \tilde \theta)$ is a feasible dual solution with the objective value $\tilde d$, then from weak duality we have $\ev {} {F} \le p^* \le \tilde d$. If we let $\tilde d \le \varepsilon$, then $\Pr [F \ge \varepsilon K] \le \frac {\ev {} {F}} {\epsilon K} \le \frac {\tilde d} {\varepsilon K} \le \frac 1  K$. Therefore any $x$ such that $\tilde d \le \varepsilon$ will be a lower bound on $R_{\cG}(\varepsilon)$. This is equivalent to 
    
    \begin{align}
        R_{\cG}(\varepsilon) \ge x \ge \left ( \frac {\varepsilon -  \one^T \tilde \theta} {\one^T \tilde \gamma} \right )^{1/n}, \qquad \text{for all} \qquad \tilde \theta, \tilde \gamma \ge \zero \text { s.t. } (I - yA) \tilde \gamma + \tilde \theta \ge \one. 
    \end{align}

    We are interested in the values of $(\tilde \gamma, \tilde \theta)$ that maximize this lower bound, and therefore the best lower bound is given by 
    
    
    \begin{align}
        \underline R_{\cG}(\varepsilon) = \max_{\gamma, \theta \ge \zero} \left ( \frac {\varepsilon -  \one^T \theta} {\one^T \gamma} \right )^{1/n}, \qquad \text{s.t.} \qquad (I - yA)\gamma + \theta \ge \one.     
    \end{align}

    Observe that increasing any $\theta_i$ from $\theta_i = 0$ would decrease the lower bound; therefore, the optimal $\theta$ is $\theta^* = \zero$. Moreover, due to monotonicity,

    \begin{align}
        \underline R_{\cG}(\varepsilon) = \left ( \frac {\varepsilon} {\min_{\gamma \ge \zero, \gamma \neq \zero} \one^T  \gamma} \right )^{1/n}, \qquad \text{s.t.} \qquad (I - yA)\gamma \ge \one.     
    \end{align}
    
    yielding our final result. We take the dual of the denominator, which corresponds to $\max_{\beta \ge \zero} \one^T \beta$ subject to $\beta \le y A^T \beta + \one$. If $y < 1/m$, from the main result of \cite{eisenberg2001systemic} (or the KKT conditions) we get that the optimal solution is $\beta_{\cG}^\katz (y)$. Similarly for its dual, the optimal solution is $\gamma_{\cG}^\katz (y)$.
    
% \subsection{Proof of \texorpdfstring{\cref{prop:katz}}{prop:katz}}\label{app:proof:prop:katz}

% First of all, it is easy to see that $\Phi(\beta)$ is a contraction under the assumption $y < \frac 1 m$, and, thus, by Banach's fixed point theorem, $\Phi(\beta)$ has a unique fixed point $\beta^*$. Finally, note that, for any $i \in \cK$ we have that $\left ( y A^T \beta^* + x^n \one \right )_i = y \sum_{j \in \cN(i)} \beta_j^* + x^n < y m + (1 - y m) < 1$. Therefore, the fixed point equation simplifies to $\beta^* = y A^T \beta^* + x^n \one$, which has a solution $\beta^* = x^n (I - y A^T )^{-1} \one$, which corresponds to the Katz centrality, since $yA$ is substochastic, and subsequently, $I - yA^T$ is invertible. Finally, from Markov's inequality $\Pr [F \ge \varepsilon K] \le \frac {x^n \one^T (I - yA^T)^{-1} \one} {\varepsilon K} = \frac {x^n \| \mathsf{vec} ((I - yA^T)^{-1}) \|_1} {\varepsilon K}$, where the last equality holds since $(I - yA^T)^{-1}$ has non-negative elements (since $A^T$ is an adjacency matrix and $(I - yA^T)^{-1}$ can be expressed as Neumann series). To make the above $1 / K$ is suffices to pick, $x = \left ( \frac {\varepsilon} {\| \mathsf{vec} ((I - yA^T)^{-1} \|_1} \right )^{1/n}$, hence the lower bound in $R_{\cG}(\varepsilon)$. 

% \mpedit{Alternatively, the proof can be thought of as a direct consequence of \cref{theorem:lp_duality_resilience}.}










 
\subsection{Proof of \texorpdfstring{\cref{prop:intervention}}{prop:intervention}}\label{app:prop:intervention}

Recall $\min_{T \subseteq \cK : |T| = B} p_{\cG}^*(T; y) $, where $p_{\cG}^*(T; y)$ is the solution to LP maximization (\cref{eq:upper_bound_lp}). Since $0 < y < \frac 1 {m}$ and $0 < x < (1 - ym)^{1/n}$, the optimal solution of the internal maximization equals $\hat \beta(t) = x (I - yA^T)^{-1} (\one - t)$. Substituting that in the objective function of \cref{eq:upper_bound_lp}, we get that 
\begin{align*}
    \hat t = \arg \min_{t \in \{ 0, 1 \}^n } \one^T \hat \beta(t) & = \arg \min_{t \in \{ 0, 1 \}^n} \one^T (I - yA^T)^{-1} (\one - t) = \arg \min_{t \in \{0, 1\}^n} (\one - t)^T ((I - yA^T)^{-1})^{T} \one \\
     & = \arg \min_{t \in \{ 0, 1 \}^n} (\one - t)^T (I - yA)^{-1} \one = \arg \min_{t \in \{ 0, 1 \}^n } \gamma_{\mathsf{Katz}}^T(\cG^R,y) (\one - t), 
\end{align*}  where, we remind that $\gamma_{\mathsf{Katz}}(\cG^R,y) = (I - yA)^{-1} \one$ is the vector of Katz centralities for the \emph{reverse graph $\cG^R$}. The third equality is true because $(I - yA^T)^{-T} = \left ( \sum_{k \ge 0} (yA^T)^k \right )^T = \sum_{k \ge 0} (yA)^k \overset {y < \frac 1 {\mu}} {=} (I - yA)^{-1}$. It is easy to observe that, by the rearrangement inequality, the optimal solution would be to intervene in the top-$T$ nodes in terms of Katz centrality in $\cG^R$.


\section{Upper and Lower Bounds on \texorpdfstring{$\ev {} {S}$}{EvS} for the \texorpdfstring{$m$}{m}-ary tree} \label{app:upper_and_lower_bounds_tree}

\begin{lemma} \label{lemma:probability_functional}
    Let $q_d$ be the probability that a product in tier $d$ can be produced. Then 
    \begin{equation}
         q_{d} = \begin{cases} \left ( 1 - x^n \right )^{\frac{m^{D - d + 1} - 1} {m - 1}}, & m \ge 2 \\
         \left ( 1 - x^n \right )^{D - d + 1}, & m = 1
         \end{cases}
    \end{equation}
\end{lemma}

\begin{proof} \newline
Let$ q_d = \Pr [\text{a product in tier $d$ can be produced}] = \Pr [\exists \text{a functional supplier at tier $d$}]$. To calculate $q_d$, note that all the inputs for a product node at tier $d$ succeed with probability $q_{d + 1}^m$, and then the probability that at least one supplier is functionally conditioned on all the inputs working is $1 - x^n$. This yields the following recurrence relation $q_d = q_{d + 1}^m (1 - x^n)$ with $q_{D + 1} = 1$. Solving this recurrence relation, we get $q_{D - d} = \left ( 1 - x^n \right )^{\sum_{l = 0}^d m^l}$ for $d \in [D]$. This yields $q_d = \begin{cases} \left ( 1 - x^n \right )^{\frac {m^{D - d + 1} - 1} {m - 1}}, & m \ge 2 \\
         \left ( 1 - x^n \right )^{D - d + 1}, & m = 1
         
         \end{cases}
    $.
\end{proof}

% \subsection{Proof of \texorpdfstring{\cref{prop:dag_lb_sparse}}{prop:daglbsparse}}\label{app:proof:prop:dag_lb_sparse}

%     Let $\cG$ be a DAG, and let $\beta^*$ be an optimal solution to \cref{theorem:general_ub}, and let the link operation probability $y$ be a value to be determined later. Assume that $v_1, \dots, v_K$ is a topological ordering of the DAG from raw materials to more complex materials. We have that for the $i+1$-th vertex: $\beta_i^* = y \sum_{j \in \cN(v_{i})} \beta_j^* + x^n \le y \sum_{j \le i} \beta_j^*$.

%     It is easy to see (by induction) that the solution to the above recurrence is: $\beta_i^*  \le (1 + y)^{i - 1} x^n$

%     which is tight when all the possible $\binom K 2$ edges in the DAG exist. Therefore, $\ev {} {F} \le \sum_{i \in \cK} \beta_i^* \le x^n \sum_{i = 1}^K (1 + y)^{i - 1} \le \frac {x^n (1 + y)^K} {y} \le \frac {K x^n e^{K y}} {Ky}$. This yields the first part of the proof. For the second part we want to minimize the RHS so that we maximize the lower bound on the resilience. Hence, we choose $z = K y > 0$ so that $\frac {e^z} {z}$ is minimized, which happens at $z = 1$, and equivalently $y = \frac 1 K$, at which case we have that $\ev {} {F} \le K x^n \min_{z > 0} \frac {e^z} {z} = K x^n e$. By Markov's inequality we can get, similarly to \cref{theorem:general_ub}, that $R_{\cG}(\varepsilon) \ge \left ( \frac {\varepsilon} {e K} \right )^{1/n}$.
    

\begin{lemma}[Upper Bound when $m \ge 2$] \label{lemma:upper_bound_tree}

Under the tree structure and $m \ge 2$ the expected size obeys $\ev {} {S} \le \frac {K x^n (D - 1)} {2} = U(x)$. 

\end{lemma}

\begin{proof} \newline 

From \cref{lemma:probability_functional} we have $q_d =\left ( 1 - x^n \right )^{\frac {m^{D - d + 1} - 1} {m - 1}}$. Using the inequality $(1 - t)^a \le \frac 1 {1 + ta}$ for $a > 0$ and $t \in (0, 1)$ we get that 
    \begin{align*}
        \ev {} {S} & = \sum_{d = 1}^D m^{d - 1} q_d \le \sum_{d = 1}^D m^{d - 1} \frac {1} {1 + x^n \frac {m^{D - d + 1}} {2(m - 1)}} \asymp \int_{t = 1}^D \frac {m^{t - 1}} {1 + x^n \frac {m^{D - t + 1}} {2(m - 1)}}.
    \end{align*}
By letting $u = \frac {x^n m^{D - t + 1}} {2 (m - 1)}$ we get that the above integral equals
\begin{align*}
    & \frac {m^D x^n} {2 \log m (m - 1)} \log \left ( \frac {(1 - x^n)(1 - p)^{\varepsilon K} (1 + (1 - x^n)(1 - p)^{K})} {(1 - x^n)(1 - p)^{K} (1 + (1 - x^n)(1 - p)^{\varepsilon K})} \right ) \bigg |_{u_2 = x^n m^D / 2(m-1)}^{u_1 = x^n m / 2(m-1)} \\ & \le \frac {m^D x^n} {2 \log m (m - 1)} \log \left ( m^{D - 1} \right ) \le \frac {K x^n (D - 1)} {2} = U(x).
\end{align*}

\end{proof}

\begin{lemma}[Lower bound when $m \ge 2$] \label{lemma:lower_bound_tree}
    Under the tree structure and $m \ge 2$ the expected cascade size obeys $\ev {} {S} \ge K (1 - x^n(D - 1)) = L(x)$ where $K = m^D - 1$ is the number of products. 
\end{lemma}

\begin{proof}\newline
    From \cref{lemma:probability_functional} we have $q_d =\left ( 1 - x^n \right )^{\frac{m^{D - d + 1} - 1} {m - 1}}$. By Bernoulli's inequality $q_d \ge 1 - x^n \left (\frac {m^{D - d + 1} - 1} {m - 1} \right)$.
    
    Since every level has $m^{d - 1}$ nodes we have that 
    \begin{align*}
        \ev {} {S} & = \sum_{d = 1}^D m^{d - 1} q_d \ge \sum_{d = 1}^D m^{d - 1} \left [ 1 - x^n \left (\frac {m^{D - d + 1} - 1} {m - 1} \right) \right ] = K (1 + x^n) - x^n \frac {1} {m - 1} \sum_{d = 1}^D m^{d - 1} m^{D - d + 1} \\ &  = K (1 + x^n) - x^n \frac {1} {m - 1} \sum_{d = 1}^D m^D = K (1 + x^n) - x^n D (K - 1) \ge K (1 - x^n (D - 1)).
    \end{align*}
 \end{proof}   

% \section{Random Width-\texorpdfstring{$w$}{W} Trellis Architectures} \label{app:random_trellis}

% % \subsection{Random Width-\texorpdfstring{$w$}{W} Trellis} 

% \begin{figure}[H]
%     \centering
%     \begin{tikzpicture}[transform shape, scale=0.7]
%         \Vertex[x=0, y=0]{u1}
%         \Vertex[x=0, y=1,color=pink]{u2}

%         \Vertex[x=1.5, y=0]{v1}
%         \Vertex[x=1.5, y=1, color=pink]{v2}

%         \Vertex[x=3, y=0, color=pink]{w1}
%         \Vertex[x=3, y=1]{w2}

%         \Edge[color=black,Direct](u1)(v1)
%         \Edge[color=black,Direct](u1)(v2)

%         \Edge[color=black,Direct](v1)(w2)
%         \Edge[color=black,Direct](v2)(w1)
        
%     \end{tikzpicture}
%     \caption{Instance of random width-$w$ trellis with $w = 2, D =3, K = 6$. Failures are drawn in pink.}
%     \label{fig:trellis}
% \end{figure}

% We analyze the resilience of the random width-$w$ trellis depicted in \cref{fig:trellis}. The width-$w$ trellis, $\mathsf{rt}(K, p, w)$ is a graph with $K = w D$ nodes, that consists of $D$ tiers of products $\cK_1, \dots, \cK_D$ and each tier has size/width $|\cK_d|= w$ for all $d \in [D]$. The edges are generated randomly and independently with probability $p$ if $i \in \cK_d, j \in \cK_{d + 1}, d \in [D - 1]$ and do not exist otherwise. We prove the following,

% % \mpcomment{revise}
% % \mpcomment{add a reference to this appendix at the end of parallel products}

% \begin{theorem} \label{theorem:trellis_resilience}
%     Let $\cG \sim \mathsf{rt}(K, p, w)$ be a random width-$w$ trellis with $K = wD$ nodes. Then the resiliency of $\cG$ satisfies, 
%     {\scriptsize
%     \begin{align}
%         R_{\cG}(\varepsilon) \ge \begin{cases} \Omega \left ( \left ( \frac {\varepsilon w^2} {K^2} \right )^{1/n} \right ) = \Omega \left ( \left ( \frac {\varepsilon } {D^2} \right )^{1/n} \right ), & \text{for } pw \le 1 \\ \left ( \frac {\varepsilon w (pw - 1)} {K (pw)^{K / w}} \right )^{1/n} = \left ( \frac {\varepsilon  (pw - 1)} {D (pw)^{K / w}} \right )^{1/n}, & \text{for } pw > 1 \end{cases}, \quad
%         R_{\cG} (\varepsilon) \le \begin{cases} 
%         \left ( \frac {(1 + \varepsilon)w (1  -pw)} {2} \right )^{1/n}, & \text{for } pw < 1 \\
%         O \left ( \left ( \frac {w^2} {K}  \right )^{1/n} \right ) = O \left ( \left ( \frac {w} {D}  \right )^{1/n} \right ), & \text{for } pw = 1 \\
%         O \left ( \left ( \frac {w} {K} \right )^{1/n} \right ) = O \left ( \left ( \frac {1} {D} \right )^{1/n} \right ), & \text{for } pw > 1
%         \end{cases}
%     \end{align}
%     }
%     % and 

%     % \begin{align}
%     % R_{\cG} (\varepsilon) \le \begin{cases} 
%     %     \left ( \frac {(1 + \varepsilon)w (1  -pw)} {2} \right )^{1/n}, & \text{for } pw < 1 \\
%     %     O \left ( \left ( \frac {w^2} {K}  \right )^{1/n} \right ) = O \left ( \left ( \frac {w} {D}  \right )^{1/n} \right ), & \text{for } pw = 1 \\
%     %     O \left ( \left ( \frac {w} {K} \right )^{1/n} \right ) = O \left ( \left ( \frac {1} {D} \right )^{1/n} \right ), & \text{for } pw > 1
%     % \end{cases}
% % \end{align}

% Therefore,

% \begin{compactitem}
%     \item If $pw \le 1$, $K, w \to \infty$, and $K / w \to D < \infty$, then $\cG$ is resilient. 
%     \item If $pw \ge 1$, $K, D \to \infty$, and $K / D \to w < \infty$, then $\cG$ is fragile.
% \end{compactitem}
% \end{theorem}

% \begin{proof} \newline
% \textbf{Lower Bound.} To determine a lower bound in $\cR_{\cG}(\varepsilon)$ we let $\gamma_d$ be the probability that a node at tier $d \in [D]$ fails (due to symmetry the probability is the same for each product of a given tier). The expected number of failures is $\ev {} {F} = D \sum_{d \in [D]} \gamma_d$. By applying the union bound -- similarly to \cref{theorem:general_ub} -- we get that $\gamma_{d + 1} \le p w \gamma_d + x^n$. Solving the recurrence, yields  $\gamma_d \le x^n \left ( (pw)^{d - 1} + \frac {1} {1 - pw} \right )$ for $pw \neq 1$, and $\gamma_d \le d x^n$ for $pw = 1$. Summing up everything,
% \begin{align*}
%     \ev {} {F} = \frac {K} {w} \sum_{d \in [D]} \gamma_d \le \begin{cases} \frac {K x^n} {w} \frac {(pw)^{K / w}} {pw - 1}, & \text{for } pw > 1  \\ \frac {K^2 x^n} {w^2} \frac {1} {1 - pw}, & \text{for } pw < 1 \\ \frac {K^2 x^2} {w^2}, & \text{for } pw = 1 \end{cases} \le \begin{cases} \frac {K x^n} {w} \frac {(pw)^{K / w}} {pw - 1}, & \text{for } pw > 1  \\ C \frac {K^2 x^n} {w^2} , & \text{for } pw \le 1 \end{cases}
% \end{align*}

% Therefore, letting $\ev {} {F} = \varepsilon$ yields the following lower bounds for the resilience: $\underline R_{\cG} (\varepsilon) = \begin{cases} \Omega \left ( \left ( \frac {\varepsilon w^2} {K^2} \right )^{1/n} \right ), & \text{for } pw \le 1 \\ \left ( \frac {\varepsilon w (pw - 1)} {K (pw)^{K / w}} \right )^{1/n}, & \text{for } pw > 1 \end{cases}$. If $w$ is constant, as $K \to \infty$ we have that $\lim_{K \to \infty} \underline R_{\cG}(\varepsilon) = 0$ for all $p \in (0, 1]$, and $w \in \mathbb N$. Now, if $w, K \to \infty$ but $K / w \to D$ where $D$ is finite, then $\cG$ is resilient  with resilience $\Omega \left ( \left ( \frac {\varepsilon} {D^2} \right )^{1/n} \right )$ for $pw \le 1$. If $pw > 1$, then it is easy to prove that the lower bound goes to 0. 

% \textbf{Upper Bound.} To construct an upper bound, we need a lower bound on $\ev {} {F}$. Let $f_{d}$ be the expected number of failures conditioned on any node in tier $d$ failing solely (and spontaneously). As base case we have that $f_D = 1$. We have that a cascade starting from any node at tier $d$ causes at least $a_{d, k}$ failures on expectation at an offset $k$ from tier $d$. We have that for all $d$, $a_{d, 0} = 1$, and that $a_{d, k + 1} \ge \min \{ w, pw a_{d, k}  \}$ since the failures are amplified by a factor of $pw$ at each level, up to being $w$, since the width of the trellis is $w$. Subsequently $f_d \ge \sum_{k = 0}^{D - d} a_{d, k}$.

% For $pw = 1$ we have that $a_{d, k + 1} \ge \min \{ w, a_{d, k} \}$, and since $a_{d, 0} = 1$ we have that $a_{d, k} = 1$ for all $k$. 

% For $pw < 1$  we have that at every new offset the number of failures is decreasing by a factor of $pw$. Therefore, we deduce that $a_{d, k + 1} \ge (pw) a_{d, k}$, and since $a_{d, 0} = 1$ we have that $a_{d, k} \ge (pw)^{k}$. 

% For $pw > 1$, for $k \le \frac {w} {\log (pw)}$ we have that at least $(pw)^k \ge k \log (pw)$ nodes are influenced and are at most $w$. For $k > \frac {w} {\log (pw)}$, $w$ nodes are infected at every step on expectation.

% Therefore, we get that $f_d \ge  \sum_{k = 0}^{D - d} a_{d, k} \ge \begin{cases}
%         \frac {1 - (pw)^{D - d + 1}} {1 - pw}, & \text{for } pw < 1 \\
%         D - d + 1, & \text{for } pw = 1 \\
%         \sum_{k = 0}^{w / \log (pw)} 
%         \log (pw) k + w \left ( D - d - \frac {w} {\log (pw)} \right ). & \text{for } pw > 1
%     \end{cases}.$

% Therefore the lower bound on $\ev {} {F}$ is $\ev {} {F} \ge x^n \sum_{d = 1}^D f_d$.  We have the following three cases

% \begin{compactenum}
%     \item If $pw = 1$ then by a change of summation indices $\ev {} {F} \ge x^n \sum_{k = 1}^D k = x^n \frac {D (D + 1)} {2} \ge \frac {x^n D^2} {2} = \frac {x^n K^2} {2w^2}$.

%     \item If $pw < 1$ then, by change of summation indices we have that $\ev {} {F} \ge \frac {x^n} {1 - pw} \sum_{k = 1}^D(D - k + 1) (pw)^{k - 1} \ge \frac {x^n D} {1 - pw}$.

%     \item If $pw > 1$, we have that for $w \ge 2$,
%     \begin{align*}
%         \ev {} {F} & \ge x^n \left \{ \log (pw) \sum_{k = 0}^{w / \log(pw)} k + w \left (D - d - \frac {w} {\log (pw)} \right ) \right \} \ge x^n \sum_{d = 1}^D \left \{ \frac {w^2 / 2 - w} {\log (pw)} + w(D - d) \right \} \\
%         & \ge x^n w \sum_{d = 1}^D (D - d) \ge x^n w \frac {D^2} {2}  = \frac {x^n K^2} {2 w}
%     \end{align*}
    
% \end{compactenum}

% We want to make each of the above lower bounds equal to $\frac {1 + \varepsilon} {2} K$, since that would give an $\frac {1 - \varepsilon}{2} K$ upper bound on $\ev {} {S}$, and due to \cref{lemma:upper_bound_resilience} we get the upper bound \\ $\overline R_{\cG}(\varepsilon)$: $\overline R_G(\varepsilon) = \begin{cases} 
%         \left ( \frac {(1 + \varepsilon)w (1  -pw)} {2} \right )^{1/n}, & \text{for } pw < 1 \\
%         O \left ( \left ( \frac {w^2} {K}  \right )^{1/n} \right ), & \text{for } pw = 1 \\
%         O \left ( \left ( \frac {w} {K} \right )^{1/n} \right ), & \text{for } pw > 1
%     \end{cases}$. Therefore, if $pw \ge 1$, $K, D \to \infty$ and $K / D \to w$ with $w$ finite, then $\cG$ is fragile. 
% \end{proof}


% \section{Supplier Heterogeneity \& Optimal Interventions} \label{app:supplier_heterogeneity}

% Here we give a proof of the derivation of the optimal intervention mechanism through supplier heterogeneity presented in \cref{sec:discussion}. Namely, we increase the number of suppliers of product $i$, from $n$ to $n + \nu_i$ for some $0 \le \nu_i \le \overline \nu_i$, subject to a budget $\sum_{i \in \cK} \nu_i \le N$, to decrease its self-percolation probability from $x^n$ to $x^{n + \nu_i}$. Under the Assumptions of \cref{prop:intervention}, the optimization problem is: 
% \begin{align*}
%     \hat \nu = \arg \min_{\zero \le \nu \le \overline \nu, \one^T \nu \le N} x^n \gamma_{\mathsf{Katz}}^T (\cG^R,y) x^{\odot \nu} = \arg \min_{\zero \le \nu \le \overline \nu, \one^T \nu \le N} \gamma_{\mathsf{Katz}}^T (\cG^R,y) x^{\odot \nu},
% \end{align*} where $x^{\odot \nu}$ is the Hadamard power (elementwise power) of $x$ raised to the vector $\nu$, i.e.  $x^{\odot \nu} = (x^{\nu_1}, \dots, x^{\nu_K})^T$. Since $x^\nu$ is a decreasing function of $\nu$, the optimal policy is to order the nodes in decreasing order $\pi$ in terms of their Katz centrality in $\cG^R$ and then try to put as many suppliers as possible in $\pi(1), \pi(2), \dots, $ while respecting the budget constraint $N$ at the same time. This yields the following optimal policy  $\hat \nu_{\pi(i)} = \left ( \overline \nu_{\pi(i)} \wedge \left ( N - \sum_{j < i} \hat \nu_{\pi(j)} \right ) \right )^+.$ where $(a)^+ = \max \{a, 0 \}$. 

\section{Extended Related Work} \label{app:further_related_work}


\paragraph{Supply Chain Contagion.} There have been multiple works on production networks in macroeconomics and how shocks in production networks propagate through the production network's input-output relations, see a comprehensive survey by \citet{carvalho2019production}. One of the earliest works dates back to \citet{horvath1998cyclicality} introduces a multi-sector model that extends the earlier Long-Plosser model \cite{long1983real} and argues that sector-specific shocks are, in fact, affected by the graph topology between producing sectors. Such arguments contrast the previous arguments of \citet{lucas1995understanding}, which argue that small microeconomic shocks would significantly affect the economy. 
In the 2008 financial crisis, the ex-CEO of Ford, Alan Mulally, argued that the collapse of GM or Chrysler would significantly impact Ford's production capabilities for nontrivial amounts of time. 
The works of \citet{acemoglu2012network, gabaix2011granular} build on the above observation and show that even small shocks lead to cascades that can have devastating effects on the economy. Specifically, \citet{gabaix2011granular} argues that firm-level idiosyncratic shocks can translate into large fluctuations in the production network when the firm sizes are heavy-tailed, and, in the sequel, \citet{acemoglu2012network} replaced \citet{gabaix2011granular}'s analysis on the firm size with the intersectoral network, building on the Long-Plosser model. \citet{hallegatte2008adaptive} introduces a supply chain model in which, when a firm cannot satisfy its orders, it rations its production to the firms that depend on it via the Input-Output matrix. The model of \citet{hallegatte2008adaptive} has been used to study supply chain effects of the COVID-19 pandemic \citep{guan2020global,walmsley2021impacts,inoue2020propagation,pichler2020production}. Finally, recent work by \cite{elliott2022supply} studies the propagation of shocks in a network (see \cref{sec:introduction}).   
Our work is related to the above works and attempts to study the propagation of individual shocks at the supplier level to the (aggregate) production network through the definition of a resilience metric. 

% \citet{perera2017network} systematically reviews the literature on modeling the topology and robustness of production networks with applications on real-world data. The first observation is that many real-world production networks follow a power law degree distribution with an exponent of around $2$. Moreover, the resilience (called ``robustness''  in their work) of such networks is defined as the largest connected component size or the average/max path length under the presence of random failures. On the contrary, we propose a novel theory-informed resilience metric that complements the existing measures in \citet{perera2017network} and the references therein, and supplement their empirical works with novel theories about how topological attributes contribute to increased risk of cascading failures in production networks. We also provide empirical results similar to the ones that \citet{perera2017network} study. 



\paragraph{Seeding Problems in Supply Chains.} In a different flavor from the literature we discussed above, the work of \citet{blaettchen2021traceability} study how to find the least costly set of firms to target as early adopters of traceability technology in a supply chain network, where hyperedges model individual supply chains. 
% Their model is also related to influence models and target set selection as in the works of \citet{granovetter1978threshold,kempe2003maximizing,chen2010scalable,chen2009approximability}. 
The connections to our paper involve the spread of information in a networked environment through interventions. However, in this paper, we do not focus on building algorithms for interventions; instead, we provide metrics to assess the vulnerability of nodes in a supply chain network, which can be informative for interventions. 


\paragraph{Node Percolation.}  \citet{goldschmidt1994reliability,yu2010uniformly} study node percolation processes, wherein graph nodes fail independently with probability $p$. Their goal is to find the graphs -- among graphs with a fixed number of nodes and edges -- such that the probability that the induced subgraph (after percolation) is connected is maximized. 

% Also, the work of \citet{kleinberg2004network} has focused on node and edge percolation in graphs and the study of network failures. \citet{piraveenan2013percolation} define a centrality metric based on graph percolation, which generalizes the betweenness centrality measure.   

\paragraph{Resilience and Risk Contagion in Financial Networks.} In a different context, similar models have been used to study financial networks, and optimal allocations in the presence of shocks \citep{eisenberg2001systemic, glasserman2015likely, papachristou2021allocating,papachristou2022dynamic,papp2021default,friedetzky2023payment,demange2018contagion,ahn2019optimal} and financial network formation and risk, see, e.g., \citep{blume2013network,jalan2022incentive,jackson2019makes,aymanns2015contagious,babus2013formation,erol2019network,erol2022network,amelkin2019yield,talamas2020free,bimpikis2019supply}. Our work is related to the above works and attempts to study the propagation of individual shocks at the supplier level to the (aggregate) production network through the definition of a resilience metric.  \citet{acemoglu2015systemic} study financial networks and state that contagion in financial networks has a phase transition: for small enough shocks, a densely connected financial network is more stable, and, on the contrary, for large enough shocks, a densely connected system is a more fragile financial system. In a similar spirit, \citet{amini2012stress} and \citet{battiston2012liaisons} characterize the size of defaults under financial contagion in random graphs. They show that firms that contribute the most to network instability are highly connected and have highly contagious links. Moreover, it has also been shown -- see, e.g., \citet{siebenbrunner2018clearing,battiston2012debtrank,bartesaghi2020risk}, and the references therein -- that risky nodes in financial networks are connected to centrality measures. 

\paragraph{Cascading Failures and Emergence of Power Laws.} There has been a large body of literature on cascading failures in networks and how the cascade distributions behave as power laws in social networks \citep{leskovec2007patterns, wegrzycki2017cascade}, and power grids, see e.g., \citep{dobson2004branching,nesti2020emergence}). We bring the perspective of supply chains and production networks to this literature and offer new insights on how complexity of products and their interdependence affect production network resilience.

\section{Experiments Addendum: World I-O Tables} \label{app:experiments_addendum}

\begin{table}[H]
    \scriptsize
    \centering
    \begin{tabular}{lllllll}
    \toprule
         Country  & Size ($K$) & Avg. Degree & Density & Min/Max In-degree & Min/Max Out-degree  & AUC  \\
    \midrule 
         USA & 55 & 54.00 & 1.000 & 54 & 54 & 0.052 \\
         Japan & 56 & 45.33 & 0.824 & 0 -- 50 & 0 -- 50  & 0.058 \\
         G. Britain & 56 & 52.05 & 0.946 & 0 -- 54 & 0 -- 54  & 0.052  \\
         China & 56 & 37.89 & 0.688 & 0 -- 46 & 0 -- 46  & 0.078 \\
         Indonesia & 56 & 35.75 & 0.65 & 0 -- 46 & 0 -- 46  & 0.078 \\
         India & 56 & 29.57 & 0.537 & 0 -- 41 & 0 -- 43 & 0.095 \\
    \bottomrule
    \end{tabular}
    \caption{Network Statistics and AUC for the world economies. The edge density is computed as $\frac {|\cE(\cG)|} {K^2 - K}$.}
    \label{tab:statistics-world}
\end{table}
\vspace{-30pt}
\begin{figure}[H]
    \centering
    \subfigure[Estimating $R_{\cG}(\varepsilon)$\label{subfig:world_io_tables_resilience}]{\includegraphics[width=0.3\textwidth]{figures/resilience_monte_carlo_vs_eps_wiot.pdf}}
    \subfigure[Optimal Interventions\label{subfig:world_io_tables_interventions}]{\includegraphics[width=0.3\textwidth]{figures/resilience_lb_vs_key_wiot.pdf}}
    \caption{World Economy Input-Output Networks. We set the number of suppliers for each product to $n = 1$.}
    \label{fig:wolrd-io}
\end{figure}

\section{Generalizing Resilience} \label{app:generalized_resilience}

\subsection{Hardness with deterministic marginals} 

If supplier failures are deterministic, the hardness follows from the following decision problem, which is equivalent to calculating the resilience.

\begin{definition}[\textsc{Resilience-Deterministic}] 
    Given a production network $\cG$, an average number of supplier failures $x$, and a non-negative integer $f$, does there exist a deterministic distribution $\nu$ such that the number of failures is $f$?
\end{definition}

\begin{theorem} \label{theorem:resilience_deterministic_hardness}
    \textsc{Resilience-Deterministic} is NP-hard. 
\end{theorem}

% \begin{proof}

The proof relies on a reduction from the \textsc{3-Set-Cover} problem, where the input consists of $c$ elements $V = \{ v_1, \dots, v_{c} \}$ and $r$ sets $S = \{ s_1, \dots, s_{r} \}$ where each set has cardinality 3, and a number $B$. The question is whether there is a collection of $B$ subsets that cover all the elements. 

To construct the reduction, we consider a bipartite production network with raw products $\cR$, labeled by $\{ s_1, \dots, s_{r} \}$, and final goods $\cC$, labeled by $\{ v_1, \dots, v_{c}\}$, with $K = c + r$ products, where the left partition (raw products $\cR$) corresponds to the sets in \textsc{3-Set-Cover} and the right partition (consumer goods $\cC$) corresponds to the elements in \textsc{3-Set-Cover}. Each product/node has a single supplier ($n_i = 1$). Each raw product $s_i \in \cR$ is used to make three complex products such that for each complex product $v_j \in \cC$ we have $v_j \in s_i$ in the \textsc{3-Set-Cover} instance. We set $f = B + c$ and $x = B / K$. The reduction runs in polynomial time, creating a graph with $O(K)$ nodes and $O(K)$ edges. 

($\implies$) Assume that there is a set cover $\mathcal J \subset S$ of size $B=|\mathcal J|$. Then we choose the raw products $J \subseteq \cR$ corresponding to $\mathcal J$ and the set $x_i = 1$ for the unique supplier of the product $i$ (so that it fails deterministically with probability $1$). $B$ of the raw products and all $c$ consumer goods fail. Therefore, the number of failures is now $B + c$.

($\impliedby$) Take any supplier failure assignment with at least $B + c$ supplier failures. Then, if there exists a scenario with $B+c$ failed products, then it should necessarily include all $c$ consumer goods and $B$ of the raw products, whose corresponding subsets in $S$ constitute a solution to the \textsc{3-Set-Cover} problem.
% \end{proof}

\subsection{Distribution Constraints}

For brevity, we let $N = \sum_{i \in \cK} n_i$. The definition of \cref{eq:resilience_general} requires defining a distribution $\nu$ over the union of the suppliers. We let $\cU = \bigcup_{i \in \cK} \cS(i)$ be the universe of suppliers. We let $\nu: 2^{\cU} \to [0, 1]$ be the distribution, where $\nu(U)$ corresponds to the probability that at subset $U \subseteq \cU$ of the suppliers is active. We require the coupling to be non-negative and normalized: 

\begin{align} \label{eq:coupling_nonnegative_normalized}
    \nu \left ( U \right ) & \ge 0, \qquad \forall U \in 2^{\cU} \\
    \sum_{U \subseteq \cU} \nu(U) & = 1,
\end{align}

and respect the corresponding marginals, i.e.

\begin{align} \label{eq:coupling_marginals}
    \sum_{T \subseteq \cU : s \notin T} \nu(T \cup \{ s \}) \le x_s.
\end{align}

Finally, we impose the budget constraint, i.e. 

\begin{align} \label{eq:budget_constraint}
    0 & \le x_{s} \le 1 \qquad \forall s \in \cU \\
    \sum_{s \in \cU} x_{s} & \le x N.
\end{align}

In matrix notation, if $\bar {x}$ is the vector of marginals,

\begin{align} \label{eq:coupling_constraint}
    \nu & \ge \zero, \bar  x \ge \zero \\
    \bar x & - \one \le \zero, \Phi \nu - \bar x \le \zero, \one^T \bar x  \le x N, \one^T \nu \le 1
\end{align}

where $\Phi$ is a matrix such that $\phi_{T, s} = 1$ if and only if $s \notin T$. The number of variables needed to define $\nu$ is $O \left ( 2^{N} \right )$.

\subsection{Upper bound on \texorpdfstring{$\ev {} {F}$}{EF}}

We extend \cref{eq:upper_bound_lp} to account for $\nu$. Specifically, we are interested in the upper bound on the number of failures for the worst possible joint distribution  $\nu$. The primal problem corresponding to this upper bound is as follows:

\begin{align} \label{eq:correlated_failures_primal}
    p^* = \max_{\beta, q, \nu \ge \zero} \quad & \one^T \beta \\
    \text{s.t.} \quad & \beta \le 1, (I - yA^T) \beta - \Psi \nu \le \zero  \\
    & \text{\cref{eq:coupling_constraint}}
\end{align}

We define $\Psi$ as the $K \times 2^N$ matrix with elements $\psi_{i, T} = 1$ iff $T \subseteq \cU \setminus \cS(i)$ and 0 otherwise. Taking the dual yields, 

\begin{align} \label{eq:correlated_failures_dual}
    d^* = \min_{\gamma, \theta, \zeta, \kappa, \eta, \xi \ge \zero} & \one^T \theta + \one^T \zeta + \eta + \xi x N \\
    \text{s.t.} \quad & (I - yA) \gamma + \theta \ge \one 
    \nonumber \\
    & \zeta - \kappa + \one \xi \ge \zero \nonumber \\
    & - \Psi^T \gamma + \Phi^T \kappa + \one \eta \ge \zero \nonumber.
\end{align}

\subsection{Lower Bound}

Similarly to \cref{theorem:lp_duality_resilience}, if we take a feasible dual solution $(\tilde \gamma, \tilde \theta, \tilde \zeta, \tilde \kappa, \tilde \eta, \tilde \xi)$ to \cref{eq:correlated_failures_dual} with objective value $\tilde d$, we can show from weak duality that it suffices to set $\tilde d \le 1 - \varepsilon$ to get a lower bound on $R_{\cG}(\varepsilon)$. This implies that 

\begin{align}
    R_{\cG}(\varepsilon) \ge \frac {\varepsilon - \one^T \tilde \theta - \one^T \tilde \zeta - \tilde \eta} {\tilde \xi N}
\end{align}

Therefore, a lower bound can be devised by taking the maximum possible value of the RHS, i.e.

\begin{align} \label{eq:lower_bound_resilience_correlated_full}
    \underline R_{\cG}(\varepsilon)  = \max_{\gamma, \theta, \zeta, \kappa \eta, \xi}  \quad & \frac {\varepsilon - \one^T \theta - \one^T \zeta - \eta} {\xi N} \\
    \text{s.t.} \quad & \text{\cref{eq:correlated_failures_dual} constraints} \nonumber
\end{align}

It is easy to show that in optimality $\eta = 0$, $\theta = \zero$ and $\zeta = \zero$, we therefore have the following. 

\begin{align} \label{eq:lower_bound_resilience_correlated_simplified}
    \underline R_{\cG}(\varepsilon)  = \max_{\gamma, \kappa, \xi}  \quad & \frac {\varepsilon} {\xi N} \\
    \text{s.t.} \quad & (I - yA) \gamma \ge \one \nonumber,\; - \kappa + \one \xi \ge \zero, \;- \Psi^T \gamma + \Phi^T \kappa \ge \zero. \nonumber
\end{align}

\subsection{Resulting Lower Bound for the Resilience}

The results yield the following theorem.

\begin{theorem} \label{theorem:general_resilience}
    If resilience is defined as in \cref{eq:resilience_general}, then a lower bound on resilience can be found by solving the following optimization problem with $O(K)$ variables and $O(2^N)$ constraints: 

    \begin{align*}
    \underline R_{\cG}(\varepsilon; y)  = \max_{\gamma, \kappa, \xi \ge \zero}  \quad & \frac {\varepsilon} {\xi N} \\
    \text{s.t.} \quad & (I - yA) \gamma \ge \one \nonumber,\; - \kappa + \one \xi \ge \zero, \;- \Psi^T \gamma + \Phi^T \kappa \ge \zero, \nonumber
    \end{align*} where $\Psi, \Phi$ are matrices given in \cref{app:generalized_resilience}. 

\end{theorem}

\section{Limitations of the LP-based bound} \label{app:innaproximability}

{\begin{theorem} \label{theorem:lp_inapproximability}
    There exist non-negative integers $m \ge 2$ and $h \ge 1$ such that there exists a family of graphs $\{ \cG_{m, h} \}_{m \ge 2, h \ge 1}$ with $K = \Theta (m^h)$ nodes, $n = 1$ suppliers and subsampling probabilities $y_{m} > 1 / m$, such that $p_{m, h}^* - \ev {} {F_{m, h}} \ge K / 8$, where $p_{m, h}^*$ is the value of \cref{eq:upper_bound_lp} for $\cG_{m, h}$ and $\ev {} {F_{m, h}}$ is the expected number of failures in $\cG_{m, h}$.
\end{theorem}

    The tree graphs with height $h$ and fanout $m$ are the family of graphs that achieve this gap. Throughout the analysis, we will set $m' = my_m$ for brevity, as the percolation process on this (random) tree graph is equivalent to percolation in a deterministic tree graph of height $h$ and fanout $m'$. We set $n = 1$, and let $g_{m, h} = p_{m, h}^* - \ev {} {F_{m, h}}$ be the additive gap between the LP approximation $p_{m, h}^*$ and the true value $\ev {} {F_{m, h}}$ for the graph $\cG_{m, h}$. 

    We start the proof with the case of $h = 1$, corresponding to a star graph graph with $m$ raw goods and a final product in the center. To find $p^*_{m, 1}$, note that each of the leaf nodes fails with probability $x$ and for the root we have $\beta_{root}^* \le m' x + x = (m' + 1) x$. If $x \ge 1 / (1 + m')$ we have $\beta_{root}^* = 1$, because the union bound is loose and the box constraint on the range of $\beta$ becomes active. Therefore, $p^*_{m, 1} = m x + 1$. In contrast, the size of the cascade is given by $\ev {} {F_{m, 1}} = mx + x + 1 - (1 - x)^m - x (1 - (1 - x))^m = mx + 1 - (1 - x)^{m + 1}$. The gap $g_{m, 1}$ equals $g_{m, 1} = (1 - x)^{m + 1}$ and for $x = 1 / (1 + m')$ we have $g_{m, 1} \ge \left ( \frac {1} {4} \right )^{(m + 1) / (m' + 1)} \ge \frac 1 4$. Moreover, for any $h \ge 2$ we can inductively show that $g_{m, h} = g_{m, 1} + m g_{m, h - 1}$, and therefore solving the recurrence yields $g_{m, h} \ge m^h / 4 \ge K / 8$ (since $m^h \ge K / 2$) which grows unbounded as $h, m \to \infty$.  
}



