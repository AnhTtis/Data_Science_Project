% \section{Additional Results for \texorpdfstring{$\mathsf{rdag}(K, p)$}{{rdag}(K, p)}}

\section{Analytical Bound on \texorpdfstring{$x$}{x} to ensure \texorpdfstring{$\Pr [F \ge \varepsilon K] = O(1/K)$}{prFgeqK} for \texorpdfstring{$\mathsf{rdag}(K, p)$}{{rdag}(K, p)}} \label{app:analytical_lower_bound}

We can convert the statement of \cref{sec:motivation} to a high-probability statement, if we require $\Pr [F \ge \varepsilon K]$ to be $O(1/K)$. Since $g(x, K, p, \varepsilon)$ is an increasing function of $x$, we are asking to find the largest possible $x$ such that $g(x, K, p, \varepsilon) \le \frac {C} {K}$. 
% This inequality cannot be solved analytically, and instead, in \cref{subfig:resilience_random_dag_lb}, we numerically solve the inequality and plot the maximum value of $x$ as a function of $K$, for $p \in \{ 0.2, 0.5 \}, \; n = 1, \; \varepsilon \in \{ 0.5, 1 / \sqrt K \}$. 
In order to get an analytically tractable expression for $x$, we use the fact that $\log t \le t$ for all $t > 0$ and get that \cref{eq:rdag_g_def} becomes 
\begin{align*}
    g(x, K, p, \varepsilon) & \le x^n \left [ 1 - \varepsilon + \frac {1} {K \log \left ( \frac 1 {1 - p} \right )} \left ( \frac {1 - (1 - x^n)(1 - p)^{K}} {1 - (1 - x^n)(1 - p)^{\varepsilon K}} \right ) \right ] \le x^n \left [ 1 - \varepsilon + \frac {1} {K \log \left ( \frac 1 {1 - p} \right )} \left ( \frac {1} {1 - (1 - x^n)(1 - p)^0} \right ) \right ] \\ & = x^n \left ( 1 - \varepsilon \right ) + \frac {1} {K \log \left ( \frac 1 {1 - p} \right )} = \overline g (x, K, p, \varepsilon) 
\end{align*}

If $p$ is constant, then, choosing $x = \left ( \frac {1} {K \log \left ( \frac {1} {1 - p} \right ) (1- \varepsilon)} \right )^{1/n},$ makes $\overline g (x, K, p, \varepsilon) \le \frac {2} {\log \left ( \frac {1} {1 - p} \right ) K} = O \left ( \frac 1 K \right )$. 

% In \cref{subfig:resilience_random_dag_lb}, we also plot \cref{eq:random_dag_analytical_resilience_lb}, which is a lower bound to the numerically calculated solution. 

% \begin{figure}[H]
%     \centering
%     % \subfigure[\label{subfig:rdag}]{
%     % \centering
%     % \begin{tikzpicture}[transform shape]
%     %     \Vertex[x=-2, y=0, label=$1$]{u1}
%     %     \Vertex[x=0, y=0, label=$2$,color=pink]{u2}
%     %     \Vertex[x=2, y=0, label=$3$]{u3}
%     %     \Vertex[x=0, y=-2, Pseudo]{s}
%     %     \Edge[Direct, color=black, label=$p$](u1)(u2)
%     %     \Edge[Direct, color=black, label=$1 -p$, style={dashed}](u2)(u3)
%     %     \Edge[Direct, color=black, label=$p$, bend=30](u1)(u3)
        
 
%     % \end{tikzpicture}
    
%     % }
%     \includegraphics[width=0.49\textwidth]{figures/random_dag_resilience_lb.pdf}
%     \caption{Maximum value of $x$ such that $\Pr [F \ge \varepsilon K] = O(1/K)$.}
%     % \label{fig:random_dag}
%     \label{subfig:resilience_random_dag_lb}
% \end{figure}


% \mpcomment{@mar: write something here on how to motivate power laws}
% \mpcomment{put the resiliency of random DAG and explain in plain english}

\section{Omitted Proofs}


\subsection{Proof of \texorpdfstring{\cref{theorem:power_laws}}{theorem:powerlaws}}\label{app:proof:theorem:power_laws}

    Let $G \sim \mathsf{rdag}(K, p)$, with nodes $1, 2, \dots, K$ (in this order). Let $P_{k, f}$ be the probability of having $f$ distinct failures in the random DAG with $k$ nodes conditioned on a failure on node 1. We have that $P_{1, 1} = 1$ and $P_{k, f} = 0$ for $f > i$ and $f < 1$. To devise a recurrence formula for $P_{i, f}$ note that for the $i$-th node we have the following: 

    \begin{compactenum}
        \item $i$ is affected by the cascade. That happens if at least one connection to $f - 1$ infected nodes upto node $i - 1$, or if $i$ fails due to percolation. This happens with probability $\big \{ [ 1 - (1 - p)^{f - 1} ] + x^n - [ 1 - (1 - p)^{f - 1} ] x^n \big \} P_{k - 1, f - 1} = \big [ 1 - (1 - p)^{f - 1}(1 - x^n) \big ]P_{k - 1, f - 1}$.
   
        \item $i$ is not affected by the cascade. That means that $i$ has $\ge 1$ functional supplier, and no connection exists from the $f$ infected nodes. That happens with probability $(1 - p)^f (1 - x^n) P_{k - 1, f}$.
        
    \end{compactenum}

This yields the following recurrence,
    \begin{align} \label{eq:recurrence}
        P_{k, f} = \left [ 1 - (1 - p)^{f - 1}(1 - x^n) \right ]P_{k - 1, f - 1} +   (1 - p)^f (1 - x^n) P_{k - 1, f}.
    \end{align}

    To determine the distribution of $F$ in $\mathsf{rdag}(K, p)$, we assume that the cascade can start at any node with equal probability $1 / K$ and that the probability of failure is conditioned on the choice of a node is $x^n$. Also, since a cascade in $\mathsf{rdag}(K, p)$ starting from node 1 is the same as starting from node $i$ in $\mathsf{rdag}(K + i - 1, p)$, the distribution obeys the following,
    \begin{align}
        \Pr [F = f] = \frac {x^n} {K} \sum_{k \in [K]} P_{k, f}
    \end{align}

    We let $Q_{K, f} =  \sum_{k \in [K]} P_{k, f}$, so that $\Pr [F = f] = \frac {x^n} {K} Q_{K, f}$. Summing \cref{eq:recurrence} for $k \in [K]$ and using the definition of $Q_{K, f}$ yields a recurrence relation for $Q_{K, f}$, i.e. $Q_{K, f} = (1 - p)^f (1 - x^n) Q_{K - 1, f} + [ 1 - (1 - p)^{f - 1} ] (1 - x^n) Q_{K - 1, f - 1}$. We take the limit for $K$ large, we let $q_f = \lim_{K \to \infty} Q_{K, f}$, and solve the recurrence $q_{f} = (1 - p)^f (1 - x^n) q_{f} + [ 1 - (1 - p)^{f - 1} ] (1 - x^n) q_{f - 1}$ to get $q_f = \frac {1} {1 - (1 - x^n) (1 - p)^f}$. Since $e^x \ge x$, we have that $(1 - p)^f \le \log (1 - p) f$ and subsequently $1 - (1 - x^n) (1 - p)^f \le f \left (1 + (1 - x^n) \log \left ( \frac 1 {1 - p} \right ) \right )$. Therefore, for sufficiently large $K$, 
    \begin{align*}
    \small
        \Pr [F = f] & \asymp \frac {x^n q_f} {K} \asymp \frac {x^n} {K(1 - (1 - x^n) (1 - p)^f)} \ge \underbrace {\frac {x^n} {K \left (1 + (1 - x^n) \log \left ( \frac 1 {1 - p} \right ) \right )}}_{C(K, p, x, n) > 0} \frac 1 f.
    \end{align*}

    % The constant in the statement of the Theorem is $C(K, p, x) = \frac {x^n} {K \left ( 1 + (1 - x^n) \log \left ( \frac {1} {1 - p} \right ) \right )} > 0$.
    



\subsection{Proof of \texorpdfstring{\cref{lemma:upper_bound_resilience}}{lemma:upperboundresilience}} \label{app:proof:upper_bound_resilience}

If $S(x)$ is the number of surviving products for percolation probability $x$, and $x_1 \le x_2$ are two percolation probabilities, a straightforward coupling argument shows that $S(x_1) \ge S(x_2)$, and subsequently for every $s \in [0, K]$ we have that $\Pr_{x = x_1} [S \ge s] \ge \Pr_{x = x_2} [S \ge s]$. Now, in order to arrive at a contradiction, let $\overline R_{\cG}(\varepsilon) \le R_\cG$, and $s = (1 - \varepsilon)K$. Then $1 - 1 / K \le \Pr_{x = R_{\cG}(\varepsilon)} [S \ge (1 - \varepsilon) K] \le \Pr_{x = \overline R_{\cG}(\varepsilon)} [S \ge (1 - \varepsilon) K] \le 1 / 2$ which yields a contradiction. 

% \subsection{Upper Bound on the Resilience of Random DAG} \label{app:upper_bound_resilience_random_dag}

% We have proven that $\Pr[F \ge \varepsilon K] \asymp g(x, K, p, \varepsilon)$. To derive an upper bound to the resilience, note that $(1 - x^n) (1 - p)^K \le  (1 - x^n) (1 - p)^{\varepsilon K}$ for all $\varepsilon \in (0, 1)$, and therefore $\log \left ( \frac {1 - (1 - x^n)(1 - p)^{K}} {1 - (1 - x^n)(1 - p)^{\varepsilon K}} \right ) \ge 0$. This implies that $g(x, K, p, \varepsilon) \ge x^n (1 - \varepsilon)$, and therefore, we can lower bound $\ev {} {F}$

% \begin{align}
%     \ev {} {F} = \int_{t = 0}^{K} \Pr [F \ge t] dt \asymp \int_{t = 0}^{K} g(x, K, p, t/K) dt \gtrsim \int_{t = 0}^K x^n \left ( 1 - \frac {t} {K} \right ) dt = \frac {x^n K} {2}
% \end{align}

% Namely, $\ev {} {S} = O \left ( (1 - x^n) K \right )$, and thus by choosing $x = O \left ((1 - \varepsilon)^{1/n} \right )$ we have that $\Pr [S \ge (1 - \varepsilon) K] \le 1 - o(1)$, establishing an upper bound to the resilience of the $\mathsf{rdag}(K, p)$, due to \cref{lemma:upper_bound_resilience}. 


\subsection{Proof of \texorpdfstring{\cref{theorem:parallel_products}}{theorem:parallelproducts}}\label{app:proof:theorem:parallel_products}

\textbf{Lower Bound.} For $\cC$, let $\varepsilon \in (0, 1)$. If $F_{\cR}$ (resp. $F_{\cC}$) is the number of failed raw materials (resp. complex products), we have that $\{ F_{\cC} \ge \varepsilon K \} \implies \{ F_{\cR} \ge {\varepsilon K} / d \} $. Let $\delta = \frac {1} {x^n} \sqrt {\frac {\log K} {2R}}$ and let $\frac {\varepsilon K} {d} = (1 + \delta) \ev {} {F_R} = (1 + \delta) \rho x^n$. We apply the one-sided Chernoff bound and get $\Pr [F_{\cC} \ge \varepsilon K] \le \Pr \left [F_{\cR} \ge \frac {\varepsilon K} {d} \right ] = \Pr \left [F_{\cR} \ge (1 + \delta) \ev {} {F_{\cR}} \right ] \le e^{-2 \delta^2 \ev {} {F_{\cR}}^2 / R} = \frac 1 K$. Finally, by resolving the last equation $(1 + \delta)\rho x^n = \frac {\varepsilon K} {d}$, we get that $x = \left ( \frac {\varepsilon K} {\rho d} + \sqrt {\frac {\log K} {2\rho}} \right )^{1/n}$. Also, we have that $\rho \le mK$ and therefore $R_{\cC}(\varepsilon) \ge \left ( \frac {\varepsilon} {dm} + \sqrt {\frac {\log K} {2mK}} \right )^{1/n}$. If $\varepsilon, m$ and $d$ are independent of $K$ then for $K \to \infty$ we have that $R_{\cC}(\varepsilon) \ge \left ( \frac {\varepsilon} {md} \right )^{1/n} > 0$.  

For $\cG$, the analysis is similar to the above. For brevity, we give the analysis in expectation (it is easy to extend it to an analysis in high probability): If on expectation $\ev {} {F_{\cR}} = \rho x^n$ raw materials fail, that implies that at most $\ev {} {F} = \ev {} {F_{\cR}} + \ev {} {F_\cC} \le \rho x^n + d \rho x^n = (d + 1) \rho x^n \le m K x^n (d + 1)$ total products fail in expectation. We want the fraction of failed products to be at least $\varepsilon (K + \rho) \ge \varepsilon K / 2$. Therefore by solving the inequality, we get that the lower bound in the resilience is $\left ( \frac {K} {2m (d + 1)} \right )^{1/n}$. The high-probability analysis would be similar to the above case, yield and extra additive $\sqrt {\frac {\log(K / 2)} {2mK}}$ factor. 

   \noindent \textbf{Upper Bound.} For $\cC$, to derive the upper bound, we first bound $\ev {} {S_{\cC}}$. It is easy to see that due to the linearity of expectation $\ev {} {S_{\cC}} = K (1 - x^n)^m$. Thus by Markov's inequality we have that $\Pr [S_{\cC} \ge (1 - \varepsilon) K] \le \frac {\ev {} {S}} {(1 - \varepsilon) K} \le \frac {(1 - x^n)^m} {1 - \varepsilon}$. To make this probability $1 / 2$ it suffices to set $x = \left ( 1 - \left ( \frac {1 - \varepsilon} {2} \right )^{1/m} \right )^{1/n}$, thus from \cref{lemma:upper_bound_resilience} this establishes an upper bound on $R_{\cC}(\varepsilon)$.

   For $\cG$, we proceed similarly by showing that the number of expected products is $\ev {} {S} = \rho (1 - x^n) + K (1 - x^n)^m \le mK (1 - x^n) + K(1 - x^n)^m \le K (m + 1) (1 - x^n)$. Similarly to the above, from \cref{lemma:upper_bound_resilience} we get that the upper bound on $R_{\cG}$ is $\left ( 1 - \frac {1 - \varepsilon} {2 (m + 1)} \right )^{1/n}$.

\subsection{Proof of \texorpdfstring{\cref{theorem:tree_resilience}}{theorem:treeresilience}}\label{app:proof:theorem:tree_resilience}

    \textbf{Lower Bound.} Depending on the range of $m$ we have two choices

    \begin{compactitem}
        \item  \textbf{Case where $m = 1$.} For every $\tau \in [D]$ we have that $\Pr[S \ge D - \tau] = \Pr \left [ \bigcap_{d > \tau} \{ Z_i = 1 \} \right ] = \Pr [Z_1 = 1] \Pr [Z_2 = 1 | Z_1 = 1] \dots = \prod_{d > \tau} (1 - x^n) = (1 - x^n)^{D - \tau}$. We let $\tau = \varepsilon D$ for some $\varepsilon \in (0, 1)$ and thus $\Pr [S \ge (1 - \varepsilon) D] = (1 - x^n)^{(1 - \varepsilon)D}$. We want to make this probability at least $1 - 1 / D$, and therefore, the resilience of the path graph is $R_{\cG}(\varepsilon) \ge \left ( 1 - \left ( 1 - \frac 1 D \right )^{\frac 1 {(1 - \varepsilon) D)}} \right)^{1/n}$. Since $K = D$ we get the desired result.  

        \item \textbf{Case where $m \ge 2$.} Let $\cK_d$ be the products of tier $d$. We let $\tau = \sup \{ d \in [D] : \exists i \in \cK_d : Z_i = 0 \}$ be the bottom-most tier for which a product failure happens. If at level $\tau$ a failure happens, all levels above $\tau$ are deactivated. The probability that all products up to tier $\tau$ operate is given by 
        \begin{align*}
             \Pr [\text{all products up to tier $\tau$ operate}] & = \Pr \left [ \bigcap_{d > \tau} \bigcap_{i \in \cK_d} \{ Z_i = 1 \} \right ] = \prod_{d = D}^{\tau + 1} \Pr \left [ \bigcap_{i \in \cK_d} \{ Z_i = 1 \} \bigg | \bigcap_{d' > d} \bigcap_{i \in \cK_{d'}} \{ Z_i = 1 \} \right ] \\
            & = \prod_{d = D}^{\tau + 1} (1 - x^n)^{m^d} = \left ( 1 - x^n \right )^{\sum_{d = D}^{\tau + 1} m^d} = \left ( 1 - x^n \right )^{\frac {m^D - m^\tau} {m - 1}}
        \end{align*}
        
        Also, $\{ \text{all products up to tier $\tau$ operate} \} \implies \{ S \ge \frac {m^D - m^\tau} {m - 1} \}$. Therefore, the tail probability of $S$ for $\tau \in [D]$ is given by $\Pr \left [S \ge \frac {m^D - m^\tau} {m - 1} \right ] = \Pr \left [ S \ge \underbrace {\left ( 1 - \frac {m^\tau} {m^D} \right )}_{:= 1 - \varepsilon}  \frac {m^D} {m - 1}  \right ] \ge \left ( 1 - x^n \right )^{(1 - \varepsilon) \frac {m^D} {m - 1}}$. For large enough $D$ we approach the continuous distribution and thus $\Pr [S \ge (1 - \varepsilon) K] \ge (1 - x^n)^{(1 - \varepsilon)K}$. Letting the above be at least $1 - 1 / K$ we get that $R_{\cG}(\varepsilon) \ge \left [ 1 - \left ( 1 - \frac 1 {K} \right )^{\frac {1} {(1 - \varepsilon) K}} \right ]^{1/n}$. 
            \end{compactitem}

    \textbf{Upper Bound.} To derive an upper bound, we have the following cases, depending on the value of $m$

    \begin{compactitem}
        \item     \textbf{Case $m = 1$.} We follow the same logic as the $m \ge 2$ case, and upper bound $\ev {} {S} \le \sum_{d \ge 0} (1 - x^n)^d = \frac {1} {x^n}$ which yields an upper bound $R_{\cG}(\varepsilon) < \left ( \frac {2} {D (1 - \varepsilon)}  \right )^{1/n} \to 0$ as $D \to \infty$. 
        \item \textbf{Case $m \ge 2$.} By Markov's Inequality we get that $\Pr [S \ge (1 - \varepsilon) K] \le \frac {\ev {} {S}} {(1 - \varepsilon) K}$. \cref{lemma:upper_bound_tree} (proved in Appendix \ref{app:upper_and_lower_bounds_tree}), implies that $\ev {} {S} \le \frac {K D x^n} {2}$, thus $\Pr [S \ge (1 - \varepsilon) K] \le \frac {K D x^n} {2 (1 - \varepsilon) K}$. To make the RHS equal to $1 / 2$, it suffices to pick $x = \left ( \frac {1 - \varepsilon} {D} \right )^{1/n}$. By \cref{lemma:upper_bound_resilience} we get that $R_{\cG}(\varepsilon) < \left ( \frac {1 - \varepsilon} {D} \right )^{1/n} \to 0$ as $D \to \infty$. 

    \end{compactitem}

\subsection{Proof of \texorpdfstring{\cref{theorem:gw_resilience}}{theorem:gwresilience}}\label{app:proof:theorem:gw_resilience}

In order to prove \cref{theorem:gw_resilience}, we first prove this auxiliary lemma:

\begin{lemma} \label{lemma:gw_roots}
    For $\tau$ finite, $\frac {\one \{ \mu > 1 \}} {\log \mu} < \alpha < \frac 1 2$ , and $0 < \beta < \one \{ \mu < 1 \} + \one \{ \mu > 1 \} \frac {\log \mu - 1} {\mu} $, let 
    \begin{align*}
        \phi(z) & = z \frac {\mu^\tau z^\tau - 1} {\mu z - 1} - \alpha \frac {\mu^\tau - 1} {\mu - 1}, \text{ for } z \neq \frac 1 \mu, \quad
        \psi(z) & = \frac {\mu^\tau - 1} {\mu - 1} - z \frac {\mu^\tau z^\tau - 1} {\mu z - 1} - \beta, \text{ for } z \neq \frac 1 \mu.
    \end{align*}

    Then

   \begin{compactenum}
       \item If $\mu < 1$, then there exist $z_1, z_2 \in (0, 1)$ such that $\phi(z_1) = \psi(z_2) = 0$. 
       \item If $\mu > e^2$, then there exists $z_1 \in (1/\mu, 1)$ such that $\phi(z_1) = 0$.
       \item If $\mu > e$, then there exists $z_2 \in (1/\mu, 1)$ such that $\phi(z_2) = 0$. 
   \end{compactenum} 

\end{lemma} 


\begin{proof} \newline 
    \textbf{Analysis for $\phi(z)$.} We do case analysis: 
    \begin{compactitem}
        \item If $\mu < 1$ then $\phi$ is defined everywhere in $[0, 1]$ and is also continuous. It is also easy to prove that $\phi$ is increasing in $[0, 1]$ since its the product of two non-negative increasing functions, $z$ and  $ \frac {(\mu z)^\tau - 1} {\mu z - 1} =\sum_{i = 0}^{\tau - 1} (\mu z)^i$. Moreover, note that $\phi(0) < 0$ and $\phi(1) > 0$. Therefore, there exists a unique solution $z_1 \in (0, 1)$ such that $\phi(z_1) = 0$. 
        
        \item If $\mu > e^2$, we study $\phi$ in $(1/\mu, 1]$. Again, $\phi$ is increasing (for the same reason as above), continuous in $(1/\mu, 1]$, and has $\phi(1) > 0$. We also have that, by using L'H\^ospital's rule,
        \begin{align*}
            \lim_{z \to 1 / \mu} \frac {\mu^\tau z^\tau - 1 } {\mu z - 1} & = \lim_{z \to 1 / \mu} \frac {(\mu^\tau z^\tau - 1)'} {(\mu z - 1)'} = \lim_{z \to 1 / \mu} \frac {\mu^\tau \tau z^{\tau - 1}} {\mu} = \tau \implies \\
            \lim_{z \to 1 / \mu} \phi(z) & = \frac {\tau} {\mu} - \alpha \frac {\mu^\tau - 1} {\mu - 1} < \frac {\tau (1 - \alpha \log \mu)} {\mu} < 0 \text{ for } \alpha > \frac {1} {\log \mu}.
        \end{align*}

        Therefore, for $\alpha \in (1/\log \mu, 1/2)$, there exists a unique solution $z_1 \in (1/\mu, 1]$ such that $\phi(z_1) = 0$. 
    \end{compactitem}

    % For $\tau \to \infty$, it's easy to see that $z_1 \to 1$. 

    \textbf{Analysis for $\psi(z)$.} Note that $\psi$ is a decreasing function of $z$. We do case analysis:

    \begin{compactitem}
        \item If $\mu < 1$, then $\psi$ is defined everywhere in $[0, 1]$ and is continuous in $[0, 1]$. We have that $\psi(0) > 0$ and $\psi(1) < 0$ therefore there exists a unique solution $z_2$ such that $\psi(z_2) = 0$. 
        \item If $\mu > e$, then $\psi$ is decreasing and contunuous in $(1/\mu, 1]$, with $\psi(1) < 0$. We also have that $\lim_{z \to 1 / \mu} \psi(z)  = \frac {\mu^\tau - 1} {\mu - 1} - \frac {\tau} {\mu} - \beta > \frac {\tau (\log \mu - 1 - \mu \beta)} {\mu} > 0 \text { for } \beta < \frac {\log \mu - 1} {\mu}$.
    \end{compactitem}

    % For $\tau \to \infty$, it's easy to see that $z_1 \to 1$. 

\end{proof}

Subsequently we prove \cref{theorem:gw_resilience}:

\subsubsection*{Proof of \cref{theorem:gw_resilience}.}

    \noindent \textbf{Upper Bound.} Let $\tau = \inf \{ d \ge 1 : |\cK_d| = 0 \}$ be the extinction time of the GW process. In order to establish an upper bound on the resilience it suffices to set the expected number of surviving products to be at most $\frac {1 - \varepsilon} {2} \ev {\cG} {K}$, since by Markov's inequality the probability of a fraction of at least $(1 - \varepsilon)$-fraction of products surviving would be at most $1 / 2$ and by \cref{lemma:upper_bound_resilience} we would get an upper bound on the resilience $R_{\cG}(\varepsilon)$; namely, $\Pr_{\cG, x} [S \ge (1 - \varepsilon) \ev {\cG} {K}] \le \frac {\ev {\cG, x} {S}} {(1 - \varepsilon) \ev {\cG} {K}} \le \frac 1 2$. Conditioned on $Z_1 = 1$, which happens with probability $1 - x^n$, the surviving products grow as a GW process with mean $\mu_x = (1 - x^n) \mu$. Therefore, the condition $\ev {\cG, x} {\cS} = \frac {1 - \varepsilon} {2} \ev {\cG} {K}$, conditioned on the extinction time being $\tau$, is equivalent to
    \begin{align}
        (1 - x^n) \frac {\mu_x^\tau - 1} {\mu_x - 1} & \le \frac {1 - \varepsilon} {2} \frac {\mu^\tau - 1} {\mu - 1} \iff (1 - x^n) \frac {\mu^\tau (1 - x^n)^\tau - 1} {\mu (1 - x^n) - 1}  \le \frac {1 - \varepsilon} {2} \frac {\mu^\tau - 1} {\mu - 1}  \label{eq:upper_bound_gw}
    \end{align}

    We have the following cases

    \begin{compactenum}    
        \item If $\mu < 1$ then $\Pr [\tau < \infty] = 1$ (i.e., the process goes extinct after a finite number of steps), then the upper bound on the resilience is always finite due to \cref{lemma:gw_roots} which can be found by numerically solving \cref{eq:upper_bound_gw}.
        \item If $\mu (1 - x^n) > 1$, then $\Pr [\tau = \infty] > 0$ and in this case \cref{eq:upper_bound_gw} is only feasible if and only iff $x = 0$, at which case the upper bound on the resilience is 0, and the GW process is not resilient. If $\tau < \infty$, which happens with non-zero probability, the  upper bound on the resilience is finite when $\mu > e^2$ due to \cref{lemma:gw_roots}. 
    \end{compactenum}

    For a specific triplet $(\mu, \tau, \varepsilon)$, let $\overline x(\mu, \tau, \varepsilon)$ be the smallest possible solution to \cref{eq:upper_bound_gw}, which exists for $\mu \in (0, 1) \cup (e^2, \infty)$ due to \cref{lemma:gw_roots}. Then the expected upper bound on the resilience $\ev {\cG} {\overline \cR_{\cG} (\varepsilon)}$, can be expressed as $\ev {\cG} {\overline \cR_{\cG} (\varepsilon)} = \ev {\tau} {\overline \cR_{\cG} (\varepsilon)} = \sum_{1 \le k < \infty} \Pr [\tau = k] \overline x(\mu, \tau, \varepsilon) > 0$.

    \noindent \textbf{Lower Bound.} Similarly to the upper bound, in order to devise a lower bound, it suffices to set $\ev {\cG} {K} - \ev {\cG} {S}$ to be at most $\varepsilon$, since, again, by Markov's inequality, we are going to get that the probability that at least a $\varepsilon$-fraction of products fails is at most $\frac {1} {\ev {\cG} {K}}$; namely, $\Pr_{\cG, x} [F \ge \varepsilon \ev {\cG} {K}] \le \frac {\ev {\cG, x} {F}} {\varepsilon \ev {\cG} {K}} \le \frac 1 {\ev {\cG} {K}}$. This yields 
    \begin{align}
        \frac {\mu^\tau - 1} {\mu - 1} - (1 - x^n) \frac {\mu_x^\tau - 1} {\mu_x - 1}  \le \varepsilon  \iff         \frac {\mu^\tau - 1} {\mu - 1} - (1 - x^n) \frac {\mu^\tau (1 - x^n)^\tau - 1} {\mu (1 - x^n) - 1}  \le \varepsilon \label{eq:lower_bound_gw}
    \end{align}

    Similarly to the upper bound, we have the following cases,

    \begin{compactenum}
        \item In the subcritical regime $\mu < 1$, we can again prove that the lower bound is always finite due to \cref{lemma:gw_roots}. 
        \item In the supercritical regime $\mu (1 - x^n) > 1$, we have that when $\tau < \infty$, which happens with positive probability then for $\mu > e$ from \cref{lemma:gw_roots} we get the existence of the resilience. When $\tau = \infty$, we have again that the only way \cref{eq:lower_bound_gw} can hold is iff $x = 0$. 
    \end{compactenum}

     For a specific triplet $(\mu, \tau, \varepsilon)$, let $\underline x(\mu, \tau, \varepsilon)$ be the largest possible solution to \cref{eq:lower_bound_gw}, which exists for $\mu \in (0, 1) \cup (e, \infty)$ due to \cref{lemma:gw_roots}. Then the expected lower bound on the resilience $\ev {\cG} {\underline \cR_{\cG} (\varepsilon)}$, can be expressed as 
 $\ev {\cG} {\underline \cR_{\cG} (\varepsilon)} = \ev {\tau} {\underline \cR_{\cG} (\varepsilon)} = \sum_{1 \le k < \infty} \Pr [\tau = k] \underline x(\mu, \tau, \varepsilon) > 0$.
 
    \noindent \textbf{Determining $\Pr [\tau < \infty]$ when $\mu (1 - x^n) > 1$.} It is known from the analysis of GW processes (see, e.g., \citet{galtonwatson_notes}) that the extinction probability $\Pr [\tau < \infty]$ can be found as the smallest solution $\eta \in [0, 1]$ to the fixed-point equation $\eta = G_{\cD}(\eta)$ where $G_{\cD}(s) = \ev {\xi \sim \cD} {e^{s \xi}}$ is the moment generating function of the branching distribution $\cD$. 

\subsection{Proof of \texorpdfstring{\cref{theorem:general_ub}}{theorem:generalub}}\label{app:proof:theorem:general_ub}

For $i \in \cK$ let $\beta_i = \Pr [Z_i = 0] \in [0, 1]$. By the union bound, we have that 
	\begin{align*}
	\beta_i & = \Pr [(\exists j \in \cN(i) : (i, j) \text{ is operational } \wedge Z_j = 0) \vee (\forall s \in \cS(i) Y_{is} = 0)] \\
	& \le \Pr [\exists j \in \cN(i) : (i, j) \text{ is operational } \wedge Z_j = 0] + \Pr [\forall s \in \cS(i) Y_{is} = 0] \\
	& \le y \sum_{j \in \cN(i)} \beta_j + x^n.
	\end{align*}

	The number of failed products equals $\ev {} {F} = \sum_{i \in \cK} \beta_i$. Thus, finding the upper bound on $\ev {} {F}$ corresponds to solving the following LP,
	\begin{align*}
		\max _{\beta \in [\zero, \one]} \quad & \sum_{i \in \cK} \beta_i \qquad \text{s.t.} \quad & \beta \le y A^T \beta + x^n \one.
	\end{align*}

	When $y \| A^T \|_1 \le 1$, which is equivalent to $y \le \frac {1} {\Delta}$, this problem resembles the financial clearing problem of \citet{eisenberg2001systemic}, and from Lemma 4 of \citet{eisenberg2001systemic}, we know that we can also compute $\beta$ by solving the fixed point equation $\beta = \one \wedge (y A^T \beta + x^n \one) = \Phi(\beta).$ The set of fixed points $\mathsf{FIX} (\Phi) = \{ \beta \in [\zero, \one] : \beta = \Phi(\beta) \}$ has a greatest fixed point $\overline \beta^*$ and a least fixed point $\underline \beta^*$, due to the Knaster-Tarski Theorem.


\subsection{Proof of \texorpdfstring{\cref{prop:katz}}{prop:katz}}\label{app:proof:prop:katz}

    First of all, it is easy to see that $\Phi(\beta)$ is a contraction under the assumption $y < \frac 1 \Delta$, and, thus, by Banach's fixed point theorem, $\Phi(\beta)$ has a unique fixed point $\beta^*$. Finally, note that, for any $i \in \cK$ we have that $\left ( y A^T \beta^* + x^n \one \right )_i = y \sum_{j \in \cN(i)} \beta_j^* + x^n < y \Delta + (1 - y \Delta) < 1$. Therefore, the fixed point equation simplifies to $\beta^* = y A^T \beta^* + x^n \one$, which has a solution $\beta^* = x^n (I - y A^T )^{-1} \one$, which corresponds to the Katz centrality, since $yA$ is substochastic, and subsequently, $I - yA^T$ is invertible. Finally, from Markov's inequality $\Pr [F \ge \varepsilon K] \le \frac {x^n \one^T (I - yA^T)^{-1} \one} {\varepsilon K} = \frac {x^n \| \mathsf{vec} ((I - yA^T)^{-1}) \|_1} {\varepsilon K}$, where the last equality holds since $(I - yA^T)^{-1}$ has non-negative elements (since $A^T$ is an adjacency matrix and $(I - yA^T)^{-1}$ can be expressed as Neumann series). To make the above $1 / K$ is suffices to pick, $x = \left ( \frac {\varepsilon} {\| \mathsf{vec} ((I - yA^T)^{-1} \|_1} \right )^{1/n}$, hence the lower bound in $R_{\cG}(\varepsilon)$. 

 
\subsection{Proof of \texorpdfstring{\cref{prop:intervention}}{prop:intervention}}\label{app:prop:intervention}

Since $0 < y < \frac 1 {\Delta}$ and $0 < x < (1 - y\Delta)^{1/n}$, the optimal solution of the internal maximization equals $\hat \beta(t) = x (I - yA^T)^{-1} (\one - t)$. Substituting that in the objective function of \cref{eq:interventions}, we get that 
\begin{align*}
    \hat t = \arg \min_{t \in \{ 0, 1 \}^n } \one^T \hat \beta(t) & = \arg \min_{t \in \{ 0, 1 \}^n} \one^T (I - yA^T)^{-1} (\one - t) = \arg \min_{t \in \{0, 1\}^n} (\one - t)^T ((I - yA^T)^{-1})^{T} \one \\
     & = \arg \min_{t \in \{ 0, 1 \}^n} (\one - t)^T (I - yA)^{-1} \one = \arg \min_{t \in \{ 0, 1 \}^n } \gamma_{\mathsf{Katz}}^T(\cG^R,y) (\one - t) 
\end{align*} 

where, we remind that $\gamma_{\mathsf{Katz}}(\cG^R,y) = (I - yA)^{-1} \one$ is the vector of Katz centralities for the \emph{reverse graph $\cG^R$}. The third equality is true since $(I - yA^T)^{-T} = \left ( \sum_{k \ge 0} (yA^T)^k \right )^T = \sum_{k \ge 0} (yA)^k \overset {y < \frac 1 {\Delta_R}} {=} (I - yA)^{-1}$. It is easy to observe that, by the rearrangement inequality, the optimal solution would be to intervene to the top-$T$ nodes in terms of Katz centrality in $\cG^R$.


\section{Upper and Lower Bounds on \texorpdfstring{$\ev {} {S}$}{EvS} for the \texorpdfstring{$m$}{m}-ary tree} \label{app:upper_and_lower_bounds_tree}

\begin{lemma} \label{lemma:probability_functional}
    Let $q_d$ be the probability that a product in tier $d$ can be produced. Then 
    \begin{equation}
         q_{d} = \begin{cases} \left ( 1 - x^n \right )^{\frac{m^{D - d + 1} - 1} {m - 1}}, & m \ge 2 \\
         \left ( 1 - x^n \right )^{D - d + 1}, & m = 1
         \end{cases}
    \end{equation}
\end{lemma}

\begin{proof} \newline
Let$ q_d = \Pr [\text{a product in tier $d$ can be produced}] = \Pr [\exists \text{a functional supplier at tier $d$}]$. To calculate $q_d$, note that all inputs for a product node at tier $d$ succeed with probability $q_{d + 1}^m$ and then the probability that at least one supplier is functionally conditioned on the all the inputs working is $1 - x^n$. That yields the following recurrence relation $q_d = q_{d + 1}^m (1 - x^n)$ with $q_{D + 1} = 1$. Solving this recurrence relation, we get that for $d \in [D]$, $q_{D - d} = \left ( 1 - x^n \right )^{\sum_{l = 0}^d m^l}$. This yields to  $q_{d} = \begin{cases} \left ( 1 - x^n \right )^{\frac {m^{D - d + 1} - 1} {m - 1}}, & m \ge 2 \\
         \left ( 1 - x^n \right )^{D - d + 1}, & m = 1
         
         \end{cases}
    $.
\end{proof}

\subsection{Proof of \texorpdfstring{\cref{prop:dag_lb_sparse}}{prop:daglbsparse}}\label{app:proof:prop:dag_lb_sparse}

    Let $\cG$ be a DAG, and let $\beta^*$ be an optimal solution to \cref{theorem:general_ub}, and let the link operation probability $y$ be a value to be determined later. Assume that $v_1, \dots, v_K$ is a topological ordering of the DAG from raw materials to more complex materials. We have that for the $i+1$-th vertex: $\beta_i^* = y \sum_{j \in \cN(v_{i})} \beta_j^* + x^n \le y \sum_{j \le i} \beta_j^*$.

    It is easy to see (by induction) that the solution to the above recurrence is: $\beta_i^*  \le (1 + y)^{i - 1} x^n$
    % \begin{align*}
    %     \beta_1^* & \le  x^n, \\
    %     \beta_2^* & \le yx^n + x^n  = (1 + y) x^n, \\
    %     \beta_3^* & \le y (1 + y) x^n + yx^n + x^n = (1 + y)^2 x^n, \\
    %     \vdots & \quad \vdots \\
    %     \beta_i^* & \le (1 + y)^{i - 1} x^n, 
    % \end{align*}
    which is tight when all the possible $\binom K 2$ edges in the DAG exist. Therefore, $\ev {} {F} \le \sum_{i \in \cK} \beta_i^* \le x^n \sum_{i = 1}^K (1 + y)^{i - 1} \le \frac {x^n (1 + y)^K} {y} \le \frac {K x^n e^{K y}} {Ky}$. This yields the first part of the proof. For the second part we want to minimize the RHS so that we maximize the lower bound on the resilience. Hence, we choose $z = K y > 0$ so that $\frac {e^z} {z}$ is minimized, which happens at $z = 1$, and equivalently $y = \frac 1 K$, at which case we have that $\ev {} {F} \le K x^n \min_{z > 0} \frac {e^z} {z} = K x^n e$. By Markov's inequality we can get, similarly to \cref{theorem:general_ub}, that $R_{\cG}(\varepsilon) \ge \left ( \frac {\varepsilon} {e K} \right )^{1/n}$.
    

\begin{lemma}[Upper Bound when $m \ge 2$] \label{lemma:upper_bound_tree}

Under the tree structure and $m \ge 2$ the expected size obeys $\ev {} {S} \le \frac {K x^n (D - 1)} {2} = U(x)$. 

\end{lemma}

\begin{proof} \newline 

From \cref{lemma:probability_functional} we have that $q_d =\left ( 1 - x^n \right )^{\frac {m^{D - d + 1} - 1} {m - 1}}$. Using the inequality $(1 - t)^a \le \frac 1 {1 + ta}$ for $a > 0$ and $t \in (0, 1)$ we get that 
    \begin{align*}
        \ev {} {S} & = \sum_{d = 1}^D m^{d - 1} q_d \le \sum_{d = 1}^D m^{d - 1} \frac {1} {1 + x^n \frac {m^{D - d + 1}} {2(m - 1)}} \asymp \int_{t = 1}^D \frac {m^{t - 1}} {1 + x^n \frac {m^{D - t + 1}} {2(m - 1)}}.
    \end{align*}
By letting $u = \frac {x^n m^{D - t + 1}} {2 (m - 1)}$ we get that the above integral equals
\begin{align*}
    & \frac {m^D x^n} {2 \log m (m - 1)} \log \left ( \frac {(1 - x^n)(1 - p)^{\varepsilon K} (1 + (1 - x^n)(1 - p)^{K})} {(1 - x^n)(1 - p)^{K} (1 + (1 - x^n)(1 - p)^{\varepsilon K})} \right ) \bigg |_{u_2 = x^n m^D / 2(m-1)}^{u_1 = x^n m / 2(m-1)} \\ & \le \frac {m^D x^n} {2 \log m (m - 1)} \log \left ( m^{D - 1} \right ) \le \frac {K x^n (D - 1)} {2} = U(x).
\end{align*}

\end{proof}

\begin{lemma}[Lower bound when $m \ge 2$] \label{lemma:lower_bound_tree}
    Under the tree structure and $m \ge 2$ the expected cascade size obeys $\ev {} {S} \ge K (1 - x^n(D - 1)) = L(x)$ where $K = m^D - 1$ is the number of products. 
\end{lemma}

\begin{proof}\newline
    From \cref{lemma:probability_functional} we have that $q_d =\left ( 1 - x^n \right )^{\frac{m^{D - d + 1} - 1} {m - 1}}$. By Bernoulli's inequality $q_d \ge 1 - x^n \left (\frac {m^{D - d + 1} - 1} {m - 1} \right)$.
    
    Since every level has $m^{d - 1}$ nodes we have that 
    \begin{align*}
        \ev {} {S} & = \sum_{d = 1}^D m^{d - 1} q_d \ge \sum_{d = 1}^D m^{d - 1} \left [ 1 - x^n \left (\frac {m^{D - d + 1} - 1} {m - 1} \right) \right ] = K (1 + x^n) - x^n \frac {1} {m - 1} \sum_{d = 1}^D m^{d - 1} m^{D - d + 1} \\ &  = K (1 + x^n) - x^n \frac {1} {m - 1} \sum_{d = 1}^D m^D = K (1 + x^n) - x^n D (K - 1) \ge K (1 - x^n (D - 1)).
    \end{align*}
 \end{proof}   

\section{Random Width-\texorpdfstring{$w$}{W} Trellis Architectures} \label{app:random_trellis}

% \subsection{Random Width-\texorpdfstring{$w$}{W} Trellis} 

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[transform shape, scale=0.7]
        \Vertex[x=0, y=0]{u1}
        \Vertex[x=0, y=1,color=pink]{u2}

        \Vertex[x=1.5, y=0]{v1}
        \Vertex[x=1.5, y=1, color=pink]{v2}

        \Vertex[x=3, y=0, color=pink]{w1}
        \Vertex[x=3, y=1]{w2}

        \Edge[color=black,Direct](u1)(v1)
        \Edge[color=black,Direct](u1)(v2)

        \Edge[color=black,Direct](v1)(w2)
        \Edge[color=black,Direct](v2)(w1)
        
    \end{tikzpicture}
    \caption{Instance of random width-$w$ trellis with $w = 2, D =3, K = 6$. Failures are drawn in pink.}
    \label{fig:trellis}
\end{figure}

We analyze the resilience of the random width-$w$ trellis depicted in \cref{fig:trellis}. The width-$w$ trellis, $\mathsf{rt}(K, p, w)$ is a graph with $K = w D$ nodes, that consists of $D$ tiers of products $\cK_1, \dots, \cK_D$ and each tier has size/width $|\cK_d|= w$ for all $d \in [D]$. The edges are generated randomly and independently with probability $p$ if $i \in \cK_d, j \in \cK_{d + 1}, d \in [D - 1]$ and do not exist otherwise. We prove the following,

% \mpcomment{revise}
% \mpcomment{add a reference to this appendix at the end of parallel products}

\begin{theorem} \label{theorem:trellis_resilience}
    Let $\cG \sim \mathsf{rt}(K, p, w)$ be a random width-$w$ trellis with $K = wD$ nodes. Then the resiliency of $\cG$ satisfies, 
    {\scriptsize
    \begin{align}
        R_{\cG}(\varepsilon) \ge \begin{cases} \Omega \left ( \left ( \frac {\varepsilon w^2} {K^2} \right )^{1/n} \right ) = \Omega \left ( \left ( \frac {\varepsilon } {D^2} \right )^{1/n} \right ), & \text{for } pw \le 1 \\ \left ( \frac {\varepsilon w (pw - 1)} {K (pw)^{K / w}} \right )^{1/n} = \left ( \frac {\varepsilon  (pw - 1)} {D (pw)^{K / w}} \right )^{1/n}, & \text{for } pw > 1 \end{cases}, \quad
        R_{\cG} (\varepsilon) \le \begin{cases} 
        \left ( \frac {(1 + \varepsilon)w (1  -pw)} {2} \right )^{1/n}, & \text{for } pw < 1 \\
        O \left ( \left ( \frac {w^2} {K}  \right )^{1/n} \right ) = O \left ( \left ( \frac {w} {D}  \right )^{1/n} \right ), & \text{for } pw = 1 \\
        O \left ( \left ( \frac {w} {K} \right )^{1/n} \right ) = O \left ( \left ( \frac {1} {D} \right )^{1/n} \right ), & \text{for } pw > 1
        \end{cases}
    \end{align}
    }
    % and 

    % \begin{align}
    % R_{\cG} (\varepsilon) \le \begin{cases} 
    %     \left ( \frac {(1 + \varepsilon)w (1  -pw)} {2} \right )^{1/n}, & \text{for } pw < 1 \\
    %     O \left ( \left ( \frac {w^2} {K}  \right )^{1/n} \right ) = O \left ( \left ( \frac {w} {D}  \right )^{1/n} \right ), & \text{for } pw = 1 \\
    %     O \left ( \left ( \frac {w} {K} \right )^{1/n} \right ) = O \left ( \left ( \frac {1} {D} \right )^{1/n} \right ), & \text{for } pw > 1
    % \end{cases}
% \end{align}

Therefore,

\begin{compactitem}
    \item If $pw \le 1$, $K, w \to \infty$, and $K / w \to D < \infty$, then $\cG$ is resilient. 
    \item If $pw \ge 1$, $K, D \to \infty$, and $K / D \to w < \infty$, then $\cG$ is fragile.
\end{compactitem}
\end{theorem}

\begin{proof} \newline
\textbf{Lower Bound.} To determine a lower bound in $\cR_{\cG}(\varepsilon)$ we let $\gamma_d$ be the probability that a node at tier $d \in [D]$ fails (due to symmetry the probability is the same for each product of a given tier). The expected number of failures is $\ev {} {F} = D \sum_{d \in [D]} \gamma_d$. By applying the union bound -- similarly to \cref{theorem:general_ub} -- we get that $\gamma_{d + 1} \le p w \gamma_d + x^n$. Solving the recurrence, yields  $\gamma_d \le x^n \left ( (pw)^{d - 1} + \frac {1} {1 - pw} \right )$ for $pw \neq 1$, and $\gamma_d \le d x^n$ for $pw = 1$. Summing up everything,
\begin{align*}
    \ev {} {F} = \frac {K} {w} \sum_{d \in [D]} \gamma_d \le \begin{cases} \frac {K x^n} {w} \frac {(pw)^{K / w}} {pw - 1}, & \text{for } pw > 1  \\ \frac {K^2 x^n} {w^2} \frac {1} {1 - pw}, & \text{for } pw < 1 \\ \frac {K^2 x^2} {w^2}, & \text{for } pw = 1 \end{cases} \le \begin{cases} \frac {K x^n} {w} \frac {(pw)^{K / w}} {pw - 1}, & \text{for } pw > 1  \\ C \frac {K^2 x^n} {w^2} , & \text{for } pw \le 1 \end{cases}
\end{align*}

Therefore, letting $\ev {} {F} = \varepsilon$ yields the following lower bounds for the resilience: $\underline R_{\cG} (\varepsilon) = \begin{cases} \Omega \left ( \left ( \frac {\varepsilon w^2} {K^2} \right )^{1/n} \right ), & \text{for } pw \le 1 \\ \left ( \frac {\varepsilon w (pw - 1)} {K (pw)^{K / w}} \right )^{1/n}, & \text{for } pw > 1 \end{cases}$. If $w$ is constant, as $K \to \infty$ we have that $\lim_{K \to \infty} \underline R_{\cG}(\varepsilon) = 0$ for all $p \in (0, 1]$, and $w \in \mathbb N$. Now, if $w, K \to \infty$ but $K / w \to D$ where $D$ is finite, then $\cG$ is resilient  with resilience $\Omega \left ( \left ( \frac {\varepsilon} {D^2} \right )^{1/n} \right )$ for $pw \le 1$. If $pw > 1$, then it is easy to prove that the lower bound goes to 0. 

\textbf{Upper Bound.} To construct an upper bound, we need a lower bound on $\ev {} {F}$. Let $f_{d}$ be the expected number of failures conditioned on any node in tier $d$ failing solely (and spontaneously). As base case we have that $f_D = 1$. We have that a cascade starting from any node at tier $d$ causes at least $a_{d, k}$ failures on expectation at an offset $k$ from tier $d$. We have that for all $d$, $a_{d, 0} = 1$, and that $a_{d, k + 1} \ge \min \{ w, pw a_{d, k}  \}$ since the failures are amplified by a factor of $pw$ at each level, up to being $w$, since the width of the trellis is $w$. Subsequently $f_d \ge \sum_{k = 0}^{D - d} a_{d, k}$.

For $pw = 1$ we have that $a_{d, k + 1} \ge \min \{ w, a_{d, k} \}$, and since $a_{d, 0} = 1$ we have that $a_{d, k} = 1$ for all $k$. 

For $pw < 1$  we have that at every new offset the number of failures is decreasing by a factor of $pw$. Therefore, we deduce that $a_{d, k + 1} \ge (pw) a_{d, k}$, and since $a_{d, 0} = 1$ we have that $a_{d, k} \ge (pw)^{k}$. 

For $pw > 1$, for $k \le \frac {w} {\log (pw)}$ we have that at least $(pw)^k \ge k \log (pw)$ nodes are influenced and are at most $w$. For $k > \frac {w} {\log (pw)}$, $w$ nodes are infected at every step on expectation.

Therefore, we get that $f_d \ge  \sum_{k = 0}^{D - d} a_{d, k} \ge \begin{cases}
        \frac {1 - (pw)^{D - d + 1}} {1 - pw}, & \text{for } pw < 1 \\
        D - d + 1, & \text{for } pw = 1 \\
        \sum_{k = 0}^{w / \log (pw)} 
        \log (pw) k + w \left ( D - d - \frac {w} {\log (pw)} \right ). & \text{for } pw > 1
    \end{cases}.$

Therefore the lower bound on $\ev {} {F}$ is $\ev {} {F} \ge x^n \sum_{d = 1}^D f_d$.  We have the following three cases

\begin{compactenum}
    \item If $pw = 1$ then by a change of summation indices $\ev {} {F} \ge x^n \sum_{k = 1}^D k = x^n \frac {D (D + 1)} {2} \ge \frac {x^n D^2} {2} = \frac {x^n K^2} {2w^2}$.

    \item If $pw < 1$ then, by change of summation indices we have that $\ev {} {F} \ge \frac {x^n} {1 - pw} \sum_{k = 1}^D(D - k + 1) (pw)^{k - 1} \ge \frac {x^n D} {1 - pw}$.

    \item If $pw > 1$, we have that for $w \ge 2$,
    \begin{align*}
        \ev {} {F} & \ge x^n \left \{ \log (pw) \sum_{k = 0}^{w / \log(pw)} k + w \left (D - d - \frac {w} {\log (pw)} \right ) \right \} \ge x^n \sum_{d = 1}^D \left \{ \frac {w^2 / 2 - w} {\log (pw)} + w(D - d) \right \} \\
        & \ge x^n w \sum_{d = 1}^D (D - d) \ge x^n w \frac {D^2} {2}  = \frac {x^n K^2} {2 w}
    \end{align*}
    
\end{compactenum}

We want to make each of the above lower bounds equal to $\frac {1 + \varepsilon} {2} K$, since that would give an $\frac {1 - \varepsilon}{2} K$ upper bound on $\ev {} {S}$, and due to \cref{lemma:upper_bound_resilience} we get the upper bound \\ $\overline R_{\cG}(\varepsilon)$: $\overline R_G(\varepsilon) = \begin{cases} 
        \left ( \frac {(1 + \varepsilon)w (1  -pw)} {2} \right )^{1/n}, & \text{for } pw < 1 \\
        O \left ( \left ( \frac {w^2} {K}  \right )^{1/n} \right ), & \text{for } pw = 1 \\
        O \left ( \left ( \frac {w} {K} \right )^{1/n} \right ), & \text{for } pw > 1
    \end{cases}$. Therefore, if $pw \ge 1$, $K, D \to \infty$ and $K / D \to w$ with $w$ finite, then $\cG$ is fragile. 
\end{proof}


\section{Supplier Heterogeneity \& Optimal Interventions} \label{app:supplier_heterogeneity}

Here we give a proof of the derivation of the optimal intervention mechanism through supplier heterogeneity presented in \cref{sec:discussion}. Namely, we increase the number of suppliers of product $i$, from $n$ to $n + \nu_i$ for some $0 \le \nu_i \le \overline \nu_i$, subject to a budget $\sum_{i \in \cK} \nu_i \le N$, to decrease its self-percolation probability from $x^n$ to $x^{n + \nu_i}$. Under the Assumptions of \cref{prop:intervention}, the optimization problem is: 
\begin{align*}
    \hat \nu = \arg \min_{\zero \le \nu \le \overline \nu, \one^T \nu \le N} x^n \gamma_{\mathsf{Katz}}^T (\cG^R,y) x^{\odot \nu} = \arg \min_{\zero \le \nu \le \overline \nu, \one^T \nu \le N} \gamma_{\mathsf{Katz}}^T (\cG^R,y) x^{\odot \nu},
\end{align*} where $x^{\odot \nu}$ is the Hadamard power (elementwise power) of $x$ raised to the vector $\nu$, i.e.  $x^{\odot \nu} = (x^{\nu_1}, \dots, x^{\nu_K})^T$. Since $x^\nu$ is a decreasing function of $\nu$, the optimal policy is to order the nodes in decreasing order $\pi$ in terms of their Katz centrality in $\cG^R$ and then try to put as many suppliers as possible in $\pi(1), \pi(2), \dots, $ while respecting the budget constraint $N$ at the same time. This yields the following optimal policy  $\hat \nu_{\pi(i)} = \left ( \overline \nu_{\pi(i)} \wedge \left ( N - \sum_{j < i} \hat \nu_{\pi(j)} \right ) \right )^+.$ where $(a)^+ = \max \{a, 0 \}$. 

\section{Extended Related Work} \label{app:further_related_work}


\paragraph{Supply Chain Contagion.} There have been multiple works on production networks in macroeconomics and how shocks in production networks propagate through the production network's input-output relations, see a comprehensive survey by \citet{carvalho2019production}. One of the earliest works dates back to \citet{horvath1998cyclicality} introduces a multi-sector model that extends the earlier Long-Plosser model \cite{long1983real} and argues that sector-specific shocks are, in fact, affected by the graph topology between producing sectors. Such arguments contrast the prior arguments of \citet{lucas1995understanding}, which argue that small microeconomic shocks would significantly affect the economy. 
In the 2008 financial crisis, the ex-CEO of Ford, Alan Mulally, argued that the collapse of GM or Chrysler would significantly impact Ford's production capabilities for non-trivial amounts of time. 
The works of \citet{acemoglu2012network, gabaix2011granular} build on the above observation and shows that even small shocks lead to cascades that can have devastating effects on the economy. Specifically, \citet{gabaix2011granular} argues that firm-level idiosyncratic shocks can translate into large fluctuations in the production network when the firm sizes are heavy-tailed, and, in the sequel, \citet{acemoglu2012network} replaced \citet{gabaix2011granular}'s analysis on the firm size with the intersectoral network, building on the Long-Plosser model. \citet{hallegatte2008adaptive} introduces a supply-chain model in which when a firm cannot satisfy its orders, it rations its production to the firms that depend on it via the Input-Output matrix. The model of \citet{hallegatte2008adaptive} has been used to study supply chain effects of the COVID-19 pandemic \citep{guan2020global,walmsley2021impacts,inoue2020propagation,pichler2020production}. Finally, the recent work of \cite{elliott2022supply} studies the propagation of shocks in a network (see \cref{sec:introduction}).   
Our work is related to the above works and attempts to study the propagation of individual shocks at the supplier level to the (aggregate) production network through the definition of a resilience metric. 

% \citet{perera2017network} systematically reviews the literature on modeling the topology and robustness of production networks with applications on real-world data. The first observation is that many real-world production networks follow a power law degree distribution with an exponent of around $2$. Moreover, the resilience (called ``robustness''  in their work) of such networks is defined as the largest connected component size or the average/max path length under the presence of random failures. On the contrary, we propose a novel theory-informed resilience metric that complements the existing measures in \citet{perera2017network} and the references therein, and supplement their empirical works with novel theories about how topological attributes contribute to increased risk of cascading failures in production networks. We also provide empirical results similar to the ones that \citet{perera2017network} study. 



\paragraph{Seeding Problems in Supply Chains.} In a different flavor from the literature that we discussed above, the work of \citet{blaettchen2021traceability} study how to find the least costly set of firms to target as early adopters of a tractability technology in a supply chain network, where hyperedges model individual supply chains. 
% Their model is also related to influence models and target set selection as in the works of \citet{granovetter1978threshold,kempe2003maximizing,chen2010scalable,chen2009approximability}. 
The connections to our paper involve the spread of information in a networked  environment through interventions. However, in this paper, we do not focus on building algorithms for interventions; rather, we provide metrics to assess the vulnerability of nodes in a supply chain network, which can be informative for interventions. 


\paragraph{Node Percolation.}  \citet{goldschmidt1994reliability,yu2010uniformly} study node percolation processes, wherein graph nodes fail independently with probability $p$. Their goal is to find the graphs -- among graphs with a fixed number of nodes and edges -- such that the probability that the induced subgraph (after percolation) is connected is maximized. 

% Also, the work of \citet{kleinberg2004network} has focused on node and edge percolation in graphs and the study of network failures. \citet{piraveenan2013percolation} define a centrality metric based on graph percolation, which generalizes the betweenness centrality measure.   

\paragraph{Resilience and Risk Contagion in Financial Networks.} In a different context, similar models have been used to study financial networks, and optimal allocations in the presence of shocks \citep{eisenberg2001systemic, glasserman2015likely, papachristou2021allocating,papachristou2022dynamic,papp2021default,friedetzky2023payment,demange2018contagion,ahn2019optimal} and financial network formation and risk, see, e.g., \citep{blume2013network,jalan2022incentive,jackson2019makes,aymanns2015contagious,babus2013formation,erol2019network,erol2022network,amelkin2019yield,talamas2020free,bimpikis2019supply}. Our work is related to the above works and attempts to study the propagation of individual shocks at the supplier level to the (aggregate) production network through the definition of a resilience metric.  \citet{acemoglu2015systemic} study financial networks and state that contagion in financial networks has a phase transition: for small enough shocks, a densely connected financial network is more stable, and, on the contrary, for large enough shocks, a densely connected system is a more fragile financial system. In a similar spirit, \citet{amini2012stress} and \citet{battiston2012liaisons} characterize the size of defaults under financial contagion in random graphs. They show that firms that contribute the most to network instability are highly connected and have highly contagious links. Moreover, it has also been shown -- see, e.g., \citet{siebenbrunner2018clearing,battiston2012debtrank,bartesaghi2020risk}, and the references therein -- that risky nodes in financial networks are connected to centrality measures. 

\paragraph{Cascading Failures and Emergence of Power Laws.} There has been a large body of literature on cascading failures in networks and how the cascade distributions behave as power laws in social networks \citep{leskovec2007patterns, wegrzycki2017cascade}, and power grids, see e.g., \citep{dobson2004branching,nesti2020emergence}). We bring the perspective of supply chains and production networks to this literature and offer new insights on how complexity of products and their interdependence affect production network resilience.

\section{Experiments Addendum: World I-O Tables} \label{app:experiments_addendum}

\begin{table}[H]
    \scriptsize
    \centering
    \begin{tabular}{lllllll}
    \toprule
         Country  & Size ($K$) & Avg. Degree & Density & Min/Max In-degree & Min/Max Out-degree  & AUC  \\
    \midrule 
         USA & 55 & 54.00 & 1.000 & 54 & 54 & 0.052 \\
         Japan & 56 & 45.33 & 0.824 & 0 -- 50 & 0 -- 50  & 0.058 \\
         G. Britain & 56 & 52.05 & 0.946 & 0 -- 54 & 0 -- 54  & 0.052  \\
         China & 56 & 37.89 & 0.688 & 0 -- 46 & 0 -- 46  & 0.078 \\
         Indonesia & 56 & 35.75 & 0.65 & 0 -- 46 & 0 -- 46  & 0.078 \\
         India & 56 & 29.57 & 0.537 & 0 -- 41 & 0 -- 43 & 0.095 \\
    \bottomrule
    \end{tabular}
    \caption{Network Statistics and AUC for the world economies. The edge density is computed as $\frac {|\cE(\cG)|} {K^2 - K}$.}
    \label{tab:statistics-world}
\end{table}
\vspace{-30pt}
\begin{figure}[H]
    \centering
    \subfigure[Estimating $R_{\cG}(\varepsilon)$\label{subfig:world_io_tables_resilience}]{\includegraphics[width=0.3\textwidth]{figures/resilience_monte_carlo_vs_eps_wiot.pdf}}
    \subfigure[Optimal Interventions\label{subfig:world_io_tables_interventions}]{\includegraphics[width=0.3\textwidth]{figures/resilience_lb_vs_key_wiot.pdf}}
    \caption{World Economy Input-Output Networks. We set the number of suppliers for each product to $n = 1$.}
    \label{fig:wolrd-io}
\end{figure}


% \mpcomment{maybe citet/citep other things}



% \section{Bounds for General Supply Chain Graphs: An Alternative Approach} \label{app:general_lb} 


% There is an alternative way to devise a lower bound for $\ev {} {S}$, which translates to an upper bound for $F = K - S$, as the solution to a convex program. More specifically, the next Theorem provides such a bound

% \begin{theorem} \label{theorem:general_lb}
%     For a deterministic graph $\cG$ with adjacency matrix $A$ and in-degree vector $d$, a lower bound for $\ev {} {S}$ can be found by solving the following convex optimization program

%     \begin{align}
%         \ev {} {S} \ge \mathsf{OPT}_2 = \max_{\gamma \ge \zero} \quad & \sum_{i \in \cK} e^{-\gamma_i} \\
%         \text{s.t.} \quad & \gamma \le A^T \gamma + \log \left ( \frac {1} {1 - x^n} \right ) \one + d \log y \nonumber
%     \end{align}
% \end{theorem}

% \begin{proof} \newline
%     We have that for each product $i \in \cK$ if $d_i = |\cN(i)|$,

%     \begin{align*}
%         \Pr [Z_i = 1] & = \Pr \left [ \exists s \in \cS(i): Y_{is} = 1 \wedge \bigcap_{j \in \cN(i)} \{ Z_j = 1 \} \wedge \{ (i, j) \text{is operational} \right ] \\
%         & = (1 - x^n) y^{d_i} \Pr \left [ \bigcap_{j \in \cN(i)} \{ Z_j = 1 \} \right ]
%     \end{align*}

%     Now, it is easy to observe that the events $\{ Z_j = 1 \}_{j \in \cN(i)}$ are positively correlated, i.e. if for a subset $U$ of $\cN(i)$ we have $Z_j = 1$, then there's a higher chance that product $j' \in \cN(i) \setminus U$ would be operational conditioned on $\bigcap_{j \in U} \{ Z_j = 1 \} $, since there may be inputs of $j'$ that belong to $U$ which are already operational, and in the case that there are no such inputs, then $\Pr \left [ Z_{j'} = 1 | \bigcup_{j \in U} Z_j = 1 \right ] = \Pr [Z_{j'} = 1]$. Therefore

%     \begin{align} \label{eq:pos_corr}
%          \Pr [Z_i = 1] & \ge  (1 - x^n) y^{d_i} \prod_{j \in \cN(i)} \Pr \left [  Z_j = 1  \right ]
%     \end{align}

%     We let $\gamma_i = -\log \left ( \Pr [Z_i = 1] \right )$. Since $\Pr [Z_i = 1] \in [0, 1]$, then $\gamma_i \ge 0$. \cref{eq:pos_corr} is now written as $\gamma \le A^T \gamma + \log \left ( \frac 1 {1 - x^n} \right ) \one + d \log y$. Finally, minimizing $\ev {} {S}$ corresponds to $\min_{\Pr [Z_i = 1] \in [0, 1]} \sum_{i \in \cK} \Pr [Z_i = 1] = \max_{\gamma \ge 0} \sum_{i \in \cK} e^{-\gamma_i}$. 
    
% \end{proof}


% \mpcomment{move to appendix}


% \begin{figure}[t]
%     \centering
%     \begin{tikzpicture}[transform shape]
%         \Vertex[x=-1, y=0, label=$1$]{u1}
%         \Vertex[x=1, y=0, label=$2$]{u2}
%         \Vertex[x=0, y=1, label=$3$]{u3}
%         \Edge[Direct, color=black](u2)(u1)
%         \Edge[Direct, color=black](u2)(u3)
%         \Edge[Direct, color=black](u3)(u1)
        
%     \end{tikzpicture}
    
    
%     \caption{Example for \cref{theorem:general_lb,theorem:general_ub} for $y = 1$. The optimization of \cref{theorem:general_ub} yields $\beta = (3x^n, x^n, 2x^n)$ and a bound $\ev {} {F} \le 6x^n$, and subsequently $\ev {} {S} \ge 3 - 6x^n$, whereas the optimization of \cref{theorem:general_lb} yields $\gamma = (-3 \log( 1 - x^n), - \log (1 - x^n), - 2 \log (1 - x^n))$ which yields $\ev {} {S} \ge (1 - x^n) + (1 - x^n)^2 + (1 - x^n)^3$. It is easy to observe that the two bounds are different and that the second bound is tight in this example.}
%     \label{fig:optimization}
% \end{figure}

