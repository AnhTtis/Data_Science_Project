In this section, we first give a detailed implementation of the proposed method. In Section \ref{sec:heuristic}, we provide intuitions to explain why it works and how it improves upon the existing algorithm. In Section \ref{sec:numerical}, we compare the numerical performances of the proposed method with the vanilla spectral method on synthetic datasets. 

Algorithm \ref{alg:main-algo} describes our proposed spectral algorithm. The first step computes the top $d$ eigenvectors of the observation matrix $X$, which can be done using any off-the-shelf numerical eigendecomposition routine. The output of Step 1 is the empirical eigenspace $U \in \R^{nd\times d}$ corresponding to the top $d$ eigenvectors. Step 2 constructs the anchor $M$ used to recover the permutations by performing $d$-means clustering on the $nd$ rows of $U$  and extracting the estimated cluster centers. Step 3 recovers the underlying permutations via the Kuhn-Munkres algorithm. Note that (\ref{eqn:alg2}) in Step 3 can be interpreted as a projection of $U_j \hatH^\top$ onto $\Pi_d$. This is because all permutation matrices in $\Pi_d$ have the same Frobenius norm and (\ref{eqn:alg2}) can be equivalently written as
\begin{align}\label{eqn:alg3}
\hat Z_j = \argmin_{P\in\Pi_d}\fnorm{P - U_j M^\top}, \forall j\in[n].
\end{align}

\begin{algorithm}[ht]
\SetAlgoLined
\KwIn{Data matrix $X \in \mathr^{nd\times nd}$}
\KwOut{$n$ permutation matrices $\hat Z_1, \hat Z_2, \ldots, \hat Z_n\in\Pi_d$}
 \nl Obtain the top $d$ eigenvectors of $X$ as $U \in \mathr^{nd \times d}$\;
 \nl Denote the rows of $U$ as $v_1,\ldots,v_{nd}\in\mathr^{d\times 1}$ such that $U=(v_1^\top,\ldots, v_{nd}^\top)^\top$. Run $d$-means on $v_1,\ldots,v_{nd}$ and denote $\hat \mu_1,\ldots,\hat \mu_d\in\mathr^{d\times 1}$ to be cluster centers:
 \begin{align}
(\hat \mu_1,\ldots,\hat \mu_d)\define\argmin_{\mu_1,\ldots,\mu_d\in\mathr^{d\times 1}} \min_{z\in[d]^{nd}} \sum_{j=1}^{nd}\norm{v_j - \mu_{z_j}}^2.\label{eqn:alg1}
 \end{align}
Define $\hatH \define (\sqrt{n}\hat \mu_1^\top,\ldots,\sqrt{n}\hat \mu_d^\top)^\top\in\mathr^{d\times d}$ such that the rows of $\hatH$ are the $d$ centers multiplied by a $\sqrt{n}$ scaling\;
%are  $\hat \mu_1,\ldots,\hat \mu_d$\;
 \nl Compute
 \begin{align}
 \hat Z_j \define \argmax_{P\in\Pi_d} \inner{P}{U_j \hatH^\top} \label{eqn:alg2}
 \end{align}
 for each $i\in[n]$.
\caption{A new spectral method for permutation synchronization. \label{alg:main-algo}}
\end{algorithm}

\subsection{Intuitions}\label{sec:heuristic}
To understand Algorithm \ref{alg:main-algo}, we need to study $U$ through the lens of spectral perturbation theory. Denote
\begin{align}\label{eqn:U_star_def}
U^*\define (U^{*\top}_1,\ldots U^{*\top}_n)^\top \in\mathr^{nd\times n},\text{ where }U_j^* \define Z^*_j/\sqrt{n},\forall j\in[n].
\end{align}
Note that the expected value of the data matrix $X$ is equal to $\E X = p ((J_n - I_n) \otimes J_d) \circ Z^*(Z^*)^\top$. One could verify that $(\E X) U^* = (n-1)p U^*$. As a result, $U^*$ is the leading eigenspace of $\E X$ that includes its top $d$ eigenvectors. Since $X$ is a perturbed version of $\E X$, $U$ can also be seen as a perturbed version of $U^*$. However, this correspondence only holds with respect to an orthogonal transformation, as the leading eigenvalue of $\E X$, $(n-1)p $, has a multiplicity of $d$. That is, $UO\approx U^*$ for some $O\in\mathcal{O}_d$. As a result, the blocks $\{U_j\}$ are close to $\{U^*_j\}$ only up to a global orthogonal transformation, i.e., $U_jO\approx U^*_j$ for all $j\in[n]$.

To estimate the latent permutation $Z_j^*$, the vanilla spectral method uses the product $U_jU_1^\top$ as $U_jU_1^\top = (U_jO)(U_1O)^\top \approx U^*_j(U_1^*)^\top = Z^*_j(Z^*_1)^\top/n$. Intuitively, if $U_jU_1^\top$ is very close to  $Z^*_jZ^*_1/n$ then by (\ref{eqn:old-spectral}), we have $\tilde Z_j = Z^*_j(Z^*_1)^\top$. As a result, when the perturbation between $UO$ and $U^*$ is sufficiently small, the vanilla spectral method is able to recover $Z^*$ up to a global permutation $Z^*_1$. However, as hinted at before, the use of the product $\{U_jU_1^\top\}$ in the vanilla spectral method inevitably leads to a crucial and fundamental limitation. For each $j\in[n]$, write $U_jO= U_j^*+\xi_j$ such that $\xi_j$ can be interpreted as the approximation `noise' of $U_jO$ with respect to $U_j^*$. Then
\begin{align*}
   U_jU_1^\top =   (U_jO)(U_1O)^\top = (U^*_j + \xi_j)(U^*_1 + \xi_1)^\top= U_j^*(U_1^*)^\top + \xi_j(U^*_1)^\top + U^*_j  \xi_1^\top + \xi_j \xi_1^\top.
\end{align*}
That is, $ U_jU_1^\top$ is an approximation of $U_j^*(U_1^*)^\top$ with an error $\xi_j(U^*_1)^\top + U^*_j  \xi_1^\top$ (the higher-order  term $\xi_j \xi_1^\top$ is ignored).  As a function of $   U_jU_1^\top$, the estimation accuracy of $\tilde Z_j$ is determined by both $\xi_j$ and $\xi_1$. As a result, the error caused by $\xi_1$  is carried forward in $\{\tilde Z_j\}_{j\geq 2}$ and impairs the numerical performance of the overall algorithm (see Figure \ref{fig:err}). 
Additionally, using $U_1$ as the anchor makes the performance of the vanilla spectral algorithm less stable because it highly depends on the accuracy of $U_1$.

Algorithm \ref{alg:main-algo} overcomes this crucial limitation of the vanilla spectral method by constructing an anchor that `averages' information across all rows of $U$ instead of just using its first block $U_1$. The key insight into our construction is to recognize the special structure of the permutation synchronization problem where the true eigenspace $U^*$ consists of $d$ unique rows $e_1^\top/\sqrt n,\ldots, e_d^\top/\sqrt n$, each of cardinality exactly $n$. 
% In addition, the euclidean distance between any two unique rows is the same and is rotationally invariant. 
%Since $UO\approx U^*$, we have $U\approx U^*O^\top$. 
The empirical eigenspace $U$, being a noisy estimate of $U^* O^\top$, thus exhibits clustering structures where the cluster centers are the transformed rows $\{e^\top_i O^\top/\sqrt n \}_{i=1}^d$
% This means rows of $U$ concentrates around $k$ rows of $O\top$ and consequently has a cluster structure. 
Clustering algorithms such as the $d$-means algorithm can be used to approximate the cluster centers accurately. With $M$ being an accurate approximation of $O^\top$ (up to a permutation), we have
\begin{align*}
 U_jM^\top \approx U_jO = U^*_j + \xi_j,
\end{align*}
and consequently the estimation accuracy of $\hat Z_j$ is only related to $\xi_j$. After proper scaling, the level of noise in $U_jM^\top $ is approximately half of that in $   U_jU_1^\top$ (i.e., $\xi_j$ only vs. $\xi_j$ and $\xi_1$). The reduced noise in $U_jM^\top$ leads to an improved and more stable numerical performance.

%This explains why

%\paragraph{A Novel and Optimal Spectral Algorithm.} In this work, we introduce a new spectral algorithm (Algorithm \ref{alg:main-algo}) that improves upon a subtle yet crucial limitation of the vanilla spectral algorithm in \cite{pachauri2013solving}. The limitation of the previous spectral method lies in the fact that it uses the first block $U_1$ as the anchor to perform the permutation recovery step in (\ref{eqn:old-spectral}). Its performance depends on how accurate $U_jU_1^\top$ is as an approximation of $U_j^*(U_1^*)^\top$. Note that each block $U_j$ itself is a noisy approximation of $U_j^*$. If the approximation error of $U_1$ is large, this amplifies the error of $ U_jU_1^\top$ for all subsequent block $j > 1$.
%
%On the other hand, one would think that using an anchor which `averages' information across all $n$ blocks of $U$ would lead to a more accurate final estimation because the approximation errors of many blocks tend to `cancel' one another. Building on this intuition, Algorithm \ref{alg:main-algo} carefully constructs an anchor $M$ by performing $d$-means clustering on the rows of the matrix $U$ and extracting the estimated cluster centers. Instead of using $U_1$ in (\ref{eqn:old-spectral}), the new algorithm uses $M$.
%\begin{align*}
%\hat Z_i = \argmin_{P\in\Pi(d)} \fnorm{P- U_i \hatH^\top}
%\end{align*}
%This crucial difference has important practical consequences. Figure \ref{fig:err} clearly shows that our spectral algorithm outperforms the vanilla algorithm in terms of the Hamming loss.
%
%The key insight into our construction of the anchor is to recognize the special structure of the permutation synchronization problem where the true eigenspace $U^*$ consist of $d$ clusters of rows, each of cardinality exactly $n$. Additionally, the euclidean distance between any two cluster centers is rotationally invariant. This means that the empirical eigenspace $U$, which approximate $U^*$ up to a global rotation, preserve this clustering structure. Global clustering algorithms such as the k-means algorithm can be used to approximate the cluster centers accurately. We then apply state-of-the-art clustering analysis to show that the new anchor enjoys a significantly better error guarantee than using single block $U_1$ as the anchor (cf. Proposition \ref{prop:hat_H_new}). 
%This allows us to obtain a sharp error bound for our spectral algorithm, summarized by the following theorem.
%
%
%\paragraph{A Toy Model and Heuristics.} To develop an intuition for the inherent limitation of the vanilla spectral algorithm and the novel insight in the design of Algorithm \ref{alg:main-algo}, let us consider the setting where $Z_i^* = I_d \,\forall i \in [n]$ and $U_i^* = \frac{I_d}{\sqrt n} \,\forall i \in [n]$. Suppose \emph{just for the sake of building intuition} that each $d\times d$ block of the empirical eigenspace is an independent and noisy estimate of the corresponding true eigenspace modulo an appropriate rotation.
%\begin{align*}
% U_i = U_i^*O^\top + \xi_i \,\numberthis\label{eqn:heuristic}
%\end{align*}
%for some $O \in \O_d$ and matrix variate random variable $\xi_i$. In this setting, perfect recovery means to predict $\hat Z_1 = \hat Z_2 = \ldots = \hat Z_n = P$ for some $P \in \Pi_d$.
%
%One naive approach is to ignore the existence of $O$ and directly estimate the permutation matrix.
%\begin{align*}
%   \hat Z_j = \argmax_{P \in \Pi_d} \inner{P}{U_j}\,.  \numberthis\label{eqn:naive}
%\end{align*}
%The naive approach would fail if (\ref{eqn:naive}) produces different permutations for different blocks. Intuitively, this is the case when the output of the maximization step is sensitive to the perturbation $\xi_j$. Consider $Z \neq Z' \in \Pi_d$. We have 
%\begin{align*}
%   \inner{Z}{U_j} - \inner{Z'}{U_j} &= \inner{Z}{U_j^*O^\top + \xi_j} - \inner{Z'}{U_j^*O^\top + \xi_j}\\
%   &= \inner{Z-Z'}{\frac{O^\top}{\sqrt n} + \xi_j} \\
%   &= \inner{Z-Z'}{\frac{O^\top}{\sqrt n}} + \inner{Z-Z'}{\xi_j}\numberthis\label{eqn:naive-decomp}\,.
%\end{align*}
%When $O$ is a `diffused' matrix -- $\max_{i, j}\abs{O_{ij}}$ is small -- as opposed to a `concentrated' matrix such as a permutation matrix, $\inner{Z-Z'}{\frac{O^\top}{\sqrt n}}$ is small. On the other hand, any dense perturbation matrix $\xi_j$, such as the matrix variate normal distribution, would lead to a high variance of (\ref{eqn:naive-decomp}). This means that the optimal permutation of (\ref{eqn:naive}) can vary significantly between $j=1,\ldots, n$.
%
%The vanilla spectral algorithm of \cite{pachauri2013solving} overcomes the limitation of the naive algorithm by removing the effect of $O$. We have
%\begin{align*}
%   U_jU_1^\top &= (U^*_jO^\top + \xi_j)(U^*_1O^\top + \xi_1)\\
%   &= {U_j^*(U_1^*)^\top} + \underbrace{\xi_j U^*_1O^\top + U^*_jO^\top \xi_1 + \xi_j \xi_1}_{\text{perturbation}}\,.
%\end{align*}
%However, note that there are now two additive error terms ${\xi_jO^\top}$ and ${O^\top \xi_1}$. While seemingly innocuous, the presence of one additional error term actually leads to the performance difference between the naive spectral algorithm and ours.
%
%Now, suppose that we knew $O$, then a good solution would be to estimate $\hat Z_i$ using $O$ as follows.
%\begin{align*}
%\hat Z_j = \argmax_{P\in \Pi_d} \inner{P}{U_jO}\,. \numberthis\label{eqn:known-O}
%\end{align*}
%Note that ${U_jO} = {U_j^* + \xi_j O}$. Compared to the vanilla spectral approach, the estimate here only has one additive error term. Therefore, the estimate (\ref{eqn:known-O}) should have a lower error than (\ref{eqn:old-spectral}). Of course in practice, we do not observe $O$ exactly. 
%
%However, we found the surprising fact that we can still estimate $O$ well up to a permutation using the anchor $M$ of Algorithm \ref{alg:main-algo}. In our setting where $U_j^* = \frac{I_d}{\sqrt n} \,\forall j \in [n]$, the true eigenspace $U^*$ contain $d$ types of rows $\{e_1^\top, \ldots, e_d^\top\}$ each of cardinality exactly $n$. Furthermore, the euclidean distance between any two types of rows is preserved under the rotation transformation $O^\top$. The empirical eigenspace $U$, being noisy estimates of $U^* O^\top$, thus exhibit clustering structures where the cluster centers are the transformed rows $\{e^\top_i O^\top \}_{i=1}^d$. This allows us to prove that the $d$-means clustering algorithm recovers, with high accuracy, the transformed cluster centers -- $\min_{P\in \Pi_d}\fnorm{M - PO^\top}$ is small. This means that the performance of the proposed spectral algorithm is close to that of (\ref{eqn:known-O}) and better than the vanilla approach.




\subsection{Numerical Analysis}\label{sec:numerical}
 To verify our intuitions and showcase the improved performance of our spectral method over the vanilla spectral method, we perform experiments on synthetic data following the model in (\ref{eqn:Xjk-def}). We fix $d = 2, p = 0.5, n = 2048$ and vary $\sigma$ to control the signal-to-noise ratio. Figure \ref{fig:hamming} shows the normalized Hamming loss $\ell$ against decreasing $\sigma$ for the two algorithms. The two lines are the average over 100 independent trials. One can see that our  method outperforms the vanilla spectral method with a significantly smaller error. %Furthermore, our algorithm is significantly more robust to input perturbations than the vanilla algorithm. 
 The box plot in Figure \ref{fig:box-plot} shows the distribution of the losses across 100 trials for both methods. The boxes extend from the first quartile to the third quartile of the observed losses with colored bolded lines at the medians and dotted points at the outliers. It is clear that the performance of the vanilla algorithm is highly variable across trials. On the contrary, the performance of our method is more concentrated, indicating that our method has a smaller variance and is more precise. Figure \ref{fig:err} reflects our intuition that because the vanilla algorithm repeatedly uses $U_1$ to estimate the latent permutations, its performance is highly dependent on the approximation error of a single block $U_1$. On the other hand, because our algorithm constructs an anchor that averages information across all $n$ blocks, it consistently outperforms the vanilla algorithm with a much tighter error spread.


\begin{figure}[ht]
\begin{subfigure}[t]{0.45\textwidth}
\centering
   \includegraphics[width=1\linewidth]{figs/hamming.png}
   \caption{
   %Normalized Hamming loss against $\sigma$.} 
   Average over 100 trials: our spectral algorithm outperforms the vanilla algorithm with a smaller error. 
   %The lines show the average over 100 trials.
    \label{fig:hamming}}
\end{subfigure}
% \hfill
\hspace{0.5cm}
\begin{subfigure}[t]{0.45\textwidth}
\centering
   \includegraphics[width=1\linewidth]{figs/box_plot_err.png}
   \caption{
   %{Box plot of normalized Hamming losses.} 
   Box plot: our algorithm also has a smaller variance compared to the vanilla method. \label{fig:box-plot}}
\end{subfigure}
% ~\\
% \begin{subfigure}[b]{0.45\textwidth}
%    \includegraphics[width=1\linewidth]{figs/pairwise_loss.png}
%    \caption{\textbf{$\frac{1}{n(n-1)} \fnorm{ZZ^\top - Z^*(Z^*)^\top}^2$   against $n$.} \label{fig:pairwise}}
% \end{subfigure}
% \begin{subfigure}[b]{0.6\textwidth}
%    \includegraphics[width=1\linewidth]{figs/phase_transition.png}
%    \caption{\textbf{Phase transition for exact recovery}. \duc{Thinking about adding this kind of plot similar to Figure 2 in Shuyang Ling's paper. However, need to do more experiments with larger $n$ to cleanly see the phase transition. x-axis is $n$ while y-axis is signal to noise ratio and the color denotes the frequency of exact recovery. The goal is to have two figures for vanilla (left) and ours (right) where the phase transition threshold is clearly different.}\label{fig:exact}}
% \end{subfigure}
% \hspace{0.5cm}
\caption{{Comparisons between our method and the vanilla spectra method on synthetic data.} \label{fig:err}}
\end{figure}