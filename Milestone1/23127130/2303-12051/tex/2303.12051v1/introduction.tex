% Group synchronization is a class of problems in various field where the goal is to identify $n$ objects based on pairwise measurements among them. In this work, we study an important subclass of the group synchronization  known as the permutation synchronization problem.
% with additive Gaussian noises and incomplete observations. 
% Since being popularized in \cite{pachauri2013solving}, permutation synchronization has become a fundamental problem in computer vision. The goal is to find a bijection between sets of features across multiple images. This is a key procedure in image registration \cite{shen2002hammer}, shape matching \cite{berg2005shape}, detecting structures from motion \cite{agarwal2011building}, among many other tasks in image processing. 
In permutation synchronization, the objective is to estimate latent permutations using noisy and potentially incomplete pairwise measurements among them. 
% Permutation synchronization is to estimate latent permutations from pairwise measurements among them. 
It is an important task in computer vision and graphics where finding correspondence between sets of features across multiple images is a fundamental problem with wide-ranging applications 
including image registration \cite{shen2002hammer}, shape matching \cite{berg2005shape}, multi-view matching \cite{gao2021isometric, maset2017practical}, detecting structures from motion \cite{agarwal2011building}, and so on. Various methods have been proposed for  permutation synchronization  including iterative algorithms \cite{gao2022iterative, zhou2015multi,chen2018projected}, semi-definite programming (SDP) \cite{huang2013consistent, chen2014near}, and  spectral methods \cite{pachauri2013solving,ling2022near,maset2017practical, shen2016normalized}. 
Compared to other approaches, spectral methods have gained increasing popularity and have been widely used in permutation synchronization thanks to their simplicity, fast computation speed, and impressive numerical performance. Despite the popularity, it remains unclear how well spectral methods perform theoretically and whether they achieve statistical optimality or not. In this paper, we address these questions by proposing a new and provably optimal spectral algorithm.

%SDP or iterative algorithms, the spectral algorithm is quite fast as eigendecomposition can be accomplished using highly optimized, off-the-shelf solvers. In fact, the spectral algorithm has been leveraged by previous authors in designing iterative algorithms that use the spectral estimate as initialization \cite{shen2016normalized,gao2022iterative}. However, the practical performance of \emph{the spectral algorithm alone is impressive} and the performance gain of the subsequent refinement procedure is often marginal. These empirical findings raise the following questions: Is it possible to obtain a sharp error guarantee for the spectral method and whether just the spectral method alone is optimal? The answer is yes but with a twist: we are not using the vanilla spectral algorithm of \cite{pachauri2013solving}. Our affirmative answers to these questions come with a \emph{new and minimax optimal} spectral algorithm.

%\paragraph{Problem Formulation and the Vanilla Spectral Algorithm.} 
\paragraph{Problem Formulation.}
The permutation synchronization problem is formulated as follows.
Let $Z_1^*, \ldots, Z_n^* \in \Pi_d$ where $\Pi_d$ is the permutation group in $d$ dimension defined as:
\begin{equation}\label{eqn:Pi-def}
\Pi_d := \left\{ P \in \{0,1\}^{d\times d} \,:\, P^\top P = PP^\top = I_d  \right\}\,.
\end{equation}
We introduce missing and noisy data by assuming that, for each pair $1\leq i < j \leq n$, the observation $X_{jk} \in \R^{d\times d}$ satisfies
\begin{equation}\label{eqn:Xjk-def}
X_{jk} \define \begin{cases} Z_i^*(Z_j^*)^\top + \sigma W_{jk}, &\text{if } A_{jk} = 1,\\
0_{d\times d}, &\text{otherwise},
\end{cases}
\end{equation}
where $A_{jk} \in \{0,1\}$ follows $ \text{Bernoulli}(p)$ for some $p \in \left(0, 1\right]$; $\sigma \in \R^+$ controls the amount of noise; $W_{jk} \in \R^{d\times d}$ is a random matrix with each entry following the standard normal distribution independently;  and $0_{d\times d}$ is the $d\times d$ matrix of all zeros. Roughly speaking, each $X_{jk}$ block, if not missing, is a noisy measurement of $Z_i^*(Z_j^*)^\top$, the `difference' between two permutation matrices. We assume that all random variables $\{ A_{jk} \}_{1\leq j < k \leq n}, \{W_{jk}\}_{1\leq j < k\leq n}$ are independent.  Denote $Z^*\define (Z_1^{*\top},\ldots, Z_n^{*\top})^\top\in \Pi_d^n$.  The goal is to estimate $Z^*$ given $\{ A_{jk} \}_{1\leq j < k \leq n}$ and $ \{X_{jk}\}_{1\leq j < k\leq n}$.
Note that $Z_1^*,\ldots, Z^*_n$ are identifiable only up to a global permutation. For any estimator $\hat Z =(\hat Z_1^\top, \ldots, \hat Z_j^\top)^\top\in \Pi_d^n$, its performance can be measured by the following normalized Hamming loss (modulo a global permutation transformation):
\begin{equation}\label{eqn:hamming}
\ell(\hat Z, Z^*) \define \min_{P \in \Pi_d} \,\frac{1}{n} \sum_{j=1}^n \indic{\hat Z_j \neq Z_j^* P^\top} \,.
\end{equation}
Note that the model (\ref{eqn:Xjk-def}) has a matrix representation. % In matrix notations, one can write 
The observation matrix $X \in \R^{nd\times nd}$ can be written  as 
\begin{equation}\label{eqn:X-def}
X = (A \otimes J_d) \circ Z^*(Z^*)^\top + \sigma (A\otimes J_d) \circ W,
\end{equation}
where $W \in \R^{nd\times nd}$ is a block-symmetric matrix with $W_{kj} = W_{jk}^\top, \forall 1\leq j < k \leq n$; $A \in\{0,1\}^{n\times n}$ is a symmetric matrix with $A_{kj}=A_{jk},A_{jj} = 0,\forall 1\leq j < k \leq n$; $J_d$ is the $d\times d$ matrix of all ones; $\otimes$ is the Kronecker product and $\circ$ is the Hadamard product.


\paragraph{Spectral Methods.}
Existing spectral methods \cite{pachauri2013solving,ling2022near,maset2017practical, shen2016normalized} use the eigendecomposition of $X$ followed by a rounding step to estimate the latent permutations. Let  $U=(U_1^\top,\ldots, U_n^\top)^\top \in \R^{nd\times d}$ be the matrix composing the top $d$ eigenvectors of $X$. That is, the columns of $U$ are the eigenvectors corresponding to the $d$ largest eigenvalues of $X$. Its $d\times d$ blocks are denoted as $U_1,\ldots, U_n$. The first block $U_1 \in \R^{d\times d}$ is then used as an `anchor' to obtain an estimator $\tilde Z_1, \ldots, \tilde Z_n \in \Pi_d$:
\begin{equation}\label{eqn:old-spectral}
\tilde Z_1 \define I_d, \quad \tilde Z_j \define \argmax_{P\in \Pi_d} \inner{P}{U_jU_1^\top} ,\forall j = 2,\ldots , n,
\end{equation}
where the optimization subproblem serves to round  $U_jU_1^\top$ into a permutation matrix and can be efficiently solved using the Kuhn-Munkres algorithm \cite{kuhn1955hungarian} (see Section \ref{sec:heuristic} for an intuitive description of the innerworking of this algorithm). To distinguish the existing algorithm from the one proposed in this paper, we refer to $\tilde Z:=(\tilde Z_1^\top,\ldots ,\tilde Z_n^\top)^\top$ as the `vanilla spectral estimator'. 
%The vanilla spectral method $\tilde Z$ 
It is computationally efficient and has decent numerical performance. 

Despite of all the aforementioned advantages, the vanilla spectral method suffers 
%a crucial limitation as $U_1$ is used 
from the repeated use of $U_1$ in constructing the estimator $\tilde Z_j$ for all $j\geq 2$. From a perturbation theoretical point of view, $U$ (along with its blocks $U_1,\ldots, U_n$) are approximations of their population counterparts. By using $U_jU_1^\top$, the estimation accuracy of $\tilde Z_j$ is determined by the approximation errors of both $U_j$ and $U_1$. As a result, the approximation error of $U_1$ is carried forward in $\{\tilde Z_j\}_{j\geq 2}$ and deteriorates the overall numerical performance.

To overcome this crucial limitation of $\tilde Z$, we propose a new spectral method that avoids the use of $U_1$ as the anchor. Instead, we  construct an anchor matrix $M\in\mathr^{d\times d}$ by carefully aggregating useful information from all of $U$ and estimate the latent permutations by $\hat Z \define (\hat Z_1^\top, \ldots, \hat Z_n^\top)^\top$ where
\begin{align*}
\hat Z_j \define \argmax_{P\in \Pi_d} \inner{P}{U_jM^\top} ,\forall j\in[n].
\end{align*}
The construction of $M$ is built on an intuition that  `averaging' information across all $n$ blocks of $U$  leads to a much more accurate anchor than $U_1$ because the approximation errors of many blocks tend to `cancel' one another. As a result,  the estimation accuracy of $\hat Z_j$ is largely determined by the approximation error of  $U_j$ only, which leads to an improved numerical performance. See Algorithm \ref{alg:main-algo} for the detailed implementation of the proposed method and Figure \ref{fig:err} for comparisons of numerical performances between the vanilla spectral estimator and ours.

\paragraph{Statistical Optimality.} By carrying out fine-grained spectral analysis, we establish a sharp upper bound for the theoretical performance  of the proposed method, summarized below in Theorem \ref{thm:intro}. See Theorem \ref{thm:main_new} for its non-asymptotic version.
\begin{theorem}\label{thm:intro}
Assume $\frac{np}{\sigma^2}\rightarrow\infty$, $\frac{np}{\log^3 n} \rightarrow\infty$, and $2\leq d = O(1)$. Then the proposed spectral method $\hat Z$ satisfies
\begin{align*}
\E \ell(\hat Z, Z^*) \leq \ebr{-(1-o(1)) \frac{np}{2\sigma^2}} + n^{-8}\,.
\end{align*}
\end{theorem}

The upper bound in Theorem \ref{thm:intro} consists of an exponential error term and a polynomial error term $n^{-8}$. Note that by properties of the normalized Hamming loss $\ell$, the polynomial error term is negligible. Considering this, our spectral algorithm achieves the minimax lower bound \cite{gao2022iterative} which states that if $\frac{np}{\sigma^2} \rightarrow \infty$, then $\inf_{Z'} \sup_{Z^*\in\Pi_d^n} \,\E \ell(Z', Z^*) \geq \ebr{-(1+o(1))\frac{np}{2\sigma^2}}$. This establishes the statistical optimality of the proposed method for the partial recovery of the latent permutations. Theorem \ref{thm:intro} immediately implies the threshold for exact recovery. When $np/(2 \sigma^2) >(1+\epsilon)\log n$ for some constant $\epsilon>0$, we have $\ell(\hat Z,Z^*)=0$ holds with high probability. According to the minimax lower bound, no estimator is able to recover $Z^*$ exactly with vanishing error if $np/(2 \sigma^2) <(1-\epsilon)\log n$. As a result, simple but powerful, the proposed spectral method $\hat Z$ is an optimal procedure.

Theorem \ref{thm:intro}  allows the observations $\{X_{jk}\}$ to be missing at random as long as the probability $p$ satisfies $np\gg \log^3 n$. Note that in order to have a connected comparison graph $A$,  $np$ needs to be at least of order $\log n$. Compared to this condition, our assumption $np\gg \log^3 n$ requires some additional logarithm factor. The assumption $np \gg \sigma^2$ is the necessary and sufficient condition to achieve estimation consistency according to the minimax lower bound. Theorem \ref{thm:intro}  assumes that $d$, the dimension of each permutation matrix, is a constant.
To establish  Theorem \ref{thm:intro}, we first provide a block-wise $\ell_\infty$ perturbation analysis for all block submatrices $U_1,\ldots, U_n$, quantifying  the maximum deviation between them and their population counterparts (see Theorem \ref{thm:l_infty_new}). In addition, we give a theoretical justification for the usage of the anchor $M$ by showing that it achieves a negligible error (see Proposition \ref{prop:hat_H_new}). With both results, we investigate the tail behavior of each $U_jM^\top$ and eventually obtain the upper bound in  Theorem \ref{thm:intro}. We leverage the leave-one-out technique \cite{bean2013optimal, abbe2020entrywise} in our proofs.


%To solve the permutation synchronization problem is to estimate the underlying permutation matrices $Z_1^*,\ldots, Z_n^*$. The reader may immediately recognize an inherent identification problem. That is, for any permutation matrix $P' \in \Pi_d$, the two distributions over $X$ parametrized by $\{Z^*_1(P')^\top, \ldots, Z^*_n(P')^\top\}$ and $\{Z_1^*, \ldots, Z_n^* \}$ are statistically indistinguishable. Hence, we can only hope to identify the matrices $\{Z_1^*, \ldots, Z_n^*\}$ up to a global permutation and the metric of interest is the Hamming loss (modulo an optimal permutation transformation).

%The vanilla spectral algorithm \cite{pachauri2013solving,ling2022near} performs an eigendecomposition of the observation matrix $X \approx U\Lambda U^\top$ where $U \in \R^{nd\times d}$ is the top $d$ eigenvectors of $X$. Define $U_j$ to be the $j$-th $d\times d$ block of $U$. The first block $U_1 \in \R^{d\times d}$ of $U$ is then used as the `anchor' to obtain the estimate $\tilde Z_1, \ldots, \tilde Z_n \in \Pi_d$. Specifically,
%\begin{equation}\label{eqn:old-spectral}
%\tilde Z_1 \define I_d, \quad \tilde Z_j \define \argmax_{P\in \Pi_d} \inner{P}{U_jU_1^\top} \,\,\text{for }\, j = 2,\ldots , n
%\end{equation}
%where $U_j \in \R^{d\times d}$ is the $j$-th block of $U$ and the optimization subproblem can be efficiently solved using the Kuhn-Munkres algorithm \cite{kuhn1955hungarian}. To see intuitively why this spectral algorithm works, note that $\E[X] = (\E[A] \otimes J_d) \circ Z(Z^*)^\top = p ((J_n - I_n) \otimes J_d) \circ Z(Z^*)^\top$. One could verify that $U^* \in \R^{nd\times d}$, of which the $j$-th $d\times d$ block is $U_j^* \define \frac{Z^*_j}{\sqrt n}$, is the top-$d$ eigenspaces of $\E[X]$. Since $X$ is a perturbed version of $\E[X]$, $U$ is a perturbed version of $U^*$ up to a global rotation. That is, $U \approx U^* O^\top$ for some $O \in \O_d$. We have $U_jU_1^\top \approx (U_j^*O^\top)(U_1^*O^\top)^\top \approx \frac{Z_j^*(Z_1^*)^\top}{n}$. When the amount of noise is low, the approximation is accurate and the maximization step in (\ref{eqn:old-spectral}) returns the correct permutation. Naturally, the performance of this approach degrades when we introduce missing data and a higher amount of noise in (\ref{eqn:X-def}). 




\paragraph{Related Literature.} 
%The work \cite{ling2022near} studies the exact recovery of the vanilla spectral method under a different permutation synchronization model where there is no missing data and  each observation is corrupted by a  random permutation matrix with probability $q$. It shows that the vanilla spectral method achieves the exact recovery when $q$ satisfies certain condition. As a comparison, our analysis allows the signal-to-noise ratio to be small when the exact recovery is impossible and only the partial recover is possible. We further establish the statistical optimality of our method in the presence of missing data and Gaussian noise.
%As a comparison, our model allows missing data and Gaussian noise. More importantly, we 

Permutation synchronization belongs to a broader class of group synchronization problems where the goal is to identify $n$ group objects  based on pairwise measurements among them. In recent years, spectral methods have been widely used and studied   in group synchronization problems. In \cite{zhang2022exact}, spectral methods are proved to be optimal for phase synchronization and orthogonal group synchronization in terms of squared $\ell_2$ losses. To obtain this result,  \cite{zhang2022exact} develops perturbation analysis toolkits to show that the leading eigenstructures can be well-approximated by its first-order approximation with a small $\ell_2$ error. However, the difference that permutations are discrete-valued while phases and orthogonal matrices are continuous is critical. For permutation synchronization, instead of $\ell_2$ perturbation analysis, we need to develop block-wise analysis in order to obtain sharp exponential rates. \cite{abbe2020entrywise} considers a $\mathbb{Z}_2$ synchronization problem where each object is $\pm 1$ and assumes that there is no missing data. It proves that a simple spectral procedure using signs of coordinates of the first eigenvector of the data matrix achieves the optimal threshold for the exact recovery of objects by $\ell_\infty$ analysis of the leading eigenvector. \cite{ling2022near} extends \cite{abbe2020entrywise}'s $\ell_\infty$ analysis to a permutation synchronization setting where there is no missing data and  each observation is corrupted with probability $q$ by a random permutation matrix. It shows that the vanilla spectral method achieves exact recovery  when $q$ satisfies certain conditions.  Our $\ell_\infty$ analysis is different from those  in \cite{abbe2020entrywise, ling2022near}  as we need to consider the low signal-to-noise ratio  regime where  exact recovery is impossible but partial recovery is possible. We go beyond $\ell_\infty$ analysis and  study the tail behavior of each block of $U$ in order to obtain exponential error bounds for partial recovery. In addition, the presence of missing data in our model complicates the theoretical analysis as the magnitude and tail behavior of each block $U_j$ is not only related to the additive Gaussian noises but also the randomness of the Bernoulli random variables $\{A_{jk}\}_{k\neq j}$.

% For permutation synchronization, since the leading eigenspace $U$ is used, we need to carry out a block-wise $\ell_\infty$ analysis for $U$.  Our $\ell_\infty$ analysis is also different from that in \cite{abbe2020entrywise}  as we allow the signal-to-noise ratio to be small such that the exact recovery is impossible but partial recovery is possible. 

%Built upon the block-wise $\ell_\infty$ analysis, we further go beyond study tail behaviors for each block of $U$



%spectral methods need to use  with a more complicated rounding steps.  As a result, 


 
%\paragraph{Related Literature.} The permutation synchronization problem is a subclass of the phase synchronization problem. Other subclasses of the phase synchronization problem and their applications include, but are not limited to, $\Z_2$ synchronization in community detection \cite{abbe2014decoding,bandeira2016low}; $\Z_k$ synchronization in joint alignment \cite{chen2018projected}; orthogonal group synchronization in angular estimation \cite{arie2012global}. There are a variety of proposed algorithms for solving the phase synchronization problem including maximum likelihood estimate (MLE) \cite{boumal2013robust,gao2021exact,zhong2018near}, generalized power method (GPM) \cite{gao2021optimal,ling2022improved,perry2018message,liu2017estimation,shen2016normalized,gao2022iterative}, semi-definite programming (SDP) \cite{arie2012global,fan2021joint,gao2022sdp,ling2022solving,javanmard2016phase,singer2011three,wang2013exact} and spectral methods \cite{arie2012global,romanov2020noise,singer2011three}. 
%
%For permutation synchronization, it has been shown that an instance of Loyld's iterative minimization algorithm achieves the minimax optimal rate with respect to the Hamming loss \cite{gao2022iterative}. Recently, \cite{ling2022near} studied the \emph{exact recovery guarantee} of the vanilla spectral algorithm for permutation synchronization and showed that its error guarantee is near-optimal up to a logarithmic and a constant factor. As mentioned briefly, the spectral method is often used as an initialization routine to more complicated procedures based on GPM or Loyld's algorithm. However, in practice, the performance of the spectral method is already good and the performance gain from GPM or Loyld's algorithm is often marginal \cite{zhang2022exact}. Our work fills an important gap in the literature by showing that the spectral estimates alone are in fact optimal up to the correct constant factor. 
%
%Our paper also contributes to the theoretical analysis of the spectral algorithm. Fine-grained analysis of eigenspace perturbation has received substantial attention in recent years. For example, $\ell_{\infty}$ analysis of the leading eigenvector and $\ell_{2,\infty}$ analysis of the leading eigenspace allow statisticians to obtain improved guarantees in a variety of problems such as community detection \cite{zhang2023fundamental}, phase synchronization \cite{zhang2022exact} and joint alignment \cite{chen2018projected}. In the permutation synchronization problem, the generalization of the $\ell_{\infty}$ analysis is the blockwise error analysis, i.e., bounding $\min_{O\in \O_d} \max_{j\in [n]}\opnorm{U_j - U_j^* O}$ \cite{ling2022near,zhang2022exact}. However, to obtain the exponentially decaying error guarantee in Theorem \ref{thm:intro}, we require more than just a high probability blockwise deviation bound which incurs an additional logarithmic factor. We go further than previous works by furnishing a tail bound on the blockwise deviation.

% \paragraph{Organization.} We describe the new spectral algorithm in Section \ref{sect:algo}. In Section \ref{sect:analysis} we will present the theoretical analyses of the spectral algorithm. \duc{Finish after full proofs}

\paragraph{Notation.} For any positive integer $n$, define $[n] := \{1,2,\ldots, n\}$. Let $I_d$ denote the $d\times d$ identity matrix and $J_d$ denote the $d\times d$ matrix of all ones. Define $\mathcal{O}_d:=\{O\in\mathr^{d\times d}:OO^\top=O^\top O=I_d\}$ to be the set of all $d\times d$ orthogonal matrices.
Given $a, b\in \R$, let $a\lor b \define \max\{a, b\}$ and $a\land b \define \min\{a, b\}$. For a matrix $B \in \R^{d_1\times d_2}$, the Frobenius norm and the operator norm of $B$ are defined as $\fnorm{B} \define \left(\sum_{i=1}^{d_1}\sum_{j=1}^{d_2} B_{ij}^2 \right)^{1/2} $ and $\opnorm{B} \define \max_{u \in \S_{d_1}, v\in \S_{d_2}} u^\top B v $ where $\S_d \define \{v \in \R^d \,:\, \norm{v} = 1\}$ is the unit sphere in $d$ dimension and $\norm{.}$ is the Euclidean norm. For two matrices $A, B \in \R^{d_1\times d_2}$, let $\inner{A}{B} := \sum_{i\in [d_1]}\sum_{j\in [d_2]} A_{ij}B_{ij}$ denote the matrix inner product. For some $d_1, d_2 \in \mathbb{N}$, we use $\calN(\mu, \Sigma)$ to denote the normal distribution with mean $\mu \in \R^{d_1}$ and covariance $\Sigma \in \R^{d_1 \times d_1}$ and $\MN(S, \Sigma_1, \Sigma_2)$ to denote the matrix normal distribution \cite{de2004matrix} with mean parameter $S\in \R^{d_1\times d_1}$ and covariance parameters $\Sigma_1 \in \R^{d_1\times d_1}$, $\Sigma_2 \in \R^{d_2\times d_2}$. Define $(a)_+ \define  \max\{a, 0\}$. For the rest of the paper, we will use $e_i \in \{0,1\}^d$ to denote the vector with $1$ in the $i$-th entry and zero everywhere else. For any matrix $Y$ and any positive integer $i$, we denote $\lambda_i(Y)$ to be the $i$th largest eigenvalue of $Y$.
%Unless specified otherwise, we use $\Lambda$ to denote the $d\times d$ diagonal matrix whose entries are the top eigenvalues of $X$. 
For any positive sequences $a_n,b_n$, we use $a_n\lesssim b_n$ and $b_n\gtrsim a_n$ if $a_n\leq C b_n$ for some constant $C>0$.
Lastly, we use $\indic{.}$ to denote the indicator function.
