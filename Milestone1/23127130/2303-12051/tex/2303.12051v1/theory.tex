In this section, we  establish theoretical guarantees for Algorithm \ref{alg:main-algo}.  Without loss of generality, starting from this part of the paper, we let $Z^*_j = I_d$ for all $j\in[n]$. That is, all the latent permutations are the identity matrix. In this way, the population eigenspace $U^*$ has a simpler expression with
\begin{align}\label{eqn:U_star_simplified}
U^*_j =I_d/\sqrt{n},\forall j\in[n].
\end{align}
To establish the statistical optimality of the proposed method, we first give a justification for the choice of the anchor $M$, followed by a fine-grained perturbation analysis for $U$.



\subsection{Theoretical Justification of $M$}
We present a justification for the  anchor $M$. Note that the empirical eigenspace $U$ and the population eigenspace $U^*$ are close modulo a suitable transformation. Readers who are familiar with the literature on spectral methods \cite{abbe2020entrywise} may recognize that a natural choice of transformation is
\begin{equation}
H \define U^\top U^*\,.
\end{equation}
Classical perturbation theory such as Davis-Kahan theorem can be applied to quantify the distance between $UH$ and $U^*$ (see (\ref{eqn:5}) of Lemma \ref{lem:event-E-prime_new} for its proof):
\begin{align*}
\opnorm{UH-U^*} \lesssim   \frac{{1+\sigma\sqrt{d}}}{\sqrt{np}}.
\end{align*}
Though $H$ is not orthonormal, 
it can be shown to be close to some orthogonal matrix (see Lemma \ref{lem:event-E-prime_new}). 
Then following the intuition given in Section \ref{sec:heuristic} and applying state-of-the-art clustering analysis, we have the following proposition.
\begin{proposition}\label{prop:hat_H_new}
There exist constants $C_1,C_2,C_3>0$ such that if $\frac{np}{\log n}>C_1$ and $\frac{np}{(\sqrt{d} + \sigma d)^2}>C_2$ then the anchor $M$ constructed in Algorithm \ref{alg:main-algo} satisfies
  \begin{align}\label{eqn:21}
    \min_{P\in\Pi_d}\fnorm{\hatH - PH^\top}\leq  \frac{C_3(\sqrt{d} + \sigma d)}{\sqrt{np}},
\end{align}
with probability at least $1- n^{-10}$.
\end{proposition}

Proposition \ref{prop:hat_H_new} shows that $ M$ is an approximation of $H^\top$ up to some permutation matrix $\hat P\define \argmin_{P\in\Pi_d} \|\hatH - PH^\top\|_{\rm F}$. The existence of $\hat P$ is due to the fact that the clusters are  identifiable only up to a permutation in cluster analysis. (\ref{eqn:21}) shows the estimation error goes to 0 when $\frac{np}{(\sqrt{d}+\sigma d)^2}$ grows. As a comparison, the operator norm of the matrix $H$ is `of a constant order' as it is close to an orthogonal matrix. Hence, the error incurred by $M$ as an estimate of $H^\top$ is of diminishing proportion. It is also worth mentioning that Proposition \ref{prop:hat_H_new} holds under weaker assumptions compared to Theorem \ref{thm:intro}. It only requires $np \gtrsim \log n$ and allows $d$ to grow as long as $np \gtrsim (\sqrt{d} + \sigma d)^2$. With Proposition \ref{prop:hat_H_new}, we can have a decomposition of the quantity $U_jM^\top$, the key quantity in the proposed method (\ref{eqn:alg2}):
\begin{align*}
U_jM^\top -U_j^* \hat P^\top &= U_j(\hat PH^\top + M - \hat PH^\top)^\top -U_j^* \hat P^\top \\
&= \br{U_j H - U_j^*}\hat P^\top + U_j(M - \hat PH^\top)^\top\\
&\approx  \br{U_j H - U_j^*}\hat P^\top. \numberthis \label{eqn:approx1}
\end{align*}
The approximation (\ref{eqn:approx1}) is due to (\ref{eqn:21}) as the term above $ U_j(M - \hat PH^\top)^\top$ turns out to be negligible. Hence, the difference $U_jM^\top -U_j^* \hat P^\top $ is primarily about the perturbation $U_j H - U_j^*$. To analyze our spectral method, we need to have a deep understanding on the behaviors of  the blockwise perturbation $U_j H - U_j^*$.
%To  its behavior, we need to have a  uniformly 
%$U_jM^\top$, the key quantity in the proposed method (\ref{eqn:alg2}), and its deviation from $U^*_j$ up to a global permutation $\hat P$, we need to 


%\subsection{Block-wise $\ell_\infty$ Perturbation Analysis}

%Our first contribution is a $\ell_{\infty}$ type bound for the blockwise deviation between the empirical eigenspaces $U$ and $U^*$ modulo a suitable transformation. Readers who are familiar with the literature on spectral methods may recognize that the optimal \emph{rotational transformation} from $U$ to $U^*$ is theoretically well defined. Let
%\begin{equation}
%H \define U^\top U^*\,.
%\end{equation}
%The matrix sign function \cite{gross2011recovering} $\sign{H} \define QV^\top$ where $QSV^\top$ is the singular value decomposition of $H$ satisfies $\sign{H} = \argmin_{O \in \O_d} \fnorm{UO - U^*}$.
%To simplify the overall analysis, we focus on bounding $\opnorm{UH - U^*}$ instead of $\opnorm{U\sign{H} - U^*}$. However, one can show that $\opnorm{H -\sign{H}}$ is small and a bound on $\opnorm{U\sign{H} - U^*}$ can be easily derived from Theorem \ref{thm:l_infty_new}.

\subsection{Statistical Optimality}\label{sec:optimality}

In Theorem \ref{thm:l_infty_new}, we first derive a block-wise $\ell_\infty$ upper bound for $UH-U^*$, i.e.,  an upper bound for $\opnormt{U_j H - U^*_j}$ that holds uniformly across all $j\in[n]$.

\begin{theorem}\label{thm:l_infty_new}
There exist constants $C,C',C''>0$ such that if $\frac{np}{(\sigma^{4/3}  \vee 1)\log n} >C$ and $\frac{np}{(\sqrt{d} + \sigma d)^2}\geq C'$, we have
\begin{align}\label{eqn:22}
\max_{j\in[n]} \opnorm{U_j H - U^*_j}\leq \frac{ C''}{\sqrt{n}}\br{\sqrt{\frac{\log n }{np}} + \frac{\sigma \sqrt{d}}{\sqrt{np}} + \frac{\sigma \sqrt{\log n}}{\sqrt{np}}}
\end{align}
with probability at least $1-6dn^{-9}$.
\end{theorem}

The upper bound in (\ref{eqn:22}) is equal to the upper bound on $\opnorm{UH - U^*}$ 
%established in \cite{zhang2022exact} 
(see Equation (\ref{eqn:5})) multiplied by a $\sqrt{(\log n)/n}$ factor. This is because $UH-U^*$ consists of $n$ blocks $\{U_jH-U^*\}_{j\in[n]}$  and all blocks behave similarly.  The magnitude of each block is  then on average $1/\sqrt{n}$ of that of the whole matrix, and the $\sqrt{\log n}$ factor is due to the use of union bound to control the supreme. Theorem \ref{thm:l_infty_new} assumes $\frac{np}{(\sigma^{4/3}  \vee 1)\log n} \gtrsim 1$. A sufficient condition is ${np}/{\log^3 n}\gtrsim 1$ as $np/\sigma^2\gtrsim 1$ is implied by the other assumption  ${np}/{(\sqrt{d} + \sigma d)^2}\gtrsim 1$.

% Note that $U^*$ is not observed and neither is $H$. The vanilla spectral algorithm uses the fact that $U_j \approx U_j^* \sign{H}^\top$ and $
% U_jU_1^\top \approx U_j^*\sign{H}^\top \sign{H} (U_1^*)^\top = U_j^*(U_1^*)^\top = \frac{1}{n} Z_j^*(Z_1^*)^\top$ to recover the permutations without explicit knowledge of $H$. 
%As hinted at before, we can estimate $H$ well \emph{up to a global permutation}. Since $U_j^* = \frac{Z_j^*}{\sqrt n} \,\forall j \in [n]$, the true eigenspaces $U^*$ contain $d$ types of rows $\{e_1^\top, \ldots e_d^\top\}$ each of cardinality exactly $n$. The euclidean distance between any two types of rows is largely preserved under a linear transformation $H^\top$. The empirical eigenspaces $U$, being noisy estimates of $U^* H^\top$, thus exhibit clustering structures where the cluster centers are close, modulo a permutation of the labels, to the transformed rows $\{e^\top_i H^\top\}_{i=1}^d$.
%
%Define $\hat P \define \argmin_{P\in \Pi_d} \, \fnorm{\hatH - P H^\top}$. As noted before, the key to the improved performance of our spectral algorithm is the construction of the anchor $M$ in Algorithm \ref{alg:main-algo}. 
%One can see that the bound on $\fnorm{M - \hat P H^\top}$ is smaller than that on $\fnorm{U_1 H - U_1^*}$ by a $O(\sqrt{\log n})$ factor. This crucial difference allows us to overcome the limitation of the previous works which analyze the vanilla algorithm and obtain a sharp bound on the Hamming loss.

%\subsection{Statistical Optimality}

With the upper bound for each $\opnormt{U_j H - U^*_j}$ derived, we further study the tail behavior of each difference $U_j H - U^*_j$. This leads to a sharp theoretical analysis of the performance of the proposed spectral method. The main theoretical result of this paper is stated below in Theorem \ref{thm:main_new}.
\begin{theorem}\label{thm:main_new}
Assume $2\leq d\leq \bar C$ for some constant $\bar C>0$.
There exist constants $\bar C',\bar C'',\bar C'''>0$ such that if $\frac{np}{\log^3 n}>\bar C'$ , $\frac{np}{\sigma^2}>\bar C''$ then the estimates $\hat Z_1,\ldots, \hat Z_n$ of Algorithm \ref{alg:main-algo} satisfy
\begin{align*}
\E \ell(\hat Z,Z^*)\leq \ebr{-\br{1-\bar C'''\br{\br{\frac{\sigma^2}{np}}^\frac{1}{4} + \br{\frac{1}{\log n}}^\frac{1}{4} }}\frac{np}{2\sigma^2}}  + n^{-8}.
\end{align*}
\end{theorem}

Theorem \ref{thm:main_new} is a non-asymptotic version of Theorem \ref{thm:intro} stated in the Introduction. By letting $np/\sigma^2$ go to infinity, the exponential term in Theorem \ref{thm:main_new} takes an asymptotic form of $\ebr{-(1-o(1))\frac{np}{2\sigma^2}}$ and matches with the minimax lower bound. In this way, we establish the statistical optimality of the proposed spectral method.

While the proof of Theorem \ref{thm:main_new} is complicated, one can still develop an intuition as to why we can achieve the error bound $\ebr{-(1-o(1))\frac{np}{2\sigma^2}}$. %, particularly with the sharp constant $\frac{1}{2}$ in the exponent. 
At a high level, we will prove and use a variant of the linear approximation \cite{abbe2020entrywise} $UH - U^* \approx XU^*\Lambda^{-1} - U^*$ where $\Lambda$ is the diagonal matrix of the leading eigenvalues of $X$ (see (\ref{eqn:decomposition1})). A further decomposition using the structure of $X$ reveals that 
\begin{align}\label{eqn:approx2}
U_jH-U_j^*\approx  \frac{\sigma}{\sqrt{n}} \,\sum_{k\neq j} A_{jk} W_{jk} \Lambda^{-1}  \approx  \frac{1}{\sqrt{n}} \frac{\sigma}{np} \sum_{k\neq j}A_{jk}W_{jk}.
\end{align}
Recall that $U_j^*= I_d/\sqrt{n}$ according to (\ref{eqn:U_star_simplified}). Then $\sqrt{n }U_jH - I_d \approx \frac{\sigma}{np} \sum_{k\neq j}A_{jk}W_{jk}$ which follows a Gaussian distribution conditioned on $\{A_{jk}\}_{k\neq j}$. Since $\sum_{k\neq j}A_{jk}$ concentrates around $(n-1)p$, roughly speaking, $\sqrt{n }U_jH - I_d$ is a random matrix with each entry i.i.d. following $\mathcal{N}(0,\frac{\sigma^2}{np})$. The Gaussian tail is used to characterize the probability of the event when $\sqrt{n }U_jH$ considerably deviates away from $I_d$ and eventually leads to a probability bound of $\ebr{-(1-o(1))\frac{np}{2\sigma^2}}$.


\subsection{Block-wise Decomposition of $UH-U^*$}
In this section, we provide a block-wise decomposition of $UH-U^*$. The decomposition is the key towards the block-wise analysis in Section \ref{sec:optimality} and provides insights on how Theorem \ref{thm:l_infty_new} and Theorem \ref{thm:main_new} are established. Recall that we let $Z^*_j = I_d$ for all $j\in[n]$. Then, (\ref{eqn:X-def}) becomes
\begin{align*}
X = (A \otimes I_d) + \sigma (A \otimes J_d) \circ W.
\end{align*}
For each $j\in[n]$, define $X_j\define (X_{j1},\ldots, X_{jn})\in\mathr^{d\times nd}$ to be the $j$-th block row of $X$ and define $W_j \in \mathr^{d\times nd}$ analogously for $W$. Then we have
\begin{align}\label{eqn:1}
X_j = (A_j \otimes I_d) + \sigma (A_j \otimes J_d) \circ W_j.
\end{align}
As a result, $X =(X_1^\top,\ldots, X_n^\top)^\top$.  Define $\Lambda\in\mathr^{d\times d}$ to be the diagonal matrix of the leading eigenvalues of $X$. That is,
%\begin{align}\label{eqn:Lambda_def}
%\Lambda_{ii}\define \lambda_i(X) \text{ and }\Lambda_{ik}\define 0, \forall 1\leq i\neq k\leq d.
%\end{align}
%% 
$\Lambda_{ii}\define \lambda_i(X)$ and $\Lambda_{ik}\define 0$ for all $1\leq i\neq k\leq d$.
Then we have
\begin{align*}
UH\Lambda - XU^*  & = U(H\Lambda - \Lambda H) + U\Lambda H -XU^*\\
& = U(H\Lambda - \Lambda H) + XU H -XU^*\\
&= U(H\Lambda - \Lambda H) + X(U H - U^*).
\end{align*}
Multiplying both sides by $\Lambda^{-1}$ and rearranging the terms, we have
\begin{align}\label{eqn:decomposition1}
UH - U^* = U(H\Lambda - \Lambda H) \Lambda^{-1}  + X(U H - U^*) \Lambda^{-1} + XU^*\Lambda^{-1} - U^* .
\end{align}
The above display involves $ X(U H - U^*) $ where $X$ and $UH-U^*$ are dependent on each other. To decouple the dependence, we approximate $UH-U^*$ by its leave-one-out counterparts.
Consider any $j\in[n]$. Define $X^{(j)}\in\mathr^{nd\times nd}$ such that
\begin{align*}
X^{(j)}_{ik} \define\quad \begin{cases}
X_{ik}, &\forall i,k\neq j,\\
0_{d\times d},  &\text{otherwise.}
\end{cases}
\end{align*}
In addition, let $U^{(j)}\in\mathr^{nd\times d}$ be the matrix including the leading $d$ eigenvectors of $X^{(j)}$. As a consequence, $X^{(j)}, U^{(j)}$ are independent of $\{A_{jk}\}_{k\neq j}$ and $\{W_{jk}\}_{k\neq j}$. 
Define
$
H^{(j)}\define U^{(j)\top}U^*\in\mathr^{d\times d}.
$
Then
\begin{align*}
UH-U^* &= UH - U^{(j)}H^{(j)} + U^{(j)}H^{(j)} - U^*  = (UU^\top - U^{(j)}U^{(j)\top})U^*+ U^{(j)}H^{(j)} - U^* .
\end{align*}
After plugging it into the right-hand side of (\ref{eqn:decomposition1}), we have
\begin{align*}
UH - U^* & = U(H\Lambda - \Lambda H) \Lambda^{-1}  + X(UU^\top - U^{(j)}U^{(j)\top})U^* \Lambda^{-1} \\
&\quad  +  X(U^{(j)}H^{(j)} - U^*) \Lambda^{-1}  + XU^*\Lambda^{-1} - U^*.
\end{align*}
Then, the $j$th block matrix of $UH - U^*$ satisfies
\begin{align}
U_jH - U^*_j & = \underbrace{U_j(H\Lambda - \Lambda H) \Lambda^{-1}  + X_j(UU^\top - U^{(j)}U^{(j)\top})U^* \Lambda^{-1}}_{\definei B_j} \nonumber\\
&\quad  +  X_j(U^{(j)}H^{(j)} - U^*) \Lambda^{-1}  + X_jU^*\Lambda^{-1} - U^*_j.\label{eqn:decomposition2}
\end{align}
The last two terms in (\ref{eqn:decomposition2}) can be further decomposed. 
Using (\ref{eqn:1}), we have
\begin{align*}
X_j(U^{(j)}H^{(j)} - U^*) \Lambda^{-1}&= \br{(A_j \otimes I_d) + \sigma (A_j \otimes J_d) \circ W_j}(U^{(j)}H^{(j)} - U^*) \Lambda^{-1}\\
&=  \underbrace{\sum_{k\neq j} A_{jk} (U^{(j)}_kH^{(j)} - U^*_k)\Lambda^{-1}}_{\definei F_{j1}}  +   \underbrace{\sigma \sum_{k\neq j} A_{jk} W_{jk}(U^{(j)}_kH^{(j)} - U^*_k)\Lambda^{-1}}_{\definei F_{j2}}  ,
\end{align*}
and
\begin{align*}
X_jU^*\Lambda^{-1} - U^*_j &= \br{(A_j \otimes I_d) + \sigma (A_j \otimes J_d) \circ W_j}U^*\Lambda^{-1} - U^*_j \\
&= \left(\sum_{k\neq j} A_{jk} U^*_k\right)\Lambda^{-1}  - U_j^*+ \sigma \,\sum_{k\neq j} A_{jk} W_{jk} U^*_k\Lambda^{-1} \\
&= \underbrace{\frac{1}{\sqrt{n}}\br{\left(\sum_{k\neq j} A_{jk} \right)\Lambda^{-1}  - I_d}}_{\definei G_{j1}} + \underbrace{ \frac{\sigma}{\sqrt{n}} \,\sum_{k\neq j} A_{jk} W_{jk} \Lambda^{-1} }_{\definei G_{j2}} ,
\end{align*}
where the last equation is due to Lemma \ref{lem:population}.
As a result, we have a decomposition of $U_jH - U^*_j$
\begin{align}\label{eqn:decomposition3}
U_jH - U^*_j & = 
B_j  + F_{j1} + F_{j2} + G_{j1} + G_{j2},
\end{align}
holds for all $j\in[n]$.

The block-wise decomposition (\ref{eqn:decomposition3}) of $UH-U^*$ is the starting point to establishing both Theorem \ref{thm:l_infty_new} and Theorem \ref{thm:main_new}. 
Note that we have a mutual independence among $\{A_{jk}\}_{k\neq i}$, $\{W_{jk}\}_{k\neq i}$, and $U^{(j)}H^{(j)} -U^*$ in the definitions of $F_{j1}$ and $F_{j2}$, which is crucial to obtaining sharp bounds and tail probabilities for them. Theorem \ref{thm:l_infty_new} is proved by establishing upper bounds for the operator norm of $B_j, F_{j1}, F_{j2}, G_{j1} $, and $G_{j2}$. By further analyzing their tail bounds, we establish Theorem \ref{thm:main_new}. Among these terms, $G_{j2}$ is the one contributing to the exponential error bound in Theorem \ref{thm:main_new}, as we illustrate in (\ref{eqn:approx2}). 


%For any other permutation matrix $R\in \Pi_d$ that is different from $$

%. We have $UM^\top - U^*\hat P^\top \approx (UH - U^*)\hat P^\top \approx \left( XU^*\Lambda^{-1} - U^*  \right)\hat P^\top $. For each block $j\in [n]$,
%\begin{align*}
%  U_jM^\top - U_j^*\hat P^\top &\approx (U_jH - U_j^*)\hat P^\top \approx (X_j U^*\Lambda^{-1} - U_j^*)\hat P^\top \\
%  &= \left(\sum_{k\neq j} A_{jk}U_k^*\Lambda^{-1} - U_j^*\right)\hat P^\top + \left(\sum_{k\neq j}\sigma A_{jk} W_{jk} U_k^*\Lambda^{-1} \right) \hat P^\top \numberthis\label{eqn:lin-approx} \,.
%\end{align*}
%We see that both terms in (\ref{eqn:lin-approx}) contribute to the errors of the approximation $U_jM^\top \approx U_j^*\hat P^\top$. However, we will show that the deviation associated with the first term is small while the second term is dominant and behaves like a sum of Gaussian random variables. Due to the maximization problem (\ref{eqn:alg2}) in Algorithm \ref{alg:main-algo}, the performance of Algorithm \ref{alg:main-algo} crucially depends on the tail property of the second term. Bounding this second term essentially gives us the desired rate. Roughly speaking, we will show the following chain of argument.
%\begin{align*}
%&\p\left( Z_j^*\hat P^\top \neq \argmax_{P\in \Pi_d} \inner{P}{U_jM^\top}   \right) = \p\left( \exists R \in \Pi_d \backslash \{ Z_j^* \} \,:\, \inner{U_jM^\top}{Z_j^*\hat P^\top  - R\hat P^\top} < 0   \right)\\
%&=  \p\left( \exists R \in \Pi_d \backslash \{ Z_j^* \} \,:\, \inner{U_jM^\top - \frac{Z_j^*\hat P^\top}{\sqrt n}}{Z_j^*\hat P^\top  - R\hat P^\top} < -\frac{\fnorm{Z_j^* - R}^2}{2\sqrt n}   \right)  \\
%&\approx \p\left(\exists R \in \Pi_d \backslash \{ Z_j^* \} \,:\, \inner{\sum_{k\neq j}\sigma A_{jk} W_{jk} U_k^*\Lambda^{-1} }{Z_j^* - R} <- \frac{\fnorm{Z_j^* - R}^2}{2\sqrt n}   \right) \numberthis\label{eqn:rough-lin} \\
%&\approx \p\left( \exists R \in \Pi_d \backslash \{ Z_j^* \} \,:\,\frac{\fnorm{Z_j^* - R}^2}{2\sqrt n} \cdot \left(\sum_{k\neq j} \frac{\sigma}{np} A_{jk} w_{jk} \right) <- \frac{\fnorm{Z_j^* - R}^2}{2\sqrt n}    \right) \numberthis\label{eqn:rough-1} \\
%&\approx \p\left( \calN\left(0, \frac{\sum_{k\neq j} A_{jk}}{np} \, \frac{\sigma^2}{np}\right) < -1 \right) = \ebr{-(1-o(1))\frac{np}{2\sigma^2}} \numberthis\label{eqn:rough-2}\,,
%\end{align*}
%where $w_{jk} \iid \calN(0, 1)$ captures the Gaussian like behaviour of the second term of (\ref{eqn:lin-approx}). To derive (\ref{eqn:rough-lin}), we use the approximation (\ref{eqn:lin-approx}). To derive (\ref{eqn:rough-1}), we prove $\opnorm{\Lambda^{-1}} \approx \frac{1}{np}$ and $\opnorm{U_j^*} = \frac{1}{\sqrt n}$. To derive (\ref{eqn:rough-2}), we will prove $\sum_{k\neq j} A_{jk} \approx np$ and apply union bounding over all $R \in \Pi_d \backslash \{Z_j^*\}$.
