\section{Introduction}
 The Uniform Information Density (UID) hypothesis states that humans distribute information in their utterances evenly for optimal communication \citep{jaeger2010redundancy, fenk_uid}. Consequently, language generation has benefitted from UID-based objectives and regularization \citep{meister2022typical, regularizer}. Specifically, \citet{meister-etal-2020-beam} argued that UID can be optimized for machine translation using beam search.  Yet, the effect of different decoding algorithms on information density distributions of generated text are unknown, as is UID's broader role in neural response generation in the special case of dialogue models. Here, we investigate (i) if different decoding algorithms follow the UID principle, and (ii) if following the UID principle is beneficial for dialogue response generation, and (iii) collect human annotations of qualitative measures for multiple candidate responses to dialog histories generated using different decoding algorithms (Figure \ref{fig:dataset_teaser}) to study the relationship of dialog response quality and UID. We operationalize UID as the variance of surprisal and measure its correlation with automatic metrics (e.g., BLEU, METEOR, BERTScore) as well as human judgments on qualitative measures of response quality and find that adherence to UID correlates negatively with human judgments when the responses have very low/high surprisal.

% \begin{figure}[t!]
%     \includegraphics[width=0.5\textwidth]{human_vs_gpt2.png}
%     \caption{Information density distribution of human and GPT-2 generated responses for the Persona-Chat dataset.}
%     \label{fig:human_gpt2}
% \end{figure}
\begin{figure}[t!]
    \includegraphics[width=0.48\textwidth]{teaser.pdf}
    \caption{Our dataset contains 4 candidate responses for every dialog history, along with human annotations for 3 qualitative measures.}
    \label{fig:dataset_teaser}
\end{figure}

 \begin{figure*}[ht]
 \centering
  \includegraphics[width=\textwidth]{uids.png}
  \caption{Histogram of \textbf{UID Scores} of responses generated using different decoding algorithms. The farther the UID score from $0$, the less uniform or more non-uniform the response. Human-generated reference text (left-top) has a higher frequency of non-uniform responses as compared to any model setting as can be seen from the wider spread of scores away from $0$. Also, as the values of \textit{p} and \textit{k} increase \textit{(left to right)}, the information density distribution slowly approaches reference text-like non-uniformity.}
  \label{fig:uid}
\end{figure*}

\begin{figure*}[t!]
    \includegraphics[width=\textwidth]{sent_example1.png}
    \caption{Surprisal at every token in candidate responses to the same dialog history, color-coded with human annotated \textbf{interesting} scores. Plots (\textit{left to right}) are arranged in increasing order of uniformity (i.e. variance along y-axis). Less uniform the surprisal (left-most), better the score.}
    \label{fig:sent_example}
\end{figure*}

\paragraph{Language production in humans.}Spreading information content evenly in utterances is a marker of optimally strategized responses, and humans follow this UID principle as a means to state their thoughts clearly and to make themselves intelligible \citep{frank2008speaking, levy2007speakers}. The probability of a sentence has been associated with the cognitive load it incurs \cite{hale2003information}. As a means to avoid salient variations in the information content (surprisal, i.e., negative log probability) of responses, speakers maintain UID through linguistic choices such as that at the  phonetic \citep{aylett2004smooth}, syntactic \citep{jaeger2010redundancy} and lexical level \citep{mahowald2013info}.

\paragraph{Response generation in machines.} While large-scale pre-trained language models provide a rich prior for dialogue response generation, the choice of decoding algorithm used at the time of generation is crucial for the quality of generated responses \citep{Holtzman2020The, zhang-etal-2021-trading, nadeem-etal-2020-systematic, golovanov-etal-2019-large, oluwatobi-mueller-2020-dlgnet}. While vanilla sampling often tends to produce incoherent text, greedy decoding leads to safe and repetitive responses. More recently, top-\textit{p}/nucleus \citep{Holtzman2020The} and top-\textit{k} sampling \citep{fan-etal-2018-hierarchical} are used to tune values of \textit{p/k} to balance the diversity-quality trade-off \citep{zhang-etal-2021-trading, mutual_info_objective}.

\paragraph{The UID principle and decoding algorithms.} Both the UID principle and decoding algorithms can be seen as guiding mechanisms for dialogue response production in humans and generation in  machines, respectively. UID's role in machine-generated dialogue is not well understood, with previous work mainly focused on machine translation and language modeling \citep{regularizer, meister2021revisiting, meister-etal-2020-beam}. To address this gap, we present a comparative study of decoding methods to develop a deeper understanding of the role of UID in dialogue response generation. 