\section{Experimental Details}
\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}[width=\textwidth]{c|cccccccc}
& \multicolumn{8}{c}{\textbf{Pearson's \textit{r} between UID score and automatic metrics}} \\
\textbf{Generation Type} & \textbf{Length} & \textbf{BLEU}  & \textbf{chrF} & \textbf{METEOR} & \textbf{BertScore} & \textbf{BLEURT} & \textbf{RoBERTa} & \textbf{SacreBLEU}\\
\hline
\textit{p} = 0.3         & -.10   & .00 & .14          & .12  & .17      & .17   & 0.19                                                            & .13      \\
\textit{p} = 0.5         & -.05 & .03  & .13       & .10 & .18      & .17   & \textbf{.2}                                                             & .15      \\
\textit{p} = 0.6         & -.04 & .06  & .14        & .13  & .01      & .06   & .01                                                            & .00         \\
\textit{p} = 0.8         & -.10 & .03  & .06          & .05  & .18      & .16   & \textbf{.2}                                                             & .15      \\
\textit{p} = 0.9         & -.11 & -.00 & .03        & .04  & .16      & .15   & .19                                                            & .14      \\
Greedy          & -.14 & .01   & .14          & .13  & .06      & .05   & .06                                                            & .06      \\
\textit{k} = 10          & -.04 & .15  & .03        & .05  & .07      & .08   & .07                                                            & .07      \\
\textit{k} = 20          & -.05 & .14  & .05          & .06  & .05      & .04   & .06                                                            & .04      \\
\textit{k} = 50          & -.09 & .01  & .03        & .03  & .06      & .03   & .03                                                            & .05      \\
\textit{k} = 100         & -.07 & .04  & .00       & .02  & .11      & .08   & .08                                                            & .08      \\
\textit{k} = 200         & -.12 & .03  & .02       & .03  & .06      & .06   & .04                                                            & .05      \\
\textit{k} = 500         & -.09 & .02  & .04          & .04  & .10       & .08   & .08                                                            & .08      \\
Vanilla         & -.09 & .01  & -.00 & .00  & .07      & .05   & .05                                                            & .05      \\

% Reference Text  & -0.095 & 0.074  & nan           & -0.057 & 0.01      & 0.01   & 0                                                               & 0.02 \\
\hline
\end{tabular}}
\caption{Pearson's correlation coefficient (\textit{r}) between \textbf{UID score and automatic metrics} of dialog responses generated using different decoding settings. All p-values < 0.05.}
\label{tab:uidcorr}
%end{adjustbox}
\end{table*}

\label{sec:experiments}
\subsection{Model \& dataset}  
 We use the fine-tuned GPT-2 \citep{radford2019language} model provided by  HuggingFace and use their data preprocessing and response generation scripts\footnote{\url{https://github.com/huggingface/transfer-learning-conv-ai}}. We used the Persona-Chat \citep{personalizing} data split provided by the ConvAI2 challenge \citep{dinan2020second}\footnote{\url{https://github.com/DeepPavlov/convai/tree/master/2018}}. We then generated responses for 7500 dialogue histories randomly picked from 7801 validation set examples using vanilla, top-\textit{p}, top-\textit{k} sampling and greedy decoding. 
 
\paragraph{Decoding algorithms.} \label{decoding_algos}
% Saranya, I've revised this a little bit. D.
\emph{Vanilla sampling} randomly picks the next token from the model's probability distribution, including many long-tail samples. \emph{Top-$k$} samples from the \textit{k} most probable tokens; \emph{Greedy decoding} is Top-$k=1$ decoding, always selecting the most probable next token. \emph{Top-$p$ (Nucleus)} sampling selects the next token from the top $p$ portion of the probability mass. 

 
% Moved more toward the top. -- D.
\subsection{Uniform Information Density score}
We measure UID as the variance of the surprisal (negative log likelihood) of each token in the response \citep{jain-etal-2018-uniform, regularizer, meister-etal-2020-beam}. This measure is able to capture any sudden variations in the surprisal of the tokens in the sentence. UID Score is formulated as follows: the dialogue model learns a conditional probability \textit{p} parameterized by $\theta$ to predict the next token ($y_{t}$) in the sentence. The surprisal ($u$) of the next token $y_{t}$ is, 
\newcommand\eqdef{\ensuremath{\stackrel{\rm def}{=}}} % Equal by definition
\begin{align} 
    u(y_{t}) = - \log  (p_{ \theta } (y | x, y<t)),
\end{align}    
for $t \geq 1$ where $y_{0} = <EOS>$, $t$ = time step, and $x$ = dialogue context. Higher the surprisal, lower its probability and vice-versa. Thus, surprisal indicates how unexpected or surprising a token is in a given context. Average surprisal of a sentence (\textit{y}) is defined as, 

\begin{align}
    \mu(y) = \frac{1}{|y|} \sum_{t}(u(y_{t}))
\end{align} 
Finally, the \textit{UID score} of a sentence (\textit{y}) is defined as the negative normalized variance of the surprisal:
\begin{align}
    \mathrm{UIDscore} (y) =  - \frac{1}{|y|} \sum_{t}(u(y_{t}) -  \mu )^{2}  
\end{align}

From this formulation, a perfectly uniform sentence would have a variance equal to $0$ (i.e. the surprisal of every token in the sentence is equal). Since we take the negative of the variance, the higher the absolute value of UID score, the more non-uniform its information density.

% \subsection{Information density of decoding algorithms}
% For all the model and human generated responses to dialogue histories taken from the Persona-Chat validation set, we calculate and plot histograms of UID scores as shown in figure \ref{fig:uid} so as to gauge how different decoding algorithms distribute information in their model responses. We also visualize the histogram of average surprisals for the same set of candidate responses as shown in figure \ref{fig:surp}.  

\subsection{Response evaluation}
\paragraph{Automatic metrics.} We measure the quality of responses using length (number of tokens), BLEU\footnote{\label{nltk}\url{https://github.com/nltk/nltk/tree/develop/nltk/translate}} \citep{bleu}, METEOR\textsuperscript{\ref{nltk}} \citep{meteor}, character level F-score (chrF)\textsuperscript{\ref{nltk}} \citep{chrf}, BLEURT\footnote{\label{metrics}\url{https://github.com/huggingface/datasets/tree/master/metrics}} \citep{bleurt}, a RoBERTa \citep{roberta} based text similarity score\footnote{\url{https://github.com/UKPLab/sentence-transformers/blob/master/docs/usage/semantic_textual_similarity.md}} \citep{sts}, BERTscore\textsuperscript{\ref{metrics}} \citep{bertscore} and SacreBLEU\textsuperscript{\ref{metrics}} \citep{sacrebleu}.

\paragraph{Human evaluation.} To study the effect of adherence to UID on the perceived quality of generated responses beyond n-gram, reference-based and learned automatic metrics, we collected human judgments along 3 measures -- \textbf{related} (to the dialogue history), \textbf{furthering} (if a response keeps the conversation going/is encouraging for the dialogue partner) and \textbf{interesting} (if the response provides engaging/new information). We provide screenshots of the task interface (Figure \ref{fig:mturk}), instructions (Figure \ref{fig:mturkex}) and details about the MTurk study design in Appendix \ref{sec:mturk}.
\\
