
%model-independent sampling methods guide machines to sample the next utterance from all possible sequences.
% Yet, UID's role in machine generation is not well understood. To address this knowledge gap, we study i. if widely used sampling strategies encourage responses that follow UID; ii. how different sampling methods distribute information in generations; and iii. if adherence to the UID property effects generation quality along automatic metrics and human judgements.

% We find that i. \textit{vanilla} sampling follows the information density distribution of the human-generated text or the UID principle; ii. \textit{Greedy} sampling leads to generations that are more uniform than the vanilla distribution. For \textit{top-p/k} sampling, as the value of p/k increases, the generations progressively become non-uniform, gradually approaching the vanilla distribution; iii. UID does not correlate with n-gram/reference-based automatic metrics, though they do correlate with human judgements along measures of relatedness and interestingness. Thus, providing evidence that UID does capture nuances about generation quality that go beyond automatic metrics that are insufficient to measure task performance for open-ended dialogue. Finally, we also propose a speaker-specific UID regularizer that attunes the information density of model generations for multi-turn dialogue.


 

% Old abstract
%  Uniform Information Density (UID) is a marker of optimal human dialogue \citep{frank2008speaking}. As UID is rooted in information theory, it can intuitively be operationalized as some function of a neural model's probability distribution. Yet, in our knowledge, there has not yet been any work towards a systematic analysis and understanding of UID's potential role in dialogue response generation. While there is prior work that makes this connection between UID and neural response generation, it is limited to one sampling strategy (beam search) for machine translation \cite{meister-etal-2020-beam}.  We still do not know the information density behaviors of widely-used decoding approaches for dialogue response generation. In this work, we aim to address this knowledge gap by investigating if i. current dialogue response generation strategies follow the UID principle, ii. if UID correlates with existing automatic metrics of dialogue response quality; and iii. we collect human annotations on qualitative metrics to further gauge the effect of UID on model generated responses. We find that i. vanilla and greedy decoding lead to generations that adhere with the UID principle, but top-\textit{p}/nucleus and top-\textit{k} sampling follow a progressively more non-uniform information density distribution for higher values of  \textit{p} and \textit{k}; ii. UID does not correlate with reference-based and n-gram based automatic evaluation metrics; and, iii. UID does correlate with human judgments on quality measures of Relatedness and Interestingness. In other words, UID does capture nuances about generation quality that go beyond n-gram based automatic metrics that are insufficient to measure task performance for open-ended dialogue. 
 % Despite  has been only one approach to make this connection between the UID principle and neural response generation using beam search\cite{meister-etal-2020-beam}. 
 
 
 
% **Will include this only if we propose the new variants of the regularizer/ depends on space/time** \citeauthor{xu2018information} extended upon their work and addressed the often quoted problem with written-text based studies in dialogue and production by basing their analysis on spoken dialogue so as to capture the nature of conversation as a `joint activity' between two speakers (Xu & Reitter, 2018). They report that UID is consistent at the inter and intra-sentential levels. In their analysis, the information content drops at the beginning of a new topic (topic shift) and then eventually grows as the topic is being discussed more, and eventually lowers again as the topic ends. They also examine the variation in the contribution to information density from speakers playing different roles (topic initiator v/s topic responder), specifically in the context of the introduction of a new topic in dialogue. They find that topic initiators and topic responders have decreasing and increasing entropy, respectively \cite{xu2016entropy, xu2018information}.

% Higher values of \textit{p} and \textit{k} have shown to produce better quality text than lower cut-off criteria. In addition, these two sampling methods have also been known to produce better text than greedy or vanilla sampling. The differences in the UID distributions and the known differences in their generation quality thus warrant a further exploration of the relationship of UID score with quality metrics. We also see similar trends for surprisal in figure \ref{surp}. Thus it is crucial to study the effect of UID while controlling for surprisal. For the task of dialogue generation on PersonaChat, we find that nucleus sampling and top-\textit{k} sampling do not follow the UID principle at higher values as argued for the task of machine translation in \citep{meister-etal-2020-beam}. This confirms that it cannot be assumed that truncated probability distributions follow the same information distribution patterns as ground truth or human-generated data. For example, we see that vanilla sampling has a UID score histogram most similar to the reference text. This can be attributed to the fact that GPT-2 was fine-tuned on human-human dialogue and vanilla sampling utilizes GPT-2's learnt probability distribution as it is. We see, in contrast to our expectation, that the reference text isn't as uniformly distributed as other types of generation, and the information density curves exhibited by the sampling distributions rely on the distribution of fine-tuning data, and thus UID might not be an inherent quality of any sampling method. It is also key to note that the task of attuning sampling methods is necessary in the cases where the truncated probability distribution no longer follows the UID distributions of the reference text or the vanilla distribution. 

% Typical MTurk tasks pay well below a living wage for the US, with median earnings at only about \$2/hr \citep{hara2018data}. 
% Though we target a fair wage of \$15/hr, MTurk as a whole is not designed to ensure fair pay for its workers. 
% We detail our estimates of worker pay to make it clear that we ensured a fair rate, but we recognize that any work using this platform has the potential to encourage more `typical' low-paying tasks.
% Additionally, we did not control for annotator demographics nor did we explicitly give annotators instructions about avoiding social biases in their writing.
% There is therefore no reason to expect that training a system on data collected via the protocol we advocate for here will result in a model that is more fair.