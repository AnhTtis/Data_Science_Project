\section{Findings}
 
\subsection{Information density of model responses} We plot the histograms of UID scores computed for all of the generated responses in Figure \ref{fig:uid}. The information densities of human-generated responses have a wider spread than responses produced by the models. Overall, the human-generated reference text has more non-uniform sentences than all model-generated responses. We notice a very high and narrow peak in the case of greedy decoding. This is not surprising as responses sampled using greedy search maximize the probability of the next token (minimize surprisal). Consequently, such responses would have very low surprisal at almost every word, hence lower variance. Vanilla sampling uses the probability distribution learned from the training data, which might be why it is also closer to the validation set (reference text) distribution. With increase in \textit{p} and \textit{k}, we see that the information density distribution spreads across a larger range and includes more non-uniform responses, slowly approaching that of the reference text. \label{sec:hist_results}


\begin{table}[ht!]
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{cc|ccc}
& &   \multicolumn{3}{c}{\textbf{Pearson's \textit{r} between}} \\
& & \multicolumn{3}{c}{\textbf{UID score and qualitative metrics}} \\
\textbf{Surprisal interval} & \textbf{n}   & \textbf{Related}              & \textbf{Furthering}  & \textbf{Interesting}          \\
\hline
& & & & \\
(0.8, 1.2)                                                  & 24  & .17        & -.03       & \textbf{-.30$^{\ast}$}        \\
(1.2, 1.6)                                                  & 64  & .12         & .08       & -.13     \\
(1.6, 2.0)                                                  & 91  & .05         & \textbf{-.23$^{\ast}$}      & -.07        \\
(2.0, 2.4)                                                  & 109 & -.04        & -.13       & -.00       \\
(2.4, 2.8)                                                  & 111 & -.06        & \textbf{-.21$^{\ast}$}       & -.05        \\
(2.8, 3.2)                                                  & 105 & -.02       & .01         & -.10       \\
(3.2, 3.6)                                                  & 99  & \textbf{-.23$^{\ast}$}      & -.10    & .19         \\
(3.6, 4.0)                                                  & 66  & .03         & -.05        & -.09       \\
(4.0, 4.4)                                                  & 42  & -.33       & -.22       & -.09     \\
(4.4, 4.8)                                                  & 24  & -.14        & \textbf{-.61$^{\ast}$}      & .04        \\
(4.8, 5.2)                                                  & 12  & -.33       & -.14     & \textbf{-.54$^{\ast}$}        \\
(5.2, 5.6)                                                  & 13  & \textbf{-.98$^{\ast}$}       & -.64      & -.38      \\
\\
\hline
\end{tabular}}
\caption{Pearson's \textit{r} between \textbf{UID score and and human judgments} of qualitative measures for dialog responses bucketed by surprisal [Surprisal interval = the ranges of surprisal values used for bucketing responses, n = number of responses in each surprisal interval, $^{\ast}$p-value < .05]}
\label{tab:humaneval}
\end{table}


\subsection{UID score \& automatic metrics}
We present the correlation between UID scores and automatic metrics calculated for the generated dialogue responses in Table \ref{tab:uidcorr}. UID scores have a weak correlation with RoBERTa-based similary scores for two settings of nucleus sampling. Other than that, UID scores are not correlated with automatic metrics of response generation. We take this to be an indication that if UID scores do capture any aspect of response quality, it goes beyond what is measured by such metrics and might provide for a better evaluation criteria.


% This is important as such reference based, and specifically n-gram based measures are insufficient for evaluating open-ended response generation.

\subsection{UID score \& human Judgments} 
Motivated by the fact that UID score is derived from surprisal, we test if surprisal is a confounding factor and find that, indeed, UID scores were highly correlated with average surprisal (Table \ref{tab:suprisal_uid}). To tease apart the effect of UID scores on response quality, we controlled for surprisal by grouping or bucketing responses into 12 intervals of surprisals (within a range of 0.4 units as shown in the first column on Table \ref{tab:humaneval}). Within these intervals, surprisal had no correlation with generation quality (Table \ref{tab:corr_human_surp}). Once we control for surprisal i.e. analyse dialog responses with similar surprisals but varying UID scores, we observe that UID scores negatively correlate with human judgments, to varying degrees of strength, for responses in very low or high surprisal intervals (see Table \ref{tab:humaneval}). Thus, for the extremities of the surprisal range, UID scores indicate that better rated responses are non-uniform.
