The last three decades have seen a growing focus on visual servoing methods to perform robotic tasks in various sectors, e.g., industry, defence, autonomous vehicles, aerospace, medicine etc. \emph{Visual servoing} refers to the dynamic control of systems using continuous visual feedback. Consequently, the key components of a classical visual servoing controller are feature extraction, matching, and tracking over time using images. Nevertheless, the feasibility and effectiveness of classical visual servoing are closely correlated with that of visual tracking methods, whose performance is a concern with low-textured images that do not have distinguishable geometric shapes.

Recently, advanced visual servoing methods emerged that allow avoiding visual tracking by directly using global image information for error regulation in the control loop. These approaches are referred to as \emph{direct visual servoing} methods \cite{deguchi2000direct}. Accordingly, different types of global image information have been investigated in the literature such as image-intensity~\cite{Tamadazte2012,collewet2011photometric}, spatio-temporal gradients \cite{Marchand2010gradient}, histograms \cite{bateux2016histograms}, mutual information \cite{dame2011mutual}, Gaussian mixtures \cite{crombez2018visual} etc. More recently, authors proposed to model time-frequency image information such as wavelets~\cite{ourak2019direct} and shearlets~\cite{duflot2019wavelet} instead of the spatio-temporal image information. However, direct methods clearly exhibit narrower convergence domains compared to the traditional visual servoing schemes. To tackle this problem, some works have used spectral domain visual features. These features are proved to be robust to noise and are used for many computer vision and robotics applications like image correlation \cite{larsson2011correlating}, range data registration \cite{bulow2012spectral}, robotic grasping \cite{adjigble2021spectgrasp} etc. In~\cite{marchand2020direct}, Discrete Cosine Transform (DCT) coefficients have been explored for direct visual servoing and in~\cite{marturi2014visual, marturi2016image}, Fourier shift property has been used to design a decoupled visual controller.
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/Fig1.pdf}
    \caption{Illustration of the model alignment process with our proposed approach for 3 different objects: (top) apple; (middle) mug; and (bottom) gas knob. Red point cloud is the reference model, gray ones are intermediate candidates during the convergence, green is the target scene point cloud and blue is the final aligned model with the target. The gray trajectory curves indicate the model convergence.}
    \label{fig:fig1}
\end{figure}
%

Most of the aforementioned state of the art on visual servoing is based on 2D image information and the literature using 3D data for visual servoing, e.g., depth maps or point clouds, is very much limited. Very few recent works have reported such methods \cite{teuliere2014dense, 8122920, dahroug2020pca}. A particular advantage of using 3D data over 2D images is that they are well-suited for complex environments, \textit{i.e.}, texture-less, varying light, unstructured etc., and avoid computation of complex pose estimations. A direct visual servoing method based only on camera-acquired depth maps is presented in~\cite{teuliere2014dense}. The control law minimises the depth error computed using the current and reference full-depth maps. A similar approach has been reported in~\cite{8122920} to control the motion of a mobile robot. Although these methods reported promising results, they require dense depth data and exhibit limited convergence. A virtual visual servoing method using a polygon mesh generated offline from point clouds is presented in~\cite{kingkan2016model}. Although point clouds are used in its offline phase, the method's main visual controller still uses stereo image pairs for model matching.

In this paper, we present a spectral domain registration-based visual servoing scheme using point clouds. Although, very few works have used spectral information in their 2D visual servoing schemes \cite{marchand2020direct, ourak2016wavelets, marturi2016image, marturi2014visual}, there is no known instance reported in the literature that uses spectrally transformed point clouds. The main basis of our approach is the 3D model/point cloud alignment or registration, which works by finding a global 6 degrees of freedom (DoF) transformation between the two point clouds corresponding respectively to a reference model and a target object or scene. Using spectrally transformed point clouds, translation is estimated by Fourier analysis whereas rotation is estimated by spherical correlation. Gradient ascent-based optimization has then been used to iteratively minimize translation and rotation costs. Object translations and rotations are considered independent. This allows us to devise a decoupled control scheme, which optimizes the two costs separately. The proposed method uses a 3D fast Fourier transform in the Cartesian space $\mathbb{R}^3$ and real spherical harmonics on the unit-sphere $\bm{S}^2$ and the rotation group $\bm{SO(3)}$ to compute the gradient of the translation and rotation costs, respectively. This method can be used for aligning the entire global scene (direct visual servoing) or a single object model in a simple or cluttered scene. Example instances of object alignments in simple scenes are shown in Fig.~\ref{fig:fig1}, where a reference model is being aligned with a (single object) scene point cloud. While the current or the target scene cloud is captured online by a scene or robot-mounted depth sensor, the reference cloud can be a transformed global scene cloud (in case of direct visual servoing) or can be obtained offline by sampling a CAD model of the target object or by registering multiple cloud samples of the object as in~\cite{marturi2019dynamic}.

The key contributions of this work are as follows:
\begin{itemize}
    \item We propose a new spectral domain-based method for full 3D model alignment, \textit{i.e.}, to estimate global translation and rotation between two point clouds.
%
    \item We propose a new 6-DoF visual servoing scheme that works directly with dense as well as partial point clouds represented in the spectral domain.
\end{itemize}

Our method's advantages are multi-fold. Unlike the existing approaches that require dense data, our method can work effectively with partial point clouds. In comparison to the state-of-the-art depth-based direct visual servoing schemes, our method possesses improved convergence domain. The use of spectral data instead of spatial data makes our method robust to noise, which is apparent when using real-world point cloud measurements. Since no colour or intensity information is required, our method can work well in the case of texture-less objects and low lighting conditions. Finally, the proposed method can be used for both aligning an object model in a densely cluttered scene, and positioning a robot manipulator in the task space.% 