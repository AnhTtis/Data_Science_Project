 Robotic grasping and manipulation has received great attention from the robotics community for decades~\cite{bicchi2000robotic}. Modern-day robotic systems can be equipped with multitude sensors such as: force~\cite{stabile2022role}, vision~\cite{saxena2008robotic}, tactile~\cite{tegin2005tactile}, position~\cite{zhang2020robotic}, etc. Significant progress has been made in the different aspects: robotic design, grippers, sensors and control methods. Surely, among the multitude of sensors investigated in the context of robotic manipulation or grasping tasks ; force and vision sensors are those that have received the most interest from the community. Where a force sensor offers the ability to feel the local forces and moments applied when manipulating or gripping an object \hl{(ref force in grasping)}, the visual sensor offers both global (position, orientation, etc.) and local information (robot-object interactions, deformations, etc.) \hl{(ref vision in grasping)}. Indeed, the use of visual information enables the control of the entire grasping task (recognition, localisation, positioning, grasping, etc.). In robotics, the dynamic control of a system using real-time and continuous visual feedback is known as \emph{visual servoing} (also referred as vision-based control)~\cite{chaumette2006visual, chaumette2007visual}. The last two decades have seen a growing focus on visual servoing methods to perform various robotic tasks, in industry \cite{lippiello2007position}, military, autonomous vehicles \hl{(ref)}, aerospace \hl{(ref)}, and even in the execution of specific tasks in surgery \hl{(ref)}. One of the key success criteria of a visual servoing controller is the ability to detect, extract, match and track visual information over time. 
The last two decades have seen a growing focus on visual servoing methods to perform various robotic tasks, in industry, military, autonomous vehicles, aerospace, and even in the execution of robotic surgeries. The key components of a visual servoing controller are the feature extraction and tracking over time using images. Recently, advanced visual servoing methods emerged that allow avoiding the challenging visual tracking tasks by directly using global image information as direct signal inputs in the control loop. These approaches are referred to as featureless visual servoing~\cite{deguchi2000direct} also known as \emph{direct visual servoing methods}. Accordingly, different types of global image information have been investigated. Over the past two decades, various direct servoing methods have been reported in the literature, such as image-intensity~\cite{kallem2007kernel, collewet2011photometric, Tamadazte2012, caron2013photometric}, spatio-temporal gradients~\cite{Marchand2010gradient}, histogram-based method~\cite{bateux2016histograms}, mutual information~\cite{dame2011mutual}, photometric Gaussian mixtures~\cite{crombez2018visual}, photometric moments~\cite{bakthavatchalam2018direct}, etc. More recently, authors proposed to model time-frequency image information such as wavelet ~\cite{ourak2019direct} and shearlets~\cite{duflot2019wavelet} instead of the above listed spatio-temporal image information. However, direct methods exhibit clearly narrower convergence domains compared to the traditional visual servoing schemes. To tackle this problem, Discrete Cosine Transform (DCT) coefficients has been considered as visual features expressed in the frequency domain as a sum of multiscale cosine functions~\cite{Marchand20b}. Before this work, Fourier shift property has been used to estimate the $xy$ translations and $r_z$ rotation in the framework of a frequency-based visual servoing~\cite{marturi2014visual, marturi2016image, guelpa2016single}.