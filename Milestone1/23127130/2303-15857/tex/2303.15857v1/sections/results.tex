\subsection{Setup Description}
Experiments are performed using camera-acquired point clouds. Two different tests are shown in this work. We first validate the model alignment process, where a full reference model of an object is converged onto a scene point cloud. Next, we show direct visual servoing tests conducted using a 7-axis cobot (KUKA iiwa) fitted with a wrist-mounted depth camera (Ensenso N35). For this case, the entire point cloud is used to position the robot at a target location. The point cloud processing and controller software are developed in C++ and are executed from a PC running Windows with Intel i7 4 core CPU with $2.9~\mathrm{GHz}$ frequency. Point cloud library (PCL) \cite{Rusu_ICRA2011_PCL} is used for point cloud processing, and FFTSO3 \cite{lee2018real} and FFTW \cite{frigo1998fftw} libraries are used for spectral analysis.  

As mentioned earlier in Sec. \ref{sec:intro}, the reference object point cloud for model alignment experiments are built offline by stitching multiple clouds as in \cite{marturi2019dynamic}. Note that the point normals are obtained directly at the time of cloud acquisition. The point clouds are voxelized with a grid of resolution $8~\mathrm{mm}$, while the surface normals are discretised on the unit sphere with a bandwidth of $B=16$. The maximum degree of expansion of the harmonic coefficients on the sphere is $l_{max} = 32$. These values are estimated empirically and provide good performance in terms of speed and alignment accuracy. The three main factors that control the convergence speed of our approach are the voxel grid resolution, the EGI bandwith, and the parameters $\lambda_t$ and $\lambda_r$. Finer grids require computing higher number of Fourier coefficients, which in turn slows down the process. With the aforementioned parameters, on an average, the current processing speed of our approach is $8.7~\mathrm{ms/iteration}$.
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/Plots_for_fig1.pdf}
    \caption{Convergence plots for the (left) mug and (right) gas knob objects shown in Fig. \ref{fig:fig1}.}
    \label{fig:fig1_plots}
\end{figure}
\subsection{Model Alignment Analysis}
%
The following three different experiments are conducted validating the model alignment ability of our approach: (C--1) a full single object model cloud is aligned on to its arbitrarily transformed version (Fig. \ref{fig:fig1} and Fig. \ref{fig:fig1_plots}); (C--2) a full model cloud is aligned on to a partially observed target cloud (Fig. \ref{fig:align_res}); and (C--3) a full model cloud is aligned onto a cluttered scene of objects (Fig. \ref{fig:clutter_res}). Note that for all these experiments, both the reference and target clouds share the same global frame. % and are randomly positioned in the task space. 
Different household objects are used for the tests and the clutter scenes are built by randomly positioning these objects as shown in Fig. \ref{fig:clutter_res}. The result images shown in Fig. \ref{fig:fig1}, \ref{fig:fig1_plots}, \ref{fig:align_res}, and \ref{fig:clutter_res},  show sample screenshots during the alignment process and the evolution of costs and errors. In case of clutter, evolution of gradients is shown as the target location, \textit{i.e.}, the true ground truth position, of the object being matched is not known beforehand. Detailed results can be seen in the supplementary video.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/partial_cost_res.pdf}
    \caption{Model alignment analysis in case of a full reference model being aligned to a partially observed scene. Red, blue and green clouds respectively represent, reference, aligned and target. Results for two objects are shown (left) glove and (right) mustard can.}
    \label{fig:align_res}
\end{figure}
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/clutter_results.pdf}
    \caption{Model alignment analysis in case of cluttered scenes. (top) The used clutter scene and alignment for two different objects are shown; and  (bottom) corresponding plots showing the evolution of translation and rotation gradients during the alignment.}
    \label{fig:clutter_res}
\end{figure}

\midsepremove
\begin{table}
    \centering
    \caption{Average final values during model alignment.}
    \label{tab:cost_table}
    \begin{threeparttable}
    \begin{tabularx}{\columnwidth}{>{\raggedright\arraybackslash}X|
 >{\hsize=1\hsize\centering\arraybackslash}X|
 >{\hsize=1\hsize\centering\arraybackslash}X|
 >{\hsize=1\hsize\centering\arraybackslash}X}
         \toprule
         &  C--1 (cost) & C--2 (cost) & C--3 (gradient) \\
         \midrule
        Trans. error\tnote{1} & 1.77524e-5 & 8.11356e-5 & 6.4e-05  \\
        Rotation error\tnote{1} & 2.747765e-2 & 3.2e-2& 1.199277e-12  \\
        \bottomrule
    \end{tabularx}
    \begin{tablenotes}
        \item[1] \footnotesize{Computed as average of $(\mathrm{real} - \mathrm{estimated})^2$. Note that this applies only to C--1 and C--2.}
    \end{tablenotes}
    \end{threeparttable}
\end{table}

From the obtained results, it can be seen that the model clouds are successfully aligned with the target clouds for all the test cases. The average final convergence costs for conditions C--1 and C--2, and the average final gradient change for C--3 are shown in the Table \ref{tab:cost_table}. This clearly demonstrates the accuracy of our approach in terms of convergence and model alignment. Furthermore, our method showcased superior performance in case of the complex conditions like matching a model to partially observed data as well as to extremely unstructured (and occluded) scenes containing a heap of objects. A good convergence and model alignment are observed even for these complex cases.
%
\subsection{Robot Positioning Tests: Direct Visual Servoing}
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/DVS_Results.pdf}
    \caption{Illustration of direct visual servoing. Top row shows the robot at starting, intermediate, and final positions. Middle row shows the initial, intermediate and final point clouds. Bottom row shows the task convergence plot and the trajectory followed by the robot end-effector. For this experiment, the entire point cloud is used instead of any local object model matching. See supplementary video for more details.}
    \label{fig:dvs_result}
\end{figure}
%
For this test, we consider a planar positioning task where the robot end-effectorâ€™s position is controlled in a direct visual servoing fashion. As stated earlier, the entire point cloud is used to generate the robot control commands, \textit{i.e.}, without using any local model matching. This test has been performed with a cluttered scene to test the ability of our approach in case of challenging conditions. For the sake of demonstration, the reference cloud is acquired at the robot home position. After this, the robot is moved to a random position in the task space from where the visual servoing starts. Note that all joints of the robots are moved to ensure large transformation between the current and reference positions. Results obtained with this experiment are depicted in the Fig. \ref{fig:dvs_result}. From these results, it is clearly evident that the method performs well while matching an entire cloud starting from a position where only a part of it is visible. The convergence plots demonstrate the smooth motion of the robot in reaching a target position. Detailed results can be found in the supplementary video.