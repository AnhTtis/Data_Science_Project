\newpage
\section*{Response to both reviewers on benchmarking against other algorithms}
Both reviewers have indicated interests in seeing comparisons of the proposed algorithm, adaptive s-step GMRES, against other existing GMRES algorithms that are not MGS-based. Reviewer \#1 is interested in seeing comparison against CGS2-GMRES and Reviewer \#2 is interested in seeing comparison against TSQR-based s-step GMRES algorithm. We agree with both reviewers that MGS-GMRES is inherently not scalable on large-scale parallel computing architectures. Comparing with a scalable variant would make the comparison more meaningful and rigorous. Since both reviewers have similar requests, we would like to address to both reviewers here regarding comparison against other GMRES algorithms. 

In the manuscript, we have included a significant amount of additional results and rewritten a significant portion of the text in \Cref{sec:scalability}. However, the general observation and conclusions drawn still align with the original manuscript. Here, we would like to highlight the main changes introduced in this manuscript, explain reasons for those changes and highlight the key results.

\begin{itemize}
    \item \textbf{Addition of two more algorithms for comparison} First of all, we took suggestions from both reviewers and included two more algorithms to compare against for parallel testing. In total, we compare four different GMRES algorithms in our weak scaling and strong scaling results: (1) MGS-GMRES: the standard GMRES algorithm based on Modified Gram-Schmidt. (2) CGS2-GMRES: the GMRES algorithm based on Classical Gram-Schmidt with re-orthogonalization. This variant uses CGS2 for orthogonalization which is numerically stable and has better parallel performance. The number of global reductions is constant (three, to be exact) at each iteration. (3) BCGS2-TSQR s-step GMRES: a BCGS2-TSQR based s-step GMRES algorithm where BCGS2 is used as the inter-orthogonalization scheme and TSQR is used as the intra-orthogonalization scheme. (4) Adaptive s-step GMRES: the algorithm proposed in this work (\Cref{alg:adaptsgmres}).
    The first two algorithms are column-wise GMRES algorithms where each iteration works on only one additional Krylov basis vector whereas the last two algorithms are s-step GMRES algorithms where each iteration works on a block of multiple Krylov basis vectors. We would like to highlight that the difference between (3) and (4) is in the intra-orthogonalization scheme. We have included a paragraph in \Cref{sec:scalability} to elaborate on the differences among various GMRES variants. 
    
    \item \textbf{Change in software} There is a change in the software used in all parallel performance tests. The results in the previous version of manuscript was generated using a code written in C++ and the matrix/vector operations were implemented through the Eigen library at version 3.4.0. The C++ code was compiled using Intel compiler 19.1.1 with MKL as the backend for math kernels. Intel MPI 19.0.9 was used for communication. In this revised manuscript, we replace Eigen with PETSc for all matrix operations. The main reason for this change is because of the availability of the TSQR algorithm in PETSc/SLEPc. TSQR algorithm is well-established in literature. However, its software implementation, particularly the parallel version, requires custom communication patterns/trees/maps for efficient reduction-like communication and it is non-trivial to implement them efficiently, especially when the number of MPI ranks is not powers-of-two. Therefore, we want to use a publicly available TSQR algorithm in credible open-source libraries for reproducibility. In this work, we chose the parallel TSQR algorithm in SLEPc, which is based on PETSc. For example, see \href{https://slepc.upv.es/documentation/current/docs/manualpages/BV/BVOrthogBlockType.html#BVOrthogBlockType}{SLEPC BV Orthogonalization Block Types} here. 
    To make fair comparisons, it is best to have all other GMRES variants implemented with the same backend for matrix/vector operations. Therefore, we refactored all of our code. All parallel results in this revised manuscript are generated using the refactored C++ code with PETSc 3.20, SLEPc 3.20 for matrix/vector operations.
    
    During refactoring, we have also upgraded the compiler to Intel oneAPI compiler 23.1.0 to leverage on any recent compiler optimizations (Intel MPI is upgraded to 21.9.0 as well). Based on our experience, the new compiler does seem to improve the performance significantly (see the next point).
    
    All tests were conducted on a cluster equipped with Intel Xeon(R) Platinum 8280 CPUs @ 2.70GHz. All nodes have 56 cores per node and are interconnected via Mellanox Infiniband, HDR-100. The hardware used in this revised manuscript is the same as the previous manuscript.

    \item \textbf{Changes in results with respect to the previous manuscript} Due to the change in the software stack and code refactorization, we re-ran all scaling results (for MGS-GMRES and Adaptive s-step GMRES) to make sure they represent a fair comparison with the newly-added algorithms (CGS2-GMRES, BCGS2-TSQR-GMRES). However, if one were to compare the results for MGS-GMRES and Adaptive s-step GMRES in this revised manuscript against those in the previous manuscript, there will be noticeable performance differences in some mathematical operations due to software changes. Here, we would like to account for how MGS-GMRES and Adaptive GMRES results changed from the initial manuscript to the current revised manuscript, in order to justify the credibility of our new set of results.
    
    \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        width=.6\textwidth,
        height=.4\textwidth,
        % bar width=.2cm,
        stack negative=separate,
        ybar stacked,
        xmin=0.5,
        xmax=3.5,
        ymin=-0.1,
        ymax=.4,
        xtick={1,2,3},
        xticklabels={Ver. 1, Ver. 2, Ver. 3$^*$},
        ylabel={time (s)},
        ymajorgrids=true,  
        yminorgrids=true,   
        % minor y tick num=1,
        legend pos=outer north east,
        % legend columns=4,
        % legend style={at={(0.5,1.22)}, anchor=north},
        % ytick={-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0},
        % yticklabels={0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0},
    ]
    \addplot +[draw=black,
    color=hcblue,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=SpMV_GMRES] {Review/comp.dat};
    \addplot +[draw=black,
    color=hcgreen,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Ortho_GMRES] {Review/comp.dat};
    \addplot +[draw=black,
    color=black,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Ortho_comm_GMRES] {Review/comp.dat};
    \addplot +[draw=black,
    color=hcred,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Other_GMRES] {Review/comp.dat};
    \legend{SpMV, Ortho. (comp), Ortho. (comm), Other};
    \end{axis}
    \begin{axis}[
        width=.6\textwidth,
        height=.4\textwidth,
        % bar width=.2cm,
        stack negative=separate,
        ybar stacked,
        xmin=0.5,
        xmax=3.5,
        ymin=-0.1,
        ymax=.4,
        hide axis,
    ]
    \addplot +[draw=black,
    color=hcblue,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=SpMV_SGMRES] {Review/comp.dat};
    \addplot +[draw=black,
    color=hcgreen,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Ortho_SGMRES] {Review/comp.dat};
    \addplot +[draw=black,
    color=black,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Ortho_comm_SGMRES] {Review/comp.dat};
    \addplot +[draw=black,
    color=hcred,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Other_SGMRES] {Review/comp.dat};
    \end{axis}
    \end{tikzpicture}
    \caption{Timing breakdown for a 1M size 3D Laplace matrix running one a single node. MGS-GMRES is on the left and Adaptive s-step GMRES is on the right. Ver. 1: Original code in the previous manuscript (Eigen library, Intel compiler 19.1.1, Intel MPI 19.0.9). Ver. 2: Original code but compiler with Intel compiler 23.1.0 and Intel MPI 21.9.0. Ver. 3: Refactored code (PETSc library, Intel compiler 23.1.0 and Intel MPI 21.9.0). }
    \label{fig:revision}
\end{figure}

    In \Cref{fig:revision}, we use the same matrix problem (1M Laplace matrix on a single node) and compare three different versions of the code. The first version was based on Eigen library with Intel compiler version 19.1.1 and was used in the previous manuscript. This serves as a baseline for comparison across code versions. The second version uses the same source code, but compiled with an upgraded Intel compiler at version 23.1.0. As seen in the figure, there is a significant speedup in the computation of orthogonalization schemes for the adaptive s-step GMRES algorithm. The MGS orthogonalization stays relatively the same. Note that the orthogonalization in s-step GMRES consists mainly of BLAS-3 kernels whereas MGS uses BLAS-1 kernels. This means that the upgraded Intel compiler (with upgraded built-in MKL) is able to perform more optimizations for BLAS-3 kernels.

    Version 3 is a refactored code that is based on PETSc library, compiled with Intel 23.1.0. The timing breakdown indicates that using PETSc library allows for further optimizations in math kernels. Specifically, speedup is observed for the orthogonalization scheme based on BLAS-3 kernels and the SpMV kernels. 

    Overall, we see significant speedup using the new compiler with refactored code based on PETSc. We would like to highlight that this does not affect the general trend of scaling results, thus it does not contradict with any observation or conclusion in this work. As shown in \Cref{fig:weakscaling} and \Cref{fig:weakscaling_pc}, scalability of the kernel and the overall algorithm is still observed. We can even argue that this speedup in orthogonalization step due to compiler/backend change aligns with the objective of this work, which is to avoid communication, but at an intra-node level. As our algorithm relies on BLAS-3 kernels, more aggressive compiler optimizations and well-implemented PETSc backends can exploit various levels of cache to reduce communication costs associated with data movement across different levels of cache memory, giving rise to more performant results in a single node.
    
    \item Since the time taken for the orthogonalization step in adaptive s-step GMRES is on the order of 0.01s, to mitigate profiling overheads, we increased the size of the weak scaling test matrix from 1M to 8M on a single node. We also included an additional set of experiments at 2048 nodes (114,688 cores). In other words, the matrix size varies from 8M to 16B on 1 to 2048 nodes, instead of the previous configuration with 1M to 256M on 1 to 256 nodes. Please see \Cref{sec:weakscaling} and \Cref{fig:weakscaling_pc} for more details. The vast difference in orthogonalization step between our proposed algorithm and other GMRES variants highlights the benefit of using BCGS2+Partial CholQR for avoiding communication at the intra-node level and at the inter-node level.
\end{itemize}

We hope these points could convince the reviewers that we have thoroughly prepared the additional benchmarking results as requested, and the changes in results are academically sound and credible. For detailed discussion of the benchmarking results among various GMRES variants, please refer to the added text in \Cref{sec:scalability}. Again, we genuinely appreciate the feedback from both reviewers and we completely agree adding these tests does make this manuscript more rigorous and complete.

