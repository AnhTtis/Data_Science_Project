\newpage
\section*{Response to reviewer \#2}
\rpy{(Responses are highlighted in red. Corresponding code changes are highlighted in red as well in the manuscript.)}

\subsection*{Major comments}
\paragraph{Comment 1} 
The first is not comparing to s-step GMRES using TSQR, as in [23,28]. The authors say that part of their motivation is fixing the LOO (loss of orthogonality) from doing Cholesky QR (CholQR) to compute an orthogonal basis of the desired Krylov subspace. But TSQR is guaranteed to provide an orthogonal basis, and also avoid communication. And the incremental condition estimation (ICE) proposed by the authors to choose which leading columns of the R factor are sufficiently well conditioned still applies. This could also be used to reduce the cost of reconstructing (a subset of) the Q factor.

\rpy{See "Response to both reviewers on benchmarking against other algorithms".}

The second omitted observation is in the paper “Theory of inexact Krylov Subspace methods and applications to scientific computing,” V. Simoncini and D. Szyld, SISC, 2003, cited in [10].
Rather than have the adaptive step size s remain constant throughout the iterations, Simoncini and Szyld show that you can relax the accuracy of the SpMV at a rate inversely proportional to the residual norm. The translation of this to s-step CG in [10] was that you could start off with a smaller s (when the residual is large) and make s larger and larger as the residual converges without losing accuracy. The work of Simoncini and Szyld certainly applies to GMRES (arguably more so than CG), so I don’t see why a similar principle shouldn’t hold for s-step GMRES. Perhaps the use of CholQR instead of TSQR limits s further?

\rpy{We appreciate your comment. This is a very good point. We did study the s-step CG work in [11] in detail and have commented on its difference from our method in Subsection 2.3. We are also aware of the work of inexact Krylov subspace methods. But we do not apply them directly in our work for three reasons.
\begin{itemize}
\item The mathematical derivation in section 4.1 in [11] shows a direct relationship between the residual and the condition number of the Krylov basis matrix in the s-step CG method. The relationship indicates one can relax the constraint on condition number of the Krylov basis matrix. However, such a relationship is not mathematically derived in this work due to the complexity of the block QR algorithms. As repetitively emphasized in this work, the key challenges in an s-step GMRES algorithm lies in the numerical stability of the block QR algorithm. There are many different choices for inter-orthogonalization and intra-orthogonalization. The backward stability of many block QR algorithms is still largely an open problem, as pointed out by [9]. It is not clear to us if a simple relation like equation (4.13) in [11] has its equivalent form in the s-step GMRES algorithm, given the wide range of choices of block QR algorithms. In this work, we focus on numerical experiments to empirically verify the numerical stability of the proposed algorithm. Hence, we did not attempt such a mathematical derivation for the our s-step GMRES algorithm. This could be an extension for future work.
\item From a practical standpoint, we did not find this method of increasing $s$ to be efficient in our framework. In the s-step CG method proposed in [11], all tests were carried out using monomials (see Section 5 of [11]) where Ritz values are not required. Hence, increasing $s$ on the fly through relaxation does not incur additional costs. In our framework, we have demonstrated the benefit of using scaled Newton polynomial to significantly increase the step size to $O(100)$ and it is important that Ritz values are pre-computed to set up the Newton polynomial basis. In practice, with an initial step size of $s_0$, only $s_0$ Ritz values are pre-computed. Allowing the step size to increase beyond $s_0$ through some form of relaxation will require additional Ritz value computation that incur additional (both computational and communication) costs. 
\item If the adapted step size is smaller than $s_0$, we could potentially increase the step size in subsequent block iterations as long as it stays below $s_0$ so that we do not have to compute additional Ritz values. However, empirically we do not observe such an increase in adapted step size in consecutive block iterations. For instance, in Fig. 6, 7 and 10, the step size decreases rapidly after the first block iteration. Without deriving the exact mathematical relationship, we conjecture this is due to the fact that subsequent block iterations are trying to construct a set of orthogonal basis in a smaller subspace that has to be orthogonal to the subspace spanned by the Krylov basis in previous blocks. Therefore, it could be numerically challenging to retain a very large step size as the block iteration continues.
In addition, the lower two figures of Fig. 6 indicate that the condition number of the Krylov basis matrix can change significantly after an inter-orthogonalization step. These observations do not seem to imply a direct relationship between the condition number of Krylov basis vector and the solution residual, in the form of equation (4.13) in [11]. Even with TSQR as an intra-orthogonalization scheme, the exponential increase in condition number of block $R(1:j,1:j)$ in Fig. 6 does not seem to suggest that TSQR could increase the step size significantly. 
\end{itemize}
In summary, given the complexity of block QR algorithms in s-step GMRES, we feel that the derivation of a mathematical expression relating the residual and adapted step size to be outside the scope of this work. Without a detailed derivation, we do not find strong empirical evidence that increasing the step size via the theory of inexact Krylov subspace could benefit the overall framework of our proposed algorithm. However, we recognize the significance of inexact Krylov subspace methods and are happy to discuss potential avenues for addressing this in future studies.

}

\subsection*{Minor comments}

\paragraph{Comment 1} Algorithm 2.2, which is called in Algorithm 2.1, repeat the calling sequence to make it easier to match variable names with Algorithm 2.1.

\rpy{Thank you for the feedback. We totally agree and have added the calling in Algorithm 2.2.}

\paragraph{Comment 2} Line 412: Change ”for arbitrary starting vector” to ”for an arbitrary starting vector”.

\rpy{Thank you for pointing out the typo. We have corrected it at line 417.}

\paragraph{Comment 3} Line 421: Change $E_{i,j}\in \mathbb{R}^{s\times s}$ to $E\in \mathbb{R}^{s\times s}$, for consistency with equation (3.15).

\rpy{We are slightly confused as equation (3.15) is defined using $E_{i,j}\in \mathbb{R}^{s\times s}$ which is consistent with the text. Please see line 427. We would appreciate if the reviewer can elaborate on this comment.}

\paragraph{Comment 4} In Fig 2 (and later figures), the same labeling (black circles or red diamonds) is used for 2 curves with 2 meanings, which makes it difficult for the reader to tell which is which. Also, in the caption, the phrase ”The LOO of the algorithm” is ambiguous as to which algorithm it refers to, be explicit.

\rpy{Thank you for the feedback. The two curves are implicitly differentiated via where they start and how they progress. The relative residuals start from 1 and gradually decrease, where indicate convergence of the various GMRES algorithm. The loss of orthogonality (LOO) error curves start from machine epsilon, gradually increase and their rates of increase reflect the accuracy of the orthogonalization schemes in each GMRES variant. This is consistent with representations in previous literature. E.g., see Figure 2 and 3 in [34], Figure 5 and 6 in [38]. Nonetheless, we added some clarification at line 479 where we first discuss results using figures. We also updated the captions in multiple figures.}

\paragraph{Comment 5} Line 536: The notation $\kappa(V_{1:j,1:j})$ seems wrong, since all rows of the tall matrix V should be included.

\rpy{Thank you for pointing out the typo. Yes, we agree this should be $\kappa(V_{:,1:j})$ and we have updated both the text at line 546 and the figure label in Fig. 6.}

\paragraph{Comment 6} References: there are many capitalization errors, eg gmres instead of GMRES, etc.

\rpy{Thank you for pointing it out. We apologize for the formatting error. We made a Latex mistake which led to capitalization inconsistencies. We have corrected the capitalization error and updated the reference.}