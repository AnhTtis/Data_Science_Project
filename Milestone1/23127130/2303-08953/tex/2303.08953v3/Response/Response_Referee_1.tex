\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{pgfplots,tikz}
\pgfplotsset{compat=1.17}
\usepackage{cleveref}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\definecolor{hcorange}{RGB}{245, 130, 48}
\definecolor{hcnavy}{RGB}{0, 0, 128}
\definecolor{hcblue}{RGB}{0, 130, 200}
\definecolor{hcpink}{RGB}{250, 190, 190}
\definecolor{hcbrown}{RGB}{128, 0, 0}
\definecolor{hclavender}{RGB}{230, 190, 255}
\definecolor{hcgrey}{RGB}{128, 128, 128}
\definecolor{hcgreen}{RGB}{60, 180, 75}
\definecolor{hcred}{RGB}{230, 25, 75}

\newcommand{\rpy}[1]{{\color{red}{}#1}} % reply

\title{Response to referee \#1}
\date{}

\begin{document}

\maketitle

\section*{Response to both reviewers on benchmarking against other algorithms}
Both reviewers have indicated interests in seeing comparisons of the proposed algorithm, adaptive s-step GMRES, against other existing GMRES algorithms that are not MGS-based. Reviewer \#1 is interested in seeing comparison against CGS2-GMRES and Reviewer \#2 is interested in seeing comparison against TSQR-based s-step GMRES algorithm. We agree with both reviewers that MGS-GMRES is inherently not scalable on large-scale parallel computing architectures. Comparing with a scalable variant would make the comparison more meaningful and rigorous. Since both reviewers have similar requests, we would like to address to both reviewers here regarding comparison against other GMRES algorithms. 

In the manuscript, we have included a significant amount of additional results and rewritten a significant portion of the text in Section 5. However, the general observation and conclusions drawn still align with the original manuscript. Here, we would like to highlight the main changes introduced in this manuscript, explain reasons for those changes and highlight the key results.

\begin{itemize}
    \item \textbf{Addition of two more algorithms for comparison} First of all, we took suggestions from both reviewers and included two more algorithms to compare against for parallel testing. In total, we compare four different GMRES algorithms in our weak scaling and strong scaling results: (1) MGS-GMRES: the standard GMRES algorithm based on Modified Gram-Schmidt. (2) CGS2-GMRES: the GMRES algorithm based on Classical Gram-Schmidt with re-orthogonalization. This variant uses CGS2 for orthogonalization which is numerically stable and has better parallel performance. The number of global reductions is constant (three, to be exact) at each iteration. (3) BCGS2-TSQR s-step GMRES: a BCGS2-TSQR based s-step GMRES algorithm where BCGS2 is used as the inter-orthogonalization scheme and TSQR is used as the intra-orthogonalization scheme. (4) Adaptive s-step GMRES: the algorithm proposed in this work (Algorithm 3.2).
    The first two algorithms are column-wise GMRES algorithms where each iteration works on only one additional Krylov basis vector whereas the last two algorithms are s-step GMRES algorithms where each iteration works on a block of multiple Krylov basis vectors. We would like to highlight that the difference between (3) and (4) is in the intra-orthogonalization scheme. We have included a paragraph in Section 5 to elaborate on the differences among various GMRES variants. 
    
    \item \textbf{Change in software} There is a change in the software used in all parallel performance tests. The results in the previous version of manuscript was generated using a code written in C++ and the matrix/vector operations were implemented through the Eigen library at version 3.4.0. The C++ code was compiled using Intel compiler 19.1.1 with MKL as the backend for math kernels. Intel MPI 19.0.9 was used for communication. In this revised manuscript, we replace Eigen with PETSc for all matrix operations. The main reason for this change is because of the availability of the TSQR algorithm in PETSc/SLEPc. TSQR algorithm is well-established in literature. However, its software implementation, particularly the parallel version, requires custom communication patterns/trees/maps for efficient reduction-like communication and it is non-trivial to implement them efficiently, especially when the number of MPI ranks is not powers-of-two. Therefore, we want to use a publicly available TSQR algorithm in credible open-source libraries for reproducibility. In this work, we chose the parallel TSQR algorithm in SLEPc, which is based on PETSc. For example, see \href{https://slepc.upv.es/documentation/current/docs/manualpages/BV/BVOrthogBlockType.html#BVOrthogBlockType}{SLEPC BV Orthogonalization Block Types}\footnote{URL: https://slepc.upv.es/documentation/current/docs/manualpages/BV/BVOrthogBlockType.html} here. 
    To make fair comparisons, it is best to have all other GMRES variants implemented with the same backend for matrix/vector operations. Therefore, we refactored all of our code. All parallel results in this revised manuscript are generated using the refactored C++ code with PETSc 3.20, SLEPc 3.20 for matrix/vector operations.
    
    During refactoring, we have also upgraded the compiler to Intel oneAPI compiler 23.1.0 to leverage on any recent compiler optimizations (Intel MPI is upgraded to 21.9.0 as well). Based on our experience, the new compiler does seem to improve the performance significantly (see the next point).
    
    All tests were conducted on a cluster equipped with Intel Xeon(R) Platinum 8280 CPUs @ 2.70GHz. All nodes have 56 cores per node and are interconnected via Mellanox Infiniband, HDR-100. The hardware used in this revised manuscript is the same as the previous manuscript.

    \item \textbf{Changes in results with respect to the previous manuscript} Due to the change in the software stack and code refactorization, we re-ran all scaling results (for MGS-GMRES and Adaptive s-step GMRES) to make sure they represent a fair comparison with the newly-added algorithms (CGS2-GMRES, BCGS2-TSQR-GMRES). However, if one were to compare the results for MGS-GMRES and Adaptive s-step GMRES in this revised manuscript against those in the previous manuscript, there will be noticeable performance differences in some mathematical operations due to software changes. Here, we would like to account for how MGS-GMRES and Adaptive GMRES results changed from the initial manuscript to the current revised manuscript, in order to justify the credibility of our new set of results.
    
    \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        width=.6\textwidth,
        height=.4\textwidth,
        % bar width=.2cm,
        stack negative=separate,
        ybar stacked,
        xmin=0.5,
        xmax=3.5,
        ymin=-0.1,
        ymax=.4,
        xtick={1,2,3},
        xticklabels={Ver. 1, Ver. 2, Ver. 3$^*$},
        ylabel={time (s)},
        ymajorgrids=true,  
        yminorgrids=true,   
        % minor y tick num=1,
        legend pos=outer north east,
        % legend columns=4,
        % legend style={at={(0.5,1.22)}, anchor=north},
        % ytick={-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0},
        % yticklabels={0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0},
    ]
    \addplot +[draw=black,
    color=hcblue,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=SpMV_GMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcgreen,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Ortho_GMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=black,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Ortho_comm_GMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcred,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Other_GMRES] {../Review/comp.dat};
    \legend{SpMV, Ortho. (comp), Ortho. (comm), Other};
    \end{axis}
    \begin{axis}[
        width=.6\textwidth,
        height=.4\textwidth,
        % bar width=.2cm,
        stack negative=separate,
        ybar stacked,
        xmin=0.5,
        xmax=3.5,
        ymin=-0.1,
        ymax=.4,
        hide axis,
    ]
    \addplot +[draw=black,
    color=hcblue,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=SpMV_SGMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcgreen,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Ortho_SGMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=black,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Ortho_comm_SGMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcred,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Other_SGMRES] {../Review/comp.dat};
    \end{axis}
    \end{tikzpicture}
    \caption{Timing breakdown for a 1M size 3D Laplace matrix running one a single node. MGS-GMRES is on the left and Adaptive s-step GMRES is on the right. Ver. 1: Original code in the previous manuscript (Eigen library, Intel compiler 19.1.1, Intel MPI 19.0.9). Ver. 2: Original code but compiler with Intel compiler 23.1.0 and Intel MPI 21.9.0. Ver. 3: Refactored code (PETSc library, Intel compiler 23.1.0 and Intel MPI 21.9.0). }
    \label{fig:revision}
\end{figure}

    In \Cref{fig:revision} here, we use the same matrix problem (1M Laplace matrix on a single node) and compare three different versions of the code. The first version was based on Eigen library with Intel compiler version 19.1.1 and was used in the previous manuscript. This serves as a baseline for comparison across code versions. The second version uses the same source code, but compiled with an upgraded Intel compiler at version 23.1.0. As seen in the figure, there is a significant speedup in the computation of orthogonalization schemes for the adaptive s-step GMRES algorithm. The MGS orthogonalization stays relatively the same. Note that the orthogonalization in s-step GMRES consists mainly of BLAS-3 kernels whereas MGS uses BLAS-1 kernels. This means that the upgraded Intel compiler (with upgraded built-in MKL) is able to perform more optimizations for BLAS-3 kernels.

    Version 3 is a refactored code that is based on PETSc library, compiled with Intel 23.1.0. The timing breakdown indicates that using PETSc library allows for further optimizations in math kernels. Specifically, speedup is observed for the orthogonalization scheme based on BLAS-3 kernels and the SpMV kernels. 

    Overall, we see significant speedup using the new compiler with refactored code based on PETSc. We would like to highlight that this does not affect the general trend of scaling results, thus it does not contradict with any observation or conclusion in this work. As shown in Fig. 13 to Fig. 15, scalability of the kernel and the overall algorithm is still observed. We can even argue that this speedup in orthogonalization step due to compiler/backend change aligns with the objective of this work, which is to avoid communication, but at an intra-node level. As our algorithm relies on BLAS-3 kernels, more aggressive compiler optimizations and well-implemented PETSc backends can exploit various levels of cache to reduce communication costs associated with data movement across different levels of cache memory, giving rise to more performant results in a single node.
    
    \item Since the time taken for the orthogonalization step in adaptive s-step GMRES is very small, to mitigate profiling overheads, we increased the size of the weak scaling test matrix from 1M to 8M on a single node. We also included an additional set of experiments at 2048 nodes (114,688 cores). In other words, the matrix size varies from 8M to 16B on 1 to 2048 nodes, instead of the previous configuration with 1M to 256M on 1 to 256 nodes. Please see Fig. 13 and Fig. 14 for more details. The vast difference in orthogonalization step between our proposed algorithm and other GMRES variants highlights the benefit of using BCGS2+Partial CholQR for avoiding communication at the intra-node level and at the inter-node level.
\end{itemize}

We hope these points could convince the reviewers that we have thoroughly prepared the additional benchmarking results as requested, and the changes in results are academically sound and credible. For detailed discussion of the benchmarking results among various GMRES variants, please refer to the added text in Section 5. Again, we genuinely appreciate the feedback from both reviewers and we completely agree adding these tests does make this manuscript more rigorous and complete.

\newpage
\section*{Response to reviewer \#1}
\rpy{(Responses are highlighted in red. Corresponding code changes are highlighted in red as well in the manuscript.)}

\subsection*{Major comments}
\paragraph{Comment 1} 
In Section 5.1 (weak scaling) and 5.2 (strong scaling), the parallel performance of the stable s-step GMRES is compared to the performance of MGS GMRES. It is pretty clear that in this case s-step GMRES is going to win over MGS-GMRES by a wide margin, because MGS-GMRES is probably the most communication-intensive variant of GMRES. Thus, the comparison is unfair. It would be valuable to include either a CGS2-GMRES or one of the low synch variants to make the comparison more meaningful, as CGS2-GMRES requires less communication and in general, is much more favorable in parallel. Also the block orthogonalization in the s-step GMRES is based on CGS2, so comparing the s-step GMRES with CGS2-GMRES would had been justified.

\rpy{See "Response to both reviewers on benchmarking against other algorithms".}

\paragraph{Comment 2}
Also, what polynomial basis was used in parallel experiments? If it was scaled polynomial basis, what was the cost of computing Ritz values?

\rpy{Thank you for your feedback. Yes, scaled Newton polynomial basis is used in all parallel experiments. We have added clarifications in different places of the text to remind readers about this. For instance, see line 731.

The Ritz values are obtained from a standard Arnoldi/GMRES algorithm, so the cost of obtaining the Ritz value would be equivalent to the cost of a standard Arnoldi/GMRES algorithm. As commented at line 341, this is a common strategy in s-step GMRES algorithms. We did not report the cost of this because (a) this can be inferred from the MGS-GMRES results; (b) the cost of obtaining Ritz values can vary depending on the application. For instance, some Ritz values can be known prior to solving the linear system, or when multiple right hand sides are to be solved, the cost of obtaining Ritz value can be amortized. If a standard GMRES algorithm is used to obtain Ritz values, the solution from the GMRES algorithm can also be used as a better initial guess for the s-step GMRES algorithms. In short, the cost of obtaining Ritz values varies depending on the use case and is therefore not included in this manuscript.

In addition to the cost of obtaining Ritz values, there is also some computational cost associated with the incremental condition estimator as well. But as indicated in Equation (3.15), all operations are element-wise operations on a $s\times s$ matrix, which have $O(s^2)$ complxity and typically have negligible computational cost comparing to the GMRES algorithms. In practice, after we obtain the Ritz values, the preprocessing steps for scaled Newton basis with $s = 100$ take about $O(10^{-3})$ seconds in our parallel experiemnts.
}

\paragraph{Comment 3}
All the parallel tests are performed on relatively "easy" problems. It would be valuable to see the performance on a problem that has high condition number, is run to completion (not just for 100 iterations) and (possibly) requires a preconditioner in order to converge. Also it seems like all parallel tests were performed with s=100 and were run for 100 iterations, hence there was no added operations/communication coming from orthogonalization between blocks and restarting GMRES. It would be valuable to see how do these not included operations influence the overall performance, as ideal scenario is unlikely in practice.

\rpy{Thank you for your suggestion. We think this is a great idea. As requested, we have included a set of results in Section 5.2 where a local ILU(0) preconditioner is applied to the Laplace problem and multiple inter-orthogonalization steps are used. See Fig. 14 for the summary of results. 

We acknowledge the importance of using numerically challenging linear systems. However, we chose to use the same Laplace problem due to several reasons. First of all, we want to use a problem with varying dimensions for weak scaling. Weak scaling allows us to demonstrate scalability up to a larger scale, as compared to strong scaling tests which have stringent memory requirements. Weak scaling also allows us to show the timing breakdown clearly. Therefore, we use a PDE problem with varying number of mesh points for weak scaling analysis.

With increasing matrix dimensions, the condition number of the linear system often increases. It becomes increasingly difficult to find an effective preconditioner that accelerates convergence to the same degree and scale well at the same time. Running to completion would mean different (and likely increasing) number of iterations for increasing matrix dimensions. We can definitely fine-tune the preconditioner to reach the same convergence at different matrix sizes, but as our paper focuses on numerical stability instead of accelerating convergence, we feel this is outside the scope of our paper. We also mentioned in Section 2 (line 80) that our work do not aim to optimize the SpMV and preconditioning operations as their communication-avoiding counterparts depend heavily on the sparsity patterns of the matrix and the preconditioner. Hence, in Figure 14, we present a standard local ILU(0) preconditioner for the same 3D Laplace equation without any fine-tuning of the preconditioner. This also allows for reproducibility of the results.

We want to highlight that the initial step size with local ILU(0) preconditioner can be as large as $s=100$ for scaled Newton polynomials in all our tests. But to mimic a numerically challenging problem, we reduced the step size to $s=25$ instead of $s=100$ for the same 100 iterations, so that multiple inter-orthogonalization steps are necessary. In practice, the tests are not fully converged (e.g., $10^{-8}$), but the runtime behavior of the algorithm with additional iterations can be inferred from the current set of results. The results (as presented in Figure 14) can also be compared with the single-block results in Figure 13. 

Overall, we want to provide a set of standard benchmarks with preconditioning steps and inter-orthogonalization steps, and also allow for reproducibility of our results. We hope the arguments above justify the scope of the added tests in Section 5.2.
}

\subsection*{Minor comments}

\paragraph{Comment 1} Line 31-32: "The performance of many operations in Krylov subspace methods, such as dot products and sparse matrix-vector multiplies (SpMVs), is bounded by communication on distributed systems" Consider reformulating - SpMV requires only local communication between (usually neighboring) ranks but dot product requires reduction which leads to global communication - I.e., all ranks communicate partial result and then the full result is distributed. Global communication is what (typically) limits performance in parallel setting.

\rpy{Thank you for your feedback. We agree that global reduction is often what limits performance on large-scale problems. We have updated the text to place emphasize on dot product operations. Please see line 31. }

\paragraph{Comment 2} Line 36: "redesign Krylov subspace methods that communicate less." Either "design Krylov methods that communicate less" or "redesign Krylov methods to communicate less".

\rpy{We have updated the text. Please see line 36.}

\paragraph{Comment 3} Line 40-41:
"BLAS-3 operations that are better optimized for fast memory [19]"
Not clear what "fast memory" means here.

\rpy{By "fast memory", we meant cache memory that allow quick access to data. It has lower latency compared to DRAM. We have updated the text to explicitly mention cache memory. Please see line 41.}

\paragraph{Comment 4} Line 47-48:
"Formulations of s-step Krylov subspace methods are mathematically equivalent to their classical counterparts"
Should probably be: "s-step methods are mathematically equivalent to their classical counterpart" or "s-step method can be formulated/written in such a way that they are mathematically equivalent to their classical counterpart".

\rpy{We have updated the text. Please see line 47.}

\paragraph{Comment 5} Paragraph starting at Line 53: Change "section 2" to "Section 2", "section 3" to "Section 3" etc (section and subsection should be capitalized). "algorithm 2.1" should be "Algorithm 2.1" (please check the entire paper for referencing algorithms and figures - they should be all capitalized).

\rpy{We have updated all references with capitalization. Thank you for your feedback on the format.}

\paragraph{Comment 6} Line 93: "But in finite precision, the Krylov basis vectors converge to the largest eigenvector of matrix A" Should be "eigenvector associated with the largest eigenvalue"

\rpy{We have updated the text for clarification, please see line 94.}

\paragraph{Comment 7} Line 127: "Each block $V^{(i)}$ first needs to be inter-orthogonalized with respect to orthogonal bases generated from previous iterations
[...]"
This statement is unclear; i.e., this is a first time the notion of "iteration" appears in this section. Should probably be $Q^{(1)}$ to $Q^{(I-1)}$ orthogonalized before or similar. Rewrite.

\rpy{We apologize for the ambiguity. We agree it would be best to delay the discussion of "iteration" to later sections. Here, we think it could be more appropriate to use "previous blocks" instead of "previous iterations". Please see line 129 for the updated text.}

\paragraph{Comment 8} Line 128:
"per s vector."
Should be "per s vectors"

\rpy{Thank you for pointing out the typo. We have corrected it in line 141.}

\paragraph{Comment 9} Line 154:
"Here, we note that the input block does not refer to the block $V^{(i)}$"
This statement is unclear I.e., what is $i$?

\rpy{We meant the $i$th block in the original matrix as defined in Equation (2.2). We have updated the text at line 157 to add the reference about Equation (2.2).}

\paragraph{Comment 10} Line 201:
"As mentioned previously, an ill-conditioned input matrix to CholQR may produce a numerically non-SPD Gram matrix"
But from what I understand, the Gram matrix ($V^TV$) IS the input to CholQR. If not, can you specify which Gram matrix are referring to?

\rpy{We apologize for the ambiguity. Here, we treat CholQR as a QR algorithm, so the input is the matrix to be factorized, which is $V$. $V^TV$ is the first computation within the CholQR algorithm. We have added $V$ in the text to further clarify this. Please see line 205. An additional clarification is also included at line 167 where there might be ambiguity.}

\paragraph{Comment 11} Line 209:
$X_{s-p}$ should be of a dimension $[N \times (s-p)]$ not $[n \times (s-p)]$

\rpy{Correct. Thank you for pointing out the typo. We have updated the text at line 212.}.

\paragraph{Comment 12} Line 236:
"that are relatively cheaper"
That are cheaper.

\rpy{We have updated the text. Please see line 239.}

\paragraph{Comment 13} Line 392
"approximate true eigenvalues, we can compute the product term approximately before the actual adaptive s-step GMRES algorithm without any communication cost."
First, in order to use the method, you need Ritz values. And computing them through Arnoldi Gram-Schmidt is not communication free.
Second, what is meant by "the product term"?

\rpy{Thank you for your feedback. We completely agree that computing Ritz values through Arnoldi/GMRES is not communication free and this statement is ambiguous. Here, our intention is to highlight that the initial step size estimator does not add extra communication cost to the original algorithm, which is using scaled Newton polynomials as a basis for adaptive s-step GMRES. 

To further clarify this, we first acknowledge that the construction of scaled Newton polynomial basis, like that of the standard Newton polynomial basis, requires Ritz value computation and is often not communication-free. But the derivations, such as equation (3.14), show that the initial step size estimator only takes in the Rtiz values as input and generates an estimation based on those Ritz values. Since Ritz values are available on all MPI ranks, all the operations in the estimator are done locally to each rank. Hence, the estimator does not incur additional communication cost, which aligns with the objective of communication-avoiding algorithms.

As for the second comment, "the product term" refers to the $\prod_{k=1}^{j-1}{\frac{\lambda_i-\theta_k}{|\bar{\theta}-\theta_k|}}$ in equation (3.14) as indicated by the product operator $\prod$. 

Nonetheless, we added some clarification to the text at line 397-398.
}

\paragraph{Comment 14} Line 415:
"inevitably increases the condition number of the Krylov basis matrix V exponentially."
Rewrite, i.e., "exponential increase in the condition number is inevitable" or similar.

\rpy{We clarified the text. Please refer to line 420.}

\paragraph{Comment 15} Line 434:
"As the Ritz values are available on all processes"
Processors or ranks, not processes.

\rpy{Thank you for your feedback. We updated the text. Please see line 441.}

\paragraph{Comment 16} Line 435:
"the estimator requires no communication cost"
"The estimator requires no communication" or "the estimator incurs no additional communication cost".

\rpy{Thank you. We updated the text. Please see line 441.}

\paragraph{Comment 17} Line 444:
"to verify the implementation sequentially since parallel implementations only introduce additional data movements that do not affect the stability of the overall algorithm."
This is not necessarily true.

\rpy{Yes, we agree this is not technically correct. Roundoff errors can have variations across software/hardwares, and parallelizing a sequential algorithm can potentially introduce these variations in roundoff errors that could modify the stability of the algorithm. However, such consideration is outside the scope of this work and we treat this as an assumption. Please see 450 for clarification.}

\paragraph{Comment 18} Line 461:
"We first start"
We start.

\rpy{Thank you for your feedback. We updated the text. Please see line 467.}

\paragraph{Comment 19} Line 500:
"The periodic pattern of LOO is because"
"The periodic pattern of LOO results from/is a consequence of"

\rpy{Thank you. We have updated the text at line 509.}

\paragraph{Figure 2, Figure 3, Figure 4}
- Use different ticks (i.e. squares, diamonds, circles, triangles, etc) for each quantity so the graphs are easy to understand even if printed in black and white or read by someone with impaired color vision.
- The LOO and convergence curve should be on separate plots.

\rpy{Thank you for your feedback. We have standardized all graphs to include ticks for all curves in the graphs, with appropriate legends. 

As for the second comment, We would like to request to keep it this way for two reasons. The first is that each figure currently consists of multiple sub-figures and is already taking up almost an entire page. Separating LOO and convergence plots will tend to make each figure go out of the scope of a single page. Although we can definitely break them up into two pages, but we feel having one figure per test case on one single page makes each figure more self-contained and helps readers to see the full picture. The second reason is that this arrangement of having LOO and convergence curve in one single plot is consistent with representations in previous literature. E.g., see Figure 2 and 3 in [34], Figure 5 and 6 in [38]. Nonetheless, we are open to adjusting them if there are strong opinions. To help, we also added a comment at line 479, where we first discuss results using figures, to clarify the content of figures.}

\paragraph{Line 546}
"In practice, especially on large distributed machines, one could use a smaller restart length and more restarts to trade convergence for communication savings."
Smaller restart means more iterations, and more iterations means more communication. Also at the end of each restart, you have an extra norm (which requires a dot product).

\rpy{Thank you for your feedback. We agree with the statement that smaller restart generally means more iterations which in turn implies more communication. However, it is possible to achieve less communication with smaller restarts in certain cases. The statement right after this line (now at line 557) explains such a scenario and we would like to expand on that.

As shown in Fig. 6, for a total of 600 iterations with 4 restarts (restart length = 150), four block iterations are required within each restart (100 + 23 + 23 + 4). Since each block iteration requires 4 global reductions and each restart requires one reduction to compute the norm of residual vectors, a total of $4\times 4 + 1 =17$ global reductions are needed per restart. This leads to a total of $17 \times 4$ = 68 global reductions. If one were to start with a smaller restart length of 100 instead of 150, a total of 6 restarts are required for the same 600 iterations in total. However, each restart of length 100 would only require one block iteration. Hence, the total number of global reductions would be $5\times 6$ = 30, which is fewer than 4 restarts.

We would like to highlight that the algorithm has the flexibility to adjust this restart length on the fly. As mentioned by the statement at line 557, one can start with a restart length of 150. With the adapted step size at $s\approx 100$, the restart length can be adjusted to $s$ for the next restart to reduce the communication costs. The initial step size estimator can also be used as a form of heuristics to inform the restart length. We acknowledge that adjusting restart length will ultimately alter the convergence behavior. Hence, it is a tradeoff between convergence and communication savings and the choice would depend on the actual use case. 

We hope this clarifies that statement. We have also added some clarification in text. See line 560.
}

\paragraph{Line 569}
"rapid growth/decay of Krylov basis vectors."
It should be either condition number or norms, but not vectors.

\rpy{Yes, we meant the condition numbers of Krylov basis vectors. We have clarified at line 582.}

\paragraph{Line 679} "growth starts depends"
Starts depending

\rpy{Grammatically we think the original text is correct. We meant the exact column (...) depends on the eigenvalue spectrum. Please see the statement currently at line 692.}

\paragraph{Line 717}
"done on Intel Xeon Platinum 8280 processors with 56 cores"
It is clear from the results that not one processor, but entire cluster of them was used. More details of hardware are needed. Also the details of software (version of compiler and libraries used) should be given for reproducibility.

\rpy{Thank you for pointing out this. We apologize for the ambiguity. We have added two short paragraphs at line 762-779 about both the software and hardware used in the parallel experiments.}

\paragraph{Line 754}
"MPK that can explore efficient fast memory usage."
This is a vague statement - please expand.

\rpy{We observed that the MPK kernel in adaptive s-step GMRES, which consists of consecutive SpMVs, is slightly faster than the standard SpMVs that are called at each iteration of MGS-GMRES. Both the MPK kernel in s-step GMRES and SpMVs in MGS-GMRES have approximately the same amount of floating point operators and communication costs. The main difference is that in MPK kernels, SpMVs are called repetitively before any orthogonalization. We want to highlight that we do not make any modification to SpMV kernels. We use the built-in SpMV available in PETSc. We simply call them multiple times in a MPK kernel (with some \texttt{axpy} vector operations for basis computation). Hence, they are essentially the same function calls, but in a different order. Hence, we conjectured that the small difference in performance of MPK/SpMV kernels is because compiler was able to optimize MPK better than standard SpMVs by exploring cache memory. For instance, in MPK, consecutive SpMVs allows the matrix (or part of it) to stay in cache longer for efficient computation. 

Due to the rewrite of results section, we have paraphrased the text with additional details. Please see the paragraph at line 825.}


\end{document}
