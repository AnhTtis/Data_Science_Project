\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{pgfplots,tikz}
\pgfplotsset{compat=1.17}
\usepackage{cleveref}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsfonts}

\definecolor{hcorange}{RGB}{245, 130, 48}
\definecolor{hcnavy}{RGB}{0, 0, 128}
\definecolor{hcblue}{RGB}{0, 130, 200}
\definecolor{hcpink}{RGB}{250, 190, 190}
\definecolor{hcbrown}{RGB}{128, 0, 0}
\definecolor{hclavender}{RGB}{230, 190, 255}
\definecolor{hcgrey}{RGB}{128, 128, 128}
\definecolor{hcgreen}{RGB}{60, 180, 75}
\definecolor{hcred}{RGB}{230, 25, 75}

\newcommand{\rpy}[1]{{\color{red}{}#1}} % reply

\title{Response to referee \#2}
\date{}

\begin{document}

\maketitle

\section*{Response to both reviewers on benchmarking against other algorithms}
Both reviewers have indicated interests in seeing comparisons of the proposed algorithm, adaptive s-step GMRES, against other existing GMRES algorithms that are not MGS-based. Reviewer \#1 is interested in seeing comparison against CGS2-GMRES and Reviewer \#2 is interested in seeing comparison against TSQR-based s-step GMRES algorithm. We agree with both reviewers that MGS-GMRES is inherently not scalable on large-scale parallel computing architectures. Comparing with a scalable variant would make the comparison more meaningful and rigorous. Since both reviewers have similar requests, we would like to address to both reviewers here regarding comparison against other GMRES algorithms. 

In the manuscript, we have included a significant amount of additional results and rewritten a significant portion of the text in Section 5. However, the general observation and conclusions drawn still align with the original manuscript. Here, we would like to highlight the main changes introduced in this manuscript, explain reasons for those changes and highlight the key results.

\begin{itemize}
    \item \textbf{Addition of two more algorithms for comparison} First of all, we took suggestions from both reviewers and included two more algorithms to compare against for parallel testing. In total, we compare four different GMRES algorithms in our weak scaling and strong scaling results: (1) MGS-GMRES: the standard GMRES algorithm based on Modified Gram-Schmidt. (2) CGS2-GMRES: the GMRES algorithm based on Classical Gram-Schmidt with re-orthogonalization. This variant uses CGS2 for orthogonalization which is numerically stable and has better parallel performance. The number of global reductions is constant (three, to be exact) at each iteration. (3) BCGS2-TSQR s-step GMRES: a BCGS2-TSQR based s-step GMRES algorithm where BCGS2 is used as the inter-orthogonalization scheme and TSQR is used as the intra-orthogonalization scheme. (4) Adaptive s-step GMRES: the algorithm proposed in this work (Algorithm 3.2).
    The first two algorithms are column-wise GMRES algorithms where each iteration works on only one additional Krylov basis vector whereas the last two algorithms are s-step GMRES algorithms where each iteration works on a block of multiple Krylov basis vectors. We would like to highlight that the difference between (3) and (4) is in the intra-orthogonalization scheme. We have included a paragraph in Section 5 to elaborate on the differences among various GMRES variants. 
    
    \item \textbf{Change in software} There is a change in the software used in all parallel performance tests. The results in the previous version of manuscript was generated using a code written in C++ and the matrix/vector operations were implemented through the Eigen library at version 3.4.0. The C++ code was compiled using Intel compiler 19.1.1 with MKL as the backend for math kernels. Intel MPI 19.0.9 was used for communication. In this revised manuscript, we replace Eigen with PETSc for all matrix operations. The main reason for this change is because of the availability of the TSQR algorithm in PETSc/SLEPc. TSQR algorithm is well-established in literature. However, its software implementation, particularly the parallel version, requires custom communication patterns/trees/maps for efficient reduction-like communication and it is non-trivial to implement them efficiently, especially when the number of MPI ranks is not powers-of-two. Therefore, we want to use a publicly available TSQR algorithm in credible open-source libraries for reproducibility. In this work, we chose the parallel TSQR algorithm in SLEPc, which is based on PETSc. For example, see \href{https://slepc.upv.es/documentation/current/docs/manualpages/BV/BVOrthogBlockType.html#BVOrthogBlockType}{SLEPC BV Orthogonalization Block Types}\footnote{URL: https://slepc.upv.es/documentation/current/docs/manualpages/BV/BVOrthogBlockType.html} here. 
    To make fair comparisons, it is best to have all other GMRES variants implemented with the same backend for matrix/vector operations. Therefore, we refactored all of our code. All parallel results in this revised manuscript are generated using the refactored C++ code with PETSc 3.20, SLEPc 3.20 for matrix/vector operations.
    
    During refactoring, we have also upgraded the compiler to Intel oneAPI compiler 23.1.0 to leverage on any recent compiler optimizations (Intel MPI is upgraded to 21.9.0 as well). Based on our experience, the new compiler does seem to improve the performance significantly (see the next point).
    
    All tests were conducted on a cluster equipped with Intel Xeon(R) Platinum 8280 CPUs @ 2.70GHz. All nodes have 56 cores per node and are interconnected via Mellanox Infiniband, HDR-100. The hardware used in this revised manuscript is the same as the previous manuscript.

    \item \textbf{Changes in results with respect to the previous manuscript} Due to the change in the software stack and code refactorization, we re-ran all scaling results (for MGS-GMRES and Adaptive s-step GMRES) to make sure they represent a fair comparison with the newly-added algorithms (CGS2-GMRES, BCGS2-TSQR-GMRES). However, if one were to compare the results for MGS-GMRES and Adaptive s-step GMRES in this revised manuscript against those in the previous manuscript, there will be noticeable performance differences in some mathematical operations due to software changes. Here, we would like to account for how MGS-GMRES and Adaptive GMRES results changed from the initial manuscript to the current revised manuscript, in order to justify the credibility of our new set of results.
    
    \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        width=.6\textwidth,
        height=.4\textwidth,
        % bar width=.2cm,
        stack negative=separate,
        ybar stacked,
        xmin=0.5,
        xmax=3.5,
        ymin=-0.1,
        ymax=.4,
        xtick={1,2,3},
        xticklabels={Ver. 1, Ver. 2, Ver. 3$^*$},
        ylabel={time (s)},
        ymajorgrids=true,  
        yminorgrids=true,   
        % minor y tick num=1,
        legend pos=outer north east,
        % legend columns=4,
        % legend style={at={(0.5,1.22)}, anchor=north},
        % ytick={-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0},
        % yticklabels={0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0},
    ]
    \addplot +[draw=black,
    color=hcblue,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=SpMV_GMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcgreen,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Ortho_GMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=black,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Ortho_comm_GMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcred,
    fill opacity=0.8,
    bar shift=-.2cm] table [x=index,y=Other_GMRES] {../Review/comp.dat};
    \legend{SpMV, Ortho. (comp), Ortho. (comm), Other};
    \end{axis}
    \begin{axis}[
        width=.6\textwidth,
        height=.4\textwidth,
        % bar width=.2cm,
        stack negative=separate,
        ybar stacked,
        xmin=0.5,
        xmax=3.5,
        ymin=-0.1,
        ymax=.4,
        hide axis,
    ]
    \addplot +[draw=black,
    color=hcblue,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=SpMV_SGMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcgreen,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Ortho_SGMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=black,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Ortho_comm_SGMRES] {../Review/comp.dat};
    \addplot +[draw=black,
    color=hcred,
    fill opacity=0.8,
    bar shift=.2cm] table [x=index,y=Other_SGMRES] {../Review/comp.dat};
    \end{axis}
    \end{tikzpicture}
    \caption{Timing breakdown for a 1M size 3D Laplace matrix running one a single node. MGS-GMRES is on the left and Adaptive s-step GMRES is on the right. Ver. 1: Original code in the previous manuscript (Eigen library, Intel compiler 19.1.1, Intel MPI 19.0.9). Ver. 2: Original code but compiler with Intel compiler 23.1.0 and Intel MPI 21.9.0. Ver. 3: Refactored code (PETSc library, Intel compiler 23.1.0 and Intel MPI 21.9.0). }
    \label{fig:revision}
\end{figure}

    In \Cref{fig:revision} here, we use the same matrix problem (1M Laplace matrix on a single node) and compare three different versions of the code. The first version was based on Eigen library with Intel compiler version 19.1.1 and was used in the previous manuscript. This serves as a baseline for comparison across code versions. The second version uses the same source code, but compiled with an upgraded Intel compiler at version 23.1.0. As seen in the figure, there is a significant speedup in the computation of orthogonalization schemes for the adaptive s-step GMRES algorithm. The MGS orthogonalization stays relatively the same. Note that the orthogonalization in s-step GMRES consists mainly of BLAS-3 kernels whereas MGS uses BLAS-1 kernels. This means that the upgraded Intel compiler (with upgraded built-in MKL) is able to perform more optimizations for BLAS-3 kernels.

    Version 3 is a refactored code that is based on PETSc library, compiled with Intel 23.1.0. The timing breakdown indicates that using PETSc library allows for further optimizations in math kernels. Specifically, speedup is observed for the orthogonalization scheme based on BLAS-3 kernels and the SpMV kernels. 

    Overall, we see significant speedup using the new compiler with refactored code based on PETSc. We would like to highlight that this does not affect the general trend of scaling results, thus it does not contradict with any observation or conclusion in this work. As shown in Fig. 13 to Fig. 15, scalability of the kernel and the overall algorithm is still observed. We can even argue that this speedup in orthogonalization step due to compiler/backend change aligns with the objective of this work, which is to avoid communication, but at an intra-node level. As our algorithm relies on BLAS-3 kernels, more aggressive compiler optimizations and well-implemented PETSc backends can exploit various levels of cache to reduce communication costs associated with data movement across different levels of cache memory, giving rise to more performant results in a single node.
    
    \item Since the time taken for the orthogonalization step in adaptive s-step GMRES is very small, to mitigate profiling overheads, we increased the size of the weak scaling test matrix from 1M to 8M on a single node. We also included an additional set of experiments at 2048 nodes (114,688 cores). In other words, the matrix size varies from 8M to 16B on 1 to 2048 nodes, instead of the previous configuration with 1M to 256M on 1 to 256 nodes. Please see Fig. 13 and Fig. 14 for more details. The vast difference in orthogonalization step between our proposed algorithm and other GMRES variants highlights the benefit of using BCGS2+Partial CholQR for avoiding communication at the intra-node level and at the inter-node level.
\end{itemize}

We hope these points could convince the reviewers that we have thoroughly prepared the additional benchmarking results as requested, and the changes in results are academically sound and credible. For detailed discussion of the benchmarking results among various GMRES variants, please refer to the added text in Section 5. Again, we genuinely appreciate the feedback from both reviewers and we completely agree adding these tests does make this manuscript more rigorous and complete.

\newpage
\section*{Response to reviewer \#2}
\rpy{(Responses are highlighted in red. Corresponding code changes are highlighted in red as well in the manuscript.)}

\subsection*{Major comments}
\paragraph{Comment 1} 
The first is not comparing to s-step GMRES using TSQR, as in [23,28]. The authors say that part of their motivation is fixing the LOO (loss of orthogonality) from doing Cholesky QR (CholQR) to compute an orthogonal basis of the desired Krylov subspace. But TSQR is guaranteed to provide an orthogonal basis, and also avoid communication. And the incremental condition estimation (ICE) proposed by the authors to choose which leading columns of the R factor are sufficiently well conditioned still applies. This could also be used to reduce the cost of reconstructing (a subset of) the Q factor.

\rpy{See "Response to both reviewers on benchmarking against other algorithms".}

The second omitted observation is in the paper “Theory of inexact Krylov Subspace methods and applications to scientific computing,” V. Simoncini and D. Szyld, SISC, 2003, cited in [10].
Rather than have the adaptive step size s remain constant throughout the iterations, Simoncini and Szyld show that you can relax the accuracy of the SpMV at a rate inversely proportional to the residual norm. The translation of this to s-step CG in [10] was that you could start off with a smaller s (when the residual is large) and make s larger and larger as the residual converges without losing accuracy. The work of Simoncini and Szyld certainly applies to GMRES (arguably more so than CG), so I don’t see why a similar principle shouldn’t hold for s-step GMRES. Perhaps the use of CholQR instead of TSQR limits s further?

\rpy{We appreciate your comment. This is a very good point. We did study the s-step CG work in [11] in detail and have commented on its difference from our method in Subsection 2.3. We are also aware of the work of inexact Krylov subspace methods. But we do not apply them directly in our work for three reasons.
\begin{itemize}
\item The mathematical derivation in section 4.1 in [11] shows a direct relationship between the residual and the condition number of the Krylov basis matrix in the s-step CG method. The relationship indicates one can relax the constraint on condition number of the Krylov basis matrix. However, such a relationship is not mathematically derived in this work due to the complexity of the block QR algorithms. As repetitively emphasized in this work, the key challenges in an s-step GMRES algorithm lies in the numerical stability of the block QR algorithm. There are many different choices for inter-orthogonalization and intra-orthogonalization. The backward stability of many block QR algorithms is still largely an open problem, as pointed out by [9]. It is not clear to us if a simple relation like equation (4.13) in [11] has its equivalent form in the s-step GMRES algorithm, given the wide range of choices of block QR algorithms. In this work, we focus on numerical experiments to empirically verify the numerical stability of the proposed algorithm. Hence, we did not attempt such a mathematical derivation for the our s-step GMRES algorithm. This could be an extension for future work.
\item From a practical standpoint, we did not find this method of increasing $s$ to be efficient in our framework. In the s-step CG method proposed in [11], all tests were carried out using monomials (see Section 5 of [11]) where Ritz values are not required. Hence, increasing $s$ on the fly through relaxation does not incur additional costs. In our framework, we have demonstrated the benefit of using scaled Newton polynomial to significantly increase the step size to $O(100)$ and it is important that Ritz values are pre-computed to set up the Newton polynomial basis. In practice, with an initial step size of $s_0$, only $s_0$ Ritz values are pre-computed. Allowing the step size to increase beyond $s_0$ through some form of relaxation will require additional Ritz value computation that incur additional (both computational and communication) costs. 
\item If the adapted step size is smaller than $s_0$, we could potentially increase the step size in subsequent block iterations as long as it stays below $s_0$ so that we do not have to compute additional Ritz values. However, empirically we do not observe such an increase in adapted step size in consecutive block iterations. For instance, in Fig. 6, 7 and 10, the step size decreases rapidly after the first block iteration. Without deriving the exact mathematical relationship, we conjecture this is due to the fact that subsequent block iterations are trying to construct a set of orthogonal basis in a smaller subspace that has to be orthogonal to the subspace spanned by the Krylov basis in previous blocks. Therefore, it could be numerically challenging to retain a very large step size as the block iteration continues.
In addition, the lower two figures of Fig. 6 indicate that the condition number of the Krylov basis matrix can change significantly after an inter-orthogonalization step. These observations do not seem to imply a direct relationship between the condition number of Krylov basis vector and the solution residual, in the form of equation (4.13) in [11]. Even with TSQR as an intra-orthogonalization scheme, the exponential increase in condition number of block $R(1:j,1:j)$ in Fig. 6 does not seem to suggest that TSQR could increase the step size significantly. 
\end{itemize}
In summary, given the complexity of block QR algorithms in s-step GMRES, we feel that the derivation of a mathematical expression relating the residual and adapted step size to be outside the scope of this work. Without a detailed derivation, we do not find strong empirical evidence that increasing the step size via the theory of inexact Krylov subspace could benefit the overall framework of our proposed algorithm. However, we recognize the significance of inexact Krylov subspace methods and are happy to discuss potential avenues for addressing this in future studies.

}

\subsection*{Minor comments}

\paragraph{Comment 1} Algorithm 2.2, which is called in Algorithm 2.1, repeat the calling sequence to make it easier to match variable names with Algorithm 2.1.

\rpy{Thank you for the feedback. We totally agree and have added the calling in Algorithm 2.2.}

\paragraph{Comment 2} Line 412: Change ”for arbitrary starting vector” to ”for an arbitrary starting vector”.

\rpy{Thank you for pointing out the typo. We have corrected it at line 417.}

\paragraph{Comment 3} Line 421: Change $E_{i,j}\in \mathbb{R}^{s\times s}$ to $E\in \mathbb{R}^{s\times s}$, for consistency with equation (3.15).

\rpy{We are slightly confused as equation (3.15) is defined using $E_{i,j}\in \mathbb{R}^{s\times s}$ which is consistent with the text. Please see line 427. We would appreciate if the reviewer can elaborate on this comment.}

\paragraph{Comment 4} In Fig 2 (and later figures), the same labeling (black circles or red diamonds) is used for 2 curves with 2 meanings, which makes it difficult for the reader to tell which is which. Also, in the caption, the phrase ”The LOO of the algorithm” is ambiguous as to which algorithm it refers to, be explicit.

\rpy{Thank you for the feedback. The two curves are implicitly differentiated via where they start and how they progress. The relative residuals start from 1 and gradually decrease, where indicate convergence of the various GMRES algorithm. The loss of orthogonality (LOO) error curves start from machine epsilon, gradually increase and their rates of increase reflect the accuracy of the orthogonalization schemes in each GMRES variant. This is consistent with representations in previous literature. E.g., see Figure 2 and 3 in [34], Figure 5 and 6 in [38]. Nonetheless, we added some clarification at line 479 where we first discuss results using figures. We also updated the captions in multiple figures.}

\paragraph{Comment 5} Line 536: The notation $\kappa(V_{1:j,1:j})$ seems wrong, since all rows of the tall matrix V should be included.

\rpy{Thank you for pointing out the typo. Yes, we agree this should be $\kappa(V_{:,1:j})$ and we have updated both the text at line 546 and the figure label in Fig. 6.}

\paragraph{Comment 6} References: there are many capitalization errors, eg gmres instead of GMRES, etc.

\rpy{Thank you for pointing it out. We apologize for the formatting error. We made a Latex mistake which led to capitalization inconsistencies. We have corrected the capitalization error and updated the reference.}



\end{document}
