% SIAM Article Template
\documentclass[final,onefignum,onetabnum]{siamart171218}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}
%\tikzexternalize[prefix=tikz/]

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={A numerically stable communication-avoiding \texorpdfstring{$s$}{s}-step GMRES algorithm},
  pdfauthor={Zan Xu, Juan J.~Alonso, and Eric Darve}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

%% Use \myexternaldocument on Overleaf
% \myexternaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Krylov subspace methods are extensively used in scientific computing to solve large-scale linear systems. However, the performance of these iterative Krylov solvers on modern supercomputers is limited by expensive communication costs. The $s$-step strategy generates a series of $s$ Krylov vectors at a time to avoid communication. Asymptotically, the $s$-step approach can reduce communication latency by a factor of $s$. Unfortunately, due to finite-precision implementation, the step size has to be kept small for stability. In this work, we tackle the numerical instabilities encountered in the $s$-step GMRES algorithm. By choosing an appropriate polynomial basis and block orthogonalization schemes, we construct a communication avoiding $s$-step GMRES algorithm that automatically selects the optimal step size to ensure numerical stability. 
  To further maximize communication savings, we introduce scaled Newton polynomials that can increase the step size $s$ to a few hundreds for many problems. An initial step size estimator is also developed to efficiently choose the optimal step size for stability. The guaranteed stability of the proposed algorithm is demonstrated using numerical experiments. In the process, we also evaluate how the choice of polynomial and preconditioning affects the stability limit of the algorithm. Finally, we show parallel scalability on more than 14,000 cores in a distributed-memory setting. Perfectly linear scaling has been observed in both strong and weak scaling studies with negligible communication costs.
\end{abstract}

% REQUIRED
\begin{keywords}
  linear algebra, GMRES, parallel computing, communication avoiding techniques, numerical stability.
\end{keywords}

% REQUIRED
\begin{AMS}
  	65F25, 65F10, 65G50, 68W40
\end{AMS}

\section{Introduction}
Krylov subspace methods are widely used to find iterative solutions to linear systems of equations. They are particularly attractive for sparse and large problems, where direct methods are often prohibitively expensive in terms of both computation and storage. However, on modern large-scale computing architectures, the performance of iterative Krylov solvers is limited by expensive communication costs~\cite{demmel2008avoiding, mohiyuddin2009minimizing}. Such costs are incurred when moving data either between levels of the local memory hierarchy in a single processor or between different processors connected over a network. The performance of many operations in Krylov subspace methods, such as dot products and sparse matrix-vector multiplies (SpMVs), is bounded by communication on distributed systems~\cite{hoemmen2010communication}. As technology trends project floating-point performance to continue to scale at a faster rate than latency reduction~\cite{national2005getting}, the communication costs in numerical algorithms are becoming a serious bottleneck. Hence, to effectively leverage new high-performance computing hardware, one needs to redesign Krylov subspace methods that communicate less.

The communication-avoiding $s$-step Krylov subspace methods~\cite{bai1994newton, chronopoulos1989s,chronopoulos1991s,chronopoulos1996parallel,de1995reducing,hoemmen2010communication, joubert1992parallelizable, carson2015communication, carson2018adaptive, mohiyuddin2009minimizing} attempts to \textit{avoid} communication by generating a series of $s$ Krylov basis vectors at once and orthogonalize them in a block fashion. This replaces most BLAS-2 operations with BLAS-3 operations that are better optimized for fast memory~\cite{dongarra1990set}. Furthermore, the appropriate choice of block orthogonalization schemes can reduce the number of global communication constructs by a factor of $s$~\cite{hoemmen2010communication}. On parallel computers, this not only decreases global communication latency by $s$ asymptotically but also reduces the number of synchronization points in the algorithm, thus mitigating system noise, load imbalances and/or hardware variations among different processors~\cite{swirydowicz2020low,yamazaki2020low}.

The formulations of $s$-step Krylov subspace methods are mathematically equivalent to their classical counterparts. However, finite-precision implementations of these variants introduce several sources of numerical instability, such as poor conditioning of the Krylov basis matrix~\cite{beckermann2000condition,philippe2012generation} and loss of orthogonality of basis vectors due to block QR schemes~\cite{carson2015communication,carson2022block}. As a result, the maximal step size $s$ in $s$-step methods has to be limited to small values for convergence. Since the communication savings rely directly on the step size, it is necessary to address the stability issues to improve the scalability of these methods on high-performance computing resources.

This paper focuses on $s$-step variants of the Generalized Minimal Residual (GMRES) algorithm~\cite{saad1986gmres}. We first review the stability and communication pattern of $s$-step GMRES algorithms in \cref{sec:background}, and highlight the limitations in the current state of the art. We then derive an adaptive $s$-step GMRES algorithm in \cref{sec:algorithm}, which dynamically determines the maximum allowed step size $s$ in a communication-avoiding manner. To achieve better communication savings, we also introduce 
scaled Newton polynomials in \cref{subsec:scalednewton} that can stably increase the step size $s$ to a few hundred for many problems, and an initial step size estimator in \cref{subsec:errorestimator} to efficiently choose initial step sizes. \Cref{sec:tests_stability} present a series of numerical experiments that assess the numerical stability of the proposed algorithm under different measures. Finally, we demonstrate the parallel scalability of the overall algorithm on more than 14,000 cores in \cref{sec:scalability} before we conclude with some remarks in \cref{sec:conclusions}.

% The stability of the algorithm is verified numerically in \cref{sec:tests_stability}. With guaranteed stability, we investigate some heuristics that can improve the stability limit and increase $s$. This is discussed in the context of numerical experiments in \cref{sec:tests_polynomialbasis} and \cref{sec:tests_preconditioners}. In \cref{sec:tests_polynomialbasis}, we also introduce scaled Newton polynomials that can stably increase the step size $s$ to a few hundred for certain problems. An initial step size estimator is introduced in \cref{susec:errorestimator} to efficiently choose initial step sizes and is numerically verified in \cref{sec:tests_final}. Finally, we demonstrate the parallel scalability of the overall algorithm on more than 14,000 cores in \cref{sec:scalability} before we conclude with some remarks on future work in \cref{sec:conclusions}.

\section{Background on \texorpdfstring{$s$}{s}-step GMRES}
\label{sec:background}

Same as the conventional GMRES algorithm, the $s$-step GMRES method solves the linear system
\begin{equation*}
    Ax = b, 
\end{equation*}
with $A \in \mathbb{R}^{N\times N}$. At each iteration, the Matrix Powers Kernel (MPK) performs $s$ SpMVs on a starting normalized column vector $Q_i$ to generate a Krylov basis matrix $V(Q_i,s)$ containing $s+1$ Krylov basis vectors
\begin{equation*}
    V(Q_i, s) = \big[p_0(A)Q_i, \, p_1(A)Q_i, \, p_2(A)Q_i, \, p_3(A)Q_i,\, \dots \,,\, p_s(A)Q_i\big] \in \mathbb{R}^{N\times (s+1)},
\end{equation*}
where $s$ is the step size (or block size\footnote{Throughout this work, we use \textit{block size} and \textit{step size} interchangeably to refer to multiple Krylov basis vectors that are generated at the same time. This terminology does not have the same connotation as \textit{block size} in block-GMRES literature, where each block refers to multiple right-hand sides.}) at each block iteration, and $p_j(z)$ is a polynomial of degree $j$ defined by a three-term recurrence
\begin{align*}
\begin{split}
    p_0(z) &= 1, \quad p_1(z) = (z-\theta_0)p_0(z)/\gamma_0, \\
    p_j(z) &= ((z-\theta_{j-1})p_{j-1}(z) - \beta_{j-2} p_{j-2}(z))/\gamma_{j-1}.
\end{split}
\end{align*}
The parameters of the polynomial function $(\theta_i, \beta_i, \gamma_i)$ can be condensed in a \textit{change of basis} matrix $B$, which is used as input to the MPK~\cite{hoemmen2010communication,ballard2014communication,carson2015communication}. Communication-avoiding MPK can be achieved~\cite{demmel2008avoiding,hoemmen2010communication} though standard SpMVs are often used to accommodate different preconditioners. The Krylov basis matrix $V(Q_i,s)$ is then orthogonalized using an appropriate QR factorization that ought to require as little communication as possible. The remaining downstream operations related to the upper Hessenberg matrix $H$ and the least-square problem are local to each processor, thus not affecting the communication pattern of the algorithm. The overall framework of the $s$-step GMRES is presented in \cref{alg:sgmres}, and detailed algorithm implementations can be found in~\cite{hoemmen2010communication,ballard2014communication,yamazaki2017improving}.

\begin{algorithm}[ht]
\caption{$s$-step GMRES}\label{alg:sgmres}
\begin{algorithmic}[1]
\REQUIRE $N\times N$ matrix $A$, right-hand side vector $b$, initial guess vector $x_0$, maximum iteration count $m$, step size \textit{s}, change of basis matrix $B$.
\ENSURE $x$, solution to the linear system $Ax=b$.
\STATE $r := b - Ax_0, \quad \beta := \|r\|_2, \quad Q_1 := r/\beta, \quad R_{1,1}=1, \quad i = 1$
\WHILE{$i \leq m$}
\STATE $V$ = MPK$(A, Q_i, s, B)$
\STATE $[Q_{i+1:i+s}, R_{1:i+s,i+1:i+s}]$ = Block QR($V, Q_{1:i}, s$)
\STATE Assemble upper Hessenberg matrix $H_{1:i+s,1:i+s-1}$
\STATE Apply Givens rotation to update matrix $H_{1:i+s,1:i+s-1}$
\STATE Check for convergence
\STATE $i = i + s$
\ENDWHILE
\STATE$y = $ argmin$\|\beta e_1 - H_{1:m+1,1:m}y\|_2$ \COMMENT{Least-squares problem}
\STATE$x = x_0 + Q_{1:m} y$
\end{algorithmic}
\end{algorithm}

\subsection{Numerical instability in the MPK}
\label{subsec:stability_MPK}
The first source of numerical instability of the $s$-step GMRES algorithm lies in the poor conditioning of the Krylov basis matrix $V(Q_i,s)$ generated from the MPK. The early work on $s$-step methods~\cite{walker1988implementation, chronopoulos1991s} started with the monomial basis,
\begin{equation}\label{eq:monomial}
    p_j(A) = A^j.
\end{equation}
But in finite precision, the Krylov basis vectors converge to the largest eigenvector of matrix $A$, making $V(Q_i,s)$ ill-conditioned or even numerically rank-deficient. Moreover, to avoid overflow or underflow, each vector obtained from SpMV needs to be normalized, leading to $s$ additional global communications that are undesirable. 

To slow the growth of the incremental condition number of $V(Q_i,s)$ for large values of $s$, many approaches have been tried. Bai et al.\ proposed a Newton basis GMRES implementation using approximate eigenvalues of $A$ as $\theta_i$'s~\cite{bai1994newton,hoemmen2010communication}. The Chebyshev basis has also been studied and has been shown to slow the growth of condition numbers with varying degrees of success~\cite{joubert1992parallelizable,joubert1992parallelizable2,philippe2012generation,ballard2014communication,carson2015communication}. The normalization step of the basis vectors can be eliminated in order to reduce global communications if $s$ is kept relatively small. Without normalization, methods such as matrix equilibration have been adopted to slow the growth/decay of the Krylov basis vector norms and increase $s$~\cite{hoemmen2010communication,ballard2014communication}. 

However, since most approaches are heuristic in nature, the maximum allowed value of $s$ for a given input matrix $A$ can only be determined by trial and error or operations involving expensive communication constructs. Due to the lack of an appropriate mechanism to monitor the conditioning of $V(Q_i,s)$, the ill-conditioning of the Krylov basis matrix $V(Q_i,s)$ poses challenges to the downstream QR factorization and undermines the stability of the overall algorithm.

\subsection{Numerical instability in block QR orthogonalization}
\label{subsec:stability_QR}
Block QR me\-th\-ods are commonly used to orthonormalize Krylov basis vectors in $V(Q_i,s)$ at the same time, giving the potential to reduce communication latency by a factor of $s$. To set the stage for a detailed discussion, we first introduce the formal notation for block QR orthogonalization.

For a matrix $\mathcal{V} \in \mathbb{R}^{N \times M}$, partitioned into $p$ blocks,
\begin{equation}
    \mathcal{V} = [\,V^{(1)}\,|\,V^{(2)}\,|\, \dots \,|\,V^{(p)}\,],
\end{equation}
we seek an ``economic" block QR factorization in the form of 
\begin{equation}
    \mathcal{V} = \mathcal{Q} R,
\end{equation}
with $\mathcal{Q} = [\,Q^{(1)}\,|\,Q^{(2)}\,|\, \dots \,|\,Q^{(p)}\,] \in \mathbb{R}^{N \times M}$ having the same block structure as $\mathcal{V}$ and $R\in\mathbb{R}^{M \times M}$. 
In this work, we focus on the \textit{loss of orthogonality} (LOO) error defined by  
\begin{equation}
    \|I - \mathcal{Q}^T\mathcal{Q}\|_F,
\end{equation}
as a measure of stability for QR factorization. Typically, block QR consists of two stages in each block iteration. Each block $\mathcal{V}^{(i)}$ first needs to be \textit{inter-orthogonalized} with respect to orthogonal bases generated from previous iterations ($Q^{(1)}$ to $Q^{(i-1)}$), then \textit{intra-orthogonalized} to ensure orthogonality among all the vectors within the same block.

The current state-of-the-art communication-avoiding $s$-step GMRES focuses on block classical Gram-Schmidt with reorthogonalization (BCGS2) for inter-ortho\-go\-na\-li\-zation and Cholesky QR (CholQR\footnote{Due to re-orthogonalization in inter-orthogonalization, CholQR is applied twice in total. However, we use the abbreviation CholQR instead of CholQR2 as each instance of intra-orthogonalization involves applying CholQR only once. We reserve the term CholQR2 for referring to applying CholQR twice \textit{within} one instance of intra-ortho\-go\-na\-li\-zation.}) for intra-orthogonalization (\cref{alg:BlockQR}). Block orthogonalization routines often utilize cache-friendly BLAS-3 kernels to minimize data movement among memory hierarchies~\cite{golub2013matrix}. In a distributed-memory setting, BCGS and CholQR only cost one global synchronization each in one block iteration. However, a re-orthogonalization step is often necessary to reduce the LOO error~\cite{barlow2013reorthogonalized,yamamoto2015roundoff}, doubling the cost of internode communication to four global synchronizations per block iteration. In~\cite{yamazaki2020low} a low-synchronization variant of BCGS2 with CholQR has been proposed that uses one global reduction per $s$ vector. 

\begin{algorithm}
\caption{Block QR in \cref{alg:sgmres} using BCGS2 and CholQR}  \label{alg:BlockQR}
\begin{algorithmic}[1]
\STATE $k = i+1:i+s$
\STATE
\STATE $W = Q_{1:i}^TV$ \COMMENT{First BCGS}
\STATE $V = V- Q_{1:i}W$
\STATE $Z = \mbox{Cholesky}(V^TV)$ \COMMENT{First CholQR}
\STATE $Q_{k} = V/Z$
\STATE
\STATE $R_{1:i, k} = Q_{1:i}^TQ_{k}$ \COMMENT{Second BCGS}
\STATE $Q_{k} = Q_{k} - Q_{1:i}R_{1:i, k}$
\STATE $\tilde{Z} = \mbox{Cholesky}(Q_{k}^TQ_{k})$ \COMMENT{Second CholQR}
\STATE $Q_{k} = Q_{k}/\tilde{Z}$
\STATE
\STATE $R_{1:i, k} = W + R_{1:i, k}Z$ \COMMENT{Combine both steps}
\STATE $R_{k, k} = \tilde{Z}Z$
\end{algorithmic}
\end{algorithm}

The reduction in communication cost of block QR methods has made them attractive for large-scale problems. However, for most block QR methods, formal proofs of their stability properties remain elusive. We refer the reader to~\cite{carson2022block} for a recent survey on the stability properties of common block QR methods based on Gram-Schmidt algorithms. The numerical experiments therein (e.g., Figs.\ 5 and 6) demonstrated that even the current state of the art is not unconditionally stable for handling arbitrary step sizes with finite precision. In this work, we focus primarily on the stability analysis of the original BCGS2 with the CholQR algorithm. The stability analysis of low-synchronization variants is left for future work.

For a numerically non-singular matrix $\mathcal{V}$ with condition number bound $\kappa(\mathcal{V}) < O(\epsilon^{-1})$, where $\epsilon$ is the unit round-off, Barlow showed that BCGS2 produce $O(\epsilon)$ orthogonality error if the intra-orthogonalization scheme can achieve the same level of orthogonality~\cite{barlow2013reorthogonalized}. The round-off error analysis performed in~\cite{yamamoto2015roundoff} indicates that CholQR applied twice does give the desired $O(\epsilon)$ level of LOO error but requires the input block to have a condition number below $O(\epsilon^{-1/2})$. Here, we note that the input block does not refer to the block $V^{(i)}$ in the original matrix $\mathcal{V}$ but to the block matrix used to generate the Gram matrix. (For example, notice how line 4 updates the block matrix $V$ in \cref{alg:BlockQR} before the CholQR step.) Without any safeguard on the conditioning of the input matrix, the output of CholQR can potentially degrade the numerical stability and lead to a significant increase in the LOO error of the overall block QR algorithm. Furthermore, we emphasize that even if efforts were made to improve and constrain the condition number of the Krylov basis matrix $V(Q_i,s)$ in the upstream MPK, the inter-orthogonalization process could nonetheless lead to an ill-conditioned input for the intra-orthogonalization. 

Sometimes, in a more catastrophic setting, an ill-conditioned input block matrix in CholQR can give rise to a Gram matrix that is not symmetric positive-definite (SPD) in finite precision, which will cause the Cholesky decomposition to break down. Such runtime errors often force the user to switch to other QR methods that are not communication-avoiding or restart the algorithm with a smaller block size $s$. 

\subsection{Limitations of current state-of-the-art methods} As the communication savings of $s$-step GMRES are directly related to the step size $s$, one would like to use a step size as large as possible. However, due to the numerical instabilities mentioned above, conservative values of $s$ (e.g., $5$, $10$) are often reported in the literature~\cite{hoemmen2010communication,ballard2014communication,yamazaki2017improving,yamazaki2020low} to mitigate numerical problems. On large-scale distributed computing architectures, finding the maximum allowed choice of $s$ of an arbitrary matrix by trial and error is highly inefficient and costly. To the best of our knowledge, no variant of $s$-step GMRES in the existing literature can determine a stable value of $s$ \textit{a priori} in a communication-avoiding fashion, nor can $s$ be dynamically adapted to ensure stability. Attempts were made to improve the stability of $s$-step GMRES using a predetermined sequence of block sizes, which is difficult to generalize~\cite{imberti2017varying}. An adaptive mechanism has been derived for the $s$-step conjugate gradient (CG) algorithm~\cite{carson2018adaptive} but does not translate to the $s$-step GMRES easily. One key difference is that the $s$-step CG method does not require an explicit block QR algorithm. Furthermore, the adaptive mechanism in $s$-step CG relies on the norm of the residual vectors, which is only available in $s$-step GMRES \textit{after} the block QR is completed. Thus, the numerical instabilities described in \cref{subsec:stability_QR} cannot be resolved using the same approach. By and large, the backward stability of $s$-step GMRES still remains an open problem, and the numerical instabilities greatly limit the potential communication savings in GMRES algorithms.

\section{Proposed solutions to resolve numerical instabilities in \texorpdfstring{$s$}{s}-step GMRES}
\label{sec:algorithm}
In this section, we aim to address the numerical instability in the current state-of-the-art $s$-step GMRES and derive an adaptive $s$-step GMRES algorithm that is numerically stable by construction. In particular, we target the $s$-step GMRES algorithm that uses BCGS2 for inter-orthogonalization and CholQR for intra-orthogonalization. We start by resolving the breakdown problem in CholQR in \cref{subsec:pchol}, which provides a path for us to design a dynamical mechanism in \cref{subsec:stopping} that monitors the step size $s$ for stability. We then provide the complete algorithm in \cref{subsec:adaptsgmres} and discuss its communication patterns and computational trade-offs. To further improve communication savings, we introduce \textit{scaled} Newton polynomials in \cref{subsec:scalednewton} as a basis to increase the step size $s$ and an initial step size estimator in \cref{subsec:errorestimator} to pick the optimal step size at the first iteration.

\subsection{Avoiding Cholesky breakdown in CholQR}\label{subsec:pchol}

As mentioned previously, an ill-conditioned input matrix to CholQR may produce a numerically non-SPD Gram matrix that causes the Cholesky decomposition to break down. To avoid this, we propose the use of a \textit{partial} Cholesky decomposition, which leads to a \textit{partial} CholQR factorization. 

Let $X \in \mathbb{R}^{N\times s}$ be the input to the CholQR factorization, and let $p$ be an integer such that
\begin{equation}
    X = [X_p\, |\, X_{s-p}],\qquad 1\leq p \leq s,
\end{equation}
where $X_p \in \mathbb{R}^{N\times p}$ represents the first $p$ columns of $X$ and $X_{s-p} \in \mathbb{R}^{n\times (s-p)}$ the remaining columns.\footnote{For brevity, the subscript in this section represents the number of rows/columns. This is to be differentiated from the previous notation, where a single subscript denotes a column vector (e.g., $Q_i$).} The first step of CholQR computes a Gram matrix $X^TX$,
\begin{equation}
    X^TX = \begin{bmatrix}
    X_p^TX_p & X_p^TX_{s-p} \\
    X_{s-p}^TX_p & X_{s-p}^TX_{s-p} \\
    \end{bmatrix}.
\end{equation}
If we represent $R$ as the upper triangular matrix obtained from Cholesky decomposition of $X^TX$,
\begin{equation}
    R = \text{Cholesky}(X^TX) = \begin{bmatrix}
    R_p & R_{p,s-p} \\
    0 & R_{s-p} \\
    \end{bmatrix},
\end{equation}
it follows that $R_p = \text{Cholesky}(X_p^TX_p)$. Hence, by using the triangular matrix $R_p$ obtained from Cholesky decomposition of $X_p^TX_p$ ($p^{th}$ order leading principal submatrix of $X^TX$), one can compute a \textit{partial} CholQR factorization of the matrix $X$ to obtain $p$ columns of the orthogonal basis $Q_p$:
\begin{equation}\label{eq:pchol}
\begin{split}
    Q_p & = X_p R_p^{-1}, \\
    X \, &= [Q_p R_p, X_{s-p}]. \\
\end{split}
\end{equation}
This formulation gives flexibility to stop the QR factorization at an arbitrary column of the input matrix $X$. In finite precision, this strategy can be used to avoid a Cholesky breakdown, as one can always salvage the factorization computed right before the breakdown. In the worst-case scenario of $p=1$ (i.e., Cholesky breaks down at the second row/column), partial CholQR is reduced to a norm computation, which is a backward stable floating point operation~\cite{higham2002accuracy}. 

The remaining columns, $X_{s-p}$, can be recursively orthogonalized. However, in the context of $s$-step GMRES, the remaining columns correspond to higher powers of a polynomial basis and are often harder to orthogonalize, leading to small values of block size $p$ in subsequent iterations. Sometimes, due to the ill-conditioned Krylov matrix from upstream MPK, the remaining columns can be linearly dependent on the previous block $X_p$ and are thus not worth orthogonalizing as they do not contribute to the construction of the Krylov subspace. Therefore, to maximize communication savings, the remaining columns are discarded, and the last column of $Q_p$ can be used to generate the next Krylov basis matrix in the next block iteration. This is justified on large-scale distributed machines as we avoid inefficient Cholesky breakdowns at the expense of some SpMVs that are relatively cheaper in terms of communication costs.

\subsection{Stopping criteria for partial CholQR} \label{subsec:stopping}
A standard implementation of Cholesky decomposition breaks down when the diagonal entry to be factorized is equal to or less than zero. The partial CholQR introduced in \cref{subsec:pchol} is sufficient to avoid Cholesky breakdowns by salvaging the calculated factorization. However, this does not guarantee minimal LOO. In other words, stopping at non-positive pivots in Cholesky is insufficient to fulfill the condition number bound for CholQR. A more stringent choice of $p$ in \cref{eq:pchol} is necessary to ensure that the columns $X_p$ have a condition number at most $O(\epsilon^{-1/2})$,
\begin{equation}\label{eq:pchol2}
    \kappa(X_p) = \kappa(R_p) \leq O(\epsilon^{-1/2}).
\end{equation}
As a result, an appropriate stopping criterion must be imposed after each rank-1 Cholesky downdate to bound the condition number growth while generating $R_p$. 

Expensive singular value decomposition (SVD) can be used to compute condition numbers for small dense matrices and was suggested in the rank-revealing TSQR algorithm~\cite{hoemmen2010communication} for condition number monitoring. However, since the algorithmic complexity to compute the SVD of $R_p$ is $O(p^3)$ and the stopping criterion must be invoked after each rank-1 update, the total cost to check against the stopping criterion is on the order of $O(p^4)$, exceeding the $O(p^3)$ complexity of Cholesky decomposition. This could be justified for small block sizes. 

If one wants to use large block sizes, the incremental condition estimator (ICE)~\cite{bischof1990incremental,bischof1991robust} can be used as a cheaper alternative. After each rank-1 update, ICE updates its condition number estimation using the newly computed column of $R_p$ without accessing the entire leading principal submatrix. Consequently, ICE costs only $O(p)$ flops at each iteration. An example of the resulting partial Cholesky decomposition is illustrated in \cref{alg:pcholICE}. 

We recognize that the condition number estimate provided by ICE is often an underestimation, but the discrepancy is generally within a factor of $10$~\cite{bischof1991robust}. In practice, we found that a condition number upper bound of $\Omega \sim O(10^{-1}\epsilon^{-1/2})$ is sufficient for \cref{eq:pchol2} to hold numerically.

\begin{algorithm}
\caption{Partial Cholesky decomposition}  \label{alg:pcholICE}
\begin{algorithmic}[1]
\REQUIRE $m\times m$ matrix $A$, and condition number upper bound $\Omega$
\ENSURE $j$, and $j \times j$ triangular matrix $R$ such that $A_{1:j,1:j} = R^TR$
\STATE $\kappa_0 = 1$
\FOR[Standard Cholesky]{$j = 1, ..., m$}
\FOR{$i = 1,...,j-1$}
\STATE$R_{i,j} = (A_{i,j} - \sum_{k=1}^{j-1}R_{k,i}R_{k,j})/R_{i,i}$
\ENDFOR
\STATE$R_{j,j} = \sqrt{A_{j,j} - \sum_{k=1}^{j-1}R_{k,j}^2}$
\IF{SVD}
\STATE $\sigma_{\max}, \sigma_{\min} = {\rm SVD}(R_{1:j,1:j})$ \COMMENT{Singular Value Decomposition}
\STATE $\kappa_j = \sigma_{\max}/\sigma_{\min}$
\ELSIF{ICE} 
\STATE $\kappa_j = {\rm ICE}(R_{1:j,j}, \kappa_{j-1})$  \COMMENT{Incremental Condition Estimator}
\ENDIF
\IF{$\kappa_j > \Omega$}
\STATE $j = j - 1$ 
\STATE break
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Adaptive \texorpdfstring{$s$}{s}-step GMRES algorithm}\label{subsec:adaptsgmres}

Using BCGS2 with a partial Chol\-QR factorization in $s$-step GMRES, we can dynamically monitor the growth of the condition number and adjust the block size $s$ to ensure that the orthogonalization process stays within the bounds of the condition number. An outline of the resulting adaptive $s$-step GMRES is presented in \cref{alg:adaptsgmres}. We make a few remarks on the overall algorithm's communication pattern, stability, and other practical aspects.

\begin{algorithm}
\caption{Adaptive $s$-step GMRES}  \label{alg:adaptsgmres}
\begin{algorithmic}[1]
\REQUIRE $N\times N$ matrix $A$, right hand side vector $b$, initial guess vector $x_0$, maximum iteration count $m$, \textit{initial} step size \textit{$s_0$}, change of basis matrix $B$, condition number upper bound $\Omega$.
\ENSURE $x$, solution to the linear system $Ax=b$.
\STATE $r := b - Ax_0, \quad Q_{1} := r/\|r\|_2, \quad R_{1,1} = 1,  \quad i = 1, \quad s = s_0$
\WHILE{$i \leq m$}
\STATE $V$ = MPK($A, Q_{i}, B, s$) 
\STATE
\STATE $W = Q_{1:i}^TV$ \COMMENT{First BCGS}
\STATE $V = V- Q_{1:i}W$
\STATE $[\,p,\, Z\,] = \mbox{Partial Cholesky}(V^TV, \Omega)$ \COMMENT{First partial CholQR}
\STATE $k = i+1:i+p$
\STATE $Q_{k} = V_{:,1:p}/Z_{1:p,1:p}$
\STATE
\STATE $R_{1:i, k} = Q_{1:i}^TQ_{k}$ \COMMENT{Second BCGS}
\STATE $Q_{k} = Q_{k} - Q_{1:i}R_{1:i, k}$
\STATE $[\, p\,, \tilde{Z}\,] = \mbox{Partial Cholesky}(Q_{k}^TQ_{k}, \Omega)$ \COMMENT{Second partial CholQR}
\STATE $k = i+1:i+p$
\STATE $Q_{k} = Q_{k}/\tilde{Z}$
\STATE
\STATE $R_{1:i, k} = W_{:,1:p} + R_{1:i, k}Z_{1:p,1:p}$ \COMMENT{Combine both steps}
\STATE $R_{k, k} = \tilde{Z}Z_{1:p,1:p}$
\STATE
\STATE $s = p$ \COMMENT{Update step size}
\STATE
\STATE Assemble upper Hessenberg matrix $H_{1:i+s,i:i+s-1}$
\STATE Apply Givens rotation to update matrix $H_{1:i+s,i:i+s-1}$
\STATE Check for convergence
\STATE $i = i + s$
\ENDWHILE
\STATE$y = $ argmin$\|\beta e_1 - H_{1:m+1,1:m}y\|_2$ \COMMENT{Least-squares problem}
\STATE$x = x_0 + Q_{1:m} y$
\end{algorithmic}
\end{algorithm}

The formal proof of the backward stability of the overall algorithm is reserved for future work. Here, we provide some conjectures on stability based on established results and verify the stability using numerical experiments in the next section.

The discussion in Section 4.2 of~\cite{carson2022block}, which builds on the analysis in~\cite{barlow2013reorthogonalized}, has already indicated that BCGS2 with a merely conditionally stable intra-orthogonalization scheme such as Classical Gram-Schmidt (CGS) can still give $O(\epsilon)$ LOO error for well-conditioned matrices. Since CholQR has the same stability bound and orthogonality error behavior as CGS, we conjecture that BCGS2 with CholQR is stable for well-conditioned matrices. For challenging matrices, the authors in~\cite{carson2022block} concluded that unconditionally stable intra-orthogonalization schemes, such as Householder QR, are necessary for overall stability, under the key assumption that each block to be factorized be of a fixed size $s$. In our work, using partial CholQR, we relax such an assumption and allow the algorithm to adjust the step size on the fly. This has several advantages from a numerical viewpoint.
\begin{itemize}
    \item As the block iteration continues, the subsequent blocks generally tend to be harder to orthogonalize than the first few blocks, as the inter-orthogonali\-za\-tion step is looking for new sets of orthogonal bases within the remaining subspace, which generally gets smaller with more block iterations. In such cases, the adaptive mechanism allows smaller step sizes for later blocks.
    \item For very ill-conditioned matrices where the adapted step size turns out to be $s = 1$, the partial CholQR is reduced to a norm computation. The overall block QR factorization is reduced to a column-wise CGS2 (Classical Gram-Schmidt with re-orthogonalization) algorithm, which is known to be stable either as a standalone QR factorization~\cite{daniel1976reorthogonalization,drkovsova1995numerical} or as an orthogonalization scheme used in GMRES~\cite{swirydowicz2020low}.
    \item With a large block size $s$, the MPK can generate Krylov matrices $V(Q_i, s)$ that could be numerically rank-deficient. Under such circumstances, there is no viable QR factorization, including unconditionally stable QR factorization schemes such as Householder QR that can expand the dimension of the Krylov subspace by $s$. The ability to reduce the block size $s$ and discard the remaining columns corresponding to higher powers of polynomial basis in partial CholQR avoids such problems. The algorithm always works on the maximum allowed set of Krylov vectors for orthogonalization.
\end{itemize}

By discarding some basis vectors in the Krylov basis matrix $V(Q_i, s)$, we acknowledge that some SpMV computations done earlier in the MPK are potentially wasted. The same issue was also reported in the adaptive $s$-step conjugate gradient method~\cite{carson2018adaptive}. Here, we argue that this is still desirable in a large-scale distributed-memory system for many applications (e.g., solutions to PDEs) where SpMVs require mostly Point-to-Point communication with neighboring nodes, but global synchronizations involve much more expensive collective communication costs. To minimize wasted effort, we update the step size at each block iteration (line 20 in \cref{alg:adaptsgmres}) since the maximum allowable step size is generally non-increasing. The initial step size, however, at the very first iteration is a user-defined value. Poor choice of the initial step size can still result in inefficient use of the algorithm. A step size that is too small may not maximize communication savings, while a step size that is too large may result in significant truncation of Krylov basis vectors, leading to wasted computations. 

To this end, the following subsections look into ways to maximize communication savings and minimizing wasted computations. We first introduce a polynomial basis in \cref{subsec:scalednewton} that allows for very large step sizes in many problems and then derive an estimator in \cref{subsec:errorestimator} to obtain well-informed initial step sizes. 

\subsection{Scaled Newton polynomials}\label{subsec:scalednewton}
A by-product of \cref{alg:adaptsgmres} is that the adaptive block size can serve as a metric for evaluating the effects of other measures on the stability limit of adaptive s-step GMRES. Previous discussions on polynomial bases in literature focus on the conditioning of the Krylov basis matrix $V(Q_i, s)$~\cite{joubert1992parallelizable,joubert1992parallelizable2,philippe2012generation,ballard2014communication,carson2015communication}. However, as we pointed out earlier in \cref{subsec:stability_QR}, the inter-orthogonalization process could alter the condition of the Krylov matrix, making them ill-conditioned for intra-orthogonalization. Hence, it is insufficient to only monitor the condition number growth of the $V(Q_i, s)$ at the end of MPK. Using the adaptive $s$-step GMRES algorithm, we would like to use the adapted step size $s$ as an additional metric to evaluate different polynomial bases from a stability point of view. 

In addition to the monomial basis introduced in \cref{eq:monomial}, we are also interested in Newton polynomials. The standard Newton polynomials are defined as
\begin{equation}\label{eq:newton}
    p_j(A) = \Pi_{i=1}^{j}{(A-\theta_i I)}.
\end{equation}
The shifts $\theta_i$ are Ritz values of $A$ in a Leja ordering. In practice, the Ritz values are obtained from $s$ iterations of classical Arnoldi or GMRES first. This is a common strategy for $s$-step GMRES implementations; see, e.g., \cite{bai1994newton,hoemmen2010communication,ballard2014communication} for further details. 

In this work, we introduce an additional variant of the Newton polynomials: \textit{scaled} Newton polynomials defined as
\begin{equation}\label{eq:scalednewton}
    p_j(A) = \Pi_{i=1}^{j}{(A-\theta_i I)/\gamma_i},
\end{equation}
where $\gamma_i$ are the scaling coefficients. The same Leja ordering of Ritz values applies. The standard Newton polynomial is considered a special case with $\gamma_i = 1$. 

In what follows, we propose a heuristic to choose $\gamma_i$. Our analysis will also help to understand the derivation of the initial step size estimator in \cref{subsec:errorestimator} and the effectiveness of the matrix equilibration in \cref{sec:test_matrixscaling}.

Let $V_j$ denote the $j$th column of $V=V(Q_i,s)\in\mathbb{R}^{N\times(s+1)}$, which corresponds to the $j$th Krylov basis vector to be orthogonalized. The scaled Newton polynomial can then be written as a two-term recurrence
\begin{equation}\label{eq:newtonrecurrence}
\begin{split}
    V_1 &= Q_i, \\
    V_j &= (AV_{j-1} - \theta_{j-1}V_{j-1})/\gamma_{j-1}.
\end{split}
\end{equation}
Recall that the first vector $V_1 = Q_i$ is an orthonormalized vector at the beginning of each block iteration, and $\theta_i$ are Ritz values that are approximations of the true eigenvalues. 

For our analysis, assume that $A$ is diagonalizable
\begin{equation}
    A = U\Lambda U^{-1},
\end{equation}
where $\Lambda$ and $U$ are the eigenvalue and eigenvectors of $A$. For simplicity, we assume that the eigenvalues are all real, though extensions to eigenvalues with complex conjugate pairs are straightforward. We express $V$ or $V_j$ using the eigenvectors of $A$ as
\begin{equation}\label{eq:vectordecomp}
    V = UC, \qquad V_j = UC_j.
\end{equation}
Substituting \cref{eq:vectordecomp} into \cref{eq:newtonrecurrence}, the entries in the matrix $C$ can be expressed using the recurrence
\[
    C_{i,j+1} = C_{i,j} \frac{\lambda_i - \theta_j}{\gamma_j}.
\]
Given the base case, $V_1 = UC_1$, another way to write the above relationship is
\begin{equation}\label{eq:Cmatrix}
    C_{i,j} = C_{i,1}\prod_{k=1}^{j-1}{\frac{\lambda_i-\theta_k}{\gamma_k}},\qquad 1 \leq i \leq N \text{ and } 1 \leq j \leq s+1.
\end{equation}
This relation describes the evolution of $V_j$ in the eigenspace of $A$ under the action of scaled Newton polynomials. One way to choose the scaling coefficients is to minimize the growth/decay of norm of each Krylov basis vector
\begin{equation}
    \Bigg\lvert\prod_{k=1}^{j-1}{\frac{\lambda_i - \theta_k}{\gamma_k}}\Bigg\rvert \sim O(1).
\end{equation}
Since in practice we do not have access to the true eigenvalue spectrum $\lambda_i$ but only Ritz values $\theta_i$, the scaling coefficients are therefore chosen as
\begin{equation}\label{eq:scalingcoeff}
    \gamma_i = |\bar{\theta} - \theta_i|,
\end{equation}
where $\bar{\theta}$ is the average of all Ritz values. 

Similar to the unscaled Newton polynomials, the Ritz values are obtained from $s$ iterations of classical Arnoldi or GMRES. The average is therefore taken over the $s$ Ritz values computed.

\cref{eq:scalednewton} and \cref{eq:scalingcoeff} complete the definition of scaled Newton polynomials. 

\subsection{Initial step size estimator for scaled Newton polynomials}\label{subsec:errorestimator}

As we will demonstrate in numerical experiments in \cref{sec:tests_stability}, the scaled Newton polynomials can give very large adapted step sizes in many cases, which is desirable for reducing communication latency. However, in certain cases (such as the one in \cref{sec:test_E20R5000_ILU}), the adapted step size from scaled Newton polynomials can be limited. By blindly setting a very large initial step size for arbitrary matrices, one might end up with relatively small step sizes, resulting in the removal of many columns of the Krylov basis matrix at the first block iteration. The wasted computation in such scenarios often can offset the gain in communication savings. In this section, we show that, using the eigendecomposition analysis in \cref{subsec:scalednewton}, an estimator can be derived that informs the choice of initial step sizes for scaled Newton polynomials.

From \cref{eq:Cmatrix} and \cref{eq:scalingcoeff}, the generation of Krylov basis vectors using scaled Newton polynomials can be expressed in the eigenspace of $A$ as
\begin{equation}\label{eq:errorindicator}
    C_{i,j} = C_{i,1}\prod_{k=1}^{j-1}{\frac{\lambda_i-\theta_k}{|\bar{\theta}-\theta_k|}},\qquad 1 \leq i \leq N, \; 1 \leq j \leq s+1.
\end{equation}
We are interested in scaled Newton polynomials in this particular form because by using Ritz values to approximate true eigenvalues, we can compute the product term approximately before the actual adaptive $s$-step GMRES algorithm without any communication cost. For illustration, the evolution of Krylov vectors $V_j$ and their corresponding eigenspace coefficients $C_{i,j}$ are visualized in \cref{fig:estimator}. Ideally, if the Ritz values $\theta_k$ are exactly equal to the true eigenvalues $\lambda_k$ and roundoff errors are absent, the numerators would cancel out with 
\[
    \lambda_i - \theta_k = 0,  \qquad \forall i = k.
\]
The relation \cref{eq:errorindicator} for all $1 \leq i \leq N$ can then be reduced to
\[
    C_{i,j} = 
    \begin{cases}
    C_{i,1}\prod_{k=1}^{j-1}{\frac{\lambda_i-\theta_k}{|\bar{\theta}-\theta_k|}},\qquad &1 \leq j \leq i, \\
    0,\qquad &i < j \leq s+1, \\
    \end{cases}
\]
The second case above implies that the strictly upper triangular entries of $C$ are all zero. Now, assuming that some eigenvalues are well-approximated with finite precision, $\lambda_i - \theta_k = O(\varepsilon)$, the second case in the above relation becomes
\begin{equation*}
    C_{i,j} = \Bigg(\prod_{k=1}^{i-1}{\frac{\lambda_i-\theta_k}{|\bar{\theta}-\theta_k|}}\Bigg)O(\varepsilon)\Bigg(\prod_{k=i+1}^{j-1}{\frac{\lambda_i-\theta_k}{|\bar{\theta}-\theta_k|}}\Bigg), \qquad i < j \leq s+1.
\end{equation*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Extra figures/estimator2.pdf}
    \caption{Evolution of Krylov vectors $V_j$ (top) and their corresponding coefficients $C_{i,j}$ in the eigenspace of $A$(bottom) under the action of Scaled Newton polynomials. The two representations are related via $V_j = UC_j$. The entries highlighted in red are $O(\varepsilon)$ if Ritz values are well-approximated.}
    \label{fig:estimator}
\end{figure}

In other words, the entries on the superdiagonal of $C$ (i.e., $C_{j,j+1}$) do not become strictly zero due to numerical error $O(\varepsilon)$. Such numerical error can either grow or decay in the subsequent application of scaled Newton polynomials depending on the evolution of the product term. Overall, the matrix $C$ is fully dense instead of being lower triangular. 

Recall that the overarching goal of introducing different polynomial bases is to minimize the condition number of the Krylov basis matrix $V$. Although it is difficult to achieve this for arbitrary starting vector, we can instead heuristically try to optimize for a less stringent metric, the vector-wise norm of Krylov basis vectors, $\|V_j\|_2$. In particular, we want $\|V_j\|_2$ to stay at $O(1)$ ($\|V_1\|_2 = 1$ by construction) since the exponential growth of $\|V_j\|_2$ inevitably increases the condition number of the Krylov basis matrix $V$ exponentially. In the eigenspace, this translates to $\|C_j\|_2 \approx O(\|C_1\|_2)$. Note that we do not have access to $C_j$ explicitly as the eigenvectors of $A$ are prohibitively expensive to compute, but we merely need to approximate the product term in \cref{eq:errorindicator} at every $j$ to monitor the growth of $\|C_j\|$. Therefore, we make use of the Ritz values from $s$ iterations of Arnoldi/GMRES to approximate the product term by defining an auxiliary matrix $E_{i,j} \in \mathbb{R}^{s\times s}$
\begin{equation}\label{eq:auxiliarymatrix}
    E_{i,j} = \begin{cases}
    \Pi_{k = 1}^{j-1} \frac{|\theta_i - \theta_k|}{|\bar{\theta} - \theta_k|} & j < i,\\
    \Big(\Pi_{k = 1}^{j-1} \frac{|\theta_i - \theta_k|}{|\bar{\theta} - \theta_k|}\Big) O(\varepsilon) & j = i,\\
    \Big(\Pi_{k = 1}^{i-1} \frac{|\theta_i - \theta_k|}{|\bar{\theta} - \theta_k|}\Big) O(\varepsilon) \Big(\Pi_{k = i+1}^{j-1} \frac{|\theta_i - \theta_k|}{|\bar{\theta} - \theta_k|}\Big) & i < j \leq s.\\
    \end{cases}
\end{equation}
In actual implementation, $O(\varepsilon)$ can be chosen as $\epsilon$, the unit round-off. Each row $i$ of $E_{i,j}$ estimates the evolution of the product term at polynomials of degree $j$ for the coefficients of $C_{i,:}$, including the evolution of the numerical error. The column-wise norm $\|E_{j}\|$ provides an estimate of the ratio $\|C_j\| / \|C_1\|$. Once $\|E_{j}\|$ grows significantly beyond $O(1)$, the scaled Newton polynomials will very likely increase the vector-wise norm of the Krylov basis vectors $V_j$. Since $\|V_1\|_2 = 1$, subsequently increasing the norm of $\|V_j\|_2$ will result in growing incremental condition number of the overall Krylov basis matrix $V$, leading to small step size for stable orthogonalization. 

Let $\Omega_\text{est}$ be a user-specified threshold significantly greater than 1. We propose a way to estimate the initial step size via 
\begin{equation}
    s_0^* = \argmax_j \{\|E_{j}\| < \Omega_\text{est}\} 
\end{equation}
where $s_0^*$ denotes the maximum initial step size predicted. As the Ritz values are available on all processes, the estimator requires no communication cost and can be used to minimize the number of columns that partial CholQR has to truncate at the \textit{first} block iteration. In our experience, a threshold of $\Omega_\text{est} = 10^{-1}\epsilon^{-1/2}$ in for the estimator does a good job at limiting the initial step size and gives reasonable prediction $s_0^*$, though more conservative values can be used.

\section{Results on numerical stability}\label{sec:tests_stability}

We numerically verify the proposed adaptive $s$-step GMRES algorithm to demonstrate the convergence and stability of the algorithm. In this section, we implement \cref{alg:adaptsgmres} in MATLAB and use double precision for all tests where $\epsilon \approx 2^{-53}$. As the work here focuses on the algorithm's stability, it suffices to verify the implementation sequentially since parallel implementations only introduce additional data movements that do not affect the stability of the overall algorithm. Benchmarking the algorithmic scalability in a distributed network will be presented in \cref{sec:scalability}.

The metrics we are interested in are the 2-norm of the relative residual at iteration $i$,
\begin{equation}
    \|b - Ax_i\|_2/\|b-Ax_0\|_2,
\end{equation}
as well as the LOO error of the block QR scheme,
\begin{equation}
    \|I - Q_{1:i}^TQ_{1:i}\|_F,
\end{equation}
where $Q$ contains the orthogonal basis vectors. 

We are also interested in the adapted block size $s$ since it can serve as a metric for evaluating the effects of other measures on the stability limit of adaptive $s$-step GMRES. 

In all tests, an initial step size $s_0$ is set at the beginning of the numerical experiment. The stopping criteria for partial CholQR are imposed at $\Omega = 10^7$. ICE is used for condition number bounds and is verified against SVD estimates (i.e., \texttt{cond} in MATLAB).

\subsection{Numerical stability of the base algorithm}
We first start with the simplest form of the algorithm in this subsection. We use a \textit{monomial} basis defined by \cref{eq:monomial} in the MPK, and no preconditioners are used to condition the linear systems. Although convergence tends to be slow in the absence of preconditioners for general problems, we place our emphasis on the agreement between the standard GMRES algorithm and the adaptive GMRES algorithm. Numerical tests with different polynomial bases and preconditioning techniques are delayed to \cref{sec:tests_polynomialbasis} and \cref{sec:tests_preconditioners}.

\subsubsection{Diagonal matrix}\label{subsubsec:test_diagonalmatrix}

To demonstrate the stability and convergence of the algorithm, we first use a diagonal matrix of size $N=10^4$ with evenly distributed eigenvalues in $(0.1, 10)$. The initial step size is $s_0 = 10$. The top sub-figure in \cref{fig:diagonalmatrix} shows the relative residual and LOO error for both the standard GMRES and the adaptive $s$-step GMRES algorithm (with a monomial basis). The convergence of the adaptive algorithm is in good agreement with the GMRES baseline. The LOO of GMRES increases gradually due to the Modified Gram-Schmidt (MGS) orthogonalization used in the standard Arnoldi procedure. The LOO error of the proposed algorithm stays at $O(\epsilon)$. Note that the LOO error associated with BCGS2 with CholQR for the same matrix and monomial basis was also analyzed in Fig.~5 of~\cite{carson2022block}. Their numerical experiments show a significantly higher LOO error because the authors chose a constant step size of $s=10$, which is above the stability limit of the QR scheme. The lower left sub-figure in \cref{fig:diagonalmatrix} shows the block size at each iteration. The larger the block size, the fewer block iterations are used for the same number of iterations. Therefore, the ``area under the graph'' for the block size plot indicates the amount of potential communication savings. With an initial user-defined step size of $s_0=10$, the algorithm adapted the step size to $s=6$. If we plot the incremental condition number growth of the triangular matrix (i.e., $R_{1:j,1:j}$ in \cref{alg:pcholICE}) for the first partial CholQR instance in the first block iteration, we also observe that $\kappa(R_{1:j,1:j}) > O(\epsilon^{-1/2})$ after $s=6$, indicating that larger value of $s$ would become unstable for intra-orthogonalization. Therefore, the algorithm has to truncate the generated Krylov basis matrix down to $s=6$ at the first block iteration and continue with this step size to maintain stability in the orthogonalization process. We also point out that in \cref{fig:diagonalmatrix}, ICE gives reasonably accurate estimates of the incremental condition number compared to the more expensive SVD decomposition. In general, we find that $\Omega = 10^7$ is usually sufficient to bound the condition number in double precision arithmetic.

\input{Figures_file/Diagonal} %% lengthy figures are isolated into individual files

\subsubsection{2D Laplace}\label{sec:test_2Dlaplace}

The second matrix comes from solving the standard 2D Laplace problem on a $400^2$ uniform grid using a 5-stencil finite difference discretization, resulting in a matrix dimension of $N=1.6\times10^5$. A maximum inner iteration of 100 with 5 restarts is used. Without any preconditioner, the relative residual slowly converges. However, excellent agreement for residuals is still observed in \cref{fig:2Dlaplace}. LOO of adaptive $s$-step GMRES is kept at $O(\epsilon)$. The periodic pattern of LOO is because of the reconstruction of orthogonal bases at the beginning of each restart. 

It should be noted that since the maximum inner iteration is capped at 100 (which is not divisible by $s=6$ as given by \cref{fig:2Dlaplace}), some Krylov basis vectors in the last block are discarded, even though they do not undermine the stability of the algorithm. This is done only for benchmarking purposes, since we are comparing this with the GMRES baseline. In practice, one is free to extend the maximum inner iterations within each restart to retain the entire last block of Krylov basis vectors for better convergence and communication savings.

\input{Figures_file/2DLaplace}  %% lengthy figures are isolated into individual files

\subsection{Numerical experiments with different polynomial bases}\label{sec:tests_polynomialbasis}
In this subsection, we compare the monomial basis \cref{eq:monomial}, the standard Newton polynomial basis \cref{eq:newton} as well as the scaled Newton polynomial basis \cref{eq:scalednewton} with the baseline GMRES. The primary emphasis is placed on the adapted step size for various bases.

\subsubsection{Diagonal matrix}
We use the same diagonal matrix as \cref{subsubsec:test_diagonalmatrix} with different polynomial bases. The initial step size is $s_0 = 100$ to illustrate the stability limit associated with each polynomial basis. The results are summarized in \cref{fig:diagonalmatrix_poly}. All polynomial bases give stable results that agree well with the baseline GMRES. In terms of step sizes, all polynomial bases outperform the monomial basis. We would like to highlight that the scaled Newton polynomial stands out because the adapted step size is the same as the number of iterations $s = 100$. In other words, the implementation using scaled Newton polynomials requires only \textit{one} block iteration for the problem to converge. This is because of the much better conditioned Krylov basis matrix, as shown in the lower left sub-figure of \cref{fig:diagonalmatrix_poly}. The difference in the adapted step sizes is reflected in the vector-wise norm of the Krylov basis matrix. We observe that the scaled Newton polynomials are capable of keeping the column vector norms to $O(1)$ for a large number of iterations, thus helping to slow down the condition number growth of the Krylov basis matrix.

\input{Figures_file/Diagonal_Poly}  %% lengthy figures are isolated into individual files

\subsubsection{E20R5000 matrix}\label{sec:test_E20R5000}
The next test matrix is E20R5000 from Matrix Market~\cite{boisvert1997matrix}, which models 2D fluid flow in a driven cavity. The matrix is known to be difficult for Krylov iterative solvers. In the absence of any preconditioning technique, the relative residual decreases very slowly, as shown in \cref{fig:E20R5000}. However, the various implementations agree with the baseline GMRES algorithm, and the LOO errors are near machine accuracy. With an initial step size of $s_0=150$, the scaled Newton polynomial allows the largest stable step size among all polynomial bases. However, there is a sharp decrease in the step size after the first block iteration. To understand this, we further plot the incremental condition number of both the input block to partial CholQR ($\kappa(R_{1:j,1:j})$) and the Krylov basis matrix ($\kappa(V_{1:j,1:j})$) in the two lower subfigures of \cref{fig:E20R5000}. We observe that the Krylov basis matrix is reasonably well-conditioned in both the first and second iterations, but the input block to partial CholQR becomes ill-conditioned rapidly in the second block iteration. This is attributed to the inter-orthogonalization process. The Krylov basis matrix in the second block iteration has to be inter-orthogonalized with respect to the first block of size $s\approx 100$ before partial CholQR factorization. The update in the inter-orthogonalization step altered the Krylov basis matrix and caused the condition number to grow exponentially. This demonstrates that a good choice of polynomial basis in MPK does not necessarily guarantee that the orthogonalization is stable for a fixed step size. 

In practice, especially on large distributed machines, one could use a smaller restart length and more restarts to trade convergence for communication savings. For instance, if one is satisfied by the large adapted step size of the first block iteration, restarting immediately after one block iteration gives a minimal number of global synchronizations for a fixed number of total iterations. 

\input{Figures_file/E20R5000}  %% lengthy figures are isolated into individual files

\subsection{Numerical experiments with different preconditioning techniques}\label{sec:tests_preconditioners}

Preconditioners can be easily incorporated into \cref{alg:adaptsgmres}, and the analysis presented so far for generic matrices extends to preconditioned matrices. We first revisit a particular class of preconditioning techniques, matrix equilibration, which is frequently used in the $s$-step GMRES literature for stability considerations rather than to speed up convergence. We numerically investigate the effectiveness of matrix equilibration on conditioning Krylov basis matrices. Subsequently, we present the numerical stability of the adaptive $s$-step GMRES for a more general preconditioning technique, incomplete LU factorization (ILU).

\subsubsection*{Matrix equilibration}\label{sec:test_matrixscaling}

Matrix equilibration has been commonly used in the $s$-step GMRES literature~\cite{hoemmen2010communication,ballard2014communication, carson2015communication} as a preconditioning technique to normalize the matrix. By scaling the spectral radius to $O(1)$, matrix equilibration has been found to be effective in conditioning Krylov basis matrices for different polynomial bases. Ballard et al.\ relied entirely on matrix equilibration techniques and found that standard Newton polynomials without any scaling are sufficient (Section 8.5.1 of~\cite{ballard2014communication}). With the introduction of \textit{scaled} Newton polynomials in this work, we reinvestigate matrix equilibration methods for Newton polynomials.

Matrix equilibration aims to scale the rows and/or columns of the matrix in order to prevent the rapid growth/decay of Krylov basis vectors. In general, matrix equilibration replaces matrix $A$ by
\begin{equation}\label{eq:equil}
    A' = D_r A D_c,
\end{equation}
with two diagonal matrices $D_r$ and $D_c$. There are many equilibration strategies. Here, we consider two approaches: scalar scaling and column scaling. Scalar scaling scales the entire matrix by a constant (e.g., $D_r = D_c = \sqrt{\alpha}I$) without modifying the relative position of eigenvalues. The constant is often chosen to be related to the spectral radius of $A$ to normalize the spectrum. Column scaling scales each column of the matrix (e.g., $D_r = I$), leading to a modified spectrum of magnitude $O(1)$. Both scaling methods do not require any form of communication. Column scaling can be efficiently computed if the sparse matrix is stored in Compressed Sparse Column (CSC) format. (The same argument applies to row scaling if the sparse matrix is stored in Compressed Sparse Row (CSR) format instead.)

\subsubsection{E20R5000 matrix with matrix equilibration}
The E20\-R5000 matrix problem of \cref{sec:test_E20R5000} is preconditioned with the two matrix equilibration techniques. The spectral radius of $A$ is used for the scalar scaling, i.e., $\alpha = \max{|\theta_i|}$. In both cases, the relative residual plots show good agreement with the baseline GMRES and the LOO is minimal. However, comparing the step size adapted in \cref{fig:E20R5000_scaling} with those in \cref{fig:E20R5000}, we observe different levels of improvement for the two Newton polynomials with different scaling methods. The observation can be explained by drawing an analogy to the approximation theory. As noted in~\cite{beckermann2000condition, hoemmen2010communication}, the generation of the Krylov basis using polynomials is equivalent to a polynomial interpolation with the eigenvalues of $A$ being the interpolation nodes. The quality of the interpolation nodes is directly related to the incremental condition number growth of Krylov basis matrices. 

The eigenvalue spectra for the original matrix and preconditioned matrices are plotted in \cref{fig:E20R5000_spectrum}. Scalar scaling effectively scales the interpolation interval to $O(1)$. Hence, monomial and standard Newton polynomial bases benefit from such normalization, and the adapted step sizes increase. The scaled Newton polynomials see no improvement for scalar scaling because of the scaling coefficient chosen in \cref{eq:scalingcoeff}. In other words, scaled Newton polynomials are insensitive to scalar scalings. Note that even with a normalized spectrum, there is still a significant difference between the adapted step sizes of two Newton polynomials under scalar scaling. The difference comes from the difference in the scaling coefficient. Standard Newton polynomial under scalar scaling is equivalent to a scalar-scaled Newton polynomial with a constant scaling coefficient of $\gamma_i = \alpha$ in \cref{eq:scalednewton}. The scaled Newton polynomials defined by \cref{eq:scalingcoeff}, on the contrary, take into account the relative position of each eigenvalue with the mean, leading to better scaling behaviors and, thus, larger maximum allowable step sizes.

In addition to the normalization effect seen in scalar scaling, column scaling further modifies the spectrum and results in a different distribution of eigenvalues. As a result, we see a significant change in block size for both Newton polynomials in the bottom two sub-figures in \cref{fig:E20R5000_scaling}. In this case, the redistributed eigenvalues lead to a better quality of interpolation for Newton polynomials and give rise to increased step sizes. Monomials are only benefited by the normalization effect and therefore same level of improvement is observed for both matrix scaling methods. 

In summary, matrix equilibration generally has two combined effects on the eigenvalue spectrum. It not only attempts to normalize the spectrum to unit magnitude, but also modifies the relative eigenvalue distribution. We demonstrated the robustness of the scaled Newton polynomial, whose performance is insensitive to scalar scaling and depends mainly on the eigenvalue spectrum. This gives rise to a revised interpretation of the need for matrix equilibration that was deemed necessary in \cite{hoemmen2010communication,ballard2014communication,carson2015communication} for large step sizes. With the utilization of scaled Newton polynomials, attention can be directed towards the modification of eigenvalue distribution for large step sizes, which may be achieved through many alternative preconditioning methods.

\input{Figures_file/E20R5000_Scaling} %% lengthy figures are isolated into individual files
\input{Figures_file/E20R5000_spectrum} %% lengthy figures are isolated into individual files

\subsubsection{2DLaplace matrix with ILU preconditioner}
As most test cases presented so far were not preconditioned with an effective preconditioner, they have not been converged down to sufficiently small residuals (except for tests with the diagonal matrix). For completeness, we used subsequent tests to demonstrate the stability of \cref{alg:adaptsgmres} up to a satisfactory level of convergence.

The same matrix problem as \cref{sec:test_2Dlaplace} is used with ILU(0) preconditioner. The initial step size is $s_0 = 400$. As shown in \cref{fig:test_laplace_precond}, convergence is accelerated by the presence of a preconditioner. However, all implementations with various polynomial bases agree with the GMRES baseline. A minimal LOO error is observed for all cases. It is worth mentioning that the adapted step size for scaled Newton polynomials is 400, indicating that one single block iteration is sufficient for the linear system to converge with guaranteed stability.

\input{Figures_file/2DLaplace_ILU} %% lengthy figures are isolated into individual files

\subsubsection{E20R5000 matrix with ILU preconditioner}\label{sec:test_E20R5000_ILU}
The E20R5000 matrix with the ILU(0) preconditioner does not converge well. As an alternative, we present the results in which the matrix is preconditioned using ILUTP (ILU with threshold and pivoting) with a drop tolerance of $10^{-4}$. Using $s_0 = 75$, an accelerated convergence is observed in \cref{fig:E20R5000_ILU} with stable orthogonalization. Even though the matrix is better conditioned for convergence, the modified eigenvalue spectrum in \cref{fig:E20R5000_ILU} shows an outlying eigenvalue around $\lambda_\text{max} = 80$ that is separated from the rest of the spectrum. Such outlying eigenvalues degrade the quality of eigenvalues as interpolation nodes, and the incremental condition number grows rapidly. Hence, a much smaller step size is observed, even for scaled Newton polynomials. This shows that preconditioning does not necessarily lead to larger step sizes for polynomial bases in the MPK. 

\input{Figures_file/E20R5000_ILU} %% lengthy figures are isolated into individual files



We demonstrate the initial step size estimator using four examples, and the results are presented in \cref{fig:errorestimator}. In all examples, we show the eigenvalue spectrum on the left. On the right, we plot the incremental condition number of the Krylov basis matrix $V$ generated by scaled Newton polynomials (i.e., $\kappa(V_{1:j})$), the column-wise norm of $V$ (i.e., $\|V_j\|$), and the corresponding column-wise norm prediction $\|E_j\|$ from the initial step size estimator.

We first use two examples of diagonal matrices with known eigenvalues. Both diagonal matrices are of size $N=200$, and we use $s_0 = 200$ in both tests. The first matrix has uniform eigenvalues $\lambda_i \in (1, 200)$. The second matrix has the same eigenvalue distribution except that the largest eigenvalue is increased to $\lambda_\text{max} = 2000$. In both test cases, the estimator prediction $\|E_j\|$ shows the same trend as the actual column-wise norm $\|V_j\|$. As an increasing column-wise norm inevitably increases the incremental condition number, $\kappa(V_{1:j})$ grows at a faster rate than the predicted and actual column-wise norms. The incremental condition number of the first test matrix increases beyond $O(10^8)$ at $j = 62$, indicating an initial step size of $s_0 = 200$ would have to be truncated down to 62 Krylov basis vectors for stable orthogonalization. The initial step size estimator based on column-wise norms predicts $s_0^* = 134$, significantly reducing the number of wasted Krylov vectors. Similarly, the initial step size estimator for the second matrix predicts $s_0^* = 17$ while $\kappa(V_{1:j})$ reaches $O(10^8)$ at $j=15$.

To illustrate the necessity of considering numerical errors in the estimator, we also include $\|E^L_j\|$ in the two test cases in \cref{fig:errorestimator}, where $E^L$ is the strictly lower triangular part of the auxiliary matrix $E$. In other words, $E^L$ denotes the evolution of the product term when Ritz values are exactly equal to the true eigenvalues without roundoff errors. In both tests, $\|E^L_j\|$, unlike $\|E_j\|$, stays at a relatively constant order of magnitude without growing exponentially. This indicates that the exponential growth of $\|V_j\|$ is largely due to roundoff errors in Ritz value approximations. The onset of exponential growth is only captured if we consider the evolution of roundoff errors in the upper triangular part of $E$, justifying the definition of the auxiliary matrix $E$ in~\eqref{eq:auxiliarymatrix}. The exact column at which exponential growth starts depends on the eigenvalue spectrum. In the second test case with $\lambda_\text{max} = 2000$, the roundoff error at $i = 1$ is constantly increased by a factor of $O(10)$ with every $j$, leading to the rapid growth of roundoff error and therefore small initial step size.

The next two examples are the unpreconditioned E20R5000 matrix from \cref{sec:test_E20R5000} and ILUTP-preconditioned E20R5000 matrix from \cref{sec:test_E20R5000_ILU}. The eigenvalue spectra are reproduced in \cref{fig:errorestimator} for ease of comparison. The predictions of the numerical error given by the initial step size estimator in both cases show excellent agreement with the growth rate of the column-wise vector norm of the Krylov basis vectors. A threshold of $\Omega_\text{est}=10^7$ restricts the initial step size to $s_0^*=113$ and $s_0^*=13$ for the two cases, respectively. Both estimates are very close to the adapted step size shown in \cref{fig:E20R5000,fig:E20R5000_ILU}.

In all examples, we show that the initial step size estimator correctly captures the growth rate of the column-wise norm of Krylov basis matrices. Since the increased vector norm unavoidably increases the incremental condition number, one could use the estimator to upper-bound the initial step size such that the vector norm does not increase beyond a certain threshold. This, in turn, limits the growth of the incremental condition number and leads to a well-informed step size, ultimately reducing the number of unnecessarily generated Krylov basis vectors.

\input{Figures_file/ErrorEstimator}

\subsection{Numerical experiments on scaled Newton polynomial with initial step size estimator}\label{sec:tests_final}

In this series of tests, we incorporate the scaled Newton polynomial with the initial step size estimator in \cref{subsec:errorestimator} for the adaptive $s$-step GMRES algorithm in \cref{alg:adaptsgmres}. We show the robustness of the overall algorithm by testing the framework on a wide range of matrices from the Suite Sparse Matrix Collection~\cite{davis2011university}. All matrices are of size $N>10^6$. Similar to the tests presented above, the relative residual agrees well with the GMRES algorithm with minimal LOO for all tests. With an initial step size of $s_0=500$, we plot the maximum adapted step size for each matrix as well as the informed step size $s_0^*$ given by the estimator in \cref{fig:suitesparse}. In many cases, the adapted step size is $O(100)$ without relying on any preconditioning or matrix equilibration techniques. With a very large starting step size, the estimator with a threshold of $\Omega_\text{est}=10^7$ is also shown to be capable of adjusting the initial step size close to the adapted step size before the actual computation begins. 

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        width=.9\textwidth,
        height=.5\textwidth,
        bar width=3pt,
        ybar,
        enlarge x limits=0.1,
        xmin=cage14,
        xmax=NACA0015,
        ymin=0,
        ymax=550,
        ytick={0,100,200,300,400,500},
        ylabel={block size, $s$},
        symbolic x coords={cage14, Hamrles3, G3\_circuit, thermal2, ecology1, atmosmodd, atmosmodj, atmosmodl, atmosmodm, roadNet-CA, roadNet-PA, roadNet-TX, delaunay\_n20, belgium\_osm, NACA0015},
        xtick=data,
        xticklabel style={rotate=90},
        % nodes near coords,
        % nodes near coords align={vertical},
        ymajorgrids=true,
    ]
    \addplot [
    fill = black,
    ] coordinates {(cage14,17) (Hamrles3,500) (G3\_circuit,14) (thermal2, 169) (ecology1, 246) (atmosmodd, 75) (atmosmodj, 76) (atmosmodl, 68) (atmosmodm, 58) (roadNet-CA, 122) (roadNet-PA, 139)(roadNet-TX, 106)  (delaunay\_n20, 116) (belgium\_osm, 128) (NACA0015, 313)}; % actual
    \addplot [
    pattern = dots,
    ] 
    coordinates {(cage14,48) (Hamrles3,500) (G3\_circuit,35) (thermal2, 190) (ecology1, 257) (atmosmodd, 80) (atmosmodj, 85) (atmosmodl, 84) (atmosmodm, 70) (roadNet-CA, 138) (roadNet-PA, 154) (roadNet-TX, 120) (delaunay\_n20, 134) (belgium\_osm, 148) (NACA0015, 360) }; % estimator
    \legend{Adapted step size, Estimator prediction ($s_0^*$)}
    \end{axis}
    \end{tikzpicture}
    \caption{Matrices from SuiteSparse Matrix Collection. The adaptive $s$-step GMRES algorithm with scaled Newton polynomials is used in all cases. With an initial step size of $s_0=500$, the adapted step size as well as the predicted step size given by the estimator $s_0^*$ are shown for each matrix. The larger the block size, the more communication savings can be achieved. The smaller the discrepancy between two step sizes, the less computation and/or communication is wasted.}
    \label{fig:suitesparse}
\end{figure}

\section{Results on parallel scaling performance}\label{sec:scalability}

% \cmt{These results are pretty amazing. I think you should consider moving this section earlier. This shows the impact and potential of this method. You want the readers to see this first. You show performance and im\-pro\-ve\-ment in speed, then look at stability, accuracy, etc.}

Numerical experiments in the previous section are conducted sequentially. This section investigates the parallel scalability of the proposed algorithm in a distributed-memory setting. For this purpose, we implement \cref{alg:adaptsgmres} in C++ where local matrix computations are implemented using the Eigen library~\cite{eigenweb} and inter-process communication for SpMV and global reductions over the distributed network are implemented using the Message Passing Interface (MPI). For comparison, we implement the standard GMRES algorithm (with MGS Arnoldi) in a similar fashion as the baseline. All performance benchmarks are done on Intel Xeon Platinum 8280 processors with 56 cores in each node. We assign 1 MPI rank to each core as the mode of parallelization.

\subsection{Weak scaling}

We present weak scaling of our proposed algorithm using 3D Laplace matrices arising from 7-stencil finite difference discretization of uniform cubic grids. This matrix choice gives us the freedom to vary the dimension of the matrix without changing the sparsity pattern and its eigenvalue spectrum, allowing for fair comparisons. We vary the dimension of the matrix from 1M to 256M and the number of nodes from 1 to 256 (56 to 14,336 cores). For all benchmarks, we run 100 iterations without any restart. No preconditioners are used so that all iterations are completed without premature convergence. For adaptive $s$-step GMRES algorithms, we use a step size of $s_0 = 100$. The initial step size estimator predicts $s_0^* = 100$ and the actual implementation produces the same step size as well in all cases. Hence, there is only one block iteration involved in each run of the adaptive $s$-step GMRES algorithm. The timing breakdown for SpMV and orthogonalization steps (both computation and communication) is shown in \cref{fig:weakscaling}.

First, we note that the most significant time difference between the adaptive $s-$step GMRES and the standard GMRES algorithm comes from the global synchronization time of reduction operations. the global synchronization time for GMRES (black bar in \cref{fig:weakscaling}) increases significantly and gradually dominates the overall algorithm time as the number of nodes increases. However, the adaptive $s$-step GMRES algorithm shows a minimal synchronization time for orthogonalization, as there is only one block iteration involving four global reductions in total. Hence, the orthogonalization time including both computation and communication shows constant time even up to 256 nodes (14,336 cores).

Second, the computational time for orthogonalization for GMRES (green bar in \cref{fig:weakscaling}) increases, while the computational time of ortho\-go\-na\-li\-zation for our commu\-ni\-ca\-tion-avoiding variant is constant. This is because orthogonalization in MGS-GMRES depends on BLAS-2 operations for orthogonalizing one Krylov vector at a time, whereas adaptive $s$-step GMRES relies on BLAS-3 operations in BCGS to orthogonalize multiple Krylov vectors at once. This highlights that our algorithm can avoid data movement even within a single node by leveraging fast memory, leading to scalable performance.

Thirdly, the scaling performance of SpMV operations (blue bar in \cref{fig:weakscaling}) in both cases is fairly similar. Both implementations use the same kernel for SpMV operations but MPK in adaptive $s$-step GMRES algorithm requires an additional vector-wise update for polynomial bases other than the monomial basis. Nonetheless, the communication-avoiding GMRES gives slightly better performance. We conjecture that this is due to the consecutive application of SpMVs in MPK that can explore efficient fast memory usage. 

Lastly, other miscellaneous operations such as updating the upper Hessenberg matrix and checking for stopping criteria are included, but their timings are minimal compared to computationally intensive tasks such as SpMVs and orthogonalizations. Therefore, their timings do not affect the scaling performance of the algorithm.

Overall, we demonstrate weak scaling of the adaptive $s$-step GMRES algorithm up to 256 nodes (14,336 cores) for 3D Laplace matrices. 

\input{Figures_file/WeakScaling}

\subsection{Strong scaling}
For strong scaling, we use a 3D Laplace matrix with a dimension of 64M on 1 to 64 nodes (56 to 3,584 cores). Similar to the weak scaling tests, we allow all benchmarks to run 100 iterations without any restart. A step size of 100 is used in all adaptive $s$-step GMRES algorithms. The initial step size estimator predicts $s_0^* = 100$ and agrees with the actual initial step size in all benchmarks. The time breakdown is shown in \cref{fig:strongscaling}. Due to the large variation in magnitude, the corresponding speedup in log-scale is shown in \cref{fig:strongscaling_speedup}.

The main observation here is that even at 1 node, the adaptive $s$-step GMRES algorithm significantly outperforms the standard GMRES algorithm. Most of the speedup comes from the difference in computational cost of orthogonalization operations, as adaptive $s$-step GMRES algorithm makes use of BLAS-3 kernels with better data locality. This difference is consistently observed from 1 node to 64 nodes. The trend is better represented in \cref{fig:strongscaling_speedup}. Orthogonalization costs (both computation and communication) in standard GMRES shows linear scaling from 1 to 8 nodes at first, followed by superlinear scaling up to 32 nodes due to increased amount of cache memory, and then degraded performance at 64 nodes due to overwhelming communication costs. On the other hand, orthogonalization operations for the adaptive $s$-step GMRES show a perfectly linear speedup, and approximately $7\times$ improvement with respect to orthogonalization costs in standard GMRES at 1 node.

In addition, both algorithms have similar SpMV/MPK costs, as they use the same SpMV kernels. In consequence, the scaling performance of the overall algorithm in \cref{fig:strongscaling_speedup} closely follows the scaling performance of orthogonalization operations. In general, the adaptive $s$-step GMRES algorithm shows linear (strong) scaling up to 64 nodes. 

\input{Figures_file/StrongScaling}

\input{Figures_file/StrongScaling_plot}

\section{Conclusions}
\label{sec:conclusions}
We have developed a communication-avoiding adaptive $s$-step GMRES algorithm that is numerically stable. The algorithm uses a partial CholQR procedure that ensures that the block orthogonalization schemes are carried out within the stability limit by adjusting the step size on the fly. The overall orthogonalization produces minimal LOO errors, which are near the machine epsilon. With the introduction of scaled Newton polynomials, parallel scaling performance of the proposed algorithm in a distributed memory architecture is presented, showing near-linear scaling of the adaptive $s$-step GMRES algorithm up to 14,336 cores.

In addition to performance results, we also used the adapted step size as a metric to evaluate different variants of the algorithm. We investigated the stability of different polynomial bases for MPK and different preconditioning techniques. The robustness of scaled Newton polynomials is demonstrated, and the influence of the eigenvalue distribution on the adapted step size is emphasized. The initial step size estimator is developed based on scaled Newton polynomials to mitigate the amount of wasted computations and communications and has been shown to give well-informed step sizes close to the actual step size limit.

\section*{Acknowledgments}
The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported within this paper. 

\bibliographystyle{siamplain}
\bibliography{references}

\appendix

\end{document}
