\section{Introduction}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/motivation2_low.pdf}
    \caption{There has been exciting advancements in large scale image generation models~\cite{rombach2022stablediffusion}. When applied independently to a sequence of images (`per-frame'), however, such methods produce inconsistent results across frames. Our method uses a pre-trained and fixed image generation model to consistently edit a video clip based on a target text prompt. We show examples of two different edits (`ours').}
    \label{fig:motivation}
\end{figure}

% image-based diffusion + edits
Diffusion-based algorithms~\cite{croitoru2022diffusion,ho2020ddpm,song2021ddim} have emerged as the  generative model of choice for image creation.  They 
are stable to train (even over huge image collections), 
produce high-quality results, and support conditional sampling. Additionally, one can invert~\cite{mokady2022null,song2021ddim} a given image into a pretrained diffusion model and subsequently edit using only textual guidance~\cite{hertz2022prompt}. Such a generic workflow, to handle real images and interact using semantic text prompts, is an exciting development and opens the door for many downstream content creation tasks.  


% nothing available for videos
However, the same workflow is barely available for videos where the development of video diffusion models is still in its infancy~\cite{Dreamix,singer2023makeavideo,yu2023video}. Not surprisingly,  naively applying an image-based workflow to each video frame  produces inconsistent results (see Figure~\ref{fig:motivation}). Alternately, while it is possible to use a single frame for style guidance and employ video stylization propagation~\cite{Jamriska19-SIG}, the challenge lies in stylizing new content revealed under changing occlusions across  frames.



% what are the challenges
In this paper, we explore the feasibility of \textit{editing a video clip using a pre-trained image diffusion model and text instructions with no additional training}. We start by inverting the input video clip and expecting the user to edit, using textual prompts, one of the video frames. The goal is then to \textit{consistently} propagate the edit across the rest of the video. The challenge is to balance between respecting the user edit and maintaining the plausibility and temporal coherency of the output video. Image generation models already generate images faithful to the edit prompt. %The alignment to the input text can further be improved as shown by recent methods~\cite{hertz2022prompt,pnpDiffusion2022}.
Hence, what remains challenging is to propagate the edit in a temporally coherent manner. 

Temporal coherency requires preserving the appearance across neighboring frames while respecting the motion dynamics. Leveraging the fact that the spatial features of the self attention layers are influential in determining both the structure and the appearance of the generated images, we propose to inject features obtained from the previously edited frames into the self attention layer of the current frame. This feature injection notably adapts the self attention layers to perform cross-frame attention and enables the generation of images with coherent appearance characteristics. To further improve consistency, we adopt a \textit{guided diffusion} strategy in which we update the intermediate latent codes to enforce similarity to the previous frame before we continue the diffusion process. While the image generation model cannot reason about motion dynamics explicitly, recent work has shown that generation can be conditioned on static structural cues such as depth or segmentation maps~\cite{zhang2023controlnet}. Being disentangled from the appearance, such structural cues provide a path to reason about the motion dynamics. Hence, we utilize a depth-conditioned image generation model and use the predicted depth from each frame as additional input.

%We focus on two types of edits, global and local. Global edits refer to changing appearance of the whole video; while, local edits refer to editing individual object(s) in the video. In both cases, the challenge is to propagate the edits across the video frames while balancing between respecting the user edits and maintaining the plausibility of the output video. 


\if0
what is the challenge? 
latest work on image inversion. this is challenging itself! what about video?
we find that 
independent inversion of frames works but leads to an internal representation that is difficult to edit. (we need to ablate the effect of consistent inversion to decide how strong of a claim we make here)
\fi

% what we propose
%Unlike other generative models (e.g., GANs, VAEs), diffusion models lack a clear semantic latent space~\cite{xx}. However, as diffusion images~(see Section~\ref{xx} for a brief introduction) are sampled over many stages, the corresponding noise images can be collectively taken as latent codes of the final images.  Our key observation is that consistency across final edited frames over time translates to ensuring that the intermediate noise images are consistent. In order to computationally realize this, we first propose an energy to measure such consistency and then formulate an optimization to control the diffusion process to minimize the consistency energy. 


%consistency term
%We measure edit consistency by the relation between edit paths over the diffusion process for the target frames and residues corresponding to the prescribed edit for the source frame. By residue, we refer to the residuals between the intermediate noise images during inversion and editing. In other words, for each frame, we treat the intermediate noise images, obtained during the inversion process, as the (latent) diffusion path and measure intermediate residues to the corresponding edit path, encoded in a local `coordinate` frame. 



\if0
as a combination of two factors. First, we 
for (i) we utilize two terms. first the edit being a shift in the latent space (an analogy to parallel transport), we enforce consistency between edit directions (i.e., residuals between noise images during inversion and editing). second since we get a prediction of the final image at each diffusion process, we can enforce consistency between such predictions across frames. 
\fi

%Solving for the edited frames can then be cast as an optimization of the consistency energy. 
%Particularly, instead of directly manipulating the noise images themselves, which requires effective handling of the dis-occluded regions, we optimize for the null embedding~\cite{xx} at each diffusion step to minimize the consistency energy. This allows us to edit the whole noise image coherently while using only the commonly visible regions across frames to constrain the optimization. 

We term our method \emph{\methodname} and evaluate it on various real video clips demonstrating both local (e.g., changing the attribute of a foreground object) and global (e.g., changing the style) edits. We compare with several state-of-the-art approaches including diffusion-based image editing methods applied per frame~\cite{meng2021sdedit}, patch-based video based stylization methods~\cite{Jamriska19-SIG}, neural layered representations that facilitate consistent video editing~\cite{bar2022text2live}, and concurrent diffusion based video editing methods~\cite{wu2022tuneavideo}. We show that \methodname is on par with or better than the baselines \textit{without} requiring any compute-intensive preprocessing~\cite{bar2022text2live} or any video-specific finetuning~\cite{wu2022tuneavideo}. 
This can be seen in Figure \ref{fig:motivation} ``ours'' columns where the appearance of the foreground objects are more consistent across frames than per-frame editing. 
% \duygu{discuss teaser}

In summary, we present a \textit{training free} approach that utilizes pre-trained large scale image generation models for video editing. Our method does not require pre-processing and does not incur any additional overhead during inference stage. This ability to use an existing image generation model paves the way to bring exciting advancements in controlled image editing to videos at no cost.

%just changing the conditioning signal is not enough, either early in the diffusion process (too many changes) or later in the diffusion process (view/time not taken into consideration); talk about null embedding


% main hypothesis



