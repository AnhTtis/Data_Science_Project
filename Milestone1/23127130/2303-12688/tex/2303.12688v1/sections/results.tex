\section{Evaluation}
\label{sec:results}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/comparisons2_low.pdf}
    \caption{\textbf{Comparisons}. Top: results from baselines; bottom: our results. The method of Jamriska et al.~relies on optical flow to propagate edits in a temporally coherent manner but fails as new content becomes visible. Text2Live suffers when multiple foreground objects are present and a neural atlas cannot be computed robustly. Without an explicit notion of structure, Tune-a-Video is not able to preserve the structure in the input video for some edits.}
    \label{fig:comparison}
\end{figure}




\begin{figure*}
    \centering
   % \begin{overpic}[grid,width=\textwidth]{figures/results_opt.pdf}
%\put(5,95){input video}
%\put(2,85){\texttt{test font}}
%    \end{overpic}
\includegraphics[width=\textwidth]{figures/results_opt_low.pdf}
\vspace*{-.3in}
    \caption{
   \textbf{ Text-guided video edits. }
    For each example, we show frames from the input video at the top and the corresponding edited frames, and the edit prompt at the bottom. Please refer to the supplemental for the videos. }
    \label{fig:results}
\end{figure*}

\textbf{Dataset.} 
Following \cite{bar2022text2live,esser2023structure,wu2022tuneavideo}, we evaluate \methodname on videos obtained from the DAVIS dataset~\cite{Perazzi2016}. %We generate a source prompt for each video by running the BLIP~\cite{li2022blip} captioning model on the first frame. The source prompt is used during the inversion step. 
For videos that have been used in previous or concurrent work, we use editing prompts provided by such work. For other videos, we generate editing prompts by consulting a few users. 
The length of these videos ranges from 50 to 82 frames.

\begin{table}[b!]
\centering
  \caption{Compared to other video-based baselines, our method does not require a heavy pre-processing stage nor a per-video or per-edit finetuning strategy.}
\small
\begin{tabular}{p{2.5cm}||p{1.5cm}|p{1.3cm}|p{1.1cm}  }
 \hline
 & pre-processing & finetuning & layering \\
\hline
 Jamriska et al.~\cite{Jamriska19-SIG}   & no & no &  no\\
Text2Live \cite{bar2022text2live} &   ~7-8 hours  & ~30 min   & yes\\
Tune-a-Video \cite{wu2022tuneavideo} & no & ~10 min &  no\\
ours    & no & no &  no\\
 \hline
 \end{tabular}
 % \caption{We compare our method to several video-based baselines of which some require a heavy pre-processing stage and a per-video or per-edit finetuning strategy.}
 \label{tab:baselines}
\end{table}

\textbf{Baselines.} We compare \methodname with both state-of-the-art image and video editing approaches. 
(i)~The method of Jamriska et al.~\cite{Jamriska19-SIG} propagates the style of a set of given frames to the input video clip. We use the edited anchor frame as a keyframe. 
% Next, we compare to the recent Text2Live~\cite{bar2022text2live} method which provides a method for text-guided editing of neural atlases computed for the foreground and background of a given video clip. 
(ii)~We compare to a recent text-guided video-editing method, Text2Live~\cite{bar2022text2live}. We note that this method first requires the computation of a neural atlas~\cite{kasten2021layered} for the foreground and background layers of a video which takes approximately 7-8 hours per video. Given the neural atlas, the method  further finetunes the text-to-image generation model which takes another 30 minutes.
(iii)~We also compare against SDEdit~\cite{meng2021sdedit} where we add noise to each input frame and denoise conditioned on the edit prompt. We experiment with different strengths of noise added and use the depth-conditioned Stable Diffusion \cite{SDv2} 
% \footnote{https://huggingface.co/stabilityai/stable-diffusion-2-depth} 
as in our backbone diffusion model. 
(iv)~Finally, we also consider the concurrent Tune-a-Video~\cite{wu2022tuneavideo} method, which performs a video-specific finetuning of a pretrained image model. Since this method generates only a limited number of frames, we generate 24 frames by sampling every other frame in the input video following the setup provided by the authors. Note that this method is not conditioned on any structure cues like depth. 
We summarize the characteristics of each baseline in Table~\ref{tab:baselines}.

\textbf{Metrics.} We expect a successful video edit to faithfully reflect the edited prompt and be temporally coherent. To capture the \textit{faithfulness}, we follow \cite{imagen-video,wu2022tuneavideo} and report CLIP score \cite{hessel2021clipscore,park2021benchmark}, which is the cosine similarity between the CLIP embedding \cite{radford2021clip} of the edit prompt and the embedding of each frame in the edited video. 
We refer to this metric as ``CLIP-Text". To measure the \textit{temporal coherency}, we measure the average CLIP similarity between the image embeddings of consecutive frames in the edited video (``CLIP-Image"). We also compute the optical flow between consecutive frames~\cite{RAFT}, and warp each frame in the edited video to the next using the flow. We compute the average mean-squared pixel error between each warped frame and its corresponding target frame as ``Pixel-MSE".

\subsection{Results}
\textbf{Qualitative results.} We provide a set of example edits our method achieves in Figure~\ref{fig:results}. For each example, we show several randomly sampled frames both from the input and edited video, along with the edit prompts. As seen in the figure, \methodname can handle videos with a clear foreground object (e.g., bear) as well as multiple foreground objects (e.g., fish). We can perform \textit{localized} edits where a prompt specifies the attribute of an object (e.g., swan) as well as \textit{global} edits which change the overall style (e.g., kite surfer). Please note that, unlike Text2Live, we do \textit{not} use any explicit mask information to specify which regions should be edited. This enables us to handle reflections automatically, as in the swan example. We refer to the supplementary material for more examples.

\textbf{Quantitative results and comparisons.} We provide quantitative comparisons to the baseline methods in Table~\ref{tab:quanti_result} and also refer to Figure~\ref{fig:comparison}. Among the baseline methods, we observe that Jamriska et al.~\cite{Jamriska19-SIG} achieve good temporal coherency as it explicitly utilizes flow information. However, as new content appears in the video or when flow information is unreliable, it fails to hallucinate details resulting in less faithful edits (Fig.~\ref{fig:comparison} top left). 
Tex2Live~\cite{bar2022text2live} performs well when a clear foreground and background separation exists, and an accurate neural atlas can be synthesized. Since this method edits the atlas itself it is temporally coherent by construction. 
However, when there are multiple foreground objects, e.g.,~Fig.~\ref{fig:comparison} top middle, a reliable neural atlas cannot be computed, and the method fails. The edited results have ``ghost shadow" that consequently deteriorates CLIP-Text scores. 
% \duygu{refer to the faithfullness and pixel metrics}. 
We also observe that, unlike our method, it is not straightforward to perform global style edits on \emph{both} the foreground and background consistently with Text2Live. 
While attaining high CLIP-Text scores, i.e., generating frames faithful to the edit prompt, SDEdit~\cite{meng2021sdedit} results in worst temporal coherency as it generates each frame independently. This is confirmed by the lowest CLIP-Image score and highest Pixel-MSE in Table~\ref{tab:quanti_result}.
% \duygu{talk about the numbers}  
The concurrent Tune-a-Video method~\cite{wu2022tuneavideo} achieves a nice balance between edit propagation and temporal coherency. However, sub-sampling a fixed number of frames in the video inevitably hurts the temporal scores. 
We also observe that for some edits it cannot preserve the structure of the objects in the input video (Fig.~\ref{fig:comparison} top right). Some edits result in very similar outputs, which could be attributed to per-example finetuning that might cause overfitting (see the pig and fish examples in the supplementary material). 
In contrast, by using the additional depth conditioning, \methodname better preserves the structure of the input video and strikes a good balance between respecting the edit as well as keeping temporal consistency without requiring any training. 
% For more discussion and analysis on the comparison, please see supplementary material

\begin{table}
 \caption{
 \textbf{Quantitative comparison.} Our method attains the highest CLIP-Text score (faithfulness) and fairly good CLIP-Image and Pixel-MSE (temporal coherency).}
\footnotesize
\begin{tabular}{lrrr}
\toprule
 & CLIP-Text $\uparrow$ & CLIP-Image $\uparrow$ & Pixel-MSE $\downarrow$ \\
\midrule  \midrule
Jamriska et al.~\cite{Jamriska19-SIG}   & 0.2684 & 0.9838 & 44.62\\
Tex2Live~\cite{bar2022text2live} &   0.2679  &  0.9806  & 72.57\\
Tune-a-Video~\cite{wu2022tuneavideo} & 0.2691 & 0.9674 &  1190.62 \\
SDEdit~\cite{meng2021sdedit}   & 0.2775 & 0.8731 &  2324.29\\
ours w/o update   & 0.2893 & 0.9740 &  371.18\\
ours    & 0.2891 & 0.9767 &  228.62\\
 \bottomrule
 \end{tabular}
 \label{tab:quanti_result}
\end{table}

\textbf{User study.} We further evaluate \methodname against the baselines with a user study. Given 10 videos with 2 different edits each, we ask \totaluser participants to compare our result with one of the baselines shown in random order. Each edited video is ranked, by pairwise comparison, by \eachuser users on average. We ask two questions to the user: (i)~Which video better represents the provided editing caption? (ii)~Which edited video do you prefer? The first question evaluates ``faithfulness" while the second indicates overall video quality via ``preference".
Please see supp.~mat.~for more details on our perceptual study. 


\begin{figure}[t]
     \centering
     \begin{subfigure}[t]{0.49\columnwidth}
         \centering
         \includegraphics[trim={1.5cm 0.5cm 1cm 0.1cm},clip,width=\linewidth]{figures/user_faithfulness.png}
         \caption{Faithfulness}
         \label{fig:user_faithfulness}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.49\columnwidth}
         \centering
         \includegraphics[trim={1.5cm 0.5cm 1cm 0.1cm},clip,width=\linewidth]{figures/user_preference.png}
         \caption{Preference}
         \label{fig:user_preference}
    \end{subfigure}
    % \hfill
    % \begin{subfigure}[t]{\columnwidth}
    %      \centering
    %      \includegraphics[trim={0 0 0 -0.5cm},clip,width=\linewidth]{figures/user_overall.png}
    %      \caption{Chosen frequency (\%) := obtained votes / total occurrence}
    %      \label{fig:user_ratio}
    %  \end{subfigure}     
     \caption{
     \textbf{User evaluation. }
     Our user study shows that \methodname not only better reflects the edits but is also perceptually preferred over other methods. }
\end{figure}




In Fig.~\ref{fig:user_faithfulness}, the majority of the participants agreed that our results reflect the edits more faithfully than others, in accordance with the higher CLIP-Text score in Table~\ref{tab:quanti_result}. 
Fig.~\ref{fig:user_preference} shows that our results are also preferred over other baselines when viewed side-by-side. 
Note that temporal smoothness plays a crucial role in the perceptual quality of a video. Despite Jamriska et al.~\cite{Jamriska19-SIG} losing edits as pointed out in Fig.~\ref{fig:comparison}, it is on par with our method in terms of overall preference which we attribute to high temporal coherency (see Table~\ref{tab:quanti_result}). 
% Finally, Fig.~\ref{fig:user_ratio} shows how often a method is chosen when presented together with other methods. 
In summary, the user study confirms that we achieve a good balance between ensuring edits and maintaining temporal consistency.
% Please \duygu{discuss the results}


\begin{figure}[b!]
    \centering
    \includegraphics[width=\columnwidth]{figures/keyframe_low.pdf}
    \caption{
    \textbf{Ablations. }
    We evaluate different choices for self-attention feature injection. Using a fixed anchor frame results in structural artifacts as the distance between the anchor and the edited frame increases. Attending only to the previous frame or a randomly selected previous frame results in temporal and structural artifacts. We obtain the best results by using    
    a fixed anchor and the previous frame.}
    \label{fig:keyframe}
\end{figure}

\textbf{Ablations.} We perform ablation studies to validate several design choices. First, we evaluate different choices of previous frames to use for self attention feature injection. In Figure \ref{fig:keyframe}, we compare scenarios where we always attend to (i) a fixed anchor frame (first frame in our experiments), (ii) the previous frame only, (iii) an anchor frame and a randomly selected previous frame, and (iv) an anchor frame and a previous frame as in our method. In cases where no previous frame information is used or a random previous frame is chosen, we observe artifacts, especially for sequences that contain more rotational motion, e.g., structure of the car not being preserved as the car rotates. This confirms our intuition that attending to the previous frame implicitly represents the state of the edit in a recurrent manner. Without an anchor frame, we observe more temporal flickering and the edit diminishes as the video progresses. By combining the previous frame with an anchor frame we strike a good balance.

\methodname consists of two main steps of feature injection and guided latent update. In Table~\ref{tab:quanti_result}, we disable the latent update step. As shown in the metrics, this results in worse CLIP-Image scores and Pixel-MSE errors confirming that this guidance is effective in enforcing temporal coherency and preserving the edit. We refer to the supplementary material for more ablation studies. 


\textbf{Implementation details.} We use the publicly available Stable Diffusion depth conditioned model~\cite{SDv2} 
% \footnote{https://huggingface.co/stabilityai/stable-diffusion-2-depth} 
as our image generation model. We obtain depth estimates for the input videos using the Midas depth estimator~\cite{Ranftl2022}. For each video, we perform the inversion once and save it. Each editing operation then amounts to generating each frame given the target prompt. We generate results at $512 \times 512$ resolution. The temporal error Pixel-MSE in Table \ref{tab:quanti_result} is therefore computed on a $512 \times 512$ image domain. Our method does not incur any additional significant cost on the image inference step. In our current implementation, we generate each frame in $5$ seconds using a batch size of 1 on an A100 GPU once the frames are inverted in a one time process. 