\section{Our Method}
Given a sequence of frames of a video clip, $\mathcal{I}:=\{I_1,\dots,I_n\}$, we would like to generate a new set of images $\mathcal{I'}:=\{I'_1,\dots,I'_n\}$ that reflects an edit denoted by a target text prompt $\mathcal{P'}$. For example, given a video of a car, the user may want to generate an edited video where attributes of the car, such as its color, are edited. We aim to exploit the power of a pretrained and fixed large-scale image diffusion model to perform such manipulations as coherently as possible, without the need for any example-specific finetuning or extensive training. We achieve this goal by manipulating the internal features of the diffusion model (Section~\ref{sec:attn}) along with additional guidance constraints (Section~\ref{sec:cfg}). 

Given that the fixed image generation model is trained with only single images, it cannot reason about dynamics and geometric changes that happen in an input video. In light of the recent progress in conditioning image generation models with various structure cues~\cite{ediffi,makeascene,voynov2022sketch}, we observe that this additional structure channel is effective in capturing the motion dynamics. Hence, we build our method on a depth-conditioned Stable Diffusion model \cite{SDv2}.
% \footnote{https://huggingface.co/stabilityai/stable-diffusion-2-depth}. 
Given $\mathcal{I}$, we perform per-frame depth prediction~\cite{Ranftl2022} and utilize it as additional input to the model.

%We note that the current state of the art image diffusion models operate either at a low resolution version of the image followed by an upsampling network (in case of pixel diffusion models~\cite{}) or work in the latent space accompanied with an encoder and decoder to map back and forth to the latent space (in case of latent diffusion models~\cite{}). In the rest of the paper, we refer to either these downsampled or latent images via $I_i$ which our method also operates on.

\subsection{Self-attention Feature Injection}
\label{sec:attn}
In the context of static images, a
large-scale image generation diffusion model typically consists of a U-Net architecture composed of residual, self-attention, and cross-attention blocks. While the cross-attention blocks are effective in terms of achieving faithfulness to the text prompt, self-attention layers are effective in determining the overall structure and the appearance of the image. At each diffusion step $t$, the input features $f_t^l$ to the self-attention module at layer $l$, are projected into \textit{queries}, \textit{keys}, and \textit{values} by matrices $W^Q$, $W^K$, and $W^V$, respectively to obtain queries $Q^l$, keys $K^l$, and values $V^l$. The output of the attention block is then computed as:
%
\begin{align*}
Q^l &= W^Q f_t^l; K^l = W^K f_t^l; V^l = W^v f_t^l \\
\hat{f}_t^l &= \text{softmax} (Q^l (K^l)^T) (V^l).
\end{align*}

In other words, for each location in the current spatial feature map $f_t^l$, a weighted summation of every other spatial features is computed to capture global information. Extending to the context of videos, 
our method captures the interaction across the input image sequence by manipulating the input features to the self-attention module. 
Specifically, we inject the features obtained from the previous frames. A straightforward approach is to attend to the features $f^{j,l}_t$ of an earlier frame $j$ while generating the features $f^{i,l}_t$ for frame $i$ as,
%
\begin{align*}
Q^{i,l} &= W^Q f_t^{i,l}; K^{i,l} = W^K f_t^{j,l}; V^{i,l} = W^v f_t^{j,l}. \\
\end{align*}
% 
With such feature injection, the current frame is able to utilize the context of the previous frames and hence preserve the appearance changes. A natural question is whether an explicit, potentially recurrent, module can be employed to fuse and represent the state of the previous frame features without explicitly attending to a specific frame. However, the design and training of such a module is not trivial. Instead, we rely on the pre-trained image generation model to perform such fusion implicitly. For each frame $i$, we inject the features obtained from frame $i-1$. 
% Since, during the generation of $i-1$, we attend to frame $i-2$ and so forth, we have an implicit way of aggregating the feature states. 
Since the editing is performed in a frame-by-frame manner, the features of $i-1$ are computed by attending to frame $i-2$. Consequently, we have an implicit way of aggregating the feature states. 
In Section~\ref{sec:results}, we demonstrate that while attending to the previous frame helps to preserve the appearance, in longer sequences it shows the limitation of diminishing the edit. 
%
%The \hl{supplementary material provides} detailed ablations on which previous $j$ frame(s) to use.  Similar to the concurrent work of Wu et al.~\cite{}, we conclude that attending to an \textit{anchor frame} $a$ along with the previous frame $i-1$ results in the most consistent results. Specifically, since $i$ and $i-1$ are more similar, they are a natural choice. However, attending only to the previous frame $i-1$ diminishes the intended edit over longer sequences. 
Attending to an additional anchor frame avoids this forgetful behavior by providing a global constraint on the appearance. Hence, in each self-attention block, we concatenate features from frames $a$ and $i-1$ to compute the key and value pairs. In our experiments, we set $a=1$, i.e., the first frame. %(\duygu{potentially add a figure here} \duygu{experiment with choosing another frame as anchor}):
%
\begin{align*}
Q^{i,l} &= W^Q f_t^{i,l}; \\
K^{i,l} &= W^K [f_t^{a,l}, f_t^{i-1,l}]; V^{i,l} = W^v [f_t^{a,l}, f_t^{i-1,l}]. \\
\end{align*}

We perform the above feature injection in the decoder layers of the UNet, which we find effective in maintaining appearance consistency. As shown in the ablation study, and also reported by the concurrent work of Tumanyan et al.~\cite{pnpDiffusion2022}, the deeper layers of the decoder capture high resolution and appearance-related information and already result in generated frames with similar appearance but small structural changes. Performing the feature injection in earlier layers of the decoder enables us to avoid such high-frequency structural changes. We do not observe further significant benefit when injecting features in the encoder of the UNet and observe slight artifacts in some examples. %(see the supplementary material). %\duygu{add a figure}. \duygu{which layers do we use?}

\begin{algorithm}[t!]
\hspace*{\algorithmicindent} \textbf{Input} $\mathcal{I} = \{I_1, \dots I_n\}$, text prompt $\mathcal{P'}$, $T=50$ diffusion steps, a pretrained diffusion model SD\\
 \hspace*{\algorithmicindent} \textbf{Output} $\mathcal{I'} = \{I'_1, \dots I'_n\}$
\begin{algorithmic}[1]
\STATE {$\mathcal{X^T} \leftarrow \{ x_T^1, ..., x_T^n\}$ by DDIM inversion }
\STATE {$\mathcal{I'} = \emptyset, \mathcal{F}^{anchor} = \emptyset, \mathcal{F}^{prev} = \emptyset, \mathcal{\hat{X}}_0^{prev} = \emptyset $ }
\FOR{f $\in$ [1,$n$]:}
\STATE{$\mathcal{\hat{X}}_0^{tmp} = [], \mathcal{\hat{F}}^{tmp} = []$}
\STATE{$x_t = x_T^f$}
\STATE{$\delta_{t-1} = 100$ if $t-1 < 25$ else $\delta_{t-1} = 0$}
\FOR{t $\in$ [1,$T$]:}
\IF{$f = 1$}
\STATE{$f^{anchor} = \emptyset, f^{prev} = \emptyset, \hat{x}_0^{prev} = \emptyset$}
\ELSE
\STATE{$f^{anchor} = \mathcal{F}^{anchor}[t]$}
\STATE{$f^{prev} = \mathcal{F}^{prev}[t]$}
\STATE{$\hat{x}_0^{prev} = \mathcal{\hat{X}}_0^{prev}[t]$}
\ENDIF
\STATE{$x_{t-1}, \hat{x}_0^t, f^t = SD(x_t, t, \mathcal{P}, f^{anchor}, f^{prev}) $}
\IF{$f = 1$}
\STATE{$\mathcal{F}^{anchor} \leftarrow  \mathcal{F}^{anchor} \cup \{f^t\}$}
\ELSE
\STATE{$x_{t-1} \leftarrow x_{t-1} - \delta_{t-1} \nabla_{x_t}\| \hat{x}_0^{prev}-\hat{x}_0^t \|^2_2$}
\ENDIF
\STATE{$\mathcal{F}^{tmp} \leftarrow \mathcal{F}^{tmp} \cup \{f^t\}$}

\STATE  {$\mathcal{\hat{X}}_0^{tmp} \leftarrow \mathcal{\hat{X}}_0^{tmp} \cup \{\hat{x}_0^t \} $}
\ENDFOR
\STATE{$\mathcal{\hat{X}}_0^{prev} = \mathcal{\hat{X}}_0^{tmp} $}
\STATE{$\mathcal{F}^{prev} = \mathcal{F}^{tmp} $}
\STATE  {$\mathcal{I'} \leftarrow \mathcal{I'} \cup \{I'_f\} $}
\ENDFOR
\end{algorithmic}
\caption{}
\label{alg}
\end{algorithm}


\subsection{Guided Latent Update}
\label{sec:cfg}
While self-attention feature injection effectively generates frames that have coherent appearance, it can still suffer from temporal flickering. In order to improve the temporal stability, we exploit additional guidance to update the latent variable at each diffusion step along the lines of classifier guidance~\cite{nichol2022glide}. To perform such an update, we first formulate an energy function that enforces consistency.

Stable Diffusion \cite{SDv2,he2022lvdm}, like many other large-scale image diffusion models, is a denoising diffusion implicit model~(DDIM) where at each diffusion step, given a noisy sample $x_t$, a prediction of the noise-free sample $\hat{x}_0$, along with a direction that points to $x_{t}$, is computed. Formally, the final prediction of $x_{t-1}$ is obtained by:
%
\begin{align*}
x_{t-1} &= \sqrt{\alpha_{t-1}} \underbrace{\hat{x}^t_0}_{\text{predicted `$x_0$'}} + \\
&\underbrace{\sqrt{1-\alpha_{t-1} - \sigma_t^2} \epsilon_{\theta}(x_t, t)}_{\text{direction pointing to $x_t$}} + \underbrace{\sigma_t \epsilon_t}_{\text{random noise}},  \\
\hat{x}^t_0 &= \frac{x_t - \sqrt{1-\alpha_t}\epsilon_{\theta}^t(x_t)}{\sqrt{\alpha_t}},
\end{align*}
%
where $\alpha_t$ and $\sigma_t$ are the parameters of the scheduler and $\epsilon_{\theta}$ is the noise predicted by the UNet at the current step $t$. %The diffusion process can be made deterministic by setting $\sigma_t=0$. 
The estimate $\hat{x}_0^t$ is computed as a function of $x_t$ and indicates the final generated image. Since our goal is to generate similar consecutive frames eventually, we define an L2 loss function $g(\hat{x}_0^{i,t}, \hat{x}_0^{i-1,t}) = \|\hat{x}_0^{i,t}-\hat{x}_0^{i-1,t}\|^2_2$ that compares the predicted clean images at each diffusion step $t$ between frames $i-1$ and $i$. We update $x_{t-1}^i$, the current noise sample of a frame $i$ at diffusion step $t$, along the direction that minimizes $g$:
%
\begin{equation}
x_{t-1}^i \leftarrow x_{t-1}^i - \delta_{t-1} \nabla_{x_{t}^i} g(\hat{x}_0^{t,i-1},\hat{x}_0^{t,i}),
\end{equation}
%
where $\delta_{t-1}$ is a scalar that determines the step size of the update. We empirically set $\delta_{t-1}=100$ in our experiments. We perform this update process for the early denoising steps, namely the first $25$ steps among the total number of $50$ steps, as the overall structure of the generated image is already determined in the earlier diffusion steps \cite{kwon2023semantic}. Performing the latent update in the remaining steps often results in lower-quality images. %, as also reported by others~\cite{xx}.

Finally, the initial noise used to edit each frame also significantly affects the temporal coherency of the generated results. We use an inversion mechanism, DDIM inversion~\cite{song2021ddim}, while other inversion methods aiming to preserve the editability of an image can be used~\cite{mokady2022null} as well. To get a source prompt for inversion, we generate a caption for the first frame of the video using a captioning model~\cite{li2022blip}. We provide the overall steps of our method in Algorithm~\ref{alg}.
