\section{Conclusion and Future Work}
We present \methodname, a method that utilizes a pre-trained and fixed text-to-image generation model for editing video clips. We demonstrate the power of our method on various inputs and editing tasks. We provide detailed comparisons to baselines along with an extensive user study. We show that \methodname is on par or superior to baselines while not requiring additional pre-processing or finetuning.

However, our method also has limitations that we would like to address in future work. We believe that there is still room for improvement in terms of temporal coherency. Exploiting other energy terms, e.g., patch-based similarity~\cite{Jamriska19-SIG} and CLIP similarity, during the latent update stage, is a promising direction. As we utilize an anchor frame for feature injection, handling longer videos where the distance from the anchor increases can cause quality degradation. Additional conditioning (e.g., image embedding conditioning~\cite{brooks2022instructpix2pix}) and a smart anchor update mechanism is a potential direction. Finally, given that our method does not require any finetuning, it has the advantage of being applied to parallel efforts that aim to introduce additional control to the image generation model, which we would like to exploit.

% \paragraph{Acknowledgment}
% \paragraph{Disclosure}