\section{Related Work}
\subsection{Image generation and editing}
While many deep generative models, e.g., GAN~\cite{NIPS2014_5ca3e9b1}, have demonstrated the ability to generate realistic images \cite{brock2018large,stylegan}, recently, diffusion models have become the choice of models due to the high quality output they achieve on large scale datasets~\cite{NEURIPS2021_49ad23d1}. 
Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020ddpm} and its variant Denoising Diffusion Implicit Model (DDIM) \cite{song2021ddim} have been  widely used for unconditional text-to-image generation. Several large scale text-to-image generation models~\cite{nichol2022glide,ramesh2022dalle2,saharia2022imagen}, which operate on the pixel space have been presented,  achieving very high quality results. Rombach et al.~\cite{rombach2022stablediffusion} have proposed to work in a latent space which has lead to the widely adopted open source Stable Diffusion model. We refer the readers to a recent survey~\cite{croitoru2022diffusion} and the extensive study~\cite{Karras2022edm} for a detailed discussion on diffusion models.

In the presence of high quality text conditioned image generation models, several recent works have focused on utilizing additional control signals for generation or editing existing images. Palette~\cite{saharia2022palette} has shown various image-to-image translation applications using a diffusion model including colorization, inpainting, and uncropping. Several methods have focused on providing additional control signals such as sketches, segmentation maps, lines, or depth maps by adapting a pretrained image generation model. These methods work by either finetuning~\cite{wang2022pretraining} an existing model, introducing adapter layers~\cite{t2iadapter} or other trainable modules~\cite{voynov2022sketch,zhang2023controlnet}, or utilizing an ensemble of denoising networks~\cite{ediffi}. Since our model uses a pretrained image diffusion model, it can potentially use any model that accepts such additional control signals. In another line of work, methods have focused on editing images while preserving structures via attention layer manipulation~\cite{hertz2022prompt,pnpDiffusion2022}, additional guidance optimization~\cite{Choi2021ILVR}, or per-instance finetuning~\cite{kawar2022imagic}. Our method also performs attention layer manipulation, specifically in the self attention layers, along with a latent update at each diffusion step. Unlike single image based editing work, however, we utilize previous frames when performing these steps. We would also like to emphasize that the edit of the anchor frame for our method can potentially be performed with any such method that utilize the same underlying image generation model.
%GLIDE \cite{nichol2022glide} propose cfg. Talk about DALLE-2 \cite{ramesh2022dalle2} and ImaGen \cite{saharia2022imagen}. Stable Diffusion \cite{rombach2022stablediffusion} improves stability by moving from pixel to latent space.
%SDEdit \cite{meng2021sdedit}, ILVR \cite{Choi2021ILVR}, Imagic \cite{kawar2022imagic},
%Prompt-to-prompt \cite{hertz2022prompt}, null inversion \cite{mokady2022null}, Plug-and-play \cite{pnpDiffusion2022},
%ControlNet \cite{zhang2023controlnet}

\subsection{Video generation and editing}
Until recently, GANs have been the method of choice for video generation, with many works designed towards unconditional generation~\cite{brooks2022generating,Gupta_2022_CVPR,TGAN2017,stylegan-v,MoCoGAN,digan,zhang2022towards}. In terms of conditional generation, several methods have utilized guidance channels such as segmentation masks or keypoints~\cite{wang2018fewshotvid2vid,wang2018vid2vid}. However, most of these methods are trained on specific domains. One particular domain where very powerful image generators such as StyleGAN~\cite{stylegan} exist is faces. Hence, several works have explored generating videos by exploring the latent space of such an image based generator~\cite{StyleFaceV,xu2022videoeditgan}. While we also exploit an image generation model, we are not focused on a specific domain.

With the success of text-to-image generation models, there has been recent attempts in text-to-video generation models using architectures such as transformers~\cite{hong2022cogvideo,villegas2023phenaki,MAGVIT} or diffusion models~\cite{he2022lvdm,imagen-video,singer2023makeavideo,yu2023video}. However, such models are still in their infancy compared to images, both due to the complexity of temporal generation as well as large scale annotated video datasets not being comparable in size to images. Concurrent works~\cite{esser2023structure,Dreamix} explore mixed image and video based training to address this limitation. In another concurrent work, Wu et al.~\cite{wu2022tuneavideo} inflate an image diffusion model and finetune on a specific input video to enable editing tasks. In our work, we use the pretrained image diffusion model as it is with no additional training.

Layered neural representations~\cite{lu2020} have recently been introduced, providing another direction for editing videos. Layered neural atlases~\cite{kasten2021layered} are such representations that map the foreground and background of a video to a canonical space. Text2Live~\cite{bar2022text2live} combines such a representation with text guidance to show compelling video editing results. While impressive, the computation of such neural representations includes extensive per-video training (7-10 hours),  which limits their applicability in practice.

Finally, video stylization is a specific type of editing task where the style of an example frame is propagated to the video. While some methods utilize neural feature representations to perform this task~\cite{RuderDB2016}, Jamriska et al.~\cite{Jamriska19-SIG} consider a patch-based synthesis approach using optical flow. In a follow-up work~\cite{Texler2020}, they provide a per-video fast patch-based training setup to replace traditional optical flow. Both methods achieve high quality results but are limited when the input video shows regions that are not visible in the provided style keyframes. They rely on having access to multiple stylized keyframes in such cases. However, generating consistent multiple keyframes itself is a challenge. Our method can also be perceived as orthogonal since the (subset of) frames generated by our method can subsequently be used as keyframe inputs to these models.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/overview.pdf}
    \caption{{\bf Method pipeline}. \methodname first inverts each frame with DDIM-inversion and consider it as the initial noise $x_T$ for the denoising process. To edit each frame $i>1$ (lower row), we select a reference frame (upper row), inject its self-attention features to the UNet. At each diffusion step, we also update the latent of the current frame guided by the latent of the reference frame. In practice, we consider both $i-1$ (previous) and $i=1$ (anchor) frames as reference for feature injection, while we use only the previous frame for the guided latent update.}
    \label{fig:pipeline}
\end{figure*}