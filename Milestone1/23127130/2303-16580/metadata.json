{
    "arxiv_id": "2303.16580",
    "paper_title": "Generalized Relation Modeling for Transformer Tracking",
    "authors": [
        "Shenyuan Gao",
        "Chunluan Zhou",
        "Jun Zhang"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-04-11"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Compared with previous two-stream trackers, the recent one-stream tracking pipeline, which allows earlier interaction between the template and search region, has achieved a remarkable performance gain. However, existing one-stream trackers always let the template interact with all parts inside the search region throughout all the encoder layers. This could potentially lead to target-background confusion when the extracted feature representations are not sufficiently discriminative. To alleviate this issue, we propose a generalized relation modeling method based on adaptive token division. The proposed method is a generalized formulation of attention-based relation modeling for Transformer tracking, which inherits the merits of both previous two-stream and one-stream pipelines whilst enabling more flexible relation modeling by selecting appropriate search tokens to interact with template tokens. An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the token division module. Extensive experiments show that our method is superior to the two-stream and one-stream pipelines and achieves state-of-the-art performance on six challenging benchmarks with a real-time running speed.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16580v1",
        "http://arxiv.org/pdf/2303.16580v2"
    ],
    "publication_venue": "Accepted by CVPR 2023. Code and models are publicly available at https://github.com/Little-Podi/GRM"
}