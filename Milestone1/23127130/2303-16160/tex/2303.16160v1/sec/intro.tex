\vspace{-0.5cm}
\section{Introduction}
Expressive whole-body mesh recovery aims to jointly estimate the 3D human body poses, hand gestures, and facial expressions from monocular images. It is gaining increasing attention due to recent advancements in whole-body parametric models (\eg, SMPL-X~\cite{Pavlakos_2019smplx}). 
This task is a key step in modeling human behaviors and has many applications, \eg, motion capture, human-computer interaction. Previous research focus on individual tasks of reconstructing human body~\cite{Kanazawa_2018_hmr,Kolotouros_2019_spin,zeng2022deciwatch,zeng2022smoothnet,Choi_2020_pose2mesh,tian2022survey}, face~\cite{OswaldAldrian2013InverseRO,AyushTewari2017MoFAMD,YuDeng2019Accurate3F,BernhardEgger20203DMF}, or hand~\cite{AdnaneBoukhayma20193DHS,LinHuang2021SurveyOD,TheocharisChatzis2020ACS}. However, whole body mesh recovery is particularly challenging as it requires accurate estimation of each part and natural connections between them.   


Existing learning-based works~\cite{PavlakosGeorgios2020expose,Feng_2021_pixie,Rong_2021frank,GyeongsikMoon2020hand4whole,HongwenZhang2022PyMAFXTW} use multi-stage pipelines for body, hand, and face estimation to achieve the goal of this task. 
%
As depicted in Figure~\ref{fig:teaser}(a), these methods typically detect different body parts, crop and resize each region, and feed them into separate expert models to estimate the parameters of each part. 
The multi-stage pipeline with different estimators for body, hand, and face results in a complicated system with a large computational complexity. Moreover, the blocked communications among different components inevitably cause incompatible configurations, unnatural articulation of the mesh, and implausible 3D wrist rotations as they cannot obtain informative and consistent clues from other components. 
Some methods~\cite{Feng_2021_pixie,GyeongsikMoon2020hand4whole,HongwenZhang2022PyMAFXTW} attempt to alleviate these issues by designing additional complicated integration schemes or elbow-twist compensation fusion among individual body parts. However, these approaches can be regarded as a late fusion strategy and thus have limited ability to enhance each other and correct implausible predictions. 

In this work, we propose a one-stage framework named \modelname for 3D whole-body mesh recovery, as shown in Figure~\ref{fig:teaser}(b), which does not require separate networks for each part.
Inspired by recent advancements in Vision Transformers~\cite{dosovitskiy2020vit,YufeiXu2022ViTPoseSV}, which are effective in capturing spatial information in a plain architecture, we design our pipeline as a component-aware Transformer (CAT) composed of a global body encoder and a local component-specific decoder. The encoder equipped with body tokens as inputs captures the global correlation, predicts the body parameters, and simultaneously provides high-quality feature map for the decoder.
The decoder utilizes a differentiable upsample-crop scheme to extract part-specific high-resolution features and adopt the keypoint-guided deformable attention to precisely locate and estimate hand and face parameters. 
 The proposed pipeline is simple yet effective without any manual post-processing. To the best of our knowledge, this is the first one-stage pipeline for 3D whole-body estimation. 
 %
We conduct comprehensive experiments to investigate the effects of the above designs and compare our method, with existing works on three benchmarks. Results show that \modelname outperforms the state-of-the-art (SOTA)~\cite{GyeongsikMoon2020hand4whole} by $9.5$\% on AGORA, $7.8$\% on EHF, and $13.4$\% on the body-only 3DPW dataset.


In addition, existing popular benchmarks, as illustrated in the first row of Figure~\ref{fig:ubody}, are either indoor single-person scenes with limited images (\eg, EHF~\cite{Pavlakos_2019smplx}) or outdoor synthetic scenes (e.g., AGORA~\cite{Patel_2021agora}), where the people are often too far from the camera and the hands and faces are frequently obscured. 
%
In fact, human pose estimation and mesh recovery is a fundamental task that benefits many downstream applications, such as sign language recognition, gesture generation, and human-computer interaction.
Many scenarios, such as talk shows and online classes, are of vital importance to our daily life yet under-explored. In such scenarios, the upper body is a major focus, whereas the hand and face are essential for analysis.
%
To address this issue, we build a large-scale upper-body dataset with fifteen human-centric real-life scenes, as shown in Figure~\ref{fig:ubody}(f) to (t). This dataset contains many unseen poses, diverse appearances, heavy truncation, interaction, and abrupt shot changes, which are quite different from previous datasets. Accordingly, we design a systematical annotation pipeline and provide precise 2D whole-body keypoint and 3D whole-body mesh annotations. With this dataset, we perform a comprehensive benchmarking of existing whole-body estimators. 

Our contributions can be summarized as follows.
\begin{itemize}
\vspace{-0.1cm}
\item We propose a one-stage pipeline, \modelname, for 3D whole-body mesh recovery, which can regress the SMPL-X parameters in a simple yet effective manner.
\vspace{-0.2cm}
\item Despite the conceptual simplicity of our one-stage framework, it achieves the new state of the art on three popular benchmarks.
%We demonstrate the effectiveness of \modelname with 
\vspace{-0.2cm}
\item We build a large-scale upper-body dataset, \dataname, to bridge the gap between the basic task and downstream applications and provide precise annotations, with which we conduct benchmarking of existing methods. We hope \dataname can inspire new research topics. 
\end{itemize}

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=0.98\linewidth]{fig/dataset.pdf}
\end{center}
\vspace{-0.6cm}
\caption{
Illustration of five previous datasets (from (a) to (e)) and the proposed Upper Body Dataset (from (f) to (t)) with fifteen real-life scenes. \dataname bridges the gap between the basic 3D whole-body estimation task and downstream tasks with highly expressive actions.
}
\label{fig:ubody}
\vspace{-0.6cm}
\end{figure*}