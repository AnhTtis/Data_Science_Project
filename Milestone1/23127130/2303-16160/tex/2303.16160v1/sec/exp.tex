\vspace{0.1cm}
\section{Experiment}
\label{sec:exp}


\subsection{Experimental Setup}
Due to the page limit, we leave the detailed experiment setup, implementation, annotation visualization, qualitative comparison with SOTA methods, and more benchmark results and analyses in the appendix.

\noindent\textbf{Datasets.}
We use COCO-Wholebody~\cite{jin2020wholebody}, MPII~\cite{andriluka2014mpii}, and Human3.6M~\cite{Ionescu_2014_hm36} as the training set. Unlike previous multi-stage methods~\cite{PavlakosGeorgios2020expose, GyeongsikMoon2020hand4whole}, we do not use additional hand-only and face-only datasets for training as a simple baseline for a one-stage method. The SMPL/SMPL-X pseudo-GTs are obtained from EFT~\cite{joo2020exemplar} and NeuralAnnot~\cite{Moon_2022NeuralAnnot}. 

\noindent\textbf{Evaluation metrics.}
For 3D whole-body mesh recovery, we utilize the mean per-vertex position error (MPVPE) as our primary metric. In addition, we apply \emph{Procrustes Analysis} (PA) to the recovered mesh, and report the PA-MPVPE after rigid alignment. For AGORA, we also report normalized mean vertex error (N-PMVPE) to compensate for missing detection. Hand error is calculated as the mean of the left and right hands.
%
For 3D body-only recovery on 3DPW, we follow previous works~\cite{zeng2022deciwatch,Kolotouros_2019_spin} to report the mean per joint position error (MPJPE) and PA-MPJPE. 
%
All reported errors are in units of millimeters.

\noindent \textbf{Implementation details.}
\modelname is implemented in Pytorch and trained using the Adam optimizer with an initial learning rate of $1\times 10^{-4}$ for 14 epochs. Scaling, rotation, random horizontal flip, and color jittering are used as data augmentations during training. We set the number of body tokens $\mathbf{T}_b$ and component tokens $\mathbf{T}_c$ to 27 and 92, respectively. 



\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}
{
\begin{tabular}{l|ccc|cc|ccc|ccc|cc}
\specialrule{.1em}{.05em}{.05em}
\multirow{3}[4]{*}{\textbf{Method}} & \multicolumn{5}{c|}{\textbf{AGORA-test}} & \multicolumn{6}{c|}{\textbf{EHF}} &\multicolumn{2}{c}{\textbf{3DPW}} \\
\cmidrule{2-14}
& \multicolumn{3}{c|}{\textbf{MPVPE $\downarrow$}} & \multicolumn{2}{c|}{\textbf{N-MPVPE $\downarrow$}}& \multicolumn{3}{c|}{\textbf{MPVPE $\downarrow$}} & \multicolumn{3}{c|}{\textbf{PA-MPVPE $\downarrow$}}&\textbf{MPJPE $\downarrow$} & \textbf{PA-MPJPE $\downarrow$} \\
\cmidrule{2-14}
 & \textbf{All} & \textbf{~Hands~} & \textbf{~Face~} & \textbf{All} & \textbf{Body} & \textbf{All} & \textbf{Hands} & \textbf{Face} & \textbf{All} & \textbf{Hands} & \textbf{Face}& \textbf{Body}& \textbf{Body}\\ 
 \hline
ExPose~\cite{PavlakosGeorgios2020expose} &  217.3 & 73.1 & 51.1 & 265.0 & 184.8 & 77.1 & 51.6 & 35.0 & 54.5 & 12.8 & 5.8 & 93.4 & 60.7 \\
FrankMocap~\cite{Rong_2021frank} &  - & 55.2 & - & - & 207.8  &  107.6 & \underline{42.8} & - &57.5 & 12.6 & - & 96.7 & 61.9 \\
PIXIE~\cite{Feng_2021_pixie} &  191.8 & 49.3 & 50.2 & 233.9 & 173.4 & 89.2 & \underline{42.8} & 32.7 & 55.0 & \underline{11.1} & \textbf{4.6} & 91.0 & 61.3 \\

Hand4Whole~\cite{GyeongsikMoon2020hand4whole}  & - &-& -& -&- &79.2&	43.2	&\textbf{25.0} &53.1&	12.1&	5.8 &-&-  \\
Hand4Whole~\cite{GyeongsikMoon2020hand4whole}$\times$ & 135.5 & 47.2 & 41.6& 144.1&	96.0  & \underline{76.8} & \textbf{39.8} & \underline{26.1}&\textbf{50.3} & \textbf{10.8} & 5.8 &  \underline{86.6} & \underline{54.4}  \\
 \midrule
\modelname (Ours) &  \textbf{122.8}{\color{Red}$\downarrow_{9.5\%}$}& \textbf{45.7} & \textbf{36.2}&~~~~\textbf{130.6}~~~~&~~~~\textbf{85.3}~~~~&\textbf{70.8}{\color{Red}$\downarrow_{7.8\%}$} &53.7 &26.4 &\underline{48.7}&15.9&6.0&\textbf{74.7}{\color{Red}$\downarrow_{13.4\%}$} & \textbf{45.1} \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}}
\vspace*{-3mm}
\caption{3D body reconstruction error comparisons on three existing datasets. $\times$ uses additional hand-only and face-only training datasets. }
\label{table:sota_compare}
\vspace{-0.2cm}
\end{table*}

 
\begin{table*}[h]
\begin{center}
\begin{minipage}[t]{0.405\textwidth}
\centering
\resizebox{0.95\linewidth}{!}
{
\begin{tabular}{l|cccc}
\toprule
\textbf{Hand}& ~\textbf{Ours}          & \textbf{w/o \emph{H.D.} }      & \textbf{w/o \emph{K.G}} & \textbf{w/o both }     \\
\midrule
\textbf{MPVPE }      & \textbf{53.7} & 55.3 & 55.1 &56.4 \\
\textbf{PA-MPVPE}   & \textbf{15.9} & 17.7 & 17.6 & 18.1 \\
\toprule
\textbf{Face}       & \textbf{Ours   }       & \textbf{w/o \emph{F.D.} }      & \textbf{w/o \emph{K.G}} & \textbf{w/o both}      \\
\midrule
\textbf{MPVPE}     & \textbf{26.4} & 27.2 & 26.4 & 26.8 \\
\textbf{PA-MPVPE}   & 6.0  & 5.9  & \textbf{5.8}  & 6.0 \\
\toprule
\textbf{Upsampling} & \textbf{$\times$ 1}&\textbf{$\times$ 2}&\textbf{$\times$ 4}&\textbf{$\times$ 8}\\
\midrule
\textbf{MPVPE} &54.9 & 54.3&\textbf{53.7} &54.1\\
\bottomrule
\end{tabular}}
\vspace{-0.2cm}
\caption{Ablation study of component-aware decoder on EHF with \emph{H.D.}, \emph{F.D.}, \emph{K.G}, and upsampling strategies. \emph{H.D.}, \emph{F.D.}, and \emph{K.G} are abbreviations for Hand Decoder, Face Decoder and Keypoint-Guided scheme. 
}
\label{tab:Break_Down}
\vspace{-0.7cm}
\end{minipage}
\quad
% Complexity
\begin{minipage}[t]{0.57\textwidth}
\centering
  \resizebox{\linewidth}{!}
{
    \begin{tabular}{l|ccc|ccc|cc}
    \toprule
    \multicolumn{1}{l|}{\multirow{2}[4]{*}{\textbf{Method}}} & \multicolumn{3}{c|}{\boldmath{}\textbf{MPVPE $\downarrow$}\unboldmath{}} & \multicolumn{3}{c|}{\boldmath{}\textbf{PA-MPVPE $\downarrow$}\unboldmath{}}&\multicolumn{2}{c}{\boldmath{}\textbf{PA-MPJPE $\downarrow$}\unboldmath{}}\\
    \cmidrule{2-9}
    & \textbf{All} & \textbf{Hand} & \textbf{Face} & \textbf{All} & \textbf{Hand} & \textbf{Face} &\textbf{Body} & \textbf{Hand} \\
    \midrule
    % SMPLify-X~\cite{Pavlakos_2019smplx} & & & & & & & & \\
    ExPose~\cite{PavlakosGeorgios2020expose} &171.5 & 83.7 & 45.1 & 66.9 & 12.0 & 3.9 &70.7	&12.3 \\
    % FrankMocap~\cite{Rong_2021frank} & & & & & & & & \\
    PIXIE~\cite{Feng_2021_pixie} &168.4 & 55.6 & 45.2 & 61.7 & 12.2  & 4.2 & 66.8	&12.3 \\
    Hand4Whole~\cite{GyeongsikMoon2020hand4whole} &104.1 & \underline{45.7} & 27.0 & 44.8 & \underline{8.9}  & 2.8 & 45.5&	\underline{9.0} \\
    %Hand4Whole~\cite{GyeongsikMoon2020hand4whole}$\dagger$ &	80.3&	40.9&	21.9&43.1&	7.4&	2.0&49.6&	- \\
    Hand4Whole~\cite{GyeongsikMoon2020hand4whole}$\times$ &157.4 & 62.2 & 49.8 & 82.2 & 9.8  & 3.9 & 92.8&	10.0\\
    \midrule
    \modelname (Ours) & \underline{92.4} & 47.7  &\underline{24.9} & \underline{42.4}& 10.8 & \underline{2.4}  & \underline{42.9}&	11.0 \\
    \modelname (Ours)$\dagger$ & \textbf{81.9}&	\textbf{41.5}&	\textbf{21.2}&\textbf{42.2}&\textbf{8.6}&	\textbf{2.0}&	\textbf{48.4}&	\textbf{8.8} \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-0.2cm}
  \caption[Reconstruction errors on the proposed \dataname test set.]{Reconstruction errors on \dataname test set on the \emph{intra-scene} protocol. All models are pretrained on previous datasets, except for the results labeled by (i) $\dagger$: finetuned on the \dataname training data; (ii) $\times$: finetuned on the AGORA training data. The result of the \emph{inter-scene} setting is in the appendix.}
  \label{tab:3d_smplx_results}%
  \end{minipage}
  \vspace{-0.9cm}
  \end{center}
\end{table*}

\subsection{Comparisons with Existing Methods}

Table~\ref{table:sota_compare} provides a comprehensive comparison of \modelname and existing whole-body mesh recovery methods. 
%
As the first one-stage method, \modelname surpasses existing multi-stage models with complex designs in most cases. Notably, \modelname has not been trained on hand-only and face-only datasets~\cite{Zimmermann_2019FreiHAND,TeroKarras2018ASG,GyeongsikMoon2020InterHand26MAD}. 
%
Our \emph{All MPVPEs} show a $9.5$\% improvement on AGORA test set and $7.8$\% improvement on EHF than SOTA~\cite{GyeongsikMoon2020hand4whole}.
%
Since AGORA is a more complex and natural dataset than EHF, previous works~\cite{GyeongsikMoon2020hand4whole,Patel_2021agora} claim it is more convincing and representative of real-world scenarios. We also visualize the misleading \emph{high-error} cases on EHF in Figure~\ref{fig:vis_ehf}.
%
Besides, we obtain a SOTA performance on the body-only dataset, 3DPW, with a $13.4$\% error reduction compared to these whole-body methods. More qualitative results are available in the appendix.

\begin{figure}[t]
\vspace{-0.2cm}
\begin{center}
\includegraphics[width=1\linewidth]{fig/ehf.pdf}
\end{center}
\vspace{-0.4cm}
\caption{
Illustration of the inconsistency between quantitative and qualitative results compared Hand4Whole~\cite{GyeongsikMoon2020hand4whole} (the middle one) with \modelname (the right figure) on EHF.
}
\vspace{-0.6cm}
\label{fig:vis_ehf}
\end{figure}
 
% \vspace{-0.1cm}
\subsection{Ablation Study}
\vspace{-0.1cm}
\noindent \textbf{Impact of the component-aware decoder.}
Unlike body-only pose estimation, whole-body mesh recovery requires attention to both the body's posture, which is on a larger spatial scale, and the gesture and expression of the hands and face, which are on a finer scale. To handle the resolution issue in a one-stage pipeline, we propose the component-aware decoder attached to the component-aware encoder.
%
First, in the upper Table~\ref{tab:Break_Down}, we verify the effectiveness of the proposed decoder for both hand and face regression. We can observe a significant drop without the decoder (\eg, \emph{w/o H.D.} and \emph{w/o F.D.}, indicating that simply regressing the low-resolution hand and face directly from the encoder is inferior. 
%
Moreover, the errors will also increase without the proposed keypoint-guided deformable attention scheme, as shown in the medium Table~\ref{tab:Break_Down}. In particular, the performance of the hand estimation is highly influenced, showing that hand pose estimation attends more to the sparsely deformable spatial information to obtain better queries.

\noindent \textbf{Impact of the up-sampling strategy.}
To relieve the low-resolution problem of hand and facial features, we design the feature up-sampling strategy in the decoder to obtain multi-scale higher-resolution features. 
%
The lower Table~\ref{tab:Break_Down} presents the impact of different up-sampling  scale. 
%
As the up-sampling scale increases, the MPVPE decreases and then reaches a saturation point. Therefore, we use three scales (i.e., $[\times 1, \times 2, \times 4]$) by default in our experiments.

\vspace{-0.2cm}
\subsection{Benchmark on UBody}
\vspace{-0.2cm}
As a new dataset, we provide both quantitative and qualitative results on \dataname. Table~\ref{tab:3d_smplx_results} presents the performance comparisons of existing 3D whole-body methods. The general result ranking is similar to AGORA. Since the upper body is closer to the camera, their errors will be smaller than AGORA. However, the hand and face will play a more important role than previous data. 
%
Besides, we finetune Hand4Whole on AGORA and test again, and we find all errors are significantly enlarged. This observation can be attributed to the data distribution gap between AGORA and \dataname, as shown in Figure~\ref{fig:statis}. Moreover, we train \modelname on our train set and find a 16.1\% improvement compared to the original pretrained model, indicating that \dataname can serve to improve the performance on downstream real-life scenes.


