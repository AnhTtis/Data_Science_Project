\vspace{-0.2cm}
\section{Related Work}
\label{sec:related}
\subsection{Methods of Whole-body Mesh Recovery}
\label{sec:related_method}
Whole-body mesh recovery targets to localize mesh vertices of all human components, including body, hands, and face from monocular images. Most previous works focus only on individual hand~\cite{AdnaneBoukhayma20193DHS,LinHuang2021SurveyOD,TheocharisChatzis2020ACS}, face~\cite{OswaldAldrian2013InverseRO,AyushTewari2017MoFAMD,YuDeng2019Accurate3F,BernhardEgger20203DMF}, and body~\cite{Kolotouros_2019_spin,zeng2022deciwatch,tian2022survey, kocabas2021pare, kocabas2021spec} reconstruction.
%
In contrast, the joint whole-body estimation methods are less addressed. %
Some optimization-based works reconstruct 3D bodies by fitting the detected 2D keypoints from images with additional constraints, but they are slow and prone to local optima~\cite{Pavlakos_2019smplx,Xiang_2019_wholebody3d}.
%
Thanks to the whole-body parametric model (\eg, SMPL-X~\cite{Pavlakos_2019smplx}), learning-based models~\cite{PavlakosGeorgios2020expose,Rong_2021frank,Zhou_2021_full,Feng_2021_pixie, sun2022learning} emerge to train networks to predict expressive body pose, shape, hand gesture, and facial expression. 
%
Due to the low resolution of hands and face, these whole-body methods crop and resize the hands and face images to higher resolutions and feed them into separate expert networks to conduct the corresponding parameter regression. 
%
Specifically, ExPose~\cite{PavlakosGeorgios2020expose} introduces body-driven attention for higher-resolution crops of the face and hand estimation, a dedicated refinement module, and part-specific knowledge from existing hand-only and face-only datasets.
%
FrankMocap~\cite{Rong_2021frank} presents a regression-and-integration method to build a fast and accurate system.
% 
PIXIE~\cite{Feng_2021_pixie} produces animatable whole body with realistic facial details via a moderator to fuse body part features adaptively.
%
Recently, Hand4Whole~\cite{GyeongsikMoon2020hand4whole} utilizes both body and hand joint features for accurate 3D wrist rotation and smooth connection between body and hands.

%
Nevertheless, these methods aim at high performance by using separate networks in a divide-and-conquer fashion for different components and a specific fusion module to paste them together.
%
The multi-stage pipelines lead to high complexity and inevitably cause inconsistent and unnatural articulation of the mesh and implausible 3D wrist rotations, especially in occluded, truncated, and blurry contexts. Until now, one-stage methods in this task are unexplored.


\subsection{Benchmarks of Expressive Body}

Some datasets with parametric model annotations~\cite{Patel_2021agora,Pavlakos_2019smplx,von_Marcard_3dpw,cai2022humman,Ionescu_2014_hm36,Moon_2022NeuralAnnot,HanbyulJoo2022eft} have been developed to advance the field. 
%
Table~\ref{tab:datasets} summarizes these datasets from the annotation type, size, scene diversity, \etc. 
%
To be specific, EHF~\cite{Pavlakos_2019smplx} is the first evaluation dataset for SMPL-X-based models, which is built by capturing 3D body shapes with a scanning system and then fitting the SMPL-X model to the scans.
%
AGORA~\cite{Patel_2021agora} is a synthetic dataset with high realism and accurate ground truth, which is by far the most commonly used test data due to the diversity of subjects, environments, clothes, and occlusions.
%
Notably, people in AGORA are often far from the camera, and their hands and face are obscured and have small resolutions, making existing methods focus more on body rather than hand and face estimation. 

Since marker-based 3D mocap labels are hard to obtain, there are a few annotation methods~\cite{Moon_2022NeuralAnnot,Pavlakos_2019smplx,Feng_2021_pixie,Muller_2021contact,rockwell2020full,pavlakos2022multishot} for high-precision labeling for both monocular indoor and outdoor scenes.
%
FBA~\cite{rockwell2020full} emphasizes the severe failure cases of existing body recovery methods on consumer video data due to unusual camera viewpoints and aggressive truncations. They annotate pseudo 2D body keypoints and SMPL annotations via HMR~\cite{Kanazawa_2018_hmr} on 13k frames across four action recognition datasets.
%
Multi-shot-AVA~\cite{pavlakos2022multishot} also argues that data from edited media, like movies with rich appearances,  interactions between humans, and various temporal contexts, is valuable. They apply the proposed multi-shot optimization on AVA~\cite{ChunhuiGu2017AVAAV} to get pseudo 3D ground truth.
%
Interestingly, a body recovery benchmark~\cite{pang2022benchmarking} finds that simply using the 2D COCO dataset with pseudo-3D labels can surprisingly achieve a better performance and generalization ability.
%
To complement these prior datasets and focus on expressive body recovery, we construct a new benchmark with high-quality 2D and 3D whole-body annotations.  


\begin{table*}[h]
  \centering
  \resizebox{\textwidth}{!}{
% Table generated by Excel2LaTeX from sheet 'Datasets (2)'
\begin{tabular}{llcccccccc}
\toprule
\textbf{Type} & \multicolumn{1}{l}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{\makecell{\#Frames}}} & \multicolumn{1}{c}{\textbf{\makecell{Scenes}}} &  \multicolumn{1}{c}{\textbf{\makecell{~Multi\\Person~}}} & \textbf{\makecell{~In-the-\\wild~}} &\textbf{\makecell{~Upper\\Body~}}&\multicolumn{1}{c}{\textbf{\makecell{~Video~}}} & \textbf{\makecell{Annotation\\Type}} & \textbf{\makecell{Annotation\\Source}} \\
\midrule
\multirow{1}[1]{*}{\makecell{Rendered}} 
      & AGORA~\cite{Patel_2021agora} & 17K   &Daily    & Y & N   &N &N & SMPL-X & \cite{Patel_2021agora} \\
\midrule
\multirow{2}[2]{*}{\makecell{Marker/Sensor-\\based MoCap}} 
      & Human3.6M~\cite{Ionescu_2014_hm36} & 3.6M  & Daily      & N     &N   &N  &Y    & SMPL-X  & \cite{Moon_2022NeuralAnnot} \\
      &  3DPW~\cite{von_Marcard_3dpw} &  $>$ 51K &  Daily &  Y &   Y&N & Y& SMPL-X &  \cite{Moon_2022NeuralAnnot} \\
\midrule
\multirow{3}[2]{*}{\makecell{Marker-less\\Multi-view\\MoCap}} 
      &  MPI-INF-3DHP~\cite{mehta2017monocular} &  $>$ 1.3M &  Daily &   N &  Y&N & Y& SMPL-X &  \cite{Moon_2022NeuralAnnot} \\
      & EHF~\cite{Pavlakos_2019smplx} & 0.1K   & Daily        & N     & N   &N    &N& SMPL-X & \cite{Pavlakos_2019smplx} \\
      & ZJU-MoCap~\cite{peng2021neural} &  $\geq$ 237K     & Daily     & N     & N   &N&Y     & SMPL-X & \cite{easymocap} \\
\midrule
\multirow{8}[2]{*}{\makecell{\\Pseudo-\\3D Labels}} 
      &  PennAction~\cite{zhang2013penn} &  77K &  Fitness &   N &   Y&N &Y&  SMPL &  \cite{zhang2019predicting} \\
      & MSCOCO~\cite{lin2014coco} & 200K   & Daily      & Y &  Y&N & N&SMPL-X  & \cite{Moon_2022NeuralAnnot} \\
      & COCO-Wholebody~\cite{jin2020wholebody} & 200K   & Daily  & Y &  Y&N&N & 2D KPT  & \cite{jin2020wholebody} \\
      &  MPII~\cite{andriluka2014mpii} &  25K &  Daily  &  Y &  Y&N &  N&SMPL-X &  \cite{Moon_2022NeuralAnnot} \\
      & MTP~\cite{muller2021self} & 3.8K & Daily   & N     &  Y&N &N& SMPL-X & \cite{muller2021self} \\
      & FBA~\cite{rockwell2020full}&13K&Vlog\&Cook\&Daily&Y& Y&N&Y&SMPL&\cite{rockwell2020full}\\
      &Multi-shot-AVA~\cite{pavlakos2022multishot}&350K&Movie&Y& Y&N&Y&SMPL&\cite{pavlakos2022multishot}\\
      \cmidrule{2-10}
      & \textbf{\dataname (Ours)} &$>$1051K & Real-life Scenes  & Y      &  Y&Y &Y& SMPL-X\&2D KPT & Ours \\
    \bottomrule
    \end{tabular}%
     }
     \vspace{-0.1cm}
    \caption{Comparison of related datasets. \dataname is a large-scale upper-body dataset with high-precision whole-body annotations. }
    \vspace{-0.2cm}
  \label{tab:datasets}%
\end{table*}%


