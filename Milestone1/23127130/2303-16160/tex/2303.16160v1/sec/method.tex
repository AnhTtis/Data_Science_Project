\section{Method}
\label{sec:method}

\begin{table}
\centering
\scalebox{0.85}{
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Method}}& \multicolumn{3}{c|}{\textbf{AGORA-val}}  & \multicolumn{3}{c}{\textbf{EHF}}      \\
\cmidrule{2-7}
& \textbf{Hand}  & \textbf{Face} & \textbf{All}   & \textbf{Hand} & \textbf{Face} & \textbf{All}   \\
\midrule
Ori.              & \underline{73.3}  & \underline{81.4} & \underline{183.8} & \underline{42.7} & 25.7 & 77.5  \\
% Ori.+1/2 Hand & 73.7  & 81.6 & 183.3 & 45.2 & 25.7 & 78.0    \\
Ori.+1/4 Hand & 75.7  & \textbf{80.8} & \textbf{183.0}   & 50.9 & 24.8 & 78.5  \\
% Ori.+1/2 Face & \textbf{73.0} & 81.6 & 184.6 & 42.6 & 25.2 & 77.6  \\
Ori.+1/4 Face & \textbf{73.2}  & 81.8 & 184.0   & \textbf{41.3} & \underline{24.2} & \textbf{77.0}    \\
\midrule
Share Backbone    & 81.1  & 91.0   & 202.3 & 55.5 & 33.5 & 84.7  \\
Share+1/4 Hand & 77.4  & 86.1 & 188.6& 57.0   & 25.8 & 84.8  \\
Share+1/4 Face & 79.5  & 85.0   & 196.7 & 57.8 & \textbf{24.4} & 82.5 \\
\bottomrule
\end{tabular}}
\vspace{-0.2cm}
\caption{A preliminary study on the effect of different component scales and share backbone for all components' feature extraction.}
\label{tab:motivation}
\vspace{-0.7cm}
\end{table}

\subsection{Motivation}
\label{sec:motivate}

A one-stage framework is vital to simplify the cumbersome processes without hand-craft and complex integration designs. 
%
However, translating from multi-stage methods directly to a one-stage method is nontrivial.
We take the present state-of-the-art method Hand4Whole~\cite{GyeongsikMoon2020hand4whole} as an example to perform some preliminary studies on bringing the gap between the multi-stage method and one-stage approach. 
On the one hand, we replace its separate backbones with a shared backbone for all human components. On the other hand, we explore different crop-and-resize image resolutions for the hands and face, as they usually have small image resolutions.


Table~\ref{tab:motivation} shows that, when we transition from the original setup (\emph{Ori.}) to a shared backbone (\emph{Share Backbone}), all recovery errors are severely deteriorated on two datasets. Specifically, MPVPE increases from $183.8$mm to $202.3$mm (a \textbf{10.1}\% drop) on AGORA~\cite{Patel_2021agora}, and from $77.5$mm to $84.7$mm (a \textbf{9.3}\% drop) on EHF for all components (\emph{All}). 
These results indicate that extracting the multi-component whole-body features with a shared backbone is difficult. Notably, the hand estimation performance deteriorates by
\textbf{30.0}\% on EHF.
Based on the results of different resolutions, we summarize some interesting observations as follows: 
(i)~Overall, changing the resolution of the hand results in a larger performance drop than the face on EHF;
(ii)~When not sharing a backbone, the results are generally worse with smaller input resolutions of the hands and face.



\begin{figure*}[t]
\begin{center}
\includegraphics[width=.95\linewidth]{fig/osx_pipeline.pdf}
\end{center}
\vspace{-0.4cm}
\caption{
The overview of the proposed one-stage framework (\modelname) with component-aware transformer. It includes (a) a component-aware Transformer encoder and (b) a component-aware Transformer decoder.
}
\label{fig:method}
\vspace{-0.4cm}
\end{figure*}


\subsection{Building Component Aware Transformer}
\label{sec:method_1}

 As an attempt to break the above status quo, we propose a one-stage framework with a vision transformer encoder and decoder for expressive full-body mesh recovery, named \modelname. It is simple in design and effective in full-body mesh prediction, as we will demonstrate later. We hope it can serve as a baseline for future one-stage methods.
%
Given a human image $\mI \in \mathbb{R}^{H \times W \times 3}$, our component-aware Transformer (\emph{CAT}) estimates the corresponding body, hand, and face parameters $\hat{\cP}= \{\hat{\mP}_{body}, \hat{\mP}_{lhand}, \hat{\mP}_{rhand}, \hat{\mP}_{face}\}$ and then feed them into a SMPL-X layer~\cite{Pavlakos_2019smplx} to obtain the final 3D whole-body human mesh. 
%
Specifically, $\hat{\mP}_{body}$ contains 3D body joint rotation $\theta_{body} \in \mathbb{R}^{22 \times 3}$, body shape $\beta\in \mathbb{R}^{10}$, and 3D global translation $t\in \mathbb{R}^{3}$. %$\textbf{t}_{body}\in \mathbb{R}^{3}$. 
%
For $\hat{\mP}_{lhand}$ and $\hat{\mP}_{rhand}$, they have 3D left and right hand joint rotation $\theta_{lhand} \in \mathbb{R}^{15 \times 3}$ and $\theta_{rhand} \in \mathbb{R}^{15 \times 3}$, respectively. 
%
$\hat{\mP}_{face}$ consists of 3D jaw rotation $\theta_{face}\in \mathbb{R}^{3}$ and facial expression $\phi \in \mathbb{R}^{10}$.
%
Our training target is to minimize the distance between the recovered parameters $\hat{\cP}$ and the ground-truth parameters $\cP$. As shown in Figure~\ref{fig:method}, the proposed \emph{CAT} consists of a component-aware encoder to capture the global correlation and extract high-quality multi-scale feature, and a component-aware decoder to strengthen the hand and face regression via an up-sampling strategy to obtain higher-resolution feature maps.

\subsection{Body Regression via Global Encoder}
\label{sec:method_2}
In the component-aware encoder, the human image $\mI$ is split into fixed-size image patches $\mathbf{P}\in \mathbb{R}^{\frac{HW}{M^2}\times (M^2\times3)}$, where $M$ is the patch size. The patches $\mathbf{P}$ are then linearly projected by a convolution layer %with a convolution layer into a feature with the shape of $\frac{H}{d} \times \frac{W}{d} \times C$,%
and added with position embeddings $\mathbf{P_e} \in \mathbb{R}^{\frac{HW}{M^2}\times C}$ to obtain a sequence of feature tokens $\mathbf{T_f}\in \mathbb{R}^{\frac{HW}{M^2}\times C}$.
%
To explicitly leverage the body prior and learn the body information in the encoder, we concatenate the feature token $\mathbf{T_f}$ with the body tokens $\mathbf{T_b} \in \mathbb{R}^{B  \times C}$, which are learnable parameters. The concatenated tokens are then fed into a standard Transformer encoder with multiple Transformer blocks~\cite{dosovitskiy2020vit}. Each block consists of a multi-head self-attention, a feed-forward network (FFN), and two layer normalization. 
%
After the global feature fusion, the body tokens and image feature tokens are updated into $\mathbf{T_b}' \in \mathbb{R}^{B \times C}$ and  $\mathbf{T_f}'\in \mathbb{R}^{\frac{HW}{M^2}\times C}$. Finally, we use several fully connected layers to regress the body parameters $\hat{\mP}_{body}=\{\theta_{body} , \beta, t\}$ based on $\mathbf{T_b}'$.


\subsection{High-Resolution Decoder for Hand and Face}
\label{sec:method_3}
% how to obtain precise estimation on low-resolution hand and face estimation
\noindent \textbf{Up-sampling for multi-scale high-resolution features.}
%
Since the hands and face in a human image are usually small, previous methods upsample the human image and crop out the hands and face to obtain higher-resolution images. However, this image-level upsampling-crop scheme requires additional backbones to extract the hand and face features separately. 
To solve this problem, we propose a differentiable feature-level upsampling-crop strategy to enhance the hands and face regression process as inspired by the recent ViTDet~\cite{Li2022vitdet}.
%
% Specifically, to relieve the cropping quantization error on a low-resolution feature map, 
Specifically, we reshape the feature tokens $\mathbf{T_f}'$ into a feature map and upsample it into multiple higher-resolution features $\mathbf{T}_{hr}$ via deconvolution layers. 
Then, since decoding the hand and face component information from the full feature map inevitably leads to redundant computation and makes the computation process inefficient, we perform differentiable RoIAlign~\cite{He_2017_maskrcnn} on the feature maps and crop out multi-scale hand feature maps $\mathbf{T}_{hand}$ and face feature maps $\mathbf{T}_{face}$, according to the predicted hand and face bounding boxes, which are regressed from  $\mathbf{T_f}'$ using FFNs. 
The up-sampling and decoding processes for hand and face components are the same, and we illustrate the case of hand parameter regression in detail in Figure~\ref{fig:method}(b).
The cropped multi-scale hand features can be represented as $\mathbf{T}_{hand}=\{\mathbf{F}_{lr}, ..., \mathbf{F}_{hr}\}$. The low-resolution feature $\mathbf{F}_{lr} \in \mathbb{R}^{\frac{H'}{M} \times \frac{W'}{M} \times C}$ is cropped from the original low-resolution feature map, where $H'$ and $W'$ are the height and width of hand image patches. $\mathbf{F}_{hr}$ is the highest-resolution feature. The cropped multi-scale features then serve as memory tokens $\mathbf{V}$ for the keypoint-guided component-aware decoder.
%
To relieve the computational pressure, we reduce the token dimension from $C$ to $C'$ in the component-aware decoder, where $C' = C/2$. 

\begin{figure*}[h]
\centering
\includegraphics[width=0.97\linewidth]{fig/annotation.pdf}
\vspace{-0.3cm}
\caption{
Illustration of the annotation pipeline of \dataname. Black lines show the annotation process of 2D whole-body keypoints, and blue lines are the 3D SMPL-X annotation procedure. Red dotted lines mean to update the information.
}
\label{fig:ubody_annotation}
\vspace{-0.3cm}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\includegraphics[width=0.97\linewidth]{fig/annotation_compare.pdf}
\end{center}
\vspace{-0.5cm}
\caption{
Comparisons of (a)~the 2D keypoints annotation quality of wildly used methods~\cite{ZheCao2018OpenPoseRM,FanZhang2020MediaPipeHO} and recent SOTA~\cite{YufeiXu2022ViTPoseSV} on \dataname (the left part), and (b)~the 3D mesh annotation quality of previous SOTA~\cite{Moon_2022NeuralAnnot} with ours on COCO (the right part).
}
\label{fig:ubody_vis_2d}
\vspace{-0.1cm}
\end{figure*}
% \vspace{-0.5cm}
\noindent \textbf{Keypoint-guided deformable attention decoder.}
To improve the precision of hand and face parameter regression, we leverage 2D keypoint positions as prior knowledge to obtain better component tokens $\mathbf{T_c}$ than random initialization.
%
We simply use the feature map $\mathbf{F}_{lr}$ to regress each 2D keypoint to trade off accuracy and efficiency and regard it as a reference keypoint. The input $\mathbf{T_c}\in \mathbb{R}^{K \times C'}$ of the decoder, which we call the keypoint-guided component tokens, is obtained by summing up reference keypoint feature, pose positional embedding, and learnable embeddings.
%
We then pass the keypoint-guided component token through $N$ deformable attention blocks as inspired by deformable DETR~\cite{Zhu_detr21}. To relieve the issue of looking over all possible spatial locations, these blocks learn a small set of sampling points (\eg, four here) around the reference keypoint and further enlarge the feature spatial resolution while maintaining computational efficiency compared to vanilla DETR~\cite{carion2020detr}.
%
Each block is composed of a multi-head self-attention layer, a multi-scale deformable cross-attention layer, and FFNs. In the deformable cross-attention layer, keypoint queries $\mathbf{Q}$ extract features from the elements of multi-scale features $\mathbf{V}$ around the position of keypoints $p_q$:
\begin{equation}
    \text{CA}(\mathbf{Q}, \mathbf{V}, p_q)=\sum_{l=1}^L\sum_{k=1}^KA_{lqk} W\mathbf{V}_l(\phi_l(p_q)+\Delta p_{lqk}),
\end{equation}
where $l$ and $k$ index the feature level and keys, $A$ and $W$  are attention weight and learnable parameter. $\phi(\cdot)$ and $\Delta p$ are position rescaling and offset. After that, the updated component tokens $\mathbf{T_c}'\in \mathbb{R}^{K \times C'}$ will be fed into hand or face regression head to output the final hand or face parameters ($\hat{\mP}_{lhand}, \hat{\mP}_{rhand}, \hat{\mP}_{face}$), respectively.

\noindent \textbf{Loss Function.}
\modelname is trained in an end-to-end manner by minimizing the following loss function:
\begin{equation}
    L = L_{smplx} + L_{kpt3D} + L_{kpt2D} +L_{bbox2D}.
    \label{eq:loss}
\end{equation}
The four items are calculated as the L1 distance between the ground truth values and the predicted ones. Specifically, $L_{smplx}$ provides the explicit supervision of the SMPL-X parameters. $L_{kpt3D}$, $L_{kpt2D}$, and $L_{bbox2D}$ are regression losses for 3D whole-body keypoints, projected 2D whole-body keypoints, and left/right hands and face 2D bounding boxes. More details are provided in the Appendix.