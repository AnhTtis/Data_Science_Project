% \title{\emph{Supplementary Material}\\ One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer}  % **** Enter the paper title here
% \maketitle
% \thispagestyle{empty}
% \appendix


% \null\vfill % Add vertical space at the top of the page
% \begin{center}
% \begin{minipage}{\textwidth} % Use a minipage to keep the text centered
%   \centering
%   \textbf{\emph{Supplementary Material}\\ One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer}
% \end{minipage}
% \end{center}
% \vfill\null % Add vertical space at the bottom of the page
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

\renewcommand{\thesection}{\Alph{section}}   
\renewcommand {\thetable} {S-\arabic{table}}
\renewcommand {\thefigure} {S-\arabic{figure}}

\pagebreak

\twocolumn[{
	\renewcommand\twocolumn[1][]{#1}
	\begin{center}
		\textbf{\Large Supplementary Material:\\One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer}
        \vspace{0.8cm}
        \end{center}
}]

\section*{Overview}
\noindent This supplementary material presents more details and additional results not included in the main paper due to page limitation. The list of items included are:


\begin{itemize}
    \item More experiment setup and details in Sec.~\ref{sec:exp_setup}.
    \item Efficiency comparison with SOTA in Sec.~\ref{sec:efficiency_comp}.
    \item Experiment on AGORA dataset in Sec.~\ref{sec:agora}. 
    \item More introduction of UBody in Sec.~\ref{sec:ubody_intro}. 
    \item Inter-scene benchmark on UBody dataset in Sec.~\ref{sec:inter_bench}. 
    \item Qualitative comparisons with SOTA in Sec.~\ref{sec:visual}.
\end{itemize}

\blfootnote{$\S$ Work done during an internship at IDEA; ${\P}$~Corresponding author.}\

\section{Experiment Setup}
\label{sec:exp_setup}

\noindent\textbf{Evaluation metrics.} To quantitatively evaluate the performance of human mesh recovery, MPVPE, PA-MPVPE, MPJPE, and PA-MPJPE are used as evaluation metrics. Besides, we also report normalized mean vertex error (NMVE) and normalized mean joint error (NMJE) by the standard detection metric, F1 score (the harmonic mean of recall and precision) to penalize models for misses and false positives on AGORA test set with many multi-person scenes.

\noindent\textbf{Implementation details.} Our \modelname model is implemented in Pytorch. It is trained with Adam optimizer ($\beta_1=0.1, \beta_2=0.999$) using the Cosine Annealing scheme for 14 epochs. The learning rate is initially set to $1\times10^{-4}$. The batch size is set to 192. Random scaling, rotation, horizontal flip, and color jittering are used as data augmentations during training. The spatial size of the input image is $256\times 192$. The number of body tokens $\mathbf{T}_b$ and component tokens $\mathbf{T}_c$ are set to 27 and 92, respectively. During experiments on the AGORA-test set, we remove the decoder as we find that the decoder increases training time and does not significantly improve performance on AGORA-test set. This observation may be attributed to the fact that the main problem of AGORA is occlusion, while the decoder aims to estimate hands/face at a finer level.

\section{Efficiency comparison with SOTA methods}
\label{sec:efficiency_comp}
\noindent We report the complexity comparisons including average inference time, number of model parameters, FLOPs, and the NMJE-All on AGORA-test in Table~\ref{tab:efficiency_comparison}. The numbers are measured for single-person regression on the same resolution input using a machine with an NVIDIA A100 GPU. OSX has \emph{the shortest inference time and lowest error}, indicating the advantages in practical applications.
        
\begin{table}[h]
    \begin{center}
        \vspace{-3.0mm}
        \hspace{-1mm}
        \scalebox{0.67}{\noindent
            \begin{tabular}{c | c c c c c }
                \toprule
                Method &ExPose~[\textcolor{green}{34}]  & PIXIE~[\textcolor{green}{13}] & H4W~[\textcolor{green}{27}] & PyMAF-X~[\textcolor{green}{48}] & OSX \\
                \midrule
                NMJE-All (mm) &263.3  &230.9  &141.1  &140.0 & 127.6    \\
                Infer Time (ms) &120.2 &192.0 & 73.3 &209.3 & 54.6 \\
                ~Params (M)~ &135.8 &192.9 &77.9 &205.9 & 102.9 \\
                ~FLOPS (G)~ &28.5 &34.3 &16.7 &35.5 & 25.3 \\
                %\midrule
                \bottomrule
        \end{tabular}}
        \vspace{-3.0mm}
        \caption{Efficiency comparisons with multi-stage methods. }
        \label{tab:efficiency_comparison}
    \end{center}\vspace{-8mm}
\end{table}

\begin{table*}[h]
  \centering
  \resizebox{\textwidth}{!}
{
    \begin{tabular}{l|cc|cc|cccc|cccc}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[4]{*}{\textbf{Method}}}  & \multicolumn{2}{c|}{\boldmath{}\textbf{NMVE $\downarrow$}} & 
    \multicolumn{2}{c|}{\boldmath{}\textbf{NMJE $\downarrow$}} & 
    \multicolumn{4}{c|}{\boldmath{}\textbf{MVE $\downarrow$}\unboldmath{}} & \multicolumn{4}{c}{\boldmath{}\textbf{MPJPE $\downarrow$}\unboldmath{}} \\
    \cmidrule{2-13} & \textbf{Full-Body} & \textbf{Body} & \textbf{Full-Body} & \textbf{Body} &
    \textbf{Full-Body} & \textbf{Body} & \textbf{Face} & \textbf{LH/RH} & \textbf{Full-Body} & \textbf{Body} & \textbf{Face} & \textbf{LH/RH} \\
    \midrule
    SMPLify-X~\cite{Pavlakos_2019smplx} & 333.1  & 263.3 & 326.5 & 256.5 & 236.5 &187.0 & 48.9  & 48.3/51.4 & 231.8 & 182.1 & 52.9  & 46.5/49.6 \\
    ExPose~\cite{PavlakosGeorgios2020expose} & 265.0 & 184.8 &263.3 & 183.4  & 217.3 & 151.5 & 51.1  & 74.9/71.3 & 215.9 & 150.4 & 55.2  & 72.5/68.8 \\
    FrankMocap~\cite{Rong_2021frank} & - & 207.8 & - & 204.0  & - & 168.3 & -     & 54.7/55.7 & -     & 165.2 & -     & 52.3/53.1 \\
    PIXIE~\cite{Feng_2021_pixie} &  233.9 & 173.4 & 230.9 & 171.1  & 191.8 & 142.2 & 50.2  & 49.5/49.0 & 189.3 & 140.3 & 54.5  & 46.4/46.0 \\
    Hand4Whole~\cite{GyeongsikMoon2020hand4whole} $^\dagger$ & 144.1 & 96.0 & 141.1 & \underline{92.7} & 135.5 & 90.2  & 41.6  & 46.3/48.1 & 132.6 & 87.1  & 46.1  & 44.3/46.2 \\
    PyMAF-X~\cite{HongwenZhang2022PyMAFXTW} $^\dagger$ & \underline{141.2} & \underline{94.4} & \underline{140.0} & 93.5 & \underline{125.7} & \underline{84.0} & \textbf{35.0} & \textbf{44.6/45.6} & \underline{124.6} & \underline{83.2} & \textbf{37.9} & \textbf{42.5/43.7} \\
    \midrule
    OSX (Ours) $^\dagger$ & \textbf{130.6}{\color{Red}$\downarrow_{7.5\%}$} & \textbf{85.3}{\color{Red}$\downarrow_{9.6\%}$} & \textbf{127.6}{\color{Red}$\downarrow_{8.9\%}$} & \textbf{83.3}{\color{Red}$\downarrow_{10.9\%}$} & \textbf{122.8} & \textbf{80.2} & \underline{36.2} & \underline{45.4/46.1} & \textbf{119.9} & \textbf{78.3} & \textbf{37.9} & \underline{43.0/43.9}  \\
    \bottomrule
    \end{tabular}%
}
  \caption[Reconstruction errors on the AGORA val set.]{Reconstruction errors on the AGORA test set. $^\dagger$ denotes the methods that are fine-tuned on the AGORA training set or similarly synthetic data~\cite{kocabas2021spec}. The best results are shown in \textbf{bold} and the second best results are highlighted with \underline{underlined font}.}
  \label{tab:agora_test}%
\end{table*}%

\section{Experiment on AGORA Dataset}
\label{sec:agora}
In this part, we report the complete result on the AGORA test set and the experiment result on the AGORA val set.

\noindent\textbf{AGORA Test Set.} Table~\ref{tab:agora_test} depicts the complete result on the AGORA test set. All the results are taken from the official leaderboard. As shown, our OSX outperforms other competitors on most metrics, especially on the evaluation of the body and full-body recovery. More specifically, for full-body reconstruction, OSX even surpasses PyMAF-X~\cite{HongwenZhang2022PyMAFXTW} by 10.6 mm, 9.1 mm, 2.9 mm, and 4.7 mm on NMVE, NMJE, MVE, and MPJPE, respectively. Since PyMAF-X has a lower detected person ratio, they have similar results on MVE and MPJPE metrics, which only calculate the matched person. The NMVE and NMJE will take the misses and false positives into account, and we have overall better multi-person estimation with more improvement under the metrics.
Notably, although OSX does not use extra hand-only and face-only datasets, it can achieve competitive results on hand and face metrics, which demonstrates the effectiveness of our component-aware decoder.

\noindent\textbf{AGORA Val Set.} Table~\ref{tab:agora_val} shows the result on the AGORA val set. All the results are taken from ~\cite{GyeongsikMoon2020hand4whole} except OSX. Although we do not use extra hand/face specific datasets during training, OSX outperforms the SOAT method Hand4Whole by 8.3\% on the MPVPE-all, demonstrating the effectiveness of our one-stage method. 

\begin{table}[h]
\centering
  \resizebox{\linewidth}{!}
{
    \begin{tabular}{l|ccc|ccc}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[4]{*}{\textbf{Method}}} & \multicolumn{3}{c|}{\boldmath{}\textbf{MPVPE $\downarrow$}\unboldmath{}} & \multicolumn{3}{c}{\boldmath{}\textbf{PA-MPVPE $\downarrow$}\unboldmath{}}\\
    \cmidrule{2-7}
    & \textbf{All} & \textbf{Hand} & \textbf{Face} & \textbf{All} & \textbf{Hand} & \textbf{Face} \\
    \midrule
    % SMPLify-X~\cite{Pavlakos_2019smplx} & & & & & & & & \\
    ExPose~\cite{PavlakosGeorgios2020expose} &219.8 & 115.4 & 103.5 & 88.0 & 12.1 & 4.8 \\
    FrankMocap~\cite{Rong_2021frank} & 218.0 & 95.2 & 105.4 & 90.6 &11.2 &4.9  \\
    PIXIE~\cite{Feng_2021_pixie} &203.0 & 89.9 & 95.4 & 82.7 & 12.8  & 5.4  \\
    Hand4Whole~\cite{GyeongsikMoon2020hand4whole} &183.9 &	72.8	&81.6 & 73.2 & \textbf{9.7} &	\textbf{4.7} \\
    \textbf{OSX (Ours)} &	\textbf{168.6\color{Red}$\downarrow_{8.3\%}$}&	\textbf{70.6}&	\textbf{77.2} & \textbf{69.4}&	\underline{11.5}& 4.8 \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-0.2cm}
  \caption[]{Reconstruction errors on the AGORA val set.}
  \label{tab:agora_val}%
  \vspace{-0.7cm}
\end{table}

\section{UBody: An Upper Body Dataset}
\label{sec:ubody_intro}

%示意每个场景我们的一张图的标注质量

\subsection{Data Collection}
To bridge the gap between the basic human mesh recovery task and its downstream applications, we design \dataname with two rules. 
%
First, we research a wide range of human-related downstream tasks with upper-body scenes, including gesture recognition~\cite{yoonICRA19,guo2021human,mitra2007gesture}, sign language recognition, and translation~\cite{duarte2021how2sign,rastgoo2021sign,subburaj2022survey,Joze2019MSASLAL,camgoz2021content4all,zhou2021spatial}, person clustering~\cite{brown2021face}, emotion analysis, speaker verification~\cite{nagrani2020voxceleb}, micro-gesture understanding~\cite{liu2021imigue}, audio-visual generation and separation~\cite{pu2017audio}, human action recognition, and localization~\cite{pavlakos2022multishot,ChunhuiGu2017AVAAV,rockwell2020full,siarohin2021motion,fouhey2018lifestyle}, and human video segmentation~\cite{kuang2021flow}. We select the corresponding high-quality datasets from these existing tasks as a part of our data for the corresponding scenarios. In order to ensure a balanced amount of data for each scene, for datasets with many videos (\eg, lasting 20k minutes), we manually selected the videos in which the upper body appeared more frequently.

Second, with all kinds of athletic competitions, entertainment shows, we media, online conferences, and online classes being more and more indispensable, we carefully selected a large number of rich videos from YouTube to provide new opportunities and challenges for potential applications. 

Since some untrimmed videos may have missing main characters, extraneous images such as opening and closing credits, and repetitive actions, we manually fine-cut the long videos. Each edited video is 10 seconds long, which ensures the high quality of the video.

In order to prevent infringement of ownership rights, we only provide download links to the corresponding videos and our labels without any personal information.

In summary, we collect fifteen real-life scenarios with more than \textbf{105,1k} frames. 
%\TODO{How many clips?}. 
We split the train/test sets from two protocols as follows. 
\begin{itemize}
\item
\emph{Intra-scene}: in each scene, the former 70\% of the videos are the training set, and the last 30\% are the test set. The benchmark was provided in the main paper.
\item
\emph{Inter-scene}: we use ten scenes of the videos as the training set and the other five scenes as the test set. Due to the page limit, we present the benchmark in Table~\ref{tab:3d_smplx_results_sup}.
%\TODO{What is the usage of this? We do not use this config in main manuscript, right?}
\end{itemize}

\subsection{Data Annotation Processes}
\label{sec:ubody_annotation}

As shown in Figure~\ref{fig:ubody_annotation_sup}, we design a thorough whole-body annotation pipeline with high precision. It is divided into two stages: 2D whole-body keypoint annotation and 3D SMPLX annotations fitting.
Since \dataname scenes have a number of unpredictable transitions and cutscenes that make it difficult to use the temporal smoothing approaches~\cite{young1995gaus1d,press1990savitzky,zeng2022smoothnet}, the annotation is conducted on a single frame.

\begin{figure*}[h]
\centering
\includegraphics[width=1\linewidth]{fig/annotation.pdf}
\vspace{-0.5cm}
\caption{
Illustration of the annotation pipeline of \dataname. Black lines show the annotation process of 2D whole-body keypoints, and blue lines are the 3D SMPL-X annotation procedure. Red dotted lines mean to update the information.
}
\label{fig:ubody_annotation_sup}
\vspace{-0.3cm}
\end{figure*}

\noindent \textbf{2D whole-body keypoint annotation:} 
%
%The definition of $133$ 2D keypoint follows COCO-wholeBody~\cite{jin2020wholebody}. 
We first detect all persons and their hands in an image via a specific human and hand detector \emph{BodyHands}~\cite{narasimhaswamy2022bodyhands} shown as \emph{Body Detector} and \emph{Hand Detector} in Figure~\ref{fig:ubody_annotation}.
%
Leveraging the recent state-of-the-art 2D pose estimator \emph{ViT-Body-only}~\cite{YufeiXu2022ViTPoseSV}, we use the pre-trained model trained on the COCO~\cite{lin2014coco} dataset to localize 17 body keypoints for each detected single person, named \emph{$K_{Body}$}, which shows highly robust results on many scenes.
%
Due to the diverse scales and motion blur for the fast-moving hands, we find that \emph{Hand Detector} will output false positive samples or miss some hands. To enhance the performance of hand detection, 
we train a 2D whole-body estimator on COCO-wholeBody~\cite{jin2020wholebody} with $133$ 2D keypoints, called \emph{ViT-WholeBody} following the model design of ViTPose~\cite{YufeiXu2022ViTPoseSV} and masked autoencoder pre-trained scheme~\cite{he2022masked}. \emph{ViT-WholeBody} can provide high-recall hand keypoints \emph{$K_{Hand}$}, but the localization precision is low because of the fully one-stage pipeline and low-resolution of hands from the raw image. Accordingly, We can obtain coarse hand bounding boxes by calculating the maximum, and minimum values of the detected left and right-hand keypoints to correct the hand boxes from \emph{Hand Detector} via an IoU matching strategy.
Then, we use the fine hand boxes to crop the hand patches, resize them to a larger size, and put them into our specific pre-trained \emph{ViT-Hand-only} model trained with the hand labels from the COCO-Whole dataset.
%
In summary, \emph{ViT-WholeBody} will output the body, hand, and face 2D keypoints. We use the body output from \emph{ViT-Body-only} to replace the \emph{$K_{Body}$}, and use the fine hand keypoints from \emph{ViT-Hand-only} to change the \emph{$K_{Hand}$}.
As the face of the current SMPL-X model does not require much detail, we simply use the 2D face keypoints \emph{$K_{Face}$} obtained from \emph{ViT-WholeBody}.


\noindent\textbf{3D whole-body mesh recovery annotation:} 
Different from previous optimization-based annotation~\cite{Pavlakos_2019smplx} that may output implausible poses, we use our proposed \modelname to estimate the SMPL-X parameters from human images as a proper 3D initialization to provide pseudo-3D constraints.
%
Benefiting from current 2D keypoint localization that tends to be more accurate, we additionally supervise the projected 2D whole-body keypoints by the above annotated 2D whole-body keypoints as a way to train \modelname.
%
More importantly, to avoid performance degradation from not accurate enough initial labeling and consistently push up the 3D annotation quality, we propose an iterative training-labeling-revision loop for every 30 epochs to train 120 epochs in total. 
% Next, we project the 3D information into a 2D coordinate and obtain the corresponding 2D whole-body keypoints. We can supervise 


% \TODO{Add the difference between previous works}



\section{Inter-Scene Benchmark on UBody dataset}
\label{sec:inter_bench}

Due to the page limit, we further provide another data protocol comparison to show the usage of the proposed \dataname. Table~\ref{tab:3d_smplx_results_sup} presents the performance comparisons of existing 3D whole-body methods. Inter-scene test shows large errors than the intra-scene test due to the different motion and gesture distributions.
%
The model finetuned on AGORA still has a significant gap than trained on the COCO dataset.
Furthermore, we also train Hand4Whole and \dataname on our training set, we can find a consistent improvement compared to the original pretrained model, indicating that \dataname can serve to bridge the gap among these downstream real-life scenes. 
%
Moreover, different from single-frame AGORA and EHF, \dataname provides videos, which can drive progress in spatial-temporal modeling on such edit media sources.

\begin{table}[h]
\centering
  \resizebox{\linewidth}{!}
{
    \begin{tabular}{l|ccc|ccc}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[4]{*}{\textbf{Method}}} & \multicolumn{3}{c|}{\boldmath{}\textbf{MPVPE $\downarrow$}\unboldmath{}} & \multicolumn{3}{c}{\boldmath{}\textbf{PA-MPVPE $\downarrow$}\unboldmath{}}\\
    \cmidrule{2-7}
    & \textbf{All} & \textbf{Hand} & \textbf{Face} & \textbf{All} & \textbf{Hand} & \textbf{Face} \\
    \midrule
    % SMPLify-X~\cite{Pavlakos_2019smplx} & & & & & & & & \\
    ExPose~\cite{PavlakosGeorgios2020expose} &185.7 & 89.5 & 47.2 & 76.4 & 11.8 & 4.0 \\
    % FrankMocap~\cite{Rong_2021frank} & & & & & & & & \\
    PIXIE~\cite{Feng_2021_pixie} &185.0 & 60.9 & 45.3 & 74.5 & 11.9  & 4.2  \\
    Hand4Whole~\cite{GyeongsikMoon2020hand4whole}$\times$ &198.1&	66.9	&51.8 & 90.2&	10.3&	4.1 \\
    Hand4Whole~\cite{GyeongsikMoon2020hand4whole} &109.4 & {50.4} & 24.8 & 57.0 & {8.9}  & 2.7 \\
    Hand4Whole~\cite{GyeongsikMoon2020hand4whole}$\dagger$ &	\underline{87.4}&	\textbf{41.6}&	\underline{22.1}&\underline{46.3}&	\textbf{8.0}&\underline{2.0} \\
    
    \midrule
    \modelname (Ours) & {100.7} & 52.5  &{24.5} & {52.9}& 9.5 & {2.6}   \\
    \modelname (Ours)$\dagger$ & \textbf{82.0}&	\underline{44.2}&	\textbf{21.5}&\textbf{44.2}&\underline{8.8}&	\textbf{1.9} \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-0.2cm}
  \caption[Reconstruction errors on the proposed \dataname test set.]{Reconstruction errors on \dataname test set on the \emph{inter-scene} protocol. All models are pretrained on previous datasets, except for the results labeled by (i) $\dagger$: finetuned on the \dataname training data; (ii) $\times$: finetuned on the AGORA training data. }
  \label{tab:3d_smplx_results_sup}%
  % \vspace{-0.2cm}
\end{table}

\section{Qualitative with SOTA method}
\label{sec:visual}

\noindent\textbf{Qualitative comparisons on AGORA:} 
We compare the mesh quality on the AGORA dataset in Figure~\ref{fig:vis_agora}. Agora is a synthetic dataset with many challenging factors like heavy occlusion, dark environment, and unnatural multi-person interaction. It only has limited actions, \eg, taking phones, walking, sitting,  \etc. We can see \modelname outperforms ExPose~\cite{PavlakosGeorgios2020expose} and Hand4Whole~\cite{GyeongsikMoon2020hand4whole} consistently in terms of global body orientations, whole-body poses, and hand pose.    


\begin{figure*}[h]
\begin{center}
\includegraphics[width=1\linewidth]{fig/agora_viz.pdf}
\end{center}
\vspace{-0.6cm}
\caption{
Comparisons of existing 3D whole-body estimation methods on AGORA.
}
\label{fig:vis_agora}
\end{figure*}


\noindent\textbf{Qualitative comparisons on EHF:} 
The visual comparisons of whole-body mesh recovery quality on the EHF dataset can be found in Figure~\ref{fig:vis_ehf_sup}. As can be seen, \modelname estimates the most accurate whole-body poses, in which the body parts like hands, feet, and hands are better aligned with the person in the image. 

\begin{figure*}[h]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig/ehf_viz.pdf}
\end{center}
\vspace{-0.6cm}
\caption{
Comparisons of existing 3D whole-body estimation methods on EHF.
}
\label{fig:vis_ehf_sup}
\end{figure*}

\noindent\textbf{Qualitative comparisons on UBody}: 
The qualitative comparison on our \dataname is in Figure~\ref{fig:vis_ubody}. \dataname focuses more on the expressive upper body part. Hand4Whole~\cite{GyeongsikMoon2020hand4whole} and our \modelname produces better body mesh recoveries than ExPose~\cite{YufeiXu2022ViTPoseSV}. Close inspection of the hand part shows that our hand recovery is more accurate than Hand4Whole. 

\begin{figure*}[h]
\begin{center}
\includegraphics[width=1\linewidth]{fig/ubody_viz.pdf}
\end{center}
\vspace{-0.6cm}
\caption{
Comparisons of existing 3D whole-body estimation methods on our proposed \dataname.
}
\label{fig:vis_ubody}
\end{figure*}


\noindent\textbf{Visualization of our annotation on UBody:} 
The visualizations of our SMPL-X annotation in our \dataname can be found in Figure~\ref{fig:vis_ubody_gt1}, \ref{fig:vis_ubody_gt2}, and \ref{fig:vis_ubody_gt6}. Our annotation produces high-quality ground truth. In many challenging cases of expressive hand poses, our estimated mesh can capture fine-level details.  

\begin{figure*}[h]
\begin{center}
\includegraphics[width=1\linewidth]{fig/ubody_gt10.pdf}
\end{center}
\vspace{-0.6cm}
\caption{
Illustration of the ground-truth SMPL-X annotation for the eight scenes in \dataname. For each scene, we show the input image (the upper) and our annotation (the lower).
}
\label{fig:vis_ubody_gt1}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\includegraphics[width=1\linewidth]{fig/ubody_gt11.pdf}
\end{center}
\vspace{-0.6cm}
\caption{
Illustration of the ground-truth SMPL-X annotation for seven other scenes in \dataname. For each scene, we show the input image (the upper) and our annotation (the lower).
}
\label{fig:vis_ubody_gt2}
\end{figure*}

% \begin{figure*}[h]
% \begin{center}
% \includegraphics[width=1\linewidth]{fig/ubody_gt3.pdf}
% \end{center}
% \vspace{-0.6cm}
% \caption{
% Illustration of the ground-truth SMPL-X annotation for the three scenes in \dataname. For each scene, we show the input image (the upper) and our annotation (the lower).
% }
% \label{fig:vis_ubody_gt3}
% \end{figure*}

% \begin{figure*}[h]
% \begin{center}
% \includegraphics[width=1\linewidth]{fig/ubody_gt4.pdf}
% \end{center}
% \vspace{-0.6cm}
% \caption{
% Illustration of the ground-truth SMPL-X annotation for the three scenes in \dataname. For each scene, we show the input image (the upper) and our annotation (the lower).
% }
% \label{fig:vis_ubody_gt4}
% \end{figure*}

% \begin{figure*}[h]
% \begin{center}
% \includegraphics[width=1\linewidth]{fig/ubody_gt5.pdf}
% \end{center}
% \vspace{-0.6cm}
% \caption{
% Illustration of the ground-truth SMPL-X annotation for the three scenes in \dataname. For each scene, we show the input image (the upper) and our annotation (the lower).
% }
% \label{fig:vis_ubody_gt5}
% \end{figure*}

\begin{figure*}[h]
\begin{center}
\includegraphics[width=1\linewidth]{fig/ubody_gt12.pdf}
\end{center}
\vspace{-0.6cm}
\caption{
Illustration of the ground-truth SMPL-X annotation for some special cases: \emph{multi-person scenes} and \emph{full body scenes} in \dataname. Our annotation pipeline can still work well on these scenes.
}
\label{fig:vis_ubody_gt6}
\end{figure*}
