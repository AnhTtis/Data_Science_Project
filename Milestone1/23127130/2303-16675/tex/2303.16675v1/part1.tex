\subsection{CVMFS to distribute software on grid resources}\label{section:11}

At the beginning of 2021, CVMFS was managing about 1 billion files delivered to more than 100,000 computing nodes by (i) 10 public data mirror servers - called \emph{Stratum1}s - located in Europe, Asia and the United States and (ii) 400 site-local cache servers \cite{CVMFS_2021}.

To keep the file system consistent and scalable, developers conceived CVMFS as a read-only file system.
Release managers - or continuous integration workers - aiming to publish a software release has to log in to a dedicated machine - named \emph{Stratum0} - with an attached storage volume providing an authoritative and editable copy of a given repository \cite{Blomer_2019}.
Changes are written into a staging area until they are committed as a consistent changeset: new and modified files are transformed into a content-addressed object providing file-based deduplication and versioning.
In 2019, Popescu et al. \cite{Popescu_2019} introduced a gateway component, a web service in front of the authoritative storage, allowing release managers to perform concurrent operations on the same repository and make CVMFS more responsive (Figure \ref{fig:cvmfsGrid}.1.b and \ref{fig:cvmfsGrid}.2.b).

The transfer of files is then done lazily via HTTP connections initiated by the CVMFS clients \cite{Popescu_2019} (Figure \ref{fig:cvmfsGrid}.3.b).
Clients request updates based on their Time-to-Live (TTL) value, which is generally about a few minutes. Once the TTL value expires, clients download the latest version of a manifest - a text file located in the top-level directory of a given repository composed of the current root hash, metadata and the revision number of this repository - and make the updated content available.
Dykstra et al. \cite{Dykstra_2014} provide additional details about data integrity and authenticity mechanisms of CVMFS to ensure that data received matches data initially sent by a trusted server.
This pull-based approach has been proven to be robust and efficient, according to Popescu et al. \cite{Popescu_2019}, and has been widely used to distribute up-to-date software on grid sites for many years (Figure \ref{fig:cvmfsGrid}.2.a). Figure \ref{fig:cvmfsGrid} presents a simplified schema summarizing the software distribution process on grid sites via CVMFS.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{CVMFSOnGrid.pdf}
    \caption{Schema of the CVMFS workflow on Grid Sites: (a) the steps to get software dependencies from the job; (b) the steps to publish a release of a software in CVMFS.}
    \label{fig:cvmfsGrid}
\end{figure}

Users may need to use various versions of software on heterogeneous computing resources implying different OS and architectures.
To provide a convenient environment for the users, release managers generally provide software along with build files related to many architectures, OS and compilers.
Framework for building and installing scientific software on heterogeneous systems can be used to supply CVMFS with build files.
Easybuild \cite{easybuild}, Spack \cite{spack}, Nix \cite{nix} or Gentoo \cite{gentoo} are popular choices in this area \cite{Vokl_2021,Benda_2020,Burr_2019}. 

\subsection{Software delivery on supercomputers}\label{section:12}

Communities working around the Large Hadron Collider (LHC) \cite{LHC} have extensively used WLCG and CVMFS to process a growing amount of data.
This approach was reliable during LHC Run1 but has demonstrated its limit.
According to the analysis of Stagni et al. \cite{Stagni_McNab_Luzzi_Krzemien_Consortium_2017} on the use of CPU cycles in 2016, all the LHC experiments have consumed more CPU-hours than those officially pledged to them by the WLCG: they found ways to exploit opportunistic and not officially supported resources.
Moreover, in the High-Luminosity Large Hadron Collider (HL-LHC) \cite{osti_1365580} era, experiments are expected to produce up to an order of magnitude more data compared to the current phase (LHC Run2).
To keep up with the computing needs, experiments have started to use supercomputers. 
They offer a significant amount of computing power and would potentially offer a more cost-effective data processing infrastructure compared to dedicated resources in the form of commodity clusters, as Sciacca emphasizes \cite{Sciacca_Weber_2020}.
Nevertheless, supercomputers have more restrictive security policies than Grid Sites: they do not allow CVMFS to be mounted on the nodes by default and many of them have limited or even no external connectivity.
The LHC communities have developed different solutions and strategies to cope with the lack of CVMFS, which is a critical component to run their workflows.

Stagni et al. \cite{Stagni_Valassi_Romanovskiy_2020} rely on a close collaboration with some supercomputer centers - Cineca in Italy and CSCS in Switzerland -  to get CVMFS mounted on the worker nodes.
Nevertheless, their strategy is limited to a few supercomputers and their approach would be difficult to reproduce on a large number of supercomputers: most of them do not allow such collaboration.

To deal with the lack of CVMFS on supercomputers with outbound connectivity, Filipčič et al. studied two solutions: \emph{rsync} and \emph{Parrot}  \cite{Filipcic_Haug_Hostettler_Walker_Weber_2015}.
The first solution consisted in copying the CVMFS software repository in the shared file system using \emph{rsync}: a utility aiming to transfer and synchronize files and directories between two different systems.
\emph{rsync} added a significant load on the shared file system of the supercomputers and required changes in the repository absolute paths.
The second solution was based on Parrot: a utility copied on the shared file system of the supercomputer, usable without any user privileges. 
Parrot is a wrapper using \emph{ptrace} attached to a process that intercepts system calls that access the file system and can simulate the presence of arbitrary file system mounts, CVMFS in this case.
Nevertheless, the solution was "unreliable in a multi-threaded environment" \cite{Filipcic_Haug_Hostettler_Walker_Weber_2015} because it was unable to handle race conditions. 
These methods did not constitute a production-level solution but contributed to further and future advanced solutions.

In recent years, developments in the Fuse user space libraries and the Linux kernel have lifted restrictions for mounting Fuse file systems such as CVMFS.
Developers of CVMFS have integrated these changes and designed a package called \emph{cvmfsexec} \cite{cvmfsexec}, which allows mounting the file system as an unprivileged user. 
The program needs a specific environment to work correctly: (i) external connectivity; (ii) the \emph{fusermount} library or unprivileged namespace mount points or a setuid installation of \emph{Singularity} (efficient High-Performance Computing container technology).
Blomer et al. provide additional details about the package \cite{Blomer_2020}.

Communities exploiting supercomputers that do not provide outbound connectivity cannot directly benefit from \emph{cvmfsexec}: the package still needs to pull updated data via HTTP, which is not available in such context.
We can distinguish two cases: (i) supercomputers that grant outside network or specific service access to a limited number of nodes and (ii) supercomputers that do not provide nodes with any external connectivity at all.

Tovar et al. recently worked on the first case \cite{Tovar_2021}.
They managed to build a virtual private network (VPN) client and server to redirect network traffic from the workloads running on the worker nodes to external services such as CVMFS.
In this configuration, the VPN client runs on a worker node along with the job, while the VPN server is hosted on one of the specific nodes of the supercomputer and can interact with external services.
Communities working on supercomputers from the second case cannot leverage the solution developed by Tovar et al.

O'Brien et al., one of the first teams to work with supercomputers in the LHC context, address the lack of external network access by copying part of it to the shared Lustre file system accessible by the WNs \cite{OBrien_Walker_Washbrook_2014}.
The approach (i) worked because the environment of the supercomputer was similar to a grid site one, (ii) required changes in the CVMFS files and (iii) degraded the performance of the software as Angius et al. described \cite{Angius_Oleynik_Panitkin_Turilli_De_Klimentov_Oral_Wells_Jha_2017}. 
To tackle the latter issue on the Titan supercomputer, Angius et al. moved the software to a read-only NFS server \cite{Angius_Oleynik_Panitkin_Turilli_De_Klimentov_Oral_Wells_Jha_2017}: this eliminated the problem of metadata contention and improved metadata read performance.

Similarly, on the Chinese HPC CNGrid, Filipčič regularly packed a part of CVMFS in a tarball.
Filipčič provided a deployment script to install the software and fix the path relocation on the shared file system to the local system administrators: they were then responsible for getting and updating the CVMFS tarball on the network when requested \cite{Filipcic_2017}.

To help communities to unpack a CVMFS repository in a file system, a team of developers designed \emph{uncvmfs} \cite{uncvmfs}. The utility deduplicates files of a software stack: it populates a given directory with the CVMFS files that are then hard-linked into it, if possible.
The program was used, in combination with Shifter \cite{Gerhardt_Bhimji_Canon_Fasel_Jacobsen_Mustafa_Porter_Tsulaia_2017}, a container technology providing a reproducible environment, in the context of the integration of the ALICE and CMS experiments workflows on the NERSC High-Performance Computing resources \cite{Fasel_2016,Hufnagel_2017}.
As a proof of concept, Gerhardt et al. used \emph{uncvmfs} to deduplicate the ATLAS repository and copy it into an ext4 image  - about 3.5Tb of data containing 50 million files and directories -, compressed into a 300Gb squashfs image; and Shifter to provide a software-compatible environment to run the jobs \cite{Gerhardt_Bhimji_Canon_Fasel_Jacobsen_Mustafa_Porter_Tsulaia_2017}.
Despite encapsulating the files in a container reduced the startup time of the applications, the solution generated large images, long to update and deliver on time.

To cope with large images, Teuber and the CVMFS developers conceived \emph{cvmfs\_shrinkwrap} \cite{Teuber_2019}.
The tool supports \emph{uncvmfs} features with certain optimizations and delivers additional features: \emph{cvmfs\_shrinkwrap} can extract specific files and directories based on specification files, deduplicate them, making them easy to export in various formats such as squashfs or tarball.
In this way, the following operations remain on behalf of the user communities: (i) trace their applications - meaning, in this context,  "capturing all their dependencies and their locations in the file system" -, (ii) call \emph{cvmfs\_shrinkwrap} to get a subset of CVMFS composed of the minimum required files, and (iii) export this subset in a certain format and deploy it on sequestered computing resources to run their jobs.

Douglas et al. already described such a project in an article \cite{Douglas_2019}, but the work remains specific to the ATLAS experiment.
They use \emph{uncvmfs} to produce a large image that has to be filtered afterward.
In this paper, we aim at assisting various user communities in this process by providing an open-source utility that would take applications of interest in input and would output - with the help of \emph{cvmfs\_shrinkwrap} - a subset of CVMFS with the minimum required files to run the given applications, in combination with a container image if needed.
To our knowledge, no paper has already covered the subject.