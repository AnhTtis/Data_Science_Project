\subsection{Gauss}\label{section:31}

To better understand experimental conditions and performances, the LHCb collaboration has developed Gauss, a Monte-Carlo simulation application - based on the Gaudi framework \cite{Barrand_2001} - that reproduces events occurring in the LHCb detector.
The application consists of two independent phases executed sequentially, namely the generation of the events \cite{Belyaev_2011} relying on Pythia \cite{Sj_strand_2001} by default; the tracking of the particles through the simulated detector depending on Geant4 \cite{geant_2003}.

In 2021, Gauss represents about 70\% of the distributed computing activities of the LHCb collaboration and 150 million events are simulated per day.
The application has originally been tailored for WLCG grid sites: Gauss is a compute-intensive single-process (SP), single-threaded (ST) application, only supporting x86 architectures and CERN-CentOS-compatible environments \cite{LinuxWebCERN}.
Gauss and most of its dependencies are delivered via CVMFS.

Gauss takes a certain number of events to process as inputs, as well as a "run number" and an "event number".
The combination of both numbers forms a seed, which ensures repeatability during the generation and simulation phases.
It mainly relies on packages such as Python, Boost and gcc to produce histograms and \emph{ntuples} under the form of a ROOT \cite{root} file.

Gauss is modular and highly configurable and constitutes a complex use-case: it can integrate extra packages such as various event generators and decay tools. 
Depending on LHCb production needs and the computing environments available, different versions of Gauss and its attached packages can be used.
A plethora of option files can also be passed as inputs to the extra packages.
Figure \ref{fig:gauss} describes the inputs, outputs and dependencies of Gauss as well as its interactions with some extra packages and their options.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{GaussDependencies.pdf}
    \caption{Example of a Gauss instance, its dependencies and some interactions with extra packages and their options.}
    \label{fig:gauss}
\end{figure}

\subsection{Mare Nostrum}\label{section:32}

To start integrating their workflows on High-Performance computing resources, LHC experiments can benefit from a collaboration with PRACE \cite{prace} and GÃ‰ANT \cite{geant,collabhpc}.
This collaboration gives them access to several European supercomputers such as Marconi in Italy and Mare Nostrum in Spain.

Managed by the Barcelona Supercomputing Center (BSC), MareNostrum is the most powerful and emblematic supercomputer in Spain \cite{marenostrum}.
MareNostrum was built in 2004 (MareNostrum 1), has been updated 3 times since then (Mare Nostrum 2,3 and 4) and was ranked 63rd in the June 2021 Top500 list \cite{top500}.
Each node composing the general-purpose block is equipped with two Intel Xeon Platinum 8160 24 cores at 2.1 GHz chips, and at least 2GB of RAM: this configuration matches with Gauss requirements.
Nevertheless, Mare Nostrum is more restrictive than a traditional Grid Site on WLCG: (i) no external connectivity at all; (ii) no service can be installed on the edge node; (iii) no CVMFS, and thus, no Gauss and its dependencies available.

% Lustre share file system?

\subsection{Running Gauss on Mare Nostrum}\label{section:33}

Running embarrassingly parallel applications such as Gauss on a supercomputer can be seen as counterproductive.
While it is true that the interconnect of the supercomputer partitions has not been designed for millions of small Monte-Carlo runs, it is better to use available, otherwise unused, cycles in agreement with the management of the supercomputer sites.
In the meantime, developers are adapting software \cite{Siddi_Muller_2019,Mazurek_2021}, but it remains a long process, requiring deep and technical software inputs.

To deliver Gauss on Mare Nostrum, LHCb can rely on (i) \emph{subcvmfs-builder} to produce a subset of CVMFS containing the required files; (ii) a CernVM Singularity container to provide a Gauss-compatible environment and to mount the subset of CVMFS as if it was a CVMFS client.

Nevertheless, as we explained in \ref{section:31}, a Gauss execution can involve different packages, extra packages, options, data and versions. Encapsulating its ecosystem requires a good understanding of the application and/or a large amount of storage to encapsulate the right dependencies.
Therefore, different options are available:
\begin{itemize}
    \item Include the whole LHCb CVMFS repository: would not require any specific knowledge about Gauss and would involve all the necessary files to run any Gauss instance. However, this option would imply a tremendous quantity of storage - the full LHCb repository needs 5.2 Terabytes -, long periods to update the subset and many unnecessary files.
    \item Include the dependencies of various Gauss runs: as the first option, would not need any specific knowledge about Gauss and would include a few gigabytes of data. Nevertheless, such an option would not guarantee the presence of all needed files and would require a tremendous amount of computing resources to trace Gauss workloads continuously.
    \item Include all the known dependencies of Gauss: would require a deep understanding of Gauss and its dependencies to include all the required files in a subset of CVMFS. While this option would not involve many computing or storage resources, it would include human resources to update the content of the subset of CVMFS according to the releases of Gauss and its extra packages. 
\end{itemize}

As the default storage quota on Mare Nostrum is smaller than the LHCb repository, we decided to reject the first option.
LHCb has access to tremendous computing power: it interacts with hundreds of WLCG Sites to run Gauss workloads and could theoretically trace them and extract their requirements.
In practice, tracing Gauss workloads in production could slow down the applications and their execution, which is not an option.
Similarly, LHCb does not have human resources to update the subset of CVMFS according to the changes done.
Thus, we chose to combine the second and the third options to propose a light and easy to update and maintain solution.
The process consists in getting insights into the structure of the Gauss dependencies by running and tracing a small set of Gauss workloads and analyzing the system calls before including the structure in \emph{subcvmfs-builder-pipeline}.

After analyzing 500 commands calling Gauss from the LHCb production environment and tracing 3 Gauss applications using \emph{subcvmfs-builder} \cite{gauss_analysis}, we noticed that:
\begin{itemize}
    \item 97\% of the workloads studied were running the same Gauss versions (v49r20) with the same extra packages and versions.
    The versions of Gauss and its extra packages seem related to the underlying architecture.
    \item 846 Mb of files were needed to run 3 Gauss (v49r20) workloads.
    About 95\% of the size is related to the Gauss version and the underlying architecture, and is common to the Gauss workloads traced, while the 5\% left is bound to the options and Geant4 data used that are specific to a given Gauss workload.
    \item Integrating all the options and Geant4 data related to Gauss v49r20 would correspond to 1.8 Gb of files.
\end{itemize}

Based on these assumptions, we created a \texttt{namelist} file containing (i) the files shared by the 3 Gauss applications that we traced and (ii) all the options and Geant4 data in order to generate a subset of CVMFS able to run any Gauss workload targeting the v49r20 version.
We used \emph{subcvmfs-builder-pipeline} to build the subset of CVMFS, to successfully test it with 5 Gauss workloads - different from the ones we used previously - and to deploy it to Mare Nostrum.
We fine-tuned the utility to disable the \emph{trace} step and to deploy the subset separately from the container.
Indeed, CernVM - the container that we use to provide a reproducible environment to the workload - does not need regular updates and merging it with the subset of CVMFS is a time-consuming operation.

This resulted in a CernVM singularity container occupying 6.4 Gb on the General Parallel File System (GPFS) of Mare Nostrum combined with a subset of CVMFS covering 6 Gb: dependencies occupies 3.2 Gb of space while 2.8 Gb are required for the \emph{cvmfs\_shrinkwrap} metadata.
Thus, 12.4 Gb of space on the GPFS of Mare Nostrum is currently sufficient to run 97\% of the Gauss workloads analyzed: 0.24\% of the LHCb repository.

% does it produce the same results? reproducibility
Even though this approach provides a light, easy and fast-to-update solution, LHCb developers need to keep it up to date to integrate new versions or structure changes.
One way to proceed would consist in automating and repeating the analysis work regularly. 
One could also integrate the \emph{trace} command of \emph{subcvmfs-builder} within the LHCb production test phase, which consists in running a few events of upcoming Gauss workloads on a given Grid Site.
LHCb developers could trace some of them during the process and store the traces in a database.
An LHCb-specific \emph{subcvmfs-pipeline-builder} could then periodically fetch the content of the database to build, test and deploy a new subset of dependencies to Mare Nostrum.

% container: passing from cern-vm to centos because a lot of unnecessary files
