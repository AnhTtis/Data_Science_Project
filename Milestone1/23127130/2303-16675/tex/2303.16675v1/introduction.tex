To study the constituents of matter and better understand the fundamental structure of the universe, HEP collaborations rely on complex software stacks and a worldwide distributed system to process a growing amount of data: the World Wide LHC Computing Grid (WLCG) \cite{WLCG}.
The infrastructure involves 170 computing centers, 1 million cores and 1 exabyte of storage spread around 42 countries.

Delivering a reproducible environment along with up-to-date software across thousands of heterogeneous computing resources is a major challenge: Buncic et al. designed CernVM and CVMFS (CernVM-File System) \cite{Buncic_2010} to tackle it by decoupling the software from the Operating System.

CernVM \cite{CVMFS} is a thin Virtual Software Appliance of about 150 Mb in its simplest form.
It supports a variety of hypervisors and container technologies and aims to provide a complete and portable user environment for developing and running HEP applications on any end-user computer and Grid Sites, independently of the underlying Operating Systems used by the targeted platforms.

CVMFS \cite{CVMFS} is a scalable and low-maintenance file system optimized for software distribution.
CVMFS is implemented as a POSIX read-only file system in user space.
Files and directories are hosted on standard web servers and mounted on the computing resources as a directory.
The file system performs aggressive file-level caching: both files and file metadata are cached on local disks as well as on shared proxy servers, allowing the file system to scale to a large number of clients \cite{Buncic_2010}.

This approach has been mainly adopted by the HEP community and is now getting users from various communities according to Arsuaga-RÃ­os et al. \cite{Arsuaga_R_os_2015}. In a few years, it has become the standard software distribution service on Grid Sites of WLCG.
Nevertheless, computing infrastructure and funding models are changing, and national science programs are consolidating computing resources and encourage using cloud systems as well as supercomputers, as Barreiro et al. explain \cite{Barreiro_2019}.
CVMFS developers have extended the features of the file system and have provided additional tools to support clouds \cite{Buncic_2011}\cite{Harutyunyan_2012} and supercomputers \cite{Blomer_2017}.

Supercomputers are highly heterogeneous architectures that pose higher integration challenges than traditional Grid Sites.
Many supercomputers do not allow a CVMFS client to be mounted on the worker nodes and/or do not provide external connectivity, which is critical to work with CVMFS.
CVMFS tools designed to interact with High-Performance Computing sites are aimed at administrators of scientific communities that would like to integrate their workflows on such machines: they ease some steps of the process but may require additional efforts on behalf of the administrators. 

In this study, we aim to automate the whole process and reduce these additional efforts by providing a utility able to extract, test and deploy parts of CVMFS on supercomputers not having outbound connectivity.
Section \ref{section:1} briefly introduces CVMFS and the ecosystem developed around it, in order to deal with supercomputers.
Section \ref{section:2} focuses on the design of the utility, the steps to extract software dependencies and to deploy them on a given supercomputer.
Finally, section \ref{section:3} presents a use case and the obtained results in detail.