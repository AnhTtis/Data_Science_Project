\subsection{Input and output data}\label{section:21}

The utility takes a directory as input that should contain: (i) a list of applications of interest (\texttt{apps}): a command along with its input data in a separate sub-directory for each application to trace; and/or (ii) a list of files composed of paths to include in the subset of CVMFS (\texttt{namelists}).
Additionally, user communities can embed a (iii) container image compatible with Singularity to get a specific environment to trace and test the applications; (iv) and a configuration file to fine-tune the utility with variables related to the deployment process, or information about repositories.
A schema of the inputs is available in Figure \ref{fig:inputTree}.

\begin{figure}[ht]
    \centering
    \input{inputTree}
    \caption{Schema of the input structure given to the utility.}
    \label{fig:inputTree}
\end{figure}

The expected output can take different forms depending on the utility configuration:
\begin{itemize}
    \item The subset of CVMFS, generated as a standalone.
    In this case, administrators representing their user communities need to provide the right environment by themselves, which might also involve discussions with the system administrators.
    \item The subset of CVMFS embedded within the given Singularity container image.
    The utility merges both elements and submits the resulting image, which can be long to generate and deploy but may limit manual operations on the remote location. 
\end{itemize}

\subsection{Features}\label{section:22}

We break down the process into four main steps, namely:
\begin{itemize}
    \item \emph{Trace}: consists in running applications contained in \texttt{apps} and trapping their system calls at runtime, using \emph{Parrot}, to identify and extract the paths of their dependencies.
    Applications can run in a Singularity container when provided, which delivers further software dependencies and a reproducible environment. 
    Dependencies are then saved in a specific file \texttt{namelist.txt}. 
    In this context, \emph{Parrot} is only used to capture system calls and, thus, is not impacted by the issues mentioned in section \ref{section:12}.
    If the step detects an error during the execution of an application, then the program is stopped.
    The step is particularly helpful for users of the utility having no technical knowledge of the applications of interest.
    \item \emph{Build}: builds a subset of CVMFS based on the paths coming from \emph{Trace} and the \texttt{namelists} directory. First, the step merges the namelist files to remove duplicated or non-existent path references, and then separates the paths in different specification files related to repositories. Finally, the step calls \emph{cvmfs\_shrinkwrap} to generate the subset of CVMFS.
    Figures \ref{fig:traceProcess} and \ref{fig:pipeline}.3 illustrate an example.
    The utility deduplicates the files, and hard-link data to populate a directory, ready to be exported in various formats as explained in Section \ref{section:12} and shown in Figure \ref{fig:pipeline}.3.

    \begin{figure}[ht]
    \centering
    \begin{minted}{yaml}
    in namelist1.txt: 
    /cvmfs/repoA/path/to/file
    /cvmfs/repoB/path/to/another/file
    in namelist2.txt:
    /cvmfs/repoA/path/to/file
    /cvmfs/repoB/path/to/yet/another/file
    
    in repoA.spec:
    /path/to/file
    in repoB.spec:
    /path/to/another/file
    /path/to/yet/another/file
    \end{minted}
    \caption{Transformation process occurring during the \emph{Trace} step: CVMFS dependencies are extracted  from \texttt{namelist.txt} and moved to specification files.}
    \label{fig:traceProcess}
    \end{figure}

    \item \emph{Test}: consists in testing certain applications - in the given Singularity container environment when provided - using the subset of CVMFS obtained during the \emph{Build} step (see Figure \ref{fig:pipeline}.4). 
    By default, applications from \texttt{apps} are used but further tests can also be provided by modifying the utility configuration.
    All the applications have to complete their execution to go to the next step.
    \item \emph{Deploy}: deploys the subset of CVMFS (Figure \ref{fig:pipeline}.5) embedded or not within the container image depending on the configuration options. If such is the case, then the utility (i) generates a new container definition file that includes the files with the container image, (ii) executes it to produce a new read-only container image. The utility supports ssh deployment via \emph{rsync}, provided the right credentials in the configuration. 
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.74\textwidth]{PipelineSteps.pdf}
    \caption{Schema of the utility workflow: from getting an application to trace to a subset of CVMFS on the Data Transfer Node of a High-Performance Computing cluster.}
    \label{fig:pipeline}
\end{figure}

\subsection{Implementation}\label{section:23}

The utility is built as a 2-layer system.
The first layer, \emph{subcvmfs-builder}\cite{subcvmfs-builder}, is the core of the system and is self-contained.
It takes the form of a Python package, which embeds the steps described in section \ref{section:22}, and provides a command-line interface to call and execute steps independently from each other.
The first layer is, and should remain, simple and generic to be easily managed by developers and used by various communities. 

The second layer is the glue code: it consists of a workflow executing - all, or some of - the steps of the first layer.
It contains the complexity required to generate and deliver a subset of dependencies according to the needs of its users.
Unlike the first layer, the second one can take several forms and each community can tailor it for its software stack.

We propose a first, simple and generic layer-2 implementation calling each step one after the other: \emph{subcvmfs-builder-pipeline}\cite{subcvmfs-builder-pipeline}.
This layer-2 implementation is executed from a GitLab CI/CD \cite{gitlabci}, which provides a runner and a docker executor bound to a CVMFS client to execute the code (see Figure \ref{fig:subcvmfsbuilderpipeline}) 
GitLab includes features such as log preservation to help debug the implementation and integrates a pipeline scheduling mechanism to regularly update a subset of dependencies. 
Even though this layer-2 solution is adapted for basic examples - implying a few commands to trace and test, having a small number of dependencies -, it might require further fine-tuning for more advanced use cases.

Indeed, this generic layer-2 implementation is not scalable as it (i) is a single-threaded and single-process program, and (ii) requires manual operations to insert additional inputs in the process.
This is not adapted to communities having to trace and test hundreds of various applications to generate large subsets of CVMFS.
Two possibilities for such communities: building a new layer-2 implementation - able to automatically fetch applications and trace/test them in parallel - based on \emph{subcvmfs-builder-pipeline} or creating one from scratch.

In the next section, we are going to study how the LHCb experiment \cite{LHCb_2008} leverages \emph{subcvmfs-builder} and \emph{subcvmfs-builder-pipeline} to deliver Gauss \cite{Clemencic_2011}, a Monte-Carlo simulation program, on the worker nodes of Mare Nostrum \cite{David_2010}, a supercomputer with no external connectivity based in Barcelona, Spain.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{SubCVMFSBuilder.pdf}
    \caption{Schema of a layer-2 implementation within GitLab CI.}
    \label{fig:subcvmfsbuilderpipeline}
\end{figure}

% talk about cvmfs_shrinkwrap that can be kept and easily updated afterwards? Is this really true? 


