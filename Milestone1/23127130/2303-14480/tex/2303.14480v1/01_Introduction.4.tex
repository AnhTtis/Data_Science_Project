Taxonomy is formulated as directed acyclic graphs or trees, which consist of \emph{hyponym-hypernym} relations between concepts.
One concept is a hypernym of another concept if the meaning of the former covers the latter \cite{sang2007extracting}.
A well-constructed taxonomy assist various downstream tasks, including web content tagging \cite{liu2020giant,liu2019tencent_taxonomy,peng2019RCNN4taxonomy}, personalized recommendation \cite{karamanolakis2020txtract, huang2019taxonomy}, query understanding  \cite{yang2020co} and so on. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{intro.pdf}
    \caption{An example of the GANTEE framework. This framework is implemented as a plugin before taxonomy expansion. The demand of generating high-quality data and taxonomy enterance evaluation are formulated as a generative adversarial network in GANTEE.}
    \label{fig:introduction}
    \vspace{-5mm}
\end{figure}

Manually constructing and maintaining a taxonomy is labor-intensive and time-consuming.
For example, there are millions of new concepts expected to be added in Taobao taxonomy, which is the largest Chinese shopping platform, in a single time \cite{jiang2019towards}.
So the task of automated taxonomy expansion is proposed \cite{jurgens2016semeval}, which aims to automatically assign an existing concept (anchor concept) as a hypernym concept to the newly input concept (query concept).
\red{
The mainstream taxonomy expansion methods assume that most query concepts are clean.
These methods model taxonomy expansion task as binary classification, sampling two concepts connected by the existing edges in the taxonomy as positive training data, and random two concepts as negative training data.
To improve the performance of the models, they spent a lot of time on learning representations for each concepts.
And they use sampled training data based-on learned representation to assign anchor concepts to all query concepts \cite{shen2020taxoexpan, yu2020steam, wang2021enquire, zhang2021taxonomy, mao2020octet, ma2021hyperexpan, wang2022qen}.
}
These models achieve great success in saving a lot of time from labor annotation in taxonomy construction and maintenance.

However, we argue that there are two main gaps between the previous studies and real-scenarios. 
\textbf{First}, the assumption made by the previous studies that most query concepts are clean is not true in the real-scenarios.
For instance, there have billions of new query generated in an ordering take-outs online application \cite{cheng2022learning} in five months time, and only thousands of them are clean and being added to the existing taxonomy.
Since the previous methods spent a lot of time on representation learning, large amount of noisy query concepts will greatly slow the \red{predicting speed}.
\textbf{Second}, there is a large gap on the semantic between the hypernym-hyponym relationships in the existing taxonomy and in real-scenrios.
Many companies invite experts to build a taxonomy consisting of thousands of concepts and expect taxonomy expansion methods to expand this taxonomy into the level of millions or even billions of concepts.
Only use concepts in the existing taxonomy as training data limits the ability of the model in finding new hypernym-hyponym relationship in real-scenrios.
So the effectiveness of the previous taxonomy expansion methods will be greatly reduced in the real-scenarios.



In this paper, we propose a plugin task called taxonomy enterance evaluation to alleviate the first problem.
This task aims at excluding noisy query concepts before taxonomy expansion in an efficient way.
At the same time, more high-quality data are required during the model training stage in taxonomy expansion task to alleviate the second problem.
Combine these two demands, we propose a corresponding plugin framework called \textbf{G}enerative \textbf{A}dversarial \textbf{N}etwork for \textbf{T}axonomy \textbf{E}nterance \textbf{E}valuation (GANTEE).

The example of GANTEE framework is shown in Figure \ref{fig:introduction}.
We model the first and second problems as an generative adversarial task.
The discriminative model is used to identify the noisy query concepts (generated query concepts) from the real query concepts.
And the generative model is used to generate fake query concepts which has similar semantic to the real query concepts to confuse the discriminative model.
The generated data is also used to train the taxonomy expansion model, since the generated positive data are fidelity and unseen to the existing taxonomy, and the generated negative data are more deceptive than random sampling methods, which can further improve the judging ability of the taxonomy expansion models.

% To ensure the efficiency, we use pretrained language models (PLMs) to get the representation for every concept based on the textual feature of the text.
% This is a trade off between the efficiency with the effectiveness, the PLMs has limited ability in representing conceptual text, but with vast of training data generated by the generative model, the shortage of the representation is compensated.

To ensure the efficiency, we use pre-trained language models (PLM) to obtain a representation of each concept based on the textual features of the concept.
This approach is actually a trade-off between efficiency and effectiveness.
PLMs can obtain a representation from the textual features of concepts in a very short time, the reason why previous works do not use PLMs is because PLMs have limited ability in representing the fine-grained semantics based on conceptual text.
We compensate for the lack of PLMs representation capability by reducing the denoising problem into an coarse-grained evaluation problem and generating large amount of high-quality training data by the generative model.
% We reduce the denoising problem to an coarse-grained evaluation problem, and compensate for the lack of PLM representation capability by generating the large amount of high-quality training data generated in the model.

% PLMs can obtain a representation from the textual features of concepts in a very short time, but the reason why previous works do not use PLMs is because PLMs have limited capability to represent the text of concepts.
% We compensate for the lack of PLMs representation capability by the large amount of high-quality training data generated in the generative model.

In the experiments, we benchmark the taxonomy entrance evaluation task on three real-world taxonomies in two different languages, English and Chinese.
We make new State-Of-Art score in the experiments by improve the hit@1 from \red{xxx} to \red{xxx}, MR from \red{xxx} to \red{xxx} and MRR from \red{xxx} to \red{xxx}.
Then we test the efficiency of our method, and GANTEE still outperforms the baselines by a large margin on all three datasets.
Finally, extensive experiments have been conducted to study how the parameters affect the result of GANTEE, and provide an effective tuning scheme for the following researchers who will use our method in the future.

\subsubsection{Contribution.}
To summarize, our major contributions include: 
(1): a more realistic task called taxonomy entrance evaluation which judge whether a query concept should be added to an existing taxonomy in an efficient way;
(2): a novel, effective method called GANTEE to generate confusing negative and fidelity positive data which boosts the performance and the efficiency for both traditional taxonomy expansion tasks and taxonomy enterance evaluation tasks.
(3): extensive experiments that verify the effectiveness of GANTEE on three datasets in two languages, and an effective tuning method was proposed by the experiment.