
GANTEE consists of a generator $G$ and two discriminators, which are hyper discriminator $D_H$ and rollout discriminator $D_R$. 
$G$ takes an anchor concept and a label that generates positive or negative samples as inputs and generates the corresponding query concept. 
$D_H$ determines whether the given anchor concept is suitable for the generated query concept based on the given label, and $D_R$ determines whether the generated text is a concept.

Specifically, when a new query concept is given in the predicting stage, GANTEE first determines if it is a concept by $D_R$. 
Then GANTEE determines whether the given concept has a hypernym-hyponym relation to the root concept by $D_H$.
Finally, GANTEE output the confidence score of if the given query concept is suitable for this taxonomy.



\begin{figure}
    \centering
    
    \resizebox{1\linewidth}{!}{\includegraphics{Framework.png}}
    \caption{The framework of GANTEE.
    The generator of the GANTEE is on the left, and duel discriminator is on the right.}
    
    \label{fig:framework.png}
    \vspace{-6mm}
\end{figure}
\subsection{The Generative Model for Query Concept Generating}
We use Transformer~\cite{vaswani2017attention} as the generative model.
Transformer maps the input sequence $x_1,\dots,x_n$ into a sequence of hidden states $h_1,\dots,h_n$.
Specifically, a Transformer block consists of stacked self-attention layers with residual connections.
Each self-attention layer receives $n$ embeddings $\{h^{(l)}_i\}^n_{i=1}$ corresponding to unique input tokens, and outputs $n$ hidden states $\{h^{(l+1)}_i\}^n_{i=1}$.
The $i$-th token is mapped via linear transformations to a key $k_i$, query $q_i$, and value $v_i$.
The $i$-th output of the self-attention layer is given by weighting the values $v_j$ by the normalized dot product between the query $q_i$ and other keys $k_j$:
\begin{equation}
\small
    h_i=\sum^n_{j=1}softmax(\langle q_i,k_{j'}\rangle^n_{j'=1})_j\cdot v_j.
\end{equation}

This allows the layer to assign ``credit'' by implicitly forming state-return associations via similarity of the query and key vectors (maximizing the dot product). 
It is worth noticing that most of the Transformer variants, such as BART~\cite{lewis2019bart} or XLNet~\cite{yang2019xlnet}, and the RNN variants, such as the Gated Recurrent Unit(GRU)~\cite{cho2014gru} or Long-Short Term Memory network(LSTM)~\cite{hochreiter1997lstm}, can be used as a generator in GANTEE.

We use GPT-2~\cite{radford2019gpt2} in this paper to give the best experimental results.
GPT-2 modifies the transformer architecture with a causal self-attention mask to enable auto-regressive generation. 
It is one of the most famous auto-regressive models, which best suits the fake query generation task.

As an auto-regressive generative model, GPT-2 could generate sequences based on all input sequences.
So we formulate the input and the output of the GPT-2 in the following format:
\begin{equation}
    GPT2(l_i\oplus a_i) = l_i\oplus a_i\oplus q_i.
\end{equation}
$GPT2$ denotes the GPT-2 model, $l_i$ denotes the given label, $a_i$ denotes the text of the given anchor concept, $q_i$ denotes the text of the given query concept, and $\oplus$ denotes the concatenation of the tokens.




\subsection{The Rollout Discriminative Model for Short Term Reward in Generating}
The rollout discriminative model is used to judge whether the generated text is a concept before the generating process finish.
Deep discriminative models such as deep neural network(DNN), convolutional neural network(CNN), and recurrent convolutional neural network(RCNN) have shown their effectiveness in complicated sequence classification tasks~\cite{guo2018leakgan, yu2017seqgan, nie2018relgan}.
In this paper, we choose LSTM~\cite{hochreiter1997lstm} as our rollout discriminator.
Since previous study~\cite{guo2018leakgan} find that severe gradient vanishing occurs when the discriminator is too strict, LSTM is a lightweight model which is easy to do the training and will not provide too strict discriminative ability.
LSTM will take in the input embedding $x_1,\dots,x_t$ and use purpose-built memory cells to calculate the hidden state for each step $h_1, \cdots, h_t$.
\begin{equation}
\small
    \begin{aligned}
    \{h_1,\cdots,h_t\} = lstm(x_1,\cdots,x_t),
    \end{aligned}
\end{equation}
where $lstm$ denotes the model of LSTM.
We take the final hidden state $h_t$ and inject it into a linear layer to calculate the reward of the generated text:
\begin{equation}
\small
    r_r = W_r \cdot h_t+b_r,
\end{equation}
where $W_r$ and $b_r$ are the parameters in a linear layer, the $r_r$ is the confidence of whether the generated text is a concept.


\subsection{The Hyper Discriminative Model for Long Term Reward in Generating}
The hyper discriminative model is used to judge whether the generated concept has a hypernym relation to the given anchor concept after the generating process finish based on the given label.
Such discriminative models offer a long-term reward for the generating process to guide the direction of the generation better.
The existing models for taxonomy expansion can be used as the hyper discriminative model, which can detect whether there is a hypernym relation between two concepts.

However, these models rely heavily on the vector representation of the concepts, many studies such as STEAM~\cite{yu2020steam} or HiExpan~\cite{shen2018hiexpan} use external information to get better representations for the concepts, and that would be time-consuming for the generating process since each time the generative model produce a new concept, it will take time to get its detailed external information and get its vector representation.

Instead, we use Bert~\cite{devlin2018bert} to retrieve the representation of concepts since it shows its ability in Dependency Judgement task~\cite{shi2019next}, which is also good at understanding the hypernym (hyponym) relation between two concepts.
We concatenate the label representation and the position embedding proposed by the TaxoExpan~\cite{shen2020taxoexpan} to the Bert representation and put them into a linear layer to get the final predicted result.

Specifically, we get the representation of the concept by Bert in the following format:
\begin{equation}
\small
% \resizebox{0.7\columnwidth}{!}{
\begin{aligned}
    &x_{a}=Bert(x^{c_0}_1,\dots,x^{c_0}_t)[1],\\
    &x_{q}=Bert(x_1,\dots,x_t)[1],\\
    &l = EMB(positive)\text{ or }EMB(negative).
\end{aligned}
% }
\end{equation}
$Bert$ denotes the BERT model, $EMB$ denotes the embedding model, and $positive$ or $negative$ denotes the input to the embedding model.

Then, we concatenate the label representation and position embedding to the representation of the anchor concept and the query concept and put them into a linear layer:
\begin{equation}
\small
    r_h=W_h\cdot(l+x_{a}+p_{a}+x_{q}+p_{q}) + b_h
\end{equation}
Where $W_h$ and $b_h$ are the parameters in a linear layer, $p_{a}$ and $p_q$ are the position embedding. 
The $r_h$ is the probability of the hypernym relation between the generated query concept and the anchor concept. 


\subsection{Training for GANTEE}
GANTEE uses a Generative Adversarial Network (GAN) model to combine taxonomy entering evaluation and high-quality training samples generating.
It needs a generative model to produce conceptual text and use discriminative models to offer supervised signals to the generative model.
However, naive GAN is used for continuous data, not discrete tokens.
The previous researchers~\cite{bachman2015data, bahdanau2016actor, yu2017seqgan} propose considering the text generation procedure as a sequential decision-making process.
The objective function of such generative models is formulated to generate a sequence from the start state $s_0$ to maximize its expected end reward via policy gradient algorithms~\cite{sutton1999policy}:
\begin{equation}
    \begin{aligned}
        J(\theta)&=\mathbb{E}[R_T|s_0=l\oplus a;\theta]\\&=\sum_{y_1\in Y}G(y_1|s_0=l\oplus a)\cdot Q^{G}_{D}(s_0=l\oplus a, y_1),
    \end{aligned}
\end{equation}
where $J(\theta)$ is the object function, $R_T$ is the reward for a complete concept that has hypernym relation to the given anchor concept $a$ and label $l$.
Note that the reward is from both the hyper discriminator $D_{H}$ and rollout discriminator $D_{R}$.
$G$ is the generative model, and $Q^{G}_{D}(s, a)$ is the action-value function of a sequence, which is calculated in the following format:
\begin{equation}
    Q^{G}_{D}(a=y_t, s=Y_{1:t-1}, s_0) = D(s_0, Y_{1:t-1})
\end{equation}
Since the reward getting from an unfinished sequence often have a large bias over the finished one, the Monte Carlo search is used to sample the unknown last $T-t$ tokens~\cite{yu2017seqgan}:
\begin{equation}
        \{Y^1_{1:T},\dots,Y^N_{1:T}\}=MC^{G}\left(Y_{1:t};N\right)
\end{equation}

We use rollout discriminator $D_{R}$ to calculate the expected reward based on Monte Carlo search during the generative process for short-term reward and use hyper discriminator $D_{H}$ to calculate the reward when the generating is done for long-term reward. 
The action-value function is transformed into the following format:
\begin{equation}
    \small
    \begin{aligned}
    &Q^{G}_{D}(a=y_t, s=Y_{1:t-1}, s_0) = \\
    &\begin{cases}
    \frac{1}{N}\sum^N_{n=1}D_{R}(s_0, Y_{1:t-1}), Y^n_{1:T}\in MC^{G}\left(Y_{1:t};N\right)& \text{for }t<T\\
    D_{H}(Y_{1:t}, s_0)& \text{for }t=T
    \end{cases}
    \end{aligned}
\end{equation}

Once we have a set of more realistic generated sequences, we will re-train the discriminator model as follows:
\begin{equation}
    \label{eqt:object fucntion}
    \min_{D}-\mathbb{E}_{Y\sim p_{data}}[\log D(Y)]-\mathbb{E}_{Y\sim G}[log(1-D(Y))]
\end{equation}

% \subsection{Implementation Techniques}
Specifically, we pre-train the generator and the discriminator before the adversarial process begins.
After the pre-training stage, the generator and discriminator have trained alternatively.
As the generator is updated via some training epochs, the discriminators need to be re-trained periodically to keep a good pace with the generator.

When training the discriminator, positive samples are sampled from the existing taxonomy $\mathcal{T}^{0}$ based on the hypernym relation $\mathcal{R}$. 
In contrast, negative samples are the fake samples generated from our generator.
To ensure the learning process is doing well, we use both generated samples and pure noisy samples as the negative query concepts to inject into the training process. 
The number of negative query concepts we use is the same as the size of positive query concepts.