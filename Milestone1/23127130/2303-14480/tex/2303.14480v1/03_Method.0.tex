
GANTEE consists of a generator $G$ and two descriminators, which are hyper discriminator $D_H$ and rollout descriminator $D_R$. 
$G$ takes an anchor concept as input and generates the corresponding query concept. 
$D_H$ determines whether the given anchor concept is suitable for the generated query concept, and $D_R$ determines whether the generated text is a concept.

Specifically, in the predicting stage, when a new query concept is given, GANTEE first determine if it is a concept by $D_R$. 
Then GANTEE determine whether the given concept has a hypernym-hyponym relation to the root concept by $D_H$.
Finally, GANTEE output a confidence score of whehther the given query concept is suitable for this taxonomy.
% $D_H$ takes the root concept as the anchor concept, and therefore $D_H$ determines whether the given query concept is suitable for this taxonomy.


\subsection{The Generative Model for Query Concept Generating}
We use Transformer \cite{vaswani2017attention} as the generative model.
Transformer maps the input sequence $x_1,\dots,x_n$ into a sequence of hidden states $\mathbf{h_1},\dots,\mathbf{h_n}$.
Specificly, a Transformer block consist of stacked self-attention layers with residual connections.
Each self-attention layer receives $n$ embeddings $\{x_i\}^n_{i=1}$ corresponding to unique input tokens, and outputs $n$ embeddings $\{h_i\}^n_{i=1}$, preserving the input dimensions.
The $i$-th token is mapped via linear transformations to a key $k_i$, query $q_i$, and value $v_i$.
The $i$-th output of the self-attention layer is given by weighting the values $v_j$ by the normalized dot product between the query $q_i$ and other keys $k_j$:
\begin{equation}
    h_i=\sum^n_{j=1}softmax(\langle q_i,k_{j'}\rangle^n_{j'=1})_j\cdot v_j
\end{equation}

This allows the layer to assign ``credit'' by implicitly forming state-return associations via similarity of the query and key vectors (maximizing the dot product). 
It is worth noticing that most of the Transformer variants, such as BART \cite{lewis2019bart} or XLNet \cite{yang2019xlnet}, and the RNN variants, such as the Gated Recurrent Unit(GRU) \cite{cho2014gru} or Long-Short Term Memory network(LSTM) \cite{hochreiter1997lstm}, can be used as a generator in GANTEE.
But we use GPT-2 \cite{radford2019gpt2} in this paper to give the best experiment results.
GPT-2 modifies the transformer architecture with a causal self-attention mask to enable auto-regressive generation, it is known as the most famous auto-regressive model, which suits the task of fake query generation best.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{intro.png}
    \caption{An example of the GANTEE framework. This framework is implemented as a plugin before taxonomy expansion. The demand of generating high-quality data and taxonomy entering evaluation are formulated as a generative adversarial network in GANTEE.}
    \label{fig:introduction}
    \vspace{-5mm}
\end{figure*}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Framework.png}
    \caption{The framework of GANTEE.
    The generator of the GANTEE is on the left, and the duel discriminator is on the right.
    The detail information of this framework is in Section \ref{03}}
    \label{fig:framework.png}
\end{figure}

\subsection{The Rollout Discriminative Model for Short Term Reward in Generating}
Rollout discriminative model is used to judge whether the generated text is a concept before the generating process finish.
Deep discriminative models such as deep neural network(DNN), convolutional neural network(CNN) and recurrent convolutional neural network(RCNN) have shown a high performance in complicated sequence classification tasks.
In this paper, we choose LSTM\cite{hochreiter1997lstm} as our rollout discriminator since it is a lightweight model which is easy to do the training and won't have a so strict discriminative ability to the generated text.
LSTM will take in the input embedding $\mathbf{x_1},\dots,\mathbf{x_t}$ and use purpose-built memory cells to calculate the hidden state for each step $\mathbf{h_0}, \mathbf{h_1} \dots, \mathbf{h_t}$.
\begin{equation}
    \begin{aligned}
    % &i_t=\sigma\left(W_{xi}\mathbf{x_t}+W_{hi}\mathbf{h_{t-1}}+W_{ci}\mathbf{c_{t-1}}+\mathbf{b_i}\right),\\
    % &f_t=\sigma\left(W_{xf}\mathbf{x_t}+W_{hf}\mathbf{h_{t-1}}+W_{cf}\mathbf{c_{t-1}}+\mathbf{b_f}\right),\\
    % &c_t=f_t c_{t-1}+i_t tanh(W_{xc}\mathbf{x_t}+W_{hc}\mathbf{h_{t-1}}+\mathbf{b_{c}}),\\
    % &o_t=\sigma\left(W_{xo}\mathbf{x_t}+W_{ho}\mathbf{h_{t-1}}+W_{co}c_t+\mathbf{b_o}\right),\\
    % &h_t=o_t tanh(c_t).
    h_t = lstm(x_1,\cdots,x_t)
    \end{aligned}
\end{equation}
Where $\sigma$ is the logistic sigmoid function, and $i,\ f,\ o\text{ and }c$ are the input gate, forget gate, output gate and cell vectors, all of which are the same size as the hidden vector $h$.
We take the final hidden state $h_t$ and inject it into a linear layer to calculate the reward of the generated text:
\begin{equation}
    r_r = W_r \cdot \mathbf{h_t}+\mathbf{b_r}
\end{equation}
Where $W_r$ and $\mathbf{b_r}$ are the parameters in linear layer, the $r_r$ is the probability of the generated text is a concept.
% In this paper, we choose Bert\cite{devlin2018bert} as our discriminator as Bert has been shown of great effectiveness in Dependency Judgment task\cite{shi2019next}, which is good at understanding the hypernym relation .
% Most discriminative models can only perform classification well for an entire sequence rather than the unfinished one. 
% In this paper, we also focus on the situation where the discriminator predicts the probability that a finished sequence is real.
% We first represent an input sequence $x_1, \dots, x_t$ with the prior anchor concept $c_0:\{x^{c_0}_1,\dots,x^{c_0}_t\}$ as:
% $$\mathbb{E}_{1:t}=x^{c_0}_1\oplus \dots\oplus x^{c_0}_t\oplus x_1\oplus x_2\oplus \dots \oplus x_t$$
% where $x_i\in\mathbb{R}^k$ is the $k$-dimensional token embedding and $\oplus$ is the concatenation operator to build the matrix $\mathbb{E_{1:T}}\in \mathbb{R}^{T\times k}$.

% The optimization target is to minimize the cross entropy between the ground truth label and the predicted probability as formulated in Eq. \ref{eqt:object fucntion}.

\subsection{The Hyper Discriminative Model for Long Term Reward in Generating}
Hyper discriminative model is used to judge whether the generated concept has a hypernym relation to the prior concept after the generating process finish.
Such discriminative model offer a long term reward for the generating process, to better guide the direction of the generation.
The existing models for taxonomy expansion can be used as the hyper discriminative model, which has the ability of detect whether there is an hypernym relation between two concepts.

However, such model rely heavily on the vector representation of the concepts, many researches such as STEAM\cite{yu2020steam} or TMN\cite{wang2021enquire} use external information to get better representations for the concepts, and that would be really time consuming for the generating process, since each time the generative model produce a new concept, it will take time to get its detailed information and get its vector representation.

Instead, we use Bert\cite{devlin2018bert} to retrieval the representation of concepts since it shows its ability in Dependency Judgement task\cite{shi2019next}, which is also good at understanding the hypernym (hyponym) relation between two concepts.
We concatnate the position embedding proposed by the TaxoExpan\cite{shen2020taxoexpan} to the Bert representation, and put them into a linear layer to get the final predict result.

Specificly, we get the representation of the concept by the Bert in the following format:
\begin{equation}
\small
\begin{aligned}
    &x_{a}=Bert(x^{c_0}_1,\dots,x^{c_0}_t)[1]\\
    &x_{q}=Bert(x_1,\dots,x_t)[1]
\end{aligned}
\end{equation}
Then, we concatnate the position embedding to the representation of the anchor concept and the query concept, and put them into a linear layer:
\begin{equation}
\small
    r_h=W_h\cdot(x_{a}+p_{a}+x_{q}+p_{q}) + b_h
\end{equation}
Where $W_h$ and $\mathbf{b_h}$ are the parameters in linear layer, $p_{a}$ and $p_q$ are the position embedding, and the $r_h$ is the probability of the hypernym relation between the generated query concept and the anchor concept. 


\subsection{Policy Gradient for GANTEE}
GANTEE use a GAN model to combine the training data generating task and noisy query denoise task.
It need generative model to generate conceptual text, and use discriminative model to offer supervised signals to the generative model, but naive GAN is used for continuous data but not for discrete tokens.
The previous researchers \cite{bachman2015data, bahdanau2016actor, yu2017seqgan} propose to consider the text generation procedure as a sequential decision making process.
The objective function of such generative models is formulated to generate a sequence from the start state $s_0$ to maximize its expected end reward via policy gradient algorithms \cite{sutton1999policy}:
\begin{equation}
    \begin{aligned}
        J(\theta)&=\mathbb{E}[R_T|s_0=c_a;\theta]\\&=\sum_{y_1\in Y}G_\theta(y_1|s_0=c_a)\cdot Q^{G_\theta}_{D_\phi}(s_0=c_a, y_1),
    \end{aligned}
\end{equation}
where $J(\theta)$ is the object function, $R_T$ is the reward for a complete concept which has hypernym relation to the given concept $c_a$.
Note that the reward is from the hyper discriminator $D_{\phi H}$ and rollout discriminator $D_{\phi R}$, which we will discuss later.
$G_\theta$ is the generative model, and $Q^{G_\theta}_{D_\phi}(s,a)$ is the action-value function of a sequence, which is calculated in the following format:
\begin{equation}
    Q^{G_\theta}_{D_\phi}(a=y_t, s=Y_{1:t-1}, s_0) = D_{\phi}(s_0, Y_{1:t-1})
\end{equation}
Since the reward getting from an unfinished sequence often have large bias from the finished one, Monte Carlo search is used to sample the unknown last $T-t$ tokens\cite{yu2017seqgan}:
\begin{equation}
        \{Y^1_{1:T},\dots,Y^N_{1:T}\}=MC^{G_\theta}\left(Y_{1:t};N\right)
\end{equation}

Since a severe gradient vanishing occurs when discriminator $D_\phi$ is too strict.
So we use rollout discriminator $D_{\phi R}$ to calculate the expected reward based on Monte Carlo search during the genertive process for short-term reward, and use hyper discriminator $D_{\phi H}$ to calculate the reward when the generating is done for long-term reward. 
And the action-value function is hence transformed into the following format:
\begin{equation}
    \small
    \begin{aligned}
    &Q^{G_\theta}_{D_\phi}(a=y_t, s=Y_{1:t-1}, s_0) = \\
    &\begin{cases}
    \frac{1}{N}\sum^N_{n=1}D_{\phi R}(s_0, Y_{1:t-1}), Y^n_{1:T}\in MC^{G_\theta}\left(Y_{1:t};N\right)& \text{for }t<T\\
    D_{\phi H}(Y_{1:t}, s_0)& \text{for }t=T
    \end{cases}
    \end{aligned}
\end{equation}

Once we have a set of more realistic generated sequences, we will re-train the discriminator model as follows:
\begin{equation}
    \label{eqt:object fucntion}
    \min_{\phi}-\mathbb{E}_{Y~p_data}[\log D_\phi(Y)]-\mathbb{E}_{Y~G_\theta}[log(1-D_\phi(Y))]
\end{equation}

In summary, we will pre-train the generator and the discriminator before the adversarial process begin.
And after the pre-training stage, the generator and discriminator are trained alternatively.
As the generator gets progressed via training on g-steps updates, the discriminator needs to be re-trained periodically to keeps a good pace with the generator.
When training the discriminator, positive examples are sampled from the existing taxonomy $\mathcal{T}^{0}$ based on the hypernym relation $\mathcal{R}$, whereas negative examples are generated from our generator.
To ensure the learning process doing well, we use both of the fake query concepts together with the pure noisy text to as the negative query concepts to inject into the training process, and the number of negative query concepts we use is the same as the size of positive query concepts.

% \subsection{Improvement in the Higher-Quality Training Data Generating}
% Here we propose three extra methods to further improve the 
% To improve the effectiveness of taxonomy expansion, here we propose three extra methods which can generate training samples with higher-quality.

% First, we propose a positive and negative given generating methods

% Second, we propose to use Graph Convolutional Network to represent the whole taxonomy, and us
% Second, we propose to use Graph Convolutional Network to representation the whole taxonomy, and use such 