\begin{table*}[]
\centering
\resizebox{2.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
                    & Model usage                                                                                                        & \thead{Second-round \\pretraining data \\used before finetuning} & \thead{Experiment \\setting}      & Task \\ \midrule
RuleTakers          & RoBERTa-large~(355M)                                                                                                      & RACE                                                 & finetuning              & 0                                                        \\ \midrule
Leap-of-Thought     & RoBERTa-large                                                                                                      & -                                                    & finetuning              & 0                                                        \\ \midrule
FOLIO               & {[}BERT and RoBERTa{]}-{[}base and large{]}                                                                        & -                                                    & \thead{finetuning \\\& few-shot} & 0                                                        \\ \bottomrule
PRover              & RoBERTa-large                                                                                                      & -                                                    & finetuning              & 1                                                        \\ \midrule
multiPRover         & RoBERTa-large                                                                                                      & -                                                    & finetuning              & 1                                                        \\ \midrule
EntailmentWriter    & T5-11B                                                                                                             & -                                                    & finetuning              & 1                                                        \\ \midrule
ProofWriter         & T5-11B                                                                                                             & -                                                    & finetuning              & 1                                                        \\ \midrule
EVR                 & T5-small~(60M)                                                                                                     & -                                                    & finetuning              & 1                                                        \\ \midrule
IBR                 & RoBERTa-large                                                                                                      & -                                                    & finetuning              & 1                                                        \\ \midrule
IRGR                & \thead{T5-large~(770M) for generation model; \\MPNet-base-v2 for retriever}                                              & -                                                    & finetuning              & 1                                                        \\ \midrule
Selection-Inference & Chinchilla-7B                                                                                                      & -                                                    & \thead{finetuning \\\& few-shot} & 1                                                        \\ \midrule
FaiRR               & \thead{RoBERTa-large for fact and rule selectors;\\ T5-large for knowledge composer}                                         & -                                                    & finetuning              & 1                                                        \\ \midrule
MetGen              & \thead{T5-large for deduction and abduction models; \\ALBERT-xxlarge-v2 for controller}                                      & -                                                    & finetuning              & 1                                                        \\ \midrule
SCSearch            & \thead{T5-large for deduction model; \\DeBERTa-v3-large~(350M) for goal entailment model}                             & \thead{DeBERTa-v3-large\\ on MNLI}                             & finetuning              & 1                                                        \\ \midrule
ADGV                & \thead{T5-large for deduction model; \\T5-3B for abduction model; \\DeBERTa-v3-large for goal entailment model} & \thead{DeBERTa-v3-large\\ on MNLI}                             & finetuning              & 1                                                        \\ \midrule
NLProofS            & \thead{T5-large for deduction model; \\RoBERTa-large for verifier}                                                            & -                                                    & finetuning              & 1                                                        \\ \midrule
Entailer            & T5-11B                                                                                                             & -                                                    & finetuning              & 1                                                        \\ \midrule
Teachme             & T5-11B                                                                                                             & -                                                    & \thead{human \\interaction}       & 1                 \\
\bottomrule
\end{tabular}
}

\caption{A collection of model usage, (second-round) pretraining data usage, and experiment setting for methods in hypothesis classification~(denoted as ``0'' in ``Task'' column) and proof generation~(denoted as ``1'' in ``Task'' column) tasks.}
\label{tab:model_usage_pretraining_data_usage}
\end{table*}