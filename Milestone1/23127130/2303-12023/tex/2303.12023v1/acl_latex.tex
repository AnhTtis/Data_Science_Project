% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% added package
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{utfsym}
\usepackage{amsmath}
\usepackage{mathrsfs}
\DeclareMathOperator*{\argmaxhi}{arg max\,h_i}
\usepackage{makecell}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}\,\,}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\definecolor{bblue}{rgb}{0.54, 0.81, 0.94}
\newcommand{\zy}[1]{\xxcomment{bblue}{Z}{Y}{#1}}
\newcommand{\xxcomment}[4]{\textcolor{#1}{[$^{\textsc{#2}}_{\textsc{#3}}$ #4]}}

\newcommand{\rui}[1]{\textcolor{red}{#1}}

\definecolor{YELLOW}{rgb}{0.83, 0.61, 0.18}
\newcommand{\jinjie}[1]{\xxcomment{YELLOW}{J}{N}{#1}}

% natural language driven logical reasoning
\title{Logical Reasoning over Natural Language as \\Knowledge Representation: A Survey}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author {
    Zonglin Yang\textsuperscript{\rm 1}
    Xinya Du\textsuperscript{\rm 2}
    Rui Mao\textsuperscript{\rm 1}
    Jinjie Ni\textsuperscript{\rm 1}
    Erik Cambria\textsuperscript{\rm 1} \\
    \textsuperscript{\rm 1} Nanyang Technological University \\
    \textsuperscript{\rm 2} University of Texas at Dallas \\
    \{zonglin.yang, rui.mao, jinjie001, cambria\}@ntu.edu.sg 
    xinya.du@utdallas.edu
}

\begin{document}
\maketitle


\begin{abstract}
Logical reasoning is central to human cognition and intelligence.
Past research of logical reasoning within AI uses formal language as knowledge representation~(and symbolic reasoners).
However, reasoning with formal language has proved
challenging~(e.g., brittleness and knowledge-acquisition bottleneck).
This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation~(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks \& methods in the future, and relation to related NLP fields.
This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.
\end{abstract}


\section{Introduction}
An argument consists of premise(s) and a conclusion.
% what is logical reasoning; why it is important
Logical reasoning is a form of thinking in which premises and relations between premises are used in a rigorous manner to infer conclusions that are entailed~(or implied) by the premises and the relations~\citep{Nunes2012}. 
It consists of three reasoning types, namely deductive reasoning, inductive reasoning, and abductive reasoning~\citep{flach2000abductive}~(more illustration on the categorization can be found in \S\ref{sec:definition}).
It is important since the ability to reach logical conclusions on the basis of prior information is recognized as central to human cognition
and intelligence~\citep{goel2017reasoning}.


% in the past reseach of logical reasoning within computer science, formal language is used as knowledge representation; however many problems
The past research of logical reasoning within AI uses formal language~(e.g., first-order logic) as knowledge representation and symbolic reasoners~\citep{DBLP:journals/jlp/MuggletonR94}.
This paradigm has resulted in impressive applications such as expert systems~\citep{DBLP:journals/jim/MetaxiotisAP02}.
However, building and reasoning over formal language have proved 
% ,DBLP:journals/ml/CropperDEM22
challenging~\citep{musen1988brittleness}, with representative disadvantages of brittleness and knowledge-acquisition bottleneck.
% inability to handle raw inputs such as natural language, sensitive to label errors, and failure to recognize different symbols with the same meaning.

\input{figures/figure1.tex}
% recently there's a trend of using natural language as a new knowledge representation for logical reasoning; many advantages of the new KR, overcoming the problems of formal language; at the same time, since the task is to do logical reasoning, a natural language based logical reasoning system also has the advantages over pure neural methods, such as ...
Since the rapidly developed NLP techniques, natural language has been explored as a new knowledge representation, and pretrained language model~(PLM) has been used as a new corresponding reasoner for deductive reasoning~\citep{DBLP:conf/ijcai/ClarkTR20}, abductive reasoning~\citep{DBLP:conf/iclr/BhagavatulaBMSH20}, and inductive reasoning~\citep{Yang2022Language}.
Therefore, all three reasoning types of logical reasoning have been investigated with natural language as knowledge representation.
Recent research also shows that PLMs can be finetuned or prompted to have a good level of ability for each of the reasoning types.

In this paper, we summarize the three previously separately investigated reasoning types together with a new overall concept, logical reasoning over natural language~(LRNL) as knowledge representation,
and provide the first survey of LRNL~\footnote{We will also summarize the datasets together as a comprehensive evaluation benchmark to promote evaluation on the three sub-reasoning aspects of logical reasoning: \url{https://github.com/ZonglinY/LRNL-bench.git}}.
% we propose a new concept, logical reasoning over natural language as knowledge representation~(LRNL).
% It includes all three specific reasoning types~(such as deductive reasoning) over natural language.
Illustrated in Figure~\ref{fig:main_framework},
LRNL means a new paradigm for logical reasoning that uses new knowledge representation~(natural language) and new reasoner~(PLM).
% While there's no standard definition of its methodology, 
Current methods in the area generally make one step of reasoning~(out of the three reasoning types) with one inference of PLM.
For complex problems, they usually have access to a knowledge base which stores relevant textual knowledge to be retrieved as premises to support the reasoning process to reach a conclusion, which might be used as a new premise for next step's reasoning.
By iteratively repeating this process, a final conclusion may be made.
Although looks similar to expert systems,
we will discuss how LRNL makes it possible to overcome many main challenges of the previous paradigm
% ~(mainly be used in logical programming or expert systems)
% ~(formal language and symbolic reasoners) 
such as brittleness and knowledge-acquisition bottleneck in \S\ref{subsec:advantages_over_formal_language}.

In addition to the comparison with formal language, in \S\ref{subsec:advantages_over_nesy_methods} we discuss that LRNL could be viewed as a new type of neural-symbolic~(NeSy) method, which has unique advantages over existing NeSy methods.
We also discuss how LRNL, as a NeSy method, has advantages over existing end-to-end neural methods~(e.g., explainability, controllability, less catastrophic forgetting) in \S\ref{subsec:advantages_over_e2e_neural_methods}.
These advantages make a LRNL system possible to deal with many challenging problems today.




% here we survey the papers on LR with NL as new KR; The scope of this paper does not cover Chain-of-thought, but on finetuned small modular-based frameworks;
% In the remaining sections of this survey, we review papers on LRNL, list challenges, and try to propose valuable future research directions.
% Note that we do not cover end-to-end out-of-box large language model~(LLM) prompting techniques such as chain-of-thought~\citep{DBLP:journals/corr/abs-2201-11903}.
% Rather, we focus on smaller, typically finetuned and modular-based methods for reasoning~(more analysis on the difference can be found in \S\ref{subsec:chain-of-thoughts}). 

In the remaining sections of this survey, we review papers on LRNL~(including deductive reasoning~\S\ref{sec:deductive}, inductive reasoning~\S\ref{sec:inductive}, and abductive reasoning~\S\ref{sec:abductive}), list challenges~(\S\ref{sec:challenges}), and try to analyze possible future research directions~(\S\ref{sec:desirable_future}).
Note that we do not cover end-to-end out-of-box large language model~(LLM) prompting techniques such as chain-of-thought~\citep{DBLP:journals/corr/abs-2201-11903}.
Rather, we focus on smaller, typically finetuned and modular-based methods for reasoning~(more analysis on the difference can be found in \S\ref{subsec:chain-of-thoughts}). 
For each reasoning type, we summarize existing task formulations, datasets, and methods under each task.
In \S\ref{appen:relation_to_related_fields} we also discuss the relation of LRNL to related NLP fields, which could be helpful to form a clear shape of LRNL in NLP.



% Overall, we introduce the definition and categorization of logical reasoning~(\S\ref{sec:definition});
% advantages of LRNL~(\S\ref{sec:advantages_of_new_paradigm});
% different reasoning types of LRNL, including deductive reasoning~(\S\ref{sec:deductive}), inductive reasoning~(\S\ref{sec:inductive}), and abductive reasoning~(\S\ref{sec:abductive});
% challenges of LRNL~(\S\ref{sec:challenges});
% desirable tasks and methods in the future~(\S\ref{sec:desirable_future});
% and conclusion~(\S\ref{sec:conclusion}).
% For each reasoning type, we summarize existing task formulations, datasets, and methods under each task.
% In \S\ref{appen:relation_to_related_fields} we also discuss the relation of LRNL to related NLP fields, which could be helpful to form a clear shape of LRNL in NLP.





 

% this section could be an important reason that others cite this paper. maybe make it as accurate, clear, and comprehensive as possible.
\section{Definition and Categorization of Logical Reasoning}
\label{sec:definition}
There are many subjects related to logical reasoning, including philosophy, logic, and AI.
Among them, the definition and categorization aspects of logical reasoning are handled by philosophy research. 
However, 
% there are two different opinions 
debate exists
in philosophy research on the categorization of logical reasoning. 

One group believes that every argument can be classified as deduction argument, inductive argument, or fallacy~\citep{salmon1989introduction}. 
Without considering fallacy, given that an argument consists of premises and a conclusion, when the premises can provide conclusively support to the conclusion~(which means that if the premises of the argument were all true, it would be impossible for the conclusion of the argument to be false), this argument is a deductive argument.
Conversely, when the premises can not provide conclusively support to the conclusion, the argument is 
% an inductive argument.
inductive.


The other group has the same definition of deductive reasoning, but they believe that further categorization of non-deductive reasoning is necessary.
Without considering fallacy, they believe in a trichotomy of deductive, inductive, and abductive reasoning~\citep{peirce1974collected}.
% Without considering fallacy, the other group believes in a trichotomy of deductive, inductive, and abductive reasoning~\citep{peirce1974collected}.
% The two groups have the same definition of deductive reasoning, but the second believes that further categorization of non-deductive reasoning is necessary.
However, even for the second group, the definition and difference between inductive and abductive reasoning are also controversy~\citep{flach2000abductive}. 
Nevertheless,~\citet{flach2000abductive} argue that from the utility perspective of AI, a distinction between inductive and abductive reasoning is possible.
Specifically, pure inductive reasoning is to only provide (usually sample to population) generalizations, while pure abductive reasoning is to only provide explanations.

For example, if a white ball is found in a bag, inductive reasoning might lead to the conclusion that ``all balls in this bag are white'', while abductive reasoning might lead to the conclusion that ``someone put the white ball into this bag''. 

Considering that inductive and abductive reasoning can be distinctive enough when formulated in NLP,
in this paper, we adopt the second group, particularly~\citet{flach2000abductive}'s view of definition and categorization of logical reasoning. 
We survey deductive, inductive, and abductive reasoning in NLP separately in the following sections. 
% \zy{maybe introduce the concept of $fact$ and $rule$ --- it might depend on the scope of information.}


\section{Advantages of LRNL}
\label{sec:advantages_of_new_paradigm}


\subsection{Advantages over Formal Language}
\label{subsec:advantages_over_formal_language}

Building and reasoning over formal language have proved challenging~\citep{musen1988brittleness,DBLP:journals/ml/CropperDEM22}, with disadvantages such as (1) brittleness~(the failure of an expert system when its knowledge base does not contain complete knowledge for a problem), (2) knowledge-acquisition bottleneck~(the knowledge base needs human experts to encode their knowledge with formal representation),
(3) inability to handle raw data such as natural language, (4) sensitivity to label errors, and (5) failure to recognize different symbols with similar meanings.


Nevertheless, the new paradigm of logical reasoning, LRNL, has systematic strengths over these challenges.
Specifically, PLMs contain knowledge themselves~\citep{DBLP:conf/emnlp/DavisonFR19}, which makes it possible for them to provide good answers even when some required explicit knowledge is not present in a knowledge base~\citep{talmor2020leap}~(less brittle), and be less affected by input errors~\citep{DBLP:conf/emnlp/0001ZHWZJ021}; 
with natural language as knowledge representation, such a system can naturally handle raw input,
% as natural language
and is possible to utilize the enormous web corpora to automatically construct a rule base using information extraction~\citep{DBLP:reference/db/Ji18} or inductive reasoning~\citep{Yang2022Language}~(less affected by knowledge-acquisition bottleneck);
using embeddings for concepts~\citep{DBLP:conf/nips/MikolovSCCD13}, it semantically ``understands'' the meaning of symbols and therefore robust for paraphrasing. 
% \rui{[break it up]}



%% It is a promising direction, since the advantages of logical reasoning with new KR make its possible to be used in many applications (previous applications for expert system; ) and alleviate many problems of current pure neural methods for NLP applications (explanability, controllable, forgetting...) (what NLP tasks could be benefited, maybe list some)
% \subsection{Advantages over\! Neural-symbolic\! Methods}
\subsection{Advantages over Existing NeSy Systems}
\label{subsec:advantages_over_nesy_methods}

LRNL could be seen as a new type of NeSy in addition to the existing 6 types summarized by ~\citet{kautz2022third}, as its goal and design of methodology are typically symbolic~(logical reasoning with knowledge bases), while avoiding any symbolic representation, using~(currently pure) neural methods.
Therefore LRNL can avoid many bottlenecks of the other NeSy methods caused by symbolic representation, such as symbolic knowledge acquisition and scalability~\citep{wang2022towards}.
% LRNL has some advantages over the other 6 types of NeSy methods since it totally avoids symbolic representation, and therefore alleviates many disadvantages of previous NeSy methods such as hard symbolic knowledge acquisition and poor scalability~\citep{wang2022towards}.


\subsection{Advantages over E2E Neural Methods}
\label{subsec:advantages_over_e2e_neural_methods}
As a NeSy method, LRNL systematically has some advantages over end-to-end neural methods, such as interpretability~\citep{DBLP:journals/ipm/CambriaMMMN23}~(since its stepwise reasoning nature), more controllability~(LRNL reasons following a given knowledge base), and less catastrophic forgetting~(LRNL uses an explicit knowledge base to store knowledge).
% \rui{[cite articles to support the above arguments]}




\section{Deductive Reasoning over Natural Language}
\label{sec:deductive}

\subsection{Existing Task Formulations}
% \zy{mention the NL-FOL translation task}
% The existing tasks on deductive reasoning over natural language can be summarized into two classes: hypothesis classification and proof generation. 
\input{tables/deductive_datasets.tex}

Existing tasks for deductive reasoning can be summarized as hypothesis classification, proof generation, proof generation with incomplete information, and implication enumeration.
Datasets for the tasks are summarized in Table~\ref{tab:deductive_reasoning_datasets}.
``Proof generation'' tab with \xmark means it is for hypothesis classification task.

\paragraph{Hypothesis Classification}
% Each example in \rui{D* [D* dataset? define D* because you mentioned it in the later context]} 

Each data example for hypothesis classification task is a tuple $(theory, hypothesis, correctness)$, where $theory$ typically has the form $(fact^*, rule^*)$, $hypothesis$ is a question, and $correctness$ can be $True$ or $False$ (or $Unknown$).
This task requires to predict the $correctness$ for the $hypothesis$ given the $theory$.
% The differences between this task and question answering are illustrated in \S\ref{append:relation_deductive_reasoning}.




\paragraph{Proof Generation}
% \zy{also introduce the form of proof in different datasets}
The proof generation task has the same setting as the hypothesis classification task, except that in addition to predicting a $correctness$, the proof generation task also requires to provide a $proof$ given $theory$ to explain the $correctness$.
The $proof$ is a directed tree $(\mathcal{N}, \mathcal{E})$ with nodes $n \in \mathcal{N}$ and edges $e \in \mathcal{E}$.
Each node is an item of knowledge in $theory$~(usually a $fact$ or a $rule$), or a generated intermediate reasoning conclusion, or the $hypothesis$ itself;
Each edge points from a premise node to a conclusion node to form a deductive argument, which typically needs one-step inference~(not multi-step).


\paragraph{Proof Generation with Incomplete Information}
This task is the same as the proof generation task, except that $theory$ lacks one $node$ to form a complete $proof$. 
Specifically, given $theory$, it requires to predict the $correctness$ of $hypothesis$ with a $proof$, as well as recovering the missing $node$.


\paragraph{Implication Enumeration}
Given a $theory$,
% which usually has the form $(fact^*, rule^*)$
this task requires to enumerate implications
% ~(usually new $facts$) 
of the $theory$, using deductive reasoning.




% \subsection{Datasets}
% % \zy{also evaluation metrics.}
% % \zy{introduce the table by introduce the properties discussed in the table.}
% % \rui{[the table can be put in the appendix if not enough space]}
% %~\citep{DBLP:conf/ijcai/ClarkTR20}
% %~\citep{DBLP:conf/acl/TafjordDC21}
% %~\citep{DBLP:conf/emnlp/DalviJTXSPC21}
% %~\citep{DBLP:journals/corr/abs-2209-00840}

% % \paragraph{Birds-Electricity}
% %~\citep{DBLP:conf/ijcai/ClarkTR20}
% % Deductive reasoning 
% Datasets are summarized in Table~\ref{tab:deductive_reasoning_datasets}.


\subsection{Methods}
\input{tables/proof_generation_methods.tex}

\subsubsection{Hypothesis Classification}
There are mainly three categories of methods for the hypothesis classification task regarding a multi-task aspect.
The first category of methods only conducts the classification task itself;
Methods from the second category can predict $correctness$ as well as generate a $proof$. 
However, the $correctness$ is not necessarily consistent with the predicted $proof$.
The third category is similar to the second, except that $correctness$ always follows $proof$.
% Here we focus on how the methods predict $correctness$, while the $proof$ generation aspect will be illustrated in \S\ref{subsec:proof_generation}.

Until now, methods from the first category directly use transformer-based PLMs~\citep{vaswani2017attention}, with the target of analyzing and benchmarking their performance in different settings~(datasets).
%
Specifically,~\citet{DBLP:conf/ijcai/ClarkTR20} find that finetuned RoBERTa-large~\citep{DBLP:journals/corr/abs-1907-11692} can achieve 95\%+ accuracy on the test set of D* and ParaRules datasets;
%
\citet{talmor2020leap} further demonstrate that LMs can be trained to reliably perform deductive reasoning using both implicit, pre-trained knowledge and explicit natural language statements~($theory$) to make predictions;
%
\citet{DBLP:journals/corr/abs-2209-00840} evaluate finetuned medium-sized language models and few-shot prompting on LLMs on the FOLIO dataset.
However, they find that LLM with few-shot prompting only performs slightly better than random results.

The second category methods typically infer PLMs only once, and then utilize the final layer embeddings or generations to obtain $correctness$ and $proof$.
Specifically, PRover~\citep{DBLP:conf/emnlp/SahaGSB20} and multiPRover~\citep{DBLP:conf/naacl/SahaYB21} use the [CLS] token to predict $correctness$, and leverage the final layer embeddings of knowledge items in $theory$ to generate $proof$;
%
All-At-Once ProofWriter~\citep{DBLP:conf/acl/TafjordDC21} and EntailmentWriter~\citep{DBLP:conf/emnlp/DalviJTXSPC21} generate $correctness$ and linearized $proof$ at the same time.

The third category methods create a $proof$ first, and then predict $correctness$ from the $proof$.
\S\ref{subsec:proof_generation} illustrates these methods in detail.




% \textbf{RuleTakers}
%~\citet{DBLP:conf/ijcai/ClarkTR20} construct D* and ParaRules and analyze RoBERTa-large's~\citep{DBLP:journals/corr/abs-1907-11692} performance on it.
% % Specifically, the input are $theory$ and $hypothesis$, and the output is $correctness$.
% They find that it can achieve 95\%+ accuracy on test set,
% % which is even with deeper reasoning chain requirement than train set.
% which indicate that 
% % when the $theory$ contains all the necessary information to predict the correctness of a $hypothesis$, 
% transformers can be trained to implicitly conduct deductive reasoning over $theory$ at least to a limited reasoning steps to obtain necessary knowledge for the prediction.

% \textbf{Leap-Of-Thought}
%~\citet{talmor2020leap} further investigate whether PLMs can utilize not only knowledge from the explicitly given $theory$ but also the implicit knowledge within the PLMs to perform deductive reasoning. 
% They find that models can be trained to reason over both explicit and implicit knowledge.

% \textbf{FOLIO}
%~\citet{DBLP:journals/corr/abs-2209-00840} propose FOLIO dataset and test PLM's fine-tuning~(around 60\% accuracy) and few-shot performance on it.
% Comparing to previous datasets, FOLIO is expert-written, realistic, has more premises and larger vocabulary. 
% Although FOLIO has more natural language variety, it might be interesting to investigate the low accuracy compared to D* and ParaRules.


\subsubsection{Proof Generation}
\label{subsec:proof_generation}

Current methods for the proof generation task roughly consist of three stages. 
In each stage, one key new technique is considered and developed.
In stage 1, PLMs are used for forming $proof$ in one inference step.
In stage 2, modular-based, stepwise frameworks are developed to create $proof$~(each module is usually implemented with a single PLM).
In stage 3, a verifier is added as a new module to make sure that each reasoning step reflects the belief of PLMs.
We will introduce the motivation and typical method for each stage.

% In addition to the original task, an additional aspect is also attended to~(whether enabling human corrections). 
% New methods are developed for the new aspects.

Methods for stage 1 typically utilize the last layer embeddings~\citep{DBLP:conf/emnlp/SahaGSB20,DBLP:conf/naacl/SahaYB21} or generations~\citep{DBLP:conf/acl/TafjordDC21,DBLP:conf/emnlp/DalviJTXSPC21} to create $proof$.
%
Methods utilizing embedding typically (1) obtain an averaged embedding for each knowledge item in $theory$, and (2) pass each embedding to a node classifier, and each embedding pairs to an edge classifier to predict nodes and edges for $proof$.
Constraints are usually used to enforce the structure of $proof$.
%
Generation methods directly generate linearized $correctness$ and full $proof$ given linearized $theory$ and $hypothesis$.


% For stage 1 methods utilizing embeddings,
% PRover~\citep{DBLP:conf/emnlp/SahaGSB20} first obtains an averaged embedding for each knowledge item in $theory$.
% It then pass each embedding to a node classifier to predict whether the corresponding knowledge item should be a node in $proof$, and each embedding pairs to an edge classifier to predict edges.
% The structure of the predicted $proof$ is enforced by several constraints they propose optimized using an integer linear program problem;
% %
%~\citet{DBLP:conf/naacl/SahaYB21} further propose multiPRover to address a new problem of generating multiple $proofs$ when multiple sound proofs exist, by posing this task as a set generation problem over structured output spaces.
% One variant of multiPRover, multilabel-multiPRover, substitutes the node and edge classifiers in PRover with multilabel node and edge classifiers correspondingly.

% For stage 1 generation methods, All-At-Once ProofWriter~\citep{DBLP:conf/acl/TafjordDC21} and EntailmentWriter~\citep{DBLP:conf/emnlp/DalviJTXSPC21} directly generate linearized $correctness$ and full $proof$ given linearized $theory$ and $hypothesis$.

The motivations of stage 2 methods are generally concerned with end-to-end methods, which is considered to lack interpretability~\citep{DBLP:conf/naacl/LiangBS21,DBLP:conf/naacl/QuCGDX22,DBLP:conf/acl/SanyalS022,DBLP:journals/corr/abs-2201-06028}, suffer from compositional generalization problems~\citep{DBLP:conf/naacl/LiangBS21,DBLP:journals/corr/abs-2205-09712}, have limited input size~\citep{DBLP:conf/naacl/Ribeiro0MDWZCXH22}, are not casual~\citep{DBLP:journals/corr/abs-2205-09712}, and lack constraints on the validity of each inference step~\citep{DBLP:conf/naacl/HongZYZ22}.

Methods in stage 2 can be summarized as having two components, an inference module and a reasoning controller.
The inference module can be a deduction module~\citep{DBLP:conf/acl/TafjordDC21,DBLP:conf/naacl/Ribeiro0MDWZCXH22,DBLP:journals/corr/abs-2205-09712,DBLP:conf/acl/SanyalS022,DBLP:journals/corr/abs-2201-06028}, an abduction module~\citep{DBLP:conf/naacl/LiangBS21,DBLP:conf/naacl/QuCGDX22}, or both~\citep{DBLP:conf/naacl/HongZYZ22,DBLP:journals/corr/abs-2211-00614}.
The deduction module performs deductive reasoning, and reasons forwardly from $theory$ to $hypothesis$ to construct $proof$;
the abduction module performs abductive reasoning, and reasons backwardly from $hypothesis$ to $theory$ to construct $proof$.
The reasoning controller in general performs a search process that each step it searches through the $theory$ and generated intermediate conclusions space to select~(retrieve) premises for next step inference.
The search processes include exhaustive search~\citep{DBLP:conf/acl/TafjordDC21,DBLP:conf/naacl/LiangBS21} or heuristic search~\citep{DBLP:conf/naacl/QuCGDX22,DBLP:conf/naacl/Ribeiro0MDWZCXH22,DBLP:journals/corr/abs-2205-09712,DBLP:conf/acl/SanyalS022,DBLP:journals/corr/abs-2201-06028,DBLP:conf/naacl/HongZYZ22,DBLP:journals/corr/abs-2211-00614}.
The reasoning controller usually can also stop the search process if it detects the goal.


% Specifically, iterative ProofWriter~\citep{DBLP:conf/acl/TafjordDC21} adopts a deduction module, and simply selects the full $theory$ and all intermediate conclusions as premise, and performs an exhaustive search that it stops until no more conclusions can be generated;
% EVR~\citep{DBLP:conf/naacl/LiangBS21} adopts an abductive module, and exhaustively search backward until $theory$ satisfies a branch of its abductive conclusions;
% IBR~\citep{DBLP:conf/naacl/QuCGDX22} adopts an abductive module, and performs heuristic search by alternatively predicting a new parent node to expand existing $proof$, and the child nodes for the parent node;
% IRGR~\citep{DBLP:conf/naacl/Ribeiro0MDWZCXH22}, Selection-Inference~\citep{DBLP:journals/corr/abs-2205-09712}, FaiRR~\citep{DBLP:conf/acl/SanyalS022}, and SCSearch~\citep{DBLP:journals/corr/abs-2201-06028} adopt an deduction module, and perform heuristic search by using a retriever that each time retrieves from $theory$ and existing intermediate conclusions as premises for next step inference;
% MetGen~\citep{DBLP:conf/naacl/HongZYZ22} adopts both deduction module and abduction module, and performs heuristic search by scoring every possible steps~(including both deduction and abduction steps) for selection. 

Motivation of stage 3 methods is similar, basically that
stage~2 methods lack explicit verifiers to avoid hallucinating invalid steps~\citep{DBLP:journals/corr/abs-2205-12443}, and to ensure that the inference processes reflect PLM's own beliefs~\citep{DBLP:journals/corr/abs-2210-12217}.

Methods in stage 3 can be summarized as utilizing explicit verifier(s)~(implemented with a PLM) to check the validity of each inference step.
One way is to add a new module (additional to the inference module and reasoning controller in stage 2), working as a ``fact checker'' to verify the generated inference step~\citep{DBLP:journals/corr/abs-2205-12443,DBLP:journals/corr/abs-2210-12217};
The other one, called round-trip consistency, is only suitable for methods that use both deduction and abduction modules, where deduction and abduction modules work as the verifier for each other~\citep{DBLP:journals/corr/abs-2211-00614}.

% , where the validity scores usually are also leveraged by reasoning controller for heuristic search~\citep{DBLP:journals/corr/abs-2205-12443,DBLP:journals/corr/abs-2210-12217}.

In addition to the general 3 stages, a new aspect is attended to, which is whether teachable by humans.
Build based on Entailer~\citep{DBLP:journals/corr/abs-2210-12217}, TeachMe~\citep{DBLP:journals/corr/abs-2204-13074} shows that user corrections can help override erroneous model beliefs, and that a system can gradually improve by accumulating user corrections.
Compared to Entailer, it adds an interaction module and a dynamic memory module to obtain and store human corrections.






% \textbf{PRover, multiPRover, and PRobr}
%~\citet{DBLP:conf/emnlp/SahaGSB20} is the first to attend to proof generation, and propose PRover for the task. 
% Compared to RuleTakers \rui{[cite]}, it further leverages RoBERTa embeddings as input to a node classifier and an edge classifier to generate a $proof$. 
% They also frame inference as an integer linear program optimization problem to enforce consistency constraints on \rui{[what?]}
% % the structure of the 
% $proof$;
% Based on PRover,~\citet{DBLP:conf/naacl/SahaYB21} propose multiPRover to address a new problem of generating multiple $proofs$ since multiple sound proofs can exist \rui{[in real-world texts?]};
%~\citet{DBLP:conf/acl/SunZCGWCZL21} argue that $proof$ generated by PRover might not contribute to its $correctness$ prediction.
% They accordingly propose PRobr, which defines a joint probabilistic distribution over proof graphs and answers via an induced graphical model.


% \textbf{ProofWriter and EntailmentWriter}
%~\citet{DBLP:conf/acl/TafjordDC21} investigate generative pretrained models for proof generation on synthesized datasets and propose ProofWriter. Specifically, the input are linearized $theory$ and $hypothesis$, and the output are linearized $correctness$ and $proof$. 
% They evaluate two methods of proof generation, ``All-At-Once'' and ``Iterative''. 
% The first is to generate the full proof and answer \rui{[a full proof and an answer or full proofs and answers?]} in one inference, and the latter is to generate a single 1-step implication for each inference;
% Similarly,~\citet{DBLP:conf/emnlp/DalviJTXSPC21} further investigate generative PLM on human written and realistic dataset and propose EntailmentWriter \rui{[a PLM or PLMs; a dataset or datasets?]}.


% \textbf{EVR and IBR}
% EVR~\citep{DBLP:conf/naacl/LiangBS21} and IBR~\citep{DBLP:conf/naacl/QuCGDX22} both reason backwardly from $hypothesis$. 
%~\citet{DBLP:conf/naacl/LiangBS21} argue that an end-to-end method 
% % such as RuleTakers 
% lacks interpretability and suffers from compositional generalization problems.
% % ~(tend to fail when the number of reasoning steps are much larger in the evaluation set).
% % Motivated by a classic reasoning paradigm, the general problem solver~\citep{DBLP:conf/ifip/NewellSS59}, they propose 
% Accordingly, they propose EVR, a framework consisting of an operator proposer, an executor, and a working memory.
% Overall EVR exhaustively explores all paths at every step;
% % Overall EVR exhaustively reasons abductively from the $hypothesis$ until $correctness$ can be entailed by comparing the new 
% % abductive 
% % conclusions with $theory$. 
%~\citet{DBLP:conf/naacl/QuCGDX22} argue that (1) PRover and PRobr lack interpretability, and (2) EVR and iterative ProofWriter are inefficient \rui{[in doing what? computationally inefficient?]}.
% Accordingly, similar to PRover, IBR first obtains the embedding of every argument in $theory$,
% with which they iteratively predict the next node to select as $proof$.
% Benefiting from using heuristic search instead of exhaustive search and using fixed embeddings, \rui{[the improved?]} IBR has better interpretability and efficiency.


% % \textbf{ParaPattern}
% %~\citet{DBLP:conf/emnlp/BostromZCD21} 


% \textbf{IRGR}
%~\citet{DBLP:conf/naacl/Ribeiro0MDWZCXH22} argue that single-shot, end-to-end generative models have limited input size~(so it's possible that not all relevant premises from $theory$ can be used in input), and can not leverage the partially generated $proof$.  
% Accordingly, they propose a modular framework with a retriever and a generator, where at each step, the retriever retrieves parts of $theory$ for the generator. 


% \textbf{Selection-Inference}
%~\citet{DBLP:journals/corr/abs-2205-09712} argue that single-shot, end-to-end generative methods are not casual, and a modular-based method is easier to adapt to complex tasks.
% Therefore, they propose a modular-based method consisting of a knowledge selector, an inference module, and an algorithm to control the reasoning process.
% They also analyze the scale effect on this task and compare their framework's performance with and without finetuning. 


% \textbf{FaiRR}
% To explicitly ensure the causality from the rule/fact selection to generating inferences, 
%~\citet{DBLP:conf/acl/SanyalS022} propose a modular-based framework consisting of a rule selector, a fact selector, and a knowledge composer which generates a conclusion from the selected rules and facts.
% % For each inference of FaiRR, in terms of proof graph, edges are created pointing from the selected nodes to the generated conclusion node.

% \textbf{MetGen}
%~\citet{DBLP:conf/naacl/HongZYZ22} argue that an end-to-end generative method 
% % for proof generation task 
% such as ProofWriter has the limits of lacking constraints on the validity of every inference step and invisible mapping mechanisms from the input to the proof graph. 
% They accordingly propose MetGen, a modular-based framework consisting of a deduction module and an abduction module that build $proof$ from $theory$ and from $hypothesis$ correspondingly, and also a reasoning controller to control the search process.


% \textbf{SCSearch}
% Similar to MetGen,
%~\citet{DBLP:journals/corr/abs-2201-06028} argue that end-to-end generative methods can not offer interpretability and propose a modular-based framework consisting of a deduction model, a learned heuristic model~(for best first search of deduction over $theory$) and a goal entailment model~(to determine when the search should end).


% \textbf{Braid}
%~\citet{DBLP:conf/aaai/KalyanpurBF22}



% \textbf{NLProofS}
%~\citet{DBLP:journals/corr/abs-2205-12443} argue that previous stepwise methods, given $hypothesis$ as input, tend to hallucinate invalid steps leading to the $hypothesis$.
% Motivated by this observation, they propose a modular-based method consisting of a stepwise prover~(reasons forwardly), a verifier, and an algorithm to search for $proof$.
% The verifier helps to generate valid and relevant steps to hypotheses.


% \textbf{Entailer}
% Entailer~\citep{DBLP:journals/corr/abs-2210-12217}, which reasons backwardly, is a concurrent work of NLProofs.
% In addition to faithfulness~($correctness$ is consistent with $proof$),
% it focuses on not only generating a valid reasoning step,
% % ~(an edge of $proof$), 
% but also a valid premise.
% % ~(a node of $proof$).
% Here validness means conforming to the model's own beliefs.
% Accordingly, Entailers consists of an abduction module, an edge validness verifier, a node validness verifier, and an algorithm to combine the modules. 


% \textbf{TeachMe}
%~\citet{DBLP:journals/corr/abs-2204-13074} show that user corrections can help override erroneous model beliefs. 
% A system can gradually improve by accumulating user corrections.
% User corrections are particularly viable for models which generate $proof$ along with $correctness$.
% TeachMe consists of three key components, which are a proof tree generator~(initialized with Entailer), 
% an interaction module~(with human users), and a dynamic memory~(stores user-corrected beliefs).



% \subsubsection{Proof Generation with Incomplete Information}
\subsubsection{Proof with Incomplete Information}
% \textbf{Deduction with Incomplete Information}

% ADGV~\citep{DBLP:journals/corr/abs-2211-00614} is the only method focusing on this task.
% Concurrent to MetGen, 
ADGV~\citep{DBLP:journals/corr/abs-2211-00614} uses both deduction and abduction modules, and the reasoning controller performs heuristic search.
It uses the abduction module to recover the missing premise.



% \textbf{ADGV} 
% Concurrent and similar to MetGen, ADGV~\citep{DBLP:journals/corr/abs-2211-00614} is a modular-based framework consisting of a deduction module, an abduction module, and a search algorithm.
% ADGV uses an abduction module to recover the missing premise.
% They also propose a new intermediate inference validation method noted as round-trip consistency, where deduction and abduction modules are used to check each other's generations.






\subsubsection{Implication Enumeration}
% To the best of our knowledge, 
%~\citet{DBLP:conf/acl/TafjordDC21} is the only paper focused on this task.
\citet{DBLP:conf/acl/TafjordDC21} compare the performance of ``All-At-Once'' and ``Iterative'' ProofWriter for this task.
They find that ``All-At-Once'' performs worse, mainly because it struggles with problems that are more complex than training examples.

% One difficulty is that only synthetic dataset seems to be cost-acceptable for this task, since the labels could be very costly to obtain under realistic and natural language settings. 


% \section{Analysis of PLM for Logical Reasoning}
\subsection{Robustness of PLM as Reasoner}
\label{subsec:robustness_deductive_reasoning}
%~\citet{DBLP:conf/nips/GontierSRP20} analyze small~(\~3M) transformer decoder architecture's systematic generalization ability on a proof generation task.
% They use the decoder to generate both $proof$ and $correctness$ given $theory$ and $hypothesis$.

% \rui{why this task is studied? what's the value of the analysis of PLM?}

The previously introduced methods only focus on solving the deductive reasoning tasks, while it is unclear whether PLMs can be used as robust deductive reasoners.

To investigate the problem, 
\citet{DBLP:conf/ijcai/GaskellMTS22} create a more challenging synthetic dataset on hypothesis classification task in terms of complexity, and test PLM's performance on it. 
% by framing it as a propositional SAT problem.
They find that with large and complex enough training examples, transformers can perform well on the dataset.
% problems that far exceed the complexity of ParaRules benchmark.
In addition, they find that transformers exhibit some degree of generalization and scale-invariance ability;
%
\citet{DBLP:conf/aaai/0001S22} propose an adversarial attack method for synthetic datasets on the hypothesis classification task.
% for the synthetic deductive reasoning task, which can keep the labels of adversarial data instances logically consistent.
They find that transformers are often fooled if the query literally appears within the body of a rule, and transformers struggle to correctly bind variables on either side of a rule;
%
\citet{DBLP:journals/corr/abs-2205-12598} proposed a synthetic deductive reasoning dataset to evaluate the robustness of language models to minimal logical edits in the inputs and different logical equivalence conditions, and find that PLMs are not robust to their proposed logical perturbations.


% propose a synthetic deductive reasoning dataset to test the logical robustness of transformers.
% Overall, they find that LMs such as RoBERTa and T5 finetuned on RobustLR are not sufficient to learn the semantics of the logical operators conjunction, disjunction, and negation.




\section{Inductive Reasoning over Natural Language}
\label{sec:inductive}

\subsection{Existing Task Formulations}
% \input{tables/inductive_datasets.tex}
\input{tables/inductive_datasets.tex}

Existing tasks for inductive reasoning can be summarized as rule classification and rule generation tasks.
Datasets for the tasks are summarized in Table~\ref{tab:inductive_datasets}.
``Generation'' tab with \xmark means it is for the rule classification task.

\paragraph{Rule Classification}
Given a generated $rule$ and $facts$ where the $rule$ is generated from, the task is to classify whether the $rule$ can be accepted. 
The current evaluation aspects are from requirements of both inductive reasoning and natural language.

% \paragraph{Rule Generation (with Manually Selected Facts)}
\paragraph{Rule Generation}
Given multiple manually selected $facts$ with similar patterns, the task is to induce a $rule$ that (1) can entail the $facts$, and (2) is more general than all of the $facts$.
% , if such a rule can be induced with given $facts$.
Here ``more general'' means larger information coverage scope. 
More detailed illustrations can be found in \S\ref{appen:general_inductive_reasoning}.





% \subsection{Datasets}
% also evaluation metrics

% DEER

% DEERLET


\subsection{Methods}

\subsubsection{Rule Classification}

\citet{DBLP:journals/corr/abs-2205-06910} analyze language model's ability to generalize novel property knowledge~(has sesamoid bones) from concept(s)~(robins) to others~(sparrows, canaries).
As illustrated in \S\ref{appen:general_inductive_reasoning}, they analyze the language models' ability to classify a new fact~(but not a rule) as correct or not, given facts.
It could be seen that the correctness of a rule is implicitly predicted by testing multiple facts entailed by the rule.

\citet{Yang2022Language} propose three requirements of rule confirmation from philosophy literature on inductive reasoning and one requirement of rule confirmation from NLP requirement. 
They analyze the LLMs' ability to classify natural language rules from each requirement under both in-context few-shot and finetuning settings.

\subsubsection{Rule Generation}

% \textbf{CoLM}
\citet{Yang2022Language} 
% propose a framework named chain-of-language-models~(CoLM).
% They 
assume that the inductive reasoning task is so difficult that a proper system should contain a rule populator and (multiple) rule verifiers that filter bad rules from different aspects.
Accordingly, they propose a framework named chain-of-language-models~(CoLM).
Specifically, one LM generates $rules$ given $facts$ and a rule template, the other four LMs filter generated rules mainly from inductive reasoning requirements that were selected from philosophy literature.




\section{Abductive Reasoning over Natural Language}
\label{sec:abductive}

\subsection{Existing Task Formulations}
% \zy{we might need to differentiate task and dataset here.}
\input{tables/abduction_datasets.tex}

Existing tasks for abductive reasoning can be summarized as explanation classification, and explanation generation w/o and w/ theory.
Datasets for the tasks are summarized in Table~\ref{tab:abduction_datasets}.
In the table, the ``generation'' tab and ``theory included'' tab can be used to determine the task it is used for.

% \paragraph{$\alpha$NLI}
\paragraph{Explanation Classification}
Given observation $O_{1}$ at time $t_1$, observation $O_2$ at time $t_2$ ($t_2 > t_1$), a plausible hypothesis $h^+$ and a implausible hypothesis $h^-$ that explain $O_1$ and $O_2$, 
% the $\alpha$NLI task
this task
is to select the most plausible hypothesis from $h^+$ and $h^-$. $O_{1}$ and $O_{2}$ each contains a single sentence.


% \paragraph{$\alpha$NLG}
\paragraph{Explanation Generation without Theory}
Given observation $O_{1}$ at time $t_1$, observation $O_2$ at time $t_2$ ($t_2 > t_1$), 
% the $\alpha$NLG task 
this task
is to generate a valid hypothesis $h^+$ given $O_{1}$ and $O_{2}$. $O_{1}$ and $O_{2}$ each is described in a single sentence.


% \paragraph{D*-Ab and AbductionRules}
\paragraph{Explanation Generation with Theory}
Given a theory $C$ and a possible observation $O$ not provable from $C$, the task is to generate a new hypothetical fact $h$ such that $C \cup \{h\} \models O$. 
Here $C$ contains multiple facts and rules, where each fact or rule contains a single sentence. 
$O$ is in single sentence.



% \subsection{Datasets}
% also evaluation metrics
% \zy{maybe mention the evaluation metrics; especially D*-Ab uses F1 and exact match, while AbductionRules uses accuracy as metrics.}
% Abductive reasoning datasets is summarized in Table~\ref{tab:abduction_datasets}.
% The task each dataset is for can be seen from the ``Generation'' and ``Theory included'' tabs.
% Overall from the table it seems that there lacks a dataset that is both realistic and theory-included.









\subsection{Methods}
% Given the large difference between abductive reasoning tasks, we categorize methods, based on their targeting tasks.
% Overall there is no apparent trend among current methods for each abductive reasoning task. 
% Instead, nearly every method focuses on a different aspect.
% Therefore we introduce them chronologically without identifying the potential trends. 
% \zy{not really. For $\alpha$NLI, a common trend is to incorporate knowledge through different ways.}

% \subsubsection{$\alpha$NLI}
\subsubsection{Explanation Classification}
Methods for this task generally introduce knowledge in various ways to improve performance.
%
Specifically,~\citet{DBLP:journals/corr/abs-1909-08855} explore ways to incorporate additional unstructured textual knowledge retrieved from a story corpus through prompt;
%
\citet{DBLP:conf/emnlp/PaulF20} encode and incorporate knowledge from COMET's generation~\citep{DBLP:conf/acl/BosselutRSMCC19} directly into transformer's internal attention;
%
\citet{DBLP:conf/aaai/LourieBBC21} and~\citet{DBLP:conf/starsem/PaulF21} incorporate knowledge by multi-task training;
% on RAINBOW, a benchmark consisting of six commonsense reasoning datasets including the abductive reasoning dataset $\mathcal{ARI}$;
%
\citet{DBLP:conf/acl/DuD0Q20} incorporate knowledge with an additional pre-training stage using $\mathcal{ARI}$ independent story corpora;

In addition to knowledge integration, many different aspects of explanation classification tasks are also investigated.
Specifically,
\citet{DBLP:conf/iclr/BhagavatulaBMSH20} rewrite the objective using Bayes Rule 
% $P(h^i|O_1,O_2) \propto P(O_2|h^i,O_1)P(h^i|O_1)$, 
and formulate a set of probabilistic models that make various independence assumptions on the new objective.
They find that the most sophisticated probabilistic model works the best;
%
\citet{DBLP:conf/sigir/ZhuPLC20} frame this task as a ranking task to also measure the plausibility of hypothesis in addition to discriminating it;
%
\citet{DBLP:conf/starsem/PaulF21} conduct this task in an unsupervised setting 
by pretraining on a counterfactual reasoning dataset, which is related to abductive reasoning.
% that they first train a PLM on a counterfactual reasoning dataset~\citep{DBLP:conf/emnlp/QinBHBCC19} and then use it to generate a possible next event using different choice of $h$ --- $P(O_2^{h_i}|O_1,h_i)$, and then make predictions based on the similarity between $O_2^{h_i}$ and $O_2$;
%
\citet{DBLP:conf/naacl/KadikisSK22} propose a method to select suitable PLMs for this task.
It is based on the cosine similarity of $embed(O_1, O_2)$ and $embed(h_i)$ for each PLM without finetuning.






%~\citet{DBLP:conf/iclr/BhagavatulaBMSH20} rewrites the objective using Bayes Rule conditioned on $O_1$, specifically $P(h^i|O_1,O_2) \propto P(O_2|h^i,O_1)P(h^i|O_1)$. 
% A set of probabilistic models are formulated based on the rewritten objective using various independence assumptions for classification, such as hypothesis only ($P(h^i)$), single observation only ($P(O_2|h^i)$ and $P(h^i|O_1)$), linear chain ($P(O_2|h^i)P(h^i|O_1)$), and fully connected ($P(O_2|h^i)P(h^i|O_1)P(h^i|O_1,O_2)$).
% Every $P$ is initiated with BERT~\citep{DBLP:conf/naacl/DevlinCLT19}.

%~\citet{DBLP:journals/corr/abs-1909-08855} investigate ways to incorporate additional unstructured knowledge to improve QA tasks including $\alpha$NLI. 
% The additional knowledge they adopt for $\alpha$NLI is a story corpus including {\it Story Cloze Test} and {\it ROCStories Corpora}~\citep{DBLP:journals/corr/MostafazadehCHP16}.


%~\citet{DBLP:conf/sigir/ZhuPLC20} is the first to frame $\alpha$NLI as a ranking task to also measure the plausibility of a hypothesis in addition to discriminating it. 
% They propose an approach, named $L2R^2$.
% Specifically, $O_1$ combined with $O_2$ are treated as a query. $h^+$ and $h^-$ are treated as candidate documents. 
% Deep text matching models and a PLM are employed as the scoring functions to get the plausible score for each observation-hypothesis pair.

%~\citet{DBLP:conf/emnlp/PaulF20} propose a multi-head knowledge attention model that encodes and incorporates knowledge directly into transformer's internal attention.
% They adopt COMET's generation~\citep{DBLP:conf/acl/BosselutRSMCC19} as additional knowledge.

%~\citet{DBLP:conf/aaai/LourieBBC21} propose a multi-task benchmark named RAINBOW that consists of six commonsense reasoning datasets including $\mathcal{ARI}$ \rui{[what else?]}.
% It uses multi-task training on RAINBOW, which improves the performance on 8 commonsense benchmarks including $\alpha$NLI \rui{[what else?]}.

%~\citet{DBLP:conf/acl/DuD0Q20} propose a variational autoencoder~\citep{DBLP:conf/nips/SohnLY15}-based framework ege-RoBERTa with an additional pre-training stage, using $\mathcal{ARI}$ independent story corpora, when ege-RoBERTa is able to learn commonsense knowledge from an event graph through the latent variable.

%~\citet{DBLP:conf/starsem/PaulF21} first generate $P(O_2^{h_i}|O_1,h_i)$ and then utilize the similarity between $O_2$ and generated $O_2^{h_i}$ for prediction (unsupervised setting) or multi-task learning (supervised setting).
% They show that this new paradigm outperforms the direct prediction PLM baselines in supervised setting.

%~\citet{DBLP:conf/naacl/KadikisSK22} propose a method to select suitable pretrained models for the $\alpha$NLI task.
% It is based on the cosine similarity of $embed(O_1, O_2)$ and $embed(h_i)$ for each PLM without finetuning.



% \subsubsection{$\alpha$NLG}
\subsubsection{Explanation Generation without Theory}
In general, methods for this task either incorporate knowledge or improve the decoding method to be more suitable for this task.

For knowledge integration,
\citet{DBLP:conf/iclr/BhagavatulaBMSH20} utilize textual knowledge generated from COMET and investigate two ways of knowledge integration --- via texts or via embeddings, and find that the embedding-based method is more effective;
\citet{DBLP:conf/emnlp/JiKHWZH20} leverage structural knowledge from ConceptNet~\citep{speer2017conceptnet} for this task.

For improving decoding method, 
\citet{DBLP:conf/emnlp/QinSWBHBBC20} are motivated by the fact that the target $h^+$ to generate happens before $O_2$. 
They accordingly propose an unsupervised decoding algorithm that can incorporate both past and future contexts.


%~\citet{DBLP:conf/iclr/BhagavatulaBMSH20} frame $\alpha$NLG as a knowledge-enhanced text generation problem.
% Specifically, they model $P(h|O_1,O_2,K)$, where $K$ is additional knowledge. 
% They choose COMET as the source of additional knowledge and investigate two ways of knowledge integration --- via texts or via embeddings, and find that the embedding-based method is more effective for this task.

%~\citet{DBLP:conf/emnlp/QinSWBHBBC20} are motivated by the fact that the target $h^+$ to generate happens before $O_2$. 
% They accordingly propose a new unsupervised decoding algorithm that incorporates both the past and future contexts using off-the-shelf, left-to-right language models.

% To attend to both semantic and structural information of a knowledge graph,~\citet{DBLP:conf/emnlp/JiKHWZH20} frame $\alpha$NLG as a knowledge graph enhanced text generation problem, specifically as $P(h|O_1,O_2,G)$, where $G$ is an extracted sub-graphs from ConceptNet~\citep{speer2017conceptnet}. 
% They propose a model called GRF to incorporate graph knowledge.


% \subsubsection{D*-Ab and AbductionRules}
\subsubsection{Explanation Generation with Theory}
\citet{DBLP:conf/acl/TafjordDC21} explore the ability of a finetuned T5-11B~\citep{raffel2020exploring} on $P(h|C,O)$. 
% using D*-Ab dataset. 
Their results indicate that finetuned T5-11B can reach a high test accuracy of 93\% on D*-Ab.


% \subsection{Findings}




% \section{Challenges in NLP $\&$ Logical Reasoning}
% \section{Challenges in NLP $\&$ Logical Reasoning}
\section{Challenges of LRNL}
\label{sec:challenges}

% In this section we try to list some challenges on the development of modular-based methods for logical reasoning.

\paragraph{Computationally Efficient Reasoner}
% For deductive reasoning, inductive reasoning, and abductive reasoning

Many tasks in logical reasoning over formal language have very high algorithmic complexity~\citep{DBLP:journals/ml/MuggletonRPBFIS12}.
Thanks to the low computational cost of each deduction step over formal language, such complex tasks could be possible.
However, each deduction step in LRNL typically costs one inference of a LLM, which makes tasks with high algorithmic complexity nearly prohibitive. 



% \subsection{Generalize Learned Reasoning Ability to Unseen Scenarios}
\paragraph{Robust Deductive Reasoner}
% For deductive reasoning

Symbolic deductive reasoners are not restricted to train data distributions, while neural deductive reasoners are restricted to their training data~\citep{DBLP:conf/nips/GontierSRP20,DBLP:conf/aaai/0001S22};
In addition, neural deductive reasoners are also vulnerable to adversarial attacks~\citep{DBLP:conf/ijcai/GaskellMTS22}, while symbolic reasoners are robust to the attacks.
%
The lack of robustness can lead to restricted application domains and incorrect deductive inferences.



\paragraph{Reliable Rule Generation}
% For inductive reasoning

Currently, the rule generation method in inductive reasoning relies on out-of-box LLMs, since a finetuned rule generation model could be restricted in a domain.
The annotation of an inductive reasoning dataset should only be done by experts and is very time consuming~\citep{Yang2022Language}.
Given the two restrictions, how to improve the quality of generated rules given related facts could be a challenging open problem.



\paragraph{Reliable Explanation Generation}
% For abductive reasoning

Abduction is a form of non-monotonic reasoning~\citep{DBLP:journals/air/Paul93}, and potentially has a large search space of conclusions given premises.
Therefore, how to generate more~(all) reasonable explanations can be challenging~\citep{DBLP:conf/iclr/BhagavatulaBMSH20}. 
% \rui{[why explanation is important in this context?]}




% \paragraph{Validness of Reasoning Step}
\paragraph{Reliable Verifier on Reasoning Steps}
% For deductive reasoning, inductive reasoning, and abductive reasoning

Many state-of-the-art methods on deductive reasoning~\citep{DBLP:journals/corr/abs-2205-12443,DBLP:journals/corr/abs-2210-12217,DBLP:journals/corr/abs-2211-00614} and inductive reasoning~\citep{Yang2022Language} use verifier to check the correctness of generated reasoning results.
However, the current verifiers only reflect the internal beliefs of PLMs. 
It is doubtful whether PLMs have obtained the knowledge for verification. 




\paragraph{Better Automatic Evaluation Metrics}
% For (generative) deductive reasoning, inductive reasoning, and abductive reasoning

It is generally difficult to automatically evaluate generative reasoning implications, especially with realistic and not synthetic datasets.
The difficulty mainly lies in that the same semantic meaning can be expressed with diversified forms, and that different conclusions might be all acceptable~(especially in abductive and inductive reasoning). 
This may lead to biased evaluation when using automatic metrics.



\paragraph{Building Larger Benchmarks}
For complicated reasoning tasks especially in realistic and natural language settings, usually experts are needed for annotation, and the process is very time consuming~\citep{DBLP:conf/emnlp/DalviJTXSPC21,DBLP:journals/corr/abs-2211-00614,Yang2022Language}.
Therefore it can be challenging to construct significantly larger benchmarks.




\paragraph{Understanding the Internal Mechanism of PLMs for Reasoning}
% For deductive reasoning, inductive reasoning, and abductive reasoning

Until now research works only focus on investigating whether the input/output behaviors of PLMs can be used to simulate a reasoner~\citep{DBLP:conf/ijcai/ClarkTR20} or complete reasoning tasks.
However, it is still a challenging open research question to understand the internal mechanism of PLMs for reasoning.




\paragraph{More Impacts on (NLP) Applications}
% For deductive reasoning, inductive reasoning, and abductive reasoning
% advantages: interpretable, not rigid, follow the given theory base (follow the rules) --> medical diagnosis, financial forecasting, legal nlp tasks...

% Natural language based logical reasoning
As illustrated in \S\ref{sec:advantages_of_new_paradigm}, 
overall LRNL can be seen as a new type of neuro-symbolic method, which
takes the advantages from both the symbolic and sub-symbolic aspects, and can systematically alleviate many main challenges of both symbolic and sub-symbolic methods.
% , such as being interpretable, less catastrophic forgetting, more controllable,
% ~(following the given knowledge base), 
% less brittle, being able to learn and generalize from data examples, adapting to raw data (web natural language corpus), capable of dealing with mislabeled and ambiguous data... 
% \rui{[break the sentence up]}
%
% overall a LRNL system has overcome several main disadvantages of the past symbolic-based expert systems, and also possesses some advantages over current sub-symbolic methods.
These characteristics make a LRNL system possible (but might still challenging at the beginning) to deal with many (NLP) applications such as medical diagnosis and legal NLP tasks,
%
since many medical and legal problems could be seen as pure logical reasoning problems with very large rule base~(e.g., medical knowledge and laws correspondingly). 





% Future Directions
\section{Desirable Tasks and Methods in the Future}
\label{sec:desirable_future}

\paragraph{Probabilistic Inference}
% For both classification and generation tasks.

In reality, pure deductive reasoning has not always been used. 
When people include ``likely'' in their expressions, uncertainty is introduced, which makes the reasoning process probabilistic;
In addition, inductive reasoning and abductive reasoning are by default non-monotonic reasoning.
This uncertainty aspect has not been focused in current research. 
It is probably beneficial to learn from how symbolic reasoning handles uncertainty~\citep{halpern2017reasoning}.



\paragraph{Reasoning with Incomplete Information}

The current proof generation task requires all necessary premises provided to create a proof tree.
Only one work~\citep{DBLP:journals/corr/abs-2211-00614} focuses on proof generation with the incomplete information task. 
However, the task they adopt 
% still needs all premises but only missing 
only overlooks
one premise, while in reality more might be missing.



\paragraph{Inductive Reasoning on Web Corpora}

Currently, the dataset for rule generation tasks in inductive reasoning provides manually selected facts~\citep{Yang2022Language}.
However, to best leverage a system's ability in handling natural language, it should be able to work on raw web corpora to induce rules, which leads to a more challenging task of inductive reasoning on web corpora.


\paragraph{Abductive Reasoning with (Long) Theory}

Many tasks such as medical diagnosis conduct abductive reasoning with a long theory~(e.g., medical knowledge).
However, current abductive reasoning research only covers abductive commonsense reasoning~\citep{DBLP:conf/iclr/BhagavatulaBMSH20} without given theory, or only given short, synthetic, not realistic knowledge as theory~\citep{DBLP:conf/acl/TafjordDC21}. 



\paragraph{Interactions between Reasoning Types}

Multiple reasoning types can be used together for complex tasks.
Existing works only utilize deductive reasoning with abductive reasoning to create a proof tree~\citep{DBLP:conf/naacl/HongZYZ22,DBLP:journals/corr/abs-2211-00614}.
However, many other collaborations are possible, such as using inductive reasoning to collect a (large) rule base, which is to be used as the theory base for deductive reasoning.



\section{Conclusion}
\label{sec:conclusion}
In this paper, 
we propose a new concept, logical reasoning over natural language as knowledge representation~(LRNL), and provide a detailed and up-to-date review of LRNL.
Moreover,
we have introduced the philosophical foundations, advantages of LRNL, benchmarks and methods, challenges, desirable tasks \& methods, and the relation of LRNL to related NLP fields~(\S\ref{appen:relation_to_related_fields}).


\section{Limitations}
% This survey paper does not summarize the experiment results of the listed benchmarks, and does not involve any new experiment results.
The scope of this survey paper does not cover end-to-end out-of-box LLMs' prompting techniques such as chain-of-thought~\citep{DBLP:journals/corr/abs-2201-11903} for reasoning.
However, chain-of-thought methods do not specifically focus on any specific reasoning type of logical reasoning -- including deductive reasoning, inductive reasoning, and abduction reasoning.
Instead, they focus on mathematical reasoning and commonsense reasoning.
We also discuss the difference of mathematical and commonsense reasoning from logical reasoning in \S\ref{appen:relation_to_related_fields}.
More discussions on the difference between chain-of-thought and papers reviewed in this paper can be found in \S\ref{subsec:chain-of-thoughts}.


% \section{Ethics Statement}
% This article follows the ACL Code of Ethics. 
% To our best knowledge, there are no foreseeable potential risks for this survey.


% \section*{Acknowledgements}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\label{sec:appendix}


\subsection{Relation to Related (NLP) Fields}
\label{appen:relation_to_related_fields}
In this section, we first introduce related NLP fields to general logical reasoning, then introduce fields that are only related to deductive reasoning, inductive reasoning, or abductive reasoning. 
We hope that this section could be helpful to form a clear shape of LRNL in NLP.

\subsubsection{Logical Reasoning}
\paragraph{Neuro-Symbolic Computing}
Neural-symbolic computing~(NeSy) is a hybrid of symbolism and connectionism to exploit advantages from both sides~\citep{wang2022towards,DBLP:conf/lrec/CambriaLDXK22}.
The knowledge representation of its symbolic part basically is a knowledge graph or propositional logic or first-order logic~\citep{wang2022towards}.
%
LRNL could be seen as a new type of NeSy in addition to the existing 6 types summarized by ~\citet{kautz2022third}, as its goal and design of methodology are typically symbolic~(logical reasoning with knowledge bases), while avoiding any symbolic representation, using~(currently pure) neural methods.

% Logical reasoning over natural language could be seen as a new special type of NeSy in addition to the summarized six types~\citep{kautz2022third}, as its goal is typically symbolic~(logical reasoning), while using~(currently pure) neural methods and natural language as knowledge representation.

\paragraph{Natural Language Inference}
Natural language inference~(NLI) is generally considered as the semantic concepts of entailment and contradiction~\citep{DBLP:conf/emnlp/BowmanAPM15}.
Here logical reasoning tasks can be viewed as special types of NLI focusing on particular reasoning aspects. 

\paragraph{Question Answering}
The form of LRNL looks similar to question answering~(QA), however, QA is conducting one-step logical reasoning only when the context provides enough information to answer the question~(deductive reasoning), or the answer is a generalization of an argument in context or question~(inductive reasoning), or the answer is to provide explanations to the question~(abductive reasoning).


\paragraph{Commonsense Reasoning}
Commonsense reasoning~(CR) and logical reasoning~(LR) are similar in that they both involve ``knowledge'' and ``reasoning''.
Compared to LR, CR focuses more on the ``knowledge'' aspect.
Some typical tasks include whether a system has commonsense knowledge~\citep{DBLP:conf/acl/BosselutRSMCC19,DBLP:conf/emnlp/YangDRC20}, and whether a system's answer is commonsense-knowledge-aware~\citep{DBLP:conf/aaai/BiskZLGC20};
LR focuses more on the ``reasoning'' aspect, e.g., whether a system's i/o behaviors follow reasoning requirements~\citep{DBLP:conf/ijcai/ClarkTR20}.



% \paragraph{Expert Systems}


\subsubsection{Deductive Reasoning}
\label{append:relation_deductive_reasoning}

\paragraph{Chain of Thoughts}
\label{subsec:chain-of-thoughts}
Chain of thoughts~(COT)~\citep{DBLP:journals/corr/abs-2201-11903} is a prompting technique that can elicit the step-by-step reasoning ability of LLMs without finetuning.
The form of COT is very similar to deductive reasoning, whereas it is argued that COT's reasoning is not casual~\citep{DBLP:journals/corr/abs-2205-09712}, limited by input size~\citep{DBLP:conf/naacl/Ribeiro0MDWZCXH22}, and contains unrelated or incorrect steps~\citep{DBLP:conf/naacl/HongZYZ22,DBLP:journals/corr/abs-2210-12217}.
Overall, COT-related methods could be a promising strategy for deductive reasoning, but in this paper, we mainly survey the finetuned methods that use smaller PLMs for deductive reasoning.


\paragraph{Multi-hop Reasoning}
%~\citet{DBLP:journals/corr/abs-2204-09140} 
Compared to proof generation, many multi-hop reasoning tasks~\citep{DBLP:conf/emnlp/Yang0ZBCSM18,DBLP:conf/emnlp/JiangBZD0B20,DBLP:conf/acl/MinZZH19,DBLP:conf/emnlp/SinhaSDPH19} are much simpler, often being single-branched~\citep{DBLP:conf/naacl/QuCGDX22}, consisting of only 2-3 supporting facts, and are more coarse-grained, involving large chunks of texts such as passages instead of simple, short sentences~\citep{DBLP:journals/corr/abs-2205-12443}. 

Nevertheless, some multi-hop reasoning datasets can be considerd as conducting deductive reasoning.
For instance, for each data in CLUTRR~\citep{DBLP:conf/emnlp/SinhaSDPH19} dataset, a set of facts that can make conclusive support to the target kinship relation is included in background information as input for each target relation, hence from the philosophical definition~\citep{salmon1989introduction}, it requires to perform deductive reasoning.


% \paragraph{Question Answering}
% The main difference between hypothesis generation task and question answering~(QA) is that here the $theory$ usually consists of knowledge that can be utilized by deductive reasoning
% % ~(usually needs multiple reasoning steps) 
% to conclusively determine an $correctness$ for a $hypothesis$, while in the other QA tasks, the context . 


\paragraph{Mathematical Reasoning}
In many mathematical reasoning tasks such as math word problem solving~\citep{DBLP:journals/tacl/Koncel-Kedziorski15} and geometry problem solving~\citep{DBLP:conf/emnlp/SeoHFEM15},
the conclusion can be conclusively entailed by the premise.
Therefore these tasks belong to deductive reasoning.
We do not review math-related papers because we want to focus solely on the challenge of deductive reasoning while mathematical reasoning involves numbers in the text, which introduces additional challenges.



\subsubsection{Inductive Reasoning}

\paragraph{Information Extraction}
Information Extraction~(IE) is a task of extracting pre-specified types of facts from written texts or speech transcripts, and converting them into structured representations~\citep{DBLP:reference/db/Ji18}.
The rule generation task here also extracts rules from facts represented in written texts.
The difference is that IE pursues extracting the exact information from existing texts, while inductive reasoning aspires to induce more general rules from existing texts, where the information in rules goes beyond what is exactly stated in the texts.

\paragraph{Case-based Reasoning}
Case-based Reasoning~(CBR) is a classic AI subject, whose methods share a general methodology of four steps: retrieve, reuse, revise, and retain~\citep{aamodt1994case}.
Recently there has been research works devoting to bridge the research of CBR and NLP, by using NLP techniques for CBR challenges~\cite{yang2023cbr} and improving NLP tasks with CBR methodologies~\cite{DBLP:conf/emnlp/DasZTGPLTPM21,DBLP:conf/icml/DasGNTZHJM22,yang2023cbr}.
CBR could be seen as a type of analogical reasoning~\citep{kolodner1997educational}, and analogical reasoning belongs to inductive reasoning~\citep{salmon1989introduction}.
However, CBR is a different inductive reasoning type than the ``generalization'' process~(from facts to rules) described in \citet{flach2000abductive}, but more on the general description on inductive reasoning~\citep{salmon1989introduction} that premises cannot provide conclusive support to the conclusion.


\subsubsection{Abductive Reasoning}

% Q: maybe rewrite here as relation to NLP fields.
\paragraph{Casual Reasoning}
In logic research, causal reasoning aims at an epistemological problem of establishing precise causal relationships between causes and effects. 
It is generally considered a form of inductive reasoning~\citep{goertzel2011real}, since inductive reasoning is to derive rules that lead from one to another.
When the focus is to derive possible causes from effects, the problem belongs to abductive reasoning~\citep{goertzel2011real}.

%~\citet{feder2022causal} provides a survey on causal inference \& NLP. 

% \subsubsection{Visual Abductive Reasoning}
%~\citep{DBLP:conf/cvpr/LiangWZY22}
%~\citep{DBLP:conf/eccv/HesselHPZBRSC22}



\subsection{Meaning of ``More General'' Required by Inductive Reasoning}
\label{appen:general_inductive_reasoning}

This section is collected from \citet{Yang2022Language}'s appendix, to help illustrate inductive reasoning.

Given an argument consisting of a premise and a conclusion, if the conclusion involves new information that is not covered by the premise and can not be conclusively entailed by the premise, the argument is an inductive argument~\citep{salmon1989introduction}.

When the conclusion has a larger scope of information coverage than the premise, and can entail the premise, it can be said that the conclusion is ``more general'' to the premise~\citep{Yang2022Language}. 
In this case, we termed the premise as a ``fact'', and the conclusion as a ``rule'';
When the conclusion contains new pieces of information and cannot entail the premise, as defined by~\citet{salmon1989introduction}, the argument is still an inductive argument.
But in this case, we termed the premise as a ``fact'', and the conclusion as another ``fact''.

For instance, if facts that are about cats and dogs are good accompaniment of humans, then some examples of a ``more general'' rule can be (1) mammals are good accompaniment of humans, or (2) domesticated animals are good accompaniment of humans, or (3) animals with four legs are good accompaniment of human.

In these examples, the rules cover a larger scope than the facts~(e.g., mammals compared to cats; domesticated animals compared to cats), and therefore the rules are ``more general'' than the facts.

``More general'' means not only about finding higher taxonomic rank, but can be in unlimited forms.
For instance, if the fact is about the Sun rises and falls every day, then some examples of a ``more general'' rule can be (1) the Earth is the king of the universe or (2) the Earth is rotating itself.

Both rule examples are ``more general'' than the given fact, since the rule can entail not only the given fact, but also other not mentioned facts such as the observable movements of the other stars in the Milky Way.


\end{document}
