
@article{weissteiner2023BOCA,
		 title={Bayesian Optimization-based Combinatorial Assignment},
		 volume={37},
		 abstractNote={We study the combinatorial assignment domain, which includes combinatorial auctions and course allocation. The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning-based preference elicitation algorithms that aim to elicit only the most important information from agents. However, the main shortcoming of this prior work is that it does not model a mechanism's uncertainty over values for not yet elicited bundles. In this paper, we address this shortcoming by presenting a Bayesian Optimization-based Combinatorial Assignment (BOCA) mechanism. Our key technical contribution is to integrate a method for capturing model uncertainty into an iterative combinatorial auction mechanism. Concretely, we design a new method for estimating an upper uncertainty bound that can be used to define an acquisition function to determine the next query to the agents. This enables the mechanism to properly explore (and not just exploit) the bundle space during its preference elicitation phase. We run computational experiments in several spectrum auction domains to evaluate BOCA's performance. Our results show that BOCA achieves higher allocative efficiency than state-of-the-art approaches.},
		 journal={Proceedings of the AAAI Conference on Artificial Intelligence},
		 author={Weissteiner, Jakob and Heiss, Jakob and Siems, Julien and Seuken, Sven},
		 year={2023},
      url = {https://arxiv.org/abs/2208.14698},
  }

@article{Cuchiero_2020,
	doi = {10.3390/risks8040101},
  
	url = {https://doi.org/10.3390\%2Frisks8040101},
  
	year = 2020,
	month = {sep},
  
	publisher = {{MDPI} {AG}
},
  
	volume = {8},
  
	number = {4},
  
	pages = {101},
  
	author = {Christa Cuchiero and Wahid Khosrawi and Josef Teichmann},
  
	title = {A Generative Adversarial Network Approach to Calibration of Local Stochastic Volatility Models},
  
	journal = {Risks}
}

@misc{ImplicitBiasRadonTransform,
  doi = {10.48550/ARXIV.2006.07356},
  
  url = {https://arxiv.org/abs/2006.07356},
  
  author = {Jin, Hui and Montúfar, Guido},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.6; G.3, 68Q32, 68T05},
  
  title = {Implicit Bias of Gradient Descent for Mean Squared Error Regression with Two-Layer Wide Neural Networks},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{herrera2021optimal,
  title={Optimal stopping via randomized neural networks},
  author={Herrera, Calypso and Krach, Florian and Ruyssen, Pierre and Teichmann, Josef},
  journal={arXiv preprint arXiv:2104.13669},
  year={2021}
}


@article{rahimi2007random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}
@article{CAO2018278,
title = {A review on neural networks with random weights},
journal = {Neurocomputing},
volume = {275},
pages = {278-287},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217314613},
author = {Weipeng Cao and Xizhao Wang and Zhong Ming and Jinzhu Gao},
keywords = {Feed-forward neural networks, Training mechanism, Neural networks with random weights},
abstract = {In big data fields, with increasing computing capability, artificial neural networks have shown great strength in solving data classification and regression problems. The traditional training of neural networks depends generally on the error back propagation method to iteratively tune all the parameters. When the number of hidden layers increases, this kind of training has many problems such as slow convergence, time consuming, and local minima. To avoid these problems, neural networks with random weights (NNRW) are proposed in which the weights between the hidden layer and input layer are randomly selected and the weights between the output layer and hidden layer are obtained analytically. Researchers have shown that NNRW has much lower training complexity in comparison with the traditional training of feed-forward neural networks. This paper objectively reviews the advantages and disadvantages of NNRW model, tries to reveal the essence of NNRW, gives our comments and remarks on NNRW, and provides some useful guidelines for users to choose a mechanism to train a feed-forward neural network.}
}



@article{Arendt_2018,
	doi = {10.4064/sm8757-4-2017},
	url = {https://doi.org/10.4064\%2Fsm8757-4-2017},
	year = 2018,
	publisher = {Institute of Mathematics, Polish Academy of Sciences},
	volume = {240},
	number = {3},
	pages = {275--299},
	author = {Wolfgang Arendt and Marcel Kreuter},
	title = {Mapping theorems for Sobolev spaces of vector-valued functions},
	journal = {Studia Mathematica}
}

@phdthesis{kreuter2019vector,
  title={Vector-valued elliptic boundary value problems on rough domains},
  author={Kreuter, Marcel},
  year={2019},
  school={Universit{\"a}t Ulm}
}
@article{implRegPart3V4,
  author    = {Jakob Heiss and
               Josef Teichmann and
               Hanna Wutte},
  title     = {Infinite wide (finite depth) Neural Networks benefit from multi-task
               learning unlike shallow Gaussian Processes - an exact quantitative
               macroscopic characterization},
  journal   = {CoRR},
  volume    = {abs/2112.15577},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.15577},
  eprinttype = {arXiv},
  eprint    = {2112.15577},
  timestamp = {Sat, 09 Apr 2022 12:27:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-15577.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{MomentumGFImplReg,
  doi = {10.48550/ARXIV.2201.05405},
  
  url = {https://arxiv.org/abs/2201.05405},
  
  author = {Wang, Li and Zhou, Yingcong and Fu, Zhiguo},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Implicit Regularization of Momentum Gradient Descent with Early Stopping},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@InProceedings{pmlr-v119-jacot20a,
  title = 	 {Implicit Regularization of Random Feature Models},
  author =       {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clement and Gabriel, Franck},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4631--4640},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/jacot20a/jacot20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/jacot20a.html},
  abstract = 	 {Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with $P$ features, $N$ data points, and a ridge $\lambda$, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an \emph{effective ridge} $\tilde{\lambda}$. We show that $\tilde{\lambda} &gt; \lambda$ and $\tilde{\lambda} \searrow \lambda$ monotonically as $P$ grows, thus revealing the \emph{implicit regularization effect} of finite RF sampling. We then compare the risk (i.e. test error) of the $\tilde{\lambda}$-KRR predictor with the average risk of the $\lambda$-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically find an extremely good agreement between the test errors of the average $\lambda$-RF predictor and $\tilde{\lambda}$-KRR predictor.}
}



@inproceedings{NEURIPS2020NeyshaburTransferLearning,
 author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {512--523},
 publisher = {Curran Associates, Inc.},
 title = {What is being transferred in transfer learning? },
 url = {https://proceedings.neurips.cc/paper/2020/file/0607f4c705595b911a4f3e7a127b44e0-Paper.pdf},
 volume = {33},
 year = {2020}
}



@misc{QuestAdaptivityBlogPostBach,
title = {{The quest for adaptivity – Machine Learning Research Blog}},
author = {Bach, Francis},
url = {https://francisbach.com/quest-for-adaptivity/},
urldate = {2022-06-08}
}


@article{Bach2017BreakingCurseDimensionalityConvexNN,
abstract = {We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non-convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
author = {Bach, Francis},
file = {:scratch/users/jheiss/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach - 2017 - Breaking the Curse of Dimensionality with Convex Neural Networks.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Neural networks,convex optimization,convex relaxation,non-parametric estimation},
pages = {1--53},
title = {{Breaking the Curse of Dimensionality with Convex Neural Networks}},
url = {http://jmlr.org/papers/v18/14-546.html.},
volume = {18},
year = {2017}
}



@book{leoni2017firstSobolevBook,
  title={A first course in Sobolev spaces},
  author={Leoni, Giovanni},
  year={2017},
  publisher={American Mathematical Soc.},
  url = {https://bookstore.ams.org/gsm-181/}
}


@InProceedings{YangFeatureLearningInfiniteWidth,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21c.html},
  abstract = 	 {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.}
}



@inproceedings{aitchison2020statisticalTheoryColdPosterior,
  title={A statistical theory of cold posteriors in deep neural networks},
  author={Aitchison, Laurence},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=Rd138pWXMvG}
}

@misc{wenzel2020ColdPosterior,
      title={How Good is the Bayes Posterior in Deep Neural Networks Really?}, 
      author={Florian Wenzel and Kevin Roth and Bastiaan S. Veeling and Jakub Swiatkowski and Linh Tran and Stephan Mandt and Jasper Snoek and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},
      year={2020},
      eprint={2002.02405},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2002.02405}
}

@misc{ruder2017overviewMultiTask,
      title={An Overview of Multi-Task Learning in Deep Neural Networks}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1706.05098},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/pdf/1706.05098.pdf}
}

@misc{tran2021facebook,
      title={Facebook AI WMT21 News Translation Task Submission}, 
      author={Chau Tran and Shruti Bhosale and James Cross and Philipp Koehn and Sergey Edunov and Angela Fan},
      year={2021},
      eprint={2108.03265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.03265},
      note={Summarizing blog post: \url{https://ai.facebook.com/blog/the-first-ever-multilingual-model-to-win-wmt-beating-out-bilingual-models}}
}

@misc{fifty2021efficiently,
      title={Efficiently Identifying Task Groupings for Multi-Task Learning}, 
      author={Christopher Fifty and Ehsan Amid and Zhe Zhao and Tianhe Yu and Rohan Anil and Chelsea Finn},
      year={2021},
      eprint={2109.04617},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2109.04617},
      note={Summarizing blog post: \url{https://ai.googleblog.com/2021/10/deciding-which-tasks-should-train.html}}
}


@misc{aribandi2021ext5ExtremeMultiTask,
      title={ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning}, 
      author={Vamsi Aribandi and Yi Tay and Tal Schuster and Jinfeng Rao and Huaixiu Steven Zheng and Sanket Vaibhav Mehta and Honglei Zhuang and Vinh Q. Tran and Dara Bahri and Jianmo Ni and Jai Gupta and Kai Hui and Sebastian Ruder and Donald Metzler},
      year={2021},
      eprint={2111.10952},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      month = 11,
      url = {https://arxiv.org/abs/2111.10952}
}


@misc{lee2018deep,
      title={Deep Neural Networks as Gaussian Processes}, 
      author={Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
      year={2018},
      eprint={1711.00165},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1711.00165}
}

@InProceedings{Chizat2020ImplicitBias,
  title = 	 {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
  author =       {Chizat, L\'ena\"ic  and Bach, Francis},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {1305--1338},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/chizat20a.html},
  abstract = 	 { Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.}
}


@article{MultitaskCaruana1997,
   abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
   author = {Rich Caruana and Lorien Pratt and Sebastian Thrun},
   doi = {10.1023/A:1007379606734},
   issn = {1573-0565},
   issue = {1},
   journal = {Machine Learning 1997 28:1},
   keywords = {Artificial Intelligence,Control,Mechatronics,Natural Language Processing (NLP),Parallel transfer,Robotics,Simulation and Modeling,Supervised learning},
   pages = {41-75},
   publisher = {Springer},
   title = {Multitask Learning},
   volume = {28},
   url = {https://link.springer.com/article/10.1023/A:1007379606734},
   year = {1997},
}


@article{CameronMartin10.2307/1969276,
 ISSN = {0003486X},
 URL = {http://www.jstor.org/stable/1969276},
 author = {R. H. Cameron and W. T. Martin},
 journal = {Annals of Mathematics},
 number = {2},
 pages = {386--396},
 publisher = {Annals of Mathematics},
 title = {Transformations of Weiner Integrals Under Translations},
 volume = {45},
 year = {1944}
}


@article{Gambara2020ConsistentRM,
  title={Consistent Recalibration Models and Deep Calibration},
  author={Matteo Gambara and Josef Teichmann},
  journal={arXiv: Computational Finance},
  year={2020},
  url={https://arxiv.org/abs/2006.09455}
}

@book{Neal1996,
   author = {Radford M. Neal},
   city = {New York, NY},
   doi = {10.1007/978-1-4612-0745-0},
   isbn = {978-1-4612-0745-0},
   publisher = {Springer New York},
   title = {Bayesian Learning for Neural Networks},
   volume = {118},
   url = {http://link.springer.com/10.1007/978-1-4612-0745-0},
   year = {1996},
}


@misc{roberts2021principles,
      title={The Principles of Deep Learning Theory}, 
      author={Daniel A. Roberts and Sho Yaida and Boris Hanin},
      year={2021},
      eprint={2106.10165},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.10165}
}

@misc{YaidaBlog2021FirstPrinciples,
title={Advancing AI theory with a first-principles understanding of deep neural networks},
author={Sho Yaida},
year={2021},
month=Jun,
day=18,
url={https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/}
}

@misc{ambrosio2012sobolev,
      title={Sobolev spaces in metric measure spaces: reflexivity and lower semicontinuity of slope}, 
      author={Luigi Ambrosio and Maria Colombo and Simone Di Marino},
      year={2012},
      eprint={1212.3779},
      archivePrefix={arXiv},
      primaryClass={math.AP},
      url={https://arxiv.org/abs/1212.3779}
}

@misc{
heiss2021reducing,
title={Reducing the number of neurons of Deep {ReLU} Networks based on the current theory of Regularization},
author={Jakob Heiss and Alexis Stockinger and Josef Teichmann},
year={2021},
url={https://openreview.net/forum?id=9GUTgHZgKCH}
}

@InProceedings{NOMUICML,
      title={{NOMU}: Neural Optimization-based Model Uncertainty}, 
      author={Jakob Heiss and Jakob Weissteiner and Hanna Wutte and Sven Seuken and Josef Teichmann},
      year={2022},
      booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
      series = 	 {Proceedings of Machine Learning Research},
      publisher =    {PMLR},
      eprint={2102.13640},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.13640}
}

@misc{NOMUarxiv,
      title={{NOMU}: Neural Optimization-based Model Uncertainty}, 
      author={Jakob Heiss and Jakob Weissteiner and Hanna Wutte and Sven Seuken and Josef Teichmann},
      year={2021},
      eprint={2102.13640},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.13640}
}

@article{LeCun2015,
	risfield_0_da = {2015/05/01},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	doi = {10.1038/nature14539},
	issn = {1476-4687},
	journal = {Nature},
	number = {7553},
	pages = {436–444},
	title = {Deep learning},
	volume = {521},
	year = {2015},
	url={https://www.nature.com/articles/nature14539}
}

@article{bronstein2017geometric,
  title={Geometric deep learning: going beyond euclidean data},
  author={Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={18--42},
  year={2017},
  publisher={IEEE}
}


@book {MR2467621,
    AUTHOR = {Kuczma, Marek},
     TITLE = {An introduction to the theory of functional equations and
              inequalities},
   EDITION = {Second},
      NOTE = {Cauchy's equation and Jensen's inequality,
              Edited and with a preface by Attila Gil\'{a}nyi},
 PUBLISHER = {Birkh\"{a}user Verlag, Basel},
      YEAR = {2009},
     PAGES = {xiv+595},
      ISBN = {978-3-7643-8748-8},
   MRCLASS = {39-02 (39Bxx)},
  MRNUMBER = {2467621},
       DOI = {10.1007/978-3-7643-8749-5},
       URL = {https://doi.org/10.1007/978-3-7643-8749-5},
}
	


@inproceedings{
anonymous2021reducing,
title={Reducing the number of neurons of Deep ReLU Networks based on the current theory of Regularization},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=9GUTgHZgKCH},
note={under review}
}

@misc{only_MLP,
      title={MLP-Mixer: An all-MLP Architecture for Vision}, 
      author={Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
      year={2021},
      eprint={2105.01601},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2105.01601}
}
@article{ViT,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020},
  url={https://arxiv.org/abs/2010.11929}
}
@article{clippaper,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021},
  url={https://arxiv.org/abs/2103.00020}
}

@article{gale2020sparse,
  title={Sparse GPU Kernels for Deep Learning},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  journal={arXiv preprint arXiv:2006.10901},
  year={2020},
  url={https://arxiv.org/abs/2006.10901}
}

@article{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1902.05040},
  year={2019},
  URL={https://arxiv.org/abs/1902.05040}
}

@inproceedings{williams2019gradient,
  title={Gradient dynamics of shallow univariate relu networks},
  author={Williams, Francis and Trager, Matthew and Panozzo, Daniele and Silva, Claudio and Zorin, Denis and Bruna, Joan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8378--8387},
  year={2019},
  URL={http://papers.nips.cc/paper/9046-gradient-dynamics-of-shallow-univariate-relu-networks.pdf}
}

@article{ongie2019function,
  title={A function space view of bounded norm infinite width relu nets: The multivariate case},
  author={Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1910.01635},
  year={2019},
  URL={https://arxiv.org/pdf/1910.01635.pdf}
}

@InProceedings{Rossetl1finite10.1007/978-3-540-72927-3_39,
author="Rosset, Saharon
and Swirszcz, Grzegorz
and Srebro, Nathan
and Zhu, Ji",
editor="Bshouty, Nader H.
and Gentile, Claudio",
title={{$\ell_1$} Regularization in Infinite Dimensional Feature Spaces},
booktitle="Learning Theory",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="544--558",
abstract="In this paper we discuss the problem of fitting {$\ell_1$} regularized prediction models in infinite (possibly non-countable) dimensional feature spaces. Our main contributions are: a. Deriving a generalization of {$\ell_1$} regularization based on measures which can be applied in non-countable feature spaces; b. Proving that the sparsity property of {$\ell_1$} regularization is maintained in infinite dimensions; c. Devising a path-following algorithm that can generate the set of regularized solutions in ``nice'' feature spaces; and d. Presenting an example of penalized spline models where this path following algorithm is computationally feasible, and gives encouraging empirical results.",
isbn="978-3-540-72927-3",
url="https://link.springer.com/chapter/10.1007/978-3-540-72927-3_39"
}


@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014},
  url={https://arxiv.org/abs/1412.6614}
}

@article{kawaguchi2017generalization,
  title={Generalization in deep learning},
  author={Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.05468},
  year={2017},
  url={https://arxiv.org/abs/1710.05468}
}

@inproceedings{OBD,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990},
  url={http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

@inproceedings{OBS,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  booktitle={Advances in neural information processing systems},
  pages={164--171},
  year={1993},
  url={http://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf}
}

@inproceedings{recentPruning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015},
  url={http://papers.nips.cc/paper/5784-learning-both-weights-andconnections-}
}

@article{lotteryTicket,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018},
  url={https://arxiv.org/abs/1803.03635}
}

@inproceedings{WANN,
  title={Weight agnostic neural networks},
  author={Gaier, Adam and Ha, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5364--5378},
  year={2019},
  url={http://papers.nips.cc/paper/8777-weight-agnostic-neural-networks}
}

@article{DivNets,
  title={Diversity networks: neural network compression using determinantal point processes},
  author={Mariet, Zelda and Sra, Suvrit},
  journal={arXiv preprint arXiv:1511.05077},
  year={2015},
  url={https://arxiv.org/abs/1511.05077}
}

@inproceedings{pruningNeurons1,
  title={Reshaping deep neural network for fast decoding by node-pruning},
  author={He, Tianxing and Fan, Yuchen and Qian, Yanmin and Tan, Tian and Yu, Kai},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={245--249},
  year={2014},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/6853595}
}

@article{pruningNeurons2,
  title={Data-free parameter pruning for deep neural networks},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1507.06149},
  year={2015},
  url={https://arxiv.org/abs/1507.06149}
}

@article{pruningFilters,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016},
  url={https://arxiv.org/abs/1608.08710}
}

@article{flowPruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
  journal={arXiv preprint arXiv:2006.05467},
  year={2020},
  url={https://arxiv.org/abs/2006.05467}
}

@article{movementPruning,
  title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
  journal={arXiv preprint arXiv:2005.07683},
  year={2020},
  url={https://arxiv.org/abs/2005.07683}
}

@article{distillingKnowledge,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015},
  url={https://arxiv.org/abs/1503.02531}
}

@inproceedings{deepReally,
  title={Do deep nets really need to be deep?},
  author={Ba, Jimmy and Caruana, Rich},
  booktitle={Advances in neural information processing systems},
  pages={2654--2662},
  year={2014},
  url={http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep}
}

@article{rethinkingPruning,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018},
  url={https://arxiv.org/abs/1810.05270}
}

@misc{nakkiran2019deepDoubleDescent,
      title={Deep Double Descent: Where Bigger Models and More Data Hurt}, 
      author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
      year={2019},
      eprint={1912.02292},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{doubleDescentbelkin2018reconciling,
    title={Reconciling modern machine learning practice and the bias-variance trade-off},
    author={Mikhail Belkin and Daniel Hsu and Siyuan Ma and Soumik Mandal},
    year={2018},
    eprint={1812.11118},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/1812.11118v2}
}

@misc{implReg2,
    title="{How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function Part II: the multi-D Case of Two Layers with Random First Layer}",
    author={Jakob Heiss and Josef Teichmann and Hanna Wutte},
    year={unpublished work in progress, will be published soon},
} 

@misc{implReg1,
    title="{How implicit regularization of Neural Networks affects the learned function {--} Part I}",
    author={Jakob Heiss and Josef Teichmann and Hanna Wutte},
    year={2019},
    month=Nov,
    eprint={1911.02903},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/1911.02903}
} 

@book{ben2003generalizedPseudoInverse,
  title={Generalized inverses: theory and applications},
  author={Ben-Israel, Adi and Greville, Thomas NE},
  volume={15},
  year={2003},
  publisher={Springer Science \& Business Media},
  url={https://doi.org/10.1007\%2Fb97366}
}


@book{Adams:SobolevSpaces1990498,
      author        = "Adams, Robert A and Fournier, John J F",
      title         = "{Sobolev spaces; 2nd ed.}",
      publisher     = "Academic Press",
      address       = "New York, NY",
      series        = "Pure and applied mathematics",
      year          = "2003",
      url           = "http://cds.cern.ch/record/1990498",
}

@ARTICLE{BalduzziBrownianMotionNN2017arXiv170208591B,
       author = {{Balduzzi}, David and {Frean}, Marcus and {Leary}, Lennox and
         {Lewis}, JP and {Wan-Duo Ma}, Kurt and {McWilliams}, Brian},
        title = "{The Shattered Gradients Problem: If resnets are the answer, then what is the question?}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2017",
        month = Feb,
          eid = {arXiv:1702.08591},
        pages = {arXiv:1702.08591},
archivePrefix = {arXiv},
       eprint = {1702.08591},
 primaryClass = {cs.NE},
 url={https://arxiv.org/abs/1702.08591v2},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170208591B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{ITOApproxNNWithoutScale1991817,
title = "Approximation of functions on a compact set by finite sums of a sigmoid function without scaling",
journal = "Neural Networks",
volume = "4",
number = "6",
pages = "817 - 826",
year = "1991",
issn = "0893-6080",
doi = "10.1016/0893-6080(91)90060-I",
url = "https://doi.org/10.1016/0893-6080(91)90060-I",
author = "Yoshifusa Ito",
keywords = "Heaviside function, Sigmoid function, Unscaled sigmoid function, Discriminatory, Strongly discriminatory, Linear combination, Uniform approximation, Compact set",
abstract = "This paper is concerned with three layered feedforward neural networks which are capable of approximately representing continuous functions on a compact set. Of particular interest here is the use of a sigmoid function without scaling. First, we prove existentially that a linear combination of unscaled shifted rotations of any sigmoid function can approximate uniformly an arbitrary continuous function on a compact set in Rd. Second, a proposition is proved constructively using the fact that a homogeneous polynomial P can be expressed as P(x) = Σi=1nai(ωi · x)r. It states that the approximation of an arbitrary polynomial on a interval in R can be extended to that of an arbitrary continuous function on a compact set in Rd. Then, four corollaries are derived. Though their statements are more or less restricted, the proofs provide algorithms for implementing the uniform approximation. In three of these corollaries, sigmoid functions are used without scaling."
}

@book{hastie_09_elements-of.statistical-learning,
  added-at = {2010-06-03T15:15:09.000+0200},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/200d858c0bd2826d4eb5f39450192d1f5/ukoethe},
  edition = 2,
  file = {:Books\\HastieTibshiraniFriedman-09-Elements-of-Statistical-Learning-2nd-edition\\hastie_09_elements-of.statistical-learning.pdf:PDF},
  interhash = {52d1772f39be836e3b298d37b8c0cfa1},
  intrahash = {00d858c0bd2826d4eb5f39450192d1f5},
  keywords = {inference mathmatics dataanalysis method clutering statistics},
  publisher = {Springer},
  timestamp = {2010-06-03T15:15:09.000+0200},
  title = {The elements of statistical learning: data mining, inference and prediction},
  url = {http://www-stat.stanford.edu/~tibs/ElemStatLearn/},
  year = 2009,
  doi = {10.1007/978-0-387-84858-7},
  isbn={978-0-387-84858-7},
  issn={0172-7397}
}

@ARTICLE{SoudryImplicitBiasSeparableData2017arXiv171010345S,
       author = {{Soudry}, Daniel and {Hoffer}, Elad and {Shpigel Nacson}, Mor and
         {Gunasekar}, Suriya and {Srebro}, Nathan},
        title = "{The Implicit Bias of Gradient Descent on Separable Data}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2017",
        month = Oct,
          eid = {arXiv:1710.10345},
        pages = {arXiv:1710.10345},
archivePrefix = {arXiv},
       eprint = {1710.10345},
 primaryClass = {stat.ML},
 url={https://arxiv.org/abs/1710.10345v4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171010345S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{GidelImplicitDiscreteRegularizationDeepLinearNN2019arXiv190413262G,
       author = {{Gidel}, Gauthier and {Bach}, Francis and {Lacoste-Julien}, Simon},
        title = "{Implicit Regularization of Discrete Gradient Dynamics in Deep Linear Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
         year = "2019",
        month = Apr,
          eid = {arXiv:1904.13262},
        pages = {arXiv:1904.13262},
archivePrefix = {arXiv},
       eprint = {1904.13262},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1904.13262v1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190413262G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{FriedmanGradientDecentLinearReg,
author = {Friedman, Jerome and E. Popescuy, Bogdan},
year = {2004},
month = Jan,
title = {Gradient Directed Regularization for Linear Regression and Classification},
journal = {Tech rep},
url ={https://www.researchgate.net/publication/244258820_Gradient_Directed_Regularization_for_Linear_Regression_and_Classification}
}

@ARTICLE{PoggioGernalizationDeepNN2018arXiv180611379P,
       author = {{Poggio}, Tomaso and {Liao}, Qianli and {Miranda}, Brando and
         {Banburski}, Andrzej and {Boix}, Xavier and {Hidary}, Jack},
        title = "{Theory IIIb: Generalization in Deep Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = "2018",
        month = Jun,
          eid = {arXiv:1806.11379},
        pages = {arXiv:1806.11379},
archivePrefix = {arXiv},
       eprint = {1806.11379},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1806.11379v1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180611379P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{MaennelDradDecentPicewiseLinear2018arXiv180308367M,
       author = {{Maennel}, Hartmut and {Bousquet}, Olivier and {Gelly}, Sylvain},
        title = "{Gradient Descent Quantizes ReLU Network Features}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2018",
        month = Mar,
          eid = {arXiv:1803.08367},
        pages = {arXiv:1803.08367},
archivePrefix = {arXiv},
       eprint = {1803.08367},
 primaryClass = {stat.ML},
          url = {https://arxiv.org/abs/1803.08367v1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180308367M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@Article{CravenSpline1978,
author="Craven, Peter
and Wahba, Grace",
title="Smoothing noisy data with spline functions",
journal="Numerische Mathematik",
year="1978",
month=Dec,
day="01",
volume="31",
number="4",
pages="377--403",
abstract="Smoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.",
issn="0945-3245",
doi="10.1007/BF01404567",
url="https://doi.org/10.1007/BF01404567"
}

@Article{ReinschSpline1967,
author="Reinsch, Christian H.",
title="Smoothing by spline functions",
journal="Numerische Mathematik",
year="1967",
month=Oct,
day="01",
volume="10",
number="3",
pages="177--183",
issn="0945-3245",
doi="10.1007/BF02162161",
url="https://doi.org/10.1007/BF02162161"
}


@ARTICLE{NeyshaburImplicitRegPhD2017arXiv170901953N,
       author = {{Neyshabur}, Behnam},
        title = "{Implicit Regularization in Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2017",
        month = Sep,
          eid = {arXiv:1709.01953},
        pages = {arXiv:1709.01953},
archivePrefix = {arXiv},
       eprint = {1709.01953},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1709.01953v2},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170901953N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{NeyshaburImplicitReg2014arXiv1412.6614N,
       author = {{Neyshabur}, Behnam and {Tomioka}, Ryota and {Srebro}, Nathan},
        title = "{In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = "2014",
        month = Dec,
          eid = {arXiv:1412.6614},
        pages = {arXiv:1412.6614},
archivePrefix = {arXiv},
       eprint = {1412.6614},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1412.6614v4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6614N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{YuanzhiImplicitRegPseudoSmoothing2018arXiv180801204L,
       author = {{Li}, Yuanzhi and {Liang}, Yingyu},
        title = "{Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2018",
        month = Aug,
          eid = {arXiv:1808.01204},
        pages = {arXiv:1808.01204},
archivePrefix = {arXiv},
       eprint = {1808.01204},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1808.01204v3},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180801204L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{BishopCurvatureSmoothingNeuralNetworks248466,
author={Bishop, Chris M.},
journal={IEEE Transactions on Neural Networks},
title={Curvature-driven smoothing: a learning algorithm for feedforward networks},
year={1993},
volume={4},
number={5},
pages={882-884},
abstract={The performance of feedforward neural networks in real applications can often be improved significantly if use is made of a priori information. For interpolation problems this prior knowledge frequently includes smoothness requirements on the network mapping, and can be imposed by the addition to the error function of suitable regularization terms. The new error function, however, now depends on the derivatives of the network mapping, and so the standard backpropagation algorithm cannot be applied. In this letter, we derive a computationally efficient learning algorithm, for a feedforward network of arbitrary topology, which can be used to minimize such error functions. Networks having a single hidden layer, for which the learning algorithm simplifies, are treated as a special case.<>},
keywords={feedforward neural nets;learning (artificial intelligence);curvature-driven smoothing;learning algorithm;feedforward neural networks;computationally efficient learning algorithm;Smoothing methods;Backpropagation algorithms;Training data;Neural networks;Interpolation;Network topology;Multilayer perceptrons;Mean square error methods;Transfer functions;Feedforward neural networks},
doi={10.1109/72.248466},
ISSN={1045-9227},
month=Sep,
url={https://pdfs.semanticscholar.org/3c39/ddfbd9822230b5375d581bf505ecf6255283.pdf}
}

@article{BishopRegularizationNoisedoi:10.1162/neco.1995.7.1.108,
author = {Bishop, Chris M.},
title = {Training with Noise is Equivalent to Tikhonov Regularization},
journal = {Neural Computation},
volume = {7},
number = {1},
pages = {108-116},
year = {1995},
doi = {10.1162/neco.1995.7.1.108},
URL = {https://doi.org/10.1162/neco.1995.7.1.108},
eprint = {https://doi.org/10.1162/neco.1995.7.1.108},
abstract = { It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise. }
}

@InProceedings{bishop1995regularizationEarlyStopping,
author = {Bishop, Chris M.},
title = {Regularization and Complexity Control in Feed-forward Networks},
booktitle = {Proceedings International Conference on Artificial Neural Networks ICANN'95},
year = {1995},
month = Jan,
abstract = {In this paper we consider four alternative approaches to complexity control in feed-forward networks based respectively on architecture selection, regularization, early stopping, and training with noise. We show that there are close similarities between these approaches and we argue that, for most practical applications, the technique of regularization should be the method of choice.},
publisher = {EC2 et Cie},
url = {https://www.microsoft.com/en-us/research/publication/regularization-and-complexity-control-in-feed-forward-networks/},
pages = {141-148},
volume = {1},
edition = {Proceedings International Conference on Artificial Neural Networks ICANN'95}
}

@ARTICLE{ADAM2014arXiv1412.6980K,
       author = {{Kingma}, Diederik P. and {Ba}, Jimmy},
        title = "{Adam: A Method for Stochastic Optimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2014",
        month = Dec,
          eid = {arXiv:1412.6980},
        pages = {arXiv:1412.6980},
archivePrefix = {arXiv},
       eprint = {1412.6980},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1412.6980},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{Goodfellow-et-al-DeepLearning-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{HornikUniversalApprox1991251,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
issn = "0893-6080",
doi = "10.1016/0893-6080(91)90009-T",
url = "https://doi.org/10.1016/0893-6080(91)90009-T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.",
pdfurl="http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf",
}

@Article{CybenkoUniversalApprox1989,
author="Cybenko, G.",
title="Approximation by superpositions of a sigmoidal function",
journal="Mathematics of Control, Signals and Systems",
year="1989",
month=Dec,
day="01",
volume="2",
number="4",
pages="303--314",
abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
issn="1435-568X",
doi="10.1007/BF02551274",
url="https://doi.org/10.1007/BF02551274",
pdfurl="https://www.researchgate.net/profile/George_Cybenko/publication/226439292_Approximation_by_superpositions_of_a_sigmoidal_function_Math_Cont_Sig_Syst_MCSS_2303-314/links/551d50c90cf23e2801fe12cf/Approximation-by-superpositions-of-a-sigmoidal-function-Math-Cont-Sig-Syst-MCSS-2303-314.pdf"
}


@article{KimeldorfSplineBayes10.2307/2239347,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2239347},
 author = {George S. Kimeldorf and Grace Wahba},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {495--502},
 publisher = {Institute of Mathematical Statistics},
 title = {A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines},
 volume = {41},
 year = {1970}
}

@book{bishop2006patternML,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher="Springer",
  address="New York, NY",
  series="Information science and statistics",
  url="http://cds.cern.ch/record/998831",
  isbn={978-0387-31073-2},
  issn={1613-9011}
}

@ARTICLE{MatthewsGaussProc2018arXiv,
	author = {{Matthews}, Alexander G. de G. and {Rowland}, Mark and {Hron}, Jiri and
	{Turner}, Richard E. and {Ghahramani}, Zoubin},
	title = "{Gaussian Process Behaviour in Wide Deep Neural Networks}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2018",
	month = Apr,
	eid = {arXiv:1804.11271},
	pages = {arXiv:1804.11271},
	archivePrefix = {arXiv},
	eprint = {1804.11271},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180411271M},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	url = {https://arxiv.org/abs/1804.11271}
}

@ARTICLE{KuboImplicitReg2019arXiv,
	author = {{Kubo}, Masayoshi and {Banno}, Ryotaro and {Manabe}, Hidetaka and
	{Minoji}, Masataka},
	title = "{Implicit Regularization in Over-parameterized Neural Networks}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = "2019",
	month = Mar,
	eid = {arXiv:1903.01997},
	pages = {arXiv:1903.01997},
	archivePrefix = {arXiv},
	eprint = {1903.01997},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190301997K},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	url={https://arxiv.org/abs/1903.01997}
}

@thesis{MasterThesisJakobHeiss,
	title = "{Implicit Regularization for Artificial Neural Networks}",
	year=2019,
	author={Heiss, Jakob M.},
	adress={Z{\"u}rich},
	note={work in progress}
}

@ARTICLE{BayesianInverseProblems2013arXiv,
	author = {{Dashti}, Masoumeh and {Stuart}, Andrew M.},
	title = "{The Bayesian Approach To Inverse Problems}",
	journal = {arXiv e-prints},
	keywords = {Mathematics - Probability},
	year = "2013",
	month = Feb,
	eid = {arXiv:1302.6989},
	pages = {arXiv:1302.6989},
	archivePrefix = {arXiv},
	eprint = {1302.6989},
	primaryClass = {math.PR},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1302.6989D},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	url={https://arxiv.org/abs/1302.6989}
}

@article{SmartMarkets,
	author = {Mccabe, Kevin and Rassenti, Stephen and Smith, Vernon},
	year = {1991},
	month = 11,
	pages = {534-8},
	title = {Smart Computer-Assisted Markets},
	volume = {254},
	journal = {Science (New York, N.Y.)},
	doi = {10.1126/science.254.5031.534},
	url = {https://doi.org/10.1126/science.254.5031.534}
}

@ARTICLE{2019arXivDeepFictitiousPlay,
	author = {{Hu}, Ruimeng},
	title = "{Deep Fictitious Play for Stochastic Differential Games}",
	journal = {arXiv e-prints},
	keywords = {Mathematics - Optimization and Control, Computer Science - Computer Science and Game Theory, Statistics - Machine Learning},
	year = "2019",
	month = Mar,
	eid = {arXiv:1903.09376},
	pages = {arXiv:1903.09376},
	archivePrefix = {arXiv},
	eprint = {1903.09376},
	primaryClass = {math.OC},
	url={https://arxiv.org/abs/1903.09376},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190309376H},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{bianchini2014complexity,
	author={M. {Bianchini} and F. {Scarselli}},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	title={On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures},
	year={2014},
	volume={25},
	number={8},
	pages={1553-1565},
	keywords={computational complexity;feedforward neural nets;pattern classification;topology;neural network classifiers;shallow architecture;deep architecture;artificial neural network;hidden layers;vision;human language understanding;feedforward neural network;high complexity functions;topological concepts;function complexity evaluation;classification;sigmoidal activation function;deep network;Complexity theory;Neurons;Biological neural networks;Polynomials;Upper bound;Computer architecture;Betti numbers;deep neural networks;function approximation;topological complexity;Vapnik--Chervonenkis dimension (VC-dim).;Betti numbers;deep neural networks;function approximation;topological complexity;Vapnik–Chervonenkis dimension (VC-dim);Algorithms;Computer Simulation;Models, Theoretical;Nerve Net;Pattern Recognition, Automated},
	doi={10.1109/TNNLS.2013.2293637},
	ISSN={2162-237X},
	month=Aug,
	url={https://doi.org/10.1109/TNNLS.2013.2293637}
}

@article{tsitsiklis1994asynchronous,
	title={Asynchronous stochastic approximation and Q-learning},
	author={Tsitsiklis, John N},
	journal={Machine learning},
	volume={16},
	number={3},
	pages={185--202},
	year={1994},
	publisher={Springer},
	url={https://doi.org/10.1007/BF00993306}
}

@article{shaham2018provable,
	title={Provable approximation properties for deep neural networks},
	author={Shaham, Uri and Cloninger, Alexander and Coifman, Ronald R},
	journal={Applied and Computational Harmonic Analysis},
	volume={44},
	number={3},
	pages={537--557},
	year={2018},
	publisher={Elsevier},
	url={https://doi.org/10.1016/j.acha.2016.04.003}
}

@article{kimeldorf1970,
	author = "Kimeldorf, George S. and Wahba, Grace",
	doi = "10.1214/aoms/1177697089",
	fjournal = "The Annals of Mathematical Statistics",
	journal = "Ann. Math. Statist.",
	month = Apr,
	number = "2",
	pages = "495--502",
	publisher = "The Institute of Mathematical Statistics",
	title = "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines",
	url = "https://doi.org/10.1214/aoms/1177697089",
	volume = "41",
	year = "1970"
}

@misc {meineBac_V15,
	title = "{Preisbildung und {\"u}berbestimmte Ungleichungssysteme}",
	year=2017,
	author={Heiss, Jakob M.},
	url={https://www.dropbox.com/s/ojywjawntmxt9zz/BacArbeit15.pdf?dl=0},
	adress={Wien}
}

@misc {meineBacV15,
	title = {Preisbildung und {\"u}berbestimmte Ungleichungssysteme},
	year=2017,
	month=mar,
	day=27,
	note = {(Version 15: {\texttt{BacArbeit15.pdf}})},
	author={Heiss, Jakob M.},
	adress={Wien}
}

@misc {meineBacV13,
	title = {Preisbildung und {\"u}berbestimmte Ungleichungssysteme},
	year=2017,
	month=feb,
	day=13,
	note = {(Version 13: {\texttt{BacArbeit13.pdf}})},
	author={Heiss, Jakob M.},
	adress={Wien}
}

@book{LinProgVanderbei2013,
	title={Linear Programming: Foundations and Extensions},
	author={Vanderbei, R.J.},
	isbn={9781475756623},
	series={International Series in Operations Research {\&} Management Science},
	url={https://books.google.at/books?id=HjznBwAAQBAJ},
	year={2013},
	publisher={Springer US}
}



@misc {XETRA_DeutscheBoerseFortlaufendeAuktion,
	title = {XETRA{\textsuperscript{\textregistered}} Release 16.0 Marktmodell Fortlaufende Auktion},
	url = {http://www.xetra.com/blob/1618544/fd5abeaa556dcbe4b7e571d181f85973/data/Marktmodell-fortlaufende-auktion.pdf},
	year=2015,
	month=sep,
	day=21,
	note = {[Online; Stand 10. Februar 2017]},
	author={{Deutsche B{\"o}rse AG}},
}

@misc {WienerBoerseAgb2_1,
	title = {Handelsregeln f{\"u}r das automatisierte Handelssystem XETRA{\textsuperscript{\textregistered}} (Exchange Electronic Trading)},
	url = {https://www.wienerborse.at/uploads/u/cms/files/recht/agb/agb-2-1.pdf},
	year=2017,
	month=feb,
	day=1,
	note = {[Online; Stand 10. Februar 2017]},
	author={{Wiener B{\"o}rse AG}},
	adress={Wien},
	SERIES = {Allgemeine Gesch{\"a}ftsbedingungen der Wiener B{\"o}rse AG},
	NUMBER = {2.1}
}
@misc {test123,
	title = {Handelsregeln f{\"u}r das automatisierte Handelssystem XETRA{$^\textregistered$} (Exchange Electronic Trading)},
	url = {https://www.wienerborse.at/uploads/u/cms/files/recht/agb/agb-2-1.pdf},
	year=2017,
	month=2,
	booktitle = {Allgemeine Gesch{\"a}ftsbedingungen der Wiener B{\"o}rse AG}
}

@article {BrainardFisher,
	author = {Brainard, William C. and Scarf, Herbert E.},
	title = {How to Compute Equilibrium Prices in 1891},
	journal = {American Journal of Economics and Sociology},
	volume = 64,
	number = 1,
	publisher = {Blackwell Publishing Ltd.},
	issn = {1536-7150},
	url = {http://dx.doi.org/10.1111/j.1536-7150.2005.00349.x},
	doi = {10.1111/j.1536-7150.2005.00349.x},
	pages = {57--83},
	year = 2005,
}

@article{salahirobustL1SolutionInequalitiesErrorInA,
  title={Robust {$\ell_1$} and {$\ell_{\infty}$} Solutions of Linear Inequalities},
  author={Salahi, Maziar},
  url={http://www.pvamu.edu/Include/Math/AAM/AAM%20Vol.%206,%20Issue%2012%20(December%202011)/09_%20Salahi%20AAM-R390-MS-032811.pdf},
  journal={Applications and Applied Mathematics},
  ISSN= {1932-9466},
  year=2011,
  pages={522--528},
  volume=6,
  number=2  
}

@book{bloomfield2012leastAbsolutDerivation,
  title={Least Absolute Deviations: Theory, Applications and Algorithms},
  author={Bloomfield, Peter and Steiger, William},
  isbn={9781468485745},
  series={Progress in Probability},
  volume=6,
  url={https://books.google.at/books?id=NRTTBwAAQBAJ},
  year={2012},
  publisher={Birkh{\"a}user Boston}
}


@Article{Rosen2000L1LinearEquations,
author="Rosen, J. Ben
and Park, Haesun
and Glick, John
and Zhang, Lei",
title="Accurate Solution to Overdetermined Linear Equations with Errors Using L1 Norm Minimization",
journal="Computational Optimization and Applications",
year=2000,
volume=17,
number=2,
pages="329--341",
abstract="It has been known for many years that a robust solution to an overdetermined system of linear equations Ax ≈ b is obtained by minimizing the L1 norm of the residual error. A correct solution x to the linear system can often be obtained in this way, in spite of large errors (outliers) in some elements of the (m {\texttimes} n) matrix A and the data vector b. This is in contrast to a least squares solution, where even one large error will typically cause a large error in x. In this paper we give necessary and sufficient conditions that the correct solution is obtained when there are some errors in A and b. Based on the sufficient condition, it is shown that if k rows of [Ab] contain large errors, the correct solution is guaranteed if (m − n)/n ≥ 2k/$\sigma$, where $\sigma$ > 0, is a lower bound of singular values related to A. Since m typically represents the number of measurements, this inequality shows how many data points are needed to guarantee a correct solution in the presence of large errors in some of the data. This inequality is, in fact, an upper bound, and computational results are presented, which show that the correct solution will be obtained, with high probability, for much smaller values of m − n.",
issn="1573-2894",
doi="10.1023/A:1026562601717",
url="http://dx.doi.org/10.1023/A:1026562601717"
}


@article{EfronRobusRegressionL1Equation,
 ISSN = {00361445},
 URL = {http://www.jstor.org/stable/2030700},
 abstract = {This is a survey of modern developments in statistical regression, written for the mathematically educated nonstatistician. It begins with a review of the traditional theory of least-squares curve-fitting. Modern developments in regression theory have developed in response to the practical limitations of the least-squares approach. Recent progress has been made feasible by the electronic computer, which frees statisticians from the confines of mathematical tractability. Topics discussed include robust regression, bootstrap measures of variability, local smoothing and cross-validation, projection pursuit, Mallows' Cp criterion, Stein estimation, generalized regression for Poisson data, and regression methods for censored data. All of the methods are illustrated with real-life examples.},
 author = {Efron, Bradley},
 journal = {SIAM Review},
 number = 3,
 pages = {421--449},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {Computer-Intensive Methods in Statistical Regression},
 volume = 30,
 year = 1988
}



@article{HollandRobustRegression,
author = {Holland, Paul W. and Welsch, Roy E.},
title = {Robust regression using iteratively reweighted least-squares},
journal = {Communications in Statistics - Theory and Methods},
volume = 6,
number = 9,
pages = {813--827},
year = 1977,
doi = {10.1080/03610927708827533},
URL = {http://dx.doi.org/10.1080/03610927708827533},
eprint = {http://dx.doi.org/10.1080/03610927708827533},
abstract = { The rapid development of the theory of robust estimation (Huber, 1973) has created a need for computational procedures to produce robust estimates. We will review a number of different computational approaches for robust linear regression but focus on one—iteratively reweighted least-squares (IRLS). The weight functions that we discuss are a part of a semi-portable subroutine library called ROSEPACK (RObust Statistical Estimation PACKage) that has been developed by the authors and Virginia Klema at the Computer Research Center of the National Bureau of Economic Research, Inc. in Cambridge, Mass. with the support of the National Science Foundation. This library (Klema, 1976) makes it relatively simple to implement an IRLS regression package. }
}

@article{HuberRobustRegression,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2958283},
 abstract = {Maximum likelihood type robust estimates of regression are defined and their asymptotic properties are investigated both theoretically and empirically. Perhaps the most important new feature is that the number p of parameters is allowed to increase with the number n of observations. The initial terms of a formal power series expansion (essentially in powers of p/n) show an excellent agreement with Monte Carlo results, in most cases down to 4 observations per parameter.},
 author = {Huber, Peter J.},
 journal = {The Annals of Statistics},
 number = 5,
 pages = {799--821},
 publisher = {Institute of Mathematical Statistics},
 title = {Robust Regression: Asymptotics, Conjectures and Monte Carlo},
 language={English},
 month=sep,
 volume = 1,
 year = 1973
}




@article{Pollard_1991L1Equations,
adress={New York, USA}, title={Asymptotics for Least Absolute Deviation Regression Estimators}, volume={7}, url={http://www.math.pku.edu.cn/teachers/xirb/Courses/QR2013/Pollard91ET.pdf}, DOI={10.1017/S0266466600004394}, abstract={The LAD estimator of the vector parameter in a linear regression is defined by minimizing the sum of the absolute values of the residuals. This paper provides a direct proof of asymptotic normality for the LAD estimator. The main theorem assumes deterministic carriers. The extension to random carriers includes the case of autoregressions whose error terms have finite second moments. For a first-order autoregression with Cauchy errors the LAD estimator is shown to converge at a 1/n rate.}, number={2}, journal={Econometric Theory}, publisher={Cambridge University Press}, author={Pollard, David}, year={1991}, month=jun, pages={186--199}
}

@misc{vimentisGleichgewichtspreis,
author={VIMENTIS},
title={Lexikon: Gleichgewichtspreis},
year=2011,
url={https://www.vimentis.ch/d/lexikon/179/Gleichgewichtspreis.html},
note = {[Online; Stand 19. November 2016]}
}


@book{legendre1805nouvellesLeastSquares,
  title={Nouvelles m{\'e}thodes pour la d{\'e}termination des orbites des com{\`e}tes},
  author={Legendre, Adrien Marie},
  series={Analyse des triangles trac{\'e}s sur la surface d'un sph{\'e}roide},
  url={https://books.google.at/books?id=7C9RAAAAYAAJ},
  number={1},
  year={1805},
  publisher={F. Didot}
}



@book{gauss1809LeastSquares1,
  title={Theoria motus corporum coelestium in sectionibus conicis solem ambientium auctore Carolo Friderico Gauss},
  author={Gau{\ss}, Carl Friedrich},
  year={1809},
  publisher={sumtibus Frid. Perthes et IH Besser},
  url={https://books.google.at/books?id=VKhu8yPcat8C}
}
@book{gauss1823theoriaLeastSquares2,
  title={Theoria combinationis observationum erroribus minimis obnoxiae.-Gottingae, Henricus Dieterich 1823},
  author={Gau{\ss}, Carl-Friedrich},
  year={1823},
  url={https://books.google.at/books?id=hrZQAAAAcAAJ},
  publisher={Henricus Dieterich}
}



@Article{Dax2008LeastSquareInequalities,
author="Dax, Achiya",
title="A hybrid algorithm for solving linear inequalities in a least squares sense",
journal="Numerical Algorithms",
year="2008",
volume="50",
number="2",
pages="97",
abstract="The need for solving a system of linear inequalities, A                        x ≥ b, arises in many applications. Yet in some cases the system to be solved turns out to be inconsistent due to measurement errors in the data vector b. In such a case it is often desired to find the smallest correction of b that recovers feasibility. That is, we are looking for a small nonnegative vector, y ≥ 0, for which the modified system A                        x ≥ b - y is solvable. The problem of calculating the smallest correction vector is called the least deviation problem. In this paper we present new algorithms for solving this problem. Numerical experiments illustrate the usefulness of the proposed methods.",
issn="1572-9265",
doi="10.1007/s11075-008-9218-3",
url="http://dx.doi.org/10.1007/s11075-008-9218-3"
}


@article{SPINGARN1987LeastSquareInequalities,
title = "A projection method for least-squares solutions to overdetermined systems of linear inequalities",
journal = "Linear Algebra and its Applications",
volume = "86",
number = "",
pages = "211--236",
year = "1987",
issn = "0024-3795",
doi = "10.1016/0024-3795(87)90296-5",
url = "http://www.sciencedirect.com/science/article/pii/0024379587902965",
author = "Spingarn, Jonathan E.",
abstract = "An algorithm previously introduced by the author for finding a feasible point of a system of linear inequalities is further investigated. For inconsistent systems, it is shown to generate a sequence converging at a linear rate to the set of least-squares solutions. The algorithm is a projection-type method, and is a manifestation of the proximal-point algorithm."
}

@Article{Lei2015LeastSquareInequalities,
author="Lei, Yuan",
title="The inexact fixed matrix iteration for solving large linear inequalities in a least squares sense",
journal="Numerical Algorithms",
year="2015",
volume="69",
number="1",
pages="227--251",
abstract="A fixed matrix iteration algorithm was proposed by A. Dax (Numer. Algor. 50, 97--114 2009) for solving linear inequalities in a least squares sense. However, a great deal of computation for this algorithm is required, especially for large-scale problems, because a least squares subproblem should be solved accurately at each iteration. We present a modified method, the inexact fixed iteration method, which is a generalization of the fixed matrix iteration method. In this inexact iteration process, the classical LSQR method is implemented to determine an approximate solution of each least squares subproblem with less computational effort. The convergence of this algorithm is analyzed and several numerical examples are presented to illustrate the efficiency of the inexact fixed matrix iteration algorithm for solving large linear inequalities.",
issn="1572-9265",
doi="10.1007/s11075-014-9892-2",
url="http://dx.doi.org/10.1007/s11075-014-9892-2"
}

@article{BramleyLeastSquareInequalities,
author = {Bramley, R. and Winnicka, B.},
title = {Solving Linear Inequalities in a Least Squares Sense},
journal = {SIAM Journal on Scientific Computing},
volume = {17},
number = {1},
pages = {275--286},
year = {1996},
doi = {10.1137/0917020},
url = {https://pdfs.semanticscholar.org/4339/381ab90b55481882df7db26fd6c596848945.pdf},
eprint = {http://dx.doi.org/10.1137/0917020}
}

@book{han1980leastSquares,
  title={Least-squares Solution of Linear Inequalities},
  author={Han, S.P. and {WISCONSIN UNIV-MADISON MATHEMATICS RESEARCH CENTER.}},
  language={English},
  series={MRC technical summary report},
  number=2141,
  url={www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA096658},
  year={1980},
  month = nov,
  publisher={Defense Technical Information Center},
  abstract={The paper deals with a system of linear inequalities in a finite dimensional
  space. When the system is inconsistent, we are interested in vectors that
  satisfy the system in a least-squares sense. We characterize such leastsquares
  solutions and propose a method to find one of these solutions. It is
  shown that for any given system of linear inequalities and for any starting
  point, the method can produce a solution in a finite number of iterations.
  Computational results are very satisfactory in terms of accuracy and number
  of iterations.}
}
@techreport{han1980leastAlternativBibTeX,
  title={Least-Squares Solution of Linear Inequalities.},
  author={Han, S-P},
  year={1980},
  number=2141,
  institution={DTIC Document}
}


@article{PoundDropNYT,
 title = {Pound Drops Sharply in Asian Trading Amid `Brexit' Fears},
 note = {[Online; Stand 18. November 2016]},
 journal = {The New York Times},
 author = {Bray, Chud and Gaugh, Neil},
 language={English},
 month = oct,
 year = {2016},
 url = {www.nytimes.com/2016/10/08/business/dealbook/britain-pound-currency-flash-crash.html}
}

 @misc{wiki:LinProg,
   author = "Wikipedia",
   title = "Lineare Optimierung --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2016",
   url = "https://de.wikipedia.org/w/index.php?title=Lineare_Optimierung&oldid=158785906",
   note = "[Komplexit{\"a}t: \url{https://de.wikipedia.org/w/index.php?title=Lineare_Optimierung&oldid=158785906#Komplexit.C3.A4t_und_L.C3.B6sungsverfahren}; Dualit{\"a}t: \url{https://de.wikipedia.org/w/index.php?title=Lineare_Optimierung&oldid=158785906#Dualit.C3.A4t}; Online; Stand 18. November 2016]"
 }
 
 @misc{ wiki:Marktpreis,
   author = "Wikipedia",
   title = "Marktpreis --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2016",
   url = "https://de.wikipedia.org/w/index.php?title=Marktpreis&oldid=159232739",
   note = "[Online; Stand 18. November 2016]"
 }

 @misc{ wiki:Marshall,
   author = "Wikipedia",
   title = "Alfred Marshall --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2016",
   url = "https://de.wikipedia.org/w/index.php?title=Alfred_Marshall&oldid=150595089",
   note = "[Online; Stand 18. November 2016]"
 }

@misc{DualesProblemUniWien,
   author = {Breunig, Franz},
   booktitle = {Simplex-Programm mit Online-Skriptum},
   title = {Das Duale Problem},
   year = {1997},
   howpublished = {Universit{\"a}t Wien},
   url = {http://www.unet.univie.ac.at/~a0025537/php/Abschnitt1/Mathe/fb_skript3.html},
   note = {[Online; abgerufen am 18. November 2016]}
 }

@article{DantzigWolfeDecomposition,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1911818},
 abstract = {A procedure is presented for the efficient computational solution of linear programs having a certain structural property characteristic of a large class of problems of practical interest. The property makes possible the decomposition of the problem into a sequence of small linear programs whose iterated solutions solve the given problem through a generalization of the simplex method for linear programming.},
 author = {Dantzig, George B. and Wolfe, Philip},
 journal = {Econometrica},
 number = {4},
 pages = {767--778},
 publisher = {[Wiley, Econometric Society]},
 title = {The Decomposition Algorithm for Linear Programs},
 volume = {29},
 year = {1961},
 month = oct,
 DOI = {10.2307/1911818}
}

 
 
 @phdthesis{TebbothDantzigWolfeDecomposition,
 title = {A Computational Study of Dantzig-Wolfe Decomposition},
 author = {Tebboth, James Richard},
 url = {http://www.blisworthhouse.co.uk/OR/Decomposition/tebboth.pdf},
 year = {2001},
 language={English},
 month = dec,
 school = {University of Buckingham},
 abstract = {{This thesis evaluates the computational merits of the Dantzig-Wolfe decomposition algorithm. We use modern computer hardware and software, and, in particular, we have developed an efficient parallel implementation of Dantzig-Wolfe decomposition. We were able to solve problems of up to 83,500 rows, 83,700 columns, and 622,000 non-zero elements, and one important instance was solved in 63 \% of the time taken by a commercial implementation of the simplex method. If we were content with a solution guaranteed to be within 0.1 \% of optimality, then the solution time can be cut by a further 32\%. Using parallel hardware we can cut the solution time further: by 59 \% on a four processor PC (equivalent to a speed-up of 2.42) or 81 \% on seven processors of a Silicon Graphics workstation (a speed-up of 5.22). Dantzig-Wolfe decomposition is an optimisation technique for solving large scale, block structured, linear programming (LP) problems. Problems from many different fields such as production planning, refinery optimisation, and resource allocation may be formulated as LP problems. Where there is some structure arising from repeated components in the problem, such as the handling of multi-periods}},
 contributor = {{The Pennsylvania State University CiteSeerX Archives}},
 language = {{en}},
 }
 
 @BOOK{DiskAlgCormen2009,
 	AUTHOR = {Cormen, Thomas H. AND Leiserson, Charles E. AND Rivest, Ronald L. AND Stein, Clifford},
 	YEAR = {2009},
 	TITLE = {Introduction to Algorithms - },
 	EDITION = {3rd edition.},
 	ISBN = {978-0-262-03384-8},
 	PUBLISHER = {MIT Press},
 	ADDRESS = {Cambridge},
 }
 
 @misc{DieSiedlerOnline,
   author = {{Blue Byte}},
   title = {Die Siedler Online},
   url = {http://www.diesiedleronline.de},
   urldate = {2016-11-17},
   howpublished ={Ubisoft},
   note = {[Online; Stand 17. November 2016]}
 }
 @Booklet{GeneralEquilibriumSkript,
	title = {General Equilibrium},
	author = {Levin, Jonathan},
	howpublished = {Stanford University},
	language={English},
	month = nov,
	year = {2006},
	url = {https://web.stanford.edu/~jdlevin/Econ%20202/General%20Equilibrium.pdf}
}
@Booklet{MikroOekonomieSkript,
	title = {Mikro{\"o}konomie - Vorlesung 2: Pr{\"a}ferenzen und Nutzenfunktionen},
	author = {F{\"u}rnkranz-Prskawetz,  Alexia},
	howpublished = {Technische Universit{\"a}t Wien},
	day = {18},
	language={German},
	month = mar,
	year = {2016},
	url = {http://www.econ.tuwien.ac.at/lva/mikro.vo/slides/Vorlesung_2_2016.pdf}
}
@article{ArrowDebreu1954existence,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1907353},
 abstract = {A. Wald has presented a model of production and a model of exchange and proofs of the existence of an equilibrium for each of them. Here proofs of the existence of an equilibrium are given for an integrated model of production, exchange and consumption. In addition the assumptions made on the technologies of producers and the tastes of consumers are significantly weaker than Wald's. Finally a simplification of the structure of the proofs has been made possible through use of the concept of an abstract economy, a generalization of that of a game.},
 author={Arrow, Kenneth J and Debreu, Gerard},
 journal = {Econometrica},
 number = {3},
 pages = {265--290},
 publisher = {[Wiley, Econometric Society]},
 title = {Existence of an Equilibrium for a Competitive Economy},
 volume = {22},
 year = {1954},
 DOI={10.2307/1907353}
}



@INCOLLECTION{generalEquilibriumDictionary,
  TITLE  =  "general equilibrium",
  BOOKTITLE  =  "The New Palgrave Dictionary of Economics",
  AUTHOR  =  "McKenzie, Lionel W.",
  EDITOR  =  "Durlauf, Steven N. and Blume, Lawrence E.",
  YEAR  =  "2008",
  PUBLISHER  =  "Palgrave Macmillan",
  ADDRESS  =  "Basingstoke",
  URL = {http://www.dictionaryofeconomics.com/article?id=pde2008_G000023&q=general%20equilibrium}
}

@book{WalrasOriginal,
  title={{\'E}l{\'e}ments d'{\'e}conomie politique pure; ou, Th{\'e}orie de la richesse sociale},
  author={Walras, L{\'e}on},
  year={1896},
  publisher={F. Rouge},
  adress={Lausanne},
  url={http://gallica.bnf.fr/ark:/12148/bpt6k111752b},
  note={franz. Original. Engl. {\"U}bersetzung: \cite{WalrasTranslate}}
}

@book{WalrasTranslate,
  title={Elements of Pure Economics: Or, the Theory of Social Wealth. Translated by William Jaffe},
  author={Walras, L{\'e}on},
  year={1954},
  publisher={Published for the American Economic Association and the Royal Economic Society},
  note={Engl. {\"U}bersetzung von \cite{WalrasOriginal}},
  catalogue-url = { http://nla.gov.au/nla.cat-vn773542 }
}

@incollection{MarktpreisGabler,
  title = {Marktpreis},
  publisher = {Springer Gabler Verlag (Herausgeber)},
  booktitle = {Gabler Wirtschaftslexikon},
  author = {Piekenbrock, Dirk },
  url = {http://wirtschaftslexikon.gabler.de/Archiv/54571/marktpreis-v6.html},
  note = "[Online; Stand 19. November 2016]"
}


@incollection{homogen,
  title = {homogene G{\"u}ter},
  publisher = {Springer Gabler Verlag (Herausgeber)},
  booktitle = {Gabler Wirtschaftslexikon},
  author = {Steven, Marion and Piekenbrock, Dirk },
  url = {http://wirtschaftslexikon.gabler.de/Archiv/7002/homogene-gueter-v8.html},
  note = "[Online; Stand 19. November 2016]"
  }
  % Stichwort: homogene Güter, online im Internet: 

 @article{DaxL1Solution,
  author = {Dax, Achiya},
  title = {The L1 Solution of Linear Inequalities},
  journal = {Comput. Stat. Data Anal.},
  issue_date = {January, 2006},
  volume = {50},
  number = {1},
  language={English},
  month = jan,
  year = {2006},
  issn = {0167-9473},
  pages = {40--60},
  numpages = {21},
  url = {http://dx.doi.org/10.1016/j.csda.2004.07.007},
  doi = {10.1016/j.csda.2004.07.007},
  acmid = {1648001},
  publisher = {Elsevier Science Publishers B. V.},
  address = {Amsterdam, The Netherlands, The Netherlands},
  keywords = {A new theorem of the alternative, Affine scaling, Duality, Inconsistent systems, Least absolute deviations, Linear inequalities},
 }
 
 @Article{DaxSmallestCorrection,
 author="Dax, Achiya",
 title="The Smallest Correction of an Inconsistent System of Linear Inequalities",
 journal="Optimization and Engineering",
 year="2001",
 volume="2",
 number="3",
 pages="349--359",
 abstract="The problem of calculating a point x that satisfies a given system of linear inequalities, Ax {\#}x2265; b, arises in many applications. Yet often the system to be solved turns out to be inconsistent due to measurement errors in the data vector, b. In such a case it is useful to find the smallest perturbation of b that recovers feasibility. This motivates our interest in the least correction problem and its properties.",
 issn="1573-2924",
 doi="10.1023/A:1015370617219",
 url="http://dx.doi.org/10.1023/A:1015370617219"
 }
 
 @BOOK{Marshall,
 title = {The Principles of Economics},
 author = {Marshall, Alfred},
 year = {1890},
 publisher = {McMaster University Archive for the History of Economic Thought},
 edition = {8. Edition},
 url = {http://eet.pixel-online.org/files/etranslation/original/Marshall,%20Principles%20of%20Economics.pdf}
 }
 
 @misc{ instabilWiki,
   author = "Wikipedia",
   title = "Von-Neumann-Stabilit{\"a}tsanalyse --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2014",
   url = "https://de.wikipedia.org/w/index.php?title=Von-Neumann-Stabilit%C3%A4tsanalyse&oldid=126537447",
   note = "[Online; Stand 27. August 2015]",
 }

@BOOK{LeVeque,
	AUTHOR = {LeVeque, Randall J.},
	YEAR = {2012},
	TITLE = {Numerical Methods for Conservation Laws - },
	EDITION = {2. Aufl.},
	ISBN = {978-3-7643-2723-1},
	PUBLISHER = {Springer},
	ADDRESS = {Berlin, Heidelberg},
}
%http://www.springer.com/us/book/9783764327231
%- am Ende?


@incollection{Godlewski,
	AUTHOR = {Godlewski, Edwige AND Raviart, Pierre-Arnaud},
	YEAR = {1992},
	NUMBER = {118},
	SERIES = {Applied Mathematical Sciences},
	TITLE = {Numerical Approximation of Hyperbolic Systems of Conservation Laws - },
	PUBLISHER = {Springer Verlag},
	ADDRESS = {Berlin Heidelberg},
}
%- am Ende?


@book{Egartner,
	author = {Wolfgang Egartner},
	title = {Grundlagen der Numerik physikalischer Erhaltungsgesetze},
	publisher = {IWR, Universit{\"a}t Heidelberg},
	year = {1998},
}
%institution = {IWR, Universit{\"a}t Heidelberg}

@misc{ WikibooksErhaltungsgleichungen,
   author = "Wikibooks",
   title = "Theorie und Numerik von Erhaltungsgleichungen --- Wikibooks{,} Die freie Bibliothek",
   year = "2015",
   url = "https://de.wikibooks.org/w/index.php?title=Theorie_und_Numerik_von_Erhaltungsgleichungen&oldid=572189",
   note = "[Online; abgerufen am 1. September 2015]"
 }


@BOOK{EvansPDE,
	AUTHOR = {Evans, Lawrence C.},
	YEAR = {2010},
	TITLE = {Partial Differential Equations - },
	EDITION = {2. Aufl.},
	ISBN = {978-0-821-84974-3},
	PUBLISHER = {American Mathematical Soc.},
	ADDRESS = {Heidelberg},
}

@Booklet{ParDiffSkript,
	title = {Partielle Differentialgleichungen},
	author = {Univ.-Prof. Dr. Ansgar J{\"u}ngel},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = dec,
	year = {2013},
}

@Booklet{fana,
	title = {Funktionalanalysis},
	author = {Harald Woracek AND Michael Kaltenb{\"a}ck AND Martin Bl{\"u}mlinger},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = mar,
	year = {2014},
	edition = {9. Aufl.}
}
@Booklet{fana2016,
	title = {Funktionalanalysis},
	author = {Harald Woracek AND Michael Kaltenb{\"a}ck AND Martin Bl{\"u}mlinger},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = feb,
	year = {2016},
	edition = {11. Aufl.},
	url = {http://www.asc.tuwien.ac.at/funkana/skripten/fana2016.pdf}
}


@BOOK{Arens2013,
	AUTHOR = {Arens, Tilo AND Busam, Rolf AND Hettlich, Frank AND Karpfinger, Christian AND Stachel, Hellmuth AND Lichtenegger, Klaus},
	YEAR = {2013},
	TITLE = {Grundwissen Mathematikstudium - Analysis und Lineare Algebra mit Querverbindungen},
	EDITION = {1. Aufl.},
	ISBN = {978-3-827-42309-2},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
}

@BOOK{Appell2009,
	AUTHOR = {Appell, J{\"u}rgen},
	YEAR = {2009},
	TITLE = {Analysis in Beispielen und Gegenbeispielen - Eine Einf{\"u}hrung in die Theorie reeller Funktionen},
	EDITION = {1. Aufl.},
	ISBN = {978-3-540-88903-8},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
}

@BOOK{Kuso,
	AUTHOR = {Kusolitsch, Norbert},
	YEAR = {2014},
	TITLE = {Ma{\ss} und Wahrscheinlichkeitstheorie - Eine Einf{\"u}hrung},
	EDITION = {2. Aufl.},
	ebookISBN = {978-3-642-45387-8},
	ISBN = {978-3-642-45386-1},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
	DOI = {10.1007/978-3-642-45387-8},
	URL = {http://www.springer.com/de/book/9783642453861}
}

@Article{LaxWendroff,
	author = {Lax, Peter D. AND Wendroff, Burton},
	title = {Systems of Conservation Laws},
	journal = {COMMUNICATIONS ON PURE AND APPLIED MATHEMATICS},
	year = {1960},
	volume = {13},
	pages = {217--237},
}

@Booklet{SchmeiserErhaltungssaetze,
	title = {Numerische Methoden f{\"u}r hyperbolische Erhaltungss{\"a}tze},
	author = {Christian Schmeiser},
	howpublished = {Technische Universit{\"a}t Wien},
	url = {http://homepage.univie.ac.at/christian.schmeiser/teaching1.htm}
}

@Booklet{Arnoldzeitabh,
	title = {Zeitabh{\"a}ngige Probleme in Physik und Technik},
	author = {Anton Arnold},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = jan,
	year = {2014},
}

@Article{BianchiniVanishingViscosity,
	author = {Bianchini, Stefan AND Bressan, Alberto},
	title = {Vanishing viscosity solutions of nonlinear hyperbolic systems},
	journal = {Annals of Mathematics},
	year = {2005},
	volume = {161},
	pages = {223--342},
}

@Article{Oleinik,
	author = {Oleinik, Olga},
	title = {Discontinuous solutions of non-linear differential equations},
	journal = {American Mathematical Society Translations},
	year = {1963},
	volume = {26},
	pages = {95--172},
}

@Article{Kruzhkov,
	author={Kru{\v z}kov, S. N.},
	title = {First-order quasilinear equations with several space variables},
	journal = {Mathematics of the USSR-Sbornik},
	year = {1970},
	volume = {10},
	pages = {217--243},
	url="http://stacks.iop.org/0025-5734/10/i=2/a=A06",
	abstract={In this paper we construct a theory of generalized solutions in the large of Cauchy's problem for the equations ##IMG## [http://ej.iop.org/images/0025-5734/10/2/A06/tex_sm_2156_img1.gif] {$\displaystyle u_t+\sum_{i=1}^n\frac{d}{dx_i}\varphi_i(t,\,x,\,u)+\psi(t,\,x,\,u)=0$} in the class of bounded measurable functions. We define the generalized solution and prove existence, uniqueness and stability theorems for this solution. To prove the existence theorem we apply the "vanishing viscosity method"; in this connection, we first study Cauchy's problem for the corresponding parabolic equation, and we derive a priori estimates of the modulus of continuity in ##IMG## [http://ej.iop.org/images/0025-5734/10/2/A06/tex_sm_2156_img2.gif] {$ L_1$} of the solution of this problem which do not depend on small viscosity. Bibliography: 22 items.},
	number={2},
}

@BOOK{Richtmyer1967,
	AUTHOR = {Richtmyer, Robert D. AND Morton, K. W.},
	YEAR = {1967},
	TITLE = {Difference Methods for Initial-value Problems},
	PUBLISHER = {Wiley-Interscience},
	ADDRESS = {New York},
}

@Article{Strang1964,
	author = {Strang, Gilbert},
	title = {Accurate Partial Difference Methods II. Non-Linear Problems},
	journal = {Numerische Mathematik},
	year = {1964},
	volume = {6},
	pages = {37--46},
}

@BOOK{Laurien2009,
	AUTHOR = {Laurien, Eckart AND jr., Herbert Oertel},
	YEAR = {2009},
	TITLE = {Numerische Str{\"o}mungsmechanik - Grundgleichungen - L{\"o}sungsmethoden - Softwarebeispiele},
	EDITION = {3. vollst. {\"u}berarb. u. erw. Aufl. 2009},
	ISBN = {978-3-834-80533-1},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
}

@Article{MacCormack1969,
	author = {MacCormack, R. W.},
	title = {The effects of viscosity in hypervelocity impact cratering},
	journal = {American Institute of Aeronautics and Astronautics},
	year = {1969},
	pages = {69--354},
}

@booklet{Angleitner2013,
	title = {Modellierung mit partiellen Differentialgleichungen},
	author = {Angleitner, Niklas},
	language={German},
	month = mar,
	year = {2013},
	url={www.asc.tuwien.ac.at/~melenk/teach/modellieren_WS1213/skript.pdf},
	howpublished = {Technische Universit{\"a}t Wien}
}

@article{leshno1993multilayer,
	title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
	author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
	journal={Neural networks},
	volume={6},
	number={6},
	pages={861--867},
	year={1993},
	publisher={Elsevier},
	url={https://doi.org/10.1016/S0893-6080(05)80131-5}
}
@article{poggio1990networks,
  title={Networks for approximation and learning},
  author={Poggio, Tomaso and Girosi, Federico},
  journal={Proceedings of the IEEE},
  volume={78},
  number={9},
  pages={1481--1497},
  year={1990},
  publisher={IEEE},
  url={https://doi.org/10.1109/5.58326}
}

@article{mei2018mean,
	title={A mean field view of the landscape of two-layer neural networks},
	author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	journal={Proceedings of the National Academy of Sciences},
	volume={115},
	number={33},
	pages={E7665--E7671},
	year={2018},
	publisher={National Acad Sciences},
	url={https://doi.org/10.1073/pnas.1806579115}
}
@inproceedings{jacot2018neural,
	title={Neural tangent kernel: Convergence and generalization in neural networks},
	author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	booktitle={Advances in neural information processing systems},
	pages={8571--8580},
	year={2018},
	url={https://arxiv.org/abs/1806.07572v3}
}
@inproceedings{chizat2018global,
	title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
	author={Chizat, Lenaic and Bach, Francis},
	booktitle={Advances in neural information processing systems},
	pages={3036--3046},
	year={2018},
	url={https://arxiv.org/abs/1805.09545v2}
}

@article{wahba1978improper,
  title={Improper priors, spline smoothing and the problem of guarding against model errors in regression},
  author={Wahba, Grace},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={40},
  number={3},
  pages={364--372},
  year={1978},
  publisher={Wiley Online Library},
  url={https://doi.org/10.1111/j.2517-6161.1978.tb01050.x}
}

@article{hastie1986,
author = "Hastie, Trevor and Tibshirani, Robert",
doi = "10.1214/ss/1177013604",
fjournal = "Statistical Science",
journal = "Statist. Sci.",
month = "08",
number = "3",
pages = "297--310",
publisher = "The Institute of Mathematical Statistics",
title = "Generalized Additive Models",
url = "https://doi.org/10.1214/ss/1177013604",
volume = "1",
year = "1986"
}

@article{friedman1981projection,
  title={Projection pursuit regression},
  author={Friedman, Jerome H and Stuetzle, Werner},
  journal={Journal of the American statistical Association},
  volume={76},
  number={376},
  pages={817--823},
  year={1981},
  publisher={Taylor \& Francis},
  url={ https://doi.org/10.1080/01621459.1981.10477729}
}
@article{10.2307/2344614,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2344614},
 abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
 author = {J. A. Nelder and R. W. M. Wedderburn},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {3},
 pages = {370--384},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Generalized Linear Models},
 volume = {135},
 year = {1972}
}

@book{guide2006infinite,
  title={Infinite dimensional analysis},
  author={Guide, A Hitchhiker’s},
  year={2006},
  publisher={Springer},
  URL={https://link.springer.com/content/pdf/10.1007/3-540-29587-9.pdf}
}

@InProceedings{pmlr-v89-ali19a,
	title = 	 {A Continuous-Time View of Early Stopping for Least Squares Regression},
	author = 	 {Alnur Ali and J. Zico Kolter and Ryan J. Tibshirani},
	booktitle = 	 {Proceedings of Machine Learning Research},
	pages = 	 {1370--1378},
	year = 	 {2019},
	editor = 	 {Kamalika Chaudhuri and Masashi Sugiyama},
	volume = 	 {89},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {},
	month = 	 {16--18 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v89/ali19a/ali19a.pdf},
	url = 	 {http://proceedings.mlr.press/v89/ali19a.html},
	abstract = 	 {We study the statistical properties of the iterates generated by gradient descent, applied to the fundamental problem of least squares regression. We take a continuous-time view, i.e., consider infinitesimal step sizes in gradient descent, in which case the iterates form a trajectory called gradient flow.  Our primary focus is to compare the risk of gradient flow to that of ridge regression. Under the calibration $t=1/\lambda$—where $t$ is the time parameter in gradient flow, and $\lambda$ the tuning parameter in ridge regression—we prove that the risk of gradient flow is no less than 1.69 times that of ridge, along the entire path (for all $t \geq 0$). This holds in finite samples with very weak assumptions on the data model (in particular, with no assumptions on the features $X$). We prove that the same relative risk bound holds for prediction risk, in an average sense over the underlying signal $\beta_0$.   Finally, we examine limiting risk expressions (under standard Marchenko-Pastur asymptotics), and give supporting numerical experiments.}
}

@techreport{friedman2003gradient,
	title={Gradient directed regularization for linear regression and classification},
	author={Friedman, Jerome and Popescu, Bogdan E},
	year={2003},
	institution={Technical Report, Statistics Department, Stanford University},
	url={https://www.researchgate.net/profile/Jerome_Friedman/publication/244258820_Gradient_Directed_Regularization_for_Linear_Regression_and_Classification/links/5afe22ce458515e9a57639de/Gradient-Directed-Regularization-for-Linear-Regression-and-Classification.pdf}
}


@book{munkres2014topology,
  title={Topology},
  author={Munkres, James},
  year={2014},
  publisher={Pearson Education Limited},
  edition = {Second},
  isbn = {978-1-292-02362-5},
}

@book{elstrodt2018massintegrationstheorie,
  title={Ma{\ss}- und Integrationstheorie},
  author={Elstrodt, J{\"u}rgen},
  year={2018},
  publisher={Springer Spektrum Berlin, Heidelberg},
  edition = {Eighth},
  isbn = {978-3-662-57939-8},
}

@book{rudin1991functionalanalysis,
  title={Functional Analysis},
  author={Rudin, Walter},
  year={1991},
  publisher={McGraw-Hill Inc.},
  edition = {Second},
  isbn = {0-07-100944-2},
}

@article{agrawal2020wide,
  title={Wide neural networks with bottlenecks are deep Gaussian processes},
  author={Agrawal, Devanshu and Papamarkou, Theodore and Hinkle, Jacob},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={175},
  year={2020},
  publisher={Oak Ridge National Lab.(ORNL), Oak Ridge, TN (United States)}
}


@article{parhi2022kinds,
  title={What kinds of functions do deep neural networks learn? Insights from variational spline theory},
  author={Parhi, Rahul and Nowak, Robert D},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={4},
  number={2},
  pages={464--489},
  year={2022},
  publisher={SIAM}
}


@article{parhi2021banach,
  title={Banach Space Representer Theorems for Neural Networks and Ridge Splines.},
  author={Parhi, Rahul and Nowak, Robert D},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={43},
  pages={1--40},
  year={2021}
}

@article{unser2019representer,
  title={A representer theorem for deep neural networks.},
  author={Unser, Michael},
  journal={J. Mach. Learn. Res.},
  volume={20},
  number={110},
  pages={1--30},
  year={2019}
}


@article{Part3Arxiv,
  author    = {Jakob Heiss and
               Josef Teichmann and
               Hanna Wutte},
  title     = {Infinite wide (finite depth) Neural Networks benefit from multi-task
               learning unlike shallow Gaussian Processes - an exact quantitative
               macroscopic characterization},
  journal   = {CoRR},
  volume    = {abs/2112.15577},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.15577},
  eprinttype = {arXiv},
  eprint    = {2112.15577},
  timestamp = {Sat, 09 Apr 2022 12:27:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-15577.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{HUANG2006489,
title = {Extreme learning machine: Theory and applications},
journal = {Neurocomputing},
volume = {70},
number = {1},
pages = {489-501},
year = {2006},
note = {Neural Networks},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2005.12.126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206000385},
author = {Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew},
keywords = {Feedforward neural networks, Back-propagation algorithm, Extreme learning machine, Support vector machine, Real-time learning, Random node},
abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.11For the preliminary idea of the ELM algorithm, refer to “Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks”, Proceedings of International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.}
}


@misc{OptimalStoppingRNarxiv.2104.13669,
  doi = {10.48550/ARXIV.2104.13669},
  
  url = {https://arxiv.org/abs/2104.13669},
  
  author = {Herrera, Calypso and Krach, Florian and Ruyssen, Pierre and Teichmann, Josef},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Numerical Analysis (math.NA), Probability (math.PR), Computational Finance (q-fin.CP), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, FOS: Economics and business, FOS: Economics and business},
  
  title = {Optimal Stopping via Randomized Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@inproceedings{NEURIPS2018_6459257d,
 author = {Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Connecting Optimization and Regularization Paths},
 url = {https://proceedings.neurips.cc/paper/2018/file/6459257ddab7b85bf4b57845e875e4d4-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{ImplRegPart1V3,
  author    = {Jakob Heiss and
               Josef Teichmann and
               Hanna Wutte},
  title     = {How implicit regularization of Neural Networks affects the learned
               function - Part {I}},
  journal   = {CoRR},
  volume    = {abs/1911.02903},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02903},
  eprinttype = {arXiv},
  eprint    = {1911.02903},
  timestamp = {Sat, 23 Jan 2021 01:13:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{MomentumGFImplReg,
  doi = {10.48550/ARXIV.2201.05405},
  
  url = {https://arxiv.org/abs/2201.05405},
  
  author = {Wang, Li and Zhou, Yingcong and Fu, Zhiguo},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Implicit Regularization of Momentum Gradient Descent with Early Stopping},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
