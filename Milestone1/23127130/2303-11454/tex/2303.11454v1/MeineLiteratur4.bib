@misc{doubleDescentbelkin2018reconciling,
    title={Reconciling modern machine learning practice and the bias-variance trade-off},
    author={Mikhail Belkin and Daniel Hsu and Siyuan Ma and Soumik Mandal},
    year={2018},
    eprint={1812.11118},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/1812.11118v2}
}

@misc{implReg1,
    title="{How implicit regularization of Neural Networks affects the learned function {--} Part I}",
    author={Jakob Heiss and Josef Teichmann and Hanna Wutte},
    year={2019},
    month=Nov,
    eprint={1911.02903},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/1911.02903}
} 

@book{ben2003generalizedPseudoInverse,
  title={Generalized inverses: theory and applications},
  author={Ben-Israel, Adi and Greville, Thomas NE},
  volume={15},
  year={2003},
  publisher={Springer Science \& Business Media},
  url={https://doi.org/10.1007\%2Fb97366}
}


@book{Adams:SobolevSpaces1990498,
      author        = "Adams, Robert A and Fournier, John J F",
      title         = "{Sobolev spaces; 2nd ed.}",
      publisher     = "Academic Press",
      address       = "New York, NY",
      series        = "Pure and applied mathematics",
      year          = "2003",
      url           = "http://cds.cern.ch/record/1990498",
}

@ARTICLE{BalduzziBrownianMotionNN2017arXiv170208591B,
       author = {{Balduzzi}, David and {Frean}, Marcus and {Leary}, Lennox and
         {Lewis}, JP and {Wan-Duo Ma}, Kurt and {McWilliams}, Brian},
        title = "{The Shattered Gradients Problem: If resnets are the answer, then what is the question?}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2017",
        month = Feb,
          eid = {arXiv:1702.08591},
        pages = {arXiv:1702.08591},
archivePrefix = {arXiv},
       eprint = {1702.08591},
 primaryClass = {cs.NE},
 url={https://arxiv.org/abs/1702.08591v2},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170208591B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{ITOApproxNNWithoutScale1991817,
title = "Approximation of functions on a compact set by finite sums of a sigmoid function without scaling",
journal = "Neural Networks",
volume = "4",
number = "6",
pages = "817 - 826",
year = "1991",
issn = "0893-6080",
doi = "10.1016/0893-6080(91)90060-I",
url = "https://doi.org/10.1016/0893-6080(91)90060-I",
author = "Yoshifusa Ito",
keywords = "Heaviside function, Sigmoid function, Unscaled sigmoid function, Discriminatory, Strongly discriminatory, Linear combination, Uniform approximation, Compact set",
abstract = "This paper is concerned with three layered feedforward neural networks which are capable of approximately representing continuous functions on a compact set. Of particular interest here is the use of a sigmoid function without scaling. First, we prove existentially that a linear combination of unscaled shifted rotations of any sigmoid function can approximate uniformly an arbitrary continuous function on a compact set in Rd. Second, a proposition is proved constructively using the fact that a homogeneous polynomial P can be expressed as P(x) = Σi=1nai(ωi · x)r. It states that the approximation of an arbitrary polynomial on a interval in R can be extended to that of an arbitrary continuous function on a compact set in Rd. Then, four corollaries are derived. Though their statements are more or less restricted, the proofs provide algorithms for implementing the uniform approximation. In three of these corollaries, sigmoid functions are used without scaling."
}

@book{hastie_09_elements-of.statistical-learning,
  added-at = {2010-06-03T15:15:09.000+0200},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/200d858c0bd2826d4eb5f39450192d1f5/ukoethe},
  edition = 2,
  file = {:Books\\HastieTibshiraniFriedman-09-Elements-of-Statistical-Learning-2nd-edition\\hastie_09_elements-of.statistical-learning.pdf:PDF},
  interhash = {52d1772f39be836e3b298d37b8c0cfa1},
  intrahash = {00d858c0bd2826d4eb5f39450192d1f5},
  keywords = {inference mathmatics dataanalysis method clutering statistics},
  publisher = {Springer},
  timestamp = {2010-06-03T15:15:09.000+0200},
  title = {The elements of statistical learning: data mining, inference and prediction},
  url = {http://www-stat.stanford.edu/~tibs/ElemStatLearn/},
  year = 2009,
  doi = {10.1007/978-0-387-84858-7},
  isbn={978-0-387-84858-7},
  issn={0172-7397}
}

@ARTICLE{SoudryImplicitBiasSeparableData2017arXiv171010345S,
       author = {{Soudry}, Daniel and {Hoffer}, Elad and {Shpigel Nacson}, Mor and
         {Gunasekar}, Suriya and {Srebro}, Nathan},
        title = "{The Implicit Bias of Gradient Descent on Separable Data}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2017",
        month = Oct,
          eid = {arXiv:1710.10345},
        pages = {arXiv:1710.10345},
archivePrefix = {arXiv},
       eprint = {1710.10345},
 primaryClass = {stat.ML},
 url={https://arxiv.org/abs/1710.10345v4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171010345S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{GidelImplicitDiscreteRegularizationDeepLinearNN2019arXiv190413262G,
       author = {{Gidel}, Gauthier and {Bach}, Francis and {Lacoste-Julien}, Simon},
        title = "{Implicit Regularization of Discrete Gradient Dynamics in Deep Linear Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
         year = "2019",
        month = Apr,
          eid = {arXiv:1904.13262},
        pages = {arXiv:1904.13262},
archivePrefix = {arXiv},
       eprint = {1904.13262},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1904.13262v1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190413262G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@techreport{friedman2003gradient,
  title={Gradient directed regularization for linear regression and classification},
  author={Friedman, Jerome and Popescu, Bogdan E},
  year={2003},
  institution={Technical Report, Statistics Department, Stanford University},
  url={https://www.researchgate.net/profile/Jerome_Friedman/publication/244258820_Gradient_Directed_Regularization_for_Linear_Regression_and_Classification/links/5afe22ce458515e9a57639de/Gradient-Directed-Regularization-for-Linear-Regression-and-Classification.pdf}
}


@ARTICLE{PoggioGernalizationDeepNN2018arXiv180611379P,
       author = {{Poggio}, Tomaso and {Liao}, Qianli and {Miranda}, Brando and
         {Banburski}, Andrzej and {Boix}, Xavier and {Hidary}, Jack},
        title = "{Theory IIIb: Generalization in Deep Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = "2018",
        month = Jun,
          eid = {arXiv:1806.11379},
        pages = {arXiv:1806.11379},
archivePrefix = {arXiv},
       eprint = {1806.11379},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1806.11379v1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180611379P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{MaennelDradDecentPicewiseLinear2018arXiv180308367M,
       author = {{Maennel}, Hartmut and {Bousquet}, Olivier and {Gelly}, Sylvain},
        title = "{Gradient Descent Quantizes ReLU Network Features}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2018",
        month = Mar,
          eid = {arXiv:1803.08367},
        pages = {arXiv:1803.08367},
archivePrefix = {arXiv},
       eprint = {1803.08367},
 primaryClass = {stat.ML},
          url = {https://arxiv.org/abs/1803.08367v1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180308367M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@Article{CravenSpline1978,
author="Craven, Peter
and Wahba, Grace",
title="Smoothing noisy data with spline functions",
journal="Numerische Mathematik",
year="1978",
month=Dec,
day="01",
volume="31",
number="4",
pages="377--403",
abstract="Smoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.",
issn="0945-3245",
doi="10.1007/BF01404567",
url="https://doi.org/10.1007/BF01404567"
}

@Article{ReinschSpline1967,
author="Reinsch, Christian H.",
title="Smoothing by spline functions",
journal="Numerische Mathematik",
year="1967",
month=Oct,
day="01",
volume="10",
number="3",
pages="177--183",
issn="0945-3245",
doi="10.1007/BF02162161",
url="https://doi.org/10.1007/BF02162161"
}


@ARTICLE{NeyshaburImplicitRegPhD2017arXiv170901953N,
       author = {{Neyshabur}, Behnam},
        title = "{Implicit Regularization in Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2017",
        month = Sep,
          eid = {arXiv:1709.01953},
        pages = {arXiv:1709.01953},
archivePrefix = {arXiv},
       eprint = {1709.01953},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1709.01953v2},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170901953N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{NeyshaburImplicitReg2014arXiv1412.6614N,
       author = {{Neyshabur}, Behnam and {Tomioka}, Ryota and {Srebro}, Nathan},
        title = "{In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = "2014",
        month = Dec,
          eid = {arXiv:1412.6614},
        pages = {arXiv:1412.6614},
archivePrefix = {arXiv},
       eprint = {1412.6614},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1412.6614v4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6614N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{YuanzhiImplicitRegPseudoSmoothing2018arXiv180801204L,
       author = {{Li}, Yuanzhi and {Liang}, Yingyu},
        title = "{Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2018",
        month = Aug,
          eid = {arXiv:1808.01204},
        pages = {arXiv:1808.01204},
archivePrefix = {arXiv},
       eprint = {1808.01204},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1808.01204v3},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180801204L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{BishopCurvatureSmoothingNeuralNetworks248466,
author={Bishop, Chris M.},
journal={IEEE Transactions on Neural Networks},
title={Curvature-driven smoothing: a learning algorithm for feedforward networks},
year={1993},
volume={4},
number={5},
pages={882-884},
abstract={The performance of feedforward neural networks in real applications can often be improved significantly if use is made of a priori information. For interpolation problems this prior knowledge frequently includes smoothness requirements on the network mapping, and can be imposed by the addition to the error function of suitable regularization terms. The new error function, however, now depends on the derivatives of the network mapping, and so the standard backpropagation algorithm cannot be applied. In this letter, we derive a computationally efficient learning algorithm, for a feedforward network of arbitrary topology, which can be used to minimize such error functions. Networks having a single hidden layer, for which the learning algorithm simplifies, are treated as a special case.<>},
keywords={feedforward neural nets;learning (artificial intelligence);curvature-driven smoothing;learning algorithm;feedforward neural networks;computationally efficient learning algorithm;Smoothing methods;Backpropagation algorithms;Training data;Neural networks;Interpolation;Network topology;Multilayer perceptrons;Mean square error methods;Transfer functions;Feedforward neural networks},
doi={10.1109/72.248466},
ISSN={1045-9227},
month=Sep,
url={https://pdfs.semanticscholar.org/3c39/ddfbd9822230b5375d581bf505ecf6255283.pdf}
}

@article{BishopRegularizationNoisedoi:10.1162/neco.1995.7.1.108,
author = {Bishop, Chris M.},
title = {Training with Noise is Equivalent to Tikhonov Regularization},
journal = {Neural Computation},
volume = {7},
number = {1},
pages = {108-116},
year = {1995},
doi = {10.1162/neco.1995.7.1.108},
URL = {https://doi.org/10.1162/neco.1995.7.1.108},
eprint = {https://doi.org/10.1162/neco.1995.7.1.108},
abstract = { It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise. }
}

@InProceedings{bishop1995regularizationEarlyStopping,
author = {Bishop, Chris M.},
title = {Regularization and Complexity Control in Feed-forward Networks},
booktitle = {Proceedings International Conference on Artificial Neural Networks ICANN'95},
year = {1995},
month = Jan,
abstract = {In this paper we consider four alternative approaches to complexity control in feed-forward networks based respectively on architecture selection, regularization, early stopping, and training with noise. We show that there are close similarities between these approaches and we argue that, for most practical applications, the technique of regularization should be the method of choice.},
publisher = {EC2 et Cie},
url = {https://www.microsoft.com/en-us/research/publication/regularization-and-complexity-control-in-feed-forward-networks/},
pages = {141-148},
volume = {1},
edition = {Proceedings International Conference on Artificial Neural Networks ICANN'95}
}

@ARTICLE{ADAM2014arXiv1412.6980K,
       author = {{Kingma}, Diederik P. and {Ba}, Jimmy},
        title = "{Adam: A Method for Stochastic Optimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2014",
        month = Dec,
          eid = {arXiv:1412.6980},
        pages = {arXiv:1412.6980},
archivePrefix = {arXiv},
       eprint = {1412.6980},
 primaryClass = {cs.LG},
          url = {https://arxiv.org/abs/1412.6980},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{Goodfellow-et-al-DeepLearning-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{HornikUniversalApprox1991251,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
issn = "0893-6080",
doi = "10.1016/0893-6080(91)90009-T",
url = "https://doi.org/10.1016/0893-6080(91)90009-T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.",
pdfurl="http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf",
}

@Article{CybenkoUniversalApprox1989,
author="Cybenko, G.",
title="Approximation by superpositions of a sigmoidal function",
journal="Mathematics of Control, Signals and Systems",
year="1989",
month=Dec,
day="01",
volume="2",
number="4",
pages="303--314",
abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
issn="1435-568X",
doi="10.1007/BF02551274",
url="https://doi.org/10.1007/BF02551274",
pdfurl="https://www.researchgate.net/profile/George_Cybenko/publication/226439292_Approximation_by_superpositions_of_a_sigmoidal_function_Math_Cont_Sig_Syst_MCSS_2303-314/links/551d50c90cf23e2801fe12cf/Approximation-by-superpositions-of-a-sigmoidal-function-Math-Cont-Sig-Syst-MCSS-2303-314.pdf"
}


@article{KimeldorfSplineBayes10.2307/2239347,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2239347},
 author = {George S. Kimeldorf and Grace Wahba},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {495--502},
 publisher = {Institute of Mathematical Statistics},
 title = {A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines},
 volume = {41},
 year = {1970}
}

@book{bishop2006patternML,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher="Springer",
  address="New York, NY",
  series="Information science and statistics",
  url="http://cds.cern.ch/record/998831",
  isbn={978-0387-31073-2},
  issn={1613-9011}
}

@ARTICLE{MatthewsGaussProc2018arXiv,
	author = {{Matthews}, Alexander G. de G. and {Rowland}, Mark and {Hron}, Jiri and
	{Turner}, Richard E. and {Ghahramani}, Zoubin},
	title = "{Gaussian Process Behaviour in Wide Deep Neural Networks}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2018",
	month = Apr,
	eid = {arXiv:1804.11271},
	pages = {arXiv:1804.11271},
	archivePrefix = {arXiv},
	eprint = {1804.11271},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180411271M},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	url = {https://arxiv.org/abs/1804.11271}
}

@ARTICLE{KuboImplicitReg2019arXiv,
	author = {{Kubo}, Masayoshi and {Banno}, Ryotaro and {Manabe}, Hidetaka and
	{Minoji}, Masataka},
	title = "{Implicit Regularization in Over-parameterized Neural Networks}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = "2019",
	month = Mar,
	eid = {arXiv:1903.01997},
	pages = {arXiv:1903.01997},
	archivePrefix = {arXiv},
	eprint = {1903.01997},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190301997K},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	url={https://arxiv.org/abs/1903.01997}
}

@thesis{MasterThesisJakobHeiss,
	title = "{Implicit Regularization for Artificial Neural Networks}",
	year=2019,
	author={Heiss, Jakob M.},
	adress={Z{\"u}rich},
	note={work in progress}
}

@ARTICLE{BayesianInverseProblems2013arXiv,
	author = {{Dashti}, Masoumeh and {Stuart}, Andrew M.},
	title = "{The Bayesian Approach To Inverse Problems}",
	journal = {arXiv e-prints},
	keywords = {Mathematics - Probability},
	year = "2013",
	month = Feb,
	eid = {arXiv:1302.6989},
	pages = {arXiv:1302.6989},
	archivePrefix = {arXiv},
	eprint = {1302.6989},
	primaryClass = {math.PR},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1302.6989D},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	url={https://arxiv.org/abs/1302.6989}
}

@article{SmartMarkets,
	author = {Mccabe, Kevin and Rassenti, Stephen and Smith, Vernon},
	year = {1991},
	month = 11,
	pages = {534-8},
	title = {Smart Computer-Assisted Markets},
	volume = {254},
	journal = {Science (New York, N.Y.)},
	doi = {10.1126/science.254.5031.534},
	url = {https://doi.org/10.1126/science.254.5031.534}
}

@ARTICLE{2019arXivDeepFictitiousPlay,
	author = {{Hu}, Ruimeng},
	title = "{Deep Fictitious Play for Stochastic Differential Games}",
	journal = {arXiv e-prints},
	keywords = {Mathematics - Optimization and Control, Computer Science - Computer Science and Game Theory, Statistics - Machine Learning},
	year = "2019",
	month = Mar,
	eid = {arXiv:1903.09376},
	pages = {arXiv:1903.09376},
	archivePrefix = {arXiv},
	eprint = {1903.09376},
	primaryClass = {math.OC},
	url={https://arxiv.org/abs/1903.09376},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190309376H},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{bianchini2014complexity,
	author={M. {Bianchini} and F. {Scarselli}},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	title={On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures},
	year={2014},
	volume={25},
	number={8},
	pages={1553-1565},
	keywords={computational complexity;feedforward neural nets;pattern classification;topology;neural network classifiers;shallow architecture;deep architecture;artificial neural network;hidden layers;vision;human language understanding;feedforward neural network;high complexity functions;topological concepts;function complexity evaluation;classification;sigmoidal activation function;deep network;Complexity theory;Neurons;Biological neural networks;Polynomials;Upper bound;Computer architecture;Betti numbers;deep neural networks;function approximation;topological complexity;Vapnik--Chervonenkis dimension (VC-dim).;Betti numbers;deep neural networks;function approximation;topological complexity;Vapnik–Chervonenkis dimension (VC-dim);Algorithms;Computer Simulation;Models, Theoretical;Nerve Net;Pattern Recognition, Automated},
	doi={10.1109/TNNLS.2013.2293637},
	ISSN={2162-237X},
	month=Aug,
	url={https://doi.org/10.1109/TNNLS.2013.2293637}
}

@article{tsitsiklis1994asynchronous,
	title={Asynchronous stochastic approximation and Q-learning},
	author={Tsitsiklis, John N},
	journal={Machine learning},
	volume={16},
	number={3},
	pages={185--202},
	year={1994},
	publisher={Springer},
	url={https://doi.org/10.1007/BF00993306}
}

@article{shaham2018provable,
	title={Provable approximation properties for deep neural networks},
	author={Shaham, Uri and Cloninger, Alexander and Coifman, Ronald R},
	journal={Applied and Computational Harmonic Analysis},
	volume={44},
	number={3},
	pages={537--557},
	year={2018},
	publisher={Elsevier},
	url={https://doi.org/10.1016/j.acha.2016.04.003}
}

@article{kimeldorf1970,
	author = "Kimeldorf, George S. and Wahba, Grace",
	doi = "10.1214/aoms/1177697089",
	fjournal = "The Annals of Mathematical Statistics",
	journal = "Ann. Math. Statist.",
	month = Apr,
	number = "2",
	pages = "495--502",
	publisher = "The Institute of Mathematical Statistics",
	title = "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines",
	url = "https://doi.org/10.1214/aoms/1177697089",
	volume = "41",
	year = "1970"
}

@misc {meineBac_V15,
	title = "{Preisbildung und {\"u}berbestimmte Ungleichungssysteme}",
	year=2017,
	author={Heiss, Jakob M.},
	url={https://www.dropbox.com/s/ojywjawntmxt9zz/BacArbeit15.pdf?dl=0},
	adress={Wien}
}

@misc {meineBacV15,
	title = {Preisbildung und {\"u}berbestimmte Ungleichungssysteme},
	year=2017,
	month=mar,
	day=27,
	note = {(Version 15: {\texttt{BacArbeit15.pdf}})},
	author={Heiss, Jakob M.},
	adress={Wien}
}

@misc {meineBacV13,
	title = {Preisbildung und {\"u}berbestimmte Ungleichungssysteme},
	year=2017,
	month=feb,
	day=13,
	note = {(Version 13: {\texttt{BacArbeit13.pdf}})},
	author={Heiss, Jakob M.},
	adress={Wien}
}

@book{LinProgVanderbei2013,
	title={Linear Programming: Foundations and Extensions},
	author={Vanderbei, R.J.},
	isbn={9781475756623},
	series={International Series in Operations Research {\&} Management Science},
	url={https://books.google.at/books?id=HjznBwAAQBAJ},
	year={2013},
	publisher={Springer US}
}



@misc {XETRA_DeutscheBoerseFortlaufendeAuktion,
	title = {XETRA{\textsuperscript{\textregistered}} Release 16.0 Marktmodell Fortlaufende Auktion},
	url = {http://www.xetra.com/blob/1618544/fd5abeaa556dcbe4b7e571d181f85973/data/Marktmodell-fortlaufende-auktion.pdf},
	year=2015,
	month=sep,
	day=21,
	note = {[Online; Stand 10. Februar 2017]},
	author={{Deutsche B{\"o}rse AG}},
}

@misc {WienerBoerseAgb2_1,
	title = {Handelsregeln f{\"u}r das automatisierte Handelssystem XETRA{\textsuperscript{\textregistered}} (Exchange Electronic Trading)},
	url = {https://www.wienerborse.at/uploads/u/cms/files/recht/agb/agb-2-1.pdf},
	year=2017,
	month=feb,
	day=1,
	note = {[Online; Stand 10. Februar 2017]},
	author={{Wiener B{\"o}rse AG}},
	adress={Wien},
	SERIES = {Allgemeine Gesch{\"a}ftsbedingungen der Wiener B{\"o}rse AG},
	NUMBER = {2.1}
}
@misc {test123,
	title = {Handelsregeln f{\"u}r das automatisierte Handelssystem XETRA{$^\textregistered$} (Exchange Electronic Trading)},
	url = {https://www.wienerborse.at/uploads/u/cms/files/recht/agb/agb-2-1.pdf},
	year=2017,
	month=2,
	booktitle = {Allgemeine Gesch{\"a}ftsbedingungen der Wiener B{\"o}rse AG}
}

@article {BrainardFisher,
	author = {Brainard, William C. and Scarf, Herbert E.},
	title = {How to Compute Equilibrium Prices in 1891},
	journal = {American Journal of Economics and Sociology},
	volume = 64,
	number = 1,
	publisher = {Blackwell Publishing Ltd.},
	issn = {1536-7150},
	url = {http://dx.doi.org/10.1111/j.1536-7150.2005.00349.x},
	doi = {10.1111/j.1536-7150.2005.00349.x},
	pages = {57--83},
	year = 2005,
}

@article{salahirobustL1SolutionInequalitiesErrorInA,
  title={Robust {$\ell_1$} and {$\ell_{\infty}$} Solutions of Linear Inequalities},
  author={Salahi, Maziar},
  url={http://www.pvamu.edu/Include/Math/AAM/AAM%20Vol.%206,%20Issue%2012%20(December%202011)/09_%20Salahi%20AAM-R390-MS-032811.pdf},
  journal={Applications and Applied Mathematics},
  ISSN= {1932-9466},
  year=2011,
  pages={522--528},
  volume=6,
  number=2  
}

@book{bloomfield2012leastAbsolutDerivation,
  title={Least Absolute Deviations: Theory, Applications and Algorithms},
  author={Bloomfield, Peter and Steiger, William},
  isbn={9781468485745},
  series={Progress in Probability},
  volume=6,
  url={https://books.google.at/books?id=NRTTBwAAQBAJ},
  year={2012},
  publisher={Birkh{\"a}user Boston}
}


@Article{Rosen2000L1LinearEquations,
author="Rosen, J. Ben
and Park, Haesun
and Glick, John
and Zhang, Lei",
title="Accurate Solution to Overdetermined Linear Equations with Errors Using L1 Norm Minimization",
journal="Computational Optimization and Applications",
year=2000,
volume=17,
number=2,
pages="329--341",
abstract="It has been known for many years that a robust solution to an overdetermined system of linear equations Ax ≈ b is obtained by minimizing the L1 norm of the residual error. A correct solution x to the linear system can often be obtained in this way, in spite of large errors (outliers) in some elements of the (m {\texttimes} n) matrix A and the data vector b. This is in contrast to a least squares solution, where even one large error will typically cause a large error in x. In this paper we give necessary and sufficient conditions that the correct solution is obtained when there are some errors in A and b. Based on the sufficient condition, it is shown that if k rows of [Ab] contain large errors, the correct solution is guaranteed if (m − n)/n ≥ 2k/$\sigma$, where $\sigma$ > 0, is a lower bound of singular values related to A. Since m typically represents the number of measurements, this inequality shows how many data points are needed to guarantee a correct solution in the presence of large errors in some of the data. This inequality is, in fact, an upper bound, and computational results are presented, which show that the correct solution will be obtained, with high probability, for much smaller values of m − n.",
issn="1573-2894",
doi="10.1023/A:1026562601717",
url="http://dx.doi.org/10.1023/A:1026562601717"
}


@article{EfronRobusRegressionL1Equation,
 ISSN = {00361445},
 URL = {http://www.jstor.org/stable/2030700},
 abstract = {This is a survey of modern developments in statistical regression, written for the mathematically educated nonstatistician. It begins with a review of the traditional theory of least-squares curve-fitting. Modern developments in regression theory have developed in response to the practical limitations of the least-squares approach. Recent progress has been made feasible by the electronic computer, which frees statisticians from the confines of mathematical tractability. Topics discussed include robust regression, bootstrap measures of variability, local smoothing and cross-validation, projection pursuit, Mallows' Cp criterion, Stein estimation, generalized regression for Poisson data, and regression methods for censored data. All of the methods are illustrated with real-life examples.},
 author = {Efron, Bradley},
 journal = {SIAM Review},
 number = 3,
 pages = {421--449},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {Computer-Intensive Methods in Statistical Regression},
 volume = 30,
 year = 1988
}



@article{HollandRobustRegression,
author = {Holland, Paul W. and Welsch, Roy E.},
title = {Robust regression using iteratively reweighted least-squares},
journal = {Communications in Statistics - Theory and Methods},
volume = 6,
number = 9,
pages = {813--827},
year = 1977,
doi = {10.1080/03610927708827533},
URL = {http://dx.doi.org/10.1080/03610927708827533},
eprint = {http://dx.doi.org/10.1080/03610927708827533},
abstract = { The rapid development of the theory of robust estimation (Huber, 1973) has created a need for computational procedures to produce robust estimates. We will review a number of different computational approaches for robust linear regression but focus on one—iteratively reweighted least-squares (IRLS). The weight functions that we discuss are a part of a semi-portable subroutine library called ROSEPACK (RObust Statistical Estimation PACKage) that has been developed by the authors and Virginia Klema at the Computer Research Center of the National Bureau of Economic Research, Inc. in Cambridge, Mass. with the support of the National Science Foundation. This library (Klema, 1976) makes it relatively simple to implement an IRLS regression package. }
}

@article{HuberRobustRegression,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2958283},
 abstract = {Maximum likelihood type robust estimates of regression are defined and their asymptotic properties are investigated both theoretically and empirically. Perhaps the most important new feature is that the number p of parameters is allowed to increase with the number n of observations. The initial terms of a formal power series expansion (essentially in powers of p/n) show an excellent agreement with Monte Carlo results, in most cases down to 4 observations per parameter.},
 author = {Huber, Peter J.},
 journal = {The Annals of Statistics},
 number = 5,
 pages = {799--821},
 publisher = {Institute of Mathematical Statistics},
 title = {Robust Regression: Asymptotics, Conjectures and Monte Carlo},
 language={English},
 month=sep,
 volume = 1,
 year = 1973
}




@article{Pollard_1991L1Equations,
adress={New York, USA}, title={Asymptotics for Least Absolute Deviation Regression Estimators}, volume={7}, url={http://www.math.pku.edu.cn/teachers/xirb/Courses/QR2013/Pollard91ET.pdf}, DOI={10.1017/S0266466600004394}, abstract={The LAD estimator of the vector parameter in a linear regression is defined by minimizing the sum of the absolute values of the residuals. This paper provides a direct proof of asymptotic normality for the LAD estimator. The main theorem assumes deterministic carriers. The extension to random carriers includes the case of autoregressions whose error terms have finite second moments. For a first-order autoregression with Cauchy errors the LAD estimator is shown to converge at a 1/n rate.}, number={2}, journal={Econometric Theory}, publisher={Cambridge University Press}, author={Pollard, David}, year={1991}, month=jun, pages={186--199}
}

@misc{vimentisGleichgewichtspreis,
author={VIMENTIS},
title={Lexikon: Gleichgewichtspreis},
year=2011,
url={https://www.vimentis.ch/d/lexikon/179/Gleichgewichtspreis.html},
note = {[Online; Stand 19. November 2016]}
}


@book{legendre1805nouvellesLeastSquares,
  title={Nouvelles m{\'e}thodes pour la d{\'e}termination des orbites des com{\`e}tes},
  author={Legendre, Adrien Marie},
  series={Analyse des triangles trac{\'e}s sur la surface d'un sph{\'e}roide},
  url={https://books.google.at/books?id=7C9RAAAAYAAJ},
  number={1},
  year={1805},
  publisher={F. Didot}
}



@book{gauss1809LeastSquares1,
  title={Theoria motus corporum coelestium in sectionibus conicis solem ambientium auctore Carolo Friderico Gauss},
  author={Gau{\ss}, Carl Friedrich},
  year={1809},
  publisher={sumtibus Frid. Perthes et IH Besser},
  url={https://books.google.at/books?id=VKhu8yPcat8C}
}
@book{gauss1823theoriaLeastSquares2,
  title={Theoria combinationis observationum erroribus minimis obnoxiae.-Gottingae, Henricus Dieterich 1823},
  author={Gau{\ss}, Carl-Friedrich},
  year={1823},
  url={https://books.google.at/books?id=hrZQAAAAcAAJ},
  publisher={Henricus Dieterich}
}



@Article{Dax2008LeastSquareInequalities,
author="Dax, Achiya",
title="A hybrid algorithm for solving linear inequalities in a least squares sense",
journal="Numerical Algorithms",
year="2008",
volume="50",
number="2",
pages="97",
abstract="The need for solving a system of linear inequalities, A                        x ≥ b, arises in many applications. Yet in some cases the system to be solved turns out to be inconsistent due to measurement errors in the data vector b. In such a case it is often desired to find the smallest correction of b that recovers feasibility. That is, we are looking for a small nonnegative vector, y ≥ 0, for which the modified system A                        x ≥ b - y is solvable. The problem of calculating the smallest correction vector is called the least deviation problem. In this paper we present new algorithms for solving this problem. Numerical experiments illustrate the usefulness of the proposed methods.",
issn="1572-9265",
doi="10.1007/s11075-008-9218-3",
url="http://dx.doi.org/10.1007/s11075-008-9218-3"
}


@article{SPINGARN1987LeastSquareInequalities,
title = "A projection method for least-squares solutions to overdetermined systems of linear inequalities",
journal = "Linear Algebra and its Applications",
volume = "86",
number = "",
pages = "211--236",
year = "1987",
issn = "0024-3795",
doi = "10.1016/0024-3795(87)90296-5",
url = "http://www.sciencedirect.com/science/article/pii/0024379587902965",
author = "Spingarn, Jonathan E.",
abstract = "An algorithm previously introduced by the author for finding a feasible point of a system of linear inequalities is further investigated. For inconsistent systems, it is shown to generate a sequence converging at a linear rate to the set of least-squares solutions. The algorithm is a projection-type method, and is a manifestation of the proximal-point algorithm."
}

@Article{Lei2015LeastSquareInequalities,
author="Lei, Yuan",
title="The inexact fixed matrix iteration for solving large linear inequalities in a least squares sense",
journal="Numerical Algorithms",
year="2015",
volume="69",
number="1",
pages="227--251",
abstract="A fixed matrix iteration algorithm was proposed by A. Dax (Numer. Algor. 50, 97--114 2009) for solving linear inequalities in a least squares sense. However, a great deal of computation for this algorithm is required, especially for large-scale problems, because a least squares subproblem should be solved accurately at each iteration. We present a modified method, the inexact fixed iteration method, which is a generalization of the fixed matrix iteration method. In this inexact iteration process, the classical LSQR method is implemented to determine an approximate solution of each least squares subproblem with less computational effort. The convergence of this algorithm is analyzed and several numerical examples are presented to illustrate the efficiency of the inexact fixed matrix iteration algorithm for solving large linear inequalities.",
issn="1572-9265",
doi="10.1007/s11075-014-9892-2",
url="http://dx.doi.org/10.1007/s11075-014-9892-2"
}

@article{BramleyLeastSquareInequalities,
author = {Bramley, R. and Winnicka, B.},
title = {Solving Linear Inequalities in a Least Squares Sense},
journal = {SIAM Journal on Scientific Computing},
volume = {17},
number = {1},
pages = {275--286},
year = {1996},
doi = {10.1137/0917020},
url = {https://pdfs.semanticscholar.org/4339/381ab90b55481882df7db26fd6c596848945.pdf},
eprint = {http://dx.doi.org/10.1137/0917020}
}

@book{han1980leastSquares,
  title={Least-squares Solution of Linear Inequalities},
  author={Han, S.P. and {WISCONSIN UNIV-MADISON MATHEMATICS RESEARCH CENTER.}},
  language={English},
  series={MRC technical summary report},
  number=2141,
  url={www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA096658},
  year={1980},
  month = nov,
  publisher={Defense Technical Information Center},
  abstract={The paper deals with a system of linear inequalities in a finite dimensional
  space. When the system is inconsistent, we are interested in vectors that
  satisfy the system in a least-squares sense. We characterize such leastsquares
  solutions and propose a method to find one of these solutions. It is
  shown that for any given system of linear inequalities and for any starting
  point, the method can produce a solution in a finite number of iterations.
  Computational results are very satisfactory in terms of accuracy and number
  of iterations.}
}
@techreport{han1980leastAlternativBibTeX,
  title={Least-Squares Solution of Linear Inequalities.},
  author={Han, S-P},
  year={1980},
  number=2141,
  institution={DTIC Document}
}


@article{PoundDropNYT,
 title = {Pound Drops Sharply in Asian Trading Amid `Brexit' Fears},
 note = {[Online; Stand 18. November 2016]},
 journal = {The New York Times},
 author = {Bray, Chud and Gaugh, Neil},
 language={English},
 month = oct,
 year = {2016},
 url = {www.nytimes.com/2016/10/08/business/dealbook/britain-pound-currency-flash-crash.html}
}

 @misc{wiki:LinProg,
   author = "Wikipedia",
   title = "Lineare Optimierung --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2016",
   url = "https://de.wikipedia.org/w/index.php?title=Lineare_Optimierung&oldid=158785906",
   note = "[Komplexit{\"a}t: \url{https://de.wikipedia.org/w/index.php?title=Lineare_Optimierung&oldid=158785906#Komplexit.C3.A4t_und_L.C3.B6sungsverfahren}; Dualit{\"a}t: \url{https://de.wikipedia.org/w/index.php?title=Lineare_Optimierung&oldid=158785906#Dualit.C3.A4t}; Online; Stand 18. November 2016]"
 }
 
 @misc{ wiki:Marktpreis,
   author = "Wikipedia",
   title = "Marktpreis --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2016",
   url = "https://de.wikipedia.org/w/index.php?title=Marktpreis&oldid=159232739",
   note = "[Online; Stand 18. November 2016]"
 }

 @misc{ wiki:Marshall,
   author = "Wikipedia",
   title = "Alfred Marshall --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2016",
   url = "https://de.wikipedia.org/w/index.php?title=Alfred_Marshall&oldid=150595089",
   note = "[Online; Stand 18. November 2016]"
 }

@misc{DualesProblemUniWien,
   author = {Breunig, Franz},
   booktitle = {Simplex-Programm mit Online-Skriptum},
   title = {Das Duale Problem},
   year = {1997},
   howpublished = {Universit{\"a}t Wien},
   url = {http://www.unet.univie.ac.at/~a0025537/php/Abschnitt1/Mathe/fb_skript3.html},
   note = {[Online; abgerufen am 18. November 2016]}
 }

@article{DantzigWolfeDecomposition,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1911818},
 abstract = {A procedure is presented for the efficient computational solution of linear programs having a certain structural property characteristic of a large class of problems of practical interest. The property makes possible the decomposition of the problem into a sequence of small linear programs whose iterated solutions solve the given problem through a generalization of the simplex method for linear programming.},
 author = {Dantzig, George B. and Wolfe, Philip},
 journal = {Econometrica},
 number = {4},
 pages = {767--778},
 publisher = {[Wiley, Econometric Society]},
 title = {The Decomposition Algorithm for Linear Programs},
 volume = {29},
 year = {1961},
 month = oct,
 DOI = {10.2307/1911818}
}

 
 
 @phdthesis{TebbothDantzigWolfeDecomposition,
 title = {A Computational Study of Dantzig-Wolfe Decomposition},
 author = {Tebboth, James Richard},
 url = {http://www.blisworthhouse.co.uk/OR/Decomposition/tebboth.pdf},
 year = {2001},
 language={English},
 month = dec,
 school = {University of Buckingham},
 abstract = {{This thesis evaluates the computational merits of the Dantzig-Wolfe decomposition algorithm. We use modern computer hardware and software, and, in particular, we have developed an efficient parallel implementation of Dantzig-Wolfe decomposition. We were able to solve problems of up to 83,500 rows, 83,700 columns, and 622,000 non-zero elements, and one important instance was solved in 63 \% of the time taken by a commercial implementation of the simplex method. If we were content with a solution guaranteed to be within 0.1 \% of optimality, then the solution time can be cut by a further 32\%. Using parallel hardware we can cut the solution time further: by 59 \% on a four processor PC (equivalent to a speed-up of 2.42) or 81 \% on seven processors of a Silicon Graphics workstation (a speed-up of 5.22). Dantzig-Wolfe decomposition is an optimisation technique for solving large scale, block structured, linear programming (LP) problems. Problems from many different fields such as production planning, refinery optimisation, and resource allocation may be formulated as LP problems. Where there is some structure arising from repeated components in the problem, such as the handling of multi-periods}},
 contributor = {{The Pennsylvania State University CiteSeerX Archives}},
 language = {{en}},
 }
 
 @BOOK{DiskAlgCormen2009,
 	AUTHOR = {Cormen, Thomas H. AND Leiserson, Charles E. AND Rivest, Ronald L. AND Stein, Clifford},
 	YEAR = {2009},
 	TITLE = {Introduction to Algorithms - },
 	EDITION = {3rd edition.},
 	ISBN = {978-0-262-03384-8},
 	PUBLISHER = {MIT Press},
 	ADDRESS = {Cambridge},
 }
 
 @misc{DieSiedlerOnline,
   author = {{Blue Byte}},
   title = {Die Siedler Online},
   url = {http://www.diesiedleronline.de},
   urldate = {2016-11-17},
   howpublished ={Ubisoft},
   note = {[Online; Stand 17. November 2016]}
 }
 @Booklet{GeneralEquilibriumSkript,
	title = {General Equilibrium},
	author = {Levin, Jonathan},
	howpublished = {Stanford University},
	language={English},
	month = nov,
	year = {2006},
	url = {https://web.stanford.edu/~jdlevin/Econ%20202/General%20Equilibrium.pdf}
}
@Booklet{MikroOekonomieSkript,
	title = {Mikro{\"o}konomie - Vorlesung 2: Pr{\"a}ferenzen und Nutzenfunktionen},
	author = {F{\"u}rnkranz-Prskawetz,  Alexia},
	howpublished = {Technische Universit{\"a}t Wien},
	day = {18},
	language={German},
	month = mar,
	year = {2016},
	url = {http://www.econ.tuwien.ac.at/lva/mikro.vo/slides/Vorlesung_2_2016.pdf}
}
@article{ArrowDebreu1954existence,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1907353},
 abstract = {A. Wald has presented a model of production and a model of exchange and proofs of the existence of an equilibrium for each of them. Here proofs of the existence of an equilibrium are given for an integrated model of production, exchange and consumption. In addition the assumptions made on the technologies of producers and the tastes of consumers are significantly weaker than Wald's. Finally a simplification of the structure of the proofs has been made possible through use of the concept of an abstract economy, a generalization of that of a game.},
 author={Arrow, Kenneth J and Debreu, Gerard},
 journal = {Econometrica},
 number = {3},
 pages = {265--290},
 publisher = {[Wiley, Econometric Society]},
 title = {Existence of an Equilibrium for a Competitive Economy},
 volume = {22},
 year = {1954},
 DOI={10.2307/1907353}
}



@INCOLLECTION{generalEquilibriumDictionary,
  TITLE  =  "general equilibrium",
  BOOKTITLE  =  "The New Palgrave Dictionary of Economics",
  AUTHOR  =  "McKenzie, Lionel W.",
  EDITOR  =  "Durlauf, Steven N. and Blume, Lawrence E.",
  YEAR  =  "2008",
  PUBLISHER  =  "Palgrave Macmillan",
  ADDRESS  =  "Basingstoke",
  URL = {http://www.dictionaryofeconomics.com/article?id=pde2008_G000023&q=general%20equilibrium}
}

@book{WalrasOriginal,
  title={{\'E}l{\'e}ments d'{\'e}conomie politique pure; ou, Th{\'e}orie de la richesse sociale},
  author={Walras, L{\'e}on},
  year={1896},
  publisher={F. Rouge},
  adress={Lausanne},
  url={http://gallica.bnf.fr/ark:/12148/bpt6k111752b},
  note={franz. Original. Engl. {\"U}bersetzung: \cite{WalrasTranslate}}
}

@book{WalrasTranslate,
  title={Elements of Pure Economics: Or, the Theory of Social Wealth. Translated by William Jaffe},
  author={Walras, L{\'e}on},
  year={1954},
  publisher={Published for the American Economic Association and the Royal Economic Society},
  note={Engl. {\"U}bersetzung von \cite{WalrasOriginal}},
  catalogue-url = { http://nla.gov.au/nla.cat-vn773542 }
}

@incollection{MarktpreisGabler,
  title = {Marktpreis},
  publisher = {Springer Gabler Verlag (Herausgeber)},
  booktitle = {Gabler Wirtschaftslexikon},
  author = {Piekenbrock, Dirk },
  url = {http://wirtschaftslexikon.gabler.de/Archiv/54571/marktpreis-v6.html},
  note = "[Online; Stand 19. November 2016]"
}


@incollection{homogen,
  title = {homogene G{\"u}ter},
  publisher = {Springer Gabler Verlag (Herausgeber)},
  booktitle = {Gabler Wirtschaftslexikon},
  author = {Steven, Marion and Piekenbrock, Dirk },
  url = {http://wirtschaftslexikon.gabler.de/Archiv/7002/homogene-gueter-v8.html},
  note = "[Online; Stand 19. November 2016]"
  }
  % Stichwort: homogene Güter, online im Internet: 

 @article{DaxL1Solution,
  author = {Dax, Achiya},
  title = {The L1 Solution of Linear Inequalities},
  journal = {Comput. Stat. Data Anal.},
  issue_date = {January, 2006},
  volume = {50},
  number = {1},
  language={English},
  month = jan,
  year = {2006},
  issn = {0167-9473},
  pages = {40--60},
  numpages = {21},
  url = {http://dx.doi.org/10.1016/j.csda.2004.07.007},
  doi = {10.1016/j.csda.2004.07.007},
  acmid = {1648001},
  publisher = {Elsevier Science Publishers B. V.},
  address = {Amsterdam, The Netherlands, The Netherlands},
  keywords = {A new theorem of the alternative, Affine scaling, Duality, Inconsistent systems, Least absolute deviations, Linear inequalities},
 }
 
 @Article{DaxSmallestCorrection,
 author="Dax, Achiya",
 title="The Smallest Correction of an Inconsistent System of Linear Inequalities",
 journal="Optimization and Engineering",
 year="2001",
 volume="2",
 number="3",
 pages="349--359",
 abstract="The problem of calculating a point x that satisfies a given system of linear inequalities, Ax {\#}x2265; b, arises in many applications. Yet often the system to be solved turns out to be inconsistent due to measurement errors in the data vector, b. In such a case it is useful to find the smallest perturbation of b that recovers feasibility. This motivates our interest in the least correction problem and its properties.",
 issn="1573-2924",
 doi="10.1023/A:1015370617219",
 url="http://dx.doi.org/10.1023/A:1015370617219"
 }
 
 @BOOK{Marshall,
 title = {The Principles of Economics},
 author = {Marshall, Alfred},
 year = {1890},
 publisher = {McMaster University Archive for the History of Economic Thought},
 edition = {8. Edition},
 url = {http://eet.pixel-online.org/files/etranslation/original/Marshall,%20Principles%20of%20Economics.pdf}
 }
 
 @misc{ instabilWiki,
   author = "Wikipedia",
   title = "Von-Neumann-Stabilit{\"a}tsanalyse --- Wikipedia{,} Die freie Enzyklop{\"a}die",
   year = "2014",
   url = "https://de.wikipedia.org/w/index.php?title=Von-Neumann-Stabilit%C3%A4tsanalyse&oldid=126537447",
   note = "[Online; Stand 27. August 2015]",
 }

@BOOK{LeVeque,
	AUTHOR = {LeVeque, Randall J.},
	YEAR = {2012},
	TITLE = {Numerical Methods for Conservation Laws - },
	EDITION = {2. Aufl.},
	ISBN = {978-3-7643-2723-1},
	PUBLISHER = {Springer},
	ADDRESS = {Berlin, Heidelberg},
}
%http://www.springer.com/us/book/9783764327231
%- am Ende?


@incollection{Godlewski,
	AUTHOR = {Godlewski, Edwige AND Raviart, Pierre-Arnaud},
	YEAR = {1992},
	NUMBER = {118},
	SERIES = {Applied Mathematical Sciences},
	TITLE = {Numerical Approximation of Hyperbolic Systems of Conservation Laws - },
	PUBLISHER = {Springer Verlag},
	ADDRESS = {Berlin Heidelberg},
}
%- am Ende?


@book{Egartner,
	author = {Wolfgang Egartner},
	title = {Grundlagen der Numerik physikalischer Erhaltungsgesetze},
	publisher = {IWR, Universit{\"a}t Heidelberg},
	year = {1998},
}
%institution = {IWR, Universit{\"a}t Heidelberg}

@misc{ WikibooksErhaltungsgleichungen,
   author = "Wikibooks",
   title = "Theorie und Numerik von Erhaltungsgleichungen --- Wikibooks{,} Die freie Bibliothek",
   year = "2015",
   url = "https://de.wikibooks.org/w/index.php?title=Theorie_und_Numerik_von_Erhaltungsgleichungen&oldid=572189",
   note = "[Online; abgerufen am 1. September 2015]"
 }


@BOOK{EvansPDE,
	AUTHOR = {Evans, Lawrence C.},
	YEAR = {2010},
	TITLE = {Partial Differential Equations - },
	EDITION = {2. Aufl.},
	ISBN = {978-0-821-84974-3},
	PUBLISHER = {American Mathematical Soc.},
	ADDRESS = {Heidelberg},
}

@Booklet{ParDiffSkript,
	title = {Partielle Differentialgleichungen},
	author = {Univ.-Prof. Dr. Ansgar J{\"u}ngel},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = dec,
	year = {2013},
}

@Booklet{fana,
	title = {Funktionalanalysis},
	author = {Harald Woracek AND Michael Kaltenb{\"a}ck AND Martin Bl{\"u}mlinger},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = mar,
	year = {2014},
	edition = {9. Aufl.}
}
@Booklet{fana2016,
	title = {Funktionalanalysis},
	author = {Harald Woracek AND Michael Kaltenb{\"a}ck AND Martin Bl{\"u}mlinger},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = feb,
	year = {2016},
	edition = {11. Aufl.},
	url = {http://www.asc.tuwien.ac.at/funkana/skripten/fana2016.pdf}
}


@BOOK{Arens2013,
	AUTHOR = {Arens, Tilo AND Busam, Rolf AND Hettlich, Frank AND Karpfinger, Christian AND Stachel, Hellmuth AND Lichtenegger, Klaus},
	YEAR = {2013},
	TITLE = {Grundwissen Mathematikstudium - Analysis und Lineare Algebra mit Querverbindungen},
	EDITION = {1. Aufl.},
	ISBN = {978-3-827-42309-2},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
}

@BOOK{Appell2009,
	AUTHOR = {Appell, J{\"u}rgen},
	YEAR = {2009},
	TITLE = {Analysis in Beispielen und Gegenbeispielen - Eine Einf{\"u}hrung in die Theorie reeller Funktionen},
	EDITION = {1. Aufl.},
	ISBN = {978-3-540-88903-8},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
}

@BOOK{Kuso,
	AUTHOR = {Kusolitsch, Norbert},
	YEAR = {2014},
	TITLE = {Ma{\ss} und Wahrscheinlichkeitstheorie - Eine Einf{\"u}hrung},
	EDITION = {2. Aufl.},
	ebookISBN = {978-3-642-45387-8},
	ISBN = {978-3-642-45386-1},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
	DOI = {10.1007/978-3-642-45387-8},
	URL = {http://www.springer.com/de/book/9783642453861}
}

@Article{LaxWendroff,
	author = {Lax, Peter D. AND Wendroff, Burton},
	title = {Systems of Conservation Laws},
	journal = {COMMUNICATIONS ON PURE AND APPLIED MATHEMATICS},
	year = {1960},
	volume = {13},
	pages = {217--237},
}

@Booklet{SchmeiserErhaltungssaetze,
	title = {Numerische Methoden f{\"u}r hyperbolische Erhaltungss{\"a}tze},
	author = {Christian Schmeiser},
	howpublished = {Technische Universit{\"a}t Wien},
	url = {http://homepage.univie.ac.at/christian.schmeiser/teaching1.htm}
}

@Booklet{Arnoldzeitabh,
	title = {Zeitabh{\"a}ngige Probleme in Physik und Technik},
	author = {Anton Arnold},
	howpublished = {Technische Universit{\"a}t Wien},
	language={German},
	month = jan,
	year = {2014},
}

@Article{BianchiniVanishingViscosity,
	author = {Bianchini, Stefan AND Bressan, Alberto},
	title = {Vanishing viscosity solutions of nonlinear hyperbolic systems},
	journal = {Annals of Mathematics},
	year = {2005},
	volume = {161},
	pages = {223--342},
}

@Article{Oleinik,
	author = {Oleinik, Olga},
	title = {Discontinuous solutions of non-linear differential equations},
	journal = {American Mathematical Society Translations},
	year = {1963},
	volume = {26},
	pages = {95--172},
}

@Article{Kruzhkov,
	author={Kru{\v z}kov, S. N.},
	title = {First-order quasilinear equations with several space variables},
	journal = {Mathematics of the USSR-Sbornik},
	year = {1970},
	volume = {10},
	pages = {217--243},
	url="http://stacks.iop.org/0025-5734/10/i=2/a=A06",
	abstract={In this paper we construct a theory of generalized solutions in the large of Cauchy's problem for the equations ##IMG## [http://ej.iop.org/images/0025-5734/10/2/A06/tex_sm_2156_img1.gif] {$\displaystyle u_t+\sum_{i=1}^n\frac{d}{dx_i}\varphi_i(t,\,x,\,u)+\psi(t,\,x,\,u)=0$} in the class of bounded measurable functions. We define the generalized solution and prove existence, uniqueness and stability theorems for this solution. To prove the existence theorem we apply the "vanishing viscosity method"; in this connection, we first study Cauchy's problem for the corresponding parabolic equation, and we derive a priori estimates of the modulus of continuity in ##IMG## [http://ej.iop.org/images/0025-5734/10/2/A06/tex_sm_2156_img2.gif] {$ L_1$} of the solution of this problem which do not depend on small viscosity. Bibliography: 22 items.},
	number={2},
}

@BOOK{Richtmyer1967,
	AUTHOR = {Richtmyer, Robert D. AND Morton, K. W.},
	YEAR = {1967},
	TITLE = {Difference Methods for Initial-value Problems},
	PUBLISHER = {Wiley-Interscience},
	ADDRESS = {New York},
}

@Article{Strang1964,
	author = {Strang, Gilbert},
	title = {Accurate Partial Difference Methods II. Non-Linear Problems},
	journal = {Numerische Mathematik},
	year = {1964},
	volume = {6},
	pages = {37--46},
}

@BOOK{Laurien2009,
	AUTHOR = {Laurien, Eckart AND jr., Herbert Oertel},
	YEAR = {2009},
	TITLE = {Numerische Str{\"o}mungsmechanik - Grundgleichungen - L{\"o}sungsmethoden - Softwarebeispiele},
	EDITION = {3. vollst. {\"u}berarb. u. erw. Aufl. 2009},
	ISBN = {978-3-834-80533-1},
	PUBLISHER = {Springer-Verlag},
	ADDRESS = {Berlin Heidelberg New York},
}

@Article{MacCormack1969,
	author = {MacCormack, R. W.},
	title = {The effects of viscosity in hypervelocity impact cratering},
	journal = {American Institute of Aeronautics and Astronautics},
	year = {1969},
	pages = {69--354},
}

@booklet{Angleitner2013,
	title = {Modellierung mit partiellen Differentialgleichungen},
	author = {Angleitner, Niklas},
	language={German},
	month = mar,
	year = {2013},
	url={www.asc.tuwien.ac.at/~melenk/teach/modellieren_WS1213/skript.pdf},
	howpublished = {Technische Universit{\"a}t Wien}
}

@article{leshno1993multilayer,
	title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
	author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
	journal={Neural networks},
	volume={6},
	number={6},
	pages={861--867},
	year={1993},
	publisher={Elsevier},
	url={https://doi.org/10.1016/S0893-6080(05)80131-5}
}
@article{poggio1990networks,
  title={Networks for approximation and learning},
  author={Poggio, Tomaso and Girosi, Federico},
  journal={Proceedings of the IEEE},
  volume={78},
  number={9},
  pages={1481--1497},
  year={1990},
  publisher={IEEE},
  url={https://doi.org/10.1109/5.58326}
}

@article{mei2018mean,
	title={A mean field view of the landscape of two-layer neural networks},
	author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	journal={Proceedings of the National Academy of Sciences},
	volume={115},
	number={33},
	pages={E7665--E7671},
	year={2018},
	publisher={National Acad Sciences},
	url={https://doi.org/10.1073/pnas.1806579115}
}
@inproceedings{jacot2018neural,
	title={Neural tangent kernel: Convergence and generalization in neural networks},
	author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	booktitle={Advances in neural information processing systems},
	pages={8571--8580},
	year={2018},
	url={https://arxiv.org/abs/1806.07572v3}
}
@inproceedings{chizat2018global,
	title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
	author={Chizat, Lenaic and Bach, Francis},
	booktitle={Advances in neural information processing systems},
	pages={3036--3046},
	year={2018},
	url={https://arxiv.org/abs/1805.09545v2}
}

@article{wahba1978improper,
  title={Improper priors, spline smoothing and the problem of guarding against model errors in regression},
  author={Wahba, Grace},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={40},
  number={3},
  pages={364--372},
  year={1978},
  publisher={Wiley Online Library},
  url={https://doi.org/10.1111/j.2517-6161.1978.tb01050.x}
}

@article{hastie1986,
author = "Hastie, Trevor and Tibshirani, Robert",
doi = "10.1214/ss/1177013604",
fjournal = "Statistical Science",
journal = "Statist. Sci.",
month = "08",
number = "3",
pages = "297--310",
publisher = "The Institute of Mathematical Statistics",
title = "Generalized Additive Models",
url = "https://doi.org/10.1214/ss/1177013604",
volume = "1",
year = "1986"
}

@article{friedman1981projection,
  title={Projection pursuit regression},
  author={Friedman, Jerome H and Stuetzle, Werner},
  journal={Journal of the American statistical Association},
  volume={76},
  number={376},
  pages={817--823},
  year={1981},
  publisher={Taylor \& Francis},
  url={ https://doi.org/10.1080/01621459.1981.10477729}
}
@article{10.2307/2344614,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2344614},
 abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
 author = {J. A. Nelder and R. W. M. Wedderburn},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {3},
 pages = {370--384},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Generalized Linear Models},
 volume = {135},
 year = {1972}
}

@book{guide2006infinite,
  title={Infinite dimensional analysis},
  author={Guide, A Hitchhiker’s},
  year={2006},
  publisher={Springer},
  URL={https://link.springer.com/content/pdf/10.1007/3-540-29587-9.pdf}
}

@article{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1902.05040},
  year={2019},
  URL={https://arxiv.org/abs/1902.05040}
}

@inproceedings{williams2019gradient,
  title={Gradient dynamics of shallow univariate relu networks},
  author={Williams, Francis and Trager, Matthew and Panozzo, Daniele and Silva, Claudio and Zorin, Denis and Bruna, Joan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8378--8387},
  year={2019},
  URL={http://papers.nips.cc/paper/9046-gradient-dynamics-of-shallow-univariate-relu-networks.pdf}
}

@article{ongie2019function,
  title={A function space view of bounded norm infinite width relu nets: The multivariate case},
  author={Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1910.01635},
  year={2019},
  URL={https://arxiv.org/pdf/1910.01635.pdf}
}



@InProceedings{pmlr-v89-ali19a,
  title = 	 {A Continuous-Time View of Early Stopping for Least Squares Regression},
  author = 	 {Alnur Ali and J. Zico Kolter and Ryan J. Tibshirani},
  booktitle = 	 {Proceedings of Machine Learning Research},
  pages = 	 {1370--1378},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Masashi Sugiyama},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/ali19a/ali19a.pdf},
  url = 	 {http://proceedings.mlr.press/v89/ali19a.html},
  abstract = 	 {We study the statistical properties of the iterates generated by gradient descent, applied to the fundamental problem of least squares regression. We take a continuous-time view, i.e., consider infinitesimal step sizes in gradient descent, in which case the iterates form a trajectory called gradient flow.  Our primary focus is to compare the risk of gradient flow to that of ridge regression. Under the calibration $t=1/\lambda$—where $t$ is the time parameter in gradient flow, and $\lambda$ the tuning parameter in ridge regression—we prove that the risk of gradient flow is no less than 1.69 times that of ridge, along the entire path (for all $t \geq 0$). This holds in finite samples with very weak assumptions on the data model (in particular, with no assumptions on the features $X$). We prove that the same relative risk bound holds for prediction risk, in an average sense over the underlying signal $\beta_0$.   Finally, we examine limiting risk expressions (under standard Marchenko-Pastur asymptotics), and give supporting numerical experiments.}
}

@article{ImplRegPart1V3,
  author    = {Jakob Heiss and
               Josef Teichmann and
               Hanna Wutte},
  title     = {How implicit regularization of Neural Networks affects the learned
               function - Part {I}},
  journal   = {CoRR},
  volume    = {abs/1911.02903},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02903},
  eprinttype = {arXiv},
  eprint    = {1911.02903},
  timestamp = {Sat, 23 Jan 2021 01:13:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@phdthesis{kreuter2019vector,
  title={Vector-valued elliptic boundary value problems on rough domains},
  author={Kreuter, Marcel},
  year={2019},
  school={Universit{\"a}t Ulm}
}
@article{implRegPart3V4,
  author    = {Jakob Heiss and
               Josef Teichmann and
               Hanna Wutte},
  title     = {Infinite wide (finite depth) Neural Networks benefit from multi-task
               learning unlike shallow Gaussian Processes - an exact quantitative
               macroscopic characterization},
  journal   = {CoRR},
  volume    = {abs/2112.15577},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.15577},
  eprinttype = {arXiv},
  eprint    = {2112.15577},
  timestamp = {Sat, 09 Apr 2022 12:27:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-15577.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{MomentumGFImplReg,
  doi = {10.48550/ARXIV.2201.05405},
  
  url = {https://arxiv.org/abs/2201.05405},
  
  author = {Wang, Li and Zhou, Yingcong and Fu, Zhiguo},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Implicit Regularization of Momentum Gradient Descent with Early Stopping},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
