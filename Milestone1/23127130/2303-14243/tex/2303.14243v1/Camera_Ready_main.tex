%\textbf{}% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{arydshln}
\usepackage{siunitx}
\sisetup{
    group-separator = {,},
    group-minimum-digits = 3
}
%\usepackage[accsupp]{axessibility}

\makeatletter
\def\adl@drawiv#1#2#3{%
        \hskip.5\tabcolsep
        \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}%
                #2\z@ plus1fil minus1fil\relax
        \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{%
  \noalign{\vskip\aboverulesep
           \global\let\@dashdrawstore\adl@draw
           \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}
  \noalign{\global\let\adl@draw\@dashdrawstore
           \vskip\belowrulesep}}
\makeatother
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{8412} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
%\title{Hyper-R2L: Distilling Dynamics to Light Field Networks with Higher-Dimensional Representations
%\title{DyLiN: Deformable Light Field Network with Higher-Dimensional Representations
\title{DyLiN: Making Light Field Networks Dynamic
}

%\thanks{Corresponding author.}
\author{Heng Yu$^{1}$ \quad Joel Julin$^{1}$  \quad Zolt\'{a}n \'{A}. Milacski$^{1}$\quad Koichiro Niinuma$^{2}$ \quad L\'{a}szl\'{o} A. Jeni$^{1}$ \vspace{4pt}\\
	$^1$Robotics Institute, Carnegie Mellon University \quad
    $^2$Fujitsu Research of America \\
    {\tt\small \{hengyu, jjulin, zmilacsk\}@andrew.cmu.edu} \quad {\tt\small kniinuma@fujitsu.com} \quad {\tt\small laszlojeni@cmu.edu} \\
}
%\iffalse
%\author{Heng Yu \\
%Carnegie Mellon University\\
% Institution1 address\\
%{\tt\small hengyu@andrew.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Joel Julin\\
%Carnegie Mellon University\\
% First line of institution2 address\\
%{\tt\small jjulin@andrew.cmu.edu}
%\and
%Zoltán Á. Milacski\\
%Carnegie Mellon University\\
% First line of institution2 address\\
%{\tt\small zmilacsk@andrew.cmu.edu}
%\and
%Koichiro Niinuma\\
%Fujitsu Research of America\\
% First line of institution2 address\\
%{\tt\small kniinuma@fujitsu.com}
%\and
%László A. Jeni\\
%Carnegie Mellon University\\
% First line of institution2 address\\
%{\tt\small laszlojeni@cmu.edu}
%}
%\fi

% \maketitle
% \begin{figure*}[h]
%   \centering
% %   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.99\linewidth]{imgs/front_compare.pdf}

%   \caption{}
%   \label{fig:cor2l2}
% \end{figure*}
%\maketitle

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
%\iffalse
\vspace{-1.04cm}
%\vspace{-0.6175cm}
\begin{center}
    \centering
    \captionsetup{type=figure}
    %\includegraphics[width=.8\textwidth,height=5cm]{example-image}
    \begin{minipage}{0.15\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/gt_americano.png} \\
    %\footnotesize
    Ground Truth \\
    Video%\phantom{a}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/hyper_americano.png} \\
    HyperNeRF \cite{park2021hypernerf}  \\
    Render time: $\SI{3}{\second}$
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/neu_americano.png} \\
    TiNeuVox \cite{tineuvox} \\
    Render time: $\SI{7}{\second}$
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/ours_americano.png} \\
    Ours \\
    Render time: $\SI{0.1}{\second}$
    \end{minipage}
    \begin{minipage}{0.28\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/plot.png} \\
    \emph{}
    \end{minipage}
    \captionof{figure}{Our proposed DyLiN for dynamic 3D scene rendering achieves higher quality than its HyperNeRF teacher model and the state-of-the-art TiNeuVox model, while being an order of magnitude faster.
    Right: DyLiN is of moderate storage size (shown by dot radii).
    %$\Delta\text{PSNR}$ over NeRF is for each methods best performing scene (real/dynamic).
    For each method, the relative improvement in Peak Signal-to-Noise Ratio over NeRF ($\Delta\text{PSNR}$) is measured for the best-performing scene.
    }
    %We train DyLiN via knowledge distillation and it also outperforms its HyperNeRF teacher model.}
    \label{fig:fancy}
\end{center}%
%\fi
}]


%\twocolumn[{%
%\renewcommand\twocolumn[1][]{#1}%
%\maketitle


%\begin{center}
%    \centering

%    \begin{minipage}{0.18\textwidth}
%    \centering
%    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/001.pdf} \\
    %\footnotesize
%    Ground Truth \\
%    \phantom{a}
%    \end{minipage}
%    \begin{minipage}{0.18\textwidth}
%    \centering
%    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/003.pdf} \\
%    HyperNeRF \cite{park2021hypernerf} \\
%    Render Time: $\SI{3}{\second}$
%    \end{minipage}
%    \begin{minipage}{0.18\textwidth}
%    \centering
%    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/004.pdf} \\
%    TiNeuVox \cite{tineuvox} \\
%    Render Time: $\SI{7}{\second}$
%    \end{minipage}
%    \begin{minipage}{0.18\textwidth}
%    \centering
%    \includegraphics[width=0.99\textwidth]{imgs/real_results/front/002.pdf} \\
%    Ours \\
%    Render Time: $\SI{0.1}{\second}$
%    \end{minipage}
%    \begin{minipage}{0.18\textwidth}
%    \centering
%    \includegraphics[width=0.99\textwidth]{imgs/plot.png} \\
%    \emph{}
%    \end{minipage}
%    \begin{minipage}
%    \centering
%    \includegraphics[width=0.99\textwidth]{imgs/plot.png} \\
%    \emph{}
%    \end{minipage}

    %\includegraphics[width=0.54\textwidth]{imgs/front_compare.pdf}
    %\includegraphics[width=0.44\textwidth]{imgs/plot.png}
%    \captionof{figure}{some text here}
%    \label{fig:overview}
%\end{center}%
%}]


% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% \begin{center}
%     \centering
% \begin{figure}
% \begin{subfigure}{0.5\textwidth}
%   \centering
%   \includegraphics[width=0.9\linewidth]{imgs/front_compare.pdf}
%   \caption{Number of sampled points}
%   \label{fig:ablation_sampled_point}
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}
%   \centering
%   \includegraphics[width=0.9\linewidth]{imgs/front_compare.pdf}
%   \caption{Number of pseudo samples}
%   \label{fig:ablation_pseudo_samples}
% \end{subfigure}
% \end{figure}
% \end{center}%
% }]




%%%%%%%%% ABSTRACT
\begin{abstract}
% \vspace{-0.38cm}
Light Field Networks, the re-formulations of radiance fields to oriented rays, are magnitudes faster than their coordinate network counterparts, and provide higher fidelity with respect to representing 3D structures from 2D observations. They would be well suited for generic scene representation and manipulation, but suffer from one problem: they are limited to holistic and static scenes.
In this paper, we propose the Dynamic Light Field Network (DyLiN) method that can handle non-rigid deformations, including topological changes. We learn a deformation field from input rays to canonical rays, and  lift them into a higher dimensional space to handle discontinuities.
We further introduce CoDyLiN, which augments DyLiN with controllable attribute inputs.
We train both models via knowledge distillation from pretrained dynamic radiance fields.
We evaluated DyLiN using both synthetic and real world datasets that include various non-rigid deformations.
%We evaluated DyLiN using both synthetic and real world datasets that include non-rigid deformations of varying difficulty and type.
DyLiN qualitatively outperformed and quantitatively matched state-of-the-art methods in terms of visual fidelity, while being $25 - 71\times$ computationally faster.
%DyLiN outperformed state-of-the art methods in terms of visual fidelity ($1.4 - 2.8 dB$ average PSNR improvement) and compute complexity ($25 - 71 \times$ render time speedup).
We also tested CoDyLiN on attribute annotated data and it surpassed its teacher model.
Project page: \url{https://dylin2023.github.io}.
%For more information, visit our project page: \url{https:/dylin2023.github.io}.
%For more information, visit our project page: \url{https:/dylin2023.github.io}.
%Our source code and rendered videos are publicly available on our project page: \url{http:/dylin2023.github.io}.
%For more information, visit our project page
%{dylin2023.github.io}.
%See \href{http:/dylin2023.github.io}{dylin2023.github.io} for a more immersive view of our results.

%   Neural Radiance Field (NeRF) and its variants show great performances on implicitly representing 3D static and dynamic scenes.
%   However, their inference process is inefficient, as it requires per pixel numerical integration via volumetric rendering.
   %: it requires per pixel numerical integration for solving the rendering equation, where many discretized points need to be sampled across the rays cast from the pixels.
   %As a solution, some works attempt to use fewer points along the rays, but still suffer from the iterative nature of sampling.
%   As a workaround, the recently proposed R2L method avoids integration by distilling knowledge from a NeRF to a Light Field Network (NeLF), where the latter regresses the final RGB color directly for each ray, considerably increasing both speed and quality .
   %
   %three orders of magnitude.
   %, and leveraging the power of a deep residual MLP network.
%   Yet, NeLF and R2L can only work on static scenes.
%   In this paper, we propose DyLiN, extending NeLFs to non-rigid deformations, including topological changes.
%   We learn a deformation field from input coordinates to canonical coordinates, and also lift NeLF into a higher dimensional space facilitating discontinuities by slicing through the hyperspace.
   %We also propose two baseline architectures and perform experiments on both synthetic and real-world scenes.
%   We experimentally show that our method achieves significant improvement in rendering quality (XXX dB average PSNR improvement) compared to the original R2L procedure, while also reducing computational complexity (XXX FLOPs reduction per camera ray) against the teacher model.
   %dynamic NeRF.
%   Finally, we further extend our DyLiN to controllability, which we call CoDyLiN, greatly expanding applicability.
   %Finally, we demonstrate that manipulating the hyperspace facilitates controlling dynamic scenes, greatly expanding applicability.
%   Our experiments are performed on both synthetic and real-world datasets.

%   Recently, Neural Radiance Field (NeRF) and its variants show great performance on implicitly representing 3D static or dynamic scenes.
%   However, their inference is inefficient: it requires per pixel numerical integration for solving the rendering equation, where many discretized points need to be sampled across the rays cast from the pixels.
   %As a solution, some works attempt to use fewer points along the rays, but still suffer from the iterative nature of sampling.
%   As a solution, the R2L method avoids integration by distilling NeRF to a Light Field Network (NeLF), which estimates the final RGB color directly for each ray.
   %, and leveraging the power of a deep residual MLP network.
%   Yet, R2L can only work on static scenes.
%   In this paper, we extend R2L to dynamic scenes with topological changes by proposing Hyper-R2L, which lifts NeLF into a higher dimensional space.
   %We also propose two baseline architectures and perform experiments on both synthetic and real-world scenes.
%   We experimentally show that our method can achieve XXX FLOPs reduction (per camera ray) and XXX inference speedup along with significant improvement (XXX dB average PSNR improvement) of rendering quality compared to its respective teacher model.
   %dynamic NeRF.
%   Furthermore, we demonstrate that manipulating the hyperspace facilitates controlling the dynamic scenes, greatly expanding applicability.
%   Our experiments are performed on synthetic and real-world data sets.
   
   
\end{abstract}

%\section{note}
%1. problem oriten, deformable light field \\
%2. 

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Machine vision has made tremendous progress with respect to reasoning about 3D structure using 2D observations. Much of this progress can be attributed to the emergence of coordinate networks \cite{chen2019net,mescheder2019occupancy,park2019deepsdf}, such as Neural Radiance Fields (NeRF) \cite{mildenhall2021nerf} and its variants \cite{barron2021mip,mildenhall2022nerf,wang2021neus,martin2021nerf}. They provide an object agnostic representation for 3D scenes and can be used for high-fidelity synthesis for unseen views.
%Learning representations of 3D scenes from 2D observations is a fundamental problem in computer vision with several applications in 3D graphics.
%Recent explorations \cite{chen2019net,mescheder2019occupancy,park2019deepsdf} in coordinate network based representations have greatly promoted the development in this direction.
%Among those, Neural Radiance Fields (NeRFs) \cite{mildenhall2021nerf} and its variants \cite{barron2021mip,mildenhall2022nerf,wang2021neus,martin2021nerf} show great potential in modeling 3D scenes and synthesizing high-quality novel-view 2D images.
While NeRFs mainly focus on static scenes, a series of works\cite{pumarola2021d,tretschk2021non,gafni2021dynamic,park2021nerfies} extend the idea to dynamic cases via additional components that map the observed deformations to a canonical space, supporting moving and shape-evolving objects. It was further shown that by lifting this canonical space to higher dimensions the method can handle changes in scene topology as well \cite{park2021hypernerf}.

\iffalse
\begin{figure}[!t]
     \centering
     \includegraphics[width=0.8\linewidth]{imgs/plot.png}
     %\iffalse
     \begin{subfigure}[b]{0.22\columnwidth}
         \centering
         %\includegraphics[width=0.99\textwidth]{imgs/T-R2L.pdf}
         \includegraphics[width=0.99\textwidth]{imgs/real_results/front/gt_americano.png}
         \caption{Ground Truth \\
         Video}
         \label{fig:T-R2L}
     \end{subfigure}
     \,\,
     \hfill
     \begin{subfigure}[b]{0.22\columnwidth}
         \centering
         %\includegraphics[width=0.99\textwidth]{imgs/D-R2L.pdf}
         \includegraphics[width=0.99\textwidth]{imgs/real_results/front/hyper_americano.png}
         \caption{HyperNeRF \cite{park2021hypernerf}\\
         Render time: $\SI{3}{\second}$}
         \label{fig:D-R2L}
     \end{subfigure}
     \,\,
     %\hfill
     \begin{subfigure}[b]{0.22\columnwidth}
         \centering
         %\includegraphics[width=0.99\textwidth]{imgs/T-R2L.pdf}
         \includegraphics[width=0.99\textwidth]{imgs/real_results/front/neu_americano.png}
         \caption{TiNeuVox \cite{tineuvox}\\
         Render time: $\SI{7}{\second}$}
         \label{fig:T-R2L}
     \end{subfigure}
     \,\,
     %\hfill
     \begin{subfigure}[b]{0.22\columnwidth}
         \centering
         %\includegraphics[width=0.99\textwidth]{imgs/D-R2L.pdf}
         \includegraphics[width=0.99\textwidth]{imgs/real_results/front/ours_americano.png}
         \caption{Ours \\
         Render time: $\SI{0.1}{\second}$}
         \label{fig:D-R2L}
     \end{subfigure}
     %\fi
        \caption{
        Our proposed DyLiN for dynamic 3D scene rendering achieves higher quality than its HyperNeRF \cite{park2021hypernerf} teacher model and is comparable to the state-of-the-art TiNeuVox \cite{tineuvox} model, while being an order of magnitude faster.
        DyLiN is of moderate storage size (shown by dot radii).}
        %Our two ablated baseline models, omiting components of our DyLiN. (a)~Without our two proposed MLPs. (b)~Pointwise deformation MLP only, predicting offsets jointly.}
        \label{fig:fancy}
\end{figure}
\fi

However, the applicability of NeRF models is considerably limited by their computational complexities.
From each pixel, one typically casts a ray from that pixel, and numerically integrates the radiance and color densities computed by a Multi-Layer Perceptron (MLP) across the ray, approximating the pixel color. Specifically, the numerical integration involves sampling hundreds of points across the ray, and evaluating the MLP at all of those locations.
%Conventional NeRFs bear large time and computation cost to optimize the networks and render novel-view images.
%Existing methods modeling dynamic scenes using additional deformation networks further increase the time and computation cost.
Several works have been proposed for speeding up static NeRFs.
These include employing a compact 3D representation structure \cite{liu2020neural,yu2021plenoctrees,fridovich2022plenoxels}, breaking up the MLP into multiple smaller networks \cite{rebain2021derf,reiser2021kilonerf}, leveraging depth information \cite{neff2021donerf,deng2022depth}, and using fewer sampling points \cite{lindell2021autoint,neff2021donerf,xu2022point}.
%Although these methods have made impressive progress in NeRF acceleration, 
Yet, these methods still rely on integration and suffer from sampling many points, making them prohibitively slow for real-time applications.
Recently, Light Field Networks (LFNs) \cite{sitzmann2021lfns} proposed replacing integration with a direct ray-to-color regressor, trained using the same sparse set of images, requiring only a single forward pass.
R2L \cite{wang2022r2l} extended LFNs to use a very deep residual architecture, trained by distillation from a NeRF teacher model to avoid overfitting.
%Unfortunately, R2L and all of the aforementioned acceleration techniques are restricted to static scenes.
%R2L and other acceleration mentioned above all focus on static scenes, which make it difficult to apply into the real world which is full of motion changes.
In contrast to static NeRF acceleration, speeding up dynamic NeRFs is a much less discussed problem in the literature.
This is potentially due to the much increased difficulty of the task, as one also has to deal with the high variability of motion.
In this direction, \cite{tineuvox,wang2022fourier} greatly reduce the training time by using well-designed data structures, but their solutions still rely on integration.
LFNs are clearly better suited for acceleration, yet, to the best of our knowledge, no works have attempted extending LFNs to the dynamic scenario.
%However, the applicability of NeRF models is considerably limited by their computational complexities.
%From each pixel, one typically casts a ray from that pixel, and numerically integrates the radiance and color densities computed by a Multi-Layer Perceptron (MLP) across the ray, approximating the pixel color.
%Specifically, the numerical integration involves sampling points across the ray, and evaluating the MLP at all of those locations.
%Conventional NeRFs bear large time and computation cost to optimize the networks and render novel-view images.
%Existing methods modeling dynamic scenes using additional deformation networks further increase the time and computation cost.
%Several works have been proposed to speeding up static NeRFs.
%These include employing a compact 3D representation structure \cite{liu2020neural,yu2021plenoctrees,fridovich2022plenoxels}, breaking up the MLP into multiple smaller networks \cite{rebain2021derf,reiser2021kilonerf}, leveraging depth information \cite{neff2021donerf,deng2022depth}, and using fewer sampling points \cite{lindell2021autoint,neff2021donerf,xu2022point}.
%Although these methods have made impressive progress in NeRF acceleration, 
%These methods still suffer from numerical integration, requiring hundreds of sampled points along each ray, which is prohibitively slow for real-time applications.
%Recently, Light Field Networks (LFNs) \cite{sitzmann2021lfns} proposed replacing integration with a direct color regressor called Light Field Network (NeLF), trained using the same sparse set of images, requiring only a single forward pass.
%R2L \cite{wang2022r2l} extended NeLF to use a very deep residual architecture, trained by distillation from a NeRF teacher model to avoid overfitting.
%Unfortunately, R2L and all of the aforementioned acceleration techniques are restricted to static scenes.
%R2L and other acceleration mentioned above all focus on static scenes, which make it difficult to apply into the real world which is full of motion changes.
%In contrast to static NeRF acceleration, speeding up dynamic NeRFs is a much less discussed problem in the literature.
%This is potentially due to the much increased difficulty of the task, as one also has to deal with the high variability of motion.
%In this direction, \cite{tineuvox,wang2022fourier} greatly reduce the training time by using well-designed data structures, but their solutions still rely on integration.

%Note that speeding up the dynamic NeRFs can be much more difficult due to the movement over time.

In this paper, we propose 2 schemes extending LFNs to dynamic scene deformations, topological changes and controllability.
%Specifically, in order to speeding up the rendering process, we distill a dynamic teacher NeRF into a student NeLF model.
First, we introduce DyLiN, by incorporating a deformation field and a hyperspace representation to deal with non-rigid transformations, while distilling knowledge from a pretrained dynamic NeRF.
Afterwards, we also propose CoDyLiN, via adding controllable input attributes, trained with synthetic training data generated by a pretrained Controllable NeRF (CoNeRF) \cite{kania2022conerf} teacher model.
%, while distilling from a dynamical or Controllable NeRF (CoNeRF) \cite{kania2022conerf}.
To test the efficiencies of our proposed schemes, we perform empirical experiments on both synthetic and real datasets.
We show that our DyLiN achieves better image quality and an order of magnitude faster rendering speed than its original dynamic NeRF teacher model and the state-of-the-art TiNeuVox \cite{tineuvox} method.
Similarly, we also show that CoDyLiN outperforms its CoNeRF teacher.
We further execute ablation studies to verify the individual effectiveness of different components of our model.
%compare our scheme with two simple baseline architectures.
%Finally, we show that distillation from Controllable NeRF (CoNeRF) \cite{kania2022conerf} is also possible, achieving better control quality.
%ZM: against what?
Our methods can be also understood as accelerated versions of their respective teacher models, and we are
not aware of any prior works that attempt speeding up CoNeRF.
%To the best of our knowledge, our work is the first attempt towards accelerating CoNeRF.
%To the best of our knowledge, our method is the first attempt towards speed up NeRF rendering process for dynamic scenes using NeLF and 
%Our method can be extended to controllable NeRFs. \\
\\Our contributions can be summarized as follows:
%In general, our contributions can be summarized as follows:
\begin{itemize}
\item We propose DyLiN, an extension of LFNs that can handle dynamic scenes with topological changes. DyLiN achieves this through non-bending ray deformations, hyperspace lifting for whole rays, and knowledge distillation from dynamic NeRFs.
%\item We propose DyLiN, extending LFNs to dynamic deformations and hyperspace representations.
% This is the first attempt towards accelerating dynamic NeRF rendering process.
\item 
%We show that DyLiN achieves state-of-the-art results, while being an order of magnitude faster than the competition.
We show that DyLiN achieves state-of-the-art results on both synthetic and real-world scenes, while being an order of magnitude faster than the competition.
%even without ray bending.
We also include an ablation study to analyze the contributions of our model components.
%Additionally, we provide an ablation study to analyze the individual contributions of our model components.
%\item We show that our method can work well on both synthetic and real-world scenes with topological changes, while ablating our model components.
\item We introduce CoDyLiN, further extending our DyLiN to handle controllable input attributes.
%, achieving better manipulation quality.
\end{itemize}


\section{Related Works}
\label{sec:formatting}
\paragraph{Dynamic NeRFs.}
NeRFs have demonstrated impressive performances in novel view synthesis for static scenes.
Extending these results to dynamic (deformable) domains has sparked considerable research interest \cite{pumarola2021d,tretschk2021non,gafni2021dynamic,park2021nerfies,park2021hypernerf}.
Among these works, the ones that most closely resemble ours are D-NeRF \cite{pumarola2021d} and HyperNeRF \cite{park2021hypernerf}.
D-NeRF uses a translational deformation field with temporal positional encoding.
HyperNeRF introduces a hyperspace representation, allowing topological variations to be effectively captured.
Our work expands upon these works, as we propose DyLiN, a similar method for LFNs. We use the above dynamic NeRFs as pretrained teacher models for DyLiN, achieving better fidelity with orders of magnitude shorter rendering times.

\paragraph{Accelerated NeRFs.}
%rendering.}
The high computational complexity of NeRFs has motivated several follow-up works on speeding up the numerical integration process.
The following first set of works are restricted to static scenarios.
NSVF \cite{liu2020neural} represents the scene with a set of voxel-bounded MLPs organized in a sparse voxel octree, allowing voxels without relevant content to be skipped.
KiloNeRF \cite{reiser2021kilonerf} divides the scene into a grid and trains a tiny MLP network for each cell within the grid, saving on pointwise evaluations.
AutoInt \cite{lindell2021autoint} reduces the number of point samples for each ray using learned partial integrals.
%One commonality between the above methods is their focus static scenes.
In contrast to the above procedures, speeding up dynamic NeRFs is much less discussed in the literature, as there are only 2 papers published on this subject.
Wang \textit{et al.} \cite{wang2022fourier} proposed a method based on Fourier plenoctrees for real-time dynamic rendering, however, the technique requires an expensive rigid scene capturing setup.
%Limitations of this work include the inability to model scenes with changes in topology, and the reliance on an expensive and rigid scene capturing setup, whereas our technique does not suffer from these drawbacks.
TiNeuVox \cite{tineuvox} reduces training time by augmenting the MLP with time-aware voxel features and a tiny deformation network, while using a multi-distance interpolation method to model temporal variations.
%Despite the merits of the above methods, 
Interestingly, all of the aforementioned methods suffer from sampling hundreds of points during numerical integration, and none of them support changes in topology, whereas our proposed DyLiN excels from both perspectives.
%does not suffer from this drawback.

%\paragraph{Neural light fields and R2L.}
%\paragraph{Approximating NeRF rendering.}
\paragraph{Light Field Networks (LFNs).}
As opposed to the aforementioned techniques that accelerate numerical integration within NeRFs, some works have attempted completely replacing numerical integration with direct per-ray color MLP regressors called Light Field Networks (LFNs).
%\emph{light fields}
%\emph{Light fields} map each ray to a color by a \textit{single} network evaluation, as opposed to point sampling along the ray and composing the MLP outputs of each sample.
%Another line of works use light fields to achieve improved rendering speed.
%These techniques can be categorized with respect to their ray representations.
Since these approaches accept rays as inputs, they rely heavily on the ray representation.
Several such representations exist in the literature.
Plenoptic functions \cite{bergen1991plenoptic,adelson1992plenoptic} encode 3D rays with 5D representations, i.e., a 3D point on a ray and 2 axis-angle ray directions.
Light fields \cite{gortler1996lumigraph,levoy1996light} use 4D ray codes most commonly through two-plane parameterization: given 2 parallel planes, rays are encoded by the 2D coordinates of the 2 ray-plane intersection points.
Sadly, these representations are either discontinuous or cannot represent the full set of rays.
%Light fields have been used as a means for scene representation for quite sometime , with later works by Gortler \textit{et al.}  and Levoy \textit{et al.} \cite{ offering fast image-based rendering.
%Since then, light fields have been applied to the popular task of novel view synthesis \cite{buehler2001unstructured} and computational photography \cite{ng2006digital}.
%Novel view images are then directly rendered by simply extracting 2D slices from the 4D light field, however, this comes at the cost of high storage requirements.
Recently, Sitzmann \textit{et al.} \cite{sitzmann2021lfns} advocate for the usage of the 6D Pl\"{u}cker coordinate representation, i.e., a 3D point on a ray coupled with its cross product with a 3D direction.
%This representation overparametrizes 4D rays: it is invariant to scaling, and the dot product of the parameters is always zero.
They argue that this representation covers the whole set of rays and is continuous.
Consequently, they feed it as input to an LFN, and additionally apply Meta-Learning across scenes to learn a multi-view consistency prior.
%, calling their overall procedure Light Field Networks (LFNs).
%LFNs parametrizes a 4D light field as an MLP.
%Partnered with this, meta-learning is employed to learn a prior over the LFNs, enabling multi-view consistency.
However, they have not considered alternative ray representations, MLP architectures or training procedures, and only tested their method on toy datasets.
R2L \cite{wang2022r2l} employs an even more effective ray encoding by concatenating few points sampled from it, and proposes a very deep (88 layers) residual MLP network for LFNs.
They resolve the proneness to overfitting by training the MLP with an abundance of synthetic images generated by a pretrained NeRF having a shallow MLP.
Interestingly, they find that the student LFN model produces significantly better rendering quality than its teacher NeRF model, while being about 30 times faster.
%leverages LFN's single evaluation capabilities to achieve impressive rendering times (faster than all of the above methods) for real-world static scenes.
%It accomplishes this task through a multi-step process in where a student LFN is trained on both standard input images and pseudodata generated from a teacher Radiance Field Network (RFN).
%This process of using synthetic data from a teacher model is more generally known as Knowledge Distillation (KD) \cite{buciluǎ2006model}, and has been widely used in both vision and language tasks \cite{chen2017learning, levoy1996light,wang2020collaborative,wang2021knowledge}.
Our work extends LFNs to dynamic deformations, topological changes and controllability, achieving similar gains over the pretrained dynamic NeRF teacher models.

\paragraph{Knowledge Distillation.}
The process of training a student model with synthetic data generated by a teacher model is called Knowledge Distillation (KD) \cite{bucilua2006model}, and it has been widely used in the vision and language domains \cite{chen2017learning, levoy1996light,wang2020collaborative,wang2021knowledge} as a form of data augmentation.
Like R2L \cite{wang2022r2l}, we also use KD for training, however, our teacher and student models are both dynamic and more complex than their R2L counterparts.

\section{Methods}
%\subsection{Background on NeRF, NeLF and R2L}
%Neural Radiance Field (or NeRF) \cite{mildenhall2021nerf} is one of the most popular implicit representations of 3D scenes. NeRF maps a sample 3d position $\textbf{x}=(x,y,z)$ and a 2d view direction $\textbf{d}=(\theta, \phi)$ to the emitted color $\textbf{c}$ and volume density $\sigma$ at position $\textbf{x}$ with view direction $\textbf{d}$ using an MLP network. Then the desired color for an oriented ray can be obtained through classical volume rendering technique \cite{kajiya1984ray}. While NeRF requires a model forward pass for each sample point, Light Field Network (or NeLF) maps a 4D oriented ray directly to its target desired RGB color, which is more straightforward and much faster than NeRF. Despite the intriguing properties, it is hard for NeLF to achieve comparable novel-view synthesis quality to NeRF, especially on complex real-world scenes. R2L \cite{wang2022r2l} takes the advantages of NeLF and achieves comparable and even better rendering quality through distilling a teacher NeRF into a student NeLF with a deep residual MLP (88 layers). Although its eye-catching performance, it can only work on static scenes. In this paper, we propose DyLiN that extends R2L to model dynamic scenes and also enable acceleration over controllable NeRFs.
In this section, we present our two solutions for extending LFNs.
First, in \cref{s:hypernelf}, we propose DyLiN, supporting dynamic deformations and hyperspace representations via two respective MLPs.
We use KD to train DyLiN with synthetic data generated by a pretrained dynamic NeRF teacher model.
Second, in \cref{s:conelf}, we introduce CoDyLiN, which further augments DyLiN with controllability, via lifting attribute inputs to hyperspace with MLPs, and masking their hyperspace codes for disentanglement.
In this case, we also train via KD, but the teacher model is a pretrained controllable NeRF.

\subsection{DyLiN}
\label{s:hypernelf}

\subsubsection{Network Architecture}
\label{ss:hypernelf_arch}
%We extend NeLF with deformation and hyperspace MLPs, both processing rays instead of 3D points as follows.
Our overall DyLiN architecture $G_\phi$ is summarized in \cref{fig:hyperr2l}.
It processes rays instead of the widely adopted 3D point inputs as follows.

\begin{figure}[ht]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.99\linewidth]{imgs/hyperR2L.pdf}

   \caption{Schematic diagram of our proposed DyLiN architecture. We take a ray $r=(o,d)$ and time $t$ as input. We deform $r$ into $r'=(o',d')$, and sample few points $x_k$, $k=1,\dots,K$ along $r'$ to encode it (blue). In parallel, we also lift $r$ and $t$ to the hyperspace code $w$ (green), and concatenate it with each $x_k$. We use the concatenation to regress the RGB color of $r$ at $t$ directly (red).}
   \label{fig:hyperr2l}
\end{figure}

Specifically, our deformation MLP $T_\omega$ maps an input ray $r=(o,d)$ to canonical space ray $r'=(o',d')$:
\begin{equation}
  (o', d') = T_\omega(o, d, t).
  \label{eq:T}
\end{equation}
Unlike the pointwise deformation MLP proposed in Nerfies \cite{park2021nerfies}, which bends rays by offsetting their points independently, our MLP outputs rays explicitly, hence no ray bending occurs.
Furthermore, after obtaining $r'$, we encode it by sampling and concatenating $K$ points along it.

Our hyperspace MLP $H_\psi$ is similar to $T_\omega$, except it outputs a hyperspace representation $w$:
\begin{equation}
  w = H_\psi(o, d, t).
  \label{eq:H}
\end{equation}
In contrast to HyperNeRF \cite{park2021hypernerf}, which predicts a hyperspace code $w$ for each 3D point, we use rays and compute a single $w$ for each ray.

Both MLPs further take the index $t$ as input to encode temporal deformations.

Once the $K$ points and $w$ are obtained, we concatenate them and feed the result into our LFN $R_\pi$, which is a deep residual color MLP regressor.
Overall, we can collect the model parameters as $\phi=[\omega,\psi,\pi]$.

Note that without our two MLPs $T_\omega$ and $H_\psi$, our DyLiN falls back to the vanilla LFN.


\subsubsection{Training Procedure}
\label{ss:hypernelf_train}
%Then, we train our DyLiN by distilling from a pretrained HyperNeRF.
Our training procedure is composed of 3 phases.

First, we pretrain a dynamic NeRF model $F_{\theta}$ (e.g., D-NeRF \cite{pumarola2021d} or HyperNeRF \cite{park2021hypernerf}) by randomly sampling time $t$ and input ray $r$, and minimizing the Mean Squared Error (MSE) against the corresponding RGB color of monocular target video $I$:
\begin{equation}
  \min_\theta \,\mathbb{E}_{t,r=(o,d)}\left[\|F_{\theta}(o,d,t)-I(o,d,t)\|_2^2\right].
  \label{eq:L1}
\end{equation}
Recall, that $F_\theta$ is slow, as it performs numerical integration across the ray $r=(o,d)$.

Second, we employ the newly obtained $F_{\theta^*}$ as the teacher model for our DyLiN student model $G_\phi$ via KD.
Specifically, we minimize the MSE loss against the respective pseudo ground truth ray color generated by $F_{\theta^*}$ across $S$ ray samples:
\begin{equation}
  \min_\phi \,\mathbb{E}_{t,r=(o,d)}\left[\|G_\phi(o,d,t)-F_{\theta^*}(o,d,t)\|_2^2\right],
  \label{eq:L2}
\end{equation}
yielding $G_{\tilde{\phi}}$.
Note how this is considerably different from R2L \cite{wang2022r2l}, which uses a static LFN that is distilled from a static NeRF.

Finally, we initialize our student model $G_\phi$ with parameters $\tilde{\phi}$ and fine-tune it using the original real video data:
\begin{equation}
  \min_{\phi,\,\phi_0=\tilde{\phi}} \,\mathbb{E}_{t,r=(o,d)}\left[\|G_{\phi}(o,d,t)-I(o,d,t)\|_2^2\right],
  \label{eq:L3}
\end{equation}
obtaining $\phi^*$.

%We emphasize that our teacher and student model are both dynamic in this setup.

\subsection{CoDyLiN}
\label{s:conelf}
\subsubsection{Network Architecture}
\label{ss:conelf_arch}
We further demonstrate that our DyLiN architecture from \cref{ss:hypernelf_arch} can be extended to the controllable scenario using attribute inputs with hyperspace MLPs and attention masks.
%We further augment our DyLiN architecture from \cref{ss:hypernelf_arch} with controllable attribute inputs, as well as hyperspace MLPs and attention masks acting on top of those.
Our proposed CoDyLiN network $Q_\tau$ is depicted in \cref{fig:cor2l}.

\begin{figure}[ht]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.99\linewidth]{imgs/coR2L.pdf}

   \caption{Schematic diagram of our proposed CoDyLiN architecture. We augment our DyLiN (blue, green, red) by introducing scalar attribute inputs $\alpha_i\in[-1,1]$, $i=1,\dots,n$ and lifting them to their respective hyperspace codes $w_i$ (orange, \dots, pink MLPs). Next, $M_i$ disentangles $w_i$ from $w_j$, $j\neq i$ by masking it into $w_i'$ (orange, \dots, pink boxes and bottom insets). We concatenate the sampled points $x_k$, $k=1,\dots,K$ with the $w_i'$, $i=1,\dots,n$ and predict the RGB color corresponding to the inputs (red).
   Arrows from $(o', d')$ and $w_0$ to $M_i$ are omitted from the top figure for simplicity.
   Compare this with \cref{fig:hyperr2l}.}
   \label{fig:cor2l}
\end{figure}

Specifically, we start from DyLiN $G_\phi$ and add scalar inputs $\alpha_i\in[-1,1]$, $i=1,\dots,n$ next to $o,d,t$.
Intuitively, these are given strength values for specific local attributes, which can be interpolated continuously.
$n$ is the total number of attributes.

Each $\alpha_i$ is then processed independently with its own hyperspace MLP $H_{i,\psi_i}$ to yield the hyperspace code $w_i$:
\begin{equation}
    w_i = H_{i,\psi_i}(o,d,t).
\end{equation}

Next, we include mask MLP regressors $M_{i,\rho_i}$ to generate scalar attention masks $\hat{m}_i\in[0,1]$ for each $w_i$ (including $w_0=w$):
\begin{equation}
\begin{alignedat}{1}
    \hat{m}_i &= M_{i,\rho_i}(w_i,w,o,d),\\
    \hat{m}_0 &= 1-\sum_{i=1}^{n}{\hat{m}_i},\\
    w_i' &= \hat{m}_i \cdot w_i, \quad i=0,\dots,n,
\end{alignedat}
\end{equation}
This helps the architecture to spatially disentangle (i.e., localize) the effects of attributes $\alpha_i$, while $\hat{m}_0$ can be understood as the space not affected by any attributes.
%, and localize to their respective hyperspace areas localized by $\hat{m}_i$.
%Additionally, we project $\hat{m}$

Finally, we sample $K$ points on the ray similarly to \cref{ss:hypernelf_arch}, concatenate those with the $w_i'$ vectors, and process the result further with LFN $R_\pi$.
Again, we can use a shorthand for the parameters: $\tau=[\omega,\psi,\psi_1,\dots,\psi_n,\rho_1,\dots,\rho_n,\pi]$.

Observe that without our MLPs $H_{i,\psi_i}$, $M_{i,\rho_i}$, $i=1,\dots,n$, our CoDyLiN reverts to our simpler DyLiN.
Different from CoNeRF \cite{kania2022conerf}, we process rays instead of points, and use the $\alpha_i$ as inputs instead of targets.


\subsubsection{Training Procedure}
\label{ss:conelf_train}
Akin to \cref{ss:hypernelf_train}, we split training into pretraining and distillation steps, but omit fine-tuning.

First, we pretrain a CoNeRF model $E_\nu$ \cite{kania2022conerf} by randomly sampling $(t,r,i)$, against 3 ground truths: ray color, attribute values $\alpha_i$ and 2D per-attribute masks $m_{2D,i}$.
%\begin{multline}
%  \min_\nu \,\mathbb{E}_{t,r=(o,d),i}\bigl[\|E_{\nu}(o,d,t)-I(o,d,t)\|_2^2 + \lambda_{prior}\cdot L_{prior} \\ + \lambda_\alpha\cdot(\hat{\alpha}_i-\alpha_i)^2 + \lambda_m\cdot CE(2D(\hat{m}_i),m_{2D,i})\bigr].
%  \label{eq:L4}
%\end{multline}
%$L_prior$ denotes a Gaussian prior for latent codes, $CE$ stands for cross-entropy loss, and $2D$ denotes the projection from hyperspace to image space (via volumetric rendering).
%We only sample triplets $(t,r,i)$ for which annotation exists.
This yields us $E_{\nu^*}$.
For brevity, we omit the details of this step, and kindly forward the reader to Section 3 in \cite{kania2022conerf}.
%further 

Second, we distill from our teacher CoNeRF model $E_{\nu^*}$ into our student CoDyLiN $Q_\tau$ by randomly sampling $t,r,\alpha_1,\dots,\alpha_n$, and minimizing the MSE against 2 pseudo ground truths, i.e., ray colors and 2D masks $\bar{m}_{2D,i}$:
\begin{multline}
  \min_\tau \,\mathbb{E}_{t,r=(o,d)}\biggl[\|Q_\tau(o,d,t,\alpha_{1:n})-\bar{E}_{\nu^*}(o,d,t,\alpha_{1:n})\|_2^2 \\ 
  +\lambda_m \cdot \sum_{i=0}^{n}{\|\hat{m}_i(o,d,t,\alpha_{i})-\bar{m}_{2D}(o,d,t,\alpha_{1:n})_{i}\|_2^2}\biggr],
  \label{eq:L5}
\end{multline}
where $\bar{E}_\nu$ is identical to $E_\nu$ except for taking $\alpha_{1:n}=[\alpha_1,\dots,\alpha_n]$ as input and outputting the masks $\bar{m}_{2D,i}$, $i=0,\dots,n$.
We denote the result of the optimization as $Q_{\tau^*}$.

%As the final step, we initialize parameters with $\tilde{\tau}$ and fine-tune our student model $Q_\tau$:
%\begin{equation}
%  \min_{\phi,\,\phi_0=\tilde{\phi}} \,\mathbb{E}_{t,r=(o,d)}\left[\|G_{\phi}(o,d,t)-I(o,d,t)\|_2^2\right],
%  \label{eq:L6}
%\end{equation}
%outputting $\tau^*$.

We highlight that our teacher and student models are both controllable in this setup.


%Error in fig3, o',d' should be o,d.
%w0 should be w






%\subsection{DyLiN}
%To distill dynamic NeRFs into NeLFs, we need to model deformation over time $t$ on the top of R2L.
%We propose to introduce hyperspace $w$ which is first used in HyperNeRF \cite{park2021hypernerf}.
%HyperNeRF learns a unique hyper-space $w$ for each spatial position and shows that it performs better than learning a single $w$ for all points.
%To avoid model forward pass for each sample point and keep the network size as small as possible, we propose a moderate method which is learning a unique $w$ for each ray as shown in Figure \ref{fig:hyperr2l}.
%The hyper-space $w$ can map the light field to higher dimensional space in which the topology changes can be better learned according to \cite{park2021hypernerf}.
%We learn a hyper field $H$ to generate $w$ for each ray $r$ along with time $t$.
%Here we represent a ray using origin $o$ and direction $d$ and we have:
%\begin{equation}
%  w = H(o, d, t)
%  \label{eq:H}
%\end{equation}
%We also learn a transformation field $T$ that maps input ray $r$ to $r'$ (represented by $(o', d')$) in canonical-space as in Eq \ref{eq:T}. 
%\begin{equation}
%  (o', d') = T(o, d, t)
%  \label{eq:T}
%\end{equation}
%$T$ learns rotation and translation transformations for each ray and we claim it is sufficient for NeLF since light field maps each ray to corresponding final RGB color directly and there is no need to bend each ray explicitly. After obtaining $r'$, we represent the ray by concatenating the spatial coordinates of $K$ (we set $K=16$ in this paper) sampled points along a ray, which is the same as \cite{wang2022r2l}. The K points are random sampled along the ray during training process and evenly spaced during testing process. The K points are then concatenated with $w$ to form an input vector with dimension $3K+W$ ($W$ is the dimension of hyper-space $w$), which is fed into a deep residual MLP and output the final RGB color for the ray. For the deep residual MLP, we take the W256D88 (W stands for width, D for depth) structure as in \cite{wang2022r2l} as it achieves better quality. We use two shallow MLPs for transformation field $T$ (W128D7) and hyper field $H$ (W64D6) to keep the efficiency of our method. The detail of network structure can be found in the supplementary material.



%To distill the teacher dynamic NeRF into our Hyper-R2L, we first generate pseudo data from the pre-trained teacher dynamic NeRF since real dataset which is typically captured for NeRF is not sufficient to train such a deep residual MLP and then fine-tune our model on real training data. The pseudo data includes $(o, d, t, c)$ pairs where $(o, d)$ represents the ray, $t$ indicates the time and $c$ is the final RGB color. For a well-trained dynamic NeRF model $F_{\theta^*}$, where $\theta^*$ stands for the converged model parameters, we have:
%\begin{equation}
%  c = F_{\theta^*}(o, d, t)
%  \label{eq:T}
%\end{equation}
%Note our model as $G$ with parameters $\phi$, the objective of pseudo data training step is minimizing MSE loss function $L$:
%\begin{equation}
%  L = MSE(G_{\phi}(o, d, t), c)
%  \label{eq:L}
%\end{equation}

%As proposed in \cite{wang2022r2l}, we randomly sample the ray origins $(x_o, y_o, z_o)$ and normalized directions $(x_d,y_d,z_d)$ as follows, where $U$ represents uniform distribution within the viewing bounding box of the camera locations and orientations which can be inferred from original training data.
%\begin{equation}
%\begin{aligned}
%  x_o \sim U(x_o^{min}, x_o^{max}), \quad  x_d \sim U(x_d^{min}, x_d^{max})\\
%  y_o \sim U(y_o^{min}, y_o^{max}), \quad  y_d \sim U(y_d^{min}, y_d^{max})\\
%  z_o \sim U(z_o^{min}, z_o^{max}), \quad  z_d \sim U(z_d^{min}, z_d^{max})
%  \label{eq:U}
%\end{aligned}
%\end{equation}
%We also randomly generate normalized $t$ as following:
%\begin{equation}
%\begin{aligned}
%  t \sim U(0, 1)
%  \label{eq:U}
%\end{aligned}
%\end{equation}

%After training using pseudo data, we fine-tune our model on real training data using the similar MSE loss function $L$ to Eq \ref{eq:L} and replace $c$ with ground-truth $c_{gt}$.

%It's worth noting that we also take the same hard example training strategy as in \cite{wang2022r2l}, which makes the network pay more attention to the harder-to-regress rays (defined by larger losses) by maintaining a hard example pool. Details can refer to \cite{wang2022r2l}.


%\subsection{Controllable NeRF Distillation}
%Thanks to the introduction of hyper-space, our Hyper-R2L can be easily extended to distill controllable NeRFs such as CoNeRF \cite{kania2022conerf} as shown in Fig \ref{fig:cor2l} which we named Co-R2L. To distill CoNeRF, we need attribute value $\alpha_i$ ($i\in 1\dots n$ for $n$ attributes) and its corresponding mask $m_i$ in addition to $(o, d, t, c)$. We also randomly generate each attribute value $\alpha_i \sim U(-1, 1)$ along with $(o, d, t)$ and for a well-trained CoNeRF model $H_{\theta^*}$, we have:
%\begin{equation}
%  c, m_1, \dots, m_n = F_{\theta^*}(o, d, t, \alpha_1, \dots, \alpha_n)
%  \label{eq:T}
%\end{equation}
%After generating the pseudo data including $(o, d, t, c, \alpha_1, \dots, \alpha_n, m_1, \dots, m_n)$ pairs, we can train Co-R2L




\section{Experimental Setup}
% \begin{figure*}[h]
%   \centering
% %   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.99\linewidth]{imgs/synthetic_compare.pdf}

%   \caption{Qualitative comparison between our DyLiN network (Ours-1, Ours-2), HyperNeRF, and TiNeuVox on synthetic scenes. Ours-1 is trained solely on pseudo data and Ours-2 is trained on both pseudo+real data.}
%   \label{fig:cor2l2}
% \end{figure*}



%\begin{figure*}[h]
%  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.99\linewidth]{imgs/real_comparisons.pdf}

%   \caption{Qualitative experimental results on synthetic dynamic scenes. We compare 2 variants of our DyLiN (Ours-1, Ours-2) with HyperNeRF and TiNeuVox. Ours-1 and Ours-2 are trained without and with fine-tuning on the original data, respectively.}
%   \label{fig:cor2l2}
%\end{figure*}








\subsection{Datasets}
To test our hypotheses, we performed experiments on three types of dynamic scenes: synthetic, real and real controllable. \\
%\bm{$360^{\circ}$} \textbf{Synthetic Scenes.}
\textbf{Synthetic Scenes.}
We utilized the synthetic $360^{\circ}$ dynamic dataset introduced by \cite{pumarola2021d}, which contains 8 animated objects with complicated geometry and realistic non-Lambertian materials.
Each dynamic scene consists of $50$ to $200$ training images and $20$ testing images.
We used $400 \times 400$ image resolution.
We applied D-NeRF \cite{pumarola2021d} as our teacher model with the publicly available pretrained weights.\\
% to be consistent with D-NeRF.
\textbf{Real Scenes.}
We collected real dynamic data from $2$ sources.
First, we  utilized 5 topologically varying scenes provided by \cite{park2021hypernerf} (Broom, 3D Printer, Chicken, Americano and Banana), which were captured by a rig encompassing a pole with two Google Pixel 3 phones rigidly attached roughly $\SI{16}{\centi\metre}$ apart.
Second, we collected human facial videos using an iPhone 13 Pro camera.
We rendered both sets at $960 \times 540$ image resolution.
We pretrained a HyperNeRF \cite{park2021hypernerf} teacher model from scratch for each scene.\\
\textbf{Real Controllable Scenes.}
We borrowed 2 real controllable scenes from \cite{kania2022conerf} (closing/opening eyes/mouth, and transformer), which are captured either with a Google Pixel 3a or an Apple iPhone 13 Pro, and contain annotations over various attributes.
We applied image resolution of $480 \times 270$ pixels.
We pretrained a CoNeRF \cite{kania2022conerf} teacher model from scratch per scene.
%First part of non-rigidly deforming scenes are provided by HyperNeRF \cite{park2021hypernerf} and collected using a capture rig comprised of a pole with two Pixel 3 phones rigidly attached roughly 16cm apart. We perform experiments on four scenes (Broom, 3D Printer, Chicken, and Americano) released by \cite{park2021hypernerf} and all images are trained and rendered at resolutions of $960 \times 540$ pixels to be consistent with HyperNeRF. We also collect a face expression sequence using an iPhone 13 Pro camera and also perform training and testing at the resolution of $960 \times 540$ pixels. Another part of real scenes is from CoNeRF \cite{kania2022conerf}, which are captured either with a Google Pixel 3a or an Apple iPhone 13 Pro and contain annotations to perform control over different attributes. We select two scenes (closing/opening eyes/mouth and transformer) and use resolutions of $480 \times 270$ pixels to evaluate distillation performance from controllable NeRF to our controllable Hyper R2L.

\subsection{Settings}
Throughout our experiments, we use the settings listed below, many of which follow \cite{wang2022r2l}.

In order to retain efficiency, we define $T_\omega$ and $H_\psi$ to be small MLPs, with $T_\omega$ consisting of $7$ layers of $128$ units with $r'\in\mathbb{R}^6$, and $H_\psi$ having $6$ layers of $64$ units with $w\in\mathbb{R}^8$.
Then, we use $K=16$ sampled points to represent rays, where sampling is done randomly during training and evenly spaced during inference.

Contrary to $T_\omega$ and $H_\psi$, our LFN $R_\pi$ is a very deep residual color MLP regressor, containing $88$ layers with $256$ units per layer, in order to have enough capacity to learn the video generation process.

We generate rays within \cref{eq:L1,eq:L2,eq:L3,eq:L5} by sampling ray origins $o=(x_o, y_o, z_o)$ and normalized directions $d=(x_d,y_d,z_d)$ randomly from the uniform distribution $U$ as follows:
%\begin{equation}
\begin{alignat}{2}
  x_o &\sim U(x_o^{min}, x_o^{max}), \quad & x_d &\sim U(x_d^{min}, x_d^{max}),\\
  y_o &\sim U(y_o^{min}, y_o^{max}), \quad & y_d &\sim U(y_d^{min}, y_d^{max}),\\
  z_o &\sim U(z_o^{min}, z_o^{max}), \quad & z_d &\sim U(z_d^{min}, z_d^{max}),
  \label{eq:U}
\end{alignat}
%\end{equation}
where the $min,max$ bounds of the 6 intervals are inferred from the original training video.
In addition to uniform sampling, we also apply the hard example mining strategy suggested in \cite{wang2022r2l} to focus on fine-grained details.
%In particular, we maintain a pool of hard-to-regress rays, by inserting the $20\%$ of rays with the largest errors from the current batch into the pool, while also putting the same number of randomly picked rays from the pool into the next batch.
%In particular, w
We used $S=\SI{10000}{}$ training samples during KD in \eqref{eq:L2}.

Subsequently, we also randomly sample time step $t$ uniformly from the unit interval: $t \sim U(0,1)$.
%\begin{equation}
%\begin{aligned}
%  t \sim U(0, 1).
%  \label{eq:U2}
%\end{aligned}
%\end{equation}

Optionally, for our CoDyLiN experiments, we define each $H_{i,\psi_i}$ to be a small MLP having $5$ layers of $128$ units with $w_i\in\mathbb{R}^8$.
During training, we uniformly sample attributes within $[-1,1]$: $\alpha_i \sim U(-1, 1)$, and let $\lambda_m=0.1$.
%\begin{equation}
%\begin{aligned}
%  \alpha_i \sim U(-1, 1),
%  \label{eq:U2}
%\end{aligned}
%\end{equation}
%and let $\lambda_m=0.1$.
%We compute the ground truth $m_i$ from $\alpha_i$

%$\lambda_a,\lambda_m$
During training, we used Adam \cite{kingma2014adam} with learning rate $\SI{5e-4}{}$ and batch size $\SI{4096}{}$.

We performed all experiments on single NVIDIA A100 GPUs.



%Further details of our setup can be found in the supplementary material.


\subsection{Baseline Models}
For testing our methods, we compared quality and speed against several baseline models, including NeRF \cite{mildenhall2021nerf}, NV \cite{Lombardi:2019}, NSFF \cite{li2020neural}, Nerfies \cite{park2021nerfies}, HyperNeRF \cite{park2021hypernerf}, two variants of TiNeuVox \cite{tineuvox}, DirectVoxGo \cite{SunSC22}, Plenoxels \cite{fridovich2022plenoxels}, T-NeRF and D-NeRF \cite{pumarola2021d}, as well as CoNeRF \cite{kania2022conerf}.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/T-R2L.pdf}
         \caption{}
         \label{fig:T-R2L}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/D-R2L.pdf}
         \caption{}
         \label{fig:D-R2L}
     \end{subfigure}
        \caption{Our two ablated baseline models, omiting components of our DyLiN. (a)~Without our two proposed MLPs. (b)~Pointwise deformation MLP only, predicting offsets jointly.}
        \label{fig:TD-R2L}
\end{figure}

In addition, we performed an ablation study by comparing against 2 simplified versions of our DyLiN architecture.
First, we omitted both of our deformation and hyperspace MLPs and simply concatenated the time step $t$ to the sampled ray points (essentially resulting in a dynamic R2L).
This method is illustrated in \cref{fig:T-R2L}.
%T-R2L is a naive network that directly concatenate the time $t$ with the $K$ sample points in the original R2L method as shown in Fig \ref{fig:T-R2L}.
Second, we employed a pointwise deformation MLP ($5$ layers of $256$ units) inspired by \cite{pumarola2021d}, which deforms points along a ray by predicting their offsets jointly, i.e., it can bend rays.
%The final sampled point $x_k$ can be written as:
%\begin{equation}
%  x_k = x_k + \Delta x_k, \quad k=1 \dots K.
%  \label{eq:xi}
%\end{equation}
This is contrast to our DyLiN, which deforms rays explicitly without bending and also applies a hyperspace MLP.
This scheme is depicted in \cref{fig:D-R2L}.
In both baselines, the deep residual color MLP regressors were kept intact.
Next, we also tested the effects of our fine-tuning procedure from \eqref{eq:L3} by training all of our models both with and without it.
Lastly, we assessed the dependences on the number of sampled points along rays $K$ and on the number of training samples $S$ during KD in \eqref{eq:L2}.

%To show the superiority of our method, we also propose two different network structures as the baseline models, T-R2L and D-R2L. T-R2L is a naive network that directly concatenate the time $t$ with the $K$ sample points in the original R2L method as shown in Fig \ref{fig:T-R2L}. D-R2L is inspired by D-NeRF \cite{pumarola2021d} that first learns offsets $\Delta x_i$ for each of $K$ sample points using a shallow MLP (W256D5) which takes the input vector of concatenating $K$ sample points and time $t$. (Note that the offset network in D-R2L generates all offsets for sample points along a ray at once while offset network D-NeRF generates an offset for one sample point in each model forward pass.) The final sample point $x_i$ can be written as:
%\begin{equation}
%  x_i = x_i + \Delta x_i, \quad i=1 \dots K
%  \label{eq:xi}
%\end{equation}
%Finally, the sample point $x_i$ are concatenated into a input vector (here no time $t$) which is fed into the deep residual MLP to get the final color. The deep residual MLPs in T-R2L and D-R2L have the same structure as in Hyper-R2L.




\subsection{Evaluation Metrics}
For quantitatively evaluating the quality of generated images, we calculated the Peak Signal-to-Noise Ratio (PSNR) \cite{hore2010image} in decibels ($\si{\deci\bel}$), the Structural Similarity Index (SSIM) \cite{wang2004image, odena2017conditional}, the Multi-Scale SSIM (MS-SSIM) \cite{wang2003multiscale} and the Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonable} metrics.
Intuitively, PSNR is a pixelwise score, while SSIM and MS-SSIM also take pixel correlations and multiple scales into account, respectively, yet all of these tend to favor blurred images. 
LPIPS compares deep neural representations of images and is much closer to human perception, promoting semantically better and sharper images.

Furthermore, for testing space and time complexity, we computed the storage size of parameters in megabytes (MB) and measured the wall-clock time in milliseconds (ms) while rendering the synthetic Lego scene with each model.

%\subsection{exper set up}
%main result r2l light field hypernerf extension
%baselines moved here
%withouut explaining naive baselines hard to explain main meth
%baseline 1 no deformations, no offset, resnet leearn everything
%baseline 2 remove hyperspace, hyperspace models topology changes
%removing two mlps, remove 2nd mlp and predict offsets
%no other approaches in the literature
%start with light field
%goal how to make deformable
%able to handle deformation, control
%2 subproblems
%/predict deformation itself. top
%/topoligical changes. bottom
%fig3 controllability extension of fig1. based on conerf

% mention that ours is the the best in table 1 and second best quantitatively in table 2 
\section{Results}
\subsection{Quantitative Results}
\cref{tab:synth} and \cref{tab:real} contain our quantitative results for reconstruction quality on synthetic and real dynamic scenes, accordingly.
We found that among prior works, TiNeuVox-B performed the best on synthetic scenes with respect to each metric. On real scenes, however, NSFF took the lead. Despite having strong metrics, NSFF is qualitatively poor and slow.
Surprisingly, during ablation, even our most basic model (DyLiN without the two MLPs from \cref{fig:T-R2L}) could generate perceptually better looking images than TiNeuVox-B, thanks to the increased training dataset size via KD.
Incorporating the MLPs $T_\omega$ and $H_\psi$ into the model each improved results slightly.
Interestingly, fine-tuning on real data as in \eqref{eq:L3} gave a substantial boost.
In addition, our relative PSNR improvement over the teacher model (\cref{tab:synth}=$+\SI{1.93}{\deci\bel}$, up to $+\SI{3.16}{\deci\bel}$ per scene; \cref{tab:real}=$+\SI{2.7}{\deci\bel}$, up to $+\SI{13.14}{\deci\bel}$) is better than that of R2L \cite{wang2022r2l} ($+\SI{1.4}{\deci\bel}$, up to $+\SI{2.8}{\deci\bel}$).
%The relative PSNR improvement of R2L \cite{wang2022r2l} is $+\SI{1.4}{\deci\bel}$ (up to $+\SI{2.8}{\deci\bel}$ per scene). Ours is better: Tab.~1=$+\SI{1.93}{\deci\bel}$ (up to $+\SI{3.16}{\deci\bel}$), Tab.~2=$+\SI{2.7}{\deci\bel}$ (up to $+\SI{13.14}{\deci\bel}$).

\begin{table}[htb]
\centering
\caption{Quantitative results on synthetic dynamic scenes. Notations: Multi-Layer Perceptron (MLP), PD (pointwise deformation), FT (fine-tuning). We utilized D-NeRF as the teacher model for our DyLiNs. The winning numbers are highlighted in bold.}
\resizebox{\columnwidth}{!}{%

\begin{tabular}{lccc}
\toprule
Method                                 & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\ \midrule
NeRF\cite{mildenhall2021nerf}          & 19.00          & 0.8700           & 0.1825              \\
DirectVoxGo\cite{SunSC22}              & 18.61          & 0.8538           & 0.1688              \\
Plenoxels\cite{fridovich2022plenoxels} & 20.24          & 0.8688           & 0.1600              \\
T-NeRF\cite{pumarola2021d}             & 29.51          & 0.9513           & 0.0788              \\
D-NeRF\cite{pumarola2021d}             & 30.50          & 0.9525           & 0.0663              \\
TiNeuVox-S\cite{tineuvox}              & 30.75          & 0.9550           & 0.0663              \\
TiNeuVox-B\cite{tineuvox}              & \textbf{32.67}          & 0.9725           & 0.0425              \\ 
%\cdashlinelr{1-4}
\midrule
DyLiN, w/o two MLPs, w/o FT (ours)                & 31.16          & 0.9931           & 0.0281                \\
DyLiN, w/o two MLPs (ours)           & 32.07          & 0.9937           & 0.0196                  \\
DyLiN, PD MLP only, w/o FT (ours)                & 31.26          & 0.9932           & 0.0279                  \\
DyLiN, PD MLP only (ours)           & 31.24          & 0.9940           & 0.0189                  \\
DyLiN, w/o FT (ours)            & 31.37          & 0.9933           & 0.0275                  \\
DyLiN (ours)       & 32.43          & \textbf{0.9943}           & \textbf{0.0184} \\                 
\bottomrule
\end{tabular}%
}
\label{tab:synth}
\end{table}

\begin{table}[ht]
\centering
\caption{Quantitative results on real dynamic scenes. Notations: Multi-Layer Perceptron (MLP), PD (pointwise deformation), FT (fine-tuning). We utilized HyperNeRF as the teacher model for our DyLiNs. The winning numbers are highlighted in bold.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
Method                                                  & PSNR$\uparrow$ & MS-SSIM$\uparrow$\\ \midrule
NeRF\cite{mildenhall2021nerf}          &  20.1         &       0.745                    \\
NV\cite{Lombardi:2019}                &   16.9        &      0.571                     \\
NSFF\cite{li2020neural}              &   \textbf{26.3}        &    \textbf{0.916}                       \\
Nerfies\cite{park2021nerfies}       &  22.2         &     0.803                      \\
HyperNeRF\cite{park2021hypernerf}   & 22.4          &       0.814                    \\
TiNeuVox-S\cite{tineuvox}              &  23.4         &       0.813                    \\
TiNeuVox-B\cite{tineuvox}              &    24.3       &   0.837                       \\
\midrule
DyLiN, w/o two MLPs, w/o FT (ours)                                 &   23.8             &     0.882                              \\
DyLiN, w/o two MLPs (ours)                            &   24.2             &     0.894                              \\
DyLiN, PD MLP only, w/o FT (ours)                                 &   23.9             &     0.885                              \\
DyLiN, PD MLP only (ours)                            &   24.6             &     0.903                              \\
DyLiN, w/o FT (ours)                             &   24.0             &     0.886                              \\
DyLiN (ours)                         &   25.1             &     0.910                            \\
\bottomrule
\end{tabular}%
}
\label{tab:real}
\end{table}

\begin{table}[ht]
\centering
\caption{Quantitative results for space and time complexity on the synthetic Lego scene. Notations: Multi-Layer Perceptron (MLP), PD (pointwise deformation), FT (fine-tuning).}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
                                & Storage  & Wall-clock\\
Method & (MB) & time (ms)\\
\midrule
NeRF\cite{mildenhall2021nerf}          &  \phantom{00}5.00          &  2,950.0                     \\
DirectVoxGo\cite{SunSC22}              &  205.00         &   1,090.0                       \\
Plenoxels\cite{fridovich2022plenoxels} &  717.00         &  \phantom{0,0}50.0                       \\
NV\cite{Lombardi:2019}                 &  439.00  &         \phantom{0,0}74.9                               \\
D-NeRF\cite{pumarola2021d}             &   \phantom{00}4.00        & 8,150.0                         \\
NSFF\cite{li2020neural}                &\phantom{0}14.17        &   5,450.0                       \\
HyperNeRF\cite{park2021hypernerf}      &   \phantom{0}15.36       & 2,900.0                  \\
TiNeuVox-S\cite{tineuvox}              &   \phantom{0}23.70        & 3,280.0                          \\
TiNeuVox-B\cite{tineuvox}              &   \phantom{0}23.70        & 6,920.0                     \\ \midrule
DyLiN, w/o two MLPs, w/o FT (ours)                                 &     \phantom{0}68.04           &   \phantom{0,}115.4                         \\
DyLiN, w/o two MLPs (ours)                            &     \phantom{0}68.04           &    \phantom{0,}115.4                                \\
DyLiN, PD MLP only, w/o FT (ours)                                 &      \phantom{0}72.60          &     \phantom{0,}115.7                               \\
DyLiN, PD MLP only (ours)                            &      \phantom{0}72.60          &  \phantom{0,}115.7                              \\
DyLiN, w/o FT (ours)                             &      \phantom{0}70.11          &   \phantom{0,}116.0                                \\
DyLiN (ours)                        &      \phantom{0}70.11          &    \phantom{0,}116.0          \\
\bottomrule
\end{tabular}%
}
\label{tab:synth_spacetime}
\end{table}


\cref{tab:synth_spacetime} shows quantitative results for space and time complexity on the synthetic Lego scene.
We found that there is a trade-off between the two metrics, as prior works are typically optimized for just one of those.
In contrast, all of our proposed DyLiN variants settle at the golden mean between the two extremes.
When compared to the strongest baseline TiNeuVox-B, our method requires $3$ times as much storage but is nearly 2 orders of magnitude faster.
Plenoxels and NV, the only methods that require less computation than ours, perform much worse in quality.

\cref{fig:ablation} reports quantitative ablation results for dependencies on the number of sampled points per ray $K$ and on the number of training samples during KD $S$, performed on the synthetic Standup scene.
For dependence on $K$ (\cref{fig:ablation_sampled_point}), we found that there were no significant differences between test set PNSR scores for $K\in\{4,8,16,32\}$, while we encountered overfitting for $K\in\{64,128\}$.
This justified our choice of $K=16$ for the rest of our experiments.
Regarding the effect of $S$ (\cref{fig:ablation_pseudo_samples}), overfitting occured for smaller sample sizes including $S\in\{100;500;\SI{1000}{};\SI{5000}\}$.
The test and training set PSNR scores were much closer for $S=\SI{10000}{}$, validating our general setting.


\begin{figure}[ht]
\centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{imgs/ablation_study_sampled_points2.png}
  \caption{}
  %Number of sampled points}
  \label{fig:ablation_sampled_point}
\end{subfigure}%
\hfill
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{imgs/ablation_study_pseudo_samples2.png}
  \caption{}
  %Number of pseudo samples}
  \label{fig:ablation_pseudo_samples}
\end{subfigure}
\caption{Quantitative results for ablation on the synthetic Standup scene. (a)~Dependence on the number of sampled points $K$ across ray $r'$. (b)~Dependence on the number of training samples $S$ during Knowledge Distillation (KD).}
%Ablation studies. Networks were trained for 200K iterations on the scene standup. (a) Different number of sampled points, (b) Different number of pseudo samples.}
\label{fig:ablation}
\end{figure}

Our controllable numerical results are collected in \cref{tab:control}.
In short, our CoDyLiN was able to considerably outperform CoNeRF with respect to MS-SSIM and speed.

\begin{table}[ht]
\centering
\caption{Quantitative results on real controllable scenes. We utilized CoNeRF as the teacher model for our CoDyLiN. The winning numbers are highlighted in bold.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{Eyes/Mouth}  & \multicolumn{3}{c}{Transformer} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 &  &   & Wall-clock & &  & Wall-clock \\ 
Method & PSNR$\uparrow$ & MS-SSIM$\uparrow$ & time ($\si{\milli\second}$) & PSNR$\uparrow$ & MS-SSIM$\uparrow$ & time ($\si{\milli\second}$)\\
 \midrule
CoNeRF\cite{kania2022conerf}          &      \textbf{21.4658}     &   0.7458    & 6230.0&       23.0319  & 0.8878 & 4360.0           \\
CoDyLiN (ours)                        &       21.4655    &   \textbf{0.9510}   &  \phantom{0}\textbf{116.3} &      \textbf{23.5882} & \textbf{0.9779} & \phantom{0}\textbf{116.0}                   
\\
\bottomrule
\end{tabular}%
}
\label{tab:control}
\end{table}


\subsection{Qualitative Results}
\cref{fig:qual_synth} and \cref{fig:qual_real} depict qualitative results for reconstruction quality on synthetic and real dynamic scenes, respectively.
Both show that our full DyLiN model generated the sharpest, most detailed images, as it was able to capture cloth wrinkles (\cref{fig:jumping_ours2}) and the eye of the chicken (\cref{fig:chicken_ours2}).
%Both show that our full DyLiN model generated the sharpest, most detailed images, as it was able to capture cloth wrinkles (\cref{fig:jumping_ours2}) and poured coffee (\cref{fig:americano_ours2}).
The competing methods tended to oversmooth these features.
We also ablated the effect of omitting fine-tuning (\cref{fig:jumping_ours1}, \cref{fig:chicken_ours1}), and results declined considerably.
%We also ablated the effect of omitting fine-tuning (\cref{fig:jumping_ours1}, \cref{fig:americano_ours1}), and results declined considerably.

For the sake of completeness, \cref{fig:qual-real-ablation} illustrates qualitative ablation results for our model components on real dynamic scenes.
We found that sequentially adding our two proposed MLPs $T_\omega$ and $H_\psi$ improves the reconstruction, e.g., the gum between the teeth (\cref{fig:exp-ours3}) and the fingers (\cref{fig:banana-ours3}) become more and more apparent.
Without the MLPs, these parts were heavily blurred (\cref{fig:exp-ours1}, \cref{fig:banana-ours1}).

We kindly ask readers to refer to the supplementary material for CoDyLiN's qualitative results.
%The results demonstrate that, with little changes, DyLiN can be augmented to the controllable scenario. 


\begin{figure*}[!ht]
     \centering
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/hook_box.png}
         \caption*{\textbf{Hook}}
         \label{fig:hook_box}
     \end{subfigure}     
     \setcounter{subfigure}{0}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/hook_gt.png}
         \caption{Ground Truth}
         \label{fig:hook_gt}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/hook_dnerf.png}
         \caption{D-NeRF \cite{pumarola2021d}}
         \label{fig:hook_dnerf}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/hook_tineuvox.png}
         \caption{TiNeuVox \cite{tineuvox}}
         \label{fig:hook_tineuvox}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/hook_ours1.png}
         \caption{Ours-1}
         \label{fig:hook_ours1}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/hook_ours2.png}
         \caption{Ours-2}
         \label{fig:hook_ours2}
     \end{subfigure}    
     
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/jumping_box.png}
         \caption*{\textbf{Jumping Jacks}}
         \label{fig:jumping_box}
     \end{subfigure}     
     %\setcounter{subfigure}{0}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/jumping_gt.png}
         \caption{Ground Truth}
         \label{fig:jumping_gt}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/jumping_dnerf.png}
         \caption{D-NeRF \cite{pumarola2021d}}
         \label{fig:jumping_dnerf}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/jumping_tineuvox.png}
         \caption{TiNeuVox \cite{tineuvox}}
         \label{fig:jumping_tineuvox}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/jumping_ours1.png}
         \caption{Ours-1}
         \label{fig:jumping_ours1}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/synthetic_results/jumping_ours2.png}
         \caption{Ours-2}
         \label{fig:jumping_ours2}
     \end{subfigure}    


        \caption{Qualitative results on synthetic dynamic scenes. We compare our DyLiN (Ours-1, Ours-2) with the ground truth, the D-NeRF teacher model and TiNeuVox.
        %We utilized D-NeRF as the teacher model for our DyLiNs.
        Ours-1 and Ours-2 were trained without and with fine-tuning on the original data, respectively.}
        \label{fig:qual_synth}
\end{figure*}


\begin{figure*}[!ht]
     \centering
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth,height=2.03cm,keepaspectratio]{imgs/real_results/chicken/001.pdf}
         \caption*{\textbf{Chicken}}
         \label{fig:chicken_box}
     \end{subfigure}     
     \setcounter{subfigure}{0}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/chicken/002.pdf}
         \caption{Ground Truth}
         \label{fig:chicken_gt}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/chicken/003.pdf}
         \caption{HyperNeRF \cite{park2021hypernerf}}
         \label{fig:chicken_hypernerf}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/chicken/004.pdf}
         \caption{TiNeuVox \cite{tineuvox}}
         \label{fig:chicken_tineuvox}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/chicken/005.pdf}
         \caption{Ours-1}
         \label{fig:chicken_ours1}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/chicken/006.pdf}
         \caption{Ours-2}
         \label{fig:chicken_ours2}
     \end{subfigure}    
\iffalse
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth,height=2.03cm,keepaspectratio]{imgs/real_results/americano/001.pdf}
         \caption*{\textbf{Americano}}
         \label{fig:americano_box}
     \end{subfigure}     
     %\setcounter{subfigure}{0}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/americano/002.pdf}
         \caption{Ground Truth}
         \label{fig:americano_gt}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/americano/003.pdf}
         \caption{HyperNeRF \cite{park2021hypernerf}}
         \label{fig:americano_hypernerf}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/americano/004.pdf}
         \caption{TiNeuVox \cite{tineuvox}}
         \label{fig:americano_tineuvox}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/americano/005.pdf}
         \caption{Ours-1}
         \label{fig:americano_ours1}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/americano/006.pdf}
         \caption{Ours-2}
         \label{fig:americano_ours2}
     \end{subfigure}    
\fi

        \caption{
        Qualitative results on a real dynamic scene.
        %Qualitative results on real dynamic scenes.
        We compare our DyLiN (Ours-1, Ours-2) with the ground truth, the HyperNeRF teacher model and TiNeuVox.
        %We utilized HyperNeRF as the teacher model for our DyLiNs.
        Ours-1 and Ours-2 were trained without and with fine-tuning on the original data, respectively.}
        \label{fig:qual_real}
\end{figure*}

\begin{figure*}[!htb]
     \centering
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth,height=2.152cm,keepaspectratio]{imgs/expressions/expression1_box.jpg}
         \caption*{\textbf{Expression}}
         \label{fig:exp}
     \end{subfigure}     
     \setcounter{subfigure}{0}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/expressions/exp1_gt.png}
         \caption{Ground Truth}
         \label{fig:exp-gt}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/expressions/exp1_hyper.png}
         \caption{HyperNeRF \cite{park2021hypernerf}}
         \label{fig:exp-hyper}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/expressions/exp1_ours1.png}
         \caption{Ours-1}
         \label{fig:exp-ours1}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/expressions/exp1_ours2.png}
         \caption{Ours-2}
         \label{fig:exp-ours2}
     \end{subfigure}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/expressions/exp1_ours3.png}
         \caption{Ours-3}
         \label{fig:exp-ours3}
     \end{subfigure}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth,height=2.48cm,keepaspectratio]{imgs/real_results/banana_box.jpg}
         \caption*{\textbf{Peel Banana}}
         \label{fig:banana}
     \end{subfigure}     
     %\setcounter{subfigure}{0}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/banana_gt.png}
         \caption{Ground Truth}
         \label{fig:banana-gt}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/banana_hyper.png}
         \caption{HyperNeRF \cite{park2021hypernerf}}
         \label{fig:banana-hyper}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/banana_ours1.png}
         \caption{Ours-1}
         \label{fig:banana-ours1}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/banana_ours2.png}
         \caption{Ours-2}
         \label{fig:banana-ours2}
     \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{imgs/real_results/banana_ours3.png}
         \caption{Ours-3}
         \label{fig:banana-ours3}
     \end{subfigure}    


        \caption{Qualitative results for ablation on real dynamic scenes. We compare our DyLiN (Ours-1, Ours-2, Ours-3) with the ground truth and the HyperNeRF teacher model.
        %We utilized HyperNeRF as the teacher model for our DyLiNs.
        Ours-1 was trained without our two MLPs.
        Ours-2 was trained with pointwise deformation MLP only.
        Ours-3 is our full model with both of our proposed two MLPs.}
        %Qualitative comparison between our DyLiN network (Ours-1, Ours-2), D-NeRF, and TiNeuVox on synthetic scenes. Ours-1 is trained solely on pseudo data and Ours-2 is trained on both pseudo+real data.}
        \label{fig:qual-real-ablation}
\end{figure*}

%\subsection{Ablation Study}

%In this experiment, we investigated the effect of different sampled points and pseudo samples. All networks were trained for 200k iterations on the scene standup.

%\paragraph{Sampled Points.}
%We evaluated six different sampled points along a ray as shown in Fig.~\ref{fig:ablation_sampled_point}. There is no significant difference between sampled points $K=4, 8, 16,$ and $32$ for the test PSNR while it is degraded for sampled points $K=32, 64$. In our experiments, we used $K=16$.

%\paragraph{Pseudo Samples.}
%We evaluated five different pseudo samples as shown in Fig.~\ref{fig:ablation_pseudo_samples}. Since pseudo samples $S=10.0k$ shows the best test PSNR, we used $S=10.0k$ in our experiment. Overfitting is observed for small numbers of pseudo images. While the train PSNR for $S=0.1k$ keeps increasing along the iteration axis, the test PSNR is not improving.

%\section{Discussion}

\section{Conclusion}
We proposed two architectures for extending LFNs to dynamic scenes.
Specifically, we introduced DyLiN, which models ray deformations without bending and lifts whole rays into a hyperspace, and CoDyLiN, which allows for controllable attribute inputs.
%Specifically, we introduced DyLiN for explicit ray deformations and lifting whole rays into a hyperspace, and CoDyLiN for controllable attribute inputs.
%Specifically, we introduced DyLiN for deformations and hyperspace representations, and CoDyLiN for controllable attribute inputs.
We trained both techniques via knowledge distillation from various dynamic NeRF teacher models.
We found that DyLiN produces state-of-the-art quality even without ray bending and CoDyLiN outperforms its teacher model,
%confirming that ray bending is not necessary to model complex scene dynamics,
while both are nearly 2 orders of magnitude faster than their strongest baselines.
%We confirmed that complex scene dynamics can be modeled without ray bending.
% In addition, our relative improvement over the teacher model is better than that of R2L.

Our methods do not come without limitations, however.
Most importantly, they focus on speeding up inference, as they require pretrained teacher models, which can be expensive to obtain.
In some experiments, our solutions were outperformed in terms of the PSNR score. Using the winners as teacher models could improve performance.
%may yield some improvement.
Additionally, distillation from multiple teacher models or joint training of the teacher and student models are also yet to be explored.
Moreover, we currently represent rays implicitly by sampling $K$ points along them, but increasing this number can lead to overfitting. An explicit ray representation may be more effective.
Finally, voxelizing and quantizing our models could improve efficiency.
\iffalse
Our methods do not come without limitations, however.
Most importantly, they are restricted to speeding up inference, as they assume the availability of pretrained teacher models, which are costly to obtain.
In some experiments, our solutions were overtaken in terms of the PSNR score.
This could be alleviated by employing the winners as teacher models.
Additionally, distillation from multiple teacher models or joint training of the teacher and student models are also yet to be explored.
Moreover, we currently represent rays implicitly by sampling $K$ points along them, and increasing this number leads to overfitting.
An explicit ray representation could be more beneficial from this perspective.
%, and spatio-temporal correlations between rays could be exploited, too.
% inference, not training
% sota image generators as priors
% per-scene training
Finally, voxelizing \cite{tineuvox} and quantizing our models could be a fruitful direction towards more efficiency.
\fi

Our results are encouraging steps towards achieving real-time volumetric rendering and animation, and we hope that our work will contribute to the progress in these areas.
%emerging areas.
%In the future, further speed-ups may be achieved via 
%Using other teacher models, e.g., TiNeuVox
%Meta-learning for generalization across scenes.

\section*{Acknowledgements}
This research was supported partially by Fujitsu. We thank Chaoyang Wang from Carnegie Mellon University for the helpful discussion.


%-------------------------------------------------------------


% conerf result
%\begin{figure*}
%\begin{minipage}{\linewidth}
%\centering
%\captionof{table}{Quantitative results on real controllable scenes. The winning numbers are highlighted in bold.}
%\bigskip

%\begin{figure*}[ht]
% \centering
%  \includegraphics[width=0.99\linewidth]{imgs/conerf_transformer_zoom.pdf}
  %\captionof{figure}{Qualitative results on real controllable scenes. Best viewed zoomed in.}
%  \caption{Qualitative results on real controllable scenes.}
%  \label{fig:cor2l_res}
%\end{figure*}
%\end{minipage}
%\end{figure*}








% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of cvpr.sty to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the CVPR70 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%   We describe a system for zero-g frobnication.
%   This system is new because it handles the following cases:
%   A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%   Ours handles it by including a foo term in the bar integral.

%   ...

%   The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%   It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%   \caption{Example of caption.
%   It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%   \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%   ``Frobnication has been trendy lately.
%   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.


% % Update the cvpr.cls to do the following automatically.
% % For this citation style, keep multiple citations in numerical (not
% % chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% % \cite{Alpher02,Alpher03,Authors14}.


% % \begin{figure*}
% %   \centering
% %   \begin{subfigure}{0.68\linewidth}
% %     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
% %     \caption{An example of a subfigure.}
% %     \label{fig:short-a}
% %   \end{subfigure}
% %   \hfill
% %   \begin{subfigure}{0.0.6+28\linewidth}
% %     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
% %     \caption{Another example of a subfigure.}
% %     \label{fig:short-b}
% %   \end{subfigure}
% %   \caption{Example of a short caption, which should be centered.}
% %   \label{fig:short}
% % \end{figure*}

% %------------------------------------------------------------------------
% % \section{Related Work}
% % \label{sec:formatting}
% % \paragraph{Neural scene representations.}
% % \paragraph{Neural light field (NeLF).}
% % All text must be in a two-column format.
% % The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% % Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% % The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% % The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% % On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% % for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% % page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



% %-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


% %-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

% \begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%   \usepackage{graphicx} ...
%   \includegraphics[width=0.8\linewidth]
%                   {myfile.pdf}
% \end{verbatim}
% }


% %-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.

% %------------------------------------------------------------------------
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
