%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure:framework
\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{Figures/framework.pdf}
\caption{The schematic illustration of our proposed PSTNet. The current frame $\mathbf{I}_{t}$ is fed into three parallel branches to extract features of three different characteristic: physical characteristic (\textcolor[RGB]{113,151,152}{top branch}), spatio characteristic (\textcolor[RGB]{159,179,150}{middle branch}), and temporal characteristic (\textcolor[RGB]{237,145,147}{bottom branch}). In bottom branch, the adjacent frame $\mathbf{I}_{t+1}$ will also be used to compute the optical flow $\mathbf{O}_{t, t+1}$. Next, a tailored feature fusion module will aggregate those multi-characteristic features in a progressive manner, thereby performing the final shadow removal prediction $\mathbf{R}_{t}^{final}$. }
\label{fig:framework}
\vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Method} \label{sec:method}
To the best of our knowledge, so far there is no deep-learning based method for video shadow removal. In this paper, we propose a baseline network for this task, termed \textbf{PSTNet}, which learns the union features of physical, spatio, and temporal characteristics to perform video shadow removal. 

\subsection{Overview of Our Network}
\label{sec:overview}
Figure~\ref{fig:framework} presents the schematic illustration of our PSTNet. The network takes the frame $\mathbf{I}_{t}$ and its next frame $\mathbf{I}_{t+1}$ as inputs, then outputs the shadow removed result of the $t$-th frame in an end-to-end manner. 
%%
The intuition behind our network is to leverage complementary information of physical, spatio, and temporal characteristics of moving shadows. 
%%
We explicitly extract the three kinds of features with three independent branches, followed by a multi-characteristics fusion module to achieve the hybrid features in a progressive manner. For convenience, we first introduce the workflows of the three branches.

\noindent
\textbf{Physical characteristic branch.} Shadows are regular physical phenomena produced by occluded lighting. The removal of shadows is inseparable from the analysis of physical lighting conditions. 
%%
Following SID~\cite{le2021physics}, shadow removal can be considered as the physical re-exposure problem. However, SID uses a uniform light model for different areas, ignoring the effects of complex lighting and background textures.
%%
To remedy this problem, we extract the physical characteristics by estimating the adaptive exposure parameters in each shadow region, and using an encoder-decoder sub-network to further learn the broad contextual information due to large receptive fields. 

%%
Specifically, the shadow image $\mathbf{I}_{t}$$\in$$\mathcal{R}^{3 \times W\times H}$ is first fed into AEEM (Adaptive Exposure Estimation Module) to estimate the re-exposure parameters, yielding the over-exposure image $\mathbf{L}_{t}$$\in$$\mathcal{R}^{3 \times W\times H}$. 
%%
Then we feed the concatenation of $\mathbf{I}_{t}$ and $\mathbf{L}_{t}$ into a encoder-decoder sub-network to learn the physical features in the hierarchical manner. 
%%
To make the physical feature more focused on the shadow regions, we append an extra  decoder served for shadow detection. Here, we utilize a widely-used UNet~\cite{UNet2015} structure as the encoder-decoder backbone. 
%%
By this way, we can obtain $\mathbf{F}_{t}^{rem}$ and $\mathbf{F}_{t}^{msk}$ from shadow removal decoder and shadow mask decoder, respectively. 
%%
To further enhance the physical features, we design a Mask-guided Supervised Attention Module (MSAM). 
%MASM provides ground-truth supervisory signals useful for shadow removal and shadow mask segmentation. In addition, 
It enhances the physical features $\mathbf{F}_{t}^{rem}$ to $\mathbf{F}_{t}^{ph}$ with the guidance of the ground-truth supervision through attention mechanism. 
%Last, physical feature $\mathbf{F}_{t}^{ph}$ and the hierarchical features in removal decoder will be passed to the fusion module for further processing.
% includes the Adaptive Exposure Estimation Module(AEEM), the couple-task encoder-decoder structure, and the Mask-guided Supervised Attention Module(MSAM). 

\noindent
\textbf{Spatio characteristic branch.} Shadow removal is a position-sensitive task, since it needs to establish the pixel-to-pixel correspondence from the input to the output.
%%
To preserve the desired fine texture in the final results, in the spatio branch, we employ a sub-network that operates on the original input frame resolution (without any downsampling operation).
%, thereby preserving the desired fine texture in the final shadow removal results. 
%%
To be specific, the sub-network extracts spatio features with a $3$$\times$$3$ convolution followed by a self channel attention block which has the same structure as Figure~\ref{fig:TAB} (b). Then we can get spatio features $\mathbf{F}_{t}^{sp}$. 
%as the output of the spatio branch.

\noindent
\textbf{Temporal characteristic branch} is committed to extract the temporal knowledge for the current frame. 
%%
In video restoration tasks~\cite{wang2019edvr, chan2021basicvsr}, the use of adjacent frames has been shown to facilitate the prediction of the current frame due to the continuity of the video. 
%%
The temporal information brought by adjacent frames are apt to distinguishes different objects in the current frame.
%%
Here, we model the temporal information by computing the optical flow map.
%%
Specifically, the current frame $\mathbf{I}_{t}$ and the next frame $\mathbf{I}_{t+1}$ are fed into a well-trained optical flow estimation network (here, we choose FlowNet2~\cite{ilg2017flownet}) to generate the optical flow $\mathbf{O}_{t, t+1}$$\in$$\mathcal{R}^{3 \times W\times H}$. 
%%
Next, $\mathbf{O}_{t, t+1}$ is further encoded by a $3$$\times$$3$ convolution followed by a self channel attention block which has the same structure as Figure~\ref{fig:TAB} (b), which yields the temporal features $\mathbf{F}_{t}^{te}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: AEEM
\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{Figures/AEEM.pdf}
\caption{The schematic illustration of our Adaptive Exposure Estimation Module (AEEM). }
\label{fig:AEEM}
\vspace{-2.5mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Adaptive Exposure Estimation Module}
\label{sec:aeem}
Considering a shadow frame $\mathbf{I}_{t}$, some recent shadow removal methods~\cite{le2021physics,fu2021auto}, which use physical shadow models, mainly learn to over-exposure $\mathbf{I}_{t}$ to a lightened version, and then fuse them to acquire the  shadow-free image by estimating a shadow matte. 
%%
The common ground of these works is that for simplicity, only one group of linear parameters (denoted as $[\mathrm{w}, \mathrm{b}]$) is predicted for the entire frame $\mathbf{I}_{t}$. With estimated $[\mathrm{w}, \mathrm{b}]$, the over-exposure image $\mathbf{L}_{t}$ can be formulized as follow:
\begin{equation}\label{Equ:I2L}  
    \mathbf{L}_t = \mathrm{w}_t \times \mathbf{I}_t + \mathrm{b}_t.
\end{equation}
%, which greatly reduces the solution space of the problem. 
%%
However, due to the complex changes in lighting and texture, it may be difficult to fit different regions with only one set of exposure parameters. 
%%
In our work, we introduce the Adaptive Exposure Estimation Module (AEEM) to estimate respective $[\mathrm{w},\mathrm{b}]$ for different regions rather than the whole image. 
%The schematic illustration of AEEM is shown in Figure~\ref{fig:AEEM}. 
%%
%Its contributions are two-fold. 
%First, the input image is split into several patches and the estimations of $[\mathrm{w},\mathrm{b}]$ are generated for these patches rather than the whole image.
This can alleviate conflicts in case that  $[\mathrm{w},\mathrm{b}]$ for different regions are inconsistent. 
%%
What's more, we utilize the transformer encoder to build the long-range relationships between image patches, which smooths the estimations of each $[\mathrm{w},\mathrm{b}]$. After AEEM, we can obtain the over-exposure image $\mathbf{L}_{t}$.

As illustrated in Figure~\ref{fig:AEEM}, AEEM takes the shadow image $\mathbf{I}_t$ as input and first splits it into $N$ patches. In our experiments, we set $N$=$4$. 
%%
%After arranging these patches in regular order, the patch embedding and position embedding operations are used to generate the transformer input. 
With patch embedding and position embedding, we employ a transformer encoder to produce the $[\mathrm{w},\mathrm{b}]$ sequence, denoted as $\{[\mathrm{w}_t^n, \mathrm{b}_t^n]\}_{n=1}^N$. 
%%
For convenience, we inherit the same transformer encoder structure as the widely known ViT~\cite{dosovitskiy2020image}, which uses 6 transformer blocks with cascaded multi-head attention module and multi-layer perceptron.
%%
After obtaining $\{[\mathrm{w}_t^n, \mathrm{b}_t^n]\}_{n=1}^N$, we re-exposure the shadow image $\mathbf{I}_t$ to a lightened version $\mathbf{L}_t$ by:
\begin{equation}\label{Equ:It2Lt}  
    \mathbf{L}_t^n = \mathrm{w}_t^n \times \mathbf{I}_t^n + \mathrm{b}_t^n,
\end{equation}
where $\mathbf{I}_t^n$ is $n$-th patch of $\mathbf{I}_t$; $\mathbf{L}_t^n$ is the corresponding lightened version of $\mathbf{}_t^n$. 
%%
Then, we obtain the estimated $\mathbf{L}_t$ by assembling $\{\mathbf{L}_t^n\}^N_{n=1}$ in the original spatial order.
Compared with estimating uniform [w,b] for the whole image as SID\cite{le2021physics}, our proposed AEEM can be proved to handle more complex scenarios (see \ref{sec:ablation} for details).

\subsection{Mask-guided Supervised Attention Module}
\label{sec:msam}

Mask-guided Supervised Attention Module (MSAM) is employed to enhance the physical features $\mathbf{F}_{t}^{rem}$ to $\mathbf{F}_{t}^{ph}$ with the guidance of the ground-truth supervision through attention mechanism. 
%%
The motivations behind MSAM include third main aspects. 
%%
First, ground-truth supervisory signals is useful for progressive shadow removal. 
%%
Second, a well-segmented shadow mask can guide the network to suppress the less informative features (mainly existing in non-shadow regions) and enhance the useful features (mainly existing in shadow regions).
%%
Third, multi-task learning can leverage a stronger encoder via training with multiple types of supervised labels.

As illustrated in Figure~\ref{fig:MSAM}, on the one hand, MSAM takes the features $\mathbf{F}_t^{rem}$$\in$$\mathcal{R}^{C \times W\times H}$ from shadow removal decoder to generate the residual image $\mathbf{X}_t$$\in$$\mathcal{R}^{3 \times W\times H}$ with a simple $1$$\times$$1$ convolution, where $W\times H$ denotes the spatial dimension and $C$ is the number of channels. 
%%
The residual image $\mathbf{X}_t$ is added to the input image $\mathbf{I}_t$ to obtain the coarse shadow-free estimation $\mathbf{R}_t^{middle}$. 
%%
\begin{equation}\label{Equ:R_middle}  
    \mathbf{R}_t^{middle} = \mathbf{I}_t + \mathtt{Conv}(\mathbf{F}_t^{rem}),
\end{equation}
where $\mathtt{Conv}$ denotes the simple $1$$\times$$1$ convolution. On the other hand, MSAM takes the feature $\mathbf{F}_t^{msk}$$\in$$\mathcal{R}^{C \times W\times H}$ from shadow detection decoder to generate the shadow mask prediction $\mathbf{M}_t$$\in$$\mathcal{R}^{1 \times W\times H}$. 
%%
For these predicted image $\mathbf{R}_t^{middle}$ and $\mathbf{M}_t$, we exert explicit supervision with the ground-truth image. 
%%
Next, per-pixel attention maps $\mathbf{F}_t^{att}$$\in$$\mathcal{R}^{C \times W\times H}$ are generated from the hybrid feature, concatenated by $\mathbf{R}_t^{middle}$ and $\mathbf{M}_t$, using a simple $1$$\times$$1$ convolution followed by the sigmoid activation.
%%
Then, $\mathbf{F}_t^{att}$ are then employed to guide the transformed $\mathbf{F}_t^{rem}$ (obtained after $1$$\times$$1$ convolution), resulting in the attention-guided residual features which are added to the identity mapping path. Consequently, the attention-augmented $\mathbf{F}_t^{ph}$ can be formulized as follows:
\begin{equation}\label{Equ:F_ph}  
\begin{aligned}
    \mathbf{F}_t^{ph} &=& \mathbf{F}_t^{rem} + \mathtt{Conv}(\mathbf{F}_t^{rem}) \odot \mathbf{F}_t^{att} \ , \\
    where~\mathbf{F}_t^{att} &=& \sigma(\mathtt{Conv}(\mathtt{Cat}(\mathbf{R}_t^{middle}, \mathbf{M}_t))) \ ,
\end{aligned}
\end{equation}
%%
where $\sigma$ denotes sigmoid activation; $\mathtt{Cat}$ denotes concatenation operation. Then, the attention-augmented feature $\mathbf{F}_t^{ph}$ will be passed to the final multi-characteristics fusion.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: MSAM
\begin{figure}[]
\centering
\includegraphics[width=0.48\textwidth]{Figures/MSAM.pdf}
\caption{The schematic illustration of  Mask-guided Supervised Attention Module (MSAM).}
\label{fig:MSAM}
\vspace{-2.5mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multi-Characteristics fusion}
\label{sec:mcf}
In this section, we present a dedicated feature fusion module to aggregate physical/spatio/temporal characteristic features. 
%%
Specifically, as shown in Figure~\ref{fig:framework}, first, the physical features $\mathbf{F}_t^{ph}$ and spatio features $\mathbf{F}_t^{sp}$ are concatenated. 
%%
Then we introduce three Temporal-aware Attention Blocks (TAB) to further fuse the temporal features $\mathbf{F}_t^{te}$.
%%
It is worth noting that due to the different ways in which temporal information and spatial information are encoded, we consider using temporal features for filtering the important channels of hybrid features instead of direct concatenation.
%%
After each TAB, the generated features will also be added with the upsampled features $\{\mathbf{D}_i\}_{i=1}^{3}$ from the shadow removal decoder in the physical branch to obtain richer scale features. The output of TABs can be formulized as:
\begin{equation}
\label{Equ:TAB}
\left\{
\begin{aligned}
\mathbf{B}^{out}_{1} &=& &\mathrm{TAB}_{1}(\mathtt{Cat}(\mathbf{F}_t^{ph}, \mathbf{F}_t^{sp}), \mathbf{F}_t^{te}), \\
\mathbf{B}^{out}_{i} &=& &\mathrm{TAB}_{i}(\mathbf{B}^{out}_{i-1} + \mathbf{D}_{i-1}, \mathbf{F}_t^{te}), i = 2, 3,
\end{aligned}
\right.
\end{equation}
where $\mathrm{TAB}_{i}$ denotes each TAB block; $\mathbf{B}^{out}_{i}$ denotes the output of each TAB block.
%%
%Finally, the last hybrid features will pass a single convolution layer then add to the original input $\mathbf{I}_{t}$ to get the final shadow removal output $\mathbf{R}_{t}^{final}$. 
Finally, the last hybrid features will add to the original input $\mathbf{I}_{t}$ to get the final shadow removal output $\mathbf{R}_{t}^{final}$ as follows:
\begin{equation}\label{Equ:TAB2}  
    \mathbf{R}_t^{final} = \mathbf{I}_t + \mathtt{Conv}(\mathbf{B}^{out}_{3} + \mathbf{D}_{3}),
\end{equation}
%%
Note that the fusion module operates on the original input image resolution.
%%
$\mathbf{F}_t^{ph}$ and $\mathbf{F}_t^{te}$ should be upsampled to $W \times H$ before being fed into the fusion module. 
%%
Meanwhile, there is no downsampling operation in fusion module, thereby preserving the desired fine texture in the final output image.

\noindent
\textbf{Temporal-aware Attention Block.} TAB mainly consists of two types of channel attention blocks. Previous works~\cite{zhang2018image,zamir2021multi} demonstrate that the stacking of channel attention modules can help to achieve significant performance gains in most image restoration tasks.
%%
In our work, we follow this progressive processing manner as \cite{zamir2021multi} and consider the extra temporal features $\mathbf{F}_t^{te}$ to make the model be aware of temporal information.
%In our work, we follow this progressive feature augmented with stack channel attentions and consider the extra temporal features $\mathbf{F}_t^{te}$ to make the feature be aware of temporal information.

As illustrated in Figure~\ref{fig:TAB} (a), TAB includes $k$ self channel attention blocks (SCAB) and one cross channel attention block (CCAB), while TAB adopts residual learning to make convergence easier. In our experiments, we set $k$=$8$. SCAB takes progressive hybrid features as input, and CCAB takes progressive hybrid features and temporal features $\mathbf{F}_t^{te}$ as inputs. 
%%
Figure~\ref{fig:TAB} (b) show the details of SCAB.
%%
It first encodes shadow removal features with two $3$$\times$$3$ convolution followed by the PReLU activation. The global information is extracted via a global average pooling operation (GAP), then fed into two $1$$\times$$1$ convolutions followed by a sigmoid activation to achieve the channel attention maps. Finally, the augmented features re-calibrated by these channel attention maps will be passed to the next block.
%%
Figure~\ref{fig:TAB} (c) show the details of CCAB.
%%
It first concatenates the $\mathbf{F}_t^{te}$ to the main branch hybrid features. Next, a simple $1$$\times$$1$ convolution is used to lower the dimension of these hybrid features, then the rest process is same as SCAB. 
%%
In this way, $\mathbf{O}_{t, t+1}$ is encoded into the shadow removal features and controls the expression of temporal information.
%Figure~\ref{fig:TAB} (b) show the details of SCAB. SCAB first encodes shadow removal features with two $3$$\times$$3$ convolution followed by the PReLU activation. The global average pooling operation (GAP) is used to extract the global information, then fed them into two $1$$\times$$1$ convolution followed by a sigmoid activation to achieve the channel attention maps. Finally, augmented features re-calibrated by these channel attention maps will be passed to the next block. Figure~\ref{fig:TAB} (c) show the details of CCAB. CCAB first concatenate the $\mathbf{F}_t^{te}$ to the main branch hybrid features. Next, a simple $1$$\times$$1$ convolution is used to lower the dimension of these hybrid features, then the rest of channel attention process is same as SCAB. In this way, $\mathbf{O}_{t, t+1}$ is encoded into the shadow removal features and control the expression of temporal information.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: TAB
\begin{figure}[]
\centering
\includegraphics[width=0.5\textwidth]{Figures/TAB.pdf}
\caption{(a) is the schematic illustration of our Temporal-aware Attention Block (TAB); (b) is one of the self channel attention block (SCAB) in (a); (c) is the cross channel attention block (CCAB) in (a). }
\label{fig:TAB}
\vspace{-2.5mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% As the first work for video shadow removal, we propose a baseline multi-stage network which will be introduced in detail in Sec.~\ref{sec:overview}. This network mainly consists of three sub-modules, including AEEM, MSAM, and MAB. We will go into the motivations and details of each of the three modules in Sec.~\ref{sec:aeem}-Sec.~\ref{sec:mab}
% \subsection{Overview of Our Network}
% \label{sec:overview}
% Figure~\ref{fig:framework} shows the schematic illustration of our Multi-Stage Adaptive Exposure Network, termed MSAE-Net. The intuition behind our network is to leverage complementary of physics, semantic, and motion knowledge. To do so, MSAE-Net is consists of three corresponding stages. 

% The first stage try to consider physical properties of shadow producing. Inspired by xxx, shadow removal can be considered as the physics re-exposure problem. A single linear function can fit the re-exposure process: $\mathbf{L}_{t} = w \times \mathbf{I}_{t} + b$, where $\mathbf{I}_t$$\in$$\mathcal{R}^{3 \times W\times H}$ denote t-th frame in a video, $\mathbf{L}_{t}$$\in$$\mathcal{R}^{3 \times W\times H}$ denote the corresponding re-exposure image, and scalars [w, b] are parameters need to be estimated. Consequently, the complicated shadow removal task is converted to the simple regression problem of two scalars. In this paper, we design AEEM to realize the estimation of [w, b]. Different from xxx, transformer-based AEEM consider the inconsistencies in exposure in different regions and estimate the adaptive [w, b] in each region. After stage one, we obtain the re-exposure image $\mathbf{L}_{t}$.

% The second stage is based on encoder-decoder subnetworks that learn the broad contextual information due to large receptive fields. Here, we focuses on extracting the high-level semantic information to achieve the coarse shadow removal and shadow mask segmentation. As shown in Figure~\ref{fig:framework}, $\mathbf{I}_{t}$ and $\mathbf{L}_{t}$ are concatenated to fed into an encoder, then produce the shadow removal feature $\mathbf{F}_{t}^{rem}$ and shadow mask feature $\mathbf{F}_{t}^{msk}$ through the two branches decoder. We utilize a widely-used UNet~\cite{UNet2015} structure as the encoder-decoder backbone. To obtain the coarse shadow-free estimation $\mathbf{R}_{t}^{middle}$ and shadow mask $\mathbf{M}_{t}$, we further design a Mask-guided Supervised Attention Module (MSAM). MASM provides ground-truth supervisory signals useful for shadow removal and shadow mask segmentation. In addition, MSAM enhances the shadow removal feature $\mathbf{F}_{t}^{rem}$ to $\mathbf{F}_{t}^{mrem}$ with the supervised predictions $\mathbf{R}_{t}^{middle}$ and $\mathbf{M}_{t}$ through attention mechanism.

% In last two stages, only single image $\mathbf{I}_{t}$ is considered. However, in some similar video restoration tasks~\cite{}, the use of adjacent frames has been shown to facilitate the prediction of the current frame due to the continuity of the video. The motion information brought by adjacent frames are apt to distinguishes different objects in the current frame. Here, we obtain the motion information by computing the optical flow map. Specifically, the current frame $\mathbf{I}_{t}$ and next frame $\mathbf{I}_{t+1}$ are fed into a well-trained optical flow estimation network (here, we choose FlowNet2~\cite{}) to generate the optical flow $\mathbf{O}_{t, t+1}$$\in$$\mathcal{R}^{3 \times W\times H}$. The third stage concentrates on leveraging these motion information to refine the result of shadow removal. Firstly, $\mathrm{Conv}(\mathbf{I}_{t})$ and $\mathbf{F}_{t}^{mrem}$ are concatenated to $\mathbf{F}_{t}^{1}$ as the input of third stage, where $\mathrm{Conv}$ denotes a single convolutional layer. Then we introduce three Motion-aware Attention Blocks (MAB)[$\mathrm{MAB}_{1}, \mathrm{MAB}_{2}, \mathrm{MAB}_{3}$] to augment features with aggregating the extra motion information. MAB consists of some cascade self and cross channel attention blocks like figure~\ref{fig:MAB}(a). $\mathrm{MAB}_{k}$ take $\mathbf{F}_{t}^{k}$ and $\mathbf{O}_{t, t+1}$ as input then output the augmented features, where $(k$$\in$$\{1,2,3\})$. After each MAB, the output feature will be also added the upsampled features from second stage upper decoder to obtain richer scale features. Finally, the last feature will path a single convolutional layer then add to the original input $\mathbf{I}_{t}$ to get the final shadow removal output $\mathbf{R}_{t}^{final}$. It is worth noting that, the third stage employs a subnetwork that operates on the original input image resolution (without any downsampling operation), thereby preserving the desired fine texture in the final output image.



% \subsection{Adaptive Exposure Estimation Module}
% \label{sec:aeem}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: AEEM
% \begin{figure}[!t]
% \centering
% \includegraphics[scale=.5]{Figures/AEEM.pdf}
% \vskip -5pt
% \caption{The schematic illustration of our Adaptive Exposure Estimation Module(AEEM); See Section~\ref{sec:aeem} for details.}
% \label{fig:AEEM}
% \vspace{-2.5mm}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Recent shadow removal methods~\cite{}, based on physical shadow models, mainly learn to re-exposure the shadow image to a lit version and then fuse them together to acquire the desired shadow-free image via a shadow matte. The common of these works is that only one group of linear parameters (denote as $[\mathrm{w}, \mathrm{b}]$) is predicted for each image, which greatly reduces the solution space of the problem. However, in some scenes, due to the changes in lighting and materials, the parameters $[\mathrm{w},\mathrm{b}]$ of different regions vary greatly. Instead, we introduce an Adaptive Exposure Estimation Module(AEEM), which leverage distinguishing $[\mathrm{w},\mathrm{b}]$ between different regions. The schematic diagram of AEEM is shown in Figure~\ref{fig:AEEM}, and its contributions are two-fold. First, the input image is split into several patches and the estimations of $[\mathrm{w},\mathrm{b}]$ are generated for these patches rather than whole image. It alleviates conflicts in which $[\mathrm{w},\mathrm{b}]$ for different regions are inconsistent. Second, we utilize the transformer encoder building the long-range relationships between patches, which smooths the estimations of each $[\mathrm{w},\mathrm{b}]$.

% As illustrated in Figure~\ref{fig:AEEM}, AEEM takes the shadow image $\mathbf{I}_t$ and first split it into $N$ patches, where t denotes the current t-th frame. In our experiments, we set $N$=$4$. After arranging these patches in regular order, the patch embedding and position embedding operations are used to generate the transformer input. Then a transformer encoder is employed to product the $[\mathrm{w},\mathrm{b}]$ sequence, denoted as $\{[\mathrm{w}_t^n, \mathrm{b}_t^n]\}_{n=1}^N$. For convenience, we inherit the same transformer encoder structure as widely known ViT~\cite{}, which use several transformer blocks with cascade multi-head attention module and multi-layer perceptron.

% After obtaining $\{[\mathrm{w}_t^n, \mathrm{b}_t^n]\}_{n=1}^N$, we re-exposure the shadow image $\mathbf{I}_t$ to a lit version $\mathbf{L}_t$ by:
% \begin{equation}\label{Equ:It2Lt}  
%     \mathbf{L}_t^n = \mathrm{w}_t^n \times \mathbf{L}_t^n + \mathrm{b}_t^n,
% \end{equation}
% where $\mathbf{I}_t^n$ is n-th patch of $\mathbf{I}_t$; $\mathbf{L}_t^n$ is the corresponding lit version of $\mathbf{}_t^n$. Then, we achieve $\mathbf{L}_t$ by assembling $\{\mathbf{L}_t^n\}^N_{n=1}$ in the original order.

% \subsection{Mask-guided Supervised Attention Module}
% \label{sec:msam}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: MSAM
% \begin{figure}[!t]
% \centering
% \includegraphics[scale=.38]{Figures/MSAM.pdf}
% \vskip -5pt
% \caption{The schematic illustration of our Mask-guided Supervised Attention Module(MSAM); See Section~\ref{sec:msam} for details.}
% \label{fig:MSAM}
% \vspace{-2.5mm}
% \end{figure}
% In this section, we will introduce our Mask-guided Supervised Attention Module(MSAM) in details. The motivations behind MSAM include third main aspects. First, multi-task learning can leverage a stronger encoder via training with multiple types of supervised labels. Second, ground-truth supervisory signals is useful for progressive shadow removal. Third, a well-segmented shadow mask can guide the network to suppress the less informative features (mainly existing in non-shadow regions) and enhance the useful features (mainly existing in shadow regions).

% As illustrated in Figure~\ref{fig:MSAM}, on the one hand, MSAM take the incoming feature $\mathbf{F}_t^{rem}$$\in$$\mathcal{R}^{C \times W\times H}$ from upper UNet decoder output in second stage and then generate the residual image $\mathbf{X}_t$$\in$$\mathcal{R}^{3 \times W\times H}$ with a simple $1$$\times$$1$ convolution, where $W\times H$ denotes the spatial dimension and $C$ is the number of channels. The residual image $\mathbf{X}_t$ is added to the input image $\mathbf{I}_t$ to obtain the coarse shadow-free estimation $\mathbf{R}_t^{middle}$. On the other hand, MSAM take the incoming feature $\mathbf{F}_t^{msk}$$\in$$\mathcal{R}^{C \times W\times H}$ from lower UNet decoder output in second stage and then generate the shadow mask prediction$\mathbf{M}_t$$\in$$\mathcal{R}^{1 \times W\times H}$. To these predicted image $\mathbf{R}_t^{middle}$ and $\mathbf{M}_t$, we provide explicit supervision with the ground-truth image. Next, per-pixel attention maps $\mathbf{F}_t^{att}$$\in$$\mathcal{R}^{C \times W\times H}$ are generated from the hybrid feature, concatenated by $\mathbf{R}_t^{middle}$ and $\mathbf{M}_t$, using a simple $1$$\times$$1$ convolution followed by the sigmoid activation. These maps are then employed to guide the transformed $\mathbf{F}_t^{rem}$ (obtained after $1$$\times$$1$ convolution), resulting in attention-guided features which are added to the identity mapping path. Finally, the attention-augmented feature $\mathbf{F}_t^{mrem}$ is passed to the third stage for further processing.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Motion-aware Attention Block}
% \label{sec:mab}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: MAB
% \begin{figure}[!t]
% \centering
% \includegraphics[scale=.55]{Figures/MAB.pdf}
% \vskip -5pt
% \caption{The schematic illustration of our Motion-aware Attention Block(MAB); See Section~\ref{sec:mab} for details.}
% \label{fig:MAB}
% \vspace{-2.5mm}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In this section, we will introduce our Motion-aware Attention Block(MAB). MAB mainly consists of two types of channel attention blocks. Some related works~\cite{} demonstrate that the stacking of channel attention modules can facilitates achieving significant performance gain in most image restoration tasks. In our work, we follow this progressive feature augmented with stack channel attentions and consider the extra optical flow to make the feature be aware of motion information.

% As illustrated in Figure~\ref{fig:MAB} (a), MAB includes $k$ self channel attention blocks (SCAB) and one cross channel attention block (CCAB). In our experiments, we set $k$=$8$. SCAB take progressive shadow removal features as input. CCAB take progressive shadow removal features and optical flow $\mathbf{O}_{t, t+1}$ as input. MAB adpots residual learning to make convergence easier. Figure~\ref{fig:MAB} (b) show the details of SCAB. SCAB first encode shadow removal features with two $3$$\times$$3$ convolution followed by the PReLU activation. Next, the global average pooling operation (GAP) is used to extract the global information, then fed them into two $1$$\times$$1$ convolution followed by a sigmoid activation to achieve the channel attention maps. Finally, augmented features re-calibrated by these channel attention maps will be passed to the next block. Figure~\ref{fig:MAB} (c) show the details of CCAB. The dispose of shadow removal features in CCAB is similar as SCAB. To the extra input $\mathbf{O}_{t, t+1}$, CCAB first encode $\mathbf{O}_{t, t+1}$ with three $3$$\times$$3$ convolutions followed by the PReLU activation, then concatenate these features and encoded shadow removal features. Next, a simple $1$$\times$$1$ convolution is used to lower the dimension of these hybrid features, then the rest of channel attention process is same as SCAB. In this way, $\mathbf{O}_{t, t+1}$ is encoded into the shadow removal features and control the expression of motion information.

\subsection{Loss Function}
\label{sec:loss}
There are three types of loss in our network: (1) regression loss ($\mathcal{L}_{reg}$) for over-exposure parameters $\{[\mathrm{w}_t^n, \mathrm{b}_t^n]\}_{n=1}^N$ in AEEM; (2) segmentation loss ($\mathcal{L}_{seg}$) for shadow detection mask $\mathbf{F}_{t}^{msk}$ in MSAM; (3) restoration loss ($\mathcal{L}_{res}$) for shadow removal predictions $\mathbf{R}_{t}^{middle}$ in MSAM and also $\mathbf{R}_{t}^{final}$ in the fusion module. 
%%
The total loss $\mathcal{L}_{total}$ can be written as follows:
\begin{equation}\label{Equ:TotalLoss}  
    \mathcal{L}_{total} = \alpha \mathcal{L}_{reg} + \beta \mathcal{L}_{seg} + \gamma \mathcal{L}_{res},
\end{equation}
where $[\alpha, \beta, \gamma]$ is the trade-off weight. In our experiments, we set them all to 1.

Regression loss $\mathcal{L}_{reg}$ in Equ~\ref{Equ:TotalLoss} can be further formulated as:
\begin{equation}\label{Equ:RegLoss}  
    \mathcal{L}_{reg} = \sum_{n=1}^{N}{\big{(}\mathrm{\Phi}_{char}(\mathbf{w}_{t}^{n}, \mathbf{\hat{w}}_{t}^{n}) + \mathrm{\Phi}_{char}(\mathbf{b}_{t}^{n}, \mathbf{\hat{b}}_{t}^{n})\big{)}},
\end{equation}
where $\mathrm{\Phi}_{char}$ is Charbonnier loss~\cite{charbonnier1994two} (similar as L1 loss and smoother than L1 loss around zero point); 
%%
$N$ represents the patch number for AEEM (in our experiments, we set $N$=$4$); $\mathbf{w}_{t}^{n}$ and $\mathbf{b}_{t}^{n}$ are $n$-th patch estimation of $[\mathrm{w}, \mathrm{b}]$ with AEEM for the $t$-th frame; $\mathbf{\hat{w}}_{t}^{n}$ and $\mathbf{\hat{b}}_{t}^{n}$ are the corresponding ground-truth. 
%%
The generation details of $\mathbf{\hat{w}}_{t}^{n}$ and $\mathbf{\hat{b}}_{t}^{n}$ are described in Section~\ref{sec:details}.

Segmentation loss $\mathcal{L}_{seg}$ in Equ~\ref{Equ:TotalLoss} can be formulated  as:
\begin{equation}\label{Equ:SegLoss}  
    \mathcal{L}_{seg} = \mathrm{\Phi}_{bce}(\mathbf{M}_{t}, \mathbf{\hat{M}}_t),
\end{equation}
where $\mathrm{\Phi}_{bce}$ is binary cross-entropy loss~\cite{rubinstein2004cross}; $\mathbf{M}_{t}$ represents shadow mask prediction for the $t$-th frame; $\mathbf{\hat{M}}_{t}$ is the corresponding ground-truth. 
% The generation details of $\mathbf{\hat{M}}_{t}$ are described in Section~\ref{sec:details}.

Restoration loss $\mathcal{L}_{res}$ in Equ~\ref{Equ:TotalLoss} can be further written as:
\begin{equation}\label{Equ:ResLoss}  
    \mathcal{L}_{res} = \mathrm{\Phi}_{char}(\mathbf{R}_{t}^{middle}, \mathbf{\hat{R}}_t) + \mathrm{\Phi}_{char}(\mathbf{R}_{t}^{final}, \mathbf{\hat{R}}_t),
\end{equation}
where $\mathrm{\Phi}_{char}$ is Charbonnier loss; $\mathbf{R}_{t}^{middle}$ and $\mathbf{R}_{t}^{final}$ denote coarse and refined shadow removal results for the $t$-th frame, respectively; $\mathbf{\hat{R}}_{t}$ is the corresponding ground-truth which denotes the shadow free image.

% \subsection{Real world video shadow removal by synthetic-to-real(S2R) strategy}
\subsection{Model adaptation in real world scenes}
\label{sec:S2R}
In the previous section, we introduce the PSTNet for video shadow removal. Due to the lack of available video shadow pairs in real world scene, in this paper, we build a synthetic dataset SVSRD-85 (details in Section~\ref{sec:dataset}) and train models with the synthetic video shadow pairs. However, there is large domain gap between synthetic scenes and real world scenes. When we trivially apply the models that trained in synthetic scenes to the real world scenes, the models tend to fail. In this section, we propose a lightweight synthetic-to-real strategy, termed S2R, to adapt the synthetic-driven models to the real world scenes without retraining.

Figure~\ref{fig:FDA} (lower) illustrate the process of S2R. First, each frame from a real world video is adapted to the synthetic domain by Fourier Domain Adaptation (FDA)~\cite{yang2020fda}. Second, a pretrained synthetic-driven video shadow removal model (e.g. PSTNet) is used to remove shadow in synthetic domain. Finally, the deshadow image in synthetic domain is adapted to real domain by FDA again, further produce the final shadow removal result. The total process can be formulized as:
\begin{equation}\label{Equ:S2R}
    \mathbf{R}_{rea} = \mathrm{FDA}(\mathrm{PSTNet}(\mathrm{FDA}(\mathbf{I}_{rea}, \mathbf{I}_{syn})), \mathbf{I}_{rea}),
\end{equation}
where $\mathbf{I}_{rea}$ and $\mathbf{I}_{syn}$ denote the input real image and synthetic image, respectively; $\mathbf{R}_{rea}$ denotes the shadow removed result of $\mathbf{I}_{rea}$; $\mathrm{FDA}(\mathtt{source}, \mathtt{target})$ denotes the FDA operation with inputs of source and target images. S2R requires only Fourier-based style transformation for the input video. With S2R, real world video can be produced trivially by synthetic-driven video shadow removal model without retraining.

\noindent
\textbf{Fourier Domain Adaptation (FDA).}
FDA~\cite{yang2020fda} is first introduced for unsupervised domain adaptation, whereby the discrepancy between the source and target distributions is reduced by swapping the low-frequency spectrum of one with the other. FDA does not require any training to perform the domain alignment, just a simple Fourier Transform and its inverse. Here, we employ FDA to reduce the domain gap between test data (from real world) and pretrained data (from synthetic scenes). Figure~\ref{fig:FDA} (upper) illustrate the process of FDA. FDA aims to adapt the source image to the target image style. First, the RGB-based source image and target image are transformed to amplitude and phase components via Fourier transform~\cite{frigo1998fftw}, respectively. Second, the low frequency part (controlled by a hyper parameter $\delta$) of the amplitude of the source image is replaced by the counterpart of the amplitude of the target image. Finally, inverse Fourier Transform~\cite{frigo1998fftw} reassemble the source phase and the new amplitude to obtain the `source image in target style'. The details of FDA can be found in \cite{yang2020fda}.

\noindent
\textbf{Choice of reference synthetic image}
Considering each frame in real world video, in S2R framework, we need to select a reference synthetic image for FDA transformation. In order to make the image transition natural, we tend to choose reference image with small gap in color space. In this paper, we use color histogram~\cite{novak1992anatomy} to filter the reference image. Specifically, we extract the first frame of each video in SVSRD-85 and compute the color histograms of them, termed $\{h_i\}_{i=1}^{s}$, where $i$ denotes the video index, $h_i$ denotes i-th color histogram and $s$ denotes number of videos in SVSRD-85. To a real world video to be processed, we also compute the color histogram of the first frame, termed $\tilde{h}$, and then computer the similarity scores between $\tilde{h}$ and $\{h_i\}_{i=1}^{s}$ with the following formulation: $score_i = |\tilde{h}-h_i|$. Finally, We choose the candidate image corresponding to the minimum value of similarity score as the final reference synthetic image as follows:
\begin{equation}\label{Equ:score}
    \hat{i} = \mathop{\mathrm{argmin}}\limits_{i\in\{1,...s\}}|\tilde{h}-h_i|,
\end{equation}
where $\hat{i}$ is the final selected reference synthetic image.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: FDA
\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{Figures/FDA.pdf}
\caption{Illustration of our proposed S2R strategy. In above figure, `PSTNet' denotes our proposed PSTNet model that pretrained in synthetic dataset (SVSRD-85). `FDA' denotes the Fourier Domain Adaptation operation~\cite{yang2020fda}.}
\label{fig:FDA}
\vspace{-2.5mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%