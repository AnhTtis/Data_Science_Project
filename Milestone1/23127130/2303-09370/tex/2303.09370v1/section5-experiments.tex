\section{Experiments}
\label{sec:experiments}
\subsection{Fundamental Settings}
\label{sec:details}
\noindent
\textbf{Evaluation measures.}
Following previous works~\cite{wang2018stacked,guo2012paired,qu2017deshadownet,le2021physics,fu2021auto}, we utilize the root mean square error (RMSE) in LAB color space between the predicted shadow removal result and the ground-truth image to evaluate different shadow removal methods. 
%We directly compare our PSTNet against several state-of-the-art methods on the SVSRD-85 test dataset in quantitative and qualitative ways.

\noindent
\textbf{Comparative Methods.}
%Since there is no existing method for video shadow removal, 
We make comparison against nine state-of-the-art methods for relevant tasks, including Guo \textit{et al.}~\cite{guo2012paired}, Gong \textit{et al.}~\cite{gong2014interactive}, DSC~\cite{hu2019direction}, SID~\cite{le2021physics}, DHAN~\cite{cun2020towards}, and Expo~\cite{fu2021auto} for single image shadow removal; MPRNet~\cite{zamir2021multi} for single image restoration; EDVR~\cite{wang2019edvr} for video restoration; and BasicVSR~\cite{chan2021basicvsr} for video super-resolution. 
We utilize their public codes, and re-train these methods on the SVSRD-85 training set to produce their best results for a fair comparison.

\noindent
\textbf{Implementation Details}
Our PSTNet is end-to-end trainable and requires no pre-training. 
The networks are trained on $256$$\times$$256$ patches on two NVIDIA GTX 2080Ti by using an Adam optimizer with a batch size of 6, 200 epochs, and an initial learning rate of $2$$\times$$10^{-4}$. 
The learning rate is then steadily decreased to $1$$\times$$10^{-6}$ using a cosine annealing strategy~\cite{loshchilov2016sgdr}. 
For data augmentation, horizontal and vertical flips are randomly applied. 
Note that our SVSRD-85 dataset provides the ground-truth of shadow-free image $\mathbf{\hat{R}_t}$ and shadow mask $\mathbf{\hat{M}_t}$ for each video frame.
Moreover, like SID~\cite{le2021physics}, we generate the ground-truth $\{[\mathrm{\hat{w}}_t^n, \mathrm{\hat{b}}_t^n]\}_{n=1}^N$ for AEEM, using a least squares method regression~\cite{chatterjee1986influential} via $\mathbf{\hat{I}_t}$, $\mathbf{\hat{R}_t}$ and $\mathbf{\hat{M}_t}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: visual SVSRD-85
\begin{figure*}[]
\centering
\includegraphics[width=1\textwidth]{Figures/SOTA.pdf}
\vskip -5pt
\caption{Qualitative comparison between our methods and other shadow removal methods on our SVSRD-85 dataset.}
\label{fig:visual_SVSRD-85}
\vspace{-2.5mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparison with the State-of-the-arts}
\input{Tables/Experiments_SOTA}

Table~\ref{table:state-of-the-art} lists RMSE scores of our PSTNet and compared methods at shadow pixels, non-shadow pixels, and all pixels of the whole video frames from our SVSRD-85 dataset.
%%and corresponding shadow-free videos 
And the first row shows the RMSE values of the input shadow videos without any shadow removal operation.
%%
% Can be deleted
% From these RMSE scores, we can find that the shadow removal results of our PSTNet have achieved smaller RMSE scores than state-of-the-art image shadow removal methods that of the input shadow video frame.  many get larger RMSE scores for non-shadow regions
%%
Among all the methods, our PSTNet obtains the smallest RMSE scores at shadow regions, non-shadow regions, and the whole image, which indicate that our network has better video shadow removal performance than the compared methods. 
%in both shadow and non-shadow regions, leading to the lowest RMSE in the whole image. 
%%
Specifically, compared against two encoder-decoder based image shadow removal methods (i.e., DSC~\cite{hu2019direction} and DHAN~\cite{cun2020towards}), our PSTNet outperforms DSC by 30.1\%/37.6\% RMSE in shadow/non-shadow region and outperforms DHAN by 14.7\%/37.3\% in shadow/non-shadow region. 
%%
Compared with physical-based methods SID~\cite{le2021physics} and Expo~\cite{fu2021auto}, PSTNet outperforms SID by 18.6\%/28.8\% in shadow/non-shadow region and outperforms Expo by 21.8\%/28.6\% in shadow/non-shadow region. 
PSTNet also outperforms the image restoration method MPRNet~\cite{zamir2021multi} by 22.9\%/20.0\%. 
In addition, compared with the video restoration method EDVR~\cite{wang2019edvr} and video super-resolution method BasicVSR~\cite{chan2021basicvsr}, PSTNet also outperforms EDVR by 22.6\%/34.7\% in shadow/non-shadow region and outperforms BasicVSR by 28.1\%/37.6\% in shadow/non-shadow region.




Figure~\ref{fig:visual_SVSRD-85} visually compares the shadow removal results produced by our network and other methods on the SVSRD-85 dataset. 
For the shadow on the deck of the train (first case), traditional methods (Guo \textit{et al.}~\cite{guo2012paired} and Gong \textit{et al.}~\cite{gong2014interactive}) can not distinguish this region as shadow, thereby generating the predictions that are similar as input. 
Most data-driven methods can remove the shadow of the central part to some extent, while they all tend to generate the ghost shadow over the original shadow boundary.
%%
In comparison, our PSTNet can yield reasonable exposure estimation and smooth removal result on the shadow boundary. 
%%
For the second case, the ground over the manhole cover is white, which makes the illustration of the shadow cast on it higher than the ground regions. 
Most image-based shadow removal methods cannot remove the persons' shadow due to being cheated by the high values of shadows. 
%%
However, the video-based methods like EDVR~\cite{wang2019edvr} and our PSTNet take advantage of the hints of temporal information between frames, which help to distinguish shadows from lit regions. Furthermore, the AEEM in PSTNet can help generate adaptive exposure estimation for different regions, thereby preventing from generating the discontiguous prediction between adjacent regions with different textures, like EDVR.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: visual TimeLapse (old)
% \begin{figure*}[]
% \centering
% \includegraphics[width=1\textwidth]{Figures/visual_compare_TimeLapse.pdf}
% \vskip -13pt
% \caption{Qualitative comparison between our method and other shadow removal methods on the SBU-TimeLapse dataset. Here, we compare the image-based methods (\textit{i.e.,} DHAN~\cite{cun2020towards} and SID~\cite{le2021physics}) and the video-based methods (\textit{i.e.,} BasicVSR~\cite{chan2021basicvsr} and our PSTNet). The penultimate column is the pesudo ground-truth images which remove the moving-shadows and retain the static-shadows. The last colume is the corresponding moving-shadow masks.}
% \label{fig:visual_TimeLapse}
% \vspace{-2.5mm}
% \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{TL_visual.tex}

\input{Tables/Experiments_ablation}

\subsection{Ablation Study}
\label{sec:ablation}
We perform ablation study experiments to verify the effectiveness of three shadow characteristics and some modules of our PSTNet. 

\vspace{2mm}
\noindent
\textbf{Effectiveness of three branches.}
Here, the first baseline ``Physical'' denotes that we only exploits the physical branch of our method to remove shadows of video frames. 
%The second ``Physical+Spatio'' is reconstructed by using physical branch, spatio branch, and feature fusion module to perform the shadow removal.
%%
The second ``Physical+Temporal'' utilizes the  physical branch, temporal branch, and feature fusion module to perform the shadow removal, 
while the third ``Physical+Spatio'' combines the physical branch, the spatio branch, and the feature fusion module to removal shadows. 


Table~\ref{table:ablation} summarizes the RMSE scores of our method and three reconstructed  baseline networks on the SVSRD-85 dataset. 
%%
From the quantitative results, we can find that
``Physical+Temporal'' has smaller RMSE scores that ``Physical'' at shadow pixels, non-shadow pixels, and all pixels of the whole video frames, which demonstrates that the temporal branch helps our method to remove shadows from video frames.
%%
Moreover, the smaller RMSE scores of ``Physical+Spatio'' over ``Physical'' indicates that considering the spatio branch in our method incurs a better video shadow removal performance. 
%%
More importantly, combining the three branches in our method has the best RMSE performance. 

%have the following observations: (i) Multi-characteristics baselines outperform the single characteristic. That shows these three properties are all relevant to the video shadow removal task. (ii) Physical characteristic is most important. 
%(iii) The tailored feature fusion module can greatly improve the results, compared with the single fusion.

%The fifth to the eighth baseline `Physical+Spatio+Temporal' denote considering all three characteristics to remove shadow. 
\vspace{2mm}
\noindent
\textbf{Effectiveness of AEEM.} \  
%%
We construct a baseline (``w/o AEEM'') by removing the adaptive exposure estimation network from our PSTNet, which means that the input shadow video frame is directly fed into the subsequent encoder for feature extraction. 
%%
Apparently, our method consistently has smaller RMSE scores at shadow regions, non-shadow regions, and the whole video frame than ``w/o AEEM'', which shows that the exposure estimation via AEEM helps our network to better removal shadow pixels from video frames. In addition, we also construct a baseline (``w/o AEEM-A'') by replacing the adaptive exposure estimation with a fixed exposure estimation, just like SID~\cite{le2021physics}. With this setting, the shadow performance reduces from 12.77 to 13.64. This shows that dynamic adjustment of parameter estimation is necessary in some complex cases.

\vspace{2mm}
\noindent
\textbf{Effectiveness of MSAM.} \ 
%%
We further construct a baseline (``w/o MSAM'') by removing mask-guided supervised attention module from PSTNet. Hence, ``w/o MSAM'' directly pass the decoder output of the physical branch to the subsequent fusion module without any mask supervision enhancement operation.
%%
According to Table~\ref{table:ablation}, we can find that removing MSAM from our PSTNet degrade its video shadow removal performance due to the superior RMSE results of our PSTNet over ``w/o MSAM''. In addition, when we just remove the shadow mask prediction branch in MSAM (``w/o MSAM-M''), shadow performance reduces from 12.77 to 13.67 and the non-shadow performance reduces from. 6.18 to 6.90. This indicates that joint learning of shadow detection and removal tasks helps to improve the performance of shadow removal.

\vspace{2mm}
\noindent
\textbf{Effectiveness of multi-characteristics fusion module.} \ 
%%
Lastly, a baseline (``w/o FM'') is reconstructed by replacing the multi-characteristics fusion module of PSTNet with a concatenation operation to assemble features form three branches. 
%The last `w/all' denotes our integrated PSTNet.
Apparently, our method outperforms ``w/o F.M.'' in terms of RMSE scores at shadow, non-shadow, and all pixels of the whole video frame.
It means that combining features from three branches via our multi-characteristics fusion module enables our method to reach a better video shadow removal result.

\subsection{Video shadow removal in real world scenes.}
In section~\ref{sec:S2R}, we introduce a lightweight model adaptation strategy `S2R' to adapt the well trained synthetic-driven video shadow removal model to real world scenes. We perform the generalisation experiments on the real world videos from SBU-Timelapse~\cite{le2021physics} to evaluate the effectiveness of S2R. In our experiments, we set the hyper parameters $\delta$ (mentioned in section~\ref{sec:S2R}) to 0.01. Since accurate shadow removal ground-truths in SBU-Timelapse are not available, we only perform visual comparisons on this dataset.

Figure~\ref{fig:comparison_TL} visually compares the video shadow removal results produced by our PSTNet (5-th column) and other methods (2-4 columns). The last column (6-th column) denotes the boosted PSTNet with S2R strategy. All models are trained with SVSRD-85 and the reference images in S2R are also from SVSRD-85. It can be found that S2R allows the model to adapt better to the real world scenes. For examples, in 3rd row, due to the overall bright color, the shadows of the trees are not recognized in vanilla PSTNet. However, in S2R boosted version, most of the shadows of the trees are reduced to the color of the background. In 4th row, vanilla PSTNet and other methods can only remove part of the shadows of tower, while S2R boosted version can remove almost all shadows.

% \ACMMM{
% \subsection{Generalisation Ability}
% % \input{Tables/Experiments_TimeLapse}

% Following SID~\cite{le2021physics}, we also conduct an experiment to evaluate the generalisation capability of the proposed PSTNet.  
% %SBU-TimeLapse dataset by comparing it against DHAN~\cite{cun2020towards}, SID~\cite{le2021physics}, and BasicVSR~\cite{chan2021basicvsr}.
% %%
% As mentioned in section~\ref{sec:dataset}, SBU-TimeLapse includes 50 videos taken by time-lapse photography. Each video contains a static scene without visible moving objects, and thus a ``max-min'' technique can be used to obtain the pseudo shadow-free frame and moving shadow mask. 
% %%
% Like SID~\cite{le2021physics}, we train our network and four state-of-the-art methods on SVSRD-85 dataset and test the trained models on the SBU-TimeLapse dataset for fair comparisons.
% %%
% The four compared methods include two image-based methods (DHAN~\cite{cun2020towards} and SID~\cite{le2021physics}), and one video-based method (BasicVSR\cite{chan2021basicvsr}).
% %and our PSTNet) are trained from SVSRD-85 dataset. %%
% %%
% %Then, We test the trained models in SBU-TimeLapse dataset and report their RMSE results in Table~\ref{table:timelapse}.
% %Apparently, our network has the smaller RMSE scores than all compared methods. 
% %It indicates that our method can better remove shadows of the SBU-TimeLapse dataset.

% Table~\ref{table:timelapse} reports the RMSE results on the moving shadow mask of our method and four state-of-the-art methods.
% %%
% Apparently, BasicVSR has the smallest RMSE result among the three compared methods, and the RMSE score is 25.17.
% %%
% Clearly, our method further outperforms BasicVSR in terms of the RMSE score, and its RMSE score is 18.93.
% It indicates that our network has a better generalization capability on the SBU-TimeLapse dataset than state-of-the-art methods.
% %The quantitative results are reported in Table~\ref{table:timelapse}. 
% %The data from SBU-TimeLapse and ISTD dataset are all come from real scenes, which should be more similar compared with the synthetic data in our SVSRD-85. 
% %However, our method still outperforms SID~\cite{le2021physics} by 5.8\% on the RMSE metric in provided moving shadow mask. 
% %It shows the strong generalisation ability of our PSTNet. 
% Moreover, Figure~\ref{fig:comparison_TL} visually compares the shadow removal results on the images from the SBU-TimeLapse dataset, and it shows our method can effectively remove the shadow pixels while other compared shadow removal methods tend to maintain many shadows in their results.

% %We also show visualisation comparison results, see Figure~\ref{fig:visual_SVSRD-85} for more details.
% }