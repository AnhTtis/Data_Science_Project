\section{SVSRD-85: Synthetic Video Shadow Removal Dataset}
\label{sec:dataset}
%采用人工合成场景的动机，以及具体的生成方式。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure: Dataset
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{Figures/Dataset.pdf}
\caption{Example frames of our  synthetic dataset (SVSRD-85), and video shadow removal results by our proposed PSTNet. }
\label{fig:dataset}
\vspace{-2.5mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Motivation.}
%To boost the video shadow removal task, we first consider the acquisition of real data.
%%
In real world, though we can easily get a shadowed video, getting the synchronous shadow-free video is quite difficult since it's hard to restore the same shooting states between two shots. 
%%
Le~\textit{et al.} collected SBU-Timelapse~\cite{le2021physics}, a video dataset of 50 videos with time-lapse photography, in which each video contains a static scene without visible moving objects.
%%
They used ``max-min'' technique to obtain a single pseudo shadow-free frame for each video.
%and then evaluated some single image shadow removal methods in this dataset. 
However, this dataset has two deficiencies. First, the obtained pseudo shadow-free image still contains much static shadows (as shown in Figure~\ref{fig:dataset}). 
%that appear on all frames. 
Second, it does not allow for any movement of targets and perspectives, which greatly limits the motion richness.
%of the collected scenes.
\input{Tables/SVSRD_vs_SBUTimeLapse}

\noindent
\textbf{Synthesize dataset from game scenes.}
In order to get the reliable video pairs to train, we consider rendering shadow and shadow-free video pairs in synthetic scenes.
%%
Thanks to the strong physical engine and the editing flexibility of the popular game: Grand Theft Auto V (GTAV), many works~\cite{richter2016playing,sidorov2019conditional} explored to drive various computer vision tasks based on this game. GTAV provides a range of APIs that can monitor the creation, modification, and deletion of resources used to specify the scene and synthesize images/videos. 
%%
What's more, it supports to control the switch of the shadow renderer, which can allow us to obtain the high-quality video shadow pairs.

In this paper, we collect a synthetic video shadow removal dataset (SVSRD-85) via the above manner. 
%%
Our dataset includes 85 videos with total 4250 frames, and each video contains a sequence of shadow images and corresponding shadow-free ground-truth. 
%%
We compute the pesudo shadow masks by operating Otsu’s algorithm to the difference between shadow and shadow-free images, similar to MaskShadow-GAN~\cite{hu2019mask}. %%
To provide guidelines for future works, we randomly split the dataset into training and testing sets with a ratio of 7:3, then obtain 59 training videos and 26 testing videos. 
%%
It is worth noting that since there are no restrictions on shooting conditions, SVSRD-85 includes rich scenes with more complex illustration and surface textures, compared with SBU-Timelapse. Table~\ref{table:SVSRD_vs_SBUTimeLapse} shows the main differences between SVSRD-85 and SBU-Timelapse. Some examples of video in SVSRD-85 can be found in Figure~\ref{fig:dataset}. In this paper, we also use some videos in SBU-Timelapse to verify the generalisation ability of our model in real scenes.