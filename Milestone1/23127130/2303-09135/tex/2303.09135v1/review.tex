training_args = TrainingArguments(
    "deleted_tweets_trainer",                  # output folder
    evaluation_strategy='steps',      # used evaluate during the fine-tuning of our model
    learning_rate=2e-5,
    #per_device_train_batch_size=16 #we have checked with the size 16, 32 but it didn't work so default size is 8 
    )  

For BERT fine-tuning, we used pretrained distilbert-base-uncased model from huggingface\footnote{https://huggingface.co/distilbert-base-uncased}, the model for the tweets was trained for four epochs with a learning rate of 2e-5, with batch size of 16 with a sequence length of 128. 

For BERT fine-tuning, we used Huggin Face's pre-trained DistilBert base (uncased) with a learning rate of 2e-5, batch size 16, and a sequence length of 128.


max features?

maxlen? => used a batch size of 16 with a sequence length of 128




For TF-IDF we used Sklearn\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html} with its default configuration. 

from sklearn.feature_extraction.text import TfidfTransformer
preprocessing = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer())
])
  


