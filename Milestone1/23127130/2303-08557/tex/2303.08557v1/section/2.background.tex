\section{Background} \label{background}
In this section, we first introduce the key concepts related to CDFSL in Section \ref{key}. And then, we provide formal definitions of the vanilla supervised learning, FSL, and CDFSL problems in Section \ref{definition} with concrete examples. To differentiate the CDFSL problem from relevant problems, we discuss their relatedness and differences in Section \ref{related}. In Section \ref{unique}, we discuss the special issue and challenges that make CDFSL difficult. Section \ref{taxonomy} presents a unified taxonomy according to how existing works handle the unique issue.
\subsection{\textcolor{black}{Key Concepts}} \label{key}
% To begin with, two key aspects, i.e., domain and task~\cite{tfsurvey,tf_2020}, may differ between the source problem and the target problem. Therefore, before giving our formal definition of CDFSL, we define two basic concepts  ``domain'' and ``task'' by following the excellent survey by Pan and Yang \cite{tfsurvey}.

Before giving our formal definition of CDFSL, we first define two key basic concepts of `domain' and `task' ~\cite{tfsurvey,tf_2020} as their specific contents may differ between the source and target problem, inspired by the excellent survey from Pan and Yang \cite{tfsurvey}.

\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:Domain}
\textbf{Domain.} Given a feature space $\mathcal{X}$ and a marginal probability distribution \textit{P(X)}, where $\textit{X}=\{x_{1}, x_{2}, ..., x_{n}\} \subseteq \mathcal{X}$, $n$ is the number of instances. A domain $\mathcal{D}=\{\mathcal{X}, \textit{P(X)}\}$ consists of $\mathcal{X}$ and $\textit{P(X)}$.
\end{MyDef}
}

% \textit{\textbf{Definition 2.1.1 Domain.}} \textit{Given a feature space $\mathcal{X}$ and a marginal probability distribution \textit{P(X)}, where $\textit{X}=\{x_{1}, x_{2}, ..., x_{n}\} \in \mathcal{X}$, $n$ is the number of instances. A domain $\mathcal{D}=\{\mathcal{X}, \textit{P(X)}\}$ consists of $\mathcal{X}$ and $\textit{P(X)}$.}

Specifically, for an image domain $\mathcal{D}$, the original images \textit{I} is mapped to a high-dimensional feature space $\mathcal{X}_{I}$. The features $\textit{X}_{I}$ in $\mathcal{X}_{I}$ is a higher-dimensional abstraction of \textit{I}, and the corresponding marginal probability distribution is $P(X_{I})$. The image domain $\mathcal{D}$ can be expressed as $\mathcal{D}=\{\mathcal{X}_{I}, P(X_{I})\}$. In general, difference in $\mathcal{X}_{I}$ or $P(X_{I})$ can lead to the different domain $\mathcal{D}$. 
%We classify cross-domain problems into two types depending on what is different in $\mathcal{X}_{I}$ or $P(X_{I})$: 1) heterogeneous cross-domain problem, and 2) homogeneous cross-domain problem. Given two domains $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, the heterogeneous cross-domain problem means that there is a domain shift between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ because of the different $\mathcal{X}_{I}$. Unlike the one mentioned above, the homogeneous cross-domain problem usually means the domain shift is caused by $P(X_{I})$. We mainly focus on the homogeneous cross-domain problem in this survey.

% \textit{给定一个特征空间$\mathcal{X}$和一个边缘概率分布\textit{P(X)}，其中$\textit{X}=\{x_{1}, x_{2}, ..., x_{n}\} \in \mathcal{X}$，$n$是实例的数量，域$\mathcal{D}=\{\mathcal{X}, \textit{P(X)}\}$由$\mathcal{X}$和$\textit{P(X)}$组成。}

% 具体地说，在一个图像域$\mathcal{D}$中，将原始图像\textit{I}映射到一个更高维的特征空间$\mathcal{X}_{I}$，特征空间中的特征$\textit{X}_{I}$是对原始图像更高维的抽象，其对应的边缘概率分布为$P(X_{I})$。图像域$\mathcal{D}$可以表示为$\mathcal{D}=\{\mathcal{X}_{I}, P(X_{I})\}$。因此，$\mathcal{X}_{I}$和$P(X_{I})$的不同均会导致域$\mathcal{D}$的不同。我们根据$\mathcal{X}_{I}$或$P(X_{I})$的不同将跨域问题分为两类：1）异质跨域问题，2）同质跨域问题。对于两个不同的域$\mathcal{D}^{s}$和$\mathcal{D}^{t}$，若$\mathcal{D}^{s}$和$\mathcal{D}^{t}$之间的域迁移是由于不同的$\mathcal{X}_{I}$引起的，则称$\mathcal{D}^{s}$与$\mathcal{D}^{t}$是异质跨域的，若其域迁移是由于$P(X_{I})$的差异导致的，则称两者是同质跨域的。如图\ref{dt}(a)所示，我们用三个坐标轴形成的三个二维空间来说明域的问题。三个二维空间分别为三个不同的特征空间$\mathcal{X}^{xy}$，$\mathcal{X}^{xz}$，以及$\mathcal{X}^{yz}$ （$xy, xz, yz$均表示坐标轴），$\{\mathcal{D}^{i}$|$i=\{1,2,...7\}\}$表示7个不同的域，其中$\{\mathcal{D}^{i}$|$i=\{1,2,3\}\}$均在$\mathcal{X}^{xy}$中，且他们的$P(X_{I})$不同，即$\{\mathcal{D}^{i}$|$i=\{1,2,3\}\}$之间的跨域为同质跨域问题。同理，$\{\mathcal{D}^{i}$|$i=\{4,5\}\}$和$\{\mathcal{D}^{i}$|$i=\{6,7\}\}$的跨域也为同质跨域问题。而当两个$\mathcal{D}$来自于不同$\mathcal{X}$时，为异质跨域问题，例如$\mathcal{D}^{1}$与$\mathcal{D}^{5}$之间的跨域为异质跨域问题。

% 本综述主要关注同质跨域问题。

% \textit{给定一个域$\mathcal{D}=\{\mathcal{X}, \textit{P(X)}\}$，任务$\mathcal{T}$由标签空间$\mathcal{Y}$和条件概率分布\textit{P(Y|X)}组成，即$\mathcal{T}=\{\mathcal{Y}, \textit{ P(Y|X)}\}$，其中$\textit{Y}=\{y_{ 1}, y_{2}, ..., y_{m}\} \in \mathcal{Y}$, $m$ 是标签的数量。}

 
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:Task}
\textbf{Task.} Given a domain $\mathcal{D}=\{\mathcal{X}, \textit{P(X)}\}$, a task $\mathcal{T}=\{\mathcal{Y}, \textit{P(Y|X)}\}$ consists of the label space $\mathcal{Y}$ and the conditional probability distribution \textit{P(Y|X)}, where $\textit{Y}=\{y_{1}, y_{2}, ..., y_{m}\} \in \mathcal{Y}$, $m$ is the number of labels.
\end{MyDef}
}

% \textit{\textbf{Definition 2.1.2 Task.}} \textit{Given a domain $\mathcal{D}=\{\mathcal{X}, \textit{P(X)}\}$, a task $\mathcal{T}=\{\mathcal{Y}, \textit{P(Y|X)}\}$ consists of a label space $\mathcal{Y}$ and the conditional probability distribution \textit{P(Y|X)}, where $\textit{Y}=\{y_{1}, y_{2}, ..., y_{m}\} \in \mathcal{Y}$, $m$ is the number of labels.}

% 比如，在一个分类任务$\mathcal{T}$中，所有的类别标签$\textit{Y}^{\mathcal{T}}=\{y^{\mathcal{T}}_{1}, y^{\mathcal{T}}_{2}, ..., y^{\mathcal{T}}_{m}\} \in \mathcal{Y}$均在标签空间$\mathcal{Y}$中，\textit{P(Y|X)}可以从训练数据$\textit{D}=\{x_{i}, y_{i}\}$中学习到，其中 $x_{i} \in \textit{X}$ 和 $y_{i} \in \textit{Y}$。\textit{P(Y|X)}的物理意义是一个预测函数\textit{f($\cdot$)}，用来为\textit{x}预测对应的标签\textit{y}。如图\ref{dt}(b)所示，比如，在给定的域$\mathcal{D}^{1}$中，一个分类任务$\mathcal{T}=\{\mathcal{Y}, \textit{P(Y|X)}\}$旨在学习一个函数$f(\cdot)$ (即\textit{P(Y|X)})来为$\mathcal{D}^{1}$中的所有样本预测标签。

Specifically, we use $x$ and $y$ to represent the input data and supervision terget. For example, for a classification task $\mathcal{T}$, all labels $\textit{Y}^{\mathcal{T}}=\{y^{\mathcal{T}}_{1}, y^{\mathcal{T}}_{2}, ..., y^{\mathcal{T}}_{m}\} \in \mathcal{Y}$ are in the label space $\mathcal{Y}$, and \textit{P(Y|X)} can be learned from the training data $\textit{D}$=\{$x_{i}, y_{i}$\}, where $x_{i} \in \textit{X}$ and $y_{i} \in \textit{Y}$. From a physical viewpoint, \textit{P(Y|X)} can be illustrated as a predict function $f(\cdot)$ that is used to predict the corresponding label \textit{y} for \textit{x}.
%As shown in Figure~\ref{dt}(b), for examplem, given a $\mathcal{D}^{1}$, the classification task $\mathcal{T}=\{\mathcal{Y}, \textit{P(Y|X)}\}$ aims to learn a function $f(\cdot)$ to predict the labels for all samples in $\mathcal{D}^{1}$.

\subsection{\textcolor{black}{Problem Definition}} \label{definition}
In this subsection, we first define the vanilla supervised learning. The definition of FSL is then illustrated before diving into the definition of CDFSL as we consider CDFSL a sub-area of FSL.

%we should recall the definition of FSL in the liter·ature due to CDFSL is a sub-area in FSL.

\iffalse
\begin{figure}%[b]
	\centering
	% \includegraphics[width=14cm]{cdfsl-domain-task-1.pdf}
	% \includegraphics[width=14cm]{cdfsl-domain-task-3.pdf}
	% \includegraphics[width=14cm]{cdfsl-domain-task-5.pdf}
	% \includegraphics[width=14cm]{cdfsl-domain-task-7.pdf}
	\includegraphics[width=14cm]{cdfsl-domain-task-9.pdf}
	\caption{\textcolor{black}{(a) shows the different domain, and (b) represents the different task in a domain. In (b), the different form means the different categories.}}
	\label{dt}
\end{figure}
\fi

\iffalse
\begin{table}
\centering
\caption{Domain situation}
\begin{tabular}{|c|c|c|c|}
\hline
 & \multicolumn{3}{c|}{$Data_{1}$ \quad \& \quad $Data_{2}$}  \\ 
 \hline
 $\mathcal{X}$ & same & same & different   \\  
 $\textit{P(X)}$ & same & different & different     \\
 \hline
 Domain & \textit{In domain} &  \textit{Homogeneous cross-domain} &  \textit{Heterogeneous cross-domain}    \\
 \hline
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Task situation}
\begin{tabular}{|c|c|c|c|}
\hline
 & \multicolumn{3}{c|}{$Data_{1}$ \quad \& \quad $Data_{2}$}  \\ 
 \hline
 $\mathcal{Y}$ & same & same & different   \\  
 $\textit{P(Y|X)}$ & same & different & different     \\
 \hline
 Task & \textit{same task} & \textit{Prior shift} & \textit{Cross-task}    \\
 \hline
\end{tabular}
\end{table}
\fi

% 任务的不同跟域有关。任务不同有两种情况：Y不同；P(Y|X)不同。Y不同的时候，P(Y|X)肯定不同。
% Y相同时，任务也有可能不同，即当X不同时，P(Y|X)不同。
% https://zhuanlan.zhihu.com/p/466721213


%\textit{\textbf{Definition 2.1.3 (Covariate shift).}}

%\textit{\textbf{Definition 2.1.4 (Prior shift).}}

%\textit{\textbf{Definition 2.2.1 Image Classification.}}
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:SupLearn}
\textbf{Vanilla Supervised Learning.} Given a domain $\mathcal{D}$, consider a supervised learning task $\mathcal{T}$, a training set %that from N different classes 
$\textit{D}^{train}$, and a test set $\textit{D}^{test}$ 
%that from the same N categories
, the goal of vanilla supervised learning is to learn a prediction function $f(\cdot)$ for $\mathcal{T}$ on $\textit{D}^{train}$, making $f(\cdot)$ has a good prediction effect on $\textit{D}^{test}$, where $\{\textit{D}^{train}, \textit{D}^{test}\} \subseteq \mathcal{D}$.
\end{MyDef}
}

% Given a domain $\mathcal{D}$, consider a supervised image classification task $\mathcal{T}$, a training set of images from N different classes $\textit{D}^{train}$, and a test set $\textit{D}^{test}$ that also from N categories, the goal of image classification is to learn a prediction function \textit{f}($\cdot$) for $\mathcal{T}$ on $\textit{D}^{train}$, which makes the function have a good prediction effect on $\textit{D}^{test}$.

For example, an image classification task is categorizing new images into a given class using a model learned from training samples. In classic image classification, training set $\textit{D}^{train}$ has enough images per class, like $\textit{ImageNet}$ with 1000 classes and over 1000 samples per class. Note that the data set $\textit{D}$ must not be confused with the domain $\mathcal{D}$. An illustration of a vanilla supervised classification problem is shown in Figure \ref{dtfc} (a).
\begin{figure}[b]
	\centering
	 \vspace{-0.2cm}
 	\includegraphics[width=13cm]{cdfsl-compare-27.pdf}
  \vspace{-0.5cm}
	\caption{\textcolor{black}{(a) the standard classification, (b) few-shot classification, (c) cross-domain few-shot classification. The different shapes mean the different categories. $\mathcal{D}$ means domain, $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ specifically represent the source and target domains, respectively. `?' indicates predict the test data.}}
 \vspace{-0.3cm}
	\label{dtfc}
\end{figure}

Like the goal of vanilla supervised learning, the goal of FSL is also to learn a model from the training set $\textit{D}^{train}$ for testing new samples. However, the key difference is that $\textit{D}^{train}$ of FSL only includes very little supervised information, making it a very challenging task. Due to the few samples in $\textit{D}^{train}$, many commonly used supervised algorithms fail to learn satisfying classification models, mainly caused by overfitting. Therefore, it is necessary and natural to introduce some prior knowledge into the FSL task to mitigate the overfitting issue. We call the task of acquiring prior knowledge the auxiliary task $\mathcal{T}^{s}$ (or source task). Usually, the categories of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$ have no intersection, \ie $\mathcal{Y}^{s} \cap \mathcal{Y}^{t}=\emptyset$, where $\mathcal{Y}^{s}$ and $\mathcal{Y}^{t}$ are the label sets of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$, respectively. A formal definition of FSL is given below.
% Compared with the vanilla supervised learning problem with many supervised training samples, FSL needs to achieve the goal based on limited supervision information, which is very challenging. 

% Like the goal of vanilla supervised learning, the goal of the few-shot learning problem (FSL) is to learn a model from the training set $\textit{D}^{train}$ for testing new samples, the difference is that $\textit{D}^{train}$ only include very little supervision information in FSL. Obviously, compared with the vanilla supervised learning problem with a large number of training samples, FSL needs to achieve the goal based on very limited supervision information, so it is a very challenging task. Due to the small number of samples in $\textit{D}^{train}$, many commonly used supervised algorithms usually fail if learning only based on $\textit{D}^{train}$. Therefore, it is necessary to introduce the prior knowledge into the target FSL process to avoid model overfitting. We call the task of acquiring prior knowledge as the source task $\mathcal{T}^{s}$ (or auxiliary task). Usually, the categories of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$ have no intersection, that is, $\mathcal{Y}^{s} \cap \mathcal{Y}^{t}=\emptyset$, where $\mathcal{Y}^{s}$ and $\mathcal{Y}^{t}$ are the label sets of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$, respectively. Formally, a definition of few-shot classification is given below.

% Like the goal of traditional image classification, the goal of the few-shot image classification problem (FSL) is to learn a classification model from the training set $\textit{D}^{train}$ for testing new samples, the difference is that $\textit{D}^{train}$ only include very little supervision information in FSL. We refer to the FSL task as the target task $\mathcal{T}^{t}$. Obviously, compared with the traditional image classification problem with a large number of training samples, FSL needs to achieve the goal of image classification based on very limited supervision information, so it is a very challenging task. Due to the small number of samples in $\textit{D}^{train}$, many commonly used supervised machine learning algorithms usually fail if learning only based on $\textit{D}^{train}$. Therefore, it is necessary to introduce the prior knowledge of tasks related to $\mathcal{T}^{t}$ into the target FSL process to avoid model overfitting. We call the task of acquiring prior knowledge as the source task $\mathcal{T}^{s}$ (or auxiliary task). Usually, the categories of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$ have no intersection, that is, $\mathcal{Y}^{s}$ ∩ $\mathcal{Y}^{t}$=∅, where $\mathcal{Y}^{s}$ and $\mathcal{Y}^{t}$ are the label sets of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$, respectively. Formally, a definition of few-shot classification is given below.

% \textit{给定一个域$\mathcal{D}$，考虑一个有监督图像分类任务$\mathcal{T}$，一个来自于N种不同类别的图像训练集$\textit{D}^{train}$，以及一个同样来自于N种类别的测试样本集$\textit{D}^{test}$，图像分类的目标是在$\textit{D}^{train}$上为$\mathcal{T}$学习一个预测函数\textit{f}($\cdot$)，使得该函数对$\textit{D}^{test}$有很好的预测效果。}

% 比如，图像分类是指将新的图像分类到给定的类别。一般是基于给定类别的训练样本，采用机器学习方法学习出分类模型，用以对新的图像进行类别预测。传统图像分类问题中，训练集$\textit{D}^{train}$每类图像样本比较多，例如著名的$\textit{ImageNet}$图像分类竞赛中，训练集有1000类，每一类有1000多个样本。图\ref{dtfc}(a)是标准的二分类问题。

% 和标准分类问题的目标一样，小样本分类问题的目标也是从训练集$\textit{D}^{train}$中学习分类模型用于测试新的样本，不同之处在于，小样本图像分类中$\textit{D}^{train}$给定的监督信息很少。我们将小样本图像分类任务称为目标任务$\mathcal{T}^{t}$。显然，与传统的有大量训练样本的图像分类问题相比较，小样本图像分类问题需要基于非常有限的监督信息实现图像分类的目标，因而这是一个极具挑战的任务。由于$\textit{D}^{train}$中样本数量较少，如果仅仅基于$\textit{D}^{train}$进行学习，很多常用的有监督机器学习算法面对小样本问题通常失效。因此，在解决小样本学习问题的时候，需要将与$\mathcal{T}^{t}$相关的任务的先验知识引入到目标小样本学习过程以避免模型的过拟合问题。我们把获取先验知识的任务称为源任务$\mathcal{T}^{s}$(或称为辅助任务)，通常，$\mathcal{T}^{s}$与$\mathcal{T}^{t}$的类别没有交集, 即 $\mathcal{Y}^{s}$ ∩ $\mathcal{Y}^{t}$=$\emptyset$，其中$\mathcal{Y}^{s}$和$\mathcal{Y}^{t}$分别为$\mathcal{T}^{s}$和$\mathcal{T}^{t}$的标签集。正式地，下面给出一个小样本分类的定义。
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:FSL}
\textbf{Few Shot Learning (FSL).} Given a domain $\mathcal{D}$, a task $\mathcal{T}^{t}$ described by a \textit{T}-specific data set $\textit{D}^{t}$ with only a few supervised information available, and a task $\mathcal{T}^{s}$ described by \textit{T}-irrelevant auxiliary data set $\textit{D}^{s}$ with sufficient supervised information, FSL aims to learn a function $f(\cdot)$ for $\mathcal{T}^{t}$ by utilizing the few supervised information in $\textit{D}^{t}$ and the prior knowledge in $(\mathcal{T}^{s}, \textit{D}^{s})$, where $\{\textit{D}^{t}, \textit{D}^{s}\} \subseteq \mathcal{D}$, and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.
\end{MyDef}
}

 
% \textit{\textbf{Definition 2.2.2 Few-shot learning (FSL).}}
% \textit{Given a domain $\mathcal{D}$, a task $\mathcal{T}^{t}$ described by a \textit{T}-specific dataset $\textit{D}^{t}$ with only a few supervised information available, and a task $\mathcal{T}^{s}$ described by \textit{T}-irrelevant auxiliary dataset $\textit{D}^{s}$ with sufficient supervised information, FSL aims to learn a function \textit{f}($\cdot$) for $\mathcal{T}^{t}$ by utilizing the few supervised information in $\textit{D}^{t}$ and the prior knowledge in $(\mathcal{T}^{s}, \textit{D}^{s})$, where $\textit{D}^{t}, \textit{D}^{s} \in \mathcal{D}$, and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.}

% $\teatit{P}^{a}(Y) \ne \textit{P}^{t}(Y)$.

%Given a domain $\mathcal{D}$, two learning task $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$. FSL aims to learn a function 

%Besides, we use $\mathcal{T}^{s}$ and $\mathcal{D}^{s}$ to represent the $\mathcal{T}^{t}$-irrelevant auxiliary task and the corresponding auxiliary dataset, respectively. $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are from the same domains. Different from $\mathcal{D}^{t}$, $\mathcal{D}^{s}$ includes the sufficient supervision information. FSL aims at learning a function for $\mathcal{T}^{t}$ by utilizing the prior knowledge learned from $(\mathcal{T}^{s}, \mathcal{D}^{s})$, and $\mathcal{D}^{t}$.

Specifically, take a few-shot classification task $\mathcal{T}^{t}$ as an example, we use the corresponding few-shot data pairs $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{t}}_{i=1}$ to represent the input data and supervision target. In addition, $\mathcal{T}^{s}$ and $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{s}}_{i=1}$ are utilized to indicate the conventional classification task and auxiliary data pairs, where $N^s \gg N^t$. $\mathcal{T}^{t}$ follows a ``\textit{C}-way \textit{K}-shot" training principle (\textit{C} indicates the number of classes, \textit{K} represents the sample numbers in each class). We learn a function $f$($\cdot$) for $\mathcal{T}^{t}$ from $\textit{D}^{t}$ and $(\mathcal{T}^{s}, \textit{D}^{s})$. Figure \ref{dtfc} (b) shows the few-shot classification (FSC) problem.
 
% Specifically, take a few-shot classification task $\mathcal{T}^{t}$ as an example, we use the corresponding few-shot data pairs $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{t}}_{i=1}$ to represent the input data and supervision target. In addition, $\mathcal{T}^{s}$ and $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{s}}_{i=1}$ are utilized to indicate the conventional classification task and auxiliary data pairs, where $N^s \gg N^t$. $\mathcal{T}^{t}$ follows a \textit{``C-way K-shot"} training principle. 
% , $\textit{D}^{t}$ includes images from different \textit{C} classes, each class includes \textit{K} images. $N_{t}+N_{s}$ samples are utilized as the training data, where $N_{t}=C \times K$. 
% We predict a classifier $f$($\cdot$) for $\mathcal{T}^{t}$ from $\textit{D}^{t}$ and $(\mathcal{T}^{s}, \textit{D}^{s})$.
% $\textit{D}^{s}$ and $\mathcal{T}^{s}$. 
% Figure \ref{dtfc} (b) shows the few-shot classification (FSC) problem.

As a branch of FSL, CDFSL also predicts the new samples with the model that is learned by $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{t}}_{i=1}$ and the prior knowledge from $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{s}}_{i=1}$. The difference is that $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{s}}_{i=1}$ and $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{t}}_{i=1}$ in CDFSL come from two different domains $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, \ie, $\mathcal{D}^{s} \ne \mathcal {D}^{t}$. Compared to the FSL problem that the data are independent identically distribution (i.i.d.), CDFSL breaks this constraint.
Therefore, CDFSL not only inherits the challenges of FSL but also contains its unique cross-domain challenges, making it a more challenging problem. Consequently, numerous conventional FSL algorithms are no longer applicable to CDFSL, which necessitates the development of a viable approach to transfer prior knowledge from the source domain $\mathcal{D}^{s}$ to the target domain $\mathcal{D}^{t}$ without overfitting the model on $\mathcal{D}^{s}$. A definition of CDFSL is formally given below.

% As a branch of FSL, CDFSL also predicts the new samples with the model that is learned by $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{t}}_{i=1}$ and the prior knowledge from $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{s}}_{i=1}$. The difference is that $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{s}}_{i=1}$ and $\{(\textit{x}_{i}, \textit{y}_{i})\}^{N^{t}}_{i=1}$ in CDFSL comes from two different domains $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, \ie, $\mathcal{D}^{s} \ne \mathcal {D}^{t}$. Obviously, compared to the FSL problem that the data are i.i.d., CDFSL breaks this constraint.
% needs to achieve the performance on FSL task $\mathcal{T}^{t}$ with $(\mathcal{T}^{s}, \textit{D}^{s})$ and $\textit{D}^{t}$, in which $\textit{D}^{s}$ and $\textit{D}^{t}$ are from different domains. 
% Therefore, CDFSL not only inherits the challenges of FSL but also contains its unique cross-domain challenges, making it a more challenging problem. As a result, many existing FSL algorithms are no longger suitable for CDFSL and this asks us to find a reasonable way of transfering prior knowledge to $\mathcal{T}^{t}$ to avoid model overfitting on $\mathcal{T}^{s}$. A definition of CDFSL is formally given below.

% As a branch of FSL, the cross-domain FSL problem (CDFSL) also predict the new samples with the model that is learned by using $\textit{D}^{t}$ and the prior knowledge from $\mathcal{T}^{s}$. The difference is that $\textit{D}^{s}$ and $\textit{D}^{t}$ in CDFSL comes from two different domains $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, ie $\mathcal{D}^{s} \ne \mathcal {D}^{t}$. Obviously, compared with the FSL problem that satisfies the i.i.d., CDFSL problem needs to achieve the performance on FSL task $\mathcal{T}^{t}$ with $\mathcal{T}^{s}$, $\textit{D}^{s}$ and $\textit{D}^{t}$, in which $\textit{D}^{s}$ and $\textit{D}^{t}$ are from different domains. It means CDFSL not only inherits the challenges of FSL, but also contains its own unique cross-domain challenges, so it is a more challenging problem. Since the cross-domain challenges, many FSL algorithms are usually not ideal for CDFSL problems. Therefore, when solving the CDFSL problem, it is necessary to find reasonable transfer way of prior knowledge for $\mathcal{T}^{t}$ to avoid the model overfitting on $\mathcal{T}^{s}$. A definition of CDFSL is formally given below.

%and $\mathcal{D}^{t}$ consists of two datasets $\mathcal{D}_{trn}=(\textit{x}_{i}, \textit{y}_{i})^{M_{trn}}_{i=1}$ and $\mathcal{D}_{tst}=(\textit{x}_{i})^{M_{tst}}_{i=1}$. $\mathcal{D}_{trn}$ include images from different \textit{C} classes, each class includes \textit{K} images. $M_{trn}=N \times K$ samples are utilized as the training data.

%The goal of $\mathcal{T}^{t}$ is to built a mapping function $f(\cdot)$ to predict on $\mathcal{D}_{tst}$. According to the definition of FSL, there are a auxiliary task $\mathcal{T}^{s}$ and the corresponding dataset $\mathcal{D}^{s}=(\textit{x}^{s}_{i}, \textit{y}^{s}_{i})^{M_{arc}}_{i=1}$, and both of the classes and instances of each class are much larger than \textit{C} and \textit{K}.

% \textit{给定一个域 $\mathcal{D}$，一个由只有少量监督信息的数据集 $\textit{D}^{t}$ 描述的分类任务 $\mathcal{T}^{t}$，和一个由包含大量有监督信息的数据集$\textit{D}^{s}$描述的分类任务$\mathcal{T}^{s}$，FSL 旨在通过利用 $\textit{D}^{t}$ 中的少数监督信息和 $(\mathcal{T}^{s}, \textit{D}^{s})$为$\mathcal{T}^{t}$学习一个函数\textit{f}($\cdot$), 其中 $\textit{D}^{t}, \textit{D}^{s} \in \mathcal{D}$，$\mathcal{T}^{s} \ne \mathcal{T}^{t}$且$\mathcal{Y}^{s}$ ∩ $\mathcal{Y}^{t}$=$\emptyset$。}

% 具体来说，我们使用 $\textit{x}$和$\textit{y}$来表示输入数据和标签。对于一个小样本分类任务$\mathcal{T}^{t}$，其对应的小样本数据集为$\textit{D}^{t}=(\textit{x}_{i},\textit{y}_{i})^{N_{t}}_{i=1}$，常规分类任务$\mathcal{T}^{s}$和对应的辅助数据集为$\textit{D}^{s}=(\textit{x}_{i}, \textit{y}_{i})^{N_{s}}_{i=1}$，其中 $N_s \gg N_t$。$\mathcal{T}^{t}$遵循\textit{"C-way K-shot"}训练原则，$\textit{D}^{t}$包含来自不同的\textit{C}个类别的图像，每个类别都包含\textit{K}个图像。$N_{t}+N_{s}$个样本作为训练数据，其中 $N_{t}=C\times K$。我们从$\textit{D}^{t}$, $\mathcal{T}^{s}$和$\textit{D}^{s}$中为$\mathcal{T}^{t}$学习一个预测函数，即分类器\textit{f}($\cdot$)。图\ref{dtfc} (b)展示了FSL问题。

% 作为小样本分类的一个分支，跨域小样本分类问题也是从($\mathcal{T}^{s}$,$\textit{D}^{s}$)以及只包含少量监督信息的$\textit{D}^{t}$中学习分类模型用来测试新的样本，不同之处在于，跨域小样本图像分类中$\textit{D}^{s}$与$\textit{D}^{t}$来自不同的域$\mathcal{D}^{s}$与$\mathcal{D}^{t}$，即$\mathcal{D}^{s} \ne \mathcal{D}^{t}$。显然，与满足独立同分布的小样本图像分类问题相比较，跨域小样本图像分类问题需要基于来自于与$\mathcal{T}^{t}$不同域的$\mathcal{T}^{s}$和$\textit{D}^{s}$提供的先验知识以及包含非常有限的监督信息的$\textit{D}^{t}$实现图像分类的目标。也就是说，跨域小样本分类在小样本分类的挑战之上增加跨域的设定，因而这是一个更加有挑战性的问题。由于$\mathcal{D}^{s} \ne \mathcal{D}^{t}$，如果直接将$\mathcal{D}^{s}$中学习的先验知识迁移到$\mathcal{D}^{t}$，很多小样本分类算法面对跨域问题通常不理想。因此，在解决跨域小样本学习问题的时候，需要寻找合理的$\mathcal{T}^{s}$的先验知识从而避免模型在$\mathcal{T}^{s}$上的过拟合问题。下面正式地给出跨域小样本分类的定义。

\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:CDFSL}
\textbf{Cross-Domain Few-Shot learning (CDFSL).} Considering a source domain $\mathcal{D}^{s}$ with sufficient supervised information and learning task $\mathcal{T}^{s}$, a target domain $\mathcal{D}^{t}$ with limited supervised information and FSL task $\mathcal{T}^{t}$, the goal of CDFSL is to learn a target perdictive function $f_T(\cdot)$ on $\mathcal{D}^{t}$ with the help of the prior knowledge in $(\mathcal{T}^{s}, \mathcal{D}^{s})$, where $\mathcal{D}^{s} \ne \mathcal{D}^{t}$, and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.
\end{MyDef}
}



% \textit{\textbf{Definition 2.2.3 Cross-domain Few-shot learning (CDFSL).}}
% \textit{Considering a source domain $\mathcal{D}^{s}$ with sufficient supervised information and learning task $\mathcal{T}^{s}$, a target domain $\mathcal{D}^{t}$ with limited supervised information and few-shot learning task $\mathcal{T}^{t}$, the goal of CDFSL is helping improve the learning of the target perdictive function $\textit{f}_{T}$($\cdot$) in $\mathcal{D}^{t}$ using the prior knowledge in $(\mathcal{T}^{s}, \mathcal{D}^{s})$, where $\mathcal{D}^{s} \ne \mathcal{D}^{t}$, and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.}


% As shown in Figure \ref{dtfc} (c), 
In a cross-domain few-shot classification (CDFSC) problem, as shown in Figure \ref{dtfc} (c), we similarly denote a source and a target classification task by $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$, respectively. They are described by the data pairs $\{(\boldsymbol{x}_i^s,y^s_i)\}_{i=1}^{N^s} \subseteq \mathcal{D}^s$ and $\{(\boldsymbol{x}_i^t,y^t_i)\}_{i=1}^{N^t} \subseteq \mathcal{D}^{t}$, where $N^s\gg N^t$, $y^s_i\in\mathcal{Y}^s$, $y^t_i\in\mathcal{Y}^t$, $\mathcal{Y}^t\bigcap\mathcal{Y}^s=\varnothing$ (\ie, the source and target domains do not share the label space). Note that $\mathcal{D}^t$ and $\mathcal{D}^s$ are sampled from two different probability distributions $p$ and $q$, respectively, where $p \ne q$. The objective of the CDFSC is learning a classifier $f_{T}$($\cdot$) for $\mathcal{T}^{t}$ using $\mathcal{D}^{t}$ and $(\mathcal{T}^{s}, \mathcal{D}^{s})$. It addresses the issue that there are no sufficient auxiliary samples in the target domain $\mathcal{D}^t$ to provide the proper prior knowledge for $\mathcal{T}^{t}$.

Furthermore, CDFSL can be classified into three broad categories based on why the image distribution differs: Fine-grain based CDFSL (FG), Art-based CDFSL (Art), and Imaging way-based CDFSL (IW). FG-CDFSL pertains to differences in the fine-grained categories between $\mathcal{D}^s$ and $\mathcal{D}^t$. Specifically, the categories of $\mathcal{D}^t$ are the fine-grained classes of a specific variety in $\mathcal{D}^s$. A-CDFSL involves differences in artistic expression, such as sketches, natural images, stick figures, oil paintings, and watercolors. And in IW-CDFSL, dissimilarities in imaging modes between $\mathcal{D}^s$ and $\mathcal{D}^t$ arise when the datasets comprise images of distinct modalities, for instance, natural images in $\mathcal{D}^s$ and medical $X$-ray images in $\mathcal{D}^t$. IW-CDFSL is generally considered the most challenging of the three categories.

% For example, in a cross-domain few-shot classification problem, as shown in Figure \ref{dtfc} (c), given a source classification task $\mathcal{T}^{s}$ that described by the dataset pairs $\{(\boldsymbol{x}_i^s,y^s_i)\}_{i=1}^{n^s}$ from source domain $\mathcal{D}^s$, and a target few-shot classification task $\mathcal{T}^{t}$ described by the dataset pairs $\{(\boldsymbol{x}_i^t,y^t_i)\}_{i=1}^{n^t}$ from target domain $\mathcal{D}^{t}$, where $n^s\gg n^t$, $y^s_i\in\mathcal{Y}^s$, $y^t_i\in\mathcal{Y}^t$, $\mathcal{Y}^t\bigcap\mathcal{Y}^s=\varnothing$ (The source and target do not share the label space). Note that $\mathcal{D}^t$ and $\mathcal{D}^s$ are sampled from two different probability distributions $p$ and $q$ respectively, $p \ne q$. The objective of the cross-domain few-shot classification problem is learning a classifier $\textit{f}_{T}$($\cdot$) for $\mathcal{T}^{t}$ using $\mathcal{D}^{t}$ and $(\mathcal{T}^{s}, \mathcal{D}^{s})$. This addresses the issue that there is no sufficient auxiliary samples in the target domain $\mathcal{D}^t$ to provide the prior knowledge for the few-shot classification task $\mathcal{T}^{t}$. In addition, in the cross-domain few-shot image classification problem, there are two cross-domain types according to different image distribution ways, as shown in Figure \ref{type}: (1) Fine-grain based cross-domain ($FG$), (2) Image imaging way based cross-domain ($IW$). $FG$ indicates that the distribution of the $\mathcal{D}^s$ and $\mathcal{D}^t$ are different in fine-grained granularity, and the categories of $\mathcal{D}^t$ is the fine-grained classes of a certain category in $\mathcal{D}^s$, while $IW$ indicates that images in $\mathcal{D}^s$ and $\mathcal{D}^t$ have different imaging modes ( Such as natural images in $\mathcal{D}^t$ and medical $X$-ray images in $\mathcal{D}^t$), it is more difficult and challenging than the previous one.

% the distribution of the source and target domain are different in fine-grained granularity, and the categories of target domain is the fine-grained classes of a certain category in the source domain

%Given a source classification task $\mathcal{T}^{s}$ that defined by the dataset pairs $\{(\boldsymbol{x}_i^s,y^s_i)\}_{i=1}^{n^s}$ from source domain $\mathcal{D}^s$, and a target classification task $\mathcal{T}^{t}$ defined by a training dataset with only a few labeled samples $\mathcal{D}^t=\{(\textbf{\emph{x}}_i^t,y^t_i)\}_{i=1}^{n^t}$ that irrelevant of $\mathcal{D}^{s}$, where $n^s\gg n^t$, $y^s_i\in\mathcal{Y}^s$, $y^t_i\in\mathcal{Y}^t$, $\mathcal{Y}^t\bigcap\mathcal{Y}^s=\varnothing$ (The source and target do not share the label space). Note that $\mathcal{D}^t$ and $\mathcal{D}^s$ are sampled from two different probability distributions $p$ and $q$ respectively, $p \ne q$. In $\mathcal{D}^t$, following the commonly used \textit{"C-way K-shot"} setting, there are $C$ classes with each class having only $K$ samples. That is $n^t=CK$. The training dataset $\mathcal{D}^t$ is also called the support set for the target domain. The objective of the cross-domain few-shot classification problem aims at solving the task $\mathcal{T}^{t}$ using $\mathcal{D}^{t}$ that has the limited supervision information, with the help of the prior knowledge learned from $(\mathcal{T}^{s}, \mathcal{D}^{s})$. This addresses the issue when there is no auxiliary samples from the same domain as the target task available.
\iffalse
\begin{figure}%[b]
	\centering
	\includegraphics[width=13cm]{cdfsl-compare-25.pdf}
	\caption{\textcolor{black}{Take classification as an example. The figure shows the difference of (a) the standard classification problem, (b) few-shot classification problem, (c) cross-domain few-shot classification problem. In the figure, the different form means the different categories. $\mathcal{D}$ means domain.}}
	\label{dtfc}
\end{figure}
\fi
%\textit{考虑一个具有足够监督信息的源域$\mathcal{D}^{s}$和一个分类任务 $\mathcal{T}^{s}$，一个具有有限监督信息的目标域 $\mathcal{D}^{t}$和小样本分类任务$\mathcal{T}^{t}$，CDFSL的目标是使用$\mathcal{D}^{t}$以及$(\mathcal{T}^{s}, \mathcal{D}^{s})$中获得的先验知识来改进目标预测函数$\textit{f}_{T}$($\cdot$)的学习，其中 $\mathcal{D}^{s} \ne \mathcal{D}^{t}$且$\mathcal{T}^{s} \ne \mathcal{T}^{t}$。}

%例如，在跨域少样本分类问题中，如图\ref{dtfc} (c)所示，给定一个由来自源域$\mathcal{D}^s$的数据集对$\{(\boldsymbol{x}_i^s,y^s_i)\}_{i=1}^{n^s}$描述的源分类任务$\mathcal{T}^{s}$，以及一个由来自目标域$\mathcal{D}^{t}$的数据集对$\{(\boldsymbol{x}_i^t,y^t_i)\}_{i=1}^{n^t}$描述的目标小样本任务$\mathcal{T}^{t}$，其中$n^s\gg n^t$，$y^s_i\in\mathcal{Y}^s$，$y^t_i\in\mathcal{Y}^t$，$\mathcal{Y}^t\bigcap\mathcal{Y}^s=\varnothing$(源域和目标域不共享标签空间)。注意$\mathcal{D}^t$和$\mathcal{D}^s$分别来自于两个不同的概率分布$p$和$q$, $p \ne q$。跨域小样本分类问题的目标是利用$\mathcal{D}^{t}$和$(\mathcal{T}^{s}, \mathcal{D}^{s})$为$\mathcal{T}^{t}$学习一个分类器$\textit{f}_{T}$($\cdot$)。这解决了目标域$\mathcal{D}^t$ 中没有足够的辅助样本来为小样本分类任务$\mathcal{T}^{t}$提供先验知识的问题。

%此外在跨域小样本图像分类问题由于跨域的方式不同被分为两类，如图\ref{type}所示：（1）基于细粒度的跨域($FG$)，（2）基于图像成像方式的跨域($IW$)。$FG$指$\mathcal{D}^{t}$的所有类别均为$\mathcal{D}^{s}$中某个类别的子类，而$IW$表示$\mathcal{D}^{t}$中的图像与$\mathcal{D}^{s}$中的图像存在不同的成像方式（如$\mathcal{D}^{s}$中是自然图像，$\mathcal{D}^{t}$中是医学$X$光图像），这种跨域方式比前一种更难且更有挑战性。

%此外，在跨域小样本图像分类问题中，根据前面域的定义中的讨论，CDFSL问题根据$\mathcal{X}$或$P(X)$的不同被分为两种：(1) 同质跨域小样本分类(Homo-CDFSL)问题，(2) 异质跨域小样本分类(Heter-CDFSL)问题。其中，Heter-CDFSL中源域与目标域来自不同的特征空间，而Homo-CDFSL中源域和目标域来自同一特征空间但不同的分布。毫无疑问，与Homo-CDFSL相比，Heter-CDFSL更加困难且更具挑战性。此外，根据特征空间的散度，Heter-CDFSL可以进一步分为两种场景。在一种情况下，源域和目标域都包含图像，特征空间的分歧主要是不同的感官设备（例如，可见光（VIS）与近红外（NIR）或RGB与深度）和不同的图像的风格（例如，草图与照片）。在另一种情况下，源域和目标域中存在不同媒体类型（例如，文本与图像以及语言与图像）。显然，第二种场景的跨域差距要大得多。

% 如图\ref{type}所示：（1）基于细粒度的跨域($FG$)，（2）基于图像成像方式的跨域($IW$)。$FG$指$\mathcal{D}^{t}$的所有类别均为$\mathcal{D}^{s}$中某个类别的子类，而$IW$表示$\mathcal{D}^{t}$中的图像与$\mathcal{D}^{s}$中的图像存在不同的成像方式（如$\mathcal{D}^{s}$中是自然图像，$\mathcal{D}^{t}$中是医学$X$光图像），这种跨域方式比前一种更难且更有挑战性。

\subsection{\textcolor{black}{Closely Related Problems}} \label{related}
In this section, we discuss the closely relevant problems. The difference and relatedness between these problems and CDFSL are illustrated in Figure~\ref{rela}.
%The difference and relatedness between these problems and CDFSL are illustrated in Figure~\ref{rela}. The left part shows the different related problems, and the right part offers the setting of FSL and CDFSL, respectively.
% We only discuss the homogeneous domain adaptation problem in the following related problems.

\begin{wrapfigure}{r}{200pt} 
\vspace{-0.3cm} 
\centering
\includegraphics[width=0.5\textwidth]{related-1.pdf}
\vspace{-0.8cm}
\caption{CDFSL related problems. The circles representing target data and its size indicating amount.}
\vspace{-0.3cm}
\label{rela}
\end{wrapfigure}

\iffalse
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:semi-da}
\textit{Semi-supervised Domain Adaptation (Semi-DA).} Given a source domain $\mathcal{D}^{s}$ with sufficient supervised information, a target domain $\mathcal{D}^{t}$ that includes a few supervised information and a large amount of unsupervised information, and a learning task on the traget domain $\mathcal{T}$, Semi-DA aims at learning a function $f(\cdot)$ for $\mathcal{T}$ by utilizing $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, where $\mathcal{D}^{s} \ne \mathcal{D}^{t}$.
\end{MyDef}
}
\fi

\textit{Semi-supervised Domain Adaptation (Semi-DA).} Semi-DA utilizes a large amount of supervised data in $\mathcal{D}^{s}$, a few labeled data and a large amount of unlabeled data in $\mathcal{D}^{t}$ to improve the performance of $\mathcal{T}$. There are the same label space and different but related sample distributions between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, \ie, $\mathcal{D}^{s} \ne \mathcal{D}^{t}$. Similar to Semi-DA, the CDFSL problem also uses a large amount of supervised data in $\mathcal{D}^{s}$ and limited supervised data in $\mathcal{D}^{t}$ to improve the performance of the task $\mathcal{T}$, $\mathcal{D}^{s} \ne \mathcal{D}^{t}$. The difference is that CDFSL does not use many unsupervised samples in the target domain to help with training. Besides, the label space of $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different in the CDFSL problem.
% Semi-DA utilizes a large amount of supervised data $\textit{D}^{sl}$ in $\mathcal{D}^{s}$ and a large amount of labeled and unlabeled data ($\textit{D}^{tl}$ , $\textit{D}^{tu}$) in $\mathcal{D}^{t}$ to improve the performance of $\mathcal{T}$ on $\mathcal{D}^{t}$. It includes same label space and different but related distributions between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$,
% The distributions of $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different but related, and the learning tasks (label space) are the same, 
% i.e., $\mathcal{D}^{s} \ne \mathcal{D}^{t}$. Similar to Semi-DA, the CDFSL problem also uses a large amount of supervised data $\textit{D}^{sl}$ in $\mathcal{D}^{s}$ to improve the performance of the task $\mathcal{T}$. The difference is that CDFSL only has limited supervised data $\textit{D}^{tl}$ in $\mathcal{D}^{t}$. Besides, the tasks on $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different in the CDFSL problem.
% , i.e., $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.

%\textit{给定一个包含大量有监督信息的源域$\mathcal{D}^{s}$, 一个同时包含有监督和无监督信息的目标域$\mathcal{D}^{t}$, 以及一个可学习的任务$\mathcal{T}$, Semi-DA旨在利用$\mathcal{D}^{s}$和$\mathcal{D}^{t}$为$\mathcal{T}$学习一个函数$\textit{f}$($\cdot$)，其中$\mathcal{D}^{s} \ne \mathcal{D}^{t}$.}

%Semi-DA通过利用源域$\mathcal{D}^{s}$中大量的有监督数据$\textit{D}^{sl}$以及目标域$\mathcal{D}^{t}$中同时包含大量有标签和无标签的数据($\textit{D}^{tl}$, $\textit{D}^{tu}$)，改进目标域上任务$\mathcal{T}$的性能。源域和目标域的分布不同但相关，且学习任务相同，即$\mathcal{D}^{s} \ne \mathcal{D}^{t}$。与Semi-DA相似，CDFSL问题同样利用源域中大量的有监督数据$\textit{D}^{sl}$改进目标域上任务$\mathcal{T}$的性能，不同的是，CDFSL问题中目标域只有少量有监督数据$\textit{D}^{tl}$，且在CDFSL问题中源域和目标域上的任务不同，比如$\mathcal{T}^{s} \ne \mathcal{T}^{t}$。
%  $\textit{P}^{s}(X) \ne \textit{P}^{t}(X)$.
\iffalse
\begin{table}
\tiny
\centering
\caption{Related problem. Similarities and differences between CDFSL and the related problems.}
\setlength{\tabcolsep}{3.8mm}{
\begin{tabular}{cccccccc|}
\hline
Problem  & Source data $\mathcal{D}^{s}$ & Target data $\mathcal{D}^{t}$ & Unseen target? & $\mathcal{D}^{s} \ne \mathcal{D}^{t}$ & $\mathcal{T}^{s} \ne \mathcal{T}^{t}$ \\ 
 \hline
 Supervised learning & Sufficient labeled & Sufficient unlabeled & $\times$ & $\times$ & $\times$ \\  % coviriate shift 
\hline
Semi-DA & Sufficient labeled & Limit labeled \& sufficient unlabeled & $\times$ & $\checkmark$ & $\times$ \\  % coviriate shift 
\hline
UDA      & Sufficient labeled & Sufficient unlabeled & $\times$ & $\checkmark$ & $\times$ \\
\hline
DG      & Sufficient labeled & Sufficient unlabeled & $\checkmark$ & $\checkmark$ & $\times$ \\
\hline
% MTL     & & $No$  & $Yes$ & large labeled & large labeled & Prior   \\
FSL      & Sufficient labeled & Limit labeled & $\checkmark$ & $\times$ & $\checkmark$ \\
\hline
DAFSL    & Sufficient labeled & Limit labeled & $\checkmark$ & $\times$ & $\times$ \\
\hline
CDFSL    & Sufficient labeled & Limit labeled & $\checkmark$ & $\checkmark$ & $\checkmark$ \\

 \hline
\end{tabular}}
\label{rela}
\end{table}
\fi

\iffalse
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:uda}
\textbf{Unsupervised domain adaptation (UDA).} Considering a source domain $\mathcal{D}^{s}$ with sufficient supervised information, a target domain $\mathcal{D}^{t}$ with only unsupervised information, and a learning task on the target domain $\mathcal{T}$, the goal of UDA is learning the function $f(\cdot)$ for $\mathcal{T}$ under the help of both $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$.
\end{MyDef}
}
\fi

\textit{Unsupervised Domain Adaptation (UDA).} UDA utilizes a large amount of supervised data in $\mathcal{D}^{s}$ and a large amount of unlabeled data in $\mathcal{D}^{t}$ to improve the performance of $\mathcal{T}$. The distributions between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different but related, \ie, $\mathcal{D}^{s} \ne \mathcal{D}^{t}$. And they share the same learning tasks. Similar to UDA, CDFSL also uses a large amount of supervised data in $\mathcal{D}^{s}$ to improve the performance of $\mathcal{T}$ in $\mathcal{D}^{t}$, $\mathcal{D}^{s} \ne \mathcal{D}^{t}$. However, $\mathcal{D}^{t}$ in CDFSL has only a few amounts of supervised data, and the tasks of $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different.

%\textit{考虑一个包含大量有监督信息的源域$\mathcal{D}^{s}$，一个只有无监督信息的目标域$\mathcal{D}^{t}$，以及一个可学习的任务$\mathcal{T}$，UDA的目标是通过利用$\mathcal{D}^{s}$和$\mathcal{D}^{t}$帮助提高目标函数$\textit{f}_{T}$($\cdot$)的学习，其中$\mathcal{D}^{s} \ne \mathcal{D}^{t}$.}

%UDA利用$\mathcal{D}^{s}$中大量的有监督数据$\textit{D}^{sl}$以及$\mathcal{D}^{t}$中大量无标签的数据$\textit{D}^{tu}$，改进$\mathcal{D}^{t}$上$\mathcal{T}$的性能。$\mathcal{D}^{s}$和$\mathcal{D}^{t}$的分布不同但相关，即$\mathcal{D}^{s} \ne \mathcal{D}^{t}$，且学习任务相同。与UDA相似，CDFSL问题同样利用$\textit{D}^{sl}$改进目标域上$\mathcal{T}$的性能，不同的是，CDFSL问题中目标域只有少量有监督数据$\textit{D}^{tl}$，且在CDFSL问题中源域和目标域上的任务不同，比如$\mathcal{T}^{s} \ne \mathcal{T}^{t}$。

\iffalse
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:dg}
\textbf{Domain Generalization (DG).} Given \textit{M} source domains $\mathcal{D}^{s}=\{\mathcal{D}^{s}_{i}|i=1,...,M\}$, where $\textit{P}_{i}^{s}(X) \ne \textit{P}_{j}^{s}(X)$, $1 \le i \ne j \le M$, an unseen target domain $\mathcal{D}^{t}$, and a learning task $\mathcal{T}$, the goal of DG is to learn a predictive function $f(\cdot)$ % $\mathcal{X} \to \mathcal{Y}$
by utilizing $\mathcal{D}^{s}$ to minimize the prediction error of $\mathcal{T}$ on $\mathcal{D}^{t}$, where $\mathcal{D}^{s} \ne \mathcal{D}^{t}$.
\end{MyDef}
}
\fi

\textit{Domain Generalization (DG).} DG uses a large amount of supervised data in $\textit{M}$ source domains $\mathcal{D}^{s}=\{\mathcal{D}^{s}_{i}|i=1,...,M\}$ to improve the performance of $\mathcal{T}$ on the unseen $\mathcal{D}^{t}$. The distributions of $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different but related, \ie $\mathcal{D}^{s} \ne \mathcal{D}^{t}$, and the tasks between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are same. Similar to DG, CDFSL also uses a large amount of supervised data in $\mathcal{D}^{s}$ to improve the performance of $\mathcal{T}$. However, CDFSL is designed to perform well on the special $\mathcal{D}^{t}$ but not all unseen $\mathcal{D}^{t}$, and the source data usually come from one source domain. Furthermore, the tasks of $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different, i.e., $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.
% The difference is that in CDFSL, $\mathcal{D}^{t}$ is not unseen, there is a small amount of supervised data $\textit{D}^{tl}$ from $\mathcal{D}^{t}$ during the training process, and in the CDFSL problem, the tasks on $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different, i.e., $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.

%\textit{给定\textit{M}个源域$\mathcal{D}^{s}=\{\mathcal{D}^{s}_{i}|i=1,...,M\}$，其中$\textit{P}^{s}_{i}(X) \ne \textit{P}^{s}_{j}(X)$, $1 \le i \ne j \le M$，一个可学习的任务$\mathcal{T}$，DG的目标是通过利用来自源域的数据学习一个预测函数\textit{f}($\cdot$)来最小化$\mathcal{T}$在unseen目标域$\mathcal{D}^{t}$上的预测误差，其中$\mathcal{D}^{s} \ne \mathcal{D}^{t}$.}

%DG利用\textit{M}个$\mathcal{D}^{s}$中大量的有监督数据$\textit{D}^{sl}$改进unseen的$\mathcal{D}^{t}$上$\mathcal{T}$的性能。$\mathcal{D}^{s}$和$\mathcal{D}^{t}$的分布不同但相关，即$\mathcal{D}^{s} \ne \mathcal{D}^{t}$，且学习任务相同。与DG相似，CDFSL问题同样利用$\textit{D}^{sl}$改进$\mathcal{D}^{t}$上$\mathcal{T}$的性能，不同的是，CDFSL问题中$\mathcal{D}^{t}$不是unseen的，在训练过程中有少量来自于$\mathcal{D}^{t}$的有监督数据$\textit{D}^{tl}$，且在CDFSL问题中源域和目标域上的任务不同，即$\mathcal{T}^{s} \ne \mathcal{T}^{t}$。

\iffalse
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:dafsl}
\textit{Domain Adaptation Few-shot Learning (DAFSL).} Considering a source domain $\mathcal{D}^{s}$ with sufficient supervised information, a target domain $\mathcal{D}^{t}$ with limited supervised details, and a few-shot learning task $\mathcal{T}$, the goal of DAFSL is helping improve the learning of the target predictive function $f(\cdot)$ for $\mathcal{T}$ in $\mathcal{D}^{t}$ using the prior knowledge in $(\mathcal{T}, \mathcal{D}^{s})$, where $\mathcal{D}^{s} \ne \mathcal{D}^{t}$.
\end{MyDef}
}
\fi

\textit{Domain Adaptation Few-shot Learning (DAFSL).} DAFSL leverages a significant amount of supervised data in the source domain $\mathcal{D}^{s}$ and a limited number of labeled data in the target domain $\mathcal{D}^{t}$ to enhance the performance of the task $\mathcal{T}$ on $\mathcal{D}^{t}$. Although the distributions of $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different, i.e., $\mathcal{D}^{s} \ne \mathcal{D}^{t}$, the learning tasks remain the same. Similarly, CDFSL utilizes the same data configurations in both domains to train the function for task $\mathcal{T}$. However, in contrast to DAFSL, the learning tasks in $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ differ in CDFSL.

%\textit{考虑一个包含大量监督信息的源域$\mathcal{D}^{s}$，一个只包含有限监督信息的目标域$\mathcal{D}^{t}$，以及一个小样本学习任务$\mathcal{T}$，DAFSL的目标是利用$\mathcal{D}^{s}$和$\mathcal{D}^{t}$为$\mathcal{D}^{t}$上的$\mathcal{T}^{t}$学习预测函数$\textit{f}$($\cdot$)，其中$\mathcal{D}^{s} \ne \mathcal{D}^{t}$.}

%DAFSL利用$\mathcal{D}^{s}$中大量的有监督数据$\textit{D}^{sl}$以及$\mathcal{D}^{t}$中少量有标签的数据$\textit{D}^{tl}$，改进$\mathcal{D}^{t}$上$\mathcal{T}$的性能。$\mathcal{D}^{s}$和$\mathcal{D}^{t}$的分布不同但相关，即$\mathcal{D}^{s} \ne \mathcal{D}^{t}$，且学习任务相同。与DAFSL相似，CDFSL问题同样利用$\textit{D}^{sl}$和$\textit{D}^{tl}$改进$\mathcal{D}^{t}$上$\mathcal{T}$的性能，不同的是，CDFSL问题中$\mathcal{D}^{s}$上的学习任务与$\mathcal{T}$不同，也就是说CDFSL问题不仅跨域还跨任务。

\iffalse
\vspace{-0.5cm}
\textcolor{black}{
\begin{MyDef}
\label{def:mtl}
\textbf{Multi-task learning (MTL).} Given a domain $\mathcal{D}$, consider $M$ learning tasks $\{\mathcal{T}_i\}^M_{i=1}$ where all or a part of tasks are related, the goal of MTL is to help improve the learning of the model for each $\mathcal{T}_i$ by using the knowledge contained in the $M$ tasks. All data is from $\mathcal{D}$.
\end{MyDef}
}
\fi

\textit{Multi-task Learning (MTL).} MTL utilizes $M$ tasks from $\mathcal{D}$ to improve the performance of every $\mathcal{T}_i$ (0 < i $\leq$ $M$). 
% All data in $\{\mathcal{T}_i\}^m_{i=1}$ is from the same domain, and 
All $\{\mathcal{T}_i\}^M_{i=1}$ are different but related. Different from MTL, the data of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$ in CDFSL is from different domains $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, \ie $\mathcal{D}^{s} \ne \mathcal{D}^{t}$ and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$, and the supervised data in $\mathcal{D}^{t}$ is limited.
% This illutrates that CDFSL problem has not only task shift also domain shift challenge.

% In this paper, we discuss the difference between CDFSL and heterogeneous-feature MTL. In the setting of supervised learning tasks, a task $\mathcal{T}_i$ is usually accompanied by a training dataset $D_i$ consisting of $n_i$ training samples, i.e., $D_i=\{(\boldsymbol{x}_j^i,y^i_j)\}_{j=1}^{n_i}$, where $\boldsymbol{x}_j^i \in \mathbb{R}^{d_i}$ is the $j$th training instance in $\mathcal{T}_i$ and $y^i_j$ is its label. We denote by $X_i$ the training data matrix for $\mathcal{T}_i$, i.e., $X_i=(\boldsymbol{x}_1^i,...,\boldsymbol{x}_{n_i}^i)$. 与CDFSL的区别

%\textit{\textbf{Few-shot Domain Generalization learning (FSDGL)}}

% \textit{\textbf{Multi-modal Few-shot learning (MMFSL)定义有问题}}
% \textit{Given \textit{M} source domains $\mathcal{D}^{s}=\{\mathcal{D}^{s}_{i}|i=1,...,M\}$ with sufficient supervised information and learning task $\mathcal{T}^{s}$, a target domain $\mathcal{D}^{t}$ with limited supervised information and few-shot learning task $\mathcal{T}^{t}$, the goal of CMFSL is to learn a target perdictive function $\textit{f}_{T}(\cdot)$ in $\mathcal{D}^{t}$ using the prior knowledge in $(\mathcal{T}^{s}, \mathcal{D}^{s})$, where $\mathcal{X}^{s}_{i} \ne \mathcal{X}^{t}$ ($1 \le i \le M$), and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$.}
% \textit{\textbf{Cross-task learning (CTL)}}

\iffalse
\begin{table}
\tiny
\centering
\caption{Related problem.}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\makecell[c]{Relative \\ problem}  & \makecell[c]{Cross-\\domain?} & \makecell[c]{Cross-\\task?} & Source \& Target set & Learning goal & knowledge shift \\ 
 \hline
Semi-DA & $\checkmark$ & $\times$ & \makecell[l]{Source: large labeled \\ Target: small labeled+unlabeled} & \makecell[l]{Improve the performance of tasks on the target domain by \\ exploiting a large amount of supervised data in the source \\ domain and a large amount of labeled and unlabeled data \\ in the target domain} & \makecell[l]{domain shift}   \\  % coviriate shift 
\hline
UDA      & $\checkmark$ & $\times$ & \makecell[l]{Source: large labeled \\ Target: large unlabeled} & \makecell[l]{Improve the performance of tasks on the target domain by \\ leveraging large amounts of supervised data in the source \\ domain and large amounts of unlabeled data in the target \\ domain}  & \makecell[l]{coviriate shift}   \\
\hline
DG      & $\checkmark$ & $\times$ & \makecell[l]{Source: large labeled source} & \makecell[l]{Leveraging large amounts of supervised data in the source \\ domain to improve performance on tasks on unseen target \\ domains} & \makecell[l]{domain shift}   \\
\hline
% MTL     & & $No$  & $Yes$ & large labeled & large labeled & Prior   \\
FSL      & $\times$ & $\checkmark$ & \makecell[l]{Source: large labeled \\ Target: limit labeled target} & \makecell[l]{The learning of target tasks is accomplished based on target \\ task data sets containing only a small amount of supervised \\ information, allowing knowledge transfer from related \\ auxiliary tasks (\ie, source tasks). The source task is different \\ from the target task} & \makecell[l]{task shift}    \\
\hline
DAFSL    & $\checkmark$ & $\times$ & \makecell[l]{Source: large labeled \\ Target: limit labeled} & \makecell[l]{Improve the performance of tasks on the target domain with \\ a large amount of supervised data in the source domain and \\ a small amount of labeled data in the target domain} & \makecell[l]{domain shift}   \\
\hline
CDFSL    & $\checkmark$ & $\checkmark$ & \makecell[l]{Source: large labeled \\ Target: limit labeled} & \makecell[l]{Improve the performance of tasks on the target domain with \\ a large amount of supervised data in the source domain and \\ a small amount of labeled data in the target domain}  & \makecell[l]{domain \& task shift}  \\

 \hline
\end{tabular}
\end{table}
\fi

\subsection{\textcolor{black}{Unique Issue and Challenge}} \label{unique}
% 在任何机器学习问题中，通常都存在预测错误，无法获得完美的预测，即经验风险最小化不可靠问题。在本节中，我们首先说明经验风险最小化，其次我们阐述了两阶段经验风险最小化，之后我们介绍了FSL的核心问题，而CDFSL作为FSL的一个重要分支，其核心问题也被讨论。

%基于监督机器学习中的错误分解的 CDFSL 的核心问题。该分析适用于 CDFSL 监督学习，包括分类和回归。

In machine learning, prediction errors are a common occurrence, making it impossible to achieve perfect predictions, \ie, the empirical risk minimization (ERM) unreliable problem. In this section, we begin by explaining the concept of empirical risk minimization (ERM). Next, we delve into the two-stage empirical risk minimization (TSERM) problem for CDFSL. Finally, we examine the distinct issues and challenges posed by CDFSL.
% In any machine learning problem, there are usually prediction errors and perfect prediction cannot be obtained, i.e. the empirical risk minimization unreliable problem. In this section, we first illustrate empirical risk minimization (ERM), and secondly, we describe the two-stage empirical risk minimization (TSERM), and then we introduce the core issues of FSL, and finally the unique issue of CDFSL, an important branch of FSL, is also discussed.

\subsubsection {Empirical Risk Minimization (ERM)}
Given an input space $\mathcal{X}$ and label space $\mathcal{Y}$, in which $X$ and $Y$ satisfy the joint probability distribution $P(X,Y)$, a loss function $l(\hat{y},y)$, a hypothesis $h \in \mathcal{H}$~\footnote{Hypothesis space $\mathcal{H}$ consists of all functions that can be represented by some choice of values for the weights~\cite{ml}. A hypothesis $h$ is a function in Hypothesis space.}, the risk (expected risk) of hypothesis $h(x)$ is defined as the expected value of the loss function:
\begin{align}
R(h)=\mathbb{E}[l(h(x),y)]=\int l(h(x),y)dP(x,y),
\end{align}
The ultimate goal of the learning algorithm is to find the hypothesis $h^{\ast}$ that minimizes the risk $R(h)$ in the hypothesis space $\mathcal{H}$:
\begin{align}
h^{\ast}=\text{argmin}_{h\in \mathcal{H}}R(h),
\end{align}
Since $P(x,y)$ is unknown, we compute an approximation called empirical risk by averaging the loss function over the training set:
\begin{align}
\hat R(h)=\frac{1}{n}\sum_{i=1}^{n}l(h(x_{i},y_{i})),
\end{align}
Therefore, the expected risk is usually infinitely approximated by empirical risk minimization ~\cite{erm1,erm2}, that is, a hypothesis $\hat{h}$ is chosen to minimize the empirical risk:
\begin{align}
\hat{h}=\text{argmin}_{h\in \mathcal{H}}\hat{R}(h)
\end{align}

In FSL, due to limited supervised information, the empirical risk $\hat R(h)$ may be far from an approximation of the expected risk $h^{\ast}$, resulting in the overfitting of empirical risk minimization hypothesis $\hat{h}$,
%The problem of overfitting, that is
\ie, the core problem of FSL is the unreliable empirical risk caused by insufficient supervised data. In current FSL approaches, transfer learning is commonly utilized to address overfitting by incorporating additional datasets to aid in task learning. However, as the tasks differ between the source and target domains, FSL is confronted with knowledge transfer challenges resulting from task shift. This is illustrated in the subsequent two-stage empirical risk minimization problem.

\iffalse
给定一个输入空间$X$和输出空间$Y$，且$X$和$Y$满足联合概率分布$P(X,Y)$，一个损失函数$l(\hat{y},y)$，一个假设$h \in \mathcal{H}$，假设$h(x)$的风险（期望风险）定义为损失函数的期望值：
$$R(h)=\mathbb{E}[l(h(x),y)]=\int l(h(x),y)dP(x,y)$$
学习算法的最终目标是在假设空间$\mathcal{H}$中找到使得风险$R(h)$最小的假设$h^{\ast}$：
$$h^{\ast}=\text{argmin}_{h\in \mathcal{H}}R(h)$$
由于$P(x,y)$是未知的，因此我们通过对训练集上的损失函数取平均值来计算一个近似值，称为经验风险：
$$\hat R(h)=\frac{1}{n}\sum_{i=1}^{n}l(h(x_{i},y_{i}))$$
因此通常通过经验风险最小化~\cite{erm1}~\cite{erm2}来无限近似期望风险，即选择一个假设$\hat{h}$将经验风险降到最低：
$$\hat{h}=\text{argmin}_{h\in \mathcal{H}}\hat{R}(h)$$

在FSL中，由于有限的监督信息，因此经验风险$\hat R(h)$可能远不是期望风险$h^{\ast}$的近似值，从而产生了经验风险最小化假设$\hat{h}$过拟合的问题，即FSL的核心问题是由于监督数据不足引起的经验风险不可靠问题。目前的FSL通常引入迁移学习来缓解$\hat{h}$过拟合的问题，即引入额外的数据集来帮助只有少量监督信息的小样本任务的学习，这些额外数据上的任务通常与小样本任务不同，这意味着FSL存在由于跨任务引起的知识迁移问题，如以下两阶段经验风险最小化问题所示。
\fi
\subsubsection{Two-Stage Empirical Risk Minimization (TSERM)}
We assume that all tasks share a generic nonlinear feature representation. The two-stage empirical risk minimization (TSERM) aims to transfer knowledge from the source task to the target task by learning this generic feature representation. In the first stage, the primary focus is on learning the general feature representation. The second stage then utilizes the acquired feature representation to construct an optimal hypothesis for the target task.

Specifically, we use $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$ to represent a source task and a target task. TSERM learns two hypotheses $f$ and $h$\footnote{both $f$ and $h$ are parametric models due to only limited supervised samples existing} in a hypothesis space $\mathcal{H}$, where $f$ learns a shared feature representation in the first stage, and $h$ utilizes it to learn a recognizer in the second stage. For convenience, we use
\begin{itemize}
\item [(1)] $(h^{\dagger},f^{\dagger})=\text{argmin}_{(f,h)\in \mathcal{H}}R(h, f)$ indicates the function that minimizes the expected risk.

\item [(2)] $(h^{\ast},f^{\dagger})$\footnote{we assume that there exist a common nonlinear feature representation $f^{\dagger}$ in $\mathcal{H}$} =
$\text{argmin}_{(f,h)\in \mathcal{H}}R(h, f)$ means the function that minimizes the expected risk in $\mathcal{H}$.

\item [(3)] $(\hat{h},\hat{f})=
\text{argmin}_{(f,h)\in \mathcal{H}}\hat{R}(h, f)$ represents the function that minimizes the empirical risk in $\mathcal{H}$.
\end{itemize}
Since $(h^{\dagger},f^{\dagger})$ is unknown, it must be approximated by $(h, f)\in \mathcal{H}$. $(h^{\ast},f^{\dagger})$ represents the most optimal approximation in $\mathcal{H}$, while $(\hat{h},\hat{f})$ represents the empirical risk minimization optimal hypothesis in $\mathcal{H}$. Suppose $(h^{\dagger},f^{\dagger})$, $(h^{\ast},f^{\dagger})$, $(\hat{h},\hat{f} )$ are all unique. In the first stage, the empirical risk of $\mathcal{T}^{s}$ is given by the following formula:
\begin{align}
\hat{R}_{s}(h_s, f)=\frac{1}{N^{s}}\sum^{N^s}_{i=1}l(h_s\circ f(x_i^s),y_i^s),
\end{align}
where $l(\cdot, \cdot)$ is the loss function,
% $h_s$ is the classification hypothesis on $\mathcal{T}^{s}$, 
$N^s$ represents the number of training samples in $\mathcal{T}^{s}$, and $x_i^s$ and $y_i^s$ represent the samples and corresponding labels in $\mathcal{T}^{s}$, respectively. $h_s$ is the hypothesis of $\mathcal{T}^{s}$, The optimal shared feature extraction function $\hat{f}(\cdot)$
%for extracting shared feature representations 
is expressed as
$\hat{f}=\text{argmin}_{(f,h_s)\in \mathcal{H}}\hat{R}_{s}(h_s,f)$.

In the second stage, the empirical risk of $\mathcal{T}^{t}$ is defined as:
\begin{align}
\hat{R}_{t}(h_t,f)=\frac{1}{N^{t}}\sum^{N^t}_{i=1}l(h_t\circ \hat{f}(x_i^t),y_i^t),
\end{align}
same as above, $h_t$ is the hypothesis of $\mathcal{T}^{t}$, $N^{t}$ denotes the number of training samples for $\mathcal{T}^{t}$, and $x_i^t$ and $y_i^t$ represent the samples and corresponding labels in $\mathcal{T}^{t}$, respectively. In the second stage, our goal is to estimate a hypothesis $\hat{h_t}=\text{argmin}_{(f,h_t)\in \mathcal{H}}\hat{R}_{t}(h_t,\hat{f})$ based on the shared feature representations learned in the first stage. We measure the function $(\hat{h_t},\hat{f})$ by the excess error on $\mathcal{T}^{t}$, namely:
\begin{equation}
    \begin{aligned}
& \mathbb{E}[R_{excess}]  =\mathbb{E}[R_{t}(\hat{h_t},\hat{f})-R_{t}(h_t^{\dagger}, f^{\dagger})] \\
& =\mathbb{E}[R_{t}(h^{\ast}_t,f^{\dagger})-R_{t}(h^{\dagger}_ t,f^{\dagger})]+\mathbb{E}[R_{t}(\hat{h}_t,\hat{f})-R_{t}(h^{\ast}_t,f^{\dagger})],
\end{aligned}
\end{equation}

Among them, $R_{t}(\cdot,\cdot)$ represents the expected risk on $\mathcal{T}^{t}$. $R_{excess}$ represents the relationship between the expected risk of $(\hat{h_t},\hat{f})$ and the optimal prediction rule $(h_t^{\dagger},f^{\dagger})$. Besides, we represents the estimation error with $\mathbb{E}[R_{t}(\hat{h}_t,\hat{f})-R_{t}(h^{\ast}_t,f^{\dagger})]$, \ie, minimizing the empirical risk $\hat{R}_{t}(h_t,f)$ in $\mathcal{H}$ instead of the expected risk $R_{t}(h_t,f)$, as shown by the blue dotted line in Figure \ref{issue1}. 
\begin{figure}%[b]
	\centering
  \vspace{-0.3cm}
	\includegraphics[width=\linewidth]{cdfsl-issue-26.pdf}
 \vspace{-0.7cm}
	\caption{\textcolor{black}{Comparison of vanilla supervised learning, FSL, and CDFSL problem. Solid circles denote distributions in which the data resides (the size means the amount of data), and dotted circles indicate the domain to which the target distribution belongs.}}
 \vspace{-0.3cm}
	\label{issue1}
\end{figure}

\subsubsection{Unique Issue and Challenge} \label{challenge}
We cannot optimize the approximation error, \ie $\mathbb{E}[R_{t}(h^{\ast}_t,f^{\dagger})-R_{t}(h^{\dagger}_ t,f^{\dagger})]$, due to the limitation of $\mathcal{H}$. Therefore, our goal is to optimize the estimation error,
%make $\hat{h_t}$ as close as possible to $h^{\ast}_t$ 
\ie $\mathbb{E}[R_{t}(\hat{h}_t,\hat{f})-R_{t}(h^{\ast}_t,f^{\dagger})]$. In Figure \ref{issue1}, the solid black arrow expresses the learning of empirical risk minimization. The solid circles indicate the different data distributions (the size of the circle means the amount of supervised information, the green and blue circles mean the source domain and target domain, respectively). The distribution where the target sample is located is depicted by the blue dotted circle.
% the length of the solid line is related to the amount of data. 
In Figure \ref{issue1}, (a) shows a vanilla supervised learning problem. It is easy to achieve ERM learning in the case of a large data set. The left part of (b) denotes the FSL problem, where the learning of ERM is not ideal when the amount of data is insufficient. The existing FSL strategy provides a good initialization for the target task through the different but relevant source task, as shown in the right part of Figure \ref{issue1} (b).

As a result of the domain gaps between the source and target datasets, a novel problem of CDFSL arises, as illustrated in Figure \ref{issue1}(c). As such, it is evident that the CDFSL problem involves both domain gaps and task shifts between the source and target domains, with limited supervised information available in $\mathcal{D}^{t}$. This makes CDFSL have its unique challenges while inheriting the challenges of FSL, namely an unreliable TSERM (estimation error optimization) due to the following factor: the CDFSL problem is characterized by domain gaps and task shifts, leading to a limited correlation between the source and target domains, thereby restricting the shared knowledge between them. As a result, it becomes challenging for the model to identify the optimal function $f$ for task $\mathcal{T}^{t}$ with the support of $\mathcal{D}^{s}$ and $\mathcal{T}^{s}$, where $\mathcal{D}^{s} \ne \mathcal{D}^{t}$ and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$. In other words, the shared knowledge between the source and target domains is challenging to extract.
%Due to the different selection of source data sets, a new problem CDFSL is derived, as shown in Figure \ref{issue1}(c). Hence, we can easily know that there are both domain gaps and task shifts between the source and target in the CDFSL problem, especially the limited supervised information contained in $\mathcal{D}^{t}$. It leads to a unique issue for CDFSL that model cannot find the optimal $f$ for $\mathcal{T}^{t}$ under the support of $\mathcal{D}^{s}$, $\mathcal{T}^{s}$, and $\mathcal{D}^{t}$ ($\mathcal{D}^{s} \ne \mathcal{D}^{t}$ and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$), resulting in an unreliable risk minimization on $\mathcal{D}^{t}$ in the second stage (Disappointing estimation error optimization). Based on the analysis of the above unique issue, the ideal of CDFSL is to achieve reliable ERM by extracting shared features with both strong domain generalization and instance recognition. 
% However, the goal suffers from the unique challenge, \ie the shared knowledge between source and target is hard to mine. Unlike other related problems (DA, FSL, \etc), the CDFSL problem has domain gaps and task shifts, leading to a further limited correlation between source and target, thereby restricting the shared knowledge between the two. Especially the distance of both domains and tasks between the source and target significantly stint the amount of shared information, making it harder to tap..
%However, the goal suffers from several practical challenges, which mainly stem from the following aspects:
\iffalse
\begin{itemize}
    \item The CDFSL problem is characterized by domain gaps and task shifts, leading to a limited correlation between the source and target domains, thereby restricting the shared knowledge between them. As a result, it becomes challenging for the model to identify the optimal function $f$ for task $\mathcal{T}^{t}$ with the support of $\mathcal{D}^{s}$ and $\mathcal{T}^{s}$, where $\mathcal{D}^{s} \ne \mathcal{D}^{t}$ and $\mathcal{T}^{s} \ne \mathcal{T}^{t}$. In other words, the shared knowledge between the source and target domains is challenging to extract.
    %\textit{Hard-to-mine shared knowledge between source and target.} Unlike other related problems (DA, FSL, \etc), the CDFSL problem has domain gaps and task shifts, leading to a further limited correlation between source and target, thereby restricting the shared knowledge between the two. Especially the distance of both domains and tasks between the source and target significantly stint the amount of shared information, making it harder to tap. % Unlike other related problems (DA, FSL, \etc), the CDFSL problem has both domain gaps and task shifts, leading to a further limited correlation between data from source and target domains, limiting shared knowledge between the two. Especially, the distance of both domains and tasks between the source and target significantly affects the FSL performance on the target domain in CDFSL. An unreasonable source easily causes a negative transfer problem, \ie, using the data or knowledge of the unreasonable source domain will make the learning performance worse.
    \item The best $h_t$ cannot be found with the limited information of $\mathcal{D}^{t}$. Ideally, the knowledge learned from the source domain should help the FSL tasks on the target domain perform well. However,  the change of image instance is affected by many natural scene factors, , which leads to enormous intra-class various and slight inter-class ambiguity. The limited supervised data in the target domain cannot fully exhibit this significant intra-class various and little inter-class ambiguity, making the model overfitting to these limited samples.
    %The best $h_t$ cannot be found with the limited information of $\mathcal{D}^{t}$. Ideally, the knowledge learned from the source domain should help the FSL tasks on the target domain perform well. However, the knowledge needs to fit better into the target task due to the limited supervised information in the target domain. Besides, the change of image instance is affected by many natural scene factors, such as lighting conditions, shooting angles, non-rigid deformation, image blur \etc, which leads to enormous intra-class various and slight inter-class ambiguity. The limited supervised data in the target domain cannot fully exhibit this significant intra-class various and little inter-class ambiguity, making the model overfitting to these limited samples.
    %\textit{Overfitting on finite samples.} Ideally, the knowledge learned from the source domain should help the FSL tasks on the target domain perform well. However, the knowledge needs to fit better into the target task due to the limited supervised information in the target domain. Besides, the change of image instance is affected by many natural scene factors, such as lighting conditions, shooting angles, non-rigid deformation, occlusion of the object itself, random noise (due to image compression, imaging conditions, etc.), image blur, which leads to enormous intra-class various and slight inter-class ambiguity. The limited supervised data in the target domain cannot fully exhibit this significant intra-class various and little inter-class ambiguity, making the model overfitting to these limited samples.
\end{itemize}
\fi

\subsection{\textcolor{black}{Taxonomy}} \label{taxonomy}
According to the above unique issue and challenge, CDFSL aims at mining as much shared knowledge as possible and finding the optimal $f$ for the target domain. Based on this consideration and to answer the question ``how to transfer", all the CDFSL techniques are categorized into the following four in this paper, as shown in Figure~\ref{step2}:
\begin{itemize}
    \item \textit{Instance-guided Approaches}. The model learns the optimal features from more diverse samples by introducing a subset of instances.
    \item \textit{Parameter-based Approaches}. Optimizing the model parameters and excluding some regions of $\mathcal{H}$ where the optimal function is unlikely to exist, reduces the scope of $\mathcal{H}$.
    \item \textit{Feature Post-processing Approaches}. Learning a feature function from the source domain and performing subsequent processing on its features. A new feature closest to $f^{\dagger}$ is obtained through post-processing operations.
    \item \textit{Hybrid Approaches}. Combining the multiple strategies from the above three categories.
\end{itemize}
\begin{figure}%[b]
	\centering
 \vspace{-0.1cm}
	\includegraphics[width=10cm]{cdfsl-method-type-1.pdf}
  \vspace{-0.1cm}
	\caption{\textcolor{black}{Different perspectives on how the CDFSL approaches find the optimal features.}}
 \vspace{-0.3cm}
	\label{step2}
\end{figure}

Accordingly, existing works can be categorized into a unified taxonomy. In the following sections, we will detail each category, performances, future works, and conclusion. The main contents of this paper are shown in Figure~\ref{out}.
 \begin{figure}[h]
	\centering
  %\vspace{-0.3cm}
	\includegraphics[width=\linewidth]{overview-my-16.pdf}
 \vspace{-0.3cm}
	\caption{\textcolor{black}{Outline of our survey. The main contents include the benchmarks, challenges, related topics, methodology, and future works of CDFSL.}}
 \vspace{-0.3cm}
	\label{out}
\end{figure}