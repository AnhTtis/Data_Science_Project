\section{Approaches} \label{methods}
CDFSL offers a unified solution to both cross-domain and few-shot learning problems. Based on the analysis of unique issue and challenges, we present a classification criterion for CDFSL algorithms, dividing them into four categories: instance-guided, parameter-based, feature post-processing, and hybrid approaches. The overview of CDFSL is depicted in Figure~\ref{overview}.
\begin{figure}%[h]
	\centering
	\includegraphics[width=\linewidth]{cdfsl-method-44.pdf}
  \vspace{-0.5cm}
	\caption{\textcolor{black}{An overall diagram of the CDFSL method. Firstly, the existing techniques pre-train the feature extractor on the source domain. Secondly, they fine-tune the feature extractor and train a novel recognizer on the target domain with limited labels. We classify existing CDFSL methods into Instance-guided, parameter-based, and feature post-processing.}}
  \vspace{-0.3cm}
	\label{overview}
\end{figure}


% \subsection{Backbone Architectures}

\subsection{Instance-guided Approaches}  \label{instance}
 \begin{wrapfigure}{l}{180pt}
	\centering
  \vspace{-0.2cm}
	\includegraphics[width=6cm]{cdfsl-data-1.pdf}
 \vspace{-0.3cm}
	\caption{\textcolor{black}{The different categories of instance-guided approaches. Instances are from different sources. $\theta$ means the recognizer.}}
 \vspace{-0.5cm}
	\label{fig_instance}
\end{wrapfigure}
This section presents approaches that learn the shared feature representation by incorporating additional valid instances from various sources, including the source domain, target domain, and additional domains. The diverse information provided by these sources offers practical guidance for finding shared features. For example, information from the source domain, often obtained from different modalities and views, expands the practical information and facilitates the learning of shared features. Furthermore, by incorporating information from the target domain, the model can better understand the target domain and generalize to it more easily. Information from multiple domains enables the model to learn a shared representation from various domains, making the learned features more generalizable. These approaches are illustrated in Figure~\ref{fig_instance}, and their details are presented in Table~\ref{table_instance}.
\begin{table}
\tiny
\centering
\caption{Representative instance-guided CDFSL approaches. `FG', `Art' and `IW' indicate that evaluation of Fine-grain based CDFSL (FG), Art-based CDFSL (Art) and imaging way-based CDFSL (IW), respectively. `CWUT' means Channel-Wise Uniform Transformation. And `KBS' represents Knowledge-Based Systems.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{llcccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Instances from} & \textbf{Introduced information} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
TriAE~\cite{lscdfsl}   & ACCV 2020 & Original data & Labels & $L_{2}$ &  & \Checkmark &      \\
% \hline
NSAE~\cite{boosting}  & ICCV 2021  & Original data & Generated images & BSR \& Log & \Checkmark &   & \Checkmark    \\
% \hline
SET-RCL~\cite{hybrid_5}   &  ACM MM 2022 & Original data &  CWUT & CE \& Contrastive \& Log &  \Checkmark &  & \Checkmark    \\
% \hline
MDKT~\cite{mdfsl} & Neurocomputing 2021 & Original data  & Class semantic & CE &   \Checkmark    &   &  \\  
% \hline
CDPSN~\cite{data_other_1} & Scientific Reports 2023 & Original data & Sketch map & CE &  &  & \Checkmark  \\
% \hline
ST~\cite{data_other_2}  & KBS 2023 & Original data & Transformation & CE &  \Checkmark   &  &    \\  
% \hline
DAML~\cite{data_multi_2}   & ICASSP 2022 & Multiple domains & 3 other datasets & CE & \Checkmark & \Checkmark &   \\
% \hline
MCDFSL~\cite{data_multi_3} & arXiv 2022 & Multiple domains & 7 auxiliary datasets & BSR \& Perceptual \& Style &  &  & \Checkmark   \\
% \hline
STARTUP~\cite{st} & ICLR 2021 & Target domain & Unlabeled target data & CE \& KL \& SimCLR &  &  & \Checkmark   \\
% \hline
DDN~\cite{dynamic} & NIPS 2021 & Target domain & Unlabeled target data & CE &  &  & \Checkmark  \\
% \hline
DSL~\cite{data_target_2} & ICLR 2022 & Target domain & Multiple targets & RCE \& Binary KLD & \Checkmark &  &   \\
% \hline
UD~\cite{data_target_1} & arXiv 2021 & Target domain & Unlabeled target data & Log &  & \Checkmark &  \\
\hline
\end{tabular}}
\vspace{-0.5cm}
\label{table_instance}
\end{table}

\subsubsection{Instances from Extra Information of Original Data} \label{data_original}
Some approaches involve the use of additional information from the original data, such as semantic and visual information, to enhance the performance of FSL tasks, as shown in Figure~\ref{data-original}. Among them, some works extract this extra information through reconstructed instances, as depicted in the green background area of the Figure~\ref{data-original}.  For example, in~\cite{lscdfsl}, a Triplet Autoencoder (TriAE) is utilized to learn a shared feature representation. It incorporates both source and target instances, and leverages semantic information as an intermediate bridge. In~\cite{boosting}, an autoencoder is used to reconstruct the input data, and the reconstructed data is then utilized as additional visual information to aid in the training process and learn the shared feature representation. And~\cite{hybrid_5} distills the knowledge of multiple tasks/domain-specific networks into a single network. This is achieved by aligning the representations of the single network with the task/domain-specific ones using small capacity adapters.
 \vspace{-0.2cm}
 \begin{wrapfigure}{r}{180pt} 
	\centering
	\includegraphics[width=6cm]{cdfsl-data1-43.pdf}
 \vspace{-0.2cm}
	\caption{\textcolor{black}{Instances from extra information of original data. The extra information can be from generation model (green area) or other modal like texts (blue part). Dotted lines indicate the process of additional information introduction.}}
	\label{data-original}
 \vspace{-0.3cm}
\end{wrapfigure}

Meanwhile, other works directly add additional information to the model, as shown in the blue part of Figure~\ref{data-original}. For instance,~\cite{mdfsl} presents a model that integrates visual and semantic information to recognize target categories, and utilizes weight imprinting for future fine-tuning. Furthermore, in~\cite{data_other_1}, the original image and its corresponding sketch map are processed separately by different branches of the network. The features extracted from the original image are combined with the contour features extracted from the sketch map branch during training, thus improving the accuracy and generalization performance of the model. Moreover,~\cite{data_other_2} proposes a task-expansion-decomposition framework for CD-FSL called the self-taught (ST) approach, which alleviates the problem of non-target guidance by constructing task-oriented metric spaces.

\subsubsection{Instances from Multiple Domains} \label{data_multiple}
% As shown in Figure~\ref{data-multiple}, t
By utilizing instances from multiple domains, a model can learn a general shared representation with broad generalization ability. The Domain-Agnostic Meta Learning (DAML) algorithm, proposed in~\cite{data_multi_2}, adapts the model to novel classes in seen and unseen domains. In contrast,~\cite{data_multi_3} introduces unlabeled data from multiple domains into the original source domain to transfer diverse styles, making the model more adaptable to various domains and styles. Moreover, most methods combine multiple strategies together with the multiple-domain introduction strategy, as shown in Section~\ref{hybrid}.

\vspace{-0.2cm}
\subsubsection{Instances from Target Domain} \label{data_target}
Approaches that leverage target domain instances aim to uncover the shared information between the source and target domains. Some of these approaches employ a teacher-student network to aid CDFSL learning. For example, in~\cite{st}, (illustrated in Figure~\ref{data-target}), a self-training method is proposed that utilizes unlabeled target data to improve the source domain representation. It is the first work to introduce the unlabeled target data into the training phase.
~\cite{dynamic} follows this setting and enforces consistency by comparing predictions of weakly-augmented unlabeled target data from a teacher network to strongly-augmented versions of the same images from a student network. Meanwhile,~\cite{data_target_2} develops a self-supervised learning approach to fully leverage unlabeled target domain data.
%(as depicted in the right part of Figure~\ref{data-target}). 
 \begin{wrapfigure}{l}{150pt} 
	\centering
 \vspace{-0.4cm}
	\includegraphics[width=5cm]{cdfsl-data3.pdf}
 \vspace{-0.4cm}
	\caption{\textcolor{black}{STARTUP~\cite{st} structure. The dotted line represents how to use the auxiliary target data.}}
	\label{data-target}
 \vspace{-0.3cm}
\end{wrapfigure}
Other works integrate all labeled target data directly into the training process. For instance,~\cite{data_target_1} presents a Domain-Switch Learning (DSL) framework that embeds cross-domain scenarios into the training phase in a "fast switching" manner using multiple target domains.

\subsubsection{Discussion and Summary}
% \textbf{Discussion and summary}
Instance-guided strategies are chosen based on the availability of data. When the source domain includes extra information such as semantic and visual information, utilizing instances from the original source (as described in Section~\ref{data_original}) is an effective approach. However, in scenarios where extra information is not available, introducing instances from the target domain (as discussed in Section~\ref{data_target}) may be a better option. In cases where target data is scarce or unavailable, utilizing instances from multiple domains (as outlined in Section~\ref{data_multiple}) can also be helpful.


\subsection{Parameter-based Approaches}  \label{parameter}
Parameter-based approaches are designed to reduce the complexity of the hypothesis space by manipulating the model's parameters to discover shared feature representations. There are three main techniques in this approach, as illustrated in Figure~\ref{parameterr}: (1) Parameter freeze involves fixing certain model parameters, simplifying the search for shared feature representations, (2) In parameter selection, the most appropriate model is selected from a pool of models based on their parameters, and (3) Parameter Reweighting employs additional parameters to constrain the hypothesis space. Table~\ref{table_parameter} provides a detailed summary of the methods that fall under this category.
% 旨在缩小假设空间范围，从而更容易找到共享特征
\begin{figure}%[b]
	\centering
	\includegraphics[width=13cm]{cdfsl-parameter-4.pdf}
 \vspace{-0.2cm}
	\caption{\textcolor{black}{Parameter-based category. (a), (b), and (c) indicate the parameter freeze, parameter selection, and parameter reweighting, respectively.}}
 \vspace{-0.5cm}
	\label{parameterr}
\end{figure}

\subsubsection{Parameter Freeze} \label{fixed}
Parameter freeze is a strategy that restricts the hypothesis space's complexity by fixing some model parameters. This method is usually used in meta-learning-based approaches, where they alternately freeze some parameters during meta-training and meta-testing phases. Among them, score-based meta transfer-learning (SB-MTL)~\cite{parameter_fix_1} combines transfer-learning and meta-learning by using a MAML-optimized feature encoder and a score-based Graph Neural Network. Some parameters in MAML are frozen in the training phase. And in~\cite{parameter_fix_5}, a meta-encoder is alternately frozen and optimized during the inner update phase to learn general features. In addition, other works propose plug-and-play augmentation modules to constrain the hypothesis space. In these works, the core idea of~\cite{feature-wise} is to asynchronously freeze and update the proposed feature-wise transformation layers and the feature extractor, as shown in Figure~\ref{parameterr} (a). Due to the inspiring of~\cite{feature-wise}, many works have improved and enhanced this work.~\cite{parameter_fix_3} proposes a diversified feature transformation based on the original feature transformation layer to solve the CDFSL problem. And~\cite{parameter_fix_7} offer two new strategies, FGNN (Flexible GNN) and a new hierarchical residual-like block, for the encoder and metric function of the metric-based network.

\subsubsection{Parameter Selection} \label{pa-select}
The parameter selection strategy, as depicted in Figure~\ref{parameterr} (b), seeks to identify the most appropriate set of parameters for the target domain to enhance performance. To achieve this, researchers have proposed various methods. For example, in~\cite{parameter_select_1}, the authors sample sub-networks by dropping neurons or feature maps, and then choose the most suitable sub-networks to form an ensemble for target domain learning. Additionally,~\cite{parameter_select_2} proposes a dynamic selection mechanism by sequentially applying multiple state-of-the-art adaptation methods, thereby enabling the configuration of the most appropriate modules for the downstream task.
\begin{table}%[b]
\tiny
\centering
\caption{Representative parameter-based CDFSL approaches. `NCA' means Neural Computing and Applications.}
\vspace{-0.3cm}
\setlength{\tabcolsep}{1.25mm}{
\begin{tabular}{llcm{5cm}cccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Strategy} & \textbf{Parameter} \textbf{operation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
 SB-MTL~\cite{parameter_fix_1}  & arXiv 2020 & Parameter freeze & Freeze partial layers in inner loop and update all network in outer loop & CE & & & \Checkmark      \\
% \hline
 MPL~\cite{parameter_fix_5} & TNNLS 2022 & Parameter freeze & Freeze network in inner loop and update it in meta update & CE & \Checkmark &  \Checkmark  &  \\
% \hline
 FWT~\cite{feature-wise}  & ICLR 2020 & Parameter freeze & Alternately update the parameters of the feature-wise transformation layers and backbone & CE &  \Checkmark & &    \\
% \hline
DFTL~\cite{parameter_fix_3} & ICAICA 2021 & Parameter freeze & Following the training setup of~\cite{feature-wise}, and utilize multiple FWT modules in each layers  & CE & \Checkmark &  & \\  
% \hline
FGNN~\cite{parameter_fix_7} & KBS 2022 & Parameter freeze & Following the training setup of~\cite{feature-wise} & Softmax & \Checkmark  &  & \\
% \hline
 AugSelect~\cite{parameter_select_1} & Big Data 2021 & Parameter selection & Select from multiple sub-network that obtained by dropping feature maps & CE & \Checkmark & \Checkmark & \\  
% \hline
 MAP~\cite{parameter_select_2} & arXiv 2021 & Parameter selection & Select from different modular adaptation pipeline & CE & \Checkmark & \Checkmark &   \\
% \hline
ReFine~\cite{parameter_weight_1} & CIKM 2022 & Parameter reweighting & Re-randomize the top layers of the feature extractor before fine-tuning on the target domain & CE &  &  & \Checkmark   \\
% \hline
VDB~\cite{parameter_fix_6} & CVPRW 2022 & Parameter reweighting & Introducing the "Visual Domain Bridge" into CNN’s Batch Normalization (BN) layers & CE &  &  & \Checkmark   \\
% \hline
AFGR~\cite{parameter_weight_3} & NCA 2022 & Parameter reweighting & Reweight the backbone with a residual attention module & CE & \Checkmark &  &   \\
% \hline
TPA~\cite{parameter_weight_4} & CVPR 2022 & Parameter reweighting & The task-specific weights are learned to adjust model parameters & CE & \Checkmark & \Checkmark &   \\
% \hline
ATA~\cite{ata} & IJCAI 2021 & Parameter reweighting & Insert a plug-and play model-adaptive task augmentation module into backbone & CE & \Checkmark &  & \Checkmark  \\
% \hline
AFA~\cite{adversarial} & ECCV 2022 & Parameter reweighting & Use an adversarial feature augmentation module to simulate distribution variations & CE \& Gram-matrix & \Checkmark &  & \Checkmark  \\
% \hline
Wave-SAN~\cite{parameter_fix_9} & arXiv 2022 & Parameter reweighting & A StyleAug module is proposed to adjust the parameter & CE \& SSL \& Style & \Checkmark &  & \Checkmark\\
% \hline
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{table_parameter}
\end{table}

\subsubsection{Parameter Reweighting} \label{pa-weight}
As depicted in Figure~\ref{parameterr} (c), the parameter reweighting technique optimizes the model's performance for the target domain by adjusting a limited number of parameters. Various studies have explored this approach to address the cross-domain challenge in few-shot learning. For instance,~\cite{parameter_weight_1} resets the parameters that were learned on the source domain before adapting to the target data. On the other hand,~\cite{parameter_fix_6} addresses the internal mismatch issue in BatchNorm by introducing the "Visual Domain Bridge" concept. Additionally,~\cite{parameter_weight_3} enhances the feature information by stacking a residual attention module into the feature encoder based on the residual network. Another study,~\cite{parameter_weight_4} trains task-specific weights from scratch on a small support set, as opposed to dynamically estimating them. Recent works like~\cite{ata} and~\cite{adversarial} propose adversarial methods to address the domain gap in few-shot learning, where~\cite{ata} considers the worst-case problem around the source task distribution and~\cite{adversarial} introduces a plug-and-play adversarial feature augmentation (AFA) method. Finally,~\cite{parameter_fix_9} adjusts the parameters of a novel Style Augmentation (StyleAug) module to achieve better performance in cross-domain few-shot learning.

\subsubsection{Discussion and Summary}
The parameter freeze strategy, as discussed in Section~\ref{fixed}, is often combined with meta-learning techniques. In the meta-training phase, two pseudo-domains, namely the pseudo-seen and pseudo-unseen domains, are used to simulate the cross-domain scenario. However, it is important to note that both of these domains are derived from the seen domain, resulting in a relatively small domain distance between them. As a result, algorithms that employ this strategy may not be effective in addressing the distant-domain problem in cross-domain few-shot learning (CDFSL).

The parameter selection strategy (Section~\ref{pa-select}) aims to adapt to the target domain by selecting the most suitable set of parameters from a pool of options. While this approach can be effective, it has a limited range of parameter sets to choose from, potentially limiting its ability to find the optimal set for the target domain. Moreover, some implementations of this strategy attempt to incorporate various techniques such as semi-supervised learning, domain adaptation, and fine-tuning within a single framework, resulting in a cumbersome and complex approach, making the framework bulky.

The parameter reweighting strategy (Section~\ref{pa-weight}) seeks to enhance the model's generalization capability through minimal parameter adjustments. This approach is critical to improving the model's performance. However, most existing reweighting methods utilize simple structures, which often result in limited improvements in terms of generalization. Therefore, further research is necessary to explore more complex and effective approaches to parameter reweighting in CDFSL.

\subsection{Feature Post-processing Approaches}  \label{feature}
In CDFSL, the transferable feature representation is achieved through post-processing of the original features, as illustrated in Figure~\ref{feature-post}. The post-processing strategies include feature selection, feature fusion, and feature transformation. Feature selection involves choosing the features from multiple domains that best fit the target domain. Feature fusion combines multiple features to generate a generalized feature representation. Lastly, feature transformation adjusts the original features using learnable weights. Table~\ref{table_feature} shows the related works in detail.
\begin{figure}%[b]
	\centering
	\includegraphics[width=13cm]{cdfsl-feature-2.pdf}
 \vspace{-0.2cm}
	\caption{\textcolor{black}{The feature post-processing categories. (a) represents the feature selection, in which the information closest to the shared feature is selected for knowledge transfer. (b) means feature fusion. Various features are stacked to approximate shared features. The left and right parts in (b) show the source of features to be merged. And (c) indicates feature transformation, \ie the shared feature is obtained through converting the original feature and the left and right parts in (c) means the different convert manners.}}
 \vspace{-0.4cm}
	\label{feature-post}
\end{figure}

\subsubsection{Feature Selection} \label{select}
The feature selection strategy involves identifying features closest to the target domain for use as the optimal shared feature representation. This approach is often employed in conjunction with the introduction of multi-domain instances. The strategy first obtains multiple features from different source domains, then selects some of them to aid target domain adaptation. As depicted in Figure~\ref{feature-post} (a),~\cite{feature_select_1} presents a Representative Multi-Domain Feature Selection (RMFS) algorithm to optimize the multi-domain feature extraction and selection process. While~\cite{feature_select_2} extracts a multi-domain representation by training a set of feature extractors and then automatically selecting the representations most relevant to the target domain.

\subsubsection{Feature Fusion (Stacking)} \label{fuse}
Feature fusion is an approach used to enhance the generalization ability of models. As depicted in Figure~\ref{feature-post} (b), this strategy combines features from different sources or dimensions into a single representation to improve FSL performance on the target domain. Many works, influenced by~\cite{shallow_layer}, believe that features from shallower layers are more transferable than those from deeper layers. Hence,~\cite{rf} proposed the CHEF method which unifies different abstraction levels of a deep neural network into one representation. Additionally,~\cite{feature_fusion_2} combined mid-level features to learn the discriminative information of each sample. Similarly,~\cite{parameter_weight_2} used a hierarchical prototype model to combine information from hierarchical memory into final prototype features. Unlike the fusion of shallow layer features, in~\cite{feature_fusion_3}, the representation of graphs is obtained by augmenting the graphs from sampled tasks into three views: one contextual and two geometric, and encoding each view with a dedicated encoder. Finally, the representations are aggregated into a single graph representation using an attention mechanism. 
The right part of Figure~\ref{feature-post} (b) shows the features from different network layers are fused, whereas the left part shows that features from a set of different networks are stacked.
\begin{table}%[b]
\tiny
\centering
\caption{Representative feature post-processing CDFSL approaches. `GR' represents geometrical regularization.}
\vspace{-0.3cm}
\setlength{\tabcolsep}{1.0mm}{
\begin{tabular}{llcm{5cm}cccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Strategy} & \textbf{Feature operation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
RMFS~\cite{feature_select_1} & IC-NIDC 2021 & Feature selection & extract the multi-domain features and select from them & CE & \Checkmark & \Checkmark &       \\
% \hline
SUR~\cite{feature_select_2} & ECCV 2020 & Feature selection & Leverage the multi-domain feature bank to autonomously identify the most pertinent representations & CE & \Checkmark & \Checkmark  &  \\
% \hline
CHEF~\cite{rf} & arXiv 2020 & Feature fusion & Accomplish the representation fusion through an ensemble of Hebbian learners operating on diverse layers of the network & CE &  &  & \Checkmark \\  
% \hline
MLP~\cite{feature_fusion_2} & ACM MM 2021 & Feature fusion & Weight the fusion of mid-level features and investigate a residual-prediction task & CE \& $L_{2}$ & \Checkmark  & \Checkmark & \\
% \hline
HVM~\cite{parameter_weight_2} & ICLR 2022 & Feature fusion & The mid-level features are weighted and fused in a hierarchical prototype model & CE \& KL &  &  & \Checkmark \\  
% \hline
TACDFSL~\cite{feature_reweight_4} & Symmetry 2022 & Feature transformation & Propose the adaptive feature distribution transformation & CE &  &  & \Checkmark   \\
% \hline
MemREIN~\cite{feature_reweight_5} & IJCAI 2022 & Feature transformation & Explore an instance normalization algorithm and a memorized module to transform the original features
 & CE \& Contrastive & \Checkmark &  &    \\
% \hline
RDC~\cite{feature_reweight_6} & CVPR 2022 & Feature transformation & Transform and reweight the original features through hyperbolic tangent transformation & CE \& KL & \Checkmark &  & \Checkmark  \\
% \hline
StyleAdv~\cite{feature_reweight_0} & arXiv 2023 & Feature transformation & Introducing variations to the initial style using the signed style gradients & CE \& KL & \Checkmark &  & \Checkmark  \\
% \hline
LRP~\cite{feature_reweight_1} & ICPR 2020 & Feature transformation & Develop a model-agnostic explanation-guided training strategy that dynamically finds and emphasizes the features which are important for the predictions & CE & \Checkmark &  &   \\
% \hline
BL-ES~\cite{feature_reweight_2} & ICME 2021 & Feature transformation & An inductive graph network (IGN) is optimizaed by MPGN module, in which include multiple features & BCE \& GR & \Checkmark &  & \Checkmark  \\
% \hline
DeepEMD-SA~\cite{feature_reweight_3} & ISCIPT 2021 & Feature transformation & Employs an attention module to enable interaction between the local features & CE &  &  & \Checkmark\\
% \hline
FUM~\cite{feature_reweight_7} & PR 2022 & Feature transformation & Using a forget-update module to regulate the features & CE & \Checkmark & \Checkmark &  \\
% \hline
ConFeSS~\cite{confess} & ICLR 2022 & Feature transformation & Utilizing a masking module to select relevant information that are more suited to target domain in the features & CE \& Divergence &  &  & \Checkmark \\
% \hline
TCT-GCN~\cite{feature_reweight_11} & SSRN 2023 & Feature transformation & Combining the multi-levelf feature fusion and feature transform & CE & \Checkmark &  & \Checkmark\\
% \hline
StabPA~\cite{feature_reweight_9} & ECCV 2022 & Feature transformation & Transform features through learning prototypical compact and cross-domain aligned representations & Softmax & \Checkmark & \Checkmark &  \\
% \hline
\hline
\end{tabular}}
\vspace{-0.4cm}
\label{table_feature}
\end{table}

\subsubsection{Feature Transformation} \label{reweights}
The feature transformation strategy reweights features to improve performance, as shown in Figure~\ref{feature-post} (c). Some methods obtain the weights through a transformation and weighting, \eg the right part of Figure~\ref{feature-post} (c), while others use a learnable module, \eg the left part of Figure~\ref{feature-post} (c). For the former category, in~\cite{feature_reweight_4}, WDMDS (Wasserstein Distance for Measuring Domain Shift) and MMDMDS (Maximum Mean Discrepancy for Measuring Domain Shift) were proposed to solve CDFSL.~\cite{feature_reweight_5} introduced the MemREIN framework which considers memorization, restitution, and instance normalization, \eg an instance normalization algorithm is explored to alleviate feature dissimilarity. And~\cite{feature_reweight_6} minimizes task-irrelevant features while keeping more transferrable discriminative information by constructing a non-linear subspace and using a hyperbolic tangent transformation. Furthermore, a novel model-agnostic meta style adversarial training (StyleAdv) method together with a novel style adversarial attack method is proposed for CDFSL in~\cite{feature_reweight_0}.

Additionally, there are methods that use a learnable module to determine the feature weights. For example,~\cite{feature_reweight_1} computes explanation scores for intermediate features and reweights them accordingly.~\cite{feature_reweight_2} acquires the weights by training a bilevel episode strategy (BL-ES) to weight the features. And~\cite{feature_reweight_3} employs an attention module upon a local-descriptor-based model called DeepEMD to enable interaction between the local features. Furthermore,~\cite{feature_reweight_7} reweights features through extracting relationship embeddings using Forget-Update Modules (FUM). And recently,~\cite{confess} employed a masking module to reweight features, selecting those that are more suited to the target domain. And a task context ransformer and graph convolutional network (TCT-GCN) method is proposed in~\cite{feature_reweight_11}. Lastly, some methods address the CDFSL problem by combining domain adaptation and few-shot learning methods. For instance,~\cite{feature_reweight_9} proposes stabPA to learn compact, cross-domain aligned representations.

\subsubsection{Discussion and Summary}
Feature selection strategies can be helpful in selecting the most suitable features for the target domain in the presence of multi-domain or auxiliary views data, as discussed in Section~\ref{select}. However, when there are no multiple domains available, features from a single source domain may have limited variability, which means selecting different features from the same source domain may not significantly improve the performance of FSL on the target domain.

The feature fusion strategy (presented in Section~\ref{fuse}) aims to obtain features from multiple sources, either from different layers within a single network or from multiple networks. However, in the case of the former, similarities among the features from the same dataset and network may require effective fusion methods, while in the latter, the use of multiple networks can increase training costs due to the need for simultaneous training.

Feature transformation (introduced in Section~\ref{reweights}) is a common approach when extra network and multi-domain data are not available. It involves reweighting features by assigning new parameters to them, either through simple transformation and weighting, or through a learnable module. However, this strategy only allows limited exploration of shared information as it only reweights the features output from the final layer.

\subsection{Hybrid Approaches}  \label{hybrid}
Hybrid approaches in CDFSL incorporate the above menthined atrategies, the related technologies is listed in Table~\ref{table_hybrid}. Combinations of instance-guided and parameter-based strategies are prevalent in CDFSL. For example, a parameter-efficient multi-mode modulator is proposed in~\cite{hybrid_2}. First, the modulator is designed to maintain multiple modulation parameters (one for each domain) in a single network, thus achieving single-network multi-domain representation. Second, it divides the modulation parameters into the domain-specific and the domain-cooperative sets to explore the intra-domain information and inter-domain correlations, respectively. Furthermore,~\cite{tgdm} explores a novel target guided dynamic mixup (TGDM) framework to generate the intermediate domain images to help the FSL task learning on the traget domain. In addition,~\cite{data_multi_1} learns the meta-learners by utilizing multiple domains, and the meta-learners are combined in the parameter space to be the Initialized parameters of a network used in the target domain. Besides, researchers explore the combination of feature post-process and and parameter-based strategies in CDFSL.~\cite{feature_reweight_10} conducts style transfer-based task augmentation with feature fusion tasks from different tasks and styles and feature modulation module (FM). And in~\cite{hybrid_6}, a feature extractor stacking (FES) is proposed to combine information from a backbones collection. 
\vspace{-0.3cm}
\begin{table}[b]
\tiny
\centering
\caption{Representative hybrid CDFSL approaches. `FCS' represents `Frontiers of Computer Science'.}
\vspace{-0.2cm}
\begin{tabular}{llccccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Instance-guided} & \textbf{Feature post-process} & \textbf{Parameter-based} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
 CosML~\cite{data_multi_1}  & arXiv 2020 & Multiple domains  & Feature fusion & \XSolidBrush & CE & \Checkmark &   &     \\
% \hline
URL~\cite{hybrid_4}   &  ICCV 2021 & Multiple domains & \XSolidBrush & Parameter reweight & CE \& CKA \& KL & \Checkmark & \Checkmark &     \\
% \hline
Meta-FDMixup~\cite{hybrid_1} & ACM MM 2021 & Labeled target  & Feature transformation & \XSolidBrush & CE \& KL & \Checkmark &   &     \\  
% \hline
Tri-M~\cite{hybrid_2}    &  ICCV 2021  & Multiple domains & \XSolidBrush & Parameter reweight & CE & \Checkmark & \Checkmark &      \\
% \hline
ME-D2N~\cite{data_target} & ACM MM 2022 & Labeled target & Feature transformation & \XSolidBrush & CE \& KL & \Checkmark &   &    \\
% \hline
TL-SS~\cite{hybrid_3}  &  AAAI 2022  & Original data & \XSolidBrush & Parameter reweight & CE \& Metric & \Checkmark &  & \Checkmark     \\  
% \hline
TGDM~\cite{tgdm}   & ACM MM 2022 & Labeled target & \XSolidBrush & Parameter reweight  & CE & \Checkmark &   &    \\
% \hline
% SET-RCL~\cite{hybrid_5}   &  ACM MM 2022 & Original data &  &  & CE \& Contrastive & \Checkmark & \Checkmark     \\
% \hline
TAML~\cite{feature_reweight_10} & arXiv 2023 & Multiple domains & Future fusion & Parameter reweight & CE & \Checkmark &  & \Checkmark     \\
TKD-Net~\cite{data_multi_0} & FCS 2023 & Multiple domains & Future fusion & \XSolidBrush & CE \& KL \& $L_{2}$ & \Checkmark &   &     \\
\hline
\end{tabular}
\vspace{-0.2cm}
\label{table_hybrid}
\end{table}

\subsubsection{Hybrid via Loss Function}
Several works solve CDFSL problem not only combine the above mentioned strategies but also with different loss function such as contrastive loss, metric loss, \etc.~\cite{hybrid_1} advocates utilizing few labeled target data to guide the model learning, and is optimized by CE loss and KL loss. Technically, a novel meta-FDMixup network is proposed to extract the disentangled domain-irrelevant and domain-specific features with a novel disentangle module and a domain classifier. And~\cite{data_target} follows this setup (introduce few labeled target domian data) and proposes a Multi-Expert Domain Decompositional Network (ME-D2N) to solve CDFSL. The loss function also include CE and KL loss.~\cite{set-rcl} proposes a Style-aware Episodic Training with Robust Contrastive Learning (SET-RCL) to make the learned model can achieve better adapt to the test tasks with domain-specific styles. And TL-SS strategy~\cite{hybrid_3} augments multiple views of tasks and proposes a high-order associated encoder (HAE) to generate proper parameters and enables the encoder to flexibly to any unseen tasks. The loss function in this work include CE and a metric loss. Moreover,~\cite{hybrid_4} learns a single set of deep universal representations by distilling the knowledge of multiple separately trained networks by using multiple domains after co-aligning their features with the help of adapters and centered kernel alignment. It is optimized by CKA, CE, and KL loss. Furthermore,~\cite{data_multi_0} proposes team-knowledge distillation networks (TKD-Net) and explores a strategy to help the cooperation of multiple teachers.

\subsubsection{Discussion and Summary}
The combination of multiple strategies in CDFSL, as discussed in Section~\ref{hybrid}, can lead to improved performance. For example, the instance-guided strategy is often easily incorporated into various methods, and as such, is frequently combined with other approaches. However, there are also challenges associated with combining strategies. The combination of feature post-processing and parameter-based strategies can be unpredictable and may lead to negative transfer, making it a less frequently explored option. To achieve optimal results, it is essential to avoid negative transfer and carefully consider the combination of strategies in hybrid approaches.