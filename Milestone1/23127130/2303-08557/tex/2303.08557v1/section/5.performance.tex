\section{Performance} \label{performance}
This section provides a comprehensive overview of the evaluation process for models in the field of Cross-Domain Few-Shot Learning (CDFSL). In order to evaluate the effectiveness of these models, we need to examine the appropriate datasets and benchmarks used. This is covered in Section~\ref{data} and Section~\ref{benchmark} respectively. In Section~\ref{per-com}, we delve deeper into a thorough analysis and comparison of the performance of various method categories in the field of CDFSL. This section provides a crucial evaluation of the models, highlighting the strengths and weaknesses of different approaches to address the challenging problem of CDFSL.

\subsection{Datasets} \label{data}
The evaluation of CDFSL models is facilitated by the availability of annotated datasets. The comparison of various algorithms and architectures is made fair through the use of these datasets. The continuous growth in complexity, size, annotation number, and transfer difficulty of the datasets represents an ongoing challenge that drives the development of innovative and superior techniques. Table~\ref{dataset} presents a list of the most widely used datasets for the CDFSL problem, and the following sections provide an in-depth description of each:
\vspace{-0.2cm}
\begin{table}[b]
\tiny
\centering
\vspace{-0.3cm}
\caption{Details of datasets in CDFSL.}
\vspace{-0.1cm}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{lcccccccc}
\hline
\textbf{Datasets} & \textbf{Derived from} & \textbf{\makecell[c]{Number of \\ images}} & \textbf{Image size} & \textbf{\makecell[c]{Number of \\ categories}} & \textbf{Content} & \textbf{Fields} & \textbf{Reference}     \\ 
 \hline
\textit{mini}ImageNet & ImageNet & 60000 & $84 \times 84$ & 100 & objects classification & natural scene & \cite{miniimagenet}    \\  
% \hline
\textit{tiered}ImageNet & ImageNet & 779165 & $84 \times 84$ & 608 & objects classification & natural scene & \cite{tieredimagenet}     \\
% \hline
Plantae  & iNat2017 & 196613 & varying & 2101 & plants \& animals classification & natural scene & \cite{plantae}  \\
% \hline
Places     & N/A & 10 million & $200 \times 200$ & 400+ & scene classification  & natural scene & \cite{places}   \\
% \hline
Stanford Cars     & N/A & 16185 & varying & 196 & cars fine-grained classification & natural scene &  \cite{cars}   \\
% \hline
CUB   & ImageNet & 11788 & $84 \times 84$ & 200 & birds fine-grained classification & natural scene &  \cite{cub}   \\  
% \hline
CropDiseases   & N/A & 87000 & $256 \times 256$ & 38 & crop leaves classification & natural scene &  \cite{cropdiseases}  \\
% \hline
EuroSAT   & Sentinel-2 satellite & 27000 & $64 \times 64$ & 10 & land classification & remote sensing &  \cite{eurosat}   \\
% \hline
ISIC 2018   & N/A & 11720 & $600 \times 450$ & 7 & dermoscopic lesion classification & medical &  \cite{isic1}   \\
% \hline
ChestX   & N/A & 100K & $1024 \times 1024$ & 15 & lung diseases classification & medical & \cite{chestx}  \\
% \hline
Omniglot   & N/A & 25260 & $28 \times 28$ & 1623 & characters classification & character &  \cite{omniglot}   \\
% \hline
FGVC-Aircraft   & N/A & 10200 & varying & 100 & Aircraft fine-grained classification & natural scene &  \cite{aircraft}   \\
% \hline
DTD   & N/A & 5640 & varying & 47 & textures classification & natural scene &  \cite{dtd}   \\
% \hline
Quick Draw   & Quick draw! & 50 million & $128 \times 128$ & 345 & drawing images classification & Art &  \cite{draw1}   \\
% \hline
Fungi   & N/A & 100000 & varying & 1394 & fungi fine-grained classification & natural scene &  \cite{fungi}   \\
% \hline
VGG Flower   & N/A & 8189 & varying & 102 & flowers fine-grained classification & natural scene &  \cite{vgg}   \\
% \hline
Traffic Signs   & N/A & 50000 & varying & 43 & Traffic signs classification & natural scene &  \cite{traffic}   \\
% \hline
MSCOCO   & N/A & 1.5 million & varying & 80 & objects classification & natural scene &  \cite{mscoco}   \\
\hline
\end{tabular}}
\vspace{-0.2cm}
\label{dataset}
\end{table}
\begin{itemize}
    \item \textit{miniImageNet}~\cite{miniimagenet}: \textit{mini}ImageNet dataset consists of 60000 images selected from the dataset ImageNet, with a total of 100 categories. Each category has 600 images, and the size of each image is $84 \times 84$.
    
    \item \textit{tieredImageNet}~\cite{tieredimagenet}: \textit{tiered}ImageNet dataset is selected from the ImageNet dataset, including 34 categories, and each category contains 10-30 sub-categories (classes). There are 608 classes and 779165 images in this dataset. Each class has multiple samples of varying numbers.
    
    \item \textit{Plantae}~\cite{plantae}: Plantae dataset is one of dataset iNat2017. There are 2101 categories and 196613 images in this dataset.
    \item \textit{Places}~\cite{places}: Places dataset contains more than 10 million images of 400+ unique scene categories. This dataset features 5000 to 30000 training images in each class, which is consistent with real-world frequency of occurrence. The image size in this dataset is $200 \times 200$.
    %\item Stanford Dogs~\cite{dogs}: The Stanford Dogs dataset contains over 20,000 images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for fine-grained image categorization.
    \item \textit{Stanford Cars}~\cite{cars}: The Cars dataset is a fine-grained classification dataset about cars. It contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images.
    \item \textit{CUB}~\cite{cub}: Images in CUB dataset overlap with images in ImageNet. It is a fine-grained classification dataset about birds that contain 11788 images in 200 categories. The size of images in this dataset is $84 \times 84$.
    \item \textit{CropDiseases}~\cite{cropdiseases}: The cropDiseases dataset consists of about 87000 RGB images of healthy and diseased crop leaves, categorized into 38 different classes. The total dataset is divided into an 80/20 training and validation set ratio. The image size in this dataset is $256 \times 256$.
    \item \textit{EuroSAT}~\cite{eurosat}: EuroSAT is a dataset for land use and land cover classification. The dataset is based on Sentinel-2 satellite images consisting of 10 classes with in total of 27,000 labeled and geo-referenced images. Each class includes 2000-3000 images, and the size of these images is $64 \times 64$.
    \item \textit{ISIC 2018}~\cite{isic1,isic2}: ISIC 2018 dataset includes 10015 dermoscopic lesion images from 7 categories for training, 193 images for evaluate, and 1512 images for testing. The size of each image is $600 \times 450$.
    \item \textit{ChestX}~\cite{chestx}: ChestX-ray14 is currently the largest lung X-ray database provided by the NIH Research Institute, which contains 14 lung diseases, and category 15 indicates no disease was found. The size of images in this dataset is $1024 \times 1024$.
    \item \textit{Omniglot}~\cite{omniglot}: The Omniglot dataset comprises 1,623 handwritten characters from 50 languages, each with 20 different handwritings. The size of each image in this dataset is $28 \times 28$.
    \item \textit{FGVC-Aircraft}~\cite{aircraft}: FGVC-Aircraft dataset includes 10200 aircraft images (102 aircraft models, 100 images per model). The image resolution is about 1-2 Mpixels.
    \item \textit{Describable Textures (DTD)}~\cite{dtd}: DTD is a texture database consisting of 5640 images, organized according to a list of 47 terms (categories) inspired by human perception. There are 120 images for each category. Image sizes range between 300x300 and 640x640.
    \item \textit{Quick Draw}~\cite{draw1}: The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!
    \item \textit{Fungi}~\cite{fungi}: This datasets contains 100000 fungi images belong to 1394 different categories, which is all fungi classes that have been spotted by the general public in Denmark.
    \item \textit{VGG Flower}~\cite{vgg}: VGG Flower dataset contains 8189 flower images belong to 102 categories. The flowers chosen to be flower commonly occuring in the United Kingdom. Each class consists of between 40 and 258 images.
    \item \textit{Traffic Signs}~\cite{traffic}: Traffic Signs dataset consists of 50,000 images of German road signs in 43 classes.
    \item \textit{MSCOCO}~\cite{mscoco}: The images in MSCOCO dataset are collected from Flickr with 1.5 million object instances belonging to 80 classes labelled and localized using bounding boxes.
\end{itemize}

\subsection{Benchmarks} \label{benchmark}
This section mainly introduces the benchmarks of the CDFSL problem, including \textit{mini}ImageNet \& CUB (mini-CUB), a standard fine-grained classification benchmark (FGCB), BSCD-FSL~\cite{bscd-fsl}. Besides, Meta-Dataset~\cite{meta-dataset} also is proposed to evaluate the cross-domain problem in FSL. Due to the mini-CUB is included in FGCB, we mainly introduce the last three benchmarks.

\textit{FGCB}.
A conventional benchmark was derived for fine-grain based CDFSL (FG-CDFSL) in the early stage of CDFSL development. It contains five datasets including \textit{mini}ImageNet, Plantae~\cite{plantae}, Places~\cite{places}, Cars~\cite{cars}, and CUB~\cite{cub}, in which we usually regard \textit{mini}ImageNet as the source domain and other datasets as the target domain. All images in this benchmark are natural images. %The source domain includes 60000 common objects. The target domains include four fine-grained datasets including Plantae, Places, Cars, and CUB. 
The main challenge across domains for this benchmark is transferring the category information from coarse to fine. %The visual display is shown in Figure\ref{fine-grained}.

\textit{BSCD-FSL}~\cite{bscd-fsl}.
As a more challenging benchmark to address imaging way based CDFSL (IW-CDFSL) in CDFSL, BSCD-FSL includes five datasets consisting of \textit{mini}ImageNet, CropDisease~\cite{cropdiseases}, EuroSAT~\cite{eurosat}, ISIC~\cite{isic1,isic2}, ChestX~\cite{chestx}. CropDisease is a fine-grained dataset of crop leaves and all-natural industrial images. EuroSAT, ISIC, and ChestX have different imaging ways with natural images. They are satellite images, dermatology images, and radiology images, respectively.

\textit{Meta-Dataset}~\cite{meta-dataset}.
Meta-Dataset is a large-scale, diverse benchmark for measuring various image classification models in realistic and challenging few-shot contexts such as CDFSL. This dataset consists of 10 publicly available natural image datasets, handwritten characters, and graffiti datasets
%, including ILSVRC-2012 \cite{imagenet}, Omniglot \cite{omniglot}, Aircraft \cite{aircraft}, CUB-200-2011 \cite{cub}, Describable Textures \cite{dtd}, Quick Draw \cite{draw1}, Fungi \cite{fungi}, VGG Flower \cite{vgg}, Traffic Signs \cite{traffic}, and MSCOCO \cite{mscoco}
. These datasets were chosen because they are free and easy to obtain, span a variety of visual concepts (natural and human-made), and vary in how fine-grained the class definition is. Hence, this benchmark can address fine-grain based (FG) and art-based CDFSL (Art) problem. And this benchmark breaks the requestment that source and target data from the same domain in FSL and limitations of \textit{N-way K-shot} form tasks. And it also introduces the class imbalance in the real world, which means it changes the number of classes in each task and the size of training set. %Figure \ref{meta-dataset} is a visual display of Meta-Dataset.

In addition to the commonly used benchmarks above, some methods adopt benchmarks initially designed for the domain adaptation (DA) problem. The benchmark DomainNet~\cite{domainnet} (designed to solve art-based cross-domain problem) is widely used in DA and comprises 6 domains, each with 345 categories of common objects. Additionally, the benchmark Office-Home~\cite{officehome} is utilized by some studies for CDFSL, consisting of 4 domains (art, clipart, product, and real world) and 65 categories per domain. The benchmark is comprised of 15,500 images, with an average of 70 images per class and a maximum of 99 images per class.

\subsection{Performance Comparison and Analysis} \label{per-com}
This section sheds light on the comparative performance of CDFSL approaches from different categorizations. The standard evaluation metric used in CDFSL is prediction accuracy and the evaluations are typically conducted under various settings, including 5-way 1-shot, 5-way 5-shot, 5-way 20-shot, and 5-way 50-shot. As CDFSL is a subfield of FSL, many classical FSL methods can be applied directly to CDFSL problems. The results of these methods are shown in Table~\ref{fsl_mtd}, where it can be observed that, the meta-learning based methods (MatchingNet, ProtoNet, RelationNet, MAML) possess slightly lower performance in CDFSL due to the presence of domain gaps, they perform comparatively less well than simple fine-tuning transfer learning based methods, particularly as the value of K increases.
\begin{table}%[b]
\tiny
\centering
\caption{The CDFSL performance on the classical FSL approaches with ResNet10 backbone. $K$ is the number of samples from $5$-way $K$-shot.}
%\vspace{-0.2cm}
\begin{tabular}{clcccccccc}
\hline 
\textbf{\textit{K}} & \textbf{Methods} & \textbf{CropDiseases} & \textbf{EuroSAT} & \textbf{ISIC} & \textbf{ChestX} & \textbf{Plantae} & \textbf{Places} & \textbf{Cars} & \textbf{CUB}   \\ 
% \cline{3-18}
\hline
\multirow{5}*{1}  & Fine-tuning~\cite{bscd-fsl} & 61.56±0.90 & 49.34±0.85 & 30.80±0.59 &  21.88±0.38 & 33.53±0.36 & 50.87±0.48 & 29.32±0.34 &  41.98±0.41    \\ 
% \cline{2-10}
  & MatchingNet~\cite{miniimagenet} & 48.47±1.01 & 50.67±0.88 & 29.46±0.56 & 20.91±0.30 & 32.70 ± 0.60 & 49.86±0.79 & 30.77±0.47 & 35.89±0.51 \\
% \cline{2-10}
%   &  MAML & - & - & - & - & - & - & - & -   \\ 
% \cline{2-10}
  &  RelationNet~\cite{relation} & 56.18±0.85 & 56.28±0.82 & 29.69±0.60 & 21.94±0.42 & 33.17±0.64 & 48.64±0.85 & 29.11±0.60 & 42.44±0.77 \\
% \cline{2-10}
  &  ProtoNet~\cite{proto} & 51.22±0.50 & 52.93±0.50 & 29.20±0.30 & 21.57±0.20 & - & - & - & - \\ 
% \cline{2-10}
  &  GNN~\cite{gnn} & \textbf{64.48±1.08} & \textbf{63.69±1.03} & \textbf{32.02±0.66} & \textbf{22.00±0.46} & \textbf{35.60±0.56} & \textbf{53.10±0.80} & \textbf{31.79±0.51} & \textbf{45.69±0.68}  \\ 
 \hline
\multirow{6}*{5}  & Fine-tuning & \textbf{90.64±0.54} & 81.76±0.48 &  \textbf{49.68±0.36} & \textbf{26.09±0.96} & \textbf{47.40±0.36} & 66.47±0.41 & 38.91±0.38 &  58.75±0.36   \\ 
% \cline{2-10}
  & MatchingNet & 66.39±0.78 & 64.45±0.63 & 36.74±0.53 & 22.40±0.70 & 46.53±0.68 & 63.16±0.77 & 38.99±0.64 & 51.37±0.77   \\ 
% \cline{2-10}
  &  MAML & 78.05±0.68 & 71.70±0.72 & 40.13±0.58 & 23.48±0.96 & - & - & - & 47.20±1.10   \\ 
% \cline{2-10}
  &  RelationNet & 68.99±0.75 & 61.31±0.72 & 39.41±0.58 & 22.96±0.88 & 44.00±0.60 & 63.32±0.76 & 37.33±0.68 & 57.77±0.69  \\ 
% \cline{2-10}
  &  ProtoNet &  79.72±0.67 & 73.29±0.71 & 39.57±0.57 & 24.05±1.01 & - & - & - & \textbf{67.00±1.00}    \\ 
% \cline{2-10}
  &  GNN & 87.96±0.67 & \textbf{83.64±0.77} & 43.94±0.67 & 25.27±0.46 & 52.53±0.59 & \textbf{70.84±0.65} & \textbf{44.28±0.63} & 62.25±0.65    \\ 
 \hline
\multirow{5}*{20}  & Fine-tuning & \textbf{95.91±0.72} & \textbf{87.97±0.42} & \textbf{61.09±0.44} & \textbf{31.01±0.59} & - & - & - & - \\ 
% \cline{2-10}
  & MatchingNet & 76.38±0.67 & 77.10±0.57 &  45.72±0.53 & 23.61±0.86 & - & - & - & -  \\ 
% \cline{2-10}
  &  MAML & 89.75±0.42 & 81.95±0.55 & 52.36±0.57 & 27.53±0.43 & - & - & - & -    \\ 
% \cline{2-10}
  &  RelationNet & 80.45±0.64 & 74.43±0.66 & 41.77±0.49 &  26.63±0.92 & - & - & - & -  \\ 
% \cline{2-10}
  &  ProtoNet  &  88.15±0.51 & 82.27±0.57 & 49.50±0.55 & 28.21±1.15 & - & - & - & -    \\ 
% \cline{2-10}
%  &  GNN  & - & - & - & - & - & - & - & -  \\ 
 \hline
\multirow{4}*{50}  & Fine-tuning & \textbf{97.48±0.56} & \textbf{92.00±0.56} & \textbf{67.20±0.59} & \textbf{36.79±0.53} & - & - & - & -  \\ 
% \cline{2-10}
  & MatchingNet & 58.53±0.73 & 54.44±0.67 &  54.58±0.65 & 22.12±0.88 & - & - & - & -   \\ 
% \cline{2-10}
%  &  MAML & - & - & - & - & - & - & - & -    \\ 
% \cline{2-10}
  &  RelationNet & 85.08±0.53 & 74.91±0.58 & 49.32±0.51 & 28.45±1.20 & - & - & - & -   \\ 
% \cline{2-10}
  &  ProtoNet & 90.81±0.43 & 80.48±0.57 & 51.99±0.52 & 29.32±1.12 & - & - & - & -    \\
% \cline{2-10}
%  &  GNN & - & - & - & - & - & - & - & -  \\ 
 \hline
\end{tabular}
\vspace{-0.3cm}
\label{fsl_mtd}
\end{table}

Besides, due to the current CDFSL approaches having various implementation requirements (specific datasets, various backbone, etc.) and configurations (training sets, learning paradigms, modules, etc.), it is impractical to compare all proposed CDFSL methods in a unified and fair manner. However, it is still important to gather and present the key details of some representative CDFSL methods, including their requirements, configurations, and performance highlights. To this end, we summarize in Table~\ref{repre_mtd} the performance of selected CDFSL approaches evaluated on the commonly used benchmarks, FGCB and BSCD-FSL. The optimal results for 1-shot and 5-shot are highlighted in blue and red, respectively. A comparison of the state-of-the-art performance of different method categories reveals that increasing the diversity of instances to increase the amount of shared knowledge is more effective than other methods that aim to mine existing shared knowledge. Hybrid methods perform best in FGCB, leveraging the benefits of multiple strategies in the context of near-domain transfer. Another promising direction in CDFSL is the integration of plug-and-play modules into existing FSL models, such as MatchingNet, RelationNet, and GNN, as illustrated in Figure~\ref{module-comp}. Recent results indicate that these modules perform best when applied to GNN, highlighting GNN's superior ability to handle CDFSL tasks compared to MatchingNet and RelationNet.
 \begin{sidewaystable}%[ph!]
  \tiny
 \vspace{5.45in}
 \begin{center}
     \caption{The CDFSL performance of the proposed methods on BSCD-FSL and FGCB benchmarks. $K$ means $5$-way $K$-shot. ‘KBS’ is Knowledge-Based Systems.}
     \vspace{-0.3cm}
% \centering
\setlength{\tabcolsep}{1.0mm}{
\begin{tabular} {c|lcccccccccccc|m{4cm}}
\hline 
\textbf{Type} & \textbf{Methods} & \textbf{Venue} & \textbf{Train set} & \textbf{Backbone} & \textbf{$K$} & \textbf{CropDiseases} & \textbf{EuroSAT} & \textbf{ISIC} & \textbf{ChestX} & \textbf{Plantae} & \textbf{Places} & \textbf{Cars} & \textbf{CUB} & \textbf{Highlight}  \\ 
 \hline
\multirow{13}*{\rotatebox{90}{Instance-guided}} & \multirow{3}*{NSAE~\cite{boosting}} & \multirow{3}*{ICCV} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 &  93.31±0.42 & 84.33±0.55 & \color{red}{\textbf{55.27±0.62}} & 27.30±0.42 & 62.15±0.77 & 73.17±0.72 & 58.30±0.75 & 71.92±0.77 & \multirow{3}{4.1cm}{The latent noise information from the source domain is utilized to capture broader variations of the feature distributions.}
\\
& & &  &  & 20 &  98.33±0.18 &  92.34±0.35 & 67.28±0.61 & 35.70±0.47 & 77.40±0.65 & 82.50±0.59 & 82.32±0.50 & 88.09±0.48 &    \\
% \cline{6-14}
& & & &  & 50 &  99.29±0.14 & 95.00±0.26 & 72.90±0.55 & 38.52±0.71 & 83.63±0.60 & 85.92±0.56 & - & 91.00±0.79 &   \\
\cline{2-15}
&  \multirow{2}*{CosML~\cite{data_multi_1}} & \multirow{2}*{arXiv} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ Cars / Places}} & \multirow{2}*{Conv-4} & 1 & - & - & - & - & 30.93±0.46 & 53.96±0.62 & 47.74±0.59 & 46.89±0.59 & \multirow{2}{4.1cm}{Exploring multi-domain pre-train schemes to quickly adapt the model to unseen domains} \\
% \cline{6-14}
& & & &  & 5 & - & - & - & - & 42.96±0.57 & 88.08±0.46 & 60.17±0.63 & 66.15±0.63 & \\
\cline{2-15}
& \multirow{2}*{ISSNet~\cite{data_multi_3}} & \multirow{2}*{arXiv} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ other 7 datasets}} & \multirow{2}*{ResNet10} & 1 & 73.40±0.86 & 64.50±0.88 & \color{blue}{\textbf{36.06±0.69}} & 23.23±0.42 & - & - & - & - & \multirow{2}{4.1cm}{Transferring styles across multiple sources to broaden the distribution of labeled sources} \\
% \cline{6-14}
&  &  &  &  & 5 & 94.10±0.41 & 83.64±0.55 & 51.82±0.67 &  \color{red}{\textbf{28.79±0.48}} &  -  & - & - & - &   \\
\cline{2-15}
& \multirow{2}*{DSL~\cite{data_target_1}} & \multirow{2}*{ICLR} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 41.17±0.80 & 53.16±0.88 & 37.13±0.69 & 50.15±0.80 & \multirow{2}{4.1cm}{Incorporating the cross-domain scenario into the training stage by rapidly switching targets}  \\
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 62.10±0.75 & 74.10±0.72 & 58.53±0.73 & 73.57±0.65 &   \\
 \cline{2-15}
& \multirow{2}*{STARTUP~\cite{st}} & \multirow{2}*{ICLR} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & 75.93±0.80 & 63.88±0.84 & 32.66±0.60 & 23.09±0.43 & - & - & - & - & \multirow{2}{4.1cm}{Self-training a source representation using unlabeled data from the target domain}  \\ % to help the model adapt to the target domain
% \cline{6-14}
&  &  &  &  & 5 & 93.02±0.45 &  82.29±0.60 & 47.22±0.61 & 26.94±0.44 & - & - & - & - &   \\
\cline{2-15}
& \multirow{2}*{DDA~\cite{dynamic}} & \multirow{2}*{NIPS} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & 82.14±0.78 & \color{blue}{\textbf{73.14±0.84}} & 34.66±0.58 & \color{blue}{\textbf{23.38±0.43}} & - & - & - & - & \multirow{2}{4.1cm}{Propose a dynamic distillation-based approach to enhance utilize unlabeled target data} \\
% \cline{6-14}
&  &  &  &  & 5 & 95.54±0.38 & \color{red}{\textbf{89.07±0.47}} & 49.36±0.59 & 28.31±0.46 & - & - & - & - &    \\
\hline
% \hline
\multirow{18}*{\rotatebox{90}{Parameter-based}} & \multirow{3}*{SB-MTL~\cite{parameter_fix_1}} & \multirow{3}*{arXiv} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 & \color{red}{\textbf{96.01±0.40}} & 87.30±0.68 &  53.50±0.79 &  28.08±0.50 & - & - & - & - & \multirow{3}{4.1cm}{Leveraging a first-order MAML algorithm to identify optimal initializations and employing a score-based GNN for prediction}    \\
% \cline{6-14}
& &  &  &  & 20 &  99.61±0.09 &  96.53±0.28 & 70.31±0.72 & 37.70±0.57 & - & - & - & - &  \\
% \cline{6-14}
& &  &  &  & 50 & 99.85±0.06 & 98.37±0.18 & 78.41±0.66 & 43.04±0.66 & - & - & - & - &   \\
\cline{2-15}
&  \multirow{8}*{VDB~\cite{parameter_fix_6}} & \multirow{8}*{CVPRW} & \multirow{8}*{\textit{mini}ImageNet} & \multirow{4}*{ResNet10} & 1 &  71.98±0.82 &  63.60±0.87 & 35.32±0.65 &  22.99±0.44 & - & - & - & - & \multirow{8}{4.1cm}{Propose a source-free approach through the introduction of the "Visual Domain Bridge" concept, aimed at mitigating internal mismatches in BatchNorm during cross-domain settings} \\ 
% \cline{6-14}
&  &  &  &  & 5  &  90.77±0.49 & 82.06±0.63 & 48.72±0.65 & 26.62±0.45 &   & - & - & - &   \\
% \cline{6-14}
&  &  &  &  & 20 &  96.36±0.27 & 89.42±0.45 & 59.09±0.59 & 31.87±0.44 & - & - & - & - &    \\
% \cline{6-14}
&  &  &  &  & 50 & 97.89±0.19 & 92.24±0.35 & 64.02±0.58 & 35.55±0.45 & - & - & - & - &    \\
\cline{5-14}
&  &  &  & \multirow{4}*{ResNet18} & 1 & 75.46±0.76 & 67.76±0.83 &  33.22±0.58 &  22.28±0.41 & - & - & - & - &  \\  
% \cline{6-14}
& &  &  &  & 5 &  93.11±0.42 & 85.29±0.52 & 47.48±0.61 & 25.25±0.42 & - & - & - & - & \\
% \cline{6-14}
&  &  &  &  & 20 & 97.61±0.21 & 91.93±0.37 & 58.89±0.59 & 29.49±0.42 & - & - & - & - &   \\
% \cline{6-14}
&  &  &  &  & 50 &  98.40±0.16 & 93.95±0.30 &  64.23±0.58 & 32.37±0.47 & - & - & - & - &   \\
\cline{2-15}

& \multirow{2}*{FGNN~\cite{parameter_fix_7}} & \multirow{2}*{KBS} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & -  & - & 41.44±0.69 & 56.74±0.82 & 34.37±0.60 & 52.97±0.75 & \multirow{2}{4.1cm}{Investigating instance normalization and the restitution module to enhance performance} \\   % Exploring an instance normalization algorithm to alleviate feature \\ dissimilarity, and a restitution module to restitute the discrimination \\ ability from the learned knowledge
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 60.81±0.66 & 76.12±0.63 & 50.19±0.69 & 71.99±0.64 &   \\
\cline{2-15}

& \multirow{2}*{MAP~\cite{parameter_select_2}} & \multirow{2}*{arXiv} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 5 & 90.29±1.56 & 82.76±2.00 &  47.85±1.95 &  24.79±1.22 & 58.45±1.15 & 75.94±0.97 & 51.64±1.16 & 67.92±1.10 & \multirow{2}{4.1cm}{Selectively performs SOTA adaptation methods in sequence with modular adaptation method} \\  % A modular adaptation method is proposed to selectively performs multiple state-of-the-art (SOTA) adaptation methods in sequence
% \cline{6-14}
&  &  &  &  & 20 & 95.22±1.13 & 88.11±1.78 &  60.16±2.70 & 30.21±1.78 & - & - & - & - &   \\
\cline{2-15}
& HVM~\cite{parameter_weight_2} & ICLR & \makecell[c]{\textit{mini}ImageNet} & ResNet10 & 5 & 87.65±0.35 & 74.88±0.45 & 42.05±0.34 & 27.15±0.45 & - & - & - & - & Introducing a hierarchical prototype model and a hierarchical alternative to address domain gaps by flexibly utilizing features at varying semantic levels  \\
 \cline{2-15}
& \multirow{2}*{ReFine~\cite{parameter_weight_1}} & \multirow{2}*{ICMLW} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 68.93±0.84 & 64.14±0.82 & 35.30±0.59 & 22.48±0.41 & - & - & - & - & \multirow{2}{4.1cm}{Randomizing the fitted parameters from the source domain before adapting to target data}  \\ 
% \cline{6-14}
&  &  &  &  & 5 & 90.75±0.49 & 82.36±0.57 & 51.68±0.63 & 26.76±0.42 & - & - & - & - &   \\
\hline % \hline
\multirow{14}*{\rotatebox{90}{Feature post-processing}} & \multirow{3}*{CHEF~\cite{rf}} & \multirow{3}*{arXiv} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet18} & 5 & 86.87±0.27 & 74.15±0.27 & 41.26±0.34 & 24.72±0.14 & - & - & - & - & \multirow{3}{4.1cm}{Ensembling representation fusion through Hebbian learners operating on different layers of the network}    \\
% \cline{4-12}
%  &  &  & 5 &  96.09±0.35 & 87.53±0.50 & 56.85±0.67 & 28.73±0.45 & 65.66±0.78   & 73.40±0.71 & 61.11±0.79 & 76.00±0.71 &  \\
% \cline{6-14}
& &  &  &  & 20 & 94.78±0.12 & 83.31±0.14 & 54.30±0.34 & 29.71±0.27 & - & - & - & - &  \\
% \cline{6-14}
& &  &  &  & 50 & 96.77±0.08 & 86.55±0.15 &  60.86±0.18 & 31.25±0.20 & - & - & - & - &   \\
\cline{2-15}
&  \multirow{2}*{LRP~\cite{feature_reweight_1}} & \multirow{2}*{ICPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 34.80±0.37 &  50.59±0.46 & 29.65±0.33 & 42.44±0.41 & \multirow{2}{4.1cm}{A training strategy guided by explanations is developed to identify important features} \\  
% \cline{6-14}
& &  &  &  & 5 & - & - & - & - & 48.09±0.35 & 66.90±0.40 & 39.19±0.38 & 59.30±0.40 & \\
\cline{2-15}
& \multirow{3}*{Confess~\cite{confess}} & \multirow{3}*{ICLR} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 & 88.88±0.51 & 84.65±0.38 & 48.85±0.29 & 27.09±0.24 & - & - & - & - & \multirow{3}{4.1cm}{Investigating a contrastive learning and feature selection system to address domain gaps between base and novel categories} \\
% \cline{6-14}
&  &  &  &  & 20 & 95.34±0.48 & 90.40±0.24 & 60.10±0.33 & 33.57±0.31 & - & - & - & - &   \\
% \cline{6-14}
 &  &  &  &  & 50 & 97.56±0.43 & 92.66±0.36 & 65.34±0.45 & 39.02±0.12 & - & - & - & - &    \\
% \cline{4-9}
% &  &  & 50 &  &  &  &  &    &&&   \\
\cline{2-15}
& BL-ES~\cite{feature_reweight_2} & ICME & \makecell[c]{\textit{mini}ImageNet}  & ResNet18 & 5 & - & 79.78±0.83 & - & - & - & - & 50.07±0.84 & 69.63±0.88 & Proposing a bilevel episode strategy to train an inductive graph network of learning comparison and induction simultaneously  \\
 \cline{2-15}
& \multirow{3}*{TACDFSL~\cite{feature_reweight_4}} & \multirow{3}*{Symmetry} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{WideResNet} & 5 & 93.42±0.55 & 85.19±0.67 & 45.39±0.67 & 25.32±0.48 & - & - & - & - & \multirow{3}{4.1cm}{Introducing the empirical marginal distribution measurement}  \\
% \cline{6-14}
&  &  &  &  & 20 & 95.49±0.39 & 87.87±0.49 & 53.15±0.59 & 29.17±0.52 & - & - & - & - &   \\
% \cline{6-14}
 &  &  &  &  & 50 & 95.88±0.35 & 89.07±0.43 &  56.68±0.58 & 31.75±0.51 & - & - & - & - &   \\
% \cline{5-13}
% &  &  &  & 50 &  &  &  &  &     &&&   \\ 
\cline{2-15}
& \multirow{2}*{RDC~\cite{feature_reweight_6}} & \multirow{2}*{CVPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & \color{blue}{\textbf{86.33±0.50}} & 71.57±0.50 & 35.84±0.40 & 22.27±0.20 & 44.33±0.60 & 61.50±0.60 & 39.13±0.50 & 51.20±0.50 & \multirow{2}{4.1cm}{Minimising task-irrelevant features by constructing subspace} \\ % Constructing a non-linear subspace to minimise task-irrelevant features and keep more discriminative information by a hyperbolic tangent transformation
% \cline{6-14}
&  &  &  &  & 5 & 93.55±0.30 & 84.67±0.30 & 49.06±0.30 & 25.48±0.20 & 60.63±0.40 & 74.65±0.40 & 53.75±0.50 & 67.77±0.40 &    \\
\hline % \hline
\multirow{8}*{\rotatebox{90}{Hybrid}} & \multirow{2}*{FDMixup~\cite{hybrid_1}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 66.23±1.03 & 62.97±1.01 & 32.48±0.64 & 22.26±0.45 & 37.89±0.58 & 53.57±0.75 & 31.14±0.51 & 46.38±0.68 & \multirow{2}{4.1cm}{Utilizing few labeled target data to guide the model learning} \\ 
% \cline{6-14}
&  &  &  &  & 5 & 87.27±0.69 & 80.48±0.79 & 44.28±0.66 & 24.52±0.44 & 54.62±0.66 & 73.42±0.65 & 41.30±0.58 & 64.71±0.68 &    \\
 \cline{2-15} 
% &  &  &  & 20 &  &  &  &  &    &&&   \\
% \cline{5-13}
% &  &  &  & 50 &  &  &  &  &     &&&   \\
 & \multirow{2}*{TL-SS~\cite{hybrid_3}} & \multirow{2}*{AAAI} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - &  65.73 & - & - & - & 55.83 & 33.22 & 45.92 & \multirow{2}{4.1cm}{Introducing a domain-irrelevant self-supervised learning method} \\ % A domain-independent task-level self-supervised method is proposed to improve the model generalization ability
% \cline{6-14}
&  &  &  &  & 5 & - & 79.36 & - & - & - &  76.33 & 49.82 & 69.16 &    \\
% \cline{5-13}
\cline{2-15}
& \multirow{2}*{TGDM~\cite{tgdm}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 52.39±0.25 & \color{blue}{\textbf{61.88±0.26}} & \color{blue}{\textbf{50.70±0.24}} & 64.80±0.26 & \multirow{2}{4.1cm}{A method generates an intermediate domain generation to facilitate the FSL task}  \\ % Re-randomizing the parameters fitted on the source domain before adapting to the target data
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 71.78±0.22 & \color{red}{\textbf{81.62±0.19}} & \color{red}{\textbf{70.99±0.21}} & \color{red}{\textbf{84.21±0.18}} &   \\
\cline{2-15}
& \multirow{2}*{ME-D2N~\cite{data_target}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & \color{blue}{\textbf{52.89±0.83}} & 60.36±0.86 & 49.53±0.79 & \color{blue}{\textbf{65.05±0.83}} & \multirow{2}{4.1cm}{AME-D2N utilizes a multi-expert learning approach to create a model}  \\ 
&  &  &  &  & 5 & - & - & - & - & \color{red}{\textbf{72.87±0.67}} & 80.45±0.62 & 69.17±0.68 & 83.17±0.56 &   \\
\hline

\end{tabular}}
\label{repre_mtd}
 \end{center}
 
 \end{sidewaystable}
% \end{adjustbox}
% }
% \end{table}

\subsubsection{\textbf{Evaluation for Instance-guided Approaches}}
Table~\ref{repre_mtd} highlights a noticeable trend in which the performance decreases as the distance between the target domain and source domain increases. For instance, the results show a drop from 93.31\% on CropDiseases to 27.30\% on ChestX (5-way 5-shot). A comparison between~\cite{data_multi_1} and~\cite{data_target_1} also reveals that the former outperforms the latter on places (88.08\%) and cars (60.17\%) but underperforms on the other two datasets (42.96\% and 66.15\% vs. 62.10\% and 73.57\%). This discrepancy can be attributed to the difference in training data, as the former incorporates places and cars into the training process leading to overfitting on these two datasets. On the other hand, the results of~\cite{st} and~\cite{dynamic} on BSCD-FSL demonstrate that incorporating target domain data into the training process can improve the performance on the target domain. However, this approach works better for near-domain transfer than for distance-domain transfer. For example,~\cite{dynamic} showed a 4.90\% improvement on CropDiseases and 7.31\% improvement on EuroSAT but a 0.32\% drop on ISIC and only a 2.22\% improvement on ChestX when compared to the classic fine-tuning method.

Instance-guided approaches for CDFSL are relatively simple in concept as they rely on adding supplementary information to enhance the model's generalization. However, their effectiveness is highly dependent on the choice of information used in the training process. If the additional domains included in training greatly diverge from the target domain or the selected target domain samples are not representative, this can negatively affect CDFSL performance.
\begin{figure}%[b]
	\centering
	\includegraphics[width=\linewidth]{compare-2.pdf}
 \vspace{-0.5cm}
	\caption{\textcolor{black}{1-shot (entity area) and 5-shot (slashed area) performance comparison of methods that propose a novel module. All methods use ResNet10 as the backbone. ``CropD" is dataset ``CropDiseases".}}
 \vspace{-0.4cm}
	\label{module-comp}
\end{figure}

\subsubsection{\textbf{Evaluation for Parameter-based Approaches}}
From the data presented in Table~\ref{repre_mtd}, it appears that the performance of parameter-based methods is generally subpar in comparison to the other two method types. Using ResNet10 as the backbone, the results of~\cite{parameter_fix_1} on BSCD-FSL (5-way 5-shot) demonstrate this trend, with scores of 96.01\% (CropDiseases), 87.30\% (EuroSAT), 53.50\% (ISIC), and 28.08\% (ChestX). The results of other methods within this category are even lower. When comparing the use of ResNet10 (90.77\% of CropDiseases, 82.06\% of EuroSAT, 48.72\% of ISIC, 26.62\% of ChestX) and ResNet18 (93.11\% of CropDiseases, 85.29\% of EuroSAT, 47.48\% of ISIC, 25.25\% of ChestX) as the backbone for ~\cite{parameter_fix_6} on BSCD-FSL, it is observed that while increasing the depth of the network enhances performance on near-domain datasets (CropDiseases, EuroSAT), it deteriorates performance on distant-domain datasets (ISIC, ChestX). As such, the best balance of near-domain and distant-domain performance is achieved when using ResNet10 as the backbone.

Our analysis of Table~\ref{repre_mtd} reveals that the performance of the parameter-based methods tends to be subpar in comparison to the first two categories of methods. The reason behind this is thought to be the local adjustment of network parameters by these methods through the use of a module to fit the new domain. Although this reduction of hypothesis space may appear advantageous, it actually limits the adaptation of the method to data distribution and hypothesis space due to the limited introduction of additional parameters. Therefore, parameter-based methods in CDFSL often face limitations in augmenting and mining shared knowledge, which makes it more challenging to solve the two-stage empirical risk minimization problem compared to other categories of methods. Thus, researchers need to explore new methods and techniques that can overcome these limitations and improve the performance of parameter-based methods in CDFSL.

\subsubsection{\textbf{Evaluation for Feature Post-processing Approaches}}
Despite the challenge of directly comparing the performance of different feature post-processing methods due to the utilization of various backbones, it can still be noted that the performance of these approaches on distant-domain tasks may fall short in comparison to instance-guided methods. This was exemplified by comparing two representative approaches: the results of~\cite{feature_reweight_6} on BSCD-FSL (93.55\% for CropDiseases, 84.67\% for EuroSAT, 49.06\% for ISIC, and 25.48\% for ChestX) were not as good as those of~\cite{boosting} on the same benchmark (93.31\% for CropDiseases, 84.33\% for EuroSAT, 55.27\% for ISIC, and 27.30\% for ChestX). This trend was also reflected in the results on FGCB (62.15\%, 73.17\%, 58.30\%, and 71.92\% vs. 60.63\%, 74.65\%, 53.75\%, and 67.77\%). These observations suggest that while feature post-processing methods can still bring some improvement, they may not be as effective as instance-guided approaches in addressing the CDFSL problem.

The comparison of the results of instance-guided and feature post-processing methods reveals a difference in their approach to uncovering shared knowledge between the source and target domains. Instance-guided methods prioritize the introduction of additional information during the training phase, effectively creating a more favorable shared feature extraction environment. On the other hand, feature post-processing methods aim to maximize the utilization of the limited shared knowledge available, which is a more restrictive approach.

\subsubsection{\textbf{Evaluation for Hybrid Approaches}}
Currently, there is a limited number of studies that explore hybrid methods in the context of CDFSL, however, our analysis of these works reveals that the performance of these hybrid methods in FGCB and BSCD-FSL is comparable to that of other methods. For instance, a study conducted in~\cite{hybrid_3} produced results of 76.33\%, 49.82\%, and 69.16\% on the Places, Cars, and CUB datasets, which are similar to the results of~\cite{parameter_select_2} which produced 75.94\%, 51.64\%, and 67.92\% on the same datasets. It is important to note that combining strategies from different categories of methods carries a degree of risk, as there may be negative interactions between the different strategies. This highlights the high degree of precision required when matching strategies in hybrid methods. Ultimately, the choice of which approach to use depends on the specific task and available data, as well as the desired level of generalization and flexibility required for the model.

\subsubsection{\textbf{Evaluation on Meta-Dataset}}
The techniques tested on the Meta-Dataset~\cite{meta-dataset} utilize non-episodic training, and the evaluation results are presented in Table~\ref{meta_per}. The evaluation is conducted using two setups: single-source-based (where the source domain is ImageNet) and multiple-sources-based (where the source domains are the first eight datasets). In the single source-based setting, ProtoNet, MAML, and Pro-MAML, serve as baselines to compare with the proposed approaches. The results reveal that~\cite{feature_select_2} achieved the best results on five target datasets, while~\cite{parameter_weight_4} attained the best results on the remaining five target datasets. Additionally, the findings indicate that deeper backbone networks, such as ResNet34 in~\cite{parameter_weight_4}, tend to outperform shallower ones, like ResNet18. The training results of multiple sources-based models illustrate that~\cite{parameter_weight_4} achieved the highest performance on all target datasets. This is believed to be due to the technique's effective combination of multiple sources and scientifically designed parameter reweighting strategy. A comparison of the two setups of~\cite{parameter_weight_4} using ResNet18 shows that the incorporation of multiple datasets results in significant performance improvements on eight seen datasets, but only a modest improvement on one unseen dataset. This indicates that introducing multiple domains without careful consideration may not necessarily enhance performance significantly. In conclusion, the proposed methods significantly enhance CDFSL performance relative to traditional FSL techniques, demonstrating the effectiveness of these methods in solving the CDFSL problem.
\begin{table}%[b]
\tiny
\centering
\vspace{-0.3cm}
\caption{The CDFSL performance of approaches on Meta-Dataset. $\star$ means the results on the seen data set (source data set).}
\vspace{-0.3cm}
\setlength{\tabcolsep}{0.9mm}{
\begin{tabular}{lccccccc|ccc}
\hline
 & \multicolumn{7}{c|}{\textbf{Single source}} & \multicolumn{3}{c}{\textbf{Multiple sources}}    \\ 
\cline{2-11}
 & ProtoNet~\cite{proto} & MAML~\cite{al21} & Pro-MAML~\cite{meta-dataset} & SUR~\cite{feature_select_2} & \multicolumn{2}{c}{TPA~\cite{parameter_weight_4}} & tri-M~\cite{hybrid_2} & RMFS~\cite{feature_select_1} & TPA~\cite{parameter_weight_4} & URL~\cite{hybrid_4}   \\ 
\hline
Backbone & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet34 & ResNet18 & ResNet18 & ResNet18 & ResNet18   \\ 
\hline
ImageNet & {44.5 $\pm$ 1.1}{$\star$} & ${32.4\pm 1.0}{\star}$ & ${47.9\pm 1.1}{\star}$ & ${57.2\pm 1.1}{\star}$ & \textbf{59.5 $\pm$ 1.1}{$\star$} & ${63.7\pm 1.0}{\star}$ & ${58.6\pm 1.0}{\star}$ & \textbf{63.1 $\pm$ 0.8}{$\star$} & ${59.5\pm 1.0}{\star}$ & ${58.8\pm 1.1}{\star}$   \\ 
\cline{2-8}
Omniglot & $79.6\pm 1.1$ & $71.9\pm 1.2$ & $82.9\pm 0.9$ & \textbf{93.2 $\pm$ 0.8} & $78.2\pm 1.2$ & $82.6\pm 1.1$ & $92.0\pm 0.6$ & \textbf{97.7 $\pm$ 0.5}{$\star$} & ${94.9\pm 0.4}{\star}$ & ${94.5\pm 0.4}{\star}$   \\ 
Aircraft & $71.1\pm 0.9$ & $52.8\pm 0.9$ & $74.2\pm 0.8$ & \textbf{90.1 $\pm$ 0.8} & $72.2\pm 1.0$ & $80.1\pm 1.0$ & $82.8\pm 0.7$ & ${65.1\pm 0.3}{\star}$ & \textbf{89.9 $\pm$ 0.4}{$\star$} & ${89.4\pm 0.4}{\star}$   \\ 

Birds & $67.0\pm 1.0$ & $47.2\pm 1.1$ & $70.0\pm 1.0$ & \textbf{82.3 $\pm$ 0.8} & $74.9\pm 0.9$ & 83.4$\pm$0.8 & $75.3\pm 0.8$ & \textbf{84.1 $\pm$ 0.6}{$\star$} & ${81.1\pm 0.8}{\star}$ & ${80.7\pm 0.8}{\star}$   \\ 

Textures & $65.2\pm 0.8$ & $56.7\pm 0.7$ & $67.9\pm 0.8$ & $73.5\pm 0.7$ & \textbf{77.3 $\pm$ 0.7} & 79.6$\pm$0.7 & $71.2\pm 0.8$ & ${67.5\pm 0.9}{\star}$ & \textbf{77.5 $\pm$ 0.7}{$\star$} & ${77.2\pm 0.7}{\star}$   \\ 

Quick Draw & $65.9\pm 0.9$ & $50.5\pm 1.2$ & $66.6\pm 0.9$ & \textbf{81.9 $\pm$ 1.0} & $67.6\pm 0.9$ & $71.0\pm 0.8$ & $77.3\pm 0.7$ & \textbf{86.2 $\pm$ 0.5}{$\star$} & ${81.7\pm 0.6}{\star}$ & ${82.5\pm 0.6}{\star}$   \\ 

Fungi & $40.3\pm 1.1$ & $21.0\pm 1.0$ & $42.0\pm 1.1$ & \textbf{67.9 $\pm$ 0.9} & $44.7\pm 1.0$ & $51.4\pm 1.2$ & $48.5\pm 1.0$ & ${62.5\pm 0.6}{\star}$ & ${66.3\pm 0.8}{\star}$ & \textbf{68.1 $\pm$ 0.9}{$\star$}   \\ 

VGG Flower & $86.9\pm 0.7$ & $70.9\pm 1.0$ & $88.5\pm 1.0$ & $88.4\pm 0.9$ & \textbf{90.9 $\pm$ 0.6} & 94.0 $\pm$ 0.5 & $90.5\pm 0.5$ & ${86.3\pm 0.3}{\star}$ & \textbf{92.2 $\pm$ 0.5}{$\star$} & ${92.0\pm 0.5}{\star}$   \\ 
\cline{9-11}
Traffic Sign & $46.5\pm 1.0$ & $34.2\pm 1.3$ & $34.2\pm 1.3$ & $67.4\pm 0.8$ & \textbf{82.5 $\pm$ 0.8} & $81.7\pm 0.9$ & $78.0\pm 0.6$ & $73.7\pm 0.4$ & \textbf{82.8 $\pm$ 1.0} & $63.3\pm 1.2$   \\ 

MSCOCO & $39.9\pm 1.1$ & $24.1\pm 1.1$ & $24.1\pm 1.1$ & $51.3\pm 1.0$ & \textbf{59.0 $\pm$ 1.0} & $61.7\pm 0.9$ & $52.8\pm 1.1$ & $56.2\pm 0.7$ & \textbf{57.6 $\pm$ 1.0} & $57.3\pm 1.0$   \\ 

MNIST & - & - & - & $90.8\pm 0.5$ & $93.9\pm 0.6$ & $94.6\pm 0.5$ & \textbf{96.2 $\pm$ 0.3} & - & \textbf{96.7 $\pm$ 0.4} & $94.7\pm 0.4$   \\ 

CIFAR 10 & - & - & - & $66.6\pm 0.8$ & \textbf{82.1 $\pm$ 0.7} & $86.0\pm 0.6$ & $75.4\pm 0.8$ & - & \textbf{82.9 $\pm$ 0.7} & $74.2\pm 0.8$   \\ 

CIFAR 100 & - & - & - & $58.3\pm 1.0$ & \textbf{70.7 $\pm$ 0.9} & $78.3\pm 0.8$ & $62.0\pm 1.0$ & - & \textbf{70.4 $\pm$ 0.9} & $63.6\pm 1.0$   \\ 
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{meta_per}
\end{table}