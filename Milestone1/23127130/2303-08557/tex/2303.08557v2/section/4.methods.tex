\section{Approaches} \label{methods}
% 在这里描述cdfsl标准的执行过程
% CDFSL offers a unified solution to both cross-domain and FSL problems. \textcolor{black}{Algorithm \ref{alg:baseline} outlines the CDFSL baseline process (contents without colorful boxes), consisting of two key steps: (1) Training a feature extractor on the source domain, and (2) Performing FSL on the target domain. The first step provides prior knowledge from the source domain, optimized via transfer learning~\cite{tfsurvey}, meta-learning~\cite{hospedales2021meta}, and metric learning~\cite{kaya2019deep} techniques. The second step focuses on learning the feature extractor and target classifier with limited supervision.} Based on the unique challenges, we categorize CDFSL algorithms into four types: \textcolor{black}{$\mathcal{D}$-Extension, $\mathcal{H}$-Constraint, $\Delta$-Adaptation, and Hybrid Methods. In Algorithm \ref{alg:baseline}, the key steps of $\mathcal{D}$-Extension, $\mathcal{H}$-Constraint, and $\Delta$-Adaptation are highlighted with red, green, and blue boxes, respectively.}
CDFSL offers a unified solution to both cross-domain and FSL problems. \textcolor{black}{Figure \ref{overview} outlines the CDFSL baseline process, consisting of two key steps: (1) Training a feature extractor on the source domain, and (2) Performing FSL on the target domain. The first step provides prior knowledge from the source domain, optimized via transfer learning~\cite{tfsurvey}, meta-learning~\cite{hospedales2021meta}, and metric learning~\cite{kaya2019deep} \etc techniques. The second step focuses on learning the feature extractor and target classifier with limited supervision.} Based on the unique challenges, we categorize CDFSL algorithms into four types: \textcolor{black}{$\mathcal{D}$-Extension, $\mathcal{H}$-Constraint, $\Delta$-Adaptation, and Hybrid Methods.}
% Specifically, the feature extractor and the source classifier are optimized through many labeled source data. The optimization in this step can be achieved by transfer learning, meta-learning, and metric learning technologies, \etc. While in the second step, following the vanilla few-shot learning setting, the final accuracy is obtained by calculating the average accuracy of multiple episodes. In each episodes, the support set and query set are first selected from the target data. The support set includes $C \times K$ samples, where $C$ means the number of classes and $K$ illustrates the samples in each category. And all the samples in query set are from the rest examples in $C$ categories. Then, the feature extractor and the target classifier are finetuned through the support set.


 % Based on the analysis of unique issue and challenges, \textcolor{black}{we present a classification criterion for CDFSL algorithms, dividing them into four categories: $\mathcal{D}$-Extension, $\mathcal{H}$-Constraint, $\Delta$-Adaptation, and Hybrid Methods.} % The overview of CDFSL is depicted in Figure~\ref{overview} (a).
 \iffalse
\begin{algorithm}
\caption{\textcolor{black}{Cross-domain Few-shot Learning (CDFSL) Baseline}}
\label{alg:baseline}
\begin{algorithmic} 
    \State \textbf{Input:} Source domain data $\mathcal{D}^{s}$ (with many labeled examples), Target domain data $\mathcal{D}^{t}$ (with few labeled examples), Initialized feature extractor $f$, Source classifier $\phi_{s}$, Target classifier $\phi_{t}$

    \State \textbf{Output:} Optimized feature extractor $f$ and target classifier $\phi_{t}$
    
    \State \textbf{Step 1: Train feature extractor on source domain}
    \State \fcolorbox{red}{white}{Augment $\mathcal{D}^{s}$}
    \State {\textbf{While} \textit{max epochs not reached} \textbf{do}}
    \State {\hspace{0.5cm} \textbf{for} each mini-batch \{$x_i$,$y_i$\} in $\mathcal{D}^{s}$ \textbf{do}} %\textcolor{red}{\Comment{$\mathcal{D}$-Extension: Augment and extend $\mathcal{D}^{s}$}}
    \State {\hspace{1.0cm} Input $x_i$ into $f$ to get the source features $\textbf{f}_i \gets f(x_i)$ 
    } %\newline \textcolor{green}{\Comment{$\mathcal{H}$-Constraint: Constrain hypothesis space $\mathcal{H}$ with strategies}}
    \State {\hspace{1.0cm} \fcolorbox{red}{white}{Augment $\textbf{f}_i$ } \fcolorbox{green}{white}{Constrain $f(\cdot)$ } \fcolorbox{blue}{white}{Adapt $\textbf{f}_i$ and $\textbf{f}_s$}}
    \State {\hspace{1.0cm} Input $\textbf{f}_{i}$ into $\phi_{s}$ to get the source predictions $\textbf{p}_i \gets \phi_{s}(\textbf{f}_{i})$ 
    }
    \State {\hspace{1.0cm} Compute the loss function between $\textbf{p}_i$ and $y_{i}$, update $f$ and $\phi_{s}$ 
    }  %\textcolor{black}{\Comment{$\Delta$-Adaptation: Adapt the distributions}}
    \State {\hspace{1.0cm} \fcolorbox{green}{white}{Constrain $\phi_{s} \circ f(\cdot)$ }}
    \State Test on the test data from the source domain, and get the pre-trained $f_{pre}$ 

    \State \textbf{Step 2: Few-shot learning on target domain}
    \State {\textbf{While} \textit{max episodes not reached} \textbf{do}}
    \State {\hspace{0.5cm} Randomly select $C$ categories and $K$ samples in each category ($C$-way $K$-shot) from $\mathcal{D}^{t}$} 
    \State {\hspace{0.5cm} Form the support set $(\mathcal{X}_s,\mathcal{Y}_s)$ with these $C \times K$ samples%, $\mathcal{X}_s$ and $\mathcal{Y}_s$ represent sample and label set in support set 
    }  %\textcolor{red}{\Comment{$\mathcal{D}$-Extension: Augment and extend $(\mathcal{X}_s,\mathcal{Y}_s)$}}
    \State {\hspace{0.5cm} \fcolorbox{red}{white}{Augment $(\mathcal{X}_s,\mathcal{Y}_s)$}}
    \State {\hspace{0.5cm} \textbf{While} \textit{max epochs not reached} \textbf{do}}
    \State {\hspace{1.0cm} Input $\{x_s,y_s\} \in (\mathcal{X}_s,\mathcal{Y}_s)$ into the pre-trained $f_{pre}$ (from step 1) to get $\textbf{f}_s \gets f_{pre}(x_s)$ } % \newline  \textcolor{green}{\Comment{Constrain hypothesis space $\mathcal{H}$ with strategies}}
    \State {\hspace{1.0cm} \fcolorbox{red}{white}{Augment $\textbf{f}_s$ } \fcolorbox{green}{white}{Constrain $f_{pre}(\cdot)$ } \fcolorbox{blue}{white}{Adapt $\textbf{f}_s$ to $\textbf{f}_i$}}
    \State {\hspace{1.0cm} Input $\textbf{f}_s$ into the initialized $\phi_{t}$ to get the target predictions $\textbf{p}_s \gets \phi_{t}(\textbf{f}_{s})$ }
    \State {\hspace{1.0cm} \fcolorbox{blue}{white}{Adapt $\textbf{p}_s$ to $\textbf{p}_i$}}
    \State {\hspace{1.0cm} Compute the loss function between $\textbf{p}_s$ and $y_{s}$, update $f_{pre}$ and $\phi_{t}$}
    \State {\hspace{1.0cm} \fcolorbox{green}{white}{Constrain $\phi_{t} \circ f_{pre}(\cdot)$ }}

    \State {\hspace{0.5cm} Randomly select the rest samples from the same $C$ categories with $(\mathcal{X}_s,\mathcal{Y}_s)$ as query set $\mathcal{X}_{q}$ }
    \State {\hspace{0.5cm} Input $x_q \in \mathcal{X}_q$ into the updated $f_{pre}$ and $\phi_{t}$ to get the $\textbf{p}_q \gets \phi_{t} \circ f_{pre} (x_q)$ }
    \State {\hspace{0.5cm} Compute the accuracy with $\textbf{p}_q$ }

    \State Calculate the mean accuracy of all the episodes
\end{algorithmic}
\end{algorithm}
\vspace{-0.3cm}
\fi


\begin{figure}%[h]
	\centering
	\includegraphics[width=11cm]{response/crop_overview1.pdf}
  \vspace{-0.3cm}
	\caption{\textcolor{black}{An overview of the CDFSL method involves two main stages. First, existing techniques pretrain the encoder on the source domain. Second, the encoder is finetuned, and a target recognizer is trained on the support set (with limited labeled data) of target domain. And inference is performed using the finetuned encoder and the target recognizer on the query set. $\textit{D}^{s}$ and $\textit{D}^{t}$ mean the source and target data.}}
  \vspace{-0.3cm}
	\label{overview}
\end{figure}


\textcolor{black}{
\subsection{$\mathcal{D}$-Extension}  \label{instance}
\textcolor{black}{$\mathcal{D}$-Extension enhances and expands existing information resources to improve the model’s generalization to new tasks, addressing the issue of unreliable TSERM. In CDFSL, information is typically represented in three dimensions: data space, feature space, and task space. Based on the dimension of information expansion, methods are classified into data augmentation, feature generation, and task synthesis. This classification demonstrates how $\mathcal{D}$-Extension systematically addresses the challenge of information scarcity across these three dimensions. Figure~\ref{fig_instance} illustrates how these methods work.}
% \textcolor{black}{$\mathcal{D}$-Extension enhances and expands existing information resources to improve the model’s ability to generalize to new tasks. It is not limited to simple data generation but spans multiple aspects, aiming to maximize the use of available resources. By focusing on different aspects, $\mathcal{D}$-Extension can be divided into three main subcategories: data augmentation, feature generation, and task synthesis. This classification systematically demonstrates how $\mathcal{D}$-Extension addresses the challenge of information scarcity across the dimensions of data, features, and tasks. It not only helps researchers and practitioners better understand and select appropriate augmentation strategies but also clarifies the specific role each method plays in enhancing model performance. Figure~\ref{fig_instance} and Table~\ref{table_instance} indicate the details of these methods.}
% This section presents approaches that extend the information of $\mathcal{D}$, including the source domain $\mathcal{D}^{s}$ and target domain $\mathcal{D}^{t}$, to improve the performance of CDFSL. Existing methods are divided into data augmentation, feature generation, and task synthesis according to the different aspects of information expansion. Specifically, data augmentation expands information at the data level, including instance augmentation and pseudo-label generation. While feature generation aims to generate new and effective features through strategies such as feature fusion or feature re-weighting. These features can fill in the gaps that the original information cannot cover, and by processing features at different levels, a more target task-friendly representation can be obtained. Furthermore, some other methods enhance the generalization ability of the model by constructing diverse tasks, especially difficult tasks. It should be noted that although task synthesis needs to be performed on the basis of data augmentation, its core idea is to increase the diversity and difficulty of tasks and thus improve the generalization of the model, rather than simply augmenting the data in a single task. These approaches are illustrated in Figure~\ref{fig_instance}, and their details are presented in Table~\ref{table_instance}.
}
\begin{figure}%[h]
	\centering
	\includegraphics[width=\linewidth]{response/crop_extension1.pdf}
  \vspace{-0.5cm}
	\caption{\textcolor{black}{Solving CDFSL problem by $\mathcal{D}$-Extension, including data augmentation, feature generation, and task synthesis. Data augmentation aims to augment data by instance/pseudo-label generation, \etc. Feature generation enhances the features by fusing or re-weighting them. And task synthesis improves model generalization by generating more guided tasks. The core operations of methods are highlighted in red. % We classify existing CDFSL methods into \textcolor{black}{$\mathcal{D}$-Extension, $\mathcal{H}$-Constraint, $\Delta$-Adaptation, and Hybrid Methods.
    }}
  \vspace{-0.3cm}
	\label{fig_instance}
\end{figure}

\iffalse
\begin{table}
\tiny
\centering
\caption{Representative $\mathcal{D}$-Extension approaches. `$FG$', `$Art$' and `$IW$' indicate that evaluation of Fine-grained CDFSL ($FG$), Art-based CDFSL ($Art$) and imaging way-based CDFSL ($IW$), respectively.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{3.6mm}{
\begin{tabular}{llcccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Augment aspect} & \textbf{Augmented information from} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
NSAE~\cite{boosting} & ICCV 2021 & Data Augmentation  & Instances & CE\& BSR & \Checkmark &  & \Checkmark \\
% \hline
TGDM~\cite{tgdm} & ACM MM 2022 & Data Augmentation  & Instances & CE & \Checkmark &  &  \\  
% \hline
ISSNet~\cite{data_multi_3}   & ICIP 2023 & Data Augmentation & Instances & BSR \& Perceptual \& Style &  &  & \Checkmark     \\
% \hline
SSA-BNS~\cite{sreenivas2023similar}  & CVPR 2023 & Data Augmentation & Instances & $L_{NCC}$ &  & \Checkmark &    \\
% \hline
CDPSN~\cite{data_other_1} & Scientific Reports 2023 & Data Augmentation  & Instances & CE &  &   & \Checkmark \\  
% \hline
FAP~\cite{extend_1} & arXiv 2024 & Data Augmentation  & Instances & CE \& KL &   \Checkmark    &   & \Checkmark \\  
% \hline
UD~\cite{data_target_2}   &  arXiv 2021 & Data Augmentation &  Pseudo Label & Contrastive &  & \Checkmark &    \\
% \hline
STARTUP~\cite{st} & ICLR 2021 & Data Augmentation & Pseudo Label & CE \& KL \& SimCLR &  &  & \Checkmark  \\
% \hline
CHEF~\cite{rf}   & arXiv 2020 & Feature Generation & Feature Fusion & CE &  &  & \Checkmark  \\
% \hline
PKTN~\cite{mdfsl}  & IJON 2021 & Feature Generation & Feature Fusion & Consistency &  \Checkmark   &  &    \\  
% \hline
RMP~\cite{feature_fusion_2} & ACM MM 2021 & Feature Generation & Feature Fusion & CE \& $L_{2}$ & \Checkmark  &  &   \\
% \hline
DFTL~\cite{parameter_fix_3} & ICAICA 2021 & Feature Generation & Feature Fusion & CE & \Checkmark &  &    \\
% \hline
GA-GNN~\cite{liu2022geometric} & APPL INTELL 2022 & Feature Generation & Feature Fusion & CE & \Checkmark &  &   \\
% \hline
RDC~\cite{feature_reweight_6} & CVPR 2022 & Feature Generation & Feature Fusion & CE \& KL & \Checkmark &  & \Checkmark  \\
% \hline
KT~\cite{li2023knowledge} & PR 2023 & Feature Generation & Feature Fusion & CE \& MSE &  &  & \Checkmark \\
%\hline
DeepEMD~\cite{feature_reweight_3} & ISCIPT 2021 & Feature Generation & Feature Re-weighting & CE & \Checkmark &  &    \\
% \hline
FUM~\cite{feature_reweight_7} & PR 2022 & Feature Generation & Feature Re-weighting & CE & \Checkmark & \Checkmark &   \\
% \hline
HVM~\cite{parameter_weight_2}  & ICLR 2022 & Feature Generation & Feature Re-weighting & ELBO &  &  & \Checkmark    \\  
% \hline
Wave-SAN~\cite{parameter_fix_9} & arXiv 2022 & Feature Generation & Feature Re-weighting & CE \& KL & \Checkmark  &  & \Checkmark  \\
% \hline
AFGR~\cite{parameter_weight_3} & NCA 2022 & Feature Generation & Feature Re-weighting & CE & \Checkmark &  &  \\
%\hline
AFA~\cite{adversarial} & ECCV 2022 & Feature Generation & Feature Re-weighting & CE \& $L_{2}$ \& Adversarial & \Checkmark &  & \Checkmark \\
%\hline
StyleAdv~\cite{feature_reweight_0} & CVPR 2023 & Feature Generation & Feature Re-weighting & CE & \Checkmark &   & \Checkmark   \\
% \hline
ReqNet~\cite{ji2024relevance}   & IJMIR 2024 & Feature Generation & Feature Re-weighting & CE \& MI \& InfoNCE \& KL & \Checkmark &  & \Checkmark  \\
% \hline
ATA~\cite{ata} & IJCAI 2021 & Task Synthes & - & CE & \Checkmark &  & \Checkmark  \\
% \hline
AGE~\cite{feature_fusion_3} & AAAI 2022 & Task Synthes & - & CE &  & \Checkmark &   \\
% \hline
ST~\cite{data_other_2} & KBS 2023 & Task Synthes & - & CE & \Checkmark &  & \Checkmark  \\
% \hline
RDProtoFusion~\cite{rao2024rdprotofusion} & IJON 2024 & Task Synthes & - & CE \& Contrastive & \Checkmark &  & \Checkmark   \\
\hline
\end{tabular}}
\vspace{-0.5cm}
\label{table_instance}
\end{table}
\fi

\iffalse
 \begin{wrapfigure}{l}{180pt}
	\centering
  \vspace{-0.2cm}
	\includegraphics[width=6cm]{cdfsl-data-1.pdf}
 \vspace{-0.3cm}
	\caption{\textcolor{black}{The different categories of instance-guided approaches. Instances are from different origins. $\theta$ means the recognizer.}}
 \vspace{-0.5cm}
	\label{fig_instance}
\end{wrapfigure}
\fi

\subsubsection{\textcolor{black}{Data Augmentation.}} \label{data_original}
\textcolor{black}{Data augmentation expands the information by generating additional samples. Current methods introduce various types of transformations for models to learn, such as flipping~\cite{shyam2017attentive}, scaling~\cite{lake2015human, zhang2018fine}, translation~\cite{benaim2018one, lake2015human}, cropping~\cite{zhang2018fine}, shearing~\cite{shyam2017attentive}, reflection~\cite{edwards2017towards}, and rotation~\cite{model32, miniimagenet}. However, it's impractical to account for all potential transformations manually. Thus, affine transformations alone cannot fully solve the CDFSL problem~\cite{shyam2017attentive, lake2015human}. Next, we explore more advanced data augmentation methods at the sample and label levels, including instance augmentation and pseudo-label generation.}
% There are some approaches involve \textcolor{black}{the use of affine transformations, synthetic new data, and pseudo labels, as shown in Figure~\ref{fig_instance}. Affine transformations include flipping~\cite{shyam2017attentive}, scaling~\cite{lake2015human, zhang2018fine}, translation~\cite{benaim2018one, lake2015human, model32, shyam2017attentive}, cropping~\cite{zhang2018fine}, shearing~\cite{shyam2017attentive}, reflection~\cite{edwards2017towards}, and rotation~\cite{model32, miniimagenet}, \etc. Currently, almost of all CDFSL methods augment the samples with affine transformation manner. Therefore, here we ignore this augment strategy.}


\textbf{Instance Augmentation.}
\textcolor{black}{Instance augmentation~\cite{data_multi_3,sreenivas2023similar,extend_1,data_other_1,adcd,boosting,tgdm} increases sample diversity by synthesizing new data. Techniques include style transfer~\cite{data_multi_3,sreenivas2023similar}, frequency domain transformation~\cite{extend_1,data_other_1}, and adversarial methods~\cite{adcd,boosting}.} For instance, \cite{data_multi_3} introduces unlabeled data from multiple domains into the original source domain, generating large amounts of data with different styles but the same content. \textcolor{black}{\cite{sreenivas2023similar} augments data by applying the style of semantically similar categories to each class.} Additionally, \cite{extend_1} enhances samples by processing high-frequency components of source images, allowing the network to mimic human visual perception by selecting different frequency cues in recognition tasks. Similarly, \cite{data_other_1} generates a sketch map from the original image to capture high-frequency information. \textcolor{black}{\cite{adcd} improves model robustness by generating adversarial instances, while \cite{tgdm} facilitates knowledge transfer by introducing an intermediate domain through mixing source and target domain images.} Furthermore,~\cite{boosting} performs adversarial training with noise data, enhancing model robustness.
% \textcolor{black}{Instance augmentation focuses on synthesizing data through the style transfer~\cite{data_multi_3,sreenivas2023similar}, frequency domain transform~\cite{extend_1}, and adversarial~\cite{adcd}, \etc.}~\cite{data_multi_3} introduces unlabeled data from multiple domains into the original source domain to transfer diverse styles, generating a large amount of data with different styles but the same content.\textcolor{black}{Besides, ~\cite{sreenivas2023similar} uses the style of semantically similar categories to augment the data of each category. Moreover,~\cite{extend_1} processes the high-frequency components of the source image to generate frequency-aware enhanced samples, allowing the network to simulate the visual perception of humans choosing different frequency cues when facing new recognition tasks. In~\cite{data_other_1}, the original image and its generated sketch map are processed separately by different branches of the network. Besides,~\cite{adcd} enhances model robustness by generating adversarial instances.~\cite{tgdm} introduces an intermediate domain generated by mixing images in the source and the target domain to help knowledge transfer.}~\cite{boosting} enhances model robustness by generating noisy data.

\textcolor{black}{
\textbf{Pseudo-label Generation.}
%Moreover, pseudo labels generation aims at introducing the unlabeled data and generating the pseudo labels for these data~\cite{data_target_2,st}. 
Pseudo-label generation involves leveraging unlabeled data by assigning pseudo labels to it~\cite{data_target_2,st}. One early approach~\cite{data_target_2} enhances unlabeled data by applying rotations and assigning the rotation degree as the pseudo labels, turning transformation tasks into supervised learning problems. Additionally,~\cite{st} introduces unlabeled target data during the pre-training stage, generating pseudo labels for these data through a pre-trained model. This unlabeled target data serves as supplementary information from the target domain, helping to enrich the model's understanding of the target domain without manual annotation.
% Pseudo-label generation aims at introducing the unlabeled data and generating the pseudo labels for these data~\cite{data_target_2,st}. An early work~\cite{data_target_2} introduces the unlabeled data, rotates them, and uses the rotation degree as their pseudo labels. Moreover,~\cite{st} introduces the unlabeled target data in the pre-training phase, and obtains the pseudo labels for them. These unlabeled target data is the extra information from the target domain.
}
% Some approaches involve the use of additional information from the original data, such as semantic and visual information, to enhance the performance of FSL tasks, as shown in Figure~\ref{data-original}. Among them, some works extract this extra information through reconstructed instances, as depicted in the green background area of the Figure~\ref{data-original}.  For example, in~\cite{lscdfsl}, a Triplet Autoencoder (TriAE) is utilized to learn a shared feature representation. It incorporates both source and target instances, and leverages semantic information as an intermediate bridge. In~\cite{boosting}, an autoencoder is used to reconstruct the input data, and the reconstructed data is then utilized as additional visual information to aid in the training process and learn the shared feature representation. And~\cite{hybrid_5} distills the knowledge of multiple tasks/domain-specific networks into a single network. This is achieved by aligning the representations of the single network with the task/domain-specific ones using small capacity adapters.
 %\vspace{-0.2cm}
 \iffalse
 \begin{wrapfigure}{r}{180pt} 
	\centering
	\includegraphics[width=6cm]{cdfsl-data1-43.pdf}
 \vspace{-0.2cm}
	\caption{\textcolor{black}{Instances from extra information of original data. The extra information can be from generation model (green area) or other modal like texts (blue part). Dotted lines indicate the process of additional information introduction.}}
	\label{data-original}
 \vspace{-0.3cm}
\end{wrapfigure}
\fi

%Meanwhile, other works directly add additional information to the model, as shown in the blue part of Figure~\ref{data-original}. For instance,~\cite{mdfsl} presents a model that integrates visual and semantic information to recognize target categories, and utilizes weight imprinting for future fine-tuning. Furthermore, in~\cite{data_other_1}, the original image and its corresponding sketch map are processed separately by different branches of the network. The features extracted from the original image are combined with the contour features extracted from the sketch map branch during training, thus improving the accuracy and generalization performance of the model. Moreover,~\cite{data_other_2} proposes a task-expansion-decomposition framework for CD-FSL called the self-taught (ST) approach, which alleviates the problem of non-target guidance by constructing task-oriented metric spaces.

\subsubsection{\textcolor{black}{Feature Generation}} \label{data_multiple}
% By utilizing instances from multiple domains, a model can learn a general shared representation with broad generalization ability. The Domain-Agnostic Meta Learning (DAML) algorithm, proposed in~\cite{data_multi_2}, adapts the model to novel classes in seen and unseen domains. In contrast,~\cite{data_multi_3} introduces unlabeled data from multiple domains into the original source domain to transfer diverse styles, making the model more adaptable to various domains and styles. Moreover, most methods combine multiple strategies together with the multiple-domain introduction strategy, as shown in Section~\ref{hybrid}.
\textcolor{black}{Feature generation, as shown in Figure~\ref{fig_instance}, creates new feature representations from existing ones, enriching the feature space and improving the model's ability to capture data patterns, especially in scenarios with limited data. The goal of generating new features is either to expand the total information content or to explore key features in greater depth. According to the focus of generating features, methods can be divided into two categories: (1) feature fusion~\cite{rf,feature_fusion_2,mdfsl,parameter_fix_3,feature_reweight_6}, aims to increase the total information content, and (2) feature re-weighting~\cite{parameter_weight_2,ji2024relevance,parameter_fix_9,feature_reweight_3,feature_reweight_7,parameter_weight_3,adversarial,feature_reweight_0}, which focuses on increasing the amount of critical information in the feature to optimize information usage.}
% \textcolor{black}{Besides directly augment data, the other manner to extend the information is generating the new features (as shown in Figure~\ref{fig_instance}), which is to the following two purposes: (1) Increase the overall amount of information, (2) Dig out more key information. Based on these two purposes, we divide methods as feature fusion~\cite{mdfsl,rf,feature_fusion_2,parameter_fix_3,liu2022geometric,li2023knowledge} and feature re-weighting~\cite{parameter_weight_2,ji2024relevance,parameter_fix_9,feature_reweight_3,feature_reweight_7,parameter_weight_3,adversarial}. Feature fusion combines features of different representations to enhance the amount of information, while feature re-weighting assigns different weights to features based on their importance to optimize the use of information. These fused and re-weighted features further augment the effective information in the existing features. }
% \textcolor{black}{Besides directly augment data, the other manner to extend the information of data is generating the new features (as shown in Figure~\ref{fig_instance}), including feature fusion~\cite{mdfsl,rf,feature_fusion_2,parameter_fix_3,liu2022geometric,li2023knowledge} and feature re-weighting~\cite{parameter_weight_2,ji2024relevance,parameter_fix_9,feature_reweight_3,feature_reweight_7,parameter_weight_3,adversarial}. Feature fusion combines features of different representations to form a new feature representation, while feature re-weighting assigns different weights to features based on their importance to adjust their impact in the model. These fused and re-weighted features further mine the effective information in the existing features, expanding the amount of effective information. }

\textbf{Feature Fusion.}
\textcolor{black}{
There are two main types of feature fusion methods~\cite{rf,feature_fusion_2,mdfsl,parameter_fix_3,feature_reweight_6} discussed here: multi-level fusion~\cite{rf,feature_fusion_2} and multi-model fusion~\cite{mdfsl,parameter_fix_3,feature_reweight_6}. Multi-level fusion focuses on combining features from different layers of the same model, addressing the limitations of using single-layer features. Low-level features may lack the specificity needed for complex tasks, while high-level features can overfit to the training data. By fusing these features, a more balanced and versatile representation is created, capable of handling a wide range of tasks~\cite{lin2017feature}. For example,\cite{rf} computes the loss for the last few layers of the model and sums them up as the final loss, while\cite{feature_fusion_2} fuses mid-level features to predict the residual term.}
% There are two ways to fuse the features, multi-level fusion~\cite{rf,feature_fusion_2} and multi-model fusion~\cite{mdfsl,parameter_fix_3,li2023knowledge,feature_reweight_0}, in the feature fusion. In which, multi-level fusion aims to fuse the features of different layers of the same model and mitigate the limitations of using single-layer features alone. Low-level features might lack the specificity needed for complex tasks, and high-level features might overfit to specific training data. The fusion of these features creates a more balanced and versatile feature representation, capable of handling a wide range of tasks and conditions effectively~\cite{lin2017feature}. Specifically,~\cite{rf} calculates the loss of the features of the last few layers of the model separately and sums them up as the final loss.~\cite{feature_fusion_2} introduces and fuses the mid-level features to predict the residual term.}
% There are two ways to fuse the features, multi-level fusion~\cite{rf,feature_fusion_2,liu2022geometric} and multi-model fusion~\cite{mdfsl,parameter_fix_3,li2023knowledge,feature_reweight_0}, in the feature fusion. In which, multi-level fusion aims to fuse the features of different layers of the same model and mitigate the limitations of using single-layer features alone. Low-level features might lack the specificity needed for complex tasks, and high-level features might overfit to specific training data. The fusion of these features creates a more balanced and versatile feature representation, capable of handling a wide range of tasks and conditions effectively~\cite{lin2017feature}. Specifically,~\cite{rf} calculates the loss of the features of the last few layers of the model separately and sums them up as the final loss. While~\cite{feature_fusion_2} introduces and fuses the mid-level features to predict the residual term. Moreover,~\cite{liu2022geometric} constructs a high-dimensional geometric algebra (GA) space in the non-Euclidean domain to help in reducing the distortion of feature information that typically increases with the depth of the network layers.

\textcolor{black}{
Multi-model fusion~\cite{mdfsl,parameter_fix_3,feature_reweight_6} refers to combining features generated by different models or strategies. Since these features come from diverse sources, fusing them leverages the strengths of each, creating a richer and more distinguishable feature representation. For instance,\cite{mdfsl} fuses knowledge from both class and semantic aspects,\cite{parameter_fix_3} averages the features from multiple models, and~\cite{feature_reweight_6} constructs a non-linear subspace, then combines the features from this subspace with the original features.
% While the multi-model fusion~\cite{mdfsl,parameter_fix_3,li2023knowledge} refers to the fusion of features generated by multiple different models or strategies. Since these features come from different models or strategies, when these features are fused together, they can give full play to their respective advantages and form a richer and more discernible feature representation.~\cite{mdfsl} fuse the knowledge from both class and semantic aspects.~\cite{parameter_fix_3} takes the average of the features from models with diversified feature transformation layers. And~\cite{li2023knowledge} introduces the information from query set. Besides, the features of the support set are combined in~\cite{li2023knowledge} with the query set features to calculate the loss. And~\cite{feature_reweight_6} uses a hyperbolic tangent transformation to construct a non-linear subspace, then combine the features from this subspace and the original feature.
}

\textbf{Feature Re-weighting.}
\textcolor{black}{
Feature re-weighting~\cite{parameter_weight_2,ji2024relevance,parameter_fix_9,feature_reweight_3,feature_reweight_7,parameter_weight_3,feature_reweight_0,adversarial} adjusts the weights of different features so the model can adaptively select the most important ones for different tasks or domains, improving performance.} For example, \cite{parameter_weight_2} learns separate weights for each multi-level feature, \cite{parameter_fix_9} re-weights features across different episodes, and~\cite{feature_reweight_7} introduces a forget-update module to re-weight features. Meanwhile, \cite{parameter_weight_3} applies attention mechanisms across multiple levels, \textcolor{black}{and \cite{ji2024relevance} re-weights both original and style-transformed features through context interaction. \cite{feature_reweight_3} enhances feature representation by leveraging local feature correlations, while \cite{feature_reweight_0} inputs style-altered features into the FSL classifier.~\cite{adversarial} explores adversarial feature augmentation to simulate distribution variations by re-weighting features.
% Feature re-weighting~\cite{parameter_weight_2,ji2024relevance,parameter_fix_9,feature_reweight_3,feature_reweight_7,parameter_weight_3,feature_reweight_0,adversarial} adjusts the weights of different features so that the model can adaptively select the most important features in different tasks or domains, thereby improving the performance of the model.} Among them,~\cite{parameter_weight_2} learns weights for each multi-level feature separately and re-weights them.~\cite{parameter_fix_9} explores a style augmentation module in each layer to re-weight the features between different episodes.~\cite{feature_reweight_7} explores a forget-update module to re-weight the feature in each layer.~\cite{parameter_weight_3} applies attention mechanisms at multiple levels to re-weight features to capture details at different scales. \textcolor{black}{Furthermore,~\cite{ji2024relevance} first transfers the style of the features, then interacts with the original and style-transformed features across context and re-weight them.~\cite{feature_reweight_3} enhance feature representation by leveraging the correlation between local features. In \cite{feature_reweight_0}, the style attacked features obtained through style transfer are input into the FSL classifier.~\cite{adversarial} explores a adversarial feature augmentation module to generates augmented features to simulate distribution variations by re-weight the features.
}



\vspace{-0.2cm}
\subsubsection{\textcolor{black}{Task Synthesis}} \label{data_target}
%Approaches that leverage target domain instances aim to uncover the shared information between the source and target domains. Some of these approaches employ a teacher-student network to aid
\iffalse
\begin{wrapfigure}{l}{150pt} 
	\centering
 \vspace{-0.4cm}
	\includegraphics[width=5cm]{cdfsl-data3.pdf}
 \vspace{-0.4cm}
	\caption{\textcolor{black}{STARTUP~\cite{st} structure. The dotted line represents how to use the auxiliary target data.}}
	\label{data-target}
 \vspace{-0.3cm}
\end{wrapfigure}
\fi
%training phase. CDFSL learning. For example, in~\cite{st}, (illustrated in Figure~\ref{data-target}), a self-training method is proposed that utilizes unlabeled target data to improve the source domain representation. It is the first work to introduce the unlabeled target data into the~\cite{dynamic} follows this setting and enforces consistency by comparing predictions of weakly-augmented unlabeled target data from a teacher network to strongly-augmented versions of the same images from a student network. Meanwhile,~\cite{data_target_2} develops a self-supervised learning approach to fully leverage unlabeled target domain data.
%(as depicted in the right part of Figure~\ref{data-target}).
%Other works integrate all labeled target data directly into the training process. For instance,~\cite{data_target_1} presents a Domain-Switch Learning (DSL) framework that embeds cross-domain scenarios into the training phase in a ``fast switching'' manner using multiple target domains.
\textcolor{black}{Task synthesis~\cite{ata,feature_fusion_3,rao2024rdprotofusion,data_other_2} (Figure~\ref{fig_instance}) expands the task space by creating new task scenarios or combinations, improving the model’s ability to generalize. Some methods generate challenging tasks to train the model in difficult situations. For example, \cite{ata} generates tasks simulating worst-case scenarios, helping the model adapt to a wider distribution of tasks during training. Other approaches synthesize tasks at the graph level. For instance, \cite{feature_fusion_3} augments task graphs into contextual and geometric views, while \cite{rao2024rdprotofusion} integrates information from different tasks using a multi-task learning framework. Additionally, \cite{data_other_2} randomly combines affine-transformed samples into diverse tasks.}
% \textcolor{black}{Task synthesis (Figure~\ref{fig_instance}) methods~\cite{ata, feature_fusion_3, rao2024rdprotofusion, data_other_2} aims to explore or synthesize diverse tasks to enhance the generalization of the model. Some of them synthesize challenging tasks to train the model's ability to handle difficult tasks. For example,~\cite{ata} generates challenging tasks that simulate worst-case scenarios, thus enabling the model to adapt to a wider distribution of tasks during training. Some other approaches synthesize the tasks at the graph level. For instance, the graphs from tasks in~\cite{feature_fusion_3} are augmented to a contextual view and two geometric view. Besides,~\cite{rao2024rdprotofusion} adopts a multi-task learning framework to integrate information from different tasks to improve the generalization ability and robustness of the model. And~\cite{data_other_2} randomly combines affine transformed samples into diverse tasks to improve model generalization.}

\subsubsection{Discussion and Summary}
% \textbf{Discussion and summary}
% 实验结果，结果分析，总结这类方法以及子类别方法的优缺点
% Instance-guided strategies are chosen based on the availability of data. When the source domain includes extra information such as semantic and visual information, utilizing instances from the original source (as described in Section~\ref{data_original}) is an effective approach. However, in scenarios where extra information is not available, introducing instances from the target domain (as discussed in Section~\ref{data_target}) could be a better option. In cases where target data is scarce or unavailable, utilizing instances from multiple domains (as outlined in Section~\ref{data_multiple}) can also be helpful.
% 每个子类别的优缺点
\textcolor{black}{
When additional unsupervised samples or data generation methods are available, the amount of information can be expanded through data augmentation. However, in data augmentation, particularly in instance augmentation, the augmented information is typically derived from the original samples, which results in limited valid information gain and a lack of real-world diversity. Additionally, the quality of generated pseudo-labels depends on the model's predictions; if the model has biases, the pseudo-label generation process may further amplify these biases.}

\textcolor{black}{When multiple features, including those from multiple models or levels, are available, feature generation methods can be applied. These methods enrich the feature space by processing multiple features to enhance the information. However, since these features come from the same examples, they are often highly correlated or redundant, which increases model complexity while providing limited additional information. Therefore, finding the optimal solution requires very fine adjustment~\cite{he2016deep}.}

\textcolor{black}{Finally, when auxiliary tasks are present, task synthesis enhances diversity by combining new tasks. However, this increased diversity can make the model’s learning objectives unclear and task relevance ambiguous. This occurs because different tasks may conflict or lack consistency, making it difficult for the model to identify which features or patterns are most important for the primary task. Additionally, auxiliary tasks can introduce irrelevant noise, further confusing the learning process. As a result, the composed tasks may not align well with the actual task, leading to sub-optimal performance in real-world applications~\cite{al32}.}
% $\mathcal{D}$-Extension methods improve the generalization ability of the model by expanding the amount of information. This is a very simple and easy method that can be directly applied to existing models. Specifically, data augmentation methods, including instance augmentation and pseudo-label generation, are effective methods when the sample generation mode is effective or unsupervised samples are available. It is the most intuitive method to expand information by increasing the amount of available data. Unlike data augmentation, which focuses on simply expanding the amount of data, task synthesis focuses more on the way of task composition. In addition, feature generation is feasible when multiple features are available or additional weights are available.

% \textcolor{black}{
% However, such methods still have some disadvantages, as they can introduce noise during the process of expanding information, which may ultimately mislead the learning of model. Specifically, data augmentation methods expand the information by introducing the augmented data, while it needs to ensure the accuracy of the generated samples or pseudo-labels, as inaccurate ones can introduce noise~\cite{inoue2018data}. Feature generation methods have different feature representations under the different combinations or weighting methods, and the optimal solution requires very fine adjustment~\cite{he2016deep}. In addition, the quality of the synthesis task is affected by the synthesis way, \ie improper task synthesis may cause task bias, which in turn leads to model training bias~\cite{al32}. }

% \textcolor{black}{However, such methods still have some disadvantages. Specifically, in data augmentation, especially in instance augmentation, the augmented information is usually from the original samples, resulting in a limited valid information gain and a lack of real-world diversity. Besides, the quality of generated pseudo-labels depends on the model's predictions. If the model has biases, the pseudo-label generation process may further amplify these biases. Task synthesis enhances diversity by composing new tasks, while these diversity tasks may results in a fuzzy model learning objectives and the unclear task relevance, \ie the synthetic tasks may not be completely relevant to the actual tasks, resulting in poor performance of the model in real applications~\cite{al32}. Recently,~\cite{data_multi_3} augments the data under the guidance of the style of the target domain, which increases the diversity of the data while also improving the relevance of the data to the task. In addition, in feature generation, choosing which features to generate and retain requires experience and domain knowledge, which is difficult. An unreasonable generated feature may be highly correlated or redundant with the original feature, which increases the complexity of the model without bringing additional information. Therefore, the optimal solution requires very fine adjustment~\cite{he2016deep}.}
% \textcolor{black}{However, such methods still have some disadvantages. Existing methods focus on expanding information to enhance the model's generalization, but this information is not specifically tailored to the target domain. As a result, the expansion of effective information relevant to the target domain is limited. Specifically, in data augmentation, the augmented data is from the original one, resulting in a limited information gain and a lack of real-world diversity. Task synthesis enhances diversity by composing new tasks, while these diversity tasks may results in a fuzzy model learning objectives and the unclear task relevance, \ie the synthetic tasks may not be completely relevant to the actual tasks, resulting in poor performance of the model in real applications~\cite{al32}. Recently,~\cite{data_multi_3} augments the data under the guidance of the style of the target domain, which increases the diversity of the data while also improving the relevance of the data to the task. In addition, in feature generation, choosing which features to generate and retain requires experience and domain knowledge, which is difficult. An unreasonable generated feature may be highly correlated or redundant with the original feature, which increases the complexity of the model without bringing additional information. Therefore, the optimal solution requires very fine adjustment~\cite{he2016deep}.}

\textcolor{black}{
\subsection{$\mathcal{H}$-Constraint}}  \label{hypothesis}
% Parameter-based approaches are designed to reduce the complexity of the hypothesis space by manipulating the model's parameters to discover shared feature representations. There are three main techniques in this approach, as illustrated in Figure~\ref{parameterr}: (1) Parameter freeze involves fixing certain model parameters, simplifying the search for shared feature representations, (2) In parameter selection, the most appropriate model is selected from a pool of models based on their parameters, and (3) Parameter Reweighting employs additional parameters to constrain the hypothesis space. Table~\ref{table_parameter} provides a detailed summary of the methods that fall under this category.
\textcolor{black}{$\mathcal{H}$-Constraint are designed to solve the problem of unreliable TSERM by reducing the complexity of the hypothesis space $\mathcal{H}$. A smaller $\mathcal{H}$ limits the model’s complexity, which helps reduce overfitting and improves performance in the target domain. The methods in this section explore constraints on $\mathcal{H}$ from three perspectives: structural constraints, guided constraints, and normative constraints. Based on how these constraints are implemented, the methods are categorized into three types: (1) parameter adjustment, (2) knowledge distillation, and (3) regularization, as illustrated in Figure~\ref{parameterr}.} 
% $\mathcal{H}$-Constraint are designed to solve CDFSL by reduce the complexity of the hypothesis space $\mathcal{H}$. \textcolor{black}{A smaller $\mathcal{H}$ limits the model’s complexity, enabling it to focus on learning domain-invariant key features, which helps reduce overfitting and improve performance in the target domain, especially when data is limited and domain shifts are significant. The methods in this section achieve this by incorporating prior knowledge or external constraints to reduce the number or complexity of hypotheses the model can select. Based on the prior knowledge or external conditions used to implement this constraint, we categorize these methods into the following three types: parameter adjust, knowledge distillation, and regularization, as illustrated in Figure~\ref{parameterr}:} (1) Parameter adjust involves adjusting certain parameters including model, adapter, and prompt, simplifying the search for the best hypothesis, \textcolor{black}{(2) Knowledge distillation, including standard and multi-level distillation, constrains the hypothesis space of the student model by using the output of a pre-trained teacher model as soft labels to guide the training of the student model,} and (3) Regularization constrains the hypothesis space by introducing additional constraints or randomness during the training process (such as adding penalty terms or applying Dropout) to limit the complexity of the model. Table~\ref{table_parameter} provides a detailed summary of the methods that fall under this category.
% \textcolor{black}{$\mathcal{H}$-Constraint are designed to solve CDFSL by reduce the complexity of the hypothesis space $\mathcal{H}$. There are three main techniques are explored to constrain $\mathcal{H}$, including constrain the parameters directly (parameter adjust) and indirectly (knowledge distillation), and explore some constraint strategies (regularization), as illustrated in Figure~\ref{parameterr}:} (1) Parameter adjust involves adjusting certain parameters including model, adapter, and prompt, simplifying the search for the best hypothesis, \textcolor{black}{(2) Knowledge distillation, including standard and multi-level distillation, constrains the hypothesis space of the student model by using the output of a pre-trained teacher model as soft labels to guide the training of the student model,} and (3) Regularization constrains the hypothesis space by introducing additional constraints or randomness during the training process (such as adding penalty terms or applying Dropout) to limit the complexity of the model. Table~\ref{table_parameter} provides a detailed summary of the methods that fall under this category.
% 旨在缩小假设空间范围，从而更容易找到共享特征
\begin{figure}%[b]
	\centering
  \vspace{-0.3cm}
	%\includegraphics[width=\linewidth]{cdfsl-parameter-5.pdf}
	\includegraphics[width=\linewidth]{response/constraint_cropped.pdf}
 \vspace{-0.5cm}
	\caption{\textcolor{black}{$\mathcal{H}$-Constraint methods, including parameter adjustment, knowledge distillation, and regularization. The gray background means the frozen parameters, and the core operations of methods are highlighted in red.}}
 \vspace{-0.4cm}
	\label{parameterr}
\end{figure}

\subsubsection{\textcolor{black}{Parameter Adjustment}} \label{fixed}
% Parameter freeze is a strategy that restricts the hypothesis space's complexity by fixing some model parameters. This method is usually used in meta-learning-based approaches, where they alternately freeze some parameters during meta-training and meta-testing phases. Among them, score-based meta transfer-learning (SB-MTL)~\cite{parameter_fix_1} combines transfer-learning and meta-learning by using a MAML-optimized feature encoder and a score-based Graph Neural Network. Some parameters in MAML are frozen in the training phase. And in~\cite{parameter_fix_5}, a meta-encoder is alternately frozen and optimized during the inner update phase to learn general features. In addition, other works propose plug-and-play augmentation modules to constrain the hypothesis space. In these works, the core idea of~\cite{feature-wise} is to asynchronously freeze and update the proposed feature-wise transformation layers and the feature extractor, as shown in Figure~\ref{parameterr} (a). Due to the inspiring of~\cite{feature-wise}, many works have improved and enhanced this work.~\cite{parameter_fix_3} proposes a diversified feature transformation based on the original feature transformation layer to solve the CDFSL problem. And~\cite{parameter_fix_7} offer two new strategies, FGNN (Flexible GNN) and a new hierarchical residual-like block, for the encoder and metric function of the metric-based network.
\textcolor{black}{
Parameter adjustment is a common strategy for imposing structural constraints, directly tuning parameters to reduce the complexity of the hypothesis space~\cite{parameter_fix_1,parameter_weight_1,parameter_weight_4,guo2023task,wu2024task,wu2024hybridprompt}. These parameters can come from existing models or be introduced through additional modules. Based on the source of the adjusted parameters, methods in this strategy are categorized into three groups: encoder models~\cite{parameter_fix_1,parameter_weight_1}, extra adapters~\cite{parameter_weight_4,guo2023task}, and prompts~\cite{wu2024task,wu2024hybridprompt}.
% Parameter adjustment is a strategy that reduces the complexity of the hypothesis space by directly tuning parameters. These parameters may originate from existing models or from additional introduced modules. Based on the source of the adjusted parameters, we categorize methods in this strategy into three groups: (1) encoder models~\cite{parameter_fix_1,parameter_weight_1}, (2) extra adapters~\cite{parameter_weight_4,guo2023task}, and (3) prompts~\cite{wu2024task,wu2024hybridprompt}.
}
% \textcolor{black}{Parameter adjust is a strategy that restricts the hypothesis space's complexity by tuning some parameters. These parameters can be from encoder models~\cite{parameter_fix_1,parameter_weight_1,feature-wise,parameter_fix_7}, the extra adapters~\cite{parameter_weight_4,guo2023task} and prompts~\cite{wu2024task,wu2024hybridprompt}.}

\textbf{Encoder Models.}
A common method for parameter adjustment is through the encoder models~\cite{parameter_fix_1,parameter_weight_1}. For instance,~\cite{parameter_fix_1} constrains the hypothesis space by alternately freezing certain model parameters in the inner and outer loops, allowing parameters in different regions to maximize their effectiveness. In addition,~\cite{parameter_weight_1} resets the parameters learned from the source domain before adapting to the target data. However, directly adjusting the parameters of the encoder model strictly limits the number of parameters that can be adjusted. When the encoder model has a large number of parameters, this strategy becomes inefficient.
% The common parameters can be adjusted is the encoder models~\cite{parameter_fix_1,parameter_weight_1,feature-wise,parameter_fix_7}. Among them,~\cite{parameter_fix_1} constrains the hypothesis space by alternately freezing certain model parameters in the inner and outer loops, allowing parameters in different regions to maximize their effectiveness. In addition,~\cite{parameter_weight_1} resets the parameters that were learned on the source domain before adapting to the target data. Moreover,~\cite{feature-wise} asynchronously freeze and update the proposed feature-wise transformation layers and the feature extractor. Due to the inspiring of~\cite{feature-wise},~\cite{parameter_fix_7} offers two new strategies, Flexible GNN and a new hierarchical residual-like block, for the encoder and metric function of the metric-based network. However, directly adjusting the parameters of the encoder model imposes strict constraints on the number of parameters. When the encoder model has a large number of parameters, this strategy becomes inefficient.

\textbf{Adapters.}
To reduce the costs of parameter adjustment, extra adapters have been explored for insertion during the training phase~\cite{parameter_weight_4,guo2023task}. Specifically,~\cite{parameter_weight_4} constrains $\mathcal{H}$ by dynamically adapting the network via task-specific adapters. \textcolor{black}{~\cite{guo2023task} adjusts parameters through dynamic selection and activation of these task-specific adapters.
% To reduce the costs of parameter adjustment, the extra adapters are explored to insert into the training phase~\cite{parameter_weight_4,guo2023task}. Specifically,~\cite{parameter_weight_4} constrains the hypothesis space by dynamically adapting the network via task-specific adapters. \textcolor{black}{Besides, the other method~\cite{guo2023task} proposes a task-aware adaptive learning, which adjusts parameters through dynamic selection and activation of task-specific adapters, thereby constraining the model's hypothesis space.
}

\textbf{Prompts.}
\textcolor{black}{With the widespread application of prompt learning, some methods~\cite{wu2024task,wu2024hybridprompt} address the CDFSL problem by adjusting only the prompt parameters. For example, \cite{wu2024task} generates task-adaptive prompts through a network, which are then used to guide the backbone network in solving the FSL task on the target domain. Building on this, \cite{wu2024hybridprompt} explores domain-aware prompting, transferring prompts learned from the source task to the target task.
}

\subsubsection{\textcolor{black}{Knowledge Distillation}} \label{pa-select}
% The parameter selection strategy, as depicted in Figure~\ref{parameterr} (b), seeks to identify the most appropriate set of parameters for the target domain to enhance performance. To achieve this, researchers have proposed various methods. For example, in~\cite{parameter_select_1}, the authors sample sub-networks by dropping neurons or feature maps, and then choose the most suitable sub-networks to form an ensemble for target domain learning. Additionally,~\cite{parameter_select_2} proposes a dynamic selection mechanism by sequentially applying multiple state-of-the-art adaptation methods, thereby enabling the configuration of the most appropriate modules for the downstream task.
\textcolor{black}{Knowledge distillation introduces additional supervisory signals through the soft labels, guiding the constraint on $\mathcal{H}$~\cite{wang2024cross,hybrid_4,yang2024leveraging,hybrid_5,data_target,data_multi_0}.} Specifically, knowledge distillation involves using the outputs of a pre-trained teacher model as soft labels to guide the training of a student model, effectively transferring the teacher model's knowledge to the student model. \textcolor{black}{We classify these methods from a technical perspective into standard knowledge distillation and multi-level knowledge distillation. For the standard knowledge distillation,~\cite{wang2024cross} uses a teacher-student network, combined with self-supervised learning, to generate soft labels that guide the training of the student model.} 

Other methods~\cite{hybrid_4,data_multi_0,yang2024leveraging,hybrid_5,data_target} explore \textcolor{black}{multi-level knowledge distillation to solve the CDFSL problem.} Specifically, there are multi-to-one and one-to-multi teacher-student networks. For example,~\cite{hybrid_4} trains a student network with multiple teacher networks, where these teacher networks are trained on datasets from different domains, respectively. ~\cite{hybrid_5} aligns the representations of the single student network with the multiple task/domain-specific ones using low-capacity adapters. Additionally,~\cite{data_target} guides the learning of the student model through two teacher models, including the source and the target teacher models. ~\cite{data_multi_0} proposes a multi-level knowledge stage to train the student network using multiple teacher models. \textcolor{black}{In contrast,~\cite{yang2024leveraging} trains multiple student adapters in different stages, with the help of a single teacher network.}
% \textcolor{black}{Different from the parameter adjustment directly constrain $\mathcal{H}$, knowledge distillation introduces additional supervisory signals through the soft labels, indirectly constraining the model's hypothesis space. Specifically, It involves using the outputs of a pre-trained teacher model as soft labels to guide the training of a student model, effectively transferring the teacher model's knowledge to the student model. We classify these methods from a technical perspective into standard knowledge distillation and multi-level knowledge distillation. For the standard knowledge distillation,~\cite{wang2024cross} uses a teacher-student network, combined with self-supervised learning, to generate soft labels that guide the training of the student model.}
% \textcolor{black}{Knowledge distillation is a primary method for constraining the hypothesis space. It involves using the outputs of a pre-trained teacher model as soft labels to guide the training of a student model, effectively transferring the teacher model's knowledge to the student model. This process not only directs the learning of the student model but also introduces additional supervisory signals through the soft labels, further constraining the model's hypothesis space. To solve CDFSL problem,~\cite{wang2024cross} uses a teacher-student network and multiple cropping strategies, combined with self-supervised learning, to generate soft labels that guide the training of the student model, thereby enhancing the CDFSL performance. }

% The other methods~\cite{hybrid_4,yang2024leveraging,hybrid_5,data_target} explore multi-level knowledge distillation to solve the CDFSL problem. Specifically, there are multi-to-one and one-to-multi teacher-student networks. For example,~\cite{hybrid_4} trains a student network with the multiple teacher networks, these teacher networks are trained by the datasets from different domains, respectively.~\cite{hybrid_5} aligns the representations of the single network with the multiple task/domain-specific ones using small capacity adapters.~\cite{data_target} guides the learning of the student model through two teacher models, including the source and the target teacher models. And~\cite{data_multi_0} proposes a multi-level knowledge stage to train the student network with the multiple teacher models. \textcolor{black}{On the contrary,~\cite{yang2024leveraging} trains the multiple student adapters in the different stages, with the help of a single teacher network.}
% In addition to the standard knowledge distillation, some methods~\cite{hybrid_4,yang2024leveraging,hybrid_5,data_target} explore multi-level distillation to solve the CDFSL problem. Specifically, there are multi-to-one and one-to-multi teacher-student networks. For example,~\cite{hybrid_4} trains a student network with the multiple teacher networks, these teacher networks are trained by the datasets from different domains, respectively.~\cite{hybrid_5} distill the knowledge of multiple tasks/domain-specific networks into a single network. This is achieved by aligning the representations of the single network with the task/domain-specific ones using small capacity adapters.~\cite{data_target} guides the learning of the student model through two teacher models, including the source and the target teacher models.~\cite{data_multi_0} proposes a multi-level knowledge stage to train the student network with the multiple teacher models. \textcolor{black}{On the contrary,~\cite{yang2024leveraging} trains the multiple student adapters in the different stages, with the help of teacher network.}

\subsubsection{\textcolor{black}{Regularization}} \label{pa-weight}
% As depicted in Figure~\ref{parameterr} (c), the parameter reweighting technique optimizes the model's performance for the target domain by adjusting a limited number of parameters. Various studies have explored this approach to address the cross-domain challenge in few-shot learning. For instance,~\cite{parameter_weight_1} resets the parameters that were learned on the source domain before adapting to the target data. On the other hand,~\cite{parameter_fix_6} addresses the internal mismatch issue in BatchNorm by introducing the ``Visual Domain Bridge'' concept. Additionally,~\cite{parameter_weight_3} enhances the feature information by stacking a residual attention module into the feature encoder based on the residual network. Another study,~\cite{parameter_weight_4} trains task-specific weights from scratch on a small support set, as opposed to dynamically estimating them. Recent works like~\cite{ata} and~\cite{adversarial} propose adversarial methods to address the domain gap in few-shot learning, where~\cite{ata} considers the worst-case problem around the source task distribution and~\cite{adversarial} introduces a plug-and-play adversarial feature augmentation (AFA) method. Finally,~\cite{parameter_fix_9} adjusts the parameters of a novel Style Augmentation (StyleAug) module to achieve better performance in cross-domain few-shot learning.
\textcolor{black}{Regularization constrains $\mathcal{H}$ by adding penalty terms to the loss function or introducing randomness, such as dropout.} Some CDFSL approaches~\cite{feature_reweight_1,ji2023cross,parameter_select_1} apply regularization to limit $\mathcal{H}$. For example,\cite{feature_reweight_1} uses explainable feedback to guide model training, while \cite{ji2023cross,parameter_select_1} introduce dropout during training to impose constraints. Specifically, \cite{parameter_select_1} samples sub-networks by dropping individual neurons or entire feature maps, then selects the most suitable sub-networks to form an ensemble for target domain learning. \textcolor{black}{Additionally, \cite{ji2023cross} combines pruning with dropout to dynamically adjust the model's complexity.}
% \textcolor{black}{Regularization constrains the hypothesis space by adding penalty terms to the loss function or introducing randomness (such as dropout), thereby limiting model complexity, preventing overfitting.} Some existing CDFSL approaches~\cite{feature_reweight_1,ji2023cross,parameter_select_1} constrain $\mathcal{H}$ by introducing the regularization strategy. Among them,~\cite{feature_reweight_1} uses the explainable feedback information generated by the explanation module to guide the training of the model. Besides,~\cite{ji2023cross,parameter_select_1} introduce dropout into the training phase to constrain $\mathcal{H}$. Specifically,~\cite{parameter_select_1} samples sub-networks by dropping neurons or feature maps, and then choose the most suitable sub-networks to form an ensemble network for target domain learning. \textcolor{black}{Moreover,~\cite{ji2023cross} uses a combination of pruning and dropout strategies to dynamically adjust the complexity of the model.}
% \textcolor{black}{Regularization constrains the hypothesis space by adding penalty terms to the loss function or introducing randomness (such as dropout), thereby limiting model complexity, preventing overfitting, and improving generalization ability.} Some existing CDFSL approaches~\cite{feature_reweight_1,ji2023cross,parameter_select_1,hybrid_3} constrain $\mathcal{H}$ by introducing the regularization strategy. Among them,~\cite{feature_reweight_1} introduces the Explanation-Guided Training method and uses the explainability feedback information generated by the explanation module to guide the training of the model. And~\cite{hybrid_3} constrains the model training process by introducing task-level self-supervision consistency loss and metric learning loss. Besides,~\cite{ji2023cross,parameter_select_1} introduce dropout into the training phase to constrain $\mathcal{H}$. Specifically,~\cite{parameter_select_1} samples sub-networks by dropping neurons or feature maps, and then choose the most suitable sub-networks to form an ensemble for target domain learning. \textcolor{black}{Moreover,~\cite{ji2023cross} uses a dense-sparse-dense regularization mechanism to improve the CDFSL performance.}
\iffalse
\begin{table}%[b]
\tiny
\centering
\caption{Representative parameter-based CDFSL approaches. `NCA' means Neural Computing and Applications.}
%\vspace{-0.2cm}
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{llcm{5cm}cccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Strategy} & \textbf{Parameter} \textbf{operation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
 SB-MTL~\cite{parameter_fix_1}  & arXiv 2020 & Parameter freeze & Freeze partial layers in inner loop and update all network in outer loop & CE & & & \Checkmark      \\
% \hline 
FWT~\cite{feature-wise}  & ICLR 2020 & Parameter freeze & Alternately update the parameters of the feature-wise transformation layers and backbone & CE &  \Checkmark & &    \\
% \hline
DFTL~\cite{parameter_fix_3} & ICAICA 2021 & Parameter freeze & Following the training setup of~\cite{feature-wise}, and utilize multiple FWT modules in each layers  & CE & \Checkmark &  & \\  
% \hline
 MPL~\cite{parameter_fix_5} & TNNLS 2022 & Parameter freeze & Freeze network in inner loop and update it in meta update & CE & \Checkmark &  \Checkmark  &  \\
% \hline
FGNN~\cite{parameter_fix_7} & KBS 2022 & Parameter freeze & Following the training setup of~\cite{feature-wise} & Softmax & \Checkmark  &  & \\
% \hline
 AugSelect~\cite{parameter_select_1} & Big Data 2021 & Parameter selection & Select from multiple sub-network that obtained by dropping feature maps & CE & \Checkmark & \Checkmark & \\  
% \hline
 MAP~\cite{parameter_select_2} & arXiv 2021 & Parameter selection & Select from different modular adaptation pipeline & CE & \Checkmark & \Checkmark &   \\
% \hline
ATA~\cite{ata} & IJCAI 2021 & Parameter reweighting & Insert a plug-and play model-adaptive task augmentation module into backbone & CE & \Checkmark &  & \Checkmark  \\
% \hline
ReFine~\cite{parameter_weight_1} & CIKM 2022 & Parameter reweighting & Re-randomize the top layers of the feature extractor before fine-tuning on the target domain & CE &  &  & \Checkmark   \\
% \hline
VDB~\cite{parameter_fix_6} & CVPRW 2022 & Parameter reweighting & Introducing the ``Visual Domain Bridge'' into CNN's Batch Normalization (BN) layers & CE &  &  & \Checkmark   \\
% \hline
AFGR~\cite{parameter_weight_3} & NCA 2022 & Parameter reweighting & Reweight the backbone with a residual attention module & CE & \Checkmark &  &   \\
% \hline
TPA~\cite{parameter_weight_4} & CVPR 2022 & Parameter reweighting & The task-specific weights are learned to adjust model parameters & CE & \Checkmark & \Checkmark &   \\
% \hline
AFA~\cite{adversarial} & ECCV 2022 & Parameter reweighting & Use an adversarial feature augmentation module to simulate distribution variations & CE \& Gram-matrix & \Checkmark &  & \Checkmark  \\
% \hline
Wave-SAN~\cite{parameter_fix_9} & arXiv 2022 & Parameter reweighting & A StyleAug module is proposed to adjust the parameter & CE \& SSL \& Style & \Checkmark &  & \Checkmark\\
% \hline
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{table_parameter}
\end{table}
\fi
\iffalse
\begin{table}%[b]
\tiny
\centering
\caption{Representative $\mathcal{H}$-Constraint approaches.}
%\vspace{-0.2cm}
\setlength{\tabcolsep}{3.6mm}{
\begin{tabular}{llcccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Constrain $\mathcal{H}$ by} & \textbf{Operation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
SB-MTL~\cite{parameter_fix_1}  & arXiv 2020 & Parameter adjust & Adjust encoder models & CE & & & \Checkmark      \\
% \hline 
FWT~\cite{feature-wise} & ICLR 2020 & Parameter adjust & Adjust encoder models & CE & \Checkmark &  &    \\
% \hline
FGNN~\cite{parameter_fix_7} & KBS 2022 & Parameter adjust & Adjust encoder models & Softmax & \Checkmark &  &    \\
% \hline
ReFine~\cite{parameter_weight_1} & CIKM 2022 & Parameter adjust & Adjust encoder models & CE &  &  & \Checkmark   \\
% \hline
TPA~\cite{parameter_weight_4} & CVPR 2022 & Parameter adjust & Adjust adapter & CE & \Checkmark & \Checkmark &   \\
% \hline
TA2-Net~\cite{guo2023task} & ICCV 2023 & Parameter adjust & Adjust adapter & log &  & \Checkmark &   \\
% \hline
TAP~\cite{wu2024task} & AAAI 2024 & Parameter adjust & Adjust prompt & CE &  & \Checkmark &   \\
% \hline
HybridPrompt~\cite{wu2024hybridprompt} & IJCV 2024 & Parameter adjust & Adjust prompt & CE \& Log &  & \Checkmark &   \\
% \hline
MC-TS~\cite{wang2024cross} & ENG APPL ARTIF INTEL 2024 & Knowledge Distillation & Standard & CE \& KL \& entropy &  &  & \Checkmark  \\
% \hline
URL~\cite{hybrid_4}   &  ICCV 2021 & Knowledge Distillation & Multi-to-one & CE \& CKA \& KL & \Checkmark & \Checkmark &     \\
% \hline
ME-D2N~\cite{data_target} & ACM MM 2022 & Knowledge Distillation & Multi-to-one & CE \& KL & \Checkmark &  &   \\
% \hline
SET-RCL~\cite{hybrid_5}   &  ACM MM 2022 & Knowledge Distillation & Multi-to-one & CE \& Contrastive & \Checkmark & \Checkmark     \\
% \hline
TKD-Net~\cite{data_multi_0} & FRONT COMPUT SCI-CHI 2023 & Knowledge Distillation & Multi-to-one & CE \& KL \& $L_{2}$ & \Checkmark &   &     \\
% \hline
ProLAD~\cite{yang2024leveraging}   & AAAI 2024 & Knowledge Distillation & One-to-multi & CE &  & \Checkmark &     \\
% \hline
LRP~\cite{feature_reweight_1} & ICPR 2020 & Regularization & Explanation-guided & CE & \Checkmark &  &   \\
% \hline
TL-SS~\cite{hybrid_3}  &  AAAI 2022  & Regularization & Loss & CE \& KL & \Checkmark &  &      \\  
% \hline
AugSelect~\cite{parameter_select_1} & Big Data 2021 & Regularization & Dropout & CE & \Checkmark & \Checkmark & \\  
% \hline
L2-SP~\cite{ji2023cross} & TCSVT 2024 & Regularization & Dropout & CE \& $L_{2}$ & \Checkmark & \Checkmark & \\  
% \hline
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{table_parameter}
\end{table}
\fi

\subsubsection{Discussion and Summary}
% The parameter freeze strategy, as discussed in Section~\ref{fixed}, is often combined with meta-learning techniques. In the meta-training phase, two pseudo-domains, namely the pseudo-seen and pseudo-unseen domains, are used to simulate the cross-domain scenario. However, it is important to note that both of these domains are derived from the seen domain, resulting in a relatively small domain distance between them. As a result, algorithms that employ this strategy may not be effective in addressing the distant-domain problem in cross-domain few-shot learning (CDFSL).
% \textcolor{black}{Instead of expanding limited information, $\mathcal{H}$-Constraint enables the model to find the optimal solution in the target domain by restricting the model's complexity, allowing it to do so with only a small amount of data. However, effectively constraining $\mathcal{H}$ typically relies on an understanding of the relationship between the source and target domains, which requires certain prior knowledge to establish appropriate constraints. If this prior knowledge is lacking or inaccurate, it could result in constraints that are either too strict or too lenient, thereby affecting the model's performance~\cite{sun2022prior}. The factors that may lead to improper constraint setting for each strategy are presented in the following sections.}

\textcolor{black}{%The parameter adjust strategy, as discussed in Section~\ref{fixed}, mainly adjust some parameters of encoder, the extra adapter, and the prompt. 
Alternating the adjustment of encoder model parameters allows for optimization across different tasks and datasets, improving model performance. However, this requires a well-designed strategy to avoid local optima, as an inappropriate adjustment strategy may lead to overfitting and reduced generalization~\cite{srivastava2014dropout}. Extra adapters and prompts can be flexibly inserted into existing models~\cite{houlsby2019parameter} to facilitate parameter updates, and prompt learning can achieve better performance with limited training data~\cite{gao2021making}. However, the design and selection of prompts are critical, as each task requires tailored prompts for optimal performance~\cite{brown2020language}.}
% By alternately adjusting some parameters of the encoder model, it is possible to optimize for different tasks and datasets and improve model performance. However, this requires a rigorous algorithm to design an adjustment strategy to avoid falling into a local optimal solution. If the adjustment strategy is inappropriate, it may lead to overfitting of the model, thereby reducing the generalization performance~\cite{srivastava2014dropout}. While the extra adapters and prompts can be flexibly inserted into existing models~\cite{houlsby2019parameter} to facilitate parameter adjustment and updating. In addition, through prompt learning, better performance can be obtained with a small amount of training data~\cite{gao2021making}. However, the design and selection of prompts have a great impact on performance. Different tasks require different prompts, and improper design may lead to performance degradation~\cite{brown2020language}.}

% The parameter selection strategy (Section~\ref{pa-select}) aims to adapt to the target domain by selecting the most suitable set of parameters from a pool of options. While this approach can be effective, it has a limited range of parameter sets to choose from, potentially limiting its ability to find the optimal set for the target domain. Moreover, some implementations of this strategy attempt to incorporate various techniques such as semi-supervised learning, domain adaptation, and fine-tuning within a single framework, resulting in a cumbersome and complex approach, making the framework bulky.
\textcolor{black}{For the knowledge distillation strategy (Section~\ref{pa-select}), by learning from the soft labels generated by the teacher model, the student model can better handle noisy data and label errors, thereby improving its robustness. However, the effectiveness of knowledge distillation depends on the quality of the teacher model—if the teacher model performs poorly, the student model will also be negatively affected~\cite{hinton2015distilling}. Additionally, knowledge distillation requires extra training steps, increasing both the complexity and duration of the training process~\cite{gou2021knowledge}.}
% \textcolor{black}{For the knowledge distillation strategy (Section~\ref{pa-select}), under the guidance of the teacher model, the student model can achieve better performance and generalization ability. In addition, by learning the soft labels of the teacher model, the student model can better cope with noisy data and label errors and improve the robustness of the model. Furthermore, by distilling knowledge from multiple teacher models, multi-task learning is achieved and the performance of the model on multiple tasks is improved. However, the effect of knowledge distillation depends on the quality of the teacher model. If the teacher model itself performs poorly, the student model will also be affected~\cite{hinton2015distilling}, especially in the CDFSL problem. In addition, knowledge distillation requires additional training steps, which increases the complexity and time of training~\cite{gou2021knowledge}.}

% The parameter reweighting strategy (Section~\ref{pa-weight}) seeks to enhance the model's generalization capability through minimal parameter adjustments. This approach is critical to improving the model's performance. However, most existing reweighting methods utilize simple structures, which often result in limited improvements in terms of generalization. Therefore, further research is necessary to explore more complex and effective approaches to parameter reweighting in CDFSL.
\textcolor{black}{Regularization (Section~\ref{pa-weight}) constrains the magnitude of weights, helping the model generalize better to unseen data~\cite{goodfellow2016deep}. However, in scenarios like small sample learning, regularization may not significantly improve performance~\cite{ng2004feature}. Additionally, if regularization parameters are not properly selected, such as when the regularization strength is too high—the model may be over-constrained, limiting its ability to learn complex patterns. In such cases, overfitting may not be effectively prevented.}
% \textcolor{black}{Regularization (Section~\ref{pa-weight}) limits the size of weights so that the model can better generalize to unseen data~\cite{goodfellow2016deep}. However, in some cases, such as small sample learning scenarios, the impact of regularization on model performance may not be as significant as expected, and may even lead to performance degradation~\cite{ng2004feature}. When the regularization parameters are not properly selected, such as when the regularization strength is too high, the model may be over-constrained and unable to fully learn the complex patterns in the data. On the contrary, overfitting cannot be effectively prevented.}

\subsection{\textcolor{black}{$\Delta$-Adaptation}}  \label{adaptation}
% In CDFSL, the transferable feature representation is achieved through post-processing of the original features, as illustrated in Figure~\ref{feature-post}. The post-processing strategies include feature selection, feature fusion, and feature transformation. Feature selection involves choosing the features from multiple domains that best fit the target domain. Feature fusion combines multiple features to generate a generalized feature representation. Lastly, feature transformation adjusts the original features using learnable weights. Table~\ref{table_feature} shows the related works in detail.
\textcolor{black}{In FSL, source and target tasks usually come from the same domain, meaning $\Delta$ is nearly zero. However, in CDFSL, the gap between the source and target tasks can be much larger, leading to an unreliable TSERM. To address this, it’s important to reduce the disparity $\Delta$ between the tasks. This can be done in two ways: (1) adapting the features of both tasks to minimize differences, or (2) exploring and leveraging shared information to improve robustness even when differences remain. Based on how $\Delta$ is optimized, methods in this section are categorized into two groups (Figure~\ref{feature-post}): (1) Feature adaptation and (2) Shared information mining.}
% \textcolor{black}{$\Delta$-Adaptation aims at solving CDFSL problem by reducing the distance between the source and target tasks, as illustrated in Figure~\ref{feature-post}. One manner is adjusting the features to achieve this goal (feature adaptation), while another one is mining and utilizing the shared information between the source and the target domains (shared information mining). Feature adaptation adjusts the feature representation to make the feature distributions of the source and target tasks as similar as possible, thereby improving the model's performance on the target task. Shared information mining identifies and utilizes common information across different tasks or domains to reduce distributional differences between them.} Table~\ref{table_feature} shows the related works in detail.

\begin{figure}%[b]
	\centering
  \vspace{-0.3cm}
	%\includegraphics[width=\linewidth]{cdfsl-feature-2.pdf}
	%\includegraphics[width=\linewidth]{response/adaptation_cropped.pdf}
 \includegraphics[width=\linewidth]{response/crop_adaptation1.pdf}
 \vspace{-0.6cm}
	\caption{\textcolor{black}{The $\Delta$-Adaptation categories, including feature adaptation and shared information mining. Feature adaptation involves feature transformation and feature alignment. Shared information mining involves selection, decoupling, and meta-knowledge mining. The core operations of methods are highlighted in red.}}
 \vspace{-0.4cm}
	\label{feature-post}
\end{figure}

\subsubsection{\textcolor{black}{Feature Adaptation}} \label{select}
% The feature selection strategy involves identifying features closest to the target domain for use as the optimal shared feature representation. This approach is often employed in conjunction with the introduction of multi-domain instances. The strategy first obtains multiple features from different source domains, then selects some of them to aid target domain adaptation. As depicted in Figure~\ref{feature-post} (a),~\cite{feature_select_1} presents a Representative Multi-Domain Feature Selection (RMFS) algorithm to optimize the multi-domain feature extraction and selection process. While~\cite{feature_select_2} extracts a multi-domain representation by training a set of feature extractors and then automatically selecting the representations most relevant to the target domain.
\textcolor{black}{Feature adaptation aims to reduce the differences between the source and target domains by adjusting or transforming their features. Since the feature forms or structures of the source and target domains may exhibit fundamental differences, these disparities are often difficult to reconcile directly in the original feature space. Therefore, depending on whether the original feature space is preserved, the methods in this section are categorized into two approaches: feature transformation~\cite{feature_reweight_4,parameter_fix_8,wang2023mmt,feature_reweight_2,liu2022geometric,li2023knowledge}, which modifies the feature space, and feature alignment~\cite{paeedeh2024cross,feature_reweight_9,lscdfsl,zhao2023dual,xu2024enhancing,feature_reweight_5,parameter_fix_6}, which attempts to align features without altering the original space. % Feature transformation~\cite{feature_reweight_4,feature_reweight_5,parameter_fix_6,wang2023mmt,feature_reweight_2,xu2024enhancing} aligns the features of the source and target domains in a new space through certain transformations applied to the original features. Feature Alignment~\cite{paeedeh2024cross,feature_reweight_9,lscdfsl,zhao2023dual} minimizes domain discrepancy by adjusting the feature distributions of the source and target domains to achieve consistent representations within the original feature space.
}

\iffalse
\textbf{Normalization Adjustment.}
Some approaches~\cite{feature_reweight_5,parameter_fix_6} reduce the distribution distance between source and target tasks by adjust the parameters of normalization. Specifically,~\cite{feature_reweight_5} explores an instance normalization algorithm and introduces a new freamwork to alleviate feature dissimilarity. Besides,~\cite{parameter_fix_6} addresses the internal mismatch issue in batchNorm by introducing the visual domain bridge concept.
\fi

\textbf{Feature Transformation.}
\textcolor{black}{Feature transformation~\cite{feature_reweight_4,parameter_fix_8,wang2023mmt,feature_reweight_2,liu2022geometric,li2023knowledge} aligns the features of the source and target domains by applying transformations to the original feature space.} For instance, \cite{feature_reweight_4} enhances CDFSL performance by using adaptive feature distribution transformations and task-adaptive strategies to measure domain discrepancies. \cite{feature_reweight_2} generates inductive meta points to abstract and transform features, while~\cite{parameter_fix_8} applies scale and shift operations via a feature transformation module. \textcolor{black}{\cite{wang2023mmt} uses memory components for effective feature transformation and alignment between domains. Similarly, \cite{liu2022geometric} maps features into a high-dimensional geometric algebra space to preserve relationships across domains, and~\cite{li2023knowledge} refines the model's understanding of domain-specific features, aligning learned features with the target domain's characteristics.}
% Feature transformation~\cite{feature_reweight_4,parameter_fix_8,wang2023mmt,feature_reweight_2,xu2024enhancing} includes both linear and nonlinear transformations. Among them,~\cite{feature_reweight_4} proposes an effective method for measuring domain discrepancies and significantly enhances CDFSL performance through adaptive feature distribution transformation and task-adaptive strategies.~\cite{feature_reweight_2} transform the features by explores a bi-level episode strategy. Then, by generating inductive meta points, it further transforms and abstracts the features, enabling better propagation and alignment within the graph structure.~\cite{parameter_fix_8} enhances the model's FSL performance across different domains through the proposed feature transformation module. \textcolor{black}{Besides, some approaches~\cite{feature_reweight_5,parameter_fix_6} reduce the distribution distance by adjusting normalization. Specifically,~\cite{feature_reweight_5} explores an instance normalization algorithm and introduces a new framework to alleviate feature dissimilarity. Besides,~\cite{parameter_fix_6} addresses the internal mismatch issue in batchNorm by introducing the visual domain bridge concept. Furthermore,~\cite{wang2023mmt} achieves effective transformation and alignment of features between the source and target domains through Meta-Memory Transfer, enhancing knowledge transfer between these domains.~\cite{xu2024enhancing} employs distance-aware contrastive learning and information maximization to transform and optimize the features of the target domain, enhancing feature consistency and distinctiveness, and thereby reducing domain discrepancy.}

\textbf{Feature Alignment.}
\textcolor{black}{Feature alignment~\cite{paeedeh2024cross,feature_reweight_9,lscdfsl,zhao2023dual,xu2024enhancing,feature_reweight_5,parameter_fix_6} aims to adjust the feature distributions of the source and target domains to achieve consistent representations.} In~\cite{lscdfsl}, the features are mapped into a shared latent subspace to align source and target features. \textcolor{black}{Similarly, \cite{paeedeh2024cross} achieves alignment using transformer blocks,} while \cite{feature_reweight_9} learns cross-domain aligned representations through bi-directional feature alignment. \textcolor{black}{\cite{xu2024enhancing} optimizes target domain features via information maximization, implicitly aligning features. Additionally, \cite{feature_reweight_5} uses instance normalization to reduce feature dissimilarity, and \cite{parameter_fix_6} finetunes Batch Normalization (BN) layers to correct inter-domain distribution differences. Finally,~\cite{zhao2023dual} combines prototypical feature alignment with normalized distribution alignment.}
% Some other methods~\cite{paeedeh2024cross,feature_reweight_9,lscdfsl,zhao2023dual} reduce the distance between source and target tasks through the feature alignment. In~\cite{lscdfsl}, a linear auto-encoder is used to map features from the source and target domains into a shared latent subspace, aligning these features within this subspace to reduce domain discrepancy. \textcolor{black}{And~\cite{paeedeh2024cross} utilizes quadruple transformer blocks to align the features of the source and target domains through an adaptive transformer network.}~\cite{feature_reweight_9} learns compact, cross-domain aligned representations through a bi-directional feature alignment.\textcolor{black}{~\cite{zhao2023dual} combines prototypical feature alignment and normalized distribution alignment. This dual adaptive representation alignment effectively reduces the feature distribution discrepancy between source and target domains.}

\subsubsection{\textcolor{black}{Shared Information Mining}} \label{fuse}
% Feature fusion is an approach used to enhance the generalization ability of models. As depicted in Figure~\ref{feature-post} (b), this strategy combines features from different sources or dimensions into a single representation to improve FSL performance on the target domain. Many works, influenced by~\cite{shallow_layer}, believe that features from shallower layers are more transferable than those from deeper layers. Hence,~\cite{rf} proposed the CHEF method which unifies different abstraction levels of a deep neural network into one representation. Additionally,~\cite{feature_fusion_2} combined mid-level features to learn the discriminative information of each sample. Similarly,~\cite{parameter_weight_2} used a hierarchical prototype model to combine information from hierarchical memory into final prototype features. Unlike the fusion of shallow layer features, in~\cite{feature_fusion_3}, the representation of graphs is obtained by augmenting the graphs from sampled tasks into three views: one contextual and two geometric, and encoding each view with a dedicated encoder. Finally, the representations are aggregated into a single graph representation using an attention mechanism. The right part of Figure~\ref{feature-post} (b) shows the features from different network layers are fused, whereas the left part shows that features from a set of different networks are stacked.
\textcolor{black}{Shared information mining focuses on efficiently leveraging common features, patterns, or knowledge between the source and target domains to enhance model generalization in cross-domain learning. These shared features must be both transferable across domains and relevant to the source and target tasks. Based on the number of tasks involved in extracting shared information, existing methods are generally divided into three categories: selection, decoupling, and meta-knowledge mining.}
% \textcolor{black}{Shared information mining encompasses selection, decoupling, and meta-knowledge mining. Selection~\cite{feature_select_1,feature_select_2,confess,zhang2023cross,data_multi_1,parameter_select_2} includes feature selection and task optimization, aiming to enhance target task performance by identifying shared information between source and target tasks. Decoupling~\cite{xu2023cross,data_target_1} seeks to separate shared features from domain-specific ones to improve model generalization across various domains. By decoupling, models focus solely on task-relevant features and ignore irrelevant domain-specific features, thereby reducing the impact of domain differences on model performance. While meta-knowledge mining~\cite{parameter_fix_5,data_multi_2} involves learning domain-agnostic meta-knowledge and use it to get an excellent initialization for the target task.}

\textbf{Selection.}
\textcolor{black}{Selection operates within a single task, focusing on filtering features to identify the most relevant information~\cite{feature_select_1,feature_select_2,confess,data_multi_1,parameter_select_2,zhang2023cross}. The selected information is then used as shared knowledge. Methods in this section are classified into feature selection, parameter weighting, and task optimization.} Feature selection~\cite{feature_select_1,feature_select_2,confess} identifies general feature components that aid the target FSL task. For instance, \cite{feature_select_1} proposes a multi-domain feature selection algorithm to optimize feature extraction and selection, while \cite{feature_select_2} uses multiple feature extractors to select representations relevant to the target domain. Additionally, \cite{confess} uses a learnable mask generator to identify shared features. Parameter weighting \cite{data_multi_1,parameter_select_2} adjusts model parameters to adapt to the target task. For example, \cite{data_multi_1} aggregates meta-model parameters from multiple domains to generate initialization parameters, and \cite{parameter_select_2} uses a dynamic mechanism to configure the best adaptation modules for the downstream task. \textcolor{black}{Finally, task optimization~\cite{zhang2023cross} focuses on selecting the best source tasks to learn valuable knowledge for the target task.}
% Selection includes feature selection, parameter weighting and task optimization, in which feature selection~\cite{feature_select_1,feature_select_2,confess,parameter_select_2} involves select the general parts of features to help the target FSL task, parameter weighting~\cite{data_multi_1} weights the parameters to adapt the target task, \textcolor{black}{and task optimization~\cite{zhang2023cross} aims at select source tasks that could help the target task.} In~\cite{feature_select_1}, a multi-domain feature selection algorithm is proposed to optimize the multi-domain feature extraction and selection process. While~\cite{feature_select_2} extracts a multi-domain representation by training a set of feature extractors and then automatically selecting the representations most relevant to the target domain.~\cite{data_multi_1} . Moreover~\cite{confess} select the shared features by a learnable mask generator. In addition,~\cite{data_multi_1} obtains meta-model parameters from multiple known domains and combines these parameters through weighted aggregation to generate new initialization parameters, enabling effective learning in a new task.~\cite{parameter_select_2} proposes a dynamic selection mechanism by sequentially applying multiple state-of-the-art adaptation methods, thereby enabling the configuration of the most appropriate modules for the downstream task. \textcolor{black}{Furthermore,~\cite{zhang2023cross} select the optimize meta-tasks by the task association modeling and task sample probability. The selected tasks are used in the meta-training phase to learn the valid knowledge for the target tasks.}

\textbf{Decoupling.}
\textcolor{black}{In CDFSL, source and target task features share both similarities and differences. Decoupling~\cite{xu2023cross,data_target_1,feature_reweight_8} aims to separate shared information from task-specific information to handle them independently and avoid interference. In~\cite{xu2023cross}, class-shared and class-specific dictionaries are combined to align source and target features, with the class-shared dictionary capturing common elements and class-specific dictionaries representing domain-specific features.} Similarly, \cite{data_target_1} employs domain-specific and domain-general encoders to support the learner, while \cite{feature_reweight_8} uses a domain discriminator to distinguish between source and target features, allowing the extractor to learn domain-invariant features and reduce domain discrepancies.
% ~\cite{xu2023cross,data_target_1,feature_reweight_8} separate shared information from specific information through decoupling. \textcolor{black}{In~\cite{xu2023cross}, class-shared and class-specific dictionaries are combined to align the feature representations of the source and target domains, where the class-shared dictionary captures common features between the source and target domains, while the class-specific dictionaries capture the unique features of each domain.} In~\cite{data_target_1}, a domain-specific prompt and a domain-general teacher are learned to assist the learner.~\cite{feature_reweight_8} uses a domain discriminator to distinguish between source and target domain features, enabling the feature extractor to learn domain-invariant features, reducing the domain discrepancy.

\textbf{Meta-knowledge Mining.}
\textcolor{black}{Meta-knowledge mining~\cite{parameter_fix_5,data_multi_2} operates across tasks and domains, extracting higher-order learning strategies that enable models to quickly adapt to new contexts.} In~\cite{parameter_fix_5}, meta-learning is applied across domain tasks during training to generate prototypes suited for new tasks. Similarly,~\cite{data_multi_2} introduces a domain-agnostic meta-learning method that learns domain-independent initial parameters by observing both seen and pseudo-unseen tasks simultaneously.
% Some methods~\cite{parameter_fix_5,data_multi_2}learns the meta-knowledge to represent the general information between source and target tasks.~\cite{parameter_fix_5} combines meta-learning and prototypical networks by conducting meta-learning on different tasks during the training phase to generate prototypes that are well-suited for new tasks. Similarly,~\cite{data_multi_2} proposes a domain-agnostic meta-learning method that learns domain-independent initial parameters by simultaneously observing tasks from seen domains and pseudo-unseen domains. This approach enables the model to adapt to novel classes in new domains during the meta-testing phase.

\iffalse
\begin{table}%[b]
\tiny
\centering
\caption{Representative feature post-processing CDFSL approaches. `GR' represents geometrical regularization.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{llcm{5cm}cccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Strategy} & \textbf{Feature operation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
SUR~\cite{feature_select_2} & ECCV 2020 & Feature selection & Leverage the multi-domain feature bank to autonomously identify the most pertinent representations & CE & \Checkmark & \Checkmark  &  \\
% \hline
RMFS~\cite{feature_select_1} & IC-NIDC 2021 & Feature selection & extract the multi-domain features and select from them & CE & \Checkmark & \Checkmark &       \\
% \hline
CHEF~\cite{rf} & arXiv 2020 & Feature fusion & Accomplish the representation fusion through an ensemble of Hebbian learners operating on diverse layers of the network & CE &  &  & \Checkmark \\  
% \hline
MLP~\cite{feature_fusion_2} & ACM MM 2021 & Feature fusion & Weight the fusion of mid-level features and investigate a residual-prediction task & CE \& $L_{2}$ & \Checkmark  & \Checkmark & \\
% \hline
HVM~\cite{parameter_weight_2} & ICLR 2022 & Feature fusion & The mid-level features are weighted and fused in a hierarchical prototype model & CE \& KL &  &  & \Checkmark \\  
% \hline
LRP~\cite{feature_reweight_1} & ICPR 2020 & Feature transformation & Develop a model-agnostic explanation-guided training strategy that dynamically finds and emphasizes the features which are important for the predictions & CE & \Checkmark &  &   \\
% \hline
BL-ES~\cite{feature_reweight_2} & ICME 2021 & Feature transformation & An inductive graph network (IGN) is optimizaed by MPGN module, in which include multiple features & BCE \& GR & \Checkmark &  & \Checkmark  \\
% \hline
DeepEMD-SA~\cite{feature_reweight_3} & ISCIPT 2021 & Feature transformation & Employs an attention module to enable interaction between the local features & CE &  &  & \Checkmark\\
% \hline
TACDFSL~\cite{feature_reweight_4} & Symmetry 2022 & Feature transformation & Propose the adaptive feature distribution transformation & CE &  &  & \Checkmark   \\
% \hline
MemREIN~\cite{feature_reweight_5} & IJCAI 2022 & Feature transformation & Explore an instance normalization algorithm and a memorized module to transform the original features
 & CE \& Contrastive & \Checkmark &  &    \\
% \hline
RDC~\cite{feature_reweight_6} & CVPR 2022 & Feature transformation & Transform and reweight the original features through hyperbolic tangent transformation & CE \& KL & \Checkmark &  & \Checkmark  \\
% \hline
FUM~\cite{feature_reweight_7} & PR 2022 & Feature transformation & Using a forget-update module to regulate the features & CE & \Checkmark & \Checkmark &  \\
% \hline
ConFeSS~\cite{confess} & ICLR 2022 & Feature transformation & Utilizing a masking module to select relevant information that are more suited to target domain in the features & CE \& Divergence &  &  & \Checkmark \\
% \hline
StabPA~\cite{feature_reweight_9} & ECCV 2022 & Feature transformation & Transform features through learning prototypical compact and cross-domain aligned representations & Softmax & \Checkmark & \Checkmark &  \\
% \hline
StyleAdv~\cite{feature_reweight_0} & CVPR 2023 & Feature transformation & Introducing variations to the initial style using the signed style gradients & CE \& KL & \Checkmark &  & \Checkmark  \\
% \hline
TCT-GCN~\cite{feature_reweight_11} & SSRN 2023 & Feature transformation & Combining the multi-levelf feature fusion and feature transform & CE & \Checkmark &  & \Checkmark\\
% \hline
\hline
\end{tabular}}
\vspace{-0.4cm}
\label{table_feature}
\end{table}
\fi
\iffalse
\begin{table}%[b]
\tiny
\centering
\caption{Representative $\Delta$-Adaptation approaches.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{3.6mm}{
\begin{tabular}{llcccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Reduce $\Delta$ by} & \textbf{Operation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
MemREIN~\cite{feature_reweight_5} & IJCAI 2022 & Feature adaptation & Normalization Adjustment
 & CE \& Contrastive & \Checkmark &  &    \\
% \hline
VDB~\cite{parameter_fix_6} & CVPRW 2022 & Feature adaptation & Normalization Adjustment & CE &  &  & \Checkmark   \\
% \hline
BL-ES~\cite{feature_reweight_2} & ICME 2021 & Feature adaptation & Feature transformation & BCE \& GR & \Checkmark &  & \Checkmark  \\
% \hline
TACDFSL~\cite{feature_reweight_4} & Symmetry 2022 & Feature adaptation & Feature transformation & CE &  &  & \Checkmark   \\
% \hline
FTM~\cite{parameter_fix_8} & ICPR 2022 & Feature adaptation & Feature transformation & CE &  &  & \Checkmark   \\
% \hline
HVM~\cite{parameter_weight_2} & ICLR 2022 & Feature adaptation & Feature transformation & CE \& KL &  &  & \Checkmark \\  
% \hline
MMT~\cite{wang2023mmt} & TPAMI 2023 & Feature adaptation & Feature transformation & CE \& $L_{2}$ &  & \Checkmark &  \\  
% \hline
IM-DCL~\cite{xu2024enhancing} & TIP 2024 & Feature adaptation & Feature transformation & CE \& $L_{2}$ &  &  & \Checkmark \\ 
% \hline
TriAE~\cite{lscdfsl} & ACCV 2020 & Feature adaptation & Feature alignment & $L_{2}$ &  &  & \Checkmark \\ 
% \hline
StabPA~\cite{feature_reweight_9} & ECCV 2022 & Feature adaptation & Feature alignment & Softmax & \Checkmark & \Checkmark &  \\
% \hline
DARA~\cite{zhao2023dual} & TPAMI 2023 & Feature adaptation & Feature alignment & CE & \Checkmark &  & \Checkmark \\ 
% \hline
ADAPTER~\cite{paeedeh2024cross} & KBS 2024 & Feature adaptation & Feature alignment & CE &  &  & \Checkmark \\ 
% \hline
SUR~\cite{feature_select_2} & ECCV 2020 & Shared Information Mining & Selection & CE & \Checkmark & \Checkmark  &  \\
% \hline
CosML~\cite{data_multi_1} & arXiv 2020 & Shared Information Mining & Selection & CE & \Checkmark &  &  \\
% \hline
RMFS~\cite{feature_select_1} & IC-NIDC 2021 & Shared Information Mining & Selection & CE & \Checkmark & \Checkmark &       \\
% \hline
MAP~\cite{parameter_select_2} & arXiv 2021 & Shared Information Mining & Selection & CE & \Checkmark & \Checkmark &   \\
% \hline
ConFeSS~\cite{confess} & ICLR 2022 & Shared Information Mining & Selection & CE \& Divergence &  &  & \Checkmark \\
% \hline
CDTC~\cite{zhang2023cross} & AAAI 2023 & Shared Information Mining & Selection & CE &  &  &  \\
% \hline
DAPN~\cite{feature_reweight_8} & WACV 2021 & Shared Information Mining & Decoupling & CE\& Adversarial & \Checkmark &  &  \\
% \hline
DSL~\cite{data_target_1} & ICLR 2022 & Shared Information Mining & Decoupling & CE\& Re-weighted CE \& BKLD & \Checkmark &  &  \\
% \hline
CSCSD~\cite{xu2023cross} & PR 2023 & Shared Information Mining & Decoupling & CE & \Checkmark &  &  \\
% \hline
MPDA~\cite{parameter_fix_5} & TNNLS 2021 & Shared Information Mining & Meta-knowledge mining & CE &  & \Checkmark &  \\
% \hline
DAML~\cite{data_multi_2} & ICLR 2022 & Shared Information Mining & Meta-knowledge mining & CE & \Checkmark & \Checkmark &  \\
% \hline
\hline
\end{tabular}}
\vspace{-0.4cm}
\label{table_feature}
\end{table}
\fi

% \subsubsection{Feature Transformation} \label{reweights}
% The feature transformation strategy reweights features to improve performance, as shown in Figure~\ref{feature-post} (c). Some methods obtain the weights through a transformation and weighting, \eg the right part of Figure~\ref{feature-post} (c), while others use a learnable module, \eg the left part of Figure~\ref{feature-post} (c). For the former category, in~\cite{feature_reweight_4}, WDMDS (Wasserstein Distance for Measuring Domain Shift) and MMDMDS (Maximum Mean Discrepancy for Measuring Domain Shift) were proposed to solve CDFSL.~\cite{feature_reweight_5} introduced the MemREIN framework which considers memorization, restitution, and instance normalization, \eg an instance normalization algorithm is explored to alleviate feature dissimilarity. And~\cite{feature_reweight_6} minimizes task-irrelevant features while keeping more transferrable discriminative information by constructing a non-linear subspace and using a hyperbolic tangent transformation. Furthermore, a novel model-agnostic meta style adversarial training (StyleAdv) method together with a novel style adversarial attack method is proposed for CDFSL in~\cite{feature_reweight_0}.

% Additionally, there are methods that use a learnable module to determine the feature weights. For example,~\cite{feature_reweight_1} computes explanation scores for intermediate features and reweights them accordingly.~\cite{feature_reweight_2} acquires the weights by training a bi-level episode strategy (BL-ES) to weight the features. And~\cite{feature_reweight_3} employs an attention module upon a local-descriptor-based model called DeepEMD to enable interaction between the local features. Furthermore,~\cite{feature_reweight_7} reweights features through extracting relationship embeddings using Forget-Update Modules (FUM). And recently,~\cite{confess} employed a masking module to reweight features, selecting those that are more suited to the target domain. And a task context transformer and graph convolutional network (TCT-GCN) method is proposed in~\cite{feature_reweight_11}. Lastly, some methods address the CDFSL problem by combining domain adaptation and few-shot learning methods. For instance,~\cite{feature_reweight_9} proposes stabPA to learn compact, cross-domain aligned representations.

% \subsubsection{\textcolor{black}{Adversarial Learning}} \label{adver}
% \textcolor{black}{Some approaches~\cite{feature_reweight_8,boosting} use adversarial techniques to improve the model's generalization across different domains.} For example,~\cite{feature_reweight_8} uses a domain discriminator to distinguish between source and target domain features, enabling the feature extractor to learn domain-invariant features, reducing the domain discrepancy. Unlike directly employing adversarial techniques,~\cite{boosting} enhances model robustness by generating noisy data, achieving a similar effect to adversarial learning.

\subsubsection{Discussion and Summary}
% Feature selection strategies can be helpful in selecting the most suitable features for the target domain in the presence of multi-domain or auxiliary views data, as discussed in Section~\ref{select}. However, when there are no multiple domains available, features from a single source domain may have limited variability, which means selecting different features from the same source domain may not significantly improve the performance of FSL on the target domain.
\textcolor{black}{
Feature adaptation strategies (Section~\ref{select}) typically achieve alignment by minimizing distribution differences between the source and target domains. These methods are flexible, easy to implement, and can be applied to various tasks and datasets. However, in focusing on matching overall distribution statistics (e.g., mean and variance), they often overemphasize global feature consistency, potentially overlooking the local or specific information crucial for accurately modeling the target domain~\cite{dasurvey1}.
% The $\Delta$-Adaptation approach helps extract cross-domain commonalities to obtain information beneficial for FSL tasks in the target domain. Feature adaptation strategies (Section~\ref{select}) can be flexibly applied to various tasks and datasets, and are typically straightforward and simple, making them easy to implement and integrate into existing systems. Shared information mining aims to search for and utilize common information between domains (presented in Section~\ref{fuse}), which can be applied to a variety of tasks and data types, especially in multi-domain and multi-task learning scenarios, and can effectively improve the overall performance.
}

\textcolor{black}{Shared information mining seeks to identify and leverage common knowledge between domains (Section~\ref{fuse}) and can be applied to single-task, dual-task, and multi-task settings. However, its effectiveness may be compromised by conflicting task objectives~\cite{zhao2019learning}. For instance, features beneficial to one task may be irrelevant or harmful to another, potentially confusing the model during training. To mitigate this, some methods incorporate task-specific knowledge alongside shared information. Rather than directly applying shared features,\cite{data_target_1} constrains their use through domain-specific insights, while\cite{zhang2023cross} evaluates the impact of shared features on the target domain and leverages them accordingly.}

% \textcolor{black}{However, $\Delta$-Adaptation may not be able to ensure the balance between shared information and specific information. For example, feature adaptation strategies typically achieve alignment by minimizing the distribution differences between the source and target domains. However, in the process of smoothing these differences, such strategies may inadvertently weaken or overlook important features that are unique to the target domain, potentially compromising the model's performance in that domain. Additionally, inherent biases in feature selection may lead the model to focus more on prominent features from the source domain, while neglecting key characteristics of the target domain~\cite{Co-training for domain adaptation}. Given that the source domain often has more labeled data, the model tends to optimize for those features, which can result in sub-optimal performance when applied to the target domain. While shared information mining naturally prioritize and compress common features between the source and target domains, while overlooking or diminishing target-specific features. This approach can reduce inter-domain differences to some extent but also limits the model's ability to utilize unique information from the target domain, thereby affecting its overall performance.}


%Feature adaptation strategies (Section~\ref{select}) can be flexibly applied to various tasks and datasets, and are typically straightforward and simple, making them easy to implement and integrate into existing systems. However, \textcolor{black}{the effect of feature adaptation is highly dependent on the selected features. If the selected features cannot fully characterize the essence of the data, the adaptation cannot effectively reduce the difference between domains. Additionally, this strategy sometimes has limited effectiveness in reducing domain differences, especially when there is a significant distribution gap between the source and target domains~\cite{liu2023reducing}.}

% The feature fusion strategy (presented in Section~\ref{fuse}) aims to obtain features from multiple sources, either from different layers within a single network or from multiple networks. However, in the case of the former, similarities among the features from the same dataset and network may require effective fusion methods, while in the latter, the use of multiple networks can increase training costs due to the need for simultaneous training.

%\textcolor{black}{Shared information mining aims to search for and utilize common information between domains (presented in Section~\ref{fuse}), which can be applied to a variety of tasks and data types, especially in multi-domain and multi-task learning scenarios, and can effectively improve the overall performance. However, it usually requires complex models and training processes, resulting in high computational costs~\cite{zhou2021unsupervised}. In the context of CDFSL tasks, this introduces another problem: with only a small amount of available data, the extractable shared information is limited, which can easily lead to model overfitting~\cite{fslsurvey22}.}

% Feature transformation (introduced in Section~\ref{reweights}) is a common approach when extra network and multi-domain data are not available. It involves reweighting features by assigning new parameters to them, either through simple transformation and weighting, or through a learnable module. However, this strategy only allows limited exploration of shared information as it only reweights the features output from the final layer.
% \textcolor{black}{Adversarial learning (introduced in Section~\ref{adver}) enhances model robustness and generalization by introducing the discriminator. However, these methods inherit issues from adversarial learning, such as training instability~\cite{qian2022survey}, which can lead to mode collapse and non-convergence problems.}

\subsection{Hybrid Methods}  \label{hybrid}
% Hybrid approaches in CDFSL incorporate the above-mentioned strategies. The related technologies are listed in Table~\ref{table_hybrid}. Combinations of instance-guided and parameter-based strategies are prevalent in CDFSL. For example, a parameter-efficient multi-mode modulator is proposed in~\cite{hybrid_2}. First, the modulator is designed to maintain multiple modulation parameters (one for each domain) in a single network, thus achieving single-network multi-domain representation. Second, it divides the modulation parameters into the domain-specific and the domain-cooperative sets to explore the intra-domain information and inter-domain correlations, respectively. Furthermore,~\cite{tgdm} explores a novel target guided dynamic mixup (TGDM) framework to generate the intermediate domain images to help the FSL task learning on the target domain. In addition,~\cite{data_multi_1} learns the meta-learners by utilizing multiple domains, and the meta-learners are combined in the parameter space to be the Initialized parameters of a network used in the target domain. Besides, researchers explore the combination of feature post-process and parameter-based strategies in CDFSL.~\cite{feature_reweight_10} conducts style transfer-based task augmentation with feature fusion tasks from different tasks and styles and feature modulation module (FM). And in~\cite{hybrid_6}, a feature extractor stacking (FES) is proposed to combine information from a backbones collection.
\textcolor{black}{The three strategies above address the unreliability of TSERM by optimizing specific factors: $\mathcal{D}$-Extension enhances the available information in $\mathcal{D}$, $\mathcal{H}$-Constraint limits the complexity of the hypothesis space $\mathcal{H}$, and $\Delta$-Adaptation optimizes $\Delta$. Each strategy has distinct advantages: $\mathcal{D}$-Extension is simple and practical, $\mathcal{H}$-Constraint reduces model complexity, and $\Delta$-Adaptation improves the transferability of knowledge from the source to the target domain. As a result, many studies have explored combining these strategies to leverage their strengths and further mitigate TSERM unreliability. Related technologies are listed in Table~\ref{table_hybrid}.}
% Hybrid approaches in CDFSL incorporate the above-mentioned strategies. The related technologies are listed in Table~\ref{table_hybrid}. The combinations of the three methods are evenly distributed in CDFSL approaches. 

Some methods combine $\mathcal{D}$-Extension and $\mathcal{H}$-Constraint techniques~\cite{dynamic,perera2024discriminative,cdfsl231,ji2024soft,hybrid_3}. For example, \cite{dynamic} introduces additional unsupervised target domain samples during the pre-training phase and augments them by training the target domain encoder through self-distillation. Similarly, \cite{cdfsl231} incorporates unlabeled target data during training, using cross-level knowledge distillation to enhance the model's capacity to extract highly discriminative features from the target dataset. \textcolor{black}{\cite{perera2024discriminative} fuses features at different levels during pre-training and introduces learnable anchors (acting as adapters) during fine-tuning to adapt to the target task. Furthermore, \cite{hybrid_3} addresses the FSL task in the target domain by synthesizing tasks from multiple perspectives and applying regularization based on consistency and correlation metrics. Finally, following \cite{ji2023cross}, \cite{ji2024soft} introduces additional unlabeled target data and proposes an iterative soft pruning training method that combines contrastive loss and regularization to remove redundant weights and optimize the model.}
% For the combination of $\mathcal{D}$-Extension and $\mathcal{H}$-Constraint methods~\cite{dynamic,perera2024discriminative,feature_reweight_10,cdfsl231,ji2024soft,hybrid_3},~\cite{dynamic} introduces additional unsupervised target domain samples during the pre-training phase and augments them while training the target domain encoder model through self-distillation.~\cite{feature_reweight_10} conducts style transfer-based task augmentation with feature fusion tasks from different tasks and styles and feature modulation module.~\cite{cdfsl231} incorporates unlabeled target data during the training process, employing a cross-level knowledge distillation technique to enhance the model's capacity for extracting highly discriminative features from the target dataset.\textcolor{black}{~\cite{perera2024discriminative} initially fuses features at different levels during the pre-training phase and then introduces learnable anchors (acting as adapters) during the fine-tuning phase to adapt to the target task. And~\cite{hybrid_3} addresses the FSL task in the target domain by constructing multiple task views and introducing regularization of consistency and correlation metrics. Besides, following the work in~\cite{ji2023cross},~\cite{ji2024soft} introduces the extra unlabeled target data and proposes an iterative soft pruning training method that combines contrastive loss and regularization to remove redundant weights and optimize the model.}

\textcolor{black}{Some methods combine $\mathcal{D}$-Extension and $\Delta$-Adaptation strategies~\cite{hybrid_1,hybrid_7,hybrid_2,feature_reweight_11,feature_reweight_10}.} For instance, \cite{hybrid_1} introduces additional labeled target instances and decouples features into domain-agnostic and domain-specific categories during fine-tuning, leveraging domain-agnostic features with a domain and FSL classifier. As an extension of \cite{hybrid_1}, \cite{hybrid_7} uses a mixed module to generate diverse images from both source and target domains and decouples features during meta-training. \cite{hybrid_2} decouples features to obtain domain-agnostic and domain-specific components, then adaptively weights these features for the target task. \cite{feature_reweight_11} fuses features at different levels to enhance generalization, then adapts to the target task through feature transformation during meta-training. \textcolor{black}{\cite{feature_reweight_10} synthesizes tasks using style transformations and employs these tasks to learn and optimize feature transformation modules.}
% For the combination of $\mathcal{D}$-Extension and $\Delta$-Adaptation~\cite{hybrid_1,hybrid_7,hybrid_2,feature_reweight_11,sreenivas2023similar}, ~\cite{hybrid_1} first introduces additional instance augmentation samples and decouples features into domain-agnostic and domain-specific features during the fine-tuning phase, leveraging domain-agnostic features through a domain classifier and FSL classifier. As the extension of~\cite{hybrid_1},~\cite{hybrid_7} enhances information during the pre-training phase to boost model generalization and decouples features in the meta-training phase.~\cite{hybrid_2} initially decouples features to obtain domain-agnostic and domain-specific features, then combines these features with weighted adaptation to the target task. ~\cite{feature_reweight_11} first fuses features at different levels to improve the model’s generalization, then adapts to the target task through feature transformation during the meta-training phase. Besides, ~\cite{sreenivas2023similar} uses the style of semantically similar categories to augment the data of each category. ~\cite{feature_reweight_10} synthesizes stylized tasks and uses these tasks to learn and optimize feature transformation modules.

\textcolor{black}{In the combination of $\mathcal{H}$-Constraint and $\Delta$-Adaptation methods~\cite{feature-wise,parameter_fix_7,zhang2024cross,ma2023prod}, \cite{feature-wise} asynchronously freezes and updates the feature-wise transformation layers and feature extractor. Building on this, \cite{parameter_fix_7} introduces a metric function to better measure the distance between labeled and unlabeled images. Meanwhile, \cite{zhang2024cross} uses multi-step self-distillation to update a feature adaptation module at each learning step. In \cite{ma2023prod}, domain-agnostic and domain-specific prompts are designed to adapt to the target task.}
% \textcolor{black}{In the combination of $\mathcal{H}$-Constraint and $\Delta$-Adaptation methods~\cite{feature-wise,parameter_fix_7,zhang2024cross,ma2023prod}, \cite{zhang2024cross} uses multi-step self-distillation to train the model, where a feature adaptation module is updated at each learning step. In\cite{ma2023prod}, domain-agnostic prompts and domain-specific prompts are designed and learned to adapt to the target task.}
%\vspace{-0.3cm}
\iffalse
\begin{table}%[b]
\tiny
\centering
\caption{Representative hybrid CDFSL approaches. `FCS' represents `Frontiers of Computer Science'.}
%\vspace{-0.2cm}
\setlength{\tabcolsep}{3.25mm}{
\begin{tabular}{llccccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{Instance-guided} & \textbf{Parameter-based} & \textbf{Feature post-process} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
 CosML~\cite{data_multi_1}  & arXiv 2020 & Multiple domains & \XSolidBrush & Feature fusion & CE & \Checkmark &   &     \\
% \hline
Tri-M~\cite{hybrid_2}    &  ICCV 2021  & Multiple domains & Parameter reweight & \XSolidBrush & CE & \Checkmark & \Checkmark &      \\
% \hline
TGDM~\cite{tgdm}   & ACM MM 2022 & Labeled target & Parameter reweight & \XSolidBrush  & CE & \Checkmark &   &    \\
% \hline
TAML~\cite{feature_reweight_10} & arXiv 2023 & Multiple domains & Parameter reweight & Future fusion & CE & \Checkmark &  & \Checkmark     \\
% \hline
URL~\cite{hybrid_4}   &  ICCV 2021 & Multiple domains & Parameter reweight & \XSolidBrush & CE \& CKA \& KL & \Checkmark & \Checkmark &     \\
% \hline
Meta-FDMixup~\cite{hybrid_1} & ACM MM 2021 & Labeled target & \XSolidBrush & Feature transformation & CE \& KL & \Checkmark &   &     \\  
% \hline
GMeta-FDMixup~\cite{hybrid_7} & TIP 2021 & Labeled target & \XSolidBrush & Feature transformation & CE \& SSL & \Checkmark &  & \Checkmark    \\  
% \hline
ME-D2N~\cite{data_target} & ACM MM 2022 & Labeled target & \XSolidBrush & Feature transformation & CE \& KL & \Checkmark &   &    \\
% \hline
TL-SS~\cite{hybrid_3}  &  AAAI 2022  & Original data & Parameter reweight & \XSolidBrush & CE \& Metric & \Checkmark &  & \Checkmark     \\  
% \hline
% SET-RCL~\cite{hybrid_5}   &  ACM MM 2022 & Original data &  &  & CE \& Contrastive & \Checkmark & \Checkmark     \\
% \hline
TKD-Net~\cite{data_multi_0} & FCS 2023 & Multiple domains & \XSolidBrush & Future fusion & CE \& KL \& $L_{2}$ & \Checkmark &   &     \\
% \hline
CLDFD~\cite{cdfsl231} & ICLR 2023 & Unlabeled target & \XSolidBrush & Future fusion & CE \& SimCLR \& KD &  &  & \Checkmark    \\
\hline
\end{tabular}}
%\vspace{-0.2cm}
\label{table_hybrid}
\end{table}
\fi


\begin{table}%[b]
\tiny
\centering
\vspace{-0.2cm}
\caption{Representative hybrid CDFSL approaches.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{llccccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{$\mathcal{D}$-Extension} & \textbf{$\mathcal{H}$-Constraint} & \textbf{$\Delta$-Adaptation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
DDN~\cite{dynamic}  & NIPS 2021 & Data Augmentation & Knowledge Distillation & \XSolidBrush & CE \& entropy &  &  & \Checkmark    \\
% \hline
DIPA~\cite{perera2024discriminative}   &  CVPR 2024  & Feature Generation & Parameter Adjustment & \XSolidBrush & CE &  & \Checkmark &      \\
% \hline
CLDFD~\cite{cdfsl231} & ICLR 2023 &  Data Augmentation & Knowledge Distillation & \XSolidBrush & CE \& SimCLR \& KD &  &  & \Checkmark    \\
% \hline
SWP-LS~\cite{ji2024soft} & TMM 2024 & Data Augmentation & Regularization & \XSolidBrush & Contrastive \& $L_{2}$ & \Checkmark &  & \Checkmark \\  
% \hline
TL-SS~\cite{hybrid_3}  &  AAAI 2022  & Data Augmentation & Parameter Adjustment & \XSolidBrush & CE \& Metric & \Checkmark &  & \Checkmark     \\  
% \hline
Meta-FDMixup~\cite{hybrid_1} & ACM MM 2021 & Data Augmentation & \XSolidBrush & Shared Information Mining & CE \& KL & \Checkmark &   &     \\  
% \hline
GMeta-FDMixup~\cite{hybrid_7} & TIP 2022 & Data Augmentation & \XSolidBrush & Shared Information Mining & CE \& SSL & \Checkmark &  & \Checkmark    \\  
% \hline
Tri-M~\cite{hybrid_2}    &  ICCV 2021  & Feature Generation & \XSolidBrush & Shared Information Mining & CE & \Checkmark & \Checkmark &      \\
% \hline
TCT-GCN~\cite{feature_reweight_11}    &  Neurocomputing 2023  & Feature Generation & \XSolidBrush & Feature Adaptation & CE & \Checkmark &  & \Checkmark     \\
% \hline
TAML~\cite{feature_reweight_10} & arXiv 2023 & Task Synthesis & \XSolidBrush & Feature Adaptation & CE & \Checkmark &  & \Checkmark     \\
% \hline
FWT~\cite{feature-wise} & ICLR 2020 & \XSolidBrush & Parameter Adjustment & Feature Adaptation & CE & \Checkmark &  &    \\
% \hline
FGNN~\cite{parameter_fix_7} & KBS 2022 & \XSolidBrush & Parameter Adjustment & Feature Adaptation & Softmax & \Checkmark &  &    \\
% \hline
FAD~\cite{zhang2024cross}    &  Neural Comput 2024  & \XSolidBrush & Knowledge Distillation & Feature Adaptation & CE \& KL &  & \Checkmark &      \\
% \hline
ProD~\cite{ma2023prod}    &  CVPR 2023  & \XSolidBrush & Parameter Adjustment & Shared Information Mining & CE \& Cos & \Checkmark &  &      \\
% \hline
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{table_hybrid}
\end{table}

% \subsubsection{Hybrid via Loss Function}
% Several works solve CDFSL problem not only combine the above-mentioned strategies but also with different loss function such as contrastive loss, metric loss, \etc.~\cite{hybrid_1} advocates utilizing few labeled target data to guide the model learning and is optimized by CE loss and KL loss. Technically, a novel meta-FDMixup network is proposed to extract the disentangled domain-irrelevant and domain-specific features with a novel disentangle module and a domain classifier. Moreover, as an extension of~\cite{hybrid_1},~\cite{hybrid_7} further introduces two mixup modules (mixup-P and mixup-M) to aid in effectively utilizing the unbalanced and disjoint source and target datasets, and a ConL module helps to improve the model generalization. And~\cite{data_target} follows this setup (introduce few labeled target domain data) and proposes a Multi-Expert Domain Decompositional Network (ME-D2N) to solve CDFSL. The loss function also includes CE and KL loss.~\cite{set-rcl} proposes a style-aware episodic training with robust contrastive learning (SET-RCL) to make the learned model can achieve better adapt to the test tasks with domain-specific styles. And TL-SS strategy~\cite{hybrid_3} augments multiple views of tasks and proposes a high-order associated encoder (HAE) to generate proper parameters and enables the encoder to flexibly to any unseen tasks. The loss function in this work includes CE and a metric loss. Moreover,~\cite{hybrid_4} learns a single set of deep universal representations by distilling the knowledge of multiple separately trained networks by using multiple domains after co-aligning their features with the help of adapters and centered kernel alignment. It is optimized by CKA, CE, and KL loss. Furthermore,~\cite{data_multi_0} proposes team-knowledge distillation networks (TKD-Net) and explores a strategy to help the cooperation of multiple teachers. And~\cite{cdfsl231} incorporates unlabeled target data during the training process, employing a cross-level knowledge distillation technique to enhance the model's capacity for extracting highly discriminative features from the target dataset.

\subsubsection{Discussion and Summary}
% The combination of multiple strategies in CDFSL, as discussed in Section~\ref{hybrid}, can lead to improved performance. For example, the instance-guided strategy is often easily incorporated into various methods, and as such, is frequently combined with other approaches. However, there are also challenges associated with combining strategies. The combination of feature post-processing and parameter-based strategies can be unpredictable and may lead to negative transfer, making it a less frequently explored option. To achieve optimal results, it is essential to avoid negative transfer and carefully consider the combination of strategies in hybrid approaches.
The combination of multiple strategies in CDFSL, as discussed in Section~\ref{hybrid}, can lead to improved performance. For instance, due to its simplicity and ease of implementation, the $\mathcal{D}$-Extension strategy is often combined with other approaches. However, challenges arise when combining strategies. \textcolor{black}{For example, combining $\mathcal{D}$-Extension and $\mathcal{H}$-Constraint methods can result in training conflicts, leading to negative transfer~\cite{zhao2024all}. Although $\mathcal{D}$-Extension increases data diversity, this diversity may not align with the target domain's characteristics, particularly when there is a significant feature distribution difference. In such cases, the features learned may be ineffective for the target domain. Conversely, $\mathcal{H}$-Constraint methods stabilize model performance by optimizing parameters, but during self-distillation, the model may become overly reliant on existing feature distributions. If these features don't match the target domain, negative transfer may occur~\cite{zhao2024all}.}


\textcolor{black}{Moreover, $\mathcal{D}$-Extension may generate additional noise, which negatively impacts methods combined with $\Delta$-Adaptation. This introduces irrelevant information during the process of reducing $\Delta$, potentially misleading the alignment or hindering the extraction of shared information, leading to performance degradation~\cite{liu2022deep}. To address this, some methods~\cite{hybrid_1,feature_reweight_0} control the augmentation process to reduce the introduction of irrelevant information.}

\textcolor{black}{The combination of $\mathcal{H}$-Constraint and $\Delta$-Adaptation has high training complexity and computational cost. Specifically, these two strategies require the model to simultaneously optimize the hypothesis space while aligning features between the source and target domains. $\mathcal{H}$-Constraint often involves intricate regularization techniques or model architecture modifications, while $\Delta$-Adaptation requires measuring and aligning distributional differences (\eg, through MMD or adversarial training). This combination increases the complexity of the optimization process, making training more demanding and significantly raising computational costs~\cite{liu2022deep,wang2020rethink}.}
% \textcolor{black}{Moreover, $\mathcal{D}$-Extension may generate additional noise, which also negatively impact methods combined with $\Delta$-Adaptation. This effectively introduces more noise or irrelevant information during the process of reducing $\Delta$, potentially misleading the alignment process or causing insufficient extraction of shared information, leading to performance degradation~\cite{liu2022deep}. Therefore, some methods~\cite{hybrid_1,feature_reweight_0} reduce the introduction of irrelevant information by controlling the augmentation process. Currently, due to the training complexity and computational cost, only a few methods have attempted to combine $\mathcal{H}$-Constraint and $\Delta$-Adaptation~\cite{ma2023prod,zhang2024cross}. However, with the rise of large-scale pre-trained models~\cite{dosovitskiy2020image,radford2021learning,woo2023convnext} and prompt learning~\cite{liu2023pre}, we believe that more future work will explore the integration of these two approaches to address CDFSL.}

\textcolor{black}{Currently, the high training complexity and computational cost prevent any method from addressing the TSERM unreliability problem by combining all three strategies. However, with the rise of large-scale pre-trained models (LMs) \cite{dosovitskiy2020image,radford2021learning,woo2023convnext} and prompt learning \cite{liu2023pre}, we expect future methods to explore this combination. The pre-training of LMs captures broad feature representations, reducing the need for extensive fine-tuning. At the same time, prompt learning provides a lightweight approach that leverages the knowledge in large models, minimizing the need for complex optimization and training costs.}