\section{Performance} \label{performance}
This section provides a comprehensive overview of the evaluation process for models in Cross-Domain Few-Shot Learning (CDFSL). To assess the effectiveness of these models, we first examine the relevant datasets and benchmarks in Section~\ref{benchmark}. In Section~\ref{per-com}, we offer a detailed analysis and comparison of the performance of various methods in CDFSL, highlighting the strengths and weaknesses of different approaches in tackling this complex problem.

\iffalse
\subsection{Datasets} \label{data}
The availability of annotated datasets facilitates the evaluation of CDFSL models and ensures a fair comparison of various algorithms and architectures. The complexity, size, annotation number, and transfer difficulty of these datasets pose an ongoing challenge that drives the development of innovative techniques. Table~\ref{dataset} lists the most widely used datasets for the CDFSL problem.
%, and the following sections provide detailed descriptions of each:
%\vspace{-0.3cm}
\begin{table}%[b]
\tiny
\centering
%\vspace{-0.4cm}
\caption{Details of datasets in CDFSL.}
\vspace{-0.1cm}
\setlength{\tabcolsep}{3.0mm}{
\begin{tabular}{lcccccccc}
\hline
\textbf{Datasets} & \textbf{Derived from} & \textbf{Number of images} & \textbf{Image size} & \textbf{\makecell[c]{Number of \\ categories}} & \textbf{Content} & \textbf{Fields} & \textbf{Reference}     \\ 
 \hline
\textit{mini}ImageNet & ImageNet & 60000 & $84 \times 84$ & 100 & objects classification & natural scene & \cite{miniimagenet}    \\  
% \hline
\textit{tiered}ImageNet & ImageNet & 779165 & $84 \times 84$ & 608 & objects classification & natural scene & \cite{tieredimagenet}     \\
% \hline
Plantae  & iNat2017 & 196613 & varying & 2101 & plants \& animals classification & natural scene & \cite{plantae}  \\
% \hline
Places     & N/A & 10 million & $200 \times 200$ & 400+ & scene classification  & natural scene & \cite{places}   \\
% \hline
Stanford Cars     & N/A & 16185 & varying & 196 & cars fine-grained classification & natural scene &  \cite{cars}   \\
% \hline
CUB   & ImageNet & 11788 & $84 \times 84$ & 200 & birds fine-grained classification & natural scene &  \cite{cub}   \\  
% \hline
CropDiseases   & N/A & 87000 & $256 \times 256$ & 38 & crop leaves classification & natural scene &  \cite{cropdiseases}  \\
% \hline
EuroSAT   & Sentinel-2 satellite & 27000 & $64 \times 64$ & 10 & land classification & remote sensing &  \cite{eurosat}   \\
% \hline
ISIC 2018   & N/A & 11720 & $600 \times 450$ & 7 & dermoscopic lesion classification & medical &  \cite{isic1}   \\
% \hline
ChestX   & N/A & 100K & $1024 \times 1024$ & 15 & lung diseases classification & medical & \cite{chestx}  \\
% \hline
Omniglot   & N/A & 25260 & $28 \times 28$ & 1623 & characters classification & character &  \cite{omniglot}   \\
% \hline
FGVC-Aircraft   & N/A & 10200 & varying & 100 & Aircraft fine-grained classification & natural scene &  \cite{aircraft}   \\
% \hline
DTD   & N/A & 5640 & varying & 47 & textures classification & natural scene &  \cite{dtd}   \\
% \hline
Quick Draw   & Quick draw! & 50 million & $128 \times 128$ & 345 & drawing images classification & Art &  \cite{draw1}   \\
% \hline
Fungi   & N/A & 100000 & varying & 1394 & fungi fine-grained classification & natural scene &  \cite{fungi}   \\
% \hline
VGG Flower   & N/A & 8189 & varying & 102 & flowers fine-grained classification & natural scene &  \cite{vgg}   \\
% \hline
Traffic Signs   & N/A & 50000 & varying & 43 & Traffic signs classification & natural scene &  \cite{traffic}   \\
% \hline
MSCOCO   & N/A & 1.5 million & varying & 80 & objects classification & natural scene &  \cite{mscoco}   \\
\hline
\end{tabular}}
%\vspace{-0.3cm}
\label{dataset}
\end{table}
\fi

\iffalse
\begin{itemize}
    \item \textit{miniImageNet}~\cite{miniimagenet}: \textit{mini}ImageNet dataset consists of 60000 images selected from the dataset ImageNet, with a total of 100 categories. Each category has 600 images.
    \item \textit{tieredImageNet}~\cite{tieredimagenet}: \textit{tiered}ImageNet dataset is selected from the ImageNet dataset, including 34 categories, and each category contains 10-30 sub-categories (classes). There are 608 classes and 779165 images in this dataset. Each class has multiple samples of varying numbers.
    \item \textit{Plantae}~\cite{plantae}: Plantae dataset is one of dataset iNat2017. There are 2101 categories and 196613 images.
    \item \textit{Places}~\cite{places}: Places dataset contains more than 10 million images of 400+ unique scene categories. This dataset features 5000 to 30000 training images in each class, which is consistent with real-world frequency of occurrence.
    %\item Stanford Dogs~\cite{dogs}: The Stanford Dogs dataset contains over 20,000 images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for fine-grained image categorization.
    \item \textit{Stanford Cars}~\cite{cars}: The Cars dataset is a fine-grained classification dataset about cars. It contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images.
    \item \textit{CUB}~\cite{cub}: Images in CUB dataset overlap with images in ImageNet. It is a fine-grained classification dataset about birds that contain 11788 images in 200 categories. The size of images in this dataset is $84 \times 84$.
    \item \textit{CropDiseases}~\cite{cropdiseases}: The cropDiseases dataset consists of about 87000 RGB images of healthy and diseased crop leaves, categorized into 38 different classes. The total dataset is divided into an 80/20 training and validation set ratio. The image size in this dataset is $256 \times 256$.
    \item \textit{EuroSAT}~\cite{eurosat}: EuroSAT is a dataset for land use and land cover classification. The dataset is based on Sentinel-2 satellite images consisting of 10 classes with in total of 27,000 labeled and geo-referenced images. Each class includes 2000-3000 images, and the size of these images is $64 \times 64$.
    \item \textit{ISIC 2018}~\cite{isic1,isic2}: ISIC 2018 dataset includes 10015 dermoscopic lesion images from 7 categories for training, 193 images for evaluate, and 1512 images for testing. The size of each image is $600 \times 450$.
    \item \textit{ChestX}~\cite{chestx}: ChestX-ray14 is currently the largest lung X-ray database provided by the NIH Research Institute, which contains 14 lung diseases, and category 15 indicates no disease was found. The size of images in this dataset is $1024 \times 1024$.
    \item \textit{Omniglot}~\cite{omniglot}: The Omniglot dataset comprises 1,623 handwritten characters from 50 languages, each with 20 different handwritings. The size of each image in this dataset is $28 \times 28$.
    \item \textit{FGVC-Aircraft}~\cite{aircraft}: FGVC-Aircraft dataset includes 10200 aircraft images (102 aircraft models, 100 images per model). The image resolution is about 1-2 Mpixels.
    \item \textit{Describable Textures (DTD)}~\cite{dtd}: DTD is a texture database consisting of 5640 images, organized according to a list of 47 terms (categories) inspired by human perception. There are 120 images for each category. Image sizes range between 300x300 and 640x640.
    \item \textit{Quick Draw}~\cite{draw1}: The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!
    \item \textit{Fungi}~\cite{fungi}: This datasets contains 100000 fungi images belong to 1394 different categories, which is all fungi classes that have been spotted by the general public in Denmark.
    \item \textit{VGG Flower}~\cite{vgg}: VGG Flower dataset contains 8189 flower images belong to 102 categories. The flowers chosen to be flower commonly occuring in the United Kingdom. Each class consists of between 40 and 258 images.
    \item \textit{Traffic Signs}~\cite{traffic}: Traffic Signs dataset consists of 50,000 images of German road signs in 43 classes.
    \item \textit{MSCOCO}~\cite{mscoco}: The images in MSCOCO dataset are collected from Flickr with 1.5 million object instances belonging to 80 classes labelled and localized using bounding boxes.
\end{itemize}
\fi

\subsection{Datasets \& Benchmarks} \label{benchmark}
\textcolor{black}{The availability of annotated datasets enables fair comparison of CDFSL models and fosters the evaluation of various algorithms and architectures. The complexity, size, number of annotations, and transfer difficulty of these datasets present ongoing challenges that drive the development of innovative techniques. Table~\ref{dataset} lists the most widely used datasets for the CDFSL problem.}
\begin{table}%[b]
\tiny
\centering
\vspace{-0.2cm}
\caption{Details of datasets in CDFSL.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{lcccccccc}
\hline
\textbf{Datasets} & \textbf{Derived from} & \textbf{Number of images} & \textbf{Image size} & \textbf{\makecell[c]{Number of \\ categories}} & \textbf{Content} & \textbf{Fields} & \textbf{Reference}     \\ 
 \hline
\textit{mini}ImageNet & ImageNet & 60000 & $84 \times 84$ & 100 & objects classification & natural scene & \cite{miniimagenet}    \\  
% \hline
\textit{tiered}ImageNet & ImageNet & 779165 & $84 \times 84$ & 608 & objects classification & natural scene & \cite{tieredimagenet}     \\
% \hline
Plantae  & iNat2017 & 196613 & varying & 2101 & plants \& animals classification & natural scene & \cite{plantae}  \\
% \hline
Places     & N/A & 10 million & $200 \times 200$ & 400+ & scene classification  & natural scene & \cite{places}   \\
% \hline
Stanford Cars     & N/A & 16185 & varying & 196 & cars fine-grained classification & natural scene &  \cite{cars}   \\
% \hline
CUB   & ImageNet & 11788 & $84 \times 84$ & 200 & birds fine-grained classification & natural scene &  \cite{cub}   \\  
% \hline
CropDiseases   & N/A & 87000 & $256 \times 256$ & 38 & crop leaves classification & natural scene &  \cite{cropdiseases}  \\
% \hline
EuroSAT   & Sentinel-2 satellite & 27000 & $64 \times 64$ & 10 & land classification & remote sensing &  \cite{eurosat}   \\
% \hline
ISIC 2018   & N/A & 11720 & $600 \times 450$ & 7 & dermoscopic lesion classification & medical &  \cite{isic1}   \\
% \hline
ChestX   & N/A & 100K & $1024 \times 1024$ & 15 & lung diseases classification & medical & \cite{chestx}  \\
% \hline
Omniglot   & N/A & 25260 & $28 \times 28$ & 1623 & characters classification & character &  \cite{omniglot}   \\
% \hline
FGVC-Aircraft   & N/A & 10200 & varying & 100 & Aircraft fine-grained classification & natural scene &  \cite{aircraft}   \\
% \hline
DTD   & N/A & 5640 & varying & 47 & textures classification & natural scene &  \cite{dtd}   \\
% \hline
Quick Draw   & Quick draw! & 50 million & $128 \times 128$ & 345 & drawing images classification & Art &  \cite{draw1}   \\
% \hline
Fungi   & N/A & 100000 & varying & 1394 & fungi fine-grained classification & natural scene &  \cite{fungi}   \\
% \hline
VGG Flower   & N/A & 8189 & varying & 102 & flowers fine-grained classification & natural scene &  \cite{vgg}   \\
% \hline
Traffic Signs   & N/A & 50000 & varying & 43 & Traffic signs classification & natural scene &  \cite{traffic}   \\
% \hline
MSCOCO   & N/A & 1.5 million & varying & 80 & objects classification & natural scene &  \cite{mscoco}   \\
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{dataset}
\end{table}

Based on these datasets, several benchmarks have been established for the CDFSL problem, including \textit{mini}ImageNet \& CUB (mini-CUB), the standard fine-grained classification benchmark (FGCB), and BSCD-FSL~\cite{bscd-fsl}. Additionally, Meta-Dataset~\cite{meta-dataset} has been proposed to evaluate the cross-domain problem in FSL. Since mini-CUB is included in FGCB, we mainly focus on introducing the last three benchmarks.

\textit{FGCB}.
An early benchmark for Fine-grained CDFSL ($FG$) was established during the initial development of CDFSL. It includes five datasets: \textit{mini}ImageNet, Plantae~\cite{plantae}, Places~\cite{places}, Cars~\cite{cars}, and CUB~\cite{cub}, with \textit{mini}ImageNet typically serving as the source domain and the others as the target domains. The benchmark consists entirely of natural images. %The source domain includes 60000 common objects. The target domains include four fine-grained datasets including Plantae, Places, Cars, and CUB. 
The main challenge across domains for this benchmark is transferring the category information from coarse to fine. %The visual display is shown in Figure\ref{fine-grained}.

\textit{BSCD-FSL}~\cite{bscd-fsl}.
% As a more challenging benchmark to address imaging way-based CDFSL ($IW$) in CDFSL, BSCD-FSL includes five datasets consisting of \textit{mini}ImageNet, CropDisease~\cite{cropdiseases}, EuroSAT~\cite{eurosat}, ISIC~\cite{isic1,isic2}, ChestX~\cite{chestx}. CropDisease is a fine-grained dataset of crop leaves and all-natural industrial images. EuroSAT, ISIC, and ChestX have different imaging ways with natural images. They are satellite images, dermatology images, and radiology images, respectively.
As a more challenging benchmark for imaging way-based CDFSL ($IW$), BSCD-FSL includes five datasets: \textit{mini}ImageNet, CropDisease~\cite{cropdiseases}, EuroSAT~\cite{eurosat}, ISIC~\cite{isic1,isic2}, and ChestX~\cite{chestx}. \textcolor{black}{Considering \textit{mini}ImageNet as the source domain, the distribution distances between the source and target datasets, from smallest to largest, are CropDiseases, EuroSAT, ISIC, and ChestX. BSCD-FSL is designed to evaluate methods across both near-domain and distant-domain scenarios.}

\textit{Meta-Dataset}~\cite{meta-dataset}.
% Meta-Dataset is a large-scale, diverse benchmark for measuring various image classification models in realistic and challenging few-shot contexts such as CDFSL. This dataset consists of 10 publicly available natural image datasets, handwritten characters, and graffiti datasets. Hence, this benchmark can address Fine-grained ($FG$) and art-based CDFSL ($Art$) problem. This benchmark breaks the requirement that source and target data from the same domain in FSL and limitations of \textit{C-way K-shot} form tasks. It also introduces the class imbalance in the real world, which means it changes the number of classes in each task and the size of training set.
Meta-Dataset is a large-scale, diverse benchmark for evaluating image classification models in realistic and challenging few-shot scenarios, such as in CDFSL. It includes 10 publicly available datasets spanning natural images, handwritten characters, and graffiti, making it suitable for addressing Fine-grained ($FG$) and art-based ($Art$) CDFSL problems. \textcolor{black}{This benchmark removes the requirement for source and target data to be from the same domain in FSL, going beyond the limitations of traditional \textit{C-way K-shot} tasks. It also introduces real-world class imbalances by varying the number of classes and training set sizes for each task.}

In addition to the commonly used benchmarks mentioned above, some methods adopt benchmarks originally designed for the domain adaptation (DA) problem. The DomainNet benchmark~\cite{domainnet}, designed to address art-based cross-domain problems, is widely used in DA and consists of 6 domains, each with 345 categories of common objects. Additionally, the Office-Home benchmark~\cite{officehome} is used by some studies for CDFSL. It consists of 4 domains (art, clipart, product, and real world) and 65 categories per domain, with a total of 15,500 images. On average, there are 70 images per class, with a maximum of 99 images per class.

\subsection{Performance Comparison and Analysis} \label{per-com}
This section provides an overview of the comparative performance of CDFSL approaches from various categories. Prediction accuracy is the standard evaluation metric in CDFSL, and evaluations are typically conducted under several settings, including \textit{5-way 1-shot}, \textit{5-way 5-shot}, \textit{5-way 20-shot}, and \textit{5-way 50-shot}. \textcolor{black}{To ensure fair evaluation, all results in this paper are directly sourced from the original studies. The experimental setups for current CDFSL methods consistently follow the settings outlined in \cite{bscd-fsl}, where the $C$-way $K$-shot data are randomly sampled from the datasets, and all reported validation results are averaged over 600 trials for robustness. %For more experimental settings, please refer to~\cite{bscd-fsl}.
} As CDFSL is a subfield of FSL, many classical FSL methods can be directly applied to CDFSL problems. Table~\ref{fsl_mtd} shows the results of these methods, where it can be observed that meta-learning methods (MatchingNet, ProtoNet, RelationNet, MAML) perform slightly worse in CDFSL due to domain gaps, especially compared to simple fine-tuning transfer learning methods, which perform better as the value of \textit{K} increases.
\begin{table}%[b]
\tiny
\centering
\vspace{-0.1cm}
\caption{The CDFSL performance on the classical FSL approaches with ResNet10 backbone. $K$ is the number of samples from \textit{5-way K-shot}.}
\vspace{-0.1cm}
\setlength{\tabcolsep}{1.9mm}{
\begin{tabular}{clcccccccc}
\hline 
\textbf{\textit{K}} & \textbf{Methods} & \textbf{CropDiseases} & \textbf{EuroSAT} & \textbf{ISIC} & \textbf{ChestX} & \textbf{Plantae} & \textbf{Places} & \textbf{Cars} & \textbf{CUB}   \\ 
% \cline{3-18}
\hline
\multirow{5}*{1}  & Fine-tuning~\cite{bscd-fsl} & 61.56±0.90 & 49.34±0.85 & 30.80±0.59 &  21.88±0.38 & 33.53±0.36 & 50.87±0.48 & 29.32±0.34 &  41.98±0.41    \\ 
% \cline{2-10}
  & MatchingNet~\cite{miniimagenet} & 48.47±1.01 & 50.67±0.88 & 29.46±0.56 & 20.91±0.30 & 32.70 ± 0.60 & 49.86±0.79 & 30.77±0.47 & 35.89±0.51 \\
% \cline{2-10}
%   &  MAML & - & - & - & - & - & - & - & -   \\ 
% \cline{2-10}
  &  RelationNet~\cite{relation} & 56.18±0.85 & 56.28±0.82 & 29.69±0.60 & 21.94±0.42 & 33.17±0.64 & 48.64±0.85 & 29.11±0.60 & 42.44±0.77 \\
% \cline{2-10}
  &  ProtoNet~\cite{proto} & 51.22±0.50 & 52.93±0.50 & 29.20±0.30 & 21.57±0.20 & - & - & - & - \\ 
% \cline{2-10}
  &  GNN~\cite{gnn} & \textbf{64.48±1.08} & \textbf{63.69±1.03} & \textbf{32.02±0.66} & \textbf{22.00±0.46} & \textbf{35.60±0.56} & \textbf{53.10±0.80} & \textbf{31.79±0.51} & \textbf{45.69±0.68}  \\ 
 \hline
\multirow{6}*{5}  & Fine-tuning & \textbf{90.64±0.54} & 81.76±0.48 &  \textbf{49.68±0.36} & \textbf{26.09±0.96} & \textbf{47.40±0.36} & 66.47±0.41 & 38.91±0.38 &  58.75±0.36   \\ 
% \cline{2-10}
  & MatchingNet & 66.39±0.78 & 64.45±0.63 & 36.74±0.53 & 22.40±0.70 & 46.53±0.68 & 63.16±0.77 & 38.99±0.64 & 51.37±0.77   \\ 
% \cline{2-10}
  &  MAML & 78.05±0.68 & 71.70±0.72 & 40.13±0.58 & 23.48±0.96 & - & - & - & 47.20±1.10   \\ 
% \cline{2-10}
  &  RelationNet & 68.99±0.75 & 61.31±0.72 & 39.41±0.58 & 22.96±0.88 & 44.00±0.60 & 63.32±0.76 & 37.33±0.68 & 57.77±0.69  \\ 
% \cline{2-10}
  &  ProtoNet &  79.72±0.67 & 73.29±0.71 & 39.57±0.57 & 24.05±1.01 & - & - & - & \textbf{67.00±1.00}    \\ 
% \cline{2-10}
  &  GNN & 87.96±0.67 & \textbf{83.64±0.77} & 43.94±0.67 & 25.27±0.46 & 52.53±0.59 & \textbf{70.84±0.65} & \textbf{44.28±0.63} & 62.25±0.65    \\ 
 \hline
\multirow{5}*{20}  & Fine-tuning & \textbf{95.91±0.72} & \textbf{87.97±0.42} & \textbf{61.09±0.44} & \textbf{31.01±0.59} & - & - & - & - \\ 
% \cline{2-10}
  & MatchingNet & 76.38±0.67 & 77.10±0.57 &  45.72±0.53 & 23.61±0.86 & - & - & - & -  \\ 
% \cline{2-10}
  &  MAML & 89.75±0.42 & 81.95±0.55 & 52.36±0.57 & 27.53±0.43 & - & - & - & -    \\ 
% \cline{2-10}
  &  RelationNet & 80.45±0.64 & 74.43±0.66 & 41.77±0.49 &  26.63±0.92 & - & - & - & -  \\ 
% \cline{2-10}
  &  ProtoNet  &  88.15±0.51 & 82.27±0.57 & 49.50±0.55 & 28.21±1.15 & - & - & - & -    \\ 
% \cline{2-10}
%  &  GNN  & - & - & - & - & - & - & - & -  \\ 
 \hline
\multirow{4}*{50}  & Fine-tuning & \textbf{97.48±0.56} & \textbf{92.00±0.56} & \textbf{67.20±0.59} & \textbf{36.79±0.53} & - & - & - & -  \\ 
% \cline{2-10}
  & MatchingNet & 58.53±0.73 & 54.44±0.67 &  54.58±0.65 & 22.12±0.88 & - & - & - & -   \\ 
% \cline{2-10}
%  &  MAML & - & - & - & - & - & - & - & -    \\ 
% \cline{2-10}
  &  RelationNet & 85.08±0.53 & 74.91±0.58 & 49.32±0.51 & 28.45±1.20 & - & - & - & -   \\ 
% \cline{2-10}
  &  ProtoNet & 90.81±0.43 & 80.48±0.57 & 51.99±0.52 & 29.32±1.12 & - & - & - & -    \\
% \cline{2-10}
%  &  GNN & - & - & - & - & - & - & - & -  \\ 
 \hline
\end{tabular}}
\vspace{-0.3cm}
\label{fsl_mtd}
\end{table}

Additionally, due to the varying implementation requirements (\eg, specific datasets, different backbones) and configurations (\eg, training sets, learning paradigms, modules) of current CDFSL approaches, it is impractical to compare all methods in a unified and fair manner. However, it remains important to present the key details of representative CDFSL methods, including their requirements, configurations, and performance highlights. To this end, Table~\ref{repre_mtd} summarizes the performance of selected CDFSL approaches evaluated on commonly used benchmarks, FGCB and BSCD-FSL. The best results for \textit{5-way 1-shot} and \textit{5-way 5-shot} are highlighted in blue and red, while the sub-optimal results are shown in light blue and light red, respectively.
\textcolor{black}{A comparison of the state-of-the-art performance of different method categories on BSCD-FSL reveals that hybrid methods are more effective than others, especially in near-domain scenarios. Moreover, methods from $\mathcal{D}$-Extension obtain sub-optimal performance. In distant-domain scenarios, $\Delta$-Adaptation methods perform best. Additionally, $\mathcal{D}$-Extension methods achieve the best performance in FGCB, leveraging the benefits of information augmentation in fine-grained migration.} 

An emerging approach in CDFSL involves integrating plug-and-play modules into existing FSL paradigms, such as MatchingNet, RelationNet, and GNN. Figures~\ref{module-comp1} and~\ref{module-comp2} illustrate the performance of these methods on the BSCD-FSL and FGCB benchmarks, respectively. Five key conclusions can be drawn from these figures. First, GNN exhibits the best overall performance, indicating its superior ability to handle CDFSL tasks compared to MatchingNet and RelationNet. Second, performance decreases as the distance between the target and source domains increases, as seen in the results from CropDiseases to ChestX. Third, there is no significant performance improvement when increasing samples (from \textit{1-shot} to \textit{5-shot}) for distant-domain tasks. Fourth, Wave-SAN performs particularly well on near-domain tasks, while this advantage becomes less obvious as the amount of data increases. Fifth, MemREIN achieves the best results across all fine-grained cross-domain datasets.
\iffalse
 \begin{sidewaystable}%[ph!]
  \tiny
 \vspace{5.45in}
 \begin{center}
     \caption{The CDFSL performance of the proposed methods on BSCD-FSL and FGCB benchmarks. $K$ means \textit{5-way K-shot}. ‘KBS’ is Knowledge-Based Systems.}
     \vspace{-0.3cm}
% \centering
\setlength{\tabcolsep}{1.0mm}{
\begin{tabular} {c|lcccccccccccc|m{4cm}}
\hline 
\textbf{Type} & \textbf{Methods} & \textbf{Venue} & \textbf{Train set} & \textbf{Backbone} & \textbf{$K$} & \textbf{CropDiseases} & \textbf{EuroSAT} & \textbf{ISIC} & \textbf{ChestX} & \textbf{Plantae} & \textbf{Places} & \textbf{Cars} & \textbf{CUB} & \textbf{Highlight}  \\ 
 \hline
\multirow{13}*{\rotatebox{90}{Instance-guided}} & \multirow{3}*{NSAE~\cite{boosting}} & \multirow{3}*{ICCV} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 &  93.31±0.42 & 84.33±0.55 & \color{red}{\textbf{55.27±0.62}} & 27.30±0.42 & 62.15±0.77 & 73.17±0.72 & 58.30±0.75 & 71.92±0.77 & \multirow{3}{4.1cm}{The latent noise information from the source domain is utilized to capture broader variations of the feature distributions.}
\\
& & &  &  & 20 &  98.33±0.18 &  92.34±0.35 & 67.28±0.61 & 35.70±0.47 & 77.40±0.65 & 82.50±0.59 & 82.32±0.50 & 88.09±0.48 &    \\
% \cline{6-14}
& & & &  & 50 &  99.29±0.14 & 95.00±0.26 & 72.90±0.55 & 38.52±0.71 & 83.63±0.60 & 85.92±0.56 & - & 91.00±0.79 &   \\
\cline{2-15}
& \multirow{2}*{ISSNet~\cite{data_multi_3}} & \multirow{2}*{ICIP} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ other 7 datasets}} & \multirow{2}*{ResNet10} & 1 & 73.40±0.86 & 64.50±0.88 & 36.06±0.69 & \color{blue!50}{\textbf{23.23±0.42}} & - & - & - & - & \multirow{2}{4.1cm}{Transferring styles across multiple sources to broaden the distribution of labeled sources} \\
% \cline{6-14}
&  &  &  &  & 5 & 94.10±0.41 & 83.64±0.55 & 51.82±0.67 &  \color{red}{\textbf{28.79±0.48}} &  -  & - & - & - &   \\
\cline{2-15}
& \multirow{2}*{DSL~\cite{data_target_1}} & \multirow{2}*{ICLR} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 41.17±0.80 & 53.16±0.88 & 37.13±0.69 & 50.15±0.80 & \multirow{2}{4.1cm}{Incorporating the cross-domain scenario into the training stage by rapidly switching targets}  \\
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 62.10±0.75 & 74.10±0.72 & 58.53±0.73 & 73.57±0.65 &   \\
 \cline{2-15}
& \multirow{2}*{STARTUP~\cite{st}} & \multirow{2}*{ICLR} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & 75.93±0.80 & 63.88±0.84 & 32.66±0.60 & 23.09±0.43 & - & - & - & - & \multirow{2}{4.1cm}{Self-training a source representation using unlabeled data from the target domain}  \\ % to help the model adapt to the target domain
% \cline{6-14}
&  &  &  &  & 5 & 93.02±0.45 &  82.29±0.60 & 47.22±0.61 & 26.94±0.44 & - & - & - & - &   \\
\cline{2-15}
& \multirow{2}*{DDA~\cite{dynamic}} & \multirow{2}*{NIPS} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & 82.14±0.78 & 73.14±0.84 & 34.66±0.58 & \color{blue}{\textbf{23.38±0.43}} & - & - & - & - & \multirow{2}{4.1cm}{Propose a dynamic distillation-based approach to enhance utilize unlabeled target data} \\
% \cline{6-14}
&  &  &  &  & 5 & 95.54±0.38 & 89.07±0.47 & 49.36±0.59 & \color{red!50}{\textbf{28.31±0.46}} & - & - & - & - &    \\
\hline
% \hline
\multirow{18}*{\rotatebox{90}{Parameter-based}} & \multirow{3}*{SB-MTL~\cite{parameter_fix_1}} & \multirow{3}*{arXiv} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 & \color{red!50}{\textbf{96.01±0.40}} & 87.30±0.68 &  53.50±0.79 &  28.08±0.50 & - & - & - & - & \multirow{3}{4.1cm}{Leveraging a first-order MAML algorithm to identify optimal initializations and employing a score-based GNN for prediction}    \\
% \cline{6-14}
& &  &  &  & 20 &  99.61±0.09 &  96.53±0.28 & 70.31±0.72 & 37.70±0.57 & - & - & - & - &  \\
% \cline{6-14}
& &  &  &  & 50 & 99.85±0.06 & 98.37±0.18 & 78.41±0.66 & 43.04±0.66 & - & - & - & - &   \\
\cline{2-15}
&  \multirow{4}*{VDB~\cite{parameter_fix_6}} & \multirow{4}*{CVPRW} & \multirow{4}*{\textit{mini}ImageNet} & \multirow{4}*{ResNet10} & 1 &  71.98±0.82 &  63.60±0.87 & 35.32±0.65 &  22.99±0.44 & - & - & - & - & \multirow{8}{4.1cm}{Propose a source-free approach through ``Visual Domain Bridge'' concept to mitigate internal mismatches in BatchNorm during cross-domain settings} \\ 
% \cline{6-14}
&  &  &  &  & 5  &  90.77±0.49 & 82.06±0.63 & 48.72±0.65 & 26.62±0.45 &   & - & - & - &   \\
% \cline{6-14}
&  &  &  &  & 20 &  96.36±0.27 & 89.42±0.45 & 59.09±0.59 & 31.87±0.44 & - & - & - & - &    \\
% \cline{6-14}
&  &  &  &  & 50 & 97.89±0.19 & 92.24±0.35 & 64.02±0.58 & 35.55±0.45 & - & - & - & - &    \\
% \cline{5-14}
% &  &  &  & \multirow{4}*{ResNet18} & 1 & 75.46±0.76 & 67.76±0.83 &  33.22±0.58 &  22.28±0.41 & - & - & - & - &  \\  
% \cline{6-14}
% & &  &  &  & 5 &  93.11±0.42 & 85.29±0.52 & 47.48±0.61 & 25.25±0.42 & - & - & - & - & \\
% \cline{6-14}
% &  &  &  &  & 20 & 97.61±0.21 & 91.93±0.37 & 58.89±0.59 & 29.49±0.42 & - & - & - & - &   \\
% \cline{6-14}
% &  &  &  &  & 50 &  98.40±0.16 & 93.95±0.30 &  64.23±0.58 & 32.37±0.47 & - & - & - & - &   \\
\cline{2-15}

& \multirow{2}*{FGNN~\cite{parameter_fix_7}} & \multirow{2}*{KBS} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & -  & - & 41.44±0.69 & 56.74±0.82 & 34.37±0.60 & 52.97±0.75 & \multirow{2}{4.1cm}{Investigating instance normalization and the restitution module to enhance performance} \\   % Exploring an instance normalization algorithm to alleviate feature \\ dissimilarity, and a restitution module to restitute the discrimination \\ ability from the learned knowledge
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 60.81±0.66 & 76.12±0.63 & 50.19±0.69 & 71.99±0.64 &   \\
\cline{2-15}

& \multirow{2}*{MAP~\cite{parameter_select_2}} & \multirow{2}*{arXiv} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 5 & 90.29±1.56 & 82.76±2.00 &  47.85±1.95 &  24.79±1.22 & 58.45±1.15 & 75.94±0.97 & 51.64±1.16 & 67.92±1.10 & \multirow{2}{4.1cm}{Selectively performs SOTA adaptation methods in sequence with modular adaptation method} \\  % A modular adaptation method is proposed to selectively performs multiple state-of-the-art (SOTA) adaptation methods in sequence
% \cline{6-14}
&  &  &  &  & 20 & 95.22±1.13 & 88.11±1.78 &  60.16±2.70 & 30.21±1.78 & - & - & - & - &   \\
\cline{2-15}
& HVM~\cite{parameter_weight_2} & ICLR & \makecell[c]{\textit{mini}ImageNet} & ResNet10 & 5 & 87.65±0.35 & 74.88±0.45 & 42.05±0.34 & 27.15±0.45 & - & - & - & - & Introducing a hierarchical prototype model and a hierarchical alternative to address domain gaps by flexibly utilizing features at varying semantic levels  \\
 \cline{2-15}
& \multirow{2}*{ReFine~\cite{parameter_weight_1}} & \multirow{2}*{ICMLW} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 68.93±0.84 & 64.14±0.82 & 35.30±0.59 & 22.48±0.41 & - & - & - & - & \multirow{2}{4.1cm}{Randomizing the fitted parameters from the source domain before adapting to target data}  \\ 
% \cline{6-14}
&  &  &  &  & 5 & 90.75±0.49 & 82.36±0.57 & 51.68±0.63 & 26.76±0.42 & - & - & - & - &   \\
\hline % \hline
\multirow{16}*{\rotatebox{90}{Feature post-processing}} & \multirow{3}*{CHEF~\cite{rf}} & \multirow{3}*{arXiv} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet18} & 5 & 86.87±0.27 & 74.15±0.27 & 41.26±0.34 & 24.72±0.14 & - & - & - & - & \multirow{3}{4.1cm}{Ensembling representation fusion through Hebbian learners operating on different layers of the network}    \\
% \cline{4-12}
%  &  &  & 5 &  96.09±0.35 & 87.53±0.50 & 56.85±0.67 & 28.73±0.45 & 65.66±0.78   & 73.40±0.71 & 61.11±0.79 & 76.00±0.71 &  \\
% \cline{6-14}
& &  &  &  & 20 & 94.78±0.12 & 83.31±0.14 & 54.30±0.34 & 29.71±0.27 & - & - & - & - &  \\
% \cline{6-14}
& &  &  &  & 50 & 96.77±0.08 & 86.55±0.15 &  60.86±0.18 & 31.25±0.20 & - & - & - & - &   \\
\cline{2-15}
&  \multirow{2}*{LRP~\cite{feature_reweight_1}} & \multirow{2}*{ICPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 34.80±0.37 &  50.59±0.46 & 29.65±0.33 & 42.44±0.41 & \multirow{2}{4.1cm}{A training strategy guided by explanations is developed to identify important features} \\  
% \cline{6-14}
& &  &  &  & 5 & - & - & - & - & 48.09±0.35 & 66.90±0.40 & 39.19±0.38 & 59.30±0.40 & \\
\cline{2-15}
& \multirow{3}*{Confess~\cite{confess}} & \multirow{3}*{ICLR} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 & 88.88±0.51 & 84.65±0.38 & 48.85±0.29 & 27.09±0.24 & - & - & - & - & \multirow{3}{4.1cm}{Investigating a contrastive learning and feature selection system to address domain gaps between base and novel categories} \\
% \cline{6-14}
&  &  &  &  & 20 & 95.34±0.48 & 90.40±0.24 & 60.10±0.33 & 33.57±0.31 & - & - & - & - &   \\
% \cline{6-14}
 &  &  &  &  & 50 & 97.56±0.43 & 92.66±0.36 & 65.34±0.45 & 39.02±0.12 & - & - & - & - &    \\
% \cline{4-9}
% &  &  & 50 &  &  &  &  &    &&&   \\
\cline{2-15}
& BL-ES~\cite{feature_reweight_2} & ICME & \makecell[c]{\textit{mini}ImageNet}  & ResNet18 & 5 & - & 79.78±0.83 & - & - & - & - & 50.07±0.84 & 69.63±0.88 & Proposing a bi-level episode strategy to train an inductive graph network of learning comparison and induction simultaneously  \\
 \cline{2-15}
& \multirow{3}*{TACDFSL~\cite{feature_reweight_4}} & \multirow{3}*{Symmetry} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{WideResNet} & 5 & 93.42±0.55 & 85.19±0.67 & 45.39±0.67 & 25.32±0.48 & - & - & - & - & \multirow{3}{4.1cm}{Introducing the empirical marginal distribution measurement}  \\
% \cline{6-14}
&  &  &  &  & 20 & 95.49±0.39 & 87.87±0.49 & 53.15±0.59 & 29.17±0.52 & - & - & - & - &   \\
% \cline{6-14}
 &  &  &  &  & 50 & 95.88±0.35 & 89.07±0.43 &  56.68±0.58 & 31.75±0.51 & - & - & - & - &   \\
% \cline{5-13}
% &  &  &  & 50 &  &  &  &  &     &&&   \\ 
\cline{2-15}
& \multirow{2}*{RDC~\cite{feature_reweight_6}} & \multirow{2}*{CVPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & \color{blue!50}{\textbf{86.33±0.50}} & 71.57±0.50 & 35.84±0.40 & 22.27±0.20 & 44.33±0.60 & 61.50±0.60 & 39.13±0.50 & 51.20±0.50 & \multirow{2}{4.1cm}{Minimising task-irrelevant features by constructing subspace} \\ % Constructing a non-linear subspace to minimise task-irrelevant features and keep more discriminative information by a hyperbolic tangent transformation
% \cline{6-14}
&  &  &  &  & 5 & 93.55±0.30 & 84.67±0.30 & 49.06±0.30 & 25.48±0.20 & 60.63±0.40 & 74.65±0.40 & 53.75±0.50 & 67.77±0.40 &    \\
\cline{2-15}
& \multirow{2}*{StyleAdv~\cite{feature_reweight_0}} & \multirow{2}*{CVPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 84.11±0.57 & 74.93±0.58 & 33.99±0.46 & 22.92±0.32 & \color{blue}{\textbf{55.52±0.66}} & \color{blue}{\textbf{72.64±0.67}} & 40.48±0.57 & \color{blue}{\textbf{84.01±0.58}} & \multirow{2}{4.1cm}{A meta Style Adversarial training method and a style adversarial attack method} \\ % Constructing a non-linear subspace to minimise task-irrelevant features and keep more discriminative information by a hyperbolic tangent transformation
% \cline{6-14}
&  &  &  &  & 5 & 95.99±0.27 & \color{red!50}{\textbf{90.12±0.33}} & 51.23±0.51 & 26.97±0.33 & \color{red}{\textbf{78.01±0.54}} & \color{red}{\textbf{88.33±0.40}} & 66.02±0.64 & \color{red}{\textbf{95.82±0.27}} &    \\
\hline % \hline
\multirow{10}*{\rotatebox{90}{Hybrid}} & \multirow{2}*{FDMixup~\cite{hybrid_1}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 66.23±1.03 & 62.97±1.01 & 32.48±0.64 & 22.26±0.45 & 37.89±0.58 & 53.57±0.75 & 31.14±0.51 & 46.38±0.68 & \multirow{2}{4.1cm}{Utilizing few labeled target data to guide the model learning} \\ 
% \cline{6-14}
&  &  &  &  & 5 & 87.27±0.69 & 80.48±0.79 & 44.28±0.66 & 24.52±0.44 & 54.62±0.66 & 73.42±0.65 & 41.30±0.58 & 64.71±0.68 &    \\
 \cline{2-15} 
% &  &  &  & 20 &  &  &  &  &    &&&   \\
% \cline{5-13}
% &  &  &  & 50 &  &  &  &  &     &&&   \\
 & \multirow{2}*{TL-SS~\cite{hybrid_3}} & \multirow{2}*{AAAI} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - &  65.73 & - & - & - & 55.83 & 33.22 & 45.92 & \multirow{2}{4.1cm}{Introducing a domain-irrelevant self-supervised learning method} \\ % A domain-independent task-level self-supervised method is proposed to improve the model generalization ability
% \cline{6-14}
&  &  &  &  & 5 & - & 79.36 & - & - & - &  76.33 & 49.82 & 69.16 &    \\
% \cline{5-13}
\cline{2-15}
& \multirow{2}*{TGDM~\cite{tgdm}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & \color{blue!50}{\textbf{52.39±0.25}} & \color{blue!50}{\textbf{61.88±0.26}} & \color{blue}{\textbf{50.70±0.24}} & 64.80±0.26 & \multirow{2}{4.1cm}{A method generates an intermediate domain generation to facilitate the FSL task}  \\ % Re-randomizing the parameters fitted on the source domain before adapting to the target data
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 71.78±0.22 & \color{red!50}{\textbf{81.62±0.19}} & \color{red}{\textbf{70.99±0.21}} & \color{red!50}{\textbf{84.21±0.18}} &   \\
\cline{2-15}
& \multirow{2}*{ME-D2N~\cite{data_target}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & \color{blue}{\textbf{52.89±0.83}} & 60.36±0.86 & \color{blue!50}{\textbf{49.53±0.79}} & \color{blue!50}{\textbf{65.05±0.83}} & \multirow{2}{4.1cm}{AME-D2N utilizes a multi-expert learning approach to create a model}  \\ 
&  &  &  &  & 5 & - & - & - & - & \color{red}{\textbf{72.87±0.67}} & 80.45±0.62 & \color{red!50}{\textbf{69.17±0.68}} & 83.17±0.56 &   \\
\cline{2-15}
& \multirow{2}*{CLDFD~\cite{cdfsl231}} & \multirow{2}*{ICLR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 &  \color{blue}{\textbf{90.48±0.72}} & \color{blue}{\textbf{82.52±0.76}} & \color{blue}{\textbf{39.70±0.69}} & 22.39±0.44 & - & - & - & - & \multirow{2}{4.1cm}{A cross-level knowledge distillation method and feature denoising operation}  \\ 
&  &  &  &  & 5 & \color{red}{\textbf{96.58±0.39}} & \color{red}{\textbf{92.89±0.34}} &  \color{red!50}{\textbf{52.29±0.62}} & 25.98±0.43 & - & - & - & - &   \\
\hline

\end{tabular}}
\label{repre_mtd}
 \end{center}
 
 \end{sidewaystable}
\fi







\begin{sidewaystable}%[ph!]
  \tiny
 \vspace{5.45in}
 \begin{center}
     \caption{\textcolor{black}{The CDFSL performance of the proposed methods on BSCD-FSL and FGCB benchmarks. $K$ means \textit{5}-way \textit{K}-shot.}}
     \vspace{-0.3cm}
% \centering
\setlength{\tabcolsep}{0.4mm}{
\begin{tabular} {c|lcccccccccccc|m{4cm}}
\Xhline{1.2pt}
\textbf{Type} & \textbf{Methods} & \textbf{Venue} & \textbf{Train set} & \textbf{Backbone} & \textbf{$K$} & \textbf{CropDiseases} & \textbf{EuroSAT} & \textbf{ISIC} & \textbf{ChestX} & \textbf{Plantae} & \textbf{Places} & \textbf{Cars} & \textbf{CUB} & \textbf{Highlight}  \\ 
\Xhline{1.2pt}
\multirow{14}*{\rotatebox{90}{$\mathcal{D}$-Extension}} & \multirow{3}*{NSAE~\cite{boosting}} & \multirow{3}*{ICCV} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 &  93.31±0.42 & 84.33±0.55 & \color{red!50}{\textbf{55.27±0.62}} & 27.30±0.42 & 62.15±0.77 & 73.17±0.72 & 58.30±0.75 & 71.92±0.77 & \multirow{3}{4.1cm}{The latent noise information from the source domain is utilized to capture broader variations of the feature distributions.}
\\
& & &  &  & 20 &  98.33±0.18 &  92.34±0.35 & 67.28±0.61 & 35.70±0.47 & 77.40±0.65 & 82.50±0.59 & 82.32±0.50 & 88.09±0.48 &    \\
% \cline{6-14}
& & & &  & 50 &  99.29±0.14 & 95.00±0.26 & 72.90±0.55 & 38.52±0.71 & 83.63±0.60 & 85.92±0.56 & - & 91.00±0.79 &   \\
\cline{2-15}
& \multirow{2}*{RDC~\cite{feature_reweight_6}} & \multirow{2}*{CVPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & \color{blue!50}{\textbf{86.33±0.50}} & 71.57±0.50 & 35.84±0.40 & 22.27±0.20 & 44.33±0.60 & \color{blue!50}{\textbf{61.50±0.60}} & 39.13±0.50 & 51.20±0.50 & \multirow{2}{4.1cm}{Minimising task-irrelevant features by constructing subspace} \\ % Constructing a non-linear subspace to minimise task-irrelevant features and keep more discriminative information by a hyperbolic tangent transformation
% \cline{6-14}
&  &  &  &  & 5 & 93.55±0.30 & 84.67±0.30 & 49.06±0.30 & 25.48±0.20 & 60.63±0.40 & 74.65±0.40 & 53.75±0.50 & 67.77±0.40 &    \\
\cline{2-15}
& \multirow{2}*{StyleAdv~\cite{feature_reweight_0}} & \multirow{2}*{CVPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 80.69±0.28 & 72.92±0.75 & 35.76±0.52 & 22.64±0.35 & 41.13±0.67 & 58.58±0.83 & 35.09±0.55 & 48.49±0.72 & \multirow{2}{4.1cm}{A meta Style Adversarial training method and a style adversarial attack method} \\ % Constructing a non-linear subspace to minimise task-irrelevant features and keep more discriminative information by a hyperbolic tangent transformation
% \cline{6-14}
&  &  &  &  & 5 & \color{red!50}{\textbf{96.51±0.28}} & \color{red!50}{\textbf{91.64±0.43}} & 53.05±0.54 & 26.24±0.35 & 64.10±0.64 & 79.35±0.61 & 56.44±0.68 & 70.90±0.63 &    \\
\cline{2-15}
& \multirow{2}*{ISSNet~\cite{data_multi_3}} & \multirow{2}*{ICIP} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ other 7 datasets}} & \multirow{2}*{ResNet10} & 1 & 73.40±0.86 & 64.50±0.88 & 36.06±0.69 & 23.23±0.42 & - & - & - & - & \multirow{2}{4.1cm}{Transferring styles across multiple sources to broaden the distribution of labeled sources} \\
% \cline{6-14}
&  &  &  &  & 5 & 94.10±0.41 & 83.64±0.55 & 51.82±0.67 &  \color{red!50}{\textbf{28.79±0.48}} &  -  & - & - & - &   \\
\cline{2-15}
& \multirow{2}*{STARTUP~\cite{st}} & \multirow{2}*{ICLR} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & 75.93±0.80 & 63.88±0.84 & 32.66±0.60 & 23.09±0.43 & - & - & - & - & \multirow{2}{4.1cm}{Self-training a source representation using unlabeled data from the target domain}  \\ % to help the model adapt to the target domain
% \cline{6-14}
&  &  &  &  & 5 & 93.02±0.45 &  82.29±0.60 & 47.22±0.61 & 26.94±0.44 & - & - & - & - &   \\
\cline{2-15}
% & \multirow{3}*{CHEF~\cite{rf}} & \multirow{3}*{arXiv} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet18} & 5 & 86.87±0.27 & 74.15±0.27 & 41.26±0.34 & 24.72±0.14 & - & - & - & - & \multirow{3}{4.1cm}{Ensembling representation fusion through Hebbian learners operating on different layers of the network}    \\
% \cline{4-12}
%  &  &  & 5 &  96.09±0.35 & 87.53±0.50 & 56.85±0.67 & 28.73±0.45 & 65.66±0.78   & 73.40±0.71 & 61.11±0.79 & 76.00±0.71 &  \\
% \cline{6-14}
% & &  &  &  & 20 & 94.78±0.12 & 83.31±0.14 & 54.30±0.34 & 29.71±0.27 & - & - & - & - &  \\
% \cline{6-14}
% & &  &  &  & 50 & 96.77±0.08 & 86.55±0.15 &  60.86±0.18 & 31.25±0.20 & - & - & - & - &   \\
% \cline{2-15}
& \multirow{2}*{TGDM~\cite{tgdm}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & \color{blue!50}{\textbf{52.39±0.25}} & \color{blue}{\textbf{61.88±0.26}} & 50.70±0.24 & \color{blue!50}{\textbf{64.80±0.26}} & \multirow{2}{4.1cm}{\textcolor{black}{A method generates an intermediate domain generation to facilitate the FSL task}}  \\ % Re-randomizing the parameters fitted on the source domain before adapting to the target data
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 71.78±0.22 & \color{red}{\textbf{81.62±0.19}} & 70.99±0.21 & \color{red}{\textbf{84.21±0.18}} &   \\
\cline{2-15}
& HVM~\cite{parameter_weight_2} & ICLR & \makecell[c]{\textit{mini}ImageNet} & ResNet10 & 5 & 87.65±0.35 & 74.88±0.45 & 42.05±0.34 & 27.15±0.45 & - & - & - & - & Introducing a hierarchical prototype model and an alternative method to solve domain gaps by using features at different semantic levels  \\
\Xhline{1.2pt}
% \hline



\multirow{11}*{\rotatebox{90}{$\mathcal{H}$-Constraint}} & \multirow{3}*{SB-MTL~\cite{parameter_fix_1}} & \multirow{3}*{arXiv} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 & 96.01±0.40 & 87.30±0.68 &  53.50±0.79 &  28.08±0.50 & - & - & - & - & \multirow{3}{4.1cm}{Leveraging a first-order MAML algorithm to identify optimal initializations and employing a score-based GNN for prediction}    \\
% \cline{6-14}
& &  &  &  & 20 &  99.61±0.09 &  96.53±0.28 & 70.31±0.72 & 37.70±0.57 & - & - & - & - &  \\
% \cline{6-14}
& &  &  &  & 50 & 99.85±0.06 & 98.37±0.18 & 78.41±0.66 & 43.04±0.66 & - & - & - & - &   \\
 \cline{2-15}
& \multirow{2}*{ReFine~\cite{parameter_weight_1}} & \multirow{2}*{ICMLW} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 68.93±0.84 & 64.14±0.82 & 35.30±0.59 & 22.48±0.41 & - & - & - & - & \multirow{2}{4.1cm}{Randomizing the fitted parameters from the source domain before adapting to target data}  \\ 
% \cline{6-14}
&  &  &  &  & 5 & 90.75±0.49 & 82.36±0.57 & 51.68±0.63 & 26.76±0.42 & - & - & - & - &   \\
\cline{2-15}
& \multirow{2}*{ME-D2N~\cite{data_target}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & \color{blue}{\textbf{52.89±0.83}} & 60.36±0.86 & 49.53±0.79 & \color{blue}{\textbf{65.05±0.83}} & \multirow{2}{4.1cm}{AME-D2N utilizes a multi-expert learning approach to create a model}  \\ 
&  &  &  &  & 5 & - & - & - & - & \color{red}{\textbf{72.87±0.67}} & \color{red!50}{\textbf{80.45±0.62}} & 69.17±0.68 & \color{red!50}{\textbf{83.17±0.56}} &   \\
\cline{2-15}
 & \multirow{2}*{MC-TS~\cite{wang2024cross}} & \multirow{2}*{EngA} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 74.36±0.88 &  63.21±0.88 & 33.87±0.59 & \color{blue!50}{\textbf{23.61±0.42}} & - & - & - & - & \multirow{2}{4.1cm}{\textcolor{black}{Match the large and small crops for images,and predict them by teacher and student network}} \\
&  &  &  &  & 5 & 92.20±0.54 & 81.52±0.60 & 47.68±0.62 & 27.06±0.37& - &  - & - & - &    \\
\cline{2-15}
&  \multirow{2}*{LRP~\cite{feature_reweight_1}} & \multirow{2}*{ICPR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 34.80±0.37 &  50.59±0.46 & 29.65±0.33 & 42.44±0.41 & \multirow{2}{4.1cm}{A training strategy guided by explanations is developed to identify important features} \\  
% \cline{6-14}
& &  &  &  & 5 & - & - & - & - & 48.09±0.35 & 66.90±0.40 & 39.19±0.38 & 59.30±0.40 & \\
% \cline{5-13}
\Xhline{1.2pt} % \hline



\multirow{12}*{\rotatebox{90}{$\Delta$-Adaptation}} 
% &  \multirow{2}*{CosML~\cite{data_multi_1}} & \multirow{2}*{arXiv} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ Cars / Places}} & \multirow{2}*{Conv-4} & 1 & - & - & - & - & 30.93±0.46 & 53.96±0.62 & 47.74±0.59 & 46.89±0.59 & \multirow{2}{4.1cm}{Exploring multi-domain pre-train schemes to quickly adapt the model to unseen domains} \\
% \cline{6-14}
% & & & &  & 5 & - & - & - & - & 42.96±0.57 & 88.08±0.46 & 60.17±0.63 & 66.15±0.63 & \\
% \cline{2-15}
&  \multirow{3}*{DARA~\cite{zhao2023dual}} & \multirow{3}*{TPAMI} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 1 &  81.50±0.66 &  69.39±0.84 & \color{blue!50}{\textbf{38.49±0.66}} &  22.93±0.40 & 51.25±0.58 & 42.08±0.55 & \color{blue!50}{\textbf{52.70±0.83}} & 35.25±0.57 & \multirow{3}{4.1cm}{\textcolor{black}{Focus on the fast adaptation capability of meta-learners by proposing an effective dual adaptive representation alignment approach}} \\ 
% \cline{6-14}
&  &  &  &  & 5  &  96.23±0.34 & 87.67±0.54 & \color{red}{\textbf{57.54±0.68}} & 28.78±0.45 & \color{red!50}{\textbf{72.15±0.43}} & 65.40±1.95 & \color{red}{\textbf{77.51±0.65}} & 58.44±2.39 &   \\
% \cline{6-14}
&  &  &  &  & 20 &  99.21±0.11 & 94.40±0.27 & 68.43±0.54 & 36.20±0.43 & 83.12±0.54 & 79.54±0.64 & 89.60±0.43 & 81.38±0.59 &    \\
\cline{2-15}
% &  \multirow{4}*{VDB~\cite{parameter_fix_6}} & \multirow{4}*{CVPRW} & \multirow{4}*{\textit{mini}ImageNet} & \multirow{4}*{ResNet10} & 1 &  71.98±0.82 &  63.60±0.87 & 35.32±0.65 &  22.99±0.44 & - & - & - & - & \multirow{4}{4.1cm}{Propose a source-free approach through ``Visual Domain Bridge'' concept to mitigate internal mismatches in BatchNorm during cross-domain settings} \\ 
% \cline{6-14}
% &  &  &  &  & 5  &  90.77±0.49 & 82.06±0.63 & 48.72±0.65 & 26.62±0.45 &   & - & - & - &   \\
% \cline{6-14}
% &  &  &  &  & 20 &  96.36±0.27 & 89.42±0.45 & 59.09±0.59 & 31.87±0.44 & - & - & - & - &    \\
% \cline{6-14}
% &  &  &  &  & 50 & 97.89±0.19 & 92.24±0.35 & 64.02±0.58 & 35.55±0.45 & - & - & - & - &    \\
%  \cline{2-15}
% & \multirow{2}*{MAP~\cite{parameter_select_2}} & \multirow{2}*{arXiv} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 5 & 90.29±1.56 & 82.76±2.00 &  47.85±1.95 &  24.79±1.22 & 58.45±1.15 & 75.94±0.97 & 51.64±1.16 & 67.92±1.10 & \multirow{2}{4.1cm}{Selectively performs SOTA adaptation methods in sequence with modular adaptation method} \\  % A modular adaptation method is proposed to selectively performs multiple state-of-the-art (SOTA) adaptation methods in sequence
% \cline{6-14}
% &  &  &  &  & 20 & 95.22±1.13 & 88.11±1.78 &  60.16±2.70 & 30.21±1.78 & - & - & - & - &   \\
% \cline{2-15}
& \multirow{3}*{TACDFSL~\cite{feature_reweight_4}} & \multirow{3}*{Symmetry} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{WideResNet} & 5 & 93.42±0.55 & 85.19±0.67 & 45.39±0.67 & 25.32±0.48 & - & - & - & - & \multirow{3}{4.1cm}{Introducing the empirical marginal distribution measurement}  \\
% \cline{6-14}
&  &  &  &  & 20 & 95.49±0.39 & 87.87±0.49 & 53.15±0.59 & 29.17±0.52 & - & - & - & - &   \\
% \cline{6-14}
 &  &  &  &  & 50 & 95.88±0.35 & 89.07±0.43 &  56.68±0.58 & 31.75±0.51 & - & - & - & - &   \\
% \cline{5-13}
% &  &  &  & 50 &  &  &  &  &     &&&   \\ 
\cline{2-15}
& \multirow{2}*{IM-DCL~\cite{xu2024enhancing}} & \multirow{2}*{TIP} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet}} & \multirow{2}*{ResNet10} & 1 & 84.37±0.99 & \color{blue!50}{\textbf{77.14±0.71}} & 38.13±0.57 & \color{blue}{\textbf{23.98±0.79}} & - & - & - & - & \multirow{2}{4.1cm}{\textcolor{black}{Solve CDFSL by an enhanced information maximization}}  \\
% \cline{6-14}
&  &  &  &  & 5 & 95.73±0.38 & 89.47±0.42 & 52.74±0.69 & \color{red}{\textbf{28.93±0.41}} & - & - & - & - &   \\
\cline{2-15}
& \multirow{3}*{Confess~\cite{confess}} & \multirow{3}*{ICLR} & \multirow{3}*{\textit{mini}ImageNet} & \multirow{3}*{ResNet10} & 5 & 88.88±0.51 & 84.65±0.38 & 48.85±0.29 & 27.09±0.24 & - & - & - & - & \multirow{3}{4.1cm}{Investigating a contrastive learning and feature selection system to address domain gaps between base and novel categories} \\
% \cline{6-14}
&  &  &  &  & 20 & 95.34±0.48 & 90.40±0.24 & 60.10±0.33 & 33.57±0.31 & - & - & - & - &   \\
% \cline{6-14}
 &  &  &  &  & 50 & 97.56±0.43 & 92.66±0.36 & 65.34±0.45 & 39.02±0.12 & - & - & - & - &    \\
% \cline{4-9}
% &  &  & 50 &  &  &  &  &    &&&   \\
\cline{2-15}
& BL-ES~\cite{feature_reweight_2} & ICME & \makecell[c]{\textit{mini}ImageNet}  & ResNet18 & 5 & - & 79.78±0.83 & - & - & - & - & 50.07±0.84 & 69.63±0.88 & Proposing a bi-level episode strategy to train an inductive graph network  \\
% \cline{2-15}
%& \multirow{2}*{DSL~\cite{data_target_1}} & \multirow{2}*{ICLR} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet}} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 41.17±0.80 & 53.16±0.88 & 37.13±0.69 & 50.15±0.80 & \multirow{2}{4.1cm}{Incorporating the cross-domain scenario into the training stage by rapidly switching targets}  \\
% \cline{6-14}
%&  &  &  &  & 5 & - & - & - & - & 62.10±0.75 & 74.10±0.72 & 58.53±0.73 & 73.57±0.65 &   \\
\Xhline{1.2pt} % \hline



\multirow{14}*{\rotatebox{90}{Hybrid}} & \multirow{2}*{FDMixup~\cite{hybrid_1}} & \multirow{2}*{ACM MM} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & 66.23±1.03 & 62.97±1.01 & 32.48±0.64 & 22.26±0.45 & 37.89±0.58 & 53.57±0.75 & 31.14±0.51 & 46.38±0.68 & \multirow{2}{4.1cm}{Utilizing few labeled target data to guide the model learning} \\ 
% \cline{6-14}
&  &  &  &  & 5 & 87.27±0.69 & 80.48±0.79 & 44.28±0.66 & 24.52±0.44 & 54.62±0.66 & 73.42±0.65 & 41.30±0.58 & 64.71±0.68 &    \\
\cline{2-15}
 & \multirow{2}*{TL-SS~\cite{hybrid_3}} & \multirow{2}*{AAAI} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - &  65.73 & - & - & - & 55.83 & 33.22 & 45.92 & \multirow{2}{4.1cm}{Introducing a domain-irrelevant self-supervised learning method} \\ % A domain-independent task-level self-supervised method is proposed to improve the model generalization ability
% \cline{6-14}
&  &  &  &  & 5 & - & 79.36 & - & - & - &  76.33 & 49.82 & 69.16 &    \\
\cline{2-15}
& \multirow{2}*{FGNN~\cite{parameter_fix_7}} & \multirow{2}*{KBS} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 & - & - & -  & - & 41.44±0.69 & 56.74±0.82 & 34.37±0.60 & 52.97±0.75 & \multirow{2}{4.1cm}{Investigating instance normalization and the restitution module to enhance performance} \\   % Exploring an instance normalization algorithm to alleviate feature \\ dissimilarity, and a restitution module to restitute the discrimination \\ ability from the learned knowledge
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 60.81±0.66 & 76.12±0.63 & 50.19±0.69 & 71.99±0.64 &   \\
\cline{2-15}
% &  &  &  & 20 &  &  &  &  &    &&&   \\
% \cline{5-13}
% &  &  &  & 50 &  &  &  &  &     &&&   \\
& \multirow{2}*{DDA~\cite{dynamic}} & \multirow{2}*{NIPS} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ target data}} & \multirow{2}*{ResNet10} & 1 & 82.14±0.78 & 73.14±0.84 & 34.66±0.58 & 23.38±0.43 & - & - & - & - & \multirow{2}{4.1cm}{Propose a dynamic distillation-based approach to enhance utilize unlabeled target data} \\
% \cline{6-14}
&  &  &  &  & 5 & 95.54±0.38 & 89.07±0.47 & 49.36±0.59 & 28.31±0.46 & - & - & - & - &    \\
\cline{2-15}
& \multirow{2}*{CLDFD~\cite{cdfsl231}} & \multirow{2}*{ICLR} & \multirow{2}*{\textit{mini}ImageNet} & \multirow{2}*{ResNet10} & 1 &  \color{blue}{\textbf{90.48±0.72}} & \color{blue}{\textbf{82.52±0.76}} & \color{blue}{\textbf{39.70±0.69}} & 22.39±0.44 & - & - & - & - & \multirow{2}{4.1cm}{A cross-level knowledge distillation method and feature denoising operation}  \\ 
&  &  &  &  & 5 & \color{red}{\textbf{96.58±0.39}} & \color{red}{\textbf{92.89±0.34}} &  52.29±0.62 & 25.98±0.43 & - & - & - & - &   \\
\cline{2-15}
 & \multirow{2}*{ProD~\cite{ma2023prod}} & \multirow{2}*{CVPR} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet}} & \multirow{2}*{ResNet10 \& ViT} & 1 & - & - & - & - & 42.86±0.59 & 53.92±0.72 & 38.02±0.63 & 53.97±0.71 & \multirow{2}{4.1cm}{Using the prompts to disentangle the domain-general and -specific knowledge} \\
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 65.82±0.65 & 75.00±0.72 & 59.49±0.68 & 79.19±0.59 &    \\
 \cline{2-15} 
 & \multirow{2}*{GMeta-FDMixup~\cite{hybrid_7}} & \multirow{2}*{TIP} & \multirow{2}*{\makecell[c]{\textit{mini}ImageNet \\ Labeled target data}} & \multirow{2}*{ResNet10} & 1 & - & - & - & - & 51.28±0.36 & 59.39±0.37 & \color{blue}{\textbf{53.10±0.35}} & 63.85±0.37 & \multirow{2}{4.1cm}{\textcolor{black}{Utilize a few labeled target data and a generalized network to solve CDFSL}} \\
% \cline{6-14}
&  &  &  &  & 5 & - & - & - & - & 69.45±0.30 & 78.80±0.28 & \color{red!50}{\textbf{71.80±0.30}} & 80.48±0.27 &    \\
\Xhline{1.2pt}
\end{tabular}}
\label{repre_mtd}
 \end{center}
 \end{sidewaystable}


\subsubsection{\textcolor{black}{Evaluation for $\mathcal{D}$-Extension Approaches}}
% Table~\ref{repre_mtd} highlights a noticeable trend in which the performance decreases as the distance between the target domain and source domain increases. For instance, the results show a drop from 93.31\% on CropDiseases to 27.30\% on ChestX (\textit{5-way 5-shot}). A comparison between~\cite{data_multi_1} and~\cite{data_target_1} also reveals that the former outperforms the latter on places (88.08\%) and cars (60.17\%) but underperforms on the other two datasets (42.96\% and 66.15\% vs. 62.10\% and 73.57\%). This discrepancy can be attributed to the difference in training data, as the former incorporates places and cars into the training process leading to overfitting on these two datasets. On the other hand, the results of~\cite{st} and~\cite{dynamic} on BSCD-FSL demonstrate that incorporating target domain data into the training process can improve the performance on the target domain. However, this approach works better for near-domain transfer than for distance-domain transfer. For example,~\cite{dynamic} showed a 4.90\% improvement on CropDiseases and 7.31\% improvement on EuroSAT but a 0.32\% drop on ISIC and only a 2.22\% improvement on ChestX when compared to the classic fine-tuning method.
\textcolor{black}{Table~\ref{repre_mtd} highlights a noticeable trend where performance decreases as the distance between the target and source domains increases. For instance, the results of \cite{boosting} show a drop from 93.31\% on CropDiseases to 27.30\% on ChestX (\textit{5-way 5-shot}). The sub-optimal performances of \cite{feature_reweight_6} and \cite{feature_reweight_0} on CropDiseases (96.51\% for \cite{feature_reweight_0} on \textit{5-way 5-shot}) and EuroSAT (91.54\% for \cite{feature_reweight_0} on \textit{5-way 5-shot}) reveal that specific guidance in information extension can further improve performance in near-domain scenarios. Additionally, the optimal results on FGCB for \cite{tgdm} also indicate the positive effect of information augmentation on CDFSL. Furthermore, the results of \cite{st} on BSCD-FSL demonstrate that incorporating target domain data into the training process can improve performance in the target domain. However, this approach works better for near-domain transfer than for distant-domain transfer. For example, \cite{st} showed a 2.38\% improvement on CropDiseases and a 0.53\% improvement on EuroSAT, but a 2.46\% drop on ISIC and only a 0.85\% improvement on ChestX when compared to the classic fine-tuning method.}

% Instance-guided approaches for CDFSL are relatively simple in concept as they rely on adding supplementary information to enhance the model's generalization. However, their effectiveness is highly dependent on the choice of information used in the training process. If the additional domains included in training greatly diverge from the target domain or the selected target domain samples are not representative, this can negatively affect CDFSL performance.
\textcolor{black}{$\mathcal{D}$-Extension approaches for CDFSL are relatively simple in concept as they rely on adding supplementary information to enhance the model's generalization. However, their effectiveness is highly dependent on the choice of information used in the training process.} If the additional information included in training greatly diverge from the target domain or the selected target domain samples are not representative, this fails to improve CDFSL performance.
\begin{figure}%[b]
	\centering
 \vspace{-0.2cm}
	%\includegraphics[width=\linewidth]{compare-16.pdf}
 \includegraphics[width=\linewidth]{response/fig12.pdf}
 \vspace{-0.5cm}
	\caption{\textcolor{black}{We group by different FSL paradigms, \ie, MatchingNet, RelationNet, and GNN. The performance of different methods on the BSCD-FSL benchmark is presented in (a) \textit{5-way 1-shot} and (b) \textit{5-way 5-shot} tasks. All methods use ResNet10 as the backbone. `CropD' refers to the dataset `CropDiseases'.}} % slashed area
% \vspace{-0.2cm}
	\label{module-comp1}
\end{figure}

\begin{figure}%[b]
	\centering
 \vspace{-0.2cm}
	%\includegraphics[width=\linewidth]{compare-17.pdf}
        \includegraphics[width=\linewidth]{response/fig13.pdf}
 \vspace{-0.5cm}
	\caption{\textcolor{black}{The performance of various approaches on the FGCB benchmark is presented in (a) \textit{5-way 1-shot} and (b) \textit{5-way 5-shot} tasks. All methods use ResNet10 as the backbone. `CropD' refers to the dataset `CropDiseases.'}} % slashed area
 \vspace{-0.4cm}
	\label{module-comp2}
\end{figure}

\subsubsection{\textcolor{black}{Evaluation for $\mathcal{H}$-Constraint Approaches}}
% From the data presented in Table~\ref{repre_mtd}, it appears that the performance of parameter-based methods is generally subpar in comparison to the other two method types. Using ResNet10 as the backbone, the results of~\cite{parameter_fix_1} on BSCD-FSL (\textit{5-way 5-shot}) demonstrate this trend, with scores of 96.01\% (CropDiseases), 87.30\% (EuroSAT), 53.50\% (ISIC), and 28.08\% (ChestX). The results of other methods within this category are even lower. When comparing the use of ResNet10 (90.77\% of CropDiseases, 82.06\% of EuroSAT, 48.72\% of ISIC, 26.62\% of ChestX) and ResNet18 (93.11\% of CropDiseases, 85.29\% of EuroSAT, 47.48\% of ISIC, 25.25\% of ChestX) as the backbone for ~\cite{parameter_fix_6} on BSCD-FSL, it is observed that while increasing the depth of the network enhances performance on near-domain datasets (CropDiseases, EuroSAT), it deteriorates performance on distant-domain datasets (ISIC, ChestX). As such, the best balance of near-domain and distant-domain performance is achieved when using ResNet10 as the backbone.
From the data presented in Table~\ref{repre_mtd}, it appears that the performance of $\mathcal{H}$-Constraint methods is generally subpar in comparison to the other two method types on BSCD-FSL. Using ResNet10 as the backbone, the results of ~\cite{parameter_fix_1} on BSCD-FSL (\textit{5-way 5-shot}) demonstrate this trend, with scores of 96.01\% (CropDiseases), 87.30\% (EuroSAT), 53.50\% (ISIC), and 28.08\% (ChestX). The results of other methods within this category are even lower. \textcolor{black}{However,~\cite{wang2024cross} achieves sub-optimal results of 23.61\% on ChestX in the 5-way 1-shot task. Moreover,~\cite{data_target} performs best on FGCB, achieving optimal results of 52.89\% (Plantae), 65.05\% (CUB) in the 5-way 1-shot task, and 72.87\% (Plantae) in the 5-way 5-shot task. These findings demonstrate the potential of knowledge distillation in distant-domain scenarios.}

% Our analysis of Table~\ref{repre_mtd} reveals that the performance of the parameter-based methods tends to be subpar in comparison to the first two categories of methods. The reason behind this is thought to be the local adjustment of network parameters by these methods using a module to fit the new domain. Although this reduction of hypothesis space may appear advantageous, it limits the adaptation of the method to data distribution and hypothesis space due to the limited introduction of additional parameters. Therefore, parameter-based methods in CDFSL often face limitations in augmenting and mining shared knowledge, which makes it more challenging to solve the two-stage empirical risk minimization problem compared to other categories of methods. Thus, researchers need to explore new methods and techniques that can overcome these limitations and improve the performance of parameter-based methods in CDFSL.
\textcolor{black}{Our analysis of Table~\ref{repre_mtd} reveals that the performance of $\mathcal{H}$-Constraint methods is generally lower than that of the other two categories. This is likely due to their reliance on locally adjusting network parameters with modules to adapt to the new domain. While reducing the hypothesis space may seem beneficial, it limits the method's ability to adapt to the data distribution and hypothesis space due to the restricted introduction of additional parameters~\cite{paszke2019pytorch}. Consequently, $\mathcal{H}$-Constraint methods in CDFSL often face limitations by only constraining the hypothesis space, making it more challenging to solve the two-stage empirical risk minimization problem compared to other categories. However, knowledge distillation methods obtain competitive results. Therefore, researchers need to explore new techniques and approaches based on knowledge distillation to overcome these limitations and improve the performance of $\mathcal{H}$-Constraint methods in CDFSL.}

\subsubsection{\textcolor{black}{Evaluation for $\Delta$-Adaptation Approaches}}
% Despite the challenge of directly comparing the performance of different feature post-processing methods due to the utilization of various backbones, it can still be noted that the performance of the recent approaches have a big advantage on near-domain tasks in comparison to instance-guided methods. This was exemplified by comparing two representative approaches:~\cite{feature_reweight_0} get the suboptimal perofrmances on near-domain datasets of BSCD-FSL (95.99\% for CropDiseases, 90.12\% for EuroSAT), which are better than those of~\cite{dynamic} on the same benchmark (95.54\% for CropDiseases, 89.07\% for EuroSAT). This trend was also reflected in the results on FGCB (78.01\%, 88.33\%, 66.02\%, and 95.82\% of~\cite{feature_reweight_0} vs. 62.15\%, 73.17\%, 58.30\%, and 71.92\% of~\cite{boosting}), achieving the best performance. These observations suggest that the feature post-processing methods are more effective than instance-guided approaches in addressing near-domain tasks. However, not all methods in this category performs better, which means that the design and application of post-processing strategies also have a great impact on performance.
The performance of recent $\Delta$-Adaptation approaches has a significant advantage on distant-domain tasks compared to other methods. This can be exemplified by comparing two representative approaches: \textcolor{black}{\cite{zhao2023dual} achieved sub-optimal performance on ISIC in the 5-way 1-shot task (38.49\%) and optimal performance in the 5-way 5-shot task (57.54\%). Meanwhile, \cite{xu2024enhancing} obtained the best results for ChestX in both the 1-shot (23.98\%) and 5-shot (28.93\%) tasks. In addition to BSCD-FSL, $\Delta$-Adaptation approaches also show overwhelming performance advantages in fine-grained migration scenarios. For instance,~\cite{zhao2023dual} achieved competitive results on Plantae (72.15\% in the 5-way 5-shot task) and Cars (77.51\% in the 5-way 5-shot task).} These observations suggest that $\Delta$-Adaptation methods are more effective than other approaches in addressing distant-domain tasks. However, not all methods in the $\Delta$-Adaptation category perform better, indicating that the design and application of $\Delta$-Adaptation strategies have a significant impact on performance.

% The comparison of the results of instance-guided and feature post-processing methods reveals a difference in their approach to uncovering shared knowledge between the source and target domains. Instance-guided methods prioritize the introduction of additional information during the training phase, effectively creating a more favorable shared feature extraction environment. On the other hand, feature post-processing methods aim to maximize the utilization of the shared knowledge available.
\textcolor{black}{As shown in Table~\ref{repre_mtd}, the performance of $\Delta$-Adaptation approaches indicates that, compared to other strategies, $\Delta$-Adaptation is the most effective for addressing distant-domain tasks. Furthermore, $\Delta$-Adaptation is also effective in handling few-shot tasks in fine-grained migration scenarios.}

\subsubsection{\textcolor{black}{Evaluation for Hybrid Approaches}}
% For instance, a study conducted in~\cite{data_target} produced results of 80.45\%, 69.17\%, and 83.17\% on Places, Cars, and CUB datasets, achieving the suboptimal results. Coincidentally,~\cite{cdfsl231} gets the optimal results on BSCD-FSL (96.58\% and 92.89\% on CropDiseases and EuroSAT datasets). However, it is important to note that combining strategies from different categories of methods carries a degree of risk, as there may be negative interactions between the different strategies. This highlights the high degree of precision required when matching strategies in hybrid methods. Ultimately, the choice of which approach to use depends on the specific task and available data, as well as the desired level of generalization and flexibility required for the model.
Hybrid methods demonstrate optimal performance on near-domain tasks. For instance, \cite{cdfsl231} achieves the best results on CropDiseases (90.48\% in the 1-shot task and 96.58\% in the 5-shot task) and EuroSAT (82.52\% in the 1-shot task and 92.89\% in the 5-shot task). \textcolor{black}{However, this advantage diminishes as the domain gap widens.} It is important to recognize that combining strategies from different categories introduces a degree of risk, as negative interactions between strategies can occur \cite{azevedo2024hybrid}. \textcolor{black}{Thus, when selecting hybrid strategies, researchers must carefully consider the compatibility of different methods, especially regarding the precision required for alignment. Ultimately, the choice of approach depends on the specific task, available data, and the level of generalization and flexibility required by the model.}

\subsubsection{\textcolor{black}{Evaluation on Meta-Dataset}}
% The techniques tested on the Meta-Dataset~\cite{meta-dataset} utilize non-episodic training, and the evaluation results are presented in Table~\ref{meta_per}. The evaluation is conducted using two setups: single-source-based (where the source domain is ImageNet) and multiple-sources-based (where the source domains are the first eight datasets). In the single source-based setting, ProtoNet, MAML, and Pro-MAML, serve as baselines to compare with the proposed approaches. The results reveal that~\cite{feature_select_2} achieved the best results on five target datasets, while~\cite{parameter_weight_4} attained the best results on the remaining five target datasets. Additionally, the findings indicate that deeper backbone networks, such as ResNet34 in~\cite{parameter_weight_4}, tend to outperform shallower ones, like ResNet18. The training results of multiple sources-based models illustrate that~\cite{parameter_weight_4} achieved the highest performance on all target datasets. This is believed to be due to the technique's effective combination of multiple sources and scientifically designed parameter reweighting strategy. A comparison of the two setups of~\cite{parameter_weight_4} using ResNet18 shows that the incorporation of multiple datasets results in significant performance improvements on eight seen datasets, but only a modest improvement on one unseen dataset. This indicates that introducing multiple domains without careful consideration may not necessarily enhance performance significantly. In conclusion, the proposed methods significantly enhance CDFSL performance relative to traditional FSL techniques, demonstrating the effectiveness of these methods in solving $FG$ and $Art$ CDFSL problems.
The techniques tested on the Meta-Dataset~\cite{meta-dataset} utilize non-episodic training, and the evaluation results are presented in Table~\ref{meta_per}. The evaluation was conducted using two setups: a single-source setup (with ImageNet as the source domain) and a multiple-source setup (with the first eight datasets as source domains). In the single-source setup, ProtoNet, MAML, and Pro-MAML serve as baselines for comparison with the proposed approaches. \textcolor{black}{The results reveal that~\cite{feature_select_2} achieved the best performance on five target datasets, while~\cite{parameter_weight_4, hybrid_2, yang2024leveraging} performed best on the remaining five target datasets.} Additionally, the findings indicate that deeper backbone networks, such as ResNet34 in~\cite{parameter_weight_4}, tend to outperform shallower ones, like ResNet18. \textcolor{black}{The results from the multiple-source models indicate that~\cite{wang2023mmt} achieved the highest performance on three target datasets, likely due to the technique's effective combination of multiple sources and a well-designed feature transformation strategy.} A comparison of the two setups for~\cite{parameter_weight_4} using ResNet18 demonstrates that incorporating multiple datasets significantly improved performance on eight seen datasets, but resulted in only modest improvements on the unseen dataset. This suggests that introducing multiple domains without careful consideration may not always lead to substantial performance gains. In conclusion, CDFSL technologies significantly enhance performance relative to traditional FSL methods, demonstrating their effectiveness in addressing $FG$ and $Art$ CDFSL problems.
\begin{table}%[b]
\tiny
\centering
\vspace{-0.3cm}
\caption{\textcolor{black}{The CDFSL performance of approaches on Meta-Dataset. $\star$ means the results on the seen data set (source data set).}}
\vspace{-0.3cm}
\setlength{\tabcolsep}{0.01mm}{
\begin{tabular}{lccccccc|ccccc}
\hline
 & \multicolumn{7}{c|}{\textbf{Single source}} & \multicolumn{5}{c}{\textbf{Multiple sources}}    \\ 
\cline{2-13}
 & ProtoNet~\cite{proto} & MAML~\cite{al21} & Pro-M~\cite{meta-dataset} & SUR~\cite{feature_select_2} & TPA~\cite{parameter_weight_4} & tri-M~\cite{hybrid_2} & ProLAD~\cite{yang2024leveraging} & RMFS~\cite{feature_select_1} & TPA~\cite{parameter_weight_4} & TSA~\cite{sreenivas2023similar} & TA2-Net~\cite{guo2023task} & MMT~\cite{wang2023mmt}   \\ 
\hline
Backbone & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18 & ResNet18   \\ 
\hline
ImageNet & {44.5 $\pm$ 1.1}{$\star$} & ${32.4\pm 1.0}{\star}$ & ${47.9\pm 1.1}{\star}$ & ${57.2\pm 1.1}{\star}$ & \textbf{59.5 $\pm$ 1.1}{$\star$} & ${58.6\pm 1.0}{\star}$ & ${57.2\pm 1.1}{\star}$ & \textbf{63.1 $\pm$ 0.8}{$\star$} & ${59.5\pm 1.0}{\star}$ & ${59.5\pm 1.0}{\star}$ & ${59.6\pm 1.0}{\star}$ & ${59.6\pm 1.1}{\star}$   \\ 
\cline{2-8}
Omniglot & $79.6\pm 1.1$ & $71.9\pm 1.2$ & $82.9\pm 0.9$ & \textbf{93.2 $\pm$ 0.8} & $78.2\pm 1.2$ & $92.0\pm 0.6$ & $84.1\pm 1.2$ & \textbf{97.7 $\pm$ 0.5}{$\star$} & ${94.9\pm 0.4}{\star}$ & ${94.9\pm 0.4}{\star}$ & ${95.5\pm 0.4}{\star}$ & ${94.4\pm 0.4}{\star}$   \\ 

Aircraft & $71.1\pm 0.9$ & $52.8\pm 0.9$ & $74.2\pm 0.8$ & \textbf{90.1 $\pm$ 0.8} & $72.2\pm 1.0$ & $82.8\pm 0.7$ & $76.1\pm 1.2$ & ${65.1\pm 0.3}{\star}$ & {89.9 $\pm$ 0.4}{$\star$} & ${89.9\pm 0.4}{\star}$ & ${90.5\pm 0.4}{\star}$ & \textbf{91.9$\pm$ 0.5}{$\star$}   \\ 

Birds & $67.0\pm 1.0$ & $47.2\pm 1.1$ & $70.0\pm 1.0$ & \textbf{82.3 $\pm$ 0.8} & $74.9\pm 0.9$ & $75.3\pm 0.8$ & $75.5\pm 1.0$ & \textbf{84.1 $\pm$ 0.6}{$\star$} & ${81.1\pm 0.8}{\star}$ & ${81.1\pm 0.8}{\star}$ & ${81.4\pm 0.8}{\star}$ & ${82.7\pm 0.5}{\star}$   \\ 

Textures & $65.2\pm 0.8$ & $56.7\pm 0.7$ & $67.9\pm 0.8$ & $73.5\pm 0.7$ & \textbf{77.3 $\pm$ 0.7} & $71.2\pm 0.8$ & $77.7\pm 0.8$ & ${67.5\pm 0.9}{\star}$ & ${77.5 \pm 0.7}{\star}$ & ${77.5\pm 0.7}{\star}$ & ${77.4\pm 0.7}{\star}$ & \textbf{78.2$\pm$ 0.9}{$\star$}   \\ 

Quick Draw & $65.9\pm 0.9$ & $50.5\pm 1.2$ & $66.6\pm 0.9$ & \textbf{81.9 $\pm$ 1.0} & $67.6\pm 0.9$ & $77.3\pm 0.7$ & $70.6\pm 0.1$ & \textbf{86.2 $\pm$ 0.5}{$\star$} & ${81.7\pm 0.6}{\star}$ & ${81.7\pm 0.6}{\star}$ & ${82.5\pm 0.6}{\star}$ & ${83.1\pm 0.2}{\star}$   \\ 

Fungi & $40.3\pm 1.1$ & $21.0\pm 1.0$ & $42.0\pm 1.1$ & \textbf{67.9 $\pm$ 0.9} & $44.7\pm 1.0$ & $48.5\pm 1.0$ & $46.8\pm 1.2$ & ${62.5\pm 0.6}{\star}$ & ${66.3\pm 0.8}{\star}$ & ${66.3\pm 0.8}{\star}$ & ${66.3\pm 0.9}{\star}$ & ${66.2\pm 0.6}{\star}$   \\ 

VGG Flower & $86.9\pm 0.7$ & $70.9\pm 1.0$ & $88.5\pm 1.0$ & $88.4\pm 0.9$ & 90.9 $\pm$ 0.6 & $90.5\pm 0.5$ & \textbf{92.9 $\pm$ 0.6} & ${86.3\pm 0.3}{\star}$ & ${92.2 \pm 0.5}{\star}$ & ${92.2\pm 0.5}{\star}$ & \textbf{92.6$\pm$ 0.4}{$\star$} & ${90.7\pm 0.4}{\star}$   \\ 
\cline{9-13}
Traffic Sign & $46.5\pm 1.0$ & $34.2\pm 1.3$ & $34.2\pm 1.3$ & $67.4\pm 0.8$ & 82.5 $\pm$ 0.8 & $78.0\pm 0.6$ & \textbf{89.4$\pm$ 0.9} & $73.7\pm 0.4$ & $82.8 \pm 1.0$ & $82.8\pm 1.0$ & \textbf{87.4$\pm$ 0.8} & $85.1\pm 0.2$   \\ 

MSCOCO & $39.9\pm 1.1$ & $24.1\pm 1.1$ & $24.1\pm 1.1$ & $51.3\pm 1.0$ & \textbf{59.0 $\pm$ 1.0} & $52.8\pm 1.1$ & $55.4\pm 1.1$ & $56.2\pm 0.7$ & $57.6 \pm 1.0$ & $57.6\pm 1.0$ & $57.9\pm 0.9$ & \textbf{58.9$\pm$ 0.5}   \\ 

MNIST & - & - & - & $90.8\pm 0.5$ & $93.9\pm 0.6$ & \textbf{96.2 $\pm$ 0.3} & 95.8 $\pm$ 0.5 & - & $96.7 \pm 0.4$ & $96.7\pm 0.4$ & $97.0\pm 0.4$ & \textbf{97.3$\pm$ 0.3}   \\ 

CIFAR 10 & - & - & - & $66.6\pm 0.8$ & \textbf{82.1 $\pm$ 0.7} & $75.4\pm 0.8$ & $79.7\pm 0.8$ & - & \textbf{82.9 $\pm$ 0.7} & $82.9\pm 0.7$ & $82.1\pm 0.8$ & $82.0\pm 0.6$   \\ 

CIFAR 100 & - & - & - & $58.3\pm 1.0$ & \textbf{70.7 $\pm$ 0.9} & $62.0\pm 1.0$ & $70.3\pm 1.0$ & - & $70.4 \pm 0.9$ & $70.4\pm 0.9$ & $70.9\pm 0.9$ & \textbf{71.5$\pm$ 1.0}   \\ 
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{meta_per}
\end{table}
