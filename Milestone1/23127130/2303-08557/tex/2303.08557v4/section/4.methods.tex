\vspace{-0.2cm}
\section{Approaches} \label{methods}
CDFSL offers a unified solution to both cross-domain and FSL problems. Figure \ref{overview} outlines the CDFSL baseline process, consisting of two key steps: (1) Training a feature extractor on the source domain, and (2) Performing FSL on the target domain. The first step provides prior knowledge from the source domain, optimized via transfer learning~\cite{tfsurvey}, meta-learning~\cite{hospedales2021meta}, and metric learning~\cite{kaya2019deep} \etc techniques. The second step focuses on learning the feature extractor and target classifier with limited supervision. Based on the unique challenges, we categorize CDFSL algorithms into four types: $\mathcal{D}$-Extension, $\mathcal{H}$-Constraint, $\Delta$-Adaptation, and Hybrid Methods.

\begin{figure}
	\centering
	\includegraphics[width=11cm]{response/crop_overview1.pdf}
  \vspace{-0.3cm}
	\caption{An overview of the CDFSL method involves two main stages. First, existing techniques pretrain the encoder on the source domain. Second, the encoder is finetuned, and a target recognizer is trained on the support set (with limited labeled data) of target domain. And inference is performed using the finetuned encoder and the target recognizer on the query set. $\textit{D}^{s}$ and $\textit{D}^{t}$ mean the source and target data.}
  \vspace{-0.3cm}
	\label{overview}
\end{figure}

\vspace{-0.2cm}
\subsection{$\mathcal{D}$-Extension}  \label{instance}
$\mathcal{D}$-Extension enhances and expands existing information resources to improve the model’s generalization to new tasks, addressing the issue of unreliable TSERM. In CDFSL, information is typically represented in three dimensions: data space, feature space, and task space. Based on the dimension of information expansion, methods are classified into data augmentation, feature generation, and task synthesis. This classification demonstrates how $\mathcal{D}$-Extension systematically addresses the challenge of information scarcity across these three dimensions. Figure~\ref{fig_instance} illustrates how these methods work.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{response/crop_extension1.pdf}
  \vspace{-0.5cm}
	\caption{Solving CDFSL problem by $\mathcal{D}$-Extension, including data augmentation, feature generation, and task synthesis. Data augmentation aims to augment data by instance/pseudo-label generation, \etc. Feature generation enhances the features by fusing or re-weighting them. And task synthesis improves model generalization by generating more guided tasks. The core operations of methods are highlighted in red.
    }
  \vspace{-0.4cm}
	\label{fig_instance}
\end{figure}

\vspace{-0.2cm}
\subsubsection{Data Augmentation.} \label{data_original}
Data augmentation expands the information by generating additional samples. Current methods introduce various types of transformations for models to learn, such as flipping~\cite{shyam2017attentive}, scaling~\cite{lake2015human, zhang2018fine}, translation~\cite{benaim2018one, lake2015human}, cropping~\cite{zhang2018fine}, shearing~\cite{shyam2017attentive}, reflection~\cite{edwards2017towards}, and rotation~\cite{model32, miniimagenet}. However, it's impractical to account for all potential transformations manually. Thus, affine transformations alone cannot fully solve the CDFSL problem~\cite{shyam2017attentive, lake2015human}. Next, we explore more advanced data augmentation methods at the sample and label levels, including instance augmentation and pseudo-label generation.


\textbf{Instance Augmentation.}
Instance augmentation~\cite{data_multi_3,sreenivas2023similar,extend_1,data_other_1,adcd,boosting,tgdm,liu2024spectral} increases sample diversity by synthesizing new data. Techniques include style transfer~\cite{data_multi_3,sreenivas2023similar}, frequency domain transformation~\cite{extend_1,data_other_1,liu2024spectral}, and adversarial methods~\cite{adcd,boosting}. For instance, \cite{data_multi_3} introduces unlabeled data from multiple domains into the original source domain, generating large amounts of data with different styles but the same content. \cite{sreenivas2023similar} augments data by applying the style of semantically similar categories to each class. Additionally, \cite{extend_1} enhances samples by processing high-frequency components of source images, allowing the network to mimic human visual perception by selecting different frequency cues in recognition tasks. Similarly, \cite{data_other_1} generates a sketch map from the original image to capture high-frequency information. \cite{liu2024spectral} explores a data augmentation method that augments the image in the frequency domain and then converts it back to the spatial domain for use as synthetic data. \cite{adcd} improves model robustness by generating adversarial instances, while \cite{tgdm} facilitates knowledge transfer by introducing an intermediate domain through mixing source and target domain images. Furthermore,~\cite{boosting} performs adversarial training with noise data, enhancing model robustness.


\textbf{Pseudo-label Generation.}
Pseudo-label generation involves leveraging unlabeled data by assigning pseudo labels to it~\cite{data_target_2,st}. One early approach~\cite{data_target_2} enhances unlabeled data by applying rotations and assigning the rotation degree as the pseudo labels, turning transformation tasks into supervised learning problems. Additionally,~\cite{st} introduces unlabeled target data during the pre-training stage, generating pseudo labels for these data through a pre-trained model. This unlabeled target data serves as supplementary information from the target domain, helping to enrich the model's understanding of the target domain without manual annotation. Following~\cite{st},~\cite{samarasinghe2023cdfsl} addresses the cross-domain few-shot video action recognition.

\vspace{-0.2cm}
\subsubsection{Feature Generation} \label{data_multiple}
Feature generation, as shown in Figure~\ref{fig_instance}, creates new feature representations from existing ones, enriching the feature space and improving the model's ability to capture data patterns, especially in scenarios with limited data. The goal of generating new features is either to expand the total information content or to explore key features in greater depth. According to the focus of generating features, methods can be divided into two categories: (1) feature fusion~\cite{rf,feature_fusion_2,mdfsl,parameter_fix_3,feature_reweight_6}, aims to increase the total information content, and (2) feature re-weighting~\cite{parameter_weight_2,ji2024relevance,parameter_fix_9,feature_reweight_3,feature_reweight_7,parameter_weight_3,adversarial,feature_reweight_0}, which focuses on increasing the amount of critical information in the feature to optimize information usage.

\textbf{Feature Fusion.}
There are two main types of feature fusion methods~\cite{rf,feature_fusion_2,mdfsl,parameter_fix_3,feature_reweight_6} discussed here: multi-level fusion~\cite{rf,feature_fusion_2} and multi-model fusion~\cite{mdfsl,parameter_fix_3,feature_reweight_6}. Multi-level fusion focuses on combining features from different layers of the same model, addressing the limitations of using single-layer features. Low-level features may lack the specificity needed for complex tasks, while high-level features can overfit to the training data. By fusing these features, a more balanced and versatile representation is created, capable of handling a wide range of tasks~\cite{lin2017feature}. For example,\cite{rf} computes the loss for the last few layers of the model and sums them up as the final loss, while\cite{feature_fusion_2} fuses mid-level features to predict the residual term.
Multi-model fusion~\cite{mdfsl,parameter_fix_3,feature_reweight_6} refers to combining features generated by different models or strategies. Since these features come from diverse sources, fusing them leverages the strengths of each, creating a richer and more distinguishable feature representation. For instance,\cite{mdfsl} fuses knowledge from both class and semantic aspects,\cite{parameter_fix_3} averages the features from multiple models, and~\cite{feature_reweight_6} constructs a non-linear subspace, then combines the features from this subspace with the original features.

\textbf{Feature Re-weighting.}
Feature re-weighting~\cite{parameter_weight_2,ji2024relevance,parameter_fix_9,feature_reweight_3,feature_reweight_7,parameter_weight_3,feature_reweight_0,adversarial} adjusts the weights of different features so the model can adaptively select the most important ones for different tasks or domains, improving performance. For example, \cite{parameter_weight_2} learns separate weights for each multi-level feature, \cite{parameter_fix_9} re-weights features across different episodes, and~\cite{feature_reweight_7} introduces a forget-update module to re-weight features. Meanwhile, \cite{parameter_weight_3} applies attention mechanisms across multiple levels, and \cite{ji2024relevance} re-weights both original and style-transformed features through context interaction. \cite{feature_reweight_3} enhances feature representation by leveraging local feature correlations, while \cite{feature_reweight_0} inputs style-altered features into the FSL classifier.~\cite{adversarial} explores adversarial feature augmentation to simulate distribution variations by re-weighting features.


\vspace{-0.2cm}
\subsubsection{Task Synthesis} \label{data_target}
Task synthesis~\cite{ata,feature_fusion_3,rao2024rdprotofusion,data_other_2} (Figure~\ref{fig_instance}) expands the task space by creating new task scenarios or combinations, improving the model’s ability to generalize. Some methods generate challenging tasks to train the model in difficult situations. For example, \cite{ata} generates tasks simulating worst-case scenarios, helping the model adapt to a wider distribution of tasks during training. Other approaches synthesize tasks at the graph level. 
For instance, \cite{feature_fusion_3} present few-shot data as graphs and augment them from contextual and geometric perspectives, while \cite{rao2024rdprotofusion} integrates information from different tasks using a multi-task learning framework. Additionally, \cite{data_other_2} randomly combines affine-transformed samples into diverse tasks.

\vspace{-0.2cm}
\subsubsection{Discussion and Summary}
When additional unsupervised samples or data generation methods are available, the amount of information can be expanded through data augmentation. However, in data augmentation, particularly in instance augmentation, the augmented information is typically derived from the original samples, which results in limited valid information gain and a lack of real-world diversity. Additionally, the quality of generated pseudo-labels depends on the model's predictions; if the model has biases, the pseudo-label generation process may further amplify these biases.

When multiple features, including those from multiple models or levels, are available, feature generation methods can be applied. These methods enrich the feature space by processing multiple features to enhance the information. However, since these features come from the same examples, they are often highly correlated or redundant, which increases model complexity while providing limited additional information. Therefore, finding the optimal solution requires very fine adjustment~\cite{he2016deep}.

Finally, when auxiliary tasks are present, task synthesis enhances diversity by combining new tasks. However, this increased diversity can make the model’s learning objectives unclear and task relevance ambiguous. This occurs because different tasks may conflict or lack consistency, making it difficult for the model to identify which features or patterns are most important for the primary task. Additionally, auxiliary tasks can introduce irrelevant noise, further confusing the learning process. As a result, the composed tasks may not align well with the actual task, leading to sub-optimal performance in real-world applications~\cite{al32}.

\vspace{-0.2cm}
\subsection{$\mathcal{H}$-Constraint}  \label{hypothesis}
$\mathcal{H}$-Constraint are designed to solve the problem of unreliable TSERM by reducing the complexity of the hypothesis space $\mathcal{H}$. A smaller $\mathcal{H}$ limits the model’s complexity, which helps reduce overfitting and improves performance in the target domain. The methods in this section explore constraints on $\mathcal{H}$ from three perspectives: structural constraints, guided constraints, and normative constraints. Based on how these constraints are implemented, the methods are categorized into three types: (1) parameter adjustment, (2) knowledge distillation, and (3) regularization, as illustrated in Figure~\ref{parameterr}.
\begin{figure}
	\centering
  \vspace{-0.3cm}
	\includegraphics[width=\linewidth]{response/constraint_cropped.pdf}
 \vspace{-0.5cm}
	\caption{$\mathcal{H}$-Constraint methods, including parameter adjustment, knowledge distillation, and regularization. The gray background means the frozen parameters, and the core operations of methods are highlighted in red.}
 \vspace{-0.4cm}
	\label{parameterr}
\end{figure}

\vspace{-0.2cm}
\subsubsection{Parameter Adjustment} \label{fixed}
Parameter adjustment is a common strategy for imposing structural constraints, directly tuning parameters to reduce the complexity of the hypothesis space~\cite{parameter_fix_1,parameter_weight_1,parameter_weight_4,guo2023task,wu2024task,wu2024hybridprompt,gondal2024domain,zhuo2024prompt}. These parameters can come from existing models or be introduced through additional modules. Based on the source of the adjusted parameters, methods in this strategy are categorized into three groups: encoder models~\cite{parameter_fix_1,parameter_weight_1}, extra adapters~\cite{parameter_weight_4,guo2023task,gondal2024domain}, and prompts~\cite{wu2024task,wu2024hybridprompt,zhuo2024prompt,xu2024step}.

\textbf{Encoder Models.}
A common method for parameter adjustment is through the encoder models~\cite{parameter_fix_1,parameter_weight_1}. For instance,~\cite{parameter_fix_1} constrains the hypothesis space by alternately freezing certain model parameters in the inner and outer loops, allowing parameters in different regions to maximize their effectiveness. In addition,~\cite{parameter_weight_1} resets the parameters learned from the source domain before adapting to the target data. However, directly adjusting the parameters of the encoder model strictly limits the number of parameters that can be adjusted. When the encoder model has a large number of parameters, this strategy becomes inefficient.

\textbf{Adapters.}
To reduce the costs of parameter adjustment, extra adapters have been explored for insertion during the training phase~\cite{parameter_weight_4,guo2023task,gondal2024domain}. Specifically,~\cite{parameter_weight_4} constrains $\mathcal{H}$ by dynamically adapting the network via task-specific adapters. ~\cite{guo2023task} adjusts parameters through dynamic selection and activation of these task-specific adapters. And~\cite{gondal2024domain} introduces a lightweight adapter that is specifically trained
with an intra-modal contrastive objective.

\textbf{Prompts.}
With the widespread application of prompt learning, some methods~\cite{wu2024task,wu2024hybridprompt,zhuo2024prompt,xu2024step} address the CDFSL problem by adjusting only the prompt parameters. For example, \cite{wu2024task} generates task-adaptive prompts through a network, which are then used to guide the backbone network in solving the FSL task on the target domain. Building on this, \cite{wu2024hybridprompt} explores domain-aware prompting, transferring prompts learned from the source task to the target task. Moreover, \cite{zhuo2024prompt} treats prompts as a ``free lunch'' to enhance diversity and utilizes textual modality to guide prompt generation. And \cite{xu2024step} proposes and optimizes a style prompt to reduce the domain gaps.

\vspace{-0.2cm}
\subsubsection{Knowledge Distillation} \label{pa-select}
Knowledge distillation introduces additional supervisory signals through the soft labels, guiding the constraint on $\mathcal{H}$~\cite{wang2024cross,hybrid_4,yang2024leveraging,hybrid_5,data_target,data_multi_0}. Specifically, knowledge distillation involves using the outputs of a pre-trained teacher model as soft labels to guide the training of a student model, effectively transferring the teacher model's knowledge to the student model. We classify these methods from a technical perspective into standard knowledge distillation and multi-level knowledge distillation. For the standard knowledge distillation,~\cite{wang2024cross} uses a teacher-student network, combined with self-supervised learning, to generate soft labels that guide the training of the student model.

Other methods~\cite{hybrid_4,data_multi_0,yang2024leveraging,hybrid_5,data_target} explore multi-level knowledge distillation to solve the CDFSL problem. Specifically, there are multi-to-one and one-to-multi teacher-student networks. For example,~\cite{hybrid_4} trains a student network with multiple teacher networks, where these teacher networks are trained on datasets from different domains, respectively. ~\cite{hybrid_5} aligns the representations of the single student network with the multiple task/domain-specific ones using low-capacity adapters. Additionally,~\cite{data_target} guides the learning of the student model through two teacher models, including the source and the target teacher models. ~\cite{data_multi_0} proposes a multi-level knowledge stage to train the student network using multiple teacher models. In contrast,~\cite{yang2024leveraging} trains multiple student adapters in different stages, with the help of a single teacher network.

\vspace{-0.2cm}
\subsubsection{Regularization} \label{pa-weight}
Regularization constrains $\mathcal{H}$ by adding penalty terms to the loss function or introducing randomness, such as dropout. Some CDFSL approaches~\cite{feature_reweight_1,ji2023cross,parameter_select_1} apply regularization to limit $\mathcal{H}$. For example, \cite{feature_reweight_1} uses explainable feedback to guide model training, while \cite{ji2023cross,parameter_select_1} introduce dropout during training to impose constraints. Specifically, \cite{parameter_select_1} samples sub-networks by dropping individual neurons or entire feature maps, then selects the most suitable sub-networks to form an ensemble for target domain learning. Additionally, \cite{ji2023cross} combines pruning with dropout to dynamically adjust the model's complexity.

\vspace{-0.2cm}
\subsubsection{Discussion and Summary}
Alternating the adjustment of encoder model parameters allows for optimization across different tasks and datasets, improving model performance. However, this requires a well-designed strategy to avoid local optima, as an inappropriate adjustment strategy may lead to overfitting and reduced generalization~\cite{srivastava2014dropout}. Extra adapters and prompts can be flexibly inserted into existing models~\cite{houlsby2019parameter} to facilitate parameter updates, and prompt learning can achieve better performance with limited training data~\cite{gao2021making}. However, the design and selection of prompts are critical, as each task requires tailored prompts for optimal performance~\cite{brown2020language}.

For the knowledge distillation strategy (Section~\ref{pa-select}), by learning from the soft labels generated by the teacher model, the student model can better handle noisy data and label errors, thereby improving its robustness. However, the effectiveness of knowledge distillation depends on the quality of the teacher model—if the teacher model performs poorly, the student model will also be negatively affected~\cite{hinton2015distilling}. Additionally, knowledge distillation requires extra training steps, increasing both the complexity and duration of the training process~\cite{gou2021knowledge}.

Regularization (Section~\ref{pa-weight}) constrains the magnitude of weights, helping the model generalize better to unseen data~\cite{goodfellow2016deep}. However, in scenarios like small sample learning, regularization may not significantly improve performance~\cite{ng2004feature}. Additionally, if regularization parameters are not properly selected, such as when the regularization strength is too high—the model may be over-constrained, limiting its ability to learn complex patterns. In such cases, overfitting may not be effectively prevented.

\vspace{-0.2cm}
\subsection{$\Delta$-Adaptation}  \label{adaptation}
In FSL, source and target tasks usually come from the same domain, meaning $\Delta$ is nearly zero. However, in CDFSL, the gap between the source and target tasks can be much larger, leading to an unreliable TSERM. To address this, it’s important to reduce the disparity $\Delta$ between the tasks. This can be done in two ways: (1) adapting the features of both tasks to minimize differences, or (2) exploring and leveraging shared information to improve robustness even when differences remain. Based on how $\Delta$ is optimized, methods in this section are categorized into two groups (Figure~\ref{feature-post}): (1) Feature adaptation and (2) Shared information mining.

\begin{figure}
	\centering
  \vspace{-0.3cm}
 \includegraphics[width=\linewidth]{response/crop_adaptation1.pdf}
 \vspace{-0.6cm}
	\caption{The $\Delta$-Adaptation categories, including feature adaptation and shared information mining. Feature adaptation involves feature transformation and feature alignment. Shared information mining involves selection, decoupling, and meta-knowledge mining. The core operations of methods are highlighted in red.}
 \vspace{-0.4cm}
	\label{feature-post}
\end{figure}

\vspace{-0.2cm}
\subsubsection{Feature Adaptation} \label{select}
Feature adaptation aims to reduce the differences between the source and target domains by adjusting or transforming their features. Since the feature forms or structures of the source and target domains may exhibit fundamental differences, these disparities are often difficult to reconcile directly in the original feature space. Therefore, depending on whether the original feature space is preserved, the methods in this section are categorized into two approaches: feature transformation~\cite{feature_reweight_4,parameter_fix_8,wang2023mmt,feature_reweight_2,liu2022geometric,li2023knowledge}, which modifies the feature space, and feature alignment~\cite{paeedeh2024cross,feature_reweight_9,lscdfsl,zhao2023dual,xu2024enhancing,feature_reweight_5,parameter_fix_6}, which attempts to align features without altering the original space. 


\textbf{Feature Transformation.}
Feature transformation~\cite{feature_reweight_4,parameter_fix_8,wang2023mmt,feature_reweight_2,liu2022geometric,li2023knowledge} aligns the features of the source and target domains by applying transformations to the original feature space. For instance, \cite{feature_reweight_4} enhances CDFSL performance by using adaptive feature distribution transformations and task-adaptive strategies to measure domain discrepancies. \cite{feature_reweight_2} generates inductive meta points to abstract and transform features, while~\cite{parameter_fix_8} applies scale and shift operations via a feature transformation module. \cite{wang2023mmt} uses memory components for effective feature transformation and alignment between domains. Similarly, \cite{liu2022geometric} maps features into a high-dimensional geometric algebra space to preserve relationships across domains, and~\cite{li2023knowledge} refines the model's understanding of domain-specific features, aligning learned features with the target domain's characteristics.

\textbf{Feature Alignment.}
Feature alignment~\cite{paeedeh2024cross,feature_reweight_9,lscdfsl,zhao2023dual,xu2024enhancing,feature_reweight_5,parameter_fix_6} aims to adjust the feature distributions of the source and target domains to achieve consistent representations. In~\cite{lscdfsl}, the features are mapped into a shared latent subspace to align source and target features. Similarly, \cite{paeedeh2024cross} achieves alignment using transformer blocks, while \cite{feature_reweight_9} learns cross-domain aligned representations through bi-directional feature alignment. \cite{xu2024enhancing} optimizes target domain features via information maximization, implicitly aligning features. Additionally, \cite{feature_reweight_5} uses instance normalization to reduce feature dissimilarity, and \cite{parameter_fix_6} finetunes Batch Normalization (BN) layers to correct inter-domain distribution differences. Finally,~\cite{zhao2023dual} combines prototypical feature alignment with normalized distribution alignment.

\vspace{-0.2cm}
\subsubsection{Shared Information Mining} \label{fuse}
Shared information mining focuses on efficiently leveraging common features, patterns, or knowledge between the source and target domains to enhance model generalization in cross-domain learning. These shared features must be both transferable across domains and relevant to the source and target tasks. Based on the number of tasks involved in extracting shared information, existing methods are generally divided into three categories: selection, decoupling, and meta-knowledge mining.

\textbf{Selection.}
Selection operates within a single task, focusing on filtering features to identify the most relevant information~\cite{feature_select_1,feature_select_2,confess,data_multi_1,parameter_select_2,zhang2023cross}. The selected information is then used as shared knowledge. Methods in this section are classified into feature selection, parameter weighting, and task optimization. Feature selection~\cite{feature_select_1,feature_select_2,confess} identifies general feature components that aid the target FSL task. For instance, \cite{feature_select_1} proposes a multi-domain feature selection algorithm to optimize feature extraction and selection, while \cite{feature_select_2} uses multiple feature extractors to select representations relevant to the target domain. Additionally, \cite{confess} uses a learnable mask generator to identify shared features. Parameter weighting \cite{data_multi_1,parameter_select_2} adjusts model parameters to adapt to the target task. For example, \cite{data_multi_1} aggregates meta-model parameters from multiple domains to generate initialization parameters, and \cite{parameter_select_2} uses a dynamic mechanism to configure the best adaptation modules for the downstream task. Finally, task optimization~\cite{zhang2023cross} focuses on selecting the best source tasks to learn valuable knowledge for the target task.

\textbf{Decoupling.}
In CDFSL, source and target task features share both similarities and differences. Decoupling~\cite{xu2023cross,data_target_1,feature_reweight_8,zhou2024meta} aims to separate shared information from task-specific information to handle them independently and avoid interference. In~\cite{xu2023cross}, class-shared and class-specific dictionaries are combined to align source and target features, with the class-shared dictionary capturing common elements and class-specific dictionaries representing domain-specific features. Similarly, \cite{data_target_1} employs domain-specific and domain-general encoders to support the learner, while \cite{feature_reweight_8} uses a domain discriminator to distinguish between source and target features, allowing the extractor to learn domain-invariant features and reduce domain discrepancies. In addition, \cite{zhou2024meta} decouples the image into low-frequency content details and high-frequency robust structural characteristics, and improves the prediction performance by combining the effective parts of two.

\textbf{Meta-knowledge Mining.}
Meta-knowledge mining~\cite{parameter_fix_5,data_multi_2} operates across tasks and domains, extracting higher-order learning strategies that enable models to quickly adapt to new contexts. In~\cite{parameter_fix_5}, meta-learning is applied across domain tasks during training to generate prototypes suited for new tasks. Similarly,~\cite{data_multi_2} introduces a domain-agnostic meta-learning method that learns domain-independent initial parameters by observing both seen and pseudo-unseen tasks simultaneously.

\vspace{-0.2cm}
\subsubsection{Discussion and Summary}
Feature adaptation strategies (Section~\ref{select}) typically achieve alignment by minimizing distribution differences between the source and target domains. These methods are flexible, easy to implement, and can be applied to various tasks and datasets. However, in focusing on matching overall distribution statistics (e.g., mean and variance), they often overemphasize global feature consistency, potentially overlooking the local or specific information crucial for accurately modeling the target domain~\cite{dasurvey1}.

Shared information mining seeks to identify and leverage common knowledge between domains (Section~\ref{fuse}) and can be applied to single-task, dual-task, and multi-task settings. However, its effectiveness may be compromised by conflicting task objectives~\cite{zhao2019learning}. For instance, features beneficial to one task may be irrelevant or harmful to another, potentially confusing the model during training. To mitigate this, some methods incorporate task-specific knowledge alongside shared information. Rather than directly applying shared features,\cite{data_target_1} constrains their use through domain-specific insights, while\cite{zhang2023cross} evaluates the impact of shared features on the target domain and leverages them accordingly.


\vspace{-0.3cm}
\subsection{Hybrid Methods}  \label{hybrid}
The three strategies above address the unreliability of TSERM by optimizing specific factors: $\mathcal{D}$-Extension enhances the available information in $\mathcal{D}$, $\mathcal{H}$-Constraint limits the complexity of the hypothesis space $\mathcal{H}$, and $\Delta$-Adaptation optimizes $\Delta$. Each strategy has distinct advantages: $\mathcal{D}$-Extension is simple and practical, $\mathcal{H}$-Constraint reduces model complexity, and $\Delta$-Adaptation improves the transferability of knowledge from the source to the target domain. As a result, many studies have explored combining these strategies to leverage their strengths and further mitigate TSERM unreliability. Related technologies are listed in Table~\ref{table_hybrid}.

Some methods combine $\mathcal{D}$-Extension and $\mathcal{H}$-Constraint techniques~\cite{dynamic,perera2024discriminative,cdfsl231,ji2024soft,hybrid_3}. For example, \cite{dynamic} introduces additional unsupervised target domain samples during the pre-training phase and augments them by training the target domain encoder through self-distillation. Similarly, \cite{cdfsl231} incorporates unlabeled target data during training, using cross-level knowledge distillation to enhance the model's capacity to extract highly discriminative features from the target dataset. \cite{perera2024discriminative} fuses features at different levels during pre-training and introduces learnable anchors (acting as adapters) during fine-tuning to adapt to the target task. Furthermore, \cite{hybrid_3} addresses the FSL task in the target domain by synthesizing tasks from multiple perspectives and applying regularization based on consistency and correlation metrics. Finally, following \cite{ji2023cross}, \cite{ji2024soft} introduces additional unlabeled target data and proposes an iterative soft pruning training method that combines contrastive loss and regularization to remove redundant weights and optimize the model.

Some methods combine $\mathcal{D}$-Extension and $\Delta$-Adaptation strategies~\cite{hybrid_1,hybrid_7,hybrid_2,feature_reweight_11,feature_reweight_10}. For instance, \cite{hybrid_1} introduces additional labeled target instances and decouples features into domain-agnostic and domain-specific categories during fine-tuning, leveraging domain-agnostic features with a domain and FSL classifier. As an extension of \cite{hybrid_1}, \cite{hybrid_7} uses a mixed module to generate diverse images from both source and target domains and decouples features during meta-training. \cite{hybrid_2} decouples features to obtain domain-agnostic and domain-specific components, then adaptively weights these features for the target task. \cite{feature_reweight_11} fuses features at different levels to enhance generalization, then adapts to the target task through feature transformation during meta-training. \cite{feature_reweight_10} synthesizes tasks using style transformations and employs these tasks to learn and optimize feature transformation modules.

In the combination of $\mathcal{H}$-Constraint and $\Delta$-Adaptation methods~\cite{feature-wise,parameter_fix_7,zhang2024cross,ma2023prod}, \cite{feature-wise} asynchronously freezes and updates the feature-wise transformation layers and feature extractor. Building on this, \cite{parameter_fix_7} introduces a metric function to better measure the distance between labeled and unlabeled images. Meanwhile, \cite{zhang2024cross} uses multi-step self-distillation to update a feature adaptation module at each learning step. In \cite{ma2023prod}, domain-agnostic and domain-specific prompts are designed to adapt to the target task.

\begin{table}
\tiny
\centering
\vspace{-0.2cm}
\caption{Representative hybrid CDFSL approaches.}
\vspace{-0.2cm}
\setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{llccccccc}
\hline
\textbf{Methods} & \textbf{Venue} & \textbf{$\mathcal{D}$-Extension} & \textbf{$\mathcal{H}$-Constraint} & \textbf{$\Delta$-Adaptation} & \textbf{Loss function} & \textbf{FG} & \textbf{Art} & \textbf{IW}      \\ 
 \hline
DDN~\cite{dynamic}  & NIPS 2021 & Data Augmentation & Knowledge Distillation & \XSolidBrush & CE \& entropy &  &  & \Checkmark    \\
% \hline
DIPA~\cite{perera2024discriminative}   &  CVPR 2024  & Feature Generation & Parameter Adjustment & \XSolidBrush & CE &  & \Checkmark &      \\
% \hline
CLDFD~\cite{cdfsl231} & ICLR 2023 &  Data Augmentation & Knowledge Distillation & \XSolidBrush & CE \& SimCLR \& KD &  &  & \Checkmark    \\
% \hline
SWP-LS~\cite{ji2024soft} & TMM 2024 & Data Augmentation & Regularization & \XSolidBrush & Contrastive \& $L_{2}$ & \Checkmark &  & \Checkmark \\  
% \hline
TL-SS~\cite{hybrid_3}  &  AAAI 2022  & Data Augmentation & Parameter Adjustment & \XSolidBrush & CE \& Metric & \Checkmark &  & \Checkmark     \\  
% \hline
Meta-FDMixup~\cite{hybrid_1} & ACM MM 2021 & Data Augmentation & \XSolidBrush & Shared Information Mining & CE \& KL & \Checkmark &   &     \\  
% \hline
GMeta-FDMixup~\cite{hybrid_7} & TIP 2022 & Data Augmentation & \XSolidBrush & Shared Information Mining & CE \& SSL & \Checkmark &  & \Checkmark    \\  
% \hline
Tri-M~\cite{hybrid_2}    &  ICCV 2021  & Feature Generation & \XSolidBrush & Shared Information Mining & CE & \Checkmark & \Checkmark &      \\
% \hline
TCT-GCN~\cite{feature_reweight_11}    &  Neurocomputing 2023  & Feature Generation & \XSolidBrush & Feature Adaptation & CE & \Checkmark &  & \Checkmark     \\
% \hline
TAML~\cite{feature_reweight_10} & arXiv 2023 & Task Synthesis & \XSolidBrush & Feature Adaptation & CE & \Checkmark &  & \Checkmark     \\
% \hline
FWT~\cite{feature-wise} & ICLR 2020 & \XSolidBrush & Parameter Adjustment & Feature Adaptation & CE & \Checkmark &  &    \\
% \hline
FGNN~\cite{parameter_fix_7} & KBS 2022 & \XSolidBrush & Parameter Adjustment & Feature Adaptation & Softmax & \Checkmark &  &    \\
% \hline
FAD~\cite{zhang2024cross}    &  Neural Comput Appl 2024  & \XSolidBrush & Knowledge Distillation & Feature Adaptation & CE \& KL &  & \Checkmark &      \\
% \hline
ProD~\cite{ma2023prod}    &  CVPR 2023  & \XSolidBrush & Parameter Adjustment & Shared Information Mining & CE \& Cos & \Checkmark &  &      \\
\hline
\end{tabular}}
\vspace{-0.3cm}
\label{table_hybrid}
\end{table}

\vspace{-0.2cm}
\subsubsection{Discussion and Summary}
The combination of multiple strategies in CDFSL, as discussed in Section~\ref{hybrid}, can lead to improved performance. For instance, due to its simplicity and ease of implementation, the $\mathcal{D}$-Extension strategy is often combined with other approaches. However, challenges arise when combining strategies. For example, combining $\mathcal{D}$-Extension and $\mathcal{H}$-Constraint methods can result in training conflicts, leading to negative transfer~\cite{zhao2024all}. Although $\mathcal{D}$-Extension increases data diversity, this diversity may not align with the target domain's characteristics, particularly when there is a significant feature distribution difference. In such cases, the features learned may be ineffective for the target domain. Conversely, $\mathcal{H}$-Constraint methods stabilize model performance by optimizing parameters, but during self-distillation, the model may become overly reliant on existing feature distributions. If these features don't match the target domain, negative transfer may occur~\cite{zhao2024all}.


Moreover, $\mathcal{D}$-Extension may generate additional noise, which negatively impacts methods combined with $\Delta$-Adaptation. This introduces irrelevant information during the process of reducing $\Delta$, potentially misleading the alignment or hindering the extraction of shared information, leading to performance degradation~\cite{liu2022deep}. To address this, some methods~\cite{hybrid_1,feature_reweight_0} control the augmentation process to reduce the introduction of irrelevant information.

The combination of $\mathcal{H}$-Constraint and $\Delta$-Adaptation has high training complexity and computational cost. Specifically, these two strategies require the model to simultaneously optimize the hypothesis space while aligning features between the source and target domains. $\mathcal{H}$-Constraint often involves intricate regularization techniques or model architecture modifications, while $\Delta$-Adaptation requires measuring and aligning distributional differences (\eg, through MMD or adversarial training). This combination increases the complexity of the optimization process, making training more demanding and significantly raising computational costs~\cite{liu2022deep,wang2020rethink}.
Currently, the high training complexity and computational cost prevent any method from addressing the TSERM unreliability problem by combining all three strategies. However, with the rise of large-scale pre-trained models (LMs) \cite{dosovitskiy2020image,radford2021learning,woo2023convnext} and prompt learning \cite{liu2023pre}, we expect future methods to explore this combination. The pre-training of LMs captures broad feature representations, reducing the need for extensive fine-tuning. At the same time, prompt learning provides a lightweight approach that leverages the knowledge in large models, minimizing the need for complex optimization and training costs.