\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{float}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{fancyhdr}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% \title{Digital Twinning to the Metaverse for Augmented Reality Assisted Driving: An Asynchronous Deep Reinforcement Learning Approach}
\pagestyle{fancy}
\fancyhead[C]{This paper appears in IEEE International Conference on Communications, 2023.}
\title{Mobile Edge Adversarial Detection for Digital Twinning to the Metaverse with Deep Reinforcement Learning}

\author{\IEEEauthorblockN{Terence Jie Chua}
\IEEEauthorblockA{Graduate College\\Nanyang Technological University\\
terencej001@e.ntu.edu.sg }
\and

\IEEEauthorblockN{Wenhan Yu}
\IEEEauthorblockA{Graduate College\\Nanyang Technological University\\
wenhan002@e.ntu.edu.sg }
\and

\IEEEauthorblockN{Jun Zhao}
\IEEEauthorblockA{School of Computer Science \& Engineering\\ Nanyang Technological University\\
junzhao@ntu.edu.sg }
}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
Real-time Digital Twinning of physical world scenes onto the Metaverse is necessary for a myriad of applications such as augmented-reality (AR) assisted driving. In AR assisted driving, physical environment scenes are first captured by Internet of Vehicles (IoVs) and are uploaded to the Metaverse. A central Metaverse Map Service Provider (MMSP) will aggregate information from all IoVs to develop a central Metaverse Map. Information from the Metaverse Map can then be downloaded into individual IoVs on demand and be delivered as AR scenes to the driver. However, the growing interest in developing AR assisted driving applications which relies on digital twinning invites adversaries. These adversaries may place physical adversarial patches on physical world objects such as cars, signboards, or on roads, seeking to contort the virtual world digital twin. Hence, there is a need to detect these physical world adversarial patches. Nevertheless, as real-time, accurate detection of adversarial patches is compute-intensive, these physical world scenes have to be offloaded to the Metaverse Map Base Stations (MMBS) for computation. Hence in our work, we considered an environment with moving Internet of Vehicles (IoV), uploading real-time physical world scenes to the MMBSs. We formulated a realistic joint variable  optimization problem where the MMSPs' objective is to maximize adversarial patch detection mean average precision (mAP), while minimizing the computed AR scene up-link transmission latency and IoVs' up-link transmission idle count, through optimizing the IoV-MMBS allocation and IoV up-link scene resolution selection. We proposed a Heterogeneous Action Proximal Policy Optimization (HAPPO)  (discrete-continuous) algorithm to tackle the proposed problem. Extensive experiments shows HAPPO outperforms baseline models when compared against key metrics.


% In AR assisted driving, physical environment scenes are first captured by Internet of Vehicles (IoVs) and are uploaded to the Metaverse. A central Metaverse Map Service Provider (MMSP) will aggregate information from all IoVs to develop a central Metaverse Map (MM). Information from the MM can then be downloaded into individual IoVs on demand and be delivered as AR scenes to the driver. Nevertheless, receiving continual, smooth updates generated by the MMSP is a challenging task due to the large data size of the AR graphics and the need for low latency transmission. Furthermore, the energy expense by IoVs have to be managed. Hence in our work, we considered an environment with users in moving Internet of Vehicles (IoV), uploading real-time physical world scenes and downloading AR world updates from MMSP via wireless communications. We formulated a realistic asymmetric optimization problem where the MMSPs' objective is to reduce the computed AR scene down-link transmission latency (discrete-case allocation). On the other hand, the drivers' objectives are to minimize output battery consumption through optimizing the up-link power allocation (continuous-case power selection). The data uplink and downlink transmissions are then executed asynchronously. We proposed a novel multi-agent, loss-sharing (MALS) reinforcement learning model to tackle the above-mentioned asynchronous and asymmetric problem. We then compare our proposed multi-agent model with other base-line models and show its superiority over other methods. 
\end{abstract}

\begin{IEEEkeywords}
Metaverse; resource allocation, reinforcement learning; multi-agent; augmented reality; digital twin; Internet of Vehicles; adversarial. 
\end{IEEEkeywords}

\section{Introduction}
\textbf{Background. }Digital twinning is the keystone of the Metaverse~\cite{lee2021all}, in which real-world objects and events are mapped to and replicated in the virtual world. This opens doors to a myriad of possible applications which require real-time information of the physical environment, such as Augmented Reality (AR)-assisted driving. To facilitate AR-assisted driving capabilities, real-world scenes have to be uploaded to a central Metaverse Map Service provider (MMSP) which functions as a virtual reality host for geographical information. The physical world scenes have to be collated, aggregated to form a coherent database. Internet of Vehicles (IoVs) can then query information from the MMSP and this information can be displayed as AR scenes on the IoVs' windshield which provide drivers with comprehensive, real-time information such as directions and landmark information to assist their driving.

\textbf{Motivation. }The development of the new-age AR-assisted driving technology invites adversaries. Adversaries may physically paste adversarial patches on cars, signboards, traffic lights or on the roads with the intent to corrupt the physical world scene which is to be uploaded to the MMSP for the development of a centralize virtual map. A successful attack as such can have disastrous effects, in which the Metaverse Map scenes may reflect erroneous information which when queried by IoVs can result in misinformation and accidents.
% Drivers often have to rely on mobile devices or Global Positioning Systems (GPS) to navigate. Similarly, any real-time updates such as traffic accidents, congestion and road closures are reflected on mobile devices. This increases drivers' reliance on these mobile devices and they frequently have to take their eyes off the road to look at the latest information on the mobile devices. This results in dangerous driving and frequent accidents. Furthermore, existing maps on phones are only able to display a limited amount of information, and are not as intuitive as they require driver's experience in map-reading and interpretation. 

% In the age of mobile edge computing, Internet of Vehicles (IoVs) features such as AR-assisted driving are becoming reality. In AR-assisted driving systems, drivers will be shown real-time augmented graphics which may indicate direction, landmark signs which require much less interpretation and are much more intuitive. However, obtaining these MM AR real-time information updates and interactions under mobility over wireless communications can be challenging, as this information is downloaded in a graphical format that can  be of large data size. With the continuous stream of data downloaded from the MMSP and continuous movement of real-world users, there is concern about the downlink latency and uplink IoV energy consumption.

\textbf{Compute intensive detection. }These adversarial patches are often inconspicuous~\cite{bai2021inconspicuous}, and a fairly high resolution image of the patch is required for patch detection. This makes real-time detection of adversarial patches compute intensive, and these adversarial patch detection task have to be offloaded to the Metaverse Map Sevice Provider Base Stations (MMBSs) for computation. However, the offloading of high-resolution physical world scenes may induce large uplink transmission latency, yet offloading low-resolution images substantially impairs the MMSPs' adversarial patch detection ability. Furthermore, too many IoVs allocated to an MMBS may result in sub-optimal performance and unreliability in the system. Hence, IoVs may be excluded from certain UL transmissions if the occasional exclusion of an IoV results in better system performance. The total number of exclusions (idle count) should be minimized to ensure that the MMSP obtains comprehensive and regular information update from the IoVs.


\textbf{Our Approach. }Hence, we proposed a Heterogenous Action Proximal Policy Optimization (HAPPO) algorithm to be employed within the MMSP orchestrator to tackle the optimization problem of (i) maximizing patch detection mean average precision (mAP) while minimizing the (ii) uplink latency and (iii) IoV idle count. The orchestrator consists of two agents, one to handle (1) discrete IoV-MMBS allocation and the other to handle (2) continuous physical scene resolution selection. Our HAPPO architecture follows the Centralized Training and Decentralized Execution (CTDE) framework~\cite{lowe2017multi}.

\subsection{Related work}
% Earlier works in adversarial detection have been focused on detecting the presence of adversarial perturbations in images~\cite{pang2018towards,zheng2018robust}. Many of these works studied the inherent statistical properties of adversarial and natural examples, and leveraged on these properties to make a prediction~\cite{cohen2020detecting,zhang2018detecting}.

% Authors of~\cite{brown2017adversarial} were pioneers in this sub-field and showcased an attempt at crafting adversarial patches to successfully fool classification models. Recent adversarial patch detection works~\cite{lee2019physical,xu2020lance} have adopted heuristic approaches to their detection algorithms.

\textbf{Adversarial Patches. }
The detection of adversarial perturbations within images has been thoroughly studied~\cite{ma2018characterizing, zheng2018robust}. Many works~\cite{feinman2017detecting,gong2017adversarial,lee2018simple} in the field of adversarial detection have built classifiers to sift out  corrupted samples from natural (unperturbed or clean) samples. However, as physical attackâ€™s practicality in real-world gains recognition, there is an increasing number of researches focused on developing better defenses against adversarial patch attacks. Recent works~\cite{arvinte2020detecting,xu2020lance}
 in adversarial defenses have been focused on adversarial detection. These works utilize heuristic-based approaches such as using wavelets~\cite{arvinte2020detecting} and Grad-CAM maps~\cite{xu2020lance} to differentiate between natural and adversarial samples.

\textbf{Metaverse applications. }Since the Metaverse is still relatively new, limited studies consider the IoT-Metaverse base station communication and computation framework. Chua~\textit{et~al.}~\cite{Chua2022} introduced a AR socialization over 6G wireless networks within the Metaverse scenario and proposed a deep RL approach to tackle it. Han~\textit{et~al.}~\cite{han2022dynamic} addressed resource allocation for the MEC of digital twinning of Internet of Things (IoT). Similarly, Ng~\textit{et~al.}~\cite{ng2022unified} tackled a resource allocation problem for the MEC of the Metaverse education sector using stochastic optimization.


% Han~\textit{et~al.}~\cite{han2022dynamic} proposed a resource allocation framework for the Internet of Things (IoT) to facilitate the synchronization of the Metaverse with the physical world. Ng \textit{et al.}~\cite{ng2022unified} proposed a framework that uses stochastic optimization based resource allocation to obtain the minimum cost of a Metaverse   service provider in the education sector context. Xu~\textit{et~al.}~\cite{xu2022wireless} introduced an incentive mechanism based on machine learning for VR in the Metaverse, using auction theory to obtain the optimal pricing and allocation, and deep reinforcement learning to accelerate the auction process. While many works have considered variants of Metaverse-related communication problems, these works did not consider the application of deep reinforcement learning in the context of adversarial defence for the digital twinning to the Metaverse.

\textbf{Resource Allocation. }Resource allocation for wireless networks have been thoroughly studied~\cite{liu2019resource2,ahsan2021resource,hieu2022joint}. Works such as those by~\cite{hieu2021optimal} utilized deep reinforcement learning approaches to allocate power for communications.

\textbf{Edge Computing. }Resource allocation and optimization of variables have been a long-standing concern in the field of mobile edge computing (MEC), and there have been several works~\cite{hu2018joint,liu2019resource} which presented edge computing problems and developed solutions to tackle their proposed problem. Some works have adopted deep reinforcement learning approaches to tackle optimization problems for mobile edge computing~\cite{huang2019deep,huang2018deep,huang2018distributed}.

\noindent\textbf{Contributions.} Our contributions are as follows:
\begin{itemize}
\item \emph{\textbf{Adversarial Detection in Defence of the digital twinning: }}
We present a novel mobile edge computing (MEC)-enabled adversarial patch detection for the defence of digital twinning to the Metaverse scenario, specifically in the context of AR-assisted driving.

\item \emph{\textbf{HAPPO approach to Asymmetric Joint Optimization Problem: }}We propose a Heterogeneous Action PPO, dual-agent (discrete-continuous) deep reinforcement learning-based IoV to MMBS orchestrator which aims to maximize adversarial patch detection mAP while minimizing total physical scene up-link transmission delay and IoV idle count.

\item \emph{\textbf{Superiority of HAPPO: }} We conducted experiments to compare the performance of our proposed HAPPO against other base-line algorithms and results demonstrate the effectiveness and superiority of our proposed method.

\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{pictures/systemmodel_new.pdf}    \caption{System Model involving 2 agents to facilitate the adversarial detection offloading.}
    \label{fig:systemmodel}
    \vspace{-0.5cm}
\end{figure}

\section{Adversarial Patch}
\textbf{Adversarial Patch Attack. }\label{sub:patch_attack}
For simplicity, we inserted an adversarial patch, digitally, onto images of cars and roads, 2000 from each of the \textit{Stanford-Cars}~\cite{krause2013collecting} and \textit{nuImages}~\cite{nuscenes2019} datasets, to mimic the placement of physical adversarial patches onto the physical environment. We assigned square-shaped patches of size smaller than 2\% of the total image area randomly on our training dataset. We adopted the Projected Gradient Descent (PGD) patch attack~\cite{madry2017towards} as an example attack to be applied on our training set. The mechanism of PGD can be described as such: The algorithm aims to find a perturbation value to be added to the natural example, which maximizes the loss function value, under the constraints in which the norm of the perturbation value falls within a pre-defined threshold (shown in equation \ref{pgd_function}).
\begin{align}
    \max_{\lVert\zeta\rVert_\infty \leq \varphi} l(\mathcal{F}(\chi_0 + \zeta; \varpi), \Upsilon_0)
    \label{pgd_function}
\end{align}
\noindent where $\chi_0$ represents the natural example, $\Upsilon_0$ represents the original label, $\zeta$ is the perturbation value to be added, $\varpi$ is the model weights, $l$ is the loss function, and $\varphi$ represents the perturbation threshold value. $\mathcal{F}$ is the predictive function that maps the input to a prediction. Implementing it by iterative gradient descent, we have:
\vspace*{-0.5cm}

\begin{align}
    \chi_{t+1} = \chi_{t} + \kappa \cdot \text{sign}(\nabla_\chi l(f(\chi_t;\varpi),\Upsilon_0)) \label{iterative}
\end{align}

\noindent where $\chi_t$ refers to the current image state, and $\kappa$ is a scalar multiplier.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{pictures/patches.pdf}
    \caption{Detection of Adversarial Patches from \textit{nuImages} (left and middle)~\cite{nuscenes2019} and \textit{Stanford-Cars} (right)~\cite{krause2013collecting} datasets.}
    \label{fig:patches}
    \vspace{-0.5cm}
\end{figure*}

\textbf{Adversarial Patch Detection. }
\label{sub:patch_detect}
After the physical scenes with adversarial patches are offloaded from the IoVs to the MMBSs, the trained adversarial patch detectors on the MMBS will detect for adversarial patches on the uploaded physical world scenes. We adopted a cutting-edge object detector, pre-trained YOLOv4 with a CSPDarknet53 backbone~\cite{bochkovskiy2020yolov4} to detect the adversarial patches (shown in Fig.~\ref{fig:patches}). In order to reduce latency, lower resolution images may be transmitted to the MMBS, which consequently result in poorer patch detection mean average precision score (mAP). Vice versa, transmission of higher resolution images results in higher latency but better patch detection mean average precision score (mAP). mAP is a common performance metric used in object detection tasks, which takes the mean of average precision (AP) scores across different intersection over union (IoU) bounding box thresholds. In our work, we adopt the IoU threshold values from 0.5 to 0.95 in incremental steps of 0.05.

\section{System model}
\label{models}
In our system, $N$ AR vehicles from a set of $\mathcal{N}=\{1,2,...,N\}$ AR vehicles are capturing and uploading physical world scenes in real-time on the go, to Metaverse Map Service Provider Base Station (MMBSs) $\mathcal{M} = \{1,2,...,M\}$.  Each AR vehicle $i \in \mathcal{N}$ moves around a defined geographical space at random and uploads physical environment scenes to an MMBS (shown in Fig.~\ref{fig:systemmodel}). As high-resolution, large data-size physical world scenes are required for patch detection, there may be a hand-over of the physical scenes uploaded, from one MMBS to another, AR vehicles move within a defined space. Several MMBSs and AR vehicles are distributed geographically. These AR vehicles transmit the physical world scenes to the MMBS and may generate interferences that disrupts the effective signal between other AR vehicles and their assigned MMBS. In our paper, we consider intra-cell interference. Intra-cell interference in this context, refers to the signal interference caused by the transmissions of other AR vehicles on the same bandwidth, and are assigned to the same MMBS, as the AR vehicle of interest.

\textbf{Uplink Communication model. }\label{sub:Communication-Model}
Each AR vehicle from a set of $\mathcal{N}=\{1,2,...,N\}$ will be assigned an MMBS's downlink channel from a set of $\mathcal{M} = \{1,2,...,M\}$ MMBS.
The physical world scenes to be uploaded from the AR vehicles to the MMBS are of size $d^t = \{d^t_{1}, d^t_{2}, ..., d^t_{N}\}$. $d^t_i$ denotes the size of data to be uploaded by AR vehicle $i \in \mathcal{N}$ at transmission iteration $t$. We denote the AR vehicle-MMBS assignment to be  $\textbf c^{t}=(c_1^t, ..., c_N^t)$, where $c_i^t=v (i \in \mathcal{N}, v \in \mathcal{M})$ denotes that AR vehicle $i$ is allocated to MMBS $v$ at iteration $t$. Considering the intra-MMBS interference, the \textit{signal to interference plus noise ratio} of AR vehicle $i$ at iteration $t$ is defined as:
\begin{align}
\Gamma_i^t(\boldsymbol{c^{t},h^{t}}) &=\frac{g^t_{i,c_i^t}h^{t}_{i}}{   \sum_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t} (g^t_{n,c_i^t} h^{t}_{n}) + B\sigma^{2}}, \nonumber
    %\label{eq:R1}
\end{align}
where $h^t_{i}$ is the power of AR vehicle $i$ used for the transmission of physical world scenes to MMBS $c_i^t$ at iteration step $t$, $g^t_{c_i^t,i}$ is the channel gain between MMBS $c_i^t$ and AR vehicle $i$ at iteration step $t$. $h^t_{n}$ is the power of AR vehicle $n$ for communicating with MMBS $c_i^t$ at iteration $t$, $B$ is the bandwidth of the communicatiion, and $\sigma^2$ denotes the additive white Gaussian background noise.

Principally, $g^t_{i,c_i^t}h^{t}_{i}$ is the received signal at MMBS $c_i^t$ from AR vehicle $i$ in iteration $t$, $\sum_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t} (g^t_{n,c_i^t} h^{t}_{n})$ is the intra-cell interference caused by other AR vehicle $n \neq i$ assigned to the same MMBS $c_i^t$, to AR vehicle $i$ at iteration $t$. In each iteration step $t$, the uplink data transfer rate $r^{t}_i$ from the AR vehicle $i$ to its assigned MMBS is influenced by the SINR as such:
\begin{align}
&r_{i}^{t}(\boldsymbol{c}^{t},\boldsymbol{h}^{t})=B\cdot \log_2 \left(1+\Gamma_i^t(\boldsymbol{c}^{t},\boldsymbol{h}^{t})\right),\label{eq:R2}
\end{align}
From Equation (\ref{eq:R2}), it is evident that the assignment of many AR vehicles to a single MMBS causes large intra-cell interference. A larger intra-cell interference would result in lower effective signals between AR vehicles and MMBS, and this causes the overall data transmission rate to decline. For a fixed data size to be transmitted, a higher data transfer rate results in a shorter uplink transmission delay at iteration step $t$ as shown: $\ell^{t}_i = \frac{d^{t}_i }{r^{t}_i}$, where $d^{t}_i$ is the size of the physical world scene to be uploaded from AR vehicle $i$ to a MMBS at iteration step $t$. We consider the transmitted physical world scenes to be square-frames, where data size $d^{t}_i$ and resolution $p^{t}_i$ captured by AR vehicles $i$ at iteration $t$ are related by: $d^{t}_i = \xi\cdot(p^{t}_i)^2$. $\xi$ represents the number of bits of information embedded within each pixel. Intuitively, as AR vehicle $i$ uplink latency at iteration step $t$ increases, the lower the consistency of update to the virtual world. Furthermore, a more efficient AR vehicle to MMBS allocation would increase each AR vehicles' SINR and consequently result in lower latency. Finally, the transmission of physical environment scenes of lower resolution reduces uplink transmission latency.
% Consider the uplink transmission from a set of $\mathcal{N}=\{1,2,...,N\}$ IoVs, capturing and uploading physical world scenes in real-time on the go, to a set of $\mathcal{M} = \{1,2,...,M\}$ Metaverse Map Service Provider Base Station (MMBS). Each IoV $i \in \mathcal{N}$ uploads physical environment scenes to an MMBS. As the physical world scenes required for patch detection are of large data sizes, the scenes are uploaded as the IoVs move and there may be a handover of the upload task from one MMBS to another. Since there are several MMBSs located geographically across our environment, we consider intra-cell interference, which may influence the data rates and consequently size of data transmitted. The IoVs move randomly around the geographical space we defined.

 % and their distance with respect to other IoVs and MMBS also influences the data transfer rates. In each transmission iteration, IoVs need not be allocated to a MMBS for computation offloading, if such a decision result system unreliability. Evidently, the choice of IoV-MMBS allocation influences data transfer rates and, consequently the time taken to complete the physical environment data transmission. We next introduce the IoV-MMBS communication model to illustrate the scene described above. Our system model is illustrated in Fig.~\ref{fig:model}.

 % Furthermore, we denote $r^{t} = \{r_{1}^{t},r_{2}^{t},...,r_{N}^{t}\}$ as the different physical world up-link data transfer rate at transmission iteration $t$. Specifically, $r_{i}^{t}$, where $i \in \mathcal{N}$ is the up-link data transfer rate from the MMBS $v$ to IoV $i$ at iteration ${t}$.

% \textbf{Up-link Communication model. }\label{sub:Communication-Model}
% Our communication scenario is based on wireless cellular network. Each MMBS from a set of $\mathcal{M} = \{0,1,2,...,M\}$ will have its uplink channel assigned to the set of IoVs $\mathcal{N}=\{1,2,...,N\}$. $d^t = \{d^t_{1}, d^t_{2}, ..., d^t_{N}\}$ is the size of the physical world scenes to be uploaded from the IoVs to the MMBSs, where $d^t_i$ denotes the size of data to be uploaded by IoV $i \in \mathcal{N}$ at transmission iteration $t$. In addition, we have channel allocation $\textbf c^{t}=(c_1^t, ..., c_N^t)$, where $c_i^t=v (i \in \mathcal{N}, v \in \mathcal{M})$ denotes IoV $i$ is allocated to MMBS $v$ at iteration $t$. Considering the intra-MMBS interference, we can derive the \textit{signal to interference plus noise ratio} (SINR) of IoV $i$ at iteration-step $t$ as:
% \begin{align}
%     \Gamma_i^t=\frac{g^{t}_{v,i}\cdot p^{t}_{i,v} }{\sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}g^{t}_{v,n}\cdot p^{t}_{n,v} +w\sigma^{2}}.\label{eq:R1}
% \end{align}
% where $p^t_{i,v}$ is the transmit power of IoV $i$ for communication with MMBS $v$ at iteration step $t$, $g^t_{v,i}$ is the channel gain between MMBS $v$ and IoV $i$ at iteration step $t$. $p^t_{n,v}$ is the power allocated by IoV $n$ to MMBS $v$ at iteration step $t$ and $w$ is the bandwidth of each IoV and $\sigma^2$ denotes the additive white gaussian background noise. Note that from here, we may discuss variables and terms without index $t$ for convenience and simplicity of explanation.

% Essentially, $g_{v,i}\cdot p_{i,v}$ is the transmission signal from MMBS $v$ to IoV $i$, $\left [\sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}g_{v,n} \cdot p_{n,v}\right]$ is the intra-cell interference caused by the interaction between MMBS $v$ and other IoV $n \neq i$ to IoV $i$.

% For each iteration step $t$, the up-link data transfer rate $r^{t}_i$ from the MMBS to IoV $i$ is influenced by the SINR as such:
% \begin{align}
% &r_{i}^{t}=w\cdot \log_2 \left(1+\Gamma_i^t\right),\label{eq:R2}
% \end{align}
% From Equation (\ref{eq:R1}) and (\ref{eq:R2}), it is evident that the connection of too many IoVs to a single MMBS causes larger intra-MMBS interference, which reduces the overall channel gain and hence data transfer rate as well. For a fixed data size to be transmitted, a higher data transfer rate results in a shorter up-link transmission delay at iteration step $t$ as shown: $\ell^{t}_i = \frac{d^{t}_i }{r^{t}_i}$, where $d^{t}_i$ is the size of the physical world scene to be uploaded from IoV $i$ to a MMBS at iteration step $t$. Intuitively, as IoV $i$ up-link latency at iteration step $t$ increases, the lower the consistency of update to the virtual world. Furthermore, a more efficient IoV to MMBS allocation would increase each IoVs' SINR and consequently result in lower latency. Finally, the transmission of physical environment scenes of lower resolution reduces uplink transmission latency.

\textbf{Detection mAP-resolution model. }
As the actual implementation of continuous real-time detection of adversarial patches is infeasible for the scale of our work, we established the relationship between physical environment image resolution and adversarial patch detection mean average precision (mAP) score empirically . We collected multiple resolution-mAP pairs from the YOLOv4 prediction output and fitted a polynomial best-fit curve to the data points (as shown in Fig.~\ref{fig:maprelationship}). We note that the image resolution $p$ and mAp are related by a polynomial relationship of $\text{mAP} = 4.5\times 10^{-6}\cdot p^3 - 4.7\times10^{-3}\cdot p^2+1.6\cdot p-90$, for $p \in[64,416]$ pixel per inch (ppi).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pictures/mAP_resol.pdf}
    \caption{mAp vs Resolution (pixel per inch, ppi).}
    \label{fig:maprelationship}
    \vspace{-0.5cm}
\end{figure}


\textbf{Idle Count. }
To ensure that we have consistent physical scene transmission for patch detection from IoVs to the MMSP, we aim to reduce the total IoVs' idle count $\sum_{t=1}^{T}\sum_{i \in \mathcal{N}}I^t_i$, which refers to minimizing the total counts in which IoVs are not uploading physical world scenes to the MMBS.

\subsection{Problem formulation} \label{problemform}
To sum up, the goal of the MMSP is to find the optimal IoV-MMBS allocation arrangement $c^{t}$ and transmitted physical environment image resolutions $p^{t}$  which minimizes the total up-link latency $\ell^{t}$ and IoV idle count $I^{t}$ while maximizing the IoV patch detection mAP $\text{mAP}(p^{t})$.  We formulated our up-link utility function as:
\begin{align}
\setlength{\belowdisplayskip}{4pt plus 1pt minus 1.0pt}
\setlength{\belowdisplayshortskip}{4pt plus 1pt minus 1.0pt}
\setlength{\abovedisplayskip}{4pt plus 1pt minus 1.0pt} \setlength{\abovedisplayshortskip}{0.0pt plus 2.0pt}
\min_{\boldsymbol{c^{t},p^{t}}} &\sum_{t=1}^{T} \sum_{i\in\mathcal{N}} q \cdot \ell^{t}_i - b\cdot \text{mAP}(p^{t}_i) + f\cdot I^{t}_{i}, \label{eq:M1}\\
s.t.~~& c_{i}^{t}\in \mathcal{M}, ~\forall i \in \mathcal{N}, \forall t \in \mathcal{T},\\
& h_{i}^t \leq h_{\max},  ~\forall i \in \mathcal{N}, \forall t \in \mathcal{T},\\
& p_{min} \leq p_i^t \leq p_{max},  ~\forall i \in \mathcal{N}, \forall t \in T
\end{align}
where $T$ is the total number of uplink transmissions of physical environment scenes from the IoVs to the MMSPs. The constraint (5) restricts each IoV to be allocated to at most one MMBS in each iteration step. Constraint (6) ensures that AR vehicle power output lies below $h_{\max}$. Constraint (7) ensures the image resolution lies between $p_{min}$ and $p_{max}$ ppi. $b,f,q$ are scaling factors which seeks to balance the order and unit difference between $\ell^t_i$ , $I^t_i$ and $\text{mAP}(p^{t}_i)$ for joint-variable optimization.

\section{Reinforcement learning settings}
\label{RL}
For our work, we assign two reinforcement learning agents, $Agent1$ and $Agent2$, with $Agent1$ performing the discrete action of IoV-MMBS allocation, and $Agent2$ performing the continuous action IoV uplink image resolution selection. Both agents are incorporated within the MMSP and represent the MMSP's interests. The rationale for adopting two agents is that we are optimizing two variables in which one has continuous and the other has discrete action spaces.

\textbf{State. }For both agents' observation state $s^t$, we have chosen to include 1) \textbf{channel gain between each IoV and all MMSPs}: $g^t_{v,i}$, 2) \textbf{image data size to be transmitted by each IoV} at each transmission iteration $t$: $d^{t}_{i}$, as these two variables influences data up-link transmission rate and latency.

\textbf{Action. }For $Agent1$, the agent's action is to decide the MMBS to IoV allocation, in which the action space for each IoV $i$ can be written as such: $a^{alloc,t}~=~\boldsymbol{c}^{t}~=~(c^{t}_{1},...,c^{t}_{N}),\\~(t\in T)$. The number of the discrete actions is $N^{M+1}$, where $N$ denotes the number of IoVs and $M$ is the total number of MMBSs. This signifies that an IoV may or may not be allocated to a MMBS.

For $Agent2$, the action space is continuous and the action dimension is $N$, in which there is one image resolution value selected for each IoV to transmit the physical environment scenes to its assigned MMBS. The uplink action space for all the IoVs at each transmission iteration is written as such: $a^{resol,t}=\boldsymbol{p}^{t}=\{p^t_{1}, ... ,p^t_{N}\},~(t\in T).$, where $p^t_{i=N}$ is the uplink image resolution selected at iteration $t$ for IoV $N$.

% Fortunately, as large discrete action space even continuous action space cases have been studied comprehensive in the literature, we can adopt a state-of-art algorithm, Proximal Policy Optimization (PPO) for our work. We discussed the algorithm in \ref{algorithms}.

\textbf{Reward. }
Although we have a single objective function, in practice, we break down the overarching objectives into smaller rewards to be assigned to each of our agents. We assign only components of the objective function which is influenced by an agent's decision to that agent.


For the $Agent1$ the reward is given at transmission iteration $t$ as such:
\begin{align}
    \mathcal{R}^{alloc,t} = - \frac{ \sum_{i \in \mathcal{N}}\left(q\cdot\ell^{t}_i + f\cdot I^{t}_i\right)}{N}
    \label{eq:reward_down}
\end{align}
while for $Agent2$, the reward given to the agent at transmission iteration $t$ is given as such:
\begin{align}
    \mathcal{R}^{resol,t} = -\frac{ \sum_{i \in \mathcal{N}}q \cdot \ell^{t}_i - b\cdot \text{mAP}(p^{t}_i)}{N}
    \label{eq:reward_up}
\end{align}
We divide the reward functions by $N$ IoVs to find an average reward, as the average reward received per IoV is more intuitive than the reward sum.

\subsection{Heterogeneous Actions PPO}
 \label{algorithms}
 Inspired by the well-known Centralized Training Decentralized Execution (CTDE) framework~\cite{lowe2017multi}, we developed the Heterogeneous Actions Proximal Policy Optimization (HAPPO) algorithm for our dual-agent RL model. This model features both discrete and continuous action spaces and utilizes PPO as the backbone, as PPO is considered a state-of-the-art algorithm with performance stability. We do not directly use traditional CTDE algorithms like Multi-Agent PPO (MAPPO) because the actions in our scenario contain both discrete and continuous actions, and it is not feasible to directly concatenate them to form a unified action. This is because the discrete action space PPO and the continuous action space PPO use different networks and distributions for sampling actions. 
 
Similar to PPO~\cite{PPO}, HAPPO uses separate policies $\pi_\theta$ and $\pi_{\theta^{'}}$ for sampling trajectories (during training) and evaluation, respectively. Here, $\pi_{\theta_1}$ and $\pi_{\theta_2}$ are two separate distributions instead of a shared distribution in policy optimization. KL divergence constraints are applied to both Actors' policies to prevent major policy changes in each update. As the Actor-network is based on policy gradient~\cite{sutton1999policy}, according to PPO~\cite{PPO}, we formulate the update function of Actors as:
\begin{align}
    \mathbb{E}_{(s^t,a^{alloc,t})\sim\pi_{\theta_1^{'}}}[f^{alloc,t}(\theta_1)(A^{alloc,t}+A^{resol,t})] \\
    \mathbb{E}_{(s^t,a^{resol,t})\sim\pi_{\theta_2^{'}}}[f^{resol,t}(\theta_2)(A^{alloc,t}+A^{resol,t})]
\end{align}
where
\begin{align}
    &f^{alloc,t}(\theta_1)=\text{min}\{\mathcal{R}^{alloc,t}(\theta_1), \text{clip}(\mathcal{R}^{alloc,t}(\theta_1), 1-\epsilon, 1+\epsilon)\}
\end{align}
\begin{align}
    &\text{and}~~~\mathcal{R}^{alloc,t}(\theta_1)=\frac{\pi_{\theta_1}(a^{alloc,t}|s^t)}{\pi_{{\theta'_1}}(a^{alloc,t}|s^t)}. 
\end{align}
$f^{resol,t}(\theta_2)$ and $\mathcal{R}^{resol,t}(\theta_2)$ is also defined in the same manner as equation (13), (14), respectively, with $resol$ replacing $alloc$ in the superscripts. $\epsilon$ refers to the policy clipping parameter.
 
Here, $A^{alloc,t}$ and $A^{resol,t}$ are the advantages of actions selected by $Agent1$ and $Agent2$, respectively. The advantages are computed by the truncated version of TD($\lambda$)~\cite{schulman2015high}. 
\begin{align}
    &A^{alloc,t} = \delta^{alloc,t} + ...+(\gamma\lambda)^{\bar{T}-1}\delta^{alloc,t+\bar{T}-1}, \\
    &\text{where}~~~\delta^{alloc,t}=\mathcal{R}^{alloc,t}+\gamma V_{\phi'}(s^{t+1})-V_{\phi'}(s^t).
\end{align}
$A^{resol,t}$ and $\delta^{resol,t}$ is defined in the same manner as equation (15), (16), respectively, with $resol$ replacing $alloc$ in the superscripts. $\bar{T}$ is the trajectory segment, $\lambda$ is the trace decay parameter and $\gamma$ is the discount rate.

In terms of the value network (Critic), HAPPO uses identical Critics as per other Actor-Critic algorithms; and the loss function can be formulated as:
% \begin{align}
%     L(\phi) = [V_\phi(s^t)-((\mathcal{R}^{alloc,t}+R^{resol,t})+\gamma V_{\phi'}(s^{t+1}))]^2 \label{eq:criticloss}
% \end{align}
\begin{align}
    L(\phi) = [V_\phi(s^t)-((A^{alloc,t}+A^{resol,t})+\gamma V_{\phi'}(s^{t+1}))]^2 \label{eq:criticloss}
\end{align}

where $V(s)$ is the widely used state-value function~\cite{RLintro}, which is estimated by a learned critic network with parameter $\phi$. We update $\phi$ by minimizing the $L(\phi)$, and the parameter $\phi'$ of target state-value function periodically with $\phi$.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{pictures/algorithm.pdf}
    \caption{Heterogeneous Action PPO (HAPPO) structure.}
    \label{fig:algorithm}
    \vspace{-0.5cm}
\end{figure}

\begin{figure}[!t] 
        \renewcommand{\algorithmicrequire}{\textbf{Initiate:}}
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \begin{algorithm}[H]
            \caption{\label{alg:PPO}Heterogenous Action PPO}
            \begin{algorithmic}[1]
                \REQUIRE critic parameter $\phi$ and target network $\phi^{'}$, $Agent1$ actor parameter $\theta_{1}$, $Agent2$ actor parameter $\theta_{2}$, initialize state $s^{t}=s^{1}$
                \FOR{iteration = $1,2,...$}
                    \STATE $Agent1$ and $Agent2$ execute action according to $\pi_{\theta^{'}_{1}}(a^{alloc,t}|s^{t})$ and $\pi_{\theta^{'}_{2}}(a^{resol,t}|s^{t})$, respectively
                    \STATE Get $\mathcal{R}^{alloc,t}$ and $\mathcal{R}^{resol,t}$ and next state $s^{t+1}$
                        
                    \STATE sample trajectories: \\$\tau$=$\{s^{t},a^{alloc,t},a^{resol,t},s^{t+1},\mathcal{R}^{alloc,t},\mathcal{R}^{resol,t}\}$ iteratively
                    \STATE Compute advantages $\{A^{alloc,t},A^{resol,t}\}$
                    \STATE Compute target values \{$V^{alloc,t}_{targ},V^{resol,t}_{targ}$\}
                    \FOR{$k$ = $1,2,...,K$}
                        \STATE Shuffle the data's order, set batch size $bs$
                        \FOR{$j$=$0,1,...,\frac{T}{bs}-1$}
                            \STATE Compute gradient for downlink and uplink actors:
                            $\triangledown \theta_{1}, \triangledown \theta_{2}$
                            \STATE Apply gradient ascent on $\theta_{1}$ using $\triangledown \theta_{1}$
                            \STATE Apply gradient ascent on $\theta_{2}$ using $\triangledown \theta_{2}$ 
                            \STATE Update critic with loss using eq.~(\ref{eq:criticloss})
                        \ENDFOR
                        \STATE Assign target network parameters $\phi^{'} \leftarrow \phi$ after $C$ iterations
                    \ENDFOR
                    
                \ENDFOR
            \end{algorithmic}
        \end{algorithm}
\vspace{-0.8cm}
\end{figure}
\section{Experiment}
\label{experiment}
In this section, we will describe our experimental configurations and provide in-depth analyses on the results.
\subsection{Configuration} \label{Configuration}
We use five congestion settings: 3 MMBS and with 3 to 7 IoVs (denoted as $"3x"$ for 3 MMBS and $x$ number of IoVs) to test our proposed HAPPO orchestrator. We compared (i) our proposed HAPPO against baseline models (ii) Independent Dual Agent PPO-PPO, (iii) Heterogeneous A2C (HAA2C) which utilizes similar structure to HAPPO, and (iv) a random IoV-MMBS allocation and image resolution selection agent. The bandwidth and noise are simulated to be $B=10$ MHz and $\sigma^2=-100$ dBm. We initialize and constrain IoV power output, image resolution, and IoV locations for different IoVs to $(1.5,2.0)$ Watt, $(64,416)$ ppi, $x,y\in(0,1000)$ m, respectively. $x$ and $y$ represents the relative longitudinal and latitudinal directions in our 1000m by 1000m map, and IoVs randomly move a maximum of 100m in $x$ and $y$ directions in each transmission iteration. We set $b$, $q$ and $f$ to be 50, 60 and 75, respectively, and these numbers are empirically derived. We adopt the ADAM optimizer\cite{adam} for all our implemented algorithms. To better observe the final performance, we use 280,000 steps for training. We conducted the training and simultaneous evaluation of the models for each of the configurations at different seed settings: seed 0 to seed 9.

% After multiple tests and extensive parameter adjustments, we list the critical hyper-parameter settings for different algorithms under different congestion degrees in Table \ref{table:parameter}.

% \begin{table}[t]
% \centering
% \caption{important hyper parameters}
% \label{table:parameter}
% \vspace{0.2mm}
% %\resizebox{\textwidth}{90mm}
% \scalebox{0.7}{
% \begin{tabular}{ccccccc}
% \hline
%  Congestion & Learning rate & Hid-layer & Batch size & Entropy-coef & GAE & Discount factor\\ \hline
% $3$ IoVs, $3$ MMBSs   & $5\times10^{-4}$ & $128$ & $64$ & $1\times10^{-4}$ & $0.95$ & $0.99$\\
% $4$ IoVs, $3$ MMBSs   & $5\times10^{-4}$ & $128$ & $64$ & $1\times10^{-4}$ & $0.95$ & $0.99$\\
% $5$ IoVs, $3$ MMBSs   & $1\times10^{-4}$ & $256$ & $64$ & $1\times10^{-4}$ & $0.95$ & $0.99$\\
% $6$ IoVs, $3$ MMBSs   & $5\times10^{-5}$ & $512$ & $64$ & $1\times10^{-4}$ & $0.93$ & $0.95$\\
% $7$ IoVs, $3$ MMBSs   & $5\times10^{-5}$ & $512$ & $64$ & $1\times10^{-4}$ & $0.93$ & $0.95$\\ \hline
% \end{tabular}
% }
% \vspace{-0.3cm}
% \end{table}


\begin{figure}[t]
\centering
\subfigtopskip=2pt
\subfigbottomskip=2pt

\subfigure[Agent1 reward in 34 scenario.]{
\begin{minipage}[t]{0.8\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/34rd.pdf}
\label{fig:34_agent1}
\vspace{-10mm}
\end{minipage}
}%

\subfigure[Agent2 reward in 34 scenario.]{
\begin{minipage}[t]{0.8\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/34ru.pdf}
\label{fig:34_agent2}
\vspace{-10mm}
\end{minipage}
}%

\subfigure[Agent1 reward in 37 scenario.]{
\begin{minipage}[t]{0.8\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/37rd.pdf}
\label{fig:37_agent1}
\vspace{-10mm}
\end{minipage}%
}%

\subfigure[Agent2 reward in 37 scenario.]{
\begin{minipage}[t]{0.8\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/37ru.pdf}
\label{fig:37_agent2}
\vspace{-10mm}
\end{minipage}%
}%

\caption{Reward during training in scenario 34 and 37.}
\vspace{-0.5cm}
\end{figure}




\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pictures/metric.pdf}
    \caption{Metrics with different User numbers.}
    \label{fig:metrics}
    \vspace{-0.5cm}
\end{figure}

% \iffalse
\begin{table}[t]
\centering
\caption{Overall rewards}
\label{table:parameter}
\vspace{-0.2mm}
%\resizebox{\textwidth}{90mm}
\scalebox{1}{
\begin{tabular}{cccc}
\hline
Number of IoV & \makecell{HAPPO} & \makecell{HAA2C} & \makecell{Independent agents\\ (PPO-PPO)} \\ \hline
\multicolumn{4}{c}{Agent1 Reward} \\ \hline
$3$ & $-47.53$ & $-$\textbf{43.82} & $-48.94$\\
$4$ & $-42.27$ & $-41.10$ & $-$\textbf{40.36}\\
$5$ & $-$\textbf{132.23} & $-160.45$ & $-154.83$ \\
$6$ & $-$\textbf{148.34} & $-173.42$ & $-165.78$ \\
$7$ & $-$\textbf{154.47} & $-197.47$ & $-184.23$ \\ \hline
\multicolumn{4}{c}{Agent2 Reward} \\ \hline
$3$ & \textbf{363.68} & $344.59$ & $360.36$ \\
$4$ & \textbf{287.43} & $286.72$ & $238.58$\\
$5$ & \textbf{234.72} & $152.53$ & $163.49$\\
$6$ & \textbf{169.34} & $134.29$ & $145.6$ \\
$7$ & \textbf{88.74} & $38.10$ & $39.34$\\ \hline
\end{tabular}
}
\label{table:results}
\vspace{-0.5cm}
\end{table}
% \fi

\subsection{Result analyses}
We present the final obtained rewards for both $Agent1$ and $Agent2$ in Table~\ref{table:results}. In the simpler "33" and "34" settings, most of the RL algorithm pairs we adopted performed fairly well and achieved convergence, with the exception of independent PPO-PPO in the "34" setting (reflected in poorer rewards obtained shown in Fig.~\ref{fig:34_agent2}). Nevertheless, we observed a notably quicker training convergence by the proposed HAPPO algorithm (shown in Fig.~\ref{fig:34_agent1}). In the more complex scenarios such as "35", "36", and "37", we found that our adopted HAPPO achieved significantly better rewards than the other baseline RL algorithms (shown in Table~\ref{table:results} and Fig.~\ref{fig:37_agent1} and Fig.~\ref{fig:37_agent2}).


The total up-link transmission (of a batch of 1000 scenes) increases substantially as the number of IoVs increase, while the mAP score achieved decreased as the number of IoVs increased (shown in Fig.~\ref{fig:metrics}). This is not unexpected as the more complex scenarios involve more IoVs sharing computing resources with an unchanging number of MBBS. HAPPO showcased its superiority over the other algorithm by obtaining the lowest average total transmission delay and considerably good mAP score, across the different congestion settings. Furthermore, HAPPO exhibits a much narrower range of total uplink transmission time and mAP score (as shown by the error bars in Fig.~\ref{fig:metrics}) when compared to other algorithm, indicating greater stability.

Despite the disparities in performance between the different algorithms, all algorithm performed better than an agent which allocates IoV-MBBS and selects uploaded image resolution randomly (shown in Fig.~\ref{fig:34_agent1},~\ref{fig:34_agent2},~\ref{fig:37_agent1},~\ref{fig:37_agent2}). This further substantiates that our proposed orchestrator improves the uplink communication in terms of maximizing accuracy, minimizing transmission delay and IoV idle counts.

% , and choose to highlight to configuration settings "34" and "37" training phase rewards. In the "33" and "34" scenario, most of the RL algorithm pairs we adopted obtained a relatively decent reward. However, we can observe that the A2C-PPO and PPO-A2C combinations struggle to find an as good solution as the PPO-PPO and A2C-A2C RL algorithm pair in obtaining $Agent1$'s rewards, while A2C-PPO does not achieve as good solutions. A prominent feature is that PPO-PPO algorithm pair seem to achieve model convergence quicker than the other algorithms, highlighting its superiority.

% In more complex scenarios such as "35", "36", "37", we observed that the PPO-PPO algorithm pair achieved the highest final $Agent1$ and $Agent2$ rewards (shown in Table~\ref{}), albeit struggling to achieve model convergence in these settings (shown in Fig~\ref{fig:metrics}). 

% In all scenarios, all the proposed algorithms performed better and achieved higher Agent1 and Agent2 rewards than a random IoV-MMBS and random image resolution selection algorithm, showcasing the utility of our proposed orchestrator. The superiority of PPO-PPO algorithm pair is also demonstrated in the performance of the underlying metrics. We observe that PPO-PPO achieved the lowest total up-link transmission delay in each experimental configuration and with a small variance in performance across all tested seeds. Similarly, PPO-PPO also obtained considerably one of the best mAp scores across the different experimental configurations and displayed stability in performance across seed settings (as observed in the smaller variance across seeds).



\section{Conclusion}
In our work, we have proposed a real-time adversarial patch detector, enabled by mobile edge computing, in the defence of digital twinning to the metaverse. We formulated a realistic joint variable optimization problem where the MMSPs' objective is to maximize adversarial patch detection mAP, while minimizing the uplink transmission latency and IoV idle counts, through optimizing the MMBS allocation and IoV uplink image resolution. We proposed a Heterogenous Action PPO (HAPPO) (discrete-continuous) to tackle our proposed problem. We have demonstrated that our proposed HAPPO model outperforms baseline models and achieved superior performance based on key metrics.

\section*{Acknowledgement}

%This research is partly supported by the Singapore Ministry of Education Academic Research Fund under Grant Tier 1 RG97/20, Grant Tier 1 RG24/20 and Grant Tier 2 MOE2019-T2-1-176; and partly by the NTU-Wallenberg AI, Autonomous Systems and Software Program (WASP) Joint Project.
This research is partly supported by the Singapore Ministry of Education Academic Research Fund under Grant Tier 1 RG90/22, RG97/20, Grant Tier 1 RG24/20 and Grant Tier 2 MOE2019-T2-1-176; and partly by the NTU-Wallenberg AI, Autonomous Systems and Software Program (WASP) Joint Project.

{\small
\bibliographystyle{IEEEtran}
%\bibliography{ref}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{lee2021all}
L.-H. Lee, T.~Braud, P.~Zhou, L.~Wang, D.~Xu, Z.~Lin, A.~Kumar, C.~Bermejo, and
  P.~Hui, ``All one needs to know about metaverse: A complete survey on
  technological singularity, virtual ecosystem, and research agenda,''
  \emph{arXiv preprint arXiv:2110.05352}, 2021.

\bibitem{bai2021inconspicuous}
T.~Bai, J.~Luo, and J.~Zhao, ``Inconspicuous adversarial patches for fooling
  image recognition systems on mobile devices,'' \emph{IEEE Internet of Things
  Journal}, 2021.

\bibitem{lowe2017multi}
R.~Lowe, Y.~I. Wu, A.~Tamar, J.~Harb, O.~Pieter~Abbeel, and I.~Mordatch,
  ``Multi-agent actor-critic for mixed cooperative-competitive environments,''
  \emph{Advances in neural information processing systems}, 2017.

\bibitem{ma2018characterizing}
X.~Ma, B.~Li, Y.~Wang, S.~M. Erfani, S.~Wijewickrema, G.~Schoenebeck, D.~Song,
  M.~E. Houle, and J.~Bailey, ``Characterizing adversarial subspaces using
  local intrinsic dimensionality,'' \emph{arXiv preprint arXiv:1801.02613},
  2018.

\bibitem{zheng2018robust}
Z.~Zheng and P.~Hong, ``Robust detection of adversarial attacks by modeling the
  intrinsic properties of deep neural networks,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~31, 2018.

\bibitem{feinman2017detecting}
R.~Feinman, R.~R. Curtin, S.~Shintre, and A.~B. Gardner, ``Detecting
  adversarial samples from artifacts,'' \emph{arXiv preprint arXiv:1703.00410},
  2017.

\bibitem{gong2017adversarial}
Z.~Gong, W.~Wang, and W.-S. Ku, ``Adversarial and clean data are not twins,''
  \emph{arXiv preprint arXiv:1704.04960}, 2017.

\bibitem{lee2018simple}
K.~Lee, K.~Lee, H.~Lee, and J.~Shin, ``A simple unified framework for detecting
  out-of-distribution samples and adversarial attacks,'' \emph{Advances in
  {N}eural {I}nformation {P}rocessing {S}ystems}, vol.~31, 2018.

\bibitem{arvinte2020detecting}
M.~Arvinte, A.~Tewfik, and S.~Vishwanath, ``Detecting patch adversarial attacks
  with image residuals,'' \emph{arXiv preprint arXiv:2002.12504}, 2020.

\bibitem{xu2020lance}
Z.~Xu, F.~Yu, and X.~Chen, ``Lance: A comprehensive and lightweight cnn defense
  methodology against physical adversarial attacks on embedded multimedia
  applications,'' in \emph{2020 25th Asia and South Pacific Design Automation
  Conference (ASP-DAC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  470--475.

\bibitem{Chua2022}
T.~J. Chua, W.~Yu, and J.~Zhao, ``Resource allocation for mobile metaverse with
  the {Internet of Vehicles} over 6g wireless communications: {A} deep
  reinforcement learning approach,'' in \emph{8th IEEE World Forum on the
  Internet of Things (WFIoT)}, 2022.

\bibitem{han2022dynamic}
Y.~Han, D.~Niyato, C.~Leung, C.~Miao, and D.~I. Kim, ``A dynamic resource
  allocation framework for synchronizing metaverse with {IoT} service and
  data,'' in \emph{ICC 2022-IEEE International Conference on
  Communications}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  1196--1201.

\bibitem{ng2022unified}
W.~C. Ng, W.~Y.~B. Lim, J.~S. Ng, Z.~Xiong, D.~Niyato, and C.~Miao, ``Unified
  resource allocation framework for the edge intelligence-enabled metaverse,''
  in \emph{ICC 2022-IEEE International Conference on Communications}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 5214--5219.

\bibitem{liu2019resource2}
X.~Liu, Z.~Qin, Y.~Gao, and J.~A. McCann, ``Resource allocation in wireless
  powered iot networks,'' \emph{IEEE Internet of Things Journal}, vol.~6,
  no.~3, pp. 4935--4945, 2019.

\bibitem{ahsan2021resource}
W.~Ahsan, W.~Yi, Z.~Qin, Y.~Liu, and A.~Nallanathan, ``Resource allocation in
  uplink {NOMA-IoT} networks: A reinforcement-learning approach,'' \emph{IEEE
  Transactions on Wireless Communications}, 2021.

\bibitem{hieu2022joint}
N.~Q. Hieu, D.~T. Hoang, D.~Niyato, D.~N. Nguyen, D.~I. Kim, and A.~Jamalipour,
  ``Joint power allocation and rate control for rate splitting multiple access
  networks with covert communications,'' \emph{arXiv preprint
  arXiv:2203.16807}, 2022.

\bibitem{hieu2021optimal}
N.~Q. Hieu, D.~T. Hoang, D.~Niyato, and D.~I. Kim, ``Optimal power allocation
  for rate splitting communications with deep reinforcement learning,''
  \emph{IEEE Wireless Communications Letters}, 2021.

\bibitem{hu2018joint}
Q.~Hu, Y.~Cai, G.~Yu, Z.~Qin, M.~Zhao, and G.~Y. Li, ``Joint offloading and
  trajectory design for uav-enabled mobile edge computing systems,'' \emph{IEEE
  Internet of Things Journal}, vol.~6, no.~2, pp. 1879--1892, 2018.

\bibitem{liu2019resource}
X.~Liu, Z.~Qin, and Y.~Gao, ``Resource allocation for edge computing in iot
  networks via reinforcement learning,'' in \emph{ICC 2019-2019 IEEE
  international conference on communications (ICC)}, 2019.

\bibitem{huang2019deep}
L.~Huang, S.~Bi, and Y.-J.~A. Zhang, ``Deep reinforcement learning for online
  computation offloading in wireless powered mobile-edge computing networks,''
  \emph{IEEE Transactions on Mobile Computing}, vol.~19, no.~11, pp.
  2581--2593, 2019.

\bibitem{huang2018deep}
L.~Huang, X.~Feng, L.~Qian, and Y.~Wu, ``Deep reinforcement learning-based task
  offloading and resource allocation for mobile edge computing,'' in
  \emph{Machine Learning and Intelligent Communications: Third International
  Conference, MLICOM 2018, Hangzhou, China, July 6-8, 2018, Proceedings
  3}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2018, pp. 33--42.

\bibitem{huang2018distributed}
L.~Huang, X.~Feng, A.~Feng, Y.~Huang, and L.~P. Qian, ``Distributed deep
  learning-based offloading for mobile edge computing networks,'' \emph{Mobile
  networks and applications}, pp. 1--8, 2018.

\bibitem{krause2013collecting}
J.~Krause, J.~Deng, M.~Stark, and L.~Fei-Fei, ``Collecting a large-scale
  dataset of fine-grained cars,'' 2013.

\bibitem{nuscenes2019}
H.~Caesar, V.~Bankiti, A.~H. Lang, S.~Vora, V.~E. Liong, Q.~Xu, A.~Krishnan,
  Y.~Pan, G.~Baldan, and O.~Beijbom, ``nuscenes: A multimodal dataset for
  autonomous driving,'' \emph{arXiv preprint arXiv:1903.11027}, 2019.

\bibitem{madry2017towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu, ``Towards deep
  learning models resistant to adversarial attacks,'' \emph{arXiv preprint
  arXiv:1706.06083}, 2017.

\bibitem{bochkovskiy2020yolov4}
A.~Bochkovskiy, C.-Y. Wang, and H.-Y.~M. Liao, ``Yolov4: Optimal speed and
  accuracy of object detection,'' \emph{arXiv preprint arXiv:2004.10934}, 2020.

\bibitem{PPO}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:1707.06347},
  2017.

\bibitem{sutton1999policy}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour, ``Policy gradient
  methods for reinforcement learning with function approximation,''
  \emph{Advances in neural information processing systems}, vol.~12, 1999.

\bibitem{schulman2015high}
J.~Schulman, P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel, ``High-dimensional
  continuous control using generalized advantage estimation,'' \emph{arXiv
  preprint arXiv:1506.02438}, 2015.

\bibitem{RLintro}
R.~S. Sutton and A.~G. Barto, \emph{Reinforcement learning: An
  introduction}.\hskip 1em plus 0.5em minus 0.4em\relax MIT press, 2018.

\bibitem{adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}

}
 


\end{document}
