\PassOptionsToPackage{dvipsnames}{xcolor}

\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm, setspace}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
%\usepackage{stfloats}
%\usepackage{fixltx2e}
%\usepackage{dblfloatfix}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

%% *** GRAPHICS RELATED PACKAGES ***
%\ifCLASSINFOpdf
%  \usepackage[pdftex]{graphicx}
%  \graphicspath{{../pdf/}{../jpeg/}}
%  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
%  \usepackage[dvips]{graphicx}
%  \graphicspath{{../eps/}}
%  \DeclareGraphicsExtensions{.eps}
%\fi
\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{comment}
\renewcommand{\Re}{{\mathbb R}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bI}{\mathbf{I}}

\newcommand{\grad}{\nabla}

\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\DeclareMathOperator*{\argmin}{arg\,min}

\renewcommand{\comment}[1]{}

\newcommand{\pa}{\text{par}}
\newcommand{\se}{\text{ser}}

% Comment out exactly one of these:
%\newcommand{\extraproof}[1]{\textcolor{red}{#1}} 
\newcommand{\extraproof}[1]{} 

% \input{bm3d_header}

\begin{document}

%% LANL release authorization: LA-UR-23-22951


\title{Projected Multi-Agent Consensus Equilibrium (PMACE) for Distributed Reconstruction with Application to Ptychography}

\author{Qiuchen Zhai,~\IEEEmembership{Student Member,~IEEE,} Gregery T. Buzzard,~\IEEEmembership{Senior Member,~IEEE,} Kevin Mertes, Brendt Wohlberg,~\IEEEmembership{Fellow,~IEEE,} Charles A. Bouman,~\IEEEmembership{Fellow,~IEEE}
        % <-this % stops a space
\thanks{
The work of Q. Zhai was supported by U.S. Department of Energy through LANL.  
The work of C. A. Bouman was supported in part by the U.S. Department of Energy and the Showalter Trust. 
The work of G.T. Buzzard was partially supported by NSF CCF-1763896. 
The work of B. Wohlberg was supported by the LDRD program of LANL under project number 20200061DR. 
% The work of K. Mertes was supported by LANL ***

Qiuchen Zhai and Charles A. Bouman are with the School of Electrical and Computer Engineering, Purdue University, 465 Northwestern Ave., West Lafayette, IN 47907, USA (qzhai@purdue.edu and bouman@purdue.edu). 

Gregery T. Buzzard is with the Department of Mathematics, Purdue University, West Lafayette, IN 47907, USA  (buzzard@purdue.edu).

%Ymir M\"akinen and Alessandro Foi are with the Unit of Computing Sciences, Tampere University, Tampere 33014, Finland (ymir.makinen@tuni.fi and alessandro.foi@tuni.fi).

Kevin Mertes is with Physical Chemistry and Applied Spectroscopy Group, Los Alamos National Laboratory, Los Alamos, NM 87545 USA (kmmertes@lanl.gov).

Brendt Wohlberg is with Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM 87545 USA (brendt@ieee.org).}}


% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Multi-Agent Consensus Equilibrium (MACE) formulates an inverse imaging problem as a balance among multiple update agents such as data-fitting terms and denoisers. 
However, each such agent operates on a separate copy of the full image, leading to redundant memory use and slow convergence when each agent affects only a small subset of the full image.  

In this paper, we extend MACE to Projected Multi-Agent Consensus Equilibrium (PMACE), in which each agent updates only a projected component of the full image, thus greatly reducing memory use for some applications.  
We describe PMACE in terms of an equilibrium problem and an equivalent fixed point problem and show that in most cases the PMACE equilibrium is not the solution of an optimization problem. 

To demonstrate the value of PMACE, we apply it to the problem of ptychography, in which a sample is reconstructed from the diffraction patterns resulting from coherent X-ray illumination at multiple overlapping spots. 
In our PMACE formulation, each spot corresponds to a separate data-fitting agent, with the final solution found as an equilibrium among all the agents. 
Our results demonstrate that the PMACE reconstruction algorithm generates more accurate reconstructions at a lower computational cost than existing ptychography algorithms when the spots are sparsely sampled.

%Finally, we introduce a complex-valued version of the BM3D/BM4D denoiser, which is used as a regularizer to obtain high-accuracy reconstructions even in the case of limited overlap between adjacent ptychographic measurements.  

\end{abstract}


\begin{IEEEkeywords}
Ptychography, consensus equilibrium, inverse problem, phase retrieval, iterative reconstruction. %, BM3D.
\end{IEEEkeywords}


% This paper (general method with application to ptychography):
% Core description of PMACE
% Use of regularization/denoising
% Complex BM3D

% Second paper (real application paper):
% Probe estimation
% Probe position estimation
% Multiple probes


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction
\section{Introduction}
\IEEEPARstart{M}{ethods} for inverse imaging typically seek to balance a fit to noisy data with some form of regularization or prior information.  
While the fit to data is often well-understood from physical models and can be captured using a log-likelihood or other penalty function, the prior information is much more difficult to capture in such a functional form.  
On the other hand, algorithmic denoisers such as BM3D~\cite{dabov2007} and convolutional neural network (CNN) denoisers~\cite{zhang2017beyond} encapsulate a great deal of prior information about images but only in an algorithmic way that is not easily amenable to description as a penalty function.  

The recently developed Multi-Agent Consensus Equilibrium (MACE)~\cite{buzzard2018plug} and Plug-and-Play (PnP) \cite{venkatakrishnan_plug-and-play_2013, sreehari2016plug, bouman2022foundations} methods provide a formulation and algorithms for incorporating both algorithmic prior information and information from penalty functions into a single reconstruction framework.  
Because the algorithmic prior information is encoded in input-output format as in an image denoiser, there is no cost function associated with this prior information, hence no cost function and no minimization associated with the reconstruction.  
Instead, MACE formulates the problem as an equilibrium among multiple input-output maps called agents.  
%As described in more detail below, MACE requires that all the agents agree on a common output and that the vector sum of their updates from input to common output must sum to 0.  

The theory underlying MACE implies that PnP methods are equivalent to Bayesian methods when the agents are proximal maps of convex functions (described below), but that PnP methods are more general in that there is no corresponding natural optimization problem when the PnP agents are not proximal maps, as in the case of neural networks. Moreover, MACE generalizes PnP to incorporate more than 2 agents, which allows the problem to be divided into smaller subproblems and solved separately with distributed implementation.  However, the original MACE framework requires that each agent maintain a separate copy of the entire reconstruction, which is very inefficient when each agent operates on only a small portion of the reconstruction.  

% Bayesian methods approach this problem by probabilistic means, quantifying both the fit to data and the regularization in terms of likelihood functions, and then performing a joint optimization~\cite{bouman2022foundations}.  While data-fitting terms are often derived from physical models and hence are naturally expressed in such functional forms, prior information is not so easily captured this way. In particular, data-driven prior information as encoded in trained neural networks~\cite{zhang2017beyond} is not easily captured in a traditional Bayesian framework, nor are more traditional algorithmic regularizers such as BM3D~\cite{dabov2007}.  

% Plug-and-Play (PnP) methods~\cite{venkatakrishnan_plug-and-play_2013, sreehari2016plug} take proximal algorithms such as alternating direction method of multipliers (ADMM) or proximal-gradient, which are often used for Bayesian optimization, and reinterpret an update of the regularization likelihood as the application of a denoiser. This simple idea opens the door to the application of a wide range of algorithmic denoisers and other types of regularization for many types of inverse problems. However, PnP methods no longer have an interpretation as an optimization; the corresponding solution is simply the fixed point of an algorithm.  

% Multi-Agent Consensus Equilibrium  (MACE)~\cite{buzzard2018plug} bridges the gap between algorithm and formulation by describing the solution of a PnP algorithm in terms of an equilibrium among multiple agents, each updating a separate candidate solution of the full problem.  The theory underlying MACE implies that PnP methods are equivalent to Bayesian methods when the agents are proximal maps of convex functions (described below), but that PnP methods are more general in that there is no corresponding natural optimization problem when the PnP agents are not proximal maps, as in the case of neural networks. Moreover, MACE generalizes PnP to incorporate more than 2 agents, which allows the problem to be divided into smaller subproblems and solved separately with distributed implementation. The ability to solve problems with distributed computation allows the MACE formulation to fit into large-scale and data-intensive problems in a wide range of applications such as ptychography. 

Ptychography is an important computational imaging technique~\cite{rodenburg2008ptychography, rodenburg2019ptychography} that is widely used in applications such as the imaging of manufactured nanomaterials~\cite{wilke2012hard, trtik2013density, guizar2014high, shapiro2017ptychographic}.
The imaging technique works by moving a coherent X-ray probe across an object plane to scan a sample in overlapping patches.
%, as shown in Figure~\ref{fig:ptycho_illustration}. 
A detector then measures the magnitude of the far-field diffraction pattern at each probe location. The magnitude and phase of the transmittance image of the sample can be recovered by exploiting the redundancy caused by the overlaps in the intensity measurements.  Notably, the reconstructed phase is significant in estimating the structure
of the sample.
%The ability to achieve nanometer resolution and its applicability over spectral ranges from X-rays \cite{rodenburg2007hard, pfeiffer2018x, shi2019x} to terahertz (THz) waves \cite{valzania2018terahertz, wan2020terahertz} make ptychography an essential imaging technique.

Since ptychography is a phase recovery problem, it can be solved using conventional phase recovery methods~\cite{fienup1982phase}.
However, a series of methods have been developed specifically for reconstruction of ptychographic data. 
The original method, known as ptychographic iterative engine (PIE)~\cite{rodenburg2004phase}, has been generalized to methods such as extended PIE (ePIE)~\cite{maiden2009improved}, regularized PIE~(rPIE)~\cite{thibault_reconstructing_2013}, and momentum-accelerated PIE (mPIE)~\cite{maiden2017further}.
All these methods tend to have rapid convergence and work by serially enforcing local intensity measurements.
However, the PIE-type algorithms cannot be easily parallelized,
%tend to be unstable in the presence of noise \cite{chang2019blind}, 
and require a relatively large overlap between scan positions to achieve a high-quality reconstruction~\cite{bunk2008influence}. 

In addition, a variety of gradient-based methods including Wirtinger Flow (WF)~\cite{candes2015phase}, accelerated WF~\cite{xu2018accelerated}, Maximum Likelihood method~\cite{odstrvcil2018iterative}, as well as other approaches based on 
% Newton's method \cite{yang2011iterative} and 
proximal algorithms~\cite{soulez2016proximity, yan2020ptychographic} have also been proposed for ptychographic reconstruction.
%{\bf Maximum likelihood methods Paul Shereer institute from Switzerland. Also a gradient-based method.}
While methods based on gradient calculations can be parallelized across probe locations, these methods tend to have slower convergence than serial methods and are limited by the requirement that the problem must be formulated as an optimization.

% \textcolor{red}{While methods based on gradient calculations can be parallelized across probe locations, these methods are limited by the requirement that the problem must be formulated as an optimization.} 
% %Also, gradient descent methods tend to have slow convergence relative to serial PIE-based methods.}
% %\textcolor{red}{While these optimization-based algorithms can be more easily parallelized, they do not provide a method for distributed inverse problems with more general data-fitting updates.}
% {\bf Is this ok?}

More recently, a method known as scalable heterogeneous adaptive real-time ptychography (SHARP) has been proposed~\cite{marchesini2016alternating} and  implemented~\cite{marchesini2016sharp} for ptychographic reconstruction that is intrinsically parallel in nature.
The SHARP method is based on a general method for phase-retrieval known as relaxed averaged alternating reflections (RAAR)~\cite{luke2004relaxed}.
However, SHARP and RAAR are designed specifically for phase retrieval and not more general inverse problems.

In this paper, we introduce Projected Multi-agent Consensus Equilibrium (PMACE) an intrinsically parallel algorithm for solving distributed inverse problems.
%and we demonstrate how it can be applied to the problem of ptychographic reconstruction.
PMACE extends the MACE framework to allow the state of a problem to be updated in smaller components that can be processed separately by localized agents. 
For some applications such as ptychography, PMACE can dramatically reduce memory requirements relative to MACE because each agent operates in parallel while storing only a small portion of the entire image.
%This requires the definition of an appropriate averaging operator to reconcile updates from multiple overlapping patches and the definition of the final reconstruction in terms of the data updates and the averaging operator. 
We show that PMACE is equivalent to an optimization formulation for some choices of update agents, but that in most cases there is no naturally corresponding optimization problem, with the solution corresponding instead to a weighted equilibrium of the updates of the individual agents. 

To illustrate the utility of the PMACE framework, we apply it to the problem of ptychographic reconstruction.   
We use the data from each scan position to implement a computationally efficient, non-optimization-based data-fitting agent that updates a local patch to better fit the measured intensity data for a single diffraction spot. 
These local patches are combined into a full image with a weighted average that uses the probe intensity to capture the uncertainty in measurement at a given location. 
The resulting formulation allows large ptychographic reconstruction problems to be solved efficiently with parallel computations.

%We also introduce a regularization framework for PMACE. As with MACE, a regularizer may be used as an agent in parallel with data-fitting agents \cite{majee2021multislice}. However, for faster convergence, and to leverage the fact that denoisers act naturally on full images, we apply the work of \cite{sridhar2020distributed} to introduce a serial denoising agent. Since the denoiser in ptychography acts on a complex reconstruction, we introduce a complex extension of the classic BM3D/BM4D algorithm for this purpose.

Our experimental results demonstrate that PMACE outperforms existing state-of-the-art ptychographic reconstruction algorithms in both convergence speed and reconstruction quality as the ptychographic spots are spaced further apart. In practice, reconstruction from sparse samples is of practical importance because it can be used to reduce the amount of data required for accurate ptychographic reconstruction.


% call \IEEEpubidadjcol in the second column for its text to clear the IEEEpubid mark.
\IEEEpubidadjcol


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% MACE and PMACE
\section{MACE and PMACE}

\subsection{MACE}

Multi-Agent Consensus Equilibrium (MACE)~\cite{buzzard2018plug} is a problem formulation that reconciles multiple agents, such as data fitting updates or denoisers, each acting to improve a candidate reconstruction.  
In this formulation, each agent, $F_j$, maintains an individual copy, $x_j$, of the full image (or volume), and $F_j(x_j)$ is an improved reconstruction according to the $j$th agent.
These copies are stacked to form a full MACE state, given by
$$
\bx = 
\left[
x_0,
\ldots,
x_{J-1} 
\right] \ ,
$$
and the agents are stacked to form a single operator defined by
\begin{equation}
\label{eq: stacked_forward_operators_x}
    \bF (\bx) = 
    \left[
    F_{0} (x_{0}), 
    \ldots,
    F_{J-1} (x_{J-1})
    \right] \ . 
\end{equation}

Note that $\bx$ denotes the full MACE state, while $x_j$ denotes the $j$th component image. 
Since each component is a full copy of the image to be reconstructed, the MACE state contains multiple, potentially inconsistent reconstructions.
In order to produce a single MACE reconstruction, we define the operator
\begin{equation}
\label{eq:consensus_operator_x}
    \bG ( \bx ) =
    \left[
    \bar{x}(\bx ),
    \ldots,
    \bar{x}(\bx )
    \right] \ , 
\end{equation}
where $\bar{x}(\bx ) = (1/J) \sum_{j=0}^{J-1} x_j.$
So $\bG ( \bx )$ computes the average of the components in $\bx$, and then returns a state vector formed by replicating this average $J$ times.
A key algebraic property of the averaging operator is that $\bG ( \bG (\bx ))= \bG (\bx )$. 

The MACE equation is then given by
\begin{equation} \label{eq:MACE-equil}
\bF(\bx^*) = \bG(\bx^*) \ , 
\end{equation}
where $\bx^*$ solves the equation, and the final reconstruction is then given by $x^* = \bar{x}(\bx^*)$.  As shown in~\cite{buzzard2018plug}, \eqref{eq:MACE-equil} has the interpretation of finding a consensus equilibrium among all the agents, in that the output of each agent is the common image $\bar{x}(\bx^*)$, while the updates $F_j(x_j^*) - x_j^*$ sum to 0.  

In the case that each $F_j$ is the proximal map for a convex, differentiable\footnote{We assume differentiability for simplicity.  Similar results hold for general convex functions by using subgradients.} function $f_j$, then $x^*$ satisfies 
\begin{equation} \label{eq:MACE-grad-zero}
   \sum_{j=0}^{J-1} \grad f_j(x^*) = 0 \ ,
\end{equation}
and $x^*$ is a minimizer of the function $f = \sum_j  f_j$~\cite{sridhar2020distributed}.  

However, when the MACE agents are not proximal maps, there may be no cost function that is minimized by solving the MACE equation. 
We discuss this in greater generality in Section~\ref{sec:PMACE}.
%More precisely, in this case, the equilibrium condition of \eqref{eq:MACE-equil} does not naturally correspond to the gradient of a conservative vector field being zero~\cite{?}.

We note that $\bF (\bx )$ is an intrinsically parallel operation that can be efficiently distributed across many compute nodes, while the averaging operator $\bG ( \bx )$ requires communication across all nodes in order to first gather the individual components, then compute their average, and then broadcast the average back to individual nodes~\cite{sridhar2020distributed}.


%That is, the analog of \eqref{eq:MACE-equil} for general agents $F_j$ uses $\Delta_j^* = F_j(x_j^*) - x_j^*$ in place of $\grad f_j(x^*)$; in the case that $F_j$ is the proximal map for $f_j$ and $\bx^*$ is a solution, then $\Delta_j^* = \grad f_j(x^*)$.
%But when $F_j$ is not a proximal map, then this vector difference (as a function of $x$ in place of $x^*$) is generally not a conservative field, hence not a gradient field of any potential function.   

% A generalization of MACE in \cite{reid2022multiresolution} modifies $\bG$ to $\bG^R(\bx)$ by replacing the $\bar{\bx}$ in \eqref{eq:consensus_operator_x} with identical entries 
% \begin{equation}  \label{eq:GR}
%   \left(\sum_j R_j \right)^{-1} \sum_j R_j x_j,  
% \end{equation}
% where the $R_j$ are specified linear transformations.
% In this case, the condition corresponding to \eqref{eq:MACE-equil} is $\sum_j R_j \grad f_j(x^*) = 0$, with an analogous change in the non-proximal case.  
% Once again, this is generally not a gradient field, not even in the proximal map case unless all the $R_j$ are multiples of the identity.

% The work in \cite{reid2022multiresolution} also shows that the same equilibrium condition is obtained by using the original $\bG$ but modifying the agents using the $R_j$.  They show also that in the case of a proximal map data fitting agent and a prior agent, the use of $R$ with the data agent corresponds to a Lipschitz modification of the prior agent.   


\subsection{PMACE}
\label{sec:PMACE}

\begin{figure}[t!]
    \centering
    \includegraphics[width=9cm]{figs/PMACE_flow.png}
    \caption{Conceptual rendering of the PMACE pipeline. A reconstruction $x$ is subdivided into possibly overlapping components, $v_j$, and distributed to multiple agents, $F_j$, each of which acts to improve this local reconstruction. These local reconstructions are combined using a pixel-weighted averaging operator, $G$, that reassembles the local components into a consistent global reconstruction.}
    \label{fig:pmace_flow}
\end{figure}

In some problems, agents operate naturally on subsets or projections of a full image rather than on the image itself.
%A motivating example for this paper is ptychography.  Here we discuss the problem in general and defer details on the application to ptychography to Section~\ref{sec:PMACEPtychography}.  
Figure~\ref{fig:pmace_flow} illustrates such a problem in which separate data is collected from each patch of an image, $x$.
In this case, the agents can operate separately to make each component more consistent with its data.
However, these individual components must also be reconciled into a single consistent reconstruction.

Mathematically, we express this extraction of a patch or component from an image as
$$
v_j = P_j x \ ,
$$
where $P_j$ is a matrix with orthonormal rows, so that $P_j P_j^T = I$.  
We use $v_j$ to emphasize that these components lie in a different space than $x$.  
Typically, $P_j$ simply selects a region of the image $x$, but more general transformations are allowed.  

As in MACE, each component, $v_j$, is processed by a corresponding agent, $F_j$, whose role is to improve the component by better fitting to data, reducing noise, etc.
Also as in MACE, PMACE stacks the component states and the agents to form the vector and operator given by
\begin{equation}
\label{eq: stacked_vj}
\bv = 
\left[
v_0,
\ldots,
v_{J-1} 
\right] \ ,
\end{equation}
\begin{equation}
\label{eq: stacked_forward_operators_v}
    \bF (\bv) = 
    \left[
    F_{0} (v_{0}), 
    \ldots,
    F_{J-1} (v_{J-1})
    \right] \ . 
\end{equation}

Note that PMACE has a potentially huge advantage over MACE because it does not require $J$ replicates of the entire reconstruction.
This can dramatically reduce memory and computation resources when implemented on a computer.
For example, in ptychography as described below, if each reconstruction is a large image and there are $1000$ agents corresponding to $1000$ probe locations, then MACE becomes impractical to implement since it requires that the full image be replicated $1000$ times in the computer's memory. 

Since the PMACE components are not complete versions of the reconstruction, the averaging operator $\bG (\bx )$ must be redefined.
To do this, we project the components back to the full image, then use a pixel-wise weighted average to combine them and reproject them back to components.  This leads to the new averaging operator
\begin{equation}
\label{eq:consensus_operator_v}
    \bG^P ( \bv ) =
    \left[
    P_0 \bar{x}(\bv),
    \ldots,
    P_{J-1} \bar{x}(\bv)
    \right] \ . 
\end{equation}
where 
\begin{equation}
\label{eq:consensus_operator_component_v}
\bar{x}(\bv) = \Lambda^{-1}  \sum_{j=0}^{J-1} P_j^T W v_j \ ,
\end{equation}
${\Lambda} = \sum_{j=0}^{J-1} P_{j}^T W P_j $, and $W$ is a positive definite and (typically) diagonal weighting matrix.
We assume that $\Lambda$ is invertible, so if $P_j$ selects subsets of the original image, then the set of $J$ components must cover the entire image being reconstructed.
The weight matrix, $W$, can be used to model the uncertainty in measurements for each component. 

Using these operators, we look for a solution $\bv^*$ to the PMACE equation, defined as 
\begin{equation} \label{eq:pmace-eqns}
    \bF(\bv^*) = \bG^P(\bv^*) \ ,
\end{equation}
in which case $x^* = \bar{x}(\bv^*)$ is the final reconstruction. In words, \eqref{eq:pmace-eqns} means that each agent generates a patch that is consistent with the entire reconstruction $x^*$.

% In Subsection~\ref{subsec:PMACEOptimization} we extend analogous results on MACE \cite{buzzard2018plug} to show (i) that the PMACE framework describes distributed optimization of a cost function in the case that the agents are proximal maps and $W$ is a scalar and (ii) that when the agents are not proximal maps or $W$ is not a scalar, then the PMACE framework corresponds to an equilibrium problem with no natural associated global cost function being minimized.


\subsection{Solving the PMACE Equations}

To solve the PMACE equation \eqref{eq:pmace-eqns}, we follow~\cite{buzzard2018plug} to show that it can be reformulated as a fixed point problem and apply Mann iterations to compute a solution.

By definition of $\bar{x}$, $G^P$, and $\Lambda$, we have
\begin{align*}
    \bar{x}(\bG^P(\bv)) &= \Lambda^{-1} \sum_j P_j^T W P_j \bar{x}(\bv)\\
    &= \bar{x}(\bv) \ ,
\end{align*}
which gives 
\begin{equation}  \label{eq:GG=G}
    \bG^P ( \bG^P (\bv ))= \bG^P (\bv ) \ .
\end{equation}
Thus $\bG^P$, like the original averaging operator $\bG$, is a linear projection.
%\extraproof{
%\begin{proof}
%By definition of $\bar{x}$, $G^P$, and $\Lambda$,
%\begin{align*}
%    \bar{x}(G^P(\bv)) &= \Lambda^{-1} \sum_j P_j^T W P_j \bar{x}(\bv)\\
%    &= \bar{x}(\bv),
%\end{align*}
%which gives \eqref{eq:consensus_operator_v}.
%\end{proof}
%}
With this, a further calculation shows that 
\begin{equation} \label{eq:2G-I-inv}
(2 \bG^P -\bI)^{-1} = (2 \bG^P -\bI) \  .  
\end{equation}
From this we obtain the equivalent fixed point formulation of \eqref{eq:pmace-eqns} as 
\begin{equation}  \label{eq:T-fixed}
    (2 \bG^P -\bI)(2 \bF - \bI)(\bv^*)  = \bv^* \ ,
\end{equation}
or $\bT \bv^* = \bv^*$, where $\bT = (2 \bG^P -\bI)(2 \bF - \bI)$.

If $\bT$ is non-expansive and has a fixed point, then Mann iterations with $\rho \in (0,1)$, defined as  
\begin{equation}  \label{eq:Mann-T}
    \bv \gets (1 - \rho) \bv + \rho \bT \bv \ ,
\end{equation}
are guaranteed to converge to a fixed point of $\bT$ and hence to a solution of \eqref{eq:pmace-eqns}. The parameter $\rho$ is a kind of step size; its choice impacts the convergence rate and the stability of the iterations but not the final solution.  



%%%%%%%%%%%%%%%%%%%%%


\subsection{PMACE for Optimization and Beyond} % Equilibrium}
\label{subsec:PMACEOptimization}

A natural question is whether PMACE corresponds to the solution of an optimization problem.  We show in Theorem~\ref{thm: using_prox_equil_problem} that the answer is affirmative in certain special cases, but not in general. (Proofs are given in the appendix.)
% Neglecting some mathematical caveats, the answer is that PMACE is equivalent to optimization when each $F_j$ is a proximal map for a smooth convex function and $W$ is a scalar multiple of the identity.
% In this case, the PMACE solution $x^* = \bar{x}(\bv^*)$ of \eqref{eq:pmace-eqns} corresponds to $\grad f(x) = 0$ where $f(x) = \sum_j f_j(P_j x)$.

To understand the relationship between equilibrium and optimization, suppose that each $F_j$ is the proximal map for a real-valued, differentiable, convex function $f_j(v_j)$:
\begin{equation}
\label{eq: proximal_map_function}
    \begin{aligned}
        F_{j} (v_{j}) = \arg \min _ {v} \left \{ f_{j}(v) + \frac{1}{2 \sigma^2} \left \| v - v_{j} \right \|^2 \right \} \ .
    \end{aligned}
\end{equation}
%This form arises by applying the ADMM algorithm to minimize $\sum_j f_j(P_j x)$ in the form of minimizing $\sum_j f_j(v_j)$ subject to $v_j = P_j x$ for all $j$ \cite[Section 7.2]{boyd2011distributed}. 
% In order to use standard real-valued partial derivatives and gradients, in this section we separate complex-valued images and patches into real and imaginary components and regard these as separate variables.  

In this setting, a PMACE solution is equivalent to a zero of a naturally associated vector field.  
\begin{theorem}
\label{thm: using_prox_equal_optimization_problem}
Let $F_j, j = 0, \cdots, J-1$ denote the proximal map function of a differentiable convex function $f_j$ as specified in \eqref{eq: proximal_map_function}.  Then $\bv^*$ is a solution to the PMACE equation in \eqref{eq:pmace-eqns} if and only if $x^* = \bar{x}(\bv^*)$ satisfies
\begin{equation}
    \label{eq:vec-field}
    \sum_{j=0}^{J-1} P_j^T W \nabla f_j(P_j x^*) = 0 \ .
\end{equation}
\end{theorem}

Note that \eqref{eq:vec-field} captures the idea of equilibrium in that the sum of the $J$ vectors is 0.  
However, as described in more detail below, because of the weight matrix $W$, this vector field may not be a gradient field, hence may not correspond naturally to a minimization problem.  
To explain this more fully, we first recall some basic results on vector fields.  

We say that $V(x)$ is a conservative vector field if it is the gradient of some potential function $f(x)$~\cite[Section 9.2A]{williamson2004multivariable}.
In this case, the condition $V(x^*)=0$ is a first-order necessary condition for the minimization of $f$.
Moreover, a continuously differentiable vector field, $V(x)$, is conservative if and only if its Jacobian is self-adjoint (or symmetric)~\cite[Section 9.2C]{williamson2004multivariable}.  

When each $f_j$ is twice continuously differentiable, the Jacobian of the vector field in \eqref{eq:vec-field} (as a function of $x$ in place of $x^*$) is
\begin{equation}
    \label{eq:grad-vec-field}
    \sum_{j=0}^{J-1} P_j^T W H_{f_j}(P_j x) P_j \ ,
\end{equation}
where $H_{f_j}(P_j x)$ is the symmetric Hessian matrix of 2nd order partial derivatives of $f_j$, evaluated at $P_j x$.  
If $W$ is a multiple of the identity, then the matrix in \eqref{eq:grad-vec-field} is symmetric, so the PMACE solution corresponds naturally to a minimization problem.  
However, this symmetry is not robust in that nonzero mixed partial derivatives in $f_j$ together with distinct entries in the diagonal matrix $W$ will destroy this symmetry.  
This leads to the following theorem.
\begin{theorem}
\label{thm: using_prox_equil_problem}
Let $f_j$ and $F_j$ be as in Theorem~\ref{thm: using_prox_equal_optimization_problem}.  
\begin{enumerate}
    \item If $W = rI$ for scalar $r$,  then PMACE is equivalent to optimization in that $\bv^*$ is a solution to the PMACE equation in \eqref{eq:pmace-eqns} if and only if $x^* = \bar{x}(\bv^*)$ satisfies
    \begin{equation}
    \label{eq: ml_est}
    x^* = \arg\min_{x}\left \{\sum_{j=0}^{J-1}f_{j}(P_j x) \right \} \ .
    \end{equation}
    \item For generic diagonal $W$ and generic convex $f_j$, the PMACE formulation does not naturally arise as an optimization problem.  That is, the vector field in \eqref{eq:vec-field} is not a conservative vector field, and hence is not the gradient field of a potential function. 
\end{enumerate}
\end{theorem}

In other words, even when the agents $F_j$ are proximal maps, if the weight matrix $W$ is not a multiple of the identity, then the natural vector field defining the PMACE solution is not a gradient field, so the PMACE solution is not naturally given by minimizing a cost function.  
Of course, it's possible to set up an optimization problem with the same solution by minimizing the norm of the vector field in \eqref{eq:vector-field-V}, but this doesn't correspond to a consensus minimization problem as in \eqref{eq: ml_est}.  

When the $f_j$ are convex but not everywhere differentiable, then the gradient $\nabla f_j$ in \eqref{eq:vec-field} is replaced with the possibly set-valued subdifferential $\partial f_j$, and the search for an $x^*$ to give a zero of the left-hand side becomes the search for an $x^*$ so that the left-hand side contains the 0 vector, which is known as a monotone inclusion problem. 
Results analogous to Theorem~\ref{thm: using_prox_equil_problem} can be obtained in this setting but are not pursued here.  
For more about the relationship between convex optimization and monotone operators, see~\cite{combettes_monotone_2018}.     

\bigskip
{\bf Non-proximal equilibrium:}  
To extend to the case in which the agents $F_j$ are not proximal maps, we note that the proximal map in \eqref{eq: proximal_map_function} can be rewritten~\cite[Cor.~17.6]{Bauschke2011ConvexAA} as $F_j(v_j) = v_j - \sigma^2 \nabla f_j(F_j(v_j))$.  
That is, a proximal map for a convex, differentiable function is an implicit gradient descent step in the sense that the gradient is evaluated at the endpoint rather than the starting point.  
With this and the fact that $F_j(v_j^*) = P_j x^*$, \eqref{eq:vec-field} can be rewritten as 
\begin{equation} \label{eq:PMACE-optimization-equilibrium-v}
     \sum_{j=0}^{J-1} P_j^T W (F_j(v_j^*) - v_j^*) = 0 \ .
\end{equation}

The vector field in this case is defined not on an image $x$ but on a stack of patches as in \eqref{eq: stacked_vj}.  In order to use this formulation to give a single reconstruction, $x^*$, we note that each $v_j^*$ must satisfy the constraint $F_j(v_j^*) = P_j x^*$.  We use this relationship to implicitly define a set-valued inverse $F_j^{-1}(P_j x))$ that maps $x$ to all patches $v_j$ consistent with $x$.   

With this, we reformulate \eqref{eq:PMACE-optimization-equilibrium-v} and hence the PMACE equation in \eqref{eq:pmace-eqns} as an inclusion problem.

\begin{theorem} 
\label{thm: pmace_sol_equal_v_goes_zeros}
Define the set-valued vector field 
\begin{equation}  \label{eq:vector-field-V}
   V(x) = \sum_{j=0}^{J-1} P_j^T W \left( P_j x  - F_j^{-1}(P_j x)\right) \ . 
\end{equation}
Then the solution $x^* = \bar{x}(\bv^*)$ of the PMACE equation in ~\eqref{eq:pmace-eqns} is the solution of the inclusion problem 
\begin{equation}  \label{eq:inclusion-V}
0 \in V(x^*) \ .
\end{equation}
\end{theorem}

We call this a consensus equilibrium problem because the common point $x^*$ yields a set of vectors $v_j^*$ with $F_j(v_j^*) = P_j x^*$ (consensus) such that the weighted average updates give the zero vector as in \eqref{eq:PMACE-optimization-equilibrium-v}.  

The verification that a given operator is monotone is often technically involved, so we do not address that here. Likewise, further conditions under which \eqref{eq:vector-field-V} corresponds to an optimization problem are beyond the scope of this paper.   

% \subsection{PMACE with Regularization}
% \label{subsec:regularization}

% The use of regularizers for inverse problems has a long history, much of which focuses on the use of a prior probability in a Bayesian framework \cite{sauer1992bayesian}.  
% % is extensively explored over the past years \cite{liao2008sparse, ravishankar2010mr}. It is shown that the combination of well-designed prior models can 
% % produce advanced results \cite{danielyan2011bm3d}. 
% However, many regularizers are not differentiable. This induces the utilization of proximal operators in various algorithms. 
% However, Venkatakrishnan \MakeLowercase{\textit{et al.}}~\cite{venkatakrishnan2013plug} demonstrate that by using the ADMM algorithm, the prior probability term converts into a proximal map update, which is interpreted as an AWGN denoiser in the plug-and-play priors (PnP) framework. Replacing a true proximal map with a black-box denoiser allows the use of state-of-the-art denoising algorithms such as BM3D \cite{dabov2007image}
% %, WNNM \cite{gu2014weighted}, TNRD \cite{chen2016trainable},
% and DnCNN \cite{zhang2017beyond} as prior agents in PnP and in MACE. 

% Therefore, one can combine the regularizer with iterative updates through substituting the proximal operators in PnP and MACE with a well-designed image denoiser such as such as BM3D \cite{dabov2007image}, WNNM \cite{gu2014weighted}, TNRD \cite{chen2016trainable}, and DnCNN \cite{zhang2017beyond}.

% In many applications of MACE, a denoiser or other regularizer is used as an agent parallel to data-fitting agents \cite{majee2021multislice,reid2022multiresolution}.  In this approach, with a single denoiser $H$, we use the $F_j, P_j$, $j=0, \ldots, J-1$ as before and define $P_J = I$ and 
% $$
% \bF_H^\pa (\bv) = [F_0(v_0), \ldots, F_{J-1}(v_{J-1}), H(v_J)] \,,
% $$
% where $v_J$ has dimension of the full image.  
% We average the output of the denoiser and the reconstruction obtained by the $v_j$ using
% \begin{equation}
% \label{eq:consensus_operator_component_vH}
% \bar{x}_H(\bv) = \frac{1}{2} \left(  \bar{x}([v_0, \ldots, v_{J-1}]) + v_J \right)\ .
% \end{equation}
% This could be replaced by a weighted average of $\bar{x}$ and $v_J$ to change the relative weight of data-fitting versus regularization.  We use $\bar{x}_H$ to define the parallel averaging operator   
% \begin{equation}
% \label{eq:consensus_operator_vH}
%     \bG_H^\pa ( \bv ) =
%     \left[
%     P_0 \bar{x}_H(\bv),
%     \ldots,
%     P_{J} \bar{x}_H(\bv)
%     \right] \ . 
% \end{equation}
 
% Alternatively, evidence presented in~\cite{sridhar2020distributed} indicates that convergence of the iterations in \eqref{eq:Mann-T} may be faster when the regularization is applied in series, {\em after} averaging the terms associated with the other agents.  
% Moreover, a typical denoiser acts on the entire image rather than on components, so in the case of PMACE, it is more natural to denoise the full image obtained by averaging and then redistribute via the $P_j$.  

% We apply serial regularization by using the operator $\bar{x}(\bv)$ in \eqref{eq:consensus_operator_component_v} to create a full image from components, then regularize this image and distribute back to components.  This leads to the serial averaging operator
% \begin{equation}  \label{eq:GH}
%     \bG_H^\se(\bv) = [P_0 H(\bar{x}(\bv)), \ldots, P_{J-1} H(\bar{x}(\bv))] \ .
% \end{equation}
% The difference between this and \eqref{eq:consensus_operator_vH} is that here the components associated with the data fitting terms are reassembled into a full image before denoising and redistribution, while in \eqref{eq:consensus_operator_vH} the denoiser maintains its own separate estimate of the full image, which is then averaged in parallel with the estimate from the data fitting terms.  

% The following theorem gives an equivalence between solving the original PMACE problem with $H$ as an agent parallel to the agents $F_j$ and solving a fixed point problem using $\bG_H^\se$.  

% \begin{theorem}
% \label{thm: equiv_sol_to_ce_eqns}
% The solutions for the three problems  
% \begin{equation} \label{eq: fHv_equal_Gv}
%     \bF_H^\pa (\bv^*) = \bG_H^\pa (\bv^*) \ ,
% \end{equation}
% \begin{equation} \label{eq: fixed_pt_of_T_map_parallel_H}
%     (2\bG_H^\pa - \bI)(2 \bF_H^\pa - \bI) (\bv^*) = \bv^* \ ,
% \end{equation}
% \begin{equation} \label{eq: fixed_pt_of_T_map_serial_H}
%     (2\bG_H^\se -\bI)(2 \bF - \bI)(\bw^*) = \bw^* \ ,
% \end{equation}
% are equivalent, where $x^* = \bar{x}_H(\bv^*) = H(\bar{x}( (2\bF - I)(\bw^*) ))$ and $w_j^* = v_j^*$ for $j=0, \ldots, J-1$.
% \end{theorem}

% Based on the third formulation, we can solve this problem using the Mann iterations of \eqref{eq:Mann-T} with $\bG_H^\se$ in place of $\bG^P$.  

% It's tempting to use $\bG_H^\se$ to set up an equation analogous to \eqref{eq:pmace-eqns}.  However, this formulation would not generally yield the same solution as applying $H$ as an agent parallel to the $F_j$.  The problem in this case is that $\bG_H^\se (\bG_H^\se ) = \bG_{H^2}^\se$, which is generally not equal to $\bG_H^\se$, so that $2\bG_H^\se - \bI$ is not self-inverse. %Instead, the key is to use the equivalence given in Theorem~\ref{thm: pmace_sol_equal_v_goes_zeros}.  


% Also, for a constant $\mu \in (0, 1)$, define the averaged agent
% $$
% H_\mu(x) = (1-\mu) x + \mu H(x).  
% $$
% This type of agent has been used for instance in \cite{selin's paper}. 


\begin{figure}[htb]
    \centering
    \vspace{0.4cm}
    \includegraphics[width=9cm]{figs/Ptychography_illustration.png}
    \vspace{0.1cm}
    \caption{In ptychographic imaging, a coherent probe is used to illuminate a sample at multiple, partially overlapping regions. The intensities of the resulting diffraction patterns are recorded by the detector.}
    \label{fig:ptycho_illustration}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PMACE for ptycho
\section{PMACE for Ptychography}
\label{sec:PMACEPtychography}

As noted in the introduction and illustrated in Figure~\ref{fig:ptycho_illustration}, a ptychographic reconstruction relies on far-field intensity measurements (also known as diffraction patterns) obtained by illuminating a sample with a coherent X-ray source at multiple locations, with significant overlap between adjacent illuminated regions.    
The goal of these measurements is to reconstruct the full complex transmittance image of the scanned sample.  
The phase of this complex transmittance image is particularly important since it gives precise information about the thickness of the sample at each location.  

A potential point of confusion in this problem is that there are two sets of phases that must be estimated for this reconstruction.  One set is the phase of the complex transmittance image, which is the primary quantity of interest because it gives very precise, quantitative information about the material properties of the sample.  The other set contains the phases associated with the far-field intensity data.  These far-field phases are essentially the phases of the Fourier transform of the illuminated sample.  Since the detector measures intensities only, the far-field phases must be estimated using the redundancies associated with overlapping intensity measurements -- this is known as the phase-retrieval problem.  However, for our purposes, the far-field phases are really just nuisance parameters that must be estimated in order to estimate the phase and magnitude of the complex transmittance.  

To apply PMACE to the ptychography problem, we define each data-fitting agent to be a local update to better match the measurements of the far-field Fraunhofer diffraction pattern in a single patch.  
Our approach does not use a penalty function/proximal map formulation but rather defines an update agent for each patch update directly from data using a weighted average between a previous estimate and new estimate that fits the available data.  This approach has the advantages of being computationally efficient and easily adjusted to account for the signal-to-noise ratio of the data.      

While probe estimation is an important component of ptychography, we assume for clarity in this paper that an accurate estimate of the probe function is known and consistent at all scanning positions.  
In a future paper we will apply PMACE to estimate both the probe and complex image simultaneously.
% In this paper we assume a known probe function.  In a future paper we will apply PMACE to estimate both the probe and complex image simultaneously.  



\subsection{Forward Model}

We let $x \in \mathbb{C}^{N_{1} \times N_{2}}$ denote the complex material transmittance of the target object and seek to recover $x$ from real-valued measurements $y_{j} \in \mathbb{R}^{N_{p}\times N_{p}}$ of the form
\begin{equation}
\label{eq: mag_measurement}
y_{j} = \sqrt{\text{Pois} \left ( | \mathcal{F} D P_{j} x |^2 \right )} \ .
\end{equation}
Here $\mathcal{F}$ denotes the 2D orthonormal discrete Fourier transform, $D = \text{Diag}(d)$ is a diagonal matrix representing the probe's complex illumination function $d$, and $P_{j}: \mathbb{C}^{N_{1}\times N_{2}} \rightarrow \mathbb{C}^{N_{p}\times N_{p}}$ is the linear operator representing the extraction of an $N_{p}\times N_{p}$ patch corresponding to the $j^{th}$ scan position, $j=0,\ldots, J-1$.
Thus $\mathcal{F} D P_{j} x$ is the mean complex far-field diffraction pattern associated with the $j^{th}$ scan position.  
We assume that the physical sensor measurements can be modeled by a Poisson distribution $\text{Pois} \left ( \lambda \right )$ with mean $\lambda$ given by the pixel-wise intensity of this diffraction pattern, and that these measurements are pre-processed by taking a square-root, which serves as a variance-stabilizing transform for the Poisson distribution~\cite{godard2012noise}.


\subsection{PMACE Formulation}
To estimate $x$ from the measurements $y_j$ using PMACE, we define data-fitting agents $F_{j}(v_j)$ by taking a weighted average between a nonlinear projection onto measured data and an input point.  
The nonlinear projection begins with a complex-valued image patch, $v_j= P_j x$, and isolates the phase in diffraction space by $\mathcal{F} D v_j / |\mathcal{F} D v_j|$ (the phase can be set arbitrarily if the magnitude is 0).  This phase is multiplied point-wise by the amplitudes $y_j$, then the Fourier transform and probe multiplication are inverted to return to image space.  Hence, the full nonlinear projection from an input image patch to one that matches the given amplitudes in diffraction space is $D^{-1}\mathcal{F}^{*} \left ( y_{j} \frac{\mathcal{F} D v_{j}}{|\mathcal{F} D v_{j}|} \right )$. For the full data-fitting agent, we average this projection with the input point to obtain 
\begin{align} \label{eq:Fjv}
    F_{j} (v_{j})
        & = (1 - \alpha ) v_{j} + \alpha D^{-1}\mathcal{F}^{*} \left ( y_{j} \frac{\mathcal{F} D v_{j}}{|\mathcal{F} D v_{j}|} \right ) \ ,
\end{align}
where $\alpha$ controls the strength of fit to data and $\alpha / (1-\alpha)$ can roughly be interpreted as the signal-to-noise ratio of the data. 

This interpolation between input and a data-fitting point is similar in spirit to a data-fitting proximal map. 
However, the Jacobian of \eqref{eq:Fjv} is not symmetric, so $F_j$ is not a proximal map (where we convert complex images to real-valued vectors by stacking real and imaginary parts in order to take real derivatives).  
In fact, Theorem~\ref{thm: pmace_sol_equal_v_goes_zeros} implies that the PMACE formulation using these $F_j$ does not naturally correspond to a minimization problem.  In the appendix, we show that $F_j$ is invertible, which implies that $V(x)$ in \eqref{eq:vector-field-V} is a single-valued vector field, which is nonconservative since the Jacobian of \eqref{eq:Fjv} is not symmetric.  Hence, as in Theorem~\ref{thm: using_prox_equil_problem}, this formulation is not naturally a minimization problem. 

%\extraproof{
%\begin{proof}[Proof that $F_j$ is invertible]
%To invert $F_j$, multiply both sides of \eqref{eq:Fjv} by $\mathcal{F} D$ to get
%\begin{equation} \label{eq:FDFj}
%\mathcal{F} D F_j(v_j) = (\alpha |\mathcal{F} D v_{j}| + (1-\alpha) y_j) \frac{\mathcal{F} D v_{j}}{|\mathcal{F} D v_{j}|}.
%\end{equation}
%Taking absolute value of both sides and using $y_j \geq 0$ allows us to solve for $|\mathcal{F} D v_{j}|$ in terms of $F_j(v_j)$, $y_j$, and $\alpha$. 
%This leaves $\mathcal{F} D v_{j}$ as the only unknown in \eqref{eq:FDFj}, so we solve for that and then multiply by $D^{-1} \mathcal{F}^*$ to get $v_j$.
%\end{proof}
%}


We use these $F_j$ to define the vectorized operator $\bF$ as in \eqref{eq: stacked_forward_operators_v}.  For $\bG$ in \eqref{eq:consensus_operator_v}, we  model the uncertainty arising from the point-to-point variation of probe illumination by using the weight matrix 
\begin{equation} \label{eq:Wkappa}
    W = |D|^\kappa
\end{equation}
to define $\Lambda$ and $\bar{x}$.  Here $\kappa \in [1,2]$ can be used to control the emphasis between brightly lit points near the center of the probe (large $\kappa$) versus dimly lit points near the edges of the probe (small $\kappa$).  
In our experiments, $\kappa \in [1.25, 1.5]$ gives the fastest convergence and best reconstruction quality; we display all results using $\kappa = 1.5$.  

The PMACE solution is then determined by solving $\bF(\bx) = \bG(\bx)$ or by iterating \eqref{eq:Mann-T} to obtain a fixed point of $\bT$. This has the effect of balancing the updates of the data fitting agents for each patch, subject to constructing an image that agrees with each individual patch.  

The resulting algorithm has similarities to SHARP~\cite{marchesini2016sharp} but also significant differences. 
First, equation \eqref{eq:Fjv} incorporates the data projector of~\cite{marchesini2016sharp}, but \eqref{eq:Fjv} is in the image domain and averaged with the identity, as opposed to the SHARP data projector, which is in the so-called frame domain (probe times image) and which is not averaged.  
The averaging factor $\alpha$ replaces the stabilization factor $\beta$ in the data projector of SHARP.  
More significantly, the averaging operator $P_Q$ of SHARP uses a fixed weighting that is essentially equivalent to taking $\kappa = 2$ in \eqref{eq:Wkappa}, which tends to underweight the contributions of locations with small to moderate probe intensities.   

%\extraproof{
%The resulting algorithm has similarities to SHARP \cite{marchesini2016sharp} but also significant differences. 
%First, \eqref{eq:Fjv} incorporates the data projector of \cite{marchesini2016sharp}, but \eqref{eq:Fjv} is in the image domain and averaged with the identity, as opposed to the SHARP data projector, which is in the so-called frame domain (probe times image) and which is not averaged.  
%The averaging factor $\alpha$ replaces the stabilization factor $\beta$ in the data projector of SHARP.  
%More significantly, the averaging operator $P_Q$ of SHARP uses a fixed weighting that is essentially equivalent to taking $\kappa = 2$ in \eqref{eq:Wkappa}, which tends to underweight the contributions of locations with small to moderate probe intensities.   
%}


The pseudo-code for computing the PMACE solution is shown in Algorithm \ref{alg: PMACE}. 
The algorithm starts with an initial guess of the complex transmittance of the unknown object $x^{(0)} \in \mathbb C^{N_{1} \times N_{2}}$, and then constructs the stacked projections. Mann iterations are then used to revise the estimates of individual projections. The final reconstruction of the complex object is given by taking the weighted average of the estimates of individual projections. 


% Pseudo-code of PMACE
\begin{algorithm}[H]
\setstretch{1.25}
\caption{Mann iteration for computing PMACE solution.}\label{alg: PMACE}
\begin{algorithmic}
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
 \renewcommand{\algorithmicensure}{\textbf{Output:}}
 \REQUIRE Initialization: $x^{(0)} \in \mathbb C^{{N_1} \times N_{2}}$; $\kappa \in [1.25, 1.5]$ 
 \ENSURE  Final Reconstruction: $\hat{x} \in \mathbb C^{{N_1} \times N_{2}}$
  \STATE $\mathbf{w} = \mathbf{v} = [v^{(0)}_{0},\dots, v^{(0)}_{J-1}],\; \text{where} \; v^{(0)}_{j} = P_{j} {x}^{(0)} $ \\
  \WHILE {not converged}
  \STATE \hspace{0.5cm}$\mathbf{w} \gets \bF ( \mathbf{v} )$
  \STATE \hspace{0.5cm}$\mathbf{z} \gets \bG (2\mathbf{w} - \mathbf{v})$
  \STATE \hspace{0.5cm}$\mathbf{v} \gets \mathbf{v} + 2 \rho (\mathbf{z} - \mathbf{w})$
  \ENDWHILE
 \RETURN $\hat{x} = \Lambda^{-1} \sum_{j=0}^{J-1} P_{j}^{T} |D|^{\kappa} v_{j}$ 
\end{algorithmic}
\end{algorithm}

% We modify this algorithm to incorporate regularization by using Theorem~\ref{thm: equiv_sol_to_ce_eqns}.  This requires only using $\bG_H^\se$ in place of $\bG$ and returning the corresponding estimate of $x^*$ based on the resulting fixed point.  
  
  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experimental Results
\section{Experimental Results}
\label{sec: experimental result}
In this section, we present the results of our approach on both synthetic and measured data and compare with reconstruction results using state-of-the-art algorithms.  
Our results indicate that PMACE matches the best-competing algorithms in convergence speed and image quality for small probe spacing, and outperforms the competitors as the probe spacing between neighboring scan points increases. \footnote{ The code for PMACE is available at \url{https://github.com/cabouman/ptycho_pmace}.}


\subsection{Synthetic Data Generation}
To simulate ptychographic data, we used the complex images in Figure~\ref{fig:ground_truth_imgs}.
The synthetic complex transmittance image of size $800 \times 800$ pixels was generated from modeling and computation of a 5-layer Copper(Cu)-Tungsten(W) composite material. The complex probe of size $256 \times 256$ pixels was simulated with a photon energy of 8.8 keV. The phase images shown in Figure~\ref{fig:ground_truth_imgs} are measured in radians.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figures for the results section

% ground truth images
\begin{figure}[!htp]
\vspace{-0.2cm}
\begin{minipage}[htb]{0.45\linewidth}
  \centering
  \hspace{0.1cm}
  \centerline{\includegraphics[height=3.4cm]{figs/gt_img/gt_obj_mag.png}}
  \vspace{-0.2cm}
  \centerline{(a)} \smallskip
\end{minipage}
\begin{minipage}[htb]{0.45\linewidth}
  \centering
  \hspace{0.1cm}
  \centerline{\includegraphics[height=3.4cm]{figs/gt_img/gt_obj_phase.png}}
  \vspace{-0.2cm}
  \centerline{(b)} \smallskip
\end{minipage}
\hfill
\begin{minipage}[htb]{0.45\linewidth}
  \centering
  \hspace{0.1cm}
  \centerline{\includegraphics[height=3cm]{figs/gt_img/gt_probe_mag.png}}
  \vspace{-0.2cm}
  \centerline{(c)} \smallskip
\end{minipage}
\hfill
\begin{minipage}[htb]{0.45\linewidth}
  \centering
  \hspace{0.1cm}
  \centerline{\includegraphics[height=3cm]{figs/gt_img/gt_probe_phase.png}}
  \vspace{-0.2cm}
  \centerline{(d)} \smallskip
\end{minipage}
\hfill
\vspace{-0.1cm}
\caption{Ground truth images for simulated data. Magnitude (a) and phase (b) of complex object, and magnitude (c) and phase (d) of complex probe.}
\label{fig:ground_truth_imgs}
%
\end{figure}



%%% reconstructed phases from noisy synthetic data 
\begin{figure*}[!htb]
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/ePIE_phase.png}}
  \vspace{0.1cm}
  \centerline{(a) ePIE}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/AWF_phase.png}}
  \vspace{0.1cm}
  \centerline{(b) AWF}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/SHARP_phase.png}}
  \vspace{0.1cm}
  \centerline{(c) SHARP}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{0.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/PMACE_phase.png}}
  \vspace{0.1cm}
  \centerline{(d) PMACE}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.05\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/colorbar_phase.png}}
  \vspace{0.1cm}
  \centerline{ }\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/ePIE_phase_err.png}}
  \vspace{0.1cm}
  \centerline{(a) $e$ = 0.175}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/AWF_phase_err.png}}
  \vspace{0.1cm}
  \centerline{(b) $e$ = 0.111}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/SHARP_phase_err.png}}
  \vspace{0.1cm}
  \centerline{(c) $e$ = 0.051}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/PMACE_phase_err.png}}
  \vspace{0.1cm}
  \centerline{(d) \textbf{$e$ = 0.037}}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.04\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/syn_result_probe_dist_68/colorbar_phase_err.png}}
  % \vspace{0.1cm}
  \centerline{ }\medskip  
\end{minipage}
%\hfill
\vspace{0.1cm}
\caption{Top:  {\bf Phases} (in radians) of the reconstructed complex transmittance images in Figure~\ref{fig:ground_truth_imgs} from synthetic data with a small overlap:  $r_{\mathrm{ovlp}} \approx 14.5\%$.  Bottom: differences between the phases of reconstruction and ground truth, with NRMSEs in the subcaptions. Note that transmittance phases are typically more important in application than magnitudes. }
\label{fig:synthetic_result_phase}
\end{figure*}



%% reconstructed magnitudes from noisy synthetic data 
\begin{figure*}[!htb]
 \begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/ePIE_mag.png}}
  \vspace{0.1cm}
  \centerline{(a) ePIE}\medskip
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/AWF_mag.png}}
  \vspace{0.1cm}
  \centerline{(b) AWF}\medskip
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/SHARP_mag.png}}
  \vspace{0.1cm}
  \centerline{(c) SHARP}\medskip 
 \end{minipage}
 \hfill
 \begin{minipage}[b]{0.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/PMACE_mag.png}}
  \vspace{0.1cm}
  \centerline{(d) PMACE}\medskip
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.05\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/syn_result_probe_dist_68/colorbar_mag.png}}
  \vspace{0.1cm}
  \centerline{ }\medskip  
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/ePIE_mag_err.png}}
  \vspace{0.1cm}
  \centerline{(a) $e$ = 0.175}\medskip
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/AWF_mag_err.png}}
  \vspace{0.1cm}
  \centerline{(b) $e$ = 0.111}\medskip
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/SHARP_mag_err.png}}
  \vspace{0.1cm}
  \centerline{(c) $e$ = 0.051}\medskip  
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/syn_result_probe_dist_68/PMACE_mag_err.png}}
  \vspace{0.1cm}
  \centerline{(d) \textbf{ $e$ = 0.037}}\medskip  
 \end{minipage}
 \hfill
 \begin{minipage}[b]{.04\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/syn_result_probe_dist_68/colorbar_mag_err.png}}
  \vspace{0.1cm}
  \centerline{ }\medskip  
 \end{minipage}
 % \hfill
 \vspace{0.1cm}
 \caption{Top: {\bf Magnitudes} of the reconstructed complex transmittance images in Figure~\ref{fig:ground_truth_imgs} from synthetic data with a small overlap:  $r_{\mathrm{ovlp}} \approx 14.5\%$. Bottom: Amplitudes of error between the complex reconstructions and ground truth, with NRMSEs in the subcaptions.  Note that transmittance phases (Figure~\ref{fig:synthetic_result_phase}) are typically more important in application than magnitudes. }
 \label{fig:synthetic_result_mag}
 \vspace{1cm}
 \end{figure*}





To simulate measurements for a single probe location, we multiply the projection of the complex transmittance image by the complex probe, then take the squared modulus of the Fourier transform to simulate forward propagation. We scale to simulate a peak photon rate, add a factor for dark current, then use this as a mean for a sample from a Poisson distribution. With $r_{p}$ as the peak photon detection rate and $\lambda$ as the mean dark current, we have
\begin{equation}
\label{eq: poisson_rand_generator}
    \hat{y}_{j} \gets \sqrt{\mathrm{Pois}\left ( r_{p} \frac{|\mathcal{F} D x_{j}|^2}{\mathrm{max}(|\mathcal{F} D x_{j}|^{2})}  + \lambda \right ) }  \ ,
\end{equation}
where $\mathrm{max}(\cdot)$ denotes the maximum value of its argument and the operations are performed pointwise.
%Thus the smaller the $r_{p}$ is selected, the smaller the signal-to-noise ratio and the noisier the simulated measurements. 

Assuming a photon detector with 14-bit dynamic range and the presence of a half-bit of dark current, $r_{p}=10^4$ and $\lambda = 0.5$ were used to simulate noisy data. 

To simulate data from a full experiment, we first select a nominal distance (in the range of 20 to 76 pixels) between nearest-neighbor probe locations.  We then use a randomly perturbed rectangular grid for the probe sampling pattern to avoid periodic reconstruction artifacts~\cite{maiden2009improved}, with random offsets selected uniformly in $[-5, 5]$ (in units of pixels).

\subsection{Probe Overlap Ratio}
The key to estimating phase information from intensity measurements is the redundancy inherent in using multiple overlapping illuminations.   
An increase in probe spacing with a fixed probe size will reduce image reconstruction quality~\cite{huang2014optimization}, while using a larger probe will compensate for larger probe spacing~\cite{zhou2020low}. Therefore, a measure of illumination overlap is more relevant for reconstruction quality than simple probe spacing.

To quantify this overlap, we define the overlap ratio between adjacent probes indexed by $j$ and $k$ as
\begin{equation}\label{eq:overlap_ratio_adjacent}
   % r_{\mathrm{ovlp}} = \frac{d_{probe}}{2 \times d_{scan}} \ , \\
%    r_{\mathrm{ovlp}} = \frac{d_{probe} - d_{scan}}{d_{probe}} 
    r_{j,k} = \frac{\left \| \; P_{j}^{T} \left | D \right | \odot  P_{k}^{T} \left | D \right |  \; \right \|_{1}}{\left \|  \; \left | D \right | \odot  \left | D \right | \;  \right \|_{1}} \ ,
\end{equation}
where $\odot$ denotes point-wise multiplication and $\left \| \cdot \right \|_{1}$ denotes the $l_1$-norm. We then define the overlap ratio $r_{\mathrm{ovlp}}$ of the full grid scan pattern as the average of the overlap ratios between paired adjacent probes
\begin{equation}\label{eq:overlap_ratio}
   r_{\mathrm{ovlp}} = \frac{1}{N} \sum_{ \left \{ \left (j, k  \right ), \  j\neq k \right \} } r_{j, k} \ ,
\end{equation}
where $N$ denotes the number of pairs of adjacent scan positions.
The overlap ratio is near 1 when adjacent probes almost fully overlap and near 0 when they are almost disjoint. 





\subsection{Quality and Convergence for Small Overlap}
To evaluate image reconstruction quality, we assume a known complex probe (we will consider joint image-probe estimation in a future paper).  Given the complex probe profile in Figure~\ref{fig:ground_truth_imgs}, we generated synthetic data using fixed (simulated) probe spacing of 68 pixels with a corresponding overlap ratio of $r_{\mathrm{ovlp}} \approx 14.5\%$. The reconstructions on the synthetic noisy data are performed using PMACE, %both with and without regularization,
ePIE~\cite{maiden2009improved}, AWF~\cite{xu2018accelerated}, and SHARP~\cite{marchesini2016sharp}. 

%We note that SHARP is based on RAAR \cite{luke2004relaxed}, but SHARP as presented in \cite{marchesini2016sharp} has a different sign relative to \cite{luke2004relaxed}.  
%Hence we also compare a modification of SHARP that we call SHARP+, which uses the sign derived from \cite{luke2004relaxed}. 

To provide a fair comparison, we optimized one algorithmic parameter using grid-search for each method: the step size of ePIE, the relaxation parameter of SHARP, and the data-fitting parameter $\alpha$ for PMACE. 
%In the case of PMACE with regularization, we tune the regularization parameter while keeping the other parameters fixed. We use reg-PMACE to denote PMACE with regularization. 
We ran each method for 100 iterations.  Each iteration for each method involves 2 FFTs per probe location, and these FFTs are the dominant computational cost for each method, so 100 iterations represent essentially the same amount of computation for each method. 

% Quality Evaluation Metrics
%To avoid edge effects, we compute a quality metric using only the central region of size 800x800 pixels. 
We computed the Normalized Root-Mean-Square Error (NRMSE) between each reconstructed image $\hat{x}$ and the ground truth image $x$.  Since the data is invariant to a constant phase shift in the image domain, we minimized this unconstrained phase shift to obtain 
\begin{equation}
    \begin{aligned}
         e &= \min_{c} \frac{\| c\hat{x}- x \|}{\| x \|} \ .
    \end{aligned}
\end{equation}

The reconstructions in Figures~\ref{fig:synthetic_result_phase} and~\ref{fig:synthetic_result_mag} show that PMACE outperforms the other methods both visually and with respect to NRMSE. 
Figure~\ref{fig:synthetic_result_phase} shows the phases 
of each reconstruction, the corresponding error images, and the final NRMSE between each complex reconstruction and ground truth image. 
Figure~\ref{fig:synthetic_result_mag} shows the analogous images for the magnitudes.

For this example, the probe spacing is relatively large and the resulting probe overlap is relatively small.  In this setting, ePIE cannot reconstruct the complex transmittance image accurately. AWF and SHARP do significantly better, but the difference images indicate that both of these methods introduce artifacts not found in the ground truth, while  PMACE reduces these artifacts.  This is particularly relevant for the phases, since transmittance phases are typically more important in application than magnitudes.  %The reconstruction using PMACE reduces the error further. %, and the use of regularization with complex BM3D yields a small additional reduction in NRMSE.

The convergence of NRMSE as a function of the number of iterations is shown in Figure~\ref{fig:synthetic_result_convergence_plot}.
Consistent with Figures~\ref{fig:synthetic_result_phase} and~\ref{fig:synthetic_result_mag}, ePIE does not converge to a reasonable solution. 
AWF introduces a large initial error, then converges fairly slowly to a large final NRMSE. 
SHARP and PMACE have a large initial decrease in error followed by a slower decay, with SHARP leveling off fairly quickly and PMACE continuing to decrease fairly rapidly, with PMACE achieving the lowest final error.  

%%% convergence plots (on noisy data)
\begin{figure}[h!]
\centering
\includegraphics[width=3.5in]{figs/convergence_plot_d_68.png}
\caption{NRMSE between reconstruction and ground truth as a function of number of iterations for reconstructions on synthetic noisy data with a small overlap: $ r_{\mathrm{ovlp}} \approx 14.5\% $.}
\label{fig:synthetic_result_convergence_plot}
\end{figure}


\subsection{Reconstruction Quality Versus Probe Overlap}

To determine the effect of probe overlap on reconstruction quality, we repeated the experiment above using multiple probe spacing values between 20 and 76 pixels, corresponding to overlap ratios from $73 \%$ to $12 \%$. For each spacing value and each method, we optimized a single algorithmic parameter as described above, and we ran the corresponding method for 100 iterations.  

We plot the final NRMSE of each approach versus the overlap ratio in Figure~\ref{fig: synthetic_result_err_vs_overlap_ratio}. %We plot the final NRMSE of each approach versus probe spacing in Figure~\ref{fig: synthetic_result_err_vs_dist}. 
For large overlap ratios, all methods yield good image quality, with NRMSE values from about 2.5\% to 4.5\%.  
Even here, ePIE and AWF underperform relative to the other methods.  
As the overlap ratio decreases, NRMSE increases, with SHARP and PMACE maintaining good reconstruction quality of about 5\% or less as the overlap ratio decreases to about 15\%.
PMACE achieves an NRMSE below 7\% even with an overlap ratio $r_{\mathrm{ovlp}}$ below 12\%, while the SHARP reconstruction has an NRMSE above 10\% at this overlap ratio.  ePIE and AWF are not competitive at small overlap ratios.
%probe spacing of 68 pixels.  Even at 76 pixels, the PMACE variants have an NRMSE below 7\%, while the SHARP variants are near or above 10\%.  ePIE and AWF are not competitive at 76 pixels.

%%% NRMSE vs. average linear overlap ratio  (on noisy data)
\begin{figure}[t]
\centering
\includegraphics[width=3.8in]{figs/plot_nrmse_vs_overlap_ratio.png}
\caption{NRMSE between reconstruction and ground truth as a function of
the overlap ratio for each approach.}
\label{fig: synthetic_result_err_vs_overlap_ratio}
\end{figure}


\subsection{Reconstruction on Measured Data}
In this subsection, we evaluate PMACE on measured data and report reconstruction results.  
We used the gold balls data set~\cite{marchesini2017ptychography} consisting of 800 ptychographic measurements, which was generated using X-rays with a photon energy of 1 keV at beamline 5.3.2 of the Advanced Light Source. The scans were performed in a $20 \times 40$ grid with scan spacing of 30 nm and recorded by a detector placed 0.112 m downstream from the test sample of gold balls.
This data set also includes 20 separately acquired dark images in addition to measured data.

For this experiment, the overlap ratio is approximately $84\%$, which is higher than any value used in our simulated data. This large overlap ratio and the results of Figure~\ref{fig: synthetic_result_err_vs_overlap_ratio} indicate that any of the methods should be able to produce a reasonably accurate reconstruction.  

We preprocessed the raw data by subtracting from each measurement the average of 20 dark scans.  We also removed 6 abnormal measurements with high deviations in the data. Then we centered and cropped each measurement from $621 \times 621$ pixels to $512 \times 512$ pixels. 
To suppress noise, each of these diffraction measurements was multiplied by a 2D Tukey window that was generated by rotating a 1D Tukey window with shape parameter of 0.5. The resulting data set contains preprocessed 794 diffraction measurements.
We used the reference probe shown in Figure~\ref{fig:ref_probe} for reconstruction.  

% phase normalization to results on real data
%We used KMPty \cite{} to analyze the data set with position refinement and obtain a reference probe. / To evaluate our approach with known probe, we used ePIE with position refinement \cite{} to acquire a reference probe as shown in Figure~\ref{fig:ref_probe}.

%With the knowledge of complex probe, we are not blind to the amplitude of complex transmittance image to be reconstructed. For the fair comparison between reconstructions, we shift the phase of each complex reconstruction such that the resulting complex image has zero average phase. 

% reference probe image
\begin{figure}[h!]
\vspace{-0.4cm}
\begin{minipage}[htb]{0.45\linewidth}
  \centering
%  \hspace{.1in}
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/ref_probe_mag.png}}
  \vspace{-0.4cm}
  \centerline{(a)} \smallskip
\end{minipage}
\begin{minipage}[htb]{0.45\linewidth}
  \centering
%  \hspace{0.1cm}
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/ref_probe_phase.png}}
  \vspace{-0.4cm}
  \centerline{(b)} \smallskip
\end{minipage}
\hfill
\caption{Reference probe images. (a) Magnitude and (b) phase of complex reference probe.}
\label{fig:ref_probe}
\end{figure}

% % phases of reconstructions on Gold Balls data set
% \begin{figure*}[t!]
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/ePIE_phase.png}}
%   \centerline{(a) ePIE}\medskip
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/AWF_phase.png}}
%   \centerline{(b) AWF}\medskip
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/SHARP_phase.png}}
%   \centerline{(c) SHARP}\medskip  
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/PMACE_phase.png}}
%   \centerline{(d) PMACE}\medskip  
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.04\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/colorbar_phase.png}}
%   \centerline{}\medskip
% \end{minipage}
% \hfill
% \vspace{-0.2cm}
% \caption{Comparison of different methods for \textbf{phases} (in radians) of complex reconstructions from real data.}
% \label{fig:GoldBalls_phase}
% \end{figure*}

% phases of reconstructions on Gold Balls data set
\begin{figure*}[th]
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/ePIE/phase_bb.png}}
  \vspace{0.1cm}
  \centerline{(a) ePIE}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/AWF/phase_bb.png}}
  \vspace{0.1cm}
  \centerline{(b) AWF}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/SHARP/phase_bb.png}}
  \vspace{0.1cm}
  \centerline{(c) SHARP}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/PMACE/phase_bb.png}}
  \vspace{0.1cm}
  \centerline{(d) PMACE}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.04\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/colorbar_phase.png}}
  \centerline{}\medskip
\end{minipage}
\hfill
% red patch
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/ePIE/phase_red_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(a) ePIE}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/AWF/phase_red_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(b) AWF}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/SHARP/phase_red_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(c) SHARP}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/PMACE/phase_red_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(d) PMACE}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.04\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/colorbar_phase_mask.jpg}}
  \centerline{}\medskip
\end{minipage}
\hfill
% yellow patch
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=2.2cm]{figs/GoldBalls/ePIE/phase_yellow_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(a) ePIE}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=2.2cm]{figs/GoldBalls/AWF/phase_yellow_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(b) AWF}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=2.2cm]{figs/GoldBalls/SHARP/phase_yellow_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(c) SHARP}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=2.2cm]{figs/GoldBalls/PMACE/phase_yellow_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(d) PMACE}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.04\linewidth}
  \centering
  \centerline{\includegraphics[height=2.2cm]{figs/GoldBalls/colorbar_phase_mask.jpg}}
  \centerline{}\medskip
\end{minipage}
\hfill
% cyan patch
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=3.4cm]{figs/GoldBalls/ePIE/phase_cyan_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(a) ePIE}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=3.4cm]{figs/GoldBalls/AWF/phase_cyan_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(b) AWF}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=3.4cm]{figs/GoldBalls/SHARP/phase_cyan_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(c) SHARP}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=3.4cm]{figs/GoldBalls/PMACE/phase_cyan_patch_bb.png}}
  \vspace{0.1cm}
  \centerline{(d) PMACE}\medskip  
\end{minipage}
\hfill
% \begin{minipage}[b]{.04\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/colorbar_phase.png}}
%   \centerline{}\medskip
% \end{minipage}
\hfill
\vspace{-0.1cm}
\caption{\textbf{Phases} (in radians) of complex reconstructions from measured data. The top row shows the full field of view along with outlines of insets displayed in rows 2-4.  All methods reconstruct fine detail in the central region (row 3, outlined in yellow).  ePIE and AWF yield artifacts near the edge of the field of view, while SHARP and PMACE produce more uniform results throughout the field of view.}
\label{fig:GoldBalls_phase}
\end{figure*}



% magnitudes of reconstruction on Gold Balls data set
\begin{figure*}[th]
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/ePIE_mag.png}}
  \vspace{0.1cm}
  \centerline{(a) ePIE}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/AWF_mag.png}}
  \vspace{0.1cm}
  \centerline{(b) AWF}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/SHARP_mag.png}}
  \vspace{0.1cm}
  \centerline{(c) SHARP}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.2\linewidth}
  \centering
  \centerline{\includegraphics[height=4cm]{figs/GoldBalls/PMACE_mag.png}}
  \vspace{0.1cm}
  \centerline{(d) PMACE}\medskip  
\end{minipage}
\hfill
\begin{minipage}[b]{.04\linewidth}
  \centering
  \centerline{\includegraphics[height=4.1cm]{figs/GoldBalls/colorbar_mag.png}}
  \centerline{}\medskip
\end{minipage}
\hfill
\vspace{-0.1cm}
\caption{\textbf{Magnitudes} of complex reconstructions from measured data.  PMACE captures fine detail in the largest field of view relative to the other methods.}
\vspace{0.5cm}
\label{fig:GoldBalls_mag}
\end{figure*}


% Obtain reference probe and reconstruction details
As in the case of simulated data, we varied one algorithmic parameter for each method and selected the parameter that produced the highest image contrast and fewest image artifacts. We ran each method for 100 iterations. 
As noted above, the fit to data is unchanged by a constant phase shift in the reconstruction.  To provide a uniform visual comparison, we shift the phase of each reconstruction to achieve zero-mean phase in the central rectangle of $105 \times 65$ pixels, which was chosen since the centers of the probe illuminations are located in this region.  

The reconstructed phases and magnitudes are shown in Figure~\ref{fig:GoldBalls_phase} and~\ref{fig:GoldBalls_mag} (in a region of $400 \times 400$ pixels). To highlight the difference in reconstruction results, we exhibit the plots of selected regions from each reconstructed phase in Figure~\ref{fig:GoldBalls_phase}. The selected regions include a $150 \times 180$ pixel region, a $120 \times 80$ pixel region, and a $180 \times 150$ pixel region enclosed by red lines, yellow lines, and blue lines, respectively. Compared with the central region, which contains a high density of scan points, the components in the other two regions are more challenging to be recovered due to less received illumination.


As can be observed, ePIE and AWF produce good image quality in the central region that is densely illuminated by the probe, but their image quality degrades rapidly outside this region.  SHARP and PMACE also produce good results in the central region, although with arguably somewhat lower contrast than ePIE and PMACE.  However, SHARP and PMACE are noticeably better at preserving finer features in the outer regions of the sample, with PMACE reconstructing gold balls in the largest field of view. 

To validate these reconstructions with no available ground truth image, we forward propagate the reconstructed complex images to the detector plane following \eqref{eq: mag_measurement} and calculate the NRMSE relative to the square-rooted data. The final errors of ePIE, AWF, SHARP and PMACE are 0.1102, 0.1041, 0.0914 and  0.0912, respectively. Since a lower NRMSE at the detector plane implies a better fit to measurements, the PMACE reconstruction provides the best fit to the measured data.   
Hence our reconstruction results on the gold balls data set reinforce the claim that PMACE provides state-of-the-art performance under high-overlap conditions for measured data. 
% The complex reconstruction using PMACE also preserves the details in outermost region. To demonstrate that PMACE reconstruction gives sharper edges, further improved contrast and less circular artifacts, we transform the cross-sections through PMACE and SHARP reconstructions to zero-mena and plot the zero-meaned cross-sections in Figure~\ref{fig: imag_val_vs_px}. Higher variance resulting from PMACE suggests that PMACE preserves finer details of sample. 



% % real parts of reconstruction on Gold Balls data set
% \begin{figure*}[htb!]
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/ePIE_real.png}}
% %   \vspace{-0.1cm}
%   \centerline{(a) ePIE}\medskip
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/AWF_real.png}}
% %   \vspace{-0.1cm}
%   \centerline{(b) AWF}\medskip
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/SHARP_real.png}}
% %   \vspace{-0.1cm}
%   \centerline{(c) SHARP}\medskip  
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/PMACE_real.png}}
% %   \vspace{-0.1cm}
%   \centerline{(d) PMACE}\medskip  
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.04\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4.1cm]{figs/GoldBalls/colorbar_real.png}}
% %   \vspace{-0.1cm}
%   \centerline{}\medskip
% \end{minipage}
% \hfill
% \vspace{-0.2cm}
% \caption{Comparison of different methods for \textbf{real parts} of complex reconstructions from real data.}
% \label{fig:GoldBalls_real}
% \end{figure*}

% %%% imaginary parts of reconstructions on Gold Balls data set
% \begin{figure*}[htb!]
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/ePIE_imag.png}}
% %   \vspace{-0.1cm}
%   \centerline{(a) ePIE}\medskip
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/AWF_imag.png}}
% %   \vspace{-0.1cm}
%   \centerline{(b) AWF}\medskip
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/SHARP_imag.png}}
% %   \vspace{-0.1cm}
%   \centerline{(c) SHARP}\medskip  
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.2\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4cm]{figs/GoldBalls/PMACE_imag.png}}
% %   \vspace{-0.1cm}
%   \centerline{(d) PMACE}\medskip  
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.04\linewidth}
%   \centering
%   \centerline{\includegraphics[height=4.2cm]{figs/GoldBalls/colorbar_imag.png}}
% %   \vspace{-0.1cm}
%   \centerline{}\medskip
% \end{minipage}
% \hfill
% \vspace{-0.2cm}
% \caption{Comparison of different methods for \textbf{imaginary parts} of complex reconstructions from real data.}
% \label{fig:GoldBalls_imag}
% \end{figure*}


\section{Conclusion}
In this paper, we provided the theoretical foundation to extend the inverse problem formulation MACE, in which each agent acts on a full reconstruction, to PMACE, with each agent acting on a portion of the reconstruction.  We described the corresponding averaging operator and described circumstances in which PMACE does and does not correspond to an optimization problem. We also developed an algorithm to find the PMACE solution. %We gave an algorithm to find the PMACE solution and described how to use state-of-art denoisers to incorporate regularization either in parallel or in serial. % We also introduced a complex-valued version of the widely-used denoiser BM3D and used this as a regularizer.

We applied PMACE to ptychography on both synthetic and measured data and found that it had improved image quality relative to competing algorithms, particularly for the case of large spacing between probe positions. This last point implies that our method requires less data for accurate reconstruction and can thus reduce acquisition time for a given level of reconstruction quality.  


% \section*{Acknowledgments}

{\appendices
%
%\input{bm3d}
%
{
\section{Proofs}

\subsection{Proof of Theorem~\ref{thm: using_prox_equal_optimization_problem}}
\extraproof{
\begin{theorem}
Let $F_j, j = 0, \cdots, J-1$ denote the proximal map function of a differentiable convex function $f_j$ as specified in \eqref{eq: proximal_map_function}.  Then $\bv^*$ is a solution to the PMACE equation in \eqref{eq:pmace-eqns} if and only if $x^* = \bar{x}(\bv^*)$ satisfies
\begin{equation}
    \sum_{j=0}^{J-1} P_j^T W \nabla f_j(P_j x^*) = 0.
\end{equation}
\end{theorem}
}
\begin{proof}
The assumptions on $f_j$ and $F_j$ imply~\cite[Sec. 6.1]{boyd2016primer} that $F_j(v_j) = (I + \sigma^2 \nabla f_j)^{-1}(v_j)$ and hence $v_j = (I + \sigma^2 \nabla f_j)(F_j(v_j))$.  With this in mind, suppose $\bv^*$ satisfies \eqref{eq:pmace-eqns}.  By definition of $\bF$, $\bG^P$, and $x^*$, this means
\begin{equation} \label{eq:FjPj}
    F_j(v_j^*) = P_j \bar{x}(\bv^*) = P_j x^*
\end{equation}
for all $j$.  Using $F_j = (I + \sigma^2 \nabla f_j)^{-1}$ and then applying $(I + \sigma^2 \nabla f_j)$ to both sides gives
\begin{equation} \label{eq:vjPj}
    v_j^* = P_j x^* + \sigma^2 \nabla f_j(P_j x^*) \ .
\end{equation}
Using this with the definition of $x^*$ gives
\begin{equation} 
    x^* = \bar{x}(\bv^*) = \Lambda^{-1} \sum_j P_j^T W(P_j x^* + \sigma^2 \nabla f_j(P_j x^*)) \ ,
\end{equation}
so
\begin{equation} \label{eq:Lambda-x-star}
    \Lambda x^* =  \sum_j P_j^T W P_j x^* + \sigma^2 \sum_j P_j^T W \nabla f_j(P_j x^*)) \ .
\end{equation}
Since the first term on the right hand side is $\Lambda x^*$, this simplifies to 
\begin{equation} \label{eq:sigma-vec-field}
    \sigma^2 \sum_j P_j^T W \nabla f_j(P_j x^*)) = 0 \ ,
\end{equation}
which gives \eqref{eq:vec-field}.

For the converse, if $x^*$ satisfies \eqref{eq:vec-field}, then by taking \eqref{eq:vjPj} as the definition of $v_j^*$, each step above is reversible to give \eqref{eq:FjPj} and hence \eqref{eq:pmace-eqns}.
\end{proof}

\subsection{Proof of Theorem~\ref{thm: using_prox_equil_problem}}
\extraproof{
\begin{theorem}
%\label{thm: using_prox_equil_problem}
Let $f_j$ and $F_j$ be as in Theorem~\ref{thm: using_prox_equal_optimization_problem}.  
\begin{enumerate}
    \item If $W = rI$ for scalar $r$.  Then PMACE is equivalent to optimization in that $\bv^*$ is a solution to the PMACE equation in \eqref{eq:pmace-eqns} if and only if $x^* = \bar{x}(\bv^*)$ satisfies
    \begin{equation}
    %\label{eq: ml_est}
    x^* = \arg\min_{x}\left \{\sum_{j=0}^{J-1}f_{j}(P_j x) \right \} \ .
    \end{equation}
    \item For generic diagonal $W$ and $f_j$, the PMACE formulation does not naturally arise as an optimization problem.  That is, the vector field in \eqref{eq:vec-field} is not a conservative vector field, and hence is not the gradient field of a potential function. 
\end{enumerate}
\end{theorem}
}
\begin{proof}
1.  Note that $\bG^P$ is independent of $r>0$ when $W = rI$, so we may assume $W = I$.  A straightforward modification to the proof of~\cite[Theorem 2]{buzzard2018plug}, shows that if $\bF$ is defined using the $F_j$, then a PMACE solution $\bv^*$ of \eqref{eq:pmace-eqns} is equivalent using $x^* = \bar{x}(\bv^*)$, $\lambda^*_j = v_j^* - P_j x^*$ to solutions  $(x^{*}, \lambda_{j}^*)$ that solve the following equations 
\begin{equation}
\label{eq: pmace_equilibrium_eq_data_fidelity}
    F_j(P_j x^* + \lambda_{j}^*) = P_j x^*
\end{equation}
\begin{equation}
\label{eq: pmace_equilibrium_condition}
     \sum_{j} P_{j}^{T} \lambda^{*}_{j} = 0 \ .
\end{equation}
These conditions are equivalent to the first-order optimality conditions for the constrained optimization problem 
\begin{equation} \label{eq:x-split}
x^* = \arg\min_{x, u_j}\left \{\sum_{j=0}^{J-1}f_{j}(u_j) \right \} \ \text{s.t.} \ u_j = P_j x \text{ for all } j \ ,
\end{equation}
which is equivalent to \eqref{eq: ml_est}. 

2. As noted above, the conservative property is equivalent to the symmetry of the Hessian matrix in \eqref{eq:grad-vec-field}, which is equivalent to 
\begin{equation} \label{eq:Hessian}
\sum_j P_j^T(W H_j(x) - H_j(x) W) P_j = 0 \ ,
\end{equation}
for all $x$, where $H_j(x)$ is $H f_j$ evaluated at $P_j x$.  After an arbitrarily small perturbation of the diagonal elements of $W$, we may assume that these diagonal entries are all distinct and positive. Then, if the left hand side of \eqref{eq:Hessian} is not identically 0, we are done.  Otherwise, by an $\epsilon$-small perturbation of $f_0$ (in the sup-norm on a ball large enough to include all images of interest), we can replace $H_0$ in \eqref{eq:Hessian} by $H_0 + \epsilon H$, with at least two off-diagonal entries of $H$ nonzero.  The assumption that $P_j P_j^T = I$ implies that $P_0^T$ is injective, so with $H_0 + \epsilon H$ in place of $H_0$, it is sufficient to show that $WH \neq HW$.   Since $W$ is diagonal with distinct, positive diagonal entries and $H$ is symmetric, they commute if and only if $H$ is also diagonal, which contradicts the choice of $H$.  Hence after an arbitrarily small perturbation of $W$ and/or $f_0$, there is no potential function associated with the vector field \eqref{eq:vec-field}.  
\end{proof}

\extraproof{
{\bf More detailed proof of 1:}
\begin{proof}
1. Define $x^*$ as in \eqref{eq:x-split}, which is equivalent to \eqref{eq: ml_est}.  The Lagrangian for this constrained optimization is 
\begin{equation}
    L(x,u) = \sum_j(\sigma^2 f_j(u_j) + (P_j x - u_j)^T \lambda_j).
\end{equation}
In this setting, the minimization is equivalent to solving the KKT conditions
\begin{align}
    \sum_j P_j^T \lambda_j^* &= 0 \label{eq:KKT1} \\
    \sigma^2 \nabla f_j(u_j^*) &= \lambda_j^*, \ \forall j\label{eq:KKT2} \\
    u_j^* &= P_j x^*, \ \forall j.\label{eq:KKT3} 
\end{align}
\eqref{eq:KKT2} is equivalent to 
$$
    (I + \sigma^2 \nabla f_j)(u_j^*) = u_j^* + \lambda_j^*.  
$$
The inverse of the operator on the left is the proximal map $F_j$, so this is also equivalent to
$$
u_j^* = F_j(u_j^* + \lambda_j^*).
$$
Since $u_j^* = P_j x^*$ by \eqref{eq:KKT3}, we have another equivalence of \eqref{eq:KKT2} and 
\begin{equation} \label{eq:KKT2a}
  P_j x^* = F_j(P_j x^* + \lambda_j^*),
\end{equation}
Define 
\begin{equation} \label{eq:vj-star}
  v_j^* = P_j x^* + \lambda_j^*,
\end{equation}
in which case \eqref{eq:KKT2a} is equivalent to
\begin{equation} \label{eq:Fj-vj-star}
F_j(v_j^*) = v_j^* - \lambda_j^*.
\end{equation}
By \eqref{eq:vj-star} and \eqref{eq:KKT1}, 
\begin{align}
\sum_j P_j^T v_j^* &= \sum_j P_j^T P_j x^* + \sum_j P_j^T \lambda_j^*\\
&= \Lambda x^*.  
\end{align}
Hence
\begin{equation} \label{eq:x-star}
x^* = \Lambda^{-1} \sum_j P_j^T v_j^* = \bar{x}(\bv^*),  
\end{equation}
where $\bv^*$ is obtained by concatenating the $v_j^*$.  
This plus \eqref{eq:vj-star} and \eqref{eq:Fj-vj-star} imply that 
\begin{equation} \label{eq:Fj-vj-star2}
F_j(v_j^*) = P_j \bar{x}(\bv^*),
\end{equation}
so $\bF(\bv^*) = \bG^P(\bv^*)$, and hence this $\bv^*$ is a solution of the PMACE equation.  \\[12pt]
Conversely, if $\bF(\bv^*) = \bG^P(\bv^*)$, then \eqref{eq:Fj-vj-star2} holds, and we define $x^*$ as in \eqref{eq:x-star}.  
Then define
$$
\lambda_j^* = v_j^* - F_j(v_j^*) = v_j^* - P_j x^*
$$
to obtain \eqref{eq:vj-star} and \eqref{eq:Fj-vj-star},
and define $u_j^* = P_j x^*$ as in \eqref{eq:KKT3}.  Finally, multiplying \eqref{eq:x-star} by $\Lambda$ gives 
\begin{align*}
    \sum_j P_j^T \lambda_j^* &= \sum_j P_j^T v_j^* - \sum_j P_j^T P_j x^* \\
    &= \Lambda \bar{x} (\bv^*) - \Lambda x^*
    &=0.
\end{align*}
Hence the KKT conditions all hold, so $x^*$ solves the optimization problem.  
\end{proof}
}

\extraproof{
{\bf Equivalent conditions for general constrained optimization problem: }\\[12pt]
Let $A$ and $B$ be matrices of appropriate size with $B$ invertible, and define
\begin{equation} \label{eq:optim-AB}
    x^* = \argmin_{x, u_j} \sum_j \sigma^2 f_j(A u_j) \text{ s.t. } u_j = B P_j x \ \forall j.
\end{equation}
The Lagrangian is
\begin{equation}
    L = \sum_j (\sigma^2 f_j(A u_j) + (B P_j x - u_j)^T \lambda_j),
\end{equation}
with equivalent KKT conditions
\begin{align}
    \sum_j P_j^T B^T \lambda_j^* &= 0 \label{eq:KKT1AB} \\
    \sigma^2 A^T \nabla f_j(A u_j^*) &= \lambda_j^*, \ \forall j\label{eq:KKT2AB} \\
    u_j^* &= B P_j x^*, \ \forall j.\label{eq:KKT3AB} 
\end{align}
\eqref{eq:KKT2AB} implies 
\begin{equation}
    (I + \sigma^2 \nabla(f_j \circ A))(u_j^*) = u_j^* + \lambda_j^*,
\end{equation}
or
\begin{equation}
    u_j^* = F_j^A(u_j^* + \lambda_j^*),
\end{equation}
where $F_j^A$ is the proximal map for $f_j(A u_j)$.  From this and \eqref{eq:KKT3AB} we have
\begin{equation}
    B P_j x^* = F_j^A(B P_j x^* + \lambda_j^*),
\end{equation}
so defining $v_j^* = B P_j x^* + \lambda_j^*$, we have
\begin{equation} \label{eq:BPj}
    F_j^A(v_j^*) = v_j^* - \lambda_j^* = B P_j x^*.
\end{equation}
Also, from \eqref{eq:KKT1AB}, we have 
\begin{align}
    \sum_j P_j^T B^T v_j^* &= \sum_j P_j^T B^T B P_j x^* + \sum_j P_j^T B^T \lambda_j^*\\
    &= \Lambda_B x^*,
\end{align}
where $\Lambda_B = \sum_j P_j^T B^T B P_j$.  Hence
\begin{equation} \label{eq:x-star-B}
    x^* = \Lambda_B^{-1} \sum_j P_j^T B^T v_j^*.
\end{equation}
Combining \eqref{eq:BPj} and \eqref{eq:x-star-B}, we see that \begin{equation}
    F_j^A(v_j^*) = B P_j \Lambda_B^{-1} \sum_k P_k^T B^T v_k^*.  
\end{equation}
With a reversal of steps as in the previous proof, this implies that the optimization in \eqref{eq:optim-AB} is equivalent to the MACE equation using agents $F_j^A$ and averaging operator defined by
\begin{equation}
    G^{BP}_j(\bv) = B P_j \Lambda_B^{-1} \sum_k P_k^T B^T v_k^*.
\end{equation}
Significant differences between this formulation and PMACE as described above are that $\Lambda_B$ includes weighting by $B^T B$ and that the variables $v_j$ are in the weighted projected space $B P_j x + \lambda_j$ as opposed to the unweighted projected space $P_j x$ as in PMACE.  
}


\subsection{Proof of Theorem~\ref{thm: pmace_sol_equal_v_goes_zeros}}
\extraproof{
\begin{theorem} 
% \label{thm: pmace_sol_equal_v_goes_zeros}
The solution of~\eqref{eq:pmace-eqns} is equivalent to solving the inclusion problem $0 \in V(x^*)$, where $V$ is the set-valued vector field 
\begin{equation}  %\label{eq:vector-field-V}
   V(x) = \sum_{j=0}^{J-1} P_j^T W \left( P_j x  - F_j^{-1}(P_j x)\right) \ . 
\end{equation}
\end{theorem}
}
\begin{proof}
Let $\bv^*$ denote the solution of~\eqref{eq:pmace-eqns} and $x^* = \bar{x}(\bv^*)$. Since $\bG^P(\bx^*) = \bG^P( \bG^P(\bx^*) )$, $\bv^*$ satisfies \eqref{eq:pmace-eqns} if and only if $F_j(v_j^*) = P_j x^*$ for all $j$ and
\begin{equation}
    \bG^P(\bv^*) = \bG^P( \bF(\bv^*) ) \ .
\end{equation}
This is equivalent to $v_j \in F_j^{-1}(P_j x^*)$ for all $j$ and 
\begin{equation}
    P_{i} \Lambda^{-1} \sum_{j=0}^{J-1} P_{j}^{T} W v_{j}^{*} = P_{i} \Lambda^{-1} \sum_{j=0}^{J-1} P_{j}^{T} W F_{j} (v_j^{*}) \ ,
\end{equation}
 for all $i$.  
Since the matrix obtained by stacking the $P_i$ is injective, this is equivalent to $v_j \in F_j^{-1}(P_j x^*)$ for all $j$ and 
\begin{equation}
    \sum_{j=0}^{J-1} P_j^T W \left( v_j^*  - F_j(v_j^*)\right) = 0 \ .
\end{equation}
Replacing both $v_j^*$ with $F_j^{-1}(P_j x^*)$ gives the theorem.  
\end{proof}
}

\subsection{Proof that the PMACE forward operator is invertible}
\begin{proof}
Recall that the PMACE forward operator from \eqref{eq:Fjv} is
\begin{align}
    F_{j} (v_{j})
        & = (1 - \alpha ) v_{j} + \alpha D^{-1}\mathcal{F}^{*} \left ( y_{j} \frac{\mathcal{F} D v_{j}}{|\mathcal{F} D v_{j}|} \right ) \ .
\end{align}
We first multiply both sides of \eqref{eq:Fjv} by $\mathcal{F} D$ to get
\begin{equation} \label{eq:FDFj}
\mathcal{F} D F_j(v_j) = \left ( \left ( 1-\alpha \right ) |\mathcal{F} D v_{j}| + \alpha y_j \right ) \frac{\mathcal{F} D v_{j}}{|\mathcal{F} D v_{j}|} \ .
\end{equation}
Taking absolute value of both sides and using $y_j \geq 0$ allows us to solve for $|\mathcal{F} D v_{j}|$ in terms of $F_j(v_j)$, $y_j$, and $\alpha$. 
This leaves $\mathcal{F} D v_{j}$ as the only unknown in \eqref{eq:FDFj}, so we solve for that and then multiply by $D^{-1} \mathcal{F}^*$ to get $v_j$.
\end{proof}
}

% \subsection{Proof of Theorem~\ref{thm: equiv_sol_to_ce_eqns}}

% \extraproof{
% \begin{theorem}
% %\label{thm: equiv_sol_to_ce_eqns}
% The solutions for the three problems  
% \begin{equation}% \label{eq: fHv_equal_Gv}
%     \bF_H^\pa (\bv^*) = \bG_H^\pa (\bv^*) \ ,
% \end{equation}
% \begin{equation}% \label{eq: fixed_pt_of_T_map_parallel_H}
%     (2\bG_H^\pa - \bI)(2 \bF_H^\pa - \bI) (\bv^*) = \bv^* \ ,
% \end{equation}
% \begin{equation}% \label{eq: fixed_pt_of_T_map_serial_H}
%     (2\bG_H^\se -\bI)(2 \bF - \bI)(\bw^*) = \bw^* \ ,
% \end{equation}
% are equivalent, where $x^* = \bar{x}_H(\bv^*) = H(\bar{x}( (2\bF - I)(\bw^*) ))$ and $w_j^* = v_j^*$ for $j=0, \ldots, J-1$.
% \end{theorem}
% }

% \begin{proof}
% As in \eqref{eq:GG=G} and \eqref{eq:2G-I-inv}, a calculation shows that $\bG_H^\pa \circ \bG_H^\pa = \bG_H^\pa$, hence $(2 \bG_H^\pa - \bI)=(2 \bG_H^\pa - \bI)^{-1}$. 

% To show that \eqref{eq: fHv_equal_Gv} and \eqref{eq: fixed_pt_of_T_map_parallel_H} have the same set of solutions, assume $\bv^*$ is a fixed point of~\eqref{eq: fHv_equal_Gv}, and rewrite~\eqref{eq: fHv_equal_Gv} as 
% \begin{equation}
%      (2 \bF_H^\pa - \bI) \bv^* = (2 \bG_H^\pa - \bI) \bv^* \ .
% \end{equation}
% Multiplying both sides by $(2 \bG_H^\pa - \bI)$ gives \eqref{eq: fixed_pt_of_T_map_parallel_H}. 
% Hence any solution to~\eqref{eq: fHv_equal_Gv} is also a solution to ~\eqref{eq: fixed_pt_of_T_map_parallel_H}, and vice versa, where the final reconstruction is obtained by $x^* = \bar{x}_H(\bv^*)$. 

% To show that the solutions to the problems \eqref{eq: fHv_equal_Gv} and \eqref{eq: fixed_pt_of_T_map_serial_H} are equivalent,  
% first suppose $\bv^*$ is a solution of \eqref{eq: fHv_equal_Gv} and let $x^* = \bar{x}_H(\bv^*)$.  
% Also, for $j=0, \ldots, J-1$, let $u_j^* = v_j^* - P_j x^*$, and let $\alpha^* = v_J^* - P_J x^*$ (so $v_j^* = P_j x^* + u_j^*$ and $v_J^* = x^* + \alpha^*$ since $P_J = I$).  
% The definitions of $\bar{x}$ and $\bar{x}_H$ give 
% \begin{align}
%     x^* &= \frac{1}{2} \left( x^* + \alpha^* + \Lambda^{-1} \sum_{j=0}^{J-1} P_j^T W(P_j x^* + u_j^*) \right)\\
%     &= x^* + \frac{1}{2} \left( \alpha^* + \Lambda^{-1} \sum_{j=0}^{J-1} P_j^T W u_j^* \right) \ ,
% \end{align}
% hence the final sum in parentheses is 0.  Since these steps are reversible, the definitions of $F_H^\pa$ and $G_H^\pa$ imply that \eqref{eq: fHv_equal_Gv} is equivalent to solving the following equations for $x^*$, $\alpha^*$, and $u_j^*$:  
% \begin{align}
% \label{eq: equilibrium_condition_F}
%     F_j(P_j x^* + u_{j}^*) &= P_j x^* \ ,\\
% \label{eq: equilibrium_condition_reg}
%     H(x^* + \alpha^*) &= x^* \ ,\\
% \label{eq: equilibrium_condition_sum}
%      \Lambda^{-1} \sum_{j=0}^{J-1} P_{j}^T W u^{*}_{j} + \alpha^* &= 0 \ .
% \end{align}

% Given such a solution, define $w_j^* = v_j^* = P_j x^* + u_j^*$ for $j = 0, \ldots, J-1$.  
% Then \eqref{eq: equilibrium_condition_F} implies
% \begin{align}  
%     \bF(\bw^*) &= [P_0 x^*, \ldots, P_{J-1} x^*] \ , \label{eq:Fw-star} \\
%     (2\bF - I)(\bw^*) &= [P_0 x^* - u_0^*, \ldots, P_{J-1} x^*-u_{J-1}^*] \ ,  \label{eq:2F-I}
% \end{align}
% so
% \begin{align}
%     \bar{x}( (2\bF - I)(\bw^*) ) 
%     &= \Lambda^{-1} \sum_{j=0}^{J-1} P_j^T W (P_j x^* - u_j^*)  \\
%     &= x^* + \alpha^* \ ,
% \end{align}
% where the final equality follows from the definition of $\Lambda$ and \eqref{eq: equilibrium_condition_sum}.
% This plus~\eqref{eq: equilibrium_condition_reg} implies $H(\bar{x}( (2\bF - I)(\bw^*) )) = x^*$, which implies
% \begin{align} \label{eq:G_2F-I}
%     \bG_H^\se( (2\bF - I)(\bw^*) )&= [P_0 x^*, \ldots, P_{J-1} x^*] \ .
% \end{align}
% Twice \eqref{eq:G_2F-I} minus \eqref{eq:2F-I} gives $\bw^*$, which implies \eqref{eq: fixed_pt_of_T_map_serial_H}.


% \bigskip

% For the converse, assume \eqref{eq: fixed_pt_of_T_map_serial_H}.  Adding $(2\bF-I)(\bw^*)$ to both sides gives
% \begin{align}  \label{eq:GHserF}
%     2 \bG_H^\se(2\bF-I)(\bw^*) = \bw^* + (2 \bF-I)(\bw^*) = 2 \bF(\bw^*) \ .
% \end{align}
% Using \eqref{eq:GHserF} with the definition of $\bG_H^\se$ and defining $x^* = H( \bar{x}((2\bF-I)(\bw^*)))$, we have
% \begin{align}  \label{eq:FjPjxstar}
%     F_j(w_j^*) = P_j H( \bar{x}((2\bF-I)(\bw^*))) = P_j x^* \ .
% \end{align}
% %so \eqref{eq:Fw-star} holds.  
% Define $u_j^*$ by \eqref{eq:2F-I}, so that 
% \begin{align}  \label{eq:FjPjuj}
%     2F_j(w_j^*) - w_j^* = P_j x^* - u_j^* \ ,
% \end{align}
% which combined with \eqref{eq:FjPjxstar} gives $w_j^* = P_j x^* + u_j^*$ and hence \eqref{eq: equilibrium_condition_F}.  
% Define $\alpha^*$ so that $x^* + \alpha^* = \bar{x}((2\bF-I)(\bw^*))$, in which case $H(x^* + \alpha^*) = x^*$ by choice of $x^*$, so \eqref{eq: equilibrium_condition_reg} holds.  Also, by definition of $\bar{x}$ and \eqref{eq:FjPjuj},
% \begin{align}
%     x^* + \alpha^* &= \Lambda^{-1} \sum_j P_j^T W (P_j x^* - u_j^*)  \\
%     &= x^* - \Lambda^{-1} \sum_j P_j^T W u_j^* \ ,
% \end{align}
% which implies \eqref{eq: equilibrium_condition_sum}.  

% Since \eqref{eq: equilibrium_condition_F}--\eqref{eq: equilibrium_condition_sum} are equivalent to \eqref{eq: fHv_equal_Gv}, 
%  the solutions for the three problems~\eqref{eq: fHv_equal_Gv}--\eqref{eq: fixed_pt_of_T_map_serial_H} are equivalent.
% \end{proof}
% }


% references section
\bibliographystyle{IEEEtran}
% \bibliography{references,refs}
\bibliography{pmace_ptycho}

\newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill

\end{document}


