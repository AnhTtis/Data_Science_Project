\vspace{-1mm}
\section{Background}
\vspace{-1mm}
{\bf Transformer-based Models. } 
A typical transformer model consists of $ L $ stacked blocks, where each block contains two submodules: a multi-head attention (MHA) and a fully connected FFN. Given the input sequence $ \X \in \R^{n\times d} $, MHA performs the attention function in parallel $ h $ heads: 
\begin{align*}
	\text{MHA}\left( \X \right) = \text{Concat}(\text{head}_1,..., \text{head}_h)\Wo, \quad \text{head}_i = \text{Softmax}\left( {\X \Wqi (\X\Wki)^{\top} } /{\sqrt{d_h}} \right) \X\Wvi ,
\end{align*}
where $ \Wo\in\R^{d\times d} $ is an output projection and $ \Wqi,\Wki,\Wvi \in\R^{d\times d_h} $ are query, key and value projections of head $ i $.  $ d_h $ is typically set to $ d/h $. The other important module is a FFN which consists of two linear transformations with a ReLU activation in between: $\text{FFN}(\X) = \text{ReLU}(\X\Wfp + \bb_1)\Wfq + \bb_2$,  where $ \Wfp \in\R^{d\times d_m} $ and $ \Wfq \in \R^{d_m \times d} $. Finally, a residual connection is used followed by a layer normalization \citep{ba2016layer}. 


{\bf Low Rank Adaptation. }
LoRA \citep{hu2022lora} models the incremental update of the pre-trained weights by the product of two small matrices. For $ \bh=\Wpre\bx $, the modified forward pass is: 
\begin{align}\label{eq:forward_lora}
\bh = \Wpre\bx + \DeltaW\bx = \Wpre \bx + \B\A \bx,  
\end{align}
where  $ \Wpre,\DeltaW \in\R^{d_1\times d_2} $, $ \A\in\R^{r \times d_2} $ and $ \B\in \R^{d_1\times r} $ with $r\ll \{ d_1, d_2 \} $.  $ \A $ typically adopts a random Gaussion initialization while $ \B $ is initialized with zero to have $ \DeltaW = 0 $ at the beginning of training. 
We further denote $ \A_{i*} $ as the $ i $-th row of $ \A $, $ \B_{* i} $ as the $ i $-th column of $ \B $, and $ \Gcal_i = \{ \A_{i * }, \B_{* i} \} $ as the $ i $-th doublet.  \citet{hu2022lora} only apply LoRA to query and value projections (i.e,~$ \Wq $ and $ \Wv $) in the MHAs. \citet{he2022towards} extend it to weight matrices of FFNs (i.e,~$ \Wfp $ and $ \Wfq $), leading to the performance improvement . Meanwhile, they propose a unified view of various efficient tuning methods including adapter tuning, prefix tuning and LoRA. 









