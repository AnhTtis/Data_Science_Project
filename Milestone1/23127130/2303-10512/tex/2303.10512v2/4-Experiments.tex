%\vspace{-1mm}
\section{Experiments}\label{sec:experiments}
%\vspace{-1mm}
We implement {\ouralg} for fine-tuning  DeBERTaV3-base \citep{he2021debertav3} and BART-large \citep{lewis2019bart}. We evaluate the effectiveness of the proposed algorithm on natural language understanding (GLUE, \citet{wang2018glue}), question answering (SQuADv1, \citet{squad1} and SQuADv2, \citet{squad2}), and natural language generation (XSum, \citet{narayan2018don} and CNN/DailyMail \citet{hermann2015teaching}). 
All the gains have passed significant tests with $ p<0.05 $. 

{\bf Implementation Details. } We use \textit{PyTorch} \citep{paszke2019pytorch} to implement all the algorithms. Our implementation is based on the publicly available \textit{Huggingface Transformers}\footnote{\url{https://github.com/huggingface/transformers}} \citep{wolf2019huggingface} code-base. All the experiments are conducted on NVIDIA V100 GPUs. 

LoRA scales $ \DeltaW\bx $ by $ \alpha/r $ where $ 
\alpha $ is a constant in $ r $. As a result, the magnitude of output can be consistent given different $ r $. It reduces the efforts of retuning learning rate when varying $ r $. Typically $ \alpha $ is set as $ 16 $ or $ 32 $ and never tuned \citep{hu2022lora,yang2020feature}. Following LoRA, we add the same scaling for (\ref{eq:svd_adaptation}) and fix $ \alpha $ as LoRA. 
Besides, in Algorithm~\ref{alg:our_algorithm}, we prune singular values every $ \DeltaT $ steps (e.g., $ \DeltaT = 100 $) such that the pruned triplets can still get updated within these intervals and possibly reactivated in future iterations. 


{\bf Baselines.} We compare {\ouralg} with the following methods: 

$ \bullet $
\textit{Full fine-tuning} is the most common approach for adaptation. During fine-tuning, the model is initialized with pre-trained weights and biases, and all model parameters undergo gradient updates. 

$\bullet$
\textit{Bitfit} \citep{zaken2021bitfit} is an effective parameter-efficient fine-tuning method. The method only fine-tunes bias vectors in the pre-trained model. 

$ \bullet $
\textit{Adapter tuning} \citep{houlsby2019parameter,pfeiffer2020adapterfusion} inserts two-layer adapters between transformer blocks. We compare with two types of adapter. \textit{Houlsby adapter} as proposed in \citet{houlsby2019parameter} is inserted between the self-attention module and the FFN module followed by a subsequent residual connection. Recently, \citet{pfeiffer2020adapterfusion} propose a more efficient design with adapters only applied after FFN modules and LayerNorm modules \citep{ba2016layer}, which we call \textit{Pfeiffer adapter}. The number of trainable parameters is determined by the number of layers, the hidden dimension of adapters and the dimension of their inputs.

$ \bullet $
\textit{LoRA} \citep{hu2022lora} is a state-of-the-art method for parameter-efficient fine-tuning. The method parameterizes incremental updates by two small matrices and only fine-tune them. The number of trainable parameter is controlled by the rank $ r $ and the number of adapted weight matrices $ n $. 
\citet{hu2022lora} apply LoRA to query and value projections only. In empirical, we find that applying LoRA to all weight matrices, i.e., $ \Wq, \Wk, \Wv, \Wfp \text{ and } \Wfq $, can further improve its performance (Please see Appendix~\ref{app:lora_ablation}). Hence, we compare with this generalized LoRA to maximize its performance. We use publicly available implementation \footnote{\url{https://github.com/microsoft/LoRA}} to run all the baselines. Please refer to \citet{hu2022lora} and reference therein for details. 


{\setlength{\tabcolsep}{0.35em}
\renewcommand{\arraystretch}{1.1}
\begin{table}[t!]
\vspace{-8mm}
\caption{\small Results with DeBERTaV3-base on GLUE development set. The best results on each dataset are shown in \textbf{bold}. 
We report the average correlation for STS-B. \textit{Full FT}, \textit{HAdapter} and \textit{PAdapter} represent full fine-tuning, Houlsby adapter, and Pfeiffer adapter respectively. 
We report mean of $5$ runs using different random seeds.
}
\vspace{-3mm}
\label{tab:glue_datasets}
\begin{center}
\begin{small}
\begin{tabular}{l|c|ccccccccc}
\toprule
\multirow{2}*{\bf Method} & \multirow{2}*{\bf \small \# Params} & {\bf MNLI} & {\bf SST-2} & {\bf CoLA} & {\bf QQP } & {\bf QNLI} & {\bf RTE}  & {\bf MRPC}  & {\bf STS-B} & {\bf All} \\
~ & ~ & {m/mm} & {Acc} & {Mcc} & {Acc/F1} & {Acc} & {Acc} & {Acc} & { Corr } & {Ave.} \\
\midrule 
{Full FT} & {184M} & {89.90/90.12} & {95.63} & {69.19} & {\bf92.40/89.80} & {94.03} & {83.75} & {89.46} & {91.60} & {88.09}
\\
\midrule
{BitFit} & {0.1M} & {89.37/89.91} & {94.84} & {66.96} & {88.41/84.95} & {92.24} & {78.70} & {87.75} & {91.35} & {86.02}  
\\
\midrule 
{\small HAdapter} & {1.22M} & {90.13/90.17} & {95.53} & {68.64} & {91.91/89.27} &  {94.11} & {84.48} & {89.95} & {91.48} & {88.12} 
\\
{\small PAdapter} & {1.18M} & {90.33/90.39} & {95.61} & {68.77} & {92.04/89.40} & {94.29} & {85.20} & {89.46} & {91.54} & {88.24}  
\\
{$\text{LoRA}_{r=8}$} & {1.33M} & {90.65/90.69} & {94.95} & {69.82} & {91.99/89.38} & {93.87} & {85.20} & {89.95} & {91.60} & {88.34} 
\\
{\ouralg} & {1.27M} & {\bf 90.76/90.79} & {\bf96.10} & {\bf71.45} & {\bf92.23/89.74} & {\bf94.55} & {\bf88.09} & {\bf90.69} & {\bf91.84} & {\bf 89.31} 
\\
\midrule 
{\small HAdapter} & {0.61M} & {90.12/90.23} & {95.30} & {67.87} & {91.65/88.95} &  {93.76} & {85.56} & {89.22} & {91.30} & {87.93} 
\\
{\small PAdapter} & {0.60M} & {90.15/90.28} & {95.53} & {69.48} & {91.62/88.86} &  {93.98} & {84.12} & {89.22} & {91.52} & {88.04} 
\\
{\small HAdapter} & {0.31M} & {90.10/90.02} & {95.41} & {67.65} & {91.54/88.81} &  {93.52} & {83.39} & {89.25} & {91.31} & {87.60} 
\\
{\small PAdapter} & {0.30M} & {89.89/90.06} & {94.72} & {69.06} & {91.40/88.62} &  {93.87} & {84.48} & {89.71} & {91.38} & {87.90} 
\\
{$\text{LoRA}_{r=2}$} & {0.33M} & {90.30/90.38} & {94.95} & {68.71} & {91.61/88.91} & {94.03} & {85.56} & {89.71} & {\bf91.68} & {88.15} 
\\
{\ouralg} & {0.32M} & {\bf 90.66/90.70} & {\bf95.80} & {\bf70.04} & {\bf91.78/89.16} & {\bf94.49} & {\bf87.36} & {\bf90.44} & {91.63} & {\bf 88.86} 
\\

\bottomrule
\end{tabular}
\end{small}
\end{center}
% \vspace{-1mm}
\end{table}
}


\subsection{Natural Language Understanding}\label{sec:NLU_expriments}
\vspace{-1mm}
{\bf Models and Datasets.} We evaluate the fine-tuning performance of DeBERTaV3-base \citep{he2021debertav3} using the proposed algorithm. 
We conduct experiments on the General Language Understanding Evaluation (GLUE, \citealt{wang2018glue}) benchmark. The benchmark includes two single-sentence classification tasks,  three similarity and paraphrase tasks and four natural language inference tasks. Dataset details are summarized in Appendix~\ref{app:glue_datesets}. 


{\bf Implementation Details.} 
DeBERTaV3-base consists of 183 millions parameters. We compare {\ouralg} with the baselines under different budget levels, for example, given the total trainable parameters as 0.3/0.6/1.2 million.  In order to match the parameter budget, we select the hidden dimensions of adapters from $ \{ 8, 16, 32, 64 \} $, set the rank $ r $ of LoRA as $ \{2, 4, 8\} $, and choose the final budget $ \BuT $ of {\ouralg} from $ \{144, 288, 576\} $.  Then we set $\Buinit$ as 1.5 times of $\BuT$ for {\ouralg} and select the regularization coefficient $ \gamma $ from $ \{ 0.1, 0.3, 0.5  \} $.  We set the exponential moving average parameters $ \beta_1 $ and $ \beta_2 $ as their default value $ 0.85 $. We select the learning rate from $\{5\times 10^{-5}, 8\times 10^{-5}, 1\times 10^{-4}, 2\times 10^{-4}  \}$.  More details are presented in Appendix~\ref{app:NLU}. 

{\bf Main results.} 
We compare {\ouralg} with the baseline methods under different budget settings. Table~\ref{tab:glue_datasets} shows experimental results on the GLUE development set. We see that {\ouralg} achieves better or on par performance compared with existing approaches on all datasets under all budget levels. For example, when the parameter budget is 0.3M, {\ouralg} achieves 87.36\% accuracy on RTE, which is 1.8\% higher than the best-performing baseline. Besides, {\ouralg} with extreme low budget can often perform better than the baselines with higher budget. For example, {\ouralg} achieve 70.04\% Mcc.~score on CoLA with 0.3M fine-tuning parameters, which is higher than all baseline methods with lager budget (e.g., 0.6M and 1.2M). 


\vspace{-2mm}
\subsection{Question Answering}\label{sec:QA_experiments}
\vspace{-1mm}
{\bf Models and Datasets.}
We evaluate performance of the proposed algorithm on two question answering (QA) datasets: SQuAD v1.1 \citep{squad1} and SQuADv2.0 \citep{squad2}, where we use {\ouralg} to fine-tune DeBERTaV3-base.  These tasks are treated as a sequence labeling problem, where we predict the probability of each token being the start and end of the answer span. Dataset details can be found in Appendix~\ref{app:question_answering}. 


{\bf Implementation Details.}  
We compare {\ouralg} with the baseline methods under different parameter budgets. That is we have the number of trainable parameters as $ 0.08\% / 0.16\% / 0.32\% / 0.65\% $ of total pre-trained parameters.  To match the budget requirements, we select the hidden dimensions of adapters from $ \{4, 8, 16, 32, 64 \} $, set the rank $ r $ of LoRA as $ \{1, 2, 4, 8\} $ and choose the final total rank $ \BuT $ of {\ouralg} from $ \{72, 144, 288, 576\} $.  We set the batch size as $16$.  We use AdamW \citep{loshchilov2017decoupled} as the optimizer and we set the learning rate as $ 1\times 10^{-3} $ for {\ouralg}. Please refer to Appendix~\ref{app:question_answering} for more details.  


{\bf Main Results. } 
Table~\ref{tab:squad_experiemnts} summarizes experimental results when we fine-tune DeBERTaV3-base under 4 different budget settings: 0.08\%, 0.16\%, 0.32\% and 0.65\% of total pre-trained parameters. From the result, we see that {\ouralg} consistently outperforms existing approaches under all the budget levels in term of two evaluation metrics: exact match (EM) and F1. Notice that the performance of Houlsby adapter and Pfeiffer adapter are notably decreased when we reduce the parameter budget. In contrast, our method shows the consistent performance under different budget levels. For example, {\ouralg} achieves 88.7\% F1 on SQuADv2.0 with the smallest budget 0.08\%. It is close to its performance under the high budget and it is also 1.2\% higher than the best-performing baseline. 


{\setlength{\tabcolsep}{0.32em}
\renewcommand{\arraystretch}{1.1}
\begin{table*}[t!]
\vspace{-3mm}
\caption{Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. Here \textit{\# Params} is the number of trainable parameters relative to that in full fine-tuning.  We report EM/F1. The best results in each setting are shown in \textbf{bold}.}
\vspace{-3mm}
\label{tab:squad_experiemnts}
\begin{center}
\begin{tabular}{l|cccc|cccc}
\toprule
 & \multicolumn{4}{|c}{\bf SQuADv1.1} & \multicolumn{4}{|c}{\bf SQuADv2.0}
\\
\midrule 
{\small Full FT} & \multicolumn{4}{|c}{86.0 / 92.7} & \multicolumn{4}{|c}{85.4 / 88.4}
\\ 
\midrule
{\small \# Params} & {0.08\%} & {0.16\%} & {0.32\%} & {0.65\%} & {0.08\%} & {0.16\%} & {0.32\%} & {0.65\%}
\\
\midrule
{\small HAdapter} & {84.4/91.5} & {85.3/92.1} & {86.1/92.7} & {86.7/92.9} & {83.4/86.6} & {84.3/87.3} & {84.9/87.9} & {85.4/88.3}
\\
{\small PAdapter} & {84.4/91.7} & {85.9/92.5} & {86.2/92.8} & {86.6/93.0} & {84.2/87.2} & {84.5/87.6} & {84.9/87.8} & {84.5/87.5}
\\
{\small LoRA} & {86.4/92.8} & {86.6/92.9} & {86.7/93.1} & {86.7/93.1} & {84.7/87.5} & {83.6/86.7} & {84.5/87.4} & {85.0/88.0}
\\
\midrule
{\small \ouralg} & {\bf 87.2/93.4} & {\bf 87.5/93.6} & {\bf 87.5/93.7} & {\bf 87.6/93.7} & {\bf85.6/88.7} & {\bf85.7/88.8} & {\bf85.5/88.6} & {\bf86.0/88.9}
\\
\bottomrule
\end{tabular}
\end{center}
% \vspace{-2mm}
\end{table*}
}
	

\vspace{-1mm}
\subsection{Natural Language Generation}\label{sec:NLG_experiments}
% \vspace{-1mm}
\begin{table*}[htb!]
% \vspace{-3mm}
\caption{\small Results with BART-large on XSum and CNN/DailyMail. Here \textit{\# Params} is the number of trainable parameters relative to that in full fine-tuning.  We report R-1/2/L. The best results are shown in \textbf{bold}.}
\vspace{-3mm}
\label{tab:summarization}
\begin{center}
\begin{tabular}{l|c|c|c}
\toprule
 {\bf \# Params} & {\bf Method} & {\bf XSum} & {\bf CNN/DailyMail}
\\
\midrule 
{\bf100\%} & {Full FT} & {\bf 45.49 / 22.33 / 37.26} & { 44.16 / 21.28 / 40.90}
\\ %param: 406M
\midrule
\multirow{2}*{\bf 2.20\%} & {LoRA} & {43.95 / 20.72 / 35.68} & {{\bf 45.03} / 21.84 / 42.15} 
\\
~ & {\ouralg} & {\bf 44.72 / 21.46 / 36.46} & {45.00 / {\bf 21.89} / {\bf 42.16}}
\\
\midrule
\multirow{2}*{\bf 1.10\%} & {LoRA} & {43.40 / 20.20 / 35.20} & {44.72 / 21.58 / 41.84} 
\\
~ & {\ouralg} & {\bf 44.35 / 21.13 / 36.13} & {\bf 44.96 / 21.77 / 42.09}
\\
\midrule
\multirow{2}*{\bf 0.26\%} & {LoRA} & {43.18 / 19.89 / 34.92} & {43.95 / 20.91 / 40.98}
\\
~ & {\ouralg} & {\bf 43.55 / 20.17 / 35.20} & {\bf 44.39 / 21.28 / 41.50}
\\
\midrule
\multirow{2}*{\bf 0.13\%} & {LoRA} & {42.81 / 19.68 / 34.73} & {43.68 / 20.63 / 40.71}
\\
~ & {\ouralg} & {\bf 43.29 / 19.95 / 35.04} & {\bf 43.94 / 20.83 / 40.96} 
\\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1mm} 
\end{table*}


{\bf Models and Datasets.}
To provide a comparison with the state-of-the-art in natural language generation (NLG) tasks, we apply {\ouralg} to fine-tune a BART-large model \citep{lewis2019bart}. We evaluate model performance on two datasets: XSum \citep{narayan2018don} and CNN/DailyMail \citep{hermann2015teaching}.  

{\bf Implementation Details.} 
Similarly as DeBERTav3-base, we apply low-rank/SVD-based adaptation to every weight matrix of both encoder and decoder layers. We report ROUGE 1/2/L scores (R-1/2/L, \citet{lin2004rouge}). We set the training epochs as 15. For XSum, we set the beam length as 8 and batch size as 64. For CNN/DailyMail, we set the beam length as 4 and batch size as 32. Please see Appendix~\ref{app:NLG} for the detailed configuration. 

{\bf Main Results.} 
Experimental results are summarized in Table~\ref{tab:summarization}, where we compare the fine-tuning performance under four budget levels: the number of trainable parameters is 0.13\%, 0.26\%, 1.10\% and 2.20\% of total pre-trained parameters.  We see that {\ouralg} achieves better or on par performance compared with the baseline on both datasets (XSum and CNN/DailyMail) under all the budget levels. For example, {\ouralg} achieves 21.13 R-2 score when budget level is 1.10\%, compared with 19.89 for LoRA.  







\vspace{-2mm}
\subsection{Analysis}\label{sec:exp_analysis}
\vspace{-1mm}

{\bf Different budget levels.} Figure~\ref{fig:budget_curve} illustrates experimental results of fine-tuning DeBERTaV3-base under different budget levels. We see that on all the three datasets (MNLI-m, SQuADv2.0 and XSum), {\ouralg} achieves consistent performance improvement under all the budget levels compared with the baseline. The performance gain is more significant when increasing the budget for the XSum task, suggesting a high budget can help NLG tasks.  Note that on the MNLI and SQuADv2.0 datasets, the performance of {\ouralg} under low budget levels ($ \leq 1\% $) can match the results of high budget settings. For example, {\ouralg} achieves $ 88.78\% $ F1 on SQuADv2.0 when the budget is $ 0.16\% $. It is close to the performance (88.89\% F1) of the highest budget ($ 4.65\% $) with a more significant gain over the baseline. 

\begin{figure*}[h!]
	%\vspace{-8mm}
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{6_mnli_acc_budget_curve_withxlabel_font30.pdf}
		\vspace{-7mm}
		\caption{MNLI}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{10_squadv2_f1_budget_curve_nolegend_ylabelF1_withxlabel_font32.pdf}
		\vspace{-7.5mm}
		\caption{SQuADv2.0}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{6_xsum_r2_budget_curve_withxlabel_nolegend_font32.pdf}
		\vspace{-7.5mm}
		\caption{XSum}
	\end{subfigure}
	\vspace{-3mm}
	\caption{Fine-tuning performance under different budget levels. We compare {\ouralg} with the generalized LoRA that applies to every weight matrix.}
	\label{fig:budget_curve}
	% \vspace{-1mm}
\end{figure*}


{\bf Comparison to low-rank parameterization.} As mentioned in Section~\ref{sec:svd_adaptation}, one can alternatively prune LoRA doublet-wise to conduct the rank allocation. In this case, the doublets are zeroed out entirely, raising the barrier to reactivate them. It can cause training instability and hurt the generalization when some crucial doublets are pruned by mistake. In Table~\ref{tab:frd_ipt_ablation}, we compare {\ouralg} with pruning LoRA on three datasets (SST-2, RTE, and CoLA) to illustrate this point. We apply the same importance score, budget scheduler and training setups as Section~\ref{sec:NLU_expriments} for pruning LoRA. We can see that {\ouralg} outperforms pruning LoRA on all the datasets under all the budget levels. 


\begin{table}[htb!]
% \vspace{-1mm} 
\caption{We present two ablation studies in this table: (i) Comparison between {\ouralg} and structured pruning on LoRA. (ii) Comparison of different importance metrics for {\ouralg}.} 
\vspace{-3mm}
\label{tab:frd_ipt_ablation}
\begin{center}
\begin{tabular}{c|ccc|ccc|ccc}
\toprule
& \multicolumn{3}{|c}{\bf SST-2} & \multicolumn{3}{|c}{\bf RTE} & \multicolumn{3}{|c}{\bf CoLA}  
\\
\midrule 
{\# Params}  
& {0.08\%}  & {0.16\%} & {0.65\%}  
& {0.08\%}  & {0.16\%} & {0.65\%} 
& {0.08\%}  & {0.16\%} & {0.65\%}
\\ 
\midrule
{\small Prune LoRA} 
& {94.84} & {94.50} & {94.95} 
& {86.28} & {86.15} & {87.00}
& {66.71} & {69.29} & {69.57}
\\
\midrule
{\ouralg} 
& {95.52} & {95.80} & {96.10}
& {87.36} & {87.73} & {88.09}
& {70.21} & {70.04} & {71.45}
\\
%\midrule 
{$ \scf(\cdot) = I(\cdot) $}
& {94.61} & {95.30} & {95.64} 
& {87.36} & {87.71} & {88.10} 
& {66.71} & {68.83} & {70.19} 
\\
{$ \Sci = |\lambda_{i}| $} 
& {95.41} & {95.41} & {95.87} 
& {87.00} & {86.28} & {88.00} 
& {67.67} & {68.44} & {70.38} 
\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

{\bf Variants of the importance score.} Recall that in {\ouralg}, the importance score is defined by the sensitivity and uncertainty of every entry in the triplet (\ref{eq:all_importance}). In Table~\ref{tab:frd_ipt_ablation}, we examine two variants of the importance score: (i) changing $ \scf(\cdot) $ in (\ref{eq:all_importance}) to sensitivity-only; (ii) directly defining $ \Sci $ as $ |\lambda_{i}| $. From the results, we can see that the proposed importance score generally performs best. The other two variants can degenerate the model performance up to $ 0.9\% $. 

{\bf The role of two components.} We remark that both two components of our method - SVD adaptation and adaptive budget allocation, play vital roles for the performance gain. To demonstrate it,  we compare {\ouralg} with the following variants:  (i) SVD-LoRA: fine-tuning only with the proposed SVD-based adaptation in (\ref{eq:svd_adaptation}) and (\ref{eq:regularization}); (ii) LoRA\textsubscript{regu}: LoRA with orthogonal regularization (\ref{eq:regularization}) on $\A$ and $\B$; (iii) {\ouralg}\textsubscript{$\gamma=0$}: {\ouralg} without orthogonal regularization (\ref{eq:regularization}). Table~\ref{tab:svd_ablation} present the results when fine-tuning DeBERTaVe-base on SST-2 and MNLI. We can see that fine-tuning only with SVD adaptation shows an improvement over LoRA but cannot match the performance of {\ouralg}. Meanwhile, without SVD orthogonal regularization, the performance of {\ouralg} can degenerate. These results validate that both components contribute to the model performance.


\begin{table}[htb!]
 \vspace{1mm} 
\caption{We present ablation studies about SVD-based adaptation, orthogonal regularization, and budget allocation in this table. For MNLI, we report the average score of m/mm acc.} 
\vspace{-3mm}
\label{tab:svd_ablation}
\begin{center}
\begin{tabular}{c|cccc|cccc}
\toprule
& \multicolumn{4}{|c}{\bf SST-2} & \multicolumn{4}{|c}{\bf MNLI} 
\\
\midrule 
{\# Params}  
& {0.08\%}  & {0.16\%} & {0.32\%} & {0.65\%} 
& {0.08\%}  & {0.16\%} & {0.32\%} & {0.65\%} 
\\ 
\midrule
{LoRA} 
& 94.38 & 94.95 & - & 94.95 
& 90.19 & 90.34 & - & 90.57 
\\
{LoRA\textsubscript{regu}} 
& - & 94.61 & 94.72 & 94.61
& - & 90.30 & 90.40 & 90.66 
\\
{SVD-LoRA}
& 95.33 & 95.18 & 95.07 & 95.53
& 90.28 & 90.25 & 90.52 & 90.62 
\\
\midrule
{{\ouralg}\textsubscript{$\gamma=0$}} 
& 95.41 & 95.10 & 95.30 & 95.10
& 90.37 & 90.34 & 90.56 & 90.43 
\\
{{\ouralg}} 
&95.64 & 95.80 & 96.10 & 96.10 
&90.65 & 90.68 & 90.66 & 90.77
\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

{\bf The resulting budget distribution.} 
Figure~\ref{fig:rank_pattern} shows the resulting rank of each incremental matrix of DeBERTaV3-base fine-tuned with {\ouralg}.
We find that {\ouralg} always prefers to allocating more budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented in  Figure~\ref{fig:budget_distribution} that weight matrices of FFN moduels and top layers are more important for model performance. Hence, it validates that our proposed importance metric can guide {\ouralg} to focus on crucial modules. Meanwhile, the rank distribution generated by {\ouralg} is consistent across different budget levels, tasks and models. It means the number of remaining parameters is linearly scaled with $\BuT$ and hence we can tune $\BuT$ to control the remaining parameters. 

\begin{figure}[htb!]
	\vspace{-1.5mm}
	\centering
	\includegraphics[width=0.95\linewidth]{rank_pattern_mnli_init12.pdf}
	\vspace{-3mm}
	\caption{The resulting rank of each incremental matrix when fine-tuning DeBERTaV3-base on MNLI with {\ouralg}. Here the $x$-axis is the layer index and the $y$-axis represents different types of adapted weight matrices.} 
	\label{fig:rank_pattern}
\end{figure}








