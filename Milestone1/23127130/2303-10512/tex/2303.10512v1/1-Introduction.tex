\section{Introduction}\label{sec:introduction}
% \setlength{\abovedisplayskip}{2mm}
% \setlength{\abovedisplayshortskip}{2mm}
% \setlength{\belowdisplayskip}{2mm}
% \setlength{\belowdisplayshortskip}{2mm}
% \setlength{\topsep}{0ex}
% \setlength{\partopsep}{0ex}
\setlength{\jot}{2pt}
\setlength{\floatsep}{1ex}
\setlength{\textfloatsep}{1ex}
\setlength{\intextsep}{1ex}
\setlength{\parskip}{1ex}

Pre-trained language models (PLMs) have manifested superior performance in various natural language processing tasks \citep{devlin2018bert,liu2019roberta,he2021deberta,radford2019language,brown2020language}. 
The most common way to adapt pre-trained models to down-stream tasks is to fine-tune all the parameters (full fine-tuning, \citet{qiu2020pre,raffel2020exploring}). 
However, pre-trained models typically incurs large memory footprint. For example, BERT model \citep{devlin2018bert} consists up to 300 million parameters; T5 \citep{raffel2020exploring} comprises up to 11 billion parameters and GPT-3 \citep{brown2020language} contains up to 175 billion parameters. When building a NLP system upon these pre-trained models, we usually handle multiple tasks 
that arrive simultaneously \citep{radford2019language}. 
Given a large number of down-stream tasks, full fine-tuning requires that each task maintains a separated copy of large models. The resulting memory consumption is prohibitively expensive. 


To address this issue, researchers have proposed two main lines of research to reduce the fine-tuning parameters, while maintaining or even improving the performance of PLMs. Specifically, one line of research focuses on adding small neural modules to PLMs and fine-tune only these modules for each task -- the base model is kept frozen and shared across tasks. 
In this way, only a small number of task-specific parameters are introduced and updated, greatly enhancing the practicality of large models. 
For example, adapter tuning \citep{houlsby2019parameter,rebuffi2017learning,pfeiffer2020adapterfusion,he2022towards} inserts small neural modules called adapters between the layers of the base model. Prefix tuning \citep{li2021prefix} and prompt tuning \citep{lester2021power} attach additional trainable prefix tokens to the input or hidden layers of the base model.  These methods have shown to achieve comparable performance to full fine-tuning, while only updating less than $1\%$ of the original model parameters, significantly releasing the memory consumption. 


Another line of research proposes to model the incremental update of the pre-trained weights in a parameter-efficient way, without modifying the model architecture \citep{zaken2021bitfit,guo2020parameter,hu2022lora}.  Given a pre-trained weight matrix\footnote{Unless specified otherwise, we use $ \Wpre$ to denote any pre-trained weight matrix.} $\Wpre $, for example, diff pruning \citep{guo2020parameter} models its incremental update $ \DeltaW $ as a sparse matrix. Diff pruning initializes $ \DeltaW $ as the same dimension as $ \Wpre $ and then prunes $ \DeltaW $ element-wise based on the magnitude of the entries.  As such, diff pruning can increase the parameter efficiency substantially by adaptively retaining important updates and pruning unimportant ones.  Nonetheless, diff pruning has several limitations. First, it relies on low-level implementation to speed up the computation of unstructured sparse matrices, which is not well supported by existing deep learning frameworks. Therefore, we have to store $ \DeltaW $ as a dense matrix during training.  Second, it needs to update every entry of $\DeltaW$ with their gradients and then prune them.  This results in similar computational cost as full fine-tuning \citep{guo2020parameter}. 

To overcome these drawbacks, \citet{hu2022lora} propose a method named LoRA, which parameterizes $ \DeltaW $ as a low-rank matrix by the product of two much smaller matrices:
\begin{align}\label{eq:lora}
\W = \Wpre + \DeltaW = \Wpre + \B\A , 
\end{align}
where $ \Wpre,\DeltaW \in\R^{d_1\times d_2} $, $ \A\in\R^{r \times d_2} $ and $ \B\in \R^{d_1\times r} $ with $r\ll \{ d_1, d_2 \} $. During fine-tuning,  only $ \A $ and $ \B $ are updated. The rank $ r $ is chosen to be much smaller than the dimension of $ \W $ (e.g.,~$ r=8 $ when $ d_1 = d_2 = 1024 $).  With less than $ 0.5\% $ additional trainable parameters, the training overhead can be reduced up to $70\%$, compared to full fine-tuning.  However, LoRA achieves comparable or even better performance than full fine-tuning \citep{hu2022lora}.  Meanwhile, the product of two samll matrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning.  

\begin{figure*}[t!]
\vspace{-10mm}
\centering
\begin{subfigure}{0.35\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{9_module_ablation_weight-6_noxlabel_font32.pdf}
    \vspace{-7mm}
    \caption{\small Selected weight matrix}
    \label{fig:module_budget}
\end{subfigure}
\hspace{8mm}
\begin{subfigure}{0.35\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{8_layer_ablation_noxlabel_font32.pdf}
    \vspace{-7mm}
    \caption{\small Selected layers}
    \label{fig:layer_budget}
\end{subfigure}
\vspace{-3mm}
\caption{\small 
Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left) or selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m.  Figure~\ref{fig:module_budget}: we only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value projection ($\Wq, \Wk, \Wv$), output projection ($\Wo$) in the self-attention, and two weight matrices ($\Wfp, \Wfq$) in two-layer FFNs. In Figure~\ref{fig:layer_budget}, we apply LoRA to every weight matrix of the selected layers.
}
\label{fig:budget_distribution}
% \vspace{-1mm}
\end{figure*}


LoRA still has limitations as it prespecifies the rank $ r $ of each incremental matrix $\DeltaW$ identical.  This ignores the fact that the importance of weight matrices varies significantly across modules and layers when fine-tuning pre-trained models.  To illustrate this point, we present an concrete example in Figure~\ref{fig:budget_distribution}. We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters.  Figure~\ref{fig:module_budget} shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules.   In addition, Figure~\ref{fig:layer_budget} demonstrates that weight matrices in top layers are more important than those in bottom layers.

Adding more trainable parameters to the critical weight matrices can lead to better model performance. In contrast, adding more parameters to those less important weight matrices yields very marginal gains or even hurt model performance.  Given the parameter budget, i.e., the number of total trainable parameters, we always prefer to allocate more parameters to those important modules. Distributing the budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix tuning), often gives suboptimal performance.  
To this end, a natural question is: 
\begin{center}
	{\bf \textit{How can we allocate the parameter budget adaptively according to importance} } \\
	{\bf \textit{of modules to improve the performance of parameter-efficient fine-tuning?} }
\end{center}


To answer this question, we propose a new method -- \textit{\ouralg} (\underline{Ada}ptive \underline{Lo}w-\underline{R}ank \underline{A}daptation), which dynamically allocates the parameter budget among weight matrices during LoRA-alike fine-tuning.  Specifically, {\ouralg} adjusts the rank of incremental matrices to control their budget.  Critical incremental matrices are assigned with high rank such that they can capture more fine-grained and task-specific information.  Less importance ones are pruned to have lower rank to prevent overfitting and save the computational budget.  There are some methods to control the rank of matrices in the existing literature of matrix approximation \citep{cai2010singular,koltchinskii2011nuclear,toh2010accelerated}.  Most of them directly compute singular value decomposition (SVD) of a matrix and then truncate the smallest singular values. Such an operation can manipulate the rank explicitly and, more importantly, minimize the difference between the resulting matrix and the original matrix. However, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD for a large number of high-dimensional weight matrices.  Therefore, instead of computing SVD exactly, we parameterize $\DeltaW$ as $\DeltaW = \PP\Lam\QQ$ to mimic SVD.  The diagonal matrix $ \Lam $ contains singular values while the orthogonal matrices $ \PP $ and $ \QQ $ represent left/right singular vectors of $ \DeltaW $.  To regularize the orthogonality of $ \PP $ and $ \QQ $, an additional penalty is added to training loss.  Such a parameterization avoids the intensive computations of SVD. Besides, another advantage is that we only need to drop the unimportant singular values while the singular vectors are maintained. This preserves the possibility of future recovery and stabilizes the training. See a detailed comparison to LoRA in Section~\ref{sec:method}. 


Based on our SVD parameterization, {\ouralg} dynamically adjusts the rank of $\Delta = PVQ$ by \textit{importance scoring}.  Specifically, we divide the incremental matrix $\PP\Lam\QQ$ into triplets, where each triplet $\Gcali$ contains the $i$-th singular value and the corresponding singular vectors.  To quantify the importance of triplets, we propose a novel importance metric, which takes account of the contribution of every entry in $ \Gcali $ to the model performance \citep{sanh2020movement,liang2021super,zhang2022platon}. Triplets with low importance scores are granted low priority and hence the singular values are zeroed out. Triplets with high importance are retained for fine-tuning.  Moreover, we also propose a global budget scheduler to facilitate the training. In particular, we start from an initial parameter budget, which is slightly higher than the final budget, and then gradually reduce it until matching the target. Such a scheduler can improve the training stability and model performance. Please see Section~\ref{sec:method} for a detailed description of our importance metric and budget scheduler. 


We conduct extensive experiments on a wide range of tasks and models to demonstrate the effectiveness of {\ouralg}.  Specifically, we evaluate the performance using DeBERTaV3-base \citep{he2021debertav3} on natural language understanding (GLUE, \citet{wang2018glue}) and question answering (SQuADv1, \citet{squad1} and SQuADv2, \citet{squad2}) datasets. We also apply our methods to BART-large \citep{lewis2019bart} and evaluate the performance on natural language generation (XSum, \citet{narayan2018don} and CNN/DailyMail, \citet{hermann2015teaching}) tasks. We show {\ouralg} consistently outperforms the baseline, especially under low budget settings. For example, with less than $0.1\%$ trainable parameters of full fine-tuning, {\ouralg} achieves a 1.2\% F1 improvement on the SQuAD2.0 dataset compared with state-of-the-art approaches.    







