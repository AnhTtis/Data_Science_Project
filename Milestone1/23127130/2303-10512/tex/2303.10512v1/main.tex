\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[colorlinks, linkcolor=blue, anchorcolor=blue, citecolor=blue]{hyperref}
\usepackage{url}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{letltxmacro}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{lineno}

\newcommand{\W}{W}
\newcommand{\Wpre}{W^{(0)}}
\newcommand{\kk}{k}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\DeltaW}{\Delta}
\newcommand{\DeltaWk}{\DeltaW_{\kk}}
\newcommand{\DeltaWki}{\DeltaW_{\kk,i}}
\newcommand{\A}{A}
\newcommand{\B}{B}
\newcommand{\Ai}{\A_{\cdot i}}
\newcommand{\Bi}{\B_{i \cdot}}
\newcommand{\PP}{P}
\newcommand{\PPk}{\PP_{\kk}}
\newcommand{\PPki}{\PP_{\kk,*i}}
\newcommand{\PPt}{\PP^{(t)}}
\newcommand{\PPtk}{\PPt_{\kk}}
\newcommand{\PPtki}{\PPt_{\kk,*i}}
\newcommand{\PPcal}{\mathcal{\PP}}
\newcommand{\PPcalt}{\PPcal^{(t)}}
\newcommand{\QQ}{Q}
\newcommand{\QQk}{\QQ_{\kk}}
\newcommand{\QQki}{\QQ_{\kk,i*}}
\newcommand{\QQt}{\QQ^{(t)}}
\newcommand{\QQtk}{\QQt_{\kk}}
\newcommand{\QQtki}{\QQt_{\kk,i*}}
\newcommand{\QQcal}{\mathcal{\QQ}}
\newcommand{\QQcalt}{\QQcal^{(t)}}
\newcommand{\Lam}{\Lambda}
\newcommand{\Lamk}{\Lam_{\kk}}
\newcommand{\Lamt}{\Lam^{(t)}}
\newcommand{\Lamtk}{\Lamt_{\kk}}
\newcommand{\Lamtpk}{\Lam^{(t+1)}_{\kk}}
\newcommand{\Lami}{\Lam_{ii}}
\newcommand{\Lamki}{\Lam_{\kk,ii}}
\newcommand{\Lamtki}{\Lamt_{\kk,ii}}
\newcommand{\Lamcal}{\mathcal{E}}
\newcommand{\Lamcalt}{\Lamcal^{(t)}}
\newcommand{\tLam}{\tilde{\Lam}}
\newcommand{\tLamt}{\tilde{\Lam}^{(t)}}
\newcommand{\tLamti}{\tilde{\Lam}^{(t)}_{ii}}
\newcommand{\tLamk}{\tLam_{\kk}}
\newcommand{\tLamki}{\tLam_{\kk,ii}}
\newcommand{\tLamtk}{\tLamt_{\kk}}
\newcommand{\lambdaki}{\lambda_{\kk,i}}
\newcommand{\bLam}{\bm{\Lam}}
\newcommand{\blam}{\bm{\lambda}}
\newcommand{\blamt}{\blam^{(t)}}
\newcommand{\blamtp}{\blam^{(t+1)}}
\newcommand{\Bu}{b}
\newcommand{\But}{\Bu^{(t)}}
\newcommand{\BuT}{\Bu^{(T)}}
\newcommand{\Buinit}{\Bu^{(0)}}
\newcommand{\rinit}{r^{(0)}}
\newcommand{\rat}{r^{(t)}}
\newcommand{\raT}{r^{(T)}}
\newcommand{\ramt}{r_{m}^{(t)}}
\newcommand{\ramT}{r_{m}^{(T)}}
\newcommand{\rkt}{r_{k}^{(t)}}
\newcommand{\rkT}{r_{k}^{(T)}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\rbart}{\rbar^{(t)}}
\newcommand{\rbarT}{\rbar^{(T)}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bh}{\bm{h}}
\newcommand{\Reg}{R}
\newcommand{\DeltaT}{\Delta_{T}}
\newcommand{\Sc}{S}
\newcommand{\Sct}{\Sc^{(t)}}
\newcommand{\Sci}{\Sc_{i}}
\newcommand{\Scki}{\Sc_{\kk,i}}
\newcommand{\Scti}{\Sc^{(t)}_{i}}
\newcommand{\Sctk}{\Sct_{\kk}}
\newcommand{\Sctki}{\Sct_{\kk,i}}
\newcommand{\scf}{s}
\newcommand{\scft}{\scf^{(t)}}
\newcommand{\I}{I}
\newcommand{\Ibar}{\overline{I}}
\newcommand{\Ibart}{\Ibar^{(t)}}
\newcommand{\Ubar}{\overline{U}}
\newcommand{\Ubart}{\Ubar^{(t)}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\X}{X}
\newcommand{\He}{H}
\newcommand{\Wq}{\W_{q}}
\newcommand{\Wk}{\W_{k}}
\newcommand{\Wv}{\W_{v}}
\newcommand{\Wqi}{\W_{q_i}}
\newcommand{\Wki}{\W_{k_i}}
\newcommand{\Wvi}{\W_{v_i}}
\newcommand{\Wo}{\W_{o}}
\newcommand{\Wfp}{\W_{f_1}}
\newcommand{\Wfq}{\W_{f_2}}
\newcommand{\bb}{\bm{b}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Gcali}{\Gcal_i}
\newcommand{\Gcalk}{\Gcal_{\kk}}
\newcommand{\Gcalki}{\Gcal_{\kk,i}}

\newcommand{\Proj}{\mathcal{T}}

\newcommand{\norm}[1]{\lVert#1 \rVert}
\newcommand{\normlarge}[1]{\left\lVert#1\right\rVert}

\newcommand{\ouralg}{AdaLoRA} 

\newtheorem*{remark}{Remark}


\title{Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}
\author{Qingru Zhang$^\dagger$\thanks{Work was done during Qingru Zhang's internship at Microsoft Azure AI.}, \ Minshuo Chen$^\ddagger$, \ Alexander Bukharin$^\dagger$, \ Pengcheng He$^\diamond$, \ Yu Cheng$^\diamond$, \\
\textbf{Weizhu Chen$^\diamond$} and \ \textbf{Tuo Zhao$^\dagger$}\\
$^\dagger$Georgia Institute of Technology \ \ 
$^\ddagger$Princeton University \ \
$^\diamond$Microsoft Azure AI \\
\texttt{\{qingru.zhang,abukharin3,tourzhao\}@gatech.edu} \\
\texttt{mc0750@princeton.edu} \\
\texttt{\{penhe,yu.cheng,wzchen\}@microsoft.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
    Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose {\ouralg}, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, {\ouralg} parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of {\ouralg}. Results demonstrate that {\ouralg} manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at \url{https://github.com/QingruZhang/AdaLoRA}.   	
\end{abstract}

\input{1-Introduction}
\input{2-Background}
\input{3-Method}
\input{4-Experiments}
\input{5-Conclusion}

\bibliography{ref}
\bibliographystyle{iclr2023_conference}

\input{6-Appendix}



\end{document}
