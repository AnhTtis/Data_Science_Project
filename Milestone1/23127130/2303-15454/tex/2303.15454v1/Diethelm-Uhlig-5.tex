\documentclass{siamart220329} %

\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{amsthm}
\usepackage[mathscr]{eucal}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{float}
\usepackage{algorithmic}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}{Lemma}[section]
%\newtheorem{corollary}{Corollary}[section]
\newsiamremark{remark}{Remark}
\newsiamremark{example}{Example}

\def\tcr{\textcolor[rgb]{1,0,0}}
\def\tcb{\textcolor[rgb]{0,0,1}}

%\usepackage{chngcntr}
%\counterwithin*{equation}{section}
%\def\theequation{\arabic{section}.\arabic{equation}}
%\def\thefigure{\arabic{section}.\arabic{figure}}


\begin{document}

\title{A new approach to shooting methods\\ 
	for terminal value problems\\
	of fractional differential equations}
\author{Kai Diethelm\thanks{Faculty of 
 	Applied Natural Sciences and Humanities (FANG), 
 	Technical University of Applied Sciences, W\"urzburg-Schweinfurt,
	Ignaz-Sch\"on-Str.\ 11, 97421 Schweinfurt, Germany,
	e-mail: kai.diethelm@thws.de, ORCID: 0000-0002-7276-454X}
	\and
	Frank  Uhlig\thanks{Department of Mathematics and Statistics, 
	Auburn University, Auburn, AL 36849-5310, USA,
	e-mail: uhligfd@auburn.edu, ORCID: 0000-0002-7495-5753}
}

\maketitle


\begin{abstract}
	For terminal value problems of fractional differential equations of 
	order $\alpha \in (0,1)$, shooting methods that use Caputo derivatives 
	are a well developed and investigated approach. 
	Based on recently established analytic properties of such problems,
	we develop a new technique to select initial values
	that solve such shooting problems quickly and accurately. 
	Numerical experiments indicate that this new technique, called
	proportional secting, converges accurately to the solution 
	in very few iterations. Run time measurements indicate a speedup factor 
	typically in the range between 4 and 10 in comparison to the often suggested 
	standard bisection method.
\end{abstract}

\begin{keywords}
	fractional differential equation, Caputo derivative,
	terminal condition, terminal value problem,
	shooting method, proportional secting, secant method
\end{keywords}

\begin{MSCcodes}
	Primary 65L10; Secondary 34A08, 65R20
\end{MSCcodes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction and Motivation} 
	\label{sec:intro}

Differential equations of fractional (i.e., non-integer) order \cite{Di2010} 
are an object of great current interest, in particular since they are 
very useful tools for modeling various phenomena in science and engineering,
see, e.g., \cite{hbfc7,hbfc8,hbfc6,hbfc4,hbfc5}. For the most part, 
the problems under consideration have the form
\begin{equation}
	\label{eq:ivp}
	D_a^\alpha y(t) = f(t, y(t)), \qquad y(a) = \tilde y_0,
\end{equation}
where $\alpha \in (0, 1)$ is the order of the differential operator (the case $\alpha > 1$
only arises in a very small number of potential applications and will not be discussed here)
with a given function $f:[a,b] \times \mathbb R^d \to \mathbb R^d$ with some $d \in \mathbb N$
and a given value $\tilde y_0 \in \mathbb R^d$ that describes the initial state of the system
whose behavior is modeled by the system \eqref{eq:ivp}, i.e.\ the function value of the
exact solution $y$ of the system at the starting point $t=a$ of the process under consideration.
In eq.~\eqref{eq:ivp}, the differential operator $D_a^\alpha$
is the so-called Caputo differential operator of order $\alpha$ with starting point $a \in \mathbb R$,
defined by \cite[Definition 3.2]{Di2010}
\begin{equation}
	\label{eq:def-caputo}
	D_a^\alpha y(t) 
	= \frac 1 {\Gamma(1-\alpha)} \frac{\mathrm d}{\mathrm d t} 
		\int_a^t (t-s)^{-\alpha} \left( y(s) - y(a) \right) \mathrm d s
\end{equation}
for $t \in [a,b]$ and functions $y : [a,b] \to \mathbb R^d$ with a sufficient degree
of smoothness.

%\end{document}  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In these equations \eqref{eq:ivp} and \eqref{eq:def-caputo} the point $a$ plays a special role. In technical applications, it designates the
starting point of the process that is being modeled. A typical example is the modeling of the mechanical
behaviour of an object made of some viscoelastic material under an external load where  
for times $t < a$ the material is still in its virgin state and forces are applied to it only for 
$t \ge a$. The problem \eqref{eq:ivp} is an \emph{initial value problem} because 
the condition $y(a) = \tilde y_0$ refers to the state of the process at the point $t = a$, i.e.,\ at its
initial point. Here we are interested in finding the function values $y(t)$ for $t \in [a,b]$ with some 
predetermined $b > a$.

From the analytical point of view, such initial value problems are well understood, 
see e.g., \cite[Chapters 6 and 7]{Di2010}. Many numerical methods  
have been proposed and investigated; cf., e.g., \cite{hbfc3}. However, from the perspective of mathematical
modeling, they are of limited use for us because they hinge on the exact state of the process
at the initial time $t=a$, which may be impossible to determine in actual applications.
If for technical reasons one can only measure the value of $y(b)$ for some $b > a$
but not $y(a)$ itself, this leads to the problem
\begin{equation}
	\label{eq:tvp}
	D_a^\alpha y(t) = f(t, y(t)), \qquad y(b) = y^*.
\end{equation}
Then our task is to solve (\ref{eq:tvp}) on some interval $[a, c]$ where
$a$ is the starting time of the process and $c \ge b$. This can be done
in two steps: 
\begin{itemize}
\item First,  solve the problem on the interval $[a,b]$. Since the given value $y_0$ 
	in eq.~\eqref{eq:tvp} refers to the end point of the interval, this is a 
	\emph{terminal value problem}.
\item After the first step, the solution is known on the entire interval $[a,b]$
	and thus the value of $y$ at the initial point $a$ is also known, i.e., $\tilde y_0 = y(a)$.\\
	In the second step we can therefore replace the terminal condition $y(b) = y^*$ in eq.~\eqref{eq:tvp}
	by the initial condition $y(a) = \tilde y_0$ and convert the original problem into 
	the classical initial value problem \eqref{eq:ivp} that can be solved on the entire interval $[a,c]$.
\end{itemize}
As indicated above,
the second step of this process has a well understood structure and can be handled by 
standard methods. Therefore it does not require any special attention. Hence we focus only
on the first step here.


\section{Analytic Properties of Terminal Value Problems}
\label{sec:analysis}

Before we discuss  numerical techniques that solve terminal value problems for fractional differential equations,  we recall some known analytical properties of this class of problems.

First of all, we  clarify under which conditions the problem is well posed. This question
has been  discussed and partially solved in \cite{Di2008,DF2012}. A complete analysis
is provided in \cite{CT2017} and additional aspects were presented in \cite{DF2018,FM2011}.

In Section \ref{sec:intro} we have  introduced the terminal value problem 
\eqref{eq:tvp} in arbitrary dimension $d$. It has been shown in \cite{CT2017}, however, that the 
problem is only well posed for $d = 1$. A counterexample for $d > 1$ was given in \cite[Section 6]{CT2017}.
This example demonstrated that there may be  more than one solution when $d > 1$. Therefore
we shall only consider the case $d=1$ from now on. Following the classical set-up, we will also assume
that the function $f$ in eq.~\eqref{eq:tvp} is defined on $[a,b] \times \mathbb R$ and maps 
into $\mathbb R$ continuously.
Besides, we assume that it satisfies a Lipschitz condition with respect to the second variable. Under these conditions 
Cong and Tuan \cite[Theorem 3.5]{CT2017} have shown:

\begin{theorem}
	\label{thm:tvp-sol-ex}
	Let $f : [a,b] \times \mathbb R \to \mathbb R$ be continuous and satisfy the Lipschitz condition 
	\begin{equation}
		\label{eq:lip}
		|f(t, x) - f(t, y)| \le L(t) | x -y |
	\end{equation}
	with respect to
	the second variable for all $t \in [a,b]$ with some function $L \in C[a,b]$. 
	Then, for any $y_0 \in \mathbb R$, the terminal value problem \eqref{eq:tvp}
	has a unique solution in $C[a,b]$.
\end{theorem}

A classical technique for investigating analytical properties of \emph{initial} value problems for 
differential equations is based on rewriting the given problems in the form of equivalent
integral equations. This can be done in the fractional case in exactly the same way as in the
classical case of a first order problem, see, e.g., \cite[Lemma 6.2]{Di2010}. In both cases
the result is a Volterra integral equation. But for the class of \emph{terminal} value problems
for fractional differential equations the situation changes significantly.  
While rewriting our problem in the form of an integral equation is still possible, the resulting integral 
equation will be  of Fredholm type, not Volterra type \cite[Theorem 6.18]{Di2010}. In 
 first order differential equations, Fredholm integral equations arise as well, but in connection
with \emph{boundary} value problems and not with \emph{initial} value problems. Therefore 
we shall employ a technique based on the principles used for boundary value 
problems for integer order initial value problems. Specifically, 
{\em shooting methods} \cite{Keller} will become the fundamental building blocks for our
numerical method for solving fractional terminal value problems.

To describe this method, we need further analytic prerequisites. 
An important tool in our work is the one-parameter Mittag-Leffler function 
$E_\alpha : \mathbb C \to \mathbb C$ defined by \cite{GKMR2020}
\[
	E_\alpha(z) = \sum_{k=0}^\infty \frac{z^k}{\Gamma(\alpha k + 1)}.
\]
For the values of $\alpha$ that are relevant in our setting we exploit the
following property:

\begin{lemma}
	\label{lem:ml}
	For all $\alpha \in (0,1)$, the function $E_\alpha$ is analytic.
	Moreover, for these values of $\alpha$ and all $z \in \mathbb  R$, we have that
	$E_\alpha(z) > 0$ and that $E_\alpha(z)$ is a strictly increasing function of $z \in \mathbb R$.
\end{lemma}

\begin{proof}
	The analyticity of $E_\alpha$ follows from \cite[Proposition 3.1]{GKMR2020}. 
	The inequality $E_\alpha(z) > 0$ is trivial for $z \ge 0$ since Euler's Gamma function
	satisfies $\Gamma(w) > 0$ for all $w > 0$. For $z<0$ the inequality is a consequence
	of the properties discussed in \cite[Subsection 3.7.2]{GKMR2020}; the strict
	monotonicity also follows from the properties shown in \cite[Subsection 3.7.2]{GKMR2020}.
\end{proof}

To familiarize oneself with  the nature of our numerical method for the terminal value problems that we describe in detail below, it helps to understand certain properties
of initial value problems for fractional differential equations. Specifically, we shall look at 
the following aspect:
Given two solutions of the same fractional differential equation, subject to 
two different initial values on the same interval,
what can be said about the difference between the two associated solutions throughout
this interval? In fact, this difference can be estimated from above and from below. First we explain this
result for a linear differential equation which is very simple and immediately
gives us important insights.

\begin{theorem}
	\label{thm:bounds-lin}
	Let a function $\ell \in C[a,b]$ be given, and
	let $y_1$ and $y_2$, respectively, be the solutions to the initial value problems
	\[
		D_a^\alpha y_k(t) = \ell(t) y_k(t), \qquad y_k(a) = y_{0,k} \qquad (k = 1,2)
	\]
	with $y_{0,1} > y_{0,2}$. Then, for all $t \in [a,b]$,
	\begin{eqnarray}
		\label{eq:diff-bounds1}
		\lefteqn{( y_{0,1} - y_{0,2} ) E_\alpha (\ell_*(t) (t-a)^\alpha) } \\
			\nonumber
		& \le & y_1(t) - y_2(t) 
			\le ( y_{0,1} - y_{0,2} ) E_\alpha (\ell^*(t) (t-a)^\alpha)
	\end{eqnarray}
	where $\ell_*(t) = \min_{s \in [a,t]} \ell(s)$ and $\ell^*(t) = \max_{s \in [a,t]} \ell(s)$.
\end{theorem}

\begin{proof}
	The upper bound is derived in \cite[Theorem 5]{DT2022},
	the lower bound has been shown in \cite[Theorem 4]{DT2022}.
\end{proof}

In the general (nonlinear) case the result is  more involved 
but the essential properties of the linear case remain intact.

\begin{theorem}
	\label{thm:bounds-nonlin}
	Assume that $f$ satisfies the hypotheses of Theorem \ref{thm:tvp-sol-ex}.
	Let $y_1$ and $y_2$, respectively, be the solutions to the initial value problems
	\[
		D_a^\alpha y_k(t) = f(t, y_k(t)), \qquad y_k(a) = y_{0,k} \qquad (k = 1,2)
	\]
	where $y_{0,1} > y_{0,2}$. Then
	\begin{eqnarray}
		\label{eq:diff-bounds2}
		\lefteqn { ( y_{0,1} - y_{0,2} ) E_\alpha (\tilde \ell_*(t) (t-a)^\alpha) } \\
			\nonumber
		& \le &  y_1(t) - y_2(t) 
			\le ( y_{0,1} - y_{0,2} ) E_\alpha (\tilde \ell^*(t) (t-a)^\alpha)
	\end{eqnarray}
	where
	\begin{subequations}
		\label{eq:ltildestar}
	\begin{equation}
		\tilde \ell_*(t) 
		= \inf_{s \in [a,t], y \ne 0} \frac{f(s, y + y_1(s)) - f(s, y_1(s))} y 
		< \infty 
	\end{equation}
	and 
	\begin{equation}
		\tilde \ell^*(t) 
		= \sup_{s \in [a,t], y \ne 0} \frac{f(s, y + y_1(s)) - f(s, y_1(s))} y 
		< \infty 
	\end{equation}
	\end{subequations}
	for all $t \in [a,b]$.
\end{theorem}

\begin{proof}
	This is the result of \cite[Theorem 7]{DT2022}.
\end{proof}

Obviously, in the linear case discussed in Theorem \ref{thm:bounds-lin}, 
Theorem \ref{thm:bounds-nonlin} can be applied as well.
In this situation, we can immediately see that the functions $\ell_*$ and $\ell^*$ 
of Theorem \ref{thm:bounds-lin} coincide with the functions $\tilde \ell_*$
and $\tilde \ell^*$, respectively, of Theorem \ref{thm:bounds-nonlin}.

We use the notation $\beta \sim \gamma$ for  expressions $\beta$ and $\gamma$
that depend on identical quantities to denote that there exist absolute constants $C_1 > 0$ and $C_2 > 0$ 
such that for all admissible values of the quantities
that $\beta$ and $\gamma$ depend on, $C_1 \beta \le \gamma \le C_2 \beta$.
With this convention, and using the findings of Lemma \ref{lem:ml}, we summarize the
statements of our earlier two Theorems in the following compact form.


\begin{corollary}
	\label{cor:bounds}
	Under the assumptions of Theorem \ref{thm:bounds-lin} or Theorem \ref{thm:bounds-nonlin},
	we have for any $y_{0,1} > y_{0,2} \in \mathbb R$ that
	\begin{equation}
		\label{eq:bounds}
		c_* \left( y_{0,1} - y_{0,2} \right) \le y_1(b) - y_2(b) \le c^* \left( y_{0,1} - y_{0,2} \right)
	\end{equation}
	where
	\begin{subequations}
		\label{eq:propconst}
	\begin{equation}
			\label{eq:propconst-a}
		c_* =  E_\alpha (\tilde \ell_*(b) (b-a)^\alpha) > 0
	\end{equation}
	and
	\begin{equation}
		c^* =  E_\alpha (\tilde \ell^*(b) (b-a)^\alpha) > 0
	\end{equation}
	\end{subequations}
	for the functions $\tilde \ell_*$ and $\tilde \ell^*$ of \eqref{eq:ltildestar}.
	In view of the bounds $c^* > c_* > 0$ that follow from \eqref{eq:propconst},
	we can rewrite eq.~\eqref{eq:bounds} as
	\begin{equation}
		y_1(b) - y_2(b) \sim y_{0,1} - y_{0,2}.
	\end{equation}
\end{corollary}

This observation will be the foundation on which we construct our numerical
method in Subsection \ref{sec:nextguess}. 

For later reference, we note a few more facts: 

\begin{remark}
	\label{rem:propconst}
	In general, we cannot expect the ratio
	\[
	 	\hat c :=  \frac{y_1(b) - y_2(b)}{ y_{0,1} - y_{0,2}},
	\]
	i.e.\ the proportionality factor between terminal and initial values of the
	solution to a given problem, to be known exactly. However, from
	eq.~\eqref{eq:bounds}, we know that $\hat c$  is bounded  above by $c^*$ 
	and  below by $c_*$ as given in \eqref{eq:propconst}.
	As long as no additional information is available that allows to
	obtain a more precise approximate value for $\hat c$, one may therefore use the mean 
	value of the upper and the lower bound, i.e.\ apply the approximation
	\begin{equation}
		\label{eq:propconstapprox}
		\hat c \approx \frac{c_* + c^*} 2.
	\end{equation}
	To compute this value in practice, it follows from \eqref{eq:propconst} that 
	we need to evaluate the quantities $\tilde \ell_*(b)$ and $\tilde \ell^*(b)$
	as defined in eq.~\eqref{eq:ltildestar}. If an approximate solution $\hat y$ to the
	differential equation in question is known at least for some grid points 
	$a = t_0 < t_1 < t_2 < \ldots < t_N = b$, one may select a step size $H>0$ and
	an integer  $M > 0$ and approximate  these upper and lower  bounds 
	by 
	\begin{subequations}
		\label{eq:capprox}
	\begin{eqnarray}
		\tilde \ell_*(b) & \approx & \min \Bigg \{ \frac{f(t_j, k H + \hat y(t_j)) - f(t_j,\hat y(t_j))}{k H} : \\
				\nonumber
				& & \qquad \qquad j \in \{ 0, 1, 2, \ldots, N \}, k \in \{ \pm1, \pm 2 , \ldots, \pm M \} \Bigg \}
	\end{eqnarray}
	and
	\begin{eqnarray}
		\tilde \ell^*(b) & \approx & \max \Bigg \{ \frac{f(t_j, k H + \hat y(t_j)) - f(t_j,\hat y(t_j))}{k H} : \\
				\nonumber
				& & \qquad \qquad j \in \{ 0, 1, 2, \ldots, N \}, k \in \{ \pm1, \pm 2 , \ldots, \pm M \} \Bigg \},
	\end{eqnarray}
	\end{subequations}
	respectively, and use these values instead of the exact values $\tilde \ell_*(b)$
	and $\tilde \ell^*(b)$ to approximately compute $c_*$ and $c^*$ and thus $\hat c$.
\end{remark}

\begin{remark}
	\label{rem:dissip}
	In practice,  estimating the proportionality factor $\hat c$ as 
	indicated in Remark \ref{rem:propconst} is especially useful when the differential equation in
	eq.~\eqref{eq:tvp} is dissipative, i.e.\ when $(f(t, y_1) - f(t, y_2)) (y_1 - y_2) \le 0$ for all
	$t \in [a,b]$ and all $y_1, y_2 \in \mathbb R$. In this case, eq.~\eqref{eq:propconst} implies that
	$\tilde \ell_*(t) \le \tilde \ell^*(t) \le 0$ for all $t$. Hence, in view of the monotonicity of the Mittag-Leffler
	function $E_\alpha$ (see Lemma \ref{lem:ml}), we find that 
	\[
		0 < c_* = E_\alpha (\tilde \ell_*(b) (b-a)^\alpha)  \le c^* 
			= E_\alpha (\tilde \ell^*(b) (b-a)^\alpha)  \le 1 = E_\alpha (0) .
	\] 
	Therefore, 
	the interval $[c_*, c^*]$ in which we are guaranteed to find the correct value of $\hat c$ is quite small,
	and so by choosing this interval's midpoint we only make a small error. If, on the other hand,
	the differential equation is not dissipative then $c^*$ may be very much larger than $c_*$,
	and the strategy described in Remark \ref{rem:propconst} may lead to an estimate for
	$\hat c$ that is very far away from the correct value. In Section \ref{sec:num} below we shall
	present examples for either case.
\end{remark}

\begin{remark}
	\label{rem:propconstauto}
	For the reasons indicated in Remark \ref{rem:dissip}, we suggest yet another method for
	approximating the value $\hat c$.
	Specifically, one may attempt a brief analysis of the given differential equation and try to find out whether
	the approach of Remark \ref{rem:propconst} is appropriate (i.e., whether this approach does
	not lead to an excessively large value of $\hat c$). If this analysis leads to the conclusion that
	Remark \ref{rem:propconst} does not yield a useful value, one then uses a smaller one.
	More precisely, our suggestion reads as follows:
	\begin{enumerate}
	\item Approximately compute the values $\tilde \ell_*(b)$ and $\tilde \ell^*(b)$ as in Remark \ref{rem:propconst}.
	\item If $\tilde \ell^*(b) \le 0$ then there is no danger of obtaining extremely large values
		for $c_*$ and $c^*$. Thus we may proceed as suggested in Remark \ref{rem:propconst}.
	\item If $\tilde \ell^*(b) > 0$ and $\tilde \ell_*(b) \le 0$ then $c_* \le 1$ but $c^*$ is
		(potentially very much) larger than $1$. To dampen the possible overestimation
		that $c^*$ might induce, ignore the precise value of $c^*$ and set $\hat c = 1$.
	\item If $\tilde \ell^*(b) > 0$ and $\tilde \ell_*(b) > 0$ then $c^* \ge c_* > 1$. 
		Again, to dampen a likely overestimation, use the lower bound of the
		interval $[c_*, c^*]$ as an estimate for $\hat c$, i.e.\ set 
		$\hat c = E_\alpha (\tilde \ell_*(b) (b-a)^\alpha)$ 
		as suggested by eq.~\eqref{eq:propconst-a}.
	\end{enumerate}
\end{remark}


\section{Description of the Method}
\label{sec:method}

\subsection{General Framework}

As indicated in Section \ref{sec:analysis}, the essential characteristics of  
problem \eqref{eq:tvp} that we consider are the same as those of  classical
boundary value problems. Therefore, our solution approach involves shooting methods
\cite{Keller}---a well established technique for boundary value problems.
The basic steps of shooting methods are
as follows:
\begin{enumerate}
\item \label{step:initialguess}
	Set $k=0$. Given  problem \eqref{eq:tvp}, make an initial guess $\tilde y_0^{(0)}$ for the
	value $y(a)$.
\item \label{step:loop}
	(Numerically) compute a solution $\tilde y_k$ to the differential equation in \eqref{eq:tvp} 
	associated with the initial condition $\tilde y_k(a) = \tilde y_0^{(k)}$,
	but without any concern  about the terminal condition in \eqref{eq:tvp}.
\item Compare the computed solution $\tilde y_k(b)$ with the desired  solution $y^*$ for  point $b$.
	\begin{enumerate}
	\item If $\tilde y_k(b)$ is sufficiently close to the desired solution $y^*(b)$, accept $\tilde y_k$ as the numerical
		solution of the given terminal value problem \eqref{eq:tvp} and stop.
	\item \label{step:newguess}
		Otherwise, increase the iteration index $k$ by $1$, construct a new (improved) guess $\tilde y_0^{(k)}$ for the
		starting value $y(a)$  and go back to step \ref{step:loop}.
	\end{enumerate}
\end{enumerate}

This  description contains a number of components that will be specified more precisely 
in the subsequent subsections. Our overarching concern here is to keep the
 chosen shooting  algorithm's computational complexity low.  
 The computational cost of shooting algorithms is  reflected in the number of operations required per 
iteration  step multiplied by the  number of iterations, while also requiring satisfactory accuracy.


\subsection{Selection of the Initial Guess $\tilde y_0^{(0)}$ for $y(a)$}
\label{sec:initialguess}

Unless specific  information about the given fractional ODE problem is available that suggests otherwise,
we choose $\tilde y_0^{(0)} = y^*$ as our initial guess
for $y(a)$ required in step~\ref{step:initialguess}, 
i.e.,\ we start by using the desired terminal value as a first guess for the initial value.

One might suspect that a good choice of the initial guess, i.e.,\ a value that is very
close to the unknown exact initial value, would lead to  convergence  and
acceptable accuracy at the end point $b$ quickly, while a poorly chosen initial guess 
might require more iterations  and result in a significantly higher overall 
computational cost. The examples in Section \ref{sec:num}, however, indicate
otherwise. Indeed, in almost every example that we have considered, 
satisfactory accuracy was achieved with very few iterations, no matter whether or not the 
starting guess for the initial value was close to that of the exact solution.

\subsection{Numerically Solving a Fractional ODE   Initial Value Problem}

The quest for useful algorithms that compute the solution of an (artificially constructed)
initial value problem in step~\ref{step:loop} has been discussed in detail by 
Ford et al.\ \cite{FM2011}; with the outcome  that
the fractional Adams-Bashforth-Moulton method \cite{DFF2002,DFF2004} is often a good choice. However,
in cases of stiff differential equations or when the interval $[a,b]$ is very large, the stability
properties of this method may be insufficient \cite{Ga2010}. If this is the case it is preferable to use an
implicit linear multistep method such as the fractional trapezoidal method \cite{Ga2015} or a 
fractional backward differentiation formula \cite{Lu1985,Lu1986}.
For our examples in Section \ref{sec:num}, we present the results
obtained with both alternative methods for comparison.
There we use a uniform discretization of the basis interval $[a,b]$,
i.e.,\ we choose a positive integer $N$ and select the equally spaced grid points 
$t_j = a + j h$ for the step size  $h = (b-a) / N$. Then we
 can employ FFT techniques to obtain a fast
implementation of the methods \cite{Ga2018,HLS1985}.
This refinement allows us to obtain the numerical solution on the interval $[a,b]$
in only $O(N (\log N)^2)$ arithmetic operations
while the standard implementation without FFT would
require $O(N^2)$ operations.

Solutions to fractional differential equations of the type considered here  
are almost never differentiable at the initial point \cite[Theorem~6.26]{Di2010}.
This  has a significant influence when considering numerical methods
because it adversely affects the convergence rate for many numerical methods,
such as the Adams method mentioned above, see \cite{DFF2004}. To improve convergence,
one could replace a uniform mesh by a graded mesh \cite{ZS2022}.
Another option is to use the non-polynomial collocation scheme that was suggested, analyzed and tested
in \cite{FMR2014}.  Both of these techniques lead to faster convergence. With them 
one can  achieve the required accuracy while using  larger step sizes and thus with 
reduced computational effort. But these ideas cannot be easily
combined with the FFT techniques and therefore the evaluations of their
associated numerical schemes become more costly computationally,  
eliminating the advantages of the increased step size.
We shall not pursue these latter approaches here any further. 

\subsection{Choice of the Improved Guess for the Initial Value}
\label{sec:nextguess}

The major contribution of our algorithm is a new and advanced  method for choosing
the iterating guesses for initial values. Traditional approaches 
\cite{Di2008,Di2015,FMR2014} have suggested to proceed by
the classical bisection method, thus halving the size of the interval in which the ``correct'' choice of
$y(a)$ can be found in each step. Clearly this scheme is convergent, but
 it takes a large number of iterations to arrive in a sufficiently small 
neighbourhood of the exact solution because the size
of the containment interval decreases rather slowly. We will suggest a different
method that converges much faster. Comparisons of the new approach with 
classical bisection based methods are given in Section \ref{sec:num}.

% \subsubsection{The Secant Method}

Like the classical bisection method, our approach also requires two initial guesses
for the initial value, $\tilde y_0^{(0)}$
and $\tilde y_0^{(1)}$. For $\tilde y_0^{(0)}$, as mentioned
in Subsection \ref{sec:initialguess},  we always choose $\tilde y_0^{(0)} := y^*$, the given 
terminal value. The next guess for the starting value, and all subsequent guesses,
are then guided by the observation that, according to Theorems \ref{thm:bounds-lin} 
and \ref{thm:bounds-nonlin}, two solution curves of a given fractional ODE with 
different initial values cannot cross each other. Hence, two solution curves for different starting 
values $y_{0,1} > y_{0,2}$ can either spread out or bunch up further over the time interval $[a, b]$ 
as time progresses. By Corollary \ref{cor:bounds}, the proportion of two solution values 
$y^*_1$ and $y^*_2$ obtained for $t = b$ and their starting values $y_{0,1}$ and $y_{0,2}$ at $t = a$ 
gives us a hint of how to space the iterations until we find a starting value $y_0^*$ that reaches 
the desired final value $y^*$ within a chosen small error bound.

As long as only the initial guess $\tilde y_0^{(0)}$ is available, i.e.\ when 
the next guess $\tilde y_0^{(1)}$ is still to be computed, we assume---due to lack of any
information that might suggest otherwise---that the proportionality factor $\hat c$ between
the terminal values (i.e.\ the function values of the solution at $t=b$) and the initial values 
(the corresponding values for $t=a$) discussed in Remark \ref{rem:propconst} 
is given by eq.~\eqref{eq:propconstapprox}, with the values $c_*$ and $c^*$ that arise in this
formula being replaced by their approximations indicated in eq.~\eqref{eq:capprox}. 
According to  Remark \ref{rem:propconst}, this means that our next guess for the initial value is given by
\begin{equation}
	\label{eq:nextguess1}
	\tilde y_0^{(1)} 
		:= \tilde y_0^{(0)} + \frac{ y^*  - \tilde y_0(b) }{\hat c} .
\end{equation}
This setting is equal to the previous guess $\tilde y_0^{(0)}$ if and only if the latter 
already resulted in the exact solution, i.e.\ if and only if $\tilde y_0(b) = y^*$ or approximately so and the problem has been solved.

\begin{remark}
	Note that  evaluating  the formulas  in \eqref{eq:capprox}
	requires  knowledge of an approximate solution to the given differential
	equation for some initial condition. At this stage, such information
	is already available because we have computed a 'solution' 
	from the first guess $\tilde y_0^{(0)}$ as initial value.
\end{remark}

\begin{remark}
	\label{rem:hatc-simple}
	\begin{itemize}
	\item The approach described in Remark \ref{rem:propconst} to compute the value $\hat c$
		requires the evaluation of the Mittag-Leffler function $E_\alpha$, cf.~eq.~\eqref{eq:propconst}.
		For this purpose, we suggest to use the algorithm developed in \cite{Ga2015b}.
	\item In case of a non-dissipative fractional differential equation, we have seen in Remark
		\ref{rem:dissip} that the approach of Remark \ref{rem:propconst}
		may lead to  very poor approximations of $\hat c$; in particular, its value
		may be massively over-estimated. Therefore, for non-dissipative problems, 
		one is likely to be better off with simply choosing an arbitrary not excessively large positive number
		for $\hat c$, e.g.\ $\hat c = 1$, in eq.~\eqref{eq:nextguess1}. 
	\item If the user believes that evaluating the formulas in \eqref{eq:capprox}
		is too expensive from the computational standpoint then one may use $\hat c = 1$ 
		for example in equation~\eqref{eq:nextguess1}, even for dissipative  equations.
		In this case one has to expect that the guess $\tilde y_0^{(1)}$ may be  
		a little bit worse than the one obtained with  $\hat c$ from \eqref{eq:propconstapprox}
		and the number of iterations until a satisfactory accuracy is obtained 
		may increase slightly. But eventually convergence will be achieved.
	\item Remark \ref{rem:propconstauto} provides a third possible idea for choosing 
		the value $\hat c$. It tries to find a compromise between the two other suggestions,
		thus attempting to avoid their respective disadvantages.
	\end{itemize}
	In our numerical experiments in Section \ref{sec:num}, we will report the results of our new method 
	for all three choices of $\hat c$ (the concept of Remark \ref{rem:propconst},
	the idea of Remark \ref{rem:propconstauto}, or simply setting $\hat c = 1$). It turns out that
	the method of Remark \ref{rem:propconstauto} usually requires the lowest computational effort.
\end{remark}

Our strategy for constructing further initial values $\tilde y_0^{(k)}$, $k = 2, 3, \ldots$, is  based on
Corollary \ref{cor:bounds}. Specifically, Corollary \ref{cor:bounds} tells us that
given two fractional initial value problems for the same  fractional differential equation but with 
different initial conditions, the difference in  terminal values of these two problems 
is approximately proportional to the difference in their initial values. Of course, when 
looking at Corollary \ref{cor:bounds} strictly, this statement is true only in the asymptotic 
sense when the differences between subsequent initial values tends to zero. But this proportionality can also be exploited in the general case, and our numerical results shown in Section~\ref{sec:num} show that this is justified.

So we now have to specify an initial 
value guess $\tilde y_0^{(k)}$ when $k \ge 2$. For this we  analyze the last two 
 iterations and compare their results (i.e., the calculated approximations
$\tilde y_{k-1}(b)$ and $\tilde y_{k-2}(b)$) with the desired value $y^*$ and see how these
three values are positioned relative to each other. To this end, it is convenient to express 
the target value $y^*$ as a convex combination of the two other $\tilde y_{\mu}(b)$ values 
($\mu \in \{k-2, k-1\}$). We write
\begin{equation}
	\label{eq:convcomb-target}
	y^* = \lambda_k  \tilde y_{k-1}(b) + (1 - \lambda_k) \tilde y_{k-2}(b)
\end{equation}
with some value $\lambda_k \in \mathbb R$ that can immediately be computed 
since all other quantities  in the linear equation \eqref{eq:convcomb-target}
are known. (Note that, strictly speaking, the notion of a convex combination is appropriate
in the classical sense only if $y^*$ lies inside of the interval bounded by $\tilde y_{k-1}(b)$
and $\tilde y_{k-2}(b)$ or $\lambda_k \in [0,1]$. However, using this concept
in a generalized meaning when $y^*$ does not lie inside  this interval, thus leading to $\lambda_k < 0$
or $\lambda_k > 1$, does not lead to any difficulties for our algorithm, and so we use the convexity 
terminology in this generalized sense.) From $\lambda_k$ we compute the
new guess for the initial value by the formula
\begin{equation}
	\label{eq:nextguess2+}
	\tilde y_0^{(k)} := \lambda_k \tilde y_0^{(k-1)}+ (1 - \lambda_k) \tilde y_0^{(k-2)},
\end{equation}
so the new initial value is a convex combination of the two preceding ones where the proportions
of each of them coincide with the proportions found in eq.~\eqref{eq:convcomb-target}. 
Evidently, if we had an exact equality in the statement of Corollary \ref{cor:bounds} and no
errors in the numerical solver for the initial value problem then this choice would lead to a 
solution that exactly hits the desired target value. 

More explicitly, when we plug in the value of $\lambda_k$ as computed from equation~\eqref{eq:convcomb-target}
into eq.~\eqref{eq:nextguess2+}, 
we  determine the next initial value by the formula
\begin{equation}
	\label{eq:c3}
	\tilde y_0^{(k)} 
		= \tilde y_0^{(k-1)} 
			+ \left (y^* - \tilde y_{k-1}(b) \right) 
				\frac{\tilde y_0^{(k-1)} - \tilde y_0^{(k-2)}}{\tilde y_{k-1}(b) - \tilde y_{k-2}(b)} .
\end{equation}
In other words, the correction term that we have to add to the previous initial value $\tilde y_0^{(k-1)}$
is proportional to the error of the previous terminal value, i.e.\ to the expression $y^* - \tilde y_{k-1}(b)$,
with a proportionality factor that amounts to the ratio of the difference of the two preceding initial values
and the difference of the two preceding terminal values.

Note that the formula for $\tilde y_0^{(k)}$ in eq.~\eqref{eq:c3} is independent of the actual lay of $\tilde y_0^{(k-1)}$ and $\tilde y_0^{(k-2)}$ with respect to each other and to $y^*$. This formulation was chosen deliberately to avoid any lay-logical tree complications when executing the proportional secting method. The reference point in eq.~\eqref{eq:c3} is always $y^*$. The algorithm is designed to compute $\tilde y_0^{(k)}$ whose 
associated function value $y_k(b)$ 
is closer to $y^*$ than at least one of those generated by the initial values $\tilde y_0^{(k-1)}$ and $\tilde y_0^{(k-2)}$, respectively. Once $\tilde y_0^{(k)}$ and the associated terminal value $\tilde y_k(b)$ have been computed, we drop the oldest point data pair $\tilde y_0^{(k-2)}$ and $\tilde y_{k-2}(b)$ and continue with the pairs with indices $k$ and $k-1$ in place of the pair with indices $k-1$ and $k-2$ and iterate on until $|y_k(b) - y^*|  $ has dropped below the required accuracy threshold.

Compared to the classical bisection method, our approach has two significant 
advantages:
\begin{enumerate}
\item Before a classical bisection method can be started, the correct 
	initial value $y(a)$ for the solution $y$ needs to be included in the 
	search interval, i.e.,\ one needs to know two numbers
	$\underline{y_0}$ and $\overline{y_0}$ so that $y(a)
	\in [\underline{y_0}, \overline{y_0}]$ for the actual solution $y$  with $y(b) = y^*$. Any first guess 
	$   y_0^{(0)}$ provides 
	one of the search interval bounds, but to find the other one on the other side of the unknown $y(a)$ one might have
	to perform an initial iteration. The proportional secting method does not require this; 
	indeed, to compute $\tilde y_0^{(k+1)}$ it is not necessary 
	that $y(a)$ is enclosed between $\tilde y_0^{(k)}$ and $\tilde y_0^{(k-1)}$.
\item In  classical bisection, one starts with the initial interval $[\underline{y_0}, \overline{y_0}]$
	in which the exact solution's value for $y(a)$  is known to be located. In each iteration step, the size 
	of this interval (and hence the accuracy with which one knows the correct initial
	value) is reduced by one half. While this method clearly converges, it is easy to see
	that its convergence is typically rather slow. When  the interval $[\underline{y_0}, \overline{y_0}]$ is large, classical bisection  often 
	requires very  many iterations for an acceptable accuracy in the $10^{-6}$ or $10^{-8}$ range. Our examples 
	demonstrate that the new proportional secting scheme reduces the size of the
	initial interval much faster and thus the number of necessary iteration steps is
	substantially decreased.
\end{enumerate}

\begin{remark}
	The search for the correct initial value is a nonlinear equation problem. 
	From this perspective, the approach that we have proposed 
	can be interpreted as solving this nonlinear equation by the secant method. Indeed, this approach
	for handling fractional terminal value problems thus far has been briefly mentioned by Ford and
	Morgado \cite[Section~3]{FM2011}. However, the focus of that paper was on 
	selecting IVP solvers and not on a shooting strategy; therefore the
	authors of \cite{FM2011} have neither stated any properties
	of this approach nor provided an analysis or given any reasons why one should use this 
	method; in particular, the two main advantages of the secant method over the biscetion approach
	that we have listed above seem to have been unnoticed so far.
\end{remark}

\begin{remark}
	Our basic approach 
	requires us to always replace the older of the two previous initial values, viz.\ $\tilde y_0^{(k-2)}$,
	by the newly computed value $\tilde y_0^{(k)}$ and then proceed to the next iteration with the
	pair $(\tilde y_0^{(k-1)}, \tilde y_0^{(k)})$ of approximate initial values.
	In the case when $y^*$ is inside of the interval bounded by $\tilde y^{(k-1)}(b)$ and 
	$\tilde y^{(k-2)}(b)$, we know that  we have obtained the guaranteed enclosure
	\[
		y(a) \in \left [\min \{ \tilde y^{(k-1)}_0, \tilde y^{(k-2)}_0 \}, 
					\max\{ \tilde y^{(k-1)}_0, \tilde y^{(k-2)}_0 \} \right ]
	\]
	for the exact initial value. Our algorithm does not guarantee such an enclosure in any
	iteration, so we do not know whether 
	\[
		y(a) \in \left [ \min \{ \tilde y^{(k)}_0, \tilde y^{(k-1)}_0 \}, 
					\max\{ \tilde y^{(k)}_0, \tilde y^{(k-1)}_0 \} \right]
	\]
	and we do not care.
	This might be seen as a disadvantage of our scheme when compared to the bisection method.
	Obviously, it is possible to modify our approach so that 
	it retains the enclosure property once it has obtained containment: 
	Instead of always replacing the older of the two previous values, we just have to replace the 
	value that is on the same side of the exact solution as the new one, thus obtaining the method known
	as \emph{regula falsi} (false position method). However, this procedure
	comes at the price of potentially requiring a significantly higher number of iterations before an
	acceptable accuracy has been reached. Therefore, we do not advise here to pursue this idea further.
\end{remark}

Figure \ref{fig:alg} shows a visualization of the proportional secting iterations for the terminal
value problem 
\[
	D_0^\alpha y(t) = \frac 1 {t+1} \sin (t \cdot y(t)) \
	\text{ with } \ y(20) = y^* = 0.8360565,
\]
that we shall discuss in more detail in Example \ref{ex8} below.
Our interval of interest is thus $[a,b] = [0, 20]$.
We start with $\tilde y_0^{(0)} = y^* \approx 0.836$ at $a = 0$ and obtain the black approximate
solution graph that arrives at $\tilde y_0(b) \approx 0.58$ for time $b = 20$. 
For the sake of simplicity, we follow the suggestion of Remark \ref{rem:hatc-simple}
and construct the next initial value $\tilde y_0^{(1)}$ based on formula \eqref{eq:nextguess1} 
with $\hat c = 1$.
Since $\tilde y_0(b) \approx 0.58 < 0.836\ldots = y^*$, this moves the initial
value exactly $y^* - \tilde y_0(b)$ units up to obtain $\tilde y^{(1)}_0 \approx 1.1$ 
and subsequently $\tilde y_1(b) \approx 0.84$ when traveling along the blue graph. 
And the next solution graph from $\tilde y^{(2)}_0 \approx 1.03$ (shown in red) arrives about halfway 
between $\tilde y_1(b)$ and $y^*$ at time~$b$. 
The dotted graph finally reaches $y^*$ at $b$ in 8 iterations with a $10^{-15}$ error. 
An absolute error of approximately $10^{-10}$ at the terminal point $b$ takes 7 iterations,
and 6 iterations suffice to obtain a $10^{-7}$ accuracy
for this example. Thus, each extra iteration
gives us between 3 and 4 more accuracy digits at time $b$.

\begin{figure}[htb]
	\centering
	\includegraphics[width = 0.95\textwidth]{Ex4multicurvesKaiUh2.png}
	\caption{\label{fig:alg}Visualization of the behavior of the algorithm when applied to Example
		\ref{ex8}. The dotted curve is the numerical solution after 8 iterations
		of our shooting method; it cannot be visually distinguished from the exact solution.}
\end{figure}



\subsection{Selection of the Step Size for the Numerical IVP Solver}

To reduce the computational cost of any overall algorithm, 
 \cite{Di2015} proposed to vary the step size of the numerical method in the 
following way: When the iteration counter $k$ for the ``shots'' in a shooting method is
small, i.e.,\ in the early stages of the iteration, one may assume to be relatively far away 
from the exact solution because the current value of the initial value is not sufficiently accurate
yet. Therefore, it does not make sense to solve the initial value problem with high accuracy,
and so in this stage one can afford to use a relatively large step size. Our numerical experiments
described in Section~\ref{sec:num} indicate that no such administratively complicated step size varying procedure
is necessary for either one of the three variants of our approach because with proportional secting 
we always arrive at a very accurate solution in a small number (3 to 8 rather than 18 to 65) of shooting 
iterations anyway, so not much can be gained by choosing a coarse mesh in the early stages.
We therefore just choose a fixed step size in advance and use it in all iterations  
throughout the entire process.

\subsection{Algorithmic Description of the Proportional Secting Scheme}

Writing down the method described above in a more formal pseudo-code like manner,
we obtain the procedure listed in Algorithm \ref{alg:propsec} below.


\begin{algorithm}[htb]
	\caption{Shooting method based on proportional secting\label{alg:propsec}}
	\renewcommand{\algorithmicrequire}{\textbf{input data:}}

	\begin{algorithmic}
	\REQUIRE{\ 
	\begin{itemize}
	\item the terminal value problem \eqref{eq:tvp}, i.e.,\ the function $f$ on the
		right-hand side of the differential equation and the terminal value $y^*$
	\item a numerical method for solving fractional initial value problems
		(e.g., the fractional Adams method, the fractional trapezoidal method 
			or a fractional BDF)
	\item a step size $h$ to be used in connection with the numerical IVP solver
		(or, more generally, an arbitrary set of mesh points for the
		discretization of the fundamental interval $[a,b]$)
	\item a desired accuracy level $\varepsilon > 0$\smallskip
	\item a decision about the strategy for choosing the value $\hat c$ in eq.~\eqref{eq:nextguess1},
		the three options being the generic choice $\hat c = 1$ and the procedures indicated in 
		Remarks \ref{rem:propconst} and \ref{rem:propconstauto}, respectively
	\end{itemize}
	}
	\hrule
	\smallskip

	\STATE{set $k := -1$}
	\REPEAT
		\STATE increment $k$ by $1$
		\IF{$k=0$}
			\STATE set $\tilde y_0^{(k)} := y^*$
		\ELSIF{$k=1$}
			\STATE set $\tilde y_0^{(k)}$ according to formula \eqref{eq:nextguess1}
		\ELSE
			\STATE set $\tilde y_0^{(k)}$ according to formula \eqref{eq:c3}
		\ENDIF
		\STATE solve the initial value problem $D_a^\alpha \tilde y_k(t) = f(t, \tilde y_k(t))$, 
			$\tilde y_k(a) = \tilde y_0^{(k)}$,
			with the given numerical method and the chosen step size $h$
			or the given mesh, respectively
	\UNTIL{$|y^* - \tilde y_{k}(b) | \leq \varepsilon$}
	\RETURN the numerical solution $\tilde y_k$
	\end{algorithmic}
\end{algorithm}



\section{Numerical Results}
\label{sec:num}

In this section, we  present some numerical results obtained by our new scheme 
(with all three variants of choosing the value $\hat c$ required in eq.~\eqref{eq:nextguess1}
when computing the second guess for the initial value)
and compare them with the conventionally used shooting method with
traditional bisection. To compute these results, we implemented and executed the algorithms
under consideration in MATLAB R2022a on a notebook with an Intel Core i7-8550U
CPU clocked at 1.8~GHz running under Windows 10.

In all cases, we have tested the shooting methods with two different solvers for the initial value problems,
the Adams-Bashforth-Moulton scheme and the second order backward differentiation formula \cite{Lu1985}. 
The Adams method was implemented in a P(EC)$^m$E structure with four corrector iterations
\cite{Di2003}. The BDF2 is an implicit method and hence needs to solve a nonlinear equation at each
time step to compute the corresponding approximate solution. To this end, we use Garrappa's implementation of the
algorithm \cite{Ga2015}. It employs
a Newton method that is terminated when two successive values differ  by 
less than $10^{-10}$. As indicated in the general description above, all shooting iterations are  
terminated when the approximate solution at the final point $b$ of the interval $[a,b]$  
differs by at most $\varepsilon$ from the desired value of $y^*$. Here we  have varied
$\varepsilon = 10^{-6}$, $\varepsilon = 10^{-8}$ and $\varepsilon = 10^{-10}$.
 
The tables below list the chosen initial value problem solver together with the corresponding step size,
the maximal error over the interval of interest and the number of iterations that each of the shooting methods
needed with the respective combination of IVP solver and step size to converge up to the required accuracy. 
In this context, we note that, since the shooting strategies---and hence the sequences of the chosen 
initial values---differ from each other, the approximate solutions computed by the four different approaches
do not coincide exactly. Therefore, the respective maximal errors are also not precisely identical.
However, our general observation in this context is that, at least for $\varepsilon = 10^{-8}$ and
$\varepsilon = 10^{-10}$, the maximal errors agree with each other at least within the accuracy 
listed in the tables. For $\varepsilon = 10^{-6}$, the variations in the errors are somewhat larger
but the values are still in a common order of magnitude; 
in this case the corresponding columns in the tables always list the errors for the worst of the four approaches.



% Throughout all the tables given in this section, we use a notation of the form $2.9(-7)$ to indicate
% the number $2.9 \cdot 10^{-7}$, etc.

%%%%%\end{document}  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{example}
	\label{ex3}
	Our first example is the terminal value problem
	\begin{eqnarray*}
		D_0^\alpha y(t)
		& = & \frac{8!}{\Gamma(9-\alpha)} t^{8-\alpha} 
			- 3 \frac{\Gamma(5+\alpha/2)}{\Gamma(5-\alpha/2)} t^{4-\alpha/2} \\
		& & {} + \frac 9 4 \Gamma(1+\alpha) + \left(\frac 3 2 t^{\alpha/2} - t^4 \right)^3 - |y(t)|^{3/2}, \\
		y(1) &=& \frac 1 4,
	\end{eqnarray*}
	whose exact solution is 
	\[
		y(t) = t^8 - 3 t^{4+\alpha/2} + \frac 9 4 t^\alpha.
	\]
	This is a standard example used for testing numerical methods in fractional
	calculus; cf., e.g., \cite{DFF2002}.
	We report the results for the special case $\alpha = 0.3$ in Tables
	\ref{tab:ex3a}, \ref{tab:ex3b} and \ref{tab:ex3c} below.
\end{example}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex3a}Computational cost and accuracy obtained when
		solving Example \ref{ex3} for $\alpha = 0.3$ with different numerical
		methods and $\varepsilon = 10^{-6}$.}
	\centering
	% \input{tableNew-ex3-epsLMM10-epsShoot06.tex}
	\input{fulltable-ex3-epsLMM10-epsShoot06.tex}
\end{table}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex3b}Computational cost and accuracy obtained when
		solving Example \ref{ex3} for $\alpha = 0.3$ with different numerical
		methods and $\varepsilon = 10^{-8}$.}
	\centering
	% \input{tableNew-ex3-epsLMM10-epsShoot08.tex}
	\input{fulltable-ex3-epsLMM10-epsShoot08.tex}
\end{table}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex3c}Computational cost and accuracy obtained when
		solving Example \ref{ex3} for $\alpha = 0.3$ with different numerical
		methods and $\varepsilon = 10^{-10}$.}
	\centering
	% \input{tableNew-ex3-epsLMM10-epsShoot10.tex}
	\input{fulltable-ex3-epsLMM10-epsShoot10.tex}
\end{table}

From the results shown in Tables \ref{tab:ex3a}, \ref{tab:ex3b} and \ref{tab:ex3c}, we can see the following:
\begin{itemize}
\item Varying the strategy for selecting the next initial guess (i.e., switching between classical
	bisection and proportional secting) but not changing the IVP solver
	does not have an influence on the final result. This is expected because a change of this
	strategy essentially means that we try to solve the same nonlinear equation by a different
	iteration method, and this should not lead to a different limit.
\item There is, however, a substantial difference in the number of iterations that the two
	strategies require to obtain a result with the desired accuracy. Indeed, the classical 
	bisection always needs a relatively large number of iterations, thus leading to a rather
	high computational cost. The proportional secting method, in contrast, performs very much better;
	roughly speaking, it typically requires only between 10\% and 25\% of the classical 
	bisection's effort.
\item A rough-and-ready measurement of the run times of the algorithms 
	in the environment described above reflects this speedup.
	\begin{itemize}
	\item For example, in the case $\varepsilon = 10^{-10}$ shown in Table \ref{tab:ex3c},
		the Adams method with a stepsize of 0.001 required 1.53 s to converge when combined with
		classical bisection, but only 0.15~s in combination with the first variant of proportional secting
		($\hat c = 1$ in eq.~\eqref{eq:nextguess1}), 
		or 0.21 s with either of its two other variants (as specified in Remarks \ref{rem:propconst} and
		\ref{rem:propconstauto},	respectively). 
		The fact that the simple choice $\hat c = 1$ is faster than the two others is a consequence
		of the observation that the latter require a relatively time consuming (approximate)
		calculation of the quantities $c^*$ and possibly also $c_*$ of eq.~\eqref{eq:propconst}.
	\item Similarly, when we used the BDF2 solver for the initial value problems with a stepsize
		of $0.0005$ then we measured run times of 3.38 s for the classical bisection but only 
		0.34 s and 0.43 s, respectively, for the first and for the two other variants of the 
		proportional secting algorithm.
	\end{itemize}
\item The differential equation in this example is not dissipative. For the proportionality factor $\hat c$
	that we require to compute the second guess for the initial value, we get the lower bound
	$c_* \approx 0.23$ and the upper bound $c^* \approx 2.27 \cdot 10^4$. This inclusion interval is
	very large, and therefore the strategy of Remark~\ref{rem:propconst} 
	gives us only a rather unprecise approximation of the correct value of $\hat c$. Nevertheless,
	this variant of our new algorithm converges in a small number of iterations, and indeed
	the alternative versions (Remark \ref{rem:propconstauto} or $\hat c = 1$) need the same number of iterations.
\end{itemize}

\begin{example}
	\label{ex5}
	In our second example, we consider the terminal value problem
	\[
		D_0^\alpha y(t) =  - \frac 3 2 y(t) \ \text {with} \ 
		y(7) = \frac {14} 5 E_\alpha \left( - \frac 3 2 \cdot 7^\alpha \right) \approx 0.6476
	\]
	again with $\alpha = 0.3$, where (as above) $E_\alpha$ is the
	Mittag-Leffler function of order $\alpha$. 
	The exact solution in this case is 
	\[
		y(t) = \frac {14} 5 E_\alpha \left( - \frac 3 2 t^\alpha \right) .
	\]
	As in the previous example, we report the results in Tables
	\ref{tab:ex5a}, \ref{tab:ex5b} and \ref{tab:ex5c}.
\end{example}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex5a}Computational cost and accuracy obtained when
		solving Example \ref{ex5} for $\alpha = 0.3$ with different numerical
		methods and $\varepsilon = 10^{-6}$.}
	\centering
	% \input{tableNew-ex5-epsLMM10-epsShoot06.tex}
	\input{fulltable-ex5-epsLMM10-epsShoot06.tex}
\end{table}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex5b}Computational cost and accuracy obtained when
		solving Example \ref{ex5} for $\alpha = 0.3$ with different numerical
		methods and $\varepsilon = 10^{-8}$.}
	\centering
	% \input{tableNew-ex5-epsLMM10-epsShoot08.tex}
	\input{fulltable-ex5-epsLMM10-epsShoot08.tex}
\end{table}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex5c}Computational cost and accuracy obtained when
		solving Example \ref{ex5} for $\alpha = 0.3$ with different numerical
		methods and $\varepsilon = 10^{-10}$.}
	\centering
	% \input{tableNew-ex5-epsLMM10-epsShoot10.tex}
	\input{fulltable-ex5-epsLMM10-epsShoot10.tex}
\end{table}

These tables allow the following observations:
\begin{itemize}
\item In Example \ref{ex5}, the exact solution satisfies $y(b) = y(7) \approx 0.6476$ 
	and $y(a) = y(0) = 14/5 = 2.8$, so $y(b)$ is a relatively poor initial guess for $y(0)$.
	In this example  we have used a significantly larger interval than in Example~\ref{ex3}.
	None of these changes  leads to significant difficulties for any of the tested methods. 
\item As far as the performance comparison between bisection and proportional secting is concerned, 
	the findings are essentially the same as or even better than those for Example \ref{ex3}: Proportional secting 
	reaches the required accuracy in a significantly smaller number of iterations. Indeed, in the
	cases that we have tested for this example, proportional secting requires only between 8.3\% and 
	15\% of the iterations that the bisection method needs. The run times
	decrease by similar amounts.
\item For both shooting strategies, the accuracy of the second-order backward differentiation formula 
	is much better than the accuracy obtained by the Adams method.
\item This example contains a dissipative differential equation. More precisely, the differential 
	equation is linear, homogeneous and has the constant coefficient $-3/2$ which is negative.
	For such equations with constant coefficients, the proportionality factor $\hat c$ discussed
	in Remark \ref{rem:propconst} is particularly simple to compute. In fact, we can see that 
	in this case the lower bound $c_*$ coincides with the upper bound; both of them have the
	value $0.23\ldots$ which we can  use for $\hat c$. From the results in the
	tables, this leads to a slight reduction in the number of required
	iterations (although not by much because in general the use of the other option already
	leads to a very small number of iterations with little room for improvement).
\end{itemize}

\begin{example}
	\label{ex8}
	The third example is based on the initial value problem
	\[
		D_0^\alpha y(t) = \frac 1 {t+1} \sin (t \cdot y(t)), \qquad
		y(0) = 1,
	\]
	now with $\alpha = 0.7$.
	The exact solution in this case is not known, but using a numerical approach
	(specifically, a second order backward differentiation formula with 16,000,000 
	steps, i.e.,\ a stepsize of 0.00000125 = 1/800,000), we can compute the approximate
	solution shown in Figure \ref{fig:alg} (the curve highlighted with the dots) 
	and be very confident that it is extremely 
	close to the exact solution. In particular, we find that $y(20) \approx 0.8360565$.
	Replacing the initial condition by this terminal condition, we obtain another terminal
	value problem that we have also solved with the approaches under consideration
	in this paper. This problem may appear to be somewhat more challenging than
	those discussed in Examples \ref{ex3} and \ref{ex5} because we work on an
	even longer interval and because of the decaying oscillatory nature of the
	exact solution.
	As in the previous examples, we report the results in Tables
	\ref{tab:ex8a}, \ref{tab:ex8b} and \ref{tab:ex8c}.
	In view of the lack of precise information about the exact solution, the
	computed errors are based on comparing the results obtained with our methods
	with the numerical solution mentioned above.
\end{example}

%\begin{figure}[htb]
%	\centering
%	\includegraphics[width=0.75\textwidth]{example8}
%	\caption{\label{fig:ex8}Plot of the solution to the
%		problem discussed in Example \ref{ex8}.}
%\end{figure}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex8a}Computational cost and accuracy obtained when
		solving Example \ref{ex8} for $\alpha = 0.7$ with different numerical
		methods and $\varepsilon = 10^{-6}$.}
	\centering
	% \input{tableNew-ex8-epsLMM10-epsShoot06.tex}
	\input{fulltable-ex8-epsLMM10-epsShoot06.tex}
\end{table}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex8b}Computational cost and accuracy obtained when
		solving Example \ref{ex8} for $\alpha = 0.7$ with different numerical
		methods and $\varepsilon = 10^{-8}$.}
	\centering
	% \input{tableNew-ex8-epsLMM10-epsShoot08.tex}
	\input{fulltable-ex8-epsLMM10-epsShoot08.tex}
\end{table}

\begin{table}[!htb]
	\footnotesize
	\caption{\label{tab:ex8c}Computational cost and accuracy obtained when
		solving Example \ref{ex8} for $\alpha = 0.7$ with different numerical
		methods and $\varepsilon = 10^{-10}$.}
	\centering
	% \input{tableNew-ex8-epsLMM10-epsShoot10.tex}
	\input{fulltable-ex8-epsLMM10-epsShoot10.tex}
\end{table}

From these tables, we once again recognize a similar behaviour as in Examples \ref{ex3} and \ref{ex5}: The
proportional secting method is substantially faster than the classical bisection method in the sense that it requires 
a much smaller number of iterations to converge up to the requested accuracy. The same observation also 
applies to the run time measurements. For example, when choosing $\varepsilon = 10^{-8}$ 
(see Table \ref{tab:ex8b}) and a BDF2 solver with stepsize $0.02$, the run time was 
0.48~s for the classical bisection method. In contrast, our proportional secting strategy 
needed only 0.14~s for the variant with $\hat c = 1$ in eq.~\eqref{eq:nextguess1}, 
0.18~s when $\hat c$ in eq.~\eqref{eq:nextguess1} was chosen as proposed in Remark \ref{rem:propconst},
and 0.15~s when the idea of Remark \ref{rem:propconstauto} was used to compute $\hat c$.

There is, however, a significant difference in Example \ref{ex8} when compared to Example \ref{ex5}:
Using the strategy of Remark \ref{rem:propconst} for computing the 
second guess for the initial value leads to convergence,
but it is slightly worse than when simply using $\hat c = 1$
as suggested in Remark \ref{rem:hatc-simple}.
This differential equation is not dissipative, and its
containment interval bounds given by \eqref{eq:propconst} are
$c_* \approx 0.05$ and $c^* \approx 5 \cdot 10^7$. Thus, the first containing interval 
for the correct proportionality factor is extremely large
and the midpoint method of Remark \ref{rem:propconst} then starts with a very
large error and so a relatively poor second approximate solution for which more iterations
are needed until convergence to the correct value.


\section{Conclusion}

We have discussed shooting methods for  numerically solving fractional 
terminal value problems. Our main focus was not on choosing the best  numerical
IVP solver. This has already been discussed 
in \cite{FM2011,Ga2010,Ga2015,Ga2018}. Rather, we have investigated 
algorithms for selecting the initial values for each iteration in  shooting
procedures. Classical bisection is often recommended, but it converges rather slowly 
and requires many iterations until approximating  the actual solution reasonably. 
The newly proposed proportional secting method is much better in this respect.
It computes the guess for the second and subsequent initial values rather differently. Three separate 
methods dealing with this aspect---differing between each other only in the definition 
of the guess for the second initial value---have been proposed and their respective performances 
differ only slightly in speed and accuracy.


\section*{Software}
The MATLAB source codes of the algorithms described in this paper, including all required
auxiliary functions, can be downloaded from a dedicated repository \cite{DGU2023} on the Zenodo 
platform.
We have tested these functions in MATLAB R2022a.


\section*{Acknowledgments}

The work described in this paper originated from a discussion between the two authors that
was initiated after a Zoom talk by the first author  in the Irish Numerical Analysis Forum (INAF) in 2021. 
We are grateful to the organizers Natalia Kopteva and Martin Stynes  for their Numerical Analysis Forum 
that brought us together by chance for this fundamental research.

Our algorithm uses some auxiliary routines implemented by Roberto Garrappa, e.g. for the
evaluation of the Mittag-Leffler function and for solving fractional initial value problems. These
routines are also of interest in their own right. We thank Roberto Garrappa for allowing us to include
his codes into our software suite \cite{DGU2023}.


\bibliographystyle{siamplain}
\bibliography{Diethelm-Uhlig}

\end{document}
