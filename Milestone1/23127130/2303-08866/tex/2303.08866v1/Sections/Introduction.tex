% Page limit 8 (pay extra for more pages)

\section{Introduction}
\IEEEPARstart{W}{ith} the recent explosion of black box machine learning models over the past few years, there has been a great demand to explain how these models work so that users can understand and trust the results \cite{confalonieri2021historical, minh2022explainable, epifano2020towards}. This is especially important in mission-critical and life-saving applications, such as clinical diagnosis and medical decision-making \cite{zhang2022applications, van2022explainable, ahmed2022failure}. Explainable AI (XAI) seeks to give a user more trust in the model explanation \cite{nielsen2022robust}. The problem is that we do not know if we can trust the explanations either. In order for XAI to be viable for real world applications there must be trust, thus there have been many attempts to evaluate these explanations \cite{hooker2019benchmark, nourelahi2022explainable, mamalakis2022investigating, hama2022deletion, zhou2021evaluating, phan2022deepface, nielsen2022robust}. 

These application areas also demand robust machine learning models that resist natural noise in the input data and malicious or adversarial attacks \cite{dera2021premium, carannante2021trustworthy, Dera:MLSP2019}. In real-world tasks, the data can often come with noise and artifacts that the machine learning model might not have previously seen \cite{Dera:IEEERadar2019}. There might even be nefarious actors trying to confuse the model with adversarial examples \cite{dera2021premium, waqas2021exploring}. This requires that models be built and trained to resist noise and attacks \cite{waqas2021brain, carannante2021trustworthy}. There are several approaches to building robust machine learning models, including training on noisy datasets and Bayesian models \cite{dera2021premium, Dera:MLSP2019, Dera:IEEERadar2019, waqas2021exploring, ahmed2022failure, carannante2021trustworthy}. The first approach only focuses on model training and data processing, while the second alters the neural network architecture and introduces probability distribution functions over learnable parameters \cite{dera2021premium}. Robust models have been shown to produce more visually plausible explanations \cite{nourelahi2022explainable, tsipras2019robustness}. However, the quantitative evaluation of the faithfulness of these explanations is a challenging task. There are many criteria for evaluating explanations. In this paper, we propose a much more direct approach (called EvalAttAI) that eliminates the errors introduced by existing approaches to evaluating the faithfulness of explanations. The contributions in this paper are as follows:
\begin{itemize}
    \item We develop a new method of evaluating the faithfulness of an attribution map, which is much different from existing approaches and avoids the errors present in current methods. Our proposed method is denoted as Evaluating Attributions by Adding Incrementally (EvalAttAI).
    \item We compare our method to the state-of-the-art in the literature and show why ours is a more fair and accurate measure of faithfulness of model explanations. 
    \item We relate the concepts of faithfulness and robustness by showing whether robust models produce more faithful explanations when evaluated using our proposed EvalAttAI method.
\end{itemize}

The article is organized as follows. In Section II, we provide definitions of various terms that are used in the article and present various explainability faithfulness evaluation methods, including our newly proposed, EvalAttAI. Section III provides an overview of our methods and simulation experiments. In Section IV, we present results and discuss the same in Section V before concluding in Section VI.

\section{Explainability Metrics and Definitions} 
% \subsection{Measurable Aspects of Explainability} 
%% Aspects of attribution maps that can be measured

To evaluate the explainability method defined for a machine learning model, quantify its faithfulness, and link it to the robustness of the machine learning model, we must understand different concepts, terms, and metrics that are used in the literature. This section provides an overview of these important concepts.


\subsection{Robustness}

A model is said to be robust if its performance shows little decrease when the distribution of the test data is different from that used during the training. A model can be robustified (made more robust) by training it on a large variety of data that includes various types of noisy and adversarial images.

\subsection{Attribution Maps}

Explainability methods for image processing applications creates a pixel-wise array of attribution scores that indicate each feature's (a pixel of the input image in most cases) importance. This is known as an attribution map or an explainability map \cite{ancona2019gradient}.

\subsection{Sensitivity}

Sensitivity of an explanation method (i.e., attribution map generation method, such as Vanilla Gradient) is defined to be how much an explainability map changes when small perturbations are applied to the input of the model \cite{nauta2022anecdotal}. 
Attribution methods that produce explanations with large sensitivity scores are likely to produce dramatically different explanations when noise is introduced at the input of the model. This means that, in general, robust models tend to perform better than their non-robust counterparts when the attribution map is evaluated for sensitivity \cite{nielsen2022robust}. Generally, attribution methods with lower sensitivity produce more similar and consistent results in the presence of imperceptible noise than methods with higher sensitivity. More consistent explanations may be beneficial in many applications, but it does not reveal how important the pixels in the attribution map actually are to the model \cite{nielsen2022robust}. This motivates the examination of other metrics for evaluating explainability methods.

\subsection{Plausibility}

The first instinct is to visually examine an explainability map to see whether the attribution makes sense to a human observer \cite{nielsen2022robust, nauta2022anecdotal}. This is called plausibility \cite{nauta2022anecdotal}, which tells the users how visually convincing the explanation is to a human. For example, an attribution map that has many important pixels within the object of interest would be more plausible than an explanation that focuses on features that are irrelevant to a human. A plausibility metric has a subjective component, since humans define what are the important features according to their own visual perception. There are multiple methods which give a plausibility score corresponding to the number of important pixels that fall within a human drawn region containing the object  \cite{zhang2018top, zhou2016learning, raatikainen2022weighting}. It has been shown that robust and adversarially trained models tend to produce more plausible explanations \cite{nourelahi2022explainable, nielsen2022robust, zhang2019interpreting, chalasani2020concise}. However, plausibility can be misleading \cite{nielsen2022robust}. The explanations that look more reasonable to a human might misrepresent the features that the trained machine learning model uses for its internal processing and decision-making. Therefore, it is important to introduce more quantitative metrics that can help us evaluate and understand the ``goodness'' or ``faithfulness'' of an explanation. 

\subsection{Faithfulness and Fidelity}

The faithfulness of an explanation is defined to be a measure of how accurate the explanation (an attribution map in our case) is to the model itself. An explanation that is very faithful will show the user what is \emph{truly} most important to the model. In other words, faithfulness tells us the extent to which pixels deemed to be important in the attribution map are \emph{actually} important to the model. 


In many recent works, fidelity and faithfulness were considered synonymous \cite{markus2021role, nguyen2020quantitative, velmurugan2021evaluating}. We argue that fidelity metrics are used to measure the faithfulness of the attribution map (or the explanation). Many different techniques have been proposed in the literature to measure the faithfulness or fidelity \cite{markus2021role, nguyen2020quantitative, velmurugan2021evaluating, yeh2019fidelity, ge2021counterfactual, petsiuk2018rise, hama2022deletion, nourelahi2022explainable, zhou2021evaluating, phan2022deepface, samek2016evaluating}. However, there is no current consensus on the best way to evaluate faithfulness of attribution maps.

The \emph{Fidelity} metric measures the correlation between the pixel attribution scores and the drop in prediction or accuracy scores when the pixel is altered or removed. This shows how well the attribution map ranks the pixels by their importance. The most common implementations of fidelity metrics involve replacing important pixels with another pixel value (most often black or the image mean) \cite{petsiuk2018rise, hama2022deletion, nourelahi2022explainable, zhou2021evaluating, samek2016evaluating, phan2022deepface}. This introduces error since the model has not been trained to understand arbitrary pixels being introduced in the input image. There have been attempts to rectify this by retraining the model for possible changes in the pixels \cite{hooker2019benchmark}. However, methods based on retraining introduce another type of error, the explanation (attribution map) is no longer being evaluated using the same model parameters (that is, the model has been modified due to retraining). Later in this work, we show that our proposed EvalAttAI eliminates the errors present in these existing methods. The reason is linked to the fact that EvalAttAI does not remove pixels, but rather perturbs pixels to a small, almost imperceptible degree. 


\begin{figure*}[htpb]
     \centering
     \begin{subfigure}[h]{0.85\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/deletion.png}
         \caption{}
         \label{fig:del_methods}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.85\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion.png}
         \caption{}
         \label{fig:ins_methods}
     \end{subfigure}
     \hfill
        \caption{Two different metrics (Insertion and Deletion) for assessing various attribution maps are presented. The variable $t$ indicates the percentage of pixels deleted/inserted where $t = 0$ refers to zero pixel and $t = 1$ refers to all pixels. Removed pixels are replaced by the channel mean. (a) Deletion and (b) Insertion examples using Vanilla Gradient (VG) and random Gaussian noise as the attribution maps.}
        \label{fig:del_ins_explained}
\end{figure*}

\subsection{Model Robustness and Attribution Faithfulness, Fidelity and Plausibility}

Using faithfulness, fidelity and plausibility, we will seek to shed more light on whether robust models are more explainable. As mentioned earlier, robust models have been shown to be more plausible. However, there is no consensus on whether the attribution maps of robust models are more faithful or whether there is a standard approach to evaluating faithfulness. %In this regard, this paper proposes a novel technique to evaluate faithfulness (EvalAttAI).

\subsection{Attribution Evaluation Methods}

The most prominent faithfulness evaluation metrics currently include Insertion \cite{samek2016evaluating} and Deletion \cite{phan2022deepface, petsiuk2018rise, samek2016evaluating}. These methods are based on replacing pixels, and recently it was reported that their results appear contradictory \cite{nourelahi2022explainable}. That is, when Deletion performed well, Insertion did not and vice versa.


\subsubsection{Deletion}
Deletion is the most widely used method of evaluating faithfulness of attribution maps and has many variations \cite{phan2022deepface, petsiuk2018rise, samek2016evaluating}. Deletion involves (1) incrementally removing pixels, (2) replacing removed pixel spaces with an arbitrary value, and (3) evaluating the resulting change in model predictions. The order in which the pixels are chosen is based on attribution scores of these pixels. The faithfulness of the attribution map is evaluated based on the change in the model prediction. However, when replacing pixels with an arbitrary value, an error is introduced. The replaced pixels create a sharp contrast in the image, which the model was never trained to process. There have been attempts to fix this by retraining the model for each increment \cite{hooker2019benchmark}. However, retraining introduces new errors since the attributions are being evaluated on a newly trained model. EvalAttAI avoids this issue by perturbing the pixels rather than removing them and replacing with arbitrary values.

% The model predictions are usually evaluated by the logit or softmax activation scores. However, we believe that evaluating based on accuracy rather than activations is a better approach since it shows whether the model can still accurately evaluate the image. For this reason, we will be using accuracy for our own Deletion experiments. 

\subsubsection{Insertion}
Insertion is another method of evaluating the faithfulness of attribution maps. This method uses the same process as Deletion, but starts with all pixels removed from the input image and incrementally inserts the most important ones back. All the issues that pertain to Deletion also apply to Insertion. Even worse, these two appear to produce contradictory results showing that these methods may not be evaluating the same thing, that is, the faithfulness of the attribution methods \cite{nourelahi2022explainable}.

\subsubsection{\lowercase{c}-Eval}

The c-Eval approach \cite{vu2021ceval} evaluates attribution maps by perturbing the most important pixels using an adversarial attack and recording how this perturbation affects model predictions. However, not all pixels in an adversarial attack have the same impact on the output. Therefore, selecting only the most important pixels to perturb, is prone to error. Moreover, the adversarial attacks are designed to be applied to all pixels in the image, so only applying these to some pixels breaks the continuity. EvalAttAI alleviates this by applying the perturbation to all pixels. EvalAttAI also takes a much more direct approach than c-Eval by perturbing the pixels using attribution scores, rather than adversarial attacks that are unrelated to the attribution maps and explainability.

% Our novel approach fixes these problems by perturbing the pixels by the attribution itself and applying them to the entire image. 
% This is a much more direct approach.

% \subsubsection{Pointing Game (PG) and Weakly Supervised Localization (WSL)} % will add if needed

\subsection{ Evaluating Attributions by Adding Incrementally (E\lowercase{val}A\lowercase{tt}AI)}

Our approach to evaluating the faithfulness of explainability methods is based on adding a scaled attribution map to the input image incrementally. We show that EvalAttAI can serve as a new metric to significantly improve user trust in machine learning models. This is especially critical in medical and clinical applications. 

In this work, we focus on gradient-based explainability methods to show the applicability of EvalAttAI. However, EvalAttAI can be used for all other attribution generation methods without any changes. In general, for gradient-based methods, the attribution scores are calculated by taking the gradient of the activation score (logit or soft-max class probability values) with respect to the input features via backpropagation \cite{ancona2019gradient}. Since we will be perturbing pixels to decrease activation scores, it makes sense to look at gradient-based methods. Moreover, gradient-based attribution methods are derived directly from the loss function, which is used to train the model. Thus, these methods can be considered appropriate and effective for our proposed evaluation metric. We consider that the Vanilla Gradient method of explainability \cite{simonyan2014deep} will be the most faithful, since it works directly on the trained model without any further modification or changes \cite{nielsen2022robust}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=1.00\columnwidth]{Images/perturb_eval_diagram.pdf}
    \caption{A schematic layout of the process of testing an attribution map using the proposed Evaluating Attributions by Adding Incrementally (EvalAttAI) method. The column labeled $\boldsymbol{x}_{s+1}$ depicts the modified input image, and $\boldsymbol{x}_s$ (s = 0) shows the clean image in the first row and the modified image from the previous iteration on all subsequent rows. The attribution map is indicated by $\boldsymbol{a}$.}
    \label{fig:faith_diagram}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=1.00\columnwidth]{Images/del_ins_evalattai_imgs.pdf}
    \caption{A visualization of each evaluation method is shown. Each image is formatted as depicted before being input into the neural network. Deletion incrementally removes and replaces pixels from the original image, and Insertion adds the original pixels back from a pixel removed image. EvalAttAI takes an entirely different approach by shifting pixel values according to the importance scores on the attribution map.}
    \label{fig:del_ins_evalattai_compared}
\end{figure}
