
\section{Results}
In this section, we present results to show how well various attribution evaluation methods including EvalAttAI perform. In order to ensure a fair comparison, we normalize the recorded output accuracy such that the random line (baseline method) is always equal to one. Any time that normalization is discussed in this paper, it is being used to explain how the results were formatted after they were collected. For instance, in Figs. \ref{fig:del_ins_results1}, \ref{fig:del_ins_results2}, \ref{fig:faith_plots1} and \ref{fig:faith_plots2}, we show the results before and after normalization of the data such that we can use the random line as a baseline across all models.


\begin{figure*}[htpb]
    \centering
    \centering
     \begin{subfigure}[h]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/AUC_faith_cifar10.pdf}
         \caption{CIFAR10 dataset}
         \label{fig:faith_cifar}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/AUC_faith_pathmnist.pdf}
         \caption{PathMNIST dataset}
         \label{fig:faith_path}
     \end{subfigure}
     \begin{subfigure}[h]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/AUC_faith_dermamnist.pdf}
         \caption{DermaMNIST dataset}
         \label{fig:faith_derma}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/AUC_faith_bloodmnist.pdf}
         \caption{BloodMNIST dataset}
         \label{fig:faith_blood}
     \end{subfigure}
    \caption{Faithfulness results for EvalAttAI evaluation. Bar plots of the area under the curve (AUC) for the normalized CIFAR10, PathMNIST, DermaMNIST and BloodMNIST data (from MedMNIST dataset \cite{medmnistv1}\cite{medmnistv2}) results shown in Figs. \ref{fig:faith_plots1} and \ref{fig:faith_plots2}. Both subfigures include the 95\% confidence interval. The lower the bar, the more faithful the explanations.}
    \label{fig:faith_auc}
\end{figure*}


\subsection{Deletion and Insertion}

The goal of this first experiment is to quantify the behavior of Deletion and Insertion metrics for evaluating various types of attribution methods using three different types of trained models (ResNet, Robust-ResNet, and VDP-CNN). These models represent a non-robust model (ResNet), a robustly-trained model (Robust-ResNet), and a Bayesian model trained using the VDP technique (VDP-CNN). The test accuracy and normalized test accuracy are presented in Fig. \ref{fig:del_ins_results1}. The AUC of normalized test accuracy are presented in Fig. \ref{fig:del_ins_results2}. The error bars represent the $95\%$ confidence interval in both figures. 

The results for Deletion metric as presented in Fig. \ref{fig:del_ins_results1} ((a) and (c)) show each of the methods performing better than randomly removing pixels. We see this most clearly in Fig. \ref{fig:auc_del} where AUC is presented. The one exception to that is GradCAM, which performs worse (with statistical significance) than random. However, when we look at the Insertion results in Fig. \ref{fig:del_ins_results1} ((b) and (d)) and Fig. \ref{fig:auc_ins}, we see that GradCAM is the \textit{only} method which performs better than random (with statistical significance). In fact, when ranking the performance of the methods, we find that both metrics show contradictory behavior. Various attribution methods are ranked according to their performance and presented in Table \ref{fig:tables_eval}. This table clearly shows that the methods that are ranked best by the Deletion metric may be the worst when evaluated using the Insertion metric and vice versa.

\subsection{E\lowercase{val}A\lowercase{tt}AI}

The goal of this second experiment is two-fold. The first is to test the consistency and validity of the proposed EvalAttAI method. The second is to test whether the proposed methods capture the relationship between the faithfulness of the attribution method and the robustness of the model. All results with the EvalAttAI approach are presented in Figs. \ref{fig:faith_plots1}, \ref{fig:faith_plots2} and \ref{fig:faith_auc}.

In Figs. \ref{fig:faith_plots1} and \ref{fig:faith_plots2}, we note the slope of the accuracy drop for each attribution method. The attribution methods that cause the steepest drop, and therefore lowest AUC, are deemed to be the most faithful. The best performing methods can be clearly identified when looking at Fig. \ref{fig:faith_auc}. We can see that Vanilla Gradient and SmoothGrad are the only two that consistently perform better than random baseline, with statistical significance across all models. Other attribution methods, including  Grad x Image, Guided Backprop, Integrated Gradients, and GradCAM do not perform well as compared to the random baseline. We also observe that EvalAttAI produces consistent results for each attribution method across all models in most of the cases. 


\begin{table}[htp]
    % \centering
    \caption{Rankings of various attribution methods for faithfulness evaluation based on AUC using CIFAR10 dataset and three different models. }
    \label{fig:tables_eval}
    \setlength{\tabcolsep}{3pt}
    \centering
    \begin{tabular}{rccc}
\hline \hline
    \multicolumn{1}{l}{}   & \multicolumn{1}{c|}{\hspace{0.5cm}ResNet\hspace{0.65cm} } & \multicolumn{1}{c|}{Robust-ResNet} & VDP-CNN \\ \hline
\hline
    \multicolumn{1}{l}{}        &  \multicolumn{1}{l}{}                          & 
    \textbf{Deletion}   &
    \multicolumn{1}{l}{} \\ 
\hline \hline
    \multicolumn{1}{r|}{1} & \multicolumn{1}{c|}{IG}     & \multicolumn{1}{c|}{GBP}           & IG                    \\ \hline
    \multicolumn{1}{r|}{2} & \multicolumn{1}{c|}{SG}     & \multicolumn{1}{c|}{IG}            & VG                    \\ \hline
    \multicolumn{1}{r|}{3} & \multicolumn{1}{c|}{IxG}    & \multicolumn{1}{c|}{SG}            & GBP                   \\ \hline
    \multicolumn{1}{r|}{4} & \multicolumn{1}{c|}{GBP}    & \multicolumn{1}{c|}{IxG}           & SG                    \\ \hline
    \multicolumn{1}{r|}{5} & \multicolumn{1}{c|}{VG}     & \multicolumn{1}{c|}{GC}            & IxG                   \\ \hline
    \multicolumn{1}{r|}{6} & \multicolumn{1}{c|}{GC}     & \multicolumn{1}{c|}{VG}            & GC                    \\ \hline
    \hline
    \multicolumn{1}{l}{}        & \multicolumn{1}{l}{}                          & \textbf{Insertion}   & \multicolumn{1}{l}{} \\ 
    \hline \hline
    \multicolumn{1}{r|}{1} & \multicolumn{1}{c|}{GC}     & \multicolumn{1}{c|}{GC}            & GC                    \\ \hline
    \multicolumn{1}{r|}{2} & \multicolumn{1}{c|}{VG}     & \multicolumn{1}{c|}{VG}            & IxG                   \\ \hline
    \multicolumn{1}{r|}{3} & \multicolumn{1}{c|}{SG}     & \multicolumn{1}{c|}{IxG}           & VG                    \\ \hline
    \multicolumn{1}{r|}{4} & \multicolumn{1}{c|}{GBP}    & \multicolumn{1}{c|}{SG}            & SG                    \\ \hline
    \multicolumn{1}{r|}{5} & \multicolumn{1}{c|}{IxG}    & \multicolumn{1}{c|}{GBP}           & IG                    \\ \hline
    \multicolumn{1}{r|}{6} & \multicolumn{1}{c|}{IG}     & \multicolumn{1}{c|}{IG}            & GBP                   \\ \hline
    \hline
    \multicolumn{1}{l}{}        & \multicolumn{1}{l}{}                          & \textbf{EvalAttAI}   & \multicolumn{1}{l}{} \\ 
    \hline \hline
    \multicolumn{1}{r|}{1} & \multicolumn{1}{c|}{SG}     & \multicolumn{1}{c|}{VG}            & VG                    \\ \hline
    \multicolumn{1}{r|}{2} & \multicolumn{1}{c|}{VG}     & \multicolumn{1}{c|}{SG}            & SG                    \\ \hline
    \multicolumn{1}{r|}{3} & \multicolumn{1}{c|}{IxG}    & \multicolumn{1}{c|}{GBP}           & GBP                   \\ \hline
    \multicolumn{1}{r|}{4} & \multicolumn{1}{c|}{IG}     & \multicolumn{1}{c|}{IxG}           & IxG                   \\ \hline
    \multicolumn{1}{r|}{5} & \multicolumn{1}{c|}{GBP}    & \multicolumn{1}{c|}{IG}            & IG                    \\ \hline
    \multicolumn{1}{r|}{6} & \multicolumn{1}{c|}{GC}     & \multicolumn{1}{c|}{GC}            & GC                    \\ \hline
    \multicolumn{4}{p{220pt}}{\small{Method names are represented as follows: IG = Integrated Gradient, SG = SmoothGrad, IxG = Input x Grad, GBP = Guided Backprop, VG = Vanilla Gradient, GC = GradCam.}}
    \end{tabular}
    \label{fig:table_EvalAttAI}
    % \centering
\end{table}


\section{Discussion}
We aimed to answer whether robust neural networks were more explainable, considering the potential usefulness of robust models and their visual explanations in medical imaging \cite{nielsen2022robust}. However, the question is difficult to answer due to three interrelated phenomena that need consideration. The first one is the robustness of the machine learning models, which is difficult to define and quantify \cite{dera2021premium, Dera:IEEERadar2019, Dera:MLSP2019, waqas2021exploring}. The second deals with various explainability methods and their internal complexities \cite{nielsen2022robust}. The third is related to metrics or measures employed to evaluate the plausibility and faithfulness of these explainability methods that try to explain the behavior of neural networks \cite{nielsen2022robust}. These three aspects of explainability research in deep neural networks are intimately intertwined. It may be challenging to disentangle these three (i.e., the robustness of models, internal dynamics of explainability methods, and metrics to evaluate this relationship) to understand the underlying relationship between robustness and explainability.

We started by limiting ourselves to two types of robust neural networks, (1) ResNet models trained using noisy datasets and (2) Bayesian deep neural networks trained using VDP technique. We acknowledge that the robustness of deep neural networks can be defined in many different ways, and consequently, many types of robust models can be built. However, we argue that these two methods represent a large class of robust models \cite{dera2021premium, carannante2020self, carannante2021trustworthy, Dera:MLSP2019, Dera:IEEERadar2019, ahmed2022failure}. On the other hand, for the explainability methods, we restricted ourselves to well-known gradient-based methods that are routinely used in image applications \cite{nielsen2022robust}. There are many different approaches to building explanations for elucidating the behavior of deep neural networks on test datasets. However, gradient-based methods represent a significantly large class of explainability methods. Finally, we wanted to use Insertion and Deletion metrics to evaluate the ``goodness'' or faithfulness of various attribution methods and quantify the explainability of robust neural networks. However, given the contradictory results produced by these two metrics due to their inherent nature, we proposed a new metric, EvalAttAI, to evaluate attribution methods.   


Recently, Nourelahi et al. analyzed the faithfulness of various attribution methods for robust and non-robust CNNs \cite{nourelahi2022explainable}. The authors used Deletion and Insertion metrics for the evaluation of faithfulness. The faithfulness results appear to show methods and models which perform best on Deletion, perform the worst on Insertion, and vice versa \cite{nourelahi2022explainable}. We get similar results. Based on the working principles of these metrics (refer to Fig. \ref{fig:del_ins_explained}), the logical conclusion would be that both methods might quantify different things in an attribution method. Perhaps, Insertion and Deletion may not be capturing any helpful information, as evident in Figs. \ref{fig:del_ins_results1} and \ref{fig:del_ins_results2}. The contradictory findings for Deletion and Insertion are likely the result of an error introduced by removing and replacing the pixels, since the machine learning model is not trained on image data containing modified features (considering each pixel as a feature). We argue that given such a discrepancy, Deletion and Insertion may not be reliable metrics for evaluating faithfulness. The proposed EvalAttAI introduces perturbations smoothly and continuously, thus, avoiding abrupt changes in feature (pixel) values. These continuous and smooth changes are controlled using the $\varepsilon$ parameter as defined in Eq. \ref{eq:addattr}.  

A summary of our results is presented in Fig. \ref{fig:faith_auc} which compares three models (ResNet, Robust-ResNet, and Bayesian VDP-CNN) for four datasets (three of medical images and one of natural images), six attribution methods and a random baseline. In all sub-figures, the y-axes present AUC numbers calculated using test accuracy values for the proposed EvalAttAI metric and error bars represent $95\%$ confidence interval. We do not observe any significant trend showing that any model (among ResNet, Robust-ResNet, or VDP-CNN) is more explainable than others across all tested attribution methods and datasets. For some attribution methods, robust models are more explainable, but not for others. On the other hand, we note that Vanilla Grad (orange color bars) and SmoothGrad (blue color bars) consistently perform better than all other attribution methods. Amongst these two attribution methods,  we note that Bayesian neural networks (VDP-CNN) are more explainable. Based on these results, we can conclude that Bayesian CNNs (trained using the VDP technique \cite{dera2021premium}) are more explainable than the standard and robustly trained neural networks when attributions are generated using Vanilla Gradient.

We also observed that the Vanilla Gradient consistently performed better than all other methods on our evaluation metric. We argue that this is expected since this method works directly on the neural network without any alterations or modifications.

These findings expand upon the best practices presented in our previous work \cite{nielsen2022robust}, where we discussed considerations for the researchers when choosing an attribution method. We also discussed how robustness could play a significant role in generating plausible-looking explanations that may not be faithful. Based on this work, we suggest that the Vanilla Gradient should be used as the primary method of generating attributions. Instead of developing new ways to create visually appealing explanations using various operations, the community should focus on improving the robustness of the machine learning models using Bayesian approaches or other training methods.

\section{Conclusions}
In this work, we introduce a new metric for testing the faithfulness of attribution methods while showing the inconsistency and unreliability of the current state-of-the-art approaches. Our experiments are performed on both natural and medical image datasets. Our proposed faithfulness evaluation metric, EvalAttAI, shows consistent results. Our evaluation found that the Vanilla Gradient and SmoothGrad performed consistently better than all other attribution methods. We could not find compelling evidence that all robust models are more explainable across the board. However, our experiments consistently show that Bayesian CNNs (trained using the VDP framework) were more explainable than all other models when used with the best performing attribution method (Vanilla Gradient). 
