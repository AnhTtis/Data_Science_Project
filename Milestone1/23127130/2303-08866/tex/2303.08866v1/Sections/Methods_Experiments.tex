

 
\begin{figure*}[h]
     \centering
     %%%%%% Non-Robust %%%%%%
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/del_det.pdf}
         \caption{Deletion ResNet}
         \label{fig:del_det}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/ins_det.pdf}
         \caption{Insertion ResNet}
         \label{fig:ins_det}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/del_det_norm.pdf}
         \caption{Deletion Norm ResNet}
         \label{fig:del_norm_det}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/ins_det_norm.pdf}
         \caption{Insertion Norm ResNet}
         \label{fig:ins_norm_det}
     \end{subfigure}
     \hfill
     %%%%%% Robust %%%%%%
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/del_rob.pdf}
         \caption{Deletion Robust-ResNet}
         \label{fig:del_rob}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/ins_rob.pdf}
         \caption{Insertion Robust-ResNet}
         \label{fig:ins_rob}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/del_rob_norm.pdf}
         \caption{Deletion Norm Robust-ResNet}
         \label{fig:del_norm_rob}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/ins_rob_norm.pdf}
         \caption{Insertion Norm Robust-ResNet}
         \label{fig:ins_norm_rob}
    \end{subfigure}
    \hfill
    %%%%%% VDP %%%%%%    
    \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/del_vdp.pdf}
         \caption{Deletion VDP-CNN}
         \label{fig:del_vdp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/ins_vdp.pdf}
         \caption{Insertion VDP-CNN}
         \label{fig:ins_vdp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/del_vdp_norm.pdf}
         \caption{Deletion Norm VDP-CNN}
         \label{fig:del_norm_vdp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/ins_vdp_norm.pdf}
         \caption{Insertion Norm VDP-CNN}
         \label{fig:ins_norm_vdp}
     \end{subfigure}
        \caption{Evaluating faithfulness of multiple different explainability attribution methods using Deletion and Insertion metrics. Each attribution method was evaluated using 5000 CIFAR10 images with three trained models. The top, middle, and bottom rows depict the standard trained ResNet-18 model, robustly-trained ResNet-18 (Robust-ResNet), and a Bayesian CNN trained using the VDP method respectively. The black line in all sub-figures show the behavior of the model when pixel are randomly deleted/inserted. The 95\% confidence interval is shown in each sub-figure using error bars. (Left) Deletion. (Center left) Insertion. (Center right) Deletion after normalization (abbreviated as Norm) with random baseline. (Right) Insertion after normalization with random baseline. A method is considered to be performing well on the Deletion method if the accuracy drops significantly more than random. Inversely, a method that is performing well on the Insertion metric will show the accuracy increase more than random.}
        \label{fig:del_ins_results1}
\end{figure*}

\section{Methods and Experiments}

This section describes the models, datasets and experimental protocol used in our study.


\subsection{Models and Datasets}
    
There are three models considered in our study. The first two are ResNet18 \cite{he2016deep} with different training scenarios. (1) The standardly trained ResNet18 uses a dataset with no modifications to the training data. (2) For the robustly trained ResNet18, random Gaussian noise is added to each input with a signal-to-noise ratio (SNR) of 5 dB. (3) The third model is a standardly trained Bayesian deep neural network, based on Variational Density Propagation (VDP-CNN) \cite{dera2021premium, Dera:MLSP2019, Dera:IEEERadar2019}. This model follows the same architecture as ResNet18, but propagates both the mean and the covariance of the probability distribution function defined over the model parameters through each layer of the model.

All three models were trained on the CIFAR10 dataset \cite{krizhevsky2009learning}. The ResNet18 standard model was trained until it achieved an $84\%$ validation accuracy. The robustly trained ResNet18 model achieved a validation accuracy of $70\%$. The VDP-CNN was trained on CIFAR10 to achieve a validation accuracy of $68\%$. 

The EvalAttAI method was also evaluated on three medical imaging datasets, which are part of MedMNIST. The dataset included PathMNIST, DermaMNIST, and BloodMNIST \cite{medmnistv1, medmnistv2}. All three are multi-class prediction datasets consisting of 9, 7, and 8 labels for PathMNIST, DermaMNIST, and BloodMNIST, respectively. The datasets consist of images that are classified to aid in the diagnosis of colon, skin and blood cancers. The images from the datasets used all 3 color channels. This was chosen because the ResNet architecture is designed to classify colored image data.

The standard trained ResNet18 model was trained on PathMNIST to $85\%$, DermaMNIST to $73\%$ and BloodMNIST to $92\%$ validation accuracy. The robustly trained ResNet18 was trained on PathMNIST to $86\%$, DermaMNIST to $73\%$ and BloodMNIST to $93\%$ validation accuracy. Lastly, the VDP model was trained on PathMNIST, DermaMNIST and BloodMNIST to get $76\%$, $72\%$, and $84\%$ validation accuracies, respectively.

We evaluated six attribution methods on each model including (1) Vanilla Gradient \cite{simonyan2014deep}, (2) Grad x Image \cite{ancona2019gradient}, (3) Guided Backprop \cite{springenberg2015striving}, (4) Integrated Gradients \cite{sundararajan2017axiomatic}, (5) SmoothGrad \cite{smilkov2017smoothgrad} and (6) GradCAM \cite{selvaraju2017gradcam}. For the GradCAM, we selected the first convolution layer. This means that the gradient was backpropogated to the first layer before performing global average pooling, linear combination and ReLU, resulting in the GradCAM attribution map. All attribution methods were generated using Captum \cite{kokhlikyan2020captum, kokhlikyan2019captumpytorch}, except for Vanilla Gradient and SmoothGrad which were implemented by the authors using built-in PyTorch functions.

\subsection{Deletion and Insertion Methods}

The Deletion metric evaluates how much the accuracy changes when important pixels are removed from the input image \cite{phan2022deepface, petsiuk2018rise, samek2016evaluating}. Pixels can be replaced with various values, but are often replaced with the image mean. The pixel importance is determined by the magnitude of the attribution scores on the map being evaluated. One starts with a clean image and incrementally removes pixels starting at the highest attribution score. The attribution maps are considered to be performing well if the accuracy drops significantly faster than the random baseline, which implies a smaller area under the curve (AUC). In our opinion, the AUC satisfactorily captures how well each method is performing overall by accounting for the performance over all increments using a single score. The random baseline consists of random Gaussian noise that is tested in place of an attribution map. The process of Deletion can be seen visually in Figs. \ref{fig:del_methods} and \ref{fig:del_ins_evalattai_compared}.

The Deletion experiment begins with a clean image. Then, starting from the most important according to attribution score, the pixels are removed in $5\%$ increments. The first increment $0\%$ evaluates the accuracy of the image with no pixels removed (i.e., the clean image). The next increment finds the accuracy with $5\%$ of the pixels in the image removed. This is continued until $45\%$ (almost half) of the pixels in the image are removed. The pixels are replaced with the mean of each color channel, respectively.

Insertion \cite{samek2016evaluating} uses the same premise as Deletion. The difference is that one starts with a blank image consisting of all removed pixels, and incrementally introduces the most important pixels. The more significantly the accuracy increases compared to the random baseline, the better the attribution map is performing with Insertion.
The Deletion and Insertion metrics may introduce error, since the models are not trained to be able to interpret missing and replaced pixels. In fact, the values that are chosen to replace the pixels can drastically alter the model accuracy in unintended ways. Insertion is depicted visually in Figs. \ref{fig:ins_methods} and \ref{fig:del_ins_evalattai_compared}.

For Insertion, we begin at $0\%$ with an image where all pixels are replaced by the per channel mean of the dataset. We then introduce pixels in $5\%$ increments until $45\%$ of the image is restored. The entire image can be restored, but the most meaningful change in accuracy occurs at the beginning of the replacement with the most important pixels.

 \begin{figure}[htpb]
     \centering
     \begin{subfigure}[h]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/AUC_Deletion_all_methods.pdf}
         \caption{Deletion metric (lower is better)}
         \label{fig:auc_del}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/insertion_deletion/AUC_Insertion_all_methods.pdf}
         \caption{Insertion metric (higher is better)}
         \label{fig:auc_ins}
     \end{subfigure}
        \caption{Area under the curve (AUC) calculated using normalized accuracy curves (from Figs. \ref{fig:del_norm_det}, \ref{fig:ins_norm_det}, \ref{fig:del_norm_rob},
        \ref{fig:ins_norm_rob}, \ref{fig:del_norm_vdp} and \ref{fig:ins_norm_vdp}) for various models and attribution methods are presented. The models include a non-robust model (ResNet), a robustly-trained model (Robust-ResNet), and a Bayesian robust model (VDP-CNN). For the Deletion metric, lower values means the evaluation method and model are more explainable. For the Insertion metric, higher scores are better. The $95\%$ confidence interval is shown in each sub-figure.}
        \label{fig:del_ins_results2}
\end{figure}
 
\begin{figure*}[htpb]
     \centering
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/CIFAR10/det.pdf}
         \caption{CIFAR10 ResNet}
         \label{fig:faith_det_cifar}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/CIFAR10/robust.pdf}
         \caption{CIFAR10 Robust-ResNet}
         \label{fig:faith_rob_cifar}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/CIFAR10/vdp.pdf}
         \caption{CIFAR10 VDP-CNN}
         \label{fig:faith_vdp_cifar}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/CIFAR10/det_norm.pdf}
         \caption{CIFAR10 Normalized ResNet}
         \label{fig:faith_det_norm_cifar}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/CIFAR10/robust_norm.pdf}
         \caption{CIFAR10 Normalized Robust-ResNet}
         \label{fig:faith_rob_norm_cifar}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/CIFAR10/vdp_norm.pdf}
         \caption{CIFAR10 Normalized VDP-CNN}
         \label{fig:faith_vdp_norm_cifar}
    \end{subfigure}
         %%%%%% PathMNIST %%%%%%
    \hfill
    \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/pathmnist/det.pdf}
         \caption{PathMNIST ResNet}
         \label{fig:faith_det_path}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/pathmnist/robust.pdf}
         \caption{PathMNIST Robust-ResNet}
         \label{fig:faith_rob_path}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/pathmnist/vdp.pdf}
         \caption{PathMNIST VDP-CNN}
         \label{fig:faith_vdp_path}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/pathmnist/det_norm.pdf}
         \caption{PathMNIST Normalized ResNet}
         \label{fig:faith_det_norm_path}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/pathmnist/robust_norm.pdf}
         \caption{PathMNIST Normalized Robust-ResNet}
         \label{fig:faith_rob_norm_path}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/pathmnist/vdp_norm.pdf}
         \caption{PathMNIST Normalized VDP-CNN}
         \label{fig:faith_vdp_norm_path}
     \end{subfigure}
        \caption{Results of the EvalAttAI metric tested using CIFAR10 and PathMNIST on three models, ResNet18, Robust-ResNet18 and VDP-CNN. The y-axis of all sub-figures shows average accuracy as attributions are added to the input image, which is captured by the x-axis. Lower accuracy values represent that the attribution method is faithful, and the metric is meant to capture this aspect. The $95\%$ confidence interval is shown in each plot using error bars. (a)-(c) and (g)-(i) Non-normalized accuracy. (d)-(f) and (j)-(l) Normalized accuracy using random as the baseline.}
        \label{fig:faith_plots1}
\end{figure*}

\begin{figure*}[htpb]
     \centering
     %%%%%% DermaMNIST %%%%%%
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/dermamnist/det.pdf}
         \caption{DermaMNIST ResNet}
         \label{fig:faith_det_derma}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/dermamnist/robust.pdf}
         \caption{DermaMNIST Robust-ResNet}
         \label{fig:faith_rob_derma}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/dermamnist/vdp.pdf}
         \caption{DermaMNIST VDP-CNN}
         \label{fig:faith_vdp_derma}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/dermamnist/det_norm.pdf}
         \caption{DermaMNIST Normalized ResNet}
         \label{fig:faith_det_norm_derma}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/dermamnist/robust_norm.pdf}
         \caption{DermaMNIST Normalized Robust-ResNet}
         \label{fig:faith_rob_norm_derma}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/dermamnist/vdp_norm.pdf}
         \caption{DermaMNIST Normalized VDP-CNN}
         \label{fig:faith_vdp_norm_derma}
    \end{subfigure}
         %%%%%% BloodMNIST %%%%%%
    \hfill
    \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/bloodmnist/det.pdf}
         \caption{BloodMNIST ResNet}
         \label{fig:faith_det_blood}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/bloodmnist/robust.pdf}
         \caption{BloodMNIST Robust-ResNet}
         \label{fig:faith_rob_blood}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/bloodmnist/vdp.pdf}
         \caption{BloodMNIST VDP-CNN}
         \label{fig:faith_vdp_blood}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/bloodmnist/det_norm.pdf}
         \caption{BloodMNIST Normalized ResNet}
         \label{fig:faith_det_norm_blood}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/bloodmnist/robust_norm.pdf}
         \caption{BloodMNIST Normalized Robust-ResNet}
         \label{fig:faith_rob_norm_blood}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Images/bloodmnist/vdp_norm.pdf}
         \caption{BloodMNIST Normalized VDP-CNN}
         \label{fig:faith_vdp_norm_blood}
     \end{subfigure}
        \caption{Results of the EvalAttAI metric tested using DermaMNIST and BloodMNIST on three models, ResNet18, Robust-ResNet18 and VDP-CNN. The y-axis of all sub-figures shows average accuracy as attributions are added to the input image, which is captured by the x-axis. Lower accuracy values correspond to the attribution method being more faithful. The $95\%$ confidence interval is shown in each plot using error bars. (a)-(c) and (g)-(i) Non-normalized accuracy. (d)-(f) and (j)-(l) Normalized accuracy using random as the baseline.}
        \label{fig:faith_plots2}
\end{figure*}
 

\subsection{E\lowercase{val}A\lowercase{tt}AI} 

% Evaluation Attributions by Adding Incrementally (EvalAttAI)

Our method of evaluating the faithfulness of attributions takes a much different approach than Deletion and Insertion by not removing pixels at all. Our approach is to perturb pixels proportionally to the pixel scores of the attribution map. This is done by adding a scaled down attribution map to the original input image and then passing the new image through the machine learning model. The accuracy of the trained model before and after adding the perturbations are used to assess the faithfulness of each attribution method. We can see how EvalAttAI looks visually in comparison to Deletion and Insertion in Fig. \ref{fig:del_ins_evalattai_compared}. We expect that the more faithful the attribution method is, the more the accuracy will decrease after the perturbation. This implies that the maps which cause the steepest drop in accuracy are considered the most faithful. This is because highly faithful attribution maps will identify and perturb the most important pixels first, which is verified by the significant drop in the model accuracy. However, if unimportant pixels are perturbed, then the accuracy will not drop as much, perhaps not at all. This will indicate that the attribution map produced by the explainability method is less faithful. The experiments that we performed also evaluate a baseline map, which consists of random Gaussian noise with a standard deviation of $0.25$ and a mean of $0$.

The EvalAttAI method is described in Eq. \ref{eq:addattr}. In our experiments, the attribution map ($\boldsymbol{a}$) is multiplied by a scaling variable epsilon ($\varepsilon$), which we set to $0.1$, resulting in a scaled down attribution map. Starting with an image ($\boldsymbol{x}_s$), we add the scaled attribution, resulting in the modified image ($x_{s+1}$). This is done in an iterative fashion until the desired number of steps ($s$) are completed. The process always begins at $s = 0$ with the clean image ($\boldsymbol{x}_0$). Figure \ref{fig:faith_diagram} and Eq. \ref{eq:addattr} describe the process, which is repeated until the desired number of steps ($s$) are completed.

\begin{equation}
    \boldsymbol{x}_{s+1} = \boldsymbol{x}_{s} + \varepsilon * \boldsymbol{a}
\label{eq:addattr}
\end{equation}

EvalAttAI is sensitive and faithful because the most important pixels according to the attribution map will be perturbed the most. At the same time, the least important pixels, which have attribution scores closest to $0$, will not significantly alter the corresponding pixels on the input image. This method avoids the error introduced by Deletion and Insertion, which occurs due to pixels being entirely removed and replaced. Our method can also be tuned using $\epsilon$ so that pixels are altered in even smaller increments. The EvalAttAI method can be thought of as a more continuous approach to evaluating faithfulness. 


