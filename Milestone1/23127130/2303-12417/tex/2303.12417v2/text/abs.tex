\begin{abstract}
Contrastive Language-Image Pre-training, benefiting from large-scale unlabeled text-image pairs, has demonstrated great performance in open-world vision understanding tasks. 
However, due to the limited Text-3D data pairs, adapting the success of 2D Vision-Language Models (VLM) to the 3D space remains an open problem. 
Existing works that leverage VLM for 3D understanding generally resort to constructing intermediate 2D representations for the 3D data, but at the cost of losing 3D geometry information. To take a step toward open-world 3D vision understanding, we propose \textbf{C}ontrastive \textbf{L}anguage-\textbf{I}mage-\textbf{P}oint Cloud \textbf{P}retraining (CLIP$^2$) to directly learn the transferable 3D point cloud representation in realistic scenarios with a novel proxy alignment mechanism. 
Specifically, we exploit naturally-existed correspondences in 2D and 3D scenarios, and build well-aligned and instance-based text-image-point proxies from those complex scenarios.
On top of that, we propose a cross-modal contrastive objective to learn semantic and instance-level aligned point cloud representation.
% \xd{too vague description, any more specific name?} objective for 3D point cloud representation learning. 
Experimental results on both indoor and outdoor scenarios show that our learned 3D representation has great transfer ability in downstream tasks, including zero-shot and few-shot 3D recognition, which boosts the state-of-the-art methods by large margins.   %\hjh{give some numbers}. \xh{comparing to XXX}
Furthermore, we provide analyses of the capability of different representations in real scenarios and present the optional ensemble scheme. %\jg{cannot understand the last sentence}. 



\end{abstract}

%We build the proxy alignment based on the observations that the data collections of real-scenarios are usually conducted in a cross-modal way, which naturally align 2D and 3D data by sensor calibration. 
%Specifically, we set a generic caption list as text proxies, then align the relative image proxies by 2D Vision-Language Model and obtain the corresponding 3D proxies, which eventually form the alignment between text-image-3D proxies.
%\jg{We exploit naturally-existed correspondences in paired 2D and 3D scenarios, and build well-aligned and instance-based text-image-point proxies from those complex scenarios.} 