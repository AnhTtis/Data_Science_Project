\section{Introduction}



Powerful 3D point cloud representation plays a crucial role in various real-world applications, e.g., 3D object recognition and detection~\cite{yin2020center,shi2019pointrcnn, mao20223d, ding2019votenet,zhang2020h3dnet}. 
%}autonomous vehicles\xd{autonomous vehicles are NOT applications}\cite{yin2020center,shi2019pointrcnn, mao20223d} and indoor robotics navigation \cite{ding2019votenet,zhang2020h3dnet}. 
Compared to 2D images, 3D point cloud provides specific information like accurate geometry that is robust to illumination changes. 
% Compared to 2D images, 3D point cloud 
However, current methods \cite{yin2020center,qi2017pointnet} that learn 3D representations generally rely on the predefined number of object categories and require plenty of labor-intensive annotations. 
Those learned 3D representations are insufficient for safety-critical scenarios like self-driving which includes
a long-tail class distribution far beyond the predefined taxonomy.
 Therefore, it is highly demanded to learn a transferable 3D representation equipped with zero-shot recognition ability in vocabulary scalable real-world scenes.  Figure ~\ref{fig-intro} shows an open-world recognition example by our CLIP$^2$ in outdoor and indoor scenes, where the 3D objects can be classified with the correlation alignment between 3D representations and open-world vocabularies.
%where ``Debris'' and ``Cleaner'' can be classified even if they are uncommon categories in 3D datasets. %\xd{by our CLIP$^2$ or others, make it clear}.

%\xd{However,} few methods focus on zero-shot learning in 3D\xd{people may be curious of why and feel strange. at least, we need to cite some. I do not agree there is only one work}. 
%The pioneer work \cite{cheraghian2019zero} splits the class name into known/unknown categories and evaluates the model under similar domain. This setting is not general enough for real-world scenes due to the domain gap between synthetic and realistic data, as well as the limited vocabulary increasing ability.



The critical ingredient of open-world understanding is that the models learn sufficient knowledge to obtain general representations. To achieve this, recent Vision-Language Models (VLM) \cite{radford2021learning,jia2021scaling,yao2022detclip} leverage Internet-scale text-image pairs to conduct vision-language pretraining, which facilitates transferable 2D representation and demonstrates promising performance in 2D open-vocabulary tasks.
% Cheraghian et al. \cite{cheraghian2019zero}
% pioneer to associate PointNet \cite{qi2017pointnet} with category semantic information, and separately proposed an unsupervised skewness
% loss \cite{cheraghian2019mitigating} to mitigate the hubness problem.
% But they are not suitable for realistic
% scenarios due to the domain gap between synthetic and real-world data, as well as thelimited vocabulary increasing ability. 
%Recently, Vision-Language Models (VLM) \cite{radford2021learning,jia2021scaling,yao2022detclip} have demonstrated promising performance in 2D open-vocabulary tasks that learn transferable 2D representation and acquire robust visual-semantic concepts and vocabularies based on the internet-scale text-image pairs.
However,
3D vision-language pretraining remains unexplored due to the limitation
of existing 3D datasets in diversity and scale compared to 
the massive data sources in 2D counterparts \cite{li2021align, radford2021learning, jia2021scaling,yao2022detclip}. Though some recent works \cite{huang2022clip2point,zhang2022pointclip,ha2022semantic}
try to avoid this problem by transferring the pretrained 2D VLM into
the intermediate representation including projected image patches \cite{ha2022semantic,lu2022open}
or depth maps \cite{zhang2022pointclip,afham2022crosspoint}, 
those representations suffer from the loss of 3D geometric information
and limited viewpoints under realistic scenarios. Especially the camera images are only sometimes available due to the sensor failure in 3D scenes. We believe the 3D representation based on original point cloud data retains most information and is the optimal solution for 3D real world understanding, which requires a rethink of learning the transferable 3D representation under realistic scenarios.


%To advance the research on 3D representation learning into an open-world era under realistic scenarios and resolve the limitation of text-3D data source, 
To this end,
we propose a \textbf{C}ontrastive \textbf{L}anguage-\textbf{I}mage-\textbf{P}oint cloud \textbf{P}retraining framework, short for CLIP$^{2}$,  which directly aligns 3D space with broader raw text and advances the 3D representation learning into an open-world era.
%endow model categories expansion capacity. 
Our learning process can be decomposed into two stages:
\textbf{Firstly,}  we introduce a \textit{Triplet Proxy Collection} to alleviate the limitation of accessible pretraining data by constructing language-image-point triplets from real-world scenes. Since the large-scale realistic 3D datasets for outdoor driving \cite{mao2021one,caesar2020nuscenes} and indoor scenarios \cite{dai2017scannet,song2015sun} are collected in open-world, it contains huge amounts of realistic objects that vary in semantics and diversity. Thus we consider them as potential pretraining data sources without extra human supervision. Specifically, we propose ``Proxy'' instances as the bridges between language descriptions, 2D images and 3D point clouds. Enabled by a well-aligned VLM, a scalable caption list and the geometry transformation between 2D and 3D, we automatically create more than 1 million triplets to facilitate pretraining. 
%contain huge amounts of images and point clouds but lack corresponding text descriptions. 
%Enabled by a language-image well-aligned VLM and scalable amounts of caption list, we use geometry structure information to automatically create more than 1 million language-image-point pairs \jg{cannot understand}, which we call \textit{Triplet Proxy}, from real-world 3D datasets without extra human supervision.\xh{A too long sentence}
% from practically unlimited amounts of raw text. 
% This \textit{ Triplet Proxy Collection} builds a scalable language and 3D space correlation directly. 
\textbf{Secondly,} we further propose a \textit{Cross-Modal Pretraining} scheme to jointly optimize the feature space alignments of three modalities, \textit{i.e.}point cloud, language and image. It contains both the contrastive learning objective of semantic-level text-3D correlation and instance-level image-3D correlation, which contributes to better transferability of learned 3D representation.
%Since semantic text space projected on instance-wise 3D representation is sparse, forcing the alignment between point cloud and raw text is hard. Therefore, we learn the text-3D correlation in an embedding space based on collected Triplet Proxies and add instance-sensitive alignment learning via image-3D projection to help convergence \jg{should we claim two losses like that?}. Note that instance-aware visual concept has been well-studied in the embedding space of pretrained VLM. After pretraining, we can obtain semantic and instance-level aligned language-3D representation. And visual concept benefits from web-scale data are implicitly distilled in our model. \jg{cannot understand}

We study the transferable capability of CLIP${^2}$ by benchmarking the zero-shot recognition performance on four popular indoor and outdoor real-world datasets, and find a significant improvement over current methods, achieving Top1 accuracy 61.3\% on SunRGBD \cite{song2015sun}, 43.8\% on ScanNet \cite{dai2017scannet}), 28.8\% on nuScenes \cite{caesar2020nuscenes} and 56.0\% on ONCE \cite{mao2021one}. For a fair comparison with existing methods \cite{zhang2022pointclip, huang2022clip2point, afham2022crosspoint, wang2021unsupervised}, we conduct zero-shot and few-shot classification on single object dataset ScanObjectNN \cite{uy2019revisiting} and find consistent dominance, 16.1\% relative improvement on zero-shot classification over previous state-of-the-art method \cite{huang2022clip2point}. To validate the vocabulary-increasing ability of CLIP${^2}$, we report the quantity results and visualizations to show the improved discovery of the long-tail categories.
%, xx\% improvement on ScanNet 384-extended vocabulary than others. 
%Moreover, we make an analysis of the effect of different representations in realistic applications. Our contributions can be summarized as follows:
%Moreover, we make ablations on representation learning and analyse the recognition abilities of different representations. We investigate the ensembling alternatives and boost the performance over single representation, which enables the complementary knowledge merging of all available representations.
Moreover, we make ablations and analisis on different representations, and investigate ensembling alternatives to merge complementary knowledge of all available representations in realistic applications. Our contributions can be summarized as follows:
%To enables the complementary knowledge merging of all available representations in realistic applications, we investigate ensembling alternatives to boost the performance over single representation. Our contributions can be summarized as follows:

\begin{itemize}
    \item We propose a novel CLIP$^2$ framework that aligns 3D space with open-world language representation, facilitating zero-shot transfer in realistic scenarios.
    \item We present a Triplet Proxies Collection scheme in real-world scenes, which alleviates the shortage of text-3D data sources and facilitates the pretraining methods.
    \item CLIP$^2$ jointly optimizes the correlation alignment between point cloud, language and image by proposed cross-modal pretraining mechanism, which enhances the transferability of learned 3D representation.
    \item Our CLIP$^{2}$ achieves the state-of-the-art zero-shot transfer performance on 5 datasets (indoor/outdoor scenes and single-object) and shows quality results on vocabulary-increasing discovery in real world.
    %\item We make the first attempt to collect Triplet Proxies for alleviating the shortage of text-3D data source and form a more flexible and scalable zero-shot 3D recognition benchmark in open-world scenarios. %\xd{will be released or forms a benchmark?}
    %\item We propose a novel CLIP$^2$ scheme that forms the correlation between language and point cloud representation directly, which is successfully enhanced from both the instance and semantic perspectives in a cross-modal alignment.
    %% \item We propose a novel CLIP$^{2}$ scheme that jointly optimize the alignment between three different modalities. This mechanism successfully enhances the correlation between language and 3D representation from both the instance and semantic perspectives.
    %\item Our CLIP$^{2}$ achieves the state-of-the-art transferable zero-shot performance on 5 datasets (indoor/outdoor scenes and single-object) and shows quality results on vocabulary-increasing discovery.
    %, i.e., xx\% and xx\% top1 accuracy on nuScenes and ScanNet respectively.
\end{itemize}


