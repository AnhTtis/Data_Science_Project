\section{Method}
In this section, we introduce CLIP${^2}$ to learn a transferable 3D point cloud representation with arbitrary category recognition ability under realistic scenarios, illustrated in Figure \ref{fig-model}. We will first present the \textit{Triplet Proxy Collection} in Section~\ref{sec:tp}, which utilizes a pretrained VLM and geometric transformation to obtain language-image-point triplets from real-world scenes. Then we will elaborate \textit{Cross-Modal Contrastive Pretraining} mechanism in Section~\ref{sec:learn}, which jointly optimizes the alignment correlations between language, image and point cloud feature space. %Finally, we present the optional ensemble scheme in Section \ref{sec:ensemble}.



\subsection{Triplet Proxy Collection} \label{sec:tp}

Inspired by the significant performance of 2D VLMs on open-vocabulary tasks, we aim to develop 3D vision-language pretraining to facilitate category-increasing capacity for real-world scenarios. 
However, the core challenge is the shortage of pretraining data. 
Compared to the 2D vision-language pretraining framework CLIP~\cite{radford2021learning}, which takes more than 400M image-language pairs from the Internet, the largest 3D single-object dataset ShapeNet \cite{ye2020sarpnet} only contains 50K CAD models with 55 categories. In addition to the insufficiency of data scale,  pretraining on such synthetic data fails to transfer well in the real world due to the huge domain gap. 
Enlightened by the recent emergence of large-scale point cloud datasets collected in indoor~\cite{song2015sun, dai2017scannet} and outdoor scenarios~\cite{mao2021one,caesar2020nuscenes}, we observe that those naturally-collected datasets potentially contain vast amounts of open-world objects that vary in semantics and diversity. Considering the data collection itself is cheap except for laborious annotation, we novelly take leverage of those available datasets without human annotations as a practical yet effective pretraining data source.
%Recently, there are some large-scale autonomous driving \cite{mao2021one,caesar2020nuscenes} and RGB-D \cite{song2015sun, dai2017scannet} datasets publicly available including a greater number and variety of objects. Even though 3D scenes datasets lack enough text description, 
% the visual concepts of pretrained VLM can be regarded as the bridge between 3D and raw text. 

Specifically, given the realistic scene data $\mathcal{S} = \{(P_s, I_s)_{s=1}^{|\mathcal{S}|}\}$, where  $P_s\in \mathbb{R}^{N_P \times 3}$ and  $I_s\in \mathbb{R}^{N_I\times H\times W \times 3}$ are corresponding 3D point clouds and images of scene $s$, we propose a novel concept, \textit{Proxy}, as the bridge between language, image and 3D point cloud. As illustrated in Figure~\ref{fig-model}, equipped by those proxy instances, we can automatically collect a massive number of language-image-point cloud pairs $\mathcal{D}_{\text{proxy}}$ in the format of proxies under open-world scenes. We detail the process as follows.

\begin{figure}[t]
	\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=1.0\linewidth]{fig/depth_v2.pdf}
	\end{center}
				\vspace{-4mm}
	\caption{\textbf{Illustration of three representation modals} of two 3D objects examples under indoor and outdoor scenarios. %Left: 3D point cloud. Medium: projected depth maps with differenct views. Right: corresponding image patch.
	}
	\label{fig-depth}
\end{figure}

\vspace{1mm}
\noindent\textbf{Language Proxy. }We set the language proxies $\textbf{\text{X}}^T\in \mathbb{R}^{V}$ as a raw text list from the 2D open-world dataset~\cite{gupta2019lvis}, where $V=1206$ denotes the vocabulary size of language proxies. 

\vspace{1mm}
\noindent\textbf{Image Proxy. }Next, we obtain the image proxies $\textbf{\text{X}}^I$ by an open vocabulary detector DetCLIP \cite{yao2022detclip}, denoted as $M$, which is trained with open-world data and performs open-set detection.
% by adopting the well-aligned correlations between language and image in pretrained VLM \cite{yao2022detclip}, denoted as $M$. 
%Since $M$ is pretrained under hybrid supervision from detection, grounding and image-text pair data, it can align 2D proposal in images to language phrases in a text prompt. 
% $M$ can align 2D proposals in images to language phrases by pretraining under the hybrid supervision of 2D vision-language data sources.
Concretely, given language proxies $\textbf{\text{X}}^T$ and input scene image $I_s$, we extract corresponding image proposals as image proxies $X^I_s$  with $M$ by the similarity between input language embeddings and proposal features as 
\begin{equation}
\{X^I_s\}_{s\in |\mathcal{S}|} = \text{M}(\{I_s\}_{s\in |\mathcal{S}|}, \textbf{\text{X}}^T). 
\end{equation} 

\vspace{1mm}
\noindent\textbf{3D Proxy. } We exploit the naturally-existed geometry relations between 2D and 3D scenes to obtain 3D proxies $\textbf{\text{X}}^P$, which consists of point cloud instances corresponding to image proposals in $\textbf{\text{X}}^I$. We simplify the geometry transformation as $\text{G}(\cdot)$ and formulate the relations as:
\begin{equation}
X^P_i =  \text{G}(X^I_i).
\end{equation}
Detailedly, for \textit{indoor scenes} equipped with RGB-D sensors,  
%we directly reconstruct the point coordinate $X^P_{s,i, xyz}\in\mathbb{R}^{n, 3}$ from depth map $X^I_{s,i,uvd}\in\mathbb{R}^{n, 2}$. 
we first remove the background pixels by unsupervised segmentation algorithm~\cite{rother2004grabcut} for each image proxy ${X^I_{s, i}}$, $i\in|X^I_{s}|$. 
Since depth information is known, 
we then transform the segmented pixels from $uvd$ coordinate $X^{I,uvd}_{s,i}\in\mathbb{R}^{n, 3}$ to $xyz$ coordinate $X^{P, xyz}_{s,i}\in\mathbb{R}^{n, 3}$ as a 3D point cloud proxy with the given camera parameters.
%, thus obtain the instance point cloud as point proxy $X^P_{s,i}$.
%And we keep the depth values at the corresponding positions of the remaining pixels to generate $X^I_{i, uvd}$. 
%Then, we transform the pixels uv coordinate to xyz coordinate with given camera parameter, and obtain instance point cloud as point proxy $X^P_{s,i}$. 
For \textit{outdoor scenes} captured by LiDAR sensors, we first create a 3D frustum for each image proxy by extruding the 2D image proposal into 3D space following \cite{paigwar2021frustum,qi2018frustum}. Then we conduct DBSCAN algorithm~\cite{schubert2017dbscan} within frustum and select the point cloud cluster as the point proxy $X^{P, xyz}_{s, i}$.

Eventually, we construct Triplet Proxy $\mathcal{D}_{\text{proxy}}=\{\textbf{\text{X}}^T,X_{s}^{I},X_{s}^{P}\}_{s=1}^{|\mathcal{S}|}$ by combining corresponding language proxies ${\textbf{\text{X}}^T}$, image proxies $\textbf{\text{X}}^I$ and 3D proxies $\textbf{\text{X}}^P$, where $\textbf{\text{X}}^I$=$\{{X_{s}^{I}}\}_{s=1}^{|\mathcal{S}|}$  and $\textbf{\text{X}}^P$=$\{{X_{s}^{P}}\}_{s=1}^{|\mathcal{S}|}$. 220K and 1.4M proxy triplets are formed for indoor and outdoor scenes, respectively. More details can be found in the appendix. 

\begin{figure}[t]
	\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=1.0\linewidth]{fig/comp_v2.pdf}
	\end{center}
		\vspace{-4mm}
	\caption{\textbf{Comparison of different pretraining strategies.} \textbf{(a)} CLIP aligns image and language embedding space~\cite{radford2021learning} as $L_{TI}$ based on large-scale text-image pairs. \textbf{(b)} PointClip~\cite{radford2021learning} aligns projected depth map to CLIP language space as $L_{TD}$. \textbf{(c)} Clip2Point aligns depth map to CLIP image space as $L_{ID}$. \textbf{(d)} our CLIP$^2$ aligns original 3D point cloud to both CLIP language space and image space via cross-modal objective $L_{CM}$.
	}
	\label{fig-model2}
\end{figure}


\subsection{Cross-Modal Contrastive Pretraining}\label{sec:learn}

With the triplet proxies $\mathcal{D}_{\text{proxy}}$, a straightforward pretraining objective is forcing the alignment between the embedding spaces of point cloud ${X^P_i}$ and language ${X^T_i}$ from scratch. 
However, it might not promise good transferability of learned representation, since the number of language-image-point pretraining data triplets remains two orders of magnitude smaller than the language-image pairs adopted by CLIP \cite{radford2021learning} and the vocabulary size is much more limited. 
Therefore, we design to learn the correlation alignment based on the pretrained embedding space of CLIP. 


The comparison of current pretraining strategies~\cite{zhang2022pointclip,huang2022clip2point} is illustrated in Figure \ref{fig-model2}, which is a series of 3D variants of CLIP. Notably, both existing methods exploit projected depth map as the intermediate representation of point cloud, which are respectively learned to align to language space~\cite{zhang2022pointclip} and image space \cite{huang2022clip2point}. Intuitively, as illustrated in Figure~\ref{fig-depth}, depth representation lost plenty of geometry information compared to the original point cloud, especially in outdoor scenarios. Moreover, images are sometimes unavailable for 3D objects. Thus we conduct pretraining on original 3D point cloud data as an optimal representation.

Toward learning more transferable representation, we introduce a cross-modal contrastive learning objective to jointly optimize the correlation alignment across language, image and point cloud, including \textit{Semantic-Level Language-3D Alignment} and \textit{Instance-Level Image-3D Alignment}. Specifically, the overall architecture of CLIP${^2}$, shown in Figure \ref{fig-model}, contains language encoder $E_{\theta}^T$, point cloud encoder $E_{\theta}^P$ and visual encoder $E_{\theta}^I$, which respectively embed the triplet proxies into text feature $f^T\in\mathcal{R}^{1\times C_T}$, point cloud feature $f^P\in\mathcal{R}^{1\times C_P}$ and image feature $f^I\in\mathcal{R}^{1\times C_I}$, where $C$ is the embedding dimension.
 

\vspace{1mm}
\noindent\textbf{Semantic-Level Language-3D Alignment. } In order to inherit the open-world recognization ability from pretrained CLIP \cite{radford2021learning}, we align the point cloud feature $f^P$ with text embedding $f^T$ from well-trained CLIP with Language-Point Proxy $\{X^{T}_i,X^{P}_i\}$ input. We replace \textit{classname} in the prompts, like ``point cloud of a \{\ \textit{classname} \}\ .‚Äù with raw text in proxy $X^{T}_i$ as language sentences. 
%Note that, we adopt multiple prompts as CLIP input following \cite{gu2021open}, prompt list can be found in appendix. 
% The contrastive alignment is conducted based on the
% sample features extracted by proxies.
The core idea is to drive the feature centroids of 3D instances and the corresponding text prompt closer. We compute the loss function of between of language proxy and point cloud proxy as:
\vspace{1mm}
\begin{equation}\footnotesize
      l(i,T,P) = -\log\frac{\exp(f^T_i\cdot f^P_{i}/\tau)}{\exp(f_i^T\cdot f^P_i/\tau) + \sum\limits_{j\in N, X^T_j\neq X^T_i} \exp(f_i^T\cdot f_j^P/\tau)},
\end{equation}
where $N$ is the mini-batch size, $\tau$ is the temperature co-efficient. Within a training mini-batch, the language-3D alignment objective $L(T,P)$ can be described as:
\begin{equation}\small
       L(T,P) = \frac{1}{N}\sum_{i\in N} l(i, T, P).
\end{equation}


\vspace{1mm}
\noindent\textbf{Instance-Level Image-3D Alignment. } 
In addition to the alignment between semantic language and 3D proxy instances, we further introduce the contrastive alignment between instance-wise image proxy and 3D proxy instances. Note that the instance-aware visual concept has been well-studied in the embedding space of pretrained CLIP. We believe instance-sensitive learning can contribute to further correlation learning and benefits to the transferability of learned 3D representation.
%% Thus we introduce an auxiliary contrastive alignment objective across 3D representation and image representation, in addition to the correlation learning between language representation and 3D representation, yielding fine-grained \hjh{why is fine-grained} alignment and facilitate 3D representation learning. 
The contrastive aligned objective $L(I,P)$ across point cloud and image is formulated as:
\begin{equation}\small
      l(i,I,P) = -\log\frac{\exp(f^I_i\cdot f^P_{i}/\tau)}{\exp(f_i^I\cdot f^P_i/\tau) + \sum\limits_{j\in N, j\neq i} \exp(f_i^I\cdot f_j^P/\tau)} ,
\end{equation}
\begin{equation}
      L(I,P) = \frac{1}{N}\sum_{i\in N} l(i, I, P) . 
\end{equation}

Finally, we obtain the resultant cross-modal contrastive learning objective $L_{CM}(T,I,P)$ as the combination of $L(T,P)$ and $L(I,P)$, where both alignments of semantic-level text-3D correlation and instance-level image-3D correlation are injected:

\begin{equation}\small
       L_{CM}(T,I,P) =  \lambda_1 L(T,P) + \lambda_1 L(I,P),
\end{equation}
where the hyper-parameters $\lambda_1$ and $\lambda_2$ are both set to 0.5.

