\section{Related Work}

\paragraph{Vision-Language Model. }
%Large vision language models (VLM) have shown promising performance in multiple downstream tasks, e.g., visual question answering, image/video captioning, and text-to-imagegeneration. Recent CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} push the limit by collecting million-scale image-text pairs and then training joint image-text models using contrastive learning. These models can directly transfer to a suite of classification datasets and achieve impressive performances. We attempt to transfer open-vocabulary recognition ability of pre-trained CLIP to the 3D domain, making language applicable to point cloud classification.
    Large vision language models (VLM)~\cite{li2021align, radford2021learning, jia2021scaling,yao2022detclip}  have demonstrated successful performance in downstream zero-shot tasks with the learned transferable 2D representations. CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} push the limit by collecting Internet-scale image-text pairs and then learning the correlation alignment between image and language feature space with contrastive pretraining objectives. Those models can be directly transferred to zero-shot 2D recognition and achieve impressive results. Recent DetClip~\cite{yao2022detclip} learns to align image patches to test phrases after pretraining under hybrid supervision from detection, grounding and image-text pair data, which extends the ability to localize open-vocabulary 2D proposals in images. In this paper, we  attempt to transfer the open-vocabulary ability of pre-trained VLM to the 3D domain, making language applicable to zero-shot point cloud recognition.
%These models can directly transfer to a suite of classification datasets and achieve impressive performances. We attempt to transfer open-vocabulary recognition ability of pre-trained CLIP to the 3D domain, making language applicable to point cloud classification.
%Through the contrastive learning on the  million-scale image-text pairs, those models learns the correlation alignment between 2D vision and language
%Through the constrative learning based on millions of million-scale image-text pairs, in downstream tasks by learning the correlation alignment between 2D vision and language
\begin{figure*}[t]
	\begin{center}
% 		\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=1.0\linewidth]{fig/framework.pdf}
	\end{center}
	\vspace{-3mm}
	\caption{\textbf{Overview of CLIP${^2}$ framework.} The main components contain two parts, the \textit{Triplet Proxy Collection} and the \textit{Cross-Modal Pretraining}. The defined Triplet Proxy set $\mathcal{D}_{\text{proxy}}$ consists of language captions ${\textbf{\text{X}}^T}$, corresponding image instances ${\textbf{\text{X}}^I}$ and raw 3D point cloud instances ${\textbf{\text{X}}^P}$, which come from the free data source under realistic scenarios without any labeling labor. On top of that, we pretrain a point cloud encoder ${E^P}$ with the cross-modal contrastive learning objective. Equipped with CLIP${^2}$, the learned 3D point cloud representation $F^P$ is well aligned to the language representation, which facilitates downstream zero-shot 3D transfer tasks in the real world. 
	%We construct $\mathcal{D}_{\text{proxy}}$ starting from a wide-used vocabulary list and collect image patch according to text-image correlation by a powerful VLM\hjh{why VLM can locate object}. The coarse point cloud of objects are obtained via 2D-3D projection relation. 
	%Based on the collected Triplet Proxy set, we pretrain a point cloud encoder ${E^P}$ in a cross-modal contrastive way to align 3D feature with language and image at the same time. Specifically, we narrow the gap between text and 3D feature directly. And auxiliary image-3D alignment within objects is adopt to enhance the convergence that image feature has been aligned with language by a pretrained CLIP. After pretraining, we obtain a text-triggered 3D representation with open-set recognition capability, even if the image is missing.
	}
	\label{fig-model}
\end{figure*}

\vspace{1mm}
\noindent\textbf{Zero-shot/Open-world Learning in 3D.}
Recognizing 3D objects with a large vocabulary is necessary for safety-critical autonomous driving and robotic tasks, yet remains under-explored. Cheraghian et al. \cite{cheraghian2019zero,cheraghian2019mitigating,cheraghian2022zero,cheraghian2020transductive} first attempt to associate PointNet \cite{qi2017pointnet} feature with category semantic information via a projection function, and separately proposed an unsupervised skewness loss \cite{cheraghian2019mitigating} to mitigate the hubness problem. The transductive case \cite{cheraghian2020transductive}
is discussed in which extends \cite{cheraghian2019mitigating} using a triplet loss. Notably, the above works conduct experiments on synthetic datasets and need to divide datasets into ``seen'' categories as training data and
``unseen'' categories as testing data. Thus they are not suitable for realistic scenarios due to the domain gap between synthetic and real-world data, as well as the limited vocabulary-increasing ability. Recently, inspired by the success of VLMs\cite{radford2021learning,jia2021scaling} in 2D tasks, some works~\cite{zhang2022pointclip, huang2022clip2point} propose to transfer the zero-shot recognition ability of pretrained CLIP~\cite{radford2021learning} into 3D area. PointCLIP \cite{zhang2022pointclip} directly projects point cloud into multi-view depth maps as image-like data input for pretrained CLIP to make classification predictions. While CLIP2Point~\cite{huang2022clip2point} trains an image-depth embedding on ShapeNet \cite{ye2020sarpnet} to better align the depth representation to the pretrained image space of CLIP. However, depth maps lost plenty of geometry information of the original point cloud data structure, resulting in poor performance especially in realistic scenarios. By contrast, we aim to learn transferable 3D representation based on the original point cloud data structure in realistic scenarios. 
%However, current methods under this setting is not suitable for realistic scenarios since domain gap between synthetic and real-world data and limited vocabulary increasing ability. Inspired by the success of VLMs \cite{radford2021learning,jia2021scaling} in 2D recognition and their good zero-shot ability,PointCLIP \cite{zhang2022pointclip} converts point cloud data to multi-view depth maps and align depth maps with text by pretrained CLIP to broaden vocabulary of the point cloud classifier. CLIP2Point \cite{huang2022clip2point} trains a image-depth embedding on ShapeNet \cite{ye2020sarpnet} to transfer semantic information to the 3D domain. But they have poor performance on realistic scenarios due to insufficient representation as well as the diversity of synthetic distributions. Our method aims to a novel and challenging setting, learning a robust 3D representation for open-vocabulary recognition in realistic scenarios (indoor and outdoor), without human supervision. 

\vspace{1mm}
\noindent\textbf{3D Representation Learning. }
Much progress has been made in learning a comprehensive 3D representation in an unsupervised manner. Most works \cite{afham2022crosspoint,sharma2020self,xie2020pointcontrast,liang2021exploring,pang2022masked, liu2022masked,zhang2022point,yu2022point} follow the paradigm that conducts pretraining on unlabeled datasets and then finetunes on the limited downstream annotations. Though the improved transferability of 3D representation, they can not be directly transferred to zero-shot tasks with open-world vocabularies. In this work, we conduct language-image-point cloud pretraining, which learns transferable 3D representation aligned to open-vocabulary language space to facilitate the zero-shot transfer.
