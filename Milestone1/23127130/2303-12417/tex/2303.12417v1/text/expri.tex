\vspace{2mm}
\section{Experiment}

In this section, we evaluate CLIP${^2}$ on realistic indoor and outdoor scenarios. We report the zero-shot transfer results on various datasets~\cite{song2015sun, dai2017scannet, caesar2020nuscenes, sun2020scalability, uy2019revisiting} and make further analysis on the designs of pretraining strategy.
%including zero-shot classificaiton task and few-shot classification task. Notably, our proxy generation scheme can provide unsupervised location of 3D instances, we also present the visualization results.

\input{table/main-sun}
\input{table/main-scannet}



\subsection{Zero-shot Transfer}
\paragraph{Setting. }

After pretraining, natural language is applied to reference learned 3D representation to enable following zero-shot transfer tasks. \textbf{(\romannumeral1) Zero-Shot Recognition: } we evaluate zero-shot recognition performance for realistic objects, where $K$ category names are transferred to text prompt \textquotedblleft point cloud of $\{$CLASS$\}$ \textquotedblright{} to encode the text features $F_K\in\mathbb{R}^{K\times C}$. Then the classification logits are calculated with the 3D feature $f^P$ and text features as:
\begin{equation}
    \text{logits}_i = \text{softmax}(f^P_i(F_K)^T).
\end{equation}
We present the results under both indoor and outdoor scenarios in Table~\ref{tab-main-sun}, Table~\ref{tab-main-scan} and Table~\ref{tab-main-out}, as well as the object-level benchmark in Table~\ref{tab-scanobj}. \textbf{(\romannumeral2) Open-vocabulary recognition: } we enlarge the category vocabularies of ScanNet to 249 and 384 to study the open-vocabulary recognition ability in Table~\ref{tab-ov}.  \textbf{(\romannumeral3) Open-vocabulary localization: } we study the open-vocabulary localization ability by localizing open-world 3D objects with our proxy generation process and then classifying them with our learned 3D representation, of which the visualization is illustrated in Figure~\ref{fig-vis} and evaluation results are reported in Table~\ref{tab-det}. Notably, we investigate representation ensembling alternatives to enable knowledge merging of all available representations for realistic applications, illustrated in Table~\ref{tab-ablation-ensemble}.

% \vspace{-3mm}

\subsubsection{Indoor Scenarios}
\paragraph{Datasets and details.}
We adopt the widely used indoor 3D dataset SUN RGB-D~\cite{song2015sun} as the realistic indoor scenario that provides pretraining data source, a single-view RGB-D dataset consisting of $\sim$10K scenes. To validate the  transferability of learned 3D representation, we also evaluate another popular indoor 3D dataset ScanNet~\cite{dai2017scannet}, which contains $\sim$1.5K scenes of 3D reconstructed meshes. 
We remove objects in ScanNet with less than 5 points, leaving 384 noisy categories.
 % with 384 noisy categories. 
For open-vocabulary recognition, we evaluate performance on the ScanNet 384-class set and a 249-class merged set.
% We quantitatively evaluate the ability of open-vocabulary recognition on the ScanNet 384-class set and a 249-class merged set.
In addition to the scene-wise indoor dataset, we conduct evaluations on ScanObjectNN~\cite{uy2019revisiting}, which collects $\sim$3K individual realistic objects with 15 categories and is applied in the previous zero-shot evaluation~\cite{zhang2022pointclip, huang2022clip2point}. During the proxy collection process, we empirically set $\epsilon=0.3$ in~\cite{yao2022detclip} as a tradeoff between filtering FPs and preserving TPs to generate image proxies. Considering the occurrence frequencies of different indoor categories vary a lot, we adopt the class balance strategy~\cite{mmdetection} to mitigate the class imbalance. During pretraining process, we adopt~\cite{qi2017pointnet++} as point cloud encoder and  set the overall training epoch number to 100. 



\vspace{-1mm}
\paragraph{Quantity results. } 
For zero-shot recognition task, we take two recent works as our baselines, \textit{i.e.}  PointClip~\cite{zhang2022pointclip} and Clip2Point~\cite{huang2022clip2point}, which study the zero-shot classification task on 3D object-level benchmarks~\cite{vishwanath2009modelnet, uy2019revisiting} by leveraging pretrained CLIP with projected depth maps. Focusing on the real-world scenarios, we conduct comparison not only on the realistic object-level~\cite{uy2019revisiting} as illustrated in Table~\ref{tab-scanobj} but also on the scene-level datasets shown in Table~\ref{tab-main-sun} and Table~\ref{tab-main-scan}, where the evaluation follows the common classes split in~\cite{ding2019votenet, zhang2020h3dnet} and reports the instance Top1 accuracy of each class. As shown in tables, our CLIP$^2$ can outperform baselines on all benchmarks by large margins. Besides, we apply our triplet proxy generation mechanism (TP.) to baseline methods, and achieve considerable improvements on SUN RGB-D and ScanNet by 26.5\%  and 19.8\% for PointClip, 38.3\% and 10.3\% for Clip2Point. On the one hand, the contrasts demonstrate the effectiveness of  our triplet proxies for open-world understanding. On the other hand, our learned 3D representation is superior in 3D object recognition by retaining more 3D-specific information than depth representation. Besides, we present the optional ensembling scheme (En.) when camera images are available, which can take advantage of multi-modal knowledge and further boost the performance by 8.3\%.  To further validate the open-vocabulary recognition ability, we conduct evaluation on a larger category set of ScanNet in Table \ref{tab-ov} and report the instance Top5 accuracy, which illustrates the superiority of our CLIP$^2$ when vocabulary increases. Beyond that, CLIP$^2$ is also equipped with zero-shot 3D localization ability by proxy generation. On indoor scenario SUN RGB-D, we compare with a SOTA indoor 3D detector 3DETR \cite{misra2021end} and a recent work OV3D~\cite{lu2022open} that studies open-vocabulary detection, where evaluation is conducted on the same ''unseen'' split in~\cite{lu2022open}. Since CLIP$^2$ do not fit the tight bounding boxes of point cloud instances, we estimate the maximum bounding box of proxies and GT instances to conduct evaluation following the same metrics mAP$_{25}$ and AR$_{25}$ in~\cite{lu2022open},  as shown in Table~\ref{tab-det}. Notably, compared to baseline works that train on ``seen'' 3D annotations and test on ``unseen'' categories, we have no access to any 3D annotations yet achieve comparable localization ability, which yields 5.3\% AR$_{25}$ improvement over OV3D~\cite{lu2022open}.
We further evaluate segmentation results  in Table~\ref{tab-det}.



\input{table/main-merge2}
\input{table/main-outdoor}

\vspace{-5mm}
\paragraph{Quality results. } The visualization results of CLIP$^2$ under a indoor scene of SUN RGB-D~\cite{song2015sun} is shown in Figure~\ref{fig-vis}\textcolor{red}{(a)}. Our triplet proxy generation process can localize open-world 3D objects in a point cloud scene. Moreover, the 3D representation learned from our cross-modal pretraining can provide more accurate classification results for 3D instances by exploiting original point cloud, which corrects the mistaken ''People'' prediction in image to ''Picture'' by considering the geometry information.


\begin{figure*}[t]
	\begin{center}
% 		\fbox{\rule{0pt}{3in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=1.0\linewidth]{fig/vis.pdf}
	\end{center}
	\vspace{-6mm}
	\caption{\textbf{Visualizations of the zero-shot localization and recognition results} by  CLIP$^2$ under open-world \textbf{(a)} indoor realistic scene~\cite{song2015sun} and \textbf{(b)} outdoor scenes~\cite{caesar2020nuscenes}.
     Notably, the whole pipeline of CLIP$^2$ not only has no access to human annotations, but also enables the open-world vocabularies beyond groundtruth annotations, such as 'Picture' in \textbf{(a)} and â€™Plastic bag', 'Tire' in \textbf{(b)}. Best viewed in colors. 
	} 
	\label{fig-vis}
\end{figure*}


\subsubsection{Outdoor Scenarios}
\paragraph{Datasets and details. }
We exploit a prevalent large-scale 3D dataset nuScenes~\cite{caesar2020nuscenes} as the outdoor data source and extra validate the performance on the ONCE dataset~\cite{mao2021one}. The nuScenes dataset consists of $\sim$28K frames with 10 categories, while ONCE contains 6 annotated sequences with 5 categories. Similarly, we set the $\epsilon=0.3$ for image proxies collection and adopt the class balance strategy ~\cite{mmdetection}.  

\input{table/main-fs}


\paragraph{Quantity results. }
Since the outdoor point cloud is collected by LiDAR sensors, it has a wider perception range than RGB-D but leads to sparse distribution. Thus the projected depth representation of baselines results in severer information lost, as illustrated in the second row in Figure~\ref{fig-depth}. As shown in Table~\ref{tab-main-out}, our CLIP$^2$ considerably outperforms the baseline recognition results by more than 20\%, and our triplet proxies respectively boost two baselines by 9.5\% and 4.8\%. Additionally, we evaluate the localization ability on the outdoor scenario nuScenes in Table \ref{tab-det}. Due to the lack of works that tackle outdoor open-vocabulary localization problems, we follow classic detection accuracy metrics Precision(P.) and Recall(R.) as evaluation metrics. Specifically, we calculate the center distance between groundtruth bounding boxes and our 3D proxies that are predicted to belong to the same category as the groundtruth, and set the distance threshold as $\lambda=2$m. For those matched pairs that are closer than $\lambda$, we count the proxies as TPs. Otherwise, for those unmatched proxies and groundtruth, we count them as FPs and FNs respectively, thus P.=$\frac{\text{TPs}}{\text{TPs}+\text{FPs}}$, R.=$\frac{\text{TPs}}{\text{TPs}+\text{FNs}}$. As shown in Table~\ref{tab-det}, our CLIP$^2$ pipeline can provide high recall for outdoor objects. Since CLIP$^2$ is highly sensitive to open-world objects and can perceive categories beyond groundtruth list, it tends to create overmuch predictions thus the precision is comparably low. The perception ability of open-world objects can be viewed in Figure~\ref{fig-vis}\textcolor{red}{(b)}.
%We perform experiments on two popular autonomous driving datasets and show the mean average accuracy across 15 categories (10 classes on nuScenes and 5 classes on ONCE). Table \ref{tab-main-nusc} demonstrates the superiority of our CLIP$^2$.

\paragraph{Quality results. }
We show off two outdoor scenes of nuScenes~\cite{caesar2020nuscenes} in  Figure~\ref{fig-vis}\textcolor{red}{(b-\romannumeral1)} and Figure~\ref{fig-vis}\textcolor{red}{(b-\romannumeral2)}. In addition to perceiving those common categories, our CLIP$^2$ surprisingly localizes and recognizes those uncommon 3D objects in 3D scenes such as the tires of vehicles, the plastic bag in the hand of pedestrian as well as the plastic bag on the road. We believe it contributes to auto-driving safety by providing the localization and recognition of universal obstacles to facilitate follow-up driving decisions.


\subsection{Few-shot Classification}
\paragraph{Setting. }
Lightweight few-shot learning is practical for application by finetuning the pretraining model with given limited data annotations, which can also validate the generalization capability of our learned representation. To make a fair comparison, we follow the existing methods~\cite{zhang2022pointclip, afham2022crosspoint} to conduct experiments under \textquotedblleft K-way N-shot\textquotedblright{} setting on the challenging realistic object-level dataset ScanObjectNN~\cite{uy2019revisiting}, where we randomly sample N point cloud objects from each of the randomly selected K classes.


\input{table/ablation-learning}

\paragraph{Quantity results. }
As illustrated in Table~\ref{tab-scanobj}, we compare with representative 3D networks PointNet++\cite{qi2017pointnet++}, the recent zero-shot approach PointClip~\cite{zhang2022pointclip} as well as a state-of-the-art representation learning method CrossPoint~\cite{afham2022crosspoint}, which conducts contrastive pretraining between point cloud and rendered images on CAD dataset ShapeNet~\cite{chang2015shapenet}. As we can see, with a slight number of samples, our CLIP$^2$ can boost the classification results by a large margin,  exceeding PointClip by 5.3\%, 9.6\% and 6.9\% with 4, 8 and 16 shots. Besides, we outperform CrossPoint with considerable gain, illustrating our pretraining strategy on collected proxies can learn sufficient knowledge from realistic open world to generate transferable 3D representation, which is superior to the pretraining on a small-scale synthetic dataset.

% \vspace{2mm}
\subsection{Ablations and Analysis }

\vspace{1mm}
\paragraph{Ablations on representation learning. }
%n Section~\ref{sec:learn}, we claim the superiority of our 3D representation and cross-modal contrastive learning objective. To validate the conclusion and , 
To observe the transferability of different representations and the effect of different learning objectives, we conduct ablations and report the  mean average Top1 accuracy across all classes of zero-shot recognition in indoor scenarios~\cite{song2015sun} (Avg.\_IN), outdoor scenarios~\cite{caesar2020nuscenes} (Avg.\_OUT) and object-level benchmark~\cite{uy2019revisiting} (Avg.\_OBJ), which is shown in Table~\ref{tab-ablation-learning}. 
Firstly, for fair comparisons, we follow~\cite{zhang2022pointclip, huang2022clip2point} to project input point cloud into depth maps in $N_V$  different views as alternative representation. Secondly, we adopt various objectives to learn different correlation alignments across language, image and point cloud feature space or depth space. Specifically, comparing (a) and (b), aligning depth space to image space yields better transfer performance due to the similar data structure of image and depth map. In (c) and (d), point cloud representation is better when aligning to image space in indoor scenes, while better to align with language space in outdoor scenes due to the data discrepancy between image-like RGB-D points and sparse LiDAR points. Generally, 3D point cloud representation outperforms depth representation in all benchmarks due to preserving the complete 3D structure and sufficient 3D-specific information. Comparing (e) and (d), the joint alignment between three feature spaces contributes to the best 3D point cloud representation transferability on all benchmarks. 
%\textcolor{red}{As we can see, point cloud representation is} 
%As mentioned in Section~\ref{sec:learn}, 3D representation is superior  

\vspace{-2mm}
\paragraph{Analysis of representation ensembling. }
% \begin{figure}[t]
% 	\begin{center}
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% 		\includegraphics[width=1.0\linewidth]{fig/depth.pdf}
% 	\end{center}
% 				\vspace{-4mm}
% 	\caption{Illustration of three representations under indoor and outdoor scenarios. %Left: 3D point cloud. Medium: projected depth maps with differenct views. Right: corresponding image patch.
% 	}
% 	\label{fig-depth}
% \end{figure}
%Intuitively, different representations contain different perspectives of knowledge, which can be potentially merged to achieve the optimum results during inference. To validate the ensembling application, we adopt three optional represeantations, \textit{i.e.} point cloud, its projected depth maps and corresponding image patches. We conduct inference on all representations and ensemble their predicted logits by simple summation as the final output, where depth representation is trained on our proxies and image representation is generated from pretrianed image branch of CLIP. We illustrate the separate recognition performance and ensembling performance with  different combinations schemes as shown in Table~\ref{tab-ablation-ensemble}. We observe that the individual image representation leads to the best performance in separate recognition results, benefiting from the 
Intuitively, different representations contain different perspectives of knowledge, which can be potentially merged to achieve the optimum results during inference. To validate the ensembling application, we adopt three optional representation modals, \textit{i.e.} point clouds, projected depth maps and corresponding image patches, where depth representation is trained on our proxies and image representation is generated from pretrained image branch of CLIP. We ensemble their predicted logits by simple summation as the final output,  and illustrate the separate recognition results and ensembling performance in Table~\ref{tab-ablation-ensemble}. Benefiting from the sufficient knowledge learned from massive CLIP training data, image representation presents best performance in separate applications. By merging the complementary knowledge, our 3D representation leads to gains of 4.6\% indoors~\cite{song2015sun} and 2.8\% outdoors. Though further improving the indoor recognition performance with 0.9\% when merged, depth representation yeilds 1.6\% drop for outdoor objects, illustrating the information lost especially for outdoor scenarios. Since image representation is sometimes missing, such as in ~\cite{uy2019revisiting}, our 3D representation is more robust for 3D applications.


