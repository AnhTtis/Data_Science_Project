\clearpage
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\appendix

\begin{figure*}[!ht]
	\begin{center}
% 		\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=0.8\linewidth]{fig_supp/appendix.pdf}
	\end{center}
	\vspace{-5mm}
	\caption{
	Illustration of triplet proxy generation process.
	}
	\label{fig-proxy}
\vspace{2mm}
\end{figure*}

\section{Implement Details}
\subsection{Triplet Proxy Collection}
Generally, the triplet proxy collection concludes given text proxies $\textbf{\text{X}}^T$, extracting image proposals as 2D proxies $\textbf{\text{X}}^I$ and constructing 3D proxies $\textbf{\text{X}}^P$ with geometry relations between images and point clouds. According to the application scenarios, we detail the construction process in indoor and outdoor scenes separately. 
\paragraph{Indoor scenes. }
The indoor scenes $S$ usually adopt RGB-D sensors to collect images with corresponding depth maps as $I_s^{uvd}$, where $s\in |S|$. Specifically, we first provide given text proxy $\textbf{\text{X}}^T$ as the input of pretrained DetCLIP~\cite{yao2022detclip} to extract 2D image proposals $I_s^{i, uvd}, i\in |X_s^I|$, where $|X_s^I|$ denotes the amount of 2D proxies in scene $s$. Then we segment the foreground images with unsupervised GrabCut~\cite{rother2004grabcut} algorithm  as $I_s^{i, uvd^{'}}$ , thus the point cloud instances can be reconstructed by the RGB-D pixels with camera calibration $\text{G}_\text{IN}$, which can be formulated as:
\begin{equation*}
    \lceil x, y, z \rceil = \text{G}_\text{IN}^{-1} \times \lceil u, v, d \rceil,
\end{equation*}
where $\text{G}_\text{IN}=\text{I}\times\text{R}_c$ denotes the combination of the intrinsics matrix $\text{I}$ and the extrinsics matrix $\text{R}_c$ of RGB-D camera. 

\paragraph{Outdoor scenes. }
Considering a much wider perception range, outdoor scenes usually have LiDAR and camera sensors to capture point clouds $P_s^{xyz}$ and camera images $I_s^{uv}$. Thus point clouds can be projected into camera pixels with sensor transformation matrix $\text{G}_\text{OUT}$ as:
\begin{equation*}
    \lceil u, v, d \rceil = \text{G}_\text{OUT} \times \lceil x, y, z \rceil,
\end{equation*}
where $\text{G}_\text{OUT} = \text{I}\times \text{R}_{c}^{-1}\times \text{R}_{l}$ are the combination of camera intrinsics matrix $\text{I}$, camera extrinsics matrix $\text{R}_c$ and the LiDAR  extrinsics matrix $\text{R}_l$. Concretely, we first conduct a similar procedure to indoor scenes that produces 2D image proposals as $I_s^{i, uvd}, i\in |X_s^I|$ for 2D proxies $X_s^I$. Then we extract the 3D frustum $P_s^{i, xyz^{'}}$ by extruding the
2D image proposal into 3D space and conduct DBSCAN clustering within the frustum. Eventually, we obtain the 3D proxy instance by filtering the point cloud cluster $P_s^{i, xyz}$. The whole process of triplet proxy collection is illustrated in Figure~\ref{fig-proxy}.

\begin{table*}
\parbox{.47\linewidth}{
 \centering
     	\resizebox{.47\textwidth}{!}{
         \begin{tabular}{c|ccc}
			\hline
% 			Encoder & $\text{AP}_\text{IN}$ & $\text{AP}_\text{OUT}$ & $\text{AP}_\text{OBJ}$\\
			Encoder & ScanNet & SUN RGB-D & ScanObjectNN\\
			\hline
			PointNet & 22.6 & 45.3 & 27.0 \\
			DGCNN & 26.0 & 52.7 & 34.0 \\
		    PointNet++  & \textcolor{blue}{38.5}  & \textcolor{blue}{61.3} & \textcolor{blue}{39.4} \\
			\hline
		\end{tabular}
		}
		% \vspace{-2mm}
\caption{Comparison of point cloud encoders.}\label{tab-backbone}
}
\hfill
\parbox{0.53\linewidth}{
  \centering
	\resizebox{.53\textwidth}{!}{
		\begin{centering}
\begin{tabular}{c|cccc}
			\hline
            \multirow{2}*{Range} & \multicolumn{2}{c}{ScanNet}  & \multicolumn{1}{c}{SUN RGB-D} & \multicolumn{1}{c}{ScanObjectNN}\\
~& Main  Top1 & 384 cls. Top5& Main  Top1  & Top1\\
			\hline
			SUN=37 & 36.6&17.1& \textcolor{blue}{63.6}&34.4  \\
			LVIS=1203 & \textcolor{blue}{38.5} & \textcolor{blue}{22.0} & 61.3 & \textcolor{blue}{39.4} \\
			SCAN=384 &  39.5&23.0& 61.6&44.6\\
			\hline
		\end{tabular}
	       \end{centering}
		      }
\caption{Comparison with different proxy range.}\label{tab-proxy}
}
\end{table*}


\subsection{Contrastive Pretraining}
Our main paper applies the popular point cloud classifier PointNet++~\cite{qi2017pointnet++} as our point cloud encoder. Concretely, we use two set abstraction layers that aggregate multi-scale information and then encode the feature vectors for point cloud instances by three fully convolutional layers. We remove the convolutional head of PointNet++  since the point cloud features of CLIP$^2$ can be directly referenced to the language embedding for downstream tasks.

We conduct all experiments using Pytorch \cite{paszke2017automatic}, 8 Tesla V100 cards on a single server. We randomly sample 2048 points on each object both for training and testing. At training time, AdamW optimizer \cite{kingma2014adam} is performed on 8 GPUs with 200 batch sizes on each. The learning rate is set to 0.006, 3e-2 as weight decay, and 0.9 as momentum. And we adopt the cosine decay with 1000 iteration warm-up. For both indoor and outdoor datasets, we train 100 epochs.

\subsection{ScanNet Dataset}
Considering the ambiguous synonyms in the raw classes, like ``handrail", ``stair rail" and ``banister", we involve a data preprocessing step aimed at merging raw classes using WordNet\footnote{\url{https://wordnet.princeton.edu/}} synonyms.
Specifically, the official file \textit{scannetv2-labels.combined.tsv} provided by ScanNet is utilized to identify synonyms for the various classes. This process resulted in the merging of 290 classes, which included 86 classes that lacked synonyms.
To further refine the merged classes, the 86 classes were subjected to an additional merging step. This step involved merging them into existing synonyms based on the path similarity in WordNet. The decision to merge was guided by a predefined threshold, such that only classes with a path similarity score above the threshold were merged.

The outcome of the above-described process was a final set of 249 classes deemed suitable for open-vocabulary evaluation. These classes represented a more refined and comprehensive set of merged classes, facilitating more reasonable and consistent evaluations.



\section{Additional Results}
\subsection{Different Point Cloud Encoder}
We compare three alternatives of point cloud encoder, including PointNet~\cite{qi2017pointnet}, DGCNN~\cite{phan2018dgcnn} and PointNet++~\cite{qi2017pointnet++}, and report the class average Top1 accuracy of zero-shot recognition in Table~\ref{tab-backbone}. Specifically, PointNet encodes point cloud features with point-wise MLP and max-pooling, DGCNN applies EdgeConv to extract edge features and then ensemble the point cloud features, while PointNet++ adopts additional hierarchical feature learning based on PointNet to leverage neighborhoods at multiple scales. As illustrated in Table~\ref{tab-backbone}, PointNet++ outperforms the other two encoders on all benchmarks, showing its superiority in extracting effective point cloud features. We believe more advanced point cloud encoder architectures can further enhance our learned 3D representation of CLIP$^2$.



\subsection{Impact Analysis of Proxy}
\paragraph{Proxy range. }
As the prior knowledge of open-world vocabularies, we adopt the caption list in 2D open-world dataset LVIS~\cite{gupta2019lvis} to set text proxies without human annotations. In Table~\ref{tab-proxy}, we transfer the text proxies to the groundtruth list of segmentation annotations of the SUN RGB-D~\cite{song2015sun}, which presents the congruous vocabulary range of dataset annotations with less noise but a narrow vision of open-vocabulary. Results in Table~\ref{tab-proxy} demonstrate that the groundtruth proxy range can improve the intra-dataset recognition performance on SUN RGB-D by 2.3\% average Top1 Acc. However, the inter-dataset performance drops 1.9\% on ScanNet and 5.0\% on ScanObjectNN, and yields a 4.9\% drop of average Top5 Acc on the extended vocabularies of ScanNet. The overall results validate that the open-world vocabulary of text proxies benefits transferable 3D representation learning.


\paragraph{Proxy quantity. }
In Figure~\ref{fig-percentage}, we present the performance curves that demonstrate the consistency between the zero-shot recognition performance and increasing proxy data. Our analysis suggests that increasing the amount of training data in future work has the potential to further improve the upper bound of performance. These findings highlight the importance of scaling proxy data in a cost-effective manner.

\begin{table}
\resizebox{.46\textwidth}{!}{
\begin{centering}
\begin{tabular}{c|c|cc}
\toprule 
Method & Backbone & SUN & ScanNet\tabularnewline
\midrule
supervised L\_Head & \multirow{3}{*}{PointNet\cite{qi2017pointnet}} & 48.5 & -\tabularnewline
supervised T\_Head &  & 44.2 & 14.4\tabularnewline
\cmidrule{1-1} \cmidrule{3-4} \cmidrule{4-4} 
CLIP$^{2}$ &  & 45.3 & \textcolor{blue}{22.6}\tabularnewline
\midrule
\midrule 
supervised L\_Head &  & 63.4 & -\tabularnewline
supervised T\_Head & PointNet++ & 60.3 & 25.5\tabularnewline
\cmidrule{1-1} \cmidrule{3-4} \cmidrule{4-4} 
PointCLIP \cite{zhang2022pointclip} & \cite{qi2017pointnet++} & 11.5 & 6.7\tabularnewline
CLIP$^{2}$ &  & 61.3 & \textcolor{blue}{38.5} \tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
}
\caption{\label{tab:exam3}Comparisons with supervised baselines. We train supervised baselines on SUN RGB-D (SUN) dataset and evaluate recognition results on SUN and zero-shot performance on ScanNet. \textbf{L\_Head} and \textbf{T\_Head} indicate logit classification head and text classification head respectively.}
\end{table}


\begin{figure}[hb]
\centering
\includegraphics[scale=0.8]{fig_supp/curve_new.png}
% \vspace{-4mm}
\caption{Performance curves for training proxy quantity.}\label{fig-percentage}
% \vspace{-8mm}
\end{figure}


\subsection{Comparison with Supervised Baselines}
We conduct supervised training with popular 3D encoder PointNet~\cite{qi2017pointnet} and PointNet++ \cite{qi2017pointnet++} using annotations from SUN RGB-D training set. We consider two different settings as supervised baselines: 1) Traditional logit classification head \textbf{L\_Head}, which is fixed to the predefined training classes and fails to identify novel classes. 2) Text classification head indicated as \textbf{T\_Head}. Specifically, we replace the logit classification head with CLIP text embeddings. And the maximum cosine similarities between the 3D feature and text embeddings are the final results. According to the text classification head, we can compare the generalization of flexible categories with supervised training.

The results in Table~\ref{tab:exam3} show that CLIP$^2$ is comparable to supervised baselines on SUN RGB-D and outperforms them on ScanNet, illustrating the effectiveness of our unsupervised approach and its superiority in open-vocabulary understanding.

% \subsection{Segmentation Results}




\section{More Qualitative Results}
\paragraph{Sailency map between text prompt and point cloud. }
To validate  our CLIP$^2$, we show a saliency map between the given text prompt and the point cloud within one scene in Figure~\ref{fig-saliency}. Specifically, we calculate the feature distances between the class texts and the point cloud scene and plot the saliency map, with lighter highlights representing smaller feature distances. The text feature has greater similarity to the point feature of the corresponding class, indicating the feature alignment between text and point cloud.

\paragraph{Visualization of zero-shot localization. }
We show more qualitative results in Figure~\ref{fig-vis1} for indoor scenes SUN RGB-D \cite{song2015sun} and Figure~\ref{fig-vis2} for outdoor scenes nuScenes \cite{caesar2020nuscenes}. The visualization results illustrate the zero-shot localization and recognition abilities of CLIP$^2$. Specifically, the proposed CLIP$^2$ enables the open-world vocabularies beyond groundtruth annotations without extra human supervision, such as 'Tire' and ’Debris' in Figure~\ref{fig-vis1}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig_supp/attentionmap2.pdf}
\vspace{-2mm}
\caption{Saliency maps between texts and point cloud scenes. }
\label{fig-saliency}
\end{figure*}


\begin{figure*}[t]
\vspace{4mm}
	\begin{center}
% 		\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=1\linewidth]{fig_supp/vis_ap1.pdf}
		\includegraphics[width=1\linewidth]{fig_supp/vis_ap2.pdf}
	\end{center}
% 	\vspace{-2mm}
	\caption{
	More Visualizations of the zero-shot localization and recognition on the nuScenes dataset. The proposed CLIP$^2$ enables the open-world vocabularies beyond groundtruth annotations without extra human supervision, such as 'Tire' and ’Debris'. Best viewed in colors.
	}
	\label{fig-vis1}
\end{figure*}

\begin{figure*}[t]
	\begin{center}
% 		\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=1\linewidth]{fig_supp/vis_ap3.pdf}
		\includegraphics[width=1\linewidth]{fig_supp/vis_ap4.pdf}
	\end{center}
% 	\vspace{-2mm}
	\caption{
	More Visualizations of the zero-shot localization and recognition on SunRGB-D dataset. The proposed CLIP$^2$ shows open-world recognition ability in realistic scenarios. Best viewed in colors.
	}
	\label{fig-vis2}
\end{figure*}