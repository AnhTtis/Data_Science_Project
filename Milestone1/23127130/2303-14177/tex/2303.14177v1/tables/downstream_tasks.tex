\begin{table*}[t!]
\centering
\small
\begin{tabular}{llrrrrrrrr}
& & \multicolumn{6}{c}{\it Few-shot Text Classification Accuracy (\%)} \\
\toprule
% \cmidrule(lr){2-9}
  & & \bf AGNews & \bf DBPedia & \bf SST-2 & \bf Amazon & \bf Phrasebank  & \bf Twitter  \\ 
&  $\downarrow$ Model (inference parameters) &  \it Topic & \it Topic & \it Sentiment & \it Sentiment & \it Sentiment & \it Hatespeech & \bf Average  \\

\midrule
& Random chance & 25.0 & 7.10 & 50.0 & 50.0 & 33.3 & 50.0 & 35.9  \\
& OPT (1.3B) &  42.9 & 57.2 & 72.8 & 81.3 & 72.5 & \bf 65.1  & 65.3 \\
& OPT (6.7B) & 51.9 & 58.9 & 77.0 & 83.8 & 76.4 & 39.6 & 64.6  \\
\midrule
\parbox[t]{3pt}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\bf {\textsc{C4}}}}}  & 1-cluster (1.3B) & 47.4 & 61.1 & 80.2 & 80.7 & 66.6 & 60.9 & 66.2 \\
& 1-cluster (6.7B) & \bf 68.1 & 62.4 & 80.7 & \bf 84.9 & \bf 80.6 & 37.4  & 69.0 \\
&   16-cluster; top-1 (1.3B) &  47.1	& \bf 62.9	& 74.3 & 	79.1 &	72.9 &	56.4  & 65.4 \\
& 16-cluster; top-4 (5.2B)  & 49.3	& 62.3 &  80.0 &	81.3 &	  78.7 & 	61.3 & 68.8 \\
& 16-cluster; top-16 (20.8B) & 50.6 &	62.0 &	\bf 84.0 & 83.2 &	78.6 &	 61.7 & \bf 69.9 \\
\bottomrule
\end{tabular}
\caption{\textbf{\cbtm models outperform dense counterparts on downstream  text classification tasks (\S\ref{sec:downstream_task_results}).} We display accuracy of models from \S\ref{sec:results} on six text classification tasks, using eight demonstrations for each example and no additional fine-tuning. We report accuracy averaged over five random seeds. The 1- and 16-cluster models are trained on 168B tokens of C4 (i.e., our highest budget). The 16-cluster model, with top-4 or top-16 inference, always outperforms the 1-cluster model, and top-1 inference usually outperforms the 1-cluster model at no additional inference cost. We include average performance of models across tasks for readability.}
\label{tab:downstream_tasks}
\end{table*}
