\begin{table}[h!]
\centering
\small
% \resizebox{\textwidth}{!}{
\begin{tabular}{ccc}
\multicolumn{3}{c}{\textbf{5.2B Tokens}} \\
\toprule
  \bf 8 Metadata  & \bf 8 Clusters &  \bf 1 Cluster  \\
\midrule 
 % HackerNews & 19.5 & 19.7 & 20.0 \\
 % OpenWebText2 & 10.3 & 10.3 & 10.4 \\
 % CommonCrawl & 13.4 & 13.4 & 13.6 \\
 % Reddit & 26.1 & 26.0 & 25.1\\
 % Gutenberg  & 15.1 & 15.4 & 15.6\\
 % STORIES & 14.1 & 14.2 & 14.5\\
 % Wikipedia & 9.9 & 9.8 & 10.0 \\
 % Mathematics & 3.4 & 3.4 & 3.4 \\
% \midrule
%  Average &  14.0 & 14.0  & 14.1 \\
8.4 & 8.3 & 8.5 \\
\bottomrule
\end{tabular}

% }
\caption{\textbf{Experts trained with clusters perform slightly better than experts trained with metadata (\S\ref{sec:analysis_metadata}).} Here, we train each model for 5.2B tokens on 8 corpora of the Pile, and evaluate perplexity on a held out random sample of the constituent datasets. See \S\ref{sec:metadata_comparison} for more details on the dataset.}
\label{tab:metadatacompare}
\end{table}