\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{rotating}


\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}

\usepackage{multirow}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{times}
\usepackage{latexsym}
\usepackage{bbm}

% custom
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUSTOM COMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\suchin[1]{{\color{orange}\{#1\}\textsubscript{SG}}}
\newcommand\margaret[1]{{\color{magenta}\{#1\}\textsubscript{ML}}}
\newcommand\mike[1]{{\color{brown}\{#1\}\textsubscript{MIKE}}}
\newcommand\luke[1]{{\color{cyan}\{#1\}\textsubscript{Luke}}}
\newcommand{\nascomment}[1]{\textcolor{blue}{\textbf{[#1 --\textsc{nas}]}}}

\newif\ifhidecomments
% \hidecommentstrue %uncomment to hide comments

\ifhidecomments
\usepackage{environ}
\NewEnviron{hide}{}
\let\suchin\comment
\let\margaret\comment
\let\mike\comment
\let\luke\comment
\let\nascomment\comment
\fi

\newcommand\clusterbtm{\textsc{cluster-BTM}\xspace}
\newcommand\cbtm{\textsc{c-BTM}\xspace}

\newcommand\cfour{\textsc{C4}\xspace}
\newcommand\semanticscholar{\textsc{S2ORC}\xspace}
\newcommand\thepile{\textsc{The Pile}\xspace}
\newcommand\demix{\textsc{DEMix}\xspace}
\newcommand\elm{\textsc{ELM}\xspace}
\newcommand\elmforest{\textsc{ELMForest}\xspace}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Scaling Expert Language Models with Unsupervised Domain Discovery}
\begin{document}

\twocolumn[
\icmltitle{Scaling Expert Language Models with Unsupervised Domain Discovery}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{wmeta}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Suchin Gururangan}{equal,uw,wmeta}
\icmlauthor{Margaret Li}{equal,uw,meta} 
\icmlauthor{Mike Lewis}{meta} 
\icmlauthor{Weijia Shi}{uw,meta} 
\icmlauthor{Tim Althoff}{uw} \\ \icmlauthor{Noah A. Smith}{uw,allenai}
\icmlauthor{Luke Zettlemoyer}{uw,meta}

\end{icmlauthorlist}

\icmlaffiliation{uw}{University of Washington}
\icmlaffiliation{meta}{Meta AI}
% \icmlaffiliation{wmeta}{Work done while at Meta AI.}
\icmlaffiliation{allenai}{Allen Institute for Artificial Intelligence}

\icmlcorrespondingauthor{Suchin Gururangan}{sg01@cs.washington.edu}
\icmlcorrespondingauthor{Margaret Li}{margsli@cs.washington.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


% describe clusterBTM independently of BTM, then mention this is BTM++ afterwards

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}

%We introduce \clusterbtm~(\cbtm), a new technique to train large language models on any heterogeneous text collection without massive multi-node synchronization. 

%\cbtm generalizes recent methods that use domain metadata~\cite{btm} for embarrassingly parallel training, creating the first algorithm that can train on arbitrary datasets. 
% We introduce \cbtm, a simple and effective technique to scale the training of large language models  without massive multi-node synchronization. \cbtm automatically discovers domains in any training corpus using a balanced k-means clustering scheme, and trains expert language models (ELMs) independently on each cluster. At test time, the resulting experts are ensembled in a top-k fashion to sparsely activate only the parts of the model that are most relevant for predicting the next token. 
% Extensive experiments, with differing numbers of 1.3B parameter experts on C4 \citep{t5} and S2ORC \citep{s2orc}, show that \cbtm consistently outperforms strong dense baselines, with modest training budgets per expert. Furthermore, as the amount of data increases, training with more clusters improves performance. Key to our method is the nature of the clusters: random clusters do not work, and unbalanced clusters do not perform as well. Overall, these results demonstrate that \cbtm is an effective technique to scale language model training to massive datasets.


Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs.
We introduce a simple but effective method to \emph{asynchronously} train large, sparse language models on arbitrary text corpora.
Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference.
This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models.
Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains.
Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models. 
\end{abstract}

% \mike{Sparse LLM architectures have often been shown to outperform their dense counterparts \cite{gshard,switch,base}, by routing different tokens to specialist parameters. However, the cost and complexity of routing has prevented widespread adoption. 
% We introduce a simple but effective method for training sparse LLMs by routing sequences (instead of tokens) using offline balanced clustering (instead of online reinforcement learning). 
% Effectively, different LLMs are trained on different clusters of documents, and then a sparse ensemble is used during inference.
% Our approach eliminates almost all costs and complexity from existing sparse LLM proposals, whilst still giving significant performance gains over dense LLMs on training corpora.
% It also requires \emph{less} communication than dense models, enabling LLMs to be trained asynchronously. 
% We find that random clustering performs badly, indicating that our specialization to different types of text is key to our method's performance.
% Further, performance gains over dense models increase with the number of experts and size of training data, suggesting that this is a highly scalable approach to training LLMs.}
% \end{abstract}

% This approach generalizes both previous MoE-like sparse models, which require significant synchronization, and a recent embarrassingly parallel training approach that requires metadata to define data clusters. \luke{I wrote previous sentence but don't like it. Do we need something like this, or just leave for intro?}

%We first automatically discover domains in the training data using unsupervised clustering. We then train smaller expert language models (ELMs) on individual clusters, with no communication between experts. At test time, we sparsely ensemble expert outputs based on the cluster of the incoming context. 

%We use \cbtm to scale 1.3B parameter experts on C4 \citep{t5} and S2ORC \citep{s2orc} training on up to 128 clusters and 84B tokens in each corpus. 

%Also, due to extreme parallelism, we can train the model to far more data at a modest budget per expert: for example, we train ELMs on 168B tokens of C4 with only four GPUs per expert. 
%We also show that C-BTM matches the performance of experts trained with provenance metadata from the Pile \citep{pile}. 


\section{Introduction}
\label{sec:introduction}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/schwartz.pdf} 
    \vspace{-2em}
    \caption{\textbf{We present \cbtm, a new technique to asynchronously scale expert LMs (\S\ref{sec:cbtm}).} \cbtm splits a corpus into $k$ clusters, trains an expert LM on each cluster, and creates a sparse ensemble during inference. Above, LMs trained with \cbtm (with 4 or 16 clusters) achieve lower validation perplexity than compute-matched dense LMs. These LMs begin with OPT-1.3B \citep{opt}, and are further trained on C4 \citep{t5}. The optimal cluster count for \cbtm, and its performance gains, increase with the size of training data (shown in log-scale).} 
    \label{fig:schwartzplot}
\end{figure}

Language models (LMs) are trained on up to trillions of tokens of text \citep{hoffmann2022training, llama}. This improves performance on many tasks, but also incurs an extreme cost: thousands of GPUs need to be active simultaneously to update all parameters at each step \citep{opt, palm}.  
 % Similarly, mixture-of-experts models \citep{}, which only sparsify some gradient updates, still require costly multi-node synchronization. % 
% for shared parameters, and incur additional high communication costs at each expert layer.
% \citealt{btm} propose a training algorithm to reduce these costs called Branch-Train-Merge (BTM). BTM divides the total compute among a collection of smaller expert language models (ELMs), each of which is independently trained on a distinct subset (or \emph{domain}) of the training corpus. Outputs or parameters of ELMs are then sparsely combined at inference time. However, BTM relies on metadata associated with each document (e.g., internet domains or other provenence attributes) to delineate the domains in a corpus that ELMs specialize to. Such supervision is not always available. For example, large Internet crawls such as C4 \citep{t5} are too large to manually label, and the distribution of URLs, which are typically the only available metadata, does not lend itself to an obvious organization of domains. The issue is only exacerbated with bigger, more diverse corpora \citep{gopher, pile}.
% BTM results in a collection of ELMs (or ELMforest) that achieves better language modeling performance than their dense counterparts at a fraction of the computational cost. 
% The resulting collection of ELMs is noa fraction of the computational cost of the equivalent dense model.
Branch-Train-Merge (BTM; \citealt{btm}) alleviates this cost by dividing the total compute among a collection of smaller expert language models (ELMs), each independently trained on a distinct subset (or \emph{domain}) of the training corpus and ensembled during inference. However, BTM relies on document metadata to identify domains, and such supervision is not always available \citep[e.g., in large Internet crawls;][]{t5, gopher, pile}.  Moreover, the optimal number of metadata-based domains for a fixed budget is unknown, since metadata cannot be easily merged or divided. 

% BTM inference also requires additional data and forward passes through all trained experts, so inference costs grow linearly with the number of ELMs. \suchin{perhaps remove the previous sentence, it's technically not correct (we also propose updating prior and show that it's quite sparse in practice)}



% We aim to alleviate much of this cost by developing Cluster-Branch-Train-Merge (\cbtm; Figure \ref{fig:schwartzplot}), which enables scaling LMs on arbitrary text corpora without massive multi-node synchronization. \margaret{Stephen: surface BTM a bit earlier for arxiv version, feels like hiding prior work rn}


% \cbtm is a sparse training procedure which divides the total compute among a collection of smaller expert language models (ELMs), each of which is independently trained on a distinct subset (or \emph{domain}) of the training corpus, with no synchronization between \elm{}s. 

% special requirements at data collection time or expensive post-hoc human labeling
In this work, we introduce Cluster-Branch-Train-Merge (\cbtm; Figure \ref{fig:schwartzplot}), a metadata-free algorithm to scale LMs without massive multi-node synchronization. We use unsupervised clustering to discover domains in a corpus, and  train an \elm{} on each cluster independently (\S\ref{sec:cbtm_iteration}).
At inference time, we \emph{sparsely} activate a subset of the trained \elm{}s (\S\ref{sec:cbtm_inference}). We ensemble \elm{}s by weighting their outputs with the distances between an embedding of the current context and each expert's cluster center. This enables simple and efficient sparse computation \citep{https://doi.org/10.48550/arxiv.2209.01667} by retrieving only the top-$k$ experts when predicting each new token. 
%Thus, despite training many parameters, \cbtm inference costs are equivalent to a much smaller LM.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/training.png} \vspace{-2em}
    \caption{
    \textbf{\cbtm training process (\S\ref{sec:cbtm_iteration}).} \cbtm begins with unsupervised domain discovery using $k$-means clustering. We then initialize expert language models (ELMs) with a seed language model (e.g., OPT; \citealt{opt}) and train an ELM on each cluster. The resulting experts are added to a larger collection for sparse inference.}
    \label{fig:process}
\end{figure*}




%At inference time, we sparsely activate a subset of the \elm{}s, which are ensembled by weighting their outputs by the relative distances between an embedding of the current context and the cluster center for each expert model. This allows for simple and efficient sparse computation, by retrieving only the top-k experts, when predicting each new token, unlike previous expert methods that require activating all experts~\cite{btm}. We find that using the top-2 experts per context yields comparable performance to activating all experts, and even using the top-1 expert per context outperforms a dense baseline. This approach is reminiscent of sparse mixture-of-experts models that activate the top-1 \citep{fedus2021switch} or top-2  \citep{lepikhin2020gshard} experts per context. Unlike these models, the simple routing mechanism of \cbtm does not need to communicate gradients or activations across nodes and avoids the need for additional techniques to balance expert utilization \citep{baselayers}. 


\cbtm generalizes BTM by allowing for fine-grained control over the number and size of data clusters, since they are automatically learned without being constrained by available metadata. 
We use this new capability to investigate the scaling properties of \cbtm as a function of the number of experts trained, controlling for a variety of factors (\S\ref{sec:experiments}). Extensive experiments show that training more clusters always results in better validation perplexity than single cluster (i.e., dense)  models, and the optimal cluster count increases with the overall compute (\S\ref{sec:core_results}). These results are consistent for both 1.3B and 6.7B parameter experts. 

With more clusters, we can aggressively parallelize expert training: for example, we train 128 \elm{}s (168B parameters in total) on 168B tokens of text in aggregate with only 8 GPUs at a time. This enables us to avoid many practical difficulties associated with training large LMs across many nodes simultaneously (\S\ref{sec:time_comparison}). Moreover, the number of  parameters at inference time can be kept constant even as the number of experts grows (\S\ref{sec:sparsity_analysis}): using just the top-2 or top-4 experts is comparable to using all experts, while using just the top-1 expert still outperforms the dense model. Training with more clusters is also more effective than training larger dense models: in \S\ref{sec:zflops}, we demonstrate that training many 1.3B expert LMs, and sparsifying them to a 5.2B parameter LM, achieves the same perplexity as a 6.7B dense model, but with only 29\% as many training FLOPs. These gains are also reflected in few-shot text classification experiments (\S\ref{sec:downstream_tasks}), which show that \cbtm  models outperform dense baselines even with heavily sparsified inference. 


\cbtm provides a radically simplified sparse modeling approach that eliminates nearly all communication overhead from existing sparse LM schemes. Existing sparse LMs typically route different tokens to specialist parameters \cite{lepikhin2020gshard,fedus2021switch,https://doi.org/10.48550/arxiv.2202.01169}. However, they have yet to be widely adopted, perhaps due in part to the communication costs of routing each token in each sparse layer \citep{artetxe2021efficient}, challenges in learning to specialize experts to tokens \citep{https://doi.org/10.48550/arxiv.2202.09368}, and the necessity of additional mechanisms to balance expert utilization \citep{baselayers}. \cbtm improves over sparse LMs by routing sequences (instead of tokens) using offline balanced clustering (instead of online load balancing) with no shared parameters between experts. We compare directly to a mixture-of-experts model with top-2 routing \citep{lepikhin2020gshard} in \S\ref{sec:moe_comparison}.


Our final analysis (\S\ref{sec:analysis}) shows that balanced clustering is key to \cbtm performance; it works as well as expert assignment with gold metadata, and strongly outperforms random and unbalanced clustering baselines. Overall, our findings suggest that \cbtm is an efficient and accessible method to scale large language models into massive datasets. We release our code and models publicly.\footnote{\url{https://github.com/kernelmachine/cbtm}}





% \section{Background}

% \luke{Could we cut this section? I think it is pretty well covered in the current intro and related work?}

% Training dense LMs \citep{opt, palm} requires massive multi-node synchronization to compute model activations and gradients \citep{kaplan2020scaling,hoffmann2022training}. For example, OPT-175B \citep{opt} was trained on 992 80GB A100 GPUs, and PaLM-540B \citep{palm} was trained on 6144 TPU v4 chips. Mixture-of-experts models, which selectively activate parts of the LM at each step \citep{lepikhin2020gshard,fedus2021switch,gururangan-etal-2022-demix}, still require simultaneous computation across many nodes to synchronize shared parameters and communicate between experts. 

% \citealt{btm} use training corpus curated by \citet{gururangan-etal-2022-demix} from 8 distinct sources (e.g., Reddit, biomedical papers), each of which defines a domain. 

% In contrast, Branch-Train-Merge (BTM; \citealt{btm}) trains a set of completely independent, smaller expert language models (\elm{}s), each initialized from one set of LM parameters, then specialized to a specific domain (e.g., scientific papers, legal documents) as defined by metadata. BTM is an embarrassingly parallel algorithm, requiring no communication between nodes dedicated to training different experts. BTM begins by training a single LM on a heterogeneous corpus, called a \emph{seed LM}. The seed LM parameters are used to initialize (or \emph{branch}) $k$ new \elm{}s, which are then trained fully independently on different domains of data. The resulting \elm{}s are merged into a larger set of experts. These \elm{}s can be output ensembled or parameter averaged, and new \elm{}s can be trained as branches of any existing ones and merged back into the collection.

% BTM results in better language modeling performance than dense training at a fraction of the computational cost. However, BTM is limited to training data with metadata labels which can be used to determine its domains. Typical LM corpora, including C4 \citep{t5} and the Pile \citep{pile}, are sourced from the Internet without retaining document provenance at collection time and are infeasible to label manually. Additionally, the optimal number of domain experts for a fixed corpus size, model architecture, and computation budget remains unknown. This variable cannot be explored in a setting with metadata-defined data domains; for example, \citet{btm} use a fixed number and size of domains which cannot be easily merged or subdivided.  A new approach is needed in order to benefit from embarrassingly parallel training on arbitrary corpora.


\section{\cbtm}
\label{sec:cbtm}

We introduce \cbtm, a method for embarrassingly parallel training that specializes expert language models to domains discovered through clustering instead of metadata. \cbtm enables scaling to arbitrary numbers of domains and compute budgets on any corpus. In this section, we outline \cbtm training (Figure \ref{fig:process}) and inference (Figure \ref{fig:inference_process}).

\subsection{Training}
\label{sec:cbtm_iteration}

\paragraph{Step 0: Cluster}

To segment our corpus, we employ $k$-means clustering, enforcing balanced clusters. ELMs trained without this constraint perform worse  (\S\ref{sec:analysis_balancing}).\footnote{Other techniques to improve clusters, e.g. $k$-means++ \citep{10.5555/1283383.1283494}, can be used to improve performance.}

Consider the iterative, hard expectation-maximization view of $k$-means clustering. In the expectation step, each document embedding is assigned to a cluster center based on its Euclidean distance to each center. In the maximization step, each cluster center is updated to be the mean embedding of the current set of documents assigned to it. To balance the clusters, we formulate the expectation step as a balanced linear assignment problem \citep{Malinen2014BalancedKF}. Given $D$ document embeddings with representations $\{w_1, \ldots, w_D\}$ and $K$ cluster centers with representations $\{h_1,\ldots,h_K\}$, we assign each document $d$ to a cluster with the assignment index $a_d \in \{0,\ldots,K\}$:
\begin{align}
\max_{a_1, \ldots, a_D} \sum_{d=1}^D -dist(h_{a_d}, w_d)
 \text{ s.t. $\forall k$, }  \sum_{d=1}^{D}\mathbbm{1}_{a_d = k} = \frac{D}{K} 
\end{align}
where $dist$ is the Euclidean distance. Many algorithms exist to solve this problem; we follow \citet{baselayers} and use the auction algorithm \citep{Bertsekas1992AuctionAF}.  We only use balancing when estimating the cluster centers; we use greedy inference when predicting clusters, as balancing at inference time is cumbersome for massive corpora. 

% Using balancing at inference time would require us to load the entire corpus at once to find the optimal balanced assignment, which is not feasible with massive corpora. We could do online balancing with sufficiently large batches, but this necessitates that each batch contains a uniform sample of the corpus, which is not guaranteed. 

In our experiments, we use a simple tf-idf embedding function, which is highly efficient at scale and leads to interpretable clusters.\footnote{In initial experiments, tf-idf outperformed other scalable text embeddings, like hash embeddings \citep{https://doi.org/10.48550/arxiv.1709.03933}.}  We only use a single shard of each corpus to train our clustering model. Any new document, once embedded, can be efficiently mapped to its nearest cluster(s) without additional training. Any embedding function can be used, though the choice of embedder may apply different assumptions of what constitutes a textual domain and come with efficiency trade-offs.\footnote{tf-idf assumes that domains are lexically-driven, which may not correspond with other notions of domain.}  Comparing to other embedding or clustering methods is an interesting area for future work, and could likely improve performance. 

% \margaret{From Armen, regarding prev sentence:This seems like an odd choice for me. Why not use BERT or other dense embedding? You mention "scalable embeddings", is bert not one of them?} \luke{Only a FAIR researcher would consider BERT cheap to run at scale... :-P}\suchin{added that tfidf is also interpretable}

% Our choice of $k$-means clustering to discover domains is also motivated by simplicity and efficiency at scale. Other techniques, including those that involve agglomerative clustering \citep{chronopoulou-etal-2022-efficient}, gaussian mixture models \citep{aharoni-goldberg-2020-unsupervised},  or mixed effect models \citep{White2022MixedeffectsTF} may yield better results.

\paragraph{Step 1: Branch (from seed LM)} To begin training experts on each of the $k$ clusters from Step 0, we first \emph{branch} from (i.e., make $k$ copies of) a \emph{seed} LM. Seed LMs are critical for the overall functionality of ELMs, and ELMs perform best when the seed LM has been trained with a diverse corpus \citep{btm}. In our experiments, we use an OPT LM \citep{opt} as our seed.\footnote{\citealt{btm} find that dedicating more compute to branching (rather than seed training) leads to better in-domain performance, and the choice of seed LM has a strong effect on the modularity of the resulting ELMs. Future work may explore the effect of different seed LMs on \cbtm performance.}

\paragraph{Step 2: Train} We assign each \elm to a single cluster, and train on each cluster with the log likelihood objective.

\paragraph{Step 3: Merge} After training on the assigned domain, we add the new \elm into a larger collection for inference.


In this work, we focus on a single iteration of \cbtm for simplicity. Future work may explore branching from already trained experts in multiple iterations.

\subsection{Inference}
\label{sec:cbtm_inference}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/inference.png}
    \vspace{-.5em}
    \caption{\textbf{\cbtm inference process (\S\ref{sec:cbtm_inference}).}  At inference time, we embed each incoming context and estimate a probability distribution over clusters, by calculating the distance between the embedded context and each cluster center. We use this probability distribution, optionally sparsified to use only the top-k experts, to weight an output ensemble of the \elm{}s.}
    \label{fig:inference_process}
\end{figure}
% Our method addresses several limitations of the technique proposed in those works, namely the necessity of additional held out data to estimate the weights, and the assumption that the test set documents come from the same overall distribution. 

At inference time, we use a sparse ensemble of the outputs of ELMs for incoming test contexts (Figure \ref{fig:inference_process}).
Formally, consider that the language model provides, at each timestep, $p(X_t \mid \boldsymbol{x}_{<t})$.  We introduce a domain variable $D$, alongside each sequence. Then the next-step conditional distribution on the history $\boldsymbol{x}_{<t}$ is:
\begin{align}\small
    p(X_t \mid \boldsymbol{x}_{<t}) & \small{= \sum_{j=1}^k p(X_t \mid \boldsymbol{x}_{<t}, D = j) \cdot \underbrace{p(D = j \mid \boldsymbol{x}_{<t})}_{\mbox{ensemble weights}}}
\end{align}
% We consider three options for estimating the ensemble weights:

% \paragraph{Bayes Rule} We can estimate the ensemble weights using Bayes' rule: 
% \begin{align}
%     \small
%     \nonumber p_{bayes}(D = j \mid \boldsymbol{x}_{<t}) &
%     = p(\boldsymbol{x}_{<t} \mid D = j) \cdot p(D = j)/p(\boldsymbol{x}_{<t}) \\ & = \frac{p(\boldsymbol{x}_{<t} \mid D = j) \cdot p(D = j)}{\sum_{j'=1}^K p(\boldsymbol{x}_{<t} \mid D = j') \cdot p(D = j')}
% \end{align}

% We set the prior $P(D = j)$ to be uniform over the experts.

With the pretrained embedder and clustering model from Step 0 (\S\ref{sec:cbtm_iteration}), we  embed the context $h_{x_{<t}}$ and use the $k$ cluster centers $\{h_{c_0} \ldots h_{c_k}\}$. We set ensemble weights as:
\begin{align}\label{eq:topk}
 p(D = j \mid \boldsymbol{x}_{<t}) \propto  \text{topk} [ \exp (-dist(h_{x_{<t}}, h_{c{_j}})^{2}/T) ]
\end{align}

Where $dist$ is the Euclidean distance, $T$ is a temperature parameter which sharpens or smoothes the probability distribution over cluster centers, and the top-k function filters for the top-$k$ probabilities and renormalizes the distribution to sum to 1. This formulation is reminiscent of nearest-neighbor retrieval mechanisms for language models  \citep{https://doi.org/10.48550/arxiv.1911.00172,https://doi.org/10.48550/arxiv.2205.13792}. 

These ensemble weights are updated for every incoming token, although in separate experiments we observe that  we find that cluster assignments (and in effect, ensemble weights) can be fixed for the second half of a document with no drop in performance; this can further speedup inference.

We find that, in practice, the performance of our models is robust to even top-2 or top-4 experts, meaning that the inference costs of the language model are equivalent to a much smaller LM. We perform an empirical study of inference variations in \S\ref{sec:sparsity_analysis}.
% To sparsify the ensemble weights, we simply take the topk. 
%Note that no language modeling evaluation test sets are used in the construction of the embedder or the clustering model. 

% \paragraph{Bayes Rule $\times$ Cluster} We combine the above inference procedures in a product:
% \begin{align}
% \small
% p(D = j \mid \boldsymbol{x}_{<t}) \propto p_{bayes}(D = j \mid \boldsymbol{x}_{<t}) \cdot p_{clust}(D = j \mid \boldsymbol{x}_{<t})
% \end{align}
% \suchin{intuitions about why this should be better than each individually?}

% The ensemble weights that are computed for every context is unique to that context
% The mechanism proposed to estimate $P(D = j)$ is the \emph{cached prior} method, where 

% They estimated $P(D=j)$ using an exponential moving average of posterior estimates on held out validation data, and fixed it during test data evaluation.

% The disadvantage to this approach for ensembling is that we need held out data to estimate the ensemble weights \citep{btm, gururangan-etal-2022-demix}. Their technique also assumes that adjacent documents in a test set come from the same overall distribution.

% A limitation of this procedure is that it is not amenable to sequence packing, since multiple documents in the same sequence may align with different clusters. So, we always evaluate one document at a time.

% Of the three approaches, we find that Bayes Rule $\times$ Cluster is the best inference method (Appendix \S\ref{sec:inference_ablation}), so we use Bayes Rule $\times$ Cluster for all experiments.

% \nascomment{something I'm confused about.  how do we set the ensemble weights at the beginning of a new test time document?  do we update them after every token?  suppose that halfway through the document, a new cluster enters the top k.  do we have to process the entire context up to that point through that cluster's ELM?  basically my question is about changes in the top-k as we proceed through a test time document.  I couldn't figure out the answer from the text above.}

\subsection{Comparing to Dense Training} 
\label{sec:comparing_dense}

Dense LMs are typically trained using hundreds or thousands of concurrent GPUs, all of which synchronize gradients each update.  For example, OPT-175B \citep{opt} was trained on 992 80GB A100 GPUs, and PaLM-540B \citep{palm} was trained on 6144 TPU v4 chips. \cbtm improves training efficiency by reducing communication overhead, as only GPUs training the same ELM must communicate. Furthermore, the chance of a GPU failure can grow considerably with the number of GPUs. \cbtm improves the resiliency of distributed training, since a single GPU failure only delays training for a single ELM, whereas in dense training, a single GPU failure afflicts training on all other GPUs.  \cbtm also makes training large LMs more feasible on shared GPU clusters, since it effectively decomposes training into smaller jobs which can run asynchronously. This makes job scheduling more efficient by reducing the number of GPUs that need to be allocated simultaneously.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/test_cropped.pdf} \vspace{-1em}
    \caption{\textbf{We train and evaluate on two large text corpora (\S\ref{sec:data}).} C4 (left; \citealt{t5}) 
 and S2ORC (right; \citealt{s2orc}) are diverse and contain many different clusters of text, indicated by these UMAP visualizations of 400K random documents in each corpus, colored with 32 automatically discovered and annotated clusters. See \S\ref{sec:analysis_specialze} for the description of our clustering and annotation procedure, and Figure~\ref{fig:c4} in the appendix for annotations of all clusters in these plots. }
    \label{fig:umaps}
\end{figure*}

\subsection{Comparing to BTM \citep{btm}}

Our method addresses several limitations of the training and inference techniques proposed by \citet{btm}.

First, BTM is limited to training data with metadata which can be used to determine its domains. Typical LM corpora, including C4 \citep{t5} and the Pile \citep{pile}, are sourced from the Internet without retaining document provenance at collection time, and are infeasible to label manually. Also, the optimal number of experts for a fixed corpus size, model architecture, and budget remains unknown, and is difficult to explore with metadata-based domains, since they cannot be easily merged or divided. \cbtm broadens the applicability of BTM to arbitrary datasets.

%, namely the necessity of additional held out data, the cost of a forward pass through each expert for all of this held out data to estimate the ensemble weights, and the assumption that the test set documents come from the same overall distribution. 
Moreover, BTM inference follows the \emph{cached prior} method introduced by \citet{gururangan-etal-2022-demix}, where the ensemble weights are estimated using Bayes' rule on additional held out data, and the prior $P(D=j)$ is estimated with an exponential moving average over sequences of posterior estimates that require forward passes on experts. This estimate is then fixed during test data evaluation. 

With \cbtm, we route based only on the current context. Thus, no additional data or forward passes through the experts are needed to estimate ensemble weights, nor do we need to assume that adjacent documents in the test set come from the same distribution. This also implies that the parameter averaging technique of \citet{btm} is not well suited to our setting, as it requires fixing the weights assigned to each expert for a set of evaluation documents. Future work may explore merging expert parameters for each context during inference. 


% With \cbtm, no additional data is needed for inference, and we do not need to assume that adjacent documents in the test set come from the same distribution; we route based only on the current context. This also implies that the parameter averaging technique of \citet{btm} is not well suited to our setting, as it requires fixing the weights assigned to each expert for a set of evaluation documents.\footnote{Future work may explore merging expert parameters for each context during inference.} Our routing protocol does not require forward passes through each expert; it only relies on computation from the pretrained embedder and clusterer, which is much more efficient and can be parallelized easily.



\subsection{Comparing to Mixture-of-Experts (MoE)}
\label{sec:moe_comparison_summary}

Like MoE models \citep[e.g.,][]{https://doi.org/10.48550/arxiv.2209.01667}, \cbtm allows for efficient scaling of large LMs while keeping inference costs manageable. However, \cbtm routes sequences (instead of tokens) using offline balanced clustering (instead of online load balancing) with no shared parameters between experts. This eliminates effectively all complexities associated with balancing expert utilization \citep{baselayers}, avoids expensive all-to-all operations between experts \citep{artetxe2021efficient}, and naturally leads to interpretable expert specialization to domains of the training corpus.  In \S\ref{sec:moe_comparison}, we compare directly to MoE baselines trained with sparse upcycling  \citep{https://doi.org/10.48550/arxiv.2212.05055}, which initializes the MoE with a dense checkpoint, mirroring how \cbtm initializes ELMs.




\section{Experimental Setup}
\label{sec:experiments}

We design a set of experiments to study \cbtm on two large corpora (Figure \ref{fig:umaps}) selected to be distinct from  the corpus used to train our seed OPT model, and report perplexity on held out data from each corpus. 

% Due to resource constraints, we follow other studies on scaling language models \citep[e.g.,][]{kaplan2020scaling,https://doi.org/10.48550/arxiv.2202.01169} and do not perform evaluations of \cbtm models on other downstream tasks, although this is a ripe area for future work.\suchin{remove previous sentence if we get downstream task results from weijia}
% \nascomment{need to say that we focus on LM perplexity on held-out samples here; some readers are going to assume you do something with downstream tasks.  should probably say this in the intro somehow too}

\subsection{Data} \label{sec:data} 
%We choose our training datasets keeping in mind that we begin training with OPT as our seed LM. \luke{Is this still too subtle? Is the key point that our seed wasn't already trained on the data?}

% \nascomment{comments relating these to OPT seem weird; establish that OPT is the seed before you get to these?} \margaret{I added one mention of OPT1.3B in the intro, but it's kind of subtle}

\paragraph{C4 \citep{t5}} C4 is a publicly available distribution of a Common Crawl snapshot on Huggingface datasets.\footnote{\url{https://huggingface.co/datasets/c4}} We use the \emph{no blocklist} version (\texttt{en.noblocklist}) to train on a dataset that is out of distribution to our seed (OPT) pretraining corpus. C4 consists of 393M documents totaling 220B BPE tokens. We train on up to 168B tokens.



\paragraph{S2ORC \citep{s2orc}} The Semantic Scholar Research Open Corpus (S2ORC) is a publicly available corpus of full-text academic papers from the Semantic Scholar.\footnote{\url{https://allenai.org/data/s2orc}} The corpus spans 20 fields of study (e.g., Biology, Computer Science, Art), and contains 16M documents, totaling 87B BPE tokens. We train on up to 168B tokens over multiple epochs.\footnote{While it is not common to train large LMs for multiple epochs, we do not observe overfitting in any of our experiments, consistent with other studies that train LMs on academic literature for multiple epochs \citep{https://doi.org/10.48550/arxiv.2211.09085}.}

\paragraph{Evaluation data} For all experiments, we report language modeling perplexity on 200 randomly-sampled held out documents. Because S2ORC does not come with pre-defined validation data, we create a validation corpus by sampling an equal number of documents from each field of study. 




\subsection{Experimental Setup}
\label{sec:experimental_setup}

\paragraph{Clustering the data} We segment each corpus using  balanced $k$-means clustering for $k \in $ \{2, 4, 8, 16, 32, 64, 128\} (\S\ref{sec:cbtm_iteration}). To train the clustering models, we first embed all data with a tf-idf vectorizer using scikit-learn,\footnote{\url{https://scikit-learn.org/}} with minimal assumptions: we only remove stop-words from a fixed lexicon and replace numbers with a dummy token. We then reduce the dimensionality of the resulting embeddings; we perform truncated SVD  with 100 dimensions, then normalize the vector by removing its mean and scaling to unit variance, which we observed in initial experiments improved the clustering quality. Finally, these representations are clustered using a custom Pytorch implementation.\footnote{\url{https://github.com/kernelmachine/balanced-kmeans}}  We present learned clusters and visualizations in Figure \ref{fig:umaps} and Figure~\ref{fig:c4} (in the appendix). We use a single shard of each training corpus (384K documents for C4, 155K documents for S2ORC) to train the clustering model and its embedder. No evaluation data is used in this process.


\paragraph{Seed LM} As LMs trained on diverse corpora make for better seeds \citep{btm}, we use pretrained OPT language models \citep{opt} as our seed for all experiments.


\paragraph{Model hyperparameters} We use the OPT architecture implemented in Metaseq \citep{opt}. We use OPT-1.3B for the initial set of experiments, and replicate our experiments with OPT-6.7B. Following \citealt{opt}, we use the GPT-2 vocabulary of 50,257 BPE types \citep{radfordlanguage}, and train with 2,048-token sequences, across document boundaries. We prepend a beginning-of-document token to each document. We set dropout to 0.1 for all parameters except those of the embedding layer. 

\paragraph{Training hyperparameters}  For all models, we fix the learning rate to that used during OPT pretraining (2e-4 for 1.3B parameter models; 1.2e-4 for 6.7B parameter models; \citealt{opt}) using a linear decay learning rate schedule to zero (with no warmup), which we found to work well for most settings after a grid search of fastest learning rates that avoided divergence.  We use a batch size of 8 for each GPU, and train with \texttt{fp16} and fully-sharded data-parallel \citep{artetxe2021efficient}. We train on NVIDIA V100 32GB GPUs. All models are trained with Metaseq \citep{opt}.  For a given number of clusters $k$ and total GPU budget $n$, each ELM is allocated $n/k$ GPUs, keeping the total effective number of FLOPs fixed across models exposed to the same number of tokens. See \S\ref{sec:comparing_flop_counts} for more details.


% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\columnwidth]{figures/synchronicity.png}
    
%     \vspace{3em}
    
%     \includegraphics[width=0.9\columnwidth]{figures/synchronicity_v2.png}

%     \caption{\textbf{\cbtm improves the resilience and efficiency of distributed training.} LLMs are typically trained using hundreds or thousands of concurrent GPUs, all of which synchronize gradients each update. (i) C-BTM improves efficiency by reducing the communication overhead, as only GPUs training the same ELM must communicate. (ii) C-BTM improves resilience as a single GPU failure only delays training for a single ELM, whereas typically a single failure prevents all training. The chance of a GPU failure grows exponentially with the number of GPUs. (iii) Decomposing training into smaller jobs which run asynchronously can make job scheduling more efficient, by reducing the number of GPUs that need to be available. \margaret{two diff versions included, one shows nodes, other abstracts it away}}
%     %\caption{\textbf{\cbtm training is more efficient and robust in practice.} Though dense models could be trained on few GPUs in theory, this is practically infeasible -- LLM training typically uses hundreds or thousands of GPUs synchronously for weeks or months, rather than training on fewer GPUS for years or decades. This demand for synchronous compute means that dense LLM training is completely blocked if even a single GPU is occupied. Similarly, if a single GPU suffers failure, the entire training job fails and all other compute for the job sits idle. In our \cbtm setup, our total compute budget is split between experts such that it's practical to train each expert for a short time on few GPUs. It is easier to schedule jobs with any small number of GPUs which are free, rather than waiting for thousands of GPUs to be free simultaneously, and if a single GPU fails, only the single expert assigned to that GPU stops training; all other experts are unaffected. \margaret{rewrite/polish the caption} }
%     \label{fig:num_gpus}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{figures/num_gpus.pdf}
%     \caption{\textbf{Synchronous compute budgets for each model and training data size in our experimental setup (\S\ref{sec:experimental_setup})} To maintain a constant training runtime across our experiments, we train for 10K steps in all experiments, and to expose the model to more tokens, we increase the number of synchronous GPUs proportionally. Under this setup, to train on 168B tokens of text and maintain about constant training runtime of about \suchin{add this} hours, we use 1024 GPUs active synchronously for the single cluster (i.e. dense) model, while we use 8 GPUs synchronously per expert for the 128 cluster model. \suchin{changing this figure to a graphic that visually describes asynchronous training} }
%     \label{fig:num_gpus}

%     % \textbf{Increasing cluster count allows for efficient scaling into massive datasets with embarrassingly parallel training.} In our experiments, to train on 168B tokens of text and maintain about constant training runtime, we need 1024 GPUs active synchronously for the single cluster (i.e. dense) model, while we only need 8 GPUs active synchronously per expert for the 128 cluster model. However, our results in \S\ref{sec:core_results} suggest there is an optimal number of clusters per compute budget.\mike{This figure is a bit confusing. Maybe label the x-axis as number of tokens in N days?} \margaret{mike: this figure is confusing and doesn't make the point we're trying to make -- that it's easier to train multiple models in parallel than to have one giant train job. We should consider nixing this figure and making that point verbally elsewhere}
% \end{figure}




\paragraph{Scaling} We train for a total of 10K steps in each run; to expose the model to more tokens, we increase the total GPU budget proportionally, up to 64 GPUs. We simulate larger budgets, up to 1024 GPUs, by increasing gradient accumulation steps with 64 GPUs. This method of scaling increases the model's effective batch size for the same number of steps, and maintains near constant run-times across our many experiments. This experimental setup also means that as the number of clusters increases, the overall set of ELMs is exposed to more data with less simultaneous computation among GPUs. 
% (Figure \ref{fig:num_gpus}). 

% \luke{This seems like too much detail for a pretty simple point? Do we need a figure, or would a short text only description be enough?}


Other ways of training on more data (e.g., by keeping total batch size fixed and increasing step count) may yield different results. The best batch size and learning rate combinations for training language models are likely specific to a variety factors, including the model size, dataset, and total compute available \citep{https://doi.org/10.48550/arxiv.1811.03600, https://doi.org/10.48550/arxiv.1812.06162, NEURIPS2021_8df7c2e3}. In preliminary experiments, we found that expert models benefit from faster learning rates and larger batch sizes. Given a sufficiently large batch size, experts are robust to a variety of learning rates. Our larger budget experiments might benefit from higher learning rates, but we leave further tuning for future work. 


\paragraph{Inference} One of the key hyperparameters for inference is the temperature $T$ (Equation 3),  which governs the sharpness of the probability distribution over experts for a given context. We find that setting $T$=0.1 works well for most settings (see \S\ref{sec:appendix_sparsity} for more details). We also compute the nearest cluster centers for every incoming context, regardless of how stable the cluster assignments already are for a document. However, we find that these assignments can be fixed for the second half of a document with no drop in perplexity; this can further speedup inference. The other important hyperparameter is the top-k value, which sparsifies the probability distribution over experts. For our core experiments in \S\ref{sec:core_results}, we set top-k to the total number of experts we have trained for each model. We explore the effect of enforcing sparsity with lower top-k values in \S\ref{sec:sparsity_analysis}. 

\paragraph{Baselines} 
% In these core experiments, we focus our baselines on widely adopted dense models. Though sparse MoE models \citep share many of our motivations and some have some design similarities, it is not clear how best to compare to them, as sparsifying a dense pretrained model is relatively unexplored. We leave a more thorough exploration of sparse baselines for future work.
In our primary experiments (\S\ref{sec:results}), we compare with a strong dense baseline (i.e., our 1-cluster model) following OPT pretraining. We also progressively increase the number of clusters we train with for a fixed number of tokens. In subsequent experiments (\S\ref{sec:moe_comparison}), we compare to MoE language models initialized from a dense checkpoint.


\subsection{Making Fair Model Comparisons} 
\label{sec:fair_comparisons}

We follow the recommendations of \citet{https://doi.org/10.48550/arxiv.2110.12894} and report results with multiple cost metrics, and detail our choices here.
When comparing model training budgets, we are primarily concerned with the true monetary cost of model training, which is typically billed in direct proportion to GPU-time. Model inference comparisons have two main considerations: monetary cost incurred by the model deployer, again measured in GPU-time, and latency for end-users, or wall-clock time (i.e., how slow a model inference is for an end-user).

We explicitly do \emph{not} compare or match the number of model parameters during training, which has minimal bearing on the cost of model training separately from its influence on GPU-time. The number of training parameters is a particularly misleading cost measure that is unsuitable for sparse models, since they can maintain the FLOPs and inference speed of dense models despite training many more parameters \citep{https://doi.org/10.48550/arxiv.2110.12894}. 

   
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/c4_scale.pdf} 
    \vspace{-2em}
    \caption{\textbf{Increasing cluster count in \cbtm improves language modeling performance for a fixed compute budget (\S\ref{sec:core_results}).} Performance of \elm{}s trained with \cbtm as a function of the total number of tokens trained on, which, in our experiments, equalizes FLOP count. Training with more than one cluster always outperforms the compute-matched, single cluster dense model, and we observe improving performance (and in \S\ref{sec:time_comparison}, faster updates) as we increase the number of clusters.} 
    \label{fig:baseresults}
\end{figure}

\paragraph{Training GPU-time} Assuming fixed hardware, GPU-time during model training is determined mostly by FLOPs and inter-machine communications. However, prior work typically only FLOP-matches, ignoring the additional inter-GPU communications incurred by some models (e.g., MoE) that increase training costs. Ideally, our comparisons could directly fix GPU-time. This is challenging in practice, as even identical computations on the same GPU node at different times can vary wildly in speed due factors like  temperature, other activity on the node, or the quality of GPU interconnect. To maintain consistency and fairness despite these confounds, our results compare FLOP-matched models with the same training data budget over the same number of updates (\S\ref{sec:core_results}), but also report the speed of training for each FLOP-matched model (\S\ref{sec:time_comparison}). This allows us to disentangle and accurately reflect multiple cost metrics of training. Since, in our experiments, models exposed to the same number of tokens incur the same number of FLOPs, we use training data size as a more interpretable measurement of the overall training budget (see \S\ref{sec:comparing_flop_counts} for more details). 

\paragraph{Inference GPU-time} Inference GPU-time is also primarily the result of FLOPs and communication costs. Since communication during inference is minimal,  we compare FLOPs via inference parameters (\S\ref{sec:sparsity_analysis}). We do not account for the FLOPs of the \cbtm router, which varies based on the clustering approach, and is relatively negligible.

\paragraph{Inference latency} FLOPs is not an ideal metric for inference latency of our models, because \cbtm allows for parallel inference across ELMs. This means that if ELMs share the same architecture (e.g., OPT-1.3B), inference latency is  always equivalent to that of a single ELM, regardless of the number of experts active.  However, inference latency may be quite different between model architectures (e.g., OPT-1.3B and OPT-6.7B); we discuss this further in \S\ref{sec:zflops}. As with inference GPU-time, we do not consider the latency of the \cbtm router.

% \paragraph{Comparing seconds-per-update} To make fair comparisons of training time across models, we only compare models with the same \emph{effective} batch size (i.e., batch size per GPU $\times$ gradient accumulation steps $\times$ number of GPUs). This means we only compare models that use up to 64 GPUs with no gradient accumulation, since we increase gradient accumulation to simulate GPU budgets greater than 64 GPUs where needed (\S\ref{sec:experimental_setup}). 

% \paragraph{Comparing parameter counts at inference time} 
% We simply compare our models with top-$k$ sparsification. 

% \paragraph{Baselines} We compare our technique to four core baselines, each of which represents a different way to generate domains in a dataset.

% \begin{itemize}
%     \item \textbf{Random clusters} We assign each document to a random cluster.

% \item \textbf{Unbalanced clusters} We train our clusterer \emph{without} balancing.

% \item \textbf{Single cluster ($k$ = 1)} We treat all documents as a single cluster. This is equivalent to the dense model in \citealt{btm, gururangan-etal-2022-demix}.

% \item \textbf{Metadata-defined clusters} We assign each document to a cluster defined by metadata indicating its provenance (e.g., whether it is a Reddit document or medical research paper). This is equivalent to the expert routing technique proposed in \citep{gururangan-etal-2022-demix}.
% \end{itemize}

 
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/sweet_spot.pdf} 
    \vspace{-2em}
    \caption{\textbf{There exists an optimal cluster count for each compute budget (\S\ref{sec:core_results}).} The optimal cluster count increases as one increases the compute budget, but using too many clusters \emph{without} sufficiently increasing compute can degrade performance. For both C4 and S2ORC, 16 clusters gets the best performance at the highest budget (168B tokens), although higher cluster counts still outperform the 1-cluster (dense) model ($x$ = 1 in this graph).}
    \label{fig:optimalclusters}
\end{figure}  


\section{Language Modeling Results}
\label{sec:results}

% For large datasets, language modeling performance almost always improves by increasing overall compute, with little risk of overfitting  \citep{} \suchin{add cite}. Therefore, to decide whether one language model is preferable to another, we have to compare the baselines with respect to a fixed computational budget. There are a wide variety of budgets a researcher may use to compare models, including model size, training time, FLOPs, or memory usage. This choice depends on a variety of factors, including the intended use cases of the model, as well as the resource constraints of the researcher. However, a model that does well with respect to one computational budget may do poorly on other axes. For example, a larger model may get better performance than a smaller one for the same amount of FLOPs, but it be much more memory intensive or slower to train.

%We begin with a set of experiments which apply \cbtm to datasets from \S\ref{sec:data}. We compare the performance of our models against the three measurements of cost outlined in \S\ref{sec:fair_comparisons}, with a focus on how the performance trends change as we increase overall compute. 
% We are interested in measuring how performance changes as we increase overall compute, for all of our different ways of measuring compute.
We begin with a set of experiments in which we train LMs with \cbtm on datasets from \S\ref{sec:data}. We are interested in measuring how performance changes as we increase overall compute.
We first compare models against training costs: \emph{total training tokens} (\S\ref{sec:core_results}) and \emph{training time} (\S\ref{sec:time_comparison}). Then, in \S\ref{sec:sparsity_analysis}, we compare model performance along an axis of inference costs: the \emph{total parameter count at inference time}. Finally, in \S\ref{sec:zflops} we compare model performance by fixing both training and inference costs. Across all computational budgets, \cbtm provides substantial benefits over dense training, and performance improvements increase as the total compute grows.

                                                          

\subsection{Controlling for Total Training Tokens}
\label{sec:core_results}

First, we compare model performance controlling for overall training data size (or equivalently, training FLOPs; \S\ref{sec:fair_comparisons}). Figure \ref{fig:baseresults} shows evaluation perplexity on C4 and S2ORC with up to 16 clusters. Training on more than one cluster always outperforms training with a single cluster (i.e., a dense model). As the amount of training data grows, the gap between our models and the dense one widens, indicating that experts make better use of larger training datasets, possibly due to their increased specialization. These results suggest that as we increase the amount of data available, \cbtm benefits from more clusters.

However, Figure \ref{fig:optimalclusters} shows that there exists an optimal cluster count for each token budget that we consider. Each number of clusters has a budget range in which they are optimal, and the optimum smoothly progresses from smaller to larger cluster counts as we increase the training data size. If we increase the cluster count past the optimum, each expert has an insufficient share of the data, resulting in worse performance. 

Nevertheless, we observe that using more clusters than optimal for the highest token budget settings still outperforms the dense model. Since it is cheaper to train with more clusters for a fixed training data size due to parallelism, it may be preferable in some settings to train with a large number of clusters despite their less-than-optimal performance. Based on the trends we observe at this scale, we expect that higher cluster counts would become optimal as we scale the training data size even further. 

The consistency of our results on C4 and S2ORC suggests that these general trends may be widely applicable to many datasets.  However, the optimal number of clusters for a given computational budget is likely dataset specific. Future work may explore relationships between dataset features and the optimal cluster count.

These trends are consistent as we increase the size of our experts to 6.7B parameters (Figure \ref{fig:67b_results}), although the gaps between our baselines reduce, likely due to the substantial increase in pretraining FLOPs for OPT-6.7B.\footnote{OPT-6.7B was pretrained for 1.83 ZFLOPs, while the OPT-1.3B was trained for 0.34 ZFLOPS.}




\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/6.7b_results.pdf} 
    \vspace{-2em}
    \caption{\textbf{Our results are consistent even as we increase expert size to 6.7B parameters (\S\ref{sec:core_results}).} 8 clusters is optimal at 21B tokens, as it is for the 1.3B parameter ELMs. However, the gaps between these models are smaller, due to the substantial increase in pretraining FLOPs for the OPT-6.7B checkpoint. }
    \label{fig:67b_results}
\end{figure}

\subsection{Comparing Training Time}
\label{sec:time_comparison}


% Jobs that require fewer nodes generally incur less GPU communication overhead, leading to slightly faster training, and are less dependent on expensive ethernet connection channels to mediate fast communication between many nodes \margaret{Made some changes. I wonder if we should nix the faster training  mention and footnote altogether. Making a low level point and then kind of walking it back in a footnote seems unnecessarily complicated} \nascomment{I can see it either way; full disclosure is a good principle to follow, but only if it's a detail that's important for most readers to know}.\footnote{However, as of this writing, recent updates to Pytorch distributed data parallel which overlap communication and backward passes reduce multi-node communication overhead \citep{lofi}.}

% In short, embarassingly parallel training makes scaling language models onto massive datasets much more feasible due to the modest computational budget per expert and asynchronous training. 


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/spu.pdf} 
    \vspace{-1em}
    \caption{\textbf{Models trained with more clusters have faster updates as we increase the total compute (\S\ref{sec:time_comparison}).} We display the maximum seconds-per-update for \cbtm and MoE models with varying GPU counts (across all experts). Under fixed compute, training with more clusters uses fewer GPUs per expert, and \cbtm avoids communication between experts, resulting in faster updates. On the other hand, MoE models are much slower to train, due to extensive communication between experts (\S\ref{sec:moe_core_results}), as well as additional FLOPs from top-2 routing \citep{artetxe2021efficient}.}
    
    % \mike{Nit: why "max seconds/update" rather than average?} }
    \label{fig:ups}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/sparsity.pdf} 
    \vspace{-1em}
    \caption{\textbf{Sparse top-$k$ inference performance at 168B token budget (\S\ref{sec:sparsity_analysis}).} ELMs perform well even with heavily sparsified inference. Top-1 inference substantially outperforms the densely trained baseline at no additional inference cost, and top-2 and top-4 inference performs comparably to (and is sometimes slightly better than) activating all experts. In our setup, inference parameters are proportional to inference FLOP count (\S\ref{sec:fair_comparisons}).}
    \label{fig:sparsity}
\end{figure}

 Now, we turn to comparing our models based on training times. We measure the speed of training each model with the maximum seconds-per-update for each training run.\footnote{Other measures of seconds-per-update (e.g., average, median) tend to be noisy, due to factors such as dataloading and bursty GPU activity.} For \cbtm models with more than one cluster, we use the maximum seconds-per-update across all experts. To make our comparisons fair, we only compare the training times of models that have the same effective batch size (\S\ref{sec:fair_comparisons}). Our results are displayed in Figure \ref{fig:ups}. As we increase the number of clusters and training data size, the update speed for \cbtm \emph{increases}, since models with higher cluster counts use fewer GPUs per expert under a fixed budget, and there is no communication between experts. This suggests that \cbtm models with more clusters can be exposed to more data for the same amount time as dense models.


As discussed in \S\ref{sec:comparing_dense}, \cbtm also provides important practical speedups when training large LMs at scale. \cbtm divides large compute budgets among many models, such that we can train on 168B tokens with only 8 GPUs per expert in the 128-cluster setting. On shared multi-node clusters, allocating many smaller jobs incurs shorter cumulative wait times than a single, large synchronous job, since they can make more efficient use of shared resources, and run on short-lived, idle nodes \citep{lofi}. Furthermore, large LM training is prone to node failures, gradient spikes, and other unexpected behaviors \citep{opt}. With dense models, when one node fails, all nodes must restart due to synchronization. With \cbtm, experts are trained independently; if a node fails, only the corresponding expert needs to be restarted, and all other experts are unaffected.

\subsection{Controlling for Inference Costs via Parameter Count}
\label{sec:sparsity_analysis}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/sparsity_larger.pdf} 
    \vspace{-2em}
    \caption{\textbf{Train large, then sparsify (\S\ref{sec:sparsity_analysis}).} Despite using more than the optimal cluster count for the 168B token budget, sparsifying the 32, 64, and 128-cluster models with top-1, top-2, or top-4 experts is usually better than training 1-, 2-, or 4-cluster models.}
    \label{fig:sparsity_larger}
\end{figure}


% This is reminiscent of topk truncation applied in existing sparse models \citep{fedus2021switch, lepikhin2020gshard}, but this topk procedure is only applied at inference time.\footnote{During training, we implicitly set topk=1, since each context will only be routed to the single expert assigned to the cluster that the context originates from.}

Comparing models with just training budgets ignores the fact that \cbtm inference GPU-time costs grow as we increase the number of clusters, since we train more parameters. To mitigate these costs, we can use the top-k function (Equation \ref{eq:topk}) to dynamically use a subset of experts for each incoming context during evaluation (\S\ref{sec:cbtm_inference}). Next, we study the effect of inference parameter count on model performance.  We focus on the largest training budget (i.e., 168B tokens) for these experiments.

 Results (Figure~\ref{fig:sparsity}) show that despite training many more parameters, training \cbtm with many clusters and then using only the top-1 expert still outperforms the dense model. Further, using the top-2 or top-4 experts yields comparable performance to activating all experts. Sometimes we observe that sparsifying can even slightly \emph{improve} performance over using all experts (for example, see the 16 cluster model for C4 in Figure \ref{fig:sparsity}). We speculate that having all experts active may introduce interference effects from experts that are specialized to clusters unrelated to test-time contexts. 

Our results in Figure \ref{fig:sparsity_larger} suggest that sparsifying even larger expert models (i.e., those with more clusters than the optimal for a given token budget) is still highly effective. At the most extreme setting, using the top-1 expert for the 128 cluster model (using 0.7\% of total parameters at inference time for each context) still outperforms the dense model, and the top-4 expert model (3.1\% of total parameters) performs comparably to using all experts.


These results suggest that \cbtm results in a highly sparse LM, and that inference costs can be kept constant even as the number of experts grows, though additional experts can be added to further boost performance. 

% \paragraph{Train embarassingly parallel, then sparsify} We observe in \S\ref{sec:core_results} that for each budget, there is a point at which increasing clusters \emph{degrades} performance (e.g., for the highest compute setting, that point is at around 16 clusters). However, what is the effect of sparsifying these larger expert models? Our results in Figure \ref{fig:sparsity_larger} suggests that these sparsified models is still highly effective. At the most extreme setting, using the top-1 expert for the 128 cluster model (using 0.7\% of total parameters at inference time for each context) still outperforms the dense model, and the top-4 expert model (3.1\% of total parameters) performs comparably to using all experts. We observe similar characteristics with the 32 and 64 expert models. Generally, these results suggest that if one cares most about parameter count at inference time, one is better off training many experts  in embarrassingly parallel fashion and sparsifying the top-k distribution, rather than training models with a few clusters.  

\subsection{Comparing to a Larger Dense Model} 
\label{sec:zflops}

% We use the analytical FLOP calculation for our models in \S\ref{sec:experimental_setup}. Because we begin training with OPT checkpoints, we include FLOPs expended during pretraining in these computations.

% This means we have to train a larger dense model that is about the same size as a set of ELMs. Larger dense models are generally more expensive to train, since they require larger batch sizes to train effectively, but they also tend to achieve lower perplexities faster than smaller models do \citep{}\suchin{cite train large then compress}. How does training many smaller (1.3B parameter) ELMs compare to training a larger (6.7B parameter) dense model?


In our final comparison of this section, we consider both training and inference costs together. We compare a 6.7B 1-cluster (dense) model and \cbtm model with 1.3B parameter experts, which uses 16 clusters (optimal in our experiments from \S\ref{sec:core_results}) and top-4 inference, resulting in 5.2B inference parameters.  This \cbtm model has lower inference cost than the larger 6.7B parameter dense model (\S\ref{sec:fair_comparisons}). The former uses fewer inference parameters, incurring a smaller inference GPU-time cost, and has lower latency, comparable to that of a single 1.3B-parameter ELM. 


We compare the FLOPs used to train each model. Following \citet{artetxe2021efficient}, we build continuous efficiency curves by interpolating between our empirical observations. Specifically, we calculate the speedup between our cluster expert models and dense model by interpolating between the discrete observations of perplexity values for a given empirical number of FLOPs.\footnote{See \S\ref{sec:perf_interpolation} for details on this interpolation.} Our goal is to identify the FLOP count necessary to achieve a particular perplexity value. If ELMs trained with \cbtm achieve the same perplexity as the dense model with half the FLOPs, we conclude that \cbtm achieves a 2$\times$ speedup.

Our results are presented in Figure \ref{fig:zflopcomparison}. A smaller \cbtm model, exposed to 168B tokens of text, can achieve the same perplexity as the larger 6.7B dense model with 3.5$\times$ speedup. These speedup estimates are dependent on the amount of pretraining performed on each model.  Future work may perform these experiments  with larger models and many more ELMs.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/cbtm_flops.pdf}
    \caption{\textbf{Training with \cbtm is substantially more efficient than training a larger dense LM (\S\ref{sec:zflops}). }   We train a 16-cluster \cbtm model and use top-4 inference, resulting in a 5.2B parameter LM, and compare its performance with a 6.7B parameter dense LM. The \cbtm model at 168B tokens achieves the same perplexity on C4 as a 6.7B dense model with 3.5x fewer ZFLOPs. The total ZFLOPs includes the cost of pretraining the seed OPT checkpoints.}
    \label{fig:zflopcomparison}
\end{figure}



\subsection{Summary} 
\label{sec:results_summary}

Our results demonstrate that controlling for a variety of different types of computational budget, \cbtm outperforms dense training in language modeling. Furthermore, we demonstrate that \cbtm results in an effective \emph{sparse} language model, where the top-1, top-2 and top-4 experts from models with at least 8 clusters significantly outperform 1-cluster, 2-cluster and 4-cluster models. These results suggest the possibility of  outperforming dense models by increasing margins, while keeping both training and inference costs fixed, as compute and the number of experts grow.

% This method of inference requires computing an embedding of every context in a document being scored (or generated), which can be expensive. In practice, we find that one only needs to embed half of the document before the overall cluster stabilizes. 

\input{tables/downstream_tasks}

\section{Downstream Task Results}
\label{sec:downstream_tasks}

Do the trends from \S\ref{sec:results} extend to downstream settings? To begin to answer this question, we perform few-shot evaluation on six downstream text classification tasks. We indeed find that models trained with \cbtm outperform their dense counterparts in these settings.

\subsection{Experimental Setup}

\paragraph{Tasks} We experiment with six text classification tasks, spanning topic, sentiment, and hatespeech classification. Details of the datasets are in Appendix \ref{sec:appendix_downstream_tasks}.

\paragraph{Few-shot inference} We perform 8-shot evaluations. For each task, we randomly sample 8 examples with their labels from the train set, and prepend them as demonstrations for each test example. For \cbtm models, we estimate ensemble weights for each example by passing both the example and the demonstrations through our pretrained clusterer (\S\ref{sec:cbtm_inference}).  We calculate the probability of each label for the task under the model, and report accuracy by counting the proportion of test examples where the gold label has the highest probability. We report average accuracy over 5 random seeds. We leave careful analysis of \cbtm with varying numbers of demonstrations and few-shot inference techniques to future work.


\paragraph{Baselines} We compare the performance of 1- and 16-cluster \cbtm models trained on 168B tokens of C4 (i.e., our highest budget from \S\ref{sec:results}). For the 16-cluster model, we also perform top-1 and top-4 inference (\S\ref{sec:sparsity_analysis}). We additionally compare against a random baseline, the original OPT-1.3B and 6.7B models (without any additional training), and the 6.7B parameter 1-cluster model trained on 20B tokens of C4.



\subsection{Results}
\label{sec:downstream_task_results}

% Results, shown in Table~\ref{tab:downstream_tasks}, compare the performance, measured in downstream task accuracy, of baseline dense (1-cluster) models with 16-cluster \cbtm models, which were shown in \S\ref{sec:core_results} to achieve the best language modeling perplexity out of the settings we tested. These two models were seeded (i.e. initialized) from a pretrained 1.3B parameter OPT model and trained further on C4 \margaret{add s2orc if we do those experiments}. For the 16-cluster model, we show the effect of sparsified top-1 and top-4 expert activation, as well as the difference between clustering-based and performance-based routing. We also compare with a random chance baseline, representing a model which outputs a random label, and the original OPT pretrained 1.3B parameter models (without any additional training). We add explicit comparisons to 6.7B parameter models, which have been trained with a much larger compute budget, including the original OPT 6.7B model and our 1-cluster 6.7B parameter baseline, to better illustrate the improvements our method provides.

Our results in Table \ref{tab:downstream_tasks} show that the 16-cluster \cbtm model always outperforms the 1-cluster, 1.3B parameter baseline, sometimes dramatically. This aligns with our language modeling results (\S\ref{sec:core_results}). The 1-cluster model achieves lower accuracy than OPT-1.3B on some tasks despite additional training, suggesting that our models may suffer from catastrophic forgetting, since the C4 corpus is out-of-domain to OPT. 


Nevertheless, the 16-cluster model outperforms OPT-1.3B on all tasks other than Twitter. Also, top-1 and top-4 inference matches or exceeds using all experts in some settings, consistent with our language modeling results in \S\ref{sec:sparsity_analysis}. We examine the clusters associated with the most likely experts for each task, and find that their top-terms are relevant to the task's domain (Table \ref{tab:topk_examples} in the appendix). This supports our hypothesis that \cbtm is able to leverage any part of the corpus which is in-domain to the test task, even if the training corpus as a whole might be sufficiently out-of-domain as to have a negative effect on performance. 

We then mirror the analysis in \S\ref{sec:zflops}, by comparing our 16-cluster models to 6.7B parameter dense models. First, we observe that our 1-cluster 6.7B model outperforms OPT-6.7B on all tasks except Twitter, possibly because this model has had less exposure to C4, and suffers from less catastrophic forgetting. Our 16-cluster model performs comparably to both 6.7B models, and on multiple tasks, our 16-cluster model \emph{outperforms} both 6.7B models, which have been trained with at least 3.5$\times$ more compute (\S\ref{sec:zflops}). With top-4 inference, \cbtm models activate even fewer parameters than the 6.7B parameter models, yet perform comparably. These results corroborate our findings in \S\ref{sec:zflops} that compared to larger dense models, models trained with \cbtm have more training efficiency and lower inference latency, and result in comparable or better performance.

In separate experiments, we observe that routing examples to experts based on their performance on few shot examples, rather their clusters, results in even better downstream task performance with \cbtm models. This is likely because few-shot performance depends on factors such as example order, label distributions, and the quality of the demonstrations, which not necessarily tied to the domain of the task \citep{min-etal-2022-rethinking, lu-etal-2022-fantastically}. We analyze this finding further in \S\ref{sec:appendix_downstream_tasks}, and leave more careful development of routing protocols for downstream tasks to future work.

\subsection{Summary}

We demonstrate that, consistent with the language modeling results in \S\ref{sec:core_results}, \cbtm improves downstream performance on a variety of few-shot text classification tasks. \cbtm models consistently outperform dense 1-cluster baselines, and usually outperform the original OPT models, despite being trained on an out-of-domain corpus. We also find that top-$k$ activation reduces inference costs with negligible effects on downstream task performance. \cbtm models perform comparably to larger, 6.7B OPT and 1-cluster dense baseline models, despite being trained with 3.5x less compute, and even when activating fewer inference parameters.


\section{Comparing to Mixture-of-Experts}
\label{sec:moe_comparison}

Finally, we compare \cbtm against an alternative sparse LM, a mixture-of-experts (MoE) which learns a routing between tokens and feedforward experts in the transformer \citep{lepikhin2020gshard, fedus2021switch}. As discussed in \S\ref{sec:moe_comparison_summary}, \cbtm is substantially simpler than MoE.

\subsection{Sparse Upcycling}

To mirror \cbtm seed initialization, we initialize our MoE with a dense checkpoint. We use the \emph{sparse upcycling} technique from \citet{https://doi.org/10.48550/arxiv.2212.05055}. Upcycling a dense model into an MoE with $k$ experts entails initializing shared parameters (e.g., attention and embedding layers) and $k$ expert parameters (e.g., every other feedforward layer) from a dense checkpoint, and initializing new parameters for the router. Then the model is simply trained as an MoE. Here, we use top-2, token-level routing \citep{lepikhin2020gshard}. 


\subsection{Experimental Setup} 

\paragraph{Hyperparameters} We train an MoE with sparse upcycling on C4, starting from OPT-1.3B and using the same general experimental setup detailed in \S\ref{sec:experimental_setup}. We follow the settings from \citet{https://doi.org/10.48550/arxiv.2212.05055} as closely as possible. We conducted experiments with 8, 16, 32, 64, and 128 experts for each compute budget. 8 and 16 experts are similar to, but slightly worse than, 32 experts; 64 experts and 128 experts consistently have exploding losses, and the few which successfully train are also similar to but slightly worse than 32 experts. In general, we find that both large expert count (and higher compute budgets) result in sparse upcycling training instability. 

We use 32 experts in our MoE, a capacity factor of 2, and continue training without resetting the optimizer from that used during OPT pretraining. We set all hyperparameters to be the same as our \cbtm models (\S\ref{sec:experimental_setup}), except that we use a peak learning rate of 2e-5, which we found to be the highest learning rate that that did not result in divergence after a sweep. We release our code for sparse upcycling, implemented in Fairseq \citep{https://doi.org/10.48550/arxiv.1904.01038}, publicly.\footnote{\url{https://github.com/kernelmachine/moe-fairseq}}

\paragraph{Baselines} We compare the 32-expert MoE LM to 1-cluster (i.e., dense) and 16-cluster \cbtm models. 



\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/moe_scale.pdf} \vspace{-2em}
    \caption{\textbf{MoE underperforms \cbtm  (\S\ref{sec:moe_core_results}).} We compare a 32-expert MoE with top-2 routing \citep{lepikhin2020gshard} trained with sparse-upcycling \citep{https://doi.org/10.48550/arxiv.2212.05055}. While the MoE outperforms \cbtm models with 16 experts at small budgets, it fails at larger budgets, even under-performing the dense model. We speculate this could be due to distribution shifts after pretraining, which might increase the instability of upcycling.}
    \label{fig:sparse_upcycling}
\end{figure}

\subsection{Results}
\label{sec:moe_core_results}

The MoE expends more FLOPs than the other models due to the additional feedforward layer at every other transformer block for top-2 routing, as well as the routing projection \citep{artetxe2021efficient}. For clarity and consistency, we update-match and separately report GPU-time cost of updates, as in \S\ref{sec:core_results}.

We display results in Figure \ref{fig:sparse_upcycling}.   MoE substantially underperforms \cbtm with 16 clusters as the compute budget grows. Surprisingly, we observe that with enough compute, MoE underperforms even the dense LM, and that when compute budgets are further increased, losses consistently explode. This suggests that sparse upcycling is highly unstable, possibly due to distribution shifts from pretraining. 

In Figure \ref{fig:ups}, we compare the maximum seconds-per-update of the MoE model with that of the \cbtm models. MoE becomes substantially slower as more GPUs are used during training. This is largely due to expensive all-to-all communication that occurs between experts during MoE training, which is necessary to route tokens to experts \citep{artetxe2021efficient}. On the other hand, our method does not have any shared parameters between experts. Also, MoE expends more FLOPs during training than the \cbtm models. Finally, MoE still requires synchronous compute to train experts due to shared parameters, so they are also afflicted by the practical difficulties of training dense language models at scale \S\ref{sec:time_comparison}. 

% Our 16-cluster \cbtm model substantially outperforms the equivalent MoE even with heavily sparsified inference. For example, the 16-cluster model achieves a perplexity of 14.95 with top-2 inference, which is still better than MoE at the same budget.

\subsection{Summary}

Our results suggest that language models trained with \cbtm substantially outperform MoEs trained to the same budget. The performance gains of our technique likely are a result of the simplicity of our deterministic routing (based on empirically derived clusters), instabilities associated with sparse upcycling, and other factors.

\section{Analysis}
\label{sec:analysis}

In \S\ref{sec:results}, \S\ref{sec:downstream_tasks}, and \S\ref{sec:moe_comparison}, we demonstrate that \cbtm outperforms compute-matched densely trained and MoE baselines. We now study our clustering approach in more detail and describe its effect on overall performance of \cbtm.

\subsection{Is clustering important?}
\label{sec:analysis_random}

To assess the importance of the clustering algorithm, we perform \cbtm as above, except that we assign each document to a random cluster, rather than a learned one. This is equivalent to the random ensemble baseline from \citet{btm}. Results in Figure~\ref{fig:random} demonstrate that using random clusters dramatically underperforms both our method and the dense baseline. Therefore, cluster specialization is vital for \cbtm. This confirms results from \citet{btm}, who found that domain specialization of ELMs is critical for performance the ensemble, as well as those from \citet{https://doi.org/10.48550/arxiv.2302.03202}, who show that instruction-specialized ELMs transfer to other tasks with similar instructions.


\subsection{Is it important to balance the clusters?}
\label{sec:analysis_balancing}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/random_scale.pdf} \vspace{-2em}
    \caption{\textbf{Random clusters underperform (\S\ref{sec:analysis_random}).} Training experts on random clusters underperforms even the dense, single cluster model, showing the importance of cluster specialization. Random clusters become more harmful as the cluster count grows.}
    \label{fig:random}
\end{figure}


\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/unbalanced_scale.pdf} \vspace{-2em}
    \caption{\textbf{Cluster balancing improves performance (\S\ref{sec:analysis_balancing}).} When training on C4 with 32 clusters, balancing consistently improves over the unbalanced version, suggesting that cluster size important aspect to \cbtm. }
    \label{fig:unbalanced}
\end{figure}


Applying a balancing constraint in $k$-means  avoids the degenerate outcome of a long tail in cluster sizes (\citealt{https://doi.org/10.48550/arxiv.1411.6235}). Indeed, with 10K documents of held out validation data in C4, we observe that balanced clustering  significantly increases the median cluster size, and narrows its range, relative to an unbalanced baseline (\S\ref{sec:appendix_balancing}).  To assess the effect of balancing cluster size on the performance of \cbtm, we perform \cbtm with a $k$-means clustering model but remove the balancing constraint. For the 8-cluster model, we observe that balancing has little effect. However, for the 32-cluster model (Figure \ref{fig:unbalanced}), unbalanced clustering consistently leads to worse performance. These results suggest that balancing becomes more important as one scales the number of clusters. This is consistent with separate experiments that show that the long tail in cluster sizes becomes a more consistent problem with higher cluster counts.


\subsection{Are clusters well defined? Do experts specialize?}
\label{sec:analysis_specialze}

Since we use tf-idf as our document embedder in \cbtm, we can perform an inverse transform from the cluster centers into the vocabulary space to identify terms that most likely would have been embedded as the cluster center. We display the top five terms per cluster in \S\ref{sec:cluster_terms}. We observe that as the number of clusters increases, the top terms across clusters become more specific and varied.

Next, we study whether ELMs trained on these clusters specialize. Using the 32-cluster model trained on 84B tokens of C4, we compute perplexity of all experts across 200 held out documents in each cluster. 
For each cluster, we then measure the ratio of the perplexity of each expert to the perplexity of the expert trained on that cluster. 
We display those ratios in Figure \ref{fig:heatmap}. We see that all experts perform best on their own cluster. Some experts do not transfer well at all to other clusters, while others do reasonably well. Cross referencing with the cluster term tables in  \S\ref{sec:cluster_terms}, we see that cluster experts 3 and 5 tend to generalize well and the top terms in these clusters are more generic (with words such as "\emph{just}, \emph{like}, \emph{love}").  The experts specialized to content such as \emph{"site, page, website"} (cluster 0) and \emph{"app, phone, video"} (cluster 29), tend to do poorly on all other clusters.\footnote{This result imply that cluster experts can be removed to filter out unwanted generations after training, without significantly impacting performance on other content. We leave such exploration to future work.} These results suggest that experts specialize to their cluster. We infer that the success of sparse \cbtm inference is a result of expert specialization, and that \cbtm performance gains may be partially due to the sample efficiency of specialized training.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/ppl_heatmap.pdf} 
    \vspace{-2em}
    \caption{\textbf{Experts specialize to their cluster (\S\ref{sec:analysis_specialze}).} Here, we use the 32-cluster model trained on 84B tokens of C4. Each cell is a ratio between one experts test perplexity on a cluster to that of the expert trained on that cluster. The diagonal indicates that experts perform best on the cluster they were trained on. Many experts transfer well across clusters, but some do not.}
    \label{fig:heatmap}
\end{figure}

\subsection{How do clusters and metadata domains compare?}
\label{sec:analysis_metadata}

The key motivation for \cbtm is to remove the reliance on metadata to delineate the domains to which ELMs specialize. How well do clusters reflect the dataset segmentation produced by metadata? We use S2ORC to study this question. First, we align the learned clusters from a 32-cluster model with the fields-of-study metadata available from S2ORC \citep{s2orc}. Then we visualize the overlap between the metadata and clusters (Figure \ref{fig:s2orc_purity}).   We observe only a partial alignment between metadata and clusters in S2ORC. Documents with some metadata labels (e.g., Enviromental Science, Political Science) are mostly assigned to their own clusters, while documents with other labels (e.g., Computer Science, Physics) are distributed across multiple clusters. 

The partial alignment between metadata and clusters suggests that \cbtm models may not have the same performance as those trained with metadata labels to delineate domains. To investigate this hypothesis further, we perform experiments using  a subset of the Pile \citep{pile} to compare the performance of experts trained with metadata and experts trained with clusters. See \S\ref{sec:metadata_comparison} for more details on this corpus. We observe that experts trained with learned clusters perform slightly better than those with metdata labels on a held out validation data (Table \ref{tab:metadatacompare} in the appendix). Both techniques perform better than training with just a single cluster on the Pile, confirming our results from \S\ref{sec:core_results}. These results imply that metadata may not correspond with the most optimal segmentation of the corpus. However, using metadata has the advantage of interpretability and simplicity, and metadata can identify domains that are not just lexically driven \citep[e.g.,][]{lucy-bamman-2021-characterizing, gururangan-etal-2022-demix}. Future work may explore combining metadata- and cluster-specialized ELMs.


\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.5]{figures/s2orc_purity.pdf}
    \caption{\textbf{Clusters and metadata do not perfectly align (\S\ref{sec:analysis_metadata}).} Each cell in the heatmap is the \% overlap between a cluster and  a metadata label identifying the field-of-study of a document in S2ORC; high overlap indicates that most documents with the corresponding label get assigned to the corresponding cluster. While documents with certain labels (e.g., Environmental Science, Political Science, Business) get primarily assigned to a single cluster, documents with other labels (e.g., Engineering, Physics, Computer Science) are distributed across multiple clusters.}
    \label{fig:s2orc_purity}
\end{figure}

% We observe that metadata and clustering labels do not always correspond (Appendix Figure \ref{fig:purity}). To compare our clustering approach to metadata-defined domain experts, we train on the Pile \citep{pile}. Each dataset in our subset of the Pile (listed in Appendix~\ref{sec:metadata_comparison}) is considered one domain. Results are listed in Appendix Table \ref{tab:metadatacompare}. Performance as measured through test perplexity is comparable. This suggests that clustering effectively provides flexibility without suffering performance losses from removing metadata labels. While clusters and metadata labels are distinct segmentations of a corpus, future work may use them in a hierarchical manner.

\subsection{Summary} Our analysis demonstrates that the improvements from \cbtm are not the result of ensembling alone. Various components of our training method, particularly the nature of the learned clusters, play a critical role in \cbtm performance. Improving the representation of domains in a corpus, perhaps using other pretrained representations \citep{aharoni-goldberg-2020-unsupervised} or more sophisticated clustering algorithms \citep{10.5555/3001460.3001507,chronopoulou-etal-2022-efficient}, are likely to improve \cbtm performance.


\section{Related Work}

\paragraph{Sparse Models} \cbtm is closely related to sparse models which activate only a subset of  parameters \citep{pmlr-v119-evci20a,pmlr-v97-mostafa19a,dettmers-sparse-from-scratch}. \cbtm is inspired by MoE, but is much simpler and more efficient to train. Most MoE methods rely on training token-based routing mechanisms \citep{lepikhin2020gshard,fedus2021switch,baselayers,roller2021hash}, but others rely on task \citep{https://doi.org/10.48550/arxiv.2110.03742} or domain \citep{gururangan-etal-2022-demix} routing. 

\paragraph{Expert Language Models} As we note throughout the study, this work is most directly related to BTM \citep{btm}. BTM is in turn partially inspired by prior work on variations of MoE models \citep{jacobs1991adaptive}, but especially DEMix layers \citep{gururangan-etal-2022-demix}, which replace transformer feedforward layers with metadata-defined domain experts. \citet{https://doi.org/10.48550/arxiv.2302.03202} train expert language models on instruction-based tasks, while \citet{pfeiffer-etal-2022-lifting} train expert language models on different languages. 

\paragraph{Cluster Routing} \citet{chronopoulou-etal-2022-efficient} and \citet{Chronopoulou2023AdapterSoupWA} use hierarchical clustering to identify domains to specialize adapter experts to, and use the adapters in an ensemble or parameter average at inference time. \citet{Duan2021EnsLMEL} build ensembles of task-specific models by clustering the training data of supervised tasks. \citet{gross2017hard} employ a cluster-based router similar to ours in an image classification setting using ResNets. However, they use a hard routing (or only activate a single expert) in both training and inference, while we use hard routing during training but ensemble experts during inference. Our inference technique is inspired by nearest neighbor retrieval mechanisms in language models \citep{https://doi.org/10.48550/arxiv.1911.00172,https://doi.org/10.48550/arxiv.2205.13792}. 

\paragraph{Communication-efficient training} Our study contributes to a line of research into communication-efficient algorithms for training large models. Some previous work proposes ways to train large dense models collaboratively over distributed networks of servers \citep{https://doi.org/10.48550/arxiv.2209.01188, https://doi.org/10.48550/arxiv.2206.01288}. Other works focus on new forms of data \citep{https://doi.org/10.48550/arxiv.2107.01499}, model \citep{https://doi.org/10.48550/arxiv.2301.11913}, and pipeline \citep{https://doi.org/10.48550/arxiv.2206.01299} parallelism to allow for model training on heterogeneous devices that can recover from node failures. \citet{lofi} propose a communication-efficient method of fine-tuning by training a collection of models with different hyperparameters on individual GPU nodes, and then averaging their parameters after training. Our work uses expert specialization for communication efficient training, and \cbtm can be combined with any of these other techniques to improve training efficiency.

% There is ample literature exploring ensemble methods in machine learning, for example in bagging, boosting, and stacking \citep{breiman1996bagging,freund1995boosting,wolpert1992stacked}. However, the models in an ensemble are typically trained on random splits of data. Related to the idea of ensembling full models is that of sparsely activating only a portion of model parameters \citep{pmlr-v119-evci20a,pmlr-v97-mostafa19a,dettmers-sparse-from-scratch}. Of these, our work is most directly related to the Branch-Train-Merge \citep{btm} training procedure. BTM is in turn partially inspired by prior work on variations of Mixture-of-Experts (MoE) models \citep{jacobs1991adaptive}, but especially DEMix layers \citep{gururangan-etal-2022-demix}, which replace Transformer feedforward layers with metadata-defined domain experts.

\section{Conclusion}

We introduce \cbtm, a new technique to efficiently train sparse LMs. \cbtm splits a corpus into $k$ clusters, trains an expert LM on each cluster, and creates a sparse ensemble during inference. We observe that the optimal number of clusters for \cbtm increases with the amount of data, and using more clusters also allows us to aggressively parallelize training to efficiently scale into massive datasets. Future work could investigate \cbtm in multitask or multilingual settings, the usefulness of multiple iterations of \cbtm on a corpus (perhaps with hierarchical clustering), or the possibility of combining metadata- and cluster-based routing to scale into many heterogeneous datasets in parallel. 

\section*{Acknowledgements}

This paper benefited from thoughtful feedback from a number of people: Armen Aghajanyan,  Tim Dettmers, Sneha Kudugunta, Stephen Roller, Swabha Swayamdipta, and Mitchell Wortsman.
\newpage
% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{icml2023}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn
\section{Appendix} \label{sec:appendix}
\begin{figure*}[h!]
\begin{subfigure}
  \centering
  \includegraphics[width=\textwidth]{figures/c4.pdf}
  \label{fig:c4}
\end{subfigure}%
\begin{subfigure}
  \centering
  \includegraphics[width=\textwidth]{figures/s2orc.pdf}
  \label{fig:s2orc}
\end{subfigure}
\caption{Fully annotated UMAP visualization of 32 clusters in C4 and S2ORC. We annotate the clusters using an inverse transformation from our cluster centers back into the tf-idf vocabulary space, identifying the most likely words to generate the cluster center. We display the top 3 terms associated with each cluster center here.}
\label{fig:full_annotations}
\end{figure*}

\subsection{Clusters} \label{sec:cluster_terms}
\input{tables/2cluster_s2orc.tex}
\input{tables/8cluster_s2orc.tex}
\input{tables/32cluster_s2orc.tex}

\input{tables/2cluster_c4.tex}
\input{tables/8cluster_c4.tex}
\input{tables/32cluster_c4.tex}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{figures/c4.pdf}
%     \caption{Fully annotated UMAP visualization of 32 clusters in C4. We annotate the clusters using an inverse transformation from our cluster centers back into the tf-idf vocabulary space, identifying the most likely words to generate the cluster center. We display the top 3 terms associated with each cluster center here.}
%     \label{fig:c4}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{figures/s2orc.pdf}
%     \caption{Fully annotated UMAP visualization of 32 clusters in S2ORC. We annotate the clusters using an inverse transformation from our cluster centers back into the tf-idf vocabulary space, identifying the most likely words to generate the cluster center. We display the top 3 terms associated with each cluster center here. }
%     \label{fig:s2orc}
% \end{figure*}

% We use the Pile to compare the performance of experts trained with metadata and experts trained with clusters. Our results are shown in Table \ref{tab:metadatacompare}; we observe that experts trained with either metadata labels or learned clusters perform comparably on a held out evaluation set. However, both do better than training with just a single cluster on the Pile, confirm our results from \S\ref{sec:core_results}. These results are consistent with the alignment between metadata and clusters (Figure \ref{fig:heatmap}). For most the constituent datasets of the Pile, clusters are able to recover their segmentation and align well with metadata labels. Some datasets (e.g., Common Crawl, OpenWebText) are unsurprisingly more diffuse in their cluster allocations, likely due to their heterogeneity, but this does not seem to have a strong effect on overall performance. 


\subsection{Comparing FLOP counts via training data size}
\label{sec:comparing_flop_counts}

To make fair comparisons across models with different numbers of ELMs, for a given number of clusters $k$ and total GPU budget $n$, each ELM is allocated $n/k$ GPUs. This keeps the total effective number of FLOPs fixed across models exposed to the same number of tokens. We can show this analytically; following \citealt{artetxe2021efficient}, we calculate the number of FLOPs to train a single ELM in our experiments:

\[ F_{\text{ELM}(T, k)} = \frac{96lh^2T}{k} \left (1 + \frac{s}{6h}  + \frac{V}{16lh} \right) \]

where $T$ is the total training tokens (i.e., sequence length $\times$ batch size per GPU $\times$ number of GPUs), $k$ is the number of clusters, $l$ is the number of layers, $h$ is the hidden dimension, $s$ is the sequence length, and $V$ is the vocabulary.

Therefore, the total cost in FLOPs to train $k$ ELMs with a particular model architecture (e.g., OPT-1.3B) on $T$ tokens in aggregate is equivalent to that of a single dense LM of the same architecture trained on $T$ tokens:

\[ k \cdot F_{\text{ELM}(T, k)} = F_{\text{ELM(T, 1)}} \]

This means that even though \cbtm trains $k$ times more parameters than an equivalent dense model, it does so \emph{at the same overall cost in FLOPs}. So, our comparisons of models with various numbers of ELMs are fair, as long as they have been exposed to the same number of training tokens and have the same underlying architecture for each ELM. Therefore, throughout the paper, we use training data size as a more interpretable metric of the overall compute budget.

\subsection{Interpolating between empirical observations when comparing training costs and performance}
\label{sec:perf_interpolation}

We interpolate between our empirical observations using the following function, proposed in \citealt{artetxe2021efficient}:

\[ c(t)  = \exp (\log c_{lo}(t) + r (
\log c_{hi}(t) - \log c_{lo}(t)))\]

where $r=\frac{t - t_{lo}}{t_{hi} - t_{lo}}$, $t_{hi}$ and $t_{lo}$ are the closest empirical performances to $t$ and $c_{lo}(t)$ and $c_{hi}(t)$ are their corresponding training cost in FLOPs. We use this interpolation to compute the speedup factor $c_{dense}(t) / c_{cbtm}(t)$.

\subsection{Effect of cluster balancing on cluster sizes}
\label{sec:appendix_balancing}

Using a held out set of 10K documents from C4, we ablate our balancing procedure from \S\ref{sec:cbtm_iteration}, and display a boxplot showing cluster sizes in Figure \ref{fig:cluster_sizes}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/cluster_sizes.pdf} \vspace{-1em}
    \caption{\textbf{Cluster balancing narrows the range, and increases the median size, of clusters  (\S\ref{sec:analysis_balancing}).} Here, we ablate our balancing procedure from \S\ref{sec:cbtm_iteration} on a 10K held-out documents in C4.}
    \label{fig:cluster_sizes}
\end{figure}


\subsection{The Pile experiments}
\label{sec:metadata_comparison}

For the experiment in \S\ref{sec:analysis_metadata}, we additionally train on The Pile, which is a publicly available corpus of diverse language modeling datasets. We use the filtered version from the OPT pretraining data \citep{opt}, and subselect 8 datasets of the 13 used for OPT pretraining, which enabled easier experimentation \S\ref{sec:analysis_metadata}. These 8 datasets include: Common Crawl, HackerNews comments, Reddit comments\footnote{Reddit comments, like the rest of these datasets, are not collected by us but were part of the Pile \citep{pile}, a publicly available third-party dataset.}, the Gutenberg corpus, STORIES corpus, OpenWebText2, Deepmind-Mathematics, and English Wikipedia. In aggregate, these datasets consist of 420M documents, totaling 116B BPE tokens. For evaluation, we sample an equal number of documents from each constituent dataset. We also train a $k$=8 clustering model on this dataset with one shard, or 1.5M documents.

\input{tables/metadata_expert_comparison.tex}

\subsection{Sparsity}
\label{sec:appendix_sparsity}

\input{tables/inference.tex}

\subsection{Downstream tasks}
\label{sec:appendix_downstream_tasks}

\paragraph{Tasks} We experiment with six text classification tasks which span topic classification (AGNews; \citealt{zhang2016characterlevel} and DBPedia; \citealt{dbpedia}); sentiment analysis (SST-2; \citealt{maas-etal-2011-learning}, Amazon; \citealt{zhang2016characterlevel}, and Phrasebank \citealt{Malo2014GoodDO}); and hatespeech detection (Twitter; \citealt{barbieri-etal-2020-tweeteval}).

\paragraph{Performance Routing}

We introduce additional routing procedures for few-shot text classification, as the clustering method described in \S\ref{sec:cbtm_inference} ignores the significance of labels and does not take into account the context's word order, which may be crucial when the context contains demonstrations for a downstream task. Further, the specific ordering of in-context demonstrations is known to affect model performance~\cite{lu-etal-2022-fantastically}. There is not sufficient evidence to suggest that the relative rank of different models on a task stays constant through these performance variances; thus it may be important to route to experts differently depending on demonstration example order. To take into account the order of tokens in the context, we introduce 3 variations on routing, based on expert performance on the end task, which take inspiration from the mixture probability estimation methods of \citet{gururangan-etal-2022-demix,btm}. 

To perform \textit{Fixed Performance Routing with demonstrations and validation set examples}, we select 8 examples randomly from the validation set, such that there is no overlap with the 8 demonstration examples used in the context at test time. We concatenate the 8 demonstrations with one validation set context and evaluate the accuracy of each \elm on the sequence, repeating for each of the 8 examples from the validation set. The final routing probability distribution over experts for this task is determined by a softmax over the average accuracy of the model over the 8 examples. We use this fixed probability distribution for all test examples. 

To perform \textit{Fixed Performance Routing with only demonstrations}, we adapt the procedure above such that no validation examples are necessary. Instead, we take a random permutation of the 8 demonstration examples. We remove the label of the last, such that we effectively use the first 7 as demonstrations, and rely on the last example to estimate performance. We repeat this 8 times, generating a new random permutation each time, and once again take the softmax over the average accuracy of the model for each permutation, fixing this distribution for all test examples.

Finally, we have \textit{Updating Performance Routing}, in which no estimations are done before test-time. At test time, we begin with a uniform probability over all experts, which we update with an exponential moving average with each test example: after each example (prepended with the 8 demonstrations), we update the expert probabilities with the softmax over the accuracy of each expert on that example. Once again, this distribution is fixed for all test examples.


\paragraph{Results} Full results, in Table~\ref{tab:downstream_tasks_appendix}, show that \textit{Fixed Performance Routing with demonstrations and validation set examples} achieves the best performance overall, with optimal performance occurring at top-4 expert activation, which we also found in the language modeling results of \S\ref{sec:core_results}. Both \textit{Fixed Performance Routing} methods perform best with top-4 expert activation, and only suffer small performance degradations when reduced to top-1 expert activation. This aligns well with the patterns observed in \S\ref{sec:core_results}, which further supports the incorporation of end task performance in routing when adapting to new tasks, even when we base this evaluation only on the demonstration examples -- that is, without any additional data. We leave for future work further tuning of the optimal settings for Performance Routing.


\input{tables/topk_examples}


\input{tables/downstream_tasks_appendix}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
