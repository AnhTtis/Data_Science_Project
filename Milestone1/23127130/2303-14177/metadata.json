{
    "arxiv_id": "2303.14177",
    "paper_title": "Scaling Expert Language Models with Unsupervised Domain Discovery",
    "authors": [
        "Suchin Gururangan",
        "Margaret Li",
        "Mike Lewis",
        "Weijia Shi",
        "Tim Althoff",
        "Noah A. Smith",
        "Luke Zettlemoyer"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "abstract": "Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14177v1"
    ],
    "publication_venue": null
}