%%
%% WinShell 3.3.1.10
%% http://www.winshell.org/
%%

% Strings


% Preamble


% BibTeX Entries
@article{gardner90,
 author               = {{Gardner RM, Golubjatnikov OK, Laub RM, Jacobson JT, Evans RS}},
 journal              = {Comput Biomed Res},
 pages                = {514-28},
 title                = {{Computer-critiqued blood ordering using the HELP system}},
 volume               = {23},
 year                 = {1990},
 }

@article{pryor83,
 author               = {{Pryor TA, Gardner RM, Clayton RD, Warner HR}},
 journal              = {J Med Sys},
 pages                = {87--101},
 title                = {{The HELP system}},
 volume               = {7},
 year                 = {1983},
 }

 @article{Long22,
 author = {{Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe}},
journal = {arXiv preprint arXiv:2203.02155},
 title = {{Training language models to follow instructions with human feedback}},
 year = {2022},
 }


 @article{Romal22,
 author = {{Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le}},
journal = {arXiv preprint arXiv:2201.08239},
 title = {{Lamda: Language models for dialog
applications}},
 year = {2022},
 }
 
@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

 @article{Tang23,
 author = {{Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu}},
journal = {arXiv preprint arXiv:2303.04360},
 title = {{Does Synthetic Data Generation of LLMs Help Clinical Text Mining?}},
 year = {2023},
 }

 @article{Paul23,
 author = {{Metty Paul, Leandros Maglaras, Mohamed Amine Ferrag, Iman AlMomani}},
journal = {ICT Express},
 title = {{Digitization of healthcare sector: A study on privacy and security concerns}},
 year = {2023},
 }

 @article{Roberto23,
 author = {{Roberto Cerchione, Piera Centobelli, Emanuela Riccio, Stefano Abbate, Eugenio Oropallo}},
journal = {Technovation},
 title = {{Blockchain’s coming to hospital to digitalize healthcare services: Designing a distributed electronic health record ecosystem}},
 year = {2023},
 }

@article{Urbain22,
 author = {{Jay Urbain, George Kowalski, Kristen Osinski, Robert Spaniol, Mei Liu, Bradley Taylor, Lemuel R. Waitman}},
journal = {AMIA Annual Symposium Proceedings},
 pages = {92},
 title = {{Natural Language Processing for Enterprise-scale De-identification of Protected Health Information in Clinical Notes}},
 volume = {2022},
 year = {2022},
 }

 @inproceedings{urbain2022natural,
  title={Natural Language Processing for Enterprise-scale De-identification of Protected Health Information in Clinical Notes},
  author={Urbain, Jay and Kowalski, George and Osinski, Kristen and Spaniol, Robert and Liu, Mei and Taylor, Bradley and Waitman, Lemuel R and others},
  booktitle={AMIA Annual Symposium Proceedings},
  volume={2022},
  pages={92},
  year={2022},
  organization={American Medical Informatics Association}
}

@article{Nandita21,
 author = {{Nandita Sharma, Ashima Anand, Amit Kumar Singh}},
journal = {Computing},
 pages = {1-35},
 title = {{Bio-signal data sharing security through watermarking: a technical survey}},
 year = {2021},
 }

@article{Asokan19,
 author = {{Asokan Sivaprakash, Samuel NE Rajan, and Sundaramoorthy Selvaperumal}},
journal = {Current Medical Imaging},
 pages = {802-809},
 title = {{Privacy protection of patient medical images using digital watermarking technique for E-healthcare system}},
 volume = {8},
 year = {2019},
 }

 @article{Katharina22,
 author = {{Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, Michael Ingrisch}},
journal = {arXiv preprint arXiv:2212.14882},
 title = {{ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports}},
 year = {2022},
 }
 @article{jeblick2022chatgpt,
  title={ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports},
  author={Jeblick, Katharina and Schachtner, Balthasar and Dexl, Jakob and Mittermeier, Andreas and St{\"u}ber, Anna Theresa and Topalis, Johanna and Weber, Tobias and Wesp, Philipp and Sabel, Bastian and Ricke, Jens and others},
  journal={arXiv preprint arXiv:2212.14882},
  year={2022}
}

 @article{Som23,
 author = {{Som Biswas}},
 journal = {Radiology},
 pages = {223312},
 title = {{ChatGPT and the future of medical writing}},
 year = {2023},
 }

@article{Ronald22,
 author = {{Ronald McDowell, Sarah Perrott, Peter Murchie, Christopher Cardwell, Carmel Hughes, and Leslie Samuel}},
 journal = {British Journal of Cancer},
 pages = {957-967},
 title = {{Oral antibiotic use and early-onset colorectal cancer: findings from a case-control study using a national clinical database}},
 volume = {6},
 year = {2022},
 }

 @article{mcdowell2022oral,
  title={Oral antibiotic use and early-onset colorectal cancer: findings from a case-control study using a national clinical database},
  author={McDowell, Ronald and Perrott, Sarah and Murchie, Peter and Cardwell, Christopher and Hughes, Carmel and Samuel, Leslie},
  journal={British Journal of Cancer},
  volume={126},
  number={6},
  pages={957--967},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

 @article{Takeshi22,
 author = {{Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa}},
journal = {arXiv preprint arXiv:2205.11916},
 title = {{Large language models are zero-shot reasoners}},
 year = {2022},
 }


 @article{Monica22,
 author = {{Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, David Sontag}},
journal = {arXiv preprint arXiv:2205.12689},
 title = {{Large language models are zero-shot clinical information extractors}},
 year = {2022},
 }

 @article{Hongjin22,
 author = {{Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu}},
journal = {arXiv preprint arXiv:2209.01975},
 title = {{Selective annotation makes language models better few-shot learners}},
 year = {2022},
 }

@article{Carmel22,
 author = {{Carmel Shachar}},
 journal = {JAMA},
 pages = {417-418},
 title = {{HIPAA, privacy, and reproductive rights in a post-Roe era}},
 volume = {5},
 year = {2022},
 }

@article{Thomas22,
 author = {{Thomas Vakili, Anastasios Lamproudis, Aron Henriksson, and Hercules Dalianis}},
 journal = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
 pages = {4245-4252},
 title = {{Downstream task performance of bert models pre-trained using automatically de-identified clinical data}},
 year = {2022},
 }


 @article{Iyadh22,
 author = {{Iyadh Ben Cheikh Larbi, Aljoscha Burchardt, and Roland Roller}},
journal = {arXiv preprint arXiv:2209.00262},
 title = {{Which anonymization technique is best for which NLP task?--It depends. A Systematic Study on Clinical Text Processing}},
 year = {2022},
 }


@article{Hercules19,
 author = {{Hercules Dalianis}},
 journal = {Proceedings of the Workshop on NLP and Pseudonymisation},
 pages = {16-23},
 title = {{Pseudonymisation of Swedish electronic patient records using a rule-based approach}},
 volume = {166},
 year = {2019},
 }


@article{Hanna20,
 author = {{Hanna Berg, Aron Henriksson, Hercules Dalianis}},
 journal = {Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis},
 pages = {1-11},
 title = {{The Impact of De-identification on Downstream Named Entity Recognition in Clinical Text}},
 year = {2012},
 }

@article{Xi19,
 author = {{Xi Yang, Tianchen Lyu, Qian Li, Chih-Yin Lee, Jiang Bian, William R. Hogan, Yonghui Wu}},
 journal = {BMC medical informatics and decision making},
 pages = {1-9},
 title = {{A study of deep learning methods for de-identification of clinical notes in cross-institute settings}},
 volume = {5},
 year = {2019},
 }

@article{Maryam21,
 author = {{Maryam Tayefi, Phuong Ngo, Taridzo Chomutare, Hercules Dalianis, Elisa Salvi, Andrius Budrionis, Fred Godtliebsen}},
 journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
 title = {{Challenges and opportunities beyond structured data in analysis of electronic health records}},
 volume = {6},
 year = {2021},
 }

 @article{tayefi2021challenges,
  title={Challenges and opportunities beyond structured data in analysis of electronic health records},
  author={Tayefi, Maryam and Ngo, Phuong and Chomutare, Taridzo and Dalianis, Hercules and Salvi, Elisa and Budrionis, Andrius and Godtliebsen, Fred},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  volume={13},
  number={6},
  pages={e1549},
  year={2021},
  publisher={Wiley Online Library}
}

@article{Joffrey20,
 author = {{Joffrey L. Leevy, Taghi M. Khoshgoftaar, Flavio Villanustre}},
 journal = {Journal of Big Data},
 pages = {1-22},
 title = {{Survey on RNN and CRF models for de-identification of medical free text}},
 volume = {7},
 year = {2020},
 }

@article{Junhak22,
 author = {{Junhak Lee, Jinwoo Jeong, Sungji Jung, Jihoon Moon, Seungmin Rho}},
 journal = {Journal of Personalized Medicine},
 pages = {190},
 title = {{Verification of De-Identification Techniques for Personal Information Using Tree-Based Methods with Shapley Values}},
 volume = {2},
 year = {2022},
 }

@article{Tanbir20,
 author = {{Tanbir Ahmed, Md Momin Al Aziz, Noman Mohammed}},
 journal = {Scientific reports},
 pages = {1-11},
 title = {{De-identification of electronic health record using neural network}},
 volume = {1},
 year = {2020},
 }

@article{Cliffford05,
 author = {{M. M., G. D. Cliffford Douglass, Andrew Reisner, W. J. Long, G. B. Moody, R. G. Mark}},
 journal = {Computers in Cardiology},
 pages = {331-334},
 title = {{De-identification algorithm for free-text nursing notes}},
 year = {2005},
 }

@article{Ishna08,
 author = {{Ishna Neamatullah, Margaret M. Douglass, Li-Wei H. Lehman, Andrew Reisner, Mauricio Villarroel, William J. Long, Peter Szolovits, George B. Moody, Roger G. Mark, Gari D. Clifford}},
 journal = {BMC medical informatics and decision making},
 pages = {1-17},
 title = {{Automated de-identification of free-text medical records}},
 volume = {1},
 year = {2008},
 }

 @article{Chengwei23,
 author = {{Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang}},
journal = {arXiv preprint arXiv:2302.06476},
 title = {{Is ChatGPT a General-Purpose Natural Language Processing Task Solver?}},
 year = {2023},
 }

 @article{Fares23,
 author = {{Fares Antaki, Samir Touma, Daniel Milad, Jonathan El-Khoury, Renaud Duval}},
journal = {medRxiv (2023): 2023-01},
 title = {{Evaluating the performance of chatgpt in ophthalmology: An analysis of its successes and shortcomings}},
 year = {2023},
 }

 @article{Xiang23,
 author = {{Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, Wenjuan Han}},
journal = {arXiv preprint arXiv:2302.10205},
 title = {{Zero-Shot Information Extraction via Chatting with ChatGPT}},
 year = {2023},
 }

 @article{Maad23,
 author = {{Maad Mijwil, Mohammad Aljanabi, Ahmed Hussein Ali}},
 journal = {Mesopotamian journal of cybersecurity},
 pages = {18-21},
 title = {{ChatGPT: Exploring the Role of Cybersecurity in the Protection of Medical Information}},
 year = {2023},
 }

@article{stubbs2015annotating,
  title={Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/UTHealth corpus},
  author={Stubbs, Amber and Uzuner, {\"O}zlem},
  journal={Journal of biomedical informatics},
  volume={58},
  pages={S20--S29},
  year={2015},
  publisher={Elsevier}
}

@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  number={3},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@article{graves2012long,
  title={Long short-term memory},
  author={Graves, Alex and Graves, Alex},
  journal={Supervised sequence labelling with recurrent neural networks},
  pages={37--45},
  year={2012},
  publisher={Springer}
}

@inproceedings{dey2017gate,
  title={Gate-variants of gated recurrent unit (GRU) neural networks},
  author={Dey, Rahul and Salem, Fathi M},
  booktitle={2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)},
  pages={1597--1600},
  year={2017},
  organization={IEEE}
}

@article{khandelwal2018sharp,
  title={Sharp nearby, fuzzy far away: How neural language models use context},
  author={Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1805.04623},
  year={2018}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, N and Nanda, N and Olsson, C and Henighan, T and Joseph, N and Mann, B and Askell, A and Bai, Y and Chen, A and Conerly, T and others},
  journal={Transformer Circuits Thread},
  year={2021}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={1872--1897},
  year={2020},
  publisher={Springer}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Trebacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{Hui15Automatric,
  title={Automatic detection of protected health information from clinic narratives},
  author={Hui Yang, Jonathan M. Garibaldi},
  journal={Journal of biomedical informatics},
  pages={S30-S38},
  year={2015},
}

@inproceedings{syed2022deidner,
  title={DeIDNER Model: A Neural Network Named Entity Recognition Model for Use in the De-identification of Clinical Notes},
  author={Syed, Mahanazuddin and Sexton, Kevin and Greer, Melody and Syed, Shorabuddin and VanScoy, Joseph and Kawsar, Farhan and Olson, Erica and Patel, Karan and Erwin, Jake and Bhattacharyya, Sudeepa and others},
  booktitle={Biomedical engineering systems and technologies, international joint conference, BIOSTEC... revised selected papers. BIOSTEC (Conference)},
  volume={5},
  pages={640},
  year={2022},
  organization={NIH Public Access}
}

@article{Rosario20,
  title={Crosslingual named entity recognition for clinical de-identification applied to a COVID-19 Italian data set},
  author={Rosario Catelli, Francesco Gargiulo, Valentina Casola, Giuseppe De Pietro, Hamido Fujita, Massimo Esposito},
  journal={Applied soft computing},
  volume={97},
  pages={106779},
  year={2020},
}

@article{catelli2020crosslingual,
  title={Crosslingual named entity recognition for clinical de-identification applied to a COVID-19 Italian data set},
  author={Catelli, Rosario and Gargiulo, Francesco and Casola, Valentina and De Pietro, Giuseppe and Fujita, Hamido and Esposito, Massimo},
  journal={Applied soft computing},
  volume={97},
  pages={106779},
  year={2020},
  publisher={Elsevier}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhou2023comprehensive,
  title={A comprehensive survey on pretrained foundation models: A history from bert to chatgpt},
  author={Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}

@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@article{qin2023chatgpt,
  title={Is chatgpt a general-purpose natural language processing task solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}

@article{alsentzer2019publicly,
  title={Publicly available clinical BERT embeddings},
  author={Alsentzer, Emily and Murphy, John R and Boag, Willie and Weng, Wei-Hung and Jin, Di and Naumann, Tristan and McDermott, Matthew},
  journal={arXiv preprint arXiv:1904.03323},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}

@inproceedings{jiang2022promptmaker,
  title={PromptMaker: Prompt-based Prototyping with Large Language Models},
  author={Jiang, Ellen and Olson, Kristen and Toh, Edwin and Molina, Alejandra and Donsbach, Aaron and Terry, Michael and Cai, Carrie J},
  booktitle={CHI Conference on Human Factors in Computing Systems Extended Abstracts},
  pages={1--8},
  year={2022}
}

@article{gu2021domain,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

@article{rogers2021primer,
  title={A primer in BERTology: What we know about how BERT works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{carleton2022architecting,
  title={Architecting the Future of Software Engineering},
  author={Carleton, Anita and Shull, Forrest and Harper, Erin},
  journal={Computer},
  volume={55},
  number={9},
  pages={89--93},
  year={2022},
  publisher={IEEE}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{white2023prompt,
  title={A prompt pattern catalog to enhance prompt engineering with chatgpt},
  author={White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C},
  journal={arXiv preprint arXiv:2302.11382},
  year={2023}
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@article{schick2020s,
  title={It's not just size that matters: Small language models are also few-shot learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}

@article{shin2020autoprompt,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.15980},
  year={2020}
}

@article{dai2022can,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{johnson2016mimic,
  title={MIMIC-III, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Lehman, Li-wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G},
  journal={Scientific data},
  volume={3},
  number={1},
  pages={1--9},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{pandey2022comprehensive,
  title={A comprehensive survey of deep learning in the field of medical imaging and medical natural language processing: Challenges and research directions},
  author={Pandey, Babita and Pandey, Devendra Kumar and Mishra, Brijendra Pratap and Rhmann, Wasiur},
  journal={Journal of King Saud University-Computer and Information Sciences},
  volume={34},
  number={8},
  pages={5083--5099},
  year={2022},
  publisher={Elsevier}
}


@article{Smith2022,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{Chowdhery2022PaLM,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, A. and Narang, S. and Devlin, J. and Bosma, M. and Mishra, G. and Roberts, A. and Barham, P. and Chung, H.W. and Sutton, C. and Gehrmann, S. and Schuh, P.},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{Laurencon2023,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Laurençon, H. and Saulnier, L. and Wang, T. and Akiki, C. and del Moral A.V. and Scao, T.L. and Von Werra, L. and Mou, C. and Ponferrada, E.G. and Nguyen, H. and Frohberg, J.},
  journal={arXiv preprint arXiv:2303.03915},
  year={2023}
}

@article{Taylor2022,
  title={Galactica: A large language model for science},
  author={Taylor, R. and Kardas, M. and Cucurull, G. and Scialom, T. and Hartshorn, A. and Saravia, E. and Poulton, A. and Kerkez, V. and Stojnic, R.},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}

@article{Lieber2021,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav },
  journal={White Paper. AI21 Labs 1},
  year={2021}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{luo2022biogpt,
  title={BioGPT: generative pre-trained transformer for biomedical text generation and mining},
  author={Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={6},
  year={2022},
  publisher={Oxford Academic}
}


@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{zhang2020accelerating,
  title={Accelerating training of transformer-based language models with progressive layer dropping},
  author={Zhang, Minjia and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14011--14023},
  year={2020}
}

@article{zeng2022acctfm,
  title={AccTFM: An Effective Intra-Layer Model Parallelization Strategy for Training Large-Scale Transformer-Based Models},
  author={Zeng, Zihao and Liu, Chubo and Tang, Zhuo and Li, Kenli and Li, Keqin},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={12},
  pages={4326--4338},
  year={2022},
  publisher={IEEE}
}

@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={J. Mach. Learn. Res},
  volume={23},
  pages={1--40},
  year={2021}
}



@article{dai2023chataug,
  title={ChatAug: Leveraging ChatGPT for Text Data Augmentation},
  author={Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and Huang, Xiaoke and Wu, Zihao and Zhao, Lin and Liu, Wei and Liu, Ninghao and Li, Sheng and Zhu, Dajiang and others},
  journal={arXiv preprint arXiv:2302.13007},
  year={2023}
}

@article{wang2023chatcad,
  title={ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models},
  author={Wang, Sheng and Zhao, Zihao and Ouyang, Xi and Wang, Qian and Shen, Dinggang},
  journal={arXiv preprint arXiv:2302.07257},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{liu2022survey,
  title={Survey on natural language processing in medical image analysis.},
  author={Liu, Zhengliang and He, Mengshen and Jiang, Zuowei and Wu, Zihao and Dai, Haixing and Zhang, Lian and Luo, Siyi and Han, Tianle and Li, Xiang and Jiang, Xi and others},
  journal={Zhong nan da xue xue bao. Yi xue ban= Journal of Central South University. Medical Sciences},
  volume={47},
  number={8},
  pages={981--993},
  year={2022}
}

@inproceedings{rezayi2022clinicalradiobert,
  title={ClinicalRadioBERT: Knowledge-Infused Few Shot Learning for Clinical Notes Named Entity Recognition},
  author={Rezayi, Saed and Dai, Haixing and Liu, Zhengliang and Wu, Zihao and Hebbar, Akarsh and Burns, Andrew H and Zhao, Lin and Zhu, Dajiang and Li, Quanzheng and Liu, Wei and others},
  booktitle={Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings},
  pages={269--278},
  year={2022},
  organization={Springer}
}

@article{cai2021chestxraybert,
  title={Chestxraybert: A pretrained language model for chest radiology report summarization},
  author={Cai, Xiaoyan and Liu, Sen and Han, Junwei and Yang, Libin and Liu, Zhenguo and Liu, Tianming},
  journal={IEEE Transactions on Multimedia},
  year={2021},
  publisher={IEEE}
}

@article{cai2022covidsum,
  title={COVIDSum: A linguistically enriched SciBERT-based summarization model for COVID-19 scientific papers},
  author={Cai, Xiaoyan and Liu, Sen and Yang, Libin and Lu, Yan and Zhao, Jintao and Shen, Dinggang and Liu, Tianming},
  journal={Journal of Biomedical Informatics},
  volume={127},
  pages={103999},
  year={2022},
  publisher={Elsevier}
}

@article{ji2021mentalbert,
  title={Mentalbert: Publicly available pretrained language models for mental healthcare},
  author={Ji, Shaoxiong and Zhang, Tianlin and Ansari, Luna and Fu, Jie and Tiwari, Prayag and Cambria, Erik},
  journal={arXiv preprint arXiv:2110.15621},
  year={2021}
}

@inproceedings{agrawal2022large,
  title={Large language models are few-shot clinical information extractors},
  author={Agrawal, Monica and Hegselmann, Stefan and Lang, Hunter and Kim, Yoon and Sontag, David},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1998--2022},
  year={2022}
}

@article{wu2022survey,
  title={A survey on clinical natural language processing in the United Kingdom from 2007 to 2022},
  author={Wu, Honghan and Wang, Minhong and Wu, Jinge and Francis, Farah and Chang, Yun-Hsuan and Shavick, Alex and Dong, Hang and Poon, Michael TC and Fitzpatrick, Natalie and Levine, Adam P and others},
  journal={NPJ digital medicine},
  volume={5},
  number={1},
  pages={186},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{abdullah2023chatgpt,
  title={ChatGPT \& Doctors: The Medical Dream Team},
  author={Abdullah, Ishan S and Loganathan, Aditya and Lee, Randall W},
  year={2023}
}

@article{haleem2023era,
  title={An era of ChatGPT as a significant futuristic support tool: A study on features, abilities, and challenges},
  author={Haleem, Abid and Javaid, Mohd and Singh, Ravi Pratap},
  journal={BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
  pages={100089},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{rezayi2022agribert,
  title={Agribert: knowledge-infused agricultural language models for matching food and nutrition},
  author={Rezayi, Saed and Liu, Zhengliang and Wu, Zihao and Dhakal, Chandra and Ge, Bao and Zhen, Chen and Liu, Tianming and Li, Sheng},
  year={2022},
  organization={IJCAI}
}

@article{liao2023mask,
  title={Mask-guided BERT for Few Shot Text Classification},
  author={Liao, Wenxiong and Liu, Zhengliang and Dai, Haixing and Wu, Zihao and Zhang, Yiyang and Huang, Xiaoke and Chen, Yuzhong and Jiang, Xi and Zhu, Dajiang and Liu, Tianming and others},
  journal={arXiv preprint arXiv:2302.10447},
  year={2023}
}



