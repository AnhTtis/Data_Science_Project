
@inproceedings{al-asqharRelationshipDescriptorsInteractive2013,
  title = {Relationship Descriptors for Interactive Motion Adaptation},
  booktitle = {Proceedings of the 12th {{ACM SIGGRAPH}}/{{Eurographics Symposium}} on {{Computer Animation}}},
  author = {{Al-Asqhar}, Rami Ali and Komura, Taku and Choi, Myung Geol},
  year = {2013},
  month = jul,
  series = {{{SCA}} '13},
  pages = {45--53},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2485895.2485905},
  abstract = {This paper presents an interactive motion adaptation scheme for close interactions between skeletal characters and mesh structures, such as moving through restricted environments, and manipulating objects. This is achieved through a new spatial relationship-based representation, which describes the kinematics of the body parts by the weighted sum of translation vectors relative to points selectively sampled over the surfaces of the mesh structures. In contrast to previous discrete representations that either only handle static spatial relationships, or require offline, costly optimization processes, our continuous framework smoothly adapts the motion of a character to large updates of the mesh structures and character morphologies on-the-fly, while preserving the original context of the scene. The experimental results show that our method can be used for a wide range of applications, including motion retargeting, interactive character control and deformation transfer for scenes that involve close interactions. Our framework is useful for artists who need to design animated scenes interactively, and modern computer games that allow users to design their own characters, objects and environments.},
  isbn = {978-1-4503-2132-7},
  keywords = {motion adaptation,motion editing,motion retargeting},
  file = {/Users/james/Zotero/storage/WRQAIHJ3/Al-Asqhar et al. - 2013 - Relationship descriptors for interactive motion ad.pdf}
}

@article{anguelovSCAPEShapeCompletion2005,
  title = {{{SCAPE}}: Shape Completion and Animation of People},
  shorttitle = {{{SCAPE}}},
  author = {Anguelov, Dragomir and Srinivasan, Praveen and Koller, Daphne and Thrun, Sebastian and Rodgers, Jim and Davis, James},
  year = {2005},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {24},
  number = {3},
  pages = {408--416},
  issn = {0730-0301},
  doi = {10.1145/1073204.1073207},
  abstract = {We introduce the SCAPE method (Shape Completion and Animation for PEople)---a data-driven method for building a human shape model that spans variation in both subject shape and pose. The method is based on a representation that incorporates both articulated and non-rigid deformations. We learn a pose deformation model that derives the non-rigid surface deformation as a function of the pose of the articulated skeleton. We also learn a separate model of variation based on body shape. Our two models can be combined to produce 3D surface models with realistic muscle deformation for different people in different poses, when neither appear in the training set. We show how the model can be used for shape completion --- generating a complete surface mesh given a limited set of markers specifying the target shape. We present applications of shape completion to partial view completion and motion capture animation. In particular, our method is capable of constructing a high-quality animated surface model of a moving person, with realistic muscle deformation, using just a single static scan and a marker motion capture sequence of the person.},
  keywords = {animation,deformations,morphing,synthetic actors}
}

@article{ashDEEPBATCHACTIVE2020,
  title = {{{DEEP BATCH ACTIVE LEARNING BY DIVERSE}}, {{UNCERTAIN GRADIENT LOWER BOUNDS}}},
  author = {Ash, Jordan T and Zhang, Chicheng and Krishnamurthy, Akshay},
  year = {2020},
  journal = {ICLR},
  pages = {26},
  abstract = {We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.},
  langid = {english},
  file = {/Users/james/Zotero/storage/IAWHVSZ5/Ash et al. - 2020 - DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN G.pdf}
}

@article{bailenson2005independent,
  title={The independent and interactive effects of embodied-agent appearance and behavior on self-report, cognitive, and behavioral markers of copresence in immersive virtual environments},
  author={Bailenson, Jeremy N and Swinth, Kim and Hoyt, Crystal and Persky, Susan and Dimov, Alex and Blascovich, Jim},
  journal={Presence},
  volume={14},
  number={4},
  pages={379--393},
  year={2005},
  publisher={MIT Press}
}
@inproceedings{narang2018simulating,
  title={Simulating movement interactions between avatars \& agents in virtual worlds using human motion constraints},
  author={Narang, Sahil and Best, Andrew and Manocha, Dinesh},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
  pages={9--16},
  year={2018},
  organization={IEEE}
}

@inproceedings{shen2021simulating,
  title={Simulating Realistic Human Motion Trajectories of Mid-Air Gesture Typing},
  author={Shen, Junxiao and Dudley, John and Kristensson, Per Ola},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  pages={393--402},
  year={2021},
  organization={IEEE}
}
@inproceedings{bhattacharyaGeneratingEmotiveGaits2020,
  title = {Generating {{Emotive Gaits}} for {{Virtual Agents Using Affect-Based Autoregression}}},
  booktitle = {2020 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Bhattacharya, Uttaran and Rewkowski, Nicholas and Guhan, Pooja and Williams, Niall L. and Mittal, Trisha and Bera, Aniket and Manocha, Dinesh},
  year = {2020},
  month = nov,
  pages = {24--35},
  issn = {1554-7868},
  doi = {10.1109/ISMAR50242.2020.00020},
  abstract = {We present a novel autoregression network to generate virtual agents that convey various emotions through their walking styles or gaits. Given the 3D pose sequences of a gait, our network extracts pertinent movement features and affective features from the gait. We use these features to synthesize subsequent gaits such that the virtual agents can express and transition between emotions represented as combinations of happy, sad, angry, and neutral. We incorporate multiple regularizations in the training of our network to simultaneously enforce plausible movements and noticeable emotions on the virtual agents. We also integrate our approach with an AR environment using a Microsoft HoloLens and can generate emotive gaits at interactive rates to increase the social presence. We evaluate how human observers perceive both the naturalness and the emotions from the generated gaits of the virtual agents in a web-based study. Our results indicate around 89\% of the users found the naturalness of the gaits satisfactory on a five-point Likert scale, and the emotions they perceived from the virtual agents are statistically similar to the intended emotions of the virtual agents. We also use our network to augment existing gait datasets with emotive gaits and will release this augmented dataset for future research in emotion prediction and emotive gait synthesis. Our project website is available at https://gamma.umd.edu/gen-emotive-gaits/.},
  keywords = {Augmented reality,Computing methodologies,Feature extraction,Human computer interaction (HCI),Human-centered computing,Interaction paradigms-Mixed / augmented reality,Legged locomotion,Machine learning,Machine learning approaches,Neural networks,Observers,Three-dimensional displays,Training},
  file = {/Users/james/Zotero/storage/RXTYS97K/Bhattacharya et al. - 2020 - Generating Emotive Gaits for Virtual Agents Using .pdf;/Users/james/Zotero/storage/NU5WQXDQ/9284667.html}
}

@inproceedings{bhattacharyaText2GesturesTransformerBasedNetwork2021,
  title = {{{Text2Gestures}}: {{A Transformer-Based Network}} for {{Generating Emotive Body Gestures}} for {{Virtual Agents}}},
  shorttitle = {{{Text2Gestures}}},
  booktitle = {2021 {{IEEE Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {Bhattacharya, Uttaran and Rewkowski, Nicholas and Banerjee, Abhishek and Guhan, Pooja and Bera, Aniket and Manocha, Dinesh},
  year = {2021},
  month = mar,
  pages = {1--10},
  issn = {2642-5254},
  doi = {10.1109/VR50410.2021.00037},
  abstract = {We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91\% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.},
  keywords = {Computer systems organization-Neural networks,Computing methodologies-Intelligent agents,Computing methodologies-Virtual reality,Correlation,Databases,Graphics processing units,Learning systems,Natural languages,Pipelines,Three-dimensional displays},
  file = {/Users/james/Zotero/storage/4JMGRPRH/Bhattacharya et al. - 2021 - Text2Gestures A Transformer-Based Network for Gen.pdf;/Users/james/Zotero/storage/2G9KVZGH/9417647.html}
}

@inproceedings{Chan2021,
  title = {Efficient Geometry-Aware {{3D}} Generative Adversarial Networks},
  booktitle = {{{arXiv}}},
  author = {Chan, Eric R. and Lin, Connor Z. and Chan, Matthew A. and Nagano, Koki and Pan, Boxiao and Mello, Shalini De and Gallo, Orazio and Guibas, Leonidas and Tremblay, Jonathan and Khamis, Sameh and Karras, Tero and Wetzstein, Gordon},
  year = {2021}
}

@article{cleggLearningDressSynthesizing2018,
  title = {Learning to Dress: Synthesizing Human Dressing Motion via Deep Reinforcement Learning},
  shorttitle = {Learning to Dress},
  author = {Clegg, Alexander and Yu, Wenhao and Tan, Jie and Liu, C. Karen and Turk, Greg},
  year = {2018},
  month = dec,
  journal = {ACM Transactions on Graphics},
  volume = {37},
  number = {6},
  pages = {179:1--179:10},
  issn = {0730-0301},
  doi = {10.1145/3272127.3275048},
  abstract = {Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. We take a model-free deep reinforcement learning (deepRL) approach to automatically discovering robust dressing control policies represented by neural networks. While deepRL has demonstrated several successes in learning complex motor skills, the data-demanding nature of the learning algorithms is at odds with the computationally costly cloth simulation required by the dressing task. This paper is the first to demonstrate that, with an appropriately designed input state space and a reward function, it is possible to incorporate cloth simulation in the deepRL framework to learn a robust dressing control policy. We introduce a salient representation of haptic information to guide the dressing process and utilize it in the reward function to provide learning signals during training. In order to learn a prolonged sequence of motion involving a diverse set of manipulation skills, such as grasping the edge of the shirt or pulling on a sleeve, we find it necessary to separate the dressing task into several subtasks and learn a control policy for each subtask. We introduce a policy sequencing algorithm that matches the distribution of output states from one task to the input distribution for the next task in the sequence. We have used this approach to produce character controllers for several dressing tasks: putting on a t-shirt, putting on a jacket, and robot-assisted dressing of a sleeve.},
  keywords = {dressing,policy sequencing,reinforcement learning},
  file = {/Users/james/Zotero/storage/EV4ZRJEN/Clegg et al. - 2018 - Learning to dress synthesizing human dressing mot.pdf}
}

@inproceedings{damianMotionCapturingEmpowered2013,
  title = {Motion Capturing Empowered Interaction with a Virtual Agent in an {{Augmented Reality}} Environment},
  booktitle = {2013 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Damian, Ionut and B{\"u}hling, Ren{\'e} and Obaid, Mohammad and Buhling, Rene and Billinghurst, Mark and Andr{\'e}, Elisabeth},
  year = {2013},
  month = oct,
  pages = {1--6},
  doi = {10.1109/ISMAR.2013.6671830},
  abstract = {We present an Augmented Reality (AR) system where we immerse the user's whole body in the virtual scene using a motion capturing (MoCap) suit. The goal is to allow for seamless interaction with the virtual content within the AR environment. We describe an evaluation study of a prototype application featuring an interactive scenario with a virtual agent. The scenario contains two conditions: in one, the agent has access to the full tracking data of the MoCap suit and therefore is aware of the exact actions of the user, while in the second condition, the agent does not get this information. We then report and discuss the differences we were able to detect regarding the users' perception of the interaction with the agent and give future research directions.},
  keywords = {Augmented Reality,Full Body Interaction,Motion Capturing,Natural Interaction,Virtual Agent},
  file = {/Users/james/Zotero/storage/UCWZSFLZ/Damian et al. - 2013 - Motion capturing empowered interaction with a virt.pdf;/Users/james/Zotero/storage/QBJZLWY8/6671830.html}
}

@inproceedings{gleicherRetargettingMotionNew1998,
  title = {Retargetting Motion to New Characters},
  booktitle = {Proceedings of the 25th Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '98},
  author = {Gleicher, Michael},
  year = {1998},
  pages = {33--42},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  doi = {10.1145/280814.280820},
  abstract = {In this paper, we present a technique for retargetting motion: the problem of adapting an animated motion from one character to another. Our focus is on adapting the motion of one articulated figure to another figure with identical structure but different segment lengths, although we use this as a step when considering less similar characters. Our method creates adaptations that preserve desirable qualities of the original motion. We identify specific features of the motion as constraints that must be maintained. A spacetime constraints solver computes an adapted motion that re-establishes these constraints while preserving the frequency characteristics of the original signal. We demonstrate our approach on motion capture data.},
  isbn = {978-0-89791-999-9},
  langid = {english},
  file = {/Users/james/Zotero/storage/ES4G377F/Gleicher - 1998 - Retargetting motion to new characters.pdf}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
  file = {/Users/james/Zotero/storage/TJBS98YX/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf}
}

@inproceedings{grabnerWhatMakesChair2011,
  title = {What Makes a Chair a Chair?},
  booktitle = {{{CVPR}} 2011},
  author = {Grabner, Helmut and Gall, Juergen and Van Gool, Luc},
  year = {2011},
  month = jun,
  pages = {1529--1536},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2011.5995327},
  abstract = {Many object classes are primarily defined by their functions. However, this fact has been left largely unexploited by visual object categorization or detection systems. We propose a method to learn an affordance detector. It identifies locations in the 3d space which ``support'' the particular function. Our novel approach ``imagines'' an actor performing an action typical for the target object class, instead of relying purely on the visual object appearance. So, function is handled as a cue complementary to appearance, rather than being a consideration after appearance-based detection. Experimental results are given for the functional category ``sitting''. Such affordance is tested on a 3d representation of the scene, as can be realistically obtained through SfM or depth cameras. In contrast to appearance-based object detectors, affordance detection requires only very few training examples and generalizes very well to other sittable objects like benches or sofas when trained on a few chairs.},
  keywords = {Detectors,Humans,Object detection,Shape,Solid modeling,Three dimensional displays,Training},
  file = {/Users/james/Zotero/storage/GUZSQ8JB/5995327.html}
}

@inproceedings{guimaraesImpactVirtualReality2020,
  title = {The {{Impact}} of {{Virtual Reality}} in the {{Social Presence}} of a {{Virtual Agent}}},
  booktitle = {Proceedings of the 20th {{ACM International Conference}} on {{Intelligent Virtual Agents}}},
  author = {Guimar{\~a}es, Manuel and Prada, Rui and Santos, Pedro A. and Dias, Jo{\~a}o and Jhala, Arnav and Mascarenhas, Samuel},
  year = {2020},
  month = oct,
  series = {{{IVA}} '20},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3383652.3423879},
  abstract = {In this work we test the hypothesis that interacting with an intelligent virtual character in Virtual Reality (VR) has a stronger impact compared to the same interaction in a traditional non-immersive platform, both in terms of presence and believability. We designed a Social Skills Training scenario of a police interview, based on interactions observed in real cases with the help of teachers and experts from that field. To test our hypothesis, we conducted experiments with two treatments: one in VR and the other displayed on a conventional computer screen. We collected qualitative and quantitative data using instruments with elements from well-established presence and situated interaction questionnaires. Results indicate that participant perception of social presence of virtual characters is higher in VR. No significant difference in believability was observed across treatments The experimental design encourages further work on measurement of effects of social presence and its impact on design of intelligent interactions in the context of Social Skills Training environments and immersive platforms.},
  isbn = {978-1-4503-7586-3},
  keywords = {intelligent virtual agents,social presence,social skills training,virtual reality}
}

@inproceedings{gupta3DSceneGeometry2011,
  title = {From {{3D}} Scene Geometry to Human Workspace},
  booktitle = {{{CVPR}} 2011},
  author = {Gupta, Abhinav and Satkin, Scott and Efros, Alexei A. and Hebert, Martial},
  year = {2011},
  month = jun,
  pages = {1961--1968},
  publisher = {{IEEE}},
  address = {{Colorado Springs, CO, USA}},
  doi = {10.1109/CVPR.2011.5995448},
  abstract = {We present a human-centric paradigm for scene understanding. Our approach goes beyond estimating 3D scene geometry and predicts the ``workspace'' of a human which is represented by a data-driven vocabulary of human interactions. Our method builds upon the recent work in indoor scene understanding and the availability of motion capture data to create a joint space of human poses and scene geometry by modeling the physical interactions between the two. This joint space can then be used to predict potential human poses and joint locations from a single image. In a way, this work revisits the principle of Gibsonian affordances, reinterpreting it for the modern, data-driven era.},
  isbn = {978-1-4577-0394-2},
  langid = {english},
  file = {/Users/james/Zotero/storage/6K7997PF/Gupta et al. - 2011 - From 3D scene geometry to human workspace.pdf}
}

@article{guSTYLENERFSTYLEBASED3DAWARE2022,
  title = {{{STYLENERF}}: {{A STYLE-BASED 3D-AWARE GENERA- TOR FOR HIGH-RESOLUTION IMAGE SYNTHESIS}}},
  author = {Gu, Jiatao and Liu, Lingjie and Wang, Peng and Theobalt, Christian},
  year = {2022},
  journal = {ICLR},
  pages = {25},
  abstract = {We propose StyleNeRF, a 3D-aware generative model for photo-realistic highresolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize highresolution images with fine details or yield noticeable 3D-inconsistent artifacts. In addition, many of them lack control over style attributes and explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. We perform volume rendering only to produce a low-resolution feature map and progressively apply upsampling in 2D to address the first issue. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs, including a better upsampler and a new regularization loss. With these designs, StyleNeRF can synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks, including style mixing and semantic editing. Code and pre-trained models are available at: https://github.com/facebookresearch/StyleNeRF.},
  langid = {english},
  file = {/Users/james/Zotero/storage/PD7GBPZW/Gu et al. - 2022 - STYLENERF A STYLE-BASED 3D-AWARE GENERA- TOR FOR .pdf}
}

@article{harveyRobustMotionInbetweening2020,
  title = {Robust Motion In-Betweening},
  author = {Harvey, F{\'e}lix G. and Yurick, Mike and Nowrouzezahrai, Derek and Pal, Christopher},
  year = {2020},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {39},
  number = {4},
  pages = {60:60:1--60:60:12},
  issn = {0730-0301},
  doi = {10.1145/3386569.3392480},
  abstract = {In this work we present a novel, robust transition generation technique that can serve as a new tool for 3D animators, based on adversarial recurrent neural networks. The system synthesises high-quality motions that use temporally-sparse keyframes as animation constraints. This is reminiscent of the job of in-betweening in traditional animation pipelines, in which an animator draws motion frames between provided keyframes. We first show that a state-of-the-art motion prediction model cannot be easily converted into a robust transition generator when only adding conditioning information about future keyframes. To solve this problem, we then propose two novel additive embedding modifiers that are applied at each timestep to latent representations encoded inside the network's architecture. One modifier is a time-to-arrival embedding that allows variations of the transition length with a single model. The other is a scheduled target noise vector that allows the system to be robust to target distortions and to sample different transitions given fixed keyframes. To qualitatively evaluate our method, we present a custom MotionBuilder plugin that uses our trained model to perform in-betweening in production scenarios. To quantitatively evaluate performance on transitions and generalizations to longer time horizons, we present well-defined in-betweening benchmarks on a subset of the widely used Human3.6M dataset and on LaFAN1, a novel high quality motion capture dataset that is more appropriate for transition generation. We are releasing this new dataset along with this work, with accompanying code for reproducing our baseline results.},
  keywords = {animation,deep learning,in-betweening,locomotion,LSTM,transition generation},
  file = {/Users/james/Zotero/storage/PL3K6CY6/Harvey et al. - 2020 - Robust motion in-betweening.pdf}
}

@inproceedings{hassanPopulating3DScenes2021,
  title = {Populating {{3D Scenes}} by {{Learning Human-Scene Interaction}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hassan, Mohamed and Ghosh, Partha and Tesch, Joachim and Tzionas, Dimitrios and Black, Michael J.},
  year = {2021},
  month = jun,
  pages = {14703--14713},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01447},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/james/Zotero/storage/2S3NH5RG/Hassan et al. - 2021 - Populating 3D Scenes by Learning Human-Scene Inter.pdf}
}

@inproceedings{hassanResolving3DHuman2019,
  title = {Resolving {{3D Human Pose Ambiguities With 3D Scene Constraints}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Hassan, Mohamed and Choutas, Vasileios and Tzionas, Dimitrios and Black, Michael},
  year = {2019},
  month = oct,
  pages = {2282--2292},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00237},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/Users/james/Zotero/storage/N83RIAKN/Hassan et al. - 2019 - Resolving 3D Human Pose Ambiguities With 3D Scene .pdf}
}

@article{holdenDeepLearningFramework2016,
  title = {A Deep Learning Framework for Character Motion Synthesis and Editing},
  author = {Holden, Daniel and Saito, Jun and Komura, Taku},
  year = {2016},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  pages = {138:1--138:11},
  issn = {0730-0301},
  doi = {10.1145/2897824.2925975},
  abstract = {We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.},
  keywords = {autoencoder,character animation,convolutional neural networks,deep learning,human motion,manifold learning},
  file = {/Users/james/Zotero/storage/FIBKZX5U/Holden et al. - 2016 - A deep learning framework for character motion syn.pdf}
}

@article{hoSpatialRelationshipPreserving2010,
  title = {Spatial Relationship Preserving Character Motion Adaptation},
  author = {Ho, Edmond S. L. and Komura, Taku and Tai, Chiew-Lan},
  year = {2010},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {29},
  number = {4},
  pages = {33:1--33:8},
  issn = {0730-0301},
  doi = {10.1145/1778765.1778770},
  abstract = {This paper presents a new method for editing and retargeting motions that involve close interactions between body parts of single or multiple articulated characters, such as dancing, wrestling, and sword fighting, or between characters and a restricted environment, such as getting into a car. In such motions, the implicit spatial relationships between body parts/objects are important for capturing the scene semantics. We introduce a simple structure called an interaction mesh to represent such spatial relationships. By minimizing the local deformation of the interaction meshes of animation frames, such relationships are preserved during motion editing while reducing the number of inappropriate interpenetrations. The interaction mesh representation is general and applicable to various kinds of close interactions. It also works well for interactions involving contacts and tangles as well as those without any contacts. The method is computationally efficient, allowing real-time character control. We demonstrate its effectiveness and versatility in synthesizing a wide variety of motions with close interactions.},
  keywords = {character animation,close interaction,motion editing,motion retargeting,spatial relationship}
}

@misc{ImpactVirtualReality,
  title = {The {{Impact}} of {{Virtual Reality}} in the {{Social Presence}} of a {{Virtual Agent}} | {{Proceedings}} of the 20th {{ACM International Conference}} on {{Intelligent Virtual Agents}}},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3383652.3423879?casa\_token=N3RVAnazVxMAAAAA:6yr5tWLibtQZs5K8L4Nxw83d3iuihJoYTP9uHX9CbJtRaPTtLP\_0mquMjA15iMs4VMMqOKggCJ0},
  file = {/Users/james/Zotero/storage/G5BPK88A/3383652.html}
}

@article{ionescuHuman36MLarge2014,
  title = {Human3.{{6M}}: {{Large Scale Datasets}} and {{Predictive Methods}} for {{3D Human Sensing}} in {{Natural Environments}}},
  shorttitle = {Human3.{{6M}}},
  author = {Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {7},
  pages = {1325--1339},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2013.248},
  abstract = {We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state of the art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large scale model can leverage our full training set to obtain a 20\% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.},
  langid = {english},
  file = {/Users/james/Zotero/storage/TY6NHU36/Ionescu et al. - 2014 - Human3.6M Large Scale Datasets and Predictive Met.pdf}
}

@inproceedings{jiangHallucinatedHumansHidden2013,
  title = {Hallucinated {{Humans}} as the {{Hidden Context}} for {{Labeling 3D Scenes}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Jiang, Yun and Koppula, Hema and Saxena, Ashutosh},
  year = {2013},
  pages = {2993--3000},
  file = {/Users/james/Zotero/storage/HCECWYL4/Jiang et al. - 2013 - Hallucinated Humans as the Hidden Context for Labe.pdf;/Users/james/Zotero/storage/4G7626MF/Jiang_Hallucinated_Humans_as_2013_CVPR_paper.html}
}

@inproceedings{jooTotalCapture3D2018,
  title = {Total {{Capture}}: {{A 3D Deformation Model}} for {{Tracking Faces}}, {{Hands}}, and {{Bodies}}},
  shorttitle = {Total {{Capture}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Joo, Hanbyul and Simon, Tomas and Sheikh, Yaser},
  year = {2018},
  month = jun,
  pages = {8320--8329},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00868},
  abstract = {We present a unified deformation model for the markerless capture of human movement at multiple scales, including facial expressions, body motion, and hand gestures. An initial model is generated by locally stitching together models of the individual parts of the human body, which we refer to as ``Frank''. This model enables the full expression of part movements, including face and hands, by a single seamless model. We capture a dataset of people wearing everyday clothes and optimize the Frank model to create ``Adam'': a calibrated model that shares the same skeleton hierarchy as the initial model with a simpler parameterization. Finally, we demonstrate the use of these models for total motion tracking in a multiview setup, simultaneously capturing the large-scale body movements and the subtle face and hand motion of a social group of people.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/james/Zotero/storage/NR8VKCQT/Joo et al. - 2018 - Total Capture A 3D Deformation Model for Tracking.pdf}
}

@article{kangEnvironmentAdaptiveContactPoses2014,
  title = {Environment-{{Adaptive Contact Poses}} for {{Virtual Characters}}},
  author = {Kang, Changgu and Lee, Sung-Hee},
  year = {2014},
  journal = {Computer Graphics Forum},
  volume = {33},
  number = {7},
  pages = {1--10},
  issn = {1467-8659},
  doi = {10.1111/cgf.12468},
  abstract = {We present a novel method to generate a virtual character's multi-contact poses adaptive to the various shapes of the environment. Given the user-specified center of mass (CoM) position and direction as inputs, our method finds the potential contacts for the character in the surrounding geometry of the environment and generates a set of stable poses that are contact-rich. Major contributions of the work are in efficiently finding admissible support points for the target environment by precomputing candidate support points from a human pose database, and in automatically generating interactive poses that can maintain stable equilibrium. We develop the concept of support complexity to scale the set of precomputed support points by the geometric complexity of the environment. We demonstrate the effectiveness of our method by creating contact poses for various test cases of environments.},
  langid = {english},
  keywords = {Categories and Subject Descriptors (according to ACM CCS),I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Animation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12468},
  file = {/Users/james/Zotero/storage/XSUR4XTV/cgf.html}
}

@article{kimShape2PoseHumancentricShape2014,
  title = {{{Shape2Pose}}: Human-Centric Shape Analysis},
  shorttitle = {{{Shape2Pose}}},
  author = {Kim, Vladimir G. and Chaudhuri, Siddhartha and Guibas, Leonidas and Funkhouser, Thomas},
  year = {2014},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {33},
  number = {4},
  pages = {120:1--120:12},
  issn = {0730-0301},
  doi = {10.1145/2601097.2601117},
  abstract = {As 3D acquisition devices and modeling tools become widely available there is a growing need for automatic algorithms that analyze the semantics and functionality of digitized shapes. Most recent research has focused on analyzing geometric structures of shapes. Our work is motivated by the observation that a majority of man-made shapes are designed to be used by people. Thus, in order to fully understand their semantics, one needs to answer a fundamental question: "how do people interact with these objects?" As an initial step towards this goal, we offer a novel algorithm for automatically predicting a static pose that a person would need to adopt in order to use an object. Specifically, given an input 3D shape, the goal of our analysis is to predict a corresponding human pose, including contact points and kinematic parameters. This is especially challenging for man-made objects that commonly exhibit a lot of variance in their geometric structure. We address this challenge by observing that contact points usually share consistent local geometric features related to the anthropometric properties of corresponding parts and that human body is subject to kinematic constraints and priors. Accordingly, our method effectively combines local region classification and global kinematically-constrained search to successfully predict poses for various objects. We also evaluate our algorithm on six diverse collections of 3D polygonal models (chairs, gym equipment, cockpits, carts, bicycles, and bipedal devices) containing a total of 147 models. Finally, we demonstrate that the poses predicted by our algorithm can be used in several shape analysis problems, such as establishing correspondences between objects, detecting salient regions, finding informative viewpoints, and retrieving functionally-similar shapes.},
  keywords = {affordance analysis,shape analysis}
}

@article{koppulaLearningHumanActivities2013,
  title = {Learning Human Activities and Object Affordances from {{RGB-D}} Videos},
  author = {Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
  year = {2013},
  month = jul,
  journal = {International Journal of Robotics Research},
  volume = {32},
  number = {8},
  pages = {951--970},
  issn = {0278-3649},
  doi = {10.1177/0278364913478446},
  abstract = {Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4\% for affordance, 63.4\% for sub-activity and 75.0\% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot.},
  keywords = {3D perception,human activity detection,object affordance,personal robots,spatio-temporal context,supervised learning},
  file = {/Users/james/Zotero/storage/EEZGSXVN/Koppula et al. - 2013 - Learning human activities and object affordances f.pdf}
}

@inproceedings{kovarFlexibleAutomaticMotion2003,
  title = {Flexible Automatic Motion Blending with Registration Curves},
  booktitle = {Proceedings of the 2003 {{ACM SIGGRAPH}}/{{Eurographics}} Symposium on {{Computer}} Animation},
  author = {Kovar, Lucas and Gleicher, Michael},
  year = {2003},
  month = jul,
  series = {{{SCA}} '03},
  pages = {214--224},
  publisher = {{Eurographics Association}},
  address = {{Goslar, DEU}},
  abstract = {Many motion editing algorithms, including transitioning and multitarget interpolation, can be represented as instances of a more general operation called motion blending. We introduce a novel data structure called a registration curve that expands the class of motions that can be successfully blended without manual input. Registration curves achieve this by automatically determining relationships involving the timing, local coordinate frame, and constraints of the input motions. We show how registration curves improve upon existing automatic blending methods and demonstrate their use in common blending operations.},
  isbn = {978-1-58113-659-3}
}

@inproceedings{kwonNeuralHumanPerformer2021,
  title = {Neural {{Human Performer}}: {{Learning Generalizable Radiance Fields}} for {{Human Performance Rendering}}},
  shorttitle = {Neural {{Human Performer}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kwon, Youngjoong and Kim, Dahun and Ceylan, Duygu and Fuchs, Henry},
  year = {2021},
  volume = {34},
  pages = {24741--24752},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses.},
  file = {/Users/james/Zotero/storage/YETGL76C/Kwon et al. - 2021 - Neural Human Performer Learning Generalizable Rad.pdf}
}

@inproceedings{langVirtualAgentPositioning2019,
  title = {Virtual {{Agent Positioning Driven}} by {{Scene Semantics}} in {{Mixed Reality}}},
  booktitle = {2019 {{IEEE Conference}} on {{Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {Lang, Yining and Liang, Wei and Yu, Lap-Fai},
  year = {2019},
  month = mar,
  pages = {767--775},
  issn = {2642-5254},
  doi = {10.1109/VR.2019.8798018},
  abstract = {When a user interacts with a virtual agent via a mixed reality device, such as a Hololens or a Magic Leap headset, it is important to consider the semantics of the real-world scene in positioning the virtual agent, so that it interacts with the user and the objects in the real world naturally. Mixed reality aims to blend the virtual world with the real world seamlessly. In line with this goal, in this paper, we propose a novel approach to use scene semantics to guide the positioning of a virtual agent. Such considerations can avoid unnatural interaction experiences, e.g., interacting with a virtual human floating in the air. To obtain the semantics of a scene, we first reconstruct the 3D model of the scene by using the RGB-D cameras mounted on the mixed reality device (e.g., a Hololens). Then, we employ the Mask R-CNN object detector to detect objects relevant to the interactions within the scene context. To evaluate the positions and orientations for placing a virtual agent in the scene, we define a cost function based on the scene semantics, which comprises a visibility term and a spatial term. We then apply a Markov chain Monte Carlo optimization technique to search for an optimized solution for placing the virtual agent. We carried out user study experiments to evaluate the results generated by our approach. The results show that our approach achieved a higher user evaluation score than that of the alternative approaches.},
  keywords = {Cameras,Geometry,Mixed Reality—Scene Understanding—Virtual Agent Positioning,Optimization,Semantics,Solid modeling,Three-dimensional displays,Virtual reality},
  file = {/Users/james/Zotero/storage/X5LH3EYA/8798018.html}
}

@article{leimerPoseSeatAutomated2020,
  title = {Pose to {{Seat}}: {{Automated}} Design of Body-Supporting Surfaces},
  shorttitle = {Pose to {{Seat}}},
  author = {Leimer, Kurt and Winkler, Andreas and Ohrhallinger, Stefan and Musialski, Przemyslaw},
  year = {2020},
  month = may,
  journal = {Computer Aided Geometric Design},
  volume = {79},
  pages = {101855},
  issn = {01678396},
  doi = {10.1016/j.cagd.2020.101855},
  langid = {english},
  file = {/Users/james/Zotero/storage/JEXRZMIX/Leimer et al. - 2020 - Pose to Seat Automated design of body-supporting .pdf}
}

@article{lingCharacterControllersUsing2020,
  title = {Character Controllers Using Motion {{VAEs}}},
  author = {Ling, Hung Yu and Zinno, Fabio and Cheng, George and Van De Panne, Michiel},
  year = {2020},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {39},
  number = {4},
  pages = {40:40:1--40:40:12},
  issn = {0730-0301},
  doi = {10.1145/3386569.3392422},
  abstract = {A fundamental problem in computer animation is that of realizing purposeful and realistic human movement given a sufficiently-rich set of motion capture clips. We learn data-driven generative models of human movement using autoregressive conditional variational autoencoders, or Motion VAEs. The latent variables of the learned autoencoder define the action space for the movement and thereby govern its evolution over time. Planning or control algorithms can then use this action space to generate desired motions. In particular, we use deep reinforcement learning to learn controllers that achieve goal-directed movements. We demonstrate the effectiveness of the approach on multiple tasks. We further evaluate system-design choices and describe the current limitations of Motion VAEs.},
  keywords = {character control,human motion model,motion synthesis,reinforcement learning},
  file = {/Users/james/Zotero/storage/NDU2PQ7H/Ling et al. - 2020 - Character controllers using motion VAEs.pdf}
}

@inproceedings{linVirtualRealityPlatform2016,
  title = {A Virtual Reality Platform for Dynamic Human-Scene Interaction},
  booktitle = {{{SIGGRAPH ASIA}} 2016 {{Virtual Reality}} Meets {{Physical Reality}}: {{Modelling}} and {{Simulating Virtual Humans}} and {{Environments}}},
  author = {Lin, Jenny and Guo, Xingwen and Shao, Jingyu and Jiang, Chenfanfu and Zhu, Yixin and Zhu, Song-Chun},
  year = {2016},
  month = nov,
  series = {{{SA}} '16},
  pages = {1--4},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2992138.2992144},
  abstract = {Both synthetic static and simulated dynamic 3D scene data is highly useful in the fields of computer vision and robot task planning. Yet their virtual nature makes it difficult for real agents to interact with such data in an intuitive way. Thus currently available datasets are either static or greatly simplified in terms of interactions and dynamics. In this paper, we propose a system in which Virtual Reality and human / finger pose tracking is integrated to allow agents to interact with virtual environments in real time. Segmented object and scene data is used to construct a scene within Unreal Engine 4, a physics-based game engine. We then use the Oculus Rift headset with a Kinect sensor, Leap Motion controller and a dance pad to navigate and manipulate objects inside synthetic scenes in real time. We demonstrate how our system can be used to construct a multi-jointed agent representation as well as fine-grained finger pose. In the end, we propose how our system can be used for robot task planning and image semantic segmentation.},
  isbn = {978-1-4503-4548-4},
  keywords = {3D scene dataset,benchmark suite,virtual reality},
  file = {/Users/james/Zotero/storage/RTVZE8RG/Lin et al. - 2016 - A virtual reality platform for dynamic human-scene.pdf}
}

@inproceedings{liPuttingHumansScene2019,
  title = {Putting {{Humans}} in a {{Scene}}: {{Learning Affordance}} in {{3D Indoor Environments}}},
  shorttitle = {Putting {{Humans}} in a {{Scene}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Xueting and Liu, Sifei and Kim, Kihwan and Wang, Xiaolong and Yang, Ming-Hsuan and Kautz, Jan},
  year = {2019},
  month = jun,
  pages = {12360--12368},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01265},
  abstract = {Affordance1 modeling plays an important role in visual understanding. In this paper, we aim to predict affordances of 3D indoor scenes, specifically what human poses are afforded by a given indoor environment, such as sitting on a chair or standing on the floor. In order to predict valid affordances and learn possible 3D human poses in indoor scenes, we need to understand the semantic and geometric structure of a scene as well as its potential interactions with a human. To learn such a model, a large-scale dataset of 3D indoor affordances is required. In this work, we build a fully automatic 3D pose synthesizer that fuses semantic knowledge from a large number of 2D poses extracted from TV shows as well as 3D geometric knowledge from voxel representations of indoor scenes. With the data created by the synthesizer, we introduce a 3D pose generative model to predict semantically plausible and physically feasible human poses within a given scene (provided as a single RGB, RGB-D, or depth image). We demonstrate that our human affordance prediction method consistently outperforms existing state-of-the-art methods. The project website can be found at https://sites.google.com/ view/3d-affordance-cvpr19.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/Users/james/Zotero/storage/2K2EVZ4R/Li et al. - 2019 - Putting Humans in a Scene Learning Affordance in .pdf}
}

@article{loperSMPLSkinnedMultiperson2015,
  title = {{{SMPL}}: A Skinned Multi-Person Linear Model},
  shorttitle = {{{SMPL}}},
  author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and {Pons-Moll}, Gerard and Black, Michael J.},
  year = {2015},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {34},
  number = {6},
  pages = {1--16},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2816795.2818013},
  langid = {english},
  file = {/Users/james/Zotero/storage/XE9TY852/Loper et al. - 2015 - SMPL a skinned multi-person linear model.pdf}
}

@misc{martinHowMakeImmersive,
  title = {How to {{Make Immersive Game Design}} | {{University}} of {{Silicon Valley}}},
  author = {Martin, Jennifer},
  abstract = {One of the biggest goals that any game can achieve is to make the player forget they are holding a controller in their hand. Hours can go by before the player snaps back to reality and realized they haven't eaten all day. It takes incredibly immersive game design to achieve this feat. As games evolve},
  howpublished = {https://usv.edu/blog/how-to-make-immersive-game-design/},
  langid = {english},
  file = {/Users/james/Zotero/storage/LY59CUSI/how-to-make-immersive-game-design.html}
}

@article{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  journal = {ECCV},
  pages = {17},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (\texttheta, {$\varphi$})) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  langid = {english},
  file = {/Users/james/Zotero/storage/G8SRKC8H/Mildenhall et al. - NeRF Representing Scenes as Neural Radiance Field.pdf}
}

@misc{mullenjrPlacingHumanAnimations2022,
  title = {Placing {{Human Animations}} into {{3D Scenes}} by {{Learning Interaction-}} and {{Geometry-Driven Keyframes}}},
  author = {Mullen Jr, James F. and Kothandaraman, Divya and Bera, Aniket and Manocha, Dinesh},
  year = {2022},
  month = sep,
  number = {arXiv:2209.06314},
  eprint = {2209.06314},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2209.06314},
  abstract = {We present a novel method for placing a 3D human animation into a 3D scene while maintaining any human-scene interactions in the animation. We use the notion of computing the most important meshes in the animation for the interaction with the scene, which we call "keyframes." These keyframes allow us to better optimize the placement of the animation into the scene such that interactions in the animations (standing, laying, sitting, etc.) match the affordances of the scene (e.g., standing on the floor or laying in a bed). We compare our method, which we call PAAK, with prior approaches, including POSA, PROX ground truth, and a motion synthesis method, and highlight the benefits of our method with a perceptual study. Human raters preferred our PAAK method over the PROX ground truth data 64.6\textbackslash\% of the time. Additionally, in direct comparisons, the raters preferred PAAK over competing methods including 61.5\textbackslash\% compared to POSA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/james/Zotero/storage/G9PIGE96/Mullen Jr et al. - 2022 - Placing Human Animations into 3D Scenes by Learnin.pdf;/Users/james/Zotero/storage/3WHR2AIR/2209.html}
}

@inproceedings{niemeyerGIRAFFERepresentingScenes2021,
  title = {{{GIRAFFE}}: {{Representing Scenes}} as {{Compositional Generative Neural Feature Fields}}},
  shorttitle = {{{GIRAFFE}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Niemeyer, Michael and Geiger, Andreas},
  year = {2021},
  month = jun,
  pages = {11448--11459},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01129},
  abstract = {Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/james/Zotero/storage/VEREPS3A/Niemeyer and Geiger - 2021 - GIRAFFE Representing Scenes as Compositional Gene.pdf}
}

@inproceedings{noguchiNeuralArticulatedRadiance2021,
  title = {Neural {{Articulated Radiance Field}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Noguchi, Atsuhiro and Xiao, Sun and Lin, Stephen and Harada, Tatsuya},
  year = {2021},
  month = oct,
  pages = {5762--5772},
  file = {/Users/james/Zotero/storage/AEHXU2ZG/Noguchi_Neural_Articulated_Radiance_Field_ICCV_2021_paper.pdf}
}

@misc{noguchiUnsupervisedLearningEfficient2022,
  title = {Unsupervised {{Learning}} of {{Efficient Geometry-Aware Neural Articulated Representations}}},
  author = {Noguchi, Atsuhiro and Sun, Xiao and Lin, Stephen and Harada, Tatsuya},
  year = {2022},
  month = apr,
  number = {arXiv:2204.08839},
  eprint = {2204.08839},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {We propose an unsupervised method for 3D geometry-aware representation learning of articulated objects. Though photorealistic images of articulated objects can be rendered with explicit pose control through existing 3D neural representations, these methods require ground truth 3D pose and foreground masks for training, which are expensive to obtain. We obviate this need by learning the representations with GAN training. From random poses and latent vectors, the generator is trained to produce realistic images of articulated objects by adversarial training. To avoid a large computational cost for GAN training, we propose an efficient neural representation for articulated objects based on tri-planes and then present a GAN-based framework for its unsupervised training. Experiments demonstrate the efficiency of our method and show that GAN-based training enables learning of controllable 3D representations without supervision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/james/Zotero/storage/DQBANLCJ/Noguchi et al. - 2022 - Unsupervised Learning of Efficient Geometry-Aware .pdf;/Users/james/Zotero/storage/2386QWN6/2204.html}
}

@incollection{osmanSTARSparseTrained2020,
  title = {{{STAR}}: {{Sparse Trained Articulated Human Body Regressor}}},
  shorttitle = {{{STAR}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020},
  author = {Osman, Ahmed A. A. and Bolkart, Timo and Black, Michael J.},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12351},
  pages = {598--613},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58539-6_36},
  abstract = {The SMPL body model is widely used for the estimation, synthesis, and analysis of 3D human pose and shape. While popular, we show that SMPL has several limitations and introduce STAR, which is quantitatively and qualitatively superior to SMPL. First, SMPL has a huge number of parameters resulting from its use of global blend shapes. These dense pose-corrective offsets relate every vertex on the mesh to all the joints in the kinematic tree, capturing spurious long-range correlations. To address this, we define per-joint pose correctives and learn the subset of mesh vertices that are influenced by each joint movement. This sparse formulation results in more realistic deformations and significantly reduces the number of model parameters to 20\% of SMPL. When trained on the same data as SMPL, STAR generalizes better despite having many fewer parameters. Second, SMPL factors pose-dependent deformations from body shape while, in reality, people with different shapes deform differently. Consequently, we learn shape-dependent posecorrective blend shapes that depend on both body pose and BMI. Third, we show that the shape space of SMPL is not rich enough to capture the variation in the human population. We address this by training STAR with an additional 10,000 scans of male and female subjects, and show that this results in better model generalization. STAR is compact, generalizes better to new bodies and is a drop-in replacement for SMPL. STAR is publicly available for research purposes at http://star.is.tue.mpg.de.},
  isbn = {978-3-030-58538-9 978-3-030-58539-6},
  langid = {english},
  file = {/Users/james/Zotero/storage/VXFFBDJT/Osman et al. - 2020 - STAR Sparse Trained Articulated Human Body Regres.pdf}
}

@inproceedings{pavlakosExpressiveBodyCapture2019,
  title = {Expressive {{Body Capture}}: {{3D Hands}}, {{Face}}, and {{Body From}} a {{Single Image}}},
  shorttitle = {Expressive {{Body Capture}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. and Tzionas, Dimitrios and Black, Michael J.},
  year = {2019},
  month = jun,
  pages = {10967--10977},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01123},
  abstract = {To facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8\texttimes{} over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/Users/james/Zotero/storage/VSYQ22F5/Pavlakos et al. - 2019 - Expressive Body Capture 3D Hands, Face, and Body .pdf}
}

@inproceedings{pavlovicLearningSwitchingLinear2000,
  title = {Learning {{Switching Linear Models}} of {{Human Motion}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pavlovic, Vladimir and Rehg, James M and MacCormick, John},
  year = {2000},
  volume = {13},
  publisher = {{MIT Press}},
  abstract = {The human  figure  exhibits  complex and  rich  dynamic  behavior that is  both nonlinear and  time-varying.  Effective models  of human dynamics  can be learned from motion capture data using switching linear dynamic  system  (SLDS)  models.  We  present results  for human  motion  synthe(cid:173) sis, classification, and visual tracking using learned SLDS models.  Since  exact inference in SLDS is intractable, we present three approximate in(cid:173) ference algorithms and compare their performance.  In particular, a new  variational  inference algorithm  is  obtained by  casting the  SLDS  model  as  a Dynamic  Bayesian  Network.  Classification experiments  show  the  superiority of SLDS over conventional HMM's for our problem domain.},
  file = {/Users/james/Zotero/storage/ASMYQKPD/Pavlovic et al. - 2000 - Learning Switching Linear Models of Human Motion.pdf}
}

@inproceedings{pejsaWhoMeHow2017,
  title = {Who, {{Me}}? {{How Virtual Agents Can Shape Conversational Footing}} in {{Virtual Reality}}},
  shorttitle = {Who, {{Me}}?},
  booktitle = {Intelligent {{Virtual Agents}}},
  author = {Pejsa, Tomislav and Gleicher, Michael and Mutlu, Bilge},
  editor = {Beskow, Jonas and Peters, Christopher and Castellano, Ginevra and O'Sullivan, Carol and Leite, Iolanda and Kopp, Stefan},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {347--359},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-67401-8_45},
  abstract = {The nonverbal behaviors of conversational partners reflect their conversational footing, signaling who in the group are the speakers, addressees, bystanders, and overhearers. Many applications of virtual reality (VR) will involve multiparty conversations with virtual agents and avatars of others where appropriate signaling of footing will be critical. In this paper, we introduce computational models of gaze and spatial orientation that a virtual agent can use to signal specific footing configurations. An evaluation of these models through a user study found that participants conformed to conversational roles signaled by the agent and contributed to the conversation more as addressees than as bystanders. We observed these effects in immersive VR, but not on a 2D display, suggesting an increased sensitivity to virtual agents' footing cues in VR-based interfaces.},
  isbn = {978-3-319-67401-8},
  langid = {english},
  keywords = {Embodied conversational agents,Gaze,Orientation,Virtual reality}
}

@inproceedings{pengAnimatableNeuralRadiance2021,
  title = {Animatable {{Neural Radiance Fields}} for {{Modeling Dynamic Human Bodies}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Peng, Sida and Dong, Junting and Wang, Qianqian and Zhang, Shangzhan and Shuai, Qing and Zhou, Xiaowei and Bao, Hujun},
  year = {2021},
  month = oct,
  pages = {14294--14303},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.01405},
  abstract = {This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-tocanonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code and supplementary materials are available at https://zju3dv.github.io/animatable nerf/.},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/james/Zotero/storage/FKIWJ45U/Peng et al. - 2021 - Animatable Neural Radiance Fields for Modeling Dyn.pdf}
}

@article{prokudinEfficientLearningPoint2019,
  title = {Efficient {{Learning}} on {{Point Clouds With Basis Point Sets}}},
  author = {Prokudin, Sergey and Lassner, Christoph and Romero, Javier},
  year = {2019},
  journal = {CVPR},
  pages = {10},
  abstract = {With an increased availability of 3D scanning technology, point clouds are moving into the focus of computer vision as a rich representation of everyday scenes. However, they are hard to handle for machine learning algorithms due to their unordered structure. One common approach is to apply occupancy grid mapping, which dramatically increases the amount of data stored and at the same time loses details through discretization. Recently, deep learning models were proposed to handle point clouds directly and achieve input permutation invariance. However, these architectures often use an increased number of parameters and are computationally inefficient. In this work we propose basis point sets (BPS) as a highly efficient and fully general way to process point clouds with machine learning algorithms. The basis point set representation is a residual representation that can be computed efficiently and can be used with standard neural network architectures and other machine learning algorithms. Using the proposed representation as the input to a simple fully connected network allows us to match the performance of PointNet on a shape classification task, while using three orders of magnitude less floating point operations. In a second experiment, we show how the proposed representation can be used for registering high resolution meshes to noisy 3D scans. Here, we present the first method for single-pass high-resolution mesh registration, avoiding time-consuming per-scan optimization and allowing real-time execution.},
  langid = {english},
  file = {/Users/james/Zotero/storage/XTI9TSSV/Prokudin et al. - Efficient Learning on Point Clouds With Basis Poin.pdf}
}

@inproceedings{rempeHuMoR3DHuman2021,
  title = {{{HuMoR}}: {{3D Human Motion Model}} for {{Robust Pose Estimation}}},
  shorttitle = {{{HuMoR}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Rempe, Davis and Birdal, Tolga and Hertzmann, Aaron and Yang, Jimei and Sridhar, Srinath and Guibas, Leonidas J.},
  year = {2021},
  month = oct,
  pages = {11468--11479},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.01129},
  abstract = {We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos. See the project page at geometry.stanford.edu/projects/humor.},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/james/Zotero/storage/S6PKPTPJ/Rempe et al. - 2021 - HuMoR 3D Human Motion Model for Robust Pose Estim.pdf}
}

@article{savvaSceneGrokInferringAction2014,
  title = {{{SceneGrok}}: Inferring Action Maps in {{3D}} Environments},
  shorttitle = {{{SceneGrok}}},
  author = {Savva, Manolis and Chang, Angel X. and Hanrahan, Pat and Fisher, Matthew and Nie{\ss}ner, Matthias},
  year = {2014},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {33},
  number = {6},
  pages = {212:1--212:10},
  issn = {0730-0301},
  doi = {10.1145/2661229.2661230},
  abstract = {With modern computer graphics, we can generate enormous amounts of 3D scene data. It is now possible to capture high-quality 3D representations of large real-world environments. Large shape and scene databases, such as the Trimble 3D Warehouse, are publicly accessible and constantly growing. Unfortunately, while a great amount of 3D content exists, most of it is detached from the semantics and functionality of the objects it represents. In this paper, we present a method to establish a correlation between the geometry and the functionality of 3D environments. Using RGB-D sensors, we capture dense 3D reconstructions of real-world scenes, and observe and track people as they interact with the environment. With these observations, we train a classifier which can transfer interaction knowledge to unobserved 3D scenes. We predict a likelihood of a given action taking place over all locations in a 3D environment and refer to this representation as an action map over the scene. We demonstrate prediction of action maps in both 3D scans and virtual scenes. We evaluate our predictions against ground truth annotations by people, and present an approach for characterizing 3D scenes by functional similarity using action maps.},
  keywords = {3D scenes,object semantics,scene understanding}
}

@article{sigalHumanEvaSynchronizedVideo2010,
  title = {{{HumanEva}}: {{Synchronized Video}} and {{Motion Capture Dataset}} and {{Baseline Algorithm}} for {{Evaluation}} of {{Articulated Human Motion}}},
  shorttitle = {{{HumanEva}}},
  author = {Sigal, Leonid and Balan, Alexandru O. and Black, Michael J.},
  year = {2010},
  month = mar,
  journal = {International Journal of Computer Vision},
  volume = {87},
  number = {1-2},
  pages = {4--27},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-009-0273-6},
  langid = {english},
  file = {/Users/james/Zotero/storage/8HTPXVUN/Sigal et al. - 2010 - HumanEva Synchronized Video and Motion Capture Da.pdf}
}

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {/Users/james/Zotero/storage/ITPZ4K5I/Sohn et al. - 2015 - Learning Structured Output Representation using De.pdf}
}

@article{starkeNeuralStateMachine2019,
  title = {Neural State Machine for Character-Scene Interactions},
  author = {Starke, Sebastian and Zhang, He and Komura, Taku and Saito, Jun},
  year = {2019},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {38},
  number = {6},
  pages = {209:1--209:14},
  issn = {0730-0301},
  doi = {10.1145/3355089.3356505},
  abstract = {We propose Neural State Machine, a novel data-driven framework to guide characters to achieve goal-driven actions with precise scene interactions. Even a seemingly simple task such as sitting on a chair is notoriously hard to model with supervised learning. This difficulty is because such a task involves complex planning with periodic and non-periodic motions reacting to the scene geometry to precisely position and orient the character. Our proposed deep auto-regressive framework enables modeling of multi-modal scene interaction behaviors purely from data. Given high-level instructions such as the goal location and the action to be launched there, our system computes a series of movements and transitions to reach the goal in the desired state. To allow characters to adapt to a wide range of geometry such as different shapes of furniture and obstacles, we incorporate an efficient data augmentation scheme to randomly switch the 3D geometry while maintaining the context of the original motion. To increase the precision to reach the goal during runtime, we introduce a control scheme that combines egocentric inference and goal-centric inference. We demonstrate the versatility of our model with various scene interaction tasks such as sitting on a chair, avoiding obstacles, opening and entering through a door, and picking and carrying objects generated in real-time just from a single model.},
  keywords = {character animation,character control,deep learning,human motion,locomotion,neural networks},
  file = {/Users/james/Zotero/storage/K2WNUKM2/Starke et al. - 2019 - Neural state machine for character-scene interacti.pdf}
}

@inproceedings{su2021anerf,
  title = {A-{{NeRF}}: {{Articulated}} Neural Radiance Fields for Learning Human Shape, Appearance, and Pose},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Su, Shih-Yang and Yu, Frank and Zollh{\"o}fer, Michael and Rhodin, Helge},
  year = {2021}
}

@inproceedings{tan2018,
  title = {Where and Who? {{Automatic}} Semantic-Aware Person Composition},
  booktitle = {{{IEEE}} Winter Conf. on Applications of Computer Vision ({{WACV}})},
  author = {Tan, Fuwen and Bernier, Crispin and Cohen, Benjamin and Ordonez, Vicente and Barnes, Connelly},
  year = {2018}
}

@inproceedings{urtasunTopologicallyconstrainedLatentVariable2008,
  title = {Topologically-Constrained Latent Variable Models},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Urtasun, Raquel and Fleet, David J. and Geiger, Andreas and Popovi{\'c}, Jovan and Darrell, Trevor J. and Lawrence, Neil D.},
  year = {2008},
  pages = {1080--1087},
  publisher = {{ACM Press}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390292},
  abstract = {In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that are poorly captured with a Euclidean space. In this paper, we present a range of approaches for embedding data in a non-Euclidean latent space. Our focus is the Gaussian Process latent variable model. In the context of human motion modeling this allows us to (a) learn models with interpretable latent directions enabling, for example, style/content separation, and (b) generalise beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data.},
  isbn = {978-1-60558-205-4},
  langid = {english},
  file = {/Users/james/Zotero/storage/63HQJP86/Urtasun et al. - 2008 - Topologically-constrained latent variable models.pdf}
}

@inproceedings{wangCriticRegularizedRegression2020,
  title = {Critic Regularized Regression},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Wang, Ziyu and Novikov, Alexander and {\.Z}o{\l}na, Konrad and Springenberg, Jost Tobias and Reed, Scott and Shahriari, Bobak and Siegel, Noah and Merel, Josh and Gulcehre, Caglar and Heess, Nicolas and {de Freitas}, Nando},
  year = {2020},
  month = dec,
  series = {{{NIPS}}'20},
  pages = {7768--7778},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Offline reinforcement learning (RL), also known as batch RL, offers the prospect of policy optimization from large pre-recorded datasets without online environment interaction. It addresses challenges with regard to the cost of data collection and safety, both of which are particularly pertinent to real-world applications of RL. Unfortunately, most off-policy algorithms perform poorly when learning from a fixed dataset. In this paper, we propose a novel offline RL algorithm to learn policies from data using a form of critic-regularized regression (CRR). We find that CRR performs surprisingly well and scales to tasks with high-dimensional state and action spaces \textendash{} outperforming several state-of-the-art offline RL algorithms by a significant margin on a wide range of benchmark tasks.},
  isbn = {978-1-71382-954-6},
  file = {/Users/james/Zotero/storage/39JTQZUE/Wang et al. - 2020 - Critic regularized regression.pdf}
}

@article{wangDiverseNaturalSceneAware2022,
  title = {Towards {{Diverse}} and {{Natural Scene-Aware 3D Human Motion Synthesis}}},
  author = {Wang, Jingbo and Rong, Yu and Liu, Jingyuan and Yan, Sijie and Lin, Dahua and Dai, Bo},
  year = {2022},
  journal = {CVPR},
  pages = {10},
  langid = {english},
  file = {/Users/james/Zotero/storage/S6I45ES9/Wang et al. - Towards Diverse and Natural Scene-Aware 3D Human M.pdf}
}

@misc{wangGeometricPoseAffordance2021,
  title = {Geometric {{Pose Affordance}}: {{3D Human Pose}} with {{Scene Constraints}}},
  shorttitle = {Geometric {{Pose Affordance}}},
  author = {Wang, Zhe and Chen, Liyan and Rathore, Shaurya and Shin, Daeyun and Fowlkes, Charless},
  year = {2021},
  month = dec,
  number = {arXiv:1905.07718},
  eprint = {1905.07718},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Full 3D estimation of human pose from a single image remains a challenging task despite many recent advances. In this paper, we explore the hypothesis that strong prior information about scene geometry can be used to improve pose estimation accuracy. To tackle this question empirically, we have assembled a novel \$\textbackslash textbf\{Geometric Pose Affordance\}\$ dataset, consisting of multi-view imagery of people interacting with a variety of rich 3D environments. We utilized a commercial motion capture system to collect gold-standard estimates of pose and construct accurate geometric 3D CAD models of the scene itself. To inject prior knowledge of scene constraints into existing frameworks for pose estimation from images, we introduce a novel, view-based representation of scene geometry, a \$\textbackslash textbf\{multi-layer depth map\}\$, which employs multi-hit ray tracing to concisely encode multiple surface entry and exit points along each camera view ray direction. We propose two different mechanisms for integrating multi-layer depth information pose estimation: input as encoded ray features used in lifting 2D pose to full 3D, and secondly as a differentiable loss that encourages learned models to favor geometrically consistent pose estimates. We show experimentally that these techniques can improve the accuracy of 3D pose estimates, particularly in the presence of occlusion and complex scene geometry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/james/Zotero/storage/QTZRSTAM/Wang et al. - 2021 - Geometric Pose Affordance 3D Human Pose with Scen.pdf;/Users/james/Zotero/storage/IA7569JJ/1905.html}
}

@inproceedings{wangSynthesizingLongTerm3D2021,
  title = {Synthesizing {{Long-Term 3D Human Motion}} and {{Interaction}} in {{3D Scenes}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Jiashun and Xu, Huazhe and Xu, Jingwei and Liu, Sifei and Wang, Xiaolong},
  year = {2021},
  month = jun,
  pages = {9396--9406},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00928},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/james/Zotero/storage/QINIIIJ5/Wang et al. - 2021 - Synthesizing Long-Term 3D Human Motion and Interac.pdf}
}

@article{xiaLearningBasedSphereNonlinear2019,
  title = {Learning-{{Based Sphere Nonlinear Interpolation}} for {{Motion Synthesis}}},
  author = {Xia, Guiyu and Sun, Huaijiang and Liu, Qingshan and Hang, Renlong},
  year = {2019},
  journal = {IEEE Transactions on Industrial Informatics},
  doi = {10.1109/TII.2019.2894113},
  abstract = {A learning-based Sphere nonlinear interpolation (Snerp) model that can generate natural in-between motions in terms of a given start\textendash end frame pair via a paired dictionary learning process. Motion synthesis technology can produce natural and coordinated motion data without a motion capture process, which is complex and costly. Current motion synthesis methods usually provide a few interfaces to avoid the arbitrariness of the synthesis process, but this actually reduces the understandability of the synthesis process. In this paper, we propose a learning-based Sphere nonlinear interpolation (Snerp) model that can generate natural in-between motions in terms of a given start\textendash end frame pair. Variety of the input frame pairs will enrich the diversity of the generated motions. The angle speed of natural human motion is not uniform and presents different change rules (we call them motion patterns) for different motions, so we first extract the motion patterns and then build the relation between motion pattern space and frame pair space via a paired dictionary learning process. After learning, we estimate the motion pattern according to the representation of a given start\textendash end frame pair on the frame pair dictionary. We select several different types of start\textendash end frame pairs from the real motion sequences as the testing data and good results of both objective and subjective evaluations on the generated motions demonstrate the superior performance of Snerp.}
}

@article{xuHierarchicalStylebasedNetworks2020,
  title = {Hierarchical {{Style-based Networks}} for {{Motion Synthesis}}},
  author = {Xu, Jingwei and Xu, Huazhe and Ni, Bingbing and Yang, Xiaokang and Wang, Xiaolong and Darrell, Trevor},
  year = {2020},
  journal = {ECCV},
  pages = {16},
  abstract = {Generating diverse and natural human motion is one of the long-standing goals for creating intelligent characters in the animated world. In this paper, we propose an unsupervised method for generating long-range, diverse and plausible behaviors to achieve a specific goal location. Our proposed method learns to model the motion of human by decomposing a long-range generation task in a hierarchical manner. Given the starting and ending states, a memory bank is used to retrieve motion references as source material for short-range clip generation. We first propose to explicitly disentangle the provided motion material into style and content counterparts via bi-linear transformation modelling, where diverse synthesis is achieved by free-form combination of these two components. The short-range clips are then connected to form a longrange motion sequence. Without ground truth annotation, we propose a parameterized bi-directional interpolation scheme to guarantee the physical validity and visual naturalness of generated results. On large-scale skeleton dataset, we show that the proposed method is able to synthesise long-range, diverse and plausible motion, which is also generalizable to unseen motion data during testing. Moreover, we demonstrate the generated sequences are useful as subgoals for actual physical execution in the animated world. Please refer to our project page for more synthesised results 4.},
  langid = {english},
  file = {/Users/james/Zotero/storage/VG7CB9EN/Xu et al. - Hierarchical Style-based Networks for Motion Synth.pdf}
}

@inproceedings{yangBANMoBuildingAnimatable2022,
  title = {{{BANMo}}: {{Building Animatable 3D Neural Models From Many Casual Videos}}},
  shorttitle = {{{BANMo}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yang, Gengshan and Vo, Minh and Neverova, Natalia and Ramanan, Deva and Vedaldi, Andrea and Joo, Hanbyul},
  year = {2022},
  pages = {2863--2873},
  langid = {english},
  file = {/Users/james/Zotero/storage/62ETWUZC/Yang et al. - 2022 - BANMo Building Animatable 3D Neural Models From M.pdf;/Users/james/Zotero/storage/S7SSX9UD/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.html}
}

@inproceedings{yangViSERVideoSpecificSurface2021,
  title = {{{ViSER}}: {{Video-Specific Surface Embeddings}} for {{Articulated 3D Shape Reconstruction}}},
  shorttitle = {{{ViSER}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yang, Gengshan and Sun, Deqing and Jampani, Varun and Vlasic, Daniel and Cole, Forrester and Liu, Ce and Ramanan, Deva},
  year = {2021},
  volume = {34},
  pages = {19326--19338},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We introduce ViSER, a method for recovering articulated 3D shapes and dense3D trajectories from monocular videos.  Previous work on high-quality reconstruction of dynamic 3D shapes typically relies on multiple camera views, strong category-specific priors, or 2D keypoint supervision. We show that none of these are required if one can reliably estimate long-range correspondences in a video, making use of only 2D object masks and two-frame optical flow as inputs. ViSER infers correspondences by matching 2D pixels to a canonical,  deformable 3D mesh via video-specific surface embeddings that capture the pixel appearance of each surface point.  These embeddings behave as a continuous set of keypoint descriptors defined over the mesh surface, which can be used to establish dense long-range correspondences across pixels.  The surface embeddings are implemented as coordinate-based MLPs that are fit to each video via self-supervised losses.Experimental results show that ViSER compares favorably against prior work on challenging videos of humans with loose clothing and unusual poses as well as animals videos from DAVIS and YTVOS. Project page: viser-shape.github.io.},
  file = {/Users/james/Zotero/storage/QT7SJU75/Yang et al. - 2021 - ViSER Video-Specific Surface Embeddings for Artic.pdf}
}

@inproceedings{yePAVALPositionAwareVirtual2021,
  title = {{{PAVAL}}: {{Position-Aware Virtual Agent Locomotion}} for {{Assisted Virtual Reality Navigation}}},
  shorttitle = {{{PAVAL}}},
  booktitle = {2021 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Ye, Zi-Ming and Chen, Jun-Long and Wang, Miao and Yang, Yong-Liang},
  year = {2021},
  month = oct,
  pages = {239--247},
  issn = {1554-7868},
  doi = {10.1109/ISMAR52148.2021.00039},
  abstract = {Virtual agents are typical assistance tools for navigation and interaction in Virtual Reality (VR) tour, training, education, etc. It has been demonstrated that the gaits, gestures, gazes, and positions of virtual agents are major factors that affect the user's perception and experience for seated and standing VR. In this paper, we present a novel position-aware virtual agent locomotion method, called PAVAL, that can perform virtual agent positioning (position+orientation) in real time for room-scale VR navigation assistance. We first analyze design guidelines for virtual agent locomotion and model the problem using the positions of the user and the surrounding virtual objects. Then we conduct a one-off preliminary study to collect subjective data and present a model for virtual agent positioning prediction with fixed user position. Based on the model, we propose an algorithm to optimize the object of interest, virtual agent position, and virtual agent orientation in sequence for virtual agent locomotion. As a result, during user navigation in a virtual scene, the virtual agent automatically moves in real time and introduces virtual object information to the user. We evaluate PAVAL and two alternative methods via a user study with humanoid virtual agents in various scenes, including virtual museum, factory, and school gym. The results reveal that our method is superior to the baseline condition.},
  keywords = {Design methodology,Humanoid robots,Navigation,optimization,Predictive models,Solid modeling,Tools,Training,Virtual Agent},
  file = {/Users/james/Zotero/storage/L229S2GQ/9583787.html}
}

@inproceedings{zhangGenerating3DPeople2020,
  title = {Generating {{3D People}} in {{Scenes Without People}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Yan and Hassan, Mohamed and Neumann, Heiko and Black, Michael J. and Tang, Siyu},
  year = {2020},
  month = jun,
  pages = {6193--6203},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00623},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/Users/james/Zotero/storage/DLK9U5BK/Zhang et al. - 2020 - Generating 3D People in Scenes Without People.pdf}
}

@inproceedings{zhangPLACEProximityLearning2020,
  title = {{{PLACE}}: {{Proximity Learning}} of {{Articulation}} and {{Contact}} in {{3D Environments}}},
  shorttitle = {{{PLACE}}},
  booktitle = {2020 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J. and Tang, Siyu},
  year = {2020},
  month = nov,
  pages = {642--651},
  publisher = {{IEEE}},
  address = {{Fukuoka, Japan}},
  doi = {10.1109/3DV50981.2020.00074},
  isbn = {978-1-72818-128-8},
  langid = {english},
  file = {/Users/james/Zotero/storage/BBHTSHFL/Zhang et al. - 2020 - PLACE Proximity Learning of Articulation and Cont.pdf}
}

@article{zhaoHumanNeRFEfficientlyGenerated2022,
  title = {{{HumanNeRF}}: {{Efficiently Generated Human Radiance Field From Sparse Inputs}}},
  author = {Zhao, Fuqiang and Yang, Wei and Zhang, Jiakai and Lin, Pei and Zhang, Yingliang and Yu, Jingyi and Xu, Lan},
  year = {2022},
  journal = {CVPR},
  pages = {11},
  abstract = {Recent neural human representations can produce highquality multi-view rendering but require using dense multiview inputs and costly training. They are hence largely limited to static models as training each frame is infeasible. We present HumanNeRF - a neural representation with efficient generalization ability - for high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated pixel-alignment feature across multiview inputs along with a pose embedded non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can already produce reasonable rendering on sparse video inputs of unseen subjects and camera settings. To further improve the rendering quality, we augment our solution with in-hour scene-specific fine-tuning, and an appearance blending module for combining the benefits of both neural volumetric rendering and neural texture blending. Extensive experiments on various multi-view dynamic human datasets demonstrate effectiveness of our approach in synthesizing photo-realistic free-view humans under challenging motions and with very sparse camera view inputs.},
  langid = {english},
  file = {/Users/james/Zotero/storage/FHD8EBGH/Zhao et al. - HumanNeRF Efficiently Generated Human Radiance Fi.pdf}
}





@inproceedings{prabhu2021active,
  title={Active domain adaptation via clustering uncertainty-weighted embeddings},
  author={Prabhu, Viraj and Chandrasekaran, Arjun and Saenko, Kate and Hoffman, Judy},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8505--8514},
  year={2021}
}
@inproceedings{roy2018deep,
  title={Deep active learning for object detection.},
  author={Roy, Soumya and Unmesh, Asim and Namboodiri, Vinay P},
  booktitle={BMVC},
  pages={91},
  year={2018}
}
@inproceedings{su2020active,
  title={Active adversarial domain adaptation},
  author={Su, Jong-Chyi and Tsai, Yi-Hsuan and Sohn, Kihyuk and Liu, Buyu and Maji, Subhransu and Chandraker, Manmohan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={739--748},
  year={2020}
}
@article{kothandaraman2022distilladapt,
  title={DistillAdapt: Source-Free Active Visual Domain Adaptation},
  author={Kothandaraman, Divya and Shekhar, Sumit and Sancheti, Abhilasha and Ghuhan, Manoj and Shukla, Tripti and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2205.12840},
  year={2022}
}
@article{vondrick2011video,
  title={Video annotation and tracking with active learning},
  author={Vondrick, Carl and Ramanan, Deva},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}}
  
@conference{AMASS:ICCV:2019,
  title = {{AMASS}: Archive of Motion Capture as Surface Shapes},
  author = {Mahmood, Naureen and Ghorbani, Nima and Troje, Nikolaus F. and Pons-Moll, Gerard and Black, Michael J.},
  booktitle = {International Conference on Computer Vision},
  pages = {5442--5451},
  month = oct,
  year = {2019},
  month_numeric = {10}
}

@misc{cmuWEB,
	title        = {{CMU MoCap Dataset}},
	author       = {{Carnegie Mellon University}},
	url          = {http://mocap.cs.cmu.edu}
}

@article{Matterport3D,
  title={Matterport3D: Learning from RGB-D Data in Indoor Environments},
  author={Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  journal={International Conference on 3D Vision (3DV)},
  year={2017}
}

@inproceedings{3dfront,
      title={3d-front: 3d furnished rooms with layouts and semantics},
      author={Fu, Huan and Cai, Bowen and Gao, Lin and Zhang, Ling-Xiao and Wang, Jiaming and Li, Cao and Zeng, Qixun and Sun, Chengyue and Jia, Rongfei and Zhao, Binqiang and others},
      booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
      pages={10933--10942},
      year={2021}
    }
    
@inproceedings{xiazamirhe2018gibsonenv,
  title={Gibson env: real-world perception for embodied agents},
  author={Xia, Fei and R. Zamir, Amir and He, Zhi-Yang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on},
  year={2018},
  organization={IEEE}
}

@inproceedings{dai2017scannet,
    title={ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes},
    author={Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
    booktitle = {Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
    year = {2017}
}

@inproceedings{zillow,
  title     = {Zillow Indoor Dataset: Annotated Floor Plans With 360º Panoramas and 3D Room Layouts},
  author    = {Cruz, Steve and Hutchcroft, Will and Li, Yuguang and Khosravan, Naji and Boyadzhiev, Ivaylo and Kang, Sing Bing},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2021},
  pages     = {2133--2143}
}

@article{ruddle2001movement,
  title={Movement in cluttered virtual environments},
  author={Ruddle, Roy A and Jones, Dylan M},
  journal={Presence},
  volume={10},
  number={5},
  pages={511--524},
  year={2001},
  publisher={MIT Press}
}


@article{lessels2005movement,
  title={Movement around real and virtual cluttered environments},
  author={Lessels, Simon and Ruddle, Roy A},
  journal={Presence: Teleoperators \& Virtual Environments},
  volume={14},
  number={5},
  pages={580--596},
  year={2005},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@article{simeone2017altering,
  title={Altering user movement behaviour in virtual environments},
  author={Simeone, Adalberto L and Mavridou, Ifigeneia and Powell, Wendy},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={4},
  pages={1312--1321},
  year={2017},
  publisher={IEEE}
}


@article{argelaguet2009efficient,
  title={Efficient 3D pointing selection in cluttered virtual environments},
  author={Argelaguet, Ferran and Andujar, Carlos},
  journal={IEEE Computer Graphics and Applications},
  volume={29},
  number={6},
  pages={34--43},
  year={2009},
  publisher={IEEE}
}


@inproceedings{ruddle2004effect,
  title={The effect of environment characteristics and user interaction on levels of virtual environment sickness},
  author={Ruddle, Roy A},
  booktitle={IEEE Virtual Reality 2004},
  pages={141--285},
  year={2004},
  organization={IEEE}
}
