\section{Related Work}



\textbf{Generating Plausible Virtual Agent Motion.}
Closely related to our work is the problem of generating plausible virtual agent motion, especially in cluttered, dense virtual environments. This is a well studied problem \cite{ruddle2001movement, ruddle2004effect, lessels2005movement, simeone2017altering, argelaguet2009efficient}, with many trying to direct or maneuver \textit{a users} motion through a dense, cluttered environments. More recently, many researchers focused on plotting virtual agent motion through a virtual environment \cite{yePAVALPositionAwareVirtual2021, langVirtualAgentPositioning2019}, or generating virtual agents to fit an environment \cite{zhangGenerating3DPeople2020, liPuttingHumansScene2019}. We aim to combine many of these threads of research by creating a method capable of generating agent-scene pairings such that interactions with the environment are natural-looking and convincing for the user.


\textbf{Human Models.}
Most prior work with virtual human agents utilizes body skeletons to model 3D humans \cite{ionescuHuman36MLarge2014, sigalHumanEvaSynchronizedVideo2010}. However, the surface of the body is important for rending the virtual agent or modeling interactions with the scene or any objects. For example, it is essential to know how far the surface of the back is from the skeleton when placing an agent laying on a surface. Learned parametric 3D body models have addressed this need \cite{anguelovSCAPEShapeCompletion2005, jooTotalCapture3D2018, loperSMPLSkinnedMultiperson2015, pavlakosExpressiveBodyCapture2019, osmanSTARSparseTrained2020}. In our work, we utilize SMPL-X \cite{pavlakosExpressiveBodyCapture2019}, a body model that captures face and hand articulation in addition to the structure of the body itself.

\textbf{Motion Synthesis.} 
Motion synthesis is a well-studied problem in computer graphics, VR,  and computer vision \cite{kovarFlexibleAutomaticMotion2003, pavlovicLearningSwitchingLinear2000, urtasunTopologicallyconstrainedLatentVariable2008, harveyRobustMotionInbetweening2020, starkeNeuralStateMachine2019, lingCharacterControllersUsing2020, xuHierarchicalStylebasedNetworks2020, holdenDeepLearningFramework2016, rempeHuMoR3DHuman2021, wangSynthesizingLongTerm3D2021, wangDiverseNaturalSceneAware2022}. Early work in motion synthesis worked to synthesize intermediate states between two given points in the motion sequence \cite{urtasunTopologicallyconstrainedLatentVariable2008, harveyRobustMotionInbetweening2020, xiaLearningBasedSphereNonlinear2019}. These methods were known to not handle large translational position changes effectively. More recent work began utilizing data-driven deep models for motion synthesis \cite{xuHierarchicalStylebasedNetworks2020, holdenDeepLearningFramework2016}. These methods show better generalization than the geometric methods from earlier work. However, most of these methods look at the animation or generated motion in isolation from the scene or environment.

Some more recent motion synthesis methods account for both the environment and the virtual agent's movement \cite{starkeNeuralStateMachine2019, lingCharacterControllersUsing2020, cleggLearningDressSynthesizing2018, wangCriticRegularizedRegression2020}. Many of these methods use greatly simplified scenarios with predefined objects or primitive motion. Our work is closest to \cite{wangSynthesizingLongTerm3D2021} which utilizes arbitrary 3D environments when synthesizing its motion. The motions produced by \cite{wangSynthesizingLongTerm3D2021} and others frame our use case by falling short of the realism of motion-captured movements. However, our system, PACE does not synthesize its own motion, but instead leverages motion capture data and tailors it to the needs of the scene, resulting in the most natural-looking and physically plausible agent-scene pairing possible.

\begin{figure*}[ht]
	\begin{center}
		\includegraphics[width=\linewidth]{media/method.pdf}
        \caption{An overview of PACE. Using exiting methods, we first estimate human-scene interactions for a given scene and use those interactions to determine the frame weighting in the virtual agent motion. Through our novel contributions, we then utilize the scene geometry to optimize the motion of the virtual agent such that it interacts with the environment, matching both the interaction in the motion and the geometry of the scene based on our two interaction metrics: non-collision and contact.}
		\label{fig:method}
	\end{center}
\end{figure*}

\textbf{Video Synthesis.}
Our work is also related to the synthesis of videos containing human actions. Generative adversarial networks (GANs) \cite{goodfellowGenerativeAdversarialNets2014} and neural radiance fields (NeRFs) \cite{mildenhallNeRFRepresentingScenes2020} have contributed to a ever growing body of work attempting to generate videos of humans completing actions in arbitrary scenes. \cite{niemeyerGIRAFFERepresentingScenes2021, pengAnimatableNeuralRadiance2021, noguchiNeuralArticulatedRadiance2021, yangBANMoBuildingAnimatable2022, su2021anerf, yangViSERVideoSpecificSurface2021, guSTYLENERFSTYLEBASED3DAWARE2022, zhaoHumanNeRFEfficientlyGenerated2022, kwonNeuralHumanPerformer2021, Chan2021, noguchiUnsupervisedLearningEfficient2022} worked towards extending NeRF to scenes with multiple objects and arbitrary backgrounds, or to articulated objects like humans. Closest to our work is \cite{noguchiUnsupervisedLearningEfficient2022} which similarly takes a human action and attempts to place it on a background. Our approach addresses many of the shortcomings of \cite{noguchiUnsupervisedLearningEfficient2022}, enabling complex natural-looking background and natural-looking interactions between the animation and background.



\textbf{Human-Scene Interaction.}
Also closely related to our work is human-scene interaction (HSI), or scene affordance. Early efforts in HSI were purely geometric with Gleicher \cite{gleicherRetargettingMotionNew1998} and Kim et al. \cite{kimShape2PoseHumancentricShape2014} using contact constraints and automating 3D skeleton generation respectively. These and subsequent works exploited the importance of contact with the environment, with some accounting for the forces present in the environment \cite{kangEnvironmentAdaptiveContactPoses2014, leimerPoseSeatAutomated2020, grabnerWhatMakesChair2011}. As a major step forward, Gupta et al. \cite{gupta3DSceneGeometry2011} estimated the human poses "afforded" by the scene by predicting a 3D scene occupancy grid and computing the support of 3D skeletons inside of it.

\textbf{Data-Driven Methods.}
Many data-driven approaches have been proposed~\cite{tan2018, jiangHallucinatedHumansHidden2013, savvaSceneGrokInferringAction2014, zhangGenerating3DPeople2020, hassanPopulating3DScenes2021, liPuttingHumansScene2019}. \cite{jiangHallucinatedHumansHidden2013} and \cite{koppulaLearningHumanActivities2013} learn to estimate human poses and object affordances in an RGB-D 3D scene while \cite{wangGeometricPoseAffordance2021} learned to utilize the affordances of the scene to optimize pose estimation. Closest related to PACE, PSI \cite{zhangGenerating3DPeople2020}, PLACE \cite{zhangPLACEProximityLearning2020}, POSA \cite{hassanPopulating3DScenes2021}, and PAAK \cite{mullenjrPlacingHumanAnimations2022} populate scenes with SMPL-X \cite{pavlakosExpressiveBodyCapture2019} human bodies. Of these methods, POSA and PAAK are unique in that they are human-centric, finding estimated interactions in the human meshes and then pairing those interactions with any relevant affordances in the environment. In PACE, similar to PAAK, we leverage POSA's model and utilize the estimated contact and semantic information it provides about a given virtual agent motion sequence. Of all of these methods, PAAK is unique in its ability to place virtual agent motion instead of single, static human meshes. Our key difference from PAAK is our ability to tailor the virtual agent motions directly to the unique needs of a scene. PAAK is handcuffed by the geometry of the motion, resulting in many unnatural-looking or physically implausible placements. For example, a PAAK placement of a sitting motion is likely to result in either the buttocks or feet levitating above the surface of the scene due to a mismatch between the motion captured seat and the one in the scene. PACE, in contrast, alters the animation to fit the unique geometry of each scene exactly.