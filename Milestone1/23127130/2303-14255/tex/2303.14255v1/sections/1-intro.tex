\firstsection{Introduction}
\maketitle
Humans interact with their environment by making contact with objects and avoiding collisions with obstacles. For example, a human sitting at a desk is sitting in a chair. Their arms are likely resting on the surface of the desk, potentially even grasping the mouse and keyboard. When this human stands up from their desk, they have to navigate around the room, avoiding any objects within it, before reaching out to grasp the door knob and open the door. These types of interactions, defined by contact, dictate how humans move through their environment. In this paper, our goal is to take motion-captured humans and translate them into virtual humans that interact with virtual or real scenes in a natural looking way.

To motivate our problem, imagine a \textit{virtual} office. To make it feel plausible, virtual human agents or avatars have to be moving around the space and interacting with objects such as the chairs, desks, or keyboards with no penetrations or awkward motions. Current methods for this type of dense, cluttered space are based on animators using modeling or animation tools to generate such motion sequences that fit the environment. However, this can be very time consuming and it requires animators with considerable experience using these tools. Additionally, cost prohibits a large number of specialized animations or motions from being produced, resulting in the same animations being repeated in the virtual environment, reducing the sense of realism of the environment and the users engagement within it. In contrast, our goal is to leverage the vast quantities of publicly available human motion-captured data and place virtual agents personifying the motion-capture into the environment. 
%To fit the environment properly, we aim to tailor the motions to the scene such that they retain their essence and natural-look, while making the virtual space feel populated and realistic. 

Populating an environment with virtual humans that interact with the environment naturally is an important problem in virtual reality (VR), augmented reality (AR) \cite{hassanPopulating3DScenes2021}, game design \cite{martinHowMakeImmersive}, and human-robot interaction \cite{linVirtualRealityPlatform2016}. Specifically, a key problem in VR is generating plausible motion of human-like virtual agents. Prior methods are limited to humans moving in open or large spaces with few obstacles. However, real world environments, especially indoor ones, are filled with  obstacles and can be dense. Most prior work in VR in terms of dense or cluttered environments is mainly limited to object selection or manipulation \cite{narang2018simulating, shen2021simulating, bailenson2005independent}. Recent work in computer vision has enabled the creation of several real world datasets of indoor scenes containing dense objects and furniture\cite{Matterport3D, dai2017scannet, xiazamirhe2018gibsonenv, zillow, 3dfront, hassanResolving3DHuman2019}, such as Habitat-Matterport or Scan-Net, with hundreds of 3D scans of rooms with many objects. Such 3D models are increasingly used for VR and AR applications. However, we need to generate appropriate methods for virtual humans to interact with such environments by generating collision-free and plausible motion.  

Prior work in human-scene interaction, or scene affordances, attempts to place 3D human models into 3D scene scans such that the placement matches real human behavior. Most methods work with static, single-pose humans, \cite{kimShape2PoseHumancentricShape2014, hoSpatialRelationshipPreserving2010, zhangPLACEProximityLearning2020, hassanResolving3DHuman2019}. Recent methods enable the generalization of human model placement into any possible scene \cite{hassanPopulating3DScenes2021}, and extend this ability to existing short virtual agent motions \cite{mullenjrPlacingHumanAnimations2022}. Motion synthesis literature similarly tries to generate 3D human motion sequences with recent methods accounting for the scene geometry and affordances \cite{starkeNeuralStateMachine2019, lingCharacterControllersUsing2020, wangSynthesizingLongTerm3D2021}. 

\subsection{Main Contributions}
We leverage widely available indoor scene datasets and populate the complex, dense, indoor scenes with natural-looking virtual agents that move and interact with their environment. For the most natural-looking movements possible, we start with motion-captured human data and tailor it to the unique nature of each dense indoor scene. This is different from existing synthesis approaches that instead attempt to create motion sequences to fit the environment after learning from many valid sequences. When using motion-captured data, properly fitting a given environment as to make plausible interactions requires making changes to each pose in the motion sequence, each with several degrees of freedom, while maintaining the natural look and flow of the underlying motion sequence. We do this through a novel optimization method that takes the virtual agent motion sequence and 3D scene as inputs. From the scene and motion sequence, the optimization method outputs the location and orientation of the most natural placement of the virtual agent in the scene based on interaction metrics, and the modified pose and translation of the individual meshes in the virtual agent motion. While we did our evaluations with motion-captured data, we can utilize any virtual agent motion that can be represented as a time-series of 3D human meshes. We make no assumptions about the scene itself, but results are better if the objects in it are labeled. In practice, our approach shows the largest improvement over prior art with virtual agent motions longer than three seconds. Our main contributions are as follows:


\begin{enumerate}
\item We present PACE, a method to generate placements for motion-captured virtual agents into 3D dense scenes with motion optimization. Using existing methods, PACE first utilizes a deep model to estimate a virtual agent motion's potential scene interactions, before using this information alongside the motion geometry with another deep model to select important frames in the motion. Using the output of the deep models, we weight meshes in the motion such that the highest weight is attributed to meshes with maximum diversity and important scene interactions. This weighting allows us to find an optimal initial placement of the virtual agent before {our novel algorithms} optimize the agent's motion itself to better fit the unique constraints of a dense, cluttered scene. Our optimizer utilizes novel pose and motion losses alongside contact and semantic losses to alter the motion sequence to best fit the scene while maintaining the essence and natural look or the original motion.

\item We qualitatively show natural and physically plausible virtual agent placement. Through a perceptual user study we show that human raters prefer PACE 67.9\% of the time over an extension of POSA to the time dimension, 81.2\% of the time over a state-of-the-art motion synthesis method, and 59.3\% of the time over PAAK.

\item We quantitatively show a significant improvement over POSA\cite{hassanPopulating3DScenes2021} and PAAK\cite{mullenjrPlacingHumanAnimations2022} in the physical plausibility and interaction metrics\cite{zhangPLACEProximityLearning2020, hassanPopulating3DScenes2021}. These metrics include the non-collision and contact metrics which denote a lack of collisions and interactions with the scene respectively. Specifically, PACE improves upon PAAK by over 1.2\% in the non-collision metric and by over 18\% in the contact metric. 
%This shows that PACE generates more meaningful interactions with the environment while penetrating or colliding with it less than competing methods. This combination results in more natural-looking virtual agent-scene pairings. 

\item We have integrated PACE with a Microsoft HoloLens and evaluate the performance in real world scenes. This allows users to populate their local environment with moving virtual humans that interact with the environment in natural-looking and physically plausible ways. The populated virtual agents move at 30 frames per second in a plausible manner.

\item We release a dataset with tens of thousands of virtual agents placed into dense and cluttered indoor scenes. We also release software to render videos from a camera perspective of the users choosing or interact with the entire 3D scene-agent pair.
\end{enumerate}
 