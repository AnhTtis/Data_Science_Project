\section{Putting Moving Virtual Agents into Dense 3D Scenes}
In this work, we approach the following problem statement: \textit{Given a sequence of virtual agent motion and a 3D scene, create the most natural-looking and physically plausible agent-scene interaction.} Specifically, our goal is to take a given virtual agent and tailor it to a scene such that it properly fits the scene and that any interactions with the scene found in the agents motion (i.e. touching objects, sitting on a couch) match the affordances of the scene. When using cluttered or dense indoor environments, it becomes increasingly difficult to find an appropriate place to put the virtual agent. We use a data-driven method and make appropriate alterations to the virtual agents motion that result in significant improvements in the physical plausibility of agent-scene interactions and result in more natural-looking placements. % especially in the cluttered, dense environments we are interested in. 
At an intuitive level, you can imagine a motion that involves reaching for an object, but in the scene provided the object is further away than that modeled in the motion-captured environment. For the most natural-looking results, it is important that the motion is altered to reach towards the object present in the scene. Agent-scene pairings that do not alter the motion in this way will result in unnatural looking interactions that will be obvious to the user in an XR setting.

\subsection{Overview}

We present an overview of PACE in \hyperref[fig:method]{Figure 2}. A list of symbols frequently used in this work are shown in \hyperref[table:symbols]{Table 1}. Rarely used symbols are defined where they are used. In terms of notation, we define a time-series using an uppercase letter, while a lowercase letter denotes an individual frame in the time-series. 

The inputs to PACE are a set of virtual agent motions and a set of 3D scene meshes. PACE outputs a location and orientation for the virtual agent in the scene, as well as the altered pose and translation of each frame in the agents motion sequence. Each virtual agent motion sequence input, $V_b$, consists of a time series of human meshes, $v_b$, which are defined by the SMPL-X \cite{pavlakosExpressiveBodyCapture2019} parametric human body model. SMPL-X uses a human skeleton of 24 joints, all of which we can alter to tailor the motion to the scene. Each 3D scene input is represented as a static triangle mesh with vertex class labels denoting what type of object a given surface is. These class labels are helpful but not required. For the altered pose and translation outputs, PACE provides the 24 joint pose and xyz translation of each frame in the motion sequence. This can be redefined by SMPL-X to create a new motion sequence similar to the original input but properly fitting the environment.
%When dealing with real-world scenes, like those from the HoloLens, we may have to translate its output mesh into a 

We first take these meshes and estimate likely human scene interactions given the geometry of the mesh's surface. We then extract geometrically and semantically important frames and utilize active learning techniques to extract a diversity score for each frame. The combination of this information and the mesh geometry are combined to create the frame weights, $k$. Using the frame weights, we can find promising initial placements for the virtual agent in the scene before we optimize both the placement in the scene and the geometry of the agents motion. To optimize the motion of the virtual agent to best fit the scene, (as defined by the physical plausibility metrics we define below) we use the geometry and semantics of the scene and try to match it to the virtual agents motion, altering the specific poses in the agents motion as needed to create a match. This results in an agent-scene pairing that is as natural-looking as possible, with the motion closely matching the geometry of the scene and the interactions it contains accurately modeled.

\begin{table}[t] \label{table:symbols}
    \centering
    \begin{tabular}{| p{0.17\columnwidth} | p{0.73\columnwidth} | } 
        \hline
        \textbf{Symbols} & \textbf{Definitions}\\  
        \hline
        $V_b$ & A virtual agent motion, consisting of a time series of 3D human meshes \\ 
        \hline
        $v_b, v_p, v_\tau$ & A single 3D human mesh, an individual frame from $V_b$, the pose of that mesh, and the translation of that mesh relative to the first mesh \\ 
        \hline
        $f_c$, $f_s$ & Contact labels and Semantic labels, attributed to each vertex in a mesh, $v_b$  \\ 
        \hline
        $k$ & frame weights \\ 
        \hline
        $E_p$, $E_{alt}$ & The objective functions utilized when optimizing the motions placement and geometry \\
        \hline
    \end{tabular}
    \vspace{5.0px}
    \caption{List of symbols used and their definitions.}
    \vspace{-20.0px}
\end{table}

We choose to use an optimization method for altering the agents motion with an end-to-end deep learning model because it will \textit{always} find the ideal placement and motion geometry for the scene. In contrast to this, a deep learning model would have to learn to generalize over all possible motions and scenes, not likely finding ideal placements or motion geometry for any of them. Additionally, data does not currently exist of humans moving and interacting with dense or cluttered 3D scenes. Our optimization method also allows for flexibility to take into account other interaction metrics. For example, if not penetrating the scene is a hard constraint, users can easily alter the weight of the penetration loss and the optimization can adapt accordingly.

\subsection{Human-Scene Interaction Estimation}
To place a virtual agent motion into a scene such that any interactions present in the motion are preserved, we must first determine those interactions. Following the lead of PAAK \cite{mullenjrPlacingHumanAnimations2022}, we directly implement the POSA pretrained \cite{hassanPopulating3DScenes2021} conditional variational autoencoder (cVAE) \cite{sohnLearningStructuredOutput2015} and feed each frame of the virtual agent motion into it individually. The POSA cVAE, $f$, generates an egocentric feature map for each vertex in each mesh, $v_b$, in the virtual agent motion, $V_b$. This feature map consists of a contact label, $f_c$, and a set of semantic labels, $f_s$, which intuitively denotes a) whether that vertex should be in contact with the scene, and b), what it should ideally be in contact with, respectively. For example, if the mesh is of a person grasping an object, the vertices on the hand should have a high contact probability and highlight the object semantic label. In contrast, vertices on the other hand or on the agents back should have a very low contact probability. We represent POSA as the function $f$ in \hyperref[eq:POSA]{Equation 1} below.

\begin{equation}\label{eq:POSA}
    f : (v_b) \rightarrow [f_c,f_s]
\end{equation}

PACE  enables the use of any length of virtual agent motion in contrast to PAAK which only tested with motions 60 frames (or 2 seconds) in length. We specifically tested conducted our quantitative tests with motions from 4-15 seconds in length with 60-450 total frames. Using longer motions is only limited by the scene size, with animations longer than 15 seconds rarely having a valid fit location in the single-room, small scenes we utilized when testing PACE. We feed all of the meshes of a given virtual agent motion individually into $f$ to extract the full set of contact labels, $F_c$, and semantic labels, $F_s$, for use as inputs to our optimization methods and frame weight extractions. 


\begin{figure*}[ht]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{media/BlockDiagram.pdf}
		\caption{An overview of our novel optimization process. We iteratively modify the motion of the virtual agent while trying to find the optimal location for it in the scene. This results in a motion that better fits the scene it is being placed in, for more natural-looking and physically plausible virtual agent motion. In this example, note the green circle highlighting changes to the arm geometry so it does not collide with the table in the final placement. In the initial motion, the arm penetrated the table when the person sat down.}
		\label{fig:opt}
	\end{center}
\end{figure*}

\subsection{Frame Weighting}
%Similar to how we utilize POSA for extracting the contact and semantic labels for each mesh in the virtual agent motion,
We implement PAAK to extract the a frame weights for each frame in the virtual agent motion. Intuitively, finding these frame weights allows our optimization methods to focus on the most important pieces of the motion when placing it into the scene. For example, you can imagine a virtual agent which stands still for a long period before sitting down in a chair. The sitting motion would only be a few frames of the overall motion and it could be easy for the optimization process to miss these frames and instead focus on optimizing the long standing still period. However, to create the most natural-looking placement, it is extremely important that when the sitting motion occurs, it ends in a chair, while the standing period of the motion can happen almost anywhere. PAAK strives to find the frames that are most important for a natural-looking placement in the scene, $K$. PAAK consists mainly of a deep model, $g$, trained to estimate a geometric frame weighting equation. PAAK then utilizes active learning techniques to give each frame in the motion a diversity score. The combination of the model output approximating geometric and semantic cues, and the diversity score, are used to create a final frame weighting for the virtual agent motion with each frame in the motion given a weight. \hyperref[eq:al]{Equation 2} and \hyperref[eq:model]{Equation 3} below show the PAAK active frame weighting framework.


\begin{equation}
\label{eq:al}
    K = \lambda_g * \hat{K_g} + \lambda_b * W_d
\end{equation}
\begin{equation}\label{eq:model}
    g : (V_b, F_c, F_s) \rightarrow \hat{K_g}
\end{equation} % I used a hat here because its not actually k_g. Not sure how to reflect this in the text

Specifically, the PAAK deep model, $g$, maps from the virtual agent motion geometry, contact labels, and semantic labels to an estimation of a geometric frame weight definition that emphasizes motion and unique semantics. To get the final frame weights, the output of the model alongside the diversity score obtained from the models gradients are summed.

For datasets with frame rates higher than 30 fps, such as the AMASS \cite{AMASS:ICCV:2019} CMU MoCAP \cite{cmuWEB} subset with a 120 fps frame rate, we modified the PAAK algorithm to sample down to an average frame rate of 30 fps. Specifically, we utilize their deep model and extract the top 25\% of the frames by frame weight and discarded the remaining frames. To make sure we did not create large gaps in motion, we created a frame rate floor of 10 fps and retained additional frames to make sure we did not drop below this amount. Sampling down to 30 fps on average allowed our optimizer to work quicker as the number of meshes it has to calculate and the number of parameters it has to optimize are greatly decreased.

\subsection{Scene Placement and Motion Optimization}

As our main novel algorithmic contribution, we place the agent into the scene such that it makes sense in context and complete any alterations to the motion needed to better fit the scene. For the placement itself, we mainly key in on the available semantic, contact, and geometric information of the virtual agent motion as weighted by the frame weights. However, we make any alterations to the motion concurrently to finding the placement as the alterations to the motion may make a given placement significantly better. Specifically, we optimize two objective functions, $E_p$ and $E_{alt}$ where $E_p$ finds the optimal location, $\tau$, and rotation, $\theta$, in the scene for the virtual agent and where $E_{alt}$ finds the optimum alterations to the agents motion to maximize the viability of the placement without generating too much movement as to hurt the perceived realism of the motion. As in \cite{mullenjrPlacingHumanAnimations2022} and \cite{hassanPopulating3DScenes2021}, $E_p$ minimizes the sum of an affordance loss, $\mathcal{L}_{afford}$ and a penetration loss, $\mathcal{L}_{pen}$, calculated for each frame weighted by the frame weights, $k_a$.

\begin{equation}
    E_p(\tau, \theta) = \sum^{|k|}_{i=0}{k_{i}*[\mathcal{L}_{afford, i}} + \mathcal{L}_{pen, i}]
\end{equation}

Intuitively, $\mathcal{L}_{afford}$ is minimized when the distance to the scene is small for vertices with a high probability of contact using $f_c$ and when the semantic label $f_s$ matches the semantics of the object vertices are in contact with. $\mathcal{L}_{pen}$ heavily penalizes placements that result in the agents motion penetrating the scene as that is a key factor in physical plausibility. The frame weights, $k$, allows the optimizer to accurately find a placement that focuses on the key interactions in the virtual agent motion.

For the alteration objective, $E_{alt}$, two more loss functions are added with the intention of maintaining consistency through the virtual agent motion. $E_{alt}$ minimizes the sum of a pose loss, $\mathcal{L}_{pose}$, and a motion loss $\mathcal{L}_{mot}$. $\mathcal{L}_{pose}$ is minimized when the pose of each individual mesh in the motion is close to the original pose, thus minimizing unrealistic poses. $\mathcal{L}_{mot}$ is minimized when the motion between two frames is the same as from the original virtual agent motion, thus minimizing the probability of overly large single motions between frames. The weighting for these two loss terms are set such that an individually large difference in pose with a number of small differences in motion is less costly than large changes in motion. This creates an effectively connected nature where the optimizer favors a smooth motion towards the goal pose over a sudden change in pose. The optimizer for $E_{alt}$ operates across the translation of each mesh in the virtual agent motion and the pose of each skeleton in each frame of the virtual agent motion. \hyperref[eq:Lpose]{Equation 6} shows the calculation pose loss, taking effectively the squared error of the pose for each frame. Similarly, \hyperref[eq:Lmot]{Equation 7} shows the calculation of the motion loss, which is the squared error of the difference between each sequential pose and translation in the motion sequence. We added a discount factor, $\lambda_{\tau}$ for the translation component of the loss as we found we achieved better results when emphasizing it less, but worse results if we removed it entirely. This process can be seen visually in \hyperref[fig:opt]{Figure 3.}


\begin{equation}\label{eq:Ealt}
    E_{alt}(V_\tau, V_p) = \sum^{|k|}_{i=0}{k_{i}*[\mathcal{L}_{pose, i}} + \lambda_{mot} * \mathcal{L}_{mot, i}]
\end{equation}
\begin{equation}\label{eq:Lpose}
    \mathcal{L}_{pose} = \sum^{|k|}_{i=0}{(v_{p,i} - \hat{v_{p,i}})^2}
\end{equation}
\begin{multline}\label{eq:Lmot}
        \mathcal{L}_{mot} = \sum^{|k|}_{i=0}{(diff(v_p)_i - diff(\hat{v_p})_i)^2} + \\ \lambda_{\tau}[\sum^{|k|}_{i=0}{(diff(v_\tau)_i - diff(\hat{v_\tau})_i)^2}]
\end{multline}

\begin{figure*}[ht]
	\begin{center}
		\includegraphics[width=\linewidth]{media/qual_comp_vr.pdf}
		\caption{Comparisons on placing the same virtual agent into the same scene across PACE, POSA-T, and PAAK. Note that two angles of each placement are provided. For the first placement, PACE is the only one that maximizes the interaction with the environment by tailoring the agents motion to maneuver it around the chair to the right of the table, placing the hand on it as a guide as a real person might. For POSA-T, the placement not only puts the agent perpendicular to the chair, but then has it wonder off the scene where its interactions make less sense. PAAK is in a middle state where it maintains contact with the environment but penetrates the chair and table and floats above the ground surface. For placement 2, PACE shows the most probable interaction given the virtual agent motion provided. It interacts with the chair and the bed, bracing its movement to the chair with the hand. It does however result in an awkward yet still valid seating position. For POSA-T, the second seating position is completely ignored, which makes sense due to its lack of frame weighting. PAAK finds a valid placement for the virtual agent but the hand placements are awkward and do not contact the scene in the way a real human might.}
		\label{fig:qual}
	\end{center}
\end{figure*}

$E_{alt}$ when used in combination with $E_p$ is especially powerful as the optimizer can fit the virtual agent motion to the scene while retaining the essence and natural look of the real motion sequence. For example, in a scenario where the virtual agent is sitting in a chair that is smaller than that which was recorded in the original motion capture, the feet of the agent would normally go through the floor. However, when using $E_{alt}$ and $E_p$ together, the actual poses of the agent can change to make sure the feet do not penetrate the floor, while maintaining a smooth and natural-looking sitting motion. Without $E_{alt}$ there are many scenarios where the virtual agent motions do not closely fit the needs of the scene resulting in penetrations, floating feet, or inaccurate affordances. These errors result in physically implausible and unnatural-looking placements for the virtual agents that quickly break users immersion and satisfaction.

We specifically use a Pytorch implementation of L-BFGS with Strong Wolfe line search as our optimizer following \cite{hassanPopulating3DScenes2021} and \cite{mullenjrPlacingHumanAnimations2022}. A learning rate of 1 is used and the optimization executes for 10 steps.

\subsection{Physical Plausibility Metrics}

Quantitatively measuring the physical plausibilty of interactions generated by PACE is important for validating its results. For this, we use the non-collision and contact metrics originally defined by Zhang et al. \cite{zhangGenerating3DPeople2020} and used by Hassan et al.\cite{hassanPopulating3DScenes2021}, and Mullen et al. \cite{mullenjrPlacingHumanAnimations2022}. Each metric is defined on the human mesh level. Given the body mesh, $v_b$, the scene mesh, $m_s$, and a scene signed distance field (SDF) that stores distances for each voxel, the non-collision score is computed as the ratio of body mesh vertices with positive SDF values divided by the total number of vertices. In contrast, the contact score is calculated as 1 if at least one vertex of $v_b$ has a non-positive distance value to the scene.