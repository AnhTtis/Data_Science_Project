\vspace{-0.3em}
\section{Related Work}
\vspace{-0.3em}
\noindent\textbf{Self-Supervised Video Representation Learning.} 
The success of contrastive learning in images~\cite{moco, chen2020simple,grill2020bootstrap,moskalev2022contrasting} inspired many video contrastive works~\cite{cvrl,videomoco-pan2021videomoco,rspnet-chen2020RSPNet,huang2021ascnet,srtc,pretext-contrast}. Alongside spatial invariances, these works learn invariances to temporal crops~\cite{videomoco-pan2021videomoco,cvrl,svt} and video speed~\cite{rspnet-chen2020RSPNet,huang2021ascnet,srtc}. Some diverge from temporal invariances and encourage equivariance~\cite{gdt-patrick2020multimodal,dave2022tclr} to learn finer temporal representations. 
For instance, TCLR~\cite{dave2022tclr} enforces within-instance temporal feature variation, while TE~\cite{jenni2021time_eqv} learns equivariance to temporal crops and speed with contrastive learning. Alternatively, many works learn to predict temporal transformations such as clip order~\cite{misra2016shuffle,xu2019self,sorting,odd}, speed~\cite{benaim2020speednet, cho2020self,prp} and their combinations~\cite{vcp,simon}. These self-supervised temporal representations are effective for classifying and retrieving coarse-grained actions but are challenged by downstream settings with subtle motions \cite{thoker2022severe, schiappa2022self}.
Other works utilize the multimodal nature of videos~\cite{alwassel_2020_xdc,selavi,gdt-patrick2020multimodal,avid-cma-morgado2021audio,coclr,motion_fit,mac2019learning} and learn similarity with audio~\cite{selavi,alwassel_2020_xdc,avid-cma-morgado2021audio} and optical flow~\cite{coclr,motion_fit, mscl, xiao2022maclr}.  
We contrast motions of synthetic tubelets to learn a video representation from only RGB data that can generalize to tasks requiring fine-grained motion understanding.


Other self-supervised works learn from the spatiotemporal dynamics of video.
Both BE~\cite{background_removing} and FAME~\cite{fame} remove background bias by adding static frames~\cite{background_removing} or replacing the background~\cite{fame} in positive pairs. %Similarly, FAME~\cite{fame} merges the foreground from one video with the background from another video to generate positive pairs for contrastive learning to enhance motion awareness in the learned video representations. 
Several works instead use masked autoencoding to learn video representations~\cite{tong2022videomae, feichtenhofer2022masked}.
However, these works are all limited to the motions present in the pretraining dataset. We prefer to be less dataset-dependent and generate synthetic motion tubelets for contrastive learning, which also offers a considerable data-efficiency benefit.
%
CtP~\cite{ctp-wang2021unsupervised} and MoSI~\cite{motion_static} both aim to predict motions in pretraining. CtP~\cite{ctp-wang2021unsupervised} learns to track image patches in video clips while MoSI~\cite{motion_static} learns to predict the speed and direction of added pseudo-motions. We take inspiration from these works and contrast synthetic motions from tubelets which allows us to learn generalizable and data-efficient representations.
%
%Different from CtP and MoSI we design a tubelet based contrastive framework for learning similarities and dissimilarities between simulated motion patterns to enhance motion-focused video representation learning.
%Both BE and Fame focus on removing spatial basis in positive pairs but are limited by the motions patterns present in original video clips, while we simulate a variety of synthetic motion patterns for contrastive learning in the form of tubelet motions and tubelet transformations which enhances downstream generalization and data efficency.
%
\begin{figure*}[t!]
\centering
\vspace{-1em}
\includegraphics[width=0.95\linewidth]{figures/framework-7.pdf}
\vspace{-1em}
\caption{\textbf{Tubelet-Contrastive Learning.} We sample two clips ($v_1, v_2$) from different videos and randomly crop an image patch from $v_1$. We generate a tubelet by replicating the patch in time and add motion through a sequence of target locations for the patch. 
We then add complexity to these motions by applying transformations, such as rotation, to the tubelet. The tubelet is overlaid $\odot$ onto both  clips to form a positive tubelet pair ($\hat{v}_{1}, \hat{v}_{2}$). We learn similarities between clips with the same tubelets (positive pairs) and dissimilarities between clips with different tubelets (negatives) using a contrastive loss.}
\label{fig:framework}
\vspace{-1.2em}
\end{figure*}


\noindent\textbf{Supervised Fine-Grained Motion Learning. } 
%  add MLFO and MSCl
%- introduce coarse grained vs fine-grained
%- fine-grained datasets
%- fine-grained needs motion
%- approaches for fine-grained include NN blocks for motion, decoupling spatial and motion features, and aggregating features at different temporal resolutions. Also optical flow.
%- tasks beyond action recognition that aim for motions, repetition counting, recognizing adverbs, others?
%- these are supervised, we aim for self-supervised representation learnt from coarse-grained and generalizable to fine-grained, new domains.
%
While self-supervised works have mainly focused on learning representations to distinguish coarse-grained actions, much progress has been made in supervised learning of motions. 
Approaches distinguish actions by motion-focused neural network blocks~\cite{kwon2021learning, mac2019learning, lin2019tsm, kim2021relational}, decoupling motion from appearance~\cite{rahmani2022dynamic, sun2022fine}, aggregating multiple temporal scales~\cite{yang2020temporal, ni2014multiple, feichtenhofer2019slowfast}, and sparse coding to obtain a mid-level motion representation~\cite{mavroudi2018end, piergiovanni2018fine, shao2020intra}. 
%
Other works exploit skeleton data~\cite{duan2022revisiting, hong2021video} or optical flow~\cite{simonyan2014two, feichtenhofer2016convolutional}. Alternatively, several works identify motion differences within an action class, by repetition counting~\cite{hu2022transrac, ucfrep-zhang2020context, zhang2021repetitive}, recognizing adverbs~\cite{doughty2020action, doughty2022you} or querying for action attributes~\cite{tqn}.  
%
Different from all these works, we learn a motion-sensitive video representation with self-supervision. We do so by relying on just coarse-grained video data in pretraining and demonstrate downstream generalization to fine-grained actions.



 \noindent\textbf{Tubelets. } 
 Jain \etal defined tubelets as class-agnostic sequences of bounding boxes over time~\cite{jain2014action}.  Tubelets can represent the movement of people and objects and are commonly used for object detection in videos\cite{kang2017object,kang2017t,feichtenhofer2017detect}, spatiotemporal action localization~\cite{kalogeiton2017action,yang2019step,li2020actions,jain2014action,hou2017tube,zhao2022tuber} and video relation detection~\cite{ChenRelationICCV21}. Initially, tubelets were obtained by supervoxel groupings and dense trajectories \cite{jain2014action,GemertBMVC15} and later from 2D CNNs~\cite{kalogeiton2017action,li2020actions}, 3D CNNs~\cite{hou2017tube,yang2019step} and transformers~\cite{zhao2022tuber}. We introduce (synthetic) tubelets of pseudo-objects for contrastive video self-supervised learning.
