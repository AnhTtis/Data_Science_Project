\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
This paper presents a contrastive learning method to learn motion-focused video representations in a self-supervised manner. Our model adds synthetic tubelets to videos so that the only similarities between positive pairs are the spatiotemporal dynamics of the tubelets. By altering the motions of these tubelets and applying transformations we can simulate motions not present in the pretraining data. Experiments show that our proposed method is data-efficient and more generalizable to new domains and fine-grained actions than prior self-supervised methods. 

\vspace{0.25em}
\noindent\textbf{Acknowledgements}. This work is part of the research programme Perspectief EDL with project number P16-25 project 3, which is financed by the Dutch Research Council (NWO) domain Applied and Engineering/Sciences (TTW). We thank Piyush Bagad for help with experiments and Artem Moskalev for useful discussions.