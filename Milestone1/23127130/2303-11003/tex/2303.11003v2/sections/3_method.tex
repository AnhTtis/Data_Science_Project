
\vspace{-0.3em}
\section{Tubelet Contrast}
\vspace{-0.3em}
We aim to learn motion-focused video representations from RGB video data with self-supervision. After revisiting temporal contrastive learning, we propose tubelet-contrastive learning to reduce the spatial focus of video representations and instead learn similarities between spatio-temporal tubelet dynamics (Section~\ref{section:tbcl}). We encourage our representation to be motion-focused by simulating a variety of tubelet motions (Section~\ref{section:motions}). To further improve data efficiency and generalizability, we add complexity and variety to the motions through tubelet transformations (Section~\ref{section:transformations}). Figure~\ref{fig:framework} shows an overview of our approach.

%\subsection{Video-Contrastive Learning}
%\label{section:tcl}
\noindent\textbf{Temporal Contrastive Learning.} Temporal contrastive learning learns feature representations via instance discrimination \cite{infonce}. This is achieved by maximizing the similarity between augmented clips from the same video (positive pairs) and minimizing the similarity between clips from different videos (negatives). Concretely given a set of videos $V$, the positive pairs $(v,v')$ are obtained by sampling different temporal crops of the same video~\cite{videomoco-pan2021videomoco,rspnet-chen2020RSPNet} and applying spatial augmentations such as cropping and color jittering. Clips sampled from other videos in the training set act as negatives. The extracted clips are passed through a video encoder and projected on a representation space by a non-linear projection head to obtain clip embeddings $(Z_{v},Z_{v'})$. The noise contrastive estimation loss InfoNCE \cite{infonce} is used for the optimization: 
%%%%%%% %%%%%%%%%%%
\vspace{-0.7em}
\begin{equation}
\resizebox{0.9\linewidth}{!}{
$\mathcal{L}_{contrast}{(v, v')} = -\log \frac{\displaystyle h(Z_{v},Z_{v'})}
{ \displaystyle h(Z_{v},Z_{v'})+\displaystyle \sum_{Z_n \sim \mathcal{N}} h(Z_{v},Z_{n})}$
}
\label{eq:tcl}
\end{equation}
%
where $h(Z_{v},Z_{v'}) {=} \mathrm{exp}(Z_{v}\cdot Z_{v'}/\tau)$, $\tau$ is the temperature parameter and $\mathcal{N}$ is a set of negative clip embeddings. %\hd{Add sentence on why this loss/way of contrasting is insufficient for us.}

\vspace{-0.3em}
\subsection{{Tubelet-Contrastive Learning}}
\vspace{-0.3em}
\label{section:tbcl}
Different from existing video contrastive self-supervised methods, we explicitly aim to learn motion-focused video representations while relying only on RGB data. To achieve this we propose to learn similarities between simulated tubelets.  Concretely, we first generate tubelets in the form of moving patches which are then overlaid onto two different video clips to generate positive pairs that have a high motion similarity and a low spatial similarity. Such positive pairs are then employed to learn video representations via instance discrimination, allowing us to learn more generalizable and motion-sensitive video representations.


\noindent\textbf{Tubelet Generation.} 
We define a tubelet as a sequence of object locations in each frame of a video clip. Let's assume an object $p$ of size ${H}'\times{W}'$ moving in a video clip $v$ of length $T$. Then the tubelet is defined as follows:
\vspace{-0.3em}
\begin{equation}
\mathrm{Tubelet}_{p} = [(x^1,y^1) ,..,(x^T,y^T)],
\label{eq:tubelet_def}
\vspace{-0.5em}
\end{equation}
 where $(x^i,y^i)$  is the center coordinate of the object $p$ in frame $i$ of clip $v$. % such that $0<x^i<W$ and $<0<y^i<H$, $W$ and $H$ is is width and height of the clip $v$. 
 For this work, a random image patch of size ${H}'\times{W}'$ acts as a pseudo-object overlaid on a video clip to form a tubelet. To generate the tubelet we first make the object appear static, \ie, $x^1{=}x^2{=}...{=}x^T$ and $y^1{=}y^2 = ...{=}y^T$, and explain how we add motion in Section~\ref{section:motions}. % we explain how we define the tubelet to give the pseudo-object motion.
 
\noindent\textbf{Tubelet-Contrastive Pairs.} To create contrastive tubelet pairs, we first randomly sample clips $v_1$ and $v_2$ of size $H{\times}W$ and length $T$ from two different videos in $V$. From $v_1$ we randomly crop an image patch $p$ of size ${H}' \times {W}'$.  such that ${H}' \ll H$  and  ${W}' \ll W$.  
From the patch $p$, we  construct a tubelet $\mathrm{Tubelet}_{p}$ as in Eq.~\eqref{eq:tubelet_def}.
Then, we overlay the generated tubelet $\mathrm{Tubelet}_{p}$ onto both $v_1$ and $v_2$ to create two modified video clips $\hat{v}_{1}$ and $\hat{v}_{2}$: %\cs{There is a lot of white space around equations, probably caused by split command, pls fix.}
%\vspace{-0.6em}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
\begin{align}
 &\hat{v}_{1} = v_{1} \odot \mathrm{Tubelet}_{p} \quad
 &\hat{v}_{2} = v_{2} \odot \mathrm{Tubelet}_{p},
\label{eq:overlay_single_tubelet}
\end{align}
where $\odot$ refers to pasting patch $p$ in each video frame at locations determined by $\mathrm{Tubelet}_{p}$. Eq.~\eqref{eq:overlay_single_tubelet} can be extended for a set of $M$ tubelets $\{\mathrm{Tubelet}_{p_1},...,\mathrm{Tubelet}_{p_M}\}$ from $M$ patches randomly cropped from $v_1$ as:
\begin{align}
 \hat{v}_{1} = v_{1} \odot \{\mathrm{Tubelet}_{p_1},...,\mathrm{Tubelet}_{p_M}\} \nonumber \\
 \hat{v}_{2} = v_{2} \odot \{\mathrm{Tubelet}_{p_1},...,\mathrm{Tubelet}_{p_M}\}.
\label{eq:overlay_M_tubelets}
\end{align}
As a result, $\hat{v}_{1}$ and $\hat{v}_{2}$ share the spatiotemporal dynamics of the moving patches in the form of tubelets and have low spatial bias since the two clips come from different videos.  %\cs{Figure~\ref{fig:clips} shows examples of $v_{1}'$ and $v_{2}'$.}
%
Finally, we adapt the contrastive loss from Eq.~\eqref{eq:tcl} and apply $\mathcal{L}_{contrast}(\hat{v}_1, \hat{v}_2)$. %from Equation \eqref{eq:tcl} to explicitly contrast tubelet pairs as follows: \cs{Is this equation really needed?}
% \begin{equation}
%\mathcal{L}_{\cs{TBCL}}{(V_1,V_2)} = -\log \frac{\displaystyle h(Z_{v_{1}'}, Z_{v_{2}'})}
%{ \displaystyle h(Z_{v_{1}'}, Z_{v_{2}'}) + \displaystyle\sum_{Z_{v_{n}'} \sim \mathcal{N}} h(Z_{v_{1}'}, Z_{v_{n}'}) }
%\label{eq:tubelet_contrastive_loss}
%\end{equation}
%where  $Z_{v_{1}'},Z_{v_{2}'}$ are the embeddings of the positive tubelet pair ($v_{1}',v_{2}'$),  $h(Z_{v_{1}'},Z_{v_{2}'}) {=} \mathrm{exp}(Z_{v_{1}'}\cdot Z_{v_{2}'}/\tau)$, $\tau$ is the temperature softening hyper-parameter and 
Here the set of negatives $\mathcal{N}$ contains videos with different tubelets. Since the only similarity in positive pairs is the tubelets, the network must rely on temporal cues causing a motion-focused video representation. 

\vspace{-0.3em}
\subsection{Tubelet Motion}
\vspace{-0.3em}
\label{section:motions}
%As discussed in the previous section, our aim is to learn the same feature representations for video clips that have the same motion tubelets and different feature representations for the clips with different motion tubelets. Thus, in order 
%To learn good video features, which are able to represent a variety of motion patterns, we need to simulate a variety of tubelet trajectories for learning. 
To learn motion-focused video representations, we need to give our tubelets motion variety. Here, we discuss how to simulate motions by generating different patch movements in the tubelets. Recall, Eq.~\eqref{eq:tubelet_def} defines a tubelet by image patch $p$ and its center coordinate in each video frame. We consider two types of tubelet motion: linear and non-linear. 

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/motions4.pdf}
\vspace{-2.3em}
\caption{ \textbf{Tubelet Motion.} Examples for \textit{Linear} (left) and \textit{Non-Linear} (right). Non-linear motions enable the simulation of a larger variety of motion patterns to learn from. 
}
\label{fig:traj}
\vspace{-1.3em}
\end{figure}

\noindent\textbf{Linear Motion.} %First, we aim to simulate motions where the patches move along a linear path.  
We randomly sample the center locations for the patch in $K$ keyframes: the first frame ($i{=}1$), the last frame ($i{=}T$),  and $K{-}2$ randomly selected frames. These patch locations are sampled from uniform distributions $x\in[0,W]$ and $y\in[0,H]$, where $W$ and $H$ are the video width and height. 
%To ensure smoothness, we constrain the difference between the center locations in neighboring keyframes to be less than $\Delta$ pixels.
%as $(x^1,y^1)$, $(x^{k1},y^{k1}),..,(x^{K-2},y^{K-2})$ and $(x^T,y^T)$.
Patch locations for the remaining frames $i\notin K$ are then linearly interpolated between keyframes so we obtain the following linear motion definition:
%\begin{equation}
\begin{align}
\label{eq:linear_motion}
%\quad 
&  \mathrm{Tubelet}^{\mathrm{Lin}} = [(x^1,y^1),(x^2,y^2),...,(x^T,y^T)], \text{ s.t.} \\ 
& (x^i,y^i) = 
\begin{cases}
(\mathcal{U}(0,W),\mathcal{U}(0,H)), & \text{if } i \in K\\
\mathrm{Interp}((x^{k},y^{k}),(x^{k+1},y^{k+1})), & \text{otherwise} \nonumber
\end{cases}
\end{align}
%The center locations for the remaining frames $i\neq(1,m,T)$ are then linearly interpolated between the neighboring keyframes.  
where $\mathcal{U}$ is a function for uniform sampling, $k$ and $k{+}1$ are the neighboring keyframes to frame $i$ and $\mathrm{Interp}$ gives a linear interpolation between keyframes. %and $(x_{i}^{k},y_{i}^{k}),(x_{i}^{k+1},y_{i}^{k+1})$ are the center coordinates in these keyframes. 
To ensure smoothness, we constrain the difference between the center locations in neighboring keyframes to be less than $\Delta$ pixels.
This formulation results in tubelet motions where patches follow linear paths across the video frames. %As a result, we generate motion patterns for learning that simulate objects with linear movements in real videos. 
%where $\Delta i$ is the index difference between the two keyframes. 
%\cs{I do not like this visualization, pls update as discussed}
The left of Figure~\ref{fig:traj} shows examples of  such linear tubelet motions. 



\noindent \textbf{Non-Linear Motion.} 
Linear motions are simple and limit the variety of motion patterns that can be generated.  Next, we simulate motions where patches move along more complex non-linear paths, to better emulate motions in real videos. %In contrast to linear movements, such paths generate a variety of more realistic motion patterns to learn from. 
%
We create non-linear motions by first sampling $N$ 2D coordinates ($N \gg T$) uniformly from $x \in [0,W]$ and $y \in [0,H]$.  Then, we apply a $1D$ Gaussian filter along $x$ and $y$ axes to generate a random smooth nonlinear path as:%\cs{sloppy equation, pls fix}
\vspace{-1em}
\begin{align}
\label{eq:nonlinear_motion}
\quad 
\mathrm{Tubelet}^{\mathrm{NonLin}} = &[(g(x^1),g(y^1)),...,(g(x^N),g(y^N))] \nonumber \\ 
\text{s.t. } \quad
&g(z) = \frac{1}{\sqrt[]{2 \pi } \sigma} e^{- {z}^{2}/{2 \sigma ^{2}}} %\ \text{where} \ x^i\in[0,W]\nonumber \\
%&y^i \in [0,H], \hspace{5pt} 
%&g(y^i) = \frac{1}{\sqrt[]{2 \pi } \sigma} e^{- {y^i}^{2}/{2 \sigma ^{2}}},
%\text{where} \ y^i\in[0,H] 
\end{align}
where $\sigma$ is the smoothing factor for the gaussian kernels. %and $W,H$ is the width and height of the video clips. 
Note the importance of sampling $N\gg T$ points to ensure a non-linear path. If $N$ is too small then the  path becomes linear after gaussian smoothing. %We also ensure that tubelets are sampled at different XY positions in the video clip and have varying lengths of pixel movement along the time axis.
%random scaling and translation is applied on the $\mathrm{Tubelet}^{\mathrm{NonLin}}$ generated from Equation \eqref{eq:nonlinear_motion} as:
%Moreover, to ensure that tubelets are sampled at different XY positions in the video clip and have varying lengths of pixel movement, random scaling and translation is applied on the $\mathrm{Tubelet}^{\mathrm{NonLin}}$ generated from Equation \eqref{eq:nonlinear_motion} as:
%\begin{equation}
%\mathrm{Tubelet}^{\mathrm{Scale}}_\mathrm{{Translate}} =  a *  \mathrm{Tubelet}^{\mathrm{NonLin}} + c, \\ 
%\label{eq:scale_transaltion}
%\end{equation}
%
 %where $a{=}(S_x,S_y)$ are the scaling factors and $c{=}(T_x,T_y)$  are translation factors along $X$ and $Y$ coordinates and are randomly sampled such that $0 < S_x < W$, $0 < S_y < H$, $0 < T_x < W$ and $0 < T_y < H$.
 %Note that the same scaling and translation  is applied to each $2D$ point in $\mathrm{Tubelet}^{\mathrm{NonLin}}$. 
 We downsample the resulting non-linear tubelet in Eq.~\eqref{eq:nonlinear_motion} from $N$ to $T$ coordinates resulting in the locations for patch $p$ in the $T$ frames.
The right of Figure~\ref{fig:traj}
shows examples of non-linear tubelet motions.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/transformations4.pdf}
\vspace{-2em}
\caption{\textbf{Tubelet Transformation.} Examples for
\textit{Scale} (left), \textit{Rotation} (middle), and \textit{Shear} (right). The patch is transformed as it moves through the tubelet.  
}
\label{fig:transformations}
\vspace{-1em}
\end{figure}



\begin{table*}[t]
    \centering
    %\setlength{\tabcolsep}{4pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llllrrrl}
    \toprule
    \textbf{Evaluation Factor} & \textbf{Experiment} & \textbf{Dataset} & \textbf{Task} & \textbf{\#Classes} & \textbf{\#Finetuning} & \textbf{\#Testing} & \textbf{Eval Metric}\\
    \midrule

    \multirow{2}{*}{\textbf{Standard}} & UCF101 & UCF 101~\cite{UCF-101-arxiv} & Action Recognition & 101 & 9,537 & 3,783 & Top-1 Accuracy\\
 & HMDB51 & HMDB 51~\cite{HMDB-51-ICCV} & Action Recognition & 51 & 3,570 & 1,530 & Top-1 Accuracy\\
    \midrule
    \multirow{2}{*}{\textbf{Domain Shift}} & SSv2 & Something-Something~\cite{SS-v2-arxiv} & Action Recognition & 174 & 168,913 &  24,777 & Top-1 Accuracy\\
     & Gym99 & FineGym~\cite{Gym-99-arxiv} & Action Recognition & 99 & 20,484 & 8,521 & Top-1 Accuracy\\
    \midrule
    \multirow{2}{*}{\textbf{Sample Efficiency}}& UCF ($10^3$) & UCF 101~\cite{UCF-101-arxiv} & Action Recognition & 101 & 1,000 & 3,783 & Top-1 Accuracy\\
     & Gym ($10^3$) & FineGym~\cite{Gym-99-arxiv} & Action Recognition & 99 & 1,000 & 8,521 & Top-1 Accuracy\\
    \midrule
    \multirow{2}{*}{\textbf{Action Granularity}} & FX-S1 & FineGym~\cite{Gym-99-arxiv} & Action Recognition & 11 & 1,882 & 777 & Mean Class Acc\\
     & UB-S1 & FineGym~\cite{Gym-99-arxiv} & Action Recognition & 15 & 3,511 & 1,471 & Mean Class Acc\\
    \midrule
    \multirow{2}{*}{\textbf{Task Shift}} & UCF-RC & UCFRep~\cite{ucfrep-zhang2020context} & Repetition Counting & - & 421 & 105 & Mean Error\\
     & Charades & Charades~\cite{charades-sigurdsson:hal-01418216} & Multi-label Recognition & 157 & 7,985 & 1,863 & mAP\\
    \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
    \caption{\textbf{Benchmark Details} for the  downstream evaluation factors, experiments, and datasets we cover. For non-standard evaluations, we follow the SEVERE benchmark~\cite{thoker2022severe}. For self-supervised pretraining, we use Kinetics-400 or Mini-Kinetics.}
    \label{tab:severe_desc}
    \vspace{-1em}
\end{table*}

\vspace{-0.3em}
\subsection{Tubelet Transformation}
\vspace{-0.3em}
\label{section:transformations}
The tubelet motions are simulated by changing the position of the patch across the frames in a video clip, \textit{i.e.} with translation. 
%Next, we propose to add more variety to the simulated motions with spatial changes in the moving patch. 
%However, in real videos when objects move across time, they may also go through some spatial transformations \eg scale changes when moving in relation to the cameras, inplane rotations, shape changes when performing an action, etc. 
In reality, the motion of objects in space may appear as other transformations in videos, for instance, scale decreasing as the object moves away from the camera or motions due to planer rotations.  Motivated by this, we propose to add more complexity and variety to the simulated motions by transforming the tubelets. In particular, we propose scale, rotation, and shear transformations.
%
%Next, we propose to add more variety to the simulated motions with spatial changes in the moving patch.
As before, we sample keyframes $K$ with the first ($i{=}0$) and last frames ($i{=}T$) always included. Transformations for remaining frames are linearly interpolated. 
Formally, we define a tubelet transformation as a sequence of spatial transformations applied to the patch $p$ in each frame $i$ as:
\begin{align}
\begin{split}
\label{eq:trans_general}
&  \mathrm{Trans}_{F} = [p,F(p,\theta^2),....,..,F(p,\theta^T)], \hspace{2pt}\text{ s.t. } \\ 
& \theta^i = 
\begin{cases}
\mathcal{U}(\mathrm{Min},\mathrm{Max}),  & \text{if } i \in K\\
\mathrm{Interp}(\theta^{k},\theta^{k+1}), & \text{otherwise}
\end{cases}
\end{split}
\end{align}
%\begin{equation}
%\begin{cases}
%\begin{split}
%\label{eq:trans_general}
%\quad 
%&  \mathrm{Trans}_{F} = [p,F(p,f^2),....,..,F(p,f^T)], \hspace{2pt}\text{ s.t. } \\ 
%& f^i \in [Min,Max], \hspace{4pt}\text{if } i \in \mathrm{Keyframes}\\
%& f^i  \in \text{Interpolate}(f_{i}^{k},f_{i}^{k+1}), \hspace{4pt}\text{otherwise}
%\end{split}
%\end{cases}
%\end{equation}
where $F(p,\theta^i)$ applies the transformation to patch $p$ according to parameters $\theta^i$, $\mathcal{U}$ samples from a uniform distribution and  $\theta^k$ and $\theta^{k{+}1}$ are the parameters for the keyframes neighboring frame $i$. % which are sampled randomly from the uniform distribution $\mathcal{U}(\mathrm{Min},\mathrm{Max})$.  %$\mathrm{Min},\mathrm{Max}$ represents the min and max values for the transformation factors.
%As before, multiple keyframes $K$ are selected randomly with the first ($i{=}0$) and  the last frame ($i{=}T$) always included.
For the first keyframe, no transformation is applied thus representing the initial state of the patch $p$. 
%For the other keyframes, $f^i$ is randomly sampled from $[Min, Max]$. 
%The transformation factors for the non-keyframes are linearly interpolated between the neighboring keyframes. 
We instantiate three types of such tubelet transformations: scale, rotation, and shear. Examples are shown in Figure~\ref{fig:transformations}.

\noindent \textbf{Scale.} We scale the patch across time with $F(p, \theta^i)$ and 
%Thus for scale tubelet transformation $F() = Resize()$ and the  transformation  factors $f^i = (w^i,h^i.)$ denote the target scale for the patch in frame $i$ with $ w^i$ being the width and $ h^i$ the height of the patch.
horizontal and vertical scaling factors $\theta^i{=}(w^i,h^i)$. % and $F(p, \theta^i)$ applies the transformation matrix $\big[\begin{smallmatrix} w^i & 0 \\ 0 & h^i \end{smallmatrix}\big]$ to patch $p$. 
%For the first frame, we set the scaling factor to 1, representing the original size of the patch $H^{'},W^{'}$ and use  
To sample $w^i$ and $h^i$, we use $\mathrm{Min}{=}0.5$ and $\mathrm{Max}{=}1.5$. 
%as we randomly select scaling factors between $U(0.5 - 1.5)$ and linearly interpolate the scaling factors for the rest of the frames.  
%Then, Eq.~\eqref{eq:trans_general} can be instantiated with $F(p,f^i) = \mathrm{Resize}(p, (w_i,h^i)$
%\cs{sloppy:}: 
%\begin{equation}
%\textit{Trans}_{scale} = [f(W',H'),..,f(w^m,h^m) ,..,f(w^T,h^T)],
%\mathrm{Trans}_{\mathrm{Resize}} = [P, \mathrm{Resize}(P,s^2),..,\mathrm{Resize}((P,s^T)],
%\end{equation}
%where $s^1 = (W',H')$ is the initial size of the patch, 
%where  $s^i {=} (w^i,h^i.)$ denotes the target scale of the patch in frame $i$ and $ w^i,h^i$ represent the width and height of the patch.
%The left of Figure~\ref{fig:transformations} shows examples of such a transformation. 


\noindent \textbf{Rotation.} In this transformation $F(p, \theta^i)$ applies in-plane rotations to tubelet patches.  
%Thus for rotation tubelet transformation, $F() = Rotate()$  and the  transformation  factors $f^i = r^i$ is the target rotation angle for the patch  in frame $i$. 
Thus, $\theta^i$ is a rotation angle sampled from $\mathrm{Min}{=}{-}90^{\circ}$ and $\mathrm{Max}{=}{+}90^{\circ}$. 
%\begin{equation}
%\mathrm{Trans}_{\mathrm{Rotate}} = [P, \mathrm{Rotate}(P,r^2),..,\mathrm{Rotate}((P,r^T)],
%\end{equation}
%where  $r^i$  is the target rotation angle for the patch  in frame $i$.
%Figure~\ref{fig:transformations} (middle) shows examples of such a rotation transformation.

\noindent \textbf{Shear.} We shear the patch  as the tubelet progresses with $F(p, \theta^i)$.  
%Then, $F() = Shear()$ is the transformation function with  factors $f^i {=} (s^{i}_{x},s^{i}_{y}.)$ denoting the target shearing factor for the patch  in frame $i$. $s^{i}_{x},s^{i}_{y}$ represents the shear along the X and Y axes respectively. 
The shearing parameters are $\theta^i{=}(r^i,s^{i})$ which are sampled using $\mathrm{Min}{=}{-}1.5$ and $\mathrm{Max}{=}1.5$. 
%Again, we set the shear factor to 0 for the first frame representing the original shape of the patch, and  sample as per Eq.~\eqref{eq:trans_general} for other frames.
%\begin{equation}
%\mathrm{Trans}_{\mathrm{Shear}} = [P, \mathrm{Shear}(P,s^2),..,\mathrm{Shear}((P,s^T)],
%\end{equation}
%where  $s^i {=} (s^{i}_{x},s^{i}_{y}.)$ denotes the target shearing factor for the patch  in frame $i$ and $s^{i}_{x},s^{i}_{y}$ represents the shear along the X and Y axes respectively. 
%Figure~\ref{fig:transformations} (right column) shows examples of the shear transformation.

With these tubelet transformations and the motions created in Section~\ref{section:motions} we are able to simulate a variety of subtle motions in videos, making the model data-efficient. By learning the similarity between the same tubelet overlaid onto different videos, our model pays less attention to spatial features, instead learning to represent these subtle motions. This makes the learned representation generalizable to different domains and action granularities.  

%\cs{This paragraph feels repetitive, can probably go. Instead we need a short closing statement for the section.}
%To summarize, for tubelet-contrastive learning  we first sample two video clips from two different videos and crop a set of patches from one of the clips. From the patches, we construct a set of tubelets with linear or non-linear motions. Each tubelet is then transformed by one of the transformation functions.  The resulting set of  tubelets is overlaid ($\odot $) on the original clips to create positive pairs that have the same motion patterns. By contrasting such positive pairs, the network is optimized to learn similarities between clips with the same motion-tublets and dissimilarities between clips with different motion tublets. Since the motion-tubelets are the only common information between the positive pairs, such optimization forces the network to rely on temporal cues when solving this contrastive task, which in turn results in learning motion-focused feature representations for videos. 

     

