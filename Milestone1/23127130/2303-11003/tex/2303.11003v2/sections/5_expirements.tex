

\vspace{-0.5em}
\subsection{Ablation Studies \& Analysis}
\vspace{-0.3em}
To ablate the effectiveness of individual components we pretrain on Mini-Kinetics and evaluate on UCF ($10^{3}$), Gym ($10^{3}$), Something-Something v2 and UB-S1. To decrease the finetuning time we use a subset of Something Something (SSv2-Sub) with 25\% of the training data (details in supplementary). Unless specified otherwise, we use non-linear motion and rotation to generate tubelets.
%\cs{what video encoder is used?}

\begin{table}
    \centering
    \setlength{\tabcolsep}{2pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc} \toprule  
    %Pre-training& UCF ($10^{3}$) & Gym ($10^{3}$) &UB-S1 & SSv2-Sub\\
    & UCF ($10^{3}$) & Gym ($10^{3}$) & SSv2-Sub & UB-S1 \\
         \midrule
         \rowcolor{Gray}
         \textbf{Temporal Contrast} & & & &\\
         Baseline    & 57.5 & 29.5        & 44.2  & 84.8      \\
         \rowcolor{Gray}
         \textbf{Tubelet Contrast} & & & &\\
         Tubelet Generation & 48.2  &28.2    & 40.1  & 84.1      \\
         Tubelet Motion
          & 63.0 & 45.6          & 47.5  & 90.3      \\
         Tubelet Transformation  & 65.5 & 48.0 & 47.9  & 90.9      \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
    \caption{\textbf{Tubelet-Contrastive Learning} considerably outperforms temporal contrast on multiple downstream settings. Tubelet motion and transformations are key. % to this improvement. %Each component contributes to the improvement of downstream performance.
    }
    \label{tab:ablation_main}
    \vspace{-1em}
\end{table}

\noindent\textbf{Tubelet-Contrastive Learning.}
Table~\ref{tab:ablation_main} shows the benefits brought by our tubelet-contrastive learning. We first observe that our full tubelet-contrastive model improves considerably over the temporal contrastive baseline, which uses MoCo~\cite{moco} with a temporal crop augmentation. This improvement applies to all downstream datasets but is especially observable with Gym ($10^{3}$) (+18.5\%) and UB-S1 (+6.1\%) where temporal cues are crucial. Our model is also effective on UCF ($10^{3}$) (+8.0\%) where spatial cues are often as important as temporal ones. These results demonstrate that learning similarities between synthetic tubelets produces generalizable, but motion-focused, video representations required for finer temporal understanding.

It is clear that the motion within tubelets is critical to our model's success as contrasting static tubelets obtained from our tubelet generation (Section~\ref{section:tbcl}) actually decreases the performance from the temporal contrast baseline. 
%\cs{Long sentence, hard to parse:}
%This is because positive pairs with such tubelets only share spatial similarity in the form of a static patch that is duplicated in all frames of the video clips at the same location, without any motion and therefore does not require any temporal understanding for learning similarities which results in  poor video feature representations. 
%
%This demonstrates that tubelet motion is key to learning effective video representations. 
When tubelet motion is added (Section~\ref{section:motions}), performance improves considerably, \eg, Gym ($10^{3}$) +17.4\% and SSv2-Sub +7.4\%. 
Finally, adding more motion types via tubelet transformations (Section~\ref{section:transformations}) further improves the video representation quality, \eg, UCF ($10^{3}$) +2.5\% and  Gym ($10^{3}$) +2.4\%. This highlights the importance of including a variety of motions beyond what is present in the pretraining data to learn generalizable video representations. %Further validating our idea that adding such motion patterns for learning improves downstream performance. 



\begin{table}
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{lcccc} \toprule Tubelet Motion & UCF ($10^{3}$) & Gym ($10^{3}$) &  SSv2-Sub & UB-S1  \\
         \midrule
         No motion & 48.2  &28.2      & 40.1 & 84.1 \\
         Linear   & 55.5 & 34.6       & 45.3 & 88.5 \\
         Non-Linear   & 63.0 & 45.6   & 47.5 & 90.3 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
    \caption{\textbf{Tubelet Motions.} Learning from tubelets with non-linear motion benefits multiple downstream settings.}
    \label{tab:ablation_motions}
    \vspace{-0.8em}
\end{table}

 
 \begin{table}
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{lcccc} \toprule Transformation & UCF ($10^{3}$) & Gym ($10^{3}$)  & SSv2-Sub &UB-S1\\
         \midrule
         None   & 63.0 & 45.6 & 47.5 & 90.5    \\
         Scale  & 65.1 & 46.5 & 47.0 & 90.5   \\
         Shear  & 65.2 & 47.5 & 47.3 & 90.9   \\
         Rotation & 65.5 & 48.0 & 47.9 & 90.9   \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
    \caption{\textbf{Tubelet Transformation.} Adding motion patterns to tubelet-contrastive learning through transformations improves downstream performance. Best results for rotation.}
    \label{tab:ablation_transformations}
    \vspace{-0.8em}
\end{table}


\noindent\textbf{Tubelet Motions.} Next, we ablate the impact of the  tubelet motion type (Section~\ref{section:motions}) without transformations. We compare the performance of static tubelets with no motion, linear motion, and non-linear motion in Table~\ref{tab:ablation_motions}. Tubelets with simple linear motion already improve performance for all four datasets, \eg, +6.4\% on Gym ($10^3$). %\cs{highlight one result?}. 
Using non-linear motion further improves results, for instance with an additional +11.0\% improvement on Gym ($10^3$). %\cs{highlight another result?}. 
We conclude that learning from non-linear motions provides more generalizable video representations. 
%This demonstrates the need to simulate a variety of motion patterns to learn video representations that can apply to a variety of downstream domains. %As a result, we use tubelets with non-linear motions in our final model.
 %\cs{It would be interesting to look at individual action recognition results to see whether there are any spectacular individual results, both pro and con. These could be reported in supplemental if of interest.}


\begin{table}
    \centering
    \resizebox{0.88\linewidth}{!}{
    \begin{tabular}{lcccc}
         \toprule \#Tubelets & UCF ($10^{3}$) & Gym ($10^{3}$)  & SSv2-Sub & UB-S1\\
         \midrule
         1 & 62.0 & 39.5 &  47.1 & 89.5 \\
         2 & 65.5 & 48.0 &  47.9 & 90.9 \\
         3 & 66.5 & 46.0 &  47.5 & 90.9 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
    \caption{\textbf{Number of Tubelets.} Overlaying two tubelets in positive pairs improves downstream performance. %Best results for two tubelets. %\cs{We can keep the ablation, but move the table with detailed numbers to supplemental?}
    }
    \label{tab:ablation_num_tubelets}
    \vspace{-1em}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/grad_cam.pdf}
\vspace{-2em}
\caption{\textbf{Class-Agnostic Activation Maps without Finetuning} for the temporal contrastive baseline and our tubelet-contrast.  Our model better attends to regions with motion. }
\vspace{-0.8em}
\label{fig:caam}
\end{figure}


\begin{table}[t]
    \centering
    %\midsepremove
    \setlength{\tabcolsep}{5pt}
    \resizebox{0.88\linewidth}{!}{\begin{tabular}
     {l  cccc}
    \toprule
        & \multicolumn{2}{Sc}{\textbf{Linear Classification}}   & \multicolumn{2}{Sc}{\textbf{ Finetuning }}  \\ 
      \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
      %\addlinespace[0.1cm]
         &   \multicolumn{1}{c}{UCF101} & \multicolumn{1}{c}{Gym99}  & \multicolumn{1}{c}{UCF101} & \multicolumn{1}{c}{Gym99}  \\ 
         \midrule
         Temporal Contrast &  \textbf{58.9}   & 19.7  &87.1   & 90.8  \\ 
         Tubelet Contrast  & 30.0   & \textbf{34.1} &  \textbf{91.0}   & \textbf{92.8} \\ 
        %\addlinespace[0.01cm]
         \bottomrule
    \end{tabular}
    }
    \vspace{-0.8em}
    \caption{\textbf{Appearance vs Motion}. Our method learns to capture motion dynamics with pretraining and can easily learn appearance features with finetuning.}
\label{tab:qa}
\vspace{-1em}
\end{table}


\begin{figure*}[t!]
\centering
\includegraphics[width=0.98\linewidth]{figures/data-eff.png}
\vspace{-1.2em}
\caption{\textbf{Video-Data Efficiency of Tubelet-Contrastive Learning.} Our approach maintains performance when using only 25\% of the pretraining data. When using 5\% of the pretraining data, our approach is still more effective than using 100\% with the baseline for Gym ($10^3$), UB-S1, and HMDB51. Results are averaged over three pretraining runs with different seeds.%\hd{Typo in figure legend, temoral $\rightarrow$ temporal} 
%\hd{Replace FX-S1 with UB-S1}
\vspace{-1em}
}
\label{fig:data_efficiency}
\end{figure*}


\noindent\textbf{Tubelet Transformation.}
Table~\ref{tab:ablation_transformations} compares the  proposed tubelet transformations (Section~\ref{section:transformations}). % when using non-linear motion.
%\cs{Do not write this from the dataset perspective but from the transformation perspective:}
All four datasets benefit from transformations, with rotation being the most effective. %We find that scaling improves results on UCF ($10^3$) (+2.1\%) and Gym ($10^3$) (+0.9\%) while maintaining performance on UB-S1 and SSv2-Sub. Shearing gives better performance over scaling on all datasets, \eg, Gym ($10^3$) (+1.0\%) however, performance for SSv2-Sub is still similar to using no transformation. Rotation is the most effective transformation, increasing performance on all four datasets.
%For UCF($10^{3}$) and Gym ($10^{3}$) all three transformations are helpful. For UB-S1, both shearing and rotation improve performance, while for SSv2-Sub only rotations are helpful. 
The differences in improvement for each transformation are likely due to the types of motion present in the downstream datasets. For instance, Gym ($10^3$) and UB-S1 contain gymnastic videos where actors are often spinning and turning but do not change in scale due to the fixed camera, therefore rotation is more helpful than scaling. %While SomethingSomething contains videos recorded with static cameras that have motion types like moving objects from left to right or top to bottom, etc. %This is probably because scale mostly adds spatial variations to the tubelets while rotation and shearing add both spatial and  motion variations to  the tubelets resulting in learning better spatial-temporal dynamics. 
We also experiment with combinations of transformations in supplementary but observe no further improvement. %Thus,  we choose rotation as the tubelet transformation for our final model.   

%\vspace{-0.20em}
\noindent\textbf{Number of Tubelets.} %\cs{Reduce amount of text and move all details incl table to appendix to win space?}
%Finally, we investigate the number of tubelets overlaid on video clips to generate positive pairs (Eq.~\ref{eq:overlay_M_tubelets}). Table~\ref{tab:ablation_num_tubelets} shows the comparison for using one, two, and three tubelets per video. 
We investigate the number of tubelets used in each video in Table~\ref{tab:ablation_num_tubelets}. One tubelet is already more effective than temporal contrastive learning, \eg, 29.5\% vs. 39.5\% for Gym ($10^3$). Adding two tubelets improves accuracy on all datasets, \eg, +8.5\% for Gym ($10^3$). %This improvement starts to saturate when using three tubelets, with only UCF ($10^3$) benefitting. Therefore, we use two tubelets in our final model.


\noindent\textbf{Analysis of Motion-Focus. } 
To further understand what our model learns, Figure~\ref{fig:caam} visualizes the class agnostic activation maps~\cite{CAAM} without finetuning for the baseline and our approach. We observe that even  without previously seeing any FineGym data, our approach attends better to the motions than the temporal contrastive baseline, which attends to the background regions. This observation is supported by the linear classification and finetuning results on  
UCF101 (appearance-focused) and Gym99 (motion-focused) in Table~\ref{tab:qa}. When directly  predicting from the learned features with linear classification, our model is less effective than temporal contrast for appearance-based actions in UCF101, but positively affects actions requiring fine-grained motion understanding in Gym99. With finetuning, our tubelet-contrastive representation is able to add spatial appearance understanding and maintain its ability to capture temporal motion dynamics, thus it benefits both UCF101 and Gym99.

\vspace{-0.8em}
\subsection{Video-Data Efficiency} 
\vspace{-0.6em}
To demonstrate our method's data efficiency, we pretrain using subsets of the Kinetics-400. In particular, we sample $5\%,10\%,25\%,33\% \text{ and } 50\%$ of the Kinetics-400 training set with three random seeds and pretrain our model and the temporal contrastive baseline. %We repeat the experiment three times to sample different subsets of data with different seeds. 
We compare the effectiveness of these representations after finetuning on UCF ($10^{3}$), Gym($10^{3}$), SSv2-Sub, UB-S1, and HMDB51 in Figure~\ref{fig:data_efficiency}.
%
On all downstream setups, our method maintains similar performance when reducing the pretraining data to just 25\%, while the temporal contrastive baseline performance decreases significantly. Our method is less effective when using only 5\% or 10\% of the data, but remarkably still outperforms the baseline trained with 100\% data for Gym ($10^3$), UB-S1, and HMDB. We attribute our model's data efficiency to the tubelets we add to the pretraining data. In particular, our non-linear motion and transformations generate a variety of synthetic tubelets that 
simulate a greater variety of fine-grained motions than are present in the original data. 
