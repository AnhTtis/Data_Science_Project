\vspace{-0.3em}
\section{Experiments}
\vspace{-0.3em}
\subsection{Datasets, Evaluation \& Implementation}
\vspace{-0.3em}

\noindent\textbf{Pretraining Datasets. } 
Following prior work \cite{videomoco-pan2021videomoco,gdt-patrick2020multimodal,dave2022tclr,ctp-wang2021unsupervised,rspnet-chen2020RSPNet,huang2021ascnet}  we use \textbf{Kinetics-400}~\cite{Kinetics-400-arxiv} for self-supervised pretraining. Kinetics-400 is a large-scale action recognition dataset containing 250K videos of 400 action classes.
To show data efficiency, we also pretrain with \textbf{Mini-Kinetics}~\cite{mini_kinetics}, a subset containing 85K videos of 200 action classes. %For both cases, we use the training set without action labels for self-supervised pretraining. 

\noindent \textbf{Downstream Evaluation. } 
To evaluate the video representations learned by our tubelet contrast, we finetune and evaluate our model on various downstream datasets summarized in Table~\ref{tab:severe_desc}.
Following previous self-supervised work, we evaluate on standard benchmarks: \textbf{UCF101}~\cite{UCF-101-arxiv} and \textbf{HMDB51}~\cite{HMDB-51-ICCV}. These action recognition datasets contain coarse-grained actions with domains similar to Kinetics-400. 
%UCF101 contains 13,320 videos from 101 action classes while HMDB51 contains 6,890 videos from 51 action classes. 
For both, we report top-1 accuracy on split 1 from the original papers.
%
We examine the generalizability of our model with the \textbf{SEVERE} benchmark proposed in our previous work~\cite{thoker2022severe}. This consists of eight experiments over four downstream generalization factors: \textit{domain shift}, \textit{sample efficiency}, \textit{action granularity}, and \textit{task shift}. %SEVERE contains two experimental setups for each factor.
\textit{Domain shift} is evaluated on Something-Something v2~\cite{SS-v2-arxiv} (SSv2) and FineGym~\cite{Gym-99-arxiv} (Gym99)  
which vary in domain relative to Kinetics-400. 
\textit{Sample efficiency} evaluates low-shot action recognition on UCF101~\cite{UCF-101-arxiv} and FineGym~\cite{Gym-99-arxiv} with 1,000 training samples, referred to as UCF ($10^{3}$) and Gym ($10^{3}$).
\textit{Action granularity} evaluates semantically similar actions using FX-S1 and UB-S1 subsets from  FineGym~\cite{Gym-99-arxiv}. In both subsets, action classes belong to the same element of a gymnastic routine, \eg, FX-S1 is types of jump.  
\textit{Task shift} evaluates tasks beyond single-label action recognition. Specifically, it uses temporal repetition counting on UCFRep~\cite{ucfrep-zhang2020context}, a subset of UCF-101~\cite{ucfrep-zhang2020context}, and multi-label action recognition on Charades~\cite{charades-sigurdsson:hal-01418216}. %where videos are an average of 30 seconds long. 
The experimental setups are detailed in Table~\ref{tab:severe_desc} and all follow SEVERE~\cite{thoker2022severe}. 

\noindent\textbf{Tubelet Generation and Transformation.} Our clips are 16 $112 {\times} 112$ frames with standard spatial augmentations: random crops, horizontal flip, and color jitter. %Accordingly, we set $T{=}16$, $W{=}112$ and $H{=}112$ for tubelet generation.  
We randomly crop 2 patches to generate $M{=}2$ tubelets (Eq.~\ref{eq:overlay_M_tubelets}). The patch size $H^{'}{\times} W^{'}$ is uniformly sampled from $[16{\times}16, 64{\times64}]$. We also randomly sample a patch shape from a set of predefined shapes. For linear motions, we use $\Delta{=}[40{-}80]$ displacement difference. For non-linear motion, we use $N{=}48$ and a smoothing factor of $\sigma{=}8$ (Eq.~\ref{eq:nonlinear_motion}). For linear motion and all tubelet transformations, we use $K{=}3$ keyframes.

\noindent\textbf{Networks, Pretraining and Finetuning.}
We use R(2+1)D-18~\cite{tran2018closer} as the video encoder, following previous self-supervision works~\cite{pace,gdt-patrick2020multimodal,rspnet-chen2020RSPNet,fame,dave2022tclr,videomoco-pan2021videomoco}. The projection head is a 2-layer MLP with 128D output. We use momentum contrast~\cite{moco} to increase the number of negatives $|\mathcal{N}|$ (Eq.~\ref{eq:tcl}) to 16,384 for Mini-Kinetics and 65,536 for Kinetics. 
We use temperature $\tau{=}0.2$ (Eq.~\ref{eq:tcl}). The model is optimized using SGD with momentum 0.9,  learning rate 0.01, and weight decay 0.0001. We use a batch size of 32 for Mini-Kinetics and 128 for Kinetics, a cosine scheduler~\cite{cosine_sch}, and pretrain for 100 epochs. After pretraining, we replace the projection head with a task-dependent head as in SEVERE~\cite{thoker2022severe} and finetune the whole network with labels for the downstream task.  We provide finetuning details in the supplementary. %