\clearpage
\noindent\textbf{\Large{Appendix}}
%\begin{center}
%\textbf{Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization\\Supplementary Material}
%\end{center}
% \begin{equation}
%\mathcal{L}(V1,V2) = -\log \frac{\displaystyle \mathrm{exp}(Z_{v_{1}^{'}}\cdot Z_{v_{2}^{'}}/\tau)}
%{ \displaystyle\mathrm{exp}(Z_{v_{1}^{'}}\cdot Z_{v_{2}^{'}}/\tau)   + \displaystyle\sum_{Z_{v_{n}^{'}} \sim \mathcal{N}} \mathrm{exp}(Z_{v_{1}^{'}}\cdot Z_{v_{n}^{'}}/\tau)},
%\label{eq:3}
%\end{equation}
\vspace{-0.3em}
\section{Generalization on Diving48} 
\vspace{-0.3em}
To further highlight the generalizability of our method to new domains and fine-grained actions, we finetune and evaluate with the challenging Diving48 dataset~\cite{diving48}. %\cite{fame,background_removing,dave2022tclr}. 
It contains 18K trimmed videos for 48 different diving sequences all of which take place in similar backgrounds and need to be distinguished by subtle differences such as the number of somersaults or the starting position. We use standard train/test split and report top-1 accuracy.

In Table~\ref{tab:diving}, we show the performance of our model when pretrained on the full Kinetics-400 and on Mini-Kinetics ($\dagger$). We compare these results to no pretraining, the temporal contrastive baseline pretrained on Kinetics-400, and supervised pretraining on Kinetics-400 with labels.  Our method increases the performance  over training from scratch by 7.9\% and the temporal contrastive baseline by 6.6\%. 
 Our method even outperforms the supervised pretraining baseline by 4.5\%. This suggests that by contrasting tubelets with different motions, our method is able to learn better video representations for fine-grained actions than supervised pretraining on Kinetics.
 When pretraining on Mini-Kinetics (3x smaller than Kinetics-400) the performance of our model does not decrease, again demonstrating the data efficiency of our approach.

\vspace{-0.3em}
\section{Evaluation with R3D and I3D Backbones }
\vspace{-0.3em}
In addition to the R(2+1)-18 backbone,  we also show the performance of our proposed method with other commonly used video encoders \ie, R3D-18~\cite{tran2018closer} and I3D~\cite{carreira2017quo}.
For R3D-18,  we use the same tubelet generation and transformation as that of  R(2+1)D-18, as described in the main paper. For I3D, we change the input resolution to 224x224 and sample the patch size $H^{'}{\times} W^{'}$  uniformly from $[32{\times}32, 128{\times128}]$. For both, we follow the same pretraining  protocol as described in the main paper. 

We compare  with prior works on the standard UCF101~\cite{UCF-101-arxiv} and HMDB51~\cite{HMDB-51-ICCV} datasets. Table~\ref{tab:R3D} shows the results with  Kinetics-400 as the pretraining dataset.   With the I3D backbone, our method outperforms prior works on both UCF101 and HMDB51.
Similarly, with the R3D-18 backbone, we outperform prior works using the RGB modality on UCF101. We also achieve comparable performance to the best-performing method on HMDB51, improving over the next best method by 6.3\%.  
On HMDB51 we also outperform prior works which pretrain on an additional optical flow modality and achieve competitive results with these methods on UCF101.

\begin{table}[t!]
    \centering
    \resizebox{0.6\linewidth}{!}{
    \begin{tabular}{lc}
         \toprule Pretraining &  Top-1\\
         \midrule
         Supervised~\cite{tran2018closer} & 84.5  \\
         \midrule
         None & 81.1 \\
         Temporal Contrast Baseline &  82.4 \\
         \midrule
         \textbf{\textit{This paper}}$^{\dagger}$ & \textbf{89.4} \\
         \textbf{\textit{This paper}}  & \textbf{89.0} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.5em}
        \caption{ \textbf{Generalization on Diving48~\cite{diving48}.} Comparison with temporal contrastive pretraining and supervised pretraining on Diving48. All models use R(2+1)D-18. $\dagger$ indicates pretraining on Mini-Kinetics, otherwise all pretraining was done on Kinetics-400.}
    \label{tab:diving}
    \vspace{-0.5em}
\end{table}


\begin{table}[t!]
\centering
\resizebox{0.75\linewidth}{!}{
    \begin{tabular}{lllcc}
        \toprule
        \textbf{Method} & \textbf{Modality} & UCF & HMDB\\
         \midrule
         \rowcolor{Gray}
         \textbf{I3D} & & & \\
         %\midrule
        SpeedNet~\cite{benaim2020speednet} & RGB &  66.7 & 43.7 \\
        DSM~\cite{wang2021enhancing} & RGB &   74.8 & 52.5 \\
        BE~\cite{background_removing} & RGB & 86.2 & 55.4 \\
        FAME~\cite{fame} & RGB & 88.6 & 61.1 \\
        \textbf{\textit{This paper}}$^{\dagger}$ & RGB&   \textbf{89.5}	& \textbf{64.0} \\
        \textbf{\textit{This paper}} & RGB&   \textbf{89.7}	& \textbf{63.9} \\
         \midrule
         \rowcolor{Gray}
         \textbf{R3D-18} & & & \\
         %\midrule
        %\rowcolor{Gray}
        %$^\dagger$SeLaVi~\cite{selavi}& RGB+Audio & R(2+1)D &   83.1 & 47.1 \\
        %\rowcolor{Gray}
        %$^\dagger$XDC~\cite{alwassel_2020_xdc} & RGB+Audio & R(2+1)D  &  86.8 & 52.6 \\
        %\rowcolor{Gray}
        VideoMoCo~\cite{videomoco-pan2021videomoco} & RGB &  74.1 & 43.6 \\
        RSPNet~\cite{rspnet-chen2020RSPNet} & RGB &   74.3 &41.6 \\
        LSFD~\cite{lsfd} & RGB & 77.2 & 53.7 \\
        MLFO~\cite{mlfo} & RGB &  79.1 & 47.6 \\
        ASCNet~\cite{huang2021ascnet} & RGB &   80.5 & 52.3 \\
        MCN~\cite{mcn} & RGB    & 85.4 & 54.8 \\
        TCLR~\cite{dave2022tclr} & RGB&   85.4 &55.4 \\
        CtP~\cite{ctp-wang2021unsupervised} & RGB&  86.2 & 57.0 \\
        %BE~\cite{background_removing} & RGB & R3D-34 & 87.1 & 55.4\\ 
        TE~\cite{jenni2021time_eqv} & RGB &   87.1 & \textbf{63.6} \\
       \color{gray}
        MSCL~\cite{mscl} & \color{gray} RGB+Flow & \color{gray}   90.7 & \color{gray} 62.3 \\
        \color{gray}
        MaCLR~\cite{xiao2022maclr} & \color{gray} RGB+Flow &  \color{gray} 91.3 & \color{gray} 62.1 \\ %\midrule
%        \rowcolor{visualblue}
        \textbf{\textit{This paper}}$^{\dagger}$ & RGB&   \textbf{88.8}	& {62.0} \\
        \textbf{\textit{This paper}} & RGB&   \textbf{90.1}	& {63.3} \\
        \bottomrule

        \end{tabular}}
      \vspace{-0.5em}    
    \caption{\textbf{Evaluation with I3D and R3D backbones:} on standard  UCF101 and HMDB51 benchmarks. 
    %All models utilize Kinetics-400 for self-supervised pretraining. 
    Gray lines indicate the use of additional modalities during self-supervised pretraining. $\dagger$ indicates pretraining on Mini-Kinetics, otherwise, all models were pretrained on Kinetics-400.
    }
    \label{tab:R3D}
        \end{table}
\begin{table*}[t]
    \centering
    %\setlength{\tabcolsep}{4pt}
    \resizebox{0.87\linewidth}{!}{
    \begin{tabular}{lllrrrr}
    \toprule
    \textbf{Evaluation Factor} & \textbf{Experiment} & \textbf{Dataset} & \textbf{Batch Size} & \textbf{Learning rate} & \textbf{Epochs} & \textbf{Steps} \\ % & \textbf{Eval Metric}\\
    \midrule
    \multirow{2}{*}{\textbf{Standard}} & UCF101 & UCF 101~\cite{UCF-101-arxiv} & 32&  0.0001  & 160 & [60,100,140] \\
 & HMDB51 & HMDB 51~\cite{HMDB-51-ICCV} & 32&  0.0001  & 160 & [60,100,140]  \\
    \midrule
    \multirow{2}{*}{\textbf{Domain Shift}} & SS-v2 & Something-Something~\cite{SS-v2-arxiv} & 32&  0.0001 & 45  & [25, 35, 40] \\
     & Gym-99 & FineGym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [60,100,140] \\
    \midrule
    \multirow{2}{*}{\textbf{Sample Efficiency}}& UCF ($10^3$) & UCF 101~\cite{UCF-101-arxiv} &  32&  0.0001 & 160 & [80,120,140]  \\
     & Gym ($10^3$) & FineGym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [80,120,140]  \\
    \midrule
    \multirow{2}{*}{\textbf{Action Granularity}} & FX-S1 & FineGym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [70,120,140]  \\
     & UB-S1 & FineGym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [70,120,140]  \\
    \midrule
    \multirow{2}{*}{\textbf{Task Shift}} & UCF-RC & UCFRep~\cite{ucfrep-zhang2020context} & 32&  0.00005 & 100 & -  \\
     & Charades & Charades~\cite{charades-sigurdsson:hal-01418216} &  16&  0.0375 & 57 & [41,49]  \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
    \caption{\textbf{Training Details} of finetuning on various downstream datasets and tasks.}
    \label{tab:finetuning}
        \vspace{-0.8em}
\end{table*}

\vspace{-0.3em}
\section{Evaluation on Kinetics Dataset}
\vspace{-0.3em}
To show whether our tubelet-contrastive pretraining can improve the performance of downstream tasks when plenty of labeled data is available for finetuning, we evaluate it on the Kinetics-400~\cite{Kinetics-400-arxiv} dataset for the task of action classification. The dataset contains about 220K labeled videos for training and 18K videos for validation. As evident from Table~\ref{tab:k400},  such large-scale datasets can still benefit from our pretraining with a 3.4\% improvement over training from scratch and 0.7\% over the temporal contrast baseline. 


\begin{table}[t!]
    \centering
    \resizebox{0.6\linewidth}{!}{
    \begin{tabular}{lc}
         \toprule Pretraining &  Top-1\\
         \midrule
         None & 61.4 \\
         Temporal Contrast Baseline &  64.1 \\
         \midrule
         \textbf{\textit{This paper}}  & \textbf{64.8} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.5em}
    \caption{\textbf{Kinetics-400 Evaluation.} Comparison with temporal contrastive pretraining for large-scale action recognition. All models use R(2+1)D-18 and pretraining was done on Kinetics-400 training set.}
    \label{tab:k400}
    \vspace{-0.7em}
\end{table}

\vspace{-0.3em}
\section{Finetuning Details} 
\vspace{-0.3em}
During finetuning, we follow the setup from the SEVERE benchmark~\cite{thoker2022severe} which is detailed here for completeness. For all tasks, we replace the projection of the pretrained model with a task-dependent head. \\
\noindent\textbf{Action Recognition}. 
Downstream settings which examine domain shift, sample efficiency, and action granularity all perform action recognition. We use a similar finetuning process for all experiments on these three factors. During the training process, a random clip of 32 frames is taken from each video and standard augmentations are applied: a multi-scale crop of 112x112 size, horizontal flipping, and color jittering. The Adam optimizer is used for training, with the  learning rate, scheduling, and total number of epochs for each experiment shown in Table~\ref{tab:finetuning}. During inference, 10 linearly spaced clips of 32 frames each are used, with a center crop of 112x112. To determine the action class prediction for a video, the predictions from each clip are averaged. For domain shift and sample efficiency, we report the top-1 accuracy.
 For action granularity experiments we report mean class accuracy, which we obtain by computing accuracy per action class and averaging over all action classes. \\
\noindent\textbf{Repetition counting}. 
The  implementation follows the original repetition counting work proposed in  UCFrep work~\cite{ucfrep-zhang2020context}.
From the annotated videos, 2M sequences of 32 frames with spatial size 112x112 are constructed. These are  used as the input. The model is trained with a batch size of 32 for 100 epochs using the Adam optimizer with a learning rate of 0.00005. For testing, we report mean counting error following\cite{ucfrep-zhang2020context}. \\
\noindent\textbf{Multi-label classification on Charades}. Following \cite{large-scale-feichtenhofer2021large},
 a per-class sigmoid output is utilized for multi-class prediction. During the training process, 32 frames are sampled with a stride of 8. %Since this task requires a longer temporal context, using more frames with a higher sampling rate can improve the results. 
Frames are cropped to 112x112 and random short-side scaling, random spatial crop, and horizontal flip augmentations are applied. The model is trained for a total of 57 epochs with a batch size of 16 and a learning rate of 0.0375. A multi-step scheduler with $\gamma = 0.1$ is applied at epochs [41, 49]. During the testing phase,  spatiotemporal max-pooling is performed over 10 clips for a single video. We report mean average precision (mAP) across all classes.    

\noindent\textbf{SSv2-Sub details}. We use a subset of Something-Something v2 for ablations. In particular, we randomly sample 25\% of the data from the whole train set and spanning all categories. This results in a subset consisting of 34409 training samples from 174 classes. We use the full validation set of Something-Something v2 for testing.

\begin{table}[t!]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lcc} \toprule Transformation & UCF ($10^{3}$) & Gym ($10^{3}$)  \\
         \midrule
         None   &                           63.0 & 45.6      \\
         \midrule
         Scale  &                           65.1 & 46.5      \\
         Shear  &                           65.2 & 47.5      \\
         Rotate &                           65.5 & 48.0      \\
         \midrule  
         Scale + Shear  &                   65.2 & 46.0      \\
         Rotate + Scale   &                 65.4 & 46.9      \\
         Rotate + Shear   &                 65.3 & 45.7      \\
         Rotate + Scale + Shear  &          65.6 & 46.0      \\
        \bottomrule
    \end{tabular}}
      \vspace{-0.8em}
    \caption{\textbf{Tubelet Transformation Combinations.} Combining transformations doesn't give a further increase in performance compared to using individual transformations. }
    \label{tab:ablation_transformations_combination}
    \vspace{-0.5em}
\end{table}

\vspace{-0.5em}
\section{Tubelet Transformation Hyperparameters} 
\vspace{-0.3em}
Table~\ref{tab:ablation_transformations_combination} shows the results when applying multiple tubelet transformations in the tubelet generation. While applying individual transformations improves results, combing multiple transformations doesn't improve the performance further. This is likely because rotation motions are common in the downstream datasets while scaling and shearing are less common. %such transformations result in simulating motion with realistic transformations which do not resemble the motion of objects/persons in downstream videos. 

Table~\ref{tab:ablation_transformations_range} shows an ablation over $\mathrm{Min}$ and $\mathrm{Max}$ values for tubelet transformations.  In the main paper, we use scale values between 0.5 and 1.5, shear values between -1.0 and 1.0, and rotation values between -90 and 90. Here, we experiment with values that result in more subtle and extreme variations of these transformations.  We observe that all values for each of the transformations improve over no transformation. Our model is reasonably robust to these choices in hyperparameters, but subtle variations \eg,  scale change between 0.5 to 1.25 or shear from 0.75 to 0.75 tend to be slightly less effective. %We also observe that performance can be improved be extending the  range of these hyperparameters \eg rotation between -180 to 180.


\begin{table}[t!]
    \centering
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{llcc} \toprule Min & Max & UCF ($10^{3}$) & Gym ($10^{3}$)  \\
         \midrule
         %\hdashline
         \rowcolor{Gray}
         \textbf{None} & & & \\
         - & - &                            63.0 & 45.6 \\
         \rowcolor{Gray}
         \textbf{Scale}  &  & &\\
         0.5 & 1.25  &                  65.6 & 45.3      \\
         0.5 & 1.5  &                   65.1 & 46.5      \\
         0.5 & 2.0  &                   65.6 & 46.0      \\
         %\midrule
         \rowcolor{Gray}
         \textbf{Shear}  &  & &\\
         -0.75 & 0.75  &                   64.4 &   47.5    \\
         -1.0 & 1.0  &                   65.2 & 48.0      \\
         -1.5 & 1.5  &                     65.2 &   47.5      \\
         %\midrule
         \rowcolor{Gray}
         \textbf{Rotation}  &  & &\\
         -45 & 45                        &    65.2 & 49.3      \\
         -90 & 90                       &    65.5 & 48.0      \\
         -180 & 180                        &    65.6 & 49.6     \\  
        \bottomrule
    \end{tabular}}
      \vspace{-0.85em}
    \caption{\textbf{Tubelet Transformation Hyperparameters.} We change $\mathrm{Min}$ and $\mathrm{Max}$ values for tubelet transformations. Our model is robust to changes in these parameters, with all choices tested giving an improvement over no tubelet transformation.}
    \label{tab:ablation_transformations_range}
\end{table}

\vspace{-0.8em}
\section{Tubelets vs. Randomly Scaled Crops} 
\vspace{-0.5em}
To show that our proposed tubelets inject useful motions in the training pipeline, we compare them with  randomly scaled crops.  In particular, we randomly crop, scale, and jitter the patches pasted into the video clips when generating positive pairs and pretrain this and our model on Mini-Kinetics.  Table~\ref{tab:scaledcrops} shows that our proposed motion tubelets outperform such  randomly scaled crops in all downstream settings. This validates that the spatiotemporal continuity in motion tubelets is important to simulate smooth motions for learning better video representations. 
\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}
     {l  cccc}
    \toprule
      %\addlinespace[0.1cm]
         &   \multicolumn{1}{c}{UCF ($10^3$)} & \multicolumn{1}{c}{Gym ($10^3$)}  & \multicolumn{1}{c}{SSv2-Sub} & \multicolumn{1}{c}{UB-S1}  \\ 
         \midrule
         Randomly Scaled Crops & 59.5 & 37.5 & 44.8 & 87.0      \\
         Tubelets & 65.5 & 48.0 &47.9 &90.9       \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-0.85em}
    \caption{\textbf{Tubelets vs Randomly Scaled Crops}. Our tubelets generate smooth motions to learn better video representations than strongly jittered crops.}
\label{tab:scaledcrops}
\end{table}

\vspace{-0.9em}
\section{Per-Class Results}
\vspace{-0.3em}
Examining the improvement for individual classes gives us some insight into our model. Figure~\ref{fig:ucf_bar} shows the difference between our approach and the baseline for the 10 classes in UCF ($10^3$) with the highest increase and decrease in accuracy.  Many of the actions that increase in accuracy are motion-focused, \eg, pullups, lunges and jump rope. Other actions are confused by the baseline because of the similar background, \eg, throw discus is confused with hammer throw and apply eye makeup is confused with haircut. The motion-focused features our model introduces help distinguish these classes. However, our model does lose some useful spatial features for distinguishing classes such as band marching and biking.

\begin{figure}[t!]
\centering
\includegraphics[width=0.95\linewidth]{figures/ucf101_bar_graph.png}
\vspace{-1.0em}
\caption{\textbf{Per-Class Accuracy Difference} on UCF ($10^3$) between our model and the temporal contrastive baseline. We show the 10 actions with the highest increase and decrease. Our model can better distinguish classes requiring motion but loses some ability to distinguish spatial classes.}
\label{fig:ucf_bar}
\end{figure}

%\vspace{-5pt}
\vspace{-0.9em}
\section{Class Agnostic Activation Maps} 
\vspace{-0.3em}
Figure~\ref{fig:caam_supp} show more examples of class agnostic activation maps~\cite{CAAM} for video clips from various downstream datasets. Note that no finetuning is performed, we directly apply the representation from our tubelet contrastive learning pretrained on Kinetics-400. For examples from FineGym, Something Something v2, and UCF101, we observe that our approach attends to regions with motion while the temporal contrastive baseline mostly attends to background. 

\vspace{-0.9em}
\section{Limitations and Future Work}
\vspace{-0.5em}
There are several open avenues for future work based on the limitations of this work. First, while we compare to transformer-based approaches, we do not present the results of our tubelet-contrast with a transformer backbone. Our initial experiments with a transformer-based encoder~\cite {dosovitskiy2020image} did not converge with off-the-shelf settings.  We hope future work can address this problem for an encoder-independent solution.
Additionally, we simulate tubelets with random image crops that can come from both background and foreground regions. Explicitly generating tubelets from foreground regions or pre-defined objects is a potential future direction worth investigating. Finally, we only simulate tubelets over short clips, it is also worth investigating whether long-range tubelets can be used for tasks that require long-range motion understanding.


\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\linewidth]{figures/grad_cam_supp.pdf}
\caption{\textbf{Class-Agnostic Activation Maps Without Finetuning} for the temporal contrastive baseline and our tubelet contrast for different downstream datasets.  Our model better attends to regions with motion irrespective of the domain. }
\label{fig:caam_supp}
\end{figure*}

%\begin{table*}[t]
%\captionsetup{font=small,skip=2mm}
%         \caption[]{\textbf{Proposed SEVERE-benchmark} for evaluating video self-supervised methods for generalization along downstream domains, samples, actions and tasks.
%    }
%    \centering
%    \midsepremove
%    \resizebox{\linewidth}{!}{\begin{tabular}
%    {
%    %l\C{77.3}{93.9}
%    lc\C{52.0}{60.8}\C{89.9}{92.1}@{\hskip 2mm}c
%    \C{38.3}{86.6}\C{22.7}{51.3}@{\hskip 2mm}c
%    \C{46.6}{79.1}\C{80.9}{88.8}@{\hskip 2mm}c
%    \CR{0.123}{0.217}\C{7.9}{23.5}@{\hskip 2mm}c\C{51.0}{66.6}\CR{4.1}{11.6}
%    }
%    % {lc cc ccccccc}
%    \toprule
%    \addlinespace[0.1cm]
%     %& \multicolumn{1}{Sc}{} & & \multicolumn{11}{Sc}{\textbf{SEVERE-benchmark}} \\
%    \addlinespace[0.04cm]
%    %\cmidrule{2-2} \cmidrule{4-14}
%      \addlinespace[0.1cm]
%         \multicolumn{1}{l}{}  & & \multicolumn{2}{Sc}{\textbf{Domains}} &  & \multicolumn{2}{Sc}{\textbf{Samples}} & & \multicolumn{2}{Sc}{\textbf{Actions}}& & \multicolumn{2}{Sc}{\textbf{Tasks}} \\
%    %   \cmidrule{4-11}
%      \cmidrule(lr){3-4} \cmidrule(lr){6-7} \cmidrule(lr){9-10} \cmidrule(lr){12-13}
%      
%      \addlinespace[0.1cm]
%         &   & \multicolumn{1}{c}{SS-v2} & \multicolumn{1}{c}{Gym-99} & & \multicolumn{1}{c}{UCF ($10^{3}$)} & \multicolumn{1}{c}{Gym ($10^{3}$)} & & \multicolumn{1}{c}{FX-S1 } & \multicolumn{1}{c}{UB-S1}& & \multicolumn{1}{c}{UCF-RC$\downarrow$} & \multicolumn{1}{c}{Charades}
%         && \multicolumn{1}{Sc}{\textbf{Mean}} & \multicolumn{1}{Sc}{\textbf{Rank$\downarrow$}}\\
%         \midrule
%           \addlinespace[0.01cm]
%         SVT~\cite{svt}   && 59.2   & 62.3   && 83.9   & 18.5   &&35.4   & 55.1 && 0.421 & 35.5   &&  51.0                     & 8.9            \\
%         VideoMAE~\cite{tong2022videomae}       && 69.7   & 85.1   &&77.2   & 27.5   &&37.0   & 78.5 &&0.172 & 12.6   &&      58.1 & 8.3            \\
%         \midrule
%         Supervised~\cite{tran2018closer}                  && 60.8   & 92.1   && 86.6   & 51.3   && 79.0   & 87.1   && 0.132   &  23.5 &&70.9  & 3.9  \\
%         \midrule
%         None                        && 57.1   & 89.8   && 38.3  & 22.7    && 46.6   & 82.3 && 0.217 & 7.9       &&52.9  &11.6  \\
%           \addlinespace[0.01cm]
%         SeLaVi~\cite{selavi}                      && 56.2   & 88.9   && 69.0   & 30.2   && 51.3   & 80.9   && 0.162   & 8.4 &&58.6   & 11.0  \\
%        \addlinespace[0.01cm]
%         MoCo~\cite{moco}                        && 57.1   & 90.7   && 60.4   & 30.9   && 65.0   & 84.5   && 0.208   & 8.3  && 59.5                       & 9.1            \\
%         VideoMoCo~\cite{videomoco-pan2021videomoco}                   && 59.0   & 90.3   && 65.4   & 20.6   && 57.3   & 83.9   && 0.185   & 10.5   && 58.6                       & 9.1            \\
%         Pre-Contrast~\cite{pretext-contrast}            && 56.9   & 90.5   & &64.6   & 27.5   && 66.1   & 86.1   && 0.164   & 8.9  && 60.5                      & 9.0            \\
%         AVID-CMA~\cite{avid-cma-morgado2021audio}                    && 52.0   & 90.4   && 68.2   & 33.4   && 68.0   & 87.3   && 0.148   & 8.2    &&61.6                       & 9.0            \\
%         GDT~\cite{gdt-patrick2020multimodal}                         && 58.0   & 90.5   && 78.4   & 45.6   && 66.0   & 83.4   && 0.123   & 8.5  &&64.8                         & 8.6           \\
%         RSPNet~\cite{rspnet-chen2020RSPNet}                      && 59.0   & 91.1   && 74.7   & 32.2   && 65.4   & 83.6   && 0.145   & 9.0   &&62.6                       & 8.0           \\
%         TCLR~\cite{dave2022tclr}                        && 59.8   & 91.6   && 72.6   & 26.3   && 60.7   & 84.7   && 0.142   & 12.2    &&61.7              & 7.6            \\
%         CtP~\cite{ctp-wang2021unsupervised}                         && 59.6   & 92.0   && 61.0   & 32.9   && 79.1   & 88.8   && 0.178   & 9.6    && 63.2                     & 5.6            \\
%         \midrule
%        \textbf{\textit{This paper}}                      &&{60.2}   &{92.8}   &&{65.7}   & 47.0   &&{80.1}   &{91.0} &&{0.150}  &{10.3}   &&{66.6}  & {4.1}    \\ 
%        %\addlinespace[0.01cm]
%         \bottomrule
%    \end{tabular}
%    }
%
%    \label{proposed-benchmarks}
%\end{table*}

        


