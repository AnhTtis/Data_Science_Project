
\vspace{-0.6em}
\subsection{Standard Evaluation: UCF101 and HMDB51}
\vspace{-0.5em}
We first show the effectiveness of our proposed method on standard coarse-grained action recognition benchmarks UCF101 and HMBD51, where we compare with prior video self-supervised works. For a fair comparison, we only report methods in Table~\ref{tab:ucf_hmdb_sota} that use the R(2+1)D-18 backbone and Kinetics-400 as the pretraining dataset. 

%\noindent\textbf{RGB-only Comparison.} 
First, we observe that %among the self-supervised methods that only use the RGB modality for pretraining, 
our method obtains the best results for UCF101 and HMDB51. % with R(2+1)D-18 backbone. 
 The supplementary material shows we also achieve similar improvement with the R3D and I3D backbones. In particular, with R(2+1)D our method beats CtP~\cite{ctp-wang2021unsupervised} by 2.6\% and 2.4\%, TCLR~\cite{dave2022tclr} by 2.8\% and 4.1\%, and TE~\cite{jenni2021time_eqv} by 2.8\% and 1.9\% all of which aim to learn finer temporal representations.   This confirms that explicitly contrasting tubelet-based motion patterns   
 results in a better video representation than learning temporal distinctiveness or prediction.  We also outperform FAME~\cite{fame} by 6.2\% and 9.6\% on UCF101 and HMDB51. FAME aims to learn a motion-focus representation by pasting the foreground region of one video onto the background of another to construct positive pairs for contrastive learning. We however are not limited by the motions present in the set of pretraining videos as we simulate new motion patterns for learning. We also outperform prior multi-modal works which incorporate audio or explicitly learn motion from optical flow. 
Since our model is data-efficient, we can pretrain on Mini-Kinetics and still outperform all baselines which are trained on the 3x larger Kinetics-400. 

 \begin{table}[t!]
\centering
    \setlength{\tabcolsep}{6pt}
\resizebox{0.92\linewidth}{!}{
    \begin{tabular}{llcc}
        \toprule
        \textbf{Method} & \textbf{Modality} & UCF101 & HMDB51\\
         %& & UCF101 & HMDB51 \\\hline
%         \midrule
        
        \midrule
        %Pace Prediction~\cite{pace} & RGB &   77.1 & 36.6 \\
        %MoCo~\cite{moco} & R(2+1)D & Kinetics-400 & 32 & V &  78.7 & 49.2 \\
        VideoMoCo~\cite{videomoco-pan2021videomoco} & RGB &   78.7 & 49.2 \\
        RSPNet~\cite{rspnet-chen2020RSPNet} & RGB  &  81.1 &44.6 \\
        SRTC~\cite{srtc} & RGB &    82.0 & 51.2 \\
        FAME~\cite{fame}  & RGB &    {84.8}	& {53.5} \\
        MCN~\cite{mcn} & RGB     & 84.8 & 54.5 \\
        \color{gray}
        AVID-CMA~\cite{avid-cma-morgado2021audio} & \color{gray} RGB+Audio & \color{gray} 87.5 & \color{gray} 60.8 \\
        TCLR~\cite{dave2022tclr} & RGB &   88.2 &60.0 \\
        TE~\cite{jenni2021time_eqv} & RGB & 88.2 &	62.2\\	
        CtP~\cite{ctp-wang2021unsupervised} & RGB & {88.4}	& {61.7} \\
        \color{gray}
        MotionFit~\cite{motion_fit} & \color{gray} RGB+Flow &   \color{gray} 88.9 & \color{gray} 61.4 \\ 
        \color{gray} GDT~\cite{gdt-patrick2020multimodal} & \color{gray} RGB+Audio &  \color{gray} 89.3 & \color{gray} 60.0 \\
        
        \midrule
        %\rowcolor{visualblue}
        \textbf{\textit{This paper $^\dagger$}} & RGB &  \textbf{90.7}	& \textbf{65.0} \\
        %\rowcolor{visualblue}
        \textbf{\textit{This paper}} & RGB & \textbf{91.0}	& \textbf{64.1} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.9em}
    \caption{\textbf{Standard Evaluation: UCF101 and HMDB51} using R(2+1)D. 
    %All models utilize Kinetics-400 for self-supervised pretraining. 
    Gray lines indicate use of additional modalities during self-supervised pretraining. Note that our method pretrained on Mini-Kinetics ($\dagger$) outperforms all methods which pretrain on the 3${\times}$ larger Kinetics-400.
    }
    \vspace{-1em}
    \label{tab:ucf_hmdb_sota}
\end{table}

\begin{table*}[t!]
    \centering
    \midsepremove
    \setlength{\tabcolsep}{5pt}
    \resizebox{\linewidth}{!}{\begin{tabular}
     {l c ccc cc ccccc}
    \toprule
    
      \addlinespace[0.1cm]
        & & \multicolumn{2}{Sc}{\textbf{Domains}}   & \multicolumn{2}{Sc}{\textbf{Samples}}  & \multicolumn{2}{Sc}{\textbf{Actions}} & \multicolumn{2}{Sc}{\textbf{Tasks}}\\
      \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
         %\multicolumn{1}{l}{\textbf{Methods}}  \\ 
      \addlinespace[0.1cm]
         &  \multicolumn{1}{c}{Backbone} & \multicolumn{1}{c}{SSv2} & \multicolumn{1}{c}{Gym99}  & \multicolumn{1}{c}{UCF ($10^{3}$)} & \multicolumn{1}{c}{Gym ($10^{3}$)}  & \multicolumn{1}{c}{FX-S1 } & \multicolumn{1}{c}{UB-S1}&  \multicolumn{1}{c}{UCF-RC$\downarrow$} &  \multicolumn{1}{c}{Charades} & \textbf{Mean } & \textbf{Rank$\downarrow$}\\
         \midrule
           %\addlinespace[0.01cm]
        \color{gray} SVT~\cite{svt} & \color{gray}ViT-B  & \color{gray}59.2   & \color{gray}62.3   & \color{gray}83.9   & \color{gray}18.5   & \color{gray}35.4   & \color{gray}55.1 & \color{gray}0.421 & \color{gray}35.5   &  \color{gray}51.0                     & \color{gray}~8.9            \\
        \color{gray} VideoMAE~\cite{tong2022videomae}  &\color{gray}ViT-B           & \color{gray}69.7   & \color{gray}85.1   & \color{gray}77.2   & \color{gray}27.5   & \color{gray}37.0   & \color{gray}78.5 & \color{gray}0.172 & \color{gray}12.6   &                     \color{gray}58.1 & \color{gray}~8.3            \\
 %\color{gray}        MME~\cite{sun2023mme}     &\color{gray}ViT-B          & \color{gray}70.5   & \color{gray}91.7   & \color{gray}84.4   & \color{gray}41.4   & \color{gray}69.7   & \color{gray}90.1   & \color{gray}0.174 & \color{gray}23.6   &  \color{gray}69.3                    & -           \\
         \midrule
         Supervised~\cite{tran2018closer}  &R(2+1)D-18             & 60.8   & 92.1   & 86.6   & 51.3   & 79.0   & 87.1 & 0.132 & 23.5   &  70.9                     & 3.9            \\
         \midrule
         None   &R(2+1)D-18                  & 57.1   & 89.8   & 38.3   & 22.7   & 46.6   & 82.3 & 0.217 & ~7.9    &  52.9                         &11.6            \\
         SeLaVi~\cite{selavi}   &R(2+1)D-18               & 56.2   & 88.9   & 69.0   & 30.2   & 51.3   & 80.9 & 0.162 & ~8.4    & 58.6                      & 11.0           \\
         MoCo~\cite{moco}     &R(2+1)D-18                & 57.1   & 90.7   & 60.4   & 30.9   & 65.0   & 84.5 & 0.208 & ~8.3    & 59.5                       & ~9.1            \\
         VideoMoCo~\cite{videomoco-pan2021videomoco}  &R(2+1)D-18              & 59.0   & 90.3   & 65.4   & 20.6   & 57.3   & 83.9 & 0.185 & 10.5   & 58.6                       & ~9.1            \\
         Pre-Contrast~\cite{pretext-contrast}   &R(2+1)D-18      & 56.9   & 90.5   & 64.6   & 27.5   & 66.1   & 86.1 & 0.164 & ~8.9    & 60.5                      & ~9.0            \\
         AVID-CMA~\cite{avid-cma-morgado2021audio}  &R(2+1)D-18               & 52.0   & 90.4   & 68.2   & 33.4   & 68.0   & 87.3 & 0.148 & ~8.2    & 61.6                       & ~9.0            \\
         GDT~\cite{gdt-patrick2020multimodal}  &R(2+1)D-18                    & 58.0   & 90.5   & \textbf{78.4}   & 45.6   & 66.0   & 83.4 & \textbf{0.123} & ~8.5    & 64.8    & ~8.6           \\
         RSPNet~\cite{rspnet-chen2020RSPNet}    &R(2+1)D-18               & 59.0   & 91.1   & 74.7   & 32.2   & 65.4   & 83.6 & 0.145 & ~9.0    & 62.6                       & ~8.0           \\
                  TCLR~\cite{dave2022tclr}    &R(2+1)D-18                 & 59.8   & 91.6   & 72.6   & 26.3   & 60.7   & 84.7 & 0.142 & \textbf{12.2}   & 61.7              & ~7.6            \\
         CtP~\cite{ctp-wang2021unsupervised}   &R(2+1)D-18                   & 59.6   & 92.0   & 61.0   & 32.9   & 79.1   & 88.8 & 0.178 & ~9.6    & 63.2                     & ~5.6            \\
         \midrule                                                                  \addlinespace[0.03cm]                          %                  \rowcolor{visualblue}       
         %\rowcolor{audiogreen}
         \textbf{\textit{This paper $^\dagger$}}    &R(2+1)D-18                  & {59.4}   & {92.2}   & {65.5}   & \textbf{48.0}   & {78.3}   & 90.9 & {0.150}  & {9.0}   & 66.0  &~5.4    \\
                  \textbf{\textit{This paper}}  &R(2+1)D-18                    & \textbf{60.2}   & \textbf{92.8}   & {65.7}   & 47.0   & \textbf{80.1}   & \textbf{91.0} & {0.150}  & {10.3}   & \textbf{66.5}  & ~\textbf{4.1}    \\
        \addlinespace[0.01cm]
         \bottomrule
    \end{tabular}
    }
    \vspace{-0.9em}
         \caption{\textbf{SEVERE Generalization Benchmark.}
         Comparison with prior self-supervised methods for generalization to downstream domains, fewer samples, action granularity, and tasks. $\downarrow$ indicates lower is better. Results for baselines are taken from SEVERE \cite{thoker2022severe}.
         Our method generalizes best, even when using the 3x smaller Mini-Kinetics dataset ($\dagger$) for pretraining.}
    \label{tab:severe-performance}
    \vspace{-1.0em}
\end{table*}
 

\vspace{-0.5em}
\subsection{SEVERE Generalization Benchmark} 
\vspace{-0.5em}
Next, we compare to prior works on the challenging SEVERE benchmark \cite{thoker2022severe}, which evaluates video representations for generalizability in \textit{domain shift}, \textit{sample efficiency}, \textit{action granularity}, and \textit{task shift}. We follow the same setup as in the original SEVERE benchmark and use an R(2+1)D-18 backbone pretrained on Kinetics-400 with our tubelet-contrast before finetuning on the different downstream settings.
Results are shown in Table~\ref{tab:severe-performance}.




\noindent \textbf{Domain Shift.} Among the evaluated methods our proposal achieves the best results on SSv2 and Gym99. These datasets differ considerably from Kinetics-400, particularly in regard to the actions,  environment and viewpoint. Our improvement demonstrates that the representation learned by our tubelet-contrast is robust to various domain shifts. 

\vspace{-0.05em}
\noindent \textbf{Sample Efficiency.}
For sample efficiency, we achieve a good gain over all prior works on Gym ($10^{3}$), \eg, +20.7\% over TCLR~\cite{dave2022tclr} and +14.1\%  over CtP~\cite{ctp-wang2021unsupervised}. Notably, the gap between the second best method GDT~\cite{gdt-patrick2020multimodal} and all others is large, demonstrating the challenge. For UCF ($10^{3}$), our method is on par with VideoMoCo\cite{videomoco-pan2021videomoco} and CtP but is outperformed by GDT and RSPNet~\cite{rspnet-chen2020RSPNet}. %\etc. 
This is likely due to most actions in UCF101 requiring more spatial than temporal understanding, thus it benefits from the augmentations used by GDT and RSPNet. Our motion-focused representation  requires more finetuning samples on such datasets. 

\vspace{-0.05em}
\noindent \textbf{Action Granularity.} For fine-grained actions in FX-S1 and UB-S1, our method achieves the best performance, even outperforming supervised Kinetics-400 pretraining.
 We achieve a considerable improvement over other RGB-only models, \eg, +19.6\% and +6.3\% over TCLR, as well as audio-visual models, \eg, +14.1\% and +7.6\% over GDT. 
These results demonstrate that the video representation learned by our method are better suited  to fine-grained actions than existing self-supervised methods. We additionally report results on  Diving48~\cite{diving48} in the supplementary. 


\vspace{-0.05em}
\noindent \textbf{Task Shift.} 
For the task shift to repetition counting, our method is on par with AVID-CMA~\cite{avid-cma-morgado2021audio} and RSPNet, but worse than GDT. For multi-label action recognition on Charades, our approach is 3rd, comparable to VideoMoCo but worse than TCLR.
 This suggests the representations learned by our method are somewhat transferable to tasks beyond single-label action recognition. However, the remaining gap between supervised and self-supervised highlights the need for future work to explore task generalizability further.
 
\vspace{-0.05em}
\noindent \textbf{Comparison with Transformers.} Table~\ref{tab:severe-performance} also contains recent transformer-based self-supervised works SVT~\cite{svt} and VideoMAE~\cite{tong2022videomae}. We observe that both SVT and VideoMAE have good performance with large amounts of finetuning data (SSv2), in-domain fine-tuning (UCF($10^3$)), and multi-label action recognition (Charades). However, they considerably lag in performance for motion-focused setups Gym99, FX-S1, UB-S1, and repetition counting compared to our tubelet contrast with a small CNN backbone. 
 %Our method also often outperforms MME~\cite{sun2023mme} which extracts  motion information from optical flow and HOG features for pretraining.

\vspace{-0.05em}
\noindent \textbf{Overall SEVERE Performance.} 
Finally, we compare the mean and the average rank across all generalizability factors. Our method has the best mean performance (66.5) and achieves the best average rank (4.1). When pretraining with the 3x smaller Mini-Kinetics our approach still achieves impressive results. We conclude our method improves the generalizability of video self-supervised representations across these four downstream factors while being data-efficient.

