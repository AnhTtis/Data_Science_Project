\begin{table}
    \centering
    \setlength{\tabcolsep}{2pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc} \toprule  
    %Pre-training& UCF ($10^{3}$) & Gym ($10^{3}$) &UB-S1 & SSv2-Sub\\
    & UCF ($10^{3}$) & Gym ($10^{3}$) & SSv2-Sub & UB-S1 \\
         \midrule
         \rowcolor{Gray}
         \textbf{Temporal Contrast} & & & &\\
         Baseline    & 57.5 & 29.5        & 44.2  & 84.8      \\
         \rowcolor{Gray}
         \textbf{Tubelet Contrast} & & & &\\
         Tubelet Generation & 48.2  &28.2    & 40.1  & 84.1      \\
         Tubelet Motion
          & 63.0 & 45.6          & 47.5  & 90.3      \\
         Tubelet Transformation  & 65.5 & 48.0 & 47.9  & 90.9      \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
    \caption{\textbf{Tubelet-Contrastive Learning} considerably outperforms temporal contrast on multiple downstream settings. Tubelet motion and transformations are key. % 
    }
    \label{tab:ablation_main}
    %\vspace{-0.5em}
\end{table}

\vspace{-0.3em}
\subsection{Ablation Studies \& Analysis}
\vspace{-0.3em}
To ablate the effectiveness of individual components we rely on Mini-Kinetics for self-supervised pretraining. For downstream evaluation, we report on UCF ($10^{3}$), Gym ($10^{3}$), Something-Something v2 and UB-S1. To decrease the finetuning time we use a subset of Something Something (SSv2-Sub) with 25\% of the training data. Unless specified otherwise, we use non-linear motion and the rotation transformation to simulate the tubelets.

\noindent\textbf{Tubelet-Contrastive Learning.}
Table~\ref{tab:ablation_main} demonstrates the benefits brought by our tubelet-contrastive learning. We first observe that our full tubelet-contrastive model improves considerably over the temporal contrastive baseline, which uses MoCo~\cite{moco} with a temporal crop augmentation. This improvement is independent of the downstream dataset used but is especially observable with Gym ($10^{3}$) (+18.5\%) and UB-S1 (+6.1\%) where temporal cues are more crucial than the spatial cues to recognize actions. Our model is also effective on UCF ($10^{3}$) (+8.0\%) where spatial cues are often as important as temporal ones. These results demonstrate that learning similarities between synthetic tubelets produces generalizable, but motion-focused, video representations required for finer temporal understanding.

It is clear that the motion within tubelets is critical to our model's success as contrasting static tubelets obtained from our tubelet generation (Section~\ref{section:tbcl}) actually  decreases the performance from the temporal contrast baseline. 
%This demonstrates that tubelet motion is key to learning effective video representations. 
When tubelet motion is added (Section~\ref{section:motions}), performance improves considerably, \eg Gym ($10^{3}$) +17.4\% and SSv2-Sub +7.4\%. 
Finally, adding more motion types via tubelet transformations (Section~\ref{section:transformations}) further improves the video representation quality, \eg UCF ($10^{3}$) +2.5\% and  Gym ($10^{3}$) +2.4\%. This highlights the importance of including a variety of motions beyond what is present in the pretraining data to learn generalizable video representations. %

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc} \toprule Tubelet Motion & UCF ($10^{3}$) & Gym ($10^{3}$) &  SSv2-Sub & UB-S1  \\
         \midrule
         No motion & 48.2  &28.2      & 40.1 & 84.1 \\
         Linear   & 55.5 & 34.6       & 45.3 & 88.5 \\
         Non-Linear   & 63.0 & 45.6   & 47.5 & 90.3 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
    \caption{\textbf{Tubelet Motions.} Learning from tubelets with non-linear motion benefits multiple downstream settings.}
    \label{tab:ablation_motions}
    %\vspace{-0.8em}
\end{table}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.98\linewidth]{figures/data-eff.png}
\vspace{-1em}
\caption{\textbf{Video-Data Efficiency of Tubelet-Contrastive Learning.} Our approach maintains performance when using only 25\% of the pretraining data. When using 5\% of the pretraining data, our approach is still more effective than using 100\% with the baseline for Gym($10^3$), UB-S1, and HMDB51. Results are averaged over three pretraining runs with different seeds.%
\vspace{-1em}
}
\label{fig:data_efficiency}
\end{figure*}

 \begin{table}
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{lcccc} \toprule Transformation & UCF ($10^{3}$) & Gym ($10^{3}$)  & SSv2-Sub &UB-S1\\
         \midrule
         None   & 63.0 & 45.6 & 47.5 & 90.5    \\
         Scale  & 65.1 & 46.5 & 47.0 & 90.5   \\
         Shear  & 65.2 & 47.5 & 47.3 & 90.9   \\
         Rotation & 65.5 & 48.0 & 47.9 & 90.9   \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
    \caption{\textbf{Tubelet Transformation.} Adding motion patterns to tubelet-contrastive learning through transformations improves downstream performance. Best results for rotation.}
    \label{tab:ablation_transformations}
    %\vspace{-0.5em}
\end{table}


\begin{table}
    \centering
    \resizebox{0.88\linewidth}{!}{
    \begin{tabular}{lcccc}
         \toprule \#Tubelets & UCF ($10^{3}$) & Gym ($10^{3}$)  & SSv2-Sub & UB-S1\\
         \midrule
         1 & 62.0 & 39.5 &  47.1 & 89.5 \\
         2 & 65.5 & 48.0 &  47.9 & 90.9 \\
         3 & 66.5 & 46.0 &  47.5 & 90.9 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
    \caption{\textbf{Number of Tubelets.} Overlaying two tubelets in positive pairs improves downstream performance. %
    }
    \label{tab:ablation_num_tubelets}
    \vspace{-0.8em}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/grad_cam.pdf}
\vspace{-2em}
\caption{\textbf{Class-Agnostic Activation Maps without Finetuning} for the temporal contrastive baseline and our tubelet-contrast.  Our model better attends to regions with motion. }
\vspace{-1em}
\label{fig:caam}
\end{figure}

\noindent\textbf{Tubelet Motions.} Next, we ablate the impact of the  tubelet motion type (Section~\ref{section:motions}) without transformations. We compare the performance of static tubelets with no motion, linear motion, and non-linear motion in Table~\ref{tab:ablation_motions}. Tubelets with simple linear motion already improve performance for all four datasets, \eg +6.4\% on Gym ($10^3$). %\cs{highlight one result?}. 
Using non-linear motion further improves results, for instance with an additional +11.0\% improvement on Gym ($10^3$). %\cs{highlight another result?}. 
We conclude that learning from non-linear motions provides more generalizable video representations. 
%


\noindent\textbf{Tubelet Transformation.}
Table~\ref{tab:ablation_transformations} compares the three proposed tubelet transformations (Section~\ref{section:transformations}). % 
All four datasets benefit from transformations. We find that scaling improves results on UCF ($10^3$) (+2.1\%) and Gym ($10^3$) (+0.9\%) while maintaining performance on UB-S1 and SSv2-Sub. Shearing gives better performance over scaling on all datasets, \eg, Gym ($10^3$) (+1.0\%) however, performance for SSv2-Sub is still similar to using no transformation. Rotation is the most effective transformation, increasing performance on all four datasets.
These differences are likely due to the types of motion present in the downstream datasets. For instance, Gym ($10^3$) and UB-S1 contain gymnastic videos where actors are often spinning and turning but do not change in scale due to the fixed camera, therefore rotation is more helpful than scaling. %
We also experiment with combinations of transformations in supplementary and observe that it does not give further improvement. %Thus,  we choose rotation as the tubelet transformation for our final model.   





\noindent\textbf{Number of Tubelets.} %
We investigate the number of tubelets overlaid on each video in Table~\ref{tab:ablation_num_tubelets}. One tubelet is already more effective than temporal contrastive learning, \eg 29.5\% vs. 39.5\% for Gym ($10^3$). Adding two tubelets improves accuracy on all datasets, \eg +8.5\% for Gym ($10^3$). %

\noindent\textbf{Qualitative Analysis. } %\cs{This takes too much space, can easily be shortened.}
To understand what our model learns Figure~\ref{fig:caam} visualizes class agnostic activation maps~\cite{CAAM} without finetuning for the temporal contrastive baseline and our approach. % 
Without previously seeing any FineGym data, our approach attends to motions. The temporal contrastive baseline instead attends to the background. % 

\vspace{-0.3em}
\subsection{Video-Data Efficiency} 
\vspace{-0.3em}
To demonstrate our method's data efficiency, we pretrain using subsets of the Kinetics-400 training data. In particular, we sample $5\%,10\%,25\%,33\% \text{ and } 50\%$ of the Kinetics-400 training set with three different seeds and use this to pretrain our model and the temporal contrastive baseline. %We repeat the experiment three times to sample different subsets of data with different seeds. 
We compare the effectiveness of these representations after finetuning on UCF ($10^{3}$), Gym($10^{3}$), SSv2-Sub, UB-S1, and HMDB51 in Figure~\ref{fig:data_efficiency}.
%
On all downstream setups, our method maintains similar performance when reducing the pretraining data to just 25\%, while the temporal contrastive baseline performance decreases significantly. Our method is less effective when using only 5\% or 10\% of the data. Remarkably, with this little data, our model still outperforms the baseline with 100\% for Gym ($10^3$), UB-S1, and HMDB. We attribute our model's data efficiency to the tubelets we add to the pretraining data. In particular, our non-linear motion and transformations generate a variety of synthetic tubelets that 
simulate a greater variety of fine-grained motions than are present in the original data. 
%when overlaid onto different backgrounds steers the model to focus on learning fine-grained motion patterns while ignoring shortcuts apparent in the background.




