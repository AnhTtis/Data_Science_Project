


\begin{table}[t!]
\centering
    \setlength{\tabcolsep}{6pt}
\resizebox{0.92\linewidth}{!}{
    \begin{tabular}{llcc}
        \toprule
        \textbf{Method} & \textbf{Modality} & UCF101 & HMDB51\\
        
        \midrule
        Pace Prediction~\cite{pace} & RGB &   77.1 & 36.6 \\
        VideoMoCo~\cite{videomoco-pan2021videomoco} & RGB &   78.7 & 49.2 \\
        RSPNet~\cite{rspnet-chen2020RSPNet} & RGB  &  81.1 &44.6 \\
        SRTC~\cite{srtc} & RGB &    82.0 & 51.2 \\
        FAME~\cite{fame}  & RGB &    {84.8}	& {53.5} \\
        MCN~\cite{mcn} & RGB     & 84.8 & 54.5 \\
        \color{gray}
        AVID-CMA~\cite{avid-cma-morgado2021audio} & \color{gray} RGB+Audio & \color{gray} 87.5 & \color{gray} 60.8 \\
        TCLR~\cite{dave2022tclr} & RGB &   88.2 &60.0 \\
        TE~\cite{jenni2021time_eqv} & RGB & 88.2 &	62.2\\	
        CtP~\cite{ctp-wang2021unsupervised} & RGB & {88.4}	& {61.7} \\
        \color{gray}
        MotionFit~\cite{motion_fit} & \color{gray} RGB+Flow &   \color{gray} 88.9 & \color{gray} 61.4 \\ 
        \color{gray} GDT~\cite{gdt-patrick2020multimodal} & \color{gray} RGB+Audio &  \color{gray} 89.3 & \color{gray} 60.0 \\
        
        \midrule
        %\rowcolor{visualblue}
        \textbf{\textit{This paper $^\dagger$}} & RGB &  \textbf{90.7}	& \textbf{65.0} \\
        %\rowcolor{visualblue}
        \textbf{\textit{This paper}} & RGB & \textbf{91.0}	& \textbf{64.1} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.8em}
    \caption{\textbf{Standard Evaluation: UCF101 and HMDB51} using R(2+1)D. 
    %All models utilize Kinetics-400 for self-supervised pretraining. 
    Gray lines indicate use of additional modalities during self-supervised pretraining. Note that our method pretrained on Mini-Kinetics ($\dagger$) outperforms all methods which pretrain on the 3${\times}$ larger Kinetics-400.
    }
    \vspace{-0.8em}
    \label{tab:ucf_hmdb_sota}
\end{table}

\begin{table*}[t!]
    \centering
    \midsepremove
    \setlength{\tabcolsep}{5pt}
    \resizebox{\linewidth}{!}{\begin{tabular}
     {l  ccc cc ccccc}
    \toprule
    
      \addlinespace[0.1cm]
        & \multicolumn{2}{Sc}{\textbf{Domains}}   & \multicolumn{2}{Sc}{\textbf{Samples}}  & \multicolumn{2}{Sc}{\textbf{Actions}} & \multicolumn{2}{Sc}{\textbf{Tasks}}\\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
         %\multicolumn{1}{l}{\textbf{Methods}}  \\ 
      \addlinespace[0.1cm]
         &   \multicolumn{1}{c}{SS-v2} & \multicolumn{1}{c}{Gym-99}  & \multicolumn{1}{c}{UCF ($10^{3}$)} & \multicolumn{1}{c}{Gym ($10^{3}$)}  & \multicolumn{1}{c}{FX-S1 } & \multicolumn{1}{c}{UB-S1}&  \multicolumn{1}{c}{UCF-RC$\downarrow$} &  \multicolumn{1}{c}{Charades-MLC} & \textbf{Mean } & \textbf{Avg Rank$\downarrow$}\\
         \midrule
           \addlinespace[0.01cm]
         Supervised~\cite{tran2018closer}               & 60.8   & 92.1   & 86.6   & 51.3   & 79.0   & 87.1 & 0.132 & 23.5   &  18.0                     & -            \\
         \midrule
         None                     & 57.1   & 89.8   & 38.3   & 22.7   & 46.6   & 82.3 & 0.217 & ~7.9    &  -                         &11.1            \\
         SeLaVi~\cite{selavi}                  & 56.2   & 88.9   & 69.0   & 30.2   & 51.3   & 80.9 & 0.162 & ~8.4    & ~5.7                       & ~9.3            \\
         MoCo~\cite{moco}                     & 57.1   & 90.7   & 60.4   & 30.9   & 65.0   & 84.5 & 0.208 & ~8.3    & ~6.6                       & ~8.5            \\
         VideoMoCo~\cite{videomoco-pan2021videomoco}                & 59.0   & 90.3   & 65.4   & 20.6   & 57.3   & 83.9 & 0.185 & 10.5   & ~5.6                       & ~8.1            \\
         Pretext-Contrast~\cite{pretext-contrast}         & 56.9   & 90.5   & 64.6   & 27.5   & 66.1   & 86.1 & 0.164 & ~8.9    & ~7.6                       & ~7.5            \\
         AVID-CMA~\cite{avid-cma-morgado2021audio}                 & 52.0   & 90.4   & 68.2   & 33.4   & 68.0   & 87.3 & 0.148 & ~8.2    & ~8.7                       & ~6.6            \\
         GDT~\cite{gdt-patrick2020multimodal}                      & 58.0   & 90.5   & \textbf{78.4}   & 45.6   & 66.0   & 83.4 & \textbf{0.123} & ~8.5    & 11.8    & ~5.4            \\
         RSPNet~\cite{rspnet-chen2020RSPNet}                   & 59.0   & 91.1   & 74.7   & 32.2   & 65.4   & 83.6 & 0.145 & ~9.0    & ~9.6                       & ~5.3            \\
         CtP~\cite{ctp-wang2021unsupervised}                      & 59.6   & 92.0   & 61.0   & 32.9   & 79.1   & 88.8 & 0.178 & ~9.6    & 10.2                      & ~4.9            \\
         TCLR~\cite{dave2022tclr}                     & 59.8   & 91.6   & 72.6   & 26.3   & 60.7   & 84.7 & 0.142 & \textbf{12.2}   & ~8.8              & ~4.6            \\
         \midrule                                                                  \addlinespace[0.03cm]                          %                  \rowcolor{visualblue}       
         %\rowcolor{audiogreen}
         \textbf{\textit{This paper $^\dagger$}}                      & {59.4}   & {92.2}   & {65.5}   & \textbf{48.0}   & {78.3}   & 90.9 & {0.150}  & {9.0}   & 13.1  &~3.6    \\
                  \textbf{\textit{This paper}}                      & \textbf{60.2}   & \textbf{92.8}   & {65.7}   & 47.0   & \textbf{80.1}   & \textbf{91.0} & {0.150}  & {10.3}   & \textbf{13.6}  & ~\textbf{2.5}    \\
        \addlinespace[0.01cm]
         \bottomrule
    \end{tabular}
    }
    \vspace{-0.8em}
         \caption{\textbf{SEVERE Generalization Benchmark.}
         Comparison with prior self-supervised methods for generalization to downstream domains, fewer samples, action granularity, and tasks. $\downarrow$ indicates lower is better. Results for baselines are taken from SEVERE \cite{thoker2022severe}.
         Our method generalizes best, even when using the 3x smaller Mini-Kinetics dataset ($\dagger$) for pretraining.}
    \label{tab:severe-performance}
    \vspace{-0.5em}
\end{table*}

\vspace{-0.3em}
\subsection{Standard Evaluation: UCF101 and HMDB51}
\vspace{-0.3em}
We first show the effectiveness of our proposed method on standard coarse-grained action recognition benchmarks UCF101 and HMBD51, where we compare with prior video self-supervised works. For a fair comparison, we only report methods in Table~\ref{tab:ucf_hmdb_sota} that use the R(2+1)D-18 backbone and Kinetics-400 as the pretraining dataset. 

%\noindent\textbf{RGB-only Comparison.} 
First, we observe that %among the self-supervised methods that only use the RGB modality for pretraining, 
our method obtains the best results for UCF101 and HMDB51. % with R(2+1)D-18 backbone. 
 The supplementary material shows we also achieve similar improvement with the R3D backbone. In particular, with R(2+1)D our method beats CtP~\cite{ctp-wang2021unsupervised} by 2.6\% and 2.4\%, TCLR~\cite{dave2022tclr} by 2.8\% and 4.1\%, and TE~\cite{jenni2021time_eqv} by 2.8\% and 1.9\% all of which aim to learn finer temporal representations.   This confirms that explicitly contrasting tubelet-based motion patterns   
 results in a better video representation than learning temporal distinctiveness or prediction. %explicitly contrasting motion patterns in the form of tubelets  results in learning much better temporal representations than learning to track patches (CtP) or temporal distinctiveness (TCLR) or temporal equivariance (TE).
%Notably, FAME~\cite{fame} shares the same motivation as ours, be it they learn motion-aware representations 
We also outperform FAME~\cite{fame} by 6.2\% and 9.6\% on UCF101 and HMDB51. FAME, like our method, aims to learn a motion-focus representation.
It does this by pasting the foreground region of one video onto the background of another to construct positive pairs for contrastive learning. We however are not limited by the motions present in the set of pretraining videos as we simulate new motion patterns for learning. We also outperform prior multi-modal works which incorporate audio or explicitly learn motion from optical flow. %
Since our model is data-efficient, we can pretrain on Mini-Kinetics and still outperform all baselines which are trained on the 3x larger Kinetics-400. 


\vspace{-0.3em}
\subsection{SEVERE Generalization Benchmark} 
\vspace{-0.3em}
Next, we compare to prior works on the challenging SEVERE benchmark \cite{thoker2022severe}, which evaluates self-supervised video representations for generalizability along \textit{domain shift}, \textit{sample efficiency}, \textit{action granularity}, and \textit{task shift}. We strictly follow the same setup as in the original SEVERE benchmark using the author-provided codebase. We use an R(2+1)D-18 backbone pretrained on Kinetics-400 with our tubelet-contrastive learning before finetuning on the different downstream settings.
%
The results are shown in Table~\ref{tab:severe-performance}.




\noindent \textbf{Domain Shift.} Among the evaluated methods our proposal achieves the best performance on SS-v2 and Gym-99. These datasets differ considerably from Kinetics-400, particularly in regard to the actions they contain, the environment and the viewpoint. Our improvement demonstrates that the representation from our tubelet-contrastive learning is robust to a variety of domain shifts. %
\noindent \textbf{Sample Efficiency.}
For the sample efficiency, we achieve a considerable improvement over all prior works on Gym ($10^{3}$), \eg +20.7\%  over TCLR, +14.8\%  over RSPNet and  +14.1\%  over CtP~\cite{ctp-wang2021unsupervised}. Notably, the gap between the second best method GDT~\cite{gdt-patrick2020multimodal} and all other works is large, demonstrating the challenge. For UCF ($10^{3}$), our method is on par with VideoMoCo\cite{videomoco-pan2021videomoco} and CtP~\cite{ctp-wang2021unsupervised} but is outperformed by GDT and RSPNet. %\etc. 
This is likely because most actions in UCF101 require more spatial than temporal understanding, thus it benefits from the additional augmentations used by GDT and RSPNet. Our motion-focused representation  requires more examples for finetuning on such datasets. 

\noindent \textbf{Action Granularity.} For fine-grained actions in FX-S1 and UB-S1, our method achieves the best performance, even outperforming supervised Kinetics-400 pre-training (top row).
 We achieve a considerable improvement over other RGB-only models, \eg, +19.6\% and +6.3\% over TCLR, as well as audio-visual models, \eg, +14.1\% and +7.6\% over GDT. 
These results demonstrate that the video representation learned by our method are better suited  to fine-grained actions than existing self-supervised methods. We additionally report results on  Diving48~\cite{diving48} in the supplementary. 


\noindent \textbf{Task Shift.} 
For the task shift to repetition counting, our method is on par with AVID-CMA and RSPNet, but worse than GDT. For multi-label action recognition on Charades, our approach is 3rd, comparable to VideoMoCo but worse than TCLR.
 %
 %
 This suggests the video representations learned by our method are somewhat transferable to tasks beyond single-label action recognition. However, the remaining gap between supervised and self-supervised highlights the need for future work to explore task generalizability further.

\noindent \textbf{Overall SEVERE Performance.} 
Finally, we compare the average rank and average improvement over no pretraining for all methods across all generalizability factors. Our method achieves the  best average rank (2.5) and has the best mean improvement (13.6). When pretraining with the 3x smaller Mini-Kinetics our method still achieves impressive results. We conclude our method improves the generalizability of video self-supervised representations across these four downstream factors while being data-efficient.

