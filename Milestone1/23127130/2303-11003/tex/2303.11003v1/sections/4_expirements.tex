\vspace{-0.3em}
\section{Experiments}
\vspace{-0.3em}
\subsection{Datasets, Evaluation \& Implementation}
\vspace{-0.3em}

\noindent\textbf{Pretraining Datasets. } 
Following prior self-supervised works \cite{videomoco-pan2021videomoco,gdt-patrick2020multimodal,dave2022tclr,ctp-wang2021unsupervised,rspnet-chen2020RSPNet,huang2021ascnet}  we use \textbf{Kinetics-400}~\cite{Kinetics-400-arxiv} for self-supervised pretraining. Kinetics-400 is a large-scale action recognition dataset containing 250K videos for 400 action classes.
To show data efficiency, we also utilize \textbf{Mini-Kinetics}~\cite{mini_kinetics} for pretraining. This is a subset of Kinetics-400 containing 85K videos of 200 action classes. %For both cases, we use the training set without action labels for self-supervised pretraining. 

\noindent \textbf{Downstream Evaluation. } 
To evaluate the video representations learned by our tubelet contrast, we finetune and evaluate our model on various downstream datasets, as summarized in Table~\ref{tab:severe_desc}.
Following previous self-supervised works, we evaluate on the standard benchmarks: \textbf{UCF101}~\cite{UCF-101-arxiv} and \textbf{HMDB51}~\cite{HMDB-51-ICCV}. These are action recognition datasets containing coarse-grained actions with domains similar to Kinetics-400. 
%UCF101 contains 13,320 videos from 101 action classes while HMDB51 contains 6,890 videos from 51 action classes. 
For both, we report top-1 accuracy on the first standard split from the original papers.
%
We examine the generalizability of our model with the \textbf{SEVERE} benchmark~\cite{thoker2022severe}. This consists of eight experiments across four downstream generalization factors: \textit{domain shift}, \textit{sample efficiency}, \textit{action granularity}, and \textit{task shift}. %SEVERE contains two experimental setups for each factor.
\textit{Domain shift} is evaluated on action recognition with Something-Something v2~\cite{SS-v2-arxiv} (SSv2) and FineGym~\cite{Gym-99-arxiv} %(referred to as Gym-99)  
which vary in domain with respect to Kinetics-400. 
\textit{Sample efficiency} evaluates low-shot action recognition on UCF101~\cite{UCF-101-arxiv} and FineGym~\cite{Gym-99-arxiv} with only 1,000 training samples, referred to as UCF ($10^{3}$) and Gym ($10^{3}$).
\textit{Action granularity} evaluates semantically similar actions using FX-S1 and UB-S1 subsets from  FineGym~\cite{Gym-99-arxiv}. In both subsets, action classes belong to the same element of a gymnastic routine, \eg, FX-S1 is types of jump.  
\textit{Task shift} evaluates tasks beyond single-label action recognition. Specifically, it uses temporal repetition counting on UCFRep~\cite{ucfrep-zhang2020context}, a subset of UCF-101~\cite{ucfrep-zhang2020context}, and multi-label action recognition on Charades~\cite{charades-sigurdsson:hal-01418216}. %where videos are an average of 30 seconds long. 
The experimental setups are detailed in Table~\ref{tab:severe_desc} and all follow SEVERE~\cite{thoker2022severe}. %

\noindent\textbf{Tubelet Generation and Transformation.} Our clips are 16 $112 {\times} 112$ frames with standard spatial augmentations: random crops, horizontal flip, and color jitter. %Accordingly, we set $T{=}16$, $W{=}112$ and $H{=}112$ for tubelet generation.  
We randomly crop 2 patches to generate $M{=}2$ tubelets (Eq.~\ref{eq:overlay_M_tubelets}). The patch size $H^{'}{\times} W^{'}$ is uniformly sampled from $[16{\times}16, 64{\times64}]$. We also randomly sample a patch shape from a set of predefined shapes. For linear motions, we use  $\Delta{=}[40{-}80]$ displacement difference. For non-linear motion, we use $N{=}48$ and a smoothing factor of $\sigma{=}6$ (Eq.~\ref{eq:nonlinear_motion}). For linear motion and all tubelet transformations, we use $K{=}3$ keyframes.

\noindent\textbf{Networks, Pretraining and Finetuning.}
We use R(2+1)D-18~\cite{tran2018closer} as the video encoder, following common practice in previous self-supervised learning research~\cite{pace,gdt-patrick2020multimodal,rspnet-chen2020RSPNet,fame,dave2022tclr,videomoco-pan2021videomoco}. The projection head is a 2-layer MLP with an output dimension of 128. We make use of momentum contrast~\cite{moco} to increase the set of negatives $\mathcal{N}$ (Eq.~\ref{eq:tcl}) to size 16,384 by storing embeddings from previous batches in a dynamic queue. %
We use temperature $\tau{=}0.2$ (Eq.~\ref{eq:tcl}). The model is optimized using SGD with a momentum of 0.9, an initial learning rate of 0.01, and a weight decay of 0.0001. We use a batch size of 32, a cosine scheduler~\cite{cosine_sch}, and pretrain for 100 epochs. After pretraining, we replace the projection head with a task-dependent head following SEVERE~\cite{thoker2022severe}. The whole network is finetuned for the  downstream task with labels. We provide   downstream finetuning and evaluation details in the supplementary. Code will be released.