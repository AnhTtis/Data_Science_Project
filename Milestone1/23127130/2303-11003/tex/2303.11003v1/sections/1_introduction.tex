\vspace{-0.5em}
\section{Introduction}
\vspace{-0.3em}
This paper aims to learn self-supervised video representations, useful for distinguishing action classes. In a community effort to reduce the manual, expensive, and hard-to-scale annotations needed for many downstream deployment settings, the topic has witnessed tremendous progress in recent years~\cite{odd,xu2019self,simon, schiappa2022self}, particularly through contrastive learning~\cite{cvrl,videomoco-pan2021videomoco,rspnet-chen2020RSPNet,large-scale-feichtenhofer2021large}. Contrastive approaches learn representations through instance discrimination~\cite{infonce}, where the goal is to increase feature similarity between spatially and temporally augmented clips from the same video. Despite temporal differences, such positive video pairs often maintain high spatial similarity (see Figure~\ref{fig:clips}), meaning the contrastive task can be solved by coarse-grained features without needing to explicitly capture local motion dynamics. This limits the generalizability of the learned video representations to a wide variety of downstream settings~\cite{thoker2022severe}. Furthermore, prior approaches are constrained by the amount and types of motions present in the pretraining video data. This makes them data-hungry, as video data has high redundancy with periods of little to no motion. In this work, we address the need for data-efficient and generalizable self-supervised video representations by proposing a contrastive method to learn local motion dynamics. 


\begin{figure}[t!]
\centering
\includegraphics[width=0.95\linewidth]{figures/concept-1.pdf}
\vspace{0.8em}
\caption{\textbf{Tubelet-Contrastive Positive Pairs} (bottom) only share the spatiotemporal motion dynamics inside the simulated tubelets, while temporal contrastive pairs (top) suffer from a high spatial bias. Contrasting tubelets results in a data-efficient and generalizable video representation. 
}
\label{fig:clips}
\vspace{-0.8em}
\end{figure}
 
We take inspiration from action detection literature, where tubelets are used to represent the motions of people and objects in videos through sequences of bounding boxes %commonly used to represent the sequence of bounding boxes encompassing actors and their actions as they appear and move throughout a video
\eg,\cite{jain2014action,kalogeiton2017action,li2020actions}. Typically, many tubelet proposals are generated for a video, which are processed to find the best prediction. %fitting one for the classification objective of interest. 
Rather than finding tubelets in the video data, we simulate them. %, similar to seminal work on learning optical flow~\cite{dosovitskiy2015flownet}. 
In particular, we sample an image patch and `paste' it with a randomized motion onto two different video clips as a shared tubelet (see Figure~\ref{fig:clips}). These two clips form a positive pair for contrastive learning where the model has to rely on the spatiotemporal dynamics of the tubelet to learn the similarity. With such a formulation, we can simulate a large variety of motion patterns that are not present in the original videos. This allows our model to be data-efficient while improving downstream generalization to new domains and fine-grained actions. 


We make four contributions. First, we explicitly learn from local motion dynamics in the form of synthetic tubelets and design a simple but effective tubelet-contrastive framework. Second, we propose different ways of simulating tubelet motion and transformation to generate a variety of motion patterns for learning. Third, we reveal the remarkable data efficiency of our proposal: on five action recognition datasets our approach maintains performance when using only 25\% of the pretraining videos. What is more, with only 5-10\% of the videos we still outperform the vanilla contrastive baseline with 100\% pretraining data for several datasets. Fourth, our comparative experiments on 10 downstream settings, including UCF101~\cite{UCF-101-arxiv}, HMDB51~\cite{HMDB-51-ICCV}, 
Something Something~\cite{SS-v2-arxiv}, and FineGym~\cite{Gym-99-arxiv}, %and the complete SEVERE Benchmark~\cite{thoker2022severe}, 
further demonstrate our competitive performance, generalizability to new domains, and suitability of our learned representation for fine-grained actions.



