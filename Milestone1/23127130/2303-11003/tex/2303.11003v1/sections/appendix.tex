\clearpage
\noindent\textbf{\Large{Appendix}}
%\begin{center}
%\textbf{Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization\\Supplementary Material}
%\end{center}
% \begin{equation}
%\mathcal{L}(V1,V2) = -\log \frac{\displaystyle \mathrm{exp}(Z_{v_{1}^{'}}\cdot Z_{v_{2}^{'}}/\tau)}
%{ \displaystyle\mathrm{exp}(Z_{v_{1}^{'}}\cdot Z_{v_{2}^{'}}/\tau)   + \displaystyle\sum_{Z_{v_{n}^{'}} \sim \mathcal{N}} \mathrm{exp}(Z_{v_{1}^{'}}\cdot Z_{v_{n}^{'}}/\tau)},
%\label{eq:3}
%\end{equation}
\section{Generalization on Diving48.} 
To further highlight the generalizability of our method to new domains and fine-grained actions, we finetune and evaluate with the challenging Diving48 dataset~\cite{diving48}. %\cite{fame,background_removing,dave2022tclr}. 
It contains 18K trimmed videos for 48 different diving sequences all of which take place in similar backgrounds and need to be distinguished by subtle differences such as the number of somersaults or the starting position. We use standard train/test split and report top-1 accuracy.

In Table~\ref{tab:diving}, we show the performance of our model when pretrained on the full Kinetics-400 and on Mini-Kinetics ($\dagger$). We compare these results to no pretraining, the temporal contrastive baseline pretrained on Kinetics-400, and supervised pretraining on Kinetics-400 with labels.  Our method increases the performance  over training from scratch by 7.9\% and the temporal contrastive baseline by 6.6\%. 
 Our method even outperforms the supervised pre-training baseline by 4.5\%. This suggests that by contrasting tubelets with different motions, our method is able to learn better video representations for fine-grained actions than supervised pretraining on Kinetics.
 When pretraining on Mini-Kinetics (3x smaller than Kinetics-400) the performance of our model does not decrease, again demonstrating the data efficiency of our approach.



\section{Standard Evaluation with R3D}
In addition to our comparison with prior works on the standard UCF101~\cite{UCF-101-arxiv} and HMDB51~\cite{HMDB-51-ICCV} datasets with the R(2+1) backbone,  here we also show a comparison with the R3D-18 backbone. Table~\ref{tab:R3D} shows the results with  Kinetics-400 as the pretraining dataset.  
Similar to R(2+1)D results, we outperform prior works using the RGB modality on UCF101. We also achieve comparable performance to the best-performing method on HMDB51, outperforming the next best method by 6.3\%.  %
On HMDB51 we also outperform prior works which pretrain on an additional optical flow modality and achieve competitive results with these methods on UCF101.
%

\begin{table}
    \centering
    \resizebox{0.6\linewidth}{!}{
    \begin{tabular}{lc}
         \toprule Pretraining &  Top-1\\
         \midrule
         Supervised~\cite{tran2018closer} & 84.5  \\
         \midrule
         None & 81.1 \\
         Temporal Contrast Baseline &  82.4 \\
         \midrule
         \textbf{\textit{This paper}}$^{\dagger}$ & \textbf{89.4} \\
         \textbf{\textit{This paper}}  & \textbf{89.0} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.5em}
    \caption{ Comparison with temporal contrastive pretraining and supervised pretraining on \textbf{Diving48}. All models use R(2+1)D-18. $\dagger$ indicates pretraining on Mini-Kinetics, all pretraining was done on Kinetics.}
    \label{tab:diving}
    \vspace{-0.5em}
\end{table}
\begin{table}[t!]
\centering
\resizebox{0.75\linewidth}{!}{
    \begin{tabular}{lllcc}
        \toprule
        \textbf{Method} & \textbf{Modality} & UCF & HMDB\\
         %& & UCF101 & HMDB51 \\\hline
         \midrule
        %\rowcolor{Gray}
        %$^\dagger$SeLaVi~\cite{selavi}& RGB+Audio & R(2+1)D &   83.1 & 47.1 \\
        %\rowcolor{Gray}
        %$^\dagger$XDC~\cite{alwassel_2020_xdc} & RGB+Audio & R(2+1)D  &  86.8 & 52.6 \\
        %\rowcolor{Gray}
        VideoMoCo~\cite{videomoco-pan2021videomoco} & RGB &  74.1 & 43.6 \\
        RSPNet~\cite{rspnet-chen2020RSPNet} & RGB &   74.3 &41.6 \\
        LSFD~\cite{lsfd} & RGB & 77.2 & 53.7 \\
        MLFO~\cite{mlfo} & RGB &  79.1 & 47.6 \\
        ASCNet~\cite{huang2021ascnet} & RGB &   80.5 & 52.3 \\
        MCN~\cite{mcn} & RGB    & 85.4 & 54.8 \\
        TCLR~\cite{dave2022tclr} & RGB&   85.4 &55.4 \\
        CtP~\cite{ctp-wang2021unsupervised} & RGB&  86.2 & 57.0 \\
        %BE~\cite{background_removing} & RGB & R3D-34 & 87.1 & 55.4\\ 
        TE~\cite{jenni2021time_eqv} & RGB &   87.1 & \textbf{63.6} \\
       \color{gray}
        MSCL~\cite{mscl} & \color{gray} RGB+Flow & \color{gray}   90.7 & \color{gray} 62.3 \\
        \color{gray}
        MaCLR~\cite{xiao2022maclr} & \color{gray} RGB+Flow &  \color{gray} 91.3 & \color{gray} 62.1 \\ \midrule
%        \rowcolor{visualblue}
        \textbf{\textit{This paper}} & RGB&   \textbf{90.1}	& {63.3} \\
        \bottomrule

        \end{tabular}}
      \vspace{-0.5em}    
    \caption{\textbf{Standard Evaluation: UCF101 and HMDB51} using the R3D-18 backbone. 
    %
    Gray lines indicate the use of additional modalities during self-supervised pretraining. %
    }
    \label{tab:R3D}
        \end{table}


\begin{table*}[t]
    \centering
    %\setlength{\tabcolsep}{4pt}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lllllll}
    \toprule
    \textbf{Evaluation Factor} & \textbf{Experiment} & \textbf{Dataset} & \textbf{Batch Size} & \textbf{Learning rate} & \textbf{Epochs} & \textbf{Steps} \\ % & \textbf{Eval Metric}\\
    \midrule

    \multirow{2}{*}{\textbf{Standard}} & UCF101 & UCF 101~\cite{UCF-101-arxiv} & 32&  0.0001  & 160 & [60,100,140] \\
 & HMDB51 & HMDB 51~\cite{HMDB-51-ICCV} & 32&  0.0001  & 160 & [60,100,140]  \\
    \midrule
    \multirow{2}{*}{\textbf{Domain Shift}} & SS-v2 & Something-Something~\cite{SS-v2-arxiv} & 32&  0.0001 & 45  & [25, 35, 40] \\
     & Gym-99 & Fine Gym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [60,100,140] \\
    \midrule
    \multirow{2}{*}{\textbf{Sample Efficiency}}& UCF ($10^3$) & UCF 101~\cite{UCF-101-arxiv} &  32&  0.0001 & 160 & [80,120,140]  \\
     & Gym ($10^3$) & Fine Gym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [80,120,140]  \\
    \midrule
    \multirow{2}{*}{\textbf{Action Granularity}} & FX-S1 & Fine Gym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [70,120,140]  \\
     & UB-S1 & Fine Gym~\cite{Gym-99-arxiv} & 32&  0.0001 & 160 & [70,120,140]  \\
    \midrule
    \multirow{2}{*}{\textbf{Task Shift}} & UCF-RC & UCFRep~\cite{ucfrep-zhang2020context} & 32&  0.00005 & 100 & -  \\
     & Charades-MLC & Charades~\cite{charades-sigurdsson:hal-01418216} &  16&  0.0375 & 57 & [41,49]  \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
    \caption{\textbf{Training Details} of fine-tuning on various downstream datasets and tasks.}
    \label{tab:finetuning}
\end{table*}

\section{Finetuning Details} 
During finetuning we follow the setup from the SEVERE benchmark~\cite{thoker2022severe} which is detailed here for completeness. For all tasks, we replace the projection of the pretrained model with a task-dependent head. \\
\noindent\textbf{Action Recognition}. 
Downstream settings which examine domain shift, sample efficiency, and action granularity all perform action recognition. We use a similar fine-tuning process for all experiments on these three factors. During the training process, a random clip of 32 frames is taken from each video and standard augmentations are applied: a multi-scale crop of 112x112 size, horizontal flipping, and color jittering. The Adam optimizer is used for training, with the  learning rate, scheduling, and total number of epochs for each experiment shown in Table~\ref{tab:finetuning}. During inference, 10 linearly spaced clips of 32 frames each are used, with a center crop of 112x112. To determine the action class prediction for a video, the predictions from each clip are averaged. For domain shift and sample efficiency, we report the top-1 accuracy.
 For action granularity experiments we report mean class accuracy, which we obtain by computing accuracy per action class and averaging over all action classes. \\
\noindent\textbf{Repetition counting}. 
The  implementation follows the original repetition counting work proposed in  UCFrep work~\cite{ucfrep-zhang2020context}.
From the annotated videos, 2M sequences of 32 frames with spatial size 112x112 are constructed. These are  used as the input. The model is trained with a batch size of 32 for 100 epochs using the Adam optimizer with a learning rate of 0.00005. For testing, we report mean counting error following\cite{ucfrep-zhang2020context}. \\
\noindent\textbf{Multi-label classification on Charades}. Following \cite{large-scale-feichtenhofer2021large},
 a per-class sigmoid output is utilized for multi-class prediction. During the training process, 32 frames are sampled with a stride of 8. %Since this task requires a longer temporal context, using more frames with a higher sampling rate can improve the results. 
Frames are cropped to 112x112 and random short-side scaling, random spatial crop, and horizontal flip augmentations are applied. The model is trained for a total of 57 epochs with a batch size of 16 and a learning rate of 0.0375. A multi-step scheduler with $\gamma = 0.1$ is applied at epochs [41, 49]. During the testing phase,  spatiotemporal max-pooling is performed over 10 clips for a single video. We report mean average precision (mAP) across all classes.    

\begin{table}
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lcc} \toprule Transformation & UCF ($10^{3}$) & Gym ($10^{3}$)  \\
         \midrule
         None   &                           63.0 & 45.6      \\
         \midrule
         Scale  &                           65.1 & 46.5      \\
         Shear  &                           65.2 & 47.5      \\
         Rotate &                           65.5 & 48.0      \\
         \midrule  
         Scale + Shear  &                   65.2 & 46.0      \\
         Rotate + Scale   &                 65.4 & 46.9      \\
         Rotate + Shear   &                 65.3 & 45.7      \\
         Rotate + Scale + Shear  &          65.6 & 46.0      \\
        \bottomrule
    \end{tabular}}
      \vspace{-0.5em}
    \caption{\textbf{Tubelet Transformation Combinations.} Combining transformations doesn't give a further increase in performance compared to using individual transformations. }
    \label{tab:ablation_transformations_combination}
\end{table}

%\section{Ablations on combining Transformations.} 
\section{Tubelet Transformation Hyperparameters.} 

Table~\ref{tab:ablation_transformations_combination} shows the results when applying multiple tubelet transformations in the tubelet generation. While applying individual transformations improves results, combing multiple transformations doesn't improve the performance further. This is likely because rotation motions are common in the downstream datasets while scaling and shearing are less common. %such transformations result in simulating motion with realistic transformations which do not resemble the motion of objects/persons in downstream videos. 

Table~\ref{tab:ablation_transformations_range} shows an ablation over $\mathrm{Min}$ and $\mathrm{Max}$ values for tubelet transformations.  In the main paper, we use scale values between 0.5 and 1.5, shear values between -1.0 and 1.0, and rotation values between -90 and 90. Here, we experiment with values that result in more subtle and extreme variations of these transformations.  We observe that all values for each of the transformations improve over no transformation. Our model is reasonably robust to these choices in hyperparameters, but subtle variations \eg  scale change between 0.5 to 1.25 or shear from 0.75 to 0.75 tend to be slightly less effective. %We also observe that performance can be improved be extending the  range of these hyperparameters \eg rotation between -180 to 180.

 

 \begin{table}
    \centering
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{llcc} \toprule Min & Max & UCF ($10^{3}$) & Gym ($10^{3}$)  \\
         \midrule
         %\hdashline
         \rowcolor{Gray}
         \textbf{None} & & & \\
         - & - &                            63.0 & 45.6 \\
         \rowcolor{Gray}
         \textbf{Scale}  &  & &\\
         0.5 & 1.25  &                  65.6 & 45.3      \\
         0.5 & 1.5  &                   65.1 & 46.5      \\
         0.5 & 2.0  &                   65.6 & 46.0      \\
         %\midrule
         \rowcolor{Gray}
         \textbf{Shear}  &  & &\\
         -0.75 & 0.75  &                   64.4 &   47.5    \\
         -1.0 & 1.0  &                   65.2 & 48.0      \\
         -1.5 & 1.5  &                     65.2 &   47.5      \\
         %\midrule
         \rowcolor{Gray}
         \textbf{Rotation}  &  & &\\
         -45 & 45                        &    65.2 & 49.3      \\
         -90 & 90                       &    65.5 & 48.0      \\
         -180 & 180                        &    65.6 & 49.6     \\  
        \bottomrule
    \end{tabular}}
      \vspace{-0.5em}
    \caption{\textbf{Tubelet Transformation Hyperparameters.} We change $\mathrm{Min}$ and $\mathrm{Max}$ values for tubelet transformations. Our model is robust to changes in these parameters, with all choices tested giving an improvement over no tubelet transformation.}
    \label{tab:ablation_transformations_range}
\end{table}

\section{Class Agnostic Activation Maps} 
Figure~\ref{fig:caam_supp} show more examples of class agnostic activation maps~\cite{CAAM} for video clips from various downstream datasets. Note that no fine-tuning is performed, we directly apply the representation from our tubelet contrastive learning pretrained on Kinetics-400. For examples from FineGym, Something Something v2, and UCF101, we observe that our approach attends regions with motion while the temporal contrastive baseline mostly attends to the background. % 

\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\linewidth]{figures/grad_cam_supp.pdf}
\caption{\textbf{Class-Agnostic Activation Maps Without Finetuning} for the temporal contrastive baseline and our tubelet contrast for different downstream datasets.  Our model better attends to regions with motion irrespective of the domain. }
\label{fig:caam_supp}
\end{figure*}
