
\vspace{-0.3em}
\section{Tubelet Contrast}
\vspace{-0.3em}
We aim to learn motion-focused video representations from RGB video data with self-supervision. We first revisit temporal contrastive learning. We then propose tubelet-contrastive learning to reduce the spatial focus of video representations and instead learn similarities between spatiotemporal tubelet dynamics (Section~\ref{section:tbcl}). We encourage our learned representation to be motion-focused by simulating a variety of tubelet motions (Section~\ref{section:motions}). To further improve the data efficiency and generalizability of our method, we add complexity and variety to the motions through tubelet transformations (Section~\ref{section:transformations}). Figure~\ref{fig:framework} shows an overview of our approach.

%\subsection{Video-Contrastive Learning}
%\label{section:tcl}
\noindent\textbf{Temporal Contrastive Learning.} Temporal contrastive learning aims to learn feature representations via instance discrimination \cite{infonce}. This is achieved by maximizing  similarity between augmented clips from the same video (positive pairs) and minimizing similarity  between clips from different videos (negatives). Concretely given a set of videos $V$, the positive pairs $(v,v')$ are obtained by sampling different temporal crops of the same video~\cite{videomoco-pan2021videomoco,rspnet-chen2020RSPNet} and applying spatial augmentations such as cropping and color jittering. Clips sampled from other videos in the training set act as negatives. The extracted clips are passed through a video encoder and projected on a representation space by a non-linear projection head to obtain clip embeddings $(Z_{v},Z_{v'})$. The noise contrastive estimation loss InfoNCE \cite{infonce} is used for the optimization: 
%%%%%%% %%%%%%%%%%%
%\vspace{-0.5em}
\begin{equation}
\resizebox{0.9\linewidth}{!}{
$ \mathcal{L}_{contrast}{(v, v')} = -\log \frac{\displaystyle h(Z_{v},Z_{v'})}
{ \displaystyle h(Z_{v},Z_{v'})+\displaystyle \sum_{Z_{n} \sim \mathcal{N}} h(Z_{v},Z_{n})} $
}
\label{eq:tcl}
\vspace{-0.5em}
\end{equation}
%
where $h(Z_{v},Z_{v'}) {=} \mathrm{exp}(Z_{v}\cdot Z_{v'}/\tau)$, $\tau$ is the temperature parameter and $\mathcal{N}$ is a set of negative clip embeddings. %\hd{Add sentence on why this loss/way of contrasting is insufficient for us.}


\subsection{{Tubelet-Contrastive Learning}}
\vspace{-0.3em}
\label{section:tbcl}
Different from existing video contrastive self-supervised methods, we explicitly aim to learn motion-focused video representations while relying only on RGB data. To achieve this we propose to learn similarities between simulated tubelets.  Concretely, we first generate tubelets in the form of moving patches which are then overlaid onto two different video clips to generate positive pairs that have a high motion similarity and a low spatial similarity. Such positive pairs are then employed to learn video representations via instance discrimination, allowing us to learn more generalizable and motion-sensitive video representations.


\noindent\textbf{Tubelet Generation.} 
We define a tubelet as a sequence of object locations in each frame of a video clip. Let's assume an object $p$ of size ${H}'\times{W}'$ moving in a video clip $v$ of length $T$. Then the tubelet is defined as follows:
\vspace{-0.3em}
\begin{equation}
\mathrm{Tubelet}_{p} = [(x^1,y^1) ,..,(x^T,y^T)],
\label{eq:tubelet_def}
\vspace{-0.5em}
\end{equation}
 where $(x^i,y^i)$  is the center coordinate of the object $p$ in frame $i$ of clip $v$. % such that $0<x^i<W$ and $<0<y^i<H$, $W$ and $H$ is is width and height of the clip $v$. 
 For this work, a random image patch of size ${H}'\times{W}'$ acts as a pseudo-object overlaid on a video clip to form a tubelet. To generate the tubelet we first make the object appear static, \ie $x^1{=}x^2{=}...{=}x^T$ and $y^1{=}y^2 = ...{=}y^T$, and explain how we add motion in Section~\ref{section:motions}. % we explain how we define the tubelet to give the pseudo-object motion.
 
\noindent\textbf{Tubelet-Contrastive Pairs.} To create contrastive tubelet pairs, we first randomly sample clips $v_1$ and $v_2$ of size $H{\times}W$ and length $T$ from two different videos in $V$. From $v_1$ we randomly crop an image patch $p$ of size ${H}' \times {W}'$.  such that ${H}' \ll H$  and  ${W}' \ll W$.  
From the patch $p$, we  construct a tubelet $\mathrm{Tubelet}_{p}$ as in Eq.~\eqref{eq:tubelet_def}.
Then, we overlay the generated tubelet $\mathrm{Tubelet}_{p}$ onto both $v_1$ and $v_2$ to create two modified video clips $\hat{v}_{1}$ and $\hat{v}_{2}$: %\cs{There is a lot of white space around equations, probably caused by split command, pls fix.}
%\vspace{-0.6em}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
\begin{align}
 &\hat{v}_{1} = v_{1} \odot \mathrm{Tubelet}_{p} \quad
 &\hat{v}_{2} = v_{2} \odot \mathrm{Tubelet}_{p},
\label{eq:overlay_single_tubelet}
\end{align}
where $\odot$ refers to pasting patch $p$ in each video frame at locations determined by $\mathrm{Tubelet}_{p}$. Eq.~\eqref{eq:overlay_single_tubelet} can be extended for a set of $M$ tubelets $\{\mathrm{Tubelet}_{p_1},...,\mathrm{Tubelet}_{p_M}\}$ from $M$ patches randomly cropped from $v_1$ as:
\begin{align}
 \hat{v}_{1} = v_{1} \odot \{\mathrm{Tubelet}_{p_1},...,\mathrm{Tubelet}_{p_M}\} \nonumber \\
 \hat{v}_{2} = v_{2} \odot \{\mathrm{Tubelet}_{p_1},...,\mathrm{Tubelet}_{p_M}\}.
\label{eq:overlay_M_tubelets}
\end{align}
As a result, $\hat{v}_{1}$ and $\hat{v}_{2}$ share the spatiotemporal dynamics of the moving patches in the form of tubelets while also having low spatial biases since the two clips come from different videos.  %\cs{Figure~\ref{fig:clips} shows examples of $v_{1}'$ and $v_{2}'$.}
%
Finally, we adapt the video contrastive learning loss from Eq.~\eqref{eq:tcl} and apply $\mathcal{L}_{contrast}(\hat{v}_1, \hat{v}_2)$. 
In this case, the set of negatives $\mathcal{N}$ contains videos with different tubelets. Since the only similarity between positive pairs is the tubelets, the network needs to rely on temporal cues resulting in a motion-focused video representation. 

 


\vspace{-0.3em}
\subsection{Tubelet Motion}
\vspace{-0.3em}
\label{section:motions}
To learn motion-focused video representations, we need to give our tubelets a variety of motions. In this section, we discuss how to simulate motions by generating different patch movements in the tubelets. Recall, Eq.~\eqref{eq:tubelet_def} defines the tubelet by the image patch $p$ and the patch's center coordinate in each frame of the video clip.  We consider two ways of generating tubelet motions: linear and non-linear.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/motions3.pdf}
\vspace{-2.2em}
\caption{ \textbf{Tubelet Motion.} Examples for \textit{Linear} (left) and \textit{Non-Linear} (right). Non-linear motions enable the simulation of a larger variety of motion patterns to learn from. 
}
\label{fig:traj}
\vspace{-0.9em}
\end{figure}

\noindent\textbf{Linear Motion.} %First, we aim to simulate motions where the patches move along a linear path.  
To give the tubelets motion, we randomly sample the center locations for the patch in $K$ keyframes: the first frame ($i{=}1$), the last frame ($i{=}T$),  and $K{-}2$ randomly selected frames. The coordinates for the keyframes are sampled from uniform distributions $x\in[0,W]$ and $y\in[0,H]$, where $W$ and $H$ are the video width and height. 
Center locations for the remaining frames $i\notin K$ are then linearly interpolated between the neighboring keyframes so we obtain the following linear motion definition:
%\begin{equation}
\begin{align}
\label{eq:linear_motion}
%\quad 
&  \mathrm{Tubelet}^{\mathrm{Lin}} = [(x^1,y^1),(x^2,y^2),...,(x^T,y^T)], \text{ s.t.} \\ 
& (x^i,y^i) = 
\begin{cases}
(\mathcal{U}(0,W),\mathcal{U}(0,H)), & \text{if } i \in K\\
\mathrm{Interp}((x^{k},y^{k}),(x^{k+1},y^{k+1})), & \text{otherwise} \nonumber
\end{cases}
\end{align}
where $\mathcal{U}$ is a function for uniform sampling, $k$ and $k{+}1$ are the neighboring keyframes to the frame $i$ and $\mathrm{Interp}$ gives a linear interpolation between these keyframes. %and $(x_{i}^{k},y_{i}^{k}),(x_{i}^{k+1},y_{i}^{k+1})$ are the center coordinates in these keyframes. 
To ensure smoothness, we constrain the difference between the center locations in neighboring keyframes to be less than $\Delta$ pixels.
This formulation results in tubelet motions where patches follow linear paths across the video frames. 
The left of Figure~\ref{fig:traj} shows examples of  such linear tubelet motions. 



\noindent \textbf{Non-Linear Motion.} 
Tubelets with linear motions are simple and limit the variety of motion patterns that can be generated.  Next, we aim to simulate motions where the patches move along more complex non-linear paths, to better simulate the motions found in real videos. In contrast to linear movements, such paths generate a variety of more realistic motion patterns to learn from. 
%
To create non-linear tubelets motions we first sample $N$ 2D coordinates ($N \gg T$) from uniform distributions $x \in [0,W]$ and $y \in [0,H]$.  Then, we apply a $1D$ Gaussian filter along the $x$ and $y$ axes to generate a random smooth nonlinear path as: %\cs{sloppy equation, pls fix}
\begin{align}
\label{eq:nonlinear_motion}
\quad 
\mathrm{Tubelet}^{\mathrm{NonLin}} = &[(g(x^1),g(y^1)),...,(g(x^N),g(y^N))] \nonumber \\ 
\text{s.t. } \quad
&g(z) = \frac{1}{\sqrt[]{2 \pi } \sigma} e^{- {z}^{2}/{2 \sigma ^{2}}} %\ \text{where} \ x^i\in[0,W]\nonumber \\
%&y^i \in [0,H], \hspace{5pt} 
%&g(y^i) = \frac{1}{\sqrt[]{2 \pi } \sigma} e^{- {y^i}^{2}/{2 \sigma ^{2}}},
%\text{where} \ y^i\in[0,H] 
\end{align}
where $\sigma$ is the smoothing factor for the 1D gaussian kernels. %and $W,H$ is the width and height of the video clips. 
Note that it's important to sample $N\gg T$ points to ensure the non-linearity of the path. If $N$ is too small then the sampled path would be linear after gaussian smoothing. %
 We downsample the resulting non-linear tubelet in Eq.~\eqref{eq:nonlinear_motion} from $N$ to $T$ coordinates resulting in the center locations for the patch $p$ in the $T$ video clip frames.
The right side of Figure ~\ref{fig:traj}
shows examples such non-linear tubelet motions.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/transformations3.pdf}
\vspace{-2em}
\caption{\textbf{Tubelet Transformation.} Examples for
\textit{Scale} (left), \textit{Rotation} (middle), and \textit{Shear} (right). The patch is transformed as it moves through the tubelet.  
}
\label{fig:transformations}
\vspace{-1em}
\end{figure}



\begin{table*}[t]
    \centering
    %\setlength{\tabcolsep}{4pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llllrrrl}
    \toprule
    \textbf{Evaluation Factor} & \textbf{Experiment} & \textbf{Dataset} & \textbf{Task} & \textbf{\#Classes} & \textbf{\#Finetuning} & \textbf{\#Testing} & \textbf{Eval Metric}\\
    \midrule

    \multirow{2}{*}{\textbf{Standard}} & UCF101 & UCF 101~\cite{UCF-101-arxiv} & Action Recognition & 101 & 9,537 & 3,783 & Top-1 Accuracy\\
 & HMDB51 & HMDB 51~\cite{HMDB-51-ICCV} & Action Recognition & 51 & 3,570 & 1,530 & Top-1 Accuracy\\
    \midrule
    \multirow{2}{*}{\textbf{Domain Shift}} & SS-v2 & Something-Something~\cite{SS-v2-arxiv} & Action Recognition & 174 & 168,913 &  24,777 & Top-1 Accuracy\\
     & Gym-99 & Fine Gym~\cite{Gym-99-arxiv} & Action Recognition & 99 & 20,484 & 8,521 & Top-1 Accuracy\\
    \midrule
    \multirow{2}{*}{\textbf{Sample Efficiency}}& UCF ($10^3$) & UCF 101~\cite{UCF-101-arxiv} & Action Recognition & 101 & 1,000 & 3,783 & Top-1 Accuracy\\
     & Gym ($10^3$) & Fine Gym~\cite{Gym-99-arxiv} & Action Recognition & 99 & 1,000 & 8,521 & Top-1 Accuracy\\
    \midrule
    \multirow{2}{*}{\textbf{Action Granularity}} & FX-S1 & Fine Gym~\cite{Gym-99-arxiv} & Action Recognition & 11 & 1,882 & 777 & Mean Class Acc\\
     & UB-S1 & Fine Gym~\cite{Gym-99-arxiv} & Action Recognition & 15 & 3,511 & 1,471 & Mean Class Acc\\
    \midrule
    \multirow{2}{*}{\textbf{Task Shift}} & UCF-RC & UCFRep~\cite{ucfrep-zhang2020context} & Repetition Counting & - & 421 & 105 & Mean Error\\
     & Charades-MLC & Charades~\cite{charades-sigurdsson:hal-01418216} & Multi-label Recognition & 157 & 7,985 & 1,863 & mAP\\
    \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
    \caption{\textbf{Benchmark Details} for the  downstream evaluation factors, experiments and datasets we cover. For non-standard evaluations, we follow the SEVERE benchmark~\cite{thoker2022severe}. For self-supervised pre-training, we use Kinetics-400 or Mini-Kinetics.}
    \label{tab:severe_desc}
    \vspace{-0.8em}
\end{table*}

\subsection{Tubelet Transformation}
\vspace{-0.3em}
\label{section:transformations}
The tubelet motions are simulated by changing the position of the patch across the frames in a video clip, \textit{i.e.} with translation. 
In reality, the motion of objects in space may appear as other  transformations in videos, for instance, scale decreasing as the object moves away from the camera or motions due to planer rotations.  Motivated by this, we propose to add more complexity and variety to the simulated motions by transforming the tubelets. In particular, we propose the transformations: scale, rotation, and shear.
%
%Next, we propose to add more variety to the simulated motions with spatial changes in the moving patch.
As before, we sample multiple keyframes $K$ to apply transformations with the first ($i{=}0$) and last frames ($i{=}T$) always included. The transformations for the remaining frames are linearly interpolated between keyframes. 
Formally, we define a tubelet transformation as a sequence of spatial transformations applied to the patch $p$ in each frame $i$ as:
\begin{align}
\begin{split}
\label{eq:trans_general}
&  \mathrm{Trans}_{F} = [p,F(p,\theta^2),....,..,F(p,\theta^T)], \hspace{2pt}\text{ s.t. } \\ 
& \theta^i = 
\begin{cases}
\mathcal{U}(\mathrm{Min},\mathrm{Max}),  & \text{if } i \in K\\
\mathrm{Interp}(\theta^{k},\theta^{k+1}), & \text{otherwise}
\end{cases}
\end{split}
\end{align}
where $F(p,\theta^i)$ applies the transformation to patch $p$ according to parameters $\theta^i$, $\mathcal{U}$ samples from a uniform distribution and  $\theta^k$ and $\theta^{k{+}1}$ are the parameters for the keyframes neighboring frame $i$. % which are sampled randomly from the uniform distribution $\mathcal{U}(\mathrm{Min},\mathrm{Max})$.  %$\mathrm{Min},\mathrm{Max}$ represents the min and max values for the transformation factors.
%As before, multiple keyframes $K$ are selected randomly with the first ($i{=}0$) and  the last frame ($i{=}T$) always included.
For the first keyframe, no transformation is applied thus representing the initial state of the patch $p$. 
%For the other keyframes, $f^i$ is randomly sampled from $[Min, Max]$. 
%The transformation factors for the non-keyframes are linearly interpolated between the neighboring keyframes. 
We instantiate three types of such tubelet transformations: scale, rotation, and shear. Examples are shown in Figure~\ref{fig:transformations}.

\noindent \textbf{Scale.} We scale the patch across time with $F(p, \theta^i)$ and 
%Thus for scale tubelet transformation $F() = Resize()$ and the  transformation  factors $f^i = (w^i,h^i.)$ denote the target scale for the patch in frame $i$ with $ w^i$ being the width and $ h^i$ the height of the patch.
horizontal and vertical scaling factors $\theta^i{=}(w^i,h^i)$. 
To sample $w^i$ and $h^i$, we use $\mathrm{Min}{=}0.5$ and $\mathrm{Max}{=}1.5$. 

\noindent \textbf{Rotation.} In this transformation $F(p, \theta^i)$ applies in-plane rotations to tubelet patches.  
Thus, $\theta^i$ is a rotation angle sampled from $\mathrm{Min}{=}{-}90^{\circ}$ and $\mathrm{Max}{=}{+}90^{\circ}$. 

\noindent \textbf{Shear.} We shear the patch  as the tubelet progresses with $F(p, \theta^i)$.  
The shearing parameters are $\theta^i{=}(r^i,s^{i})$ which are sampled using $\mathrm{Min}{=}{-}1.5$ and $\mathrm{Max}{=}1.5$. 

With these tubelet transformations and the motions created in Section~\ref{section:motions} we are able to simulate a variety of subtle motions in videos, making the model data-efficient. By learning the similarity between the same tubelet overlaid onto different videos, our model pays less attention to spatial features, instead learning to represent these subtle motions. This makes the learned representation generalizable to different domains and action granularities.  
