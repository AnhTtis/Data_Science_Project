\vspace{-0.2em}
\section{Related Work}
\vspace{-0.3em}
\noindent\textbf{Self-Supervised Video Representation Learning.} 
The success of contrastive learning in images~\cite{moco, chen2020simple,grill2020bootstrap} inspired many to investigate video contrastive learning~\cite{cvrl,videomoco-pan2021videomoco,rspnet-chen2020RSPNet,huang2021ascnet,srtc,pretext-contrast}. Alongside spatial invariances, these works learn temporal invariances to  temporal crops~\cite{videomoco-pan2021videomoco,cvrl} and speed changes~\cite{rspnet-chen2020RSPNet,huang2021ascnet,srtc}. Some diverge from temporal invariances and encourage temporal equivariance~\cite{gdt-patrick2020multimodal,dave2022tclr} to learn  finer temporal representations. 
For instance, TCLR~\cite{dave2022tclr} enforces within-instance temporal feature variation, while TE~\cite{jenni2021time_eqv} learns equivariance to temporal crops and speed in a contrastive framework. Alternatively, many works learn to predict temporal transformations such as clip order~\cite{misra2016shuffle,xu2019self,sorting,odd}, speed~\cite{benaim2020speednet, cho2020self,prp} and their combinations~\cite{vcp,simon}. These self-supervised temporal  representations are effective for classifying and retrieving coarse-grained actions but are challenged by downstream settings with subtle motions \cite{thoker2022severe, schiappa2022self}.
Other works utilize the multimodal nature of videos~\cite{alwassel_2020_xdc,selavi,gdt-patrick2020multimodal,avid-cma-morgado2021audio,coclr,motion_fit,mac2019learning} and learn similarity with audio~\cite{selavi,alwassel_2020_xdc,avid-cma-morgado2021audio} and optical flow~\cite{coclr,motion_fit, mscl, xiao2022maclr}.  %\cs{Do you have another reference to back this up? Robustness CVPR23 paper by UCF-folks?}
%\cs{Add concluding sentence on what we do different in this paper.}
We contrast motions of synthetic tubelets to learn a video representation from only RGB data that can generalize to tasks requiring fine-grained motion understanding.

Other self-supervised works learn from the spatiotemporal dynamics of video.
Both BE~\cite{background_removing} and FAME~\cite{fame} remove background bias by adding static frames~\cite{background_removing} or replacing the background~\cite{fame} in positive pairs. 
Several works instead use masked autoencoding in transformers to learn video representations~\cite{tong2022videomae, feichtenhofer2022masked}.
However, all of these works are limited to the motion variation present in the pretraining dataset. We prefer to be less dataset-dependent and generate synthetic motion tubelets for contrastive learning, which also offers a considerable data-efficiency benefit.
%
CtP~\cite{ctp-wang2021unsupervised} and MoSI~\cite{motion_static} both aim to predict motions to the training data. CtP~\cite{ctp-wang2021unsupervised} learns to track image patches in video clips to focus on local motion features while MoSI~\cite{motion_static} adds pseudo-motions to static images and learns to predict the speed and direction of motions to enhance video representations. We take inspiration from these works and propose to contrast synthetic motions from tubelets which allows us to learn generalizable and data-efficient representations.
\begin{figure*}[t!]
\centering
\includegraphics[width=0.95\linewidth]{figures/framework-6.pdf}
\vspace{-1em}
\caption{\textbf{Tubelet-Contrastive Learning.} We sample two clips ($v_1, v_2$) from different videos and randomly crop an image patch from $v_1$. We generate a tubelet by replicating the patch in time and add motion through a sequence of target locations for the patch. 
We then add complexity to these motions by applying transformations, such as rotation, to the tubelet. The tubelet is overlaid $\odot$ onto both  clips to form a positive tubelet pair ($\hat{v}_{1}, \hat{v}_{2}$). We learn similarities between clips with the same tubelets (positive pairs) and dissimilarities between clips with different tubelets (negatives) using a contrastive loss.}
\label{fig:framework}
\vspace{-0.8em}
\end{figure*}


\noindent\textbf{Supervised Fine-Grained Motion Learning. } 
%
While self-supervised works have mainly focused on learning representations to distinguish coarse-grained actions, much progress has been made in supervised learning of motions. 
Approaches distinguish actions by motion-focused neural network blocks~\cite{kwon2021learning, mac2019learning, lin2019tsm, kim2021relational}, decoupling motion from appearance~\cite{rahmani2022dynamic, sun2022fine}, aggregating multiple temporal scales~\cite{yang2020temporal, ni2014multiple, feichtenhofer2019slowfast}, and sparse coding to obtain a mid-level motion representation~\cite{mavroudi2018end, piergiovanni2018fine, shao2020intra}. 
%
Other works exploit skeleton data~\cite{duan2022revisiting, hong2021video} or optical flow~\cite{simonyan2014two, feichtenhofer2016convolutional}. Alternatively, several works identify motion differences within an action class, by repetition counting~\cite{hu2022transrac, ucfrep-zhang2020context, zhang2021repetitive}, recognizing adverbs~\cite{doughty2020action, doughty2022you} or querying for action attributes~\cite{tqn}.  
%
Different from all these works, we learn a motion-sensitive video representation with self-supervision. We do so by relying on just coarse-grained video data in pretraining and demonstrate downstream generalization to fine-grained actions.

 \noindent\textbf{Tubelets. } %\cs{this paragraph can also go to intro, as second paragraph, with some shortening.}
 Jain \etal defined tubelets as class-agnostic sequences of bounding boxes over time~\cite{jain2014action}. %, without the need to address the problem of linking boxes from one frame to another. 
 Tubelets can represent the movement of people and objects and are commonly used for object detection in videos\cite{kang2017object,kang2017t,feichtenhofer2017detect}, spatiotemporal action localization~\cite{kalogeiton2017action,yang2019step,li2020actions,jain2014action,hou2017tube,zhao2022tuber} and video relation detection~\cite{ChenRelationICCV21}.
  Initially, tubelets were obtained by supervoxel groupings and dense trajectories \cite{jain2014action,GemertBMVC15} and later from 2D CNNs~\cite{kalogeiton2017action,li2020actions}, 3D CNNs~\cite{hou2017tube,yang2019step} and transformers~\cite{zhao2022tuber}. %Tubelets proved beneficial over a sliding-window tactic in spatiotemporal action detection. 
 %
 We introduce (synthetic) tubelets of pseudo-objects for contrastive video self-supervised learning.
