{
    "arxiv_id": "2303.11935",
    "paper_title": "Vision Transformer-based Model for Severity Quantification of Lung Pneumonia Using Chest X-ray Images",
    "authors": [
        "Bouthaina Slika",
        "Fadi Dornaika",
        "Hamid Merdji",
        "Karim Hammoudi"
    ],
    "submission_date": "2023-03-18",
    "revised_dates": [
        "2024-10-28"
    ],
    "latest_version": 1,
    "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
    ],
    "abstract": "To develop generic and reliable approaches for diagnosing and assessing the severity of COVID-19 from chest X-rays (CXR), a large number of well-maintained COVID-19 datasets are needed. Existing severity quantification architectures require expensive training calculations to achieve the best results. For healthcare professionals to quickly and automatically identify COVID-19 patients and predict associated severity indicators, computer utilities are needed. In this work, we propose a Vision Transformer (ViT)-based neural network model that relies on a small number of trainable parameters to quantify the severity of COVID-19 and other lung diseases. We present a feasible approach to quantify the severity of CXR, called Vision Transformer Regressor Infection Prediction (ViTReg-IP), derived from a ViT and a regression head. We investigate the generalization potential of our model using a variety of additional test chest radiograph datasets from different open sources. In this context, we performed a comparative study with several competing deep learning analysis methods. The experimental results show that our model can provide peak performance in quantifying severity with high generalizability at a relatively low computational cost. The source codes used in our work are publicly available at https://github.com/bouthainas/ViTReg-IP.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11935v1"
    ],
    "publication_venue": "This work has been submitted to the IEEE for possible publication"
}