\section{Introduction}
\label{sec:intro}
Suppose we have never seen white bears (\ie, polar bears) before. Can we picture what it would look like? This is not difficult because we have seen many white animals in daily life (\eg, white dogs and white rabbits) and different bears with various visual attributes in the zoo (\eg, brown bears and black bears). Humans have no difficulty in disentangling ``white" and ``bear" from seen instances and combining them into the unseen composition. Inspired by this property of human intelligence, researchers attempt to make machines learn compositions of concepts as well. Compositional zero-shot learning (CZSL) is a specific problem studying visual compositionality, aiming to learn visual concepts from seen compositions of attributes and objects and generalize concept knowledge to unseen compositions.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/intro.pdf}
    \caption{Motivation illustration. Given images from seen attribute-object compositions, human can disentangle the attribute ``yellow" from ``yellow bird" and ``yellow pear", and the object ``flower" from ``purple flower" and ``red flower". After learning visual properties of the concepts ``yellow" and ``flower", human can then recognize images from the unseen composition ``yellow flower".}
    \label{fig:overview}
    \vspace{-3pt}
\end{figure}

Learning attribute-object compositions demands prior knowledge about attributes and objects. However, visual concepts of attributes and objects never appear alone in a natural image. To learn exclusive concepts for compositionality learning, we need to disentangle the attribute concept and the object concept. As illustrated in~\cref{fig:overview}, if we want to recognize the image of ``yellow flower", it is necessary to learn the ``yellow" concept and the ``flower" concept, \ie, disentangle visual concepts, from images of seen compositions. Previous works~\cite{misra2017red,nagarajan2018attributes,purushwalkam2019task,wei2019adversarial,li2020symmetry,naeem2021learning,mancini2021open,mancini2022learning} tackle CZSL by composing attribute and object word embeddings, and projecting word and visual embeddings to a joint space. They fail to disentangle visual concepts.
Recently, some works~\cite{ruis2021independent,zhang2022learning,li2022siamese, Saini_2022_CVPR} consider visual disentanglement but still have limitations despite their good performance. SCEN~\cite{li2022siamese} learns concept-constant samples contrastively without constructing concept embedding prototypes to avoid learning irrelevant concepts shared by positive samples. IVR~\cite{zhang2022learning} disentangles visual features into ideal concept-invariant domains. This ideal domain generalization setting requires a small discrepancy of attribute and object sets and would degenerate on vaster and more complex concepts. ProtoProp~\cite{ruis2021independent} and OADis~\cite{Saini_2022_CVPR} learn local attribute and object prototypes from spatial features on convolutional feature maps. However, spatial disentanglement is sometimes infeasible because attribute and object concepts are highly entangled in spatial features. Taking an image of ``yellow flower" as an example, the spatial positions related to the attribute ``yellow" and the object ``flower" completely overlap, which hinders effective attribute-object disentanglement.

To overcome the above limitations, we propose a simple visual disentangling framework exploiting \textbf{A}ttention as \textbf{D}is\textbf{E}ntangler (\framework) on top of vision transformers~\cite{dosovitskiy2020vit}. We notice that vision transformers (ViT) have access to more sub-space global information across multi-head attentions than CNNs~\cite{raghu2021vision}. Therefore, with the expressivity of different subspace representations, token attentions of ViT may provide a more effective way for disentangling visual features, compared to using traditional spatial attentions across local positions on convolutional features~\cite{Saini_2022_CVPR, ruis2021independent}. Specifically, it is difficult to disentangle the attribute-object composition ``yellow flower" by spatial positions, but it is possible for ViT multi-head attentions to project attribute concept ``yellow" and object concept ``flower" onto different subspaces. Inspired by this property, we propose to learn cross-attention between two inputs that share the same concept, \eg, ``yellow bird" and ``yellow pear" share the same attribute concept ``yellow". In this way, we can derive attribute- and object-exclusive visual representations by cross-attention disentanglement. 
To ensure that the concept disentangler is exclusive to the specific concept, we also need to constrain the disentanglers to learn the concept of interest instead of the other concept. For example, given attribute-sharing images, the attribute attention should output similar attribute-exclusive features while the object attention should not. To achieve this goal, we apply a regularization term adapted from the earth mover's distance (EMD)~\cite{emd_paper} at the attention level. This regularization term forces cross-attention to learn the concept of interest by leveraging the feature similarity captured from all tokens. 
Mancini \etal~\cite{mancini2021open} propose an open-world evaluation setting, which is neglected by most previous works. We consider both closed-world and the open-world settings in our experiments, demonstrating that our method is coherently efficient in both settings. 
The contributions of this paper are summarized below: 
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt,leftmargin=15pt]
    \item We propose a new CZSL approach, named \framework, using cross-attentions to disentangle attribute- and object-exclusive features from paired concept-sharing inputs.
    \item We force attention disentanglers to learn the concept of interest with a regularization term adapted from EMD, ensuring valid attribute-object disentanglement.
    \item We comprehensively evaluate our method in both closed-world and open-world settings on three CZSL datasets, achieving consistent state-of-the-art.
\end{itemize}
