\appendix
\label{sec:appendix}

\section{Coefficient $\beta$ during inference}
At inference time, we use a coefficient $\beta$ to strike a balance between the composition score $p(c)$ and the product of the attribute and object scores $p(a)\cdot p(o)$. The final prediction score $\tilde{p}(c)$ is given by:
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
    \tilde{p}(c) =  p(c) + \beta \cdot p(a) \cdot p(o)
    \label{eq:supp-predict}
\end{equation}
where $c \in \mathcal{C}_{test}$ and $c=(a, o) \in \mathcal{A} \times \mathcal{O}$. We first fix $\beta=1.0$ during training, and then validate $\beta=0.0, 0.1, \cdots, 1.0$ to choose the best $\hat{\beta}$ on the validation set. We report the chosen $\hat{\beta}$ values for different datasets in~\cref{tab:beta}.

\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{15pt}
    \scalebox{0.82}{
    \begin{tabular}{lcc}
    \toprule
    Datasets & Closed-world & Open-world \\
    \midrule
    Clothing16K~\cite{zhang2022learning} & $\hat{\beta}=0.1$ & $\hat{\beta}=0.1$ \\
    UT-Zappos50K~\cite{yu2014fine} & $\hat{\beta}=0.9$ & $\hat{\beta}=0.9$ \\
    C-GQA~\cite{naeem2021learning} & $\hat{\beta}=1.0$ & $\hat{\beta}=0.7$ \\
    \bottomrule
    \end{tabular}}
    \caption{The chosen $\hat{\beta}$ values for different datasets under the closed-world and the open-world settings.}
    \label{tab:beta}
\end{table}

\section{Unseen-seen accuracy curve}
For the CZSL evaluation metric, we follow the generalized evaluation protocol~\cite{purushwalkam2019task, chao2016empirical}. To overcome the negative bias on seen compositions, we use a calibration term for unseen compositions. This calibration term increases unseen composition scores and leads to the following classification rule:
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
    \hat{c} = \mathop{\arg\max}_{c \in \mathcal{C}_{test}} \ \tilde{p}(c) + \gamma\mathbb{I}[c \in \mathcal{C}_{u}]
\end{equation}
where the prediction $\tilde{p}(c)$ is computed by~\cref{eq:supp-predict}, $\gamma$ is the calibration term, $\mathbb{I}[\cdot] \in \{0, 1\}$ indicates whether or not $c$ is an unseen composition, \ie, $c \in \mathcal{C}_{u}$. When using different calibration terms, we can obtain different paired top-1 accuracy of seen and unseen compositions. 
Without any constraints, we can obtain the highest unseen accuracy by  $\gamma=+\infty$  and the highest seen accuracy by $\gamma=-\infty$, leading to trivial solutions.
To construct a feasible list of different calibration values, we first compute $\gamma_i$ for each image $i$ of unseen compositions:
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
    \gamma_i = \mathop{\max}_{c \in \mathcal{C}_{s}}\ \tilde{p}(c\ |\ i) - \tilde{p}(c_{i}\ |\ i)
    \label{eq:calibration}
\end{equation}
where $c_i \in \mathcal{C}_{u}$ is the ground-truth composition of the image $i$ and $\tilde{p}(c\ |\ i)$ denotes the prediction score of composition $c$ for the image $i$. A list of $\gamma_i$ can be derived by applying~\cref{eq:calibration} on all unseen-composition images. We then sort the list, in which the smallest value makes the highest seen accuracy and the largest value makes the highest unseen accuracy. We pick $\gamma_i$ in the list with a specific interval and obtain multiple seen-unseen accuracy pairs. In this way, we can plot a curve with all scatters of seen and unseen accuracy, from which the evaluation metrics AUC (area under curve) and HM (the best harmonic mean accuracy) are obtained. 

In~\cref{fig:acc-curve}, we show the unseen-seen accuracy curve of all compared CZSL methods on all datasets under the closed-world and open-world settings. With the increase of the calibration value, the classification accuracy of seen compositions decreases while the accuracy of unseen compositions increases. The evaluation metrics in the paper, \ie, area under curve (AUC), the best harmonic mean value (HM), the best seen accuracy (Seen), and the best unseen accuracy (Unseen), are all derived from the unseen-seen accuracy curve. We can observe that compared to other methods, our \framework consistently achieves the best trade-off between the accuracy of seen and unseen compositions, especially on the large-scale C-GQA~\cite{naeem2021learning} dataset.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/curve.pdf}
    \caption{Unseen-seen accuracy curve on Clothing16K~\cite{zhang2022learning}, UT-Zappos50K~\cite{yu2014fine}, and C-GQA~\cite{naeem2021learning} under the closed-world and open-world settings. We compare our \framework with SymNet~\cite{li2020symmetry}, CompCos~\cite{mancini2021open}, GraphEmb~\cite{naeem2021learning}, SCEN~\cite{li2022siamese}, IVR~\cite{zhang2022learning}, and OADis~\cite{Saini_2022_CVPR}. Area under curve (AUC) is reported in brackets.}
    \label{fig:acc-curve}
\end{figure}

\section{Ablation study with ResNet18 backbone}
\hsznew{In this paper, we use ViT as our backbone, while ResNet18 is a common choice in previous works. In~\Cref{tab:ab-backbone}, we show experimental results on ablating every component in our model with both backbones to verify the effectiveness of the proposed method. We can observe that every component is crucial for both backbones. The results indicate that our model is backbone-agnostic and performs better with ViT backbone, thanks to the capability of ViT in excavating high-level sub-space information.}
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{15pt}
    \scalebox{0.75}{
    \begin{tabular}{lcccc>{\columncolor{tabcolor}}cccc>{\columncolor{tabcolor}}cccc}
        \toprule
        & & &  &  & \multicolumn{4}{c}{ViT} & \multicolumn{4}{c}{ResNet18} \\
         \cmidrule(lr){6-9} \cmidrule(lr){10-13}
         & CA & AA & OA & Reg & AUC & HM & Seen & Unseen & AUC & HM & Seen & Unseen\\
         \midrule
         (0) & \xmark & \xmark & \xmark & \xmark & 64.3 & 69.1 & 97.5 & 71.8 & 48.3 & 58.7 & 96.7 & 54.8 \\
         (1) & self & \xmark & \xmark & \xmark & 65.8 & 71.6 & 98.2 & 71.6 & 48.8 & 58.7 & 96.9 & 56.7 \\
         (2) & self & self & self & \xmark & 67.3 & \textbf{74.3} & 98.5 & 72.1 & 50.8 & 61.7 & 95.7 & 58.2 \\
         (3) & self & cross & cross & \xmark & 67.3 & 73.0 & 98.7 & 72.7 & 52.3 & 61.3 & \textbf{97.2} & 60.4 \\
         (4) & self & cross & cross & \cmark & \textbf{68.0} & 74.2 & \textbf{99.0} & \textbf{73.1} & \textbf{53.7} & \textbf{64.1} & \textbf{97.2} & \textbf{60.7}\\
         
        \bottomrule
    \end{tabular}}
    \caption{Ablate the components in \framework on open-world Clothing16K with both backbones. CA, AA, and OA denote composition, attribute, and object attention. Reg denotes the regularization term. We test self- or cross-attention for AA and OA.}
    \label{tab:ab-backbone}
\end{table*}

\section{Comparison with CNN-based models}
\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{15pt}
    \scalebox{0.75}{
    \begin{tabular}{lccccccccc}
        \toprule
         & \multicolumn{3}{c}{Composition} & \multicolumn{2}{c}{Train} & \multicolumn{2}{c}{Val} & \multicolumn{2}{c}{Test}  \\
         \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
         Datasets & $|\mathcal{A}|$ & $|\mathcal{O}|$ & $|\mathcal{A}|\times|\mathcal{O}|$ & $|\mathcal{C}_{s}|$ & $|\mathcal{X}|$ & $|\mathcal{C}_{s}|$ / $|\mathcal{C}_{u}|$ & $|\mathcal{X}|$ & $|\mathcal{C}_{s}|$ / $|\mathcal{C}_{u}|$ & $|\mathcal{X}|$
         \\ \midrule
        Clothing16K~\cite{zhang2022learning} & 9 & 8 & 72 & 18 & 7242 & 10 / 10 & 5515 & 9 / 8 & 3413\\
        UT-Zappos50K~\cite{yu2014fine} & 16 & 12 & 192 & 83 & 22998 & 15 / 15 & 3214 & 18 / 18 & 2914 \\
        C-GQA~\cite{naeem2021learning} & 413 & 674 & 278362 & 5592 & 26920 & 1252 / 1040 & 7280 & 888 / 923 & 5098 \\
        Vaw-CZSL~\cite{Saini_2022_CVPR} & 440 & 541 & 238040 & 11175 & 72203 & 2121 / 2322 & 9524 & 2449 / 2470 & 10856 \\
        \bottomrule
    \end{tabular}}
    \caption{Comparison of data split statistics.}
    \label{tab:data-splits-vaw}
\end{table*}

OADis~\cite{Saini_2022_CVPR} and ProtoProp~\cite{ruis2021independent} are two CZSL methods, which heavily depend on the spatial structure of convolutional features extracted from CNN models, \eg, ResNet18~\cite{he2016deep}. ProtoProp~\cite{ruis2021independent} extracts local prototypes of attribute and object features in the spatial dimension propagated through a GCN-based compositional graph. OADis~\cite{Saini_2022_CVPR} uses attribute and object affinity modules to capture the high-similarity regions in the spatial features of images with the same attribute or object. We compare our \framework with these two CNN-based models in~\cref{tab:compare-cnn}. We can observe that our model consistently outperforms other methods on all datasets. \framework increases AUC by 6.3 on Clothing16K, 1.1 on UT-Zappos59K, and 2.1 on C-GQA. In the meanwhile, \framework increases the best harmonic mean value (HM) by 4.0\% on Clothing16K, 2.3\% on UT-Zappos50K, and 4.4\% on C-GQA. The experimental results demonstrate ViT-based \framework is more efficient than the current CNN-based state-of-the-art models.
\begin{table*}[t]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{l>{\columncolor{tabcolor}}cccccc>{\columncolor{tabcolor}}cccccc>{\columncolor{tabcolor}}cccccc}
        \toprule
         & \multicolumn{6}{c}{Clothing16K} & \multicolumn{6}{c}{UT-Zappos50K} & \multicolumn{6}{c}{C-GQA} \\
         \cmidrule(lr){2-7} \cmidrule(lr){8-13} \cmidrule(lr){14-19}
         Models & AUC & HM & Seen & Unseen & Attr & Obj & AUC & HM & Seen & Unseen & Attr & Obj & AUC & HM & Seen & Unseen & Attr & Obj \\
         \midrule
         ProtoProp$^\dag$~\cite{ruis2021independent} & 86.1 & 84.1 & 97.7 & 93.4 & 86.6 & 89.7 & 34.0 &  48.8 & 60.6 & \textbf{66.8} & \textbf{48.3} & 74.7 & 2.0 & 11.0 & 26.4 & 9.4 & 11.2 & 26.6\\
         OADis$^\dag$~\cite{Saini_2022_CVPR} &  85.5 & 84.7 & 96.7 & 94.1 & 84.9 & 92.5 & 30.0 & 44.4 & 59.5 & 65.5 & 46.5 & \textbf{75.5} & 3.1 & 13.6 & 30.5 & 12.7 & 10.6 & 30.7\\
         \midrule
         \framework (ours) & \textbf{92.4} & \textbf{88.7} & \textbf{98.2} & \textbf{97.7} & \textbf{90.2} & \textbf{93.6} & \textbf{35.1} & \textbf{51.1} & \textbf{63.0} & 64.3 & 46.3 & 74.0 & \textbf{5.2} & \textbf{18.0} & \textbf{35.0} & \textbf{17.7} & \textbf{16.8} & \textbf{32.3} \\ 
        \bottomrule
    \end{tabular}}
    \caption{Comparison results of \framework and two CNN-based models. We conduct experiments on Clothing16K~\cite{zhang2022learning}, UT-Zappos50K~\cite{yu2014fine}, and C-GQA~\cite{naeem2021learning} under the closed-world setting. The superscript $^\dag$ denotes the model using ResNet18~\cite{he2016deep} as the backbone.} 
    \label{tab:compare-cnn}
\end{table*}


Saini \etal~\cite{Saini_2022_CVPR} propose a new CZSL dataset, named Vaw-CZSL, a subset of Vaw~\cite{pham2021learning}, which is a multi-label attribute-object dataset. Saini \etal~\cite{Saini_2022_CVPR} sample one attribute per image, leading to a much larger dataset in comparison to previous datasets as shown in~\cref{tab:data-splits-vaw}. We compare \framework with all ViT-adapted methods in the main paper and CNN-based OADis~\cite{Saini_2022_CVPR} on Vaw-CZSL~\cite{Saini_2022_CVPR} in~\cref{tab:vaw-results}. Similar to the results on standard CZSL datasets, \framework outperforms all the other models. \framework increases AUC by 0.3 ($\sim$27.3\% relatively) and increases the best harmonic mean value (HM) by 1.2\% ($\sim$14.8\% relatively). Overall, \framework achieves stable state-of-the-art performance across various small-scale and large-scale datasets.

\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{8pt}
    \scalebox{0.8}{
    \begin{tabular}{l>{\columncolor{tabcolor}}cccccc}
        \toprule
          & \multicolumn{6}{c}{Vaw-CZSL}  \\
         \cmidrule(lr){2-7} 
         Models & AUC & HM & Seen & Unseen & Attr & Obj \\
         \midrule
         SymNet~\cite{li2020symmetry} & 0.89 & 7.4 & 12.3 & 10.2 & 9.9 & 32.4 \\
         CompCos~\cite{mancini2021open} & 0.92 & 7.5 & 14.2 & 8.7 & 8.4 & 30.5\\
         GraphEmb~\cite{naeem2021learning} & 1.02 & 7.8 & 14.1 & 9.9 & 10.8 & 29.8\\
         SCEN~\cite{li2022siamese} & 0.84 & 7.1 & 14.2 & 8.1 & 7.6 & 30.0 \\ 
         IVR~\cite{zhang2022learning} & 0.91 & 7.4 & 13.0 & 9.6 & 8.9 & 31.9 \\
         OADis$^\dag$~\cite{Saini_2022_CVPR} & 0.87 & 7.1 & 13.6 & 9.4 & 9.7 & 31.4 \\
         OADis~\cite{Saini_2022_CVPR} & 1.10 & 8.1 & 15.2 & 10.1 & 9.9 & 31.6 \\
         \midrule
         \framework (ours) & \textbf{1.40} & \textbf{9.3} & \textbf{15.5} & \textbf{12.0} & \textbf{11.5} & \textbf{33.8} \\ 
        \bottomrule
    \end{tabular}}
    \caption{Experimental results on Vaw-CZSL~\cite{Saini_2022_CVPR}. We compare \framework with baseline models in the main paper and OADis~\cite{Saini_2022_CVPR}. The superscript $^\dag$ denotes the model using ResNet18~\cite{he2016deep} as the backbone. The others use ViT-B-16~\cite{dosovitskiy2020vit} as the backbone.} 
    \label{tab:vaw-results}
\end{table}

\section{Additional qualitative results}
We show additional qualitative results of \framework in this section. We follow the main paper to conduct more experiments of text-to-image retrieval, image-to-text retrieval, and visual concept retrieval, adding some results on Vaw-CZSL~\cite{Saini_2022_CVPR}.

In~\cref{fig:supp-txt2img}, we retrieve the top-5 closest images for texts of attribute-object compositions. For the relatively easier Clothing16K~\cite{zhang2022learning} dataset, all the retrieved images are correct. For the more challenging large-scale Vaw-CZSL~\cite{Saini_2022_CVPR} dataset with more complicated semantics of attributes and objects, some wrong images may be retrieved but they are highly semantically-related to the given text.
% , solely with a mismatched ground-truth label. 
Taking the ``flying plane" (row 5) as an example, the mismatched images are the ``in-the-air jet", the ``metal plane", the ``diagonal jet", and the ``in-the-air plane". These images are labelled with synonyms or from a different perspective, but they are essentially images of a ``flying plane". We can observe that \framework performs equally well for seen and unseen compositions.

In~\cref{fig:supp-img2txt}, we retrieve the top-5 closest compositional texts for images of seen and unseen compositions. For seen compositions, it is difficult to retrieve the ground-truth label in the top-1 closest result, but all the retrieved texts are related to the image, giving the reasonable attribute-object compositions which the ground-truth label fails to incorporate. For unseen compositions, although it is quite hard to retrieve the unseen ground-truth label because of the learning bias on seen compositions, the retrieved texts are mostly reasonable to describe the given image. These results indicate \framework efficiently connects the compositional texts and the corresponding images by transferring knowledge from seen concepts to unseen compositions.

The property of \framework to disentangle concept-exclusive features enables us to conduct visual concept retrieval experiments. In~\cref{fig:supp-visual-concept}, we retrieve the attribute-related or the object-related images for the given image based on their visual concept feature distances. We report the top-5 retrieval results of four images by their attribute-exclusive and object-exclusive features. The results show that \framework effectively disentangles the attribute and object concepts from visual images and produces reliable concept-exclusive features.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/supp_concept_retrieval.pdf}
    \caption{Retrieve \emph{seen} compositions for \emph{unseen} compositions based on the visual concept feature distance. We report the top-5 retrieval results on Clothing16K~\cite{zhang2022learning}. All the retrieved images for the corresponding concept are correct (in the green box).}
    \label{fig:supp-visual-concept}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/supp_txt2img.pdf}
    \caption{Text-to-image retrieval of seen (left) and unseen (right) compositions. We report the top-5 closest retrieval results on Clothing16K~\cite{zhang2022learning} (top three rows) and Vaw-CZSL~\cite{Saini_2022_CVPR} (bottom three rows). The correct image is in the green box, and the wrong image is in the red box with its ground-truth label below (black text).}
    \label{fig:supp-txt2img}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/supp_img2txt.pdf}
    \caption{Image-to-text retrieval of seen (left) and unseen (right) compositions. We report the top-5 closest retrieval results on Vaw-CZSL~\cite{Saini_2022_CVPR}. The black text denotes the ground-truth label, the green text denotes the correct result, and the orange text denotes the wrong result.}
    \label{fig:supp-img2txt}
\end{figure*}

\section{Pseudocode for \framework}
\framework is simple and easy to implement. For reproducibility, we show the PyTorch-style pseudocode of \framework for training in~\cref{algo:pseudocode-train} and for inference in~\cref{algo:pseudocode-infer}. 
The complete source code of ADE is available: \url{https://github.com/haoosz/ade-czsl}.
\begin{algorithm}[h]
\small
\SetAlgoLined
    \PyComment{emb\li a, emb\li o, emb\li c: embeddings of attributes, objects, and compositions} \\
    \PyComment{f: visual encoder} \\
    \PyComment{attn\li a, attn\li o, attn\li c: attribute, object, and composition attention blocks} \\
    \PyComment{proj\li a, proj\li o, proj\li c: projection with attribute, object, composition embedders} \\
    \PyComment{emd: adapted EMD at the attention level}\\
    \PyCode{} \\
    \PyComment{initialize attribute/object embeddings; compose them to composition embeddings}\\
    \PyCode{emb\li a = init(all\li attr)} \\
    \PyCode{emb\li o = init(all\li obj)} \\
    \PyCode{emb\li c = compos(emb\li a, emb\li o)} \\
    \PyComment{load 3 images and labels}\\
    \PyCode{for x, x\li a, x\li o, c in train\li loader:} \\
    \Indp   % start indent
        \PyComment{composition, attribute, object label}\\
        \PyCode{y, y\li a, y\li o = c}\\
        \PyComment{encoded tokens}\\
        \PyCode{z, z\li a, z\li o = f(x), f(x\li a), f(x\li o)}\\
        \PyComment{concept features and attention maps}\\
        \PyCode{out\li a1, amap\li aa1 = attn\li a(z, z\li a)} \\ 
        \PyCode{out\li a2, amap\li aa2 = attn\li a(z\li a, z)} \\
        \PyCode{out\li o1, amap\li oo1 = attn\li o(z, z\li o)} \\ 
        \PyCode{out\li o2, amap\li oo2 = attn\li o(z\li o, z)} \\
        \PyCode{out\li c, \li\ = attn\li c(z, z)} \\
        \PyComment{when inputs are of no interest}\\
        \PyCode{\li, amap\li ao1 = attn\li a(z, z\li o)} \\ 
        \PyCode{\li, amap\li ao2 = attn\li a(z\li o, z)} \\
        \PyCode{\li, amap\li oa1 = attn\li o(z, z\li a)} \\ 
        \PyCode{\li, amap\li oa2 = attn\li o(z\li a, z)} \\
        \PyComment{probabilities}\\
        \PyCode{p\li a1 = proj\li a(out\li a1) @ emb\li a.T}\\
        \PyCode{p\li a2 = proj\li a(out\li a2) @ emb\li a.T}\\
        \PyCode{p\li o1 = proj\li o(out\li o1) @ emb\li o.T}\\
        \PyCode{p\li o2 = proj\li o(out\li o2) @ emb\li o.T}\\
        \PyCode{p\li c = proj\li c(out\li c) @ emb\li c.T}\\
        \PyComment{cross entropy losses}\\
        \PyCode{l\li a1 = cross\li entropy(p\li a1, y\li a)}\\
        \PyCode{l\li a2 = cross\li entropy(p\li a2, y\li a)}\\
        \PyCode{l\li o1 = cross\li entropy(p\li o1, y\li o)}\\
        \PyCode{l\li o2 = cross\li entropy(p\li o2, y\li o)}\\
        \PyCode{l\li c = cross\li entropy(p\li c, y)}\\
        \PyComment{adapted EMDs}\\
        \PyCode{s\li aa = emd(amap\li aa1, amap\li aa2)}\\
        \PyCode{s\li oo = emd(amap\li oo1, amap\li oo2)}\\
        \PyCode{s\li ao = emd(amap\li ao1, amap\li ao2)}\\
        \PyCode{s\li oa = emd(amap\li oa1, amap\li oa2)}\\
        \PyComment{loss}\\
        \PyCode{l\li ce = l\li a1 + l\li a2 + l\li o1 + l\li o2 + l\li c}\\
        \PyCode{l\li reg = s\li ao + s\li oa - s\li aa - s\li oo}\\
        \PyCode{loss = l\li ce + l\li reg} \\
        \PyComment{optimization step}\\
        \PyCode{loss.backward()}\\
        \PyCode{optimizer.step()}\\
    \Indm % end indent, must end with this, else all the below text will be indented
\caption{PyTorch-style pseudocode for training}
\label{algo:pseudocode-train}
\end{algorithm}

\begin{algorithm}[h]
\small
\SetAlgoLined
    \PyComment{emb\li a, emb\li o, emb\li c: embeddings of attributes, objects, and compositions} \\
    \PyComment{f: visual encoder} \\
    \PyComment{attn\li a, attn\li o, attn\li c: attribute, object, and composition attention blocks} \\
    \PyComment{proj\li a, proj\li o, proj\li c: projection with attribute, object, composition embedders} \\
    \PyComment{p: a dictionary storing probabilities}\\
    \PyComment{beta: probability coefficient}\\
    \PyCode{}\\
    \PyComment{initialize attribute/object embeddings; compose them to composition embeddings}\\
    \PyCode{emb\li a = init(all\li attr)} \\
    \PyCode{emb\li o = init(all\li obj)} \\
    \PyCode{emb\li c = compos(emb\li a, emb\li o)} \\
    \PyComment{encoded tokens} \\
    \PyCode{z = f(x)}\\
    \PyComment{concept features}\\
    \PyCode{out\li a, \li\ = attn\li a(z, z)} \\ 
    \PyCode{out\li o, \li\ = attn\li o(z, z)} \\ 
    \PyCode{out\li c, \li\ = attn\li c(z, z)} \\
    \PyComment{probabilities} \\
    \PyCode{p\li a = proj\li a(out\li a) @ emb\li a.T}\\
    \PyCode{p\li o = proj\li o(out\li o) @ emb\li o.T}\\
    \PyCode{p\li c = proj\li c(out\li c) @ emb\li c.T}\\
    \PyComment{initialize an empty p} \\
    \PyCode{p = \{\}}\\
    \PyComment{enumerate all compositions} \\
    \PyCode{for c in all\li comp:} \\
    \Indp
    \PyCode{a, o = c} \PyComment{c = (a, o)}\\
    \PyComment{combine 3 probabilities} \\
    \PyCode{p[c] = p\li c[c] + beta * p\li a[a] * p\li o[o]}\\
    \Indm
    \PyCode{return p} \PyComment{return final probabilities}\\
\caption{PyTorch-style pseudocode for inference}
\label{algo:pseudocode-infer}
\end{algorithm}

\section{Broader Impacts}
Compositional zero-shot learning is a new topic of learning visual features for the objects and the corresponding attributes. Our work efficiently disentangles attribute features and object features to learn the compositionality of visual images in the real life. Our work can be used to recognize attribute-object compositions, significantly extending the traditional object recognition, which has various positive
implications, \eg, object detection, fine-grained recognition, and action recognition. Besides, our work also contributes to the explainability of deep learning models by exploring how they learn unseen things in the real world. However, there are negative impacts as well. Although it seems far from becoming true, models may be used for harmful purposes, \eg, building weapons and conducting surveillance. When the learning ability is no longer constrained by the training data and the specific training task, it is possible to train models with normal and harmless data for evil implementations because we have no idea what compositional information we can derive from the training data. In general, learning attribute and object features for compositional zero-shot learning has both positive and negative impacts, depending on how people implement this technology.

\section{Data licences}
Clothing16K\footnote{\url{https://www.kaggle.com/datasets/kaiska/apparel-dataset}}~\cite{zhang2022learning} is a split of a public dataset in Kaggle competitions under CC0 license. UT-Zappos50K\footnote{\url{https://vision.cs.utexas.edu/projects/finegrained/utzap50k/}} is collected by Yu \etal~\cite{yu2014fine}, allowing non-commercial research use. C-GQA~\cite{naeem2021learning} is a split built on top of Stanford GQA dataset\footnote{\url{https://cs.stanford.edu/people/dorarad/gqa/index.html}}~\cite{hudson2019gqa}, which is free for non-commercial research use. 
Vaw-CZSL~\cite{Saini_2022_CVPR} is a subset of Vaw\footnote{\url{https://vawdataset.com/}}~\cite{pham2021learning} under MIT license. 
