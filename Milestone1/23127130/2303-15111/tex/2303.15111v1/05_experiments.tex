\section{Experiments}
\label{sec:experiments}
\subsection{Experimental details}
\paragraph{Datasets} We use three benchmark datasets in CZSL problem, namely Clothing16K~\cite{zhang2022learning}, UT-Zappos50K~\cite{yu2014fine}, and C-GQA~\cite{naeem2021learning}. Clothing16K~\cite{zhang2022learning} contains different types of clothing (\eg, shirt, pants) with color attributes (\eg, white, black). UT-Zappos50K~\cite{yu2014fine} is a fine-grained dataset consisting of different kinds of shoes (\eg, sneakers, sandals) with texture attributes (\eg, leather, canvas). C-GQA~\cite{naeem2021learning} is a split built on top of Stanford GQA dataset~\cite{hudson2019gqa}, composed of extensive common attribute concepts (\eg, old, wet) and object concepts (\eg, dog, bus) in real life. We follow the common data splits of these three datasets 
(see~\cref{tab:data-splits}). 

\begin{table}[h]
    \centering
    \scalebox{0.54}{
    \begin{tabular}{cccccccccc}
        \toprule
         & \multicolumn{3}{c}{Composition} & \multicolumn{2}{c}{Train} & \multicolumn{2}{c}{Val} & \multicolumn{2}{c}{Test}  \\
         \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
         Datasets & $|\mathcal{A}|$ & $|\mathcal{O}|$ & $|\mathcal{A}|\times|\mathcal{O}|$ & $|\mathcal{C}_{s}|$ & $|\mathcal{X}|$ & $|\mathcal{C}_{s}|$ / $|\mathcal{C}_{u}|$ & $|\mathcal{X}|$ & $|\mathcal{C}_{s}|$ / $|\mathcal{C}_{u}|$ & $|\mathcal{X}|$
         \\ \midrule
        Clothing16K~\cite{zhang2022learning} & 9 & 8 & 72 & 18 & 7242 & 10 / 10 & 5515 & 9 / 8 & 3413\\
        UT-Zappos50K~\cite{yu2014fine} & 16 & 12 & 192 & 83 & 22998 & 15 / 15 & 3214 & 18 / 18 & 2914 \\
        C-GQA~\cite{naeem2021learning} & 413 & 674 & 278362 & 5592 & 26920 & 1252 / 1040 & 7280 & 888 / 923 & 5098 \\
        \bottomrule
    \end{tabular}}
    \caption{Summary of data split statistics.}
    \label{tab:data-splits}
    \vspace{-10pt}
\end{table}

\paragraph{Open-world setting} In addition to the standard closed-world setting, we also evaluate our model on the open-world setting~\cite{mancini2021open}, which is neglected by most previous works. The open-world setting considers all possible compositions, which requires a much larger testing space than the closed-world setting during inference. Taking UT-Zappos50K as an example (see~\cref{tab:data-splits}), the closed world only considers 36 compositions in the testing set while the open world considers total 192 compositions, in which $\sim$81\% are ignored under the standard closed-world setting.

\paragraph{Evaluation metrics} Since CZSL models have an inherent bias for seen compositions, we follow the generalized CZSL evaluation protocol~\cite{purushwalkam2019task}. To overcome the negative bias for seen compositions, we apply different calibration terms to unseen compositions and compute the corresponding top-1 accuracy of seen and unseen compositions, where a larger bias makes higher unseen accuracy and lower seen accuracy, and vice versa.  We treat seen accuracy as $x$-axis and unseen accuracy as $y$-axis to derive an unseen-seen accuracy curve. We can then compute the area under curve (AUC), the best harmonic mean, the best seen accuracy, and the best unseen accuracy from the curve. In our experiments, we report these four metrics for evaluation, among which AUC is the most representative and stable metric for measuring CZSL model performance. \hsz{Note that the attribute accuracy or the object accuracy alone does not reflect CZSL performance, because the individual accuracy on attribute or object does not necessarily decide the accuracy of their composition.}

\paragraph{Implementation details}
We use a frozen ViT-B-16~\cite{dosovitskiy2020vit} backbone pretrained with DINO~\cite{caron2021emerging} on ImageNet~\cite{deng2009imagenet} in a self-supervised manner as our visual feature extractor. The ViT-B-16 outputs contain 197 tokens (1 \texttt{[CLS]} and 196 patch tokens) of 768 dimensions. For three attention disentangler modules, we implement one-layer multi-head attention framework following~\cite{vaswani2017attention}, changing the single input to paired inputs for cross-attentions. The embedders $\pi_a$, $\pi_c$, $\pi_o$ are the two-layer MLPs following the previous works~\cite{mancini2021open, zhang2022learning}, projecting the 768-dimension visual features to 300-dimension word embedding space. The word embedding prototypes are initialized with word2vec~\cite{mikolov2013distributed} for all datasets and learnable during training. The composition function $\psi$ is one linear layer. We train our model using Adam optimizer~\cite{kingma2015adam} with a learning rate of $5\times 10^{-6}$ for UT-Zappos50K and Clothing16K, and $5\times 10^{-5}$ for C-GQA. All models are trained with 128 batch size for 300 epochs.

\begin{table*}[t]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{l>{\columncolor{tabcolor}}cccccc>{\columncolor{tabcolor}}cccccc>{\columncolor{tabcolor}}cccccc}
        \toprule
         Closed-world & \multicolumn{6}{c}{Clothing16K} & \multicolumn{6}{c}{UT-Zappos50K} & \multicolumn{6}{c}{C-GQA} \\
         \cmidrule(lr){2-7} \cmidrule(lr){8-13} \cmidrule(lr){14-19}
         Models & AUC & HM & Seen & Unseen & Attr & Obj & AUC & HM & Seen & Unseen & Attr & Obj & AUC & HM & Seen & Unseen & Attr & Obj \\
         \midrule
         SymNet~\cite{li2020symmetry} & 78.8 & 79.3 & 98.0 & 85.1 & 75.6 & 84.1 & 32.6 & 45.6 & 60.6 & 68.6 & 48.2 & 77.0 & 3.1 & 13.5 & 30.9 & 13.3 & 11.4 & 34.6 \\
         CompCos~\cite{mancini2021open} & 90.3 & 87.2 & 98.5 & 96.8 & \textbf{90.2} & 91.8 & 31.8 & 48.1 & 58.8 & 63.8 & 45.5 & 72.4 & 2.9 & 12.8 & 30.7 & 12.2 & 10.4 & 33.9 \\
         GraphEmb~\cite{naeem2021learning} & 89.2 & 84.2 & 98.0 & 97.4 & 90.0 & 93.1 & 34.5 & 48.5 & 61.6 & \textbf{70.0} & \textbf{50.8} & \textbf{77.1} & 3.8 & 15.0 & 32.3 & 14.9 & 13.8 & 33.2 \\
         Co-CGE~\cite{mancini2022learning} & 88.3 & 87.9 & 98.5 & 94.7 & 87.4 & 91.4 & 30.8 & 44.6 & 60.9 & 62.6 & 46.0 & 73.5 & 3.6 & 14.7 & 31.6 & 14.3 & 12.6 & 34.6 \\
         SCEN~\cite{li2022siamese} & 78.8 & 78.5 & 98.0 & 89.6 & 81.2 & 85.4 & 30.9 & 46.7 & \textbf{65.7} & 62.9 & 44.0 & 74.4 & 3.5 & 14.6 & 31.7 & 13.4 & 10.7 & 31.4 \\ 
         IVR~\cite{zhang2022learning} & 90.6 & 86.6 & \textbf{99.0} & 97.0 & 89.3 & \textbf{93.6} & 34.3 & 49.2 & 61.5 & 68.1 & 48.4 & 74.6 & 2.2 & 10.9 & 27.3 & 10.0 & 10.3 & \textbf{37.5} \\
         OADis~\cite{Saini_2022_CVPR} & 88.4 & 86.1 & 97.7 & 94.2 & 84.9 & 93.1 & 32.6 & 46.9 & 60.7 & 68.8 & 49.3 & 76.9 & 3.8 & 14.7 & 33.4 & 14.3 & 8.9 & 36.3 \\
         \midrule
         \framework (ours) & \textbf{92.4} & \textbf{88.7} & 98.2 & \textbf{97.7} & \textbf{90.2} & \textbf{93.6} & \textbf{35.1} & \textbf{51.1} & 63.0 & 64.3 & 46.3 & 74.0 & \textbf{5.2} & \textbf{18.0} & \textbf{35.0} & \textbf{17.7} & \textbf{16.8} & 32.3\\ 
        \bottomrule
    \end{tabular}}
    \caption{Closed-world results on three datasets. We report the area under curve (AUC), the best harmonic mean (HM), the best seen accuracy (Seen), the best unseen accuracy (Unseen), the attribute accuracy (Attr), and the object accuracy (Obj) of the unseen-seen accuracy curve under the closed world-setting. AUC is the core CZSL metric. All models use the same DINO ViT-B-16 backbone.} 
    \label{tab:cw-results}
\end{table*}

\begin{table*}[t]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{l>{\columncolor{tabcolor}}cccccc>{\columncolor{tabcolor}}cccccc>{\columncolor{tabcolor}}cccccc}
        \toprule
         Open-world & \multicolumn{6}{c}{Clothing16K} & \multicolumn{6}{c}{UT-Zappos50K} & \multicolumn{6}{c}{C-GQA} \\
         \cmidrule(lr){2-7} \cmidrule(lr){8-13} \cmidrule(lr){14-19}
         Models & AUC & HM & Seen & Unseen & Attr & Obj  & AUC & HM & Seen & Unseen & Attr & Obj & AUC & HM & Seen & Unseen & Attr & Obj  \\
         \midrule
         SymNet~\cite{li2020symmetry} & 57.4 & 68.3 & 98.2 & 60.7 & 57.6 & 81.2 & 25.0 & 40.6 & 60.4 & 51.0 & 38.2 & \textbf{75.0} & 0.77 & 4.9 & 30.1 & 3.2 & 18.4 & 37.5 \\
         CompCos~\cite{mancini2021open} & 64.1 & 70.8 & 98.2 & 69.8 & 71.7 & 83.7 & 20.7 & 36.0 & 58.1 & 46.0 & 36.4 & 71.1 & 0.72 & 4.3 & 32.8 & 2.8 & 15.1 & 37.8 \\
         GraphEmb~\cite{naeem2021learning} & 62.0 & 68.3 & 98.5 & 69.7 & 71.8 & 82.4 & 23.5 & 40.0 & 60.6 & 47.0 & 37.1 & 69.3 & 0.81 & 4.8 & 32.7 & 3.2 & 17.2 & 36.7 \\
         Co-CGE~\cite{mancini2022learning} & 59.3 & 69.2 & 98.7 & 63.8 & 68.5 & 76.2 & 22.0 & 40.3 & 57.7 & 43.4 & 33.9 & 67.2 & 0.48 & 3.3 & 31.1 & 2.1 & 15.5 & 35.7 \\
         SCEN~\cite{li2022siamese}& 53.7 & 61.5 & 96.7 & 62.3 & 63.6 & 79.1 & 22.5 & 38.0 & \textbf{64.8} & 47.5 & 34.9 & 73.3 & 0.34 & 2.5 & 29.5 & 1.5 & 14.8 & 32.3 \\ 
         IVR~\cite{zhang2022learning} & 63.6 & 72.0 & 98.7 & 69.0 & 70.3 & 84.8 & 25.3 & 42.3 & 60.7 & 50.0 & 38.4 & 71.4 & 0.94 & 5.7 & 30.6 & 4.0 & 16.9 & 36.5 \\
         OADis~\cite{Saini_2022_CVPR} & 53.4 & 63.2 & 98.0 & 58.6 & 57.3 & \textbf{85.4} & 25.3 & 41.6 & 58.7 & \textbf{53.9} & \textbf{40.3} & 74.7 & 0.71 & 4.2 & 33.0 & 2.6 & 14.6 & \textbf{39.7} \\
         \midrule
         \framework (ours) & \textbf{68.0} & \textbf{74.2} & \textbf{99.0} & \textbf{73.1} & \textbf{75.0} & 84.5 & \textbf{27.1} & \textbf{44.8} & 62.4 & 50.7 & 39.9 & 71.4 & \textbf{1.42} & \textbf{7.6} & \textbf{35.1} & \textbf{4.8} & \textbf{22.4} & 35.6 \\ 
        \bottomrule
    \end{tabular}}
    \caption{Open-world results on three datasets. Different from~\cref{tab:cw-results}, open-world setting considers all possible compositions in testing.} 
    \label{tab:ow-results}
\end{table*}

\subsection{Comparison}
\hsznew{To ensure a fair comparison and demonstrate that our improvement over baseline models is not merely by ViT, we adopt ViT backbone to state-of-the-art CZSL models and \emph{re-train} all models.} We compare our method with them: \hsznew{(1)~OADis~\cite{Saini_2022_CVPR} disentangles attribute and object features from spatial convolutional maps;} (2)~SymNet~\cite{li2020symmetry} introduces the symmetry principle of attribute-object transformation and group theory as training objectives; (3)~CompCos~\cite{mancini2021open} extends CZSL to an open-world setting considering all possible compositions during inference, proposing a feasibility score based on data statistics to remove unfeasible compositions; (4)~GraphEmb~\cite{naeem2021learning} and Co-CGE~\cite{mancini2022learning} propose to use graph convolutional networks (GCN) to represent attribute-object relationships and compositions; (5)~SCEN~\cite{li2022siamese} projects visual features to a Siamese contrastive space to capture concept prototypes, and introduces complex state transition module to produce virtual compositions; (6)~IVR~\cite{zhang2022learning} proposes to disentangle visual features into concept-invariant domains from a perspective of domain generalization, by masking specific channels of visual features. 

\paragraph{Closed-world evaluation} In~\cref{tab:cw-results}, we compare our \framework model with the state-of-the-art methods. \framework consistently outperforms others by a significant margin. \framework increases the core metric AUC by 1.8 on Clothing16K, 0.6 on UT-Zappos50K, and 1.4 on C-GQA ($\sim$37\% relatively). Similarly, \framework increases the best harmonic mean (HM) by 0.8\% on Clothing16K, 1.9\% on UT-Zappos50K, and 3.0\% on C-GQA. We notice that SymNet~\cite{li2020symmetry} and SCEN~\cite{li2022siamese} perform badly on Clothing16K. The reason might be that not learning concept prototypes harms the word embedding expressivity on small-scale concepts. We also notice that IVR~\cite{zhang2022learning} performs very well on curated datasets Clothing16K and UT-Zappos50K but badly on larger-scale real-world dataset C-GQA. We hypothesize ideal concept-invariant domains might be difficult to learn from natural images and large-scale concepts of C-GQA. In contrast, our \framework model achieves state-of-the-art performance on all datasets.

\paragraph{Open-world evaluation} In~\cref{tab:ow-results}, we consider the open-world setting to compare our \framework with other methods. Likewise, \framework also performs the best among all methods under open-world setting. \framework increases AUC by 3.9 on Clothing16K, 1.8 on UT-Zappos50K, and 0.48 on C-GQA ($\sim$51\% relatively). \framework also increases the best harmonic mean (HM) by 2.2\% on Clothing16K, 2.5\% on UT-Zappos50K, and 1.9\% on C-GQA ($\sim$33\% relatively). From the above results, \framework surpasses others by a larger margin on open-world AUC and HM than closed-world ones, indicating \framework maintains utmost efficiency when turning from the closed world to the open world. It is worth mentioning that \framework does not apply any special operations (\eg, feasibility masking~\cite{mancini2021open}) for the open world and deals with the two settings in exactly the same way. 
IVR~\cite{zhang2022learning} keeps its performance to a great extent but still lags behind our method significantly.

\begin{table*}[t]
\begin{minipage}[t]{0.44\linewidth}
    \centering
    \scalebox{0.72}{
    \begin{tabular}{lcccc>{\columncolor{tabcolor}}cccc}
        \toprule
         & CA & AA & OA & Reg & AUC & HM & Seen & Unseen\\
         \midrule
         (0) & \xmark & \xmark & \xmark & \xmark & 23.8 & 41.1 & 59.0 & 48.9 \\
         (1) & self & \xmark & \xmark & \xmark & 25.3 & 42.3 & 61.1 & 49.9 \\
         (2) & self & self & self & \xmark & 26.7 & 44.6 & 61.9 & 49.8 \\
         (3) & self & cross & cross & \xmark & 26.9 & 44.5 & \textbf{63.4} &48.7 \\
         (4) & self & cross & cross & \cmark &  \textbf{27.1} & \textbf{44.8} & 62.4 & \textbf{50.7}\\
         
        \bottomrule
    \end{tabular}}
    \caption{Ablate the components in \framework on open-world UT-Zappos50K. CA, AA, and OA denote composition, attribute, and object attention. Reg denotes the regularization term. We test self- or cross-attention for AA and OA.} 
    \label{tab:model-ab}
\end{minipage}
\hspace{2mm}
\begin{minipage}[t]{0.54\textwidth}
\centering
    \scalebox{0.68}{
    \begin{tabular}{ll>{\columncolor{tabcolor}}cccc>{\columncolor{tabcolor}}cccc}
        \toprule
        & & \multicolumn{4}{c}{C-GQA} & \multicolumn{4}{c}{Clothing16K} \\
        \cmidrule(lr){3-6} \cmidrule(lr){7-10}
        & Inference formulation  & AUC & HM & Seen & Unseen & AUC & HM & Seen & Unseen\\
        \midrule
        (0) & $p(c)$ & 4.6 & 16.8 & \textbf{35.1} & 16.0 & \textbf{92.4} & \textbf{88.8} & \textbf{98.2} & \textbf{97.7} \\
        (1) & $p(a) \cdot p(o)$ &  4.0 & 15.8 & 31.4 & 15.1 & 57.3 & 66.3 & 96.7 & 63.1\\
        (2) & $p(c) + p(a) \cdot p(o)$ & \textbf{5.2} & \textbf{18.0} & 35.0 & \textbf{17.7} & 90.4 & 85.9 & 98.2 & 97.0\\
        (3) & $p(c) + \beta \cdot p(a) \cdot p(o)$ & \textbf{5.2} & \textbf{18.0} & 35.0 & \textbf{17.7} & \textbf{92.4} & 88.7 & \textbf{98.2} & \textbf{97.7} \\
        \bottomrule
    \end{tabular}}
    \caption{Results on closed-world Clothing16K and C-GQA using different inference formulations. Rows (0)-(2) respectively represents the cases when $\beta=0.0$, $\beta=+\infty$, and $\beta=1.0$. Row (3) is our inference formulation, which applies an $\beta$ optimized on the validation set.} 
    \label{tab:eval-ab}
\end{minipage}
\end{table*}

\subsection{Ablation study}
\paragraph{Backbone: ResNet \textit{vs} ViT}
\hsznew{Our work leverages ViT as the default backbone to excavate more high-level sub-space information, while ResNet18 is the most common backbone in previous works. 
In~\Cref{tab:backbone}, we compare our \framework to OADis~\cite{Saini_2022_CVPR}  with both backbones. Our \framework performs similarly to OADis with ResNet18, but outperforms it significantly with ViT. Additionally, we present an ablation study on different components of our method with the ResNet18 backbone in the Appendix. These experiments indicate that our model benefits from ViT and all components of our method are effective regardless of the backbone.}

\begin{table}[h]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{ll>{\columncolor{tabcolor}}cc>{\columncolor{tabcolor}}cc>{\columncolor{tabcolor}}cc}
        \toprule
          \multicolumn{2}{l}{Closed-world} & \multicolumn{2}{c}{Clothing16K} & \multicolumn{2}{c}{UT-Zappos50K} & \multicolumn{2}{c}{C-GQA} \\
         \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
         Backbone & Models & AUC & HM & AUC & HM & AUC & HM \\
         \midrule
         \multirow{2}{*}{ResNet18} & OADis~\cite{Saini_2022_CVPR} & 85.5 & 84.7 & \textbf{30.0} & 44.4 & \textbf{3.1} & 13.6 \\
          & \framework (ours) & \textbf{87.2} & \textbf{85.1}  & 29.5 & \textbf{47.0} & \textbf{3.1} & \textbf{13.7} \\
         \midrule
         \multirow{2}{*}{ViT-B-16} & OADis~\cite{Saini_2022_CVPR} & 88.4 & 86.1  & 32.6 & 46.9 & 3.8 & 14.7 \\
          & \framework (ours) & \textbf{92.4} & \textbf{88.7}  & \textbf{35.1} & \textbf{51.1} & \textbf{5.2} & \textbf{18.0} \\
        \bottomrule
    \end{tabular}}
    \caption{Compare \framework and OADis~\cite{Saini_2022_CVPR} with ResNet18 and ViT.} 
    \label{tab:backbone}
    \vspace{-5pt}
\end{table}

\paragraph{Different parts of \framework} We evaluate the effectiveness of attention disentanglers (composition, attribute, and object attention) and the regularization term in our model. We report the ablation study results on the open-world UT-Zappos50K in~\cref{tab:model-ab}. Rows~(0)-(2) show attention disentanglers can significantly improve the performance. Rows~(2)-(3) show that cross-attention learns disentangled concepts better than self-attention for AA and OA. Rows~(3)-(4) show the regularization term can further benefit the visual disentanglement, improving the unseen accuracy and overall AUC.

\paragraph{Inference formulation} We also investigate the effect of our inference formulation $p(c) + \beta \cdot p(a) \cdot p(o)$ in~\cref{tab:eval-ab}. We report the results with extreme values of $\beta$, \ie, $\beta=0.0$ and $\beta=1.0$. Note that $\beta=0.0$ means only using composition probability for prediction. In addition, we also test the performance only using the product of attribute and object probabilities $p(a) \cdot p(o)$. We can observe that the best fixed $\beta$ value is unfixed among datasets. For example, $\beta=1.0$ gives the highest AUC for C-GQA in row (2) while $\beta=0.0$ for Clothing16K in row (0). In contrast, our validated $\beta$ consistently gives the best inference results for both datasets. Another observation on C-GQA is that $p(a) \cdot p(o)$ alone is not a good prediction, but adding it to $p(c)$ can increase the unseen accuracy. This indicates that the disentangled attribute prediction $p(a)$ and object prediction $p(o)$ indeed enhance the unseen generalization for CZSL problem.

\paragraph{Effect of regularization term} We propose an EMD-adapted regularization term at the attention level to force attentions to disentangle the concept of interest. We also investigate the effect of applying the regularization term at the feature level. Specifically, 
we compare our EMD-based distance to the cosine and euclidean feature distances. The results on open-world UT-Zappos50K are shown in~\cref{tab:reg-ab}. Our EMD-based regularization outperforms other distance forms, because our attention-level EMD distance considers token-wise similarity capturing the specific concept-related attention responses.
\begin{table}[h]
   \centering
   \setlength{\tabcolsep}{20pt}
    \scalebox{0.7}{
    \begin{tabular}{l>{\columncolor{tabcolor}}cccc}
        \toprule
        Reg & AUC & HM & Seen & Unseen \\
        \midrule
        Cosine & 26.8 & 44.7 & \textbf{63.0} & 48.6 \\
        Euclidean & 26.2 & 44.3 & 62.6 & 47.5 \\
        Ours (EMD) & \textbf{27.1} & \textbf{44.8} & 62.4 & \textbf{50.7}\\
        \bottomrule
    \end{tabular}}
    \caption{Comparison of different regularization terms on open-world UT-Zappos50K.} 
    \label{tab:reg-ab} 
    \vspace{-5pt}
\end{table}

\subsection{Qualitative analysis}
Visual disentanglement in feature space is hard to visualize~\cite{Saini_2022_CVPR}. Inspired by previous work attempts~\cite{Saini_2022_CVPR,zhang2022learning,li2020symmetry,nagarajan2018attributes}, we conduct qualitative analysis of image and text retrieval to show how our \framework model correlates the visual image and the concept composition. In addition, to further validate \framework is efficient to disentangle visual concepts, we conduct unseen-to-seen image retrieval based on their visual concept features extracted by attribute and object attentions.

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=0.92\linewidth]{images/txt2img.pdf}
    \caption{Top-5 text-to-image retrieval.}
    \label{fig:wrd2img}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.35\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/img2wrd.pdf}
    \caption{Top-5 image-to-text retrieval.}
    \label{fig:img2wrd}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{images/retrieve_visual.pdf}
    \caption{Top-5 visual concept retrieval.}
    \label{fig:concept-retrieve}
     \end{subfigure}
    \caption{\hsznew{Qualitative analysis. (a) In the last row of ``suede sandals", the wrong image (red box) is ``fake leather sandals". (b) Each image has the ground-truth label (black text) and 5 retrieval results (colored text), in which the green text is the correct prediction. (c) We retrieve images sharing the same visual concepts by their visual concept features for unseen images of ``yellow skirt" and ``pink pants".}}
    \label{fig:qualitative}
    \vspace{-4pt}
\end{figure*}

\paragraph{Image and text retrieval}
We first consider text-to-image retrieval. Given a text composition, \eg, ``leather heels", we embed it and retrieve the top-5 closest visual features based on the feature distance. We display four text compositions of the different objects sharing the same attributes and vice versa in~\cref{fig:wrd2img}. We can observe that the retrieved images are correct in most of the cases. One exception is when retrieving ``suede sandals", the third closest image is ``fake leather sandals". Although ``suede sandals" and ``fake leather sandals" are not the same composition, they are quite visually similar. We then consider image-to-text retrieval, shown in~\cref{fig:img2wrd}. Given an image, \eg, the image of a ``brown zebra", we extract its visual feature and retrieve the top-5 closest text composition embeddings. It is difficult to retrieve the ground-truth label in the top-1 closest text composition, but all top-5 results are all semantically related to the image. We take the image of ``blond person" (row 3, col 2) as an example. Although the text composition ``blond person" is not retrieved in the top-5 matches, the retrieved results ``white shirts", ``white outfit", ``white shorts", ``white pants", and ``young girl" are all reasonable and actually present in the image. Image and text retrieval experiments validate that our \framework efficiently projects visual features and word embeddings into a uniform space.
\vspace{-2pt}

\paragraph{Visual concept retrieval}
Because the attribute and the object are visually coupled in an image to a high degree of entanglement, it is challenging to visualize the disentanglement in feature space~\cite{Saini_2022_CVPR}. Saini \etal~\cite{Saini_2022_CVPR} retrieve single attribute or object text from test images. However, this process is the same as multi-label classification and insufficient to validate that disentangled visual concepts are learned from images. \hsz{Based on the disentanglement ability of ADE, we construct a visual concept retrieval experiment to investigate the distances between visual concept features, \ie, the embedded attribute feature $\pi_a(v_a)$ and the embedded object feature $\pi_o(v_o)$, extracted from different images. Prior to our work, no existing models can do so, because none of them extracts concept-exclusive features like ADE.} The results are shown in~\cref{fig:concept-retrieve}. We first extract attribute features and object features from all seen images. Given an unseen image, we retrieve the top-5 closest images by measuring the feature distance between the attribute feature of the given image and that of all seen images, and the same goes for the object feature. For the image of ``yellow skirt", all retrieval results for the visual concept ``yellow" are all  ``yellow \texttt{[OBJ]}", and all retrieval results for ``skirt" are ``\texttt{[ATTR]} skirt". For the ``pink pants" image, our model also perfectly retrieves the visual concepts, \ie, the attribute ``pink" and the object ``pants". Our experimental results demonstrate that our \framework model is effective to disentangle visual concepts from seen compositions and combine learned concept knowledge into unseen compositions.