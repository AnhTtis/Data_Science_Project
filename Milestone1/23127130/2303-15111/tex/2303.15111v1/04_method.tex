\section{Our approach}
\label{sec:method}
We aim to disentangle attribute and object features for visual representation by learning from image pairs sharing the same attributes or objects. For example, by learning the object ``flower" from the ``purple flower" and the ``red flower" and learning the attribute ``yellow" from the ``yellow bird" and the ``yellow pear", we can infer what a ``yellow flower" looks like. To this end, we propose ADE to learn cross-attentions as concept disentanglers with the regularization of a novel token earth mover's distance. The whole framework of our method is shown in~\cref{fig:pipeline}.

\subsection{Disentanglement with cross-attention}
Multi-head self-attention in transformers~\cite{vaswani2017attention} is powerful for extracting token connections. Attention maps each query token, and key-value token pairs to an output token. Every output token is a weighted sum of the values, where the weights is the similarity of the corresponding query token and the key tokens. The attention is formulated as:
\begin{equation}
    Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V
    \label{eq:attention}
\end{equation}
where three inputs are query tokens ($Q$), key tokens ($K$), and value tokens ($V$) and the scaling factor $d_k$ is the dimension of the query and key. Self-attention uses the same input for $Q$, $K$, $V$ and effectively captures relationships among \verb+[CLS]+ and patch tokens of a single input image. Normally, the \texttt{[CLS]} token is exclusively used as a global representation of the input image for downstream tasks, but exclusive \texttt{[CLS]} token highly entangles attribute and object concepts.

\hsz{For CZSL task, we want to disentangle the exclusive concept by exploiting the semantic similarity between tokens from different concept-sharing features.} For this purpose, we introduce a \emph{cross}-attention mechanism to extract attribute-exclusive or object-exclusive features for paired images sharing the same attribute or object concept. The cross-attention has the same structure as the self-attention, but works on different inputs. Self-attention uses the same input $I$ for $Q$, $K$, $V$ (\ie, $Q$=$K$=$V$=$I$) while cross-attention uses one of the paired inputs $I$ for the query and the other $I^{\prime}$ for the key and the value (\ie, $Q$=$I$, $K$=$V$=$I^{\prime}$). 
In the cross-attention, the query input and the key input play different roles. 
The cross-attention output is the weighted sum of the value (also the key) input based on the similarity between the query and key inputs. For a pair of input images, we swap the query and key inputs in computing cross-attention to derive two cross-attended outputs for the respective inputs. Cross-attention with query-key swapping (QKS) is illustrated in~\cref{fig:compare}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/comparison.pdf}
    \caption{Illustration of three types of attention.}
    \label{fig:compare}
\end{figure}

The cross-attention outputs paired features of the exclusive concept with paired concept-sharing inputs. Taking the object-sharing pair as an example, object cross-attention leverages the shared object features of the key input to enhance the query input, thus making the outputs exclusive to object feature space, \ie, object-exclusive features $v_o$ and $v_o^{\prime}$. The same applies to attribute-exclusive features $v_a$ and $v_a^{\prime}$ extracted from paired attribute-sharing images. We also use a standard self-attention to extract composition feature $v_c$.

To train cross-attentions, we adopt common cross entropy loss after MLP embedders ($\pi_a$, $\pi_c$, $\pi_o$) for attribute-exclusive features ($v_a$, $v_a^\prime$), object-exclusive features ($v_o$, $v_o^\prime$), and composition feature ($v_c$). \hsz{We denote any of these visual features as $v$ and the corresponding embedder as $\pi$. We denote the word embedding as $w_z$, where the concept $z \in \mathcal{Z}$ can be any of the attribute $a \in \mathcal{A}$, the object $o \in \mathcal{O}$, or the composition $c \in \mathcal{C}$. The class probability of L2-normalized $\pi(v)$ and $w_z$ can be computed as:}
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
     p_\pi(z \ |\ v)  = \frac{\exp{(\pi(v) \cdot w_z/\tau)}}{\sum_{\hat{z} \in \mathcal{Z}} \exp{(\pi(v) \cdot w_{\hat{z}}/\tau)}}, z \in \mathcal{Z}
\end{equation}
where $\tau$ is the temperature.  \hsz{We then compute the cross entropy to classify visual features:}
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
    H_\pi(v, z) = - \log p_\pi(z \ |\ v), z \in \mathcal{Z}
    \label{eq:cross-entropy}
\end{equation}
\hsz{Considering different classification objectives of attribute, object, and composition, the general~\cref{eq:cross-entropy} can be applied to five visual features, \ie, $v_a$, $v_a^\prime$, $v_o$, $v_o^\prime$, $v_c$.} Therefore, the training objective is composed of five cross entropy losses:
{\setlength{\abovedisplayskip}{0pt}
\begin{align}
    \mathcal{L}_{ce} = \ & \textcolor{attr}{\underbrace{\textcolor{black}{H_{\pi_a}(v_a, a)}}_{\mathcal{L}_{attr}}} + \textcolor{attr}{\underbrace{\textcolor{black}{H_{\pi_a}(v_a^\prime, a)}}_{\mathcal{L}_{attr}^\prime}} +  \textcolor{comp}{\underbrace{\textcolor{black}{H_{\pi_c}(v_c, c)}}_{\mathcal{L}_{com}}} + \notag \\ 
    &  \textcolor{obj}{\underbrace{\textcolor{black}{H_{\pi_o}(v_o, o)}}_{\mathcal{L}_{obj}}} + \textcolor{obj}{\underbrace{\textcolor{black}{H_{\pi_o}(v_o^\prime, o)}}_{\mathcal{L}_{obj}^\prime}}
\end{align}
}where the attribute label $a \in \mathcal{A}$, the object label $o \in \mathcal{O}$, and the composition label $c=(a, o) \in \mathcal{C}_{test}$.

\subsection{Disentangling constraint at the attention level}
\hsz{So far, we have enabled cross-attentions to disentangle concepts, but we want to further ensure that the attribute and the object disentanglers learn the corresponding attribute and object concepts instead of the opposite concepts. To this end, we introduce an attention-level earth mover's distance (EMD) to constrain disentanglers to learn the concept of interest.} The EMD is formulated as an optimal transportation problem in~\cite{emd_paper}. Suppose we have supplies of $n_s$ sources $\mathcal{S}=\{s_i\}_{i=1}^{n_s}$ and demands of $n_d$ destinations $\mathcal{D}=\{d_j\}_{j=1}^{n_d}$. Given the moving cost from the $i$-th source to the $j$-th destination $c_{ij}$, an optimal transportation problem aims to find the minimal-cost flow $f_{ij}$ from sources to destinations:
\begin{align}
    \underset{f_{ij}}{\text{minimize}} \quad &\sum\nolimits_{i=1}^{n_s}\sum\nolimits_{j=1}^{n_d} c_{ij} f_{ij} \\
    \text{subject to} \quad &f_{ij} \geqslant 0,\ i=1, ..., n_s,\ j= 1, ..., n_d \\
    &\sum\nolimits_{j=1}^{n_d} f_{ij} = s_i, \ i=1, ..., n_s \\
    &\sum\nolimits_{i=1}^{n_s} f_{ij} = d_j, \ j=1, ..., n_d
\end{align} 
where the optimal flow $\tilde{f}_{ij}$ is computed by the moving cost $c_{ij}$, the supplies $s_i$, and the demands $d_j$. The EMD can be further formulated as:
\begin{equation}
    \text{EMD}(c_{ij}, s_i, d_j)= (1-c_{ij})\tilde{f}_{ij}.
\end{equation}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/emd.pdf}
    \caption{Illustration of the adapted EMD at the attention level. Branches \textcolor{branchone}{\ding{182}} and \textcolor{branchtwo}{\ding{183}} respectively denote using one of the paired inputs as the query $Q$. We compute the EMD with two attention maps from \textcolor{branchone}{\ding{182}} and \textcolor{branchtwo}{\ding{183}} branches. We use the \texttt{[CLS]}-to-patch logits as the supplies $s_i $ and the demands $d_j$, and one minus the mean matrix of patch-to-patch logits as the moving cost $c_{ij}$. In this way, we adapt the EMD to our cross-attention module.}
    \label{fig:emd}
\end{figure}

When the EMD is greater, the distributions $\mathcal{S}$ and $\mathcal{D}$ are closer. \hsz{In \cref{fig:emd}, we show how to use the EMD as a feature similarity metric at the attention level in our cross-attention module.} We can derive a cross-attention map from~\cref{eq:attention} and the counterpart cross-attention map after query-key swapping. We use the logits of \verb+[CLS]+-to-patch attentions as supplies $s_i$ and demands $d_j$, which represent how similar the global feature of one image is to the local patches of the other image. We use one minus the average map of patch-to-patch attentions as the moving cost $c_{ij}$. In this way, we can compute an adapted EMD $\lambda$ for the cross-attention:
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
    \lambda^p_q = \text{EMD}(c_{ij}, s_i, d_j; p, q), p, q \in \{a, o\}
\end{equation}
where $p$ represents the type of the cross-attention, \ie, attribute cross-attention when $p=a$ and object cross-attention when $p=o$, and $q$ represents the type of the inputs, \ie, attribute-sharing inputs when $q=a$ and object-sharing inputs when $q=o$. 

For attribute cross-attention, attribute-sharing inputs should have large EMD $\lambda^a_a$ while object-sharing inputs should have small EMD $\lambda^a_o$. It is opposite for object cross-attention, \ie, small $\lambda^o_a$ of attribute-sharing inputs and large $\lambda^o_o$ of object-sharing inputs. Thus, we formulate the regularization term as:
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
    \mathcal{L}_{reg} = \lambda^a_o + \lambda^o_a - \lambda^a_a - \lambda^o_o
    \label{eq:reg}
\end{equation}

\subsection{Training and inference}
At the training phase, we formulate our final loss as a cross entropy with regularization for all inputs:
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
    \mathcal{L} = \mathcal{L}_{ce} + \mathcal{L}_{reg}
\end{equation}

At inference phase, we feed the same test input into three attention branches in a self-attention manner, deriving three class probabilities $p(c)$, $p(a)$, and $p(o)$\footnote{Abbreviations for $p_{\pi_c}(c\ |\ v_c)$, $p_{\pi_a}(a\ |\ v_a)$, and $p_{\pi_o}(o\ |\ v_o)$, where $v_c$, $v_a$, and $v_o$ are outputs from three self-attentions.}, where $c=(a, o)$. Unlike most methods modeling single $p(c)$ for prediction, we compute our prediction score by synthesizing attribute, object and composition probabilities:
\begin{equation}
    \hat{c} = \mathop{\arg\max}_{c \in \mathcal{C}_{test}} \ p(c) + \beta \cdot p(a) \cdot p(o)
    \label{eq:predict}
\end{equation}
We first fix $\beta$ = 1.0 during training and then validate $\beta$ = 0.0, 0.1, $\cdots$, 1.0 to choose the best $\beta$ on the validation set. Finally, we compute the composition prediction as~\cref{eq:predict} with the chosen $\beta$, making the best prediction trade-off between generalized composition and independent concept.
