\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/pipeline_new.pdf}
    \caption{Method overview. Left (our framework \framework): Given one target image of ``red bus", we sample two auxiliary images of the same attribute ``red wall" and of the same object ``blue bus". We feed the three images into a frozen ViT initialized with DINO~\cite{caron2021emerging}. We then input all encoded tokens (\ie, \texttt{[CLS]} and patch tokens) to three attention modules: (1) attribute cross-attention taking paired attribute-sharing tokens as inputs; (2) object cross-attention taking paired object-sharing tokens as inputs; (3) composition self-attention taking tokens of single target image as input. We then project the \texttt{[CLS]} tokens of attention outputs with three MLP embedders $\pi_a$, $\pi_c$, and $\pi_o$. We finally compute cross entropy losses of embedded visual features with three learnable word embeddings: attribute embeddings, object embeddings, and their linear fused composition embeddings. Right: Illustration of five losses in attribute, object, and composition embedding spaces.}
    \label{fig:pipeline}
\end{figure*}

\section{Preliminary}
\label{sec:preliminary}
\vspace{-3pt}
Compositional zero-shot learning aims at learning a model from limited compositions of attributes (\eg, yellow) and objects (\eg, flower) to recognize an image from novel compositions. Given a set of all possible attributes $\mathcal{A}=\{a_0, a_1, \cdots, a_n\}$ and a set of all possible objects $\mathcal{O}=\{o_0, o_1, \cdots, o_m\}$, a compositional class set $\mathcal{C} = \mathcal{A} \times \mathcal{O}$ includes all attribute-object compositions. We divide $\mathcal{C}$ into two disjoint sets, namely the set of seen classes $\mathcal{C}_{s}$ and the set of unseen classes $\mathcal{C}_{u}$, where $\mathcal{C}_{s} \cap \mathcal{C}_{u} = \varnothing$ and $\mathcal{C}_{s} \cup \mathcal{C}_{u} = \mathcal{C}$. Training images are only from classes in $\mathcal{C}_{s}$ and testing images are from classes in $\mathcal{C}_{test} = \mathcal{C}_{s} \cup \mathcal{C}_{u}^\prime$. For closed-world evaluation, $\mathcal{C}_{u}^\prime$ is a predefined subset of $\mathcal{C}_{u}$, \ie, $\mathcal{C}_{u}^\prime \subset \mathcal{C}_{u}$. For open-world evaluation, all possible compositions are considered for testing, \ie, $\mathcal{C}_{u}^\prime = \mathcal{C}_{u}$. CZSL task aims to learn a model $f: \mathcal{X} \rightarrow \mathcal{C}_{test}$ to predict labels in the testing composition set $c \in \mathcal{C}_{test}$ for input images $x \in \mathcal{X}$.

To learn a CZSL model, a common method is to minimize the cosine similarity score between the visually encoded feature and the word embedding of attribute-object compositions. The similarity score can be expressed as
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
    s(x, (a, o)) = \frac{\phi(x)}{||\phi(x)||} \cdot \frac{\psi(a,o)}{||\psi(a,o)||}
\end{equation}
where $\phi(\cdot)$ is the visual encoder, and $\psi(\cdot, \cdot)$ is the composition function. The CZSL model $f$ can be formulated as
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
    f(x; \theta_\phi, \theta_\psi) = \mathop{\arg\max}_{a_i \in \mathcal{A}, o_j \in \mathcal{O}} s(x, (a_i, o_j))
\end{equation}
The model $f$ is optimized by learning the model parameters $\theta_\phi$ and $\theta_\psi$ to match the input $x$ with the most similar attribute-object composition. Visual encoder $\phi$ and composition function $\psi$, as two core components, take multiple forms in different methods. For visual encoder, works~\cite{ruis2021independent, zhang2022learning, Saini_2022_CVPR} manually design networks on top of a pretrained backbone (\eg, ResNet18~\cite{he2016deep}) to disentangle attribute and object embeddings for visual features. For composition functions, the object conditioned network~\cite{Saini_2022_CVPR}, the graph embedding~\cite{naeem2021learning, ruis2021independent}, and the linear projector~\cite{mancini2021open, zhang2022learning} are commonly used. Unlike employing complex composition functions, we adopt the simple linear projector in our method. 
