\section{Results}
\subsection{Qualitative Evaluation}

\subsubsection{Capacity Analysis}
We first qualitatively compare the capacity of our proposed method with recent approaches using four different criteria: 2D Photo-realistic, One-shot Synthesis, Style Learning, and Style Transfer. Here is the description of each criterion:

\begin{itemize}
    \item 2D Photo-realistic:  Indicate the visual fidelity and detailness of the generated output in each method.
    \item One-shot Synthesis: Indicate if the method inputs a single 2D image to generate the talking head or not.
    \item Style Learning: Indicate the method's ability to learn the person-specific style of a particular character. 
    \item Style Transfer: Indicate the method's ability to transfer a particular character style to a new target. 
\end{itemize}


\begin{table}[!ht]
\centering
\caption{Method capacity comparison between our method and recent approaches.}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.15 em} 
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c}
\hline
\multirow{3}{*}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{Criteria}}  \\ \cline{2-5} 
                                   & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}2D Photo-\\ Realistic\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}One-shot\\ Synthesis\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Style\\ Learning\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Style\\ Transfer\end{tabular}}}\\
                             & & & &\\\hline
MIT~\cite{zhou2020makelttalkMIT}     & \xmark                             & \cmark                                    & \xmark                              & \xmark                                \\ \hline
AudioDVP~\cite{wen2020photorealistic}     & \cmark                             & \cmark                                    & \xmark                              & \xmark                                \\ \hline
PCT~\cite{zhou2021pose} & \xmark                              & \cmark                                    & \cmark                              & \xmark                                 \\ \hline
AD-NERF~\cite{guo2021adneft} & \cmark                              & \xmark                                    & \xmark                              & \xmark                                \\ \hline
FACIAL~\cite{zhang2021facial}     & \cmark                              & \xmark           & \xmark                          & \xmark                                                             \\ \hline
LSP~\cite{lu2021liveLSP}     & \cmark                              & \xmark           & \cmark                          & \xmark                                                             \\ \hline
Ours     & \cmark                           & \cmark                                    & \cmark                              & \cmark                                \\ \hline
\end{tabular}
}}
\vspace{2ex}

\vspace{-2ex}
\label{tab:ad_method_compare}
\end{table}




\begin{figure*}[ht] 
   \centering
  \huge
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{cccccccccc}

\rotatebox[origin=l]{90}{\hspace{-0.3cm} \textbf{[Vougioukas et. al.]}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/May_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/May_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/May_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/Nadel_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/Nadel_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/Nadel_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/Obama2_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/Obama2_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Vou/Obama2_2.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.4cm}  \textbf{[Chen et. al.]}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/May_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/May_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/May_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/Nadel_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/Nadel_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/Nadel_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/Obama2_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/Obama2_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Chen/Obama2_3.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.5cm}  \textbf{[Zhou et. al.]}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/May_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/May_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/May_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/Nadel_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/Nadel_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/Nadel_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/Obama2_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/Obama2_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Zhou/Obama2_3.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.8cm} \textbf{[Lu et. al.]}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/May_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/May_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/May_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/Nadel_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/Nadel_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/Nadel_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/Obama2_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/Obama2_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Lu/Obama2_3.jpg}}\\[15pt]
\hline\\
\rotatebox[origin=l]{90}{\hspace{0cm} \textbf{Ours (Original)}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/May_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/May_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/May_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/Nadel_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/Nadel_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/Nadel_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/Obama2_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/Obama2_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/Our/Obama2_3.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0cm} \textbf{Ours (Cropped)}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/May_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/May_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/May_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/Nadel_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/Nadel_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/Nadel_3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/Obama2_1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/Obama2_2.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Comparison/OurCrop/Obama2_3.jpg}}\\[1pt]
\end{tabular}
}
    \caption{Comparison between different 2D talking head galleries.}
    \label{fig:2DCompare}
\end{figure*}
Table~\ref{tab:ad_method_compare} illustrates the overall capacity comparison between our method and the recent state of the arts. Our proposed method can output a 2D photo-realistic talking head, learn personalized style information, and transfer the specific character style by using only a single input image and an audio stream. In practice, our method can provide high-resolution 2D output and smooth motion based on the audio input. Furthermore, our transfer learning step allows us to quickly adapt and generate new motion based on a specific reference style. 



\subsubsection{2D Talking Head Animation}

We compare our method with other image-based methods for 2D talking head generation. Specifically, we compare with~\cite{vougioukas2019end, chen_photo-realistic_2019, zhou2020makelttalkMIT, lu2021liveLSP}. \cite{vougioukas2019end, chen_photo-realistic_2019, zhou2020makelttalkMIT} train their model for unseen face generation. Note that \cite{vougioukas2019end, chen_photo-realistic_2019} generates the talking head animation only on the cropped faces, which fails to capture the head pose motions.

Figure~\ref{fig:2DCompare} shows the qualitative results driven by the audio input in all methods. Since~\cite{zhou2020makelttalkMIT} warps both the background and the talking head, which can lead to the foreground and the background moving together. Besides, the mouth is twisted, and the synthesized region is blurred. ~\cite{lu2021liveLSP} can synthesize sharper images with higher fidelity. \cite{lu2021liveLSP} also disentangles the head motion and the background. However, while~\cite{vougioukas2019end,chen_photo-realistic_2019,zhou2020makelttalkMIT} and our method only requires a single 2D image to generate the corresponding talking head (one-shot synthesis), in~\cite{lu2021liveLSP} work, the authors require a character's video to learn and render the 2D head. This characteristic of~\cite{lu2021liveLSP} shows limitations in practice when it is challenging to collect video data for each target character. Besides, due to the training process, the target talking style mentioned in~\cite{lu2021liveLSP} tends to fit into the visual information of the target character and cannot be transferred to a different target. Finally, although~\cite{lu2021liveLSP} can synthesize sharp images with high quality, the sharpness of teeth and wrinkles are limited in some extreme cases where the mouth and head variations are high.


Compared to these baselines, our method can generate smooth and natural motion for the 2D talking head. We can produce high-resolution realistic photo output while the head foreground and the background are successfully disentangled . Furthermore, our proposed method not only generate realistic and natural motions for talking motions but also for singing styles such as \texttt{ballad}, \texttt{rap}, \texttt{opera}, etc. The styles can be transferred into different characters using our style transfer process. It is worth noting that we only need a single input image to create a high-fidelity 2D talking head animations while being robust to different challenging talking or singing styles. 








\subsubsection{Style Transfer Results}
  
Figure~\ref{fig:styleTransfer} shows that our method successfully transfers different styles such as \texttt{ballad}, \texttt{rap}, or \texttt{opera} to a new target character. For the \texttt{ballad} style, we use the short singing clip of Adele as the reference. The \texttt{ballad} style usually contains short echoes, a slightly moving head, and closed eyes during the performance.    
For the \texttt{rap} style, the short rapping clip of Mac is used for extracting the \texttt{rap} style reference. \texttt{Rap} style may have rapid head sharking and fast lips movement. 
And for the \texttt{opera} style, we use Andrea's sample clip to obtain the style reference. The \texttt{opera} style has a long echo, a curl of the lips, closed eyes, and slow head movement during the performance.
Note that the audio sequences used for these animations are different from the audio of the style references and unseen during training. 
  

\begin{figure}[!ht] 
  \centering
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{cccccc}

\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Adele/Adele1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Mona/Mona1c.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/MacLethal/Mac1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/StyledMcStay/McStay1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Adrea/Andrea1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/StyledTrump/Trump1.jpg}}\\[1pt]
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Adele/Adele3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Mona/Mona3c.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/MacLethal/Mac3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/StyledMcStay/McStay3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Adrea/Andrea3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/StyledTrump/Trump3.jpg}}\\[1pt]
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Adele/Adele4.jpg} \\  (a)}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Mona/Mona4c.jpg} \\ (b)}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/MacLethal/Mac4.jpg} \\ (c)}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/StyledMcStay/McStay4.jpg} \\ (d)}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/Adrea/Andrea4.jpg} \\ (e)}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/2D/StyledTrump/Trump4.jpg} \\ (f)}\\ [1pt]
\end{tabular}
}

    \caption{Our 2D photo-realistic talking head results with different styles. (a), (c), (e) are \texttt{ballad}, \texttt{rap}, and \texttt{opera} style references, respectively; (b), (d), (f) are the corresponding style transfer results. For more details, please visit our demonstration video.}
    \label{fig:styleTransfer}
\vspace{-1.5 pt}

\end{figure}






In the \texttt{ballad} case, our method successfully captures the personalized singing style of Adele and transfer it to Mona Lisa's talking head. The initial pose of the Mona Lisa is kept the same as the input static image. 
In the \texttt{rap} style, although the head and mouth motions of the character have extremely active dynamics and high intensity, which is common in fast-rap music, our method is still successful in capturing these behaviors of the animations. 
In the \texttt{opera} case, our method can identify the special mouth and teeth which are unique in \texttt{opera} style.
We note that our method can learn and transfer style to animate any arbitrary image. Besides, our proposed method is not only able to animate real person but also non-realistic ones such as human-like portraits, arts, or painting images (e.g., Mona Lisa). For more details, please refer to the demonstration video. 


Figure~\ref{fig:2DStyleComparison} shows the comparison between our method and recent works on 2D photorealistic talking head animation~\cite{zhou2020makelttalkMIT,lu2021liveLSP} when the character sings an opera song. Focusing on the mouth, we notice that our method produces better results in mouth motion variance and eyes expression compared to the results from~\cite{zhou2020makelttalkMIT} and ~\cite{lu2021liveLSP}. Specifically, in~\cite{zhou2020makelttalkMIT}, the visual fidelity of teeth and pores, as well as the realization of the mouth shape and motion, are not well presented. In~\cite{lu2021liveLSP}, although the quality is reasonable, the mouth shape is rigid and does not present well the pose of lips and eyes in the \texttt{opera} style. The results confirm that our proposed method achieves better lip-synchronization in such extreme cases in comparison with other baselines.

Figure~\ref{fig:fixedImg} shows the comparison between different styles when they are presented in a fixed input image to generate talking heads. The results illustrate the differences of various styles affecting the 2D talking head animation. Specifically, \texttt{neural} style shows how natural Obama is (in terms of head poses, eye contact, and mouth motions) during his given speech. \texttt{Opera} style focuses on eyes closures, has a curl of the lips, and slow head movement. \texttt{Rap} style encourages the head and mouth motions of the character to be highly dynamic. \texttt{Ballad} style has high variation of mouth motions during the performance, short echoes, slightly moving head, and closed eyes.



\begin{figure}[!ht] 
 \centering

\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccc}
\rotatebox[origin=l]{90}{\hspace{0.3cm}  \textbf{Zhou \etal \cite{zhou2020makelttalkMIT}}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Zhou/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Zhou/4.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Zhou/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Zhou/7.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.2cm} \textbf{Lu \etal\cite{lu2021liveLSP}}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Lu/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Lu/4.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Lu/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Lu/7.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.8cm} \textbf{Ours}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Our/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Our/4.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Our/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleComparison/Our/7.jpg}}\\[1pt]

\vspace{-5ex}
\rotatebox[origin=l]{90}{\hspace{0.4cm} \textbf{Lyric}} &
\shortstack{\includegraphics[width=0.2\linewidth]{images/StyleComparison/Lyric/Lyric1.png}}&
\shortstack{\includegraphics[width=0.2\linewidth]{images/StyleComparison/Lyric/Lyric4.png}}&
\shortstack{\includegraphics[width=0.2\linewidth]{images/StyleComparison/Lyric/Lyric5.png}}&
\shortstack{\includegraphics[width=0.2\linewidth]{images/StyleComparison/Lyric/Lyric7.png}}\\[1pt]


\end{tabular}
}
\vspace{0ex}
    \caption{Comparison between different 2D talking head galleries on \texttt{opera} style. Our method generates more natural and realistic motion, especially around the mouth and the eye of the character.}
    \label{fig:2DStyleComparison}

\end{figure}


Additionally, in Figure~\ref{fig:fixedAudio}, we also show the comparison between different styles when they are encoded in one input audio to generate talking heads. Note that, in this case, different input images are used to verify the synthesis effectiveness of our method. Although different styles are encoded into different images to generate different talking heads, the animation is realistic and the performance of lip-synchronization is well-reserved. More illustrative results can be found in our demonstration video.

\subsection{Quantitative Evaluation}


\subsubsection{Evaluation Metric}

\begin{figure}[!ht] 
  \centering
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccc}
\rotatebox[origin=l]{90}{\hspace{0.3cm} \textbf{Neutral Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Neural/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Neural/4.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Neural/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Neural/7.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.2cm}  \textbf{Opera Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Opera/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Opera/4.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Opera/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Opera/7.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.3cm} \textbf{Rap Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Rap/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Rap/4.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Rap/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Rap/7.jpg}}\\[1pt]
% \hline\\
\rotatebox[origin=l]{90}{\hspace{0.2cm} \textbf{Ballad Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Ballad/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Ballad/4.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Ballad/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Image/Ballad/7.jpg}}\\[1pt]
\end{tabular}
}
    \caption{Comparison between different styles in the same input image.}
    \label{fig:fixedImg}
\end{figure}



\begin{figure}[!ht] 
  \centering
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccc}
\rotatebox[origin=l]{90}{\hspace{0.3 cm} \textbf{Neutral Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Natalie/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Natalie/3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Natalie/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Natalie/6.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.3cm}  \textbf{Opera Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Trump/1.png}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Trump/3.png}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Trump/5.png}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Trump/6.png}}\\[1pt]
% \hline\\
\rotatebox[origin=l]{90}{\hspace{0.6cm} \textbf{Rap Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/McStay/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/McStay/3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/McStay/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/McStay/6.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.4cm} \textbf{Ballad Style}} &
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Lea/1.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Lea/3.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Lea/5.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/Fixed_Audio/Lea/6.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.2cm} \textbf{Lyric}} &
\shortstack{\includegraphics[width=0.15\linewidth]{images/Fixed_Audio/Lips/1.png}}&
\shortstack{\includegraphics[width=0.15\linewidth]{images/Fixed_Audio/Lips/3.png}}&
\shortstack{\includegraphics[width=0.15\linewidth]{images/Fixed_Audio/Lips/5.png}}&
\shortstack{\includegraphics[width=0.15\linewidth]{images/Fixed_Audio/Lips/6.png}}\\[1pt]
\end{tabular}
}
    \caption{Comparison between different styles in the same input audio.}
    \label{fig:fixedAudio}
\end{figure}



\textbf{Metrics to evaluate 2D talking head results.} We use six different metrics to evaluate how good and natural the animation of the generated talking head is. They are: Cumulative Probability Blur Detection (CPBD)~\cite{vougioukas2019realistic}, Landmark Distance (D-L)~\cite{zhou2020makelttalkMIT}, Landmarks Distance around the Mouth (LMD), Landmark Velocity difference (D-V)~\cite{zhou2020makelttalkMIT}, Difference in the open mouth area (D-A)~\cite{zhou2020makelttalkMIT}.

\textbf{Metric to evaluate style transfer.} The aforementioned metrics such as D-L, LMD, D-V, and D-A require the ground truth and the predicted sample to be synchronized with each other to measure the accuracy of the generated talking head. They also consider all frames to be equally important and calculate the accuracy by averaging all frames. Therefore, these metrics do not take into account the style information, which has more temporal dynamics and special facial expressions during a short duration.
To evaluate style transfer results efficiently, we introduce three new following metrics. 

\textit{Style-Aware Landmarks Distance} (SLD): To evaluate the style information encoded in a generated talking head, we design a metric called Style-Aware Landmarks Distance (SLD). This metric calculates the accuracy of mouth, eyes, head pose shapes between a chunked window of style reference and a chunked window of corresponding talking head animation. Lower is better. 
Let's assumed that a style reference video with $N_s$ frames is split into multiple temporal periods of $\rm F$ frames (window size), i.e., style reference windows $\rm W_s = \left(w_s^{(0:F)}, w_s^{(v:F+v)}, w_s^{(2v:F+2v)},\cdots, w_s^{(\kappa v:F +\kappa v)}\right)$, with $\rm w_s^{(i:F+i)}$ being the frames from $\rm i$\textit{-th} to $\rm (F+i)$\textit{-th}
of the reference video, $\rm v$ is the stride, and $\rm \kappa = \lfloor(N_s - F) / v\rfloor$.
Similar to the reference video, we chunk the generated animation video
into smaller chunked windows $\rm W_a = \Bigl(w_a^{(0:F)}, w_a^{(v:F+v)}, w_a^{(2v:F+2v)}, \cdots , \allowbreak w_a^{(\kappa v:F+ \kappa v)}\Bigr)$. The SLD is then calculated with the core is the D-L metric as:
\begin{equation}
    \label{eq:SLD}
    \rm{SLD} = \frac{1}{\rm \vert W_s\vert}\sum_{\rm w_s \in \rm W_s} \left(\underset{\rm w_a \in \rm W_a}{\rm{min}} \left(\rm{D\rm{-}L}\left(\rm w_s, \rm w_a\right)\right)\right) 
\end{equation} 
where $\rm{D\rm{-}L}$ is the Landmark Distance metric~\cite{zhou2020makelttalkMIT}.

Similarly, we calculate the 
\textit{Style-Aware Landmarks Velocity Difference} (SLV) and \textit{Style-Aware Mouth Area Difference} (SMD) as follow:

\begin{equation}
    \label{eq:SLV}
    \rm{SLV} = \frac{1}{\rm \vert W_s\vert}\sum_{\rm w_s \in \rm W_s} \left(\underset{\rm w_a \in \rm W_a}{\rm{min}} \rm{D\rm{-}V}\left(\rm w_s, \rm w_a\right)\right)
\end{equation} 
where ${\rm D\rm{-}V}$ is the Landmark Velocity difference metric~\cite{zhou2020makelttalkMIT}.

\begin{equation}
    \label{eq:SMD}
    \rm{SMD} = \frac{1}{\rm \vert W_s\vert}\sum_{\rm w_s \in \rm W_s} \left(\underset{\rm w_a \in \rm W_a}{\rm{min}} \left(\rm LMD\left(\rm w_s, \rm w_a\right)\right)\right)
\end{equation} 
where ${\rm LMD}$ is Landmarks Distance around the Mouth~\cite{chung2016lip}.

To robustly compare our results, we construct a grid of window size $\rm {F}=\{1,2,...,100\}$ and stride $\rm{v}=\{1,2,...,20\}$ and then compute the above metrics, i.e., SLD, SLV, and SMD, for each element on the grid. The final value is then calculated as the average over the grid of all computed metric values corresponding to each window size and stride setting.
In all metrics for evaluating style transfer, the function $\rm{min}(\cdot)$ is used to search for the best matched local window, i.e., a temporal period of frames considered to contain the best-matched style information. 
 In this way, our style metrics can take into account the temporal, and then support validating the style information encoded in it. Note that, we assume one video would have only one style when applying our proposed metrics.


\subsubsection{Dataset}  Since our method focus on learning different character styles in different circumstances, we evaluate and benchmark our results in the RAVDESS dataset~\cite{livingstone2018ryersonRAVDESS}. The RAVDESS is a validated multimodal database of emotional speech and song, which is suitable and challenging to validate our method and different baselines. Note that, we only use this dataset for benchmarking  to avoid training bias. 


\subsubsection{2D Talking Head Generation Results}

\begin{table}[!ht]
\centering
\caption{Result of different 2D talking head generation methods.}


\setlength{\tabcolsep}{0.3 em} 
{\renewcommand{\arraystretch}{1.5}
\begin{tabular}{c|c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{5}{c}{\textbf{Metrics}}  \\ \cline{2-6} 
                                   & \multicolumn{1}{c|}{\textbf{CPBD$\uparrow$}} & \multicolumn{1}{c|}{\textbf{LMD$\downarrow$}} & \textbf{D-L$\downarrow$} & \textbf{D-V$\downarrow$} & \textbf{D-A$\downarrow$}
                        \\ \hline
Ground Truth    	&0.28	&0.00	&0.00 \% &0.00 \% &0.00 \%                 \\ \hline
MIT~\cite{zhou2020makelttalkMIT}  &0.18	&2.28	&2.78\%	&0.88\%	 &14.52\%                \\
PCT~\cite{zhou2021pose}   	&0.09	&3.22	&3.27\%	&0.86\%	&36.84\%                   \\
LSP~\cite{lu2021liveLSP}   	&0.20	&3.29	&5.43\%	&0.85\%	&30.65\%               \\
AD-NERF~\cite{guo2021adneft} 	&0.21	&2.43 &2.67\% &0.85\% &13.34\%	                  \\ \hline \hline
Ours     &\textbf{0.26}	&\textbf{1.83}  &\textbf{2.65\%}	&\textbf{0.83\%}	&\textbf{10.53\%}               \\\hline
\end{tabular}
}
\vspace{2ex}

\vspace{-2ex}
\label{tab:baseline}
\end{table}

Table~\ref{tab:baseline} shows the 2D talking head result comparison between our method and recent baselines, including~\cite{zhou2020makelttalkMIT, zhou2021pose, lu2021liveLSP, guo2021adneft}. From Table~\ref{tab:baseline}, we can see that our method outperforms recent state-of-the-art approaches by a large margin. In particular, our method achieves the highest accuracy in CPBD, LMD, D-L, D-V, and D-A metrics. These results show that our method successfully renders the 2D talking head and increases the quality of the rendered results. Overall, our method can increase the sharpness of the head (identified by CPBD) metric, while generating natural facial motion (identified by LMD, D-L, D-V, and D-A metric). 

\subsubsection{Style Comparison}

Table~\ref{tab:style_quantitative} shows the comparison between our method and four baselines~\cite{zhou2020makelttalkMIT, zhou2021pose, lu2021liveLSP, guo2021adneft} in terms of style transfer. Three designed metrics (SLD, SLV, and SMD) are used for evaluation and benchmarking.
The results show that our method outperforms others by a large margin in all three  metrics, which suggests that our method effectively captures style information from the style reference and successfully transfer it to the target image. 

\begin{table}[!ht]
\centering
\caption{Result comparison in terms of style transfer between different 2D talking head generation methods.}

\setlength{\tabcolsep}{1.0 em} 
{\renewcommand{\arraystretch}{1.5}
\begin{tabular}{c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Metrics}} \\ \cline{2-4} 
 & \textbf{SLD$\downarrow$} & \textbf{SLV$\downarrow$} & \textbf{SMD$\downarrow$} \\ \hline
MIT~\cite{zhou2020makelttalkMIT} &3.00  &0.94  &5.03   \\ 
PCT~\cite{zhou2021pose} &3.58  &0.93  &7.28  \\ 
LSP~\cite{lu2021liveLSP} &5.40  &0.91  &6.89  \\ 
AD-NEFT~\cite{guo2021adneft} &4.69 &0.92  &5.48  \\ \hline \hline
Ours &\textbf{2.84}  &\textbf{0.89}  &\textbf{4.26} \\ \hline
\end{tabular}
}
\vspace{2ex}

\vspace{-2ex}
\label{tab:style_quantitative}
\end{table}

\begin{figure}[t]
   \centering
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{ccccc}

\shortstack{\rotatebox[origin=l]{90}{\hspace{0.35cm}
}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/Maps/0046.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/Maps/0049.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/Maps/0052.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/Maps/0057.jpg}}\\[1pt]
\rotatebox[origin=l]{90}{\hspace{0.4cm} 
}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/woSpotlights/0046.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/woSpotlights/0049.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/woSpotlights/0052.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/woSpotlights/0057.jpg}}\\[1pt]
% \hline\\
\rotatebox[origin=l]{90}{\hspace{0.55cm} 
}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/wSpotlights/0046.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/wSpotlights/0049.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/wSpotlights/0052.jpg}}&
\shortstack{\includegraphics[width=0.33\linewidth]{images/StyleExtreme/wSpotlights/0057.jpg}}\\[1pt]
\end{tabular}
}
    \caption{Effectiveness of ISP and Style-Aware loss. First row: The facial map; Second row: the 2D talking head without ISP and Style-Aware loss; Third row: the 2D talking head with ISP and Style-Aware loss.}
    \label{fig:AblStyle}
\end{figure}

\subsection{Intermediate Style Pattern Analysis}
In Figure~\ref{fig:AblStyle}, we investigate the effectiveness of our Style-Aware Generator when using the ISP. This figure illustrates how the ISP and the Style-Aware loss (Equation~\ref{eq_style_aware_loss}) provide meaningful information to generate better photo-realistic results. Overall, we observe that the rendered frames using the ISP have more realistic and detailed faces, especially around the mouth and the eye of the character.

Specifically, in some exceptional circumstances (e.g., in the \texttt{opera} singing style, the mouth of the character is widely opened), the face motions contain the unique shape of landmarks which is rare or not well captured in the training data (see the first row of Figure~\ref{fig:AblStyle}). As a consequence, the Style-Aware Generator, which is responsible for rendering 2D motions, cannot handle this problem. Hence, the rendered faces are not realistic and have blurry and ghosting effects (i.e., the second row of Figure~\ref{fig:AblStyle}). To address this problem, the ISP and Style-Aware loss can be utilized to mitigate these effects. The ISP can give useful information about the appearance of the stylized motion and help the image generator focus on critical visual cues. As the result, our designed Style-Aware Generator produces better photo-realistic results (see the third row of Figure~\ref{fig:AblStyle}).


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, keepaspectratio=true]{imgs/UserStudy1.png}
    \caption{User study results of three criteria forming natural talking heads.}
    \label{fig:UserStudy1}
\end{figure}

\subsection{User Study}
\label{subsec:UserStudy}
We further conduct user studies to verify our style-aware talking head generation method. We set up three user studies and recruit 56 people with different backgrounds for our experiment. In the first and second studies, we compare the naturality of talking heads and how well the styles are transferred between our work and recent work PCT~\cite{zhou2021pose}, MIT~\cite{zhou2020makelttalkMIT}, LSP~\cite{lu2021liveLSP}. In the third study, we verify the robustness of our proposal in transferring personalized style. Note that, to achieve fair judgment, the users only see the output images/videos, and not the name of any methods in all studies.

\subsubsection{Natural Talking Head Animation Study}
Throughout this study, our app will play one video at a time in a randomized order, and each participant will be asked to rate the video based on three statements: \textit{(i)} Is the mouth of the talking person synchronized with the corresponding audio? \textit{(ii)} Is the expression of the face appropriate for the audio? And \textit{(iii)} Is the head motion natural?. We set a score band between 1 to 4 (4-yes, 3-yes but some parts of the video are not good enough, 2-no but some parts of the video are pleased, 1-no). We make 30 videos for each method. 
All of the mentioned videos have inputs unseen in the training set. Figure~\ref{fig:UserStudy1}  demonstrates the average scores of different methods on three  questions. 

It can be seen that our method achieves the best results across all three questions. Our approach received the highest score of $2.6$ for the first question, indicating that our findings have the best mouth synchronization outcomes,  especially with challenging cases like \texttt{rap} or \texttt{opera} style. We believe that the Style-Aware Generator is successfully trained to generalize and capture unique mouth shapes in challenging cases. However, based on the user feedback, we note that there is room for further improvement. In the meanwhile, other questions show that our method captures the facial expression more effectively and provides more realistic head motions than other recent methods.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth, keepaspectratio=true]{imgs/UserStudy3.png}
    \caption{User study results which verify the effectiveness of the style transfer process when different styles are transferred on fixed input images to generate talking heads.}
    \label{fig:UserStudy3}
\end{figure}

\subsubsection{Style Transfer: Robustness Study} This study analyses different characteristics of the style transfer results in our method. In this study, our app shows an input image of a character, a style reference video, an audio, and the corresponding generated talking head video. 
To be more specific, we use a single image from each of the five persons in this study, which are Lea, Scarlett, Trump, Emma, and Natalie. We also collected nine talking/singing clips on the internet to use as the style reference videos. For each style reference, we first apply our transferring procedure and leverage the learned model to generate 10 style transferred talking head animations corresponding to 10 audio sequences. 
Each participant is asked if the style reference video and the style transferred 2D talking head animations represent a similar talking style.
There are three questions to verify whether the style is successfully transferred: \textit{(i)} How successful is the style of head pose transferred?, \textit{(ii)} How successful is the style of eye contact transferred?, and \textit{(iii)} How successful is the style of mouth motions transferred?
The band score is similar to the previous user study.

Figure~\ref{fig:UserStudy3} demonstrates the scores of our methods on five fixed images that have talking heads generated from different style references. Although our method achieves reasonable scores in head pose transfer and eye contact criteria (3.3 and 3.5 on average, respectively), the score on the mouth motion transfer criteria is lower (2.9 in average). 
This result confirms that, although our method can capture the unique shape of mouth landmarks in style references and transfer it into input images, this task is not trivial and needs more improvement in the future.





