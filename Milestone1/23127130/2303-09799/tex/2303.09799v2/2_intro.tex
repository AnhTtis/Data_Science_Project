\section{Introduction}

Talking head animation is an active research topic in both academia and industry. This task has a wide range of real-world interactive applications such as digital avatars~\cite{cudeiro_capture_2019}, speech tutoring~\cite{dey2010talking}, video conferencing~\cite{wang_one-shot_2021}, virtual reality~\cite{morishima1998real,badin1998towards,latif2021talking}, computer games~\cite{xie2015expressive,guo2021adneft}, and digital animations~\cite{aiozGdance}. Given an arbitrary input audio and a 2D image (or a set of 2D images) of a character, the goal of talking head animation is to generate photo-realistic frames. 
The output can be the 2D~\cite{zhou2020makelttalkMIT,guo2021adneft,lu2021liveLSP} or 3D talking head~\cite{cudeiro_capture_2019,zhou_visemenet_2018,taylor_deep_2017}. 
With recent advances in deep learning, especially generative adversarial networks~\cite{goodfellow_generative_2014}, several works have addressed  different aspects of the talking head animation task such as head pose control~\cite{zhou2021pose,zhang20213d}, facial expression~\cite{le2022global,le2023uncertainty}, emotion generation~\cite{livingstone2018ryersonRAVDESS,eskimez2021speech}, and photo-realistic synthesis~\cite{zhou2020makelttalkMIT,vougioukas2019end,chen_photo-realistic_2019}.


While there has been considerable advancement in the generation of talking head animation, achieving photo-realistic and fidelity animation is not a trivial task. It is even more challenging to render natural motion of the head with different styles~\cite{cudeiro_capture_2019}. In practice, several aspects contribute to this challenge. First, generating a photo-realistic talking head using only a single image and audio as inputs requires multi-modal synchronization and mapping between the audio stream and facial information~\cite{edwards2016jali}. In many circumstances, this process may result in fuzzy backgrounds, ambiguous fidelity, or abnormal face attributes~\cite{zhou2020makelttalkMIT}. 
Second, various talking and singing styles can express diverse personalities~\cite{walker1997improvising}. Therefore, the animation methods should be able to adapt and generalize well to different styles~\cite{walker1997improvising}. Finally, controlling the head motion and connecting it with the full-body animation remains an open problem~\cite{jiang2016real}.
 
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{imgs/IntroVis_v6.png}
\label{fig:introvis}
\caption{Given an audio stream, a single image, and a set of style reference frames, our method generates realistic 2D talking head animation. Please also see our supplementary video. Video \textit{Adele \copyright YouTube} (public domain). Image \textit{Lea \copyright Twitter} (public domain).}\vspace{3ex}
\end{figure}

Recently, several methods have been proposed to generate photo-realistic talking heads~\cite{guo2021adneft,lu2021liveLSP,suwajanakorn_synthesizing_2017,zhou2020makelttalkMIT} or to match the pose from a source video~\cite{zhou2021pose} while little work has focused on learning the personalized character style~\cite{lu2021liveLSP}. In practice, apart from personalized talking style, we have different singing styles such as \texttt{ballad} and \texttt{rap}. These styles pose a more challenging problem for talking head animation as they have the unique eye, head, mouth, and torso motion. The facial movements of singing styles are also more varied and dynamic than the talking style. Therefore, learning and bringing these styles into 2D talking heads is more challenging. Currently, most of the style-aware talking head animation methods do not fully disentangle the audio style information and the visual information, which causes ambiguity during the transferring process~\cite{lu2021liveLSP}.



In this work, we present a new deep learning framework called Style Transfer for 2D talking head animation. Our framework provides an effective way to transfer talking or singing styles from the style reference to animate single 2D portrait of a character given an arbitrary input audio stream. We first generate photo-realistic 2D animation with natural expression and motion. We then propose a new method to transfer the personalized style of a character into any talking head with a simple style-aware transfer process. 
Figure~\ref{fig:introvis} shows an overview of our approach. 




In summary, our contributions are as follows:
\begin{itemize}
    \item We propose a new framework for generating photo-realistic 2D talking head animations from the audio stream as input. 
    \item We present a style-aware transfer technique, which enables us to learn and apply any new style to the animated head. Our generated 2D animation is photo-realistic and high fidelity with natural motions.
    
    \item We conduct intensive analysis to show that our proposed method outperforms recent approaches qualitatively and quantitatively. Our source code and trained models will be released for reproducibility.
\end{itemize}
