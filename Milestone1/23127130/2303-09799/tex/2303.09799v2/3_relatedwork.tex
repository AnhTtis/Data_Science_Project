\section{Literature Review}
\label{Sec:Literature}

\textbf{2D Talking Head Animation.} 
Creating talking head animation from an input image and audio has been widely studied in the past few years. 
One of the earliest works~\cite{bregler_video_1997} considered this as a sorting task that reorders images from footage video using the phoneme sequence. 
Based on~\cite{bregler_video_1997},~\cite{garrido_vdub_2015} proposed to capture 3D model from dubber and actor in order to synthesize photo-realistic face.~\cite{ezzat_trainable_2004} introduced a learning approach to create a trainable system that could synthesize a mouth shape from an unseen utterance. 
Later works focused on audio-driven to generate realistic mouth shapes~\cite{suwajanakorn_synthesizing_2017,taylor_deep_2017} or realistic faces~\cite{zhou2019talking,songtalking,chen2019hierarchical,mittal2020animating, grassal2022neural}. 
The authors in~\cite{eskimez_generating_2018} focused on generating full facial landmarks using the input audio. ~\cite{wiles_x2face_2018} moved into a different direction by focusing on generating talking face that include pose and expression of another face video.
Instead of creating talking face,~\cite{greenwood_joint_2018} designed a model that produces head motion from the joint latent space using BiLSTM.~\cite{zakharov2019few,zakharov2020fast,chen2020talking,kumar2020robust,liang2022expressive} pave the way for creating realistic head avatars.~\cite{vougioukas2019end, vougioukas_realistic_2020,wangaudio2head,ren2021pirenderer,zhang2021flow,hong2022depth,wang2022one} used only a single image and audio to develop an end-to-end generation network. 
\cite{chen2019hierarchical} focused on handling noise and different facial shapes and angles. 
\cite{ginosar_learning_2019} proposed a model that can learn conversational gestures. 
\cite{yin2022styleheat,yao2022dfa,liu2022semantic,hong2022headnerf} focused on generating fidelity talking head which natural head pose and photo-realistic motions. Recently, \cite{lu2021liveLSP,zhou2022dialoguenerf} proposed to generate photo-realistic talking head with personalized information encoded. 


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio=true]{imgs/2DStyleProposal_V9.png}
    \vspace{-3ex}
    \caption{A detailed illustration of our method.}
    \vspace{-1ex}
    \label{fig:overview}
\end{figure*}



\textbf{Speaker Style Estimation.} There are many kinds of speaker styles such as generic, personal, controlled pose, or special expression. Generic style could be learned by training on multiple videos \cite{zhou2020makelttalkMIT,thies_neural_2020,richard_meshtalk_2021}, while personalized style could be decided by training on one avatar particularly~\cite{suwajanakorn_synthesizing_2017, lu2021liveLSP, lahiri_lipsync3d_2021}. In~\cite{zhou2021pose}, the authors introduced a method that generates controllable poses with an input video.~\cite{wiles_x2face_2018} transferred poses and expressions from another video input.~\cite{garrido_vdub_2015} mapped the style from dubber to actor.~\cite{siarohin2019first, romero2021smile, wang_one-shot_2021} captured motions from the driven video and transferred them into input image during the generation process.~\cite{tsao2009ensemble} tried to ensemble speaker and speaking environment to characterize the speaker variability in the environment.~\cite{liu2015video} leveraged a pre-captured database of 3D mouth shapes and associated speech audio from one speaker to refine the mouth shape of a new actor.
Likewise, many works did not restrict to a specific style but could be adapted to general and controllable results~\cite{zhou_visemenet_2018,cudeiro_capture_2019}.

\textbf{Speech Representation for Face Animation.} Some prior works used hand-crafted models to match phoneme and mouth shape in each millisecond audio signal as speech representation~\cite{bregler_video_1997, zhou_visemenet_2018}. DeepSpeech~\cite{hannun_deep_2014} paved the way for learning a speech recognition system using an end-to-end deep network. Following that, ~\cite{greenwood_joint_2018} trained Deep Bi-Directional LSTMs to learn a language-long-term structure that model the relationship between speech and the complex activity of faces.
~\cite{suwajanakorn_synthesizing_2017} used Mel-frequency spectral coefficients to synthesize high-quality mouth texture of a character, and then combined it with a 3D pose matching method to synchronize the lip motion with the audio in the target animation.
In our work, similar to~\cite{lu2021liveLSP}, we use manifold learning to generalize the style information from speech representation.


