\documentclass[11pt]{article}
%\documentclass[11pt, onecolumn]{IEEEtran}
%\documentclass[lettersize,journal]{IEEEtran}

\usepackage{amsthm, amsmath, amsfonts,amssymb}

\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{verbatim}
\usepackage{graphicx}

\usepackage[noadjust]{cite}

\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\allowdisplaybreaks
\usepackage[colorlinks=true, allcolors=red]{hyperref}

\usepackage{soul, color, xcolor}

%New enviorments%
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

%Macros%
\def\mathbi#1{{\textbf{\textit #1}}}
\newcommand{\ff}{{\mathbb F}}
\newcommand{\Fq}{\mathbb{F}_{q}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}

\newcommand{\red}[1]{\color{red}#1~\color{black}}
\newcommand{\blue}[1]{\color{blue}#1~\color{black}}
\newcommand{\gray}[1]{\color{gray}#1\color{black}}

\newcommand{\bI}{\mathbf{I}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}

\newcommand{\smat}{\ \vert \ }

\newlength{\spacelen}
\newcommand{\ws }[1]{\settowidth{\spacelen}{#1}\makebox[\spacelen]{}}
\newcommand{\wid}[3]{\settowidth{\spacelen}{#1}\makebox[\spacelen][#2]{$#3$}}
\newcommand{\win}[2]{\makebox[#1][l]{$#2$}}
\newlength{\lenx}\settowidth{\lenx}{$-\bI_{s^a}\otimes L_{as^2+(s-1)s}$}
\newlength{\leny}\settowidth{\leny}{$-\bI_{s^a}\otimes L_{as^2+(s-1)s+1}$}
\newlength{\lenz}\settowidth{\lenz}{$-\bI_{s^a}\otimes L_{as^2+s^2-1}$}
\newcommand{\wn}{\ws{$-$}}
\newcommand{\sL}[2][\ws{$-$}]{#1 \bI_{s^a} \otimes L_{#2}^{(t)}}
\newcommand{\cvdots}{\wn \hspace*{0.2625in}\vdots}
\newcommand{\cgbO}{\wn \hspace*{0.22in}\gbO}
\newcommand{\rL}[2][\ws{$-$}]{\wid{$-L_{00}$}{l}{#1 L_{#2}}}
\newcommand{\rz}{\wid{$-L_{00}$}{l}{\ws{$-$}\gbzero}}
\newcommand{\tL}[3][\ws{$-$}]{#1L_{#2}^{(#3)}}
\newcommand{\tz}[2][\ws{$-$}]{#1 \color{gray}\mathbf{0}\color{black}}

\newcommand{\bz}[1]{\mathbf{0}^{(#1)}}
\newcommand{\bzero}{\mathbf{0}}

\newcommand{\gbz}[1]{\color{gray}\mathbf{0}\color{black}}
\newcommand{\gbzero}{\color{gray}\mathbf{0}\color{black}}

\newcommand{\gzero}{\color{gray}0\color{black}}
\newcommand{\gbO}{\color{gray}\mathbf{O}\color{black}}

\newcommand{\wngbz}[1]{\wn\color{gray}\mathbf{0}\color{black}}

\title{MSR codes with linear field size and smallest sub-packetization for any number of helper nodes}
\author{Guodong Li, \and Ningning Wang, \and Sihuang Hu, \and Min Ye}
\date{}
\usepackage{inputenc}

\newcommand{\margnote}[1]{\marginpar{\blue{#1}}}

\begin{document}

\maketitle
{
	\renewcommand{\thefootnote}{}\footnotetext{

		\vspace{-.1in}

		%\vspace{-.2in}

		%\noindent\rule{1.5in}{.4pt}

		Research partially funded by
		National Key R\&D Program of China under Grant No. 2021YFA1001000,
		National Natural Science Foundation of China under Grant No. 12001322 and 12231014,
		Shandong Provincial Natural Science Foundation under Grant No. ZR202010220025,
		Shenzhen Stable Support program under Grant No. WDZC20220811170401001, RISC-V International Open Source Laboratory,
		and a Taishan scholar program of Shandong Province.

		Guodong Li, Ningning Wang and Sihuang Hu are with Key Laboratory of Cryptologic Technology and Information Security, Ministry of Education, Shandong University, Qingdao, Shandong, 266237, China and School of Cyber Science and Technology, Shandong University, Qingdao, Shandong, 266237, China.
		S. Hu is also with Quan Cheng Laboratory, Jinan 250103, China.
		Email: \{guodongli, nningwang\}@mail.sdu.edu.cn, husihuang@sdu.edu.cn

		Min Ye is with Tsinghua-Berkeley Shenzhen Institute, Tsinghua Shenzhen International Graduate School, Shenzhen 518055, China. Email: yeemmi@gmail.com
	}
}

\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

\begin{abstract}
	An $(n,k,\ell)$ array code has $k$ information coordinates and $r = n-k$ parity coordinates, where each coordinate is a vector in $\mathbb{F}_q^{\ell}$ for some field $\mathbb{F}_q$.
	An $(n,k,\ell)$ MDS array code has the additional property that any $k$ out of $n$ coordinates suffice to recover the whole codeword.
	Dimakis \emph{et al.} considered the problem of repairing the erasure of a single coordinate and proved a lower bound on the amount of data transmission that is needed for the repair.
	A minimum storage regenerating (MSR) array code with repair degree $d$ is an MDS array code that achieves this lower bound for the repair of any single erased coordinate from any $d$ out of $n-1$ remaining coordinates.
	An MSR code has the optimal access property if the amount of accessed data is the same as the amount of transmitted data in the repair procedure.

	The sub-packetization $\ell$ and the field size $q$ are of paramount importance in the MSR array code constructions.
	For optimal-access MSR codes, Balaji \emph{et al.} proved that $\ell\geq s^{\left\lceil n/s \right\rceil}$, where $s = d-k+1$.
	Rawat \emph{et al.} showed that this lower bound is attainable for all admissible values of $d$ when the field size is exponential in $n$.
	After that, tremendous efforts have been devoted to reducing the field size.
	However, till now, reduction to linear field size is only available for $d\in\{k+1,k+2,k+3\}$ and $d=n-1$.
	In this paper, we construct optimal-access MSR codes with linear field size and smallest sub-packetization $\ell = s^{\left\lceil n/s \right\rceil}$ for all $d$ between $k+1$ and $n-1$.
	We also construct another class of MSR codes that are not optimal-access but have even smaller sub-packetization $s^{\left\lceil n/(s+1)\right\rceil }$.
	The second class also has linear field size and works for all admissible values of $d$.
	Previously, MSR codes with $\ell=s^{\left\lceil n/(s+1)\right\rceil }$ and $q=O(n)$ were only known for $d=k+1$ and $d=n-1$.
	The key insight that enables a linear field size in our construction is to reduce $\binom{n}{r}$ global constraints of non-vanishing determinants to $O_s(n)$ local ones, which is achieved by carefully designing the parity check matrices.
\end{abstract}

\section{Introduction}\label{sect:intro}

Maximum Distance Separable (MDS) codes are widely used in storage systems because they provide the maximum failure tolerance for a given amount of storage overhead.
With the emergence of large-scale distributed storage systems, the notion of \emph{repair bandwidth} was introduced to measure the efficiency of recovering the erasure of a single codeword coordinate.
Following the literature on distributed storage, a codeword coordinate is referred to as a (storage) node, and the erasure of a codeword coordinate is called a node failure.
Repair bandwidth is the amount of data that needs to be downloaded from the remaining nodes to repair the failed node.
For an $(n,k)$ MDS array code, Dimakis \emph{et al.}~\cite{Dimakis10} showed that if we access $d$ out of $n-1$ remaining nodes, then at least $1/(d-k+1)$ fraction of data needs to be downloaded from each of the $d$ helper nodes.
The range of $d$ is between $k+1$ and $n-1$.
A minimum storage regenerating (MSR) code with {\em repair degree} $d$ is an MDS code that achieves the minimum repair bandwidth for the repair of any single node failure from any $d$ helper nodes.
We further say that an MSR code has the \emph{optimal access} property if the amount of accessed data in the repair procedure is equal to the minimum repair bandwidth.

The study of MSR codes is divided into two major categories---scalar MSR codes and MSR array codes.
Reed-Solomon (RS) code is the most prominent example of scalar MDS codes, whose repair problem was first studied by Guruswami and Wootters  \cite{Guruswami16STOC, Guruswami16}.
In particular, Guruswami and Wootters viewed each codeword coordinate as a vector over a subfield of RS codes' symbol field and proposed repair schemes with smaller bandwidth than the naive repair scheme.
The degree of field extension is called sub-packetization.
Building on the framework in \cite{Guruswami16STOC, Guruswami16}, Tamo \emph{et al.}~\cite{Tamo17RS} proposed an RS code construction with optimal repair bandwidth, which is a scalar MSR code.
In contrast, for an $(n,k,\ell)$ MSR array code, each codeword coordinate is a vector in $\mathbb{F}_q^{\ell}$ for some field $\mathbb{F}_q$, where the parameter $\ell$ is sub-packetization.
Compared to scalar MSR codes, their array code counterparts allow more flexibility in the code construction, which results in a smaller sub-packetization.
Indeed, the best-known sub-packetization of MSR array code construction is $\exp(O(n))$ while the sub-packetization of (high-rate) scalar MSR codes needs to be $\exp(\Theta(n\log(n))$, according to a lower bound in \cite{Tamo17RS}.
For this reason, we only consider MSR array codes in this paper.

\subsection{Previous results on MSR codes with small sub-packetization}
Numerous constructions of MSR codes have been proposed in recent years.
The first explicit construction of MSR codes is the product matrix construction \cite{Rashmi11}.
Yet it is limited to the low code rate regime.
After that, explicit constructions of high-rate MSR codes were given in \cite{Tamo13, Wang16, Ye16, Ye16a, Sasid16, Raviv17, Tamo17RS, Li18, Duursma21, Vajha21}.
A complete list of constructions and parameters of MSR codes can be found in an excellent survey paper~\cite{Kumar18}.
In practical storage systems, the number of parity nodes $r=n-k$ is usually a rather small number in order to achieve a small storage overhead.
In light of this, we consider the regime of fixed $r$ and growing $n$ throughout this paper.

All the constructions mentioned above have exponentially large sub-packetization, which is in fact inevitable for MSR codes.
More precisely, for MSR codes with repair degree $d=n-1$, Goparaju \emph{et al.} \cite{Goparaju14} proved that $\ell\ge \exp(\Theta(\sqrt{n}))$.
This lower bound was later improved to $\ell\ge \exp(\Theta(n))$ by Alrabiah and Guruswami \cite{Alrabiah19}.
Moreover, a tight lower bound was known for optimal-access MSR codes: It was shown in \cite{Tamo14} that $\ell \ge r^{\frac{k-1}{r}}$ for such codes with repair degree $d=n-1$; Balaji \emph{et al.} \cite{Balaji22} generalized this bound to a restricted class of optimal-access MSR codes for all admissible values of $d$.
The lower bound in \cite{Balaji22} is $\ell \ge s^{\lceil\frac{n}{s}\rceil}$, where $s=d-k+1$.

For $d=n-1$, papers~\cite{Ye16a, Sasid16, Li18} constructed
optimal-access MSR codes with field size $q=O(n)$ and sub-packetization  $\ell=r^{\lceil\frac{n}{r}\rceil}$, matching the lower bound mentioned above.
For general values of $d$, a construction of optimal-access MSR codes with $\ell=s^{\lceil\frac{n}{s}\rceil}$ was presented in~\cite{Rawat16}.
However, the construction in~\cite{Rawat16} is not explicit and requires an exponentially large field size.
For $d<n-1$, optimal-access MSR codes with the smallest sub-packetization $\ell=s^{\lceil\frac{n}{s}\rceil}$ and linear field size are only known for $d\in\{k+1,k+2,k+3\}$, which come from a recent paper~\cite{Vajha21}.
If we do not require the optimal-access property, then the best-known sub-packetization level is $\ell=s^{\left\lceil n/(s+1)\right\rceil }$ for MSR codes with repair degree $d$.
MSR codes with $\ell=s^{\left\lceil n/(s+1)\right\rceil }$ and linear field size are only known for $d=n-1$ (see~\cite{Wang16}) and $d=k+1$ (see~\cite{LWHY22}).
In~\cite{Liu22, Zhang23, Li23} explicit constructions of MSR codes with $\ell=s^{\left\lceil\frac{n}{2}\right\rceil}$ for any $d$ are presented.

\subsection{Our contribution and key technique}

In this paper, we propose two classes of MSR codes with linear field size $q=O_s(n)$ for any repair degree $d$ between $k+1$ and $n-1$, where $O_s$ means that we treat $s$ as a constant\footnote{We can do so because $s$ is upper bounded by $r$, which is assumed to be a constant.}.
The first class is optimal-access and has sub-packetization $s^{\left\lceil n/s \right\rceil}$, matching the lower bound.
This settles the open problem of constructing optimal-access MSR codes with linear field size and the smallest sub-packetization for general values of $d$.
The second class is not optimal-access but has a smaller sub-packetization $s^{\left\lceil n/(s+1)\right\rceil }$,
which is the best-known sub-packetization among all the existing MSR code constructions.
%\textcolor{red}{This makes a great improvement upon the existing constructions of sub-packetization level $s^{n/2}$.}

We give an overview of our proof techniques here.
We define an $(n, k, \ell)$ array code $\cC$ using the parity check equations
\begin{equation}\label{eq:parity_check}
	\cC=\{(C_0, C_1, \dots, C_{n-1}): H_0C_0 + H_1C_1 + \cdots + H_{n-1}C_{n-1} = \bzero\},
\end{equation}
where each node $C_i = (C_i(0), C_i(1), \dots, C_{i}(\ell-1))^T$ is a column vector in $\Fq^{\ell}$,
each $H_i$ is an $r\ell \times \ell$ matrix over $\Fq$, and $\bzero$ on the right-hand side is the all-zero column vector of length $r\ell$.
The MDS property of $\cC$ is equivalent to the following $\binom{n}{r}$ \emph{global constraints} of non-vanishing determinants:
\begin{equation}\label{eq:detMDS}
	\det([H_{i_1} \smat  H_{i_2} \smat \cdots \smat H_{i_r}])\neq0 \text{ for~all~} \cF = \{i_1, i_2, \dots, i_r\}\subseteq\{0,\dots,n-1\} \text{~with size~} |\cF|=r.
\end{equation}
We divide $n$ parity check matrices $H_0, H_1,\dots,H_{n-1}$ into $n/s$ groups of size $s$ and
$n/(s+1)$ groups of size $s+1$ in the first and the second construction, respectively.
By carefully designing the inner structures of each $H_i$, we are able to make the matrices involved in global
constraints~\eqref{eq:detMDS} block upper triangular, and
reduce $\binom{n}{r}$ global constraints to
$O_s(n)$ \emph{local (in-group) constraints} of non-vanishing determinants.
This allows us to prove that a linear field size suffices for our construction.

When discussing our $(n,k,\ell = s^{\lceil n/s \rceil})$ MSR code, we always assume that $n$ is divisible by $s$.
For the case of $s\nmid n$, the construction can be easily obtained by removing some nodes from a slightly longer code in the divisible case.
Similarly, when we discuss our $(n,k,\ell = s^{\lceil n/(s+1) \rceil})$ MSR code, we always assume that $n$ is divisible by $s+1$.

The rest of this paper is organized as follows:
In Section~\ref{sect:cons}, we give the constructions of two new MSR codes.
In Section~\ref{sect:example}, we provide a concrete example of $(n, k, \ell = s^{n/s})$ MSR codes and use it to illustrate
how to prove the MDS property and the optimal repair bandwidth.
In Section~\ref{sect:mds}, we prove the MDS property of the new codes.
In Section~\ref{sect:repair}, we prove that these two classes of MDS array codes have the optimal repair bandwidth.

\section{New code constructions}\label{sect:cons}

Let $\Fq$ be a finite field with order $q$,
and choose $ns$ distinct elements $\lambda_0, \lambda_1, \dots, \lambda_{ns-1}$ from $\Fq$.
For a positive integer $m$, we set $[m]=\{0,1,\dots, m-1\}$.
For a positive integer $t$ and $i \in [ns]$,  we define
\begin{equation}\label{eq:L}
	L_i^{(t)} := \left[
		\begin{array}{c}
			1           \\
			\lambda_i   \\
			\lambda_i^2 \\
			\vdots      \\
			\lambda_i^{t-1}
		\end{array}
		\right].
\end{equation}
We use $\bz{t}$ to denote the all-zero column vector of length $t$.
We may omit the superscripts of both $L_i^{(t)}$ and $\bz{t}$ when their lengths are clear from the context.

\subsection{Construction of $(n,k,\ell = s^{n/s})$ MSR code with optimal access property}\label{sect:consA}

In this subsection, we set the sub-packetization $\ell = s^{n/s}$.
We divide the codeword $(C_0, C_1, \cdots C_{n-1})$ of the $(n,k,\ell)$ array code into $n/s$ groups of size $s$.
We use $a\in [n/s], b\in [s]$ to denote the index of the group and the index of the node within its group respectively.
In other words, the $a$th group consists of the $s$ nodes $(C_{as+b}: b\in [s])$.

Before we define those parity check matrices $H_i$, we first introduce the following \emph{kernel matrices} for our convenience.
For an integer $m$ we denote $\bI_{m}$ as the $m\times m$ identity matrix over $\Fq$.
For $a\in [n/s], b\in [s]$, and a positive integer $t$, the kernel matrix $K_{a,b}^{(t)}$ is an $s\times s$ block matrix with block entry
\begin{equation}\label{eq:kerA}
	K_{a,b}^{(t)}(i, j) = \left\{
	\begin{aligned}
		  & \bI_{s^a}\otimes  L_{as^2+bs+j}^{(t)} &  & \text{~if~} i = j       \\
		- & \bI_{s^a}\otimes L_{as^2+bs+j}^{(t)}  &  & \text{~if~} i = b\not=j \\
		  & \bI_{s^a}\otimes \bz{t}               &  & \text{~otherwise},
	\end{aligned}
	\right.
\end{equation}
where we use $i,j \in [s]$ to denote the block row index and block column index of $K_{a,b}^{(t)}$ respectively.
The $s$ kernel matrices in the $a$th group can be written as
\begin{align*}
	K_{a,0}^{(t)}   & =
	\left[\begin{array}{llll}
			      \win{\lenx}{\sL{as^2}} & \win{\leny}{\sL[-]{as^2+1}} & \cdots & \win{\lenz}{\sL[-]{as^2+s-1}} \\
			      \cgbO                  & \sL{as^2+1}                 & \cdots & \cgbO                         \\
			      \cvdots                & \cvdots                     & \ddots & \cvdots                       \\
			      \cgbO                  & \cgbO                       & \cdots & \sL{as^2+s-1}
		      \end{array} \right], \\
	K_{a,1}^{(t)}   & =
	\left[\begin{array}{llll}
			      \sL{as^2+s}                 & \cgbO                      & \cdots & \cgbO                          \\
			      \win{\lenx}{\sL[-]{as^2+s}} & \win{\leny}{\sL{as^2+s+1}} & \cdots & \win{\lenz}{\sL[-]{as^2+2s-1}} \\
			      \cvdots                     & \cvdots                    & \ddots & \cvdots                        \\
			      \cgbO                       & \cgbO                      & \cdots & \sL{as^2+2s-1}
		      \end{array}\right], \\
	\dots           &                                                                                                              \\
	K_{a,s-1}^{(t)} & =
	\left[\begin{array}{llll}
			      \sL{as^2 + (s-1)s}                 & \cgbO                                & \cdots & \cgbO                          \\
			      \cgbO                              & \sL{as^2 + (s-1)s+1}                 & \cdots & \cgbO                          \\
			      \cvdots                            & \cvdots                              & \ddots & \cvdots                        \\
			      \win{\lenx}{\sL[-]{as^2 + (s-1)s}} & \win{\leny}{\sL[-]{as^2 + (s-1)s+1}} & \cdots & \win{\lenz}{\sL{as^2 + s^2-1}}
		      \end{array}\right], \\
\end{align*}
where $\otimes$ is the Kronecker product, and $\bO = \bI_{s^a}\otimes \bz{t}$ is the $s^at\times s^a$ all-zero matrix.
We can see that
\begin{enumerate}
	\item[(i)] the kernel matrix $K_{a,b}^{(t)}$ is an $s^{a+1}t\times s^{a+1}$ matrix  over $\Fq$, and thus the sizes of kernel matrices in distinct groups are different.
	\item[(ii)] the block entry of $K_{a,b}^{(t)}$ is nonzero if and only if it lies on the main block diagonal, or the $b$th block row.
\end{enumerate}
Now we use these kernel matrices to define the following $\ell t\times \ell$ matrices
\begin{equation}\label{eq:matA_use_kerA}
	A_{a,b}^{(t)} = \bI_{\ell/s^{a+1}}\otimes K_{a,b}^{(t)}.
\end{equation}
We also treat $A_{a,b}^{(t)}$ as an $\ell\times \ell$ block matrix in which each block is a column vector of length $t$.
For $i\in [s^{n/s}]$, we can write its corresponding $n/s$-digit base-$s$ expansion as
$i = (i_{n/s-1}, i_{n/s-2}, \dots, i_{1}, i_0)$ where $i_u\in [s]$ for all $u\in [n/s]$.
It is not hard to check that the block entry
\begin{equation}\label{eq:matA}
	A_{a,b}^{(t)}(i, j) = \left\{
	\begin{aligned}
		  & L_{as^2+bs+j_a}^{(t)} &  & \text{~if~} i = j                                                  \\
		- & L_{as^2+bs+j_a}^{(t)} &  & \text{~if~} i_a = b\not=j_a \text{~and~} i_u=j_u ~\forall~ u\not=a \\
		  & \bz{t}                &  & \text{~otherwise}.
	\end{aligned}
	\right.
\end{equation}
where we use $i,j \in [\ell]$ to denote the block row index and column index of $A_{a,b}^{(t)}$ respectively.

Next, for  $a\in [n/s]$, a positive integer $t$ and a non-empty subset $\cB = \{b_1, b_2, \dots, b_{|\cB|}\} \subseteq[s]$,
we define the following $\ell t\times \ell|\cB|$ matrix
\begin{equation}\label{eq:localmatsA}
	A_{a, \cB}^{(t)}= [A_{a, b_1}^{(t)} \smat A_{a, b_2}^{(t)} \smat \cdots \smat A_{a, b_{|\cB|}}^{(t)} ].
\end{equation}
If $t=|\cB|$, then $A_{a, \cB}^{(t)}$ is a $t\ell\times t\ell$ square matrix, and
we write $A_{a, \cB}  = A_{a, \cB}^{(|\cB|)}$ for convenience.

Recall that we first require all the $ns$ $\lambda_i$s to be distinct.
In addition, we also need that
\begin{equation}\label{eq:localA}
	\det(A_{a, \cB})\neq 0 \text{~for~all~} a\in [n/s] \text{~and~} \cB \subseteq [s],
\end{equation}
i.e., all the square matrices $A_{a,\cB}$ are invertible.
We will refer to these constraints in~\eqref{eq:localA} as \emph{local (or in-group) constraints}.
The existence of such $\lambda_i$s in a field of order $O_{s}(n)$ is guaranteed by the following result.
\begin{lemma}\label{lemma:fieldsizeA}
	If $q \ge ns + \sum_{t = 1}^{s}\binom{s}{t}\frac{st(t-1)}{2}$, then we can always choose $ns$ distinct elements $\lambda_0,\dots, \lambda_{ns-1}$ from $\Fq$ satisfying \eqref{eq:localA}.
\end{lemma}
We put the proof of Lemma~\ref{lemma:fieldsizeA} in Appendix~\ref{sect:prooflemma1}.

Finally, we are ready to define the parity check matrices for our $(n,k,\ell = s^{n/s})$ MSR codes.
For each $a\in [n/s]$ and $b\in [s]$, the $\ell r\times \ell$ matrix $H_{as+b}$ in \eqref{eq:parity_check} is defined as
\begin{equation}\label{eq:pcmA}
	H_{as+b} = A_{a,b}^{(r)}.
\end{equation}
The MDS property and optimal repair of our MSR codes defined by \eqref{eq:parity_check} and \eqref{eq:pcmA} will follow directly from
the following \emph{global constraints}.
\begin{lemma}\label{lemma:globalA}
	For any $c$ distinct integers $a_0, a_1, \dots, a_{c-1}\in [n/s]$ and $c$ subsets $\cB_0,\cB_1,\dots, \cB_{c-1}\subseteq [s]$ satisfying
	$|\cB_0| + |\cB_1| + \cdots + |\cB_{c-1}| = t \le r$, we have
	\begin{equation*}
		\det\left([ A_{a_0, \cB_0}^{(t)}\smat A_{a_1, \cB_1}^{(t)} \smat \cdots  \smat A_{a_{c-1}, \cB_{c-1}}^{(t)} ]\right)\neq0.
	\end{equation*}
\end{lemma}
One key contribution of our paper is to show that these global constraints can
be derived from the above local ones~\eqref{eq:localA}.
We will give the detailed proof in Section~\ref{sect:mds}.

\subsection{Construction of $(n,k,\ell = s^{n/(s+1)})$ MSR code}\label{sect:consB}

In this subsection, we set the sub-packetization $\ell = s^{n/(s+1)}$,
and the codeword $(C_0, C_1, \cdots C_{n-1})$ is divided into $n/(s+1)$ groups of size $s+1$.
(Note that in previous construction the group size is $s$.)
We use $a\in [n/(s+1)], b\in [s+1]$ to denote the index of the group and the index of the node within its group, i.e.,
for each $a\in [n/(s+1)]$, the $s+1$ nodes $(C_{a(s+1)+b}: b\in [s+1])$ are in the $a$th group.

Similarly to the above subsection, we first define kernel matrices.
For  $a\in [n/(s+1)], b\in [s+1]$, and a positive integer $t$,
if we use $i,j\in [s]$ to denote the block row index and the block column index of $K_{a, b}^{(t)}$,
then the block entry of $K_{a,b}^{(t)}$ at the cross of the $i$th block row and $j$th block column is
\begin{equation}\label{eq:kerB}
	K_{a,b}^{(t)}(i, j) = \left\{
	\begin{aligned}
		  & \bI_{s^a}\otimes  L_{as(s+1)+bs+j}^{(t)} &  & \text{~if~} i = j       \\
		- & \bI_{s^a}\otimes L_{as(s+1)+bs+j}^{(t)}  &  & \text{~if~} i = b\not=j \\
		  & \bI_{s^a}\otimes \bz{t}                  &  & \text{~otherwise}  .
	\end{aligned}
	\right.
\end{equation}
Note that the first $s$ kernel matrices in the $a$th group $K_{a,b}^{(t)}, b\in [s]$,
have the same structure as their counterparts of the previous construction,
except that
\begin{align*}
	K_{a,s}^{(t)} =
	\left[
		\begin{array}{llll}
			\sL{as(s+1)+s^2} & \cgbO              & \cdots & \cgbO                \\
			\cgbO            & \sL{as(s+1)+s^2+1} & \cdots & \cgbO                \\
			\cvdots          & \cvdots            & \ddots & \cvdots              \\
			\cgbO            & \cgbO              & \cdots & \sL{as(s+1)+s^2+s-1}
		\end{array}
		\right].
\end{align*}
We can see that a block entry in $K_{a,s}^{(t)}$ is nonzero if and only if it is on the main block diagonal.
Next, we define
\begin{equation}\label{eq:matB_use_kerB}
	A_{a,b}^{(t)} = \bI_{\ell/s^{a+1}}\otimes K_{a,b}^{(t)}.
\end{equation}
It is not hard to check that $A_{a,b}^{(t)}$ is a $ \ell t\times \ell$ matrix over $\Fq$.
For $i\in [s^{n/(s+1)}]$, similar to Section~\ref{sect:consA}, we can write the $n/(s+1)$-digit base-$s$ expansion as
$i = (i_{n/(s+1)-1}, i_{n/(s+1)-2}, \dots, i_0)$, where $i_u\in [s]$ for all $u\in [n/(s+1)]$.
We also treat $A_{a,b}^{(t)}$ as an $\ell\times \ell$ block matrix in which each block is a column vector of length $t$.
We can check that the block entry $A_{a,b}^{(t)}(i, j)$ at the cross of $i$th block row and $j$th column of $A_{a,b}^{(t)}$ is
\begin{equation}\label{eq:matB}
	A_{a,b}^{(t)}(i, j) = \left\{
	\begin{aligned}
		  & L_{as(s+1)+bs+j_a}^{(t)} &  & \text{~if~} i = j                                                     \\
		- & L_{as(s+1)+bs+j_a}^{(t)} &  & \text{~if~} i_a = b\not=j_a \text{~and~} i_u=j_u, \ \forall\  u\not=a \\
		  & \bz{t},                  &  & \text{~otherwise}.
	\end{aligned}
	\right.
\end{equation}
\begin{remark}
	We would like to point out that negative signs in \eqref{eq:kerA} before off-block-diagonal entries are not necessary.
	In other words, removing these negative signs does not affect any property of the array code defined in Section~\ref{sect:consA}.
	In contrast, all negative signs in \eqref{eq:kerB} before off-block-diagonal entries are required.
	Just for consistency, we keep the negative signs in \eqref{eq:kerA}.
\end{remark}
For $a\in [n/(s+1)]$, a positive integer $t$ and a non-empty subset $\cB = \{b_1, b_2, \dots, b_{|\cB|}\} \subseteq[s+1]$,
we define the following $\ell t\times \ell|\cB|$ matrix
\begin{equation*}%\label{eq:localmatsB}
	A_{a, \cB}^{(t)}= [A_{a, b_1}^{(t)} \smat A_{a, b_2}^{(t)} \smat \cdots \smat A_{a, b_{|\cB|}}^{(t)} ].
\end{equation*}
For convenience, we write $A_{a, \cB}  = A_{a, \cB}^{(|\cB|)}$.
The construction in this subsection requires that the $ns$ distinct $\lambda_i$s satisfy the
following local constraints
\begin{equation} \label{eq:localB}
	\det(A_{a, \cB})\not=0 \text{~for~all~} a \in [n/(s+1)] \text{~and~} \cB \subseteq [s+1],
\end{equation}
i.e., all the square matrices $A_{a, \cB}$ are invertible.
The existence of such $\lambda_i$s in a field of order $O_{s}(n)$ is guaranteed by the following result.
\begin{lemma}\label{lemma:fieldsizeB}
	If a prime power $q \ge ns + \sum_{t = 1}^{s+1}\binom{s+1}{t}\frac{st(t-1)}{2}$, then we can always choose $ns$ distinct elements $\lambda_0, \dots, \lambda_{ns-1}$ from $\Fq$ satisfying \eqref{eq:localB}.
\end{lemma}
We omit the proof of Lemma~\ref{lemma:fieldsizeB} as it is similar to that of Lemma~\ref{lemma:fieldsizeA}.

Now we are ready to define the parity check matrices for the $(n,k,\ell = s^{n/(s+1)})$ array code.
For each $a\in [n/(s+1)]$ and $b\in [s+1]$,
the $r\ell\times \ell$ matrix $H_{a(s+1)+b}$ in \eqref{eq:parity_check} is defined as
\begin{equation}\label{eq:pcmB}
	H_{a(s+1)+b} = A_{a,b}^{(r)}.
\end{equation}
The MDS property and optimal repair of our MSR codes defined by \eqref{eq:parity_check} and \eqref{eq:pcmB} will follow directly from
the following global constraints.
\begin{lemma}\label{lemma:globalB}
	For any $c$ distinct integers $ a_0, a_1, \dots, a_{c-1} \in [n/(s+1)]$ and $c$ subsets $\cB_0,\cB_1,\dots, \cB_{c-1}\subseteq [s+1]$ satisfying
	$|\cB_0| + |\cB_1| + \cdots + |\cB_{c-1}| = t\le r$, we have
	\begin{equation*}
		\det\left([ A_{a_0, \cB_0}^{(t)}\smat A_{a_1, \cB_1}^{(t)}\smat \dots \smat A_{a_{c-1}, \cB_{c-1}}^{(t)} ]\right)\not=0.
	\end{equation*}
\end{lemma}
Since the proof of Lemma~\ref{lemma:globalB} is exactly the same as that of Lemma~\ref{lemma:globalA},
we omit it.

\section{A concrete example}\label{sect:example}
In this section, we use an example to show the $(n,k,\ell = s^{n/s})$ MSR code defined in Section~\ref{sect:consA}.
We take $n = 6, k = 2$ and $d = 4$.
Then $r = n-k = 4, s = d-k+1 = 3$ and $\ell =s^{n/s} = 9$.
In order to explicitly present the code construction, we write down the sequence of matrices $H_0, H_1, \dots, H_{n-1}$.
Each of them has $\ell$ block rows and $\ell$ columns.
The entry at the cross of the $i$th block row and the $j$th column of $H_{as+b}$ is $H_{as+b}(i,j)$.
Note that each entry here is a column vector of length $4$.
For the concrete example above, we have
\allowdisplaybreaks
{\scriptsize
	\begin{align*}
		 & H_0 =
		\left[\begin{array}{ccc|ccc|ccc}
				      \rL{0} & \rL[-]{1} & \rL[-]{2} & \rz    & \rz       & \rz       & \rz    & \rz       & \rz       \\
				      \rz    & \rL{1}    & \rz       & \rz    & \rz       & \rz       & \rz    & \rz       & \rz       \\
				      \rz    & \rz       & \rL{2}    & \rz    & \rz       & \rz       & \rz    & \rz       & \rz       \\
				      \hline
				      \rz    & \rz       & \rz       & \rL{0} & \rL[-]{1} & \rL[-]{2} & \rz    & \rz       & \rz       \\
				      \rz    & \rz       & \rz       & \rz    & \rL{1}    & \rz       & \rz    & \rz       & \rz       \\
				      \rz    & \rz       & \rz       & \rz    & \rz       & \rL{2}    & \rz    & \rz       & \rz       \\
				      \hline
				      \rz    & \rz       & \rz       & \rz    & \rz       & \rz       & \rL{0} & \rL[-]{1} & \rL[-]{2} \\
				      \rz    & \rz       & \rz       & \rz    & \rz       & \rz       & \rz    & \rL{1}    & \rz       \\
				      \rz    & \rz       & \rz       & \rz    & \rz       & \rz       & \rz    & \rz       & \rL{2}
			      \end{array}\right],       \\
		 & H_1 =
		\left[\begin{array}{ccc|ccc|ccc}
				      \rL{3}    & \rz    & \rz       & \rz       & \rz    & \rz       & \rz       & \rz    & \rz       \\
				      \rL[-]{3} & \rL{4} & \rL[-]{5} & \rz       & \rz    & \rz       & \rz       & \rz    & \rz       \\
				      \rz       & \rz    & \rL{5}    & \rz       & \rz    & \rz       & \rz       & \rz    & \rz       \\
				      \hline
				      \rz       & \rz    & \rz       & \rL{3}    & \rz    & \rz       & \rz       & \rz    & \rz       \\
				      \rz       & \rz    & \rz       & \rL[-]{3} & \rL{4} & \rL[-]{5} & \rz       & \rz    & \rz       \\
				      \rz       & \rz    & \rz       & \rz       & \rz    & \rL{5}    & \rz       & \rz    & \rz       \\
				      \hline
				      \rz       & \rz    & \rz       & \rz       & \rz    & \rz       & \rL{3}    & \rz    & \rz       \\
				      \rz       & \rz    & \rz       & \rz       & \rz    & \rz       & \rL[-]{3} & \rL{4} & \rL[-]{5} \\
				      \rz       & \rz    & \rz       & \rz       & \rz    & \rz       & \rz       & \rz    & \rL{5}
			      \end{array}\right],       \\
		 & H_2 =
		\left[\begin{array}{ccc|ccc|ccc}
				      \rL{6}    & \rz       & \rz    & \rz       & \rz       & \rz    & \rz       & \rz       & \rz    \\
				      \rz       & \rL{7}    & \rz    & \rz       & \rz       & \rz    & \rz       & \rz       & \rz    \\
				      \rL[-]{6} & \rL[-]{7} & \rL{8} & \rz       & \rz       & \rz    & \rz       & \rz       & \rz    \\
				      \hline
				      \rz       & \rz       & \rz    & \rL{6}    & \rz       & \rz    & \rz       & \rz       & \rz    \\
				      \rz       & \rz       & \rz    & \rz       & \rL{7}    & \rz    & \rz       & \rz       & \rz    \\
				      \rz       & \rz       & \rz    & \rL[-]{6} & \rL[-]{7} & \rL{8} & \rz       & \rz       & \rz    \\
				      \hline
				      \rz       & \rz       & \rz    & \rz       & \rz       & \rz    & \rL{6}    & \rz       & \rz    \\
				      \rz       & \rz       & \rz    & \rz       & \rz       & \rz    & \rz       & \rL{7}    & \rz    \\
				      \rz       & \rz       & \rz    & \rz       & \rz       & \rz    & \rL[-]{6} & \rL[-]{7} & \rL{8}
			      \end{array}\right],       \\
		 & H_3 =
		\left[\begin{array}{ccc|ccc|ccc}
				      \rL{9} & \rz    & \rz    & \rL[-]{10} & \rz        & \rz        & \rL[-]{11} & \rz        & \rz        \\
				      \rz    & \rL{9} & \rz    & \rz        & \rL[-]{10} & \rz        & \rz        & \rL[-]{11} & \rz        \\
				      \rz    & \rz    & \rL{9} & \rz        & \rz        & \rL[-]{10} & \rz        & \rz        & \rL[-]{11} \\
				      \hline
				      \rz    & \rz    & \rz    & \rL{10}    & \rz        & \rz        & \rz        & \rz        & \rz        \\
				      \rz    & \rz    & \rz    & \rz        & \rL{10}    & \rz        & \rz        & \rz        & \rz        \\
				      \rz    & \rz    & \rz    & \rz        & \rz        & \rL{10}    & \rz        & \rz        & \rz        \\
				      \hline
				      \rz    & \rz    & \rz    & \rz        & \rz        & \rz        & \rL{11}    & \rz        & \rz        \\
				      \rz    & \rz    & \rz    & \rz        & \rz        & \rz        & \rz        & \rL{11}    & \rz        \\
				      \rz    & \rz    & \rz    & \rz        & \rz        & \rz        & \rz        & \rz        & \rL{11}
			      \end{array}\right], \\
		 & H_4 =
		\left[\begin{array}{ccc|ccc|ccc}
				      \rL{12} & \rz     & \rz     & \rz     & \rz     & \rz     & \rz        & \rz        & \rz        \\
				      \rz     & \rL{12} & \rz     & \rz     & \rz     & \rz     & \rz        & \rz        & \rz        \\
				      \rz     & \rz     & \rL{12} & \rz     & \rz     & \rz     & \rz        & \rz        & \rz        \\
				      \hline
				      -L_{12} & \rz     & \rz     & \rL{13} & \rz     & \rz     & \rL[-]{14} & \rz        & \rz        \\
				      \rz     & -L_{12} & \rz     & \rz     & \rL{13} & \rz     & \rz        & \rL[-]{14} & \rz        \\
				      \rz     & \rz     & -L_{12} & \rz     & \rz     & \rL{13} & \rz        & \rz        & \rL[-]{14} \\
				      \hline
				      \rz     & \rz     & \rz     & \rz     & \rz     & \rz     & \rL{14}    & \rz        & \rz        \\
				      \rz     & \rz     & \rz     & \rz     & \rz     & \rz     & \rz        & \rL{14}    & \rz        \\
				      \rz     & \rz     & \rz     & \rz     & \rz     & \rz     & \rz        & \rz        & \rL{14}
			      \end{array}\right],       \\
		 & H_5 =
		\left[\begin{array}{ccc|ccc|ccc}
				      \rL{15}    & \rz        & \rz        & \rz        & \rz        & \rz        & \rz     & \rz     & \rz     \\
				      \rz        & \rL{15}    & \rz        & \rz        & \rz        & \rz        & \rz     & \rz     & \rz     \\
				      \rz        & \rz        & \rL{15}    & \rz        & \rz        & \rz        & \rz     & \rz     & \rz     \\
				      \hline
				      \rz        & \rz        & \rz        & \rL{16}    & \rz        & \rz        & \rz     & \rz     & \rz     \\
				      \rz        & \rz        & \rz        & \rz        & \rL{16}    & \rz        & \rz     & \rz     & \rz     \\
				      \rz        & \rz        & \rz        & \rz        & \rz        & \rL{16}    & \rz     & \rz     & \rz     \\
				      \hline
				      \rL[-]{15} & \rz        & \rz        & \rL[-]{16} & \rz        & \rz        & \rL{17} & \rz     & \rz     \\
				      \rz        & \rL[-]{15} & \rz        & \rz        & \rL[-]{16} & \rz        & \rz     & \rL{17} & \rz     \\
				      \rz        & \rz        & \rL[-]{15} & \rz        & \rz        & \rL[-]{16} & \rz     & \rz     & \rL{17}
			      \end{array}\right],
	\end{align*}
}where $\bzero$ is the all-zero vector of length $4$, and each $L_i=L_i^{(4)}$ is defined as in \eqref{eq:L} according to the following selected $\lambda_0, \lambda_1, \dots, \lambda_{17}$.
Let $\theta$ be a root of the primitive polynomial $x^5 + x^2 +1$ in $\ff_{32}$, and we set $\lambda_i=\theta^{i}$ for $0\leq i\leq 17$.
One can check that these $\lambda_i$'s satisfy the local constraints \eqref{eq:localA}.
More precisely, for each positive integer $t \le 3$, we choose $t$ distinct matrices from $H_0, H_1, H_2$ (or $H_3, H_4, H_5$), and we only save the first $t$ rows of every block row, i.e., all $L_i$s and the all-zero vector $0$s now are length-$t$ column vectors obtained from cutting the last $r-t$ rows, then the $9t\times 9t$ submatrix of $H_0, H_1, H_2$ (or $H_3, H_4, H_5$) is invertible.

\subsection{MDS property}\label{sect:exampleMDS}
In this subsection, we show the MDS property of the above concrete $(n = 6, k = 2, \ell = 9)$ MSR code over $\ff_{32}$.
We need to prove that any $4$ nodes of this code can be recovered from the remaining $2$ nodes.
To prove this, according to the definition \eqref{eq:parity_check}, we only need to show that if any $2$ nodes both are all-zero vectors of length $9$, then the remaining $4$ nodes must be all-zero vectors.
This is equal to prove that the $36\times 36$ square matrix obtained from combining any $4$ out of the $6$ matrices $H_0, H_1, \dots, H_5$ is invertible.

To give an illustration of our proof technique, we now show how to recover $C_0, C_1, C_3, C_4$ from the remaining $2$ nodes $C_2, C_5$.
To that end, we only need to show that if all the coordinates of $C_2, C_5$ are $0$, then $C_0=C_1=C_3=C_4=\bz{9}$ is the unique solution to the parity check equation \eqref{eq:parity_check}.
Recall that $H_0 = A_{0,0}^{(4)}, H_1 = A_{0,1}^{(4)}, H_3 = A_{1, 0}^{(4)}$ and $H_4 = A_{1,1}^{(4)}$.
Then this is equivalent to proving that the square matrix
$[A_{0,0}^{(4)}\smat A_{0,1}^{(4)}\smat A_{1, 0}^{(4)} \smat A_{1,1}^{(4)}]$ is invertible.
In the following, we show that there exists a matrix $M^{(4)}_{0, \{0,1\}}$ such that
\begin{equation*}
	M^{(4)}_{0, \{0,1\}}\cdot [A_{0,0}^{(4)} \smat A_{0,1}^{(4)} \smat  A_{1,0}^{(4)} \smat  A_{1,1}^{(4)}] = \left[\begin{array}{*{1}{@{\hspace*{0.00in}}cc|cc}}
			A_{0,0}^{(2)} & A_{0,1}^{(2)} & A_{1,0}^{(2)}        & A_{1,1}^{(2)}        \\
			\bO           & \bO           & \tilde A_{1,0}^{(2)} & \tilde A_{1,1}^{(2)}
		\end{array} \hspace*{-0.05in}\right],
\end{equation*}
in which the square matrix $[\tilde A_{1,0}^{(2)} \smat \tilde A_{1,1}^{(2)}]$ is column equivalent to the matrix $[A_{1,0}^{(2)} \smat A_{1,1}^{(2)}]$.
Now the result follows directly from the
local constraints.

Since the 18 $\lambda_i$s satisfy local constraints~\eqref{eq:localA}, we know that the square matrices
$A_{0, \{0,1\}} = [A_{0,0}^{(2)}\smat A_{0,1}^{(2)}]$ and $A_{1, \{0,1\}} = [A_{1,0}^{(2)}\smat A_{1,1}^{(2)}]$
are invertible.
It is not hard to check that the square matrix
$$[A_{0,0}^{(2)}\smat A_{0,1}^{(2)}]=[\bI_3 \otimes K_{0,0}^{(2)}\smat \bI_3 \otimes K_{0,1}^{(2)} ]$$
is column equivalent to $\bI_3 \otimes [ K_{0,0}^{(2)}\smat K_{0,1}^{(2)}]$.
This implies that the square matrix
\begin{align*}
	[ K_{0,0}^{(2)}\smat K_{0,1}^{(2)}] =
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}cccccc}}
			      \wn L_0^{(2)} & -L_1^{(2)}    & -L_2^{(2)}    & \wn L_3^{(2)} & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wn L_1^{(2)} & \wngbz{2}     & -L_3^{(2)}    & \wn L_4^{(2)} & -L_5^{(2)}    \\
			      \wngbz{2}     & \wngbz{2}     & \wn L_2^{(2)} & \wngbz{2}     & \wngbz{2}     & \wn L_5^{(2)}
		      \end{array} \hspace*{-0.05in}\right]
\end{align*}
is invertible, and hence its row space is $\ff_{32}^{6}$.
Therefore, we can find a set of coefficients $a_0, a_1, \dots, a_5$, $b_0, b_1, \dots, b_5,$ $ c_0, c_1, \dots, c_5$ in $\ff_{32}$ such that
\begin{align}\label{eq:allzero}
	E^*\cdot
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}cccccc}}
			      \wn L_0^{(4)} & -L_1^{(4)}    & -L_2^{(4)}    & \wn L_3^{(4)} & \wngbz{4}     & \wngbz{4}     \\
			      \wngbz{4}     & \wn L_1^{(4)} & \wngbz{4}     & -L_3^{(4)}    & \wn L_4^{(4)} & -L_5^{(4)}    \\
			      \wngbz{4}     & \wngbz{4}     & \wn L_2^{(4)} & \wngbz{4}     & \wngbz{4}     & \wn L_5^{(4)}
		      \end{array} \hspace*{-0.05in}\right]
	= \bO,
\end{align}
where
\begin{align*}
	E^* = \left[\begin{array}{*{1}{@{\hspace*{0.00in}}cccccccccccc}}
			            a_0 & a_1 & 1      & \gzero & a_2 & a_3 & \gzero & \gzero & a_4 & a_5 & \gzero & \gzero \\
			            b_0 & b_1 & \gzero & \gzero & b_2 & b_3 & 1      & \gzero & b_4 & b_5 & \gzero & \gzero \\
			            c_0 & c_1 & \gzero & \gzero & c_2 & c_3 & \gzero & \gzero & c_4 & c_5 & 1      & 0
		            \end{array} \hspace*{-0.05in}\right]
\end{align*}
and $\bO$ is the $3\times 6$ all-zero matrix.
We define a $3\times3$ square matrix
\begin{align*}%\label{eq:f_lambda}
	F(\lambda_i) = E^*\cdot
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      L_i^{(4)} & \gbz{4}   & \gbz{4}   \\
			      \gbz{4}   & L_i^{(4)} & \gbz{4}   \\
			      \gbz{4}   & \gbz{4}   & L_i^{(4)}
		      \end{array} \hspace*{-0.05in}\right]
	=
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}lll}}
			      a_0 + a_1\lambda_i + \lambda_i^2 & a_2 + a_3\lambda_i              & a_4 + a_5\lambda_i               \\
			      b_0 + b_1\lambda_i               & b_2 + b_3\lambda_i+ \lambda_i^2 & b_4 + b_5\lambda_i               \\
			      c_0 + c_1\lambda_i               & c_2 + c_3\lambda_i              & c_4 + c_5\lambda_i + \lambda_i^2 \\
		      \end{array} \hspace*{-0.05in}\right]
\end{align*}
and its corresponding determinant $f(\lambda_i) = \det(F(\lambda_i))$.
Now we claim that if $i\notin\{0,1,2,3,4,5\}$, then $f(\lambda_i)\neq0$, i.e., the $3\times3$ square matrix $F(\lambda_i)$ is invertible.

To see this, we observe that $f(\lambda_i)$ is a nonzero polynomial with degree $6$ of variable $\lambda_i$ over $\ff_{32}$.
Then we know that $f(\lambda_i)$ has no more than $6$ roots in $\ff_{32}$.
In order to prove $f(\lambda_i)\not=0$ when $i\notin\{0,1,\dots,5\}$,
we only need to show all the $6$ roots of $f$ are exactly $\lambda_0, \lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5$.
From \eqref{eq:allzero} we can get the following results.
\begin{itemize}
	\item In $F(\lambda_0)$, the $1$st column is
	      \begin{align*}
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     L_0^{(4)} \\
				                     \gbz{4}   \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right] = \gbz{3}.
	      \end{align*}

	\item In $F(\lambda_1)$, subtract the $1$st column from the $2$nd column, we get
	      \begin{align*}
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \gbz{4}   \\
				                     L_1^{(4)} \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right]
		      -
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     L_1^{(4)} \\
				                     \gbz{4}   \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right]
		      =
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     -L_1^{(4)}    \\
				                     \wn L_1^{(4)} \\
				                     \wngbz{4}
			                     \end{array} \hspace*{-0.05in}\right] = \gbz{3}.
	      \end{align*}

	\item  In $F(\lambda_2)$, subtract the $1$st column from the $3$rd column, we get
	      \begin{align*}
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \gbz{4} \\
				                     \gbz{4} \\
				                     L_2^{(4)}
			                     \end{array} \hspace*{-0.05in}\right]
		      -
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     L_2^{(4)} \\
				                     \gbz{4}   \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right]
		      =
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     -L_2^{(4)} \\
				                     \wngbz{4}  \\
				                     \wn L_2^{(4)}
			                     \end{array} \hspace*{-0.05in}\right] = \gbz{3}.
	      \end{align*}
	\item  In $F(\lambda_3)$, subtract the $2$nd column from the $1$st column, we get
	      \begin{align*}
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     L_3^{(4)} \\
				                     \gbz{4}   \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right]
		      -
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \gbz{4}   \\
				                     L_3^{(4)} \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right]
		      =
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \wn L_3^{(4)} \\
				                     -L_3^{(4)}    \\
				                     \wngbz{4}
			                     \end{array} \hspace*{-0.05in}\right] = \gbz{3}.
	      \end{align*}
	\item In $F(\lambda_4)$, the $2$nd column is
	      \begin{align*}
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \gbz{4}   \\
				                     L_4^{(4)} \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right] = \gbz{3},
	      \end{align*}
	\item In $F(\lambda_5)$, subtract the $2$nd column from the $3$rd column, we get
	      \begin{align*}
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \gbz{4} \\
				                     \gbz{4} \\
				                     L_5^{(4)}
			                     \end{array} \hspace*{-0.05in}\right]
		      -
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \gbz{4}   \\
				                     L_5^{(4)} \\
				                     \gbz{4}
			                     \end{array} \hspace*{-0.05in}\right]
		      =
		      E^*\cdot \left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
				                     \wngbz{4}  \\
				                     -L_5^{(4)} \\
				                     \wn L_5^{(4)}
			                     \end{array} \hspace*{-0.05in}\right] = \gbz{3}.
	      \end{align*}
\end{itemize}
The above itemized arguments show that all the matrices $F(\lambda_i), 0\leq i\leq 5$ do not have full column rank,
thus the $6$ roots of $f(\lambda_i)$ are exactly $\lambda_i, 0\leq i\leq 5$.

Let
\begin{align*}
	E =
	\left[
		\begin{array}{*{1}{@{\hspace*{0.00in}}cccc|cccc|cccc}}
			1      & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero \\
			\gzero & 1      & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero \\
			a_0    & a_1    & 1      & \gzero & a_2    & a_3    & \gzero & \gzero & a_4    & a_5    & \gzero & \gzero \\
			\gzero & a_0    & a_1    & 1      & \gzero & a_2    & a_3    & \gzero & \gzero & a_4    & a_5    & \gzero \\
			\hline
			\gzero & \gzero & \gzero & \gzero & 1      & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero \\
			\gzero & \gzero & \gzero & \gzero & \gzero & 1      & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero \\
			b_0    & b_1    & \gzero & \gzero & b_2    & b_3    & 1      & \gzero & b_4    & b_5    & \gzero & \gzero \\
			\gzero & b_0    & b_1    & \gzero & \gzero & b_2    & b_3    & 1      & \gzero & b_4    & b_5    & \gzero \\
			\hline
			\gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & 1      & \gzero & \gzero & \gzero \\
			\gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & \gzero & 1      & \gzero & \gzero \\
			c_0    & c_1    & \gzero & \gzero & c_2    & c_3    & \gzero & \gzero & c_4    & c_5    & 1      & \gzero \\
			\gzero & c_0    & c_1    & \gzero & \gzero & c_2    & c_3    & \gzero & \gzero & c_4    & c_5    & 1
		\end{array} \hspace*{-0.05in}\right].
\end{align*}
Then we have
\begin{align*}
	E
	\cdot
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_0^{(4)} & -L_1^{(4)}    & -L_2^{(4)}    \\
			      \wngbz{4}     & \wn L_1^{(4)} & \wngbz{4}     \\
			      \wngbz{4}     & \wngbz{4}     & \wn L_2^{(4)}
		      \end{array} \hspace*{-0.05in}\right]
	=
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_0^{(2)} & -L_1^{(2)}    & -L_2^{(2)}    \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wn L_1^{(2)} & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wn L_2^{(2)} \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}
		      \end{array} \hspace*{-0.05in}\right],
\end{align*}
and
\begin{align*}
	E
	\cdot
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_3^{(4)} & \wngbz{4}     & \wngbz{4}     \\
			      -L_3^{(4)}    & \wn L_4^{(4)} & -L_5^{(4)}    \\
			      \wngbz{4}     & \wngbz{4}     & \wn L_5^{(4)}
		      \end{array} \hspace*{-0.05in}\right]
	=
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_3^{(2)} & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      -L_3^{(2)}    & \wn L_4^{(2)} & -L_5^{(2)}    \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wn L_5^{(2)} \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}
		      \end{array} \hspace*{-0.05in}\right],
\end{align*}
where all $\bzero$s on the left-hand side are the all-zero column vectors of length $4$ while all $\bzero$s on the right-hand side are the all-zero column vectors of length $2$.
Let
\begin{equation*}
	E^{(4)}_{0, \{0,1\}} = \left[\begin{array}{l}
			\bI_3\otimes \left[\begin{array}{cc} \bI_2 & \bO\end{array}\hspace*{.007in}\right] \\[1ex]
			\bI_3\otimes \left[\begin{array}{cc} \bO   & \bI_{2}\end{array}\right]
		\end{array}\right]\cdot E,
\end{equation*}
in which the permutation matrix on the left-hand side  is used to move the last $2$ rows of each block row to the bottom.
We compute that
\begin{align*}
	E^{(4)}_{0, \{0,1\}}
	\cdot
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_0^{(4)} & -L_1^{(4)}    & -L_2^{(4)}    \\
			      \wngbz{4}     & \wn L_1^{(4)} & \wngbz{4}     \\
			      \wngbz{4}     & \wngbz{4}     & \wn L_2^{(4)}
		      \end{array} \hspace*{-0.05in}\right]
	=
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_0^{(2)} & -L_1^{(2)}    & -L_2^{(2)}    \\
			      \wngbz{2}     & \wn L_1^{(2)} & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wn L_2^{(2)} \\
			      \hline
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}
		      \end{array} \hspace*{-0.05in}\right],
\end{align*}
and
\begin{align*}
	E^{(4)}_{0, \{0,1\}}
	\cdot
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_3^{(4)} & \wngbz{4}     & \wngbz{4}     \\
			      -L_3^{(4)}    & \wn L_4^{(4)} & -L_5^{(4)}    \\
			      \wngbz{4}     & \wngbz{4}     & \wn L_5^{(4)}
		      \end{array} \hspace*{-0.05in}\right]
	=
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_3^{(2)} & \wngbz{2}     & \wngbz{2}     \\
			      -L_3^{(2)}    & \wn L_4^{(2)} & -L_5^{(2)}    \\
			      \wngbz{2}     & \wngbz{2}     & \wn L_5^{(2)} \\
			      \hline
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}     \\
			      \wngbz{2}     & \wngbz{2}     & \wngbz{2}
		      \end{array} \hspace*{-0.05in}\right].
\end{align*}
In addition, for $\lambda_i\notin\{\lambda_0, \lambda_1, \dots, \lambda_5\}$, we have
\begin{align*}
	E^{(4)}_{0, \{0,1\}}
	\cdot
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      \wn L_i^{(4)} & \wngbz{4}     & \wngbz{4}     \\
			      \wngbz{4}     & \wn L_i^{(4)} & \wngbz{4}     \\
			      \wngbz{4}     & \wngbz{4}     & \wn L_i^{(4)}
		      \end{array} \hspace*{-0.05in}\right]
	=
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
			      \bI_3\otimes L_i^{(2)} \\
			      \hline
			      F(\lambda_i)\otimes L_i^{(2)},
		      \end{array} \hspace*{-0.05in}\right],
\end{align*}
where $F(\lambda_i)$ is a $3\times 3$ invertible matrix.

Finally, let
\begin{equation*}
	M^{(4)}_{0, \{0,1\}} = \left[\begin{array}{c}
			\bI_{3}\otimes \left[\begin{array}{cc} \bI_{6} &\bO\end{array} \right] \\[1ex]
			\bI_{3}\otimes \left[\begin{array}{cc} \bO     &\bI_{6}\end{array}\right]
		\end{array} \right]\cdot  (\bI_3\otimes E^{(4)}_{0, \{0,1\}}),
\end{equation*}
and we get
\begin{equation*}
	M^{(4)}_{0, \{0,1\}}\cdot [A_{0,0}^{(4)} \smat A_{0,1}^{(4)} \smat  A_{1,0}^{(4)} \smat  A_{1,1}^{(4)}] = \left[\begin{array}{*{1}{@{\hspace*{0.00in}}cc|cc}}
			A_{0,0}^{(2)} & A_{0,1}^{(2)} & A_{1,0}^{(2)}        & A_{1,1}^{(2)}        \\
			\bO           & \bO           & \tilde A_{1,0}^{(2)} & \tilde A_{1,1}^{(2)}
		\end{array} \hspace*{-0.05in}\right],
\end{equation*}
where
	{\scriptsize
		\begin{align*}
			[\tilde A_{1,0}^{(2)} \smat \tilde A_{1,1}^{(2)}] =
			\left[\begin{array}{*{1}{@{\hspace*{0.00in}}lll|lll}}
					      F(\lambda_9)\otimes L_9^{(2)} & -F(\lambda_{10})\otimes L_{10}^{(2)}    & -F(\lambda_{11})\otimes L_{11}^{(2)}    & \wn F(\lambda_{12})\otimes L_{12}^{(2)} & \hspace*{0.38in}\bO                 & \hspace*{0.475in}\bO                    \\
					      \hspace*{0.3275in}\bO         & \wn F(\lambda_{10})\otimes L_{10}^{(2)} & \hspace*{0.475in}\bO                    & -F(\lambda_{12})\otimes L_{12}^{(2)}    & F(\lambda_{13})\otimes L_{13}^{(2)} & -F(\lambda_{14})\otimes L_{14}^{(2)}    \\
					      \hspace*{0.3275in}\bO         & \hspace*{0.475in}\bO                    & \wn F(\lambda_{11})\otimes L_{11}^{(2)} & \hspace*{0.475in}\bO                    & \hspace*{0.38in}\bO                 & \wn F(\lambda_{14})\otimes L_{14}^{(2)} \\
				      \end{array} \hspace*{-0.05in}\right].
		\end{align*}
	}
Let
\begin{equation*}
	\bQ = \left[\begin{array}{cccccc}
			F(\lambda_9)^{-1} &                      &                      &                      &                      &                      \\
			                  & F(\lambda_{10})^{-1} &                      &                      &                      &                      \\
			                  &                      & F(\lambda_{11})^{-1} &                      &                      &                      \\
			                  &                      &                      & F(\lambda_{12})^{-1} &                      &                      \\
			                  &                      &                      &                      & F(\lambda_{13})^{-1} &                      \\
			                  &                      &                      &                      &                      & F(\lambda_{14})^{-1} \\
		\end{array}\right].
\end{equation*}
According to the mixed-product property of the Kronecker product\footnote{
	If $A$, $B$, $C$ and $D$ are matrices such that $AC$ and $BD$ are valid matrix products, then $(A\otimes B)(C\otimes D) = (AC)\otimes(BD)$.
}, we have
$$
	(F(\lambda_i)\otimes L_i^{(t)})\cdot F(\lambda_i)^{-1} = (F(\lambda_i)\cdot F(\lambda_i)^{-1})\otimes L_i^{(t)} = \bI_s\otimes L_i^{(t)}.
$$
Hence  $[\tilde A_{1,0}^{(2)} \smat \tilde A_{1,1}^{(2)}]\cdot \bQ = [A_{1,0}^{(2)} \smat A_{1,1}^{(2)}]$,
that is,
the square matrix $[\tilde A_{1,0}^{(2)} \smat \tilde A_{1,1}^{(2)}]$ is column equivalent to the matrix $[A_{1,0}^{(2)} \smat A_{1,1}^{(2)}]$.
Recall that the two square matrices $A_{0,\{0,1\}} = [A_{0,0}^{(2)} \smat A_{0,1}^{(2)}]$ and $A_{1,\{0,1\}} = [A_{1,0}^{(2)} \smat A_{1,1}^{(2)}]$   are invertible,
so $\det([\tilde A_{1,0}^{(2)} \smat \tilde A_{1,1}^{(2)}])\not=0$.
Furthermore, we have
$$\det(M^{(4)}_{0,\{0,1\}}\cdot[A_{0,0}^{(4)} \smat A_{0,1}^{(4)} \smat  A_{1,0}^{(4)} \smat  A_{1,1}^{(4)}]) = \pm\det([A_{0,0}^{(2)} \smat A_{0,1}^{(2)}])\cdot \det([\tilde A_{1,0}^{(2)} \smat \tilde A_{1,1}^{(2)}])\not=0.$$
Finally, we get $\det([H_0, H_1, H_3, H_4])\neq0$, i.e., the square matrix $[H_0, H_1, H_3, H_4]$ is invertible.
This completes the proof of MDS property.

\subsection{Optimal-access repair for single node failure}
In this subsection, we show how to repair single node failure with the optimal repair bandwidth for the above example.
We will only present here how to repair $C_0$ from any other $4$ nodes.
In the parity check equation \eqref{eq:parity_check}, each block row in the matrices $H_0, H_1, \dots, H_5$ corresponds to a set of $r = n-k = 4$ parity check equations because the length of $L_i$ is $4$.
Since there are $9$ block rows in each matrix $A_i$, we have $9$ sets of parity check equations in total.
The repair of $C_0$ only involves $3$ out of these $9$ sets of parity check equations.
More precisely, among the $9$ block rows in each matrix $A_i$, we only need to look at the block rows whose indices lie in $\{0,3,6\}$.
These $3$ block rows of parity check equations can be reorganized in the following matrix from
\begin{equation}\label{eq:repairC0}
	\tilde{H}_0\tilde{C}_0 + \widehat{H}_0\widehat{C}_0 + \bar{H}_0\bar{C}_0 +  \sum_{i = 1}^{5}\bar{H}_i\bar{C}_i = 0,
\end{equation}
where $\tilde{H}_0,\widehat{H}_0,\bar{H}_i, 0\le i \le 5$ are all $3\times 3$ block matrices,
and $\tilde{C}_0,\widehat{C}_0,\bar{C}_i, 0\le i \le 5$ are all column vectors of length $3$.
To be specific, the matrices in \eqref{eq:repairC0} are
	{
		\begin{align*}
			\tilde{H}_0 & =
			\left[\begin{array}{ccc}
					      \tL{0}{4} & \tz{4}    & \tz{4}    \\
					      \tz{4}    & \tL{0}{4} & \tz{4}    \\
					      \tz{4}    & \tz{4}    & \tL{0}{4}
				      \end{array} \hspace*{-0.05in}\right],
			\widehat{H}_0 =
			\left[\begin{array}{ccc}
					      \tL[-]{1}{4} & \tz{4}       & \tz{4}       \\
					      \tz{4}       & \tL[-]{1}{4} & \tz{4}       \\
					      \tz{4}       & \tz{4}       & \tL[-]{1}{4}
				      \end{array} \hspace*{-0.05in}\right],
			\bar{H}_0  =
			\left[\begin{array}{ccc}
					      \tL[-]{2}{4} & \tz{4}       & \tz{4}       \\
					      \tz{4}       & \tL[-]{2}{4} & \tz{4}       \\
					      \tz{4}       & \tz{4}       & \tL[-]{2}{4}
				      \end{array} \hspace*{-0.05in}\right], \\
			\bar{H}_1   & =
			\left[\begin{array}{ccc}
					      \tL{3}{4} & \tz{4}    & \tz{4}    \\
					      \tz{4}    & \tL{3}{4} & \tz{4}    \\
					      \tz{4}    & \tz{4}    & \tL{3}{4}
				      \end{array} \hspace*{-0.05in}\right],
			\bar{H}_2    =
			\left[\begin{array}{ccc}
					      \tL{6}{4} & \tz{4}    & \tz{4}    \\
					      \tz{4}    & \tL{6}{4} & \tz{4}    \\
					      \tz{4}    & \tz{4}    & \tL{6}{4}
				      \end{array} \hspace*{-0.05in}\right],
			\bar{H}_3 =
			\left[\begin{array}{ccc}
					      \tL{9}{4} & \tL[-]{10}{4} & \tL[-]{11}{4} \\
					      \tz{4}    & \tL{10}{4}    & \tz{4}        \\
					      \tz{4}    & \tz{4}        & \tL{11}{4}
				      \end{array} \hspace*{-0.05in}\right],  \\
			\bar{H}_4   & =
			\left[\begin{array}{ccc}
					      \tL{12}{4}    & \tz{4}     & \tz{4}        \\
					      \tL[-]{12}{4} & \tL{13}{4} & \tL[-]{14}{4} \\
					      \tz{4}        & \tz{4}     & \tL{14}{4}
				      \end{array} \hspace*{-0.05in}\right],
			\bar{H}_5 =
			\left[\begin{array}{ccc}
					      \tL{15}{4}    & \tz{4}     & \tz{4}     \\
					      \tz{4}        & \tL{16}{4} & \tz{4}     \\
					      \tL[-]{15}{4} & \tL{16}{4} & \tL{17}{4}
				      \end{array} \hspace*{-0.05in}\right],
		\end{align*}
	}
and the column vectors in \eqref{eq:repairC0} are
\begin{align*}
	\tilde{C}_0 =
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
			      C_0(0) \\
			      C_0(3) \\
			      C_0(6)
		      \end{array} \hspace*{-0.05in}\right],
	\widehat{C}_0 =
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      C_0(1) \\
			      C_0(4) \\
			      C_0(7)
		      \end{array} \hspace*{-0.05in}\right],
	\bar{C}_0 =
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      C_0(2) \\
			      C_0(5) \\
			      C_0(8)
		      \end{array} \hspace*{-0.05in}\right],
	\bar{C}_i =
	\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
			      C_i(0) \\
			      C_i(3) \\
			      C_i(6)
		      \end{array} \hspace*{-0.05in}\right],
	\text{~for~} 1 \le i \le 5.
\end{align*}
We observe that the matrices $\bar{H}_3, \bar{H}_4,\bar{H}_5$ are precisely the $3$ parity check matrices that would appear in our $(n,k,\ell = s^{n/s})$ MSR code construction with code length $n = 3$
and sub-packetization $\ell = 3^{3/3} = 3$.
The other $5$ matrices $\tilde{H}_0, \widehat{H}_0, \bar{H}_0, \bar{H}_1, \bar{H}_2$ are all block-diagonal matrices, and the diagonal entries are the same within each matrix.
It is easy to see that the method we used to prove the MDS property of our MSR code construction in Section~\ref{sect:exampleMDS} can be easily generalized to show that \eqref{eq:repairC0} also defines an MDS array code
$(\tilde{C}_0, \widehat{C}_0, \bar{C}_0, \bar{C}_1, \dots, \bar{C}_5)$ with code length $8$ and code dimension $4$.
Therefore, $\tilde{C}_0, \widehat{C}_0, \bar{C}_0$ can be recovered from any $4$ vectors in the set $\{\bar{C}_1, \bar{C}_2, \dots, \bar{C}_5\}$.
Then we can recover all the coordinates of $C_0$ by combining $\tilde{C}_0, \widehat{C}_0$ and $\bar{C}_0$.
It is also easy to see that this repair scheme is optimal access.

\section{MDS property: reduction from global constraints to local constraints}\label{sect:mds}
Recall that an $(n,k,\ell)$ array code is an MDS array code if it allows us to recover any $r=n-k$ node erasures from
the remaining $k$ nodes.
We write the index set of failed nodes as $\cF = \{i_1, i_2, \dots, i_r\}$.
Then the parity check equation~\eqref{eq:parity_check} can be rewritten as
\begin{equation*}
	\sum_{i\in\cF}H_iC_i = -\sum_{i\notin\cF}H_iC_i.
\end{equation*}
In order to recover the failed nodes $\{C_i, i\in\cF\}$, we require the above equation has a unique solution.
Thus the MDS property of our new codes constructed in Section~\ref{sect:cons} follows directly from the following arguments:
\begin{equation}\label{eq:mds}
	[H_{i_1} \smat  H_{i_2} \smat \cdots \smat H_{i_r}] \text{~is invertible for~all~} \cF = \{i_1, i_2, \dots, i_r\}\subseteq[n].
\end{equation}
Note that if we set  $t = r$ in Lemma~\ref{lemma:globalA} and Lemma~\ref{lemma:globalB}, then we obtain~\eqref{eq:mds} directly.
In the following, we only prove Lemma~\ref{lemma:globalA} and omit the proof for Lemma~\ref{lemma:globalB} since they are almost the same.

Similar to the definition of $A_{a,\cB}^{(t)}$,
for $a\in [n/s]$, a positive integer $t$ and subset $\cB = \{b_1, b_2, \dots, b_{|\cB|}\} \subseteq [s]$, we define
$$K_{a, \cB}^{(t)} = [K_{a, b_1}^{(t)} \smat K_{a,b_2}^{(t)} \smat \cdots \smat K_{a, b_{v}}^{(t)}],$$
where $v = |\cB|$.
Similar to $A_{a,\cB}^{(t)}$ defined in \eqref{eq:localmatsA}, we may omit the superscript $t$ if $t = v$.
From
\begin{equation*}
	\begin{aligned}
		A_{a,\cB} & = [A_{a, b_1}^{(v)}                           &  & \smat A_{a, b_2}^{(v)}                           &  & \smat \cdots &  & \smat A_{a, b_v}^{(v)}                           &  & ] \\
		          & = [\bI_{\ell/s^{a+1}}\otimes K_{a, b_1}^{(v)} &  & \smat \bI_{\ell/s^{a+1}}\otimes K_{a, b_2}^{(v)} &  & \smat \cdots &  & \smat \bI_{\ell/s^{a+1}}\otimes K_{a, b_v}^{(v)} &  & ]
	\end{aligned}
\end{equation*}
we see that $A_{a,\cB}$ is column equivalent to $\bI_{\ell/s^{a+1}}\otimes K_{a, \cB}$.
According to local constraints~\eqref{eq:localA}, we know that $A_{a,\cB}$ is invertible.
Therefore $K_{a, \cB}$ is also invertible.

The following key technical lemma will be used to make the kernel matrices involving in  global constraints~\eqref{lemma:globalA} block upper triangular.

\begin{lemma}\label{lemma:K0}
	For a subset $\cB\subseteq[s]$ with size $|\cB| = v$ and an integer $t > v$, there exists an $st\times st$ matrix $E^{(t)}_{0, \cB}$ such that
	\begin{itemize}
		\item [(i)]
		      \begin{align}\label{eq:cancel_K}
			      E^{(t)}_{0, \cB}\cdot K_{0, \cB}^{(t)} =
			      \left[\begin{array}{c}
					            K_{0,\cB}^{(v)} \\
					            \bO
				            \end{array} \hspace*{-0.05in}\right],
		      \end{align}
		      where $\bO$ is an $s(t-v) \times sv$ all-zero matrix,
		\item [(ii)] and for those $L_i^{(t)}$ not appearing in $K_{0, \cB}^{(t)}$, i.e., $i\notin\{bs + j : b\in \cB, j\in [s]\}$,
		      \begin{align}\label{eq:cancel_diag_L}
			      E^{(t)}_{0, \cB}\cdot (\bI_{s}\otimes L_i^{(t)}) =
			      \left[\begin{array}{c}
					            \bI_{s}\otimes L_i^{(v)} \\
					            F(\lambda_i)\otimes L_i^{(t-v)}
				            \end{array} \hspace*{-0.05in}\right],
		      \end{align}
		      where $F(\lambda_i)$ is an $s\times s$ invertible matrix.
	\end{itemize}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item[(i)]
			We can regard $K_{0,b}^{(t)}$ as an $s\times s$ block matrix whose block entries are $t\times 1$ column vectors.
			Since $K_{0,\cB}^{(v)}$ is invertible and $K_{0,\cB}^{(v)}$ is a submatrix of $K_{0,\cB}^{(t)}$,
			every row of $K_{0,\cB}^{(t)}$ can be written as a linear combination of the rows of $K_{0,\cB}^{(v)}$.
			In particular, the $(it+v)$th row of $K_{0,\cB}^{(t)}$, $i\in[s]$,  can be written as a linear combination of the rows of $K_{0,\cB}^{(t)}$
			whose row indices are in the set $\{it+j:i\in[s],j\in[v]\}$.
			In other words, for each $i\in [s]$, there exists a row vector
			\begin{align*}
				\be_i & = (\be_{i,0}, \be_{i,1}, \dots, \be_{i,s-1})                                                                 \\
				      & =(e_{i,0}(0),\dots, e_{i,0}(t-1), e_{i,1}(0), \dots, e_{i,1}(t-1),\dots, e_{i,s-1}(0),\dots, e_{i,s-1}(t-1))
			\end{align*}
			with $e_{i,i}(v) = 1$, $e_{i,i}(c) = 0$ if $c>v$, and $e_{i,j}(c) = 0$ if $i\not=j, c\notin [v]$, such that
			$$
				(\be_{i,0}, \be_{i,1}, \dots, \be_{i,s-1}) \cdot K_{0, \cB}^{(t)} = \bzero,
			$$
			where $\bzero$ on the right-hand side is the all-zero row vector of length $vs$.
			We collect those vectors $\be_i$ to define an $s\times st$ matrix
			\begin{align*}
				E^* =
				\left[\begin{array}{*{1}{@{\hspace*{0.00in}}c}}
						      \be_{0} \\
						      \vdots  \\
						      \be_{s-1}
					      \end{array} \hspace*{-0.05in}\right]
				=
				\left[\begin{array}{*{1}{@{\hspace*{0.00in}}ccc}}
						      \be_{0, 0}   & \cdots & \be_{0,s-1}   \\
						      \vdots       & \ddots & \vdots        \\
						      \be_{s-1, 0} & \cdots & \be_{s-1,s-1}
					      \end{array} \hspace*{-0.05in}\right].
			\end{align*}
			Obviously, we have
			\begin{align}\label{eq:EKO}
				E^* \cdot K_{0,\cB}^{(t)} = \bO.
			\end{align}
			Furthermore, we denote the row vector
			obtained by loop shifting $\be_{i,j}$ to the right $c$ bits as $\be_{i,j}^{(c)}$.
			We can readily check that
			$$
				(\be_{i,0}^{(c)},\be_{i,1}^{(c)}, \dots, \be_{i,s-1}^{(c)}) \cdot K_{0, \cB}^{(t)} = \bzero,
			$$
			which tells us how to write $(it+v+c)$th row of $K_{0,\cB}^{(t)}$ as the linear combination of the rows
			in $K_{0,\cB}^{(t)}$ whose indices in the set $\{it+j+c: i\in [s], j\in [v]\}$.

			Now we define the matrix $E$ by replacing the $(it+v+c)$-row, $i\in[s],c\in[t-v] $ of the identity matrix $\bI_{st}$ with the vector $\be_i^{(c)}$,
			and let
			\begin{equation*}
				E^{(t)}_{0, \cB} = \left[\begin{array}{l}
						\bI_s\otimes \left[\begin{array}{cc} \bI_v & \bO\end{array}\hspace*{.135in}\right] \\[1ex]
						\bI_s\otimes \left[\begin{array}{cc} \bO   & \bI_{t-v}\end{array}\right]
					\end{array}\right]\cdot E.
			\end{equation*}
			From the above discussion, it is not hard to check that
			\begin{align*}
				E^{(t)}_{0, \cB}\cdot K_{0, \cB}^{(t)} =
				\left[\begin{array}{l}
						      \bI_s\otimes \left[\begin{array}{cc} \bI_v & \bO\end{array}\hspace*{.135in}\right] \\[1ex]
						      \bI_s\otimes \left[\begin{array}{cc} \bO   & \bI_{t-v}\end{array}\right]
					      \end{array}\right]\cdot E \cdot K_{0, \cB}^{(t)}
				=
				\left[\begin{array}{c}
						      K_{0,\cB}^{(v)} \\
						      \bO
					      \end{array} \hspace*{-0.05in}\right].
			\end{align*}
			Note that here we first use the matrix $E$ to eliminate the $(it+v+c)$th row ($i\in[s],c\in[t-v]$) of $K_{0,\cB}^{(t)}$,
			and then we use the permutation matrix
			\begin{align*}
				\left[\begin{array}{l}
						      \bI_s\otimes \left[\begin{array}{cc} \bI_v & \bO\end{array}\hspace*{.135in}\right] \\[1ex]
						      \bI_s\otimes \left[\begin{array}{cc} \bO   & \bI_{t-v}\end{array}\right]
					      \end{array}\right]
			\end{align*}
			to move the last $t-v$ rows of each block row, in this case they are all-zero rows, to the bottom.
		\item[(ii)]
			We first define the $s\times s$ matrix
			$$F(\lambda_i)= E^*\cdot(\bI_{s}\otimes L_i^{(t)}) =(f_{g,h}(\lambda_i):g,h\in[s]). $$
			For any $m$, we have $\be_{i,j}^{(c)}\cdot L_{m}^{(t)} = \lambda_m^{c}\be_{i,j}\cdot L_{m}^{(t)}$.
			This implies that
			\begin{align*}
				(\be_{i,0}^{(c)}, \be_{i,1}^{(c)}, \dots, \be_{i,s-1}^{(c)})\cdot(\bI_s\otimes L_m^{(t)}) =\lambda_i^{c} \cdot (\be_{i,0}, \be_{i,1}, \dots, \be_{i,s-1})\cdot (\bI_s\otimes L_m^{(t)}).
			\end{align*}
			Similarly, as in~(i), it is not hard to check that
			\begin{align*}
				E^{(t)}_{0, \cB}\cdot  (\bI_{s}\otimes L_i^{(t)}) & =
				\left[\begin{array}{l}
						      \bI_s\otimes \left[\begin{array}{cc} \bI_v & \bO\end{array}\hspace*{.135in}\right] \\[1ex]
						      \bI_s\otimes \left[\begin{array}{cc} \bO   & \bI_{t-v}\end{array}\right]
					      \end{array}\right]\cdot E \cdot (\bI_{s}\otimes L_i^{(t)}) \\
				                                                  & =
				\left[\begin{array}{c}
						      \bI_{s}\otimes L_i^{(v)} \\
						      (E^*\cdot(\bI_{s}\otimes L_i^{(t)})) \otimes L_i^{(t-v)}
					      \end{array} \hspace*{-0.05in}\right]                             \\
				                                                  & =
				\left[\begin{array}{c}
						      \bI_{s}\otimes L_i^{(v)} \\
						      F(\lambda_i)\otimes L_i^{(t-v)}
					      \end{array} \hspace*{-0.05in}\right].
			\end{align*}

			We now claim that if $\lambda_i\notin\{\lambda_{bs+j} : b\in\cB, j\in [s]\}$,
			then
			$F(\lambda_i)$
			is invertible.
			To prove this claim we regard $f_{g,h}(\lambda_i)$ as a polynomial of $\lambda_i$.
			As $f_{g,h}(\lambda_i)= \be_{g,h}\cdot L_i^{(t)},$
			we can see that $\deg(f_{g,h}(\lambda_i)) = v$ if $g = h$, and  $\deg(f_{g,h}(\lambda_i))  < v$ if $g\neq h$.
			So the determinant $f(\lambda_i) = \det(F(\lambda_i))$ is a nonzero polynomial of $\lambda_i$ with $\deg(f) = vs$,
			and $f$ has at most $vs$ roots in $\Fq$.
			According to~\eqref{eq:kerA}, we have
			\begin{equation*}
				K_{0,b}^{(t)}(i,j)= \left\{
				\begin{aligned}
					  & L_{bs+j}^{(t)}, &  & \text{~if~} i= j        \\
					- & L_{bs+j}^{(t)}, &  & \text{~if~} i = b\not=j \\
					  & \bz{t},         &  & \text{~otherwise}.
				\end{aligned}
				\right.
			\end{equation*}
			We use $K_{0,b}^{(t)}(:,j)$ to denote the $j$-th column of $K_{0,b}^{(t)}$.
			Fix any $b\in\cB$ and $j\in [s]$.
			By comparing the columns of the matrix $K_{0,b}^{(t)}$ and $\bI_s\otimes L_{bs+j}^{(t)}$,
			we observe that
			\begin{itemize}
				\item[(1)]  if $b = j$, then $K_{0,b}^{(t)}(:,j)=(\bI_s\otimes L_{bs+j}^{(t)})(:,j)$;
				\item[(2)]  if $b\not=j$, then $K_{0,b}^{(t)}(:,j)=(\bI_s\otimes L_{bs+j}^{(t)})(:,j)-(\bI_s\otimes L_{bs+j}^{(t)})(:,b)$.
			\end{itemize}
			From~\eqref{eq:EKO} we have $E^*\cdot K_{0,b}^{(t)}(:,j)=\bzero$.
			Combining this with above observation, we see that
			the columns of $F(\lambda_{bs+j})(=E^*\cdot (\bI_s\otimes L_{bs+j}^{(t)}))$ are linearly dependent.
			Therefore the determinant $f(\lambda_i) = 0$ for $\lambda_i\in\{\lambda_{bs+j} : b\in\cB, j\in [s]\}$,
			and these are exactly the $vs$ roots of $f(\lambda_i)$.
			This implies that for each $\lambda_i\notin\{\lambda_{bs+j} : b\in\cB, j\in [s]\}$, $F(\lambda_i)$ is invertible.
	\end{itemize}
\end{proof}

Accroding to~\eqref{eq:matA_use_kerA},
we can amplify the matrices in Lemma~\ref{lemma:K0} to make the matrices in  global constraints~\eqref{lemma:globalA} block upper triangular.
\begin{lemma}\label{lemma:A0}
	For subset $\cB\subseteq[s]$ with size $|\cB| = v$ and an integer $t > v$, there exists an $\ell t \times \ell t$ matrix $M_{0, \cB}^{(t)}$ such that
	\begin{itemize}
		\item [(i)]
		      \begin{align*}
			      M_{0, \cB}^{(t)}\cdot A_{0, \cB}^{(t)} =
			      \left[\begin{array}{c}
					            A_{0,\cB}^{(v)} \\
					            \bO
				            \end{array}\right],
		      \end{align*}
		      where $\bO$ is an $\ell(t-v) \times \ell v$ all-zero matrix,
		\item [(ii)] and for  any $0< a < n/s$, $b\in [s]$,
		      \begin{align*}
			      M_{0, \cB}^{(t)}\cdot A_{a, b}^{(t)} =
			      \left[\begin{array}{c}
					            A_{a,b}^{(v)} \\
					            \widehat{A}_{a,b}^{(t-v)}
				            \end{array} \hspace*{-0.05in}\right],
		      \end{align*}
		      where $\widehat{A}_{a,b}^{(t-v)}$ is a matrix column equivalent to $A_{a,b}^{(t-v)}$.
	\end{itemize}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item[(i)]
			Recall that
			$A_{a,b}^{(t)} = \bI_{\ell/s^{a+1}}\otimes K_{a,b}^{(t)}$.
			By Lemma~\ref{lemma:K0} there exists an $st\times st$ matrix $E_{0,\cB}^{(t)}$ satisfying \eqref{eq:cancel_K} and \eqref{eq:cancel_diag_L}.
			Then we have
			\begin{equation*}
				\begin{aligned}
					(\bI_{\ell/s}\otimes E^{(t)}_{0,\cB}) \cdot A_{0,\cB}^{(t)} = & (\bI_{\ell/s}\otimes E^{(t)}_{0,\cB}) \cdot \left[\bI_{\ell/s}\otimes K_{0, b_1}^{(t)} \smat \bI_{\ell/s}\otimes K_{0, b_2}^{(t)}\smat \cdots \smat \bI_{\ell/s}\otimes K_{0, b_v}^{(t)}\right]                           \\
					=                                                             & \left[\bI_{\ell/s}\otimes (E^{(t)}_{0,\cB} \cdot K_{0,b_1}^{(t)}) \smat \bI_{\ell/s}\otimes (E^{(t)}_{0,\cB} \cdot K_{0,b_2}^{(t)}) \smat \cdots \smat \bI_{\ell/s}\otimes (E^{(t)}_{0,\cB} \cdot K_{0,b_v}^{(t)})\right] \\
					=                                                             & \left [
						\bI_{\ell/s}\otimes \left [\begin{array}{c}
								                           K_{0,b_1}^{(v)} \\
								                           \bO
							                           \end{array}\right]
						\smat
						\bI_{\ell/s}\otimes \left [\begin{array}{c}
								                           K_{0,b_2}^{(v)} \\
								                           \bO
							                           \end{array}\right]
						\smat \cdots \smat
						\bI_{\ell/s}\otimes \left [\begin{array}{c}
								                           K_{0,b_v}^{(v)} \\
								                           \bO
							                           \end{array}\right]
						\right].
				\end{aligned}
			\end{equation*}
			Let
			\begin{equation*}
				M^{(t)}_{0,\cB} =
				\left[\begin{array}{l}
						\bI_{\ell/s}\otimes \left[\begin{array}{cc} \bI_{sv} & \bO\end{array}\hspace*{.23in}\right] \\[1ex]
						\bI_{\ell/s}\otimes \left[\begin{array}{cc} \bO   & \bI_{s(t-v)}\end{array}\right]
					\end{array}\right]
				\cdot (\bI_{\ell/s}\otimes E^{(t)}_{0,\cB})
			\end{equation*}
			then we know that
			\begin{align*}
				M^{(t)}_{0,\cB}\cdot A_{0,\cB}^{(t)} =
				\left[\begin{array}{c}
						      A_{0,\cB}^{(v)} \\
						      \bO
					      \end{array} \right].
			\end{align*}
			Here we use the permutation matrix
			$$
				\left[\begin{array}{l}
						\bI_{\ell/s}\otimes \left[\begin{array}{cc} \bI_{sv} & \bO\end{array}\hspace*{.23in}\right] \\[1ex]
						\bI_{\ell/s}\otimes \left[\begin{array}{cc} \bO   & \bI_{s(t-v)}\end{array}\right]
					\end{array}\right]
			$$
			to move the last $t-v$ rows of each block row, in this case they are all-zero rows, to the bottom.
		\item[(ii)]
			For any $0 < a < n/s$, and $b\in [s]$, we view $A_{a,b}^{(t)}$ as an $\ell/s \times \ell/s$ block matrix of which each block is an $st\times s$ matrix.
			Note that its block entry $A_{a,b}^{(t)}(x,y), x,y\in[\ell/s]$ is either $\pm\bI_{s}\otimes L_{i}^{(t)}$ where $i\in \{as^2+bs + j: j\in [s]\}$, or $\bI_s\otimes \bz{t}$.
			Combining $A_{a,b}^{(t)} = \bI_{\ell/s^{a+1}}\otimes K_{a,b}^{(t)}$ and~\eqref{eq:cancel_diag_L}, we can compute the block entry of the product matrix
			$(\bI_{\ell/s}\otimes E_{0,\cB}^{(t)}) \cdot A_{a,b}^{(t)}$ as
			\begin{align}\label{eq:blockentryA}
				\begin{split}
					[(\bI_{\ell/s}\otimes E_{0,\cB}^{(t)}) \cdot A_{a,b}^{(t)}](x,y)
					&=E_{0,\cB}^{(t)} \cdot A_{a,b}^{(t)}(x,y)\\
					&=
					\begin{cases}
						\bI_s\otimes \bz{t}             & \text{ if } A_{a,b}^{(t)}(x,y)= \bI_s\otimes \bz{t}     \\
						\pm\left[\begin{array}{c}
								         \bI_{s}\otimes L_i^{(v)} \\
								         F(\lambda_i)\otimes L_i^{(t-v)}
							         \end{array}\right] & \text{ if } A_{a,b}^{(t)}(x,y)= \pm \bI_s\otimes L_i^{(t)}.
					\end{cases}
				\end{split}
			\end{align}
			According to the mixed-product property of the Kronecker product,
			we have
			\begin{align}\label{eq:mixed-product}
				(F(\lambda_i)\otimes L_i^{(t-v)})\cdot F(\lambda_i)^{-1} = (F(\lambda_i)\cdot F(\lambda_i)^{-1})\otimes L_i^{(t-v)} = \bI_s\otimes L_i^{(t-v)}.
			\end{align}
			Therefore $F(\lambda_i)\otimes L_i^{(t-v)}$ is column equivalent to $\bI_s\otimes L_i^{(t-v)}$.
			Then similarly as in~(i), we compute that
			\begin{equation*}%\label{eq:bigEtimesA}
				\begin{aligned}
					M_{0,\cB}^{(t)} \cdot A_{a, b}^{(t)} =         \left[\begin{array}{l}
							                                                     \bI_{\ell/s}\otimes \left[\begin{array}{cc} \bI_{sv} & \bO\end{array}\hspace*{.23in}\right] \\[1ex]
							                                                     \bI_{\ell/s}\otimes \left[\begin{array}{cc} \bO   & \bI_{s(t-v)}\end{array}\right]
						                                                     \end{array}\right]\cdot (\bI_{\ell/s}\otimes E_{0,\cB}^{(t)}) \cdot A_{a,b}^{(t)}
					=  \left[\begin{array}{c}
							         A_{a,b}^{(v)} \\
							         \widehat{A}_{a,b}^{(t-v)}
						         \end{array} \hspace*{-0.05in}\right],
				\end{aligned}
			\end{equation*}
			where $\widehat{A}_{a,b}^{(t-v)}$ is an $\ell/s \times \ell/s$ block matrix.
			From~\eqref{eq:blockentryA}, the block entry of $\widehat{A}_{a,b}^{(t-v)}$ is
			\begin{align*}
				\widehat{A}_{a,b}^{(t-v)}(x,y)
				=
				\begin{cases}
					\bI_s\otimes \bz{t-v}               & \text{ if } A_{a,b}^{(t-v)}(x,y)= \bI_s\otimes \bz{t-v}         \\
					\pm F(\lambda_i)\otimes L_i^{(t-v)} & \text{ if } A_{a,b}^{(t-v)}(x,y)= \pm \bI_s\otimes L_i^{(t-v)}.
				\end{cases}
			\end{align*}
			Note that the subscripts of $F(\lambda_i)$ in the same block column of $\widehat A_{a,b}^{(t-v)}$ are equal to each other.
			Finally by~\eqref{eq:mixed-product} we can check that the matrix $\widehat{A}_{a,b}^{(t-v)}$ is column equivalent to the matrix $A_{a, b}^{(t-v)}$.
	\end{itemize}
\end{proof}
According to the permutation equivalence property of our MSR codes, we can generalize Lemma~\ref{lemma:A0} as follows.
\begin{lemma}\label{lemma:Aa}
	For $a\in [n/s]$, a subset $\cB\subseteq [s]$ of size $|\cB| = v$, and  a positive integer $t > v$, there exists an $\ell t \times \ell t$ matrix $M_{a, \cB}^{(t)}$ such that
	\begin{itemize}
		\item [(i)]
		      \begin{align*}
			      M_{a, \cB}^{(t)}\cdot A_{a, \cB}^{(t)} =
			      \left[\begin{array}{c}
					            A_{a,\cB}^{(v)} \\
					            \bO
				            \end{array}\right],
		      \end{align*}
		      where $\bO$ is an $\ell(t-v) \times \ell v$ all-zero matrix,
		\item [(ii)]
		      and for any $a< a' < n/s$, $b\in [s]$,
		      \begin{align*}
			      M_{a, \cB}^{(t)}\cdot A_{a', \cB}^{(t)} =
			      \left[\begin{array}{c}
					            A_{a',\cB}^{(v)} \\
					            \widehat A_{a', \cB}^{(t-v)}
				            \end{array}\right],
		      \end{align*}
		      where $\widehat{A}_{a',b}^{(t-v)}$ is a matrix column equivalent to $A_{a',b}^{(t-v)}$.
	\end{itemize}
\end{lemma}
The proof of Lemma~\ref{lemma:Aa} is given in Appendix~\ref{sect:Aa}.
Now we prove our main result by applying Lemma~\ref{lemma:Aa} recursively to make the matrices in global constraints~\eqref{lemma:globalA} block upper triangular.
\begin{proof}[Proof of Lemma~\ref{lemma:globalA}]
	We prove it by induction on the positive integer $c$.
	If $c = 1$, since all the selected $\lambda_i$s satisfying local constraints~\eqref{eq:localA}, we have $\det(A_{a_0, \cB_0})\neq0$.

	For the inductive hypothesis, we assume that the conclusion holds for an arbitrary positive integer $c$.
	For the case of $c+1$, according to Lemma~\ref{lemma:Aa} we have
	\begin{align}\label{eq:det}
		\begin{split}
			\det(M^{(t)}_{a_0,\cB_0}\cdot [ A_{a_0, \cB_0}^{(t)} \smat A_{a_1, \cB_1}^{(t)} \smat \cdots \smat A_{a_{c},\cB_{c}}^{(t)} ])
			&=
			\det(\left[\begin{array}{c|c|c|c}
					A_{a_0,\cB_0}^{(v)} & A_{a_1,\cB_1}^{(v)}            & \cdots & A_{a_c,\cB_c}^{(v)}            \\
					\bO                 & \widehat A_{a_1,\cB_1}^{(t-v)} & \cdots & \widehat A_{a_c,\cB_c}^{(t-v)}
				\end{array} \hspace*{-0.05in}\right])\\
			&= \det\left( A^{(v)}_{a_0, \cB_0}\right) \cdot \det\left([\widehat A_{a_1,\cB_1}^{(t-v)}\smat  \cdots\smat  \widehat A_{a_c,\cB_c}^{(t-v)}]\right)
		\end{split}
	\end{align}
	where $v = |\cB_0|$ and $\widehat{A}_{a_i,\cB_i}^{(t-v)}, 1\le i \le c$ is column equivalent to $A_{a_i,\cB_i}^{(t-v)}$.
	Note that $|\cB_1|+\cdots +|\cB_c| = t-v$.
	By the induction hypothesis, the matrix  $[A_{a_1,\cB_1}^{(t-v)} \smat A_{a_2,\cB_2}^{(t-v)} \smat  \cdots\smat  A_{a_c,\cB_c}^{(t-v)}]$ is invertible.
	Therefore the matrix $[\widehat A_{a_1,\cB_1}^{(t-v)}\smat  \widehat A_{a_2,\cB_2}^{(t-v)} \smat  \cdots\smat  \widehat A_{a_c,\cB_c}^{(t-v)}]$ is also invertible because each $\widehat A_{a_i,\cB_i}^{(t-v)}$ is column equivalent to $A_{a_i,\cB_i}^{(t-v)}$.
	From~\eqref{eq:det} we see that  $\det([ A_{a_0, \cB_0}^{(t)}\smat A_{a_1, \cB_1}^{(t)}\smat \dots \smat A_{a_{c}, \cB_{c}}^{(t)} ])\neq 0$.
\end{proof}

\section{Optimal repair scheme for single node failure}\label{sect:repair}
The repair scheme of our new MSR codes follows easily from the results in Section~\ref{sect:mds}.
\subsection{Repair single node failure for $(n,k,\ell = s^{n/s})$ MDS array code}\label{sect:repairA}
We first describe repair schemes of the MSR codes defined in Section~\ref{sect:consA}.
We denote the index set of helper nodes as $\cH$.

{\bf Repair scheme of $(n,k,s^{n/s})$ MSR codes.}
For every $a\in [n/s]$ and $b\in [s]$, the node $C_{as+b}$ can be recovered from $\{C_{j}(i) : i_a = b, j\in \cH\}$ for any set $\cH\subseteq[n]\setminus\{as+b\}$ with size $|\cH| = d$.
Therefore this repair procedure also has the optimal-access repair property.
According to Appendix~\ref{sect:permu}, no matter which node fails, we can always convert it into the first group.
Therefore, we only need to prove that every node in the first group, i.e. $a=0$, can be repaired.

	{\bf How to repair $C_{h}$,  $h\in [s]$.}
Each $H_i$ in \eqref{eq:parity_check} has $\ell$ block rows, and each block row includes $r$ rows.
In other words, we have $\ell$ sets of parity check equations and each set of them has $r$ parity check equations.
The repair of $C_h$ only involves $\ell/s$ out of these $\ell$ sets of parity check equations.
More precisely, among the $\ell$ block rows in each matrix $H_i$, we pick out the block rows whose indices lie in the set
$\{i\in [\ell]: i_0=h\}$.
These block rows of parity check equations can be again written in the matrix form
\begin{equation}\label{eq:pc_Ch}
	\sum_{z=0}^{h-1}\bar H_z \bar C_z +\sum_{c=0}^{s-1}\bar H_h^{\left\langle c\right\rangle }\bar C_h^{\left\langle c\right\rangle }  + \sum_{z=h+1}^{n-1}\bar{H}_z\bar{C}_z = \bzero,
\end{equation}
where all these $s+n-1$ matrices $\bar H_h^{\left\langle c\right\rangle }, c\in [s]$ and $\bar{H}_z, z\in [n]\setminus \{h\}$ are $\ell/s \times \ell/s$ block matrices whose entries are column vectors of length $r$,
and $C_h^{\left\langle c\right\rangle }, c\in [s]$, $\bar{C}_z, z\in [n]\setminus \{h\}$ are length-$\ell/s$ column vectors.
To be specific, we have
\begin{itemize}
	\item $\bar H_{h}^{ \left\langle h\right\rangle } =   \bI_{\ell/s}\otimes L_{hs+h}^{(r)},$ and for $i\in[\ell/s]$,
	      $\bar{C}_h^{\left\langle h\right\rangle }(i) = C_h(is + h),$
	\item for $c\in[s]\setminus\{h\}$ and $i\in[\ell/s]$,
	      \begin{equation*}
		      \quad  \bar H_{h}^{ \left\langle c\right\rangle } = - \bI_{\ell/s}\otimes L_{hs+c}^{(r)},\quad \text{and}\quad \bar{C}_h^{\left\langle c\right\rangle }(i) = C_h(is + c),
	      \end{equation*}
	\item for $z\in[s]\setminus\{h\}$ and $i\in[\ell/s]$,
	      \begin{equation*}
		      \bar H_z = \bI_{\ell/s}\otimes L_{zs+h}^{(r)},\quad \text{and}\quad \bar{C}_z(i) = C_z(is+h),
	      \end{equation*}
	\item and for $z\in[n]\setminus [s]$ with $z = as+b$ where $a\in[n/s]\setminus\{0\}, b\in[s]$ and $i,j\in[\ell/s]$
	      \begin{align*}
		      \bar{H}_{z}(i,j) & =
		      \left\{
		      \begin{aligned}
			        & L_{as^2+bs+j_{a-1}}^{(r)} &  & \text{~if~} i = j                                                               \\
			      - & L_{as^2+bs+j_{a-1}}^{(r)} &  & \text{~if~} i_{a-1} = b\not=j_{a-1} \text{~and~} i_u=j_u\ \forall\  u\not={a-1} \\
			        & \bz{r}                    &  & \text{~otherwise}  ,
		      \end{aligned}\right. \\
		      \bar{C}_{z}(i)   & = C_{z}(is+h).
	      \end{align*}
\end{itemize}
We observe that the matrices $\bar{H}_{s}, \dots, \bar{H}_{n-1}$ are precisely the $n-s$ parity check matrices that would appear in the
MSR code construction with code length $n-s$ and sub-packetization $\ell/s$.
The other matrices $\bar{H}_{h}^{\left\langle c\right\rangle }, c\in [s]$ and $\bar{H}_{z}, z\in [s]\setminus\{h\}$ are all block-diagonal matrices,
and the diagonal entries are the same within each matrix.
Moreover, the $\lambda_i$'s that appear in $\bar{H}_{h}^{\left\langle c\right\rangle }, c\in [s]$ and $\bar{H}_{z}, z\in [s]\setminus\{h\}$
are distinct from the $\lambda_i$'s that appear in  $\bar{H}_{s}, \dots, \bar{H}_{n-1}$.
The method we used to prove the MDS property of $(n,k,\ell = s^{n/s})$ MSR code construction in Section~\ref{sect:mds} can be generalized to show that \eqref{eq:pc_Ch} also
defines an $(n+s-1,k+s-1,\ell/s)$ MDS array code
$$(\bar{C}_0,\dots, \bar{C}_{h-1},\quad \bar{C}_{h}^{\left\langle 0\right\rangle }, \dots, \bar{C}_{h}^{\left\langle s-1\right\rangle },\quad \bar{C}_{h+1},\dots, \bar{C}_{n-1}).$$
Therefore, $(\bar{C}_h^{\left\langle c\right\rangle }, c\in [s])$ can be recovered from any $d = k+s-1$ vectors in the set $(\bar{C}_z, z\in[n]\setminus\{h\})$.
When we know the values of $(\bar{C}_h^{\left\langle c\right\rangle }, c\in [s])$, we are able to recover all the coordinates of $C_h$.
Finally, according to the definition of $(\bar{C}_z, z\in[n]\setminus\{h\})$, we note that the $(n,k,s^{n/s})$ MSR code defined in Section~\ref{sect:consA} has optimal access property.

\subsection{Repair single node failure of $(n,k,\ell = s^{n/(s+1)})$ MDS array code}\label{sect:repairB}

We now describe repair schemes of the MSR codes defined in Section~\ref{sect:consB}.
We still denote the index set of helper nodes as $\cH$.

{\bf Repair scheme of $(n,k,s^{n/(s+1)})$ MSR codes.} We divide the discussion into two cases:
\begin{itemize}
	\item For every $a\in [n/(s+1)]$, $b\in [s]$ and any set $\cH\subseteq[n]\setminus\{a(s+1)+b\}$ with size $|\cH| = d$, the node $C_{a(s+1)+b}$ can be recovered from $\{C_{j}(i) : i_a = b, j\in \cH\}$.
	\item For every $a\in [n/(s+1)]$ and for any set $\cH\subseteq[n]\setminus\{a(s+1)+s\}$ with size $|\cH| = d$,
	      let $\cH = \cH_1\cup \cH_2$, where $\cH_1\subseteq\{a(s+1), a(s+1)+1, \dots, a(s+1)+s\}$ and $\cH_2\subseteq [n]\setminus \cH_1$.
	      Then the last node $C_{a(s+1)+s}$ in the $a$-th group can be recovered from
	      $$\{C_{j}(i) : i_a\equiv j\mod(s+1), j\in \cH_1\}$$
	      and
	      $$\{\sum_{e=0}^{s-1}C_{j}(i+es^a) : i_a = 0, j\in \cH_2\}.$$
\end{itemize}

Next, we also use the nodes in the first group, i.e., $a=0$ to illustrate the repair procedure.

	{\bf How to repair the first $s$ nodes $C_{b}$, $b\in [s]$.}
This procedure is exactly the same as the repair procedure of $(n,k,\ell = s^{(n/s)})$ MSR code described in Section~\ref{sect:repairA}, so we omit the detail.
Note that the repair of $C_{as+b}, a\in [n/(s+1)], b\in [s]$ in this case also has the optimal access property.

	{\bf How to repair the last node $C_{s}$.}
In order to repair $C_{s}$, we sum up each $s$ block rows of all matrices in \eqref{eq:parity_check}.
More precisely, for each $i\in [\ell/s]$, we sum up block rows whose indices in the set $\{is, is+1, \dots, is+s-1\}$.
Then we obtain $\ell/s$ blocks rows of parity check equations from the original $\ell$ block rows of parity check equations in \eqref{eq:parity_check}.
These $\ell/s$ block rows of parity check equations can be written in the matrix form
\begin{equation}\label{eq:pc_Cs}
	\sum_{z=0}^{s-1}\bar{H}_z\bar{C}_z +  \sum_{c=0}^{s-1}\bar{H}_s^{\left\langle c\right\rangle }\bar{C}_s^{\left\langle c\right\rangle } + \sum_{z=s+1}^{n-1}\bar{H}_z\bar{C}_z = \bzero,
\end{equation}
where all these $n+s-1$ matrices $\bar{H}_z, z\in[n]\setminus\{s\}$, $\bar{H}_s^{\left\langle c\right\rangle }, c\in [s]$ are all $\ell/s \times \ell/s$ matrices, and
$\bar{C}_z, z\in[n]\setminus\{s\}$, $\bar{C}_s^{\left\langle c\right\rangle }, c\in [s]$ are all column vectors of length $\ell/s$.
To be specific, we have
\begin{itemize}
	\item for $z\in[s]$ and $i\in[\ell/s]$,
	      \begin{equation*}
		      \bar H_z = \bI_{\ell/s} \otimes L_{zs+z}^{(r)}, \quad \text{and}\quad \bar C_z(i) = C_z(is+z),
	      \end{equation*}
	\item for $c\in[s]$ and $i\in[\ell/s]$,
	      \begin{equation*}
		      \bar H_s^{\left\langle c\right\rangle } = \bI_{\ell/s}\otimes L_{s^2+c}^{(r)} , \quad \text{and}\quad \bar C_s^{\left\langle c\right\rangle }(i) = C_s(is+c),
	      \end{equation*}
	\item for $z = a(s+1) + b$ where $a\in[n/(s+1)]\setminus\{0\}, b\in[s+1]$ and $i,j\in[\ell/s]$
	      \begin{align}
		       & \bar{H}_{z}(i,j)= \left\{
		      \begin{aligned}
			        & L_{as(s+1)+bs+j_{a-1}}^{(r)} &  & \text{~if~} i = j                                                               \\
			      - & L_{as(s+1)+bs+j_{a-1}}^{(r)} &  & \text{~if~} i_{a-1} = b\not=j_{a-1} \text{~and~} i_u=j_u\ \forall\  u\not={a-1} \\
			        & \bz{r}                       &  & \text{~otherwise}  ,
		      \end{aligned}
		      \right.\nonumber                                                                                   \\
		       & \bar{C}_{z}(i) = C_{z}(is) + C_{z}(is+1) + \cdots +C_{z}(is+s-1). \label{eq:not_optimal_access}
	      \end{align}
\end{itemize}
One can see that the matrices $\bar H_z, z\in[n]\setminus[s+1]$ are precisely the $n-s-1$ parity check matrices that would appear in the
MSR code construction with code length $n-s-1$ and sub-packetization $\ell/s$.
The other matrices $\bar{H}_{z}, z\in [s]$ and $\bar{H}_{s}^{\left\langle c\right\rangle }, c\in [s]$ are all block-diagonal matrices,
and the diagonal entries are the same within each matrix.
Moreover, the $\lambda_i$'s that appear in $\bar{H}_{z}, z\in [s]$ and $\bar{H}_{s}^{\left\langle c\right\rangle }, c\in [s]$
do not intersect with the $\lambda_i$'s that appear in  $\bar{H}_{s+1}, \dots, \bar{H}_{n-1}$.
The method we used to prove the MDS property of $(n,k,\ell = s^{n/(s+1)})$ MSR code construction can be generalized to show that \eqref{eq:pc_Cs} also defines an
$(n+s-1,k+s-1,\ell/s)$ MDS array code
$$(\bar{C}_0,\dots, \bar{C}_{s-1},\quad \bar{C}_{s}^{\left\langle 0\right\rangle }, \dots, \bar{C}_{s}^{\left\langle s-1\right\rangle },\quad \bar{C}_{s+1},\dots, \bar{C}_{n-1}).$$
Therefore, $(\bar{C}_s^{\left\langle c\right\rangle }, c\in [s])$ can be recovered from any $d = k+s-1$ vectors in the set $(\bar{C}_z, z\in[n]\setminus\{s\})$.
When we know the values of $(\bar{C}_s^{\left\langle c\right\rangle }, c\in [s])$, we are able to recover all the coordinates of $C_s$.
Finally, according to the definition of $(\bar{C}_z, z\in[n]\setminus[s+1])$ in \eqref{eq:not_optimal_access}, we note that the $(n,k,s^{n/(s+1)})$ MSR code defined in Section~\ref{sect:consB} does not have optimal access property in the case of repairing the single failed node $C_{a(s+1)+s}$.

\bibliographystyle{ieeetr}
%\bibliographystyle{alpha}
\bibliography{msr}

%\appendices
\appendix

\section{Permutation equivalence of our new MSR codes}\label{sect:permu}

Recall the definition of $A_{a,b}^{(t)}$ in \eqref{eq:matA_use_kerA} and the definition of $H_i$ in \eqref{eq:pcmA}.
The MSR code in Section~\ref{sect:consA} is defined by
\begin{equation}\label{eq:pcpermu}
	\cC = \{(C_0, C_1, \dots, C_{n-1}): \sum_{a\in [n/s],\ b\in [s]} A_{a,b}^{(r)}C_{as+b} = \bzero\}.
\end{equation}
The $n$ nodes are divided into $n/s$ groups of size $s$.
We choose any two distinct groups from these $n/s$ groups, say the $g$th group and the $h$-group.
Now we prove that we can switch nodes in group $g$ with nodes in group $h$ in the following way without destructing the MDS property.

Firstly, we define a permutation function
$\pi_{g,h} : [\ell]\mapsto [\ell]$ by
\begin{align*}
	\pi_{g,h}(i) = & \pi_{g,h}((i_{n/s-1}, \dots, i_{g+1}, i_{g}, i_{g-1}, \dots, i_{h+1}, i_{h}, i_{h-1}, \dots, i_0))                              \\
	=              & (i_{n/s-1}, \dots, i_{g+1}, i_{h}, i_{g-1}, \dots, i_{h+1}, i_{g}, i_{h-1}, \dots, i_0)                                         \\
	=              & (\bar i_{n/s-1}, \dots, \bar i_{g+1}, \bar i_{g}, \bar i_{g-1}, \dots, \bar i_{h+1}, \bar i_{h}, \bar i_{h-1}, \dots, \bar i_0) \\
	=              & \bar i,
\end{align*}
i.e., $\bar i$ is obtained from exchanging values of $i_g$ and $i_h$ in base-$s$ expansion of $i$.
Hence $\pi_{g,h}=\pi_{h,g}$ and $\pi_{g,h} \circ \pi_{g,h}$ is the identity permutation.
We denote the permutation matrix associated to $\pi_{g,h}$ as $\bP_{g,h}$.
Then it is not hard to check that the $i$-th row of $\bP_{g,h}$ is exactly the $\bar i$th row of $\bI_{\ell}$, i.e.,
$$\bP_{g,h}(i,j) = \bI_{\ell}(\pi_{g,h}(i), j) = \bI_{\ell}(i, \pi_{g,h}(j)).$$
We also have $\bP_{g,h}=\bP_{h,g}$ and $\bP_{g,h}\cdot \bP_{g,h} = \bI_{\ell}.$

For $a\in[n/s]$, $b\in[s]$ and a positive integer $t$, we define
$$\bar A_{a,b}^{(t)} = (\bP_{g,h}\otimes \bI_t)A_{a,b}^{(t)}\bP_{g,h}.$$
Now we describe the symmetry of our codes via matrix representation.
\begin{lemma}\label{lemma:mat_permu}
	The matrix $\bar  A_{g,b}^{(t)}$ has the same structure with $A_{h,b}^{(t)}$.
	More precisely, if we replace $\lambda_{hs^2+bs+j}$ by $\lambda_{gs^2+bs+j}, j\in [s]$ in $A_{h,b}^{(t)}$ then we obtain the matrix $\bar A_{g,b}^{(t)}$.
	For $a\notin\{g,h\}$, we have $\bar  A_{a,b}^{(t)} =  A_{a,b}^{(t)}$.
\end{lemma}
\begin{proof}
	Recall the equivalent definition of $A_{a,b}^{(t)}$ in \eqref{eq:matA}, we can check that
	$$\bar{A}_{a,b}^{(t)}(i,j) = A_{a,b}^{(t)}(\pi_{g,h}(i), \pi_{g,h}(j)) = A_{a,b}^{(t)}(\bar{i}, \bar{j}).$$
	For $b\in [s]$, $i,j\in[\ell]$, we have
	\begin{equation*}
		\bar{A}_{g,b}(i, j) = A_{g,b}^{(t)}(\bar{i}, \bar{j})= \left\{
		\begin{aligned}
			  & L_{gs^2+bs+j_h}^{(t)} &  & \text{~if~} i = j                                                   \\
			- & L_{gs^2+bs+j_h}^{(t)} &  & \text{~if~} i_h = b\not=j_h \text{~and~} i_u=j_u\ \forall\  u\not=h \\
			  & \bz{t}                &  & \text{~otherwise}.
		\end{aligned}
		\right.
	\end{equation*}
	If we replace $\lambda_{hs^2+bs+j}$ by $\lambda_{gs^2+bs+j}, j\in [s]$ in $A_{h,b}^{(t)}$ then we obtain $\bar A_{g,b}^{(t)}$.
	Hence $\bar{A}_{g,b}^{(t)}$ has the same structure as ${A}_{h,b}^{(t)}$.
	Similarly as above, we can show that if $a\notin\{g,h\}$, then $\bar{A}_{a,b}^{(t)} = A_{a,b}^{(t)}$ for $b\in [s]$.
\end{proof}
Now we define another MSR code with the same parameters by
$$
	\bar{\cC} = \{( C_0,  C_1, \dots,  C_{n-1}): \sum_{a\in [n/s],\ b\in [s]} \bar{A}_{a,b}^{(r)}  C_{as+b} =  \bzero\}.
$$
Then the parity check equation in \eqref{eq:pcpermu} is equivalent to the following equation
\begin{equation*}
	\begin{aligned}
		  & (\bP_{g,h}\otimes \bI_{t})\cdot \sum_{a\in[n/s], b\in[s]}  A_{a,b}^{(r)}\cdot (\bP_{g,h}\cdot \bP_{g,h}) \cdot C_{as+b}   \\
		= & \sum_{a\in[n/s], b\in[s]} ((\bP_{g,h}\otimes \bI_{t})\cdot A_{a,b}^{(r)} \cdot \bP_{g,h})\cdot (\bP_{g,h} \cdot C_{as+b}) \\
		= & \sum_{a\in[n/s], b\in[s]} \bar A_{a,b}^{(r)}\cdot (\bP_{g,h} \cdot C_{as+b})                                              \\
		= & \bzero.
	\end{aligned}
\end{equation*}
Hence, if $(C_0, C_1, \dots, C_{n-1})\in \cC$, then $(\bar{C}_0, \bar{C}_1, \dots, \bar{C}_{n-1})\in \bar{\cC}$ where $\bar{C}_z = \bP_{g,h}\cdot C_z$ for $z\in [n]$.
Then we conclude that $\cC$ and $\bar{\cC}$ are permutation equivalent.

We further construct a new code by exchanging the nodes in the $g$th group with the nodes in the $h$th group of $\bar{\cC}$.
To be specific, we define $\widehat{A}_{a,b}^{(r)} = \bar A_{a,b}^{(r)}$ if $a\notin \{g,h\}$, $\widehat{A}_{g,b}^{(r)} = \bar A_{h,b}^{(r)}$,  $\widehat{A}_{h,b}^{(r)} = \bar A_{g,b}^{(r)}$, and
$$
	\widehat{\cC} = \{({C}_0, {C}_1, \dots, {C}_{n-1}) : \sum_{a\in [n/s], b\in [s]} \widehat{A}_{a,b}^{(r)}{C}_{as+b} = \bzero \}.
$$
We can readily check that if
$({C}_0, {C}_{1}, \dots {C}_{n-1})\in\bar\cC$
then
$(\widehat{C}_0,\dots, \widehat{C}_1, \dots, \widehat{C}_{n-1})\in\widehat{\cC}$,
where $\widehat{C}_{gs+b} = C_{hs+b}, \widehat{C}_{hs+b} = C_{gs+b}$ for $b\in[s]$ and $\widehat{C}_{as+b} = C_{as+b}$ for $a\notin \{g,h\}, b\in[s]$.
Hence
$\widehat{\cC}$ is permutation equivalent to  $\bar\cC$, and thus permutation equivalent to $\cC$.
Note each $\widehat{A}_{a,b}^{(t)}$ has the same structure with $A_{a,b}^{(t)}$.
The only difference between $\widehat{A}_{a,b}^{(t)}$ and $A_{a,b}^{(t)}$ is that
the values of $\lambda_{gs^2+bs+j}$ and $\lambda_{hs^2+bs+j}$ are switched for $b,j\in [s]$.
We can define $\widehat{A}_{a, \cB}^{(t)}$ similarly as~\eqref{eq:localmatsA}.
Then we can compute that
\begin{align*}
	\widehat{A}_{g, \cB}^{(t)} & = (\bP_{g,h}\otimes \bI_{t})\cdot {A}_{h, \cB}^{(t)}\cdot (\bI_{|\cB|} \otimes  \bP_{g,h}), \\
	\widehat{A}_{h, \cB}^{(t)} & = (\bP_{g,h}\otimes \bI_{t})\cdot {A}_{g, \cB}^{(t)}\cdot (\bI_{|\cB|} \otimes  \bP_{g,h}),
\end{align*}
and $\widehat{A}_{a, \cB}^{(t)}= {A}_{a, \cB}^{(t)}$ for $a\notin\{g,h\}$.
As the matrices $\bP_{g,h}\otimes \bI_{t}$ and $\bI_{|\cB|} \otimes  \bP_{g,h}$ are invertible, the local constraints for $\widehat{\cC}$ and $\cC$ are the same.
In summary, if we switch nodes in group $g$ with nodes in group $h$ of the code $\cC$ in above way, then we get the code $\widehat{\cC}$, and this
does not destruct the MDS property.

As a final remark, for the $(n,k,\ell = s^{n/(s+1)})$ MSR code defined in Section~\ref{sect:consB}, there exists a similar permutation equivalence, and we omit the detail.

\section{Proof of Lemma~\ref{lemma:fieldsizeA}}\label{sect:prooflemma1}

Let $\alpha(s) = \sum_{t = 1}^{s}\binom{s}{t}\frac{st(t-1)}{2}$.
We will use the following well-known result in our proof.
\begin{lemma}[Combinatorial Nullstellensatz, see, N. Alon \cite{alon_1999}]\label{lemma:alon}
	Let $F$ be an arbitrary field, and let $f = f(x_1, \dots, x_n)$ be a  polynomial in $F[x_1, \dots, x_n]$.
	Suppose the degree $\deg(f)$ of $f$ is $\sum_{i = 1}^{n}t_i$, where each $t_i$ is a nonnegative integer, and suppose the coefficient of $\prod_{i=1}^{n}x_i^{t_i}$ in $f$ is nonzero.
	Then, if $S_1,\cdots,S_n$ are subsets of $F$ with $|S_i| > t_i$, there are  $s_1\in S_1, s_2\in S_2, \dots, s_n\in S_n$ so that
	$$f(s_1, \dots, s_n)\neq0.$$
\end{lemma}
We first prove that we can choose $\{\lambda_{j} : j\in [s^2]\}$ from a subset $S\subseteq \Fq$ with size $|S| = s^2+\alpha(s)$ satisfying that $\det(A_{0,\cB})\neq0$ holds for all subset $\cB\subseteq[s]$.
For each subset $\cB = \{b_1, b_2, \dots, b_{|\cB|}\}\subseteq [s]$, we know that the matrix
\begin{equation*}
	\begin{aligned}
		A_{0,\cB} & = [A_{0, b_1}^{(|\cB|)}                     &  & \smat A_{0, b_2}^{(|\cB|)}                     &  & \smat \cdots &  & \smat A_{0, b_{|\cB|}}^{(|\cB|)}                     &  & ]  \\
		          & = [\bI_{\ell/s}\otimes K_{0, b_1}^{(|\cB|)} &  & \smat \bI_{\ell/s}\otimes K_{0, b_2}^{(|\cB|)} &  & \smat \cdots &  & \smat \bI_{\ell/s}\otimes K_{0, b_{|\cB|}}^{(|\cB|)} &  & ],
	\end{aligned}
\end{equation*}
which is column equivalent to $\bI_{\ell/s}\otimes[ K_{0, b_1}^{(|\cB|)} \smat K_{0, b_2}^{(|\cB|)} \smat \cdots \smat K_{0, b_{|\cB|}}^{(|\cB|)}]$.
Therefore, $A_{0,\cB}$ is invertible if and only if $K_{0,\cB}$ is invertible, where
$$K_{0,\cB} = [K_{0, b_1}^{(|\cB|)} \smat K_{0, b_2}^{(|\cB|)} \smat \cdots \smat K_{0, b_{|\cB|}}^{(|\cB|)}].$$
We regard $\det(K_{0,\cB})$ as a polynomial function of variables $\lambda_{bs+j}, b\in \cB , j\in [s]$.
Next, we show that $\det(K_{0,\cB})$ is a nonzero polynomial and $\deg(\det(K_{0,\cB})) = \frac{st(t-1)}{2}$, where $t = |\cB|$.
To see this,  recall that $K_{0,\cB}$ is an $st\times st$ square matrix, and nonzero entries on the same row have the same degree.
To be specific, for $i\in [s], j\in [t]$,  the nonzero entries in the $it+j$th row of $K_{0,\cB}$ all have degree $j$.
Therefore, $\det(K_{0,\cB})$ is a homogeneous polynomial.
Note that the term
$$
	c\cdot \prod_{j\in [s]}1\cdot \prod_{j\in [s]} \lambda_{b_2s+j}^1 \cdot \prod_{j\in [s]} \lambda_{b_3s+j}^2  \cdots \prod_{j\in [s]} \lambda_{b_ts+j}^{t-1}
$$
where $c\in\{1,-1\}$ appears in the expansion of $\det(K_{0,\cB})$, and can only be obtained by
picking constant entries from $K_{0,b_1}$, degree-$1$ entries from $K_{0, b_2}^{(|\cB|)}$, and so on.
Therefore, we can conclude that $\det(K_{0,\cB})$ is a homogeneous polynomial of degree $(0+1+\dots+t-1)s=\frac{st(t-1)}{2}.$

As $A_{0,\cB}$ is invertible if and only if $K_{0,\cB}$ is invertible, the local constraints for group-$0$ are equivalent to
$$     \det(K_{0, \cB})\neq 0 \text{~for~all~} \cB \subseteq [s].$$
We define
$$
	D(\lambda_0, \lambda_1, \dots, \lambda_{s^2-1}) = \prod_{\cB\subseteq [s]} \det(K_{0,\cB}).
$$
One can check that $D$ is a homogeneous polynomial with $s^2$ indeterminates of degree $\sum_{t=1}^{s}{\tbinom{s}{t}}\frac{st(t-1)}{2} \linebreak= \alpha(s)$.
Applying the Combinatorial Nullstellensatz to $D(\lambda_0, \lambda_1, \dots, \lambda_{s^2-1})$,
there exist $s^2$ distinct elements $\lambda_0, \lambda_1, \dots, \lambda_{s^2-1}$ in any subset $S\subseteq\Fq$ with $|S|\ge s^2 + \alpha(s)$ satisfying
$D(\lambda_0, \lambda_1, \dots, \lambda_{s^2-1})\neq0$, and also the local constraints for gourp-$0$.

From the discussion in Appendix~\ref{sect:permu}, we can see that the same result holds for any group, that is,
there exist $s^2$ distinct elements $\{\lambda_{as^2+j}:j \in [s^2]\}$ in any subset $S\subseteq\Fq$ with $|S|\ge s^2 + \alpha(s)$ satisfying the local constraints for group $a$.
Suppose $q \ge ns + \sum_{t = 1}^{s}\binom{s}{t}\frac{st(t-1)}{2}$.
We first choose $s^2$ distinct elements $\{\lambda_{j}:j \in [s^2]\}$ from $\Fq$ satisfying
the local constraints for gourp-$0$.
Then we choose $\{\lambda_{s^2+j}:j \in [s^2]\}$ from $\Fq\setminus \{\lambda_{j}:j \in [s^2]\}$ satisfying
the local constraints for gourp-$1$.
As $q \ge ns + \sum_{t = 1}^{s}\binom{s}{t}\frac{st(t-1)}{2}$, we can repeat this process $n/s$ times.
This concludes our proof.

\section{Proof of Lemma~\ref{lemma:Aa}}\label{sect:Aa}
The case $a = 0$ is already proved in Lemma~\ref{lemma:A0}.
Assume $0 < a < n/s$ and let
$$\bar A_{a, \cB}^{(t)} = (\bP_{a, 0}\otimes \bI_{t})A_{a, \cB}^{(t)}(\bI_{v}\otimes \bP_{a, 0})$$
here the permutation matrix $\bP_{a, 0}$ is defined in Appendix~\ref{sect:permu}.
According to Lemma~\ref{lemma:mat_permu}, we know that $\bar A_{a, \cB}^{(t)}$ has the same structure with $A_{0,\cB}^{(t)}$, i.e., $\bar A_{a, \cB}^{(t)}$ can be obtained by replacing
$L_{bs + j}^{(t)}$ in $A_{0, \cB}^{(t)}$ by $L_{as^2 + bs + j}^{(t)}$ for all $b\in \cB , j \in [s]$.
According to proofs of Lemma~\ref{lemma:K0} and Lemma~\ref{lemma:A0}, one can easily see that there exist an $\ell t \times \ell t$ matrix $\bar M_{a, \cB}^{(t)}$ satisfying that
\begin{equation*}
	\begin{aligned}
		\bar M_{a, \cB}^{(t)} \bar A_{a, \cB}^{(t)} =
		\left [ \begin{array}{c}
				        \bar A_{a, \cB}^{(v)} \\
				        \bO
			        \end{array} \right ]
	\end{aligned}
\end{equation*}
and for all $a < a' < n/s$, $b\in [s]$
\begin{equation*}
	\begin{aligned}
		\bar M_{a, \cB}^{(t)}   A_{a', b}^{(t)} =
		\left [ \begin{array}{c}
				        A_{a', b}^{(v)} \\
				        \widetilde A_{a', b}^{(t-v)}
			        \end{array} \right ]
	\end{aligned}
\end{equation*}
where $\widetilde A_{a', b}^{(t-v)}$ is column equivalent to $A_{a', b}^{(t-v)}$.
Let
\begin{equation*}
	M_{a, \cB}^{(t)} = \left [\begin{array}{c|c}
			\bP_{a,0}\otimes \bI_{v} & \bO             \\
			\hline
			\bO                      & \bI_{\ell(t-v)}
		\end{array}\right]\cdot \bar M_{a, \cB}^{(t)} \cdot (\bP_{a,0}\otimes \bI_{t}).
\end{equation*}
Then we have that
\begin{equation*}
	\begin{aligned}
		M_{a,\cB}^{(t)} \cdot A_{a, \cB}^{(t)}
		 & = \left [\begin{array}{c|c} \bP_{a,0}\otimes \bI_{v}  & \bO \\ \hline \bO & \bI_{\ell(t-v)} \end{array}\right]
		\cdot \bar M_{a, \cB}^{(t)} \cdot (\bP_{a,0}\otimes \bI_{t}) \cdot A_{a,\cB}^{(t)} \cdot (\bI_{v}\otimes \bP_{a,0})^2                           \\
		 & = \left [\begin{array}{c|c} \bP_{a,0}\otimes \bI_{v}  & \bO \\ \hline \bO & \bI_{\ell(t-v)} \end{array}\right]
		\cdot \bar M_{a, \cB}^{(t)} \cdot \bar A_{a, \cB}^{(t)} \cdot (\bI_{v}\otimes \bP_{a,0})                                                        \\
		 & = \left [\begin{array}{c|c} \bP_{a,0}\otimes \bI_{v}  & \bO \\ \hline \bO & \bI_{\ell(t-v)} \end{array}\right]
		\cdot \left [ \begin{array}{c} \bar A_{a, \cB}^{(v)}\\ \bO \end{array} \right ] \cdot  (\bI_{v}\otimes \bP_{a,0})                               \\
		 & = \left [ \begin{array}{c} (\bP_{a,0}\otimes \bI_{v})\cdot \bar A_{a, \cB}^{(v)} \cdot (\bI_{v}\otimes \bP_{a,0})\\ \bO \end{array} \right ] \\
		 & = \left [ \begin{array}{c} A_{a, \cB}^{(v)}\\ \bO \end{array} \right ],
	\end{aligned}
\end{equation*}
and
\begin{equation*}
	\begin{aligned}
		M_{a,\cB}^{(t)}\cdot A_{a',b}^{(t)}
		 & = \left [\begin{array}{c|c} \bP_{a,0}\otimes \bI_{v}  & \bO \\ \hline \bO & \bI_{\ell(t-v)} \end{array}\right]
		\cdot \bar M_{a,\cB}^{(t)} \cdot  (\bP_{a,0}\otimes \bI_{t}) \cdot A_{a',\cB}^{(t)} \cdot \bP_{a,0}^2                                                            \\
		 & = \left [\begin{array}{c|c} \bP_{a,0}\otimes \bI_{v}  & \bO \\ \hline \bO & \bI_{\ell(t-v)} \end{array}\right]
		\cdot \bar M_{a,\cB}^{(t)}  A_{a',\cB}^{(t)} \cdot \bP_{a,0}                                                                                                     \\
		 & = \left [\begin{array}{c|c} \bP_{a,0}\otimes \bI_{v}  & \bO \\ \hline \bO & \bI_{\ell(t-v)} \end{array}\right]
		\cdot \left [ \begin{array}{c} A_{a', b}^{(v)}\\ \widetilde A_{a', b}^{(t-v)} \end{array} \right ] \cdot \bP_{a,0}                                               \\
		 & = \left [ \begin{array}{c} (\bP_{a,0}\otimes \bI_v)\cdot A_{a', b}^{(v)} \cdot  \bP_{a,0}\\ \widetilde A_{a', b}^{(t-v)} \cdot \bP_{a,0} \end{array} \right ]
		= \left [ \begin{array}{c}  A_{a', b}^{(v)} \\ \widehat A_{a', b}^{(t-v)} \end{array} \right ],
	\end{aligned}
\end{equation*}
where $\widehat A_{a', b}^{(t-v)} = \widetilde A_{a', b}^{(t-v)} \cdot \bP_{a,0}$ is also  column equivalent to $A_{a', b}^{(t-v)}$.

\end{document}