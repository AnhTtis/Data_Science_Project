
\appendix


\section{Experimental Details}\label{exp_detail}
% \subsection{Model Architecture of Toy Example}
The details of the CNN used for feasibility study in Section \ref{ftim} are recorded in Table \ref{toy_model}. The control model takes the same architecture as the co-submodel. 


For the experiments in Section \ref{sec_eval}, we search for the optimal the setting of the learning rates and the number of cloud-side local epochs and device-side local epochs per round. The hyper-parameters used in DC-CCL are listed in Table \ref{localepochs}. For fairness in comparison, all the baselines will correspondingly take the same setting as DC-CCL. 

The experimental platform is a workstation with the operating system Ubuntu 18.04.3, CUDA version 11.4, and one NVIDIA GeForce RTX 2080Ti GPU.

\section{Additional Consideration}
In the main body, we propose DC-CCL for the collaborative learning between the cloud and a single mobile device. In fact, the underlying data parallelism framework can be naturally extended to support multiple mobile devices. We need to consider the effect of cross-device data heterogeneity and system heterogeneity on the isomorphism of the device-cloud co-submodel. In particular, the vision samples on different mobile devices are non-iid (e.g., involving different classes), and different mobile devices have heterogeneous resources. We first recall that the size of the device-cloud co-submodel is adjusted according to the resource constraint of the mobile device, not depending on the local samples of the mobile device. As shown in Table \ref{acc_cmp_ideal}, we adopt the same device-side model for the settings of different device-side sample classes. Therefore, the cross-device heterogeneity will not affect the co-submodel. Regarding system heterogeneity, we focus on the container environment of a certain mobile APP, while the runtime capacity of the mobile APP, typically 10MB, has already considered and smoothed the heterogeneity on different types (i.e., low-end, mid-end, and high-end) of mobile devices. If the runtime capacity of the mobile APP varies, DC-CCL also allows scaling up or scaling down the co-submodel with a good performance guarantee, as evaluated in Section \ref{sec:eval:device}.
