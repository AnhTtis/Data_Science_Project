
\begin{table*}[!t]
    \caption{The CNN architecture for the feasibility study in Section \ref{ftim}.} \label{toy_model}
    \vspace{-0.4em}
    \centering
    \resizebox{0.94\textwidth}{!}{
    \begin{tabular}{cccccccc}
    \toprule
Model Component    &  Layer Name & \#Channels/Neurons & Kernel Size & Stride & Padding & \#Params & Size (MB)\\
    \midrule
    {Shared Encoder}     & conv1 & 3 & 5 & 1 & 2 & 9,600 &  0.04 \\
    \cmidrule(lr){1-8}
    \multirow{6}{*}{Cloud-Side Submodel}    & conv1 & 128 & 3 & 1 & 1 & 258,048 & \multirow{6}{*}{5.37}\\
     & maxpool & 128 & 2 & 2 & / & / \\
     & conv2 & 224 & 3 & 1 & 1 & 451,584\\
     & conv3 & 224 & 5 & 1 & 2 & 627,200 \\
     & maxpool & 224 & 2 & 2 & / & / \\
     & fc1 & 7168 & / & / & / & 71,680\\
     \cmidrule(lr){1-8}
    \multirow{6}{*}{Co-Submodel} & conv1 & 128 & 3 & 1 & 2 & 36,864 & \multirow{6}{*}{0.31}\\
     & maxpool & 128 & 2 & 2 & / & / \\
     & conv2 & 32 & 3 & 1 & 1 & 9,216\\
     & conv3 & 32 & 5 & 1 & 2 & 25,600 \\
     & maxpool & 224 & 2 & 2 & / & / \\
     & fc1 & 1024 & / & / & / & 10,240\\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}[!t]
    \caption{The hyper-parameters used in DC-CCL. \#Cloud Epochs and \#Device Epochs denote the numbers of the local epochs in each round of collaborative learning on the sides of the cloud and the mobile device, respectively.}
    % \vspace{-5pt}
    \label{localepochs}
    \centering
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{ccccccccccccc}
    \toprule
\multirow{2}{*}{Dataset}     & \multicolumn{2}{c}{Cloud-Side Submodel} & & \multicolumn{2}{c}{Control Model} & & \multicolumn{3}{c}{Co-Submodel} & & \multicolumn{2}{c}{Co-Submodel's Classifier} \\
\cmidrule{2-3}\cmidrule{5-6}\cmidrule{8-10}\cmidrule{12-13}
 & LR & \#Epochs & & LR & \#Epochs & & LR & \#Cloud Epochs & \#Device Epochs & & LR & \#Epochs \\
    \midrule
    {CIFAR10}      & 0.01 & 15 & & 0.001 & 15 & & 0.01 & 1 & 4 & & 0.01 & 10 \\
    \midrule
    % \cmidrule{1-9}
    {CIFAR100}     & 0.02 & 50 & & 0.002 & 20 & & 0.02 & 1 & 4 & & 0.02 & 10 \\
    \midrule
    % \cmidrule{1-9}
    {StanfordCars} & 0.01 & 100 & & 0.001 & 100 & & 0.01 & 1 & 4 & & 0.01 & 25 \\
    \midrule
    % \cmidrule{1-9}
    {UCF101}       & 0.001 & 50 & & 0.0001 & 50 & & 0.001 & 1 & 4 & & 0.01 & 25 \\
    \midrule
    % \cmidrule{1-9}
    ImageNet-1K \&    & \multirow{2}{*}{0.01} & \multirow{2}{*}{1} & & \multirow{2}{*}{0.001} & \multirow{2}{*}{5} & & \multirow{2}{*}{0.01} & \multirow{2}{*}{1} & \multirow{2}{*}{5} & & \multirow{2}{*}{0.01} & \multirow{2}{*}{1} \\
    StanfordCars & & & & & & & & \\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\section{Conclusion}
In this work, we have proposed DC-CCL, which enables the cloud-side large vision models to continue to learn from fresh device-side samples, given the non-existence of raw sample up-link or large model down-link. DC-CCL vertically splits the base model into a cloud-side submodel and a co-submodel, and only the co-submodel is offloaded to the mobile device for local training with the help of a control model. The evaluation results have demonstrated the effectiveness, efficiency, and superiority of DC-CCL.
