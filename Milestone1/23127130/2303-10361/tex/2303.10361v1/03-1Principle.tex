
\section{Design Rationale}\label{ADR}

In this section, we demystify the design rationale and the key insights of DC-CCL. 

For a large base model using an existing backbone network architecture, we view its optimization process over the samples on the cloud and on the mobile device as a full learning task. The key is to free on-device learning from the dependence on the full base model. Intuitively, the size of device-side samples is small, and there is no need to use a large model. Therefore, as shown in Figure~\ref{dense_model_arc}, a natural idea is to split out most of the base model, called cloud-side submodel, be responsible for the large-scale samples on the cloud, and let the rest small part, called device-cloud co-submodel, learn from device-side samples and further fuse device-side new knowledge with cloud-side knowledge. Based on this intuition, we propose to divide the full learning task of the base model into two subtasks: one subtask for first training the cloud-side submodel over the samples on the cloud; and the other subtask for the mobile device and the cloud to collaboratively train the co-submodel using their respective samples in a data parallelism way. 

However, the subtask (i.e., the forward pass and the backward propagation) of device-cloud co-submodel depends on the cloud-side submodel, as depicted by the red connections in Figure~\ref{dense_model_arc}. This implies that the mobile device still needs to use the unaffordable cloud-side submodel. To decouple two submodels, we propose to cut off the connections between their neurons, as shown in Figure \ref{iso_model_arc}. From the direction of network splitting (i.e., being perpendicular to the input layer), we essentially design a vertical splitting method for the base model. The final model output now takes the sum of the submodels' outputs. This indicates that when the mobile device optimizes the co-submodel over its local samples, the training loss and the gradient should be derived based on the sum of the cloud-side submodel output and the co-submodel output. Nevertheless, the cloud-side submodel cannot be offloaded to the mobile device, and its output is absent. To deal with this problem, we introduce a light-weight control model to mimic the cloud-side submodel output with knowledge distillation. As shown in Figure \ref{device_model_arc}, the model deployed on the mobile device includes the device-cloud co-submodel and the control model, which jointly can approximate the desired gradient for local optimization.         

By overviewing the design flow above, DC-CCL starts from the base model with a certain backbone network architecture, vertically splits it into two submodels with the same backbone, and further decouples the learning processes of two submodels by cutting off their neuron connections and introducing a light-weight control model to mimic the large submodel for on-device deployment. Given the isolation feature of training two submodels, DC-CCL also supports that the cloud-side submodel and the device-cloud co-submodel take different backbone network architectures.  

\section{Feasibility Study}\label{ftim}

\begin{figure}[!t] 
\centering 
\begin{minipage}{0.99\columnwidth} 
\centering 
\includegraphics[width=0.85\linewidth]{.//images//simulation_legends.pdf}
\end{minipage}
\begin{minipage}{0.99\columnwidth} 
\vspace{-0.1cm}
\centering
\subfigure[Whole Training Process]{
\includegraphics[width=0.45\columnwidth]{.//images//submodel_simulation_1.pdf}
}
\subfigure[Training Co-Submodel]{
\includegraphics[width=0.47\columnwidth]{.//images//submodel_simulation_2.pdf}
}
\end{minipage} 
\vspace{-5pt}
\caption{Feasibility study results of model decoupling. Training the device-cloud co-submodel under the correction of the light-weight control model achieves similar accuracy to ideally training the base model.}
\vspace{-10pt}
\label{submodel-acc-val}
\end{figure}



In this section, we study the feasibility of the design rationale. In particular, we validate that through vertical model splitting and submodel learning process decoupling, (1) the device-cloud co-submodel can learn device-side new knowledge and fuse it with cloud-side knowledge; and (2) the device-cloud co-submodel can be trained well with the light-weight control model, not depending on the cloud-side submodel and the full base model any more, but cannot be trained well without the control model.  

We take CIFAR10 dataset and divide it into two subsets for the cloud and the mobile device, covering 8 classes and 2 classes of images, respectively. We use a 5-layer CNN as the base model for image classification, comprised of 4 convolution layers and 1 fully connected layer. We reserve the first convolution layer as a shared low-level encoder and vertically split the other layers into a cloud-side submodel and a device-cloud co-submodel. We let the control model take the same structure as the device-cloud co-submodel. The size of the device-side model, consisting of the low-level encoder, the device-cloud co-submodel, and the control model, is only 5.1\% of the base model. More details about the experimental setups, including model/submodel structures and sizes, are deferred to Table \ref{toy_model} in Appendix \ref{exp_detail}.

To verify the feasibility of model decoupling, we train the decoupled model in two stages. In the first stage, the shared encoder and the cloud-side submodel are trained over the cloud-side samples of 8 classes and then frozen (i.e., will not be updated in the backward propagation) in the second stage, where only the device-cloud co-submodel is trained over the full dataset. We plot the classification accuracy of the decoupled model in Figure \ref{submodel-acc-val}. We observe that compared with the ideal baseline of training the full base model over the full dataset, the decoupled model can achieve similar accuracy. Intuitively, as depicted in Figure \ref{submodel-acc-val-visual}, the cloud-side submodel learns how to classify the samples on the cloud, while the device-cloud co-submodel is responsible not only for classifying device-side samples but also for distinguishing the device-side samples from the cloud-side ones.

\begin{figure}[!t]
    \centering
    \subfigure[Cloud-Side Submodel]{
    \includegraphics[width=0.45\columnwidth]{.//images//conv_2label_cloud.pdf}
    }
    \subfigure[Decoupled Model]{
    \includegraphics[width=0.45\columnwidth]{.//images//conv_2label_full.pdf}
    }
    \vspace{-0.5em}
    \caption{Difference in recognition ability without and with device-cloud co-submodel.}\label{submodel-acc-val-visual}
    %\vspace{-1em}
\end{figure}

We further verify the feasibility and the necessity of the control model in training the device-cloud co-submodel, when the cloud-side submodel is absent. The key difference from the above experiment lies in the second stage: we first train the light-weight control model to mimic the cloud-side submodel with knowledge distillation over the cloud-side samples; and then replace the cloud-side submodel with the control model. We also remove the control model for ablation study. As shown in Figure \ref{submodel-acc-val}, with the control model, the accuracy can approach the ideal baseline; but without it, the accuracy is roughly 15\% lower than the ideal baseline.

%in the training process of the device-cloud co-submodel,


\begin{comment}
We use the summation of the co-submodel and the control model's logits to compute the training loss, i.e., 
\begin{equation}\label{control_loss}
\begin{aligned}
    L \overset{\triangle}{=} \frac{1}{B}\sum_{(x_i,y_i)\in B} l(control\_output_i+co\_output_i, y_i), 
\end{aligned}
\end{equation}
where $(x_i,y_i)$ denotes a data sample in batch $B$, $l(\cdot,\cdot)$ denotes the loss function, $control\_output_i$ denotes the output of the control model, and $co\_output_i$ denotes the output of the co-submodel.
\end{comment}
