\section{Design Details}

\begin{figure*}[!t]
\centering
\subfigure[Cloud-Based Training and Distillation]{
\includegraphics[width=0.28\linewidth]{.//images//DC-CCL-cloud.pdf}
}
\subfigure[Device-Cloud Collaborative Training]{
\includegraphics[width=0.28\linewidth]{.//images//DC-CCL-DL.pdf}
}
\subfigure[Classifier Finetuning]{
\includegraphics[width=0.35\linewidth]{.//images//DC-CCL-finetune.pdf}
}
\caption{Workflow of DC-CCL: (1) the cloud trains the shared encoder and the cloud-side submodel; (2) the cloud trains the light-weight control model to mimic the large cloud-side submodel with knowledge distillation; (3) the cloud and the mobile device collaboratively train the co-submodel under the correction of the control model; and (4) the classifier of the co-submodel is finetuned to mitigate device-cloud sample skewness.}
\label{DC-CCL_workflow}
\end{figure*}

% over its local samples

In this section, we introduce the design details of DC-CCL. We depict the whole workflow in Figure \ref{DC-CCL_workflow}. From device-cloud submodel structures, a low-level feature encoder is shared at the bottom of the large cloud-side submodel, and the light-weight control model, and the small device-cloud co-submodel (Section \ref{sms}). The cloud first trains the shared encoder and the cloud-side submodel over its samples, and then trains the control model to mimic the cloud-side submodel through knowledge distillation (Section \ref{TCM}). Then, the mobile device and the cloud collaboratively train the co-submodel under the correction of the control model in a data parallelism way, and finetune the classifier to mitigate device-cloud sample skewness (Section \ref{dcdt}). We finally introduce how to support that the cloud-side submodel takes a pre-trained model, while the co-submodel and the control model take different backbone network architectures from the pre-trained model (Section \ref{sec:pretrain}).


\subsection{Device-Cloud Submodel Structures}\label{sms}

A base vision model, typically a CNN, from the bottom to the top, is normally stacked by a low-level feature encoder, a high-level feature encoder, and a classifier. The low-level encoder comprises a few shallow layers, whereas the high-level encoder contains many dense layers and dominates the size of the base model. In DC-CCL, the low-level feature encoder of the base model is shared at the bottom of the cloud-side submodel and the device-cloud co-submodel to extract basic and common features. In contrast, the high-level feature encoder and the classifier of two submodels are completely separated. More specifically, with the same low-level features as input, the feature mappings as well as the processes of activation forward and gradient backward are functionally decoupled, and the final outputs (i.e., the logits of the classifier) are aligned and aggregated. The architecture design of the high-level encoders in two submodels can follow the same backbone as the base model's high-level encoder, but take a varying number of filters in each network layer to scale up or scale down the parameter size. We let $\alpha_{cl}$ (resp., $\alpha_{co}$) denote the ratio between the number of filters in the cloud-side submodel (resp., the device-cloud co-submodel) and the number of filters in the base model. If $\alpha_{cl} + \alpha_{co} = 1$, the construction process of two submodels can also be viewed from vertically splitting the base model and cutting off the neural connections, as depicted in Figure \ref{iso_model_arc}. Of course, DC-CCL allows adjusting $\alpha_{cl}$ and $\alpha_{co}$ to adapt to the resource richness and limitation of the cloud and the mobile device. For example, if $\alpha_{co} = \frac{1}{8}$, the numbers of input and output filters are both reduced to their $\alpha_{co} = \frac{1}{8}$, and the device-cloud co-submodel size is reduced to roughly $\alpha_{co}^2=\frac{1}{64}$ of the base model size.


\subsection{Cloud-Based Training}\label{TCM}

The cloud first trains the shared low-level feature encoder and the cloud-side submodel for several epochs. Then, the cloud trains a light-weight control model with knowledge distillation to mimic the cloud-side submodel. Isomorphic to the submodels, the control model shares the low-level encoder at the bottom, takes the same backbone as the base model, but sets a small number of filters. Additionally, the knowledge distillation loss over the cloud-side dataset $D_c$ is:
\begin{equation*}%\label{kd_loss}
\begin{aligned}
    L_{KD} \overset{\triangle}{=} \frac{1}{|D_c|}\sum_{(x_i,y_i)\in D_c} l_{kd}\left(cloud\_output_i, control\_output_i\right),\\
\end{aligned}
\end{equation*}
where $l_{kd}(\cdot, \cdot)$ takes mean squared error (MSE), $cloud\_output_i$ and $control\_output_i$ denote the outputs of the cloud-side submodel and the control model, respectively. After several epochs of knowledge distillation, the output of the control model can approximate that of the cloud-side submodel.  

\subsection{Device-Cloud Collaborative Training}\label{dcdt}


Section \ref{ftim} has demonstrated the feasibility of training the device-cloud co-submodel under the correction of the control model over the global dataset. We now adapt the centralized training of the co-submodel in a practical distributed way. We adopt the classical data parallelism framework, where the cloud and the mobile device keep their samples locally and train the co-submodel over their respective samples. During local training, besides freezing the control model, the shared encoder, the output of which functions as the input of the control model, should also be frozen to ensure the effectiveness of the control model in mimicking the cloud-side submodel. In addition, the cloud-side or the device-side training loss is derived based on the sum of the logits of the control model and the co-submodel, formally defined as
\begin{equation*}%\label{control_loss}
    L \overset{\triangle}{=} \frac{1}{|B|}\sum_{(x_i,y_i)\in B} l_{CE}\left(control\_output_i + co\_output_i, y_i\right), 
\end{equation*}
where $(x_i,y_i)$ denotes a sample from the cloud-side or the device-side data batch $B$, $l_{CE}(\cdot,\cdot)$ takes the cross entropy (CE) loss function, and $control\_output_i$ and  $co\_output_i$ denote the outputs of the control model and the co-submodel, respectively. After multiple iterations of local training, the cloud, as a parameter server, aggregates its updated co-submodel with the updated co-submodel from the mobile device and generates a new co-submodel, which works as the starting point of the next round collaborative training. 

If the distributions of the samples on the cloud and on the mobile device significantly differ from each other (e.g., involving different classes of images), and are long-tailed from the perspective of the global distribution (i.e., missing the samples on the other side), the local updates of device-cloud co-submodel will be skew, degrading the performance of collaborative training. Existing work \cite{lt1, lt2} ever studied cloud-based training over long-tailed distribution and proposed to adjust the classifier. In our device-cloud context, after standard collaborative training, we propose to finetune the co-submodel's classifier over the global dataset, as required. Finetuning the classifier mainly needs the outputs of the co-submodel's high-level encoder over the cloud-side and the device-side samples. Specifically, the size of the high-level features is much smaller than the size of the raw sample. For example, a 224$\times$224 image with RGB channels occupies 150,528 bytes, while the corresponding VGG16's high-level features take only 512 bytes, which is 0.3\% of the raw image size. 
Therefore, if the high-level features of the device-side samples are sensitive and cannot be uploaded to the cloud, classifier finetuning should be performed on the mobile device, and the high-level features of the cloud-side insensitive samples are efficient to be downloaded. Otherwise, the finetuning can be performed on the cloud.

\subsection{Supporting Pre-Trained Models}\label{sec:pretrain}

%Cloud-side pre-trained model, device-cloud co-submodel, control model with different backbone network architectures.

\begin{figure}[!t]
\centering
\includegraphics[width=0.77\columnwidth]{.//images//pretrain-structure.pdf}
\caption{DC-CCL with cloud-side pre-trained model, while the device-cloud co-submodel and the control model take different backbone architectures.}
\label{pre-str}
\end{figure}

The design of DC-CCL above starts from a base model with an existing backbone network architecture, decouples it into the shared low-level encoder, the cloud-side submodel, and the device-cloud co-submodel, and trains them from scratch. We further consider a common case that the cloud-side submodel takes a large pre-trained model. If the device-cloud co-submodel and the control model adopt the same backbone network architecture with the cloud-side pre-trained model, then our design still applies. If the co-submodel and the control model take different backbone architectures from the cloud-side pre-trained model (e.g., MobileNet vs. EfficientNet), then the key difference is that as shown in Figure~\ref{pre-str}, no low-level feature encoder is shared at the bottom any more.   
