
\section{Related Work}

In this section, we review related work and clarify the key difference from DC-CCL.


\subsection{On-Device Inference}

To reduce service latency and communication cost, many models have been offloaded from the cloud to mobile devices for real-time inference (e.g., facial recognition, photo beautification, question answering, and video analytics), no longer requiring to upload the sensitive samples to be predicted. Related work mainly focused on how to enable on-device inference under the resource constraints of mobile devices. 


At the algorithm level, different model compression techniques were proposed to reduce model size and optimize model structure, including quantization~\cite{quan,deep_compress}, model pruning~\cite{prune}, knowledge distillation \cite{arxiv15:hinton:knowdist}, and neural architecture search (NAS)~\cite{onceforall, fewshotnas, nasvit}. Mistify~\cite{mistify} further automates the cloud-to-device model porting process, satisfying the customized requirements of heterogeneous mobile devices. At the system level, existing work focused on how to schedule device-side resources. CoDL \cite{codl} splits each layer of a model vertically into subtasks and adaptively allocate them on a CPU and a GPU based on runtime resource conditions. Band \cite{band} partitions the model into a set of operator groups and assigns them to different processors based on their dependencies. Gemel~\cite{gemel} proposed a GPU memory management technique, which exploits architectural similarities among multiple on-device vision models for layer sharing.


The above work involves the cloud-to-device model offloading process and the on-device inference phase, but does not incorporate the on-device training phase and the device-to-cloud knowledge fusion process as in DC-CCL. 


\subsection{Efficient Finetuning}

Adapting the model pre-trained on the cloud to diverse downstream tasks is important in practical deployment. Model finetuning over task-specific data is a widely used method. Existing work mainly focused on improving finetuning efficiency, specific to different downstream platforms.


When the downstream tasks are on powerful servers, storing the large full model is allowed, and how to reduce the size of tunable parameters becomes a key problem, especially for large lanange models (e.g., the series of GPT). \citet{lora} proposed LoRA, where the original weights are frozen, and the update is represented with a low-rank decomposition. \citet{bitfit} introduced BitFit, a sparse update method that modifies only the bias terms of the model. \citet{offsite} proposed offsite-tuning, which finetunes the top and bottom layers of model and compresses the large middle layers into a emulator using layer-drop. \citet{proc:emnlp21:prompt} proposed prompt tuning, requiring to store and tune only a small task-specific prompt (i.e., a few tokens) for each downstream task.

When the downstream tasks are on resource-constraint mobile devices, the key problem turns to reducing the size of the entire model. Besides general compression techniques, one line of work developed light-weight convolution kernels specifically for CNNs. \citet{mobilenet} proposed depthwise separable convolution in MobileNets, where the regular convolution is replaced by the combination of a single layer filter and a 1$\times$1 filter. \citet{shufflenet} proposed pointwise group convolution in ShuffleNet, which splits the input channels into groups and applied a 1$\times$1 filter convolution to each group separately. Another line of work further considered reducing training memory. \citet{tinytl} proposed to update only bias term for device-affordable CNNs. \citet{melon} applied memory saving techniques, such as re-computation and micro-batch. \citet{mgemm} designed a novel matrix decomposition and recombination method.

These work, however, did not consider how to transfer the new knowledge from the downstream mobile device to the upstream cloud server, which is the focus of DC-CCL. 


\subsection{Device-Cloud Collaborative Learning} 


To integrate the natural advantages and the resources of both mobile devices and the cloud in supporting intelligent services, device-cloud collaborative learning (including inference and training) emerged as a new paradigm.  

In device-cloud collaborative inference, existing work considered how to split the inference task between the cloud and the mobile devices, or let the mobile devices perform pre-inference. Neurosurgeon \cite{neurosurgeon} automatically partitions the model computation at the granularity of DNN layers. SPINN \cite{spinn} co-optimises the horizontal splitting of CNN and the early-exit policy to meet user-defined service-level requirements. AgileNN~\cite{agilenn} vertically splits the outputs of the low-level feature extractor, lets the cloud and the mobile device separately complete the remaining forward processes, the outputs of which are combined as the inference results. Reducto \cite{reducto} filters out unnecessary frames on the mobile device using a light-weight model and uploads the other frames to the cloud for video analytics.

In device-cloud collaborative training, according to the interaction mechanism, existing work can be generally divided into three categories: model-based, sample-based, and feature-based. (1) The model-based line of work enables device-cloud collaboration through exchanging model and its update periodically, and requires the model to be deployable on the mobile device. The most typical framework is FL \cite{fed}, which requires the cloud and the mobile devices to use the full model. The follow-up work \cite{niu_mobicom20} focused on the large embedding layer of recommendation or NLP models and proposed to let each mobile device retrieve a few embedding vectors in a key-value way for local training. \cite{hermes} instead focused on CNN, required each mobile device to download the full model for initialization, and introduced structured sparsity regularization into local training, thereby gradually pruning low-magnitude weights. (2) The sample-based line of work allows the mobile devices and the cloud to exchange samples, which is feasible in some application scenarios where data privacy and communication overhead (e.g., the size of each user's behavior data in recommender systems is not very large.) are not major concerns. \citet{yao_kdd21} trained the model over all the users' data and further improved the model performance over long-tail users with knowledge distillation. \citet{yan_kdd22} and \citet{gu_alibaba21} proposed to let each mobile device retrieve some similar samples from the cloud-side global pool, thereby augmenting the local data for personalized training. (3) The feature-based line of work facilitates device-cloud collaborative learning through exchanging the model's intermediate outputs. The typical framework is split learning \cite{splitl}, which offloads the low-level encoder to the mobile device and uploads the low-level features to the cloud. However, the exchange of low-level features may raise privacy concerns and incur high communication overhead. The follow-up work \cite{fedgkt} proposed to alternately optimize the device-side low-level encoder and the cloud-side layers to avoid frequent device-cloud communication.


DC-CCL differs from the above work in several practical considerations, including no large model down-link, no raw sample up-link, and no low-level feature up-link.


\subsection{Decoupled Structure}

In CV scenarios, the deep models adopt dense connections. Some existing work on the cloud-based distributed learning studied model parallelism methods. \citet{modelassemble} proposed to divide the model horizontally into submodels, train them in parallel, and link them for final finetuning. \citet{alexnet} proposed group convolution to facilitate model parallelism, which splits the channels of a convolutional layer into multiple groups and performed convolution within each group, thus decoupling their forward processes. \citet{fed2} later applied group convolution to cross-device FL, decoupling the model into several submodels and linking each submodel with a certain class to mitigate the local training bias caused by label missing. Therefore, if the local samples of a mobile device involve many classes, the number of required submodels is large, breaking the resource limit.



In contrast to CV, the models in recommender systems and NLP normally contain a large and sparse embedding layer for the full set of items/words. A mobile device tends to involve a few items/words, retrieves only the corresponding embedding vectors, and directly takes the other dense network layers \cite{niu_mobicom20}. However, such a decoupling method may be not applicable to other model structures.


Compared with the above work, DC-CCL focuses on CV models, and not only decouples the forward passes of two submodels through vertical splitting but also decouples their training processes via a control model. The model size for the mobile device depends only on the available resource. 
