\section{Problem Formulation}

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{.//images//setting.pdf}
\caption{An illustration of the considered device-cloud collaborative learning scenario.}
\label{problem_setting}
\end{figure}

\begin{figure*}[!t]
\centering
\subfigure[Base Model]{
\includegraphics[width=0.29\linewidth]{.//images//base_model.pdf}
\label{dense_model_arc}
}
\subfigure[Decoupled Model]{
\includegraphics[width=0.33\linewidth]{.//images//isolated_full_model.pdf}
\label{iso_model_arc}
}
\subfigure[Device-Side Model]{
\includegraphics[width=0.33\linewidth]{.//images//device_training_model.pdf}
\label{device_model_arc}
}
\caption{Illustrations of a base model with an existing backbone network architecture, the corresponding decoupled model on the cloud, and the device-side model. The forward/backward processes of the cloud-side submodel and the device-cloud co-submodel are mutually dependent in the base model, but are isolated in the decoupled model. The device-side model contains the co-submodel and a light-weight control model that mimics the cloud-side submodel. The parallelograms denote feature maps, and the cubes denote filters.}
\end{figure*}

In this section, we describe the considered scenario, the problem, and the motivations in detail. 

As shown in Figure \ref{problem_setting}, we focus on CV tasks with two practical considerations on the sample up-link from a mobile device to the cloud and the model down-link from the cloud to the mobile device. In particular, (1) users can generate or collect some fresh vision samples and store them locally on their mobile devices. In addition, a user may be reluctant to upload (part of) his/her samples to the cloud in practice. The concerns mainly come from privacy and security (e.g., sensitive personal photos) as well as communication overhead (e.g., because the size of vision data is normally larger than the size of data in other modals, and the up-link of a mobile device tends to pose the bottleneck); and (2) the cloud maintains a large model with a strong recognition ability. Nevertheless, the model size far exceeds the runtime capacity of a mobile device (i.e., 10MB for mobile APPs in industry). and cannot be offloaded for on-device learning; otherwise, the limits of memory and CPU will be broken, leading to service crash and degrading user experience. 

Given the disconnections of the sample up-link and the large model down-link between the cloud and the mobile device, we consider how to enable the large model on the cloud to benefit from the local samples on the mobile device, without uploading the local samples and without offloading the large model. Such a problem is well motivated, because the mainstream cloud-based learning framework optimizes the large model only over the cloud-side samples, but fails to leverage device-side samples, while more data can effectively improve the model's generalization ability. Meanwhile, this problem is also atypical in the isolation setting of the large model and the samples, whereas existing learning frameworks require the model and the samples to be put together (e.g., on the cloud under the cloud-based learning framework or on each mobile device under the FL framework).
