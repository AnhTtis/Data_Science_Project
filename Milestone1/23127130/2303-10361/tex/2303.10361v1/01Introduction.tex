\section{Introduction}

Nowadays, more and more deep vision models are deployed on the cloud server to provide mobile APP users with diverse intelligent services, such as image recognition (e.g., in Google Lens and Google Photos), which categorizes the photos taken by mobile device users in daily life; livestreaming highlight recognition (e.g., in TikTok, Taobao Live, and Weixin Channels Live), which identifies the key time points of a streamer in introducing attractive information; and video analytics (e.g., in YouTube, YouKu, and Kuaishou), which adds tags, extracts topics, and identifies key events from the published videos for viewers. In the meanwhile, during the usage of these mobile APPs, fresh samples with user feedback are continuously generated on each served mobile device.

To learn from the device-side samples and exploit the natural advantages of mobile devices being close to users and data sources, some work explored the new paradigm of device-cloud collaborative learning \cite{proc:osdi22:walle}. One line of work \cite{yao_kdd21,yan_kdd22} relied on the raw sample up-link from the mobile device to the cloud, which is feasible only when data privacy and communication overhead do not pose major concerns. Another line of work resorted to model down-link. The celebrated framework is cross-device federated learning (FL) \cite{fed}, which requires mobile devices to download the full affordable model and train it from scratch, while the cloud periodically aggregates the model updates. Other work on efficient on-device learning \cite{quan,prune,mobilenet,tinytl}
considered the case that the original model breaks the resource limit of the mobile device, enabled model offloading by reducing model size and optimizing model structure, and further allowed cloud-to-device knowledge transfer and device-side adaptation. 

Different from existing work, we consider how the cloud-side model benefits from the fresh device-side samples (i.e., the need to transfer the device-side knowledge back to the cloud and fuse it with the cloud-side knowledge) without raw sample up-link and without large model down-link in the computer vision (CV) scenarios. 

On the one hand, the local vision samples of each mobile device, which contain sensitive information, cannot be uploaded to the cloud to protect user privacy. In addition, compared with each user's daily typed words in natural language processing (NLP) scenarios and daily behavior data in recommender systems, which are in the size of KB, the size of 100 images with $224\times224$ pixels is roughly 14MB, and the size of a 5 minutes-long, 1080p, and 8Mbps video is roughly 300MB. Therefore, it is also communication efficient for each mobile device to avoid uploading the vision samples. 

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{.//images//cnn_sizes.pdf}
\caption{Sizes of some classical CNNs on ImageNet-1K.}
%\vspace{-5pt}
\label{cnn_sizes}
\end{figure}

On the other hand, the cloud-side vision model normally has a large size of parameters for a strong recognition ability, which, however, may exceed the runtime capacity of mobile APPs. According to the deployment practice in industry, the size of the model to be trained in a mobile APP should be strictly no larger than 10MB, preventing APP crash and poor user experience. In contrast, as depicted in Figure \ref{cnn_sizes}, the sizes of several mainstream convolutional neural networks (CNNs) designed for the cloud-based image classification task with high accuracy (e.g., VGG19, ResNet152, and EfficientNet-B7) are larger than 200MB. Furthermore, different from NLP and recommendation models with a dominant sparse embedding layer \cite{niu_mobicom20}, the vision models follow dense connection patterns and do not have much structural redundancy. As a result, some cloud-based large vision models cannot be structurally compressed with a high compression ratio and a low accuracy loss such that they can be offloaded to the mobile device for local training and further facilitate transferring the device-side knowledge back to the cloud for fusion.  

Given the practical requirement and challenges above, we propose a new Device-Cloud Collaborative Controlled Learning framework, denoted as DC-CCL for short. The key of DC-CCL is to decouple the on-device learning process from the dependence on the full model. The motivating intuition is that training over the mobile device's few fresh samples needs to update only a small part of the full model, not requiring most of the model parameters for learning over the cloud-side, large-scale samples. Therefore, DC-CCL vertically splits the base model with an existing backbone network architecture into: (1) a large cloud-side submodel for learning the cloud-side knowledge as regular; and (2) a small device-cloud co-submodel for learning device-side knowledge and fusing it with the cloud-side knowledge. Due to the decoupling feature, two submodels can take different backbone architectures. In addition, the optimization process is correspondingly divided into two decoupled phases: (1) training the cloud-side submodel purely on the cloud from scratch or directly taking a pre-trained model; and (2) training the device-cloud co-submodel with the collaboration of the cloud and the mobile device in a data parallelism way. However, on-device training of the co-submodel still requires the outputs of the cloud-side submodel, which is unaffordable by the mobile device. To deal with this problem, DC-CCL introduces a light-weight model to mimic the cloud-side submodel with knowledge distillation and offloads it to the mobile device, helping compute the desired gradient of the co-submodel and controlling its optimization direction. 

We summarize the key contributions as follows: 
\begin{itemize}
    \item DC-CCL is the first device-cloud collaborative learning framework that enables cloud-side large vision models to learn from device-side samples without the raw sample up-link and the large model down-link.
    \item DC-CCL decouples on-device learning from the dependence on the cloud-side full model, by updating only a small submodel vertically split out and further introducing a light-weight model, which can approximate the remaining large submodel's outputs, to control local optimization.
    \item Evaluation results over several public datasets and different models reveal that (1) the device-side model size is 4.32\% -- 27.86\% of the cloud-side model size and can scale down or up to balance efficiency and accuracy in DC-CCL; and (2) DC-CCL improves the accuracy by 3.52\% -- 41.32\%, compared with the baselines of training only with the cloud-side samples or adopting a small device-affordable model.
\end{itemize}
 