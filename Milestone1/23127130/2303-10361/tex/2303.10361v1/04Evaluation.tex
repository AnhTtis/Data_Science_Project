\section{Evaluation}\label{sec_eval}

In this section, we first introduce evaluation setup (Section \ref{sec:eval:setup}); then compare with ideal and practical baselines (Sections \ref{sec:eval:large} and \ref{sec:eval:small}); further show ablation study (Section \ref{sec:eval:abl}); and finally reveal device-side model flexibility (Section \ref{sec:eval:device}).   

\begin{table*}[!t]
    \caption{DC-CCL vs. Baselines with large model. The class ratio denotes the ratio between the number of cloud-side sample classes and that of the device-side classes. $\Delta$ denotes the accuracy improvement of DC-CCL over Cloud-B.}
    \vspace{-0.3em}
    \label{acc_cmp_ideal}
    \centering
    \resizebox{0.93\textwidth}{!}{
    \begin{tabular}{cccccccccc}
    \toprule
\multirow{2}{*}{Dataset}     & \multirow{2}{*}{Central-D} & \multirow{2}{*}{Central-B} & \multicolumn{2}{c}{Model Size (MB)}
& \multirow{2}{*}{Class Ratio} & \multicolumn{1}{c}{\multirow{2}{*}{Distr-D}} & \multirow{2}{*}{Cloud-B} & \multirow{2}{*}{DC-CCL} & \multirow{2}{*}{$\Delta$} \\
\cmidrule{4-5}
 &  &  & Cloud-Side & Device-Side & & & &  \\
    \midrule
\multirow{4}{*}{CIFAR10} & \multirow{4}{*}{84.38\%} & \multirow{4}{*}{86.01\%} & \multirow{4}{*}{98.42} & \multirow{4}{*}{4.27} & 1/9 & \multicolumn{1}{c|}{79.77\%} & 76.63\% & 80.60\% & +3.97\%\\
            & & & &    & 2/8 & \multicolumn{1}{c|}{79.42\%} & 67.95\% & 80.32\% & +12.37\% \\
            & & & &    & 3/7 & \multicolumn{1}{c|}{76.38\%} & 60.10\% & 77.40\% & +17.30\% \\
            & & & &    & 4/6 & \multicolumn{1}{c|}{77.10\%} & 54.33\% & 77.31\% & +22.98\% \\
    \midrule
\multirow{4}{*}{CIFAR100} & \multirow{4}{*}{67.21\%}  & \multirow{4}{*}{67.52\%}  & \multirow{4}{*}{32.83} & \multirow{4}{*}{1.43} & 10/90 & \multicolumn{1}{c|}{66.22\%} & 61.75\% & 65.27\% & +3.52\% \\
            & & & &   & 20/80 & \multicolumn{1}{c|}{57.98\%} & 56.54\% & 62.74\% & +6.20\% \\
            & & & &   & 30/70 & \multicolumn{1}{c|}{60.11\%} & 49.65\% & 61.65\% & +12.00\% \\
            & & & &   & 40/60 & \multicolumn{1}{c|}{61.34\%} & 43.80\% & 58.55\% & +14.75\% \\
    \midrule
\multirow{4}{*}{StanfordCars} & \multirow{4}{*}{66.12\%}   & \multirow{4}{*}{71.01\%}   & \multirow{4}{*}{12.90} & \multirow{4}{*}{3.11}  & 20/176 & \multicolumn{1}{c|}{66.71\%} &  59.05\% & 63.20\% & +4.15\% \\
            & & & &   & 30/166 & \multicolumn{1}{c|}{64.27\%} & 55.15\% & 59.61\% & +4.46\% \\
            & & & &   & 40/156 & \multicolumn{1}{c|}{62.28\%} & 52.53\% & 57.05\% & +4.52\% \\
            & & & &   & 50/146 & \multicolumn{1}{c|}{61.53\%} & 47.68\% & 55.20\% & +7.52\% \\
    \midrule
\multirow{3}{*}{UCF101} & \multirow{3}{*}{86.56\%}  & \multirow{3}{*}{87.45\%}  & \multirow{3}{*}{229.28} & \multirow{3}{*}{9.91} & 10/101 & \multicolumn{1}{c|}{85.63\%} & 79.04\% & 83.78\% & +4.74\% \\
            & & & &   & 20/91 & \multicolumn{1}{c|}{83.67\%} & 71.08\% & 83.89\% & +12.81\% \\
            & & & &   & 30/81 & \multicolumn{1}{c|}{83.82\%} & 61.90\% & 81.23\% & +19.33\% \\
            % & 40  & \% & \% &           &           & \% & \% & \% & \% \\
    \midrule
ImageNet-1K \&   & \multirow{2}{*}{72.93\%} & \multirow{2}{*}{69.42\%} & \multirow{2}{*}{75.12} &  \multirow{2}{*}{20.93} & \multirow{2}{*}{196/1000} & \multicolumn{1}{c|}{\multirow{2}{*}{69.38\%}} &  \multirow{2}{*}{60.11\%} & \multirow{2}{*}{69.32\%} & \multirow{2}{*}{+9.21\%} \\
StanfordCars & & & & & & \multicolumn{1}{c|}{} & &\\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\subsection{Experimental Setup}\label{sec:eval:setup}

% with RGB channels

\quad\textbf{Datasets.} We use CIFAR10~\cite{cifar}, CIFAR100~\cite{cifar}, StanfordCars~\cite{cars_data}, ImageNet-1K~\cite{imagenet}, and UCF101~\cite{ucf101}. CIFAR10 and CIFAR100 comprise 10 and 100 classes, respectively, 50,000 training images and 10,000 testing images. StanfordCars consists of 196 classes of car images, 8,144 training images and 8,041 testing images. ImageNet-1K involves 1,000 classes and comprises 1,281,167 training images and 50,000 testing images. UCF101 contains 13,320 videos of 101 action classes, 80\% for training and 20\% for testing. 


\textbf{Device-Cloud Sample Partition.} We divide the full training set of CIFAR10, CIFAR100, StanfordCars, or UCF101 into two subsets according to labels, one for the mobile device and the other for the cloud. We also vary the number of device-side sample classes, and the number of cloud-side sample classes varies as well. For CIFAR10 and CIFAR100, we augment the device-side training set by randomly introducing a few cloud-side samples, the size of which is less than 10\% of the size of the original device-side training set. In the inference phase, the accuracy is computed over each dataset's full test set. We also leverage ImageNet-1K and StanfordCars to evaluate the case of pre-trained model, where the cloud keeps the training set of ImageNet-1K, and the mobile device holds the training set of StanfordCars, while the inference accuracy is evaluated over the union of the test sets of two datasets. To mitigate the long-tail distribution due to the difference between ImageNet-1K and StanfordCars, the cloud randomly chooses only $8,144 / 196 \times 1,000 \approx 40,000$ training images from ImageNet-1K, achieving the best performance. 


\textbf{Base Models and Device-Cloud (Sub)models.} For the image classification task over CIFAR10, CIFAR100, and StanfordCars, as well as the action recognition task over UCF101, we take VGG16~\cite{vgg}, ResNet18~\cite{resnet}, EfficientNet-B0~\cite{efficientnet}, and C3D~\cite{c3d} as the base model, respectively. We choose the first layer as the shared low-level encoder and perform vertical splitting for the upper layers. In particular, we set the number filters in the cloud-side submodel and the device-cloud co-submodel $\alpha_{cl}=\frac{7}{8}$ and $\alpha_{co}=\frac{1}{8}$ of that in the base model, respectively, by default. We also vary $\alpha_{co}$ to scale up and scale down the co-submodel. For the cloud-side pre-trained model over ImageNet-1K, we take EfficientNet-B4~\cite{efficientnet}, while adopting a pre-trained MobileNetV3-small~\cite{mobilenetv3-small} as the device-cloud co-submodel. In each evaluation, the control model takes the same structure as the device-cloud co-submodel. We list the size of the cloud-side model (i.e., the shared encoder, the cloud-side submodel, and the co-submodel) and the size of the device-side model (i.e., the shared encoder, the control model, and the co-submodel) in  Table~\ref{acc_cmp_ideal}.


\textbf{Implementation Details.} We implement DC-CCL with {PyTorch 3.7}. We take adaptive moment estimation (Adam) as the optimizer of the knowledge distillation for the control model and take stochastic gradient descent (SGD) as the optimizer of all the other training phases. For device-cloud collaborative training, we set the total number of communication rounds to 15, 20, 25, 25, and 10 for CIFAR10, CIFAR100, StanfordCars, UCF101, and ImageNet-1K \& StanfordCars. More details on the learning rates, the number of epochs for training cloud-side submodel, knowledge distillation, and classifier finetuning, are deferred to {Table \ref{localepochs}} in Appendix \ref{exp_detail}.

\begin{figure*}[!t]
\begin{minipage}{0.62\linewidth}
\centering 
\begin{minipage}{0.8\textwidth} 
\centering 
\includegraphics[width=\textwidth]{.//images//legends.pdf}
\end{minipage}
\begin{minipage}{\textwidth} 
\vspace{-0.6em}
\centering
\subfigure[Parameters Size]{
\includegraphics[width=0.47\textwidth]{.//images//param_size-v3.pdf}
}
\subfigure[FLOPs]{
\includegraphics[width=0.47\textwidth]{.//images//flops-v2.pdf}
}
\end{minipage}
\vspace{-0.8em}
\caption{The sizes and FLOPs of the base model, the decoupled model, and the device-side model.} \label{model-efficiency}
\end{minipage}
\hfill
\begin{minipage}{0.35\linewidth}
\vspace{0.6em}
\begin{minipage}{\textwidth} 
\centering 
\includegraphics[width=0.93\textwidth]{.//images//curve_legends.pdf}
\end{minipage}
\begin{minipage}{\textwidth} 
\centering 
\includegraphics[width=0.95\textwidth]{.//images//acc_curve.pdf}
\vspace{0.1em}
\caption{Device-cloud collaborative learning process over StanfordCars.}
\label{acc_curve}
\end{minipage}
\end{minipage}
\end{figure*}


\subsection{DC-CCL vs. Baselines with Large Model}\label{sec:eval:large}


We first validate the necessity of exploiting both cloud-side and device-side samples. We introduce two baselines:
\begin{itemize}
    \item {\bf Central-B}, which ideally puts aside that the device-side samples cannot be uploaded to the cloud and trains the base model over the full training set;
    \item {\bf Cloud-B}, which trains the base model only over the cloud-side training set.
\end{itemize}
We further consider exploiting the full training set under the practical settings that the vision samples are distributed on the mobile device and the cloud, and the resource-constraint mobile device cannot hold a large model. We introduce another two baselines:
\begin{itemize}
    \item {\bf Central-D}, which ideally puts aside that the device-side samples cannot be uploaded to the cloud and trains the decoupled model over the full training set;  
    \item {\bf Distr-D}, which ideally puts aside that the decoupled model is unaffordable to be offloaded to the mobile device, lets the mobile device and the cloud train the decoupled model in a data parallelism way, and also applies classifier finetuning as DC-CCL.  
\end{itemize}
We report in Table \ref{acc_cmp_ideal} the inference accuracy of the four baselines and the proposed DC-CCL as well as the number of sample classes and the model size on the sides of the mobile device and the cloud. 


By comparing Central-B with Cloud-B, we can see that as the size of device-side samples becomes larger, the advantage of Central-B over Cloud-B is more remarkable. For example, for StanfordCars and UCF101, when the number of device-side sample classes is roughly 1/3 of the number of cloud-side sample classes, the inference accuracy of Central-B is at least 23\% higher than that of Cloud-B. These results demonstrate that it is quite necessary to optimize the model over both the cloud-side and the device-side samples. 

By comparing Distr-D with Central-D, we can see that Distr-D underperforms Central-D, especially over CIFAR10. In addition, when the sample classes are more balanced distributed on the mobile device and the cloud, the performance of Distr-D generally becomes worse. Such performance degradation is mainly due to the practical non-independent and identically distributed (non-iid) feature of the device-side and the cloud-side samples. 


\begin{comment}
\begin{table*}[!t]
\begin{minipage}{0.54\textwidth}
%\begin{table}
    \caption{DC-CCL vs. Baselines with small model. $\Delta_1$ and $\Delta_2$ denote the accuracy improvements of DC-CCL over Distr-S and Incr-S, respectively.}
    %\vspace{-5pt}
    \label{acc_cmp_light}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccc}
    \toprule
Dataset     & %Light-Central %& 
Class Ratio & Distr-S & Incr-S & DC-CCL & $\Delta_1$ & $\Delta_2$\\
    \midrule
\multirow{4}{*}{CIFAR10}  %& \multirow{4}{*}{78.84\%}    
& 1/9   & 75.26\%      & 68.99\%    & 80.60\%  & +5.34\% & +11.61\% \\
            & 2/8   & 70.28\%      & 66.35\%   & 80.32\% & +10.04\% & +13.97\% \\
            & 3/7   & 64.63\%      & 68.18\%   & 77.40\% & +12.77\% & +9.22\% \\
            & 4/6   & 61.69\%      & 66.62\%   & 77.31\% & +15.62\% & +10.69\% \\
    \midrule
\multirow{4}{*}{CIFAR100}  %& \multirow{4}{*}{57.30\%}    
& 10/90  & 52.36\%      & 44.59\%   & 65.27\% & +12.91\% & +20.61\%\\
            & 20/80  & 51.64\%      & 45.59\%   & 62.74\% & +11.10\% & +17.15\% \\
            & 30/70  & 48.69\%      & 46.81\%   & 61.65\% & +12.96\% & +14.84\% \\
            & 40/60  & 46.15\%      & 46.73\%   & 58.55\% & +12.40\% & +11.82\% \\
    \midrule
\multirow{4}{*}{StanfordCars}  %& \multirow{4}{*}{55.35\%}   
& 20/176  & 54.87\% & 39.26\%  & 63.20\% & +8.33\% & +23.94\% \\
            & 30/166  & 54.10\% & 34.78\%   & 59.61\% & +5.51\% & +24.83\% \\
            & 40/156  & 52.02\% & 34.18\%   & 57.05\% & +5.03\% & +22.87\% \\
            & 50/146  & 48.13\% & 32.94\%   & 55.20\% & +7.07\% & +22.26\% \\
    \midrule
\multirow{3}{*}{UCF101} %& \multirow{3}{*}{80.64\%} 
& 10/91 & 78.60\% & 63.94\%  & {83.78\%}  & +5.18\% & +19.84\% \\
            & 20/81  & 76.34\% & 65.72\% & 83.89\% & +7.55\% & +18.17\% \\
            & 30/71  & 75.82\% & 69.53\% & 81.23\% & +5.41\% & +11.70\% \\
            % & 40  & \% & \% &           &           & \% & \% \\
    \midrule
ImageNet-1K \& %& \multirow{2}{*}{49.46\%} 
& \multirow{2}{*}{196/1000} & \multirow{2}{*}{39.74\%} & \multirow{2}{*}{28.00\%}  & \multirow{2}{*}{69.32\%} & \multirow{2}{*}{+29.58\%} & \multirow{2}{*}{+41.32\%} \\
StanfordCars & & & &\\ 
    \bottomrule
    \end{tabular}
    }
%\end{table}
\end{minipage}
\hfill
\begin{minipage}{0.43\textwidth}
%\begin{table}
    \caption{DC-CCL without the control model or the classifer finetuning} \label{acc_ablation}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccc}
    \toprule
Dataset     & Class Ratio   & w/o control model & w/o Finetuning \\
    \midrule
\multirow{4}{*}{CIFAR10}     & 1/9   & -1.89\%     & -17.30\%       \\
            & 2/8   & -10.08\%    & -9.19\%        \\
            & 3/7   & -13.29\%    & -16.06\%       \\
            & 4/6   & -19.44\%    & -14.00\%       \\
    \midrule
\multirow{4}{*}{CIFAR100}    & 10/90  & -2.71\%     & -4.42\%        \\
            & 20/80  & -5.37\%     & -6.83\%        \\
            & 30/70  & -11.90\%    & -6.21\%        \\
            & 40/60  & -14.39\%    & -4.76\%        \\
    \midrule
\multirow{4}{*}{StanfordCars} & 20/176  & -1.76\%     & -1.42\%        \\
            & 30/166  & -2.89\%     & -1.31\%        \\
            & 40/156  & -1.47\%     & -1.35\%        \\
            & 50/146  & -2.56\%     & -2.73\%        \\
    \midrule
\multirow{4}{*}{UCF101} & 10/91  & -4.48\%     & -6.62\%        \\
            & 20/81  & -13.88\%     & -25.87\%        \\
            & 30/71  & -20.25\%     & -27.95\%        \\
    \midrule
{\makecell[c]{ImageNet-1K \& \\ StanfordCars}}    & 196/1000 & -6.76\%     & -3.60\%        \\
    \bottomrule
    \end{tabular}
    }
%\end{table}
\end{minipage}
\end{table*}
\end{comment}

\begin{table*}[!t]
    \caption{DC-CCL vs. Baselines with small model vs. DC-CCL without the control model or the classifier finetuning. $\Delta_1$ and $\Delta_2$ denote the accuracy improvements of DC-CCL over Distr-S and Incr-S, respectively.}
    \label{acc_cmp_light}
    \centering
    \resizebox{1.95\columnwidth}{!}{
    \begin{tabular}{ccccccccc}
    \toprule
Dataset     & %Light-Central %& 
Class Ratio & Distr-S & Incr-S & DC-CCL & $\Delta_1$ & $\Delta_2$ & - Control Model & - Finetuning \\
    \midrule
\multirow{4}{*}{CIFAR10}  %& \multirow{4}{*}{78.84\%}    
& 1/9   & 75.26\%      & 68.99\%    & 80.60\%  & +5.34\% & \multicolumn{1}{c|}{+11.61\%} & -1.89\%     & -17.30\% \\
            & 2/8   & 70.28\%      & 66.35\%   & 80.32\% & +10.04\% & \multicolumn{1}{c|}{+13.97\%} & -10.08\%    & -9.19\% \\
            & 3/7   & 64.63\%      & 68.18\%   & 77.40\% & +12.77\% & \multicolumn{1}{c|}{+9.22\%} & -13.29\%    & -16.06\%  \\
            & 4/6   & 61.69\%      & 66.62\%   & 77.31\% & +15.62\% & \multicolumn{1}{c|}{+10.69\%} & -19.44\%    & -14.00\% \\
    \midrule
\multirow{4}{*}{CIFAR100}  %& \multirow{4}{*}{57.30\%}    
& 10/90  & 52.36\%      & 44.59\%   & 65.27\% & +12.91\% & \multicolumn{1}{c|}{+20.61\%} & -2.71\%     & -4.42\%\\
            & 20/80  & 51.64\%      & 45.59\%   & 62.74\% & +11.10\% & \multicolumn{1}{c|}{+17.15\%} & -5.37\%     & -6.83\%  \\
            & 30/70  & 48.69\%      & 46.81\%   & 61.65\% & +12.96\% & \multicolumn{1}{c|}{+14.84\%} & -11.90\%    & -6.21\% \\
            & 40/60  & 46.15\%      & 46.73\%   & 58.55\% & +12.40\% & \multicolumn{1}{c|}{+11.82\%} & -14.39\%    & -4.76\% \\
    \midrule
\multirow{4}{*}{StanfordCars}  %& \multirow{4}{*}{55.35\%}   
& 20/176  & 54.87\% & 39.26\%  & 63.20\% & +8.33\% & \multicolumn{1}{c|}{+23.94\%} & -1.76\%     & -1.42\% \\
            & 30/166  & 54.10\% & 34.78\%   & 59.61\% & +5.51\% & \multicolumn{1}{c|}{+24.83\%} & -2.89\%     & -1.31\% \\
            & 40/156  & 52.02\% & 34.18\%   & 57.05\% & +5.03\% & \multicolumn{1}{c|}{+22.87\%} & -1.47\%     & -1.35\% \\
            & 50/146  & 48.13\% & 32.94\%   & 55.20\% & +7.07\% & \multicolumn{1}{c|}{+22.26\%} & -2.56\%     & -2.73\% \\
    \midrule
\multirow{3}{*}{UCF101} %& \multirow{3}{*}{80.64\%} 
& 10/91 & 78.60\% & 63.94\%  & {83.78\%}  & +5.18\% & \multicolumn{1}{c|}{+19.84\%} & -4.48\%     & -6.62\% \\
            & 20/81  & 76.34\% & 65.72\% & 83.89\% & +7.55\% & \multicolumn{1}{c|}{+18.17\%} & -13.88\%     & -25.87\% \\
            & 30/71  & 75.82\% & 69.53\% & 81.23\% & +5.41\% & \multicolumn{1}{c|}{+11.70\%} & -20.25\%     & -27.95\% \\
            % & 40  & \% & \% &           &           & \% & \% \\
    \midrule
ImageNet-1K \& %& \multirow{2}{*}{49.46\%} 
& \multirow{2}{*}{196/1000} & \multirow{2}{*}{39.74\%} & \multirow{2}{*}{28.00\%}  & \multirow{2}{*}{69.32\%} & \multirow{2}{*}{+29.58\%} & \multicolumn{1}{c|}{\multirow{2}{*}{+41.32\%}} & \multirow{2}{*}{-6.76\%} & \multirow{2}{*}{-3.60\%} \\
StanfordCars & & & & & & \multicolumn{1}{c|}{} \\ 
    \bottomrule
    \end{tabular}
    }
\end{table*}

By comparing DC-CCL with Distr-D, we can see that their inference accuracy are close. Averagely, DC-CCL decreases the inference accuracy only by 1.16\%, compared with Distr-D. The slight accuracy drop comes from the practical infeasibility of deploying the large model on the mobile device. We also compare the size and the FLOPs (floating point operations) of the models deployed on the mobile device in Distr-D and DC-CCL. From Figure \ref{model-efficiency} and Table \ref{acc_cmp_ideal}, we can observe that (1) in DC-CCL, the sizes of the device-side models for CIFAR10, CIFAR100, StanfordCars, and UCF101 are less than 10MB. For ImageNet-1K \& StanfordCars, the mobile device take two MobileNetV3-small as the co-submodel and the control model, the total size of which is 20.93MB; and (2) compared with the light-weight device-side models in DC-CCL, the decoupled models on the mobile device in Distr-D are roughly $24\times$, $23\times$, $5\times$, $24\times$, and $5\times$ larger from parameter size, and $5\times$, $16\times$, $2\times$, $7\times$, and $13\times$ more time-consuming from FLOPs, for CIFAR10, CIFAR100, StanfordCars, UCF101, and ImageNet-1K \& StanfordCars, respectively. We further compare the communication overhead. DC-CCL needs to exchange the device-cloud co-submodel (update) in each communication round, while  Distr-D exchanges the decoupled model (update). In addition, the number of communication rounds required by DC-CCL and Distr-D are consistent. Therefore, compared with DC-CCL, Distr-D is roughly $48\times$, $46\times$, $10\times$, $48\times$, and $10\times$ more bandwidth-consuming. We also plot in Figure \ref{acc_curve} the device-cloud collaborative training process of DC-CCL over StanfordCars, which requires the most communication rounds among all the datasets. We can see that the number of communication rounds is no more than 25. The results above demonstrate that DC-CCL approaches the ideal baseline Distr-D from accuracy, enables device-cloud collaborative learning of the large vision models, and sharply reduces computation and communication overhead.  



By comparing DC-CCL with Cloud-B, DC-CCL increases the inference accuracy on average (i.e., over different settings of the class ratio) by 14.16\%, 9.12\%, 5.16\%, 12.29\%, and 9.21\%, for CIFAR10, CIFAR100, StanfordCars, UCF101, and ImageNet-1K \& StanfordCars, respectively. These results reveal the benefit of leveraging device-side samples using our design under the practical setting of no raw sample up-link and no large model down-link. 

\begin{comment}
To validate that it is necessary to learn from the on-device data except for the on-cloud data, we first compare the ideal centralized training with pure on-cloud training. 

In addition, with the practical consideration that the data are distributed on the cloud server and the mobile device, we also evaluate an ideal device-cloud collaborative learning algorithm without considering the mobile device's resource constraint.

More specially, we introduce the following baselines for comparison.

\begin{itemize}%[leftmargin=*]
    \item Aligned on-cloud training (denoted as Cloud-A) trains the shared encoder plus the cloud-side submodel, which is the same as in DC-CCL, over the on-cloud training set.
    \item Basic on-cloud training (denoted as Cloud-B) trains the base model over the on-cloud training set.
    \item Aligned centralized learning (denoted as Central-A) trains the isolated global model, which is the same as in DC-CCL, over the full training set. 
    \item Basic centralized learning (denoted as Central-B) trains the base model over the full training set.
    \item Device-cloud distributed learning (denoted as DC-DL for short) trains the isolated global model, which is the same as in DC-CCL, by the collaboration of the cloud server and the mobile device. In particular, DC-DL adopts the same data parallelism and finetuning training methods as DC-CCL, but the training is with respect to the full model. 
\end{itemize}

We plot the inference accuracy of the above baselines in Table \ref{acc_cmp_ideal}\footnote{Cloud-B and Central-B are not performed over ImageNet-StanfordCars since there is no base model in the learning task.}. As Central-A/B significantly outperform Cloud-A/B, we can draw that it is necessary to make full use of both the on-cloud and on-device data to train the global model. However, as the on-cloud and on-device data are not identically and independently distributed, it is generally impossible to achieve the inference accuracy as high as Central-A/B and DC-DL is the practical upper bound. 


However, DC-DL cannot be deployed in practice since the model size that can be supported by the mobile device is 5MB while the models used in our experiments are more than 10$\times$ of it. 
Therefore, we next validate that DC-CCL makes the device-cloud collaborative learning of the large vision models possible. 
For each learning task, we report the parameters sizes of the on-device model (i.e., the shared encoder and the co-submodel) in DC-CCL, the full global model, and the base model in Figure \ref{model-efficiency}. We also compare their numbers of FLOPs (floating-point operations), when the batch size is equal to 1.

From Figure \ref{model-efficiency}, we can observe that: (1) For the first three learning tasks, which train the mainstream vision models from scratch, all the on-device parameters sizes of DC-CCL are less than 5MB, and the parameters sizes of the base models are 16-128MB, which are far beyond the mobile device's capacity. For the last learning task, which uses a pre-trained EfficientNet as the cloud-side model, a pre-trained MobileNet as the co-submodel, and another pre-trained MobileNet as the control model, DC-CCL reduce the on-device parameters size from 75MB (the size of the EfficientNet) to 20MB (the size of two MobileNets). (2) DC-CCL significantly reduce the number of FLOPs in four learning tasks. More specially, DC-CCL reduces the number of on-device FLOPs by 6-19 times compared with that of the base models. 

In addition, we evaluate DC-CCL and find that it maintains high performance. We report its inference accuracy in Table \ref{acc_cmp_ideal}. From Table \ref{acc_cmp_ideal}, we can observe that DC-CCL significantly outperforms pure on-cloud learning and almost achieves the practical upper bound of the inference accuracy: (1) DC-CCL increases the averaged inference accuracy (with respect to different settings in a learning task) by 8.70\%-14.10\% compared with Cloud-A and by 5.16\%-14.16\% compared with Cloud-B; (2) DC-CCL outperforms the ideal DC-DL in two learning tasks while DC-DL performs better in the other two tasks. Averagely, DC-CCL decreases the inference accuracy only by 0.95\% compared with DC-DL. 
\end{comment}

\subsection{DC-CCL vs. Baselines with Small Model}\label{sec:eval:small}

    
Considering the fact that the mobile device cannot hold a large model, a practical and trivial idea is using an affordable small model, typically, the shared encoder plus the device-cloud co-submodel in DC-CCL. We introduce two baselines:
\begin{itemize}
    \item {\bf Distr-S}, which differs from Distr-D only in that the mobile device and the cloud leverage the shared encoder plus the co-submodel rather than the large decoupled model; 
    \item {\bf Incr-S}, which trains the shared encoder plus the co-submodel in an incremental learning way, first over the cloud-side samples and then over the device-side samples, and finally applies classifier finetuning.
\end{itemize}
We report in Table \ref{acc_cmp_light} the accuracy of the baselines and DC-CCL. From Table \ref{acc_cmp_light}, we can observe that compared with Distr-S, DC-CCL improves the average accuracy by {10.94\%, 12.34\%, 6.49\%, 6.05\%, and 29.58\%}; and compared with Incr-S, DC-CCL improves the average accuracy by {11.37\%, 16.12\%, 23.48\%, 16.58\%, and 41.32\%}, for CIFAR10, CIFAR100, StanfordCars, UCF101, ImageNet-1K \& StanfordCars, respectively. We can draw that it is necessary to maintain a high model capacity in device-cloud collaborative learning. 

%DC-CCL significantly outperforms Distr-S and Incr-S. In particular,


\begin{comment}
While the base model or the full global model are out of the mobile device's capacity, a naive approach to enable collaborative learning is using a light-weight model instead. For the fairness of comparison, we use the on-device model (i.e., the shared encoder plus the co-submodel) as in DC-CCL and introduce the following device-cloud collaborative learning baselines:

\begin{itemize}[leftmargin=*]
    \item Light device-cloud distributed learning (denoted as Light-DC-DL for short) trains the on-device model by the collaboration of the cloud server and the mobile device. It adopts the same data parallelism and finetuning training methods as DC-CCL and DC-DL, but the training is with respect to the on-device model. 
    \item On-device incremental learning (denoted as Device-I) first pre-trains the on-device model over the on-cloud training set, and then trains it over the on-device training set and a small subset of the on-cloud training set, and finally finetunes the classifier over the full training set. 
    \item Light centralized learning (denoted as Light-Central) trains the on-device model over the full training set. 
\end{itemize}

We record the inference accuracy the baselines and DC-CCL in Table \ref{acc_cmp_light}. From Table \ref{acc_cmp_light}, we can observe that DC-CCL significantly outperforms the baselines. More specially, DC-CCL increases the averaged inference accuracy (with respect to different settings in a learning task) by 6.49\%-29.58\% compared with Light-DC-DL, by 11.37\%-41.32\% compared with Device-I, and by 0.07\%-19.86\% compared with Light-Central. The evaluation results demonstrate that it is necessary to use DC-CCL to train a large vision model in practice.
\end{comment}

\subsection{Ablation Study}\label{sec:eval:abl}

To validate the necessity of each ingredient in DC-CCL, we first remove the control model. Specifically, after training the shared low-level encoder and the cloud-side submodel over the cloud-side training set, these parameters are frozen, and only the device-cloud co-submodel is trained over the full training set. We also remove the classifier finetuning step from DC-CCL to reveal its necessity. As shown in Table \ref{acc_cmp_light}, without the control model, the average accuracy drops by {11.18\%, 8.59\%, 2.17\%, 12.87\%, and 6.76\%}; and without the classifier finetuning, the average accuracy drops by {14.14\%, 5.56\%, 1.70\%, 20.15\%, and 3.60\%}, for CIFAR10, CIFAR100, StanfordCars, UCF101, ImageNet-1K \& StanfordCars, respectively. The  results above validate the compactness of DC-CCL. 




\subsection{Scaling of Device-Side Model Size}\label{sec:eval:device}

\begin{figure}[!t]
    \centering
    \subfigure[CIFAR10]{
    \includegraphics[width=0.472\columnwidth]{.//images//cifar10_sizes.pdf}
    }
    \subfigure[CIFAR100]{
    \includegraphics[width=0.472\columnwidth]{.//images//cifar100_sizes.pdf}
    }
    \caption{Accuracy of DC-CCL with the varying device-side model size over CIFAR10 and CIFAR100.}\label{device-modelsize}
\end{figure}

We finally evaluate how scaling the size of the device-side model (i.e., the shared encoder, the device-cloud co-submodel, and the control model) will affect the performance of DC-CCL. In particular, we keep the shared encoder and the control model by default and vary the number filters in the device-cloud co-submodel, accounting for $\alpha_{co}$ of the number of filters in the base model. We let $\alpha_{co}$ decrease from the default setting $\frac{1}{8}$ to $\frac{1}{16}$ and increase to $\frac{1}{8}$, $\frac{1}{4}$, and $\frac{3}{8}$, such that the decoupled model is no larger than the base model (i.e., $\alpha_{co}^2 + \alpha_{cl}^2 \leq \frac{3}{8}^2 + \frac{7}{8}^2 < 1$). We plot in Figure \ref{device-modelsize} the accuracy of DC-CCL and the device-side model size. One key observation is that compared with the default setting $\alpha_{co} = \frac{1}{8}$, DC-CCL with $\alpha_{co}=\frac{1}{16}$ decreases the device-side model size by {42\% and 36\%}, while the accuracy drops only by 2.13\% and 2.57\% on average, for CIFAR10 and CIFAR100, respectively. The second key observation that with a larger device-side model, the accuracy of DC-CCL will improves until reaching a critical point. Specifically, with $\alpha_{co}=\frac{1}{4}$, the device-side model size increases by $1.41\times$ and $1.43\times$, and the accuracy improves by 0.77\% and 1.17\% on average, for CIFAR10 and CIFAR100, respectively. Therefore, DC-CCL allows to adjust the size of the device-side model according to available resources, thereby balancing accuracy and efficiency.


\begin{comment}
We evaluate the impact of the co-submodel's size to DC-CCL. In the experiments, we takes a varying number of filters in each network layer to scale up or scale down the co-submodel's size. 
More specially, we set $\alpha_{co}$ to $\frac{1}{16}$, $\frac{1}{8}$, $\frac{1}{4}$, and $\frac{3}{8}$. We note that we always keep the parameters size of the global model less than that of the base model, i.e., $\alpha_{cl}^2+\alpha_{co}^2<\frac{7}{8}^2+\frac{3}{8}^2<1$.
We record the inference accuracy over CIFAR10 and CIFAR100 and the corresponding on-device parameters size in Figure \ref{device-modelsize}. From Figure \ref{device-modelsize}, we can observe that the averaged inference accuracy of DC-CCL with $\alpha_{co}=\frac{1}{16}$ only decreased by 2.13\% and 2.57\% over CIFAR10 and CIFAR100, respectively, compared with that of DC-CCL with the default $\alpha_{co}=\frac{1}{8}$. In addition, the averaged inference accuracy is increased when we use larger co-submodel. 
The evaluation result demonstrates that (1) with a smaller co-submodel, the inference accuracy of DC-CCL is decreased but still acceptable; and (2) with a larger co-submodel, DC-CCL can achieve a higher inference accuracy.
Therefore, we could adjust the size of the co-submodel based on the available capacity of the mobile device.
\end{comment}