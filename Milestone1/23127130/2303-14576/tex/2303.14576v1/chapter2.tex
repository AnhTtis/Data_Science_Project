\chapter{Related Work}

\section{Automatic Question Generation}
Automatic question generation (QG), first studied by Wolfe \cite{wolfe1976automatic} as a means to aid independent study, has since attracted increasing attentions in two lines of methodologies: transformative and generative. 

\subsection{Transformative Methods}
Transformative methods transform key phrases from a given single declarative sentence into factual questions.
Existing methods are rule-based on syntax, semantics, or templates.

Syntactic-based methods follow the same basic strategy: Parse sentences using a syntactic parser to identify key phrases and transform a sentence to a question based on syntactic rules. 
These include methods to identify key phrases from input sentences and use syntactic rules for different types of questions \cite{varga2010wlv}, generate questions and answers using a syntactic parser, a POS tagger, and an NE analyzer \cite{ali2010automation}, transform a sentence into a set of questions using a series of domain-independent rules \cite{danon2017syntactic}, and generate questions using relative pronouns and adverbs from complex English sentences \cite{khullar2018automatic}. 

Semantic-based methods create questions using predicate-argument structures and semantic roles \cite{mannem2010question}, semantic pattern recognition  \cite{mazidi2014linguistic}, subtopics based on Latent Dirichlet Allocation \cite{chali2015towards}, or
%generate factual questions using 
semantic-role labeling %as the main form of text analysis 
\cite{flor2018semantic}.

These methods are similar. The only difference is that semantic-based methods use semantic parsing while syntactic-based methods use syntactic parsing to determine which specific words or phrases should be asked. In a language with many syntactic and semantic exceptions, such as English, these methods would require substantial manual labor to construct rules.

Template-based methods are for special-purpose applications with built-in templates. Research in this line devises a Natural Language Generation Markup Language (NLGML) \cite{cai2006nlgml}; 
uses a phrase structure parser to parse text and construct questions using enhanced XML \cite{rus2007experiments};
devise a self-questioning strategy to help children generate questions from narrative fiction \cite{mostow2009generating};
use informational text to enhance the self-questioning strategy \cite{chen2009aist};
apply pattern matching, variables, and templates to transform source sentences into questions similar to NLGML \cite{wyse2009generating};
defines a question template as pre-defined text with placeholder variables to be replaced with content from the source text \cite{lindberg2013automatic}; 
or incorporates semantic-based methods into a template-based method to support online learning \cite{lindberg2013generating}.


\subsection{Generative Methods}
% \paragraph{Generative methods}
%Generative methods are neural networks trained
%on large datasets, which tend to just learn how to generate questions
%without concerning what the correct answers are.
%or just learn how to provide an answer to a question.

%Recent advancements of neural-network methodologies have shed new light on
%generative methods.
%For example, the attention mechanism \cite{luong-etal-2015-effective} is used to determine what content in a sentence should be asked, and the sequence-to-sequence  \cite{bahdanau2014neural,cho-etal-2014-learning} and the long short-term memory  \cite{Sak2014LongSM}  mechanisms are used to generate each word in a question (see, e.g., \cite{du-etal-2017-learning,duan-etal-2017-question,Harrison_2018,sachan-xing-2018-self}).
%These models, however, only deal with question generations without generating correct answers. % to a randomly generated question. 
%Moreover, training these models require a dataset comprising over 100K questions.
%
%To address the problem of generating questions without answers, researchers have explored ways to encode a passage (a sentence or multiple sentences) and an answer word (or a phrase) as input, and determine what questions are to be generated for a given answer \cite{10.1007/978-3-319-73618-1_56,zhao-etal-2018-paragraph,song-etal-2018-leveraging}. Kim et al. \cite{Kim_2019} pointed out that these models could generate a number of answer-revealing questions (namely, questions contain in them the corresponding answers). They then devised a new method by encoding answers separately, at the expense of having substantially more parameters. Their experiments show that the BLEU-4 \cite{10.3115/1073083.1073135}, 
%METEOR \cite{banerjee-lavie-2005-meteor}, and ROUGE-L \cite{lin-2004-rouge} scores on the questions generated are, respectively,
%16.2, 19.92, 43.96, which are 3 to 4 points higher than the earlier results on the same dataset %of 12.98, 16.22, 39.75 
%\cite{du-etal-2017-learning}.
%%Such low accuracies are still a long way to go to meet the requirement for our application, and 
%On top of low accuracy, it is also unknown whether the questions generated are grammatically correct because these measures do not measure grammatical correctness.

Recent advances of neural-network research provide new tools to build generative models.
For example, the attention mechanism can help determine what content in a sentence should be asked \cite{luong-etal-2015-effective}, and the sequence-to-sequence  \cite{bahdanau2014neural,cho-etal-2014-learning} and the long short-term memory  \cite{Sak2014LongSM}  mechanisms are used to generate words to form a question (see, e.g., \cite{du-etal-2017-learning,duan-etal-2017-question,Harrison_2018,sachan-xing-2018-self}).
These models generate questions without the corresponding correct answers. 
%Moreover, training these models require a dataset comprising over 100K questions.
To address this issue, % the problem of generating questions without answers, 
researchers have explored ways to encode a passage (a sentence or multiple sentences) and an answer word (or a phrase) as input, and determine what questions are to be generated for a given answer \cite{10.1007/978-3-319-73618-1_56,zhao-etal-2018-paragraph,song-etal-2018-leveraging}. 
However, as pointed out by
Kim et al. \cite{Kim_2019}, these methods could generate answer-revealing questions, namely, questions contain in them the corresponding answers. They then devised a new method by encoding answers separately, at the expense of learning substantially more parameters. 

More recently, researchers have explored how to use pretrained transformers to generate answer-aware questions \cite{dong2019unified,Zhang2019AddressingSD,Zhou2019QuestiontypeDQ,qi-etal-2020-prophetnet,su-etal-2020-multi,10.1145/3442381.3449892}.
For example, Kettip et al. \cite{Kriangchaivech190905017} presented an architecture for a transformer to generate questions. Rather than fully encoding the context and answers as they appear in the dataset, they applied certain transformations such as the change of named entities both on the context and the answer. 
Lopez et al. \cite{Lopez2020TransformerbasedEQ} finetuned the pretrained GPT-2 \cite{radford2019language} transformer without using any additional complex components or features to enhance its performance.
Chen \cite{Chen2020ReinforcementLB} described a fully transformer-based reinforcement learning generator evaluator architecture to generate questions.

The recent introduction of T5 has escalated NLP research in a number of ways.
%, which achieves the state-of-the-art results on several NLP task trained on a newer and larger text corpus. % called C4
%\subsection{An Overview of T5}
%
%We present a deep-learning-based end-to-end question generation model to utilize the power of Text-to-Text Transfer Transformer (T5) \cite{raffel2020exploring}. 
T5 is a encoder-decoder text-to-text transformer using the teacher forcing method on a wide variety of NLP tasks, including text classification, question answering, machine translation, and abstractive summarization. Unlike other transformer models (e.g. GPT-2 \cite{radford2019language}) that take in text data after converting them to corresponding numerical embeddings, T5 handles each task by taking in data in the form of text and producing text outputs. 
% Fig \ref{fig:1} depicts the architecture of T5 \cite{raffel2020exploring}. 
%The model has an encoder-decoder based transformer architecture \cite{raffel2020exploring}. 

% \begin{figure}[h]
%   \centering
%   \includegraphics[width= \linewidth]{T5Architecture}
%   \caption{T5 Architecture}
%   \label{fig:1}
% %   \Description{}
% \end{figure}


% In particular,
% the encoder consists of a stack of identical layers. Every layer is composed of two sub-layers. The first sub-layer of each encoder layer is a multi-head self-attention mechanism. The second sub-layer  is a fully connected feed-forward network. Residual connections are employed around these sub-layers, each followed by a normalization layer. 
% %
% The decoder also consists of a stack of identical layers. In addition to the two sub-layers already present in the encoder layer, the third sub-layer performs multi-head attention on the output received by the encoder stack. Residual connections are employed around these sub-layers, such as those in the encoder, each followed by a normalization layer. 

%
%
%most recent innovation on question generation train a %pretrained %the Text-to-Text Transfer Transformer (
%T5 model \cite{raffel2020exploring} %, it pretrained upon 
%on a newer and larger text corpus called C4, which achieves the state-of-the-art results on several NLP tasks.
Taking the advantage of pretrained T5, 
Lidiya et al. \cite{mixqg.2110.08175} combined nine question-answering datasets to finetune a single T5 model and evaluated generated questions using a new semantic measure called BERTScore \cite{bert-score}.
Their method achieves so far the best results. % state-of-art result.
We present a finetuned T5 model on a
single SQuAD dataset %with preprocessing and postprocessing pipelines 
to produce better results. %than the Lidiya's approach.



\section{Automatic Distractor Generation}
Methods of generating adequate distractors for MCQs are typically following two directions: domain-specific knowledge bases and semantic similarity \cite{pho-etal-2014-multiple,CH2018Automatic}.

Methods in the first direction are focused on lexical information 
% synonym, hyponym and hypernym
have used part-of-speech(POS) tags, word frequency, WordNet, domain ontology, distributional hypothesis, and pattern matching, to find the target word's synonym, hyponym and hypernym as distractor candidates. \cite{10.3115/1118894.1118897,10.1007/978-3-642-28885-2_19,Susanti2015AutomaticGO}
% Liu et al \cite{liu-etal-2005-applications} Collecting similar candidates of the target word in terms of their frequency and dictionary-based collocation.

Methods in the second direction %semantic similarity methods 
analyze the semantic similarity of the target word using Word2Vec model for generating distractor candidates \cite{Jiang2017DistractorGF,Susanti2018AutomaticDG}. 

However, it is difficult to use Word2Vec or other word-embedding methods to find adequate distractors for polysemous answer words.
Moreover, previous efforts have focused on finding some forms of distractors, instead of making them look more distracting. This paper is an attempt to tackle these issues.