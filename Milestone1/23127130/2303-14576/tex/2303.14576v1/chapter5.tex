\chapter{Distractor Generation}

Our distractor generation method takes the original article and the answer as input, and generates distractors as output. Figure~\ref{fig:dgf} depicts the data flow of our method.

\begin{figure}[!h]
  \centering
  \includegraphics[width= \linewidth]{DG-Figure-2}
  \caption{Distractor generation flowchart}
  \label{fig:dgf}
%   \Description{}
\end{figure}

Answers in QAPs are classified into two kinds. The first kind consists of just a single target word while the second kind consists of multiple target words. The latter is the case when  the
answer is a sentence or a sentence segment. 

For an answer of the first kind, if it is a type-1 or type-2 target, we use the methods
described in Section \ref{sec:distractors2} to generate three distractors; if it is a type-3 target, we use
the method described in Section \ref{sec:distractors3} to generate distractor candidates.
If there are at least three candidates, then select three candidates
with the highest ranking scores. 
%If there are less than three candidates, then we may fail to
%generate an adequate MCQ for this QAP.

For an answer of the second kind, for each type of a target word contained in it, we use the methods described in both Sections \ref{sec:distractors2} and
\ref{sec:distractors3} to generate distractors for target words in a fixed ordered preference of subjects, objects, adjectives for subjects, adjectives for objects,
predicates, adverbs, which can be obtained by semantic-role labeling.
Target words are replaced
according to the following preference:
%
%If there are more than three distractors, then select
%three distractors according to the following preference:
type-1 temporal, type-1 numerical value, type-2 person, type-2 location,
type-2 organization, type-3 noun (phrasal noun), type-3 adjective, type-3 verb (phrasal verb), and type-3 adverb. 

If the number of distractors for a given preference is less than three, then we generate extra distractors for a target word in the next preference. If all preference is gone through we still need more distractors, we could extend the selection threshold values to allow more candidates to be selected.

\section{Distractors for Type-1 and Type-2 Targets}
\label{sec:distractors2}

If a type-1 target is a point in time, a time range, a numerical number, an ordinal number, or anything that can be converted to a numerical number or an ordinal number (e.g. Friday may be converted to 5), which can be
recognized by regular expressions based on a POS tagger, then we devise several algorithms to alter time and number, and randomly select one of these algorithms when generating distractors. For example, we may  increase or decrease the answer value by one or two units, change the answer value at random from a small range of values around the answer,
or simply change the answer value at random. If a numerical value or an ordinal number
is converted from a word, then the result is converted back to the same form. For example,
suppose that the target word is ``Friday", which is converted to a number 5. If the distractor
is a number 4, then it is converted to Thursday. 

If a type-2 target is a person, then we first look for different person names that appear in the article using an NE tagger to identify them, and then randomly choose a name as a distractor. 
If there are no other names in the article, 
then we use Synonyms (http://www.synonyms.com) or a domain knowledge base on notable people we constructed to find a distractor. 
If a type-2 target is a location or an organization, we find a distractor in the same way by first
looking for other locations or organizations in the article, and then using
Synonyns and domain knowledge bases to look for them if they cannot be found in
the article.
For example, If the target word is a city, then a distractor should also  be city that is "closely" related to the target word. Distractors to the answer word "New York" should be cities in the same league, such as ``Boston", ``Philadelphia", and ``Chicago".  

\section{Distractors for Type-3 Targets}
\label{sec:distractors3}
\noindent
%For a given answer in a QAP, we may choose a (phrasal) noun, (phrasal) verb, adjective, or adverb contained in it in a fixed order (e.g., from right to left) as a sequence of target words. 
For a type-3 target, %which could be a (phrasal) noun or an adjective,
we find distractor candidates 
using word embeddings %antonyms,
%Word2vec \cite{10.5555/2999792.2999959} 
with similarity in a threshold interval (e.g.,[0.6,0.85]) so that
a candidate is not too close nor too different from the correct answer
and %then find synonyms and %Antonyms, and 
hypernyms using WordNet \cite{10.1145/219717.219748}. 
Note that a similarity interval of $[0.6,0.85]$ for word embeddings often
include antonyms of the target word, and we can use WordNet or an online dictionary to determine
antonyms.

Not all distractor candidates are suitable. Thus, we first filter out unsuitable candidates as follows:
\begin{enumerate}
\item Remove distractor candidates that contain the target word, for it may be
too close to the correct answer. For example, if 
``breaking news" is
a generated distractor candidate for the target word "news", then it is
removed from the candidate list since it contains the target word.
%the "Breaking news" need to be excluded.
% 1.	排除distractor  candidates中包含target word的candidates，例如 breaking news 中包含news，所以排除breaking news

\item Remove distractor candidates that have the same prefix of the target word with edit distance less than three, for such candidates may often be misspelled words from the target word.
For example, suppose the target word is "knowledge", then 
Word2vec may return a misspelled candidate "knowladge" with a high similarity,
which should be removed.
% 2.	排除前缀prefix相同且edit distance 小于3的单词
\end{enumerate}

We then rank each remaining candidate using the following measure:
\begin{enumerate}
\item Compute the Word2vec cosine similarity score $S_v$ for each distractor candidate $w_c$ with the 
target word $w_t$. Namely, $$S_v = \mbox{sim}(v(w_c),v(w_t)),$$
where $v(w)$ denotes a word embedding of $w$.
%If the candidate $w_c$ is an acronym of the target word, then let
%$$S_v = \frac{1}{\mbox{sim}(w_c,w_t)}.$$
%If no distractor candidates or an insufficient number of distractor candidates are found in a Word2vec dataset, then we use the WordNet Wu-Palmer (WUP) \cite{10.3115/981732.981751} score as the similarity score.
% 3.	计算每个 distractor candidates 和 target word 的word2vec similarity score Sv， 如果word2vec中查询不到这个candidate，则取 WUP score作为 similarity score

\item Compute the WordNet WUP score \cite{10.3115/981732.981751} $S_n$ for each distractor candidate with the target word. If the distractor candidate cannot be found in the WordNet dataset, set the WUP score to 0.1
for the following reason: If a word with a high score of word-embedding similarity to the target word but does not exist in the WordNet dataset, then it is highly likely that the word is misspelled, and so its ranking score should be reduced.
% 4.	计算每个distractor candidates 和 target word 的 WordNet Wu-Palmer (WUP) score Sn，如果WordNet中查询不到这个candidate，则取 WUP socre 为 0.1， 因为在word2vec中similarity score较高但在WordNet中不存在的单词，很大的可能性是拼写错误的单词，应该降低他的排序。例如knowledge和knowladge(e拼错成了a)，newspaper和newpaper。

\item Compute the edit distance score $S_d$ of each distractor candidate with target word by the
following formula:
\begin{equation*}\label{eq1}
    S_d = 1 - \frac{1}{1 + e^E},
\end{equation*}
where $E$ is the edit distance. Thus, a lager edit distance $E$ results in a smaller score $S_d$.

\item Compute the final ranking score $R$ for each distractor candidate $w_c$ with respect to the target word $w_t$ by
\begin{align*}\label{eq2}
R'(w_c,w_t) &= \left\{
\begin{array}{ll}
\frac{1}{4}(2S_v + S_n + S_d), &\mbox{if $w_c$ is an} \\
							&\mbox{antonym of $w_t$}, \\
\frac{1}{3}(S_v + S_n+S_d), & \mbox{otherwise},
\end{array}\right. \\
R(w_c,w_t) &=  - R'(w_c,w_t)\log R'(w_c,w_t).\\
\end{align*}
Note that $S_v, S_n, S_d$ are each between 0 and 1, and so
$R'(w_c,w_t)$ is between 0 and 1, which implies that $- \log R'(w_c,w_t) > 0$.
%Also note that $\log R'(w_c,w_t)$ results in a larger value of 
Also note that we give more weight to antonyms. 
% 6.	计算每个distractor candidates 和 target word 的Ranking score，
% R = Sv * Sn * Sd

\end{enumerate}



\section{Running Samples}
Given below are a few adequate MCQs with automatically generated distractors by our method:

\subsubsection*{Example 1}
Question: What does no man like to acknowledge? (SAT practice test 2 article 1)

Correct answer: that he has made a mistake in the choice of his profession.

Distractors:
\begin{enumerate}
\item that he has made a mistake in the choice of his association.
\item that he has made a mistake in the choice of his engineering.
\item that he has made a mistake in the way of his profession.
\end{enumerate}

\subsubsection*{Example 2}

When should ethics apply? (SAT practice test 2 article 2)

Correct answer: 
when someone makes an economic decision.

Distractors:
\begin{enumerate}
%\item when someone makes an economic consideration.
\item when someone makes an economic request.
\item when someone makes an economic proposition.
\item when someone makes a political decision.
\end{enumerate}

\subsubsection*{Example 3}

Question: 
What did Chie hear? (SAT practice test 1 article 1)

Correct answer: 
her soft scuttling footsteps, the creak of the door.

Distractors:
\begin{enumerate}
\item her soft scuttling footsteps, the creak of the driveway.
\item her soft scuttling footsteps, the creak of the stairwell.
\item her soft scuttling footsteps, the knock of the door.
\end{enumerate}

\subsubsection*{Example 4}
Question: 
Who might duplicate itself? (SAT practice test 1 article 3)

Correct answer: 
the deoxyribonucleic acid molecule.

Distractors:
\begin{enumerate}
\item the deoxyribonucleic acid coenzyme.
\item the deoxyribonucleic acid polymer.
\item the deoxyribonucleic acid trimer
\end{enumerate}


\begin{comment}
Question: 
What is a very long chain, the backbone of which consists of a regular alternation of sugar and phosphate groups? (SAT practice test 1 article 3)

Correct answer: 
The molecule.

Distractors:
\begin{enumerate}
\item The coenzyme.
\item The polymer.
\item The trimer.
\end{enumerate}
\end{comment}

\subsubsection*{Example 5}

Question: 
When does Deep Space Industries of Virginia hope to be harvesting metals from asteroids?
(SAT practice test 1 article 5)

Correct answer: 
by 2020.

Distractors:
\begin{enumerate}
\item by 2021.
\item by 2030.
\item by 2019.
\end{enumerate}

%The following is an example with one distractor without an adequate distracting effect.

\subsubsection*{Example 6}

%This example contains a distractor without distracting effect.

Question: 
What did a British study of the way women search for medical information online indicate?
(SAT practice test 2 article 3)

Correct answer: 
An experienced Internet user can, at least in some cases, assess the trustworthiness and probable value of a Web page in a matter of seconds.

Distractors:
\begin{enumerate}
\item An experienced Supernet user can, at least in some cases, assess the trustworthiness and probable value of a Web page in a matter of seconds.
\item An experienced CogNet user can, at least in some cases, assess the trustworthiness and probable value of a Web page in a matter of seconds.
\item An inexperienced Internet user can, at least in some cases, assess the trustworthiness and probable value of a Web page in a matter of seconds.
% (This distractor does not have
%a desired distracting effect as ``SUV user" is clearly not related to the context.)
\end{enumerate}

\subsubsection*{Example 7}

What does a woman know better than a man? (SAT test 2 article 4)

Correct answer: 
the cost of life.

Distractors:
\begin{enumerate}
\item the cost of happiness.
\item the cost of experience.
\item the risk of life.
\end{enumerate}

\subsubsection*{Example 8}

This example presents a distractor without sufficient distraction.

Question: 
What are subject to egocentrism, social projection, and multiple attribution errors?

Correct answer: 
their insights.

Distractors:
\begin{enumerate}
\item their perspectives.
\item their findings.
\item their valuables. 
\end{enumerate}
The last distractor can be spotted wrong by just looking at the question: It is easy to tell
that it is out of place without the need to read the article.



\section{Evaluations}

We implemented our method using the latest versions of POS tagger\footnote{https://nlp.stanford.edu/software/tagger.shtml}, NE tagger \cite{peters2017semi},
semantic-role labeling \cite{shi2019simple}, and fastText \cite{mikolov2018advances}.
We used the US SAT  practice reading tests\footnote{https://collegereadiness.collegeboard.org/sat/practice/full-length-practice-tests} as a dataset for evaluations. 
There are a total of eight SAT practice reading tests, each consisting of five articles for a total of 40 articles.
Each article in the SAT practice reading tests consists of around 25 sentences and we
generated about 10 QAPs from each article. To evaluate our distractor generation algorithm, we selected independently at random slightly over 100 QAPs. After removing a smaller number of QAPs with
pronouns as target words, we have a total of 101 QAPs for evaluations.

We generated 3 distractors for each QAP for a total of 303 distractors, and evaluated distractors 
based on the following criteria:
\begin{enumerate}
\item A distractor is \textsl{adequate} if it is grammatically correct and relevant to the question with distracting effects.
\item An MCQ is \textsl{adequate} if each of the three distractors is adequate.
\item An MCQ is \textsl{acceptable} if one or two  distractors are adequate. 
%\item \textsl{unacceptable} if none of the distractors is adequate.
\end{enumerate}
We define two levels of distracting effects: (1) \textsl{sufficient distraction}: It requires 
an understanding of the underlying article to choose the correct answer;
(2) \textsl{distraction}: It only requires an understanding of the underlying question to choose
the correct answer.
A distractor has no distracting effect if it can be determined wrong by just looking at
the distractor itself.

%
%as following three points:
%\begin{itemize}
%    \item The distractor is grammatically consistent with the context.
%    \item The distractor is relevant to the question with distracting effects.
%    \item The distractor provides enough confusion.
%\end{itemize}

\begin{comment}
\begin{table}[h]
\caption{Evaluation result}
\label{tab:evaluation-table} \centering
\begin{tabular}{|l|l|l|l|l|}
    \hline
                             & number of distractor & percentage of distractor & \\
    \hline                         
    Grammatically acceptable &   &   & \\
    \hline
    Relevantly acceptable    &   &   & \\
    \hline
    Confusion level          &   &   & \\
    \hline
\end{tabular}
\end{table}
\end{comment}

Evaluations were carried out by humans and the results are listed below:
\begin{enumerate}
\item All distractors generated by our method are grammatically correct.
\item 98\% distractors (296 out of 303) are relevant to the QAP with distraction.
\item 96\% distractors (291 out of 303) provide sufficient distraction.
\item 84\% MCQs are adequate.
\item All MCQs are acceptable (i.e., with at least one adequate distractor).
\end{enumerate}

\begin{comment}
0  grammatical issue
7  semantic issue
12  grammatical and semantic correct but no enough confusion

303 / 100% distractor is grammatically correct
296 / 98% distractor is relevant to the question with distracting effects
291 / 96% distractor provides enough confusion

16 / 16% MCQ acceptable if one or two distractors are adequate
0 MCQ unacceptable if none of the distractors is adequate.
\end{comment}


\chapter{Conclusion}

We presented 3 methods for generating MCQs.
\begin{enumerate}

\item  TP3, a deep-learning-based end-to-end Transformer with preprocessing and postprocessing pipelines, the large model achieved over 95\% of accuracy on Gaokao-EN dataset.

\item  MetaQA, a meta-sequence-learning-based scheme, which achieved 97\% of accuracy on SAT dataset.

\item  A distractor generation method, a combination of various NLP tools and algorithms, which also achieved a high accuracy.

\end{enumerate}

The TP3 works very well on unseen data, it can generate a large number of QAPs. 
On the other hand, the MetaQA can generate QAPs based on the meta sequences it has learned, but rarely generate inadequate question, which good complement the TP3.

All generated adequate questions are grammatically correct, and the corresponding answer is correct to the question.

The generated adequate distractors are the incorrect answers to the question, relate to the correct answer, and provide enough degree of distraction.

Overall, our methods for generating MCQs achieved over 95\% of accuracy.

