\chapter{Introduction}

In an effort to build an online learning tool for helping students improve reading comprehension, it calls for a system to automatically generate adequate multiple-choice questions (MCQs) to assess student's understanding of a given article.
%'s main points. % from an arbitrary document. 
An article's main points include \textsl{direct} and \textsl{derived} points. 
A direct point is expressed in a declarative sentence. A derived point is inferred from multiple direct points, which could be a causal relation between them, an aggregation over them, or a conclusion drawn from them.
An MCQ typically consists of a question-answer pairs (QAPs) and a fixed number of distractors. 

% We study automatic generation of question-answer pairs (QAPs) with
Automatic generation of adequate MCQs can be divided into two tasks: (1) automatic generation of QAPs;  (2) automatic generation of distractors. Part of this dissertation is built on my prior work on QAP and distractor generation \cite{Zhang-Sun-Chen-Wang-2020,metaseq-2020,zhang2022downstream}.


\section{Automatic Question Generation}
We study automatic generation of QAPs with an emphasis on grammatical correctness of the questions and the suitability of the answers. 
By grammatical correctness we mean that the questions being generated are syntactically and semantically correct and conform to what a native speaker would say.
% By suitability we mean that the answers being generated consist of one correct answer and multiple adequate distractors.
% We refer to such QAPs as \textsl{adequate} QAPs.
% Other tasks of generating MCQs not addressed in this paper are how to provide adequate distractors for an answer.


Existing methods on QAP generation are based on handcrafted features or neural networks.
While they have met with certain success from different perspectives, the grand challenge of generating adequate QAPs still remains. 

%We present a new approach to tackling this challenge.
We present two approach to tackling this challenge from two different perspectives. 
(1) A deep-learning-based end-to-end Transformer with Preprocessing and Postprocessing Pipelines (TP3) for generating adequate QAPs, 
and (2) A sequence-learning-based MetaQG for generating adequate QAPs via meta sequence representations of sentences.

TP3 utilizes the power of Text-to-Text Transfer (T5) Transformer model.
It is a downstream task on a pretrained T5 that is finetuned on a QAP dataset, with a preprocessing pipeline to select appropriate answers and a postprocessing pipeline to filter undesirable questions.

While the TP3 system performs very well on unseen data and generates well-formed and grammatically correct questions from the given answer and context, it may still generate a few types of incoherent questions. For example, asking the type of verbs or asking clauses that express reason or purpose may generate incoherent QAPs. But these types of questions can be easily generated by MetaQA.
% -- a meta sequence learning based question generation system.

A meta sequence is a sequence of vectors comprising semantic and syntactic tags. 
In particular, we use a sequence of vectors to represent a sentence, where each vector consists of a semantic-role (SR) tag, a part-of-speech (POS) tag, and other syntactic and semantic tags, and we refer to such a sequence as a \textsl{meta sequence}.
We then present a scheme called MetaQA to learn meta sequences of declarative sentences and the corresponding interrogative sentences from a training dataset. % consisting of such sentences.
Combining and removing redundant meta sequences yields a set called MSDIP (Meta-Sequence-Declarative-Interrogative Pairs), with each element being a pair of an MD and corresponding MI(s), where MD and MI stand for, respectively, a meta sequence for a declarative sentence and for an interrogative sentence.
A trained MetaQA model generates QAPs for a given declarative sentence $s$ as follows:
Generate a meta sequence for $s$, find a best-matched MD from MSDIP, generates meta sequences for interrogative sentences according to the corresponding MIs and the meta sequence of $s$, identifies the meta-sequence answer to each MI, and coverts them back to text to form a QAP.

We implement MetaQA for the English language using SR, POS, and
NE (named-entity) tags.
%, and
%allow fuzzy representation when an existing tool fails to produce a tag for
%a given word. 
We then train MetaQA using a moderate initial dataset and show that MetaQA generates efficiently a large number of adequate QAPs with an accuracy of 97\% on the official SAT practice reading tests.
These tests contain a large number of declarative sentences in different patterns, and there is no match in the initial MSDIP for some of these sentences. After learning interrogative for some of these sentences, MetaQA successfully generate many more adequate QAPs.

\section{Automatic Distractor Generation}
Given a QAP for a given article, we also study how to generate adequate distractors that are grammatically correct and semantically related to the correct answer in the sense that the distractors, while incorrect, look similar to the correct answer with a sufficient distracting effect---that is, it should be hard to distinguish distractors from the correct answer without some understanding of the underlying article.
An distractor could be a single word, a phrase, a sentence segment, or a complete sentence.
It must satisfy the following requirements:
(1) it is an incorrect answer to the question;
(2) it is grammatically correct and consistent with the underlying article;
(3) it is semantically related to the correct answer; and
(4) it provides distraction so that the correct answer could be identified only with some understanding of the underlying article.

In particular, we are to generate three adequate distractors for a QAP to form an MCQ. One way to generate a distractor is to substitute a word or a phrase contained in the answer with an appropriate word or a phrase that maintains the original part of speech. 
Such a word or phrase could be an answer itself or contained in an answer sentence or sentence segment. For convenience, we refer to such a word or phrase as a target word. 

If a target word is a number with an explicit or implicit quantifier,
%is a point in time, a numerical value, 
or anything that can be converted to a number, we call it a type-1 target.
If a target word is a person, location, or organization, we call it a type-2 target. 
Other target words (nouns, phrasal nouns, adjectives, verbs, and adjectives) are referred to as type-3 targets.
We use different methods to generate distractors for targets of different types.

Our distractor generation method is a combination of part-of-speech (POS) tagging \cite{toutanova2003feature}, 
named-entity (NE) tagging \cite{nadeau2007survey,ali2010automation,peters2017semi}, 
semantic-role labeling \cite{martha2005proposition,shi2019simple},
regular expressions, domain knowledge bases on people, locations, and organizations,
word embeddings (such as Word2vec \cite{10.5555/2999792.2999959}, GloVe \cite{pennington-etal-2014-glove}, Subwords \cite{bojanowski2017enriching}, and spherical text embedding \cite{sphericalembedding2019}), word edit distance \cite{levenshtein1966binary}, WordNet (https://wordnet.princeton.edu), and some other algorithms.
We show that, via experiments, our method can generate adequate distractors for a QAP to form an MCQ with a high successful rate. 