\chapter{Sequence-learning-based Question Generation}

We present a learning scheme to generate QAPs via meta sequence representations of sentences. A meta sequence is a sequence of vectors comprising semantic and syntactic tags.

In particular, we use a sequence of vectors to represent a sentence, where each vector consists of a semantic-role (SR) tag, a part-of-speech (POS) tag, and other syntactic and semantic tags, and we refer to such a sequence as a \textsl{meta sequence}.
We then present a scheme called MetaQA to learn meta sequences of declarative sentences and the corresponding interrogative sentences from a training dataset. % consisting of such sentences.
Combining and removing redundant meta sequences yields a set called MSDIP (Meta-Sequence-Declarative-Interrogative Pairs), with each element being a pair of an MD and corresponding MI(s), where MD and MI stand for, respectively, a meta sequence for a declarative sentence and for an interrogative sentence.
A trained MetaQA model generates QAPs for a given declarative sentence $s$ as follows:
Generate a meta sequence for $s$, find a best-matched MD from MSDIP, generates meta sequences for interrogative sentences according to the corresponding MIs and the meta sequence of $s$, identifies the meta-sequence answer to each MI, and coverts them back to text to form a QAP.


Our objective is to generate adequate QAPs on a given declarative sentence written in a given language $L$. We %present a general framework 
%by assuming 
assume that $L$ has an oracle $O_L$ to
provide syntactic and semantic information on a given sentence.

\begin{enumerate}
\vspace*{-5pt}
\item  $O_L$ can distinguish simple sentences (i.e., there is only one predicate) and complex sentences (i.e., there are two or more predicates). A complex sentence has two kinds: The first kind consists of a simple sentence as a main clause and a few 
subordinate clauses (simple or complex sentences) or sentence segments
with  normalized verbs.
The second kind consists of 
a few independent sentences (simple or complex) connected by conjunction.

\vspace*{-5pt}
\item $O_L$ can segment sentences into a sequence of basic units.
A basic unit could be a phrasal verb, a phrasal noun, or simply a word 
that does not belong to any phrase (if any) contained in the sentence.

\vspace*{-5pt}
\item $O_L$ can assign each basic unit in a sentence with an SR tag and a POS tag.
For a complex sentence of the first kind, $O_L$ can tag the main clause as a simple sentence and each subordinate clause with one SR tag (such as time and cause), and tag each subordinate clause itself
as a sentence. For a complex sentence of the second kind, $O_L$ simply separates the sentence into a collection of individual sentences and tags them accordingly.
Moreover, $O_L$ may be able to produce  other semantic or syntactic tags for each basic unit. 

\vspace*{-5pt}
\item $O_L$ can identify an interrogative pronoun by a POS tag.
An interrogative sentence, however, may or may not include an interrogative pronoun. 
\end{enumerate}

For example, exiting NLP tools for
the English language provide a reasonable approximation to such an oracle. Better approximations are expected when more NLP techniques are developed.
%We suspect that any natural language would
%satisfy these assumptions as well. 
\begin{definition}
Let $k \geq 2$ be a number of tags that $O_L$ can assign to a basic unit.
A $k$-semantic-syntactic unit ($k$-SSU) is a $k$-dimensional vector of tags,
denoted by $(t_1, t_2, \ldots, t_k)$, where $t_1$ is an SR tag, $t_2$ is a POS tag, and $t_i$ ($i>2$) represent other tags of fixed types.
\end{definition}

For example, 
we may add an NE tag to a basic unit to form a 3-SSU; adding one more tag on sentiment  forms a 4-SSU. 
Let $U = (t_1,t_2,\ldots, t_k)$ be an SSU. Denote by $U.i = t_i ~(i \geq 1)$.
The prefix $k$ is omitted when there is no confusion. 

Two consecutive SSUs $A$ and $B$ with $A.1 = B.1$ (i.e., they have the same SR tag)
and $A$ appearing on the left side of $B$ in a sentence may be merged to a new SSU $C$ as follows:
(1) If $A = B$, then set $C \leftarrow A$.
(2) Otherwise, based on the underlying language $L$, either set $C.2 \leftarrow A.2$ (i.e., use the POS tag on the left) or set $C.2 \leftarrow B.2$.
For the rest of the tags in $C$, select a corresponding tag in $A$ or $B$ according to $L$. The following proposition is evident:

\begin{proposition} \label{prop:1}
For any sequence of SSUs, after merging, the new sequence of SSUs does not
have two consecutive SSUs with the same SR tag.
\end{proposition}

To accommodate the situation without proper segmentation of phrasal verbs,
% (see Section \ref{sec:4.7}), 
it is desirable to allow a fixed number of consecutive SSUs to have the same SR tag
in a meta sequence. 
 
\begin{definition} \label{def:2}
A {\itshape meta sequence} is a sequence  of SSUs such that 
each SR tag appears at most $r$ times,
with interrogative pronouns (if any) left as is without tagging, where
$r \geq 1$ is a positive constant. % depending on the underlying language.
\end{definition}

We assume the availability of \textsl{sentence segmentation} that can segment
%will only consider 
a complex sentence to form simple sentences for each clause (main and subordinate),
%with
%each clause being a simple sentence, 
and we treat such a sentence as 
a set of simple sentences. If a clause itself is a complex sentence, it can be further
segmented as a set of simple sentences. A declarative sentence consists of at least three different SR tags corresponding to
subject, object, and predicate.

Since a complex sentence can be treated as a list of simple sentences, 
MetaQA learns meta sequences 
of declarative sentences and the corresponding interrogative sentences from a training dataset
consisting of such pairs of sentences, where a declarative sentence is a simple
sentence. %After training, Meta generates QAPs on a given declarative sentence.

However, there are complex sentence that are not easily segmented into
a set of simple sentences using the existing NLP tools. To represent this type of complex sentences, we may
define a meta sequence as a recursive list of SSUs with a tree structure to represent a sentence using the notion of \textsl{list} in the LISP programming language. This
will be addressed in a separate paper.
% it is not needed for the purpose of this paper.

MetaQA consists of two phases: learning and generation. In the learning phase,
MetaQA learns meta sequence pairs from an initial training dataset to generate an initial MSDIP.
In the generation phase, it takes a declarative sentence as input and 
generates QAPs using MSDIP.
Figure \ref{fig:3} depicts the general architecture and data flow of MetaQA,
which consists of six components: Preprocessing (PP), Meta Sequence Generation (MSG), Meta Sequence Learning (MSL), 
Meta Sequence Matching (MSM), and QAP Generation (QAPG) 
%(see Section \ref{sec:4} for detailed explanations of these components in connection to an implementation of the English language). 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9 \linewidth]{MetaQA.png}
  \caption{MetaQA architecture and data flow}
  \label{fig:3}
%   \Description{}
\end{figure}

Both phases use the same PP and MSG components. The PP component is responsible for
tagging basic units in a given sentence (declarative or interrogative) with SR tags, POS tags, and other syntactic and semantic tags, and segmenting complex sentences into a set of simple sentences using oracle $O_L$. The MSG component is responsible for merging SSUs to form a meta sequence. Moreover, for an input sentence in the generation phase, MSG also maps each SSU after merging to the underlying text. 

\section{Learning phase}   

%MetaQA
% first uses $O_L$ to assign SR tags, POS tags, and other syntactic and semantic tags
%to basic units in a given sentence (declarative or interrogative).
%It then merges consecutive SSUs 
%to form a meta sequence.
%Recall that an interrogative pronoun identified by POS tag is left as is without using its SSU.
The MSL component removes redundant meta sequences for each pair of
MD and MI generated from MSG and stores the remaining pairs in the MSDIP database.  
Recall that an interrogative pronoun identified by POS tag in an MI is left as is without using its SSU

%The learned result is a dataset called MSDIP (Meta-sequence
%Declarative-Interrogative Pairs). Inside each pair in MSDIP, the first element is
%a meta sequence for a declarative sentence (MD) and the second element is a set of
%meta sequences for the corresponding interrogative sentences (MI) in the dataset.

Note that for any language, $k$ is a constant, 
so are the number of SR tags, the number of POS tags, and
the number of other tags. 
The following proposition is straightforward.

\begin{proposition} \label{prop:2}
(1) For a given language, the length of a meta sequence is bounded above
by a constant, so is the size of MSDIP. 
(2) The length of a meta sequence for a declarative sentence
is at least 3. 
% is bounded above by a constant, so is 
\end{proposition}


\section{Generation phase} \label{sec:3}

Let $M$ be a meta sequence. Denote by
$M'$ the set of SSUs contained in $M$ and $|M|$ the size of $M'$.
After MetaQA is trained, it generates QAPs from
a given declarative sentences $s$ using the following
{\itshape QAP-generation algorithm}, where
$X_s$ is
the meta sequence for $s$ generated from MSG. Recall that the text
for each SSU is stored in the SSU-Text Map. 
%Note that $|X_s| \geq 3$ because $X_s$ must contain at least three SSUs with the corresponding SR tags being subject, object, and predicate.

Step 1. Find a meta sequence MD $X$ from (MD, MI) pairs in MSDIP 
that is the \textsl{best match} of $X_s$. 
This means that the longest common substring of $X$ and $X_s$,
denoted by $Z = \text{LCS}(X,X_s)$, is the longest 
among all MDs in MSDIP.  A substring is a sub-sequence of consecutive SSUs.
If $Z$ contains SSUs for, respectively, a subject, a predicate, and an object, then
we say that it is a \textsl{successful} match.
If furthermore, $Z = X = X_s$, then we say that it is  
a \textsl{perfect} match. If $Z$ is missing a subject SSU, a predicate SSU, or
an object SSU, then it is an \textsl{unsuccessful} matching.
If a match is successful, got Step 2.
If a match is unsuccessful or successful but not perfect, then notify the user that 
MetaQA needs to learn a new pattern and
ask for interrogative sentences for $s$ from the user. After this, go to Step 2.

% an unsuccessful match and a non-perfect successful match,


Step 2.
The goal is to generate all possible interrogative sentences for $s$.  
For each pair $(X,Y) \in \text{MSDIP}$,
generate a meta sequence $Y_s$ from $Y$ with
$$Y'_s = [Y'-(X' \cap Y'-X'_s)] \cup (X'_s - Z').$$
This means that $Y'_s$ is obtained from $Y'$ by removing SSUs that are in both
meta sequences in the matched pair
but not in the input sentence,
and adding SSUs in the input sentence but not in the matched MD.
Since $Z = \text{LCS}(X,X_s)$, the following proposition is straightforward:

\begin{proposition} \label{prop:3}
$X'_s - Z' = X'_s - X'$.
\end{proposition}
Order SSUs in $Y'_s$ appropriately to form $Y_s$, which requires
localization according to the underlying language.
If an SSU in $Y'_s$ has the corresponding text stored in Step 1,
then replace it. If not, then it requires localization to resolve it.
This generate an interrogative sentence $Q_{s}$ for $s$.

Step 3. For each interrogative sentence $Q_{s}$ generated in Step 3,
the SSUs in $A'_s = X' - Y'$ represent a correct answer.
Place SSUs in $A'_s$ in the same order as in $X'_s$ and replace each SSU with the corresponding text in $s$ to obtain an answer $A_s$ for $Q_{s}$.


% \begin{comment}
\section{An Implementation of MetaQA for English} \label{sec:4}

SR, POS, and NE tags are used in this implementation. Existing
NLP tools for generating these tags are for words, not for phrases. 
We could, however, use phrase segmentation to resolve this by appropriate merging operations.
While word segmentation is not needed %is a must in processing logographic languages, 
%there are ways to get around it 
in alphabetic languages such as English, 
phrase segmentation provides a better interpretation of the underlying sentence.
We first assume the existence of an ideal phrase segmentation for English,  
and then discuss how to get around it at the end of this section.

\subsection{Preliminaries}
The following NLP tools are used to generate tags: 
Semantic-Role Labeling (SRL) \cite{shi2019simple} for SR tags,
POS Tagging \cite{toutanova2003feature} for POS tags, and
Named-Entity Recognition (NER) \cite{peters2017semi}
for NE tags.

SR tags are defined in PropBank\footnote{https://verbs.colorado.edu/~mpalmer/projects/ace/EPB-annotation-guidelines.pdf} \cite{bonial2012english,martha2005proposition}, 
which consist of three types: ArgN (arguments of predicates), ArgM (modifiers or adjuncts of the predicates) , and V (predicates).
ArgN consists of six tags: ARG0, ARG1, $\ldots,$ ARG5, and
ArgM consist of
multiple subtypes such as
LOC as location, EXT as extent, DIS as discourse connectives, 
ADV as general purpose, NEG as negation, MOD as modal verb, CAU as cause, TMP as time, PRP as purpose, MNR as manner, GOL as goal, and DIR as direction.

POS tags$\,$\footnote{https://www.ling.upenn.edu/courses/Fall\_2003/ling001/penn\_treebank\_pos.html} are defined in the Penn Treebank tagset \cite{toutanova2003feature,marcus1993building}. For example, NNP is for singular proper noun,
VBZ for third-person-singular-present-tense verb,
DT for determiner, and IN for preposition or subordinating conjunction.

NE tags include PER for persons, ORG for organization, LOC for locations, and numeric expressions for time, date, money, and percentage.



\subsection{PP, MSG, and MSL Localization}

The PP, MSG, and MSL components, on top of what is described in
Section \ref{sec:3}, incur the following localization.
%
% are the same as in
%Section 3, except the following localization in preprocessing:
%
%\subsection{Preprocessing}
%
%The PP component first 
PP first replaces contractions and slang with
words or phrases to help improve tagging accuracy.
For example,  contractions \textsl{'m, 's, 're, 've, n't, e.g., i.e., a.k.a.} are replaced by,
respectively, \textsl{am, is, are, have, not, for example, that is, also known as}.
Slang \textsl{gonna, wanna, gotta, gimme, lemme, ya}  are replaced by,
respectively, \textsl{going to, want to, got to, give me, let me, you}.

PP then segments sentences and
tags words in sentences using SRL, POS Tagging, and NER
for the training dataset and later for input sentences for generating QAPs.
Use SRL to segment a complex sentence into a set of simple sentences and discard all simple sentences without 
a subject or an object. Note that there are complex sentences that are hard to segment using SRL.
Moreover, for each sentence, PP removes
all the words with a CC (coordinating conjunction) as POS tag before its subject, including
\textsl{and, but, for, or, plus, so, therefore}, and \textsl{because}.

%\subsection{Meta Sequence Generation}

%The MSG component generates a meta sequence for a sentence
%in the learning phase and a meta sequence for a declarative sentence in the 
%generation phase.

%Every word in a sentence has an SR tag and a POS tag from preprocessing,
%but NE tags may be null. 
% Then MSG merges the SSUs for words in each basic unit as follows:
% (1) If the unit contains a V-SSU (i.e., an SSU with V as the SR tag),
% then use this V-SSU for the entire unit.
% (2) If the unit contains no V-SSU, then it must contain a noun,
% use the first SSU from the right 
% with a noun POS tag.

MSG then merges the remaining SSUs if two consecutive SSUs are identical.
If they are not identical but have the same SR tag,
then use this SR tag in the merged SSU, and
the POS tag in the first SSU from the right. 
If they contain a noun, use the first SSU from the right with a noun POS tag.
Moreover, the NE tag in the merged SSU is null
if both SSUs contain a null NE tag; otherwise,
use the first non-empty NE tag from the right. 

%For an input sentence in the
%generation phase,  MSG also stores a mapping between a merged SSU and the
%corresponding text in the SSU-Text Map.

%\subsection{Meta Sequence Learning} 
%The MSL component removes redundant meta-sequence pairs generated for the training dataset, with each pair in the form of (MD, MI), where MD representing the meta sequence of
%a declarative sentence and MI the meta sequence for the corresponding interrogative sentence. %Not that there may be multiple MIs for the same MD. The remaining meta-sequence
%pairs are stored in the MSDIP file, which is typically small, and bounded above by
%a constant (Proposition \ref{prop:2}).

\subsection{MSM Localization}
The MSM component takes a meta sequence $X_s$ of a sentence $s$ as input 
and executes Step 1 in the
QAP-generation algorithm described in Section \ref{sec:3} using Ukkone's Suffix-Tree algorithm \cite{ukkonen1985algorithms} to compute a longest common substring of two
meta sequences, which runs in linear time. During matching, the POS tags for various types of nouns 
are treated equal; they are NN, NNP, NNS, and NNPS, The POS tags 
for third-person-singular-present verbs are treated equal; they are VBP and VBZ.
To use Ukkone's algorithm,,
we encode a meta sequence as a sequence of symbols using $/$ to separate tags in an SSU.
That is, vector $(t_1,t_2,t_3)$ is now written as $t_1/t_2/t_3$. If $t_2$ is null, then write it as
$t_1//t_3$. If $t_3$ is null, then write it as $t_1/t_2/$. If both are null, then write it
as $t_1//$. SSUs in a sequence are just written as concatenation. For example,
the sentence ``Abraham Lincoln the 16th president of the United States" has
the following SSUs:

\textrm{Abraham (ARG1/NNP/PER) Lincoln (ARG1/NNP/PER) was (V/VBZ/) the (ARG2/DT/)
16th (ARG2/JJ/) president (ARG2/NN/) of (ARG2/IN/) the (ARG2/DT/) United (ARG2/NNP/LOC)
States (ARG2/NNP/LOC)}.

The meta sequence for this sentence is, after merging: 
\textrm{ARG1/NNP/PER V/VBZ/ ARG2/NNP/LOC}.

Let $X$ be an MD in MSDIP such that LCS($X,X_s$) is the longest
among all MDs in MDDIP,
denoted by $Z$. 

%If matching is perfect, move to the QAPG component.
%If matching is not successful, then
%move to the UEIS component,
%inform the user that a new pattern is encountered, and ask for input of
%interrogative sentences to train new pairs for MSDIP.
%If matching is successful but not perfect, then the sentence $s$ still represents a new pattern %not yet learned, even though a question may still be generated.
%In this case, MSM moves to QAPG, as well as to UEIS as an option for new training.

\subsection{QAPG Localization}

The QAPG component executes Steps 2--3 in the QAP-generation algorithm described in
Section \ref{sec:3}. Recall that $Z = \text{LCS}(X,X_s)$ is the longest match
among all MDs in MSDIP, and
after 
the set of SSUs $Y'_s$ is generated, localization is needed to form $Y_s$.

\textsl{Case 1:} $Z=X_s$. Then $Y_s = Y$. 
%Replace each SSU in $Y_s$ with the corresponding text stored in Step 1. 

\textsl{Case 2:} $Z$ is a proper substring of $X_s$. Then each SSU in $X'_s - Z$ appears either 
before $Z$ or after $Z$. Form a string $Y_b$ and $Y_a$ of the SSUs that appear,
respectively, before and after $Z$ in the same order as they appear in $X_s$.
Let $Y_s =  [Y-(X'\cap Y'-X'_s)]Y_aY_b$, where $Y-(X'\cap Y'-X'_s)$
means to remove from $Y$ the SSUs in $X'\cap Y'-X'_s$.
 
For each SSU in $Y_s$ if a corresponding text can be found in the SSU-Text Map,
then replace it with the text. An SSU that doesn't have a matched text in the
SSU-Text Map is
due to the helping verbs added in the interrogative sentence that generates $Y$.
%
%Some SSUs in $Y_s$, however, may not have a match in $X_s$ and so
%cannot be replaced with appropriate text directly from the SSU-Text Map.
%Since each declarative sentence must include a subject, an object, and a predicate, there %must be texts for the SSUs that represent these entities in the SSU-Text Map. 
There are five POS tags for verbs: VBG for gerund or present participle,
VBD past tense, VBN past participle, VBP non-3rd person singular present,
and VBZ 3rd person singular present.
Present participle and past participle have already included helping verbs, and so do the negative forms of
past tense and present tense.
Thus, only positive forms of past tense (VBD) and present tense (VBP, VBZ) 
do not include
helping verbs, which need to be resolved.

\section*{Rule for resolving helping verbs}

The first V-SSU in $Y$ (i.e., the SSU that contains the SR tag of V) is a helping verb. To determine its form, 
check the POS tag in the subject SSU (usually it is ARG0)
% check the POS tag in the ARG0-SSU 
and determine if it is singular or plural. Then check the POS tag in the first V-SSU in $Y$ 
to determine the tense. Replace the second V-SSU with the verb in its original form 
for the V-SSU in the SSU-Text MAP. 

For example, suppose that the following declarative sentence ``John traveled to Boston last week" and its
interrogative sentence about location ``Where did John travel to last week"
are in the training dataset, which generate the following SSUs before merging:

John (ARG0/NNP/PER) traveled (V/VBD/) to (ARG1/IN/) Boston (ARG1/NNP/LOC) last (TMP/NN/) week (TMP/NN/).

Where (Where) did (V/VBD) John (ARG0/NNP/PER) travel (V/VB/) to (ARG1/IN/) 
last (TMP/NN/) week (TMP/NN/)?

Since ``travel to" is a phrasal verb, after merging, we have

John (ARG0/NNP/PER) traveled  to (V/VBD/) Boston (ARG1/NNP/LOC) last week (TMP/NN/).

Where (Where) did (V/VBD) John (ARG0/NNP/PER) travel to (V/VB/) last week (TMP/NN/)?

The following meta-sequence pair $(X,Y)$ is learned for MSDIP:

X  = \textrm{ARG0/NNP/PER V/VBD/ ARG1/NNP/LOC TMP/NN/}

Y = \textrm{Where V/VBD/ ARG0/NNP/PER V/VB/ TMP/NN/}

Suppose that we are given a sentence $s=$ ``Mary flew to London last month." Its meta sequence $X_s$ is exactly the same as $X$, with
ARG0/NNP/PER for ``Mary",
V/VBD/ for ``flew to", ARG1/NNP/LOC for ``London", and TMP/NN/ for ``last month", which
are stored in the SSU-Text Map.
Thus, $Y_s = Y$. 
We can see that 
the SSU of V/VB/ %and ARG1/IN/ 
in $Y$ is not in the SSU-Text Map.
To resolve the unmatched V/VB/, check the POS tag in the ARG0-SSU, which is NNP, indicating a singular noun.
The POS tag in the first V-SSU is VBD, indicating past tense. Thus, the correct form of the helping verb is ``did". The text for V/VBD is ``flew to" in the SSU-Text Map. The original form of the verb is ``fly".
Thus, the second V-SSU is replaced with ``fly". 
%The remaining text ``to" is for ARG1/IN/.
This generates the following interrogative sentence: ``Where did Mary fly to last month?"
The answer SSU is $X'-Y'$, which is ARG1/NNP/LOC, corresponding to ``London".

\subsection{SSU Merging without Segmentation} \label{sec:4.7}

To the best of our knowledge, no tools exist at this point that can segment English sentences to identify phrasal nouns and phrasal verbs. It is worth mentioning that AutoPhrase \cite{shang2018automated} 
can be used for identifying certain phrasal nouns. 
We could deal with 
phrasal verbs using a list of common phrasal verbs or by modifying merging operations.
A phrasal verb consists of a preposition or an adverb, or both. 
There are four POS tags  IN for preposition or subordinating conjunction,
RB for adverb, RBR for comparative adverb, and RBS for superlative adverb.

To see this problem, let us look at the same example aforementioned. After merging,
we have

John (ARG0/NNP/PER) traveled (V/VBD/) to Boston (ARG1/NNP/LOC) last week (TMP/NN/).

Where (Where) did (V/VBD/) John (ARG0/NNP/PER) travel (V/VB/) to (ARG1/IN/) last week (TMP/NN/)?

For the input sentence we have

Mary (ARG0/NNP/PER) flew (V/VBD/) to London (ARG1/NNP/LOC) last moth (TMP/NN/).

The interrogative sentence is ``Where did Mary fly ARG1/IN/ last week?" after replacing SSUs with text in
the SSU-Text Map, with ARG1/IN/ unmatched with text.
We can resolve this by modifying the merging operation as follows:
%
When an SSU with a POS tag for preposition or adverb appears appears before or after a V-SSU, leave it as is without merging it with its neighboring SSUs of the same SR tag, unless the POS tags 
in them are also for prepositions or adverbs. The rest of the merging operations are the same. Then we have, after merging,

John (ARG0/NNP/PER) traveled (V/VBD/) to (ARG1/IN/) Boston (ARG1/NNP/LOC) last week (TMP/NN/).

Where (Where) did (V/VBD/) John (ARG0/NNP/PER) travel (V/VB/) to (ARG1/IN/) last week (TMP/NN/)?

Now the input sentence becomes, after SSU merging,

Mary (ARG0/NNP/PWR) flew (V/VBD) to (ARG1/IN/) London (ARG1/NNP/LOC) last moth (TMP/NN/).

All the SSUs in the meta sequence  ``Where V/VBD/ ARG0/NNP/PER V/VB/ ARG1/IN/ TMP/NN/" have corresponding text 
 in the SSU-Text Map after resolving for helping verbs.  The answer
SSU is in $X'-Y'=$ ARG1/NNP/LOC, which is ``London". 
% \end{comment}



% \begin{comment}
\section{Running Samples}  \label{sec:examples}

\paragraph{Example 1.}
Suppose that the following declarative sentence and the corresponding
two interrogative sentences are given as training data at the learning phase:

``Amanda has a story book on the American history." 
 
``Who has a story book on the American history?" 

``What does Amanda have?"

PP generates corresponding SSUs as follows:

\textrm{
Amanda (ARG0/NNP/PER) has (V/VBZ/) a (ARG1/NN/) nice (ARG1/NN/) textbook (ARG1/NN/) on (ARG1/NN/) programming (ARG1/NN/).}

\textrm{Who (Who) has (V/VBZ/) a (ARG1/NN/) nice (ARG1/NN/) textbook (ARG1/NN/) on (ARG1/NN/) programming (ARG1/NN/)?}

\textrm{What (What) does (V/VBZ/) Amanda (ARG0/NNP/PER) have (V/VB/)?}

MSG merges SSUs and MSL places two pairs $(X_1, Y_{1,1})$ and $(X_1, Y_{1,2})$ in MSDIP if they are
not already present, where

$X_1 =$ \textrm{ARG0/NNP/PER V/VBZ/ ARG1/NN/, }

$Y_{1,1} =$ \textrm{Who V/VBZ/ ARG1/NN/}, 

$Y_{1,2} =$ \textrm{What V/VBZ/ ARG}. 

Suppose that the following two declarative sentences are given at the generation phrase:

$s_1=$ ``Tom has a story book on the American history."

$s_2=$ ``Duncan Watts agrees with the conclusion."

PP generates SSUs as follows, with ``agrees with" recognized as a phrasal verb.

\textrm{Tom (ARG0/NNP/PER) has (V/VBZ/) a (ARG1/NN/) story (ARG1/NN/) book (ARG1/NN/) on (ARG1/NN/) the (ARG1/NN/) American (ARG1/NN/) history (ARG1/NN/)}.

\textrm{Duncan (ARG0/NNP/PER) Watts (ARG0/NNP/PER) agrees (V/VBZ/) with (ARG1/IN/) the (ARG1/NN/) conclusion (ARG1/NN/)}.

MSG merges SSUs, generates the following two meta sequences:

$X_{s_1} = X_{s_2}=$ \textrm{ARG0/NNP/PER V/VBZ/ ARG1/NN/}, 

and
places the following in the
SSU-Text Map:

\textrm{Tom (ARG0/NNP/PER) has (V/VBZ/) a story book on the American history (ARG1/NN/)}.

\textrm{Duncan Watts (ARG0/NNP/PER) agrees with (V/VBZ/) the  conclusion (ARG1/NN/)}.

MSM  finds a match
$X_1 = X_{s_1} = X_{s_2}$. QAPG
generates

$Y_{s_1,1} = Y_{s_2,1}=$ \textrm{Who V/VBZ/ ARG1/NN/},

$Y_{s_1,2}= Y_{s_2,2}=$ \textrm{What V/VBZ/ ARG0/NNP/PER V/VB/}.

The corresponding interrogative sentences are, after applying the rule for resolving VB for
$Q_{s_1,2}$ and $Q_{s_2,2}$:

$Q_{s_1,1} =$ ``Who has a story book on the American history?"

$Q_{s_1,2}=$ ``What does Tom have?"

$Q_{s_1,1} =$ ``Who agrees with the conclusion?"

$Q_{s_1,2}=$ ``What does Duncan Watts agree with?"

The SSU for the answer to $Y_{s_1,1}$ is ARG0/NNP/PER with ``Tom" being
the text and 
the answer to $Y_{s_1,2}$ is ARG1/NN/ with ``a story book on American history" being the 
text. Likewise, the SSU for the answer to $Y_{s_2,1}$ is ARG0/NNP/PER with
``Duncan Watts" being the text and the answer to $Y_{s_2,2}$ is ARG1/NN/ with
``the conclusion" being the text.

\textsl{Remark.} Using the modified merging operation without using segmentation, we have$X_{s_2}=$ ARG0/NNP/PER V/VBZ/ ARG1/IN/ ARG1/NN/, and so 
$X$ is no longer a successful match. A new pattern is needed to learn for MSDIP.

% \begin{comment}
\paragraph{Example 2.}
Suppose that the following two pairs of declarative and interrogative sentences
are in the training dataset: 

``A doughnut is a fried dough confection."

``What is a doughnut?"

``Uranus is a unusual planet because it is tilted."

``Why is Uranus an unusual planet?"

The SSUs for these sentences are

\textrm{A (ARG1/NN/) doughnut (ARG1/NN/) is (V/VBZ/) a (ARG2/NN/) fried (ARG2/NN/) dough (ARG2/NN/) confection (ARG2/NN/).}

\textrm{What (What) is (V/VBZ/) a (ARG2/NN/) doughnut (ARG2/NN/)?}

\textrm{Uranus (ARG1/NNP/) is (V/VBZ/) an (ARG2/NN/) unusual (ARG2/NN/) planet (ARG2/NN/) because (CAU/VBN/) it (CAU/VBN/) is (CAU/VBN/) tilted (CAU/VBN/).} 

\textrm{Why (Why) is (V/VBZ/) Uranus (ARG1/NNP/) an (ARG2/NN/) unusual (ARG2/NN/) planet (ARG2/NN/)?}

After merging SSUs, the following two meta-sequence pairs are learned:

$(X_2 Y_2) =$ (\textrm{ARG1/NN/ V/VBZ/ ARG2/NN/}, \textrm{What V/VBZ/ ARG2/NN/}),

$(X_3, Y_3) =$ (\textrm{ARG1/NNP/ V/VBZ/ ARG2/NN/ CAU/VBN/}, \textrm{Why V/VBZ/ ARG1/NNP/ ARG2/NN/}).

Now suppose $s_3=$ ``The solar panel manufacturing industry is in the doldrums because supply far exceeds demand" is an input sentence for generating QAPs, which has
the following SSUs:

\textrm{The (ARG1/NN/) solar (ARG1/NN/) panel (ARG1/NN/) manufacturing (ARG1/NN/) industry (ARG1/NN/) is (V/VBZ/) in (ARG2/NN/) the (ARG2/NN/) doldrums (ARG2/NN/) because (CAU/NN/) supply (CAU/NN/) far (CAU/NN/) exceeds (CAU/NN/) demand (CAU/NN/).}

After merging SSUs we have a meta sequence $X_{s_3} =$ \textrm{ARG1/NN/ V/VBZ/ ARG2/NN/ CAU/NN/} with the following SSU-Text Map:

\textrm{The solar panel manufacturing industry (ARG1/NN/) is (V/VBZ/) in the doldrums (ARG2/NN/) because supply far exceeds demand (CAU/NN/)}.

We can see that $\text{LCS}(X_2, X_{s_3}) = \text{LCS}(X_3, X_{s_3}) =$ ARG1/NN/ V/VBZ/ ARG2/NN/, which generates the following two meta sequences:

$Y_{s_3,1} =$ (\textrm{What V/VBZ/ ARG2/NN/}),

$Y_{s_3,2} =$ (\textrm{Why V/VBZ/ ARG1/NNP/ ARG2/NN/ CAU/VBN/, where CAU/VBN/})
$\in X'_{s_3} - X'_2$ is added to $Y_3$ to get $Y_{s_3,2}$.

The corresponding interrogative sentences are, after applying the rule for resolving helping verbs:

$Q_{Y_{s_3,1}} =$ ``What is in the doldrums because supply far exceeds demand?"

$Q_{Y_{s_3,2}}=$ ``Why is the solar panel manufacturing industry in the doldrums?"

The answer to $Q_{Y_{s_3,1}}$ is the text for SSU $\in  X'_2-Y'_2 =$ ARG1/NN/, which is
``The solar panel manufacturing industry."
Likewise, the answer to $Q_{Y_{s_3,2}}$ is the text for SSU $ \in X'_3-Y'_3=$ CAU/NN/,
which is ``because supply far exceeds demand."
% \end{comment}

\section{Evaluations}

To evaluate MetaQA, we need to have appropriate evaluation measures,
training data, and evaluation data.
BLUE \cite{10.3115/1073083.1073135}, ROUGE \cite{lin-2004-rouge}, and Meteor \cite{10.1007/s10590-009-9059-4} are standard evaluation metrics for measuring automatic summarization and machine translation, which are good for computing text similarity
and have also been used to evaluate QG. Another commonly-used measure is 
human judgments.
BLEU and ROUGE-N count the number of overlapping units between the candidate text and the reference text by using N-grams.
ROUGE-L measures the cognateness between the candidate text and the reference text by using Longest common sub-sequence.
Meteor compares the candidate text with the reference text in terms of exact, stem, synonym, and paraphrase matches between words and phrases.
%It was shown that these metrics are either weak or have no correlation with human judgments  \cite{callison-burch-etal-2006-evaluating,liu-etal-2016-evaluate} . 
These metrics, however, do not evaluate grammatical correctness. Thus, human judgment 
is the only liable measure for grammatical correctness.

SQuAD \cite{Rajpurkar_2016} is a  dataset that has been
used for training and evaluating generative methods for QG.
However, not all QAPs in SQuAD are well-formed or with correct answers. 
There are also about 20\% of questions in the dataset that require paragraph-level information.
Thus, SQuAD is unsuitable for evaluating QAPs for our purpose.
Instead, we constructed an initial training dataset by writing a number of declarative sentences
and the corresponding interrogative sentences to cover the major
tense, participles, voice, modal verbs, and some common phrasal verbs such as
``be going to" and ``be about to" for the following six interrogative pronouns: 
\textsl{Where, Who, What, When, Why, How many}. A total of 112 meta-sequence pairs (MD, MI) were learned as  the initial MSDIP.

To evaluate MetaQA, we extracted declarative sentences from the official SAT practice reading tests$\,$\footnote{https://collegereadiness.collegeboard.org/sat/practice/full-length-practice-tests}, for the reason that
SAT practice reading tests provide a large number of different patterns of declarative sentences. 
 There are a total of eight SAT practice reading tests, each consisting of five articles and
 each article consisting of around 25 sentences, for a total of 40 articles and 1,136 sentences. 
After removing easy-to-identify interrogative sentences and imperative sentences,
we harvested a total of 1,025 sentences (which may still contain imperative
sentences).
Using the initial MSDIP, MetaQA generated a total of 796 QAPs.
% with the following breakdowns:

Three native Chinese speakers evaluated the QAPs on a shared
Google doc file based on the following criteria:
For questions: Check both syntax and semantics: (1) correct; (2) acceptable (e.g., a minor would make it correct); (3) not acceptable.
 For answers: (1) matched---the answer matches well with the question; (2) acceptable; (3) not acceptable.
 The final results were agreed by the three judges. Presented below 
 are questions generated with detailed breakdowns in each
category, where ``all correct" means both syntactically and semantically correct and conforming to native-speaker norms,
``not acceptable" means either syntactically or semantically unacceptable, and
``How" means ``How many": 

\medskip
\begin{center}
\begin{tabular}{l|c|c|c|c|c|c|c}
\hline
& \textbf{Where} & \textbf{Who} & \textbf{What} & \textbf{When} & \textbf{Why} & \textbf{How} & \textbf{Total} \\
\hline
MSDIP pairs & 18 & 45 & 23 & 22 & 6 & 8 & 122 \\
\hline
QAPs generated & 26 & 216 & 466 & 51 & 15 & 22 & 796 \\
\hline
All correct & 21 & 208 & 458 & 51 & 15 & 20 & 773 \\
\hline
Syntactically acceptable & 4 & 4 & 3 & 0 & 0 & 2 & 13 \\
\hline
Semantically acceptable & 1 & 2 & 5 & 0 & 0 & 0 & 8	 \\
\hline
Not acceptable & 0 & 2 & 0 & 0 & 0 & 0 & 2 \\
\hline
\end{tabular}
\end{center}
\smallskip
The percentage of generated questions that are both syntactically and semantically correct
is 97\%. %Among the 773 all-correct questions, their answers are also all correct. 
We noticed that there is a strong correlation between
the correctness of the questions and their answers. In particular, 
when a generated question is all correct, its answer is also all correct.
When a question is acceptable, its answer may be all correct or acceptable.
Only when a question is unacceptable, its answer is also unacceptable.
 
%The evaluation result is 97\%. 
%Mismatched QAs are due to inaccuracy of NER, 

The 13 incorrect but syntactically acceptable questions are mostly due to some minor issues
in segmenting a complex sentence into simple sentences, where a better
handling of sentence segmentation is expected to correct these issues. Two questions whose
interrogative pronoun should be ``how much" are mistakenly using ``how many".
Further refinement of POS tagging that distinguish uncountable nouns from countable nouns would solve this problem. The eight semantically acceptable questions are all due to 
NE tags that cannot distinguish between persons, location, and things. Further refinement of NE tagging will solve this problem. The two unacceptable questions are due to
serious errors induced when segmenting complex sentences. This suggests that we should
look into using a recursive list to represent complex sentences.

There were 589 sentences for which no matched meta sequences are found from 
the initial MSDIP. By learning new meta sequences from user inputs,
535 of these sentences found perfect matching, which generate
QAPs that are both syntactically and semantically correct. For the remaining 84 sentences,
some of then are imperative sentences without a clear structure of subject-predicate-object,
and some are hard to segment into a set of simple sentences due to inaccurate SR tagging
and so no
appropriate (MD, MI) pairs were learned. This suggests that we should
look into better sentence segmentation methods or meta trees as recursive lists of meta sequences to represent complex sentences
as a whole, which is left for future work.
