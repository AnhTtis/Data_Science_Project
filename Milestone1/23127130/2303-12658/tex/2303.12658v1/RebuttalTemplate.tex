\documentclass[10pt,onecolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
\setcounter{figure}{5}
\setcounter{table}{6}
\setcounter{equation}{15}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2048} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Material: \\ Reliable and Efficient Evaluation of Adversarial Robustness for Deep Hashing-based Retrieval}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\section{Proof of PGM}
\label{a1}
\textbf{Theorem}
pharos code $\boldsymbol{b}^\star$ which satisfies Eq. (6) can be calculated by the Pharos Generation Method (PGM), \textit{i.e.}, 
\begin{equation*}
    \begin{aligned}
    \boldsymbol{b}^\star &= \arg\min_{\boldsymbol{b}^\star\in\{-1,+1\}^K} \sum_{i}\sum_{j}[w_i {D}_{\rm{H}}(\boldsymbol{b}^\star, \boldsymbol{b}_i^{(\rm{p})}) - w_j {D}_{\rm{H}}(\boldsymbol{b}^\star, \boldsymbol{b}_j^{(\rm{n})})] \\
    &=\operatorname{sign}\left(\sum_{i}^{N_{\rm{p}}}\sum_{j}^{N_{\rm{n}}}(w_{i}\boldsymbol{b}_i^{(\rm{p})} - w_{j}\boldsymbol{b}_j^{(\rm{n})})\right).
    \end{aligned}
\end{equation*}
\textit{proof.}
We define the following function:
\begin{equation*}
    \begin{aligned}
    \psi(\boldsymbol{b})=\sum_{i}\sum_{j} [w_i {D}_{\rm{H}}(\boldsymbol{b}, \boldsymbol{b}_i^{(\rm{p})}) - w_j {D}_{\rm{H}}(\boldsymbol{b}, \boldsymbol{b}_j^{(\rm{n})})]
    \end{aligned}.
\end{equation*}
As the pharos code $\boldsymbol{b}^\star$ need to be the optimal solution of the minimizing objective, the above theorem is equivalent to prove the following inequality:
\begin{equation*}
    \begin{aligned}
    \psi(\boldsymbol{b})\geq \psi(\boldsymbol{b}^\star),
    \quad \forall~\boldsymbol{b}\in\{-1,+1\}^K
    \end{aligned}.
\end{equation*}
Let $\boldsymbol{b}=\{b_1,b_2,...,b_K\}$, then we have
\begin{equation*}
    \begin{aligned}
    &\psi(\boldsymbol{b})
    =\sum_{i}\sum_{j} [w_i \frac{1}{2}(K-\boldsymbol{b}^\top \boldsymbol{b}_i^{(\rm{p})}) - w_j \frac{1}{2}(K-\boldsymbol{b}^\top \boldsymbol{b}_j^{(\rm{n})})] \\
    =&-\frac{1}{2}\sum_{i}\sum_{j} [w_i\boldsymbol{b}^\top \boldsymbol{b}_i^{(\rm{p})} - w_j\boldsymbol{b}^\top \boldsymbol{b}_j^{(\rm{n})}] + \xi \\
    =&-\frac{1}{2}\sum_{i}\sum_{j} [w_i\sum_{k=1}^K{b}_k{b}_{ik}^{(\rm{p})}-w_j\sum_{k=1}^K{b}_k{b}_{jk}^{(\rm{n})}]+\xi \\
    =&-\frac{1}{2}\sum_{i}\sum_{j} \left(\sum_{k=1}^K w_i{b}_k{b}_{ik}^{(\rm{p})}-\sum_{k=1}^K w_j{b}_k{b}_{jk}^{(\rm{n})}\right) + \xi \\
    =&-\frac{1}{2}\sum_{i}\sum_{j} \sum_{k=1}^K{b}_k( w_i{b}_{ik}^{(\rm{p})}-w_j{b}_{jk}^{(\rm{n})}) + \xi \\
    =&-\frac{1}{2}\sum_{k=1}^K b_k\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})+\xi, \\
    \end{aligned}
\end{equation*}
where $\xi$ is a constant.
Similarly, 
\begin{equation*}
    \begin{aligned}
    \psi(\boldsymbol{b}^\star)=-\frac{1}{2}\sum_{k=1}^K b^\star_{k}\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})+\xi.
    \end{aligned}
\end{equation*}
Due to the nature of absolute value, we have
\begin{equation*}
    \begin{aligned}
    &\psi(\boldsymbol{b})
    =-\frac{1}{2}\sum_{k=1}^K b_k\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})+\xi \\
    \geq&-\frac{1}{2}\sum_{k=1}^K \left|b_k\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})\right|+\xi \\
    =&-\frac{1}{2}\sum_{k=1}^K\vert{b_k}\vert \left|\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})\right|+\xi \\
    =&-\frac{1}{2}\sum_{k=1}^K \left|\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})\right|+\xi \\
    =&-\frac{1}{2}\sum_{k=1}^K \operatorname{sign}(\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})}))\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})+\xi \\
    =&-\frac{1}{2}\sum_{k=1}^K b^\star_{k}\sum_{i}\sum_{j}(w_ib_{ik}^{(\rm{p})}-w_jb_{jk}^{(\rm{n})})+\xi \\
    =&\psi(\boldsymbol{b}^\star).
    \end{aligned}
\end{equation*}
That is, $\psi(\boldsymbol{b})\geq\psi(\boldsymbol{b}^\star)$. Hence, the Theorem is proved.



\section{Attack results on CIFAR-10}
\label{ap:cifar}
Table \ref{tab:cifar} shows the results of the hashing attack methods on the single-label dataset CIFAR-10 \cite{cao2017hashnet}. We can observe that our PgA is a little bit better than the state-of-the-art SDHA for DPH. However, the proposed PgA outperforms HAG and SDHA over 2.23\%. Especially under the case of 64 bits, PgA brings an boost of 4.05\% and 10.19\% compared to HAG and SDHA, respectively. 

\begin{table}[ht]
\scriptsize
\begin{center}
\caption{\small MAP (\%) of attack methods on CIFAR-10.}
\label{tab:cifar}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
~ & \multicolumn{3}{c}{DPH}& \multicolumn{3}{c}{ATRDH} \\
\cmidrule(r){2-4} \cmidrule(r){5-7}
Method &16 bits &32 bits &64 bits &16 bits &32 bits &64 bits \\
\midrule
Clean &67.72 &78.00 &79.64 &60.98 &62.74 &63.08 \\
P2P &4.11 &3.44 &2.96 &31.55 &31.94 &32.19   \\
DHTA &2.08 &1.24 &0.91 &29.87 &31.12 &31.04  \\
ProS-GAN &2.93 &6.13 &5.17 &64.14 &66.27 &66.98 \\
THA &2.64 &6.77 &8.42 &31.95 &32.79 &35.06   \\
HAG &0.95 &1.16 &1.60 &16.41 &16.51 &18.30   \\
SDHA &{0.32} &0.52 &0.55 &18.90 &20.75 &24.54  \\
PgA (Ours) &\textbf{0.31} &\textbf{0.49} &\textbf{0.48} &\textbf{14.18} &\textbf{13.48} &\textbf{14.25}  \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}



\section{Adversarial Training}
\label{ap:adv}
We use the generated adversarial samples for adversarial training to verify whether the proposed method is still valid. The objective of the adversarial training is formulated as follows:
\begin{equation}
    \begin{aligned}
        \min_{\theta}\mathcal{L}_{adv}=\mathcal{L}_{ori} -\sum_{i=1 }^N\frac{1}{K}(\boldsymbol{b}^\star_{i})^{\top}f_{\theta}(\boldsymbol{x}_i^\prime)
    \end{aligned},
    \label{eq:obj_adv}
\end{equation}
where $\boldsymbol{b}^\star_{i}$ is the pharos code of the instance $\boldsymbol{x}_i$, and $\boldsymbol{x}_i^\prime$ is the adversarial example of $\boldsymbol{x}_i$. The latter term in Eq. (\ref{eq:obj_adv}) can rebuild similarity between the adversarial sample and the true semantics. $\mathcal{L}_{ori}$ is the original loss function of the deep hashing model, which ensures the basic performance of hashing learning. The experimental results are illustrated in Table \ref{table:adv_train}. The adversarial training does improve the defense capability of the deep hashing model, but our attack method is still valid and significantly outperforms the other methods.



\begin{table}[h]
\scriptsize
\begin{center}
\caption{\small MAP (\%) of attack methods on NUS-WIDE.}
\label{table:adv_train}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
Method &16 bits &32 bits &64 bits \\
\midrule
Clean &70.51 &68.50 &62.34 \\
P2P &45.50 &53.08 &56.78   \\
DHTA &43.12 &50.30 &55.47       \\
ProS-GAN &64.27 &67.81 &62.49   \\
THA &48.36 &55.74 &59.90        \\
HAG &45.26 &51.32 &52.26        \\
SDHA &34.67 &45.28 &51.03       \\
PgA (Ours) &\textbf{26.72} &\textbf{37.70} &\textbf{49.70}   \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
