\appendix
\label{sec:appendix}

\section*{\centering{Appendix}}
\section{Validation-free Evaluations}
\label{sec:more-validation-free}

We present the full results of validation-free evaluation for cross-manipulation deepfake detection. 
We ran the public code of Xception~\cite{xception}, RECCE~\cite{recce} and UIA-ViT~\cite{uia} to obtain their results under our validation-free setting. In specific, we train all models for exactly $15k$ iterations, which is long enough for them to reach peak validation performances and continue training for a while. We then use the final model to directly test on all subsets of FF. Results are reported in~\cref{tab:validation-free-cross-manipulation}.

We emphasize that this setting more closely resembles realistic scenarios, where validation sets are generally not available. Under this setting, we outperform the compared state-of-the-art methods by large margins of $7.46\%$, $8.64\%$ and $5.46\%$ when trained on DF, F2F and FSW respectively, and exhibit competitive performance on NT as well. Note that although we do not use validation sets, our results are not significantly weakened. We observe an only $2.05\%$ average decrease compared to evaluating with validation sets, which is reported in~\cref{tab:cross-manipulation} of the main paper. This comprehensive result demonstrates an impressive ability of RFFR-based deepfake detectors to avoid overfitting and be applied effectively for practical deepfake detection. 
\begin{table}[h]
\setlength\tabcolsep{4.5pt} 
\caption{Validation-free cross-manipulation performances in terms of AUC(\%). Classifiers are trained on one subset of FF and tested on all four subsets. \emph{No validation set is used for model selection.} Intra-domain results are marked in gray.}
\vspace{-1em}

\label{tab:validation-free-cross-manipulation}
\begin{center}  
\scalebox{0.8}{
\begin{tabular}{c|l|cccc|c}
\toprule
Training &\multirow{2}*{Method} & \multicolumn{4}{c|}{Test data} & \multirow{2}*{Avg} \\
\cmidrule(lr){3-6}
     data  &            ~                   & DF    & F2F   & FSW   & NT    & ~   \\
    \midrule
\multirow{4}*{DF}  & Xception~\cite{xception} &\cellcolor{Gray}\textbf{99.61} & 57.17 & 25.99 & 62.52 & 61.32 \\
       & RECCE~\cite{recce}       & \cellcolor{Gray}99.51 & 66.29 & 40.38 & \textbf{74.57} & 70.19 \\
       & UIA-ViT~\cite{uia}       & \cellcolor{Gray}99.37 & 62.86 & 54.54 & 65.10 & 70.47 \\
       & Ours                                  & \cellcolor{Gray}99.30  & \textbf{73.45} & \textbf{67.52} & 71.45 & \textbf{77.93} \\
\midrule
\multirow{4}*{F2F} & Xception~\cite{xception} & 83.08 & \cellcolor{Gray}99.12 & 46.63 & 64.93 & 73.44 \\
       & RECCE~\cite{recce}       & 74.51 & \cellcolor{Gray}99.22 & 50.17 & 59.46 & 70.84 \\
       & UIA-ViT~\cite{uia}        & 83.95 & \cellcolor{Gray}99.01 & 61.86 & 63.97 & 77.20 \\
       & Ours                                  & \textbf{91.56} & \cellcolor{Gray}\textbf{99.38} & \textbf{76.01} & \textbf{76.41} & \textbf{85.84} \\
\midrule
\multirow{4}*{FSW} & Xception~\cite{xception} & 53.31 & 57.48 & \cellcolor{Gray}\textbf{99.72} & 44.56 & 63.77 \\
       & RECCE~\cite{recce}       & 49.85 & 65.77 & \cellcolor{Gray}99.68 & 55.95 & 67.81 \\
       & UIA-ViT~\cite{uia}        & 79.33 & 65.60 & \cellcolor{Gray}99.23 & 50.90 & 73.77 \\
       & Ours                                  & \textbf{85.24} & \textbf{75.14} & \cellcolor{Gray}99.49 & \textbf{57.06} & \textbf{79.23} \\
\midrule
\multirow{4}*{NT}  & Xception~\cite{xception} & \textbf{90.83} & 68.68 & 38.45 &\cellcolor{Gray}\textbf{97.11} & 73.77 \\
       & RECCE~\cite{recce}       & 86.98 & 72.20 & \textbf{51.10} & \cellcolor{Gray}97.06 & \textbf{76.84} \\
       & UIA-ViT~\cite{uia}        & 78.98 & 64.80 & 44.55 & \cellcolor{Gray}95.62 & 70.99 \\
       & Ours                                  & 81.09 & \textbf{73.59} & 50.40  & \cellcolor{Gray}95.84 & 75.23 \\
\bottomrule
\end{tabular}}
\vspace{-1em}
\end{center}
\end{table}

\section{Additional Ablation Study}
\subsection{Ablation Study for Residuals}
\label{sec:ablation_residuals}

We use a pretrained MAE to obtain residual images that signal potential artifacts. In this section, we perform ablation study for the residuals, where we compare our method with other residual generation techniques. All models are trained on F2F and tested on FSW. As a baseline, we train a single-branch ViT that learns without any residuals, and only accepts random original image blocks. Subsequently, we train three dual-branch ViTs that accepts different residuals. In addition to our residual generated by MAE, we propose two other options of residual generation. We present the results in \cref{tab:unet}.

As mentioned in the paper, we use a UNet-based autoencoder trained with real faces to produce similar residual images by subtracting the reconstructed images from the originals. We show that these residual blocks cause a slight decrease ($-0.39\%$) in generalization performance. This is likely due to perfect reconstructions that leave both real and fake residual images with no information to exploit.

We also explore high-pass filters, another potential source of residual images. In some cases, directly applying high-pass filters on deepfake images yields visually similar results to our MIM-based residuals. However, they tend to treat all image regions equally and fail to expose artifacts that are distinct from the rest of the image. In our experiment, we show that while high-pass filtered images improve upon the baseline, the improvement is marginal ($0.79\%$) compared to our MIM-based residuals.

Finally, we show that our design of MIM-based residuals brings about a $7.86\%$ improvement in performance, significantly outperforming compared residual generation methods. As demonstrated in the main paper, the MIM-based method effectively differentiates between the processing of real and fake samples and successfully highlights potential forgery patterns in its residuals. Therefore, it makes a substantial contribution to the generalization performance of deepfake detectors.

\begin{table}[]
\setlength\tabcolsep{4.5pt} 
\caption{Deepfake detection performances of different residuals. Classifiers are trained on F2F and tested on FSW. We present the detection results in AUC (\%). }
\vspace{-1em}
\label{tab:unet}
\begin{center}
\begin{tabular}{c | c | c}
\toprule
Training data & Residuals & Test AUC (\%)\\
\midrule
\multirow{4}*{F2F} & None & 70.76 \\
& Autoencoder & 70.37 \\
& High-pass filter & 71.55 \\
& MIM (Ours) & \textbf{78.62} \\
\bottomrule
\end{tabular}
\vspace{-1em}

\end{center}
\end{table}

\subsection{Ablation Study for Block Sizes}
\label{sec:ablation_block}

To perform masked image modeling, we split each image into $k\times k$ blocks and inpaint one block at a time. Selecting an appropriate block size requires balancing performance and efficiency. Large blocks hinder deepfake detection with increased noise due to the difficulty to accurately inpaint. Small blocks cause longer inference time with more forward passes required to complete a reconstruction. To strike a balance, we opt for $k = 4$ for optimal detection performance and efficient inference. We present deepfake detection results of different block sizes in~\cref{tab:block}. Note that smaller blocks does not improve detection, but significantly prolongs inference time.

\begin{table}[!h]
\setlength\tabcolsep{4.5pt} 
\caption{Comparing performances and inference time of different block sizes. Models trained on F2F. Results in AUC(\%)}
\label{tab:block}
\vspace{-1em}
\begin{center}
\scalebox{0.8}{
\begin{tabular}{c|ccccc|c}
\toprule
Split &  DF & F2F & FSW & NT & Avg & Inference Time \\ 
\midrule
$2 \times 2$ & $91.44$ & $98.07$ & $74.14$ & $75.67$ & $84.83$ & $4\times$ MAE Inference \\ 
$4 \times 4$ & $\textbf{93.44}$ & $\textbf{99.61}$ & $78.62$ & $\textbf{79.56}$ & $\textbf{87.81}$ & $16 \times$ MAE Inference\\
$6 \times 6$ & $93.08$ & $99.18$ & $\textbf{79.34}$ & $78.58$ & $87.55$ & $36 \times$ MAE Inference\\
\bottomrule
\end{tabular}}
\vspace{-1em}
\end{center}
\end{table}

