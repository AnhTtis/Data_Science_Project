
\setlength{\abovedisplayskip}{5pt} % 对公式上下间隔有用
\setlength{\belowdisplayskip}{5pt}


\section{Method}
\label{sec:method}

\subsection{Overview}

Deepfake detectors suffer from severe performance drops when applied to unseen manipulations, because artifacts of specific manipulations are hardly generalizable features. At the same time,  it is almost impossible to collect sufficient representative fake images to ensure model awareness of all possible manipulations, considering the diversity and ever-developing nature of deepfakes. Therefore, instead of focusing on the fake faces, we set out to learn an accurate real face distribution, and aim at identifying fake faces that violate the boundary of this distribution. In this work, we propose Real Face Foundation Representation Learning (RFFR) to achieve this purpose. We attempt to learn rich representations from large-scale real face datasets, so the model is able to identify fake faces outside the distribution of RFFR.

As shown in \cref{fig:pipeline}, we use real faces to train a model by masked image modeling (MIM), which masks a random block from an input image and learns to recover it based on the rest of the image. The trained model inpaints masked real faces well, but it tends to fail at inpainting fake faces based on the representations learned with real faces. This creates a discrepancy between the input and output in the form of a residual image block. We train a classifier with residual image blocks and corresponding original image blocks to perform deepfake detection. 

\subsection{Real Face Foundation Representation Learning}

\begin{figure*}
\vspace{-2em}
\begin{center}
   \includegraphics[width=1\linewidth]{figs/Figure_3_ICCV_Final.pdf}
\end{center}
\vspace{-2em}
   \caption{Visualizations of processing real and fake samples with our RFFR-learning inpainting model. Clear artifacts in the original fake images (first row) are effectively removed in the reconstructed images (second row) and highlighed in the residual images (third row). Owing to an inevitable information loss during inpainting, the real residuals on the left are not completely empty, but they are in contrast with the fake ones that indicate clear forgery patterns.}
\vspace{-1.5em}
\label{fig:residual}
\end{figure*}


We leverage MIM to train a representation learning model with real faces to learn the RFFR. Given a locally masked real face image, the model learns to reconstruct the masked region based on the visible region of this image. Specifically, given a real face image $X^i$,  we divide it into a grid of $k\times k$ image blocks with a division rule $g$, where each block is to be individually masked and restored later:
\begin{equation}
  B_1^i, B_2^i, \cdots, B_{k\times k}^i  = g(X^i).
  \label{eq:divide}
\end{equation}
The MIM model $M$ accepts an input image $X^i$ along with a randomly selected mask $m_j$, and reconstructs the masked block to obtain $\hat{B^i}$:
\begin{equation}
  \hat{B^i} = M(X^i, m_j),
  \label{eq:inpaint}
\end{equation}
where we ensure the $j$-th mask covers the $j$-th block of an image. For coordinate $(x, y)$,
\begin{equation}
  m_j (x, y) = \left\{
  \begin{aligned}
  0, \ \ \ &(x, y) \in B_j	\\
  1, \ \ \ &otherwise	\\
  \end{aligned}
  \right..
  \label{eq:mask}
\end{equation}Finally, this representation model is trained with a simple $L_2$ loss
\begin{equation}
  L_{rep} = \sum_{i=1}^n||\hat{B^i_j} - B^i_j||_2,
  \label{eq:l2}
\end{equation}
so that the model learns to inpaint masked images with supervisory signal from the images themselves.

By learning to inpaint any random region of real face images, the model is expected to comprehensively learn real face representations, or RFFR. When provided with a masked real face image, the model infers the representation of the face and decodes this representation by generating a reasonable image block to fill in the masked region. Upon processing fake samples, this model tends to infer a real face representation based on its input and use real textures to restore masked regions of the samples. This creates a large discrepancy between the input and output of the model, which signals low-level artifacts present in the fake images. 


In \cref{fig:residual}, we visualize both real and fake samples processed by our RFFR-learning inpainting model. For each image, we iteratively mask all $k\times k$ blocks, restore them with the inpainting model, and collect all output blocks to assemble a whole reconstructed image. We subsequently subtract the original image from the reconstructed image to obtain the residual image (amplified for better visibility). The reconstructed faces closely resemble the original faces with successful reconstructions of their high-level facial attributes. This preservation of high-level semantic information benefits from the learning of RFFR. However, low-level artifacts in the fake images, many clear enough to be observable on the foreheads, eyes, mouths, \etc, are effectively removed, as they are not represented in the distribution of RFFR. This also results in highlights in the residual images, which provides essential information to guide deepfake detection. 


\subsection{Deepfake Detection}
\label{sec:method_deepfake_detection}
The previous section guarantees that we have a compact distribution of real faces, which can be used to reveal low-level artifacts outside the distribution of RFFR. This revelation is instantiated by the residual images we describe above. In this section, we detail the process of leveraging residual images generated with RFFR to train a deepfake detector. 

Formally, we obtain residual image blocks by subtracting the original block from the block generated with RFFR:
\begin{equation}
  R_j^i = \alpha(\hat{B_j^i} - B_j^i),
  \label{eq:difference}
\end{equation}
where we use a constant factor $\alpha$ to amplify the subtle signals in the residuals and match its scale to that of natural images.

Unlike visualizations in \cref{fig:residual}, we do not generate the whole residual image for training the classifier. Instead, we develop a random input mechanism, which randomly selects a subset of all image blocks to enter the classifier. Every block in an image is selected with a pre-determined probability $p$. Upon selecting block $B_j^i$ for input, we invoke \cref{eq:inpaint} and \cref{eq:difference} to generate corresponding residual image block $R_j^i$. This process is repeated until we make the decision for each block whether to utilize it for classification or not. Eventually, we obtain residual image blocks $ \{R_j^i\}_{j = k_1, k_2, \cdots}$ and their corresponding original image blocks $\{B_j^i\}_{j = k_1, k_2, \cdots}$, where $k_1, k_2, \cdots$ are the indexes of all selected blocks. 

This random input mechanism benefits our deepfake detector in two ways. Firstly, compared to random input, a complete reconstruction carried out by an inpainting model is very time-consuming. To obtain residual images in the form of~\cref{fig:residual}, we need a total of $k\times k$ inferences for every batch of images. This seriously prolongs the training process. Secondly, random input improves generalization, as we show in the ablation study. We hypothesize that learning with randomly selected blocks reduces overfitting in that the model is forced to learn from different locations. A model that accepts full images tends to focus on the most suspicious regions of artifacts. By only providing a subset of all image blocks to the classifier, we, in effect, mask out the rest of the blocks. The model then learns with artifacts from random locations, which could be too subtle to be spotted by the model when prominent artifacts are within the input image~\cite{rfm, gfsl}. This allows the model to form a complete set of feature revealed with RFFR, thus improving generalization.

With both sets of image blocks collected, we integrate them as input to enter a classifier $F$ to perform deepfake detection. Two Vision Transformers (ViTs)~\cite{vit} are adopted to form a dual-branch classifier, with each accepting one set of image blocks. They jointly produce one prediction $\hat{Y^i}$:

\begin{equation}
  \hat{Y^i} = F(R_{k_1}^i, R_{k_2}^i, \cdots ; B_{k_1}^i, B_{k_2}^i, \cdots).
  \label{eq:detect}
\end{equation}
As we use ViTs for training, the blocks are further broken down into smaller patches to enter the network. In addition, each patch is aided with its own position embedding, which helps the model better identify artifacts in images based on their specific locations. At the end of processing, each ViT branch generates a class token, and the two tokens are merged to create a final feature for the current input image and subsequently a prediction. 

Finally, given the ground truth $Y^i$s, we train the deepfake detector with a simple classification loss:

\begin{equation}
  L_{cls} = -\sum_{i=1}^n Y^i log(\hat{Y^i})
  \label{eq:ce}
\end{equation}