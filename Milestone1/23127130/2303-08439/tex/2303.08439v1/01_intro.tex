\section{Introduction}
\label{sec:intro}

Deepfake technologies~\cite{deepfakes} have become a growing concern of the society. Deepfake algorithms from recent studies~\cite{faceshifter, megapix, firstordermotion} are able to create increasingly realistic face images and videos, which can be maliciously used to spread misinformation. To tackle this issue, much effort has been put into the detection of deepfakes~\cite{ff, celeb-df, blink, headposes, va, forensictransfer}. While there exists some promising results in detecting particular manipulations~\cite{ff}, recent studies reveals that the performance of existing detection models drops drastically when presented with images manipulated with unseen methods~\cite{xray, celeb-df}. This deems generalization a major challenge for deepfake detection, as it is difficult to determine manipulation methods when suspicious images emerge in practice. 

\begin{figure}
\begin{center}
   \includegraphics[width=1\linewidth]{figs/Figure_1_ICCV_Final.png}
\end{center}
	\vspace{-1.5em}
   \caption{The downstream task of deepfake detection consistently achieves better generalization performance with the use of RFFR, and this performance can be further improved by training RFFR with additional real face datasets.}
   \vspace{-1.5em}
\label{fig:extra_data}
\end{figure}

A number of studies in recent years devote to performing generalized deepfake detection~\cite{xray, lip, realforensics, f3net, gfsl, recce, multiatt, sola, sladd, sbi, dcl, uia} where a model is required to identify face images or videos manipulated with methods unseen during training. Though some progress are made, existing results on cross-manipulation tests are still far from satisfying. One major problem is that the models are generally trained on a limited source of fake data~\cite{ff, celeb-df, dfdc} and learn features that may not be adaptable to unseen manipulations. 

The inherent inaccessibility of sufficient fake face images limits the ability of models to identify fake images in all possible forms. Each deepfake algorithm leaves different traces on their creations, and the sheer number of existing algorithms denies the possibility of collecting enough representative types of manipulated images. However, real faces are considered to have a well-defined distribution without potentially unknown forms. We intend to focus on learning the distribution of the real face images, and treat fake faces as anomalies with rare occurrences~\cite{ADReview}. The priority of such a model is to gain sufficient knowledge of real faces, such as their structure and local texture, and be able to identify fake samples that deviate from the real face distribution. 

In this study, we propose Real Face Foundation Representation Learning (RFFR), which aims at learning the representation of real faces and detect the fake faces outside the distribution of RFFR. Specifically, we use a large number of real face images to train a model by masked image modeling (MIM)~\cite{simmim}. This model inpaints partially masked images with information from visible regions. By training exclusively on real faces, it is expected to implicitly learn the representation of real faces, or RFFR, and then reconstruct the masked regions based on its knowledge of the structure and texture of real faces. Applying this model to locally masked real images tends to result in a faithful reconstruction. On the other hand, since the model infers masked contents based on RFFR, it restores forged images into artifacts-free images with differences between its input and output. Such a discrepancy allows us to use a simple difference operation between the original masked image region and its recovered version to find a residual image, which signals the presence of potential anomalies in the region. 

Lastly, we combine residual and original image blocks as input, and train another dual-branch Vision Transformer~\cite{vit} as a deepfake detector. With the support of RFFR, our classifier demonstrates impressive abilities of generalizing to unseen fake images and avoiding severe overfitting during training. Additionally, we showcase a remarkable scalability of this framework, where representation learning aided by extra real faces consistently enhances downstream generalization, as illustrated in  ~\cref{fig:extra_data}.

We summarize our contribution as follows:

(1)	We propose to learn the distribution of real face images with Real Face Foundation Representation Learning (RFFR), which aims at improving downstream tasks with the learned representation. 

(2) We show the residual images obtained by inferring with our RFFR model highlights potential artifacts left by face manipulation algorithms, and therefore facilitates the identification of deepfake images.

(3) Extensive experiments on cross-manipulation and cross-dataset benchmarks for deepfake detection show that RFFR brings about a scalable generalization performance as well as remarkable resistance to overfitting, and significantly outperforms the state-of-the-art methods. 

