
\documentclass[journal]{IEEEtran}
\usepackage{ifpdf}
\usepackage{cite}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{url}
\usepackage{multirow}
\usepackage[backref]{hyperref}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Crowd Counting with Online Knowledge Learning}

% Strong-cues boosting online distillation for crowd counting

% High-capacity teacher guided online distillation for crowd counting
% Privileged teacher guided online distillation for crowd counting
% Strong-cues boosting online distillation for crowd counting


% \iffalse â€¦. \fi
% \iffalse
\author{Shengqin Jiang, Bowen Li, Fengna Cheng,  Qingshan Liu,~\IEEEmembership{Senior Member, IEEE}
       

%\thanks{This work is done when the first author visits the University of Adelaide.}% <-this % stops a space

\thanks{Manuscript received *** **, 2023; revised *** **, 2023. (\it{Corresponding author: Qingshan Liu.})} % <-this % stops a space
% This work is supported by the National Natural Science Foundation of China (Nos.62001237 and 61902092), Jiangsu Planned Projects for Postdoctoral Research Funds(No.2021K052A), China Postdoctoral Science Foundation Funded Project (No. 2021M701756) and the Startup Foundation for Introducing Talent of NUIST (No.2020r084)

\thanks{S. Jiang, B. Li and Q. Liu with the Jiangsu Key Lab of Big Data Analysis Technology (B-DAT), School of Computer, Nanjing University of Information Science and Technology, Nanjing 210044, China (e-mail: jiangshengqin@126.com; jslibowen@126.com; qsliu@nuist.edu.cn)}
\thanks{F. Cheng  is with College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, China (e-mail: cfn1218@163.com)} 
}% <-this % stops a space
% \fi

% % The paper headers
\markboth{IEEE Transactions on XXX}   %{IEEE Transactions on xxxxxxx }
{Jiang \MakeLowercase{\textit{et al.}}:Crowd Counting with Online Knowledge Learning}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
	% edge computing and mobile devices

Efficient crowd counting models are urgently required for the applications in scenarios with limited computing resources, such as edge computing and mobile devices. A straightforward method to achieve this is knowledge distillation (KD), which involves using a trained teacher network to guide the training of a student network. However, this traditional two-phase training method can be time-consuming, particularly for large datasets, and it is also challenging for the student network to mimic the learning process of the teacher network. To overcome these challenges, we propose an online knowledge learning method for crowd counting. Our method builds an end-to-end training framework that integrates two independent networks into a single architecture, which consists of a shared shallow module, a teacher branch, and a student branch. This approach is more efficient than the two-stage training technique of traditional KD. Moreover, we propose a  feature relation distillation method which allows the student branch to more effectively comprehend the evolution of inter-layer features by constructing a new inter-layer relationship matrix. It is combined with response distillation and feature internal distillation to enhance the transfer of mutually complementary information from the teacher branch to the student branch. Extensive experiments on four challenging crowd counting datasets demonstrate the effectiveness of our method which achieves comparable performance to state-of-the-art methods despite using far fewer parameters.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
crowd counting; knowledge distillation; online training; feature relation
\end{IEEEkeywords}
\IEEEpeerreviewmaketitle

% At the same time, we hold the opinion that our results will provide further impetus to the hardware architecture community to customize the next generation of deep learning accelerator architectures to efficiently handle sparse matrix storage and computations. 

\section{Introduction}
\IEEEPARstart{C}{rowd} Counting is a task that aims at automatically estimating the number of people in images or videos. It is a challenging task, as it involves dealing with varying densities, heavy occlusions, large scales, and complicated scenery. Due to its numerous practical applications, such as video surveillance, traffic management, and production forecasting, it has recently gained a lot of interest from both academia and industry.


% Thanks to the superior performance of convolutional neural network (CNN) in visual tasks, it has become the mainstream model in the field of crowd counting. MCNN~\cite{zhang2016single} considered using a Multi-column CNN to learn the target density maps in order to handle the varying density. This network, albeit lightweight, is challenging to estimate high-density situations effectively. To address this issue, CP CNN~\cite{sindagi2017generating} realized accurate crowd prediction by mining local and global context information. Later,  CSRNet~\cite{li2018csrnet} built on the first 10 layers as the front-end and a dilated CNN for the back-end to learn the scale variation. More recently, some well-designed models are committed to extracting more powerful features by developing specific yet complicated structures~\cite{shi2019counting, liu2019crowd}. These elaborately built networks give some performance benefits, but they have to face the problems of high computational complexity and long reasoning time. However, in practical applications, such as edge computing and mobile terminal, we have to contend with limited processing resources, which makes the use of those robust networks difficult.

Convolutional neural networks (CNNs) have demonstrated outstanding performance in visual tasks, leading to their widespread exploration in the field of crowd counting. Multi-column CNNs, as used in MCNN~\cite{zhang2016single}, have been employed to learn target density maps and handle varying crowd densities. However, these networks struggle to estimate high-density cases effectively. To overcome this limitation, Sindagi and Patel~\cite{sindagi2017generating} proposed CP CNN, which leverages local and global contextual information for accurate crowd prediction. CSRNet~\cite{li2018csrnet} used the first 10 layers as the front-end and dilated convolution layers as the back-end to learn scale variation. Recent models, such as SANet~\cite{cao2018scale} and CAN~\cite{liu2019context}, have incorporated some specific yet complicated structures to extract more powerful features. While these models offer performance benefits, they suffer from high computational complexity and long inference time. This makes them challenging to use in practical applications, such as edge computing and mobile terminals, where processing resources are limited.





% This hinders the application of 3D CNNs in practical scenarios and raises an open problem of how to design lightweight 3D CNNs for real-time video classification

% the soft targets from deeper models do not often serve as good cues for the shallower models due to the gap of compatibility. cite{MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation}
%
%
%
\begin{figure}[!tb] 
	\centering
	\includegraphics[scale=0.55]{./fig1.png}
	\caption{Comparison of distillation pipeline (traditional method vs. our method). (a) The traditional method involves first training a teacher network, which is then followed by student network training; (b) Our method employs a distillation mechanism to jointly train teacher and student branches.}
	\label{fig:fig0}
\end{figure} 


Numerous strategies have been proposed to develop small yet efficient models with comparable performance to larger models, including pruning~\cite{cai2017deep, rao2018runtime} and quantization~\cite{park2018value}. However, most of these methods require meticulous hyperparameter tuning or specialized hardware platforms~\cite{liu2020efficient,liu2020metadistiller}. As an alternative, knowledge distillation (KD) makes use of supervised information from a superior-performing large model (teacher) to train a smaller model (student)~\cite{hinton2015distilling, zhang2018deep, zhang2019your, hou2019learning}, as shown in Fig.~\ref{fig:fig0} (a). While this technique enhances the performance of the student model~\cite{dai2021general,liu2020efficient}, it also typically requires additional training time and may hinder the ability of the student model to truly master the learning process of the teacher model. Self-distillation is a potential solution to this problem, as it has been successfully applied to image classification~\cite{yang2019snapshot, mirzadeh2020improved, liu2020metadistiller}. However, applying it directly to high-density crowd counting tasks is challenging due to the difficulty in creating efficient soft targets for lightweight branching and achieving effective knowledge transfer.




% To date,  lots of strategies, including as pruning~\cite{cai2017deep, rao2018runtime} and quantization~\cite{park2018value}, have been proposed to achieve a small and efficient model with comparable performance to a large one. Nevertheless, the majority of them require either a tedious hyper-parameters search or a specialized hardware platform~\cite{liu2020efficient,liu2020metadistiller}. As an alternative, knowledge distillation (KD) employs the supervisory information of the big model with superior performance to train a small model, resulting in improved performance and accuracy~\cite{hinton2015distilling, zhang2018deep, zhang2019your, hou2019learning}. Traditionally, the teacher model is required to be trained in advance before being transferred to a small model~\cite{dai2021general,liu2020efficient}, as shown in Fig.~\ref{fig:fig0} (a). This indicates that the two-phase training method will take a lot of time, particularly for training a teacher model. This poses an unanswered question for crowd counting: how could an efficient one-phase network be designed to accomplish high-quality information transfer? Although some self distillation work has been applied to image classification~\cite{yang2019snapshot, mirzadeh2020improved, liu2020metadistiller}, applying it directly to high-density crowd counting task is very challenging. There are two reasons for this: the first is how to create soft targets that give high-quality cues for the lightweight branch; the second is how to efficiently transfer knowledge.


To address the aforementioned challenges, we propose a solution  in the form of an online knowledge learning network for crowd counting. This framework, depicted in Fig.~\ref{fig:fig0} (b), is an end-to-end knowledge distillation approach, which avoids the need for two-phase training methods. It consists of a shared shallow module, a teacher branch, and a student branch. The student branch employs approximately 1/4 of the channels of the teacher branch to reduce computational complexity. It is important to note that the pre-trained model initializes the shared shallow module and teacher branch to gain adequate domain-specific knowledge. To facilitate knowledge transfer between the two branches, we propose a  feature relation distillation method. It enables the student branch to better understand the evolution of inter-layer features with the help of a newly constructed inter-layer relationship matrix. It complements response distillation as well as feature internal distillation in order to help the student branch out to understand the a prior knowledge of the teacher branch from a different perspective. We conduct experiments on several challenging datasets, demonstrating that our approach significantly improves the performance of the student branch. Specifically, the student branch surpasses the teacher branch on three large-scale datasets, achieving comparable results with some state-of-the-art methods.



% To address the above issues, we propose an online knowledge learning network for crowd counting. Instead of two-phase training method, it is an end-to-end knowledge distillation framework as shown in Fig.~\ref{fig:fig0} (b). It is made up of shared shallow module, teacher branch and student branch, where the student branch uses around 1/4 of the channels of the teacher branch to avoid heavy computation. It should be emphasized that the pre-trained model is used to initialize the shared shallow module and teacher branch with aim to obtain sufficient specific knowledge. We built three different types of knowledge transfer at the same time to promote knowledge transfer between the two branches. In particular, feature internal distillation directly realizes the transfer of features at the same level, feature relation distillation investigates the transfer of relationships between different feature layers of the network, and response distillation provides some regularization for the output. Experiments conducted on several challenging datasets show that the proposed method improves the student network by a significant margin. In particular, the student network outperforms the teacher network on three large-scale datasets, achieving comparable results with some state-of-the-art methods. 

\begin{itemize}
\item To the best of our knowledge, we at the first time propose an end-to-end online knowledge distillation framework for crowd counting, which can significantly reduce the training time, especially for large-scale datasets.  

\item We propose a new feature relation distillation method to assist the student branch in better understanding the evolution of inter-layer features by distilling the inter-layer relationship matrix from both branches. 


\item To effectively achieve knowledge transfer for this framework, three kinds of knowledge distillation are explored including feature internal distillation, feature relation distillation and response distillation. 


\item Extensive experiments on four benchmarks show the effectiveness of our framework. Moreover, our distilled model achieves comparable results to some state-of-the-art methods, despite having limited parameters.
\end{itemize}



\section{Related Works}

In this section, we will review some works related to our model. In what follows, the CNN-based crowd counting will be introduced first. Then we will go through some recent efforts on knowledge distillation.

\subsection{Crowd Counting}


The early methods for crowd counting relied on detection-based approaches, which applied object detection techniques to identify the entire person or head in the image and label the target with boxes to determine the final counting result. Although detection-based approaches perform well in sparse settings, they may result in significant counting inaccuracies in highly dense crowds. 


To address this challenge, density-based methods have been introduced to reduce the difficulty of counting prediction. For instance, Zhang et al.\cite{zhang2016single} suggested generating a density map using Gaussian kernels for each input, which was then used for regression. This method has been shown to effectively mitigate the challenge of detecting densely populated regions, resulting in accurate crowd count prediction and density perception of its distribution.  Following this, CSRNet\cite{li2018csrnet} utilized dilated convolution layers to expand the receptive field, resulting in a significant improvement in the prediction performance of the network. Several other approaches have been proposed to enhance network performance, including ~\cite{jiang2019mask}, ~\cite{jiang2020attention}, and ~\cite{yang2020embedding}. However, these methods have more complex structural designs and heavier parameters, which could potentially impede efficient reasoning abilities.





%Early works on crowd counting rely on detection-based approaches. So far, several object detection techniques (e.g.,  Fast R-CNN~\cite{girshick2015fast}, YOLO~\cite{redmon2016you}) have been applied. They typically identify the entire person or head in the image and label the target with boxes, with the number of boxes determining the final counting result. In sparse settings, detection-based approaches often perform well. They will, however, make huge counting inaccuracies in highly dense crowds.

% As an alternative, density-based methods are put forward to reduce the difficulty of counting prediction. Zhang et al.~\cite{zhang2016single} created a multi-column CNN to solve the scale change problem that employs three branches with varying kernel sizes. This model achieves remarkable performance, but when the network goes deeper, a substantial amount of training time and unsatisfactory accuracy hinder the flexibility of this structure.  To this end, CSRNet~\cite{li2018csrnet} adopts the dilated convolution layers to expand the receptive field. The prediction performance of the network is greatly improved with the aid of the pre-trained VGG model. ~\cite{jiang2019mask} put forward using a specialized network branch to predict the object/non-object mask, which would then be combined with the input image to generate the density map. ~\cite{jiang2020attention} introduced a CNN model based on attention scaling that uses attention masks and scaling factors to adjust density estimations in different density levels. ~\cite{yang2020embedding} merged the perspective analysis approach with the counting CNN to properly address the scale variations. The approaches described above have resulted in good network performance. However, the more complicated structural design and heavier parameters have a negative impact on the efficient reasoning of these models.

\subsection{Knowledge Distillation}

Knowledge distillation (KD) is a compression technique that transfers the knowledge from a large pre-trained model to a compact model. Hinton et al.~\cite{hinton2015distilling} first proposed to use the output of a well-trained model as the supervised signal to assist the training of student network. ~\cite{romero2014fitnets} utilized both the outputs and intermediate representations of teacher as cues to improve the performance of student. ~\cite{sau2016deep} proposed a noise-based regularization method to strength the robustness of student.  SKT~\cite{liu2020efficient} exploited the structured knowledge of a well-optimized teacher to build a lightweight student. All of these strategies, however, go through the time-consuming two-phase training process. That is, before directing the learning of a student model, a high-capacity teacher model must be trained.

To sidestep this issue, online KD is put forward to distill the network itself~\cite{yang2019snapshot, mirzadeh2020improved, liu2020metadistiller}. ~\cite{yang2019snapshot} leverages the previous iteration's outputs as soft targets, but this risks increasing the mistake in the learning process, and deciding which iteration to use as a teacher is difficult. ~\cite{zhang2019your} proposed a self-distillation framework in which knowledge from the deeper layers can guide the learning of shallow layers.  ~\cite{mirzadeh2020improved} put forwards a multi-step knowledge distillation by using a teacher assistant to bridge the gap between student and teacher networks.  ~\cite{liu2020metadistiller} built an improved soft targets for the intermediate output layers by top-down fusing feature maps from deep layers with a label generator. While these works do decrease the duration needed for two-phase knowledge distillation, they fail to consider the inter-feature information. Moreover,  these studies solely address relatively straightforward classification tasks, rather than the more complex challenges of high-density prediction.


% Hou et al.~\cite{hou2019learning} distill the shallow layers of the network under the guidance of attention map.\par

% Most of these knowledge distillation frameworks mentioned above are performed in classification area, and it stll lacks exploration in the field of crowd counting.


% 


\section{Method}

In this section, we introduce our proposed online knowledge learning method for crowd counting, which uses a one-stage training paradigm and effectively transfers knowledge. As depicted in Fig.~\ref{fig:fig1}, the network framework comprises a teacher branch and a student branch. The teacher branch is rich in experience and can extract useful information from training data, while the student branch has fewer parameters and faster reasoning speed. We will begin by presenting our network architecture and then proceed to explain how we distill knowledge from the teacher branch to the student branch.

% In this section, we first propose an online knowledge learning method for crowd counting that uses one stage training paradigm to effectively transfer knowledge, as show in Fig.~\ref{fig:fig1}. Specifically, our framework includes a teacher branch and a student branch. Teacher branch is rich in experience and can efficiently extract effective information from data, while student branch has less parameters and faster reasoning speed.  In what follows, we will start by introducing our network architecture, then go over how to distill knowledge from teacher branch to student branch.


\begin{figure*}[!t]
	\centering
    \includegraphics[scale=0.7]{./fig2.png}
	\caption{An overview of the proposed online distillation network. It consists of shared shallow module, teacher branch and student branch. Meanwhile, feature distillation, relation distillation and response distillation are introduced to guide the knowledge learning from teacher branch to student branch.}
	\label{fig:fig1}
\end{figure*} 

\subsection{Network Architecture}


Figure ~\ref{fig:fig1} illustrates the network structure, which is composed of a shared shallow module, a teacher branch, and a student branch. In particular, the input image is initially processed by the shared shallow module to extract base features before being forwarded to the two branches. Both the teacher branch and the student branch output their own density maps, with the former serving as a mentor to the latter during training.



% As shown in Fig.~\ref{fig:fig1}, the overall network is make up of shared shallow module, teacher branch and student branch. In particular, an input image is passed into a shared shallow module before being processed further through two branches. The two branches output a density map respectively, with the teacher branch serving as a mentor to the student branch during their studying.


\subsubsection{Shared Shallow Module}

The early layers of 2D CNNs are commonly used to extract low-level features such as edges and corners. Here, we design a shared two-layer convolutional module for extracting spatial patterns. Specifically, we adopt the first two layers of VGG-16~\cite{simonyan2014very} for this purpose, as they are effective and widely used in various tasks. This module consists of two $3\times 3$ convolutions followed by max-pooling with a stride of (2, 2).

Since the teacher branch requires a lot of experience (please refer to the subsection \ref{teabranch} for details), the starting point of the module is to keep the network structure consistent with the shallow layer of the pre-trained model. In light of this, we initialize this module using the trained parameters. It is important to note that the shared module also serves as the initial layers of the student branch, so it should not have too many parameters. In other words, we should avoid introducing too many shallow layers in the teacher branch when designing the shared module.

% Since our design involves a two-branch structure, with the teacher branch requiring extensive expertise, we use the initial layers of the trained model as the shared shallow module for easy initialization of the teacher branch. It is important to note that the shared module also serves as the initial layers of the student branch, so it should not have too many parameters. In other words, we should avoid introducing too many shallow layers in the teacher network when designing the shared module.




% Early layers of 2D CNNs are known to extract low-level features such as edges and corners. Here, we design a shared two-layer convolutional module to extract spatial patterns. Without losing generality, we utilize the first two layers of VGG-16~\cite{simonyan2014very} as this module. Specifically, two $ 3\times 3$ convolutions are employed before max-pooling with a (2, 2) stride. This is due to the fact that we employ a two-branch design, with the teacher branch requiring extensive expertise. To this purpose, the first few layers of trained model should be placed in the shared shallow module for the easy initialization of teacher branch. Moreover, it should be emphasized that because this module is also the initial few layers of student branch, it should not have too many parameters. To put it another way, don't introduce too many shallow layers in the teacher network when designing such a module.


\subsubsection{Teacher branch} \label{teabranch}

% In addition to the first two layers in the shared shallow module, we use the remaining layers of the first 10 layers of VGG-16 as the frond-end of teacher branch. In this way, we are able to utilize the pre-trained model, as it has a good transfer learning capacity and a flexible design. Thus, this branch is employed as a teacher with specific experience who has the capacity to learn new information rapidly and the ability to guide students. To further improve the characterization capabilities of teacher branch, we use a structure similar to CSRNet~\cite{li2018csrnet}, in which the back-end is deployed by dilated convolutional layers. This type of convolution enhances the network's receptive field directly without increasing its parameters or computation complexity. Specifically, we use 3 dilated convolution layers, which is a light version of CSRNet.
In addition to using the first two layers of VGG-16 in the shared shallow module, we employ the remaining layers of the first 10 layers of VGG-16 as the front-end of the teacher branch. This allows us to utilize the pre-trained model, which has strong transfer learning capabilities and a flexible design. The teacher branch is intended to act as an experienced mentor who can quickly learn new information and guide the student branch.

To improve the characterization capabilities of the teacher branch further, we adopt a structure similar to CSRNet~\cite{li2018csrnet} by incorporating dilated convolutional layers in the back-end. This type of convolutional layers can directly enhance the network receptive field without increasing its parameters or computational complexity. Specifically, we use a light version of CSRNet consisting of 3 dilated convolutional layers.

\subsubsection{Student branch}

% Channel capacity is a key component in determining network inference speed as well as memory consumption. To this end, we approximately use $1/4$ channels of  teacher branch to avoid heavy computation. However, referring to the principle of VGG network design, the number of channels is directly reduced to $1/4$ of the teacher branch, which is not conducive to the further expression of shallow features. To this end, we use a compromise method in which we keep the number of channels of student branches constant before the first pool layer. The remaining layers can cut the number of channels by one-fourth. In addition, we can lower the number of channels by various multiples in a similar manner.

The channel capacity of a network is a critical factor that affects both inference speed and memory consumption. In our approach, we limit the number of channels in the teacher branch to approximately one-fourth of its original size to reduce the computational burden. However, following the design principle of VGG networks, reducing the number of channels directly to one-fourth may limit the network's ability to express shallow features.

To address this issue, we adopt a compromise method in which we keep the number of channels in the student branch constant until the first pooling layer. After that, we gradually reduce the number of channels in the remaining layers by one-fourth. Additionally, we can further reduce the number of channels by using multiples of one-fourth. This approach allows us to strike a balance between channel capacity and computational efficiency while maintaining the expressive power of the network.


\subsection{Knowledge distillation}

%The limited number of channels in the student branch hinders its ability to extract features efficiently. Fortunately, knowledge distillation gives extra supervision signals to ease this problem and accomplish effective feature learning through techniques like feature transfer and response distillation~\cite{hinton2015distilling, li2020local, gou2021knowledge, heo2019knowledge}. To this end, we employ individual feature distillation, relation-in-relation distillation and response distillation to strengthen the learning ability of student branch. Before we describe the distillation techniques, we will first introduce the feature grouping to better convey knowledge distillation.


The limited number of channels in the student branch can hinder their ability to extract features effectively. Fortunately, knowledge distillation offers an effective strategy to mitigate this problem by providing additional supervision signals for knowledge transfer~\cite{hinton2015distilling, li2020local, gou2021knowledge, heo2019knowledge}. To this end, we adopt three distillation techniques to enhance the learning capabilities of the student branch: individual feature distillation, relation-in-relation distillation, and response distillation. Before delving into the details of these techniques, we introduce a feature grouping approach to better understand the proposed knowledge distillation strategy.



\subsubsection{Feature grouping}

In Fig.~\ref{fig:fig1}, an input image is first processed by the shared shallow module, which outputs a group of low-level features. These features are then fed into two separate branches with different channel capacities. Let us denote these features as $t_1 (s_1)$ to represent the teacher (student) branch output. The feature map output by the $i$-th block in the teacher branch is denoted as $t_i$, where block partitioning begins from the convolution following the previous pooling operation to the next Max-Pooling. Similarly, the output of the $i$-th block in the student network is denoted as $s_i$. Formally, the features are grouped as follows:



% As shown in Fig.~\ref{fig:fig1}, an input image will first go through a shared shallow module and output a group of low-level features. These features are fed into two separate branches with different channel capacity. Without loss of generality, denote these features as $t_1 (s_1)$. Next, denote the output feature map of the $i$-th block in the teacher network as $t_i$ where the partition of blocks starts from the convolution after the previous pooling operation to the next max-pooling. Comparatively, the output of $i$-th block in the student network is denoted as $s_i$. Formally, the features are grouped into the following sets:

\begin{equation}
	T{\rm{  =  \{ }}{t_2}{\rm{,}}\;{t_3}{\rm{,}}\;{t_4}{\rm{,}}\;{t_5}{\rm{\} ,}}\;S{\rm{  =  \{ }}{s_2}{\rm{,}}\;{s_3}{\rm{,}}\;{s_4}{\rm{,}}\;{s_5}{\rm{\} ,}}
\end{equation}\\
where $T$ and $S$ denote the feature sets of the teacher branch and the student branch, respectively.


The features cannot be distilled directly since the number of channels in the two branches is not aligned. Therefore, to maintain consistency in the feature dimensions, we apply a linear transformation function $f_{ad}(\cdot)$ to the output features of various blocks of the student network. This transformation function adapts $s_i$ to $s_i^\prime$, which has the same dimension as $t_i$. The transformation is defined as follows:


% The features cannot be distilled directly since the number of channels in the two branches is not aligned. As a result, we transform the output features of various blocks of the student network to maintain feature dimension consistency. More specifically, we introduce a liner transformation function $f_{ad} (\cdot)$ to adapt $s_i$ to $s_i^\prime$, who has the same dimension as $t_i$. This transformation is described as follows:

\begin{equation}
{S^\prime } = {f_{ad}}(S)  = \{ s_2^\prime ,\;s_3^\prime ,\;s_4^\prime ,\;s_5^\prime \}, 
\end{equation}\\
where $S^\prime$ is the feature groups transformed from $S$. It should be noted that this transformation is only employed during the training stage.



\subsubsection{Feature internal distillation}


Fine-grained semantic information is contained in multiple levels of features. Distilling features is a direct method for helping students express themselves in the same way as teacher do. To this end, we build a feature internal distillation method to minimize distribution similarity between  $s_i^\prime$ and $t_i$. To achieve such a goal, mean square error (MSE) between features is a natural and simple choice. As the semantic structure of teacher and student branches differs, this choice may be overly tight, which will have a negative impact on the distillation. Moreover, our experiments show that this distillation method has poor compatibility with other types of distillation, not to mention its heavy calculation. Instead, we use a light metric,  vector feature loss, to measure the similarity of  $s_i^\prime$ and $t_i$. This metric not only reduces the redundant information between features, but also forms a complementary learning strategy with other methods. Concretely, we first feed both features $s_i^\prime$ and $t_i$ into adaptive average pool operation $p\left( \cdot \right) \in {\mathbb{R}^{B \times C \times H \times W}} \to {\mathbb{R}^{B \times C \times 1 \times 1}}$. Then the feature internal distillation is calculated by:

\begin{equation}
L_f=\sum_{i=2}^N \frac{1}{C_i}\| p(s_i^\prime) - p(t_i) \|_2^2
\end{equation}\\
where $C_i$ is the channel number of $s_i^\prime$ and $t_i$ and $N$ is the number of blocks.

\subsubsection{Feature relation distillation}


\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.6]{./fig3.png}
	\caption{Overview of feature relation module.}
	\label{fig:fig2}
\end{figure} 

%To some extent, the semantic changes between layers demonstrate the evolution of features, termed as so-called inter-layer 'flow' in~\cite{yim2017gift}. As a result,  exploring their relationship is beneficial in establishing the robust features of student network. For example,~\cite{liu2020efficient} densely computed the pairwise feature relationships using down-sampled block features. However, existing approaches exclusively focus on the relational information of individual pixels and neglect the relationship of different pixels in specific layers. To address this issue, inspired by ~\cite{wang2018non}, we propose a feature relation module to model the relationship of different pixels and their corresponding similarity between different layers. Furthermore, we employ distillation to assist student branch in learning feature evolution between layers.

Semantic changes between layers reflect the evolution of features within the network, which is referred to as the inter-layer "flow" in~\cite{yim2017gift}. Therefore, exploring the relationships between layers is crucial for helping student networks capture robust features. For example,~\cite{liu2020efficient} densely computes pairwise feature relationships using down-sampled block features. However, existing methods mainly focus on the relationship information of individual pixels and overlook the relationships between different pixels within the same layer. To address this issue, we propose a feature relationship module inspired by~\cite{wang2018non} that models the relationships between different pixels and the corresponding similarity between layers,  after which a distillation strategy is used to help students branch out to learn the evolution of features between layers.


The overview of feature relation module is depicted in Fig.~\ref{fig:fig2} to better explain the distillation process. Let us consider the teacher branch as an example. Firstly, we can obtain two feature maps $t_i\in \mathbb{R}^{B\times C_1\times H\times W}$ and $t_j\in \mathbb{R}^{B\times C_2\times H\times W}(2\le i< j\le N)$ from different blocks. Next, we apply a reshape operation to transform the shape of the two feature maps, for example, for $t_i$, we obtain two new feature maps: $t_i^1\in \mathbb{R}^{B\times (HW) \times C_1}$ and $t_i^2\in \mathbb{R}^{B\times C_1\times (HW)}$.  Next, Multiply $t_i^1$ by $t_i^2$, we obtain the correction matrix $K_i \in \mathbb{R}^{B\times HW\times HW}$ after sigmoid operation $g( \cdot )$. After the same operation, we have the correction matrix $K_j$ for the feature map $t_j$ as well. Finally, the relation matrix between the $i$-th and $j$-th block in the teacher branch is obtained, i.e., $R_{ij}^t = K_i \bigotimes K_j$. Formally, the inter-layer relationship matrix can be expressed as follows:

\begin{equation}
R_{ij}^t = \frac{{g(t_i^1 \cdot t_i^2) \otimes g(t_j^1 \cdot t_j^2)}}{{H \cdot W}}
\end{equation} \\
where $\bigotimes$ means element-wise product. After the same operation, we get the same relationship matrix of the student branch $R_{ij}^{s^\prime}$ for the $i$-th and $j$-th block.

Here we simply use the L2 distance to measure the gap between two corresponding relationship. The feature relation loss is present as follows:
\begin{equation}
	L_r=\sum_{2\le i < j \le N }\| R_{ij}^{s^\prime}-R_{ij}^t \|_2^2
\end{equation}

\subsubsection{Response distillation}


Recent work has revealed that response distillation is a type of learned label smoothing regularization~\cite{yuan2020revisiting}. In the traditional two-phase architecture, MSE is typically used as the loss function, but it is difficult to obtain comparable results in our distillation paradigm. We speculate that this is because it is challenging for the teacher branch to produce a strong output early in the training process, and therefore, response distillation based on MSE is not particularly strict, making it difficult to achieve effective regularization. Inspired by~\cite{wang2004image}, we use SSIM loss as the response-based distillation loss, which is defined as follows:


% Recent work revealed that response distillation is a type of learned label smoothing regularization~\cite{yuan2020revisiting}. MSE is typically used as the loss function in the classic two-phase architecture, however it is difficult to get a comparable result in our distillation paradigm. We speculate that because it is difficult for the teacher branch to generate a strong output early in the training process, the response distillation based on MSE is not particularly stringent, making it impossible to produce an effective regularization effect. Inspired by~\cite{wang2004image}, SSIM loss is set as the response-based distillation loss as follows:


\begin{equation}
\begin{aligned}
L_s&=1-SSIM(t_N,s_N^\prime),\\
SSIM({t_N},s_N^\prime ) &= \frac{{(2{\mu _{{t_N}}}{\mu _{s_N^\prime }} + {c_1})(2{\sigma _{{t_N}s_N^\prime }} + {c_2})}}{{(\mu _{{t_N}}^2 + \mu _{s_N^\prime }^2 + {c_1})(\sigma _{{t_N}}^2 + \sigma _{s_N^\prime }^2 + {c_2})}},
\end{aligned}
\end{equation}   \\
where ${{\mu _{{x}}}}$ denotes the mean intensity of input $x$, ${\sigma _{{y}}}$ denotes the standard deviation of input $y$, and ${\sigma _{xy}}$ denotes the covariance of input $x$ and $y$. 

Finally, we perform knowledge distillation by minimizing the total loss as follows:

\begin{equation} \label{loss}
L = L_{st}+L_{tea}+\alpha_1L_f+\alpha_2L_r+\alpha_3L_s
\end{equation} \\
where $L_{st}$ and $L_{tea}$ are the MSE loss of student and teacher branches, $\alpha_1,\alpha_2$ and $\alpha_3$ are tunable hyper-parameters to balance the loss terms.


\section{Experiment}

In this part, we conduct comprehensive experiments on four challenging datasets to illustrate the effectiveness of our proposed strategy, including the ShanghaiTech dataset (Part A)~\cite{zhang2016single} and UCF-QNRF~\cite{idrees2018composition},  JHU-CROWD++~\cite{sindagi2020jhu} and NWPU-Crowd~\cite{wang2020nwpu}. We employ ShanghaiTech Part A to conduct ablation studies to demonstrate the efficiency of our suggested module and framework. Then, we compare our solution to other state-of-the-art methods on the four datasets.



\subsection{Experiment Settings}

% All the experiments in this study are implemented with PyTorch.  For data augmentation, we first randomly scale the image to 0.8 to 1.2 times its original size. The scaled picture is then randomly cropped into a 400 $\times$ 400 patch. Finally, we randomly flip it and apply gamma correction to it. As for the proposed network, VGG-16 pretrained on ImageNet is used to initialize the first ten layer parameters of the shared shallow module and the teacher branch. The other layers are initialized by a random Gaussian distribution with 0.01 standard deviation. As for training, we set the initial learning rate of student branch to be $1e^{-4}$, while that of teacher branch is $1e^{-6}$. The same hyper-parameters settings $( \alpha_1=1, \alpha_2=10, \alpha_3=1000 )$ are adopted for all the experiments. We train the network for 600 epochs on ShanghaiTech Part-A and UCF-QNRF, 500 epochs on the rest two datasets with Adam optimizer. The four benchmarks mentioned will be introduced in the following parts.


All experiments in this study were implemented using PyTorch. Data augmentation is performed as follows: first, the image is randomly scaled to 0.8 to 1.2 times its original size. The scaled image is then randomly cropped into a 400 $\times$ 400 patch. Next, random flipping  and gamma correction are applied. VGG-16 pretrained on ImageNet is used to initialize the first ten layers of the shared shallow module and the teacher branch. The remaining layers are initialized by a random Gaussian distribution with a standard deviation of 0.01. For training, the initial learning rate of the student branch is set to $1e^{-4}$, while that of the teacher branch is $1e^{-6}$. The same hyper-parameters settings $( \alpha_1=1, \alpha_2=10, \alpha_3=1000 )$ are adopted for all experiments. The network is trained for 600 epochs on ShanghaiTech Part-A and UCF-QNRF and 500 epochs on the remaining two datasets, using the Adam optimizer. The following subsection will introduce the four benchmarks mentioned.




\subsection{Datasets}

\textbf{ShanghaiTech Part-A}~\cite{zhang2016single}: This dataset includes 482 images collected from the Internet, where 300 images are used for training and the rest for testing.

\textbf{UCF-QNRF}~\cite{idrees2018composition}: It is a challenging dataset, which contains 1,535 images collected from Internet with high-resolution. The annotated heads of samples range form 49 to 12,865, which has a huge variation of crowd density. There are 1,201 images for training and 334 images for testing.

\textbf{JHU-CROWD++}~\cite{sindagi2020jhu}: This is a large scale unconstrained crowd counting dataset. It is collected under a variety of diverse scenarios and environmental conditions which contains 4,372 images with 1.51 million annotations.  There are 2,772 images for training and 1600 images for testing.\par

\textbf{ NWPU-Crowd}~\cite{wang2020nwpu}:This is the largest crowd counting and localization benchmark. It contains various illumination scenes and has the largest density range from 0 to 20,033. It is made up of 1,525 images with a total of 1,251,642 label points. The average number of pedestrians per image is 815, with a high of 12,865. For most models, this configuration is a tremendous difficulty.


\subsection{Ablation Study}


In this part, we conduct an ablation study on the ShanghaiTech Part A dataset to assess the effectiveness of our proposed method, which we refer to as the Crowd Counting Distillation (CCD) Net for simplicity. As mentioned in~\cite{liu2020efficient}, the number of channels is an important factor in balancing efficiency and accuracy in knowledge distillation. Therefore, we follow their approach and set the channel number of the student branch to be one-fourth of the teacher branch. We provide a detailed analysis of our ablation study in the following subsections

% In this part, we perform ablation study on ShanghaiTech Part A to evaluate the effectiveness of our proposed method. For simplicity,  we will refer to our proposed crowd counting distillation network as CCD Net. As reported in~\cite{liu2020efficient}, channel numbers are a key factor in the balance between efficiency and accuracy for knowledge distillation. Following their study, we default the channel number of the student branch to 1/4 of the teacher branch. In what follows, a more detailed analysis of ablation study will be offered. 


\paragraph{Exploration on Online Knowledge Learning}
In this section, we present the results of our experiments to evaluate the effect of online knowledge learning. Table~\ref{tab:tab6} shows the comparison between two-phase distillation and online distillation. Our proposed online distillation method outperforms the two-phase distillation method by a large margin, demonstrating the benefits of joint training. Specifically, online distillation achieved a performance gain of approximately 8\% over the two-phase distillation method. Moreover, the online version is almost 1.4 times faster than the two-phase version, even with only 300 training samples. This indicates that the time discrepancy between the two methods becomes increasingly apparent as the sample size grows. For example, our online distillation method requires only 7310 minutes of training time for the NWPU-Crowd dataset.

Our proposed solution of online distillation, with only 0.67 times the parameters of the student network studied in SKT~\cite{liu2020efficient}, is slightly better than the two-phase method (70.02 Vs. 71.55). These results demonstrate the efficacy of our distillation method as well as its high potential for knowledge transfer. Furthermore, our final solution with the pre-trained VGG model achieved significant performance improvement compared to the model without pre-trained VGG (70.2 Vs. 101.78). This demonstrates that the teacher branch with prior knowledge can better support the student branch to capture effective knowledge.








% In this part, we conduct experiments to evaluate the effect of online knowledge learning. Table~\ref{tab:tab6} presents the comparison results of two-phase distillation and online distillation. To begin with, Online distillation outperforms the two-phase distillation by a large margin, which indicates the benefits of joint training. In particular, online distillation obtaining a performance gain of about 8\% over two-phase distillation. It can also be shown that the speed of the online version is almost 1.4 times that of the two-stage version, even with only 300 training samples. That means that the training time discrepancy becomes increasingly apparent as the sample size grows; for example, our one-phase version requires more than 7310 minutes of training time for NWPU-Crowd dataset. Then, with just 0.67 times the parameters of SKT~\cite{liu2020efficient}, our solution of online distillation is slightly better than this two-phase method (70.02 Vs. 71.55). These results demonstrate the efficacy of our distillation method as well as its high potential for knowledge transfer.  Last,  compared with the model without pre-trained VGG version, our last solution has achieved significant performance improvement (70.2 Vs. 101.78). This demonstrates that the teacher branch with prior knowledge can better support the student branch to capture effective knowledge.


\begin{table*}
	\caption{Comparison of Two-phase Distillation and Online Distillation.}
	\label{tab:tab6}
	\centering
	\begin{tabular}{cccc}
		\hline
		Method                                                 & MAE            & Param (M)     & Training Time (min) \\
		\hline    
		SKT~\cite{liu2020efficient}                            &71.55           &  1.02         &   ---                \\
		\hline
		Two-phase Distillation (ours)                          &76.07           &  0.68         &  about 405            \\
		CCD Net  w\verb|\|o pre-trained VGG   &101.78          &  0.68         &   ---                 \\
		CCD Net                                              &\textbf{70.02}  &  0.68         &  about 280             \\
		\hline
	\end{tabular}
\end{table*}

\paragraph{Exploration on Knowledge Distillation Configurations}


To facilitate knowledge distillation, we adopt three modules: feature internal distillation (FID), feature relation distillation (FRD), and response distillation (RD). To evaluate the effectiveness of these modules, we establish a baseline, which is a student network consisting of the shared shallow module and the student branch, without knowledge distillation.

Table~\ref{tab:tab1} summarizes the results of an ablation study on the three knowledge distillation modules. The results show that both FRD loss and RD loss significantly improve the performance of the student network compared to the baseline, indicating that they successfully transfer effective features. Moreover, the combination of these two losses further improves performance, with a MAE of 70.62. This suggests that these two modules are complementary to each other. However, FID loss seems to have little effect on knowledge transfer and achieves similar performance to the baseline. This indicates that the supervised signal provided by FID loss is weak when used alone. When combined with other losses, its performance varies. Interestingly, when paired with FRD loss and RD loss, the performance is marginally enhanced, and the MAE is increased by 0.65. We speculate that FRD and RD loss may be too stringent for knowledge transfer, and FID loss can effectively alleviate this issue. Next, we will evaluate the performance of each module separately.




% To accomplish the process of knowledge distillation, three modules are employed: feature internal distillation (FID), feature relation distillation (FRD), and response distillation (RD). To demonstrate the power of these three modules, we present a baseline, which is the combination of the shared shallow module and the student branch (referred to as the student network) without knowledge distillation. 

% The ablation study of three knowledge distillation modules is summarized in Table~\ref{tab:tab1}. As can be observed, both FRD loss and RD loss have significantly improved their performance compared to the baseline, indicating that they have successfully transferred certain regional features. Furthermore, the combination of the two will continue to increase their performance, with the MAE reaching 70.62. This demonstrates that the two are, to some extent, complimentary. Besides, it is easy to observe that this loss does not seem to play a role in the process of knowledge transfer and achieves about the same performance as the baseline. This implies that the supervised signal provided by the loss is weak when utilized alone. When it is combined with other losses, the performance degrades differently. Interestingly, when we paired it with FRD loss and RD loss, the performance was marginally enhanced, and the MAS rose by 0.65. We hypothesize that FRD and RD loss are excessively strict for knowledge transfer, and that FID can adequately ease this issue. Following that, we will examine the performance of each module one by one.



\begin{table}
	\caption{Ablation study of the three knowledge distillation modules.}
	\label{tab:tab1}
	\centering
	\begin{tabular}{ccccc}
		\hline
		Module         & FID loss     & FRD loss          & RD loss         & MAE \\
		\hline
		Baseline       &              &                   &                 &92.45  \\
		\hline        
		& \checkmark   &                   &                 &92.61  \\
		&              &  \checkmark       &                 &74.43\\
		&              &                   &  \checkmark     &72.69  \\
		&\checkmark    &\checkmark         &                 &83.83 \\
		&\checkmark    &                   &\checkmark       &75.22 \\
		&              &\checkmark         &\checkmark       &\textbf{70.67} \\
		&\checkmark    &\checkmark         &\checkmark       &\textbf{70.02} \\
		\hline
	\end{tabular}
\end{table}



{\bf{ Feature internal distillation }} Here, we go over the impact of feature internal distillation on the framework of this study in detail. To verify the benefit of our solution, we compare it to two commonly used metrics: MSE loss and Cos loss~\cite{liu2020efficient}. As shown in Table~\ref{tab:tab1}, MSE and Cos losses are capable of fully transferring the knowledge stored at multiple layers, but KID loss is unable of doing so. This is mostly due to the fact that the KID only saves information between channels after a series of pooling operations, and the learned feature information is severely lost. It should be noted that the KID enjoys the benefits of less computation compared to the other two losses.  As depicted in Table~\ref{tab:tab3},  it has good compatibility despite the poor performance of KID loss. As discussed above, we speculate that this is because MSE and COS loss can give a strong supervision signal for feature-level knowledge distillation, but they have a competing relationship with FRD loss and RD loss, making it difficult for the network to focus on effective feature learning. 



\begin{table}
	\caption{Comparison of Feature Internal Distillation Losses.}
	\label{tab:tab2}
	\centering
	\begin{tabular}{cc}
		\hline
		Loss         & MAE \\
		\hline
		Baseline     &92.45  \\
		\hline
		+ MSE loss     &77.97  \\
		+ Cos loss     &76.52\\
		+ FID loss     &92.61  \\
		\hline
	\end{tabular}
\end{table}


\begin{table}
	\caption{Comparison of Feature-level Transfer Configuration. All Modules Are Used, and Only Feature Loss Is Changed.}
	\label{tab:tab3}
	\centering
	\begin{tabular}{cc}
		\hline
		Transfer Configuration       & MAE \\
		\hline
		W/O Feature Loss             &70.67  \\
		+ MSE loss                   &71.78  \\
		+ Cos loss                   &70.76\\
		+ FID loss                   &\textbf{70.02}  \\
		\hline
	\end{tabular}
\end{table}



{\bf{Feature relation distillation}} To explore the information contained in features, we propose the relation distillation between features, which will capture the relationship between different pixels and different levels of features. There are two setups for our proposed FRD: sparse connected version (Sparse FRD) and densely connected version (FRD). FRD means that the elements in the feature sets (i.e.,  Eq. (1)) will be calculated for their corresponding relationships while Sparse RD only focuses on the relationship between adjacent elements. As shown in Table~\ref{tab:tab4}, the performance of both versions has shown significant improvement, with FRD achieving the best MAE of 74.43. Notice that FSP loss~\cite{liu2020efficient} proposed for the inter-layer relation transfer only achieves a small amount of performance improvement. We attribute the weaker performance of the model to two factors. On the one hand, the FSP loss only considers the correlation of element pixels across different feature sets and overlooks the correlation of pixels within the features themselves. On the other hand, the correlation of the FSP loss in the original teacher network is fixed, while the features of the teacher branch in our network are dynamic, resulting in a weaker correlation established by the FSP.




% We attribute its performance to two factors: first, FSP loss only considers the correlation of element pixels in different feature sets, while ignoring the correlation of pixels within the feature itself; second, the relevance of FSP loss in the original teacher network is fixed, whereas the features of teacher branches in this network architecture are dynamic, resulting in the weak relevance built by FSP. Our solution effectively addresses the aforementioned issues and produces superior performance.

% the two versions have improved significantly in performance, with FRD achieving the best MAE of 74.43.


\begin{table}
	\caption{Comparison of Feature Relation Distillation Losses.}
	\label{tab:tab4}
	\centering
	\begin{tabular}{cc}
		\hline
		Loss                                 & MAE \\
		\hline
		Baseline                             &92.45  \\
		\hline
		+ FSP loss~\cite{liu2020efficient}     &80.24  \\
		
		+ Cos loss                             &84.92\\
		
		+ Sparse FRD loss                       &75.86  \\
		
		+ FRD loss                              &\textbf{74.43} \\
		\hline
	\end{tabular}
\end{table}


% In addition, Fig.~\ref{fig:fig4} visualizes the output of FRD module, namely the relation matrix of different feature groups. From the first row, we can observe that in low-level features, the relation matrix is decentralized, but in high-level features, the relation matrix is highly centralized. This is simple to understand because the low-level features contain some detailed information such as edge, corner and even texture so that each pixel in the feature has have a significant degree of similarity. In contrast, high-level features may extract extensive semantic information, and there are significant variances between channels and various blocks. After distillation, we find that the student branch approximates the relation matrix of teacher branch while its distribution is more uniform than that of teacher branch. This demonstrates that the FRD will assist students in better understanding the differences between hierarchical features.

Furthermore, Fig.~\ref{fig:fig4} presents a visualization of the output of the FRD module, which is the relation matrix of different feature groups. The first row of the visualization shows that in low-level features, the relation matrix is decentralized, whereas in high-level features, the relation matrix is highly centralized. This can be attributed to the fact that low-level features contain detailed information such as edges, corners, and texture, leading to a significant degree of similarity between each pixel in the feature. On the other hand, high-level features extract extensive semantic information, resulting in significant variances between channels and various blocks. After distillation, we observe that the student branch approximates the relation matrix of the teacher branch while its distribution is more uniform compared to that of the teacher branch. This demonstrates that the FRD helps students in better understanding the differences between hierarchical features.


\begin{figure*}[!t]
	\centering
	\includegraphics[scale=0.6]{./fig4.png}
	\caption{Relation matrices of different feature groups. The relation matrix is produced by trained network on a test sample of ShanghaiTech Part A. The relationship matrix of the teacher and student branches is represented in the first and second rows, respectively. ${t_i} \to {t_j} ({s_i} \to {s_j})$ denotes the relation matrix generated by ${t_i} ({s_i})$ and ${t_j}  ({s_j})$. Brighter colors denote stronger relation values}
	\label{fig:fig4}
\end{figure*} 


{\bf{Response distillation}} Table~\ref{tab:tab5} presents a comparison of the response distillation losses, where we observe a significant improvement in the SSIM loss, achieving an MAE of 72.69 compared to the MSE loss. This improvement can be due to the fact that the SSIM loss evaluates the local pattern consistency of the output from both the teacher and student branches, whereas the MSE loss only evaluates the element pairwise similarity.



%  Table~\ref{tab:tab5} shows the comparison of response distillation losses. Clearly, SSIM loss has improved significantly, reaching 72.69 in terms of MAE when compared to MSE loss. This is because the SSIM loss is able to assess the local pattern consistency of the output of the teacher and student branches, whereas the MSE loss only assesses element pairwise similarity.

\begin{table}[!htb] 
	\caption{Comparison of Response Distillation Losses.}
	\label{tab:tab5}
	\centering
	\begin{tabular}{cc}
		\hline
		Loss                  & MAE \\
		\hline
		Baseline             &92.45  \\
		\hline
		+ MSE Loss             &86.68  \\
		+ SSIM Loss            &\textbf{72.69}\\
		\hline
	\end{tabular}
\end{table}



%
%\subsection{Comparison with Model Compression Methods}
%
%There are various current approaches for compressing crowd counting models. Here, we compare some representative models~\cite{liu2020efficient} to demonstrate the strength of our method.
%
%As is shown in Table~\ref{tab:tab7}, our method performs best compared with different compression strategies. Specially,  
%The CSRNet parameters are quantized into 8 bits using QAT~\cite{jacob2018quantization}, yielding an MAE of 75.50.  For pruning method, CP~\cite{he2017channel} provides an iterative two-step algorithm to effectively prune each layer of CSRNet which obtains an MAE of 82.05 with 6.89M parameters. AGP~\cite{zhu2017prune} uses a simple gradual pruning approach to compress CSRNet and obtains a MAE of 78.51. We also compare two distillation-based approaches, AT~\cite{zagoruyko2016paying} and AGP~\cite{zhu2017prune}. Clearly, the results show the satisfactory performance of our method which achieves a balance in accuracy and efficiency.
%
%
%\begin{table}
%\caption{Comparison of Compression Methods On ShanghaiTech Part-A.}
%\label{tab:tab7}
%\centering
%\begin{tabular}{cc|c}
%\hline
%\multicolumn{2}{c|}{Method}                                         & MAE \\
%\hline
%Quantization                    &QAT~\cite{jacob2018quantization}   &75.50  \\
%\hline
%\multirow{2}*{Pruning}          &CP~\cite{he2017channel}            &82.05   \\
%~                               &AGP~\cite{zhu2017prune}            &78.51   \\                         
%\hline
%\multirow{3}*{Distillation}     &AT~\cite{zagoruyko2016paying}      &74.65  \\
%~                               &SKT~\cite{liu2020efficient}        &71.55 \\
%~                               &Ours                               &\textbf{70.02} \\
%\hline
%\end{tabular}
%\end{table}

  
\subsection{Comparison with State-of-the-art Methods}
To demonstrate the effectiveness of our method, we further compare CCD Net to some state-of-the-art crowd counting methods. Four datasets, i.e., Shanghaitech Part-A, UCF-QNRF, JHU-Crowd++ and NWPU-Crowd are used for performance evaluation. 

The final results are summarized in Tables~\ref{tab:tab8},~\ref{tab:tab9},~\ref{tab:tab10} and ~\ref{tab:tab11}. As can be seen, our model performs admirably across all datasets with a small number of parameters. There are two findings on these datasets: 
\begin{itemize}
\item Compared to training alone, the use of knowledge distillation in the student branch has resulted in significant improvements. The magnitude of this improvement becomes more pronounced with larger data collections. On the JHU-Crowd++ and NWPU-Crowd datasets, for example, the performance is more than or almost doubled.

\item Interestingly, the student branch outperforms the teacher branch on three large-scale datasets, namely, UCF-QNRF, JHU-Crowd++, and NWPU-Crowd, while there is still a certain gap between the student branch and teacher branch on ShanghaiTech Part-A. This indicates that the network can be easily overfitted on a small dataset during the distillation step. Furthermore, we speculate that joint training indeed helps student networks better capture effective features, allowing it to outperform the teacher network.



% Interestingly, the student network outperforms the teacher network on three large-scale datasets, that is,  UCF-QNRF, JHU-Crowd++ and NWPU-Crowd, while there is still a certain gap between student network and teacher network on ShanghaiTech Part A. This demonstrates that the network is easily overfitted on a small dataset during the distillation step. Meanwhile, we peculate that the joint training indeed helps student networks better capture effective features so that it can outperform the teacher network.

\end{itemize}



Our findings suggest that our online training network is capable of effectively transferring knowledge. While we have achieved excellent performance, it is important to note that there is still a gap between our results and the latest benchmarks. For instance, the BL model outperforms ours by 12\% on UCF-QNRF. The primary reason for this is due to the limited performance of our backbone. Therefore, in future work, we aim to enhance its applicability and further improve its performance.

%These findings suggest that our online training network can realize the effective transfer of knowledge. Although we have achieved excellent performance, it also should be noted that there is a certain gap between our performance and the latest results. For example, the BL model obtains a performance gain of 12\% over ours on the UCF-QNRF dataset. The primary reason for this is because the network we developed has limited performance. Thus, we will enhance its applicability in the next work. 

\begin{table}[!htb] 
	\caption{Comparison of Crowd Counting Methods on Shanghaitech Part-A.}
	\label{tab:tab8}
	\centering
	\begin{tabular}{ccc}
		\hline
		Method                                 & MAE     & MSE \\
		\hline
		MCNN~\cite{zhang2016single}            &110.2    &173.2 \\
		CP-CNN~\cite{sindagi2017generating}    &73.6     &106.4 \\
		DNCL~\cite{shi2018crowd}               &73.5     &112.3 \\
		ACSCP~\cite{shen2018crowd}             &75.7     &102.7 \\
		L2R~\cite{liu2018leveraging}           &73.6     &112.0 \\
		IG-CNN~\cite{sam2018divide}            &72.5     &118.2 \\
		IC-CNN~\cite{ranjan2018iterative}      &68.5     &116.2 \\
		CFF~\cite{shi2019counting}             &65.2     &109.4 \\
		SKT~\cite{liu2020efficient}            &71.55    &114.40 \\
		\hline
		CSRNet                                 &66.67    &105.99 \\
		1/4 CSRNet                             &92.45    &146.68\\
		CCD Net                                   &\textbf{70.02}  &\textbf{118.38}\\
		\hline
	\end{tabular}
\end{table}

\begin{table}[!htb] 
	\caption{Comparison of Crowd Counting Methods on UCF-QNRF.}
	\label{tab:tab9}
	\centering
	\begin{tabular}{ccc}
		\hline
		Method                          & MAE   &MSE \\
		\hline
		MCNN~\cite{zhang2016single}     &277    &426 \\
		CMTL~\cite{sindagi2017cnn}      &252    &514\\
		ResNet-101~\cite{he2016deep}    &190    &277\\
		CL~\cite{idrees2018composition} &132    &191\\
		TEDNet~\cite{jiang2019crowd}    &113    &188\\
		CANNet~\cite{liu2019context}    &107    &183\\
		S-DCNet~\cite{xiong2019open}    &104.40 &176.10\\
		DSSINet~\cite{liu2019crowd}     &99.10  &159.20\\
		\hline
		CSRNet                          &145.54   &233.32\\
		1/4 CSRNet                      &186.31  &287.65\\
		CCD Net                            &\textbf{136.26}  &\textbf{240.83}\\
		\hline
	\end{tabular}
\end{table}
% ~\cite{ma2019bayesian, liu2019crowd} 

\begin{table}[!htb] 
	\caption{Comparison of Crowd Counting Methods on JHU-CROWD++.}
	\label{tab:tab10}
	\centering
	\begin{tabular}{ccc}
		\hline
		Method                          & MAE    &MSE \\
		\hline
		MCNN~\cite{zhang2016single}     &188.9   &483.4  \\
		CMTL~\cite{sindagi2017cnn}      &157.8   &490.4\\
		DSSINet~\cite{liu2019crowd}     &133.5   &416.5\\
		LSCCNN~\cite{sam2020locate}     &112.7   &454.5\\
		CANNet~\cite{liu2019context}    &100.1   &314.0\\
		SANet~\cite{cao2018scale}       &91.1    &320.4\\
		BL~\cite{ma2019bayesian}        &75.0    &299.9\\
		CG-DRCN-CC-VGG16~\cite{sindagi2020jhu}   &82.3  &328.0\\
		\hline
		CSRNet                         &85.9     &309.2\\
		1/4 CSRNet                     &182.86   &539.00\\
		CCD Net                       &\textbf{85.51}  &\textbf{315.71}\\
		\hline
	\end{tabular}
\end{table}

\begin{table}[!htb] 
	\caption{Comparison of Crowd Counting Methods on NWPU-Crowd.}
	\label{tab:tab11}
	\centering
	\begin{tabular}{ccc}
		\hline
		Method                                 & MAE    &MSE\\
		\hline
		MCNN~\cite{zhang2016single}            &232.5   &714.6 \\
		SANet~\cite{cao2018scale}              &190.6   &491.4\\
		C3F-VGG~\cite{gao2019c}                &127.0   &439.6\\
		CANNet~\cite{liu2019context}           &106.3   &386.5\\
		SCAR~\cite{gao2019scar}                &110.0   &495.3\\
		BL~\cite{ma2019bayesian}               &105.4   &452.4\\
		\hline
		CSRNet                                 &121.3  &433.48\\
		1/4 CSRNet                             &206.90   &622.20\\
		CCD Net                                &\textbf{119.46} &\textbf{430.57}\\
		\hline
	\end{tabular}
\end{table}





\section{Conclusion}


In this study, we propose an efficient online distillation network for crowd counting. Unlike the traditional two-phase distillation technique, we introduce an end-to-end online knowledge distillation framework. The framework comprises three modules: a shared shallow module, a teacher branch, and a student branch, which integrate two traditionally distinct networks into a single trainable network. Then, we propose a new method for distilling feature relations, which enables the student branch to better understand the evolution of inter-layer features. This is achieved by constructing an inter-layer relationship matrix that captures the relationship between features across layers. It is integrated with response distillation and feature internal distillation methods to improve the transfer of mutually complementary information from the teacher branch to the student branch. Finally, our approach achieves comparable results to several state-of-the-art models, with significantly fewer parameters, as demonstrated through extensive experiments on four challenging datasets. 


%Extensive experiments on four challenging datasets demonstrate the validity of our approach, which achieves comparable results to several cutting-edge models with significantly fewer parameters.


%In this work, we put forward an efficient online distillation network for crowd counting. In contrast to the classical two-phase distillation technique, we present an end-to-end online knowledge distillation framework. The framework consists of three modules, i.e., shared shallow module, teacher branch and student branch, to integrate two traditional different networks into a single trainable network. Moreover, we build three distinct knowledge techniques for this framework to assist students in better learning the various knowledge of teacher branch. To be more specific,  feature internal distillation aims at transferring the knowledge from the same level of teachers and students; feature relation distillation is designed to capture the relationship of different hierarchical features by exploring new relationship between them; response distillation provide the regularization of two branches' output. Finally, extensive experiments on four challenging datasets show the validity of our method, and while needing much fewer parameters, our network achieves comparable results to several cutting-edge models.


% \iffalse â€¦. \fi
\iffalse
\section*{Acknowledgment}
 This work is supported by  xxxx.
\fi


% references section
\bibliographystyle{IEEEbib}
\bibliography{bib_tip}



% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% that's all folks
\end{document}


