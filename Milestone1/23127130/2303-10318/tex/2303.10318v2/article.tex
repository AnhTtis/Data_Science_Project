
\documentclass[journal]{IEEEtran}
\usepackage{ifpdf}
\usepackage{cite}
\usepackage{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{url}
\usepackage{multirow}
\usepackage[backref]{hyperref}
\usepackage{subcaption}
\usepackage{color}
\usepackage{makecell}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Remote Sensing Object Counting with  Online Knowledge Learning}
% Interactive

% Strong-cues boosting online distillation for crowd counting

% High-capacity teacher guided online distillation for crowd counting
% Privileged teacher guided online distillation for crowd counting
% Strong-cues boosting online distillation for crowd counting


% \iffalse …. \fi
% \iffalse
\author{Shengqin Jiang, Yuan Gao, Bowen Li, Fengna Cheng, Renlong Hang,  Qingshan Liu,~\IEEEmembership{Senior Member, IEEE}
       

%\thanks{This work is done when the first author visits the University of Adelaide.}% <-this % stops a space

\thanks{Manuscript received *** **, 2024; revised *** **, 2024. This work is supported by the National Key Research and Development Program of China (No.2021ZD0112200), the Joint Funds of the National Natural Science Foundation of China (No.U21B2044), the National Natural Science Foundation of China (No.62001237), the Jiangsu Planned Projects for Postdoctoral Research Funds (No.2021K052A), the China Postdoctoral Science Foundation Funded Project (No. 2021M701756), the Startup Foundation for Introducing Talent of NUIST (No.2020r084). (\it{Shengqin Jiang, Yuan Gao and Bowen Li contributed equally to this work.}) (\it{Corresponding author: Qingshan Liu and Renlong Hang.})} % <-this % stops a space
% This work is supported by the National Natural Science Foundation of China (Nos.62001237 and 61902092), Jiangsu Planned Projects for Postdoctoral Research Funds(No.2021K052A), China Postdoctoral Science Foundation Funded Project (No. 2021M701756) and the Startup Foundation for Introducing Talent of NUIST (No.2020r084)

\thanks{S. Jiang, Y. Gao, B. Li and R. Hang are with the School of Computer Science, Nanjing University of Information Science and Technology, Nanjing 210044, China, Ministry of Education Engineering Research Center of Digital Forensics, Nanjing University of Information Science and Technology, Nanjing  210044, China, and Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), Nanjing University of Information Science and Technology, Nanjing 210044, China (e-mail: jiangshengmeng@126.com; gaoyuan\_mr@126.com; jslibowen@126.com; renlong\_hang@163.com)}
\thanks{F. Cheng is with the College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, China (e-mail: cfn1218@163.com)} 
\thanks{Q. Liu is with the School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, 210023, China (e-mail: qsliu@nuist.edu.cn).}
}% <-this % stops a space
% \fi

% % The paper headers
\markboth{IEEE Transactions on Geoscience and Remote Sensing}   %{IEEE Transactions on xxxxxxx }
{Jiang \MakeLowercase{\textit{et al.}}:Remote Sensing Object Counting with  Online Knowledge Learning}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
	% edge computing and mobile devices

Efficient models for remote sensing object counting are urgently required for applications in scenarios with limited computing resources, such as drones or embedded systems. A straightforward yet powerful technique to achieve this is knowledge distillation, which steers the learning of student networks by leveraging the experience of already-trained teacher networks. However, it faces a pair of challenges: Firstly, due to its two-stage training nature, a longer training period is essential, especially as the training samples increase. Secondly, despite the proficiency of teacher networks in transmitting assimilated knowledge, they tend to overlook the latent insights gained during their learning process. To address these challenges, we introduce an online distillation learning method for remote sensing object counting. It builds an end-to-end training framework that seamlessly integrates two distinct networks into a unified one. It comprises a shared shallow module, a teacher branch, and a student branch. The shared module serving as the foundation for both branches is dedicated to learning some primitive information. The teacher branch utilizes prior knowledge to reduce the difficulty of learning and guides the student branch in online learning. In parallel, the student branch achieves parameter reduction and rapid inference capabilities by means of channel reduction. This design empowers the student branch not only to receive privileged insights from the teacher branch but also to tap into the latent reservoir of knowledge held by the teacher branch during the learning process. Moreover, we propose a relation-in-relation distillation method that allows the student branch to effectively comprehend the evolution of the relationship of intra-layer teacher features among different inter-layer features. Extensive experiments on two challenging datasets demonstrate the effectiveness of our method, which achieves comparable performance to state-of-the-art methods despite using far fewer parameters.



% The student branch achieves parameter reduction and fast inference through channel reduction. This network architecture allows student branches to not only enjoy privileged information from teacher branches, but also perceive the potential knowledge of teacher branches in the learning process. Moreover, we propose a  feature relation distillation method which allows the student branch to more effectively comprehend the evolution of inter-layer features by constructing a new inter-layer relationship matrix. Extensive experiments on four challenging crowd counting datasets demonstrate the effectiveness of our method which achieves comparable performance to state-of-the-art methods despite using far fewer parameters.


% But it faces two challenges: firstly, this is a two-stage training method, which means it requires longer training time. Secondly, knowledgeable teachers can directly impart the knowledge they have learned but overlook the potential knowledge in their own learning process.

% A straightforward method to achieve this is knowledge distillation (KD), which involves using a trained teacher network to guide the training of a student network. However, this traditional two-stage training method can be time-consuming, particularly for large datasets, and it is also challenging for the student network to mimic the learning process of the teacher network. To overcome these challenges, we propose an online knowledge learning method for crowd counting. Our method builds an end-to-end training framework that integrates two independent networks into a single architecture, which consists of a shared shallow module, a teacher branch, and a student branch. This approach is more efficient than the two-stage training technique of traditional KD. Moreover, we propose a  feature relation distillation method which allows the student branch to more effectively comprehend the evolution of inter-layer features by constructing a new inter-layer relationship matrix.Extensive experiments on four challenging crowd counting datasets demonstrate the effectiveness of our method which achieves comparable performance to state-of-the-art methods despite using far fewer parameters.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
remote sensing; object counting; knowledge distillation; online training
\end{IEEEkeywords}
\IEEEpeerreviewmaketitle

% At the same time, we hold the opinion that our results will provide further impetus to the hardware architecture community to customize the next generation of deep learning accelerator architectures to efficiently handle sparse matrix storage and computations. 

% Although achieving promising performance on some benchmarks [18,2] with supervised learning or unsupervised learning [14,6], many of them apply computationally heavy networks, which hinders their deployment to the practical applications, such as autonomous driving and personalized recommendations. Accordingly, building an efficient video understanding system is a crucial step towards widespread deployment in the real world.

% NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition

\section{Introduction}
\IEEEPARstart{R}{emote} sensing object counting involves  estimating the number of targets using images captured by satellites or drones. This technique is essential for applications such as resource management, urban planning, and environmental protection. Unlike general object counting, this task deals with large-scale images taken from high altitudes, where targets may be small, densely packed, and set against complex and variable backgrounds~\cite{gao2020counting, yi2023lightweight,xu2024rethinking}. These factors make robust object counting particularly challenging. Fortunately, recent rapid advancements in deep learning have significantly improved our ability to address these challenges.

\begin{figure}[!tb] 
	\centering
	\includegraphics[scale=0.55]{./fig1.jpg}
	\caption{Comparison of distillation pipeline (traditional method vs. our method). (a) The traditional method involves training the teacher network first, followed by training the student network; (b) Our method employs an online distillation method to train the teacher and student branches jointly.}
	\label{fig:fig0}
\end{figure} 


%sensing object counting plays a crucial role in various computer vision tasks, focusing on accurately counting and locating objects within remote sensing images. This technology has significant applications in fields such as aviation reconnaissance, satellite observation, and urban security. One of the primary challenges in this area is handling scale variation, which complicates the object counting process. However, with the rapid growth of urban populations and the increasing density of geographic objects like buildings and vehicles, there is a pressing need to develop methods for counting these objects in remote sensing imagery. Recent studies have begun to address this need.
%However, the counting of dominant ground objects, such as buildings and ships, remains largely overlooked. Accurately estimating the number of these objects can provide valuable insights for practical applications, including urban planning, environmental monitoring, digital urban model construction, and disaster response and assessment.
%Crowd Counting is a task that aims at automatically estimating the number of people in images or videos. It is a challenging task, as it involves dealing with varying densities, heavy occlusions, large scales, and complicated scenery. Due to its numerous practical applications, such as video surveillance, traffic management, and production forecasting, it has recently gained a lot of interest from both academia and industry.
% Thanks to the superior performance of convolutional neural network (CNN) in visual tasks, it has become the mainstream model in the field of crowd counting. MCNN~\cite{zhang2016single} considered using a Multi-column CNN to learn the target density maps in order to handle the varying density. This network, albeit lightweight, is challenging to estimate high-density situations effectively. To address this issue, CP CNN~\cite{sindagi2017generating} realized accurate crowd prediction by mining local and global context information. Later,  CSRNet~\cite{li2018csrnet} built on the first 10 layers as the front-end and a dilated CNN for the back-end to learn the scale variation. More recently, some well-designed models are committed to extracting more powerful features by developing specific yet complicated structures~\cite{shi2019counting, liu2019crowd}. These elaborately built networks give some performance benefits, but they have to face the problems of high computational complexity and long reasoning time. However, in practical applications, such as edge computing and mobile terminal, we have to contend with limited processing resources, which makes the use of those robust networks difficult.


So far, considerable efforts have been devoted to the task of remote sensing object counting, yielding notable results. Most of these approaches achieve superior object counting performance through the careful design of specialized modules or loss functions. Gao et al.~\cite{gao2020counting} collected a large-scale dataset for remote sensing object counting and subsequently benchmarked it by designing a novel neural network based on the learning paradigm of MCNN~\cite{zhang2016single}. Duan et al.~\cite{duan2021distillation} fused context information from different receptive fields effectively and utilized feature maps from the deeper layer of the network to supervise feature maps from the earlier layer of the network. Yi et al.~\cite{yi2023lightweight} explored a multiscale feature fusion method to deal with challenges such as large-scale variation and complex background interference. Wang et al.~\cite{wang2024hierarchical} proposed several hierarchical kernel interaction modules to simultaneously preserve high-resolution features and extract deep-layer semantic information. These methods have established a crucial foundation for advancing remote sensing object counting. Upon further scrutiny, it becomes evident that current models predominantly adopt specific strategies built upon pre-trained networks. This strategy not only capitalizes on the powerful generalization capabilities of pre-trained models in downstream tasks but also allows for the customization of strategies tailored to the unique characteristics of remote sensing targets. While these models offer performance benefits, they typically exhibit a large number of parameters and high computational complexity. This makes them challenging to use in practical applications, such as drones or embedded systems, where processing resources are limited. 






% Multi-column CNNs, as used in MCNN~\cite{zhang2016single}, have been employed to learn target density maps and handle varying object densities. However, these networks struggle to estimate high-density cases effectively. To overcome this limitation, Sindagi and Patel~\cite{sindagi2017generating} proposed CP CNN, which leverages local and global contextual information for accurate object prediction. CSRNet~\cite{li2018csrnet} used the first 10 layers as the front-end and dilated convolution layers as the back-end to learn scale variation. Recent models, such as SANet~\cite{cao2018scale} and CAN~\cite{liu2019context}, have incorporated some specific yet complicated structures to extract more powerful features.For remote sensing object counting, ASPDNet~\cite{gao2020counting}proposed consists of deformable convolution module to accommodate large-scale dataset for remote sensing object counting.





% This hinders the application of 3D CNNs in practical scenarios and raises an open problem of how to design lightweight 3D CNNs for real-time video classification

% the soft targets from deeper models do not often serve as good cues for the shallower models due to the gap of compatibility. cite{MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation}
%
%
%



Numerous strategies have been proposed to develop small yet efficient models with comparable performance to larger models, such as pruning~\cite{cai2017deep, rao2018runtime} and quantization~\cite{park2018value}. Nonetheless, most of those methods require meticulous hyperparameter tuning or specialized hardware platforms~\cite{liu2020efficient,liu2020metadistiller}. An alternative approach, knowledge distillation, employs supervised information from a high-performing large model (referred to as the teacher) to train a smaller model (referred to as the student)~\cite{hinton2015distilling, zhang2018deep, zhang2019your, liu2020efficient}, as shown in Fig.~\ref{fig:fig0} (a). This method can substantially enhance the performance of the student and is both straightforward and practical to implement~\cite{dai2021general}. However, it has to face two challenges: 1) The method typically relies on two-stage training, which significantly increases the time required for training, particularly as the number of samples grows. 2) During the distillation process, although the student can learn rich semantic knowledge from the teacher network, it overlooks the insights gained by the teacher network during its own training. 




% Self-distillation is a potential solution to this problem, as it has been successfully applied to image classification~\cite{yang2019snapshot, mirzadeh2020improved, liu2020metadistiller}. However, applying it directly to high-density remote sensing object counting tasks is challenging due to the difficulty in creating efficient soft targets for lightweight branching and achieving effective knowledge transfer.




% To date,  lots of strategies, including as pruning~\cite{cai2017deep, rao2018runtime} and quantization~\cite{park2018value}, have been proposed to achieve a small and efficient model with comparable performance to a large one. Nevertheless, the majority of them require either a tedious hyper-parameters search or a specialized hardware platform~\cite{liu2020efficient,liu2020metadistiller}. As an alternative, knowledge distillation (KD) employs the supervisory information of the big model with superior performance to train a small model, resulting in improved performance and accuracy~\cite{hinton2015distilling, zhang2018deep, zhang2019your, hou2019learning}. Traditionally, the teacher model is required to be trained in advance before being transferred to a small model~\cite{dai2021general,liu2020efficient}, as shown in Fig.~\ref{fig:fig0} (a). This indicates that the two-stage training method will take a lot of time, particularly for training a teacher model. This poses an unanswered question for crowd counting: how could an efficient one-phase network be designed to accomplish high-quality information transfer? Although some self distillation work has been applied to image classification~\cite{yang2019snapshot, mirzadeh2020improved, liu2020metadistiller}, applying it directly to high-density crowd counting task is very challenging. There are two reasons for this: the first is how to create soft targets that give high-quality cues for the lightweight branch; the second is how to efficiently transfer knowledge.


To address the aforementioned challenges, we propose  an online knowledge learning method for remote sensing object counting. This framework, depicted in Fig.~\ref{fig:fig0} (b), is an end-to-end knowledge distillation approach, which avoids the need for two-stage training methods. It consists of a shared shallow module, a teacher branch, and a student branch. The student branch employs 1/4 of the channels of the teacher branch to reduce computational complexity. It is important to note that the pre-trained model initializes the shared shallow module and teacher branch to gain adequate domain-specific knowledge. Furthermore, we propose a relation-in-relation distillation strategy building upon feature distillation, which captures the evolution of intra-layer feature relationships across layers, thereby better capturing the intrinsic knowledge of the teacher branch. Finally, extensive experiments on two publicly available datasets validate the effectiveness of our method.

\begin{itemize}
\item To the best of our knowledge, we at the first time propose an end-to-end online knowledge distillation framework for remote sensing object counting. It utilizes a parallel dual-branch learning architecture to efficiently enhance the performance of the low-parameter student network while avoiding the time-consuming two-stage training process.
% \item We employ a point-based framework which is a simple, intuitive, and competitive approach by directly predicting individual locations as points for Remote Sensing Object Counting.
\item We propose a new relation-in-relation distillation method to assist the student branch in better understanding the evolution of intra-layer features by distilling the inter-layer relationship matrix from both branches. 


\item Extensive experiments on two challenging datasets show the effectiveness of our framework. Moreover, our distilled model achieves comparable results to some state-of-the-art methods, despite having limited parameters.
\end{itemize}



\section{Related Works}

In this section, we will review some works related to our model. In what follows, the methods for remote sensing object counting will be introduced first. Then we will go through some recent efforts on knowledge distillation.

\subsection{Remote Sensing Object Counting}

In recent years, remote sensing object counting has garnered significant attention from both academia and industry~\cite{xia2018dota}. Early methods for object counting typically rely on detection-based approaches~\cite{wu2023yolo, mei2024scd}, which use object detection techniques to identify objects of interest in an image and predict bounding box information to determine the final count. Li et al.\cite{li2019simultaneously} introduced a unified framework that simultaneously detects and counts vehicles in drone images. Bayraktar et al.\cite{bayraktar2020low} employed YOLO V3 to detect filtered areas and locate target plants, subsequently estimating their number using geometrical relations and a predefined average plant size. Wu et al.~\cite{wu2023yolo} proposed a multiple-frequency feature fusion module and a bottleneck aggregation layer to enhance feature representations for counting smaller penguins. While detection-based approaches perform well in sparse scenarios, they may lead to significant inaccuracies when counting in highly dense or small-scale object settings.

%In recent years, remote sensing object counting has attracted significant attention from both academia and industry~\cite{xia2018dota}.  The early methods for object counting rely on detection-based approaches, which apply object detection techniques to identify the the objects of interest in the image and predict the box information of targets to determine the final counting result. Li et al.~\cite{li2019simultaneously} introduced a  unified framework to simultaneously detect and count vehicles from drone images. Bayraktar et al.~\cite{bayraktar2020low} utilized YOLO V3 to detect the filtered area to find interested plants, and then estimate the number of these plants using the geometrical relations and predefined average plant size. Wu et al.~\cite{wu2023yolo} proposed a multiple frequency features fusion module and a bottleneck aggregation layer to strengthen feature representations for counting smaller penguins. Although detection-based approaches perform well in sparse settings, they may result in significant counting inaccuracies in highly dense or tiny objects. 



A direct strategy involves designing a network to learn a mapping from input data to the number of people, which can be represented by a density map. This method simplifies the task of predicting bounding box coordinates. Gao et al.\cite{gao2020counting} developed an attention module, a scale pyramid module, and a deformable convolution module to address challenges in remote sensing object counting. They also collected a large-scale object counting dataset featuring four types of geographic objects. Ding et al.\cite{ding2022object} proposed an adaptive density map-assisted learning method to mitigate the loss of spatial features caused by groundtruth generated with fixed-size Gaussian kernels. To tackle issues such as scale variation and complex background interference, Yi et al.\cite{yi2023lightweight} explored a multiscale feature fusion method. More recently, Wang et al.\cite{wang2024hierarchical} addressed the loss of significant features in tiny-scale objects by sacrificing resolution to capture semantic information and designed a hierarchical kernel interaction network to resolve these issues. Guo et al.~\cite{guo2024balanced} introduced a balanced density regression network to reduce regression inaccuracies in Gaussian distributions caused by numerical variances.



% A direct approach involves training a network to map to a target count, which can be substituted with a density map. This method can simplify the task of directly predicting the coordinates of bounding box. Gao et al.~\cite{gao2020counting} developed attention module, scale pyramid module, and deformable convolution module to address the challenges in remote sensing counting task. Meanwhile, they collected a large-scale object counting dataset  including four geographic objects. Ding et al.~\cite{ding2022object} proposed an adaptive density map-assisted learning method to address the loss of spatial features of the objects for the groundtruth generated by fix-sized Gaussian kernels. Afterwards, to address the challenges such as large-scale variation and complex background interference, Yi et al.~\cite{yi2023lightweight} studied a multiscale feature fusion method. More recently, Wang et al.~\cite{wang2024hierarchical} considered the loss of significant features of tiny-scale objects by sacrificing resolution to obtain semantic information, and then designed a hierarchical kernel interaction network to attack these issues. Guo et al.~\cite{guo2024balanced} presented a balanced density regression network  to mitigate regression inaccuracies in Gaussian distributions due to numerical variances. 


The aforementioned methods typically build upon pre-trained models and then design specialized modules or strategies to address the challenges of the current task. As is well known, pre-trained models possess certain prior information, which allows them to quickly adapt to new scenarios. However, these models possess a substantial number of parameters, and incorporating additional modules can significantly elevate their computational resource requirements, thereby constraining their broader applicability. To address this, we explore a novel knowledge distillation framework that improves the training speed and enhances the transferability of privileged knowledge.



% enhances both the training speed and the transferability of the network.


 % due to its applications in urban traffic, public safety, and road planning. However, this problem has become increasingly challenging in the field of computer vision because of technical difficulties such as large-scale variations, complex background interference, and uneven density distribution. The introduction of large-scale Dataset for object detection in Aerial images (DOTA)\cite{xia2018dota} marked large progress to aerial imagery. 
 
 
 
 
 
 
 
 % More recently, adaptive multi-scale context aggregation module (AMCAM) and the self-context distillation module (SCDM) \cite{duan2021distillation} are combined, achieving good performance on the remote sensing object counting.

% The early methods for object counting relied on detection-based approaches, which applied object detection techniques to identify the entire person or head in the image and label the target with boxes to determine the final counting result. Although detection-based approaches perform well in sparse settings, they may result in significant counting inaccuracies in highly dense crowds. 


% To address this challenge, density-based methods have been introduced to reduce the difficulty of counting prediction. For instance, Zhang et al.\cite{zhang2016single} suggested generating a density map using Gaussian kernels for each input, which was then used for regression. This method has been shown to effectively mitigate the challenge of detecting densely populated regions, resulting in accurate object count prediction and density perception of its distribution.  Following this, CSRNet\cite{li2018csrnet} utilized dilated convolution layers to expand the receptive field, resulting in a significant improvement in the prediction performance of the network. Several other approaches have been proposed to enhance network performance, including ~\cite{jiang2019mask}, ~\cite{jiang2020attention}, and ~\cite{yang2020embedding}. Instead, P2PNet\cite{song2021rethinking} proposed a purely point-based framework for joint counting and individual localization for object counting. This approach fosters fine-grained predictions, addressing the practical demands of downstream tasks in object analysis.

% However, these methods have more complex structural designs and heavier parameters, which could potentially impede efficient reasoning abilities.





%Early works on crowd counting rely on detection-based approaches. So far, several object detection techniques (e.g.,  Fast R-CNN~\cite{girshick2015fast}, YOLO~\cite{redmon2016you}) have been applied. They typically identify the entire person or head in the image and label the target with boxes, with the number of boxes determining the final counting result. In sparse settings, detection-based approaches often perform well. They will, however, make huge counting inaccuracies in highly dense crowds.

% As an alternative, density-based methods are put forward to reduce the difficulty of counting prediction. Zhang et al.~\cite{zhang2016single} created a multi-column CNN to solve the scale change problem that employs three branches with varying kernel sizes. This model achieves remarkable performance, but when the network goes deeper, a substantial amount of training time and unsatisfactory accuracy hinder the flexibility of this structure.  To this end, CSRNet~\cite{li2018csrnet} adopts the dilated convolution layers to expand the receptive field. The prediction performance of the network is greatly improved with the aid of the pre-trained VGG model. ~\cite{jiang2019mask} put forward using a specialized network branch to predict the object/non-object mask, which would then be combined with the input image to generate the density map. ~\cite{jiang2020attention} introduced a CNN model based on attention scaling that uses attention masks and scaling factors to adjust density estimations in different density levels. ~\cite{yang2020embedding} merged the perspective analysis approach with the counting CNN to properly address the scale variations. The approaches described above have resulted in good network performance. However, the more complicated structural design and heavier parameters have a negative impact on the efficient reasoning of these models.

\subsection{Knowledge Distillation}

Knowledge distillation (KD) is a compression technique that transfers knowledge from a parameter-heavy pre-trained model to a more compact model. Hinton et al.\cite{hinton2015distilling} initially proposed using the output of a well-trained model as a supervisory signal to aid in the training of a student network. Romero et al.\cite{romero2014fitnets} utilized both the outputs and intermediate representations of the teacher model as cues to enhance the performance of the student model. Sau et al.\cite{sau2016deep} introduced a noise-based regularization method to strengthen the robustness of the student model. SKT\cite{liu2020efficient} leveraged the structured knowledge of a well-optimized teacher model to develop a lightweight student model. However, these strategies all require a time-consuming two-stage training process. Specifically, a high-capacity teacher model needs to be trained in advance before the student model can be effectively trained.






% Knowledge distillation (KD) is a compression technique that transfers the knowledge from a large pre-trained model to a compact model. Hinton et al.~\cite{hinton2015distilling} first proposed to use the output of a well-trained model as the supervised signal to assist the training of student network. Romero et al.~\cite{romero2014fitnets} utilized both the outputs and intermediate representations of teacher as cues to improve the performance of student. Sau et al.~\cite{sau2016deep} proposed a noise-based regularization method to strength the robustness of student.  SKT~\cite{liu2020efficient} exploited the structured knowledge of a well-optimized teacher to build a lightweight student. All of these strategies, however, go through the time-consuming two-stage training process. That is, before directing the learning of a student model, a high-capacity teacher model need to be trained in advance.

To sidestep this issue, online KD has been proposed to enable distillation within the network itself~\cite{yang2019snapshot, mirzadeh2020improved}. Yang et al.\cite{yang2019snapshot} utilized outputs from previous iterations as soft targets, but this approach risks introducing errors into the learning process and presents difficulties in selecting the appropriate iteration as a teacher. Zhang et al.~\cite{zhang2019your} introduced a self-distillation framework wherein knowledge from deeper layers assists in guiding the learning of shallower layers. Mirzadeh et al.~\cite{mirzadeh2020improved} proposed a multi-step KD method that employs a teacher assistant to bridge the gap between student and teacher networks. {Recently, Sun et al.~\cite{sun2024logit} studied the negative effects of the shared temperature between teacher and student during KD, and proposed setting the temperature as the weighted standard deviation of logit. FreeKD~\cite{zhang2024freekd} argued that consecutive downsamplings in the spatial domain of teacher models hinder the transfer of rich knowledge to student models. To address this, they introduced a frequency prompt to generate pixel-wise imitation signals and a channel-wise position-aware relational loss to improve the model's sensitivity to objects.} While these approaches reduce the training duration required for two-stage KD, they do not consider inter-feature information. Moreover, these studies primarily focus on relatively straightforward classification tasks, rather than addressing the more complex challenges associated with high-density prediction.














% online KD is put forward to distill the network itself~\cite{yang2019snapshot, mirzadeh2020improved, liu2020metadistiller}. Yang et al.~\cite{yang2019snapshot} leverages the previous iteration's outputs as soft targets, but this risks increasing the mistake in the learning process, and deciding which iteration to use as a teacher is difficult. Zhang et al.~\cite{zhang2019your} proposed a self-distillation framework in which knowledge from the deeper layers can guide the learning of shallow layers.  Mirzadeh et al.~\cite{mirzadeh2020improved} put forwards a multi-step knowledge distillation by using a teacher assistant to bridge the gap between student and teacher networks.  Liu et al.~\cite{liu2020metadistiller} built an improved soft targets for the intermediate output layers by top-down fusing feature maps from deep layers with a label generator. While these works do decrease the duration needed for two-stage knowledge distillation, they fail to consider the inter-feature information. Moreover,  these studies solely address relatively straightforward classification tasks, rather than the more complex challenges of high-density prediction.


% Hou et al.~\cite{hou2019learning} distill the shallow layers of the network under the guidance of attention map.\par

% Most of these knowledge distillation frameworks mentioned above are performed in classification area, and it stll lacks exploration in the field of crowd counting.


% 

\begin{figure*}[!t]
	\centering
	\includegraphics[scale=0.6]{./fig2.png}
	\caption{An overview of the proposed online distillation network. It consists of the shared shallow module, teacher branch and student branch. Meanwhile, feature distillation and relation-in-relation distillation are introduced to guide knowledge transfer from the teacher branch to the student branch.}
	\label{fig:fig1}
\end{figure*} 


\section{Method}

{In this section, we present a novel Online Knowledge Learning method, termed OnKL Net, designed for remote sensing object counting.} Unlike the two-stage distillation training paradigm, our method is a one-stage training framework that can effectively transfer the inherent knowledge from the teacher branch, which holds privileged information, to the student branch. To accomplish this, we employ the point-based framework P2PNet~\cite{song2021rethinking} as our network foundation, which directly predicts individual locations as points. In what follows, we will begin by presenting our network architecture and then proceed to explain how we distill knowledge from the teacher branch to the student branch.
%  Our network framework comprises a teacher branch and a student branch. The teacher branch, rich in experience, can extract useful information from training data, while the student branch has fewer parameters and faster reasoning speed.


% In this section, we first propose an online knowledge learning method for crowd counting that uses one stage training paradigm to effectively transfer knowledge, as show in Fig.~\ref{fig:fig1}. Specifically, our framework includes a teacher branch and a student branch. Teacher branch is rich in experience and can efficiently extract effective information from data, while student branch has less parameters and faster reasoning speed.  In what follows, we will start by introducing our network architecture, then go over how to distill knowledge from teacher branch to student branch.




\subsection{Network Architecture}


Fig.~\ref{fig:fig1} illustrates the overall network structure, which is composed of a shared shallow module, a teacher branch, and a student branch. {As previously mentioned, we adopt a point-based counting framework that generates a set of potential point prompts through a regression head~\cite{song2021rethinking}. The optimal target positions are then refined using a classification head, which estimates the probabilities of these prompts. In particular, the input image is initially processed by the shared shallow module to extract base features before being forwarded to the two branches. Both the teacher branch and the student branch predict a set of point proposals and their corresponding confidence scores,  with the former serving as a mentor to the latter during training. To ensure the learning objectives of these proposals, we use a one-to-one matching between point proposals and ground truth points.} 



% As shown in Fig.~\ref{fig:fig1}, the overall network is make up of shared shallow module, teacher branch and student branch. In particular, an input image is passed into a shared shallow module before being processed further through two branches. The two branches output a density map respectively, with the teacher branch serving as a mentor to the student branch during their studying.


\subsubsection{Shared Shallow Module}

The early layers of 2D CNNs are commonly used to extract low-level features such as edges and corners. Here, we utilize a shared two-layer convolutional module for extracting spatial patterns. Specifically, we adopt the first two layers of VGG-16~\cite{simonyan2014very} for this purpose, as they are effective and widely used in various tasks. This module consists of two $3\times 3$ convolutions followed by max-pooling with a stride of (2, 2).

Since the teacher branch requires prior knowledge (please refer to the subsection~\ref{teabranch} for details), the starting point of the module is to keep the network structure consistent with the shallow layer of the pre-trained model. In light of this, we initialize this module using the pre-trained parameters. It is important to note that the shared module also serves as the initial layers of the student branch, so it should not have too many parameters. In other words, we should avoid introducing too many shallow layers in the teacher branch when designing the shared module.

% Since our design involves a two-branch structure, with the teacher branch requiring extensive expertise, we use the initial layers of the trained model as the shared shallow module for easy initialization of the teacher branch. It is important to note that the shared module also serves as the initial layers of the student branch, so it should not have too many parameters. In other words, we should avoid introducing too many shallow layers in the teacher network when designing the shared module.




% Early layers of 2D CNNs are known to extract low-level features such as edges and corners. Here, we design a shared two-layer convolutional module to extract spatial patterns. Without losing generality, we utilize the first two layers of VGG-16~\cite{simonyan2014very} as this module. Specifically, two $ 3\times 3$ convolutions are employed before max-pooling with a (2, 2) stride. This is due to the fact that we employ a two-branch design, with the teacher branch requiring extensive expertise. To this purpose, the first few layers of trained model should be placed in the shared shallow module for the easy initialization of teacher branch. Moreover, it should be emphasized that because this module is also the initial few layers of student branch, it should not have too many parameters. To put it another way, don't introduce too many shallow layers in the teacher network when designing such a module.


\subsubsection{Teacher Branch} \label{teabranch}

% In addition to the first two layers in the shared shallow module, we use the remaining layers of the first 10 layers of VGG-16 as the frond-end of teacher branch. In this way, we are able to utilize the pre-trained model, as it has a good transfer learning capacity and a flexible design. Thus, this branch is employed as a teacher with specific experience who has the capacity to learn new information rapidly and the ability to guide students. To further improve the characterization capabilities of teacher branch, we use a structure similar to CSRNet~\cite{li2018csrnet}, in which the back-end is deployed by dilated convolutional layers. This type of convolution enhances the network's receptive field directly without increasing its parameters or computation complexity. Specifically, we use 3 dilated convolution layers, which is a light version of CSRNet.
In addition to using the first two layers of VGG-16 in the shared shallow module, we employ the remaining layers of the first 10 layers of VGG-16 as the front-end of the teacher branch. This allows us to utilize the pre-trained model, which has strong transfer learning capabilities and a flexible design. The teacher branch is intended to act as an experienced mentor who can quickly learn new information and guide the student branch.

% To improve the characterization capabilities of the teacher branch further, we adopt a structure similar to P2PNet~\cite{song2021rethinking} which discards superfluous steps and directly predicts a set of point proposals to represent heads in an image by incorporating first 10 convolutional layers in VGG-16 to extract deep features. 
\subsubsection{Student Branch}

% Channel capacity is a key component in determining network inference speed as well as memory consumption. To this end, we approximately use $1/4$ channels of  teacher branch to avoid heavy computation. However, referring to the principle of VGG network design, the number of channels is directly reduced to $1/4$ of the teacher branch, which is not conducive to the further expression of shallow features. To this end, we use a compromise method in which we keep the number of channels of student branches constant before the first pool layer. The remaining layers can cut the number of channels by one-fourth. In addition, we can lower the number of channels by various multiples in a similar manner.

The channel capacity of a network is a critical factor that affects both inference speed and memory consumption. {In our framework, we restrict the number of channels in the student branch to one-fourth of those in the teacher branch to reduce the computational burden of the student branch.} However, following the design principles of the VGG-16 network, directly reducing the number of channels may limit the network's feature representation capability.

To address this issue, we adopt a compromise method in which we keep the number of channels in the shared shallow module constant until the first pooling layer. After that, we reduce the number of channels in the remaining layers by one-fourth. Additionally, we can further reduce the number of channels by using powers of one-half. This approach allows us to strike a balance between channel capacity and computational efficiency while maintaining the expressive power of the network.


\subsection{Relation-in-Relation Distillation}

%The limited number of channels in the student branch hinders its ability to extract features efficiently. Fortunately, knowledge distillation gives extra supervision signals to ease this problem and accomplish effective feature learning through techniques like feature transfer and response distillation~\cite{hinton2015distilling, li2020local, gou2021knowledge, heo2019knowledge}. To this end, we employ individual feature distillation, relation-in-relation distillation and response distillation to strengthen the learning ability of student branch. Before we describe the distillation techniques, we will first introduce the feature grouping to better convey knowledge distillation.


The limited number of channels in the student branch can hinder its ability to extract features effectively. Fortunately, feature distillation offers an effective solution to this issue by providing additional supervisory signals for knowledge transfer~\cite{li2020local, gou2021knowledge, heo2019knowledge}. This method can allow the student network to capture the representational details of the teacher network at different levels but overlooks how the teacher model combines different features to form higher-level abstractions. To this end, we propose a relation-in-relation distillation strategy that enables the student network to effectively learn the associative information embedded between features in the teacher network. Before delving into the details of these techniques, we introduce a feature grouping approach to better understand our proposed knowledge distillation strategy.



\subsubsection{Feature Grouping}

In Fig.~\ref{fig:fig1}, an input image is first processed by the shared shallow module, which outputs a set of low-level features. These features are then fed into two separate branches with different channel capacities. Let us denote these features as $t_{base} (s_{base})$ to represent the outputs of the shared shallow module. To facilitate clarity and simplify analysis, we sequentially number the feature outputs from the deeper layers to the shallower layers in the network. The group partition begins with the convolution following shallow pooling operations and ends just before deep pooling operations. The student branch follows the same convention. We define the feature output of the deepest group in the teacher (student) branch as $t_1$ ($s_1 $). Formally, the features are grouped as follows:




% The feature output by the $i$-th block in the teacher branch is denoted as $t_i$, where the block partitioning begins from the convolution following the previous pooling operation to the next max-pooling. Similarly, the output of the $i$-th block in the student network is denoted as $s_i$. Formally, the features are grouped as follows:



% As shown in Fig.~\ref{fig:fig1}, an input image will first go through a shared shallow module and output a group of low-level features. These features are fed into two separate branches with different channel capacity. Without loss of generality, denote these features as $t_1 (s_1)$. Next, denote the output feature map of the $i$-th block in the teacher network as $t_i$ where the partition of blocks starts from the convolution after the previous pooling operation to the next max-pooling. Comparatively, the output of $i$-th block in the student network is denoted as $s_i$. Formally, the features are grouped into the following sets:

\begin{equation}
T{\text{ = }}\{ {t_1},\;{t_2},\;...,{t_M}\} ,\;S{\text{ = }}\{ {s_1},\;{s_2},\;...,\;{s_M}\} ,
\end{equation}\\
where $T$ and $S$ denote the feature sets of the teacher branch and the student branch, respectively. $M$ denotes the number of feature groups.


The features cannot be distilled directly since the number of channels in the two branches is not aligned. Therefore, to maintain consistency in the feature dimensions, we apply a linear transformation function $f_{ad}(\cdot)$ to the output features of various groups of the student network. This transformation function adapts $s_i$ to $s_i^\prime$, which has the same dimension as $t_i$. The transformation is defined as follows:


% The features cannot be distilled directly since the number of channels in the two branches is not aligned. As a result, we transform the output features of various blocks of the student network to maintain feature dimension consistency. More specifically, we introduce a liner transformation function $f_{ad} (\cdot)$ to adapt $s_i$ to $s_i^\prime$, who has the same dimension as $t_i$. This transformation is described as follows:

\begin{equation}
{S^\prime } = {f_{ad}}(S)  = \{ s_1^\prime ,\;s_2^\prime ,...,s_M^\prime \}, 
\end{equation}\\
where $S^\prime$ means the feature set obtained by transforming each element of $S$. It should be noted that this transformation is only employed during the training stage. 


\subsubsection{Relation-in-Relation Distillation}


\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.6]{./fig3.png}
	\caption{Overview of relation-in-relation feature transformation. {The inputs come from two different feature maps, $t_i (s_i^\prime)$ and $t_j (s_j^\prime)$, from different groups in the teacher (or student) branch, where $1 \le i < j \le M$.}}
	\label{fig:fig2}
\end{figure} 

%To some extent, the semantic changes between layers demonstrate the evolution of features, termed as so-called inter-layer 'flow' in~\cite{yim2017gift}. As a result,  exploring their relationship is beneficial in establishing the robust features of student network. For example,~\cite{liu2020efficient} densely computed the pairwise feature relationships using down-sampled block features. However, existing approaches exclusively focus on the relational information of individual pixels and neglect the relationship of different pixels in specific layers. To address this issue, inspired by ~\cite{wang2018non}, we propose a feature relation module to model the relationship of different pixels and their corresponding similarity between different layers. Furthermore, we employ distillation to assist student branch in learning feature evolution between layers.

% identifying the most repetitive patterns

%Semantic changes between layers reflect the evolution of features within the network, which is referred to as the inter-layer "flow" in~\cite{yim2017gift}. 

{In deep networks, the ability to abstract intrinsic information in images increases layer by layer, reflecting the evolution of features within the network, which is referred to as the inter-layer "flow" in~\cite{yim2017gift}.} Therefore, exploring the relationships between layers is crucial for helping student networks capture robust features. For example, SKT~\cite{liu2020efficient} densely computed pairwise feature relationships using down-sampled block features.  Existing methods mainly focus on the relationships between individual feature pixels across different groups, which does not effectively capture the relationships among pixels within features of the same group. This makes it difficult to identify regions of interest with highly repetitive patterns in remote sensing scenes, particularly when the targets are small, have irregular shapes, or are similar to other objects. To address this issue, we propose a relation-in-relation distillation method which consists of a relation-in-relation feature transformation for features from different groups. The transformation models the relationships between different pixels (named intra-feature relation) and the corresponding similarity between layers (named inter-feature relation),  after which a distillation loss is applied to help the student branch  to learn the evolution of intra-feature relations between layers.




% a distillation loss is applied to help the student branch out to learn the evolution of intra-feature relations between layers.


The overview of the relation-in-relation feature transformation is depicted in Fig.~\ref{fig:fig2}. To better elucidate the transformation, we  will utilize the teacher branch as an example. Firstly, we can obtain two different feature maps $t_i\in \mathbb{R}^{B\times C_1\times H\times W}$ and $t_j\in \mathbb{R}^{B\times C_2\times H\times W} (1\le i<j\le M)$ from different groups. Next, we apply a reshape operation to transform the shape of the two feature maps, for example, for $t_i$, we obtain two new feature maps: $t_i^1\in \mathbb{R}^{B\times (HW) \times C_1}$ and $t_i^2\in \mathbb{R}^{B\times C_1\times (HW)}$.  Multiplying $t_i^1$ by $t_i^2$, we obtain the correction matrix $K_i \in \mathbb{R}^{B\times HW\times HW}$ after applying the softmax operation $g( \cdot )$. Following the same operation, we derive the correction matrix $K_j$ for the feature map $t_j$. {To model the inter-layer relationships, we apply element-wise multiplication, which maintains the spatial structure of the intra-layer correlations while processing each position on the feature map independently. This allows the model to selectively enhance or suppress feature responses in specific regions.} Finally, the relation matrix between the $i$-th and $j$-th group in the teacher branch is obtained, denoted as $R_{ij}^t = K_i \otimes K_j$.  Formally, the inter-layer relationship matrix can be expressed as follows:
% {t_i}(s_i^\prime )
\begin{equation}\label{eq3}
R_{ij}^t = \frac{{g(t_i^1  t_i^2) \otimes g(t_j^1 t_j^2)}}{{HW}}
\end{equation} \\
where $\otimes$ denotes the element-wise product. After performing the same operation, we get the same relationship matrix of the student branch $R_{ij}^{s^\prime}$ for the $i$-th and $j$-th group.

Here we simply use the L1 distance to measure the gap between the corresponding relationship matrices. The relation-in-relation distillation loss is defined as follows:
\begin{equation}
	{L_r} = \sum\limits_{1 \le i < j \le M} {{{\left\| {R_{ij}^{{s^\prime }} - R_{ij}^t} \right\|}_1}}.
\end{equation}

% \subsubsection{Response distillation}


% Recent work has revealed that response distillation is a type of learned label smoothing regularization~\cite{yuan2020revisiting}. In the traditional two-stage architecture, MSE is typically used as the loss function, but it is difficult to obtain comparable results in our distillation paradigm. We speculate that this is because it is challenging for the teacher branch to produce a strong output early in the training process, and therefore, response distillation based on MSE is not particularly strict, making it difficult to achieve effective regularization. Inspired by~\cite{wang2004image}, we use SSIM loss as the response-based distillation loss, which is defined as follows:


% Recent work revealed that response distillation is a type of learned label smoothing regularization~\cite{yuan2020revisiting}. MSE is typically used as the loss function in the classic two-stage architecture, however it is difficult to get a comparable result in our distillation paradigm. We speculate that because it is difficult for the teacher branch to generate a strong output early in the training process, the response distillation based on MSE is not particularly stringent, making it impossible to produce an effective regularization effect. Inspired by~\cite{wang2004image}, SSIM loss is set as the response-based distillation loss as follows:


%\begin{equation}
%\begin{aligned}
%L_s&=1-SSIM(t_N,s_N^\prime),\\
%SSIM({t_N},s_N^\prime ) &= \frac{{(2{\mu _{{t_N}}}{\mu _{s_N^\prime }} + {c_1})(2{\sigma _{{t_N}s_N^\prime }} + {c_2})}}{{(\mu _{{t_N}}^2 + \mu _{s_N^\prime }^2 + {c_1})(\sigma _{{t_N}}^2 + \sigma _{s_N^\prime }^2 + {c_2})}},
%\end{aligned}
%\end{equation}   \\
%where ${{\mu _{{x}}}}$ denotes the mean intensity of input $x$, ${\sigma _{{y}}}$ denotes the standard deviation of input $y$, and ${\sigma _{xy}}$ denotes the covariance of input $x$ and $y$. 
%




\subsection{Total Loss}

To optimize the network parameters, the overall optimization loss is given as follows:


\begin{equation} \label{loss}
	{L_{total}} =  {\alpha _1}{L_f} + {\alpha _2}{L_r} + {L^s }+ {L^t },
\end{equation} \\
where $L_f$ denotes the feature distillation loss, defined as follows:
\begin{equation}
L_f=\sum_{i=1}^M{\left\| t_i-s_{i}^{\prime} \right\| _{2}^{2}.}
\end{equation}
where ${\left\|  \cdot  \right\|_2}$ denotes the 2-norm. Here, we use  the mean squared error loss (MSE loss) to align features from the same group of teacher and student networks, where we apply $1\times 1$ convolutions to the features of the student branch to match the number of channels with that of the teacher branch.  $\alpha_1$ and $\alpha_2$ are tunable hyper-parameters to balance the loss terms. ${L^s }$ and ${L^t }$ denote the supervision signals of two branches, respectively. {For each branch, following the optimization loss in P2PNet~\cite{song2021rethinking}, we employ the cross-entropy loss $L_{cls}^\ell$ and the MSE loss $L_{loc}^\ell $ to supervise the classification head and regression head, respectively, where $\ell = s, t$. Formally, the final loss ${L^\ell }$  is defined as follows:}
\begin{equation}
\begin{array}{l}
	{L^\ell } = L_{cls}^\ell  + \vec \lambda L_{loc}^\ell ,\\
	L_{cls}^{\ell}=-\frac{1}{V}\left( \sum_{i=1}^U{\log \widehat{c}_{\xi (i)}}+\hat{\lambda}\sum_{i=U+1}^V{\log\mathrm{(}1-\widehat{c}_{\xi (i)})} \right), \\
	L_{loc}^{\ell}=\frac{1}{U}\sum_{i=1}^U{\left\| p_i-\widehat{p}_{\xi (i)} \right\| _{2}^{2},}
\end{array}
\end{equation}
{where $U$ and $V$ denote the number of positive and total proposals, respectively,  ${p_i}$ represents the head’s center point of the individual, ${\widehat c_{\xi (i)}}$ is the confidence score of the predicted point ${\widehat p_{\xi (i)}}$, $\hat \lambda $  is a weight factor for negative proposals, and $\vec \lambda$  is a weight term to balance the effect of the regression loss.}







% the classification head and regression head, respectively. Here, in line with the optimization loss used in P2PNet, we employ cross-entropy loss and Euclidean loss to supervise the aforementioned two heads, respectively.





%\subsubsection{Feature distillation}
%
%
%Fine-grained semantic information is contained in multiple levels of features. Distilling features is a direct method for helping students express themselves in the same way as teacher do. To this end, we build a feature internal distillation method to minimize distribution similarity between  $s_i^\prime$ and $t_i$. To achieve such a goal, mean square error (MSE) between features is a natural and simple choice. 
%As the semantic structure of teacher and student branches differs, this choice may be overly tight, which will have a negative impact on the distillation. Moreover, our experiments show that this distillation method has poor compatibility with other types of distillation, not to mention its heavy calculation. Instead, we use a light metric,  vector feature loss, to measure the similarity of  $s_i^\prime$ and $t_i$. This metric not only reduces the redundant information between features, but also forms a complementary learning strategy with other methods. Concretely, we first feed both features $s_i^\prime$ and $t_i$ into adaptive average pool operation $p\left( \cdot \right) \in {\mathbb{R}^{B \times C \times H \times W}} \to {\mathbb{R}^{B \times C \times 1 \times 1}}$. Then the feature internal distillation is calculated by:

%\begin{equation}
%	L_f=\sum_{i=2}^N \frac{1}{C_i}\| p(s_i^\prime) - p(t_i) \|_2^2
%\end{equation}\\
%where $C_i$ is the channel number of $s_i^\prime$ and $t_i$ and $N$ is the number of blocks.
%
%
%
%
%Finally, we perform knowledge distillation by minimizing the total loss as follows:
%
%\begin{equation} \label{loss}
%L = L_{st}+L_{tea}+\alpha_1L_f+\alpha_2L_r
%\end{equation} \\
%where $L_{st}$ and $L_{tea}$ are the MSE loss of student and teacher branches, $\alpha_1$ and $\alpha_2$ are tunable hyper-parameters to balance the loss terms.


\section{Experiment}

In this section, we conduct extensive experiments on two challenging datasets to illustrate the effectiveness of our proposed strategy.


\subsection{Experiment Settings}

% All the experiments in this study are implemented with PyTorch.  For data augmentation, we first randomly scale the image to 0.8 to 1.2 times its original size. The scaled picture is then randomly cropped into a 400 $\times$ 400 patch. Finally, we randomly flip it and apply gamma correction to it. As for the proposed network, VGG-16 pretrained on ImageNet is used to initialize the first ten layer parameters of the shared shallow module and the teacher branch. The other layers are initialized by a random Gaussian distribution with 0.01 standard deviation. As for training, we set the initial learning rate of student branch to be $1e^{-4}$, while that of teacher branch is $1e^{-6}$. The same hyper-parameters settings $( \alpha_1=1, \alpha_2=10, \alpha_3=1000 )$ are adopted for all the experiments. We train the network for 600 epochs on ShanghaiTech Part-A and UCF-QNRF, 500 epochs on the rest two datasets with Adam optimizer. The four benchmarks mentioned will be introduced in the following parts.


All experiments in this study are implemented using PyTorch. Data augmentation is performed as follows: The image is randomly scaled to 0.7 to 1.3 times its original size. The scaled image is then randomly cropped into four 128 $\times$ 128 patches. Next, random flipping is applied. VGG-16\_bn pretrained on ImageNet is used to initialize the first ten layers of the shared shallow module and the teacher branch. The remaining layers are initialized by a random Gaussian distribution with a standard deviation of 0.01. For training, the initial learning rate of the student branch and teacher branch is set to $1e^{-4}$. The same hyper-parameter settings $( \alpha_1=0.5, \alpha_2=5e^{-3} )$ are adopted for all experiments. The network is trained for 1500 epochs on the building dataset and 3000 epochs on the remaining datasets. The Adam optimizer is utilized to optimize the network parameters. 

{To ensure a fair evaluation of  counting methods, we use the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) as evaluation metrics. Additionally, we characterize the model's computational complexity using training time (Time), the number of parameters (Param), and Floating Point Operations Per second (FLOPs), with the input resolution set to $256 \times 256$. Note that Time denotes the training time of models on Large-vehicle.}




\subsection{Datasets}
%\begin{table*}
%	\centering
%	\caption{Remote Sensing Object Counting Datasets }
%	\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
%		
%		\hline
%		\textbf{Dataset} & \textbf{Platform} & \textbf{Images} & \textbf{Training/test} & \textbf{Average Resolution} & \textbf{Annotation Format} & \multicolumn{4}{c}{\textbf{Count Statistics}}  \\
%		\cline{7-10}
%		& & & & & & \textbf{Total} & \textbf{Min} & \textbf{Average} & \textbf{Max} \\
%		\hline\hline
%		Building & satellite & 2468 & 1205/1263  & 512×512 & center point & 76,215 & 15 & 30.88 & 142 \\
%		Small-vehicle & satellite & 280 & 222/58  & 2473 × 2339 & oriented bounding box & 148,267 & 17  & 529.52  & 9930 \\
%		Large-vehicle & satellite & 172 & 108/64  & 1552 × 1573 & oriented bounding box & 16,594 & 12  & 96.48  & 1336 \\
%		Ship & satellite & 137  & 97/40  & 2558 × 2668  & oriented bounding box & 44,892 & 50  & 327.68  & 1661 \\
%
%		\hline
%
%
%	\end{tabular}
%	\label{statistics}
%\end{table*}
%\textbf{ShanghaiTech Part-A}~\cite{zhang2016single}: This dataset includes 482 images collected from the Internet, where 300 images are used for training and the rest for testing.
%
%\textbf{UCF-QNRF}~\cite{idrees2018composition}: It is a challenging dataset, which contains 1,535 images collected from Internet with high-resolution. The annotated heads of samples range form 49 to 12,865, which has a huge variation of crowd density. There are 1,201 images for training and 334 images for testing.
%
%\textbf{JHU-CROWD++}~\cite{sindagi2020jhu}: This is a large scale unconstrained crowd counting dataset. It is collected under a variety of diverse scenarios and environmental conditions which contains 4,372 images with 1.51 million annotations.  There are 2,772 images for training and 1600 images for testing.\par
%
%\textbf{ NWPU-Crowd}~\cite{wang2020nwpu}:This is the largest crowd counting and localization benchmark. It contains various illumination scenes and has the largest density range from 0 to 20,033. It is made up of 1,525 images with a total of 1,251,642 label points. The average number of pedestrians per image is 815, with a high of 12,865. For most models, this configuration is a tremendous difficulty.
%The remote sensing dataset consists of four sub-datasets, as detailed in Table~\ref{statistics}.


 RSOC is a publicly available dataset~\cite{gao2020counting} used to evaluate the counting performance of various classes of remote sensing objects. It includes 280 images of small vehicles, 172 of large vehicles, 137 of ships, and 2,468 images of buildings, totaling 3,057 images.  The dataset excludes easy cases with disperse objects, focusing on challenging scenarios like crowded ships near shores and densely packed vehicles in parking lots. For a fair evaluation of our method, we partition the dataset as follows for each category:  Small-vehicle includes 200 images for training, 22 for validation, and 58 for testing;  Large-vehicle has 98 training images, 10 validation images, and 64 testing images;  Ship consists of 88 training images, 9 validation images, and 40 testing images; and Building, which has the highest count, features 964 training images, 241 validation images, and 1,263 testing images.


{STAR~\cite{li2024star} is introduced for scene graph generation in large-scale high-resolution satellite imagery. The dataset includes over 210,000 annotated objects, with an emphasis on diverse scenarios. We select five typical object categories from this dataset as analysis objects: airplanes, windmills, lattice towers, bridges, and runways.  Specifically, for the airplane category, 233 images are selected for training and 106 for validation. The windmill category includes 29 images for training and 11 for validation. For lattice towers, there are 132 images for training and 39 for validation. In the case of large-scale objects, the bridge category contains 198 images for training and 43 for validation, while the runway category includes 239 images for training and 106 for validation.}

%  To evaluate our method fairly, we split the dataset for each category into the following parts: the small-vehicle category has 200 images for training, 22 for validation, and 58 for testing; the large-vehicle category consists of 98 training images, 10 for validation, and 64 for testing; the ship category contains 88 training images, 9 validation images, and 40 testing images; and lastly, the building category, with the highest count, includes 964 training images, 241 validation images, and 1263 testing images. 


% For building dataset, its images are sourced from Google Earth, while the images of small and large vehicles, along with ships, are taken from the DOTA dataset\cite{xia2018dota}, which is designed for object detection in aerial imagery.

 % As for small vehicle dataset, the annotations are quite dense, averaging around 530 per image, with a maximum of 9,930 annotations for a single image. Compare with it, large vehicle dataset has fewer annotations on average, with about 96 per image and a maximum of 1,336 among all samples. The ship dataset has a moderate level of density, with an average of 328 annotations per image and a maximum of 1,661. 


% The images for the building category are sourced from Google Earth, providing a diverse collection of building imagery, while the images for the small and large vehicles, as well as the ships, are derived from the DOTA dataset, which is specifically designed for object detection in aerial imagery. Notably, these datasets exclude easy cases with dispersed objects, instead focusing on challenging scenarios such as crowded ships near shores and densely packed vehicles in parking lots. The distribution of images in each category is as follows: the small-vehicle category has 200 images for training, 22 for validation, and 58 for testing, totaling 280 images; the large-vehicle category consists of 98 training images, 10 for validation, and 64 for testing, making a total of 172 images; the ship category contains 88 training images, 9 validation images, and 40 testing images, summing to 137 images; and lastly, the building category, with the highest count, includes 964 training images, 241 validation images, and 1263 testing images, bringing the total to 2468 










\subsection{Ablation Study}
\begin{table*}
	\caption{Comparison of two-stage distillation and online distillation. {The parameters provided here refer exclusively to those used during the inference phase of the model. }}
	\label{tab:two phase}
	\centering
	\begin{tabular}{ccccc}
		\hline
		Method                                             & Param(M)     & Time (min)     & MAE        &RMSE     \\
		\hline    
		SKT~\cite{liu2020efficient}                        &  1.02         &   459     &42.43      &   68.42                \\
		\hline
		Two-stage Distillation (Ours)                     & 0.76       &   141       &21.97	& 50.57                    \\
		OnKL Net  w/o pre-trained VGG-16            &0.76     & 85   &  14.58		&24.09                       \\
		OnKL Net                                       &  0.76         &  85         &\textbf{12.27}  &\textbf{16.70}             \\
		\hline
	\end{tabular}
\end{table*}

In this part, we conduct  ablation studies on Large-vehicle to assess the effectiveness of our proposed method. As mentioned in~\cite{liu2020efficient}, the number of channels is an important factor in balancing efficiency and accuracy in knowledge distillation. Therefore, we follow its approach and set the channel number of the student branch to be one-fourth of the teacher branch. We provide a detailed analysis of our ablation study in the following subsections.

% In this part, we perform ablation study on ShanghaiTech Part A to evaluate the effectiveness of our proposed method. For simplicity,  we will refer to our proposed crowd counting distillation network as OnKL Net. As reported in~\cite{liu2020efficient}, channel numbers are a key factor in the balance between efficiency and accuracy for knowledge distillation. Following their study, we default the channel number of the student branch to 1/4 of the teacher branch. In what follows, a more detailed analysis of ablation study will be offered. 

%\renewcommand{\thesubfigure}{a$_1$}
%\begin{subfigure}[b]{0.16\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{./tea0.jpg}
%	\caption{$t_1 \rightarrow t_2$}
%\end{subfigure}
%\hspace{-0.35cm}
%\renewcommand{\thesubfigure}{b$_1$}
%\begin{subfigure}[b]{0.16\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{./tea1.jpg}
%	\caption{$t_2 \rightarrow t_3$}
%\end{subfigure}
%\hspace{-0.35cm}
%\renewcommand{\thesubfigure}{c$_1$}
%\begin{subfigure}[b]{0.16\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{./tea2.jpg}
%	\caption{$t_1 \rightarrow t_3$}
%\end{subfigure}
%
%\vspace{0.2cm}
%
%% Second row
%\renewcommand{\thesubfigure}{a$_2$}
%\begin{subfigure}[b]{0.16\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{./st0.jpg}
%	\caption{$s_1 \rightarrow s_2$}
%\end{subfigure}
%\hspace{-0.35cm}
%\renewcommand{\thesubfigure}{b$_2$}
%\begin{subfigure}[b]{0.16\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{./st1.jpg}
%	\caption{$s_2 \rightarrow s_3$}
%\end{subfigure}
%\hspace{-0.35cm}
%\renewcommand{\thesubfigure}{c$_2$}
%\begin{subfigure}[b]{0.16\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{./st2.jpg}
%	\caption{$s_1 \rightarrow s_3$}
%\end{subfigure}







% Brighter colors denote stronger relation values.


%\begin{figure*}[htbp]
%	\centering
%	% First row
%	\renewcommand{\thesubfigure}{a$_1$}
%	\begin{subfigure}[b]{0.33\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{./tea0.jpg}
%		\caption{$t_1 \rightarrow t_2$}
%	\end{subfigure}
%	\hspace{-0.35cm} % Negative space to reduce gap
%	\renewcommand{\thesubfigure}{b$_1$}
%	\begin{subfigure}[b]{0.33\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{./tea1.jpg}
%		\caption{$t_2 \rightarrow t_3$}
%	\end{subfigure}
%	\hspace{-0.35cm} % Adjust as needed
%	\renewcommand{\thesubfigure}{c$_1$}
%	\begin{subfigure}[b]{0.33\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{./tea2.jpg}
%		\caption{$t_1 \rightarrow t_3$}
%	\end{subfigure}
%	
%	\vspace{0.2cm}
%	
%	% Second row
%	\renewcommand{\thesubfigure}{a$_2$}
%	\begin{subfigure}[b]{0.33\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{./st0.jpg}
%		\caption{$s_1 \rightarrow s_2$}
%	\end{subfigure}
%	\hspace{-0.15cm} % Negative space to reduce gap
%	\renewcommand{\thesubfigure}{b$_2$}
%	\begin{subfigure}[b]{0.33\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{./st1.jpg}
%		\caption{$s_2 \rightarrow s_3$}
%	\end{subfigure}
%	\hspace{-0.15cm} % Adjust as needed
%	\renewcommand{\thesubfigure}{c$_2$}
%	\begin{subfigure}[b]{0.33\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{./st2.jpg}
%		\caption{$s_1 \rightarrow s_3$}
%	\end{subfigure}
%\end{figure*}

\paragraph{Exploration on Online Knowledge Learning}\label{online}
We present the experimental results to evaluate the effect of online knowledge learning. Table~\ref{tab:two phase} shows the comparison between two-stage distillation and online distillation. Our proposed online distillation method outperforms the two-stage distillation method by a large margin, demonstrating the benefits of joint training. Specifically, our online distillation achieves a performance gain of more than 44\% over the two-stage distillation (Ours) in terms of MAE and RMSE. Moreover, compared to the two-stage version, the online version only requires about 60\% of its training time, even with only 98 training samples. This means that the time discrepancy between the two methods can become increasingly apparent as the sample size grows. To further validate the superiority of our method, we compare our network with different baselines in terms of inference speed. Here, all experiments are tested on an NVIDIA GeForce RTX 3090, with the test images having a resolution of $512 \times 512$. Our inference speed reaches 319.9 frames per second (FPS), which is superior to P2PNet$^{*}$'s 111.3 FPS and P2PNet's 88.6 FPS. Thereinto,  P2PNet$^{*}$ is a simplified version of P2PNet, with reduced channel numbers in both its regression and classification branches. 



Our proposed solution of online distillation, with only 0.75 times the parameters of the student network in SKT~\cite{liu2020efficient}, is greatly better than this two-stage method (12.27 (16.70) vs. 42.43 (68.42)). These results demonstrate the efficacy of our distillation method as well as its high potential for knowledge transfer. Furthermore, our final solution initializes with the parameters of a pre-trained VGG-16 model, resulting in a slight performance improvement compared to the model without it (12.27(16.70) vs. 14.58(24.09)). This demonstrates that the teacher branch with prior knowledge can better support the student branch to capture effective knowledge. {We observe that the version of OnKL Net without a pre-trained model outperforms the two-stage distillation version. We speculate that this may be due to the current distillation strategies, which fail to effectively transfer knowledge from the teacher model, hindering the network's ability to achieve the desired results. In comparison, OnKL Net, which is also initialized by a pre-trained model, achieves even more superior performance. This further demonstrates the effectiveness of the  distillation strategies within our distillation framework.}








% In this part, we conduct experiments to evaluate the effect of online knowledge learning. Table~\ref{tab:tab6} presents the comparison results of two-stage distillation and online distillation. To begin with, Online distillation outperforms the two-stage distillation by a large margin, which indicates the benefits of joint training. In particular, online distillation obtaining a performance gain of about 8\% over two-stage distillation. It can also be shown that the speed of the online version is almost 1.4 times that of the two-stage version, even with only 300 training samples. That means that the training time discrepancy becomes increasingly apparent as the sample size grows; for example, our one-phase version requires more than 7310 minutes of training time for NWPU-Crowd dataset. Then, with just 0.67 times the parameters of SKT~\cite{liu2020efficient}, our solution of online distillation is slightly better than this two-stage method (70.02 Vs. 71.55). These results demonstrate the efficacy of our distillation method as well as its high potential for knowledge transfer.  Last,  compared with the model without pre-trained VGG version, our last solution has achieved significant performance improvement (70.2 Vs. 101.78). This demonstrates that the teacher branch with prior knowledge can better support the student branch to capture effective knowledge.




\paragraph{Exploration on Knowledge Distillation Configurations}


To achieve efficient knowledge transfer, we employ two knowledge distillation strategies: feature distillation (FD) and relation-in-relation distillation (RiRD). We establish a baseline without any distillation strategies to evaluate the effectiveness of different losses. Table~\ref{tab:tab1} presents the ablation results for the two distillation losses. As shown in the table, both loss functions significantly enhance the performance of the student network, demonstrating the effectiveness of these methods. The combination of these two losses further improves performance, yielding an MAE of 12.27 and an RMSE of 16.70, which suggests that these losses are complementary.


% To facilitate knowledge distillation, we adopt two modules: feature distillation (FD), feature relation distillation (FRD). To evaluate the effectiveness of these modules, we establish a baseline(P2PNet$^*$), which is a student network consisting of the shared shallow module and the student branch, without knowledge distillation.(Baseline: our solution without two modules)

% Table~\ref{tab:tab1} summarizes the results of an ablation study on the two knowledge distillation modules. The results show that FD loss significantly improve the performance of the student network compared to the baseline, indicating that they successfully transfer effective features. Moreover, the combination of these two losses further improves performance, with a MAE of 12.27. This suggests that these two modules are complementary to each other.
%  However, FID loss seems to have little effect on knowledge transfer and achieves similar performance to the baseline. This indicates that the supervised signal provided by FID loss is weak when used alone. When combined with other losses, its performance varies. Interestingly, when paired with FRD loss and RD loss, the performance is marginally enhanced, and the MAE is increased by 0.65. We speculate that FRD and RD loss may be too stringent for knowledge transfer, and FID loss can effectively alleviate this issue. Next, we will evaluate the performance of each module separately.




% To accomplish the process of knowledge distillation, three modules are employed: feature internal distillation (FID), feature relation distillation (FRD), and response distillation (RD). To demonstrate the power of these three modules, we present a baseline, which is the combination of the shared shallow module and the student branch (referred to as the student network) without knowledge distillation. 

% The ablation study of three knowledge distillation modules is summarized in Table~\ref{tab:tab1}. As can be observed, both FRD loss and RD loss have significantly improved their performance compared to the baseline, indicating that they have successfully transferred certain regional features. Furthermore, the combination of the two will continue to increase their performance, with the MAE reaching 70.62. This demonstrates that the two are, to some extent, complimentary. Besides, it is easy to observe that this loss does not seem to play a role in the process of knowledge transfer and achieves about the same performance as the baseline. This implies that the supervised signal provided by the loss is weak when utilized alone. When it is combined with other losses, the performance degrades differently. Interestingly, when we paired it with FRD loss and RD loss, the performance was marginally enhanced, and the MAS rose by 0.65. We hypothesize that FRD and RD loss are excessively strict for knowledge transfer, and that FID can adequately ease this issue. Following that, we will examine the performance of each module one by one.



\begin{table}
	\caption{Ablation study of knowledge distillation strategies.}
	\label{tab:tab1}
	\centering
	\begin{tabular}{ccccc}
		\hline
		Module         & FD loss     & RiRD loss       & MAE           & RMSE \\
		\hline
		Baseline       &             &                & 19.64         &37.78  \\
		\hline        
		               & \checkmark  &                &   13.59       &18.73  \\
		               &             &  \checkmark    &15.42          &24.29  \\
		               & \checkmark  &  \checkmark    &  \textbf{12.27}     &\textbf{16.70}\\
		\hline
	\end{tabular}
\end{table}


\begin{table}
	\caption{Comparison of different feature relation distillation losses. {RiRD loss denotes the relation-in-relation distillation loss.}}
	\label{tab:tab3}
	\centering
	\begin{tabular}{ccc}
		\hline
		Transfer Configuration       & MAE  &RMSE\\
		\hline
		% W/O RiRD \& FD losses     & 19.64                &37.78 \\
		W/O RiRD loss             &13.59  &18.73\\
		\hline
		+ FSP loss~\cite{liu2020efficient}     &14.375  &20.92\\
		
		+ Cos loss                             &14.73 &19.65\\
		
		% + Sparse FRD loss                       &13.02  &17.80\\
		
		+ RiRD loss                              &\textbf{12.27}  &\textbf{16.70}\\
		\hline
	\end{tabular}
\end{table}

\paragraph{Effects of Relation-in-Relation Distillation} 

Table~\ref{tab:tab3} compares the performance of our proposed relation-in-relation distillation with other strategies. Our results show superior performance. It is notable that incorporating both FSP loss~\cite{liu2020efficient} and Cos loss to measure relationships between features in different groups results in lower performance compared to using only feature distillation. We speculate that this may be due to the highly similar patterns in remote sensing images, which complicate the task of mining feature relationships and may even degrade performance. Additionally, FSP is designed for distillation from a well-trained teacher network, whereas the features of the teacher branch in our network are dynamically changing, leading to weaker pixel-level feature correlations. This suggests that our method is effective in handling such complex scenarios.



% To explore the information contained in features, we propose the relation distillation between features, which will capture the relationship between different pixels and different levels of features. There are two setups for our proposed FRD: sparse connected version (Sparse FRD) and densely connected version (FRD). FRD means that the elements in the feature sets (i.e.,  Eq. (5)) will be calculated for their corresponding relationships while Sparse RD only focuses on the relationship between adjacent elements. As shown in Table~\ref{tab:tab3}, Among all versions, both the FRD loss and sparse FRD loss demonstrated a significant improvement, achieving the best MAE of 12.27. Notice that FSP loss~\cite{liu2020efficient} proposed for the inter-layer relation transfer haven't achieved a small amount of performance improvement. We attribute the weaker performance of the model to two factors. On the one hand, the FSP loss only considers the correlation of element pixels across different feature sets and overlooks the correlation of pixels within the features themselves. On the other hand, the correlation of the FSP loss in the original teacher network is fixed, while the features of the teacher branch in our network are dynamic, resulting in a weaker correlation established by the FSP.


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{./relation-new.jpg}
	\caption{Visualization of  the inter-layer relationship matrix. The relationship matrices of the teacher and student branches are represented in the first and second rows, respectively. ${t_i} \rightarrow {t_j} ({s_i} \rightarrow {s_j})$ denotes the relation matrix generated by ${t_i} ({s_i})$ and ${t_j}  ({s_j})$. To clearly highlight between strong and weak relevant relationships, we apply the transformation $-1/log(x)$ to the non-zero values of the relationship matrix.}
	\label{fig:fig4}
\end{figure}



\begin{figure*}[ht] 
	\centering
	\includegraphics[scale = 0.55]{./vis.jpg}
	\caption{Comparison of predicted results on Building and Small-vehicle. {The first three rows show the images sampled from Building, while the remaining rows feature images of Small-vehicle.}}
	\label{fig:vis}
\end{figure*} 








Furthermore, Fig.~\ref{fig:fig4} depicts a visualization of the inter-layer relationship matrix, as presented in Eq.~\ref{eq3}. The first row of visualizations reveals that parts of relationship matrices between shallow and intermediate layer features are dispersed, whereas the association matrices between these features and deep layer features are highly concentrated. {This difference may be because shallow features and intermediate features capture basic characteristics and slightly more complex structural information of inputs, respectively. In contrast to deep semantic features, these relatively coarse features, which may contain some noise, lead to greater variation in the internal relationships between features. High-level features, on the other hand, capture more abstract semantic information, resulting in higher similarity within their internal relationships. This causes the feature tensors representing intra-layer relationships to have higher brightness values along the diagonal, with lower values elsewhere. Here, we apply element-wise multiplication to model the inter-layer relationships. This operation allows the model to selectively enhance or suppress feature responses in specific regions. Therefore, we can observe that $b_1$($b_2$) and $c_1$($c_2$) in the figure are relatively similar. Furthermore, we also notice that the relationship matrix of the student branch is very close to that of the teacher branch, indicating that RiRD can effectively help the student network better understand the hierarchical feature differences.}




% This discrepancy is likely because shallow and intermediate features capture basic and slightly more complex partial structural information, leading to greater variability in their internal feature relationships. In contrast, high-level features extract more abstract semantic information, resulting in higher similarity within their internal feature relationships. Additionally, we observe that the relationship matrices of the student branch closely approximate those of the teacher branch, suggesting that RiRD effectively helps the student network better understand the hierarchical feature differences.





% The first row of the visualization shows that in low-level features, the relation matrix is decentralized, whereas in high-level features, the relation matrix is highly centralized. This can be attributed to the fact that low-level features contain detailed information such as edges, corners, and texture, leading to a significant degree of similarity between each pixel in the feature. On the other hand, high-level features extract extensive semantic information, resulting in significant variances between channels and various blocks. After distillation, we observe that the student branch approximates the relation matrix of the teacher branch while its distribution is more uniform compared to that of the teacher branch. This demonstrates that the FRD helps students in better understanding the differences between hierarchical features.

\begin{table}[t]
	\caption{Comparison of different feature distillation losses. {FD loss denotes the feature distillation loss.}}
	\label{tab:tab4}
	\centering
	\begin{tabular}{ccc}
		\hline
		Transfer Configuration       & MAE  &RMSE\\
		\hline
		% W/O RiRD \& FD losses     & 19.64                &37.78 \\
		W/O FD loss             &15.42  &24.29\\
		\hline
		+ SSIM loss                       &15.23  &24.08\\
		+ Cos loss                        &12.56  &17.81\\
		+ MSE loss                        &\textbf{12.27}  &\textbf{16.70}\\
		
		\hline
	\end{tabular}
\end{table}

\paragraph{Effects of Feature Distillation} 


In this study, we assess the impact of different distillation losses on the performance of the student network by comparing three common losses: SSIM loss, Cos loss~\cite{liu2020efficient} and MSE loss. As shown in Table~\ref{tab:tab4}, all three feature distillation methods  can improve the student network's performance compared to not using any feature distillation loss. We notice that SSIM loss provides the least improvement. This may be because it places excessive emphasis on structural information while neglecting pixel relationships. It can hinder the student network's ability to capture subtle differences, which is important for detecting weak and small targets in remote sensing images. Both Cos loss and MSE loss perform similarly in terms of MAE, but MSE loss shows better generalization capability. Therefore, we select MSE loss as the feature distillation loss.


\paragraph{Effects of Different Numbers of Feature Groups} 

{In relation-in-relation distillation, the number of feature groups $M$ plays a crucial role in determining the effectiveness of the distillation. In this paper, we set $M$ to 3, based on the fact that the first ten layers of VGG-16 have a total of 4 blocks, with the shallowest block serving as a common block. Thus, we investigate the impact of the number of feature groups on network performance, as presented in Table~\ref{tab:tab4x}. From the results, it is evident that using only the deepest feature $ t_1 $ fails to generate meaningful inter-feature relationships, leading to poor performance. However, as shallower features are incorporated, the network performance improves significantly, with the best results achieved when $ M = 3 $.}



\begin{table}[t]
	\caption{Comparison of different numbers of feature groups. {$t_i$ ($s_i$) denotes the $i$th feature group in teacher (student) branch.}}
	\label{tab:tab4x}
	\centering
	\begin{tabular}{ccccc}
		\hline
		$t_1$ ($s_1$) & $t_2$ ($s_2$) & $t_3$ ($s_3$) & MAE   & RMSE  \\
		\hline
		$\checkmark$          &             &             & 16.34 & 23.92 \\
		$\checkmark$          & $\checkmark$           &             & 14.73 & 18.40 \\
		$\checkmark$           & $\checkmark$           & $\checkmark$           & \textbf{12.27} & \textbf{16.70} \\
		\hline
	\end{tabular}
\end{table}
\begin{table}[t]
	\caption{Comparison of different pretrained models. OnKL Net* denotes that we remove the residual connections in the student network.}
	\label{tab:tab4y}
	\centering
	\begin{tabular}{c|c|cc}
		\hline
		Method                   & Backbone & MAE   & RMSE  \\
		\hline
		\multirow{3}{*}{P2PNet}  & ResNet-34 & 19.91 & 64.97 \\
		& ResNet-50 & \textbf{11.28} & 18.50 \\
		& VGG-16      & 11.61 & \textbf{16.94} \\
		\hline
		\multirow{3}{*}{P2PNet*} & ResNet-34 & 16.50  & 37.95 \\
		& ResNet-50 & \textbf{12.63} & 19.08 \\
		& VGG-16      & 12.89 & \textbf{17.50} \\
		\hline
		\multirow{3}{*}{OnKL Net}    & ResNet-34 & 19.66 & 40.46 \\
		& ResNet-50 & 19.11 & 27.90 \\
		& VGG-16      & \textbf{12.27} & \textbf{16.70} \\
		\hline
		\multirow{2}{*}{OnKL Net*}   & ResNet-34 & 18.06 & 33.55 \\
		& ResNet-50 & \textbf{14.98} & \textbf{27.30}\\
		\hline
	\end{tabular}
\end{table}
% P2PNet$^{*}$  denotes a simplified channel version of P2PNet in both the regression and classification branches.

\paragraph{Effects of Different Pretrained Models} 


{We further examine the performance of different pre-trained backbone networks within our distillation framework, as summarized in Table~\ref{tab:tab4y}. The results show that in terms of pre-trained ResNets, the deeper version performs better on P2PNet and P2PNet*. By comparison, the VGG-16-based counting model shows greater potential for application than the ResNet-based one when considering both evaluation metrics. We speculate that this is due to the significant down-sampling in the early layers of ResNet, which limits the network's ability to capture fine details in remote sensing scenes—an essential factor for accurate object counting. When directly applying ResNet-34 and ResNet-50 to our method, we do not achieve satisfactory results. To address this, we build the student network by removing the residual structures and retaining only the $3\times3$ convolutional layers based on the channel-reduced ResNets. Clearly, it results in a notable  improvement. This suggests that, while residual structures aid in optimizing deep networks, they may not be suitable for the student model in our proposed framework. We speculate that with small datasets, residual structures could lead to overfitting, which in turn degrades performance.}




% Please add the following required packages to your document preamble:
% \usepackage{multirow}





% Here, we go over the impact of feature distillation on the framework of this study in detail. To verify the benefit of our solution, we compare it to three commonly used metrics: MSE loss , Cos loss~\cite{liu2020efficient} and SSIM loss. As shown in Table~\ref{tab:tab4}, the performance of all versions has shown significant improvement, with Cos Loss achieving the best MAE of 13.34.However, we chose to use MSE loss as the feature distillation loss. We speculate that this is because MSE  losses can provide a stronger supervision signal than Cos loss for feature-level knowledge distillation, providing a more straightforward way to minimize the error in feature representations. Using MSE loss allows for better compatibility with relational distillation, which is more conducive to effective feature learning.
%MSE and Cos losses are capable of fully transferring the knowledge stored at multiple layers, but KID loss is unable of doing so. 
%This is mostly due to the fact that the KID only saves information between channels after a series of pooling operations, and the learned feature information is severely lost. It should be noted that the KID enjoys the benefits of less computation compared to the other two losses.  As depicted in Table~\ref{tab:tab3},  it has good compatibility despite the poor performance of KID loss. As discussed above, we speculate that this is because MSE and COS loss can give a strong supervision signal for feature-level knowledge distillation, but they have a competing relationship with FRD loss and RD loss, making it difficult for the network to focus on effective feature learning. 


%
%\begin{table}
%	\caption{Comparison of Feature Distillation Losses. }
%	\label{tab:tab2}
%	\centering
%	\begin{tabular}{ccc}
%		\hline
%		Loss         & MAE  & RMSE\\
%		\hline
%		Baseline     &19.64   &37.78\\
%		\hline
%		+ MSE loss     &13.59 &18.73 \\
%		+ Cos loss     &13.34 &17.72\\
%		+ SSIM loss     &13.64 &21.81  \\
%		\hline
%	\end{tabular}
%\end{table}











% We attribute its performance to two factors: first, FSP loss only considers the correlation of element pixels in different feature sets, while ignoring the correlation of pixels within the feature itself; second, the relevance of FSP loss in the original teacher network is fixed, whereas the features of teacher branches in this network architecture are dynamic, resulting in the weak relevance built by FSP. Our solution effectively addresses the aforementioned issues and produces superior performance.

% the two versions have improved significantly in performance, with FRD achieving the best MAE of 74.43.


%\begin{table}
%	\caption{Comparison of Feature Relation Distillation Losses.}
%	\label{tab:tab4}
%	\centering
%	\begin{tabular}{cc}
%		\hline
%		Loss                                 & MAE \\
%		\hline
%		Baseline                             &92.45  \\
%		\hline
%		+ FSP loss~\cite{liu2020efficient}     &80.24  \\
%		
%		+ Cos loss                             &84.92\\
%		
%		+ Sparse FRD loss                       &75.86  \\
%		
%		+ FRD loss                              &\textbf{74.43} \\
%		\hline
%	\end{tabular}
%\end{table}


% In addition, Fig.~\ref{fig:fig4} visualizes the output of FRD module, namely the relation matrix of different feature groups. From the first row, we can observe that in low-level features, the relation matrix is decentralized, but in high-level features, the relation matrix is highly centralized. This is simple to understand because the low-level features contain some detailed information such as edge, corner and even texture so that each pixel in the feature has have a significant degree of similarity. In contrast, high-level features may extract extensive semantic information, and there are significant variances between channels and various blocks. After distillation, we find that the student branch approximates the relation matrix of teacher branch while its distribution is more uniform than that of teacher branch. This demonstrates that the FRD will assist students in better understanding the differences between hierarchical features.







%\begin{figure*}[!t]
%	\centering
%	\includegraphics[scale=0.3]{./relation1.png}
%	\caption{Relation matrices of different feature groups. The relation matrix is produced by trained network on a test sample of RSOC\_building. The relationship matrix of the teacher and student branches is represented in the first and second rows, respectively. ${t_i} \to {t_j} ({s_i} \to {s_j})$ denotes the relation matrix generated by ${t_i} ({s_i})$ and ${t_j}  ({s_j})$. Brighter colors denote stronger relation values}
%	\label{fig:fig4}
%\end{figure*} 

%\begin{figure*}[!t]
%	\centering
%	\includegraphics[scale=0.3]{./relation2.png}
%
%\end{figure*} 

%{\bf{Response distillation}} Table~\ref{tab:tab5} presents a comparison of the response distillation losses, where we observe a significant improvement in the SSIM loss, achieving an MAE of 72.69 compared to the MSE loss. This improvement can be due to the fact that the SSIM loss evaluates the local pattern consistency of the output from both the teacher and student branches, whereas the MSE loss only evaluates the element pairwise similarity.



%  Table~\ref{tab:tab5} shows the comparison of response distillation losses. Clearly, SSIM loss has improved significantly, reaching 72.69 in terms of MAE when compared to MSE loss. This is because the SSIM loss is able to assess the local pattern consistency of the output of the teacher and student branches, whereas the MSE loss only assesses element pairwise similarity.

%\begin{table}[!htb] 
%	\caption{Comparison of Response Distillation Losses.}
%	\label{tab:tab5}
%	\centering
%	\begin{tabular}{cc}
%		\hline
%		Loss                  & MAE \\
%		\hline
%		Baseline             &92.45  \\
%		\hline
%		+ MSE Loss             &86.68  \\
%		+ SSIM Loss            &\textbf{72.69}\\
%		\hline
%	\end{tabular}
%\end{table}



%
%\subsection{Comparison with Model Compression Methods}
%
%There are various current approaches for compressing crowd counting models. Here, we compare some representative models~\cite{liu2020efficient} to demonstrate the strength of our method.
%
%As is shown in Table~\ref{tab:tab7}, our method performs best compared with different compression strategies. Specially,  
%The CSRNet parameters are quantized into 8 bits using QAT~\cite{jacob2018quantization}, yielding an MAE of 75.50.  For pruning method, CP~\cite{he2017channel} provides an iterative two-step algorithm to effectively prune each layer of CSRNet which obtains an MAE of 82.05 with 6.89M parameters. AGP~\cite{zhu2017prune} uses a simple gradual pruning approach to compress CSRNet and obtains a MAE of 78.51. We also compare two distillation-based approaches, AT~\cite{zagoruyko2016paying} and AGP~\cite{zhu2017prune}. Clearly, the results show the satisfactory performance of our method which achieves a balance in accuracy and efficiency.
%
%
%\begin{table}
%\caption{Comparison of Compression Methods On ShanghaiTech Part-A.}
%\label{tab:tab7}
%\centering
%\begin{tabular}{cc|c}
%\hline
%\multicolumn{2}{c|}{Method}                                         & MAE \\
%\hline
%Quantization                    &QAT~\cite{jacob2018quantization}   &75.50  \\
%\hline
%\multirow{2}*{Pruning}          &CP~\cite{he2017channel}            &82.05   \\
%~                               &AGP~\cite{zhu2017prune}            &78.51   \\                         
%\hline
%\multirow{3}*{Distillation}     &AT~\cite{zagoruyko2016paying}      &74.65  \\
%~                               &SKT~\cite{liu2020efficient}        &71.55 \\
%~                               &Ours                               &\textbf{70.02} \\
%\hline
%\end{tabular}
%\end{table}




\begin{table*}[ht] 
	\caption{Comparison of our method with other methods on RSOC.  DM denotes the density map.}
	\label{tab:result}
	\centering
	\begin{tabular}{c|c|c|c|c|cc|cc|cc|cc}
		\hline
		\multirow{2}{*}{{Type}} & \multirow{2}{*}{{Method}} &  \multirow{2}{*}{{Time(min))}} & \multirow{2}{*}{{Param(M)}} & \multirow{2}{*}{{FLOPs(G)}}   & \multicolumn{2}{c|}{{Building}} & \multicolumn{2}{c|}{{Small-vehicle}} & \multicolumn{2}{c|}{{Large-vehicle}} & \multicolumn{2}{c}{{Ship}} \\
		\cline{6-13}
		& & & & & {MAE} & {RMSE} & {MAE} & {RMSE} & {MAE} & {RMSE} & {MAE} & {RMSE} \\
		\hline
		\multirow{9}{*}{\makecell{DM-based \\ methods}} & MCNN~\cite{zhang2016single} & 129 & 0.13 & 1.38  &12.13 &17.35  & 488.45 & 1310.44 & 21.69 & 39.41 & 93.76 & 133.18 \\
		& SKT~\cite{liu2020efficient} & 459 & 1.02 & 1.71 & \textbf{7.12} & 11.46  & 309.05 & 1073.72 &42.43 & 68.42 & 106.15 &146.74 \\
		\cline{2-13}
		
		% \hline
		& CSRNet~\cite{li2018csrnet} & 196 & 16.26 & 27.07 & 7.18 & 10.53 & 345.03 & 1039.16 & 35.88 & 48.32 & 71.92 & 109.63  \\
		& Bayesian\cite{ma2019bayesian} & 63 &21.50 & 26.99  & 28.95 & 32.93 & 185.79 & 703.31 & 26.03 & 51.54 & 64.15 & 86.01  \\
		& ASPD\cite{gao2020counting} & 265 &22.70 & 37.96  &7.59 &10.66 &--- & --- &42.87 & 63.49 &216.27 &352.23 \\
		& MAN\cite{lin2022boosting} & 103 &40.41 & 27.13  &29.01 &32.98 &505.10 &1326.12 &62.78 &79.65 &\textbf{47.20} &\textbf{70.24}  \\
		& PET\cite{liu2023point} & 75 &20.91 & 28.42  &10.78 &13.96 &419.74 &1212.18 &36.39 &54.52 & 258.83 &396.39  \\
		& GCFL\cite{shu2023generalized} & 8 &21.51 & 27.01  &7.79 &10.74 & 222.40 & 768.83 & 16.30 &23.31 	&55.32 &77.05  \\
		& PML\cite{yan2023progressive} & 43 &16.26 & 21.65  &8.3 &12.78 &217.17 &652.98&32.15 & 48.2 &139.58 &172.17 \\
		% FRE &21.51 &7.79 & 10.74  & 222.40 & 768.83 &16.30 & 23.31 & 55.32 & 77.05    \\
		\hline
		\multirow{3}{*}{\makecell{Point-based \\ methods}} & P2PNet\cite{song2021rethinking} & 76  &19.20 & 23.29 &7.46 &\textbf{10.34} &174.00 &666.44 & \textbf{11.61} & 16.94 & 62.00 & 76.43 \\
		& P2PNet$^{*}$ & 72 &8.99 & 19.70  &8.25 & 11.12 & \textbf{159.33} & 638.36 & 12.89 & 17.50  &56.63 & 81.78  \\
		& OnKL Net & 85  & 0.76 & 4.00 &7.58 &10.62 &169.86 & \textbf{613.30} & 12.27 &\textbf{16.70} & 81.80 &105.57 \\
		\hline
		
		
	\end{tabular}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[]
	\caption{Comparison of our method with other methods on STAR. DM denotes the density map.}
	\label{tab:result1}
	\centering
	\begin{minipage}{\textwidth}\centering
		\scalebox{0.95}{\begin{tabular}{c|c|c|cc|cc|cc|cc|cc|cc}
		\hline
		 \multirow{2}{*}{{Type}} & \multirow{2}{*}{{Method}} & \multirow{2}{*}{{Param(M)}} & \multicolumn{2}{c|}{{Airplane}} & \multicolumn{2}{c|}{{Wind   mill}} & \multicolumn{2}{c|}{{Lattice   tower}} & \multicolumn{2}{c|}{{Bridge}} & \multicolumn{2}{c|}{{Runway}} & \multicolumn{2}{c}{{Average}} \\
		\cline{4-15}
		& &                           & MAE          &RMSE        &MAE            & RMSE           & MAE              &RMSE             &MAE          & RMSE        & MAE         & RMSE       &MAE          &RMSE         \\
		\hline
		%\cmidrule{2-5}  \parbox{1.2cm}{
		\multirow{4}{*}{\makecell{DM-based \\ methods}} & Bayesian\cite{ma2019bayesian}   & 21.50                     & 45.94         & 66.08        & 18.94          & 25.22          & 25.93            & 36.00            & 2.28         & 5.20        & 1.39         & 1.76        & 18.90        & 26.85        \\
		& MAN\cite{lin2022boosting}   & 40.41                     & 46.82         & 66.68        & 19.73          & 25.88          & 26.54            & 36.43            & 3.02         & 5.96        & 2.06         & 2.31        & 19.63        & 27.45        \\
		& PML\cite{yan2023progressive}    & 16.26                     & 65.66         & 77.98        & 2.68           & 9.26           & 12.15            & 24.00            & 3.63         & 5.38        & 5.46         & 5.94        & 17.92        & 24.51        \\
		& GCFL\cite{shu2023generalized}   & 21.51                     & 14.18         & 23.37        & 2.42           & 3.42           & 17.96            & 28.41            & 2.10         & 5.39        & \textbf{0.82}         & \textbf{1.11}        & 7.50         & 12.34        \\
		\hline 
		% \multirow{3}{*}{\parbox[c]{1.2cm}{DM-based Methods}} &  & & & & & & & & & & & & &\\
		\multirow{3}{*}{\makecell{Point-based \\ methods}} 
		& P2PNet\cite{song2021rethinking}  & 19.20                     & 12.20         & 21.92        & \textbf{0.45}           & \textbf{0.67}           & 12.21            & 21.05            & 1.60         & 2.46        & 1.18         & 1.51        & 5.53         & \textbf{9.52}         \\
		& P2PNet*                 & 8.99                      & \textbf{12.17}         & 21.61        & 0.64           & 1.00           & \textbf{11.56}            & \textbf{20.75}            & 1.65         & 2.83        & 1.35         & 1.75        & \textbf{5.47}         & 9.59         \\
		& OnKL Net                    & 0.76                      & 12.93         & \textbf{18.07}        & 2.00           & 2.56           & 15.36            & 23.53            & \textbf{1.53}         & \textbf{2.39}        & 1.18         & 1.67        & 6.60          & 9.64   \\
		\hline     
	\end{tabular}}
	\end{minipage}
\end{table*}




\subsection{Comparison with SOTA Methods}


Table~\ref{tab:result} shows the comparison results with some state-of-the-art (SOTA) methods. {As shown, our method achieves highly competitive results on RSOC with only 0.76M parameters and 4.0G FLOPs.} P2PNet serves as a strong baseline. Our method performs comparably or even better than P2PNet in the Building, Small-vehicle, and Large-vehicle datasets. However, in the Ship dataset, our results are less favorable, likely due to the training samples containing only 88 images, which makes the model prone to overfitting with such limited data. Similarly, {While our method yields results comparable to the baseline, P2PNet, it uses just 4\% of the parameters and 17\% of the FLOPs.} Moreover, we compare our method against density map-based methods such as MAN~\cite{lin2022boosting}, GCFL~\cite{shu2023generalized}, and PML~\cite{yan2023progressive}. {We notice that SKT outperforms our method in terms of MAE on Building. We speculate that this is due to two reasons: Firstly, SKT has a small number of parameters but still more than those of our method, giving it a stronger capability to fit the current dataset. Secondly, SKT utilizes a fixed Gaussian distribution to characterize each target, which can help it better distinguish between foreground and background areas. This could be important in the noisy remote sensing scenarios for regressing these buildings. In contrast, our method needs to generate a series of proposals and predict the confidence of these proposals, undoubtedly increasing the difficulty for the model to identify these regions of interest.} The results show that these methods achieve competitive outcomes on one or two subdatasets of RSOC but perform poorly on others, indicating limited adaptability. In contrast, our method consistently delivers highly competitive results across multiple datasets. Additionally, our method focuses on predicting point positions as the network's regression objective, which is more intuitive but also more challenging than density map-based methods. 

{While our method achieves satisfactory performance with reduced model inference overhead, it does require more training time compared to most other methods. This is primarily due to the different training objectives, i.e., the point-based objective vs. the density map-based objective. However, when compared to our baselines, P2PNet and P2PNet*, the increase in training time is minimal. Notably, in contrast to the two-stage distillation method SKT~\cite{liu2020efficient}, our method requires less than one-fifth of its training time. This demonstrates that our method incurs only a marginal increase in training cost while delivering an efficient inference model.}


Fig.~\ref{fig:vis} qualitatively presents the comparative results of our method with SKT and P2PNet under different densities. As shown, our method achieves excellent performance across various scenarios, particularly in medium- to low-density scenes. We also observe that in high-density scenes, there is a slight deviation between our predicted outcomes and the actual results. This deviation is due to the inherent challenges of robust counting and localization in scenes with extremely high target density. Furthermore, in contrast to SKT, which is based on density maps, our method is capable of predicting the location information of targets, thereby providing more robust support for scene understanding.



{In addition, we evaluate our method in the STAR dataset. Note that it is used for scene graph generation in large-scale satellite imagery, with object localization annotated via bounding boxes. Our method focuses on  object counting, while generating point-based locations for object localization. Note that STAR evaluates models through an online test dataset; however, due to inconsistencies with its label format, we are unable to assess our method’s performance using the online version. Therefore, we use the validation set to evaluate our method. The results, shown in Table~\ref{tab:result1}, demonstrate that our method achieves highly competitive performance with significantly fewer parameters. Specifically, it outperforms recent SOTA density map-based methods, such as PML and GCFL. Moreover, our method provides the location coordinates of objects, which facilitates a better analysis and understanding of remote sensing objects. When compared with P2PNet and P2PNet*, although our method yields slightly weaker results on average, it requires only about 4\% and 8\% of their parameters, respectively. In summary, these results further validate the effectiveness of our method.}


\subsection{Failure Cases}


{To better illustrate our method, we present three examples of failed predictions of our network in Fig.~\ref{fig:failure}. In the first column, we show the predicted results for large vehicles. The predicted value is 61, which is approximately half of the true count. This discrepancy can be attributed to the high-altitude angle from which the image was taken, reducing the apparent size of the target. Since object size is a key factor in recognition, it significantly complicates the accurate identification of vehicles. Additionally, certain background features, such as white rooftop decorations, closely resemble the targets, leading to potential misidentification by the model. The second column presents localization results for small vehicles. Due to the considerable distance between the camera and the targets, identifying these vehicles becomes more challenging. As shown, the predicted count is notably lower than the actual number. In the third scenario, we over-predict the number of small vehicles, highlighting the ongoing challenge of mitigating background interference in remote sensing tasks.}


\begin{figure}[ht] 
	\centering
	\includegraphics[scale = 0.3]{./failure.jpg}
	\caption{Examples of failed predictions of our network.}
	\label{fig:failure}
\end{figure} 



% Following your advice, we further test our method on STAR. It is worth noting that this dataset is used for scene graph generation in large-size satellite imagery, where object localization is annotated in the form of bounding boxes. Our work emphasizes the statistics of the number of objects, simultaneously generating point locations as information for object localization. This dataset evaluates the models through an online test dataset, but due to inconsistency with its label format, we cannot assess the performance of our method. Therefore, we use the validation set for evaluating our method's performance. The results are shown in Table 2. These results show that our method achieves highly competitive results with significantly fewer parameters. Specifically, our method surpasses recent state-of-the-art density map methods such as PML and GCFL. Moreover, our method outputs the location coordinates of objects, aiding in better analysis and understanding of these remote sensing objects. Compared with P2PNet and P2PNet*, although our method generally yields slightly weaker results than these two methods, it only requires about 8\% and 3\% of their parameter sizes, respectively. In summary, these results further demonstrate the effectiveness of our method.

%To demonstrate the effectiveness of our method, we further compare our method to some state-of-the-art (SOTA) methods. 
%
%The final results are summarized in Table~\ref{tab:result}. As can be seen, our model performs admirably across all datasets with a small number of parameters. There are two findings on these datasets: 
%\begin{itemize}
%\item Compared to training alone, the use of knowledge distillation in the student branch has resulted in significant improvements. The magnitude of this improvement becomes more pronounced with larger data collections.
%
%\item Interestingly, the student branch outperforms the teacher branch on two large-scale datasets, namely, RSOC\_building and RSOC\_large Vehicle, while there is still a certain gap between the student branch and teacher branch on RSOC\_ship. This indicates that the network can be easily overfitted on a small dataset during the distillation step. Furthermore, we speculate that joint training indeed helps student networks better capture effective features, allowing it to outperform the teacher network.







% Interestingly, the student network outperforms the teacher network on three large-scale datasets, that is,  UCF-QNRF, JHU-Crowd++ and NWPU-Crowd, while there is still a certain gap between student network and teacher network on ShanghaiTech Part A. This demonstrates that the network is easily overfitted on a small dataset during the distillation step. Meanwhile, we peculate that the joint training indeed helps student networks better capture effective features so that it can outperform the teacher network.

%\end{itemize}



%Our findings suggest that our online training network is capable of effectively transferring knowledge. While we have achieved excellent performance, it is important to note that there is still a gap between our results and the latest benchmarks. For instance, the BL model outperforms ours by 12\% on UCF-QNRF. The primary reason for this is due to the limited performance of our backbone. Therefore, in future work, we aim to enhance its applicability and further improve its performance.

%These findings suggest that our online training network can realize the effective transfer of knowledge. Although we have achieved excellent performance, it also should be noted that there is a certain gap between our performance and the latest results. For example, the BL model obtains a performance gain of 12\% over ours on the UCF-QNRF dataset. The primary reason for this is because the network we developed has limited performance. Thus, we will enhance its applicability in the next work. 




\section{Conclusion}


In this study, we propose an efficient online distillation network for remote sensing object counting. Unlike the traditional two-stage distillation technique, we introduce an end-to-end online knowledge distillation framework. The framework comprises three components: a shared shallow module, a teacher branch, and a student branch, which integrates two traditionally distinct networks into a single trainable network. Then, we propose a new method for distilling feature relations, termed relation-in-relation distillation, which enables the student branch to better understand the evolution of inter-layer features. This is achieved by constructing an inter-layer relationship matrix that captures the relationship among inter-layer features.  Finally, our method achieves comparable results to several state-of-the-art models, with significantly fewer parameters, as demonstrated through extensive experiments on two challenging datasets. 

{While our method achieves competitive performance with fewer parameters, there’s still plenty of room for improvement. On one hand, since the student branch in our framework uses an empirical architecture, exploring automated architecture search strategies could help create a more lightweight and robust student network. On the other hand, exploring more efficient knowledge transfer strategies could further enhance the performance of the student branch.}


%Extensive experiments on four challenging datasets demonstrate the validity of our approach, which achieves comparable results to several cutting-edge models with significantly fewer parameters.


%In this work, we put forward an efficient online distillation network for crowd counting. In contrast to the classical two-stage distillation technique, we present an end-to-end online knowledge distillation framework. The framework consists of three modules, i.e., shared shallow module, teacher branch and student branch, to integrate two traditional different networks into a single trainable network. Moreover, we build three distinct knowledge techniques for this framework to assist students in better learning the various knowledge of teacher branch. To be more specific,  feature internal distillation aims at transferring the knowledge from the same level of teachers and students; feature relation distillation is designed to capture the relationship of different hierarchical features by exploring new relationship between them; response distillation provide the regularization of two branches' output. Finally, extensive experiments on four challenging datasets show the validity of our method, and while needing much fewer parameters, our network achieves comparable results to several cutting-edge models.


% \iffalse …. \fi
\iffalse
\section*{Acknowledgment}
 This work is supported by  xxxx.
\fi


% references section
\bibliographystyle{IEEEbib}
\bibliography{bib_tip}



% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% that's all folks
\end{document}


