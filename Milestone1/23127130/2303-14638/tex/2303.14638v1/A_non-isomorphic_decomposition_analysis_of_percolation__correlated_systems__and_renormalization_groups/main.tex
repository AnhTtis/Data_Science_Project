%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    INSTITUTE OF PHYSICS PUBLISHING                                   %
%                                                                      %
%   `Preparing an article for publication in an Institute of Physics   %
%    Publishing journal using LaTeX'                                   %
%                                                                      %
%    LaTeX source code `ioplau2e.tex' used to generate `author         %
%    guidelines', the documentation explaining and demonstrating use   %
%    of the Institute of Physics Publishing LaTeX preprint files       %
%    `iopart.cls, iopart12.clo and iopart10.clo'.                      %
%                                                                      %
%    `ioplau2e.tex' itself uses LaTeX with `iopart.cls'                %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% First we have a character check
%
% ! exclamation mark    " double quote  
% # hash                ` opening quote (grave)
% & ampersand           ' closing quote (acute)
% $ dollar              % percent       
% ( open parenthesis    ) close paren.  
% - hyphen              = equals sign
% | vertical bar        ~ tilde         
% @ at sign             _ underscore
% { open curly brace    } close curly   
% [ open square         ] close square bracket
% + plus sign           ; semi-colon    
% * asterisk            : colon
% < open angle bracket  > close angle   
% , comma               . full stop
% ? question mark       / forward slash 
% \ backslash           ^ circumflex
%
% ABCDEFGHIJKLMNOPQRSTUVWXYZ 
% abcdefghijklmnopqrstuvwxyz 
% 1234567890
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\documentclass[12pt]{iopart}
\usepackage{iopams}  
\usepackage{amstext}
\usepackage{lipsum}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{floatrow}
\usepackage{array}
\renewcommand\arraystretch{2}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{hyperref}
\expandafter\let\csname equation*\endcsname\relax
\expandafter\let\csname endequation*\endcsname\relax
\usepackage{amsmath}

\definecolor{codegreen}{rgb}{0,0.6,0.4}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}
\definecolor{keywordcolor}{rgb}{0,0,0.8}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{keywordcolor},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
%Uncomment next line if AMS fonts required

\usepackage{etoolbox}

\makeatletter
\newrobustcmd{\fixappendix}{%
  \patchcmd{\l@section}{1.5em}{7em}{}{}%
  \patchcmd{\l@subsection}{2.3em}{7em}{}{}%
}

\begin{document}

\title{A non-isomorphic decomposition analysis of percolation, correlated systems, and renormalization groups}

\author{Yang Tian}
\address{Department of Psychology \& Tsinghua Laboratory of Brain and Intelligence, Tsinghua University, Beijing, 100084, China.}
\address{Laboratory of Advanced Computing and Storage, Central Research Institute, 2012 Laboratories, Huawei Technologies Co. Ltd., Beijing, 100084, China.}
\ead{tiany20@mails.tsinghua.edu.cn}
\author{Pei Sun}
\address{Department of Psychology \& Tsinghua Laboratory of Brain and Intelligence, Tsinghua University, Beijing, 100084, China.}
\ead{peisun@tsinghua.edu.cn}
\vspace{10pt}
\begin{indented}
\item[]February 2023
\end{indented}

\begin{abstract}
Any system with internal correlations (e.g., interactions), such as spin glasses, random media, and brains, can be studied as a network where nodes are elements and edges denote correlated interactions. As internal correlations increase, correlated behaviours originate and propagate among elements to create percolation phenomena. In this work, we study percolation in correlated systems by proposing a framework referred to as the non-isomorphic decomposition analysis. This framework analyzes the evolution of the space of non-isomorphic percolation configuration classes governed by internal correlations, where a metric called as the automorphism entropy is proposed to measure the reduction of information (i.e., freedom degrees) in the space. A special case of our theory under the mean-field approximation is elaborated, where all concepts can be explicitly calculated based on group theory and percolation theory. Our framework has been applied to studying absorbing phase transition, spreading process, and synchronization, whose results reveal the shared properties of percolation and dynamics in different correlated systems. First, the percolation process driven by internal correlations is intrinsically an information loss process in the probability space of non-isomorphic percolation configuration classes. The metric of information loss degree, automorphism entropy, serves as a sensitive indicator of percolation transition and related processes in diverse systems. Second, the information loss intensity depends on the strength of internal correlations. Sufficiently strong correlations ensure slight information losses and make specific freedom degrees omissible, which supports the implementation of renormalization groups. Sufficiently weak correlations, on the contrary, imply significant information losses and make renormalization groups less effective for the correlated system. 
\end{abstract}

%
% Uncomment for keywords
%\vspace{2pc}
\noindent{\it Keywords}: percolation, correlated systems, group theory
%
% Uncomment for Submitted to journal title message
%\submitto{\JPA}
%
% Uncomment if a separate title page is required
%\maketitle
% 
% For two-column output uncomment the next line and choose [10pt] rather than [12pt] in the \documentclass declaration
%\ioptwocol
%
\tableofcontents

\section{Introduction}
\subsection{Correlation systems and percolation}\label{Sec1-1}

Correlated systems (or referred to as interacting systems), such as those described by the Wilson-Cowan equation \cite{wilson1973mathematical,amari1977dynamics,bressloff2010metastable,bressloff2010stochastic}, spin glasses \cite{binder1986spin}, the random cluster model \cite{grimmett2006random}, the Kuramoto model \cite{acebron2005kuramoto}, and the cellular Potts model \cite{scianna2012multiscale,chen2007parallel,chiang2016glass}, are important research objects in physics. By its nature, a correlated system can be represented by the dynamics on a network $G\left(V,E\right)$, where nodes in $V$ are system elements and edges in $E$ denote correlated interactions. The existence of edge between elements $i$ and $j$ is non-trivially determined by $C_{ij}$, the $\left(i,j\right)$-th entity in a specific correlation matrix $C$. The definition of element correlation can be arbitrary, which is not limited to the statistical correlation.            

If we seek an abstract formalization that characterizes different correlated systems as networks in a unified manner, percolation on network \cite{li2021percolation,duminil2018sixty} may be a natural choice. The key idea is to generate an all-to-all network of elements (i.e., a complete graph) during initialization. Then each edge is occupied if and only if the associated elements synchronously behave (i.e., these two elements are correlated). All unoccupied edges are removed from the initial network, after which a filtered network is obtained to describe the propagation of correlated behaviours. In a probabilistic manner, we can consider an occupation probability vector
\begin{align}
    \rho=f\left(C\right)=\left(\rho_{e}:\;e=\left(i,j\right)\in E\right)\in\left[0,1\right]^{E},\label{EQ1}
\end{align}
where $f\left(\cdot\right)$ is a non-trivial element-wise function that maps correlation matrix $C$ to occupation probability vector $\rho$. A percolation configuration 
\begin{align}
\mathbf{\chi}=\left(\chi_{e}:\;e=\left(i,j\right)\in E\right)\in\{0,1\}^{E}\label{EQ2}
\end{align}
lives in a probability space $\mathbb{P}_{\rho}=\left(\{0,1\}^{ E},\sigma_{\chi} ,P_{\rho}\right)$, where $1$ denotes occupation (i.e., the edge is kept) and $0$ denotes non-occupation (i.e., the edge is deleted). Notion $\sigma_{\chi}$ denotes a $\sigma$-algebra and $P_{\rho}$ is a probability measure characterized by vector $\rho$. The propagation of correlated behaviours within the system is fully characterized by $\mathbb{P}_{\rho}$, based on which we can apply percolation theory to study the emergence of a giant cluster of correlated elements or the evolution of the average size of clusters \cite{li2021percolation,georgakopoulos2018analyticity,duminil2018sixty}. 

\subsection{Our framework and contribution}\label{Sec1-2}
In this work, we focus on the associate dynamics in the statistical ensemble of percolation configurations during the percolation process of correlated behaviours. We suggest a new perspective to analyze percolation and correlated systems in terms of the variation of information (i.e., freedom degrees) in percolation configuration ensemble.

Mathematically, we can explore this perspective by comparing $\mathbb{P}_{\rho}$ with $\mathbb{P}_{\infty}=\left(\{0,1\}^{ E},\sigma_{\chi},P_{\infty}\right)$, a space that includes the percolation configurations generated by all possible relations among system elements (i.e., element relation can be arbitrary rather than be limited to $\rho$) and reflects the neutral properties of the target system when there is no constraint on element relations. We pursue to develop a possible approach to realize this comparison and suggest how it inspires new techniques in studying percolation and correlated systems. 

In Sec. \ref{Sec2}, we develop a framework, referred to as the non-isomorphic decomposition analysis, to transform our concerned question into the space of non-isomorphic percolation configuration classes. The change degree of information in $\mathbb{P}_{\rho}$ compared with that in $\mathbb{P}_{\infty}$ can be reflected by the evolution of the probability measure of non-isomorphic percolation configuration classes, which is more practical to analyze. In this paper, we primarily introduce a special case of the non-isomorphic decomposition analysis under the mean-field approximation, yet more general versions of our framework can be freely developed without mean-field approximation.

In Sec. \ref{Sec3}, we develop a metric referred to as automorphism entropy to measure the reduction of information (i.e., the loss of freedom degrees) in the space of non-isomorphic percolation configuration classes as a function of $\rho$. Automorphism entropy can be calculated based on the orbit-stabilizer theorem and its lemmas \cite{hamermesh2005group}. A smaller automorphism entropy corresponds to a lower degree of information loss when element relations are constrained by $\rho$. 

In Sec. \ref{Sec4}, we present a calculation instance of the automorphism entropy in an abstract case where the propagation of correlated behaviours among the elements of a system is characterized by Bernoulli percolation on a network \cite{georgakopoulos2018analyticity}. We propose the infinite cluster automorphism entropy to measure the information loss degree associated with the infinite cluster of correlated elements. The quantified information loss is observed to serve as an indicator of percolation transition. This instance suggests the quantitative relation between information losses in the space of non-isomorphic percolation configuration classes and phase transitions in correlated systems.

In Sec. \ref{Sec5}, we propose the automorphism entropy spectrum to measure the information loss degree associated with the finite cluster of correlated elements. Similar to the infinite cluster automorphism entropy, the evolution of the automorphism entropy spectrum and its moments can be used to predict or analyze phase transitions controlled by internal correlations.

In Sec. \ref{Sec6}, we apply our framework to analyzing the dynamics in several representative correlated systems, including the absorbing phase transition in branching models \cite{hinrichsen2000non,lubeck2004universal,henkel2008non}, spreading processes in epidemic models \cite{moreno2002epidemic,newman2002spread}, and
synchronization in oscillator models \cite{rodrigues2016kuramoto,acebron2005kuramoto}. The phase transition phenomena in these correlated systems, irrespective of their distinct details, can all be reflected and analyzed in terms of information loss in percolation configuration ensemble.

In Sec. \ref{Sec7}, we elaborate the physics idea underlying the non-isomorphic decomposition analysis. We suggest how our concerned question relates to renormalization group theories \cite{zinn2007phase,wilson1983renormalization,goldenfeld2018lectures}, based on which, an abstract coarse graining approach is proposed under the guidance of the non-isomorphic decomposition analysis. This abstract framework has concrete manifestations in various existing renormalization groups, such as the Laplacian renormalization group \cite{villegas2023laplacian}, the moment space phenomenological renormalization group \cite{bradde2017pca,lahoche2022generalized}, and the real space phenomenological renormalization group \cite{meshulam2018coarse,meshulam2019coarse}. The non-isomorphic decomposition analysis enables us to study when and how a correlated system can be manipulated by renormalization groups in an effective way.

In Sec. \ref{Sec8}, we present a summary of our work and suggests potential directions to generalize our framework. 

Although our work is primarily presented in an abstract form, we endow equal importance to the computational practicality of our framework. Therefore, we elaborate lots of details of computer programming in the appendixes. Moreover, because numerous concepts across different branches of statistical physics are used in our work, it is less feasible to assign a unique notion for every concept. As a compromise, some
notions are slightly abused if they are contextually dependent and do not cause confusions. 

\section{Non-isomorphic decomposition}\label{Sec2}
To realize our target comparison, we analyze the spaces of non-isomorphic percolation configuration classes associated with $\mathbb{P}_{\infty}$ and $\mathbb{P}_{\rho}$ separately. Our analysis framework is referred to as the non-isomorphic decomposition analysis because it begins with decomposing the percolation configuration ensemble into numerous non-isomorphic parts. We elaborate the basic framework of non-isomorphic decomposition analysis by formalizing a special case of it under the mean-filed approximation.

\subsection{Mean-filed approximation}
We first narrow down our target question to homogeneous systems, where a mean-field approximation can be applied on Eq. (\ref{EQ1}). Specifically, all elements in the correlated system are assumed to be homogeneous such that the occupation
probability vector in Eq. (\ref{EQ1}) can be reasonably replaced by a mean occupation
probability
\begin{align}
    \rho=\frac{1}{\vert E\vert}\sum_{\left(i,j\right)\in E}g\left(C_{ij}\right),  \label{EQ3}
\end{align}
where $g\left(\cdot\right)$ is an arbitrary mapping that normalizes all entities in $C$ into $\left[0,1\right]$ (e.g., the min-max normalization). Based on Eq. (\ref{EQ3}), the percolation process underlying the propagation of correlated behaviours reduces to the Bernoulli percolation on networks \cite{georgakopoulos2018analyticity}. Please note that the mean-filed approximation is not a necessary condition of our theory. It only corresponds to a special case of our theory that is easy to interpret (see a calculation instance of our theory without the mean-filed approximation in Sec. \ref{Sec6}).

Given a mean occupation
probability $\rho$ in Eq. (\ref{EQ3}), we can define the corresponding $\mathbb{P}_{\rho}$ and analyze $\rho_{n}$, the associated probability for a random node to belong to the
giant cluster (i.e., an infinite connected component with endless branching process). Under the mean-filed approximation (i.e., the probability is invariant across different nodes), the giant cluster size is
\begin{align}
N=\lfloor\rho_{n}\vert V\vert\rfloor,\label{EQ4}
\end{align} 
where $\lfloor\cdot\rfloor$ is the floor function. Please note that although the giant cluster is theoretically infinite, its embodiment within a finite correlated system must have a finite size (i.e., $N$) as well. Meanwhile, we can also analyze the size distribution of finite clusters (i.e., an connected component with finite branching process) following the approach proposed by Refs. \cite{newman2007component,kryven2017general,kryven2017finite,moore2000exact,newman2001random} (see Sec. \ref{Sec4} for detailed derivations). 

The difference between $\mathbb{P}_{\infty}$ and $\mathbb{P}_{\rho}$ manifests as the sets of nodes included by specific infinite or finite clusters. In space $\mathbb{P}_{\rho}$, the nodes included by an arbitrary cluster are required to form a connected component following a certain wiring diagram. In space $\mathbb{P}_{\infty}$, there is no restraint on the wiring diagram of any set of nodes. This difference implies the distinct freedom degrees of these two spaces and may serve as a window to explore our target question.

\subsection{Non-isomorphic percolation configuration class}

Assuming that $V_{c}\subset V$ is the node set of an arbitrary cluster (i.e., infinite or finite) of size $M$, we can see the arbitrariness of $V_{c}$, i.e., we only require $\vert V_{c}\vert=M$ and have no constraint on which set of $M$ nodes should be included (nodes are homogeneous to each other). Therefore, our analysis of the difference mentioned above always involves plentiful duplicated cases. To prune these redundancies, we can transform our analysis into the space of non-isomorphic percolation configuration classes.

For convenience, we consider a complete graph $G_{c}\left(V_{c},E_{c}\right)$ defined on $M$ nodes. We mark $E_{\chi}\subset E_{c}$ as the edge set corresponding to a percolation configuration $\chi\in\{0,1\}^{E_{c}}$
\begin{align}
    E_{\chi}=\{e\vert e=\left(i,j\right)\in E_{c},\; \chi_{e}=1\}. \label{EQ5}
\end{align}
We consider a symmetric group $S_{\vert E_{c}\vert}$ that acts on the filtered edge set $E_{\chi}$, i.e., a permutation acts on any edge $\left(i,j\right)\in E_{\chi}$ and maps it to $\left(\sigma\left(i\right),\sigma\left(j\right)\right)$. A permutation $\sigma:E_{\chi}\rightarrow E^{\sigma}_{\chi}$ is treated as auto-isomorphism if
\begin{align}
    \left(i,j\right)\in E_{\chi}\Leftrightarrow \left(\sigma\left(i\right),\sigma\left(j\right)\right)\in E^{\sigma}_{\chi}. \label{EQ6}
\end{align}
Then, we define the automorphism group $\mathsf{Aut}\left(\chi\right)$ as the set of all automorphisms of $E_{\chi}$ under mapping composition. Based on it, we can define an equivalence relation $\sim$ between percolation configurations 
\begin{align}
    \chi\sim\chi^{\prime}\Leftrightarrow \left(E_{\chi^{\prime}}=E^{\sigma}_{\chi},\;\sigma\in\mathsf{Aut}\left(\chi\right)\right).  \label{EQ7}
\end{align}
The space of non-isomorphic percolation configuration classes is formally defined as 
\begin{align}
    \Gamma\left(M\right)=\{0,1\}^{E_{c}}/\sim,  \label{EQ8}
\end{align}
which corresponds to a partition of set $\{0,1\}^{E_{c}}$. Each element in $\gamma$ is a set of percolation configurations that are isomorphic. Each pair of elements in $\Gamma$ are not equivalent to each other in terms of $\sim$ (i.e., they are not isomorphic).

Now, let us consider the probability space defined on $\Gamma$. We begin with the case where the probability of each percolation configuration is defined by $\mathbb{P}_{\infty}$. Although $\mathbb{P}_{\infty}$ can not be explicitly defined because it is impossible to enumerate all possible element relations, a simple idea can be applied to make the analysis possible. If one percolation configuration class $\gamma$ is less to occur given a relation among elements, there will be another relation that makes $\gamma$ occur with a higher probability. Therefore, we can reasonably consider a symmetry on $\Gamma$, i.e., a postulate of \emph{equal a priori probabilities}
\begin{align}
    P_{\infty}\left(\gamma\right)= \frac{1}{\vert \Gamma\left(M\right)\vert}.  \label{EQ9}
\end{align}
Please note that Eq. (\ref{EQ9}), similar to the postulate of \emph{equal a priori probabilities} for equilibrium system in statistical mechanics, has no strict proof yet. It is used as a basic assumption in our theory. 

Then we turn to the case where the probability of percolation configuration is defined by $\mathbb{P}_{\rho}$. For convenience, we define $L\left(\gamma\right)\leq M$ as the largest connected component size of the network associated with each percolation configuration $\chi$ in class $\gamma$ (note that the largest connected component size is invariant across all configurations in $\gamma$ because these configurations are isomorphic). The probability of percolation configuration class $\gamma$ is defined as
\begin{align}
    P_{\rho}\left(\gamma\right)= \frac{1}{Z_{\gamma}}\Theta\left(L\left(\gamma\right)-M\right),\label{EQ10}
\end{align}
where $\Theta\left(\cdot\right)$ is the Heaviside step function and $Z_{\gamma}$ is a partition function
\begin{align}
    Z_{\gamma}= \sum_{\gamma\in\Gamma\left(M\right)}\Theta\left(L\left(\gamma\right)-N\right).\label{EQ11}
\end{align}
 Mathematically, term $\frac{1}{Z_{\gamma}}$ can be understood as a re-normalized version of $P_{\infty}\left(\gamma\right)$, where all percolation configurations whose largest connected component sizes are smaller than $M$ have been excluded (i.e., $V_{c}$ forms a connected component of size $N$ so it always corresponds to the percolation configurations whose largest connected component sizes are $M$). 
 
 As we suggest below, the above definitions enable us to implement our target comparison in a practical way.

\section{Automorphism entropy}\label{Sec3}
Given the basic framework in Sec. \ref{Sec2}, we can compare between $\mathbb{P}_{\infty}$ and $\mathbb{P}_{\rho}$ by proposing a possible metric of the variation degree of information in $\mathbb{P}_{\rho}$ controlled by $\rho$. The developed metric, as a fundamental concept of our non-isomorphic decomposition analysis, is referred to as the automorphism entropy.

\subsection{General definition of automorphism entropy}

Our metric arises from a natural idea. Let us measure the variability of $\Gamma$ in terms of the Shannon entropy $H$
\begin{align}
H_{\infty}\left(M\right)&=\sum_{\gamma\in\Gamma\left(N\right)}P_{\infty}\left(\gamma\right)\log\frac{1}{P_{\infty}\left(\gamma\right)},\label{EQ12}\\
H_{\rho}\left(M\right)&=\sum_{\gamma\in\Gamma\left(N\right)}P_{\rho}\left(\gamma\right)\log\frac{1}{P_{\rho}\left(\gamma\right)}.\label{EQ13}
\end{align}
It is trivial to know
\begin{align}
H_{\rho}\left(M\right)\leq H_{\infty}\left(M\right)\label{EQ14}
\end{align}
because numerous freedom degrees (i.e., the percolation configurations whose largest connected component sizes are smaller than $M$) have been excluded while defining $P_{\rho}\left(\cdot\right)$. The variability of $\Gamma\left(M\right)$ is larger given the symmetric structure in $\mathbb{P}_{\infty}$ while it is smaller given the limited freedom degrees in $\mathbb{P}_{\rho}$. 

A natural metric of the latent information loss can be defined as
\begin{align}
A\left(M\right)=1-\frac{H_{\rho}\left(M\right)}{H_{\infty}\left(M\right)}\in\left[0,1\right], \label{EQ15}
\end{align}
which is referred to as the automorphism entropy since it is defined in the space of non-isomorphic percolation configuration classes. A smaller automorphism entropy suggests a smaller degree of information loss. 

\subsection{Precise formalization of automorphism entropy}
Below, we propose a calculation approach of Eq. (\ref{EQ15}) based on group theory. Key steps of this approach is summarized in Fig. \ref{G1}.

\begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{G1.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G1} Conceptual illustrations of the non-isomorphic decomposition analysis. (a) summarizes the key steps of calculating $\vert \Gamma\vert$ and $\vert \Gamma^{*}\vert$, based on which, other concepts of the non-isomorphic decomposition analysis can be readily derived. (b) illustrates the instance of spaces $\Gamma$ and $\Gamma^{*}$ on a correlated system with a gaint connected component of size $4$ (i.e., $M=4$).} 
 \end{figure}

We first deal with the calculation of Eq. (\ref{EQ9}), which essentially requires us to measure $\vert \Gamma\left(M\right)\vert$, the total number of non-isomorphic percolation configuration classes that can emerge from $M$ nodes. Mathematically, we can calculate $\vert \Gamma\left(M\right)\vert$ following the P{\'o}lya enumeration theorem \cite{harary2014graphical}, a generalization of the lemma of the orbit-stabilizer theorem \cite{hamermesh2005group}
\begin{align}
\vert \Gamma\left(M\right)\vert=\frac{1}{\vert S_{M}\vert}\sum_{\omega\in S_{N}}2^{c\left[\sigma\left(\omega\right)\right]}, \label{EQ16}
\end{align}
where $2=\vert\{0,1\}\vert$ measures the number of possible occupation states and $c\left(\cdot\right)$ measures the number of cycles in the permutation. Notion $\sigma\left(\omega\right)\in S_{\vert E_{c}\vert}$ denotes the corresponding permutation on edges given $\omega\in S_{M}$, the permutation on nodes. In general, there are two ways to calculate Eq. (\ref{EQ16}). The first way depends the precise derivation of cycle index, a multivariate generating function, of group $S_{\vert E_{c}\vert}$ and may require more manual derivations \cite{harary2014graphical,bona2015handbook}. The second way, shown in \ref{ASec-1}, utilizes the properties of group orbits and is easier to follow. In our work, the second approach is applied, which calculates $\vert \Gamma\left(M\right)\vert$ following Eqs. (\ref{AEQ3}-\ref{AEQ4}) and solves Eq. (\ref{EQ9}) as
\begin{align}
P_{\infty}\left(\gamma\right)=\frac{\sum_{O\left(\omega\right)\in \mathbf{O}_{M}} \vert O\left(\omega\right)\vert}{\sum_{O\left(\omega\right)\in \mathbf{O}_{M}} \vert O\left(\omega\right)\vert 2^{c\left[\sigma\left(\omega\right)\right]}}, \label{EQ17}
\end{align}
where $O\left(\omega\right)$ denotes the conjugate class of $\omega$ and $\mathbf{O}_{M}$ is the set of all conjugate classes of group $S_{M}$. Based on Eq. (\ref{EQ17}), entropy $H_{\infty}\left(M\right)$ in Eq. (\ref{EQ12}) can be derived.

Then we turn to calculating Eq. (\ref{EQ10}), which requires us to count the size of $\Gamma^{*}\left(M\right)\subset \Gamma\left(M\right)$, the set of all percolation configuration class associated with connected graphs on $M$ nodes (i.e., all nodes belong to the same connected component). Please see Fig. \ref{G1} for the illustrations of $ \Gamma\left(M\right)$ and $ \Gamma^{*}\left(M\right)$ in a case with $M=4$. Although the counting task of $\vert\Gamma^{*}\left(M\right)\vert$ is hard to be implemented with the P{\'o}lya enumeration theorem \cite{harary2014graphical}, previous studies have suggested that the sequence of $\{\vert\Gamma^{*}\left(1\right)\vert,\vert\Gamma^{*}\left(2\right)\vert,\vert\Gamma^{*}\left(3\right)\vert,\ldots\}$ is the inverse Euler transform of the sequence of $\{\vert\Gamma\left(1\right)\vert,\vert\Gamma\left(2\right)\vert,\vert\Gamma\left(3\right)\vert,\ldots\}$ \cite{sloane2003line,harary2014graphical}. For convenience, we introduce a new sequence $\{\Lambda\left(1\right),\Lambda\left(2\right),\Lambda\left(3\right),\ldots\}$ where $\Lambda\left(1\right)=\vert\Gamma\left(1\right)\vert$. Then we can have the following relations
\begin{align}
\Lambda\left(M\right)&=N\vert\Gamma\left(M\right)\vert-\sum_{i=1}^{M-1}\Lambda\left(i\right)\vert\Gamma\left(M-i\right)\vert, \label{EQ18}\\
\vert\Gamma^{*}\left(M\right)\vert&=\frac{1}{M}\sum_{i | M}\phi\left(\frac{M}{i}\right)\Lambda\left(i\right), \label{EQ19}
\end{align}
where $\phi\left(\cdot\right)$ denotes the M\"obius function and notion $i | M$ represents that $i$ divides $M$. As a special case of the empty graph, i.e., $M=0$, we define $\vert\Gamma\left(0\right)\vert=1$ and $\vert\Gamma^{*}\left(0\right)\vert=1$ because the empty graph can have any property \cite{harary2006null}. Based on Eqs. (\ref{EQ18}-\ref{EQ19}), we can transform Eqs. (\ref{EQ10}-\ref{EQ11}) as
\begin{align}
    P_{\rho}\left(\gamma\right)=\frac{M}{\sum_{i | M}\phi\left(\frac{M}{i}\right)\Lambda\left(i\right)},\label{EQ20}
\end{align}
which enables us to calculate entropy $H_{\rho}\left(M\right)$ in Eq. (\ref{EQ13}). 

\begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{G2.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G2} Calculation of the automorphism entropy as a function of $M$. (a) shows fraction $\frac{\vert\Gamma^{*}\vert}{\vert\Gamma\vert}$ as a function of $M$ in a log-log plot. The inserted sub-plot shows the sequences of $\vert\Gamma\vert$, $\vert\Gamma^{*}\vert$, and $\vert\Lambda\vert$ for reference. (b) illustrates the automorphism entropy across different $M$ in a log-log plot. The actual values of entropies $H_{\infty}$ and $H_{\rho}$ are shown in the inserted sub-plot.} 
 \end{figure}

Based on the above derivations, we can precisely define the automorphism entropy as a function of mean occupation probability $\rho$. Specifically, given a correlated system, we can measure $\rho$ to derive $M$, based on which we obtain $P_{\infty}\left(\cdot\right)$ and $P_{\rho}\left(\cdot\right)$ to calculate the automorphism entropy $A\left(\cdot\right)$. In our research, we primarily define $A\left(0\right)=A\left(1\right)=1$ because they share an ill-posed expression of $1-\frac{0}{0}$ following Eq. (\ref{EQ15}). Other possible definitions (e.g., $A\left(0\right)=A\left(1\right)=0$) are also acceptable because they do not affect our analysis. In \ref{ASec-2}, we offer a step-by-step explanation of the code for computing the automorphism entropy.

In Fig. \ref{G2}, we present the calculation results of the automorphism entropy with $1\leq M\leq 50$. We first show the value of $\frac{\vert\Gamma^{*}\left(M\right)\vert}{\vert\Gamma\left(M\right)\vert}$ as a function of $N$. While a sharp drop of $\frac{\vert\Gamma^{*}\left(M\right)\vert}{\vert\Gamma\left(M\right)\vert}$ occurs when $M$ changes from $1$ to $2$, a continuous and bounded increase of $\frac{\vert\Gamma^{*}\left(M\right)\vert}{\vert\Gamma\left(M\right)\vert}$ can be observed once $N\geq 2$. In other words, as $M$ enlarges, an increasing proportion of non-isomorphic percolation configuration classes in space $\Gamma$ corresponds to connected graphs. We also show the automorphism entropy across different values of $M$. As $M$ increases, the automorphism entropy rapidly reduces to $0$, suggesting the decrease of information loss degree according to the definition of the automorphism entropy.

\subsection{Empirical formula of automorphism entropy}
Apart from analytic calculation, we also present a numerical estimation of the automorphism entropy based on the least squares method. The simple variation trend of the automorphism entropy in Fig. \ref{G2} ensures an accurate fitting of it for $M\geq 2$ (we exclude $A\left(0\right)$ and $A\left(1\right)$ from fitting because their definitions can be arbitrary). Specifically, an exponential decay of $A\left(M\right)$ is obtained with high accuracy ($R^{2}=1$ and $RMSE=0.0005618$)
\begin{align}
    A\left(M\right)= 3.956^{+ 0.014}_{-0.014}\exp(-0.6876^{+ 0.0015}_{-0.0015}M),\;M\geq 2\label{EQ21}
\end{align}
where $3.956^{+ 0.014}_{-0.014}$ (i.e., $\left[3.942, 3.970\right]$) and $-0.6876^{+ 0.0015}_{-0.0015}$ (i.e., $\left[-0.6891, -0.6861\right]$) denote the intervals of regression coefficients with 95\% confidence, respectively. This empirical formula is more convenient in the case with a sufficiently large $M$ (e.g., $M=10^{5}$), where the analytic calculation method presented above becomes computationally costly (i.e., the analytic method requires integer partition). Please see \ref{ASec-3} for the code implementation of the empirical formula.

\section{Infinite cluster automorphism entropy}\label{Sec4}
\subsection{Giant cluster in Bernoulli percolation process}
In this section, we present a calculation instance of the automorphism entropy in a concrete percolation process. We implement a Bernoulli percolation process controlled by occupation probability $\rho$ in an initial network with $100$ nodes (note that the initial network is a complete graph). The code is presented in \ref{ASec-4}.

We primarily focus on the automorphism entropy defined on the giant cluster (i.e., infinite cluster), which is referred to as the infinite cluster automorphism entropy, $A_{\infty}$, in our work. To calculate this quantity, we need to deal with the calculation of $N$, the size of the giant cluster of correlated elements in the concerned system. Because we have reduced our question to the Bernoulli percolation on networks \cite{georgakopoulos2018analyticity} based on mean-field approximation, the precise value of $N$ given a mean occupation probability $\rho$ can be readily calculated by the generating function approach. Specifically, the probability for a random node to belong to the giant cluster, $\rho_{n}$, and the probability for a random edge to belong to the giant cluster, $\rho_{e}$, obey the following relations
\begin{align}
    \rho_{e}&=1-\sum_{k}P_{k}\frac{k}{\sum_{k}kP_{k}}\left(1-\rho_{e}\rho\right)^{k-1},\label{EQ22}\\
    \rho_{n}&=1-\sum_{k}P_{k}\left(1-\rho_{e}\rho\right)^{k},\label{EQ23}
\end{align}
where $P_{k}$ denotes the probability of finding a node with $k$ degrees in the corresponding graph (i.e., the initial network) of the correlated system \cite{li2021percolation,moore2000exact,newman2001random}. Because the initial network is set as a complete graph \cite{li2021percolation,duminil2018sixty}, i.e., $P_{k}=\delta\left(k,\vert V\vert-1\right)$ where $\delta\left(\cdot,\cdot\right)$ denotes the Kronecker delta, we can reformulate Eqs. (\ref{EQ22}-\ref{EQ23}) as
\begin{align}
    \rho_{e}&=1-\left(1-\rho_{e}\rho\right)^{\vert V\vert-2},\label{EQ24}\\
    \rho_{n}&=1-\left(1-\rho_{e}\rho\right)^{\vert V\vert-1}.\label{EQ25}
\end{align}
In practice, one can first solve Eq. (\ref{EQ24}) and insert $\rho_{e}$ into Eq. (\ref{EQ25}) to derive $\rho_{n}$. Given a $\rho_{n}$, the size of the giant cluster of correlated elements, $N$, can be directly obtained following Eq. (\ref{EQ4}). 

\subsection{Infinite cluster automorphism entropy in Bernoulli percolation}

\begin{figure}[b!]
\includegraphics[width=0.7\columnwidth]{G3.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G3} Calculation of the infinite cluster automorphism entropy during the Bernoulli percolation process. (a) shows the order parameter $\rho_{n}$ of the Bernoulli percolation as a function of $\rho$. The inserted sub-plot presents $\rho_{e}$ as a function of $\rho$ for reference. (b) illustrates the infinite cluster automorphism entropy across different values of $\rho$. The inserted sub-plot shows the size of the giant connected component as a function of $\rho$. } 
 \end{figure}

In Fig. \ref{G3}, we show the order parameter, $\rho_{n}$, of the Bernoulli percolation and the associated value of $N$ as the functions of $\rho$. Meanwhile, we calculate the associated infinite cluster automorphism entropy $A_{\infty}\left(N\right)$ at each value of $N$ for a qualitative comparison. As $\rho$ increases from $0$ to $1$, a percolation transition emerges at a certain threshold, after which a system-level connected component with a size of $\mathcal{O}\left(\vert V\vert\right)$ occurs in the network (here $\mathcal{O}\left(\cdot\right)$ denotes the big O notion of growth rate). A coherent variation trend can be observed in $A_{\infty}\left(N\right)$, where the percolation transition threshold is near a change point for $A_{\infty}\left(N\right)$ to exhibit a sharp reduction. Therefore, the infinite cluster automorphism entropy has the potential to serve as an indicator of percolation transition. The interval of $\rho$ in which $A_{\infty}\left(N\right)=1$ principally corresponds to the period before percolation transition. The change point at which $A_{\infty}\left(N\right)$ starts to depart from $1$ is close to the transition threshold. The reduction process of $A_{\infty}\left(N\right)$ is also the process for a giant connected component to include more nodes.

After qualitatively suggesting the sensitivity of the infinite cluster automorphism entropy towards percolation transition, we turn to deriving the quantitative relation between the infinite cluster automorphism entropy and percolation transition. According to Eq. (\ref{EQ25}) and our setting, we know $A_{\infty}\left(N\right)<1$ if and only if $N>2$ and $A_{\infty}\left(N\right)=1$ when $N\in\{0,1,2\}$. Therefore, the change point $\rho_{c}^{A}$ of $A_{\infty}\left(N\right)$ in terms of occupation probability is $\rho_{c}^{A}=\frac{2}{N}$. Meanwhile, we know that the percolation threshold $\rho_{c}$ of the Bernoulli percolation follows a form \cite{li2021percolation,moore2000exact,newman2001random}
\begin{align}
    \rho_{c}=\left(\frac{\partial }{\partial x} \sum_{k}P_{k}\frac{k}{\sum_{k}kP_{k}}x^{k-1}\Bigg\vert_{x=1}\right)^{-1},\label{EQ26}
\end{align}
which can be reformulated as 
\begin{align}
    \rho_{c}=\frac{\mathbb{E}\left(k\right)}{\mathbb{E}\left(k^{2}\right)-\mathbb{E}\left(k\right)}=\frac{1}{N-2}\label{EQ27}
\end{align}
because the initial network is a complete graph. Consequently, we can obtain the following relation
\begin{align}
    \rho_{c}=\frac{N}{2N-4}\rho_{c}^{A}.\label{EQ28}
\end{align}
At a limit of $N\rightarrow\infty$, Eq. (\ref{EQ28}) reduces to $\rho_{c}=\frac{1}{2}\rho_{c}^{A}$. In sum, the change point of $A_{\infty}\left(N\right)$ is proportional to the percolation threshold according to $N$. The infinite cluster automorphism entropy can be used to derive the percolation threshold in arbitrary correlated systems.

\section{Automorphism entropy spectrum}\label{Sec5}
\subsection{Cluster size distribution and automorphism entropy spectrum}
In the previous sections, we have introduced the infinite cluster automorphism entropy for comparing between $\mathbb{P}_{\infty}$ and $\mathbb{P}_{\rho}$. This metric quantifies the reduction of freedom degree in $\mathbb{P}_{\rho}$ and is primarily defined based on the giant cluster (i.e., the infinite cluster where the branching process is endless) emerged during percolation. Although this idea is natural and comprehensible, we can not ignore the fact that the giant cluster may not cover all nodes completely (i.e., there may exist finite clusters in the network). To fill this gap, we generalize our calculation to a spectrum of automorphism entropies measured for all finite clusters.  

Let us consider the distribution of finite clusters during percolation \cite{newman2007component,kryven2017general,kryven2017finite,moore2000exact,newman2001random}. The probability for a randomly selected node to belong to a cluster of size $r\in\mathbb{N}^{+}$ given an occupation probability $\rho$ is calculated as (see Refs. \cite{newman2007component,kryven2017general} for detailed derivations of Eq. (\ref{EQ29}))
\begin{align}
    \pi_{r}\left(\rho\right)=\frac{\rho^{r-1}\sum_{k}kP_{k}}{\left(r-1\right)!}\left[\frac{\partial^{r-2}}{\partial x^{r-2}}\left(\sum_{k=0}^{\infty}\frac{\left(k+1\right)P_{k+1}}{\sum_{k}kP_{k}}x^{k}\right)^{r}\right]\Bigg\vert_{x=1-\rho},\;r\geq 2,\label{EQ29}
\end{align}
which reduces to 
\begin{align}
    \pi_{r}\left(\rho\right)&=\frac{\rho^{r-1}\left(\vert V\vert-1\right)}{\left(r-1\right)!}\left[\frac{\partial^{r-2}}{\partial x^{r-2}}x^{r\left(\vert V\vert-2\right)}\right]\bigg\vert_{x=1-\rho},\;r\geq 2,\label{EQ30}\\
    &=\frac{\rho^{r-1}\left(\vert V\vert-1\right)}{\left(r-1\right)!}\prod_{i=0}^{r-3}\left[r\left(\vert V\vert-2\right)-i\right]\left(1-\rho\right)^{r\left(\vert V\vert-3\right)+2},\;r\geq 3\label{EQ31}
\end{align}
in our case (i.e., the initial network is a complete graph with $\vert V\vert$ nodes). For the special case with $r=1$ (i.e., the node is isolated), we have $\pi_{1}\left(\rho\right)=\left(1-\rho\right)^{\vert V\vert-1}$ (i.e., all the associate edges of the node in the initial network are not occupied). For the special case with $r=2$ (i.e., the node is connected with only one node) in Eq. (\ref{EQ30}), Eq. (\ref{EQ31}) is reformulated as $\pi_{2}\left(\rho\right)=\rho\left(\vert V\vert-1\right)\left(1-\rho\right)^{2\vert V\vert-4}$.

In fact, we can derive a distribution of $\pi_{r}\left(\rho\right)$ in a much simpler manner. Let us consider $Q_{k}$, the degree distribution of the diluted network. Given an occupation probability $\rho$, it is trivial to know that $Q_{k}$ is nothing else but the Poisson degree distribution
\begin{align}
   Q_{k}&\simeq \exp\left(\rho\vert V\vert-\rho\right)\frac{\left(\rho\vert V\vert-\rho\right)^{k}}{k!}\label{EQ32}.
\end{align}
The distribution of $\pi_{r}\left(\rho\right)$ in a network with a Poisson degree distribution can be readily derived as (note that we can no longer repeatedly analyze occupation while calculating $\pi_{r}\left(\rho\right)$ because the network has already been diluted before deriving $Q_{k}$)
\begin{align}
\pi_{r}\left(\rho\right)&=\frac{\sum_{k}kQ_{k}}{\left(r-1\right)!}\left[\frac{\partial^{r-2}}{\partial x^{r-2}}\left(\sum_{k=0}^{\infty}\frac{\left(k+1\right)Q_{k+1}}{\sum_{k}kQ_{k}}x^{k}\right)^{r}\right]\Bigg\vert_{x=0},\;r\geq 2,\label{EQ33}\\
&=\frac{\exp\left(\rho r-\rho\vert V\vert r\right)\left(\rho\vert V\vert r-\rho r\right)^{r-1}}{r!},\;r\geq 2.\label{EQ34}
\end{align}
One can easily prove the equivalence between Eq. (\ref{EQ31}) and Eq. (\ref{EQ34}). In \ref{ASec-5}, we present our code for computing $\pi_{r}\left(\rho\right)$.

Given the probability for a randomly selected node to belong to a cluster of a given size, we can derive $\theta_{r}\left(\rho\right)$, the distribution of cluster size in the diluted network, according to the following relation \cite{li2021percolation,moore2000exact,newman2001random}
\begin{align}
\pi_{r}\left(\rho\right)=\frac{r\theta_{r}\left(\rho\right)}{\sum_{r}r\theta_{r}\left(\rho\right)}.\label{EQ35}
\end{align}
Eq. (\ref{EQ35}) holds because the probability for a randomly selected node to belong to a cluster is proportional to the size of this cluster. The derivation of $\theta_{r}\left(\rho\right)$ based on Eq. (\ref{EQ35}) is presented in \ref{ASec-6}. The obtained result, organized according to the Kronecker delta $\delta\left(\cdot,\cdot\right)$, is
\begin{align}
&\theta_{r}\left(\rho\right)\notag\\=&\left(\frac{\exp\left(\rho r-\rho\vert V\vert r\right)\left(\rho\vert V\vert r-\rho r\right)^{r-1}}{r^2\left(r-1\right)!\left(1-\rho\right)^{\vert V\vert-1}}\right)^{1-\delta\left(1,r\right)}\left(1+\sum_{k=2}^{\vert V\vert}\frac{\exp\left(\rho k-\rho\vert V\vert k\right)\left(\rho\vert V\vert k-\rho k\right)^{k-1}}{k^2\left(k-1\right)!\left(1-\rho\right)^{\vert V\vert-1}}\right)^{-1}\label{EQ36}
\end{align}
for any $r\geq 1$. 

\begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{G4.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G4} Calculation of the automorphism entropy spectrum during the Bernoulli percolation process. (a) shows the distribution of $\pi_{r}\left(\rho\right)$ given different $\rho$. (b) shows the distribution of $\theta_{r}\left(\rho\right)$ given different $\rho$. (c) illustrates the automorphism entropy spectrum across different values of $\rho$.} 
 \end{figure}

Based on the non-trivial cluster size distribution presented in Eq. (\ref{EQ36}), we can define the automorphism entropy spectrum as $\zeta_{A}\left(\rho\right)$, the probability distribution of the automorphism entropies measured for every possible cluster. In the case where the empirical formula of the automorphism entropy in Eq. (\ref{EQ25}) is applied, the spectrum can be directly defined as
\begin{align}
\zeta_{A}\left(\rho\right)&=\theta_{r}\left(\rho\right),\;A<1,\label{EQ37}\\
r&=\frac{1}{-0.6876^{+ 0.0015}_{-0.0015}}\ln\left(\frac{1}{3.956^{+ 0.014}_{-0.014}}A\right).\label{EQ38}
\end{align}
Please note that $\zeta_{1}\left(\rho\right)=\theta_{1}\left(\rho\right)+\theta_{2}\left(\rho\right)$ because $A\left(1\right)=A\left(2\right)=1$. An efficient function for computing the automorphism entropy spectrum is proposed in \ref{ASec-6} . Certainly, Eq. (\ref{EQ38}) can also be replaced by the analytic expression of the automorphism entropy (see Eqs. (\ref{EQ16}-\ref{EQ20})). Here we use empirical formula only for convenience.

 \begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{G5.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G5} The evolution of the automorphism entropy spectrum near percolation transition. (a) shows several instances of the evolution of $\zeta_{A}\left(\rho\right)$ where $A\neq 1$. (b) illustrates the evolution of $\zeta_{A}\left(\rho\right)$ where $A= 1$ (we use $A\left(1\right)$ as an instance). Given $\zeta_{1}\left(\rho\right)=\theta_{1}\left(\rho\right)+\theta_{2}\left(\rho\right)$, the inserted sub-plot also presents the evolution of $\theta_{1}\left(\rho\right)$ and $\theta_{2}\left(\rho\right)$ for reference.} 
 \end{figure}

 In Fig. \ref{G4}, we present the instances of $\pi_{r}\left(\rho\right)$, $\theta_{r}\left(\rho\right)$, and $\zeta_{A}\left(\rho\right)$ given different values of $\rho$. These instances are calculated during the Bernoulli percolation process in a network with $100$ nodes, where $\rho$ increases from $\rho_{c}$ to $10\rho_{c}$. As shown in Fig. \ref{G4}, the increase of $\rho$ on a large scale (e.g., from $\rho_{c}$ to $10\rho_{c}$) seems to robustly imply a sharper decay of the probability for the automorphism entropy to reach at a smaller value. However, our subsequent analysis on a small scale (e.g., near $\rho_{c}$) suggests that this phenomenon is not universal and only serves as a part of a latent property of the automorphism entropy spectrum.

 As demonstrated in Fig. \ref{G5}, the automorphism entropy spectrum exhibits a clear change at $\rho_{c}$, the percolation transition threshold. Given the same experimental setting of Fig. \ref{G4}, we primarily focus on the cases where $\rho$ is near $\rho_{c}$. Computation results suggest that the value of $\zeta_{A}\left(\rho\right)$ ($A\neq 1$) increases when $\rho<\rho_{c}$ and decreases when $\rho>\rho_{c}$. A maximum value of $\zeta_{A}\left(\rho\right)$ ($A\neq 1$) emerges at the percolation transition threshold. An exactly opposite variation trend can be seen on $\zeta_{A}\left(\rho\right)$ ($A=1$), whose value is minimized at the percolation transition threshold. Therefore, the phenomenon observed in Fig. \ref{G4} is only a special case of the evolution of the automorphism entropy spectrum when $\rho>\rho_{c}$, which can not be treated as universal.

 If we summarize the above finding in terms of the condensation (i.e., the degree to which the probability density is condensed at a sub-set of its support) of probability distribution, the following conclusion on the property of the automorphism entropy spectrum can be drawn: The condensation degree of the automorphism entropy spectrum at $A=1$ decreases before percolation transition and is minimized at the transition threshold. After percolation transition, the condensation degree at $A=1$ gradually increases. The condensation degree of the automorphism entropy spectrum in the interval of $A>1$ increases before percolation transition and is maximized at the transition threshold. The condensation degree in this interval decreases after percolation transition.

   \begin{figure}[b!]
\includegraphics[width=0.7\columnwidth]{G6.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G6} The evolution of the moments of the automorphism entropy spectrum near percolation transition. (a) shows several instances of the evolution of $\mathbb{E}\left( A^{m}\right)$ where $m\in\{1,\ldots,20\}$. (b) illustrates $\Big\vert\frac{\partial}{\partial\frac{\rho}{\rho_{c}}}\mathbb{E}\left( A^{m}\right)\Big\vert$, the absolute change rate of the moments for $m\in\{1,\ldots,20\}$, near percolation transition.} 
 \end{figure}

\subsection{Moments of automorphism entropy spectrum}
Given the evolution of the automorphism entropy spectrum near percolation transition, we are also interested in its embodiment in the moments of the automorphism entropy spectrum. 

Let us consider the $m$-th order central moment of the spectrum
\begin{align}
&\mathbb{E}\left( A^{m}\right)=\sum_{A}\zeta_{A}\left(\rho\right)\left(A-\mathbb{E}\left( A\right)\right)^{m},\;m\geq 1.\label{EQ39}
\end{align}

 In Fig. \ref{G6}, we calculate the moments of the automorphism entropy spectrum based on the data in Fig. \ref{G5}. For convenience, we primarily derive the first twenty moments as instances. As shown in Fig. \ref{G6}, all these moments exhibit a kind of convex-function-like evolution and are minimized at the percolation transition threshold. Meanwhile, we also measure $\Big\vert\frac{\partial}{\partial\frac{\rho}{\rho_{c}}}\mathbb{E}\left( A^{m}\right)\Big\vert$, the absolute change rate with respect to $\frac{\rho}{\rho_{c}}$, for these moments. It turns out that $\Big\vert\frac{\partial}{\partial\frac{\rho}{\rho_{c}}}\mathbb{E}\left( A^{m}\right)\Big\vert$ is minimized at the transition threshold as well. According to these properties, we suggest that the evolution of the automorphism entropy spectrum and its moments may serve as indicators to reflect percolation transition.

\section{Non-isomorphic decomposition and the correlated systems}\label{Sec6}
To this point, we have introduced the infinite cluster automorphism entropy and the automorphism entropy spectrum as two basic concepts in the non-isomorphic decomposition analysis. Our previous analyses are presented in an abstract manner, where all the details of correlated systems are omitted to offer a clear vision. Below, we apply our theory on concrete examples of correlated systems with full details to show its potential in correlated system analysis. 

\subsection{Non-isomorphic decomposition and absorbing phase transition}
Our first concerned system is the branching model. This model is of interest because it and its variants have extensive applications across different fields, such as neuroscience \cite{haldeman2005critical,williams2014quasicritical,fosque2021evidence,marshall2016analysis,dalla2019modeling,tian2022theoretical}, social science \cite{sood2010interacting,ermakova2019branching,panaretos2007partially}, and geophysics \cite{kovchegov2022invariant,turcotte2009implications}.

For convenience, we consider a branching model defined on a square lattice with periodic boundary conditions \cite{marshall2016analysis}. In this model, all elements have two possible states, i.e., being active or silent. Each element only interacts with its four neighbors. A silent element has a small probability (e.g., $10^{-4}$ in our research) to be spontaneously activated in every time step and it will return to silence in the next time step (i.e., resetting). An active element has a probability of $p$ to activate each of its neighbors, creating a propagation process of activation. Given a small $p$, the propagation process rapidly vanishes because of resetting. As $p$ increases, activation may propagate among elements for a longer time. Given a sufficiently large $p$, the propagation process of activation may even become self-sustaining, i.e., the propagation is strong enough to suppress resetting. A self-sustaining propagation process can last for an infinite time, which is intrinsically different from those who are not self-sustaining and only last for a finite time. Therefore, two phases can be distinguished from each other in the phase space. The first phase, referred to as the active phase, corresponds to the cases where  propagation is self-sustaining. The second phase, referred to as the absorbing phase, corresponds to the cases where propagation vanishes in a finite time. The boundary between these two phases corresponds to the absorbing phase transition characterized by the directed percolation universality class, where the avalanches of activation emerge in the system \cite{hinrichsen2000non,lubeck2004universal,henkel2008non}. One can see reviews of these concepts in terms of neuroscience in Refs. \cite{tian2022theoretical,girardi2021brain} to acquire more detailed and pictorial explanations. Here we no longer elaborate these explanations and directly focus on the mathematical properties of the absorbing phase transition.

Mathematically, the branching model exhibits several scaling behaviours when it is captured by the directed percolation universality class. Specifically, the emerged avalanches during phase transition are exhibit the following power-law behaviours \cite{tian2022theoretical,girardi2021brain}
\begin{align}
    P\left(t\right)&\sim t^{-\alpha},\label{EQ40}\\
    P\left(s\right)&\sim s^{-\beta},\label{EQ41}
\end{align}
where $t$ and $s$ denote the lifetime and size of an avalanche. Meanwhile, the mean avalanche size given a certain lifetime, denoted by $\mathbb{E}\left(s\left(t\right)\right)$, is expected to follow an exponential relation
\begin{align}
\mathbb{E}\left(s\left(t\right)\right)&\sim t^{\gamma}.\label{EQ42}
\end{align}
In the directed percolation universality class, a scaling relation can be derived for the above exponents \cite{tian2022theoretical,girardi2021brain}
\begin{align}
\gamma=\frac{\alpha-1}{\beta-1}.\label{EQ43}
\end{align}
Apart from the scaling relation, an universal collapse should be observed in the scaled average temporal shape of avalanches. Specifically, there is
\begin{align}
\mathbb{E}\left(s\left(\tau|t\right)\right)&=t^{\psi}h\left(\frac{\tau}{t}\right),\;\forall\tau\in\left[0,t\right],\label{EQ44}\\
\psi&=\gamma-1,\label{EQ45}
\end{align}
where $f\left(\cdot\right)$ denotes a universal scaling function. In most cases, a parabolic form of $f\left(\cdot\right)$ can be derived \cite{laurson2013evolution}. On can see the derivation of Eqs. (\ref{EQ40}-\ref{EQ45}) in Refs. \cite{hinrichsen2000non,lubeck2004universal,henkel2008non,tian2022theoretical}.

 \begin{figure}[b!]
\includegraphics[width=1\columnwidth]{G7.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G7} The power-law behaviours, scaling relation, and universal collapse shape of avalanches. (a) shows the power-law behaviour of avalanche size $s$, suggesting that $\beta=1.453$. (b) shows the power-law behaviour of avalanche lifetime $t$, suggesting that $\alpha=1.625$. (c) illustrates the exponential behaviour of $\mathbb{E}\left(s\left(t\right)\right)$, deriving that $\gamma=1.4892$. The scaling relation is principally satisfied because $\frac{\alpha-1}{\beta-1}=1.3796$. (d) illustrates the scaled temporal shapes of different avalanches (marked by colorful solid lines). Meanwhile, a quadratic polynomial is defined to fit $f\left(\cdot\right)$ for the scaled average temporal shape (marked by black dashed line) to derive that $\psi=0.49\simeq \gamma-1$.} 
 \end{figure}

 In Fig. \ref{G7}, we implement a branching model with $100$ elements and $p=\frac{1}{4}$. Here we set $p=\frac{1}{4}$ because lattices are squares and $p=\frac{1}{4}$ implies that each active element can activate $1$ neighbor on average. According to Ref. \cite{marshall2016analysis}, this setting is sufficient to create avalanches. This property can be understood in terms of the Galton-Watson branching process, where the critical point corresponds to the case where an activate element gives birth to $1$ child on average \cite{athreya2004branching}. In the experiment, each element generates an activity sequence with a time length of $10^{6}$ to support counting avalanches. Given the generated data, we verify Eqs. (\ref{EQ40}-\ref{EQ45}) using the statistical toolbox proposed in Ref. \cite{marshall2016analysis}. As shown in Fig. \ref{G7}, the power-law behaviours, scaling relation, and universal collapse shape defined in Eqs. (\ref{EQ40}-\ref{EQ45}) are principally satisfied by the avalanche data. Although some errors are inevitably caused by data noise and finite size effects, their magnitudes are controlled within a reasonable interval.

 Now, let us consider a general case where we lack \emph{a priori} knowledge about the branching model, i.e., element iteration rules and lattice properties are unknown, and the model is involved with non-negligible noise. Although Eqs. (\ref{EQ40}-\ref{EQ45}) are expected to be satisfied when the system is critical (i.e., the necessary condition), we may not be able to use them to precisely determine if an unknown branching model is critical or not (i.e., the sufficient condition). This is because the latent power-law behaviours, scaling relation, and universal collapse shape of the system can be easily disturbed or covered by data noise, finite size effects, or other factors. As we demonstrate in \ref{ASec-7}, the determination of criticality based the latent power-law behaviours, scaling relation, and universal collapse shape can be amphibolous or even misleading, especially when we lack information about the system property. This may serve as a reason for the controversies about criticality in diverse correlated systems (e.g., see those in the brain \cite{tian2022theoretical,girardi2021brain}).

  \begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{G8.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G8} The evolution of the infinite cluster automorphism entropy and the automorphism entropy spectrum as $p$ increases. (a) visualizes the automorphism entropy measured on the giant cluster across different $p$. (b) shows several instances of the evolution of $\mathbb{E}\left( A^{m}\right)$ as $p$ increases, where $m\in\{1,\ldots,20\}$. (c) illustrates $\Big\vert\frac{\partial}{\partial p}\mathbb{E}\left( A^{m}\right)\Big\vert$, the absolute change rate of the moments for $m\in\{1,\ldots,20\}$, as $p$ increases.} 
 \end{figure}

 To verify the latent criticality of real data in a more precise manner, we need to ensure that the indicator is significant (i.e., change drastically at the critical point) and robust (i.e., can not be covered by data noise) enough. Inspired by the evolution of the automorphism entropy measured on the giant cluster, $A_{\infty}$, and the automorphism entropy spectrum, $\zeta_{A}$, near percolation transition, we suggest them as possible candidates. We present a possible approach to calculate $A_{\infty}$ and $\zeta_{A}$ in any branching model (please note that our approach only serves as an example and other approaches are also acceptable). The branching model is defined with $100$ elements and $p\in\left[0,0.4\right]$. Under each condition of $p$, the model runs for $10^{6}$ time steps to generate element activities, based on which we can estimate a Pearson correlation matrix $C\left(p\right)$ among these elements. Then we calculate a mean absolute correlation value, $\mathbb{E}_{i,j}\vert C_{ij}\left(p\right)\vert$, given each $p$. A mean occupation probability $\rho$ is defined as $\rho=g\left(\mathbb{E}_{i,j}\vert C_{ij}\left(p\right)\vert\right)$, where $g\left(\cdot\right)$ is a linear function that normalizes $\left[g\left(\mathbb{E}_{i,j}\vert C_{ij}\left(0\right)\vert\right),\ldots,g\left(\mathbb{E}_{i,j}\vert C_{ij}\left(0.4\right)\vert\right)\right]$ into an interval of $\left[0,1\right]$ (i.e., the min-max normalization). Finally, our concerned $A_{\infty}$ and $\zeta_{A}$ can be derived given each mean occupation probability $\rho$. 

 In Fig. \ref{G8}, we show the infinite cluster automorphism entropy, $A_{\infty}$, across different values of $\rho$, suggesting that $A_{\infty}$ is sensitive to change of $p$ and minimized near $p=0.25$. Then we calculate the first twenty central moments of the automorphism entropy spectrum under each condition of $p$. As shown in our results, all these moments exhibit sharp changes near $p=0.25$, the point where the branching model is captured by the directed percolation universality class. Quantitatively, we measure the absolute change rate of each moment as a function of $p$, demonstrating that $p=0.25$ implies the most dramatic change of each moment of the spectrum. Compared with the results in \ref{ASec-7}, our calculated $A_{\infty}$ and $\zeta_{A}$ can offer a much more clear indication of the latent criticality.

 An unexpected finding is that the automorphism entropy spectrum and its moments seem to exhibit less dramatic but still observable changes near $p=0.1$. We can not provide a clear explanation about this phenomenon yet and suggest it as a remaining question for future explorations.

 \subsection{Non-isomorphic decomposition and spreading processes}
Spreading processes, such as the epidemic spreading, are important topics in statistical physics due to their high application values \cite{li2021percolation}. A spreading process essentially corresponds to a correlated system because spreading relays on element interactions. 
 
To offer clear vision, let us begin with the simple SIR model defined with $\vert V\vert$ elements (i.e., agents) \cite{moreno2002epidemic,newman2002spread}. Each element has three possible states, i.e., being susceptible (S), infectious (I), or recovered (R). For convenience, we consider a case where there is no constraint on element adjacency relations such that each element is free to interact all other elements as its neighbors. This is a special case of the SIR model on networks, where the network is a uncorrelated complete graph (see the general case in Ref. \cite{li2021percolation}). The infection rate of the epidemic is $\xi$ such that an infectious element has a probability of $\xi\mathsf{d}t$ to make another susceptible element become infectious in an infinitesimal time interval $\mathsf{d}t$. Given a recovery rate $\mu$, each infectious element has a recovery probability of $\mu\mathsf{d}t$ to recover and become susceptible after $\mathsf{d}t$. These definitions allow us to derive the evolution equations
\begin{align}
\frac{\partial}{\partial t}I\left(t\right)&=\xi\left(\vert V\vert-1\right)S\left(t\right)\Theta\left(t\right)-\mu I\left(t\right),\label{EQ46}\\
\frac{\partial}{\partial t}S\left(t\right)&=-\xi\left(\vert V\vert-1\right)S\left(t\right)\Theta\left(t\right),\label{EQ47}\\
\frac{\partial}{\partial t}R\left(t\right)&=\mu I\left(t\right),\label{EQ48}
\end{align}
where $I\left(t\right)$, $S\left(t\right)$, and $R\left(t\right)$ denote the fraction of infections, susceptible, and recovered elements among elements at moment $t$. Notion $\Theta\left(t\right)$ measures the probability for an interaction (i.e., an edge) from an infected element links to another infected element 
\begin{align}
\Theta\left(t\right)&=\frac{\left(\vert V\vert-2\right)I\left(t\right)}{\vert V\vert-1},\label{EQ49}
\end{align}
whose integral during a duration can be calculated as 
\begin{align}
\Phi\left(t\right)&=\int_{0}^{t}\Theta\left(t^{\prime}\right)\mathsf{d}t^{\prime},\label{EQ50}\\
&=\frac{\vert V\vert-2}{\mu\left(\vert V\vert-1\right)}R\left(t\right).\label{EQ51}
\end{align}

\begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{G9.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G9} The evolution of the infinite cluster automorphism entropy and the automorphism entropy spectrum as $\tau$ changes. (a) presents the infinite cluster automorphism entropy across different $\frac{\left(\vert V\vert-2\right)\left(2\vert V\vert-4\right)}{\tau\vert V\vert}$. (b) shows the instances of the evolution of $\mathbb{E}\left( A^{m}\right)$ across different $\tau$, where $m\in\{1,\ldots,20\}$. (c) illustrates $\Big\vert\frac{\partial}{\partial p}\mathbb{E}\left( A^{m}\right)\Big\vert$, the absolute change rate of the moments for $m\in\{1,\ldots,20\}$, when $\tau$ increases.} 
 \end{figure}
 
At the time limit $t\rightarrow\infty$, infection vanishes and all elements are either susceptible or recovery \cite{li2021percolation,moreno2002epidemic,newman2002spread}. The steady state of this system can be discovered according to 
\begin{align}
0&=\frac{\partial}{\partial t}\Phi\left(t\right)\Big\vert_{t\rightarrow\infty},\label{EQ52}\\
&=\frac{\vert V\vert-2}{\vert V\vert-1}I\left(\infty\right),\label{EQ53}\\
&=\frac{\vert V\vert-2}{\vert V\vert-1}\left(1-S\left(\infty\right)\right)-\mu\Phi\left(\infty\right).\label{EQ54}
\end{align}
Meanwhile, given the initial conditions, $S\left(0\right)\simeq 1$, $I\left(0\right)\simeq 0$, and $R\left(0\right)= 0$, of Eqs. (\ref{EQ46}-\ref{EQ48}), we can derive a solution
\begin{align}
S\left(t\right)=\exp[-\xi\left(\vert V\vert-1\right)\Phi\left(t\right)].\label{EQ55}
\end{align}
Combining Eqs. (\ref{EQ54}-\ref{EQ55}), we have the self-consistent equation of $\Phi\left(\infty\right)$
\begin{align}
\Phi\left(\infty\right)=\frac{\vert V\vert-2}{\mu\left(\vert V\vert-1\right)}\left(1-\exp[-\xi\left(\vert V\vert-1\right)\Phi\left(\infty\right)]\right).\label{EQ56}
\end{align}
Eq. (\ref{EQ56}) only has a solution $\Phi\left(\infty\right)=0$ (i.e., there is no epidemic process) if 
\begin{align}
\frac{\partial}{\partial \Phi\left(\infty\right)}\left\{\frac{\vert V\vert-2}{\mu\left(\vert V\vert-1\right)}\left(1-\exp[-\xi\left(\vert V\vert-1\right)\Phi\left(\infty\right)]\right)\right\}\Bigg\vert_{\Phi\left(\infty\right)=0}<1.\label{EQ57}
\end{align}
Therefore, the epidemic threshold, i.e., the threshold of $\frac{\xi}{\mu}$ that allows the epidemic to break out, can be discovered as 
\begin{align}
\frac{\partial}{\partial \Phi\left(\infty\right)}\left\{\frac{\vert V\vert-2}{\mu\left(\vert V\vert-1\right)}\left(1-\exp[-\xi\left(\vert V\vert-1\right)\Phi\left(\infty\right)]\right)\right\}\Bigg\vert_{\Phi\left(\infty\right)=0}&=1,\label{EQ58}\\
\frac{\xi_{c}}{\mu_{c}}&=\frac{1}{\vert V\vert-2}. \label{EQ59}
\end{align}

The result in Eq. (\ref{EQ59}) is exactly the percolation threshold of a complete graph (i.e., see Eq. (\ref{EQ27}) for explanations). Therefore, the phase transition of the above epidemic process can readily predicted by $A_{\infty}$, the infinite cluster automorphism entropy. Specifically, we have $\rho_{c}^{A}=\frac{2\vert V\vert-4}{\vert V\vert}\frac{\xi_{c}}{\mu_{c}}$ according to Eq. (\ref{EQ28}) and Eq. (\ref{EQ59}). As for $\zeta_{A}$, the automorphism entropy spectrum, we can define occupation probability $\rho$ as the probability of effective epidemic spreading controlled by $\frac{\xi}{\mu}\in\left[0,1\right]$. A small value of $\rho=\frac{\xi}{\mu}$ corresponds to the case where epidemic spreading is less effective and vanishes soon. To distinguish between the phases of this system at the time limit, we can consider the outbreak time scale \cite{li2021percolation,moreno2002epidemic,newman2002spread}
\begin{align}
\tau=\frac{\mu\mathbb{E}\left(k\right)}{\xi\left(\mathbb{E}\left(k^{2}\right)-\mathbb{E}\left(k\right)\right)}=\frac{\mu}{\xi\left(\vert V\vert-2\right)}=\frac{\mu}{\xi}\frac{\xi_{c}}{\mu_{c}}, \label{EQ60}
\end{align}
where a small $\tau$ indicates a sufficiently fast spreading of the infection and $\tau=1$ holds at the epidemic threshold.

 In Fig. \ref{G9}, we show the evolution of the infinite cluster automorphism entropy and the automorphism entropy spectrum across different values of $\tau$. The epidemic spreading is defined on a system with $100$ elements, i.e., $\vert V\vert=100$. By simple calculation, we can know that the transition point of $A_{\infty}$ in such a system is expected as $\rho_{c}^{A}=\frac{\left(\vert V\vert-2\right)\left(2\vert V\vert-4\right)}{\tau\vert V\vert}\simeq 0.2$. Our results in Fig. \ref{G9} are consistent with this calculation. Meanwhile, it can be seen that all the moments of the automorphism entropy spectrum are minimized at $\tau=1$, which corresponds to the epidemic threshold. Meanwhile, the epidemic threshold implies the local minima of the absolute change rates of these moments. In sum, the evolution behaviours of the infinite cluster automorphism entropy and the automorphism entropy spectrum may be used to distinguish between 
 and further study different phases of epidemic outbreak, a typical instance of spreading processes.

 One thing needs to be mentioned about the automorphism entropy spectrum. In the more universal cases where there are certain constraints on element adjacency relations, i.e., the epidemic spreading happens on an arbitrary network, our derivation of the spectrum can be generalized as well. Specifically, one needs to derive the degree distribution $P_{k}$ in the given network and insert it into Eq. (\ref{EQ29}) to recalculate distribution $\pi_{r}\left(\rho\right)$, cluster size distribution, and all subsequent results. Note that Eqs. (\ref{EQ30}-\ref{EQ36}) can no longer be followed because they are limited to complete graphs. Here we do not elaborate these generalizations in details.

 \subsection{Non-isomorphic decomposition and synchronization}
 Synchronization is a widespread phenomenon in diverse correlated systems, such as brains, social networks, and bird flocks \cite{boccaletti2002synchronization,arenas2008synchronization,boccaletti2016explosive,ghosh2022synchronized}. The Kuramoto model, a model of coupled phase
oscillators, is a representative framework for analyzing synchronization \cite{rodrigues2016kuramoto,acebron2005kuramoto}. In our subsequent analysis, we apply the non-isomorphic decomposition analysis to studying the change of freedom degrees of the Kuramoto model during synchronization.

For convenience, we consider the classic Kuramoto model with $\vert V\vert$ oscillators \cite{rodrigues2016kuramoto,acebron2005kuramoto}
\begin{align}
\frac{\partial}{\partial t}\theta_{i}\left(t\right)=\omega_{i}+\frac{\kappa}{\vert V\vert}\sum_{i=1}^{\vert V\vert}\sin\left[\theta_{j}\left(t\right)-\theta_{i}\left(t\right)\right], \label{EQ61}
\end{align}
where $\theta_{i}$ and $\omega_{i}$ denote the phase and natural frequency of the $i$-th oscillator, respectively. Parameter $\kappa$ measures the coupling strength among oscillators. The order parameter of the above correlated system is 
\begin{align}
r\left(t\right)\exp\left[\mathsf{i}\psi\left(t\right)\right]=\frac{1}{\vert V\vert}\sum_{i=1}^{\vert V\vert}\exp\left[\mathsf{i}\theta_{j}\left(t\right)\right], \label{EQ62}
\end{align}
in which $\psi\left(t\right)$ is the time-dependent average phase and $r\left(t\right)\in\left[0,1\right]$ measures the synchronization degree of the system. Based on the order parameter, we can reorganize Eq. (\ref{EQ61}) as
\begin{align}
\frac{\partial}{\partial t}\theta_{i}\left(t\right)=\omega_{i}+\kappa r\left(t\right)\sin\left[\psi\left(t\right)-\theta_{i}\left(t\right)\right]. \label{EQ63}
\end{align}
According to Eq. (\ref{EQ63}), a feedback loop exists between coupling strength $\kappa$ and order parameter $r$, i.e., any increment in $r$ due to the increasing $\kappa$ will enlarge the effective
coupling among oscillators and attract more oscillators to the synchronous populations in return \cite{rodrigues2016kuramoto,acebron2005kuramoto}.

We are interested in the non-trivial evolution of the infinite cluster automorphism entropy and the automorphism entropy spectrum while the system becomes increasingly synchronous (e.g., by enlarging coupling strength). We define a Kuramoto model with $100$ oscillators. Each oscillator has a random natural frequency uniformly selected from $\left[1,5\right]$ and exhibits activities for $1000$ time steps. All oscillators are initialized with a random phase selected from $\left[-\pi,\pi\right]$. We set an increasing coupling strength $\kappa\in\left[0,5\right]$ and repeat the experiment under each condition of $\kappa$. In Fig. \ref{G10}, we show the sequence of order parameter $r\left(t\right)$ across different $\kappa$. Given a sufficiently large $\kappa$, the sequence of $r\left(t\right)$ gradually converges to $1$ as the number of time step accumulates, suggesting that all the oscillators become strongly synchronous. Given a small $\kappa$, the sequence of $r\left(t\right)$ exhibits irregular oscillations rather than converge to a certain value, corresponding to the case where oscillators are not synchronous. These results can be validated according to $\mathbb{E}\left(r\right)$, the mean order parameter derived by time-averaging under each condition of $\kappa$, as well. A sharp increase of $\mathbb{E}\left(r\right)$ can be seen near $\kappa\simeq 2$.

\begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{G10.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{G10} The evolution of the infinite cluster automorphism entropy and the automorphism entropy spectrum as the synchronization degree changes. (a) illustrates the time series of $r\left(t\right)$ under each condition of $\kappa$. As $\kappa$ increases from $1$ to $5$, the color of the line of $r\left(t\right)$ changes from blue to green. (b) presents the mean order parameter (upper parallel) and the infinite cluster automorphism entropy (bottom parallel) across different $\kappa$. (c) shows the instances of the evolution of $\mathbb{E}\left( A^{m}\right)$ across different $\kappa$, where $m\in\{1,\ldots,20\}$.} 
 \end{figure}
 
To relate these results with our theory, we define a correlation matrix $C$ in terms of synchronization
\begin{align}
C_{ab}&=\mathbb{E}\left[r_{ab}\left(t\right)\right], \label{EQ64}\\
r_{ab}\left(t\right)\exp\left[\mathsf{i}\psi\left(t\right)\right]&=\frac{1}{2}\left\{\exp\left[\mathsf{i}\theta_{a}\left(t\right)\right]+\exp\left[\mathsf{i}\theta_{b}\left(t\right)\right]\right\}, \label{EQ65}
\end{align}
where the $\left(a,b\right)$-th entity measures the local synchronization degree between oscillators $a$ and $b$. Then we generate the network of oscillators, where there exists an edge between oscillators $a$ and $b$ only if $C_{ab}\simeq 1$, i.e., these two oscillators are strongly synchronous. Here we do not require $C_{ab}= 1$ because numerical errors may lead to a non-one synchronization degree even if oscillators $a$ and $b$ are strictly synchronous (during computation, we determine $C_{ab}\simeq 1$ if $C_{ab}\geq 1-10^{-3}$). Given the network under each condition of $\kappa$, we can calculate the infinite cluster automorphism entropy and the automorphism entropy spectrum. Although the calculation lacks analytic expressions (i.e., the formulas of infinite cluster size and finite cluster size distribution remain highly non-trivial), we can empirically count cluster sizes in each network to support calculation. In Fig. \ref{G10}, the derived infinite cluster automorphism entropy is shown as a function of $\kappa$. It can be seen that the infinite cluster automorphism entropy exhibits drastic changes when the mean synchronization degree of the system, $\mathbb{E}\left(r\right)$, starts to increase sharply. Meanwhile, the moments of the automorphism entropy spectrum maintain generally constant before $\mathbb{E}\left(r\right)$ sharply increases near $\kappa\simeq 2$. Once $\mathbb{E}\left(r\right)$ increases, these moments starts to oscillate and slightly decrease. Therefore, the evolution of the infinite cluster automorphism entropy and the automorphism entropy spectrum can reflect the emerging synchronization within the system.

Apart from the discussion on synchronization, our analysis presented above also serves as an instance of the non-isomorphic decomposition analysis without mean-field approximation, i.e., our network generation approach presented above does not require a mean occupation probability $\rho$ and defines each edge according to its corresponding local synchronization degree. This instance highlights the fact that mean-field approximation is not a necessary condition of our theory. However, we note that excluding mean-field approximation does create difficulties for analytic derivations. In the future, more theoretical explorations are necessary for establishing a non-mean-field version of our framework.

\section{Physics aspects of non-isomorphic decomposition}\label{Sec7}
In our previous sections, we have defined the basic concepts of the non-isomorphic decomposition analysis and presented calculation instances in concrete correlated systems. In this section, we return to the very beginning and summarize the physics idea underlying the proposed framework.

\subsection{Coarse graining guided by non-isomorphic decomposition}
As we have mentioned before, our framework is motivated by analyzing percolation and correlated systems in terms of the variation of information (i.e., freedom degrees) in percolation configuration ensemble. This analysis can be implemented by comparing between $\mathbb{P}_{\rho}$, a space corresponding to the target properties of a correlated system with constrained element relations, and $\mathbb{P}_{\infty}$, a space corresponding to the neutral properties of the correlated system when there
is no constraint on element relations. 

We first note that $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\infty}$ actually correspond to two extreme cases of a correlated system. According to the definition, space $\mathbb{P}_{\infty}$ denotes an extreme case where all system sates share the same probability to occur. Space $\mathbb{P}_{\rho}$ stands for an extreme case where only the system states corresponding to given element correlations are allowed to occur (e.g., any system state in which the clusters of correlated elements do not satisfy the desired infinite cluster size or finite cluster size distribution is not allowed to occur). A real correlated system always places itself at somewhere between these two extreme cases. This is because internal correlations manifest themselves in probabilistic forms and can not ensure the property of a real system in a deterministic way (e.g., a real system with probability $\rho$ has a giant cluster of size $N$ on average rather than deterministically). Therefore, if we denote the actual probability space where the system lives in as $\mathbb{P}_{\mathsf{ac}}$ and measure its information quantity as $H_{\mathsf{ac}}$, we can extend Eq. (\ref{EQ14}) to 
\begin{align}
    H_{\rho}\leq H_{\mathsf{ac}} \leq H_{\infty}. \label{EQ66} 
\end{align}
Eq. (\ref{EQ66}) holds because $\mathbb{P}_{\mathsf{ac}}$ is more flexible in maintaining freedom degrees than $\mathbb{P}_{\rho}$ while $\mathbb{P}_{\infty}$ actually maximizes entropy (i.e., the uniform distribution in a finite interval is the maximum entropy distribution).

In our work, we propose the infinite cluster automorphism entropy, $A_{\infty}$, and the automorphism entropy spectrum, $\zeta_{A}$, to measure the difference in freedom degrees between $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\infty}$ that are related to the infinite and finite clusters, respectively. Because these clusters are pre-determined by a correlation matrix, i.e., a matrix of element correlation strength, our proposed $A_{\infty}$ and $\zeta_{A}$ naturally evolve as internal correlation changes. As shown in Secs. \ref{Sec4}-\ref{Sec6}, the increase of internal correlation, irrespective of being related to percolation, emerged avalanches, the outbreak of spreading process, synchronization, or other mechanisms, generally reduces $A_{\infty}$ and $\zeta_{A}$. In other words, as the system becomes increasingly correlated, the informational difference between $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\infty}$ gradually decreases. 

Given a sufficiently small difference between $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\infty}$, the difference between $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\mathsf{ac}}$ must be small as well 
\begin{align}
    H_{\mathsf{ac}}-H_{\rho}\leq H_{\infty}-H_{\rho}. \label{EQ67} 
\end{align}
Therefore, we can find omissible freedom degrees and approximate $\mathbb{P}_{\mathsf{ac}}$ based on the properties of $\mathbb{P}_{\rho}$. Specifically, for an arbitrary cluster of multiple nodes, we can safely omit the possibilities where these nodes are not connected in $\mathbb{P}_{\mathsf{ac}}$. This is because omitting these freedom degrees does not imply a significant information loss, i.e., the derived $\mathbb{P}_{\rho}$ is still similar to $\mathbb{P}_{\mathsf{ac}}$. 

The operation described above is essentially similar to coarse graining in renormalization group \cite{zinn2007phase,wilson1983renormalization,goldenfeld2018lectures}, where omissible freedom degrees (i.e., those with low contributions or effects) are eliminated as well. Given this similarity, we may find the physics connotations of our proposed non-isomorphic decomposition analysis, i.e., our framework may convey information about when and how a real correlated system in $\mathbb{P}_{\mathsf{ac}}$ can be transformed into $\mathbb{P}_{\rho}$ by specific coarse graining approaches. 

In this section, we propose a possible coarse graining approach based on the non-isomorphic decomposition analysis, which enables us to discuss the relation between our framework and diverse renormalization groups \cite{zinn2007phase,wilson1983renormalization,goldenfeld2018lectures}.

Let us consider an arbitrary correlated system with a correlation matrix $C$. If element relations are weak (i.e., the absolute values of entities in $C$ are small), coarse graining on elements may be improper because every element is unique and independent. In an opposite case, each certain group of elements with strong correlations can be replaced by a super-element to realize coarse graining. In more general cases, we can identify elements to coarse grain using a function of the correlation matrix, denoted by $K=y\left(C\right)$. Here $y\left(\cdot\right)$ is an appropriate function that satisfies application demands and ensures 
\begin{align}
    \frac{\partial }{\partial \vert C_{ij}\vert}y\left(C_{ij}\right)\geq 0,\label{EQ68} 
\end{align}
for any $\left(i,j\right)$ pair.

Based on the above definition, we develop a possible way to coarse grain the system:
\begin{itemize}
    \item[(A) ] The coarse graining is initialized by representing the correlated system as a complete graph $G^{\left(0\right)}\left(V^{\left(0\right)},E^{\left(0\right)}\right)$ whose adjacency matrix is defined as $\vert C\vert$. 
    \item[(B) ] In the $k$-th iteration of coarse graining, we generate $H^{\left(k\right)}$, a copy of $G^{\left(k\right)}$, as a reference. Then the edges corresponding to small values in $K^{\left(k\right)}$, the function of the correlation matrix in the $k$-th iteration, are removed from $H^{\left(k\right)}$ one by one until there remain $n_{k}$ clusters (i.e., connected components) in $H^{\left(k\right)}$. Here $n_{k}$ is a non-trivial parameter that requires a careful definition. Each remaining cluster in $H^{\left(k\right)}$ contains a set of $M$ elements with relatively strong correlations (i.e., such that the associate values in $K^{\left(k\right)}$ can be large). According to the non-isomorphic decomposition analysis, this cluster can be equivalently analyzed as a network with $M$ nodes, which has a sufficiently small automorphism entropy because of strong correlations (e.g., $A_{\infty}$ and $\zeta_{A}$). Therefore, the difference between the associate spaces, $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\infty}$, is small (note that here $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\infty}$ denote the probability spaces of non-isomorphic percolation configuration classes that can form on $M$ nodes). According to Eq. (\ref{EQ67}), the difference between $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\mathsf{ac}}$ is small as well. Therefore, we can treat the correlated elements contained in this cluster as always coherent (i.e., they always form a connected component as required by $\mathbb{P}_{\rho}$). The possibilities that they may occasionally be incoherent can be eliminated without significant information loss because $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\mathsf{ac}}$ have become sufficiently similar for these elements. Consequently, we can analyze these elements as a whole and replace them by a super-element. 
     \item[(C) ] The coarse graining can be done in $G^{\left(k\right)}$ by aggregating this set of nodes and replacing them by a super-node. After realizing this coarse graining for every cluster in $H^{\left(k\right)}$, we can generate $G^{\left(k+1\right)}$ to continue the iteration.
\end{itemize}
Please note that the reference graph $H^{\left(k\right)}$ in step (B) is proposed only for convenience (i.e., for idea clarification and computational programming). We can also realize step (B) without the help of $H^{\left(k\right)}$.

The key steps of the above approach are to define an appropriate function $y\left(\cdot\right)$ and a meaningful $n_{k}$ in each $k$-th iteration, which can vary across different application demands. Below, we suggest their manifestations in existing renormalization groups \cite{zinn2007phase,wilson1983renormalization,goldenfeld2018lectures}.

\subsection{Manifestation in Laplacian renormalization group}

We first consider the Laplacian renormalization group (LRG) \cite{villegas2023laplacian}. In the spirit of free field theories \cite{zinn2007phase,wilson1983renormalization,goldenfeld2018lectures},  a Laplacian is defined in the LRG
\begin{align}
L_{ij}=\delta\left(i,j\right)\sum_{k}\vert C_{ik}\vert-\vert C_{ij}\vert \label{EQ69} 
\end{align}
using the Kronecker delta function $\delta\left(\cdot,\cdot\right)$ and a normalized network propagator 
\begin{align}
    K=y\left(C\right):=\frac{\exp\left(-\tau L\right)}{\mathsf{Tr}\left[\exp\left(-\tau L\right)\right]}\label{EQ70} 
\end{align}
is considered, where $\mathsf{Tr}\left(\cdot\right)$ denotes the trace of an operator. Given a time scale $\tau$, this propagator is known as the counterpart of the path-integral of general diffusion processes \cite{feynman2010quantum}. 

In each $k$-th iteration, parameter $n_{k}$ is defined by the following way. Based on the bra-ket formalism, $L^{\left(k\right)}$, the Laplacian of $G^{\left(k\right)}$,  is expressed as
\begin{align}
L^{\left(k\right)}=\sum_{\lambda}\lambda\vert \lambda\rangle\langle\lambda\vert, \label{EQ71} 
\end{align}
where each $\lambda$ and $\vert \lambda\rangle$ denote an eigenvalue and the associate (column) eigenvector, respectively. Parameter $n_{k}$ is defined as 
\begin{align}
n_{k}=\sum_{\lambda}\Theta\left(\frac{1}{\tau}-\lambda\right), \label{EQ72} 
\end{align}
where $\Theta\left(\cdot\right)$ is the Heaviside step function. In the momentum space, the eigenvalues satisfying $\lambda>\frac{1}{\tau}$ correspond to strong correlations and fast diffusion processes, whose contributions to $K^{\left(k\right)}$ exhibit a rapid decay because
\begin{align}
K^{\left(k\right)}=\prod_{\lambda}\exp\left(-\left(\tau\lambda\right)\vert \lambda\rangle\langle\lambda\vert\right). \label{EQ73} 
\end{align}
According to the non-isomorphic decomposition analysis, this kind of eigenvalues can be eliminated during coarse graining because of strong correlations. After coarse graining, the remaining $n_{k}$ eigenvalues correspond to slow diffusion processes, whose contributions to $K^{\left(k\right)}$ are long-range and long-term. In the real space, the LRG simultaneously makes $G^{\left(k\right)}$ coarse grained in a way similar to the step (B) of our approach. Specifically, nodes are gradually aggregated if they are connected by the edges that correspond to large values in $K^{\left(k\right)}$. Each set of aggregated nodes is replaced by a super-node. This process is repeated until there remain $n_{k}$ nodes (including super-nodes) in the coarse grained network. In fact, the step (B) of our approach, which includes $H^{\left(k\right)}$ as a reference, realizes the above coarse graining in a more easy-to-program way. 

In the LRG, the correspondence between the moment space and the real space is realized based on the renormalized Laplacian $L^{\left(k+1\right)}$
\begin{align}
L^{\left(k+1\right)}=\tau L^{\left(k\right)}_{\mathsf{cg}}, \label{EQ74} 
\end{align}
where $L^{\left(k\right)}_{\mathsf{cg}}\in\mathbb{R}^{n_{k}\times n_{k}}$ is the Laplacian of the coarse grained network in the $k$-th iteration. This Laplacian is expected to be consistent with the coarse graining in the moment space
\begin{align}
U^{-1}\left(\sum_{\lambda}\Theta\left(\frac{1}{\tau}-\lambda\right)\lambda\vert \lambda\rangle\langle\lambda\vert\right)U&=\mathsf{diag}\left(L^{\left(k\right)}_{\mathsf{cg}},\mathbf{0}\right), \label{EQ75}
\end{align}
where $U$ denotes a similarity transformation, notion $\mathsf{diag}\left(\cdot\right)$ stands for the diagonal, and notion $\mathbf{0}$ is a all-zero square matrix whose width is $D_{k}-n_{k}$ (here we mark $D_{k}=\vert V^{\left(k\right)}\vert=\mathsf{dim}\left(L^{\left(k\right)}\right)$). The left side of Eq. (\ref{EQ75}) corresponds to the moment space, where all eigenvalues satisfying $\lambda>\frac{1}{\tau}$ are eliminated, and the right side of Eq. (\ref{EQ75}) concerns the real space. Clearly, the similarity transformation exactly corresponds to the coarse graining on $G^{\left(k\right)}$
\begin{align}
U=\left(\vert \nu_{1}\rangle,\ldots, \vert \nu_{n_{k}}\rangle, \vert \sigma_{n_{k}+1}\rangle,\ldots, \vert \sigma_{D_{k}}\rangle\right), \label{EQ76}
\end{align}
where each $\nu_{i}$ is a super-node in the coarse grained network. Notion $\vert \nu_{i}\rangle$ is a $D_{k}$-dimensional column ket, whose unitary components correspond to all the nodes in $G^{\left(k\right)}$ that are aggregated in $\nu_{i}$ while zero components correspond to all other nodes in $G^{\left(k\right)}$ that are not covered by $\nu_{i}$. Each ket $\sigma_{j}$ is designed such that $\left(\vert \nu_{1}\rangle,\ldots, \vert \nu_{n_{k}}\rangle, \vert \sigma_{n_{k}+1}\rangle,\ldots, \vert \sigma_{D_{k}}\rangle\right)$ defines a group of orthonormal bases. Meanwhile, Eq. (\ref{EQ76}) is required to satisfy
\begin{align}
\langle\nu_{i}\vert \left(\sum_{\lambda}\Theta\left(\frac{1}{\tau}-\lambda\right)\lambda\vert \lambda\rangle\langle\lambda\vert\right)\vert \nu_{j}\rangle&=\left(L^{\left(k\right)}_{\mathsf{cg}}\right)_{\nu_{i}\nu_{j}}, \label{EQ77}\\
\langle\nu_{i}\vert \left(\sum_{\lambda}\Theta\left(\frac{1}{\tau}-\lambda\right)\lambda\vert \lambda\rangle\langle\lambda\vert\right)\vert \sigma_{j}\rangle&=0, \label{EQ78}\\
\langle\sigma_{i}\vert \left(\sum_{\lambda}\Theta\left(\frac{1}{\tau}-\lambda\right)\lambda\vert \lambda\rangle\langle\lambda\vert\right)\vert \sigma_{j}\rangle&=0. \label{EQ79}
\end{align}
Given Eqs. (\ref{EQ77}-\ref{EQ79}), the validity of Eq. (\ref{EQ75}) is ensured.

To summarize, the LRG implements a special case of our proposed coarse graining process described in steps (A-C), where $y\left(\cdot\right)$ serves as a mapping from the Laplacian to the normalized network propagator and $n_{k}$ in each $k$-th iteration is calculated according to the number of modes corresponding to slow diffusion processes.

\subsection{Manifestation in moment space phenomenological renormalization group}
Then we turn to considering the moment space phenomenological renormalization group (MSPRG) proposed by William Bialek \emph{et al.} \cite{bradde2017pca} (also see discussions in Ref. \cite{lahoche2022generalized}). 


Following the idea of perturbative theory, the MSPRG considers the Gaussian-like (i.e., may not be strictly Gaussian) truncated fluctuations of the probability of the system state $\{x_{i}\}$
\begin{align}
P\left(\{x_{i}\}\right)=\frac{1}{Z}\exp\left[-\frac{1}{2}\sum_{i,j}x_{i} K_{ij}x_{j}-\frac{1}{4!}q\sum_{i}x_{i}^{4}\right], \label{EQ80}
\end{align}
where $Z$ serves as a normalization term and $q$ describes the kurtosis (i.e., the fluctuations become Gaussian if $q=0$). Matrix $K$ may lack a explicit expression in most cases. However, once $q\rightarrow 0$ is satisfied, we can observe an convergence relation between $K$ and the inverse of covariance matrix $\Sigma$ \cite{bradde2017pca,lahoche2022generalized}. Therefore, eliminating the short-term contributions of large eigenvalues and the associate eigenvectors of $K$ in the moment space is similar to, but not necessarily equivalent to, eliminating the contributions of small eigenvalues and the associate eigenvectors of $\Sigma$ (the equivalence holds only when $q=0$ is strictly ensured). This property can be understood in terms of the correlation function. Specifically, the eigenvalues of the
covariance matrix in the Fourier space is fully characterized by $L\left(\cdot\right)$, the Fourier transform of the correlation function 
\begin{align}
\sum_{\mathbf{x}_{j}}\Sigma\left(\mathbf{x}_{i}-\mathbf{x}_{j}\right)\exp\left(\mathsf{i}\mathbf{k}\mathbf{x}_{j}\right)=\exp\left(\mathsf{i}\mathbf{k}\mathbf{x}_{i}\right)L\left(\mathbf{k}\right), \label{EQ81}
\end{align}
where we have slightly abused the notion to mark $\Sigma_{ij}=\Sigma\left(\mathbf{x}_{i}-\mathbf{x}_{j}\right)$ in the Fourier space as $q\rightarrow 0$. According to Eq. (\ref{EQ81}), we can generally treat the Fourier transform of the correlation function as the eigenvalue spectrum of $\Sigma$. Therefore, eliminating the contributions of small eigenvalues and the associate eigenvectors of $\Sigma$ is similar to eliminating the short-term or short-range correlation in the Fourier space.

Inspired by these facts, the MSPRG is proposed as the following way. In the $k$-th iteration, the coarse graining projection $W^{\left(k\right)}$ is defined as
\begin{align}
W^{\left(k\right)}=\sum_{i=1}^{n_{k}}\vert \lambda_{i}\rangle\langle\lambda_{i}\vert, \label{EQ82}
\end{align}
where $\{\lambda_{1}\geq\ldots\geq\lambda_{D_{k}}\}$ denotes the sorted eigenvalues of $\Sigma^{\left(k\right)}$ (here $\Sigma^{\left(k\right)}$ is the covariance matrix estimated in the system in the $k$-th iteration) and $n_{k}$ is simply set as a constant for all iterations in Ref. \cite{bradde2017pca}. Given Eq. (\ref{EQ82}), the coarse graining is realized as
\begin{align}
x_{i}^{\left(k\right)}\rightarrow x_{i}^{\left(k+1\right)}=\sum_{j}W^{\left(k\right)}_{ij}x_{j}^{\left(k\right)}. \label{EQ83}
\end{align}

To summarize, the MSPRG implements a procedure similar to our proposed coarse graining process in steps (A-C) in the moment space, where $y\left(\cdot\right)$ is chosen as a mapping from an arbitrary correlation matrix to the inverse of covariance matrix of the system under the assumption of translational invariance (i.e., $q=0$). Parameter $n_{k}$ is simply defined as a constant according to application demands.

\subsection{Manifestation in real space phenomenological renormalization group}
Finally, we consider the real space phenomenological renormalization group (RSPRG), a real space counterpart of the MSPRG \cite{meshulam2018coarse,meshulam2019coarse}. Compared with the LRG and the MSPRG, the implementation of RSPRG is rather simple.

Given the system described by Eq. (\ref{EQ80}), one can define $K$ as the Pearson correlation matrix
\begin{align}
K_{ij}=y\left(C_{ij}\right):=\frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}\Sigma_{jj}}}. \label{EQ84}
\end{align}
If correlation matrix $C$ has been derived following the Pearson correlation, then $y\left(\cdot\right)$ is nothing more but an identity mapping. The coarse graining is simply implemented by 
\begin{align}
x_{i}^{\left(k\right)}\rightarrow x_{i^{\prime}}^{\left(k+1\right)}:=x_{i}^{\left(k\right)}+\sum_{j}\delta\left(K_{ij},\max_{r}K_{ir}\right)x_{j}^{\left(k\right)}, \label{EQ85}
\end{align}
where $\delta\left(\cdot,\cdot\right)$ is the Kronecker delta function. In the case where there exists more than one possibilities of $j$ that satisfy $\delta\left(K_{ij},\max_{r}K_{ir}\right)=1$, the RSPRG only chooses one of these possibilities while calculating Eq. (\ref{EQ85}). In other words, parameter $n_{k}$ is set as $n_{k}=2^{-k}\vert V^{\left(0\right)}\vert$ for the $k$-th iteration (e.g., about half of elements are coarse grained in each iteration). 

Therefore, the RSPRG realizes our coarse graining approach in the real space by designing $y\left(\cdot\right)$ as a mapping from an arbitrary correlation matrix to the Pearson correlation matrix and defining $n_{k}=2^{-k}\vert V^{\left(0\right)}\vert$.

\subsection{Non-isomorphic decomposition and renormalization groups}
Let us summarize our analyses presented above. We have proposed an abstract coarse graining approach under the guidance of the non-isomorphic decomposition analysis. As we have suggested, this general framework has concrete manifestations in various existing renormalization groups, such as the LRG \cite{villegas2023laplacian}, the MSPRG \cite{bradde2017pca,lahoche2022generalized}, and the RSPRG \cite{meshulam2018coarse,meshulam2019coarse}. 

Our abstract coarse graining process is valid when the internal correlations inside the investigated system are not weak. Under this condition, each set of $M$ nodes that need to be aggregated together in step (B) can have a small value of the automorphism entropy (e.g., a small infinite cluster automorphism entropy, $A_{\infty}$, or a small value of the first moment of the automorphism entropy spectrum, $\zeta_{A}$). In other words, for this set of nodes (i.e., elements), two extreme probability spaces of non-isomorphic percolation configuration classes, $\mathbb{P}_{\rho}$ and $\mathbb{P}_{\infty}$, are enough similar to each other. Consequently, the actual probability space of non-isomorphic percolation configuration classes, $\mathbb{P}_{\mathsf{ac}}$, must be similar to $\mathbb{P}_{\rho}$ as well (i.e., according to Eq. (\ref{EQ67})). Although in real cases we may never know the precise expression of $\mathbb{P}_{\mathsf{ac}}$, we can treat $\mathbb{P}_{\mathsf{ac}}$ as $\mathbb{P}_{\rho}$ because of the small difference between these two spaces. Therefore, for the concerned set of $M$ elements, we can safely treat them as perfectly and unconditionally coherent to study them as a whole, even though these elements are only correlated in a probabilistic manner. The implied information loss, reflected by the automorphism entropy, is small and acceptable. The way that we analyze these elements as a whole is exactly the abstract coarse graining process. Various renormalization groups that realize this abstract coarse graining approach can help us find the underlying patterns of correlated systems (i.e., potential criticality at the non-trivial fixed point of renormalization flows).

One may be curious about the consequences if we insist to renormalize the correlated system when internal correlations are not sufficiently strong, i.e., we insist to eliminate some freedom degrees in $\mathbb{P}_{\mathsf{ac}}$ to make $\mathbb{P}_{\mathsf{ac}}$ approach to $\mathbb{P}_{\rho}$ even though the informational difference, $H_{\mathsf{ac}}-H_{\rho}$, is not small. In the LRG \cite{villegas2023laplacian}, sufficiently weak internal correlations (i.e., small values of $\vert C\vert$) imply a small trace of the Laplacian $L^{\left(k\right)}$ according to Eq. (\ref{EQ69}). Based on the relation between trace and eigenvalues and the non-negativity of Laplacian eigenvalues, we know that more Laplacian eigenvalues approach to zero. Consequently, we derive a larger value of $n_{k}$ in Eq. (\ref{EQ72}) given a constant $\tau$. In certain extreme cases, we may derive $n_{k}=\vert V^{\left(k\right)}\vert$ such that the coarse graining process has no effect on the system at all (i.e., there is no coarse grained element in each iteration). In the MSPRG \cite{bradde2017pca,lahoche2022generalized}, and the RSPRG \cite{meshulam2018coarse,meshulam2019coarse}, the renormalization flows drive the correlated system with weak internal correlations towards a Gaussian distribution \cite{bradde2017pca,lahoche2022generalized,meshulam2018coarse,meshulam2019coarse}. The emerged Gaussian distribution at the fixed points of the renormalization flows, however, are not equivalent to the Gaussian fixed points on the critical surface when the correlated system is embedded in a space whose dimension is higher than the upper critical dimension \cite{jona2001renormalization}. On the contrary, this Gaussian distribution is the consequence of the central limit theorem because system elements are weakly correlated or independent \cite{bradde2017pca,lahoche2022generalized,meshulam2018coarse,meshulam2019coarse}. In a critical system, strongly correlated elements make the central limit theorem fail, which implies non-Gaussian fixed points.


\section{Conclusion}\label{Sec8}
In this work, we study the dynamics of the information contained in the statistical ensemble of percolation configurations during the percolation process of correlated behaviours. In Secs. \ref{Sec2}-\ref{Sec5}, we develop a framework referred to as the non-isomorphic decomposition analysis to support a quantitative analysis of our concerned question. We discover that the informational difference (i.e., the proposed automorphism entropy and its variants) between $\mathbb{P}_{\rho}$, a space of the extreme cases where the correlated system has strictly constrained element relations, and $\mathbb{P}_{\infty}$, a space of the extreme cases where there exists no constraint on element relations, decreases as the internal correlations inside the system become increasingly strong. On the one hand, this property enables the automorphism entropy and its variants to serve as general indicators of phase transition phenomena in diverse correlated systems, such as the absorbing phase transition in branching models \cite{hinrichsen2000non,lubeck2004universal,henkel2008non}, spreading processes in epidemic models \cite{moreno2002epidemic,newman2002spread}, and
synchronization in oscillator models \cite{rodrigues2016kuramoto,acebron2005kuramoto} shown in Sec. \ref{Sec6}. On the other hand, this property also reduces the informational difference between $\mathbb{P}_{\mathsf{ac}}$, the actual space where the correlated system lives in, and $\mathbb{P}_{\rho}$. In other words, this property suggests the possibility to approximate $\mathbb{P}_{\mathsf{ac}}$ based on $\mathbb{P}_{\rho}$ with small information loss. In Sec. \ref{Sec7}, this possibility is demonstrated as related to renormalization group theories \cite{zinn2007phase,wilson1983renormalization,goldenfeld2018lectures}. We propose an abstract coarse graining approach based on the non-isomorphic decomposition analysis, which features concrete manifestations in diverse renormalization groups, including the Laplacian renormalization group \cite{villegas2023laplacian}, the moment space phenomenological renormalization group \cite{bradde2017pca,lahoche2022generalized}, and the real space phenomenological renormalization group \cite{meshulam2018coarse,meshulam2019coarse}. Our non-isomorphic decomposition analysis is suggested to convey key information about when and how renormalization groups can process correlated systems effectively. In sum, this work suggests a new perspective to uncover the underlying connections between percolation, correlated systems, and renormalization groups.

As a preliminary exploration, our work certainly has limitations. The present version of the non-isomorphic decomposition analysis is rooted in the developed automorphism entropy and lacks other mathematical concepts to enrich its frameworks. The analytic calculation of the automorphism entropy and its variants, as we have suggested, may become highly non-trivial until we apply the mean-field approximation. Therefore, the potential direction for future explorations may be perfecting the details of the non-isomorphic decomposition analysis and developing more concepts built on non-isomorphic decomposition, especially the analytic method to compute the automorphism entropy without the mean-field approximation. Based on these improvements, we may be enabled to uniformly analyze percolation, correlated systems, and renormalization groups in an information-theoretical manner.

 \section*{Acknowledgements}
This project is supported by the Huawei Innovation Research Program (TC20221109044), the Artificial and General Intelligence Research Program of Guo Qiang Research Institute at Tsinghua University (2020GQG1017) as well as the Tsinghua University Initiative Scientific Research Program. 

Authors appreciate Hedong Hou, who studies at the UFR de Math\'{e}matiques, Universit\'{e} de Paris, for the inspiring discussions. Aohua Cheng, who studies at the Tsien Excellence in Engineering Program, Tsinghua University, is appreciated for proof reading. 

% The \nocite command causes all entries in a bibliography to be printed out
% whether or not they are actually referenced in the text. This is appropriate
% for the sample file to show the different styles of references, but authors
% most likely will not want to use it.
% Produces the bibliography via BibTeX.

\appendix
\addtocontents{toc}{\fixappendix}
\section{A practical calculation of the P{\'o}lya enumeration theorem}\label{ASec-1}
In this section, we present a calculation approach of Eq. (\ref{EQ16}) parallel to the generating function method \cite{harary2014graphical,bona2015handbook}. This approach is more friendly to computer programming.

Let us represent each permutation $\omega\in S_{M}$ as a product of disjoint cycles (note that $S_{M}$ denotes the symmetric group). We cluster all permutations according to their cycle types to obtain all conjugate classes. The total number of conjugate classes, $\lambda\left(\cdot\right)$, of $S_{N}$ satisfies
\begin{align}
    \lambda\left(S_{M}\right)=\frac{1}{\vert S_{M}\vert}\sum_{\omega\in S_{M}}\vert O\left(\omega\right)\vert,\label{AEQ1}
\end{align}
where $O\left(\cdot\right)$ denotes the conjugate class. The size of each conjugate class is fully determined by the corresponding cycle type \cite{scott2012group}. Assuming $\omega$ is the product of $k\in\mathbb{N}^{+}$ types of cycles, where the $i$-th ($i\in\{1,\ldots,k\}$) type of cycle has an order (i.e., length) of $l_{i}$ and occurs $m_{i}$ times, we have
\begin{align}
    \vert O\left(\omega\right)\vert=\frac{M!}{\prod_{i=1}^{k}m_{i}!l_{i}^{m_{i}}}.\label{AEQ2}
\end{align}

For each conjugate class $O\left(\omega\right)$, we can analyze the property of $\sigma\left(\omega\right)\in S_{\vert E_{c}\vert}$, the corresponding permutation on edges. 

First, given every cycle of order $l_{i}$ in $\omega$ (we assume the node set involved with this cycle is $V_{i}$), there exist $\lfloor \frac{l_{i}}{2}\rfloor$ associate cycles in $\sigma\left(\omega\right)$ that consist of the edges formed among nodes in $V_{i}$. For instance, if there exists a cycle $\left(12345\right)$ in $\omega$, then there are $\lfloor \frac{5}{2}\rfloor=2$ associate cycles in $\sigma\left(\omega\right)$, i.e., $\left[\left(1,2\right)\left(2,3\right)\left(3,4\right)\left(4,5\right)\right]$ and $\left[\left(1,3\right)\left(2,4\right)\left(3,5\right)\left(4,1\right)\right]$ (note that notion $\left(a,b\right)$ denotes an edge between nodes $a$ and $b$). 

Second, given two cycles of orders $l_{i}$ and $l_{j}$ in $\omega$ (we assume the node sets involved with these two cycles are $V_{i}$ and $V_{j}$), there would be $\frac{l_{i}l_{j}}{\mathsf{lcm}\left(l_{i},l_{j}\right)}$ associate cycles in $\sigma\left(\omega\right)$ that consist of edges between node sets $V_{i}$ and $V_{j}$. Here notion $\mathsf{lcm}\left(\cdot,\cdot\right)$ denotes the least common multiple and $\mathsf{lcm}\left(l_{i},l_{j}\right)$ exactly derives the least common period length of these two cycles.

Based on these two properties analyzed above, we have enumerated all the possible cycles in $\sigma\left(\omega\right)$ that are created by cycles in $\omega$. Therefore, we know
\begin{align}
    c\left[\sigma\left(\omega\right)\right]=\sum_{i=1}^{k}\lfloor \frac{l_{i}}{2}\rfloor+\sum_{j>i\in \{1,\ldots,k\}}\frac{l_{i}l_{j}}{\mathsf{lcm}\left(l_{i},l_{j}\right)}.\label{AEQ3}
\end{align}
We can equivalently calculate Eq. (\ref{EQ16}) as
\begin{align}
\vert \Gamma\left(M\right)\vert=\frac{1}{\sum_{O\left(\omega\right)\in \mathbf{O}_{M}} \vert O\left(\omega\right)\vert}\sum_{O\left(\omega\right)\in \mathbf{O}_{M}} \vert O\left(\omega\right)\vert 2^{c\left[\sigma\left(\omega\right)\right]}, \label{AEQ4}
\end{align}
where $\mathbf{O}_{M}$ is the set of all conjugate classes of group $S_{M}$. The elements of $\mathbf{O}_{M}$ can be enumerated in a manner of the integer partition of $M$.

\section{Computation of automorphism entropy}\label{ASec-2}
In this section, we elaborate the details of calculating the non-isomorphic decomposition analysis in Python. We primarily focus on computing the automorphism entropy as a function of $M$. 

We first enumerate all possible conjugate classes of symmetric group $S_{N}$ to derive $\mathbf{O}_{N}$. The code implementation is developed as
 \begin{lstlisting}[language=Python]
import numpy as np
import scipy.special

def ConjugateClasses(M):
    # This function enumerate all possible conjugate classes of group S_M
    # CC is the array of conjugate classes. 
    # For instance, CC= [[4], [3, 1], [2, 2], [2, 1, 1], [1, 1, 1, 1]] given group S_4 
    # [4] denotes a conjugate class with a form (****), where each element is 1 cycle of length 4
    # [3,1] denotes a conjugate class with a form (***)(*), where each element is the product of 1 cycle of length 3 and 1 cycle of length 1
    # [2, 2] denotes a conjugate class with a form (**)(**), where each element is the product of 2 cycles of length 2
    # [2, 1, 1] denotes a conjugate class with a form (**)(*)(*), where each element is the product of 1 ccle of length 2 and 2 cycles of length 1
    Sol = Solution()
    CC = Sol.UniquePartitions(M)
    return CC

\end{lstlisting}
The enumeration is equivalent to integer partition in modern number theory, which can be derived by
 \begin{lstlisting}[language=Python]
class Solution:
    # This function enumerate all possible partitions of a given integer
    def __init__(self):
        self.temp = []

    def Solve(self, A, V, ID, S, N):
        if S == N:
            V.append(self.temp.copy())
            return
        if ID < 0:
            return
        self.Solve(A, V, ID-1, S, N)
        if S < N:
            self.temp.append(A[ID])
            self.Solve(A, V, ID, S+A[ID], N)
            self.temp.pop()
 
    def UniquePartitions(self, N):
        A = [ID for ID in range(1, N+1)]
        V = []
        self.Solve(A, V, N-1, 0, N)
        V.reverse()
        return V
\end{lstlisting}
Given each conjugate class of symmetric group $S_{M}$, we can calculate its size based on the following code.
 \begin{lstlisting}[language=Python]
def Size_ConjugateClasses(CC,M):
    # This function calculates the size of each conjugate class of group S_M
    # Each element in CCSize is the size of the corresponding conjugate class in CC
    CCSize=np.zeros(len(CC))
    for ID in range(len(CC)):
        CClass=np.array(CC[ID])
        Uni,Count=np.unique(CClass, return_counts=True)
        Denominator=np.prod(scipy.special.factorial(Count)*np.power(Uni,Count))
        CCSize[ID]=np.math.factorial(M)/Denominator
    return CCSize
\end{lstlisting}
Given each conjugate class $O\left(\omega\right)$, we can count the number of all the possible cycles in $\sigma\left(\omega\right)$ that are created by cycles in $\omega$ 
 \begin{lstlisting}[language=Python]
def Num_Cycles(CC):
    # This function calculates the number of cycles in the permutation \sigma(\omega)
    # Each element in CNum is the number of cycles in the corresponding conjugate class in CC
    CNum=np.zeros(len(CC))
    for ID in range(len(CC)):
        CClass=np.array(CC[ID])
        Term1=np.sum(np.floor(CClass/2))
        Term2=0
        for ID1 in range(len(CClass)):
            for ID2 in range(ID1+1,len(CClass)):
                Term2=Term2+np.gcd(CClass[ID1], CClass[ID2])
        CNum[ID]=Term1+Term2
    return CNum
\end{lstlisting}
Based on these functions, we can calculate $\vert\Gamma\left(M\right)\vert$ with the following code.
 \begin{lstlisting}[language=Python]
def Gamma_M(CCSize,CNum):
    ## This function calculates |Gamma(M)|
    PartitionF=np.sum(CCSize)
    Numerator=np.sum(CCSize*np.power(2,CNum))
    GM=Numerator/PartitionF
    return GM
\end{lstlisting}
Meanwhile, probability measure $P_{\infty}\left(\cdot\right)$ can be readily calculated by the following function.
 \begin{lstlisting}[language=Python]
def Prob_Inf(M):
    ## This function calculates the probability measure P_inf 
    CC=ConjugateClasses(M)
    CCSize=Size_ConjugateClasses(CC,M)
    CNum=Num_Cycles(CC)
    GM=Gamma_M(CCSize,CNum)
    ProbI=1/GM
    return GM,ProbI
\end{lstlisting}

After obtaining $\vert\Gamma\left(M\right)\vert$, we turn to deriving $\vert\Gamma\left(M\right)\vert$ based on the inverse Euler transform. In our code implementation, several necessary functions are proposed to support the transform. For a given integer, we can enumerate all its divisors
 \begin{lstlisting}[language=Python]
def AllDivisor(N):
    # This function enumerate all possible divisors of a given integer
    Divisors = set()
    for ID in range(1, int(N**0.5)+1):
        if N % ID == 0:
            Divisors.add(ID)
            Divisors.add(N//ID)
    return list(Divisors)
\end{lstlisting}
and determine if it is a prime number.
 \begin{lstlisting}[language=Python]
def isPrime(N) :
    # This function determines if a given integer is a prime
    if (N < 2) :
        return False
    for ID in range(2, N + 1) :
        if (ID  * ID  <= N and N % ID == 0) :
            return False
    return True
\end{lstlisting}
Based on these two functions, we can computationally implement the M\"obius function.
 \begin{lstlisting}[language=Python]
def Mobius(N) :
     # This function determines if a given integer is a prime
    if (N == 1) :
        return 1
    P = 0
    for ID in range(1, N + 1) :
        if (N % ID == 0 and
                isPrime(ID)) :
            if (N % (ID * ID) == 0) :
                return 0
            else :
                P = P + 1
    if(P % 2 != 0) :
        return -1
    else :
        return 1
\end{lstlisting}
These functions enable us to define the inverse Euler transform of the sequence of $\{\vert\Gamma\left(1\right)\vert,\vert\Gamma\left(2\right)\vert,\vert\Gamma\left(3\right)\vert,\ldots\}$ to derive $\{\vert\Gamma^{*}\left(1\right)\vert,\vert\Gamma^{*}\left(2\right)\vert,\vert\Gamma^{*}\left(3\right)\vert,\ldots\}$.
 \begin{lstlisting}[language=Python]
def Prob_Rho(M):
    ## This function calculates the probability measure P_rho 
    ## GMVec is the sequence of |Gamma(0)|, |Gamma(1)|, |Gamma(2)|, ......, |Gamma(M)|
    ## ProbIVec is the sequence of P_inf(0), P_inf(1), P_inf(2), ......, P_inf(M)
    ## GStarMVec is the sequence of |Gamma*(0)|, |Gamma*(1)|, |Gamma*(2)|, ......, |Gamma*(M)|
    ## ProbRhoVec is the sequence of P_rho (0), P_rho (1), P_rho (2), ......, P_rho(M)
    ## LambdaVec is the sequence of Lambda(1), Lambda(2), ......, Lambda(M)
    GMVec=np.zeros(M+1)
    ProbIVec=np.zeros(M+1)
    for ID in range(M+1):
        print('Processing P_inf: %f' % (ID/(M+1)))
        GM,ProbI=Prob_Inf(ID)
        GMVec[ID]=GM
        ProbIVec[ID]=ProbI
    
    LambdaVec=np.zeros(M+1)
    LambdaVec[1]=GMVec[1]
    for ID in range(2,len(LambdaVec)):
        print('Processing Lambda: %f' % (ID/(M+1)))
        SumTerm=0
        for ID2 in range(1,ID):
            SumTerm=SumTerm+LambdaVec[ID2]*GMVec[ID-ID2]
        LambdaVec[ID]=ID*GMVec[ID]-SumTerm
    
    GStarMVec=np.zeros(M+1)
    ProbRhoVec=np.zeros(M+1)
    GStarMVec[0]=GMVec[0]
    ProbRhoVec[0]=ProbIVec[0]
    for ID in range(1,M+1):
        print('Processing P_rho: %f' % (ID/(M+1)))
        Divisors=AllDivisor(ID)
        MobiusResult=0
        for Divisor in Divisors:
            MobiusResult=MobiusResult+Mobius(int(ID/Divisor))*LambdaVec[Divisor]
        GStarMVec[ID]=MobiusResult/ID
        ProbRhoVec[ID]=ID/MobiusResult
    return GMVec,ProbIVec,GStarMVec,ProbRhoVec,LambdaVec
\end{lstlisting}

Finally, we calculate the automorphism entropy as a function of $i\in\{0,\dots,M\}$. 
 \begin{lstlisting}[language=Python]
def AutomorphismEntropyFunction(M):
    ## GMVec is the sequence of |Gamma(0)|, |Gamma(1)|, |Gamma(2)|, ......, |Gamma(M)|
    ## ProbIVec is the sequence of P_inf(0), P_inf(1), P_inf(2), ......, P_inf(M)
    ## GStarMVec is the sequence of |Gamma*(0)|, |Gamma*(1)|, |Gamma*(2)|, ......, |Gamma*(M)|
    ## ProbRhoVec is the sequence of P_rho (0), P_rho (1), P_rho (2), ......, P_rho(M)
    ## LambdaVec is the sequence of Lambda(1), Lambda(2), ......, Lambda(M)
    ## SEIVec is the sequence of Shannon entropy H_inf(0), H_inf(1), H_inf(2), ......, H_inf(M)
    ## SERhoVec is the sequence of Shannon entropy H_rho(0), H_rho(1), H_rho(2), ......, H_rho(M)
    ## AEVector is the sequence of Automorphism entropy A(0), A(1), A(2), ......, A(M)
    ## Please note that we define A(0)=A(1)=1 in this work, yet other definitions are acceptable
    GMVec,ProbIVec,GStarMVec,ProbRhoVec,LambdaVec=Prob_Rho(M)
    SEIVec=ShannonEntropyFunction(ProbIVec)
    SERhoVec=ShannonEntropyFunction(ProbRhoVec)
    AEVector=1-SERhoVec/SEIVec
    AEVector[0]=1
    if len(AEVector)>1:
        AEVector[1]=1
    return GMVec,ProbIVec,GStarMVec,ProbRhoVec,LambdaVec,SEIVec,SERhoVec,AEVector
\end{lstlisting}
The Shannon entropy terms used in the above function are calculated based on the following codes.
 \begin{lstlisting}[language=Python]
def ShannonEntropy(P):
    ## This function calculates the Shannon entropy of a given probability measure
    S=1/P*(-1*P*np.log(P))
    return S

def ShannonEntropyFunction(ProbVec):
    ## This function calculates the Shannon entropy as a function of M
    SEVec=np.zeros(len(ProbVec))
    for ID in range(len(ProbVec)):
        SEVec[ID]=ShannonEntropy(ProbVec[ID])
    return SEVec
\end{lstlisting}

To run the above program in a unified manner, one can consider the following instance where $M=100$.
 \begin{lstlisting}[language=Python]
M=100 
GNVec,ProbIVec,GStarNVec,ProbRhoVec,LambdaVec,SEIVec,SERhoVec,AEVector=AutomorphismEntropyFunction(M)
\end{lstlisting}

\section{Empirical estimation of automorphism entropy}\label{ASec-3}
Based on the estimated coefficients of exponential decay in Eq. (\ref{EQ25}), we can estimate the automorphism entropy based on the following code.
 \begin{lstlisting}[language=Python]
def AutomorphismEntropyEstimation(M):
    ## This function estimates the exponential decay of A(M)
    ## AE is the estimated value of A(M)
    AE=3.956*np.exp(-0.6876*M)
    if M<2:
        AE=1
    return AE
\end{lstlisting}

\section{Computation of the Bernoulli percolation}\label{ASec-4}
In this section, we propose the Python functions for implementing the Bernoulli percolation.

Let us begin with network generation and dilution. Given a target network size and an occupation probability $\rho$, we first generate a complete graph of the corresponding size and randomly delete each edge according to a probability of $1-\rho$.
 \begin{lstlisting}[language=Python]
import numpy as np
import networkx as nx

def NetworkGeneration(Size,rho):
    ## Size is the size of the initial network
    ## rho is the occupation probability
    ## AMatrix is the adjacency matrix
    ## UniDeg is the vector of unique values of node degree
    ## ProbDegree is the vector of the probability of each unique value of node degree
    ## MeanDeg is the average node degree
    G = nx.complete_graph(Size)
    AMatrix=nx.to_numpy_matrix(G)
    for ID in range(Size):
        for ID1 in range(ID,Size):
            AMatrix[ID,ID1]=np.random.binomial(1,rho,1)
            AMatrix[ID1,ID]=AMatrix[ID,ID1]
    DegreeV=np.sum(np.asarray(AMatrix), axis=0)
    UniDeg, Freq=np.unique(DegreeV, return_counts=True)
    ProbDegree=Freq/Size
    MeanDeg=np.mean(DegreeV)
    return UniDeg,ProbDegree,MeanDeg
\end{lstlisting}

Then we calculate the order parameter $\rho_{n}$ of the Bernoulli percolation as a function of $\rho$.
 \begin{lstlisting}[language=Python]
def BernoulliPercolationProcess(Size):
    ## Size is the size of the initial network
    ## Rho_eVector is the value of Rho_e defined as a function of rho=0,0.01,0.02,......1
    ## Rho_nVector is the value of Rho_n defined as a function of rho=0,0.01,0.02,......1 
    ## Note that Rho_n is the order parameter of the Bernoulli percolation
    rhoVector=np.arange(0.005,1.005,0.005)
    Rho_eVector=np.zeros(len(rhoVector))
    Rho_nVector=np.zeros(len(rhoVector))
    for ID in range(len(rhoVector)):
        rho=rhoVector[ID]
        UniDeg,ProbDegree,MeanDeg=NetworkGeneration(Size,rho)
        print('Processing Percolation: %f' % (ID/len(rhoVector)))
        Rho_e_Vec=np.arange(0.005,1.005,0.005)
        Rho_e_Eq=np.zeros(len(rhoVector))
        for ID1 in range(len(Rho_e_Vec)):
            PRho_e=Rho_e_Vec[ID1]
            Rho_e_Eq[ID1]=1-np.sum(ProbDegree*UniDeg/MeanDeg*np.power(1-rho*PRho_e,UniDeg-1))
        Rho_e=Rho_e_Vec[np.argmin(np.abs(Rho_e_Eq-Rho_e_Vec))]
        Rho_n=1-np.sum(ProbDegree*np.power(1-rho*Rho_e,UniDeg))
        Rho_eVector[ID]=Rho_e
        Rho_nVector[ID]=Rho_n
    return Rho_eVector,Rho_nVector
\end{lstlisting}

A sample instance of the Bernoulli percolation process in a network with $1000$ nodes can be defined by the following code.
\begin{lstlisting}[language=Python]
Size=1000
Rho_eVector,Rho_nVector=BernoulliPercolationProcess(Size)
\end{lstlisting}

\section{The probability for a node to belong to a cluster}\label{ASec-5}

In this section, we present the Python function to calculate $\pi_{r}\left(\rho\right)$, the probability for a randomly selected node to belong to a cluster whose size is $r$. The function is defined as the following.
\begin{lstlisting}[language=Python]
def PiDistribution(Size,rho,Type):
    ## Size is the size of the initial network
    ## rho is the occupation probability
    ## Type=0 or Type=1 indicates the method used to calculate the pi distribution
    ## Type=0 use the method in Eq.(31)
    ## Type=1 use the method in Eq.(34)
    ## PiProbSize is a vector of probability for a node to belong to a cluster of a given size
    ## The i-th element in ProbSize is the probability for a node to belong to a cluster whose size is i
    if Type==0:
        ClusterSize=np.arange(1,Size,1)
        PiProbSize=np.zeros(len(ClusterSize))
        for ID in range(len(ClusterSize)):
            CSize=ClusterSize[ID]
            if CSize==1:
                PiProbSize[ID]=np.power(1-rho,Size-1)
            elif CSize==2:
                PiProbSize[ID]=rho*(Size-1)*np.power(1-rho,2*Size-4)
            elif CSize>2:
                Term1=np.power(rho,CSize-1)*(Size-1)/scipy.special.factorial(CSize-1)
                Term2=np.prod(CSize*(Size-2)-np.arange(0,CSize-2,1))
                Term3=np.power(1-rho,CSize*(Size-3)+2)
                PiProbSize[ID]=Term1*Term2*Term3
    elif Type==1:
        ClusterSize=np.arange(1,Size,1)
        PiProbSize=np.zeros(len(ClusterSize))
        for ID in range(len(ClusterSize)):
            CSize=ClusterSize[ID]
            Mean=(Size-1)*rho
            PiProbSize[ID]=1/scipy.special.factorial(CSize)*np.exp(-1*Mean*CSize)*np.power(Mean*CSize,CSize-1)
    PiProbSize=np.nan_to_num(PiProbSize, nan=0, posinf=0)
    return PiProbSize
\end{lstlisting}

\section{Computation of automorphism entropy spectrum}\label{ASec-6}
In this section, we elaborate the details of calculating the automorphism entropy spectrum.

We begin with deriving the probability distribution of cluster size. According to Eq. (\ref{EQ35}), the derivation is equivalent to solving the following linear equations
\begin{align}
\left[
\begin{matrix}
\pi_{1}\left(\rho\right)-1 & 2\pi_{1}\left(\rho\right) & \ldots & \vert V\vert\pi_{1}\left(\rho\right)\\
\pi_{2}\left(\rho\right) & 2\pi_{2}\left(\rho\right)-2 & \ldots & \vert V\vert\pi_{2}\left(\rho\right)\\
\vdots & \vdots & \vdots & \vdots \\
\pi_{\vert V\vert}\left(\rho\right) & 2\pi_{\vert V\vert}\left(\rho\right) & \ldots & \vert V\vert\pi_{\vert V\vert}\left(\rho\right)-\vert V\vert
\end{matrix}
\right]
\left[
\begin{matrix}
\theta_{1}\left(\rho\right)\\ \theta_{2}\left(\rho\right)\\
\vdots\\
\theta_{\vert V\vert}\left(\rho\right)
\end{matrix}
\right]=\left[
\begin{matrix}
0\\ 0\\
\vdots\\
0
\end{matrix}
\right],\label{AEQ5}
\end{align}
which are subject to $\sum_{r}\theta_{r}\left(\rho\right)=1$. Unfortunately, the first matrix in Eq. (\ref{AEQ5}) is not row full rank (i.e., one can readily find linear correlations among rows after adding the first row with all the rest rows). To find a non-trivial solution, we need to replace the first rows of the first and the third matrices in Eq. (\ref{AEQ5}) by the following rows
\begin{align}
\left[
\begin{matrix}
1 & 1 & \ldots &  1\\
\pi_{2}\left(\rho\right) & 2\pi_{2}\left(\rho\right)-2 & \ldots & \vert V\vert\pi_{2}\left(\rho\right)\\
\vdots & \vdots & \vdots & \vdots \\
\pi_{\vert V\vert}\left(\rho\right) & 2\pi_{\vert V\vert}\left(\rho\right) & \ldots & \vert V\vert\pi_{\vert V\vert}\left(\rho\right)-\vert V\vert
\end{matrix}
\right]
\left[
\begin{matrix}
\theta_{1}\left(\rho\right)\\ \theta_{2}\left(\rho\right)\\
\vdots\\
\theta_{\vert V\vert}\left(\rho\right)
\end{matrix}
\right]=\left[
\begin{matrix}
1\\ 0\\
\vdots\\
0
\end{matrix}
\right].\label{AEQ6}
\end{align}
Solving Eq. (\ref{AEQ6}), we can derive
\begin{align}
\theta_{1}\left(\rho\right)&=\frac{1}{\sum_{r}\frac{\pi_{r}\left(\rho\right)}{r\pi_{1}\left(\rho\right)}},\label{AEQ7}\\&=\left(1+\sum_{k=2}^{\vert V\vert}\frac{\exp\left(\rho k-\rho\vert V\vert k\right)\left(\rho\vert V\vert k-\rho k\right)^{k-1}}{k^2\left(k-1\right)!\left(1-\rho\right)^{\vert V\vert-1}}\right)^{-1},\label{AEQ8}
\end{align}
and 
\begin{align}
\theta_{r}\left(\rho\right)&=\frac{\frac{\pi_{r}\left(\rho\right)}{r\pi_{1}\left(\rho\right)}}{\sum_{r}\frac{\pi_{r}\left(\rho\right)}{r\pi_{1}\left(\rho\right)}},\label{AEQ9}\\&=\frac{\exp\left(\rho r-\rho\vert V\vert r\right)\left(\rho\vert V\vert r-\rho r\right)^{r-1}}{r^2\left(r-1\right)!\left(1-\rho\right)^{\vert V\vert-1}}\left(1+\sum_{k=2}^{\vert V\vert}\frac{\exp\left(\rho k-\rho\vert V\vert k\right)\left(\rho\vert V\vert k-\rho k\right)^{k-1}}{k^2\left(k-1\right)!\left(1-\rho\right)^{\vert V\vert-1}}\right)^{-1},\label{AEQ10}
\end{align}
where $r\geq2$ in Eq. (\ref{AEQ10}). For convenience, we use the Kronecker delta $\delta\left(\cdot,\cdot\right)$ to reorganize Eqs. (\ref{AEQ7}-\ref{AEQ10}) as
\begin{align}
&\theta_{r}\left(\rho\right)\notag\\=&\left(\frac{\exp\left(\rho r-\rho\vert V\vert r\right)\left(\rho\vert V\vert r-\rho r\right)^{r-1}}{r^2\left(r-1\right)!\left(1-\rho\right)^{\vert V\vert-1}}\right)^{1-\delta\left(1,r\right)}\left(1+\sum_{k=2}^{\vert V\vert}\frac{\exp\left(\rho k-\rho\vert V\vert k\right)\left(\rho\vert V\vert k-\rho k\right)^{k-1}}{k^2\left(k-1\right)!\left(1-\rho\right)^{\vert V\vert-1}}\right)^{-1},\label{AEQ11}
\end{align}
where $r\geq 1$. 

Based on Eq. (\ref{AEQ11}), a Python function can be proposed to calculate the cluster size distribution.
\begin{lstlisting}[language=Python]
def ThetaDistribution(Size,rho):
    ## Size is the size of the initial network
    ## rho is the occupation probability
    ## ThetaProbSize is a vector of probability for a cluster with a given size to occur in the network (i.e., the cluster size distribution)
    ## The i-th element in ThetaSize is the probability for a cluster with a size of i to occur

    ClusterSize=np.arange(1,Size,1)
    ThetaProbSize=np.zeros(len(ClusterSize))
    for ID in range(len(ClusterSize)):
        CSize=ClusterSize[ID]
        if CSize==1:
            Term0=np.exp(rho*np.arange(2,Size,1)-rho*Size*np.arange(2,Size,1))*np.power((rho*Size*np.arange(2,Size,1)-rho*np.arange(2,Size,1)),np.arange(1,Size-1,1))
            Term1=np.power(np.arange(2,Size,1),2)*scipy.special.factorial(np.arange(1,Size-1,1))*np.power(1-rho,Size-1)
            ThetaProbSize[ID]=1/(1+np.sum(Term0/Term1))
        elif CSize>1:
            Term0=np.exp(rho*np.arange(2,Size,1)-rho*Size*np.arange(2,Size,1))*np.power((rho*Size*np.arange(2,Size,1)-rho*np.arange(2,Size,1)),np.arange(1,Size-1,1))
            Term1=np.power(np.arange(2,Size,1),2)*scipy.special.factorial(np.arange(1,Size-1,1))*np.power(1-rho,Size-1)
            Term2=np.exp(rho*CSize-rho*Size*CSize)*np.power((rho*Size*CSize-rho*CSize),CSize-1)
            Term3=np.power(CSize,2)*scipy.special.factorial(CSize-1)*np.power(1-rho,Size-1)
            ThetaProbSize[ID]=Term2/Term3*1/(1+np.sum(Term0/Term1))
        ThetaProbSize=np.nan_to_num(ThetaProbSize, nan=0, posinf=0)
    return ThetaProbSize
\end{lstlisting}

Finally, we can calculate the automorphism entropy spectrum based on Eqs. (\ref{EQ37}-\ref{EQ38}). A Python function is proposed to realize this calculation.
\begin{lstlisting}[language=Python]
def AutomorphismEntropySpectrum(Size,rho,Type):
    ## Size is the size of the initial network
    ## rho is the occupation probability
    ## Type=0 or Type=1 indicates the method used to calculate the pi distribution
    ## Type=0 use the method in Eq.(31)
    ## Type=1 use the method in Eq.(34)
    ## AEValue is a vector of automorphism entropies calculated in all possible clusters (i.e., clusters whose sizes are 1,.....,Size)
    ## AEProbability is a vector of the probability of each automorphism entropy 
    PiProbSize=PiDistribution(Size,rho,Type)
    ThetaProbSize=ThetaDistribution(Size,rho)
    ClusterSize=np.arange(1,Size,1)
    AEValue=np.zeros(len(ClusterSize))
    for ID in range(len(ClusterSize)):
        CSize=ClusterSize[ID]
        AEValue[ID]=AutomorphismEntropyEstimation(CSize)
    ## Note that AEValue[1]=AEValue[2]=1 (i.e., the automorphism entropy satisfies A(1)=A(2)=1)
    ## Therefore, we need to deal with AEValue[1:2] and AEProbability[1:2] to ensure that the automorphism entropy spectrum is a probability distribution
    AEValue=np.delete(AEValue, 0)
    AEProbability=np.delete(ThetaProbSize, 0)
    AEProbability[1]=ThetaProbSize[1]+ThetaProbSize[0]
    return PiProbSize,ThetaProbSize,AEValue,AEProbability
\end{lstlisting}

\section{Computation of directed percolation in branching model}\label{ASec-7}

In our experiment, a branching model with $100$ elements and $p\in\left[0,0.4\right]$ is proposed. Given each value of $p\in\left[0,0.4\right]$, the experiment runs for $10^{6}$ time steps to generate the data. Under each condition of $p$, we can implement an independent analysis of the power-law behaviours, scaling
relation, and universal collapse shape to verify if Eqs. (\ref{EQ40}-\ref{EQ45}) hold. 

\begin{figure}[t!]
\includegraphics[width=0.7\columnwidth]{AG1.jpg}
 %\captionsetup{justification=raggedright}
\caption{\label{AG1} The power-law behaviours, scaling
relation, and universal collapse shape analyzed under different conditions of $p$. (a) shows the plane of $\left(\frac{\alpha-1}{\beta-1},\gamma\right)$, where each data point corresponds to a specific value of $p$. The inserted sub-plot visualize the absolute difference between $\frac{\alpha-1}{\beta-1}$ and $\gamma$ as a function of $p$, where a low-difference interval (i.e., $\vert\gamma-\frac{\alpha-1}{\beta-1}\vert\leq 0.1$) is marked by a blue region. (b) illustrates $\vert\gamma-1-\psi\vert$ and $\vert\frac{\alpha-1}{\beta-1}-1-\psi\vert$ as the functions of $p$.} 
 \end{figure}

In Fig. \ref{AG1}, we first verify if the scaling relation in Eq. (\ref{EQ43}) holds. We illustrate the plane of $\left(\frac{\alpha-1}{\beta-1},\gamma\right)$ to see whether $\left(\frac{\alpha-1}{\beta-1},\gamma\right)$ is close to the line of $y=x$. As shown in our result, although an increasing value of $p\in\left[0,0.4\right]$ makes $\left(\frac{\alpha-1}{\beta-1},\gamma\right)$ approach to the line of $y=x$, it is hard to find a piece of strict evidence to demonstrate $p=0.25$ as the critical point. Meanwhile, we also measure $\vert\gamma-1-\psi\vert$ and $\vert\frac{\alpha-1}{\beta-1}-1-\psi\vert$ as the functions of $p$. When the power-law behaviours, scaling
relation, and universal collapse shape of the system are strictly satisfied, we expect to see small values of these functions. As shown in Fig. \ref{AG1}, although the increase of $p$ reduces $\vert\gamma-1-\psi\vert$ and $\vert\frac{\alpha-1}{\beta-1}-1-\psi\vert$ gradually, data noise significantly fuzzifies the variation trends of these functions. Consequently, one may not be able to precisely confirm the critical point based on Fig. \ref{AG1}, especially when the \emph{a priori} knowledge about the branching model is limited.

%\nocite{*}
\bibliographystyle{iopart-num}
\bibliography{Ref}
\end{document}

