\makeatletter
\makeatother

\SetKwComment{Comment}{/* }{ */}

\SetArgSty{textnormal}

\begin{algorithm}
    \small
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwInput{KwInput}{Given}
    \KwInput{
    \begin{itemize}
        \item Max iteration number $I$
        \item Number of evaluations per solution $E$
        \item Sample size $N$
        \item MAP-Elites repertoire $\mathbb{M}$
        \item Replay Buffer $\mathbb{B}$
        \item A critic network $Q_v$
    \end{itemize}
    }
    
    \texttt{\\}
    \tcp{Initialization}
    Create $N$ random policies $\{\pi_{\theta_{i}}\}_{i=\{1,N\}}$\;
    Evaluate and insert them in $\mathbb{M}$
    
    \texttt{\\}
    \tcp{Main loop}
    $iteration\_number \xleftarrow{} 0$\;
    \While{$iteration\_number < I$}{
        \texttt{\\}
        \tcp{Sampling and mutation}
        Sample N policies $\{\pi_{\theta_i}\}_{i = 1,N}$ in repertoire $\mathbb{M}$\;
        Mutate half the policies using the TD3 update \cite{fujimoto2018addressing} using $Q_v$\;
        Mutate the other half with random genetic mutations\;

        \texttt{\\}
        \tcp{Train the critic}
        Sample batches of transitions in replay buffer $\mathbb{B}$\;
        Update the critic $Q_v$\ using TD3 \cite{fujimoto2018addressing}
            
        \texttt{\\}
        \tcp{Evaluation}
        Evaluate each new policy over $E$ trajectories and store all transitions in buffer $\mathbb{B}$\;
        Compute each policy's fitness as its avg. fitness over the $E$ trajectories\;
        Compute each policy's BD as its most frequent BD over the $E$ trajectories
        
        \texttt{\\}
        \tcp{Insertion in repertoire}
        For each new policy, insert it in $\mathbb{M}$ only if its fitness is higher and its spread is lower than the corresponding policy in $\mathbb{M}$

        \texttt{\\}
        $iteration\_number = iteration\_number + 1$
    }

    \caption{PGA-MAP-Elites Low-Spread}
    \label{alg:PGAME-LS}
\end{algorithm}