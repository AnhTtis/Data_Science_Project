
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.



@inproceedings{cully2019autonomous,
  title={Autonomous skill discovery with quality-diversity and unsupervised descriptors},
  author={Cully, Antoine},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={81--89},
  year={2019}
}
@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}
@inproceedings{peters2007reinforcement,
  title={Reinforcement learning by reward-weighted regression for operational space control},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={745--750},
  year={2007}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{gae,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@article{shi2020efficient,
  title={Efficient Novelty Search Through Deep Reinforcement Learning},
  author={Shi, Longxiang and Li, Shijian and Zheng, Qian and Yao, Min and Pan, Gang},
  journal={IEEE Access},
  volume={8},
  pages={128809--128818},
  year={2020},
  publisher={IEEE}
}

@article{khadka2019collaborative,
  title={Collaborative evolutionary reinforcement learning},
  author={Khadka, Shauharda and Majumdar, Somdeb and Miret, Santiago and Tumer, Evren and Nassar, Tarek and Dwiel, Zach and Liu, Yinyin and Tumer, Kagan},
  journal={arXiv preprint arXiv:1905.00976},
  year={2019}
}

@inproceedings{khadka2018evolutionaryNIPS,
  title={Evolution-Guided Policy Gradient in Reinforcement Learning},
  author={Khadka, Shauharda and Tumer, Kagan},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

@article{stanton2016curiosity,
  title={Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime},
  author={Stanton, Christopher and Clune, Jeff},
  journal={PloS one},
  volume={11},
  number={9},
  pages={e0162235},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}


@inproceedings{doncieux2019novelty,
  title={Novelty search: a theoretical perspective},
  author={Doncieux, Stephane and Laflaqui{\`e}re, Alban and Coninx, Alexandre},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={99--106},
  year={2019}
}

@article{lehman2011abandoning,
  title={Abandoning objectives: Evolution through the search for novelty alone},
  author={Lehman, Joel and Stanley, Kenneth O},
  journal={Evolutionary computation},
  volume={19},
  number={2},
  pages={189--223},
  year={2011},
  publisher={MIT Press}
}

@article{cully2015robots,
  title={Robots that can adapt like animals},
  author={Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  journal={Nature},
  volume={521},
  number={7553},
  pages={503--507},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{koos2012transferability,
  title={The transferability approach: Crossing the reality gap in evolutionary robotics},
  author={Koos, Sylvain and Mouret, Jean-Baptiste and Doncieux, St{\'e}phane},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={17},
  number={1},
  pages={122--145},
  year={2012},
  publisher={IEEE}
}

@article{mouret2015illuminating,
  title={Illuminating search spaces by mapping elites},
  author={Mouret, Jean-Baptiste and Clune, Jeff},
  journal={arXiv preprint arXiv:1504.04909},
  year={2015}
}

@article{cully2017quality,
  title={Quality and diversity optimization: A unifying modular framework},
  author={Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={22},
  number={2},
  pages={245--259},
  year={2017},
  publisher={IEEE}
}

@article{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={Proceedings of the 30th International Conference in Machine Learning},
  year={2014}
}

@inproceedings{colas2020scaling,
  title={Scaling MAP-Elites to deep neuroevolution},
  author={Colas, C{\'e}dric and Madhavan, Vashisht and Huizinga, Joost and Clune, Jeff},
  booktitle={Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  pages={67--75},
  year={2020}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{salimans2017evolution,
  title={Evolution strategies as a scalable alternative to reinforcement learning},
  author={Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1703.03864},
  year={2017}
}

@inproceedings{conti2018improving,
  title={Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents},
  author={Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={5027--5038},
  year={2018}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@article{haarnoja2018soft,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{frans2017meta,
  title={Meta learning shared hierarchies},
  author={Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  journal={Proc. of ICLR},
  year={2018}
}

@article{paolo2019unsupervised,
  title={Unsupervised Learning and Exploration of Reachable Outcome Space},
  author={Paolo, Giuseppe and Laflaquiere, Alban and Coninx, Alexandre and Doncieux, Stephane},
  journal={algorithms},
  volume={24},
  pages={25},
  year={2019}
}
@article{pere2018unsupervised,
  title={Unsupervised learning of goal spaces for intrinsically motivated goal exploration},
  author={P{\'e}r{\'e}, Alexandre and Forestier, S{\'e}bastien and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1803.00781},
  year={2018}
}

@article{pong2019skew,
  title={Skew-fit: State-covering self-supervised reinforcement learning},
  author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.03698},
  year={2019}
}
@article{lee2019efficient,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019}
}
@article{islam2019marginalized,
  title={Marginalized State Distribution Entropy Regularization in Policy Optimization},
  author={Islam, Riashat and Ahmed, Zafarali and Precup, Doina},
  journal={arXiv preprint arXiv:1912.05128},
  year={2019}
}

@article{badia2020agent57,
  title={Agent57: Outperforming the Atari Human Benchmark},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  journal={arXiv preprint arXiv:2003.13350},
  year={2020}
}
@ARTICLE{deboer05tutorial,
  author = {De Boer, P-T. and Kroese, D.P and Mannor, S. and Rubinstein, R.Y.},
  title = {A Tutorial on the Cross-Entropy Method},
  journal = {Annals of Operations Research},
  year = {2005},
  volume = {134},
  number = {1},
  pages = {19--67},
  file = {:/home/stulp/docs/bibliography/robotics/adaptive-exploration/Boer, Kroese, Mannor, Rubinstein - A Tutorial on the Cross Entropy Method.pdf:PDF},
  keywords = {tutorial, exploration},
}
@article{silver2017mastering,
  title={Mastering the game of Go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Research}
}
@article{parascandolo2020divide,
  title={Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning},
  author={Parascandolo, Giambattista and Buesing, Lars and Merel, Josh and Hasenclever, Leonard and Aslanides, John and Hamrick, Jessica B and Heess, Nicolas and Neitz, Alexander and Weber, Theophane},
  journal={arXiv preprint arXiv:2004.11410},
  year={2020}
}
@article{andersen2018learning,
  title={Learning High-level Representations from Demonstrations},
  author={Andersen, Garrett and Vrancx, Peter and Bou-Ammar, Haitham},
  journal={arXiv preprint arXiv:1802.06604},
  year={2018}
}
@article{eysenbach2018diversity,
  title={Diversity is All You Need: Learning Skills without a Reward Function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018}
}
@article{jaderberg2017population,
  title={Population-based training of neural networks},
  author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1711.09846},
  year={2017}
}
@article{matheron2020pbcs,
  title={{PBCS}: Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning},
  author={Matheron, Guillaume and Perrin, Nicolas and Sigaud, Olivier},
  journal={arXiv preprint arXiv:2004.11667},
  year={2020}
}

@article{kume2017map,
  title={Map-based multi-policy reinforcement learning: enhancing adaptability of robots by deep reinforcement learning},
  author={Kume, Ayaka and Matsumoto, Eiichi and Takahashi, Kuniyuki and Ko, Wilson and Tan, Jethro},
  journal={arXiv preprint arXiv:1710.06117},
  year={2017}
}

@inproceedings{tobin2018domain,
  title={Domain randomization and generative models for robotic grasping},
  author={Tobin, Josh and Biewald, Lukas and Duan, Rocky and Andrychowicz, Marcin and Handa, Ankur and Kumar, Vikash and McGrew, Bob and Ray, Alex and Schneider, Jonas and Welinder, Peter and others},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3482--3489},
  year={2018},
  organization={IEEE}
}
@article{akkaya2019solving,
  title={Solving Rubik's Cube with a Robot Hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}


@article{andrychowicz2017hindsight,
  title={Hindsight {E}xperience {R}eplay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017}
}

@inproceedings{schaul2015universal,
  title={Universal Value Function Approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={1312--1320},
  year={2015}
}

@article{sigaud2019policy,
  title={Policy Search in Continuous Action Domains: an Overview},
  author={Sigaud, Olivier and Stulp, Freek},
  journal={Neural Networks},
  pages = {28-40},
  volume = {113},
  year={2019}
}

@article{forestier2017intrinsically,
  title={Intrinsically motivated goal exploration processes with automatic curriculum learning},
  author={Forestier, S{\'e}bastien and Mollard, Yoan and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1708.02190},
  year={2017}
}

# explo

@article{tang2016exploration,
  title={\# Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.04717},
  year={2016}
}
@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1471--1479},
  year={2016}
}
@article{fortunato2017noisy,
  title={Noisy Networks for Exploration},
  author={Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and others},
  journal={arXiv preprint arXiv:1706.10295},
  year={2017}
}
@article{plappert2017parameter,
  title={Parameter Space Noise for Exploration},
  author={Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  journal={arXiv preprint arXiv:1706.01905},
  year={2017}
}

# diversity and RL

@article{jaderberg2019human,
  title={Human-level performance in 3D multiplayer games with population-based reinforcement learning},
  author={Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others},
  journal={Science},
  volume={364},
  number={6443},
  pages={859--865},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{doan2019attraction,
  title={Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning},
  author={Doan, Thang and Mazoure, Bogdan and Durand, Audrey and Pineau, Joelle and Hjelm, R Devon},
  journal={arXiv preprint arXiv:1909.07543},
  year={2019}
}

@inproceedings{jung2020population,
  title={Population-Guided Parallel Policy Search for Reinforcement Learning},
  author={Jung, Whiyoung and Park, Giseung and Sung, Youngchul},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{pourchot2018cem,
  title={CEM-RL: Combining evolutionary and gradient-based methods for policy search},
  author={Pourchot, Alo{\"\i}s and Sigaud, Olivier},
  journal={arXiv preprint arXiv:1810.01222},
  year={2018}
}
@article{colas2018gep,
  title={{GEP-PG}: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms},
  author={Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1802.05054},
  year={2018}
}

@inproceedings{parker2020effective,
  title={Effective Diversity in Population-Based Reinforcement Learning},
  author={Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen},
  booktitle={Neural Information Processing Systems},
  year={2020}
}

# quality-diversity, novelty

@article{pugh2016quality,
  title={Quality diversity: A new frontier for evolutionary computation},
  author={Pugh, Justin K and Soros, Lisa B and Stanley, Kenneth O.},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016},
  publisher={Frontiers}
}

@article{such2017deep,
  title={Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
  author={Petroski Such, Felipe and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  year={2017}
}

@article{ecoffet2019go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}

@inproceedings{kingmaadam,
  title={ADAM: AMETHOD FOR STOCHASTIC OPTIMIZATION},
  author={Kingma, Diederik P and Ba, Jimmy Lei},
  booktitle={Proc. of ICLR},
  year={2015}
}


@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  publisher={MIT Press},
  year={2018}
}

@article{van2018deep,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv},
  year={2018}
}

@inproceedings{lehman2011evolving,
  title={Evolving a diversity of virtual creatures through novelty search and local competition},
  author={Lehman, Joel and Stanley, Kenneth O},
  booktitle={Proceedings of the 13th annual conference on Genetic and evolutionary computation},
  pages={211--218},
  year={2011}
}

@inproceedings{cully2013behavioral,
  title={Behavioral repertoire learning in robotics},
  author={Cully, Antoine and Mouret, Jean-Baptiste},
  booktitle={Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  pages={175--182},
  year={2013}
}

@article{cazenille2019exploring,
title={Exploring Self-Assembling Behaviors in a Swarm of Bio-micro-robots using Surrogate-Assisted MAP-Elites},
author={Cazenille, Leo and Bredeche, Nicolas and Aubert-Kato, Nathanael},
journal={arXiv preprint arXiv:1910.00230},
year={2019}
}

@inproceedings{alvarez2019empowering,
  title={Empowering quality diversity in dungeon design with interactive constrained MAP-Elites},
  author={Alvarez, Alberto and Dahlskog, Steve and Font, Jose and Togelius, Julian},
  booktitle={2019 IEEE Conference on Games (CoG)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}
@inproceedings{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation.},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay and others},
  booktitle={NIPs},
  volume={99},
  pages={1057--1063},
  year={1999},
  organization={Citeseer}
}

@inproceedings{vemula2019contrasting,
  title={Contrasting exploration in parameter and action space: A zeroth-order optimization perspective},
  author={Vemula, Anirudh and Sun, Wen and Bagnell, J},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2926--2935},
  year={2019},
  organization={PMLR}
}

@article{nasiriany2019planning,
  title={Planning with goal-conditioned policies},
  author={Nasiriany, Soroush and Pong, Vitchyr H and Lin, Steven and Levine, Sergey},
  journal={arXiv preprint arXiv:1911.08453},
  year={2019}
}

@article{Nilsson2021,
   abstract = {Quality-Diversity optimization algorithms such as MAP-Elites, aim to generate collections of both diverse and high-performing solutions to an optimization problem. MAP-Elites has shown promising results in a variety of applications. In particular in evolutionary robotics tasks targeting the generation of behavioral repertoires that highlight the versatility of robots. However, for most robot-ics applications MAP-Elites is limited to using simple open-loop or low-dimensional controllers. Here we present Policy Gradient Assisted MAP-Elites (PGA-MAP-Elites), a novel algorithm that enables MAP-Elites to efficiently evolve large neural network controllers by introducing a gradient-based variation operator inspired by Deep Reinforcement Learning. This operator leverages gradient estimates obtained from a critic neural network to rapidly find higher-performing solutions and is paired with a traditional genetic variation to maintain a divergent search behavior. The synergy of these operators makes PGA-MAP-Elites an efficient yet powerful algorithm for finding diverse and high-performing behaviors. We evaluate our method on four different tasks for building behavioral repertoires that use deep neural network controllers. The results show that PGA-MAP-Elites significantly improves the quality of the generated repertoires compared to existing methods.},
   author = {Olle Nilsson and Antoine Cully},
   doi = {10.1145/3449639.3459304Ã¯},
   isbn = {9781450383509},
   keywords = {MAP-Elites,Neuroevolution,Quality-Diversity},
   title = {Policy Gradient Assisted MAP-Elites; Policy Gradient Assisted MAP-Elites},
   url = {https://hal.archives-ouvertes.fr/hal-03135723v2},
   year = {2021},
}

@article{Vassiliades2018,
   abstract = {Evolution has produced an astonishing diversity of species, each filling a different niche. Algorithms like MAP-Elites mimic this divergent evolutionary process to find a set of behaviorally diverse but high-performing solutions, called the elites. Our key insight is that species in nature often share a surprisingly large part of their genome, in spite of occupying very different niches; similarly, the elites are likely to be concentrated in a specific "elite hypervolume" whose shape is defined by their common features. In this paper, we first introduce the elite hypervolume concept and propose two metrics to characterize it: the genotypic spread and the genotypic similarity. We then introduce a new variation operator, called "directional variation", that exploits interspecies (or inter-elites) correlations to accelerate the MAP-Elites algorithm. We demonstrate the effectiveness of this operator in three problems (a toy function, a redundant robotic arm, and a hexapod robot).},
   author = {Vassilis Vassiliades and Jean-Baptiste Mouret},
   doi = {10.1145/3205455.3205602},
   journal = {GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary Computation Conference},
   keywords = {Illumination algorithms,MAP-Elites,Quality diversity},
   month = {4},
   pages = {149-156},
   publisher = {Association for Computing Machinery, Inc},
   title = {Discovering the Elite Hypervolume by Leveraging Interspecies Correlation},
   url = {http://arxiv.org/abs/1804.03906 http://dx.doi.org/10.1145/3205455.3205602},
   year = {2018},
}

@article{Fontaine2021,
  author    = {Matthew C. Fontaine and
               Stefanos Nikolaidis},
  title     = {Differentiable Quality Diversity},
  journal   = {CoRR},
  volume    = {abs/2106.03894},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.03894},
  eprinttype = {arXiv},
  eprint    = {2106.03894},
  timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-03894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{AGAC2021,
   abstract = {Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks.},
   author = {Yannis Flet-Berliac and Johan Ferret and Olivier Pietquin and Philippe Preux},
   title = {ADVERSARIALLY GUIDED ACTOR-CRITIC},
}

@article{EvoRL,
   abstract = {Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.},
   author = {Shauharda Khadka and Kagan Tumer},
   title = {Evolution-Guided Policy Gradient in Reinforcement Learning},
}



@article{DBLP:journals/corr/VassiliadesCM16,
  author    = {Vassilis Vassiliades and
               Konstantinos I. Chatzilygeroudis and
               Jean{-}Baptiste Mouret},
  title     = {Scaling Up MAP-Elites Using Centroidal Voronoi Tessellations},
  journal   = {CoRR},
  volume    = {abs/1610.05729},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.05729},
  eprinttype = {arXiv},
  eprint    = {1610.05729},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/VassiliadesCM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{CERL,
   abstract = {Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyper-parameters. One reason is that most approaches use a noisy version of their operating policy to explore-thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scal-able framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners-typically proven algorithms like TD3-optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevo-lution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient-notably solving the Mujoco Hu-manoid benchmark where all of its composite learners (TD3) fail entirely in isolation.},
   author = {Shauharda Khadka and Somdeb Majumdar and Tarek Nassar and Zach Dwiel and Evren Tumer and Santiago Miret and Yinyin Liu and Kagan Tumer},
   title = {Collaborative Evolutionary Reinforcement Learning},
}


@article{fontaine2020quality,
  title={A quality diversity approach to automatically generating human-robot interaction scenarios in shared autonomy},
  author={Fontaine, Matthew and Nikolaidis, Stefanos},
  journal={arXiv preprint arXiv:2012.04283},
  year={2020}
}

@inproceedings{fontaine2019,
title={Mapping Hearthstone Deck Spaces with Map-Elites with Sliding Boundaries},
author={Matthew C. Fontaine and Scott Lee and L. B. Soros and Fernando De Mesentier Silva and Julian Togelius and Amy K. Hoover},
booktitle={Proceedings of The Genetic and Evolutionary Computation Conference},
year={2019},
organization={ACM} }


@article{pierrot2022multi,
  title={Multi-Objective Quality Diversity Optimization},
  author={Pierrot, Thomas and Richard, Guillaume and Beguir, Karim and Cully, Antoine},
  journal={arXiv preprint arXiv:2202.03057},
  year={2022}
}

@article{charity2020baba,
title={Baba is Y'all: Collaborative Mixed-Initiative Level Design},
author={Charity, Megan and Khalifa, Ahmed and Togelius, Julian},
journal={arXiv preprint arXiv:2003.14294},
year={2020}}

@inproceedings{pierrot2022diversity,
  title={Diversity Policy Gradient for Sample Efficient Quality-Diversity Optimization},
  author={Pierrot, Thomas and Mac{\'e}, Valentin and Chalumeau, Felix and Flajolet, Arthur and Cideron, Geoffrey and Beguir, Karim and Cully, Antoine and Sigaud, Olivier and Perrin-Gilbert, Nicolas},
  booktitle={GECCO 2022 - Proceedings of the 2022 Genetic and Evolutionary Computation Conference},
  year={2022}
}


@article{zhang2020autoalpha,
  title={AutoAlpha: an Efficient Hierarchical Evolutionary Algorithm for Mining Alpha Factors in Quantitative Investment},
  author={Zhang, Tianping and Li, Yuanqi and Jin, Yifei and Li, Jian},
  journal={arXiv preprint arXiv:2002.08245},
  year={2020}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{freeman2021brax,
  title={Brax--A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  author={Freeman, C Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  journal={arXiv preprint arXiv:2106.13281},
  year={2021}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{chalumeau2022neuroevolution,
  title={Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery},
  author={Chalumeau, Felix and Boige, Raphael and Lim, Bryan and Mac{\'e}, Valentin and Allard, Maxime and Flajolet, Arthur and Cully, Antoine and Pierrot, Thomas},
  journal={arXiv preprint arXiv:2210.03516},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{dong2018speech,
  title={Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition},
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5884--5888},
  year={2018},
  organization={IEEE}
}


@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{mace2019using,
  title={Using whole document context in neural machine translation},
  author={Mac{\'e}, Valentin and Servan, Christophe},
  journal={arXiv preprint arXiv:1910.07481},
  year={2019}
}

@article{flageat2022empirical,
  title={Empirical analysis of PGA-MAP-Elites for Neuroevolution in Uncertain Domains},
  author={Flageat, Manon and Chalumeau, Felix and Cully, Antoine},
  journal={ACM Transactions on Evolutionary Learning},
  year={2022},
  publisher={ACM New York, NY}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{lim2022accelerated,
  title={Accelerated Quality-Diversity for Robotics through Massive Parallelism},
  author={Lim, Bryan and Allard, Maxime and Grillotti, Luca and Cully, Antoine},
  journal={arXiv preprint arXiv:2202.01258},
  year={2022}
}

@inproceedings{nilsson2021policy,
  title={Policy gradient assisted map-elites},
  author={Nilsson, Olle and Cully, Antoine},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={866--875},
  year={2021}
}

@article{flageat2020fast,
  title={Fast and stable MAP-Elites in noisy domains using deep grids},
  author={Flageat, Manon and Cully, Antoine},
  journal={arXiv preprint arXiv:2006.14253},
  year={2020}
}

@article{flageat2022benchmarking,
  title={Benchmarking Quality-Diversity Algorithms on Neuroevolution for Reinforcement Learning},
  author={Flageat, Manon and Lim, Bryan and Grillotti, Luca and Allard, Maxime and Smith, Sim{\'o}n C and Cully, Antoine},
  journal={arXiv preprint arXiv:2211.02193},
  year={2022}
}

@inproceedings{rakicevic2021policy,
  title={Policy manifold search: Exploring the manifold hypothesis for diversity-based neuroevolution},
  author={Rakicevic, Nemanja and Cully, Antoine and Kormushev, Petar},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={901--909},
  year={2021}
}

@article{morrison2020egad,
title={EGAD! an Evolved Grasping Analysis Dataset for diversity and reproducibility in robotic manipulation},
author={Morrison, Douglas and Corke, Peter and Leitner, Jurgen},
journal={IEEE Robotics and Automation Letters},
year={2020},
publisher={IEEE}}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@article{reed2022generalist,
  title={A generalist agent},
  author={Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others},
  journal={arXiv preprint arXiv:2205.06175},
  year={2022}
}

@inproceedings{
lee2022multigame,
title={Multi-Game Decision Transformers},
author={Kuang-Huei Lee and Ofir Nachum and Sherry Yang and Lisa Lee and C. Daniel Freeman and Sergio Guadarrama and Ian Fischer and Winnie Xu and Eric Jang and Henryk Michalewski and Igor Mordatch},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=0gouO5saq6K}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{cully2018hierarchical,
  title={Hierarchical behavioral repertoires with unsupervised descriptors},
  author={Cully, Antoine and Demiris, Yiannis},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={69--76},
  year={2018}
}

@article{engebraaten2020framework,
  title={A framework for automatic behavior generation in multi-function swarms},
  author={Engebraaten, Sondre A and Moen, Jonas and Yakimenko, Oleg A and Glette, Kyrre},
  journal={Frontiers in Robotics and AI},
  volume={7},
  pages={579403},
  year={2020},
  publisher={Frontiers Media SA}
}

@article{janner2021offline,
  title={Offline reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1273--1286},
  year={2021}
}

@article{jegorova2020behavioral,
  title={Behavioral repertoire via generative adversarial policy networks},
  author={Jegorova, Marija and Doncieux, St{\'e}phane and Hospedales, Timothy M},
  journal={IEEE Transactions on Cognitive and Developmental Systems},
  year={2020},
  publisher={IEEE}
}

@article{flageat2023uncertain,
  title={Uncertain Quality-Diversity: Evaluation methodology and new methods for Quality-Diversity in Uncertain Domains},
  author={Flageat, Manon and Cully, Antoine},
  journal={arXiv preprint arXiv:2302.00463},
  year={2023}
}

@inproceedings{justesen2019map,
  title={Map-elites for noisy domains by adaptive sampling},
  author={Justesen, Niels and Risi, Sebastian and Mouret, Jean-Baptiste},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  pages={121--122},
  year={2019}
}
