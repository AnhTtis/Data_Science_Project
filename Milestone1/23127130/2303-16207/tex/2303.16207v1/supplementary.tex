\section{Algorithms Pseudocodes}
\label{sec:algo_pseudocodes}
All algorithms are implemented based on the QDax framework \cite{lim2022accelerated}, which provides highly parallelized versions of many state-of-the-art QD and RL algorithms using the Brax physics engine \cite{freeman2021brax}, and allows to create custom algorithms by providing basic building blocks. The duration of a MAP-Elites (resp. PGA-MAP-Elites) run requiring hundreds of millions of interactions with the environment does not exceed a few hours on modern GPUs using QDax.

\subsection{MAP-Elites}
\label{sec:me_pseudocode}
Algorithm~\ref{alg:ME} depicts the standard MAP-Elites algorithm \cite{mouret2015illuminating}. Contrary to its Low-Spread version, MAP-Elites evaluates each solution only once to measure fitness and BD. While in practice, the main operations (selection, variation and insertion) are paralellized over batches of solutions at each iteration, Algorithm~\ref{alg:ME} portrays the canonical form originally introduced in \cite{mouret2015illuminating} for the sake of readability. In this work, we used a batch size of $1000$ for both environments to speed-up computation time.
\input{algorithms/map_elites.tex}



\subsection{PGA-MAP-Elites}
\label{sec:pgame_pseudocode}
The Policy Gradient Assisted MAP-Elites (PGA-MAP-Elites or simply PGAME) \cite{nilsson2021policy} algorithm builds upon MAP-Elites and introduces a variation operator based on policy gradient reinforcement learning to optimize for fitness.
PGA-MAP-Elites uses a replay buffer to store the transitions experienced by policies from the population during evaluation steps. It also employs an actor-critic model, using the TD3 algorithm \cite{fujimoto2018addressing} to train on the stored transitions. The critic is utilized at each iteration to calculate the policy gradient estimate for half of the offsprings selected from the MAP-Elites repertoire. A separate actor, known as the greedy controller, is also trained with the critic. This greedy controller is updated at each iteration and added to the MAP-Elites repertoire. Unlike other individuals, the greedy controller is never discarded, even if its fitness is lower than other individuals with similar behavior. Algorithm~ \ref{alg:PGAME} contains the pseudocode for PGA-MAP-Elites.
\input{algorithms/pga_me.tex}


\subsection{PGA-MAP-Elites Low-Spread}
\label{sec:pgame_lowspread_pseudocode}

The PGA-MAP-Elites Low-Spread (PGAME-LS) algorithm is analogous to ME-LS and simply modifies the PGAME algorithm to include an additional constraint over the policies spread (see Equation~\ref{eq:spread_computation}) during the insertion phase. Its overall structure is identical to the standard PGAME algorithm except for the fact that PGAME-LS evaluates solutions over multiples trajectories and insert new solutions into the repertoire only if they present higher fitnesses \textit{and} lower spreads than their corresponding solutions in the repertoire. Algorithm~\ref{alg:PGAME-LS} shows the pseudocode for PGAME-LS.

\input{algorithms/pga_me_lowspread.tex}

\subsection{Quality-Diversity Transformer}
\label{sec:qdt_pseudocode}
The Quality-Diversity Transformer model, training method and evaluation method are described in Algorithm~\ref{alg:QDT}. The QDT takes sequences of conditioning BD (which stays the same along the whole trajectory), observations and actions, and produces actions for all time steps in the trajetory in once inference. During training, it allows to compute actions for entire trajectories at once and compare them against labels. Since the QDT is a causal Transformer (GPT-based), for any given time step, it can only attend to all elements that precede this time step, which allows to run inference on whole trajectories without cheating. During evaluation, we simply take the predicted action that corresponds to the current time step and feed it to the environment to obtain the new state. We autoregressively build sequences of conditioning BD, observations and actions that are given as input to the QDT at each time step until the end of the episode. 

\input{algorithms/qdt.tex}



\section{Hyperparameters}
\label{sec:hyperparameters}
In this section we present hyperparameters used in our experiments. For each algorithm presented, we used the same hyperparameters for both environments.

As mentioned in Appendix~\ref{sec:algo_pseudocodes}, we based this work on the QDax framework \cite{lim2022accelerated} and used hyperparameters values presented in \cite{chalumeau2022neuroevolution} for MAP-Elites and PGA-MAP-Elites (as well as their Low-Spread counterparts), which are standard values in the literature.
Considering that QD algorithms and their Low-Spread versions share almost all their hyperparameters, we include them in the same tables, the only additional hyperparameter in ME-LS and PGAME-LS being the number of times each solution is evaluated, which is $10$ for all settings. Table~\ref{table:hp_mapelites} presents hyperparameters for MAP-Elites and MAP-Elites Low-Spread, and Table~\ref{table:hp_pgame} presents hyperparameters for PGA-MAP-Elites and PGA-MAP-Elites Low-Spread. Note that the environment batch size corresponds to the number of solutions that are evolved at each iteration, as we take advantage of the parallelization capabilities of QDax.

\begin{table}[h!]
    \begin{center}
    \begin{tabular}{lc}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Environment batch size & $1000$ \\
        Policy hidden layers size & $[256,256]$\\
        Iso sigma &  $0.005$ \\
        Line sigma &  $0.05$ \\
        \bottomrule
    
    \end{tabular}
    \end{center}
    \caption{Hyperparameters for MAP-Elites and MAP-Elites Low-Spread.}
    \label{table:hp_mapelites}
\end{table}

\begin{table}[h!]
    \begin{center}
    \begin{tabular}{lc} 
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Environment batch size & $1000$ \\
        Policy learning rate &  $0.001$\\
        Critic learning rate &  $0.0003$\\
        Policy hidden layers size &  $[256,256]$\\
        Critic hidden layers size &  $[256,256]$\\
        
        Policy noise & $0.2$\\
        Noise clip & $0.5$\\
        Discount & $0.99$\\
        Reward scaling & $1.0$\\
        
        Policy gradient proportion & $50\%$ \\
        Critic training steps & $300$\\
        Policy training steps & $100$\\
        Iso sigma & $0.005$ \\
        Line sigma & $0.05$ \\
        \bottomrule
    
    \end{tabular}
    \end{center}
    
    \caption{Hyperparameters for PGA-MAP-Elites and PGA-MAP-Elites Low-Spread.}
    \label{table:hp_pgame}
\end{table}

\begin{table}[ht!]
    \begin{center}
    \begin{tabular}{lc}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Number of layers & $4$ \\
        Number of attention heads & $8$\\
        Embedding dimension &  $256$ \\
        Nonlinearity function &  ReLU \\
        Batch size & $256$ \\
        Dropout & $0.1$ \\
        Learning rate & $0.0007$ \\

        \bottomrule
    
    \end{tabular}
    \end{center}
    \caption{Hyperparameters for the Quality-Diversity Transformer.}
    \label{table:hp_qdt}
\end{table}

\section{Additional Results}
\label{sec:additional_results}
In this section we present additional results that could not be included in the main paper. We first show that the reproducibility problem holds for PGA-MAP-Elites and that similar observation can be made in Halfcheetah-Uni for both MAP-Elites and PGA-MAP-Elites. Secondly we present traditional QD training metrics for ME, PGAME, ME-LS and PGAME-LS and show that although these training metrics are significantly in favor of the original versions (ME and PGAME), they do not capture the true quality of final repertoires. To support these claim we present a reassessment experiment in which we evaluate all repertoires post-training. We also present the results of the accuracy experiment of the main paper for PGAME variants and of the generalization experiment in Halfcheetah-Uni. Finally, we present evaluation fitness results of the QDT in both environment and demonstrate that it achieves fitnesses that are in line with the true repertoire fitnesses shown in Table~\ref{tab:metrics_xp}.

\subsection{The Reproducibility Problem}
\label{sec:add_results_qd_problem}
Figure~\ref{fig:add_qd_problem}a illustrates the reproducibility problem in Ant-Omni for PGAME. We selected $3$ representative policies from a final repertoire produced by PGAME and ran multiple ($N=30$) episodes with each policy. Results are similar to ME policies in that they generate irregular trajectories and demonstrate high spread in the behavior space even though PGAME incorporates a policy-gradient-based mutation operator during its training process. Figure~\ref{fig:add_qd_problem}b show that the Low-Spread version of PGAME, PGAME-LS, does not suffer from these problems and produces policies that are consistent in the BD space and which produce smooth, regular trajectories.

We reproduce these experiments and show in Figure~\ref{fig:add_qd_problem_halfcheetah} that the same observation can be made in the Halfcheetah-Uni environment for both algorithm families. Original versions of these algorithms (ME and PGAME) produce solutions that display high spread in the BD space while their Low-Spread counterparts create consistent solutions. For this environment we do not show whole trajectories as the behavior space is of 
a different nature which is not suitable for such plots.

\begin{figure*}[ht]
\centering
    \includegraphics[width=1\textwidth]{images/add_qd_problem.png}
    \caption{Illustration of the reproducibility problem in Ant-Omni. We select 3 representative policies from final repertoires that have been generated by a) PGA-MAP-Elites and our proposed variant b) PGA-MAP-Elites Low-Spread, and play 30 episodes with each policy using varying initial states. The top row depicts the final BDs obtained by each policy and the bottom row represents the corresponding entire trajectories in the behavior space.}
    \label{fig:add_qd_problem}
\end{figure*}

\begin{figure*}[ht]
\centering
    \includegraphics[width=1\textwidth]{images/add_qd_problem_halfcheetah.png}
    \caption{Illustration of the reproducibility problem in Halfcheetah-Uni. We select 3 representative policies from final repertoires that have been generated by a) MAP-Elites, b) MAP-Elites Low-Spread, c) PGA-MAP-Elites, d) PGA-MAP-Elites Low-Spread and play 30 episodes with each policy using varying initial states. Plots depict the final BDs obtained by each policy. Contrary to Ant-Omni in Figures~\ref{fig:qd_problem} and \ref{fig:add_qd_problem} we do not show entire trajectories since it would not be relevant considering the different nature of the behavior space in Halcheetah-Uni.}
    \label{fig:add_qd_problem_halfcheetah}
\end{figure*}

\subsection{QD Algorithms Results}
\label{sec:add_results_qd_algos}

\begin{figure*}[ht]
\centering
    \includegraphics[width=1\textwidth]{images/results_qd_algos.png}
    \caption{Results of the Quality-Diversity algorithms: MAP-Elites (ME), PGA-MAP-Elites (PGAME) and their Low-Spread variations (ME-LS and PGAME-LS) in both environments over 5 seeds. Coverage indicates the proportion of the behavior space that have been covered in the repertoire, max fitness reports the best fitness obtained by any solution evaluated so far and the QD score represents the total sum of fitness across all solutions in the repertoire. Performances are plotted against the number of interactions with the environment.}
    \label{fig:add_results_qd_algos}
\end{figure*}

\begin{figure*}[ht]
\centering
    \includegraphics[width=1.0\textwidth]{images/coverage_maps.png}
    \caption{Coverage maps of the Quality-Diversity algorithms: a) MAP-Elites, b) MAP-Elites Low-Spread, c) PGA-MAP-Elites and d) PGA-MAP-Elites Low-Spread. Fitness is represented by color: lighter is better.}    \label{fig:add_results_coverage_maps}
\end{figure*}

Figure~\ref{fig:add_results_qd_algos} and Figure~\ref{fig:add_results_coverage_maps} gather results for the QD algorithms runs, namely MAP-Elites (ME), PGA-MAP-Elites (PGAME) and their Low-Spread versions (ME-LS and PGAME-LS), and show their respective coverages,  maximum fitnesses, and QD scores in both environments.
In accordance with standard practices in QD research, we add an offset to the fitnesses when computing the QD score to ensure that it is an increasing function of the coverage. The initial repertoires, which are identical for all methods, are of size 1024 and are generated using Centroidal Voronoi Tessellations \cite{DBLP:journals/corr/VassiliadesCM16}.

Results show that the original versions of these algorithms (ME and PGAME) obtain the best performances over all training metrics by a significant margin. However, it is important to recall that first, the original versions evaluate each policy only once, contrary to the Low-Spread versions that do multiple evaluations (usually $10$), which strongly promotes accidental policies that have been lucky over their unique evaluation episode obtaining abnormally high fitnesses and inaccurate BDs. And second, The Low-Spread versions include an additional insertion criteria that drives the search process towards policies that are more consistent in the BD space, leading to fewer insertions in the repertoire --policies have to show higher average fitness and better consistency than their counterpart in the repertoire to be selected.
We argue that these very points are responsible for the difference in coverage and, \textit{in fine}, in maximum fitness and QD score. But more importantly, even though the original versions present better training metrics, they remain limited as these metrics do not take into account the actual usefulness of a QD repertoire and its capacity to deliver accurate and consistent solutions that behave according to their BDs, as shown in Section~\ref{sec:reproductibility_problem} and Figure~\ref{fig:results_precision}.

\begin{table}[!ht]
    \small
    \caption{Results of the reassessment experiment in Halfcheetah-Uni. For each algorithm, we take a final repertoire of policies and test them again over multiple episodes. We insert them into a new, empty repertoire and report its coverage, max fitness and QD score. "Initial" columns show values for the initial repertoire, that is, the repertoire that was used during the algorithm run. "Recalc." columns refer to values of the new repertoire that contains solutions after re-evaluation. It appears that after re-evaluation the repertoires issued from Low-Spread methods show superior performance compared to the original methods.
    }
        \centering
        \label{tab:metrics_xp}
        %\hspace{-0.5cm}
        \begin{tabular}{ccccccc}
            \toprule
            
             & \multicolumn{2}{c}{Coverage}  & \multicolumn{2}{c}{Max Fitness} & \multicolumn{2}{c}{QD Score}\\
             & \multicolumn{2}{c}{(in \%)}  & \multicolumn{2}{c}{} & \multicolumn{2}{c}{$(\times10^6)$}\\
                & Initial & {\bf Recalc.} & Initial & {\bf Recalc.} & Initial &{\bf Recalc.}\\
            \midrule
            ME & $100$ &  ${\bf 43}$ & $1226$ & ${\bf 770}$ & $3.05$ & ${\bf 1.17}$\\
            ME-LS & $100$ & ${\bf 55}$ & $992$ & ${\bf 730}$ & $2.63$ & ${\bf 1.41}$\\
            PGAME & $100$ & ${\bf 40}$ & $1417$ & ${\bf 977}$ & $3.07$ & ${\bf 1.10}$\\
            PGAME-LS & $100$ & ${\bf 53}$ & $1194$ & ${\bf 1073}$ & $2.63$ & ${\bf 1.36}$\\
            \bottomrule
        \end{tabular}
\end{table}

\begin{figure*}[h]
\centering
    \includegraphics[width=0.95\textwidth]{images/precision_xp_pgame.png}
    \caption{Results of the accuracy experiment for the PGAME variants. This experiment can be described in 2 steps: 1. We select multiple evaluation goals (target BDs) in the behavior space, 100 and 50 for Ant-Omni and Halfcheetah-Uni respectively. To get meaningful goals, we simply compute a CVT of the BD space in which goals are the centers of each zone, 2. For each goal, we play 10 episodes and plot their average Euclidean distance to the goal. For PGAME and PGAME-LS, trajectories are played by the nearest policy to the goal in the repertoire. For the QDT, we simply condition it on the goal. Distance is represented by color: lighter is better. The QDT(PGAME-LS) appears to be the most accurate method to achieve behaviors on demand.}
    \label{fig:results_precision_pgame}
\end{figure*}


\subsubsection{The Reassessment Experiment}
\label{sec:reassessment_xp}
To further prove this statement, we take a final repertoire of each method --namely ME, ME-LS, PGAME and PGAME-LS-- and conducted the following experiment: 1. We take each policy from the repertoire and play 10 episodes with varying initial states, 2. We compute the policy's average fitness and its most frequent behavior, 3. We add evaluated policies into a new, empty repertoire according to their recalculated fitnesses and behaviors. Table~\ref{tab:metrics_xp} gathers results for this experiment for the Halfcheetah-Uni environment. It is clear that after re-evaluation, ME-LS and PGAME-LS demonstrate superior coverages and QD scores to ME and PGAME, and comparable max fitnesses. This experiment shows that for uncertain domains, training metrics such as those usually presented in the QD literature (see Figure~\ref{fig:add_results_qd_algos}) can be misleading and may not capture true value of a final repertoire. Note that similar results concerning MAP-Elites have been observed in different settings \cite{engebraaten2020framework}.


\subsection{Accuracy Experiment}
\label{sec:add_accuracy_xp}

Figure~\ref{fig:results_precision_pgame} depicts the accuracy experiment described in Section~\ref{sec:accuracy_xp} for PGAME variants. We evaluate each method against multiple evaluation goals (target BDs) that reasonably cover the behavior space and report the average distance for each goal, which is represented by color (lighter is better). Similar to results presented in Section~\ref{sec:accuracy_xp}, PGAME fails to achieve target BDs on demand, while the QDT(PGAME) improves over this result. The QDT(PGAME-LS) appears to be the most accurate method to achieve target BDs in both environments. Importantly, note that all methods struggle to reach the most outer goals in Ant-Omni, this is due to the fact that no policy --hence no data-- is available for these zones of the BD space as shown in the dataset representation in Figure~\ref{fig:results_generalization}.

\subsection{Generalization Experiment}
\label{sec:add_generalization_experiment}

Figure~\ref{fig:add_results_generalization} depicts the generalization experiment described in Section~\ref{sec:generalization_experiment} for the Halfcheetah-Uni environment. We observe that, even though the QDT demonstrates good accuracy up to the 30\% density setting, it has more difficulties to generalize in this environment, both for interpolation and extrapolation. We hypothesize that this difference between Ant-Omni and Halfcheetah-Uni comes from the very different nature of their behavior spaces. After execution of a QD algorithm in Ant-Omni, two policies that are close in the BD space often produce similar full-body trajectories, meaning that they walk on the 2D plane and reach their final positions, which happens to be slightly different, but both policies walk with similar gaits. In Halfcheetah-Uni, two policies that are close in the BD space can demonstrate radically different full-body behaviors. As an extreme example, it occurred that we observed neighboring policies in the BD space, one of which was doing backflips while the other was running normally. We believe that these gaps in real behaviors prevent effective generalization for the QDT. Finally, note that in these generalization experiments we simply prune trajectories from the datasets and do not increase the number of trajectories in preserved zones, which can affect the model in the sense that it has strictly less data to train on.

\begin{figure*}[h]
\centering
    \includegraphics[width=1\textwidth]{images/generalization_halfcheetah.png}
    \caption{Results of the QDT generalization experiment in Halfcheetah-Uni. In this experiment we run accuracy experiments (bottom row) on truncated datasets (top row) which are deprived of a part of their trajectories. The QDT shows strong interpolation ability on the 50\%, and 30\% density datasets and a limited ability to extrapolate in "Tiles" and "Upper part" datasets where entire zones of the BD space are deprived of data.}
    \label{fig:add_results_generalization}
\end{figure*}


\subsection{QDT Fitness Results}
\label{sec:qdt_fitness}

Figure~\ref{fig:qdt_training_fitness} reports the performances of all variants of the QDT in terms of fitness for Halfcheetah-Uni. During evaluation phases of the training process, which are described in Section~\ref{sec:training_and_ablations}, we record the average fitness obtained for each goal (target BD). The maximum fitness reported in Figure~\ref{fig:qdt_training_fitness} simply corresponds to maximum over all goals. To be fair, these results should be compared to results of the reassessment experiment in Table~\ref{tab:metrics_xp} as we want to know what is the maximum average fitness that we can expect from each method at evaluation time. It appears that the QDT is able to reproduce the maximum fitnesses of the QD policies that were used to generate its dataset.

\begin{figure*}[ht]
    \includegraphics[width=0.6\textwidth]{images/qdt_training_fitness.png}
    \caption{Maximum fitness of the QDT in Halfcheetah-Uni for evaluations during the training phase (average values and std ranges on 3 seeds). We report the maximum fitness obtained over all goals. Performances are similar --if not superior-- to values reported in the reassessment experiment in Table~\ref{tab:metrics_xp}, meaning that our model is able to replicate the fitness of QD policies.}
    \label{fig:qdt_training_fitness}
\end{figure*}