\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}
\usepackage[preprint,nonatbib]{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{amsmath, bm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{multirow}


% \usepackage{biblatex}  
% \addbibresource{ref.bib}

\newcommand{\name}{\texttt{Uni-Mol+}\xspace}
\newcommand{\sa}{\texttt{SelfAttentionPairBias}\xspace}
\newcommand{\opm}{\texttt{OuterProduct}\xspace}
\newcommand{\tm}{\texttt{TriangularUpdate}\xspace}
\newcommand{\ffn}{\texttt{FFN}\xspace}
\newcommand{\embedding}{\texttt{Embedding}\xspace}
\newcommand{\LZ}[1]{{\color{blue}[LZ: #1]}}

\title{Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+}

% Uni-Mol with triangular Multiplication
% Triangular M Update TU 

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{Shuqi Lu$^1$, Zhifeng Gao$^1$, Di He$^2$, Linfeng Zhang$^1$, Guolin Ke$^1$ \\
$^1$DP Technology\\
$^2$Peking University\\
\{lusq, gaozf\}@dp.tech \\ 
 dihe@pku.edu.cn, \{zhanglf, kegl\}@dp.tech \\
}

\begin{document}


\maketitle


\begin{abstract}
% Recent advancements in deep learning have shown promising results in accelerating quantum chemical (QC) property prediction, eliminating the need for costly density functional theory (DFT) calculations. However, to further improve accuracy, instead of 1D SMILES sequences or 2D molecular graphs, it is crucial to incorporate 3D conformations, as QC properties are largely based on the 3D equilibrium conformations optimized by DFT. To address this need, this paper introduces a novel approach, \name, which accurately predicts QC properties by utilizing 3D conformations generated by cheap methods, iteratively optimized towards the equilibrium conformations generated by DFT. In particular, \name contains 1) a two-track Transformer model backbone to effectively predict the 3D positions; 2) a novel training strategy called linear trajectory injection for effective conformation optimization training. Extensive benchmarks demonstrate the effectiveness of the proposed method in improving the accuracy of QC property prediction. The code and model are made publicly available at \url{}.
% Recent developments in deep learning have made remarkable progress in speeding up the prediction of quantum chemical (QC) properties by removing the need for expensive density functional theory (DFT) calculations. Previous approaches usually perform the prediction task simply using 1D SMILES sequences or 2D molecular graphs. However, QC properties largely depend on the 3D equilibrium conformations optimized by DFT. 1D and 2D data don't contain enough 3D equilibrium information, making most works fail to achieve high accuracy in practice. To address this challenge, we propose a novel approach, \name, which contains three steps. Given a 2D molecular graph, \name first generates an initial 3D conformation from inexpensive methods, such as RDKit. In the second step, an model is used to iteratively optimize the initial conformation to its equilibrium conformation. Lastly, a prediction model is trained to estimate the QC properties from the optimized conformation. Both models use the same backbone neural network and are learned automatically using proper supervisions.

Recent developments in deep learning have made remarkable progress in speeding up the prediction of quantum chemical (QC) properties by removing the need for expensive electronic structure calculations like density functional theory. 
However, previous methods that relied on 1D SMILES sequences or 2D molecular graphs failed to achieve high accuracy as QC properties are primarily dependent on the 3D equilibrium conformations optimized by electronic structure methods. 
In this paper, we propose a novel approach called \name to tackle this challenge. Firstly, given a 2D molecular graph, \name generates an initial 3D conformation from inexpensive methods such as RDKit. Then, the initial conformation is iteratively optimized to its equilibrium conformation, and the optimized conformation is further used to predict the QC properties. All these steps are automatically learned using Transformer models. We observed the quality of the optimized conformation is crucial for QC property prediction performance. To effectively optimize conformation, we introduce a two-track Transformer model backbone in \name and train it together with the QC property prediction task. We also design a novel training approach called linear trajectory injection to ensure proper supervision for the \name learning process. Our extensive benchmarking results demonstrate that the proposed \name significantly improves the accuracy of QC property prediction. We have made the code and model publicly available at \url{https://github.com/dptech-corp/Uni-Mol}.
\end{abstract}



\section{Introduction}

The design of new materials and molecules through computational methods heavily relies on the prediction of quantum chemical (QC) properties, such as free energy and HOMO-LUMO gap. High-throughput screening on a large database is the most straightforward approach to discovering molecules with desired properties~\cite{jain2013commentary}. However, this method can be cost-prohibitive due to the extensive computational requirements of electronic structure methods, which can take several hours to calculate properties on a single molecule. Therefore, finding ways to reduce the costs of calculating QC properties is a critical task. 
Hereafter, we take density functional theory (DFT)~\cite{jones2015density} as the default electronic structure method we consider.

Recent studies have demonstrated the potential of deep learning in accelerating QC property prediction~\cite{luo2022one, ying2021transformers, shi2022benchmarking}. This involves training a deep neural network model to predict the property using molecular inputs, thus replacing the heavy DFT calculations. Previous research has mainly utilized 1D SMILES \cite{wang2019smiles, ross2022large, gomez2018automatic} sequences or 2D molecular graphs \cite{gilmer2017neural, luo2022your, kim2022pure, park2022grpe, hussain2022global, ying2021transformers} as molecular inputs. However, predicting QC properties from 1D SMILES and 2D molecular graphs can be challenging due to their inherent discrepancy from the 3D equilibrium conformations optimized by DFT, which are critical for QC property estimation. Therefore, establishing a direct relationship between the 1D/2D data and QC properties may be ineffective, despite their easy obtainability.

% Recent studies have shown that deep learning can be used for QC property prediction acceleration~\cite{luo2022one, ying2021transformers, shi2022benchmarking}, i.e.,  one can train a deep neural network model to predict the property using molecular inputs, replacing heavy DFT calculations. Previous research mainly use 1D SMILES \cite{wang2019smiles, ross2022large, gomez2018automatic} sequences or 2D molecular graphs \cite{gilmer2017neural, luo2022your, kim2022pure, park2022grpe, hussain2022global, ying2021transformers} as molecular inputs. 
% On the one hand, these data formats can be easily obtained with little computational cost. On the other hand, it may be challenging to predict the QC properties from these data in a direct way, as QC properties largely depend on 3D equilibrium conformations optimized by DFT, which is far different from any cheaply obtained data.

%By training a deep neural network model using molecular inputs, a cost-efficient means of predicting properties without costly DFT calculations can be achieved. 

To address this challenge, in this paper, we propose a method called \name. Unlike previous approaches that directly predict QC properties from 1D/2D data, \name first generates raw 3D conformation from 1D/2D data using cheap methods, such as RDKit \cite{landrum2016rdkit}. As the raw conformation is usually inaccurate, \name then optimizes it towards equilibrium conformation obtained from DFT and predicts QC properties with optimized conformation. In this framework, the quality of the optimized conformation is crucial to the performance of QC property prediction. Fortunately, we can use large-scale datasets (e.g.,  PCQM4MV2 benchmark) to build up millions of pairs of RDKit-generated conformation and high-quality DFT equilibrium conformation and learn the optimization process automatically, making the approach plausible. With a carefully designed model backbone and training tasks, \name shows superior performance in various benchmarks.

% To overcome this challenge, we introduce a novel method called \name in this paper. Unlike previous approaches that directly predict QC properties from 1D/2D data, \name initially generates raw 3D conformation using cost-effective methods, such as RDKit \cite{landrum2016rdkit} and OpenBabel \cite{o2011open}. Since the raw conformation is often inaccurate, \name optimizes it towards the equilibrium conformation obtained from DFT and predicts QC properties with the optimized conformation. By doing so, \name establishes a mapping between 1D/2D data and QC properties, with an intermediate step of equilibrium conformation estimation. We train \name using large-scale datasets, such as the PCQM4MV2 benchmark, by pairing the raw 3D conformation and equilibrium conformation of molecules, and we train \name in a fully differentiable way. With a carefully designed model backbone and training tasks, \name exhibits superior performance in various benchmarks.

\paragraph{Model Backbone} 
We have adopted a two-track Transformer backbone, similar to Uni-Mol \cite{zhou2022uni}. This backbone consists of an atom representation track and a pair representation track.
Besides, we propose several modifications to further enhance the model capacity. These include:
i) An enhanced pair representation that is initialized with 3D spatial positional encoding and graph positional encoding. Subsequently, it is repeatedly updated via three operators: an outer product of atom representation, a triangular update operator, and a feed-forward network.
ii) A 3D position prediction that uses attention to calculate position updates on the three axes.
iii) Iterative optimization that improves the predicted 3D positions through iterations, towards DFT-generated conformations.
% iv) Confidence prediction, which provides an additional confidence score, to be used for selecting, ranking, or filtering the generated 3D conformations. 

\paragraph{Training Strategy}
Utilizing DFT-optimized equilibrium conformations as the target, we present a method called \emph{linear trajectory injection} (LTI) for efficient conformation optimization training. The proposed method involves selecting a random conformation from the trajectory between the cheap conformation and the DFT conformation, which is then taken as input to the model. Notably, since the actual trajectory is unknown in most cases, we use a pseudo trajectory, which assumes the movement between two conformations is a linear process. The LTI method provides several advantages, including a plethora of input conformations for a molecule, which serves as an efficient data augmentation strategy. Moreover, starting from an intermediate point in the pseudo trajectory simplifies training towards the DFT equilibrium conformation. Additionally, with iterative optimization, the model can progressively converge towards the target DFT equilibrium conformation along the trajectory.

\paragraph{Experimental Result}
Our proposed \name is evaluated on the widely-used PCQM4MV2 benchmark dataset \cite{DBLP:conf/nips/HuFRNDL21}, whose task is predicting the HOMO-LUMO gap based on molecular SMILES. Specifically, we leverage the DFT equilibrium conformations provided in the PCQM4MV2 training set, as well as the cheap conformations generated by RDKit, to train a \name model with LTI strategy. In the validation set of PCQM4MV2, by taking RDKit-generated conformations as inputs, our \name outperforms all previous state-of-the-art methods by a significant margin. 
The results provide clear evidence of the effectiveness of the proposed \name.
% These results suggest that our proposed \name has the potential to serve as a general-purpose model for predicting quantum chemical properties.



 \begin{figure*}[!tbp]
 %\vspace{-4pt}
 \centering
\includegraphics[width=0.95\linewidth]{Pictures/pcq_model_compare.pdf}
%\vspace{-10pt}
\caption{In contrast to prior methods that directly predict QC properties from 1D/2D data, \name employs a distinct approach. It firstly generates raw 3D conformation from 1D/2D data using cheap tools like RDKit, and subsequently optimizes it towards the equilibrium conformation obtained from DFT. Finally, it predicts QC properties using the optimized conformation. } \label{fig:pcq_model_compare}
\end{figure*}


\section{Related Work}
\paragraph{Deep QC property prediction models with 1D/2D information}
Some prior works have solely utilized 1D information, such as SMILES sequences, to make predictions. These include Wang et al.~\cite{wang2019smiles}, Ross et al.~\cite{ross2022large}, and Gomez et al.~\cite{gomez2018automatic}. However, to incorporate more information, a number of studies have employed 2D graphs or fingerprints derived from SMILES sequences. Gilmer et al.~\cite{gilmer2017neural} utilized Graph Neural Network (GNN) to learn the molecular graph representation, while Graphormer~\cite{ying2021transformers} extended Transformer models to graph tasks through graph structural encodings. Numerous subsequent studies~\cite{gilmer2017neural, luo2022your, kim2022pure, park2022grpe, hussain2022global} have employed Transformer models on graph tasks and have made significant improvements and innovations by including self-attention mechanisms and relative positional encoding, thereby enhancing the capabilities of Transformer models in graph tasks. 

\paragraph{Deep QC property prediction models with 3D information}
As 3D information is critical in predicting QC property, several recent studies have incorporated 3D information.
Some works use 3D structure information to enhance the 2D representation during training. St{\"a}rk et al.~\cite{stark20223d} maximize the mutual information between 3D vectors and the 2D representations of a GNN to make them contain latent 3D information and Liu et al.~\cite{liu2021pre} leverage the correspondence and consistency between 2D topological structures and 3D geometric views to learn a 2D molecular graph encoder enhanced by richer and more discriminative 3D geometry. Zhu et al. propose a unified 2D and 3D pre-training scheme and make the 2D encoder and the 3D encoder promote each other. However, these works only implicitly embed 3D structural information into 2D representation, while during inference, only 2D information is used. 
Luo et al.~\cite{luo2022one} propose Transformer-M, an encoder capable of handling both 2D and 3D input formats. They are among the first to demonstrate that incorporating 3D information during training can enhance the performance of using 2D input alone in inference. However, the application of Transformer-M is restricted to taking 2D molecular graph or 3D DFT equilibrium conformation as  input.
Some works take 3D conformation as input~\cite{schutt2017schnet, klicpera2003directional, hutchinson2021lietransformer, tholke2022equivariant, shi2022benchmarking, ying2021transformers, zhou2022uni} and consider the equivalence or invariance of rotation and translation in the model. 
For instance, Zhou et al.~\cite{zhou2022uni} introduce Uni-Mol, which utilizes the 3D conformation generated by RDKit as input and incorporates a 3D position recovery task for pretraining. Similarly, Shi et al.~\cite{shi2022benchmarking} propose Graphormer-3D, an extension of Graphormer~\cite{ying2021transformers} to a 3D Transformer model. Graphormer-3D takes the initial 3D conformation provided by the OC20 dataset~\cite{chanussot2021open} and predicts the energy at equilibrium. While this setup is similar to \name, the backbone of the model and the training strategy of the two models differ significantly.

\paragraph{3D conformation optimization}
% rewrite this
Optimizing molecular conformations towards an equilibrium state is a crucial challenge in computational chemistry. The most popular method for this task is Density Functional Theory (DFT), which offers high accuracy, but at a substantial computational cost. To address this issue, several deep potential models have been proposed, such as DeePMD ~\cite{zhang2018deep}. These models use neural networks to replace the costly potential calculations in DFT, thereby improving efficiency. However, deep potential models still require dozens or hundreds of steps to optimize the conformation iteratively based on the predicted potentials. In contrast, \name only requires a few rounds of optimization. Additionally, \name can end-to-end optimize the conformation, whereas deep potential models cannot.
Although there are other works \cite{guan2022energyinspired, zhou2022uni} that also optimize RDKit-generated conformations towards DFT conformations, these works generally focus on benchmarking conformation rather than QC property. Furthermore, compared with \name, these works differ significantly in terms of model backbone and training strategy.




% compared with transform-m 
% compared with Graphormer-3D in OC20
% compared with Uni-Mol

\section{Method}


% Density Function Theory (DFT) \cite{} is currently the most effective way to calculate quantum chemical properties. During DFT calculation, a high-quality conformation is obtained by optimizing the energy, then the quantum chemical properties are calculated based on the conformation. However, DFT calculation is very costly, which limits its usage in real-world applications. 

% Therefore, to effectively and efficiently predict the quantum chemical properties, we proposed to use 

% to accurately predict the quantum chemical properties, the key is to predict the DFT-generated conformation. 

% The quantum property prediction highly depends on the conformation. For example, based on B3LYP~\cite{lee1988development, becke1993new}, we can get DFT-level conformation, 


% As aforementioned, the DFT-optimized 3D conformation is critical in QC properties, since these properties are calculated based on it. Therefore, unlike previous works that predicted the QC properties based on 1D SMILES sequences and 2D molecular graphs, we propose directly predicting QC properties based on 3D conformation. However, as DFT is too costly to be used in the runtime, we cannot use the DFT-generated 3D conformation as the input to the model. Thus, the proposed model takes the cheap 3D conformation as input. Levering the training set's DFT-generated 3D conformations, the model also is trained to (iteratively) optimize the  to DFT-generated. Then, based on model-optimized conformation, the proposed model can predict the quantum chemical properties accurately. We call the proposed model \name . 

For any molecule, \name first obtains an initial 3D conformation generated by cheap methods, such as template-based methods from RDKit and OpenBabel. It then optimizes the initial conformation iteratively towards the target conformation, i.e., the conformation generated by Density Functional Theory (DFT). Finally, using the optimized conformation as model input, \name learns to predict the QC properties. 
% To achieve this objective, we introduce a new model backbone and a novel training strategy for conformation optimization and property prediction, elaborated in the subsequent subsections.
To achieve this objective, we introduce a new model backbone and a novel training strategy for optimizing conformation and predicting QC properties, which we will discuss in subsequent subsections.


\subsection{Model Backbone}

We design a novel model backbone that can optimize conformation and predict QC property simultaneously, denoted as $(y, \bm{\hat{r}}) = f(\bm{X}, \bm{E}, \bm{r} ; \bm{\theta})$.
This model uses atom features ($\bm{X} \in \mathbb{R}^{n \times d_f}$, where $n$ is the number of atoms and $d_f$ is atom feature dimension), edge features ($\bm{E} \in \mathbb{R}^{n \times n \times d_e}$, where $d_e$ is the edge feature  dimension), and 3D coordinates of atoms ($\bm{r} \in \mathbb{R}^{n \times 3}$), with learnable parameters $\bm{\theta}$, to predict a quantum property $y$ and new 3D coordinates $\bm{\hat{r}} \in \mathbb{R}^{n \times 3}$.
Like the Uni-Mol model \cite{zhou2022uni}, two tracks of representations are maintained: 1) Atom representation ($\bm{x} \in \mathbb{R}^{n \times d_x}$, where $d_x$ is the dimension of the representation) and 2) Pair representation ($\bm{p} \in \mathbb{R}^{n \times n \times d_p}$, where $d_p$ is the dimension of the representation). The model has $L$ blocks, with $\bm{x}^{(l)}$ and $\bm{p}^{(l)}$ representing the $l$-th block's outputs.
The overall architecture of \name is depicted in Fig.~\ref{fig:model}. Compared with Uni-Mol, there are several key differences, which will be outlined in the following paragraphs.

 \begin{figure*}[!tbp]
 %\vspace{-4pt}
 \centering
\includegraphics[width=0.95\linewidth]{Pictures/unimol_pcq.pdf}
%\vspace{-10pt}
\caption{The \name model backbone consists of two tracks of representations - atom and pair, initialized by atom features and 2D graph/3D conformation respectively. These representations communicate with each other at every block. Besides, \name optimizes the predicted 3D position iteratively using the previous iteration's predicted conformations as input for the current iteration.} \label{fig:model}
\end{figure*}

\paragraph{Positional Encoding}
Similar to previous works~\cite{ying2021transformers, zhou2022uni}, we use pair-wise encoding to encode the 3D spatial and 2D graph positional information. For 3D spatial information, we use the Gaussian kernel
\begin{equation}
    s_{i,j}^{k} = \frac{1}{\sigma^k\sqrt{2\pi}} \text{exp} \left(-\frac{1}{2}
    \left( {\frac{\alpha_{i,j} ||\bm{r}_i -\bm{r}_j|| -\mu^k + \beta_{i,j}}{\sigma^k}} \right)^2 
    \right),
    k = \{1, 2, ... K\},
\end{equation}
where $s_{i,j}^{k}$ is the $k$-th Gaussian kernel of the atom pair $(i, j)$, $K$ is the number of kernels. The input 3D coordinate of the $i$-th atom is represented by $\bm{r}_i \in \mathbb{R}^3$, and $\alpha_{i,j}$ and $\beta_{i,j}$ are learnable scalars indexed by the pair of atom types. $\mu^k$ and $\sigma^k$ are predefined constants. Specifically, $\mu^k = w \times (k-1) / K  $ and $\sigma^k = w / K  $, where the width $w$ is a hyper-parameter. Compared to Uni-Mol~\cite{zhou2022uni} and Transform-M \cite{luo2022one}, our approach includes fixed (non-learnable) $\mu^k$ and $\sigma^k$, which improves training stability. The 3D positional encoding is then obtained using a feed-forward network, represented by $\bm{\psi}^{\text{3D}}_{i,j} = \ffn(\bm{s}_{i,j})$.

In addition to the 3D positional encodings, we also incorporate graph positional encodings similar to those used in Graphormer. This includes the shortest-path encoding, represented by $\bm{\psi}^{SP}_{i,j} = \embedding(\text{sp}_{ij})$ where $\text{sp}_{ij}$ is the shortest path between atoms $(i,j)$ in the molecular graph. Additionally, instead of the time-consuming multi-hop edge encoding method used in Graphormer, we utilize a more efficient one-hop bond encoding, denoted by $\bm{\psi}^{Bond} = \sum_{i=1}^{d_e}\embedding(\bm{E}_i)$, where $\bm{E}_{i}$ is the $i$-th edge feature.

Combined above, the positional encoding is denoted as $\bm{\psi} = \bm{\psi}^{\text{3D}} + \bm{\psi}^{\text{SP}} + \bm{\psi}^{\text{Bond}}$. And the pair representation $\bm{p}$ is initialized by $\bm{\psi}$, i.e., $\bm{p}^{(0)} = \bm{\psi}$.

\paragraph{Update of Atom Representation}
The atom representation $\bm{x}^{(0)}$ is initialized by the sum of embeddings of atom features $\bm{X}$ and the degree encodings from the graph structure, the same as Graphormer, denoted as:
\begin{equation}
    \bm{x}^{(0)} = \embedding(\bm{D}) + \sum_{i=1}^{d_f} \embedding (\bm{X}_i),
\end{equation}
where $\bm{D}$ is the node degrees and $\bm{X}_i$ is the $i$-th atom feature. Then, at $l$-th block, we update the atom representation as follows:
\begin{equation}
\begin{aligned}
    \bm{x}^{(l)} = \bm{x}^{(l-1)} &+ \sa\left(\bm{x}^{(l-1)}, \bm{p}^{(l-1)} \right), \\
    \bm{x}^{(l)} = \bm{x}^{(l)} &+ \ffn\left(\bm{x}^{(l)} \right).
\end{aligned}
\end{equation}

The update process is very similar to the transformer layer, which contains a self-attention and an FFN layer, and the \sa can be denoted as:
\begin{equation}
\begin{aligned}
    \bm{Q}^{(l, h)} &= \bm{x}^{(l-1)} \bm{W}_Q^{(l, h)}; \quad \bm{K}^{(l, h)} = \bm{x}^{(l-1)} \bm{W}_K^{(l, h)}; \\
    {\color{blue} \bm{B}^{(l, h)}} &= \bm{p}^{(l-1)} \bm{W}_B^{(l,h)}; \quad \bm{V}^{(l, h)} = \bm{x}^{(l-1)} \bm{W}_V^{(l, h)}; \\
    \bm{A}^{(l, h)} &=  \text{softmax} \left( \frac{ \bm{Q}^{(l,h)} (\bm{K}^{(l,h)})^T}{\sqrt{d_h}} + {\color{blue} \bm{B}^{(l, h)}} \right) \bm{V}^{(l, h)}; \\
    \bm{o} &= \text{concat}_{h}(\bm{A}^{(l, h)})\bm{W}_O^{(l)},
\end{aligned}
\end{equation}

where $d_h$ is the head dimension, $\bm{W}_Q^{(l, h)}, \bm{W}_K^{(l, h)}, \bm{W}_V^{(l, h)} \in \mathbb{R}^{d_x \times d_h}$,  $\bm{W}_B^{(l, h)} \in  \mathbb{R}^{d_p \times 1}$, $\bm{W}_O^{(l)} \in  \mathbb{R}^{(d_h \times H) \times d_x}$, $H$ is the number of heads, and $\bm{o}$ is the return result. Please note that the $\bm{o}$ is a temporary variable in the \sa function. 
For simplicity, layer normalization and dropout are omitted. 
In addition to the standard transformer layer, an attention bias term ${\color{blue}\bm{B}^{(l,h)}}$ has been incorporated within the self-attention. This bias, which is derived from the pair representation $\bm{p}^{(l-1)}$, is used to improve the pair-wise interactions between atoms.
%Besides the standard transformer layer, in self-attention, the attention bias term ${\color{blue} \bm{B}^{(l, h)}}$, projected from pair representation $\bm{p}^{(l-1)}$, is introduced to enhance the  pair-wise interaction.


\paragraph{Update of Pair Representation}
The pair representation $\bm{p}^0$ is initialized by the positional encoding $\bm{\psi}$. The update process of pair representation begins with an outer product of $\bm{x}^{(l)}$, followed by a $\mathcal{O}(n^3)$ triangular multiplication, and is then concluded with an \ffn layer. 
Formally, at $l$-th block, the update process is denoted as: 
\begin{equation}
\begin{aligned}
    \bm{p}^{(l)} &= \bm{p}^{(l-1)} + \opm(\bm{x}^{(l)}); \\
    \bm{p}^{(l)} &= \bm{p}^{(l)} + \tm(\bm{p}^{(l)}); \\
    \bm{p}^{(l)} &= \bm{p}^{(l)} + \ffn(\bm{p}^{(l)}).
\end{aligned}
\end{equation}


The \opm is used for atom-to-pair communication, denoted as:
\begin{equation}
\begin{aligned} 
    \bm{a} &= \bm{x}^{(l)} \bm{W}_{O1}^{(l)}; \\ 
    \bm{b} &= \bm{x}^{(l)} \bm{W}_{O2}^{(l)}; \\
    \bm{o}_{i,j} &= \text{flatten} (\bm{a}_i \otimes \bm{b}_j); \\
    \bm{o} &=  \bm{o} \bm{W}_{O3}^{(l)},  
\end{aligned}
\end{equation}
where $\bm{W}_{O1}^{(l)}, \bm{W}_{O2}^{(l)} \in \mathbb{R}^{d_x \times d_o}$, $d_o$ is the hidden dimension of \opm, and $\bm{W}_{O3}^{(l)} \in \mathbb{R}^{d_o^2 \times d_p}$, $\bm{o}$ is the return result. Please note that $\bm{a}, \bm{b}, \bm{o}$ are temporary variables in the \opm function. 

\tm is used to further enhance pair representation, denoted as:
\begin{equation}
\begin{aligned}
    \bm{a} &= \text{sigmoid} \left(\bm{p}^{(l)} \bm{W}_{T1}^{(l)}\right) \odot \left(\bm{p}^{(l)} \bm{W}_{T2}^{(l)}\right); \\
    \bm{b} &= \text{sigmoid} \left(\bm{p}^{(l)} \bm{W}_{T3}^{(l)}\right) \odot \left(\bm{p}^{(l)} \bm{W}_{T4}^{(l)}\right); \\
    \bm{o}_{i,j} &= \sum_{k} {\bm{a}_{i,k} \odot \bm{b}_{j,k}} + \sum_{k} {\bm{a}_{k,i} \odot \bm{b}_{k,j}};    \\
    \bm{o} &=  \text{sigmoid} \left(\bm{p}^{(l)} \bm{W}_{T5}^{(l)}\right) \odot \left(\bm{o} \bm{W}_{T6}^{(l)} \right),
\end{aligned}
\end{equation}
where $\bm{W}_{T1}^{(l)}, \bm{W}_{T2}^{(l)}, \bm{W}_{T3}^{(l)}, \bm{W}_{T4}^{(l)} \in \mathbb{R}^{d_p \times d_t} $, $\bm{W}_{T5}^{(l)} \in \mathbb{R}^{d_p \times d_p}$, $\bm{W}_{T6}^{(l)} \in \mathbb{R}^{d_t \times d_p}$, and $d_t$ is the hidden dimension of \tm. Please note that $\bm{a}, \bm{b}, \bm{g}, \bm{o}$ and $k$ are temporary variables in the \tm function. The \tm is inspired by the Evoformer in AlphaFold \cite{jumper_highly_2021}. The difference is that AlphaFold uses two modules, ``outgoing'' ($\bm{o}_{i,j} = \sum_{k} {\bm{a}_{i,k} \odot \bm{b}_{j,k}}$) and ``incoming''  ($\bm{o}_{i,j} = \sum_{k} {\bm{a}_{k,i} \odot \bm{b}_{k,j}}$) respectively. In \name's \tm, we merge the two modules into one, to save the computational cost. 

         
\paragraph{3D Position Prediction}
% random angles data augmentation
The ability to predict 3D position is necessary in \name. Whereas in Uni-Mol, an SE(3)-equivariant head is used, in \name, we implement an attention-based head, similar to the one proposed in Graphormer-3D \cite{shi2022benchmarking}. Formally, the 3D position prediction could be denoted as
\begin{equation}
\begin{aligned}
    \bm{P}^{(h)} &= \text{softmax}\left(\frac{\bm{Q}^{(h)}  \left(\bm{K}^{(h)}\right)^T}{d_h} + \bm{B}^{(h)} \right); \\
    \bm{d}_{i,j, k}^{(h)} &= \bm{P}^{(h)}_{i,j} \odot (\bm{r}_i - \bm{r}_j)_k, \quad k \in \{1, 2, 3\}; \\
    \bm{d}_{k} &= \text{concat}_h \left( \bm{d}_{k}^{(h)}  \bm{V}^{(h)} \right); \\
    \bm{\delta}_{k} &= \bm{d}_{k} \bm{W}_O^{k}; \\
    \bm{\hat{r}} &= \bm{r} + \eta \bm{\delta},
\end{aligned}
\end{equation}
where $(\bm{r}_i - \bm{r}_j) \in \mathbb{R}^3$ represents the delta position of pair $(i,j)$, 
$\bm{P}^{(h)}$ is the attention probabilities of the $h$-th head,
$\bm{W}_O^{k} \in \mathbb{R}^{d_x \times 1}$ is a projection matrix, $\bm{\delta}_{k} \in \mathbb{R}^{n \times 1}$ denotes the position update on the $k$-th axis, $\eta$ is a scalar hyperparameter controlling the step-size of the update, and $\bm{\hat{r}} \in \mathbb{R}^{n \times 3}$ represents the predicted 3D position. To simplify the presentation, we have not included the projections to obtain $\bm{Q}^{(h)}, \bm{K}^{(h)}, \bm{V}^{(h)}, \bm{B}^{(h)}$. Please note that $\bm{P}$ and $\bm{d}$ are temporary variables in this function.
The 3D position update takes into account the global rotation of the system by using the difference in positions $(\bm{r}_i - \bm{r}_j)$. However, it does not have the property of SE(3)-equivariance like in Uni-Mol. Despite this, we found it performs better than the SE(3)-equivariant head in Uni-Mol. To mitigate this lack of SE(3)-equivariance, random global rotations are applied to $\bm{r}$ during the training process.

% 

\paragraph{Iterative Optimization}
The optimization process in many conformation optimization applications, such as Molecular Dynamics, is iterative. This approach is also employed in the \name. The number of iterations, denoted as $R$, is a hyperparameter. 
To distinguish the position from different iterations, the initial 3D position is denoted as $\bm{r}^{(0)} = \bm{r}$, and at the $i$-th iteration, the model can be denoted as $ (y, \bm{r}^{(i)} ) = f(\bm{X}, \bm{E}, \bm{r}^{(i-1)} ; \bm{\theta})$. 
It is noteworthy that parameters $\bm{\theta}$ are shared across all iterations, and only $r^{(i)}$ is updated iteratively during the iterations. Moreover, please note that the Iterative Optimization in \name involves only a few rounds, such as 2 or 4, instead of dozens or hundreds of steps in Molecular Dynamics.
% Notably, the parameters $\bm{\theta}$ are shared across all iterations, and only $\bm{r}^{(i)}$ is iteratively updated during iterations. Besides, please note that the Iterative Optimization in \name only contains several rounds, like 2 or 4, instead of dozens or hundreds of steps in DFT/MD. 


% \paragraph{Confidence Prediction} 
% The model's predictions can also include a confidence score, allowing us to select or rank the generated 3D conformations. In practical applications, we can use this confidence score to balance performance and efficiency by using DFT calculations for molecules with lower confidence scores.


\subsection{Model Training}


 \begin{figure*}[!tbp]
 %\vspace{-4pt}
 \centering
\includegraphics[width=0.95\linewidth]{Pictures/unimol_pcq_point.pdf}
%\vspace{-10pt}
\caption{Illustration of the linear trajectory injection (LTI) method for conformational sampling. The leftmost cheap conformation is generated by RDKit/OpenBabel, while the rightmost conformation is the target DFT conformation. LTI assumes a linear trajectory from the left to the right and enables us to sample conformations by controlling the parameter $q$. To highlight the differences between the conformations, we include the target DFT conformation as a reference in translucent gray.} \label{fig:point}
\end{figure*}

In the field of DFT geometry optimization or Molecular Dynamics simulations, a conformation is usually optimized through a step-by-step process, resulting in a trajectory of conformations. However, saving such a trajectory can be expensive, and publicly available datasets often only provide the optimized equilibrium conformations generated by DFT. The provision of a trajectory for training purposes would greatly facilitate the learning of conformation optimization, as intermediate states can be used to guide the model's training. 

To address this issue, we propose a novel approach \emph{linear trajectory injection} (LTI), which generates a pseudo trajectory and samples a random conformation from it as the model input during training. This approach allows us to use a set of equilibrium conformations generated by DFT as a reference for training, which is much more efficient than using full trajectories. 
Specifically, we adopt the assumption that the trajectory from a cheap conformation $\bm{r}$ to a target DFT conformation $\bm{\bar{r}}$ is a linear process. To sample an intermediate conformation along this trajectory, we employ the following sampling method:

\begin{equation}
    \bm{r}^{(0)} = q \bm{r} + (1-q) \left(\bm{\bar{r}} + \bm{c} \right),
\end{equation}
where scalar $q$ is a sampling factor that ranges from 0 to 1. The intermediate conformation $\bm{r}^{(0)}$ is obtained by applying $q$ to the cheap conformation and $(1-q)$ to the target DFT conformation, with the addition of Gaussian noise $\bm{c} \in \mathbb{R}^{n \times 3}$, which has a mean of 0 and standard deviation $\upsilon$ (a hyper-parameter). 
During inference, the sampling factor $q$ is fixed to 1 because the target DFT conformation is unknown. During training, we can design a sampling strategy for $q$.

By utilizing randomly sampled $q$ and noise $\bm{c}$, an extensive range of input conformations $\bm{r}^{(0)}$ can be obtained, which serves as an effective data augmentation. Furthermore, training from an intermediate point in the pseudo trajectory streamlines the process towards achieving the DFT equilibrium conformation. Additionally, through iterative optimization, the model can progressively approach the target DFT equilibrium conformation along the trajectory.

The model takes $\bm{r}^{(0)}$ as input and generates $\bm{r}^{(R)}$ after $R$ iterations. The loss function for 3D conformation prediction is defined as follows:
\begin{equation}
\begin{aligned}
    \bm{d}_{ij}^{(R)} &= \left\lVert \bm{r}^{(R)}_i - \bm{r}^{(R)}_j \right\rVert_2, \quad \bm{\bar{d}}_{ij} = \left\lVert \bm{\bar{r}}_i - \bm{\bar{r}}_j \right\rVert_2, \\
    \mathcal{L}_{3d} &=\gamma_{\text{coord}} \frac{1}{n} \left\lVert \bm{r}^{(R)} - \bm{\bar{r}} \right\rVert_1 + \gamma_{\text{dist}} \frac{1}{n^2} \left\lVert \bm{d}^{(R)} - \bm{\bar{d}} \right\rVert_1,
\end{aligned}
\end{equation}
where $\bm{d}_{ij}^{(R)}$ measures the distance between atoms $i$ and $j$ in the predicted conformation $\bm{r}^{(R)}$, $\bm{\bar{d}}_{ij}$ is the corresponding distance in the ground truth conformation $\bm{\bar{r}}$, and $\gamma_{\text{coord}}$ and $\gamma_{\text{dist}}$ are hyperparameters that control the weight of the coordinate loss and the distance loss, respectively. Notably, the loss is only calculated based on the last iteration, despite the fact that there are $R$ iterations. The gradients are backpropagated through all iterations via $\bm{r}^{(i)}$.

% Given $\bm{r}^{(0)}$ as input, after $R$ iterations, the model predicts $\bm{r}^{(R)}$. The loss function for 3D conformation prediction is denoted as
% \begin{equation}
% \begin{aligned}
%     \bm{d}_{ij}^{(R)} &= \left\lVert \bm{r}^{(R)}_i - \bm{r}^{(R)}_j \right\rVert_2, \quad \bm{\bar{d}}_{ij} = \left\lVert \bm{\bar{r}}_i - \bm{\bar{r}}_j \right\rVert_2, \\
%     \mathcal{L}_{3d} &=\gamma_{\text{coord}} \frac{1}{n} \left\lVert \bm{r}^{(R)} - \bm{\bar{r}} \right\rVert_1 + \gamma_{\text{dist}} \frac{1}{n^2} \left\lVert \bm{d}^{(R)} - \bm{\bar{d}} \right\rVert_1,
% \end{aligned}
% \end{equation}
% which computes the L1 losses on the 3D coordinates and the inter-atomic distances, and $\gamma_{\text{coord}}, \gamma_{\text{dist}}$ are hyper-parameters controlling the loss weights. Notably, although there are $R$ iterations, the loss is only calculated based on the last iteration. And the gradients will backward to all iterations through $\bm{r}^{(i)}$.


% \paragraph{Confidence Prediction} We follow the pLDDT in AlphaFold \cite{} for the confidence score, but with different thresholds. In particular, the per-atom LDDT is defined as:
% \begin{equation}
%     \text{LDDT}_{\text{atom}} = \text{mean} \left(\sum_{t \in \mathcal{T}} \frac{1}{|\mathcal{T}|} \left(  \left\lVert \bm{d}^{(R)} - \bm{\bar{d}} \right\rVert_1 < t\right), \text{axis=-1} \right), \mathcal{T} \in \{0.05, 0.1, 0.2, 0.4 \},
% \end{equation}
% where $\mathcal{T}$ is the set of thresholds, and $\text{LDDT}_{\text{atom}} \in \mathbb{R}^{n}$ is the per-atom LDDT score. Based on atom representation, an additional head is trained to fitting the $\text{LDDT}_{\text{atom}}$. During inference, we use the average of $\text{LDDT}_{\text{atom}}$ of a molecule's atoms to represent the confidence score of the molecule. 

% \paragraph{Misc.}  
Similar to Graphormer~\cite{ying2021transformers}, a virtual atom (node), which connects to all atoms, is used to represent the whole molecule. The quantum property is predicted based on the atom representation of the virtual atom, and L1 loss is used for the training. 
For the conformation learning, we include an additional regularization loss, which limits the global movement of the whole molecule to zero. 


% \paragraph{Input }

% \paragraph{Compare to Transformer-M}

\section{Experiment}

This section presents an empirical analysis of the performance of \name. The configuration of \name is described first, followed by the results obtained from the PCQM4MV2 benchmark. We are currently conducting experiments on additional datasets and performing ablation studies, which will be included in the forthcoming version of this study.

\subsection{Model Configuration}

% We created two versions of \name, which are listed in Table \ref{tab:model}. The first version, \name, has the same model size as the previous Graphormer and Transformer-M models. The second version, \name Large, is a larger model that demonstrates the scalability of the proposed method. 
% Similar to graphormer~\cite{ying2021transformers} and Transformer-M~\cite{luo2022one}, \name consists of 12 layers, with atom representation dimension $d_x=768$, and pair representation dimension $d_p=256$. The number of Gaussian kernels $K$ is set to 128, and the width of Gaussian kernel is set to $9.0$ \AA. In \sa, the number of attention heads is set to $48$, and the hidden dimension of \ffn in atom representation track is set to $768$, and the hidden dimension of \ffn in pair representation track is set to $256$. The hidden dimension in \opm $d_o$ is set to $32$, and the hidden dimension in \tm is set to $32$. The number of iteration $R$ is set to 2, and the position update step size $\eta$ is set to $0.01$.
Similar to both Graphormer~\cite{ying2021transformers} and Transformer-M~\cite{luo2022one}, \name comprises 12 layers with an atom representation dimension of $d_x=768$ and a pair representation dimension of $d_p=256$. The model employs 128 Gaussian kernels, each with a width of $w=9.0$ \AA, and 48 attention heads in the \sa module. The hidden dimension of \ffn in the atom representation track is set to $768$, while that of the pair representation track is set to $256$. Additionally, the hidden dimension in the \opm is $d_o=32$, and the hidden dimension in the \tm is $d_t=32$ as well. The number of iteration $R$ is set to $2$, and the position update step size is set to $\eta=0.01$.
For LTI training strategy, we specified a standard deviation of $\upsilon = 0.2$ for random noise and employed a particular sampling method for $q$. Specifically, $q$ was set to $0.0$ with probability $0.7$, set to $1.0$ with probability $0.2$, and uniformly sampled from $[0.4, 0.6]$ with probability $0.1$. For the weights of loss functions, we set $\gamma_{\text{coord}}$ to $0.2$, $\gamma_{\text{dist}}$ to $1.0$, and the weight for QC property loss to $1.0$. With this setting, the number of parameters of \name is about 52.4M. 

% \begin{table}[t]

%   \caption{The configurations of \name .} \label{tab:model}
%   \centering
%   \small
%   \begin{tabular}{l|cc}
%     \toprule
%     Setting & \name & \name Large \\
%     \midrule
%     Layers $L$ & 12 & 18 \\
%     Atom Repr. size $d_x$ & 768 & 768 \\
%     Pair Repr. size $d_p$ & 256 & 256 \\
%     Gaussian kernels $K$ & 128 & 128 \\
%     Gaussian width $w$ & 9.0 & 9.0 \\
%     Attention heads & 48 & 48 \\
%     Attention head dim $d_h$ & 16 & 16 \\
%     Atom's FFN hidden & 768 & 768 \\
%     Pair's FFN hidden & 256 & 256 \\
%     \opm hidden $d_o$ & 32 & 32 \\
%     \tm hidden $d_t$ & 32 & 32 \\
%     Position update step size $\eta$ & 0.01 & 0.01 \\
%     \#Iterations  $R$ & 2 & 2 \\
%     \#Parameters  & 52M & 77M \\ 
%     \bottomrule
%   \end{tabular}
% \end{table}

\subsection{PCQM4MV2}
\begin{table}[t]
\vspace{-8pt}
  \centering
  % \small
  % 
  \caption{Performance on PCQM4MV2.} \label{tab:pcq}
  \vskip 0.1in
  %\addtolength{\tabcolsep}{-3pt}
    \begin{tabular}{l|c|c|c|c}
    \toprule
    Model & \# param. & \# layers & Valid MAE ($\downarrow$) & Leaderboard MAE ($\downarrow$)  \\
    \midrule
    MLP-Fingerprint~\cite{hu2021ogb} & 16.1M &- & 0.1735 & 0.1760	 \\
    GCN~\cite{kipf2016semi} & 2.0M &- & 0.1379 & 0.1398 \\
    GIN~\cite{xu2018powerful} & 3.8M &- & 0.1195 & 0.1218 \\
    \text{GINE}-$_\text{VN}$~\cite{brossard2020graph, gilmer2017neural,luo2022one} & 13.2M &- & 0.1167 & -\\
    \text{GCN}-$_\text{VN}$~\cite{kipf2016semi, gilmer2017neural} & 4.9M &- & 0.1153 & 0.1152	 \\
    \text{GIN}-$_\text{VN}$~\cite{xu2018powerful, gilmer2017neural} & 6.7M &- & 0.1083 & 0.1084 \\
    \text{DeeperGCN}-$_\text{VN}$~\cite{li2020deepergcn, luo2022one} & 25.5M & 12 & 0.1021 & - \\
    $\text{GraphGPS}_\text{SMALL}$~\cite{rampavsek2022recipe} & 6.2M & 5 & 0.0938 & -\\
    % CoAtGIN~\cite{zhang2022coatgin} & 5.2M &4 0.0933 \\
    TokenGT~\cite{kim2022pure} & 48.5M & 12 & 0.0910 & 0.0919 \\
    $\text{GRPE}_\text{BASE}$~\cite{park2022grpe} & 46.2M & 12 & 0.0890 & -\\
    EGT~\cite{hussain2022global} & 89.3M & 24 & 0.0869 & 0.0872 \\
    $\text{GRPE}_\text{LARGE}$~\cite{hussain2022global} & 46.2M & 18 & 0.0867 & 0.0876 \\
    Graphormer~\cite{ying2021transformers, shi2022benchmarking} & 47.1M & 12 & 0.0864 & -\\
    $\text{GraphGPS}_\text{BASE}$~\cite{rampavsek2022recipe} & 19.4M & 10 & 0.0858 & - \\
    $\text{GraphGPS}_\text{DEEP}$~\cite{rampavsek2022recipe} & 13.8M & 16 & 0.0852 & 0.0862 \\
    GEM-2~\cite{liu2022gem} & 32.1M & 12 & 0.0793 &0.0806 \\
    GPS++~\cite{masters2022gps++} & 44.3M & 16 &  0.0778 & 0.0720 \footnotemark[1] \\
    \midrule
    \multirow{2}{*}{Transformer-M~\cite{luo2022one}} & 47.1M & 12 & 0.0787 & -\\
     & 69M & 18 & 0.0772 & 0.0782  \\
    \midrule
    \multirow{3}{*}{\name} & 27.7M & 6 & 0.0751  & -\\
    & 52.4M & 12 &  0.0708 & -\\
     & 77M &  18 & 0.0701 &  0.0710 \\
    \bottomrule
    \end{tabular}
  \label{tab:binding_overall}
\vskip -0.1in
\end{table}

\footnotetext[1]{GPS++'s leaderboard submission consists of a 112-model ensemble and utilizes the validation data for training.}


\paragraph{Setting}

To evaluate the performance of \name, we tested it on the PCQM4Mv2 dataset from the OGB Large-Scale Challenge \cite{DBLP:conf/nips/HuFRNDL21}. This dataset consists of 3.7 million training molecules, each with a SMILES representation, a DFT-generated 3D conformation, and a HOMO-LUMO gap label. We generated 8 conformations for each molecule using RDKit, at a per-molecule cost of approximately 0.01 seconds. During training, we randomly sampled 1 conformation as input $\bm{r}$ at each epoch, while during inference, we used the average HOMO-LUMO gap prediction based on 8 conformations. We used the AdamW optimizer with a learning rate of $2e\text{-}4$, a batch size of $1024$, $(\beta_1, \beta_2)$ set to $(0.9, 0.999)$, and gradient clipping set to $5.0$ during training, which lasted for 1.5 million steps, with 150K warmup steps. We also utilized exponential moving average (EMA) with a decay rate of 0.999. The training process required around 5 days, powered by 8 NVIDIA A100 GPUs. Additionally, inference on the 147k test-dev set took approximately 7 minutes, utilizing 8 NVIDIA V100 GPUs. 

We incorporate previous submissions to the PCQM4MV2 leaderboard as baselines. In addition to the default 12-layer model, we evaluate the performance of \name with two variants consisting of 6 and 18 layers, respectively, to investigate the impact of varying model parameter sizes.

\paragraph{Result} We summarize the result in the Table~\ref{tab:pcq}. Our observations are as follows: 1) All three variants of \name exhibit significant performance improvements over previous baselines. 2) Despite having considerably fewer model parameters, the 6-layer \name still outperforms all previous baselines. 3) Further increasing the layers from 6 to 12 yields a substantial accuracy improvement, surpassing all baselines by a large margin. 4) The 18-layer \name demonstrates the highest performance, outperforming all baselines by a significant margin. These outcomes are a clear testament to the efficacy of \name. 5) The performance of the single 18-layer \name model on the leaderboard (test-dev set) is remarkable, especially considering that it has outperformed previous state-of-the-art methods without using an ensemble or other additional techniques. In comparison, the previous SOTA GPS++ relied on a 112-model ensemble and incorporated the validation set for training.





% our L9, 40M, 0.715
% our L6, 27.7M, 
% our L3, 15.4M,
% our L1, 7.2M


%\subsection{Ablation Study}
% 1. our model backbone
% 2. training strategy
% 3. recycling times
% 4. our model backbone + transformer-m
% 5. RDKIT input only.

\subsection{Discussion}


\paragraph{Computational Cost} The computational complexity of \name is directly linked to the number of atoms $n$ in the system. When the number of atoms is small, such as in small molecules, the runtime cost is acceptable. For example, in PCQM4MV2, the per-molecule cost is approximately 0.02 seconds when utilizing a single V100 GPU. However, for larger systems, we can decrease the cost in two ways: firstly, by removing the \tm component, which is the most time-consuming aspect of \name; and secondly, by reducing self-attention and pair-representation from fully connected pairs $n \times n$ to top-k-nearest cutoff (or radius cut-off) pairs $n \times k$.

\paragraph{Initial Conformation} In an ideal scenario, \name would learn how DFT optimizes conformations, and it would be preferable to use the input conformation for DFT as the input for \name. Unfortunately, public datasets like QM9 and PCQM4MV2 do not provide initial conformations, so \name relies on RDKit to generate them in these datasets. However, running DFT with these RDKit's generated conformations may not yield the same equilibrium conformations as in the datasets. If the actual initial DFT conformation were provided in these benchmarks, the performance of \name would be further improved.

% \paragraph{More Iterations} Considering the computational cost, currently the number of iteration $R$ only set to 2. With more iterations, ideally we can 


\section{Conclusion}

This paper presents \name, a new model to improving the accuracy of quantum chemical property prediction through deep learning. Our method optimizes conformations generated by cheap methods towards equilibrium ones generated by DFT, to predict quantum chemical properties with high accuracy. To facilitate effective learning of the conformation optimization process, we developed a new two-track Transformer backbone and linear trajectory injection training task. Our experimental results demonstrate the effectiveness of \name and our belief that it has significant potential for future research in this field.




% {
% \small
% \printbibliography
% }

\bibliographystyle{plain}
\bibliography{ref}





% \appendix


% \section{Appendix}


% Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
% This section will often be part of the supplemental material.


\end{document}
