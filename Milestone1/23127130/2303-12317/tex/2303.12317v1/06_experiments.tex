\begin{figure*}[t]
\centering
\vspace*{-0.5cm}
\includegraphics[width=0.98\linewidth]{figure/experiments/bar_graph/cvpr_graph_2.pdf}
\vspace{-5pt}
\caption{Winning percentage across six categories. We also added defeat percentage, the black hatched bar that represents the percentage at which \algname{} has been defeated by each baseline. Among total experiments, only statistically reliable values ($t$-score $>$ 2.776) are considered.
Thus, the lower value of the colored bar and the higher value of the black bar indicate a more comparable baseline.}
\label{fig:bar_graph}
\vspace*{-3pt}
\end{figure*}

\vspace{-0.2cm}
\section{Evaluation}
\label{sec:evaluation}

\subsection{Experimental Configuration}
\label{app:implementation_detail}

\paragraph{Training Settings.}
In a FAL framework, a central server should carefully consider the fairness of labeling and training costs between clients. Therefore, we assume that ten clients have the same size of unlabeled data pool and query the same number of instances per every AL round.
Moreover, we focus on a cross-silo FL setting\cite{survey_fl} where every client participates in every FL round.

We compared \algname{} with six active learning strategies on five benchmark dataset (CIFAR-10\cite{cifar}, SVHN\cite{svhn}, PathMNIST, DermaMNIST, and OrganAMNIST\cite{medmnist}).
We considered 38 comprehensive experimental settings combined into six categories (see Figure\,\ref{fig:bar_graph}).  
Except for learning ablations of the architectures, labeling budgets, and initialization schemes, in default, we implemented four layers of CNN and trained the encoder from scratch with the labeling budget of 5\% for every AL round. 
We repeat all experiments four times and report their averaged values. Refer to Appendix \ref{sec:exp_settings} for detailed experimental settings.

\vspace{-0.4cm}
\paragraph{Baselines.}

\begin{figure}[t!]
 \centering
 \includegraphics[width=0.85\linewidth]{figure/experiments/comparison/total_comparison.pdf}
 \vspace*{-0.1cm}
\caption{Pairwise penalty matrix over 38 experimental settings. The value $P_{ij}$ indicates the number of times that the $i$-th strategy outperforms the $j$-th strategy (\ie, sum of ${\sf win}^{ij}$ in Eq.\,\eqref{eq:cell_value} over 38 settings).
The last row is the average number of times the j-th strategy is defeated by the rest strategies; the lower, the better.
}
 \vspace*{-0.3cm}
\label{fig:comparision_matrix}
\end{figure}

We considered six standard AL strategies.
\emph{Random} randomly samples the instances from the unlabeled pool. 
\emph{Entropy} selects instances with the largest entropy\cite{confidence_sampling}.
\emph{CoreSet} chooses the small subset that can represent the whole unlabeled set\cite{coreset}.
\emph{BADGE} selects the diverse points with high magnitude in a hallucinated gradient space\cite{badge}.
\emph{GCNAL} adapts CoreSet on a sequential graph convolution network to measure the relation between labeled and unlabeled instances\cite{gcnal}.
\emph{ALFA-Mix} identifies the valuable instances by seeking inconsistencies in the prediction of mixing features\cite{alfa_mix}.
We adopt these sampling strategies with either the global model or local-only model in FedAvg pipeline.
Refer to Appendix\,\ref{sec:computational_cost} for the comparison of query selection cost. Besides, we summarized the results with various FL methods in Appendix\,\ref{sec:various_fl_algo}.


\subsection{Overall Comparison}
\label{exp:baselines}
\subsubsection{Pairwise Penalty Matrix}
We summarized the overall comparison results as a pairwise penalty matrix $P$ in Figure\,\ref{fig:comparision_matrix}, following the recent work\cite{badge, alfa_mix}.
Each cell of the pairwise penalty matrix represents the summation of the winning rate of Eq.\,\eqref{eq:cell_value} calculated above for each of the 38 experimental sets.
As the rows of the matrix $P_i$ indicate how many times $i$-th algorithm outperforms the other algorithms, the brighter color means the better algorithm (conversely, the darker columns are the better).
Note that we only considered statistically reliable results throughout the two-sided t-test.

\algname{} defeats all baselines in general (see 7-th row values) while under-performed only 0.9 out of 38 times on average (see 7-th column values in the last row).
In particular, \algname{} outperforms BADGE and Entropy sampling, which are top-2 baselines, with 13.7 and 12.0 out of 38 times, while \algname{} only loses 2.4 and 1.9 out of 38 times, respectively. 
This results demonstrate the robust effectiveness of our LoGo algorithm in various experimental settings.


\vspace{-8pt}
\subsubsection{{Winning Rate Bar Plots}}
\vspace{-1.5pt}

Figure \ref{fig:bar_graph} summarizes the comparison results according to the six categories, which is the breakdown of Figure\,\ref{fig:comparision_matrix} on a more systematic categorization.
For example, the colored bar of the overall category is calculated by dividing the total setting number of 38 in the 7-th row vector of Figure\,\ref{fig:comparision_matrix} (the black bar is from the 7-th column vector).
The colored bar represents the average percentage at which \algname{} defeats each baseline algorithm, and the higher bar means that \algname{} has won more.
Appendix\,\ref{sec:detail_comp_matrix} provides the detailed comparison penalty values according to each category.

Overall, \algname{} consistently shows an overwhelming winning percentage toward any baseline against the defeat percentage.
Interestingly, ALFA-Mix, the latest AL strategy, shows performance considerably varying depending on which query selector is selected (see the query selector in Figure\,\ref{fig:bar_graph}).
The reason would be that a local-only model, which is separately trained on the highly heterogeneous dataset, is not appropriate for the feature mixing or the sensitivity to lots of hyperparameters.
Moreover, Coreset, which does not consider uncertainty, cannot resolve the local and global imbalance, showing similar performance as Random sampling.
Thus, \algname{} is a superior algorithm for a FAL framework in that it is robust to the type of query selector or most combinations of experimental settings. 

\input{table/benchmark_comparision.tex}

\vspace*{-0.15cm}
\subsubsection{Under Query Selector Category}
Table\,\ref{tab:acc_comparision} shows the test accuracy according to the increased labeling budgets over rounds on four datasets.
Even with the same active learning strategy, a gap in test accuracy occurs depending on which query selector is used because the global imbalance varies by $1.0$--$58.7$ across datasets.
For example, in general, global models outperform local-only models for SVHN and DermaMNIST, while the opposite trend is observed for CIFAR-10 and PathMNIST.
However, irrespective of the benchmarks and querying model types, our \algname{} shows the best performance in most cases.
Two-step selection strategy enables \algname{} to be robust by utilizing both the benefits of global and local-only models.
Because we cannot know the degree of local and global imbalance in advance, our proposed method has a strong advantage in providing data-agnostic performance improvements over all baselines.
Appendix\,\ref{sec:detail_comp_performance} provides a detailed performance comparison between \algname{} and baselines under various experimental settings.


\subsection{\algname{} vs. Simple Ensemble}
\label{sec:simple_ensemble}
To prove that \algname{} is an effective way to leverage the advantages of both models, we compared \algname{} with three na\"ive implementation of ensemble methods for global and local-only models: (1) the average of logits\,(for Entropy sampling) or gradient embedding\,(for BADGE) from two models, (2) weighing the instances based on the selection ranks\,(\ie, more weights if it is selected from both models), and (3) fine-tuning the global model on local datasets.

In Table\,\ref{tab:compare_ens}, \algname{} consistently shows better classification accuracy over increasing labeling budgets than three counterparts. 
Compared with the results in both Tables\,\ref{tab:acc_comparision} and \ref{tab:compare_ens}, all three ensemble methods show lower performance than using a single superior query selector.
That is, the na\"ive ensemble suffers from a performance trade-off between two query selector models and, therefore, their results fall somewhere in the middle of using global and local models.

\input{table/ens_comparision}

 