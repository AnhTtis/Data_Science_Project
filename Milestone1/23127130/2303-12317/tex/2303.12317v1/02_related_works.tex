\section{Related Work}

\noindent\textbf{Federated Learning} is a collaborative learning framework with multiple clients while maintaining the privacy of each client dataset.
In this decentralized framework, FedAvg\cite{fedavg} is considered a de facto algorithm in which a server and clients communicate only model parameters efficiently.
One of the main bottlenecks for FL is the statistical heterogeneity problem across the clients' dataset, as the weight divergence from heterogeneous distributions hinders convergence during the aggregation scheme.
Hence, several algorithms have tackled the local heterogeneity through the alignment between the local-updated gradient and the aggregated gradient, as a form of correction term\cite{scaffold, feddc}, regularization loss\cite{fedprox, feddyn}, or distillation-based loss\cite{lee2021preservation, li2021model}.
Furthermore, there are existing literature to alleviate the class imbalance on the local side and they eventually achieve the global balance via rebalancing datasets~\cite{astraea} or a weighted loss function with a monitoring scheme~\cite{wang2021addressing}.


\smallskip\smallskip
\noindent\textbf{Active Learning} minimizes the labeling effort by querying the most informative instances from the unlabeled data pool. 
There are three major types of active learning strategies, namely uncertainty-based sampling, diversity-based sampling, and hybrid strategy.
Uncertainty-based sampling queries the most uncertain instances that lie on the current decision boundary\cite{margin_sampling, confidence_sampling, deep_bayesian_al, deepfool}, while diversity-based sampling selects a set of unlabeled instances that represents the entire unlabeled data distribution\cite{coreset, ff_active}. 
Recently, hybrid strategies simultaneously consider both uncertainty and diversity.
BADGE\cite{badge} utilized the gradient embedding as the uncertainty measure and selected the diverse query set by a \textit{k}-means++ initialization scheme\cite{kmeans}.
Several hybrid AL methods are based on a submodular data subset selection\cite{fass}, pairwise contextual diversity\cite{cdal}, or feature mixing\cite{alfa_mix}.
In addition to the three categories, model-based strategies have also been recently proposed to train additional networks for query selection, such as a VAE and discriminator\cite{vaal} or a sequential graph neural network\cite{gcnal}. 
%

\smallskip\smallskip
\noindent\textbf{Federated Active Learning} has been recently studied to address a more realistic scenario, where clients have lots of the unlabeled instances\cite{fedema, fedu, fedmatch}. 
However, previous works\cite{fal_waste_disaster, f_al} have not deeply discussed the challenges of FAL framework and na\"ively applied existing AL strategies. Ahn \etal\cite{f_al} have even considered the local-only query selector, but concluded that the global model is superior to the local-only model with limited experimental settings, three benchmarks and one heterogeneity level.
In this work, we observe the counterparts in varying benchmarks and heterogeneity levels, which motivates us to design a novel sampling strategy by exploiting both global and local-only models. Our extensive analysis and experimental results encourage future research on the FAL problem.
