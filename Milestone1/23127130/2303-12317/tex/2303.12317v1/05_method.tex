\section{Method}
\label{subsec:lg_fal}

From three observations in Section \ref{sec:analysis}, we confirmed that FAL framework requires delicate consideration of local and global inter-class diversity. 
However, since clients are reluctant to share their data information, we should simultaneously utilize local-only and global models to ensure both sides of inter-class diversity.
To this end, we propose a novel query sample strategy, named \algname{}, that composed of \textit{macro} and \textit{micro} steps.
Pseudo algorithm for \algname{} is provided in Appendix\,\ref{sec:pseudo_algorithm}.
Before describing details of each step, let us assume a scenario where $k$-th client queries $B$ unlabeled instances at the round of $r$.



\smallskip \smallskip
\noindent 
\textbf{Macro Step: Clustering with Local-Only Model.} \\
\indent The ultimate goal of macro step is to satisfy local inter-class diversity by primarily sampling the informative instances by the \emph{local-only} model.
In detail, we introduce $k$-means clustering\cite{k_means} on the hypothetical gradient embedding.
Let $z$ be the embedding vector before forwarding to the last layer $W$ for $x \in U_k^r$.
Here, we utilize the gradient of negative cross-entropy loss, induced by a pseudo label, with respect to the last layer of encoder as follows:
\vspace{-1pt}
\begin{equation}
g^x_{c} = -\frac{\partial}{\partial W_{c}} \ell_{CE}(x, \hat{y}; \Theta_{k*}^r) = z\!\cdot\!(\mathbbm{1}_{[\hat{y}=c]} - p_{c}),
\label{eq:gradient}
\vspace{-1pt}
\end{equation}
where $\hat{y} = \arg\max_{c \in [C]} \,p_c$ and $W_{c}$ is the weights connected to $c$-th neuron of logits. 
Gradient embedding is widely used in conventional AL algorithm\cite{badge, venkatesh2020ask}, and we consider only the gradients corresponding to the pseudo label (\ie, $g^x_{\hat{y}}$) for computation efficiency.

Then, we calculate $B$ number of centroids on the hallucinated gradient space via EM algorithm\cite{dempster1977maximum} of $k$-means clustering by minimizing
\vspace{-1pt}
\begin{equation}
    J = \sum_{i=1}^{N} \sum_{b=1}^{B} w_{ib} \lVert g^{x_i}_{\hat{y}} - \mu_b  \rVert^2,
\label{eq:kmeans}
\vspace{-1pt}
\end{equation}
where $w_{ib}$ is an indicator function whether $g^{x_i}_{\hat{y}}$ is assigned to $\mu_b$ for E-step.
Eq.\,\eqref{eq:gradient} shows that the gradient embedding is a just scaling of feature embedding $z$, especially with the scale of uncertainty. 
In other words, if an instance is uncertain to predict\,(low value of $p_{\hat{y}}$), its gradient will be much highly scaled.
In this sense, Eq.\,\eqref{eq:kmeans} can be regarded as a weighted $k$-means clustering\cite{duda2006pattern, spath1980cluster} on the feature embedding space by using the function of softmax response\cite{geifman2017selective}. 
Hence, the macro step of \algname{} enable the query set to contain both diversity in the embedding space and uncertainty from the perspective of local-only model.

\smallskip \smallskip
\noindent 
\textbf{Micro Step: Cluster-wise Sampling with Global Model.}

\indent
In micro step, the \emph{global model} selects the final instances that results in the higher global inter-class diversity.
Given $B$ selected instances from the macro step, the most uncertain instance is selected for each cluster:
\begin{equation}
L^{r}_k = \{\mathcal{A}(\mathcal{C}_1, \Theta^{r*}, 1), ..., \mathcal{A}(\mathcal{C}_B, \Theta^{r*}, 1)\}
\label{eq:micro_step}
\end{equation}
$C_b$ denotes $b$-th cluster, generated by the query set from the macro step. We simply used Entropy sampling for $\mathcal{A}$, but we should note that $\mathcal{A}$ can be any AL sampling strategy. 

The cluster-wise sampling in the micro step is a simple yet effective strategy to leverage the advantages of both query selector models, ensuring local and global inter-class diversity. 
Here, we discuss how \algname{} can be a promising sampling strategy for the FAL framework.

\begin{remark}
The micro step is same with one step of EM algorithm. We first update cluster assignments with $B$ number of centroids from the macro step\,(E-step).
Then, within each cluster, we select the most informative instance based on the uncertainty score. This can be regarded as one M-step of the weighted $k$-means clustering, using infinitely scaled weight function of uncertainty measure.
\label{remark1}
\end{remark}

\noindent
Then, let $c_b$ be a centroid of $C_b$ and we define $M$ as follows:
\begin{equation}
    M = \sum_{b=1}^B \| c_b - \Tilde{x}_b \|^2,\,\, \text{where  } \Tilde{x}_b = \arg\min_{x} \| c_b - x \|^2
    \label{eq:remark_2}
\end{equation}
where $x \in L_k^r$ (Eq.\,\eqref{eq:micro_step}) and $\Tilde{x}$ is one-to-one mapping with the minimal transport cost.
The lower value of $M$ means that the final query set much consider the diversity in the macro step.
\begin{remark}
EM-based sampling in the micro step guarantees the diversity information of the macro step.
Through sampling one instance per cluster, each $\Tilde{x}_b$ is disjointly assigned to one cluster. Therefore, \algname{} conservatively ensures the local inter-class diversity from the macro step (\ie, lower $M$ value), comparing to any other strategy that query at least two instances for one cluster.
\label{remark2}
\end{remark}