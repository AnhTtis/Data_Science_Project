\vspace{-9pt}
\section{Introduction}

Federated learning\,(FL) is a distributed framework that allows multiple parties to learn a unified deep learning model cooperatively with preserving the privacy of the local client\cite{fedavg, fedprox, scaffold}.
Typically, FL has been actively studied in a standard supervised learning setting, where all the training instances are labeled, but it is more realistic for each client to contain both labeled and unlabeled data due to the high labeling cost\cite{fedema, orchestra}. 
Here, active learning\,(AL) can be a promising solution to improve the performance of a cooperated model with the pool of unlabeled data.
In practice, federated active learning\,(FAL) framework has recently attempted to bridge two different philosophies in FL and AL\cite{fal_waste_disaster, f_al}.
As illustrated in Figure\,\ref{fig:overview}-(a) FAL framework alternates an FL procedure\,(red line) of training a predictive model collaboratively through local updates and aggregation phases, and an AL procedure\,(green line) of querying and annotating informative instances separately per client.

Although the overall framework just appears to be a straightforward fusion of two research fields, FL factors introduce two major challenges to the AL procedure.
\textbf{First}, the class imbalance of the local dataset originates from heterogeneous distribution across local clients\cite{fedprox, scaffold, feddc}.
Hence, in the FAL framework, the active selection algorithm has to ensure \emph{inter-class} diversity from both local and global perspectives.
\textbf{Second}, there are two available types of query-selecting models, a \emph{global} model, which is globally optimized through the FL pipeline, and a \emph{local-only} model\cite{fedbabu, fedrep}, which can be separately trained only for each client. 
In the query selection phase, the global model can leverage the aggregated knowledge of all clients, while the local-only model is able to detect the most valuable instances for the local updates.

\begin{figure*}[t!]
\vspace{-2pt}
\centering
\begin{subfigure}[b]{0.435\linewidth}
\centering
\includegraphics[width=\linewidth]{figure/introduction/overview_clear2.pdf}
\caption{\small {Federated Active Learning framework.}}
\label{fig:overview_left}
\end{subfigure}
% \hfill
\hspace{10pt}
\begin{subfigure}[b]{0.435\linewidth}
\centering
\includegraphics[width=\linewidth]{figure/introduction/main_figure1b.pdf}
\caption{\small {Superiority change of query selector.}} 
\label{fig:overview_right}
\end{subfigure}
\vspace*{-0.15cm}
\caption{Motivation: (a) The red and green lines correspond to the conventional FL and AL framework. We focus on a sampling strategy for FAL\,(green box) with the considerations of hierarchy structure and two available query-selecting models. (b) For a fixed querying strategy\,(Entropy sampling), the performance gap occurs only by changing the query selector. The y-axis is the gap in the winning rate in total active rounds\,(refer to Section\,\ref{sec:analysis}). Closer to 1 indicates that global models outperforms than local-only models, and -1 is the opposite. }
\label{fig:overview}
\vspace*{-5pt}
\end{figure*}

Prior FAL literature\cite{fal_waste_disaster, f_al}, which simply adapt conventional AL strategies, had little discussion on these challenges.
As our first contribution, we found the significant performance gap between two types of query selector\,(see Figure\,\ref{fig:overview}-(b)), and it is the first study to solve a conundrum of dominance trend
by introducing two indicators of inter-class diversity\footnote{We use the term inter-class diversity interchangeably with class balance throughout the paper.}-- \textit{local heterogeneity level}\,($\alpha$) and \textit{global imbalance ratio}\,($\rho$). 
The first indicator $\alpha$ is the concentration parameter of Dirichlet distribution, commonly seen in the FL literature\cite{scaffold, feddyn, feddc}, resulting in more locally imbalanced class distribution at lower values.
Besides, $\rho$ indicator is the ratio of class imbalance for the aggregated global data of all clients\cite{buda2018systematic, johnson2019survey}.
We discovered three meaningful insights on selector dominance:
\textbf{(Obs.\,\ref{obs:obs1})} Interestingly, the superiority of the two selectors varies depending on the two indicators of inter-class diversity, $\alpha$ and $\rho$.
\textbf{(Obs.\,\ref{obs:obs2})} When local heterogeneity is severe\,($\alpha$ is low), a local-only model is preferred for weighing minority instances for each client, and \textbf{(Obs.\,\ref{obs:obs3})} when globally minor classes exist\,($\rho$ is high), the knowledge of the entire data distribution, inherent in a global model, is more essential.\,\,
$\vartriangleright$ See Section\,\ref{sec:analysis}


In a real FAL scenario, the superiority of two query models for a given dataset cannot be known in advance due to privacy preservation. Therefore, as our second contribution, we design a simple yet effective FAL querying approach, \textbf{\algname{}}, that simultaneously leverages local-only and global models, to be robust to varying heterogeneity levels and global imbalance ratios.
\algname{} is a clutering-based sampling strategy, which is composed of {macro} and {micro} step exploiting local-only and global models, respectively. 
The rationale behind our method is that the optimal querying policy needs to evaluate the informativeness of instances with \emph{both} models, which implicitly learn local and global data distribution, respectively.
In a \emph{macro step}, to improve the local inter-class diversity first, we perform $k$-means clustering\cite{k_means} in hallucinated gradient space generated from local-only models. 
Then, in a \emph{micro step}, the final query set is determined via one step of the EM algorithm\cite{dempster1977maximum}, making cluster boundaries using instances from macro step\,(E-step) and cluster-wise sampling with the global model\,(M-step).
The proposed cluster-wise sampling conservatively guarantees the diversity information of the macro step, \ie, the local inter-class diversity obtained by local-only models, while also considering the global minority classes via the global model.\,\,
$\vartriangleright$ See Section\,\ref{subsec:lg_fal}

As our third contribution, we conduct \emph{a total number of 38 experiments} on five datasets using seven AL strategies including our \algname{} algorithm.
To verify the superiority of our method in real-world scenarios, we build comprehensive combinations of six categories, including query selector types (local-only vs. global models), local heterogeneity levels ($\alpha \in [0.1, \infty)$), global imbalance ratios ($\rho \in [1, 58])$, model architectures, budget sizes, and model initialization scheme. As a result, the experimental results empirically prove our three observations\,(Obs.\,\ref{obs:obs1}--\ref{obs:obs3}). Besides, our method outperforms all other AL baselines and na\"ive implementations for an ensemble of two query selectors in extensive experimental settings.\,\,
$\vartriangleright$ See Section\,\ref{sec:evaluation}