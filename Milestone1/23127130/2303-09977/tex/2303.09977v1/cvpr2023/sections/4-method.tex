% --------------------------------
\section{Cleaner Self (CleanerS)}
% --------------------------------
In this section, we introduce the implementation details of our approach CleanerS.
Section~\ref{ssec:framework} presents the overall framework of CleanerS. 
Section~\ref{ssec:distillation} elaborates on the feature-based as well as the logit-based KD pipelines implemented in CleanerS. 
Section~\ref{ssec:loss} introduces the overall training losses used in CleanerS.

% --------------------------------
\subsection{Overall Framework} 
\label{ssec:framework}
% --------------------------------
Figure~\ref{fig:framework} illustrates the overall framework of CleanerS, consisting of two networks: a teacher network $T$, and a student network $S$. The two networks share the same architectures of 2D and 3D processing modules.
%
The training process of CleanerS includes the following two steps. \emph{Step 1} is a fully supervised training of $T$ on noise-free depth values that are rendered from 3D voxels ground-truth. \emph{Step 2} includes the fully supervised training of $S$ on noisy depth values, as well as feature-based and logit-based KD training between teacher $T$ and student $S$. 
%

Specifically, for each network, the {input} consists of an RGB image and a TSDF volume (noise-free TSDF-CAD for $T$, and noisy TSDF for $S$). The outputs are 3D voxels, each of which contains volumetric occupancy and semantic labels. 
% 
In the following, we introduce the three modules in the network: 
% 
1)~Image feature extraction and reformation module. This module has a 2D network to extract image features (a D-channel 2D feature $\mathbf{F}_r$) and a 2D to 3D projection layer that translates 2D feature to 3D feature $\mathbf{V}_r \in \mathbb{R}^{D \times G_X \times G_Y \times G_Z}$. $G_X,G_Y,G_Z$ are the sizes of 3D voxels. The 2D to 3D projection layer is used to lift the feature vector at 2D position $(u,v)$ in $\mathbf{F}_r$ to the corresponding 3D position $(x,y,z)$ according to the depth values of all visible surfaces (otherwise filled by zero vectors).
%
2)~TSDF feature extraction module. This module transforms the 1-channel 3D input TSDF into a D-channel 3D feature $\mathbf{V}_t$, where $\mathbf{V}_t \in \mathbb{R}^{D \times G_X \times G_Y \times G_Z}$.
3)~SSC prediction module. This module is used to fuse both features $\mathbf{V}_r$ and $\mathbf{V}_t$ by element-wise addition, and then makes predictions via 3D-SSCNet~\cite{chen20203d}. 
%
The prediction is denoted as $\hat{\mathbf{Y}} \in \mathbb{R}^{(C + 1) \times G_X \times G_Y \times G_Z}$, where $C$ is the number of classes in the dataset, ``$+1$" is for the volumetric occupancy, \ie ``empty" or not.
% 

% --------------------------------
\subsection{Distilling ``Cleaner'' Knowledge} 
\label{ssec:distillation}
% --------------------------------
% 
In the following, we introduce the implementation details of {feature-based} KD and {logit-based} KD.

% --------------------------------
\noindent
\textbf{Feature-Based KD.} 
% --------------------------------
% 
We use TSDF features for feature-based KD, as follows,
% 
% --------------------------------
\begin{equation} 
\label{eq:distsdf}
L_{KD-T} = MSE(\mathbf{V}_t^S, \mathbf{V}_t^T),
\end{equation}
% --------------------------------
where $MSE$ denotes mean square error. $\mathbf{V}_t^S$ and $\mathbf{V}_t^T$ are the TSDF features output by the 3D-TSDFNets of student and teacher, respectively.
% 
For visualization (Figure~\ref{fig:framework}), these features are flattened and normalized with a softmax layer along the spatial dimension, and then re-scaled their values into $(0,1)$. In the ``Feature-based KD'' block of Figure~\ref{fig:framework}, we visualize the features of large values ($>0.1$). 
%
%
Intuitively, minimizing $L_{KD-T}$ encourages the student to learn to complete the visible ``hole'' in $\mathbf{V}_t^S$ (as shown by the red dashed circle in Figure~\ref{fig:framework}). 
% 

\noindent
\textbf{Logit-Based KD.}
% --------------------------------
Teacher and student have different inputs, \ie, clean TSDF-CAD \vs noisy TSDF. This results in a large output gap. It is thus ineffective if 
forcing straightforward distillation between teacher's and student's prediction logits. We show an empirical validation in Section~\ref{ssec:ablation}.
% 
On the other hand, teacher and student have the common RGB image input, and aim to predict 3D voxels of the same semantic scene. 


Motivated by these, we design two strategies, Semantic-Centered distillation (\emph{KD-SC}) and Semantic Affinity distillation (\emph{KD-SA}), to distill clean semantic knowledge from teacher to student via their prediction logits. 
\emph{KD-SC} is a semantic-centered distillation to transfer global semantic knowledge (from teacher to student).
But it ignores local structures.
%
\emph{KD-SA} is complementary to \emph{KD-SC}. It is a voxel-to-center affinity distillation to transfer the local structure knowledge. 
Both of them are based on prediction logits, denoted as $\bar{\mathbf{Y}}^T$ (teacher) and $\bar{\mathbf{Y}}^S$ (student), as elaborated in the following.

\noindent
\textbf{KD-SC.}
% 
Given an input sample, we use its ground-truth $\mathbf{Y}$ to generate a binary foreground mask $\mathbf{M}_c \in \mathbb{R}^{G_X \times G_Y \times G_Z}$ for each class $c$. In $M_c$, each voxel stores $1$ if its semantic label is $c$, and otherwise $0$.
%  
Then, we distill the semantic-centered logits for each sample by using the loss:
% --------------------------------
\begin{equation} 
\label{eq:lossscenter}
L_{KD-SC} = \frac{1}{N_c} \sum_{c=0}^{C} KL(\bar{\mathbf{y}}_c^S, \bar{\mathbf{y}}_c^T),
\end{equation}
% --------------------------------
where $KL$ is the Kullbackâ€“Leibler divergence. $N_c \leq (C + 1)$ is the number of classes appearing on the input sample. $\bar{\mathbf{y}}_c^S$ and $\bar{\mathbf{y}}_c^T$ are the semantic-centered logits of class $c$ in $S$ and $T$, respectively
% 
These logits are calculated based on the prediction logits by applying a masked average pooling along the spatial dimension. E.g., in the student network, they can be derived as:
% 
% --------------------------------
\begin{equation} 
\label{eq:scenter}
\bar{\mathbf{y}}^S_c = \frac{\sum_{i=1}^ {G_X \cdot G_Y \cdot G_Z} (\bar{\mathbf{Y}}^S[i] \cdot \mathbf{M}_c[i])}{\sum_{i=1}^ {G_X \cdot G_Y \cdot G_Z} \mathbf{M}_c},
\end{equation}
% --------------------------------
where $i$ is the voxel index along the spatial dimension. Due to delta noises, $\bar{\mathbf{y}}_c^S$ will include certain features of the voxels that do not belong to the class $c$ into the average pooling. This results in noisy semantic-centered logits.
% 
Similarly, we can calculate $\bar{\mathbf{y}}_c^T$ for the teacher network, which is ``cleaner'' than $\bar{\mathbf{y}}_c^S$.
% 

\noindent
\textbf{KD-SA.}
To encode local structures, we calculate the voxel-to-center affinity matrices between every voxel and the semantic-centered logits, and then conduct the distillation from $T$ to $S$, as follows,
% 
% --------------------------------
\begin{equation} 
\label{eq:lossaffinity}
L_{KD-SA} = \frac{1}{N_c} \sum_{c=0}^C MSE(\mathbf{A}_c^S, \mathbf{A}_c^T),
\end{equation}
% --------------------------------
where $\mathbf{A}_c^S$, and $\mathbf{A}_c^T$ are the voxel-to-center affinity matrices, which are calculated in the student and teacher networks, respectively.
E.g., in the student network, the affinity score $a_c^S \in \mathbf{A}_c^S$ is calculated between the semantic-centered logits $\bar{\mathbf{y}}_c^S$ and the logits of each voxel $\bar{\mathbf{y}}^S \in \bar{\mathbf{Y}}^S$ as:
% --------------------------------
\begin{equation} 
\label{eq:saffinity}
a_c^S = cos(\bar{\mathbf{y}}_c^S, \bar{\mathbf{y}}^S),
\end{equation}
% --------------------------------
where $cos$ denotes cosine similarity, $\mathbf{A}_c^S \in \mathbb{R}^{G_X \times G_Y \times G_Z}$ is the resultant affinity matrix. The higher affinity value indicates a stronger semantic correlation between the voxel and the center of its label class, and vice versa. 
% 

% ------------------------------
\input{cvpr2023/Tables/tab_sota}
% ------------------------------

\subsection{Overall Loss} 
\label{ssec:loss}
% 
The overall loss of CleanerS consists of two parts: SSC loss and KD loss. 
% 
The SSC loss is applied in both teacher and student networks. First, we minimize the difference between prediction $\hat{\mathbf{Y}}$ and ground-truth $\mathbf{Y}$. Then, we incorporate the following additional supervision for 2DNet. For the 2D feature $\mathbf{F}_r$, we add a $3\times3$ convolution for 2D semantic prediction, which outputs $\hat{\mathbf{Y}}_{2D} \in \mathbb{R}^{(C + 1) \times H \times W}$. 
% 
The 2D semantic ground-truth is noted as $\mathbf{Y}_{2D}$, the class of each pixel at 2D coordinate $[u,v]$ is $\mathbf{Y}[x,y,z]$, where $[x,y,z]$ is the corresponding 3D position after 2D-3D projection. 
% 
Hence, the SSC loss is formulated as:
% ------------------------------
\begin{equation} 
\label{eq:lossSSC}
L_{SSC} = SCE(\hat{\mathbf{Y}}, \mathbf{Y}) + \lambda SCE(\hat{\mathbf{Y}}_{2D},\mathbf{Y}_{2D}),
\end{equation}
% ------------------------------
where $SCE$ denotes smooth cross entropy loss~\cite{szegedy2016rethinking}, and $\lambda$ is a coefficient to balance between 3D and 2D semantic losses.
% 
The KD loss is applied in the student network. 
Therefore, the overall loss for the student network can be formulated as:
% ------------------------------
\begin{equation} \label{eq:lossall}
L_{all}^S = L_{SSC} + \beta \cdot (L_{KD-T} + L_{KD-S}),
\end{equation}
% ------------------------------
where $L_{KD-S} = L_{KD-SC} + L_{KD-SA}$ is the logit-based KD loss, and $\beta$ is a coefficient to balance between SSC and distillation losses.


