\section{Related Work}
% ----------------------------------
\noindent
\textbf{Semantic Scene Completion.}
% ---------------------------------- 
SSCNet~\cite{song2017semantic} is the pioneering work to predict both volumetric occupancies and semantic labels for completing 3D scenes.
%
Follow-up works can be divided into four categories:
1)~\emph{Volume-based methods}. With TSDF as input~\cite{song2017semantic,dourado2019edgenet}, these methods use 3D CNNs for volumetric and semantic predictions.
%
The disadvantage is the high computational cost of 3D CNNs~\cite{chen20193d,dourado2020semantic}.
% 
2)~\emph{View-volume-based methods}. With RGB image and depth value as inputs, these methods use 2D CNNs to extract features,
%  
and then reshape the features to 3D features by a 2D-3D projection layer for predicting 3D voxels~\cite{guo2018_VVNet,liu2018see,li2019rgbd,li2020attention,liu20203d}. 
These methods are less effective in extracting 3D geometry information due to the limitations of 2D CNNs.
3)~\emph{Point-based methods}. SPCNet~\cite{zhong2020semantic} is a classic method in this category. It particularly resolves the problem of discretization in voxels and predicts SSC by using points as input.  
%
However, it is sensitive to point-label misalignment (\ie, delta noise).
% 
4)~\emph{Hybrid methods}. Many recent works~\cite{rist2021semantic,rist2020scssnet,cai2021semantic,tang2022not} take at least two kinds of inputs from the set of RGB image, depth value, TSDF, and points. 
%
They use dual-branch network architectures for feature fusion and achieve state-of-the-art performance.
% 
Our work belongs to this category using RGB image and TSDF as input.
We highlight that our key difference with the above works is that we are the first to particularly resolve the problem of noisy TSDF estimated from noisy depth values.
%

% ----------------------------------
\noindent
\textbf{Learning with Noises.}
% ----------------------------------
% 
Both image collection and image annotation bring \emph{data noise} and \emph{label noise}  to computer vision datasets, respectively~\cite{rolnick2017deep,brummer2019natural}. 
%
Specifically, \emph{data noises} are usually caused by the defects of sensors (\eg, limited visual fields~\cite{henry2014rgb} and inaccurate cameras~\cite{mallick2014characterizations}) or data collection environments (\eg, fog~\cite{gibson2013fast}, rain~\cite{chen2022unpaired}, and nighttime~\cite{rivera2012content}).
To reduce the negative effects on the learning of models, the mainstream idea is to learn clean~\cite{xie2019feature} and robust~\cite{li2021learning,adeli2018semi} feature representations~\cite{zhang2020causal,zhang2020feature}. 
%
Existing methods can be divided into two categories: noise-cleaning based methods~\cite{ollion2021joint,zheng2021adaptive,liang2021swinir} and robust-modeling based methods~\cite{yang2022treatment,yang2019causal,momeny2021convolutional}.
% 
In particular, the existing models tend to result in unsatisfactory performance when the noises are unpredictable. 
% 
Our method aims to address the problem of noisy data for SSC. 
We propose to transfer the clean knowledge learned from the ground-truth depth value into the noisy-depth pipeline during training. In the inference phase, we can directly use this pipeline without ground-truth. 
% 

% -------------------------------------------

\noindent
\textbf{Knowledge Distillation (KD).}
% -------------------------------------------
KD is proposed in ~\cite{hinton2015distilling} to transfer the knowledge from a teacher model (a stronger or larger model) to a student model (a weaker or smaller model).
%
The original motivation is model compression.
% 
After KD, the student can achieve a competitive or even superior performance~\cite{li2020few,zhang2021self}.
KD can be roughly categorized into 1) feature-based~\cite{jung2021fair,ji2021show}, logit-based~\cite{zhao2022decoupled,kim2021distilling}, and hybrid~\cite{gou2021knowledge,hsu2022closer}. In this work, we leverage KD for a new purpose: denoising in the SSC task. The teacher model learns cleaner knowledge than the student model with noisy input. This enables the teacher model to provide intermediate supervision for learning the student model via KD.
% 