% ----------------------------------
\section{Experiments}
% ----------------------------------
\subsection{Datasets and Evaluation Metrics} \label{ssec:datametric}
% ----------------------------------
\noindent\textbf{Datasets.} 
Following~\cite{song2017semantic,Garbade2018_twoStream,chen20203d}, we use the challenging NYU~\cite{silberman2012indoor} dataset and its variants NYUCAD~\cite{firman2016NYUCAD} dataset in our experiments. 
NYU~\cite{silberman2012indoor} consists of 1,449 RGB-D images (795 for training, and 654 for testing) captured via a Kinect sensor~\cite{zhang2012microsoft}. NYU is composed of mainly office and house room scenes. The 3D voxel ground-truth is generated by voxelizing the computer-aided design (CAD) mesh annotations provided by \cite{guo2013support}.
The difference between the two datasets is that the depth values of NYUCAD are rendered from 3D voxel ground-truth, but the depth values of NYU are noisy.
We use the training set of NYUCAD to train the teacher network, and use training and testing sets of NYU to respectively train and evaluate the student network.


% ----------------------------------
\noindent\textbf{Evaluation Metrics.}
Following~\cite{song2017semantic,Garbade2018_twoStream,chen20203d}, we use Precision (Prec.), Recall, and Intersection over Union (IoU) as evaluation metrics for two tasks: semantic scene completion (SSC), and scene completion (SC). 
For SSC, we evaluate the IoU of each category on both observed and occluded voxels, and mean IoU across all categories. 
For SC, we treat every voxel as a binary prediction task, \ie, empty or non-empty, and we evaluate the performance on the occluded 3D voxels.

% ----------------------------------
\subsection{Implementation Details}
% ----------------------------------
\noindent\textbf{Network Architectures.} 
For 2DNet, we adopt the ``B2'' Segformer~\cite{xie2021segformer} as baseline, and initialize it with ImageNet~\cite{deng2009imagenet} pretrained weights. 
%
For 3D-TSDFNet, we follow 3D-Sketch~\cite{chen20203d} to use the following architecture: three layers of 3D convolution; two DDR blocks~\cite{li2019rgbd} with feature downsampling by a rate of $4$; and two layers of 3D deconvolution for upsampling.
% 
%
For 3D-SSCNet, we use the same architecture as 3D-TSDFNet except that we remove the first three layers of 3D convolution. 
\emph{Please refer to the supplementary materials for the detailed architecture of each net.}
% 
The feature channel size in both 3D-TSDFNet and 3D-SSCNet is set to $D=256$, and the 3D resolution of prediction $(G_X,G_Y,G_Z)$ is set to $(60,36,60)$. 

% ----------------------------------
\noindent\textbf{Training Settings.} We implement all experiments in this work on the PyTorch~\cite{paszke2019pytorch} with 2 NVIDIA 3090 GPUs. Our model is trained by AdamW~\cite{loshchilov2017decoupled} with a weight decay of 0.05. The batch size is $4$ and the learning rate is initially set as $0.001$ and scheduled by a cosine learning rate decay policy~\cite{loshchilov2016sgdr} with a minimum learning rate of $1e$-$7$. We train our model for $100$ epochs. For each input RGB image, we apply resize, random cropping, and random flipping as 2D data augmentation. For 3D data augmentation, we use random x-axis and z-axis flipping as in \cite{dourado2022data}. In our loss function, both $\lambda$ and $\beta$ are set as $0.25$.

% ----------------------------------
\subsection{Comparisons with State-of-the-Arts}
% ----------------------------------
% 
Table~\ref{tab:sota} shows the quantitative results of our CleanerS compared to state-of-the-art methods on the test set of NYU. 
%
We can see that CleanerS outperforms these methods by clear margins. 
%
In particular, it achieves the improvements of $3.2\%$ SC-IoU and $3.3\%$ SSC-mIoU compared to the top-performing method FFNet~\cite{wang2022ffnet}, and becomes the new state-of-the-art.
% 
Another closely related work is 3D-Sketch~\cite{chen20203d}. It is also a voxel-based framework with the inputs of RGB image and TSDF.
% 
%
However, 3D-Sketch uses ResNet50 for 2D feature extraction.
%
To compare with it fairly, we implement the 2DNet in our method with ResNet50~\cite{he2016deep}, and name it as CleanerS (Res50) in Table~\ref{tab:sota}. 
%
Results show that our CleanerS (Res50) outperforms 3D-Sketch by $1.8\%$ SC-IoU and $4.1\%$ SSC-mIoU.
% 
% 

% ---------------------------------
\input{cvpr2023/Figures/visual_sota}
% ---------------------------------

\noindent\textbf{Qualitative Results.} 
Figure~\ref{fig:sota} demonstrates the qualitative results compared to the state-of-the-arts methods, including SSCNet~\cite{song2017semantic} and 3D-Sketch~\cite{chen20203d}. 
%
On the first row, zero noises in depth values cause incomplete surfaces (in (b)).
%
As a result, both SSCNet and 3D-Sketch can hardly ``imagine'' the objects on the incomplete surface, resulting in an incomplete prediction in SSCNet~\cite{song2017semantic} and wrong semantic predictions in 3D-Sketch\cite{chen20203d}. 
%
In the second row, delta noises cause confusion about semantic classes. Thus, both SSCNet and 3D-Sketch are confused about the semantics between the ``{furniture}" and the ``{table}", as well as between the ``{furniture}" and the ``{sofa}". 
%
Compared to them, we explicitly handle these two kinds of noises, and achieve more accurate predictions as shown in (e) and (f).
% 

\subsection{Ablation Study} 
\label{ssec:ablation}
% ----------------------------------
\noindent\textbf{CleanerS Components.}  
In Table~\ref{tab:ablation}, the $1{st}$ row shows the performance of the teacher model with inputs of RGB and TSDF-CAD (\emph{without noise}). 
It can be taken as the upper bound of the student model. 
%
The $2{nd}$ row shows the performance of the baseline taking \emph{noisy depth values} as inputs. It is the lower bound.
%
It is clear that there is a great gap between the upper and lower bounds.
% 
The $3{rd}$ row lists the performance of our CleanerS with \textbf{feature-based KD} only. It shows a significant improvement at SC-IoU $(+2.7\%)$, validating that CleanerS helps completing the scene and gets higher occupancy.
% 
% 
The $4{th}$ row shows the performance of our CleanerS with \textbf{logit-based KD} only. The model learns from the semantic-centered logits and semantic affinities between voxels and class centers given by the teacher network.
%
It achieves a clear improvement, i.e., $1.7$ percentage points on SSC-mIoU, for the semantic prediction task SSC.
%
It also brings a surprising improvement on SC-IoU ($2.1$ percentage points). The reason is that the TSDF-CAD features are fused for final prediction, and this enables an implicitly cleaner surface distillation.
% 
Combining both feature- and logit-based KDs achieves the best results.
%
1) it boosts the best SSC mIoU (in $4_{th}$ row) by an extra $0.5\%$. The more accurate completion means more true positives on volumetric occupancy. This provides the model a chance to infer correct semantic labels;
% 
2) it achieves a further gain of $0.4\%$ above the best SC-IoU (in $3_{rd}$ row). The accurate semantic prediction in turn enhances the prediction of shapes of occluded regions. 
% 
Overall, our CleanerS gains the improvements of $3.1\%$ SC-IoU and $2.2\%$ SSC-mIoU, compared to the baseline.


% ---------------------------------
\input{cvpr2023/Tables/tab_ablation}
% ---------------------------------


\noindent\textbf{Logit-based KD \vs Soft-target KD.} As mentioned in the second paragraph of Section~\ref{ssec:distillation}, directly forcing straightforward distillation is ineffective.
% 
To validate this, we compare our logit-based distillation (including $L_{KD-SC}$ and $L_{KD-SA}$) with soft target distillation~\cite{hinton2015distilling}, and we denote the loss of the latter one as $L_{KD-Soft}$. %
%
From the results in Table~\ref{tab:dis_semantic}, we can observe that the $L_{KD-SC}$ plays a major role, and $L_{KD-Soft}$ achieves similar performance as using $L_{KD-SA}$. 
%
More specifically, we have the following conclusions: 1)~it is sub-optimal to apply only a local voxel-wise logits distillation, and 2)~it is a better choice to distill the global semantic center embedding. The reasons might be that the inputs to teacher and student models are not exactly the same, clean TSDF-CAD \emph{v.s.} noisy TSDF, and it is hard to match their outputs in a voxel-wise manner. Besides, with the same RGB image input, both teacher and student models aim to infer 3D semantic voxels in the same scene, and it is thus reasonable to enforce the two models to predict similar semantic-centered logits for each semantic class. 


% ----------------------------------
\input{cvpr2023/Tables/tab_distill_semantic}
% ----------------------------------