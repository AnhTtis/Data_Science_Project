%\beginsupp
% --------------------------------------
This supplementary includes an introduction to the background of 2D-3D projection (\ref{subsec_2d3dprojection}), 
implementation details of both 3D-TSDFNet and 3D-SSCNet (\ref{suppsec_3dnet}), 
ablation results of CleanerS with different 2DNets (\ref{suppsec_2dnet}) and with different resolutions (\ref{suppsec_resolution}), comparison results of using different methods of mitigating the depth noise (\ref{suppsec_denoise}), the correlation between the noise accuracy degrade (\ref{suppsec_noiserate}), and more visualization results (\ref{suppsec_visual}).

% --------------------------------------
\section{2D-3D Projection}
\label{subsec_2d3dprojection}
% --------------------------------------
{\color{red}{This supplementary is for the background introduction of the main paper.}} 
2D-3D projection layer is used to recover the visible surface in a 3D scene such as to map every pixel in a 2D image to its corresponding 3D spatial position. Given the depth image $\mathbf{I}_{d}$, each pixel at 2D position $[u,v]$, with a depth value $d=\mathbf{I}_d(u,v)$, is projected to the 3D position $[x,y,z]$. This mapping $\mathbb{M}$ can be expressed as follows:
% --------------------------------------
\begin{equation} \label{eq:proj}
[x,y,z] = \mathbb{M}(u,v,d).
\end{equation}
% --------------------------------------
% 
Specifically, this projection includes the following two steps:

\noindent
\emph{Step 1}: Mapping each 2D pixel to an individual 3D point based on the imaging information including an intrinsic camera matrix $\mathbf{K} \in \mathbb{R}^{3\times3}$ and an extrinsic camera matrix $[\mathbf{R}|\mathbf{t}] \in \mathbb{R}^{3\times4}$. The mapping satisfies the following equation:
% --------------------------------------
\begin{equation} \label{eq:2d3d}
[\mathbf{R}|\mathbf{t}][x_p,y_p,z_p,1]^{\top} = \mathbf{K}^{-1}([u, v, 1]^{\top} \cdot d).
\end{equation}
% --------------------------------------
Based on Eq.~\eqref{eq:2d3d}, we can solve the $[x_p,y_p,z_p]$, i.e., the 3D position of the corresponding point.

\noindent
\emph{Step 2}: Discretizing the point position into a grid voxel with a given unit voxel size $g$,
\begin{equation} \label{eq:discret}
[x,y,z] = [\lfloor x_p / g + 0.5 \rfloor, \lfloor y_p / g + 0.5 \rfloor, \lfloor z_p / g + 0.5 \rfloor],
\end{equation}
where $\lfloor \cdot \rfloor$ is the floor rounding. The unit grid size $g$ is $0.08m$ and the resultant 3D voxel size is $(60, 36, 60)$. 
% 
% 
The 2D-3D projection is used for two purposes in this work: 1) getting the class labels of 2D pixels (resulting $Y(d)$ in Section~\textcolor{red}{3} and $\mathbf{Y}_{2D}$ in Section~\textcolor{red}{4.3}); 2) translating a 2D feature into a 3D feature (translating from $\mathbf{F}_r$ to $\mathbf{V}_r$ in Section~\textcolor{red}{4.1}). 

% 
% ----------------------------
\input{cvpr2023/Figures/3dnet}
% ----------------------------


\section{Implementation Details of 3D-TSDFNet and 3D-SSCNet}
\label{suppsec_3dnet}
{\color{red}{This supplementary is for Section 4.1 of the main paper.}}  We introduce the details of the 3D-TSDFNet and 3D-SSCNet in both student and teacher networks, as presented in Figure~\ref{fig:3dnet}. Following 3D-Sketch~\cite{chen20203d}, the 3D-TSDFNet includes 3 layers of 3D convolutions to encode the input TSDF into high dimensional features, 8 DDR blocks with different dilations and a downsample rate of 4 to enlarge receptive fields and reduce computation costs, and 2 layers of 3D deconvolutions to upsample features to have the same volume size as the input TSDF volume. Besides, a skip connection is added between each pair of DDR block and deconvolution layer for efficient gradient back-propagation. The 3D-SSCNet uses the same architecture as the 3D-TSDFNet except that it removes the first 3 layers of 3D convolutions.



% ----------------------------
\input{cvpr2023/Tables/tab_2dnet}
% ----------------------------

\section{Ablation Results with Different 2DNet}
\label{suppsec_2dnet}
% ----------------------------
{\color{red}{This supplementary is for Section 5.4 of the main paper.}}  
We supplement the ablation study for CleanerS-Res50 and compare the results with those of CleanerS in Table~\ref{tab:2dnet}. From the table, we can observe that: 
% 
1) For all the methods, including baseline, teacher, and CleanerS, better performances are achieved by using Segformer-B2 (than using ResNet50), especially on the metric of SSC mIoU. The reason is that image features extracted from the transformer-based Segformer-B2 encode better global (i.e., longer-range) context information.
% 
2) With different 2DNet architectures (Segformer-B2 or ResNet50), the proposed CleanerS can consistently improve over baseline by large margins, \ie, over 2.5\% for SC-IoU and over 2.0\% for SSC-mIoU. In addition, using better 2DNets (Segformer-B2), our CleanerS achieves even higher improvement (over baseline). This demonstrates the effectiveness and robustness of the proposed CleanerS for resolving the problem of depth noise in SSC.
% 

% 
\section{Ablation Results with Different Resolutions}
\label{suppsec_resolution}
% 

{\color{red}{This supplementary is for Section 5.4 of the main paper}}. We conduct experiments with a lower resolution under the 3D size of ($30$, $18$, $30$). Experimental results are given in Table~\ref{Rtab:resolution}, where ``HR/LR'' denotes high/low resolution. Furthermore, ``HR2LR'' is a variant by distilling from an HR teacher to an LR student, which is meant to verify if an HR teacher will enable better knowledge distillation. We can observe that: 1) compared to HR, our CleanerS with LR results in a higher SC-IoU and a lower SSC-mIoU; 2) HR2LR performs even worse than LR2LR, which suggests the same resolution inputs enable a better knowledge distillation.

%---------------------------
\input{cvpr2023/Tables/Rtab-resolution.tex}
%---------------------------

\section{Feature-based KD \vs. Data-based Denoise}
\label{suppsec_denoise}
% ----------------------------
{\color{red}{This supplementary is for Section 5.4 of the main paper.}}  In related works, there is a common practice to mitigate noises by using the corresponding clean data as learning targets~\cite{yang2018proximal,liu2021disentangling}. We validate here that it is not working for our cases. Specifically, we mitigate the noise in TSDF by using the noise-free TSDF-CAD input as a learning target.
First, we add an extra prediction layer after 3D-TSDFNet, which with an input TSDF feature $V_t^S$. The prediction layer includes a DDR block~\cite{li2019rgbd} and a 3D convolutional layer, which outputs a 3-channel prediction (2-channel for the sign prediction and 1-channel for distance prediction).
%
Then, we compare its results with our feature-based KD, in Table~\ref{tab:dis_tsdf}.
%  
As shown in Table~\ref{tab:dis_tsdf}, it achieves a limited performance gain (0.4\% on SC-IoU and 0.4\% on SSC-mIoU).
In contrast, our feature-based KD significantly improves the SC-IoU. We think there are two reasons. 1) The TSDF-CAD features are task-oriented features and have a richer representation than the TSDF-CAD input. 2) Taking the TSDF-CAD input as a learning target needs extra prediction layers, which might distract the optimization. 
% 

% ----------------------------------
\input{cvpr2023/Tables/tab_lean_tsdfcad}
% ----------------------------------
% 

\section{Correlation between the Noise Rate and Accuracy Degrade}
\label{suppsec_noiserate}
% 


{\color{red}{This supplementary is for Section 5.4 of the main paper}}. To figure out the correlation between the noise rate and accuracy degradation, we perform the synthetic noise depth input by randomly adding either one or both of the zero noise and delta noise into the clean depth-CAD. The noise rate is gradually set to $20\%$, $50\%$, and $80\%$.
Experimental results are given in Table~\ref{Rtab:noiserate}. We can observe that 1) the higher the noise rate, the more degradation of the accuracy; 2) mixing both zero noise and delta noise will result in a complex noise and drops the performance drastically, especially for the SSC-mIoU.

%---------------------------
\input{cvpr2023/Tables/Rtab-noiserate.tex}
%---------------------------


% --------------------------------------
\input{cvpr2023/Figures/visual_ablation}
% --------------------------------------

% --------------------------------------
\input{cvpr2023/Figures/visual_sota2}
% --------------------------------------

\section{More Visualization Results}
\label{suppsec_visual}
% ----------------------------
{\color{red}{This supplementary is for Section 5.4 of the main paper.}}  As shown in Figure~\ref{fig:ablation}, compared with the baseline method (in (c)), the cleaner surface distillation by feature-based KD (in (d)) helps to get cleaner occupancy predictions but may confuse the semantics. Combining it with the cleaner semantic distillation by logit-based KD (in (f)) can resolve this confusion.
% 



% 
In Figure~\ref{fig:sota2}, we supplement more visual examples compared to state-of-the-art methods.





