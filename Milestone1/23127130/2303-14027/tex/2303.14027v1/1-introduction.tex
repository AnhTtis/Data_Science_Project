Deep learning in hyperbolic space has gained traction in recent years empowered by their inherent ability to embed hierarchical data with arbitrarily low distortion \cite{sarkar2011trees} and being more compact and dense \cite{chami2019, nickel2017poincare, shimizu2021}. These promising characteristics have led to rapid developments in hyperbolic representation learning for tree-like structures \cite{balazevic2019multi,chami2020trees,ganea2018entailment,law2019lorentzian,nickel2017poincare,sala2018representation}, graphs \cite{chami2019,dai2021hyperbolic,liu2019hyperbolic,zhang2021hyperbolic}, text \cite{dhingra2018embedding,tifrea2019poincar}, action skeletons \cite{franco2023hyperbolic}, biological structures \cite{klimovskaia2020poincare}, and more.

Recently, hyperbolic learning has also been investigated for visual understanding. Hyperbolic embeddings of images and videos have been shown to improve few-shot learning \cite{fang2021kernel,gao2021curvature,guo2022clipped,ma2022adaptive,zhang2022hyperbolic}, hierarchical recognition \cite{dhall2020hierarchical,ghadimi2021hyperbolic,long2020searching,yu2022skin}, segmentation \cite{chen2022hyperbolic,atigh2022hyperbolic} and metric learning \cite{ermolov2022hyperbolic,zhang2021learning} amongst others. While promising, the use of hyperbolic geometry in computer vision has been limited to the classification space, with all visual representations being learned on conventional networks that operate in Euclidean space.

This paper explores the possibility of learning visual representations entirely in hyperbolic space. The ability to learn hyperbolic representations directly from the pixel-level will allow us to unlock the broad potential of hyperbolic geometry for vision, such as capturing latent hierarchical visual representations \cite{khrulkov2020hyperbolic}, training compact network architectures \cite{chami2019, nickel2017poincare, shimizu2021}, and creating networks that better mimic visual representation learning in the brain \cite{zhang2022hippocampal}. Empowered by successful hyperbolic implementations of non-visual layers~\cite{ganea2018,shimizu2021}, the time is ripe for visual hyperbolic feature learning.

As a step towards fully hyperbolic visual learning, we start from the highly celebrated ResNet \cite{he2015b} and rebuild its architecture in hyperbolic space; from 2D convolutions to residual connections. Optimizing a ResNet in the Poincar\'e ball model comes with several challenges. First, we find that existing network initializations in hyperbolic space lead to vanishing signals, which derail learning over many convolutional layers. We provide an identity-based network initialization that preserves the output norm over many layers. Second, ResNets rely extensively on batch normalization, but its generalization to hyperbolic space requires expensive Fr\'echet mean calculations \cite{lou2020}. We introduce Poincar\'e midpoint batch normalization, which allows us to compute approximate means at a fraction of the computational cost. Third, the basic gyrovector operations in the Poincar\'e ball model consist of many intermediate calculations. In modern deep learning libraries, all these calculations are stored for automatic differentiation, blowing up the computation graph. We have derived and implemented the backward pass of core hyperbolic gyrovector operations to contain the computation graph.

Empirically, we show that our network initialization is indeed norm-preserving and improves network generalization. We show that our midpoint batch normalization speeds up training by 25\% with no loss in classification accuracy. We furthermore demonstrate the potential of Poincar\'e ResNet for out-of-distribution detection, adversarial robustness, and learning complementary representations compared to Euclidean ResNet.
