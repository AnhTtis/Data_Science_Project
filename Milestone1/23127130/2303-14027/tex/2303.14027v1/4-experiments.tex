

\begin{figure*}[ht]
    \begin{subfigure}[t]{0.25\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/norm_of_layer_outputs.png}
        \caption{Output norm over layers.}
        \label{fig:init_toy_ex}
    \end{subfigure}
    \hspace{0.8em}
    \begin{subfigure}[t]{0.25\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/initialization_comparison.png}
        \caption{5-layer ConvNet training.}
        \label{fig:init_convnet_1}
    \end{subfigure}
    \hspace{0.8em}
    \begin{subfigure}[t]{0.25\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/initialization_comparison_v2.png}
        \caption{7-layer ConvNet training.}
        \label{fig:init_convnet_2}
    \end{subfigure}
    \centering
    \caption{\textbf{Comparison between our initialization and the initialization of \cite{shimizu2021}.} In (a), we show the logarithm of the mean of the norms of each layer's output during the forward pass of an untrained 10-layer fully-connected network for random Poincar\'e gyrovectors. The figure shows that the baseline initialization is suffering from vanishing signals with outputs that collapse to the origin over multiple layers. Our identity-based initialization however maintains output norms over layers. In (b) and (c), we show the test accuracy over epochs for a 5-layer and a 7-layer ConvNet. For a 5-layer network, the baseline initialization converges slower, while it is no longer capable of learning for 7-layers. We conclude that our initialization is preferred for training convolutional networks in the Poincar\'e ball model.}
    \label{fig:init_exps}
\end{figure*}

For the empirical analyses, we investigate (i) the importance of a suitable network initialization over many layers, (ii) the effect of curvature and ReLU activations, (iii) the comparison between Fr\'echet-based and our midpoint-based batch normalization and (iv) the robustness of hyperbolic residual networks.
We seek to evaluate Poincar\'e ResNets in isolation and hence stick to minimal augmentation and fixed hyperparameters. Specifically, we use random cropping and horizontal flipping with Adam optimization with fixed learning rate $10^{-3}$ and weight-decay $10^{-4}$.

\subsection{Identity initialization is norm-preserving}\label{subsect:id_init}
The approach of Shimizu \etal \cite{shimizu2021} is the current leading initialization in hyperbolic networks. This initialization, however, leads to vanishing signals, which we empirically validate here.
We take a stack of 10 Poincar\'e linear layers with a constant curvature of $c = 1$, with 20 input and output neurons.
We then perform a single forward pass on a batch of 16 Poincar\'e gyrovectors which are generated by sampling vectors in the tangent space at the origin from the multivariate normal distribution $\mathcal{N}(0, \frac{1}{10} I_{20})$ and mapping these to the Poincar\'e ball. Figure \ref{fig:init_toy_ex} shows the behaviour of the norms during the forward pass for both initialization methods. Where the baseline initialization suffers from vanishing signals, our identity initialization keeps the norms constant up to the rounding effects of the repeated application of non-linear operations.

In Figures \ref{fig:init_convnet_1} and \ref{fig:init_convnet_2} we additionally show what happens when training a simple ConvNet on CIFAR-10 with both initialization methods trained with SGD with learning rate $10^{-3}$, momentum $0.9$ and weight decay $10^{-4}$. For a 5-layer ConvNet, the baseline initialization converges more slowly. For a 7-layer ConvNet, we find that the baseline is no longer capable of learning meaningful representations. Identity-based initialization is still able to train in this setting. We conclude that our identity-based hyperbolic network initialization is preferred for training hyperbolic networks.




\subsection{Curvatures and ReLUs stabilize optimization}
Previous works claim that nonlinearities, such as the ReLU operation, are redundant in hyperbolic neural networks due to the many nonlinearities inherent to such networks \cite{ganea2018,shimizu2021}. Here, we test this claim by training a small Poincar\'e ResNet-20 on CIFAR-10 with small channel widths of (4, 8, 16) with and without the ReLU nonlinearity as activation layer. The results are shown in Figure \ref{fig:curvature_and_relu_comp} (left). We find that training with the ReLU nonlinearity leads to faster convergence and a greater final accuracy. This shows that nonlinear activation functions remain important despite the inherent nonlinearity of hyperbolic networks.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{src/curvatures_and_relu.png}
    \caption{\textbf{ReLUs and small curvatures improve} the optimization and performance of Poincar\'e ResNet. Both experiments were performed using a small ResNet-20 with small channel widths (4, 8, 16). While hyperbolic layers are already non-linear, adding ReLUs further improves generalization. The same holds for using smaller curvatures.}
    \label{fig:curvature_and_relu_comp}
\end{figure}

\setcounter{table}{1}
\begin{table*}
\centering
\begin{tabular}{l l cc cc cc cc cc cc}
\toprule
 & \textbf{Manifold} & \multicolumn{6}{c}{\textbf{CIFAR-10}} & \multicolumn{6}{c}{\textbf{CIFAR-100}}\\
 \cmidrule(lr){3-8} \cmidrule(lr){9-14}
& & \multicolumn{2}{c}{FPR95 ↓} & \multicolumn{2}{c}{AUROC ↑} & \multicolumn{2}{c}{AUPR ↑}  & \multicolumn{2}{c}{FPR95 ↓} & \multicolumn{2}{c}{AUROC ↑} & \multicolumn{2}{c}{AUPR ↑}\\
 \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
& & R20 & R32 & R20 & R32 & R20 & R32 & R20 & R32 & R20 & R32 & R20 & R32\\
\midrule
{\multirow{2}{*}{Places-365}} & Euclidean & \bf{64.2} & 72.3 & \bf{84.7} & 82.0 & \bf{96.2} & 95.6 & 89.5 & 93.9 & 62.5 & 57.9 & 89.3 & 87.9\\
& \cellcolor{Gray}Poincar\'e & \cellcolor{Gray}70.2 & \cellcolor{Gray}\bf{70.7} & \cellcolor{Gray}82.3 & \cellcolor{Gray}\bf{82.6} & \cellcolor{Gray}95.7 & \cellcolor{Gray}\bf{95.9} & \cellcolor{Gray}\bf{82.8} & \cellcolor{Gray}\bf{83.8} & \cellcolor{Gray}\bf{71.5} & \cellcolor{Gray}\bf{71.1} & \cellcolor{Gray}\bf{92.3} & \cellcolor{Gray}\bf{92.2}\\
\midrule
{\multirow{2}{*}{SVHN}} & Euclidean & 97.3 & 94.7 & 68.8 & 73.4 & 92.8 & 94.1 & 99.5 & 98.8 & 43.7 & 54.6 & 83.7 & 88.2\\

& \cellcolor{Gray}Poincar\'e & \cellcolor{Gray}\bf{66.0} & \cellcolor{Gray}\bf{69.3} & \cellcolor{Gray}\bf{85.0} & \cellcolor{Gray}\bf{83.6} & \cellcolor{Gray}\bf{96.6} & \cellcolor{Gray}\bf{96.3} & \cellcolor{Gray}\bf{76.9} & \cellcolor{Gray}\bf{83.0} & \cellcolor{Gray}\bf{76.8} & \cellcolor{Gray}\bf{72.6} & \cellcolor{Gray}\bf{94.1} & \cellcolor{Gray}\bf{92.9}\\
\midrule
{\multirow{2}{*}{Textures}} & Euclidean & 87.3 & 88.0 & 73.6 & 77.3 & 93.2 & 94.7 & 98.1 & 96.0 & 33.5 & 42.9 & 75.9 & 79.4\\

& \cellcolor{Gray}Poincar\'e & \cellcolor{Gray}\bf{68.2} & \cellcolor{Gray}\bf{66.2} & \cellcolor{Gray}\bf{82.1} & \cellcolor{Gray}\bf{82.3} & \cellcolor{Gray}\bf{95.5} & \cellcolor{Gray}\bf{95.6} & \cellcolor{Gray}\bf{83.9} & \cellcolor{Gray}\bf{84.2} & \cellcolor{Gray}\bf{67.7} & \cellcolor{Gray}\bf{68.8} & \cellcolor{Gray}\bf{91.0} & \cellcolor{Gray}\bf{91.5}\\
\bottomrule
\end{tabular}
\caption{\textbf{Out-of-distribution detection} on CIFAR-10 and CIFAR-100 with Places365, SVHN, and DTD as out-of-distribution datasets. R20 and R32 denote ResNet-20 and ResNet-32 architectures, both with channel widths (8, 16, 32). Across different in- and out-of-distribution datasets, hyperbolic ResNets are more robust than their Euclidean counterpart.}
\label{tab:ood}
\end{table*}

Poincar\'e balls of various curvatures have similar geometric properties.
For numerical computations however, setting the right curvature impacts the down-stream performance \cite{gao2021curvature}. In this analysis, we investigate the effect of various curvatures for optimizing Poincar\'e ResNets.
We again perform the experiments on a small Poincar\'e ResNet-20 with small channel widths of (4, 8, 16) using a curvature of $1$, $0.1$ or $0.01$.
We show the results in Figure \ref{fig:curvature_and_relu_comp} (right). We first find that training with a curvature of $c=1$ leads to suboptimal accuracies. As the curvature becomes smaller, the Euclidean volume of the Poincar\'e ball increases. As a result, representing elements within this manifold using floating-point representations becomes easier with smaller curvatures. Indeed, when training with curvatures $c=0.1$ and $c=0.01$, we find that the model converges faster and has a higher final accuracy. Overall, we find that a curvature of $c=0.1$ works best for training Poincar\'e ResNets and we will use this setting for the rest of the experiments.

\setcounter{table}{0}
\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{ lc c c}
\toprule
 & & \multicolumn{1}{c}{\textbf{ResNet-20}} & \multicolumn{1}{c}{\textbf{ResNet-32}}\\
\midrule
\multirow{2}{*}{Accuracy} & Fr\'echet mean & 79.4 & 82.4 \\
& Poincar\'e midpoint & 80.9 & 81.9 \\
\midrule
Time & Fr\'echet mean & 179.0 & 169.4\\
\scriptsize{($s \text{ epoch}^{-1}$)} & Poincar\'e midpoint & 137.5 & 132.0 \\
\rowcolor{Gray} & & -23\% & -22\%\\
\bottomrule
\end{tabular}
}%
\caption{\textbf{Poincar\'e midpoints for batch normalization} in hyperbolic space are as effective for classification as Fr\'echet means while being faster to optimize.}
\label{tab:bn_results}
\end{table}



\subsection{Midpoints make batch norm efficient}
To compare the computational efficiency and the performance of our Poincar\'e midpoint batch normalization to the batch normalization by \cite{lou2020}, we perform multiple experiments using Poincar\'e ResNet-20 or Poincar\'e ResNet-32 on CIFAR-10 with small channel widths of (4, 8, 16). %
We opt for a small ResNet width and fixed hyperparameters to allow for faster evaluation, all models obtain higher scores with more depth and hyperparameter tuning.
Each network is then trained with Fr\'echet-based batch normalization \cite{lou2020} or with our Poincar\'e midpoint batch normalization.

The results of the experiment are shown in Table \ref{tab:bn_results}. First, we find that both batch normalization methods lead to similar accuracies, which indicates that Poincar\'e midpoints are as effective as Fr\'echet means for classification. Second, training a network with Poincar\'e midpoint batch normalization leads to a reduction in computation time of approximately 20-25\%. We recommend Poincar\'e midpoints when performing batch normalization in hyperbolic networks.


\subsection{Hyperbolic networks are robust}
For the final experiment, we investigate the robustness and complementary nature of Poincar\'e ResNet compared to its Euclidean alternative. Specifically, we investigate whether Poincar\'e ResNet is (i) robust to out-of-distribution samples, (ii) can handle adversarial examples, and (iii) learns complementary representations compared to Euclidean ResNet.

\paragraph{Out-of-distribution detection.}
To check whether Poincar\'e ResNets are robust to out-of-distribution samples, we compare the out-of-distribution detection performance of Euclidean and Poincar\'e ResNet-20 and ResNet-32 with channel widths (8, 16, 32), trained on either CIFAR-10 or CIFAR-100 using the same hyperparameters and optimizer as before. For each architecture, the Euclidean and hyperbolic variants have similar classification performance, hence any difference in out-of-distribution performance is not a result of improved training. We use the Places-365 dataset \cite{zhou2017places}, the SVHN dataset \cite{netzer2011reading} and the Textures dataset \cite{cimpoi2014describing} as out-of-distribution datasets. For detecting out-of-distribution samples, we use the energy score as introduced by Liu \etal~\cite{liu2020energy}. The comparisons are performed on the commonly used metrics FPR95, AUROC and AUPR.

The results are shown in Table \ref{tab:ood}. We find that with a ResNet-32 architecture, Poincar\'e ResNet outperforms Euclidean ResNet across all datasets and metrics. With a ResNet-20 architecture, Poincar\'e ResNet is better in five of the six combinations of in- and out-of-distribution datasets. We conclude that a hyperbolic ResNet is more robust to out-of-distribution samples than its Euclidean counterpart.

\paragraph{Adversarial attacks.}
To see if Poincar\'e ResNet is robust to adversarial samples, we compare the performance against an adversarial attack between Euclidean ResNet-32 and Poincar\'e ResNet-32, both with channel widths (8, 16, 32), trained on CIFAR-10. Note that, after training, both models have similar performance on the test set of CIFAR-10. We apply the fast gradient signed method (FGSM) \cite{goodfellow2015fgsm} attack with perturbations $\epsilon = \frac{0.8}{255}, \frac{1.6}{255}, \frac{3.2}{255}$ to the models. The results are shown in Figure \ref{fig:adversarial}. We find that Poincar\'e ResNet is more resistant to adversarial attacks than the Euclidean ResNet, even though both architectures were trained similarly and obtained similar classification performance.This result highlights the potential of hyperbolic learning in the presence of adversarial agents.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{src/cifar_fgsm_8_16_32_resnet_32.png}
    \caption{\textbf{Comparison of robustness to the FGSM adversarial attack} between Euclidean and Poincar\'e ResNets. These results are obtained by attacking a Poincar\'e and a Euclidean ResNet-32, with small channel widths of (8, 16, 32), trained on CIFAR-10 to similar performance, with adversarial examples of varying perturbation sizes $\epsilon$. The results show that the Poincar\'e ResNet is more robust to the FGSM adversarial attack.}
    \label{fig:adversarial}
\end{figure}

\paragraph{Fusing ResNets.}
To show that the representations learned by Poincar\'e ResNets are complementary to the features from Euclidean ResNets, we evaluate the performance of a fusion model, where each image is forwarded through both ResNets and the resulting logits are averaged to obtain predictions. Note that both models are trained independently and the fusion model is only evaluated with no further training being performed. The results are shown in Figure \ref{fig:fusion}. For each architecture, the performance on both manifolds is similar. Clearly, the performance of the fusion models is better than that of the individual ResNets. In fact, with respect to the number of parameters, it appears that it is more efficient to create a fusion model than it is to increase the size of whichever ResNet we are using. This result highlights the strong complementary nature of learning visual representation on both manifolds and suggests that learning on multiple manifolds is more beneficial than learning on one manifold with more parameters.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{src/fusion_models.png}
    \caption{\textbf{Performance of fusion ResNets} plotted against the number of model parameters. The circle markers represent ResNet-20 and the diamond markers represent ResNet-32. Other differences in number of parameters are due to varying channel widths of (4, 8, 16), (8, 16, 32) and (16, 32, 64). Note that fusing the Poincar\'e and Euclidean ResNets not only improves accuracy, but is actually more efficient than increasing the number of parameters of the individual models, highlighting the strong complementary nature of learning visual representations in both spaces.}
    \label{fig:fusion}
\end{figure}



