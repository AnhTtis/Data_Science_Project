We consider the problem of image classification where our dataset is denoted by $(\bm{x}_i, y_i)_{i=1}^N$, with $\bm{x}_i \in \mathbb{R}^{H \times W \times 3}$ and $y_i \in \{1, \ldots, C\}$. Here, $\bm{x}_i$ denotes the pixel values of the $i$-th input image with height $H$ and width $W$, while $y_i$ denotes the corresponding label. Our goal is to train a network $y = \phi(\bm{x})$ that maps an input image $\bm{x}$ to a label $y$. Specifically, we strive to formulate the celebrated ResNet \cite{he2015b} architecture in the Poincar\'e ball model.

In residual networks, the basic building block consists of two weight layers with a ReLU activation between the layers. Afterwards, the input is added to the transformed output through a residual connection, followed by another ReLU activation. A weight layer is typically given as a convolutional layer followed by a batch normalization.
Thus, to create Poincar\'e residual blocks, all these operations need to be formulated in hyperbolic space. Below, we separately outline how to formalize and construct (i) Poincar\'e 2D convolutions and residual blocks, (ii) how to initialize hyperbolic networks, (iii) Poincar\'e midpoint batch normalization, and (iv) forward and backward propagation of core hyperbolic operations.

\subsection{Poincar\'e convolutions and residuals}
We start by formalizing 2D convolutional operations for images in the Poincar\'e ball model using the approach of Shimizu \etal~\cite{shimizu2021}. Suppose we have an input image $\bm{x}$ with pixel values
\begin{equation}
    \bm{x}_{ij} \in \mathbb{B}_{c}^{C_{in}}, \quad i = 1, \ldots, H_{in}, \enspace j = 1, \ldots, W_{in},
\end{equation}
where $C_{in}$ is the number of input channels and where $H_{in}$ and $W_{in}$ are the height and width of the image, respectively. Then we can define a 2D Poincar\'e convolution operation with $C_{out}$ output channels and with receptive field size $K \times K$, with $K$ odd.
This approach and its Euclidean counterpart have the same grid connections between the input values and output values. Only the convolutional operations behind these connections are defined differently. So, the output will have pixel values
\begin{equation}
    \bm{h}_{ij} \in \mathbb{B}_c^{C_{out}}, \quad i = 1, \ldots, H_{out}, \enspace j = 1, \ldots, W_{out},
\end{equation}
where $\bm{h}_{kl}$ is computed from the pixels $\bm{x}_{ij}$ in the receptive field at that position, so where
\begin{align}
\begin{split}
    k - \Big\lfloor\frac{K}{2}\Big\rfloor \leq i \leq k + \Big\lfloor \frac{K}{2} \Big\rfloor, \\
    l - \Big\lfloor \frac{K}{2}\Big\rfloor \leq j \leq l + \Big\lfloor \frac{K}{2} \Big\rfloor.
\end{split}
\end{align}
We denote this receptive field at position $(k, l)$ by $X_{kl}$. Note that $H_{out}$ and $W_{out}$ depend on the input dimensions, the receptive field size $K$ and, optionally, on stride and padding.

\begin{figure}
    \centering
    \includegraphics[width=8.5cm]{src/hyperbolic_residual_block.pdf}
    \caption{\textbf{A Poincar\'e residual block}, the basic building block of our Poincar\'e ResNet architectures and a direct generalization of the original residual block of He \etal \cite{he2015b}.}
    \label{fig:residual_block}
\end{figure}

Similar to the Euclidean convolutional layer, for each $\bm{h}_{kl}$, we want to apply a fully connected layer to the concatenation of the vectors within the receptive field, so we want to compute the output as
\begin{equation}
    \bm{h}_{kl} = \mathcal{F}^c ({\parallel} X_{kl}; Z, \bm{r}),
\end{equation}
where ${\parallel} \cdot$ denotes some concatenation operation and $\mathcal{F}^c$ is the Poincar\'e fully-connected layer defined in equation (\ref{eq:FC_layer}) with parameters $Z$ and $\bm{r}$. Note that the usual concatenation is inappropriate for vectors on the Poincar\'e ball as this can result in vectors outside the manifold. We therefore employ $\beta$-concatenation as an alternative, which is a concatenation operation that preserves the expectation of the Poincar\'e norm of the output vector \cite{shimizu2021}. This operation, applied to $M$ Poincar\'e vectors $\{\bm{b}_i \in \mathbb{B}_c^{n_i}\}_{i=1}^M$ with $n = \sum_i n_i$, is defined in three steps:
\begin{enumerate}
    \item Map each of the vectors to the tangent space at the origin of their respective Poincar\'e balls: $\bm{v}_i = \log_0^c (\bm{b}_i)$;
    \item Let $\beta_n = B(\frac{n}{2}, \frac{1}{2})$, with $B$ the beta function, scale each of the vectors $\bm{v}_i$ by $\beta_n \beta_{n_i}^{-1}$ and let $\bm{v}$ be the concatenation of these scaled vectors, so $\bm{v} = (\beta_n \beta_{n_1}^{-1} \bm{v}_1^T, \ldots, \beta_n \beta_{n_N}^{-1} \bm{v}_N^T)^T$;
    \item Project the resulting vector back onto the $n$-dimensional Poincar\'e ball: $\exp_0^c (\bm{v})$.
\end{enumerate}
We denote this operation by $\prescript{}{\beta}{\parallel} \cdot$. Now, we can write the 2D Poincar\'e convolution operation as
\begin{equation}
    \bm{h}_{kl} = \mathcal{F}^c (\prescript{}{\beta}{\parallel} X_{kl}; Z, \bm{r}),
\end{equation}
where $k = 1, \ldots, H_{out}$, $l = 1, \ldots, W_{out}$ and where $\mathcal{F}^c$ maps from $\mathbb{B}_c^{K^2 \times C_{in}}$ to $\mathbb{B}_c^{C_{out}}$. 

Next, we define a Poincar\'e version of the residual block by replacing the convolutional layers by Poincar\'e convolutional layers and by applying a hyperbolic batch normalization, which will be defined in the next subsection. Pointwise nonlinearities can still be applied in the tangent space at the origin of the Poincar\'e ball by using the logarithmic and exponential maps. So, the Poincar\'e version of the ReLU nonlinearity becomes
\begin{equation}
    \text{ReLU}_P = \exp_0^c \circ \; \text{ReLU} \circ \log_0^c,
\end{equation}
where $\circ$ denotes function composition. We will use this Poincar\'e version to replace the two ReLU nonlinearities.
We can furthermore replace the skip connection by $\bm{x} \oplus_c \mathcal{G} (\bm{x})$, where $\mathcal{G}$ denotes the transformation given by the two Poincar\'e convolutional layers and Poincar\'e batch normalizations. Figure \ref{fig:residual_block} visualizes the Poincar\'e residual block. 



\begin{algorithm}[tb]
    \caption{Poincar\'e midpoint batch normalization}
    \label{alg:batch_norm}
    \begin{algorithmic}
        \STATE {\bfseries Training Input:} Data batches $\{\bm{x}_1^{(t)}, \ldots, \bm{x}_m^{(t)}\} \subseteq \mathbb{B}_c^n$ for $t \in [1, \ldots, T]$, testing momentum $\eta \in [0, 1]$
        \STATE {\bfseries Learned Parameters:} $\beta \in \mathbb{B}_c^n$, $\gamma \in \mathbb{R}$
        \STATE {\bfseries Normalization Algorithm:} 
        \FOR{$t = 1, \ldots, T$}
            \STATE $\mu \gets \text{Poincar\'eMidpoint}(\{\bm{x}_1^{(t)}, \ldots, \bm{x}_m^{(t)}\})$
            \STATE $\sigma^2 \gets \frac{1}{m} \sum_{i=1}^m d(\bm{x}_i^{(t)}, \mu)^2$
            \FOR{$i = 1, \ldots, m$}
                \STATE $\tilde{\bm{x}}_i^{(t)} \gets \exp_{\beta}^c \Big( \sqrt{\frac{\gamma}{\sigma^2}} P_{\mu \rightarrow \beta}^c (\log_{\mu}^c \bm{x}_i^{(t)}) \Big)$
            \ENDFOR
            \RETURN normalized batch $\tilde{\bm{x}}_1^{(t)}, \ldots, \tilde{\bm{x}}_m^{(t)}$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsection{Poincar\'e midpoint batch normalization}
In a residual block, each convolutional layer is immediately followed by a batch normalization step. Lou \etal \cite{lou2020} have previously defined a Poincar\'e version of batch normalization based on their iterative approximation to the Fr\'echet mean.
While more efficient than previously available methods, this iterative approach still makes the Fr\'echet mean a computationally expensive step. Directly plugging a Fr\'echet-based batch normalization in our Poincar\'e ResNet would account for roughly 77\% of the computation in a forward step. We therefore seek to perform batch normalization with greater computational efficiency.



We suggest to take an alternative aggregation of Poincar\'e vectors, namely the Poincar\'e midpoint \cite{ungar2009}. The midpoint of the Poincar\'e vectors $\{\bm{x}_i \in \mathbb{B}_c^n\}_{i=1}^N$ is computed as
\begin{equation}
    \mu = \frac{1}{2} \otimes_c \frac{\sum_{i=1}^N \lambda_{\bm{x}_i}^c \bm{x}_i}{\sum_{i=1}^N (\lambda_{\bm{x}_i}^c - 1)}.
\end{equation}
The resulting midpoint batch normalization algorithm is outlined in Algorithm \ref{alg:batch_norm}.
The goal of batch normalization is to keep feature vectors centered around the origin and to keep the variance of their norms within a manageable range. By replacing the Fr\'echet mean by the Poincar\'e midpoint, the vectors will no longer be centered exactly at the origin, but still close enough to achieve the improved stability that batch normalization normally results in. Moreover, the Poincar\'e midpoint can be computed directly without any iterative methods, making it substantially faster to compute than the Fr\'echet mean.



\subsection{Hyperbolic network initialization} %
The canonical ResNet architecture uses Kaiming initialization, which aims to prevent reduction or magnification of input signals as this would hinder convergence during training \cite{he2015a}. This is achieved by maintaining the variance of the components of both the features and the gradients throughout the network. However, such an approach is inappropriate for the Poincar\'e fully connected and convolutional layers as the components of a Poincar\'e vector are necessarily dependent, since the Euclidean norm of such vectors is bounded by $c^{-\frac{1}{2}}$. 

To that end, Shimizu \etal \cite{shimizu2021} propose to initialize the weights $Z$ of the Poincar\'e fully connected layer through sampling from the normal distribution $\mathcal{N}(0, (2mn)^{-1})$, where $m$ is the input dimension and $n$ the output dimension of the layer. The biases $r$ are initialized as zeros. We find that this initialization results in vanishing signals, where the norm of an input converges to zero after a few layers. To obtain a norm-preserving network initialization in hyperbolic space,
we take the initialization for the weights of a Poincar\'e layer mapping from $\mathbb{B}_{c}^m$ to $\mathbb{B}_{c}^n$ with $m \leq n$ as
\begin{equation}\label{eq:id_init}
    Z = 
    \begin{cases}
    \frac{1}{2} I_n & m = n, \\
    \frac{1}{2} [I_m | O_{m, n-m}] & m < n,
    \end{cases}
\end{equation}
where $I_n$ is the $n \times n$-identity matrix and where $O_{i,j}$ is the $i \times j$-zero matrix. We initialize the biases $r$ as a vector of zeros.
Using this initialization, for $m = n$, we see that
\begin{equation}
    v_k(\bm{x}) = \frac{1}{\sqrt{c}} \sinh^{-1} \Big(\sqrt{c} \lambda_{\bm{x}}^c x_k\Big),
\end{equation}
and, therefore,
\begin{align}
    w = \lambda_{\bm{x}}^c \bm{x},
\end{align}
from which it follows that $\bm{y} = \bm{x}$. When $m < n$, we get $\bm{y} = (\bm{x}^T | \bm{0}_{n-m}^T)^T$ instead, where $\bm{0}_{n-m}$ is an $(n-m)$-dimensional vector of zeros. Thus, for the cases $m \leq n$, this initialization keeps the norms of the vectors constant throughout the network.

For residual networks, $m \leq n$ for each layer except for the linear layer at the end of the network. Therefore, we initialize each of the convolutional layers using our identity initialization. The final linear layer will be initialized using the initialization by \cite{shimizu2021}. We find the vanishing effect of this single layer to be harmless to the performance.


\subsection{Optimization and backward propagation}
For neural networks on Riemannian manifolds, one generally has to consider the manifold on which the parameters live for optimization \cite{bonnabel2013riemanniansgd}. For Poincar\'e residual networks, we need to consider the weights of three different layers, namely, the fully-connected layer, the convolutional layer, and the batch normalization. The parameters of the fully-connected layer and the convolutional layer as proposed by Shimizu \etal~ \cite{shimizu2021} live in Euclidean space, so we can use Euclidean optimizers for these layers. However, the batch normalization algorithm shown in Algorithm \ref{alg:batch_norm} makes use of a parameter vector living on $\mathbb{B}_c^n$. To avoid difficulties with optimizers, we instead supply the algorithm with a parameter vector in $\mathbb{R}^n$ that is mapped to the Poincar\'e ball using the exponential map around the origin. This is used by Lou \etal \cite{lou2020} as well. As a result, we can optimize Poincar\'e residual networks using traditional Euclidean optimizers.

A direct consequence of applying hyperbolic operations in a neural network is the large computational cost incurred by the many applications of nonlinear operations. This leads to a significant increase in memory requirements as all these intermediate steps become part of the computation graph during training. To maintain compact computation graphs, we have manually derived the backward pass of several core hyperbolic operations, namely M\"obius addition, the exponential and logarithmic maps and the conformal factor $\lambda_{\bm{x}}^c$. The use of these manually defined derivatives also reduces the size of the computation graph of many other operations defined on the Poincar\'e ball, as these generally build upon the more basic operations. We find that using manually defined derivatives reduces memory usage by approximately 30\%, but increases computation time. Due to the length of the derivations, we provide a full breakdown in the supplementary materials.



