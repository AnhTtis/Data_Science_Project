

\begin{figure*}[ht]
    \begin{subfigure}[t]{0.225\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/norm_of_layer_outputs.png}
        \caption{Output norm over layers.}
        \label{fig:init_toy_ex}
    \end{subfigure}
    \hspace{0.8em}
    \begin{subfigure}[t]{0.225\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/initialization_comparison.png}
        \caption{5-layer ConvNet training.}
        \label{fig:init_convnet_1}
    \end{subfigure}
    \hspace{0.8em}
    \begin{subfigure}[t]{0.225\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/initialization_comparison_v2.png}
        \caption{7-layer ConvNet training.}
        \label{fig:init_convnet_2}
    \end{subfigure}
    \centering
    \caption{\textbf{Comparison to the initialization of \cite{shimizu2021}.} In (a), we show the logarithm of the mean of the norms of each layer's output during the forward pass of an untrained 10-layer fully-connected network for random Poincar\'e gyrovectors. The figure shows that the baseline initialization is suffering from vanishing signals with outputs that collapse to the origin over multiple layers. Our identity-based initialization maintains output norms over layers. In (b) and (c), we show the test accuracy over epochs for a 5-layer and a 7-layer ConvNet. For a 5-layer network, the baseline initialization converges more slowly, while it no longer learns for 7-layers. Our initialization is preferred for training convolutional networks in the Poincar\'e ball model.}
    \label{fig:init_exps}
\end{figure*}

We investigate (i) the effect of network initialization over many layers, (ii) the effect of curvature and ReLU activations, (iii) the comparison between Fr\'echet-based and our midpoint-based batch normalization and (iv) the robustness of hyperbolic residual networks.
We seek to evaluate Poincar\'e ResNets in isolation and hence stick to minimal augmentation and fixed hyperparameters. We use random cropping and horizontal flipping with Adam optimization with fixed learning rate $10^{-3}$ and weight-decay $10^{-4}$.

\subsection{Identity initialization is norm-preserving}\label{subsect:id_init}
The approach of Shimizu \etal \cite{shimizu2021} is the current leading initialization in hyperbolic networks. This initialization, however, leads to vanishing signals, which we empirically validate here.
We take a stack of 10 Poincar\'e linear layers with a constant curvature of $c = 1$, with 20 input and output neurons.
We then perform a single forward pass on a batch of 16 Poincar\'e gyrovectors which are generated by sampling vectors in the tangent space at the origin from the multivariate normal distribution $\mathcal{N}(0, \frac{1}{10} I_{20})$ and mapping these to the Poincar\'e ball. Figure \ref{fig:init_toy_ex} shows the behaviour of the norms during the forward pass for both initialization methods. Where the baseline initialization suffers from vanishing signals, our identity initialization keeps the norms constant up to the rounding effects of the repeated application of non-linear operations.

In Figures \ref{fig:init_convnet_1} and \ref{fig:init_convnet_2} we additionally show what happens when training a simple ConvNet on CIFAR-10 with both initialization methods trained with SGD with learning rate $10^{-3}$, momentum $0.9$ and weight decay $10^{-4}$. For a 5-layer ConvNet, the baseline initialization converges more slowly. For a 7-layer ConvNet, we find that the baseline is no longer capable of learning meaningful representations. Identity-based initialization is still able to train in this setting. We conclude that our identity-based hyperbolic network initialization is preferred for training hyperbolic networks.




\subsection{Curvatures and ReLUs stabilize optimization}
Previous works claim that nonlinearities, such as the ReLU operation, are redundant in hyperbolic neural networks due to the many nonlinearities inherent to such networks \cite{ganea2018,shimizu2021}. Here, we test this claim by training a small Poincar\'e ResNet-20 on CIFAR-10 with small channel widths of (4, 8, 16) with and without the ReLU nonlinearity as activation layer. The results are shown in Figure \ref{fig:curvature_and_relu_comp} (left). We find that training with the ReLU nonlinearity leads to faster convergence and a greater final accuracy. This shows that nonlinear activation functions remain important despite the inherent nonlinearity of hyperbolic networks.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{src/curvatures_and_relu.png}
    \caption{\textbf{ReLUs and small curvatures improve} the optimization and performance of Poincar\'e ResNet. Both experiments were performed using a small ResNet-20 with small channel widths (4, 8, 16). While hyperbolic layers are already non-linear, adding ReLUs further improves generalization. The same holds for using smaller curvatures.}
    \label{fig:curvature_and_relu_comp}
\end{figure}

\setcounter{table}{1}
\begin{table*}
\centering
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{l l cc cc cc cc cc cc}
\toprule
 & \textbf{Manifold} & \multicolumn{6}{c}{\textbf{CIFAR-10}} & \multicolumn{6}{c}{\textbf{CIFAR-100}}\\
 \cmidrule(lr){3-8} \cmidrule(lr){9-14}
& & \multicolumn{2}{c}{FPR95 ↓} & \multicolumn{2}{c}{AUROC ↑} & \multicolumn{2}{c}{AUPR ↑}  & \multicolumn{2}{c}{FPR95 ↓} & \multicolumn{2}{c}{AUROC ↑} & \multicolumn{2}{c}{AUPR ↑}\\
 \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
& & R20 & R32 & R20 & R32 & R20 & R32 & R20 & R32 & R20 & R32 & R20 & R32\\
\midrule
{\multirow{3}{*}{Places-365}} & Euclidean & 64.2 & 72.3 & \bf{84.7} & 82.0 & \bf{96.2} & 95.6 & 89.5 & 93.9 & 62.5 & 57.9 & 89.3 & 87.9\\
& w/ HNN++ & \bf{63.8} & 72.7 & 79.6 & 77.7 & 94.5 & 94.2 & 93.2 & 86.3 & 63.3 & 66.6 & 89.8 & 91.1\\
& \cellcolor{Gray}Poincar\'e & \cellcolor{Gray}70.2 & \cellcolor{Gray}\bf{70.7} & \cellcolor{Gray}82.3 & \cellcolor{Gray}\bf{82.6} & \cellcolor{Gray}95.7 & \cellcolor{Gray}\bf{95.9} & \cellcolor{Gray}\bf{82.8} & \cellcolor{Gray}\bf{83.8} & \cellcolor{Gray}\bf{71.5} & \cellcolor{Gray}\bf{71.1} & \cellcolor{Gray}\bf{92.3} & \cellcolor{Gray}\bf{92.2}\\
\midrule
{\multirow{3}{*}{SVHN}} & Euclidean & 97.3 & 94.7 & 68.8 & 73.4 & 92.8 & 94.1 & 99.5 & 98.8 & 43.7 & 54.6 & 83.7 & 88.2\\
& w/ HNN++ & 73.1 & 79.1 & \bf{85.5} & 82.2 & \bf{96.9} & 96.1 & 92.1 & 88.6 & 66.4 & 68.9 & 91.1 & 92.0\\
& \cellcolor{Gray}Poincar\'e & \cellcolor{Gray}\bf{66.0} & \cellcolor{Gray}\bf{69.3} & \cellcolor{Gray}85.0 & \cellcolor{Gray}\bf{83.6} & \cellcolor{Gray}96.6 & \cellcolor{Gray}\bf{96.3} & \cellcolor{Gray}\bf{76.9} & \cellcolor{Gray}\bf{83.0} & \cellcolor{Gray}\bf{76.8} & \cellcolor{Gray}\bf{72.6} & \cellcolor{Gray}\bf{94.1} & \cellcolor{Gray}\bf{92.9}\\
\midrule
{\multirow{3}{*}{Textures}} & Euclidean & 87.3 & 88.0 & 73.6 & 77.3 & 93.2 & 94.7 & 98.1 & 96.0 & 33.5 & 42.9 & 75.9 & 79.4\\
& w/ HNN++ & \bf{63.8} & \bf{56.6} & 79.6 & \bf{85.8} & 94.5 & \bf{96.6} & 85.9 & \bf{77.5} & 58.9 & 65.7 & 86.8 & 89.0\\
& \cellcolor{Gray}Poincar\'e & \cellcolor{Gray}68.2 & \cellcolor{Gray}66.2 & \cellcolor{Gray}\bf{82.1} & \cellcolor{Gray}82.3 & \cellcolor{Gray}\bf{95.5} & \cellcolor{Gray}95.6 & \cellcolor{Gray}\bf{83.9} & \cellcolor{Gray}84.2 & \cellcolor{Gray}\bf{67.7} & \cellcolor{Gray}\bf{68.8} & \cellcolor{Gray}\bf{91.0} & \cellcolor{Gray}\bf{91.5}\\
\bottomrule
\end{tabular}
}%
\caption{\textbf{Out-of-distribution detection} on CIFAR-10 and CIFAR-100 with Places365, SVHN, and DTD as out-of-distribution datasets. R20 and R32 denote ResNet-20 and ResNet-32 architectures, both with channel widths (8, 16, 32). Across different in- and out-of-distribution datasets, hyperbolic ResNets are more robust than their Euclidean counterparts.}
\label{tab:ood}
\end{table*}

Poincar\'e balls of various curvatures have similar geometric properties.
For numerical computations however, setting the right curvature impacts the down-stream performance \cite{gao2021curvature}. In this analysis, we investigate the effect of various curvatures for optimizing Poincar\'e ResNets.
We again perform the experiments on a small Poincar\'e ResNet-20 with small channel widths of (4, 8, 16) using a curvature of $1$, $0.1$ or $0.01$.
We show the results in Figure \ref{fig:curvature_and_relu_comp} (right). We first find that training with a curvature of $c=1$ leads to suboptimal accuracies. As the curvature becomes smaller, the Euclidean volume of the Poincar\'e ball increases. As a result, representing elements within this manifold using floating-point representations becomes easier with smaller curvatures. Indeed, when training with curvatures $c=0.1$ and $c=0.01$, we find that the model converges faster and has a higher final accuracy. Overall, we find that a curvature of $c=0.1$ works best for training Poincar\'e ResNets and we will use this setting for the rest of the experiments.

\setcounter{table}{0}
\begin{table}[t]
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{ lc c c}
\toprule
 & & \multicolumn{1}{c}{\textbf{ResNet-20}} & \multicolumn{1}{c}{\textbf{ResNet-32}}\\
\midrule
\multirow{2}{*}{Accuracy} & Fr\'echet mean & 79.4 & 82.4 \\
& Poincar\'e midpoint & 80.9 & 81.9 \\
\midrule
Time & Fr\'echet mean & 179.0 & 169.4\\
\scriptsize{($s \text{ epoch}^{-1}$)} & Poincar\'e midpoint & 137.5 & 132.0 \\
\rowcolor{Gray} & & -23\% & -22\%\\
\bottomrule
\end{tabular}
}%
\caption{\textbf{Poincar\'e midpoints for batch normalization} in hyperbolic space are as effective for classification as Fr\'echet means while being faster to optimize.}
\label{tab:bn_results}
\end{table}



\subsection{Midpoints make batch norm efficient}
To compare the computational efficiency and the performance of our Poincar\'e midpoint batch normalization to the batch normalization by \cite{lou2020}, we perform multiple experiments using Poincar\'e ResNet-20 or Poincar\'e ResNet-32 on CIFAR-10 with small channel widths of (4, 8, 16). %
We opt for a small ResNet width and fixed hyperparameters to allow for faster evaluation, all models obtain higher scores with more depth and hyperparameter tuning.
Each network is then trained with Fr\'echet-based batch normalization \cite{lou2020} or with our Poincar\'e midpoint batch normalization.

The results of the experiment are shown in Table \ref{tab:bn_results}. First, we find that both batch normalization methods lead to similar accuracies, which indicates that Poincar\'e midpoints are as effective as Fr\'echet means for classification. Second, training a network with Poincar\'e midpoint batch normalization leads to a reduction in computation time of approximately 20-25\%. We recommend Poincar\'e midpoints when performing batch normalization in hyperbolic networks.


\begin{figure*}[ht]
    \begin{subfigure}[t]{0.25\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/cifar_fgsm_8_16_32_resnet_32_v3.png}
        \caption{Adversarial robustness.}
        \label{fig:adversarial}
    \end{subfigure}
    \hspace{0.8em}
    \begin{subfigure}[t]{0.25\paperwidth}
        \centering
        \includegraphics[width=\textwidth]{src/fusion_models.png}
        \caption{Hyperbolic/Euclidean fusion.}
        \label{fig:fusion}
    \end{subfigure}
    \hspace{0.8em}
    \begin{subfigure}[t]{0.25\paperwidth}
        \centering
        \vspace{-3cm}
        \includegraphics[width=\textwidth]{src/gradcam.pdf}
        \vspace{.358cm}
        \caption{Grad-CAM visualizations.}
        \label{fig:gradcam}
    \end{subfigure}
    \centering
    \caption{Comparisons and fusions between hyperbolic and Euclidean ResNets. (a) \textbf{Robustness to FGSM adversarial attacks} between Euclidean and Poincar\'e ResNets. These results are obtained by attacking a {\color{black}Poincar\'e ResNet-32, or a Euclidean ResNet-32 with either a Euclidean classifier or a Poincar\'e classifier \cite{shimizu2021}}, with small channel widths (8, 16, 32), trained on CIFAR-10 to similar performance, with adversarial examples of varying perturbation sizes $\epsilon$. Poincar\'e ResNet is more robust to FGSM adversarial attacks. (b) \textbf{Fusion ResNets} plotted as a function of model parameters. The circle markers represent ResNet-20 and the diamond markers represent ResNet-32, with other differences due to varying channel widths of (4, 8, 16), (8, 16, 32) and (16, 32, 64). Fusing the Poincar\'e and Euclidean ResNets not only improves accuracy, but is more efficient than increasing the number of parameters of individual models, highlighting the strong complementary nature of learning visual representations in both spaces. (c) \textbf{Grad-CAM visualizations} of Euclidean (middle) and Poincar\'e (right) ResNets. (a) + (b) Both models predict the correct class while focusing on different discriminants in the image. (c) + (d) Failure case of respectively Euclidean and Poincar\'e ResNet due to a focus on ambiguous object parts.}
    \label{fig:robustness}
\end{figure*}

\subsection{Hyperbolic networks are robust}
Finally, we investigate the robustness and complementary nature of Poincar\'e ResNet compared to its Euclidean alternative. We investigate whether Poincar\'e ResNet is (i) robust to out-of-distribution samples, (ii) can handle adversarial examples, and (iii) learns complementary representations compared to Euclidean ResNet.

\textbf{Out-of-distribution detection.}
To check whether Poincar\'e ResNets are robust to out-of-distribution samples, we compare the out-of-distribution detection performance of Euclidean and Poincar\'e ResNet-20 and ResNet-32 with channel widths (8, 16, 32), trained on either CIFAR-10 or CIFAR-100 using the same hyperparameters and optimizer as before {\color{black}and where the Euclidean ResNets have either a Euclidean or Poincar\'e classifier \cite{shimizu2021}}. For each architecture, the Euclidean and hyperbolic variants have similar classification performance, hence any difference in out-of-distribution performance is not a result of improved training. We use the Places-365 dataset \cite{zhou2017places}, the SVHN dataset \cite{netzer2011reading} and the Textures dataset \cite{cimpoi2014describing} as out-of-distribution datasets. For detecting out-of-distribution samples, we use the energy score as introduced by Liu \etal~\cite{liu2020energy}. The comparisons are performed on the commonly used metrics FPR95, AUROC and AUPR.

The results are shown in Table \ref{tab:ood}. We find that with a ResNet-32 architecture, Poincar\'e ResNet outperforms {\color{black}both types of Euclidean ResNets on nearly all metrics for five out of six combinations of in- and out-of-distribution datasets}. With a ResNet-20, Poincar\'e ResNet is better {\color{black} for each combination with CIFAR-100 as the in-distrution dataset}. We conclude that a hyperbolic ResNet is more robust to out-of-distribution samples than its Euclidean counterpart{\color{black}, especially in the presence of many in-distribution classes}.

\textbf{Adversarial attacks.}
To see if Poincar\'e ResNet is robust to adversarial samples, we compare the performance against an adversarial attack between Euclidean ResNet-32 {\color{black}with either a Euclidean classifier or a Poincar\'e classifier \cite{shimizu2021}} and Poincar\'e ResNet-32, {\color{black}each} with channel widths (8, 16, 32), trained on CIFAR-10. Note that, after training, {\color{black}each model has} similar performance on the test set of CIFAR-10. We apply the fast gradient signed method (FGSM) \cite{goodfellow2015fgsm} attack with perturbations $\epsilon = \frac{0.8}{255}, \frac{1.6}{255}, \frac{2.4}{255}, \frac{3.2}{255}$ to the models. The results are shown in Figure \ref{fig:adversarial}. We find that Poincar\'e ResNet is more resistant to adversarial attacks than the Euclidean ResNet, even though both architectures were trained similarly and obtained similar classification performance. This result highlights the potential of hyperbolic learning in the presence of adversarial agents.
{\color{black}We note that Euclidean ResNets normally use running statistics, while Poincar\'e ResNets do not. Here we have disabled running statistics for the Euclidean models to ensure a fair comparison as running statistics make a model far more susceptible to adversarial attacks. The results when using running statistics are shown in the appendix.}


\textbf{\color{black}Complementary representations.}
To show that the representations learned by Poincar\'e ResNets are complementary to the features from Euclidean ResNets, we evaluate the performance of a fusion model, where each image is forwarded through both ResNets and the resulting logits are averaged to obtain predictions. Note that both models are trained independently and the fusion model is only evaluated with no further training being performed. The results are shown in Figure \ref{fig:fusion}. For each architecture, the performance on both manifolds is similar. Clearly, the performance of the fusion models is better than that of the individual ResNets. With respect to the number of parameters, we find that it is more efficient to create a fusion model than it is to increase the size of whichever ResNet we are using. In Figure \ref{fig:gradcam}, we also show Grad-CAM visualizations \cite{selvaraju2017grad}, highlighting that our approach focuses more on the different parts that form the object, instead of the single most discriminative component like in Euclidean ResNets.








