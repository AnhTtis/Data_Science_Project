\subsection{Poincar\'e ball model of hyperbolic space}
This paper operates on the most commonly used model of hyperbolic geometry in deep learning, namely the Poincar\'e ball model. We will therefore restrict the background discussion to this model and refer to Peng \etal~\cite{peng2021} for a more comprehensive discussion on the different isometric models of hyperbolic space.
The $n$-dimensional Poincar\'e ball model with constant negative curvature $-c$ is defined as the Riemannian manifold $(\mathbb{B}_c^n, \mathfrak{g}_c)$, where 
\begin{equation}
    \mathbb{B}_c^n = \{\bm{x} \in \mathbb{R}^n : ||\bm{x}||^2 < \frac{1}{c}\},
\end{equation}
and where
\begin{equation}
    \mathfrak{g}_c = \lambda_{\bm{x}}^c I_n, \quad \lambda_{\bm{x}}^c = \frac{2}{1 - c ||\bm{x}||^2},
\end{equation}
with $I_n$ being the $n$-dimensional identity matrix. 
The Poincar\'e ball model can be turned into a gyrovector space \cite{ungar2009} by endowing it with M\"obius addition and M\"obius scalar multiplication, respectively defined as
\begin{equation}
\begin{split}
    \bm{x} \oplus_c \bm{y} = & \frac{(1 + 2c \langle \bm{x}, \bm{y} \rangle + c ||\bm{y}||^2) \bm{x} + (1 - c ||\bm{x}||^2) \bm{y}}{1 + 2c \langle \bm{x}, \bm{y} \rangle + c^2 ||\bm{x}||^2 ||\bm{y}||^2},\\
    r \otimes_c \bm{x} = &\frac{1}{\sqrt{c}} \tanh \big(r \tanh^{-1} (\sqrt{c} ||\bm{x}||)\big) \frac{\bm{x}}{||\bm{x}||},
\end{split}
\end{equation}
where $\bm{x}, \bm{y} \in \mathbb{B}_c^n$, $r \in \mathbb{R}$ and where $||\cdot||$ and $\langle \cdot, \cdot \rangle$ denote the Euclidean norm and inner product, respectively. An important map related to this gyrovector space is the gyrator $\text{gyr} : \mathbb{B}_c^n \times \mathbb{B}_c^n \rightarrow \text{Aut}(\mathbb{B}_c^n, \oplus_c)$, where $\text{Aut}(\mathbb{B}_c^n, \oplus_c)$ denotes the set of automorphisms on $\mathbb{B}_c^n$ \cite{ungar2009}. This map is implicitly defined as
\begin{equation}
    \text{gyr}[\bm{x}, \bm{y}] \bm{z} = - (\bm{x} \oplus_c \bm{y}) \oplus_c \big(\bm{x} \oplus_c (\bm{y} \oplus_c \bm{z})\big),
\end{equation}
where $\bm{x}, \bm{y}, \bm{z} \in \mathbb{B}_c^n$, which can be used to measure the extent to which M\"obius addition deviates from commutativity. It will be used later on to define parallel transport. Furthermore, we can compute the distance between any two points $\bm{x}, \bm{y} \in \mathbb{B}_c^n$ as
\begin{equation}
    d_c(\bm{x}, \bm{y}) = \frac{2}{\sqrt{c}} \tanh^{-1} (\sqrt{c} ||-\bm{x} \oplus_c \bm{y}||).
\end{equation}
For an in-depth analysis of this gyrovector space approach to the Poincar\'e ball see \cite{ungar2009}.
Using the definition of M\"obius addition, the exponential and logarithmic maps can be written as \cite{ganea2018}
\begin{equation*}
\begin{split}
    \exp_{\bm{x}}^c (\bm{v}) &= \bm{x} \oplus_c \Big(\tanh\big(\frac{\sqrt{c} \lambda_{\bm{x}}^c ||\bm{v}||}{2}\big) \frac{\bm{v}}{\sqrt{c} ||\bm{v}||}\Big),\\
    \log_{\bm{x}}^c (\bm{y}) &= \frac{2}{\sqrt{c} \lambda_{\bm{x}}^c} \tanh^{-1} \big(\sqrt{c} ||-\bm{x} \oplus_c \bm{y}||\big) \frac{-\bm{x} \oplus_c \bm{y}}{||-\bm{x} \oplus_c \bm{y}||},
\end{split}
\end{equation*}
where $\bm{x}, \bm{y} \in \mathbb{B}_c^n$ and $\bm{v} \in \mathcal{T}_{\bm{x}} \mathbb{B}_c^n$. Moreover, we can define parallel transport $P_{\bm{x} \rightarrow \bm{y}}^c : \mathcal{T}_{\bm{x}} \mathbb{B}_c^n \rightarrow \mathcal{T}_{\bm{y}} \mathbb{B}_c^n$ as follows \cite{shimizu2021}
\begin{equation}
    P_{\bm{x} \rightarrow \bm{y}}^c (\bm{v}) = \frac{\lambda_{\bm{x}}^c}{\lambda_{\bm{y}}^c} \text{gyr}[\bm{y}, -\bm{x}]\bm{v},
\end{equation}
which allows us to transport a tangent vector at a point $\bm{x} \in \mathbb{B}_c^n$ to the tangent space at another point $\bm{y} \in \mathbb{B}_c^n$, used for example in batch normalization.


\subsection{The Poincar\'e ball model in neural networks}
\label{subsect:poincare_nn}
To perform deep learning on the Poincar\'e ball model, Ganea \etal~\cite{ganea2018} outline a theoretical framework for incorporating this model into core layers of neural networks, such as hyperbolic logistic regression, hyperbolic fully-connected, and hyperbolic recurrent layers.
More recently, Shimizu \etal~\cite{shimizu2021} made important improvements to this framework to ensure that the hyperbolic geometry was fully taken advantage of without the need for additional learnable parameters. We will therefore use this work as a starting point for the rest of this paper and provide a short overview here.

As a foundation, Poincar\'e multinomial logistic regression is defined by computing the score for each of $n$ classes for some input $\bm{x} \in \mathbb{B}_c^m$ as
\begin{align*}
    v_k (\bm{x}) = \frac{2}{\sqrt{c}} ||\bm{z}_k|| \sinh^{-1} \Big(\lambda_{\bm{x}}^c \langle \sqrt{c} \bm{x}, \frac{\bm{z}_k}{||\bm{z}_k||} \rangle \cosh(2 \sqrt{c} r_k) \\
    - (\lambda_{\bm{x}}^c - 1) \sinh(2 \sqrt{c} r_k)\Big),
\end{align*}
where $\bm{z}_k \in \mathcal{T}_{\bm{0}} \mathbb{B}_c^m = \mathbb{R}^m$ and $r_k \in \mathbb{R}$ are the parameters for the $k$-th class. These scores are equivalent to the distances between the input $\bm{x}$ and the $n$ different Poincar\'e hyperplanes determined by the parameters $\{(\bm{z}_k, r_k)\}_{i=1}^n$. Here, $\bm{z}_k$ determines the orientation of the hyperplane while $r_k$ determines its offset with respect to the origin. A Poincar\'e fully connected layer mapping input $\bm{x} \in \mathbb{B}_c^m$ to $\mathbb{B}_c^n$ is in turn defined as
\begin{equation}\label{eq:FC_layer}
    \bm{y} = \mathcal{F}^c (\bm{x}; Z, \bm{r}) = \frac{\bm{w}}{1 + \sqrt{1 + c ||\bm{w}||^2}},
\end{equation}
with 
\begin{equation}
    \bm{w} = \Big(\frac{1}{\sqrt{c}} \sinh(\sqrt{c} v_k (\bm{x}))\Big)_{k=1}^n,
\end{equation}
where the $v_k (\cdot)$ are the scores from the Poincar\'e multinomial logistic regression and where $Z = [\bm{z}_1 | \ldots | \bm{z}_n] \in (\mathcal{T}_{\bm{0}} \mathbb{B}_c^m)^n = \mathbb{R}^{m \times n}$ and $\bm{r} = (r_k)_{k=1}^n \in \mathbb{R}^m$ are the parameters of the layer. Given hyperbolic fully connected layers, Shimizu \etal~\cite{shimizu2021} outline general formulations for self-attention and convolutional operations in hyperbolic space. We take such investigations to the visual domain and arrive at Poincar\'e ResNets, which require 2D convolutions, fast batch normalization, residual blocks, norm-preserving initialization and derived backpropagation of core operations in order to be realized.

\subsection{Hyperbolic learning in computer vision}
Khrulkov \etal~\cite{khrulkov2020hyperbolic} have shown that both image data and labels contain hierarchical structures and introduced Hyperbolic Image Embeddings to exploit these observations. In their approach, embeddings of images from standard networks are mapped to hyperbolic space, followed by a final classification layer based on hyperbolic logistic regression or hyperbolic prototypical learning, directly improving few-shot learning and uncertainty quantification.

A wide range of works have investigated hyperbolic visual embeddings, see Mettes \etal \cite{mettes2023hyperbolic}. Several works have proposed prototypes-based hyperbolic embeddings for few-shot learning \cite{fang2021kernel,gao2021curvature,guo2022clipped,ma2022adaptive,zhang2022hyperbolic}, where hyperbolic space consistently outperforms Euclidean space. Hyperbolic embeddings of classes based on their hierarchical relations has also shown to be effective for zero-shot learning \cite{liu2020hyperbolic,xu2022meta} and hierarchical recognition \cite{dhall2020hierarchical,ghadimi2021hyperbolic,long2020searching,yu2022skin}. Hyperbolic embeddings have furthermore been effective in metric learning \cite{ermolov2022hyperbolic,zhang2021learning}, object detection \cite{valada2022hyperbolic}, image segmentation \cite{chen2022hyperbolic,atigh2022hyperbolic} and future prediction in videos~\cite{suris2021learning}. 

In generative learning, hyperbolic variational auto-encoders \cite{hsu2021capturing,mathieu2019continuous,nagano2019wrapped}, generative adversarial networks \cite{lazcano2021hgan} and normalizing flows \cite{bose2020latent,mathieu2020riemannian} have been shown to obtain competitive results in data-constrained settings. A number of recent works have proposed unsupervised hyperbolic learning approaches \cite{hsu2021capturing,monath2019gradient,weng2021unsupervised,yan2021unsupervised}, allowing for learning and discovering hierarchical representations.

This body of literature highlights that hyperbolic geometry is fruitful for visual understanding. In current literature, however, hyperbolic learning is restricted to the final embedding layers, with all visual representations being learned by standard networks. This paper strives to learn hyperbolic representations in an end-to-end manner, from pixels to labels, complementing current research on computer vision with hyperbolic embeddings.