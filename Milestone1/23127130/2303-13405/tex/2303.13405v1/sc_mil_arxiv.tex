\documentclass[runningheads]{llncs}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}

\begin{document}

\title{SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced Classification in Pathology}
\titlerunning{Supervised Contrastive Multiple Instance Learning}


\author{Dinkar Juyal$^*$\inst{1}\and
Siddhant Shingi$^{*+}$\inst{2} \and
Syed Ashar Javed\inst{1} \and
Harshith Padigela\inst{1} \and
Chintan Shah\inst{1} \and
Anand Sampat\inst{1} \and
Archit Khosla\inst{1} \and
John Abel\inst{1} \and
Amaro Taylor-Weiner\inst{1}}

\authorrunning{D. Juyal et al.}
\institute{PathAI Inc, Boston, USA \and
University of Massachusetts, Amherst, USA
\\
\email{dinkar.juyal@pathai.com}}


\maketitle             
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work}
\def\thefootnote{+}\footnotetext{Work done during an internship at PathAI}

\begin{abstract}

Multiple Instance learning (MIL) models have been extensively used in pathology to predict biomarkers and risk-stratify patients from gigapixel-sized images. Machine learning problems in medical imaging often deal with rare diseases, making it important for these models to work in a label-imbalanced setting. Furthermore, these imbalances can occur in out-of-distribution (OOD) datasets when the models are deployed in the real-world. We leverage the idea that decoupling feature and classifier learning can lead to improved decision boundaries for label imbalanced datasets. To this end, we investigate the integration of supervised contrastive learning with multiple instance learning (SC-MIL). Specifically, we propose a joint-training MIL framework in the presence of label imbalance that progressively transitions from learning bag-level representations to optimal classifier learning. We perform experiments with different imbalance settings for two well-studied problems in cancer pathology: subtyping of non-small cell lung cancer and subtyping of renal cell carcinoma. SC-MIL provides large and consistent improvements over other techniques on both in-distribution (ID) and OOD held-out sets across multiple imbalanced settings.

\keywords{Multiple Instance Learning  \and Supervised Contrastive Learning \and Label Imbalance \and Out-Of-Distribution Performance \and Pathology}
\end{abstract}


\section{Introduction}
Pathology is the microscopic study of tissue and a key component in medical diagnosis and drug development \cite{walk2009role}. The digitization of tissue slides, resulting in whole slide images (WSIs), has made pathology data more accessible for quantitative analysis. However, the large size (billions of pixels) and information density (hundreds of thousands of cells and heterogeneous tissue organization) of WSIs make manual analysis challenging \cite{niazi2019digital,javed2022rethinking}, highlighting the need for machine learning (ML) approaches \cite{wang2016deep,campanella2019clinical,bosch2021machine,SelfTrainingML4H,bulten2021artificial}. ML techniques have been used for predicting a patient's clinical characteristics from a WSI. These models predict a label or score for the entire WSI, referred to as a slide-level prediction. Traditional approaches for handling large WSIs include the use of hand-engineered representations or breaking the slide into thousands of smaller patches \cite{diao2021human}. Both of these approaches require pixel or patch level annotations which are costly. To overcome the need for patch level labels, multiple instance learning (MIL) \cite{maron1997framework} has been applied to pathology by treating patches from slides as instances that form a bag, with a slide-level label associated with each bag. The MIL framework thus provides an end-to-end learning approach for problems in pathology. 

Label distribution in real-world settings can vary considerably depending on factors such as disease prevalence, population characteristics and the hospital or laboratory of origin. For example, a dataset of WSIs from a diagnostic lab may have a different class distribution compared to a dataset from a clinical trial enriched for certain disease characteristics. MIL models should be robust to variations in label distribution to succeed in clinical applications and maintain physician trust. Different approaches have been proposed to deal with label imbalance, ranging from data resampling (oversampling of minority classes or undersampling of majority classes) \cite{more2016surveyresample,mera2015bagoversample}, loss reweighting \cite{ren2018LearnReweigh}, selective enrichment of minority classes in image or feature space \cite{chou20120remix}, decoupling representation learning from classification \cite{zhou2020bbn}, and custom loss functions \cite{Cao2019ldam}. 

Contrastive learning aims to learn representations that maximize the agreement among positive instances, e.g., different augmentations of the same image, and minimize the agreement with negative instances, e.g., other images in the dataset \cite{Ciga2020SelfSC}. In supervised contrastive learning (SCL) \cite{Khosla2020scl}, the contrastive loss formulation incorporates label information by treating all instances within the same class as positive examples for a given image. SCL adopts a two-stage learning technique where a feature extractor is learned in the first stage using a contrastive loss, followed by learning a classifier using the cross-entropy loss in the second stage.

This work proposes SC-MIL: a novel MIL technique to tackle label imbalance that integrates SCL into the MIL framework. We take inspiration from prior work \cite{kang2021balancedfeat,Graf2021DissectingSC} which shows that a) contrastive loss learns balanced feature spaces (i.e., feature spaces with similar inter-class separation for all classes) compared to cross-entropy, and b) this balance is positively related to performance across imbalanced settings. Additionally, we use a smooth transition from feature learning to classifier learning in the course of training, which allows the model to learn a more discriminative latent space, aiding in imbalanced classification \cite{Wang2021HybridContrastive}. In the MIL setting, labels are only available for a bag (i.e., a collection of patches) and not individual patches. Applying SCL to patch features assumes assigning a bag-label to individual patches. However, a single patch might not have any information about the WSI label. For example, a malignant WSI might have many patches which contain only normal tissue. This motivates our bag-level formulation of SC-MIL where contrastive loss is applied to the bag features. In addition, learning robust WSI representations is more desirable than patch representations for modeling patient outcome and clinical characteristics in pathology. The contributions of this work are as follows:
\begin{enumerate}
    \item We tackle the problem of label imbalance by proposing a formulation that extends SCL to the MIL setting. We investigate two training strategies for optimal feature and classifier learning with SC-MIL.
    \item We conduct an extensive study on the performance of this technique across different degrees of label imbalance on two open-source datasets: subtyping in non-small cell lung cancer (NSCLC) and renal cell carcinoma (RCC). We compare this to previous state-of-the-art methods used for label imbalance and demonstrate the effectiveness of using SC-MIL over these methods.
    \item We show substantial performance improvements with SC-MIL on OOD data across multiple degrees of label imbalance, making a strong case for its utility in real-world deployment scenarios.
\end{enumerate}
\begin{figure}[h!]
\centering

\includegraphics[scale=0.5]{SC-MIL.pdf}
\caption{SC-MIL integrates supervised contrastive learning into the MIL framework. The model performs joint feature and classifier learning on bag representations computed using an attention-based aggregation on patches. The training objective transitions progressively from a contrastive to a classification loss.}
\label{sc_mil_fig}
\end{figure}


\section{Supervised Contrastive Multiple Instance Learning}

\subsection{Multiple Instance Learning}
MIL is a weakly supervised learning approach that allows learning and making predictions on a group of instances. Unlike supervised learning, the MIL framework only requires labels for the group of instances, called a bag, but not the individual instances. This is valuable in the context of pathology, where a collection of patches from a WSI can be treated as a bag and this allows learning of slide-level predictors without the need for fine-grained patch-level annotations.
In the binary case, a bag is considered positive if it has at least one positive instance and negative if there are none. Given a set of instances $X = \{x_0, x_1,\dots\, x_n\}$, the MIL prediction $p(X)$ is 
\begin{equation} \label{eq:1}
p(X) = a(f(x_0), f(x_1), ....., f(x_n))
\end{equation}
where $f$ is an encoder for instances, $a$ is a permutation-invariant aggregator, mapping from feature space to the prediction space. 
Learnt aggregation functions like AttentionMIL and its variants DSMIL \cite{li2021dual}, TransMIL \cite{shao2021transmil}, AdditiveMIL \cite{javed2022additive} have shown significant improvements over heuristic aggregators like Max or Mean in various tasks \cite{ilse2018attention}. We will focus on the AttentionMIL formulation for our discussion.

The aggregator function $a$ in AttentionMIL has two components. An attention module $m$ induces a soft-attention $\alpha_i$ over the instances and computes an attention weighted aggregation of instance features to generate the bag embedding $b(X)$. A classifier $h$ maps the bag feature to the bag prediction.
\begin{equation} \label{eq:3}
p(X) = h(b(X))
\end{equation}
\begin{equation}
b(X) = m(f(x_0), f(x_1), ....., f(x_n)) = \sum_{i=0}^{i=n} \alpha_i f(x_i)
\end{equation}
\begin{equation}
\alpha_i = softmax(\phi_m(x_i))
\end{equation}
where $\phi_m$ is a neural network with a non-linear activation.


\subsection{SC-MIL: Supervised Contrastive Multiple Instance Learning} \label{sec:sc_mil}
SCL \cite{Khosla2020scl} proposes a way to leverage contrastive learning and incorporate supervision. It learns instance representations by pulling instances from same class together and those from different classes apart in the representation space. In MIL, we can use SCL for learning either instance or bag representations. Considering we only have labels for bags and not individual instances, using SCL to learn instance representations needs using bag labels as instance labels, thus introducing label noise and breaking the MIL assumption. Instead, we use SCL to learn bag representations. 

Specifically, given a set of instances for a bag $X_i = \{x_0, x_1,\dots\, x_n\}$, we compute the bag representation $b(X_i)$ using the MIL formulation, where $i$ denotes the index of a bag in a given batch. We now use a non-linear multi-layer perceptron $g$ to generate the projection $z_i$ for the bag representation. We then compute the SCL loss for MIL $\mathcal{L}_{SCL}$ as follows:
\begin{equation}
    z_i = g(b(X_i)) 
\end{equation}
\begin{equation}
    \mathcal{L}_{SCL}=\sum_i -\frac{1}{|\textbf{P}_i^{+}|}\sum_{z_j \in \textbf{P}_i^{+}}\log\frac{\exp(z_i \cdot z_j/\tau)}{\sum_{z_k \in \textbf{B}_i}\exp(z_i \cdot z_k/\tau)}
\end{equation}
where $\textbf{P}_i^{+}$ denotes the positive bags sharing the same class label as bag $z_i$ and $\textbf{B}_i$ is the set of all bags in the batch excluding bag $z_i$.

 Curriculum-based feature and classifier learning using both contrastive and cross entropy losses has been shown to be effective in long-tailed image classification \cite{Kang2020Decoupling}. We apply the same approach to the MIL setting at a bag level. For classifier learning, we use the cross-entropy loss. The classifier branch projects the bag embedding $b(X)$ to the prediction $p(X)$ as shown in equation \ref{eq:3} and uses cross entropy $\mathcal{L}_{CE}$ to learn the classifier:
\begin{equation}
    \mathcal{L}_{SC-MIL} = \beta_t \mathcal{L}_{SCL} + (1 - \beta_t) \mathcal{L}_{CE}
\end{equation}
 where the weight $\beta_t \in [0,1]$ is decayed through the course of training iterations $t$ using a curriculum to gradually transition from feature to classifier learning.


\begin{table}
    \parbox{.48\linewidth}{
    \centering
        \caption{Training data-distribution of TCGA RCC sub-typing across imbalance ratios}
        \begin{tabular*}{\linewidth}{@{\extracolsep{\stretch{1}}}@{}cccc@{}}
            \toprule
            \multicolumn{1}{c}{\multirow{2}{*}{Classes}} & \multicolumn{3}{c}{Imbalance Ratio} \\ \cmidrule(lr){2-4} 
            \multicolumn{1}{c}{}                         & 1          & 5          & 10        \\ \midrule
            KIRC                                         & 96         & 205        & 240       \\
            KIRP                                         & 96         & 41         & 24        \\
            KICH                                         & 96         & 41         & 24        \\ \midrule
            Total                                        & 288        & 287        & 288      \\ \bottomrule
            \end{tabular*}%
           
        \label{tab:rcc_data}
    }
    \hfill
    \parbox{.46\linewidth}{
    \centering
    \caption{Training data-distribution of TCGA NSCLC sub-typing across imbalance ratios}
    
    \begin{tabular*}{\linewidth}{@{\extracolsep{\stretch{1}}}@{}cccc@{}}
        \toprule
        \multicolumn{1}{c}{\multirow{2}{*}{Classes}} & \multicolumn{3}{c}{Imbalance Ratio} \\ \cmidrule(lr){2-4} 
        \multicolumn{1}{c}{}                         & 1          & 5          & 10        \\ \midrule
        LUAD                                        & 158        & 265        & 290       \\
        LUSC                                     & 158        & 53         & 29        \\  \midrule
        Total                                        & 316        & 318        & 319      \\ \bottomrule
        \end{tabular*}%
        
        \label{tab:nsclc_data}
    }
    
\end{table}


\section{Experiments and Results} 
We first introduce the datasets used for experimentation. We describe the mechanism of simulating different degrees of imbalance in these datasets while ensuring that the total number of samples remains consistent. We then discuss results on all datasets using SC-MIL and other baselines. Finally, we present ablation studies to understand the tradeoffs made in terms of training supervised contrastive loss with cross-entropy jointly vs sequentially, and the impact of hyperparameters. 

\subsection{Datasets and Setup} 
We considered two datasets from The Cancer Genome Atlas (TCGA) \cite{weinstein2013cancer}  - prediction of cancer subtypes in non-small cell lung carcinoma (NSCLC) and renal cell carcinoma (RCC). TCGA-NSCLC contains a total of 1002 WSIs stained with H\&E, 538 of which were collected from patients with the adenocarcinoma histologic subtype (LUAD) and 464 from squamous cell carcinoma (LUSC). TCGA-RCC contains 948 WSIs with three histologic subtypes: 158 WSIs with the label chromophobe RCC (KICH), 504 WSIs belonging to clear cell RCC (KIRC), and 286 to papillary RCC (KIRP).

We performed a label-stratified split of both datasets while ensuring there is no leakage of case information (i.e., combination of tissue source site and study participant) across splits. The splitting ratio was 60:15:25 (train:val:test); other clinical or sample characteristics were not used during splitting. To simulate varying degrees of label imbalance, we sampled WSIs from the available classes to generate imbalance in the train set, while the heldout sets were kept the same. In line with previous works \cite{Cao2019ldam,Wang2021HybridContrastive}, we used imbalance ratio $\rho = \frac{\max_i\{n_i\}}{\min_i\{n_i\}}$ which denotes the ratio of number of examples of the majority class to the minority class. We experimented with imbalance ratios of 1, 5 and 10. We ensured that the number of training examples remained consistent across different imbalance ratios to remove any confounding effect of the number of data points and to enable comparison of model performance across imbalance ratios. Since there were three classes in TCGA-RCC, the two classes with least number of samples (KIRP and KICH) were treated as minority classes. The details of the resulting dataset composition is shared in tables \ref{tab:rcc_data} and \ref{tab:nsclc_data}.

We also deployed all models on two OOD datasets collected from different patient populations and having different sample characteristics for NSCLC and RCC. These OOD datasets are acquired from other laboratories using varying image acquisition and processing steps resulting in visual differences from their TCGA counterparts. OOD NSCLC has 162 LUAD and 45 LUSC WSIs, while OOD RCC has 254 KIRC, 134 KIRP and 46 KICH WSIs. Example images comparing ID and OOD datasets are shared in the Supplementary Section.

\subsection{Implementation Details} 
We trained five models: a baseline attention MIL model with random sampling (ERM-RS) and class balanced sampling (ERM-CB), a version using label-distribution-aware margin loss with deferred reweighting (LDAM-DRW \cite{Cao2019ldam}, previously shown to be successful for addressing label imbalance in single instance classification), and our proposed SC-MIL with random (SC-MIL-RS) and class balanced sampling (SC-MIL-CB). Non-overlapping patches of size $224 \times 224$ pixels were selected from tissue regions (using a separate model which masks background and artifacts) at a resolution of 1 micron per pixel. We extracted 1.45 million patches from TCGA-NSCLC and 768k patches from TCGA-RCC. Bag sizes (number of patches in a bag) varied from 24 to 50 patches and batch sizes (number of bags in a batch) varied from 16 to 32. An ImageNet-pretrained ShuffleNet was used to extract features from input patches. All models were trained end-to-end with the Adam optimizer and a learning rate of $1$e-$4$. SC-MIL models were trained with a temperature $\tau = 1$, and the training was performed jointly with cross entropy with a linear curriculum as described in Section \ref{sec:sc_mil}, with $\beta_t = 1$ at the start of training. For inference, patches were exhaustively sampled from a WSI and the majority prediction across bags was selected as the WSI-level prediction. For RCC, macro-averaged F1 score and macro-average of 1-vs-rest AUROC was computed.  Training and inference was performed on Quadro RTX 8000 GPUs using PyTorch v1.11 and CUDA 10.2. The training time for SC-MIL was comparable with other techniques (10-14 GPU hours). 

\subsection{Experimental Results and Ablation Studies} 

\subsubsection{Comparison of SC-MIL with other techniques.}
We compared the predictive performance of SC-MIL with other techniques across different imbalance ratios. Table \ref{tab:nsclc_results} and \ref{tab:rcc_results} show results on the NSCLC and RCC test sets respectively. SC-MIL outperforms other techniques across all imbalance ratios, and the difference is more pronounced at higher imbalance ratios. 

To further stress test these methods, we also deployed these models on independent OOD test datasets described above. We found that baseline model performance dropped notably across imbalance ratios, highlighting the difficulty in generalization, and the tendency of these models to overfit in an imbalanced setting. Performance improvements using SC-MIL persist in this OOD setting.

\vspace{-0.3in}
\begin{center}
\begin{table}[]
\caption{Comparison of SC-MIL with other label imbalance techniques on ID (TCGA-NSCLC) and OOD test sets for NSCLC subtyping. We observe that SC-MIL demonstrates strong performance and is able to overcome the failure of the baselines on the OOD set (RS - Random Sampling, CB - Class Balanced)}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{ccccccc@{\hskip 0.1in}cccccc}
    \toprule
    Dataset         & \multicolumn{6}{c}{TCGA-NSCLC}                                                                                                                                                                                      & \multicolumn{6}{c}{OOD NSCLC}                                                                                                                                                                                       \\ \cmidrule(lr){1-1} \cmidrule(lr){2-7} \cmidrule(lr){8-13}  
    Imbalance Ratio & \multicolumn{2}{c}{1}                                                      & \multicolumn{2}{c}{5}                                                      & \multicolumn{2}{c}{10}                                & \multicolumn{2}{c}{1}                                                      & \multicolumn{2}{c}{5}                                                      & \multicolumn{2}{c}{10}                                \\ \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
    Metric (\%)         & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & AUC           & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & AUC           \\ \midrule
    ERM-RS          & \multicolumn{1}{c}{82.21}          & \multicolumn{1}{c}{91.42}          & \multicolumn{1}{c}{81.57}            & \multicolumn{1}{c}{89.73}          & \multicolumn{1}{c}{77.36}          & 89.63          & \multicolumn{1}{c}{70.97}          & \multicolumn{1}{c}{92.37}          & \multicolumn{1}{c}{20.00}          & \multicolumn{1}{c}{75.95}           & \multicolumn{1}{c}{8.51}          & 76.74          \\ 
    ERM-CB          & \multicolumn{1}{c}{83.27}          & \multicolumn{1}{c}{91.75}          & \multicolumn{1}{c}{82.70}          & \multicolumn{1}{c}{90.21}          & \multicolumn{1}{c}{78.14}          & 88.56          & \multicolumn{1}{c}{36.67}          & \multicolumn{1}{c}{92.50}          & \multicolumn{1}{c}{16.33}          & \multicolumn{1}{c}{77.12}          & \multicolumn{1}{c}{12.5}          & 88.56          \\ \midrule
    LDAM-DRW        & \multicolumn{1}{c}{85.94}          & \multicolumn{1}{c}{91.99}          & \multicolumn{1}{c}{80.93}          & \multicolumn{1}{c}{89.82}          & \multicolumn{1}{c}{81.74}          & 89.35          & \multicolumn{1}{c}{61.97}          & \multicolumn{1}{c}{90.23}          & \multicolumn{1}{c}{26.92}          & \multicolumn{1}{c}{89.67}          & \multicolumn{1}{c}{29.09}          & 88.30          \\ \midrule
    SC-MIL-RS          & \multicolumn{1}{c}{\textbf{87.60}}  & \multicolumn{1}{c}{\textbf{94.80}}  & \multicolumn{1}{c}{86.67}        & \multicolumn{1}{c}{\textbf{92.44}} & \multicolumn{1}{c}{\textbf{84.26}} & \textbf{91.71} & \multicolumn{1}{c}{\textbf{76.74}} & \multicolumn{1}{c}{\textbf{93.64}} &       \multicolumn{1}{c}{37.93} & \multicolumn{1}{c}{91.63}                   & \multicolumn{1}{c}{\textbf{41.38}} & \textbf{92.89}          \\ 
    SC-MIL-CB          & \multicolumn{1}{c}{84.87}         & \multicolumn{1}{c}{93.12}          & \multicolumn{1}{c}{\textbf{87.14}}   & \multicolumn{1}{c}{92.23}          & \multicolumn{1}{c}{80.37}          & 90.95          & \multicolumn{1}{c}{59.09}          & \multicolumn{1}{c}{84.90}          & \multicolumn{1}{c}{\textbf{50.00}}   & \multicolumn{1}{c}{\textbf{92.66}}          & \multicolumn{1}{c}{29.09}          & 79.14 \\ \bottomrule
    \end{tabular}%
    }
    
    \label{tab:nsclc_results}
    \end{table}


\vspace{-0.5in}
\begin{table}[]
\caption{Comparison on TCGA-RCC and OOD datasets for RCC subtyping.  SC-MIL had SOTA performance across all imbalance ratios for both ID and OOD sets (RS - Random Sampling, CB - Class Balanced Sampling)}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccc@{\hskip 0.08in}cccccc}
\toprule
Dataset         & \multicolumn{6}{c}{TCGA-RCC             }                                                                                                                                                                                      & \multicolumn{6}{c}{OOD RCC}                                                                                                                                                                                       \\ \cmidrule(lr){1-1} \cmidrule(lr){2-7} \cmidrule(lr){8-13}  
Imbalance Ratio & \multicolumn{2}{c}{1}                                                      & \multicolumn{2}{c}{5}                                                      & \multicolumn{2}{c}{10}                                & \multicolumn{2}{c}{1}                                                      & \multicolumn{2}{c}{5}                                                      & \multicolumn{2}{c}{10}                                \\ \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
Metric (\%)          & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & AUC           & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & \multicolumn{1}{c}{AUC}           & \multicolumn{1}{c}{F1}              & AUC           \\ \midrule
ERM-RS          & \multicolumn{1}{c}{87.07}          & \multicolumn{1}{c}{96.20}          & \multicolumn{1}{c}{83.83}            & \multicolumn{1}{c}{95.74}          & \multicolumn{1}{c}{78.68}          & 93.42          & \multicolumn{1}{c}{73.94}          & \multicolumn{1}{c}{93.85}          & \multicolumn{1}{c}{73.79}          & \multicolumn{1}{c}{91.18}           & \multicolumn{1}{c}{70.39}          & 88.05          \\
ERM-CB          & \multicolumn{1}{c}{89.41}          & \multicolumn{1}{c}{97.42}          & \multicolumn{1}{c}{85.14}          & \multicolumn{1}{c}{94.09}          & \multicolumn{1}{c}{74.15}          & 95.22         & \multicolumn{1}{c}{77.47}          & \multicolumn{1}{c}{92.93}          & \multicolumn{1}{c}{72.50}          & \multicolumn{1}{c}{89.57}          & \multicolumn{1}{c}{71.72}          & 91.07          \\ \midrule
LDAM-DRW        & \multicolumn{1}{c}{89.43}          & \multicolumn{1}{c}{97.58}          & \multicolumn{1}{c}{83.77}          & \multicolumn{1}{c}{95.09}          & \multicolumn{1}{c}{80.90}          & 92.97          & \multicolumn{1}{c}{79.89}          & \multicolumn{1}{c}{94.01}          & \multicolumn{1}{c}{73.42}          & \multicolumn{1}{c}{88.45}          & \multicolumn{1}{c}{72.47}          & 91.91          \\ \midrule
SC-MIL-RS          & \multicolumn{1}{c}{88.54}          & \multicolumn{1}{c}{\textbf{98.13}} & \multicolumn{1}{c}{\textbf{86.41}} & \multicolumn{1}{c}{\textbf{96.84}} & \multicolumn{1}{c}{\textbf{87.52}} & \textbf{96.53} & \multicolumn{1}{c}{\textbf{82.10}} & \multicolumn{1}{c}{\textbf{94.70}} & \multicolumn{1}{c}{\textbf{81.93}} & \multicolumn{1}{c}{\textbf{93.43}} & \multicolumn{1}{c}{\textbf{81.10}} & 92.47          \\ 
SC-MIL-CB          & \multicolumn{1}{c}{\textbf{90.01}} & \multicolumn{1}{c}{97.89}          & \multicolumn{1}{c}{85.49}          & \multicolumn{1}{c}{96.68}          & \multicolumn{1}{c}{81.17}          & 96.17          & \multicolumn{1}{c}{76.65}          & \multicolumn{1}{c}{93.78}          & \multicolumn{1}{c}{78.89}          & \multicolumn{1}{c}{92.66}          & \multicolumn{1}{c}{79.60}          & \textbf{92.84} \\ \bottomrule
\end{tabular}%
}

\label{tab:rcc_results}
\end{table}
\end{center}


\subsubsection{Impact of sampling.} We found that SC-MIL with random sampling performs better than class balanced sampling in most cases. We hypothesize that this is due to reduced diversity in the feature space as a side effect of oversampling the minority classes or under sampling the majority class when using class-balanced sampling, which ultimately hurts performance by interfering with feature learning \cite{Wang2021HybridContrastive}. 

\subsubsection{Impact of temperature.}
We experimented with temperature values of $\tau \in \{0.1, 0.5, 1\}$ and found that the models are generally robust to temperature changes as shown in Table \ref{tab:abalations}. We reason about this through two desirable properties of representations learned through contrastive learning: \textit{uniformity} in the hypersphere, i.e, inter-class separation and \textit{tolerance} to potential positives, i.e., intra-class similarity \cite{Wang2021understandingCL}. The former is favored by low values of temperature while higher values favor the latter. As shown in \cite{Wang2021understandingCL},  in problems with a larger number of classes, uniformity is harder to achieve and higher values of temperature harm feature quality. In contrast, we see that for RCC and NSCLC subtyping with 3 and 2 classes respectively, model performance is less sensitive to changes in temperature. 


\begin{table}
\parbox{.48\linewidth}{
\centering
    \caption{Impact of temperature ($\tau$) on single-stage training SC-MIL-RS on TCGA-RCC subtyping.}
\begin{tabular*}{\linewidth}{@{\extracolsep{\stretch{1}}}@{}cccc@{}}
    \toprule
    Imb. Ratio          & Temp & F1     & AUC    \\ \midrule
    \multirow{3}{*}{1}  & 0.1  & 86.51 & 97.89 \\
                        & 0.5  & 87.72 & 97.72  \\
                        & 1.0  & \textbf{88.54}  & \textbf{98.13}  \\ \midrule
    \multirow{3}{*}{5}  & 0.1  & 86.24  & 97.02 \\
                        & 0.5  & \textbf{87.60} & \textbf{97.56} \\
                    
                        & 1.0  & 86.41  & 96.84  \\ \midrule
    \multirow{3}{*}{10} & 0.1  & 85.88 & 96.05 \\
                        & 0.5  & 86.45 & 96.61 \\
                        & 1.0  & \textbf{87.52}  & \textbf{96.53}  \\ \bottomrule
    \end{tabular*}%

     \label{tab:abalations}
}
\hfill
\parbox{.46\linewidth}{
\centering
\caption{Comparison of one-stage vs two-stage training on TCGA-RCC subtyping. All comparisons used SC-MIL-RS training and $\tau=1$}

\begin{tabular*}{\linewidth}{@{\extracolsep{\stretch{1}}}@{}cccc@{}}
\toprule
Ratio               & Stage & F1    & AUC   \\ \midrule
\multirow{2}{*}{1}  & 1     & \textbf{88.54} & \textbf{98.13} \\
                    & 2     & 87.72 & 97.72 \\ \midrule
\multirow{2}{*}{5}  & 1     & \textbf{86.41} & 96.84 \\
                    & 2     & 85.84 & \textbf{97.24} \\ \midrule
\multirow{2}{*}{10} & 1     & \textbf{87.52} & \textbf{96.53} \\
                    & 2     & 85.26 & 96.38 \\ \bottomrule
\end{tabular*}%

\label{tab:training_stages}
}

\end{table}


\subsubsection{Two-stage vs single-stage training.}
We conducted an ablation by training models in a two-stage manner, with SCL loss in the first stage for feature learning followed by cross-entropy (CE) loss in the second stage. We see that single-stage SC-MIL model (joint SCL and CE training) performs better overall as shown in Table \ref{tab:training_stages}. This could be due to incompatible feature learning between SCL and CE stages in two-stage training. Using a smooth curriculum allows a gradual transition from feature learning to classifier learning, leading to superior performance.

\section{Conclusion} 
We propose SC-MIL, a simple technique to integrate supervised contrastive learning into the MIL framework. This model shows consistent performance improvements for the problem of label imbalance across both in and out-of-distribution pathology datasets. We also conduct detailed ablations for applying contrastive losses with MIL and offer insights on the model design, training scheme, and choice of hyperparameters. We hope that this improved generalization performance in imbalanced settings drives adoption of ML in real-world clinical applications. 


\bibliographystyle{splncs04}
\bibliography{ref}



\title{Supplementary Section for SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced Classification in Pathology}
\author{}
\institute{}
\titlerunning{Supervised Contrastive Multiple Instance Learning}
\maketitle

\begin{figure}[h!]
\centering
\includegraphics[scale=0.3]{id_and_ood.png}
\caption{Visual comparison of in-distribution (ID) and out-of-distribution (OOD) WSIs from the cancer subtyping datasets. The first and third rows show in-distribution TCGA WSIs from NSCLC and RCC respectively. The second and fourth rows show WSIs procured from a different lab site and scanner. We can see the variations in tissue preparation and scanning which lead to significant drops in performance.}
\label{id_ood_fig}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.67]{classwise_f1_scores_RCC_ratio_5_10.pdf}
\caption{Class-wise F1 score comparison for RCC subtyping: SC-MIL-RS outperforms other methods across different imbalance ratios. The performance gap increases with a higher imbalance ratio and on moving from ID to OOD datasets,  specifically on minority classes (KICH and KIRP).}
\label{classwise_f1_fig}
\end{figure}

\begin{table}
\centering
\caption{Augmentation policies across all training methods}
\begin{tabular}{ccc}
\hline
\textbf{Augmentation Class}              & \textbf{Augmentations} & \textbf{Range}    \\ \hline

\multirow{2}{*}{Blur and Sharpen Augmentations} & Gaussian Blur          & {[}0, 2{]}        \\
                                         & Gaussian Sharpen       & {[}0, 2{]}        \\ \hline
\multirow{4}{*}{Color Augmentations}     & Random Gray Scale             & -       \\
                                         & Brightness             & {[}0, 1{]} \\
                                         & Hue                    & {[}-0.5, 0.5{]}   \\
                                         & Saturation             & {[}0, 1{]} \\ \hline
\multirow{3}{*}{Geometry Augmentations}  & Vertical Flip          & -        \\
                                         & Horizontal Flip        & -        \\
                                         & Center Crop            & -        \\ \hline
\end{tabular}
\end{table}



\end{document}
