%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
% \documentclass[sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
\documentclass[default,iicol]{sn-jnl}% Default with double column layout
\usepackage{amsmath}
\usepackage{diagbox}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{threeparttable}
\usepackage{xspace}
\usepackage{siunitx}
\usepackage{subfigure}
%\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{threeparttable}
%\usepackage{mathtools}
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{float}
\usepackage{amsfonts}

\usepackage{bm}
\usepackage{array}
\PassOptionsToPackage{table}{xcolor} % https://tex.stackexchange.com/a/74922
\definecolor{mygray}{RGB}{200,200,200}
\definecolor{reda}{RGB}{255,0,0}
\definecolor{redb}{RGB}{217,148,143}
\definecolor{myyellow}{RGB}{190,144,0}
\definecolor{mygreen}{RGB}{0,136,51}
\definecolor{myblue}{RGB}{0,102,204}
\usepackage{colortbl}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
% \usepackage{diagbox}
% \usepackage{rotating}
% \usepackage{booktabs}

% \usepackage{overpic}
% \usepackage{enumitem}
% \usepackage{colortbl}
% \usepackage{algorithm}

% \usepackage{algorithmic}
% \usepackage{cite} % https://blog.csdn.net/hyluglare/article/details/111624138
\usepackage{pifont}
\newcommand{\HL}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
%\newcommand{\red}[1]{{\textcolor{black}{\textbf{#1}}}}
\newcommand{\red}[1]{{\textcolor{black}{{#1}}}}
\newcommand{\blue}[1]{{\textcolor{black}{#1}}}
\newcommand{\cyan}[1]{{\textcolor{black}{#1}}}
\newcommand{\yes}{\text{\ding{51}}}
\newcommand{\no}{\text{\ding{55}}}

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\makeatother

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}


\title[Article Title]{Towards Diverse Binary Segmentation via  A Simple yet General Gated Network}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%
\author[1]{\fnm{Xiaoqi} \sur{Zhao}}\email{zxq@mail.dlut.edu.cn}
\author[1]{\fnm{Youwei} \sur{Pang}}\email{lartpang@mail.dlut.edu.cn}
\author*[1]{\fnm{Lihe} \sur{Zhang}}\email{zhanglihe@dlut.edu.cn}
\author[1,2]{\fnm{Huchuan} \sur{Lu}}\email{lhchuan@dlut.edu.cn}
\author[3,4]{\fnm{Lei} \sur{Zhang}}\email{cslzhang@comp.polyu.edu.hk}


\affil*[1]{\orgdiv{Dalian University of Technology, China}}

\affil[2]{\orgdiv{Peng Cheng Laboratory, China}}

\affil[3]{\orgdiv{Dept. of Computing, The Hong Kong Polytechnic University, China}}
\affil[4]{\orgdiv{OPPO Research, China}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{In many binary segmentation tasks, most CNNs-based methods use a U-shape encoder-decoder network as their basic structure. They ignore two key problems when the encoder exchanges information with the decoder: one is the lack of interference control mechanism between them, the other is without considering the disparity of the contributions from different encoder levels. In this work, we propose a simple yet general gated network (GateNet) to tackle them all at once. With the help of multi-level gate units, the valuable context information from the encoder can be selectively transmitted to the decoder. In addition, we design a gated dual branch structure to build the cooperation among the features of different levels and improve the discrimination ability of the network. Furthermore, we introduce a “Fold” operation to improve the atrous convolution and form a novel folded atrous convolution, which can be flexibly embedded in ASPP or DenseASPP to accurately localize foreground objects of various scales. GateNet can be easily generalized to many binary segmentation tasks, including general and specific object segmentation and multi-modal segmentation. Without bells and whistles, our network consistently performs favorably against the state-of-the-art methods under $10$ metrics on $33$ datasets of $10$ binary segmentation tasks.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Binary Segmentation, Gated Network,  Gated Dual Branch, Folded Atrous Convolution.}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle
\section{Introduction}\label{sec:introduction}
Image segmentation
 is the process of dividing a digital image into segments that simplify and/or change the representation of the image to something more meaningful and easier to analyze.
 From the perspective of pixel-level classification, image segmentation can be specifically divided into binary segmentation, semantic segmentation, instance segmentation and panoramic segmentation. Compared with the others, segmentation problems considered in binary segmentation are more pure and focused, that is, accurately distinguishing the foreground and background. 
 As shown in Fig.~\ref{fig:diverse_bs}, binary segmentation has a wide range of applications in military, industrial, medical, etc. 
  %%%%%%%%%%%%%%%%%%% Figure 1 %%%%%%%%%%%%%%%%%%%%%%
  \begin{figure}[!htb]
  	\centering
  	\includegraphics[width=0.9\linewidth]{figs/sunburst.pdf}
  	\caption{Some meaningful binary segmentation tasks.
  	}
  	\label{fig:diverse_bs}
  \end{figure}
   %%%%%%%%%%%%%%%%%%% Figure 1 %%%%%%%%%%%%%%%%%%%%%%
   
Rich foreground definitions prompt binary segmentation with numerous branches, such as salient object detection, camouflaged object detection, shadow detection and transparent object detection. 
In recent years, with the development of deep learning, there are many effective methods proposed and achieve good performance.
Although each branch of binary segmentation  is thriving and show a gratifying state, almost all methods focus on researching single one branch and ignore cross-branch comparison in experiments and techniques. 
As we know, each branch belongs to the binary segmentation trunk because they have a same mathematical definition. They face many same challenges in segmentation techniques.
However, these task branches have become more and more independent, which will impede the development of the entire binary segmentation field.
To this end, it is urgent to provide a general method for diverse binary segmentation branches.

There are three challenges in accurate binary segmentation: 
\textbf{Firstly}, most methods~\cite{BASNet,MINet,DANet_RGBDSOD,SINet_COD,Amulet,DGRL,BMPM,CPD,DHS,DSS} tend to adopt U-shape~\cite{Unet,FPN} as the baseline and then combine multi-level features in either the encoder~\cite{Amulet,DGRL,BMPM,BASNet,CPD} or the decoder~\cite{DHS,DSS,PAGRN,CPD,UNet++} to gradually reconstruct the high-resolution feature maps. In each convolutional block, they separately formulate the relationships of internal features during forward update. It is well known that the high-quality segmentation predicted in the decoder relies heavily on the effective features provided by the encoder. 
Nevertheless, these methods directly use an all-pass skip-layer structure to concatenate the features of the encoder to the decoder in the isolated~
\cite{Unet,BMPM,SINet_COD,DANet_RGBDSOD,PraNet_Polyp,VST} or nested~\cite{U2Net,UNet++,MINet,CPFP_RGBDSOD,SINet_COD,MSNet_Polyp} manner. 
The effectiveness of feature aggregation at different levels is not quantified. This not only introduces misleading context information into the decoder but also causes that the typically useful features can not be adequately utilized. In cognitive science, Yang \textit{et al.}~\cite{NM} show that inhibitory neurons play an important role in how the human brain chooses to process the most important information from all the information presented to us. And inhibitory neurons ensure that humans respond appropriately to external stimuli by inhibiting other neurons and balancing excitatory neurons that stimulate neuronal activity. Inspired by this work, we think that it is necessary to set up an information screening unit between each pair of encoder and decoder blocks in binary prediction. It will help distinguish the most task-aware features of foreground regions and suppress background interference.
\textbf{Secondly}, due to the limited receptive field, a single-scale convolutional kernel is difficult to capture context information of size-varying objects. This motivates many efforts~\cite{R3Net,BMPM,UCNet_RGBDSOD,S2MA_RGBDSOD,DMRA_RGBDSOD,CoNet_RGBDSOD,BBSNet_RGBDSOD,JLDCF_RGBDSOD,PoolNet} to investigate multi-scale feature extraction. These methods directly equip an atrous spatial pyramid pooling module~\cite{ASPP} (ASPP) or DenseASPP~\cite{DenseASPP} in their networks. However, when using a convolution with a large dilation rate, the information under the kernel seriously lacks correlation due to inserting too many zeros. This may be detrimental to the discrimination of subtle image structures.
\textbf{Thirdly}, both body and boundary of the foreground need to accurately segmented. Most existing models either use progressive decoder~\cite{PraNet_Polyp,MSNet_Polyp,PDNet_Mirror,GDNet_Glass,ADNet_Shadow,DENets_DBD,SG_DBD,UGTR_COD} or parallel decoder~\cite{R3Net,PFA,SCRN,BANet,CSNet,CoNet_RGBDSOD,DeFusionNet_DBD,R2MRF_DBD}. The progressive structure begins with the top layer and gradually utilizes the output of the higher layer as prior knowledge to fuse the encoder features. This mechanism is not conducive to the recovery of details because the high-level features lack fine information. While the parallel structure easily results in inaccurate localization of objects since the low-level features without semantic information directly interfere with the capture of global structure cues. 

In this paper, we propose a simple yet general gated network (GateNet) for binary segmentation. Firstly, based on the feature pyramid network (FPN), we construct multi-level gate units to combine the features from the decoder and the encoder. We use convolution operation and nonlinear functions to calculate the correlations among features and assign gate values to different blocks. In this process, a partnership is established between different blocks by using weight distribution and the decoder can obtain more efficient information from the encoder and pay more attention to the target-aware regions. Secondly, we construct a folded atrous spatial pyramid pooling (Fold-ASPP) module to gather multi-scale high-level foreground cues. With the ``Fold'' operation, the atrous convolution is implemented on a group of local neighborhoods rather than a group of isolated sampling points, which can help generate more stable features and more adequately depict finer structure. Thirdly, we design a mix feature aggregation decoder that a parallel branch by concatenating the output of the progressive branch and the features of the gated encoder, so that the residual information complementary to the progressive branch is supplemented to generate the final prediction. 

Our main contributions can be summarized as follows.
\begin{itemize}
	\item We provide a unified perspective of binary segmentation by comprehensively analyzing many binary segmentation tasks.
	\item We propose a simple gated network to adaptively control the amount of information that flows into the decoder from each encoder block. With multi-level gate units, the network can balance the contribution of each encoder block to the the decoder block and suppress the features of background regions.
	\item We design a novel folded atrous convolution that can transfer existing multi-scale modules into our Fold style and enjoy more effective feature representation.
	\item We build a dual branch architecture. They form a residual structure, complement each other through the gated processing and generate better results. 
	\item We construct both single-stream and two-stream gated networks to adapt the binary segmentation required one or two input sources. 
	\item Extensive comparisons with $42$ state-of-the-art methods on $33$ challenging datasets of $10$ binary segmentation tasks, including RGB, RGB-D and optical remote sensing image salient object detection, camouflaged object detection, defocus blur detection, shadow detection, transparent detection, glass detection, mirror detection and polyp segmentation in medical images,  show that our method performs much better than other competitors under $10$ metrics and possess strong generalization. Hence, it can be seen a strong baseline for the binary segmentation field.
\end{itemize}

\textit{Compared with the ECCV version~\cite{GateNet} (Oral) of this work, the following extensions are made. 
\textbf{\uppercase\expandafter{\romannumeral1})} We conduct a survey on the field of binary segmentation, covering $10$ popular branches and $141$ fully supervised methods, evaluation metrics and datasets.
\textbf{\uppercase\expandafter{\romannumeral2})} Deeper theoretical explanations of the proposed gate unit design are added and we improve the previous gate unit into a stronger version.
\textbf{\uppercase\expandafter{\romannumeral3})} Based on the overall structure of the original single-source input GateNet, we expand a two-stream version of GateNet suitable for two-source input tasks. Meanwhile, our multi-level gate units can further carry forward the spirit of suppress and balance between different sources.
\textbf{\uppercase\expandafter{\romannumeral4})} We report much more extensive experimental results that demonstrate the superiority of both single-stream and dual-stream GateNet in $10$ popular binary segmentation tasks.
\textbf{\uppercase\expandafter{\romannumeral5})} We further provide more implementation details and thorough ablation studies at qualitative and quantitative aspects.
\textbf{\uppercase\expandafter{\romannumeral6})} We perform in-depth analyses and discussion for our gate unit.}


 \section{Retrospect}\label{sec:2}

 \subsection{Diverse Binary Segmentation Tasks (DBS)}\label{sec:2.1}
As shown in Fig.~\ref{fig:diverse_bs}, there are many kinds of binary segmentation  in real life. %Surveying all the branches and methods is almost impossible and unnecessary to accomplish.
We select $10$ currently well-developed and hot tasks that cover the requirements of general and specific object segmentation in natural images, remote sensing images, and medical images.
%as shown in Fig.~\ref{fig:binary_segmentation_10_tasks}.
According to the rapid development of deep learning technology, we only review the research progress in recent five years in order to provide the latest and comprehensive content.
\subsubsection{General Object Segmentation}
\noindent$\bullet$~\textbf{RGB Salient Object Detection.}
Salient object detection (SOD) aims to segment the most salient (judged by different consciousnesses) regions or objects in various scenes with or without the engineered cues, such as visual cues, geodesic cues, temporal cues, and human attention cues. Usually, it is adopted as a pre-processing step in many computer vision applications, such as scene classification~\cite{classification}, person re-identification~\cite{Reid} and image captioning~\cite{Imagecaption}. 

\noindent$\bullet$~\textbf{RGB-D Salient Object Detection.}
Although RGB SOD methods can achieve satisfactory performance in segmenting visually salient objects, some complex scenarios are still open to be resolved. For example, salient objects share similar appearance to the background or the other similar trivial objects. In recent years, various depth-assisted salient object detection (RGB-D SOD) methods~\cite{PCA_RGBDSOD,CPFP_RGBDSOD,DMRA_RGBDSOD} have been proposed, in which absorbing geodesic cues from the depth map is the hardcore.

\noindent$\bullet$~\textbf{Remote Sensing Image Salient Object Detection.}
Remote sensing images (RSIs) are usually captured by sensors on anairplane as an aerial view under various viewing angle conditions. 
Although recent decades have witnessed the remarkable success of SOD for natural scene images, there is only a limited amount of researches focusing on SOD for optical remote sensing images (RSIs). 
Typically, optical RSIs cover a wide scope with complicated background and diverse noise interference. 

\noindent$\bullet$~\textbf{Camouflaged Object Detection.}
The study of camouflage has a long history in biology, and more details can be found in~\cite{camouflage}. In the field of computer vision, research on camouflaged object detection (COD) is often associated with salient object detection task. In general, saliency models are designed for finding visually salient objects. They are not suitable for finding hidden objects. The local features of the camouflaged object are usually slightly different from the surrounding background. Recently, Fan \textit{et al.}~\cite{SINet_COD} make some attempts towards this direction. They first build the largest COD dataset, which contains $10,000$ images covering $78$ camouflaged object categories.

\subsubsection{Specific Object Segmentation}
\noindent$\bullet$~\textbf{Defocus Blur Detection.}
Defocus blur is a blurring degradation caused by defocusing and inappropriate depth of focus. 
Defocus blur is a common phenomenon in real life when the scene is beyond the focal distance of the camera.
Defocus blur detection can be potentially used to many vision tasks (\textit{e.g.}, autofocus, depth estimation).

\noindent$\bullet$~\textbf{Shadow Detection.}
Shadow is the light effect caused by surface occlusion and are almost ubiquitous in our daily lives. 
One one hand, shadow can be used as auxiliary information due to rich depth and geometry visual cues.
On the other hand, some important details of the object may be hidden when overlapping with shadows.
Hence, shadow detection is important for shadow removal~\cite{DAS_shadow_removal}, scene geometry~\cite{Shadow_scene_geometry} and camera parameters~\cite{camera_parameters_shadow}.

\noindent$\bullet$~\textbf{Glass and Transparent Detection.}
Transparent objects are widely present in the real world, such as glass, vitrines, and bottles. 
And most of them appear in indoor scenes, especially glass-like objects with brittle and smooth properties.
Smart robot operates tasks in living rooms or offices, it needs to avoid fragile objects.
Hence, it is essential for vision systems to be able to detect and segment transparent objects from input images.


\noindent$\bullet$~\textbf{Mirror Detection.}
As a very important object in daily life, mirrors are ubiquitous. They can not only reflect light, but also present a similar mirror image of surrounding objects or scenes. 
As a result, once the computer vision system or robot encounters a scene with a mirror, the performance will drop significantly. To avoid this problem, it requires these systems to be able to detect and segment mirrors.

\noindent$\bullet$~\textbf{Polyp Detection.}
According to GLOBOCAN 2020 data, colorectal cancer is the third most common cancer worldwide and the second most common cause of death. 
It usually begins as small, noncancerous (benign) clumps of cells called polyps that form on the inside of the colon.  
Over time some of these polyps can become colon cancers. 
Therefore, the best way of preventing colon cancer is to identify and remove polyps before they turn into cancer. 

% \subsection{Evaluation Metrics}\label{sec:2.2}

% \noindent\textbullet~\textbf{Pixel Accuracy ($PA$)} 
% is calculated based on {the binarized} prediction mask and ground-truth:
% \begin{equation}\label{equation:PA}\small
%   PA= \frac{TP+TN}{TP+TN+FP+FN},
% \end{equation}
% where TP, TN, FP, FN denote true-positive, true-negative, false-positive, and false-negative, respectively.

% \noindent\textbullet~\textbf{F-measure ($F_{\beta}$)~\cite{colorcontrast_Fm}}
% is a metric that comprehensively considers both precision and recall:
% \begin{equation}\label{equation:Fm}\small
%   F_{\beta}= \frac{(1+\beta^2)\text{Precision}\times \text{Recall}}{\beta^2 \text{Precision} + \text{Recall}},
% \end{equation}
% \begin{equation}\label{equation:pr}\small
%   \text{Precision}= \frac{{TP}}{{TP}+{FP}},~~~~\text{Recall}= \frac{{TP}}{{TP}+{FN}},
% \end{equation}
% where $\beta^2$ is set to $0.3$ as suggested in~\cite{colorcontrast_Fm} to emphasize the precision.
% Some methods report the maximum F-measure ($F_{\beta}^{max}$) across the binary maps of different thresholds or the mean F-measure ($F_{\beta}^{mean}$) score by an adaptive threshold.

% \noindent\textbullet~\textbf{weighted F-measure ($F_{\beta}^{\omega}$)~\cite{Fwb}} 
% is proposed to improve the metric F-measure. 
% It assigns different weights ($\omega$) to precision and recall across different errors at different locations, considering the neighborhood information: 
% \begin{equation}\label{equation:wFm}\small
%   F_\beta^\omega= \frac{(1+\beta^2)\text{Precision}^\omega\times \text{Recall}^\omega}{\beta^2 \text{Precision}^\omega + \text{Recall}^\omega}.
% \end{equation}

% \noindent\textbullet~\textbf{S-measure ($S_m$)~\cite{S-m}} 
% evaluates the spatial structure similarity by combining the region-aware structural similarity $S _ { r }$ and the object-aware structural similarity $S _ { o}$: 
% \begin{equation}\label{equation:Sm}\small
% S_m = \alpha \times S_{ o } + ( 1 - \alpha ) \times S_{ r },
% \end{equation}
% where $\alpha$ is empirically set to $0.5$.

% \noindent\textbullet~\textbf{E-measure ($E_m$)~\cite{E-m}}
% can jointly capture image level statistics and local pixel matching information:
% \begin{equation}\label{equation:Em}\small
%   Q_{\bm{S}}= \frac{1}{W\!\times\!H} \sum\nolimits_{i=1}^{W} \sum\nolimits_{j=1}^{H} \phi_{\bm{S}}(i,j),
% \end{equation}
% where $\phi_{\bm{S}}$ is the enhanced alignment matrix, reflecting the correlation between prediction $\bm{S}$ and the ground truth $\bm{G}$ after subtracting their global means, respectively.

% \noindent\textbullet~\textbf{IOU}
% is the most common metric for evaluating classification accuracy:
% %
% \begin{equation}\label{equation:IOU}\small
% IOU  = \frac{TP}{TP+FP+FN}.
% \end{equation}

% \noindent\textbullet~\textbf{Dice}
%  is a statistic used to gauge the similarity of two samples and become the most used metric in validating medical image segmentation:
% %
% \begin{equation}\label{equation:Dice}\small
% Dice  = \frac{2TP}{FP+2TP+FN}.
% \end{equation}

% \noindent\textbullet~\textbf{Balanced error rate ($BER$)~\cite{BER}}
%  is a common metric to evaluate shadow detection performance, where shadow and non-shadow regions contribute equally to the overall performance without considering their relative areas:
% %
% \begin{equation}\label{equation:BER}\small
% BER \ = \ (1-\frac{1}{2}(\frac{TP}{TP+FN}+\frac{TN}{TN+FP})).
% \end{equation}
% %

% \noindent\textbullet~\textbf{MAE ($\mathcal{M}$)~\cite{MAE}} 
% measures the average absolute difference between the prediction $\bm{S}\!\in\![0,1]^{W\!\times\! H}$ and the  ground truth $\bm{G}\!\in\!\{0,1\}^{W\!\times\! H}$ pixel by pixel:
% \begin{equation}\label{equation:mae}\small
%   \text{MAE}= \frac{1}{W\!\times\!H} \sum\nolimits_{i=1}^{W} \sum\nolimits_{j=1}^{H} \lvert \bm{G}(i,j)-\bm{S}(i,j) \rvert.
% \end{equation}

% The lower value is better for the BER and MAE, and higher is better for others.
% F-measure, weighted F-measure, S-measure, E-measure and MAE are widely used in salient object detection, camouflaged object detection and defous blur detection tasks.
% IOU and Dice are popular with medical image segmentation. 
% BER and PA are more commonly used for shadow, mirror, glass and transparent detection. 
% In fact, all above metrics can be used for any binary segmentation sub-task. In this paper, we are the first to introduce all ten metrics into the quantitative comparison to provide a comprehensive performance evaluation.
% \subsection{Popular Datasets}\label{sec:2.3}
% The construction of datasets is an important factor that drives a thriving field. 
% Tab.~\ref{table:datasets_survey} summarizes $35$ binary segmentation datasets, which are highly representative and widely used for training or testing. Scale of a single dataset, number of available datasets for a single task and corresponding built times usually effect the development of a domain. 
% We can see that RGB salient object detection (SOD) has all three advantages at the same time: large scale datasets, multiple datasets and early establishment, which help it to become the most well developed among these $10$ binary segmentation branches. 
% Both RGB-D SOD and polyp segmentation fields have multiple datasets for research, which provides important help for the development of the community. Limited by the need for depth devices in RGB-D SOD and the reliance on medical expertise in polyp segmentation annotation, existing datasets are still small-scale and can not reflect the effectiveness of each method very accurately. 
% Transparent, glass, mirror and defocus blur detection only have one or two datasets, future works are necessary to build more datasets to verify the generalization ability of each model.  
% With the establishment of ORSI-4199~\cite{MJRBM_RSISOD}, more and more works on ORSI SOD will emerge. 
%  COD has developed rapidly in recent years. COD10K~\cite{SINet_COD} provides a high-quality and large-scale benchmark for the camouflaged object detection (COD) task. And then, NC4K~\cite{Rank-Net_COD} plays a supplement role in enlarging test set scale.
%%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
	\centering
	\caption{% Summary of popular DBS methods.
		{\textbf{Summary of essential characteristics for reviewed fully-supervised binary segmentation methods.}  
			The superscript \textbf{``$*$''} in the fifth column (code link) regards this repository does not provide pre-trained weights for re-evaluating performance publicly and \textbf{``N/A''} represents that the code is not available. 
			\textbf{STL} is single task learning 
			and \textbf{MTL} is multi-task learning.}
	}
	\vspace{-5pt}
	\label{table:Survey_DBS_methods_1}
	\begin{threeparttable}
		\resizebox{1\textwidth}{!}{
		%	\setlength\tabcolsep{1pt}
		%	\renewcommand\arraystretch{1}
			
			 \renewcommand\tabcolsep{1.0pt} % 调整表格列间的宽度
			\renewcommand\arraystretch{1.}
			
			\input{tables/DBS_methods_suervey}
			% \begin{tabular}{|c|c|r||c|c|c|c|c|c|c|c|c|}  % {lccc}

		}
	\end{threeparttable}
	%\vspace{-8pt}
\end{table*}

\begin{table*}
	\centering
	
	\vspace{-5pt}
	\label{table:Survey_DBS_methods_2}
	\begin{threeparttable}
		\resizebox{1\textwidth}{!}{
			%	\setlength\tabcolsep{1pt}
			%	\renewcommand\arraystretch{1}
			
			\renewcommand\tabcolsep{1.0pt} % 调整表格列间的宽度
			\renewcommand\arraystretch{1.}
			
			\input{tables/DBS_methods_suervey_2}
			% \begin{tabular}{|c|c|r||c|c|c|c|c|c|c|c|c|}  % {lccc}
			
		}
	\end{threeparttable}
	%\vspace{-8pt}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%


\begin{table*}
	\centering
	\caption{% Summary of popular DBS methods.
		{\textbf{Summary of essential characteristics for reviewed fully-supervised binary segmentation methods.}  
			The superscript \textbf{``$*$''} in the fifth column (code link) regards this repository does not provide pre-trained weights for re-evaluating performance publicly and \textbf{``N/A''} represents that the code is not available. 
		}
	}
	\vspace{-5pt}
	\label{table:Survey_DBS_methods_3}
	\begin{threeparttable}
		\resizebox{1\textwidth}{!}{
			%	\setlength\tabcolsep{1pt}
			%	\renewcommand\arraystretch{1}
			
			\renewcommand\tabcolsep{1.0pt} % 调整表格列间的宽度
			\renewcommand\arraystretch{1.}
			
			\input{tables/DBS_methods_suervey_3}
			% \begin{tabular}{|c|c|r||c|c|c|c|c|c|c|c|c|}  % {lccc}
			
		}
	\end{threeparttable}
	%\vspace{-8pt}
\end{table*}

\begin{table*}
	\centering
	
	\vspace{-5pt}
	\label{table:Survey_DBS_methods_4}
	\begin{threeparttable}
		\resizebox{1\textwidth}{!}{
			%	\setlength\tabcolsep{1pt}
			%	\renewcommand\arraystretch{1}
			
			\renewcommand\tabcolsep{1.0pt} % 调整表格列间的宽度
			\renewcommand\arraystretch{1.}
			
			\input{tables/DBS_methods_suervey_4}
			% \begin{tabular}{|c|c|r||c|c|c|c|c|c|c|c|c|}  % {lccc}
			
		}
	\end{threeparttable}
	%\vspace{-8pt}
\end{table*}

\subsection{Fully Supervised Binary Segmentation Models}\label{sec:2.4}
%Before reviewing recent fully supervised binary segmentation models in details, 
We first formulate the image-based binary segmentation problem. 
Formally, let \bm{$\mathcal{X}$} and \bm{$\mathcal{Y}$} denote the input space and output segmentation space, respectively. 
Fully supervised learning-based models generally seek to learn an \textit{ideal} image-to-segment mapping $f^{*\!}:\bm{\mathcal{X}}\mapsto\bm{\mathcal{Y}}$ through directly utilizing ground truth masks as supervision signal. 

In Tab.~\ref{table:Survey_DBS_methods_1} and Tab.~\ref{table:Survey_DBS_methods_3}, we categorize recent fully supervised  models.
Through the analyses of $141$ methods in $10$ branches, we summarize some instructive findings:
% \textbf{\uppercase\expandafter{\romannumeral1})} The open source situation of code is indeed an important factor in promoting the development of a field. Intuitively, RGB and RGB-D SOD are the two branches with the largest number of papers published in top conferences/journals in recent years, and their publicly accessible codes reach the scale $34/40$ and $36/42$, respectively.
\textbf{\uppercase\expandafter{\romannumeral1})} Single task learning (STL) is still the main learning paradigm in binary segmentation. Compared with STL, the proportion of MTL-based methods is only $34/141$ and they finish MTL via cooperating with boundary prediction or depth estimation usually. It is worth noting that MTL-based RGB SOD methods have reached the $6/9$ scale in 2021. We believe that the potential of MTL is huge, and more effective and richer strategies will emerge in the future under the continuous efforts of researchers.
\textbf{\uppercase\expandafter{\romannumeral2})} Our GateNet is the only one mixes both progress and parallel structures among $141$ methods, thereby enjoying the advantages of both. 
Most methods still adopt the  single progressive style.
\textbf{\uppercase\expandafter{\romannumeral3})} Conditional random field (CRF) gradually disappear in many models. Only some shadow and  mirror detection methods adopt the CRF as post-processing.
\textbf{\uppercase\expandafter{\romannumeral4})} Deep supervision becomes a popular supervision approach. 
$73/141$ methods build the network with side outs to perform deep supervision. On one hand, deep supervision~\cite{Deepsupervision} is originally designed to speed up network convergence. On the other hand, it may bring extra performance gain for most models.  
\textbf{\uppercase\expandafter{\romannumeral5})} Targeted loss function is conducive to performance improvement. $72/141$ models directly adopt previous or re-design a new targeted loss, such as the hybrid loss~\cite{BASNet}, consistency-enhanced loss~\cite{MINet}, pixel position aware loss~\cite{F3Net}, etc. It is clear that there is increasing competition in the research of targeted loss.
\subsection{Multi-scale Feature Extraction}\label{sec:2.5}
The multi-scale paradigm is mainly inspired by the scale-space theory that has been widely validated as an effective and theoretically sound framework. 
It is well suited for addressing naturally occurring scale variations.
Common forms in the field of computer vision mainly include the image pyramid~\cite{ImagePyramid} and the feature pyramid~\cite{Unet,FPN}.
Although the former has shown good performance, its application is limited by high computational and latency costs associated with the multi-input parallel processing paradigm, which makes it gradually give way to the more efficient latter in the era of deep learning.
The feature pyramid can be roughly divided into two categories according to the form, namely, the inter-layer pyramid and the intra-layer pyramid.
The former is based on features with different scales extracted by the feature encoder, such as the U-shape~\cite{Unet,FPN,U2Net,LargeKernel,PAGRN,MINet} architecture. In this way, the internal cross-layer information propagation path progressively integrates semantic context and texture details from diverse scale representations.
The intra-layer pyramid~\cite{PPM,ASPP,DenseASPP,BMPM,DANet_RGBDSOD} can enhance the diversity of semantic content by constructing the multi-path structure within a layer to obtain a rich combination of receptive fields.
Its good pluggability has also made it an important component in the architecture design of modern segmentation methods.
%
Recently, the atrous spatial pyramid pooling module (ASPP)~\cite{ASPP} and its variants~\cite{DenseASPP,DilatedConvolution-SDC,DilatedConvolution-ESDC,DilatedConvolution-C3Convolution,DilatedConvolution-ESPNet,DilatedConvolution-TKCN,DilatedConvolution-D3DNet,DANet_RGBDSOD}, which typify this structure, are widely applied in many segmentation tasks and networks. 
Some methods~\cite{BMPM,UCNet_RGBDSOD,PFNet_COD,BDRAR_Shadow,AFFPN_Shadow,RCARP_Glass} insert several ASPP modules into the encoder/decoder blocks of different levels, while some ones~\cite{R3Net,AFNet,DMRA_RGBDSOD,CoNet_RGBDSOD,Rank-Net_COD,EBLNet_Glass} install it on the highest-level encoder block.
As a basic component of ASPP, atrous convolution has the advantage of enlarging the receptive field to obtain large-scale features without increasing the computational cost compared to the vanilla convolution.
Nevertheless, the repeated stride and pooling operations already make the top-layer features lose much fine information. 
With the increase of atrous rate, the correlation of sampling points further degrades, which leads to difficulties in capturing the changes of image details (\textit{e.g.}, lathy background regions between adjacent objects or spindly parts of objects). 
In this work, we  propose a folded atrous convolution to alleviate these issues and achieve a \textsl{local-in-local} effect. 
The folded atrous convolution can seamlessly replace the original atrous convolution in ASPP and other variants (\textit{e.g.}, DenseASPP~\cite{DenseASPP}, PAFE~\cite{DANet_RGBDSOD}), thus significantly improving performance.


\subsection{Gated Mechanisms}
\label{sec:2.6}
The gated mechanism plays an important role in controlling the flow of information and is widely used in the long short term memory (LSTM).
% gate unit
In~\cite{gatedlabel}, the gate unit combines two consecutive feature maps of different resolutions from the encoder to generate rich contextual information.
And the gated mechanism is also integrated into the block feedback mechanism to bridge multiple iterations in the recurrent architecture~\cite{Gate-RIGNet}.
Zhang \textit{et al.}~\cite{BMPM} adopt gate function to control the message passing when combining feature maps at all levels of the encoder.
Chen \textit{et al.}~\cite{DPANet_RGBDSOD}  propose a gate function controller to focus on regulating the fusion rate of the cross-modal information.
Zhang \textit{et al.}~\cite{FRDT_RGBDSOD} utilize the gated select fusion module to selectively process the useful information from two modal features in low-levels.
% attention
Due to the ability to filter information, the gated mechanism can also be seen as a special kind of attention mechanism.
Wang \textit{et al.}~\cite{PASE} exploit the pyramid attention module to enhance saliency representations for each layer in the encoder and enlarge the receptive field.
Chen \textit{et al.}~\cite{GCPANet} propose a head attention  module  to  reduce  information  redundancy and enhance the top layers features by leveraging both spatial and channel-wise attention.
Zhang \textit{et al.}~\cite{PAGRN} apply both spatial and channel attention to each layer of the decoder.
Liu \textit{et al.}~\cite{S2MA_RGBDSOD} construct both self-attention and mutual-attention in a non-local~\cite{Nonlocal} style for extracting the complementary information between the different modalities. 
Zhu \textit{et al.}~\cite{BDRAR_Shadow} design the recurrent attention residual module to combine and process spatial contexts in two adjacent CNN layers.
Zhang \textit{et al.}~\cite{ACSNet_Polyp} apply both the local context attention and SE-like~\cite{SENet} channel-wise attention for context selection.
Taehun \textit{et al.}~\cite{UACANet_Polyp} propose the uncertainty augmented context attention module to  incorporate uncertain area for rich semantic feature extraction.
More description about attention-based methods can found in Tab.~\ref{table:Survey_DBS_methods_3}.
% ours
In general, the above methods unilaterally consider the information interaction between different layers or intra-layer in the encoder or decoder.
We integrate the features from the encoder and the decoder to formulate gate function, which has the function of \textbf{\textsl{block-wise attention}} and models the overall distribution of all blocks in the network from the global perspective. 
However, while previous methods utilize the block-specific features to compute dense attention weights for the corresponding block, they directly feed the encoder features into the decoder and do not consider their mutual interference. 
Our proposed gate unit can naturally balance their contributions, thereby suppressing the response of the encoder to background regions. 
Experimental results in Fig.~\ref{Fig:gatevalue} and Fig.~\ref{fig:Gate_suppress_visual_results} intuitively demonstrate the effect of multi-level gate units on the above two aspects, respectively.

 %%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
	\includegraphics[width=\linewidth]{figs/GateNetv2.pdf}\\ %,height=0.3\linewidth
	      \vspace{-6mm}
	\centering
	\caption{Overall architecture of the gated network. It consists of   five encoder blocks ($\mathbf{E}^1 \sim \mathbf{E}^5$), five transition layers ($\mathbf{T}^1 \sim \mathbf{T}^5$), five gate units ($\mathbf{G}^1 \sim \mathbf{G}^5$), five decoder blocks ($\mathbf{D}^1 \sim \mathbf{D}^5$) and the {Fold-ASPP} module. We employ twice supervision in this network. One acts at the end of the FPN branch ${D}^1$. The other is used to guide the fusion of the two branches.} 		
	\label{fig:GateNet}
	%   \vspace{-1mm}
\end{figure*} 
 

\section{Proposed Method}
\label{sec:proposedmethod}
The gated network architecture is shown in Fig.~\ref{fig:GateNet}, in which encoder blocks, transition layers, decoder blocks and  gate units are denoted as $\mathbf{E}^i$, $\mathbf{T}^i$, $\mathbf{D}^i$ and $\mathbf{G}^i$, respectively ($i \in \left \{1, 2, 3, 4, 5 \right \}$ indexes different levels). Their output feature maps are denoted as $E^i$, $T^i$, $D^i$ and $G^i$, respectively. 
The final prediction is obtained by combining the FPN branch and the parallel branch. 
%In this section, we first describe the overall architecture, and then detail the gated dual branch structure and the folded atrous spatial pyramid pooling module.
\subsection{Network Overview}\label{sec:Network}
\noindent\textbf{Encoder Network.} In our model, the encoder is based on a common pretrained backbone network, \textit{e.g.}, the VGG~\cite{VGG}, ResNet~\cite{Resnet} or ResNeXt~\cite{ResNeXt}. We take the VGG-16 network as an example, which contains thirteen Conv layers, five max-pooling layers and two fully connected layers. In order to fit saliency detection task, similar to most previous approaches~\cite{Amulet,DSS,PAGRN,BMPM}, we cast away all the fully-connected layers of the VGG-16 and remove the last pooling layer to retain details of last convolutional layer. 

\noindent\textbf{Decoder Network.} The decoder comprises three main components: i) the FPN branch, which continually fuses different level features from ${T}^1 \sim{T}^5$ by element-wise addition; ii) the parallel branch, which combines the saliency map of the FPN branch and the feature maps of transition layers by cross-channel concatenation (At the same time, multi-level gate units ($\mathbf{G}^1 \sim \mathbf{G}^5$) are inserted between the transition layer and the decoder layer); iii) the Fold-ASPP module, which improves the original atrous spatial pyramid pooling (ASPP) by using a ``Fold'' operation. It can take advantage of semantic features learned from ${E}^5$ to provide multi-scale information to the decoder.
 %%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%
\subsection{Gated Dual Branch}\label{sec:Gated Dual Branch}
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/GateUnit_v2.pdf}
	\caption{Detailed illustration of the gate unit. ${D}^{i+1}$ indicates feature maps of the previous decoder block. \textcircled{\scriptsize S} is sigmoid function.}\label{fig:Gate_Unit}
	\label{fig:GateUnit_v2}
	%\vspace{-5.5mm}
\end{figure}
 %%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/gateunit_global.pdf}
	\caption{Architecture comparison between the Gated FPN with gate units-v1 and gate units-v2.}
	\label{fig:GateUnit_v1_v2_comparison}
	%\vspace{-5.5mm}
\end{figure}
 %%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%
The gate unit can control the message passing between scale-matching encoder and decoder blocks. By combining the feature maps of the previous decoder block, the gate value also characterizes the contribution that the current block of the encoder can provide. Fig.~\ref{fig:Gate_Unit} shows the internal structure of the proposed gate unit. In particular, the aggregated encoder feature ${E}$ and decoder feature ${D}^{i+1}$ are integrated to obtain feature ${F}^i$, and then the output is fed into two branches, which includes a series of convolution, activation and pooling operations, to compute a pair of gate values ${G}^i$. The entire gated process can be formulated as,
\begin{equation}\label{equ:3}
E = Conv(Cat(E^1,E^2,E^3,E^4,E^5)),
\end{equation}
\begin{equation}\label{equ:gate_value}
{G}^i
= \left\{\begin{matrix}
P(S(Conv(Cat(E, D^{i+1})))) & \text{ if } i=1, 2, 3, 4\\
P(S(Conv(Cat(E, T^i)))), & \text{ if } i=5
\end{matrix}\right.
\end{equation}
where $Cat(\cdot)$ is the concatenation operation among channel axis, $Conv(\cdot)$ is the convolution layer, $S(\cdot)$ is the element-wise sigmoid function, and $P(\cdot)$ is the global average pooling. The output channel of $Conv(\cdot)$ in Eq.~\ref{equ:gate_value} is 2. The resulted gate vector ${G}^i$ has two different elements which correspond to two gate values in Fig.~\ref{fig:GateUnit_v2}. Given the gate values, we can apply them to the FPN branch and the parallel branch to weight the transition-layer features ${T}^1 \sim {T}^5$, which are generated by exploiting $3 \times 3$ convolution to reduce the dimension of ${E}^1 \sim {E}^4$ and the Fold-ASPP to finely process ${E}^5$ (See Fig.~\ref{fig:GateNet} for details). Through multi-level gate units, we can suppress and balance the information flowing from different encoder blocks to the decoder. 

Compared to the ECCV version~\cite{GateNet} of Gate Units-v1, we modify the input feature maps of the current encoder block  ${E}^{i}$  to the all-level aggregated feature maps ${E}$. As shown in Fig.~\ref{fig:GateUnit_v1_v2_comparison}, the Gated FPN with Gate Units-v2 enjoys bidirectional soft supervision, which motivates the gating values of each layer to consider their corresponding contributions from a global perspective, rather than the local perspective in Gate Unit-v1. In this way, the cooperation among the various layers is closer, thereby, making the optimization of the network more efficient.

% In Fig.~\ref{fig:Gate_value}, we statistically demonstrate the curves of gate value with a convolutional level as the horizontal axis. 
% It can be seen that the high-level encoder features contribute more contextual guidance to the decoder than the low-level encoder features in the FPN branch. This trend is just the opposite in the parallel branch. It is because the FPN branch is responsible for predicting the main body of the salient object by progressively combining multi-level features, which needs more high-level semantic knowledge, while the parallel branch, as a residual structure, aims to fill in the details, which are mainly contained in the low-level features. In addition, some visual examples are shown in Fig.~\ref{fig:Gate_suppress} demonstrate that multi-level gate units can significantly suppress the interference from each encoder block and enhance the contrast between salient and non-salient regions. Since the proposed gate unit is simple yet effective, a raw FPN network with multi-level gate units can be viewed as a new baseline for binary segmentation task.
 %%%%%%%%%%%%%%%%%%% Figure 6 %%%%%%%%%%%%%%%%%%%%%%
 \begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/Dual_branch.pdf}\\
	%\vspace{-3mm}
	\caption{ Illustration of different decoder architectures. (a) Progressive structure. (b) Parallel structure. (c) Dual branch structure.}
	\label{fig:Dual_branch}
	%\vspace{-5.5mm}
\end{figure} 
 %%%%%%%%%%%%%%%%%%% Figure 6 %%%%%%%%%%%%%%%%%%%%%%
Generally, binary segmentation methods usually adopt the progressive or parallel structure as decoder architecture, as shown in Fig.~\ref{fig:Dual_branch}(a, b). Progressive style is more conducive to the localization of the objects through the high-level feature guidance, while the parallel style is easier to restore details by making full use of low-level features.
As can be seen from Tab.~\ref{table:Survey_DBS_methods_3}, previous methods only adopt either progressive or parallel mode and ignore the advantages brought by the other. 
% Most existing models either use progressive decoder~\cite{BMPM,DGRL,PAGRN,PASE} or parallel decoder~\cite{R3Net,PFA}, as shown in Fig.~\ref{fig:Dual_branch}. The progressive structure begins with the top layer and gradually utilizes the output of the higher layer as prior knowledge to fuse the encoder features. This mechanism is not conducive to the recovery of details because the high-level features lack fine information. While the parallel structure easily results in inaccurate localization of objects since the low-level features without semantic information directly interfere with the capture of global structure cues. 
In this work, we mix the two structures to build a dual branch decoder to overcome the above restrictions.
We briefly describe the FPN branch. Taking $D^i$ as an example, we firstly apply bilinear interpolation to upsample the higher-level feature $D^{i+1}$ to the same size as ${T}^{i}$. Next, to decrease the number of parameters, ${T}^{i}$ is reduced to $32$ channels and fed into gate unit $G^{i}$. Lastly, the gated feature is fused with the upsampled feature of $D^{i+1}$ by element-wise addition and convolutional layers. This process can be formulated as:
\begin{equation}\label{equ:fpn}
D^i = \left\{\begin{matrix}
Conv(G_1^i \cdot T^i + Up(D^{i+1})) & \text{ if } i=1, 2, 3, 4\\
Conv(G_1^i \cdot T^i), & \text{ if } i=5,
\end{matrix}\right.
\end{equation}
where $D^{1}$ is a single-channel feature map with the same size as the input image. 

In the parallel branch, we firstly upsample ${T}^1 \sim {T}^5$ to the same size of $D^1$. Next, the multi-level gate units are followed to weight the corresponding transition-layer features. Lastly, we combine $D^1$ and the gated features by cross-channel concatenation. The whole process is written as:
\begin{equation}\label{equ:1}
\begin{split}
F_{Cat} = Cat(&D^1, Up(G_2^1 \cdot T^1), Up(G_2^2 \cdot T^2),\\
&  Up(G_2^3 \cdot T^3), Up(G_2^4 \cdot T^4), Up(G_2^5 \cdot T^5)) .
\end{split} 
\end{equation}

The final saliency map $S^F$ is generated by integrating the predictions of the two branches with a residual connection as shown in Fig.~\ref{fig:Dual_branch}(c),
\begin{equation}\label{equ:2}
\begin{split}
S^F = S(Conv(F_{Cat}) + D^1)),
\end{split}
\end{equation}
where $S(\cdot)$ is the element-wise sigmoid function.
 %%%%%%%%%%%%%%%%%%% Figure 7 %%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/FoldConv.pdf}\\
	%\vspace{-3mm}
	\caption{ Illustration of the folded atrous convolution. We use \textcircled{\scriptsize 1}, \textcircled{\scriptsize 2} and \textcircled{\scriptsize 3} to respectively indicate ``Fold'' operation, atrous convolution and ``Unfold'' operation. \textcircled{\scriptsize 4} shows the comparison between atrous convolution (Left) and the folded atrous convolution (Right).}
	\label{fig:FoldConv}
	%\vspace{-4mm}
\end{figure}
 %%%%%%%%%%%%%%%%%%% Figure 7 %%%%%%%%%%%%%%%%%%%%%%
 
  %%%%%%%%%%%%%%%%%%% Figure 8 %%%%%%%%%%%%%%%%%%%%%%
 \begin{figure*}[!t]
	\centering
	%\vspace{-3mm}   
	\includegraphics[width=\linewidth]{figs/Twostream_GateNet.pdf}\\
	%\vspace{-3mm}
	\caption{Overall pipeline of the two-stream gated network. Firstly, we use two independent encoders to extract features for each modality separately. 
	%And then, we perform cross-modal fusion for each pair of features at the same level.
	Fold-ASPP is followed and embedded in the top layer of the encoder. And then, we utilize multi-level gate units to control both cross-modal fusion and fused information transmitted to the decoder. The final prediction is yielded by the gated dual branch.}
	\label{fig:Twostream_GateNet}
	%\vspace{-5mm}
\end{figure*}
 %%%%%%%%%%%%%%%%%%% Figure 8 %%%%%%%%%%%%%%%%%%%%%%
 
\subsection{Folded Atrous Convolution}\label{sec:Fold}
In order to obtain robust segmentation results by integrating multi-scale information, atrous spatial pyramid pooling (ASPP) is proposed in Deeplab~\cite{ASPP}. And many works~\cite{BMPM,UCNet_RGBDSOD,PFNet_COD,BDRAR_Shadow,AFFPN_Shadow,RCARP_Glass,DMRA_RGBDSOD,CoNet_RGBDSOD,Rank-Net_COD,EBLNet_Glass} also show its effectiveness in different binary segmentation branches. The ASPP uses multiple  parallel  atrous  convolutional layers  with  different  dilation  rates. 
The sparsity of atrous convolution kernel, especially when using a large dilation rate, results in that the association relationships among sampling points are too weak to extract stable features. 
In this paper, we apply a simple ``Fold'' operation to effectively relieve this issue. We visualize the folded atrous convolution structure in Fig.~\ref{fig:FoldConv}, which not only further enlarges the receptive field but also extends each valid sampling position from an isolate point to a $2\times2$ connected region.

Let $\mathbf{X}$ represent feature maps with the size of $N \times N \times C$ (C is the channel number). We slide a $2 \times 2$ window on $\mathbf{X}$ in stride $2$ and then conduct atrous convolution with kernel size  $K \times K$ in different dilation rates. Fig.~\ref{fig:FoldConv} shows the computational process when $K = 3$ and dilation rate is $2$. Firstly, we collect  $2 \times 2 \times C$ feature points in each window from  $\mathbf{X}$ and then it is stacked by channel direction, we call this operation ``Fold'', which is shown in Fig.~\ref{fig:FoldConv}\textcircled{\scriptsize 1}. After the fold operation, we can get new feature maps with the size of $N/2 \times N/2 \times 4C$. A point on the new feature maps corresponds to a $2 \times 2$ area on the original feature maps. Secondly, we adopt an atrous convolution with a kernel size of $3 \times 3$ and dilation rate is $2$. Followed by the reverse process of ``Fold'' which is  called ``Unfold'' operation, the final feature maps are obtained. By using the folded atrous convolution, in the process of information transfer across convolution layers, more contexts are merged and the certain local correlation is also preserved, which provides the fault-tolerance capability for subsequent operations. 

The Fold-ASPP is only equipped on the top of the encoder network, which consists of three folded atrous convolutional layers with  dilation rates $[2, 4, 6]$ to fit the size of feature maps. Just as group convolution~\cite{ResNeXt} is a trade-off between depthwise convolution~\cite{Xception,Mobilenet} and vanilla convolution in the channel dimension, the proposed folded atrous convolution is a trade-off between atrous convolution and vanilla convolution in the spatial dimension. 


 
\subsection{Supervision}\label{sec:supervision}
We use the pixel position aware loss $L_{ppa}$~\cite{F3Net} which have been widely adopted in segmentation tasks. 
We use the same definitions as in~\cite{SPNet_RGBDSOD,F3Net,PraNet_Polyp,MSNet_Polyp,UCNet_RGBDSOD}.  As shown in Fig.~\ref{fig:GateNet}, we apply supervision for both the intermediate prediction from the FPN branch and the final prediction from the dual branch. In the dual branch decoder, since the FPN branch gradually combines all-level gated encoding and decoding features, it has very powerful prediction ability. We expect that it can predict salient objects as accurately as possible under the supervision of ground truth. While the parallel branch only combines the gated encoding features, which is helpful to remedy the ignored details with the design of residual structure. Moreover, the supervision on $D^{1}$ can drive gate units to learn the weight of the contribution of each encoder block to the final prediction. The total loss \emph{L} could be written as:
\begin{equation}\label{equ:5}
\begin{split}
L = L_{ppa}^{s1}+L_{ppa}^{sf},
\end{split}
\end{equation}
where $L_{ppa}^{s1}$ and $L_{ppa}^{sf}$ are respectively used to regularize the output of the FPN branch and the final prediction. 

\subsection{Two-Stream Network}\label{sec:Two_Stream_Network}
To finish some two-source input tasks, \textit{e.g.}, RGB-D salient object detection, we extend the GateNet to a two-stream architecture  to further demonstrate its effectiveness. 
The proposed two-stream GateNet is shown in Fig.~\ref{fig:Twostream_GateNet}. 
Compared with the single-stream network for single source input tasks, there are two main differences: (1) We add an extra encoder to extract features of other modals. (2) We convey the output features from the encoding blocks of two modalities to the gate unit to achieve cross-modal fusion at each level. 
The motivation of embedding gate units when performing cross-modal fusion is straightforward, that is, different modalities present different characteristics in each layer of the encoder, and low-quality modal features can interfere with the other one, leading to build a poor decoder. The form and composition of all gate units are the same as Fig.~\ref{fig:GateUnit_v2} and Eq.~\ref{equ:gate_value} except that the input features are different.

 %%%%%%%%%%%%%%%%%%% table 1 %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
	\centering
	\caption{
		{Summary of essential characteristics about popular binary segmentation datasets.}
	}
	\label{table:datasets_survey}
	\begin{threeparttable}
		\resizebox{1\textwidth}{!}{
			\setlength\tabcolsep{6pt}
			\renewcommand\arraystretch{1.02}
            \input{tables/Datasets_survey}
		}
	\end{threeparttable}
	%\vspace{-8pt}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\subsection{Datasets}
For the training and test dataset, we follow the settings of the most state-of-the-art methods~\cite{VST,TriTransNet_RGBDSOD,UACANet_Polyp,UGTR_COD,DENets_DBD,DSDNet_Shadow,TransLab_Transparent,GDNet_Glass,MirrorNet_Mirror,MJRBM_RSISOD} in Tab.~\ref{table:Survey_DBS_methods_1} on each binary segmentation task. 
And the details about these datasets can find in Tab.~\ref{table:datasets_survey}. 
\subsection{Evaluation Metrics}
There are ten popular metrics used in different binary segmentation branches. 
F-measure~\cite{colorcontrast_Fm} ($F_{\beta}^{max}$, $F_{\beta}^{mean}$), weighted F-measure~\cite{Fwb} ($F_{\beta}^{\omega}$), S-measure~\cite{S-m} ($S_m$), E-measure~\cite{E-m} ($E_m$) and MAE~\cite{MAE} ($\mathcal{M}$) are widely used in salient object detection, camouflaged object detection and defous blur detection tasks.
IOU and Dice scores are popular with medical image segmentation. 
BER~\cite{BER} and Pixel Accuracy (PA) are more commonly used for shadow, mirror, glass and transparent detection. 
 The lower value is better for the BER and MAE, and higher is better for others.
 
\noindent\textbullet~\textbf{Pixel Accuracy ($PA$)} 
 is calculated based on {the binarized} prediction mask and ground-truth:
 \begin{equation}\label{equation:PA}\small
   PA= \frac{TP+TN}{TP+TN+FP+FN},
 \end{equation}
 where TP, TN, FP, FN denote true-positive, true-negative, false-positive, and false-negative, respectively.

 \noindent\textbullet~\textbf{F-measure ($F_{\beta}$)~\cite{colorcontrast_Fm}}
 is a metric that comprehensively considers both precision and recall:
 \begin{equation}\label{equation:Fm}\small
   F_{\beta}= \frac{(1+\beta^2)\text{Precision}\times \text{Recall}}{\beta^2 \text{Precision} + \text{Recall}},
 \end{equation}
 \begin{equation}\label{equation:pr}\small
   \text{Precision}= \frac{{TP}}{{TP}+{FP}},~~~~\text{Recall}= \frac{{TP}}{{TP}+{FN}},
 \end{equation}
 where $\beta^2$ is set to $0.3$ as suggested in~\cite{colorcontrast_Fm} to emphasize the precision.
 Some methods report the maximum F-measure ($F_{\beta}^{max}$) across the binary maps of different thresholds or the mean F-measure ($F_{\beta}^{mean}$) score by an adaptive threshold.

 \noindent\textbullet~\textbf{weighted F-measure ($F_{\beta}^{\omega}$)~\cite{Fwb}} 
 is proposed to improve the metric F-measure. 
 It assigns different weights ($\omega$) to precision and recall across different errors at different locations, considering the neighborhood information: 
 \begin{equation}\label{equation:wFm}\small
   F_\beta^\omega= \frac{(1+\beta^2)\text{Precision}^\omega\times \text{Recall}^\omega}{\beta^2 \text{Precision}^\omega + \text{Recall}^\omega}.
 \end{equation}

 \noindent\textbullet~\textbf{S-measure ($S_m$)~\cite{S-m}} 
 evaluates the spatial structure similarity by combining the region-aware structural similarity $S _ { r }$ and the object-aware structural similarity $S _ { o}$: 
 \begin{equation}\label{equation:Sm}\small
 S_m = \alpha \times S_{ o } + ( 1 - \alpha ) \times S_{ r },
 \end{equation}
 where $\alpha$ is empirically set to $0.5$.

 \noindent\textbullet~\textbf{E-measure ($E_m$)~\cite{E-m}}
 can jointly capture image level statistics and local pixel matching information:
 \begin{equation}\label{equation:Em}\small
   Q_{\bm{S}}= \frac{1}{W\!\times\!H} \sum\nolimits_{i=1}^{W} \sum\nolimits_{j=1}^{H} \phi_{\bm{S}}(i,j),
 \end{equation}
 where $\phi_{\bm{S}}$ is the enhanced alignment matrix, reflecting the correlation between prediction $\bm{S}$ and the ground truth $\bm{G}$ after subtracting their global means, respectively.

 \noindent\textbullet~\textbf{IOU}
 is the most common metric for evaluating classification accuracy:
 %
 \begin{equation}\label{equation:IOU}\small
 IOU  = \frac{TP}{TP+FP+FN}.
 \end{equation}

 \noindent\textbullet~\textbf{Dice}
  is a statistic used to gauge the similarity of two samples and become the most used metric in validating medical image segmentation:
 %
 \begin{equation}\label{equation:Dice}\small
 Dice  = \frac{2TP}{FP+2TP+FN}.
 \end{equation}

 \noindent\textbullet~\textbf{Balanced error rate ($BER$)~\cite{BER}}
  is a common metric to evaluate shadow detection performance, where shadow and non-shadow regions contribute equally to the overall performance without considering their relative areas:
 %
 \begin{equation}\label{equation:BER}\small
 BER \ = \ (1-\frac{1}{2}(\frac{TP}{TP+FN}+\frac{TN}{TN+FP})).
 \end{equation}
 %

 \noindent\textbullet~\textbf{MAE ($\mathcal{M}$)~\cite{MAE}} 
 measures the average absolute difference between the prediction $\bm{S}\!\in\![0,1]^{W\!\times\! H}$ and the  ground truth $\bm{G}\!\in\!\{0,1\}^{W\!\times\! H}$ pixel by pixel:
 \begin{equation}\label{equation:mae}\small
   \text{MAE}= \frac{1}{W\!\times\!H} \sum\nolimits_{i=1}^{W} \sum\nolimits_{j=1}^{H} \lvert \bm{G}(i,j)-\bm{S}(i,j) \rvert.
 \end{equation}

In fact, all above metrics can be used for any binary segmentation sub-task. In this paper, we are the first to introduce all ten metrics into the quantitative comparison to provide a comprehensive performance evaluation.
\subsection{Implementation Details}
We use the PyTorch framework to implement our models on  one RTX 3090 GPU for $100$ epochs.
The input resolutions of images are resized to $352\times352$ and we employ a  general multi-scale training strategy as most methods~\cite{F3Net,GCPANet,Rank-Net_COD,SPNet_RGBDSOD,PraNet_Polyp,MSNet_Polyp}.
We adopt some image augmentation techniques to avoid overfitting, including random flipping,  rotating, and border clipping. For the optimizer, we use the Adam~\cite{Adam}. For the learning rate, initial learning rate  is  set  to $0.0001$. We adopt the ``step'' learning rate decay policy, and set the decay size as $30$ and decay rate as $0.9$.
For any sub-tasks, the above training strategy is used for all the gated network models involved in this paper.
The difference among these models is only in the mini-batch size due to adopting different backbones.
Specifically, the mini-batch size settings in the gatenet using VGG-16, ResNet-50, Res2Net-50, and ResNeXt-101 as the backbone are $8$, $24$, $24$ and $16$, respectively. 
The source code can be available at \url{https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency}.
\subsection{Performance}
We compare our models with state-of-the-art approaches in terms of ten metrics on all test sets corresponding for each binary segmentation task in Tab.~\ref{tab:RGBSOD_performance} - Tab.~\ref{tab:ORSISOD_performance}.
Since there are many test sets for RGB SOD, RGB-D SOD and polyp segmentation, we not only compare the performance under each metric, but also count the proportion of top $3$ and top $1$ performance to get an overall evaluation. Some quantitative analyses are as follows:
 %%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!t]
\large
\centering
	\scriptsize
	\setlength{\abovecaptionskip}{2pt}
	\caption{Quantitative comparison of different RGB SOD methods.  Top $3$ and Top $1$ scores are highlighted in {\color{myblue}{\textbf{blue}}} and {\color{reda}{\textbf{red}}}, respectively.}
 \begin{threeparttable}
   \resizebox{\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{0.9}
    \input{tables/RGBSOD_performance}
	}
	\setlength{\abovecaptionskip}{2pt}
	%	\vspace{-5mm}
	\label{tab:RGBSOD_performance}
	\end{threeparttable}
\end{table}
 %%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%% Table 3 %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!t]
\large
	\scriptsize
	\centering
	\setlength{\abovecaptionskip}{2pt}
	\caption{Quantitative comparison of different polyp segmentation methods. Top $3$ and Top $1$ scores are highlighted in {\color{myblue}{\textbf{blue}}} and {\color{reda}{\textbf{red}}}, respectively.}
 \begin{threeparttable}
   \resizebox{\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{0.9}
\input{tables/Polyp_performance}
	}
	\setlength{\abovecaptionskip}{2pt}
	%	\vspace{-5mm}
	\label{tab:Polyp_performance}
	\end{threeparttable}
\end{table}
 %%%%%%%%%%%%%%%%%%% Table 3 %%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%% Table 4 %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!t]
% \large
	\small
\centering
	\setlength{\abovecaptionskip}{2pt}
	\caption{Quantitative comparison of different RGB-D SOD methods. Top $3$ and Top $1$ scores are highlighted in {\color{myblue}{\textbf{blue}}} and {\color{reda}{\textbf{red}}}, respectively.}
 \begin{threeparttable}
   \resizebox{0.98\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1}
    \input{tables/RGBDSOD_performance}
	}
	\setlength{\abovecaptionskip}{2pt}
	%	\vspace{-5mm}
	\label{tab:RGBDSOD_performance}
	\end{threeparttable}
\end{table}
 %%%%%%%%%%%%%%%%%%% Table 4 %%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%% Table 5 %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htbp]
  \centering
  \caption{Quantitative comparison of different camouflaged object methods. The best scores are highlighted in {\color{reda}{\textbf{red}}}.}
  \begin{threeparttable}
   \resizebox{0.85\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1.05}
     \input{tables/COD_performance}
}
\end{threeparttable}
\label{tab:COD_performance}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 5 %%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%% Table 6 %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htbp]
  \centering
  \caption{Quantitative comparison of different defocus blur detction methods. The best scores are highlighted in {\color{reda}{\textbf{red}}}.}
  \begin{threeparttable}
   \resizebox{0.85\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1.05}
 \input{tables/DBD_performance}
   }
  \end{threeparttable}
 	\label{tab:DBD_performance}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 6 %%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%% Table 7 %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htbp]
  \centering
  \caption{Quantitative comparison of different shadow detection methods. The best scores are highlighted in {\color{reda}{\textbf{red}}}.}
  \begin{threeparttable}
   \resizebox{0.85\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1.05}
   \input{tables/Shadow_performance}
   }
  \end{threeparttable}
 	\label{tab:Shadow_performance}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 7 %%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%% Table 8 %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htbp]
  \centering
  \caption{Quantitative comparison of different transparent, glass and mirror detection methods. The best scores are highlighted in {\color{reda}{\textbf{red}}}.}
  \begin{threeparttable}
   \resizebox{0.85\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1.05}
   \input{tables/Glass_like_performance}
   }
  \end{threeparttable}
	\label{tab:Glass_like_performance}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 8 %%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%% Table 9 %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htbp]
  \centering
  \caption{Quantitative comparison of different ORSI SOD methods. The best scores are highlighted in {\color{reda}{\textbf{red}}}.}
  \begin{threeparttable}
   \resizebox{\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1}
    \input{tables/ORSI_SOD_performance}
   }
  \end{threeparttable}
	\label{tab:ORSISOD_performance}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 9 %%%%%%%%%%%%%%%%%%%%%%

  %%%%%%%%%%%%%%%%%%% Table 10 %%%%%%%%%%%%%%%%%%%%%%   
\begin{table*}
% \large
\centering
	\scriptsize
	\setlength{\abovecaptionskip}{2pt}
	\caption{Efficiency comparisons of the top-performing methods in Tab.~\ref{tab:RGBSOD_performance} - Tab.~\ref{tab:ORSISOD_performance}. The best and worst results are shown in {\color{reda}\textbf{red}} and {\color{myblue}\textbf{blue}}, respectively.}
 \begin{threeparttable}
   \resizebox{\linewidth}{!}{
    \setlength\tabcolsep{2pt}
    \renewcommand\arraystretch{1}
    \input{tables/model_size_params_flops_fps.tex}
	}
	\setlength{\abovecaptionskip}{2pt}
	%	\vspace{-5mm}
	\label{tab:modelsize_para_flops}
	\end{threeparttable}
\end{table*}
 %%%%%%%%%%%%%%%%%%% Table 10 %%%%%%%%%%%%%%%%%%%%%%
  
  
 %%%%%%%%%%%%%%%%%%% Table 11 %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htbp]
  \centering
  \caption{Ablation  experiments  for seven binary segmentation tasks. 
  M1: FPN Baseline. 
  M2: + Residual Parallel Branch. 
  M3: + Gate Units v1.
  M4: + Gate Units v2.
  M5: + Fold-ASPP.}
  \begin{threeparttable}
   \resizebox{\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1.05}
    \input{tables/ablation_rgb_task.tex}
   }
  \end{threeparttable}
	\label{tab:Ablation_study_RGBSOD}
\end{table}
 %%%%%%%%%%%%%%%%%%% Table 11 %%%%%%%%%%%%%%%%%%%%%%
 
 %%%%%%%%%%%%%%%%%%% Table 12 %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htbp]
  \centering
  \caption{Ablation  experiments  for RGB-D salient object detection. 
  M1: FPN Baseline. 
  M2: + Residual Parallel Branch. 
  M3: + Cross-modal Gate Units.
  M4: + Encoder-Decoder Gate Units. 
  M5: + Fold-ASPP.}
  \begin{threeparttable}
   \resizebox{\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1.05}
    \input{tables/ablation_rgbd_task.tex}
   }
  \end{threeparttable}
  \label{tab:Ablation_study_RGBDSOD}
\end{table}
 %%%%%%%%%%%%%%%%%%% Table 12 %%%%%%%%%%%%%%%%%%%%%%
 
 %%%%%%%%%%%%%%%%%%% Table 13 %%%%%%%%%%%%%%%%%%%%%%
\begin{table}
	\small
\centering
	\setlength{\abovecaptionskip}{2pt}
	
	\caption{Evaluation of the folded atrous convolution. (x) stands for different sampling rates of atrous convolution. D-ASPP is DenseASPP~\cite{DenseASPP}.}
 \begin{threeparttable}
   \resizebox{\linewidth}{!}{
    \setlength\tabcolsep{5pt}
    \renewcommand\arraystretch{1}
		\input{tables/ablation_fold_aspp}
	}
	\setlength{\abovecaptionskip}{2pt}
	%	\vspace{-5mm}
	\label{tab:Ablation_study_Fold}
	\end{threeparttable}
\end{table}
 %%%%%%%%%%%%%%%%%%% Table 13 %%%%%%%%%%%%%%%%%%%%%%
\noindent$\bullet$ In Tab.~\ref{tab:RGBSOD_performance}, among $50$ scores of all \textit{\textbf{RGB SOD}} datasets, our GateNet achieves significant performance improvement compared to the second best method CTDNet~\cite{CTDNet} in terms of top 3 ($49/50$ vs. $37/50$) and top 1 ($40/50$ vs. $1/50$), respectively.
And, we still consistently outperform the VST~\cite{T2T} model even if it is equipped with a stronger transformer backbone T2T~\cite{T2T}. 
\\
\noindent$\bullet$ Tab.~\ref{tab:Polyp_performance} shows performance comparisons on five \textit{\textbf{polyp segmentation}} datasets. 
Our GateNet  consistently outperforms the second best approach UACANet~\cite{UACANet_Polyp} under top 3 ($49/50$ vs. $39/50$) and top 1 ($30/50$ vs. $13/50$), respectively. 
In particular, GateNet achieves a predominant performance on the CVC-ClinicDB~\cite{CVC-ClinicDB} in terms of all ten metrics.
\\
\noindent$\bullet$ For fair comparison with other \textit{\textbf{RGB-D SOD}} methods, we show the performance of GateNet with ResNet-50 and Res2Net-50 as backbone. In Tab.~\ref{tab:RGBDSOD_performance}, we can see that GateNet-Res-50 and GateNet-Res2-50 achieve the top 1 performance of $43/80$ and $32/80$  while the TriTransNet~\cite{TriTransNet_RGBDSOD} and SPNet~\cite{SPNet_RGBDSOD} only reach $11/80$ and $10/70$, respectively. Further, the comparison of GateNet-Res-50 + GateNet-Res2-50 and the TriTransNet +  SPNet is $62/80$ vs. $19/80$. 
\\
\noindent$\bullet$ Tab.~\ref{tab:COD_performance} - Tab.~\ref{tab:ORSISOD_performance} show performance comparisons with \textit{\textbf{camouflaged, defcus blur, shadow, transparent, glass, mirror and ORSI SOD}} methods, respectively. Without too much claim, our models achieve the best performance in terms of all ten metrics across $16$ out of  $17$ different datasets.
\\
\noindent$\bullet$. Tab.~\ref{tab:modelsize_para_flops} lists the \textit{\textbf{model sizes, parameters, FLOPs and speed}} of different methods with superior performance in Tab.~\ref{tab:RGBSOD_performance} - Tab.~\ref{tab:ORSISOD_performance} in detail. It can be seen that both two-stream and single stream GateNets still have obvious advantages against most state-of-the-art methods with different backbones. 
  %%%%%%%%%%%%%%%%%%% Figure 9 %%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
	\subfigure[\scriptsize{RGB Salient Object Detection}]{
		\includegraphics[width=6.5cm]{figs/rgb_sod_gatevalue.pdf}
		%\caption{fig1}
	}	
%	\quad
	\subfigure[\scriptsize{RGB-D Salient Object Detection}]{
		\includegraphics[width=6.5cm]{figs/rgbd_sod_gatevalue.pdf}
	}	
%	\quad
	\subfigure[\scriptsize{ORSI Salient Object Detection}]{
		\includegraphics[width=6.5cm]{figs/rgb_orsi_gatevalue.pdf}
	}	
%	\quad
	\subfigure[\scriptsize{Camouflaged Object Detection}]{
		\includegraphics[width=6.5cm]{figs/rgb_cod_gatevalue.pdf}
	}	
%	\quad	
	\subfigure[\scriptsize{Defocus Blur Detection}]{
		\includegraphics[width=6.5cm]{figs/rgb_dbd_gatevalue.pdf}
		%\caption{fig1}
	}	
%	\quad
	\subfigure[\scriptsize{Shadow Detection}]{
		\includegraphics[width=6.5cm]{figs/rgb_shadow_gatevalue.pdf}
	}	
	%	\quad
	\subfigure[\scriptsize{Transparent, Glass, Mirror Object Detection}]{
		\includegraphics[width=6.5cm]{figs/rgb_glass_like_gatevalue.pdf}
	}	
	%	\quad
	\subfigure[\scriptsize{Polyp Segmentation}]{
		\includegraphics[width=6.5cm]{figs/rgb_polyp_gatevalue.pdf}
	}
	\centering
	\caption{\small Distributions of gate weights separately presented in the FPN and parallel branch on $35$ datasets of $10$ tasks.  }
	\label{Fig:gatevalue}	
\end{figure*}
 %%%%%%%%%%%%%%%%%%% Figure 9 %%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Studies}
To reflect the general contribution of each component to the overall performance, we conduct ablation studies on the largest dataset for each sub-task individually. Tab.~\ref{tab:Ablation_study_RGBSOD} and Tab.~\ref{tab:Ablation_study_RGBDSOD} are the results for single-input tasks and the two-input task (RGB-D SOD), respectively. Tab.~\ref{tab:Ablation_study_Fold} verifies the effect of folded atrous convolution thoroughly.  
\\
\noindent\textbullet~\textbf{{Dual Branch Decoder.}} 
The baseline (M$1$) is a FPN structure with a progressive decoder. 
We add the residual  parallel branch to construct the dual branch decoder. 
We can see that M$2$ consistently outperforms M$1$ across all datasets in terms of all ten metrics.
Meanwhile, M$2$ has been able to surpass SINet~\cite{SINet_COD}, PFNet~\cite{PFNet_COD}, IS2CNet~\cite{IS2CNet_DBD} and BDRAR~\cite{BDRAR_Shadow}.
Based on this strong dual branch network, the subsequent performance gain of gate units and fold atrous convolution is more convincing.
\\
\noindent\textbullet~\textbf{{Gate Units.}} 
We embed multi-level gate units in both the FPN and parallel branches. 
In Tab.~\ref{tab:Ablation_study_RGBSOD}, the M$3$ achieves a significant improvement compared to the M$4$ indicates the necessity of designing gate units-v2 with a global information perspective. 
Further, the performance gap between M$2$ and M$4$ shows that the dual branch gated network obtains a considerable performance gain. 
In Tab.~\ref{tab:Ablation_study_RGBDSOD}, M$3$ vs. M$2$ and  M$4$ vs. M$3$ demonstrate the effectiveness of gate units in cross-modal fusion and encoder-decoder feature transition, respectively.
In addition, the curves of gate value on each dataset in ten tasks as shown in Fig.~\ref{Fig:gatevalue}. Through the characteristics of the gated pattern, we reveal some insightful findings:
\textbf{\uppercase\expandafter{\romannumeral1})}  
For the distribution of gate values at all levels in the FPN branch, Fig.~\ref{Fig:gatevalue}(a), (b), (e), (f), (g) present G1 and G2 are smaller than G3, G4, G5, while G1 in Fig.~\ref{Fig:gatevalue}(c), (d), (h) has the opposite trend. 
Analyzed from the visual perception, camouflaged objects, orsi object and polyps are easy to be confused with the background. The boundary information is very important to distinguish the fore/background, which drives the network to pay more attention on low-level features.
\textbf{\uppercase\expandafter{\romannumeral2})}  
For the distribution of gate values at all levels in the parallel branch, the greater contribution of G1 and G2 in Fig.~\ref{Fig:gatevalue}(d), (h) compared to the other tasks further illustrates the importance of details information in camouflaged and polyp segmentation tasks.
\textbf{\uppercase\expandafter{\romannumeral3})}
As shown in Fig.~\ref{Fig:gatevalue}(e), G4 and G5 have high values in both FPN branch and parallel branch, indicating that the accurate localization of focused regions is extremely crucial for defocus blur detection and motivate the network to consistently maintain a high pass-through pattern for high-level features.
\textbf{\uppercase\expandafter{\romannumeral4})}
Compared to other gate values in the FPN branch, G4 is the largest one and even exceeds above $0.9$ in almost all tasks. 
This phenomenon is also in line with our general understanding for deep networks that level-4 not only has stable semantic features but also has the larger spatial resolution than level-5 to construct the body of foreground effectively.
\textbf{\uppercase\expandafter{\romannumeral5})}
 Distributions of gate weights can effectively depict the similarities and differences among diverse binary segmentation sub-tasks.
 %%%%%%%%%%%%%%%%%%% Figure 10 %%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=\linewidth]{figs/Gate_suppress.pdf}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
	\end{center}
	\centering
	      % \vspace{-6mm}
	\caption{\small Visual comparison of feature maps for showing the effect of the multi-level gate units. D5 $\sim$ D1 represent the feature maps of each decoder block from high level to low level. Odd rows and even rows are the results of the FPN baseline without or with multi-level gate units, respectively.}\label{fig:Gate_suppress_visual_results}
\end{figure}
 %%%%%%%%%%%%%%%%%%% Figure 10 %%%%%%%%%%%%%%%%%%%%%%
   %%%%%%%%%%%%%%%%%%% Figure 11 %%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=\linewidth]{figs/RGBD_SOD_Gate_feature_visualization.pdf}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
	\end{center}
	\centering
	      % \vspace{-3mm}
	\caption{\small Visual results of feature maps. Each RGB-D input image corresponds to four rows of feature maps. The first two rows are RGB and Depth encoder feature maps (E5 - E1), respectively. The third row is the cross-modal fusion feature maps (C5 - C1). The last row is the decoder feature maps (D5 - D1). The naming of these feature maps is consistent with those in Fig.~\ref{fig:Twostream_GateNet}. }\label{fig:Gate_suppress_visual_results_rgbd}
\end{figure}

 To show the effect of the gate units more intuitively, we visualize the features of different levels in Fig.~\ref{fig:Gate_suppress_visual_results}. It can be observed that even if the dog has a very low contrast with the chair or the billboard (see the $1^{st}$ $\sim$ $4^{th}$ rows), through using multi-level gate units, the high contrast between the object region and the background is always maintained at each layer while the detailed information is continually regained, thereby making salient objects be effectively distinguished. 
 And, the gate units can avoid excessive suppression of the slender parts of objects (see the $5^{th}$ $\sim$ $8^{th}$ rows). The corners of the poster, the limbs and even tentacles of the mantis are retained well. 
 Besides, we show the visual results of the gate units in the two-stream network for RGB-D SOD, as shown in Fig.~\ref{fig:Gate_suppress_visual_results_rgbd}. 
 Intuitively, the depth branch has more significant and pure position and edge information about the foreground (cloth) than the RGB branch on E4, E2 and E1, thus distributes larger gate weights correspondingly in cross-modal fusion. 
\\
\noindent\textbullet~\textbf{{Folded Atrous Convolution.}} Based on the gated dual branch network, we design a series of experimental options to verify the effectiveness of the folded atrous convolution. Tab.~\ref{tab:Ablation_study_Fold} illustrates the results in detail. We adopt the atrous convolution with  dilation rates of $[2, 4, 6]$ and the same dilation rates are also applied to the folded atrous convolution. It can be observed that the folded atrous convolution consistently yields significant performance improvement at each dilation rate than the corresponding atrous convolution in terms of all ten metrics. 
And the single-layer Fold(6) already performs better than the ASPP and DenseASPP of aggregating three atrous convolution layers. The Fold-ASPP and Fold-DenseASPP  naturally outperforms the ASPP and DenseASPP, respectively. 
Our fold operation can naturally increase the receptive field. For a fair comparison, we can also see that compared with Atrous(4) with the same receptive field, Fold(2) still has an advantage under all metrics.


Fig.\ref{fig:ablation_study_visual_results} shows visual results of the above  ablation  studies  on some examples. It can be seen that the gated FPN model accurately determines where is the foreground object. 
%
With the help of Fold-ASPP, the overall integrity of the object is further captured. 
%
It should also be noted that the gated parallel branch can improve perceptual results greatly by highlighting the fore/back-ground difference and preserving the intra-class consistency, thereby yielding the sharpened boundary.


   %%%%%%%%%%%%%%%%%%% Figure 12 %%%%%%%%%%%%%%%%%%%%%%
 \begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{figs/ablation_study_gatepami.pdf}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
	\end{center}
	\centering
	      % \vspace{-6mm}
	\caption{Illustration of the benefit of each component. %Qualitative results of ablation studies on some examples.
	}\label{fig:ablation_study_visual_results}
\end{figure}
  %%%%%%%%%%%%%%%%%%% Figure 12 %%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%% Figure 13 %%%%%%%%%%%%%%%%%%%%%%
 \begin{figure}[!t]
	\centering
	%\vspace{-3mm}   
	\includegraphics[width=\linewidth]{figs/potential_applications.pdf}\\
	% \vspace{-3mm}
	\caption{Some examples of surface defect detection (\textit{e.g.}, crack, magnetic tiles,  car parts, electronic components).  }
	\label{fig:potential_applications}
	%\vspace{-5mm}
\end{figure}
 %%%%%%%%%%%%%%%%%%% Figure 13 %%%%%%%%%%%%%%%%%%%%%%
 
\section{Discussion}\label{sec:Discussion}

In this section, we further discuss the multi-level gated mechanism and give some potential applications: %
\\
\noindent$\bullet$ Our gate unit can suppress both each channel and spatial pixel of feature maps. In this way, the network actually learns adaptive thresholding. The area of which feature values are below this threshold has a lower response in the prediction, while the feature values above this threshold correspond to the task-specific activation area. It helps the decoder to gradually filter out the region with strong feature response. This kind of region can usually attract our attention. If we remove the gate units, the network will not have such strong constraints. Therefore, the gated mechanism can also be regarded as an implicit supervision.
\\
\noindent$\bullet$ The proposed gated mechanism has its limitations. It is unsuitable for semantic segmentation task. Because this task needs to treat all pixels of the whole image equally importantly and all categories have the same importance, the information suppression design is out of place. 
This also means that the GateNet has wider applicability to binary segmentation problems. 
\\
\noindent$\bullet$ In this paper, we have given detailed experimental analyses in ten popular binary segmentation tasks. Besides, GateNet has potential application in the field of industry with complex scenes as shown in Fig~\ref{fig:potential_applications}. We hope that this study can provide deep insights into the underlying design for more binary segmentation tasks and spark novel ideas.   

\section{Conclusions}
    As far as we know, this is the first work to comprehensively review recent progress in binary segmentation, which summarizes more than $140$ fully supervised models according to task settings, technique contributions, and learning strategies.
    %covering ten sub-branches: salient object detection (RGB, RGB-D and ORSI), camouflaged object detection, defocus blur detection, shadow detection, transparent object detection, glass detection, mirror detection and poly segmentation (medical image).
    %
    %We provide the reader with the necessary background knowledge including $10$ widely used metrics and $35$ popular datasets and then summarize more than $140$ fully supervised models according to task settings, technique contributions, and learning strategies. Based on the survey, we further give some instructive findings which are expected to be more or less helpful to researchers.
    %
    To unify all the sub-branches and establish a fair model benchmark to promote the prosperous development of the binary segmentation field, we propose a novel yet general gated network architecture. 
	%
	We first adopt multi-level gate units to balance the contribution of each encoder block and suppress the activation of the features of non-task-aware regions, which can provide useful context information for the decoder while minimizing interference. 
	We quantitatively reveal the role played by features at all levels of the encoder for different segmentation tasks, which provides a new perspective on the interpretability of deep learning.
	%
	Next, we use the Fold-ASPP to gather multi-scale semantic information for the decoder. By the folded operation, the atrous convolution achieves a local-in-local effect, which not only expands the receptive field  but also retains the correlation among local sampling points.
	%
	Finally, to further supplement the details, we combine all encoder features in parallel and construct a residual structure. 
	%
	Experimental results on $33$ benchmark datasets towards $10$ binary segmentation tasks demonstrate that the proposed model outperforms $42$ state-of-the-art methods under $10$ evaluation metrics.

{
\small
\bibliographystyle{plain}
\bibliography{sn-bibliography}% common bib file
}
%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. please ignore this.             %%
%%===================================================%%


\begin{appendices}
\section{Qualitative Evaluation}\label{secA1}
Fig.~\ref{fig:RGB_SOD} - Fig.~\ref{fig:polyp} illustrate some visual comparisons on each sub-task. We summarize the advantages of the GateNet compared to others when facing some challenges:
%
\textbf{\uppercase\expandafter{\romannumeral1})} 
% 伪装
\textbf{Interference produced by complex.}
In camouflaged object detection and poly segmentation tasks,  foreground  objects usually share the similar appearance to the  background, which can easily deceive predictors.  But the GateNet can accurately capture the hidden objects and separate them from the surrounding environment (see the Fig.~\ref{fig:cod} and Fig.~\ref{fig:polyp}). The gated mechanism also plays an important role in RGB -D salient object detection. As shown in Fig.~\ref{fig:rgbd_sod}, the proposed two-stream GateNet can effectively utilize the guidance information provided by the high-quality depth map while suppressing the interference information from the low-quality depth map, thereby identifying the whole object precisely. 
\textbf{\uppercase\expandafter{\romannumeral2})} 
\textbf{Interference produced by adjacent objects.} In the real world, shadows often exist on the ground or desktop, and are closely adjacent to the original object. This characteristic requires shadow detection networks to have the ability to distinguish between adjacent objects. As shown in Fig.~\ref{fig:shadow}, most methods are disturbed by the surface or the original object, but our method can focus on the shadow regions.
\textbf{\uppercase\expandafter{\romannumeral3})}
\textbf{The foreground exists multiple or small objects.} On the one hand, glass-like objects are often present in groups in the real world, which poses a serious challenge to the perception capability of the network for the multiple objects. On the other hand, small objects usually appear in remote sensing images. Benefiting from the Fold-ASPP, both multiple and small objects can be localized accurately. Fig.~\ref{fig:glass} and Fig.~\ref{fig:transparent} show that our method can accurately distinguish each independent connected region without sticking to each other. GateNet is the only one can provide clean prediction maps and maintain the basic shape of the aircraft (see the $6^{th}$ - $8^{th}$ columns in Fig.~\ref{fig:eorssd}).
\textbf{\uppercase\expandafter{\romannumeral4})}
\textbf{Boundary and details.} Our GateNet has a mix feature aggregation decoder that a parallel branch by concatenating the output of the progressive branch and the features of the gated encoder, so that the residual information complementary to the progressive branch is supplemented to generate the final prediction. In this way,  the prediction can restore more details, therefore, the limbs and even tentacles of the insects are retained well (see the $3^{th}$ and $8^{th}$ columns in Fig.~\ref{fig:cod}).  
\textbf{\uppercase\expandafter{\romannumeral5})}
\textbf{Regional consistency.} In defocus blur detection task, the focused area usually has incomplete semantic information because the blurred region may also belong to the semantic part of the foreground.  Benefiting from the folded operation, our model can obtain more stable structural features to improve the intra-class consistency. 
From the results in Fig.~\ref{fig:DBD}, it can be observed that our method can segment the foreground well while the other methods more or less lose similar areas inside or around focused regions. 


 

  \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=0.98\linewidth]{figs/RGB_SOD.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (CTDNet~\cite{CTDNet}, VST~\cite{VST}, LDF~\cite{LDF}, Auto-MSF~\cite{Auto-MSFNet}, KRN~\cite{KRN}, MINet~\cite{MINet}, ITSD~\cite{ITSD}, F3Net~\cite{F3Net}) on \textbf{RGB SOD} datasets. }
%   	\vspace{-8pt}
  	\label{fig:RGB_SOD}
  \end{figure*}
  \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=0.98\linewidth]{figs/eorssd.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (RRNet~\cite{RRNet_RSISOD}, MJRBM~\cite{MJRBM_RSISOD}, DAFNet~\cite{DAFNet_RSISOD}) on \textbf{ORSI SOD} datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:eorssd}
  \end{figure*}
  
    \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=\linewidth]{figs/rgbd_sod.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (TriTransNet~\cite{TriTransNet_RGBDSOD}, SPNet~\cite{SPNet_RGBDSOD}, DSNet~\cite{DSNet_RGBDSOD}, UTA~\cite{UTA_RGBDSOD}, RD3D~\cite{RD3D_RGBDSOD}, DCF~\cite{DCF_RGBDSOD}) on \textbf{RGB-D SOD} datasets.
  	}
%   	\vspace{-8pt}
  	\label{fig:rgbd_sod}
  \end{figure*}
  
        \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=\linewidth]{figs/glass.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (EBLNet~\cite{EBLNet_Glass}, GDNet~\cite{GDNet_Glass}) on \textbf{Glass Object Detection} datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:glass}
  \end{figure*}
  
    \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=0.98\linewidth]{figs/shadow.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (DSD~\cite{DSDNet_Shadow}, BDRAR~\cite{BDRAR_Shadow}, ADNet~\cite{ADNet_Shadow}, DSC~\cite{DSC_Shadow}) on \textbf{Shadow Detection} datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:shadow}
  \end{figure*}
  
        \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=0.98\linewidth]{figs/DBD.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (DENets~\cite{DENets_DBD}, IS2CNet~\cite{IS2CNet_DBD}, SG~\cite{SG_DBD}, Depth-Distill~\cite{Depth-Distill_DBD}, CENet~\cite{CENet_DBD}) on \textbf{Defocus Blur Detection} datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:DBD}
  \end{figure*}

  

  
      \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=0.95\linewidth]{figs/transparent.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art method (Translab~\cite{TransLab_Transparent}) on \textbf{Transparent Object Detection} datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:transparent}
  \end{figure*}
  

  
      \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=\linewidth]{figs/mirror.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art method (MirrorNet~\cite{MirrorNet_Mirror}) on \textbf{Mirror Detection} datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:mirror}
  \end{figure*}
  
      \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=\linewidth]{figs/cod.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (UGTR~\cite{UGTR_COD}, IS2CNet~\cite{MGL_COD}, RankNet~\cite{Rank-Net_COD}, PFNet~\cite{PFNet_COD}, SINet~\cite{SINet_COD}) on \textbf{Camouflaged Object Detection } datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:cod}
  \end{figure*}
  
    
      \begin{figure*}[!t]
  	\centering
  	\includegraphics[width=\linewidth]{figs/polyp.pdf}
  	\caption{Visual comparison between our GateNet results and the state-of-the-art methods (UACA~\cite{UACANet_Polyp}, MSNet~\cite{MSNet_Polyp}, SANet~\cite{SANet_Polyp}, PraNet~\cite{PraNet_Polyp}, SFA~\cite{SFA_Polyp}, UNet++~\cite{UNet++}, UNet~\cite{Unet}) on \textbf{Polyp Segmentation} datasets. 
  	}
%   	\vspace{-8pt}
  	\label{fig:polyp}
  \end{figure*}

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

% \bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%% Default %%
%%\input sn-sample-bib.tex%
\end{document}
