\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9206} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{
\vspace{-2mm}
CoMaL: Conditional Maximum Likelihood Approach to \\
Self-supervised Domain Adaptation in Long-tail Semantic Segmentation\vspace{-4.5mm}}  
% **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

% R1: The paper shows strong empirical performances, however is very poorly written and appears to have technical errors. The novelty seems limited in the light of very standard prior approaches, and the paper needs to clarify how the performance improvements are justified technically.

% 



%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
We would like to thank the reviewers for their constructive comments and suggestions.
R2 strongly accepts our paper; R1 and R3 weakly reject due to the concerns with the paper organization and assumptions.
% due to paper organization and assumption of our work. 
The technique details and typos have been updated 
% revised and updated in the paper 
and further explained below. We appreciate \textbf{all three reviewers} agree our experiments are \textbf{strong and %significantly 
outperformed prior SOTA methods}.
Indeed, solving corner cases is one of the crucial topics in computer vision to improve the accuracy with confidence in \textit{real-world}.
The long-tail problem in semantic segmentation is one of them.
This paper presents \textbf{one of the first works} to highlight the long-tail problem in both source and target domains in semantic segmentation and introduces a new robust and effective CoMaL approach. The method has been fully proved with mathematical theory (Sec 3.2) and consistently achieved SOTA results on multiple benchmarks.
We thank the AC for his/her careful attention. %to our paper. 
There is wide variation in the reviews. The concerns of the more negative reviewers have been addressed in the key points.
% The more negative reviewers have major concerns, which should be addressed. The rebuttal on these points will be critical in the final decision.

\noindent
\textbf{(KP1.)} R1 
%is unclear about our contributions of the solutions approach to the long-tail issue and 
degrades our contributions compared to prior methods. We respectfully but strongly disagree with the reviewer. Although the losses 
%(class-weighted, focal losses) 
in prior work 
%e.g. class-weighted loss, focal loss, etc.
have shown their advantages to mitigate the long-tail issue, % these losses cannot be applied for segmentation problem as 
they are mostly adopted for \textbf{classification} problem where the prediction is a \textit{single class per sample/image}. In the segmentation problem, an assumption of \textit{pixel independence} is required to be applicable. Moreover, \textbf{these losses cannot} be adopted to a new domain since labeled samples are not available.
Meanwhile, without the pixel-independence assumption, \textbf{CoMaL} addresses the \textbf{long-tail semantic segmentation} in three-folds: (1) multiple predictions per image (i.e., one class per pixel) with class-balanced constraint; (2) local and global constraints via structural learning with the novel conditional maximum likelihood loss; and (3) robustly adopted for both source and unlabeled target domains to maintain its high accuracy on the new  domain.
%are not advisable for the segmentation problem.


% \noindent
% \textbf{(KP1)} R1 is unclear about our contributions of the solutions approach to the long-tail issue and degrades our contributions compared to prior approaches.
% % several prior approaches to addressing the long-tailed issue. 
% We respectfully, but strongly disagree with the reviewer.
% Although the prior losses (e.g. class-weighted loss, focal loss, TF-IDF, etc) have shown their advantage in the long-tail classification problem, these losses are not advisable for the segmentation problem. 
% % 
% In particular, there is only one \textbf{class predicted per sample} in the classification problem in which there is no constraint about the structure of the output; 
% meanwhile, in the segmentation problem, there is \textbf{a class per every single pixel} and the prediction of each pixel also has its \textbf{local and global structural constraints} in the output.
% % 
% In addition, these prior methods only focus on the prediction of a single sample or a single pixel with the assumption of pixel-independence assumption and ignore the structural information. 
% % 
% Far apart from prior approaches, our contributions are to design a \textbf{novel conditional maximum likelihood loss} that considers both the prediction of each pixel and its \textbf{structural constraints} to address the long-tail issue in domain adaptation to semantic segmentation, i.e. the class-weighted loss
% % (L505-506 of Eqn (7)) 
% imposes the class in-balance of the prediction and the conditional maximum likelihood loss 
% % (Eqn (8)) 
% enforces the structural constraints of pixels in the segmentation.

\noindent
\textbf{(KP2.)} In Fig. 2, we demonstrate the long-tail issue occurs 
% in both source domain (with labeled data) \textbf{and} new target domains (without labeled data). 
in both labeled source domain \textbf{and} new \textbf{\textit{unlabeled}} target domains.
\textbf{Unlike} prior work where only labeled part is on their focus, our motivation (mentioned in L132-139) is to address the \textbf{long-tail} issue in \textbf{both two domains} which is novel and more challenging. Experiments in Table 3(a,b) have emphasized the \textbf{advantages} of CoMaL on improving the semantic segmentation accuracy on new unlabeled \mbox{domains}.

% R1 concerns about our motivation to the domain adaptation approach instead of the segmentation approach in general. As shown in Fig. 2, we have shown the long-tail issues in both source and target domains. This has been motivated in our introduction mentioned in L132-139.
% % Our proposal are applicable to both source and target domains
% % and Fig. 2 in our paper, and 
% % Thank you for your suggestion. This question gives a chance to emphasize our contributions. 
% In our work, our approach aims at two important goals: (1) The first one is the long-tail problem in semantic segmentation, (2) The second one is the long-tail problem in the target domain where the label information is not available, which is more challenging.
% Therefore, our method is designed as the domain adaptation approach to address the long-tail issue in both source domain and the unlabeled target domains. 
% % Obviously, our approach can be adopted for the long-tail semantic segmentation problem in general.


\noindent
\textbf{(KP3.)} Rather than extracting neighborhood dependencies from local structures (i.e. pixels within a particular range) with Markovian assumption as in prior work, Eqn (8), $q_s(\mathbf{y}_s^{\setminus i} | \mathbf{y}_s^i)$ emphasizes more on generalized structural constraints as it embeds the dependencies on all remaining pixels of the segmentation map.
%where the distributions of remaining pixels ($q_s(\mathbf{y}_s^{\setminus i})$) has to be consistent to structures of the entire segmentation.
Particularly, after learning the conditional distribution $q_s(\mathbf{y}_s^{\setminus i} | \mathbf{y}_s^i)$ via our proposed multihead attention network, during the training process, this term is considered as a metric to measure how good a predicted segmentation maintains the structural consistency (i.e., relative structure among objects) in comparison to the distributions of actual segmentation maps. 
%Prior approaches such as DeepLab-based models are not able to capture global constraints.

% a conditional distribution to capture the structural constraints of the entire segmentation. This conditional modeling enforces more generalized structures where both local constraints (i.e. pixels around $\mathbf{y}_s^i$ have to be consistent) and global structure ($q_s(\mathbf{y}_s^{\setminus i})$...) can be embedded via the learned distributions. 


% In our approach, the condition $q_s(\mathbf{y}_s^{\setminus i} | \mathbf{y}_s^i)$ forms the \textbf{conditional structures of the entire segmentation} which is effectively modeled by our the multi-head attention network. 
% This conditional modeling enforces the global structure constraints which plays an important role since it guarantee the the relative structures among objects and instance in the segmentation image.
% Although the Markovian assumption is widely adopted in the semantic segmentation problem, this assumption is just able to model the local structure and ignore the global structure since it just considers the neighboring pixels.
% % Also, modeling global structure with Markovian assumption is a challenging problem according to the complex relations among objects/instances.
% Therefore, DeepLab-based models are not able to fully capture global structure.

\noindent
\textbf{(KP4.)} CoMaL \textbf{does not require} the presence of ideal data in its training procedure. As in Eqn (8), with any form of ideal distributions, the 
% proposed 
CoMaL %conditional maximum likelihood 
loss is proven to be the upper bound of the loss obtained by ideal data. 
%Therefore, \textbf{CoMaL can be easily adopted} using only practical training datasets as other approaches.
%with respect to any form of ideal distribution $q’_s(\mathbf{y}_s^{\setminus i} | \mathbf{y}_s^i)$. 
CoMaL also gives a flexibility in setting the desired class distribution $q’_s(\mathbf{y}_i)$ even it is derived from non-uniform class distributions. 
%, in the case of non-uniform class distribution, this distribution could be set to any desired distribution.
%The consistently outperformed experimental results have shown the advantages of our proposed approach.

% In our work, the assumption of ideal data is not required in the end.
% % to confirm the assumption is not required in the end. 
% Therefore, it’s practical and proved via the consistently outperformed experimental results.
% % ….
% In particular, the assumption of uniform class distribution is used to derive our formulation and avoid the long-tail problem caused by the im-balance class distribution of $q_s(\mathbf{y}_s^i)$. Although we derive our formulation from the ideal data with the mentioned assumption, we do not need the presence of the ideal data during the training procedure.
% % As shown in the experimental results, without the presence of the ideal data, our approach is still able to achieve the state-of-the-art performance.
% In addition, as shown in Eqn (8), we have proven that conditional maximum likelihood loss is the upper bound of the loss with the ideal data with respect to any form of ideal distribution $q’_s(\mathbf{y}_s^{\setminus i} | \mathbf{y}_s^i)$. Also, our proposed approach gives flexibility in the choice of class distribution $q’_s(\mathbf{y}_i)$, in the case of non-uniform class distribution, this distribution could be set to any desired distribution.



\noindent
\textbf{Reviewer \#1}

\noindent
\textbf{R1\_1:} The paper starts with the analysis of long-tail problem
% in domain adaptation 
caused by imbalance class distribution (Sec 3.1). 
Then, %under the assumption of the ideal class distribution, 
the new CoMaL is presented
% conditional maximum likelihood loss 
to address 
%the long-tail 
this problem (Sec 3.2). Finally,
% to model the conditional structure, 
conditional structural learning modeled by multihead attention is presented (Sec 4) and acts as a metric for CoMaL loss in Eqn. (8).
% Thank you for your constructive comments. We have updated our paper as suggested.

\noindent
\textbf{R1\_2:} Please refer % to
% \textbf{(KP1,KP2)} 
\textbf{(KP1,KP2)} 
% \textbf{(KP1)}, \textbf{(KP2)} 
for %our 
motivations  %and
and novelty.
% contributions.
%in this work.
%approach. %instead of semantic segmentation in general.
% We respect the reviewer but strongly disargree the review that nulifies our contributions.
% 
% The standard approaches (e.g. class-weighted loss, focal loss, TF-IDF, etc) have shown their advantage in the long-tail classification problem. However, these losses are not advisable for the segmentation problem. In particular, there is only one class predicted per sample in the classification problem in which there is no constraint about the structure of the output; meanwhile, in the segmentation problem, there is a class per every single pixel and the prediction of each pixel also has its local and global structural constraints in the image. In addition, the prior losses as mentioned above only focus on the prediction of a single sample (or a single pixel) and ignore the structural information. Far apart from prior approaches, our contributions in this work are to design a novel loss that considers both the prediction of each pixel and its structural constraints to address the long-tail issue in domain adaptation to the segmentation problem. In particular, the class-weighted loss (L505-506 of Eqn (7)) imposes the class in-balance of the prediction and the conditional maximum likelihood loss (Eqn (8)) imposes the structural constraints of the pixel in the segmentation.


% \noindent
% \textbf{R1\_3:} Please refer to for 
% Thank you for your suggestion. This question gives a chance to emphasize our contributions. In our work, our approach aims at two important goals: (1) The first one is the long-tail problem in the segmentation, (2) The second one is the long-tail problem in the target domain where the label information is not available, which is more challenging. Therefore, our method is designed as the domain adaptation approach to address the long-tail issue in both source domain and the unlabeled target domains. Obviously, our approach can be adopted for the long-tail semantic segmentation problem in general.


\noindent
\textbf{R1\_3:} 
%In the source domain, 
$\mathbf{y}_s \sim q_s(\mathbf{y}_s)$ and $\hat{\mathbf{y}}_s \sim q_s(\hat{\mathbf{y}}_s)$ are the predicted and ground-truth distributions, respectively. The joint distribution is $q_s(\mathbf{y}_s, \hat{\mathbf{y}}_s) = q_s(\mathbf{y}_s)q_s(\hat{\mathbf{y}}_s | \mathbf{y}_s) = q_s(\mathbf{y})q_s(\hat{\mathbf{y}}_s)$ (as the two distributions
%of the prediction and  ground-truth 
are independent). Therefore, the expectation over the join distribution in Eqn (1) is equivalent to the one drawn independently from these distributions.
%predicted and ground-truth distributions.

% When the predictions and ground truths are drawn independently from their distributions, it's equivalent to 
% $q_s(\mathbf{y})q_s(\hat{\mathbf{y}}_s)$. 
% Meanwhile, in the case of drawing from the joint distribution,
% % $q_s(\mathbf{y}_s, \hat{\mathbf{y}}_s)$, 
% it's equivalent to $q_s(\mathbf{y}_s, \hat{\mathbf{y}}_s) = q_s(\mathbf{y}_s)q_s(\hat{\mathbf{y}}_s | \mathbf{y}_s)$. Also, $q_s(\hat{\mathbf{y}}_s | \mathbf{y}_s)=q_s(\hat{\mathbf{y}}_s)$ as the prediction is independent on the ground truth. Thus, the expectation over the join distribution in Eqn (1) is equivalent to the one drawing independently from the predicted and ground-truth distributions.
% $\mathbf{y}_t \sim p_t(\mathbf{y}_t)$ is the predicted distribution of target domain.
% % . Therefore, the expectation in Eqn (1) is drawn 
% % \textcolor{red}{distribution $q_s$}

\noindent
\textbf{R1\_4:} Please refer to \textbf{(KP3)} for the Markovian assumption.
% Although the Markovian assumption is widely adopted in the semantic segmentation problem, this assumption is just able to model the local structure and ignore the global structure since it just considers the neighboring pixels. However, global structure plays an important role since it will guarantee the the relative structures among objects and instance in the segmentation image. Nevertheless, modeling global structures is a challenging problem according to the complex relations among objects/instances. In our approach, we propose conditional maximum likelihood to model the global structure of the segmentation effectively.

\noindent
\textbf{R1\_5:} 
The auto-regressive form in Eqn. (10) provides several advantages: (1) the consistency of local pixels; and (2) constraints on the global structure of the predicted segmentation (see \textbf{KP3}). However, prior %auto-regressive 
approaches 
%(PixelRNN,  PixelCNN) 
require a pre-defined pixel order (i.e. image rasterization). 
%Therefore, i
It is infeasible to learn numerous models of all possible pixel orders for computing $q_s(\mathbf{y}_s^{\setminus i} | \mathbf{y}_s^i)$. 
In L570-578, to mitigate this issue, a proposal with a mask $\mathbf{m}$ is introduced in Eqn. (11). 
% We have updated the paper organization 
% %of our paper 
% as suggested.

% In Sec 4, %we model 
% the conditional structural learning is modeled in the auto-regressive form as Eqn (10). 
% % Then, it 
% It is effectively addressed by conditioning masked pixels on unmasked pixels and solved by our proposed multi-head attention.
% %with different mask strategies presented in the paper.
% The global structure and spatial relationship among pixels are learned by the network during the training procedure.
% Although PixelRNN or PixelCNN can be adopted to autoregressively model the conditional structure, it is ineffective due to conditional permutations as mentioned in L570-574.
% We have updated organization of our paper as suggested.
% 
% Intuitively, Sec 4 presents our conditional structure learning by multi-head attention networks. We model the conditional structure learning under the auto-regressive form. This auto-regressive form can be solve by PixelRNN or PixellCNN. However, it is sufficient since the conditional permutations are matter in these approaches as mentioned in the Sec 4 (L570-574). Therefore, we propose to model the conditional structure by a binary mask in which the masked pixel in conditioned on the unmasked pixel. By this way, we are able to model the conditional structure without the consideration of permutation. Then, we adopt the design of multi-head attention network and different masking strategies to model our conditional structure. The global structure and the spatial relationship among pixels are learned by the network during the training procedure. We have noted suggestion and reorgnize our Sec 4 to be easy to follow. 


\noindent
\textbf{R1\_6:}
% We use expectation in our Eqn (1) as we consider the average errors over the entire continuous distribution as a common used in [21, 37]. 
The expectation in Eqn (1) considers the average errors over entire distributions as common use [21,37].

\noindent
\textbf{R1\_7:} Please refer to \textbf{(R3\_1)} for the components that introduce improvements; and Sec 5.2 (Table 2 and Fig. 5) for their roles as well as the effectiveness of learning gradients.
% This is equivalent to the summation by multiplying the number of samples in the dataset.

% \noindent
% \textbf{R1\_8:}
% Thank you for your suggestion. We have updated notations in our paper.

% \noindent
% \textbf{R1\_7:} Please refer to \textbf{(R1\_3)} for the distribution $q_s$. 
% $q_s$ indicate the source distribution where the images and their segmentation ground-truths are drawn from. 
% $\mathbf{y}_t \sim p_t(\mathbf{y}_t)$ is the predicted distribution on the target domain.
%produced by the the model. 
% \textcolor{red}{need to double check with R1\_4.}

\noindent
\textbf{Reviewer \#2}
% We highly appreciate the reviewer strongly accepting our work without any concerns.
We thank the reviewer for the strong accept. % our work.

\noindent
\textbf{Reviewer \#3}

\noindent
\textbf{R3\_1} 
% \textcolor{red}{Sec 3.2}
In Sec 3.2, 
we %firstly 
form 
the adaptation formula 
% is formed 
to address long-tail problems in both source and target domains % under the assumption of ideal data 
as Eqn (4) followed by deriving it from conditional structural constraints as Eqn (5-6). 
% and Eqn (6).
Then, 
Eqn (7) 
% the entire objective % to address the long-tail issue 
is factorized into three terms: (1) the adaptation term,
% to adapt the know from the source domain to the target domain, 
(2) the class-weighted loss to impose class distributions, (3) the conditional likelihood loss to enforce the structural constraints. Finally, Eqn (8) proves the upper-bound property as mentioned in \textbf{(KP4)} and removes the requirement of the ideal data during learning.
% Finally, we prove our CoMaL is proven as an upper bound the loss of the ideal case with any form of the ideal distribution. 
We have updated the paper organization 
%of our paper 
as suggested.

\noindent
\textbf{R3\_2} Please refer to \textbf{(KP4)} for our assumption.
% \textcolor{red}{our assumption}
% In our work, the assumption of uniform class distribution is used to derive our formulation and avoid the long-tail problem caused by the im-balance class distribution of $q_s(\mathbf{y}_s^i)$. Although we derive our formulation from the ideal data with the mentioned assumption, we do not need the presence of the ideal data during the training procedure.
% As shown in the experimental results, without the presence of the ideal data, our approach is still able to achieve the state-of-the-art performance.
% In addition, as shown in Eqn (8), we have proven that conditional maximum likelihood loss is the upper bound of the loss with the ideal data with respect to any form of ideal distribution $q’_s(\mathbf{y}_s^{\setminus i} | \mathbf{y}_s^i)$. Also, our proposed approach gives flexibility in the choice of class distribution $q’_s(\mathbf{y}_i)$, in the case of non-uniform class distribution, this distribution could be set to any desired distribution.




% %%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
