% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
 \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}}%
\newcommand{\cmark}{\ding{51}}%
\usepackage{bm}
\newcommand{\bx}{{\bm x}}
% \usepackage{float}
\usepackage[accsupp]{axessibility}
\usepackage{float}
\usepackage[export]{adjustbox}
% \usepackage{subcaption}
\usepackage{graphics}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{capt-of,etoolbox}
% \usepackage{hyperref}
% \usepackage{subcaption}


% \usepackage{flushend}
% \usepackage{xcolor,soul,colortbl}
% \usepackage{authblk}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{10} % *** Enter the CVPR Paper ID here
\def\confName{CVPRW}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE

%\title{Spatrans: Spatiotemporal Transformers Approach to Self-supervised Group Action Recognition\vspace{-4mm}}%


\title{SPARTAN: Self-supervised Spatiotemporal Transformers Approach to \\ Group Activity Recognition\vspace{-6mm}}


\author{Naga VS Raviteja Chappa$^{1}$, Pha Nguyen$^{1}$, Alexander H Nelson$^{1}$, Han-Seok Seo$^{1}$, Xin Li$^{5}$ \\Page Daniel Dobbs$^{1}$,Khoa Luu$^{1}$\\
% $^{1}$CVIU Lab, University of Arkansas \\
% $^{2}$Dep. of Computer Science and Computer Engineering, University of Arkansas \\
% $^{3}$Dep. of Food Science, University of Arkansas \quad
% $^{4}$Dep. of Health, Human Performance and Recreation \\
$^{1}$University of Arkansas
$^{5}$West Virginia University\\
% Institution1 address\\
\tt\small \{nchappa, panguyen, ahnelson, hanseok, pdobbs, khoaluu\}@uark.edu,  
\tt\small Xin.Li@mail.wvu.edu \\
\url{https://uark-cviu.github.io}
\vspace{-2mm}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

In this paper, we propose a new, simple, and effective Self-supervised Spatio-temporal Transformers (SPARTAN) approach to Group Activity Recognition (GAR) using unlabeled video data. Given a video, we create local and global Spatio-temporal views with varying spatial patch sizes and frame rates. The proposed self-supervised objective aims to match the features of these contrasting views representing the same video to be consistent with the variations in spatiotemporal domains. To the best of our knowledge, the proposed mechanism is one of the first works to alleviate the weakly supervised setting of GAR using the encoders in video transformers. Furthermore, using the advantage of transformer models, our proposed approach supports long-term relationship modeling along spatio-temporal dimensions. The proposed SPARTAN approach performs well on two group activity recognition benchmarks, including NBA and Volleyball datasets, by surpassing the state-of-the-art results by a significant margin in terms of MCA and MPCA metrics\footnote{The implementation of SPARTAN is available at \url{https://github.com/uark-cviu/SPARTAN}}.

\end{abstract}
\vspace{-2mm}
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
% 1. What is the Group Action Recognition (GAR) problem? \cmark\\
% 2. Why is it important? Its applications? \cmark\\
% 3. What is WS-GAR problem? How is it related to GAR? \cmark \\
% 4. What are the current SOTA of GAR and WS-GAR? \cmark \\ 
% 5. What are the limitations of GAR and WS-GAR? \cmark \\ 
% Then link to our proposed method ... 
Group Activity Recognition (GAR) aims to classify the collective actions of individuals in a video clip. This field has gained significant attention due to its diverse applications such as sports video analysis, video monitoring, and interpretation of social situations. 
Diverging significantly from the traditional methods of action recognition that concentrate on comprehending indivudal actions~\cite{wang2016temporal, carreira2017quo, wang2018non, Ranasinghe_2022_CVPR}, whereas the GAR requires a fine-grained analysis of multiple actors interactions in a given scene. Thereby providing a challenging scenario which include the consistency of interaction analysis in spatial and temporal domains and precise identification of the corresponding actors.
Considering the discussed challenges, the necessity of actors' ground-truth bounding boxes is inherent during training and testing and their corresponding action labels during only training phase for most of the current GAR methods~\cite{ibrahim2016hierarchical, wu2019learning, hu2020progressive, gavrilyuk2020actor, pramono2020empowering, ehsanpour2020joint, yan2020higcin, yuan2021learning, li2021groupformer, quach2022non, 9897440}. By utilizing approaches like \emph{RoIPool}~\cite{ren2015faster} and \emph{RoIAlign}~\cite{he2017mask}, the actor features are extracted using the bounding box information which aids to the accurate understanding of their spatiotemporal relationships. To perform the group activity classification, video representations are formed at the group level by combining all the extracted individual actor features while considering the inter-actor relationships, which are employed as input to a classifier.

\begin{figure}[!t]
    \centering
\includegraphics[width=0.46\textwidth]{gar_figures/page1_v4.png}
% \put(-205,-8){(a)}
% \put(-125,-8){(b)}
% \put(-45,-8){(c)}
% % \put(-250,110){(a)}
% % \put(-250,65){(b)}
% \put(-250,20){(c)}
\put(-55,95){\tiny Predicted confidence scores} 
\put(-41,90){\tiny per category}
\put(-46,103){\tiny False prediction}
\put(-46,113){\tiny True prediction}
\put(-45,121){\tiny 2p.-succ.}
\put(-45,128){\tiny 3p.-succ.}
\put(-27,82){\tiny DFWSGAR}
\put(-24,37){\tiny SPARTAN}
\put(-150, -8){(i)}
\put(-35, -8){(ii)}

    \caption{Visualization of attention captured by the model. (i) The attention in this example focuses on how the relationship is established between the actors. Original sequence from NBA dataset~\cite{yan2020social} (top), Attention captured by DFWSGAR~\cite{kim2022detector} (middle), and SPARTAN model (bottom). \textcolor{red}{Red}-colored actors are the irrelevant information to determine the group activity, whereas \textcolor{ForestGreen}{green}-colored actors, including their positions, are the most relevant. (ii) illustrates that DFWSGAR predicts the category wrong due to the effects shown in (i) whereas SPARTAN is more confident in the prediction, which is further justified by the t-SNE plot as shown in \cref{fig:tsne}.}
     \vspace{-0.2in}
    \label{fig:page1}
\end{figure}
Despite the fact that these approaches performed admirably on the difficult task, their reliance on bounding boxes at inference and substantial data labelling annotations makes them unworkable and severely limits their application.
% 
% To overcome this problem, one approach is to simultaneously train person detection and group activity recognition using bounding box labels~\cite{bagautdinov2017social, zhang2019fast}. This method estimates the bounding boxes of actors in inference. However, this method calls for individual actor ground-truth bounding boxes for training videos. Yan~\etal~\cite{yan2020social} presented the Weakly Supervised GAR (WSGAR) learning approach, which does not need actor-level labels in both training and inference, to further lower the annotation cost.
A potential solution to address this issue is simultaneous training of detecting the actors and performing the classification of group activity utilizing all the action labels and bounding box information~\cite{bagautdinov2017social, zhang2019fast}. However, this method implemented training in a fully-supervised setting. Yan~\etal~\cite{yan2020social} presented the Weakly Supervised GAR (WSGAR) learning approach, in the process of mitigating the usage of fine-grained labels during both upstream and downstream tasks.
% To address this issue, a potential solution involves jointly training person detection and group activity recognition with the utilization of bounding box labels~\cite{bagautdinov2017social, zhang2019fast}. This technique predicts the bounding boxes of actors during inference. However, the prerequisite for this approach is the availability of individual actor ground-truth bounding boxes for training videos.
% In a related vein, Yan~\etal~\cite{yan2020social} introduced the Weakly Supervised GAR (WSGAR) learning paradigm, which eliminates the need for actor-level annotations in both training and inference phases, thereby significantly reducing annotation costs.
They generate actor box suggestions using a detector that has been pre-trained on an external dataset in order to solve the absence of bounding box labels. They then learn to eliminate irrelevant possibilities.
Recently, Kim~\etal~\cite{kim2022detector} introduced a detector-free method for WSGAR task which captures the actor information using partial contexts of the token embeddings.
However, the previous methods~\cite{yan2020social, kim2022detector} have various drawbacks as follows. 
First, a detector~\cite{yan2020social} often leads to missing detection of people in case of occlusion, which minimizes overall accuracy. 
Second, partial contexts~\cite{kim2022detector} can only learn if and only if there is movement in consecutive frames. This can be inferred from the illustration in \cref{fig:page1}. 
Third, the temporal information among the tokens must be consistent, and \cite{kim2022detector} does not consider different tokens.

In this paper, we introduce a new simple but effective Self-Supervised \textbf{Spa}tio-tempo\textbf{r}al \textbf{T}r\textbf{an}sformers (SPARTAN) approach to the task of Group Action Recognition that is independent of ground-truth bounding boxes, labels during pre-training, and object detector. Our mechanism only exploits the motion as a supervisory signal from the RGB data modality. 
As seen in Fig.~\ref{fig:page1} (i), our model captures not only the key actors but also their positions, which shows that our method is more effective in group activity classification than DFWSGAR~\cite{kim2022detector}. Our approach is designed to benefit from varying spatial and temporal details within the same deep network. We use a video transformer \cite{gberta_2021_ICML} based approach to handle varying temporal resolutions within the same architecture. Furthermore, the self-attention mechanism in video transformers can capture local and global long-range dependencies in both space and time, offering much larger receptive fields compared to standard convolutional kernels \cite{naseer2021intriguing}. 
% \textbf{Fail cases of DFWSGAR:}
% \begin{itemize}
%     \item They can learn the partial contexts \underline{if and only if there is motion across the patches.} To demonstrate this, it fails to distinguish the 2p and 3p classes based on the t-SNE diagram.
%
%     \item To learn the temporal information, the tokens must be consistent across all the frames. They need to consider the information across different tokens.
%
% \end{itemize}
%
%In this paper, we propose a 
%\noindent\textbf{Our Contributions in this Work:}
The contributions of this work can be summarized as follows.
\begin{itemize}
    \item Instead of considering only motion features across consecutive frames \cite{kim2022detector}, we introduce the first training approach to GAR by exploiting spatial-temporal correspondences. The proposed method varies the space-time features of the inputs to learn long-range dependencies in spatial and temporal domains. 
    %(targeting limitation of DFWSGAR paper \textbf{which considers motion feature across consecutive frames.}.

    \item A new self-supervised learning strategy is performed by jointly learning the inter-frame, i.e., frame-level \textit{temporal}, and intra-frame, i.e., patch-level \textit{spatial}, correspondences further forming into \emph{Inter Teacher-Inter Student loss} and \emph{Inter Teacher-Intra Student loss}. In particular, the spatiotemporal features global, from the entire sequence, and local from the sampled sequence are matched by the learning objectives of the frame level and the patch level in the latent space.
    
    % \item \textcolor{red}{Flow-attention modeling replaces traditional self-attention for strong attention in space and time domains.}
    
    \item With extensive experiments on NBA\cite{yan2020social}, and Volleyball\cite{ibrahim2016hierarchical} datasets, the proposed method shows the State-of-the-Art (SOTA) performance results using only RGB inputs.
\end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=0.90\textwidth]{gar_figures/initial_framework_v5.png}
    %\vspace{-4mm}
    \put(-330,18){$\mathcal{L}_{g_{t}-l_{t}}$:\scriptsize Inter Teacher-Inter Student Loss}
    \put(-330,04){$\mathcal{L}_{g_{t}-l_{s}}$:\scriptsize Inter Teacher-Intra Student Loss}
    \put(-235, 145){\rotatebox[origin=c]{90}{$\mathcal{L}_{g_{t}-l_{t}}$}}
    \put(-221, 145){\rotatebox[origin=c]{90}{$\mathcal{L}_{g_{t}-l_{s}}$}}
    \put(-245,190){\scriptsize $\bm{\Tilde{f}_{g_{t}}}$}
    \put(-245,93){\scriptsize $\bm{\Tilde{f}_{l_{t}}}$}
    \put(-225,75){\scriptsize $\bm{\Tilde{f}_{l_{s}}}$}
    \put(-167,26){$\scriptsize \bm{\Tilde{f}_{x}}$}
    \put(-408,181){$\bm{g}_{t}$}
    \put(-404,110){$\bm{l}_{t}$}
    \put(-404,68){$\bm{l}_{s}$}
    \put(-430, 235){\scriptsize Global}
    \put(-440, 225){\scriptsize Temporal Views}
    \put(-452, 215){\scriptsize \textbf{[B$\times$$K_{g}$$\times$C$\times$$H_{g}$$\times$$W_{g}$]}}
    \put(-410, 40){\scriptsize Local}
    \put(-430, 31){\scriptsize Spatiotemporal Views}
    \put(-430, 20){\scriptsize \textbf{[B$\times$$K_{l}$$\times$C$\times$$H_{l}$$\times$$W_{l}$]}}
    \put(-485, 78){\scriptsize Input Video}
    \put(-450,78){\scriptsize ($\bm{X}$)}
    \put(-488, 70){\scriptsize \textbf{[B$\times$T$\times$C$\times$H$\times$W]}}
    \caption{\textbf{The proposed SPARTAN Framework} samples gave input video into global and local views. The sampling strategy for video clips results in different frame rates and spatial characteristics between global views and local views, which are subject to spatial augmentations and have limited fields of view. The teacher model processes global views ($\bm{g}_{t}$) to generate a target, while the student model processes local views ($\bm{l}_{t}$ \& $\bm{l}_{s}$) where $K{l}$ $\le$ $K_{g}$. The network weights are updated by matching the online student local views to the target teacher global views, which involves \emph{cross-view correspondences} and \emph{motion correspondences}. Our approach utilizes a standard ViT backbone with separate space-time attention \cite{gberta_2021_ICML} and an MLP to predict target features from online features.}
    %\vspace{-0.2in}
    \label{fig:framework}
\end{figure*}
\vspace{-3mm}
\section{Related Work}


\subsection{Group Activity Recognition (GAR)}
Due to the wide range of applications, GAR has recently gained more attention. The initial approaches in the field utilized probabilistic graphical methods ~\cite{amer2014hirf,amer2013monte,amer2015sum,lan2012social,lan2011discriminative,wang2013bilinear} and AND-OR grammar methods ~\cite{amer2012cost,shu2015joint} to process the extracted features. 
As deep learning evolved over the years, methods involving Convolutional Neural Networks (CNN) ~\cite{bagautdinov2017social,ibrahim2016hierarchical}, Recurrent Neural Networks (RNN) ~\cite{wang2017recurrent,yan2018participation,qi2018stagnet,bagautdinov2017social,deng2016structure,shu2019hierarchical,li2017sbgar,ibrahim2016hierarchical,ibrahim2018hierarchical} achieved outstanding performance thanks to their learning power of high-level information and temporal context.

Recent methods for identifying group actions~\cite{wu2019learning,gavrilyuk2020actor,hu2020progressive,yan2020social,ehsanpour2020joint,pramono2020empowering,li2021groupformer,yuan2021spatio} typically utilize attention-based models and require explicit character representations to model spatial-temporal relations in group activities. Graph convolution networks, as described in~\cite{wu2019learning,yuan2021spatio}, are used to learn spatial and temporal information of actors by constructing relational graphs, while Rui~\etal~\cite{yan2020social} suggest building spatial and temporal relation graphs to infer actor links. Kirill~\etal~\cite{gavrilyuk2020actor} use a transformer encoder-based technique with different backbone networks to extract features for learning actor interactions from multimodal inputs. Li~\etal~\cite{li2021groupformer} use a clustered attention approach to capture contextual spatial-temporal information.
% They dynamically cluster the individuals to have better learning of the semantic representations. 
Mingfei~\etal~\cite{han2022dual} proposed MAC-Loss which is a combination of spatial and temporal transformers in two complimentary orders to enhance the learning effectiveness of actor interactions and preserve actor consistency at the frame and video levels. 
\textbf{Weakly supervised group activity recognition (WSGAR).} Several techniques have been developed to tackle WSGAR with limited supervision, like training detectors within the framework using bounding boxes. One approach is WSGAR, which does not rely on bounding box annotations during training or inference and incorporates an off-the-shelf item detector into the model. Another technique, proposed by Zhang et al. \cite{zhang2021multi}, uses activity-specific characteristics to improve WSGAR, but is not specifically designed for GAR. Kim et al. \cite{kim2022detector} proposed a detector-free method that uses transformer encoders to extract motion features. Our proposed method is a self-supervised training approach dedicated to WSGAR, which does not require actor-level annotations, object detectors, or labels.
% \subsection{Transformers}

\noindent\textbf{Transformers in Vision}. 
Vaswani~\etal~\cite{vaswani2017attention} introduced the transformer architecture for sequence-to-sequence machine translation. This architecture has since been widely adopted to many various natural processing tasks. 
Dosovitskiy~\etal~\cite{dosovitskiy2020image} introduced a transformer architecture that is not based on convolution for image recognition tasks. For different downstream computer vision tasks, these works~\cite{li2021ffa, yuan2021tokens,liu2021swin,wang2021pyramid} used transformer architecture as a general backbone to make exceptional performance progress. In the video domain, many works~\cite{han2020mining, arnab2021vivit, li2022uniformer, bertasius2021space,fan2021multiscale,patrick2021keeping, Quach_2021_CVPR, 9895423, quach2022depth} exploited spatial and temporal self-attention to learn video representation efficiently. 
% Bertasius~\etal~\cite{bertasius2021space} explore the different mechanisms of space and time attention to learn spatiotemporal features efficiently. 
% Fan et al. \cite{fan2021multiscale} uses multiscale feature aggregation to improve the learning performance of features. 
Patrick~\etal~\cite{patrick2021keeping} introduce a self-attention block that focuses on the trajectory, which tracks the patches of space and time in a video transformer.

\section{SPARTAN}

The proposed method aims to recognize a group activity in a given video in the absence of the ground-truth information like actor corresponding bounding boxes. The general architecture of our self-supervised training within the teacher-student framework for group activity recognition is illustrated in Fig. \ref{fig:framework}. Unlike the other contrastive learning methods, we process two clips from the same video by changing their spatial-temporal characteristics, which do not rely on the memory banks. The proposed loss formulation matches the features of the two dissimilar clips to impose consistency in motion and spatial changes in the same video. The proposed SPARTAN framework will be discussed further in the following sections.
 % \vspace{-2mm}
\subsection{Self-Supervised Training}\label{subsec:ssltraining}
% \vspace{-7mm}
Given the high temporal dimensionality of videos, motion and spatial characteristics of the group activity will be learned, such as 3p.-succ. (from NBA dataset~\cite{yan2020social}) or l-spike (from Volleyball dataset~\cite{ibrahim2016hierarchical}) during the video. Thus, several video clips with different motion characteristics can be sampled from a single video. A key novelty of the proposed approach involves predicting these different video clips with varying temporal characteristics from each other in the feature space. It leads to learning contextual information that defines the underlying distribution of videos and makes the network invariant to motion, scale, and viewpoint variations. Thus, self-supervision for video representation learning is formulated as a motion prediction problem that has three key components: \textbf{a)} We generate multiple temporal views consisting of different numbers of clips with varying motion characteristics from the same video as in \cref{subsec:mo_pred}, \textbf{b)} In addition to motion, we vary spatial characteristics of these views as well by generating local, i.e., smaller spatial field, and global, i.e., higher spatial field, of the sampled clips as in \cref{subsec:cross_view_corrspondences}, and \textbf{c)} We introduce a loss function in \cref{subsec:loss} that matches the varying views across spatial and temporal dimensions in the latent space.
 % \vspace{-2.15em}
\vspace{-3mm}
\subsubsection{Motion Prediction as Self-supervision Learning}
\label{subsec:mo_pred}
% A defining characteristic of a video is the frame rate, i.e., the number of frames sampled from a video, and varying the frame rate can change the motion context of a video, e.g., walking slow v.s walking fast while controlling nuanced actions, e.g., subtle body movements of walking action. Video clips are generally sampled at a fixed frame rate~\cite{qian2020spatiotemporal, xiao2021modist}. However, given two views of varying frame rates, i.e., the different number of total clips for each view, predicting one from the other in feature space explicitly involves modeling the motion of objects across those clips. In addition, predicting subtle movements captured at high frame rates will force a model to learn contextual information related to motion from a low frame rate input. 
The frame rate is a crucial aspect of a video as it can significantly alter the motion context of the content. For instance, the frame rate can affect the perception of actions, such as walking slowly versus walking quickly, and can capture subtle nuances, such as the slight body movements in walking. Traditionally, video clips are sampled at a fixed frame rate \cite{qian2020spatiotemporal, xiao2021modist}. However, when comparing views with different frame rates, i.e., varying numbers of clips, predicting one view from another in feature space requires explicitly modeling object motion across clips. Furthermore, predicting subtle movements captured at high frame rates compels the model to learn contextual information about motion from a low frame rate input.
 
\textbf{Temporal Views:} 
We refer to a collection of clips sampled at a specific video frame rate as a temporal view. We generate different views by sampling at different frame rates, producing temporal views with varying resolutions. The number of temporal tokens ($T$) input to ViT varies in different views. Our proposed method enforces the correspondences between such views, which allows for capturing different motion characteristics of the same action. We randomly sampled these views to create motion differences among them. Our ViT models process these views, and we predict one view from the other in the latent space. In addition to varying temporal resolution, we also vary the resolution of clips across the spatial dimension within these views. It means that the spatial size of a clip can be lower than the maximum spatial size (224), which can also decrease the number of spatial tokens.   Similar sampling strategies have been used \cite{feichtenhofer2019slowfast, kahatapitiya2021coarse} but under multi-network settings, while our approach handles such variability in temporal resolutions with a single ViT model by using vanilla positional encoding~\cite{vaswani2017attention}. %Moreover, lower spatial resolutions increase temporal resolution significantly while maintaining a fixed compute requirement for a given view. 
\vspace{-2mm}
\subsubsection{Cross-View Correspondences}
\label{subsec:cross_view_corrspondences}
Our training strategy aims to learn the relationships between a given video's temporal and spatial dimensions. To this end, we propose novel cross-view correspondences by altering the field of view during sampling. We generated global and local temporal views from a given video to achieve this.

\textbf{Global Temporal Views ($\bm{g}_{t}$):} We randomly sample $K_{g}$ (is equal to $T$) frames from a video clip with spatial size fixed to $W_{global}$ and $H_{global}$. These views are fed into the teacher network which yields an output denoted by $\bm{\Tilde{f}_{g_{t}}}$.

\textbf{Local Spatiotemporal Views ($\bm{l}_{t}$ and $\bm{l}_{s}$):} 
Local views cover a limited portion of the video along both spatial and temporal dimensions. We generate local temporal views by randomly sampling several frames $K_{l}$ ($\leq$ $K_{g}$) with a spatial size fixed to $W_{local}$ and $H_{local}$. These views are fed into the student network which yields two outputs denoted by $\bm{\Tilde{f}_{l_{t}}}$ and $\bm{\Tilde{f}_{l_{s}}}$ respectively.

\textbf{Augmentations:} 
We apply different data augmentation techniques to the spatial dimension, that is, to the clips sampled for each view. Specifically, we apply color jittering and gray scaling with probability 0.8 and 0.2, respectively, to all temporal views. We apply Gaussian blur and solarization with probability 0.1 and 0.2, respectively, to global temporal views.

% Our intuition is that predicting a global temporal view of a video from a local temporal view in the latent space forces the model to learn high-level contextual information by modeling  \textbf{a)} spatial context in the form of the possibilities surrounding a given spatial crop, and \textbf{b)} temporal context in the form of possible previous or future clips from a given temporal crop. Note that spatial correspondences also involve a temporal component; for a given clip from a local view at timestamp $t=i$, our approach also tries to predict a global view at timestamp $t=j$. These cross-view correspondences are enforced by our similarity objective, which predicts different views from each other. 
Our approach is based on the intuition that learning to predict a global temporal view of a video from a local temporal view in the latent space can help the model capture high-level contextual information. Specifically, our method encourages the model to model both spatial and temporal context, where the spatial context refers to the possibilities surrounding a given spatial crop and the temporal context refers to possible previous or future clips from a given temporal crop. It is important to note that spatial correspondences also involve a temporal component, as our approach attempts to predict a global view at timestamp $t=j$ from a local view at timestamp $t=i$. To enforce these cross-view correspondences, we use a similarity objective that predicts different views from each other.

\subsection{The Proposed Objective Function}\label{subsec:loss}
Our model is trained with an objective function that predicts different views from each other. These views represent different spatial-temporal variations that belong to the same video. 

Given a video $\bm{X}=\{\bx_t\}_{t=1}^T$, where $T$ represents the number of frames, let $\bm{g}_{t}$, $\bm{l}_{t}$ and $\bm{l}_{s}$ represent global temporal views, local temporal and spatial views such that $\bm{g}_{t}=\{\bx_t\}_{t=1}^{K_{g}}$ and $\bm{l}_{t} = \bm{l}_{s} =\{\bx_t\}_{t=1}^{K_{l}}$, where $\bm{g}_{t}$, $\bm{l}_{t}$ and $\bm{l}_{s}$ are subsets of video $\bm{X}$ and $K_{l} \le K_{g}$ where $K_{g}$ and $K_{l}$ are the number of frames for teacher and student (global and local) inputs. We randomly sample $K_{g}$ global and $K_{l}$ local temporal views as in \cref{subsec:cross_view_corrspondences}. These temporal views are passed through the student and teacher models to get the corresponding class tokens or feature $\bm{f}_g$ and $\bm{f}_l$. These class tokens are normalized as follows.
%
\begin{equation}
     \bm{\Tilde{f}}^{(i)} = \frac{\text{exp}(\bm{f}^{(i)}) / \tau}{\sum_{i=1}^n \text{exp}(\bm{f}^{(i)})/ \tau },
\end{equation}
%
\begin{equation}
\bm{\Tilde{f}}^{(i)} = \frac{\sum_{j=1}^{m}\text{exp}(\beta_j \cdot \bm{f}^{(i)}) / \tau^{\frac{1}{\alpha_j}}}{\sum_{i=1}^{n} \sum_{j=1}^{m} \text{exp}(\beta_j \cdot \bm{f}^{(i)})/ \tau^{\frac{1}{\alpha_j}}},
\end{equation}
where $\tau$ is a temperature parameter used to control the sharpness of the exponential function \cite{caron2021emerging} and $\bm{f}^{(i)}$ is each element in $\bm{\Tilde{f}^{(i)}}\in\mathbb{R}^{n}$.  

\textbf{Inter Teacher-Inter Student Loss:} 
Our $\bm{g}_{t}$ have the same spatial size but differ in temporal content because the number of clips/frames is randomly sampled for each view. One of the $\bm{g}_{t}$ always passes through the teacher model that serves as the target label. We map the student's $\bm{l}_{t}$ with the teacher's $\bm{g}_{t}$ to create a global-to-local temporal loss as in Eqn. \eqref{eq:global_to_global_loss}.
%
\begin{align}\label{eq:global_to_global_loss}
    \mathcal{L}_{g_{t}-l_{t}} &=  -\bm{\Tilde{f}}_{g_{t}} * log(\bm{\Tilde{f}}_{l_{t}}),
\end{align}
%
where $\bm{\Tilde{f}_{g_{t}}}$ and $\bm{\Tilde{f}_{l_{t}}}$ are the tokens of the class for $\bm{g}_{t}$ and $\bm{l}_{t}$ produced by the teacher and student, respectively.

% Using these normalized features, our base loss function is cross-entropy loss computed as follows, which we minimize w.r.t. to the parameters $\theta_s$ of our joint student network $H_s$ and  $G_s$:
% \begin{align}
%     \mathcal{L}_\text{base}(a, b) &= \sum_{i} - \hat f_{i,b,t} * log(\hat f_{i,a,s})
% \end{align}
% where the summation is over a mini-batch and the output of the teacher network (weights frozen), $\hat f_{i, b, t}$, is the fixed target. 

\textbf{Inter Teacher-Intra Student Loss:} 
Our $\bm{l}_{t}$ have a limited field of vision along the spatial and temporal dimensions compared to the $\bm{g}_{t}$. However, the number of local views is four times higher than that of global views. All $\bm{l}_{s}$ are passed through the student model and mapped to $\bm{g}_{t}$ from the teacher model to create the loss function as in \cref{eq:local_to_global_loss}.
%
\begin{align}\label{eq:local_to_global_loss}
    \mathcal{L}_{g_{t}-l_{s}} &= \sum_{n=1}^{q} -\bm{\Tilde{f}}_{g_{t}} * log(\bm{\Tilde{f}}^{(n)}_{l_{s}}),
\end{align}
%
where $\bm{\Tilde{f}_{l_{s}}}$ are the tokens of the class for $\bm{l}_{s}$ produced by the student and $q$ represents the number of local temporal views set to sixteen in all our experiments. The overall loss to train our model is simply a linear combination of both losses, as in Eqn. \eqref{eq:global_to_global_loss} and Eqn. \eqref{eq:local_to_global_loss}, given as in Eqn. \eqref{eq:totalloss}.
%
\begin{equation} \label{eq:totalloss}
    \mathcal{L} =  \mathcal{L}_{g_{t}-l_{t}} +  \mathcal{L}_{g_{t}-l_{s}}
\end{equation}
%
\subsection{Inference}\label{subsec:Inference}

Fig. \ref{fig:my_label1} illustrates our inference framework. During this stage, fine-tuning of the trained self-supervised model is performed. We use the pre-trained SPARTAN model and fine-tune the model with the available labels, followed by a linear classifier. We use this on downstream tasks to improve performance.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{gar_figures/downstreamv2.png}
\caption{\textbf{Inference}. We uniformly sample the video clip and pass it through a shared network and generate feature vectors (class tokens). These vectors are fed to the downstream task classifier.}
    \vspace{-0.1in}
    \label{fig:my_label1}
\end{figure}


\section{Experiments}


\subsection{Datasets}
\noindent\textbf{Volleyball Dataset}\cite{ibrahim2016hierarchical} 
% contains 3,493 training and 1,337 testing clips, a total of 4,830 labeled clips, from 55 volleyball videos. The dataset has 8 group activity categories that are annotated to each clip. In addition, nine individual action labels and their corresponding bounding boxes are annotated to the middle frame in each clip. However, in the setting of WSGAR, models including ours utilize the group activity labels only and ignore the other useful annotations. We adopt Multi-class Classification Accuracy (MCA) and Merged MCA for evaluation throughout our experiments. In particular, for computing Merged MCA, we merge the classes right set and right pass into right pass-set, and left set and left pass into left pass-set as in SAM~\cite{yan2020social} and DFWSGAR~\cite{kim2022detector} for a fair comparison.
comprises 3,493 training and 1,337 testing clips, totaling 4,830 labeled clips, from 55 videos. The dataset contains annotations for eight group activity categories and nine individual action labels with corresponding bounding boxes. However, in our WSGAR experiments, we only use the group activity labels and ignore the individual action annotations. For evaluation, we use Multi-class Classification Accuracy (MCA) and Merged MCA metrics, where the latter merges the right set and right pass classes into right pass-set and the left set and left pass classes into left pass-set, as in previous works such as SAM~\cite{yan2020social} and DFWSGAR~\cite{kim2022detector}. This is done to ensure a fair comparison with existing methods.
% \noindent\textbf{Collective Dataset} \cite{choi2009they} contains 44 videos with 2481 clips where individual actions and their corresponding bounding boxes are annotated for every ten frames. Here, the group activity class is defined by the most significant number of classes. We are following \cite{yan2020higcin,yan2018participation,yuan2021spatio} regarding the dataset splits.

\noindent\textbf{NBA Dataset}\cite{yan2020social} 
% contains 7,624 training and 1,548 test clips, a total of 9,172 labeled clips, from 181 NBA videos where each clip is annotated with one of the nine group activities. There is no information provided on individual actions or bounding boxes. For evaluation, we adopt Multi-class Classification Accuracy (MCA) and Mean Per Class Accuracy (MPCA) metrics; MPCA is adopted due to the class imbalance issue of the dataset.
in our experiment comprises a total of 9,172 labeled clips from 181 NBA videos, with 7,624 clips used for training and 1,548 for testing. Each clip is annotated with one of nine group activities, but there is no information on individual actions or bounding boxes. In evaluating the model, we use the Multi-class Classification Accuracy (MCA) and Mean Per Class Accuracy (MPCA) metrics, with MPCA used to address the issue of class imbalance in the dataset.
% \noindent\textbf{JRDB-Act Dataset}\cite{ehsanpour2022jrdb} contains 2206 training and 1419 testing clips (total of 3625 labeled clips) from 54 indoor and outdoor video sequences. This dataset has over 2.3 million annotated bounding boxes on the image data. There are 26 individual actions and social activities annotated for every frame in each video sequence. \textcolor{red}{Will be updated further.}

\subsection{Deep Network Architecture}
Our video processing approach uses a vision transformer (ViT) \cite{gberta_2021_ICML} to apply individual attention to both the temporal and spatial dimensions of the input video clips. The ViT consists of 12 encoder blocks and can process video clips of size $(B\times T\times C\times W\times H)$, where $B$ and $C$ represent the batch size and the number of color channels, respectively. The maximum spatial and temporal sizes are $W=H=224$ and $T=18$, respectively, meaning that we sample 18 frames from each video and rescale them to $224\times 224$.
Our network architecture (see Fig.~\ref{fig:framework}) is designed to handle variable input resolution during training, such as differences in frame rate, number of frames in a video clip, and spatial size. However, each ViT encoder block processes a maximum of 196 spatial and 16 temporal tokens, and each token has an embedding dimension of $\mathbb{R}^{m}$ \cite{dosovitskiy2020image}. Along with these spatial and temporal input tokens, we also use a single classification token as a characteristic vector within the architecture \cite{devlin2018bert}. This classification token represents the standard features learned by the ViT along the spatial and temporal dimensions of a given video.
During training, we use variable spatial and temporal resolutions that are $W \le 224$, $H \le 224$, and $T \le 18$, which result in various spatial and temporal tokens. Finally, we apply a projection head to the class token of the final ViT encoder \cite{caron2021emerging, grill2020bootstrap}.
 
\textbf{Self-Distillation.} 
In our approach (shown in \cref{fig:framework}), we adopt a teacher-student setup for self-distillation inspired by \cite{caron2021emerging, grill2020bootstrap}. The teacher model has the same architecture as the student model, including the ViT backbone and predictor MLP, but it does not undergo direct training. Instead, during each training step of the student model, we update the teacher weights using an exponential moving average (EMA) of the student weights \cite{caron2021emerging}. This approach enables us to use a single shared network to process multiple input clips.

\subsection{Implementation Details }
For both the NBA and Volleyball datasets, frames are sampled at a rate of T ($K_{g}$) using segment-based sampling \cite{wang2016temporal}. The frames are then resized to $W_{g} = 224$ \& $H_{g} = 224$ for the teacher input and $W_{l} = 96$ \& $H_{l} =96$ for the student input, respectively. For the Volleyball dataset, we use $K_{g}$ = 5 ($K_{l}\in{3,5}$), while for the NBA dataset, we use $K_{g}$ = 18 ($K_{l}\in{2,4,8,16,18}$). We randomly initialize weights relevant to temporal attention, while spatial attention weights are initialized using a ViT model trained self-supervised over ImageNet-1K \cite{imagenet}. This initialization setup allows us to achieve faster convergence of space-time ViT similar to the supervised setting \cite{gberta_2021_ICML}. We use an Adam optimizer \cite{kingma15adam} with a learning rate of $5\times10^{-4}$, scaled using a cosine schedule with a linear warm-up for five epochs \cite{Steiner2021HowTT, chen2021mocov3}. We also use weight decay scaled from 0.04 to 0.1 during training. For the \textbf{downstream task}, we train a linear classifier on our pretrained SPARTAN backbone. During training, the backbone is frozen, and the classifier is trained for 100 epochs with a batch size of 32 on a single NVIDIA-V100 GPU using SGD with an initial learning rate of 1e-3 and a cosine decay schedule. We also set the momentum to 0.9.
 % \begin{table}[t]
 %    % \parbox{1.\linewidth}{
 %        \centering
 %        \footnotesize
 %        \setlength{\tabcolsep}{0.7mm}{
 %        \begin{tabular}{lccc}
 %        % \toprule
        
 %        % \vspace
 %        Method         & Backbone        & \begin{tabular}{@{}c@{}} \textbf{Individual}\\\textbf{Action} \end{tabular}  & \begin{tabular}{@{}c@{}} \textbf{Group}\\\textbf{Activity} \end{tabular} \\
 %        % \midrule
 %        \hline
 %         HDTM\cite{ibrahim2016hierarchical}   &AlexNet              &   -                   & 81.9      \\
 %         CERN\cite{shu2017cern}  & VGG16              & -                   & 83.3      \\
 %         StageNet\cite{qi2018stagnet}& VGG16           & -                 & 89.3     \\
 %        HRN\cite{ibrahim2018hierarchical} & VGG19              & -                 & 89.5      \\
 %        SSU\cite{bagautdinov2017social} & Inception-v3       & 81.8              & 90.6      \\
 %        AFormer\cite{gavrilyuk2020actor} & I3D         & -              & 91.4      \\
 %        ARG\cite{wu2019learning}  & Inception-v3          & 83.0               & 92.5      \\
 %        TCE+STBiP \cite{yuan2021learning} & Inception-v3         & -               & 93.3      \\
 %        % DIN \cite{yuan2021spatio} &   VGG-16      &   100\%         &                   & -              & 93.6 \\
 %        DIN \cite{yuan2021spatio} &   ResNet-18      & -             & 93.1 \\
 %        GFormer\cite{li2021groupformer} &   Inception-v3       & 83.7              & 94.1 \\
 %        % \midrule
        
 %        Dual-AI\cite{han2022dual}         & Inception-v3          &    \textbf{84.4(contr. loss)}      & \textbf{94.4(contr. loss)}     \\
 %        \hline
 %        % \midrule
 %        % {[}3{]}  & I3D                &   100\%        & Flow              &                   & 91.47     \\
 %        % {[}4{]}  & I3D                &   100\%         &  Flow                 &                   & 91.5      \\
 %        % Ours     & Inception          &   50\%          &  Flow                 &                   & 92.74     \\
 %        %          & Inception          &   100\%         &  Flow                 &    86.16       & \textbf{94.09}     \\

    
        % Ours        & Inception-v3 & \textcolor{red}{84.2}& \textcolor{red}{94.10}\\
 %        % \bottomrule
 %        \hline
 %        \end{tabular}}
 %        % \vspace{-2mm}
 %        \caption{Comparison with state-of-the-art methods on \textbf{Volleyball dataset} in term of Acc.\%. \textcolor{red}{\textbf{CE loss} }
 %        % "Flow" denotes whether the additional optical flow is used to produce the result. 
 %        }
 %        % \vspace{-2mm}
 %        \label{tab:volleyball_full}
 %    % }
 %    \end{table}

 \begin{table}[!t]

\vspace{-4mm}
\begin{center}
\begin{tabular}{>{\arraybackslash}m{2.35cm} | >{\centering\arraybackslash}m{1.3cm}>{\centering\arraybackslash}m{0.85cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.9cm}}
\hline
Method                                                        & MCA       & MPCA      \\
\hline
% \addlinespace[0.5ex]
\multicolumn{3}{c}{\textbf{Video backbone}} \\
% \addlinespace[0.5ex]
\hline
TSM~\cite{lin2019tsm}                                        & 66.6      & 60.3      \\
VideoSwin~\cite{liu2021video}                                & 64.3      & 60.6      \\
\hline
% \addlinespace[0.5ex]
\multicolumn{3}{c}{\textbf{GAR model}} \\
% \addlinespace[0.5ex]
\hline
ARG~\cite{wu2019learning}                                      & 59.0      & 56.8      \\
AT~\cite{gavrilyuk2020actor}                                  & 47.1      & 41.5      \\
SACRF~\cite{pramono2020empowering}                            & 56.3      & 52.8      \\
DIN~\cite{yuan2021spatio}                                   & 61.6      & 56.0      \\

SAM~\cite{yan2020social}                          & 54.3      & 51.5      \\

DFWSGAR~\cite{kim2022detector}                                                          & 75.8     & 71.2     \\
\hline
\textbf{Ours}  &\textbf{82.1} & \textbf{72.8}\\%\textbf{}\\
\hline
\end{tabular}
 
\end{center}
\vspace{-4mm}
\caption{Comparisons with the State-of-the-Art GAR models and video backbones on the NBA dataset \cite{yan2020social}.
}
\vspace{-1.5em}

\label{table:SOTA_NBA}
\vspace{-0.5em}
\end{table}

% \begin{table}[t]
%     % \hfill
%     % \parbox{1.\linewidth}{
%     \footnotesize
%     \centering
%     \setlength{\tabcolsep}{3.5mm}{
%     \begin{tabular}{ccc}
%         \hline
%         Method & Backbone & MPCA \\
%         % \hline
%         \hline
%         HDTM\cite{ibrahim2016hierarchical}  & AlexNet & 89.7  \\
%         PCTDM\cite{yan2018participation} & AlexNet & 92.2  \\
%         CERN-2\cite{shu2017cern} & VGG-16 & 88.3  \\
%         Recurrent\cite{Wang_2017_CVPR} & VGG-16 & 89.4 \\
%         stagNet\cite{qi2018stagnet} & VGG-16 & 89.1  \\
%         SPA+KD\cite{tang2018mining} & VGG-16 & 92.5  \\
%         PRL\cite{hu2020progressive}   & VGG-16 & 93.8  \\
%         CRM\cite{azar2019convolutional} & I3D & 94.2  \\
%         % SPA+KD\cite{tang2018mining} & VGG-16 & 95.7  \\
%         ARG\cite{wu2019learning}   & ResNet-18 & 92.3  \\
%         HiGCIN\cite{yan2020higcin} & ResNet-18 & 93.0  \\
%         DIN\cite{yuan2021spatio} & ResNet-18 & 95.3 \\
%         TCE+STBiP\cite{yuan2021learning} & Inception-v3 & 95.1  \\
%         Dual-AI\cite{han2022dual} & Inception-v3 & \textbf{96.5} \\
%         \hline
%         \multirow{1}[1]{*}{Ours}     & Inception-v3 & \textcolor{red}{95.4} \\
%         % Ours                        & Inception-v3 & \textbf{97.1} \\
%         % \hline
%         % Ours    & ResNet-18 & \checkmark & \textbf{-}  \\
%         \hline
%         \end{tabular}%
%     }
%         \caption{Comparisons with previous state-of-the-art methods on \textbf{Collective Activity datatset}. \textcolor{red}{CE loss}}
%         \vspace{-4mm}
%         \label{tab:collective}%
%     \end{table}
  
\begin{table}[!t]

\vspace{-4mm}
\begin{center}
\begin{tabular}{>{\arraybackslash}m{2.4cm}| >  {\centering\arraybackslash}m{2.3cm} >{\centering\arraybackslash}m{0.9cm}>{\centering\arraybackslash}m{1.0cm}}

\hline
Method                             & Backbone              & MCA   & Merged MCA\\ [0.3ex]
\hline
% \addlinespace[0.5ex]
\multicolumn{4}{c}{\textbf{Fully supervised}} \\
% \addlinespace[1ex]
\hline
SSU~\cite{bagautdinov2017social}                 & Inception-v3          & 89.9  & - \\
PCTDM~\cite{yan2018participation}                & ResNet-18             & 90.3  & 94.3\\ %
StagNet~\cite{qi2018stagnet}                     & VGG-16                & 89.3  & - \\
ARG~\cite{wu2019learning}                        & ResNet-18             & 91.1 & \underline{95.1}\\ %
CRM~\cite{azar2019convolutional}        & I3D                   & 92.1  & - \\
HiGCIN~\cite{yan2020higcin}                      & ResNet-18             & 91.4  & - \\
AT~\cite{gavrilyuk2020actor}                     & ResNet-18             & 90.0 & 94.0\\ %
SACRF~\cite{pramono2020empowering}               & ResNet-18             & 90.7 & 92.7   \\ %
DIN~\cite{yuan2021spatio}                        & ResNet-18             & \underline{93.1}  & \textbf{95.6} \\ %
TCE+STBiP~\cite{yuan2021learning}        & VGG-16                & \textbf{94.1}  & - \\
GroupFormer~\cite{li2021groupformer}                      & Inception-v3             & \textbf{94.1}  & - \\

% \addlinespace[1ex]
\hline
% \addlinespace[0.5ex]
\multicolumn{4}{c}{\textbf{Weakly supervised}} \\
% \addlinespace[1ex]
\hline 
PCTDM~\cite{yan2018participation}               & ResNet-18             & 80.5  & 90.0\\
ARG~\cite{wu2019learning}                       & ResNet-18             & 87.4  & 92.9\\
AT~\cite{gavrilyuk2020actor}                    & ResNet-18             & 84.3  & 89.6\\
SACRF~\cite{pramono2020empowering}              & ResNet-18             & 83.3  & 86.1  \\
DIN~\cite{yuan2021spatio}                       & ResNet-18             & 86.5  & 93.1\\
SAM~\cite{yan2020social}                        & ResNet-18             & 86.3  & 93.1\\

DFWSGAR~\cite{kim2022detector}                                         & ResNet-18             & 90.5  & 94.4\\
\hline
\textbf{Ours} & ViT-Base & \textbf{92.9}& \textbf{95.6} \\
\hline
\end{tabular}

\vspace{-5mm}
\end{center}
\caption{Comparison with the state-of-the-art methods on the Volleyball dataset.~\cite{ibrahim2016hierarchical}}
 \vspace{-1.1em}

\label{table:SOTA_Volleyball}
% \vspace{-1.0em}
\end{table}

\begin{table*}[!t]
\begin{minipage}{.55\textwidth}
	\centering \small
    
	\setlength{\tabcolsep}{8pt}
	\scalebox{1.0}[1.0]{
    % \resizebox{0.48\textwidth}{!}{%
	\begin{tabular}{c|c|c|c|c|c}
		\toprule 
		
		$\bm{l_{t}}\to \bm{g_{t}}$  & $\bm{l_{s}}\to\bm{g_{t}}$  & $\bm{l_{s}}\to\bm{l_{t}}$  & $\bm{g_{t}}\to\bm{l_{t}}$ & NBA     & Volleyball  \\  \midrule
		\cmark   & \xmark   & \xmark   & \xmark  & 61.03   & 62.70 \\ 
		\xmark   & \cmark   & \xmark   & \xmark  & 62.59   & 65.40 \\ 
		\cmark   & \cmark   & \xmark   & \xmark  & \textbf{81.20}   & \textbf{90.80} \\
		\cmark   & \cmark   & \cmark   & \xmark  & 72.11   & 77.62 \\
		\cmark   & \cmark   & \xmark   & \cmark  & 78.17   & 85.88 \\
		\xmark   & \xmark   & \cmark   & \cmark  & 64.36   & 71.87 \\ \bottomrule
	\end{tabular}
    % }
    
	}
 \caption{\textbf{View Correspondences (VC).} The most optimal combination for predicting view correspondences involves predicting local-to-global (temporal) and local-to-global (spatial) views, outperforming other combinations.}
    \label{tbl:ablation_correspondences}
\end{minipage}
\hfill
\begin{minipage}{.42\textwidth}
	\centering\small
 \vspace{-1.0em}
 
	\setlength{\tabcolsep}{7pt}
	\scalebox{1.0}[1.0]{
	\begin{tabular}{c|c|c|c}
		\toprule
		 
		Spatial  & Temporal & NBA     & Volleyball \\ \midrule
		\cmark   & \xmark   & 69.38   & 78.59   \\ 
		\xmark   & \cmark   & 72.90   & 81.45   \\ 
		\cmark   & \cmark   & \textbf{81.20}   & \textbf{90.80}   \\ \bottomrule
	\end{tabular}}
\caption{\textbf{Spatial vs Temporal variations.}  The best results are achieved by utilizing cross-view correspondences with varying fields of view along both spatial and temporal dimensions. It is observed that temporal variations between views have a greater impact on performance compared to applying only spatial variation.}
    \label{tbl:ablation_st}
\end{minipage}
\vspace{-0.18in}
\end{table*}


\begin{table}[!t]
\begin{center}


\setlength{\tabcolsep}{5pt}
	\scalebox{1.0}[1.0]{
	\begin{tabular}{c|c|c}
		\toprule
		
		   Method   & NBA     & Volleyball  \\  \midrule
		Ours + TIS \cite{qian2020spatiotemporal} & 78.45   & 88.11 \\ 

		Ours + MC & \textbf{81.20}   & \textbf{90.80} \\ \bottomrule
	\end{tabular}}
 \caption{\textbf{Temporal Sampling Strategy 
    }. We evaluate the effectiveness of our proposed temporal sampling strategy, called "\emph{motion correspondences (MC)}" (\cref{subsec:mo_pred}), by comparing it with an alternate approach, the "temporal interval sampler (TIS)" \cite{qian2020spatiotemporal}, used with CNNs under contrastive settings. 
    }
    \label{tbl:ablation_temporal}

\end{center}

\vspace{-1.0em}
% \vspace{-1.0em}
\end{table}

\begin{table}[!t]
\begin{center}

\setlength{\tabcolsep}{5pt}
	\scalebox{0.95}[0.95]{
	\begin{tabular}{c|c|c}
		\toprule
		       Patch size   & NBA     & Volleyball  \\  \midrule
		8 & 78.71   & 87.10 \\
        16 & \textbf{81.20}   & \textbf{90.80} \\
	    32    & 72.56   & 79.21 \\ \bottomrule
	\end{tabular}}
 \caption{\textbf{Spatial Augmentations (SA)}: Applying different patch sizes randomly over the spatial dimensions for different views leads to consistent improvements on both NBA and Volleyball datasets. 
}
    \label{tbl:ablation_aug}

\end{center}
\vspace{-1.0em}

\vspace{-1.0em}
\end{table}

\begin{table}[!t]
\begin{center}

\setlength{\tabcolsep}{5pt}
	\scalebox{1.0}[1.0]{
	\begin{tabular}{c|c|c}
		\toprule
		
		Multi-view      & NBA    & Volleyball  \\  \midrule
		\xmark      & 76.17   & 88.35 \\
		\cmark      & \textbf{81.20}   & \textbf{90.80} \\ \bottomrule
	\end{tabular}}
 \caption{\textbf{Inference}: Providing multiple views of different spatiotemporal resolutions to a shared network (multiview) leads to noticeable performance improvements compared to using a single view for both the NBA and Volleyball datasets.}
    \label{tbl:ablation_inf}

\end{center}
\vspace{-2.1em}

% \vspace{-2.0em}
\end{table}

\subsection{Comparison with state-of-the-art methods}

\noindent\textbf{NBA dataset}
We compare our approach to the state-of-the-art in GAR and WSGAR, which leverage bounding box recommendations produced by SAM~\cite{yan2020social}, as well as to current video backbones in the weakly supervised learning environment, using the NBA dataset.
We exclusively utilise RGB frames as input for each approach, including the video backbones, to ensure a fair comparison.
Table~\ref{table:SOTA_NBA} lists the findings.
Please take note that the results of SAM~\cite{yan2020social} has been listed from~\cite{kim2022detector}.
With 6.3\%p of MCA and 1.6\%p of MPCA, the proposed method outperforms existing GAR and WSGAR methods by a significant margin.
Additionally, our approach is contrasted with two current video backbones utilised in traditional action detection, ResNet-18 TSM~\cite{lin2019tsm} and VideoSwin-T~\cite{liu2021video}. These strong backbones perform admirably in WSGAR, but ours is the finest.

\noindent
\textbf{Volleyball dataset.}
For the volleyball dataset, we compare our approach to the most recent GAR and WSGAR approaches in two different supervision levels: fully supervised and weakly supervised.
The usage of actor-level labels, such as individual action class labels and ground-truth bounding boxes, in training and inference differs across the two settings.
To have a fair comparison, all the results reported are using only RGB input and ResNet-18 backbone.
% For a fair comparison, we report the results of previous methods~\cite{azar2019convolutional, yuan2021learning, li2021groupformer, yan2020higcin, bagautdinov2017social, qi2018stagnet} using only the RGB input, and the results~\cite{yan2018participation, wu2019learning, gavrilyuk2020actor, pramono2020empowering, yuan2021spatio} using the ResNet-18 backbone. 
Please consider the note that first is from the original papers, and the second is the MCA values of~\cite{yuan2021spatio}.
We eliminate the individual action classification head and substitute an object detector trained on an external dataset for the ground-truth bounding boxes in the weakly supervised situation.
Table~\ref{table:SOTA_Volleyball} presents the results.
Results from earlier techniques in fully supervised and weakly supervised environments are displayed in the first and second sections, respectively.
In weakly supervised conditions, our technique significantly outperforms all GAR and WSGAR models, outperforming them by 2.4\% of MCA and 1.2\% of Merged MCA when compared to the models' utilising ViT-Base backbone.
% It also beats the current state-of-the-art based on Inception-v3.
Our technique outperforms current GAR methods, such as~\cite{bagautdinov2017social, yan2018participation, qi2018stagnet, gavrilyuk2020actor, pramono2020empowering}, by employing more thorough actor-level supervision.
% \vspace{-3mm}
\subsection{Ablation Study}
% We systematically dissect the contribution of each component of our method. 
% We study the effect of five individual elements: 
% \textbf{a)} different combinations of local and global view correspondences;
% \textbf{b)} varying field of view along temporal vs. spatial dimensions;
% \textbf{c)} temporal sampling strategy;
% \textbf{d)} spatial augmentations;
% \textbf{e)} Inference
We perform a comprehensive analysis of the different components that contribute to the effectiveness of our method. Specifically, we evaluate the impact of five individual elements:
\textbf{a)} various combinations of local and global view correspondences;
\textbf{b)} different field of view variations along the temporal and spatial dimensions;
\textbf{c)} the choice of temporal sampling strategy;
\textbf{d)} the use of spatial augmentations;
and \textbf{e)} the inference approach.

\noindent\textbf{View Correspondences}:
% Learning correspondences between local and global views is the fundamental motivation behind our proposed cross-view correspondences (VC). Since multiple combinations of local-global views can be considered for matching and prediction between views, we explore the effect of predicting each type of view from the other in Table~\ref{tbl:ablation_correspondences}. We observe that jointly predicting $\bm{l_{t}}\to \bm{g_{t}}$  and $\bm{l_{s}}\to\bm{g_{t}}$ view correspondences results in optimal performance while predicting $\bm{g_{t}}\to \bm{l_{t}}$ or $\bm{l_{s}}\to\bm{l_{t}}$ views leads to reduced performance. We believe this trend exists due to the emphasis on learning rich context in the case of joint prediction, which is absent for individual cases. 
% {Furthermore, the performance drop for $\bm{l_{s}}\to\bm{l_{t}}$ correspondences (no overlap views) is consistent with previous findings on the effectiveness of temporally closer positive views for contrastive self-supervised losses \cite{qian2020spatiotemporal, Feichtenhofer_large}.}
We propose cross-view correspondences (VC) to learn correspondences between local and global views. To investigate the effect of predicting each type of view from the other, we conduct experiments presented in Table~\ref{tbl:ablation_correspondences}. Our results show that jointly predicting $\bm{l_{t}}\to \bm{g_{t}}$ and $\bm{l_{s}}\to\bm{g_{t}}$ view correspondences leads to optimal performance. However, predicting $\bm{g_{t}}\to \bm{l_{t}}$ or $\bm{l_{s}}\to\bm{l_{t}}$ views results in reduced performance, possibly because joint prediction emphasizes learning rich context, which is absent for individual cases. We also observe a consistent performance drop for $\bm{l_{s}}\to\bm{l_{t}}$ correspondences (no overlap views), consistent with previous findings on the effectiveness of temporally closer positive views for contrastive self-supervised losses \cite{qian2020spatiotemporal, Feichtenhofer_large}.

\vspace{0.1em}
\noindent\textbf{Spatial vs. Temporal Field of View}: 
We determine the optimal combination of spatio-temporal views in Table~\ref{tbl:ablation_correspondences} by varying the field of view (crops) along both spatial and temporal dimensions (as described in Sec.\ref{subsec:cross_view_corrspondences}). To evaluate the effects of variations along these dimensions, we conduct experiments as presented in Table\ref{tbl:ablation_st}. Specifically, we compare the performance of our approach with no variation along the spatial dimension (where all frames have a fixed spatial resolution of $224\times 224$ with no spatial cropping) and with no variation along the temporal dimension (where all frames in our views are sampled from a fixed time-axis region of a video). Our findings show that temporal variations have a significant impact on NBA, while variations in the field of view along both spatial and temporal dimensions lead to the best performance (as shown in Table~\ref{tbl:ablation_st}).

\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=0.70\textwidth]{gar_figures/vis1_v2.png}
    % \put(0,190){(a)}
% \put(0,115){(b)}
% \put(0,40){(c)}
    \vspace{-2mm}
    \caption{Visualization of the Transformer attention maps for NBA dataset. (top) Original sequence from NBA dataset~\cite{yan2020social}, (middle) Attention maps from DFWSGAR~\cite{kim2022detector} and (bottom) Attention maps from our SPARTAN model.}
     % \vspace{0.1in}
    \label{fig:vis}
\end{figure*}

\begin{figure*}[!htbp]
\begin{center}
\includegraphics[width=0.9\linewidth]{gar_figures/tsne.png}
\put(-465, 100){Base model}
% \put(-380, 110){View Correspondences}
\put(-340, 100){VC}
% \put(-270, 110){Motion Correspondences}
\put(-240, 100){MC + VC}
\put(-140, 100){MC + VC + SA}
\end{center}
\vspace{-1.5em}
\caption{Visualization of the $t$-SNE~\cite{van2008visualizing} plots of embedding features learned by different modules of our SPARTAN model for the NBA dataset. }
% \vspace{-0.2in}
\label{fig:tsne}
\end{figure*}

\vspace{0.1em}
\noindent\textbf{Temporal Sampling Strategy}:
% We study how our proposed temporal sampling strategy for motion correspondences (MC) could be replaced with alternate sampling approaches. To verify the effectiveness of MC, we replace it within SPARTAN with an alternate method. The temporal interval sampling (TIS) strategy in \cite{qian2020spatiotemporal} obtains state-of-the-art performance in its self-supervised contrastive video environment with CNN backbones. Our experiments incorporating TIS in SPARTAN (Table \ref{tbl:ablation_temporal}) highlight the advantage of our proposed MC sampling strategy over TIS.  
Our investigation examines the possibility of replacing the temporal sampling strategy for motion correspondences (MC) proposed in our study with alternate sampling methods. To evaluate the effectiveness of MC, we replace it with an alternative approach within SPARTAN. Specifically, we test the temporal interval sampling (TIS) strategy introduced in \cite{qian2020spatiotemporal}, which has achieved state-of-the-art performance in self-supervised contrastive video settings with CNN backbones. Our experiments incorporating TIS in SPARTAN (Table \ref{tbl:ablation_temporal}) demonstrate that our proposed MC sampling strategy offers superior performance compared to TIS.
\vspace{0.1em}

\noindent\textbf{Spatial Augmentations}: 
% Next, we explore standard spatial augmentations (SA) used on videos by varying the patch size. Different patch sizes lead to improvements in various CNN-based video self-supervision approaches. We evaluate its effect on our approach in Table~\ref{tbl:ablation_aug}, which shows better improvements with a patch size of 16. Given these performance gains, we also adopt the patch size of 16 in our SPARTAN training process.
We then investigate the impact of standard spatial augmentations (SA) on video data by experimenting with different patch sizes. Previous studies have shown that varying patch sizes can enhance the performance of CNN-based video self-supervision approaches. In our study, we evaluate the effect of patch size on our approach and present the results in Table~\ref{tbl:ablation_aug}, indicating that a patch size of 16 yields the best improvements. Based on these findings, we incorporate a patch size of 16 in our SPARTAN training process.

\vspace{0.1em}
\noindent\textbf{Inference}: 
% Finally, we study the effect of our proposed inference (\cref{subsec:Inference}) in Table~\ref{tbl:ablation_inf}. We observe higher gains on  NBA~\cite{yan2020social} and Volleyball~\cite{ibrahim2016hierarchical} datasets, where the classes are easier to separate with motion information~\cite{han2020self}. 
To assess the impact of our proposed inference method (\cref{subsec:Inference}), we analyze the results presented in Table~\ref{tbl:ablation_inf}. Our findings demonstrate that our approach yields greater improvements on the NBA~\cite{yan2020social} and Volleyball~\cite{ibrahim2016hierarchical} datasets, which contain classes that can be more easily distinguished using motion information~\cite{han2020self}.
% \vspace{-2mm}
\subsection{Qualitative Results}
For the better interpretation of the results, on the NBA dataset, we visualise the attentions of the last layer of the transformer encoder in Fig.~\ref{fig:vis}. These results clearly represent the learning capability of the model towards the essential concepts, such as the position of the players along with the overall group activity classification. To demonstrate the efficiency of each module, in Fig.~\ref{fig:tsne}, we visualise the t-SNE plots. From that, we conclude that all modules combined to provide a clear seperation for each class.

% In Fig.~\ref{fig:vis}, we present the attention visualizations obtained from the final Transformer encoder layer using the NBA dataset. The outcomes demonstrate the model's proficiency in allocating attention to crucial concepts, including players' positions, and adeptly tracking activities within specific video clips.

% Moreover, in Fig.\ref{fig:tsne}, we showcase the results of the $t$-SNE\cite{van2008visualizing} visualization for our model and its variations. The two-dimensional spatial representation illustrates the final group representations of each model on the NBA dataset. Notably, the suggested modules contribute to a clear demarcation between individual classes.
% \vspace{-4mm}
\section{Conclusion}
\label{sec:conclusion}
% \vspace{-3mm}
Our work introduces SPARTAN, a self-supervised video transformer-based model. The approach involves generating multiple spatio-temporally varying views from a single video at different scales and frame rates. Two sets of correspondence learning tasks are then defined to capture the motion properties and cross-view relationships between the sampled clips. The self-supervised objective involves reconstructing one view from the other in the latent space of teacher and student networks. Moreover, our SPARTAN can model long-range spatio-temporal dependencies and perform dynamic inference within a single architecture. We evaluate SPARTAN on two group activity recognition benchmarks and find that it outperforms the current state-of-the-art models.

\noindent\textbf{Limitations:}
Our paper investigates the application of SPARTAN in the context of the RGB input modality. Currently, we do not utilize the additional supervision provided by alternate modalities in large-scale multimodal video datasets. However, in future work, we plan to explore ways in which we can modify SPARTAN to take advantage of multimodal data sources.

\small{
\noindent
\textbf{Acknowledgment} 
This work is supported by Arkansas Biosciences Institute (ABI) Grant, NSF WVAR-CRESH and NSF Data Science, Data Analytics that are Robust and Trusted (DART).
}
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
% \bibliography{egbib}
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{amer2014hirf}
Mohamed~Rabie Amer, Peng Lei, and Sinisa Todorovic.
\newblock Hirf: Hierarchical random field for collective activity recognition
  in videos.
\newblock In {\em European Conference on Computer Vision}, pages 572--585.
  Springer, 2014.

\bibitem{amer2015sum}
Mohamed~R Amer and Sinisa Todorovic.
\newblock Sum product networks for activity recognition.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  38(4):800--813, 2015.

\bibitem{amer2013monte}
Mohamed~R Amer, Sinisa Todorovic, Alan Fern, and Song-Chun Zhu.
\newblock Monte carlo tree search for scheduling activity recognition.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1353--1360, 2013.

\bibitem{amer2012cost}
Mohamed~R Amer, Dan Xie, Mingtian Zhao, Sinisa Todorovic, and Song-Chun Zhu.
\newblock Cost-sensitive top-down/bottom-up inference for multiscale activity
  recognition.
\newblock In {\em European Conference on Computer Vision}, pages 187--200.
  Springer, 2012.

\bibitem{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock {\em arXiv preprint arXiv:2103.15691}, 2021.

\bibitem{azar2019convolutional}
Sina~Mokhtarzadeh Azar, Mina~Ghadimi Atigh, Ahmad Nickabadi, and Alexandre
  Alahi.
\newblock Convolutional relational machine for group activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7892--7901, 2019.

\bibitem{bagautdinov2017social}
Timur Bagautdinov, Alexandre Alahi, Fran{\c{c}}ois Fleuret, Pascal Fua, and
  Silvio Savarese.
\newblock Social scene understanding: End-to-end multi-person action
  localization and collective activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4315--4324, 2017.

\bibitem{gberta_2021_ICML}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In {\em International Conference on Machine Learning,}, July 2021.

\bibitem{bertasius2021space}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In {\em International Conference on Machine Learning,}, 2021.

\bibitem{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\'e J\'egou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, 2021.

\bibitem{carreira2017quo}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6299--6308, 2017.

\bibitem{chen2021mocov3}
Xinlei Chen*, Saining Xie*, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock {\em arXiv}, 2021.

\bibitem{deng2016structure}
Zhiwei Deng, Arash Vahdat, Hexiang Hu, and Greg Mori.
\newblock Structure inference machines: Recurrent neural networks for analyzing
  relations in group activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4772--4781, 2016.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{ehsanpour2020joint}
Mahsa Ehsanpour, Alireza Abedin, Fatemeh Saleh, Javen Shi, Ian Reid, and Hamid
  Rezatofighi.
\newblock Joint learning of social groups, individuals action and sub-group
  activities in videos.
\newblock In {\em European Conference on Computer Vision}, pages 177--195.
  Springer, 2020.

\bibitem{fan2021multiscale}
Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock {\em arXiv preprint arXiv:2104.11227}, 2021.

\bibitem{feichtenhofer2019slowfast}
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
\newblock Slowfast networks for video recognition.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 6202--6211, 2019.

\bibitem{Feichtenhofer_large}
Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He.
\newblock A large-scale study on unsupervised spatiotemporal representation
  learning.
\newblock {\em arXiv}, 2021.

\bibitem{gavrilyuk2020actor}
Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, and Cees~GM Snoek.
\newblock Actor-transformers for group activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 839--848, 2020.

\bibitem{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock In {\em Advances in neural information processing systems}, 2020.

\bibitem{han2020mining}
Mingfei Han, Yali Wang, Xiaojun Chang, and Yu Qiao.
\newblock Mining inter-video proposal relations for video object detection.
\newblock In {\em European conference on computer vision}, pages 431--446.
  Springer, 2020.

\bibitem{han2022dual}
Mingfei Han, David~Junhao Zhang, Yali Wang, Rui Yan, Lina Yao, Xiaojun Chang,
  and Yu Qiao.
\newblock Dual-ai: Dual-path actor interaction learning for group activity
  recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2990--2999, 2022.

\bibitem{han2020self}
Tengda Han, Weidi Xie, and Andrew Zisserman.
\newblock Self-supervised co-training for video representation learning.
\newblock {\em Advances in neural information processing systems}, 2020.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2961--2969, 2017.

\bibitem{hu2020progressive}
Guyue Hu, Bo Cui, Yuan He, and Shan Yu.
\newblock Progressive relation learning for group activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 980--989, 2020.

\bibitem{ibrahim2018hierarchical}
Mostafa~S Ibrahim and Greg Mori.
\newblock Hierarchical relational networks for group activity recognition and
  retrieval.
\newblock In {\em European Conference on Computer Vision}, pages 721--736,
  2018.

\bibitem{ibrahim2016hierarchical}
Mostafa~S Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, and Greg
  Mori.
\newblock A hierarchical deep temporal model for group activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1971--1980, 2016.

\bibitem{kahatapitiya2021coarse}
Kumara Kahatapitiya and Michael~S Ryoo.
\newblock Coarse-fine networks for temporal activity detection in videos.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2021.

\bibitem{kim2022detector}
Dongkeun Kim, Jinsung Lee, Minsu Cho, and Suha Kwak.
\newblock Detector-free weakly supervised group activity recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 20083--20093, 2022.

\bibitem{kingma15adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em Int. Conf. Learn. Represent.}, 2015.

\bibitem{lan2012social}
Tian Lan, Leonid Sigal, and Greg Mori.
\newblock Social roles in hierarchical models for human activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1354--1361. IEEE, 2012.

\bibitem{lan2011discriminative}
Tian Lan, Yang Wang, Weilong Yang, Stephen~N Robinovitch, and Greg Mori.
\newblock Discriminative latent models for recognizing contextual group
  activities.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  34(8):1549--1562, 2011.

\bibitem{9895423}
Yunjia Lei, Son~Lam Phung, Abdesselam Bouzerdoum, Hoang Thanh~Le, and Khoa Luu.
\newblock Pedestrian lane detection for assistive navigation of vision-impaired
  people: Survey and experimental evaluation.
\newblock {\em IEEE Access}, 10:101071--101089, 2022.

\bibitem{li2022uniformer}
Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng
  Li, and Yu Qiao.
\newblock Uniformer: Unifying convolution and self-attention for visual
  recognition, 2022.

\bibitem{li2021ffa}
Mingjie Li, Wenjia Cai, Rui Liu, Yuetian Weng, Xiaoyun Zhao, Cong Wang, Xin
  Chen, Zhong Liu, Caineng Pan, Mengke Li, et~al.
\newblock Ffa-ir: Towards an explainable and reliable medical report generation
  benchmark.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\bibitem{li2021groupformer}
Shuaicheng Li, Qianggang Cao, Lingbo Liu, Kunlin Yang, Shinan Liu, Jun Hou, and
  Shuai Yi.
\newblock Groupformer: Group activity recognition with clustered
  spatial-temporal transformer.
\newblock {\em Proceedings of the IEEE international conference on computer
  vision}, 2021.

\bibitem{li2017sbgar}
Xin Li and Mooi Choo~Chuah.
\newblock Sbgar: Semantics based group activity recognition.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2876--2885, 2017.

\bibitem{lin2019tsm}
Ji Lin, Chuang Gan, and Song Han.
\newblock Tsm: Temporal shift module for efficient video understanding.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7083--7093, 2019.

\bibitem{liu2021swin}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, 2021.

\bibitem{liu2021video}
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
\newblock Video swin transformer.
\newblock {\em arXiv}, 2021.

\bibitem{naseer2021intriguing}
Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad~Shahbaz
  Khan, and Ming-Hsuan Yang.
\newblock Intriguing properties of vision transformers.
\newblock {\em arXiv}, 2021.

\bibitem{9897440}
Pha Nguyen, Thanh-Dat Truong, Miaoqing Huang, Yi Liang, Ngan Le, and Khoa Luu.
\newblock Self-supervised domain adaptation in crowd counting.
\newblock In {\em 2022 IEEE International Conference on Image Processing
  (ICIP)}, pages 2786--2790, 2022.

\bibitem{patrick2021keeping}
Mandela Patrick, Dylan Campbell, Yuki~M Asano, Ishan Misra~Florian Metze,
  Christoph Feichtenhofer, Andrea Vedaldi, Jo Henriques, et~al.
\newblock Keeping your eye on the ball: Trajectory attention in video
  transformers.
\newblock In {\em NeurIPS}, 2021.

\bibitem{pramono2020empowering}
Rizard Renanda~Adhi Pramono, Yie~Tarng Chen, and Wen~Hsien Fang.
\newblock Empowering relational network by self-attention augmented conditional
  random fields for group activity recognition.
\newblock In {\em European Conference on Computer Vision}, pages 71--90.
  Springer, 2020.

\bibitem{qi2018stagnet}
Mengshi Qi, Jie Qin, Annan Li, Yunhong Wang, Jiebo Luo, and Luc Van~Gool.
\newblock stagnet: An attentive semantic rnn for group activity recognition.
\newblock In {\em European Conference on Computer Vision}, pages 101--117,
  2018.

\bibitem{qian2020spatiotemporal}
Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge
  Belongie, and Yin Cui.
\newblock Spatiotemporal contrastive video representation learning.
\newblock {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2021.

\bibitem{quach2022depth}
Kha~Gia Quach, Huu Le, Pha Nguyen, Chi~Nhan Duong, Tien~Dai Bui, and Khoa Luu.
\newblock Depth perspective-aware multiple object tracking.
\newblock {\em arXiv preprint arXiv:2207.04551}, 2022.

\bibitem{quach2022non}
Kha~Gia Quach, Ngan Le, Chi~Nhan Duong, Ibsa Jalata, Kaushik Roy, and Khoa Luu.
\newblock Non-volume preserving-based fusion to group-level emotion recognition
  on crowd videos.
\newblock {\em Pattern Recognition}, Volume 128:Article 108646, August, 2022.

\bibitem{Quach_2021_CVPR}
Kha~Gia Quach, Pha Nguyen, Huu Le, Thanh-Dat Truong, Chi~Nhan Duong, Minh-Triet
  Tran, and Khoa Luu.
\newblock Dyglip: A dynamic graph model with link prediction for accurate
  multi-camera multiple object tracking.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 13784--13793, June 2021.

\bibitem{Ranasinghe_2022_CVPR}
Kanchana Ranasinghe, Muzammal Naseer, Salman Khan, Fahad~Shahbaz Khan, and
  Michael~S. Ryoo.
\newblock Self-supervised video transformer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 2874--2884, June 2022.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In {\em NIPS}, pages 91--99, 2015.

\bibitem{imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li Fei-Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em Int. J. Comput. Vis.}, 2015.

\bibitem{shu2015joint}
Tianmin Shu, Dan Xie, Brandon Rothrock, Sinisa Todorovic, and Song Chun~Zhu.
\newblock Joint inference of groups, events and human roles in aerial videos.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4576--4584, 2015.

\bibitem{shu2019hierarchical}
Xiangbo Shu, Jinhui Tang, Guojun Qi, Wei Liu, and Jian Yang.
\newblock Hierarchical long short-term concurrent memory for human interaction
  recognition.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  2019.

\bibitem{Steiner2021HowTT}
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob
  Uszkoreit, and Lucas Beyer.
\newblock How to train your vit? data, augmentation, and regularization in
  vision transformers.
\newblock {\em arXiv}, 2021.

\bibitem{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of machine learning research}, 9(11), 2008.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NIPS}, pages 5998--6008, 2017.

\bibitem{wang2016temporal}
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc
  Van~Gool.
\newblock Temporal segment networks: Towards good practices for deep action
  recognition.
\newblock In {\em European Conference on Computer Vision}, pages 20--36.
  Springer, 2016.

\bibitem{wang2017recurrent}
Minsi Wang, Bingbing Ni, and Xiaokang Yang.
\newblock Recurrent modeling of interaction context for collective activity
  recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3048--3056, 2017.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock {\em arXiv preprint arXiv:2102.12122}, 2021.

\bibitem{wang2018non}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7794--7803, 2018.

\bibitem{wang2013bilinear}
Zhenhua Wang, Qinfeng Shi, Chunhua Shen, and Anton Van Den~Hengel.
\newblock Bilinear programming for human activity recognition with unknown mrf
  graphs.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1690--1697, 2013.

\bibitem{wu2019learning}
Jianchao Wu, Limin Wang, Li Wang, Jie Guo, and Gangshan Wu.
\newblock Learning actor relation graphs for group activity recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 9964--9974, 2019.

\bibitem{xiao2021modist}
Fanyi Xiao, Joseph Tighe, and Davide Modolo.
\newblock Modist: Motion distillation for self-supervised video representation
  learning.
\newblock {\em arXiv}, 2021.

\bibitem{yan2018participation}
Rui Yan, Jinhui Tang, Xiangbo Shu, Zechao Li, and Qi Tian.
\newblock Participation-contributed temporal dynamic model for group activity
  recognition.
\newblock In {\em Proceedings of the 26th ACM international conference on
  Multimedia}, pages 1292--1300, 2018.

\bibitem{yan2020higcin}
Rui Yan, Lingxi Xie, Jinhui Tang, Xiangbo Shu, and Qi Tian.
\newblock Higcin: hierarchical graph-based cross inference network for group
  activity recognition.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  2020.

\bibitem{yan2020social}
Rui Yan, Lingxi Xie, Jinhui Tang, Xiangbo Shu, and Qi Tian.
\newblock Social adaptive module for weakly-supervised group activity
  recognition.
\newblock In {\em European Conference on Computer Vision}, pages 208--224.
  Springer, 2020.

\bibitem{yuan2021learning}
Hangjie Yuan and Dong Ni.
\newblock Learning visual context for group activity recognition.
\newblock In {\em AAAI}, volume~35, pages 3261--3269, 2021.

\bibitem{yuan2021spatio}
Hangjie Yuan, Dong Ni, and Mang Wang.
\newblock Spatio-temporal dynamic inference network for group activity
  recognition.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, 2021.

\bibitem{yuan2021tokens}
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis~EH
  Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock {\em arXiv preprint arXiv:2101.11986}, 2021.

\bibitem{zhang2019fast}
Peizhen Zhang, Yongyi Tang, Jian-Fang Hu, and Wei-Shi Zheng.
\newblock Fast collective activity recognition under weak supervision.
\newblock {\em IEEE Transactions on Image Processing}, 29:29--43, 2019.

\bibitem{zhang2021multi}
Yanyi Zhang, Xinyu Li, and Ivan Marsic.
\newblock Multi-label activity recognition using activity-specific features and
  activity correlations.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14625--14635, 2021.

\end{thebibliography}

}
\end{document}
