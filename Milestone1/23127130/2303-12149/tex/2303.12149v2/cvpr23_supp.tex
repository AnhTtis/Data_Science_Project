% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage{epsfig}
\usepackage[inline]{enumitem}
% \usepackage[caption=false]{subfig}
\usepackage{soul}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{numprint}
\usepackage{siunitx}
\usepackage{etoolbox}% <-- for bold fonts
\usepackage{cancel}
\newcommand{\ubold}{\fontseries{b}\selectfont}% <-- for bold fonts
\robustify\ubold% <-- for bold fonts
\usepackage{bbold}
\usepackage{wrapfig}
\usepackage[skip=3pt]{subcaption}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{lineno}
\usepackage{array,multirow,graphicx}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 2pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1.5pt\hskip\tabcolsep}}
\makeatother

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2140} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
%\title{Self-Supervised cross-domain Multiple Object Tracking}

\title{Supplementary Material \\
SPARTAN: Self-supervised Spatiotemporal Transformers Approach to \\ Group Action Recognition}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

\section{Experimental details}



\subsection{Implementation of reproduction}

\noindent
\textbf{NBA dataset. }
We reproduce GAR~\cite{wu2019learning, gavrilyuk2020actor, pramono2020empowering, yuan2021spatio} and WSGAR~\cite{yan2020social,kim2022detector} methods following the official code of DIN\footnote{\label{note1}Original DIN codes are available at \url{https://github.com/JacobYuan7/DIN_GAR}.}~\cite{yuan2021spatio}, DFWSGAR\footnote{\label{note2}Original DFWSGAR codes are available at \url{https://github.com/dk-kim/DFWSGAR}.}~\cite{kim2022detector} and the implementation description illustrated in its original paper, repectively. 
For a fair comparison, segment-based sampling~\cite{wang2016temporal}, batch size of 4, the number of bounding boxes $N=12$, and the number of frames $T=18$ are applied for all methods.
The only difference from the implementation of DIN is that all methods are trained in an end-to-end manner. 
Unless specified, all other hyperparameters are identical to those of the code from DIN.
We provide more implementation details of each model below. 

\begin{itemize}
\itemsep=-0.5mm
    \item \textbf{ARG}~\cite{wu2019learning}
    ResNet-18 backbone replaces the original backbone of Inception-v3. 
    \item \textbf{AT}~\cite{gavrilyuk2020actor}
    A single RGB branch is utilized and ResNet-18 backbone replaces the backbone of I3D and HRNet. 
    \item \textbf{SACRF}~\cite{pramono2020empowering}
    The backbone is replaced to ResNet-18, and its multiple modalities are substituted to single RGB input. 
    Since NBA dataset does not have individual action labels, we remove the unary energy term. 
    \item \textbf{DIN}~\cite{yuan2021spatio}
    The experiment is conducted following its official implementation. 
    \item \textbf{SAM}~\cite{yan2020social}
    The number of proposals ($N^p$) and the number of selected proposals ($K^p$) are set to 14 and 8, respectively.
    \item \textbf{DFWSGAR}~\cite{kim2022detector} is conducted following its official implementation.
\end{itemize}

\noindent
\textbf{Volleyball dataset.}
Reproduction of the following models is also based on the code of DIN\textsuperscript{\ref{note1}}~\cite{yuan2021spatio}.
Each reproduction first goes through backbone training process regarding the number of categories, then proceeds to train each inference module afterward.
Note that MCA values of the following models under fully supervised setting are brought from DIN~\cite{yuan2021spatio}, hence we provide implementation detail of experiments under \textbf{A)} fully supervised setting with the aim of classifying actions into 6 (merged) labels, \textbf{B)} weakly supervised setting/8 labels, and \textbf{C)} weakly supervised setting/6 labels.
In weakly supervised setting, actor bounding boxes are replaced to proposal boxes generated by Faster R-CNN~\cite{girshick2015fast} pretrained on COCO dataset~\cite{lin2014microsoft} and individual action annotations are eliminated.
In general, we use a batch size of 2 and the number of frames $T = 10$ for the following work.
Unless mentioned, other hyperparameters are set based on the code provided by DIN.
Followings are further implementation details of each model.

\begin{itemize}
\itemsep=-0.5mm
\item \textbf{PCTDM}~\cite{yan2018participation}
ResNet-18 is applied instead of AlexNet, and RoIAlign features replace cropped/resized individual images of the original paper. 
Furthermore, weight decay rate of $1 \times 10^{-4}$ is applied to \textbf{C)}.
\item \textbf{ARG}~\cite{wu2019learning} 
Likewise, its backbone is changed to ResNet-18. Unlike its original setting, the backbone training is allowed in the model training process for a fair comparison with other models.

\item \textbf{AT}~\cite{gavrilyuk2020actor}
A single RGB branch is utilized and ResNet-18 backbone replaces the backbone of I3D and HRNet.

\item \textbf{SACRF}~\cite{pramono2020empowering}
The backbone is replaced to ResNet-18, and its multiple modalities are substituted to single RGB input.
Due to the removal of individual action labels, the unary energy term is removed.

\item \textbf{DIN}~\cite{yuan2021spatio}
The experiment is conducted following its official implementation.

\item \textbf{DFWSGAR}~\cite{kim2022detector} is conducted following its official implementation.

\end{itemize}

\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=0.80\textwidth]{gar_figures/vis_supp.png}
    % \put(0,190){(a)}
% \put(0,115){(b)}
% \put(0,40){(c)}
    \vspace{-2mm}
    \caption{Visualization of the Transformer attention maps for Volleyball dataset. (top) Original sequence from Volleyball dataset~\cite{ibrahim2016hierarchical}, (bottom) Attention maps from our SPARTAN model.}
     % \vspace{0.1in}
    \label{fig:vis_supp}
\end{figure*}

SAM~\cite{yan2020social} is reproduced following the method described in the original paper.
The major difference with DIN-based reproductions is that it occupies a batch size of 8, a dropout rate of 0.1, and $T=3$ frames. 

\begin{itemize}
\item \textbf{SAM}~\cite{yan2020social}
Note that SAM itself is a WSGAR work, so \textbf{A)} is disregarded. Since \textbf{C)} is already conducted in the original paper, we only reproduce \textbf{B)}. The number of proposals ($N^p$) and the number of selected proposals ($K^p$) are set to 16 and 12, respectively. 
\end{itemize}




\subsection{Implementation of video backbones}
We reproduce recent video backbones, ResNet-18 TSM~\cite{lin2019tsm} and VideoSwin-T~\cite{liu2021video} following the official codes. 
For a fair comparison, sampling strategy and training details are the same as ours. 




\section{More ablation studies}

In this section, we provide additional ablation on NBA and Volleyball datasets. 
\noindent
\textbf{Effects of the different frame rates.}
Table~\ref{tbl:ablation_frame} summarizes the performance according to different frame rates for NBA and Volleyball. 
Note that we do not utilize zero-padding in this experiment and varying the frame rate of one view (global or local), we fix the other one constant(if global views are varying we fix the local views and vice versa).  


\begin{table}[!t]
\begin{center}
\caption{\textbf{Varying Frame Rate}: Using different frame rates applied randomly over the different views results in consistent improvements on NBA and Volleyball datasets. 
}
\setlength{\tabcolsep}{5pt}
	\scalebox{0.95}[0.95]{
	\begin{tabular}{c|c}
		\toprule
		       Frame Rate   & NBA     \\  \midrule
		6 & 71.27 \\
        10 & 73.42 \\
        14 & 77.31   \\
	    18    & \textbf{81.20}  \\ \midrule
            & Volleyball \\
            \midrule
        	3 & 70.17 \\
        5 & \textbf{80.97}   \\
	    8   & 78.85  \\ \bottomrule
               
	\end{tabular}}
    \label{tbl:ablation_frame}

\end{center}
\end{table}

\section{Qualitative Results}
\cref{fig:vis_supp} shows the visualizations on Volleyball dataset using our SPARTAN model. For  more elaborated qualitative results, please refer to the attached video file.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}
