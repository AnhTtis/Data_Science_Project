
\section{Prompt Engineering}

While most GEC systems have used non-autoregressive architectures such as BERT~\cite{devlin2019bert} and RoBERTa~\cite{liu2019roberta}, GPT-3 is an autoregressive LLM with a natural language text prompt as context.
Given a  prompt $c$ and input sentence $x$, which is tokenized as $(w_1, w_2, \dots w_T)$, GPT-3 generates a text sequence $\tilde{y}$ based on the following log likelihood:
\begin{align*}
\log p_{\theta}(x, c) = \sum_{t=1}^{T} \log p_{\theta}(w_t | c, w_{< t-1})
\end{align*}

To best apply GPT-3 to this task, it is necessary to first devise an appropriate prompt. Therefore, our first step is prompt engineering.

Since the format and even exact wording of GPT-3's prompts can have a significant effect on task performance \cite{jiang-etal-2020-know,shin-etal-2020-autoprompt,schick-schutze-2021-just}, we design several different candidate prompts for the GEC task. We start with a zero-shot setting. 
In addition, we use nucleus (top-p) sampling~\cite{Holtzman2020The} to generate tokens, repeating experiments with temperature hyperparameters $\tau$ of 0.1, 0.5, and 0.9.\footnote{Other hyperparameters used include logprobs=0, num\_outputs=1, top\_p=1.0, and best\_of=1}

To select the best prompt, we use GLEU scores~\cite{Napoles2015-en} on the JFLEG~\cite{jfleg} development set to rank combinations of prompts and temperature values. 

Table~\ref{tab:prompts} shows the zero-shot prompts we experimented with, as well as their results. Elsewhere in this paper, we will refer to these prompts by number based on their index from this table. As this is a preliminary exploration of the behavior of GPT-3 in this task, we limit our experiments to these ten prompts, making no claims of an exhaustive search or that our highest scoring prompt is the best possible approach.

Consistent with expectations, we find that the content of the prompt is very significant for performance. Our best zero-shot prompt has more than double the score of the worst on automated metrics. It is also clear that the temperature hyperparameter has an effect on performance in this task, with lower temperatures consistently performing better than higher temperatures. 

\subsection{Analysis}

Observing the outputs during zero-shot prompt development, we identified several interesting phenomena and issues, which we will describe here.

First, some of the prompts wrapped input sentences in quotes. These often returned corrections which were also wrapped with quotes (93\% for Prompt No. 4). Other prompts without quotes in the input nevertheless occasionally returned quoted lines as well (\textasciitilde3\% for Prompt No. 9).

Sometimes, the output includes ``meta'' information such a note that no edits were made, declaring a sentence correct, or responding in a chat-like manner. Several examples can be found below:

\begin{enumerate}
    \item[] \textbf{Input:} \\
    But ideas of how you can solve this problem will actually give you a solution to the problem. 
    \item[] \textbf{Output (Prompt No. 3):} \\
    By revising the sentence, it can be improved to: Generating ideas for how to solve this problem will help you find a solution.
\end{enumerate}

\begin{enumerate}
    \item[] \textbf{Input:} \\
    People also do not do nothing. 
    \item[] \textbf{Output (Prompt No. 5):} \\
    People also do not do nothing. (no change)
\end{enumerate}

\begin{enumerate}
    \item[] \textbf{Input:} \\
    They try new things. 
    \item[] \textbf{Output (Prompt No. 9):} \\
    There are no errors in this sentence.
\end{enumerate}

Another interesting phenomenon occurs when a source sentence does not end with proper punctuation. GPT-3 has a tendency to start by adding the appropriate punctuation mark, then continue by correcting the sentence, resulting in an extra punctuation mark at the beginning of the output. An example is seen below:

\begin{enumerate}
    \item[] \textbf{Input:} \\
    The second one is to specialize in one specific subject, one has his own tast in studing, so let us talk about this subject 
    \item[] \textbf{Output (Prompt No. 2):} \\
    . The second one is to specialize in one specific subject; one has their own taste in studying, so let us talk about this subject.
\end{enumerate}

A more severe version of this issue is also observed, in which the model continues generating text based on the context of the source sentence. This can result in \textit{spurious expansions} of the original writing, repetitions at the beginning of the sequence, or extra lines. An example is seen below:

\begin{enumerate}
    \item[] \textbf{Input:} \\
    For example, if the city says that 8am~9 am is the most busiest time, than worker would argues that they drive a bit later
    \item[] \textbf{Output (Prompt No. 7):} \\
    when the traffic is more light. 

    For example, if the city says that 8am-9am is the busiest time, then workers would argue that they should drive a bit later when the traffic is lighter.
\end{enumerate}

In this case, the added text and newline at the beginning are very problematic, and even resulted in an issue in the evaluation script by breaking the symmetry of source-reference pairs. It is also not desirable to show this to a user of a theoretical system, since the output is noticeably strange. 

\input{tab/num_prompts}

After identifying the best zero-shot prompt, we proceeded to experiments in a few-shot setting, adding one or more example sentence-correction pairs to our best zero-shot prompt to demonstrate the GEC task. We experimented with up to six examples, finding that performance modestly improves over the zero-shot prompt in all cases, but peaks at two examples. Results from this experiment can be seen in Table \ref{tab:num_prompts}.

\input{tab/final_prompt}

Our final prompt, presented in Table~\ref{tab:final_prompt}, produced very few of the above-mentioned problematic outputs. There were no spurious new lines or outputs wrapped in quotes. This emphasizes the importance of prompt design when applying GPT-3.