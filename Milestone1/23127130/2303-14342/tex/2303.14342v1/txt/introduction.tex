\section{Introduction}

Over the past few years, significant strides have been made in the field of Natural Language Processing (NLP).
One particular breakthrough that has gained widespread attention among researchers and industry practitioners is the GPT-3 family of models~\cite{brown2020language}, which have demonstrated impressive performance across a variety of tasks in both zero-shot and few-shot settings.

However, information about GPT-3's performance in the task of grammatical error correction (GEC) is relatively scarce in literature, and we are aware of no studies which include information about the latest GPT-3 model at the time of this writing, \texttt{text-davinci-003}. \citet{yasunaga-etal-2021-lm} present scores for ``GPT3 (175B) with prompting,'' with no details about the prompts used. 
\citet{schick2022peer} present GEC scores using the simple zero-shot prompt ``Fix grammar errors.'' 
\citet{dwivediyu2022editeval} offer more detailed quantitative analysis with a variety of zero-shot prompts. These instances all concern a previous version of GPT-3, \texttt{text-davinci-001}, and do not perform close qualitative analysis of the output of GPT-3 versus baseline models or humans. 

In this paper, we examine the performance of GPT-3 in the task of GEC. We carry out an empirical comparison of the performance of the most advanced GPT-3.5 model at the time of this writing (\texttt{text-davinci-003})\footnote{As of March 1st, 2023} against models on GEC leaderboards and the reference edits included in the source datasets. We report the relative scores of the systems and reference edits, and analyze the differences in their respective corrections. We also describe our prompt development process and the effect of the temperature hyperparameter on GPT-3's performance in this task.

We find that automatic metrics and human evaluations sometimes disagree on the relative quality of corrections. We examine several such cases in Section~\ref{Analysis}, finding that the differences stem from the automatic metrics' dependency on specific reference corrections, resulting in low scores for edits which are valid (and scored highly by human raters), but not present in the reference files. 

Our experimental results reveal certain limitations in the reference-based evaluation approach when assessing the performance of large generative language models such as GPT-3. These results emphasize the importance of exploring new directions in the field of GEC, such as paragraph- or document-level grammatical error correction~\cite{mita2022towards}, reference-less evaluation metrics~\cite{napoles-etal-2016-theres}, or initiatives such as Bidimensional Leaderboards~\cite{kasai-etal-2022-bidimensional}, which promote the development of metrics by ranking them by correlation to humans and their tendency to overrate machine outputs. By rethinking our evaluation strategies and embracing innovative techniques, we can improve the efficacy and utility of NLP models in real-world applications.

\input{tab/prompts}
