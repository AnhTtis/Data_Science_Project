\section{Evaluation Experiments} \label{sec:experiments}

\subsection{Data and Benchmarks}
We use two benchmark datasets: the development set\footnote{We use the development set (from the W\&I + LOCNESS dataset~\cite{bryant-etal-2019-bea}) because human-written references  are not publicly available for the test set.} of the BEA-2019 Shared Task~\cite{bryant-etal-2019-bea} and the test set of the JFLEG dataset~\cite{jfleg}.
We selected 100 sentences from each dataset for human evaluation.

\subsection{Human Evaluation}
\label{sec:human_eval}
In our study, we use the method from~\citet{sakaguchi-van-durme-2018-efficient}, which efficiently elicits scalar annotations as a probability distribution by combining two approaches: direct assessment and online pairwise ranking aggregation.

For the human evaluation task, we instruct crowdworkers to compare and score the quality of corrections among the following four versions of each sentence: the source sentence (with no corrections), an output from one of the recent models for each benchmark (\citet{yasunaga-etal-2021-lm} for BEA-2019 and \citet{liu-etal-2021-neural} for JFLEG)\footnote{We appreciate that they shared their model outputs.}, which we refer to as baseline systems, a human-written reference correction (included in the original datasets), and the corrections generated by GPT-3 using our best prompt (as seen in Table \ref{tab:final_prompt}).
For each comparison, we assign three crowdworkers from Amazon Mechanical Turk to score the quality of corrections on a scale of 0 (very poor correction) to 10 (excellent correction).\footnote{From our initial experiment, participants received compensation at a rate of \$1.0 per HIT, which roughly translated to an hourly wage of \$15. The inter-annotator agreement, as indicated by Cohen's kappa, varied between 0.53 (for JFLEG) and 0.66 (for BEA-2019).}

\subsection{Results} \label{section:results}
\input{tab/results}

The results of our investigation are shown in Table~\ref{tab:main_results}. When interpreting results, please note that in the BEA-2019 benchmark, the {F$_{0.5}$} score is essentially 0 for the source. The {F$_{0.5}$} score for human reference is not 100 despite the same single reference because the edits are automatically extracted in the evaluation script~\cite{bryant-etal-2019-bea}. To obtain the score for the ``Human'' corrections in the JFLEG dataset, which has multiple references, we randomly selected one human reference file and compared with it the other three references.

Our results show that GPT-3 achieves high performance on the task of GEC according to human evaluations and the JFLEG GLEU scorer. However, performance seems noticeably lower on the {F$_{0.5}$} metric used for BEA-2019. This is intriguing considering its high scores in human evaluation. 

\subsection{Analysis} \label{Analysis}

We believe that the gap in scores is due to the nature of the BEA-2019 dataset and metric, in which there is a single reference for each sentence, generally with what could be described as minimal edits. Meanwhile, there are often multiple valid ways to correct an erroneous sentence, including fluency edits which involve alterations of multiple words while preserving the writer's intended meaning. We hypothesize that GPT-3 often makes such edits, which are acceptable to humans but penalized by the BEA-2019 scorer. Meanwhile, the JFLEG corpus offsets this issue to an extent by prioritizing fluency corrections and incorporating multiple parallel reference sentences for each source sentence, resulting scores which are more in line with the human evaluation.

To understand the behavior of GPT-3 as a grammatical correction model, we examine its outputs in parallel with the source and reference sentences and those of the baseline error correction models. Consider the following example sentences, presented with associated scores from human raters:

\begin{enumerate}
    \item[] \textbf{Source: (6.67)} \\
    Also, you'll meet friendly people who usually ask to you something to be friends and change your telephone number.	

    \item[] \textbf{Baseline: (6.67)} \\
    Also, you'll meet friendly people who usually ask you to be friends and change your telephone number.

    \item[] \textbf{Human: (8.67)} \\
    Also, you'll meet friendly people who usually ask you to be friends and exchange telephone numbers.

    \item[] \textbf{GPT-3: (7.67)} \\
    Also, you'll meet friendly people who usually ask to be friends and exchange phone numbers.
\end{enumerate}
In this case, the baseline system produces ``ask you to... change your telephone number,'' which is a grammatically valid minimal edit. However, the human editor \emph{hypothesizes} that the writer means ``exchange'' and is making the error of using a similar-sounding word in its place. The human's edit based on this hypothesis produces a better sentence than the baseline system.

Meanwhile, GPT-3 successfully produces such an edit as well. While a human relies on their real-world knowledge for this hypothesis, GPT-3 generates text based on the patterns in its large-scale training data, in which phrases like ``meeting friendly people'' are more associated with ``exchanging'' phone numbers than ``changing'' them. Nevertheless, GPT-3 seems able to ``hypothesize'' edits as part of GEC text completion tasks, allowing it to make more liberal and fluent changes than the minimal edits favored by the baselines.

Another example of GPT-3's tendency to make fluency edits can be seen below:

\begin{enumerate}
    \item[] \textbf{Source: (3.67)} \\
    This reminds me of a trip that I have recently been to and the place is Agra.

    \item[] \textbf{Baseline: (7.33)} \\
    This reminds me of a trip that I have recently been on and the place I visited was Agra.

    \item[] \textbf{Human: (6.67)} \\
    This reminds me of a trip that I have recently been to and the place is Agra.

    \item[] \textbf{GPT-3: (9.66)} \\
    This reminds me of a trip I recently took to Agra.
\end{enumerate}

In this case, the edit made by GPT-3 is the most natural and correct sentence, and it was given the highest score by the raters. Meanwhile, the BEA-2019 scorer gives GPT-3 a score of zero and the baseline a score of 1, and JFLEG's scorer gives scores of 24.8 vs. 25.8. Neither reflects the strong preference humans displayed for GPT-3's edit, indicating the limitation of reference-based metrics~\cite{napoles-etal-2016-theres,asano-etal-2017-reference}.