% Table 5
\begin{table*}[ht]
\small
\centering
\begin{tabular}{@{} l r c c c c c @{}}
\toprule[.1em]

\multicolumn{1}{c}{}
&
\multicolumn{3}{c}{\textbf{BEA-2019 (Dev Subset)}}
&
\multicolumn{3}{c}{\textbf{JFLEG (Test Subset)}}
\\
             & {F$_{0.5}$}  & {Human} & {Scribendi} & {GLEU} & {Human} & {Scribendi}\\ 
             Scale: & (0-100)  & (0-1)  & (0-1) & (0-100)  & (0-1) & (0-1) \\  \midrule
Source       & 0 & 0.449 & 0 & 36.51 & 0.465 & 0\\
Reference    & \textbf{83.97} & 0.706 & \textbf{0.83} & 54.63 & 0.712 & 0.74\\
Baseline     & 39.14  & 0.568 & 0.67 & 57.70 & 0.662 & 0.71\\
GPT-3.5      & 37.87 & 0.769 & 0.71 & 63.02 & \textbf{0.819} & \textbf{0.78}\\
GPT-4        & 37.99 & \textbf{0.788} & 0.75 & \textbf{63.78} & 0.809 & 0.75\\ \bottomrule
\end{tabular}
\caption{Comparison of automated metrics and human evaluation scores for different versions of sentences in our human evaluation subset of 100 sentences from each dataset, as described in section~\ref{sec:human_eval}. The best scores are in bold. In human evaluations, the difference between GPT-3.5 and GPT-4 is not statistically significant for either the BEA-2019 or JFLEG benchmarks ($p>$0.19, $p>$0.4).}
\vspace{-0.0cm}
\label{tab:subset_results}
\end{table*}