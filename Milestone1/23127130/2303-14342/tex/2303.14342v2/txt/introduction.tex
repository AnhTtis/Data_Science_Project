\section{Introduction}

Over the past few years, significant strides have been made in the field of Natural Language Processing (NLP).
OpenAI's GPT models, including GPT-3~\cite{brown2020language} and GPT-4~\cite{openai2023gpt4}, have gained widespread attention among researchers and industry practitioners and demonstrated impressive performance across a variety of tasks in both zero-shot and few-shot settings.

However, information about these models' performance in the task of grammatical error correction (GEC) is still relatively scarce.
OpenAI's technical reports do not include benchmark scores for GEC, as are present for other tasks such as Question Answering.
As OpenAI updates its latest model, there have been only a few studies that try to shed some light on GPTâ€™s performance on the GEC task. 
These works, which we discuss further in Section~\ref{sec:background}, present a preliminary analysis on \texttt{text-davinci-002} and \texttt{gpt-3.5-turbo}. 
Our work seeks to add to and complement these, targeting different GPT models, presenting a more fine-grained prompt and hyperparameter search, and collecting comparative edit quality ratings from human annotators.

%%%% Figure 1
\begin{figure}[t!]
\centering
\includegraphics[width=0.9\columnwidth]{fig/intro_example4.pdf}
\caption{OpenAI's example prompt for ``grammar correction,'' showing an input and output (highlighted in green) for the sentence-level revision task.
Our experiments with GPT-3.5 and GPT-4 are based on this pattern.}

\label{fig:intro_example}
\end{figure}

In this work, we assume a prompt setting in which the input is a single potentially ungrammatical sentence and the output is a single correction, as seen in Figure \ref{fig:intro_example}.
We have chosen this setting to match the format of widely used GEC benchmarks which are scored by comparing parallel sentences. 
In addition, we assume a specific task setting of GEC for text revision, taking an ill-formed sentence as input and producing a well-formed version of the sentence which preserves the perceived meaning.

Following a prompt search, we report the performance of \texttt{text-davinci-003}, as well as a current GPT-4 model (\texttt{gpt-4-0314}), on GEC benchmark test sets. 
We then define a subset of sentences and perform side-by-side comparisons of the GPT models' generations, the outputs of two baseline GEC systems, and the human reference edits included in the benchmark datasets.
We report scores from both automated metrics and human raters and perform qualitative analysis of the differences between the respective corrections. 
We also describe our prompt development process and the effect of the temperature hyperparameter on GPT-3.5 and GPT-4's performance on this task.

Based on our experiments, we observe that:
\begin{itemize}
    \item Given a suitable prompt, the GPT models behave reliably in the single-sentence prompt setting, generating no unexpected sequences such as comments or new lines. 
    \item The models show strong performance on the sentence revision task, with GPT-4 achieving a new high score on the JFLEG test set.
    \item The models exhibit some prompt sensitivity. Both the error correction quality and the reliability of the output format differ significantly based on simple changes to wording or punctuation. 
    \item Using our final prompt, the models seem to favor fluency corrections, underperforming on metrics and datasets which rely on a single reference with minimal edits, but performing well on fluency edit tasks and in human evaluations.
    \item The models occasionally over-edit, changing the meaning of a sentence during correction, or expanding fragments with new material.
    \item As a result of the above, different automatic metrics and human raters sometimes disagree on the relative quality of corrections. We examine some cases of this in Section~\ref{Discussion}.
\end{itemize}

Our experimental results emphasize the importance of the specific task setting and choice of benchmark when prompt engineering for large language models such as GPT-3.5 and GPT-4. 

\input{tab/prompts}