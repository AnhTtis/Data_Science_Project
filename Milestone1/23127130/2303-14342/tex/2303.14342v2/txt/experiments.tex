\section{Evaluation Experiments} \label{sec:experiments}

\subsection{Data and Benchmarks}

We use two benchmark datasets: the BEA-2019 shared task dataset and JFLEG. For GEC benchmark scores, we use the test set for both. For qualitative analysis and a human evaluation experiment in which different corrections are compared side-by-side, we define a smaller sample of 200 sentences. We select the first 100 sentences each from BEA-2019 development set\footnote{We use the development set (from the W\&I + LOCNESS dataset~\cite{bryant-etal-2019-bea}) because human-written references are not publicly available for the test set.} and the JFLEG test set, excluding sentences with fewer than 10 tokens, which were mostly greetings or highly fragmentary.

\subsection{Human Evaluation}
\label{sec:human_eval}
In our study, we use the method from~\citet{sakaguchi-van-durme-2018-efficient}, which efficiently elicits scalar annotations as a probability distribution by combining two approaches: direct assessment and online pairwise ranking aggregation.

For the human evaluation task, we asked crowdworkers to compare and score the quality of corrections, with a focus on maintaining the original meaning and ensuring the output is fluent and natural-sounding. 
Participants rated the following five versions of each sentence:~the source sentence (with no corrections), a human-written reference correction (included in the original datasets), the corrections generated by GPT-3.5 and GPT-4 using our best-performing prompt (as seen in Table \ref{tab:final_prompt}), and an output from baseline GEC models for each benchmark (\citet{yasunaga-etal-2021-lm} for BEA-2019 and \citet{liu-etal-2021-neural} for JFLEG). These systems were chosen due to the availability of their outputs, allowing for direct side-by-side comparisons.

For each comparison, we assign three crowdworkers to score the quality of corrections on a scale of 0 (very poor correction) to 10 (excellent correction).
Additional details about the human evaluation task can be found in the appendix.