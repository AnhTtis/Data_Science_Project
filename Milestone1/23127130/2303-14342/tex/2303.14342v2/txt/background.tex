\section{Background}\label{sec:background}

\subsection{OpenAI Models}

Following the success of Transformer-based large language models (LLMs) on several NLP tasks, in which increasing the number of the model's parameters consistently showed improvements,~\citet{brown2020language} trained a 175 billion parameter auto-regressive LM: GPT-3.
GPT-3.5 models are refined from GPT-3 using reinforcement learning from human
feedback~\cite{ouyang2022training}. 
The successor to these, GPT-4, is assumed to be even larger, but the parameter counts were not described in its technical report~\cite{openai2023gpt4}. 
Both models were evaluated on ``over two dozen NLP datasets'', whose tasks range from Question Answering (QA) to Natural Language Inference (NLI) and Reading Comprehension (RC).
GPT-4 was additionally tested on a set of exams that were originally designed for humans
However, no GEC dataset was considered in either of the models' evaluation, necessitating independent task-specific analysis.

\citet{ostlingreally} use a single 2-shot prompt to investigate \texttt{text-davinci-002} in Swedish GEC, finding its performance strong considering it was trained on very little Swedish text.

Following the release of ChatGPT, \citet{wu2023chatgpt} assess its GEC capabilities using a single zero-shot prompt on the CoNLL-2014 dataset \cite{ng-etal-2014-conll}.
\citet{fang2023chatgpt}, investigate \texttt{gpt-3.5-turbo} with both zero-shot and few-shot prompting, as well as human evaluations of the results. 
These studies both find that the GPT models tend to make fluency edits and over-corrections.

Our work differs from the above in the models assessed, the nature of our prompt search, which is more fine-grained in order to investigate prompt sensitivity, and in the aims of our human experiments.
The previous studies on ChatGPT ask participants to identify phenomena such as over-corrections and under-corrections, whereas our experiment elicits comparative error quality ratings.

\subsection{Grammatical Error Correction}

Writing is not an easy task.
Given a goal, we have to decide what to say and how to say it, making sure that the chosen words can be integrated into a coherent whole and conform to the grammar rules of a language \cite{zock2017use}.
This has motivated the NLP community to develop innovative approaches for writing assistance, which are particularly focused on error correction.

GEC research can generally be defined in terms of one of two broad task settings.
The first is education for language learners, in which case easily comprehensible minimal edits are employed, with an emphasis on achieving \textit{grammaticality} but otherwise leaving the sentence as-is.
The other is a revision task in which a sequence is edited to sound \textit{fluent} and natural, and any number or type of changes can be applied as long as the intended meaning, as interpreted by the editor, is preserved.

Research on GEC has primarily been investigated based on the CoNLL-2014 and BEA-2019 \cite{bryant-etal-2019-bea} shared tasks, where systems are evaluated by {F$_{0.5}$} score. 
Since the datasets provided by these two tasks focus on \textit{grammaticality}, \citet{jfleg} released the JFLEG dataset as a new gold standard to evaluate how \textit{fluent} a text is.
Results on this dataset are evaluated with GLEU~\cite{Napoles2015-en}, which relies on n-gram overlap rather than the number of error corrections found in a sentence.

The best systems on each of the aforementioned tasks show a variety of approaches: classification with logistic regression \cite{qorib-etal-2022-frustratingly}, a combination of Statistical and Neural Machine Translation \cite{grundkiewicz-junczys-dowmunt-2018-near, junczys-dowmunt-etal-2018-approaching, kiyono-etal-2019-empirical}, sequence tagging with encoder-only Transformer models \cite{omelianchuk-etal-2020-gector, tarnavskyi-etal-2022-ensembling}, a multilayer CNN encoder-decoder \cite{chollampatt2018multilayer}, and Transformers-based encoder-decoder models \cite{stahlberg-kumar-2021-synthetic, kaneko-etal-2020-encoder}.