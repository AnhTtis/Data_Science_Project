\section{Limitations}

The scores presented in this paper are based on proprietary models accessed via API. 
They may be updated internally or deprecated in the future.

As this is a preliminary exploration of the behavior of GPT-3.5 and GPT-4 in this task, we limit our experiments to the ten listed prompts, making no claims of an exhaustive search.
We do not try such techniques as chain-of-thought prompting.
We leave such experiments to future research.

For time and budget reasons, the metric scores reported are for a single output file for each dataset and model combination.
Our human annotation experiment was similarly limited by budget, and qualitative analysis was only performed on two hundred sets of candidate sentences.

Due to our sentence revision setting, our experiments focused more on fluency edits than minimal edits, and our human raters tended to prefer the extensive rewrites and that the GPT models often output. 
However, more constrained corrections may be desirable in different GEC task settings, such as in language education, where a learner may more clearly understand the information presented by a minimal edit.
A similar study can be done to investigate how well GPT models can adhere to a minimal editing task.
We leave this to future work.