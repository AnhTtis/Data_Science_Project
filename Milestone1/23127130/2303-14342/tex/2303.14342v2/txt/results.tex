\section{Results} \label{sec:results}

\subsection{Prompt Engineering}

Scores for different zero-shot prompts can be seen in Table \ref{tab:prompts}. Consistent with expectations, we find that the content of the prompt is very significant for performance. 
Our best zero-shot prompt has more than double the score of the worst on automated metrics. 
It is also clear that the temperature hyperparameter has an effect on performance in this task, with lower temperatures consistently performing better than higher temperatures. 

\input{tab/num_prompts}
\input{tab/final_prompt}
Moving on to few-shot prompts, we experimented by adding examples to Prompt \#10. 
Results from this experiment can be seen in Table \ref{tab:num_prompts}. 
We find that for GPT-3.5, performance modestly improves over the zero-shot prompt in all cases, but peaks at two examples. 
For GPT-4, the few-shot examples seem to have a negligible or slight negative effect, with two examples once again scoring the highest among few-shot prompts.

Against expectations, there were many zero-shot prompts in which GPT-3.5 outperformed GPT-4.
This was observed in all prompts except for Prompts \#2 and \#10, in which GPT-4 performed better.
However, several of the same trends can be seen for both models, such as the tendency for lower temperature to improve performance.
Ultimately, both models showed the strongest performance on the same prompt, and had the same best number of examples in few-shot experimentation.

Observing the outputs during zero-shot prompt development, we identified several interesting phenomena and issues, which we will describe here.

\paragraph{``Meta'' information:} Sometimes, the models include a note that no edits were made (\ref{ex:inout1}), declare that a sentence is correct (\ref{ex:inout2}), or respond in a chat-like manner (\ref{ex:inout3}). Examples can be found below:

% Example 1
\enumsentence{
\small
\textbf{Input:} \\
People also do not do nothing.

\textbf{Output (Prompt \#5):} \\
People also do not do nothing. (no change)
\label{ex:inout1}
}

% Example 2
\enumsentence{
\small
\textbf{Input:} \\
They try new things.

\textbf{Output (Prompt \#9):} \\
There are no errors in this sentence.
\label{ex:inout2}
}

% Example 3
\enumsentence{
\small
\textbf{Input:} \\
But ideas of how you can solve this problem will actually give you a solution to the problem. 

\textbf{Output (Prompt \#3):} \\
By revising the sentence, it can be improved to: Generating ideas for how to solve this problem will help you find a solution.
\label{ex:inout3}
}

\paragraph{Outputs with extra punctuation:}
Another interesting phenomenon occurs when a source sentence does not end with proper punctuation. The models may start by adding the appropriate punctuation mark, then continue by correcting the sentence, resulting in an extra punctuation mark at the beginning of the output. 
An example is seen below:

% Example 4
\enumsentence{
\small
\textbf{Input:} \\
The second one is to specialize in one specific subject, one has his own tast in studing, so let us talk about this subject

\textbf{Output (Prompt \#2):} \\
. The second one is to specialize in one specific subject; one has their own taste in studying, so let us talk about this subject.
\label{ex:inout4}
}

\paragraph{Outputs with spurious expansions:}
A more severe version of the aforementioned issue, in which the model continues generating text based on the context of the source sentence. 
This can result in spurious expansions of the original writing, such as repetitions at the beginning of the sequence or extra lines. 
An example is seen below:

% Example 5
\enumsentence{
\small
\textbf{Input:} \\
For example, if the city says that 8am~9 am is the most busiest time, than worker would argues that they drive a bit later 

\textbf{Output (Prompt \#7):} \\
 when the traffic is more light. \\
For example, if the city says that 8am-9am is the busiest time, then workers would argue that they should drive a bit later when the traffic is lighter.
\label{ex:inout5}
}
%
\input{tab/results_benchmark}

In this case, the added text and newline at the beginning are problematic, resulting in an issue in the GLEU evaluation script by breaking the symmetry of lines in the input files. 
It is also not desirable to show this as-is to a user of a GEC system, since the output is noticeably strange. 


For our final prompt, we choose Prompt \#10 with two examples, which can be seen in Table~\ref{tab:final_prompt},
Despite GPT-4's slightly higher performance with a zero-shot prompt, we use this 2-shot prompt with both models in our experiments in order to observe the differences between the models given the exact same input sequence.
This ``best'' prompt produced few or none of the above unexpected outputs with either GPT-3.5 or GPT-4. 
There were no repetitions or new lines.
This emphasizes the importance of prompt design when applying GPT models.

\subsection{Benchmark Scoring} 

GEC benchmark scores, calculated on the BEA-2019 and JFLEG test sets, are shown in Table~\ref{tab:benchmark_results}. 
To score GPT-3.5 and GPT-4's outputs against the references and baseline systems, we use the standard scores for each dataset, {F$_{0.5}$} for BEA-2019 and GLEU for JFLEG.
When interpreting results, note that in the BEA-2019 benchmark, the {F$_{0.5}$} score is essentially 0 for the source. 
The human reference score is unknown, as the reference edits are part of the withheld test set.
To obtain the score for the ``Human Reference'' corrections in the JFLEG dataset, which has multiple references, we randomly selected one human reference file and compared with it the other three references.

The results show that the GPT models perform well on the JFLEG test set, with GPT-4 obtaining a score that is the highest yet reported to the best of our knowledge. 
In contrast, the scores on the BEA-2019 test set are well below those of the baseline systems.
We discuss this disparity in Section~\ref{Discussion}.


\subsection{Human Evaluation and Subset Analysis} \label{section:results}
\input{tab/results_subset}

For the subset of 100 sentences each from the BEA-2019 development set and the JFLEG test set, we gather human ratings as described in Section~\ref{sec:human_eval} and place them alongside the respective datasets' automated metrics.
Additionally, we apply a ``reference-less'' automatic metric, Scribendi Score~\cite{islam-magnani-2021-end}, which assesses grammaticality, fluency, and  syntactic similarity using token sort ratio, levenshtein distance ratio, and perplexity as calculated by GPT-2. 
We use an unofficial implementation,\footnote{\url{https://github.com/gotutiyan/scribendi_score}} as the authors seem not to have made their code available. 

The scores from our experiments are shown in Table \ref{tab:subset_results}.
Note that the BEA-2019 benchmark's {F$_{0.5}$} score for human reference is not 100 despite the same single reference because the edits are automatically extracted in the evaluation script~\cite{bryant-etal-2019-bea}. 
Scores from Scribendi are returned on a per-sentence basis, so we report the mean for each output file. A score of 0 indicates no edits.

The results suggest that GPT-3.5 and GPT-4 achieve high performance on the task of GEC according to human evaluations and the automatic metrics, with a majority of the best scores being obtained by either GPT-3.5 or GPT-4. 