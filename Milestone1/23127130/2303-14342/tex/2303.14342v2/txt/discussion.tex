\section{Discussion} \label{Discussion}

\subsection{Scoring Disparities} 

The results in Tables~\ref{tab:benchmark_results} and \ref{tab:subset_results} show that GPT-3.5 and GPT-4 achieve strong performance on the sentence revision task as measured by GLEU score on the JFLEG dataset, human ratings, and Scribendi scores, outperforming the baseline systems on these metrics.
However, their {F$_{0.5}$} scores on the BEA-2019 datasets are comparatively lower.

We believe that this is a result of differences in the priorities expressed in the human reference edits present in the two datasets.
In the BEA-2019 dataset, there is a single reference for each sentence, generally with what could be described as minimal edits. 
Meanwhile, our primary task setting is one of sentence revision, and our prompt engineering experiments were performed using JFLEG, a benchmark for fluency.
This seems to have contributed to a propensity for the GPT models to output fluency corrections which display extensive editing.
These are scored well on JFLEG's GLEU metric, but penalized on BEA-2019's {F$_{0.5}$} metric.

This is supported by the fact that the models were given similar scores in both datasets by human raters and the Scribendi metric, which is not connected to references from either dataset and is thus not affected by any differences between the reference edits found in BEA-2019 and JFLEG.

\subsection{Qualitative Analysis}

The scores discussed above describe the performance of the different systems in aggregate.
However, there are a number of cases in which the GPT models' outputs are given scores which differ from those assigned by the automated metrics.
Additionally, there are cases in the human evaluation experiments in which the GPT models significantly over-perform or under-perform the human reference edits or the baseline systems. 
We consider a performance discrepancy notable if the candidate sentences show a difference of more than 2 points in the mean of human ratings assigned to them.

To investigate such cases and better understand the behavior of the GPT models as grammatical correction systems, we examine the models' outputs in parallel with the source and reference sentences and those of the baseline error correction systems. Below, we present output sentences along with their respective scores from human raters.

\paragraph{GPT Models Outscoring Human References}

We found 24 cases in JFLEG Test and 14 cases in BEA-2019 Dev in which the GPT models both outscored the human reference edits. 
We find that these cases usually occur when a human editor leaves a grammatical error or non-fluent construction unchanged, but the GPT models revise it. An example can be seen below:

\enumsentence{
\small
\textbf{Source Sentence: (3)} \\
This reminds me of a trip that I have recently been to and the place is Agra.

\textbf{Human Reference: (3.66)} \\
This reminds me of a trip that I have recently been on and the place I visited was Agra.

\textbf{Baseline System: (3)} \\
This reminds me of a trip that I have recently been to and the place is Agra.

\textbf{GPT-3.5: (9.66)} \\
This reminds me of a trip I recently took to Agra.

\textbf{GPT-4: (10)} \\
This reminds me of a recent trip I took to Agra.
\label{ex:inout7}
}

In this case, the edits made by GPT models are the most natural and correct sentences, and are given the highest scores by the raters.
However, this is not to say that the human reference edit was mistaken or inferior, especially if we consider that this example is taken from the BEA dataset, in which minimal edits are common.
Nevertheless, there are also a number of such cases in our subset from JFLEG, where the goal of the task is fluency editing.
This demonstrates that humans tasked with performing or evaluating corrections do not always agree on the ideal extent of revision.

\paragraph{Over-editing} There are some cases in which the GPT models add or change words in a way that results in changes in the meaning of the sentence. An example can be seen below:

% Example 8
\enumsentence{
\small
\textbf{Source Sentence: (4)} \\
I consider that is more convenient to drive a car because you carry on more things in your own car than travelling \underline{by car}.

\textbf{Human Reference: (4)} \\
I consider it more convenient to drive a car, because you carry more things in your own car than when travelling \underline{by car}.

\textbf{Baseline System: (6.67)} \\
I consider that it is more convenient to drive a car because you carry on more things in your own car than travelling \underline{by car}.

\textbf{GPT-3.5: (7.67)} \\
I consider it more convenient to drive a car because you can carry more things in your own car than when travelling \underline{by public transport}.

\textbf{GPT-4: (9)} \\
I consider it more convenient to drive a car because you can carry more things in your own car than when traveling \underline{by public transportation}.	
\label{ex:inout8}
}

Here, it seems likely that public transportation is what the writer is comparing cars to, but the term does not appear in the source sentence.

While such cases in our data generally result in sequences that seem likely, it may be desirable to control for this behavior depending on the GEC task setting.

There are also cases where a fragmentary sentence is expanded by the GPT models.
For these as well, suggesting completions is not necessarily in the scope of GEC.
An example can be seen below:

% Example 9
\enumsentence{
\small
\textbf{Source Sentence: (1.33)} \\
If the film doesn't arrive on time, it immediately.

\textbf{Human Reference: (1.33)} \\
If the film doesn't arrive on time, it immediately.

\textbf{Baseline System: (1.66)} \\
If the film doesn't arrive on time, it will immediately.

\textbf{GPT-3.5: (9.66)} \\
If the film doesn't arrive on time, it will be \underline{cancelled} immediately.

\textbf{GPT-4: (4)} \\
If the film doesn't arrive on time, it will be \underline{shown} immediately.	
\label{ex:inout9}
}

In this case, it seems as if the GPT models, given only this fragment as context, attempt to fix it by adding some plausible verb, with GPT-3.5's completion being judged more reasonable.
However, depending on the task setting, it may be desirable to take some action other than suggesting a correction in these cases. 
For example, a system may simply highlight the sentence as ungrammatical, or perhaps a feedback comment about missing verbs or fragments could be generated instead.
These actions exceed the scope of our experiments, but could certainly be achieved with a more complex writing assistance program.
Whether any such alternative behaviors could reliably be achieved by prompting the GPT models is left to future work.

\paragraph{GPT Models Underperforming}

In the majority of cases in the subset, the GPT models had comparable or superior performance to the baseline systems.
However, there were some cases (4 in BEA-2019 and 7 in JFLEG) where the baseline systems outperformed the GPT models.

The human references were more likely to outperform the GPT models, with 13 cases in BEA-2019 and 10 in JFLEG.
We examine a case of GPT underperformance below:

\enumsentence{
\small
\textbf{Source Sentence: (3.33)} \\
\underline{By the time up} everyone should be gathered up in a certain place.

\textbf{Human Reference: (9.33)} \\
\underline{When the time is up}, everyone should be gathered in a certain place.

\textbf{Baseline System: (6.66)} \\
\underline{By the time everyone gets up}, everyone should be gathered up in a certain place.

\textbf{GPT-3.5: (6.66)} \\
\underline{By the time}, everyone should be gathered in a certain place.

\textbf{GPT-4: (3.33)} \\
\underline{By the time up}, everyone should be gathered in a certain place.
\label{ex:inout6}
}

In this case, only the human editor successfully infers the intended phrase, as judged by the raters.
The baseline edit presents an alternative, grammatically correct possibility.
Meanwhile, the GPT models leave an ungrammatical span of the original sentence unchanged.

This ``under-editing'' behavior is interesting given that we also observe that the GPT models make frequent and extensive edits.
Given the size of our subset, is difficult to generalize about the circumstances in which the models under-edit or over-edit, or if there are ways to control either behavior. 
We leave such investigation to future work.
