
@misc{Authors14,
 author = {Full Author Name},
 title = {The Frobnicatable Foo Filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {Full Author Name},
 title = {Frobnication Tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {Alvin Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe},
title = {Frobnication Revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe and Gavin Gamow},
title = {Can a Machine Frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}

@misc{Konda2015DropoutAD,
  doi = {10.48550/ARXIV.1506.08700},
  
  url = {https://arxiv.org/abs/1506.08700},
  
  author = {Bouthillier, Xavier and Konda, Kishore and Vincent, Pascal and Memisevic, Roland},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dropout as data augmentation},
  
  publisher = {arXiv},
  
  year = 2015,
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{gao-etal-2021-simcse,
    title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
    author = {Gao, Tianyu  and
      Yao, Xingcheng  and
      Chen, Danqi},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.552",
    doi = "10.18653/v1/2021.emnlp-main.552",
    pages = "6894--6910",
    abstract = "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using {``}entailment{''} pairs as positives and {``}contradiction{''} pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3{\%} and 81.6{\%} Spearman{'}s correlation respectively, a 4.2{\%} and 2.2{\%} improvement compared to previous best results. We also show{---}both theoretically and empirically{---}that contrastive learning objective regularizes pre-trained embeddings{'} anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
    year = 2021
}

@inproceedings{wang2020hypersphere,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  booktitle={International Conference on Machine Learning},
  organization={PMLR},
  pages={9929--9939},
  year=2020
}

@misc{CKAsimilarity,
  doi = {10.48550/ARXIV.1905.00414},
  
  url = {https://arxiv.org/abs/1905.00414},
  
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  
  keywords = {Machine Learning (cs.LG), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  
  title = {Similarity of Neural Network Representations Revisited},
  
  publisher = {arXiv},
  
  copyright = {arXiv.org perpetual, non-exclusive license},
  
  year = 2019
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = {Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina},
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    year = 2019
}

@inproceedings{simclr,
author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
title = {A Simple Framework for Contrastive Learning of Visual Representations},
publisher = {JMLR.org},
abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {149},
numpages = {11},
series = {ICML'20},
year = 2020
}


@inproceedings{bigbird,
author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
title = {Big Bird: Transformers for Longer Sequences},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1450},
numpages = {15},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@misc{wavenet,
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio
waveforms. The model is fully probabilistic and autoregressive, with the
predictive distribution for each audio sample conditioned on all previous ones;
nonetheless we show that it can be efficiently trained on data with tens of
thousands of samples per second of audio. When applied to text-to-speech, it
yields state-of-the-art performance, with human listeners rating it as
significantly more natural sounding than the best parametric and concatenative
systems for both English and Mandarin. A single WaveNet can capture the
characteristics of many different speakers with equal fidelity, and can switch
between them by conditioning on the speaker identity. When trained to model
music, we find that it generates novel and often highly realistic musical
fragments. We also show that it can be employed as a discriminative model,
returning promising results for phoneme recognition.},
  added-at = {2022-01-22T18:25:51.000+0100},
  author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  biburl = {https://www.bibsonomy.org/bibtex/29652008052c26216e09ee88063315940/leophill},
  description = {WaveNet: A Generative Model for Raw Audio},
  interhash = {b9e02f1ffc1411c7752c89f39a13ef9f},
  intrahash = {9652008052c26216e09ee88063315940},
  keywords = {Thesis},
  note = {cite arxiv:1609.03499},
  timestamp = {2022-01-22T18:25:51.000+0100},
  title = {WaveNet: A Generative Model for Raw Audio},
  url = {http://arxiv.org/abs/1609.03499},
  year = 2016
}


@misc{contrastive1,
  doi = {10.48550/ARXIV.1807.03748},
  
  url = {https://arxiv.org/abs/1807.03748},
  
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Representation Learning with Contrastive Predictive Coding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}

  @inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{granularityMugs,
  title={Mugs: A Multi-Granular Self-Supervised Learning Framework},
  author={Pan Zhou and Yichen Zhou and Chenyang Si and Weihao Yu and Teck Khim Ng and Shuicheng Yan},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.14415}
}

@article{glom,
    author = {Hinton, Geoffrey},
    title = "{How to Represent Part-Whole Hierarchies in a Neural Network}",
    journal = {Neural Computation},
    volume = {35},
    number = {3},
    pages = {413-452},
    year = 2023,
    month = {02},
    abstract = "{This article does not describe a working system. Instead, it presents a single idea about representation that allows advances made by several different groups to be combined into an imaginary system called GLOM.1 The advances include transformers, neural fields, contrastive representation learning, distillation, and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy that has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01557},
    url = {https://doi.org/10.1162/neco\_a\_01557},
    eprint = {https://direct.mit.edu/neco/article-pdf/35/3/413/2072221/neco\_a\_01557.pdf}
}

@article{cnnclasshierarchy,
	doi = {10.1109/tvcg.2017.2744683},
  
	url = {https://doi.org/10.1109%2Ftvcg.2017.2744683},
  
	year = 2018,
	month = {jan},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {24},
  
	number = {1},
  
	pages = {152--162},
  
	author = {Alsallakh Bilal and Amin Jourabloo and Mao Ye and Xiaoming Liu and Liu Ren},
  
	title = {Do Convolutional Neural Networks Learn Class Hierarchy?},
  
	journal = {{IEEE} Transactions on Visualization and Computer Graphics}
}

@misc{rickardrelative,
  doi = {10.48550/ARXIV.2202.01145},
  
  url = {https://arxiv.org/abs/2202.01145},
  
  author = {Brüel-Gabrielsson, Rickard and Scarvelis, Chris},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Relative Position Prediction as Pre-training for Text Encoders},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@INPROCEEDINGS{rickardexposition,
  author={Brüel Gabrielsson, Rickard and Carlsson, Gunnar},
  booktitle={2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)}, 
  title={Exposition and Interpretation of the Topology of Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1069-1076},
  doi={10.1109/ICMLA.2019.00180}}


@InProceedings{rickardtopologyapproach,
author={Carlsson, Gunnar
and Brüel-Gabrielsson, Rickard},
editor="Baas, Nils A.
and Carlsson, Gunnar E.
and Quick, Gereon
and Szymik, Markus
and Thaule, Marius",
title="Topological Approaches to Deep Learning",
booktitle="Topological Data Analysis",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="119--146",
abstract="In this work we introduce an algebraic formalism to describe and construct deep learning architectures as well as actions on them. We show how our algebraic formalism in conjunction with topological data analysis enables the construction of neural network architectures from a priori geometries, geometries obtained from data analysis, and purely data driven geometries. We also demonstrate how these techniques can improve the transparency and performance of deep neural networks.",
isbn="978-3-030-43408-3"
}


@misc{effectivenessofdeepfeaturesasmetrics,
  doi = {10.48550/ARXIV.1801.03924},
  
  url = {https://arxiv.org/abs/1801.03924},
  
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{dataaugmentationimagessurvey,
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	title = {{A survey on Image Data Augmentation for Deep Learning}},
	journal = {J. Big Data},
	volume = {6},
	number = {1},
	pages = {1--48},
	year = {2019},
	month = dec,
	issn = {2196-1115},
	publisher = {SpringerOpen},
	doi = {10.1186/s40537-019-0197-0}
}


@article{sslreview,
	author = {Rani, Veenu and Nabi, Syed Tufael and Kumar, Munish and Mittal, Ajay and Kumar, Krishan},
	title = {{Self-supervised Learning: A Succinct Review}},
	journal = {Arch. Comput. Methods Eng.},
	pages = {1--15},
	year = {2023},
	month = jan,
	issn = {1886-1784},
	publisher = {Springer Netherlands},
	doi = {10.1007/s11831-023-09884-2}
}

@inproceedings{
mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1Ddp1-Rb}
}

@misc{manifoldmixup,
  doi = {10.48550/ARXIV.1806.05236},
  
  url = {https://arxiv.org/abs/1806.05236},
  
  author = {Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Courville, Aaron and Lopez-Paz, David and Bengio, Yoshua},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Manifold Mixup: Better Representations by Interpolating Hidden States},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
modals,
title={{\{}MODALS{\}}: Modality-agnostic Automated Data Augmentation in the Latent Space},
author={Tsz-Him Cheung and Dit-Yan Yeung},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XjYgR6gbCEc}
}


@InProceedings{deepimageprior,
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
title = {Deep Image Prior},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}


@misc{data-aug-in-feature-sapce-canada,
  doi = {10.48550/ARXIV.1702.05538},
  
  url = {https://arxiv.org/abs/1702.05538},
  
  author = {DeVries, Terrance and Taylor, Graham W.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dataset Augmentation in Feature Space},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@ARTICLE{data-augmentation-lecun,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}


  @INPROCEEDINGS{Siamese,
  author={Chen, Xinlei and He, Kaiming},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Exploring Simple Siamese Representation Learning}, 
  year={2021},
  volume={},
  number={},
  pages={15745-15753},
  doi={10.1109/CVPR46437.2021.01549}}

@inproceedings{agirre-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 6: A Pilot on Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1051",
    pages = "385--393"
}

@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2001",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
    abstract = "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017)."
}

@inproceedings{marelli-etal-2014-sick,
    title = "A {SICK} cure for the evaluation of compositional distributional semantic models",
    author = "Marelli, Marco  and
      Menini, Stefano  and
      Baroni, Marco  and
      Bentivogli, Luisa  and
      Bernardi, Raffaella  and
      Zamparelli, Roberto",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf",
    pages = "216--223",
    abstract = "Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes."
}

@misc{supcontrast,
  doi = {10.48550/ARXIV.2004.11362},
  
  url = {https://arxiv.org/abs/2004.11362},
  
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Supervised Contrastive Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
