{
    "arxiv_id": "2303.14537",
    "paper_title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning",
    "authors": [
        "Rickard Br√ºel-Gabrielsson",
        "Tongzhou Wang",
        "Manel Baradad",
        "Justin Solomon"
    ],
    "submission_date": "2023-03-25",
    "revised_dates": [
        "2025-03-07"
    ],
    "latest_version": 4,
    "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
    ],
    "abstract": "Despite dropout's ubiquity in machine learning, its effectiveness as a form of data augmentation remains under-explored. We address two key questions: (i) When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely effective under these conditions? To explore these questions, we propose Deep Augmentation, a network- and modality-agnostic method that applies dropout or PCA transformations to targeted layers in neural networks. Through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning, we find that uniformly applying dropout across layers does not consistently improve performance. Instead, dropout proves most beneficial in deeper layers and can be matched by alternative augmentations (e.g., PCA). We also show that a stop-gradient operation is critical for ensuring dropout functions effectively as an augmentation, and that performance trends invert when moving from contrastive tasks to supervised tasks. Our analysis suggests that Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable issue in self-supervised learning due to the absence of labeled data. Drawing on these insights, we outline a procedure for selecting the optimal augmentation layer and demonstrate that Deep Augmentation can outperform traditional input-level augmentations. This simple yet powerful approach can be seamlessly integrated into a wide range of architectures and modalities, yielding notable gains in both performance and generalization.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14537v1",
        "http://arxiv.org/pdf/2303.14537v2",
        "http://arxiv.org/pdf/2303.14537v3",
        "http://arxiv.org/pdf/2303.14537v4"
    ],
    "publication_venue": null
}