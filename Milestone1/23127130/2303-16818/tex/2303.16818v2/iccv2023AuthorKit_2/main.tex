\pdfoutput=1
\documentclass[10pt,twocolumn]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx} 
\usepackage{float} 
\usepackage{subfigure} 
\usepackage{color}
\usepackage{booktabs}
\definecolor{qmcolor}{RGB}{255,165,0}
\newcommand{\zqm}[1]{{\color{qmcolor}#1}}
\definecolor{red}{RGB}{255,0,0}
\newcommand{\red}[1]{{\color{red}#1}}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy
\def\iccvPaperID{2130} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{BEVSimDet: Simulated Multi-modal Distillation in Bird's-Eye View for Multi-view 3D Object Detection
}

\author{Haimei Zhao$^1$, \quad Qiming Zhang$^1$, \quad Shanshan Zhao$^2$, \quad Jing Zhang$^1$, \quad Dacheng  Tao$^1$
\\
The University of Sydney, Australia \quad $^2$JD Explore Academy, China\\
{\tt\small \{hzha7798, qzha2506\}@uni.sydney.edu.au, jing.zhang1@sydney.edu.au,} \\
{\tt\small\{sshan.zhao00, dacheng.tao\}@gmail.com}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
%%%%%%%%% ABSTRACT
\begin{abstract}

Multi-view camera-based 3D object detection has gained popularity due to its low cost. But accurately inferring 3D geometry solely from camera data remains challenging, which impacts model performance. One promising approach to address this issue is to distill precise 3D geometry knowledge from LiDAR data. However, transferring knowledge between different sensor modalities is hindered by the significant modality gap. In this paper, we approach this challenge from the perspective of both architecture design and knowledge distillation and present a new simulated multi-modal 3D object detection method named BEVSimDet. We first introduce a novel framework that includes a LiDAR and camera fusion-based teacher and a simulated multi-modal student, where the student simulates multi-modal features with image-only input. To facilitate effective distillation, we propose a simulated multi-modal distillation scheme that supports intra-modal, cross-modal, and multi-modal distillation simultaneously. By combining them together, BEVSimDet can learn better feature representations for 3D object detection while enjoying cost-effective camera-only deployment. Experimental results on the challenging nuScenes benchmark demonstrate the effectiveness and superiority of BEVSimDet over recent representative methods. The source code will be released at \href{https://github.com/ViTAE-Transformer/BEVSimDet}{BEVSimDet}.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{startpic.pdf}
\caption{Comparison of our framework with previous methods. (a) Intra-modal distillation between camera-only teacher and student models cannot learn accurate 3D information because due to the limited capacity of the teacher model for inferring 3D geometry. (b) Cross-modal distillation between the LiDAR teacher and Camera student enables learning useful 3D information from the teacher but suffers from the large cross-modal gap. (c) Multi-modal fusion model or multi-modal distillation between the fusion-based teacher and student benefits from both modalities but also requires them during deployment. (d) Our simulated multi-modal distillation enables effective knowledge distillation within/between modalities while enjoying cost-effective camera-only deployment.}
\label{fig:startpic}
\end{figure}

3D object detection is a pivotal technique with extensive applications in fields such as autonomous driving, robotics, and virtual/augmented reality. In recent years, camera-based 3D object detection methods, which infer objects' 3D locations from multi-view images \cite{huang2021bevdet,li2022bevdepth,li2022bevformer}, have gained great attention from both academia and industry due to their high perceptual ability of dense color/texture information and low deployment cost. However, due to the lack of accurate 3D geometry reasoning ability, their detection performance falls largely behind the LiDAR-based methods \cite{yan2018second,yin2021center}, which poses a challenge to the practical deployment of camera-based methods.

To address this issue, researchers attempt to impose LiDAR data to provide accurate 3D geometry information. Some multi-view camera-based methods \cite{li2022bevdepth,li2022bevstereo,wang2022sts} generate ground truth depth from LiDAR point cloud and use it as the supervisory signal for depth estimation to help transform features to the BEV space accurately. Except for directly using LiDAR as supervision, some recent work employs LiDAR information by applying the knowledge distillation (KD) technique~\cite{gou2021knowledge} to improve the detection performance of camera-based methods.

KD-based methods usually leverage the informative features or predictions of a well-trained teacher model to facilitate the learning of the student model. One straightforward approach is intra-modal distillation \cite{li2022bev-lgkd} between a large teacher model and a small student model, as shown in Figure~\ref{fig:startpic} \red{(a)}, which conducts distillation in the image modality.
However, the ceiling performance of the model can be limited since the teacher model infers 3D geometry solely from image data. Another approach is cross-modal distillation, as shown in Figure~\ref{fig:startpic} \red{(b)}, which utilizes LiDAR data as the input of teacher models to transfer 3D knowledge to camera-based students~\cite{chong2022monodistill,chen2022bevdistill,li2022unifying}. The student is usually forced to learn and mimic the output of a LiDAR-based teacher in different representation spaces, including monocular view feature \cite{chong2022monodistill}, BEV feature \cite{chen2022bevdistill}, and voxel feature~\cite{li2022unifying}, depending on the specific architecture of the teacher model. Nevertheless, knowledge distillation between different modalities is hindered by the large cross-modal gap, leading to limited performance gain. To overcome this challenge, one may adopt the multi-modal distillation to guarantee modality consistency, \ie, Figure~\ref{fig:startpic} \red{(c)}, but the requirement of LiDAR data during inference increases deployment cost. 

In this paper, we address these limitations from the perspective of both architecture design and knowledge distillation and present a new 3D object detection method BEVSimDet. We first design a novel framework comprising a LiDAR and camera fusion-based teacher and a simulated multi-modal student. The student model not only involves the camera path of the teacher model but also introduces an additional simulated LiDAR path which is parallel to the camera counterpart, as shown in Figure \ref{fig:startpic} \red{(d)}. Different from other distillation manners in Figure \ref{fig:startpic} \red{(a)} and \red{(b)}, our student model possesses the simulated LiDAR path that simulates LiDAR features from images, thus benefiting the knowledge learning from multiple modalities in the teacher model. Therefore, despite the simulation nature, our student shares a nearly ``identical'' pipeline as the teacher to produce the camera feature, LiDAR feature, fusion feature, and detection predictions. 

Built upon this architecture, we propose a new simulated multi-modal distillation (SimMMD) scheme that supports intra-modal, cross-modal, and multi-modal distillation simultaneously. We adopt the popular MSE loss on the feature representations in the unified BEV space for distillation, while an additional quality-aware prediction distillation \cite{hong2022cross} is employed to further improve the multi-modal fusion features. It is noteworthy that directly transferring knowledge from the LiDAR feature to the simulated LiDAR feature is also challenging due to the cross-modal gap mentioned above. To approach this challenge, we devise geometry compensation modules that collect useful surroundings from the learned locations, offering a geometry regularization. We also introduce an object-aware masking strategy to selectively distill informative features from object regions while avoiding those trivial features in the background. Equipping the proposed model with the SimMMD scheme, our BEVSimDet could effectively learn better feature representations for 3D object detection while enjoying cost-effective camera-only deployment.

The main contribution of this paper is threefold. \textbf{Firstly}, we present a novel model architecture for distillation-based 3D object detection, including a LiDAR and camera fusion-based teacher and a simulated multi-modal student, which facilitates knowledge distillation within and between different modalities while enjoying cost-effective camera-only deployment. \textbf{Secondly}, we propose a novel simulated multi-modal distillation scheme that supports intra-modal, cross-modal, and multi-modal distillation simultaneously. By mitigating the transferring difficulties between modalities, it enables the model to learn better feature representation for 3D object detection. \textbf{Thirdly}, experiments on the challenging nuScenes benchmark \cite{caesar2020nuscenes} show the effectiveness of our BEVSimDet, its superiority over existing state-of-the-art methods, and the potential for practical applications. 
%-------------------------------------------------------------------------

\section{Related Work}

\paragraph{Camera-based 3D Object Detection}
 Monocular 3D object detection methods have been widely studied and made great progress \cite{simonelli2019disentangling,chen2020monopair,reading2021categorical,wang2021fcos3d,zou2021devil,lu2021geometry,ma2021delving,liu2021autoshape,huang2022monodtr} on the KITTI \cite{geiger2012we} benchmark. However, with the release of large-scale datasets with multi-view cameras such as nuScenes \cite{caesar2020nuscenes} and Waymo \cite{sun2020scalability}, there is a growing need for accurate 3D object detection in more challenging scenes.
Recent works adopt the Bird's-eye view (BEV) representation as an ideal feature space for multi-view perception due to its excellent ability to address scale-ambiguity and occlusion issues \cite{huang2021bevdet,huang2022bevdet4d,li2022bevformer}. Various methods have been proposed to transform perspective image features to the BEV space, such as the lifting operation from LSS \cite{philion2020lift} used by BEVDet \cite{huang2021bevdet} and the cross-attention mechanism-based grid queries used by BEVFormer \cite{li2022bevformer}. The camera-based BEVDet approach has been further improved by imposing depth supervision \cite{li2022bevdepth,li2022bevstereo,wang2022sts,chu2023oa} and temporal aggregation \cite{huang2022bevdet4d,park2022time}, resulting in better performance. However, there is still a significant performance gap compared to LiDAR-based and fusion-based counterparts.

 \paragraph{Fusion-based 3D Object Detection}
LiDAR differs from cameras in its ability to capture precise geometric and structural information. However, the data it produces is sparse and irregular, with a large volume. Some methods use PointNet \cite{qi2017pointnet} directly on the raw point cloud \cite{qi2017pointnet++,shi2019pointrcnn} to learn 3D features, while others voxelize the point cloud into pillars \cite{lang2019pointpillars,wang2020pillar,yin2021center} or voxels \cite{zhou2018voxelnet,yan2018second} before extracting features using SparseConvNet \cite{graham20183d}. State-of-the-art techniques \cite{yin2021center,bai2022transfusion} typically transform 3D features into the BEV representation to simplify operations in 3D space, and then feed the resultant features to subsequent detection heads.

Due to their distinct strengths in perceiving the environment, both cameras and LiDAR are commonly integrated using sensor fusion methods to enhance the performance of perception systems. Existing fusion-based approaches can be categorized as input-level methods \cite{qi2018frustum,vora2020pointpainting,wang2021pointaugmenting,xu2021fusionpainting} and feature-level methods \cite{chen2022autoalign,bai2022transfusion,chen2022autoalignv2,yang2022deepinteraction,liu2022bevfusion,liang2022bevfusion}, depending on the stage at which information from different sensors is combined. Recently, it has been proposed that the BEV space is an ideal space for the multi-modal fusion of multi-view camera and LiDAR data, resulting in outstanding performance \cite{liu2022bevfusion,liang2022bevfusion}. These methods follow a simple yet effective pipeline that involves extracting features from both modalities, transforming these features into the BEV space, fusing multi-modal features using fusion modules, and conducting subsequent detection.
 
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=1\linewidth]{mainfigure.pdf}
\end{center}
  \caption{Overall pipeline of our BEVSimDet. It consists of a fusion-based teacher model (top) and a simulated multi-modal student model (bottom). The proposed SimMMD scheme comprises (1) Intra-modal Distillation (IMD) between the camera features of the teacher and student; (2) Cross-modal Distillation (CMD) between the LiDAR feature and Simulated-LiDAR feature. To address the cross-modal gap in CMD, we introduce a Geometry Compensation Module (GCM) combined with the view projection and Object-aware Masking (OAM) strategy; (3) Multi-modal Distillation (MMD) between the fusion features (MMD-F) and predictions (MMD-P) of the teacher and student.}
% \label{fig:long}
\label{fig:mainfig}
\end{figure*}
 
 
\paragraph{Knowledge Distillation in 3D Object Detection}
Knowledge distillation has shown great potential in enabling smaller models (\ie, students) to acquire effective representations by transferring learned knowledge from larger ones (\ie, teachers). Previous research \cite{cho2022itkd,zhang2022pointdistiller,zhang2022structured,yang2022towards} has brought knowledge distillation techniques to the 3D object detection task, \eg, forcing the student network to mimic the features or predictions learned by a teacher within the same modality. Recently, several methods have advanced the field of KD-based 3D object detection by utilizing teachers from a different modality \cite{guo2021liga,chong2022monodistill,li2022bev-lgkd,hong2022cross,li2022unifying,chen2022bevdistill,huang2022tig}, \eg, a LiDAR-based teacher.
UVTR \cite{li2022unifying} unifies the features from both LiDAR and cameras in voxel space, facilitating the application of knowledge distillation. BEVDistill \cite{chen2022bevdistill} transforms the features from both modalities into the BEV space and performs BEV feature distillation and instance-wise prediction distillation. Furthermore, the concurrent works BEV-LGKD \cite{li2022bev-lgkd} and TiG-BEV \cite{huang2022tig} also focus on knowledge distillation in the BEV space. BEV-LGKD uses LiDAR data to guide the distillation process of a large model into a lighter one. TiG-BEV \cite{huang2022tig} incorporates inner-depth supervision and inner-feature distillation to achieve more precise geometry information distillation in the BEV space. These cross-modal distillation approaches demonstrate the effectiveness of transferring knowledge from a strong LiDAR teacher to a camera student. Nevertheless, the large gap between different modalities limits the distillation performance. In addition, these methods neglect the more expressive multi-modal knowledge for distillation.


\section{Methodology}
In this section, we present the details of how the proposed BEVSimDet realizes comprehensive multi-modal knowledge distillation for 3D object detection. We first introduce the model structure, which consists of a multi-modal fusion-based teacher and a simulated multi-modal student. Next, we describe the simulated multi-modal distillation scheme that supports knowledge distillation within and between modalities. Lastly, we present the overall training objectives for our method.

\subsection{Overall Architecture of BEVSimDet}\label{sec:structure}
\paragraph{Multi-modal Teacher}\label{sec:teacher}
To enable the transfer of multi-modal knowledge from the teacher model to the student model, we need a fusion-based 3D detector teacher to encode such knowledge. Specifically, we adopt the state-of-the-art fusion-based method, \ie, BEVFusion \cite{liu2022bevfusion}, as the teacher in this paper. The BEVFusion architecture comprises two branches, as depicted in the top part of Figure~\ref{fig:mainfig}. The LiDAR branch follows the standard pipeline of a LiDAR-based detector \cite{yan2018second,yin2021center}. It uses SparseConvNet \cite{graham20183d} $En^T_{3D}$ to extract the 3D features, and obtains the BEV features $F^{T}_{L_{bev}}$ through vertical dimension reduction ($\rm Flatten$). On the other hand, the camera branch follows the paradigm of BEVDet \cite{huang2021bevdet}, using a 2D feature extractor $En^T_{2D}$ and an efficient projection $\Phi_T$ to transform features from the camera view to the BEV view $F^{T}_{C_{bev}}$. Both modalities' features are then embedded in a unified BEV space using a fully-convolutional fusion module $fuse^T$, which produces the fused BEV features $F^{T}_{U_{bev}}$. Finally, a detection head $head^T$ predicts the objects' bounding boxes and classes $P^{T}$. This process can be formulated as follows:
\begin{equation}
    \begin{split}
        F^{T}_{L_{bev}} &= {\rm Flatten}(En^T_{3D}(L)),\\
        F^{T}_{C_{bev}} &= \Phi_T(En^T_{2D}(I)),\\
        F^{T}_{U_{bev}} &= fuse^T(F^{T}_{L_{bev}}, F^{T}_{C_{bev}}),\\
        P^{T} &= head^T(F^{T}_{U_{bev}}).
    \end{split}
\end{equation} where $L$ and $I$ denotes LiDAR and image input. $T$ and $S$ in the formulations represent the teacher and student models.

\paragraph{Simulated Multi-modal Student}\label{sec:student}
To realize LiDAR-free deployment, \ie, the detectors only need images from multi-view cameras as input, we adopt the BEVFusion's camera branch as the basis for the student model architecture. To mimic the multi-modal fusion pipeline of the teacher model, we make a modification to the network, as shown in the bottom part of Figure~\ref{fig:mainfig}. Specifically, after feature extraction from the 2D encoder $En^S_{2D}$, we devise an extra simulated LiDAR branch in parallel to the camera branch to simulate LiDAR features from images, which are supervised by the LiDAR features from the teacher (Sec.~\ref{sec:CMD}).

In the camera branch, we adopt the popular and efficient view projection $\Phi_C$ in line with the teacher model \cite{philion2020lift,liu2022bevfusion} to transform camera-view features to the corresponding BEV features, \ie, $F^{S}_{C_{bev}}$. During the feature transformation, the extracted 2D feature $F^{S}_{C_{uv}}$ is first feed to a light DepthNet $\Theta$ to predict the depth distribution on each pixel. Then, each 2D feature pixel can be scattered into D discrete points along the camera ray by rescaling the context feature $\Upsilon(F^{S}_{C_{uv}})$ with their corresponding depth probabilities. The resulting 3D feature point cloud is then processed by the efficient BEV pooling operation $\rho$, to aggregate features in BEV grids and obtain the BEV features:
\begin{equation}
F^{S}_{C_{bev}}= \Phi_C(F^{S}_{C_{uv}})  = \rho(\Upsilon(F^{S}_{C_{uv}}) \times \Theta(F^{S}_{C_{uv}})).
\end{equation}

On the other hand, the view projection $\Phi_L$ in the simulated LiDAR branch is combined with a specifically designed geometry compensation module (detailed in Sec.~\ref{sec:compensation}) in both camera-view and BEV spaces. It offers the ability to mitigate the geometry misalignment caused by the modality gap during distillation. After obtaining BEV features from two branches $F^{S}_{L_{bev}},F^{S}_{C_{bev}}$, we use the fusion module $fuse^S$ to obtain the multi-modal fusion features $F^S_{U_{bev}}$. Then, the detection head $head^S$ is exploited to yield the final detection results $P^{S}$. Both the fusion module and detection head have the same architecture as the teacher. This process can be formulated as:
\begin{equation}
    \begin{split}
    F^{S}_{C_{uv}} = &En^S_{2D}(I),\\
    F^{S}_{L_{bev}} = \Phi_L(F^{S}_{C{uv}}),&\quad
    F^{S}_{C_{bev}} = \Phi_C(F^{S}_{C{uv}}),\\
    F^S_{U_{bev}} = &fuse^S(F^{S}_{L_{bev}}, F^{S}_{C_{bev}}),\\
    P^{S} = &head^S(F^S_{U_{bev}}).
    \end{split}
\end{equation}
Thanks to the simulated LiDAR branch, the student model can learn features from multiple modalities without equipping a real LiDAR. In the next part, we will detail how this architecture facilitates effective knowledge distillation within and between modalities, including intra-modal, cross-modal, and multi-modal distillation.

\subsection{Simulated Multi-Modal Distillation} \label{sec:simMMD}
To better utilize the abundant knowledge encoded by different modal features of the teacher model, we propose a new simulated multi-modal distillation scheme. The technical details will be presented as follows.

\subsubsection{Intra-modal Distillation (IMD)}\label{sec:IMD}
Since both the teacher and student models take images as input, a straightforward strategy is to align the image BEV features from the camera branch of both models, which we name as intra-modal distillation.
Specifically, we leverage the BEV feature of the teacher $F^{T}_{C_{bev}}$ as the supervisory signal for learning the counterpart of the student $F^{S}_{C_{bev}}$ via an MSE loss, \ie,
\begin{equation}
     \mathcal L_{IMD} = {\rm MSE}(F^T_{C_{bev}}, F^S_{C_{bev}}).
\end{equation}
Due to the same modality in IMD, the student model can be trained directly through the above distillation objective to gain useful visual domain knowledge to facilitate 3D object detection performance. However, relying on images alone may not provide enough geometry-related information to help detect target objects. To address this limitation, we implement cross-modal distillation on the proposed simulated LiDAR branch in the student model, enabling it to gain knowledge from the LiDAR modality.

\subsubsection{Cross-modal Distillation (CMD)}\label{sec:CMD}
CMD aims to align the LiDAR BEV features of the teacher and the simulated LiDAR BEV features of the student. While images usually produce dense features, LiDAR data are very sparse. For example, only around 5\% of camera features will be matched to a LiDAR point in the BEV space for a typical 32-beam LiDAR \cite{liu2022bevfusion}. As a result, directly applying the distillation loss in a point-to-point correspondence manner may lead to an incorrect mimic of the noisy features and inaccurate 3D geometry representation for the simulated LiDAR branch, which significantly affects the detection performance. To mitigate this issue, we propose a geometry compensation module (GCM) in view projection and an object-aware masking (OAM) strategy.

\paragraph{Geometry Compensation Module (GCM)}\label{sec:compensation}
A crucial process in the multi-view camera-based detection method is the view projection operation, which transforms camera-view (UV) features into the BEV space. In the simulated LiDAR branch, to obtain accurate 3D geometry information and align with the LiDAR information from the teacher, we propose to use two GCMs before and after the feature transforming process to learn better geometry features in both UV and BEV views. Technically, the GCM is constructed by stacking several deformable self-attention \cite{zhu2020deformable} layers.
For geometry compensation in the UV view, we first generate a uniform grid of points $p_{uv} \in \mathbf{R}^{H_{uv}\times W_{uv} \times 2}$ for each 2D camera features $F^{S}_{C_{uv}} \in \mathbf{R}^{H_{uv}\times W_{uv} \times N}$. Then, following \cite{xia2022vision}, we learn offsets based on each point $(u,v)\in p_{uv}$ to generate a set of most related points around it. These learned points, together with reference points $(u,v)\in p_{uv}$, are used to sample the key and value features from the 2D camera features. With the optimization signals gradually improving attentive locations, the module facilitates geometry correction in the x-y plane. Besides, we follow the practice of standard multi-head attention to learn individual offsets for each head. The abundant information captured in various heads collaboratively improves the feature representations and benefits the subsequent context learning, depth estimation, and 3D geometry inference.

Similarly, we employ a BEV geometry compensation module after transforming the camera-view features to the BEV space, which is responsible for correcting the key feature locations in the x-z plane. By doing so, the geometry compensation in the two complementary 2D views can comprehensively improve the feature representation. Overall, the view projection with GCM used in the simulated-LiDAR branch can be formulated as follows:
\begin{equation}
\begin{split}
    F^{S}_{L_{bev}}&= \Phi_L(F^{S}_{C_{uv}}) \\
    &= GC_{bev}(\rho(\Upsilon(GC_{uv}(F^{S}_{C_{uv}})) \times \Theta(GC_{uv}(F^{S}_{C_{uv}})))),
\end{split}
\end{equation} 
where $GC_{uv}$ and $GC_{bev}$ denote the UV Geometry Compensation and the BEV Geometry Compensation, respectively.

\paragraph{Object-Aware Masking (OAM)}\label{sec:mask}
While GCM offers 3D geometry refinement, the modality gap caused by the fundamental differences between the two kinds of sensors still remains. The sparsity of LiDAR data, particularly in the background context which is irrelevant to object perception, hinders effective knowledge distillation. To address this issue, we propose a simple yet effective object-aware masking strategy to select the most informative features for object detection. 

Inspired by CenterPoint \cite{yin2021center}, we generate masks in the BEV space from the ground truth center points and bounding boxes using a heatmap-like approach. For each category with $N$ objects, we first produce the heatmap $H$ by aggregating the Gaussian distributions over each object center:
\begin{equation}
    H(x,y) = \sum^{N}_i exp(-\frac{(x-\overline{x}_i)^2+(y-\overline{y}_i)^2}{2\sigma^2}),
\end{equation}
where $\overline{x}$ and $\overline{y}$ denote the coordinates of the object center.
We set $\sigma = max(f(h,w), \tau)$, where $\tau = 2$ is the smallest allowable value and $f$ is a function to decide the value of $\sigma$ according to the object size, following CenterPoint \cite{yin2021center}.
To ensure that all categories are well-presented, we sum the heatmaps of all categories as the overall heatmap $H$.
Then we use the ground truth bounding boxes to generate a binary map $B$. It filters out the regions outside the bounding boxes and obtains the final object-aware mask $M_o$ by element-wise multiplication with the overall heatmap $H$: 
\begin{equation}
    M_o = B \odot H.
\end{equation} 
The resulting object-aware mask $M_o$ isolates the adverse effects of irrelevant sparse background information during knowledge distillation and indicates the most informative features in the BEV space. Thus, the CMD loss can be defined as follows:
\begin{equation}
    \begin{split}
        \mathcal L_{CMD} &= M_o \odot {\rm MSE}(F^{T}_{L_{bev}}, F^{S}_{L_{bev}}). \\
    \end{split}
 \end{equation}

\subsubsection{Multi-modal Distillation (MMD)}\label{sec:MMD}
Like the teacher model, the student model also produces multi-modal fusion features as well as detection predictions, although relying on simulated LiDAR features and camera features. To make the fused feature and predictions align with those in the teacher model, we devise multi-modal distillation in both feature level (MMD-F) and prediction level (MMD-P). Due the proximity of the fusion module and the detection head, MMD-F aims to distill the useful multi-modal knowledge that directly contributes to the detection, \ie, by aligning the fusion feature of the teacher model and the simulated fusion feature of the student model:
\begin{equation}
     \begin{split}
         \mathcal L_{MMD-F} &= {\rm MSE}(F^T_{U_{bev}}, F^S_{U_{bev}}). \\
     \end{split}
 \end{equation}

After the fusion module, the fused feature in the student model is fed into the detector to output the detection results in the same way as the teacher model. Thus, except for the feature-level distillation, we also employ MMD-P by taking the predictions from the teacher model as soft labels. We adopt the quality-aware prediction distillation loss $L_{MMD-P}$ presented in CMKD \cite{hong2022cross}, which consists of the classification loss $\mathcal L_{cls}$ for object categories and the regression loss $\mathcal L_{reg}$ for 3D bounding boxes:
\begin{equation}
\begin{split}
    \mathcal L_{MMD-P} &= \mathcal L_{reg} + \mathcal L_{cls},\\
        &={\rm SmoothL1}(P^T_B,P^S_B) \cdot s + {\rm QFL}(P^T_C,P^S_C) \cdot s,
\end{split}
\end{equation}
where $P^T_B$ and $P^T_C$ (resp. $P^S_B$ and $P^S_C$) denote the predicted bounding boxes and categories by teacher model (resp. the student model). $QFL(\cdot)$ denotes the quality focal loss~\cite{li2020generalized}. $s$ is a quality score used as the loss weight, obtained by measuring the IoU between the predictions and the ground truth to determine the confidence of the teacher's prediction.

\subsection{Training Objective}
\label{sec:loss}
As described above, the proposed SimMMD scheme supports IMD, CMD, and MMD simultaneously, which effectively transfers knowledge encoded by different modality features from the multi-modal fusion-based teacher model to the simulated multi-modal student model. Apart from the above distillation losses, the student model is also optimized by the common loss of 3D object detection task $\mathcal{L}_{det}$. The overall training objective $L$ is defined as:
\begin{equation}
    \mathcal{L} = \mathcal L_{IMD} + \mathcal L_{CMD}+ \mathcal L_{MMD-F}+ \mathcal L_{MMD-P} + \mathcal{L}_{det}.
\end{equation}


\begin{table*}[!htp]
\footnotesize
\caption{Quantitative comparisons on the nuScenes validation Set. L and C in the second column denote the input modality, \ie, LiDAR and camera, while C* means using LiDAR for depth supervision or knowledge distillation during training. T and S in the Backbone column denote the setting of the teacher and student models, respectively. Frames denotes the number of temporal frames used during inference.}
\setlength{\tabcolsep}{0.0092\linewidth}
\centering
\begin{tabular}[t]{c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
Methods &Modality &Frames & Backbone& Image Size & mAP$\uparrow$ &NDS$\uparrow$ &mATE$\downarrow$ &mASE$\downarrow$ &mAOE$\downarrow$ &mAVE$\downarrow$ &mAAE$\downarrow$ \\
\midrule
CenterPoint\cite{yin2021center}&L & 1&VoxelNet &-  & 59.6 &66.8 & - & -&- & -&- \\
BEVFusion \cite{liang2022bevfusion}&LC & 1&VoxelNet SwinT &$448\times 800$   & 67.9 &71.0 & -& -& - & -&-  \\

DeepInteraction \cite{yang2022deepinteraction}&LC &1&VoxelNet R50 &$450\times 800$ & 69.9 &72.6 & -& -&- && -- \\
BEVFusion \cite{liu2022bevfusion}&LC& 1 &VoxelNet SwinT&$256\times 704$  & 68.5 &71.4 &28.6  &25.3&30.0&25.4&18.6\\
  \midrule
  \midrule
  
FCOS3D \cite{wang2021fcos3d}&C & 1&R101 &$900\times 1600$&  29.5 &37.2 & 80.6&26.8& 51.1&113.1 &17.0 
  \\
 
 BEVFormer \cite{li2022bevformer} &C& 3&R101 &$900\times 1600$&  41.6 &51.7 & 67.3&27.4& 37.2&39.4 &19.8 \\
 
PolarDETR-T \cite{chen2022polar}&C&2&R101 &$900\times 1600$&  38.3
 & 48.8&70.7&26.9&34.4&51.8&19.6\\
  
 PolarFormer \cite{chen2022polar} &C&2&R101 &$900\times 1600$&  43.2
 & 52.8&64.8&27.0&34.8&40.9&20.1\\

 BEVDet \cite{huang2021bevdet} &C& 1& R50 &$256\times 704$ &  29.8 & 37.9 & 72.5 &27.9 &58.9 &86.0 &24.5\\
  
   PETR \cite{liu2022petr} &C&1& R50 &$384\times 1056$&   31.3 &38.1 &76.8 &27.8 &56.4 &92.3 &22.5 \\
 
DETR3D \cite{huang2022bevdet4d} &C& 2&R101 &$900\times 1600$&   34.9 &43.4 & 71.6&26.8& 37.9&84.2 &20.0 
 \\
 \hline
 BEVDepth \cite{li2022bevdepth} &C* &2& R50 &$256\times 704$  &35.1 &47.5&63.9
 &26.7&47.9&42.8&19.8
 \\
STS \cite{wang2022sts}&C*&2 & R50 &$256\times 704$  &37.7 &48.9&60.1
 &27.5&45.0&44.6&21.2
\\
BEVStereo \cite{li2022bevstereo} &C*&2& R50 &$256\times 704$ &37.2 &50.0&59.8
 &27.0&43.8&36.7&19.0\\
 SOLOFusion \cite{park2022time} &C*&16& R50 &$256\times 704$ & 42.7 &53.4&56.7
 &27.4&41.1&25.2&18.8\\
\midrule
 \midrule
  BEV-LGKD \cite{li2022bev-lgkd}&C*&1& T: R101 S: R18 &$256\times 704$  &30.9 &44.9&64.6
 &27.6&40.6&41.2&19.0\\
 \hline
Set2Set \cite{li2022unifying}&C*&1& R50 &$900\times 1600$ &33.1 &41.0&-
 &-&-&-&-\\
  MonoDistill \cite{chong2022monodistill}&C*&1& R50 &$900\times 1600$ &36.4 &42.9&-
 &-&-&-&-\\
 UVTR \cite{li2022unifying}&C*&1& R50 &$900\times 1600$ &36.2 &43.1&-
 &-&-&-&-\\
 BEVDistill \cite{chen2022bevdistill}&C*&1& R50 &$900\times 1600$ &38.6 &45.7&69.3
 &26.4&39.9&80.2&19.9\\

TiG-BEV \cite{huang2022tig}&C*&1& R50 &$256\times 704$ & 33.1 &41.1&67.8
 &27.1&58.9&78.4&21.8\\
 UniDistill \cite{zhou2023unidistill} &C*&1& R50 &$256\times 704$ &26.5&37.8&-&-&-&-&-\\
 BEVFusion-C \cite{liu2022bevfusion} &C&1& SwinT &$256\times 704$ & 35.5 &41.2&66.8
 &27.3&56.1&89.6&25.9\\
 Ours &C*&1& SwinT &$256\times 704$ & \textbf{40.4} &\underline{45.3}&52.6
 &27.5&60.7&80.5&27.3\\
  Ours &C*&1& ViTAEv2-S &$256\times 704$ & \underline{40.1} &\textbf{46.3}&51.1
 &27.4&56.5&77.8&24.3\\
 \bottomrule
\end{tabular}
\label{tab:maintable}
\end{table*}


\section{Experiment}
\subsection{Experiment Setting}
\paragraph{Datasets and Evaluation Metrics} We follow the common practice \cite{huang2021bevdet,liu2022bevfusion,liang2022bevfusion,li2022bevdepth,huang2022bevdet4d,chen2022bevdistill,li2022bevstereo} to evaluate our method on the most challenging benchmark, \ie, nuScenes \cite{caesar2020nuscenes}. It comprises 700 scenes for training, 150 scenes for validation, and 150 scenes for testing. Each scene includes panoramic LiDAR data and surrounding camera images, which are synchronized to provide convenience for multi-modal-based research. The dataset comprises a total of 23 object categories, and 10 popular classes are considered for computing the final metrics. To align with official evaluation code, we adopt \textbf{mAP} and \textbf{NDS} as the main metrics. Other metrics like mATE, mASE, mAOE, mAVE, and mAAE are also reported for reference. We don't use test-time augmentation and model ensembling in all evaluations.


\paragraph{Implementation Details} 
Our method is implemented with PyTorch using 8 NVIDIA A100 (40G Memory), based on the MMDetection3D codebase \cite{mmdet3d2020}. We adopt BEVFusion \cite{liu2022bevfusion} as the default teacher model, which takes images with a size of $256\times704$ and LiDAR point cloud with a voxel size of (0.075m, 0.075m, 0.2m) as input and uses VoxelNet \cite{zhou2018voxelnet} and Swin-T \cite{liu2021swin} as backbones for the two modalities, respectively. During distillation, we utilize the official BEVFusion checkpoint, freeze the teacher model, and train the student model for 20 epochs with batch size 24. The backbone of the student model and the input image size keep the same as the camera branch of the teacher model (\ie, BEVFusion-C) (see Supplementary Material for details of architectures and implementation).



\subsection{Main Results}
We compare our BEVSimDet with state-of-the-art methods on the nuScenes validation set and present the results in Table~\ref{tab:maintable}. We group the methods according to the input modality and present the knowledge distillation-based methods in the last six rows (except BEVFusion-C) for straightforward comparisons. From the table, we can see the methods using LiDAR or fusion data as input usually possess a stronger perception ability and achieve better performance than camera-based methods. However, the high cost of LiDAR may restrict their practical use in the real world. Compared with the baseline BEVFusion-C, our BEVSimDet boosts the performance significantly by 4.84\% mAP and 4.1\% NDS, clearly validating the effectiveness of the proposed distillation method. Although other camera-based methods using LiDAR data for depth supervision and temporal integration \cite{li2022bevdepth,li2022bevstereo,wang2022sts} could also achieve good performance, the superior performance indicated by mAP demonstrates that the proposed SimMMD scheme provides a competitive alternative solution in utilizing multi-modal knowledge from the teacher model and delivers better detection performance. 
Compared with the strongest competitor BEVDistill \cite{chen2022bevdistill}, our BEVSimDet uses much smaller images (\ie, 87.5\% less pixels) to obtain a significant gain of 1.8 mAP and an on-par performance of 45.3 NDS, demonstrating the superiority of the proposed student architecture and SimMMD distillation scheme.

\begin{table}[t]
\footnotesize
\caption{Ablation study of the model architecture. CMD is the vanilla version without GCM and OAM here.}
\setlength{\tabcolsep}{0.0092\linewidth}
\centering
\begin{tabular}[t]{c|c|c|c|c|c}
\toprule
 & Teacher & Student & Distillation & mAP$\uparrow$ & NDS$\uparrow$ \\
\midrule
a&BEVFusion&BEVFusion-C&MMD-F &35.94&41.75 \\
b&BEVFusion&BEVSimDet &MMD-F&38.34&44.15 \\
c&BEVFusion-L&BEVFusion-C &CMD&35.88&42.87 \\
d&BEVFusion-L&BEVSimDet &CMD&36.80&42.79 \\
 \bottomrule
\end{tabular}

\label{tab:architecture}
\end{table}



\subsection{Ablation Studies}
To validate the effectiveness of each component in BEVSimDet, we conduct comprehensive experiments to ablate the structure and distillation method, respectively.

\paragraph{Why choose multi-modal architectures?}~\\
To demonstrate the superiority of the proposed simulated multi-modal structure,
we replace the multi-modal teacher (BEVFusion) and the simulated multi-modal student (BEVSimDet) with their single-modal counterpart (BEVFusion-L and BEVFusion-C), respectively. The results are presented in Table \ref{tab:architecture}. 
 
We first investigate the influence of using a simulated multi-modal student. In model (a) and (b), we adopt the multi-modal teacher (BEVFusion) but distill the fusion feature to different student architectures. The experiment results show that the multi-modal student (b) outperforms the single-modal one (a) with a clear gain of 2.4 in both mAP and NDS. We then change the teacher to a single-modal one (BEVFusion-L) to verify the performance of the student. Although directly learning from a cross-modal teacher adversely affects performance due to the modality gap, the multi-modal student (d) still achieves better performance in mAP and comparable results in NDS compared with the single-modal student (c). The two groups of comparisons validate the superiority of using a multi-modal student. Besides, The experiments of (b) and (d) also validate the importance of using a multi-modal teacher, \ie, with a gain of 1.54\% mAP and 1.36\% NDS.
\begin{table}[t]
\footnotesize
\caption{Ablation study of each component in BEVSimDet on the nuScenes validation Set. SMS: simulated multi-modal architecture. OAM: object-aware masking. GCM: geometry compensation module. CMD, IMD, MMD (including MMD-F and MMD-P) are the distillation methods in SimMMD (Sec.~\ref{sec:simMMD}).}
\setlength{\tabcolsep}{0.0038\linewidth}
\centering
\begin{tabular}[t]{c|c|c|c|c|c|c|c}
\toprule
&SMS&CMD  & IMD & MMD&mAP$\uparrow$ &NDS$\uparrow$ &Improvement\\
\hline
a&    &  &&& 35.56 &41.21&- \\
b&$\checkmark$   &&&& 35.71 &41.97&- \\
\hline
c&$\checkmark$ & w/o GCM$\&$OAM &  &&  36.80 &42.79  &+1.09 / +0.82 \\

d&$\checkmark$ & w/o OAM&  &&  38.90 &44.18  &+3.19 / +2.21 \\
e&$\checkmark$ & w/o GCM&  &&  37.16 &44.18  &+1.45 / +2.21 \\
f&$\checkmark$ & $\checkmark$   && &39.83  &44.79  &+4.12 / +2.82 \\
\hline
g&$\checkmark$& &  $\checkmark$ & & 37.14 &42.67  &+1.43 / +0.70 \\
 \hline
h&$\checkmark$& &   & w/o MMD-P & 38.34 &44.15  &+2.63 / +2.18 \\
i&$\checkmark$& &   & w/o MMD-F& 36.73 &42.52  &+1.02 / +0.55 \\
m& $\checkmark$   &$\checkmark$ &$\checkmark$   &$\checkmark$ & 40.40 &45.31   &+4.69 / +3.34 \\
 \bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

\paragraph{How Simulated Multi-modal Distillation Works?}~\\
In order to explain the specific impact of each component in SimMMD, we perform separate experiments for each distillation module and summarize their results in Table \ref{tab:ablation}. Model (a) denotes the camera-only baseline while model (b) denotes the proposed simulated multi-modal architecture (denoted as SMS) without knowledge distillation. We present the relative gains over the model of (b) in the last column. 
As shown in (c), (g), (h), and (i), employing CMD, IMD, MMD-F, and MMD-P on the baseline model leads to 1.09\%, 1.43\%, 2.63\%, and 1.02\% absolute gains in mAP, respectively, where MMD-F brings the largest gain owing to the rich multi-modality knowledge contained in the fusion features. Interestingly, while the simulated LiDAR branch should process more accurate 3D geometry than the camera branch, IMD (g) produces a slightly larger gain than CMD (c). We attribute it to the cross-modal gap between LiDAR features and simulated LiDAR features, and thus propose the GCM and OAM to improve the performance of CMD.
~\\
~\\
\noindent\textbf{How GCM and OAM improve CMD?} ~\\We ablate the two techniques used in CMD for addressing the cross-modal gap here.
In Table \ref{tab:ablation} (d), we can see that GCM helps CMD achieve a gain of 3.19\% mAP and 2.21\% NDS over the vanilla version in (c), validating the effectiveness of GCM in overcoming the side effect of the cross-modal gap during distillation. Besides, in (e), OAM also helps CMD obtain a performance improvement of 1.45\% mAP and 2.21\% NDS, demonstrating the importance of suppressing noisy background features when transferring knowledge. Combining GCM and OAM together delivers better performance in (f), implying the complementary of the two techniques in solving the cross-modal gap issue. After incorporating all the components together, we get our BEVSimDet model in (m), which delivers the best performance of 40.4 mAP and 45.31 NDS.

\subsection{Model Efficiency}
We compare the model efficiency with other representative methods in Table \ref{tab:efficiency}. Our method achieves a fast inference speed with 11 FPS on a single GPU, running much faster than BEVDistill and BEVFormer and comparable to BEVDet. Meanwhile, it outperforms BEVDet and BEVFormer significantly in terms of mAP and NDS. Compared with the strong competitor BEVDistill, our BEVSimDet has 4$\times$ fewer FLOPs, 5$\times$ higher inference speed, and significantly better mAP performance, demonstrating its great potential in practical applications.
\begin{figure}[t]
\begin{center}
  \includegraphics[width=\linewidth]{visualization.pdf}
\end{center}
  \caption{Visualization of prediction results and BEV features.}
\label{fig:visualization}
\end{figure}

\subsection{Qualitative Analysis}
In the top row of Figure \ref{fig:visualization}, we visualize the detection results of the baseline and our method. The camera-only baseline model produces many false positives, which can be effectively addressed by our method. Besides, the visualization of BEV features learned by different branches of our simulated multi-modal student is also presented in the bottom row. The camera branch and the simulated LiDAR branch learn distinct features, implying their complementary roles in helping deliver better detection performance. More results can be found in Supplementary Materials.

\begin{table}[t]
\footnotesize
\caption{Comparison of model efficiency. $\dagger$ denotes our re-implementation following the settings in the paper.}
\centering
\begin{tabular}[t]{c|ccc|cc}
\toprule
Methods&Mod&FPS &GFlops &mAP&NDS\\
\midrule
BEVFusion \cite{liu2022bevfusion}&LC&8.4 &234.4&68.5&71.4\\
\hline
BEVDet \cite{huang2021bevdet}&C&15.6&215.3&31.2&39.2\\
BEVFormer \cite{li2022bevformer}&C&2.4&1303.5&37.5&44.8 \\
BEVDistill$\dagger$ \cite{chen2022bevdistill}&C&2.8&1128.3&38.6&45.7 \\
BEVSimDet&C&11.1&219.1&40.2&45.3\\
 \bottomrule
\end{tabular}
\label{tab:efficiency}
\end{table}
% %-------------------------------------------------------------------------

\section{Conclusion}
In this paper, we propose a novel 3D object detection method named BEVSimDet by carefully investigating the architecture design and effective distillation method. We identify the importance of the multi-modal architecture for knowledge distillation and devise a simulated multi-modal model accordingly. Built upon it, we develop a novel simulated multi-modal distillation scheme that supports intra-modal, cross-modal, and multi-modal knowledge distillation. Experiments on the challenging nuScenes benchmark have validated the above findings and the superiority of the proposed distillation methods over representative competitors. BEVSimDet enjoys cost-effective deployment without the need for LiDAR during inference, showing great potential in practical applications, such as autonomous driving.
\section{Supplementary}
\begin{table}[!htp]
\footnotesize
\caption{Quantitative comparisons on the nuScenes validation Set. C in the Mod column mean models' input modality is camera-only, while C* means imposing LiDAR as the depth supervision or used for knowledge distillation during training. TF denotes the temporal frames used during inference. Swin-T and ViTAEv2-S are backbone networks with similar quantities of parameters with ResNet-50 (R50).}
\setlength{\tabcolsep}{0.01\linewidth}
\centering
\begin{tabular}[t]{c|c|c|c|c|c|c}

  \toprule
Methods &Mod &TF & Backbone& Image Size & mAP$\uparrow$ &NDS$\uparrow$ \\
\midrule
 Baseline \cite{liu2022bevfusion} &C&1& R50 &$256\times 704$ & 31.6 &39.3\\
Baseline \cite{liu2022bevfusion} &C&1& Swin-T &$256\times 704$ & 35.5 &41.2\\
 
 Ours &C*&1& R50 &$256\times 704$ &37.3  &43.8\\
 Ours &C*&1& Swin-T &$256\times 704$ & \textbf{40.4} &\underline{45.3}\\
  Ours &C*&1& ViTAEv2-S &$256\times 704$ &\underline{40.1} &\textbf{46.3}\\
 \bottomrule
\end{tabular}
\label{tab:backbone}
\end{table}
\subsection{Additional Quantitative Comparison}
To further evaluate our approach, we conducted additional quantitative experiments using different backbones, including ResNet-50 \cite{he2016deep} and ViTAEv2-S \cite{zhang2023vitaev2}, as shown in Table \ref{tab:backbone}. Swin-T and ViTAEv2-S have similar parameter quantities as ResNet-50, but they exhibit stronger representation abilities, resulting in superior performance.
\begin{table}[t]
\footnotesize
\caption{Quantitative comparisons on the nuScenes val Set.}
\setlength{\tabcolsep}{0.0092\linewidth}
\centering
\begin{tabular}[t]{c|c|c|c}
\toprule
&Methods &mAP$\uparrow$ &NDS$\uparrow$ \\
\midrule
a&DeformAttn &38.90&44.18 \\
b&DeformConv&37.69&43.25 \\
 \bottomrule
\end{tabular}
\label{tab:deformchoice}
\end{table}

\subsection{Additional Qualitative Analysis}
\subsubsection{Prediction Results}
To demonstrate the performance of BEVSimDet, we visualize the prediction results in both LiDAR TOP view (Figure \ref{fig:lidar}) and surrounded image view (Figure \ref{fig:surround}). The green and red boxes represent the predictions and the ground truth, respectively. As shown in the figures, our predictions are very close to the ground truth.

\begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{ supplementary-lidar.pdf}
\end{center}
  \caption{Visualization of the detection results inferred by BEVSimDet, on LiDAR top view. The green and red boxes represent the prediction and ground truth, respectively.}
% \label{fig:long}
\label{fig:lidar}
\end{figure*}
 \begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{ supplementary-pre1.pdf}
  \\
  ~\\
  \includegraphics[width=\linewidth]{ supplementary-pre2.pdf}
\end{center}
  \caption{Visualization of the detection results inferred by BEVSimDet, on both LiDAR top view and surrounded image view. The green and red boxes represent the prediction and ground truth, respectively.}
% \label{fig:long}
\label{fig:surround}
\end{figure*}


\subsubsection{Feature Map}
To validate the effectiveness of BEVSimDet, we visualize the BEV feature maps learned in different stages, including the BEV features of the camera branch, the BEV features of the simulated LiDAR branch, and the simulated fusion features.

As shown in Figure \ref{fig:featuremap}, the camera branch learns frustum-shape features, identifying the significant area and assigning the representation focus along the camera ray, while the features learned by the simulated LiDAR branch emphasize more concise regions. The simulated fusion features aggregate the features of the two branches and enable more precise identification of object positions that correspond to the prediction and ground truth. The visualization of features demonstrates that the two branches do learn different and complementary information and the fusion features learn more precise and informative representations for detection.


In Figure \ref{fig:featuremap}, we also visualize the simulated LiDAR feature without GCM and OAM. Simulated LiDAR features without GCM and OAM are similar to the features learned by the camera branch, in the frustum shape. Compared to them, features with GCM but without OAM can focus more on precise object regions but with much irrelevant context areas for detection. 


\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.9\linewidth]{ supplementary-feature1.pdf}
  \includegraphics[width=0.9\linewidth]{ supplementary-feature2.pdf}
\end{center}
  \caption{Visualization of BEV feature maps. BEV feature-cam, BEV feature-SimLiDAR and BEV feature-SimFusion means the BEV features of the camera branch, the simulated LiDAR branch and the fusion ones. The camera branch identifies significant areas and assigns the representation focus along the camera ray. The simulated LiDAR branch emphasizes more concise regions. The simulated fusion features combine features from both branches to enable more precise identification of object positions that correspond to the prediction and ground truth. SimLiDAR w/o GCM\&OAM means the BEV features of the simulated LiDAR branch of the model without GCM and OAM. SimLiDAR w/o OAM means the BEV features of the simulated LiDAR branch of the model without OAM but with GCM.}

\label{fig:featuremap}
\end{figure*}







\subsubsection{Analysis of GCM}
 

To enable geometry compensation, we utilize the multi-head deformable attention layers in GCM, which optimize attentive locations and corresponding attention weights during training to facilitate geometry correction and improve representations. To assess the impact of GCM, we conduct an ablation experiment implementing GCM with deformable convolutions, as shown in Table \ref{tab:deformchoice}. While the deformable convolution can attend to various relative regions and improve representations, the evaluation results demonstrate the superiority of multi-head deformable attention operations.


To further validate the effect of GCM, we visualize the attention maps in Figure \ref{fig:gcm}. The first two columns show the attention maps learned in UV GCM, where areas with large attention weights align with the object regions in camera views. The last two columns are attention maps learned in the BEV GCM, which also focus primarily on areas with objects in the BEV space.


 \begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{bevgcm.pdf}
\end{center}
  \caption{Visualization of the attention maps generated from GCM. The first two columns show the camera-view attention map learned by UV GCM, which mainly attends to the object regions in camera views. The last two columns show the BEV attention map learned by BEV GCM, which primarily focuses on areas with objects in the BEV space.}

\label{fig:gcm}
\end{figure*}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{iccv2023AuthorKit/main}
}

\end{document}