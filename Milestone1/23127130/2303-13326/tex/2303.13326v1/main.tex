\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
\usepackage{amssymb,amsfonts,amsthm}
\usepackage[linkcolor=blue,colorlinks=true, citecolor = blue]{hyperref}
\usepackage{amsmath}               
  {
      \theoremstyle{plain}
      \newtheorem{assumption}{Assumption}
  }
\usepackage{algorithm}
\usepackage{algpseudocode}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\usepackage{graphicx}
\usepackage[misc]{ifsym} 
\usepackage{subfigure} 
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{booktabs} 
\usepackage{diagbox}
\usepackage{verbatim}
\usepackage{setspace}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\allowdisplaybreaks[4]
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\title{Decentralized Adversarial Training over Graphs}
%\footnotesize \textsuperscript{\star}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{
%Emails: \{ying.cao, elsa.rizk, ali.sayed\}@epfl.ch, s.vlaski@imperial.ac.uk}


\author{Ying~Cao,  Elsa~Rizk,  Stefan~Vlaski, \IEEEmembership{Member,~IEEE,} Ali~H. Sayed, \IEEEmembership{Fellow,~IEEE}
\thanks{A short version of this work without proofs and focusing only on the convex case appears in the conference publication \cite{ying_icassp2023}.}
\thanks{Ying Cao, Elsa Rizk, Ali H. Sayed are with the  School of Engineering, École Polytechnique Fédérale de Lausanne. Stefan Vlaski is with the Department of Electrical and Electronic Engineering, Imperial College London. Emails: \{ying.cao, elsa.rizk, ali.sayed\}@epfl.ch, s.vlaski@imperial.ac.uk.}


}
%\text{$^\star${School of Engineering}, École Polytechnique Fédérale de Lausanne}}
%\IEEEauthorblockA{\text{$^\dagger$Department of Electrical and Electronic %Engineering, Imperial College London}}
%\IEEEauthorblockA{\text{Emails: \{ying.cao, elsa.rizk, ali.sayed\}@epfl.ch, s.vlaski@imperial.ac.uk}}



\maketitle

\begin{abstract}

The vulnerability of machine learning models to adversarial attacks has been attracting considerable attention in recent years. Most existing studies focus on the behavior of stand-alone single-agent learners. In comparison,  this work studies adversarial training over graphs, where individual agents are subjected to perturbations of varied strength levels across space. It is expected that interactions by linked agents, and the heterogeneity of the attack models that are possible over the graph, can help enhance robustness in view of the coordination power of the group. Using a min-max formulation of diffusion learning, we develop a decentralized adversarial training framework for multi-agent systems. We analyze the convergence properties of the proposed scheme for both convex and non-convex environments, and illustrate the enhanced robustness to adversarial attacks.
%Adversarial training is one popular strategy to improve the robustness of learning systems. In this work, we propose an adversarial diffusion training framework in which agents from a multi-agent system cooperate locally with their neighbors.
%In theory, it can be verified that all agents reach consensus after enough iterations. Furthermore, we prove the convergence of the diffusion based algorithm. The performance of the proposed approach is demonstrated by the experimental results, in which we show that local collaboration among agents enhances the robustness of multi-agent systems than the non-cooperative ones in the adversarial environment.
\end{abstract}

\begin{IEEEkeywords}
  robustness, adversarial training, decentralized setting, diffusion strategy, multi-agent system.
\end{IEEEkeywords}

\section{Introduction}
%background:(1) single-agent adversarial training; (2) decentralized processing. try to motivate the decentralized adversarial training.
In many machine learning algorithms, small malicious perturbations that are imperceptible to the human eye can cause classifiers to reach erroneous conclusions  \cite{szegedy2013intriguing, BaiL0WW21,sayed_2023}. This sensitivity is problematic for important applications, such as computer vision \cite{song2017pixeldefend, HendrycksZMTLSS22}, natural language processing \cite{miyato2016adversarial,jia2017adversarial}, and reinforcement learning \cite{pinto2017robust}.  Several defense mechanisms have been proposed in the literature \cite{GuR14, kannan2018adversarial, LuoBRPZ15, PapernotM0JS16, madry2017towards}  to mitigate the negative effect of adversarial examples, including the popular scheme based on adversarial training \cite{madry2017towards}. In this approach, clean training samples are augmented by adding purposefully crafted perturbations. Due to the lack of an explicit definition for the imperceptibility of perturbations, additive attacks  are usually restricted within a small bounded region. 

Most earlier studies, such as \cite{miyato2016adversarial,goodfellow2014explaining,madry2017towards,maini2020adversarial,zhang2019theoretically}, {focus} on studying adversarial training in the context of single agent learning. However, distributed settings, which consist of a group of agents, are becoming more prevalent {in a} highly connected world \cite{chang2020distributed, sayed2014adaptive}.  Examples abound in transportation networks, communication networks and biological networks. In this work, we devise a robust training algorithm for multi-agent networked systems by relying on diffusion learning \cite{sayed_2023,sayed2014adaptation,sayed2014adaptive}, which has been shown to have a wider stability range and improved performance guarantees for adaptation in comparison to other decentralized strategies \cite{sayed_2023,sayed2014adaptation,sayed2014adaptive}.


There of course exist other works in the literature that have applied adversarial learning to a multiplicity of agents, albeit using a different architecture. For example, the works \cite{qin2019adversarial,zhang2020distributed,liu2021concurrent} employ multiple GPUs and a fusion center, while the works \cite{feng2019graph,xu2019topology,wang2019graphdefense} consider graph neural networks. In this work, we focus on a fully decentralized architecture where each agent corresponds to a learning unit in its own right, and interactions occur locally over neighborhoods determined by a graph topology.

{The contributions of this work are listed as follows:}

(1) We formulate a sequential minimax optimization problem involving adversarial samples, {where} the perturbations are within some general $\ell_p$ norm-bounded region. To solve the {problem}, we propose a decentralized framework {based on} diffusion learning, where all agents {are subjected to adversarial examples and work together through local interactions to defend the network globally. Although we motivate our framework by focusing on stochastic gradient implementations, we hasten to add that other more complex optimizers can be used as well.}


(2) In the performance analysis, we examine the convergence of the proposed framework for both cases of convex and non-convex optimization environments. In particular, we show that for strongly-convex loss functions, and despite the perturbations, the proposed algorithm is able to approach the global minimizer 
within $O(\mu)$ after sufficient iterations, where $\mu$ is the step-size parameter. In comparison, for non-convex losses, the algorithm is guaranteed to converge to a point that is $O(\mu) + O(\epsilon^2)$ close to an approximate stationary point of the global objective function, where $\epsilon$ is the maximum perturbation bound over the entire graph.  
{Our results are more general than some earlier investigations in the literature where the loss function in the inner maximization step is required to be concave \cite{sinha2017certifying, WangM0YZG19, zhang2020distributed}.}

{(3) In the {simulations}, we illustrate how the robustness of the multi-agent systems can be improved by the proposed {framework}. We simulate both the convex and non-convex {environments}, and both homogeneous and heterogeneous networks. {We show how the adversarial diffusion strategy enables more robust behavior than non-cooperative and even centralized methods in the non-convex environments. This fact demonstrates the role of the graph topology in resisting adversarial settings.}

\begin{comment}
Most earlier studies \cite{goodfellow2014explaining,madry2017towards,maini2020adversarial,zhang2019theoretically,WongRK20} have focused on studying adversarial training in the context of single agent learning. {The following reasons motivate us to study the robustness of multi-agent systems: First, rapid progress in network science  in recent decades has demonstrated the importance of distributed processing in the highly connected world \cite{sayed2014adaptation,chang2020distributed}.  Moreover, in the field of adversarial training, the attackers need to know some private information of the models from all agents, i.e., the structure or the parameters of the models, the output of the models, etc., to construct powerful perturbations to fool all agents. In this manner, it is in general more difficult and costly for the attackers to fool a group of agents than a single unit in the decision process.  The worst situation is when the attackers successfully obtain the private information of the whole network so that all agents receive adversarial examples that are maliciously perturbed, which is the focus of the experimental results of this paper. That is, if this worst performance can be guaranteed, other cases can also be safe.}  In this work, we devise a decentralized robust training algorithm for multi-agent networked systems by relying on diffusion learning\cite{stefan_new}, which has been shown to have a wider stability range and improved performance guarantees for adaptation in comparison to other decentralized strategies \cite{sayed2014adaptation}.

In traditional single-agent learning, adversarial training can be formulated as a minimax game between an attacker and a defender, where the inner attacker maliciously perturbs data by imperceptible noise to maximize the objective function so that makes the model misbehave while the outer defender learns to minimize the worst-case performance from the attacker \cite{madry2017towards}. Due to the lack of an explicit definition for the imperceptibility of perturbations, additive attacks  are usually restricted within a small bounded region. According to the above discussion, we formulate a sequential minimax optimization problem involving adversarial samples for the multi-agent system, {where each agent has its own inner attacker to generate adversarial examples locally and all agents defend together to minimize the global performance. Also, the perturbations are within general $\ell_p$ norm-bounded region.} Here we emphasize that the multi-agent system is more flexible compared with the traditional single-agent case. That is, in the multi-agent system, it is not necessary that all agents should always work in the same (i.e., homogeneous) way. For instance, all agents across the network may receive perturbations restricted by different upper bounds or even by different norm types.%the explanation of the formulation

There of course exist other works in the literature that applied adversarial learning to a multiplicity of agents, albeit using a different architecture. For example, the works \cite{qin2019adversarial,zhang2020distributed,liu2021concurrent} employ multiple GPUs to speed up the training process or enable large-batch training, and also a fusion center is considered so they actually utilize the centralized architecture, while the works \cite{feng2019graph,xu2019topology,wang2019graphdefense} consider graph neural networks to learn from graph data. In this work, we focus on a fully decentralized architecture where each agent corresponds to a learning unit in its own right, and interactions occur locally over neighborhoods determined by a graph topology.%how our work is different form other distributed adversarial training.

The core of this work is to improve the robustness of networked agents.
(1) We propose a general decentralized framework with the basis of diffusion learning to improve the robustness of the multi-agent system, where all agents locally process the information they privately receive and then only cooperate with their neighbors. Note that we analyze its performance with the stochastic gradient descent (SGD) optimizer as it is the most basic one, while other more complex optimizers could also be applied when it is necessary.  
 (2) In the theoretical analysis, we examine how the proposed framework converges in both convex and non-convex settings. Basically, we show here that with strongly-convex loss functions where the worst-case attack in the bounded region can be exactly obtained, the proposed algorithm approaches the global minimizer within $O(\mu)$ after sufficient iterations, where $\mu$ is the step-size parameter. However, in the non-convex environment where the inner maximization problem is intractable, it is only possible to employ an approximation for it. As a result, the algorithm is only guaranteed to converge to a point that is $O(\mu) + O(\epsilon^2)$ close to an approximate stationary point of the global objective function, where $\epsilon$ is the maximum norm bound over the whole graph. This demonstrates that adversarial training in the non-convex case is much more complicated than the convex one, and also suggests that we should use small $\mu$ and $\epsilon$ in practice. 
(3) In the simulation, we show how the robustness of the multi-agent systems can be improved by the proposed framework over MNIST and CIFAR10 datasets. We simulate both the convex and non-convex cases, and both homogeneous and heterogeneous networks. The performance is illustrated from two perspectives: First, we depict the convergence plots for each experiment. Then, we show how the robustness of the trained networks to different attacks can be improved. 


%(1) the general bckground
%(2) introduce the problem we want to solve in this work.
%(3) theb related works
%(4) our contribution:(a); propose the algorithm for the decentralized adversarial learning, and give the theory fo both the convex and non0convex case: the convergence, so the cost can be effcetively minimized. The multi-agent is more flexible than the single -agent case;(b) the robustness can be  improved not only to the trained attacks compared with the l, but also other attacks;

%In machine learning, small malicious perturbations, also called attacks, which are imperceptible to the human eyes can cause a sharp degradation on the performance of well-trained models \cite{szegedy2013intriguing}. This phenomenon makes the robustness of learning systems critical in many real domains \cite{song2017pixeldefend,jia2017adversarial,pinto2017robust}. To mitigate the negative effects of adversaries, one of the most popular methodolodies is adversarial training \cite{madry2017towards}, in which clean training samples are augmented as adversarial ones by adding purposefully crafted perturbations. Due to the lack of an explicit definition of ``imperceptibility'' of perturbations, additive attacks are usually restricted by small norm balls. Most existing works \cite{miyato2016adversarial,goodfellow2014explaining,madry2017towards,maini2020adversarial} 
%only study the single-agent case. This motivates us to investigate the robustness of multi-agent learning systems, where information is collected by dispersed agents over a graph.

%Rapid progress of network science \cite{sayed2014adaptation, sayed2014adaptive} in recent decades has demonstrated the importance of distributed processing in the highly connected world. In general, coordination among units of a network derives sophisticated behaviors, such as information exchange and aggregation. While a single unit in a complex multi-agent system is not able to capture all information and the aforementioned behaviors, it is necessary to consider the interactions among agents. This work focuses on how collaboration among agents in a decentralized setting helps improve the robustness of multi-agent learning systems in the adversarial environment. To this end, we propose an adversarial diffusion learning algorithm. By implementing local interactions among agents, the diffusion strategy \cite{sayed2014adaptation} has been proven to be effective 
%in solving distributed optimization problems in many domains \cite{rizk2021graph,bordignon2021adaptive,kayaalp2020dif}. Compared with the centralized method \cite{sayed2014adaptation}, which involves central aggregation at a fusion center, decentralized approaches allows data to remain private, can be more communication efficient, and helps avoid collapse caused by the failed central processor. 

%Some works \cite{qin2019adversarial,zhang2020distributed,liu2021concurrent}  scaled adversarial training to large batches and made the training process faster by using multiple GPUs, and they focused on specific applications and actually employed the centralized approach. Another line of work \cite{feng2019graph,xu2019topology,wang2019graphdefense} studied adversarial training over graph neural networks.  In our work, each agent corresponds to a learning system, and the purpose is to study the performance of local interactions among agents both theoretically and empirically. Thus, our work can  be applied to not only the case of multiple GPUs, but also many other real settings, e.g., robots in multi-agent systems and users in social networks. 

%Our algorithm is inspired by \cite{vlaski2019diffusion} in which the properties of the diffusion algorithm has been carefully analyzed. However, we study the sequential minimax formulation in non-convex and non-concave environment involving adversarial examples, while \cite{vlaski2019diffusion} considered the minimization case in non-convex context which just considers unperturbed data.  Although only details of our method under $\ell_2$ norm-restricted perturbations are analyzed in this paper, our method can be extended to some others, i.e., $\ell_1$ and $\ell_\infty$. In addition, as the real distribution of the data is unknown to us, the exact gradient of the loss function cannot be computed. This triggers us use stochastic approximations, which also means that our algorithm can be applied to streaming data. As for the theoretical analysis, we verify that  all agents can reach consensus after enough iterations in the adversarial context. This is consistent with the minimization case. Furthermore, we prove the convergence of our algorithm in non-convex and non-concave environments. Simulations with neural networks are provided to demonstrate the performance of our algorithm.
\end{comment}
\section{Problem formulation}
We consider a collection of $K$ agents over a graph, where each agent $k$ observes independent realizations of some random data $(\boldsymbol{x}_k,\boldsymbol{y}_k)$, in which $\boldsymbol{x}_k$ plays the role of the feature vector and $\boldsymbol{y}_k$ plays the role of the label variable. Adversarial learning in the heterogeneous decentralized setting deals with the following stochastic minimax optimization problem:
\begin{equation}\label{formu}
w^\star  = \mathop{\mathrm{argmin}}\limits_{{w}\in \mathbbm{R}^M} \left\{J( w) \overset{\Delta}{=} \sum\limits_{k=1}^K \pi_k J_k( w)\right\}
\end{equation}
where $M$ is the dimension of the feature vectors,  $\{\pi_k\}_{k=1}^{K}$ are positive scaling weights adding up to one, and each individual risk function is defined by
\begin{equation}\label{jk}
\begin{split}
    J_k(w) &= \mathds{E}_{\{\boldsymbol{x}_k,\boldsymbol{y}_k\}}\left\{ \max\limits_{\left\Vert\delta_k\right\Vert_{p_k}\le \epsilon_k}  Q_k(w;\boldsymbol{x}_k +\delta_k, \boldsymbol{y}_k)\right\} \\
\end{split}
\end{equation}
in terms of a loss function $Q_k(\cdot)$. In this formulation, $\boldsymbol{y}_k$ is the true label of sample $\boldsymbol{x}_k$, and the variable $\delta_k$ represents a norm-bounded perturbation {added to the clean sample}.  In this paper, we assume all agents observe independently and identically distributed (i.e., iid) data over time, but allow {for} non-iid samples over space. The type of the norm $p_k$ and the upper bound $\epsilon_k$ {are allowed to} vary across agents, thus {leading to the possibility of heterogeneous models} over the graph. In particular, $\ell_2$ and $\ell_\infty$ norms are among two of the possible choices for which the results of this paper hold. We refer to $w^{\star}$ as the \textit{robust model} since it relies on the worst-case perturbation found from (\ref{jk}).% Also, for any model $w$, larger $\epsilon_k$ enables stronger attacks. For example, assume we have $\epsilon_1$ and $\epsilon_2$ satisfying that $\epsilon_1<\epsilon_2$, now we test the robustness of the model to the two attacks, we have
%\begin{equation}\label{la_e}
%\mathds{E}\left\{\max\limits_{\left\Vert\boldsymbol{\delta}\right\Vert_{p}\le \epsilon_1}  Q (w;\boldsymbol{x} +\boldsymbol{\delta}, \boldsymbol{y})\right\} \le  \mathds{E}\left\{\max\limits_{\left\Vert\boldsymbol{\delta}\right\Vert_{p}\le \epsilon_2}  Q (w;\boldsymbol{x} +\boldsymbol{\delta}, \boldsymbol{y}) \right\}
%\end{equation}
%since $\Vert\boldsymbol{\delta}\Vert_{p}\le \epsilon_1$ is involved in $\Vert\boldsymbol{\delta}\Vert_{p}\le \epsilon_2$.
%In this paper, we assume all agents collect d\espata independently from a same distribution. 在这里加一些performance analysis,即阐述为什么此formulation可以提升模型的鲁棒性。

One methodology for solving (\ref{formu}) is to first determine the inner maximizer in (\ref{jk}), thus reducing the minimax problem to a standard stochastic minimization formulation. Then, the traditional {stochastic gradient method} could be used to seek the minimizer. We denote the true maximizer of the perturbed loss function in (\ref{jk}) by
\begin{align}\label{maximizer_t}
    \boldsymbol{\delta}_k^{\star} (w) \in \mathop{\mathrm{argmax}}\limits_{\left\Vert\delta_k\right\Vert_{p_k}  \le \epsilon_k}   Q_k(w;\boldsymbol{x}_k +\delta_k,\boldsymbol{y}_k)
\end{align}
where the dependence of $\boldsymbol{\delta}_k^\star$ on $w$ is shown explicitly. We use the boldface notation for $\boldsymbol{\delta}_{k}^{\star}(w)$ because it is now a function of the random data $(\boldsymbol{x}_k,\boldsymbol{y}_k)$. To apply the stochastic gradient method, we would need to evaluate the gradient of $Q(w;\boldsymbol{x}_k+\boldsymbol{\delta}_k^{\star}(w),\boldsymbol{y}_k)$ relative to $w$, which can be challenging since $\boldsymbol{\delta}_k^{\star}(w)$ is also dependent on $w$. Fortunately, this difficulty can be resolved by appealing to Danskin's theorem -- see section 6.11 in \cite{bertsekas2009convex} and section B.5 in \cite{Thekumparampil019}. Let 
\begin{align}\label{f_g}
    g(w) \overset{\Delta}{=} \max_{\left\Vert\delta_k\right\Vert_{p_k}\le \epsilon_k} Q_k(w;\boldsymbol{x}_k+\delta_k,\boldsymbol{y}_k)
\end{align}Then, the theorem asserts that $g(w)$ is convex over $w$ if $Q_k(w;\cdot,\cdot)$ is convex over $w$. However, $g(w)$ need not be differentiable over $w$ even when $Q_k(w;\cdot,\cdot)$ is differentiable. Nevertheless, and importantly for our purposes, we can determine a subgradient for $g(w)$ by using the actual gradient of the loss evaluated at the worst perturbation, namely, it holds that
\begin{align}
\label{danskin}
    \nabla_w Q_k(w;\boldsymbol{x}_k+\boldsymbol{\delta}_k^\star,\boldsymbol{y}_k)\in \partial_w g(w)
\end{align}
where $\partial_{w}$ refers to the subdifferential set of its argument. In (\ref{danskin}), the gradient of $Q_k(\cdot)$ relative to $w$  is computed by treating $\boldsymbol{\delta}_k^{\star}$ as a stand-alone vector and ignoring its dependence on $w$. When $\boldsymbol{\delta}_k^{\star}$ in (\ref{maximizer_t}) happens to be \textit{unique}, then the gradient on the left in (\ref{danskin}) will be equal to the right side, so that in that case the function $ g(w)$ will be \textit{differentiable}.


Motivated by these properties, and using (\ref{danskin}), we can now devise an algorithm to enhance the robustness of multi-agent systems to adversarial perturbations. To do so, we rely on the adapt-then-combine (ATC) version of the diffusion strategy  {\cite{sayed_2023, sayed2014adaptive}} and write down the mini-batch adversarial extension to solve (\ref{formu})--(\ref{jk}). The details can be found {in the listing of} Algorithm \ref{al_ro_1}, where 
\begin{equation}\label{max_k}
    \boldsymbol{\delta}^{b,\star}_{k,n} \in \mathop{\mathrm{argmax}}\limits_{\left\Vert\delta_k\right\Vert_{p_k}  \le \epsilon_k}   Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^b +\delta_k,\boldsymbol{y}_{k,n}^b)
\end{equation}
computes the perturbation for each sample from {the} mini batch at iteration $n$, and $\boldsymbol{\phi}_{k,n}$ is the intermediate state obtained by performing a stochastic gradient step. The intermediate states $\boldsymbol{\phi}_{\ell,n}$ from the neighbors of agent $k$ are combined together using the scalars  $a_{\ell k}$, which are non-negative and add to one over $\ell \in {\cal N}_k$.

\begin{algorithm}[htb]
\caption{{: Adversarial diffusion training (ADT).}}
\label{al_ro_1}
\begin{algorithmic}
\Require {Step size $\mu$, batch size $B$, combination matrix $A$.}
\Ensure{A group of robust models $\{w_{k,N}\}_{k=0}^{K}$.}
 \State{Initialize models for all agents at $\{w_{k,0}\}_{k=0}^{K}$.}
 \For{Iteration $n = 1 \to N$}
    \For{ Agent $k = 1 \to K$}
        \State Select mini-batch training samples $\{\boldsymbol{x}_{k,n}^b,\boldsymbol{y}_{k,n}^b\}_{b=1}^B$.
        \For{  $b = 1 \to B$}
        \State 
        {{Construct $\boldsymbol{\delta}^{b,\star}_{k,n}$ using (\ref{max_k})}}
        \State
        {$\boldsymbol{x}_{k,n}^{b,\star}= \boldsymbol{x}_{k,n}^b+\boldsymbol{\delta}^{b,\star}_{k,n} $}
 \EndFor
 \State{$\boldsymbol{\phi}_{k,n} =\boldsymbol{w}_{k,n-1} - \dfrac{\mu}{B }\sum\limits_{b=1}^{B}\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^{b,\star},\boldsymbol{y}_{k,n}^b)$}
 \EndFor
 \State{$\boldsymbol{w}_{k,n}  = \sum\limits_{\ell \mathcal{\in N}_k} a_{\ell k} \boldsymbol{\phi}_{\ell,n}$}
 \EndFor
\end{algorithmic}
\end{algorithm}


We still need to compute the exact inner maximizer $\boldsymbol{\delta}^{b,\star}_{k,n}$ in (\ref{max_k}), and this step will be discussed for convex and non-convex environments separately in this paper. But first we list the following assumptions for both convex and non-convex cases, which are commonly used in the literature of decentralized multi-agent learning, as well as in works on single-agent adversarial training {\cite{sayed_2023, sayed2014adaptation, sinha2017certifying,  vlaski2021distributed, vlaski2021distributed2}.}

First, it is necessary to define the combination matrix $A$ according to which the agents interact over the graph topology.
\begin{assumption}\label{as1}
    \textbf{(Strongly-connected graph)} The entries of the combination matrix $A = [a_{\ell k}]$ satisfy $a_{\ell k } \ge 0$ and the entries on each column add up to one, which means that $A$ is left-stochastic. Moreover, the graph is  strongly-connected, meaning that there exists a path with nonzero weights $\{a_{\ell k}\}$ linking any pair of agents and, in addition, at least one node $k$ in the network has a self-loop with $a_{kk}>0$.
    
     $\hfill\square$
\end{assumption}

It follows from the Perron-Frobenius theorem \cite{sayed2014adaptation,sayed_2023} that $A$ has a single eigenvalue at 1. Let $\pi = \{\pi_k\}_{k=1}^{K}$ be the corresponding right eigenvector of $A$. It then follows from the same theorem that we can normalize the entries of $\pi$ to satisfy:
\begin{align}\label{comb_matrix}
    A\pi = \pi, \quad \mathbbm{1}^{\sf T}\pi=1, \quad \pi_k>0
\end{align}

Next, we require the loss function $Q_k$ at each agent to be smooth, which is a common assumption in the field of min-max optimization and adversarial learning \cite{sinha2017certifying, lin2020gradient, Thekumparampil019}. 
\begin{assumption}\label{as2}
 (\textbf{Smooth loss functions}) For each agent $k$, the gradients of the loss function relative to $w$ and $x$ are Lipschitz in relation to the  variables $\{w, x, y\}$. More specifically, it holds that
 \begin{small}
\begin{subequations}
\begin{align}
\label{smooth_q}
&\left\Vert\nabla_w  Q_k(w_2;x+\delta,y) -  \nabla_w  Q_k(w_1;x+\delta,y) \right\Vert\le   L_{1}\left\Vert w_2-w_1\right\Vert\\
\label{as2_4}
&\left\Vert\nabla_w  Q_k(w;x_2+\delta,y) - \nabla_w Q_k(w;x_1+\delta,y) \right\Vert\le L_{2}\left\Vert x_2-x_1\right\Vert\\
\label{as2_5}
&\left\Vert\nabla_w  Q_k(w;x+\delta,y_2) - \nabla_w Q_k(w; x+\delta, y_1) \right\Vert\le L_{3}\left\Vert y_2-y_1\right\Vert
\end{align}
\end{subequations}
\end{small}
and
\begin{small}
\begin{subequations}
\begin{align}
\label{as2_3}
&\left\Vert\nabla_x  Q_k(w_2;x+\delta,y) - \nabla_x  Q_k(w_1;x+\delta,y) \right\Vert\le L_{4}\left\Vert w_2-w_1\right\Vert\\
\label{as2_2}
&\left\Vert\nabla_x  Q_k(w;x_2+\delta,y) - \nabla_x  Q_k(w;x_1+\delta,y) \right\Vert\le L_{5}\left\Vert x_2-x_1\right\Vert
\end{align}
\end{subequations}
\end{small}with $\Vert\delta\Vert_{p_k} \le \epsilon_k$. To simplify the notation, we {let}
\begin{align}
    L = \max\{L_{1}, L_{2}, L_{3},L_4, L_5\}.
\end{align}

$\hfill\square$
\end{assumption}
\section{analysis in Convex environments}
This section analyzes the convergence of {adversarial diffusion algorithm} for the case of strongly convex loss functions. Besides Assumptions \ref{as1} and \ref{as2}, we consider the following assumptions for the convex environment.
\begin{assumption}\label{as3}
  (\textbf{Strong convexity}) For each agent k, the loss function $Q_k(w;\cdot)$ is $\nu-$strongly convex over $w$, namely, for any $w_1, w_2, x\in \mathbbm{R}^M$ and $y \in \mathbbm{R}$, it holds that
 \begin{align}\label{asconvex}
 Q_k(w_2;x,y) \ge \; & Q_k(w_1;x,y) + \nabla_{w^{\sf T}} Q_k(w_1;x,y)(w_2 - w_1) 
     \notag\\ 
     &+ \frac{\nu}{2}\Vert w_2 - w_1\Vert^2
 \end{align}
 
 $\hfill\square$
\end{assumption}
We remark that it also follows from Danskin's theorem \cite{lin2020gradient, rockafellar2015convex, Thekumparampil019} that, when $Q_k(w;\cdot,\cdot)$ is $\nu-$strongly convex over $w$, then the adversarial risk $J_k(w)$ defined by (\ref{jk}) will  be strongly convex. As a result, the aggregate risk $J(w)$ in (\ref{formu}) will be strongly-convex as well.

\begin{assumption}\label{as4}
(\textbf{Bounded gradient disagreement}) For any pair of agents $k$ and $\ell$, the squared gradient disagreements are uniformly bounded on average, namely, for any $w, \delta\in \mathbbm{R}^M$, it holds that
\begin{equation}\label{as4_1}
    \mathds{E}_{\{\boldsymbol{x},\boldsymbol{y}\}} \Vert \nabla_w Q_k(w;\boldsymbol{x} + \delta,\boldsymbol{y}) - \nabla_w Q_{\ell}(w;\boldsymbol{x} + \delta,\boldsymbol{y})\Vert^2 \le C^2
\end{equation}
for any $\Vert\delta\Vert_{p_k} \le \epsilon_k${,} $\Vert\delta\Vert_{p_\ell}  \le \epsilon_\ell$.

 $\hfill\square$
\end{assumption}

This assumption is similar to the one used in \cite{vlaski2021distributed, vlaski2021distributed2}, and is automatically satisfied when all agents use the same loss function, or in the centralized case where only a single agent is considered.

%for the linear classifiers, the loss function $Q(w;x+\delta)$ is strongly convex over both $w$ and $x$, which implies that the maximizer in (\ref{max_k}) is unique. Moreover,

To evaluate the performance {of the} adversarial diffusion algorithm, it is critical to compute the inner maximizer $\boldsymbol{\delta}_{k,n}^{b,\star}$ defined by (\ref{max_k}). Fortunately, {for some  convex problems involving the logistic loss, the quadratic loss, and  the exponential loss, the maximization in (\ref{max_k}) has a \textit{unique} closed-form solution (e.g., we will show this for the logistic loss in a later section).} This uniqueness property enables us to analyze the convergence properties of the algorithm.    We first establish the following affine Lipschitz result for the risk function in (\ref{jk}). The proof can be found in Appendix \ref{ap1}.
\begin{lemma}\label{affine_l}
(\textbf{Affine Lipschitz})  For each agent $k$, the gradient of $J_k(w)$ is affine Lipschitz, namely, for any $w_1, w_2 \in \mathbbm{R}^M$, it holds that
\begin{equation}\label{affine_l_1}
    \Vert \nabla_w J_k(w_2) - \nabla_w J_k(w_1) \Vert^2 \le 2L^2\Vert w_2 - w_1 \Vert^2 + O(\epsilon_k^2)
\end{equation}
%{where $ \epsilon = \max\limits_{k}\epsilon_k$}.
$\hfill\square$
\end{lemma}

Contrary to the traditional analysis of decentralized learning algorithms where the risk functions $J_k(w)$ are generally Lipschitz, it turns out that under adversarial perturbations, the risks in (\ref{jk}) are now {\em affine} Lipschitz {as shown by (\ref{affine_l_1}) due to the additional term $O(\epsilon_k^2)$. This fact} requires adjustments to the convergence arguments. A similar situation arises, for example, when we study the convergence of decentralized learning under non-smooth losses {and clean environments} --- see \cite{ying2018performance2,sayed_2023}.

To proceed with the convergence analysis, we introduce the gradient noise process, which is defined by
\begin{align}\label{d_gn}
     \boldsymbol{s}_{k,n}\overset{\Delta}{=} \nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^\star,\boldsymbol{y}_{k,n}) - \nabla_w J_k(\boldsymbol{w}_{k,n-1}) 
\end{align}This quantity measures the difference between the approximate gradient (represented by the gradient of the loss) and the true gradient (represented by the gradient of the risk) at iteration $n$. Similarly, we define the gradient noise for the mini-batch version as
\begin{align}\label{d_gn_B}
  \boldsymbol{s}_{k,n}^{B} \overset{\Delta}{=}  \frac{1}{ B}\sum\limits_{b=1}^{B}\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^{b,\star},\boldsymbol{y}_{k,n}^{b})- \nabla_w J_k(\boldsymbol{w}_{k,n-1}) 
\end{align} The following result establishes some useful properties for the gradient noise process, namely, it has zero mean and bounded second-order moment (conditioned on past history), for which proofs can be found in Appendix \ref{ap2}.
\begin{lemma}\label{p_gn}
(\textbf{Moments of gradient noise}) For each agent k, the gradient noise defined in {(\ref{d_gn_B}) has zero mean} and its variance is bounded:
\begin{align}
   &\label{s0_convex} \mathds{E}\left\{\boldsymbol{s}_{k,n}^{B }|\boldsymbol{\mathcal{F}}_{n-1}\right\}=0 \\
    &\label{s2_convex} \mathds{E}\left\{\left\Vert \boldsymbol{s}_{k,n}^{B }\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\} \le \beta_{k}^2 \left\Vert \widetilde{\boldsymbol{w}}_{k,n-1} \right\Vert^2 + \sigma_{k}^2
\end{align}
for some non-negative scalars given by
\begin{align}
    &\beta_{k}^2 = \frac{16L^2}{B}  \notag\\
    &\sigma_{k}^2  = \frac{8}{B}\mathds{E}\Vert \nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n})\Vert^2 + O(\epsilon^2_k)
\end{align} where 
\begin{align}
\boldsymbol{\delta}_k^\star(w^\star) = \mathop{\rm{argmax}}\limits_{\left\Vert{\delta}\right\Vert_{p_k}  \le \epsilon_k}   Q_k(w^\star;\boldsymbol{x}_{k,n} +{\delta},\boldsymbol{y}_{k,n})
\end{align} The quantity
 $\boldsymbol{\mathcal{F}}_{n-1}$ is the filtration generated by the past history of the random process $\mbox{{\rm{col}}} {\{\boldsymbol{w}_{k,j}\}_{k=1}^K}$ for $j\le n-1$, and
\begin{align}
   \widetilde{\boldsymbol{w}}_{k,n-1} \overset{\Delta}{=} w^\star - {\boldsymbol{w}}_{k,n-1} 
\end{align}
$\hfill\square$
\end{lemma}
%introduce the network algorithm here.

To enable network analysis, we collect the error vectors from all agents into the $K\times 1$ block column vector{:}
\begin{align}\label{AC_1}
    {\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n} \overset{\Delta}{=} {\mbox{\rm col}}\{\widetilde{\boldsymbol{w}}_{k,n}\}_{k=1}^{K}
\end{align}
Similarly, we gather the true gradients and the gradient noise from all agents into:
%&\nabla_w Q({\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}) = {\mbox{\rm col}}\left\{\frac{1}{B}\sum\limits_{b=1}^B\nabla_w Q_k(\boldsymbol{w}_{k,n-1};{\boldsymbol{x}}_{k,n}^{b,\star},\boldsymbol{y}_{k,n}^b)\right\}\notag\\
\begin{align}
    &\nabla J({\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}) \overset{\Delta}{=} {\mbox{\rm col}}\left\{\nabla_w J_k(\boldsymbol{w}_{k,n-1})\right\}\\
    &\boldsymbol{s}_n^B \overset{\Delta}{=}   {\mbox{\rm col}}\left\{\boldsymbol{s}_{k,n}^B\right\}
\end{align}
Using these variables, the recursions  {of the adversarial diffusion training algorithm}  can be rewritten as
\begin{align}\label{AC_2}{\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n} &= \mathcal{A}^{\sf T}\left(\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1} + \mu \nabla J({\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1})+ \boldsymbol{s}_n^B\right)%\mathcal{A}^{\sf T}\left(\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1} + \mu \nabla_w Q({\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1})\right)\notag\\   
\end{align}
where
\begin{equation}
    \mathcal{A} = A \otimes I_{M}
\end{equation}
{and} $I_{M}$ is the identity matrix of {size} $M$, {while} $\otimes$ denotes the Kronecker product operation. We exploit the eigen-structure of $A$ by noting that the $K\times K$ matrix $A$ admits a Jordan canonical decomposition of the form\cite{sayed2014adaptation}:
\begin{equation}\label{decompose}
    A = V_{\alpha}J V_{\alpha}^{-1}
\end{equation}
{where the matrices $\{V_{\alpha}, J\}$ are}
\begin{equation}\label{vjv}
    V_{\alpha} = \left[{\pi}\quad V_{R}\right],\quad
    J = \left[
        \begin{array}{cc}
        1&0\\
        0&J_{\alpha}
        \end{array}
    \right],\quad
    V_{\alpha}^{-1} = \left[
    \begin{array}{c}
    \mathbbm{1}^{\sf T}\\
    V_{L}^{\sf T}
    \end{array}
    \right]
\end{equation}
Here, $J_{\alpha}$ is a block Jordan matrix with eigenvalues from the second largest eigenvalue $\lambda_2$ to the smallest  eigenvalue  $\lambda_K$ {of $A$ appearing} on the diagonal {and the positive scalar $\alpha$}, which can be chosen arbitrarily small, {appearing} on the first lower sub-diagonal. Then, the extended matrix $\mathcal{A}$ satisfies
\begin{equation}\label{AC_9}
    \mathcal{A} = \mathcal{V}_{\alpha}\mathcal{J}\mathcal{ V}_{\alpha}^{-1}
\end{equation}
where
\begin{align}\label{evjv}
    \mathcal{V}_{\alpha} = V_\alpha\otimes I_M,  \quad \mathcal{J} = J\otimes I_M,\quad \mathcal{ V}_{\alpha}^{-1} = { V}_{\alpha}^{-1}\otimes I_M
\end{align}
Since
\begin{align}
    V_{\alpha} V_{\alpha}^{-1} =  V_{\alpha}^{-1}V_{\alpha} = I_K
\end{align}
we have
\begin{subequations}
\begin{align}\label{AC_12a}
    &{\pi}\mathbbm{1}^{\sf T} + V_RV_L^{\sf T} = \mathbbm{1}\pi^{\sf T} + V_LV_R^{\sf T} = I_K\\
\label{AC_12b}
    &V_L^{\sf T}V_R = I_{K-1}\\
\label{AC_12c}
    &V_L^{\sf T}\pi = \mathbbm{1}^{\sf T}V_R = 0
\end{align}
\end{subequations}
Multiplying the error recursion (\ref{AC_2}) from the left by $\mathcal{V}_\alpha^{\sf T}$, and substituting (\ref{AC_9}) into it, we obtain the following recursion by following a similar derivation to \cite{sayed2014adaptation,  ChenS15}:
\begin{align}\label{AC_13}
    & \mathcal{V}_\alpha^{\sf T}{\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n} = \left[\begin{array}{c}
         (\pi^{\sf T} \otimes I_M){\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n} \\
         (V_R^{\sf T}\otimes I_M){\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n}
    \end{array}\right] \overset{\Delta}{=} \left[\begin{array}{c}
         \bar{\boldsymbol{w}}_n  \\
          \check{\boldsymbol{w}}_n
    \end{array}\right]\notag\\
    &= \mathcal{J}^{\sf T}\mathcal{V}_{\alpha}^{\sf T}\left({\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1} + \mu\nabla J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1})+\mu \boldsymbol{s}_n^B\right)\notag\\
     &=  \left[\begin{array}{cc}
        I_M & \boldsymbol{0} \\
        \boldsymbol{0} & J_{\alpha}^{\sf T}\otimes I_M
    \end{array}\right] \left[\begin{array}{c}
         \bar{\boldsymbol{w}}_{n-1}  \\
          \check{\boldsymbol{w}}_{n -1}
    \end{array}\right]+ \notag\\
    & \quad\mu \left[\begin{array}{cc}
         I_M& \boldsymbol{0} \\
        \boldsymbol{0} & J_{\alpha}^{\sf T}\otimes I_M
    \end{array}\right]\left[\begin{array}{c}
         \pi^{\sf T} \otimes I_M  \\
          V_R^{\sf T}\otimes I_M
    \end{array}\right]\bigg(\nabla J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1}) + \boldsymbol{s}_n^B\bigg)
\end{align}
It follows that we can split
(\ref{AC_2}) into the following two {recursions in terms of the transformed quantities $\{\bar{\boldsymbol{w}}_{n}, \check{\boldsymbol{w}}_{n}\} $:}
\begin{align}\label{AC_14}
    \bar{\boldsymbol{w}}_{n} &= \sum_k \pi_k\widetilde{\boldsymbol{w}}_{k,n-1} \notag\\
    & = \bar{\boldsymbol{w}}_{n-1} + \mu\sum_k\pi_k\nabla_w J_k(\boldsymbol{w}_{k,n-1}) + \mu\sum_k\pi_k \boldsymbol{s}_{k,n}^B
\end{align}
and
\begin{align}\label{AC_15}
    \check{\boldsymbol{w}}_{n} = \mathcal{J}_\alpha^{\sf T}\check{\boldsymbol{w}}_{n-1} + \mu\mathcal{J}_\alpha^{\sf T}\mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1}) + \mu\mathcal{J}_\alpha^{\sf T}\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B
\end{align}
where
\begin{align}\label{AC_16}
    \mathcal{J}_\alpha = J_\alpha \otimes I_M,\quad \mathcal{V}_R = V_R\otimes I_M
\end{align}
The convergence results for (\ref{AC_14}) and (\ref{AC_15}) are stated next, for which proofs can be found in Appendix \ref{ap3}.
\begin{theorem}\label{th_mse}
(\textbf{Network mean-square-error stability}) Consider a network of $K$ agents running the {adversarial diffusion strategy} {from}  Algorithm \ref{al_ro_1}. Under Assumptions \ref{as1}--\ref{as4} and for sufficiently small step-sizes $\mu$, it holds that 
\begin{align}
     \mathop{\lim \sup}\limits_{n\to\infty} \mathds{E}\left\Vert \check{\boldsymbol{w}}_{n}\right\Vert^2 \le O(\mu^2),\;\mathop{\lim \sup}\limits_{n\to\infty} \mathds{E}\left\Vert \bar{\boldsymbol{w}}_{n}\right\Vert^2 \le  O(\mu)
\end{align}
More specifically, the network converges asymptotically to a small negiborhood of the global minimizer $w^{\star}$ at an exponential rate:
\begin{align}
  \mathop{\lim \sup}\limits_{n\to\infty} \mathds{E}\left\Vert {\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n}\right\Vert^2 
    \le O(\lambda^n) + O(\mu)
\end{align}
where
\begin{align}
    \lambda= 1 - 2\nu\mu  + O(\mu^2)
\end{align}
%with the convergence rate 
%\begin{align}
    %\lambda = 1 - 2\mu\nu + \mu^2 %b_{\epsilon}
%\end{align}
%Proof. Omitted due to space limitations.
$\hfill\square$
\end{theorem}
The above theorem indicates that the proposed algorithm enables the network to approach {in the mean-square-error sense} an $O(\mu)$-neighborhood of the robust minimizer $w^{\star}$ after enough iterations, so that the average worst-case performance {over all possible perturbations in the small region bounded by $\epsilon_k$ is}  effectively minimized.

\section{Analysis in Non-Convex environments}
For nonconvex losses, it is generally not possible to obtain closed form expressions for the maximizer $\boldsymbol{\delta}_k^{b,\star}$. Instead, one is satisfied with some common approximations for it \cite{maini2020adversarial,miyato2016adversarial,madry2017towards}. One popular strategy is to resort to a first-order Taylor expansion for each agent around $\boldsymbol{x}_k$ as follows:
\begin{align}\label{eqtaylor}
 & Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k +\delta_k,\boldsymbol{y}_k)\notag\\
    &\approx  Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)+ \delta_k^{\sf T}\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)
\end{align}
from which the following approximate maximizer can be derived:
\begin{align}\label{maximizer}
    \boldsymbol{\widehat{\delta}}_k &= \mathop{\mathrm{argmax}}\limits_{\left\Vert\delta_k\right\Vert_{p_k} \le \epsilon_k}   \Big\{ Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)+ \delta_k^{\sf T}\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)\Big\}\notag\\
    &= \mathop{\text{argmax}}\limits_{\left\Vert\delta_k\right\Vert_{p_k} \le \epsilon_k} \delta_k^{\sf T} \nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)
%& =  \epsilon \frac{\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)}{\left\Vert\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)\right\Vert}    
\end{align}
Note that different choices for the $p_k$ norm lead to different expressions for the approximate maximizer. For instance,  when $p_k=2$, we get:
\begin{equation}\label{p2}
\widehat{\boldsymbol{\delta}}_k = \epsilon_k \frac{\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)}{\left\Vert\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)\right\Vert}    
\end{equation}
 which motivates the fast gradient method (FGM) \cite{miyato2016adversarial} in single-agent learning, while for $p= \infty$ we have
 \begin{equation}\label{pin}
    \widehat{\boldsymbol{\delta}}_k = \epsilon_k 
\mathop{\mathrm{sign}}\Big(\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)\Big) 
 \end{equation}
which relates to the fast gradient sign method (FGSM) \cite{goodfellow2014explaining}. Moreover, depending on the number of iterations used when generating perturbations, the first-order gradient based methods include single-step attacks, such as FGM and FGSM, and multi-step ones, such as {projected} gradient descent (PGD). Although the latter perturbation has been empirically shown to be stronger than the former ones \cite{madry2017towards} and {leads} to  more robust models, its computational complexity is also much higher\cite{WongRK20}. In the simulations we use single-step methods, while noting that our framework can also be combined with multi-step attacks if necessary.

The above discussion motivates us to consider the following adversarial extension of the diffusion strategy for non-convex environments:
\begin{subequations}
\begin{align}
\label{newx_e_n}
&\widehat{\boldsymbol{x}}_{k,n}^{b}= \boldsymbol{x}_{k,n}^b+\widehat{\boldsymbol{\delta}}^{b}_{k,n}\\
\label{a2_e_n}
    &\boldsymbol{\phi}_{k,n} =\boldsymbol{w}_{k,n-1} - \frac{\mu}{B }\sum\limits_{b=1}^{B}\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\widehat{\boldsymbol{x}}_{k,n}^{b},\boldsymbol{y}_{k,n}^b)\\
\label{a3_e_n}
&\boldsymbol{w}_{k,n}  = \sum\limits_{\ell \mathcal{\in N}_k} a_{\ell k} \boldsymbol{\phi}_{\ell,n}  
\end{align}
\end{subequations}
In these expressions, the approximate maximizer $\widehat{\boldsymbol{\delta}}^{b}_{k,n}$  replaces the exact $\boldsymbol{\delta}_{k,n}^{b,\star}$ in Algorithm \ref{al_ro_1}. In other words, we consider the same framework for both convex and non-convex cases, but different methods are implemented to compute the adversarial examples. 

To analyze the performance of (\ref{newx_e_n})--(\ref{a3_e_n}), we assume the following conditions concerning non-convex loss functions. First, we suppose that the expected norm of the gradient of $Q_k(\cdot)$ over $w$ is bounded, which is common in non-convex optimization \cite{lin2020gradient, kayaalp2022dif, sinha2017certifying}.
\begin{assumption}\label{as5}
(\textbf{Bounded gradients}) For each agent k, the expectation of the norm of the gradient $\nabla_w Q_k(w;\boldsymbol{x}_k+{{\delta}}_k,\boldsymbol{y}_k)$ is bounded, namely, for any $w \in \mathcal{\mathbbm{R}}^M$:
\begin{equation}\label{b1}
    \mathds{E}\left\Vert\nabla_w  Q_k(w;\boldsymbol{x}_k+\delta_k,\boldsymbol{y}_k)\right\Vert \le G 
\end{equation}
with $\Vert\delta_k\Vert_{p_k} \le \epsilon_k$.

$\hfill\square$
\end{assumption}

Recall that according to Danskin's theorem, the function $g(w)$ defined in (\ref{f_g}) is not necessarily differentiable if  the inner maximizer $\boldsymbol{\delta}_k^{b,\star}$ is not unique. It then follows that $J(w)$ defined by (\ref{formu}) is also not necessarily differentiable. To enable the analysis of (\ref{newx_e_n})--(\ref{a3_e_n}), {we introduce the following proxy for the ``gradient" operator}:
\begin{align}\label{j_bar}
    \overline{\nabla_w {J}_k}(\boldsymbol{w}_{k,n-1}) \overset{\Delta}{=} \mathds{E} \{\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\widehat{\boldsymbol{x}}_{k,n},\boldsymbol{y}_{k,n})\}
\end{align} which is the expectation of the gradient of $Q_k(\cdot)$ evaluated at the {\em approximate} maximizer. Then, similar to the convex case, we introduce the gradient noise to measure the difference between the stochastic gradient and the proxy gradient as follows:
 \begin{equation}\label{dgn}
\begin{split}
    \bar{\boldsymbol{s}}_{k,n}(\boldsymbol{w}_{k,n-1}) \overset{\Delta}{=} &\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\widehat{\boldsymbol{x}}_{k,n},\boldsymbol{y}_{k,n})- \overline{\nabla_w {J}_k}(\boldsymbol{w}_{k,n-1})
\end{split}
\end{equation}
It can be easily shown that the gradient of $Q_k$ at one sample is an unbiased estimate of its expectation over all samples. {That is,}
\begin{align}\label{s0}
    &\mathds{E}\left\{\bar{\boldsymbol{s}}_{k,n}(\boldsymbol{w}_{k,n-1})|\boldsymbol{\mathcal{F}}_{n-1}\right\}  
 = \mathds{E} \Big\{\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\widehat{\boldsymbol{x}}_{k,n},\boldsymbol{y}_{k,n}) \notag\\
 &- \mathds{E} \{\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\widehat{\boldsymbol{x}}_{k,n},\boldsymbol{y}_{k,n}) \Big\vert \boldsymbol{\mathcal{F}}_{n-1}\Big\} = 0
\end{align}
Similarly, the gradient noise for the mini-batch version is defined by
\begin{align}\label{bbs}
\bar{\boldsymbol{s}}^{B}_{k,n} \overset{\Delta}{=} \frac{1}{B}\sum\limits_{b=1}^{B}\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\widehat{\boldsymbol{x}}^b_{k,n},\boldsymbol{y}_{k,n}^b) - \overline{\nabla_w {J}_k}(\boldsymbol{w}_{k,n-1})
\end{align}The following assumption is extended from the convex case, and it is common for non-convex environments\cite{kayaalp2022dif, vlaski2021distributed, vlaski2021distributed2}. It assumes that the variance of the gradient noise is bounded by a term associated with $\boldsymbol{w}_{k,n-1}$.  This assumption is weaker than the one where the term is bounded by a constant\cite{lin2020gradient, sinha2017certifying}.
\begin{assumption}\label{as6}
(\textbf{Moments of gradient noise})
For each agent $k$,  the gradient noise satisfies
\begin{align}
    \label{s2} \mathds{E}\left\{\left\Vert\bar{\boldsymbol{s}}_{k,n}(\boldsymbol{w}_{k,n-1})\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\} \le 
            \beta^2 \left\Vert\overline{\nabla_w {J}_k}(\boldsymbol{w}_{k,n-1}) \right\Vert^2 + \sigma^2
\end{align}
for some non-negative scalars $\beta$ and $\sigma$.

$\hfill\square$
\end{assumption}

Now consider the weighted average of the weight iterates at all agents {defined} by
\begin{equation}\label{cluster}
   \boldsymbol{w}_{c,n} \overset{\Delta}{=} \sum\limits_{k=1}^K \pi_k\boldsymbol{w}_{k,n}
\end{equation}
and introduce the following block vectors:
\begin{align}
    &{\boldsymbol{\scriptstyle\mathcal{W}}}_{n} \overset{\Delta}{=} {\mbox{\rm col}}\{\boldsymbol{w}_{k,n}\}_{k=1}^{K}\\
& {\boldsymbol{\scriptstyle\mathcal{W}}}_{c,n} \overset{\Delta}{=} \mathbbm{1}_{K}\otimes\boldsymbol{w}_{c,n}\\
    \label{dq}
    &\mathcal{G}_{n-1}\overset{\Delta}{=} {\mbox{\rm col}}\left\{\overline{\nabla_w {J}_k}(\boldsymbol{w}_{k,n-1})\right\} \\
    \label{s_n_bar}
    &\bar{\boldsymbol{s}}_n^B \overset{\Delta}{=} {\mbox{\rm col}} \left\{\bar{\boldsymbol{s}}_{k,n}^B\right\}
\end{align}
Then, algorithm (\ref{newx_e_n})--(\ref{a3_e_n}) can be rewritten as
\begin{equation}\label{r_non_c}
    \boldsymbol{\scriptstyle\mathcal{W}}_{n} = \mathcal{A}^{\sf T}(\boldsymbol{\scriptstyle\mathcal{W}}_{n-1} - \mu \mathcal{G}_{n-1} - \mu\bar{\boldsymbol{s}}_n^B  )
\end{equation}
We then have the next statements to measure the difference between $\boldsymbol{\scriptstyle\mathcal{W}}_{n}$ and $\boldsymbol{\scriptstyle\mathcal{W}}_{c,n}$.  The associated proofs can be found in Appendix \ref{ap4}.
\begin{lemma}\label{lemma3}
(\textbf{Network disagreement}) Under Assumptions \ref{as1}--\ref{as2} and \ref{as5}--\ref{as6}, and with sufficiently small step size $\mu$, the {mean-square} disagreement between $\boldsymbol{\scriptstyle\mathcal{W}}_{n}$ and $\boldsymbol{\scriptstyle\mathcal{W}}_{c,n}$ can be bounded after enough iterations $n_0$, namely, it holds that:
\begin{align}\label{nd}
 {\mathds{E}}\left\Vert\boldsymbol{\scriptstyle\mathcal{W}}_{n} - \boldsymbol{\scriptstyle\mathcal{W}}_{c,n}\right\Vert^2 \le  O(\mu^2)
\end{align}
for
\begin{align}
       n\ge n_0 \ge O \left(\frac{\rm{log} \mu}{\rm{log} \textit{t}}\right) 
\end{align}
where $t =  \left\Vert J_\alpha^{\sf T} \right\Vert <1$.

$\hfill\square$
\end{lemma}


The above result extends Lemma 1 from \cite{vlaski2021distributed,vlaski2021distributed2,kayaalp2022dif}, which studies the unperturbed environment without adversarial observations. Result (\ref{nd}) means that $\boldsymbol{w}_{k,n}$ will cluster around the centroid $\boldsymbol{w}_{c,n}$ after sufficient iterations, and the bounded perturbations do not ruin this property. Thus, the evolution of $\boldsymbol{w}_{k,n}$ can be well-described by the centroid $\boldsymbol{w}_{c,n}$, which allows us to use $\boldsymbol{w}_{c,n}$ {as a proxy to {examine}} the dynamics of all agents across the network.

In order to {progress further with} the network analysis in the non-convex environment, we write down the following recursion for the centroid:
\begin{align}\label{wc}
     \boldsymbol{w}_{c,n} &= \sum\limits_{k=1}^{K}\pi_k \boldsymbol{w}_{k,n} =(\pi^{\sf T}\otimes I_M)\boldsymbol{\scriptstyle\mathcal{W}}_{n}\notag\\
     &= (\pi^{\sf T}\otimes I_M)\mathcal{A}^{\sf T}(\boldsymbol{\scriptstyle\mathcal{W}}_{n-1}-\mu \mathcal{G}_{n-1} - \mu\bar{\boldsymbol{s}}_n^B)\notag\\
     &\overset{(a)}{=}(\pi^{\sf T} \otimes I_M)\boldsymbol{\scriptstyle\mathcal{W}}_{n-1} - \mu(\pi^{\sf T}\otimes I_M)(\mathcal{G}_{n-1} +\bar{\boldsymbol{s}}_n^B) \notag\\
     & = \boldsymbol{w}_{c,n-1} - \mu\sum\limits_{k=1}^{K} \pi_k \overline{\nabla_w J_k}(\boldsymbol{w}_{k,n-1}) - \mu \sum\limits_{k=1}^{K} \pi_k\bar{\boldsymbol{s}}^{B}_{k,n}
\end{align}
where $(a)$ follows from (\ref{comb_matrix}). 
{We can rework this recursion by using $\boldsymbol{w}_{c,n-1}$ as the argument for $\overline{\nabla_w J_k}(\cdot)$ instead of $\boldsymbol{w}_{k,n-1}$}:
\begin{align}\label{d_57}
   \overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})\overset{\Delta}{=} \mathds{E} \{\nabla_w Q_k(\boldsymbol{w}_{c,n-1};\widehat{\boldsymbol{x}}_{k,n},\boldsymbol{y}_{k,n})\}
\end{align}
where
\begin{align}\label{d_572}
    \widehat{\boldsymbol{x}}_{k,n} = \boldsymbol{x}_{k,n} + \widehat{\boldsymbol{\delta}}_{k,n}(\boldsymbol{w}_{k,n-1})
\end{align}
{Observe that the gradient of $Q_k(\cdot)$ is evaluated at $\boldsymbol{w}_{c,n-1}$ in (\ref{d_57}), while the term $\boldsymbol{w}_{k,n-1}$ is used in (\ref{d_572}) to emphasize that the perturbation $\widehat{\boldsymbol{\delta}}_{k,n}$ is still computed from $\boldsymbol{w}_{k,n-1}$ using (\ref{maximizer}). In this way, we can rewrite (\ref{wc}) in the form}%Note that $\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})$ and $\overline{\nabla_w J_k}(\boldsymbol{w}_{k,n-1})$ are two vectors defined as the auxiliary terms to progress on the analysis.
\begin{align}\label{wce}
     \boldsymbol{w}_{c,n} &= \boldsymbol{w}_{c,n-1} -\mu\sum\limits_k \pi_k \overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1}) - \mu\boldsymbol{d}_{n-1} - \mu\widehat{\boldsymbol{s}}_n^B
\end{align}
{where we are introducing}
\begin{align}
    \label{d_n}
     &\boldsymbol{d}_{n-1} \overset{\Delta}{=}  \sum\limits_{k=1}^{K} \pi_k \Big(\overline{\nabla_w J_k}(\boldsymbol{w}_{k,n-1})-\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})\Big)\\
    \label{sn}
     &\widehat{\boldsymbol{s}}_{n}^B \overset{\Delta}{=} \sum\limits_{k=1}^{K} \pi_k \bar{\boldsymbol{s}}^{B}_{k,n}
\end{align}
The vector $\boldsymbol{d}_{n-1}$ arises from the disagreement between $\boldsymbol{\scriptstyle\mathcal{W}}_{n-1}$  and the centroid $\boldsymbol{\scriptstyle\mathcal{W}}_{c,n-1}$, while $\widehat{\boldsymbol{s}}_{n}^B$ {aggregates} the gradient noise across the network. It can be verified that the two error terms $\boldsymbol{d}_{n-1}$ and $\widehat{\boldsymbol{s}}_{n}^B$ are bounded in the mean-square sense, using Assumption \ref{as6} and Lemma \ref{lemma3}. The proof can be found in Appendix \ref{ap5}.

\begin{lemma}\label{lemma4}
(\textbf{Error bounds}) Under Assumptions \ref{as1}--\ref{as2} and \ref{as5}--\ref{as6}, and with sufficiently small step size $\mu$, the two error terms $\widehat{\boldsymbol{s}}_{n}^B$ and $\boldsymbol{d}_{n-1}$ are bounded in the mean-square sense, namely, it holds that
\begin{align}
    &\mathds{E}\Vert \widehat{\boldsymbol{s}}_n^B\Vert^2 \le \frac{1}{B}(\beta^2 G^2+ \sigma^2)\\
    &\mathds{E}\Vert \boldsymbol{d}_{n-1}\Vert^2 \le O(\mu^2)
\end{align}
$\hfill{\square}$
\end{lemma}
\enlargethispage{\baselineskip}
Since $J(w)$ defined by (\ref{formu}) is not guaranteed to be differentiable in the non-convex case, the convergence analysis of the adversarial diffusion strategy relies on the use of subgradients for $J(w)$. Basically, $w^o$ is a stationary point for $J(w)$ if $0\in \partial J(w^o)$. If the subdifferential $\partial J(w)$ at $w^o$ does not contain the value zero but is close to it, then we say that $w^o$ is an approximate stationary point. Here we use  the  Moreau envelope method and $L$-weak convexity \cite{jin2020local,rockafellar2015convex,DavisD19,lin2020gradient,Thekumparampil019} to study near-stationarity for non-smooth loss functions\footnote{ A function $f(z)$ is said to be $L$-weakly convex if \cite{Thekumparampil019}:  $$f(z_2) \ge f(z_1) + \partial_z f(z_1)(z_2-z_1) - \frac{L}{2}\Vert z_2- z_1\Vert^2$$ It is also shown in \cite{Thekumparampil019} that $L$-weak convexity holds if $\nabla_z f(z)$ is $L$-Lipschitz.}. Let $\epsilon = \max\limits_k\{\epsilon_k\}$. {The proof} for the next theorem can be found in Appendix \ref{ap6}.


\begin{theorem}\label{th2}
{Consider the adversarial diffusion strategy (\ref{newx_e_n})--(\ref{a3_e_n}).} Under Assumptions \ref{as1}--\ref{as2} and \ref{as5}--\ref{as6}, and {assuming} $J(w)$ is lower bounded, the following inequality holds for $N$ large enough:
\begin{equation}
    \Vert\widehat{\boldsymbol{w}}_{N-1} - \boldsymbol{w}_{c,N-1}\Vert^2 \le \rho = O\left(\frac{1}{\mu N}\right)+ O(\mu) + O(\epsilon^2)
\end{equation}
where $\widehat{\boldsymbol{w}}_{N-1}$ is an approximate stationary point {for} $J(w)$ satisfying
\begin{align}\label{convergence}
  \mathds{E}\left\{\min_{\boldsymbol{\zeta}\in\partial_w J(\widehat{\boldsymbol{w}}_{N-1})}||\boldsymbol{\zeta}||^2 \right\}
  \le  \rho
\end{align} which implies that the algorithm arrives at a point $\boldsymbol{w}_{c,N-1}$ that is $O(\mu) + O(\epsilon^2)$ close to an approximate stationary point of $J(w)$ after enough iterations.
$\hfill\square$
\end{theorem}

Theorem \ref{th2} is based on properties of the Moreau envelope, and it guarantees that when $\mu$ and $\epsilon$ {are small}, the {adversarial diffusion algorithm (\ref{newx_e_n})--(\ref{a3_e_n})} will converge to a point that is $O(\mu)+O(\epsilon^2)$  {close to} an approximate stationary point of $J(w)$. Furthermore, the term $O(\epsilon^2)$ in (\ref{convergence}) {arises from} the approximation error of the inner maximization.  When the maximizer can be found exactly, this term can be {eliminated}. As it is usually intractable to compute the accurate maximizer in the non-convex case, approximation methods are applied. Then, the smaller the $\epsilon$ is, the better the exact maximizer can be approximated by the Taylor expansion.

\section{Computer simulations}
\begin{figure*}[htbp]
\centering
\subfigure[]
{\label{fig1}\includegraphics[width=4.5cm]{graph20.png}}
\subfigure[]{\label{g_mnist}\includegraphics[width=4.5cm]{graph30_mnist.png}}
\subfigure[]{\label{g_cifar}\includegraphics[width=4.5cm]{graph25_cifar.png}}
\caption{{The randomly generated graph structures used in the experiments. (a) Graph for convex scenario (MNIST and CIFAR10). (b) Graph for non-convex scenario (MNIST). (c) Graph for non-convex scenario (CIFAR10).}}
\label{fig4}
\end{figure*}
\subsection{Logistic regression}
\begin{figure*}[htbp]
\centering
\subfigure[MNIST, $\ell_2$ {perturbations}.]{\label{sub_1}\includegraphics[width=4.4cm]{error_convergence_4.png}}
\subfigure[MNIST, $\ell_\infty$ perturbations.]{\label{sub_2}\includegraphics[width=4.4cm]{error_convergence_0.3.png}}
\subfigure[CIFAR, $\ell_2$ perturbations.]{\label{sub_3}\includegraphics[width=4.4cm]{error_convergence_1.5.png}}x
\subfigure[CIFAR, $\ell_\infty$ perturbations.]{\label{sub_4}\includegraphics[width = 4.4cm]{error_convergence_0.035.png}}
\caption{Convergence plots for the two datasets using the logistic loss. {The legends show the perturbation bounds used in the training phase. (a) The evolution of the average classification error across all agents over $\ell_2$ adversarial examples bounded by $\epsilon_k = 4$ during training for MNIST. (b)--(d) follow a similar logic but for different norms and datasets.}} %(b) The evolution of the average classification error over $\ell_\infty$ adversarial examples bounded by $\epsilon_k = 0.3$ during training for MNIST; %(c) The evolution of the average classification error of $\ell_2$ adversarial examples bounded by $\epsilon_k = 1.5$ during training for CIFAR10; (d) The evolution of the average classification error of $\ell_\infty$ adversarial examples bounded by $\epsilon_k = 0.035$ during training for CIFAR10. }
\label{fig2}
\end{figure*}
In this section, we illustrate the performance of the {adversarial} diffusion strategy from Algorithm \ref{al_ro_1} using a logistic regression application. Let $\boldsymbol{\gamma}$ be a binary variable that takes values from $\{-1,1\}$, and {let} $\boldsymbol{h} \in \mathbbm{R}^M$ be a feature variable. The robust logistic regression problem by a network of agents employs the risk functions:
\begin{align}\label{18}
J_k (w) = \mathds{E}\max\limits_{\Vert{\delta}\Vert_{p_k}\le \epsilon_k}\left\{\ln{(1+e^{-\boldsymbol{\gamma}(\boldsymbol{h} + {\delta})^{\sf T}{w}}})\right\}
\end{align}
Note that when $p_k = 2$, the analytical solution for the inner maximizer (i.e., the worst-case perturbation) is given by
\begin{align}
    \boldsymbol{\delta}^{\star} = -\epsilon_k\boldsymbol{\gamma}\frac{{w}}{\Vert{w}\Vert}
\end{align}
which is consistent with the perturbation computed from FGM \cite{miyato2016adversarial}. {On the other hand,} if $p_k = \infty$, the closed-form expression for the inner maximizer changes to
\begin{align}
    \boldsymbol{\delta}^{\star} = -\epsilon_k\boldsymbol{\gamma}\mathrm{sign}(w)
\end{align}
which is  the perturbation computed from the FGSM method \cite{goodfellow2014explaining}.



\begin{figure*}[htbp]
\centering
\subfigure[MNIST, $\ell_2$ perturbations.]{\label{sub_1_3}\includegraphics[width=4.4cm]{robustness_plot_4.png}}
\subfigure[MNIST, $\ell_\infty$ perturbations.]{\label{sub_2_3}\includegraphics[width=4.4cm]{robustness_plot_0.3.png}}
\subfigure[CIFAR, $\ell_2$ perturbations.]{\label{sub_3_3}\includegraphics[width=4.4cm]{robustness_plot_1.5.png}}
\subfigure[CIFAR, $\ell_\infty$ perturbations.]{\label{sub_4_3}\includegraphics[width=4.4cm]{robustness_plot_0.035.png}}
\caption{{Robustness plots for the two datasets in convex environments. (a) Average classification error over the graph versus perturbation size for MNIST to $\ell_2$ attacks. The legends demonstrate the perturbation bound used in the training phase and the attack method in the test phase. For instance, {$\epsilon = 0$ (FGM) corresponds to robustness of the clean network to FGM attack.} (b)--(d) follow the similar logic but for different norms and datasets.}}% (b) Classification error versus perturbation size for MNIST to $\ell_\infty$ attacks; (c) Classification error versus perturbation size for CIFAR10 to $\ell_2$ attacks; (d)  Classification error versus perturbation size for CIFAR10 to $\ell_\infty$ attacks. Each subplot show four curves illustrating the robust behavior of the traditional (nonrobust) algorithm and the proposed adversarial algorithm to worst-case perturbations generated by means of FGM or FGSM and attacks by DeepFool.}
\label{fig3}
\end{figure*}
In our experiments, we use both the MNIST \cite{deng2012mnist} and CIFAR10 \cite{krizhevsky2010cifar} datasets, and randomly generate a graph with 20 nodes, shown in {Fig.} \ref{fig1}. For each dataset, three networks are trained, including two homogeneous adversarial networks {using $\ell_2$ and $\ell_\infty$ bounded perturbations} where all agents are attacked by the same {type} of perturbations, and one clean network {trained with original clean samples}. We limit our simulations to binary classification in this example.  For this reason, we consider samples with digits \textit{$0$} and \textit{$1$} from MNIST, and images for airplanes and automobiles from CIFAR10. For the $\ell_2$ {case}, we set the perturbation bound in (\ref{18}) to $\epsilon_k = 4$ for MNIST and $\epsilon_k = 1.5$ for CIFAR10. {In the same token}, the attack {strengths} are set to $\epsilon_k = 0.3$ for MNIST and $\epsilon_k = 0.035$ for CIFAR10 under the $\ell_\infty$ {setting}. In the test phase, we compute the average classification error across the networks to measure the performance of the multi-agent system against perturbations of different strengths. {In other words,  we test the robustness of the trained networks in the worst situation where all agents receive adversarial examples}. %$Also, we consider homogeneous networks in where al agents are attacked by the same perturbation during the training process. We will


We first illustrate the convergence of the algorithm, as anticipated by Theorem \ref{th_mse}. From {Fig.} \ref{fig2}, we observe a steady decrease in the classification error towards a limiting value for each subplot.

The robust behavior of the algorithm to $\ell_2$ and $\ell_\infty$ attacks is illustrated in {Fig.} \ref{fig3} for both MNIST and CIFAR10. We comment on the curves for MNIST and similar remarks hold for CIFAR10. In the test phase, we use perturbations generated in one of two ways: using the worst-case (FGM for $\ell_2$ {and} FGSM for $\ell_\infty$) construction and also using the DeepFool construction \cite{moosavi2016deepfool}. %Each subplot shows four curves that are consistent with (\ref{la_e}). 
The red curve is obtained by training the network using the traditional diffusion learning strategy without accounting for robustness. The network is fed with worst-case perturbed samples (generated using FGM or FGSM) during testing corresponding to different levels  of attack strength. The red curve shows that the classification error deteriorates rapidly. The blue curve repeats the same experiment except that the network is now trained with Algorithm \ref{al_ro_1}.  It is seen in the blue curve that the testing error is more resilient and the degradation is better controlled than the red curve. The same experiments are repeated using the same adversarially trained network and the clean one to show robustness to DeepFool attacks, where the perturbed samples are now generated using DeepFool as opposed to FGM or FGSM {in the test phase}. The purple curve shows the robustness of the clean network to DeepFool, while the green one corresponds to the adversarially trained network. {Note that for  $\ell_\infty$ attacks, i.e., Figs. \ref{sub_2_3} and \ref{sub_4_3}, the DeepFool and FGSM possess similar destructive power so that the red and the purple curves, and also the blue and green curves, coincide with each other}. Here robustness can be explained from two perspectives. First, the attack strength of DeepFool never {outperforms} the worst-case perturbations for the same model, so that robustness to the worst-case perturbation guarantees robustness to DeepFool. Second, {comparing} the purple and green curves, the evolution of the robustness of the networks to DeepFool is better controlled by our algorithm than the clean networks.


To further show the generalization ability of our algorithm, we test the robustness of the $\ell_2$--trained networks to a kind of heuristic noise called decision boundary attack (DBA) \cite{BrendelRB18}. The associated results are shown in Table \ref{tab1}, from which we observe that the robustness of the adversarially trained networks {is} better than their clean counterparts.
\begin{table}[h!t]
\scriptsize
   \centering
   \begin{spacing}{1.5}
   \caption{The classification error of the $\ell_2$ trained networks under DBA.}
   \label{tab1}
\begin{tabular}{c c c}
\toprule
&clean&adv\\
\midrule
MNIST&53.61\%&4.12\%\\
CIFAR10&40.76\%& 27.43\%\\
\bottomrule
\end{tabular}
\end{spacing}
\end{table}

In Fig. \ref{fig4_visual}, we plot some randomly selected CIFAR10 images, their perturbed versions, and the classification decisions generated by the nonrobust algorithm and its adversarial version {listed in Algorithm \ref{al_ro_1}}. We observe from the figure that no matter which attack method is applied, the perturbations are always imperceptible to the human eye. Moreover, while the nonrobust algorithm fails to classify correctly in most cases, the adversarial algorithm is more robust and leads to fewer classification errors.

\begin{figure}[htbp]
\centerline{\includegraphics[width=8cm]{images.png}}
\caption{Visualization of the original and adversarial samples. The first row consists of 10 random original samples with the titles representing their true classes. The second row shows the adversarial examples generated by DeepFool and applied to {a graph trained by the standard nonrobust algorithm.} The third row shows the results obtained by the adversarial {diffusion} algorithm. The titles are the predictions by the corresponding models. The same construction is repeated in the last two rows using FGM. If the prediction of an image is wrong, the title is shown in red color. It is seen that the adversarial algorithm fails less frequently.} 
\label{fig4_visual}
\end{figure}


\subsection{Neural networks}

In this section, we illustrate the effectiveness of the {adversarial diffusion strategy (\ref{newx_e_n})--(\ref{a3_e_n})} in {non-convex environments for the} MNIST and CIFAR10 datasets for both homogeneous and heterogeneous networks.  In the simulation, we utilize the random initialization technique from \cite{WongRK20}, which has been shown to enhance the stability of the adversarial training process in the single-agent case.

\textbf{MNIST setup}. For MNIST, we randomly generate a graph with 30 nodes for the distributed setting, and its structure is shown in {Fig.} \ref{g_mnist},  where each node corresponds to a convolutional neural network 
 (CNN) model following a similar structure to the one used in \cite{madry2017towards} but adding the batch normalization \cite{sayed_2023, IoffeS15} layer. The output size of the last layer is 10. We set the perturbation bounds to $\epsilon_k = 2$ for $\ell_2$ attacks and $\epsilon_k = 0.3$ for $\ell_\infty$ ones.                                                                                 

\textbf{CIFAR10 setup}. We use a randomly generated graph with 25 agents, where we now use the CNN structure from \cite{ZhangYJXGJ19} for each agent. The graph structure is {shown in Fig. \ref{g_cifar}}. The perturbation bounds are set to $\epsilon_k = 0.5$ for $\ell_2$ attacks 
 and $\epsilon_k = 0.03$ for $\ell_\infty$ attacks. Note that as mentioned before, our algorithm is not restricted to the {stochastic gradient} optimizer. {Since} the CIFAR10 dataset is more complex than MNIST, the  {stochastic gradient} momentum method \cite{Qian99} with cyclic learning rates \cite{WongRK20} is applied to each agent.
 

\subsubsection{Homogeneous networks} We show the results corresponding to the homogeneous networks, where all agents in {the} graph are attacked by the same type of perturbation (i.e., {$\epsilon_k$ and $p_k$ is uniform for all $k$}). For instance, for MNIST with  $\ell_2$ attacks, all agents receive perturbations bounded by $\epsilon_k = 2$, and the global risk is
\begin{align}
    J(w) = \sum\limits_{k=1}^{30} \pi_k\mathds{E}\max\limits_{\Vert\delta_k\Vert\le 2}Q_k(w;\boldsymbol{x}_k+\delta_k,\boldsymbol{y}_k)
\end{align} 

\begin{figure*}[htbp]
\centering
\subfigure[MNIST, $\ell_2$ perturbations.]{\label{sub_1_5}\includegraphics[width=4.4cm]{convergence_2_homo.png}}
\subfigure[MNIST, $\ell_\infty$ perturbations.]{\label{sub_2_5}\includegraphics[width=4.4cm]{convergence_0.3_homo.png}}
\subfigure[CIFAR, $\ell_2$ perturbations.]{\label{sub_3_5}\includegraphics[width=4.4cm]{convergence_0.5_homo.png}}
\subfigure[CIFAR, $\ell_\infty$ perturbations.]{\label{sub_4_5}\includegraphics[width = 4.4cm]{convergence_0.03_homo.png}}
\caption{Convergence plots for the two datasets in non-convex {environments. The legends show the perturbation bound used in the training phase. (a) The evolution of the average classification error across all agents over $\ell_2$ adversarial examples bounded by $\epsilon_k = 2$ during training for MNIST. (b)--(d) follow a similar logic but for different norms and datasets.}}
\label{fig5}
\end{figure*}

Fig. \ref{fig5} shows the convergence plots for the two datasets, from which we observe that the average classification error across the networks to the trained attacks converges after {sufficient} iterations.

\begin{figure*}[htbp]
\centering
\subfigure[MNIST, $\ell_2$ perturbations.]{\label{sub_1_6}\includegraphics[width=4.4cm]{robustness_plot_2.png}}
\subfigure[MNIST, $\ell_\infty$ perturbations.]{\label{sub_2_6}\includegraphics[width=4.4cm]{robustness_plot_0.3_non-convex.png}}
\subfigure[CIFAR, $\ell_2$ perturbations.]{\label{sub_3_6}\includegraphics[width=4.4cm]{robustness_plot_0.5.png}}
\subfigure[CIFAR, $\ell_\infty$ perturbations.]{\label{sub_4_6}\includegraphics[width=4.4cm]{robustness_plot_0.03.png}}
\caption{{Robustness plots for the two datasets in non-convex environments. The red, blue, green and purple curves in each subplot correspond to the robustness behaviour of the clean, homogeneous, clean+adv, and $\ell_2+\ell_\infty$ heterogeneous networks respectively.  (a) Average classification error across the networks versus perturbation size to FGM attack for MNIST. (b)--(d) follow a similar logic but for different norms and  datasets.}}
\label{fig6}
\end{figure*}

\begin{table}[h!t]
\scriptsize
   \centering
   \begin{spacing}{1.5}
   \caption{The classification error for the homogeneous networks under various attacks.}
   \label{tab2}
\begin{tabular}{c c c c c c c c c c  c c c c c}
\toprule
data&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST\\
norm&$\ell_2$& $\ell_2$&$\ell_2$&$\ell_\infty$&$\ell_\infty$&$\ell_\infty$\\
attack&PGM&DeepFool&FMN&PGD&DeepFool&FMN\\
error-clean&100\%&100\%&100\%&100\%&100\%&100\%\\
error-diff&21\%&18\%&24\%&20\%&14\%&25\%\\
error-nonco&33\%&37\%&40\%&52\%&34\%&54\%\\
error-centra&22\%&22\%&28\%&24\%&14\%&25\%\\
\midrule
data&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR\\
norm&$\ell_2$& $\ell_2$&$\ell_2$&$\ell_\infty$&$\ell_\infty$&$\ell_\infty$\\
attack&PGM&DeepFool&FMN&PGD&DeepFool&FMN\\
error-clean&99\%&98\%&83\%&100\%&100\%&100\%\\
error-diff&32\%&35\%&34\%&50\%&49\%&57\%\\
error-nonco&63\%&66\%&66\%&79\%&82\%&83\%\\
error-centra&37\%&39\%&39\%&57\%&59\%&63\%\\
\bottomrule
\end{tabular}
\end{spacing}
\end{table}
We next show how the robustness of the multi-agent systems can be improved by the proposed algorithm compared with the clean networks. {Similar to the convex case, three networks are trained for each dataset, including two homogeneous networks adversarially trained by $\ell_2$ and $\ell_\infty$ bounded perturbations respectively, and one clean system fed only with clean samples in the training phase, which is equivalent to the case of $\epsilon = 0$.} Fig. \ref{fig6} illustrates the robust behavior of the multi-agent systems to the attacks that are visible in the training process. That is, {we observe from the red curve in each subplot a rapid increase for the average classification error across the network to adversarial examples.} This demonstrates the vulnerability of the clean models. Fortunately, this phenomenon can be mitigated by the proposed algorithm as the blue curve shows. We further test the robustness of the networks to some multi-step attacks, i.e., PGD or PGM \cite{madry2017towards}, DeepFool\cite{moosavi2016deepfool}, and FMN \cite{PintorRBB21}, with the Foolbox package \cite{rauber2017foolbox}, and the results are shown in Table \ref{tab2}, where the row \textit{error-clean} corresponds to the classification error of the clean networks, and \textit{error-diff} relates to the robustness of the adversarially trained networks by our diffusion algorithm. Different from the convex case where the closed-form solution for the inner maximization can be exactly obtained, it is only possible to find an approximation for it in the non-convex case. Thus, the FGM and FGSM attacks used in the training process are not the most powerful (i.e., worst-case) perturbations {within the norm-bound regions}.\\
\begin{figure*}[htbp]
\centering
\subfigure[MNIST, $\ell_2$ perturbations.]{\label{fs_11}\includegraphics[width=4.4cm]{coop_noncoop_centra_2.png}}
\subfigure[MNIST, $\ell_\infty$ perturbations.]{\label{fs_12}\includegraphics[width=4.4cm]{coop_noncoop_centra_0.3.png}}
\subfigure[CIFAR, $\ell_2$ perturbations. ]{\label{fs_13}\includegraphics[width=4.4cm]{coop_noncoop_centra_0.5.png}}
\subfigure[CIFAR, $\ell_\infty$ perturbations.]{\label{fs_14}\includegraphics[width = 4.4cm]{coop_noncoop_centra_0.03.png}}
\caption{{Comparison among three distributed strategies: diffusion, non-cooperative, and centralized. (a) Classification error versus perturbation size for MNIST to FGM; (b)--(d) follow a similar logic but for different norms and  datasets.}}
\label{fig_coopvsnoncoop}
\end{figure*}
 We finally compare the robustness of the networks adversarially trained by three different distributed strategies: diffusion (i.e., our approach), {non-cooperative,} and centralized. Basically, all agents train their own models independently in the non-cooperative setting, while all agents share their data with a fusion center to do the computation in the centralized method. The results are shown in Fig. \ref{fig_coopvsnoncoop} and Table \ref{tab2}, where the row \textit{error-nonco} corresponds to the robustness of the non-cooperative networks, and  \textit{error-centra} shows the results associated with the centralized method. {Comparing} the results associated with the three methods from Fig. \ref{fig_coopvsnoncoop} and Table \ref{tab2}, we observe that the diffusion networks are  more robust than the non-cooperative ones. This is due to the cooperation over the graph topology. Furthermore, we surprisingly observe that the robustness of the diffusion networks outperforms that of the centralized architectures, especially for the CIFAR10 dataset.   {This behavior is different from what one would observe under clean training \cite{sayed2014adaptation,kayaalp2022dif}  and from the view that the decentralized method may hurt generalization \cite{ZhuHZNST22}.} {One explanation for this phenomenon in the context of adversarial training is that decentralized stochastic gradient algorithms {have been shown} to favor more flat minimizers than the centralized method, and flat minimizers { in turn have} been shown to generalize better than sharp minimizers \cite{KeskarMNST17} . To illustrate this effect, we plot the following empirical risk function:}%Actually, the recent work \cite{zhuwould} also show a similar empirical result that the decentralized SGD generalized better than the centralized one in the clean training using large batch size. Similar to \cite{zhuwould}, 
 \begin{align}
        \frac{1}{N}\sum\limits_{i=1}^{N}Q(w;x_i + \widehat{\delta}_i(w), y_i)
\end{align}
{for choices of the form $w = \alpha w_{d} + (1 - \alpha) w_{c}$, where $w_d$ corresponds to the centroid model obtained by our decentralized algorithm, and $w_c$ is the centralized model. The result is shown in Fig. \ref{loss_visual}}, from which we observe that the neighborhood of the centroid {from the distributed solution is more flat than the sharp minima for the centralized solution}. We also notice that the superiority of the diffusion method {over the centralized solution} is amplified in the context of adversarial training. For example, for CIFAR10, the clean accuracy of the diffusion network is $1\%$ higher than the centralized one in the clean training. {However, this gap increases to $5\%$ or even more (as Fig. \ref{fig_coopvsnoncoop} and Table \ref{tab2} show) under adversarial training. This observation suggests that there is merit to networked learning where the graph topology contributes to robustness.} %
 

 \begin{figure}[htbp]
\centering
\subfigure[CIFAR, $\ell_2$ perturbations.]{\label{sub_loss_v_1}\includegraphics[width=4.3cm]{losses_0.5.png}}
\subfigure[CIFAR, $\ell_\infty$ perturbations.]{\label{sub_loss_v_2}\includegraphics[width=4.3cm]{losses_0.03.png}}
\caption{The loss visualization for CIFAR10. The choice $\alpha = 0$  corresponds to the centralized model, while $\alpha = 1$  corresponds to the centroid model.}
\label{loss_visual}
\end{figure}
 

\subsubsection{Heterogeneous networks}
It is possible to consider situations where the perturbations at the agents are bounded by different norms $p_k$ or attack {strengths} $\epsilon_k$. This motivates us to study the robustness in heterogeneous settings. We simulate two kinds of heterogeneity in the experiment. In the first case, which is {referred to} \textit{clean+adv}, half of the agents are trained adversarially while the other half is trained in the normal {manner} with clean samples. In the second type of heterogeneous networks, which are {referred to} \textit{$\ell_2$+$\ell_\infty$}, half of the agents receive attacks generated from the $\ell_2$ norm, while the other half are attacked by $\ell_\infty$ perturbations.

\begin{figure*}[htbp]
\centering
\subfigure[MNIST: clean+adv, $\ell_2$]{\label{sub_1_7}\includegraphics[width=4.4cm]{convergence_2_clean+adv.png}}
\subfigure[MNIST: clean+adv, $\ell_\infty$]{\label{sub_2_7}\includegraphics[width=4.4cm]{convergence_0.3_clean+adv.png}}
\subfigure[MNIST: $\ell_2$+$\ell_\infty$, $\ell_2$]{\label{sub_3_7}\includegraphics[width=4.4cm]{convergence_2_multi.png}}
\subfigure[MNIST: $\ell_2$+$\ell_\infty$, $\ell_\infty$]{\label{sub_4_7}\includegraphics[width = 4.4cm]{convergence_0.3_multi.png}}
\subfigure[CIFAR10: clean+adv, $\ell_2$]{\label{sub_5_7}\includegraphics[width = 4.4cm]{convergence_0.5_clean+adv.png}}
\subfigure[CIFAR10: clean+adv, $\ell_\infty$ ]{\label{sub_6_7}\includegraphics[width = 4.4cm]{convergence_0.03_clean+adv.png}}
\subfigure[CIFAR10: $\ell_2$+$\ell_\infty$, $\ell_2$]{\label{sub_7_7}\includegraphics[width = 4.4cm]{convergence_0.5_multi.png}}
\subfigure[CIFAR10: $\ell_2$+$\ell_\infty$, $\ell_\infty$]{\label{sub_8_7}\includegraphics[width = 4.4cm]{convergence_0.03_multi.png}}
\caption{Convergence plots for the two datasets in non-convex environments. (a) The evolution of the average classification error over $\ell_2$ adversarial examples bounded by $\epsilon_k = 2$ when training the clean+adv network associated with $\ell_2$ attacks for MNIST. (b) The evolution of the average classification error over $\ell_\infty$ adversarial examples bounded by $\epsilon_k = 0.3$ when training the clean+adv network associated with $\ell_\infty$ attacks for MNIST. (c) The evolution of the average classification error over $\ell_2$ adversarial examples bounded by $\epsilon_k = 2$ when training the $\ell_2$+$\ell_\infty$  network for MNIST. (d) The evolution of the average classification error over $\ell_\infty$ adversarial examples bounded by $\epsilon_k = 0.3$ when training the $\ell_2$+$\ell_\infty$  network for MNIST. (e)--(h) follow a similar logic to (a)--(b) except now the dataset is CIFAR10.
}
\label{fig7}
\end{figure*}

To train both  heterogeneous networks, we repeat the procedure from training the homogeneous networks except that the upper bounds $\epsilon_k$ or the $\ell_{p_k}$ norms may vary over the graph. The convergence plots are shown in Fig. \ref{fig7}, where we show the evolution of the average classification error across all agents to the attacks seen in the training process. 

\begin{table*}[h!t]
\scriptsize
   \centering
   \begin{spacing}{1.5}
   \caption{The classification error of heterogeneous networks to various attacks.}
   \label{tab3}
\begin{tabular}{c c c c c c c c c c c c c c c}
\toprule
dada&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST&MNIST\\

attack&PGM&DeepFool&FMN&PGD&DeepFool&FMN&PGM&DeepFool&FMN&PGD&DeepFool&FMN\\
model&clean+adv&clean+adv&clean+adv&clean+adv&clean+adv&clean+adv&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&\\
$\epsilon$&2&2&2&0.3&0.3&0.3&2&2&2&0.3&0.3&0.3\\
error&24\%&22\%&29\%&54\%&22\%&54\%&14\%&13\%&17\%&55\%&22\%&52\%
\\
\midrule
dada&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR&CIFAR\\
attack&PGM&DeepFool&FMN&PGD&DeepFool&FMN&PGM&DeepFool&FMN&PGD&DeepFool&FMN\\
model&clean+adv&clean+adv&clean+adv&clean+adv&clean+adv&clean+adv&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&$\ell_2$+$\ell_\infty$&\\
$\epsilon$&0.5&0.5&0.5&0.03&0.03&0.03&0.5&0.5&0.5&0.03&0.03&0.03\\
error&37\%&38\%&37\%&57\%&53\%&67\%&31\%&33\%&31\%&52\%&53\%&63\%\\
\bottomrule
\end{tabular}
\end{spacing}
\end{table*}

The robustness of the heterogeneous networks are illustrated by Fig. \ref{fig6} and Table \ref{tab3}. The green curves in Fig. \ref{fig6} show the robustness evolution of the clean+adv networks to the trained perturbations against different attack strength and the purple curves correspond to the $\ell_2+\ell_\infty$ networks. Comparing the results of the clean+adv and traditional clean networks in Fig. \ref{fig6}, {from} Table \ref{tab2} and Table \ref{tab3}, it is obvious that the former systems perform better than the latter ones. Thus, due to cooperation across the networks, the adversarial agents help improve the robustness of clean agents in the clean+adv networks. Likewise, comparing the results between the homogeneous and clean+adv networks, we observe some improvement for the clean accuracy in the clean+adv networks while their robustness {deteriorates since} now only a half of the agents are trained adversarially. This {observation is associated with the} trade-off between clean accuracy and robustness \cite{zhang2019theoretically,PangLYZY22}, which is still an open issue even in the single-agent case.  Similarly, as for the performance of the $\ell_2+\ell_\infty$ networks, we observe that this kind of heterogeneous networks are still more robust than the traditional clean ones. Moreover, their robustness to  $\ell_2$ attacks are enhanced by the $\ell_\infty$ agents. However, their robustness to $\ell_\infty$ agents decreases. This phenomenon relates to the trade-off among multiple perturbations \cite{tramer2019adversarial, maini2020adversarial} in single-agent learning.  %related to the trade-off problemsand should be studied futher in the future. The robustness is not as good as the homogeneous ones, but the degradation is acceptable.

%the visualization.

%: (1) We show the convergence of our algorithm; (2) We verify that the robustness of multi-agent systems can be improved via adversarial training compared with standard training. We use a randomly generated graph with 30 nodes for the distributed setting, where each agent is a neural network, and all agents work dependently in the cooperative manner according to the proposed algorithm. 


%\begin{figure}[htbp]
%\centerline{\includegraphics[width=6cm]{loss_convergence.png}}
%\caption{The evolution of the loss trained by the diffusion strategy with $\epsilon = 3$}
%\label{fig4}
%\end{figure} 

%First, we show the experimental results that support our convergence theory.  From Figure\ref{fig3}, it can be demonstrated that the average adversarial loss across the network trained with $\epsilon = 3$ can converge after enough iterations.


%Then, we show that the robustness of the multi-agent systems after adversarial training with the diffusion algorithm can be improved compared with standard diffusion training, where the term \textit{standard} represents the clean environment without adversarial examples. The experimental results are shown in Figure\ref{fig4}, where we compare the robustness (i.e., classification error) of networks to different levels of $\ell_2$ attacks with $\epsilon$ ranging from 0 to 3 after training with $\epsilon = \{0,1,2,3\}$ respectively. Note that the curve corresponding to $\epsilon = 0$ means standard training, and the larger value of $\epsilon$ mean stronger attack. According to Figure\ref{fig4}, it is obvious that the clean network (i.e., trained with $\epsilon = 0$) is vulnerable to the crafted perturbations as its classification error goes up quickly when the attack becomes stronger. Fortunately, this phenomenon can be alleviated by doing adversarial training as
%the evolution of the classification error of the networks trained by $\epsilon  = \{1,2,3\}$ are more flat than the standard one. What is more, in $\ell_2$ attacks with $\epsilon$ from $1$ to $3$, the robustness of networks is improved with the increase of $\epsilon$ used in the training process.

\section{Conclusion}
In this paper, we proposed a diffusion defense mechanism for adversarial attacks {over graphs}. We analyzed the different convergence properties of the proposed framework for convex and non-convex losses. We further illustrated the behavior of the trained networks to perturbations generated by different attack methods, and {illustrated} the enhanced robust behavior. Future directions can be on the trade-off problems in heterogeneous networks, which add more flexibility to the system than traditional single-agent learning.




\begin{appendices}
\section{Proof of lemma \ref{affine_l}}\label{ap1}
\begin{proof}

Before proving the affine Lipshitz condition, we first note that the $\ell_2$ distance between any two possible perturbations $\delta_{k,1}$ and $\delta_{k,2}$ from the region $\Vert\delta\Vert_{p_k}\le \epsilon_k$ can be bounded {in view of} the equivalence of vector norms. {Specifically, for any two vector norms} $\Vert\cdot\Vert_a$ and $\Vert\cdot\Vert_b$ defined {over a} finite-dimensional space, there exist real numbers $0<c_1\le c_2$ {such that for any $\delta \in \mathbbm{R}^M$}:
\begin{align}
c_1\Vert\delta\Vert_a \le \Vert\delta\Vert_b \le c_2\Vert\delta\Vert_a
\end{align}
according to which we say that for any $\ell_p$ norm, if $\Vert\delta\Vert_p$ is bounded by $\epsilon_k$, then $\Vert\delta\Vert$ is bounded by $O(\epsilon_k)$. For example, when $p_k = 1$, we have
\begin{align}
   \Vert \delta_{k,2} - \delta_{k,1}\Vert \le \Vert \delta_{k,2} - \delta_{k,1}\Vert_1 \le 2\epsilon_k
\end{align}
{and when} $p_k = \infty$, we have
\begin{align}
   \Vert \delta_{k,2} - \delta_{k,1}\Vert \le \sqrt{M}\Vert \delta_2 - \delta_1\Vert_\infty \le 2\sqrt{M}\epsilon_k
\end{align}
Furthermore, consider the {maximum perturbation bound across all agents and denote it by} 
\begin{equation}
    \epsilon = \max\limits_{k}\epsilon_k
\end{equation}
{Then, the $\ell_2$ distance between any two possible perturbations $\delta_k$ and $\delta_\ell$ from the regions} $\Vert\delta\Vert_{p_k}\le \epsilon_k$ and  $\Vert\delta\Vert_{p_\ell}\le \epsilon_\ell$ can also be bounded even if $p_k \neq p_\ell$ and $\epsilon_k \neq \epsilon_\ell$. For example, if $p_k = \infty$ and $p_\ell = 2$, we have
\begin{align}
    \Vert \delta_k - \delta_\ell\Vert \le \Vert\delta_k\Vert + \Vert\delta_\ell\Vert \le \sqrt{M}\epsilon_k + \epsilon_\ell = O(\epsilon)
\end{align}
{Thus, for later use, no matter which perturbation norm is used, we can assume for any agents $k$ and $\ell$ that the Euclidean norm of the respective perturbations satisfy}
\begin{align}
\label{e_p_1}\Vert \delta_{k,2} - \delta_{k,1}\Vert \le O(\epsilon_k),\;\Vert \delta_k - \delta_\ell\Vert \le  O(\epsilon)
\end{align}
{Next, for any  $w_1, w_2 \in \mathbbm{R}^M$, consider the problems:}
\begin{align}
  \boldsymbol{\delta}^\star_{2} =  \mathop{\rm{argmax}}\limits_{\left\Vert{\delta}\right\Vert_{p_k}  \le \epsilon_k}   Q_k(w_2;\boldsymbol{x}_{k} +{\delta},\boldsymbol{y}_{k})\\
  \boldsymbol{\delta}^\star_{1} =  \mathop{\rm{argmax}}\limits_{\left\Vert{\delta}\right\Vert_{p_k}  \le \epsilon_k}   Q_k(w_1;\boldsymbol{x}_{k} +{\delta},\boldsymbol{y}_{k})
\end{align}
{we then have:}
\begin{align}\label{AA_3}
  & \Vert \nabla_w  Q_k(w_2; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) - \nabla_w Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_1^\star,\boldsymbol{y}_k)\Vert\notag\\
  & =  \Vert \nabla_w  Q_k(w_2; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) - \nabla_w  Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k)  \notag \\
  &\quad + \nabla_w Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) - \nabla_w Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_1^\star,\boldsymbol{y}_k)\Vert\notag\\
  & \le  \Vert \nabla_w  Q_k(w_2; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) - \nabla_w  Q_k(w_1; \boldsymbol{x}_k + \boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) \Vert  \notag \\
    & \quad  + \Vert \nabla_w Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) - \nabla_w Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_1^\star,\boldsymbol{y}_k)\Vert\notag\\
  & \overset{(a)}{\le}  L\Vert w_2 - w_1 \Vert + L\Vert \boldsymbol{\delta}_{2}^\star - \boldsymbol{\delta}_1^\star \Vert  \notag\\
   & \le  L\Vert w_2 - w_1 \Vert + O(\epsilon_k)
\end{align}
where $(a)$ follows from (\ref{smooth_q}) and (\ref{as2_3}). {Then, exchanging the expectation and differentiation operations (as permitted by the dominated convergence theorem in our cases of interest [4]) we get}
\begin{align}
    & \Vert \nabla_w J_k(w_2) - \nabla_w J_k(w_1) \Vert^2 \notag\\
  & \overset{(a)}{=}  \Vert \nabla_w \mathds{E} Q_k(w_2; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) - \nabla_w \mathds{E} Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_1^\star,\boldsymbol{y}_k)\Vert^2 \notag\\
   & \le  \mathds{E}\Vert \nabla_w  Q_k(w_2; \boldsymbol{x}_k+\boldsymbol{\delta}_2^\star,\boldsymbol{y}_k) - \nabla_w Q_k(w_1; \boldsymbol{x}_k+\boldsymbol{\delta}_1^\star,\boldsymbol{y}_k)\Vert^2 \notag\\
     & \overset{(b)}{\le}   2L^2\Vert w_2 - w_1 \Vert^2 + O(\epsilon^2_k)
\end{align}
where $(a)$ follows from (\ref{danskin}), and $(b)$ follows from (\ref{AA_3}) and { Jensen's inequality:}
\begin{align}\label{AA_5}
    \Vert x+ y\Vert^2 \le 2\Vert x \Vert^2+ 2\Vert y \Vert^2
\end{align}
\end{proof}

\section{Proof of Lemma \ref{p_gn}}\label{ap2}
\begin{proof}
We first {verify that the gradient noise has zero mean. Thus, note that}
\begin{align}
    & \mathds{E}\left\{\boldsymbol{s}_{k,n}|\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
    & =  \mathds{E} \left\{\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^{\star},\boldsymbol{y}_{k,n}) - \nabla_w J_k(\boldsymbol{w}_{k,n-1}) |\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
    & = \mathds{E} \left\{\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^{\star},\boldsymbol{y}_{k,n})|\boldsymbol{\mathcal{F}}_{n-1}\right\}  \notag\\
    &\quad   - \left\{ \nabla_w  \mathds{E} Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^{\star},\boldsymbol{y}_{k,n})|\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
    &=  0
\end{align}
Then, for the mini-batch gradient noise, we have
\begin{align}\label{sb0}
    &\mathds{E}\Big\{\boldsymbol{s}^{B}_{k,n}|\boldsymbol{\mathcal{F}}_{n-1}\Big\} \notag\\
    & =  \mathds{E}\Bigg\{\frac{1}{B}\sum\limits_{b=1}^{B}\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}^{b,\star}_{k,n},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad -  \nabla_w J_k(\boldsymbol{w}_{k,n-1})\Big\vert\boldsymbol{\mathcal{F}}_{n-1}\Bigg\}\notag\\
    & =  \frac{1}{B}\sum\limits_{b=1}^{B}\mathds{E} \left\{\boldsymbol{s}_{k,n}|\boldsymbol{\mathcal{F}}_{n-1}\right\} = 0
\end{align}
{Next we assess} the variance of  the gradient noise:
\begin{align}\label{AB_2}
    & \mathds{E}\left\{\Vert \boldsymbol{s}_{k,n}\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
    & =   \mathds{E} \left\{ \Vert\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^\star, \boldsymbol{y}_{k,n})\right. \notag\\
    & \quad \left. - \nabla_w J_k(\boldsymbol{w}_{k,n-1})\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
    & =  \mathds{E}\Vert\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^\star, \boldsymbol{y}_{k,n}) \notag\\
    &\quad - \nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n}) \notag\\
    &\quad  +  \nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n}) - \nabla_w J_k(\boldsymbol{w}_{k,n-1}) \Vert^2 \notag\\
     & \overset{(a)}{\le} 4\mathds{E}\Vert\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^\star,\boldsymbol{y}_{k,n}) \notag\\
    & \quad - \nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n})\Vert^2 \notag\\
    &\quad + 4\mathds{E}\Vert\nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n})\Vert^2\notag\\
    &\quad + 4\mathds{E}\Vert \nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \nabla_w J_k(w^\star)\Vert^2 +4\Vert\nabla_w J_k(w^\star)\Vert^2
\end{align}
where $(a)$ follows from (\ref{AA_5}), $\boldsymbol{x}_{k,n}^\star$ follows from (\ref{max_k}) in which $\boldsymbol{\delta}_{k,n}^\star$ is the maximizer associated with $\boldsymbol{w}_{k,n-1}$, {and} 
\begin{align}
\boldsymbol{\delta}_k^\star(w^\star) = \mathop{\rm{argmax}}\limits_{\left\Vert{\delta}\right\Vert_{p_k}  \le \epsilon_k}   Q_k(w^\star;\boldsymbol{x}_{k,n} +{\delta},\boldsymbol{y}_{k,n})
\end{align}
is the maximizer corresponding to the global minimizer $w^\star$. Note that 
\begin{align}\label{AB_4}
    \nabla_w J_k(\boldsymbol{w}_{k,n-1}) = \mathds{E}\nabla_w Q_k(\boldsymbol{w}_{k,n-1}; \boldsymbol{x}_{k,n}^\star, \boldsymbol{y}_{k,n})
\end{align}
\begin{align}\label{AB_5}
    \nabla_w J_k(w^\star) = \mathds{E}\nabla_w Q_k(w^\star; \boldsymbol{x}_{k,n}+ \boldsymbol{\delta}_k^\star(w^\star), \boldsymbol{y}_{k,n})
\end{align}
and from Jensen's inequality, we have
\begin{align}\label{AB_6}
    \Vert\mathds{E}\boldsymbol{x}\Vert^2 \le \mathds{E}\Vert \boldsymbol{x}\Vert^2
\end{align}
Substituting (\ref{AB_4}), (\ref{AB_5}) and (\ref{AB_6}) into (\ref{AB_2}), we obtain
\begin{align}\label{B_e1}
    & \mathds{E}\left\{\Vert \boldsymbol{s}_{k,n}\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
    & \le   8\mathds{E}\Vert\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_{k,n}^\star,\boldsymbol{y}_{k,n}) \notag\\
    & \quad - \nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n})\Vert^2 \notag\\
    & \quad + 8 \mathds{E}\Vert\nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n})\Vert^2\notag\\
     & \overset{(a)}{\le} 16L^2\Vert \boldsymbol{w}_{k,n-1} - w^\star\Vert^2 + O(\epsilon^2_k) \notag\\
    &\quad + 8\mathds{E}\Vert \nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n})\Vert^2
\end{align}
where $(a)$ follows from (\ref{AA_3}) and (\ref{AA_5}). Then for the mini-batch gradient noise, we have
\begin{align}\label{sb2}
        &\mathds{E}\left\{\left\Vert\boldsymbol{s}^{B}_{k,n}\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
        & =  \mathds{E}\Bigg\{\bigg\Vert\frac{1}{B}\sum\limits_{b=1}^{B}\nabla_w Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}^{b,\star}_{k,n},\boldsymbol{y}_{k,n}^{b,\star})  \notag\\
        & \quad  -  \nabla_w J_k(\boldsymbol{w}_{k,n-1})\bigg\Vert^2\bigg\vert\boldsymbol{\mathcal{F}}_{n-1}\Bigg\}\notag\\
    & \overset{(a)}{=}\frac{1}{B^2}\sum\limits_{b=1}^{B}\mathds{E} \left\{\left\Vert\boldsymbol{s}_{k,n}\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
    & \overset{(b)}{\le} \frac{16L^2}{B}\Vert \boldsymbol{w}_{k,n-1} - w^\star\Vert^2 + O(\epsilon^2_k) \notag\\
    & \quad + \frac{8}{B}\mathds{E}\Vert \nabla_w Q_k(w^\star;\boldsymbol{x}_{k,n}+\boldsymbol{\delta}_k^\star(w^\star),\boldsymbol{y}_{k,n})\Vert^2
\end{align}
where $(a)$ is due to the fact that all data are sampled independently, and $(b)$ follows from (\ref{B_e1}). 
\end{proof}

\section{Proof of Theorem \ref{th_mse}}\label{ap3}
\begin{proof}
 We first verify the convergence of $\mathds{E}\Vert\check{\boldsymbol{w}}_{n}\Vert^2$. Conditioning both sides of (\ref{AC_15}) on $\boldsymbol{\mathcal{F}}_{n-1}$, we obtain
\begin{align}\label{AC_17}
    &\mathds{E}\left\{\Vert\check{\boldsymbol{w}}_{n}\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
    & =  \mathds{E}\Big\{ \Vert\mathcal{J}_\alpha^{\sf T}\check{\boldsymbol{w}}_{n-1} + \mu\mathcal{J}_\alpha^{\sf T}\mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1})\notag\\
    & \quad + \mu\mathcal{J}_\alpha^{\sf T}\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B\Vert^2\vert \boldsymbol{\mathcal{F}}_{n-1}\Big\}\notag\\
    & \overset{(a)}{=} \left\Vert\mathcal{J}_\alpha^{\sf T}\check{\boldsymbol{w}}_{n-1} + \mu\mathcal{J}_\alpha^{\sf T}\mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1})\right\Vert^2\notag\\
    & \quad + \mu^2\mathds{E}\left\{\Vert\mathcal{J}_\alpha^{\sf T}\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
    & \overset{(b)}{\le}  t\Vert\boldsymbol{\check{w}}_{n-1}\Vert^2 + \frac{\mu^2t^2}{1-t}\Vert\mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1})\Vert^2 \notag\\
    & \quad + \mu^2t^2\mathds{E}\left\{\Vert\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}
    \end{align}
    where $(a)$ follows from $(\ref{s0_convex})$, {and $(b)$} from Jensen's inequality for any $t\in (0,1)$. We select $t$ as follows, {and} assume a small enough $\alpha$ such that (see \cite[ch.9]{sayed2014adaptation}): 
    \begin{align}\label{AC_18}
        t = \Vert J_{\alpha}^{\sf T}\Vert = \sqrt{\tau (J_\alpha J_\alpha^{\sf T})} = \lambda_2 + \alpha < 1
    \end{align}
Taking expectations of both sides, we have
\begin{align}\label{AC_19}
    \mathds{E}\Vert\check{\boldsymbol{w}}_{n}\Vert^2 \le& t\mathds{E}\Vert\check{\boldsymbol{{w}}}_{n-1}\Vert^2 +  \frac{\mu^2t^2}{1-t}\mathds{E}\Vert\mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1})\Vert^2 \notag\\
     & + \mu^2t^2\mathds{E}\Vert\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B\Vert^2
\end{align}
We now examine terms on the right-hand side of (\ref{AC_19}) one by one. First, with (\ref{AC_12c}), we have
{
\begin{align}\label{AC_20}
   & \left\Vert\mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1})\right\Vert^2 \notag\\
   & =  \left\Vert \mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1}) -\mathcal{V}_R^{\sf T}(\mathbbm{1}\pi^{\sf T}\otimes I_M)\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1}) \right\Vert^2\notag\\
   & \overset{(a)}{\le}  \Vert \mathcal{V}_R^{\sf T}\Vert^2\left(\sum_{k=1}^{K} \left\Vert\nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \sum_{\ell=1}^{K}\pi_\ell\nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\right\Vert^2\right)\notag\\
   & =   \Vert\mathcal{V}_R^{\sf T}\Vert^2\left(\sum_{k=1}^{K} \left\Vert\sum\limits_{\ell=1}^K \pi_\ell\Big(\nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\Big)\right\Vert^2\right)\notag\\
   & \overset{(b)}{\le}  \Vert\mathcal{V}_R^{\sf T}\Vert^2\left(\sum_{k=1}^{K}\sum_{\ell=1}^{K} \pi_\ell\left\Vert\nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\right\Vert^2\right)
\end{align}}where $(a)$ follows from the triangle inequality of norms, and $(b)$ follows from Jensen's inequality. To proceed, the gradient disagreement between any two agents can be bounded as follows:
\begin{align}\label{AC_21}
& \left\Vert\nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\right\Vert^2 \notag\\
&=   \left\Vert\nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \nabla_w J_k(\boldsymbol{w}_{\ell,n-1}) + \nabla_w J_k(\boldsymbol{w}_{\ell,n-1}) 
\right.\notag\\
&\quad - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\left.\right\Vert^2 \notag\\
& \le  2\left\Vert\nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \nabla_w J_k(\boldsymbol{w}_{\ell,n-1})\right\Vert^2 \notag\\
&\quad + 2 \left\Vert\nabla_w J_k(\boldsymbol{w}_{\ell,n-1}) - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\right\Vert^2\notag\\
  &  \overset{(a)}{\le} 4L^2\Vert\boldsymbol{w}_{k,n-1} - \boldsymbol{w}_{\ell,n-1}\Vert^2 + c^2 \notag\\
& \le  8L^2 \Vert\widetilde{\boldsymbol{w}}_{k,n-1} - \bar{\boldsymbol{w}}_{n-1} \Vert^2 + 8L^2\Vert\widetilde{\boldsymbol{w}}_{\ell,n-1} - \bar{\boldsymbol{w}}_{n-1} \Vert^2 + c^2
\end{align}
where $(a)$ follows from Lemma \ref{affine_l} and {the following inequality in which the constant $c$ is defined and the non-iid data over space is considered}:
\begin{align}\label{AC_22}
    & \left\Vert\nabla_w J_k(\boldsymbol{w}_{\ell,n-1}) - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\right\Vert^2 \notag\\
    & \le  \mathds{E}\Vert\nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_k  +\boldsymbol{\delta}_k^\star, \boldsymbol{y}_k) \notag\\
    & \quad - \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_\ell +\boldsymbol{\delta}_\ell^\star, \boldsymbol{y}_\ell) \Vert^2\notag\\
    & \le  \mathds{E}\Vert \nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_k  +\boldsymbol{\delta}_k^\star, \boldsymbol{y}_k) \notag\\
    &\quad - \nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_\ell  +\boldsymbol{\delta}_\ell^\star, \boldsymbol{y}_k) \notag\\
    &\quad + \nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_\ell  +\boldsymbol{\delta}_\ell^\star, \boldsymbol{y}_k) \notag\\
    &\quad - \nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_\ell  +\boldsymbol{\delta}_\ell^\star, \boldsymbol{y}_\ell) \notag\\
    &\quad +  \nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_\ell  +\boldsymbol{\delta}_\ell^\star, \boldsymbol{y}_\ell) \notag\\
    &\quad -  \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_\ell +\boldsymbol{\delta}_\ell^\star, \boldsymbol{y}_\ell) \Vert^2\notag\\
    & \overset{(a)}{\le}  6L^2\mathds{E}\Vert\boldsymbol{x}_k - \boldsymbol{x}_\ell\Vert^2 + 6L^2\mathds{E}\Vert\boldsymbol{\delta}_k - \boldsymbol{\delta}_\ell\Vert^2 + 3L^2 \mathds{E}\Vert\boldsymbol{y}_k - \boldsymbol{y}_\ell\Vert^2 \notag\\
    &\quad + 3C^2\notag\\
    & \overset{(b)}{\le}  O(\epsilon^2) + 3C^2 +  6L^2\mathds{E}\Vert\boldsymbol{x}_k - \boldsymbol{x}_\ell\Vert^2 + 3L^2 \mathds{E}\Vert\boldsymbol{y}_k - \boldsymbol{y}_\ell\Vert^2 \overset{\Delta}{=} c^2
\end{align}
where $(a)$ follows from the Lipschitz conditions in Assumptions \ref{as1} and \ref{as4}, $(b)$ from (\ref{e_p_1}), and $\boldsymbol{\delta}_k^\star$ and $\boldsymbol{\delta}_\ell^\star$ are maximizers associated with agents $k$ and $\ell$ respectively, namely,
\begin{align}\label{AC_23}
  \boldsymbol{\delta}^\star_{k} =  \mathop{\rm{argmax}}\limits_{\left\Vert{\delta}\right\Vert_{p_k} \le \epsilon_k}   Q_k(\boldsymbol{w}_{\ell,n-1};\boldsymbol{x}_k +{\delta},\boldsymbol{y}_k)\\
  \boldsymbol{\delta}^\star_{\ell} =  \mathop{\rm{argmax}}\limits_{\left\Vert{\delta}\right\Vert_{p_\ell}  \le \epsilon_\ell}   Q_\ell(\boldsymbol{w}_{\ell,n-1};\boldsymbol{x}_\ell +{\delta},\boldsymbol{y}_\ell)
\end{align}
Substituting (\ref{AC_21}) into (\ref{AC_20}), we get
\begin{align}\label{AC_25}
    & \left\Vert\mathcal{V}_R^{\sf T}\nabla_w J({{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1})\right\Vert^2 \notag\\ 
    & \le \Vert\mathcal{V}_R^{\sf T}\Vert^2\Bigg(\sum_{k=1}^{K}\sum_{\ell=1}^{K} \pi_\ell\bigg(8L^2 \Vert\widetilde{\boldsymbol{w}}_{k,n-1} - \bar{\boldsymbol{w}}_{n-1} \Vert^2 \notag\\
    & \quad + 8L^2\Vert\widetilde{\boldsymbol{w}}_{\ell,n-1} - \bar{\boldsymbol{w}}_{n-1} \Vert^2 + c^2\bigg)\Bigg)\notag\\
    & =  \Vert\mathcal{V}_R^{\sf T}\Vert^2\Bigg(8L^2\sum_k \Vert\widetilde{\boldsymbol{w}}_{k,n-1} - \bar{\boldsymbol{w}}_{n-1} \Vert^2 +  \notag\\
    &\quad 8KL^2\sum_\ell \pi_\ell\Vert\widetilde{\boldsymbol{w}}_{\ell,n-1} - \bar{\boldsymbol{w}}_{n-1} \Vert^2   + Kc^2\Bigg)\notag\\
    &\overset{(a)}{\le}  \Vert\mathcal{V}_R^{\sf T}\Vert^2\Vert\mathcal{V}_L\Vert^2(8L^2+8KL^2)\Vert\check{\boldsymbol{w}}_{n-1}\Vert^2 + \Vert\mathcal{V}_R^{\sf T}\Vert^2Kc^2 
\end{align}
where in $(a)$ we utilize  (\ref{AC_12a}) and introduce the block vector ${\bar{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1} = {\mbox{\rm col}}\{\bar{\boldsymbol{w}}_{n-1}\}_{k=1}^K$ {to note that}:
\begin{align}\label{AC_26}
   & \sum_k \Vert\widetilde{\boldsymbol{w}}_{k,n-1} - \bar{\boldsymbol{w}}_{n-1} \Vert^2 \notag\\
   &= \Vert {\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1} -{\bar{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1} \Vert^2 =  \Vert(I_{KM} - \mathbbm{1}\pi^{\sf T}\otimes I_M){\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1}\Vert^2\notag\\
   &=\Vert \mathcal{V}_{L} \mathcal{V}_R^{\sf T}{\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1}\Vert^2 = \Vert \mathcal{V}_{L}\check{\boldsymbol{w}}_{n-1}\Vert^2
\end{align}

Next, we bound $\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B$. By utilizing  (\ref{AC_12c}) again, we have
\begin{align}\label{AC_27}
    &\mathds{E}\Vert\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B\Vert^2 \notag\\
    &=\mathds{E}\Vert\mathcal{V}_R^{\sf T}\boldsymbol{s}_n^B - \mathcal{V}_R^{\sf T}(\mathbbm{1}\pi^{\sf T}\otimes I_M)\boldsymbol{s}_n^B\Vert^2 \notag\\
    &\le   \Vert\mathcal{V}_R^{\sf T}\Vert^2\left(\sum_k\sum_\ell\pi_\ell\mathds{E}\left\Vert\boldsymbol{s}_{k,n}^B(\boldsymbol{w}_{k,n-1}) - \boldsymbol{s}_{\ell,n}^B(\boldsymbol{w}_{\ell,n-1})\right\Vert^2\right)\notag\\
    &\overset{(a)}{\le}  (36KL^2+36L^2)\Vert\mathcal{V}_R^{\sf T}\Vert^2\Vert\mathcal{V}_L\Vert^2 \mathds{E}\Vert\check{\boldsymbol{w}}_{n-1}\Vert^2  + \Vert\mathcal{V}_R^{\sf T}\Vert^2a^2K
\end{align}
where $(a)$ follows from (\ref{AC_26}) {and the following inequality in which the constant $a$ is defined}:
\begin{align}\label{AC_28}
    &\mathds{E}\Vert \boldsymbol{s}_{k,n}^B(\boldsymbol{w}_{k,n-1}) - \boldsymbol{s}_{\ell,n}^B(\boldsymbol{w}_{\ell,n-1})\Vert^2 \notag\\
    &= \mathds{E}\bigg\Vert \frac{1}{B}\sum\limits_b \nabla_wQ_k(\boldsymbol{w}_{k,n -1}; \boldsymbol{x}_{k,n}^b +\boldsymbol{\delta}_{k,n}^{b,\star},\boldsymbol{y}_{k,n}^b)  \notag\\
    &\quad-  \frac{1}{B}\sum\limits_b \nabla_w Q_\ell(\boldsymbol{w}_{\ell, n-1}; \boldsymbol{x}_{\ell,n}^b+\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{\ell,n}^b) \notag\\
    & \quad- \nabla_w J_k(\boldsymbol{w}_{k,n-1})  + \nabla_w J_\ell(\boldsymbol{w}_{\ell,n-1})\bigg\Vert^2\notag\\
    &\overset{(a)}{\le} \frac{2}{B}\sum\limits_b\mathds{E}\Vert\nabla_w  Q_k(\boldsymbol{w}_{k,n-1}; \boldsymbol{x}_{k,n}^b +\boldsymbol{\delta}_{k,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag \\  
    &\quad - \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{\ell,n}^{b} +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{\ell,n}^b)\Vert^2 \notag\\
    &\quad + 2 \mathds{E} \left\Vert\nabla_w J_k(\boldsymbol{w}_{k,n-1}) - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\right\Vert^2 \notag\\
    &{\le}  \frac{2}{B} \mathds{E} \Vert\nabla_w Q_k(\boldsymbol{w}_{k,n-1}; \boldsymbol{x}_{k,n}^b +\boldsymbol{\delta}_{k,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad- \nabla_w Q_k(\boldsymbol{w}_{k,n-1}; \boldsymbol{x}_{k,n}^b +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad + \nabla_w Q_k(\boldsymbol{w}_{k,n-1}; \boldsymbol{x}_{k,n}^b +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad - \nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{k,n}^b + \boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad + \nabla_w Q_k(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{k,n}^b+\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b) 
    \notag\\
    &\quad - \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{k,n}^b +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad + \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{k,n}^b +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad -  \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{\ell,n}^b 
    +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b)\notag\\ 
    &\quad+ \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{\ell,n}^b +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{k,n}^b) \notag\\
    &\quad- \nabla_w Q_\ell(\boldsymbol{w}_{\ell,n-1}; \boldsymbol{x}_{\ell, n}^b +\boldsymbol{\delta}_{\ell,n}^{b,\star},\boldsymbol{y}_{\ell,n}^b)\Vert^2\notag\\
    &\quad + 2\mathds{E}\Vert\nabla_w J_k(\boldsymbol{w}_{k,n-1})  - \nabla_w J_{\ell}(\boldsymbol{w}_{\ell,n-1})\Vert^2\notag\\
    &\overset{(b)}{\le}  10 L^2\mathds{E}\Vert\boldsymbol{\delta}_{k,n}^{b,\star} -\boldsymbol{\delta}_{\ell,n}^{b,\star} \Vert^2 + 10L^2\mathds{E}\Vert\boldsymbol{w}_{k,n-1} -\boldsymbol{w}_{\ell,n-1}  \Vert^2 \notag\\ 
    &\quad+ 10L^2C^2 + 10L^2\mathds{E}\Vert\boldsymbol{x}_k - \boldsymbol{x}_\ell\Vert^2 + 10L^2\mathds{E}\Vert\boldsymbol{y}_k - \boldsymbol{y}_\ell\Vert^2 \notag\\
    &\quad + 8L^2\Vert\boldsymbol{w}_{k,n-1} - \boldsymbol{w}_{\ell,n-1}\Vert^2 + O(\epsilon^2) + 8C^2 \notag\\
    & \le  18L^2\mathds{E}\Vert\boldsymbol{w}_{k,n-1} -\boldsymbol{w}_{\ell,n-1}  \Vert^2 + a^2 
\end{align}
where $(a)$ follows from (\ref{AA_5}), and $(b)$ from  (\ref{affine_l_1}), (\ref{AC_21}), Assumptions \ref{as2} and \ref{as4}, and
\begin{align}
    a^2 \overset{\Delta}{=}&  10L^2(\mathds{E}\Vert\boldsymbol{x}_k - \boldsymbol{x}_\ell\Vert^2 +\mathds{E}\Vert\boldsymbol{y}_k - \boldsymbol{y}_\ell\Vert^2) + C^2(10L^2+8) \notag\\
    & +O(\epsilon^2)
\end{align}

Now substituting (\ref{AC_25}) and (\ref{AC_27}) into (\ref{AC_19}), we get
\begin{align}\label{AC_29}
    &\mathds{E}\Vert\check{\boldsymbol{w}}_{n}\Vert^2 \notag\\
    &\le t\mathds{E}\Vert\boldsymbol{\check{w}}_{n-1}\Vert^2 +  \frac{\mu^2t^2}{1-t}\Vert\mathcal{V}_R^{\sf T}\Vert^2\Vert\mathcal{V}_L\Vert^2(8L^2+8KL^2)\mathds{E}\Vert\check{\boldsymbol{w}}_{n-1}\Vert^2 \notag\\
     &\quad + \mu^2t^2(36KL^2+36L^2)\Vert\mathcal{V}_R^{\sf T}\Vert^2\Vert\mathcal{V}_L\Vert^2 \mathds{E}\Vert\check{\boldsymbol{w}}_{n-1}\Vert^2 \notag\\
    & \quad + \frac{\mu^2t^2}{1-t}\Vert\mathcal{V}_R^{\sf T}\Vert^2Kc^2 + \mu^2t^2\Vert\mathcal{V}_R^{\sf T}\Vert^2 a^2K\notag\\
     &= (t + O(\mu^2))\mathds{E}\Vert\check{\boldsymbol{w}}_{n-1}\Vert^2 + O(\mu^2)
\end{align}
where $t+O(\mu^2) < 1 $ when $\mu$ is sufficiently small. Then, iterating recursion (\ref{AC_29}) gives
\begin{align}
   &\mathds{E}\Vert\check{\boldsymbol{w}}_{n}\Vert^2 \le (t+O(\mu^2))^{n+1}\mathds{E}\Vert\check{\boldsymbol{w}}_{-1}\Vert^2 + O(\mu^2) 
\end{align}
from which after enough iterations $\check{n}$ with
\begin{align}\label{checkn}
    \check{n} \ge O\left(\frac{\log \mu}{\log (t+O(\mu^2))}\right)
\end{align}
we get
\begin{align}\label{AC_30}
       \mathop{\lim \sup}\limits_{n\to\infty} \mathds{E}\left\Vert \check{\boldsymbol{w}}_{n}\right\Vert^2 \le \mu^2\check{c}_{\epsilon}^2 = O(\mu^2)
\end{align}
where 
\begin{align}\label{AC_31}
     \check{c}_{\epsilon} ^2=& \frac{2\Vert\mathcal{V}_R^{\sf T}\Vert^2t^2}{1-t-O(\mu^2)}\left(\frac{Kc^2}{1-t}+a^2K\right)
\end{align}

{Next we examine} the convergence of $\mathds{E}\Vert\bar{\boldsymbol{w}}_{n}\Vert^2$. Squaring and taking expectations of (\ref{AC_14}) conditioned on $\boldsymbol{\mathcal{F}}_{n-1}$,  we have
\begin{align}\label{AC_32}
    & \mathds{E}\left\{\Vert\bar{\boldsymbol{w}}_{n}\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
    &  \overset{(a)}{=}  \left\Vert\bar{\boldsymbol{w}}_{n-1} + \mu\sum_k \pi_k\nabla_w J_k(\boldsymbol{w}_{k,n-1}) \right\Vert^2 \notag\\
    & \quad + \mu^2\mathds{E}\left\{\left\Vert\sum_k \pi_k \boldsymbol{s}_{k,n}^B(\boldsymbol{w}_{k,n-1})\right\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
    &=   \Vert\bar{\boldsymbol{w}}_{n-1}\Vert^2 + 2\mu\sum_k\pi_k\nabla_{w^{\sf T}} J_k(\boldsymbol{w}_{k,n-1})\bar{\boldsymbol{w}}_{n-1} \notag\\
    & \quad+ \mu^2 \left\Vert \sum_k \pi_k \nabla_{w} J_k(\boldsymbol{w}_{k,n-1})\right\Vert^2 \notag\\
    & \quad+ \mu^2\mathds{E}\left\{\left\Vert\sum_k \pi_k \boldsymbol{s}_{k,n}^B(\boldsymbol{w}_{k,n-1})\right\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}
\end{align}
where $(a)$ follows from (\ref{s0_convex}). 
We explore the right-hand terms of (\ref{AC_32}) one by one.  First, for the term associated with the gradient noise, we have
\begin{align}\label{AC_33}
    & \mathds{E}\left\{\left\Vert\sum_k \pi_k \boldsymbol{s}_{k,n}^B(\boldsymbol{w}_{k,n-1})\right\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
    &\overset{(a)}{\le}  \sum_k\pi_k \mathds{E}\left\{\Vert\boldsymbol{s}_{k,n}^B(\boldsymbol{w}_{k,n-1})\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
    &\overset{(b)}{\le}  \beta_{\max}^2 \sum_k\pi_k\Vert\widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2 + \sigma^2
\end{align}
where $(a)$ follows from Jensen's inequality, $(b)$ follows from (\ref{s2_convex}), and
\begin{align}\label{AC_34}
    \beta_{\max}^2 = \max\limits_{k}\{\beta_{k,\epsilon}^2\}, \quad \sigma^2 = \sum_k \pi_k\sigma_{k}^2
\end{align}

Second, {since} $w^\star$ is the global minimizer, we have
\begin{align}\label{AC_35}
    \sum_k \pi_k \nabla_{w} J_k(w^\star)= 0
\end{align}
However, it is not guaranteed that $\nabla_{w} J_k(w^\star) = 0$ {since} the local minimizers $w_k^\star$ and the global {global minimizer} $w^\star$ may be different. {Thus,} we have
\begin{align}\label{AC_36}
    & \left\Vert \sum_k \pi_k \nabla_{w} J_k(\boldsymbol{w}_{k,n-1})\right\Vert^2  \notag\\
    &= \left\Vert\sum_k \pi_k \nabla_{w} J_k(\boldsymbol{w}_{k,n-1}) - \sum_k \pi_k \nabla_{w} J_k(w^\star) \right\Vert^2\notag\\
    &\overset{(a)}{\le} \sum_k\pi_k\Vert\nabla_{w} J_k(\boldsymbol{w}_{k,n-1}) -  \nabla_{w} J_k(w^\star) \Vert^2\notag\\
    &\overset{(b)}{\le}2L^2\sum_k\pi_k\Vert\widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2 + O(\epsilon^2)
\end{align}
where $(a)$ follows from Jensen's inequality, and $(b)$ follows from (\ref{affine_l_1}).

Next, taking expectations of the inner product term, and substituting (\ref{AC_35}) into it, we have
\begin{align}\label{AC_37}
    & \mathds{E}\left\{\sum_k\pi_k\nabla_{w^{\sf T}} J_k(\boldsymbol{w}_{k,n-1})\bar{\boldsymbol{w}}_{n-1}\right\} \notag\\
    &{=} \sum_k\pi_k\mathds{E}\left(\nabla_{w} J_k(\boldsymbol{w}_{k,n-1}) - \nabla_{w} J_k(\boldsymbol{w}^\star)\right)^{\sf T}(\bar{\boldsymbol{w}}_{n-1} \notag\\
    & \quad- \widetilde{\boldsymbol{w}}_{k,n-1} + \widetilde{\boldsymbol{w}}_{k,n-1})
\end{align}
where {since} $J_k(\boldsymbol{w})$ is $\nu$-strongly convex, we get
\begin{align}\label{AC_38}
    \mathds{E}\left(\nabla_{w} J_k(\boldsymbol{w}_{k,n-1}) - \nabla_{w} J_k(\boldsymbol{w}^\star)\right)^{\sf T}\widetilde{\boldsymbol{w}}_{k,n-1} \le -\nu\mathds{E}\Vert\widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2
\end{align}
and
\begin{align}\label{AC_39}
    & \sum_k \pi_k\mathds{E}\left(\nabla_{w} J_k(\boldsymbol{w}_{k,n-1}) - \nabla_{w} J_k(\boldsymbol{w}^\star)\right)^{\sf T}(\bar{\boldsymbol{w}}_{n-1} 
     - \widetilde{\boldsymbol{w}}_{k,n-1}) \notag\\
    & \overset{(a)}{\le} \sum_k\pi_k\mathds{E}\Vert\nabla_{w} J_k(\boldsymbol{w}_{k,n-1}) - \nabla_{w} J_k(\boldsymbol{w}^\star)\Vert\Vert\bar{\boldsymbol{w}}_{n-1} 
     - \widetilde{\boldsymbol{w}}_{k,n-1}\Vert\notag\\
     &\overset{(b)}{\le} \sum_k\pi_k \sqrt{\mathds{E}\Vert\nabla_{w} J_k(\boldsymbol{w}_{k,n-1}) - \nabla_{w} J_k(\boldsymbol{w}^\star)\Vert^2}\notag\\
     &\quad\times\sqrt{\mathds{E}\Vert\bar{\boldsymbol{w}}_{n-1}
     - \widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2}\notag\\
     &\overset{(c)}{\le}\sum_k\pi_k h\mu\sqrt{2L^2\mathds{E}\Vert\widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2+O(\epsilon^2)}\notag\\
     &\overset{(d)}{\le}  h \mu L^2 \sum_k\pi_k\mathds{E}\Vert\widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2 +  h\mu\left(O(\epsilon^2)+\frac{1}{2}\right)
\end{align}
where $(a)$ and $(b)$ follow from the Cauchy-Schwarz inequality, $(c)$ follows from (\ref{affine_l_1}), (\ref{AC_26}) and (\ref{AC_30}), and $h \overset{\Delta}{=} \Vert \mathcal{V}_{L} \Vert \check{c}_{\epsilon}$. That is, from (\ref{AC_26}) and (\ref{AC_30}), we know after enough iterations:
\begin{align}
    & \sqrt{\mathds{E}\Vert\bar{\boldsymbol{w}}_{n-1}
     - \widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2} \le  \sqrt{\mathds{E}\Vert {\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1} -{\bar{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n-1} \Vert^2}  \notag\\
     &\le  \sqrt{\mathds{E}\Vert \mathcal{V}_{L}\check{\boldsymbol{w}}_{n-1}\Vert^2} \le \Vert \mathcal{V}_{L} \Vert \mu\check{c}_{\epsilon} = h\mu
\end{align}
{With regard to $(d)$, we use} the following inequality:
\begin{align}\label{AC_40}
    \sqrt{x} \le \frac{1}{2}(x+1)
\end{align}
Combining (\ref{AC_38}) and (\ref{AC_39}), we have
\begin{align}\label{AC_41}
    &\mathds{E}\left\{\sum_k\pi_k\nabla_{w^{\sf T}} J_k(\boldsymbol{w}_{k,n-1})\bar{\boldsymbol{w}}_{n-1}\right\} \notag\\
    &\le - (\nu - h \mu L^2) \sum_k\pi_k \mathds{E}\Vert \widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2  + h\mu\left(O(\epsilon^2)+\frac{1}{2}\right)
\end{align}
where $\nu - h \mu L^2 > 0 $ when $\mu$ is sufficiently small. Now substituting (\ref{AC_33}), (\ref{AC_36}) and (\ref{AC_41}) into (\ref{AC_32}), and taking expectations on both sides of it gives
\begin{align}\label{AC_42}
   & \mathds{E}\Vert\bar{\boldsymbol{w}}_{n}\Vert^2 \notag\\
   & \le  \mathds{E}\Vert\bar{\boldsymbol{w}}_{n-1}\Vert^2+  2h\mu^2\left(O(\epsilon^2)+\frac{1}{2}\right) + O(\mu^2 \epsilon^2) +\mu^2 \sigma^2 -
   \notag\\
   & \quad (2\mu \nu - 2 \mu^2h  L^2- 2\mu^2 L^2 - \mu^2\beta_{\max}^2 )\sum_k\pi_k\mathds{E}\Vert\widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2 \notag\\
   & \overset{(a)}{\le}   \mathds{E}\Vert\bar{\boldsymbol{w}}_{n-1}\Vert^2+  2h\mu^2\left(O(\epsilon^2)+\frac{1}{2}\right) + O(\mu^2\epsilon^2)+\mu^2 \sigma^2 -
   \notag\\
   & \quad (2\mu \nu - 2 \mu^2h  L^2- 2\mu^2 L^2 - \mu^2\beta_{\max}^2 )\mathds{E}\Vert\sum_k\pi_k \widetilde{\boldsymbol{w}}_{k,n-1}\Vert^2 \notag\\
   & =  \mathds{E}\Vert\bar{\boldsymbol{w}}_{n-1}\Vert^2+  2h\mu^2\left(O(\epsilon^2)+\frac{1}{2}\right) + O(\mu^2\epsilon^2)+\mu^2 \sigma^2 -
   \notag\\
   & \quad (2\mu \nu - 2 \mu^2 h  L^2- 2\mu^2 L^2- \mu^2\beta_{\max}^2 )\mathds{E}\Vert \bar{\boldsymbol{w}}_{n-1}\Vert^2 \notag\\
   &=  \lambda \mathds{E}\Vert \bar{\boldsymbol{w}}_{n-1}\Vert^2 + \mu^2\bar{c}_{\epsilon}^2
\end{align}
where $(a)$ follows from Jensen's inequality, and 
\begin{align}
    0< \lambda = 1 - 2\mu \nu + 2 \mu^2 h  L^2+ 2\mu^2 L^2 + \mu^2\beta_{\max}^2<1
\end{align}
when $\mu$ is sufficiently small. Also,
\begin{align}
    \bar{c}_{\epsilon}^2 = 2h\left(O(\epsilon^2)+\frac{1}{2}\right) +O(\epsilon^2) +  \sigma^2
\end{align}
Iterating (\ref{AC_42}), and after enough iterations $\bar{n}$ with 
\begin{align}\label{barn}
    \bar{n} \ge  O\left(\frac{\log \mu}{\log (1 - O(\mu))}\right)
\end{align}
we have
\begin{align}\label{AC_45}
       \mathop{\lim \sup}\limits_{n\to\infty} \mathds{E}\left\Vert \bar{\boldsymbol{w}}_{n}\right\Vert^2 \le 2\mu \frac{\bar{c}_{\epsilon}^2}{1-\lambda} = O(\mu)
\end{align}

Combining (\ref{checkn}), (\ref{AC_31}), (\ref{barn}), and (\ref{AC_45}), we conclude that after enough iterations $n$ with 
\begin{align}\label{step_n}
    n =& \check{n} + \bar{n} \ge O\left(\frac{\log \mu}{\log (t+O(\mu^2))}\right)+O\left(\frac{\log \mu}{\log (1 - O(\mu))}\right)\notag\\
    \overset{(a)}{=}& O\left(\frac{\log \mu}{\log (1 - O(\mu))}\right)
\end{align}
where $(a)$ follows from the following equality:
\begin{align}
    \lim\limits_{\mu \to 0}\frac{\frac{\log \mu}{\log (t+O(\mu^2))}}{\frac{\log \mu}{\log (1 - O(\mu))}} = 0
\end{align}
we have
\begin{align}
    \mathop{\lim \sup}\limits_{n\to\infty} \mathds{E}\left\Vert {\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n}\right\Vert^2  = & \mathop{\lim \sup}\limits_{n\to\infty} \mathds{E}\Vert\mathcal{V}_\alpha^{\sf -T}\mathcal{V}_\alpha^{\sf T}{\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n}\Vert^2\notag\\
    \overset{(a)}{\le} & \mathop{\lim \sup}\limits_{n\to\infty}\Vert\mathcal{V}_\alpha^{\sf -T}\Vert^2 \left\{\mathds{E}\left\Vert \bar{\boldsymbol{w}}_{n}\right\Vert^2+ \mathds{E}\left\Vert \check{\boldsymbol{w}}_{n}\right\Vert^2 \right\}\notag\\
    \le & c_\epsilon^2\mu = O(\mu)
\end{align}
where $(a)$ follows from (\ref{AC_13}), and
\begin{align}
   c_\epsilon^2 = \Vert\mathcal{V}_\alpha^{\sf -T}\Vert^2\left(\frac{2\bar{c}_{\epsilon}^2}{1-\lambda} + \mu\check{c}_{\epsilon}^2\right) 
\end{align}
{Note from (\ref{step_n}) that} the convergence speed of $\mathds{E}\Vert\boldsymbol{\bar{w}}\Vert^2$ dominates the convergence of $\mathds{E}\Vert {\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n}\Vert^2$, thus $\mathds{E}\Vert {\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}}_{n}\Vert^2$ converges linearly with rate $\lambda$.
\end{proof}



\section{Proof of Lemma \ref{lemma3}}\label{ap4}
\begin{proof}
We first {list two} properties for the gradient noise over a batch of data for later use.  
%\begin{align}
    %\label{sb0} %\mathds{E}\left\{\boldsymbol{s}^{B}_{k,n}\Big\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}&=0\\
    %\label{sb2} 
    %\mathds{E}\left\{ \left\Vert\boldsymbol{s}^{B}_{k,n}\right\Vert^2 \Big\vert\boldsymbol{\mathcal{F}}_{n-1}\right\} &\le \frac{\beta^2G^2+\sigma^2}{|B|}
%\end{align}
By using the same method {used to prove} (\ref{AB_2}) and (\ref{sb2}), we can {verify that the stochastic gradient is unbiased with bounded second-order moment, i.e., it holds that}:
\begin{align}
\label{sb0_n}
    &\mathds{E}\left\{\bar{\boldsymbol{s}}^{B}_{k,n}\Big\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}=0\\
    \label{sb2_n}
        &\mathds{E}\left\{\left\Vert\bar{\boldsymbol{s}}^{B}_{k,n}(\boldsymbol{w}_{k,n-1})\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\}
    {\le} \frac{1}{B}(\beta^2G^2+\sigma^2)
\end{align}

{Next, we} analyze the difference between the {iterates at the agents and the centroid. To begin with, the block centroid $\boldsymbol{\scriptstyle\mathcal{W}}_{c,n}$ satisfies the following relation}:
\begin{align}
        \boldsymbol{\scriptstyle\mathcal{W}}_{c,n} &= \mathbbm{1}\otimes \boldsymbol{w}_{c,n} =  (\mathbbm{1}\pi^{\sf T}\otimes I)\boldsymbol{\scriptstyle\mathcal{W}}_{n}\notag\\
        &\overset{(a)}{=} (\mathbbm{1}\pi^{\sf T}\otimes I)(\boldsymbol{\scriptstyle\mathcal{W}}_{n-1} - \mu\mathcal{G}_{n-1} - \mu\bar{\boldsymbol{s}}_n^B)
\end{align}
where $(a)$ follows from (\ref{r_non_c}) and (\ref{comb_matrix}). We thus obtain the following  recursion which relates for the network disagreement:
\begin{align}\label{rewc}
    &  \boldsymbol{\scriptstyle\mathcal{W}}_{n} - \boldsymbol{\scriptstyle\mathcal{W}}_{c,n}\notag\\
    & = (\mathcal{A}^{\sf T} - \mathbbm{1}\pi^{\sf T}\otimes I )(\boldsymbol{\scriptstyle\mathcal{W}}_{n-1} - \mu\mathcal{G}_{n-1}- \mu\bar{\boldsymbol{s}}_n^B)\notag\\
    &\overset{(a)}{=} (\mathcal{A}^{\sf T} - \mathbbm{1}\pi^{\sf T}\otimes I )(I -\mathbbm{1}\pi^{\sf T}\otimes I  )(\boldsymbol{\scriptstyle\mathcal{W}}_{n-1} - \mu\mathcal{G}_{n-1}  - \mu\bar{\boldsymbol{s}}_n^B)\notag\\
    &= (\mathcal{A}^{\sf T} - \mathbbm{1}\pi^{\sf T}\otimes I )(I -\mathbbm{1}\pi^{\sf T}\otimes I  )\boldsymbol{\scriptstyle\mathcal{W}}_{n-1}-\notag\\
    &\quad\;(\mathcal{A}^{\sf T} - \mathbbm{1}\pi^{\sf T}\otimes I )(I -\mathbbm{1}\pi^{\sf T}\otimes I)\mu (\mathcal{G}_{n-1} + \bar{\boldsymbol{s}}_n^B)\notag\\
    &= (\mathcal{A}^{\sf T} - \mathbbm{1}\pi^{\sf T}\otimes I )(\boldsymbol{\scriptstyle\mathcal{W}}_{n-1}-\boldsymbol{\scriptstyle\mathcal{W}}_{c,n-1}-\mu\mathcal{G}_{n-1} - \mu\bar{\boldsymbol{s}}_n^B)
\end{align}
where $(a)$ follows from
\begin{equation}
    \mathcal{A}^{\sf T} - \mathbbm{1}\pi^{\sf T}\otimes I = (\mathcal{A}^{\sf T} - \mathbbm{1}\pi^{\sf T}\otimes I )(I -\mathbbm{1}\pi^{\sf T}\otimes I  )
\end{equation}
{Using} eigen-structure of the combination matrix $A$ from (\ref{decompose})--(\ref{AC_12c}), we have
\begin{equation}
    \mathcal{A}^{\sf T} -  \mathbbm{1}\pi^{\sf T}\otimes I_M =   (\mathcal{V}_{\alpha}^{\sf T})^{-1}\widetilde{\mathcal{J}}^{\sf T}\mathcal{V}_{\alpha}^{\sf T}
\end{equation}
where
\begin{equation}
    \widetilde{\mathcal{J}}= \widetilde{J}\otimes I_M = \left[
    \begin{array}{cc}
         0&0  \\
         0& J_{\alpha}
    \end{array}\right]\otimes I_M 
\end{equation}
Now consider
\begin{equation}
    \overline{\boldsymbol{\scriptstyle\mathcal{W}}}_n \overset{\Delta}{=} \mathcal{V}_{\alpha}^{\sf T}(\boldsymbol{\scriptstyle\mathcal{W}_{n}} - \boldsymbol{\scriptstyle\mathcal{W}_{c,n}})
\end{equation}
and substituting (\ref{rewc}) into it, we get
\begin{align}\label{12}
&\left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_n\right\Vert^2= \left\Vert\mathcal{V}_{\alpha}^{\sf T}(\boldsymbol{\scriptstyle\mathcal{W}_{n}} - \boldsymbol{\scriptstyle\mathcal{W}_{c,n}})\right\Vert^2\notag\\
        &=\left\Vert \mathcal{V}_{\alpha}^{\sf T}( (\mathcal{V}_{\alpha}^{\sf T})^{-1}\widetilde{\mathcal{J}}^{\sf T}\mathcal{V}_{\alpha}^{\sf T})(\boldsymbol{\scriptstyle\mathcal{W}}_{n-1} - \boldsymbol{\scriptstyle\mathcal{W}}_{c,n-1} - \mu\mathcal{G}_{n-1} - \mu\bar{\boldsymbol{s}}_n^B))\right\Vert\notag\\
        &=\left\Vert\widetilde{\mathcal{J}}^{\sf T}\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}-\mu\widetilde{\mathcal{J}}^{\sf T}\mathcal{V}_\alpha^{\sf T}(\mathcal{G}_{n-1}+ \bar{\boldsymbol{s}}_n^B)\right\Vert^2\notag\\
        &=\left\Vert t\frac{1}{t}\widetilde{\mathcal{J}}^{\sf T}\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}-(1-t)\frac{\mu}{1-t}\widetilde{\mathcal{J}}^{\sf T}\mathcal{V}_\alpha^{\sf T}(\mathcal{G}_{n-1}+\bar{\boldsymbol{s}}_n^B)\right\Vert^2\notag\\
        &\overset{(a)}{\le}\frac{1}{t}\left\Vert\widetilde{\mathcal{J}}^{\sf T}\right\Vert^2\left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}\right\Vert^2 + \frac{\mu^2\left\Vert\widetilde{\mathcal{J}}^{\sf T}\right\Vert^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2}{1-t}\left\Vert\mathcal{G}_{n-1}+\bar{\boldsymbol{s}}_n^B)\right\Vert^2
\end{align}
where $(a)$ follows from Jensen's inequality for any $0<t<1$. Similar to (\ref{AC_18}), {we select}
\begin{align}\label{vt}
t = \left\Vert\widetilde{\mathcal{J}}^{\sf T}\right\Vert = \left\Vert J_\alpha^{\sf T} \right\Vert = \sqrt{\tau(J_\alpha J_\alpha^{\sf T})} = \lambda_2+\alpha <1
\end{align}
{where $\alpha$ is arbitrarily small to guarantee the rightmost inequality}. Then, we have
\begin{align}\label{13}
& \mathds{E}\left\{ \left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_n\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
&\overset{(a)}{\le}t\left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}\right\Vert^2+ \frac{\mu^2 t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2}{1-t}\mathds{E}\left\{\sum\limits_{k=1}^K\left\Vert\overline{\nabla_w J_k}(\boldsymbol{w}_{k,n-1})\right.\right.\notag\\
&\left.\left.\quad\ + \bar{\boldsymbol{s}}^{B}_{k,n}\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}
        \right\}\notag\\
&\overset{(b)}{\le} t\left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}\right\Vert^2+\frac{\mu^2 t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2}{1-t}\mathds{E}\left\{\sum\limits_{k=1}^K\left\Vert\overline{\nabla_w J_k}(\boldsymbol{w}_{k,n-1})\right\Vert^2\right.\notag\\
&\left.\quad\ + \left\Vert\bar{\boldsymbol{s}}^{B}_{k,n}\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
&\overset{(c)}{\le}  t\left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}
\right\Vert^2 + 
\frac{\mu^2 t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2}{1-t}K\left(G^2+\frac{\beta^2G^2+\sigma^2}{B}\right)
\end{align}
where $(a)$ follows from (\ref{bbs}) and (\ref{dq}), $(b)$ follows from (\ref{sb0_n}), and  $(c)$ follows from (\ref{sb2_n}) and (\ref{b1}). Taking expectations of both sides, we arrive at
\begin{align}
   & {\mathds{E}}\left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_n\right\Vert^2  \notag\\
   &\le t\mathds{E}\left\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{n-1}
\right\Vert^2 + 
\frac{\mu^2 t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2}{1-t}K\left(G^2+\frac{\beta^2G^2+\sigma^2}{B}\right)
%\le & t^{n+1}\mathds{E}\Vert\widetilde{\boldsymbol{\scriptstyle\mathcal{W}}}_{-1}\Vert^2 + \frac{\mu^2 t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2}{(1-t)^2}K\left(G^2+\frac{\beta^2G^2+\sigma^2}{|B|}\right)
\end{align}
Then after enough iterations $n_0$ such that
\begin{align}
   t^{n_0 + 1}\mathds{E}\Vert\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_{-1}\Vert^2 \le& \frac{\mu^2 t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2\left\Vert(\mathcal{V}_\alpha^{\sf T})^{-1}\right\Vert^2}{(1-t)^2}K\notag\\
   & \times\left(G^2+\frac{\beta^2G^2+\sigma^2}{B}\right)\notag  
\end{align}
which is equivalent to
\begin{align}
    n_0 \ge O \left(\frac{\rm{log} \mu}{\rm{log} \textit{t}}\right) 
\end{align}
we get
\begin{align}\label{nd_a}
    &{\mathds{E}}\left\Vert\boldsymbol{\scriptstyle\mathcal{W}}_{n} - \boldsymbol{\scriptstyle\mathcal{W}}_{c,n}\right\Vert^2 ={\mathds{E}}\left\Vert(\mathcal{V}_\alpha^{\sf T})^{-1}\overline{\boldsymbol{\scriptstyle\mathcal{W}}}_n\right\Vert^2\\ 
    &\le\frac{2\mu^2 t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2\left\Vert(\mathcal{V}_\alpha^{\sf T})^{-1}\right\Vert^2}{(1-t)^2}K\left(G^2+\frac{\beta^2G^2+\sigma^2}{B}\right)\notag\\
\end{align}
Furthermore,
\begin{align}\label{disa_w1}
    &\mathds{E}\Vert\boldsymbol{w}_{k,n}-\boldsymbol{w}_{c,n}\Vert\notag\\
    &\le(\mathds{E}\Vert\boldsymbol{w}_{k,n}-\boldsymbol{w}_{c,n}\Vert^2)^{\frac{1}{2}}\notag\\
    &\le({\mathds{E}}\left\Vert\boldsymbol{\scriptstyle\mathcal{W}}_{n} - \boldsymbol{\scriptstyle\mathcal{W}}_{c,n}\right\Vert^2)^{\frac{1}{2}}\notag\\
    &\le \frac{\mu t\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert\left\Vert(\mathcal{V}_\alpha^{\sf T})^{-1}\right\Vert}{(1-t)}\sqrt{K\left(G^2+\frac{\beta^2G^2+\sigma^2}{|B|}\right)}
\end{align}
\end{proof}

\section{Proof of Lemma \ref{lemma4}}\label{ap5}
\begin{proof}
From (\ref{sb0}) and (\ref{sn}), we know that
\begin{align}\label{psz}
\mathds{E}\left\{\widehat{\boldsymbol{s}}_n^B|\boldsymbol{\mathcal{F}}_{n-1}
\right\} &= \mathds{E}\left\{\sum\limits_k \pi_k\bar{\boldsymbol{s}}^B_{k,n}|\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
&= \sum_k \pi_k \mathds{E}\left\{\bar{\boldsymbol{s}}^B_{k,n}|\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
&= 0 
\end{align}
Moreover, expression (\ref{sb2_n}) gives
\begin{align}\label{pbs}
      {\mathds{ E}}\left\{\left\Vert\widehat{\boldsymbol{s}}_n^B\right\Vert^2\right\} &= {\mathds{ E}}\left\{{\mathds{ E}}\left\{\left\Vert\widehat{\boldsymbol{s}}_n^B\right\Vert^2\vert\boldsymbol{\mathcal{F}}_{n-1}\right\}\right\}\notag\\
     &= {\mathds{ E}}\left\{{\mathds{ E}}\left\{\left\Vert\sum_k \pi_k \bar{\boldsymbol{s}}^B_{k,n}\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\}\right\}\notag\\
     &\overset{(a)}{\le} {\mathds{ E}}\left\{{\mathds{ E}}\left\{\sum_k \pi_k \left\Vert\bar{\boldsymbol{ s}}^B_{k,n}\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\}\right\} \notag\\
     &\le \frac{1}{B}(\beta^2 G^2+ \sigma^2)
\end{align}
where $(a)$ follows from Jensen's inequality.

As for $\boldsymbol{d}_{n-1}$, after enough iterations $n_0$, we get
\begin{align}\label{51}
        &  {\mathds{ E}}\left\Vert\boldsymbol{d}_{n-1}\right\Vert^2 = {\mathds{ E}}\left\Vert\sum\limits_{k=1}^{K} \pi_k \boldsymbol{d}_{k,n-1}\right\Vert^2\notag\\
        &\overset{(a)}{\le}\sum\limits_k \pi_k \mathds{E}\left\Vert\overline{\nabla_w J_k}(\boldsymbol{w}_{k,n-1})-\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})\right\Vert^2\notag\\
        &\overset{(b)}{\le} \sum\limits_k \pi_k L^2 \mathds{E}\left\Vert\boldsymbol{w}_{k,n-1}-\boldsymbol{w}_{c,n-1}\right\Vert^2\notag\\
        &\le \pi_{\max}L^2\mathds{E}\left\Vert\boldsymbol{\scriptstyle\mathcal{W}}_{n-1} - \boldsymbol{\scriptstyle\mathcal{W}}_{c,n-1}\right\Vert^2\notag\\
        &\overset{(c)}{\le}\pi_{\max}L^2\frac{2\mu^2t^2\left\Vert\mathcal{V}_\alpha^{\sf T}\right\Vert^2\left\Vert(\mathcal{V}_\alpha^{\sf T})^{-1}\right\Vert^2}{(1-t)^2}K(G^2+\frac{\beta^2G^2+\sigma^2}{B})
\end{align}
where $(a)$ follows from Jensen's inequality,  $(b)$ follows from (\ref{smooth_q}), $(c)$ follows from (\ref{nd_a}), and $\pi_{\max}$ is the largest entry of $\pi$.
\end{proof}
\section{Proof of Theorem \ref{th2}}\label{ap6}
\begin{proof}
{One difficulty in the analysis of the convergence of the adversarial diffusion strategy (\ref{newx_e_n})--(\ref{a3_e_n}) is due to the non-smoothness of $J(w)$.} Thus, we resort to the  Moreau envelope method and $L$-weak convexity \cite{jin2020local,rockafellar2015convex,DavisD19,lin2020gradient,Thekumparampil019} to study near-stationarity for non-smooth loss functions. Consider function $f(z;w) $ defined by
\begin{align}
    f(z; w) \overset{\Delta}{ = }J(z)+\frac{1}{2\gamma}\left\Vert w-z\right\Vert^2  
\end{align}
{We already know from \cite{Thekumparampil019} that} the maximum function $g(z)$ in (\ref{f_g}) is $L$-weakly convex if (\ref{smooth_q}) is satisfied. Therefore, its expectation $J(z)$ is also $L$-weakly convex so that $f(z; w)$ is strongly convex over $z$ if $\gamma < \frac{1}{L}$. Then, the Moreau envelope of $J(w)$ is defined by \cite{sayed_2023}:
\begin{equation}
    J_{\gamma}(w)\overset{\Delta}{=} \min\limits_{z}  f(z;w)
\end{equation}
Let
\begin{equation}
     \widehat{z} = \mathop{\text{argmin}}\limits_{z} f(z;w)%\left\{J(z)+ \frac{1}{2\gamma}\left\Vert w-z\right\Vert^2\right\}
\end{equation}
then $\widehat{z}$ is uniquely determined by $w$ since $f(z;w)$ is strongly convex. Even when $J(z)$ is non-differentiable, the Moreau envelope will be differentiable relative to $w$. In particular, it holds that \cite{sayed_2023, lin2020gradient}:
\begin{subequations}
\begin{align}\label{65}
   \frac{1}{\gamma}(w - \widehat{z}) = \nabla_w J_{\gamma}(w) \\
   \nabla_w J_{\gamma}(w) \in \partial_w J(\widehat{z})
\end{align}
\end{subequations}
It follows that if $ J_{\gamma}(w)$ arrives at an approximate stationary point $w^o$, i.e., 
 if $\left\Vert\nabla_w J_{\gamma}(w^o)\right\Vert^2$ is bounded by a small value $\kappa^2$, then
 \begin{equation}\label{e_154}
     \min_{\zeta \in \partial_w J(\widehat{z}^o)}\Vert\zeta \Vert^2 \le \Vert w^o - \widehat{z}^o\Vert^2 = \gamma^2\Vert \nabla_w J_{\gamma}(w^o) \Vert^2 \le \gamma^2\kappa^2
 \end{equation} 
 where
 \begin{align}
     \widehat{z}^o =  \mathop{\text{argmin}}\limits_{z} f(z;w^o)
 \end{align}
 which implies that $w^o$ is $O(\kappa^2)$ near a $\kappa^2$-approximate stationary point of $J(w)$. Thus, it is reasonable to treat $\nabla_w J_\gamma(w)$ as a surrogate for $\partial_w J(w)$. 
{Consider the proximal operator associated with the original risk from (\ref{jk}) at location $\boldsymbol{w}_{c,n-1}$:
\begin{align}\label{zn}
    \boldsymbol{\widehat{w}}_{n-1} = \mathop{\mathrm{argmin}}\limits_{\boldsymbol{z}} \left\{J(\boldsymbol{z}) + \left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{z} - \boldsymbol{w}_{c,n-1}\right\Vert^2\right\}
\end{align}
The resulting Moreau envelope is given by:
\begin{align}
    J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1}) =  \min\limits_{\boldsymbol{z}} J(\boldsymbol{z}) + \left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{z} - \boldsymbol{w}_{c,n-1} \right\Vert^2
\end{align}
We know from expression (\ref{pr1}) in Appendix \ref{ap7} that} 
\begin{align}\label{pr2}
    &\overline{\nabla_{w^{\sf T}} J_k}(\boldsymbol{w}_{c,n-1})(\boldsymbol{\widehat{w}}_{n-1}-\boldsymbol{w}_{c,n-1}) \notag\\
    &\le J_k(\boldsymbol{\widehat{w}}_{n-1}) - J_k(\boldsymbol{w}_{c,n-1}) + \boldsymbol{\eta}_k  +\frac{L}{2}\left\Vert\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}\right\Vert^2
\end{align}
where
\begin{align}
    \boldsymbol{\eta}_k \le O(\epsilon_k)\Vert \boldsymbol{w} - \boldsymbol{w}_{c, n-1}\Vert+O(\epsilon_k^2)
\end{align}
{It is argued in \cite{Thekumparampil019} that the function $g(\boldsymbol{w})$ defined by (\ref{f_g}) can be $L$-weakly convex if (\ref{smooth_q}) is satisfied. It follows that the the function $g(\boldsymbol{w}) + \left(L+\frac{1}{2}\right)||\boldsymbol{w}||^2$ is $(L+1)$-strongly-convex over $\boldsymbol{w}$ and its expectation $J(\boldsymbol{w}) + \left(L+\frac{1}{2}\right)||\boldsymbol{w}||^2$ is also $(L+1)$-strongly-convex. This implies that}
\begin{align}\label{pr3}
    & J(\boldsymbol{w}_{c,n-1}) - J(\boldsymbol{\widehat{w}}_{n-1})- \frac{1}{2}(L+1)\left\Vert\boldsymbol{w}_{c,n-1}-\boldsymbol{ \widehat{z}}_{n-1}\right\Vert^2\notag\\
    &= J(\boldsymbol{w}_{c,n-1}) + \left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{w}_{c,n-1}-\boldsymbol{w}_{c,n-1}\right\Vert^2 - J(\boldsymbol{\widehat{w}}_{n-1})  \notag\\
    &\quad\ -\left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{w}_{c,n-1} - \boldsymbol{\widehat{w}}_{n-1}\right\Vert^2 + \frac{L}{2} \left\Vert\boldsymbol{w}_{c,n-1} - \boldsymbol{\widehat{w}}_{n-1}\right\Vert^2\notag\\
    &= J(\boldsymbol{w}_{c,n-1}) + \left(L + \frac{1}{2}\right)\left\Vert\boldsymbol{w}_{c,n-1}-\boldsymbol{w}_{c,n-1}\right\Vert^2 -\notag\\
    &\quad \ \min\limits_{\boldsymbol{w}}\left\{J(\boldsymbol{w}) + \left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{w} - \boldsymbol{w}_{c,n-1}\right\Vert^2\right\}\notag\\
    &\quad\ + \frac{L}{2}\left\Vert\boldsymbol{w}_{c,n-1} -\boldsymbol{ \widehat{z}}_{n-1}\right\Vert^2\notag\\
    &\overset{(a)}{\ge} (L+ \frac{1}{2})\left\Vert\boldsymbol{w}_{c,n-1} - \boldsymbol{\widehat{w}}_{n-1}\right\Vert^2 \notag\\
    &\overset{(b)}{=} \frac{1}{4L+2}\left\Vert\nabla_w J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1})\right\Vert^2
\end{align}
where $(a)$ follows from the strong convexity of the function $J(\boldsymbol{w}) + \left(L+\frac{1}{2}\right)||\boldsymbol{w}||^2$, and $(b)$ follows from (\ref{65}) with $\gamma = \frac{1}{2L+1}$. Then we get
\begin{align}\label{pr4}
& J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n}) =  \min\limits_{\boldsymbol{z}} J(\boldsymbol{z}) + \left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{w}_{c,n} - \boldsymbol{z}\right\Vert^2\notag\\
            & \le  J(\boldsymbol{\widehat{w}}_{n-1}) + \left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{w}_{c,n}-\boldsymbol{\widehat{w}}_{n-1}\right\Vert^2\notag\\
        & \overset{(a)}{=}  J(\boldsymbol{\widehat{w}}_{n-1}) + \left(L+\frac{1}{2}\right)\bigg\Vert\boldsymbol{w}_{c,n-1}-\boldsymbol{\widehat{w}}_{n-1}- \mu\boldsymbol{d}_{n-1}-\mu\widehat{\boldsymbol{s}}_n^B \notag\\
        & \quad\ - \mu\sum\limits_k \pi_k\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1}) \bigg\Vert^2\notag\\
        &=  J(\boldsymbol{\widehat{w}}_{n-1}) + \left(L+\frac{1}{2}\right)\left\Vert\boldsymbol{w}_{c,n-1}-\boldsymbol{\widehat{w}}_{n-1}\right\Vert^2 + 2\mu \left(L+\frac{1}{2}\right) \times \notag\\
        &\quad\ \Big(\boldsymbol{d}_{n-1}+ \widehat{\boldsymbol{s}}_n^B +\sum\limits_k \pi_k \overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1}) \Big)^{\sf T}\left(\boldsymbol{\widehat{w}}_{n-1} -\boldsymbol{w}_{c,n-1}\right) \notag\\
        & \quad\ +\mu^2\left(L+\frac{1}{2}\right)\Big\Vert\sum\limits_k \pi_k\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})+ \boldsymbol{d}_{n-1}+\widehat{\boldsymbol{s}}_n^B \Big\Vert^2\notag\\
         &\overset{(b)}{\le} J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1}) \notag\\
         &\quad\ +2\mu \left(L+\frac{1}{2}\right) (\boldsymbol{d}_{n-1}+\widehat{\boldsymbol{s}}_n^B)^{\sf T}(\boldsymbol{\widehat{w}}_{n-1}-\boldsymbol{w}_{c,n-1}) \notag\\    
         &\quad\ + 2\mu \left(L+\frac{1}{2}\right)\Big(\sum\limits_k \pi_k J_k(\boldsymbol{\widehat{w}}_{n-1}) - \sum\limits_k \pi_k J_k(\boldsymbol{w}_{c,n-1}) \notag\\
         &\quad\ +\sum\limits_k \pi_k\boldsymbol{\eta}_k  +\frac{L}{2}\left\Vert\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}\right\Vert^2\Big) \notag\\
         &\quad\   + \mu^2\left(L+\frac{1}{2}\right)\Big\Vert\sum\limits_k \pi_k\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})+\boldsymbol{d}_{n-1}+\widehat{\boldsymbol{s}}_n^B\Big\Vert^2\notag\\
         &\le J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1})  + 2\mu \left(L+\frac{1}{2}\right)\Big(J(\boldsymbol{\widehat{w}}_{n-1}) - J(\boldsymbol{w}_{c,n-1})  \notag\\
         &\quad\ +\boldsymbol{\eta} +   \frac{L}{2}\left\Vert\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}\right\Vert^2 \Big)\notag\\
         &\quad\ + 2\mu \left(L+\frac{1}{2}\right) \left(\boldsymbol{d}_{n-1}+\widehat{\boldsymbol{s}}_n^B\right)^{\sf T}(\boldsymbol{\widehat{w}}_{n-1} -\boldsymbol{w}_{c,n-1}) \notag\\
         &\quad\ + \mu^2(L +\frac{1}{2})\left\Vert\sum\limits_k \pi_k\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})+\boldsymbol{d}_{n-1}+\widehat{\boldsymbol{s}}_n^B\right\Vert^2
\end{align}
where $(a)$ follows from (\ref{wce}), $(b)$ follows from (\ref{pr2}), and
\begin{align}
\boldsymbol{\eta} &= \sum\limits_k \pi_k\boldsymbol{\eta}_k \notag\\
& \le O(\epsilon)\sum\limits_k \pi_k\Vert\boldsymbol{w}_{k,n-1}-\boldsymbol{w}_{c,n-1}\Vert+O(\epsilon^2)
\end{align}
follows from (\ref{eta_kkk}). Then, conditioning both side of (\ref{pr4}) and substituting (\ref{psz}) into it, we get
\begin{align}\label{pr5}
   & \mathds{E}\left\{J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n})|\boldsymbol{\mathcal{F}}_{n-1}\right\}\notag\\
   & \le J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1}) + 2\mu \left(L+\frac{1}{2}\right) \Big( J(\boldsymbol{\widehat{w}}_{n-1}) -  J(\boldsymbol{w}_{c,n-1})\notag\\
   & \quad\ +\boldsymbol{\eta}  +\frac{1}{2}(L+1)\left\Vert\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}\right\Vert^2  \Big) \notag\\
   &\quad\ + \mu \left(L+\frac{1}{2}\right)\mathds{E}\left\{\left\Vert\boldsymbol{d}_{n-1}\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\right\} \notag\\
   &\quad\ +  \mu^2 \left(L+\frac{1}{2}\right)\mathds{E}\bigg\{2\left\Vert\sum\limits_k \pi_k \overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})\right\Vert^2   \notag\\
   &\quad\  + 2\left\Vert\boldsymbol{d}_{n-1}\right\Vert^2 +\left\Vert\widehat{\boldsymbol{s}}_n^B\right\Vert^2|\boldsymbol{\mathcal{F}}_{n-1}\bigg\}
\end{align}
where we use the inequality
\begin{align}
    \boldsymbol{d}^{\sf T}_{n-1}(\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}) \le \frac{1}{2}\left\Vert\boldsymbol{d}_{n-1}\right\Vert^2+ \frac{1}{2}\left\Vert\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}\right\Vert^2
\end{align}
Taking the expectation of both sides of (\ref{pr5}), we obtain
\begin{align}\label{pr6}
        & \mathds{E}J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n}) \notag\\
        & \le  \mathds{E} J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1}) +2\mu \left(L+\frac{1}{2}\right) \mathds{E} 
        \Big\{J(\boldsymbol{\widehat{w}}_{n-1}) - J(\boldsymbol{w}_{c,n-1})\notag\\
        & \quad\ + \boldsymbol{\eta}
   +\frac{1}{2}(L+1)\left\Vert|\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}\right\Vert^2\Big\}
   \notag\\
   &\quad\ +\mu^2 \left(L+\frac{1}{2}\right) \mathds{E}\left\Vert\widehat{\boldsymbol{s}}_n^B\right\Vert^2\notag\\
   &\quad\ + 2\mu^2\left(L+\frac{1}{2}\right)\mathds{E}\left\Vert\sum\limits_k \pi_k\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})\right\Vert^2\notag\\
   &\quad\ +(\mu + 2 \mu^2)\left(L+\frac{1}{2}\right)\mathds{E}\left\Vert \boldsymbol{d}_{n-1}\right\Vert^2
\end{align}
Then substituting (\ref{pr3}) into (\ref{pr6}), we have
\begin{align}\label{pr7}
     & 2 \mu \left(L+\frac{1}{2}\right)\cdot\frac{1}{4L+2} \mathds{E}\left\Vert\nabla_w J_{\frac{1}{2L + 1}}(\boldsymbol{w}_{c,n-1})\right\Vert^2 \notag\\
     & \le  2 \mu \left(L+\frac{1}{2}\right)\cdot \mathds{E} \Big\{ J(\boldsymbol{w}_{c,n-1}) - J(\boldsymbol{\widehat{w}}_{n-1}) - \notag\\
   & \quad\ \frac{1}{2}(L+1)\left\Vert\boldsymbol{\widehat{w}}_{n-1} - \boldsymbol{w}_{c,n-1}\right\Vert^2\Big\}\notag\\
   &\le \mathds{E}J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1})-\mathds{E}J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n}) + 2\mu\left(L+\frac{1}{2}\right)\mathds{E}\boldsymbol{\eta} \notag\\
   &\quad\ + 2\mu^2\left(L+\frac{1}{2}\right)\mathds{E}\left\Vert\sum\limits_k \pi_k\overline{\nabla_w J_k}(\boldsymbol{w}_{c,n-1})\right\Vert^2\notag\\
   &\quad\ +(\mu + 2 \mu^2)\left(L +\frac{1}{2}\right)\mathds{E}\left\Vert \boldsymbol{d}_{n-1}\right\Vert^2+\mu^2 \left(L+\frac{1}{2}\right) \mathds{E}\left\Vert\widehat{\boldsymbol{s}}_n^B\right\Vert^2\notag\\
\end{align}
{Substituting (\ref{b1}), (\ref{disa_w1}), (\ref{pbs}), and (\ref{51}) into (\ref{pr7}), after sufficient iterations $\widehat{N}$, we obtain
\begin{align}\label{pr10}
&\frac{1}{\widehat{N}}\sum\limits_{n}\mathds{E}||\nabla_w J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,n-1})||^2 \notag\\
       &\le  \frac{2(J_{\frac{1}{2L+1}}(w_{c,n_0})-\Delta)}{\mu \widehat{N}}  + O(\mu) + O(\epsilon^2)
\end{align}
where $\Delta = \min\limits_{\boldsymbol{w}} J(\boldsymbol{w})$. This implies that $\exists n_0\le N \le \widehat{N}$, {for which} we have
\begin{align}
    &\mathds{E}||\nabla_w J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,N-1})||^2 \notag\\
       &\le  \frac{2(J_{\frac{1}{2L+1}}(w_{c,n_0})-\Delta)}{\mu \widehat{N}}  + O(\mu) + O(\epsilon^2)
\end{align}
Then, according to  (\ref{e_154}), we have
\begin{align}\label{pr8}
       &\mathds{E}\left\{\min\limits_{\boldsymbol{\zeta} \in \partial_w J(\widehat{\boldsymbol{w}}_{N-1})}||\zeta||^2\right\} \le  \mathds{E}||\nabla_w J_{\frac{1}{2L+1}}(\boldsymbol{w}_{c,N-1})||^2 \notag\\
       & \le  \frac{2(J_{\frac{1}{2L+1}}(w_{c,n_0})-\Delta)}{\mu \widehat{N}}  + O(\mu) + O(\epsilon^2)
\end{align}
and 
\begin{align}
    \Vert\widehat{\boldsymbol{w}}_{N-1} - \boldsymbol{w}_{c,N-1}\Vert^2 \le \frac{2(J_{\frac{1}{2L+1}}(w_{c,n_0})-\Delta)}{\mu \widehat{N}}  + O(\mu) + O(\epsilon^2)
\end{align}}
\end{proof}

\section{Proof of (\ref{pr2})}\label{ap7}
\begin{proof}
{Note that}
\begin{align}\label{eofmax}
    &Q_k(\boldsymbol{w}_{c};\boldsymbol{x}_k+\boldsymbol{\delta}_k^\star,\boldsymbol{y}_k) -  Q_k(\boldsymbol{w}_c;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k) \notag\\
    & \overset{(a)}{\le} \nabla_{x^{\sf T}} Q_k(\boldsymbol{w}_c;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k)(\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k)+\frac{L}{2}\Vert\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k\Vert^2\notag\\
    & =\Big(\nabla_x Q_k(\boldsymbol{w}_c;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k) -\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k)\Big)^{\sf T}\notag\\
    &\quad\ \times(\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k) +\Big(\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k)\notag\\
    &\quad\ -\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)\Big)^{\sf T}(\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k)\notag\\
    &\quad\ +\nabla_{x^{\sf T}} Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)(\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k)+\frac{L}{2}\Vert\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k\Vert^2\notag\\
    &\overset{(b)}{\le}\Vert\nabla_x Q_k(\boldsymbol{w}_c;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k)-\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k)\Vert\notag\\
    &\quad\ \times\Vert\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k\Vert+\Vert\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k+\widehat{\boldsymbol{\delta}}_k,\boldsymbol{y}_k)\notag\\
    &\quad\ -\nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)\Vert \Vert\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k\Vert\notag+ \frac{L}{2}\Vert\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k\Vert^2\notag\\
    &\overset{(c)}{\le} (L\Vert \boldsymbol{w}_k-\boldsymbol{w}_c\Vert + L\Vert\widehat{\boldsymbol{\delta}}_k\Vert)\Vert\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k\Vert +  \frac{L}{2}\Vert\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k\Vert^2\notag\\
    &\le \Vert \boldsymbol{w}_k-\boldsymbol{w}_c\Vert O(\epsilon_k)+O(\epsilon_k^2)
\end{align}
where $(a)$ and $(c)$ follow from the Lipschitz conditions in Assumption \ref{as2}, $(b)$ is due to
\begin{align}
    \widehat{\boldsymbol{\delta}}_k = \mathop{\mathrm{argmax}}\limits_{\left\Vert\delta_k\right\Vert_{p_k} \le \epsilon_k} \delta_k^{\sf T} \nabla_x Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)
\end{align}
where $\widehat{\boldsymbol{\delta}}_k$ is the exact maximizer of the linear approximation, so that we have
\begin{align}
    \nabla_{x^{\sf T}} Q_k(\boldsymbol{w}_k;\boldsymbol{x}_k,\boldsymbol{y}_k)(\boldsymbol{\delta}_k^\star-\widehat{\boldsymbol{\delta}}_k) \le 0
\end{align}
Meanwhile,
\begin{align}
    \boldsymbol{\delta}_k^\star = \mathop{\mathrm{argmax}}\limits_{\left\Vert\delta_k\right\Vert_{p_k} \le \epsilon_k} Q_k(\boldsymbol{w}_c;\boldsymbol{x}_k+ \delta_k,\boldsymbol{y}_k)
\end{align}
is the true maximizer for $\boldsymbol{w}_c$.
Then for any ${\boldsymbol{w}} \in  \boldsymbol{\mathcal{F}}_{n-1}$, we have
\begin{align}\label{pr1}
         & J_k({\boldsymbol{w}}) =\mathds{E}_{\{\boldsymbol{x}_k, \boldsymbol{y}_k\}}\left\{\max\limits_{\left\Vert\delta_k\right\Vert_{p_k}\le \epsilon_k} Q_k(\boldsymbol{w};\boldsymbol{x}_{k}+\delta_k,\boldsymbol{y}_k) \right\}\notag\\
         &\ge  \mathds{E}_{\{\boldsymbol{x}_k, \boldsymbol{y}_k\}}Q_k(\boldsymbol{{w}};\widehat{\boldsymbol{x}}_{k},\boldsymbol{y}_k)\notag\\
        &\overset{(a)}{\ge} \mathds{E}_{\{\boldsymbol{x}_k, \boldsymbol{y}_k\}}Q_k(\boldsymbol{w}_{c,n-1};\widehat{\boldsymbol{x}}_{k},\boldsymbol{y}_k)  - \frac{L}{2}\left\Vert\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1}\right\Vert^2\notag\\
        &\quad\ + \mathds{E}_{\{\boldsymbol{x}_k, \boldsymbol{y}_k\}} \nabla_{w^{\sf T}}  Q_k(\boldsymbol{w}_{c,n-1};{\widehat{\boldsymbol{x}}}_{k},\boldsymbol{y}_k)(\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1})\notag\\
         &= \mathds{E}_{\{\boldsymbol{x}_k, \boldsymbol{y}_k\}}Q_k(\boldsymbol{w}_{c,n-1};\widehat{\boldsymbol{x}}_{k},\boldsymbol{y}_k)  - \frac{L}{2}\left\Vert\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1}\right\Vert^2\notag\\
        &\quad\ + \overline{\nabla_{w^{\sf T}} J_k}(\boldsymbol{w}_{c,n-1})(\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1})\notag\\
        %& = \widehat{J}_k(\boldsymbol{w}_{c,n-1}) + \nabla_w \mathds{E}_{\boldsymbol{x_k}} Q_k(\boldsymbol{w}_{c,n-1};\widehat{\boldsymbol{x}}_{k})^{\sf T}(\boldsymbol{\widetilde{w}}-\boldsymbol{w}_{c,n-1}) \notag\\
        %& \quad\ \left\Vert\widetilde{\boldsymbol{w}} -\boldsymbol{w}_{c,n-1}\right\Vert^2\notag\\
        &\overset{(b)}{\ge} \mathds{E}_{\{\boldsymbol{x}_k, \boldsymbol{y}_k\}}\left\{\max\limits_{\left\Vert\delta_k\right\Vert_{p_k} \le \epsilon_k} Q_k(\boldsymbol{w}_{c,n-1};\boldsymbol{x}_{k}+\delta_k,\boldsymbol{y}_k)\right\} - \boldsymbol{\eta}_k \notag\\
        &\quad\ - \frac{L}{2}\left\Vert\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1}\right\Vert^2 + \overline{\nabla_{w^{\sf T}} J_k}(\boldsymbol{w}_{c,n-1})(\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1}) \notag\\
         &= J_k(\boldsymbol{w}_{c,n-1}) - \boldsymbol{\eta}_k - \frac{L}{2}\left\Vert\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1}\right\Vert^2 \notag\\
         &\quad\ + \overline{\nabla_{w^{\sf T}} J_k}(\boldsymbol{w}_{c,n-1})(\boldsymbol{{w}}-\boldsymbol{w}_{c,n-1}) 
\end{align}
{where $(a)$ follows from the L-weak convexity of $Q(\cdot)$ over $w$}, $(b)$ follows from (\ref{eofmax}), and
\begin{align}\label{eta_kkk}
&\boldsymbol{\eta}_k \le O(\epsilon_k)\Vert \boldsymbol{w} - \boldsymbol{w}_{c, n-1}\Vert+O(\epsilon_k^2)\\
&\widehat{\boldsymbol{x}}_k = \boldsymbol{x}_k + \mathop{\mathrm{argmax}}\limits_{\left\Vert\delta_k\right\Vert_{p_k} \le \epsilon_k} \delta_k^{\sf T} \nabla_x Q_k(\boldsymbol{w}_{k,n-1};\boldsymbol{x}_k,\boldsymbol{y}_k)
\end{align}
where the perturbation added to $\boldsymbol{x}_k$ is evaluated at $\boldsymbol{w}_{k,n-1}$.
\end{proof}
%\section{Proof for Corollary \ref{co1}}\label{apE}
%The proof for Corollary \ref{co1} is similar with Theorem \ref{th1}, the difference is just in (\ref{55}) and (\ref{56}), as with $\ell_\infty$ bound we get
%\begin{align}\label{upfgsm}
     %&\quad\ Q_k(\boldsymbol{w}_{c,n-1};\boldsymbol{x}_{k,n}+\delta^\star) - Q_k(\boldsymbol{w}_{c,n-1};\widehat{\boldsymbol{x}}_{k,n}) \notag\\
     %&\le C\left\Vert\boldsymbol{\delta}^\star - \widehat{\boldsymbol{\delta}}_{k,n}\right\Vert \le 2C\sqrt{M}\epsilon
%\end{align}
%Substituting (\ref{upfgsm}), we can obtain (\ref{convergencefgsm}). Note that although the upper bound in (\ref{upfgsm}) is stronger than (\ref{56}), $\epsilon$ in $\ell_\infty$ attack can be smaller than it in $\ell_2$ to make the attack works.
\end{appendices}
\bibliographystyle{ieeetr} %ieeetr国际电气电子工程师协会期刊
\bibliography{reference} % ref就是之前建立的ref.bib文件的前缀
\end{document}

