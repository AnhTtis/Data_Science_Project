\section{Background}
\label{sec:background}
\subsection{Surface variation}
 \label{sec:surface}
Consider an unstructured dense point cloud $P=\{\boldsymbol{p_{1}}, \boldsymbol{p_{2}}, ..., \boldsymbol{p_{N}}\}$ of size $N$ existing in 3D Euclidean space, $\mathbb{R}^3$. We can generate the local neighbourhood $N_{\boldsymbol{p_i}}$ of each point $\boldsymbol{p_{i}}$ in $P$ by two different methods. Firstly, we can gather all of the points within a certain Euclidean distance $r$ from $\boldsymbol{p_i}$; this approach is referred to as \textit{radius search}. Alternatively, we can gather all of the k-nearest Euclidean neighbours of $\boldsymbol{p_i}$, which is referred to as \textit{KNN search}. The choice of this scale-factor ($r$ or $k$) not only depends on the size and density of a point cloud but also on the desired level of detail for a given application. These aspects make the task of automatic estimation of the neighbourhood of a point in a cloud an important, yet challenging one \cite{rusu2010semantic}. In this work, we implement the approach taken by the \textit{CloudCompare} software package \cite{girardeau2016cloudcompare}, where this process is automated by first calculating an approximate surface per point from the bounding box volume. This estimated value, along with a user-defined approximate neighbour number, is used to estimate a radius $r$, which is then used to perform radius search for each point. In our method, we have fixed this approximate neighbour number to $25$ as it provides good empirical performance across a wide variety of point clouds.

Several local surface properties \cite{thomas2018semantic} of the point cloud at a given query point $\boldsymbol{p_i}$ can be estimated by analysing the eigenvalues and eigenvectors of the covariance matrix $\mathbf{C_i}$ defined by the point's neighbourhood $N_{\boldsymbol{p_i}}=\{\boldsymbol{p_{i_1}}, \boldsymbol{p_{i_2}}, ..., \boldsymbol{p_{i_n}}\}$: 
\begin{equation} \label{}
\mathbf{C_i}={\begin{bmatrix}
\boldsymbol{p_{i_1}}-\boldsymbol{\bar{p_i}} \\
\boldsymbol{p_{i_2}}-\boldsymbol{\bar{p_i}} \\
... \\
\boldsymbol{p_{i_n}}-\boldsymbol{\bar{p_i}}
\end{bmatrix}}^T 
\cdot 
\begin{bmatrix}
\boldsymbol{p_{i_1}}-\boldsymbol{\bar{p_i}} \\
\boldsymbol{p_{i_2}}-\boldsymbol{\bar{p_i}} \\
... \\
\boldsymbol{p_{i_n}}-\boldsymbol{\bar{p_i}}
\end{bmatrix},
\end{equation}
where, $\boldsymbol{\bar{p_i}}$ is the centroid of all the points $\boldsymbol{p_{i_i}} \in N_{\boldsymbol{p_i}}$. By means of \textit{principal component analysis} (PCA), we may now fit a plane tangent to the 3D surface, formed by all of the points within $N_{\boldsymbol{p_i}}$, at $\boldsymbol{\bar{p_i}}$. As $\mathbf{C_i}$ is a $3\times3$ symmetric and positive semi-definite matrix, all of its eigenvalues $\left(\lambda_j, j\in\{0,1,2\}\right)$ are positive and real, whilst the corresponding eigenvectors ($\boldsymbol{v_j}$) form an orthogonal frame corresponding to the principal components of $N_{\boldsymbol{p_i}}$. If $0\leq\lambda_0\leq\lambda_1\leq\lambda_2$, then $\boldsymbol{v_2}$ and $\boldsymbol{v_1}$ span the aforementioned tangent plane, whilst $\boldsymbol{v_0}$ represents the vector perpendicular to it. Therefore, $\boldsymbol{v_0}$ can be considered as an estimate of the surface normal to the point cloud (without actual surface reconstruction) at query point $\boldsymbol{p_i}$. Furthermore, as defined by Pauly \textit{et al.} \cite{pauly2002efficient}, we can calculate the \textit{surface variation} at the query point as:
%
\begin{equation} \label{sf}
\sigma_n(\boldsymbol{p_i})=\frac{\lambda_0}{\lambda_0+\lambda_1+\lambda_2}.
\end{equation}
%
This quantity is not only closely related to the surface curvature at $\boldsymbol{p_i}$ but also serves as a more suitable criterion for simplification, as discussed in detail by the authors \cite{pauly2002efficient}.
%-------------------------------------------------------------------------
%
\subsection{Gaussian processes on Riemannian manifolds}
\label{sec:riemannian_gp}
Gaussian processes (GPs) are non-parametric Bayesian models which allow for a rigorous estimation of predictive uncertainty, and have been widely studied and applied by the machine learning community over the last two decades. Consider a scenario where we have a training dataset of $N$ observations, $\{\mathbf{x}_i, y_i\}_{i=1}^N$, where $\mathbf{x}_i \in \mathbb{R}^P$ and $y_i \in \mathbb{R}$. In our application, $\mathbf{x}_i \in \mathbb{R}^3$ is a Euclidean coordinate, and $y_i$ is the surface variation associated with said coordinate. We assume access to noisy observations of an underlying latent function, such that $y_i = f(\mathbf{x}_i) + \epsilon_i$, where $\epsilon_i \sim \mathcal{N}(0, \sigma_y^2)$. A GP defines a distribution over functions which we can use to infer the form of the true latent function which generated our training data. The GP prior can be written as
$f \sim \mathcal{GP}\left(\mu\left(\mathbf{x}\right), k\left(\mathbf{x}, \mathbf{x}^\prime\right)\right)$,
where, $\mu(\cdot)$ and $k(\cdot)$ are the mean and kernel functions respectively, which completely describe our process \cite{rasmussen2006gaussian}. As is common, we assume a zero-mean prior throughout this work, using the kernel as the primary means of modeling the variation in our function over its domain. A popular choice for GP kernels is the Mat\'ern class of covariance function, which takes the form,
$k_\nu(\mathbf{x}, \mathbf{x}^\prime) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{r\sqrt{2\nu}}{\kappa} \right)^\nu K_\nu \left(\frac{r\sqrt{2\nu}}{\kappa} \right)$,
% \begin{align}
%     k_\nu(\mathbf{x}, \mathbf{x}^\prime) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{r\sqrt{2\nu}}{\kappa} \right)^\nu K_\nu \left(\frac{r\sqrt{2\nu}}{\kappa} \right),
%     \label{eq:matern_kernel}
% \end{align}
where $r=\lVert \mathbf{x}-\mathbf{x}^\prime \rVert$ and $K_\nu$ is a modified Bessel function. We define $\boldsymbol{\theta} = \{ \sigma^2, \kappa, \nu\}$ to be the set of kernel hyperparameters; $\sigma^2$ controls the variance of the GP, $\kappa$ the lengthscale of its variation and $\nu$ its degree of differentiability.

\paragraph{Inference:} Using Bayes' Rule, we can condition our GP on the training data and derive closed form expressions for the posterior mean and covariance:
\begin{align}
\label{eq:post_mu}
\boldsymbol{\mu}_{\text{post}} &= \mathbf{K}_{*}(\mathbf{K} + \sigma_y^2 \mathbf{I})^{-1} \mathbf{y}, \\
\label{eq:post_sigma}
\boldsymbol{\Sigma}_{\text{post}} &= \mathbf{K}_{**} - \mathbf{K}_* (\mathbf{K} + \sigma_y^2 \mathbf{I})^{-1} \mathbf{K}_*^\top .
\end{align}
Generally, the noise variance $\sigma_y^2$ and the kernel function hyperparameters $\boldsymbol{\theta}$ are optimised via maximisation of the log-marginal likelihood, which can also be derived analytically. Where $\mathbf{X} \in \mathbb{R}^{N \times P}$ and $\mathbf{y} \in \mathbb{R}^{N}$ are matrix and vectorial representations of our training inputs and targets respectively, the log-marginal likelihood takes the form \cite{rasmussen2006gaussian}, 
\begin{align} \label{eq:ll}
\begin{split}
    \log p(\mathbf{y} \mid \mathbf{X}, \boldsymbol{\theta}, & \sigma_y^2) = -\frac{1}{2} \mathbf{y}^\top \left(\mathbf{K} + \sigma_y^2\mathbf{I} \right)^{-1} \\ &- \frac{1}{2} \log \mid \mathbf{K} + \sigma_y^2\mathbf{I} \mid - \frac{N}{2} \log (2\pi).
\end{split}
\end{align}

\paragraph{Kernels on manifolds:} Many different kernel functions for GPs exist, and choosing a kernel is in itself a model selection problem as some kernels are more suited to modeling certain types of data. However, one characteristic which many kernels share is that they are defined using Euclidean distance. This presents an issue should we wish to use a GP to model variation in a quantity over a non-Euclidean space. Borovitskiy \textit{et al.} \cite{borovitskiy2020matern} proposed a solution to this problem in the form of an extension to the Mat\'ern kernel, which allows for modeling of functions whose domains are compact Riemannian manifolds. The approach proposed by the authors involves two stages. Firstly, numerical estimation of the eigenvalues $\lambda_n$ and eigenfunctions $f_n$ corresponding to the Laplace-Beltrami operator of the given manifold is performed. Secondly, for a manifold of dimensionality $d$, the kernel is approximated using a finite truncation of:
\begin{equation} \label{eq:kernel}
    k_{\nu}(\mathbf{x}, \mathbf{x}^\prime) = \frac{\sigma^2}{C_{\nu}} \sum_{n=0}^{\infty} \left(\frac{2\nu}{\kappa^2} + \lambda_n \right)^{-\nu - \frac{d}{2}} f_n(\mathbf{x}) f_n(\mathbf{x}^\prime) ,
\end{equation}
where, $C_{\nu}$ is a normalizing constant. The hyperparameters $\sigma^2$, $\kappa$ and $\nu$ have similar interpretations to those introduced for the conventional Euclidean Mat\'ern kernel.
%
\subsection{Greedy subset-of-data algorithm}
\label{sec:sod}
A major challenge which arises when working with GPs in practice is the $\mathcal{O}(N^3)$ complexity associated with performing exact inference, which arises due to the matrix inversions in Equations \eqref{eq:post_mu} and \eqref{eq:post_sigma}. To circumvent this issue, numerous formulations of \textit{sparse GPs} have been proposed, many of which are based on approximate inference techniques and concepts such as \textit{inducing points} \cite{liu2020gaussian}. In this work however, we consider the \textit{subset-of-data} (SoD) approach \cite{cao2013efficient, lalchand2018fast}. As explained in Section 8.3.3 of \cite{rasmussen2006gaussian}, it is a conceptually simple form of sparse approximation which allows for exact Bayesian inference. In this setting, rather than modifying the formulation of the GP itself, we simply perform exact inference using a carefully selected subset of $M (<< N)$ observations. Specifically, for our case we modify the greedy SoD approach of \cite{lalchand2018fast}, which uses a selection criterion to sequentially construct a subset of size $M$ which is representative of our full training set of $N$ observations. We use this technique for GP sparsification in order to construct a set of inducing points for a point cloud which are best capable of representing the changes in surface variation over the cloud; this set of points forms our simplified point cloud. The original method involves randomly selecting one initial inducing point and then adding one point to the set at each iteration \cite{lalchand2018fast}, however we have employed \textit{farthest point sampling} (FPS) for selecting a set of initial inducing points and instead of one, and we add several points to our set of inducing points at each iteration. Our approach is explained in further detail in Section \ref{sec:methodology}.

Our method forms a simplified point cloud which is a subset of the original, thus the optimization problem is a discrete one. There has been recent work on inducing point optimization on discrete domains \cite{fortuin2021sparse}, however such methods only obtain comparable performance to methods based on greedy selection of the inducing points from the input domain, which are considerably conceptually simpler. The main disadvantage of a greedy approach is that the training set does not necessarily span the whole input domain, however in our setting this is indeed the case, making our application especially well-suited to a greedy approach. Additionally, our proposed method allows us to obtain competitive results for clouds containing millions of points, whilst still employing exact Bayesian inference rather than approximate variational schemes, which can often underestimate the variance of the posterior distribution \cite{blei2017variational}.