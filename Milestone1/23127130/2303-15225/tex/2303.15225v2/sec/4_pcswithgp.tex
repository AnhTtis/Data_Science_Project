\section{Point cloud simplification with Riemannian Gaussian processes} 
\label{sec:methodology}
In this section, we outline our GP-based approach with the help of a concise algorithm. We can represent a point cloud of size $N$ as a set of 3D Euclidean coordinates $P = \{\mathbf{x}_i\}^N_{i=1}$, where $\mathbf{x}_i \in \mathbb{R}^{3}$. The surface variation $y_i \in \mathbb{R}$ at each point in $P$ can be computed using Equation \ref{sf} of Section \ref{sec:surface}. Using this data we formulate a regression problem, whereby we employ a Gaussian process with a Mat\'ern kernel defined on a Riemannian manifold (as described in Section \ref{sec:riemannian_gp}) to predict the surface variation from the coordinates of each point. We then employ the greedy subset-of-data scheme discussed in Section \ref{sec:sod} in order to obtain a simplified set of $M (<< N)$ 3D coordinates, $P_{\text{simp}} = \{\mathbf{x}_j\}_{j=1}^M$, where $P_{\text{simp}} \subset P$.

We formally outline our proposed approach in Algorithm \ref{alg:simpli}.
$\text{FPS}(P, k_\text{init})$ denotes a function which selects $k_\text{init}$ initial points from $P$ using FPS; we use this to initialise our active set $P_{\text{simp}}$ with an initial set of points from across the point cloud. $\text{MAX}(\mathbf{s}, R, k_\text{add})$ selects the points from the remainder set $R$ which are associated with the $k_\text{add}$ largest values in our selection criterion vector $\mathbf{s}$. The notation $\mathbf{y}(R)$ denotes a vector containing the target surface variation values associated with each of the points contained within the set $R$. At each step $t$ of the algorithm, we update the posterior mean $\boldsymbol{\mu}_t$ and covariance $\boldsymbol{\Sigma}_t$ using Equations \eqref{eq:post_mu} and \eqref{eq:post_sigma} respectively, where the active set $P_{\text{simp}}$ is used as training data, whilst the remainder set $R$ is unseen test data.

\begin{algorithm}
\caption{GP-based simplification algorithm}\label{alg:simpli}
\begin{algorithmic}
\State \textbf{Data:} $P$, $\mathbf{y}$, $M$, $k_\text{init}$, $k_\text{add}$,  $k_\text{opt}$, GP prior $\mathcal{GP}(0, k(\cdot, \cdot))$, where $k$ is defined in Eq. \eqref{eq:kernel}
\State \textbf{Result:} $P_{\text{simp}}$
\State $P_{\text{opt}} \leftarrow$ random subset of  $k_\text{opt}$ points from $P$;
\State Optimise GP hyperparameters using Eq. \eqref{eq:ll}, $P_{\text{opt}}$ and $\mathbf{y}(P_{\text{opt}})$;
\State Active set $P_{\text{simp}} \leftarrow \text{FPS}(P, k_\text{init})$;

\State Remainder set $R \leftarrow P - P_{\text{simp}}$;
\While{$\lvert P_{\text{simp}} \rvert < M$}
\State Compute $\boldsymbol{\mu}_t$ and $\boldsymbol{\Sigma}_t$ using Eq. \eqref{eq:post_mu} and Eq. \eqref{eq:post_sigma};
    
\State $\mathbf{s} \leftarrow \sqrt{\text{diag}(\boldsymbol{\Sigma}_t)}+\lvert \boldsymbol{\mu}_t - \mathbf{y}(R) \rvert$;

\State $P_{\text{simp}} \leftarrow P_{\text{simp}} + \text{MAX}(\mathbf{s}, R, k_\text{add})$;
\State $R \leftarrow R - \text{MAX}(\mathbf{s}, R, k_\text{add})$;
\EndWhile
\end{algorithmic}
\end{algorithm}
To clarify, we predict the surface variation and the uncertainty values for $R$ based on $P_\text{simp}$ at each iteration of our algorithm. The selection criterion which we use favours selection of points within the original cloud which lie in regions of high predictive uncertainty and/or error. By selecting a set of points using this criterion, we form a simplified cloud which implicitly favours selection of points surrounding finer details within the cloud, where the error and uncertainty is likely to be high if we have not yet selected a sufficient number of points around said location.

As $P_\text{simp}$ grows with each iteration to be gradually more representative of our input data, the uncertainty and predicted surface variation values for points in $R$ also change. For example, consider two neighbouring points on the tip of one of the Stanford bunnyâ€™s ears, and assume that neither of them are currently in $P_\text{simp}$. If one of these points is added to $P_\text{simp}$, the elements of the uncertainty $\sqrt{diag(\boldsymbol{\Sigma_t})}$ and error $|\boldsymbol{\mu}_t - \mathbf{y}(R)|$ associated with the second point will decrease, and in subsequent iterations it may no longer be one of the top-ranked points based on the selection metric $\mathbf{s}$.