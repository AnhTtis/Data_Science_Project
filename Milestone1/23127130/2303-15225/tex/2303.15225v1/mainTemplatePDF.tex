\documentclass[10pt,twocolumn,letterpaper]{article} 

\usepackage{simpleConference}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{xcolor}
\usepackage{booktabs}
\pagenumbering{arabic}

\begin{document}

\title{One-shot Feature-Preserving Point Cloud Simplification with Gaussian Processes on Riemannian Manifolds}

\author{Stuti Pathak$^\dagger$\\
University of Antwerp\\
% Prinsstraat 13, Antwerpen 2000\\
{\tt\small stuti.pathak@uantwerpen.be}
\and
Thomas M. McDonald$^\dagger$\\
University of Manchester\\
% Oxford Road, Manchester, M13 9PL\\
{\tt\small thomas.mcdonald-2@postgrad.manchester.ac.uk}
\and
Rudi Penne\\
University of Antwerp\\
% Prinsstraat 13, Antwerpen 2000\\
{\tt\small rudi.penne@uantwerpen.be}
}


\maketitle
\def\thefootnote{$\dagger$}\footnotetext{Equal contribution.}\def\thefootnote{\arabic{footnote}}


\begin{abstract}
The processing, storage and transmission of large-scale point clouds is an ongoing challenge in the computer vision community which hinders progress in the application of 3D models to real-world settings, such as autonomous driving, virtual reality and remote sensing. We propose a novel, one-shot point cloud simplification method which preserves both the salient structural features and the overall shape of a point cloud without any prior surface reconstruction step. Our method employs Gaussian processes with kernels defined on Riemannian manifolds, allowing us to model the surface variation function across any given point cloud. A simplified version of the original cloud is obtained by sequentially selecting points using a greedy sparsification scheme. The selection criterion used for this scheme ensures that the simplified cloud best represents the surface variation of the original point cloud. We evaluate our method on several benchmark datasets, compare it to a range of existing methods and show that our method is competitive both in terms of empirical performance and computational efficiency.
\end{abstract}

\section{Introduction}
\label{intro}
Recent years have seen a growing need for the conversion of real-world objects to computerized models \cite{xiao2021estimating, fernandes2021point} across several domains, such as digital preservation of cultural heritage \cite{PIERACCINI200163} and manufacturing of mechanical parts for industry \cite{LI200253}. This need has given rise to a range of modern data acquisition techniques such as laser scanning, which densely samples the surface of a 3D object, thereby generating millions of significantly redundant data points. 

\begin{figure}[H]
     \centering
        \includegraphics[scale=0.6, trim=0cm 5cm 19cm 0cm, clip, width=\linewidth]{images/page1.pdf}
         \caption{Point cloud simplification methods typically fail to strike a balance between preserving sharp features and maintaining the overall structure of the original cloud. Our approach circumvents this trade-off by achieving both of these aims, as is evident from the shown simplified versions of the Stanford Bunny \cite{levoy2005stanford} obtained using the proposed technique and some existing methods such as Hierarchical Clustering (HC) and Weighted Locally
Optimal Projection (WLOP).}
\vspace{-0.4cm}
         \label{fig:figure1}
\end{figure}

3D models can be obtained from this \textit{point cloud} by constructing a polygonal mesh using techniques such as the \textit{ball-pivoting algorithm} and \textit{Poisson surface reconstruction} \cite{bernardini1999ball, kazhdan2006poisson, berger2014state}. However, the sheer size of these dense point clouds makes this task computationally expensive in terms of both memory and time. Furthermore, the size of such generated meshes impedes further processing efforts, and necessitates the use of costly mesh simplification strategies \cite{garland1997surface, hoppe1993mesh, cignoni1998comparison} for size reduction. This makes efficient simplification of the underlying point cloud itself, prior to any surface reconstruction, an extremely important and impactful problem which if addressed, has the potential to significantly improve the scalability of several computer vision applications. 

The inherent dependency of surface reconstruction methods on surface normals, makes the visual perceptual quality of a point cloud an indirect yet important aspect of any mesh processing pipeline \cite{ cignoni1998comparison}. Although it is difficult to quantify this visual degradation in the case of point cloud simplification methods, one can say that the more enhanced the characteristic features of an object (such as sharp edges and high curvature regions) are in the simplified cloud, the higher its human perceptual quality \cite{lee2005mesh}. Therefore, an optimal point cloud simplification technique should preserve both the global structural appearance, and the salient features of the point cloud in question. Some of these methods will be discussed in detail in the upcoming section.

Given that the point cloud representing an object exists on a Riemannian manifold in 3D space, Euclidean distance fails to measure the intrinsic distance between any two points on its surface. Recent advances in the machine learning community have seen gained popularity
in extending the existing concepts
to functions defined on manifolds. For instance, \textit{Gaussian
processes} (GPs), a widely used class of non-parametric statistical
models, which often use Euclidean distance-based
covariance functions, have been made compatible for functions
whose domains are compact Riemannian manifolds
using ideas from harmonic analysis \cite{borovitskiy2020matern}.

In this work, we propose a novel, one-shot, feature-preserving simplification method using GPs with kernels defined on Riemannian manifolds. Using a greedy algorithm for GP sparsification, we iteratively construct a simplified representation of a point cloud without the need for any prior surface reconstruction or training on large point cloud datasets. We achieve competitive results both empirically and in terms of computational efficiency as compared to a number of existing simplification techniques. Qualitatively, as shown in Figure \ref{fig:figure1}, our method effectively preserves visual features whilst providing a sufficiently dense coverage of the domain of the original cloud.

\paragraph{Outline of the paper:} Section \ref{sec:rw} briefly reviews a number of existing point cloud simplification techniques which are relevant to our work. Section \ref{sec:background} provides background details regarding the computation of surface variation, GPs with kernels defined on non-Euclidean domains and a greedy subset-of-data scheme for GP inference. Section \ref{sec:methodology} outlines the proposed GP-based point cloud simplification algorithm. Section \ref{sec:experiments} includes an empirical evaluation of our method on various benchmark point cloud datasets, with comparisons to competing simplification techniques. Finally, Section \ref{sec:conc} summarises our contributions and provides a brief discussion of the scope for future work.
%------------------------------------------------------------------------
\section{Related work}
\label{sec:rw}
In this section we will introduce a number of existing point cloud simplification techniques, with a particular focus on works which have a feature-preserving element to their approach. Some of the earliest curvature-sensitive simplification techniques were proposed by Pauly \textit{et al.} \cite{pauly2002efficient} and Moenning \textit{et al.} \cite{moenning2003new}. The former method, termed \textit{Hierarchical Clustering} (HC), recursively divides the original dataset into two sets, until each child set attains a size smaller than a threshold \textit{size parameter}. Additionally, a \textit{variation parameter} plays an important role in sparsifying regions of low curvature by selective splitting. The perceptual quality and the size of the simplified cloud depend entirely on these two parameters, which must be carefully and manually tuned, making HC unsuitable for automated applications. Additionally, the surface reconstructions obtained from HC simplified point clouds are often poor for clouds with complex surfaces, as will be seen in Section \ref{sec:experiments}. This is because it is challenging to tune the parameters of HC in such a way that preservation of sharp features is achieved whilst still ensuring dense coverage of the original cloud.

Another widely-used technique is \textit{Weighted Locally Optimal Projection} (WLOP) proposed by Huang \textit{et al.} \cite{huang2009consolidation}. In this work, the authors modified the existing parameterization-free denoising simplification scheme termed \textit{Locally Optimal Projection} (LOP) \cite{lipman2007parameterization}, which is  unsuitable for non-uniformly distributed point clouds. WLOP overcomes this limitation by incorporating locally adaptive density weights into LOP. Although WLOP results in an evenly distributed simplified cloud, it still lacks sensitivity towards salient geometric features which will also become apparent in Section \ref{sec:experiments}. Recently, Potamias \textit{et al.} \cite{potamias2022revisiting} have proposed a graph neural network-based learnable simplification technique which uses a modified variant of Chamfer distance in order to backpropagate errors. Their method can simplify point clouds in real-time but involves a computationally intensive training process using large point cloud datasets such as TOSCA \cite{bronstein2008numerical} and ModelNet10 \cite{wu20153d}. Moreover, their model's efficiency is limited to simplifying point clouds which are structurally similar to the learned data, as inherently neural networks struggle to generalize outside of the domain of the training data. 

\textit{Approximate Intrinsic Voxel Structure for Point Cloud Simplification} (AIVS), introduced by Lv \textit{et al.} \cite{lv2021approximate}, combines global voxel structure and local farthest point sampling to generate simplification demand-specific clouds which can be either isotropic, curvature-sensitive or have sharp edge preservation. As with HC however, AIVS requires manual tuning of user-specified parameters in order to obtain optimal results. Additionally, even in parallel computation mode, AIVS is quite costly in terms of computational runtime. Potamias \textit{et al.} and  Lv \textit{et al.} do not provide open-source implementations of their curvature-sensitive simplification techniques, which poses a challenge for reproducibility and benchmarking. Qi \textit{et al.} \cite{qi2019feature} introduced \textit{PC-Simp}, a method which aims to produce uniformly-dense and feature-sensitive simplified clouds, leveraging ideas from graph signal processing. This uniformity depends on a \textit{weight parameter} which is once again user-specified (as with HC and AIVS). Alongside simplification, they also apply their technique to point cloud registration. However, in practice PC-Simp is unreliable for complex-surfaced point clouds as it fails to provide a high degree of feature-preservation, regardless of the weight parameter chosen. Additionally, as discussed later in Section \ref{sec:rw}, the runtime of this technique is considerably longer than any other method tested.

Finally, it has been observed that most of the aforementioned works on feature-preserving point cloud simplification schemes experiment on structurally simple point clouds. Furthermore, surface reconstruction results are rarely presented and discussed. Hence, to underline the efficiency of our method, we experiment on point clouds generated from three complex-surfaced objects and provide the corresponding reconstruction results.

%-------------------------------------------------------------------------
\section{Background}
\label{sec:background}
\subsection{Surface variation}
 \label{sec:surface}
Consider an unstructured dense point cloud $P=\{\boldsymbol{p_{1}}, \boldsymbol{p_{2}}, ..., \boldsymbol{p_{N}}\}$ of size $N$ existing in 3D Euclidean space, $\mathbb{R}^3$. We can generate the local neighbourhood $N_{\boldsymbol{p_i}}$ of each point $\boldsymbol{p_{i}}$ in $P$ by two different methods. Firstly, we can gather all of the points within a certain Euclidean distance $r$ from $\boldsymbol{p_i}$; this approach is referred to as \textit{radius search}. Alternatively, we can gather all of the k-nearest Euclidean neighbours of $\boldsymbol{p_i}$, which is referred to as \textit{KNN search}. The choice of this scale-factor ($r$ or $k$) not only depends on the size and density of a point cloud but also on the desired level of detail for a given application. These aspects make the task of automatic estimation of the neighbourhood of a point in a cloud an important, yet challenging one \cite{rusu2010semantic}. In this work, we implement the approach taken by the \textit{CloudCompare} software package \cite{girardeau2016cloudcompare}, where this process is automated by first calculating an approximate surface per point from the bounding box volume. This estimated value, along with a user-defined approximate neighbour number, is used to estimate a radius $r$, which is then used to perform radius search for each point. In our method, we have fixed this approximate neighbour number to $25$ as it provides good empirical performance across a wide variety of point clouds.

Several local surface properties \cite{thomas2018semantic} of the point cloud at a given query point $\boldsymbol{p_i}$ can be estimated by analysing the eigenvalues and eigenvectors of the covariance matrix $\mathbf{C_i}$ defined by the point's neighbourhood $N_{\boldsymbol{p_i}}=\{\boldsymbol{p_{i_1}}, \boldsymbol{p_{i_2}}, ..., \boldsymbol{p_{i_n}}\}$: 
\begin{equation} \label{}
\mathbf{C_i}={\begin{bmatrix}
\boldsymbol{p_{i_1}}-\boldsymbol{\bar{p_i}} \\
\boldsymbol{p_{i_2}}-\boldsymbol{\bar{p_i}} \\
... \\
\boldsymbol{p_{i_n}}-\boldsymbol{\bar{p_i}}
\end{bmatrix}}^T 
\cdot 
\begin{bmatrix}
\boldsymbol{p_{i_1}}-\boldsymbol{\bar{p_i}} \\
\boldsymbol{p_{i_2}}-\boldsymbol{\bar{p_i}} \\
... \\
\boldsymbol{p_{i_n}}-\boldsymbol{\bar{p_i}}
\end{bmatrix},
\end{equation}
where, $\boldsymbol{\bar{p_i}}$ is the centroid of all the points $\boldsymbol{p_{i_i}} \in N_{\boldsymbol{p_i}}$. By means of \textit{principal component analysis} (PCA), we may now fit a plane tangent to the 3D surface, formed by all of the points within $N_{\boldsymbol{p_i}}$, at $\boldsymbol{\bar{p_i}}$. As $\mathbf{C_i}$ is a $3\times3$ symmetric and positive semi-definite matrix, all of its eigenvalues $\left(\lambda_j, j\in\{0,1,2\}\right)$ are positive and real, whilst the eigenvectors ($\boldsymbol{v_j}$) form an orthogonal frame corresponding to the principal components of $N_{\boldsymbol{p_i}}$. If $0\leq\lambda_0\leq\lambda_1\leq\lambda_2$, then $\boldsymbol{v_2}$ and $\boldsymbol{v_1}$ span the aforementioned tangent plane, whilst $\boldsymbol{v_0}$ represents the vector perpendicular to it. Therefore, $\boldsymbol{v_0}$ can be considered as an estimate of the surface normal to the point cloud (without actual surface reconstruction) at query point $\boldsymbol{p_i}$. Furthermore, as defined by Pauly \textit{et al.} \cite{pauly2002efficient}, we can calculate the \textit{surface variation} at the query point as:

\begin{equation} \label{sf}
\sigma_n(\boldsymbol{p_i})=\frac{\lambda_0}{\lambda_0+\lambda_1+\lambda_2}.
\end{equation}

This quantity is not only closely related to the surface curvature at $\boldsymbol{p_i}$ but also serves as a more suitable criterion for simplification, as discussed in detail by the authors \cite{pauly2002efficient}.
%-------------------------------------------------------------------------
%
\subsection{Gaussian processes on Riemannian manifolds}
\label{sec:riemannian_gp}
Gaussian processes (GPs) are non-parametric Bayesian models which allow for a mathematically rigorous estimation of predictive uncertainty, and have been widely studied and applied by the machine learning community over the last two decades. Consider a scenario where we have a training dataset of $N$ observations, $\{\mathbf{x}_i, y_i\}_{i=1}^N$, where $\mathbf{x}_i \in \mathbb{R}^P$ and $y_i \in \mathbb{R}$. In our application, $\mathbf{x}_i \in \mathbb{R}^3$ is a Euclidean coordinate, and $y_i$ is the surface variation associated with said coordinate. We assume access to noisy observations of an underlying latent function, such that $y_i = f(\mathbf{x}_i) + \epsilon_i$, where $\epsilon_i \sim \mathcal{N}(0, \sigma_y^2)$. A GP defines a distribution over functions which we can use to infer the form of the true latent function which generated our training data. The GP prior can be written as:
\begin{equation}
f \sim \mathcal{GP}\left(\mu\left(\mathbf{x}\right), k\left(\mathbf{x}, \mathbf{x}^\prime\right)\right) ,
\end{equation}
where, $\mu(\cdot)$ and $k(\cdot)$ are the mean function and the kernel function respectively, which completely describe our process \cite{rasmussen2006gaussian}. As is common, we assume a zero-mean prior throughout this work, using the kernel as the primary means of modeling the variation in our function over the input domain. A popular choice for GP kernels is the Mat\'ern class of covariance function, which takes the following form:
\begin{align}
    k_\nu(\mathbf{x}, \mathbf{x}^\prime) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{r\sqrt{2\nu}}{\kappa} \right)^\nu K_\nu \left(\frac{r\sqrt{2\nu}}{\kappa} \right),
    \label{eq:matern_kernel}
\end{align}
where $r=\lVert \mathbf{x}-\mathbf{x}^\prime \rVert$ and $K_\nu$ is a modified Bessel function. We define $\boldsymbol{\theta} = \{ \sigma^2, \kappa, \nu\}$ to be the set of kernel hyperparameters; $\sigma^2$ controls the variance of the GP, $\kappa$ the lengthscale of its variation and $\nu$ its degree of differentiability.

\paragraph{Inference:} Using Bayes' Rule, we can condition our GP on the training data and derive closed form expressions for the posterior mean and covariance:
\begin{align}
\label{eq:post_mu}
\boldsymbol{\mu}_{\text{post}} &= \mathbf{K}_{*}(\mathbf{K} + \sigma_y^2 \mathbf{I})^{-1} \mathbf{y}, \\
\label{eq:post_sigma}
\boldsymbol{\Sigma}_{\text{post}} &= \mathbf{K}_{**} - \mathbf{K}_* (\mathbf{K} + \sigma_y^2 \mathbf{I})^{-1} \mathbf{K}_*^\top .
\end{align}
Generally, the noise variance $\sigma_y^2$ and the kernel function hyperparameters $\boldsymbol{\theta}$ are optimised via maximisation of the log-marginal likelihood, which can also be derived analytically. Where $\mathbf{X} \in \mathbb{R}^{N \times P}$ and $\mathbf{y} \in \mathbb{R}^{N}$ are matrix and vectorial representations of our training inputs and targets respectively, the log-marginal likelihood takes the form \cite{rasmussen2006gaussian}, 
\begin{align} \label{eq:ll}
\begin{split}
    \log p\left(\mathbf{y} \mid \mathbf{X}, \boldsymbol{\theta}, \sigma_y^2\right) &= -\frac{1}{2} \mathbf{y}^\top \left(\mathbf{K} + \sigma_y^2\mathbf{I} \right)^{-1} \\ &\quad - \frac{1}{2} \log \mid \mathbf{K} + \sigma_y^2\mathbf{I} \mid - \frac{N}{2} \log (2\pi).
\end{split}
\end{align}

\paragraph{Kernels on manifolds:} Many different kernel functions for GPs exist, and choosing a kernel is in itself a model selection problem as some kernels are more suited to modeling certain types of data. However, one characteristic which many kernels share is that they are defined using Euclidean distance. This presents an issue should we wish to use a GP to model variation in a quantity over a non-Euclidean space. Borovitskiy \textit{et al.} \cite{borovitskiy2020matern} proposed a solution to this problem in the form of an extension to the Mat\'ern kernel, which allows for modeling of functions whose domains are compact Riemannian manifolds. The approach proposed by the authors involves two stages. Firstly, numerical estimation of the eigenvalues $\lambda_n$ and eigenfunctions $f_n$ corresponding to the Laplace-Beltrami operator of the given manifold is performed. Secondly, for a manifold of dimensionality $d$, the kernel is approximated using a finite truncation of:
\begin{equation} \label{eq:kernel}
    k_{\nu}(\mathbf{x}, \mathbf{x}^\prime) = \frac{\sigma^2}{C_{\nu}} \sum_{n=0}^{\infty} \left(\frac{2\nu}{\kappa^2} + \lambda_n \right)^{-\nu - \frac{d}{2}} f_n(\mathbf{x}) f_n(\mathbf{x}^\prime) ,
\end{equation}
where, $C_{\nu}$ is a normalizing constant. Parameters $\sigma^2$, $\kappa$ and $\nu$ have similar interpretations to those for the Euclidean Mat\'ern kernel shown in Equation \eqref{eq:matern_kernel}.
%
\subsection{Greedy subset-of-data algorithm}
\label{sec:sod}
A major challenge which arises when working with GPs in practice is the $\mathcal{O}(N^3)$ complexity associated with performing exact inference, which arises due to the matrix inversions in Equations \eqref{eq:post_mu} and \eqref{eq:post_sigma}. To circumvent this issue, numerous formulations of \textit{sparse GPs} have been proposed, many of which are based on approximate inference techniques and concepts such as \textit{inducing points} \cite{liu2020gaussian}. In this work however, we consider the \textit{subset-of-data} (SoD) approach, a conceptually simple form of sparse approximation which allows for exact Bayesian inference. In this setting, rather than modifying the formulation of the GP itself, we simply perform exact inference using a carefully selected subset of $M (<< N)$ observations. Specifically, for our case we modify the greedy SoD approach of \cite{lalchand2018fast}, which uses a selection criterion to sequentially construct a subset of size $M$ which is representative of our full training set of $N$ observations. We use this technique for GP sparsification in order to construct a set of inducing points for a point cloud which are best capable of representing the changes in surface variation over the cloud; this set of points forms our simplified point cloud. The original method involves randomly selecting one initial inducing point and then adding one point to the set at each iteration \cite{lalchand2018fast}, however we have employed \textit{farthest point sampling} (FPS) for selecting a set of initial inducing points and instead of one, and we add several points to our set of inducing points at each iteration. Our approach is explained in further detail in Section \ref{sec:methodology}.

Our method forms a simplified point cloud which is a subset of the original, thus the optimization problem is a discrete one. There has been recent work on inducing point optimization on discrete domains \cite{fortuin2021sparse}, however such methods only obtain comparable performance to methods based on greedy selection of the inducing points from the input domain, which are considerably conceptually simpler. The main disadvantage of a greedy approach is that the training set does not necessarily span the whole input domain, however in our setting this is indeed the case, making our application especially well suited to a greedy approach. Additionally, our proposed method allows us to obtain competitive results for clouds containing millions of points, whilst still employing exact Bayesian inference rather than approximate variational schemes, which can often underestimate the variance of the posterior distribution \cite{blei2017variational}.
%
%-------------------------------------------------------------------------
\section{Point cloud simplification with Riemannian Gaussian processes} 
\label{sec:methodology}
In this section, we outline our GP-based approach with the help of a concise algorithm. We can represent a point cloud of size $N$ as a set of 3D Euclidean coordinates $P = \{\mathbf{x}_i\}^N_{i=1}$, where $\mathbf{x}_i \in \mathbb{R}^{3}$. The surface variation $y_i \in \mathbb{R}$ at each point in $P$ can be computed using the technique presented in Section \ref{sec:surface}. Using this data we formulate a regression problem, whereby we employ a Gaussian process with a Mat\'ern kernel defined on a Riemannian manifold (as described in Section \ref{sec:riemannian_gp}) to predict the surface variation from the coordinates of each point. We then employ the greedy subset-of-data scheme discussed in Section \ref{sec:sod} in order to obtain a simplified set of $M (<< N)$ 3D coordinates, $P_{\text{simp}} = \{\mathbf{x}_j\}_{j=1}^M$, where $P_{\text{simp}} \subset P$.

We formally outline our proposed approach in Algorithm \ref{alg:simpli}.
$\text{FPS}(P, k_\text{init})$ denotes a function which selects $k_\text{init}$ initial points from $P$ using FPS; we use this to initialise our active set $P_{\text{simp}}$ with an initial set of points from across the point cloud. $\text{MAX}(\mathbf{s}, R, k_\text{add})$ selects the points from the remainder set $R$ which are associated with the $k_\text{add}$ largest values in our selection criterion vector $\mathbf{s}$. The notation $\mathbf{y}(R)$ denotes a vector containing the target surface variation values associated with each of the points contained within the set $R$. At each step $t$ of the algorithm, we update the posterior mean $\boldsymbol{\mu}_t$ and covariance $\boldsymbol{\Sigma}_t$ using Equations \eqref{eq:post_mu} and \eqref{eq:post_sigma} respectively, where the active set $P_{\text{simp}}$ is used as training data, whilst the remainder set $R$ is unseen test data.

\begin{algorithm}
\caption{GP-based simplification algorithm}\label{alg:simpli}
\begin{algorithmic}
\State \textbf{Data:} $P$, $\mathbf{y}$, $M$, $k_\text{init}$, $k_\text{add}$,  $k_\text{opt}$, GP prior $\mathcal{GP}(0, k(\cdot, \cdot))$, where $k$ is defined in Eq. \eqref{eq:kernel}
\State \textbf{Result:} $P_{\text{simp}}$
\State $P_{\text{opt}} \leftarrow$ random subset of  $k_\text{opt}$ points from $P$;
\State Optimise GP hyperparameters using Eq. \eqref{eq:ll}, $P_{\text{opt}}$ and $\mathbf{y}(P_{\text{opt}})$;
\State Active set $P_{\text{simp}} \leftarrow \text{FPS}(P, k_\text{init})$;

\State Remainder set $R \leftarrow P - P_{\text{simp}}$;
\While{$\lvert P_{\text{simp}} \rvert < M$}
\State Compute $\boldsymbol{\mu}_t$ and $\boldsymbol{\Sigma}_t$ using Eq. \eqref{eq:post_mu} and Eq. \eqref{eq:post_sigma};
    
\State $\mathbf{s} \leftarrow \sqrt{\text{diag}(\boldsymbol{\Sigma}_t)}+\lvert \boldsymbol{\mu}_t - \mathbf{y}(R) \rvert$;

\State $P_{\text{simp}} \leftarrow P_{\text{simp}} + \text{MAX}(\mathbf{s}, R, k_\text{add})$;
\State $R \leftarrow R - \text{MAX}(\mathbf{s}, R, k_\text{add})$;
\EndWhile
\end{algorithmic}
\end{algorithm}

To clarify, we predict the surface variation and the uncertainty values for $R$ based on $P_\text{simp}$ at each iteration of our algorithm. The selection criterion which we use favours selection of points within the original cloud which lie in regions of high predictive uncertainty and/or error. By selecting a set of points using this criterion, we form a simplified cloud which implicitly favours selection of points surrounding finer details within the cloud, where the error and uncertainty is likely to be high if we have not yet selected a sufficient number of points around said location.

As $P_\text{simp}$ grows with each iteration to be gradually more representative of our input data, the uncertainty and predicted surface variation values for points in $R$ also change. For example, consider two neighbouring points on the tip of one of the Stanford bunnyâ€™s ears, and assume that neither of them are currently in $P_\text{simp}$. If one of these points is added to $P_\text{simp}$, the elements of the uncertainty $\sqrt{diag(\boldsymbol{\Sigma_t})}$ and error $|\boldsymbol{\mu}_t - \mathbf{y}(R)|$ associated with the second point will decrease, and in subsequent iterations it may no longer be one of the top-ranked points based on the selection metric $\mathbf{s}$. 
%-------------------------------------------------------------------------
\section{Empirical evaluation}
\label{sec:experiments}
\begin{figure}
     \centering
        \includegraphics[scale=0.58, trim=0cm 10.3cm 18.5cm 0cm, clip]{images/originals.pdf}
         \caption{The original Stanford meshes corresponding to the three point cloud datasets evaluated.}
         \vspace{-0.4cm}
         \label{fig:original}
\end{figure}
%
\begin{figure*}[!ht]
     \centering
        \includegraphics[trim=0cm 2.5cm 0cm 5cm, clip, width=\linewidth]{images/dragon.pdf}
         \caption{Simplified representations of the Dragon point cloud for $\alpha=0.03$ (top row) and associated reconstructed meshes (bottom row) for all evaluated simplification techniques.}
         \label{fig:dragon_all}

\end{figure*}
%
\begin{figure*}[!ht]
     \centering
        \includegraphics[trim=0cm 0.5cm 0cm 0cm, clip,width=\linewidth]{images/armadillo.pdf}
         \caption{Simplified representations of the Armadillo point cloud for $\alpha=0.05$ (top row) and associated reconstructed meshes (bottom row) for all evaluated simplification techniques.}
        \label{fig:armadillo_all}
        \vspace{-0.4cm}
\end{figure*}
%
\begin{figure*}[t]
     \centering
        \includegraphics[trim=0cm 3cm 0cm 3cm, clip, width=\linewidth]{images/lucy.pdf}
         \caption{Surface reconstruction results of the simplified version of the Lucy point cloud for $\alpha=0.002$ for all evaluated simplification techniques except PC-Simp.}
         \label{fig:lucy_all}
         \vspace{-0.4cm}
\end{figure*}
% trim= l d r u
%
\subsection{Experimental details}

\paragraph{Evaluation criteria:}
\label{sec:eval_criteria}
In order to evaluate the performance of the methods tested, we firstly use each simplified point cloud to form a simplified mesh, using \textit{screened Poisson surface reconstruction} \cite{kazhdan2013screened}. We can then compute the reconstruction errors between the original meshes, and the reconstructed meshes formed from our simplified clouds. Specifically, we choose to evaluate the mean and maximum \textit{Hausdorff distance} \cite{cignoni1998metro}. Evaluating the error associated with mesh reconstruction is effective at quantifying the ability of each method to preserve features from the original cloud, as accurate reconstruction of a mesh from a simplified point cloud requires that a high density of points be placed in the vicinity of finer details within the cloud. The \textit{MeshLab} software \cite{meshlab} was used to reconstruct all surfaces and compute the Hausdorff distances. Also, given that one of our primary aims is to preserve sharp features within each point cloud, we also report the \textit{average surface variation} over each simplified point cloud. The surface variation at each point is computed using the approach described in Section \ref{sec:surface}.
%
\paragraph{Baselines:}
We use the aforementioned evaluation procedure to compare our method (denoted \textit{GP}) empirically to a number of competing point cloud simplification techniques discussed in Section \ref{sec:rw}. We compare our proposed approach to \textit{PC-Simp}, \textit{AIVS}, \textit{HC} and \textit{WLOP}, with the latter two approaches implemented using the CGAL library \cite{cgal:eb-23a}. For the HC method, the size-parameter and the variation parameter, discussed in Section \ref{sec:rw}, were both manually tuned to obtain approximate desired simplified sizes. Also, as noted in Section \ref{sec:rw}, we use the non-curvature aware version of the AIVS algorithm, as there is no available open-source implementation of the curvature-aware variant. 

\paragraph{Datasets and hardware:}
We evaluate our proposed method and the aforementioned baselines on three complex point clouds from the Stanford 3D Scanning Repository \cite{levoy2005stanford}, namely \textit{Armadillo} ($N = 1,72,974$), \textit{Dragon} ($N = 4,37,645$) and \textit{Lucy} ($N = 1,40,27,872$). The corresponding original meshes are shown in Figure \ref{fig:original}. Let the \textit{simplification ratio} be defined as $\alpha = M/N$. In this work we focus on the challenging regime where we wish to significantly reduce the size of the point cloud, such that $\alpha << 1$. It is in this regime that feature-preserving techniques such as ours become particularly important, as we do not have a large number of points to select, thus we must efficiently select points which allow us to capture the salient features of the original cloud. We chose $\alpha$ for each dataset by finding the minimum $\alpha$ at which all the evaluated techniques were capable of forming simplified clouds from which meshes visually comparable to the original dataset could be generated. This value varies depending on the surface complexity of each point cloud, thus for \textit{Armadillo}, \textit{Dragon} and \textit{Lucy} we chose $\alpha = 0.05, 0.03$ and $0.002$ respectively.

Our approach practically has no user-specified parameters, as it generates a simplified cloud based on the desired simplification ratio ($\alpha$). However, we supply to our algorithm (\ref{alg:simpli}) some fixed default values which work well for most point clouds. We set $k_{\text{opt}} = 200$ for all experiments as even with a small subset of the original point cloud, the GP typically converges on an optimal set of hyperparameters within 100 iterations. $k_{\text{init}}$ was chosen to be $1/3$ of the target number of points in the simplified cloud, a ratio which empirically works well across all datasets tested. Finally, $k_{\text{add}}$ is determined adaptively based on $k_{\text{init}}$, $N$ and $M$. Our algorithm is implemented in the PyTorch framework \cite{paszke2019pytorch}, and whilst the runtimes reported in Table \ref{tab:timings} were achieved with GPU acceleration using an NVIDIA A100 with 80GB of RAM, our algorithm can also be run purely on a CPU. All baselines were run on an Intel i7-11800H CPU with 32GB RAM. Our code is included in the supplementary material, and will be released as open source upon publication.
%
\begin{table*}
    \centering
    \begin{tabular}{cccccccccc}
    \toprule
    {} & \multicolumn{3}{c}{Mean Hausdorff Distance ($\downarrow$)} & \multicolumn{3}{c}{Max. Hausdorff Distance ($\downarrow$)} & \multicolumn{3}{c}{Mean Surface Variation ($\uparrow$)}  \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
    {} & Armadillo & Dragon & Lucy & Armadillo & Dragon & Lucy & Armadillo & Dragon & Lucy \\
    \midrule
     GP (ours) & 0.246 & \underline{0.000246} & \textbf{1.11} & \textbf{3.26} & \underline{0.00457} & 195.78 & \underline{0.0728} & \underline{0.0546} & \underline{0.0724} \\
     HC & 0.374 & 0.000758 & \underline{1.14} & \textbf{3.26} & 0.0141 & \textbf{195.41} & \textbf{0.0803} & \textbf{0.0686} & \textbf{0.0762} \\
     WLOP & \textbf{0.197} & \textbf{0.000188} & 1.287 & 4.14 & \textbf{0.00417} & \underline{195.52} & 0.0557 & 0.0413 & 0.0631 \\
     PC-Simp & \underline{0.241} & 0.000487 & - & 5.48 & 0.00802 & - & 0.0364 & 0.0433 & - \\
     AIVS & 0.715 & 0.000638 & 8.75 & 4.11 & 0.00539 & 196.45 & 0.0513 & 0.0441 & 0.0666 \\
    \bottomrule
    \\
    \end{tabular}
    \vspace{-0.3cm}
    \caption{Empirical results for all tested simplification methods and datasets. We report the maximum and mean Hausdorff distances between the original meshes, and the meshes reconstructed from the simplified point clouds. Also reported is the average surface variation over each simplified point cloud. Best results are boldfaced, whilst second-best are underlined.}
    \label{tab:results}
    \vspace{-0.2cm}
\end{table*}
%
\begin{table}
    \centering
    \begin{tabular}{cccc}
    \toprule
    {} & \multicolumn{3}{c}{Time (s) ($\downarrow$)}  \\
    \cmidrule(lr){2-4}
    {} & Armadillo & Dragon & Lucy \\
    \midrule
     GP (ours) & \underline{0.809} & \underline{1.362} & \underline{12.86} \\
     HC & \textbf{0.144} & \textbf{1.078} & \textbf{9.992} \\
     WLOP & 3.536 & 6.501 & 84.158 \\
     PC-Simp & 132.586 & 245.498 & -\\
     AIVS & 17.236 & 44.570 & 1983.485 \\
    \bottomrule
    \\
    \end{tabular}
    \vspace{-0.3cm}
    \caption{Total runtimes for all tested simplification methods and datasets. Best times are boldfaced, whilst second-best are underlined.}
    \label{tab:timings}
    \vspace{-0.4cm}
\end{table}
%
\subsection{Discussion}
From the results presented in Table \ref{tab:results}, it is clear that our proposed method is capable of comparable empirical performance to many of the existing methods for simplifying point clouds. The GP-based approach outperforms the AIVS baseline across all experiments and performance metrics, and outperforms the PC-Simp baseline on all but the mean Hausdorff distance for the Armadillo experiment. As shown in Table \ref{tab:timings}, our algorithm also runs considerably faster than both of these approaches. Note that due to the scale of \textit{Lucy}, we were unable to evaluate PC-Simp on this dataset as it was taking more than two hours to run. 

HC is the only baseline tested which has a shorter runtime than our method, and obtains maximum Hausdorff distances comparable to those obtained by our approach. However, as discussed in Section \ref{sec:rw}, tuning the user-specified HC parameters can make striking a balance between feature preservation and retaining a sufficient density of points across the cloud relatively challenging. We tuned this baseline to attempt to balance this trade-off, and whilst the HC simplified clouds shown in Figures \ref{fig:armadillo_all} and \ref{fig:dragon_all} do have clearly preserved features (an observation supported by the high mean surface variation across all datasets), the density of points away from these areas is very low. This leads to inferior mesh reconstructions compared to our approach, as evidenced by the fact that we obtain superior mean Hausdorff distance compared to HC across all three datasets.

The WLOP baseline does not efficiently preserve the features and favours uniformly covering the domain of the original cloud. Therefore, the mean surface variation of the WLOP simplified clouds is lower, but overall the Hausdorff distances obtained from the reconstructed meshes are superior to those obtained by our method. However, it is noteworthy that on the largest and unarguably the most challenging dataset, \textit{Lucy}, our method achieves a superior mean Hausdorff distance as compared to all of the other techniques evaluated, including WLOP. Additionally, WLOP is significantly slower than our approach, as shown in Table \ref{tab:timings}. Our surface variation computation is currently performed on a CPU, therefore further improvements to the runtimes of our method shown in Table \ref{tab:timings} could be achieved by re-implementing this in a GPU-compatible framework.

Overall, these results show that our approach provides a computationally efficient option for performing point cloud simplification in settings where the user wishes to strike a balance between preserving high levels of fidelity around sharp features in the cloud, and ensuring that the simplified cloud covers the manifold defined by the original cloud with a sufficient density of points. This is extremely important for generating reconstructions which resemble the original dataset (as discussed in Section \ref{intro}), evident from the visual inspection of the reconstruction results shown in Figures \ref{fig:dragon_all}, \ref{fig:armadillo_all} and \ref{fig:lucy_all}. In terms of surface reconstruction results, our method clearly outperforms all of the other techniques for the \textit{Dragon} (compare the tail, teeth, horns and the face detailing for all methods and additionally the curved body for HC) and the \textit{Armadillo} (compare the ears, hands and feet across all the methods) and gives competitive results for \textit{Lucy}.

The $\mathcal{O}(M^3)$ and $\mathcal{O}(M^2 N)$ complexities associated with training and prediction respectively in the greedy inference scheme described in Section \ref{sec:sod} allow for increased scalability compared to typical GP regression, in which inference has $\mathcal{O}(N^3)$ complexity. The scalability of our approach is limited by the fact that, as in a conventional exact GP, we have a storage demand associated with $\mathbf{K}$ matrix which scales according to $\mathcal{O}(N^2)$. However, we can circumvent this issue when $N$ is very large by simply using Algorithm \ref{alg:simpli} with a randomly selected subset of $P$. For \textit{Armadillo} and \textit{Dragon} we obtain the above results with just 25,000 randomly selected points. For a large point cloud such as \textit{Lucy}, we obtain competitive results using a subset of just 40,000 points to run our simplification algorithm.  

%-------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conc}
In this work we have presented a novel, one-shot point cloud simplification algorithm which is capable of preserving both the salient features and the overall structure of the original cloud. We reduce the cloud size by multiple orders of magnitude without the need for computationally intensive training, which is generally associated with any machine learning-based method, over huge datasets. This is achieved by using a greedy algorithm which iteratively selects points based on a selection criterion determined by modeling the surface variation over the original point cloud using Gaussian processes with kernels which operate on Riemannian manifolds. We show that our technique achieves competitive results and runtimes when compared to a number of relevant methods, on point clouds with up to 14 million points.

\paragraph{Future work:} 
Although we have used the benchmark Hausdorff distance as a metric, it is not the ideal candidate for assessing the feature sensitivity of a simplification algorithm, as it tends to return lower errors for more evenly distributed clouds. Whilst out of the scope of this work, there is a clear need for a well-defined and widely adopted error metric for curvature-sensitive simplification, as arguably, currently the best way to evaluate this is a qualitative visual inspection of the resulting point cloud (or the reconstructed mesh). This viewpoint is supported by the fact that some recent works include user studies in order to assess the effectiveness of their feature-preserving approaches \cite{potamias2022revisiting}.

In this work we study the setting where we enforce the restriction that the simplified cloud be a subset of the original; as discussed in Section \ref{sec:sod}, a greedy inference scheme is appropriate in this setting. However, this assumption could be relaxed and sparse GPs can be used to perform continuous optimization of the inducing points across the point cloud \cite{hutchinson2021vector}. This would also allow noisy point clouds, where the original observations do not necessarily lie on the true surface of the manifold, to be denoised and/or simplified.

\section*{Acknowledgments}
Stuti Pathak would like to thank the University of Antwerp for funding the research work. Thomas M. McDonald thanks the Department of Computer Science at the University of Manchester for their financial support. The authors would like to acknowledge the assistance given by Research IT and the use of the Computational Shared Facility at The University of Manchester.

{\small
\bibliographystyle{abbrv}
\bibliography{mainTemplatePDF}
}

\end{document}
