% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{algorithm2e}
\usepackage{booktabs}
\usepackage{float}  %设置图片浮动位置的宏包
% \usepackage{subfigure}  %插入多图时用子图显示的宏包
\usepackage{multirow}
\usepackage{caption}
\usepackage{soul}
\usepackage{dsfont}
\usepackage{enumitem}

\usepackage{color}
\usepackage{amsthm}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% \usepackage{colortbl}
\newcommand{\rongyu}[1]{\textcolor{magenta}{#1}}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{257} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\makeatletter
\newbox\abstract@box
\renewenvironment{abstract}
  {\global\setbox\abstract@box=\vbox\bgroup
     \hsize=\textwidth\linewidth=\textwidth
    \small
    \begin{center}%
    {\bfseries \abstractname\vspace{-.5em}\vspace{\z@}}%
    \end{center}%
    \quotation}
  {\endquotation\egroup}
\expandafter\def\expandafter\@maketitle\expandafter{\@maketitle
  \ifvoid\abstract@box\else\unvbox\abstract@box\if@twocolumn\vskip1.5em\fi\fi}
\makeatother

\begin{document}
\title{Unimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation}
\author{Rongyu Zhang\textsuperscript{\rm 1,4}\thanks{Equal contribution: rongyuzhang@link.cuhk.edu.cn}, 
Xiaowei Chi\textsuperscript{\rm 2*},
Guiliang Liu\textsuperscript{\rm 1},
Wenyi Zhang\textsuperscript{\rm 3},\\
Yuan Du\textsuperscript{\rm 4},
Fangxin Wang\textsuperscript{\rm 1}\thanks{Corresponding author: wangfangxin@cuhk.edu.cn}\\
\textsuperscript{\rm 1}The Chinese University of Hong Kong, Shenzhen, \textsuperscript{\rm 2}The Chinese University of Hong Kong, \\
\textsuperscript{\rm 3}University of California, Irvine,
\textsuperscript{\rm 4}Nanjing University\\ 
}
%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Exploring M for Adaptive Multi-view 3D Object Detection}
% \title{Domain Adaptive Multi-view 3D Object Detection via Multi-space Feature Alignment}
%\title{BEVUDA: Unsupervised Domain Adaptive for Multi-view 3D Object Detection in Bird-Eye-View}
% \title{BEV-$M^2$ATS: A Multi-level Multi-space Alignment Teacher-Student Framework for BEV Domain Adaptation}

% \maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
%First version
% Camera-based 3D object detectors in Bird-Eye-View (BEV) have shown promising performance and attracted increasing attention in autonomous driving (e.g. BEVDepth). However, its domain adaptation setting and cross-domain ability is still unexplored and unclear.
% By discovering the obvious domain gap that lies in scene, weather, and day-night adaptation, we make the first attempt to design three unsupervised domain adaptation scenarios.
% Moreover, we propose a novel Multi-space Alignment teacher-student ($M^{2}ATS$) framework, an adaptation paradigm specially designed for BEV 3D detection, which fully explores the target domain unlabeled data.
% In particular, $M^{2}ATS$ consists of a Depth-aware mean-teacher (DAMT), in which the teacher model better compensates image features with composite depth information (ground-truth depth and adaptive selected depth prediction) to construct BEV feature in target domain. Meanwhile, the teacher model transfers image-pixel knowledge to student model via reliable pseudo labels and Depth-aware BEV features.
% Note that, the domain shift can also be alleviated without target domain depth information. Along with DAMT, $M^{2}ATS$ contains a Multi-space Feature Alignment method (MSFA) to bridge the domain gap and synchronously help DAMT to generate more reliable pseudo labels, by aligning the task relevant BEV, point cloud, camera parameter, and image space feature. To verify the effectiveness of $M^{2}ATS$, we conduct extensive experiments on three domain adaptation scenarios and achieve state-of-the-art performance, especially the result of sunny to rainy weather scenario is remarkably improved by xxxx MAP and xxxx NDS. Code will be released.

%before 10.28 version 2
% Camera-based 3D object detectors in Bird-Eye-View (BEV) have shown promising performance and attracted increasing attention in autonomous driving. However, the domain adaption problem of existing methods is still unexplored. By discovering the domain gap between scene, weather, and day-night, we make the first attempt to solve the domain adaption problem for multi-view 3D object detection.
% We propose a novel Multi-space Alignment Teacher-Student ($M^{2}ATS$) framework to fully explore the unlabeled data of target domain.
% In particular, $M^{2}ATS$ consists of a Depth-Aware Teacher (DAT) model that uses composite depth information (both ground truth depth and adaptive selected prediction) to construct BEV features from multi-view image features in the target domain. The teacher model then transfers the image-pixel knowledge to student model via reliable pseudo labels and the constructed BEV feature. Meanwhile, the domain shift can also be alleviated without the depth ground-truth in target domain. To further bridge the domain gap in different feature spaces and help DAT to generate more reliable pseudo labels, our framework contains a Multi-space Feature Alignment method (MFA) that aligns the task-relevant BEV, point-cloud, camera parameters, and image features. To verify the effectiveness of $M^{2}ATS$, we conduct extensive experiments on three domain adaptation scenarios and achieve state-of-the-art performance. For example, the result of sunny to rainy is remarkably improved by XXXX MAP and XXXX NDS. Code and dataset will be released.

%ljm re-write(10.28) version3
%background
% Vision-Centric Bird-Eye-View (BEV) perception has shown promising potential and attracted increasing attention in autonomous driving.
% %motivation
% Recent works mainly focus on designing a high-performance framework but neglect the domain shift problem, resulting in performance degradation.
% With extensive empirical observations, we figure out the significant domain gaps existed in the scenes, weather, and day-night change scenarios, we make the first attempt to solve the domain adaption problem for multi-view 3D object detection. 
% %limitation and challenge in BEV UDA.
% The main challenge lies on the accumulation of domain shift on multi-latent spaces during BEV perception. 
% %our proposed method 
% In this paper, we propose a novel Multi-space Alignment Teacher-Student ($M^{2}ATS$) framework to bridge the domain gaps in multi-level.
% %part a. via transfers multi-spaces knowledge
% In particular, $M^{2}ATS$ consists of a Depth-Aware Teacher (DAT) model that uses composite depth-aware information to better construct BEV features in the target domain. DAT then transfers multi-spaces knowledge and pseudo label to student model, which addressing multi-latent spaces domain shift in pixel and instance level respectively.
% %part b. via Multi-space Feature Alignment
% To further ease the domain shift in global level, our framework contains a Multi-space Feature Alignment method (MFA) in student model that aligns the task-relevant BEV, 3D Voxel, and 2D image features. 
% %SOTA
% To verify the effectiveness of $M^{2}ATS$, we conduct extensive experiments on four domain adaptation scenarios and achieve state-of-the-art performance.
% For instance, the result of sunny to rainy is remarkably improved by 3.5 NDS and 2.7 mAP.
% Code and dataset will be released.
% Vision-Centric Bird-Eye-View (BEV) perception has shown promising potential and attracted increasing attention in autonomous driving. Recent works mainly focus on designing a high-performance framework but neglect the domain shift problem, resulting in a degradation of transfer performance. With extensive empirical observations, we figure out the significant domain gaps existed in the scenes, weather, and day-night change scenarios, we make the first attempt to solve the domain adaption problem for multi-view 3D object detection. The main challenge lies on the accumulation of domain shift on multi-latent spaces during BEV perception. In this paper, we propose a novel Multi-level Multi-space Alignment Teacher-Student ($M^{2}ATS$) framework to bridge the domain shift of multiple feature spaces. In particular, $M^{2}ATS$ consists of a Depth-Aware Teacher (DAT) model that uses composite depth-aware information to construct domain invariant voxel and BEV features in target domain. DAT then transfers multi-spaces knowledge and pseudo label to student model, which addressing domain shift in pixel and instance level respectively. To further ease the domain shift in global level, our framework contains a Multi-space Feature Alignment method (MFA) in student model that aligns the task-relevant BEV, 3D Voxel, and 2D image features. To verify the effectiveness of $M^{2}ATS$, we conduct extensive experiments on four domain adaptation scenarios and achieve state-of-the-art performance (\eg, +12.6\% NDS and +9.1\% mAP on Day-Night). Code and dataset will be released.
%11.05
%Vision-Centric Bird-Eye-View (BEV) perception has shown promising potential and attracted increasing attention in autonomous driving. Recent works mainly focus on designing a high-performance framework but neglect the domain shift problem, resulting in a degradation of transfer performance. With extensive empirical observations, we figure out the significant domain gaps existed in the scenes, weather, and day-night change scenarios and make the first attempt to solve the domain adaption problem for multi-view 3D object detection. The main challenge lies in the accumulation of domain shifts on multi-latent spaces during BEV perception. In this paper, we propose a novel Multi-level Multi-space Alignment Teacher-Student ($M^{2}ATS$) framework to bridge the domain shift on multiple feature spaces. In particular, $M^{2}ATS$ consists of a Depth-Aware Teacher (DAT) model that uses depth-aware information to better construct voxel and BEV features in the target domain. DAT then transfers pixel and instance-level domain invariant knowledge to the student model. To further alleviate the domain shift at the global level, the $M^{2}ATS$ framework introduces a Multi-space Feature Aligned (MFA) student model that aligns the task-relevant multi-space features. To verify the effectiveness of $M^{2}ATS$, we conduct extensive experiments on four domain adaptation scenarios and achieve state-of-the-art performance (\eg, +12.6\% NDS and +9.1\% mAP on Day-Night). Code and dataset will be released.
%11.07
Multimodal learning has seen great success mining data features from multiple modalities with remarkable model performance improvement. Meanwhile, federated learning (FL) addresses the data sharing problem, enabling privacy-preserved collaborative training to provide sufficient precious data. Great potential, therefore, arises with the confluence of them, known as multimodal federated learning. However, limitation lies in the predominant approaches as they often assume that each local dataset records samples from all modalities. In this paper, we aim to bridge this gap by proposing an \texttt{Unimodal Training - Multimodal Prediction} (UTMP) framework under the context of multimodal federated learning. 
We design \texttt{HA-Fedformer}, a novel transformer-based model that empowers unimodal training with only a unimodal dataset at the client and multimodal testing by aggregating multiple clients' knowledge for better accuracy. The key advantages are twofold. Firstly, to alleviate the impact of data non-IID, we develop an uncertainty-aware aggregation method for the local encoders with layer-wise Markov Chain Monte Carlo sampling. Secondly, to overcome the challenge of unaligned language sequence, we implement a cross-modal decoder aggregation to capture the hidden signal correlation between decoders trained by data from different modalities. Our experiments on popular sentiment analysis benchmarks, CMU-MOSI and CMU-MOSEI, demonstrate that \texttt{HA-Fedformer} significantly outperforms state-of-the-art multimodal models under the UTMP federated learning frameworks, with 15$\%$-20$\%$ improvement on most attributes.
\end{abstract}
\maketitle
%%%%%%%%% BODY TEXT
\section{Introduction}
Human perception of the world usually consists of information with multiple modalities, including sounds, texts, images, etc. 
%The information of different modalities complements each other to help people better understand the world. 
With the continuous development of artificial intelligence, multimodal learning has gradually become the focus of attention. However, although multimodal learning can improve inference accuracy, many users still fail to get satisfactory prediction accuracy due to the limited training data. To surmount the problem of poor model performance caused by insufficient user data, a paradigm of federated learning (FL)~\cite{mcmahan2017communication} with multi-client co-training a global model in a privacy-preserving manner has been applied to multimodal learning.


\begin{figure}[t]
\centering
\includegraphics[scale=0.23]{./images/intro_new.pdf}
\caption{Comparison of (a) Traditional multimodal federated learning taking multimodal data and (b) UTMP taking unimodal data as training input.} 
% \guiliang{nice plot! please update the text as cross-modal \textbf{data fusion at local clients} and cross-modal \textbf{parameters fusion at the server}. }
\vspace{-1em}
\label{clustering}
\end{figure}


% \st{Existing multimodal federated learning focused on training a multimodal model with multiple input modalities.}
Existing multimodal federated learning methods~\cite{liang2020think,chen2022towards} are based on an implicit assumption over the congruity of data modalities across different clients,
%enabling data alignment in local clients. H
however, in practice, many clients only have sensors that can collect unimodal data. 
For example, many IoT devices (e.g., smart speakers) only collect audio data through conversations, and surveillance cameras collect video data due to noisy environments. Since it is generally difficult to determine data ownership (as sensors are indiscriminate in collecting data and data transfer may lead to data leakage), it is infeasible to collect and align the data of different modalities for supporting central training. It requires an FL solution to answer the following question: \textbf{Can a multimodal model be trained via clients with access to only unimodal data as showed in Figure \ref{clustering}?}
Although Chen et al.~\cite{chen2022fedmsplit} proposed FedMSplit that studies the occasional missing modality while training. 
To answer this question, we propose a UTMP framework that puts a step forward: training a multimodal model with only unimodal data in FL.
% \xiaowei{Although some previous works (i.e. Chen et al.~\cite{chen2022fedmsplit} ) have studies the occasional missing modality while training, we propose a UTMP framework that puts a step forward on a more extreme condition, that is, training a multimodal model with only unimodal data in FL.}
% \xiaowei{
% For most of the previous work, they focus on the non-IID data over clients, and there is a common hidden prerequisite that the modality between clients is congruity. In reality, there are many scenarios facing more complex inconsistent problems that sensors are working on different working sets, even different modalities. As data ownership is hard to define, and then a high risk of data transmission, it is hard to collect and align the data of different modalities for training centrally. For a more usual case, data might not always stay highly aligned and integrated. What if the multimodality FL model is training with random data lost? 
% Considering the extreme case, an aforementioned question calls for an FL solution comes up: Can a multi-modal model be trained via clients with limited access to only unimodal data? 
% }
% \st{Intuitively, we may consider participating in federated learning by sharing the models trained with unimodal data and obtaining a multimodal global model for better inference accuracy with multimodal data.}


% Striving for better inference performance, federated learning algorithms must enable knowledge aggregation from these unimodal models at local clients to a global model at the service node. 

In this paper, we formally define the aforementioned scenario as a \texttt{Unimodal Training -Multimodal Prediction} (UTMP) framework. Such a framework allows users with only \textit{unimodal} data to participate in the \textit{multimodal} federated learning, providing a more comprehensive application scope for multimodal learning.
% \st{However, how to use unimodal data as the single modality input for multimodal learning through federated learning has never been explored.} 
% To the best of our knowledge, the cross-modality model or knowledge aggregation has never been explored in federated learning. Our empirical results demonstrate the limited performance of existing federated learning methods under the UTMP framework.
% For example, the SOTA work FedHGB~\cite{chen2022towards} proposed Hierarchical Gradient Blending to overcome the challenge of non-IID data distribution in the context of multimodal federated learning. 
% \st{We discover that none of the existing federated learning methods~\cite{liu2020federated,liang2020think,chen2022towards} can achieve the aforementioned task.} 
However, learning a global multimodal model under the UTMP setting raises new challenges that require a novel solution for several reasons:
% that require a novel solution for several reasons
\textbf{Data non-IID:} our empirical study shows a difficulty that the local models are inclined to {\it overfit the unimodal data distribution} under the UTMP setting, especially for the data with simple representations (e.g., text). 
%Aggregating these local models to a global model for multimodal prediction is difficult. 
\textbf{Data Unalignment:} an essential prerequisite for multimodal learning is creating the data alignment across modalities. 
%For example, when given both a sequence of texts and videos as inputs, the model must be able to create an alignment to illustrate which words and images provide the information for the same object. 
However, directly aligning data under the UTMP setting is impossible since the data cannot be communicated across clients. 
% How to build a model-oriented alignment for federate learning will be an important challenge. 


We foray into uncharted territory to tackle the above-mentioned challenges by proposing Hierarchical Aggregated Multimodal Federated Transformer (\textit{HA-Fedformer}) with a tailored model aggregation strategy.
% \st{
% Our proposed method realizes a so-called \texttt{unimodal training - multimodal prediction}, suggesting that each local client will be trained by a unimodal dataset while users can infer with a multimodal dataset in order to receive higher accuracy. Such a framework enables users who cannot hold precious multimodal data to participate in federated learning, providing a wider application scope for multimodal learning.}
% \guiliang{"nice description,  and I move it to the front."}
% for better inference accuracy, overcoming the data non-IID and multimodal language sequences' unaligned nature inherently caused by the separated local unimodal data. 
Specifically, \textit{HA-Fedformer} consists of a transformer-based encoder for each modality and one shared decoder for cross-modality feature fusion. 
Such structure provides a prerequisite for our UTMP framework. By concatenating different unimodal-trained encoders during model aggregation, we can obtain a multimodal model with full-modal encoders, capable of making predictions with multimodal data for higher accuracy. 
However, due to the two challenges mentioned above, aggregating encoders and decoders directly through traditional FL methods cannot achieve satisfactory results. To this end, we propose a hierarchical aggregation method for encoders and decoders, respectively. 
\textbf{Solution 1: PbEA (Posterior-based Encoder Aggregation)} Uncertainty is a metric measuring if the network \textit{knows what it don't knows}~\cite{gal2016dropout}. We consider the layer-wise uncertainty of each local model and aggregate encoders based on the mean and variance of their posterior inferred with Markov Chain Monte Carlo sampling to alleviate data non-IID problem.
% \xiaowei{Besides, we also proposed another light version based on posterior prediction which is less accurate in comparison, but faster.}
%We also provide an alternative posterior-based method that is both time and computing resources consuming giving competitive benefit\guiliang{?}.
\textbf{Solution 2: CmDA (Cross-modal Decoder Aggregation)} We also implement a cross-modal aggregation method to find signal correlations between different modalities implicit in the model weights of decoders trained on data from different modalities to achieve alignment in model parameter level instead of feature level.
% In the model aggregation stage, the hierarchical aggregation strategy can be separated into two phases. 1) For encoder aggregation, we first aggregate the encoders with the same modality according to the model uncertainty and then \textbf{'exchange'} the aggregated encoders. We proposed a posterior-based uncertainty which can act as a metric measuring if the network \textit{knows what it knows}. In this way, we can obtain a model capable of \texttt{unimodal training - multimodal prediction}. 2) As for decoder aggregation, we proposed an attention-based cross-modal aggregation in order to find the key correlation of the different decoders.
% \guiliang{"This paragraph requires significant expansion. Please add more details."}

% We evaluate \textit{HA-Fedformer} against two widely-used multimodal sentiment analyses datasets: CMU-MOSI and CMU-MOSEI, to prove that our proposed \textit{HA-Fedformer} can overcome the difficulties of data non-IID with only single unimodal data input during local training, yet achieves better performance than parts of the naive multimodal learning SOTA works with multiple input modalities under language sequence unaligned natures.

We evaluate \textit{HA-Fedformer} against two widely-used multimodal sentiment analyses datasets: CMU-MOSI and CMU-MOSEI, to prove that our proposed \textit{HA-Fedformer} can overcome the difficulties of data non-IID and unalignment with only single unimodal data input during local training and inference with sufficient multimodal data for higher inference accuracy. 
The main contributions of this paper can be summarized as the following:
% \setlist[itemize]{leftmargin=*}
\begin{itemize}
\setlength\itemsep{0em}
    \item[-] First, we define \textit{Unimodal Training - Multimodal Prediction} framework in the context of multimodal federated learning, which greatly expands its application scope.

    \item[-] Second, we proposed a hierarchical model aggregation method with Posterior-based Encoder Aggregation and Cross-modal Decoder Aggregation for \textit{HA-Fedformer}, overcoming the data non-IID and sequence unalignment challenges in UTMP.

    \item[-] Third, to our best knowledge, our work Hierarchical Aggregated Multimodal Federated Transformer (\textit{HA-Fedformer}) is the first to achieve UTMP with only unimodal data locally trained. 

\end{itemize}

%-------------------------------------------------------------------------
\section{Related works}
\subsection{Federated learning}
 Federated learning is proposed ~\cite{mcmahan2017communication} to protect user privacy as a critical learning scenario in large-scale applications. 
 % Coordinated by a central server with no access to user data, a Federated learning algorithm trained statistical models from heterogeneous data scattered across multiple entities (or clients)~\cite{kairouz2021advances}. 
 % The key challenge of federated learning is effectively aggregating local models. 
 Many works ~\cite{karimireddy2020scaffold, woodworth2020local, woodworth2020minibatch,li2019convergence,li2021federated} have demonstrated that none-independent and identically distributed (non-IID) data brought by heterogeneous users have a significant adverse effect on the convergence and accuracy of the traditional aggregation strategies~\cite{reddi2020adaptive, khodak2019adaptive}. 
 % Some existing work aims to solve the non-IID problem for federated learning. 
 Ji et al. proposed FedAtt~\cite{wang2020optimizing}, which aggregates model updates from the updates with biased weights in order to train models with higher qualities. 
 Liang et al. ~\cite{liang2020think} jointly optimized mixed global and local models to seek a trade-off between overfitting and generalization.
 Boughorbel et al. ~\cite{boughorbel2019federated} also introduced an uncertainty-aware learning algorithm into federated learning for model aggregation, providing a new perspective for the aggregation of network parameters. 
 
 % However, the above works are only limited to the non-IID problem under the unimodal scenario. For non-IID data from different modalities, the enormous specificity of information feature expression between modalities brings more significant challenges to FL.
 

\subsection{Multimodal learning}

Multi-modality in learning analytics and learning science is under the spotlight, especially for human language processing. 
Since human language contains time series, analyzing human language requires synthesizing and fusing time-varying signals. ~\cite{liang2018multimodal, tsai2018learning}. 
% Many early works proved improved performance compared to learning from a single modality through the fusion of multimodal input features. 
Many advanced models ~\cite{yu2021learning, sun2020learning, wang2019words, pham2019found} have been proposed but are very dependent on the context information in the short term and can only capture the relationship between the various modes on the aligned multimodal data. 
With the introduction of Transformer ~\cite{vaswani2017attention}, many researchers ~\cite{zolfaghari2021crossclr, li2020unimo, nagrani2021attention}have proposed a cross-attention fusion mechanism between modal vectors by borrowing its self-attention mechanism and has proved that cross-attention mechanism has an extended scale for unaligned language sequences. 

% The excellent performance and high accuracy of unaligned language sequence data sets point to a new human multimodal language learning approach.




\subsection{Multimodal federated learning}

Currently, most federated learning frameworks are based on single-modal data classification or recognition tasks, and only a few work on multimodal federated learning tasks. Chen et al. ~\cite{chen2022towards} proposed hierarchical gradient blending (HGB) to alleviate these inconsistencies in collaborative learning. Zhao et al. ~\cite{zhao2022multimodal} proposed a multimodal and semi-supervised federated learning framework that trains auto-encoders to extract shared or correlated representations from different local data modalities on clients. 
Chen et al.~\cite{chen2022fedmsplit} proposed FedMSplit considering a scenario similar to UTMP by constructing a dynamic and multi-view graph structure to adaptively select multimodal client models where some modality may be missing. Yang et al.~\cite{yang2022cross} also considered taking unimodal data in specific Human Activity Recognition FL tasks. However, their methods take unimodal data for testing, which potentially undermines the model's performance.

% However, all current state-of-the-art algorithms must contain training data from all modalities locally preserved, essentially a training framework based on locally pre-trained multimodal models. 
%-------------------------------------------------------------------------

\section{Problem definition}
% In this section, we will first provide the preliminaries of the multimodal federated learning and further define our \textit{Unimodal Training - Multimodal Prediction} (UTMP) framework.

\begin{figure*}[t]
\centering
\includegraphics[scale=0.2]{./images/framework_v1.pdf}
\caption{UTMP framework and the construction of HA-Fedformer. Each transformer-based encoder extracts data features from one modality (L, A, or V), and their training data are unaligned with the non-IID distribution. These clients' models will be aggregated to a multimodal model on the server by PbEA and CmDA. Then, the merged global model will be sent back to each client for local training.}
\vspace{-1em}
\label{framework}
\end{figure*}

\subsection{Preliminaries}
% Federated learning combines the results from different local clients' training. \guiliang{"do we have to emphasize it? reviewers might ask how about extending to other tasks? and they will ask for more experiments."} We consider a classification task, where local dataset ~$\mathcal{D}_{k}:\{\lambda:(\boldsymbol{X},y)\}$ following the unknown data distribution $\Lambda_{k}$. \guiliang{"what is $\lambda$"?} $\mathit{k}\in\{1,2,3,...,N\}$ indicating $\mathit{k}^{th}$ device. The local model parameter $\theta_{k}^{t}$ can be obtained by stochastic gradient descent(SGD) performed locally in communication round $t$, which is:


Federated learning aims at training a global model by utilizing the dataset stored at local clients. Each client stores a local dataset $\mathcal{D}_{k}$ where 1) $k\in\{1,\dots,K\}$ denotes the $k^{th}$ client and 2) the inputs $\boldsymbol{X}$ and their labels $\boldsymbol{y}$ are sampled from a local data distribution $(\mathcal{X},\mathcal{Y})^{k}$. At communication round $t$, the local model parameter $\boldsymbol{\theta}_{k}^{t}$ can be updated by stochastic gradient descent(SGD):


\begin{equation}
\begin{aligned}
\boldsymbol{\theta}_{k}^{t+1} &= \boldsymbol{\theta}^{t}_{k} - \varphi\nabla\ell_{k}(\upsilon(\boldsymbol{X};\boldsymbol{\theta}^{t}_{k}),\boldsymbol{y}) 
\end{aligned}
\end{equation}
where $\upsilon$ denotes a model parameterized by $\boldsymbol{\theta}_{k}^{t}$, $\varphi$ is the learning rate, and $\ell(\cdot)$ is a user-specific loss function (e.g., Mean Square Error loss). Thus, the final optimization problem of FL can be formulated as follows:

\begin{equation}
\min\limits_{\boldsymbol{\theta}}\Big\{F(\boldsymbol{\theta}) = \sum_{k=1}^{N}\alpha_{k}f_{k}(\boldsymbol{\theta})\Big\}
\end{equation}
% \guiliang{"where is the weight in this formula? Are they equally weighted?"}
where $\alpha_{k}$ is the weight for client $k$. We assume the objective function $f_{k}$ is \textit{convex} and \textit{L-smooth}~\cite{chen2022towards}.

\subsubsection{Vanilla multimodal federated
learning} 

% \guiliang{I have rewritten your problem definition. Please check whether the math will work here.}

Traditional multimodal FL algorithms assume a client has access to the data from all modalities.
Let $\rho_{k}(\boldsymbol{X},\boldsymbol{y})$ defines the density of a data point in dataset $\mathcal{D}_{k}$ ($k\in[1,K]$ denotes client number.). Since $\mathcal{D}_{k}$ stores data from all modality, $\rho_{k}(\boldsymbol{X},\boldsymbol{y})=\sum_{m=1}^{M}p_{k}(m)\rho_{m}(\boldsymbol{X},\boldsymbol{y})$ where $\rho_{m}(\boldsymbol{X},\boldsymbol{y})$ denotes the density of dataset for the $m^{th}$ modality and $p_{k}(m)\in[0,1]$ is a mixing coefficient that merges unimodal densities to a client density.
Under this setting, these algorithms can learn a {\it local multimodal models} at each client with the objective:

\begin{equation}
    f_{k}=\frac{1}{|D_{k}|}\sum_{(\boldsymbol{X},\boldsymbol{y})\in D_{k}}\oplus_{m=1}^{M}\mathds{1}_{(\boldsymbol{X},\boldsymbol{y})\in\mathcal{D}_{m}}\ell\left[\upsilon_{m,k}(\boldsymbol{X};\boldsymbol{\theta}_{k}),\boldsymbol{y}\right]
\end{equation}
% \begin{equation}
%     f_{k}=-\frac{1}{|D_{k}|}\sum_{(x,y)\in D_{k}}\ell\left[\upsilon(x;\theta_{k}),y\right]
% \end{equation}
where $\upsilon_{m,k}$ denotes the model for the modality $m$ at client $k$, $\oplus$ denotes loss aggregation across modality $1$ to $M$ and $\mathds{1}_{(\boldsymbol{X},\boldsymbol{y})\in\mathcal{D}_{m}}$ identifies whether the data $(\boldsymbol{X},\boldsymbol{y})$ is sampled from $\mathcal{D}_{m}$ (as a part of our prior knowledge, this identifier is known before training).
Given the pre-trained local model parameters $\{\boldsymbol{\theta}_{k}\}_{k=1}^{K}$, the global model aggregates them into a global one for better prediction accuracy. Note that this process {does not involve cross-modality aggregation} since the local models are trained with multi-modal data.


% Traditional FL learners train a DNN model $\upsilon_{m}$ with parameter $\theta_{u}$ unimodal data $X=x_{m}$ ($u$ suggests unimodal) as input data. Therefore, $f_{k}$ can be represented as:

% \begin{equation}
%     f_{k}=\frac{1}{|D_{k}|}\mathop{\sum}\limits_{\lambda\in D_{k}}\ell(\upsilon_{u}(x_{u},\theta_{u});y)
% \end{equation}

% in order to minimize the empirical loss.
% % \subsubsection{Multimodal Federated Learning}

% As for typical late-fusion multimodal federated, each local model will be jointly trained with $M$ sub-networks $\{\upsilon_{m}\}_{m=1}^{M}$ with parameters $\{\theta_{m}\}_{m=1}^{M}$, each taking $X=\{x_{m}\}_{m=1}^{M}$ as input where M represent for number of different data modalities. Thus, the optimization problem can be formulated as\guiliang{" which variable are you optimising?"}:

% \begin{equation}
%     f_{k}=\frac{1}{|D_{k}|}\mathop{\sum}\limits_{\lambda\in D_{k}}\ell(\mathbb{C}(\upsilon_{1}(x_{1}),...,\upsilon_{m}(x_{m})),y)
% \end{equation}

% \guiliang{"what $\lambda$ means?"}where $\mathbb{C}$ is a classifier and is usually constructed with fully-connected (FC) layers\guiliang{"Again, why it has to be a classifier, why wouldn't a regressor work? how about we name it as a aggregation model or aggregated decision model?"}.

\subsection{Unimodal Training - Multimodal Prediction}
% \st{Here, we define our \texttt{unimodal training - multimodal prediction} framework.} 
In this work, we focus on a more challenging \textit{Unimodal Training - Multimodal Prediction} (UTMP) framework.
UTMP enables clients with only unimodal data to participate in federated learning. The algorithms must build a global model by utilizing knowledge learned by local clients. Ideally, the global model can predict multimodal data with better performance than local models. 
% \st{We consider a federated learning multimodal scenario in which users holding data from either modality can participate. That is to say, users only need to provide a unimodal training dataset to participate in federated learning, and finally, obtain a multimodal model that can take both unimodal data and multimodal data as inference input for higher-precision prediction.}


 Under the UTMP framework, each client stores only data from a single modality, so $\rho_{k}(\boldsymbol{X},\boldsymbol{y})=\alpha_{\xi}\rho_{m}(\boldsymbol{X},\boldsymbol{y})$ where $\alpha_{\xi}$ denotes the potential distribution shift.
Therefore, the model $\upsilon_{m,k}$ at the local client $k$ takes only the data from a single modality $m$ as input. The optimization problem of \textit{local unimodal training} can be formulated as:

% \begin{equation}
%     f_{k}=-\frac{1}{|D_{k}|}\sum_{(x,y)\in D_{k}}\oplus_{m=1}^{M}\mathds{1}_{(x,y)\in\mathcal{D}_{k}}\ell\left[\upsilon_{m}(x;\theta_{k}),y\right]
% \end{equation}

\begin{equation}
    f_{k}=\frac{1}{|D_{k}|}\sum_{(\boldsymbol{X},\boldsymbol{y})\in D_{k}}\ell\left[\upsilon_{m,k}(\boldsymbol{X};\boldsymbol{\theta}_{k}),\boldsymbol{y}\right]
\end{equation}



Note that UTMP requires unimodal training at the local client but cross-modal aggregation at the global server, so the global model parameters $\boldsymbol{\theta}=\oplus_{m=1}^{M}\odot_{k=1}^{K}\boldsymbol{\theta}_{k,m}$, where $\odot_{k=1}^{K}$ and $\oplus_{m=1}^{M}$ denotes single-modality and cross-modality aggregation in federated learning. This hierarchical aggregation serves as the main motivation of the proposed approach, including 1) Posterior-based Encoder Aggregation(PbEA) (corresponding to $\odot$) and 2) Cross-modal Decoder Aggregation(CmDA) (corresponding to $\oplus$).
% to aggregate encoders and decoders separately for better performance. 


However, moving cross-modal aggregation from clients to servers creates a more challenging problem since 1)
UTMP requires aggregating model without access to local data. However, the model parameters trained for different modalities are highly independent and thus become hard to aggregate. 2) Unlike traditional cross-modal prediction approaches, the unimodal data scattered among different clients cannot be aligned through pre-processing in federated learning. Accordingly, cross-modal information fusion cannot be achieved in local training. These difficulties make it impossible for traditional cross-modal and federated learning methods to perform model aggregation.
% \rongyu{as it is risky to feed all sub-networks which is designed for different modality the same data. An extreme case for local optimization shown in the equation will produce a model fit only for one unimodal prediction. Then, a series of model parameters that are trained for different modalities will be highly independent of each other and hard to aggregate.} 2) unalignment: \rongyu{since the unimodal data scattered among different users cannot be aligned through pre-processing, nor can cross-modal information fusion be achieved in local training making it impossible for traditional federated learning methods to perform effective model aggregation directly.}

Despite these challenges, we believe this UTMP framework has a more close connection to real-world applications: UTMP enables edge computing devices to participate in federated learning by utilizing only the unimodal data collected by themselves. UTMP provides a broader range of applications for multimodal federated learning and thus has a more considerable impact than the previous designs.
% \rongyu{As more and more IoT devices have edge computing capabilities, artificial intelligence devices applied to smart homes and smart cities can participate in federated learning through the unimodal data collected by themselves, improving the generalization capabilities while avoiding user privacy issues due to data ownership defining. Thus, UTMP provides a wider range of applications for multimodal federated learning.} 


% To be notified, it is risky to feed all sub-networks which is designed for different modality the same data. An extreme case for local optimization shown in the equation will produce a model fit for one uni-modal prediction. However, the large difference in data distribution between modalities brings a new challenge to the server aggregation stage.\guiliang{merge this statement to the above challenge.} 


% $\{\theta_{m}\}_{m=1}^{M}$ is encoder formed by transformer~\cite{vaswani2017attention}, $\mathbb{C}$ is decoder constructed with LSTM~\cite{hochreiter1997long} and FC layers.



\section{Proposed method}
In this work, we split the feature extraction and prediction layers to cope with the hierarchical aggregation. Specifically, we introduce a transformer-based encoder for feature extraction and an RNN-based decoder for prediction. Based on these structures, we propose {\it Posterior-based Encoder Aggregation} (PbEA) and {\it Cross-modal Decoder Aggregation} (CmDA) that handle the encoder and decoder parameters respectively.
The overall architecture of \textit{HA-Fedformer} can be illustrated in Figure \ref{framework}. 
% \st{To be specific, the encoders are consist of a temporal convolutions layer, positional encoding, and three self-attention Layers. The decoder consists of multiple LSTM layers and FC layers, in which we add a residual module to further improve the network performance.}


% In this section, we will first provide the preliminaries of the multimodal federated learning in \S \textbf{Preliminaries} and present in detail the various ingredients of the \textit{HA-Fedformer} in \S \textbf{Overall Architecture}.




\subsection{Model architecture}
\subsubsection{Encoder}
We consider three major modalities that are commonly studied in multimodal learning, including language (\textit{L}), audio (\textit{A}), and video (\textit{V}) modalities. For modality $m \in\{L,A,V \}$, the input matrix $\boldsymbol{X}_m \in \{\mathbb{R}^{T_m \times d_m}\}$ is used to denote the three input feature sequences, where $T(\cdot)$ denotes the length of the sequence and $d(\cdot)$ denotes the feature dimension of the sequence. The main components are:

{\it Temporal Convolutions.}
Although the training data of the local model belongs to different modalities, multimodal data is still required as input when making predictions. Therefore, it is necessary to reshape the input sequence to a uniform shape. We choose 1$\times$1 temporal convolution to perform the adjustment: 

\begin{equation}
\begin{aligned}
        \dot{\boldsymbol{X}}_{m} &=
    Conv1D(\boldsymbol{X}_m, \iota_{m})
\end{aligned}
\end{equation}
where $\iota_m$ denotes the convolutional kernel for modality $m$, $d_{model}$ is a customize dimension, and the output of encoders $\dot{\boldsymbol{X}}_{m} \in \mathbb{R}^{T_{m}\times d_{model}}$ are matrices with uniformed shape.
% It can simplify perceive the connection between different modal data and is also admittable to the decoder.


{\it Positional Encoding.}
We add positional embedding (PE) to $\dot{\boldsymbol{X}}_{m}$ for incorporating temporal information into the adjusted sequences. Following \textit{Transformer}~\cite{vaswani2017attention}, we augment PE to $\dot{\boldsymbol{X}}_{m}$: 

% PE can be computed with \textit{sim} and \textit{cos} function:

% \begin{equation}
% \begin{aligned}
%         PE(pos,2i) = sim(\frac{pos}{10000^{2i/d_{m}}})\\
%         PE(pos,2i+1) = cos(\frac{pos}{10000^{2i/d_{m}}})
% \end{aligned}
% \end{equation}

% where $pos=\{1,...,T_{\{L,A,V\}}\}$ and $i=\{0,\lfloor d_{m}/2\rfloor\}$. $d_{m}$ is the output dimension and $(d_{m}/h)=d_{\{key,value\}}=d_{\{L,A,V\}}$. It should be noted that $d_{\{key,value\}}$ is the dimension of the \textit{key/value}
% pairs in the following encoder and $h$ is the parallel attention layers or heads. Then, we can further compute the encoded modal sequences $\ddot{x}_{\{L,A,V\}}$ which can be represented as:

\begin{equation}
\begin{aligned}
        \ddot{\boldsymbol{X}}_{m} = \dot{\boldsymbol{X}}_{m} + PE(T_{m},d_{model})
\end{aligned}
\end{equation}
where $\ddot{\boldsymbol{X}}_{m}$ represents the low-level position-aware embeddings and $PE(T_{m},d_{model})$ computes the embeddings for each position index of the data sequences. We leave the details of the computation of PE in Appendix A.1.
% \guiliang{add the appendix section title}.

{\it Self-Attention Transformer.}
We perform self-attention based embedding feature extraction~\cite{vaswani2017attention} for the input temporal sequences $\ddot{\boldsymbol{X}}_{m}$. As local clients only hold unimodal data, the output of the three encoders ${attention}(\ddot{\boldsymbol{X}}_{m})$ will be the embeddings of one modality. 
% (e.g., for clients holding language modal data, the output of the encoders will be 
% $3\times attention(\ddot{x}_{\{m\}})$

\subsubsection{Decoder}
% \st{The output of the three encoders will eventually pass the decoder for the final prediction.} 
The decoder is constructed with LSTM and fully-connected layers with the residual module, which is a classifier module for decoding predicted labels instead of decoding input features like auto-encoding models. 
% The output of the encoders $attention(\ddot{x}_{\{L,A,V\}})$ will be average merged first:
% \begin{equation}
% \begin{aligned}
%         \dddot{x}_{\{L,A,V\}} = \mathop{\sum}\limits_{m=0}^{M-1} attention(\ddot{x}_{m,\{L,A,V\}}) / M
% \end{aligned}
% \end{equation}
% $\delta$ is the hyperparameter and $M$ is the number of modalities involved in this training (e.g., $M=len(\{L,A,V\})=3$ in this paper). It is necessary to give different weights to the encoder output. Because when the model is aggregated on the server and the encoder is \textbf{exchanged}, the three encoders will represent the mapping of the three modalities L, A, and V. This makes when the model is sent to the client for local training, there will always be two encoder modalities that do not match the local data modalities. 
Note that the local client has only the unimodal data, but each client maintains $M$ (the total number of modalities, e.g., three for L, A, and V) encoders for extracting features from multimodal data. During implementation, we feed $\boldsymbol{X}_{m}$ to all the encoders and use the average embedding from all encoders as the input of our decoder $\dddot{\boldsymbol{X}}_{m} =\frac{1}{M} \mathop{\sum}\limits_{m=0}^{M-1} attention(\ddot{\boldsymbol{X}}_{m})$.

% \begin{equation}
% \begin{aligned}
% \dddot{\boldsymbol{X}}_{m} =\frac{1}{M} \mathop{\sum}\limits_{m=0}^{M-1} attention(\ddot{\boldsymbol{X}}_{m})
% \end{aligned}
% \end{equation}

This design has several advantages: 1) The goal of UTMP is predicting from multimodal data, and thus the global model on the server side must have $M$ encoders. We maintain the exact size of model parameters for each client for ease of model uploading and downloading. 2) Intuitively, utilizing outputs from unmatched encoders (e.g., feed $\boldsymbol{X}_L$ to the encoder for $A$) can influence training efficiency and efficacy. However, an intriguing finding is that our model, in fact, utilizes these outputs as noisy signals which prevents client models from overfitting local data. 
However, unlike random noise, these noisy signals carry structural information of sequential input. Although their modalities do not match those of the input data, they can be used as an auxiliary input for preventing overfitting in the following CmDA.
% be used as an auxiliary input to the Decoder which incorporates the characteristics of different modalities according to CmDA. PbEA also plays a role here in minimizing the impact of misinformation by reducing uncertainty. 
For more evidence, Figure~\ref{noisy} visualizes the latent features with T-SNE from the encoders across the different communication rounds. After multiple runs of training, we observe a shrinkage effect: the extracted features from different encoders become more similar and their distance in latent space shrinks significantly, which represents similar structural information.
% which further demonstrates the effectiveness of CmDA and PbEA.
% \st{Empirically, we may assume that the output of the two unmatch encoders will harm the training. However, we are surprised to find that they may act as noises that alleviate the overfitting and thus contribute to the overall training. After the average merged, $\dddot{x}_{\{L,A,V\}} \in \mathbb{R}^{T_{\{u\}}}$ will be able to pass through the LSTM\cite{hochreiter1997long} and fully-connected layers to make predictions. }

\begin{figure}[t]
\centering
\includegraphics[scale=0.33]{./images/noise.pdf}
\caption{T-SNE visualization of encoder output features (a) in the first communication round (b) in the $30^{th}$ communication rounds}
% \guiliang{nice plot! please update the text as cross-modal \textbf{data fusion at local clients} and cross-modal \textbf{parameters fusion at the server}. }
\label{noisy}
\vspace{-0.5em}
\end{figure}

% \vspace{-0.5em}
\subsection{Hierarchical aggregation}
\subsubsection{Posterior-based Encoder Aggregation (PbEA)} 
The datasets for the same modality are non-IID at local clients, resulting in considerable distributional shifts among local models. 
% \st{As the model and the basic setting mentioned in the previous chapter, the unimodal training for each client could face an obviously non-IID data distribution.}
During the model aggregation, Traditional methods~\cite{mcmahan2017communication, sahu2018convergence} aggregated the models at local clients by averaging the mean of parameters without modeling their uncertainty, which might cause biased prediction.
 Thus, we propose a modality-oriented uncertainty-guided method based on the layer-wise local posteriors to aggregate models trained with data of the same modality using the Markov Chain Monte Carlo (MCMC) method as shown in Algorithm 1. Given the training dataset $(\boldsymbol{X},\boldsymbol{y}) \in D_k$
% where the input of model $\boldsymbol{X}_{\{L,A,V\}}$ follows Gaussian Distribution. 
and a linear model, its least squares loss function  $\ell\left[\upsilon(\boldsymbol{X};\boldsymbol{\theta}_{k}),\boldsymbol{y}\right] = \frac{1}{|D_k|}\|\boldsymbol{X} \boldsymbol{\theta}_k-\boldsymbol{y}\|_{2}^{2}$. Since the square loss corresponds to likelihood under a Gaussian model, the log-likelihood client loss by posteriors becomes:
\begin{equation}
\begin{aligned}
     \ell\left[\upsilon(\boldsymbol{X};\boldsymbol{\theta}_{k}),\boldsymbol{y}\right] & =\log (e^{ \left\{\frac{1}{|D_k|}({\boldsymbol{X} \boldsymbol{\theta} - \boldsymbol{y}})^2  \right\}}) \\ 
    & = \log (e^{  \left\{\frac{1}{|D_k|}\left({\boldsymbol{\theta}_i}-{\mu}_{k}\right)^{\top} {\boldsymbol{\Sigma}}_{k}^{-1}\left({\boldsymbol{\theta}_k}-\boldsymbol{\mu}_{k}\right)\right\}}) + \epsilon
\end{aligned}
\end{equation}
where the mean $\boldsymbol{\mu}_k = (\boldsymbol{X}_k^T\boldsymbol{X}_k)^{-1}\boldsymbol{X}_k^T\boldsymbol{y}_k $, covariance $\boldsymbol{\Sigma}_k^{-1} = \boldsymbol{X}_k^T\boldsymbol{X}_k$, 
% and the weight $\boldsymbol{\theta}$ is also initialized as Gaussian
and $\epsilon$ denotes a constant.
\begin{algorithm}[]
    \SetAlgoLined
    \SetKwInOut{Input}{input}
    \Input{Sample times $S$, Update steps $T$, Datasets $\{D_0,...,D_K\}$, global model parameter $\boldsymbol{\theta}$}
    \For{$k^{th}$client \textbf{in} K clients}{
    % \KwData{$(\boldsymbol{X}_k, \boldsymbol{k}_i) \in D_k$} 
    \textbf{Init:} $ SamplesSet = \{\}, \boldsymbol{\theta}_k \xleftarrow{download}  \boldsymbol{\theta}$ \;
        \For{$s \textbf{ in } [1,S]$}{
            Sample $(\boldsymbol{X}_k, \boldsymbol{y}_k) \sim D_k$ \;
            $\boldsymbol{\theta}_k^s = \boldsymbol{\theta}_k$\;
            \For{$t \textbf{ in } [1,T]$}{
                ${\boldsymbol{\theta}_k^s} \gets \boldsymbol{ClientOPT}(\boldsymbol{X}_k, \boldsymbol{y}_k, \boldsymbol{\theta}_k^s)$
                }
                $SamplesSet \cup \{{\boldsymbol{\theta}_k^s}\}$
            }
            % // $\boldsymbol{\theta}_k^{s,l}$:  $\boldsymbol{\theta}$ in $l^{th}$ layer of $k^{th}$ client's $s^{th}$ sample \;
            \For{$l^{th}$ layer \textbf{in} model}{
            Calculate $\Sigma_k^{l},\mu_k^{l}$ with $SamplesSet$\;
            % $  \{ \boldsymbol{\theta}_k^{0,l}, ... \boldsymbol{\theta}_k^{S,l}\}$\;
             $\Delta_k^l = {\Sigma}_{k}^{-1}({\boldsymbol{\theta}_{k}^{l}-\mu_{k}^{l}})$
            }
            Record $\boldsymbol{\Delta}_k=[{\Delta}^{1}_k,...,{\Delta}^{L}_k]$ and $\boldsymbol{\Delta}_k \xrightarrow{to} server$
    }
    ${\boldsymbol{\theta}^{\prime}} \longleftarrow \boldsymbol{ServerUpdate}(\boldsymbol{\theta}; \boldsymbol{\Delta}_0, ..., \:\boldsymbol{\Delta}_K)$ \;
    \SetKwInOut{Output}{output}
    \Output{$\boldsymbol{\theta}^{\prime}$}
\caption{Posterior-based Encoder Aggregation}
\end{algorithm}

In federated learning, 
% \guiliang{a previous work shows~(add citation)}
according to the proposition given by \cite{al2020federated}, the global posterior can be calculated by-product of local posteriors as $\mathbb{P}(\boldsymbol{\theta}|(\boldsymbol{X},\boldsymbol{y})) \propto \prod_{k=1}^{K} \mathbb{P}\left({\boldsymbol{\theta}_k}|(\boldsymbol{X}_{k},\boldsymbol{y}_{k})\right)$ where the $K$ represents the number of clients. Accordingly, the mean of global model parameters can be represented by:
\begin{equation}
\begin{aligned}
    \boldsymbol{\mu}:=\left(\frac{1}{K}\sum_{k=1}^{K} \boldsymbol{\Sigma}_{k}^{-1}\right)^{-1}\left(\frac{1}{K}\sum_{k=1}^{K} \boldsymbol{\Sigma}_{k}^{-1} {\boldsymbol{\mu}}_{k}\right)
\end{aligned}
\label{mean}
\end{equation}
 % \textbf{Uncertainty based} 


Equations (\ref{mean}) requires sending all local means and covariance matrices to the client, which often incurs a high communication and computation burden. To solve this issue, we follow the proposition of global posterior inference~\cite{al2020federated}: $\boldsymbol{\mu}$ is a minimizer of the function $\mathcal{Q}(\boldsymbol{\theta}) := \frac{1}{2}\boldsymbol{\theta}^T (\sum_{k=0}^K \boldsymbol{\sigma}_k^{-1})\boldsymbol{\theta} + (\sum_{k=0}^K \boldsymbol{\sigma}_k^{-1}\boldsymbol{\mu}_k)^T\boldsymbol{\theta}$ whose gradient can be disentangled to local gradients by $\Delta\mathcal{Q}= \sum_{k=1}^{K}\frac{1}{K}\Delta\mathcal{Q}_{k}$ and $ \Delta\mathcal{Q}_{k}=\boldsymbol{\Sigma}_{k}^{-1}({\boldsymbol{\theta}_{k}-\boldsymbol{\mu}_{k}})$.
% According to this proposition, the calculation of gradient $\Delta$ of $Q(\boldsymbol{\theta})$  can be simplified on the local clients, 
Accordingly, in order to develop a layer-wise estimation of the global mean $\boldsymbol{\mu}^{l}$, we calculate its gradient by:
\begin{equation}
\begin{aligned}
    \Delta^{l} {:=} K^{-1}{\sum_{k=1}^{K} }({\boldsymbol{\Sigma}}_{k}^{l})^{-1}({\boldsymbol{\theta}_{k}^{l}-\boldsymbol{\mu}_{k}^{l}})
\end{aligned}
\end{equation}


Note that the ${\boldsymbol{\Sigma}}_{k}^{l}$ and $\boldsymbol{\mu}_{k}^{l}$ are the layer-wise covariance and mean. Thus, we obtain $M$ aggregated models consisting of $M$ decoders for CmDA and $M\times M$ encoders for further encoder-decoder concatenation.
% Compared to the previous parameters sampling method~\cite{al2020federated}, our sampling approach is based on an ensemble of local models for a better estimation of the epistemic uncertainty during training~\cite{WenTB20}. It should be noted that $\boldsymbol{\theta}_k^{s,l}$ indicates the parameters in the $l^{th}$ layer of sample $s^{th}$ in client $k^{th}$.


% After PbEA, we concatenate encoders, which constructs a new model capable of multimodal prediction. The details are illustrated in Figure \ref{exchange}.


% \textbf{Simplified selection} Besides, the calculation of uncertainty is time-consuming while training sample times, especially the covariance are hard to obtain. So we also proposed another encoder aggregation method to simplify this process. With a similar precondition mentioned in posterior-based encoder aggregation, we sample only the output of each training to calculate the KL divergence as an uncertainty factor to represent the efficiency of this training. This factor is further used to decide which group of model weight $\theta_k$ will be used to join the server aggregation.
% \begin{equation}
% \begin{aligned}
%     \theta_k {:=} \mathop{arg\thinspace max}\limits_{\theta_k} KL\left\{P(x_i,y_i; \theta_i) || P(x_i,y_i; \theta_k^s)\right\}
% \end{aligned}
% \end{equation}



% \textbf{Encoder Exchange} After aggregating the models of the same modality, we will get three \textit{HA-Fedformer} models representing the three modalities L, A, and V with nine encoders combined. We randomly abandon two encoders for each \textit{HA-Fedformer} and remember their positions in the first communication round of FL. We then assemble the rest of the three encoders with three modalities forming a new \textit{HA-Fedformer} for multimodal inference. It should be noted that encoders abandoned in the following training round will always be those that share the same positions in the first communication round.

% \begin{equation}
%     Encoder_{\{L,A,V\}} = \mathds{1}_{m\in\mathcal{\{L,A,V\}}}\frac{1}{|D_{m}|} \mathop{\sum}\limits_{k=0}^{|D_{m}|}(\Delta_k + \theta)
% \end{equation}



\subsubsection{Cross-modal Decoder Aggregation (CmDA)}
 Inspired by Mult\cite{tsai2019multimodal}, which uses the attention mechanism of the Transformer to align the data of different modalities in pairs at the feature level. Since federated learning cannot directly access the data, we consider the alignment at the model parameter level instead. By exploring the correlation between model weights, we implement an attention-alike method\cite{ji2019learning} to align the decoder weights trained on different modality data. Followed by Mult, our proposed cross-modal aggregation strategy takes two decoders' parameters each time from different modalities to compute a self-adaptive coefficient $\psi$, which determines the significance of their hidden signal correlations and can help to facilitate alignment in the modal parameter space. By pairing decoders from all modalities, we get $C_{M}^{2}$ (e.g., for $m\in\{L,A,V\}$,$C_{M}^{2}$=3) optimization objectives. For the $c\in \{1,...,C_{M}^{2}\}$ objective, the corresponding optimization objective can be defined as:

\begin{equation}
\begin{aligned}
    \mathop{arg\thinspace min}\limits_{\boldsymbol{\theta}_{m}^{t}}\frac{1}{2}\psi_{c}^{t}\ast \Gamma(\boldsymbol{\theta}_{m}^{t}, \boldsymbol{\theta}_{\hat{m}}^{t})^{2}
\end{aligned}
\end{equation}
where $m, \hat{m}\in \{L,A,V\} (m\neq\hat{m})$, $\boldsymbol{\theta}_{c}^{t}$ denotes the estimated decoder parameters at communication round $t$, and $\Gamma(,)$ denotes the distance between the model parameters.
% Followed by Mult\cite{tsai2019multimodal}, our proposed cross-modal aggregation strategy takes two decoder parameters from different modalities as query and key, so as to compute the attentive weight $\psi_{k,m}$ to determine the significance of the hidden signal correlations. 
% We first compute the norm difference between the query and key as their similarity:

We first compute the norm difference between the query $\boldsymbol{\theta}^{t,l}_{m}$ and key $\boldsymbol{\theta}^{t,l}_{\hat{m}}$ in each model layer $l\in \{0,1,...,L\}$ to obtain the layer-wise coefficient $\psi_{c}^{t}=\{\psi_{c}^{t,0}, \psi_{c}^{t,1},..., \psi_{c}^{t,L}\}$:
% \begin{equation}
% \begin{aligned}
%     \gamma^{l}_{k,m} = ||\boldsymbol{\theta}^{l}_{k,m} - \boldsymbol{\theta}^{l}_{k,\hat{m}}||_{p}
% \end{aligned}
% \end{equation}

% Then, we can calculate the layer-wise attentive weight $\psi^{l}_{k}$ with the \textit{softmax} function:

\begin{equation}
\begin{aligned}
    % \psi^{l}_{k,m} = softmax(\gamma^{l}_{k,m}) = \frac{e^{\gamma^{l}_{k,m}}}{\sum_{k=0}^{M-1}e^{\gamma^{l}_{k,m}}}
    \psi^{t,l}_{c} = softmax(\gamma^{t,l}_{c}) = softmax(||\boldsymbol{\theta}^{t,l}_{m} - \boldsymbol{\theta}^{t,l}_{\hat{m}}||_{p})
\end{aligned}
\end{equation}

% We may further compute the gradient with the Euclidean distance for $\Gamma(,)$ and the derivative of Eq. 13:

% \begin{equation}
% \begin{aligned}
%     \nabla = \mathop{\sum}\limits_{k=0}^{M-1}\psi^{k}_{m}(\theta_{t,m} - \theta^{k}_{t,\hat{m}})
% \end{aligned}
% \end{equation}

Then, we perform gradient descent to update decoder parameters with the gradients computed by the Euclidean distance for $\Gamma(,)$ and the derivative of Equations (10):

\begin{equation}
\begin{aligned}
    \boldsymbol{\theta}^{t}_{c} \gets \boldsymbol{\theta}^{t}_{m} - \eta\nabla :=\boldsymbol{\theta}^{t}_{m} - \eta\mathop{\sum}\limits_{m=0}^{M-1}\psi_{c}^{t}(\boldsymbol{\theta}^{t}_{m} - \boldsymbol{\theta}^{t}_{\hat{m}})
\end{aligned}
\end{equation}
where $\eta$ is the learning rate. We update the global decoder's parameters by aggregating $\boldsymbol{\theta}^{t}_{c}$ corresponding to the solutions of $C_{M}^{2}$ optimization problems:

\begin{equation}
\begin{aligned}
    \boldsymbol{\theta}^{t+1}_{global} = \frac{1}{C_{M}^{2}}\mathop{\sum}\limits_{c=0}^{C_{M}^{2}-1}\boldsymbol{\theta}^{t}_{c}
\end{aligned}
\end{equation}

In this way, we obtain a global decoder with parameters well represented across the multiple modalities.

\subsection{Encoder-Decoder concatenation}
We concatenate the $M\times M$ aggregated encoders and the one global decoder obtained from PbEA and CmDA, which constructs a new model capable of multimodal prediction. To be more specific, as illustrated in Figure \ref{exchange}, after PbEA, each aggregated local model has $M$ ($M=3$ in our example) encoders. Since PbEA is trained with the data in a single modality, we record only the encoder for this modality (i.e., abandoning the rest of $M-1$ encoders) and combine it with the aggregated decoder to be a part of our global multimodal model.

\subsection{Overall Computation Cost}
The training/inference cost of HA-Fedformer is $\mathcal{O}(S^{2}d+k)$/$\mathcal{O}(n)$, while standard FedAvg is $\mathcal{O}(d)$/$\mathcal{O}(n)$, where $S$ indicates the size of sampling set, $k$ represents the number of CmDA, and $d$ is the number of clients.  The model size of Mult is 4.38MB, HA-Fedformer is 846KB, and the communication cost is related to the model size.


\begin{figure}[t]
\centering
\includegraphics[scale=0.28]{./images/concat.pdf}
\caption{Illustration of encoder-decoder concatenation. }
% After PbEA, each aggregated local model has $M$ encoders. Since PbEA is modality-oriented, we record only the encoder corresponding to the data in the targeted modality (i.e., the other $M-1$ encoders are abandoned) and use it as one of the encoders in our global model.} 
\label{exchange}
\end{figure}

\section{Experiments}
\begin{table*}[t]
\centering
  \caption{Results for multimodal sentiment analysis on (relatively
large scale) CMU-MOSEI with unimodal local sequences. $\Downarrow$ means p value of significant test $<$0.01 compare to the HA-Fedformer++. $\pm$ means the variance. $\uparrow$ means higher is better, and $\downarrow$ means lower is better.  Superscript A stands for FedAvg, P for FedProx(e.g., $Mult^{A}$ stands for Mult+FedAvg). HA-Fedformer++ stands for complete HA-Fedformer, HA-Fedformer++(S) is its simplified form, HA-Fedformer+ stands for HA-Fedformer++ minus PbEA, and HA-Fedformer stands for HA-Fedformer+ further minus CmDA.}
\footnotesize
  \label{tab:freq}
  \setlength{\tabcolsep}{2.75mm}{
  \begin{tabular}{|c||ccccc|}
      \toprule

    Metric & $Acc_{7}\uparrow$ & $Acc_{2}\uparrow$ & $F1\uparrow$ & $MAE\downarrow$ & $Corr\uparrow$ \\
    \midrule
    \midrule
        \multicolumn{6}{|c|}{\textbf{CMU-MOSEI Sentiment} (\textcolor{red}{Unimodal local data})}  \\
  \midrule
  \midrule
    $TBJE^{A}$(20' ACL) $\Downarrow$ & 41.5($\pm$3.43E-5) & 68.3($\pm$4.15E-5) & 68.5($\pm$1.14E-5) & 0.843($\pm$5.48E-5) & 0.456($\pm$4.48E-5) \\
    $TBJE^{P}$(20' ACL) $\Downarrow$ & 41.2($\pm$2.25E-5) & 67.4($\pm$1.84E-5) & 68.1($\pm$1.47E-5) & 0.863($\pm$5.15E-5) & 0.477($\pm$7.14E-5) \\
    $MAT^{A}$(20' EMNLP) $\Downarrow$ & 39.4($\pm$2.31E-5) & 63.4($\pm$5.53E-6) & 64.7($\pm$3.14E-6) & 0.857($\pm$8.51E-5) & 0.422($\pm$4.77E-5) \\
    $MAT^{P}$(20' EMNLP) $\Downarrow$ & 39.9($\pm$4.53E-5) & 66.8($\pm$4.22E-5) & 67.3($\pm$2.75E-5) & 0.844($\pm$2.76E-4) & 0.456($\pm$6.69E-5) \\
    $MNT^{A}$(20' EMNLP) $\Downarrow$ & 38.8($\pm$1.70E-4) & 63.2($\pm$1.53E-4) & 64.6($\pm$8.93E-5) & 0.871($\pm$1.47E-5) & 0.376($\pm$1.31E-4) \\
    $MNT^{P}$(20' EMNLP) $\Downarrow$ & 36.0($\pm$5.54E-5) & 63.1($\pm$3.82E-5) & 64.7($\pm$2.23E-5) & 0.964($\pm$7.47E-6) & 0.435($\pm$6.44E-4) \\
    $Mult^{A}$(19' ACL) $\Downarrow$ & 42.7($\pm$1.49E-5) & 69.0($\pm$2.13E-5) & 72.7($\pm$2.26E-4) & 0.783($\pm$1.88E-5) & 0.374($\pm$1.30E-3) \\
    $Mult^{P}$(19' ACL) $\Downarrow$ & 41.9($\pm$6.52E-6) & 65.5($\pm$7.43E-6) & 70.4($\pm$8.94E-5) & 0.806($\pm$4.96E-6) & 0.269($\pm$8.33E-5) \\
    $FedMSplit$(22' KDD) $\Downarrow$ & 43.8($\pm$7.53E-5) & 73.5($\pm$8.42E-5) & 75.2($\pm$7.79E-5) & 0.702($\pm$6.96E-5) & 0.522($\pm$9.41E-5) \\
      \midrule
    \midrule
      \multicolumn{6}{|c|}{\textbf{CMU-MOSEI Sentiment} (\textcolor{blue}{Ablation Study})}  \\
          \midrule
    \midrule
    % L only(Transformer 17' NIPS)$\Downarrow$ & 44.7($\pm$1.2E-5) & 75.4($\pm$2.2E-5) & 76.2($\pm$1.5E-5) & 0.689($\pm$8.6E-4) & 0.551($\pm$1.1E-5) \\
    % A only(Transformer 17' NIPS)$\Downarrow$ & 41.4($\pm$1.2E-5) & 65.6($\pm$1.9E-5) & 68.8($\pm$1.5E-5) & 0.764($\pm$7.9E-4) & 0.310($\pm$0.6E-5) \\
    % V only(Transformer 17' NIPS)$\Downarrow$ & 43.5($\pm$0.4E-5) & 66.4($\pm$2.5E-5) & 69.3($\pm$4.1E-5) & 0.759($\pm$5.4E-5) & 0.343($\pm$2.1E-5) \\
    % \midrule
    % \midrule
    only L \& A(ours) $\Downarrow$ & 47.8($\pm$4.22E-5) & 78.4($\pm$7.45E-5) & 78.1($\pm$8.93E-6) & 0.644($\pm$2.37E-5) & 0.614($\pm$1.75E-5) \\
    only V \& L(ours) $\Downarrow$ & 46.5($\pm$8.95E-6) & 78.0($\pm$6.34E-5) & 78.0($\pm$2.26E-5) & 0.656($\pm$1.15E-5) & 0.606($\pm$9.83E-6) \\
    only A \& V(ours) $\Downarrow$ & 42.3($\pm$1.23E-6) & 63.0($\pm$4.37E-5) & 76.4($\pm$3.24E-5) & 0.803($\pm$2.28E-5) & 0.195($\pm$1.42E-5) \\
  \midrule
    \midrule
    HA-Fedformer $\Downarrow$ & 45.4($\pm$3.33E-5) & 77.7($\pm$3.71E-5) & 78.7($\pm$5.12E-7) & 0.662($\pm$3.26E-5) & 0.604($\pm$3.83E-5) \\
    HA-Fedformer+ $\Downarrow$ & 46.7($\pm$3.72E-5) & 78.0($\pm$2.84E-5) & 78.4($\pm$9.76E-6) & 0.647($\pm$1.91E-5) & \textbf{0.625($\pm$4.14E-5)} \\
    HA-Fedformer++(S) $\Downarrow$ & \textbf{49.1($\pm$4.42E-5)} & 78.5($\pm$2.74E-5) & 78.9($\pm$3.72E-5) & 0.639($\pm$1.26E-5) & 0.617($\pm$2.46E-5) \\
    HA-Fedformer++ & 48.6($\pm$1.92E-6) & \textbf{79.1($\pm$2.25E-5)} & \textbf{79.2($\pm$2.92E-5)} & \textbf{0.638($\pm$7.43E-6)} & 0.624($\pm$3.94E-6) \\
  \bottomrule
\end{tabular}}
\label{mosei}
\end{table*}


In this section, we empirically evaluate \textit{HA-Fedformer} on two well-known datasets that are frequently used to benchmark the multimodal sentiment analysis in prior works.~\cite{wang2019words, pham2019found, tsai2019multimodal}. Our goal is to compare \textit{HA-Fedformer} with previous SOTA baselines under the context of data non-IID and \textit{unaligned} multimodal language sequences under the UTMP framework. 
% \st{Our proposed \textit{HA-Fedformer} can achieve significant testing performance, taking only unimodal data as model input in the local training stage, while other baselines fail to accomplish cross-modal learning directly through model aggregation.} 
% All experiments are conducted on 1 RTX-3080Ti.

\subsection{Datasets and evaluation metrics}
\textbf{CMU-MOSI and CMU-MOSEI} 
CMU-MOSI~\cite{zadeh2016multimodal} is a multimodal sentiment analysis dataset consisting of 2,199 short monologue video clips, while CMU-MOSEI~\cite{zadeh2018multimodal} consists of 23,454 movie review video clips from YouTube. Each video clip's length is equivalent to the length of a sentence.
% For CMU-MOSI and CMU-MOSEI, the samples' labels range from -3 to 3, denoting the sentiment scores from highly negative to highly positive. 
We evaluate the performance with the following metrics (in accordance with those employed in previous works~\cite{wang2019words, tsai2019multimodal, pham2019found}): 7-class accuracy (i.e., $Acc_{7}\in[-3,3]$), binary accuracy (i.e., $Acc_{2}\in\{negative, positive\}$), F1 score, mean absolute error (MAE),  and the correlation of the model’s prediction with human (Corr). It should be noted that the dataset is distributed equally among all clients, while 10 out of 30 clients participate in FL in each communication round.



% \begin{figure*}[t]
% \centering
% \subfigure[Validation loss $v.s.$ Communication rounds for CMU-MOSI]
% {\includegraphics[width=5.85cm]{mosi_test_shadow_new.pdf}}
% \subfigure[Validation loss $v.s.$ Communication rounds for CMU-MOSEI]{\includegraphics[width=5.85cm]{mosei_test_shadow_new.pdf}}
% \subfigure[Validation loss $v.s.$ Communication rounds for CMU-MOSEI with different $\aleph$]{\includegraphics[width=5.85cm]{ablation_shadow.pdf}}

% \caption{(a),(b) Validation/Training loss $v.s.$ Communication rounds for two datasets. (c) Comparison of sample times $S$. It should be noted that the same baseline uses the same color, and different dashed lines represent different federated learning methods. All representations follow the same as Table 1.} %图片标题
% \label{eval}  %图片交叉引用时的标签
% \end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[scale=0.7]{./images/experiment.pdf}
\caption{(a),(b) Training loss $v.s.$ Communication rounds for two datasets. (c) Comparison of sample times $S$. It should be noted that the same baseline uses the same color, and different dashed lines represent different federated learning methods. All representations follow the same as Table 1.}
\vspace{-1em}
\label{eval}
\end{figure*}



\subsection{Baselines}

We choose Multimodal Transformer (Mult)~\cite{tsai2019multimodal}, Transformer-based Joint Encoding (TBJE)~\cite{delbrouck2020transformer}, Modulated Normalization Transformer (MNT), Modulated Attention Transformer (MAT)~\cite{delbrouck2020transformer}, and FedMSplit~\cite{chen2022fedmsplit} that achieves SOTA results on various multimodal learning tasks as our baselines. We further combine two SOTA federated learning approaches, FedAvg~\cite{mcmahan2017communication} and FedProx~\cite{sahu2018convergence} with each non-federated baseline in order to extend them to the federated learning context. We use superscript $A$ to denote FedAvg and $P$ to denote FedProx (e.g., $Mult^{A}$ represents Mult+FedAvg). HA-Fedformer++ stands for complete HA-Fedformer, HA-Fedformer+ stands for HA-Fedformer++ minus PbEA, and HA-Fedformer stands for HA-Fedformer+ further minus CmDA. We further provide a simplified solution HA-Fedformer++(S) (refer to Appendix A.4.) for HA-Fedformer++, that only takes advantage of the mean value of the sampling results.

\begin{table}[t]
\centering
  \caption{Results for multimodal sentiment analysis on  CMU-MOSI with unimodal data. All representations follow the same as in Table 1. Refer to Appendix A.2. for further results.}
\footnotesize
  \label{tab:freq}
  \resizebox{0.475\textwidth}{!}{
  \setlength{\tabcolsep}{0.9mm}{
  \begin{tabular}{|c||ccc|}
      \toprule

    Metric & $Acc_{7}\uparrow$ & $Acc_{2}\uparrow$  & $Corr\uparrow$ \\
    \midrule
    \midrule
        \multicolumn{4}{|c|}{\textbf{CMU-MOSI Sentiment} (\textcolor{red}{Unimodal local data})}  \\
  \midrule
  \midrule
    $TBJE^{A}$ $\Downarrow$ & 26.1($\pm$3.3E-5) & 69.4($\pm$3.2E-5)  & 0.38($\pm$7.1E-5)  \\
    $TBJE^{P}$ $\Downarrow$ & 26.6($\pm$1.1E-5) & 69.7($\pm$4.2E-5)  & 0.37($\pm$6.5E-5) \\
    $MAT^{A}$ $\Downarrow$ & 25.2($\pm$5.4E-5) & 68.5($\pm$4.8E-5)  & 0.37($\pm$7.1E-5) \\
    $MAT^{P}$ $\Downarrow$ & 25.6($\pm$2.4E-4) & 68.0($\pm$3.2E-5)  & 0.34($\pm$2.7E-5) \\
    $MNT^{A}$ $\Downarrow$ & 22.0($\pm$4.7E-5) & 67.8($\pm$3.7E-5)  & 0.38($\pm$1.5E-5) \\
    $MNT^{P}$ $\Downarrow$ & 22.8($\pm$3.6E-5) & 67.4($\pm$2.7E-5)  & 0.36($\pm$8.3E-5) \\
    $Mult^{A}$ $\Downarrow$ & 23.5($\pm$6.3E-5) & 63.7($\pm$3.2E-4)  & 0.37($\pm$5.3E-4) \\
    $Mult^{P}$ $\Downarrow$ & 22.2($\pm$1.2E-4) & 64.6($\pm$3.3E-5)  & 0.39($\pm$5.5E-4) \\
    $FedMSplit$ $\Downarrow$ & 27.7($\pm$2.3E-4) & 68.9($\pm$3.3E-4)  & 0.44($\pm$4.77E-4) \\
      \midrule
    \midrule
      \multicolumn{4}{|c|}{\textbf{CMU-MOSI Sentiment} (\textcolor{blue}{Ablation Study})}  \\
          \midrule
    \midrule
    % L only(Transformer 17' NIPS)$\Downarrow$ & 27.4($\pm$3.5E-4) & 70.3($\pm$8.6E-4) & 69.3($\pm$2.5E-5) & 1.108($\pm$3.9E-5) & 0.521($\pm$4.1E-4) \\
    % A only(Transformer 17' NIPS)$\Downarrow$ & 15.4($\pm$5.6E-6) & 44.6($\pm$5.5E-6) & 45.3($\pm$3.4E-6) & 1.521($\pm$8.7E-6) & 0.068($\pm$4.6E-6) \\
    % V only(Transformer 17' NIPS)$\Downarrow$ & 16.6($\pm$3.9E-5) & 48.2($\pm$2.3E-5) & 49.3($\pm$8.7E-5) & 1.489($\pm$2.2E-5) & 0.101($\pm$6.2E-6) \\
    % \midrule
    % \midrule
    L \& A $\Downarrow$ & 28.1($\pm$1.7E-6) & 72.5($\pm$5.3E-4)  & 0.55($\pm$9.6E-4) \\
    V \& L $\Downarrow$ & 29.3($\pm$6.3E-5) & 73.5($\pm$3.1E-4)  & 0.55($\pm$1.3E-3) \\
    A \& V $\Downarrow$ & 17.8($\pm$3.7E-5) & 52.7($\pm$4.4E-5)  & 0.17($\pm$3.3E-4) \\
  \midrule
    \midrule
    HA-Fed. $\Downarrow$ & 29.0($\pm$1.0E-4) & 72.0($\pm$1.6E-5)  & 0.51($\pm$9.8E-5) \\
    HA-Fed.+ $\Downarrow$ & 29.3($\pm$3.5E-5) & 72.1($\pm$4.3E-5)  & 0.52($\pm$5.8E-4) \\
    HA-Fed.++(S)$\Downarrow$ & 30.7($\pm$9.2E-5) & 74.8($\pm$1.5E-5)  & 0.54($\pm$1.2E-5) \\
    HA-Fed.++ & \textbf{31.1($\pm$2.3E-6)} & \textbf{76.3($\pm$6.2E-5)}  & \textbf{0.55($\pm$7.7E-5)} \\
  \bottomrule
\end{tabular}}
}
\label{mosi}
\vspace{-0.5cm}
\end{table}


% \begin{table*}[t]
% \centering
%   \caption{Results for multimodal sentiment analysis on (relatively
% large scale) CMU-MOSEI with unimodal local sequences. $\Downarrow$ means p value of significant test $<$0.01 compare to the HA-Fedformer++. $\pm$ means the variance. $\uparrow$ means higher is better, and $\downarrow$ means lower is better.  Superscript A stands for FedAvg, P for FedProx(e.g., $Mult^{A}$ stands for Mult+FedAvg). HA-Fedformer++ stands for complete HA-Fedformer, HA-Fedformer++(S) is its simplified form, HA-Fedformer+ stands for HA-Fedformer++ minus PbEA, and HA-Fedformer stands for HA-Fedformer+ further minus CmDA.}
% \footnotesize
%   \label{tab:freq}
%   \setlength{\tabcolsep}{2.75mm}{
%   \begin{tabular}{|c||ccccc|}
%       \toprule

%     Metric & $Acc_{7}\uparrow$ & $Acc_{2}\uparrow$ & $F1\uparrow$ & $MAE\downarrow$ & $Corr\uparrow$ \\
%     \midrule
%     \midrule
%         \multicolumn{6}{|c|}{\textbf{CMU-MOSEI Sentiment} (\textcolor{red}{Unimodal local data})}  \\
%   \midrule
%   \midrule
%     $TBJE^{A}$(20' ACL) $\Downarrow$ & 41.5($\pm$3.43E-5) & 68.3($\pm$4.15E-5) & 68.5($\pm$1.14E-5) & 0.843($\pm$5.48E-5) & 0.456($\pm$4.48E-5) \\
%     $TBJE^{P}$(20' ACL) $\Downarrow$ & 41.2($\pm$2.25E-5) & 67.4($\pm$1.84E-5) & 68.1($\pm$1.47E-5) & 0.863($\pm$5.15E-5) & 0.477($\pm$7.14E-5) \\
%     $MAT^{A}$(20' EMNLP) $\Downarrow$ & 39.4($\pm$2.31E-5) & 63.4($\pm$5.53E-6) & 64.7($\pm$3.14E-6) & 0.857($\pm$8.51E-5) & 0.422($\pm$4.77E-5) \\
%     $MAT^{P}$(20' EMNLP) $\Downarrow$ & 39.9($\pm$4.53E-5) & 66.8($\pm$4.22E-5) & 67.3($\pm$2.75E-5) & 0.844($\pm$2.76E-4) & 0.456($\pm$6.69E-5) \\
%     $MNT^{A}$(20' EMNLP) $\Downarrow$ & 38.8($\pm$1.70E-4) & 63.2($\pm$1.53E-4) & 64.6($\pm$8.93E-5) & 0.871($\pm$1.47E-5) & 0.376($\pm$1.31E-4) \\
%     $MNT^{P}$(20' EMNLP) $\Downarrow$ & 36.0($\pm$5.54E-5) & 63.1($\pm$3.82E-5) & 64.7($\pm$2.23E-5) & 0.964($\pm$7.47E-6) & 0.435($\pm$6.44E-4) \\
%     $Mult^{A}$(19' ACL) $\Downarrow$ & 42.7($\pm$1.49E-5) & 69.0($\pm$2.13E-5) & 72.7($\pm$2.26E-4) & 0.783($\pm$1.88E-5) & 0.374($\pm$1.30E-3) \\
%     $Mult^{P}$(19' ACL) $\Downarrow$ & 41.9($\pm$6.52E-6) & 65.5($\pm$7.43E-6) & 70.4($\pm$8.94E-5) & 0.806($\pm$4.96E-6) & 0.269($\pm$8.33E-5) \\
%       \midrule
%     \midrule
%       \multicolumn{6}{|c|}{\textbf{CMU-MOSEI Sentiment} (\textcolor{blue}{Ablation Study})}  \\
%           \midrule
%     \midrule
%     % L only(Transformer 17' NIPS)$\Downarrow$ & 44.7($\pm$1.2E-5) & 75.4($\pm$2.2E-5) & 76.2($\pm$1.5E-5) & 0.689($\pm$8.6E-4) & 0.551($\pm$1.1E-5) \\
%     % A only(Transformer 17' NIPS)$\Downarrow$ & 41.4($\pm$1.2E-5) & 65.6($\pm$1.9E-5) & 68.8($\pm$1.5E-5) & 0.764($\pm$7.9E-4) & 0.310($\pm$0.6E-5) \\
%     % V only(Transformer 17' NIPS)$\Downarrow$ & 43.5($\pm$0.4E-5) & 66.4($\pm$2.5E-5) & 69.3($\pm$4.1E-5) & 0.759($\pm$5.4E-5) & 0.343($\pm$2.1E-5) \\
%     % \midrule
%     % \midrule
%     only L \& A(ours) $\Downarrow$ & 47.8($\pm$4.22E-5) & 78.4($\pm$7.45E-5) & 78.1($\pm$8.93E-6) & 0.644($\pm$2.37E-5) & 0.614($\pm$1.75E-5) \\
%     only V \& L(ours) $\Downarrow$ & 46.5($\pm$8.95E-6) & 78.0($\pm$6.34E-5) & 78.0($\pm$2.26E-5) & 0.656($\pm$1.15E-5) & 0.606($\pm$9.83E-6) \\
%     only A \& V(ours) $\Downarrow$ & 42.3($\pm$1.23E-6) & 63.0($\pm$4.37E-5) & 76.4($\pm$3.24E-5) & 0.803($\pm$2.28E-5) & 0.195($\pm$1.42E-5) \\
%   \midrule
%     \midrule
%     HA-Fedformer $\Downarrow$ & 45.4($\pm$3.33E-5) & 77.7($\pm$3.71E-5) & 78.7($\pm$5.12E-7) & 0.662($\pm$3.26E-5) & 0.604($\pm$3.83E-5) \\
%     HA-Fedformer+ $\Downarrow$ & 46.7($\pm$3.72E-5) & 78.0($\pm$2.84E-5) & 78.4($\pm$9.76E-6) & 0.647($\pm$1.91E-5) & \textbf{0.625($\pm$4.14E-5)} \\
%     HA-Fedformer++(S) $\Downarrow$ & \textbf{49.1($\pm$4.42E-5)} & 78.5($\pm$2.74E-5) & 78.9($\pm$3.72E-5) & 0.639($\pm$1.26E-5) & 0.617($\pm$2.46E-5) \\
%     HA-Fedformer++ & 48.6($\pm$1.92E-6) & \textbf{79.1($\pm$2.25E-5)} & \textbf{79.2($\pm$2.92E-5)} & \textbf{0.638($\pm$7.43E-6)} & 0.624($\pm$3.94E-6) \\
%   \bottomrule
% \end{tabular}}
% \label{mosi}
% \end{table*}

\subsection{Benchmark results}
We evaluate \textit{HA-Fedformer} on two datasets, and the results are shown in Table \ref{mosei}, \ref{mosi}. 
Traditional multimodal learning methods applying FL can hardly achieve satisfactory results under the UTMP, e.g., the averaged $Corr$ for CMU-MOSEI and CMU-MOSI are $0.408$ and $0.376$, respectively.
In comparison, \textit{HA-Fedformer} can get a satisfying $Corr$ (e.g., $Corr$=0.625) on every benchmark dataset. The improvement upon the baseline methods is between 15$\%$-20$\%$ under a majority of evaluation metrics. Meanwhile, \textit{HA-Fedformer} can even achieve the result of $acc_{7}$\textgreater50.

Figure \ref{eval} (a) and (b) illustrate the validation losses of baseline methods under two datasets. We observe that it is difficult for traditional multimodal learning methods to converge under UTMP. Especially when using CMU-MOSEI, some baselines exhibit serious overfitting issues. While our proposed \textit{HA-Fedformer} can stably reduce the validation loss until the model converges.

% Moreover, we also observe empirically that \textit{HA-Fedformer} converges faster than any other baselines in Figure \ref{eval}. We may also surprisingly discover that FedProx fails to show superiority in unimodal training cases where model weights offset too much. This observation suggests that our proposed method not only improves model performance but also accelerates model training to a large extent.

We also measure the statistical significance of the results with
one-tailed Wilcoxon’s signed-rank test~\cite{wilcoxon1992individual}, and the test result is shown in Table 1.
Each method is compared with HA-Fedformer++, and $\Downarrow$ denotes 'significantly worse' with p$<$0.01. The test shows that our approach is significantly better than others, both on source and target.




\subsection{Ablation study}


We further conduct a comprehensive ablation analysis with two benchmark datasets. The results are shown in Table \ref{mosei}, \ref{mosi}, and Figure \ref{eval} (c).

First, we study the influence of data modalities on the performance baseline methods. We take L and A, V and L, A and V as local input data to train \textit{HA-Fedformer}. We can observe from the bottom of Table \ref{mosei}, \ref{mosi} that reducing the data of one modality will degrade the performance of the model, and the impact of removing the (L) data is the most obvious. Even using only two modalities of data, our method outperforms the vast majority of baselines, which further demonstrates the superiority of our method.

Second, we study the importance of hierarchical aggregation. From Table \ref{mosei}, \ref{mosi}, we can observe that after applying the hierarchical aggregation, all metrics can be significantly improved, especially for $Acc_{7}$ and $Corr$. This shows that the hierarchical aggregation can substantially alleviate the data non-IID and unaligned data sequences issues by providing a better representation of multi-modal data.


Finally, we examine the influence of choosing different sample times $S$ during PbEA, and the result is shown in Figure \ref{eval} (c). Following \cite{lakshminarayanan2017simple}, the default value of $S$ is set to 5, and we explore using different $S$ for training the model. We can observe that as $S$ increases, the model converges faster. However, when $S=10$, the training process falls into overfitting. We find the model obtains its best performance when $S=5$ and $S=7$. Considering the resource consumption, we empirically set $S=5$ throughout the experiments.

\begin{table}[t]
\centering
  \caption{Results for multimodal sentiment analysis on CMU-MOSEI under different modality Missing Rate (MR). All representations follow the same as in Table 1.}
\footnotesize
  \label{tab:freq}
  \resizebox{0.475\textwidth}{!}{
  \setlength{\tabcolsep}{0.9mm}{
  \begin{tabular}{|c||ccc|}
      \toprule

    MR ($Acc_{7}$) & $\Xi=0.7$ & $\Xi=0.5$  & $\Xi=0.3$ \\
    \midrule
    \midrule
        \multicolumn{4}{|c|}{\textbf{CMU-MOSEI Sentiment} (\textcolor{blue}{Ablation Study})}  \\
  \midrule
  \midrule
    FedMSplit& 41.6($\pm$1.7E-5) & 41.8($\pm$2.1E-5)  & 42.5($\pm$4.3E-5)  \\
    HA-Fed.++& 43.6($\pm$2.1E-4) & 44.3($\pm$2.4E-4)  & 47.8($\pm$7.8E-5)  \\
  \bottomrule
\end{tabular}}
}
\label{robust}
\end{table}


\subsection{Robustness analysis}
We also examine the robustness of HA-Fedformer with CMU-MOSEI when some modalities are missing.
We let $\Xi_{J}$ to indicate Missing Rate (MR) suggesting the probability a client does not have the modality-$j$ for inference. We set equal missing rates for each modality $\Xi_{1}=\Xi_{M}=\Xi$. As shown in Tab. \ref{robust}, HA-Fedformer still outperforms SOTA method FedMSplit~\cite{chen2022fedmsplit} under every scenario and is even competitive with FedMSplit when $\Xi=0$ ($Acc_{7}$=43.8). 

% It should also be noted that our model is in float32 and takes only 896KB of storage, suggesting that HA-Fedformer can be easily piggybacked on edge devices with quantization.}


\section{Conclusion and limitations}
In this paper, we proposed a Multimodal Federated Transformer with Hierarchical Aggregation. Unlike prior approaches that used multimodal data as local input, \textit{HA-Fedformer} for the first time solved the multimodal FL problem under the \textit{Unimodal Training - Multimodal Prediction} framework. Together with PbEA and CmDA, we further boost the performance of \textit{HA-Fedformer} to the point where it is compatible with SOTA multimodal works (i.e., not an FL setting). Although the uncertainty estimation in PbEA brings considerable computational cost and extra time consumption,
we believe \textit{HA-Fedformer}, with only 846KB,  opens up a new path for multimodal federated learning, making it no longer limited by the size or data modality. 

% \section{Appendix}

% In the supplementary material, we first present more details of our generated Foggy Nuscenes dataset in Sec .\ref{sec:1}, which aims at adding a foggy weather scene in Nuscenes \cite{caesar2020nuscenes} and providing an open source dataset (Foggy-Nuscenes) for research in autonomous driving. Secondly, in Sec .\ref{sec:2}, we show the details of each scenario and the partition of dataset. In Sec .\ref{sec:3}, the extra ablation studies are conducted on Day-night cross-domain scenario, which investigate the impact of each component. 


%%%%%%%%% REFERENCES
% \clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
