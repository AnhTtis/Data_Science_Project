\section{Case Study 3: Impact of Link Latency}
\label{sec:cs3}

So far, we have assumed there is no extra latency between the local (near) memory as DRAM cache and the far (remote)
memory as its backing store. In this case study, we will analyze the impact of latency of the link between these two nodes
on the system's performance. For this purpose, we consider the system shown in Figure \ref{fig:case-studies} on the bottom right.
\begin{figure}

    \subfloat[DDR4]{%
      \includegraphics[clip,width=\linewidth]{figures/cs3_bips_ddr4.pdf}%
      \vspace{-1em}
      \label{fig:cs3BipsDdr4}
    }
    \vspace{-1em} \\
    \subfloat[NVM]{%
      \includegraphics[clip,width=\linewidth]{figures/cs3_bips_nvm.pdf}%
      \vspace{-1em}
      \label{fig:cs3BipsNvm}
    }
    \caption{Performance of GAPBS and NPB on DRAM cache for 100 ns, 500 ns, and 100 ns round-trip
    link latency of the far memory.
    BIPS refers to a billion instructions per second. DDR4 and NVM technologies are used for remote main memory.}
    \label{fig:cs3BipsDDR4NVM}
    \vspace{-2em}
\end{figure}

% \Maryam{add CXL here} One way to use DRAM cache is to have it as a cache for memories that are lower bandwidth and larger capacity compared to DRAMs.
% This is highly desirable for disaggregated systems to attach remote large capacity memories through a link to the local
% DRAMs. Another use case is to have DRAMs as a cache for NVM memories. NVM memories are known to provide large capacity;
% however, they have limited bandwidth and higher latency compared to DRAM. Thus, DRAM caches, if designed properly,
% can help hide the higher latency and limited bandwidth of far and larger capacity memories.

%  We ask
% \emph{what is the impact of latency of the link between local DRAM caches and far backing stores on the
% performance of systems and does DRAM cache system perform better than the same system without DRAM cache?}

The DRAM cache model we described in this paper is capable of employing any memory technologies that are modeled in \gem{},
including DDR3, DDR4, HBM1, HBM2, and NVM. In this study, we use the baseline DRAM cache design (inspired by Intel's Cascade Lake). We use a single channel of HBM2 (2 pseudo-channels) DRAM cache
with a single channel of DDR4 remote backing store. In a second case, we change the remote main memory to a single channel of NVM. The DDR4 and NVM models of
\gem{} provide 19.2 GB/s theoretical peak bandwidth per single channel, though NVM has higher read and write latencies than DDR4.
% Both systems will have the same configuration except the technology used for the
% backing store as the far memory.
We also add a link between the DRAM cache manager and the remote backing store
with a configurable latency, as shown in Figure \ref{fig:case-studies} on the bottom right. We conduct a full-system simulation to run GAPBS and NPB on the two systems in three
different cases where the round-trip latency of the link will be set to 100 ns, 500 ns, and 1000 ns.
Similar upper bound latency numbers are reported for direct attached, serially attached, and network attached memory devices in the related industrial~\cite{gupta2020genz} and academic~\cite{maruf2022tpp} work.
%Directly attached memory refers to the traditional memory devices connected to the CPU using a parallel bus and might be shared among CPU sockets.
%Serially attached memory uses serial interfaces like CXL and will be inside the same chassis as the CPU.
%Network-attached memory devices do not have to be in the same chassis as the CPU and today can rely on interfaces like RDMA. However, we expect that CXL will also enable network-attached memory in the near future.
The rest of the methodology for the experiments remains the same as described in Section~\ref{method}.

\subsection*{Results and Discussions}

Figures \ref{fig:cs3BipsDdr4} and \ref{fig:cs3BipsNvm} show a billion instructions per second (BIPS) of DRAM cache systems while running GAPBS and NPB
workloads for DDR4 and NVM remote main memories, respectively.
For 100 ns link latency, the system with DDR4 performs better than the system with NVM.
This is expected given the higher read and write latencies of NVM devices compared to DDR4.

Once we increase the link latency to 500 ns, as shown in the figures, the DDR4 system still has higher performance compared to the NVM system.
However, the performance gap between the two systems in 500 ns is smaller than the gap in 100 ns link latency. 
Once the link latency increases to 1000 ns, for most of the workloads
the performance gap between DDR4 and NVM decreases compared to the 500 ns case.
As the link latency increases, the overall system performance drops in each system (DDR4 and NVM);
however, it drops less for the NVM system. 
With increased link latency both DDR4 and NVM remote memories have enough bandwidth to respond to the requests and the
latency of the NVM device gets amortized over the requests. 
Thus, the performance of the system with NVM drops less.

We also see in the figures that once the latency increase from 500 ns to 1000 ns the performances of $bt$ and $sp$
drop significantly in NVM system compared to DDR4 system.
These two workloads have the highest ratio of dirty line misses (Figure \ref{fig:cs1Mpki}). The DRAM
cache manager require to write the evicted dirty lines to the backing store. The limited write buffer of NVM
devices~\cite{wang2020characterizing} and the high link latency to access the remote NVM, create back pressure on the
NVM backing store. Thus, the performance drops significantly if the application is write intensive with a high miss
ratio for the NVM system.

Finally, we compare the performance of the DRAM cache systems to the same systems without the DRAM cache. Figures
\ref{fig:cs3SuDdr4} and \ref{fig:cs3SuNvm} show the speedup of the DRAM cache system compared to the same system
without the DRAM cache for DDR4 and NVM remote main memories, respectively. 
For GAPBS, Figure \ref{fig:cs3SuDdr4} shows once the link latency increases toward 500 ns and 1000 ns, the DRAM cache with DDR4 far main memory outperforms
the same system without the DRAM cache. However, as we observe in Figure \ref{fig:cs3SuNvm}, the DRAM cache with remote NVM main memory outperforms the same system without the DRAM cache for all the link latencies. The
differences between the read and write latencies of the DDR4 and NVM devices can explain the case for 100 ns link latency.
For NPB, the DRAM cache systems do not outperform the systems without the DRAM cache, for both DDR4 and NVM cases.
In Figures \ref{fig:cs1MissRatio} and \ref{fig:cs1Mpki} we observe that NPB is more write-intensive and has higher miss ratio compared to the GAPBS.
Thus, workloads in NPB generate more write backs to the main memory compared to GAPBS. Thus,
as the link latency grows, the pressure on the remote main memory increases. This becomes more critical for
NVM main memory systems due to their limited write buffer. This pressure does not exist in the system without DRAM cache.
Thus, NPB's performance drops as link latency increases in Figure \ref{fig:cs3SuNvm}.


% To better demonstrate how the NVM system's performance compares to the DDR4 system in these experiments, Figure \ref{fig:cs3suNvmDdr4}
% shows the speedup of NVM system compared to DDR4 system per benchmark suite for three link latencies. For speedup calculation,
% we used the geometric mean of all workloads' BIPS within each suite.
% For GAPBS, we see that as the link latency increases the speedup of the NVM system also increases. In other words, the NVM system's
% performance gets closer to the DDR4 system. For NPB, the figure shows as the link latency increases from 100 ns to 500 ns, the
% speedup also increases. However, once the latency increased from 500 ns to 1000 ns the speedup drops.
% The reason for this case is that the performance of two specific workloads in the suite, $bt$ and $sp$, drop significantly in NVM system with 1000 ns link latency, compared to DDR4 system.
% Our data shows these two workloads
% have the highest ratios of write-backs to the backing store. Given the limited write buffer NVM devices
% have (64 entries in our experiment) and adding the high link latency to access the NVM, has put back pressure on the backing store.

% As the link latency increases both DDR4 and NVM remote memories have enough bandwidth to respond to the requests and the
% latency of the NVM device gets amortized over the requests. Thus, the performance of the system with NVM grows
% towards the performance of the DDR4 system. This is shown in Figure \ref{fig:suNVM1000Cs3} for link latency of 1000 ns.
% As explained before, $bt$ and $sp$ have the highest write-back ratios, thus, they have less speedup than other workloads.

% Note that we used the baseline DRAM cache architecture for this experiments which can be optimized further in many ways, as
% discussed in the previous section, for future works.

\textbf{Takeaway:} This case study shows for systems with long latencies (e.g., 1000 ns) between local DRAM caches and remote main memories, NVM can perform close to
DDR4 devices. This can turn into an interesting use case to utilize large capacity of NVM devices. The study
also shows that for the workloads that are not write-intensive with high miss ratio, DRAM cache systems can outperform
the same system without the DRAM cache, if the remote main memory's link latency is higher than 100 ns.

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.48]{figures/cs3_su_geomean.pdf}
%     \vspace{-1ex}
%     \caption{Speedup of NVM compared to DDR4 for GAPBS and NPB.
%              Geometric mean of workload's BIPS has been calculated for overall performance for each benchmark suite. }
%     \label{fig:cs3suNvmDdr4}
%   \end{figure}


% \begin{figure}
%     \center
%     \subfloat[\scriptsize{GAPBS}]{
%     \includegraphics[width=.48\linewidth, scale=0.5]{figures/cs3_gap_suNVM_1000ns.pdf}
%     \label{fig:gapSuNVM1000Cs3}
%     }
%     \subfloat[\scriptsize{NPB}]{
%     \includegraphics[width=.48\linewidth, scale=0.5]{figures/cs3_npb_suNVM_1000ns.pdf}
%     \label{fig:npbSuNVM1000Cs3}
%     }
%     \caption{Speedup of NVM system compared to DDR4 system for link latency of 1000 ns.}
%     \label{fig:suNVM1000Cs3}
% \end{figure}

\begin{figure}
    \center
    \subfloat[\scriptsize{DDR4}]{
    \includegraphics[width=.49\linewidth]{figures/cs3_su_geomean_mm_ddr4.pdf}
    \label{fig:cs3SuDdr4}
    }
    \subfloat[\scriptsize{NVM}]{
    \includegraphics[width=.49\linewidth]{figures/cs3_su_geomean_mm_nvm.pdf}
    \label{fig:cs3SuNvm}
    }
    \caption{Speedup of DRAM cache system backed up by (a) DDR4 remote memory and (b) NVM remote memory compared to the same system without the DRAM cache.
    Geometric mean of workloads' throughput is used for speedup.}
    \label{fig:cs3SuDcacheMm}
    \vspace{-1.5em}
\end{figure}
