\section{Introduction}

Today's computing systems must meet the large processing power and memory capacity demands of a broad range of applications including machine learning, artificial intelligence, graph analytics, etc.
These applications demand both large capacity and high performance from their main memory technology.
Unfortunately, memory technology is not a panacea and high-performance memory (e.g., HBM) comes with low capacity and high-capacity memory (e.g., NVRAM) comes at the cost of lower performance.
Current high bandwidth memory technologies provide limited capacities such as HBM. Other memory technologies with large capacity usually have limited bandwidth and high latency, such as non-volatile memories (NVRAM).
Thus, to provide both high-performance and high-capacity memories to fulfill applications' memory requirements, vendors have moved to heterogeneous memory systems where the processing units can use multiple memory devices.
This trend is accelerating with new interconnect technologies such as Compute Express Link (CXL) which allows systems to have both local memory and remotely attached memory devices.
For instance, Intel's Sapphire Rapids will provide HBM2, DDR5, 3DXPoint, and CXL support within the same system.
Another trend to respond to applications' memory demand is disaggregated memory systems where they employ different memories and interconnects in their local or remote nodes.
These kinds of disaggregated memory systems with local and CXL-attached memories are anticipated to be highly employed in cloud services~\cite{lipond}.

One way to implement these heterogeneous memory systems without burdening the programmer with manual data movement is to use the faster memory as a cache of the slower memory.
Hardware-managed DRAM caches are seen as one way to enable heterogeneity and disaggregation in memory systems to be more easily programmable.
These \emph{DRAM-based} caches can hide the latency of lower bandwidth or higher latency memories in heterogeneous systems.
The last decade has seen significant academic research on DRAM caches, and today these ideas are becoming a reality with CPU vendors implementing DRAM cache-based computer systems, e.g., Intel's Cascade Lake and Sapphire Rapids~\cite{sapphire}.

Despite the large body of research on DRAM caches, recent work has shown that these transparent hardware-based data movement designs are much less efficient than manual data movement~\cite{hildebrand2021case}. While the recent work investigating Intel's Cascade Lake systems provides some insight into real implementations on DRAM caches~\cite{hildebrand2021case,izraelevitz2019basic,wang2020characterizing}, there is a gap in the community's access to cycle-level simulation models for DRAM caches for further studies.

This paper describes a new \gem{}-based DRAM cache model capable of modeling different topologies of DRAM cache designs. We mainly enable design space exploration of a unified (inspired from Intel Cascade Lake) or a disaggregated (useful for heterogeneous and disaggregated memory systems) DRAM cache hardware.

Previous work has explored many aspects of DRAM cache design in simulation such as the replacement policy, caching granularity~\cite{qureshi2012fundamental,jevdjic2013stacked}, DRAM cache tag placement~\cite{huang2014atcache,loh2012supporting,loh2011efficiently}, associativity~\cite{qureshi2012fundamental,kotra2018chameleon,young2018accord}, and other metadata to improve performance~\cite{loh2011efficiently,jevdjic2013stacked,young2018accord}.
Trace-based or non-cycle-level simulation can appropriately evaluate these mostly high-level memory system design investigations. However, as shown in recent work, the micro-architecture of the DRAM cache interface and controller can lead to unexpected performance pathologies not captured in these trace-based simulation studies. For instance, a dirty miss to the DRAM cache requires up to \emph{five accesses} to memory in Intelâ€™s Cascade Lake~\cite{hildebrand2021case}.
This interference between extra accesses to the DRAM to fulfill a single demand access causes performance degradation not seen in prior simulation studies~\cite{hildebrand2021case}.

To better understand these realistic DRAM cache systems, it is imperative to build a detailed DRAM cache simulation model to perform a design space exploration of DRAM caches for emerging memory systems. The previous research works on DRAM cache design improvements do not provide any (open source) DRAM cache modeling platform for the community to perform a detailed micro-architectural and timing analysis. In this work we describe a cycle-level model of DRAM caches for heterogeneous and disaggregated memory systems in the \gem{} simulator~\cite{lowepower2020gem5}.

Our model is capable of implementing different caching policies and architectures. The baseline protocol of our model takes inspiration from the actual hardware providing DRAM cache in Intel's Cascade Lake which has direct-mapped, insert-on-miss, and write-back caching policy, and tags and metadata are stored in the ECC bits along with the data in the same cache location. Our model leverages the cycle-level memory models of \gem{} which include DRAM~\cite{hansson2014simulating} and NVRAM~\cite{gem5-workshop-presentation} models in \gem{}. Our model implements the timing and micro-architectural details enforced by the memory interfaces including the memory technology timing constraints, scheduling policies, and internal buffers. This enables a cycle-level full system analysis of the DRAM caches in emerging memory systems, which is not possible by the prior works. To demonstrate such capabilities, we investigate three case studies.

First, we answer the question of \emph{how a DRAM cache system's performance compares to those without a DRAM cache.}
We find out for the baseline DRAM cache architecture (similar to Intel's Cascade Lake), the system with the DRAM cache has a lower throughput than the same system without a DRAM cache.
We show similar results to the DRAM cache hardware studies. We demonstrate that the interference between the demand accesses and the extra accesses to the DRAM cache and main memory devices to respond to the demand causes bandwidth under-utilization for both devices.

In the second case study, we investigate optimized DRAM cache designs to mitigate the overheads observed by the baseline DRAM cache architecture in the first case study.
First, we implement the technique proposed by BEAR DRAM cache~\cite{chou2015bear} on top of the baseline design which optimizes the write demands that hit on DRAM cache.
Second, on top of the first optimization, we implement another optimized design which
optimizes write hits as well as all demands that miss on the DRAM cache to a clean cache line. We show that both designs provide better throughput compared to the baseline
architecture and the second optimized design outperforms the first one. However, a system without a
DRAM cache still outperforms both optimized designs.

In the third case study, we answer the question \emph{what is the impact of latency of the link between local DRAM caches and remote backing stores on the
performance of systems?} For this study we consider the baseline DRAM cache design, once backed up by a DDR4 far memory, then by a NVRAM far memory.
We find that as the far memory link latency increases, the system throughput drops.
However, with increased link latency, the performance of the NVM system drops less than the DDR4 system.
Moreover, we find out once the link latency of the backing store increases, DRAM cache finally outperforms the same systems without a DRAM cache.


We have made the DRAM cache model presented in this work open-source and publicly available to the research community~\cite{dcacheGem5Code}. This work will be integrated into \gem{} mainstream. We believe this model enables community to perform research for future heterogeneous and disaggregated memory systems supporting DRAM caches.
