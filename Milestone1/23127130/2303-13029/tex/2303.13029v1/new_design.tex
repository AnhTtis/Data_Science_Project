\section{Design of DRAM cache model} \label{design}
% We strive to support a generic DRAM cache model. For this purpose, our design models two main scenarios which are flexible enough to cover a set of different DRAM cache designs:

We strive to support a generic DRAM cache model. For this purpose, we implement a discrete DRAM cache model which is flexible enough to cover a set of different DRAM cache designs.
This model relies on a separate DRAM cache manager which interacts with a near/fast/local memory controller (for a DRAM cache) and a far/slow/remote memory controller (for the backing store). This model does not require DRAM cache and the backing store to share a data bus.

A prior work proposed a unified DRAM cache controller to model the hardware of Intel's Cascade Lake~\cite{babaie2022cycle}.
It implements only one DRAM cache architecture where a DDR4 and NVM memory devices as the cache and the main memory are controlled by the same controller and share the data bus. This model is not flexible and generic to simulate different DRAM cache designs.
Our model in this work is able to cover this case. Since the discrete DRAM cache model is more flexible and can model future systems with DRAM cache designs, we will focus on that model in this paper.

% \begin{itemize}
% \item Discrete DRAM cache model: This model relies on a separate DRAM cache manager which interacts with a near/fast/local memory controller (for a DRAM cache) and a far/slow/remote memory controller (for the backing store). This model does not require DRAM cache and the backing store to share a data bus.
% \item Unified DRAM cache model: This is a model proposed by a prior work which uses a unified DRAM cache controller. It implements only one caching policy and also controls a DDR4 and NVM memory devices as the cache and the main memory while sharing the data bus~\cite{babaie2022cycle}.
% \end{itemize}

\subsection{Background on \gem{}'s memory subsystem}

The \gem{} simulator is based on an event-driven full-system simulation engine.
It supports models of many system components, including memory controllers, memory device models, CPUs, and others.
The original memory controller module added to \gem{} by Hansson et al.~\cite{hansson2014simulating} is a \emph{cycle level} memory controller model designed to enable fast and accurate memory system exploration. The memory controller in \gem{} was refactored in~\cite{gem5-workshop-presentation} where two components were defined to simulate any controller.
(1) The \textit{memory controller} receives commands from the CPU and enqueues them into appropriate queues and manages their scheduling to the memory device.
(2) The \textit{memory interface} deals with device-specific timings and operations and communicates with the memory controller.
Moreover, the most recent release of \gem{} further improved the flexibility and modularity of memory controller and interfaces~\cite{akram2022modeling} and added HBM2 controller and interfaces.
Most importantly, it provides support for HBM2 interface and memory controller, where each physical channel is consisted of two pseudo-channels with peak theoretical bandwidth of 32 GB/s per channel.
The DRAM cache model presented in this work can employ any of the memory controller and interface model of \gem{} without any modifications.
Like \gem{}'s current memory controller, our DRAM cache model's goal is for cycle-level simulation to enable micro-architectural exploration and flexibility, not cycle-by-cycle accuracy to a single design.

\subsection{DRAM Cache Model}

Figure~\ref{fig:dcache} shows an overview of the DRAM cache model we implement in this work. DRAM cache manager receives all the incoming memory traffic (coming from the CPU, LLC, DMA).
This cache manager is not in the on-chip coherence domain and can be a drop-in replacement for the memory controller.

The DRAM cache manager is responsible for implementing different DRAM cache policies and interacts with two controllers.
In this work, near memory refers to the memory device which acts as a cache of the far memory (or the backing store).
The DRAM cache manager sends requests to and receives responses from the near and far memories. These requests include reads and writes
to the near and far memories and receiving a response for read requests and an acknowledgement for the write requests.
We allow the use of any memory controller in \gem{} as local and far memory controllers.
The \gem{} memory controllers take care of all the device-specific timing control and the DRAM cache manager allow us to isolate all DRAM cache specific controls.
This isolation makes the model modular and generic to implement different DRAM cache policies and architectures, as shown in the three case studies.

Our goal is to keep our DRAM cache model flexible enough to be able to simulate different DRAM cache designs.
Therefore, instead of modeling one specific micro-architecture with buffers for every device, we abstract away the hardware resources required to implement a DRAM cache controller by the use of two simple buffers: \textit{Outstanding Requests Buffer (ORB)} and \textit{Conflicting Requests Buffer (CRB)} shown in Figure~\ref{fig:dcache}.
All incoming memory requests reside in the \textit{ORB} unless a request conflicts with an already existing request in the \textit{ORB}.
Two requests are conflicting if they both map to the same location in the DRAM cache.
The conflicting request goes to the \textit{CRB} until the request it was conflicting with has been serviced and is taken out of the \textit{ORB}.
Each entry in these buffers contains other metadata in addition to the address of the request, as shown in Figure~\ref{fig:dcache}.
This metadata provides helpful information about the request, e.g., current state, and relative arrival time.
We also model a \textit{Write Back (WB) Buffer} for the DRAM cache dirty lines that are to be written back to the backing store.
What each entry in these buffers holds depend on the DRAM cache architecture and our model is flexible.


  % \begin{figure*}
  % \centering
  % \scriptsize{Unified cache controller layout}{
  %   \fbox{\includegraphics[scale=0.6]{figures/simulatedHW.jpg}}
  %   \label{fig:buffers}
  % }
  %  \scriptsize{State machine of unified cache controller}{
  %   \fbox{\includegraphics[scale=0.5]{figures/protocol.jpg}}
  %   \label{fig:StateMachine}
  % }
  % \caption{Unified DRAM cache controller design}
  % \label{fig:DController}
  % \end{figure*}


% \begin{figure}
%   \centering
%   \includegraphics[scale=0.45]{figures/simulatedHW.jpg}
%   \vspace{-1ex}
%   \caption{Hardware abstraction of \textit{UDCC}. (\Ayaz{Need to update the figure}) (\Maryam{Do we need to show the hardware and buffers?}). These buffers are implemented in the DRAM cache manager or the unified DRAM cache controller.}
%   \label{fig:buffers}
% \end{figure}


% \begin{figure}
%   \centering
%   \includegraphics[scale=0.4]{figures/design.jpg}
%   \vspace{-1ex}
%   \caption{Overview of the discrete DRAM cache model. DRAM cache manager is responsible for implementing different DRAM caching policies and interacting with memory controllers of near and far memory.}
%   \label{fig:dcache}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[scale=0.4]{figures/state_machine.jpg}
%   \vspace{-1ex}
%   \caption{Example of a state machine diagram implemented by the DRAM cache manager.}
%   \label{fig:StateMachine}
% \end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{figures/hardware.pdf}
  \vspace{-1ex}
  \caption{Overview of DRAM cache manager hardware.
  DRAM cache manager implements different DRAM caching policies and interacts with memory controllers of DRAM cache and its backing store.
  It includes three main buffers: (i) Outstanding Requests Buffer (ORB), (ii) Conflicting Requests Buffer (CRB), (iii) Write Back (WB) Buffer.
  }
  \label{fig:dcache}
  \vspace{-1.5em}
\end{figure}

Figure~\ref{fig:dcache} also shows a state machine and a memory request goes through different steps while in the DRAM cache manager.
These steps depend on the DRAM cache design and the state machine contains the logic for implementing them.
Our model uses a tag and metadata storage in the DRAM cache manager to track the status of the DRAM cache locations.
This separate storage allows flexible implementation of different DRAM cache designs.
The model assures the timing implementation of the architecture as if this storage does not exist. Thus, the performance of each design is captured.

The steps in Figure~\ref{fig:dcache} represents an example of a DRAM cache design which is inspired by the real DRAM
cache implemented by Intel's Cascade Lake.
%For a different design, the state machine should be modified and our model comes with APIs to handle this.
%\Maryam{come back here if you have time}
For the baseline architecture, we implement a DRAM cache which is direct-mapped (with caching granularity of 64 bytes), inserts on misses, and writes back the dirty cache lines upon evictions.
The tag and metadata are stored in ECC bits alongside the data.% in the same cache line.

Since \gem{} is an event driven simulator, the simulation model relies on scheduled events to transition between different states.
Below is an example of the state machine shown in Figure~\ref{fig:dcache}:

\paragraph*{Initializing a Request}
\textcircled{1} The CPU package (cores or LLC) sends a request to the main memory.
\textcircled{2} The DRAM cache manager (as the replacement for the memory controller) receives this request and places it in the ORB (if not conflicting with an existing request in the ORB). In case of writes, it sends an acknowledgement to the CPU/LLC.
\textcircled{3} If a conflict exists, the DRAM cache manager holds it in the CRB until the conflicting request leaves ORB.

\paragraph*{Request to the Local Memory}
\textcircled{4} Based on a scheduling policy (such as First-Come First-Serve) the DRAM cache manager sends a read request to the DRAM cache controller for tag check, and it receives
the response whenever it is ready. This response will provide the whole cache line that the demand access maps to in the DRAM cache, including data, tag and metadata.
If the tag matches, it is a hit, otherwise it is a miss on DRAM cache.
In case of read demand hit, the DRAM cache manager sends the response to the CPU/LLC.
In case of write demand hit or miss, the DRAM cache manager sends a write request with the data from demand access to the DRAM cache controller.
\textcircled{5} If any of the misses had a dirty flag set in the metadata, the DRAM cache manager inserts that cache line into the WB buffer.
Whenever there is bandwidth available or if the WB buffer becomes full, the DRAM cache manager sends these write backs as a write request to the main memory controller.

\paragraph*{Request to the Remote Memory}
\textcircled{6} In case of read demand miss, the DRAM cache manager sends a read request to the main memory controller to fetch the missing cache line from backing store.
Once the DRAM cache manager receives the response from main memory controller, it finishes the miss handling process.
First, \textcircled{7} it sends a write request to the DRAM cache controller to fill the missing cache line.
Second, \textcircled{8} it sends the response for the demand to CPU/LLC.

% \noindent
% \textbf{Receive Request}: All memory requests (from the CPU package) are received by the DRAM cache manager and placed in the \textit{ORB} (if not conflicting with an existing request in the \textit{ORB}).
%
% \noindent
% \textbf{Local DRAM Read:} Since every request must first check the tag in the DRAM cache, a memory packet moves to the \textit{DRAM Read} state and is sent to the local memory controller. Local memory controller will perform the DRAM read operation and send the response back to the DRAM cache manager. This operation is not needed if the designed DRAM cache model does not have the tag-store in the DRAM itself.
%
% \noindent
% \textbf{DRAM Read Response:} Once the response of a DRAM (cache) read is received (this operation models a tag read), the DRAM cache manager checks the DRAM cache tags to ascertain the hit/miss and clean/dirty status of the request and perform needed actions accordingly.
% For example, in case of read request if the tag check shows a hit there is no need to perform any other operation as the DRAM read would have brought the correct data along-with the tags.
%
% \noindent
% \textbf{Far Mem Read:} If the tag check by the DRAM cache manager indicates a DRAM cache miss for a read or a write request (if allocate on a write-miss policy is used), the DRAM cache manager sends a read request to the far memory controller. If the original request was a read request, on getting the response back from the far memory, the response is also sent to the requestor.
%
% \noindent
% \textbf{Local DRAM Write:} DRAM cache manager sends a local DRAM write request in multiple cases. For example, if the tag read for a DRAM cache write request indicates a hit, the cache manager sends the write request to local DRAM controller to perform actual data write. Similarly, in case of a DRAM cache miss and then a read from the far memory, DRAM cache manager sends a write request to local memory controller to perform cache fill operation.
%
% \noindent
% \textbf{Far Mem Write:} DRAM cache manager sends write requests to far memory controller only if there is a dirty miss in the DRAM cache. This operation leads to writing of dirty dirty line is written back to the far memory.
%
% \noindent
% \textbf{Done:}  When a memory request is fully serviced, it moves to the \textit{Done} state and the request is eventually removed from \textit{ORB}.

% \subsection{Unified DRAM cache model}
% Unified DRAM cache is largely based on Intel Cascade Lake's DRAM cache management strategy.
% This model uses a unified DRAM cache controller which manages both local and far memory devices/interfaces.
% This model does not require separate memory controllers but is less flexible.
