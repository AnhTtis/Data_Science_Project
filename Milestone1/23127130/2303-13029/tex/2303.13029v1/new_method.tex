\section{Methodology}
\label{method}

% We explain our simulation methodology for evaluating DRAM cache designs using our DRAM cache model.

\begin{figure}
  \centering
  \subfloat[Target AMD EPYC-like system.]{
    \centering
  \includegraphics[width=0.44\linewidth]{figures/whole-system}
  \label{fig:whole-system}
  }
  \hfill
  \subfloat[A $\nicefrac{1}{8}$ system used in simulation. The system used in each case study is shown.]{
    \centering
  \includegraphics[width=0.48\linewidth]{figures/case-studies}
  \label{fig:case-studies}
  }
  \caption{(a) Target system: AMD EPYC-like system, which contains eight cores per CCD. CCDs are connected to the eight main memory channels via an I/O die.
  The memory system contains an HBM stack which can act as a cache to the 8-channel DDR4-based main memory.
  We model $\nicefrac{1}{8}$ as shown on the right.}
  \label{fig:system}
  \vspace{-1.5em}
\end{figure}


\subsection{Modeled System}
Figure~\ref{fig:whole-system} shows the target system we simulated for our experiments.
We consider $\nicefrac{1}{8}$ of a single socket of an AMD EPYC-like system~\cite{naffziger2021pioneering} which consists of a single
core complex die (CCD) and a single channel of main memory, as shown in Figure~\ref{fig:case-studies} on the top. The modeled CCD contains eight cores that have private L1 caches and share the last level cache among the cores.
The main reason for our decision to model $\nicefrac{1}{8}$ of the system is that with the current core and on-chip cache models of \gem{}, it will take a long simulation time to model the entire system in a simulated environment.

To incorporate our DRAM cache model with this system, we replace the memory controller with a DRAM cache manager,
as shown in Figure~\ref{fig:case-studies} on the bottom. For the baseline, the cache manager uses a single channel of HBM2
(which consists of two pseudo-channels and acts as a DRAM cache), and a single channel of DDR4 (which serves as the main memory in the system).
Note that in these figures the memory interfaces and their controllers are shown in the same box. Table \ref{tab:baseConfig} summarizes the baseline system configuration.

% The type of memory technologies for DRAM cache and its backing store can change based on the experiment. Moreover, the connection between
% the cache manager and DRAM cache and the backing store has a configurable latency. In case studies 1 and 2, we assumed no extra latency for this connection.
% In case study 3, we varied the latency of the backing store's connection as the remote memory.

\begin{table}[!h]
  \begin{center}
  \caption{\label{tab:baseConfig}Baseline System Configuration}
  \begin{tabular}{ |p{3.5cm}||p{2.5cm}|}
    \hline
    \multicolumn{2}{|c|}{Processors} \\
    \hline
    Number of cores & 8\\
    Frequency & 5 GHz\\
    \hline
    \multicolumn{2}{|c|}{On-chip Caches} \\
    \hline
    Private L1 Instruction & 32 KB\\
    Private L1 Data & 512 KB\\
    Shared L2 & 8 MB\\
    \hline
    \multicolumn{2}{|c|}{DRAM Cache Manager} \\
    \hline
    ORB & 128 entries\\
    CRB & 32 entries\\
    WB Buffer & 64 entries\\
    Frontend/Backend Latencies& 20 ns round-trip\\
    \hline
    \multicolumn{2}{|c|}{DRAM Cache (HBM2)} \\
    \hline
    Capacity & 128 MB\\
    Theoretical Peak Bandwidth & 32 GB/s\\
    Read/Write Buffer & 64 entries each\\
    \hline
    \multicolumn{2}{|c|}{Main Memory (DDR4/NVM)} \\
    \hline
    Capacity & 3 GB\\
    Theoretical Peak Bandwidth & 19.2 GB/s\\
    Read/Write Buffer & 64 entries each\\
    \hline
  \end{tabular}
  \end{center}
  \end{table}

\subsection{Workloads used}
We evaluated DRAM caches using a subset of multithreaded workloads in NPB~\cite{bailey1991parallel} and GAPBS~\cite{beamer2015gap} that assess high-performance systems.
We use the C class of the NPB workloads and a synthetic graph as an input (size of $2^{22}$ vertices) for the GAPBS workloads.
These workloads' working-set sizes varies between few hundreds MB to 1 GB. Thus, we set the size of the DRAM cache and the main memory to 128 MB and 3 GB, respectively, so the DRAM cache is smaller than the workloads' memory footprints.

\subsection{Simulation methodology for benchmarks}
Figure~\ref{fig:method} shows our methodology for the experiments we ran in this paper.
First, Linux kernel boots on the target system in \gem{} and the execution of the program starts and continues until the start of the region of interest (ROI) of the workload. The benchmarks are marked with ROI begin markers using \gem{} pseudo instruction support.
Beginning at the ROI, we simulate the workload for 100ms to warm up the system including the DRAM cache. At the end of 100ms, we take a checkpoint. This process is done once per workload. Later, we restore from the checkpoint to run all our simulations with different DRAM cache configurations.
Using a checkpoint ensures that all of our runs have the same starting point with the same system state for a fair comparison across different tested configurations.
% Without the checkpoints, mainly because we simulate multithreaded workloads on a multicore system, the starting system state for the detailed simulation can be different across different runs.
We simulate the restored checkpoint for either one second of simulation time or reaching the end of ROI, whichever comes first.
Our results show an average cold-miss ratio of 3.5\% for DRAM cache, during the restore simulation.

Figure~\ref{fig:trafGen} demonstrates a validation of our DRAM cache model. We used a traffic generator to replace LLC/CPU side in Figure~\ref{fig:dcache}. This traffic generator creates synthetic memory traffic configurable for read/write percentage, random/linear pattern, etc. We used a single physical channel of HBM2 (32 GB/s peak bandwidth) as the DRAM cache and a single channel of DDR4 (19.2 GB/s peak bandwidth) as the main memory. We controlled the miss ratio,
percentage of read requests, and the ratio of dirty or clean cache lines in case of misses. We ran the tests for 10 ms, enough to reach the saturated bandwidth in all cases.
%was enough for all cases to reach the saturated bandwidth and
%keeping the pipeline of the controllers full.
For read-only (RO) traffic with a 100\% hit ratio, the DRAM cache should perform the same as the main memory, which is observed in Figure~\ref{fig:trafGen}. DRAM cache shows effective traffic of 29.94 GB/s (similar to \gem{} HBM2 memory controller effective bandwidth).
%We observe in Figure~\ref{fig:trafGen} that for this case, the effective traffic is
%29.94 GB/s. The HBM2 memory controller of \gem{} also provides the same utilized bandwidth
%for a similar pattern.
As we add more writes to the accesses and test with 67\% read (33\% write) and 100\% write patterns, the effective bandwidth drops as write hits require two accesses to the DRAM cache.
The effective bandwidth drops further for the 100\% miss ratio due to more extra accesses each demand request needs to make to the DRAM cache and the backing store.
%As we incorporate more write accesses to the 100\% hit ratio,
%the effective bandwidth drops for 67\% read (the remaining 33\% are writes) and write-only (WO)
%since write hits require two accesses from the DRAM cache. For 100\% miss ratios in all six cases,
%the effective traffic drops due to the extra accesses each request needs to do in the DRAM cache and
%the backing store.
For the Miss-Dirty case, there is a write-back to the main memory. However,
the main memory controller has enough bandwidth to handle these writes while the DRAM cache
pipeline is saturated.
% Thus, these writes are not on the critical path for this test.
%\Maryam{this needs be read by another reader too}

\begin{figure}
    \centering
    \frame{\includegraphics[width=0.7\linewidth]{figures/methodology.pdf}}
    \caption{Summary of the simulation methodology. Ckpt: checkpoint, ROI: region of interest. We use a warm-up period of 100ms and detailed simulation time of 1s.}
    \label{fig:method}
    \vspace{-1.5em}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/traf_gen.pdf}

  \caption{Effective traffic at the LLC. Synthetic traffic was injected to the DRAM cache manager.
  The traffic was controlled for percentage of read/writes, miss ratio, and clean or dirty line eviction ratio.}
  \label{fig:trafGen}
  \vspace{-1.5em}
\end{figure}

% \subsection{Traffic generator based studies}
% In addition to the use of benchmarks or real workloads, we rely on \gem{}'s traffic generators to create synthetic traffic patterns.
% Using traffic generators allows us to explore the behavior of the DRAM cache design more directly and more clearly understand the fundamental trade-offs in its design.
% We use \gem{}'s traffic generator to generate two patterns of addresses: linear or random.



