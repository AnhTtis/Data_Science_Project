\section{Case Study 2: Cache Architecture Exploration}
\label{sec:cs2}

In previous section we showed the performance degradation of the baseline system with DRAM cache (Figure \ref{fig:case-studies} bottom left) compared to the same system without the DRAM cache (Figure \ref{fig:case-studies} top). In this study,
we investigate optimized DRAM cache designs to mitigate the overheads shown in Section \ref{cs1}.
The baseline DRAM cache protocol of our model which is used in Section \ref{cs1} is inspired by the real hardware
of Intel's Cascade Lake. However, our model is capable of implementing different DRAM cache
designs. To show this capability, in this section we show two different cache architecture optimizations
on top of the baseline model.

% For this purpose we focus on optimizing the initial read access to the DRAM cache for tag check,
% since in the baseline model the tag and metadata are part of ECC, stored along with the data within the same cache line.

%We get the motivation for the two optimizations from the results shown in Figures \ref{fig:cs1Mpki} and \ref{fig:cs1Hpki}.
As explained in the Section \ref{design}, the baseline DRAM cache architecture stores the tag and metadata in ECC bits in the cache line, along with the data.
Thus, to check whether a memory request is hit or miss on the DRAM cache, it reads the entire cache line from DRAM, while
only the tag and metadata is needed for tag comparison. If the memory request is miss clean (for both read and write requests) or write hit on DRAM cache,
reading the data becomes a waste of bandwidth. We elaborate the details of these cases as follows:

\paragraph {Write Hits}
Figure \ref{fig:cs1Hpki} shows write hit requests consist a noticeable
portion of total hit accesses. For these accesses in the baseline design, cache manager reads the cache line from DRAM cache for tag comparison.
Once the cache manager receives the cache line and the tag in the request address and the tag in the cache line match, it sends a write request with the data from demand access to the cache line address.
In this process, the data in the cache line fetched for tag check is never used and is overwritten, eventually. Figure \ref{fig:cs1Hpki} shows 
write hit accesses consist a large part of all hit accesses.

\paragraph {Read or Write Miss-Cleans}
%This is more noticeable for read requests, but the write requests are also contributing to this portion.
As we described above for write hits, the DRAM cache manager reads the entire cache line that the request maps to for tag check.
Once the tag comparison in the DRAM cache manager fails to match, if the request is a write, the cache manager sends a write request with the data from the demand access to the DRAM cache line address.
If the request is read, the DRAM cache manager sends a read request to the backing store to get the missing cache line from the main memory.
In these processes, the data in the cache line which was fetched for tag check is never used. 
Figure \ref{fig:cs1Mpki} shows that significant portions of the total memory requests that miss on DRAM cache consist of miss accesses to clean cache lines.


Given these two scenarios, where there is a bandwidth waste by the baseline DRAM cache, we investigate two different designs for the DRAM cache state machine.
First, we implement the optimization introduced by the BEAR cache for some write accesses~\cite{chou2015bear}.
BEAR cache tries to avoid reading the line from DRAM cache for tag check, for the write hit accesses to the DRAM cache.
BEAR determines the write hit accesses using the metadata stored in the last level cache.
We apply this optimization to the baseline architecture of our model and name this case \emph{\bear{}}.

Second, we change the baseline DRAM cache design in a way that avoids the read access to DRAM cache for tag
check, not only for write hit demands, but also if the demand access (either read or write) will miss on DRAM cache and the cache line is
clean. We call this case \emph{\oracle{}}. We assume \oracle{} has a zero latency SRAM storage in the cache manager to hold all the tag and metadata
it needs to determine if the demand access will hit or miss to a clean or dirty cache line. Table \ref{tab:accAmp} summarizes the optimizations of \bear{} and \oracle{} compared to the baseline in terms of
reducing extra accesses from pathology of DRAM cache design in all possible memory request cases.

To compare the performance of these three different designs (baseline, \bear{}, and \oracle{}), we run the GAPBS and NPB through full-system simulation
as explained in Section \ref{method}. In all three cases, the cache manager uses HBM2 DRAM cache and DDR4 main memory.

\begin{table}[!h]
    \begin{center}
    \caption{\label{tab:accAmp} Comparison of number of extra accesses needed for a given memory request in
    Baseline, \bear{}, and \oracle{} designs, written in black, blue, and red, respectively.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}[width=\linewidth]{| c | c | c | c | c | c | c | c | c |}
      \hline
      Access & \multicolumn{4}{c|}{Read} & \multicolumn{4}{c|}{Write}\\
      \hline
      Hit/Miss & \multicolumn{2}{c|}{Hit} & \multicolumn{2}{c|}{Miss} & \multicolumn{2}{c|}{Hit} & \multicolumn{2}{c|}{Miss}\\
      \hline
      Dirty/Clean & \multicolumn{1}{c|}{Dirty} & \multicolumn{1}{c|}{Clean} & \multicolumn{1}{c|}{Dirty} & \multicolumn{1}{c|}{Clean} & \multicolumn{1}{c|}{Dirty} & \multicolumn{1}{c|}{Clean} & \multicolumn{1}{c|}{Dirty} & \multicolumn{1}{c|}{Clean}\\
      \hline
      Local Read &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{55}} &
       \ding{51} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{51} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{55}} \\
      \hline
      Far Read &
      \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
      \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} \\
      \hline
      Local Write &
      \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
      \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} \\
      \hline
      Far Write &
      \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
      \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} &
       \ding{51} \textcolor{blue}{\ding{51}} \textcolor{red}{\ding{51}} &
       \ding{55} \textcolor{blue}{\ding{55}} \textcolor{red}{\ding{55}} \\
      \hline
      \hline
      Tot. Baseline & 1 & 1 & 4 & 3 & 2 & 2 & 3 & 2 \\
      \hline
      Tot. \bear{} & \textcolor{blue}{1} & \textcolor{blue}{1} & \textcolor{blue}{4} & \textcolor{blue}{3} & \textcolor{blue}{1} & \textcolor{blue}{1} & \textcolor{blue}{3} & \textcolor{blue}{2} \\
      \hline
      Tot. \oracle{} & \textcolor{red}{1} & \textcolor{red}{1} & \textcolor{red}{4} & \textcolor{red}{2} & \textcolor{red}{1} & \textcolor{red}{1} & \textcolor{red}{3} & \textcolor{red}{1} \\
      \hline
    \end{tabular}
    }
    \end{center}
\end{table}

\subsection*{Results and Discussions}

Figure \ref{fig:cs2SuAll} shows the performance speedup of \oracle{} and \bear{} compared to the baseline.
\oracle{} and \bear{} outperform the baseline architecture. The speedup gain for
\oracle{} is higher compared to \bear{}, because the optimization of \oracle{} is more extensive than \bear{}, and
it optimizes more demand access pathologies in DRAM cache protocol, compared to \bear{}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cs2_all_sup.pdf}
    \caption{Speedup of \bear{} and \oracle{} compared to the baseline DRAM cache design in GAPBS and NPB.
    \string*$bt$ is excluded from \oracle{} analysis, as it started a different phase of program execution.}
    \label{fig:cs2SuAll}
    \vspace{-1.5em}
\end{figure}

\subsection{GAPBS in \bear{}}

$bc$ has a significant number of write hits in its hits per thousand instructions or HPKI (Figure \ref{fig:cs1Hpki}).
$bc$ also has the lowest miss ratio of all the workloads in GAPBS (Figure \ref{fig:cs1MissRatio}).
Removing the DRAM read for tag check from write hit handling path (what \bear{} does) reduces the latency of servicing misses
and increases system throughput.
Figure \ref{fig:cs2SuAll} shows $bc$ has the highest speedup amongst all workloads in GAPBS for \bear{}.

$pr$ has the lowest amount of speedup for \bear{} because it has the least amount of write hits in its HPKI
(Figure \ref{fig:cs1Hpki}) and it has the highest miss ratio (Figure \ref{fig:cs1MissRatio}).
Optimizing a non-significant number of write hits in this workload does not reduce the latency of misses.
As a result, the speedup is hardly above one.

Another interesting case is $tc$ which has a significant number of write hits in its HPKI (Figure \ref{fig:cs1Hpki});
however, it does not benefit from \bear{}.
The reason is $tc$ has significantly higher ratio of miss dirty lines, than the rest of workloads in GAPBS.
All the dirty evicted lines must be written back to the main memory. DRAM cache manager holds them in Write Back buffer
(WB buffer in Figure \ref{fig:dcache}). Once the WB buffer is full, the cache manager prioritizes the writes in
the WB buffer over the requests in outstanding requests buffer (ORB in Figure \ref{fig:dcache}).
In this way, the cache manager makes entry available in the WB buffer for incoming DRAM cache line
evictions that are dirty. This prioritization can increase the latency of the requests in ORB.
Even though the write hit optimization in \bear{} can reduce the latency of misses, for a write intensive application
with relatively high miss ratio like $tc$ is not beneficial.
The rest of the workloadsâ€™ speedups are proportional to their write hits and miss ratios.

\subsection{NPB in \bear{}}

Figure \ref{fig:cs2SuAll} shows $lu$ has the highest speedup in NPB for \bear{}.
$lu$ has a large number of write hits in its HPKIs (Figure \ref{fig:cs1Hpki})
and the lowest miss ratios compared to the rest of workloads in NPB (Figure \ref{fig:cs1MissRatio}).
As we explained above, removing the DRAM read for tag check from write hit handling path in \bear{},
reduces the latency of servicing misses and increases system throughput.

$cg$ dose not benefit from \bear{} since it has close to 100\% miss ratio (Figure \ref{fig:cs1MissRatio})
and very limited number of write hits (Figure \ref{fig:cs1Hpki}).

\subsection{GAPBS in \oracle{}}

Figure \ref{fig:cs2SuAll} shows $tc$ and $bc$ have the highest speedup amongst all workloads for \oracle{}.
$tc$ and $bc$ have highest portion of write hits in their HPKIs (Figure \ref{fig:cs1Hpki}), also,
lower miss ratios compared to the other workloads in the suite (Figure \ref{fig:cs1MissRatio}).
Moreover, they have the highest portion of write miss cleans in their Misses per Thousand Instructions or MPKI
(Figure \ref{fig:cs1Mpki}) which can have an out of size impact on the performance improvement.
The speedups of the other workloads in GAPBS is proportional to the number of read miss cleans accesses (Figure \ref{fig:cs1Mpki})
that \oracle{} optimizes.

\subsection{NPB in \oracle{}}

The MPKIs of $cg$, $is$, and $sp$ workloads are dominated by read miss cleans (Figure \ref{fig:cs1Mpki})
and they have the highest miss ratios (Figure \ref{fig:cs1MissRatio}).
Removing the DRAM read for tag check from read miss cleans handling path (what \oracle{} does)
reduces their latencies and increases system throughput. Thus, $cg$, $is$, and $sp$ have the highest
speedups amongst all workloads as Figure \ref{fig:cs2SuAll}. $lu$ has the lowest speedup because its MPKI is dominated by read miss dirties, not optimized by \oracle{}.

$bt$ in NPB is an outlier in this evaluation because our analysis shows it started another phase of execution. Thus, we exclude it from analysis of \bear{} and \oracle{}.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/cs2_all_sup_ddr4mm.pdf}
  \caption{Speedup of \bear{} and \oracle{} compared to the system without the DRAM cache in GAPBS and NPB.
  \string*$bt$ is excluded from \oracle{} analysis, as it started a different phase of program execution.}
  \label{fig:cs2SuAllMm}
  \vspace{-1.5em}
\end{figure}

Finally, we show the speedup of \bear{} and \oracle{} compared to the same system
without the DRAM cache (with a DDR4 main memory only) in Figure~\ref{fig:cs2SuAllMm}. We observe that both optimized DRAM cache designs we investigate
in this case study still perform less than a system without the DRAM cache. \oracle{} optimizes more pathologies
in the baseline DRAM cache design compared to \bear{}; thus, it has higher speedup than \bear{}. Even though
\oracle{} uses a zero-latency SRAM tag and metadata storage in its implementation, it still needs further optimization
to outperform a system without the DRAM cache.

\textbf{Takeaway:} 
% In Section \ref{cs1} we showed the baseline DRAM cache design which is inspired by real hardware of Intel's
% Cascade Lake has performance degradation compared to the same system without the DRAM cache. The baseline DRAM cache design stores
% tag and metadata along with the data in the same DRAM cache location. Thus, for tag comparison the cache manager reads the entire cache line from DRAM.
% If the demand access is write and it hits on DRAM cache, or if the demand access misses on DRAM cache to a clean cache location, the cache manager will not use the data in the read line for tag check.
% Essentially, for such memory requests the latency will be increased, and it reduces the system throughput.
In this case study we showed if a cache design can provide a mechanism to determine the hit/miss and clean/dirty for a given request without accessing the DRAM,
it can provide performance improvement compared to the baseline DRAM cache design. However, these optimizations 
are not enough for the DRAM cache system to outperform the same system without the DRAM cache.

% This data shows that even though storing tag and metadata along with the data in the same cache line simplifies
% the DRAM cache architecture, it contributes to performance degradation (baseline design).
% By fetching the entire cache line for
% demand accesses that will eventually be discarded (miss-clean for both read and write demands) or overwritten
% (write demands hits), essentially two downsides will happen. First, a waste of bandwidth will be enforced
% to the system, since the data part (significant portion of the fetched line) is not going to be used.
% Second, the miss handling process will be delayed until after the entire cache line
% (containing tag and metadata) is received from the DRAM. This will increase the latency.
% In \oracle{} and \bear{} we explored potential performance benefits of overcoming these downsides existing in the real hardware of DRAM caches
% as well as the prior research work in this area. Given the miss ratio and high portion of aforementioned demand accesses all the
% workloads had, they all benefited from such optimizations.


