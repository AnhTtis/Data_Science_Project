
\section{Case Study 1: Performance of Baseline Cache}
\label{cs1}

% In this section, we use our simulation model to evaluate the performance of DRAM cache systems.
% We simulate the target system from Figure~\ref{fig:case-studies} for this study, where a single channel of HBM2 DRAM cache
% is backed up by a single channel of DDR4 main memory.

In this case study, we will investigate the performance and different pathologies memory requests go through, in the baseline DRAM cache design.
Specifically, we ask \emph{how a DRAM cache system's performance compares to those without a DRAM cache.} For this purpose, we consider the simulated system described in Section~\ref{method} in two different configurations: 1) HBM2 as a DRAM cache with DDR4 as main memory, and 2) no DRAM cache and DDR4 as main memory.
We run all workloads in these two configurations.

Typically, we would expect an HBM2-based DRAM cache to perform better, specifically in comparison to DDR4 main memory, because of its higher bandwidth. This case study considers the scenario where there is no additional latency, and the main memory is close to the local memory (e.g., connected to same package).
The similar latency of HBM2 and DDR4 might result in low-performance improvements with the HBM2 DRAM cache. In Section~\ref{sec:cs3}, we will evaluate a more realistic scenario of a high latency to main memory (e.g., disaggregated systems).
We expect that the DRAM caches perform similarly to a DRAM main memory if the working set of the workload fits in the cache (or has a high DRAM cache hit rate). Therefore, rather than focus on the obvious case, we evaluate the performance of the DRAM cache-based system when the workload does not fit in the DRAM cache. We assess the performance of selected workloads on the target system discussed in Section~\ref{method} to accomplish the previously mentioned goal.
We compare the amount of work each configuration has done during its execution to evaluate its performance.

\begin{figure}

    \subfloat[Performance]{%
      \includegraphics[clip,width=\linewidth]{figures/cs1_all_bips.pdf}%
      \vspace{-3ex}
      \label{fig:cs1Bips}
    } \\
    \subfloat[Speedup]{%
      \includegraphics[clip,width=\linewidth]{figures/cs1_all_spu.pdf}%
      \label{fig:cs1Sup}
    }
    \caption{Performance comparison of the baseline DRAM cache to the same system without the DRAM cache, based on billion instructions per second (BIPS). DRAM cache based system always performs less than the system without DRAM cache.
    \string*$bt$ is excluded from this analysis, as it started a different phase of program execution.}
    \label{fig:cs1BipsSu}
    \vspace{-1.5em}

\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/cs1_all_bips.pdf}
%     \vspace{-1ex}
%     \caption{Performance comparison of the baseline DRAM cache to two systems without the DRAM cache, based on billion instructions per second (BIPS). DRAM cache based system always perform worse than systems without DRAM cache.}
%     \label{fig:cs1Bips}
% \end{figure}
%
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/cs1_all_spu.pdf}
%     \vspace{-1ex}
%     \caption{Speedup the baseline DRAM cache compared to the system with DDR4 main memory only.
%     The DRAM cache system shows performance degradation compared to the system without the DRAM cache.}
%     \label{fig:cs1Sup}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cs1_all_miss_ratio.pdf}
    \vspace{-1ex}
    \caption{Miss ratio of different workloads on DRAM cache. Miss ratios range
    from 20\% to 100\% approximately.}
    \label{fig:cs1MissRatio}
    \vspace{-1.5em}
\end{figure}

\subsection*{Results and Discussions}
Figure~\ref{fig:cs1Bips} shows the performance for NPB and GAPBS workloads in the two configurations we described above. We use a metric of billion instructions per second (BIPS) to represent performance.
Figure~\ref{fig:cs1Sup} shows the speed-up achieved with a DRAM cache-based system compared to a system without a DRAM cache (and DDR4 main memory only).
We expect DRAM caches to perform better than a far memory alone.
However, as Figure~\ref{fig:cs1Sup} shows, DRAM cache configuration performs worse than just a DDR4 main memory for all these tests (speed-up is consistently below 1).
We observe that the DRAM caches start to perform poorly as the DRAM cache miss rate is above 20\% (in the workloads we ran). Figure~\ref{fig:cs1MissRatio} shows the miss ratio of DRAM cache per workload.
We define the miss ratio as the total number of miss accesses to the DRAM cache divided by the sum of total miss and hit accesses.
As shown in Figure~\ref{fig:cs1MissRatio}, GAPBS workloads have miss ratios ranging from 20\% to 50\%, and NPB has miss ratios mostly around 50\%. These miss ratios are high but not unreasonable, and we can expect real-life workloads on high-performance systems to show such miss ratios~\cite{hildebrand2021case}.
As described in Section~\ref{design}, miss handling in current hardware-managed DRAM caches can lead to multiple accesses to the cache (local) and backing store (far) interfaces.

This mostly serialized process cannot fully utilize the available bandwidth at the local and far interfaces and
affects to the overall system performance by increasing the latency.
~The interference between these extra accesses and the actual demand access leads to performance degradation, and the previous
DRAM cache studies were not able to capture it. Note that we made sure that the DRAM cache had been warmed-up, so cold misses
are not contributing to the performance observed from the system.
Note: $bt$ in NPB is an outlier in this evaluation because our analysis shows it started another phase of execution. Thus, we exclude it from this analysis.

In addition to the total number of DRAM cache misses, we must look at the types of misses to fully explain the DRAM cache slowdown.
Figure~\ref{fig:cs1Mpki} shows the misses per thousand instruction (MPKI), and the breakdown of the misses into different types in terms of
read or write request and whether the cache location that the request mapped to was clean or dirty.
The type of DRAM cache miss determines how many extra accesses or operations need to be performed for the demand access under consideration.
For example, $tc$ and $bc$ in Figure~\ref{fig:cs1Sup} show the highest slowdown among the GAPBS workloads as they have the highest fraction of dirty misses (Figure~\ref{fig:cs1Mpki}). Similar behavior is true for $sp$ from NPB benchmarks.
$pr$ from GAPBS and $cg$ from NPB do not have a high fraction of dirty cache misses but still show significant DRAM cache slowdown as the total number of misses demonstrated by these benchmarks is relatively high.
Figure~\ref{fig:cs1Hpki} shows hits per thousand instructions (HPKI) and the read/write distribution of DRAM cache hits.
Since write hits have one more access to the DRAM than read hits, even write hits can cause performance degradation. For example, $tc$ and $bc$ from GAPBS have a high fraction of write hits contributing to their large slowdowns.

Figure~\ref{fig:cs1BwUtil} shows the bandwidth utilization per workload for each memory interface (local and far memory). The HBM2 DRAM cache
interface and the DDR4 backing store interface of \gem{} used in this study have a theoretical peak bandwidth of 32 GB/s and 19.2 GB/s, respectively.
Figure~\ref{fig:cs1BwUtil} shows that for all the cases, a significant amount of bandwidth remains unused for both memory interfaces. The reason for low utilization is the latency cost because of extra memory accesses needed in case of a DRAM cache miss (for both read and write), or a DRAM cache write hit.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cs1_all_mpki.pdf}
    \vspace{-1ex}
    \caption{DRAM cache misses per thousand instructions for GAPBS and NPB.
    The figure also shows the distribution of different miss cases out of the total number of miss accesses.}
    \label{fig:cs1Mpki}
    \vspace{-1.5em}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cs1_all_hpki.pdf}
    \vspace{-1ex}
    \caption{DRAM cache hits per thousand instructions for GAPBS and NPB.
    The figure also shows the distribution of read and write hits out of the total number of hit accesses.}
    \label{fig:cs1Hpki}
    \vspace{-1.5em}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cs1_all_bw_util.pdf}
    \vspace{-1ex}
    \caption{Bandwidth utilization of different workloads on DRAM cache (local) and backing store (far) interfaces. Peak bandwidth is 32 GB/s and 19.2 GB/s for local and far memory. Figure shows that the workloads significantly under-utilize the available bandwidth due to access amplification.}
    \label{fig:cs1BwUtil}
    \vspace{-1.5em}
\end{figure}

\textbf{Takeaway:} the current DRAM cache designs perform poorly even when the DRAM cache miss ratio is as low as 20\%. In case of DRAM cache miss, the extra memory accesses required for single demand access lead to increased access latency and under-utilization of available bandwidth to the memory devices.
This behavior shows that we need to optimize the DRAM cache design for both miss and write-hit handling to improve the performance.
