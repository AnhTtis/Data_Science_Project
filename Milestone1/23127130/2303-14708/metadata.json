{
    "arxiv_id": "2303.14708",
    "paper_title": "Exploring Multimodal Sentiment Analysis via CBAM Attention and Double-layer BiLSTM Architecture",
    "authors": [
        "Huiru Wang",
        "Xiuhong Li",
        "Zenyu Ren",
        "Dan Yang",
        "chunming Ma"
    ],
    "submission_date": "2023-03-26",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.MM"
    ],
    "abstract": "Because multimodal data contains more modal information, multimodal sentiment analysis has become a recent research hotspot. However, redundant information is easily involved in feature fusion after feature extraction, which has a certain impact on the feature representation after fusion. Therefore, in this papaer, we propose a new multimodal sentiment analysis model. In our model, we use BERT + BiLSTM as new feature extractor to capture the long-distance dependencies in sentences and consider the position information of input sequences to obtain richer text features. To remove redundant information and make the network pay more attention to the correlation between image and text features, CNN and CBAM attention are added after splicing text features and picture features, to improve the feature representation ability. On the MVSA-single dataset and HFM dataset, compared with the baseline model, the ACC of our model is improved by 1.78% and 1.91%, and the F1 value is enhanced by 3.09% and 2.0%, respectively. The experimental results show that our model achieves a sound effect, similar to the advanced model.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14708v1"
    ],
    "publication_venue": null
}