@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}
@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}
@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={91--99},
  year={2015}
}
@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}
@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}
@ARTICLE{9736584,
  author={Zhu, Tong and Li, Leida and Yang, Jufeng and Zhao, Sicheng and Liu, Hantao and Qian, Jiansheng},
  journal={IEEE Transactions on Multimedia}, 
  title={Multimodal Sentiment Analysis With Image-Text Interaction Network}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMM.2022.3160060}}
@ARTICLE{9783103,
  author={Yang, Bo and Wu, Lijun and Zhu, Jinhua and Shao, Bo and Lin, Xiaola and Liu, Tie-Yan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Multimodal Sentiment Analysis With Two-Phase Multi-Task Learning}, 
  year={2022},
  volume={30},
  number={},
  pages={2015-2024},
  doi={10.1109/TASLP.2022.3178204}}
@article{ye2022sentiment,
  title={Sentiment-aware multimodal pre-training for multimodal sentiment analysis},
  author={Ye, Junjie and Zhou, Jie and Tian, Junfeng and Wang, Rui and Zhou, Jingyi and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={Knowledge-Based Systems},
  volume={258},
  pages={110021},
  year={2022},
  publisher={Elsevier}
}
@INPROCEEDINGS{9747542,
  author={Xiao, Luwei and Wu, Xingjiao and Wu, Wen and Yang, Jing and He, Liang},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Multi-Channel Attentive Graph Convolutional Network with Sentiment Fusion for Multimodal Sentiment Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={4578-4582},
  doi={10.1109/ICASSP43922.2022.9747542}}
@ARTICLE{9932611,
  author={Tang, Jiajia and Liu, Dongjun and Jin, Xuanyu and Peng, Yong and Zhao, Qibin and Ding, Yu and Kong, Wanzeng},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={BAFN: Bi-direction Attention based Fusion Network for Multimodal Sentiment Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TCSVT.2022.3218018}}
@article{zhao2019image,
  title={An image-text consistency driven multimodal sentiment analysis approach for social media},
  author={Zhao, Ziyuan and Zhu, Huiying and Xue, Zehao and Liu, Zhao and Tian, Jing and Chua, Matthew Chin Heng and Liu, Maofu},
  journal={Information Processing \& Management},
  volume={56},
  number={6},
  pages={102097},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{xu2017analyzing,
  title={Analyzing multimodal public sentiment based on hierarchical semantic attentional network},
  author={Xu, Nan},
  booktitle={2017 IEEE international conference on intelligence and security informatics (ISI)},
  pages={152--154},
  year={2017},
  organization={IEEE}
}
@inproceedings{yang2021multimodal,
  title={Multimodal sentiment detection based on multi-channel graph neural networks},
  author={Yang, Xiaocui and Feng, Shi and Zhang, Yifei and Wang, Daling},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={328--339},
  year={2021}
}
@inproceedings{borth2013large,
  title={Large-scale visual sentiment ontology and detectors using adjective noun pairs},
  author={Borth, Damian and Ji, Rongrong and Chen, Tao and Breuel, Thomas and Chang, Shih-Fu},
  booktitle={Proceedings of the 21st ACM international conference on Multimedia},
  pages={223--232},
  year={2013}
}
@article{baecchi2016multimodal,
  title={A multimodal feature learning approach for sentiment analysis of social network multimedia},
  author={Baecchi, Claudio and Uricchio, Tiberio and Bertini, Marco and Del Bimbo, Alberto},
  journal={Multimedia Tools and Applications},
  volume={75},
  pages={2507--2525},
  year={2016},
  publisher={Springer}
}
@inproceedings{cai2015convolutional,
  title={Convolutional neural networks for multimedia sentiment analysis},
  author={Cai, Guoyong and Xia, Binbin},
  booktitle={Natural Language Processing and Chinese Computing: 4th CCF Conference, NLPCC 2015, Nanchang, China, October 9-13, 2015, Proceedings 4},
  pages={159--167},
  year={2015},
  organization={Springer}
}
@article{yu2016visual,
  title={Visual and textual sentiment analysis of a microblog using deep convolutional neural networks},
  author={Yu, Yuhai and Lin, Hongfei and Meng, Jiana and Zhao, Zhehuan},
  journal={Algorithms},
  volume={9},
  number={2},
  pages={41},
  year={2016},
  publisher={MDPI}
}
@inproceedings{xu2018co,
  title={A co-memory network for multimodal sentiment analysis},
  author={Xu, Nan and Mao, Wenji and Chen, Guandan},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={929--932},
  year={2018}
}
@article{zhang2020sentiment,
  title={Sentiment analysis of social media via multimodal feature fusion},
  author={Zhang, Kang and Geng, Yushui and Zhao, Jing and Liu, Jianxin and Li, Wenxiao},
  journal={Symmetry},
  volume={12},
  number={12},
  pages={2010},
  year={2020},
  publisher={MDPI}
}
@article{yang2020image,
  title={Image-text multimodal emotion classification via multi-view attentional network},
  author={Yang, Xiaocui and Feng, Shi and Wang, Daling and Zhang, Yifei},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={4014--4026},
  year={2020},
  publisher={IEEE}
}
@inproceedings{yu2017multi,
  title={Multi-modal factorized bilinear pooling with co-attention learning for visual question answering},
  author={Yu, Zhou and Yu, Jun and Fan, Jianping and Tao, Dacheng},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1821--1830},
  year={2017}
}
@inproceedings{jiang2020fusion,
  title={Fusion-extraction network for multimodal sentiment analysis},
  author={Jiang, Tao and Wang, Jiahai and Liu, Zhiyue and Ling, Yingbiao},
  booktitle={Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11--14, 2020, Proceedings, Part II 24},
  pages={785--797},
  year={2020},
  organization={Springer}
}
@article{peng2021cross,
  title={Cross-modal complementary network with hierarchical fusion for multimodal sentiment classification},
  author={Peng, Cheng and Zhang, Chunxia and Xue, Xiaojun and Gao, Jiameng and Liang, Hongjian and Niu, Zhengdong},
  journal={Tsinghua Science and Technology},
  volume={27},
  number={4},
  pages={664--679},
  year={2021},
  publisher={TUP}
}
@article{li2022clmlf,
  title={CLMLF: a contrastive learning and multi-layer fusion method for multimodal sentiment detection},
  author={Li, Zhen and Xu, Bing and Zhu, Conghui and Zhao, Tiejun},
  journal={arXiv preprint arXiv:2204.05515},
  year={2022}
}
@inproceedings{schifanella2016detecting,
  title={Detecting sarcasm in multimodal social platforms},
  author={Schifanella, Rossano and De Juan, Paloma and Tetreault, Joel and Cao, Liangliang},
  booktitle={Proceedings of the 24th ACM international conference on Multimedia},
  pages={1136--1145},
  year={2016}
}
@inproceedings{xu2017multisentinet,
  title={Multisentinet: A deep semantic network for multimodal sentiment analysis},
  author={Xu, Nan and Mao, Wenji},
  booktitle={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages={2399--2402},
  year={2017}
}
@inproceedings{cai2019multi,
  title={Multi-modal sarcasm detection in twitter with hierarchical fusion model},
  author={Cai, Yitao and Cai, Huiyu and Wan, Xiaojun},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={2506--2515},
  year={2019}
}
@inproceedings{bird2006nltk,
  title={NLTK: the natural language toolkit},
  author={Bird, Steven},
  booktitle={Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions},
  pages={69--72},
  year={2006}
}
@article{wang2023multimodal,
  title={Multimodal Sentiment Analysis Representations Learning via Contrastive Learning with Condense Attention Fusion},
  author={Wang, Huiru and Li, Xiuhong and Ren, Zenyu and Wang, Min and Ma, Chunming},
  journal={Sensors},
  volume={23},
  number={5},
  pages={2679},
  year={2023},
  publisher={MDPI}
}
@inproceedings{woo2018cbam,
  title={Cbam: Convolutional block attention module},
  author={Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}