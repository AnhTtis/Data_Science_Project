\documentclass[11pt]{article}

% This file will be kept up-to-date at the following GitHub repository:
%
% https://github.com/automl-conf/LatexTemplate
%
% Please file any issues/bug reports, etc. you may have at:
%
% https://github.com/automl-conf/LatexTemplate/issues

\usepackage{microtype} % microtypography
\usepackage{booktabs}  % tables
\usepackage{url}  % urls

\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor, xspace}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}

% AMS math
\usepackage{amsmath}
\usepackage{amsthm}

\newcommand{\eat}[1]{}    
\newcommand{\red}[1]{{\color{red} #1}}    
\newcommand{\blue}[1]{{\color{blue} #1}}  

% With no package options, the submission will be anonymized, the supplemental
% material will be suppressed, and line numbers will be added to the manuscript.
%
% To hide the supplementary material (e.g., for the first submission deadline),
% use the [hidesupplement] option:
%
% \usepackage[hidesupplement]{automl}
%
% To compile a non-anonymized camera-ready version, add the [final] option (for
% the main track), or the [finalworkshop] option (for the workshop track), e.g.,
%
% \usepackage[final]{automl}
% \usepackage[finalworkshop]{automl}
%
% or
%
% \usepackage[final, hidesupplement]{automl}
% \usepackage[finalworkshop, hidesupplement]{automl}

% automl
%\usepackage[]{automl}
% arxiv remove footnote about automl
\usepackage[final]{automl}

% You may use any reference style as long as you are consistent throughout the
% document. As a default we suggest author--year citations; for bibtex and
% natbib you may use:

\usepackage{natbib}
\bibliographystyle{apalike}
\setcitestyle{numbers}
\setcitestyle{square}

% and for biber and biblatex you may use:

% \usepackage[%
%   backend=biber,
%   style=authoryear-comp,
%   sortcites=true,
%   natbib=true,
%   giveninits=true,
%   maxcitenames=2,
%   doi=false,
%   url=true,
%   isbn=false,
%   dashed=false
% ]{biblatex}
% \addbibresource{...}

\title{Efficient Multi-stage Inference on Tabular Data}

% The syntax for adding an author is
%
% \author[i]{\nameemail{author name}{author email}}
%
% where i is an affiliation counter. Authors may have
% multiple affiliations; e.g.:
%
% \author[1,2]{\nameemail{Anonymous}{anonymous@example.com}}

\author[1]{\nameemail{Daniel S. Johnson}{dansj@stanford.edu}}
\author[2]{\nameemail{Igor L. Markov}{imarkov@meta.com}}

% the list might continue:
% \author[2,3]{\nameemail{Author 2}{email2@example.com}}
% \author[3]{\nameemail{Author 3}{email3@example.com}}
% \author[4]{\nameemail{Author 4}{email4@example.com}}

% if you need to force a linebreak in the author list, prepend an \author entry
% with \\:

% \author[3]{\\\nameemail{Author 5}{email5@example.com}}

% Specify corresponding affiliations after authors, referring to counter used in
% \author:

\affil[1]{Stanford University}
\affil[2]{Meta}

% the list might continue:
% \affil[2]{Institution 2}
% \affil[3]{Institution 3}
% \affil[4]{Institution 4}

% define PDF metadata, please fill in to aid in accessibility of the resulting PDF
\hypersetup{%
  pdfauthor={}, % will be reset to "Anonymous" unless the "final" package option is given
  pdftitle={},
  pdfsubject={},
  pdfkeywords={}
}

\begin{document}

\maketitle

\begin{abstract}
Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30\%, and network communication between application front-end and ML back-end by about 50\% for a commercial end-to-end ML platform that serves millions of real-time decisions per second.
\end{abstract}

\section{Introduction}

The recent availability of sophisticated ML tools \cite{paszke2019pytorch, abadi2016tensorflow} fueled the development of many new data-driven applications, but considerable efforts are required to engineer robust high-performance ML systems with sufficient throughput to make a practical impact \cite{burkov2020machine, huyen2022designing, amershi2019software, hazelwood2018applied, hermann2017meet, paleyes2020challenges}. Other than newer systems designed with ML in mind, a greater variety of existing production systems can be enriched with ML capabilities. Broadly speaking, such capabilities model the operating environment when closed-form descriptions are not available, help avoid redundant work and optimize interactions between system components, also predict user behaviors and preferences to enhance user experience \cite{markov2022looper}. As data patterns change, regular retraining, monitoring, and alerts add significant software complexity, but this ML complexity should not overburden product code. To ensure SW development velocity and facilitate performance optimizations, it is common to separate ML code into libraries and services --- data collection, model training and offline evaluation, real-time inference \cite{orr2021managing, hazelwood2018applied}, etc --- invoked from product code via RPC APIs.\footnote{RPC calls within our data center are comfortably within the constraints of real-time inference in the deployed ML platform we work with (see Table \ref{tab:latency}). Therefore we optimize the mean latency and the overall CPU usage. 
}
Unlike the well-publicized large language models (e.g.,~GPT-3\cite{brown2020language}) and image-understanding models (e.g.,~CNN models such as ResNet~\cite{he2016deep} or attention models \cite{vaswani2017attention}), many applications must retrain models before data trends change, often on an hourly or daily basis. With dozens, hundreds, or low thousands of features, these models typically train on 100K-10M rows of data. Unlike image pixels, video frames, or audio samples, features in {\em tabular data} often exhibit different scales and do not correlate ~\cite{grinsztajn2022tree}. Surprisingly enough, ML competitions with
tabular data have been dominated not by deep learning models, but by gradient boosting models \cite{carlensstateof} such as XGBoost \cite{chen2016xgboost}, LightGBM \cite{ke2017lightgbm}, and CatBoost \cite{prokhorenkova2018catboost}.\footnote{The popularity of these packages is affected by training efficiency and support for various hardware accelerators.} Even though 
deep models can be optimized for tabular data \cite{arik2021tabnet}
for improved performance, decision trees still outperform on tabular datasets and structured data in general \cite{shwartz2022tabular}. Despite some ongoing progress \cite{gorishniy2021revisiting}, deep learning models consistently suffer 
serious limitations \cite{grinsztajn2022tree}, such as struggling with uninformative features.
In fact, a recent work \cite{caglar} investigating the black-box nature of neural networks proved that feed-forward networks with piecewise-linear activation functions can be represented exactly by decision trees. The claim is then extended to arbitrary continuous activation functions via piecewise-linear approximation. Practical aspects aside, this suggests viewing neural networks as a collection of subtrees and subnetworks that are optimized to handle different inputs. 
% This is analogous to our approach to multi-stage inference in that we break the inputs up in such a manner to perform better inference (which in our case means using a simpler model for easier to evaluate inputs).
 
 We now focus on the bottleneck of many high-performance production ML systems --- real-time inference. As noted earlier, deep learning tends
 to lose out to XGBoost on structured data, and this trend is stronger when training data is limited in size. Additionally, batch-processing efficiencies available for DNNs are not helpful for real-time inference.
 Running on CPUs, inference for XGBoost models can be an order of magnitude faster than for DNNs and more compact in memory, and this shifts the inference bottleneck to RPC API calls issued by product code to ML services. The idea explored in our work is to process at least some inferences quickly with a simple model embedded into product code to bring down mean latency when possible and fall back on RPC APIs when necessary. 
 We develop such {\em multi-stage inference} in detail and show that
 it produces consistently good results for various tabular datasets.
 To reduce API latency and avoid CPU overhead of network communications, it is important to make the first-stage model dramatically simpler than the second-stage model (accessed via RPC), rather than just instantiate the second-stage model with fewer features as done in~\cite{kraft2020willump}. In our environment, product code happens to be written
 in PHP and the first-stage model embedded into it does not rely on any ML packages.\footnote{Inference for the first-stage model can also be implemented in hardware.} Note that first-stage model {\em training} does not need to be simple, and here we do use existing high-performance ML packages for this purpose \cite{sklearn_api}. Another critical aspect of our work is how to determine which inputs are served by which-stage model.
%
 Multi-stage inference has been explored in \cite{samie2020hierarchical} which uses a lightweight classifier on computationally constrained IoT devices to decide where to perform inference, in \cite{park2015big} which tries to be energy-efficient by executing "little" deep neural networks as often as possible while reverting to "big" DNNs when necessary, in \cite{daghero2022two} which decides between a decision tree and a CNN operating on and embedded device, and in \cite{daghero2021adaptive} which straddles a tradeoff between accuracy and energy consumption by limiting the size of random forest models on low-power embedded devices. For tabular
 data used in our work, CNNs would be irrelevant and random-forest models would be inferior to SOTA. Our applications have high accuracy requirements as well as much greater available DRAM and much lower latency than networking with low-power embedded devices can allow.
 %
 We validate our proposed multi-stage inference in two ways. First, we show that inference quality is largely preserved across diverse public tabular datasets, and specifically, that the decline in ROC AUC and accuracy is minor compared to the large performance gain by handling a significant amount of data within the product code. Second, we evaluate actual reductions in inference latency and CPU resource usage in a high-performance production system [omited for blind review] with inference latency seeing a 1.3x speedup and CPU resource usage down 30\%.

\eat{
The recently deployed Looper platform \cite{markov2022looper} at Meta handles the full data pipeline for training and inference for product engineers so that no ML knowledge is required, and only a simple API links products to the platform. This platform hosts 440-1,000 ML models that made 4-6 million real-time decisions per second during the 2021 production deployment, but with its increased popularity also comes a strain on resources. Therefore, finding ways to improve inference efficiency has become a desired objective. When making a prediction, network latency is incurred when making a RPC call first to obtain desired features, and then again to pass these features to an ML model returning a prediction. Additionally, CPU resources are used by computing all of the model features. To cause the largest impact on inference efficiency, we focus on binary classification as it is the most common task in Looper. 
}

In Section \ref{sec:ml rationale and tradeoffs}, we outline the rationale behind our approach, key insights, and three implied tradeoffs. In Section \ref{sec:lrwbins algorithm}, we propose the first-stage model called Logistic Regression with Bins (LRwBins) as well as the approach to allocate the inferences between the stages of the model. In Section \ref{sec:system implementation}, we discuss implementation of this multi-stage system. In Section \ref{sec:empirical evaluation}, we evaluate this approach for public datasets as well as in a commercially-deployed ML platform that performs millions of inferences per second. Conclusions and perspectives are given in Section \ref{sec:conclusion}.

\section{ML Rationale and Tradeoffs}
\label{sec:ml rationale and tradeoffs}

Our proposal provisions for the first-stage model to use simple and fast inference algorithms that can be embedded in product code without significantly increasing complexity. This way, we maximize improvements in latency and CPU usage. There is no reason to simplify training, and if we do, the simple model might significantly underperform the more sophisticated model behind the RPC API calls. This tradeoff between the {\em sophistication of training and inference} leads us to consider Logistic Regression (LR) as an ML component.\footnote{We have also evaluated SVMs instead of LR, but they did not improve performance of our LR-based solution. Additionally, experiments adding quadratic and nonlinear features to the model did not show improved performance.}
Indeed, the formula for LR can be implemented directly in any popular programming language without using ML libraries
%The simplicity of computing the classification probabilities via the logistic function
%\begin{equation}
$
\left(
    h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}
\right)
$
%\end{equation}
%in our PHP product code highlights the goal of simple inference.
but bare LR is too limited to serve as the first-stage model.

A second tradeoff is between {\em ML performance and efficiency of inference}: a small sacrifice in model quality (ROC AUC) may bring large gains in inference efficiency. By training the first-stage model on a subset of the (most important) features of the sophisticated model, we can additionally reduce CPU usage --- both in the model itself and during feature fetching, which can also be a CPU bottleneck in practice~\cite{markov2022looper}.
%, so reducing the feature computations for a significant fraction of
%the inferences would have a sizable impact on reducing CPU resource usage.
To address the performance-efficiency tradeoff, we use a third tradeoff --- between {\em performance and input coverage}. In other words, we limit the first-stage model to only some inputs to keep its performance drop (vs the second-stage model) negligible. The fraction of inputs served by the first-stage model ({\em coverage}) must be sufficiently large to ensure efficiency gains --- in practice, 50\% is a reasonable target.

To determine which inferences can be handled by a simpler model, we are motivated by linear approximations to high-dimensional separating hypersurfaces. By breaking our datasets up into subsets of data with similar features and subsequently using a simple model for each subset, we can determine which subsets are suitable for simple models. In these subsets of feature space, it is conceivable that linear approximations to a more complex separating surface could do a good job at separating the data as illustrated in Figure \ref{fig:linear_approximations}. Here, the quadrants with red linear approximations to the blue separating curve are candidates to be handled by a first-stage linear model rather than the slower complex model because they do a good job within their respective quadrants (better approximations can be found by LR).
% add another figure from powerpoint presentation

\begin{figure}
\centering
\caption*{
\textbf{Figure \ref{fig:linear_approximations}:}
As a motivating example to using linear approximations of high-dimensional separating hypersurfaces, consider some data consisting of two features $(x_1, x_2)$ and label (represented by either a circle or a diamond). First, by looking at only the data points, we see that the data is not \textit{linearly} separable, but that the \textit{nonlinear} blue curve does a good job. If we arbitrarily break up the data into quadrants by the green line, then we can choose red lines that do a good job of separating the data in each quadrant and can be thought of as linear approximations of the blue curve. Other red lines (such as linear SVMs) might better separate the data in each region. From these improved divisions, one could imagine a new nonlinear blue curve that also does a good job at separating the data.
}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/linear_approximations.png}
  \captionof{figure}{
  }
  \label{fig:linear_approximations}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/combined_bins_reformat.png}
  \captionof{figure}{
  }
  \label{fig:combined_bin}
\end{minipage}
\caption*{
\textbf{Figure \ref{fig:combined_bin}:}
This diagram illustrates the mapping of a data point into a combined bin. If each of the $n=4$ features (represented by $x_i$) are broken into $b=3$ quantiles (represented by $q_i$), then the ordered pair in which the data point falls into determines the associated combined bin. Each combined bin can store an ML model trained on the data falling into this bin (where enough data is available).
}
\vspace{-6mm} 
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=.65\linewidth]{figures/linear_approximations.png}
%    \caption{As a motivating example to using linear approximations of high-dimensional separating hypersurfaces, consider some data consisting of two features $(x_1, x_2)$ and label (represented by either a circle or a diamond). First, by looking at only the data points, we see that the data is not \textit{linearly} separable, but that the \textit{nonlinear} blue curve does a good job. If we arbitrarily break up the data into quadrants by the green line, then we can choose red lines that do a good job of separating the data in each quadrant and can be thought of as linear approximations of the blue curve. Other red lines (such as linear SVMs) might better separate the data in each region. From these improved divisions, one could imagine a new nonlinear blue curve that also does a good job at separating the data.}
%    \label{fig:linear_approximations}
%\end{figure}

\section{LRwBins Algorithm}
\label{sec:lrwbins algorithm}

In this section, we introduce our general method of multi-stage inference called Logistic Regression with Bins (LRwBins) as well as the method of dividing the data into subsets of similar data.
In practice, each subset of similar data can be constructed with the following procedure. We first use a model-free (such as MRMR \cite{ding2005minimum}) or model-based (such as XGBoost feature importance ranking \cite{chen2016xgboost}) approach to determine the relative importance of our features. We split each of the $n$ most important features into $b$ bins dictated by the quantiles of the feature over the normalized training set.
Quantiles are used here because there are features with very different distributions and we generally want to distribute the data equally between the bins to adequately train a linear model in each bin.
While quantiles work naturally to break up numerical data, we specifically handle boolean features by naturally splitting into two bins instead of $b$ bins, and categorical data in a similar manner using a one-hot encoding.
The $n$ bins that a datum falls into can be considered an ordered tuple (Figure \ref{fig:combined_bin}). This ordered tuple determines a "combined bin" which contains all of the data falling into the same ordered tuple, thus creating $b^n$ subsets each consisting of similar data. In general, since the number of combined bins grows exponentially, both $b$ and $n$ should be kept to reasonably small values to prevent situations where there are many combined bins with very small amounts of data within. In this way, we are essentially building a decision tree that has $b$ branches and depth $n$ and where each split is determined by the quantiles of the data. Continuing with the linear approximation motivation from the previous section, our proposed first-stage model called LRwBins will use a logistic regression classifier within each combined bin. 

%\begin{figure}
%    \centering
%    \includegraphics[width=.7\linewidth]{figures/combined_bin.png}
%    \caption{This diagram illustrates the mapping of a data point into a combined bin. If each of the $n=4$ features are broken into $b=3$ quantiles, then the ordered pair in which the data point falls into determines the associated combined bin.}
%    \label{fig:combined_bin}
%\end{figure}

For the multi-stage approach between LRwBins and a secondary, more complex model to work, one must determine how to pick the model to perform the inference. This decision will be split up based on the performance of the models on each combined bin on a validation set of data. Then, during inference, one can simply map the incoming features to a combined bin, check a stored value to see which model should perform inference, and call the model. To maximize the amount of data using the efficient first-stage model, we proceed as follows. We start by evaluating our desired performance metric (ROC AUC, accuracy, etc) of each model on each combined bin. The combined bins are then sorted by how much the secondary model beats the first model. This means that at the start of the order, we find the combined bins where LRwBins is competitive with or is outperforming the complex model. These bins are ideal for first-stage inference. We combine the first two bins in this order and evaluate the performance metric on the cumulative data. We then add the next bin in this order to the cumulative data, evaluate the performance metric, and repeat until all of the combined bins are being evaluated together. Each evaluation along the way presents an opportunity to split the combined bins between the first-stage and second-stage model. As more and more combined bins are accumulated, the first-stage model handles more inferences  but its ML performance deteriorates. In practice, using the accuracy to determine the combined bin separation gives the best results. Figure \ref{fig:bins} explores LR performance per bin and discusses a variant approach to separate data between the first and second stage models.

\begin{figure}
\centering
\caption*{
\textbf{Figure \ref{fig:bins}:}
To allocate combined bins for inference by first or second-stage models, we evaluate ML performance metrics per bin. Each bar represents a combined bin with the height representing the ROC AUC for that bin, the width representing the number of data rows within each bin, and the color representing the correlation between the global importance of the features (based on the entire dataset) and the local importance of the features (based on the data contained within the bin). The bins are sorted by ROC AUC (or any performance metric such as accuracy) to partition them between first and second stages. A steep dropoff in performance around 50K data rows offers a good separator. Bin-local feature importance shows surprisingly little correlation to global feature importance (for most bins). We explain this by the use of most important features to define combined bins.
%  Since the data in each combined bin have similar feature values for the most globally important features by design, it makes sense that these features become less important within each combined bin. 
}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/bins.png}
  \captionof{figure}{
  }
  \label{fig:bins}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/automl.png}
  \captionof{figure}{
  }
  \label{fig:automl}
\end{minipage}
\caption*{
\textbf{Figure \ref{fig:automl}:}
AutoML supports automated tuning of parameters ($b$ representing the number of quantiles and $n$ representing the number of most important features to use)
on a validation dataset to optimize the shape of the combined bins used by LRwBins. Here we compare the ROC AUC of the LRwBins model for a variety of $n$ and $b$ with the ROC AUC of XGBoost model for a variety of $n$. Notice that we include the ROC AUC for XGBoost using all of the available features (176).
}
\vspace{-6mm}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=.5\linewidth]{figures/bins.png}
%    \caption{To allocate combined bins for inference by first or second-stage models, we evaluate ML performance metrics per bin. Each bar represents a combined bin with the height representing the ROC AUC for that bin, the width representing the number of data rows within each bin, and the color representing the correlation between the global importance of the features (based on the entire dataset) and the local importance of the features (based on the data contained within the bin). The bins are sorted by ROC AUC (or any performance metric such as accuracy) to partition them between first and second stages. A steep dropoff in performance around 50K data rows offers a good separator. Bin-local feature importance shows surprisingly little correlation to global feature importance (for most bins). We explain this by the use of most important features to define combined bins.
%    %  Since the data in each combined bin have similar feature values for the most globally important features by design, it makes sense that these features become less important within each combined bin. }
%    }
%    \label{fig:bins}
%\end{figure}

Once the combined bins are divided by which model performs inference, the next logical step is to retrain the individual models on the data within their associated combined bins. The first-stage model typically does not see noticeable improvement. The second-stage model also does not improve likely because the gradient-boosted decision tree (GBDT) models often used for binary classification of tabular data generalizes well and an improvement by training on this subset of the training data would indicate that the original GBDT was not properly capturing all of the data and perhaps its hyperparameters needed to be changed. After separating the data, if we train a new LRwBins model on the data that was not designated for first-stage inference, the new important features on this subset of the data create combined bins which can be evaluated as a second-stage before falling back to the RPC inference. Experiments on production datasets show that this method gave a marginal improvement in the fraction of data handled by the product embedded models meaning that an extra 1 to 3\% of the data could be handled by the product embedded model with no model performance loss. For simplicity, we present results with only the first-stage LRwBins model that falls back to the RPC prediction.

\newcommand{\idone}{Case 1\xspace}
\newcommand{\idtwo}{Case 2\xspace}
\newcommand{\idthree}{Case 3\xspace}
\newcommand{\idfour}{Case 4\xspace}

%\begin{figure}
%    \centering
%    \includegraphics[width=.5\linewidth]{figures/automl.png}
%    \caption{AutoML allows for the automated tuning of parameters ($b$ and $n$) to optimally pick the shape of the combined bins used by LRwBins by experimenting on a validation set. Here we compare the ROC AUC of the LRwBins model for a variety of $n$ and $b$ with the ROC AUC of XGBoost model for a variety of $n$.}
%    \label{fig:automl}
%\end{figure}

\section{System Implementation}
\label{sec:system implementation}

We now describe the practical implementation of our multi-stage approach including training, inference, and the use of AutoML.

\noindent
{\bf Training and Inference.}
To implement the proposed approach, we train the second-stage model on all of the data to ensure a reliable fallback option for the first-stage model. All training is done with high-performance ML packages while first-stage inference is implemented directly in the product code and reads configuration from a table (rather than loading and running a serialized trained model, as is common in ML platforms today). To this end, we checked that our inference implementations agree to within machine precision. Compared to XGBoost, it takes about half the time to train LRwBins on the same data. To minimize configuration tables for LRwBins, we only store ($i$) quantiles of the $n$ most important features that are used to determine a combined bin and ($ii$) logistic-regression weights for the combined bins designated for first-stage inference. An example LRwBins model trained on 1M data rows takes up $\sim0.3$KB for the quantiles and $\sim2.3$KB for logistic regression weights map when storing 32-bit floats. These tables can likely be compressed when stored, but here we present their size in RAM. During inference, the important features map the inferences to the correct combined bin which is used as input to a hash map to get either the logistic regression weights for first-stage inference or a miss indicating the inference should use the second-stage model. If the logistic regression weights are found, they are used with the features in the logistic function to obtain the probability. 

\noindent 
{\bf Use of ML Automation.}
AutoML can optimize a high-performance ML platform in several ways, including model hyperparameter tuning, feature engineering, and feature selection. For LRwBins, AutoML helps by ($i$) determining the shape of combined bins in terms of $b$ (quantiles) and $n$ (important features used) as shown in Figure \ref{fig:automl}, ($ii$) optimizing local models trained on the data in each individual combined bin, and ($iii$) allocating bins between first- and second-stage models. 

\begin{figure}
\centering
\caption*{
\textbf{Figure \ref{fig:mrmr}:}
Visualizing the features of \idtwo in 2D using \cite{vu2021picasso} clarifies feature selection for inference in the LRwBins model. Each square represents a feature, colors indicate feature types, opacity and geometric proximity to the center reflect feature importance, and integers represent rank by importance.
}
\begin{minipage}{.47\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/case2.png}
  \captionof{figure}{
  }
  \label{fig:mrmr}
\end{minipage}%
\hspace{.05\linewidth}
\begin{minipage}{.47\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/scale.png}
  \captionof{figure}{
  }
  \label{fig:scale}
\end{minipage}
\caption*{
\textbf{Figure \ref{fig:scale}:}
Scaling of our multi-stage approach to 10M data rows in terms of ROC AUC. We compare LRwBins (orange), XGBoost (blue), and the multi-stage model using each model 50\% of the time (green) as we train them on larger subsets of the \idtwo training dataset.
}
\vspace{-6mm}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=.6\linewidth]{figures/case2.png}
%    \caption{Visualizing the features of \idtwo in 2D using \cite{vu2021picasso} clarifies feature selection for inference in the LRwBins model. Here, each square represents a feature. Colors indicate feature types, whereas opacity and geometric proximity to the center represent feature importance and integers represent rank by importance.}
%    \label{fig:mrmr}
%\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.6\linewidth]{figures/scale.png}
%    \caption{Scaling of our multi-stage approach to 10M data rows in terms of ROC AUC. We compare LRwBins (orange), XGBoost (blue), and the multi-stage model using each model 50\% of the time (green) as we train them on larger subsets of the \idtwo training dataset.}
%    \label{fig:scale}
%\end{figure}

\section{Empirical Evaluation}
\label{sec:empirical evaluation}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/datasets_accuracy.png}
         \label{fig:initial_accuracy}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figures/datasets_rocauc.png}
         \label{fig:initial_rocauc}
    \end{subfigure}
    \vspace{-5mm}
    \caption{
    \label{fig:initial_result}
    The relative performance of the hybrid models and XGBoost as a function of the percentage of data handled by LRwBins is the central aspect of our argument. We compare these models to multi-stage inference (solid lines) and XGBoost (dashed line) on several datasets.
    %For various datasets, increasing the fraction of the data handled by the first-stage model decreases the performance of the ML model in terms of accuracy (top) and ROC AUC (bottom).
    The very slight decline in ML performance allows a sizable fraction of the data to be handled by LRwBins with minimal loss in performance. Our key insight is that heavy use of the first-stage model entails only a very small ML performance loss.}
\vspace{-2mm}
\end{figure}

We now present results of our multi-stage inference model that uses LRwBins as a simple first-stage model and XGBoost \cite{chen2016xgboost} as a more complex model used via RPC. XGBoost performance is close to that of GBDTs trained as production models. 
We perform full evaluation on four proprietary datasets from a deployed real-time ML platform. Additional offline evaluation uses the 20+ public datasets from \cite{shwartz2022tabular}. The subset of results reported for public datasets are representative of all of our experiments. We emphasize the improvements in mean latency and CPU usage. We also discuss the limitations of our approach.
%and alternative approaches that did not improve our results.

\subsection{ML Performance Benchmarks}
Among the public datasets, Adult Census Income (ACI) \cite{kohavi1996scaling} is based on the 1994 US Census and seeks to predict whether the income of a person is $>$\$50k/year. Blastchar \cite{shwartz2022tabular} is trying to predict customer retention. Shrutime \cite{shwartz2022tabular} predicts if a customer closes their bank account or not.
%Pima Indians Diabetes Dataset \cite{smith1988using} is a dataset used to predict whether or not a patient has diabetes based on diagnostic measurements.
Patient \cite{raffa2022global} dataset looks at the severity of illness. Banknote \cite{Dua:2019} dataset seeks to determine authenticity based on various factors. Cases 1-4
%\idone, \idtwo, \idthree and \idfour 
are production use cases that represent a company-internal service, optimize client-server data transfers in a large social network, support user authentication and access to online resources. Figure \ref{fig:mrmr} visualizes the features of \idtwo in 2D using \cite{vu2021picasso} to clarify feature selection for inference in  LRwBins. Colors show that the most important features (near the geometric center) include diverse types.

First, we explore standalone performance of LRwBins. By searching over the hyperparameters, we have found that 2-3 quantile bins per feature ($b$) work best and prevent the explosion in the number of combined bins ($b^n$). For larger $b$, many combined bins lack data to train a logistic regression model well. Additionally, about 7 of the most important features used to create the combined bins and 20 features used for inference typically give good results, although these hyperparameters can be tuned for each dataset. In Table \ref{tab:results}, we compare logistic regression (LR), LRwBins, and XGBoost across a number of datasets. The LR and LRwBins models use the top $n$ important features determined by hyperparameter tuning while XGBoost always uses all available features. LRwBins outperforms logistic regression and slightly underperforms XGBoost. 

\begin{table*}[]
\footnotesize
\hspace*{-1.2cm}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Size} & \multirow{2}{*}{Features}
& \multicolumn{1}{ c|}{LR}
& \multicolumn{1}{ c|}{LRwBins}
& \multicolumn{1}{ c|}{XGB}
& \multicolumn{1}{ c|}{LR}
& \multicolumn{1}{ c|}{LRwBins}
& \multicolumn{1}{ c|}{XGB}
\\ 
\cline{ 4- 9}
& & & \multicolumn{3}{ c|}{ROC AUC} & \multicolumn{3}{ c|}{Accuracy} \\ 
\hline
%Dataset & Size & Feats. & LR ROC AUC & LRwBins ROC AUC & XGB ROC AUC & LR Acc. & LRwBins Acc. & XGB Acc.\\ \hline
\idone & 1,000,000 & 62 & 0.830 & 0.845 & 0.866 & 0.907 & 0.909 & 0.911\\ \hline
\idtwo & 1,000,000 & 176 & 0.712 & 0.734 & 0.739 & 0.915 & 0.915 & 0.916\\ \hline
\idthree & 59,094 & 22 & 0.580 & 0.615 & 0.654 & 0.783 & 0.785 & 0.786\\ \hline
\idfour & 73,100 & 268 & 0.565 & 0.577 & 0.602 & 0.900 & 0.901 & 0.905\\ \hline
ACI & 32,561 & 15 & 0.902 $\pm$ 0.004 & 0.903 $\pm$ 0.004 & 0.922 $\pm$ 0.003 & 0.849 $\pm$ 0.004 & 0.849 $\pm$ 0.005 & 0.867 $\pm$ 0.004\\ \hline
Blastchar & 7,043 & 20 & 0.839 $\pm$ 0.009 & 0.839 $\pm$ 0.010 & 0.839 $\pm$ 0.010 & 0.800 $\pm$ 0.011 & 0.800 $\pm$ 0.011 & 0.798 $\pm$ 0.009\\ \hline
Shrutime & 10,000 & 11 & 0.763 $\pm$ 0.010 & 0.845 $\pm$ 0.006 & 0.861 $\pm$ 0.008 & 0.809 $\pm$ 0.006 & 0.846 $\pm$ 0.006 & 0.861 $\pm$ 0.004\\ \hline
%Pima & 767 & 7 & 0.836 & 0.837 & 0.842 & 0.780 & 0.786 & 0.792\\ \hline
Patient & 91,703 & 186 & 0.860 $\pm$ 0.005 & 0.872 $\pm$ 0.004 & 0.899 $\pm$ 0.003 & 0.926 $\pm$ 0.002 & 0.926 $\pm$ 0.002 & 0.932 $\pm$ 0.001 \\ \hline
Banknote & 1,372 & 4 & 0.879 $\pm$ 0.015 & 0.938 $\pm$ 0.016 & 0.989 $\pm$ 0.004 & 0.801 $\pm$ 0.014 & 0.838 $\pm$ 0.020 & 0.947 $\pm$ 0.013 \\ \hline
\end{tabular}
\caption{A comparison of logistic regression (LR), LRwBins, and XGBoost (a strong baseline model for tabular data) using the ROC AUC and the accuracy as metrics. Cases 1-4 are production use cases on our commercial ML platform. Other datasets are a representative subset of the 20+ public datasets from \cite{shwartz2022tabular} that we used for evaluation. For each row, we report the mean of 20 random experiments with the standard deviation reported for the public datasets as error.}
\label{tab:results}
\vspace{-5mm}      
\end{table*}

As per Section \ref{sec:lrwbins algorithm}, the tradeoff between model performance and inference efficiency comes from deciding which combined bins are handled by which stage of the model. As illustrated in Figure \ref{fig:initial_result} (blue) for the Adult Census Income dataset, increasing the fraction of the data handled by first-stage inference decreases the performance of the ML model in terms of both accuracy and ROC AUC. However, the slight decline in performance of the first 40\% of the data provides justification to allow a sizable fraction of the data to be handled by LRwBins with minimal loss in performance. The most important result of this paper is that the initial slope of these lines is so small that the first-stage model can be used on a large fraction of data with only a small ML performance loss. Figure \ref{fig:initial_result} gives representative results on several datasets, but our results on many more datasets (not shown) are similar. Interestingly, a few datasets seemed to show marginal improvement to the XGBoost model at small fractions of data using the first-stage model. As this fraction increased, overall ML performance quickly dropped below the break-even point. Selecting a sensible fraction of data handled by the first-stage model for each of our considered datasets, Table \ref{tab:coverage} documents small losses in performance metrics.
%
Figure \ref{fig:scale} shows that the multi-stage approach scales well to datasets with millions of data rows and preserves the percentage of data handled by the first stage.

\begin{table*}[]
\small
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\multirow{2}{*}{Dataset} & \multicolumn{2}{ c|}{ML Performance Difference (XGBoost Model - Hybrid Model)} & \multirow{2}{*}{Coverage} \\ 
\cline{ 2- 2}\cline{ 3-3}
 & \hspace*{1cm} ROC AUC \hspace*{1cm} & Accuracy & \\ \hline 
\idone & 0.003 & 0.000 & 54.2\% \\ \hline
\idtwo & 0.003 & 0.000 & 49.4\% \\ \hline
\idthree & 0.006 & 0.001 & 60.7\% \\ \hline
\idfour & 0.010 & 0.002 & 58.4\% \\ \hline
ACI & 0.002 & 0.001 & 39.1\% \\ \hline
Blastchar & 0.005 & 0.001 & 24.0\% \\ \hline
Shrutime & 0.001 & 0.002 & 65.1\% \\ \hline
%Pima & 0.004 & 0.006 & 39.6\% \\ \hline
Patient & 0.009 & 0.000 & 50.0\% \\ \hline
Banknote & 0.011 & 0.018 & 60.4 \% \\ \hline
\end{tabular}
\caption{Analysis of select hybrid models by comparing ML metrics to XGBoost. For each dataset, we select a sensible percentage of the data to be handled by the first-stage model (i.e.~Coverage). The percentages are chosen to be as large as possible while allowing for a small tolerance in degradation of ML performance.}
\label{tab:coverage}
\vspace{-5mm}      
\end{table*}

\subsection{Resource and Latency Improvement}
Using multi-stage inference improves mean latency because the product code directly evaluates first-stage model without latency overhead of ML services. Table \ref{tab:latency} shows the total amount of time it takes for a number of first-stage inferences, second-stage inferences via RPC, and multi-stage inferences. In these experiments, multi-stage inference is using the first-stage 50\% of the time and RPC 50\% of the time although this will change based on the dataset as discussed before. We can see that the first-stage inference model is about 5 times faster than the RPC, and the multi-stage inference is about 1.3 times faster than the RPC. To verify the multi-stage inference latency, we include a \textit{projected multi-stage} inference latency time based on the first-stage and RPC latencies. For example, if it takes $t$ time for a RPC prediction (and therefore $.2t$ time for the first-stage prediction), then the multi-stage prediction should take $.2t$ for half of the inferences. For the other half of the inferences, it will take $.2t$ time to attempt the first-stage prediction and discover that the RPC should be used, and then $t$ time to make the RPC prediction. This all leads to $0.5(0.2t) + 0.5(0.2t+t) = 0.7t$ or 1.4 times speed-up over RPC, close to the empirical 1.3x speed-up.
%
\begin{table*}[]
\small
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Inferences} & \multicolumn{4}{ c|}{Average latencies (in milliseconds) for:} \\ 
\cline{ 2- 5}
& $1^\text{st}$-stage Inference &  $2^\text{nd}$-stage Inference via RPC & \textbf{Multi-stage} & Proj. Multi-stage \\ \hline
10x&15&85&82&57 \\ \hline
100x&13&65&50&45 \\ \hline
1000x&11&74&57&48 \\ \hline
10000x&8&67&45&42 \\ \hline
%totals
%10x&0.148&0.848&0.818&0.572 \\ \hline
%100x&1.263&6.518&5.015&4.523 \\ \hline
%1000x&10.930&73.646&57.140&47.753 \\ \hline
%10000x&82.318&667.135&454.666&415.885 \\ \hline
\end{tabular}
\caption{
\label{tab:latency}
Latency for first-stage inferences, inferences that require RPC, as well as measured and projected multi-stage inferences. The multi-stage inference latency involves the time for determining which stage should conduct inference, any network latencies between stages, and the time for inference itself. Latency is averaged over inference batches of very different sizes to check for possible measurement overheads. We see that first-stage inference is 5x faster than the second-stage inference, and multi-stage inference is $~$1.3x faster than always using second-stage inference. The projected multi-stage inference latency (based on the first- and second-stage latencies, 
used 50\% of the time) is 1.4x smaller than the second-stage inference, close to our empirical results. We report data for a use case with higher-than-average latency, although other use cases exhibit consistent trends. 
%Note that these RPC calls are comfortably within the constraints of real-time inference in the deployed ML platform we work with. 
}
\vspace{-3mm}
\end{table*}
%\begin{figure}
%    \centering
%    \includegraphics[width=0.9\linewidth]{figures/latency.png}
%    \caption{Latency performance for first-stage inferences (blue), RPC inferences (green), and multi-stage inferences using each model 50\% of the time (yellow). The first-stage inference model is 5x faster than the RPC, and the multi-stage inference is about 1.3x faster than the RPC. The projected multi-stage inference (red) latency is based on the first-stage and RPC latencies and is 1.4x faster than the RPC which is consistent with our empirical results.}
%    \label{fig:latency}
%\end{figure}
%
The multi-stage inference model improved the CPU resource usage as well. While the full model uses all available features, LRwBins fetches only a subset of the most important features (Section \ref{sec:lrwbins algorithm}). In practice, this gave a 1.2x speedup and used 70\% of the resources compared to the full model.

\subsection{Limitations of LRwBins and Unsuccessful Techniques}
\label{sec:limitations}
We found good multi-stage models for a majority of the 20+ public datasets we experimented with, but a few datasets (less than 10\%) benefited little from our approach because they exhibited a steep dropoff in performance with a small fraction of data using the first-stage model. In these cases, the independently-trained second-stage model robustly handles the majority of the inferences.

Additional experiments included using the first $n$ trees trained by XGBoost to similarly bin the data and then train LR models on these bins, but this did not help, and neither did using linear SVMs instead of LR in each combined bin. Retraining the networks after splitting the data and adding more stages of inference showed at most negligible improvement in our experiments.

\section{Conclusion}
\label{sec:conclusion}

We introduced and developed a proposal for multi-stage inference that includes a much-simplified first stage that can be embedded into the product code to reduce network communication and lower CPU overhead for a negligible loss in ML performance. For validation, we used public datasets and company-internal production datasets from a high-performance ML platform that makes millions of inferences per second. In high-performance applications where network latency from RPC APIs is noticeable, the multi-stage inference approach may be desired to handle up to 50\% of the inferences in a quick and efficient manner reducing network communication between the application front-end and ML back-end.
%Such is the Looper platform making millions of real-time decisions per second. 
The tradeoff between ML performance and inference efficiency can be easily tuned with the LRwBins model which brought a 1.3x drop in latency and 30\% drop in CPU usage compared to the RPC prediction.

Our proposed approach depends heavily on the use of AutoML.
Specifically, the optimal choice of hyperparameters when determining the composition of the combined bins and in choosing the separation threshold between the stages of the inference can vary greatly depending on the use case and the goal tradeoff (which is how much ML performance degradation is acceptable for the computational resource and latency improvements). 
Optimization of these parameters can result in a significant enhancement in resource efficiency and latency, distinguishing it from scenarios where no improvement is achieved.
Note that this approach improves average resource-efficiency of inference and thus improves energy-efficiency as well.

Our approach to improve high-performance inference appears compatible with hardware acceleration. We believe that accelerators for LRwBins would be much simpler than DNN-accelerators, use smaller amounts of embedded memory, and likely do well when tree-based ML models outperform DNNs on tabular data. This simplicity comes at the cost of handling only half the inputs without falling back to a heavier model. FPGAs with embedded CPUs appear promising for this application.
When dealing with hardware accelerators, AutoML is especially important to tune performance based on specific characteristics of hardware components.

Code for this project is publicly available at:
%\hyperlink{https://github.com/removed-for-blind-review}{https://github.com/removed-for-blind-review}
\url{https://github.com/facebook/lr-with-bins}

\bibliography{mybib}

%\input{dac_reviews}
%\input{broader_impact_and_checklist}

% content will be automatically hidden during submission
\begin{acknowledgements}

\end{acknowledgements}

% print bibliography -- for bibtex / natbib, use:

% \bibliography{...}

% and for biber / biblatex, use:

% \printbibliography

% supplemental material -- everything hereafter will be suppressed during
% submission time if the hidesupplement option is provided!
\appendix

\end{document}
