\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}

\newpage

\red{AutoML Todo list}
\begin{itemize}
\item Reviewer 1
\begin{todolist}
\item 
Potential Impact On The Field Of AutoML:
I believe the framework proposed by the authors is too problem specific and the speed-up improvements are calculated on a millisecond basis. This could potentially have an impact on an organization that spends a very large fraction of time doing predictions on a set of fixed problems which can decrease the costs of the organization as well as have positive impacts on the environment. The speed-up comes with a trade-off in performance which could make the acceptance into mission-sensitive systems less acceptable.
\item[\done]
I could not easily find what the colors stand for in Figure 7 as the description of the figure does not include the datasets (except for blue belonging to adult census in Line 215.)
\red{This has been added to the caption.}
\item[\done]
Table 1 is breaking the template.
\red{This has been fixed.}
\item[\wontfix]
Line 65 and following, I would personally not use in [reference] and I would stay consistent with the way the references are done in the rest of the paper.
\item[\done]
Line 123: Creating subsets.
If I understand correctly, for each numerical feature we have b bins, for a binary categorical feature we have 2 bins, while for categorical features we have n unique cat bins, so 
 is not the correct term, since for a categorical feature we can have either fewer or more bins, making the term neither a minimum, neither a maximum constraint.
\item[\wontfix]
Could the authors describe the number of features and bins that were discovered for the considered datasets in Table 1?
\red{We omit these for the sake of meeting the page requirements, but they are included in the hyperparameters file of our code supplement.}
\item
Is there any reason for why standard deviations were not provided for Cases 1-4 in Table 1?
\red{Maybe we just ignore this comment.}
\item[\done]
"Other datasets are a representative subset of the 20+ public datasets from [31] that we used for evaluation."
What was the criteria for filtering the datasets and only including a subset?
\red{We chose a representative subset of the datasets showing a few that worked well with our approach and a few that did not work as well.}
\item[\done]
I would like the authors to add a few more datasets to their experiment. In particular, I would request 3 interesting public datasets from the AutoML Benchmark[1], KDDCup09appetency, jasmine, Amazonemployeeaccess, and Higgs from [31] (binary classification problems that fit the experimental framework of the work).
[1] Gijsbers, P., LeDell, E., Thomas, J., Poirier, S., Bischl, B., Vanschoren, J. (2019). An open source AutoML benchmark. arXiv preprint arXiv:1907.00909.
[2] The work.
\red{I added 2 of the suggested datasets. Of the other two, one was too big for my computer and one would take too long to format. I cited the automl benchmark paper.}
\item[\done]
Line 38: decision trees still outperform on tabular datasets
I would correct decision trees to gradient boosted decision trees since by themselves decision trees are weak learners.
\item[\done]
Line 35: ML competitions with tabular data have been dominated not by deep learning models, but by gradient boosting models
I believe ML competitions quite a few times include an ensembling of DNNs and GBDTs for the top solutions.
\red{This has been included}
\item[\done]
Line 47, Deep learning tends to lose out to XGBoost on structured data and this trend is stronger when training data is limited in size.
\red{This has been omitted.}
\item[\done]
The related work section seems to be lacking, since there have been several works that argue the opposite, that deep learning outperforms tabular data and the works do not mention any trends that deep learning does not perform well when training data is limited [1][2]. From personal experience, I have not observed the trend either.
Kadra, A., Lindauer, M., Hutter, F.,  Grabocka, J. (2021). Well-tuned simple nets excel on tabular datasets. Advances in neural information processing systems, 34, 23928-23941.
Gorishniy, Y., Rubachev, I., Khrulkov, V.,  Babenko, A. (2021). Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34, 18932-18943.
\red{We have expanded our related work section to account for this.}
\item
The method proposed by the authors affects a specific niche, that is an organization that needs to perform a large number of predictions continuously on an already trained model. I do not believe the proposed method affects the majority of researchers in the field of AutoML, since the time taken even without using the multi-stage inference is in the range of milliseconds. Additionally, the latency reduction comes with a performance deterioration. Moreover, preparing the method has a significant overhead on the practitioner since one needs to detect the feature importances (the authors also suggest XGBoost), then organize the data into bins, run LRwithBins and detect the correct hyperparameters based on the hyperparameter n for the number of important features and b number of bins via HPO. Lastly, the practitioner would need to detect the threshold that is also dataset specific for where the performance degrades from using LRBins and XGBoost.
\red{
In a major difference from academic ML research, industrial applications of ML spend much greater resources on inference, to support products. This can be illustrated by ChatGPT, which reportedly costs \$700M/day to run, but only tens of millions USD total to train. The cost of inference is determined by CPU/GPU resources, which are closely related to mean latency. Whether the mean CPU latency is in seconds or milliseconds, it is multiplied by the large number of queries served to obtain total resources. The economics of ML inference often determines business viability.
Additionally, user-observed latency includes network latency and that of database lookups. Many online applications require latency below the cognitive threshold of 300ms, to appear instantaneous, rather than sluggish, to the users.
}
\item
There is a larger space overhead that would be required for every dataset depending on the number of features and number of bins.
\end{todolist}

\item Reviewer 2
\begin{todolist}
\item Highlight impact
\red{
We note that real-time inference dominates resource usage in industry applications by 1-2 orders of magnitude compared to training.
In a major difference from academic ML research, industrial applications of ML spend much greater resources on inference, to support products. This can be illustrated by ChatGPT, which reportedly costs \$700M/day to run, but only tens of millions USD total to train. The cost of inference is determined by CPU/GPU resources, which are closely related to mean latency. Whether the mean CPU latency is in seconds or milliseconds, it is multiplied by the large number of queries served to obtain total resources. Additionally, user-observed latency includes network latency and that of database lookups. Many online applications require latency below the cognitive threshold of 300ms, to appear instantaneous.
}
\item
The technical aspects of the proposed approach are presented in a clear manner, but the section on the algorithm could benefit from more detail. 
\red{
We have added pseudocode and more comments in order to provide more detail.
}
\item
Some parts of the paper lack technical explanations and details, which could make it difficult for readers to fully understand the system. Moreover, while the AutoML part is mentioned, it could have been better highlighted as an important aspect of the proposed approach.
\red{
We have expanded our discussion of using AutoML.
}
\item
Additionally, the paper discusses the tradeoff between ML performance and efficiency of inference, but this aspect could have been given more attention and elaboration, especially since it is an important consideration for practical applications.
\red{limited in space}
\item
The AutoML contribution is limited since AutoML is only used as a tool in the system. Moreover, this part lacks details.
\red{we have expanded on this}
\item
Better highlight AutoML part. Try to better highlight the contribution within the global system.
\red{we have expanded on this}
\end{todolist}

\item Reviewer 3
\begin{todolist}
\item
The authors seem to relegate the AutoML task to hyperparameter tuning, when in my opinion, the work contributes to the automation of multi-stage inference systems. Clarifying the degree of contribution or the current status of that automation could be something interesting.

\item[\done]
Table 1 shows a comparison of logistic regression (LR), LRwBins, and XGBoost, but in table 2 we see the difference between xgboost and hybrid model which is confusing. Is LRwBins the hybrid model? If it is something else, shouldn't it be in table1?
\red{This has been clarified in the caption of table 1.}

\item[\done]
Table 1 leaves the margins. Perhaps you could make the columns smaller by using scientific notation in dataset "size" columns, using .01 instead of 0.01, or using some other shortcut.
\red{We appreciate the suggestion and have incorporated this in the paper.}

\item[\done]
As noted in the Clarity section, comparison between XGBoost and a hybrid model is confusing, and it is unclear if the hybrid model is the same as LRwBins or something different. Clarifying this is important to relate and improve traceability between tables 1 and 2. -More information about the experimental setup could further clarify the contributions of the work. Specifically, information about the infrastructure for the remote model and product code.
\end{todolist}

\item Reviewer 4
\begin{todolist}
\item[\done] This was the code reviewer and has been handled.
\end{todolist}

\item Reviewer 5
\begin{todolist}
\item
The paper presents a solution to an interesting problem, however the perceived importance or widespread adoption is not clearly evident. Authors do not clearly explain real-world adoption scenarios. While they mention some of the advantages(including things like hardware acceleration on edge devices), it is not clear what is scope and scale where such a problem solution will have an impact.
\red{
Both the importance and the widespread adoption of the problem we solve have been described in a recent KDD publication not shown during blind review. In particular, our ML platform is deployed for over a hundred real-time industry applications.
}

\item
The basics of the proposed approach appear to be sound. The way the algorithm is presented, is however, makes it difficult to understand the details easily. There are some design choices and ther corresponding effects not clearly explained. Partitioning the data into subregions through feature grouping is explored. Why is dimensionality reduction not considered ? Line 124: Both b and n should be kept small. But n is data dependent. b i.e quantiles are user provided. It is not clear if b same for all features. If the cardinality of categorical features is rather high, how does the approach perform ? That is not addressed. LRwBins uses hyperparameter tuning, which is a part of AutoML as the authors noted. But the overall approach does not utilize any other autoML concept significantly. In terms of novelty, a simpler model to perform inference is very similar to the idea of knowledge distillation. So the application/implementation is novel but the entire idea is not. Some of the points in the conclusion may require further consideration and explanation.

\item[\done]
Clarity is by far the most significant drawback of this paper. The worst thing is the the entire algorithm is verbosely described in Section 3, without any pseudocode. This makes it very difficult for a reader to understand the details in a linear fashion. Some of the sentences are unnecessarily long(e.g. Line 150). Figures are not very informative. In Fig 1, the authors explain a familiar concept without the insights and connections to the proposed approach in a clear manner. Similarly, Figures 3 and 4 have long winded captions that do not concisely convey the core ideas. In Figure 7, the legends are missing making it hard to interpret. The content is not organized well. Tables are few, and tables like Table 1 misses providing the main message.
\red{We added pseudocode as suggested. The long sentence was fixed. Updated Fig 1 caption to connect to LRwBins. Figure 7 legend added in caption.}

\item
The paper is interesting, but is lacking in several aspects. The narrative clarity is poor and information is not well presented which makes it difficult to understand the details. The technical aspects while sound, authors do not present edge cases or generalization study. The empirical evaluation uses only a few datasets. While some of them are sizable, the applicability of this approach in a broader scenario and different data characteristics is not clear. On the positive side, the authors provide adequate references. The analysis performed is also good, especially to compare loss of accuracy. The case of RPC causing delays in inference which is a valid problem the authors are proposing a solution for.
\end{todolist}
\end{itemize}

\clearpage
Review responses:
\begin{itemize}
\item Review 1:\\
Dear Reviewer,

Thank you for your thoughtful review and valuable feedback on our paper. We have carefully addressed your comments and made the necessary changes to improve our manuscript. Please find our detailed responses below.

Potential Impact On The Field Of AutoML:
We appreciate your insights on the potential impact of our work. We acknowledge that our method solves a specific practical challenge, but it is an important one. Industrial applications of ML often have significant resource demands for inference to support products, and our method aims to optimize the tradeoffs between performance, speed and resource usage.

Figure 7 colors:
Thank you for pointing this out. We have updated the caption to include the information about the colors and datasets in Figure 7.

Table 1 formatting:
We have fixed the formatting issue with Table 1 as you suggested.

Referencing style:
We have carefully considered your suggestion, but have decided to maintain our current referencing style for consistency throughout the paper.

Line 123: Creating subsets:
You are correct in your understanding of the binning process. We have revised the terminology in the manuscript to more accurately reflect the constraints of our binning approach.

Features and bins in Table 1:
Due to page limitations, we have not included these details in the paper. However, we have provided this information in the hyperparameters file of our code supplement, as you suggested.

Dataset selection criteria:
We chose a representative subset of the datasets showing a few that worked well with our approach and a few that did not work as well.

Additional datasets:
Thank you for suggesting additional datasets for our experiment. We have included 2 of the suggested datasets and cited the AutoML benchmark paper. Unfortunately, we were unable to include the other two datasets due to technical limitations.

Line 38, 35, 47 corrections:
We appreciate your suggestions and have made the necessary corrections in our manuscript.
We have expanded our related work section to include the papers you mentioned and to better reflect the current state of research in the field.

Method's applicability and overhead:
In a major difference from academic ML research, industrial applications of ML spend much greater resources on inference, to support products. This can be illustrated by ChatGPT, which reportedly costs \$700M/day to run, but only tens of millions USD total to train. The cost of inference is determined by CPU/GPU resources, which are closely related to mean latency. Whether the mean CPU latency is in seconds or milliseconds, it is multiplied by the large number of queries served to obtain total resources. Additionally, user-observed latency includes network latency and that of database lookups. Many online applications require latency below the cognitive threshold of 300ms, to appear instantaneous. We also clarifiy that the overhead that would be required for every dataset depending on the number of features and number of bins is small in general.

We hope that our revisions have addressed your concerns and improved the quality of our paper. Once again, we appreciate your valuable feedback and the opportunity to enhance our work.

\item Review 2:\\
Dear Reviewer,

Thank you for your valuable feedback on our paper. We appreciate your insights and have made several revisions to address your concerns. Please find our detailed responses below.

Highlight impact:
We note that real-time inference dominates resource usage in industry applications by 1-2 orders of magnitude compared to training.
In a major difference from academic ML research, industrial applications of ML spend much greater resources on inference, to support products. This can be illustrated by ChatGPT, which reportedly costs \$700M/day to run, but only tens of millions USD total to train. The cost of inference is determined by CPU/GPU resources, which are closely related to mean latency. Whether the mean CPU latency is in seconds or milliseconds, it is multiplied by the large number of queries served to obtain total resources. Additionally, user-observed latency includes network latency and that of database lookups. Many online applications require latency below the cognitive threshold of 300ms, to appear instantaneous.

Algorithm detail and clarity:
We have added pseudocode and more comments to provide a clearer and more detailed presentation of the algorithm, making it easier for readers to understand the system.

Technical explanations and AutoML integration:
We have expanded our discussion on using AutoML and its importance as an aspect of the proposed approach. This should provide a more comprehensive understanding of our system and its practical applications.

Tradeoff between ML performance and efficiency of inference:
While we would have liked to provide more attention and elaboration on this tradeoff, we are limited by the space constraints of the paper. We believe our current discussion provides a reasonable balance between exploring this tradeoff and addressing other important aspects of our work.

Once again, we appreciate your valuable feedback and the opportunity to enhance our work.

\item Review 3:\\
Thank you for the insightful comments that help improve the manuscript.
We have clarified the hybrid model confusion in the caption of table 1 and have also modified table 1 according to your comments.
Indeed, our work's contribution to AutoML is beyond hyperparameter tuning, with the main focus on configuring multistage inference to ensure proper tradeoff between efficiency and coverage.  A key empirical result is that the first stage can typically handle over 50\% of inferences, and this does not require hand tuning.
We appreciate your suggestions!

\item Review 4: Already handled.

\item Review 5:\\
Dear Reviewer,

Thank you for your valuable feedback on our paper. We appreciate your insights and have made several revisions to address your concerns. Please find our detailed responses below.

Real-world adoption and impact:
Both the importance and the widespread adoption of the problem we solve have been described in a recent KDD publication not shown during blind review. In particular, our ML platform is deployed for over a hundred real-time industry applications, with millions of inferences per second.

Algorithm clarity and design choices:
We have added pseudocode to improve the clarity of our algorithm presentation, making it easier for readers to understand the details. We have also fixed the long sentence on Line 150 and updated Figure 1 caption to better connect it to LRwBins. Additionally, we added to the Figure 7 caption clarifying the missing information.

Technical explanations and AutoML integration:
We have expanded our discussion on using AutoML and its importance as an aspect of the proposed approach. This should provide a more comprehensive understanding of our system and its practical applications.

Additional comments:
We have made efforts to improve the narrative clarity of our paper by revising sections and adding pseudocode for a better understanding of our proposed algorithm. We acknowledge that presenting edge cases and a generalization study on different data characteristics would be beneficial. However, space constraints make it difficult to include these details in the current version of our paper. 
We plan to investigate these aspects in future work to provide a more comprehensive evaluation of our approach in the arxiv version. Regarding the empirical evaluation, we have added more datasets to demonstrate the applicability of our approach in various scenarios. 

Once again, we appreciate your valuable feedback and the opportunity to enhance our work.

\item General comment:\\
Dear Reviewers,

Thank you for the time and effort you have invested in evaluating our work. We have carefully addressed the comments provided and improved our revised manuscript as a result.

Notably, most reviewers found our paper to be clear ("The authors provide a clear description of the problem they are trying to solve, and their proposed solution appears to be logically consistent with the stated problem", "The work is very well written and is very easy to read. However, it has some details that could improve clarity", "The technical aspects of the proposed approach are presented in a clear manner, but the section on the algorithm could benefit from more detail").
We further enhanced the clarity based on constructive feedback received:
\begin{itemize}
\item We added pseudocode, making it easier for readers to understand the details and follow the proposed method. 
\item We provided requested details.
\item We fixed a variety of small glitches and omissions noted by reviewers, and improved formatting as requested.
\item We added two new datasets in the empirical evaluation.
\item Admittedly, we may have understated the importance of AutoML in our original manuscript. Therefore, our revised manuscript articulates the crucial role of AutoML in the success of multistage inference --- to facilitate key tradeoffs between the two stages of inference and to configure the first stage. 
% We now provide a clearer explanation of how AutoML is integrated into our approach,
% further highlighting its significance in the overall framework.
\item We are particularly grateful to the reviewer who provided comments on our code, and we addressed those as well, making the code easier to understand and reuse.
\end{itemize}

% Finally, we elaborated on the use of AutoML in our approach, going beyond hyperparameter tuning and demonstrating its integral role in the overall system.

We also addressed several misconceptions raised by the reviewers.
\begin{itemize}
\item Optimizing inference for real-time ML systems is not a niche area but rather a mainstream concern in the industry, where efficient ML inference impacts resource consumption and affects business profitability. This is critical in a growing number of real-time ML deployments --- within Web search, ad networks, many applications in social networks, various recommendation systems, etc.
\item Optimizing mean inference latency in terms of (tens and hundreds of) milliseconds is not a limitation, but rather a typical industry context. Optimizing mean CPU latency (without increasing the parallelism) helps reduce overall CPU resources.
\item The widespread impact of our proposed solution in real-time industry applications should be clearer when we disclose our affiliation and prior publications in the published paper (but we cannot do this during blind review). Our solution is deployed in a real-time industry ML platform that performs millions inferences per second for a variety of applications with billions (with a B) users worldwide.
%We highlighted the importance and adoption of our approach in real-world scenarios, emphasizing its relevance and potential impact on a large scale. 
\end{itemize}

We trust that our revisions address the reviewers' concerns and enhance the quality of the manuscript, making it suitable for publication.

Thank you once again for your valuable feedback and the opportunity to improve our work.

The Authors

\end{itemize}
