\section{\red{DAC Reviews that need to be addressed}}

\red{
Consolidated comments to consider:
\begin{itemize}
    \item The test data should be verified as easy and difficult before inference. However, in Table III, the latency incurred by this verification is not considered.
    \blue{
    \item This is already counted in the multi-stage inference time. I added a comment in red in Table \ref{tab:latency} for clarification.
    }
    \item In the 3rd paragraph of Section III, the authors said that test data are sorted by ROC AUC to divide them into the first and second stages. However, they did not describe how to get the RUC AUC of the test data.
    \blue{
    \item The test data is never divided as such. The test inference is described well in the Training and Inference section in section 4. The validation data separation by ROCAUC and accuracy is described well in section 3. I think no action is required.
    }
    \item In the 1st paragraph of Section III, the authors split each of the n features into b bins, but they did not describe how to extract data features. The authors should also describe how to select important features in detail.
    \blue{
    \item We state: "We first use a model-free (such as MRMR \cite{ding2005minimum}) or model-based (such as XGBoost feature importance ranking \cite{chen2016xgboost}) approach to determine the relative importance of our features." I think no action is required.
    }
    \item In the 2nd paragraph of the Introduction, the authors said that it is important to make the first-stage model simpler than the second-stage model,  please first specify what the two stages are.
    \blue{
    \item This is a good point. I added some clarification in red in the second paragraph of the intro.
    }
    \item In Fig. 1, if there is a complex data set, more linear approximations may be required to fit the nonlinear blue curve, which incurs large area overheads.
    \blue{
    \item True, but this is motivation. I think no action required.
    }
    \item In Fig. 2, the authors should specify the meaning of q and x.
    \blue{
    \item Added clarification in red in caption text.
    }
    \item In Fig. 3, the authors should describe what the task is, and why the box represented by the green column has a lot of similar data.
    \blue{
    \item I think we describe the task. I don't think it is important to discuss the green column with a lot of similar data. I think no action required.
    }
    \item In Fig, 3, a steep drop-off in performance around 50K data rows offers a good separator.  The authors should specify the data before 50K is allocated to the first stage.
    \blue{
    \item This is already stated.
    }
    \item In Fig. 4, please describe what the vertical axis represents. In addition, the distribution of scale values in XGBoost is uneven.
    \blue{
    \item I think this is described well in the figure and caption.
    }
    \item In Fig.5, the authors clarify feature selection for inference in the LRwBins model, both the opacity and integers represent the importance. The leftmost red feature has an importance of integer 171 and the color is close to transparent, while the middle red feature has an importance of integer 0 and the color is opaque. The authors did not describe how to sort the importance of features like these two. In addition, The authors should also describe how many important features are selected for the task shown in Fig. 5.
    \blue{
    \item todo
    }
    \item Fig. 6 only illustrates the meaning of the blue line, the authors should add the meaning of the red and green lines. Besides, please explain why the accuracy loss of the red line is smaller than the ROC AUC performance loss.
    \blue{
    \item todo
    }
    \item In the 1st paragraph of the Introduction, ‘make practical impact' should be corrected to ‘make practical impacts'.
    \blue{
    \item Fixed.
    }
    \item In Section III-A, ‘use the the top n important features' should be corrected to ‘use the top n important features'.
    \blue{
    \item Fixed.
    }
    \item The figures in the submission are blurred. I recommend the authors use pdf to draw figures.
    \blue{
    \item todo
    }
    \item There are many works on multi-stage/hierarchical inference, yet there is almost no discussion of related work.
    \blue{
    \item Added in the intro.
    }
    \item There is no experimental comparison to similar works.
    \blue{
    \item True, but I don't think I am going to be able to add such a comparison.
    }
    \item The work strongly motivates real-time inference but the presented technique is not real-time capable.
    \blue{
    \item I think we addressed this with recent changes.
    }
    \item Most importantly, despite many existing prior works on multi-stage/hierarchical inference [2-5], there is almost no discussion of them. Only a single related work [1] is briefly discussed in half a sentence. This is despite very similar concepts being explored in [2-5]. ([2]: use a lightweight classifier to decide whether to perform local inference with a small model or whether to offload inference via the network to a powerful model; [3 and 4]: use a lightweight model that classifies simple examples and indicates "unknown" for more complex examples, which are then classified by a powerful model; [4]: incrementally increase the size of decision forests depending on the complexity of the example). Without such discussion, the novelty of this approach is not clear. Furthermore, the experimental analysis does not compare against alternative approaches (e.g., [1,2,5]). Without such comparison, it is not clear whether the proposed technique is beneficial compared to others. Finally, the work strongly motivates real-time inference but the presented technique is not real-time capable. In the worst case, still network RPC calls are needed for inference, which is unpredictable. The authors should reduce the motivation for real-time and instead motivate for the average inference latency.
    [1] (Already cited by this work) Kraft et al. Willump: A statistically-aware end-to-end optimizer for machine learning inference. MLSys. 2020.
    [2] Samie et al. Hierarchical Classification for Constrained IoT Devices: A Case Study on Human Activity Recognition. IoT-J. 2020.
    [3] Park et al. Big/Little Deep Neural Network for Ultra Low Power Inference. CODES. 2015.
    [4] Daghero et al. Two-stage Human Activity Recognition on Microcontrollers with Decision Trees and CNNs. PRIME. 2022.
    [5] Daghero et al. Adaptive Random Forests for Energy-Efficient Inference on Microcontrollers. VLSI-SoC. 2021.
    \blue{
    \item Added related work discussion in red in the intro.
    }
    \item More ablation studies are needed.
    \blue{
    \item Maybe, but I don't think I have the time to add this.
    }
    \item The running time results should be provided.
    \blue{
    \item These are provided.
    }
\end{itemize}
}

\begin{verbatim}
REVIEWER #1

Clarity / Writing Style (1-5): 3

      Originality / Innovativeness (1-5): 3
      
    Impact of Ideas and/or Results (1-5): 2
    
            OVERALL RECOMMENDATION (1-5): 2
\end{verbatim}

Summarize shortly the contributions of the paper in your own words:
The traditional machine learning system performs data inference via Remote Procedure Call (RPC) APIs, which increases network latency and entails additional CPU costs. In this paper, the authors embed a simple model into the product code to quickly process partial inferences. When data is difficult to infer, the system calls the complex model ‘XGBoost' through the RPC API to perform more precise inference. The proposed multi-stage inference method reduces the network latency and CPU costs efficiently.

Strengths:
1. The proposed multi-stage inference method achieves large performance gain by handling half of the data within the product code, it also maintains accuracy across various tabular datasets.
2. The proposed high-performance production system can achieve a 1.3x drop in latency and a 30\% drop in CPU usage compared to the traditional method.

Weaknesses:
1. The test data should be verified as easy and difficult before inference. However, in Table III, the latency incurred by this verification is not considered.
2. In the 3rd paragraph of Section III, the authors said that test data are sorted by ROC AUC to divide them into the first and second stages. However, they did not describe how to get the RUC AUC of the test data.
3. In the 1st paragraph of Section III, the authors split each of the n features into b bins, but they did not describe how to extract data features. The authors should also describe how to select important features in detail.

Main Discussion of Paper:
1. In the 2nd paragraph of the Introduction, the authors said that it is important to make the first-stage model simpler than the second-stage model,  please first specify what the two stages are.
2. In Fig. 1, if there is a complex data set, more linear approximations may be required to fit the nonlinear blue curve, which incurs large area overheads.
3. In Fig. 2, the authors should specify the meaning of q and x.
4. In Fig. 3, the authors should describe what the task is, and why the box represented by the green column has a lot of similar data.
5. In Fig, 3, a steep drop-off in performance around 50K data rows offers a good separator.  The authors should specify the data before 50K is allocated to the first stage.
6. In Fig. 4, please describe what the vertical axis represents. In addition, the distribution of scale values in XGBoost is uneven.
7. In Fig.5, the authors clarify feature selection for inference in the LRwBins model, both the opacity and integers represent the importance. The leftmost red feature has an importance of integer 171 and the color is close to transparent, while the middle red feature has an importance of integer 0 and the color is opaque. The authors did not describe how to sort the importance of features like these two. In addition, The authors should also describe how many important features are selected for the task shown in Fig. 5.
8. Fig. 6 only illustrates the meaning of the blue line, the authors should add the meaning of the red and green lines. Besides, please explain why the accuracy loss of the red line is smaller than the ROC AUC performance loss.
9. In the 1st paragraph of the Introduction, ‘make practical impact' should be corrected to ‘make practical impacts'.
10. In Section III-A, ‘use the the top n important features' should be corrected to ‘use the top n important features'.

Most prominent Strength or Weakness:
1. The test data should be verified as easy and difficult before inference. However, in Table III, the latency incurred by this verification is not considered.

\begin{verbatim}
                            REVIEWER #2

           Clarity / Writing Style (1-5): 4
           
      Originality / Innovativeness (1-5): 4
      
    Impact of Ideas and/or Results (1-5): 4
    
            OVERALL RECOMMENDATION (1-5): 4
\end{verbatim}


Summarize shortly the contributions of the paper in your own words:
The authors argue that conventional wisdom favors segregating ML code into services queried by product code causes extra overhead and CPU costs, and therefore propose to simplify inference algorithms and embed them into the product code to reduce network communication. Such inference optimization leads to 30\% reduction in CPU resources and 1.3x reduction in mean latency for a commercial ML platform that serves millions of real-time decisions per second.

Strengths
+ ...The paper is clearly stated and well-motivated.
+ ... The proposed algorithm sounds solid.
+ ... The computation reduction is promising.
+ ... The source code is provided.

Weaknesses:
- ... LRwBins suffers from a minor performance drop compared with XGB. 
- ... The figures in the submission are blurred. I recommend the authors use pdf to draw figures.

Main Discussion of Paper:
The paper is well-motivated by the fact that the common-used method segregates ML code into services queried by product code causing extra overhead. Stemming from this motivation, the paper further proposes an efficient inference that reduces 30\% of inference time while achieving comparable performance. Overall, I believe this paper is above the acceptance bar.

Most prominent Strength or Weakness:
The paper is self-containable with good motivation, sound method and convincing results.

\begin{verbatim}
                            REVIEWER #3

           Clarity / Writing Style (1-5): 5
           
      Originality / Innovativeness (1-5): 3
      
    Impact of Ideas and/or Results (1-5): 3
    
            OVERALL RECOMMENDATION (1-5): 2
\end{verbatim}


Summarize shortly the contributions of the paper in your own words:
This work presents a two-stage inference technique to reduce the required computations for inference in the average case. They divide the feature space into areas and train a separate lightweight classifier for each area. They then decide for each area whether the lightweight classifier should be used for fast inference or whether the powerful backend classifier should be invoked to maintain high accuracy.

Strengths:
+ The evaluation was performed in a production setting.
+ The technique is well described and the source code would be released, facilitating reproducing the results.
+ The discussion of limitations and unsuccessful design explorations is helpful.

Weaknesses:
- There are many works on multi-stage/hierarchical inference, yet there is almost no discussion of related work.
- There is no experimental comparison to similar works.
- The work strongly motivates real-time inference but the presented technique is not real-time capable.

Main Discussion of Paper:
This work presents a two-stage inference technique to reduce the required computations for inference in the average case. They divide the feature space into areas and train a separate lightweight classifier (logistic regression) for each area. They then decide for each area whether the lightweight classifier should be used for fast inference or whether the powerful backend classifier should be invoked via the network to maintain high accuracy, exploiting a trade-off between average latency and average accuracy.

The technique is well described and the source code would be released, which would facilitate reproducing the results. This includes a discussion of the limitations and unsuccessful design choices, which help the reader better grasp the ideas and underlying explorations. Finally, the technique was evaluated in a production setting, which strengthens the significance of the reported improvements.

However, there are also several significant shortcomings in this work. Most importantly, despite many existing prior works on multi-stage/hierarchical inference [2-5], there is almost no discussion of them. Only a single related work [1] is briefly discussed in half a sentence. This is despite very similar concepts being explored in [2-5]. ([2]: use a lightweight classifier to decide whether to perform local inference with a small model or whether to offload inference via the network to a powerful model; [3 and 4]: use a lightweight model that classifies simple examples and indicates "unknown" for more complex examples, which are then classified by a powerful model; [4]: incrementally increase the size of decision forests depending on the complexity of the example). Without such discussion, the novelty of this approach is not clear. Furthermore, the experimental analysis does not compare against alternative approaches (e.g., [1,2,5]). Without such comparison, it is not clear!
  whether the proposed technique is beneficial compared to others. Finally, the work strongly motivates real-time inference but the presented technique is not real-time capable. In the worst case, still network RPC calls are needed for inference, which is unpredictable. The authors should reduce the motivation for real-time and instead motivate for the average inference latency.

[1] (Already cited by this work) Kraft et al. Willump: A statistically-aware end-to-end optimizer for machine learning inference. MLSys. 2020.
[2] Samie et al. Hierarchical Classification for Constrained IoT Devices: A Case Study on Human Activity Recognition. IoT-J. 2020.
[3] Park et al. Big/Little Deep Neural Network for Ultra Low Power Inference. CODES. 2015.
[4] Daghero et al. Two-stage Human Activity Recognition on Microcontrollers with Decision Trees and CNNs. PRIME. 2022.
[5] Daghero et al. Adaptive Random Forests for Energy-Efficient Inference on Microcontrollers. VLSI-SoC. 2021.

Most prominent Strength or Weakness:
Missing discussion and comparison to related work.

\begin{verbatim}
                            REVIEWER #4

           Clarity / Writing Style (1-5): 3
           
      Originality / Innovativeness (1-5): 3
      
    Impact of Ideas and/or Results (1-5): 4
    
            OVERALL RECOMMENDATION (1-5): 3
\end{verbatim}

\red{
Dan:
I think we can delete this review since we have runtime results in Table \ref{tab:latency} and I feel like we clearly state why we are getting better performance.
}

Summarize shortly the contributions of the paper in your own words:
This paper provides an efficient multi-stage inference on tabular data. It improves the efficiency of real-time inference by reducing the inference latency and CPU resource usage with a little drop in performance, such as AUC and accuracy.

Strengths:
+ The paper is well-written. It clearly states its technical contribution.
+ The experimental results are comprehensive.

Weaknesses:
- More ablation studies are needed.
- The running time results should be provided.

Main Discussion of Paper:
The authors compared XGBoost and LRwBins with the proposed multi-stage model. However, it is unclear to me which component is the major reason for improved performance. It would be good if the authors could conduct some ablation studies.
Since the paper mainly claims system-level improvement, the running time comparison should be included.

Most prominent Strength or Weakness:
Good idea with a clean presentation. But more experiments are needed.
