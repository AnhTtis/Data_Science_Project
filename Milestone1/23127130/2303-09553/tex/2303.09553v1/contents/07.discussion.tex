
\input{figures/limitations}
\section{Limitations}
\label{sec:limitations}
\algname{} has limitations associated with both CLIP and NeRF; some are visualized in Fig.~\ref{fig:limitations}. 
Like CLIP, language queries from \algname{} often exhibit ``bag-of-words" behavior (i.e., \textit{``not red"} is similar to \textit{``red"}) and struggles to capture spatial relationships between objects. \algname{} can be prone to false positives with queries that appear visually or semantically similar: \textit{``zucchinis"} activate on other similarly-shaped vegetables, though zucchinis are more relevant than the distractors (Fig.~\ref{fig:limitations}).


\algname{} requires known calibrated camera matrices and NeRF-quality multi-view captures, which aren't always available or easy to capture. The quality of language fields is bottlenecked by the quality of the NeRF recontsruction. In addition, because of the volumetric input to $F_\text{lang}$, objects which are near other surfaces without side views can result in their embeddings being blurred to their surroundings since no views see the background without the object. This results in similar blurry relevancy maps to single-view CLIP (Fig.~\ref{fig:clip-3d-comparison}). In addition, we only render language embeddings from a single scale for a given query. Some queries could benefit from or even require incorporating context from multiple scales (eg \textit{``table"}).

\section{Conclusions}
We present \algname{}, a novel method of fusing raw CLIP embeddings into a NeRF in a dense, multi-scale fashion without requiring region proposals or fine-tuning. We find that it can support a broad range of natural language queries across diverse real-world scenes, strongly outperforming pixel-aligned LSeg in supporting natural language queries. \algname{} is a general framework that supports any aligned multi-modal encoders, meaning it can naturally support improvements to vision-language models. Code and datasets will be released after the submission process.

\section{Acknowledgements}
This material is based upon work supported by the National Science Foundation Graduate Research
Fellowship Program under Grant No. DGE 2146752. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. The authors were supported in part by equipment grants from NVIDIA. We thank our colleagues who provided helpful feedback and suggestions, in particular Jessy Lin, Simeon Adebola, Satvik Sharma, Abhik Ahuja, Rohan Mathur, and Alexander Kristoffersen for their helpful feedback on paper drafts, and Boyi Li, Eric Wallace, and members of the Nerfstudio team for their generous conversations about the project.