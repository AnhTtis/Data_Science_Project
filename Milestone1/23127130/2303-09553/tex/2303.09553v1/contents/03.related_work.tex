\input{figures/02.methods_pipeline}
\section{Related Work}
\paragraph{Open-Vocabulary Object Detection}
A number of approaches study detecting objects in 2D images given natural language prompts. These lie on a spectrum from purely zero-shot to fully trained on segmentation datasets.
LSeg~\cite{li2022language_lseg} trains a 
2D image encoder on labeled segmentation datasets, which outputs pixel-wise embeddings that best match the CLIP text embedding of the segmentation label at that given pixel. CRIS~\cite{wang2022cris} and CLIPSeg~\cite{luddecke2022image_clipseg} train a 2D image decoder to output a relevancy map based on the query CLIP embedding and the intermediate outputs of the pretrained CLIP image encoder. However, such fine-tuning approaches tend to lose significant language capabilities by training on a smaller dataset.

Another common approach for 2D images is a two-stage framework wherein class-agnostic region or mask proposals direct where to query open-vocabulary classification models. OpenSeg~\cite{ghiasi2021open_openseg} simultaneously learns a mask prediction model while predicting the text embeddings for each mask, while ViLD~\cite{gu2021open_vild} directly uses CLIP~\cite{radford2021learning} to classify 2D regions from class-agnostic mask proposal networks. Detic~\cite{zhou2022detecting_detic} builds on existing two-stage object detector approaches, but demonstrates a greater generalization ability by allowing detector classifiers to train with image classification data. OWL-ViT~\cite{minderer2022simple_owlvit} attaches lightweight object classification and localization heads after a pre-trained 2D image encoder. Although region proposal-based approaches can leverage more detection data, these proposal generators still tend to output within the training set distribution. Consequently, as noted by the authors of OV-Seg~\cite{liang2022open_ovseg}, such networks often face difficulties in accurately segmenting unlabeled hierarchical components of the original masks, such as object parts. \algname{} strives to avoid region proposals by incorporating language embeddings in a dense, 3D, multi-scale field which allows hierarchical text queries.

%0-shot 
Grad-CAM~\cite{selvaraju2017grad} and attention-based methods \cite{chefer2021generic} provide a relevancy mapping between 2D images and text in vision-language models (e.g., CLIP). Works such as Semantic Abstraction~\cite{ha2022semantic} have shown that these frameworks can be used to detect long-tail objects for scene understanding. Outputs from \algname{} are most similar in spirit to these methods, outputting a 3D relevancy score given a query. However, \algname{} builds a 3D representation that can be queried with different text prompts without reconstructing the underlying representation each time, and in addition fuses multiple viewpoints into a single shared scene representation, rather than operating per-image.
 
\paragraph{Distilling 2D Features into NeRF}
NeRF has an attractive property of averaging information across multiple views. Several prior works leverage this to improve the quality of 2D semantics, segmentations, or feature vectors by distilling them into 3D. Semantic NeRF~\cite{semantic_nerf} and Panoptic Lifting~\cite{panoptic_lifting} embed semantic information from semantic segmentation networks into 3D, showing that combining noisy or sparse labels in 3D can result in crisp 3D segmentations. This concept has been applied to segmenting objects with extremely sparse user input scribbles of foreground-background masks~\cite{ren-cvpr2022-nvos}. Our approach draws inspiration from these works by averaging multiple potentially noisy language embeddings over input views. 

Distilled Feature Fields~\cite{kobayashi2022decomposing} and Neural Feature Fusion Fields~\cite{tschernezki22neural} explore embedding pixel-aligned feature vectors from LSeg or DINO~\cite{dino} into a NeRF, and show they can be used for 3D manipulations of the underlying geometry. \algname{} similarly embeds feature vectors into NeRF, but also demonstrates a method to distill non pixel-aligned embeddings (e.g., from CLIP) into 3D without fine-tuning.
\input{figures/full-page-results}
\paragraph{3D Language Grounding}
Incorporating language into 3D has been explored in a wide range of contexts: 3D visual question answering~\cite{gordon2018iqa,azuma2022scanqa,cascante2022simvqa} leverage 3D information to extract answers to queries about the environment. In addition, incorporating language with shape information can improve object recognition via text~\cite{corona2022voxel,thomason2022language}. 

\algname{} is more similar to 3D scene representations in robotics which fuse vision-language embeddings to support natural language interaction. VL-Maps~\cite{vlmaps} and OpenScene~\cite{peng2022openscene} build a 3D volume of language features which can be queried for navigation tasks, by using pre-trained pixel-aligned language encoders~\cite{ghiasi2021open_openseg, li2022language_lseg}.
In \algname{}, we compare against one such encoder, LSeg, in 3D and find it loses significant expressive capability compared to CLIP.

CLIP-Fields~\cite{shafiullah2022clip} and NLMaps-SayCan~\cite{chen2022open} fuse CLIP embeddings of crops into pointclouds, using a contrastively supervised field and classical pointcloud fusion respectively. In CLIP-Fields, the crop locations are guided by Detic~\cite{zhou2022detecting_detic}. On the other hand, NLMaps-SayCan relies on region proposal networks. These maps are sparser than \algname{} as they primarily query CLIP on detected objects rather than densely throughout views of the scene. Concurrent work ConceptFusion~\cite{conceptfusion} fuses CLIP features more densely in RGBD pointclouds, using Mask2Former~\cite{cheng2022masked} to predict regions of interest, meaning it can lose objects which are out of distribution to Mask2Former's training set. In contrast, \algname{} does not use region or mask proposals.

\algname{} contributes a new dense, volumetric interface for 3D text queries which can integrate with a broad range of downstream applications of 3D language, improving the resolution and fidelity at which these methods can query the environment when multi-view inputs are available. This is enabled by the smoothing behavior of embedding potentially noisy feature vectors from multiple views into the dense \algname{} structure.