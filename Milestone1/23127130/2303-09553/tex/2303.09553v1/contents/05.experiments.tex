\input{figures/lseg_compare}

\section{Experiments}
We examine the capabilities of \algname{} and find that it can effectively process a wide variety of input text queries, encompassing various aspects of natural language specifications that current open-vocab detection frameworks encounter difficulty with. Though existing 3D scan datasets exist, they tend to be either of singulated objects~\cite{co3d,objaverse}, or are RGB-D scans without enough views to optimize high quality NeRFs~\cite{scannet}, and such simulated or scanned scenes contain few long-tail objects~\cite{straub2019replica}. Emphasizing the capability of \algname{} to handle real-world data, we collect 13 scenes containing a mixture of in-the-wild (grocery store, kitchen, bookstore) and posed long-tail (teatime, figurines, hand) scenes. We capture scenes using the iPhone app Polycam, which runs on-board SLAM to find camera poses, and use images of resolution 994$\times$738.
\input{figures/03.point_at_table}
\subsection{Qualitative Results}
\label{sec:qualitative}
We visualize relevancy score by normalizing the colormap for each query from 50\% (less relevant than canonical phrases) to the maximum relevancy. Extensive visualizations of all scenes can be found in the Appendix, and in Fig.~\ref{fig:results} we select 5 representative scenes which demonstrate \algname{}'s ability to handle natural language. Visual comparison with LSeg in 3D are presented in Fig~\ref{fig:lseg_compare}.

\algname{} captures language features of a scene at different levels of detail, supporting queries of properties like \textit{``yellow"}, as well as highly specific queries like names of books and specific characters from TV shows (\textit{``jake from adventure time"}).
Because of the lack of discrete categories, objects can be relevant to multiple queries: in the figurine scene, abstract text queries can create semantically meaningful groupings. \textit{``Cartoon"} selects the cat, Jake, rubber duck, miffy, waldo, toy elephant. \textit{``Bath toy"} selects rubber-like objects, such as rubber duck, Jake, and toy elephant (made of rubber). Toy elephant is strongly highlighted for three different queries, demonstrating the ability of \algname{} to support different semantic tags for the same object.


\input{figures/existence_plot}

\subsection{Existence Determination}
\label{sec:existence}
We evaluate if \algname{} can detect whether an object exists within a scene. We label ground truth existence for 5 scenes, collecting two sets of labels: 1) labels from COCO to represent in-distribution objects to LSeg and 2) our own long-tail labels, which consist of queries of 15-20 objects in each scene concatenated together, for 81 total queries.
See the Appendix for all queries. 
\algname{} determines whether an object exists in the scene by rendering a dense pointcloud over visible geometry, and returns ``True" if any point has a relevancy score over a threshold.

We compare against distilling LSeg features into 3D as in DFF~\cite{kobayashi2022decomposing}, but implemented in our own codebase for an apples-to-apples comparison. We remove scale as a parameter to $F_\text{lang}$ for LSeg since it outputs pixel-aligned features. We report precision-recall curves over relevancy score thresholds in Fig.~\ref{fig:existence_pr}. This experiment reveals that LSeg, trained on limited segmentation datasets, lacks the ability to represent natural language effectively. Instead, it only performs well on common objects that are within the distribution of its training set, as demonstrated in Fig.~\ref{fig:lseg_compare}.




\subsection{Localization}
To evaluate how well \algname{} can localize text prompts in a scene we render \textit{novel} views and label bounding boxes for 72 objects across 5 scenes. For 3D methods we consider a label a success if the highest relevancy pixel lands inside the box, or for OwL-ViT if the center of the predicted box does. Results are presented in Table~\ref{tab:point_at} and Fig.~\ref{fig:localization_test}, and suggest that language embeddings embedded in \algname{} strongly outperform LSeg in 3D for localizing relevant parts of a scene. We also compare against the 2D open-vocab detector OWL-ViT by rendering full-HD NeRF views and selecting the bounding box with the highest confidence score for the text query. OwL-ViT outperforms LSeg in 3D, but suffers compared to \algname{} on long-tail queries.


\subsection{Ablations}
\label{sec:ablations}
\textbf{No DINO}: Removing DINO results in a qualitative deterioration in the smoothness and boundaries of relevancy maps, especially in regions with few surrounding views or little geometric separation between foreground and background. We show two illustrative examples where DINO improves the quality of relevancy maps in Fig.~\ref{fig:ablations}.

\textbf{Single-Scale Training}: We ablate multi-scale CLIP supervision from the pipeline by only training on a fixed $s_0=15\%$ image scale. Doing so significantly impairs \algname{}'s ability to handle queries of \textit{all} scales, failing on both large (\textit{``espresso machine"}) queries it doesn't have enough context for, as well as queries for which it does (\textit{``creamer pods"}). These results imply that multi-scale training regularizes the language field at all scales, not just ones with relevant context for a given query.