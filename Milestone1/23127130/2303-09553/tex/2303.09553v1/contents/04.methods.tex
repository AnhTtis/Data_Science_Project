

\section{\algfull{}}
\label{sec:methods}

Given a set of calibrated input images, we ground CLIP embeddings into a 3D field within NeRF. However, querying a CLIP embedding for a single 3D point is ambiguous, as CLIP is inherently a global image embedding and not conducive to pixel-aligned feature extraction. To account for this property, we propose a novel approach that involves learning a field of language embeddings over \textit{volumes} centered at the sample point. Specifically, the output of this field is the average CLIP embedding across all training views of image crops containing the specified volume. By reframing the query from points to volumes, we can effectively supervise a dense field from coarse crops of input images, which can be rendered in a pixel-aligned manner by conditioning on a given volume scale. 

\subsection{\algname{} Volumetric Rendering}
NeRF takes in a position $\vec x$ and view direction $\vec d$ and outputs color $\vec c$ and density $\sigma$. Samples of these values can be composited along a ray to produce a pixel's color. To create \algname{}, we augment NeRF's outputs with a language embedding $F_\text{lang}(\vec x,s)\in \mathbb{R}^d$, which takes an input position $\vec x$ and physical scale $s$, and outputs a $d$-dimensional language embedding. We choose this output to be view-\textit{independent}, since the semantic meaning of a location should be invariant to viewing angle. This allows multiple views to contribute to the same field input, averaging their embeddings together. The scale $s$ represents the side length in world coordinates of a cube centered at $\vec x$, and is analogous to how Mip-NeRF\cite{mipnerf,mipnerf360} incorporates different scales via integrated positional encodings.

Rendering color and density from \algname{} remains exactly the same as NeRF. To render language embeddings into an image, we adopt a similar technique as prior work\cite{kobayashi2022decomposing,semantic_nerf} to render language embeddings along a ray $\vec{r}(t)=\vec{o}_t+t\vec{d}$. However, since \algname{} is a field over \textit{volumes}, not points, we must also define a scale parameter for each position along the ray. We achieve this by fixing an initial scale in the image plane $s_\text{img}$ and define $s(t)$ to increase proportionally with focal length and sample distance from the ray origin: $s(t)=s_\text{img}*f_{xy}/t$ (Fig.~\ref{fig:pipeline}, left). Geometrically, this represents a frustrum along the ray. We calculate rendering weights as in NeRF: $T(t)=\int_t\text{exp}\left(-\sigma(s)ds\right) ,~w(t)=\int_tT(t)\sigma(t)dt$, then integrate the \algname{} to obtain raw language outputs: 
$\hat \phi_\text{lang}=\int_t w(t) F_\text{lang}\left(r(t),s(t)\right)dt$, and finally normalize each embedding to the unit sphere as in CLIP: $\phi_\text{lang}=\hat \phi_\text{lang}/||\hat \phi_\text{lang}||$.  We find that spherical interpolation between samples on a ray is unnecessary because non-zero weight samples along a ray tend to be spatially close, and instead opt for a weighted euclidean average followed by normalization for implementation simplicity. % and speed.

\input{figures/clip_2d_3d}
\subsection{Multi-Scale Supervision}
To supervise language field outputs $F_\text{lang}$, recall that we can only query language embeddings over image patches, not pixels. Therefore, to supervise the multi-scale \algname{}, we supervise each rendered frustrum with an image crop of size $s_\text{img}$ centered at the image pixel where the ray originated. In practice, re-computing a CLIP embedding for each ray during \algname{} optimization would be prohibitively expensive, so instead we pre-compute an image pyramid over multiple image crop scales and store the CLIP embeddings of each crop (Fig~\ref{fig:pipeline}, right). This pyramid has $n$ layers sampled between $s_\text{min}$ and $s_\text{max}$, with each crop arranged in a grid with 50\% overlap between crops. 

During training, we randomly sample ray origins uniformly throughout input views, and uniformly randomly select $s_\text{img}\in(s_\text{min},s_\text{max})$ for each. Since these samples don't necessarily fall in the center of a crop in this image pyramid, 
we perform trilinear interpolation between the embeddings from the 4 nearest crops for the scale above and below
to produce the final ground truth embedding $\phi_\text{lang}^\text{gt}$. We minimize a loss between rendered and ground truth embeddings which maximizes cosine similarity between the two, scaling by a constant $\lambda_\text{lang}$ (Sec.~\ref{sec:dino}): $L_\text{lang}=-\lambda_\text{lang}\phi_\text{lang}\cdot \phi_\text{lang}^\text{gt}$.


\input{figures/ablations}
\subsection{DINO Regularization} 
\label{sec:dino}
Na\"ively implementing \algname{} as described produces cohesive results, but the resulting relevancy maps can sometimes be patchy and contain outliers in regions with few views or little foreground-background separation (Fig. \ref{fig:ablations}). 

To mitigate this, we additionally train a field $F_{\text{dino}}(\vec x)$ which outputs a DINO\cite{dino} feature at each point. DINO has been shown to exhibit emergent object decomposition properties despite training on no labels\cite{amir2021deep}, and additionally distills well into 3D fields\cite{kobayashi2022decomposing}, making it a good candidate for grouping language in 3D  without relying on labeled data or imparting too strict a prior. Because DINO outputs pixel-aligned features, $F_\text{dino}$ does not take in scale as an input, and is directly supervised for each ray with the DINO feature it corresponds to. We render $\phi_\text{dino}$ identically to $\phi_\text{lang}$ except without normalizing to a unit sphere, and supervise it with MSE loss on ground-truth DINO features.
% $$L_\text{dino}=||\phi_\text{dino}-\phi_\text{dino}^\text{gt}||_2$$
DINO is used explicitly during inference, and only serves as an extra regularizer during training since CLIP and DINO output heads share an architectural backbone.
\subsection{Field Architecture}
Intuitively, optimizing a language embedding in 3D should not influence the distribution of density in the underlying scene representation. We capture this inductive bias in \algname{} by training two separate networks: one for feature vectors (DINO, CLIP), and the other for standard NeRF outputs (color, density). Gradients from $L_\text{lang}$ and $L_\text{dino}$ \textit{do not affect} the NeRF outputs, and can be viewed as jointly optimizing a language field in conjunction with a radiance field.

We represent both fields with a multi-resolution hashgrid \cite{muller2022instant}. The language hashgrid has two output MLPs for CLIP and DINO respectively. Scale $s$ is passed into the CLIP MLP as an extra input in addition to the concatenated hashgrid features. We adopt the Nerfacto method from Nerfstudio~\cite{tancik2023nerfstudio} as the backbone for our approach, leveraging the same proposal sampling, scene contraction, and appearance embeddings

\subsection{Querying \algname{}}
\label{sec:querying}
Often, language models like CLIP are evaluated on zero-shot classification, where a category is selected from a list guaranteed to include the correct category~\cite{radford2021learning}. However, in practical usage of \algname{} on in-the-wild scenes, an exhaustive list of categories is not available. We view the open-endedness and ambiguity of natural language as a benefit, and propose a method to query 3D relevancy maps from the \algname{} given an arbitrary text query. Querying \algname{} has two parts: 1) obtaining a relevancy score for a rendered embedding and 2) automatically selecting a scale $s$ given the prompt.

\textbf{Relevancy Score}: To assign each rendered language embedding $\phi_\text{lang}$ a score, we compute the CLIP embedding of the text query $\phi_\text{quer}$, along with a set of canonical phrases $\phi_\text{canon}^i$. We compute cosine similarity between the rendered embedding and canonical phrase embeddings, then compute the pairwise softmax between the rendered embedding text prompts. The relevancy score is then
$\text{min}_i~\frac{\text{exp}(\phi_\text{lang}\cdot\phi_\text{quer})}{\text{exp}(\phi_\text{lang}\cdot\phi_\text{canon}^i)+\text{exp}(\phi_\text{lang}\cdot\phi_\text{quer}))}$.
Intuitively, this score represents how much closer the rendered embedding is towards the query embedding compared to the canonical embeddings. All renderings use the same canonical phrases: \textit{``object"}, \textit{``things"}, \textit{``stuff"}, and \textit{``texture"}. We chose these as qualitatively ``average" words for queries users might make, and found them to be surprisingly robust to queries ranging from incredibly specific to visual or abstract. We acknowledge that choosing these phrases is susceptible to prompt-engineering, and think fine-tuning them could be an interesting future work, perhaps incorporating feedback from user interaction about negative prompts they \textit{do not} consider relevant.

\textbf{Scale Selection}: For each query, we compute a scale $s$ to evaluate $F_\text{lang}$. To accomplish this, we generate relevancy maps across a range of scales 0 to 2 meters with 30 increments, and select the scale that yields the highest relevancy score. This scale is used for all pixels in the output relevancy map. We found this heuristic to be robust across a broad range of queries and is used for all the images and videos rendered in this paper.
% This does assume all relevant parts of a scene are the same scale (see Limitations Sec~\ref{sec:limitations})
This assumes relevant parts of a scene are the same scale, see Limitations Sec.~\ref{sec:limitations}.

\textbf{Visibility Filtering}: Regions of the scene that lack sufficient views, such as those in the background or near floaters, may generate noisy embeddings. To address this issue, during querying we discard samples that were observed by fewer than five training views (approximately 5\% of the views in our datasets).
\input{figures/clip_localize}
\subsection{Implementation Details}
We implement \algname{} in Nerfstudio~\cite{tancik2023nerfstudio}, on top of the Nerfacto method. Proposal sampling is the same except we reduce the number of \algname{} samples from 48 to 24 to increase training speed. We use the OpenClip~\cite{cherti2022reproducible} ViT-B/16 model trained on the LAION-2B dataset, with an image pyramid varying from $s_\text{min}=.05$ to $s_\text{min}=.5$ in 7 steps. The hashgrid used for representing language features is much larger than a typical RGB hashgrid: it has 32 layers from a resolution of 16 to 512, with a hash table size of $2^{21}$ and feature dimension of 8. The CLIP MLP used for $F_\text{lang}$ has 3 hidden layers with width 256 before the final 512 dimension CLIP output. The DINO MLP for $F_\text{DINO}$ has 1 hidden layer of dimension 256.

\looseness=-1 We use the Adam optimizer for proposal networks and fields with weight decay $10^{-9}$, with an exponential learning rate scheduler from $10^{-2}$ to $10^{-3}$ over the first 5000 training steps. All models are trained to 30,000 steps (45 minutes), although good results can be obtained in as few as 6,000(8 minutes) as presented in the Appendix. We train on an NVIDIA A100, which takes roughly 20GB of memory total. %Rendering is instant; o
One can interactively query in real-time within the Nerfstudio viewer. The $\lambda$ used in weighting CLIP loss is $0.01$, chosen empirically and ablated in Sec \ref{sec:ablations}. When computing relevancy score, we multiply similarity by $10$ as a temperature parameter within the softmax.