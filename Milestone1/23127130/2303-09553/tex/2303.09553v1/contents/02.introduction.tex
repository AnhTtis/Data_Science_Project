\section{Introduction} 
\label{sec:introduction}

Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf} have emerged as a powerful technique for capturing photorealistic digital representations of intricate real-world 3D scenes. 
However, the immediate output of NeRFs is nothing but a colorful density field, devoid of meaning or context, which inhibits building interfaces for interacting with the resulting 3D scenes. 

Natural language is an intuitive interface for interacting with a 3D scene. Consider the capture of a  kitchen in Figure~\ref{fig:splash_fig}. Imagine being able to navigate this kitchen by asking where the \textit{``utensils''} are, or more specifically for a tool that you could use for \textit{``stirring''}, and even for your favorite mug with a specific logo on it --- all through the comfort and familiarity of everyday conversation.
This requires not only the capacity to handle natural language input queries but also the ability to incorporate semantics at multiple scales and relate to long-tail and abstract concepts.

In this work, we propose Language Embedded Radiance Fields (\algname{}), a novel approach that grounds language within NeRF by optimizing embeddings from an off-the-shelf vision-language model like CLIP into 3D scenes. Notably, \algname{} utilizes CLIP directly without the need for finetuning through datasets like COCO or reliance on mask region proposals, which limits the ability to capture a wide range of semantics. Because \algname{} preserves the integrity of CLIP embeddings at multiple scales, it is able to handle a broad range of language queries, including visual properties (\textit{``yellow''}), abstract concepts (\textit{``electricity''}), text (\textit{``boops''}), and long-tail objects (\textit{``waldo''}) as illustrated in Figure~\ref{fig:splash_fig}. 


We construct a \algname{} by optimizing a language field jointly with NeRF, which takes both position and physical scale as input and outputs a single CLIP vector. During training, the field is supervised using a multi-scale feature pyramid that contains CLIP embeddings generated from image crops of training views. This allows the CLIP encoder to capture different scales of image context, thus associating the same 3D location with distinct language embeddings at different scales (\eg \textit{``utensils"} \vs \textit{``wooden spoon"}). The language field can be queried at arbitrary scales during test time to obtain 3D relevancy maps. To regularize the optimized language field, self-supervised DINO~\cite{dino} features are  also incorporated through a shared bottleneck. 

\algname{} offers an added benefit: since we extract CLIP embeddings from multiple views over multiple scales, the relevancy maps of text queries obtained through our 3D CLIP embedding are more localized compared to those obtained via 2D CLIP embeddings. By definition, they are also 3D consistent, enabling queries directly in the 3D fields without having to render to multiple views.

\algname{} can be trained without significantly slowing down the base NeRF implementation. Upon completion of the training process, \algname{} allows for the generation of 3D relevancy maps for a wide range of language prompts in real-time. We evaluate the capabilities of \algname{} on a set of hand-held captured in-the-wild scenes and find it can localize both fine-grained queries relating to highly specific parts of geometry (\textit{``fingers"}), or abstract queries relating to multiple objects (\textit{``cartoon"}). \algname{} produces view-consistent relevancy maps in 3D across a wide range of queries and scenes, which are best viewed in videos on our website. We also provide quantitative evaluations against popular open-vocab detectors LSeg~\cite{li2022language_lseg} and OWL-ViT~\cite{minderer2022simple_owlvit}, by distilling LSeg features into 3D~\cite{kobayashi2022decomposing} and querying OWL-ViT from rendered novel views. Our results suggest that features in 3D from \algname{} can localize a wide variety of queries across in-the-wild scenes. The zero-shot capabilities of \algname{} leads to potential use cases in robotics, analyzing vision-language models, and interacting with 3D scenes. Code and data will be made available at \url{https://lerf.io}.