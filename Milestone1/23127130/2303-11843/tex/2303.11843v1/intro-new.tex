
\section{Introduction}

Clustering is a fundamental and well-studied problem in computer science,
which arises in approximation algorithms, unsupervised learning, computational
geometry, classification, community detection, image segmentation,
databases, and other areas \cite{hansen1997cluster,schaeffer2007graph,fortunato2010community,shi2000normalized,arthur2006k,tan2013data,coates2012learning}.
The goal of clustering is to find a structure in data by grouping
together similar data points. Clustering algorithms optimize a given
objective function which characterizes the quality of a clustering.
One of the classical and best studied clustering objectives is the
$k$-center objective.

Specifically, given a metric space $(\cX,d)$ and a set of points
$P\subseteq\cX$, the goal of $k$-center clustering is to output
a set $C\subset\cX$ of at most $k$ ``centers'', so that the
maximum distance of any point $p\in P$ to the nearest center $c\in C$
is minimized. In other words, the goal is to minimize the objective
function $\max_{p\in P}d(p,C)$, where $d(p,C)=\min_{c\in C}d(p,c)$. The $k$-center clustering problem
admits several well-known greedy $2$-approximation algorithms \cite{gonzalez1985clustering,hochbaum1986unified}.
However, it is known to be NP-hard to approximate the objective to
within a factor of $(2-\epsilon)$ for any constant $\eps>0$~\cite{hsu1979easy}.
Moreover, even restricted to Euclidean space, it is still NP-hard
to approximate beyond a factor of $1.822$ \cite{feder1988optimal,bern1997approximation}.

While the approximability of many clustering tasks, including $k$-center
clustering, is fairly well understood in the static setting, the same
is not true for \textit{dynamic datasets}. Recently, due to the proliferation
of data and the rise of modern computational paradigms where data
is constantly changing, there has been significant interest in developing
dynamic clustering algorithms~\cite{cohen2016diameter,lattanzi2017consistent,ChaFul18,DBLP:conf/esa/GoranciHL18,schmidt2019fully,goranci2019fully,HenzingerK20,henzinger2020dynamic,fichtenberger2021consistent}.
In the incremental dynamic setting, the dataset $P$ is observed via
a sequence of insertions of data points, and the goal is to maintain
a good $k$-center clustering of the current set of active points.
In the \textit{fully dynamic} setting, points can be both inserted
and deleted from $P$.

The study of dynamic algorithms for $k$-center was initated by Charikar,
Chekuri, Feder, and Motwani~\cite{charikar2004incremental}, whose
``doubling algorithm'' maintains an $8$-approximation in amortized
$O(k)$ update time. However, the doubling algorithm is unable to
handle deletions of data points. It was not until recently that the
first fully dynamic algorithm for $k$-center, with update time better
than naively recomputing a solution from scratch, was developed. In
particular, the work of Chan, Guerqin, and Sozio \cite{ChaFul18}
proposed a randomized algorithm that maintains an optimal $(2+\eps)$-approximation
in $O(\frac{\log\Delta}{\eps}k^{2})$ amortized time per update, where
$\Delta$ is the aspect ratio of the dataset. The algorithm is randomized
against an \emph{oblivious adversary}, i.e., the adversary has to
fix the input sequence in advance and cannot adapt it based on the
decisions of the algorithm. 

Since then, algorithms with improved running time have been demonstrated
for the special cases of Euclidean space \cite{schmidt2019fully}
(albeit, with a larger approximation factor), and for spaces with
bounded doubling dimension \cite{goranci2019fully}. However, despite
this progress, for general metrics the best known dynamic algorithm
for $k$-center is still the algorithm by Chan et al.~\cite{ChaFul18}
and it is open to find a dynamic algorithm with an update time that
is sub-quadratic in $k$ with any non-trivial approximation guarantee. Furthermore, it is open to find a dynamic algorithm for $k$-center
with any non-trivial approximation guarantee and update time against
an \emph{adaptive adversary}, i.e., an adversary that can choose the
next update depending on the previous decisions of the algorithm.
Note that deterministic algorithms are necessarily robust against adaptive adversaries, however, most of the aforementioned algorithms are randomized. This raises the following question.


\begin{center}
\emph{What are the best possible update times and approximation
ratios of\\ dynamic algorithms for $k$-center against oblivious and
adaptive adversaries?}
\end{center}



In addition to $k$-center, there are other popular clustering objectives whose complexity in the fully dynamic model is not fully understood. For example, in
the well-studied $k$-median and $k$-means problems, we minimize
$\sum_{p\in P}d(p,C)$ and $\sum_{p\in P}(d(p,C))^{2}$, respectively.  The latter problems are special cases of the more general \emph{$(k,z)$-clustering problem}, where one minimizes $\sum_{p\in S}d(p,C)^{z}$.  There are $O(1)$-approximation
algorithms known for these problems in the offline setting~\cite{gonzalez1985clustering,hochbaum1985best,hochbaum1986unified,ahmadian2016approximation}
as well as in the dynamic setting~\cite{HenzingerK20}.
However, as is the case for $k$-center, these dynamic algorithms are
randomized, and thus far no deterministic dynamic $O(1)$-approximation algorithms for these objectives
are known.

In addition to $k$-means and median, a natural variant of the $k$-center is the \emph{$k$-sum-of-radii} problem
(also known as the \emph{$k$-cover problem}) where we choose $k$ centers $C = \{c_1,\dots,c_k\}$, and seek to minimize
their sum $\sum_{i}r_{i}$ where $r_i$ is the maximum distance $d(p,c_i)$ over all points point $p$ assigned to $c_i$ (i.e., the radius of the clustered centered at $c_i$). Related to this is the \emph{$k$-sum-of-diameters}
problem, where one minimizes the sum of the diameters
of the clusters.  For each of these closely related variants to $k$-center, no algorithms with non-trivial guarantees were known to exist. 

\subsection{Our Contributions}
Our main contribution is the resolution of the complexity of fully dynamic $k$-center
clustering, up to polylogarithmic factors in the update time, against oblivious adversaries, thereby answering the prior open question, as well as the resolution of the problem against adaptive adversaries for the regime when $k = O(\sqrt{\log n/ \log \log (n+\Delta)})$. A summary of our upper and lower bounds for fully dynamic $k$-center is given in Table~\ref{tab:k-center-main}.

{
\def\arraystretch{1.1}
\begin{table}
\centering
\begin{tabular}{ccccc}
\toprule
$k$-center & \multicolumn{2}{c}{Upper bound} & \multicolumn{2}{c}{Lower bound}\tabularnewline \cmidrule(r){2-3} \cmidrule(l){4-5}
Adversary & update time & approx. ratio & update time & approx. ratio\tabularnewline \midrule
Oblivious & \textbf{\textcolor{black}{$\bm{\tilde{O}(k/\eps)}$}} & \textbf{\textcolor{black}{$\bm{2+\eps}$}} & \textbf{\textcolor{black}{$\bm{\Omega(k)}$}} & \bf{any}\tabularnewline
 & $\tilde{O}(k^{2}/\eps)$ & $2+\eps$~\cite{ChaFul18} &  & \tabularnewline
Adaptive & $\bm{\tilde{O}(k)}$ & $\bm{O\left(\min\{\frac{\log(n/k)}{\log \log n},k\}\right)}$ & $\bm{f(k,n)}$ & $\bm{\Omega\left(\min\{\frac{\log(n)}{k \log f(k,2n)},k\}\right)}$\tabularnewline
\bottomrule
\end{tabular}%
\def\arraystretch{1}


\caption{\label{tab:k-center-main}Our main new upper and lower bounds for $k$-center
where $f(k,n)$ is an arbitrary function. The notation $\tilde{O}$ hides factors that are polylogarithmic in $n,\Delta$.
}
\end{table}
}

\paragraph{Oblivous adversaries.}
We first design an algorithm with update time that is \textit{linear} in $k$, 
and only logarithmic in $n$ and $\Delta$, where $n$ is an upper
bound on the number of active points at a given point in time,\footnote{We remark that our algorithm does not need to know $n$ or the total number of updates in advance.} and
$\Delta$ denotes the aspect ratio of the given metric space. Our algorithm achieves an
approximation ratio of $2+\epsilon$, which is essentially tight since
it is NP-hard to obtain an approximation ratio of $2-\epsilon$. This improves on the prior best known quadratic-in-$k$ update time of \cite{ChaFul18}, for the same approximation ratio. 
\begin{theorem}
\label{thm:main} There is a randomized fully dynamic algorithm that,
on a sequence of insertions and deletions of points from an arbitrary
metric space, maintains a $(2+\eps)$-approximation to the optimal
$k$-center clustering. The amortized update time of the algorithm
is $O(\frac{\log\Delta\log n}{\eps}(k+\log n))$ in expectation, and
$O(\frac{\log\Delta\log n}{\eps}(k+\log n)\log\delta^{-1})$ with
probability $1-\delta$ for any $\delta\in(0,\frac{1}{2})$. 
\end{theorem}
 
We demonstrate that our update time is tight up to logarithmic factors,
even in the insertion only setting. In fact, our lower bound extends
immediately to all the other clustering problems defined above, see
Table~\ref{tab:other-LB} for a further overview of our lower bounds beyond those given in Table~\ref{tab:k-center-main}.
\begin{theorem}
\label{thm:LB}If a dynamic algorithm for $k$-center, $k$-median,
$k$-means, $(k,z)$-clustering for any $z>0$, $k$-sum-of-radii,
or $k$-sum-of-diameter has an approximation ratio that is bounded
by any function in $k$ and $n$, then it has an update time of $\Omega(k)$.
This holds already if points can only be inserted but not deleted.
\end{theorem}
We remark that our lower bound is unconditional, i.e., it does not depend
on any complexity theoretic assumptions. In fact, we prove an even
stronger statement: any \textit{offline} algorithm must 
query the distances of $\Omega(nk)$ pairs of points in order to guarantee
any bounded approximation ratio with constant probability (see \cref{sec:LB}). 
The amortized per-update time
of $\Omega(k)$ for dynamic insertion-only algorithms then follows immediately. 

{
\def\arraystretch{1.2}
\begin{table}
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Oblivious adversary} & \multicolumn{2}{c}{Adaptive adversary}\tabularnewline \cmidrule(r){2-3} \cmidrule(l){4-5}
\multicolumn{1}{c}{} & update time & approx. ratio & update time & approx. ratio\tabularnewline
\midrule
$k$-median & \textbf{\textcolor{black}{$\bm{\Omega(k)}$}} & \bf{any} & $\bm{f(k,n)}$ & $\bm{\Omega\left(\frac{\log(n)}{\log f(1,2n)}\right)}$\tabularnewline
 & $\tilde{O}(k^{2}/\eps^{O(1)})$ & $5.3+\eps$~\cite{HenzingerK20} &  & \tabularnewline
$k$-means & \textbf{\textcolor{black}{$\bm{\Omega(k)}$}} & \bf{any} & $\bm{f(k,n)}$ & $\bm{\Omega\left(\left(\frac{\log(n)}{\log f(1,2n)}\right)^{2}\right)}$\tabularnewline
 & $\tilde{O}(k^{2}/\eps^{O(1)})$ & $36+\eps$~\cite{HenzingerK20} &  & \tabularnewline
$(k,z)$-clustering & \textbf{\textcolor{black}{$\bm{\Omega(k)}$}} & \bf{any} & $\bm{f(k,n)}$ & \textbf{\textcolor{black}{$\bm{\Omega\left(\left(\frac{\log(n)}{z+\log f(1,2n)}\right)^{z}\right)}$}}\tabularnewline
$k$-sum-of-radii & \textbf{\textcolor{black}{$\bm{\Omega(k)}$}} & \bf{any} & $\bm{f(k,n)}$ & $\bm{\Omega\left(\frac{\log(n)}{\log f(1,2n)}\right)}^\star$\tabularnewline
 & \textcolor{black}{$\bm{k^{O(1)}\log\Delta}\Delta$} & $\bm{O(1)}$ &  & \tabularnewline
$k$-sum-of-diam. & \textbf{\textcolor{black}{$\bm{\Omega(k)}$}} & \bf{any} & $\bm{f(k,n)}$ & $\bm{\Omega\left(\frac{\log(n)}{\log f(1,2n)}\right)}^\star$\tabularnewline
 & \textcolor{black}{$\bm{k^{O(1)}\log \Delta}\Delta$} & $\bm{O(1)}$ &  & \tabularnewline
\bottomrule
\end{tabular}

\caption{\label{tab:other-LB}Our lower bounds for $k$-median, $k$-means,
$k$-sum-of-radii, and $k$-sum-of-diameter and our upper bounds for
the latter two problems, where $f(k,n)$ is an arbitrary function. For the ratios marked with $\star$, we assume that the algorithm also outputs an estimate of the cost of an optimal solution.}
\end{table}
\def\arraystretch{1}
}

\paragraph{Deterministic algorithms and adaptive adversaries.}
For arbitrary metrics, our algorithms access the metric via queries to a distance oracle returning $d(x,y)$ between any two points which were inserted (and possibly deleted) up to that point. In the oblivious model, both the underlying metric and point insertions and deletions are fixed in advance. However, in the adaptive adversary model, the adversary can adaptively choose the values of $d(x,y)$ based on past outputs of the algorithm. Thus, the values of $d(x,y)$ are fixed only as they are queried (subject to obeying the triangle inequality). We refer to this as the \textit{metric-adaptive} model (see discussion below).
 
Given our bounds against oblivious adversaries, it is natural to ask whether constant factor approximations for $k$-center are also achievable in $\tilde{O}(k)$, or even  $\tilde{O}(\mathrm{poly}(k))$, time against metric-adaptive adversaries.
Perhaps surprisingly, we show that any $k$-center algorithm with update time poly-logarithmic in $n + \Delta$ (and arbitrary dependency on $k$) must incur an essentially logarithmic approximation factor if it needs to report an estimation of the cost. Moreover, for a large range of $k$, such a runtime is not possible even if the algorithm is tasked only with returning an approximately optimal set of $k$-centers. 
\begin{theorem}[{restate=[name=]thmDetLB}]
\label{thm:det-1lb} For any $k  \geq 1$, any dynamic algorithm which returns a set of $k$-centers against a metric-adaptive adversary
with an amortized update time of $f(k,n)$, for
an arbitrary function $f$,  must have an approximation ratio of $\Omega\left(\min\{ \frac{\log(n)}{k \log f(k,2n)}, k\}\right)$ for the $k$-center problem. In addition, even for the case of $k=1$, we show that \emph{any} algorithm
with an update time of $f(k,n)$

\begin{itemize}
\item for $1$-median
has an approximation ratio of $\Omega\left(\frac{\log(n)}{\log f(1,2n)}\right)$, 
\item for $1$-means has an approximation ratio of $\Omega\left(\left(\frac{\log(n)}{\log f(1,2n)}\right)^{2}\right)$, 
\item for $(1,z)$-clustering has an approximation ratio of $\Omega\left(\left(\frac{\log(n)}{z+\log f(1,2n)}\right)^{z}\right)$,
\item for $1$-center, $1$-sum-of-radii or $1$-sum-of-diameters has an approximation ratio of
$\Omega\left(\frac{\log(n)}{\log f(1,2n)}\right)$ if the algorithm is also able to estimate the cost of the optimal clustering.
\end{itemize}
\end{theorem}
In particular, Theorem \ref{thm:det-1lb} demonstrates that for any $k$ satisfying $\omega(1) \leq k \leq o(\frac{\log n}{\log \log (n+\Delta)})$, there is no constant-factor approximation algorithm for $k$-center running in $\polylog(n,\Delta)$ time which is correct against an adaptive adversary. 
In fact, we prove even more fine-grained trade-offs between the update
time of an algorithm and the possible approximation ratio (see \Cref{sec:LBadap} for details).

\paragraph{Separation between Adversarial Models.} An important consequence of Theorem \ref{thm:det-1lb} is a separation in the complexity of $k$-clustering tasks between two distinct types of adaptive adversaries. Specifically, Theorem \ref{thm:det-1lb} is a lower bound for adversaries which decide on the answers to distance queries on the fly, without having fixed the metric beforehand, subject to the constraint that the answers are always consistent with some metric. In other words, the metric itself, in addition to the points which are inserted and deleted, are adaptively chosen (i.e., they are metric-adaptive adversaries). This is as opposed to the setting where the metric is fixed in advance, and only the insertions and deletions of points from that space are adaptively chosen (call these \textit{point-adaptive} adversaries). 

 
 To illustrate the difference, suppose the current active point set is $P$. When a point-adaptive adversary inserts a new point $q$, since the metric is fixed, it must irrevocably decide on the value of $d(p,q)$ for all $p \in P$. On the other hand, a metric-adaptive adversary can defer fixing these value until it is queried for them. For example, suppose $P=\{a,b\}$ and then a point $c$ is inserted, after which the algorithm queries the adversary for the value of $d(a,c)$, and then makes a (possibly randomized) decision $\cD$ based on $d(a,c)$ (e.g. $\cD$ could be whether to make $c$ a center). Then a metric-adaptive adversary can adaptively decide on the value of $d(b,c)$ based on the algorithm's decision $\cD$ (subject to not violating the triangle inequality), whereas a point-adaptive algorithm must have fixed $d(b,c)$ independent of $\cD$.
 
This distinction is nuanced, but has non-trivial consequences. 
 Namely, while Theorem \ref{thm:det-1lb} rules out fully dynamic $O(1)$-approximation algorithms with $\polylog(n,\Delta)$ update time for $k$-means and $k$-medians for metric-adaptive adversaries, in \cite{HenzingerK20} the authors design such algorithms against point-adaptive adversaries. Thus, Theorem \ref{thm:det-1lb} demonstrates that the algorithms of \cite{HenzingerK20} would not have been possible against metric-adaptive adversaries.
To the best of our knowledge, this is the first separation between such adversarial models for dynamic clustering. 




 



Now while the \textit{randomized} algorithms of \cite{HenzingerK20} apply only to the weaker point-adaptive adversaries, note that deterministic algorithms are necessarily correct even against the stronger metric-adaptive adversaries. We show that such a deterministic algorithm in fact exists, with complexity matching the lower bound of \Cref{thm:det-1lb} for $k = O(\sqrt{\log n / \log \log n})$.
\begin{theorem}
\label{thm:mpt-det-upper} Fix any $B \geq  2$ and $\eps  \in (0,1)$. Then there is a deterministic dynamic algorithm
for $k$-center with an amortized update time of $O\left(\frac{kB \log n\log\Delta }{\eps}\right)$
and an approximation factor of $(4+\eps)\min\left\{\frac{\log(n/k)}{\log B},k\right\}$. 
 Furthermore, the worst-cast insertion time is $O\left(\frac{k B \log n\log\Delta}{\eps}\right)$, and the worst-case deletion time is $O\left(\frac{k^2 B \log n\log\Delta}{\eps}\right)$.
\end{theorem}
In particular, by setting $B = \log n$, we obtain a fully dynamic and deterministic algorithm for $k$-centers with an approximation 
ratio of $O\left(\min\left\{\frac{\log(n/k)}{\log \log n},k\right\}\right)$, which, by \cref{thm:det-1lb}, is optimal for the regime when $k = O(\sqrt{\frac{\log n }{\log \log n}})$
among algorithms that are robust against metric-adaptive adversaries and whose
update time is polylogarithmic in $n$
and~$\Delta$. Moreover, that by setting $B = n^{\eps}$ for any $\eps  \in (0,1)$ we obtain a $O(1/\eps)$-approximation in time $\tilde{O}(k n^\eps )$. Thus, \cref{thm:mpt-det-upper} gives a deterministic $O(1)$-approximate algorithm for fully dynamic $k$-centers running in time $\tilde{O}(k n^\eps )$ for any constant $\eps > 0$. 

\paragraph{Improved Fully Dynamic $k$-center via Locally Sensitive Hashing.}

The lower bound of Theorem \ref{thm:LB} demonstrates that, even against an oblivious adversary, one cannot beat $\Omega(k)$ amortized update time for fully dynamic
$k$-center. However,
in \cite{schmidt2019fully,goranci2019fully} it was shown that for
the case of Euclidean space or metrics with bounded doubling dimension,
update times \textit{sublinear }in $k$ are possible. These results rely on
nearest neighbor data structures with running times that are sublinear
in $k$, which are designed specifically for the respective metric
spaces, along with specialized clustering algorithms to employ them.
Note that for the case of Euclidean space, the resulting approximation
factors were still logarithmic. 

We significantly generalize and strengthen the above results, by demonstrating that \textit{any} metric space admitting sublinear
time nearest neighbor search data structures also admits fully dynamic
$k$-center algorithms whose update time is sublinear in~$k$.
We do this via a black-box reduction, showing that we can obtain a fully dynamic algorithm for $k$-center for any metric space, given a \textit{locally sensitive hash function}
(LSH) for that space.

Informally, an LSH for a space $(\cX,d)$ is a hash function $h$ mapping $\cX$ to some number of hash buckets, such that closer points in the metric are \textit{more} likely to collide than far points. Thus, after hashing a subset $S \subset \cX$ into the hash table, given a query point $q \in \cX$, to find the closest points to $q$ in $S$, it (roughly) suffices to search only through the hash bucket $h(q)$. The ``quality'' of an LSH family $\cH$ is parameterized by four values $(r,cr,p_1,p_2)$, meaning that for any $x,y \in \cX$:
 \begin{itemize}
 	\item If $d(x,y) \leq r$, then $\prb{h \sim \cH}{h(x) = h(y)} \geq p_1$. 
 	\item If $d(x,y) > cr$, then $\prb{h \sim \cH}{h(x) = h(y)} \leq p_2$. 
 \end{itemize}
Given the above definition, we can now informally state our main result for LSH-spaces (see Section \ref{sec:LSH} for formal statements). 

\renewcommand{\arraystretch}{1.3}
\begin{table}[t]
	\centering
	\begin{tabular}{ccccc}
		\toprule		
		Metric space & \textbf{Our approx.} &\textbf{Our runtime} & Prior approx. &  Prior runtime \\ \midrule

		Arbitrary metric space& $\bm{2+\eps} $& $\bm{\tilde{O}(  k)} $& $2+\eps$ &  $ \tilde{O}( k^2) $ \cite{ChaFul18}\\ %
		$(\R^d,\ell_p)$, $p \in [1,2]$ & $\bm{c (4+\eps) }$ &$\bm{\tilde{O}(  n^{1/c})} $& $O(c \cdot \log n)$ &  $\tilde{O}( n^{1/c}) $ \cite{schmidt2019fully}\\ %
		Eucledian space $(\R^d,\ell_2)$  & $\bm{c (\sqrt{8}+\eps)  }$ & $\bm{\tilde{O}(  n^{1/c^2 + o(1)}) }$& $O(c \cdot \log n)$ & $ \tilde{O}( n^{1/c}) $ \cite{schmidt2019fully}\\ %
		Hamming metric &  $\bm{c (4+\eps)} $ &\bm{$\tilde{O}(  n^{1/c})} $& -- &  -- \\ %
		Jaccard metric &  $\bm{c (4+\eps)} $ &\bm{$\tilde{O}(  n^{1/c})} $&  -- &  -- \\ %
		\begin{tabular}{c}
			EMD over $[D]^d$   \\
			$d= O(1)$
		\end{tabular} &  $\bm{O(c \cdot \log D)}  $ &$\bm{\tilde{O}(  n^{1/c})} $& -- &  -- \\ %
		\begin{tabular}{c}
			EMD over $[D]^d$   \\
			sparsity $s$
		\end{tabular} &  $\bm{O(c \cdot \log s n \log d )} $ &$\bm{\tilde{O}(  n^{1/c})} $& -- &  -- \\ %
		\bottomrule
	\end{tabular}
\caption{Our upper bounds for Fully Dynamic $k$-Centers against oblivious adversaries in different metric spaces. Results for specific metric spaces are obtained by applying known LSH families with Theorem \ref{thm:lshinformal}. } \label{table:LSHResults}
\end{table}


\begin{theorem}[informal]\label{thm:lshinformal}
 Let $(\cX,d)$ be a metric space that admits an LSH $\cH$ with
parameters $(r,cr,p_{1},p_{2})$ for every $r \geq 0$, and a running time of $\Run(\cH)$.
Then there is a fully dynamic algorithm for $k$-center on $(\cX,d)$
with an approximation ratio of $c(2+\eps)$ and an update time of
$O\left(\frac{\log\Delta}{\eps p_{1}}n^{2\rho}\cdot\Run(\cH)\right)$, where $\rho = \ln p_1^{-1}/ \ln p_2^{-1}$.
\end{theorem}
Using known LSH hash functions from the literature, Theorem \ref{thm:lshinformal} immediately yields improved state of the art algorithms for Euclidean spaces,
the Hamming metric, the Jaccard Metric, and the Earth Mover Distance
(EMD). In particular, our bounds significantly improve the prior best known results of \cite{schmidt2019fully} for Euclidean space, by at least a logarithmic factor in the approximation \aw{ratio}. 
See Table \ref{table:LSHResults} for an summary of these results, and see Section \ref{sec:LSHcorollaries} for the formal theorem statements for each metric space. 


\paragraph{$k$-sum-of-radii and $k$-sum-of-diameter. }

Finally, we study the $k$-sum-of-radii and $k$-sum-of-diameter problems, for which there were previously no fully dynamic $O(1)$-approximation algorithms known with non-trivial update time. We design the first such algorithms, which hold against oblivious adversaries. Note that, as a consequence of Theorem \ref{thm:det-1lb}, such a constant-factor approximation is only possible against an oblivious adversary.

\begin{theorem}[{restate=[name=]thmPdMain}]
\label{thm:pd-main}
There are randomized dynamic algorithms for the
$k$-sum-of-radii and the $k$-sum-of-diameters problems with update
time $k^{O(1/\epsilon)}\log\Delta$ and with approximation ratios
of $13.008+\epsilon$ and $26.016+\epsilon$, respectively, against
an oblivious adversary. 
\end{theorem}
Thus, we complete the picture that all clustering problems defined above admit dynamic $O(1)$-approximation
algorithms against an oblivious adversary, but only (poly-)logarithmic
approximation ratios against an adaptive adversary.
