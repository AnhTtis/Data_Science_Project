\subsection{Technical Overview}\label{sec:tech}
We now describe the main technical steps employed in the primary results of the paper. 
For the remainder of the section, we fix a metric space $(\cX,d)$. Let $r_{\min}$ and $r_{\max}$ be values such that
$r_{\min} \leq d(x,y) \leq r_{\max}$ for  any $x,y \in \cX$. We set $\Delta = r_{\max}/r_{\min}$ as an upper bound on the aspect ratio of $(\cX,d)$. For any fixed point in time, we denote the current input points of a dynamic clustering algorithm by $P$, and write $n$ to denote the maximum size of $P$ during the dynamic stream.

\subsubsection{Algorithm for General Metric Spaces} Our starting point is the well-known reduction of Hochbaum and Shmoys \cite{hochbaum1986unified} from approximating $k$-center to computing a maximal independent set (MIS) in a collection of \textit{threshold} graphs. Formally, given a real $r>0$, the $r$-threshold graph of a point set $P$ is the graph $G_r = (V,E_r)$ with vertex set $V = P$, and where  $(x,y) \in E_r$ is an edge if and only if $d(x,y) \leq r$. One computes an MIS $\cI_r$ in the graph $G_r$ for each $r = (1+\eps)^i r_{\min}$ with $i=0,1,\dots,\lceil \log_{1+\eps} \Delta \rceil$. If $|\cI_r| \leq k$, then $\cI_r$ is a $k$-center solution of cost at most $r$. If $|\cI_r| > k+1$, then there are $k+1$ points whose pair-wise distance is at least $r$. Therefore, by the triangle inequality, the optimal cost is at least $r/2$. These facts together yield a $(2+\eps)$-approximation.


By the above, it suffices to maintain an MIS in $O(\eps^{-1} \log \Delta)$ threshold graphs. Now the problem of maintaining an MIS in a fully dynamic sequence of \textit{edge} insertions and deletions to a graph is very well studied \cite{assadi2019fully,gupta2018simple, onak2018fully,du2018improved, censor2016optimal, chechik2019fully, behnezhad2019fully}. Notably, this line of work has culminated with the algorithms of \cite{chechik2019fully,behnezhad2019fully}, which maintain an MIS in expected $\polylog n$ update time per edge insertion or deletion. Unfortunately, point insertions and deletions from a metric space correspond to \textit{vertex} insertions and deletions in a threshold graph. Since a single vertex update can change up to $O(n)$ edges in the graph at once, one cannot simply apply the prior algorithms for fully dynamic edge updates. Moreover, notice that in this vertex-update model, we are only given access to the graph via queries to the adjacency matrix. Thus, even finding a single neighbor of $v$ can be expensive. 


On the other hand, observe that in the above reduction to MIS, one does not always need to compute the entire MIS; for a given threshold graph $G_r$, the algorithm can stop as soon as it obtains an independent set of size at least $k+1$. This motivates the following problem, which is to return either
an MIS of size at most $k$, or an independent set of size at least $k+1$. We refer to this as the $k$-Bounded MIS problem. Notice that given an MIS $\cI$ of size at most $k$ in a graph $G$, and given a new vertex $v$, if $v$ is not adjacent to any $u \in \cI$, then $\cI \cup \{v\}$ is an MIS, otherwise $\cI$ is still maximal. Thus, while an insertion of a vertex $v$ can add $\Omega(n)$ edges to $G$, for the $k$-Bounded MIS problem, one only needs to check the $O(k)$ potential edges between $v$ and $\cI$ to determine if $\cI$ is still maximal. Thus, our goal will be to design a fully dynamic algorithm for $k$-Bounded MIS with $\tilde{O}(k)$ amortized update time in the vertex-update model. 


\paragraph{The Algorithm for $k$-Bounded MIS.}
To accomplish the above goal, we will adapt several of the technical tools employed by the algorithms for fully dynamic MIS in the edge-update model. Specifically, one of the main insights of this line of work is to maintain the \textit{Lexicographically First Maximal Independent Set} (LFMIS) with respect to a random permutation $\pi: V \to [0,1]$ of the vertices.\footnote{LFMIS with respects to random orderings were considered in \cite{censor2016optimal,assadi2019fully,chechik2019fully,behnezhad2019fully}.} The LFMIS is a natural object obtained by greedily adding the vertex with smallest $\pi(v)$ to the MIS, removing it and all its neighbors, and continuing iteratively until no vertices remain. Maintaining an LFMIS under a random ranking has several advantages from the perspective of dynamic algorithms. Firstly, it is \textit{history-independent}, namely, once $\pi$ is fixed, the current LFMIS depends only on the current graph, and not the order of insertions and deletions which led to that graph. Secondly, given a new vertex $v$, the probability that adding $v$ to the graph causes a large number of changes to be made to the LFMIS is small, since $\pi(v)$ must have been similarly small for this to occur.


Given the above advantages of an LFMIS, we will attempt to maintain the set $\LFMIS_{k+1}$ consisting of the first $\min\{k+1,|\LFMIS|\}$ vertices in the overall LFMIS with respect to a random ranking $\pi$; we refer to $\LFMIS_{k+1}$ as the top-$k$ LFMIS (see Definition \ref{def:topkLFMIS}). Notice that maintaining this set is sufficient to solve the $k$-Bounded MIS problem. The challenge in maintaining the set $\LFMIS_{k+1}$ will be to handle the ``excess'' vertices which are contained in the $\LFMIS$ but are not in $\LFMIS_{k+1}$, so that their membership in $\LFMIS_{k+1}$ can later be quickly determined when vertices with smaller rank in $\LFMIS_{k+1}$ are removed.
To handle these excess vertices, we make judicious use of a priority queue $\cQ$, with vertex priorities given by the ranking $\pi$. 


Now the key difficulty in dynamically maintaining an MIS is that when a vertex $v$ in an MIS is deleted, potentially all of the neighbors of $v$ may need to be added to the MIS, resulting in a large update time. Firstly, in order to keep track of which vertices could possibly enter the LFMIS when a vertex is removed from it, we maintain a mapping $\ell:V \to \LFMIS$, such that for each $u \notin \LFMIS$, we have $\ell(u) \in \LFMIS$ and $(u,\ell(u))$ is an edge. The ``leader'' $\ell(u)$ of $u$ serves as a certificate that $u$ cannot be added to the MIS. When a vertex $v \in \LFMIS$ is removed from the LFMIS, we only need to search through the set $\cF_v = \{u \in V \;| \; \ell(u) = v\}$ to see which vertices should be added to the LFMIS. Note that this can occur when $v$ is deleted, or when a neighbor of $v$ with smaller rank is added to the LFMIS. Consequentially, the update time of the algorithm is a function of the number points $u$ whose leader $\ell(u)$ changes on that step. For each such $u$, we can check in $O(k)$ time if it should be added to $\LFMIS_{k+1}$ by querying the edges between $u$ and the vertices in $\LFMIS_{k+1}$. By a careful amortized analysis, we can prove that the total runtime of this algorithm is indeed at most an $O(k)$ factor larger than the total number of leader changes. This leaves the primary challenge of designing and maintaining a leader mapping which changes infrequently. 


A natural choice for such a leader function is to set $\ell(u)$ to be the \textit{eliminator} of $v$ in the LFMIS. Here, for any vertex $u$ not in the LFMIS, the eliminator $\elim_\pi(u)$ of $u$ is defined to be its neighbor with lowest rank  that belongs to the LFMIS. 
The eliminators have the desirable property that they are also history-independent, and therefore the number of changes to the eliminators on a given update depends only on the current graph and the update being made. An important key result of \cite{behnezhad2019fully} is that the expected number of changes to the eliminators of the graph, even after the insertion or removal of an entire vertex, is at most $O(\log n)$. Therefore, if we could maintain the mapping $\ell(v) = \elim_{\pi}(v)$ by keeping track of the eliminators, our task would be complete.


Unfortunately, keeping track of the eliminators will not be possible in the vertex-update model, since we can only query a small fraction of the adjacency matrix after each update. In particular, when a vertex $v$ is inserted, it may change the eliminators of many of its neighbors, but we cannot afford to query all $\Omega(n)$ potential neighbors of $v$ to check which eliminators have changed. Instead, our solution is to maintain a leader mapping $\ell(v)$ which is an ``out-of-date'' version of the eliminator mapping. Each time we check if a vertex $v$ can be added to $\LFMIS_{k+1}$, by searching through its neighbors in $\LFMIS_{k+1}$, we ensure that either $v$ is added to $\LFMIS_{k+1}$ or its leader $\ell(v)$ is updated to the current eliminator of $v$, thereby aligning $\ell(v)$ with $\elim_\pi(v)$. However, thereafter, 
the values of $\ell(v)$ and $\elim_{\pi}(v)$ can become misaligned in several circumstances. In particular, the vertex $v$ may be moved into the queue $\cQ$ due to its leader $\ell(v)$ either leaving the LFMIS, or being pushed out of the top $k+1$ vertices in the LFMIS. In the second case, we show that $v$ can follow its leader to $\cQ$ without changing $\ell(v)$, however, in the first case $\ell(v)$ is necessarily modified. On the other hand, as noted, the eliminator of $v$ can also later change without the algorithm having to change $\ell(v)$.
Our analysis proceeds by a careful accounting, in which we demonstrate that each change in an eliminator can result in at most a constant number of changes to the leaders $\ell$, from which an amortized bound of $O(\log n)$ leader changes follows. %

\paragraph{Comparison to the prior $k$-center algorithm of \cite{ChaFul18}.}
The prior fully dynamic $k$-center algorithm of Chan, Gourqin, and Sozio \cite{ChaFul18}, which obtained an amortized $O(\eps^{-1}\log \Delta \cdot k^2)$ update time, also partially employed the idea of maintaining an LFMIS (although the connection to MIS under lexicographical orderings was not made explicit in that work). However, instead of consistently maintaining the LFMIS with respect to a random ranking $\pi$, they begin by maintaining an LFMIS with respect to the ordering $\pi'$ in which the points were originally inserted into the stream. Since this ordering is adversarial, deletions in the stream can initially be very expensive to handle. To prevent bad deletions from repeatedly occurring, whenever a deletion to a center $c$ occurs, the algorithm of \cite{ChaFul18} randomly reorders all points which are contained in clusters that come after $c$ in the current ordering being used. %
In this way, the algorithm of \cite{ChaFul18} gradually converts the adversarial ordering $\pi'$ into a random ordering $\pi$. However, by reordering \textit{all} points which occurred after a deleted center $c$, instead of just the set of points which were led by that center (via a mapping $\ell$), the amortized update time of the algorithm becomes $O(k^2)$.\footnote{Consider the stream which inserts $k$ clusters of equal size $n/k$, and then begins randomly deleting half of each cluster in reverse order. By the time a constant fraction of all the points are deleted, for each deletion the probability a leader is deleted is $\Omega(k/n)$, but such a deletion causes $O(nk)$ work to be done by the algorithm.} 
In contrast, one of our key insights is to update the entire clustering to immediately reflect a random LFMIS ordering after each update. 

\subsubsection{Algorithm for LSH Spaces}
The extension of our algorithm to LSH spaces is based on the following observation: each time we attempt to add a vertex $v$ to $\LFMIS_{k+1}$, we can determine the fate of $v$ solely by finding the vertex $u \in \LFMIS_{k+1}$ in the neighborhood of $v$ of minimal rank (i.e., the eliminator of $v$, if it is contained in $\LFMIS_{k+1}$). If $\pi(u) < \pi(v)$, we simply set $\ell(v) = u$ and proceed. Otherwise, we must find all other neighbors $w$ of $v$ in $\LFMIS_{k+1}$, remove them from the LFMIS, and set $\ell(w) = u$. Finding the vertex $u$ can therefore be cast as an $r$-\textit{near neighbor search} problem: here, one wants to return any $u \in \LFMIS_{k+1}$ which is at distance at most $r$ from $u$, with the caveat that we need to return such vertices in order based on their ranking. 
Since, whenever $u$ enters the LFMIS, each point $w$ that we search through which leaves $\LFMIS_{k+1}$ had its leader change, if we can find each consecutive neighbor of $u$ in $\LFMIS_{k+1}$ in time $\alpha$, we could hope to bound the total runtime of the algorithm by an $O(\alpha)$ factor more than the total number of leader changes, which we know to be small by analysis of the general metric space algorithm. 

To achieve values of $\alpha$ which are sublinear in $k$, we must necessarily settle for an \textit{approximate near neighbor search} (ANN) algorithm. A randomized, approximate $(r,cr)$-nearest neighbor data structure will return any point in $\LFMIS_{k+1}$ which is at distance at most $cr$, assuming there is at least one point at distance at most $r$ in $\LFMIS_{k+1}$. In other words, such an algorithm can be used to find all edges in $G_r$, with the addition of any arbitrary subset of edges in $G_{cr}$. By relaxing the notion of a threshold graph to allow for such a $c$-approximation, one can hope to obtain a $c(2+\eps)$-approximation to $k$-center via solving the $k$-Bounded MIS problem on each relaxed threshold graph. 

The key issue above is that, when using an ANN data structure, the underlying relaxed threshold graph is no longer a deterministic function of the point set $P$, and is instead ``revealed'' as queries are made to the ANN data structure. We handle this issue by demonstrating that, for the class of ANN algorithms based on locally sensitive hash functions, one can define a graph $G$ which is only a function of the randomness in the ANN data structure, and not the ordering $\pi$. The edges of this graph are defined in a natural way --- two points are adjacent if they collide in at least one of the hash buckets. By an appropriate setting of parameters, the number of collisions between points at distance larger than $cr$ can be made small. By simply ignoring such erroneous edges as they are queried, the runtime increases by a factor of the number of such collisions. %

\subsubsection{Lower bounds against an oblivious adversary}
\awr{commented out description of $\Omega(k^2)$ bound which is no longer mentioned above}
\aw{For proving the lower bound of $\Omega(k)$ we use the following hard distribution:}
in one case we randomly plant $k$ clusters each of size roughly $n/k$, where points within a cluster are close and points in separate clusters are far. In the second case, we do the same, and subsequently choose a point $i \sim [n]$ randomly and move it very far from all points (including its own cluster). Adaptive algorithms can gradually winnow the set of possible locations for $i$ by discovering connected components in the clusters, and eliminating the points in those components. Our proof follows by demonstrating that a large fraction of the input distribution results in any randomized algorithm making a sequence of distance queries which eliminates few data points, and therefore gives only a small advantage in discovering the planted point $i$.


\subsubsection{Lower bounds against an adaptive adversary}

\aw{We sketch our main ideas behind the lower bounds against an adaptive adversary.
For ease of presentation, }we discuss here a simplified setting in which the algorithm \begin{inenum}
\item can query only the distances between points that are currently
in $P$ (i.e., that have been introduced already but not yet removed)
and
\item has a worst-case update time of $f(k,n)$ (rather than amortized
update time) for some function $f$\end{inenum}. Note that these assumptions are not needed for the full proof presented in \Cref{sec:LBadap}. \aw{Our goal is to prove a lower bound on the 
approximation ratio of such an algorithm.}

The adversary starts by adding points into $P$ and maintains an auxiliary
graph $G=(V,E)$ with one vertex $v_{p}$ for each point $p$. Whenever
the algorithm queries the distance between two points $p,p'\in P$,
the adversary reports that they have a distance of 1 and adds an edge
$\{v_{p},v_{p'}\}$ of length 1 to $G$. Intuitively, the adversary
uses $G$ to keep track of the previously reported distances.
Whenever there is a vertex $v_{p}$ with a degree of at least $100f(k,n)$,
in the next operation the adversary deletes the corresponding point
$p$. There could be several such vertices, and then the adversary
deletes them one after the other. Thanks to assumption (i), once a point $p$ is deleted, the
degree of $v_{p}$ cannot increase further. Hence, the degrees of
the vertices in $G$ cannot grow arbitrarily. More precisely, one
can show that the degree of each vertex can grow to at most $O(\log n\cdot f(k,n))$.
 In particular, at least half of the
vertices in $G$ are at distance at least $\Omega(\log_{O(\log n\cdot f(k,n))}n)=\Omega\left(\log n\;/\;[\,\log\log n\cdot\log(f(k,n))\,]\right)$
to $v_{p}$.


Now observe that the algorithm knows only the point distances that
it queried, i.e., those that correspond to edges in $G$. For all other distances,
the triangle inequality imposes only an upper bound for any pairs of points whose corresponding
vertices are connected in $G$. Thus,
the algorithm cannot distinguish the setting where the underlying metric is
the shortest path metric in $G$ from the setting where all points
are at distance 1 to each other. If $k=1$, for any of the problems
under consideration, then for the selected center $c$ the algorithm
cannot distinguish whether all points are at distance 1 to $c$ or
if half of the points in $P$ are at distance $\Omega\left(\log n\;/\;[\,\log\log n\cdot\log(f(1,n))\,]\right)$
to $c$. Therefore, any algorithm which is correct with constant probability will suffer an approximation of at least $\Omega\left(\log n\;/\;[\,\log\log n\cdot\log(f(1,n))\,]\right)$.



We improve the above construction so that it also works for algorithms
that have amortized update times, are allowed to query
distances to points that are already deleted, and may output $O(k)$ instead of $k$ centers. 
To this end, we adjust
the construction so that for points with degree of at least $100f(k,n)$, where $f(k,n)$ is the (amortized) number of distance queries per operation,
we report distances that might be larger than~1 and that still allow
us to use the same argumentation as above. At the same time, we remove
the factor of $\log\log n$ in the denominator.




\subsubsection{Algorithms for $k$-Sum-of-radii and $k$-Sum-of-diameters}

Our algorithms for $k$-sum-of-radii and
$k$-sum-of-diameters against an oblivious adversary use the following paradigm:
we maintain bi-criteria approximations, i.e., solutions with a small
approximation ratio that might use more than $k$ centers, i.e., up to $O(k/\eps)$ centers.
In a second step, we use the centers of this solution as the input
to an auxiliary dynamic instance for which we maintain a solution
with only $k$ centers. These centers then form our solution to the
actual problem, by increasing their radii appropriately. Since the
input to our auxiliary instance is much smaller than the input to
the original instance, we can afford to use algorithms for it whose
update times have a much higher dependence on $n$, \aw{and in fact we can 
even recompute the whole solution from scratch. }



More precisely, recall that in the static offline setting there is a $(3.504+\eps)$-approximation
algorithm with running time $n^{O(1/\eps)}$ for $k$-sum-of-radii~\cite{CharikarP04}.
We provide a black-box reduction that transforms any such offline
algorithm with running time of $g(n)$ and an approximation ratio of
$\alpha$ into a fully dynamic $(6+2\alpha+\eps)$-approximation algorithm
with an update time of $O(((k/\epsilon)^{5}+g(\mathrm{poly}(k/\epsilon)))\log\Delta)$.
To this end, we introduce a dynamic primal-dual algorithm that maintains
a bi-criteria approximation with up to $O(k/\epsilon)$ centers. After
each update, we use these $O(k/\epsilon)$ centers as input for the
offline $\alpha$-approximation algorithm, run it from scratch, and
increase the radii of the computed centers appropriately such that
they cover all points of the original input instance.

For computing the needed bi-criteria approximation, there is a polynomial-time
offline primal-dual $O(1)$-approximation algorithm with a running
time of $\Omega(n^{2}k)$~\cite{CharikarP04} from which we borrow
ideas. Note that a dynamic algorithm with an update time of $(k\log n)^{O(1)}$
yields an offline algorithm with a running time of $n(k\log n)^{O(1)}$
(by inserting all points one after another) and no offline algorithm
is known for the problem that is that fast. Hence, we need new ideas.
First, we formulate the problem as an LP $(P)$ which has a variable
$x_{p}^{(r)}$ for each combination of a point $p$ and a radius~$r$
from a set of (suitably discretized) radii $R$, and a constraint
for each point $p$. Let $(D)$ denote its dual LP, see below, where
$z:=\epsilon\OPT'/k$ and $\OPT'$ is a $(1+\epsilon)$-approximate estimate on the value $\OPT$ of the optimal solution.
{
\thickmuskip=4mu
\medmuskip=3mu
\thinmuskip=2mu
\begin{alignat*}{9}
\min & \,\,\, & \sum_{p\in P}\sum_{r\in R}x_{p}^{(r)}(r+z) &  &  &  &  &  &  &  & \max & \,\,\, & \sum_{p\in P}y_{p}\,\,\,\,\,\,\,\,\,\,\,\\
\mathrm{s.t.} &  & \sum_{p'\in P}\sum_{r:d(p,p')\le r}x_{p'}^{(r)} & \ge1 & \,\,\, & \forall p\in P & \,\,\,\,\,\,\,\: & (P) & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\, &  & \mathrm{s.t.} &  & \sum_{p'\in P:d(p,p')\le r}y_{p'} & \le r+z & \,\,\, & \forall p\in P,r\in R & \,\,\,\,\,\,\,\:(D)\\
 &  & x_{p}^{(r)} & \ge0 &  & \forall p\in P\,\,\forall r\in R &  &  &  &  &  &  & y_{p} & \ge0 &  & \forall p\in P
\end{alignat*}
}

We select a point $c$ randomly and raise its dual variable $y_{c}$.
Unlike \cite{CharikarP04}, we raise $y_{c}$ only until the constraint
for~$c$ and some radius $r$ becomes \emph{half-tight, }i.e., $\sum_{p':d(c,p')\le r}y_{p'}=r/2+z$.\emph{
}We show that then we can guarantee that no other dual constraint
is violated, which saves running time since we need to check only
the (few, i.e. $|R|$ many) dual constraints for~$c$, and not the constraints for
all other input points when we raise $y_{c}$. We add $c$ to our
solution and assign it a radius of~$2r$. Since the constraint for
$(c,r)$ is half-tight, our dual can still ``pay'' for including $c$
with radius $2r$ in our solution. %

In primal-dual algorithms, one often raises all primal variables whose
constraints have become tight, in particular in the corresponding
routine in~\cite{CharikarP04}. However, since we assign to $c$
a radius of $2r$, we argue that we do not need to raise the up to
$\Omega(n)$ many primal variables corresponding to dual constraints
that have become (half-)tight. Again, this saves a considerable amount
of running time. Then, we consider the points that are not covered
by $c$ (with radius $2r$), select one of these points uniformly
at random, and iterate. After a suitable pruning routine (which is
faster than the corresponding routine in~\cite{CharikarP04}) this
process opens $k'= O(k/\eps)$ centers at cost $(6+\eps)\OPT'$,
or asserts that $\OPT>\OPT'$. We show that we can maintain this solution dynamically.


As mentioned above, after each update we feed the centers of the bi-criteria
approximation as input points to our (arbitrary) static offline $\alpha$-approximation
algorithm for $k$-sum-of-radii and run it. Finally, we translate
its solution to a $(6+2\alpha+\eps)$-approximate solution to the original
instance. For the best known offline approximation algorithm \cite{CharikarP04}
it holds that $\alpha=3.504$, and thus we obtain a ratio of $13.008+\eps$
overall. For $k$-sum-of-diameters the same solution yields a $(26.016+2\epsilon)$-approximation.
\vspace{-.2cm}

\subsection{Other Related Work}


\paragraph{$k$-means and $k$-median.}
For $k$-means~\cite{HenzingerK20} gave a conditional lower bound against an oblivious adversary for any dynamic better-than-4 approximate $k$-means algorithm showing that for any $\gamma > 0$ no algorithm with $O(k^{1-\gamma})$ update time and $O(k^{2-\gamma})$ query time exists. 
There exists a fully dynamic constant approximation algorithm for $k$-means and $k$-median with expected amortized update time $\tilde O(n + k^2)$ that tries to minimize the number of center changes~\cite{cohen2019fully}. 
For $d$-dimensional Euclidean metric space dynamic algorithms for $k$-median and $k$-means exist by combining dynamic coreset algorithms for that metric space with a static algorithm, but their running time is in $O(poly(\epsilon^{-1}, k, \log \Delta, d))$~\cite{FrahlingS05,FeldmanSS13,BravermanFLSY17,DBLP:conf/focs/SohlerW18}.

\paragraph{$k$-sum-of-radii.} For a variant of the sum-of-radii problem~\cite{henzinger2020dynamic} gives a fully dynamic algorithm in metric spaces with doubling dimension $\kappa$ that achieves a $O(2^{2\kappa})$ approximation in time $O(2^{6\kappa}\log \Delta)$.
