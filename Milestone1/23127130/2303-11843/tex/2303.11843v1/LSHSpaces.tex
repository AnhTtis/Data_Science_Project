\section{Fully Dynamic $k$-Centers via Locally Sensitive Hashing}\label{sec:LSH}

In this section, we demonstrate how the algorithm for general metric spaces of Section \ref{sec:generalMetric} can be improved to run in \textit{sublinear} in $n$ amortized update time, even when $k = \Theta(n)$, if the metric in question admits good locally sensitive hash functions (introduced below in Section \ref{sec:LSHAlg}). Roughly speaking, a locally sensitive hash function is a mapping $h: \cX \to U$, for an universe $U$, which has the property that points which are close in the metric should collide, and points which are far should not. Thus, when searching for points which are close to a given $x \in \cX$, one can first apply a LSH to quickly prune far points, and search only through the points in the hash bucket $h(x)$. We will use this approach to speed up the algorithm from Section \ref{sec:generalMetric}. 

There are several serious challenges when using an arbitrary ANN data structure to answer nearest-neighbor queries for our $k$-center algorithm. Firstly, the algorithm \textit{adaptively} queries the ANN data structure: the points which are inserted into $\LFMIS_{k+1}$, as well as the future edges which are reported by the data structure, depend on the prior edges which were returned by the data structure. Such adaptive reuse breaks down traditional guarantees of randomized algorithms, hence designing such algorithms which are robust to adaptivity is the subject of a growing body of research \cite{ben2020framework,cherapanamjeri2020adaptive,HassidimKMMS20,WoodruffZ20,ACSS21}. More nefariously, the adaptivity also goes in the other direction: namely, the random ordering $\pi$ influences which points will be added to the set $\LFMIS_{k+1}$, in turn influencing the future queries made to the ANN data structure, which in turn dictate the edges which exist in the graph (by means of queries to the ANN oracle). Thus, the graph itself cannot be assumed to be independent of $\pi$! However, we show that we can leverage LSH functions to define the graph independent of $\pi$.

\paragraph{Summary of the LSH-Based Algorithm. }
We now describe the approach of our algorithm for LSH spaces.
Specifically, first note that the factor of $k$ in the amortized update time in Proposition \ref{prop:T} comes from the time required to compute $S = \ALG \cap N(v)$ in Algorithm \ref{alg:ins}. However, to determine which of the three cases we are in for the execution of Algorithm \ref{alg:ins}, we only need to be given the value $u^* = \arg \min_{u' \in S} \pi(u')$ of the vertex in $S$ with smallest rank, or $S = \emptyset$ if none exists. If $S = \emptyset$, then the remainder of Algorithm \ref{alg:ins} runs in constant time. If $\pi(u^*) < \pi(v)$, where $v$ is the query point, then the remaining run-time is constant unless $v$ was a leader, in which case it is proportional to the number of followers of $v$, each of which are inserted into $\cQ$ at that time. Lastly, if $\pi(u^*) > \pi(v)$, then we search through each $u \in S$, make $u$ a follower of $v$, and add the followers of $u$ to $\cQ$. If $u$ was previously in $\ALG$, it must have also been in $\LFMIS_{k+1}$ on the prior time step. Thus it follows that each such $u \in S$ changes its eliminator on this step.

In summary, after the computation of $S = \ALG \cap N(v)$, the remaining runtime is bounded by the sum of the number of points added to $\cQ$, and the number of points that change their eliminator on that step. Since, ultimately, the approach in Section \ref{sec:amortized} was to bound $T$ by the total number of times a point's eliminator changes, our goal will be to obtain a more efficient data structure for returning $S = \ALG \cap N(v)$. Specifically, if after a small upfront runtime $R$, such a data structure can read off the entries of $S$ in the order of the rank, each in constant time, one could therefore replace the factor of $k$ at both parts of the sum in the running time bound of Proposition \ref{prop:T} by $R$. We begin by formalizing the guarantee that such a data structure should have. 

We will demonstrate that approximate nearest neighbor search algorithms based on locally sensitive hashing can be modified to have the above properties. However, since such an algorithm will only be approximate, it will sometimes returns points in $S$ which are farther than distance $r$ from $v$, where $r$ is the threshold. Thus, the resulting graph defined by the locally sensitive hashing procedure will now be an \textit{approximate threshold graph:}

\begin{definition}\label{def:approxThreshGraph}
Fix a point set $P$ from a metric space $(\cX,d)$, and real values $r>0$ and $c\geq 1$. A $(r,c,L)$-approximate threshold graph $G_{r,c} = (V(G_{r,c}),E(G_{r,c}))$ for $P$ is any graph with $V(G_{r,c}) = P$, and whose whose edges satisfy $E(G_r) \subseteq E(G_{r,c})$ and $|E(G_{r,c}) \setminus E(G_{cr})| \leq L$, where $E(G_r), E(G_{cr})$ are the edge set of the threshold graphs $G_r,G_{cr}$ respectively. 


\end{definition}

If $L = 0$, it is straightforward to see that an algorithm for solving the top-$k$ LFMIS problem on a $(r,c,0)$-approximate threshold graph $G_{r,c}$ can be used to obtain a $c(2+\eps)$ approximation to $k$-center. When $L>0$, an algorithm can first check, for each edge $e \in E(G_{r,c})$ it considers, whether $e \in G_{rc}$, and discard it if it is not the case. We will see that the runtime of handling a $(r,c,L)$-approximate threshold graph will depend linearly on $L$. Moreover, we will set parameters so that $L$ is a constant in expectation. 



\subsection{Locally Sensitive Hashing and the LSH Algorithm}\label{sec:LSHAlg}

We begin by introducing the standard definition of a locally sensitive hash family for a metric space \cite{indyk1998approximate}.

\begin{definition}[Locally sensitive hashing \cite{indyk1998approximate,har2012approximate}]\label{def:LSH}
Let $\cX$ be a metric space, let $U$ be a range space, and let $r \geq 0$ $c \geq 1$ and $0 \leq p_2 \leq p_1  \leq 1$ be reals.
A family $\cH = \{h : \cX \to U\}$ is called $(r,cr,p_1,p_2)$-sensitive if for any $q,p \in \cX$:

\begin{itemize}
    \item If $d(p,q) \leq r$, then $\prb{\cH}{h(q) = h(p)} \geq p_1$. 
    \item If $d(p,q) > cr$, then $\prb{\cH}{h(q) = h(p)} \leq p_2$. 
\end{itemize}
\end{definition}

Given a $(r,cr,p_1,p_2)$-sensitive family $\cH$, we can define $\cH^t$ to be the set of all functions $h^t: \cX \to U^t$ defined by $h^t(x) = (h_1(x),h_2(x),\dots,h_t(x))$, where  $h_1,\dots,h_k \in \cH$. In other words, a random function from $\cH^t$ is obtained by drawing $t$ independent hash functions from $\cH$ and concatenating the results. It is easy to see that the resulting hash family $\cH^t$ is $(r,cr,p_1^t,p_2^t)$-sensitive. 
We now demonstrate how a $(r,c)$-approximate threshold graph can be defined via a locally sensitive hash function. 

\begin{definition}\label{def:inducedGraph}
Fix a metric space $(\cX,d)$ and a finite point set $P \subset \cX$, as well as integers $t,s \geq 1$. Let $\cH: \cX \to U$ be a $(r,cr,p_1,p_2)$-sensitive family. Then a graph $G_{r,cr}(P,\cH,t,s) = ( V,E)$ induced by $\cH$ is a random graph which is generated via the following procedure. First, one randomly selects hash functions $h_1,h_2,\dots,h_s \sim \cH^t$. Then the vertex set is given by $V=P$, and then edges are defined via $(x,y) \in E$ if and only if $h_i(x) = h_i(y)$ for some $i \in [s]$. 
\end{definition}

We now demonstrate that, if $\cH$ is a sufficiently sensitive hash family, the random graph $G_{r,cr}(P,\cH,t,s)$ constitutes a $(r,c,L)$-approximate threshold graph with good probability, where $L$ is a constant in expectation. 

\begin{proposition}\label{prop:isAThreshold}
Fix a metric space $(\cX,d)$ and a point set $P \subset \cX$ of size $|P|= n$, and let $\cH: \cX \to U$ be a $(r,cr,p_1,p_2)$-sensitive family. Fix any $\delta \in (0,\frac{1}{2})$. Set $s = \ln(n^2/\delta) n^{2\rho}/p_1 $, where $\rho = \ln \frac{1}{p_1}/\ln \frac{1}{p_2}$, and  $t = \lceil 2 \log_{1/p_2} n  \rceil$. Then, with probability at least $1-\delta$, the the random graph $G_{r,cr}(P,\cH,t,s)$ is a $(r,c,L)$-approximate threshold graph, where $L$ is a random variable satisfying $\ex{L} < 2$ (Definition \ref{def:approxThreshGraph}). 
\end{proposition}
\begin{proof}

First, fix any $x,y \in P$ such that $d(x,y) > c r$. We have that $\prb{h \sim \cH^k}{h(x) = h(y)} \leq p_2^t < \frac{1}{ n^2}$. It follows that 
\[\exx{h_1,\dots,h_s \sim \cH^t}{|E(G_{r,cr}(P,\cH,t,s) \setminus E(G_{r })|} \leq \sum_{(x,y) \in P^2} \frac{1}{n^2} < 1\]
Namely, we have $\ex{L} < 1$ where $L = |E(G_{r,cr}(P,\cH,t,s) \setminus E(G_{r })|$.
Next, fix any $(x,y) \in E(G_r)$. We have 
\[\prb{h \sim \cH^k}{h(x) = h(y)} \geq p_1^t > p_1^{2 \log_{1/p_2} n  + 1}= p_1 (n^2)^{-\rho}\] Thus, the probability that at least one $h_i$ satisfies $h_i(x) = h_i(y)$ is at least
\[1-(1- p_1 n^{-2\rho})^s >1-(1/e)^{\ln(n^2/\delta)} =  1- \delta/n^2\] 
After a union bound over all such  possible pairs, it follows that $(x,y) \in E(G_{r,cr}(P,\cH,t,s)) $ for all $(x,y) \in E(G_r)$ with probability at least $1-\delta$. Note that since $\delta < 1/2$ and $L$ is a non-negative random variable, it follows that conditioning on the prior event can increase the expectation of $L$ by at most a factor of $2$, which completes the proof. 
\end{proof}

We will now describe a data structure which allows us to maintain a subset $\cL$ of vertices of the point set $P$, and quickly answer queries for neighboring edges of a vertex $v$ in the graph $G$ defined by the intersection of $G_{r,cr}(P,\cH,t,s)$ and $G_{cr}$. Note that if $G_{r,cr}(P,\cH,t,s)$ is a $(r,c,L)$-approximate threshold graph, then this intersection graph $G$ satisfies $G_r \subseteq G \subseteq G_{cr}$, and is therefore a  $(r,c,0)$-approximate threshold graph. It is precisely this graph $G$ which we will run our algorithm for top-$k$ LFMIS on. However, in addition to finding all neighbors of $v$ in $\cL$, we will also need to quickly return the neighbor with smallest rank $\pi$, where $\pi:V \to [0,1]$ is a random ranking as in Section \ref{sec:generalMetric}. Roughly, the data structure will hash all points in $\cL$ into the hash buckets given by $h_1,\dots,h_s$, and maintain each hash bucket via a binary search tree of depth $O(\log n)$, where the ordering is based on the ranking $\pi$. 

For the following Lemma and Theorem, we fix a metric space $(\cX,d)$ and a point set $P \subset \cX$ of size $|P|= n$, as well as a scale $r > 0$, and approximation factor $c$. Moreover, let $\cH: \cX \to U$ be a $(r,cr,p_1,p_2)$-sensitive hash family for the metric space $\cX$, and let $\Run(\cH)$ be the time required to evaluate a hash function $h \in \cH$. Furthermore, let $\pi: P \to [0,1]$ be any ranking over the points $P$, such that $\pi(x)$ is truncated to $O(\log n)$ bits, and such that $\pi(x) \neq \pi(y)$ for any distinct $x,y \in P$ (after truncation). Note that the latter holds with probability $1-1/\poly(n)$ if $\pi$ is chosen uniformly at random. Lastly, for a graph $G = (V(G),E(G))$, let $N_G(v)$ be the neighborhood of $v$ in $G$ (in order to avoid confusion when multiple graphs are present). 
\begin{lemma}\label{lem:binaryDataStructure}
Let $G_{r,cr}(P,\cH,t,s)$ be a draw of the random graph as in Definition \ref{def:inducedGraph}, where $r,c,P,\cH$ are as above, such that $G_{r,cr}(P,\cH,t,s)$ is a $(r,c,L)$-approximate threshold graph. Let $G = G_{r,cr}(P,\cH,t,s)\cap G_{cr}$. Then there is a fully dynamic data structure, which maintains a subset $\cL \subset V(G)$ and can perform the following operations:
\begin{itemize}
    \item $\ttx{Insert}(v)$: inserts a vertex $v$ into $\cL$ in time $O(s  \log n + ts \Run(\cH))$
    \item $\ttx{Delete}(v)$: deletes a vertex $v$ from $\cL$ in time $O(s \log n + ts \Run(\cH))$
    \item $\ttx{Query-Top}(v)$: returns $u^* = \arg \min_{u \in N_G \cap \cL} \pi(u)$, or \textsc{Empty} if $N_G(v) \cap \cL = \emptyset$, in time $O(sL \log n + ts \Run(\cH)) $
    \item $\ttx{Query-All}(v)$: returns the set $N_G(v) \cap \cL$, running in time $O(s(|N_G(v) \cap \cL| + L) \log n +  ts \Run(\cH))$. 
\end{itemize}
Moreover, given the hash functions $h_1,\dots,h_s \in \cH^t$ which define the graph $G_{r,cr}(P,\cH,t,s)$, the algorithm is deterministic, and therefore correct even against an adaptive adversary. 
\end{lemma}
\begin{proof}
For each $i \in [s]$ and hash bucket $b \in U$, we store a binary tree $T_{i,b}$ with depth at most $O(\log n)$, such that each node $z$ corresponds to an interval $[a,b] \subset [0,1]$, and the left and right children of $z$ correspond to the intervals $[a,(a+b)/2]$ and $[(a+b/2),b]$, respectively. Moreover, each node $z$ maintains a counter for the number of points in $\cL$ which are stored in its subtree. Given a vertex $v$ with rank $\pi(v)$, one can then insert $v$ into the unique leaf of $T_{i,b}$ corresponding to the $O(\log n)$ bit value $\pi(v)$ in time $O(\log n)$. Note that, since the keys $\pi(v)$ are unique, each leaf contains at most one vertex. Similarly, one can remove and search for a vertex from $T_{i,b}$ in time $O(\log n)$.

When processing any query for an input vertex $v \in P$, one first evaluates all $ts$ hash functions required to compute $h_1(v),\dots,h_s(v)$, which requires  $ts \Run(\cH)$ time. For insertions and deletions, one can insert $v$ from each of the $s$ resulting trees in time $O(\log n)$ per tree, which yields the bounds for $\ttx{Insert}(v)$ and $\ttx{Delete}(v)$. For $\ttx{Query-Top}(v)$, for each $i \in [s]$, one performs an in-order traversal of $T_{i,h_i(v)}$, ignoring nodes without any points stored in their subtree, and returns the first leaf corresponding to a vertex $u \in N_G(v)$, or \textsc{Empty} if all points in the tree are examined before finding such a neighbor. Each subsequent non-empty leaf in the traversal can be obtained in $O(\log n)$ time, and since by definition of a $(r,c,L)$-approximate threshold graph, $h_i(v) = h_i(u')$ for at most $u'$ vertices with $(v,u') \notin G_{cr}$, it follows that one must examine at most $L$ vertices $u'$ with $(v,u') \notin G_{cr}$ before one finds  $u^* = \arg \min_{u \in N_G \cap \cL} \pi(u)$ (or exhausts all points in the tree). Thus, the runtime is $O(L \log n)$ to search through each of the $s$ hash functions, which results in the desired bounds. 

Finally, for $\ttx{Query-All}(v)$, one performs the same search as above, but instead completes the full in-order traversal of each tree $T_{i,h_i(v)}$. By the  $(r,c,L)$-approximate threshold graph property, each tree $T_{i,h_i(v)}$ contains at most $|N_G(v) \cap \cL| + L$ vertices from $\cL$, after which the runtime follows by the argument in the prior paragraph. 
\end{proof}

Given the data structure from Lemma \ref{lem:binaryDataStructure}, we will now demonstrate how the algorithm from Section \ref{sec:generalMetric} can be implemented in sublinear in $k$ time, given a sufficently good LSH function for the metric. The following theorem summarizes the main consequences of this implementation, assuming the graph $G_{r,cr}(P,\cH,t,s)$ is a $(r,c,L)$-approximate threshold graph. 

\begin{theorem}\label{thm:LSHPreMain}
 Let $G_{r,cr}(P,\cH,t,s)$ be a draw of the random graph as in Definition \ref{def:inducedGraph}, where $r,c,P,\cH$ are as above, such that $G_{r,cr}(P,\cH,t,s)$ is a $(r,c,L)$-approximate threshold graph. Let $G = (V,E)$ be the graph with $V = P$ and $E  = E(G_{r,cr}(P,\cH,t,s))\cap E(G_{cr})$. Then there is a fully dynamic data structure which, under a sequence of vertex insertions and deletions from $G$, maintains a top-$n$ LFMIS with leaders (Definition \ref{def:LFMISLead}) of $G$ at all time steps. The expected amortized per-update runtime of the algorithm is $O( ( sL \log n + ts \Run(\cH)) \log n + \log^2 n )$, where the expectation is taken over the choice of $\pi$.
\end{theorem}
\begin{proof}
The algorithm is straightforward: we run the fully dynamic algorithm for top-$k$ LFMIS with leaders from Section \ref{sec:generalMetric}, however we utilize the data structure from Lemma \ref{lem:binaryDataStructure} to compute $S = \cL \cap N_G(v)$ in Algorithm \ref{alg:ins} (where $\cL = \cL_{n+1}$), as well as handle deletions from $\cL$ in Algorithm \ref{alg:del}. Note that to handle a call to $\ins(v)$ of Algorithm \ref{alg:ins}, one first calls  $\ttx{Query-Top}(v)$ in data structure from Lemma \ref{lem:binaryDataStructure}. If the result is $\emptyset$, or $u$ with $\pi(u) < \pi(v)$, then one can proceed as in Algorithm \ref{alg:ins} but by updating $\cL$ via Lemma \ref{lem:binaryDataStructure}. If the result is $u$ with $\pi(u) > \pi(v)$, one then calls  $\ttx{Query-All}(v)$ to obtain the entire set $S = N_G(v) \cap \cL$, each of which will subsequently be made a follower of $v$. 

Let $T$ be the total number of times that a vertex is inserted into the queue $\cQ$ over the entire execution of the algorithm (as in Proposition \ref{prop:T}), and as in 
Section \ref{sec:amortized}, we let $\cC^t_\pi$ denote the number of vertices whose eliminator changed after the $t$-th time step. We first prove the following claim, which is analogous to  Proposition \ref{prop:T}. 

\begin{claim}
The total runtime of the algorithm, over a sequence of $M$ insertions and deletions of vertices from $G$, is at most $O(T(\lambda + \log n) + \lambda (M+ \sum_{t \in [M}) \cC^t_\pi))$, where $\lambda = sL \log n + ts \Run(\cH)$. 
\end{claim}
\begin{proof}
First note that $\lambda$ upper bounds the cost of inserting and deleting from $\cL$, as well as calling $\ttx{Query-Top}(v)$. For every vertex, when it is first inserted into the stream, we pay it a cost of $\lambda$ to cover the call to $\ttx{Query-Top}(v)$. Moreover, whenever a vertex is added to the queue $\cQ$, we pay a cost of $\lambda$ to cover a subsequent call to $\ttx{Query-Top}(v)$ when it is removed from the queue and inserted again, plus an additional $O(\log n)$ required to insert and remove the top of a priority queue. The only cost of the algorithm which the above does not cover is the cost of calling $\ttx{Query-All}(v)$, which can be bounded by $O(\lambda \cdot |N_G(v) \cap \cL| )$. Note that, by correctness of the top-$k$ LFMIS algorithm (Lemma \ref{lem:correctness}), each vertex in $\cL$ is its own eliminator at the beginning of each time step. It follows that each vertex $u \in N_G(v) \cap \cL$ had its eliminator changed on step $t$, since $\ttx{Query-All}(v)$ is only called on time step $t$ in the third case of Algorithm \ref{alg:ins}, where all points in $|N_G(v) \cap \cL|$ will be made followers of $v$. Thus the total cost of all calls to $\ttx{Query-All}(v)$ can be bounded by $\lambda \sum_{t \in [M}) \cC^t_\pi$, which completes the proof of the claim. 
\end{proof}

Given the above, by Lemma \ref{lem:main} we have that $T \leq M \leq 6 \cC^t_\pi$, and by Theorem \ref{thm:elimBound} we have $\exx{\pi}{\sum_t \cC^t_\pi} = O(M \log n)$. It follows that, the expected total runtime of the algorithm, taken over the randomness used to generate $\pi$ (with $h_1,\dots,h_s$ previously fixed and conditioned on) is at most $O(M \log^2 n + M \lambda \log n)$ as needed.
\end{proof}


\begin{theorem}\label{thm:lshMain}
Let $(\cX,d)$ be a metric space, and fix $\delta \in (0,1/2)$. Suppose that for any $r \in (r_{\min}, r_{\max})$ there exists an $(r,cr,p_1,p_2)$-sensitive hash family $\cH_r: \cX \to U$, such that each $h \in \cH_r$ can be evaluated in time at most $\Run(\cH)$, and such that $p_2$ is bounded away from $1$. Then there is a fully dynamic algorithm that, on a sequence of $M$ insertions and deletions of points from $\cX$, given an upper bound $M \leq \hat{M} \leq \poly(M)$, with probability $1-\delta$, correctly maintains a $c(2+\eps)$-approximate $k$-center clustering to the active point set $P^t$ at all time steps $t$, simultaneously for all $k \geq 1$. The total runtime of the algorithm is at most 
\[\tilde{O}\left(M \cdot \frac{\log \Delta \log \delta^{-1}}{\eps p_1} n^{2 \rho} \cdot \Run(\cH) \right)\]
where $\rho = \frac{\ln p_1}{\ln p_2}$, and $n$ is an upper bound on the maximum number of points at any time step.
\end{theorem}
\begin{proof}

We first demonstrate that there exists an algorithm $\cA(\delta,M)$ which takes as input $\delta \in (0,1/2)$ and $M \geq 1$, and on a sequence of at most $M$ insertions and deletions of points in $\cX$, with probability $1-\delta$ correctly solves the top-$M$ LFMIS with leaders (Definition \ref{def:LFMISLead}) on a graph $G$ that is $(r,c,0)$-approximate threshold graph for $P$ (Definition \ref{def:approxThreshGraph}), where $P \subset \cX$ is the set of all points which were inserted during the sequence, and runs in total expected time $\alpha_{\delta,M}$, where 

\[\alpha_{\delta,M} = \tilde{O}\left(M^{1+2\rho} \log(M/\delta)  \Run(\cH)  \right)\]
The reduction from having such an algorithm to obtaining a $k$-center solution for every $k \geq 1$, incurring a blow-up of $\eps^{-1} \log \Delta$, and requiring one to scale down $\delta$ by a factor of $O(\eps^{-1} \log \Delta)$ so that all $\eps^{-1} \log \Delta$ instances are correct, is the same as in Section \ref{sec:kCenters}, with the modification that the clustering obtained by a MIS $\cL$ at scale $r \geq 0$ has cost at most $cr$, rather than $r$. Thus, in what follows, we focus on a fixed $r$. 

First, setting $s_{\delta,M} = O(\log (M/\delta ) M^{2\rho} / p_1)$ and $t_M = O(\log_{1/p_2} M)$, by Proposition \ref{prop:isAThreshold} it holds that with probability $1-\delta$ the graph $G_{r,cr}(P,\cH,t_M,s_{\delta,M})$ is a $(r,c,L)$-approximate threshold graph, with $\ex{L} < 2$. Then by Theorem \ref{thm:LSHPreMain}, there is an algorithm which maintains a top-$M$ LFMIS with leaders to the graph $G = G_{r,cr}(P,\cH,t_M,s_{\delta,M}) \cap G_{rc}$, which in particular is a $(r,c,0)$-approximate threshold graph for $P$, and runs in expected time at most  

\[O( M ( s_{\delta,M} L \log M+ t_M s_{\delta,M} \Run(\cH)) \log M+ M \log^2 M )\]
where the expectation is taken over the choice of the random ranking $\pi$. Taking expectation over $L$, which depends only on the hash functions $h_1,\dots,h_{s_{\delta,M}}$, and is therefore independent of $\pi$, the expected total runtime is at most $\alpha_{\delta,M}$ as needed. 

To go from the updated time holding in expectation to holding with probability $1-\delta$, we follow the same proof of Proposition \ref{prop:highProb}, except that we set the failure probability of each instance to be $O(\delta/\log^2(M/\delta))$. By the proof of Proposition \ref{prop:highProb}, the total number of copies ever run by the algorithm is at most $O(\log(1/\delta) \sum_{i=1}^{\log M} i) = O(\log(1/\delta) \log^2 M)$ with probability at most $1-\delta/2$, and thus with probability at least $1-\delta$ it holds that both at most $O(\log(1/\delta) \log^2 M)$  copies of the algorithm are run, and each of them is correct at all times. Note that whenever the runtime of the algorithm exceeds this bound, the algorithm can safely terminate, as the probability that this occurs is at most $\delta$ by Proposition \ref{prop:highProb}.

Put together, the above demonstrates the existence of an algorithm $\bar{\cA}(\delta,M)$ which takes as input $\delta \in (0,1/2)$ and $M \geq 1$, and on a sequence of at most $M$ insertions and deletions of points in $\cX$, with probability $1-\delta$ correctly maintains a $c(2+\eps)$-approximation to the optimal $k$-center clustering simultaneously for all $k \geq 1$, with total runtime at most $\tilde{O}(\eps^{-1} \log \Delta \alpha_{\delta,M})$. However, we would like to only run instances of $\bar{\cA}(\delta,M)$ with $M = O(n)$ at any given point in time, so that the amortized update time has a factor of $n^{2 \rho}$ instead of $M^{2\rho}$. To accomplish this, we greedily pick time steps $1 \leq t_1 < t_2 < \dots < M$ with the property that $t_i - t_{i-1} = \lceil |P^{t_{i-1}}|/2 \rceil$. Observe that for such time steps, we have $|P^{t_{i}}| <  2|P^{t_{i-1}}|$. We then define time steps $1 \leq t_1' < t_2' < \dots < M$ with the property that $t_i'$ is the first time step where the active point set size exceeds $2^i$. We then run an instance of $\bar{\cA}(\delta_0,2^i)$, starting with $i=1$, where $\delta_0 = \delta/M$. Whenever we reach the next time step $t_{i+1}'$ we restart the algorithm $\bar{\cA}$ with parameters $(\delta_0, 2^{i+1})$, except that instead of running $\bar{\cA}$ on the entire prefix of the stream up to time step $t_{i+1}'$, we only insert the points in $P^{t_{i+1}'}$ which are active at that time step. Similarly, when we reach a time step $t_{j}$, we restart $\bar{\cA}$ with the same parameters, and begin by isnerting the active point set $P^{t_{j}}$, before continuing with the stream. 

Note that $\bar{\cA}$ is only restarted $\log n$ times due to the active point set size doubling. Moreover, each time it is restarted due to the time step being equal to $t_j$ for some $j$, the at most $2|P^{t_{j-1}}|$ point insertions required to restart the algorithm can be amortized over the $t_j - t_{j-1} \geq |P^{t_{j-1}}| /2$ prior time steps. Since each instance is correct with probability $1-\delta_0$, by a union bound all instances that are ever restarted are correct with probability $1-\delta$. Note that, since the amortized runtime dependency of the overall algorithm on $M$ is polylogarithmic, substituting $M$ with an upper bound $\hat{M}$ satisfying  $M \leq \hat{M} \leq \poly(M)$ increases the runtime by at most a constant. Moreover, we never run $\bar{\cA}(\delta,t)$ with a value of $t$ larger than $2 n$, which yields the desired runtime.  
\end{proof}

\subsection{Corollaries of the LSH Algorithm to Specific Metric Spaces}\label{sec:LSHcorollaries}
We now state our results for specific metric spaces by applying Theorem \ref{thm:lshMain} in combination with known locally sensitive hash functions from the literature. 
The following corollary follows immediately by application of the locally sensitive hash functions of \cite{datar2004locality,har2012approximate}, along with the bounds from Theorem \ref{thm:lshMain}. 

\begin{corollary}\label{cor:Euclidean} Fix any $c \geq 1$. Then there is a fully dynamic algorithm which, on a sequence of $M$ insertions and deletions of points from $d$-dimensional Euclidean space $(\R^d , \ell_p)$, for $p \geq 1$ at most a constant,  with probability $1-\delta$, correctly maintains a $c(4+\eps)$-approximate $k$-center clustering to the active point set $P^t$ at all time steps $t \in [M]$, and simultaneously for all $k \geq 1$. The total runtime is at most 

\[\tilde{O}\left( M \frac{ \log \delta^{-1} \log \Delta  }{\eps} d n^{1/c}\right)\]
\end{corollary}

For the case of standard Euclidean space ($p=2$), one can used the improved ball carving technique of Andoni and Indyk \cite{andoni2006near} to obtain better locally sensitive hash functions, which result in the following:

\begin{corollary}\label{cor:Euclidean2} Fix any $c \geq 1$. Then there is a fully dynamic algorithm which, on a sequence of $M$ insertions and deletions of points from $d$-dimensional Euclidean space $(\R^d , \ell_2)$,  with probability $1-\delta$, correctly maintains a $c(\sqrt{8}+\eps)$-approximate $k$-center clustering to the active point set $P^t$ at all time steps $t \in [M]$, and simultaneously for all $k \geq 1$. The total runtime is at most 

\[\tilde{O}\left( M\frac{ \log \delta^{-1} \log \Delta  }{\eps} d n^{1/c^2 + o(1)}\right)\]
\end{corollary}

Additionally, one can use the well-known MinHash \cite{broder1997resemblance} to obtain a fully dynamic $k$-center algorithm for the \textit{Jaccard Distance}. Here, the metric space is the set of all subsets of a finite universe $X$, equipped with the distance $d(A,B) = 1- \frac{|A \cap B|}{|A \cup B|}$ for $A,B \subseteq X$. We begin stating a standard bound on the value of $\rho$ for the MinHash LSH family.

\begin{proposition}[\cite{indyk1998approximate}]
Let $\cH$ be the hash family given by 
\[ \cH = \{h_\pi :2^X \to X \; | \;  h_\pi(A) = \arg \min_{a \in A} \pi(a), \; \pi \text{ is a permutation of } X \} \] 
Then for any $c \geq 1$ and $r \in [0,1/(2c)]$, we have that $\cH$ is $(r,cr,1-r,1-cr)$-sensitive for the Jaccard Metric over $X$, where $\rho = 1/c$. 
\end{proposition}
\begin{proof}
If $d(A,B) = r$, for any $r \in [0,1]$, we have 
\[ \prb{h\sim \cH}{h(A) = h(B)} = \frac{|A \cap B|}{|A \cup B|} = 1-r \]
Thus, we have $p_1 = 1-r$ and $p_2 = 1-cr$. Now by Claim $3.11$ of \cite{har2012approximate}, we have that for all $x \in [0,1)$ and $c \geq 1$ such that $1-cx > 0$, the following inequality holds:
\[ \frac{\ln(1-x)}{\ln(1-cx)} \leq \frac{1}{c} \]
Thus we have the desired bound:

\[       \rho \leq \frac{\ln(1-r)}{\ln(1- cr)} \leq 1/c \qedhere Â¸\]
\end{proof}

\begin{corollary}\label{cor:Jaccard}
Let $X$ be a finite set, and fix any $c \geq 1$. Then there is a fully dynamic algorithm which, on a sequence of $M$ insertions and deletions of subsets of $X$ equipped with the Jaccard Metric,  with probability $1-\delta$, correctly maintains a $c(4+\eps)$-approximate $k$-center clustering to the active point set $P^t$ at all time steps $t \in [M]$, and simultaneously for all $k \geq 1$. The total runtime is at most 
\[\tilde{O}\left(M \frac{ \log \delta^{-1} \log \Delta  }{\eps} |X| n^{1/c}\right)\]
\end{corollary}
\begin{proof}
Letting $r_{\min}$ be the minimum distance between points in the stream, we run a copy of the top-$M$ LFMIS algorithm of Theorem \ref{thm:LSHPreMain} for $r = r_{\min}, (1+\eps)r_{\min},\dots,1/(2c)$, where we set the value of the approximation factor $c$ in Theorem \ref{thm:LSHPreMain} to be scaled by a factor of $2$, so that each instance computes a LFMIS on a $(r,2c,0)$-approximate threshold graph. Note that for any time step $t$, if the copy of the algorithm for $r=1/c$ contains a LFMIS with at least $k+1$ vertices, it follows that the cost of the optimal clustering is at most $1/(4c)$. In this case, we can return an arbitrary vertex as a $1$-clustering of the entire dataset, which will have cost at most $1$, as the Jaccard metric is bounded by $1$, and thereby yielding a $4c$ approximation. Otherwise, the solution is at most a 
$c(4+2\eps)$-approximation, which is the desired result after a re-scaling of $\eps$. Note that $\Run(\cH) = \tilde{O}(|X|)$ to evaluate the MinHash, which completes the proof.  
\end{proof}


  
