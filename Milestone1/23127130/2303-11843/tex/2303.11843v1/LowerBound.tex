
\section{Lower Bound for Arbitrary Metrics}\label{sec:LB}
We now demonstrate that any algorithm which approximates the optimal $k$-center cost, in an arbitrary metric space of $n$ points, must run in $\Omega(nk)$ time. Specifically, the input to an algorithm for $k$-center in arbitrary metric spaces is both the point set $P$  \textit{and} the metric $d$ over the points. In particular, the input can be represented via the distance matrix  distance matrix $\bD \in \R^{n \times n}$ over the point set $P$, and the behavior of such an algorithm can be described by a sequences of adaptive queries to $\bD$. 

The above setting casts the problem of approximating the cost of the optimal $k$-center clustering as a \textit{property testing} problem  \cite{goldreich1998property,goldreich2017introduction}, where the goal is to solve the approximation problem while making a small number of queries to $\cD$. Naturally, the query complexity of such a clustering algorithm lower bounds its runtime, so to prove optimality of our dynamic $k$-center algorithms it suffices to focus only on the query complexity. In particular, in what follows we will demonstrate that any algorithm that approximates the optimal $k$-center cost to any non-trivial factor with probability $2/3$ must query at least $\Omega(nk)$ entries of the matrix.  In particular, this rules out any fully dynamic algorithm giving a non-trivial approximation in $o(k)$ amortized update time for general metric spaces. 

Moreover, we demonstrate that this lower bound holds for the $(k,z)$-clustering objective, which includes the well studied $k$-medians and $k$-means. Recall that this problem is defined as outputting a set $\cC \subset \cX$ of size at most $k$ which minimizes the objective function
\[  \cost[P]{\cC}[k][z] =   \sum_{p \in P} d^z(p,\ell(p))    \]
where $\ell(p)$ is the cluster center associated with the point $p$. 
Note that  $( \cost[P]{\cC}[k][z])^{1/z}$ is always within a factor of $n$ of the optimal $k$-center cost. Thus, it follows that if $k$-center cannot be approximated to any non-trivial factor (including factors which are polynomial in $n$) in $o(nk)$ queries to $\bD$, the same holds true for $(k,z)$-clustering for any constant $z$. Thus, in the proofs of the following results we focus solely on proving a lower bound for approximation $k$-center to any factor $R$, which will therefore imply the corresponding lower bounds for $(k,z)$-clustering. 

We do so by first proving Theorem \ref{thm:LBBig}, which gives a $\Omega(nk)$ lower bound when $n = \Omega(k \log k)$. Next, in Proposition \ref{prop:lb}, we prove a general $\Omega(k^2)$ lower bound for any $n > k$, which will complete the proof of Theorem \ref{thm:LB}. We note that the proof of Proposition \ref{prop:lb} is fairly straightforward, and the main challenge will be to prove Theorem \ref{thm:LBBig}. 


\begin{theorem} \label{thm:LBBig}
Fix and $k \geq 1$ and $n > C k \log k$ for a sufficiently large constant $C$. Then any algorithm which, given oracle access the distance matrix $\cD \in \R^n$ of a set $X$ of $n$ points in a metric space, determines correctly with probability $2/3$ whether the optimal $k$-center cost on $X$ is at most $1$ or at least $R$, for any value $R >1$, must make at least $\Omega(k n)$ queries in expectation to $\cD$. The same bound holds true replacing the $k$-center objective with $(k,z)$-clustering, for any constant $z>0$. 
\end{theorem}
\begin{proof}
We suppose there exists such an algorithm that makes at most an expected $k n/8000$ queries to $\cD$. By forcing the the algorithm to output an arbitrary guess for $c$ whenever it queries a factor of $20$ more entries than its expectation, by Markov's inequality it follows that there is an algorithm which correctly solves the problem with probability $2/3 - 1/20 > 6/10$, and always makes at most $kn/400$ queries to $\cD$.

\paragraph{The Hard Distribution.}
We define a distribution $\cD$ over $n \times n$ distance matrices $\cD$ as follows. First, we select a random hash function $h:[n] \to [k]$, a uniformly random coordinate $i \sim [n]$. We then set $\bD(h)$ to be the matrix defined by 
$\bD_{p,q}(h) = 1$ for $p \neq q$ if $h(p) = h(q)$, and $\bD_{p,q}(h) = R$ otherwise, where $R$ is an arbitrarily large value which we will later fix. We then flip a coin $c \in \{0,1\}$. If $c=0$, we return the matrix $\bD(h)$, but if $c = 1$, we define the matrix $\bD(h,i)$ to be the matrix resulting from changing every $1$ in the $i$-th row and column of $\bD(h)$ to the value $2R$. It is straightforward to check that the resulting distribution satisfies the triangle inequality, and therefore always results in a valid metric space. We write $\cD_0 ,\cD_1$ to denote the distribution $\cD$ conditioned on $c=0,1$ respectively. In the testing problem, a matrix $\bD \sim \cD$ is drawn, and the algorithm is allowed to make an adaptive sequence of queries to the entries of $\bD$, and thereafter correctly determine with probability $2/3$ the value of $c$ corresponding to the draw of $\bD$. 

Note that a draw from $\cD$ can then be described by the values $(h,i,c)$, where $h \in \cH = \{h': [n] \to [k] \}$, $i\in[n]$, and $c \in \{0,1\}$. Note that, under this view, a single matrix $\bD \sim \cD_0$ can correspond to multiple draws of $(h,i,0)$. Supposing there is a randomized algorithm which is correct with probability $6/10$ over the distribution $\cD$ and its own randomness, it follows that there is a deterministic algorithm $\cA$ which is correct with probability $6/10$ over just $\cD$, and we fix this algorithm now. 

Let $(d_1,p_1),(d_2,p_2),\dots,(d_t,p_t)$ be an adaptive sequence of queries and observations made by an algorithm $\cA$, where $d_i \in \{1,R,2R\}$ is a distance and $p_i \in \binom{n}{2}$ is a position in $\cD$, such that the algorithm queries position $p_i$ and observed the value $d_i$ in that position.

\begin{claim}\label{claim:lb1}
There is an algorithm with optimal query vs. success probability trade-off which reports $c=1$ whenever it sees an entry with value $d_i = 2R$, otherwise it reports $c=0$ d if it never sees a distance of value $2R$. 
\end{claim}
\begin{proof}
 To see this, first note that if $c=0$, one never sees a value of $2R$, so any algorithm which returns $c=0$ after observing a distance of size $2R$ is always incorrect on that instance. 

For the second claim, suppose an algorithm $\cA$ returned that $c=1$ after never having seen a value of $2R$. Fix any such sequence  $S= \{(d_1,p_1),(d_2,p_2),\dots,(d_t,p_t)\}$ of adaptive queries and observations such that $d_i \neq 2R$ for all $i=1,2,\dots,t$. We claim that $\pr{c=0 | S} \geq \pr{c=1 |S}$. To see this, let $(h,i,1)$ be any realization of a draw from $\cD_1$, and note that $\pr{(h,i,1)} = \pr{(h,i,0)} = \frac{1}{2 n } k^{-n}$. Let $F_0(S)$ be the set of tuples $(h,i)$ such that the draw $(h,i,0)$ could have resulted in $S$, and $F_1(S)$ the set of tuples $(h, i)$ such that $(h,i,1)$  could have resulted in $s$. Let $(h,i,1)$ be a draw that resulted in $S$. Then $(h,i,0)$ also results in $S$, because the difference between the resulting matrices $\cD$ is supported only on positions which were initially $2R$ in the matrix generated by $(h,i,1)$. Thus $F_1(S) \subseteq F_2(S)$, which demonstrates that $\pr{c=0 | S} \geq \pr{c=1 |S}$. Thus the algorithm can only improve its chances at success by reporting $c=0$, which completes the proof of the claim.
\end{proof}

\paragraph{Decision Tree of the Algorithm.}
The adaptive algorithm $\cA$ can be defined by a $3$-ary decision tree $T$ of depth at most $ k n/400$, where each non-leaf node $v \in T$ is labelled with a position $p(v) = (x_v,y_v) \in [n] \times [n]$, and has three children corresponding to the three possible observations $\bD_{p(x)} \in \{1,R,2R\}$. Each leaf node contains only a decision of whether to output $c=0$ or $c=1$. For any $v \in T$, let $v_1,v_{R}, v_{2R}$ denote the three children of $v$ corresponding to the edges labelled $1,R$ and $2R$,
Every child coming from a ``$2R$'' edge is a leaf, since by the above claim the algorithm can be assumed to terminate and report that $c=1$ whenever it sees the value of $2R$. For any vertex $v \in T$ at depth $\ell$, let $S(v) = \{ (d_1,p_1),\dots,(\cdot, p(v))\}$ be the unique sequence of queries and observations which correspond to the path from the root to $v$. Note that the last entry $(\cdot,  p(v)) \in S(v)$ has a blank observation field, meaning that at $v$ the observation $p(v)$ has not yet been made.

For any $v \in T$ and $i \in [n]$, we say that a point $i$ is \textit{light} at $v$ if the number of queries $(d_j,p_j) \in S $ with $i \in p_j$ is less than $k/2$. If $i$ is not light at $v$ we say that it is \textit{heavy} at $v$. For any $i \in [n]$, if in the sequence of observations leading to $v$ the algorithm observed a $1$ in the $i$-th row or column, we say that $i$ is \textit{dead} at $v$, otherwise we say that $i$ is \textit{alive}. We write $\pr{v}$ to denote the probability, over the draw of $\bD \sim \cD$, that the algorithm traverses the decision tree to $v$, and $\pr{v | \; c= b}$ for $b \in \{0,1\}$ to denote this probability conditioned on $\bD \sim \cD_b$.  Next, define $F_b(v) = F_b(S(v))$ for $b \in \{0,1\}$, where $F_b(S)$ is as above. 
Note that if $(h,i) \in F_0(v)$ for some $i \in [n]$, then $(h,j) \in F_0(v)$ for all $j \in [n]$, since the matrices generated by $(h,i,0)$ are the same for all $i \in [n]$. Thus, we can write $h \in F_0(v)$ to denote that $(h,i) \in F_0(v)$ for all $i \in [n]$.



\begin{claim}\label{claim:lb2}
Let $v \in T$ be a non-leaf node where at least one index $i$ in $p(v) = (i,j)$ is alive and light at $v$. Then we have \[\prb{\bD \sim \cD}{\bD_{p(v)} = 1 | S(v), c=0} \leq \frac{2}{k}\]
\end{claim}
\begin{proof}
Fix any function $h \in \cH$ such that $h \in F_0(v)$: namely, $h$ is consistent with the observations seen thus far. 
Let $h_1,\dots,h_k \in \cH$ be defined via $h_t(j) = h(j)$ for $j \neq i$, and $h_t(i) = t$, for each $t \in [k]$. We claim that $h_t \in F_0(v)$ for at least $k/2$ values of $t$. 
To show this, first note that the values of $\{h(j)\}_{j \neq i}$ define a graph with at most $k$ connected components, each of which is a clique on the set of values $j \in [n] \setminus \{i\}$ which map to the same hash bucket under $h$. 
The only way for $h_t \notin F_0(v)$ to occur is if an observation $(i,\ell)$ was made in $S(v)$ such that $h(\ell) = t$. Note that such an observation must have resulted in the value $R$, since $i$ is still alive (so it could not have been $1$). In this case, one knows that $i$ was not in the connected component containing $\ell$. However, since $i$ is light, there have been at most $k/2$ observations involving $i$ in $S(v)$. Each of these observations eliminate at most one of the $h_t$'s, from which the claim follows. 

Given the above, it follows that if at the vertex $v$ we observe $\bD_{p(v)} = \bD_{i,j} = 1$, then we eliminate every $h_t$ with $t \neq h(j)$ and $h_t \in F_0(v)$. Since for every set of values $\{h(j)\}_{j \neq i}$ which are consistent with $S(v)$ there were $k/2$ such functions $h_t \in F_0(v)$, it follows that only a $2/k$ fraction of all $h \in \cH$ result in the observation $\bD_{p(v)} = 1$. Thus, $|\cF_0(v_1)| \leq \frac{2}{k}|\cF_0(v)|$, which completes the proof of the proposition.
\end{proof}


Let $\cE_1$ be the set of leafs $v$ which are children of a $2R$ labelled edge, and let $\cE_0$ be the set of all other leaves. Note that we have $\pr{v \; | \; c=0} = 0$ for all $v \in \cE_1$, and moreover $\sum_{v \in \cE_0} \pr{v | c=0} = 1$. For $v \in T$, let $\theta(v)$ denote the number of times, on the path from the root to $v$, an edge $(u,u_1)$ was crossed where at least one index $i \in p(u)$ was alive and light at $u$. Note that such an edge kills $i$, thus we have $\theta(v) \leq n$ for all nodes $v$. Further, define $\hat{\cE}_0 \subset \cE_0$ to be the subset of vertices $v \in \cE_0$ with $\theta(v) < n/20$. We now prove two claims regarding the probabilities of arriving at a leaf $v \in \hat{\cE}_0$. 

\begin{claim}\label{claim:lb3}
Define $\hat{\cE}_0$ as above. Then we have 
\[  \sum_{v \in \hat{\cE}_0} \pr{v | c=0} > 9/10 \]
\end{claim}
\begin{proof}
We define indicator random variables $\bX_1,\bX_2,\dots,\bX_t \in \{0,1\}$, where $t \leq  kn/400$ is the depth of $T$, such that $\bX_i = 1$ if the $i$-th observation made by the algorithm causes a coordinate $i \in [n]$, which was prior to observation $i$ both alive and light, to die, where the randomness is taken over a uniform draw of $\bD \sim \cD_0$. Note that the algorithm may terminate on the $t'$-th step for some $t'$ before the $t$-th observation, in which case we all trailing variables $\bX_{t'},\dots,\bX_t$ to $0$. By Claim \ref{claim:lb2}, we have $\ex{\bX_i} \leq 2k$ for all $i \in [t]$, so $\ex{\sum_{i \in [t]} \bX_i} <  n/ 200$. By Markov's inequality, we have $\sum_{i \in [t]} \bX_i < n/20$ with probability at least $9/10$. Thus with probability at least $9/10$ over the draw of $\bD \sim \cD_0$ we land in a leaf vertex $v$ with $\theta(v) < n/20$, implying that $v \in \hat{\cE}_0$ as needed. 
\end{proof}

\begin{claim}\label{claim:lb4}
For any $v \in \hat{\cE_0}$, we have $  \pr{v \; | \; c=1} > (9/10 )\pr{v \; | \; c=0}$.
\end{claim}
\begin{proof}
Fix any $v \in \hat{\cE}_0$. By definition, when the algorithm concludes at $v$, at most $n/5$ indices were killed while having originally been light. Furthermore, since each heavy index requires by definition at least $k/2$ queries to become heavy, and since each query contributes to the heaviness of at most $2$ indices, it follows that at most $kn/400 (4/k) = n/100$ indices could ever have become heavy during any execution. Thus there are at least $n - n/20 - n/100 > (9/10)n$ indices $i$ which are both alive and light at $v$. 

Now fix any $h \in \cF_0(v)$. We show that $(h,i) \in \cF_1(v)$ for at least $(9/10)n$ indices $i \in [n]$, which will demonstrate that $|\cF_1(v)| > (9/10)  |\cF_0(v)|$, and thereby complete the proof. In particular, it will suffice to show that is true for any $i \in [n]$ which is alive at $v$. To see why this is the case, note that by definition if $i$ is alive at $v \in \cE_0$, then $S(v)$ includes only observations in the $i$-th row and column of $\cD$ which are equal to $R$. It follows that none of these observations would change if the input was instead specified by $(h,i,1)$ instead of $(h,j,0)$ for any $j \in [n]$, as the difference between the two resulting matrices are supported on values where $\cD$ is equal to $1$ in the $i$-th row and column of $\cD$. Thus if $i$ is alive at $v$, we have that $(h,i) \in \cF_1(v)$, which completes the proof of the claim.
\end{proof}
\noindent Putting together the bounds from Claims \ref{claim:lb3} and \ref{claim:lb4}, it follows that 
\[  \sum_{v \in \hat{\cE}_0} \pr{v\; |\;  c=1} > (9/10)^2 = .81 \]
Moreover, because by Claim \ref{claim:lb1} the algorithm always outputs $c=0$ when it ends in any $v \in \cE$, it follows that the algorithm incorrectly determined the value of $c$ with probability at least $.81$ conditioned on $c=1$, and therefore is incorrect with probability at least $.405 > 4/10$ which is a contradiction since $\cA$ was assumed to have success probability at least $6/10$. 

\paragraph{From the Hard Distribution to $k$-Centers.} 

To complete the proof, it suffices to demonstrate that the optimal $k$-center cost is at most $1$ when $\bD \sim \bD_0$, and at least $R$ when $\bD \sim \bD_1$. The first case is clear, since we can choose at least one index in the pre-image of $h^{-1}(t)\subseteq [n]$ for each $t \in [k]$ to be a center. For the second case, note that conditioned on $|h^{-1}(t)| \geq 2$ for all $t \in [k]$, the resulting metric contains $k+1$ points which are pairwise-distance at least $R$ from each other. In particular, for the resulting metric, at least one point must map to a center which it is distance at least $R$ away from, and therefore the cost is at least $R$. Now since $n = \Omega( k \log k)$ with a sufficently large constant, it follows by the coupon collector's argument that with probability at least $1/1000$, we have that $|h^{-1}(t)| \geq 2$ for all $t \in [k]$. Moreover, that the $1/1000$ probability under which does not occur can be subtracted into the failure probability of $.405$ in the earlier argument, which still results in a $.404 > 4/10$ failure probability, and therefore leads and leading to the same contradiction, which completes the proof. 
\end{proof}


\begin{proposition}\label{prop:lb}
Fix any $1 \leq k < n$. Then any algorithm which, given oracle access the distance matrix $\cD \in \R^n$ of a set $X$ of $n$ points in a metric space, determines correctly with probability $2/3$ whether the optimal $k$-center cost on $X$ is at most $1$ or at least $R$, for any value $R >1$, must make at least $\Omega(k^2)$ queries in expectation to $\cD$. The same bound holds true replacing the $k$-center objective with $(k,z)$-clustering, for any constant $z>0$. 
\end{proposition}
\begin{proof}
By the same arguements given in Theorem \ref{thm:LBBig}, one can assume that the existence of such an algorithm that would violate the statement of the proposition implies the existence of a deterministic algorithm which always makes at most $c k^2$ queries to $\cD$ and is correct with probability $3/5$, for some arbitrarily small constant $c$. 
In what follows, we assume $n=k+1$, and for larger $n$ we will simply add $n-(k+1)$ duplicate points on top of the first point in the following distribution; note that any algorithm for the dataset with the duplicate point can be simulated, with no increase in query complexity, via access to the distance matrix on the first $k+1$ points.

The hard instance is as then as follows.
With probability $1/2$, we give as input the distance matrix $\bD \in \R^{k+1 \times k+1} $ with $\bD_{i,j} =R $ for all $i \neq j$. Note that any $k$-center solution must have one of the points in a cluster centered at another, and therefore the optimal $k$-center cost is at least $R$ for this instance. In the second case, the input is $\bD$ but with a single entry $\bD_{i,j} = \bD_{j,i} = 1$, where $(i,j)$ is chosen uniformly at random. Note that the result is still a valid metric space in all cases. Moreover, note that the optimal $k$-center cost is $1$, and is obtained by choosing all points except $i$ (or alternatively except $j$). 

By the same (and in fact simplified) argument as in Claim \ref{claim:lb1}, we can assume the algorithm returns that the $k$-center cost is at most $1$ if and only if it sees an entry with value equal to $1$. Since the algorithm is deterministic, and since the only distance other than $1$ is $R$, we can define a deterministic set $S$ of $c k^2$ indices in $\binom{k+1}{2}$ such that the adaptive algorithm would choose exactly the set $S$ if, for every query it made, it observed the distance $R$ (and therefore would return that the $k$-center cost was at most $1$ at the end). Then in the second case, the probability that $(i,j)$ is contained in $S$ is at most $\frac{4}{c}$. Setting $c > 40$, it follows that the algorithm is incorrect with probability at least $1/2-\frac{4}{c} > 2/5$, contradicting the claimed success probability of $3/5$, and completing the proof. 
\end{proof}
