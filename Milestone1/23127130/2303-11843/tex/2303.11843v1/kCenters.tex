
\section{From Fully Dynamic $k$-Center to $k$-Bounded MIS} 
\label{sec:kCenters}

In this section, we describe our main results for fully dynamic $k$-center clustering, based on our main algorithmic contribution, which is presented in Section \ref{sec:generalMetric}. We begin by describing how
the problem of $k$-center clustering of $P$ can be reduced to maintaining a maximal independent set (MIS) in a graph. In particular, the reduction will only require us to solve a weaker version of MIS, where we need only return a MIS of size at most $k$, or an independent set of size at least $k+1$. Formally, this problem, which we refer to as the $k$-Bounded MIS problem, is defined as follows. 


\begin{definition}[$k$-Bounded MIS]
Given a graph $G = (V,E)$ and an integer $k \geq 1$, the $k$-bounded MIS problem is to output a maximal independent set $\cI \subset V$ of size at most $k$, or return an independent set $\cI \subset V$ of size at least $k+1$.%
\end{definition}



\paragraph{Reduction from $k$-center to $k$-Bounded MIS.}
The reduction from $k$-center to computing a maximum independent set in a graph is well-known, and can be attributed to the work of Hochbaum and Shmoys \cite{hochbaum1986unified}. The reduction was described in Section \ref{sec:tech}, however, both for completeness and so that it is clear that only a $k$-Bounded MIS is required for the reduction, we spell out the full details here.

Fix a set of points $X$ in a metric space, such that $r_{\min} \leq d(x,y) \leq  r_{\max}$ for all $x,y \in \cX$. Then, for each $r= r_{\min}, (1+\eps/2) r_{\min} , (1+\eps/2)^2 r_{\min}, \dots, r_{\max}$, one creates the \textit{$r$-threshold graph} $G_r = (V,E_r)$, which is defined as the graph with vertex set $V = X$, and $(x,y) \in E_r$ if and only if $d(x,y) \leq r$. One then runs an algorithm for $k$-Bounded MIS on each graph $G_r$, and finds the smallest value of $r$ such that the output of the algorithm $\cI_r$ on $G_r$ satisfies $|\cI_r| \leq k$ --- in other words, $\cI_r$ must be a MIS of size at most $k$ in $G_r$. Observe that $\cI_r$ yields a solution to the $k$-center problem with cost at most $r$, since each point in $X$ is either in $\cI_r$ or is at distance at most $r$ from a point in $\cI_r$. Furthermore, since the independent set $\cI_{r/(1+\eps/2)}$ returned from the algorithm run on $G_{r/(1+\eps/2)}$ satisfies $|\cI_{r/(1+\eps/2)}| \geq k+1$ it follows that there are $k+1$ points in $X$ which are pair-wise distance at least $r/(1+\eps/2)$ apart. Hence, the cost of any $k$-center solution (which must cluster two of these $k+1$ points together) is at least $r/(2+\eps)$ by the triangle inequality. It follows that $\cI$ yields a $2+\eps$ approximation of the optimal $k$-center cost.

Note that, in addition to maintaining the centers $\cI$, for the purposes of answering membership queries, one would also like to be able to return in $O(1)$ time, given any $x \in \cX$, a fixed $y \in \cI$ such that $d(x,y) \leq r$. We will ensure that our algorithms, whenever they return a MIS $\cI$ with size at most $k$, also maintain a mapping $\ell: V \setminus \cI \to \cI$ which maps any $x$, which is not a center, to its corresponding center $\ell(x)$. 

Observe that in the context of $k$-clustering, insertions and deletions of points correspond to insertions and deletions of entire vertices into the graph $G$. This is known as the fully dynamic \textit{vertex update model}. Since one vertex update can cause as many as $O(n)$ edge updates, we will not be able to read all of the edges inserted into the stream. Instead, we assume our dynamic graph algorithms can query for whether $(u,v)$ is an edge in constant time (i.e., constant time oracle access to the adjacency matrix).\footnote{This is equivalent to assuming that distances in the metric space can be computed in constant time, however if such distances require $\alpha$ time to compute, this will only increase the runtime of our algorithms by a factor of $\alpha$.}


Our algorithm for $k$-Bounded MIS will return a very particular type of MIS. Specifically, we will attempt to return the first $k+1$ vertices in a \textit{Lexicographically First MIS} (LFMIS), under a random lexicographical ordering of the vertices. 


\paragraph{Lexicographically First MIS (LFMIS).} The LFMIS of a graph $G = (V,E)$ according to a ranking of the vertices specified by a mapping $\pi:V \to [0,1]$ is a unique MIS defined as by the following process. Initially, every vertex $v \in V$ is alive. We then iteratively select the alive vertex with minimal rank $\pi(v)$, add it to the MIS, and then kill $v$ and all of its alive neighbors. We write $\LFMIS(G,\pi)$ to denote the LFMIS of $G$ under $\pi$. For each vertex $v$, we define the \textit{eliminator} of $v$, denoted $\elim_{G,\pi}(v)$ to be the vertex $u$ which kills $v$ in the above process; namely,  $\elim_{G,\pi}(v)$ is the vertex with smallest rank in the set $(N(v) \cup \{v\}) \cap \LFMIS(G,\pi)$. 


\begin{definition}\label{def:topkLFMIS}
Given a graph $G = (V,E)$, $\pi:V \to [0,1]$, and an integer $k \geq 1$, we define the top-$k$ LFMIS of $G$ with respect to $\pi$, denoted $\LFMIS_k(G,\pi)$ to be the set consisting of the first $\min\{k,|\LFMIS(G,\pi)|\}$ vertices in $\LFMIS(G,\pi)$ (where the ordering is with respect to $\pi$). When $G,\pi$ are given by context, we simply write $\LFMIS_k$. 
\end{definition}

It is clear that returning a top-$(k+1)$ LFMIS of $G$ with respect to any ordering will solve the $k$-Bounded MIS problem. In order to also obtain a mapping $\ell$ from points to their centers in the independent set, we define the following augmented version of the top-$k$ LFMIS problem, which we refer to as a \textit{top-$k$ LFMIS with leaders}.


\begin{definition}\label{def:LFMISLead}
A top-$k$ LFMIS with leaders consists of the set $\LFMIS_k(G,\pi)$, along with a \textit{leader mapping function} $\ell:V \to V \cup \{\bot\}$, such that $(v,\ell(v)) \in E$ whenever $\ell(v) \neq \bot$, and such that if $\LFMIS_k(G,\pi) = \LFMIS(G,\pi)$, then $\ell(v) \in \LFMIS_k(G,\pi)$ for all $v \in V \setminus \LFMIS_k(G,\pi)$, and $\ell(v) = \bot$ for all $v \in \LFMIS_k(G,\pi)$. 
\end{definition}

Within our algorithm, the event that $\ell(v) = \bot$ will occur only if $v$ itself is a leader in $\LFMIS_{k}$, or if $v$ is among a set of ``unclustered'' points whose leader mapping to a point in $\LFMIS$ may be out of date. We choose to set $\ell(v) = \bot$ when $v \in \LFMIS_k$ is a leader, rather than setting $\ell(v) = v$, to maintain the invariant that $(v,\ell(v)) \in E$ whenever $\ell(v) \neq \bot$. 

The main goal of the following Section \ref{sec:generalMetric} will be to prove the existence of a $\tilde{O}(k)$ amortized update time algorithm for maintaining a top-$k$ LFMIS with leaders of a graph $G$ under a fully dynamic sequence of insertions and deletions of vertices from $G$. Specifically, we will prove the following theorem.



\noindent \textbf{Theorem} \ref{thm:LFMISMain}. {\it There is a algorithm which, on a fully dynamic stream of insertions and deletions of vertices to a graph $G$, maintains at all time steps a top-$k$ LFMIS of $G$ with leaders under a random ranking $\pi: V \to [0,1]$. The expected amortized per-update time of the algorithm is $O(k \log n + \log^2 n)$, where $n$ is the maximum number active of vertices at any time. Moreover, the algorithm does not need to know $n$ in advance. }

Next, we demonstrate how Theorem \ref{thm:LFMISMain} immediately implies the main result of this work (Theorem \ref{thm:main}). Firstly, we prove a proposition which demonstrates that any fully dynamic Las Vegas algorithm with small runtime in expectation can be converted into a fully dynamic algorithm with small runtime with high probability. 


\begin{proposition}\label{prop:highProb}
Let $\cA$ be any fully dynamic randomized algorithm that correctly maintains a solution for a problem $\cP$ at all time steps, and runs in amortized time at most $\Run(\cA)$ in expectation. Then there is a fully dynamic algorithm for $\cP$ which runs in amortized time at most $O(\Run(\cA) \log\delta^{-1})$ with probability $1-\delta$ for all $\delta \in (0,1/2)$.
\end{proposition}
\begin{proof}
The algorithm is as follows: we maintain at all time steps a single instance of $\cA$ running on the dynamic stream. If, whenever the current time step is $t$, the total runtime of the algorithm exceeds $4 t \Run(\cA)$, we delete $\cA$ and re-instantiate it with fresh randomness. We then run the re-instantiated version of $\cA$ from the beginning of the stream until either we reach the current time step $t$, or the total runtime again exceeds $4 t \Run(\cA)$, in which case we re-instantiate again. 

Let $M$ be the total length of the stream. For each $i=0,1,2,\dots,\lceil \log M \rceil$, let $\bZ_i$ be the number of times that $\cA$ is re-instantiated while the current time step is between $2^i$ and $2^{i+1}$. Note that the total runtime is then at most 
\[ 4 \Run(\cA) \cdot \left(M +  \sum_{i=0}^{\lceil \log M \rceil} 2^{i+1} \bZ_i \right) \]
Fix any $i \in \{0,1,\dots,\lceil \log M \rceil\}$, and let us bound the value $\bZ_i$. 
Each time that $\cA$ is restarted when the current time $t$ step is between $2^i$ and $2^{i+1}$, the probability that the new algorithm runs in time more than $2^{i+2}\Run(\cA) \geq 4  t \Run(\cA) $ on the first $2^{i+1}$ updates is at most $1/2$ by Markov's inequality. Thu, the probability that $\bZ_i > T_i + 1$ is at most $2^{-T_i}$, for any $T_i \geq 0$. Setting $T_i = \log(2/\delta) + \lceil \log M \rceil - i $, we have
\begin{equation}
    \begin{split}
        \sum_{i=0}^{\lceil \log M \rceil} \pr{\bZ_i > T_i + 1 }& \leq \frac{\delta}{2}   \sum_{i=0}^{\lceil \log M \rceil} \left(\frac{1}{2}\right)^{\lceil \log M \rceil - i} \\ 
        & < \delta
    \end{split}
\end{equation}
In other words, by a union bound, we have $\bZ_i \leq T_i+1$ for all $i=0,1,2,\dots,\lceil \log M \rceil$, with probability at least $1-\delta$. Conditioned on this, the total runtime is at most 

\begin{equation}
    \begin{split}
         4 \Run(\cA) \cdot \left(M +  \sum_{i=0}^{\lceil \log M \rceil} 2^{i+1} (T_i+1) \right) &= O\left(\Run(\cA)\left( \sum_{i=0}^{\lceil \log M \rceil} 2^{i} (  \log \delta^{-1} + \lceil \log M \rceil - i )  \right)\right) \\
         & = O\left(\Run(\cA)\left( M \log \delta^{-1} + M \sum_{i=0}^{\lceil \log M \rceil} \frac{i }{2^{i}}  \right)\right) \\
          & = O\left(\Run(\cA)\cdot  M \log \delta^{-1} \right) \\
    \end{split}
\end{equation}
which is the desired total runtime.
\end{proof}

Given Theorem \ref{thm:LFMISMain}, along with the reduction to top-$k$ LFMIS from $k$-center described in this section, which runs $O(\eps^{-1} \log \Delta)$ copies of a top-$k$ LFMIS algorithm, we immediately obtain a fully dynamic $k$-center algorithm with $\tilde{O}(k)$ expected amortized update time. By then applying Proposition \ref{prop:highProb}, we obtain our main theorem, stated below. 

	\noindent \textbf{Theorem} \ref{thm:main}. {\it 
    There is a fully dynamic algorithm which, on a sequence of insertions and deletions of points from a metric space $\cX$, maintains a $(2+\eps)$-approximation to the optimal $k$-center clustering. The amortized update time of the algorithm is $O(\frac{\log \Delta \log n}{\eps}(k  + \log n))$ in expectation, and $O(\frac{\log \Delta \log n}{\eps}(k  + \log n) \log \delta^{-1})$  with probability $1-\delta$ for any $\delta \in (0,\frac{1}{2})$, where $n$ is the maximum number of active points at any time step. 
    
    The algorithm can answer membership queries in $O(1)$-time, and enumerate over a cluster $C$ in time $O(|C_i|)$. 
}
