
 
 

This supplemental material presents more details of our proposed CLC.
Appendix~ \ref{sec:mmc} describes the Multi-modality Sample Cleaning.
Appendix~\ref{sec:vis} lists some visualization examples and qualitative results in MoiveLights.
Finally, we present the broader impact in Appendix~\ref{sec:Broader_Impact}.


% \vspace{1cm}

\section{Multi-modality Sample Cleaning}
\label{sec:mmc}


The detailed procedure of our MMC is presented in Algorithm~\ref{alg:MMC}.
The MMC module contains three branches, \emph{i.e.,} multi-modal branch $G^{MM}$, visual branch $G^{{UM}_v}$, and audio branch $G^{{UM}_a}$.
First, all instances in the two uni-modal branches are fed into the network to obtain the uni-modal losses, \emph{i.e.,} $\mathcal{L}^{UM_v}$ and $\mathcal{L}^{UM_a}$.
Next, we select a proportion of instances $N^{v}$ and $N^{a}$ that have small training losses in each branch independently. The number of instances is controlled by $\tau$.
Then, we combine the selected samples $N^{v}$ and $N^{a}$ and take them as clean samples to train the multi-modal branch. Assume that the multi-modal loss is denoted as $\mathcal{L}^{MM}$.
Finally, we aggregate all the losses in three branches to update the network parameters.
Through joint optimization, the multi-modal branch progressively attains more trustable labels that make learning more robust.






% {
%   \renewcommand{\thefootnote}%
%     {\fnsymbol{footnote}}
%   \footnotetext[1]{Corresponding author.}
% }

%%%%%%%%% BODY TEXT
%-------------------------------------------------------------------------
 


% This supplemental document reports details on our proposed CLC.
% Appendix~ \ref{sec:mmc} describe the details of Multi-modality Sample Cleaning.
% In Appendix~\ref{sec:vis}, we list some visualization examples and qualitative results in MoiveLights.
% Finally, we present the broader impact in Appendix~\ref{sec:Broader_Impact}.


%-------------------------------------------------------------------------




% The detailed procedure of MMC is presented in Algorithm~\ref{alg:MMC}.
% We maintain multiple models \emph{i.e.,} a multi-modal model $G^{MM}$, two uni-modal models $G^{{UM}_v}$ and $G^{{UM}_a}$.
% For each uni-modal model,  all instances in the batch are fed into the network to obtain the uni-modal loss $\mathcal{L}^{UM_v}$ and $\mathcal{L}^{UM_a}$.
% We select a proportion of instances $N^{v}$ and $N^{a}$ in each modality independently that have small training loss; the number of instances is controlled by $\tau$.
% We take the union of $N^{v}$ and $N^{a}$ as the clean samples and calculate the multi-modal loss $\mathcal{L}^{MM}$ on these samples for further backpropagation.
% After obtaining the multiple losses,  we calculate the average loss in each branch for update parameters, respectively.
% By the joint optimization, the multi-modal model progressively attains more trustable labels that make learning more robust.




% \begin{figure*}[h]
%   \begin{center}
%   \includegraphics[width=1\linewidth]{./figures/vis_badcase_v3.pdf}
%   \end{center}
%       \vspace{-0.5cm}
%       \caption{
%          badcase}
%       \vspace{-0.5cm}
%   \label{fig:badcase}
% \end{figure*}



\begin{figure*}[h]
   \begin{center}
   \includegraphics[width=1\linewidth]{./figures/vis_com_v3.pdf}
   \end{center}
      \vspace{-0.5cm}
      \caption{Qualitative results. Prediction curve on MoivieLights detected by CLC and UMT. 
         }
      \vspace{-0.5cm}
   \label{fig:compare}
\end{figure*}

\begin{figure*}[h]
   \begin{center}
   \includegraphics[width=1\linewidth]{./figures/vis_badcase_v4.pdf}
   \end{center}
      \vspace{-0.5cm}
      \caption{
         Badcase on MoiveLights}
      \vspace{-0.5cm}
   \label{fig:badcase}
\end{figure*}

\section{Visualization}
\label{sec:vis}

% \vspace{-1cm}

As shown in Fig.~\ref{fig:summary}, we present a summary to introduce our MovieLights.
As seen in Fig.~\ref{fig:compare}, the predicted scores detected by CLC and UMT~\cite{UMTCVPR2022}  are shown by lines.
The results show that in different highlightness clips, the prediction score of UMT~\cite{UMTCVPR2022}  has a slight change, while the highlight moments and background scenes can be well distinguished by our CLC.
It indicates that our CLC is better at understanding movies and is more discriminative for highlights.
Fig.~\ref{fig:badcase} presents the badcase on MovieLights.
It incorrectly localized a highlight moment where one of the annotators regards it as a highlight and the other doesn't.
Since the subjectivity of the highlight detection, some highlights moment we detect maybe attract only a part of the audience.
It reflects the challenge of video highlight detection, and we hope to find better solutions in future work.
% \vspace{2cm}

% In addition, we provide several highlight videos in supplementary material detected by CLC for a better understanding.

%-------------------------------------------------------------------------

\section{Broader Impact} 
\label{sec:Broader_Impact}
With the growing number of new publications of movies and the rapid rise of short videos, it is necessary to train automatic movie highlight detection algorithms.
This work provides a scene-aware paradigm to learn highlight moments in movies without any manual annotation.
Besides, our  introduce a framework named Collaborative noisy Label Cleaner (CLC) to learn from these pseudo noisy labels. 
Finally, the collected dataset MovieLights could foster the further study of movie analysis.
The potential negative impact lies in that this dataset may be abused and may cause copyright issues. 
Hence, to avoid privacy and copyright issues, trailers and movies will be released in the form of extracted features in visual and audio modalities. 
If actual business data needs to be applied, it should be regulated and consented to by media providers.


