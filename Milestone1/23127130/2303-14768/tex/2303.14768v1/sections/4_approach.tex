% !TEX root = ../main.tex

\section{Approach}
\label{sec:approch}


\subsection{Overview}
\label{subsec:Overview.}



\begin{figure*}[t]
   \begin{center}
   \includegraphics[width=1.0\linewidth]{./figures/overview_v7.pdf}
   \end{center}
      \vspace{-0.5cm}
      \caption{
         Overview of the proposed CLC. It includes three modules: feature extraction, augmented cross-propagation (ACP), and multi-modal cleaning (MMC). The visual and audio modalities of the input video are represented as vectors by the feature extraction module. Then the features are augmented by ACP module to capture semantic associations acorss modalities. MMC is used to filter outs noisy and incomplete labeling with additional uni-modal branches. During inference, we remove the uni-modal branches and only rely on the prediction of multi-modal branch. More details of the CLC are shown in Sec.~\ref{subsec:Overview.}}
      \vspace{-0.5cm}
   \label{fig:pipeline}
\end{figure*}



The overall architecture of our Collaborative noisy Label Cleaner (CLC) framework is illustrated in Fig.~\ref{fig:pipeline}, which includes three modules: feature extraction, augmented cross-propagation (ACP), and multi-modal cleaning (MMC). 

In our framework, both the visual and audio modalities are utilized. 
% For feature extraction and encoding, we first split the video $V$ into shots, resulting in $N$ shots. We characterize the $i^{th}$ shots by two vectors, \emph{i.e.,} $\mathbf{v}_i$ for the visual features, and $\mathbf{a}_i^r$ for the audio features, where $i = 1, 2...n$. These features are extracted using pre-trained visual~\cite{dosovitskiy2020image} and audio feature extractors~\cite{kong2020panns}. Parameters of the two extractors are frozen during training.
For feature extraction and encoding, we first split the video $V$ into $T$ shots. We characterize the $i^{th}$ shots by two vectors, \emph{i.e.,} $\mathbf{v}_i$ for the visual features, and $\mathbf{\hat{a}_i}$ for the audio features, where $i = 1, 2, ..., T$. These features are extracted using pre-trained visual~\cite{dosovitskiy2020image} and audio feature extractors~\cite{kong2020panns}. The parameters of the two extractors are frozen during training. % modified by rqiao 1111

% The two modules, \emph{i.e.,} ACP and MMC, are the core components of our framework. Since the visual-audio signals in videos are closely related but not always contribute to highlight detection, the ACP module exploits the relationship via uni-modal and cross-modal interactions. Then, it learns unified multi-modal representations for highlight detection. As the obtained highlight moments after scene segmentation are noisy, the MMC module firstly observes the uni-modal loss changes, then filters the noisy labels and utilizes the clean ones for multi-modal supervised learning. Next, we will provide details for each component.
%by shuxj 1106
The ACP and MMC are the core components of our framework. Since the visual-audio signals in videos are closely related but do not always contribute to highlight detection, the ACP module exploits the relationship via uni-modal and cross-modal interactions. Then, it learns unified multi-modal representations for highlight detection. As the obtained highlight moments after scene segmentation are noisy, the MMC module firstly observes the changes in uni-modal losses, then filters the noisy labels and utilizes the clean ones for multi-modal supervised learning. Next, we will introduce details for each component.

% For the augmented cross-propagation module, the self-attention layer captures interactions among clips of the same modality, and bimodal attention captures the interplay between the two modalities.
% Then, a cross-modal similarity matrix is able to augment that positive relation of audio-visual clip pairs and the augmented audio-visual features after fused.

% We introduce a Bi-LSTM[]-based structure to model representations of longer shot which is able to preserve long-range dependencies while being sensitive to short-term changes.
% It takes a sequence of shot features encoded by augmented cross-propagation module as input and each time step corresponds to a shot in the movie sequence.
% Given shot features sequence, it produces a score si for the i-th shots, indicating whether this clip is a highlight or not.


% In the training stage, for uni-modal and multi-mode, three Bi-LSTM is utilized to encode temporal relations in video clips respectively. 
% To reduce the gap of heterogeneity between the two modalities, we use a consistency loss to further enhance the model capability.

% Due to annotation noise being inevitable in highlight detection, we introduce the dynamic sample selection method named multi-modality noisy label cleaner to adopt the spirit of providing clean data for the training which is detailed in Sec.~\ref{subsec:Multi-modality Noisy Label Cleaner.}.

% Finally, a median filtering for post-processing is used to smooth the highlight curve.



% \subsection{Multi-modality Filter for Noisy Labels.}
% \label{subsec:Multi-modality Filter for Noisy Labels.}
\subsection{Augmented Cross-Propagation}
\label{subsec:Augmented Cross-Propagation.}
\noindent\textbf{Cross-Propagation.} 
To predict the highlights, the model needs to understand the storylines of the movie. Meanwhile, visual and audio inputs do not always contribute to accurate prediction. Therefore, temporal modeling and modality interaction are the keys to achieving successful highlight detection. To achieve this goal, we design our ACP module to fuse the visual-audio modalities. It involves three steps in total.
% integrates augmented self-attention and cross-attention 
%by shuxj 1107 

% To reliably fuse visual feature $\mathbf{v}_i$ and audio feature $\mathbf{a}_i$ , we develop the ACP module to learn more representative features by exploiting the similarities of the two modalities. It involves three steps.

% To reliably fuse the visual feature $\mathbf{v}_i$ and audio feature $\mathbf{a}_i$ , we develop a new fusion module which uses a self-attention network and a cross-attention network to learn more representative features and exploite the similarities of the two modalities to enhance features.
% It involves three steps.

% First, in order to implicitly enforce features into a similar latent space, we introduce a fully-connected (FC) layer to further project the audio feature.
% First, in order to align the multi-modal features, we introduce $f^a$ which is fully-connected (FC) layer with ReLU to the audio feature such that it has same dimension as the video feature. % modified by rqiao
First, in order to align the multi-modal features, we introduce $h$ which is fully-connected (FC) layer with ReLU to the $\mathbf{\hat{a}_i}$ such that it has same dimension as the $\mathbf{v}_i$.
\begin{equation}
\mathbf{a}_i=h(\mathbf{\hat{a}_i}).
\end{equation}

% Movies tend to contain continuous rather than isolated audio and visual clips.
Movies are composed of consecutive audio-visual clips. % modified by rqiao 1111
% A sparse shot is often inadequate to measure whether it is a highlight, and it is necessary to consider the relationship of the shot with other adjacent shots.
Therefore, to measure the highlighted ness of a given shot, one must consider the relationship of the shot with its adjacent shots. % modified by rqiao 1111
% The self-attention mechanism has been shown effective in capturing the dependencies in previous work.
The self-attention mechanism has shown effectiveness in capturing the long-term dependencies in previous works. % modified by rqiao
% For each clip, we leverage self-attention to capture the temporal relationship for both $\mathbf{v}_i$ and $\mathbf{a}_i$. 
We leverage self-attention to capture the temporal relationship for $\mathbf{v}_i$ and $\mathbf{a}_i$ via Eq.~\ref{eq:self-attention-v} and Eq.~\ref{eq:self-attention-a}, respectively. 
% \begin{align}
% \mathbf{v}_i^s = \mathbf{x}^\text{major} + \pmb{\delta}.
% \label{eq:self-attention}
% \end{align}
% \begin{equation}
% \mathbf{v}_{i}^{s}=\sum_{i=1}^{T} w_{i}^{sv} \mathbf{v}_{i}=softmax\left(\frac{\mathbf{v}_{i} \mathbf{v}^{\top}}{\sqrt{d}}\right) \mathbf{v},
% \label{eq:self-attention-v}
% \end{equation}
% \begin{equation}
% \mathbf{a}_{i}^{s}=\sum_{i=1}^{T} w_{i}^{sa} \mathbf{a}_{i}=softmax\left(\frac{\mathbf{a}_{i} \mathbf{a}^{\top}}{\sqrt{d}}\right) \mathbf{a},
% \label{eq:self-attention-a}
% \end{equation}
\begin{equation}
% \mathbf{v}_{i}^{s}=softmax\left(\frac{\mathbf{v}_{i} \mathbf{v}^{\top}}{\sqrt{d}}\right) \mathbf{v},
\mathbf{v}_{i}^{s}=softmax\left(\frac{(\mathbf{v}_{i}{W}_{1}^{v})( \mathbf{v}{W}_{2}^{v})^{\top}}{\sqrt{d}}\right) (\mathbf{v}{W}_{3}^{v}),
\label{eq:self-attention-v}
\end{equation}
\begin{equation}
% \mathbf{a}_{i}^{s}=softmax\left(\frac{\mathbf{a}_{i} \mathbf{a}^{\top}}{\sqrt{d}}\right) \mathbf{a},
\mathbf{a}_{i}^{s}=softmax\left(\frac{(\mathbf{a}_{i}{W}_{1}^{a})( \mathbf{a}{W}_{2}^{a})^{\top}}{\sqrt{d}}\right) (\mathbf{a}{W}_{3}^{a}),
\label{eq:self-attention-a}
\end{equation}
where $\mathbf{v}=[\mathbf{v}_{1};\mathbf{v}_{2};...;\mathbf{v}_{T}]$ and $\mathbf{a}=[\mathbf{a}_{1};\mathbf{a}_{2};...;\mathbf{a}_{T}]$;
% $w_{i}^{sv}$ and $w_{i}^{sa}$ are learnable parameters;
the scaling factor $d$ is equal to the visual/audio feature dimension and $(*)^{\top}$ denotes the transpose operator;
$W^v$ and $W^a$ are learnable matrices of two modalities, which are implemented by a linear layer.
Uni-modal self-attention can well capture uni-modal temporal contexts and enhance clip features within the same modality.

% However, the self-attention only captures the clip interactions within the uni modality. 
% To capture the interactions across modalities, we introduce cross-attention to compute the query from one modality, and the key and value from the other modality.
%Despite the above self-attention capturing the clip interactions within the uni modality, it is critical to capture the interactions cross modalities.
Despite the above self-attention capturing the clip interactions within the uni-modality, it is critical to capture the interactions across modalities. % modified by rqiao 1111
% To capture semantic associations based on these multi-modal signals, we introduce cross-attention to compute the query from one modality, and the key and value from the other modality.
% To capture semantic associations based on multi-modal signals, we introduce cross-attention to compute the query from one modality, as well as the key and value from the other modality.
To capture semantic associations based on multi-modal signals, we introduce cross-attention to update the features of each modality.
% modified by rqiao 1111
% \begin{equation}
% \mathbf{v}_{i}^{c}=\sum_{i=1}^{T} w_{i}^{cv} \mathbf{a}_{i}=softmax\left(\frac{\mathbf{v}_{i} \mathbf{a}^{\top}}{\sqrt{d}}\right) \mathbf{a},
% \label{eq:cross-attention-v}
% \end{equation}
% \begin{equation}
% \mathbf{a}_{i}^{c}=\sum_{i=1}^{T} w_{i}^{ca} \mathbf{v}_{i}=softmax\left(\frac{\mathbf{a}_{i} \mathbf{v}^{\top}}{\sqrt{d}}\right) \mathbf{v}.
% \label{eq:cross-attention-a}
% \end{equation}
\begin{equation}
% \mathbf{v}_{i}^{c}=softmax\left(\frac{\mathbf{v}_{i} \mathbf{a}^{\top}}{\sqrt{d}}\right) \mathbf{a},
\mathbf{v}_{i}^{c}=softmax\left(\frac{(\mathbf{v}_{i}{W}_{4}^{v}) (\mathbf{a}{W}_{4}^{a})^{\top}}{\sqrt{d}}\right) (\mathbf{a}{W}_{5}^{a}),
\label{eq:cross-attention-v}
\end{equation}
\begin{equation}
% \mathbf{a}_{i}^{c}=softmax\left(\frac{\mathbf{a}_{i} \mathbf{v}^{\top}}{\sqrt{d}}\right) \mathbf{v}.
\mathbf{a}_{i}^{c}=softmax\left(\frac{(\mathbf{a}_{i}{W}_{6}^{a}) (\mathbf{v}{W}_{5}^{v})^{\top}}{\sqrt{d}}\right) (\mathbf{v}{W}_{6}^{v}),
\label{eq:cross-attention-a}
\end{equation}

% Then, we augment the relevant positive connections and dampen the irrelevant connections by cross-correlation matrix $\mathbf{c}$:
% Through cross-attention, the information from two modalities influences each other.
Through cross-attention, the information from two modalities are connected. % modified by rqiao 1111
However, considering audio-visual temporal asynchrony, it is necessary to select effective information from multi-modality. % modified by rqiao 1111
We augment the relevant positive connections and dampen the irrelevant connections.
The strength of these connections is measured by the cross-correlation matrix, computed by,
\begin{equation}
\mathbf{c}^v=ReLU\left(\frac{\mathbf{v}\mathbf{a}^{\top}}{\sqrt{d}}\right),
\mathbf{c}^a=ReLU\left(\frac{\mathbf{a}\mathbf{v}^{\top}}{\sqrt{d}}\right).
\label{eq:cross-correlation}
\end{equation}


% Specifically, a cross-correlation matrix is used to measure the similarity of different modal segments.
% The i-th column of $\mathbf{c}$ corresponds to the relevance of x $\mathbf{v}_i^a$ and  audio features $\mathbf{a}_1^a$ to $\mathbf{a}_n^a$.
% Then, for each modality, the cross-correlation matrix $\mathbf{c}$ is used to re-weight the cross-attention features by dot product operation.
% Clearly, the cross-correlation matrix will assign large weights to clips which are similar to the the other modality and dampen the irrelevant modality.
% For each modality, the cross-correlation matrix $\mathbf{c}$ is used to re-weight the cross-attention features.
% \begin{equation}
% \mathbf{v}_i^a=\sum_{j=1}^{N} c_{ij}^{v} v_{i}
% \end{equation}
% Finally, we add the original features, cross-modal features, and enhanced features and fuse them via the  fully-connected (FC) layers  to obtain augmented features.
% Finally, we obtain updated visual features $\mathbf{v}_i^a$ and audio features $\mathbf{a}_i^a$ by adding the original features, enhanced uni-modal features, and cross-modal features and fuse them via the fully-connected (FC) layers.

For each modality, the cross-correlation matrix $\mathbf{c}$ is used to re-weight the cross-attention features.
Finally, we obtain updated visual features $\mathbf{\bar{{v}_i}}$ and audio features $\mathbf{\bar{{a}_i}}$ by fusion of the original features, enhanced uni-modal features, and cross-modal features.
% \todo{formula}
% $$v_{i}$$
% $\mathbf{\tilde{v}}_i^s$
% \begin{equation}
% \mathbf{v}_i^a=f(\mathbf{v}_i) + f^s(\mathbf{v}_i^s) + f^c((\sum_{j=1}^{N} \mathbf{c}_{ij}^{v}) * \mathbf{v}_i^c),
% \end{equation}
% \begin{equation}
% \mathbf{a}_i^a=f(\mathbf{a}_i) + f^s(\mathbf{a}_i^s) + f^c((\sum_{j=1}^{N} \mathbf{c}_{ij}^{a}) * \mathbf{a}_i^c),
% \end{equation}
\begin{equation}
\mathbf{\bar{{v}_i}}=f(\mathbf{v}_i, \mathbf{v}_i^s, (\sum_{j=1}^{T} \mathbf{c}_{ij}^{v}) * \mathbf{v}_i^c),
\end{equation}
\begin{equation}
\mathbf{\bar{{a}_i}}=f(\mathbf{a}_i, \mathbf{a}_i^s, (\sum_{j=1}^{T} \mathbf{c}_{ij}^{a}) * \mathbf{a}_i^c),
\end{equation}
% 
% where $f$, $f^s$ and $f^c$ are FC layers, respectively, and the FC layer is used to further project the features.
% where $f$, $f^s$ and $f^c$ are FC layers with ReLU to further project the features. % modified by rqiao
where $f$ is the fusion function consisting of FC layers and ReLU to further project the features. 

% Clearly, the cross-correlation matrix will assign large weights to clips that are corresponding to the other modality.
Clearly, the cross-correlation matrix will assign large weights to clips that are relevant to the other modality. % rqiao 1111
% Here the weight, the i-th column of the matrix corresponds to the relevance of $\mathbf{v}_i^a$ and all audio features which from $\mathbf{a}_1^a$ to $\mathbf{a}_n^a$.
% In addition, the ReLU activation function cut off connections with negative similarity values and only relevant positive connections would be collected.
In addition, the ReLU activation in 
% \eqref{eq:cross-correlation} 
Eq.~\ref{eq:cross-correlation}
cuts off connections with negative similarity values and only relevant positive connections would be preserved. % rqiao
% By the adding operation, the magnitude and direction of vectors representing the original feature will be changed and contain rich information.
% By the above operations, the vectors representing the original feature will be infused with richer information. % modified by rqiao 1111
By the above operations, the original feature will be infused with richer information. % modified by rqiao 1111
% Here, the FC layer can implicitly enforce features into a similar latent space.
% Clearly, the cross-correlation matrix will assign large weights to clips which are similar to the the other modality and selectively choose effective information from multi-modality.
% Clearly, the cross-correlation matrix will assign large weights to clips that are similar to the other modality, and the ReLU activation function cut off connections with negative similarity values.


% \begin{equation}
% \mathbf{v}_i^a=\mathbf{v}_i + \mathbf{v}_i^s + \left(\sum_{j=1}^{N} \mathbf{c}_{ij}^{v}\right) * \mathbf{v}_i^c
% \end{equation}

% $\mathbf{v}_i$
% $\mathbf{a}_i$
% $\mathbf{v}_i^s$
% $\mathbf{a}_i^s$
% $\mathbf{v}_i^c$
% $\mathbf{a}_i^c$
% $\mathbf{v}_i^a$
% $\mathbf{a}_i^a$

\noindent\textbf{Consistency Loss.} 
% Multiple input modalities help to comprehensively learn by integrating different senses and boosting model performance.
% Actually, they are not enough exploited because some modal specifies feature that may be weakened in the fusion even when the multimodal model outperforms its uni-modal respectively.
% As shown in Fig.~\ref{fig:pipeline}, we provide a additional branch for each mode separately to obtain uni-mode prediction during training.
% Owe to multi-branch inference, all modal information can be fully expressed, but it will also increase the amount of calculation.
% To address the issue, only intermediate fusion branches are enabled during inference and an auxiliary consistency loss is used to guide and regularize fusion features. 
% Specifically, we propose an approach that augments the prediction objective with a notion of modality consistency.
% We consider a prediction consistent if the same prediction is made given the different mode features.
% The consistency loss is defined as the cross-entropy between the fusion feature distribution $\mathbf{y}_i$ and uni-modal  feature $\mathbf{y}_i^u$ :
% \begin{equation}
% \mathcal{L}_{\text {cons }}=-\sum_{u=1}^M \sum_{i=1}^N \sum_{c=1}^C y_i^u \log {y}_i
% \end{equation}
% where ${y}_i$ denotes the predicted probability of the $i^{th}$ clip over the class $c$ and $M$ is the number of modal.
% The consistency loss not only implicitly enhances uni-modal information, but also explicitly guides the fusion branch to better learning.
%Multiple input help to comprehensively learn by integrating different senses and boosting model performance.
Multi-modal inputs help to comprehensively learn by integrating different aspects and boosting model performance. % rqiao
% Actually, they are not enough exploited because some modality-specific features may be weakened in the fusion even when the multi-modal model outperforms its uni-modal respectively.
However, they are not fully exploited because some modality-specific features may be weakened in the fusion even when the multi-modal model outperforms its uni-modal counterpart. % rqiao 1111
% In this work, we provide parallel branches for each mode separately to obtain uni-mode prediction during training.
In this work, we provide parallel branches for each modality separately to obtain uni-modal prediction during training. % rqiao 1111
As seen in Fig.~\ref{fig:pipeline}, all branches share the same clip feature extracted from ACP and the feature is fed into different branches independently. In each branch, we develop a temporal model $G$ to obtain its prediction score $\mathbf{y}_i$ of the $i^{th}$ clip being a highlight as follows:
\begin{equation}
\mathbf{y}_i^{MM}=G(\mathbf{\bar{{v}_i}},\mathbf{\bar{{a}_i}}),\mathbf{y}_i^{UM_v}=G(\mathbf{\bar{{v}_i}}),\mathbf{y}_i^{UM_a}=G(\mathbf{\bar{{a}_i}}).
\end{equation}
% where $L$ is the $\mathbf{y}_i$ indicating the probability of the i-th clip being a highlight.
%However, the gap of heterogeneity between the different modalities would bring instability in the joint optimization process.
where ${MM}$ is multi-modal branch; ${UM_v}$ is visual branch and ${UM_a}$ is audio branch.

However, the gap between the different modalities would bring instability in the joint optimization process. % rqiao 1111
We employ the auxiliary consistency loss to guarantee consistency between the different modalities.
% Given the multi-modal features, it is consistent if the prediction is the same.
Given the multi-modal features of a clip, they are consistent if they share the same prediction. % rqiao
Specifically, the consistency loss is defined as the cross-entropy between the multi-modal prediction probability $\mathbf{y}_i^{MM}$ and uni-modal prediction probability $\mathbf{y}_i^{UM}$ :
% \begin{equation}
% \mathcal{L}_{\text {cons }}=-\sum_{u=1}^M \sum_{i=1}^N \sum_{c=1}^C \mathbf{y}_i^{um} \log \mathbf{y}_i^{mm},
% \end{equation}
\begin{equation}
\footnotesize
\mathcal{L}_{\text {cons }}=-\left(\sum_{i=1}^N \mathbf{y}_i^{UM_v} \log \mathbf{y}_i^{MM} + \sum_{i=1}^N \mathbf{y}_i^{UM_a} \log \mathbf{y}_i^{MM}\right),
\end{equation}
% 
% where ${y}_i$ denotes the predicted probability of the $i^{th}$ clip over the class $c$ and $M$ is the number of modality.
where $N$ is the number of samples in a batch.

% This consistency loss not only implicitly enhances uni-modal information, but also explicitly guides the multi-modal branch to better learning.
This consistency loss not only implicitly enhances uni-modal information, but also explicitly guides the multi-modal branch to robuster supervision. % rqiao 1111
% The predictions from different branches will fit an approximate distribution, leading to a boosted robustness and generalization performance.
%Owe to multi-branch training, all information can be fully expressed, but it will also increase the calculation.
%To address the issue, the addition branches are applied in training while only multi-modal branches are enabled during inference.
The added uni-modal branches are only utilized in the training phase and are disabled during the inference stage. % rqiao 1111

% \subsection{Across Modality Attention Fusion Mechanic.}
% \label{subsec:Across Modality Attention Fusion Mechanic.}

% \subsection{Collaborative Noise Filtering.}
% \label{subsec:Collaborative Noise Filtering.}

% \subsection{Auxiliary Consistency Loss.}
% \label{subsec:Auxiliary Consistency Loss.}

% \subsection{Consistency Regression.}
% \label{subsec:Consistency Regression.}


\subsection{Multi-modal Cleaning}
\label{subsec:Multi-modality Noisy Label Cleaner.}

\noindent\textbf{Multi-modality Sample Cleaning.} 
% Annotation noise is inevitable in highlight detection and the start and end of a highlight moment can vary significantly depending on the users.
% For example, the audience1 might be interested in a loose moment with a few minutes included in its edited version. In contrast, audience2 might be interested in a tight moment with a few seconds.
% 
% In this paper, we argue that highlight detection should be regarded as Learning with Noisy Labels (LNL).
% From the perspective of the sample, the core idea for learning with noisy labels is to perform sample re-weighting or sample selection. 
% Sample selection methods primarily seek to distinguish clean samples and noise samples.
% Deeming low-loss instances as clean samples has been widely adopted in tackling label noise and we follow state-of-the-art methods regarding high-loss instances as noisy samples in each individual.
% 
% More specifically, we introduce a noise filter method to adaptively discount noisy samples with noisy visual or audio modality information.
% Intuitively, if we hear something interesting or see anything interesting when we look, we would like to take a closer look at this moment.
% In addition, if we not only do not hear anything of interest but also do not see anything of interest,  it is also not worth looking. 
% From this observation, we first filter noise samples in separate individual modes and then integrate the results. 
% 
% If each mode considers the modified sample as a noise sample, the loss of the sample will not be backward in the fusion branch during training, 
% In this way, the samples for the fusion branch training will be selected dynamically, and all samples will participate in the uni-mode branch.
% In other words, a Multi-modality Filter can obtain information on all samples to avoid data skew.
% 
% $\mathcal{L}_{\text {cons}}$ can make use of the pair-wise characteristic to learn robust latent representations and the model capability is further enhanced.
% In the joint optimization process, for the multi-modal (MM) branch, cross-entropy and consistency loss are updated along with a re-weighting schema with the uni-modal (UM) branch update by cross-entropy only:
% 
% \begin{equation}
% \mathcal{L}_{\text {ce }}=-\sum_{i=1}^N \sum_{c=1}^C g_i \log {y}_i
% \end{equation}
% 
% \begin{equation}
% \mathcal{L}^{\text {MM }}=\mathcal{L}_{\text {ce }}^{\text {MM }}+\beta \mathcal{L}_{\text {cons }}
% \end{equation}
% 
% \begin{equation}
% \mathcal{L}^{\text {UM }}=\mathcal{L}_{\text {ce }}^{\text {UM }}
% \end{equation}
% 
% where ${y}_i$ denotes the predicted probability of the $i^{th}$ clip over the class c.
% 
%Annotation noise is inevitable in highlight detection and the start and end of a highlight moment can vary significantly depending on the users.
%For example, the audience1 might be interested in a loose moment with a few minutes. In contrast, audience2 might be interested in a tight moment with a few seconds.
Annotation noise is inevitable in VHD due to subjectivity depending on the users and annotators. % rqiao
In this paper, we argue that highlight detection should be regarded as Learning with Noisy Labels.
%To alleviate the performance drop caused by noisy labels, we adopt a multi-modality collaborative cleaning to adaptively discount noisy samples with noisy visual and audio modality information.
To alleviate the performance drop caused by noisy labels, we adopt a multi-modality collaborative cleaning to adaptively filter noisy samples with noisy modality information.
%It works as the following procedure.

%Firstly, we maintain varied outputs simultaneously which are predicted by different branches.
%Then, different uni-modal branches independently select clean samples based on the low-loss criterion which treats deeming low-loss instances as clean samples has been widely adopted in tackling label noise.
%Contrary to existing sample selection methods~\cite{ExtractingUsefulMM2021, ConfidentLearningJAIR2021}, which directly discard high-loss samples, we keep all samples to train the uni-modal branches.
%Then, we update the multi-modal branch using clean samples in the back-propagation process.
%The samples for the multi-modal branch training will be selected dynamically, and all samples participate in the uni-mode branch training.
%In other words, MMC can obtain information on all samples to avoid the model defects of favoring easy samples.
%The detailed procedure of MMC is presented in the supplementary material.

Firstly, we maintain multiple outputs simultaneously which are predicted by different branches.
The uni-modal branches independently select clean samples based on the low-loss criterion in which instances with lower losses are treated clean samples. 
Contrary to existing noisy sample selection methods~\cite{ExtractingUsefulMM2021, ConfidentLearningJAIR2021}, which directly discard high-loss samples, we keep all samples to train the uni-modal branches.
Then, we update the multi-modal branch using only clean samples selected by both uni-modal branches in the back-propagation.
The samples for the multi-modal branch training are selected dynamically while all samples participate in the uni-mode branches training.
In this way, MMC obtains information on all samples to avoid the model defecting to favoring easy samples.
More details of MMC are presented in the supplementary material. % rqiao 1111

%Intuitively, if we hear or see something interesting at the moment, we would like to take a closer look at it.
%Otherwise, it is not worth looking at. 
%From this observation, we select clean samples in individual modalities separately and then integrate the results. 
%If all modalities consider the sample as the noise sample, the loss of the sample will not be backward in the multi-modal branch during training.


% In the joint optimization process, for the multi-modal (MM) branch, cross-entropy and consistency loss are updated along with a re-weighting schema with the uni-modal (UM) branch update by cross-entropy only.
%We adopt different loss functions for different branches. 
%For the uni-modal branch (UM), we employ the cross-entropy loss with all samples:
Each branch has its own loss function. 
For the uni-modal branch, we employ the cross-entropy loss with all samples as follows: % rqiao 1111
% \begin{equation}
% \mathcal{L}_{\text {ce }}=-\sum_{i=1}^N \sum_{c=1}^C g_i \log {y}_i
% \end{equation}
% \begin{equation}
% \mathcal{L}^{\text {UM }}=\mathcal{L}_{\text {ce }}^{\text {UM }}=-\sum_{i=1}^N \mathbf{g}_i \log \mathbf{y}_i^{UM},
% \end{equation}
\begin{equation}
\mathcal{L}^{UM_v}=\mathcal{L}_{\text {ce }}^{UM_v}=-\sum_{i=1}^N \mathbf{g}_i \log \mathbf{y}_i^{UM_v},
\end{equation}
\begin{equation}
\mathcal{L}^{UM_a}=\mathcal{L}_{\text {ce }}^{UM_a}=-\sum_{i=1}^N \mathbf{g}_i \log \mathbf{y}_i^{UM_a}.
\end{equation}

% For the multi-modal branch (MM), the cross-entropy loss and consistency loss are updated along with a re-weighting scheme with clean samples.
For the multi-modal branch, its cross-entropy loss and consistency loss are updated with a re-weighting scheme with clean samples. % rqiao
\begin{equation}
\mathcal{L}_{\text {ce }}^{\text {MM }}=-\sum_{i=1}^{N^{\prime}} \mathbf{g}_i \log \mathbf{y}_i^{MM},
\end{equation}
% \begin{equation}
% \mathcal{L}_{\text {ce }}^{\text {MM }}=-\left(\sum_{i=1}^{N^{\prime}} \mathbf{g}_i \log \mathbf{y}_i^{UM_v}\right),
% \end{equation}
% \begin{equation}
% \mathcal{L}_{\text {cons }}^{\text {MM }}=-\sum_{u=1}^M \sum_{i=1}^{N^{\prime}} \sum_{c=1}^C \mathbf{y}_i^{UM} \log \mathbf{y}_i^{MM},
% \end{equation}
\begin{equation}
\footnotesize
\mathcal{L}_{\text {cons }}^{MM }=-\left(\sum_{i=1}^{N^{\prime}} \mathbf{y}_i^{UM_v} \log \mathbf{y}_i^{MM} + \sum_{i=1}^{N^{\prime}} \mathbf{y}_i^{UM_a} \log \mathbf{y}_i^{MM}\right),
\end{equation}
\begin{equation}
\mathcal{L}^{\text {MM }}=\mathcal{L}_{\text {ce }}^{\text {MM }}+\beta \mathcal{L}_{\text {cons }}^{MM },
\label{eq:LOSS}
\end{equation}
where $\mathbf{g}_i$ and $\mathbf{y}_i^{*}$ denote the ground-truth and predicted probability of the $i^{th}$ clip, respectively; $N$ is the number of samples in a batch and ${N^{\prime}}$ is the number of samples seleted by MMC;
and $\beta$ are designed to balance different loss terms.
% and set $\beta  = 0.1$.

\noindent\textbf{Post processing.} 
% In experiments we observed that noise makes the resulting curves jitter, hindering the threshold selection for highlights clips.
In experiments we observe that noise makes jitter prediction curves along the temporal dimension, which may cause discontinuous thresholds for highlight selection. % modified by rqiao 1111
Therefore, we apply a median filter to smooth the prediction curves. Supposing 
$\mathbf{y}=[\mathbf{y}_{1};\mathbf{y}_{2};...;\mathbf{y}_{T}]$ is the original curve predicted by the CLC, the smoothed curve 
$\mathbf{s}=[\mathbf{s}_{1};\mathbf{s}_{2};...;\mathbf{s}_{T}]$  is given by:
\begin{equation}
\mathbf{s}_i= \begin{cases}\operatorname{Med}\left(\mathbf{y}_{i-k}, \mathbf{y}_{i+k}\right), & k<i \leq T-k \\ \mathbf{y}_i, & \text { otherwise }\end{cases}
\label{eq:median-filter}
\end{equation}
where $k$ is the window size, and ``Med'' denotes the median filter.
%The prediction curves for the different datasets are erratic and a fixed window size cannot produce satisfactory results. To solve the problem, we apply adaptive window sizes $k$ for different datasets. 


