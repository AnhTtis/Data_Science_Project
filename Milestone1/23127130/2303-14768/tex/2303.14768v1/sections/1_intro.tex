% !TEX root = ../main.tex

\section{Introduction}
\label{sec:intro}




% \begin{figure}[t]
%   \begin{center}
%   \includegraphics[width=0.9\linewidth]{./figures/trailers_v2.pdf}
%   \vspace{-0.8cm}
%   \end{center}
%       \caption{fig1}
%       \vspace{-0.5cm}
%   \label{fig:trailers}
% \end{figure}

% \vspace{-1cm}
% \twocolumn[{%
% \maketitle
% \vspace{-20pt}
% \begin{figure}[H]
%     \hsize=\textwidth
%     \centering
%     \includegraphics[width=2\linewidth]{./figures/trailers_v2.pdf}
%     \vspace{-0.8cm}
%     \captionof{figure}{caption for figure1}
%     % \vspace{-3mm}
%     \label{fig:fig1}
% \end{figure}
% }]




%With thousands of movies produced every year, it is incredibly time-consuming for the audience to go through them completely.
%It is an effective way that watches through movie trailers which consist of short shots of movies and often contain the most significant shots selected carefully.
%However, trailers are a combination of distinctive shots and lose the integrity of the story.
%Consequently, it is necessary to provide a pattern to detect highlight moments in movies efficiently.

With the growing number of new publications of movies in theaters and streaming media, audiences become even harder to choose their favorite one to enjoy for the next two hours.
An effective solution is to watch the movie trailers before choosing the right movie. This is because trailers are generally carefully edited by filmmakers and contain the most prominent clips from the original movies.
As a condensed version of full-length movies, trailers are elaborately made with highlight moments to impress the audiences.
Consequently, they are high potential in serving as supervision sources to train automatic video highlight detection algorithms and facilitating the mass production of derivative works for video creators in online video platforms, e.g., YouTube and TikTok.

% Consequently, they are highly potential in serving as supervision sources to train automatic video highlight detection algorithms due to their abundance, and facilitating the mass production of derivative works for video creators in online video platforms, e.g., YouTube and TikTok. %modified by rqiao 1103





%A related task is video highlight detection, while the existing video highlight detection(VHD) approaches cannot be directly applied to movie highlight moments detection due to the following reasons.
%First, there is no labeled data available for movie highlight detection. 
%Annotating highlight moments in movies is challenging, the labeling of movie highlight moments may be subjective and the scoring of the same shots by the annotation personnel may be different.
%Previous work~\cite{LeziWangECCV2020} leverages the oï¬ƒcially-released trailers as the weak supervision to learn a model that can detect the key moments from full-length movies. 
%As shown in Fig.~\ref{fig:trailers},  it is extremely noisy that consider trailers as a label directly.
%On the one hand, trailers focus on short video clips which last for seconds, and it is missing key information and storyline.
%On the other hand, the selection of trailer moments might attribute to various factors such as environment or artistic style, which may be far away from the movie theme.
%Second, the existing VHD approaches ignore the subjectivity of highlight detection, which often brings a lot of noise to training.

% Existing video highlight detection (VHD) approaches, which are generally trained with annotated key moments of long-form videos, however, are not suitable to tackle the movie highlight detection task by directly learning from trailers.
% The edited shots in trailers are not equivalent to ground-truth highlight annotations in movies. 
% Although a previous work~\cite{LeziWangECCV2020} leverages the officially-released trailers as the weak supervision to train a model that can detect the key moments from full-length movies, the highlighted ness of trailer shots is extremely noisy and varies with the preference of audiences, as shown in Fig.~\ref{fig:trailers}. 
% On one hand, trailers can be purposefully edited to avoid spoilers and thus missing key moments of the storylines.
% On the other hand, due to some artistic or commercial factors, some less important moments in the original movie are over-emphasized in the trailer.
% The subjective nature of trailer shots makes them noisy labels for the VHD task, which is ignored by existing VHD approaches.  %modified by rqiao 1103

Existing video highlight detection (VHD) approaches are generally trained with annotated key moments of long-form videos. However, they are not suitable to tackle the movie highlight detection task by directly learning from trailers.
The edited shots in trailers are not equivalent to ground-truth highlight annotations in movies. 
Although a previous work~\cite{LeziWangECCV2020} leverages the officially-released trailers as the weak supervision to train a highlight detector, the highlighted ness of trailer shots is extremely noisy and varies with the preference of audiences, as shown in Fig.~\ref{fig:trailers}. 
On one hand, trailers tend to be purposefully edited to avoid spoilers, thus missing key moments of the storylines.
On the other hand, some less important moments in the original movies are over-emphasized in the trailers because of some artistic or commercial factors.
The subjective nature of trailer shots makes them noisy for the VHD task, which is ignored by existing VHD approaches. 


% Existing video highlight detection (VHD) approaches, which are generally trained with annotated key moments of long-form videos, however, are not suitable to tackle the movie highlight detection task by directly learning from trailers.
% The edited shots in trailers are not equivalent to ground-truth highlight annotations in movies. 
% Although a previous work~\cite{LeziWangECCV2020} leverages the officially-released trailers as the weak supervision to train a model that can detect the key moments from full-length movies, the highlighted ness of trailer shots is extremely noisy and varies with the preference of audiences, as shown in Fig.~\ref{fig:trailers}. 
% On one hand, trailers tend to be purposefully edited to avoid spoilers and thus missing key moments of the storylines.
% On the other hand, due to some artistic or commercial factors, some less important moments in the original movies are over-emphasized in the trailers.
% The subjective nature of trailer shots makes them noisy labels for the VHD task, which is ignored by existing VHD approaches. 

%Aiming to solve the problem above, We provide a scene-aware paradigm to learn highlight moments in movies from trailers avoiding human annotations.
%Specifically, we use the scene-aware trailer as the label in training, and in testing, we used manual annotation to find the story segment corresponding to the trailer and exclude irrelevant segments.
%Considering the noise inherent in the automatic annotation and highlight detection tasks, we regard highlight detection as learning with noisy labels.
%We introduce a MultiModal Collaborative Filtering Network(MCFNet) to adaptively discount a noisy visual or audio modality. 
%Specifically, it deems low-loss instances as clean data in each mode respective to select samples dynamically for the training to handle noisy and incomplete labeling.
%In addition, we design an augmented cross-propagation module and consistency regression to augment the prediction objective with a notion of mode perceptual consistency.


To alleviate the issue, we reformulate the highlight detection task as ``learning with noisy labels''. Specifically, we first leverage a scene-segmentation model to obtain the movie scene boundaries. 
The clips containing trailers and clips from the same scenes as the trailers provide more complete storylines.
% The clips that contain trailers and within the same boundaries provide more complete storylines. 
They have a higher probability of being highlight moments but still contain some noisy moments. Subsequently, we introduce a framework named Collaborative noisy Label Cleaner (CLC) to learn from these pseudo-noisy labels. 
% The framework first filters outs noisy and incomplete labeling via the augmented cross-propagation (ACP) module, which exploits closely related audio-visual signals during training. In addition, a multi-modality cleaning (MMC) mechanism is designed to enhance the modality perceptual consistency. %modified by rqiao 1104, this part requires a careful check on the method details, lets do it later together.
The framework firstly enhances the modality perceptual consistency via the augmented cross-propagation (ACP) module, which exploits closely related audio-visual signals during training. 
In addition, a multi-modality cleaning (MMC) mechanism is designed to filter out noisy and incomplete labels. 

To support this study and facilitate benchmarking existing methods in this direction, we construct MovieLights, a Movie Highlight Detection Dataset. MovieLights contains 174 movies and the highlight moments are all from officially released trailers. The total length of these videos is over 370 hours. We conduct extensive experiments on MovieLights, in which our CLC exhibits promising results. We also demonstrate that our proposed CLC achieves significant performance-boosting over the state-of-the-art on the public VHD benchmarks. %modified by rqiao 1104, please check the red words. Are Movielights annotations mannual or automatic generated?



%To support this study and facilitate research in this direction, we construct Movielights, a Movie Highlight Detection Dataset, which contains 174 movies and their highlight moments. The total length of these videos is over 370 hours. We conduct experiments on Movielights, and our MCFNet shows promising results. We also demonstrate that our proposed MCFNet module significantly achieves marginal performance-boosting over the state-of-the-art on the public VHD benchmarks Highlight.

% To support this study and facilitate benchmarking existing methods in this direction, we construct Movielights, a Movie Highlight Detection Dataset, which contains 174 movies and their highlight moments 
% % \textcolor{red}{(trailer?)}
% offical trailers. The total length of these videos is over 370 hours. We conduct experiments on Movielights, where our CLC exhibits promising results. We also demonstrate that our proposed 
% % \textcolor{red}{(which?)} module 
% CLC 
% achieves significant performance-boosting over the state-of-the-art on the public VHD benchmarks. %modified by rqiao 1104, please check the red words. Are Movielights annotations mannual or automatic generated?





% Our \textbf{contributions} in this work include:
% \begin{itemize}

%   \item We provide a paradigm that detects highlight moments in full-length movies by trailer and scene segmentation technology automatically without manual annotation.

%   \item We incorporate a multi-modality filter strategy to tack label noise, which further improves the robustness of networks to annotation noise.
  
%   \item We present an attention fusion mechanic to capture the interactions across modalities and an auxiliary consistency loss to maximize the agreement between the different modalitiy output.

%   \item Experiments on movie datasets and benchmark datasets validate the effectiveness of our framework.

In summary, our major contributions are as follows:
\begin{itemize}

  \item We introduce a scene-aware paradigm to learn highlight moments in movies without any manual annotation. To the best of our knowledge, this is the first time that highlights detection is regarded as learning with noisy labels.
  
  \item We present an augmented cross-propagation to capture the interactions across modalities and a consistency loss to maximize the agreement between the different modalities.
  
%   \item We provide a Collaborative noisy Label Cleaner (CLC) framework to tackle the noisy highlight moments. Further, we propose an augmented cross-propagation (ACP) module and a multi-modality cleaning (MMC) module. 
  \item We incorporate a multi-modality noisy label cleaner to tackle label noise, which further improves the robustness of networks to annotation noise.

  \item Experiments on movie datasets and benchmark datasets validate the effectiveness of our framework.

  
\end{itemize}

