% !TEX root = ../main.tex


\section{Movie Highlight Dataset}
\label{sec:Movie_Highlight_Dataset}



% \subsection{Data Collection.}
% \label{subsec:Data_Collection.}


% \subsection{Data Annotation.}
% \label{subsec:Data_Annotation.}
 

%In this paper, we aim to detect the highlight moments in movies with public trailers as auxiliary supervision. 
%The existing movie or trailer-related benchmarks  ~\cite{MMTF14KACM2018, CondensedMoviesACCV2020} are not appropriate for it. 
%They collect the trailers or the movie for other tasks without full movies provided. 
%Huang et al.~\cite{Fromtrailerstostorylines2018} learn the vision models from both movies and trailers by proposing a Large-Scale Movie and Trailer Dataset (LSMTD). 
%Wang~\cite{LeziWangECCV2020}  constructs a Trailer Moment Detection Dataset (TMDD) that automatically detects trailer moments from full-length movies without the need for human annotation.
%However, LSMTD and TMDD are not publicly available and TMDD only contains three domains according to the genre, lacking diversity and limiting applications.
%
% Moreover, TMDD only contains three domains according to the genre, including “Action”, “Drama”, and “Sci-Fi”, it is lack diversity and limits apply.
%

In this paper, we aim to detect the highlight moments in movies by learning from easily accessible trailers as the noisy supervision. 
However, the existing movie and trailer-related benchmarks ~\cite{MMTF14KACM2018, CondensedMoviesACCV2020, MovieNetECCV2020} lack sufficient functions for this task, such as the absence of full-length movies and ground-truth highlight annotations.
% Huang et al.~\cite{Fromtrailerstostorylines2018} propose to learn a \textcolor{red}{(vision-modality only?)} model from both the movies and trailers in their constructed a Large-Scale Movie and Trailer Dataset (LSMTD). % need more details or combine with (LeziWangECCV2020), commented by Qiao
Huang et al.~\cite{Fromtrailerstostorylines2018} propose to respectively learn visual representations from trailers and temporal structure from full-length movies in their constructed Large-Scale Movie and Trailer Dataset (LSMTD).
% modified by BGAN 1104
Wang~\cite{LeziWangECCV2020}  constructs a Trailer Moment Detection Dataset (TMDD) for detecting trailer moments from full-length movies without explicit human annotation. Both LSMTD and TMDD are not publicly available, while TMDD only contains three movie genres. % modified by Qiao, please also check the comments. 1104




\begin{figure}[t]
   \begin{center}
   \includegraphics[width=1\linewidth]{./figures/annotation_v4.pdf}
   \vspace{-0.8cm}
   \end{center}
      \caption{The labeling process of MovieLights. For the training set, we introduce a scene-aware paradigm to obtain labels automatically. For the testing set, we collect $2$ sets of labels from different annotators.}
    %   \vspace{-0.2cm}
   \label{fig:annotation}
\end{figure}




%To facilitate our study, we construct a new dataset, named Movie Highlight Detection Dataset (MovieLights).
%
% We collect a set of movies from commercial channels, and download the trailers from YouTube, covering popular movies in recent years.
%
% We collect a set of movies from commercial channels, and download the trailers from YouTube, covering popular movies and a wide range of genres.
% Moreover, MovieLights is signiﬁcantly larger than other datasets in terms of the number of videos and the total time duration, making our dataset more comprehensive and general. 
%
%We collect a set of movies from commercial channels, and download the trailers from YouTube, covering popular movies and a wide range of genres, making our dataset richer in diversity and general.
%
% We analyze the movie on the basis of shots that consists of consecutive frames in one camera recording time.
% We apply shot boundary detection ~\cite{TransNetV2} and scene segmentation ~\cite{SceneConsistencyCVPR2022} to segment movies into multiple shots and scenes.
%
%We first divide movies into shots using  ~\cite{TransNetV2} which consist of consecutive frames in one camera recording time
%Then, we apply ~\cite{SceneConsistencyCVPR2022} to segment movies into multiple scenes.

The inaccessibility of public benchmarks motivates us to construct a new dataset, named Movie Highlight Detection Dataset (MovieLights). 
In particular, we purchase a set of movies from commercial channels and collect their corresponding trailers from streaming platforms such as YouTube, covering at least $25$ genres to ensure content diversity.
The movies and trailers are then prepossessed by shot segmentation ~\cite{TransNetV2} and scene segmentation ~\cite{SceneConsistencyCVPR2022}, respectively. 
The resulting shots are a series of consecutive frames taken by the camera until a physical interruption, and the scenes are consecutive shots that share a semantically related theme. % modified by Qiao 1104

% 
% As seen in Fig.~\ref{fig:annotation}, to build the ground-truth , we conduct visual similarity matching between trailers and movies at the frame-level as auxiliary-label.
% For the training set, we expand the auxiliary-label frame to the scene-level as positive sample.
% For the testing set, we collect 2 sets of moments for each movie from different workers.
% Though all selected shots are relevant to the trailers, as highlightness can be subjective, they may still vary in their saliency and time span, therefore we calculate the union between every pair of moments annotated for the same auxiliary-label.
%
% As seen in Fig.~\ref{fig:annotation}, to build the ground-truth, we conduct visual similarity matching between trailers and movies at the frame-level as the annotation reference. % please be more specific, commented by rqiao

% For the training set, we expand the auxiliary label frame to the scene span as positive samples automatically without manual annotation for the training set.
% For the testing set, we collect $2$ sets of moments for each movie from different workers and all data are annotated by different annotators independently.
% To ensure the consistency of results from different annotations, during the annotation procedure, we provided a list of trailer examples for reference.
% The vast diversity of movie storylines makes challenging for annotators. 
% Though all selected shots are relevant to the trailers, as highlight moments can be subjective, they may still vary in their saliency and time span.
% Finally, we calculate the union between every pair of moments annotated as the ground-truth.  % this part is not clear，I will check later
%

% As seen in Tab{}, MovieLights contains 174 movies in full length with their highlight moments.
% It splits into 100 domains according to the genre with the rich domain.
% The movies contain 100000 movie shots and 10000 trailer shots.
% True positives of movies take 35\% in training set while only take 26\% in testing set.
% Hence, MovieLights on this dataset is a quite challenging task as the labels with noisy.

As seen in Fig.~\ref{fig:annotation}, to build the ground truth, we conduct Faiss~\cite{Faiss2019} to obtain visual similarity matching between trailer frames and movie frames.  We locate trailer moments in the movie and align them with the movie shots as annotation references.
For the testing set, we collect $2$ sets of moments for each movie from different workers, and these moments are annotated by different annotators independently.
To ensure the consistency of results from different annotations, during the annotation procedure, all highlight moments must be related to the annotation references.
Though all selected shots are relevant to the trailers, as highlight moments can be subjective, they may still vary in their saliency and time span.
We calculate the intersection between every pair of moments annotated as the ground truth.
%However, the vast diversity of movie storylines makes challenging for annotators that it is time-consuming and requires annotators to be fully aware of the movie.
However, the vast diversity of movie storylines makes the annotation challenging as it is time-consuming and requires annotators to be familiar with the movie. %modified by rqiao 1111
To collect a large amount of training data efficiently, we introduce a scene-aware paradigm to obtain the highlight moments label without any manual annotation.
Specifically, we expand the shot-level annotation references to the scene span as positive samples automatically.
It will capture the complete scene context of the trailer shot with movie storylines.
Since the trailer shots may contain some less important moments, the acquired highlight labels are still noisy. % modified by rqiao 1111
% modified by BGAN 1110


\begin{table}[]
   \caption{
   The basic statistics of MovieLights.
   }

   \vspace{-0.3cm}
   \label{tab:dataset}
   \small
   \centering
   \begin{tabular}{c|cc}
   \hlinew{1.1pt}
     & \multicolumn{1}{l}{Train} & \multicolumn{1}{l}{Test}  \\\hline\hline
    Movie Number                            & 144                       & 30                        \\
    Avg Durations per Movie                       & \multicolumn{1}{l}{2.19h} & \multicolumn{1}{l}{2.14h} \\
    Avg Shot Number per Movie                            & 1852               & 1940           \\
    % Avg Shot Durations Time                   & 4.80              & 4.29               \\
    Avg Scene Number per Movie                           & 207              & 193              \\
    % scene durations time                  & 4.72               & 4.72              \\
    Annotator1 Positive sample Proportion & -          & 0.27                      \\
    Annotator2 Positive sample Proportion & -          & 0.30                      \\
    Positive sample proportion            & 0.35                      & 0.21     \\     
    \hlinew{1.1pt}
   \end{tabular}
\end{table}



As seen in Tab.~\ref{tab:dataset}, MovieLights contains $174$ movies in full length with their official trailers and it is split into a training set with $144$ movies, and a testing set with $30$ movies. % is trailer one-to-one attatched to a movie? % Yes 
%
% It splits into 100 domains according to the genre with the rich domain and contains 100000 movie shots and 10000 trailer shots.
%It is with the rich domain according to the genre and contains $325k$ movie shots and $36k$ movie scenes.
The content diversity is ensured by the rich domain informations (more than $25$ genres) and abundant segments ($325k$  shots and $36k$ scenes). % modified by rqiao 1111
%Most movies in our dataset have a time duration between $90$ to $150$ minutes, and the length of the annotated moments varies from less than one minute to more than several minutes.
Most movies in our dataset are between $90$ to $150$ minutes, and the length of the annotated moments varies from tens of seconds to several minutes. % modified by rqiao
% As the true positives of movies take $35\%$ in the training set while only taking $26\%$ in the testing set, it is more challenging as the labels with noise.
As the acquired positive highlight moments take up $35\%$ in the training set while the annotated true positives take up $21\%$ in the testing set, the difference tells the obvious existence of label noise. % modified by rqiao 1111


%We plan to release the dataset to promote further study of movie analysis. Due to legal constraints, trailers and movies will be released in the form of urls and feature maps. 
We plan to release the dataset publicly to promote further study of movie analysis. Due to copyright issues, trailers and movies will be released in the form of extracted features in visual and audio modalities. 
% check the wording of movieNet and other media-related datasets. commented by riqao 1104
