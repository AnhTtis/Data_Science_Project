% !TEX root = ../main.tex

\section{Experiment}
\label{sec:experiment}




\subsection{Datasets and Experimental Settings}
\label{subsec:Datasets and Experimental Settings}







\noindent\textbf{Datasets.}
\label{subsubsection:Datasets.}
%We evaluate our CLC on the constructed dataset MovieLights and YouTube Highlights dataset~\cite{RankingDomainspecificECCV2014}. 
We evaluate our CLC on the constructed dataset MovieLights and public YouTube Highlights dataset~\cite{RankingDomainspecificECCV2014}. % modified by rqiao 1110
%We randomly split the movies into the training and testing sets, containing 144 and 30 movies respectively. 
MovieLights is split into training and testing sets, each containing 144 and 30 movies respectively. % modified by rqiao 1110

%We split movies into shots, which consists of consecutive frames in one camera recording time.
%For visual modality, each shot consists of $1$ keyframes and the middle frame is chosen to represent the shot.
%Frame-level visual features obtain from ViT~\cite{VITICLR2021} pre-trained by CLIP~\cite{CLIP2021}. 
%We align audio clip with the visual shots and get their features respectively in a shot with $16K$ Hz sampling rate and $512$ windowed signal length and then a PANN audio network~\cite{kong2020panns} pretrained on AudioSet~\cite{Audioset2017} is used to obtain their feature maps.
We split movies into shots. The movie features are represented at shot-level. 
We use the middle frame of each shot to extract its visual feature with ViT~\cite{VITICLR2021} pre-trained by CLIP~\cite{CLIP2021}. 
We align timestamps of audio clips with the visual shots, and sample the audio clip of each with $16K$ Hz sampling rate and $512$ windowed signal length. The resulted shot-level audio features are obtained with the PANN audio network~\cite{kong2020panns} pretrained on AudioSet~\cite{Audioset2017}. % modified by rqiao 1110


%The YouTube Highlights dataset contains six different categories: dog, gymnastics, parkour, skating, skiing, and surfing with 422 videos currently available.
The YouTube Highlights contains six distinct categories with a total 422 videos currently available. % modified by rqiao 1110
% There are approximately 100 videos for each category. 
Following the practice of prior efforts, we train a highlight detector for each category. 
% 
%Youtube provides two annotations Harvested Highlight and Mturk Highlight.
%The former is selected by user as highlight, and the latter by turkers
%For Harvested Highlight, each match label file specifies if each clip is matched in the edited video. Label 1 denotes matched clip, label -1 denotes unmatched clip, and label 0 denotes borderline cases. 
%For  Mturk Highlight, each Mturk label file specifies how many turkers select a clip as highlight. 
% For instance, 2 means the first clip is selected by 2 people.
%And turkers cast soft votes depending on the coverage between the clip and tuker selected video segment.
YouTube Highlights provides two annotations: Harvested Highlight and Mturk Highlight. 
In the Harvested annotation, the match label specifies if each clip is matched in the edited video, where 1 denotes matched, -1 denotes unmatched and 0 denotes the borderline cases.
In the Mturk annotation, the highlight labels are marked by multiple turkers of different styles, making it noisier than the Harvested annotation. % modified by rqiao 1110. plese check
%The former is selected by user as highlight, and the latter by turkers. 
%And turkers cast soft votes depending on the coverage between the clip and tuker selected video segment. 

%On the YouTube Highlights , we follow the work of ~\cite{UMTCVPR2022} and obtain cliplevel visual features using an I3D~\cite{I3DCVPR2017} pre-trained on Kinetics400~\cite{K400}. 
%We use a PANN audio network~\cite{kong2020panns} pretrained on AudioSet~\cite{Audioset2017} to obtain audio features that align with the visual clips. Frame-level features are averagepooled within each clip for both audio and visual features to generate a clip-level feature. Since each feature vector captures 32 consecutive frames, we follow ~\cite{UMTCVPR2022} and consider the feature vector belonging to a clip if their overlap is more than $50\%$. 

% On YouTube Highlights , we follow ~\cite{UMTCVPR2022} and obtain clip-level visual features using I3D~\cite{I3DCVPR2017} pre-trained on Kinetics400~\cite{K400}. 
On YouTube Highlights, we use the same protocol and data preprocessing as ~\cite{UMTCVPR2022}.
It obtain clip-level visual features and optical flow features using I3D~\cite{I3DCVPR2017} pre-trained on Kinetics400~\cite{K400}.
It use a PANN audio network~\cite{kong2020panns} pretrained on AudioSet~\cite{Audioset2017} to obtain audio features that align with the visual clips. 
Frame-level features are average-pooled within each clip for both audio and visual features to generate a clip-level feature. 
Since each feature vector spans 32 consecutive frames, we follow ~\cite{UMTCVPR2022} and consider the feature vector corresponded to a clip if their overlap is more than $50\%$.  %modified by rqiao



% We follow the tradition to work on a random 0.8/0.2 training/test split.



\noindent\textbf{Benchmarks.}
\label{subsubsection: Benchmarks.}
%To better reflect the robustness of our CLC with the noisy labels, for YouTube Highlights, we apply perturbations in training set annotation and test sets keep unchanged. 
%There are three benchmarks to be compared in YouTube Highlights.
To better inspect the robustness of our CLC against noisy labels, we also apply label perturbations in training set of YouTube Highlights while keeping the ground-truths in the testing set unchanged.
YouTube Highlights has two benchmarks for comparison. % modified by rqiao 1110
%1) \textbf{Harvested with matched.} 
%we pick the clip labeled matched as the highlighted clip and it is the same as previous works and test set.
1) \textbf{Harvested with matched:} 
we regard the clips labeled with matched as the highlighted clips and this is the same setting as in previous works ~\cite{UMTCVPR2022,CrosscategoryICCV2021}. %modified by riqao 1110. I do not understand why mention the testing set? please clarify.
%2) \textbf{Harvested with matched and borderline.}
%clips labeled matched and borderline would be treated as the highlight moment.
2) \textbf{Harvested with matched and borderline:}
clips labeled with matched and borderline are treated as the highlight moments.
In this benchmark, the training set contains some highlighted clips with weak confidence. % modified by rqiao 1110
%3) \textbf{Mturk} annotated by turkers.
We select the clips whose mturk-label is over score 1 as the highlighted clips, which means that at least one turker selects the clip as a highlight. 
There are labeled by different types of annotators between Mturk and the test set and bring greater noise. 
%3) \textbf{Mturk} annotated by turkers：
We select the clips whose mturk-label is over score 1 as the highlighted clips, which means that at least one turker selects the clip as a highlight. Due the labeling gap between the Mturk annotation in the training set and the clean testing set, this benchmark is even noisier.
% Please clarify. commented by rqiao 1110
% As is shown in the table {}, the proportion of positive samples of these three benchmarks
% It is obvious that with the increase of noise, the proportion of positive samples varies more.

\noindent\textbf{Baselines.}
\label{subsubsection:Baselines.}
% We introduce a Bi-LSTM[]-based model (CLC base)  to model representations of longer shot.
% Similar to CLC, the proposed baseline takes the shot features sequence as the basic temporal input unit while lack of augmented cross-propagation and multi-modality noisy label cleaner.
% The CLC base integrated multimodal information and based on sequence-to-sequence learning.
% We introduce degenerated CLC$-$ serving as a baseline.
%Similar to CLC, the CLC$-$  is a Bi-LSTM-based model and takes the shot features sequence as the basic temporal input unit while lack of augmented cross-propagation and multi-modality noisy label cleaner.
We introduce CLC$-$, the degenerated version of CLC, as a baseline.
Similar to CLC, CLC$-$ is a Bi-LSTM-based model and takes the temporal sequence of shot features as input but lacks the modules of augmented cross-propagation and multi-modality noisy label cleaner. % modified by rqiao 1110

\noindent\textbf{Evaluation Metric.}
\label{subsubsection: Evaluation Metric.}
%We adopt the widely-used mean Average Precision or mAP as the evaluation metric for the MovieLights and YouTube datasets. 
%Considering a highlighted moment in one video is not necessarily more interesting than non-highlight moments in other videos; we calculate each test video and average overall videos.
We adopt mean Average Precision (mAP) as the evaluation metric for MovieLights and YouTube Highlights. 
Considering that a highlighted moment in one video is not necessarily more interesting than non-highlight moments in other videos, we evaluate on each test video independently and report the averaged results.%modified by rqiao 1110



% \noindent\textbf{Comparison Baselines.}
% \label{subsubsection:Comparison Baselines.}
% Comparison Baselines. 
% To better reflect the robustness of our CLC with noisy label, we compare our four baselines in YouTube Highlights.
% We apply perturbations in training set annotation although test sets remain.
% %  1) \textbf{Harvested Highlight with socre 1}.
% % For Harvested Highlight, we pick the clip labeled 1 as the highlighted clip and it is the same with the previous work.
% %  2) \textbf{Harvested Highlight with socre 0 and 1}.
% % We select the clips marked 0 and 1 as the highlighted clips.
% % In other words, the training set and the test set are labeled by the same method, but the training contains some clips with weak confidence.
% %  3) \textbf{Mturk Highlight over 0.5}.
% %  4) \textbf{Mturk Highlight over 1}.





\noindent\textbf{Implementation Details.}
\label{subsec: Implementation Details.}
%We train our model using SGD, with a learning rate of 0.01. We train for 500 epochs on the YouTube datasets, and 50 epochs on the MovieLights dataset. Before the cross-attention modules, we project each modality into a vector of fixed-size length. The key, query, and value vectors all follow the same size.
% We train our model using SGD, with a learning rate of 0.01. We train for 500 epochs on the YouTube datasets, and 50 epochs on the MovieLights dataset. Before the cross-attention modules, we project each modality into a vector of 512 dimension. The key, query, and value vectors all share the same dimension. %modified by rqiao 1110
On the MovieLights, we train our model using SGD, with a learning rate of 0.01. We train for 50 epochs. Before the cross-attention modules, we project each modality into a vector of 512 dimension. The key, query, and value vectors all share the same dimension.
Weight $\beta$ in Eq.~\ref{eq:LOSS} is empirically set to 0.1 and window size $k$ in Eq.~\ref{eq:median-filter} is set to 9.
 
% The window size of PP is $9$ for MoiveLights.
\subsection{Results on MovieLights}
\label{subsec:Results on MovieLights.}


% \begin{table}[]
% \begin{tabular}{l|lr}
% Methods    & Modality & \multicolumn{1}{l}{MAP}   \\ \hline
% UMT        & VA       & 38.7                      \\ \hline
% LSTM       & VA       & \multicolumn{1}{l}{39.65} \\ \hline
% LSTM + SCE & VA       & 39.83                     \\ \hline
% LSTM + LS  & VA       & 40.49                     \\ \hline
% MCNF       & VA       & 43.88                     \\ \hline
% \end{tabular}
% \end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% by shuxj
 
% \begin{table}[!t]
%     \caption{movielights.}
%     \small
% 	\centering  
% 	\label{tab:moviesota}
% 	\renewcommand\arraystretch{1} 
% 	\vspace{-0.3cm}
%     \begin{tabular}{p{2.5cm}p{1.5cm}<{\centering}|p{1.5cm}<{\centering}} 
%   \hlinew{1.1pt}
%     Methods    & Modality & MAP   \\ 
%     \hline \hline
%     GIFs       & V       & 25.48\\ 
%     SL-Module       & V       & 32.34 \\ 
%     SL-Module       & VA       & 34.27 \\ 
%     MINI-Net\bgan{(remove?)} & VA       & 28.48                     \\ 
%     UMT        & VA       & 38.7                      \\ 
%     CLC       & VA       & 43.88                     \\ 
%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table}


% \begin{table}[!t]
%     \caption{movielights.}
%     \small
% 	\centering  
% 	\label{tab:expsmovie}
% 	\renewcommand\arraystretch{1} 
% 	\vspace{-0.3cm}
%     \begin{tabular}{p{2.5cm}p{1.5cm}<{\centering}|p{1.5cm}<{\centering}} 
%   \hlinew{1.1pt}
%     Methods    & Modality & MAP   \\ \hline \hline
%     CLC$-$       & VA       & 39.65 \\ 
%     CLC$-$ w/ SCE & VA       & 39.83                     \\ 
%     CLC$-$ w/ LS & VA       & 40.49                     \\ 
%     CLC       & VA       & 43.88                     \\ 
%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[]
%   \caption{
%   movielights.
%   }

%   \vspace{-0.3cm}
%   \label{tab:moviesota}
%   \small
%   \centering
%   \begin{tabular}{cl|cccc}
%   \hlinew{1.1pt}
%     Methods    & Modality & \multicolumn{1}{l}{MAP}   \\ \hline \hline
%     GIFs       & V       & \multicolumn{1}{l}{\bgan{TBD}} \\ 
%     SL-Module       & V       & 32.34 \\ 
%     SL-Module       & VA       & 34.27 \\ 
%     MINI-Net\bgan{(remove?)} & VA       & 28.48                     \\ 
%     UMT        & VA       & 38.7                      \\ 
%     CLC       & VA       & 43.88                     \\ 
%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table}


% \begin{table}[]
%   \caption{
%   movielights.
%   }

%   \vspace{-0.3cm}
%   \label{tab:expsmovie}
%   \small
%   \centering
%   \begin{tabular}{cl|cccc}
%   \hlinew{1.1pt}
%     Methods    & Modality & \multicolumn{1}{l}{MAP}   \\ \hline \hline
%     CLC$-$       & VA       & \multicolumn{1}{l}{39.65} \\ 
%     SCE + CLC$-$ & VA       & 39.83                     \\ 
%     LS + CLC$-$ & VA       & 40.49                     \\ 
%     CLC       & VA       & 43.88                     \\ 
%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table}



 
\begin{table}[!t]
    \caption{Results on MovieLights.}
    \small
	\centering  
	\label{tab:moviesota}
	\renewcommand\arraystretch{1} 
	\vspace{-0.3cm}
    \begin{tabular}{p{2.5cm}p{1.5cm}<{\centering}|p{1.5cm}<{\centering}} 
   \hlinew{1.1pt}
    Methods    & Modality & mAP   \\ 
    \hline \hline
    GIFs~\cite{Video2gifCVPR2016}       & V       & 25.48\\ 
    SL-Module~\cite{CrosscategoryICCV2021}       & V       & 32.34 \\ 
    SL-Module~\cite{CrosscategoryICCV2021}      & VA       & 34.27 \\ 
    UMT~\cite{UMTCVPR2022}        & VA       & 38.7 \\\hline
    CLC$-$       & VA       & 39.65 \\ 
    CLC$-$ w/ SCE~\cite{SymmetricCrossEntropyICCV2019} & VA       & 39.83                     \\ 
    CLC$-$ w/ LS~\cite{LabelsmoothingCVPR2016} & VA       & 40.49 \\\hline
    CLC       & VA       & \textbf{43.88}                     \\ 
    \hlinew{1.1pt}
   \end{tabular}
\end{table}




%For MovieLights dataset, we frist train a deep network using noisy datasets directly. 
%To compare with previous highlight detection work, we choose some models as baseline methods: Video2GIF~\cite{Video2gifCVPR2016}
% , MINI-Net~\cite{MININetECCV2020}
%, UMT~\cite{UMTCVPR2022} and  SL-Module~\cite{CrosscategoryICCV2021}.
%Tab.~\ref{tab:moviesota} illustrates a comparison between our method with state-of-the-art methods of highlight detection.
%From this table, the leading performance obtained by our method can be witnesse

On MovieLights, we train our model with the noisy pseudo labels. 
To compare with previous state-of-the-art highlight detection works, we train UMT~\cite{UMTCVPR2022} and  SL-Module~\cite{CrosscategoryICCV2021} using the same protocol and data preprocessing as in CLC. We also compare with Video2GIF~\cite{Video2gifCVPR2016} using its off-the-shelf tool\footnote{\href{https://github.com/gyglim/video2gif_code}{https://github.com/gyglim/video2gif\_code}}. The upper part of Tab.~\ref{tab:moviesota} illustrates the significant performance gain of CLC. % modified by rqiao


%To demonstrate the advantage of CLC in learning with noisy labels,  we make comparisons with two main-stream label noise approaches: Label Smoothing~\cite{LabelsmoothingCVPR2016} and SCE loss~\cite{SymmetricCrossEntropyICCV2019}.
%Specifically, we insert Label Smoothing and SCE loss into our base CLC frameworks respectively.
% To be fair, the same base framework is used in all experiments.
%From Tab.~\ref{tab:moviesota}, we can observe that result of  two baseline label noise methods.
%The results clearly confirm that CLC outperforms two baseline label noise methods significantly.
%This shows the effectiveness of our augmented cross-propagation and multi-modality noisy label cleaner.

To demonstrate the advantages of CLC in learning with noisy labels,  we make comparisons with two main-stream label noise approaches: Label Smoothing~\cite{LabelsmoothingCVPR2016} and SCE loss~\cite{SymmetricCrossEntropyICCV2019}.
Specifically, we insert Label Smoothing or SCE into our CLC$-$ framework to create two baseline VHD methods to tackle label noise.
% To be fair, the same base framework is used in all experiments.
The bottom part of Tab.~\ref{tab:moviesota} shows that CLC outperforms the two baseline methods by a notable margin, indicating that our augmented cross-propagation and multi-modality noisy label cleaner are more effective than vanilla label noise approaches in VHD tasks. % modified by rqiao



\subsection{Ablation Study}
\label{subsec:Ablation Study.}


% \begin{table}[]
%   \caption{
%   ablationstudy.
%   }

%   \vspace{-0.3cm}
%   \label{tab:ablationstudy}
%   \small
%   \centering
%   \begin{tabular}{cl|cccc}
%   \hlinew{1.1pt}
%     xxx    & xx & \multicolumn{1}{l}{xxx}   \\ \hline \hline
%     xxx        & xx       & xxx                      \\ 

%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table}



% \begin{table*}[]
%   \caption{
%   Ablation Study.
%   }

%   \vspace{-0.3cm}
%   \label{tab:ablationstudy}
%   \small
%   \centering
%     \begin{tabular}{llll|r}
%     \hlinew{1.1pt}
%     Multi-modality Sample Cleaning. & Feature Augment & Consistency Loss & Post Processing. & \multicolumn{1}{l}{MAP} \\\hline \hline
%     ×                      & ×               & ×                & ×                & 39.65                   \\
%     \checkmark\                      & ×               & ×                & ×                & 41.69                   \\
%     \checkmark                      & \checkmark               & ×                & ×                & 42.79                   \\
%     \checkmark                      & \checkmark               & \checkmark                & ×                & 43.22                   \\
%     \checkmark                      & \checkmark               & \checkmark                & \checkmark                & \textbf{43.88}       \\
%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% by shuxj
\begin{table}[!t]
    \caption{Ablation results of MoiveLighgts.}
    \small
	\centering  
	\label{tab:ablationstudy}
	\renewcommand\arraystretch{1} 
	\vspace{-0.3cm}
    \begin{tabular}{p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}|p{1.5cm}<{\centering}} 
   \hlinew{1.1pt} 
    MMSC & CP & CL & PP  & mAP \\\hline \hline
    ×                      & ×               & ×                & ×                & 39.65                   \\
    \checkmark\                      & ×               & ×                & ×                & 41.69                   \\
    \checkmark                      & \checkmark               & ×                & ×                & 42.79                   \\
    \checkmark                      & \checkmark               & \checkmark                & ×                & 43.22                   \\
    \checkmark                      & \checkmark               & \checkmark                & \checkmark                & \textbf{43.88}       \\
    \hlinew{1.1pt}
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \begin{table}[]
%   \caption{
%   Ablation Study.
%   }

%   \vspace{-0.3cm}
%   \label{tab:ablationstudy}
%   \small
%   \centering
%     \begin{tabular}{llll|r}
%     \hlinew{1.1pt}
%     MMSC & FA & CL & PP. & \multicolumn{1}{l}{MAP} \\\hline \hline
%     ×                      & ×               & ×                & ×                & 39.65                   \\
%     \checkmark\                      & ×               & ×                & ×                & 41.69                   \\
%     \checkmark                      & \checkmark               & ×                & ×                & 42.79                   \\
%     \checkmark                      & \checkmark               & \checkmark                & ×                & 43.22                   \\
%     \checkmark                      & \checkmark               & \checkmark                & \checkmark                & \textbf{43.88}       \\
%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table}

% In this experiment, we analyze the impact of each module.
% Since we define highlight detection task with learning with noisy labels, we first verify the impact of the multi-modality filter module on the experiment, and the subsequent experiments are conducted under the setting of noise filtering
% As shown in Tab.~\ref{tab:ablationstudy}


% In this experiment, we analyze the impact of each module.

% Since we define highlight detection tasks by learning with noisy labels, we first study the impact of the \textbf{Multi-modality Sample Filter} module on the experiment.
% We show the result in Tab.~\ref{tab:ablationstudy}. 
% The result demonstrates that using the multi-modality filter improves the performance movielights datasets.
% Therefore, the subsequent experiments are conducted under the setting of noise filtering.


% % \noindent\textbf{Multi-modality Filter.}
% % \label{subsec:Multi-modality filter.}

% % \noindent\textbf{Feature Augment.}
% % \label{subsec:Feature Augment.}
% We evaluate the impact of the \textbf{feature augment} module in {}.
% Compared to the naive concatenation, performance is improved with cross-modal feature augment.
% This shows preserving the feature augments leads to better representation.

% \noindent\textbf{Consistency Loss.}
% \label{subsec:Consistency Loss.}

% \noindent\textbf{Post Processing.}
% \label{subsec:Post processing.}


In this experiment, we analyze the impact of each module. The results are summaried in Tab.~\ref{tab:ablationstudy}.
%From Tab.~\ref{tab:ablationstudy}, we can observe that each ingredient exhibits a non-trivial significance in our approach. % removed by rqiao 1110

\noindent\textbf{Multi-modality Sample Cleaning.}
We first inspect the impact of the multi-modality sample filter module because we are primary concerned with learning with noisy labels in VHD. % modified by rqiao 1110
The $2\%$ performance gain from the module over baseline shows the importance of filtering noisy sample. %and using the multi-modality sample filter improves the performance MovieLights datasets. % removed by rqiao 1110
%Based on the above conclusions, the subsequent ablation experiments are conducted under the setting of noise filtering.
Based on the this observation, the subsequent ablation experiments are conducted under the setting of noise filtering. % modified by rqiao


\noindent\textbf{Cross-Propagation.}
%We then evaluate the impact of the feature augmentation module.
%Compared to the naive concatenation, the models with augmented features show superior performance.
%Compared to the naive concatenation, the models with augmented features show superior performance.
% The results validate that exploring the relations among continue shots and the cross-modal feature enhances the feature representation and significantly boosts performance.
% Our interpretation is that the feature augment module is introduced to guide the attention between different modal, encouraging enhance strong connections and prune the weak connections. 
%The results validate that   feature augment module can better explore the complementary information from different modalities and suppress the possible disturbtion during information transfer.
We then evaluate the impact of the feature augmentation module.
Compared to naive feature concatenation, the models with augmented features show superior performance.
% The results validate that exploring the relations among continue shots and the cross-modal feature enhances the feature representation and significantly boosts performance.
% Our interpretation is that the feature augment module is introduced to guide the attention between different modal, encouraging enhance strong connections and prune the weak connections. 
The results validate that feature augmentation module can better explore the complementary information from different modalities and suppress the mutual disturbance of desynchronized uni-modal information. % modified by rqiao

\noindent\textbf{Consistency Loss.}
%we examine our contribution of the consistency loss.
%The results demonstrate that the employment of the auxiliary constraint of multi-modal are further advanced in robustness.
%Keeping the modality specific information not only enhances uni-modal information but also guides a better fusion.
We examine the contribution of the consistency loss.
The results demonstrate that the employment of the auxiliary multi-modal constraint further increases the model robustness.
% Keeping the modality specific information not only enhances uni-modal information but also guides a better multi-modal feature fusion. 
This consistency loss not only implicitly enhances uni-modal information, but also explicitly guides the multi-modal branch to better learning.

\noindent\textbf{Post Processing.}
%We remove the post processing, as shown in Fig.~\ref{fig:postprocessing}, after a median filter, the probability curves are much ﬂatter.
%This result demonstrates that a filter is essential.
We compare the highlight prediction curves with and without the post processing median filter in Fig.~\ref{fig:filter_and_noise}(a,b), which prevents disruptive prediction variation and improves the overall performance. % modified by rqiao 1110



\begin{figure*}[t]
   \begin{center}
   \includegraphics[width=1.0\linewidth]{./figures/filter_and_noise_v3.pdf}
   \end{center}
      \vspace{-0.4cm}
      \caption{
         (a, b) Comparison of original prediction curve with filtered prediction curve.
         (c) Results on YouTube Highlights with noisy label.}
      \vspace{-0.3cm}
   \label{fig:filter_and_noise}
\end{figure*}


% \begin{figure}[t]
%   \begin{center}
%   \includegraphics[width=1.0\linewidth]{./figures/filter_v3.pdf}
%   \vspace{-0.8cm}
%   \end{center}
%       \caption{Comparison of the original prediction curve (top) with the filtered prediction curve (bottom).
% }
%     %   \vspace{-0.5cm}
%   \label{fig:postprocessing}
% \end{figure}

\subsection{Results on YouTube Highlights}
\label{subsec:Results on Youtube Highlights.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% by shuxj 
\begin{table}[!t]
    \caption{
     Results on YouTube Highlights.
   } 
    \small
	\centering  
	\label{tab:youtubehighligh}
	\renewcommand\arraystretch{1} 
	\vspace{-0.3cm}
    \begin{tabular}{p{2.1cm}|p{0.42cm}<{\centering}p{0.42cm}<{\centering}p{0.42cm}<{\centering}p{0.42cm}<{\centering}p{0.42cm}<{\centering}p{0.42cm}<{\centering}p{0.44cm}<{\centering}} 
    \hlinew{1.1pt} 
    Methods      & dog & gym. & park. & ska. & ski. & surf. & Avg. \\ \hline \hline
    GIFs~\cite{Video2gifCVPR2016}            & 30.8                    & 33.5                     & 54                        & 55.4                     & 32.8                     & 54.1                      & 46.4                     \\
    LSVM~\cite{RankingDomainspecificECCV2014}           & 60.0                    & 41.0                     & 61.0                      & 62.0                     & 36.0                     & 61.0                      & 53.6                     \\
    HighlightMe~\cite{HighlightMeICCV2021}           & 63                      & 73                       & 72                        & 64                       & 52                       & 62                        & 64                       \\
    MINI-Net~\cite{MININetECCV2020}           & 58.2                    & 61.7                     & 70.2                      & 72.2                     & 58.7                     & 60.1                      & 64.4                     \\
    CHD~\cite{ContrastiveLearningCVPR2022}           & 60.6                    & 71.1                     & 74.2                      & 49.8                     & 68.2                     & 68.5                      & 65.4                     \\
    Trail~\cite{LeziWangECCV2020}             & 63.3                    & 82.5                     & 62.3                      & 52.9                     & 74.5                     & 79.3                      & 69.1                     \\
    SL-Module~\cite{CrosscategoryICCV2021}           & 70.8                    & 53.2                     & 77.2                      & 72.5                     & 66.1                     & 76.2                      & 69.3                     \\
    Joint-VA~\cite{JointVisualandAudioICCV2021}          & 64.5                    & 71.9                     & 80.8                      & 62                       & 73.2                     & 78.3                      & 71.8                     \\
    PLD~\cite{LearningPixelLevelCVPR2022}          & 74.9                    & 70.2                     & 77.9                      & 57.5                     & 70.7                     & 79                        & 73                       \\
    CO-AV~\cite{ProbingVisualAudio2022}          & 60.9                    & 66                       & 89                        & 74.1                     & 69                       & 81.1                      & 74.7                     \\
    UMT~\cite{UMTCVPR2022}            & 65.9                    & 75.2                     & 81.6                      & 71.8                     & 72.3                     & 82.7                      & 74.9                     \\
    \textbf{CLC}(ours) & 70.5                    & 79.4                     & 83.9                      & 83.5                     & 79.5                     & 83.6                      & \textbf{80.1}      \\
    \hlinew{1.1pt}
   \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \begin{table}[]
%   \caption{
%   youtubehighligh \todo{method name and ref}.
%   }

%   \vspace{-0.3cm}
%   \label{tab:youtubehighligh}
%   \small
%   \centering
%   \begin{tabular}{l|rrrrrrr}
%   \hlinew{1.1pt}
%     methods      & \multicolumn{1}{l}{dog} & \multicolumn{1}{l}{gym.} & \multicolumn{1}{l}{park.} & \multicolumn{1}{l}{ska.} & \multicolumn{1}{l}{ski.} & \multicolumn{1}{l}{surf.} & \multicolumn{1}{l}{avg.} \\ \hline \hline
%     GIFs~\cite{Video2gifCVPR2016}            & 30.8                    & 33.5                     & 54                        & 55.4                     & 32.8                     & 54.1                      & 46.4                     \\
%     CHD~\cite{ContrastiveLearningCVPR2022}           & 60.6                    & 71.1                     & 74.2                      & 49.8                     & 68.2                     & 68.5                      & 65.4                     \\
%     SL-Module~\cite{CrosscategoryICCV2021}           & 70.8                    & 53.2                     & 77.2                      & 72.5                     & 66.1                     & 76.2                      & 69.3                     \\
%     HighlightMe~\cite{HighlightMeICCV2021}           & 63                      & 73                       & 72                        & 64                       & 52                       & 62                        & 64                       \\
%     MINI-Net~\cite{MININetECCV2020}           & 58.2                    & 61.7                     & 70.2                      & 72.2                     & 58.7                     & 60.1                      & 64.4                     \\
%     Trail~\cite{LeziWangECCV2020}             & 63.3                    & 82.5                     & 62.3                      & 52.9                     & 74.5                     & 79.3                      & 69.1                     \\
%     LSVM~\cite{RankingDomainspecificECCV2014}           & 49.5                    & 77.4                     & 65.1                      & 64.9                     & 49.2                     & 55.8                      & 64.9                     \\
%     Joint-VA~\cite{JointVisualandAudioICCV2021}          & 64.5                    & 71.9                     & 80.8                      & 62                       & 73.2                     & 78.3                      & 71.8                     \\
%     PLD~\cite{LearningPixelLevelCVPR2022}          & 74.9                    & 70.2                     & 77.9                      & 57.5                     & 70.7                     & 79                        & 73                       \\
%     CO-AV~\cite{ProbingVisualAudio2022}          & 60.9                    & 66                       & 89                        & 74.1                     & 69                       & 81.1                      & 74.7                     \\
%     UMT~\cite{UMTCVPR2022}            & 65.9                    & 75.2                     & 81.6                      & 71.8                     & 72.3                     & 82.7                      & 74.9                     \\
%     \textbf{CLC}(ours) & 70.5                    & 79.4                     & 83.9                      & 83.5                     & 79.5                     & 83.6                      & \textbf{80.1}      \\
%     \hlinew{1.1pt}
%   \end{tabular}
% \end{table}



%
% \noindent\textbf{Clean.} 

% \noindent\textbf{Noise.} 

\begin{table*}[]
   \caption{
   Results on YouTube Highlights with Noisy Label.
   }

  \vspace{-0.3cm}
   \label{tab:noisyyoutubehighligh}
   \small
   \centering
    \begin{tabular}{ll|l|rrrrrrl}
    \hlinew{1.1pt}
    Annotation & Noise        & Methods & \multicolumn{1}{l}{dog} & \multicolumn{1}{l}{gym.} & \multicolumn{1}{l}{park.} & \multicolumn{1}{l}{ska.} & \multicolumn{1}{l}{ski.} & \multicolumn{1}{l}{surf.} & \multicolumn{1}{l}{Avg.}         \\\hline\hline
    Harvested matched    & clean        & UMT~\cite{UMTCVPR2022}      & 65.90                   & 75.20                    & 81.60                     & 71.80                    & 72.30                    & 82.70                     & 74.90                                            \\
    Harvested matched    & clean        & CLC    & 70.51                   & 79.43                    & 83.85                     & 83.51                    & 79.46                    & 83.56                     & 80.05 ($\uparrow5.15$)            \\\hline
    Harvested borderline & slight noise & UMT~\cite{UMTCVPR2022}      & 65.93                   & 74.31                    & 81.58                     & 71.84                    & 70.24                    & 82.46                     & 74.39                                           \\
    Harvested borderline & slight noise & CLC    & 69.41                   & 80.73                    & 78.50                     & 85.36                    & 81.11                    & 83.16                     & 79.71 ($\uparrow5.32$)                \\\hline
    Mturk      & severe noise & UMT~\cite{UMTCVPR2022}      & 63.78                   & 76.16           & 75.02                     & 73.62                    & 69.99                    & 81.59                     & 73.36                                              \\
    Mturk      & severe noise & CLC    & 66.92                   & 80.44                    & 85.92                     & 82.33                    & 78.05                    & 81.72                     & 79.22 ($\uparrow5.86$)                     \\ 
    \hlinew{1.1pt}
\end{tabular}
\end{table*}


% \begin{figure}[t]
%   \begin{center}
%   \includegraphics[width=0.9\linewidth]{./figures/youtube_v1.pdf}
%   \vspace{-0.8cm}
%   \end{center}
%       \caption{Results on Youtube Highlight with noisy label. CLC consistently achieves the leading performance both in slight noise and severe noise and with the increase in noise level, we advantage by a comfortable margin.}
%     %   \vspace{-0.5cm}
%   \label{fig:youtube}
% \end{figure}


\begin{figure*}[t!]
   \begin{center}
   \includegraphics[width=1.0\linewidth]{./figures/vis_v4.pdf}
   \end{center}
      \vspace{-0.5cm}
      \caption{
         The highlight moments selected by our CLC from Skyfall and Sherlock Holmes. Top: Bond gives chase to a professional hitman by car to find a classified hard drive, and then a firefight erupted in the market. Bottom: Holmes and Blackwood are facing off, and then Blackwood reaches for a weapon to kill Holmes, but accidentally trips off a scaffolding and falls to his death.}
      \vspace{-0.5cm}
   \label{fig:visualization}
\end{figure*}



% Beyond the above evaluations, we conduct experiments on the public VHD benchmarks YouTube Highlight, including six domain video datasets, to verify the effectiveness of CLC.

% For \textbf{Harvested Highlight with matched}, the setting is consistent with the previous work[].
% As shown in Tab.~\ref{tab:youtubehighligh}, we achieve state-of-the-art performance on the YouTube dataset with CLC, which is outperform existing multi-modal works.
% We see a further gain of 5.2% when we use the proposed CLC. 
% Specifically, CLCarchitecture achieves best performance in three out of six categories, while maintaining reasonably good performance in the other three category.
% It supporting our claim that  highlight detection should be regarded as Learning with Noisy Labels.


%Beyond the above evaluations, we conduct experiments on the public video highlight detection benchmarks YouTube Highlight, including six domain video datasets, to verify the effectiveness of CLC.
We conduct experiments on the public video highlight detection benchmarks YouTube Highlights, including six domain video datasets, to verify the generalization ability of CLC. % modified by rqiao 1110

%For \textbf{Harvested Highlight with matched}, the setting is consistent with the previous work~\cite{UMTCVPR2022}.
%As shown in Tab.~\ref{tab:youtubehighligh}, we achieve state-of-the-art performance on the YouTube dataset with CLC, which is outperform existing multi-modal highlight detection works.
%Specifically, CLC architecture achieves best performance in three out of six categories, while maintaining reasonably good performance in the other three category.
%We see a further gain of $5.2\%$ when we use the proposed CLC and it supporting our claim that  highlight detection should be regarded as learning with noisy labels.

The setting of \textbf{Harvested Highlight with matched} is consistent with the previous work~\cite{UMTCVPR2022}.
As shown in Tab.~\ref{tab:youtubehighligh}, CLC achieves state-of-the-art performance on the YouTube Highlights, outperforming the existing multi-modal highlight detection methods in the average metric across all categories.
Specifically, CLC achieves best performance in three out of the six categories, while maintaining reasonably competitive performance in the other three categories. % modified by rqiao 1110, Please clarify the following commented sentence. I do not understand its purpose.
%We see a further gain of $5.2\%$ when we use the proposed CLC and it supporting our claim that  highlight detection should be regarded as learning with noisy labels. % removed by rqiao 1110
These results support our claim that  highlight detection should be regarded as learning with noisy labels.

% To better prove the effectiveness of CLC in different noise situations, varying noise would be structured in youtube highlights which are
% \textbf{Harvested Highlight with matched and borderline} 
% and
% \textbf{Mturk Highlight}.
% Tab.~\ref{tab:noisyyoutubehighligh}  exhibits the prediction accuracy of our CLC and UMT in different types of noise levels.
% Despite the lack of clean annotation, CLC still consistently achieves the leading performance both in slight noise and severe noise.
% Fig.~\ref{fig:youtube} reveals that with the increase in noise level, our CLC has more advantages.
% The remarkable superiority of our method indicates that CLC will achieve satisfactory results in multi-modal noisy datasets.

%To better prove the effectiveness of CLC in different noise situations, varying noise would be structured in youtube highlights which are
%\textbf{Harvested Highlight with matched and borderline} and \textbf{Mturk Highlight}.

To quantify how CLC is robust to different levels of label noise, we inspect the settings of \textbf{Harvested Highlight with matched and borderline} and \textbf{Mturk Highlight}, which are perturbed by varying degrees of label noise in YouTube Highlights. % modified by rqiao 1110


%Tab.~\ref{tab:noisyyoutubehighligh}  exhibits the prediction accuracy of our CLC and UMT~\cite{UMTCVPR2022} in different types of noise levels.
%With graded noise of annotation, highlight detection becomes more difficult.
%Despite the lack of clean annotation,  knowledge from noisy samples is still learned through our approach.

Tab.~\ref{tab:noisyyoutubehighligh} exhibits the performances of CLC and UMT~\cite{UMTCVPR2022} at different noise levels.
As the noise level increases, the VHD task becomes more difficult, but the performance superiority of our CLC over UMT becomes even more obvious. 
It is illustrated in Fig.~\ref{fig:filter_and_noise}(c) that compared with the most recent state-of-the-art UMT, our model can achieve better performance even when disturbed by severe label noise. % modified by rqiao
%The remarkable superiority of our method indicates that CLC will achieve satisfactory results in multi-modal noisy datasets.

% \subsection{Results on QVHighlights.}
% \label{subsec:Results on QVHighlights Highlights.}

% \noindent\textbf{Clean.} 

% \noindent\textbf{Noise.} 




\subsection{Visualization}
\label{Visualization}

% \begin{figure*}[t]
%   \begin{center}
%   \includegraphics[width=1.0\linewidth]{./figures/vis_v4.pdf}
%   \end{center}
%       \vspace{-0.5cm}
%       \caption{
%          The highlight moments selected by our CLC from Skyfall and Sherlock Holmes. Top: Bond gives chase to a professional hitman by car to find a classified hard drive, and then a firefight erupted in the market. Bottom: Holmes and Blackwood are facing off, and then Blackwood reaches for a weapon to kill Holmes, but accidentally trips off a scaffolding and falls to his death.}
%       \vspace{-0.5cm}
%   \label{fig:visualization}
% \end{figure*}


%As it is shown in Fig.~\ref{fig:visualization} and the supplementary material, we present some visual examples for video highlight detection in MovieLights.
%By multi-modal representations and cleaner highlight annotation, our framework can perform video highlight detection effectively.
% As shown in Fig.~\ref{fig:visualization} and the supplementary material, we present some visualization examples of the detected highlight clips in MovieLights by CLC. The examples clearly shows that the prediction of CLC is in accordance with user expectation. % modified by rqiao 1110
As shown in Fig.~\ref{fig:visualization}, we present some visualization examples of the detected highlight clips in MovieLights by CLC. The examples clearly shows that the prediction of CLC is in accordance with user expectation. % modified 
We will provide more examples in the supplementary material.
%By multi-modal representations and cleaner highlight annotation, our framework can perform video highlight detection effectively.








