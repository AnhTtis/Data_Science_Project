% !TEX root = main.tex
\section{Background and Related Work}
\label{sec:related}

\subsection{mmWave Radar \& Comparison with Lidar}

% \begin{wraptable}{R}{.3\textwidth}
%   \captionof{table}{Comparing AWR1843 mmWave radar with A2M8 Lidar.
%      \label{tab:rlCompare}
%    }
%   \begin{tabular}{|p{2.2cm}|p{0.8cm}|p{0.8cm}|}
%     \hline
%     & Radar &Lidar\\   \hline      
%     Dimension & 3D & 2D\\   \hline
%     Sensing range & 40m & 12m \\ \hline
%     Resolution & 5cm & 1cm \\ \hline
%     Acoustic noise & 3.2dB & 15dB \\ \hline
%   \end{tabular}

% \end{wraptable}

\begin{comment}
Most mmWave radars adopt the frequency-modulated continuous wave (FMCW) technology, which transmits a periodic radio frequency (RF) signal with linearly increasing or decreasing frequency in each period. The signal in a period that sweeps the whole bandwidth is called a {\em chirp}.
%Frequency Modulated Continuous Wave (FMCW) radar has proved to be advantageous over traditional Continuous Wave (CW) radar for its ability to measure very small ranges between the radar and the target and also its ability to measure the target range and its relative velocity  simultaneously.
%Specifically the mmWave radar transmits a linear chirp  and
%The mmWave radars' sensing process is as follows.
After receiving the signal reflected by the target, the FMCW radar mixes the transmitted signal and the received signal to generate the intermediate frequency (IF) signal.
% the received signals are mixed with the transmitted chirp to generate intermediate-Frequency (IF) signals.
FMCW radar uses the following approach to estimate the position of the target, i.e., $(r, \theta, \phi)$, in the spherical coordinate system originated at the radar. Because the target's radial range $r$ is proportional to the IF signal's frequency, it can be estimated by identifying the peak of the Fourier transform of a single chirp's IF signal. The $\theta$ and $\phi$, i.e., azimuthal and polar angles, can be obtained by estimating the received signal's angle of arrival using multiple antennas.
%For instance, the AWR1843 mmWave radar used in this paper has four receiving antennas.
FMCW radar estimates the target's radial velocity $v_r$ by $v_r = \frac{\lambda \cdot \Delta \psi}{4\pi \cdot T_c}$, where $\lambda$ is wavelength, $\Delta \psi$ is difference between the phases of two consecutive chirps' IF signals, and $T_c$ is chirp duration. The radar outputs a 3D Cartesian point cloud of all the targets in the field of view (FoV), where each point is associated with a radial velocity.
\end{comment}

\begin{comment}
When there are multiple targets at different ranges, each peak in the IF signal's frequency-domain representation corresponds to a target. To further distinguish the targets at the same range with different radial relative velocity,
%The radar-target distance can be calculated based on this IF signal, because the IF signal's frequency is proportional to the distance. In general, when there are multiple obstacles at different ranges in the field of view, a fast Fourier transform (FFT) is performed on the IF signal, resulting in a series of peak. Each peak after FFT represents one or more obstacles at a corresponding distance. We refer the above process as Range-FFT. To further distinguish the obstacles at the same range with different speed,
\todo{the radar can leverage the phase difference in the Range-FFT result. Assuming a 
target with a constant speed $v$ during the chirp duration $T_c$, 
the two neighbors with phase difference $w$ will appear on range-FFT.
Specifically, $w$ is equal to}
\begin{equation*}
  w = \frac{4\pi* v*T_c}{\lambda}
\end{equation*}
\end{comment}
%Compared with the unmodulated continuous wave radars, FMCW radars (e.g., mmWave radars) are more advantageous in maintaining accuracy when the $r$ is small and the capability of directly measuring $v_r$\cite{zhang2021comprehensive}.



% \begin{figure}{L}
%   %\hfill
%   \begin{minipage}{.3\textwidth}
%     \captionof{table}{Comparing AWR1843 mmWave radar with A2M8 Lidar.
%       \label{tab:rlCompare}
%     }
%     %\vspace{-1em}
%     \begin{tabular}{|p{2.2cm}|p{0.8cm}|p{0.8cm}|}
%       \hline
%       & Radar &Lidar\\   \hline      
%       Dimension & 3D & 2D\\   \hline
%       Sensing range & 40m & 12m \\ \hline
%       Resolution & 5cm & 1cm \\ \hline
%       Acoustic noise & 3.2dB & 15dB \\ \hline
%     \end{tabular}
%   \end{minipage}
%   \hspace{5em}
%   \begin{minipage}{.27\textwidth}
%     \includegraphics[width=\linewidth]{figures/density_compare}
%     \vspace{-2.5em}
%     \caption{Radar/lidar's point cloud density versus target range.}
%     \label{fig:denstiyCompare}
%   \end{minipage}
%   %\hfill
%   %\vspace{-1em}
% \end{wrapfigure}

\begin{figure}
  \centering
  \begin{minipage}[t]{.44\linewidth}
    \centering
    \vspace{0pt}
    \begin{tabular}{lll}
      \hline
       & \begin{tabular}[c]{@{}l@{}}AWR1843\\ radar\end{tabular} & \begin{tabular}[c]{@{}l@{}}A2M8\\ lidar\end{tabular} \\ \hline
      Dimension & 3D & 3D \\
      Range & 40m & 12m \\
      Resolution & 5cm & 1cm \\
      Noise & 3.2db & 15db \\ \hline
      \end{tabular}
      \vspace{0.2em}
      \captionof{table}
    {%
      Comparison between AWR1843 mmWave radar and A2M8 lidar.
      \label{tab:rlCompare}%
    }
    %\vspace{0.2em}    
  \end{minipage}
  \hspace{2em}
  \begin{minipage}[t]{.46\linewidth}
    \centering
    \vspace{0pt}
    \includegraphics[width=\linewidth]{figures/density_compare}
    \vspace{-2.8em}
    \captionof{figure}
      {%
        Radar/lidar's point cloud density versus target range.
        \label{fig:denstiyCompare}%
      }%
    \end{minipage}
    \vspace{-1em}
\end{figure}

% \begin{wrapfigure}{R}{.3\textwidth}
%   \includegraphics[width=\linewidth]{figures/density_compare}
%     \vspace{-2.5em}
%     \caption{Radar/lidar's point cloud density versus target range.}
%     \label{fig:denstiyCompare}  
% \end{wrapfigure}

An mmWave radar can output a three-dimensional (3D) Cartesian point cloud of all the targets in the field of view (FoV), where each point is associated with a radial velocity.
%The mmWave radar sensing belongs to the broader RF imaging.
%RF signals with various wavelengths (e.g., $12.5\,\text{cm}$ for 2.4GHz Wi-Fi, millimeters for mmWave radars, micrometers to sub-micrometer for lidars and cameras) can be used for imaging.
%\todo{From the Rayleigh criterion \cite{urone2012limits}}, the angular resolution limit, denoted by $\theta_{\min}$, follows $\theta_{\min} \propto \frac{\lambda}{D}$, where $D$ is the baseline length (e.g., the diameter of the lens' aperture, the distance between receiving antennas). As the baseline lengths for different RF imaging approaches are often similar due to form factor constraint,
%The wavelength of the RF signal is a main factor of the imaging resolution limit. Due to Wi-Fi's decimeter wavelength, Wi-Fi imaging can only achieve decimeters resolutions \cite{huang2014feasibility}.
%The high resolutions of cameras raise privacy concerns.
Lidar and mmWave radar are often competing technologies in various applications. Lidars' higher susceptibility to occlusion, due to their short wavelengths, makes them less suitable for moving people tracking. In addition, we provide a brief comparison between the AWR1843 mmWave radar used in this paper and the A2M8 360\textdegree{} lidar used on a robot to gain more insights. The list prices of these two devices are similar. Table~\ref{tab:rlCompare} shows their sensing dimensions, ranges, resolutions, and acoustic noise levels during operation. The AWR1843 radar outperforms the A2M8 lidar except on resolution. However, the radar's $5\,\text{cm}$ resolution is satisfactory for interpersonal distance tracking.
%Note that the lidar's noise is mainly from its mechanical rotation.
We also measure the two devices' point cloud densities when the target's radial range varies. Fig.~\ref{fig:denstiyCompare} shows the result. As radar is mainly based on specular reflection, its point cloud density is insensitive to the target range. Differently, as lidar is mainly based on diffuse reflection, its point cloud density decreases with the target range. The attenuation may create a challenge in system design. Thus, although lidar-based interpersonal distance tracking is possible, we design ImmTrack based on mmWave radar.

%though the radar point cloud is sparse due to the specular reflection, the point cloud density is less sensitive to distance. However, most of the lidar reflection is diffuse, so the scattering effect is significant in long range.



\subsection{Related Work}

\subsubsection{Wireless localization, target identification, and IMU tracking}
\yimin{
Wireless indoor localization has received extensive research in the last two decades. Except camera-based surveillance, the device-free approaches in general suffer the anonymity problem in the multi-target setting \cite{xiao2016survey}.
Recently, wireless signals are used for designing device-free systems that perform both target tracking and identification. For instance, the studies \cite{zeng2016wiwho,wang2019wipin} design human subject identification systems using Wi-Fi signal. However, the used channel state information is unstable in various environments. In addition, the studies \cite{yang2020mu,zhao2019mid,pegoraro2020multiperson} train a deep learning model for human subject identification using mmWave radar signal and achieve accuracy above 92\%. However, the required extensive training data collection introduces high overheads in practice. Moreover, these systems \cite{yang2020mu,zhao2019mid,pegoraro2020multiperson} do not perform well with increased number of human subjects. This is because the distinctiveness among the radar data features is weakened when the number of human subjects increases.}
%RF imaging, which can be used for device-free localization in the single-user setting, faces the same problems in the multi-target setting.
The device-based approaches, in which each target carries a signal transmitter/receiver, are free of the anonymity issue. However, as discussed in \sect\ref{sec:intro}, the device-based approaches using various modalities have respective limitations.

%The paper \cite{eichelberger2017indoor} provides a summary of the existing wireless indoor localization approaches in terms of their requirements.

% Then researchers explore  new localization approaches which leverage  electromagnetic radiation signals for its wide sensing range, which can provide a global view. Generally, the signals can be divided into three types based on their wavelength: light wave, radio wave, and sound wave.

Besides Bluetooth, acoustic sensing is another candidate for neighbor discovery and ranging. The BeepBeep system \cite{peng2007beepbeep} performs ranging between two smartphones with audible acoustic signals.
% Using acoustic signals to perform ranging among mobile devices has been studied a decade ago \cite{peng2007beepbeep}.
However, the beeps in continued use is annoying.
%In addition, the operational range is about $10\,\text{m}$ only.
%A recent study \cite{allegro2022acoustic} uses a home loudspeaker to transmit acoustic beacons in the near-inaudible band (i.e., $18\,\text{kHz}$ to $22\,\text{kHz}$) for a smartphone to perform ranging. The operational range is just $2\,\text{m}$ if the required ranging error is $10\,\text{cm}$.
When adapting to the near-inaudible frequency band, the operational resolution becomes unsatisfactory as the inter-device distance increases, because the smartphone audio systems are not designed to work in the inaudible band.
The studies \cite{yun2017strata} and \cite{han2016amil} that use the near-inaudible band manage to evaluate their systems when the inter-device distance is up to $0.4\,\text{m}$ and $1.2\,\text{m}$, respectively.
%Although the work \cite{yun2017strata} achieves subcentimeter resolution when the inter-device distance is within $0.2\,\text{m}$, it only evaluates the system when the distance is up to $0.4\,\text{m}$.
Thus, inaudible acoustic ranging is limited to near-field scenarios.

%Thus, near-inaudible acoustic ranging is not promising for interpersonal distance tracking.

\begin{comment}
Before comparing the pros and cons among different technologies used the emerging indoor localization and tracking system, we define the following criterias for comparison

%a system which requires the people to wear extra sensors or a system which undermines the user's privacy or disturb user's normal life would be invasive.
%, including the cost of the sensor itself, the cost of careful calibration before deployment, whether the sensor needs extensively data collection to fine-tune its system parameters before usage.
\begin{enumerate}
    \item Resolution: the accuracy with which the user or device position is obtained.
    \item Invasiveness: the disturbance  of deploying the system
    \item Robustness: the system's performance in different environments and under the interference.
    \item Deployability: the cost of the deploying the sensor.
    \item Scalability: the system's ability to localize and track multiple people.
\end{enumerate}

The common used light wave based sensors include Lidar, camera and some other sensors which use visible light such as Led. 
The short wavelength of the light wave contributes to the potential of high resolution of light wave sensors.  Moreover,  The light based sensors
can work on multiple people easily due to their large perception range and simpler processing algorithms compared to RF based technologies.  However, the visible light can disturb 
people and the camera will largely undermine the privacy. 
The physics of light determines the sensors require line of sight and their performance will significantly drop when obstacles appear or under the interference from other light sources.
In general, the light wave sensors are accurate , scalable but not robust. They can aslo cause large disturbance.
% With continuous development in deep learning, even a cheap camera can reach a sub-10cm accuracy.And for visible light, angle of arrival(AoA) is used for localization the person and object and can also reaches sub-10cm resolution.

On the contrary, the radio and sound wave  based technologies, such as IEEE 802.11 (WIFI), bluetooth, are more robust due to the longer wavelength.  
They can also reach high resolution with more complex processing algorithms, which undermines its scalability.

On the other hand,  the WIFI and sound based technologies require
the sensor to build the environment map before deployment. 

Here we choose to use mmWave as it combines the advantages of light wave based and radio wave based technologies. It can reaches high
resolution and will not disturb the passing people. According to our measurement, it only generates 3.2 DB noise when using while the Lidar generates up to 15 DB noise. 
As it is also a radio wave based sensor, it is robust under interference. Moreover, it can be deployed without extra data collection like the Lidar.
\end{comment}

% \todo{comparison between Lidar and radar}
% \begin{table}[!h]
% \caption{Comparison between radar and Lidar with similar price}
% \label{tab:rlCompare}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% Model                                                     & \begin{tabular}[c]{@{}c@{}}Range(m)\\ (min,max,res)\end{tabular} & Coordinate & \begin{tabular}[c]{@{}c@{}}Noise Level \\ (db)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Cost\\ (usd)\end{tabular} \\ \hline
% \begin{tabular}[c]{@{}c@{}}RPLidar \\ A2M6\end{tabular}   & (0.2,16,0.02)                                                           & 2D         & 15                                                          & 348                                                  \\ \hline
% \begin{tabular}[c]{@{}c@{}}AWR1843 \\ mmWave\end{tabular} & (0.1,40,0.04)                                                           & 3D         & 3.2                                                         & 370                                                  \\ \hline
% \end{tabular}
% \end{table}

%Because GPS does not work well indoors due to its low signal
%strength,  new methods needed to be designed for indoor localization and tracking.

IMU can be used to track user's movements by dead reckoning. Embedding the resulting trajectory into the global coordinate system requires either the global coordinates of at least one point on the trajectory or certain prior knowledge like the spatial constraints expressed in the global coordinate system that any trajectory is subjected to. Dead reckoning suffers from the error accumulation problem.
%To manage this problem, certain restrictions or sophisticated mechanisms are needed.
%For instance, the work \cite{tian2015enhanced} requires that the smartphone's orientation aligns with the user's motion direction.
A recent study \cite{ridi} applies machine learning to improve the accuracy of dead reckoning, which, however, requires massive training data and suffers domain shifts \cite{imuAdaption}.
% achieves over 90$\%$ accuracy on trajectory length estimation via step counting if the smartphone's orientation aligns with the user's motion direction. The recent studies \cite{ridi,ronin} propose learning-based approaches to advance the accuracy of the double integration in dead reckoning. However, they are subject to domain shifts \cite{imuAdaption} and require extensive training data collection. ImmTrack only uses a short-term dead reckoning result from the IMU.
Therefore, IMU is better for complementing other sensing modalities that can perform localization in the global coordinate system. ImmTrack uses IMUs to re-identify the mmWave radar sensing results. As ImmTrack only requires IMU's short-term dead reckoning result, it is not sensitive to the IMU dead reckoning's error accumulation problem.

\begin{comment}
The most straight forward method is using the widely adopted Inertial and magnetic Unit(IMU)
It has been widely studied as it is cost-efficient and also energy-efficient. 
A typical approach leverages the impressive performance of step counting algorithms using IMU and then multiply the step counts with a fixed step length. 
The  works such as \cite{2017step-count} using the approach  and achieves reasonable result due to a carefully  monitored constraint: the device orientation should be aligned with the motion direction. 
In recent years, data-driven approaches, such as \cite{ridi,ronin} provide a new paradigm to solve the problem and demonstrate its great potential.
However, it requires extensively data-collection and introduces the domain adaption problem\cite{imuAdaption}.
Pehaps the most adoped solution is the double integration approach, which integrates the accelerations to estimate the speed of the subject and then further integrates to estimate the position information. 
The drawback of using an IMU is that it lacks the global view to transfer the result to a unifom coordinate system.
\end{comment}

\subsubsection{Multi-modality data processing}

The existing works can be classified into the following three broad categories.

%In general,sensor fusion refers to the process of combining sensor data obtained or derived from a set of heterogeneous or homogeneous sensors \cite{SensorFusionBook}. Previous applications focus on sensing a single variable or deploy a set of homogeneous sensors, so the information can be directly combined. 


%Unlike previous applications where limited sensors are available, in the era of  Internet-of-Things (IoT), sensing devices become pervasive and the widely deployed devices can provide complementary sensing modalities.  It requires the fusion system to find the proper subset of information to combine

%In recent years, there is a series of work To leverage the information provided by the cross-modality sensors. In general, the solutions provided in the works can be divided into two categories. 


\textbf{Cross-modality data translation} generates synthetic data in the target modality from real data in the source modality.
%The generation based method solve the problem by transfering the data in one sensor to the same format of the other modality sensor.
% The rapid progress in applications like object classification,human activity recognition, motion tracking in computer vision not only benefits 
%  from the the advance in machine learning  but also benefits from the largely and easily accessible dataset, which 
%  Comparing to data collection with camera or microphone, 
%  collecting with sensors like radar or inertial sensor requires more carefully calibration and more complicated post-processing pipeline.  
The studies \cite{videoImu,kwon2020imutube,rey2019rgbV2Imu} generate synthetic IMU data from videos of human activities. 
% addresses the problem by synthesizing inertial sensor data in various human activities from the video frame with the help of deep learning tools. Similarly,
The work \cite{vid2doppler} generates mmWave radar data from videos. Since computer vision techniques can be employed to recognize the human activities from the videos, the synthetic IMU or mmWave radar data can be automatically labeled and used to train human activity recognition (HAR) models. %Note that IMU-/mmWave-based HAR often faces the training data scarcity problem.

\textbf{Multi-modal data fusion} fuses data from complementary modalities at the feature level or score/decision level to improve the robustness of sensing.
%maintain the overall sensing performance when the conditions of the sensed target or the environment vary \cite{SensorFusionBook}.
The work \cite{shuai2021millieye} fuses camera and mmWave radar to manage their respective limitations for robust object detection. Fusing camera, lidar, and radar data has been studied in the context of autonomous driving. The milliEgo system \cite{lu2020milliego} improves the accuracy of trajectory reconstruction by fusing mmWave radar data and IMU data in the single-user setting.

\textbf{Cross-modality data association} associates the sensing results in different modalities to increase information about the monitored process. The work \cite{kempfle2021quaterni} matches body-worn IMU data traces with the body joints recognized by a camera. The work \cite{ruiz2020idiot} applies the same approach to re-identify the body-worn IMUs from the video. The studies \cite{rgbw,eyefi,identitylink} associate camera data with Wi-Fi data for various purposes of augmenting the camera with depth information \cite{rgbw} or simultaneous human subject identification and tracking \cite{eyefi}. The work \cite{liuvi} associates users' smartphone Wi-Fi fine timing measurements and IMU data with a camera footage.
% \textbf{Middleare Based Method} The middleare based method jointly transfer the data from diffent modalities to the same format. Works proposed in \cite{kempfle2021quaterni,ruiz2020idiot} attempt to jointly utilize the information provided by the cross-modality sensors. For example, work proposed in \cite{ruiz2020idiot} transfers both the  inertial and magnetic unit (IMU) data and camera image to body joint position By matching the data together, the IMU data can be further utilized in 3D pose estimating and tracking by extending the 2D data from camera to 3D.

ImmTrack belongs to the cross-modality data association category. Different from the existing studies \cite{ruiz2020idiot,rgbw,eyefi,identitylink,liuvi} that use camera as an association source, we employ mmWave radar that is less privacy-intrusive. Moreover, technically, mmWave radar directly provides 3D locations and velocities of the human subjects, which facilitate the association.
%However, there are still challenges to be solved, which will be detailed in Section~\ref{sec:problem}.

%Our work also follows the middleare based method for associating the mmWave and IMU. Previous efforts such as IdentityLink and IDIoT \cite{identitylink, ruiz2020idiot, eyefi, rgbw} use RGB camera as the ambient sensor and leverage vision-based approaches to differentiate people.  In this paper, we propose to use mmWave radars, instead. We have the following considerations. First, compared to RGB cameras, the outputs from mmWave radars contain less sensitive information such as the appearance. Second, mmWave radars can be hidden behind the furniture, which is more non-intrusive and aesthetic. In addition, mmWave radars can directly provide the velocity and depth information of objects which is helpful for tracking the absolute 3D positions of objects. 
%For the portable sensor attached to objects, we utilize the inertial and magnetic unit (IMU) which has been widely integrated in common mobile phones, earphone, or smart glasses. 


% \subsection{Challenges}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: