\subsection{System Performance}
To thoroughly evaluate the system performance in real-world scenarios, we recruit seven volunteers and collect data over 3 hours in both indoor and outdoor environments, as shown in Figure \ref{fig:different scene}.
% In this section, we present the system performance on both association accuracy in each window (W-ACC) and the end-to-end association accuracy. 
In the following, we first introduce the baseline and metric, and present the overall performance in Section \ref{main_acc_result}. Then, we conduct experiments in two practical setting, when there are passengers or the number of people changes continuously over time. Their results are presented in Section \ref{eval_passenger} and Section \ref{eval_change_people}, respectively. 

% We also evaluate the system under two challenging scenarios, where there are passengers or the number of people changes continuously over time.  
% Different real-world settings are involved in this Section:
% \begin{enumerate}
%   \item  All users enable their IMU while nobody leaves or enters the area.
%   \item  All users enable their IMU while someone leaves or enters the area.
%   \item  Part of the users enable his/her IMU  while some passengers who do not enable IMU pass by. 
% \end{enumerate}

% Our collect the data from  includes 3 locations, including stadium, football field, office meeting room. 
% We also include two scenarios: all the people walk simultaneously and the number of people stays the same, 
% in the other case, we will vary the number of people and let some people enter or leave during the data collection. 



\begin{figure}
\centering

\begin{subfigure}{.24\textwidth}
  \centering
  \includegraphics[width=.89\linewidth]{figures/scene1}
 % \caption{Case 1: users walks in trajectories with different shapes in the same speed }
  \label{fig:scene1}
\end{subfigure}%
\begin{subfigure}{.24\textwidth}
  \centering
  \includegraphics[width=.89\linewidth]{figures/scene2}
  
  \label{fig:scene2}
\end{subfigure}
\caption{Different Data Collection Environments.}
\label{fig:different scene}
\end{figure}


\subsubsection{Baseline}
We introduce two baselines for comparison: UniverSense \cite{pan2018universense} and iCTrack. \textcolor{red}{[make sure names are consistent in figure and text]}

\noindent \textbf{UniverSense}. It is a pairing method between IoT devices with an IMU and another edge device equipped with a camera. When users in front of the camera are waving the IoT device, the IMU and camera can both estimate the acceleration of the IoT device, and establish the pairing by comparing the acceleration signals. 
% However, the acceleration-based matching solution does not work for mmWave 
% due to the sparisty of the point cloud and we can not tell which part of the point cloud 
% which does not hold in our system because the IMU caputures the acceleration of the arms and the mmWave focuses on the whole body part. In consideration of this point, 4
In this paper, we follow most of the designs of UniverSense but with two adaptations. 
First, we use the mmWave radar instead of the camera as the ambient sensor. Second, we leverage the velocity tracked by the radar and the velocity estimated from the IMU tracking module for pairing, rather than acceleration, considering that mmWave radars directly provide accurate velocity estimation of the point cloud. 
Similar with UniverSense, we transform the acquired velocity sequence data into fingerprints using binarization with a threshold $\alpha$. Then a similarity score of every pair of fingerprints will be calculated, which will be used for final association.

\noindent \textbf{iCTrack}.
We implement a visual multi-object tracking system using camera, with YOLOv4 \cite{yolov4} as the object detector to generate bounding boxes for every frame and Deep SORT \cite{deepsort} to associate candidate boxes in adjacent frames. Deep SORT is a popular tracker that utilizes the deep representation extracted by the feature extractor for bipartite matching. In our implementation, the feature dimension for each bounding box is 416. 
% After that, we apply the Deep SORT, an extension to SORT (Simple Real-time Tracker) for cluster association and tracking.
The tracking system can provide 2D location of each detected person in the image coordinate. We then transfer the 2D location into the world coordinate for the global trajectory generation. Finally, we match those global trajectories with the per-IMU ones using our proposed trajectory-based matching algorithm.
For clarity, we name this baseline as \textbf{iCTrack} where “i” stands for IMU and “C” for camera. 


\subsubsection{Metric}
A common metric for the association accuracy is the ratio of the number of correctly associated pairs to the total number of subjects. We denote this \underline{acc}uracy in each \underline{w}indow (\textbf{W-ACC}) by: 
$$
\text{W-ACC} = \frac{\# \text{correct pairs }}{\# \text{users}}
$$
\textbf{W-ACC} indicates the performance of association only using the information within one time window. 
Moreover, as mentioned in Section \ref{oneshot_id_ass}, we augment the trajectory-based matching by aggregating the distance matrix in multiple time windows with a soft voting algorithm. 
We call the matching accuracy obtained with the soft voting end-to-end accuracy, or \textbf{E2E-ACC} for short. 
However, as we will discuss in Section \ref{main_acc_result}, a higher W-ACC does not necessarily mean a higher E2E-ACC, as there are other factors that influence.


\subsubsection{Overall Association Performance} \label{main_acc_result}

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.225\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/camera_error_before_mos}
      \caption[]%
      {{\small View of camera before person 5 is occlulded}}    
      \label{fig:camViewBef}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.225\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{figures/error_camera_after_mos}
      \caption[]%
      {{\small After occlusion, camera assigns a new ID to person 5 }}    
      \label{fig:camViewAft}
  \end{subfigure}
  % \vskip\baselineskip
  \begin{subfigure}[b]{0.225\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{figures/PointCloud_v1}
      \caption[]%
      {{\small view the mmWave point cloud along Y axis, some of the clusters  can not separated from each other}}    
      \label{fig:pcView1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.225\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{figures/PointCloud_v2}
      \caption[]%
      {{\small view the mmWave point cloud along X axis, all the clusters can be separated along Y axis }}    
      \label{fig:pcView2}
  \end{subfigure}
  \caption[ ]
  {\small Illustration of the perception view of cameras and mmWave radars. } 
  \label{fig:viewCamera}
\end{figure}


In Figure \ref{fig:accEachWin} and \ref{fig:Finalfix}, we present the W-ACC and E2E-ACC of proposed ImmTrack system and other two baselines in the whole dataset including both indoor and outdoor environments.
The E2E-ACC of ImmTrack reaches $93.3\%$ in the best case and $83.3$\% in the worst case. 
Moreover, when the number of people is greater than three, ImmTrack even outperforms iCTrack on E2E-ACC, although its W-ACC is still slightly lower than that of iCTrack. 
In Figure \ref{fig:final2scene}, we show the environment-specific performance. iCTrack exhibits higher accuracy in indoor environment due to less dynamic illumination conditions, while the performance of ImmTrack does not show a clear discrepancy in two environments.

There are mainly two reasons why in some indoor cases, ImmTrack can even outperform iCTrack, a camera-based tracking system.
First, the camera relies on the image features for discriminating people. However, when occlusion occurs, which is very common in the indoor cases, the camera can only perceive part of the person. As a result, the generated features for the occluded person could be different from those with a full view of the body, and thus the camera tracking system may regard it as a new person and assign a new ID. Figure\ref{fig:camViewBef} and \ref{fig:camViewAft} illustrate an example that the camera regards the partial occluded person 5 as a new person 6 after the occlusion.

The critical drawback of the pure shape based feature is that the feature vector is independent of the previous one. However, for point cloud, we embed the feature of location, velocity and shape to make the feature coherent in time.

% is that it can only perceive information in 2D. Then when it is occulded in one angle, the camera can only gather 
% partial information and 
In addition, the mmWave perception view is 3D compared to camera, which can only perceive 2D view.
We show the  point cloud at the figure\ref{fig:pcView1} when the radar senses the same scene as shown in figure\ref{fig:camViewBef}. However, the radar is able to 
collect 3D information and based on figure\ref{fig:pcView2}, the cluster is easy to divide from the Y-axis. Therefore, the information loss is less compared to image when 
occlusion occurs

% the 3D point cloud that mmWave collect is more suitable for distinguishing
% the people in the field of view compared to the 2D image in camera


\begin{figure}
  \begin{subfigure}{0.14\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/end-to-end-accuracy-new}
    \caption[]%
    {{\small Overall W-ACC}}    
    \label{fig:accEachWin}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.14\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/finalMatch-fix}
   \caption{Overall E2E-ACC}
    \label{fig:Finalfix}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{.14\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/finalMatch-fix-2scene}
    \caption{Indoor and Outdoor E2E-ACC.}
    \label{fig:final2scene}
  \end{subfigure}

  \caption{Overall association performance of competing approaches. The total number of people is seven.}
  \label{fig:accFinal}
  \end{figure}


\subsubsection{W-ACC under Different Settings.} \label{eval_passenger}
  
\begin{figure*}[t]
  \centering
  % \begin{subfigure}[b]{0.235\textwidth}
  %     \centering
  %     \includegraphics[width=\textwidth]{figures/end-to-end-accuracy-new}
  %     \caption[]%
  %     {{\small 7 people without passenger}}    
  %     \label{fig:accEachWin}
  % \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.235\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{figures/end-to-end-accuracy-with-pass-5people}
      \caption[]%
      {{\small 5 people with passengers }}    
      \label{fig:accEachWin5Pass}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.235\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{figures/end-to-end-accuracy-with-pass-6people}
      \caption[]%
      {{\small 6 people with passengers }}    
      \label{fig:accEachWin6Pass}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.235\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{figures/end-to-end-accuracy-with-pass-7people}
      \caption[]%
      {{\small 7 people with passengers }}    
      \label{fig:accEachWin7Pass}    
  \end{subfigure}
  \caption[ ]
  {\small Matching Accuracy in each
  window (W-ACC) of competing approaches under different settings. \todo{What are the meanings of the bar and the error bar?} } 
  \label{fig:accEachWinAll}
\end{figure*}


%We use the precision score as the evaluation metric, where the true postive reprenets the number of pairs that are matching correctly.
We present the average W-ACC under different number of people in Figure\ref{fig:accEachWin}. The matching accuracy is an indicator of the overall performance of the mmWave tracking moduel, IMU tracking module and the trajectory based matching algorithm. 
First of all, as shown in Figure\ref{fig:accEachWin}, accuracy for matching based on the velocity profile drops significantly when 
the number of people increases. Its performance is close to random guessing when the number of people is seven.  The possible  reason is the velocity of the whole body part
is a feature that changes frequently, but in the long term, most of the values are nearly the same for different people.

% The reason behind is that   in an short period 
% velocity of differnt person's does not 
% have a large difference. On the other hand, the large discrepancy between the IMU velocity pofile and the mmWave velocity profile due to the different sensing part makes the two profile
% different even for the same person.

\todo{Discuss on if we need to modify the name of camera system}
On the other hand, we observe that the W-ACC of ImmTrack system is above 72.5$\%$ even for 7 people. Its performance
is only slightly lower than iCTrack, which indicates that in general, our mmWave tracking module 
can correctly identify and localize multiple people.We also observe that
the W-ACC of ImmTrack drops to zero in some extreme cases. It is mainly because of the occlusions among different people and there will be fewer or near to zero points for the occulded person. 

% \subsubsection{Influence On The Passengers}
Here We also evaluate the influence of the number of passengers (i.e, the number of people who do not participate in our system) on the W-ACC.
We use the same experiment process while let different number of participants  shut their IMU to act as passengers. 
The main influence on the passengers are the clustering result.
Refer to figure\ref{fig:accEachWin5Pass},\ref{fig:accEachWin6Pass},\ref{fig:accEachWin7Pass},  the passengers do not influence on the iCTrack's performance much, which is expected.
On the other hand the performance drops slightly  for about ImmTrack. However, we also don't see a clear relationship between 
the ratio of passengers with the  accuracy loss. Thus, we assume that the performance loss is due to some extreme case where the 
passengers are very close to the participants.

\todo{ track with single mmwave goes here}








%=======================================================
\subsubsection{Performance when number of people is changing} \label{eval_change_people}
In this experiment, we evaluate the system’s performance when
the number of people changes during a period.
We recruited 5 participants and let two of them enter first,
followed by another one after 1 minute. Then we let the other two people enter after 2 minutes.
After all of them continuously walk for 3 minutes, we let them leave on by one every minute in reverse order as they enter.

Refer to figure\ref{fig:Finalchange}, both of the ImmTrack and iCTrack system's accuracy increases compared to the case when number of people is fixed. Specifically, the accuracy for 
ImmTrack reaches 90$\%$.
We show a deep analysis on the timeline of a specific workflow when the accuracy is 100$\%$ in figure\ref{fig:assocTimeline}.
We can find that when the first two people enter the field of view, they are already associated with the IMU ids before 
the third person enters. Then the third person is also associated before the remaining two people enter since there is only one IMU.



\begin{figure}
\centering
\begin{subfigure}{.12\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/finalMatch-changing}
 \caption{xxxx}
  \label{fig:Finalchange}
\end{subfigure}%
\hfill
\begin{subfigure}{.34\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/time}
  \caption{xxx}
  \label{fig:assocTimeline}
\end{subfigure}
\caption{xxxx}
\label{fig:various-people}
\end{figure}
 