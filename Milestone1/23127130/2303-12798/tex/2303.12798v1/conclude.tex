% !TEX root = main.tex
\section{Conclusion}
\label{sec:conclude}

This paper presents ImmTrack, an interpersonal distance tracking system using one or more low-cost mmWave radar(s) and the IMUs of the users' smartphones. By associating the users' trajectories reconstructed from the mmWave radar and IMU sensing in terms of the trajectory features extracted by a Siamese neural network,
%ImmTrack features a novel IMU-assisted mmWave radar tracking algorithm for generating trajectories of each user in the global coordinate, and a learning-based association module to match the radar sensing results with per-user trajectories from IMUs. Based on the association results,
ImmTrack transfers the users' pseudo identities tagged to the IMU data to the radar's global-view sensing results.
Extensive experiments with up to 27 people show that ImmTrack achieves similar tracking accuracy and lower computation overhead compared with the more privacy-intrusive camera surveillance. ImmTrack achieves decimeters-seconds spatio-temporal accuracy in tracing contacts, outperforming the prevailing Bluetooth neighbor discovery approach that suffers inaccurate distance estimation and up to 80 seconds discovery delays in our experiments.

% In this case, the cross-modality collaboration is also required. 
% Specifically, to prompt such a message, the wearable device needs a global ID map, which contains the information of neighboring people and its own position. An edge node connected with an ambient sensor is responsibility for providing such a global ID map by associating the objects in the global view and the IDs of wearable devices.



% Experiments on the public and a new real-world multimodal HAR dataset show that, Cosmo achieves over 30\% accuracy improvement to state-of-art baseline approaches, and can converge much faster than conventional supervised fusion learning.
% In the first stage, we design a fusion-based contrastive learning framework that trains the unimodal feature encoders to learn consistent information from unlabeled multimodal data. In the second stage, we design a quality-guided attention fusion module to capture complementary information of different modalities based only on limited labeled data, and combine it with consistent information learned by contrastive learning through an iterative fusion learning strategy.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: