




\subsection{Global Tracking}
\subsubsection{Preprocessing}
The \textit{global tracking module} starts with removing static points.
The static removal  is to exclude the static background and preserve the moving objects. 
The static points can be detected by applying doppler fourier transform on the received data, whose results are the velocity of the detected subjects.
Then we will feed the processed point cloud to the \textit{Heuristic Clustering} module.


\subsubsection{Heuristic Clustering}
\begin{figure}
    \centering
    
    \begin{subfigure}{.24\textwidth}
      \centering
      \includegraphics[width=.89\linewidth]{figures/PointCloud_cluster_phase1}
      \caption{xxx}
      \label{fig:clusterP1}
    \end{subfigure}%
    \begin{subfigure}{.24\textwidth}
      \centering
      \includegraphics[width=.89\linewidth]{figures/PointCloud_cluster_phase2}
      \caption{xxx}
      \label{fig:clusterP2}
    \end{subfigure}
    \caption{When there are 3 people in the mmWave field of view, one of them is the passenger who does not enable IMU, 
    the algorithm first divide the clusters into 2 parts based on the 2 centriods (marked as $\times$). Then in each cluster, we remove the outliers.
    }
    \label{fig:clusterDemo}
    \end{figure}


Two important variables in the clustering algorithms are the number of clusters and the  initialization of the centroids.  Therefore, specifying the two parameters can significantly increase the performance of the clustering algorithm. 

We call the module \textit{Heuristic Clustering} because we use three heuristics to boost the performance of the clustering algorithm.
\begin{enumerate}
    \item The number of IMU sensors.
    \item The points in the cluster should be within a limted distance from the centriod due to the limit of human 
    body part size.
    
    \item For each cluster centroid's  position $C_{i,t}$ at time $t$, we use the Kalman Filter to predict the position $(C_{i,t+1})$ and use  it as the initialization of the centroids position at time $t+1$.
    
\end{enumerate}


For the Kalman filter, we  utilize  the recursive  Kalman filter with  a predefined motion model.  The recursive Kalman filter was shown to be more robust in\cite{huang2021indoor} for indoor tracking  compared to extended Kalman filter.
 Given an observed  point $x_i$, its state can be represented as 
$$
x_{k}=[r \dot{r} \theta \dot{\theta}]^{T}
$$
Specifically, here we assume the user's velocity is constant in time $\Delta t$, so the motion model and the observation model are defined as followings:
$$
\begin{aligned}
x_{k} &=\left[\begin{array}{cccc}
1 & \Delta t & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & \Delta t \\
0 & 0 & 0 & 1
\end{array}\right] x_{k-1}+Q, \\
y_{k} &=\left[\begin{array}{llll}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right] x_{k}+R,
\end{aligned}
$$


After the Kalman filter predicts the next position $(C_{i,t+1})$, 
we transform  $(C_{i,t+1})$ from  polar system to Cartesian coordinates and use it as the initialization of the centrodds for the cluster in the next frame.


For the first heuristic, the number of IMU sensors should be equal to or smaller than the number of clusters because we can not assume every user in the field of view participates in the system.
The logic here is that the system is only interested in the people who enable their IMU sensors, so the clustering result  of the people that do not participate is out of interest. 
Here we divide the algorithm into the  following two steps to leverage the first and third heuristics.  

First of all, we apply \textit{K}-Means on the frame at time $t$ with centroids initialized position as $C_{t+1}$ and we set the number of clusters equal
to the number of imu sensors $N$.


Because the number of people is larger than or equal to $N$, there must be extra points in some of the clusters in stage one.
Then  for each cluster $c$, we select the outlier points in $c$ based on the third heuristic  and remove them from $c$. Specifically, we select the outliers based 
on their distance from the centroid position. We first set a size limitation $S$ of a person with 0.6m in length, 0.3m in width and 2m in height. We then draw
a bounding box with size $S$ centered at the cluster centroud.  The outliers are those who are not in the bounding box. Refer to figure\ref{fig:clusterDemo} to see a
demo on the process when there are 3 people totally and 2 of them are the participants.


\todo{modify  the title here}
\subsubsection{Leverage Cross-Modalities Information for Cluster Association}

After removing the clutters (e.g non-human object) and separating the  clusters, the remaining problem is the cluster association: 
an inherent mechanism needs to be designed to construct consistent trajectories across consecutive frames.   For tracking with the camera, rich 
features related to the shape and coloring can be extracted from the region of interest by using deep learning. Then the tracking systems using the camera compare the similarity between the 
features across consecutive frames to solve the cluster association.  

On the contrary,  the existing solutions for the sparse point cloud is to rely on the location (e.g. nearest neighbor) and velocity data  which are directly obtained from measurements. 
However, the naive solution suffers from high false-positive rate in indoor applications since people tend to gather together and the velocity is limited to a 
small range, which makes it indistinguishable among different people.  On the other hand,  due to the sparsity of the point cloud in mmWave radar,  the shape and color features embedded in the point cloud is not reliable enough for 
cluster association.  

In our cluster association module, we propose two innoviations to solve the above challenges, which utilize multiple features of the point cloud including shape, location, and velocity.
\begin{enumerate}
    \item We leverage the velocity information in imu to assist the cluster association process and overcome the sparsity problem.
    \item  We design the PointTrack Net (PTrack Net) to jointly embed the shape, velocity and location information in each cluster to a feature vector. 
\end{enumerate}




\textit{\textbf{PointTrack Net Structure}}

\todo{add downstream task, change position encoding to imu id encoding}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.98 \textwidth]{figures/feature_extractor}
    \caption{xxxxxxxxxxxxxx}
    \label{fig:feature extractor}
\end{figure*}


% In most of the tracking systems, they focus on Kalemn Filter design or data pre-processing pipeline. For the cluster association similarity calculation, most of work use simple heuristics  such as nearest point using the euclidean distance \cite{wu2020mmtrack}. This simple approach is not robust and accurate enough because 
% \begin{enumerate}
%     \item The approach does not make full use of the existing information such as the velocity, radical velocity, number of points in the cluster, shape of the cluster.
%     \item A single heuristic can not cover all the situations. For example the nearest point solution works when the object are separated far away but will fail when the object are relatively near to each other.
% \end{enumerate}
We propose a novel deep-learning-based feature extractor to embed features in different aspects (shape, velocity ,and location) to a single feature vector. 
Our network is designed to handle the sparsity in radar point clouds and output the feature vector for clustering association. 

Our full network architecture is visualized in figure\ref{fig:feature extractor}
Instead of directly using the whole point cloud as the input, we feed each cluster to the model one by one. Feeding the single cluster makes 
the model learn the features of each cluster rather than the whole frame. For each cluster, we first feed the location vector to a shared MLP layer, then followed by a max-pooling 
layer to aggregate the information from all the points and output the global feature $G$. The shared-MLP is designed for making the 
model invariant to input permutation\cite{pointnet} and learning the individual features. Then we concatenate $G$ with the individual features before and add the velocity vector in the end. Then we feed concatenated vector to another shared-MLP layer with a larger size, followed by a max-pooling layer.
The output features $F$ will be added with the imu id  encoding with the same length. Then final output feature will be used in the cluster association process. 


Then we will explain the \textit{imu id  encoding} in details.
Besides the model structure, the downstream task plays an important role. The downstream task will control the features the model  learns.
 In our system, we choose bounding box prediction because the task requires the feature from  both the shape and location information.

\textit{\textbf{Imu Id Encoding}}


%  Instead of using the entire captured point cloud, the model handles each cluster  separately. The model can be divided into two phases.
% \begin{enumerate}
%     \item  \textbf{Embed shape information: } The model first embed the information about the shape and outline of the object by using the shared mlp originally proposed in pointnet\cite{pointnet}.
%     \item \textbf{Bounding Box refinement:}  Due to the sparsity of the radar point cloud, the feature directly extracted from the point cloud is not robust enough. With the help of heuristic clustering, the cluster result is promising so we further concatenate the centriod information , including the $(x,y)$ position and the velocity to the feature out from phase 1. Then we apply the similar shared mpl with more neurons and one more layers. After that, the features pass through a max pooling layer and then become  a single representative feature for 2D-bounding box generation.

% \end{enumerate}

The features learned from the PointTrack Net embeds the shape, velocity ,and position information of the cluster in the point cloud. However, due to the 
sparsity of the point cloud, such features may not be time coherent among multiple frames.  An invariant  property is that there exists a 1 to 1 mathching between each cluster and each Imu. 
We leverage the  property to assign the id of the most similar imu sensor. 

Given the number of Imu sensors $n$, we first assign a pseudo id  $i$ from $1 \dots n$ to each Imu. 
Then we adopt the position encoding function, which is proposed in \cite{vaswani2017attention} with dimension D to vectorize the id.

$$
PE(i) = 
\left[
  \begin{array}{c}
           \sin(i/1000^{\dfrac{0}{D}}),
           \cos(i/1000^{\dfrac{0}{D}})\\
           \vdots \\
           \sin(i /1000^{\dfrac{D-1}{D}}),
           \cos(i/1000^{\dfrac{D-1}{D}})
  \end{array}
  \right]
$$
Here we use  $I_{v_t}^{i}$ to denote the velocity on the x,y,z axis from the $i^{th}$ imu at time $t$. We use $t_{sample}$ to denote the sampling window length.
Similarly, we use  $M_{v_t}^{(j)}$  denote the velocity of  the $j^{th}$  cluster of mmWave at time $t$ and $M_{v_t}^{(k,j)}$ to denote the velocity of $k^{th}$  point in the $j^{th}$  cluster of mmWave.

We assign the id to the cluster  based  on  similarity between $M_{v_t}^{(j)}$  and $I_{v_t}^{i}$   There main difficulty here is that 
\begin{enumerate}
    \item  $M_{v_t}^{(j)}$  contains the estimated velocity of all the points in the point cloud, which is corrsponding to different body part. However, the profile $I_{v_t}^{i}$ only 
    contains one estimated data. 

    %\item  $M_{v_t}^{(k,j)}$   stores the velocity respect to mmWave  as a scalar while $I_{v_t}^{i}$  is the velocity of the user in world frame. 
\end{enumerate}


% For $I_{v_t}^{i}$, we choose 2 dimensions with larger variance from x,y,z ,  The key observation is that it is enough to dinstinguish the velocity profile only based on 2 dimensions data. Moreover, reduced the problem from 
% 3D to 2D will ease  it.



We overcome the challenge by calcuating the weighted average velocity  $\hat{M_{v_t}^{(j)}}$ based on the following procedure: 

% for $M_{v_t}^{(k,j)}$, we assign it to the direction $\theta^{j,k,t}$ in 2D plane. Here 
% the $\theta^{j,k,t}$ is the arrival of angle(AOA) of the point. Then the average velocity $\overline{V_{mmWave}^{j,t}}$  is calculated as 
$$
\hat{M_{v_t}^{(j)}}=\sum_{i=0}^{m} \frac{1}{RANK(d\left(p_{c}, p_{i}\right)) * \lg^m} * M_{v_t}^{(k,j)}
$$


$p_{c}$ is the centroid of the cluster and $d$ denotes the distance between two points.  The \textit{RANK} function will sort on the distance between each point and 
the centroid in ascending order. Then it outputs the index(ranking) of the distance.

Up to this stage, we can  calculate the  similarity score   by using the euclidean distance of their magnitudes.
 After finding the  most similar imu $i$ for cluster $j$, we add the $PE(i)$ to the feature vector of cluster j.
% \todo{cosine在这里没有意义, 关注的是维度}






Typically, IMU outputs  body-frame accelerations, 
angular rates and magnetic field measurements. Compared to the wireless sensors, the IMU 
sensor can reach a much higher sampling rate, which makes it suitable for deploying in a tracking system.
In order to avoid extensively data collection and design a general system, our IMU tracking module adopts
the double integration approach. The high level idea is to apply double integration on the acceleration data to obtain the distance.

However, two main challenges needed to be addressed before we utilize IMU in the tracking system.
\begin{enumerate}
    \item  Suppose  the letters \textit{I,W,B} denote inertial, world and body frames coordination system respectively. 
    Generally \textit{I} and \textit{ B} are the same but diffent from \textit{W}. Therefore, we need to  transfer the input or the tracking result from \textit{I} to 
    \textit{W}.  
    \item  The tracking result of IMU will drift over
    time during the process of double integration of acceleration. In other word, any measurement errors are accumulated over time. 

    
    
\end{enumerate}

\textbf{Utilize Mahony Filter For Transformation In Different Coordination Systems } The Mahony Filter\cite{mahony} is designed to 
estimate  the orientation of the IMU in the world frame(\textit{ W}). Here we use $q^W$ and $q^I$ to represent the orientation in 
\textit{ W} and \textit{ I} in the form of \textbf{quaternion}.  The algorithm first calcuates an orientation error from previous step base on the 
acceleration data $I_{a_{t}}$ as shown in equation\ref{eq:mahonyError} where ${I}_{\hat{\mathbf{a}}_{t+1}}$ is the normalized accelerations, followed by 
 a correction step based on a proportional-integral compensator in order to correct the measured angular velocity $I_{g_{t}}$as shown in equation\ref{eq:errorCorrect}.
The corrected angular velocity is then used to calculate the orientation increment, which will be integrated to transfer 
$q^I$ to $q^W$ 

The whole algorithm only requires two hyper-parameters, the proportional filter gain $k_p$ and 
integral filter gain $k_I$.  According to \cite{mahony},  typically proportional filter gain $k_p$ is a more sensitive to the 
value than integral filter gain $k_I$. Therefore, we only need to fine-tune $k_p$, which makes the algorithms suitable on diffent people.



\begin{equation}
    \label{eq:mahonyError}
    \begin{aligned}
    &\mathbf{v_t}=\left[\begin{array}{c}
    2\left(q_{t-2}*q_{t-4}-q_{t-1}*q_{t-3}\right) \\
    2\left(q_{t-1} *q_{t-2}+q_{t-3}*q_{t-4}\right) \\
    \left(q_{t-1}^{2}-q_{t-2}^{2}-q_{t-3}^{2}+q_{t-4}^{2}\right)
    \end{array}\right] \\
    &\mathbf{e}_{t+1}={ }^{I} \hat{\mathbf{a}}_{t+1} \times \mathbf{v_t}\\
    &\mathbf{e}_{i, t+1}=\mathbf{e}_{i, t}+\mathbf{e}_{t+1} \Delta t
    \end{aligned}
\end{equation}

\begin{equation}
    \label{eq:errorCorrect}
    \begin{aligned}
    &I_{g_{t+1}}=I_{g_{t+1}}+\mathbf{k}_{p} \mathbf{e}_{t+1}+\mathbf{k}_{i} \mathbf{e}_{i, t+1}
    \end{aligned}
\end{equation}

\textbf{Correct Integral Drift By Intentifing  Static Moment} The measurement errors incurred by the IMU hardware 
itself will result in a non-zero value when the actual  value is zero, which result in the integral drift.
The key observation is that the static moment, when the acceleration is zero, can be detected by setting a threshold on the 
acceleration magnitude. Then we correct the drift by removing the acceleration data during the static moment.














% The IMU tracking module first apply a first order band-pass butterworth filter to the data collected by accelerometer, with a cutoff frequency at 0.02 HZ and 10 HZ to filter out the noise components. 
% Then we apply use a threshold $d=0.05$ on the accelerometer data to detect the static position (e.g the moment the upper limb moves to the lowest position or the feet touch the floor.) Then we design a mahony filter, whose goal is to estimate the orientation 
%  by fusing/combining attitude estimates by integrating gyroscope measurements 
%  and direction obtained by the accelerometer measurements. 
%  Specifically, in the mahony filter, we set the proportional filter gain $k_p = 0.5$ for static moment and $k_p = 0$ for non-static moment. 
%  When $k_p = 0$, the estimated orientation is the same with the last frame's estimation. 
%  The reason for the setting is that the orientation of the IMU should be aligned with the person when he/she is moving.  
%  However, a person may change the direction when the feet touches the floor. 
 

 
 \subsection{Trajectory-Based Association}
 The module's primary goal is to assign  a virtual id of  imu  to a cluster in the mmWave.
 Essentially, it is a $N-M$ matching problem, where $N$ is the number of participants  and $M$ is the number of occupants within the field of view of the mmWave sensor. 
 After tracking the trajectories for each imu sensor and cluster in the mmWave, we need to define a similarity score.
The trajectory-based matching is considered to be a robust criterion because of the uniqueness of the trajectory for different users. Moreover, since 
trajectory represents the feature over time, short-term noise should have a relatively lower influence. 
Though the idea is straightforward, we still need to overcome the following problems to successfully utilize the trajectory-based matching.
 
  \begin{enumerate}
     \item \textit{Handel the difference of the trajectory from imu  and the global tracking module}. 
     Due to the different sampling rate and source of noise,  even the trajectories for the same person at the same time from the two sensors will not be identical. 
     \item \textit{Embed direction information in the trajectory}. Specifically, we need to distinguish between two users, who walk with the same
     shape of trajectories but from different direction.
     
 \end{enumerate}
 

\subsubsection{Heat map Generation From Trajectory}

 To handle the differences between the trajctories in imu tracking module and the global tracking module, 
 the trajectories are reduced into 2D heat maps to facilitate the matching process. Specifically, instead of denoting each position as a point on the image, we divide the field of view into squares with length $s$.  
 The heatmap can preserve the shape information in the original trajectory and weaken the influence of errors and noises in the trajectories in the two different sensing modalities.

 Moreover, the describe the motion of an object by the function $f(x,y,t)$, where $x,y$ is the location and $t$ is the time,
  the shape of the trajectory contains the location information. Therefore, to solve the 
 second challenge, we need to add another variable to  embed the information related to the time in the trajectory. 
 Here we use the variable intensity of the heatmap $p$ and we map the function $f(x,y,t)$ to $G(x^{'},y^{'},p)$. Here $x^{'},y^{'}$ is the location
 on the image. We use ${h}_x,{h}_y$ to represent the size of the heat map, $x_0,y_0$ to represent the starting position of the trajectory, $s$ to represent the 
 grid size of the heat map, $f$ to represent the sampling frequency of the sensor. Then specifically:
 $$
x^{'} =  \lfloor \frac{x-x_0}{s \cdot {h}_x} \cdot b_x \rfloor  \hspace{1cm} y^{'} =  \lfloor \frac{y-y_0}{s \cdot {h}_y} \cdot b_y \rfloor  
 $$
 
 $$
 p(x,y) =  p(x,y) + \lceil f \cdot t \rceil \pmod{p_{max}}
 $$

The value $b_x,b_y$ is the length of the targeted sensing area.
Here $p_{max}$ is the maximum intensity of the heat map. Then since $ p = \frac{R+G+B}{3} $,  we change the $R$ equal to $p$ first, once the $R$ value is staurated, we modify the value of $G,B$ .

 
\subsubsection{Siamese Network For Comparing Trajectory Similarity}

After we transfer the trajectory to the heat map, 
 we design a siamese network for comparing the similarity between the trajectory. 
 Typically, a siamese network  contains two or more identical subnetworks used to generate feature vectors for each input. Moreover, any parameter updates are 
 mirrored across all subnetworks. The specific architecture is specifically designed for tasks related to image similarity comparison, such as face recognition.
 In addition, the siamese network is designed for one shot learning , whose task is  learning representations from a single sample. In our scenarios, each trajectory
 is unique and there isn't a lot of samples with the same shape. 

 In our network, a convolutional neural network (CNN) is used as the feature extractor. Three consecutive blocks of convolutional layer with RELU  activation.
 followed by a max-pooling layer with a final  fully-connected layer. This layer produces the feature vectors that the weighed distance layer will fuse for comparing the similarity. 
 O Our model uses three identical CNN sub-networks with shared weights for three different inputs: anchor, positive, and negative input.
 Specifically, the inputs are three heatmaps, where two of them will be the trajectories from the same person at the same time, one from mmWave and one from imu 
 and the third one a negative example either from mmWave or imu, which is not similar to the previous one.   After the distance layer, we adopt the triplet loss\cite{facenet} shown in equation\ref{eq:triplet} and the loss will maximize the distance beteen 
 anchor and negative pair while minimizing the distance beteen anchor and positive pair.  Finally, during the training phase, we will mixed simulated trajectories with the trajectories generated from self-collected data.  As for the 
 inference phase, the input will be a pair of trajectories and the output will be the distance beteen them.

 \begin{equation*}
    \mathcal{L}=\max (d(anch, pos)-d(anch, neg)+\operatorname{margin}, 0)
    \label{eq:triplet}
    =\end{equation*}
 
 
\subsubsection{One-shot Id Association}
% It is clear that if a  bipartite graph $G = (L, R, E)$ has a perfect matching, then it must have $\|L\| = \|R\|$. Furthermore, for a set of vertices $S \subseteq V$, we define its set of neighbors $\Gamma(S)$ by:

% $$
% \Gamma(S)=\{v \in V \mid \exists u \in S \text { s.t. }\{u, v\} \in E\} .
% $$

%Suppose that for every $ S \subseteq L$,  we have $|\Gamma(S)| \geq |S|$ ,then  $G$ has a perfect matching\cite{perfectMatching}. In the \textit{Heuristic Clustering}, the number of cluster $N_{clu}$ will be equal to the number of IMU $N_{IMU}$, which ensures that $\|L\| = \|R\|$. 
After  feeding each  trajectory pair from mmWave cluster and imu sensor to the siamese network, we can  construct a $M * N$ matrix $S$, in which $S_{(i,j)}$ is the $1-$ the cosine distance between the feature vectors of trajectories from the $i^{th}$  cluster of  mmWave and  the $j^{th}$ imu.
The range of $S_{(i,j)}$ is $[0,2]$, where the similar pair should result in a value between o to 1.

We then solve the association problem in each window using Hungarian Algorithm for matrix $S$, which works in polynomial time. Then the remaining issue
is choosing the most confident association result or  generate a final association result based on the result in each window.


Here we first use the  soft voting algorithm based on $p$  windows to propose a final associating proposal.
For each window $w$, we generate the matrix $S_{w}$, then we update the final matrix $S_{final}$ by 
$S_{final} = \frac{1}{p} \cdot \sum_{1}^{p}  S_{w}$. Then we apply the Hungarian Algorithm again on the $S_{final}$ to 
generate the final association proposal.


We further define two criteria listed below to  ensure the proposal is with a high confidence level. Only if the 
two criteria are satisfied, then we will adopt the proposal and associate
the cluster with the imu id. Otherwise, we remove the eariest window's proposal from $S_{final}$ and wait for outcome from new window.



\begin{enumerate}
    \item If it it proposed that the $i^{th}$ cluster should pair with the $j^{th}$ imu, then $S_{[i,j]} \leq \alpha$
    
    \item $ \forall$ $ 0 \leq i \leq M$, if $S_{[i,j]}$ is the minimm value on row $i$, then $ \nexists$  $0 \leq k \leq M$ such that $S_{[k,j]}$ is the minimum value of row $k$.
    
    
\end{enumerate}

Intuitively, the first criterion sets a lower confidence bound on associating the imu and mmWave cluster. 
The second criterion is set to make sure each imu will be associated with the cluster with the highest similarity rather than the sub-optimal one. 
\todo{discuss the benefits of setting the criteria}

 
\begin{algorithm}
	\caption{OneShot Matching} 
	\begin{algorithmic}[1]
	    \State $N \leftarrow \text{number of IMU}$
	    \State $M \leftarrow \text{number of cluster}$
	    \State $IMUList \leftarrow [1,2,\ldots,N]$
	    \State $clusterList \leftarrow [1,2,\ldots,M]$
	    \State $S_{final} \leftarrow [0]_{m \cdot n}$
		\For {$window:w=1,2,\ldots $} 
		    \State $ b \leftarrow 1$
    		\State $T_{IMU}^w,T_{mmWave}^w \leftarrow genTrajectory(w)$
            \Comment{T stands for trajectory}
    		\State $S_{w}  \leftarrow similarity(T_{IMU}^w,T_{mmWave}^w)$ 
    		
    		\If{$b \le p $}
    		    \State $S_{final} \leftarrow S_{final} +  S_{w}$
    		    \State $b \leftarrow b+1$
    		\EndIf
    		
    		\If{$b == p$}
    		\State $S_{final} \leftarrow Hungarian(S_{final})$
    		
    		
    		\If {\text{both two criteria satsified}}
    		\State lock the id
            \Else
            \State $S_{final} \leftarrow S_{final} \cdot p -  S_{w-p+1}$
            \State $b \leftarrow b-1$
            
    		% \State $IMUList \leftarrow IMUList \setminus \text{matched IMU id } $
    		% \State $clusterList \leftarrow clusterList \setminus \text{Matched cluster id}$
    		\EndIf
    		
    		\EndIf
    		\State Detect new IMU
		    
			
		\EndFor
	\end{algorithmic} 
      \end{algorithm}
 

 
 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: