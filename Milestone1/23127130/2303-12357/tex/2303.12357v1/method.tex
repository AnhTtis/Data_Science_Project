\vspace{-0.5em}
\section{Methods} 
In this section, we present our proposed Wasserstein adversarial example against time series models. We rely on the most common method of creating adversarial examples, the variation of projected gradient descent (PGD). The original PGD algorithm uses $L_\infty$ clipping to perform the projection while we will use Wasserstein projection in our method. First, we will explain how to perform the Wasserstein projection. Followed by that, we will explain the Wasserstein PGD algorithm and the two-step projection. 

%In the end, we briefly introduce the certified robustness mechanism we use for evaluating Wasserstein adversarial examples.

\vspace{-0.5em}
\subsection{Wasserstein Projection}
Let $(x, y)$ be a data point and its label, and $\mathcal{B}(x, \epsilon)$ be a ball around $x$ with radius $\epsilon$. The (general) projection of a point $w$ on to $\mathcal{B}(x, \epsilon)$ can be formulated as:
\begin{equation} 
    \underset{\mathcal{B}(x, \epsilon)}{proj(w)} = \underset{z \in \mathcal{B}(x, \epsilon)}{\argmin} ||w-z||_2^2.
\end{equation}
A Wasserstein ball around sample $x$ with radius $\epsilon$ can be defined as:
\begin{equation} 
    \mathcal{B}_w(x, \epsilon)= \{x+\delta: d_\mathcal{W}(x, x+\delta) \leq \epsilon\},
\end{equation}
where $d_\mathcal{W}(u, v)$ refers to the Wasserstein distance between two sample distributions  in the space $\mathcal{X} = \mathbb{R}^2$, which can be calculated as:
\begin{equation} 
    d_\mathcal{W} (u, v) = [\underset{\gamma \in \Pi(u, v)}{inf} \int_{\mathcal{X}^2} ||x-y||^p d\gamma(x, y)]^{1/p}.
\end{equation}
Here $\gamma$ defines the joint probability distribution, called coupling, which has marginal distribution exactly as $u$ and $v$.
\vspace{-0.1cm}
\begin{algorithm}
\caption{1D\_Wasserstein\_Projection}\label{alg:projection}
\KwIn{
     Original input: $x$; Initial adversarial input $x_{adv}^0$; Wasserstein projection bound: $\mathcal{C}$;
     Step size: $\alpha$}
\KwOut {
    Projected Adversarial example: $x_{adv}$}
 $i \gets 0$\;
 $L_{w}^0 =d_\mathcal{W}(x, x_{adv}^0)$ (Formula \ref{equ:dw})\;
\While{$L_{w} \geq \mathcal{C}$}
{
         $x_{adv}^{i+1} = x_{adv}^i - \alpha \cdot \frac{\partial L_{w}(x, x_{adv}^i) }{\partial x_{adv}^i}$\;
         $L_{w}^i = d_\mathcal{W}(x, x_{adv}^i)$\;
         i++\;
        \If {$i \geq \mathbb{I}$}
        {
             Break\;
             \textbf{Return} False\;
        }
}
\end{algorithm}
\vspace{-0.1cm}

When the input distribution satisfies the dimension being one, there is a closed-form solution for the above $d_\mathcal{W}$:
\begin{equation} \label{duv}
\begin{split}
d_\mathcal{W} (u, v) & = ||F_u^{-1} - F_v^{-1}||^p \\
                     & = (\int_{0}^{1} ||F_u(\alpha)^{-1}- F_v(\alpha) ^{-1}||^p d \alpha)^{1/p}.
\end{split}
\end{equation}
When $p$ equals 1 and the inputs are in the discrete case, formula \ref{duv} can be further simplified as:
\begin{equation}
\begin{split}
     d_\mathcal{W} (u, v) & = \int_{\mathbb{R}}| F_u(\alpha)- F_v(\alpha) | d \alpha \\
                            & = \sum_{i = 1}^n | \sum_{j = 1}^i u_i - \sum_{j = 1}^i v_i|.
\label{equ:dw}
\end{split}
\end{equation}
In this case, the transport plan is $t = F_v^{-1} \odot F_u$.


Specifically, projecting $w$ onto the Wasserstein ball around $x$ with radius $\epsilon$ is defined as:
\begin{equation} 
    \underset{\mathcal{B_\mathcal{W}}(x, \epsilon)}{proj(w)} = \underset{z \in \mathcal{B}_\mathcal{W}(x, \epsilon)}{\argmin} d_\mathcal{W} (x, z).
\end{equation}
As $1D$ Wasserstein distance has a closed-form solution and the above formula is also differentiable, we can apply the gradient descend method to the projection, the algorithm is explained in Algorithm \ref{alg:projection}. Note that this method may not find the exact projection onto the Wasserstein ball, but it can converge to an example within the Wasserstein ball. %In the original PGD algorithm, the author uses $L_\infty$ clipping to perform the projection.





\subsection{Wasserstein PGD Attack}
\vspace{-0.1cm}
\begin{algorithm}

\caption{ Wasserstein PGD Attack on Univarient Time series}\label{alg:cap}
\KwIn{
     Original input: $\mathcal{X} = \{x, y\}$; Target model: $\mathcal{F_{\theta}}$; Attack Iteration: $\mathbb{T}$; Step size: $\epsilon$; Wasserstein projection bound: $\zeta$; $L_{\infty}$ Norm clipping value: $\delta$ \\}
\KwOut {
    Adversarial example: $x_{adv}$}
    $S_t \gets 0$\;
 Init $x_{adv}^0 = x + \mathcal{N}(0,1)$\;
\While{$t \leq \mathbb{T}$}
{       \tcc{gradient descend step}
        $\eta = \epsilon \cdot sign(\nabla_x L_{cross\_entropy}(x, y, \mathcal{F}_{\theta}))$\;

         $x_{adv}^{t+1} = x_{adv}^t +\eta$\;
          \tcc{norm ball clipping with center $x$ and radius $\zeta$}
          $x_{adv}^{t+1} = \min(\max(x_{adv}^{t+1}, x_{adv}^{t+1}-\zeta), x_{adv}^{t+1}+\zeta)$\; 
          
          \tcc{Wasserstein ball projection with center $x$ and radius $\delta$}
         $x_{adv}^{t+1}$ = 1D\_Wasserstein\_Projection($x, x_{adv}^{t+1}, \delta)$ (Algorithm \ref{alg:projection})\;

         t++\;
}
\end{algorithm}
\vspace{-0.1cm}

PGD attack utilizes the concept of back-propagation. It takes the gradient of the loss function over inputs, to generate small perturbations at each time step and iteratively adds the perturbation to the clean input to generate adversarial examples followed by projection (clipping into the $L_\infty$ ball), such that the new adversarial example has a greater tendency towards being misclassified. For Wasserstein adversarial attack, each iteration of the algorithm includes a gradient descent step to update the perturbed example followed by Wassersten projection, which is formulated as:

\begin{equation}
    x^{t+1}_{adv} = \underset{\mathcal{B}_\mathcal{W}(x, \epsilon)}{proj}  (x^t_{adv} +  \alpha ^T \nabla \mathcal{L}(x^t_{adv}, y)).
\end{equation}

Theoretically, Wasserstein projection can be performed from any starting point. However, %during experiments, we noticed that 
direct projection onto the Wasserstein ball using gradient descent can be time consuming and converge to a sub-optimal solution. We propose a two-step projection method (shown in Algorithm 2) that first projects the adversarial example to a norm-ball and then uses the projected example as the starting point for Wasserstein projection. As shown in Figure \ref{fig:step}, the blue curve refers to the direct Wasserstein projection using gradient descent (this is the gradient descent used to perform projection shown in line 3-5 in Algorithm 1). This is performed after each  step of gradient descent that updates the adversarial example (this is the gradient descent used to generate the perturbation shown in line 4-5 in Algorithm 2). The red curve and green curve refer to the two-step projection that first projects (clips) the example to the norm ball (line 6) and then projects it to the Wasserstein ball (line 7). In this way, the search in the Wasserstein space is guided and constrained, which is more effective and efficient.

\iffalse
$x_{adv}: v$\\
$x_{adv}$ after projection: $v'$\\
$x_{clean}: u$\\

\begin{equation}
   d_\mathcal{W} (u, v) & = \int_{\mathbb{R}}| F_u(\alpha)- F_v(\alpha) | d \alpha  
\end{equation}

we want to project v into a Wasserstein ball with center $u$ and radius $t$:
\begin{equation*}
\begin{split}
    
   &min \int_{\mathbb{R}}| F_{v'}(\alpha)- F_v(\alpha) | d \alpha \\
   
   &st. \int_{\mathbb{R}}| F_u(\alpha)- F_v(\alpha) | d \alpha  \leq t
  \end{split}
\end{equation*}

\fi






