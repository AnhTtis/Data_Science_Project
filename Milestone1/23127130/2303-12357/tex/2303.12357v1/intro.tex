\vspace{-0.2cm}
\section{Introduction}

Deep learning has been widely applied to real-world time series classification tasks including healthcare, power consumption monitoring, as well as social safety and security \cite{TSC-review, 7552276, Ma2018HealthATMAD}. Univariant time series modeling is a common class of applications that deals with one observed variable over time \cite{karim2019multivariate}.  The data are often collected from a sensor, such as Electrocardiography (ECG) to measure patient's heart rhythms \cite{castells2007principal}, and can be used for diagnosis and monitoring. %Many such applications are  security-concerned. 
Deep learning systems have  achieved state-of-the-art performance in most of these tasks.
\vspace{-0.1cm}
\begin{figure}[ht]
  \centering
  \setlength{\abovecaptionskip}{0.cm}
  \includegraphics[width=0.5\textwidth]{./fig/intuition.png}
  \caption{An example to illustrate the differences between Wasserstein distance and Euclidean distance. The two ECG signals in the upper figure are close in the Wasserstein distance and similar to human perception while they have a large $L_1$ distance. In the bottom figure, the $L_{\infty}$ distance from the red curve to the blue and purple curve are both 2.88, but from human perception, the purple curve is much further away from the red curve than the blue curve. Therefore, the Wasserstein distance is more appropriate to measure the distance in this case.}
  \label{fig:intuition}
  \vspace{-0.4cm}
\end{figure}

However, recent studies have shown that adversarial examples can be generated to force a well-trained deep learning model to misclassify by applying small and unnoticeable perturbations to the input data \cite{szegedy2013intriguing}. The existence of such adversarial examples reveals the vulnerability of deep learning models, especially when applied to security-critical domains such as healthcare. 

The notion of indistinguishability of adversarial examples, in the context of computer vision, was originally taken to be $L_{\infty}$ bounded perturbations, which refers to noise with limited magnitude injected to each pixel \cite{goodfellow2014explaining}. Followed by that, the scale of adversarial perturbations is usually constrained by $L_p$ norm, %which is to search adversarial examples in, 
i.e., bounded by an $L_p$ norm ball centered at the normal example
\cite{kurakin2018adversarial, carlini2017towards}. As deep learning models are increasingly used for time series data, %and potential adversarial attacks are present in many applications where the use of time series data is crucial, 
several works  adapted  adversarial attacks from images to time series data \cite{TSC, AATS, MTSR} using $L_p$ norms. While adversarial examples under $L_p$ norm are intuitive for image classification tasks,
%as the Euclidean ball is a convenient source of adversarial perturbations on the image. 
$L_p$ norm is not an appropriate metric for the time series data. 



In time series analysis, there are more effective metrics for measuring similarity between two temporal sequences, especially when the sequences vary in length and speed \cite{alt1995computing, berndt1994using}.  Wasserstein distance \cite{vallender1974calculation}, which is the numerical cost of an optimal transportation problem, allows us to analyze the distance between two time sequences. This distance can be intuitively understood for time sequence as the cost of moving around feature mass from one time step to another (transportation plan) in order to make two sequences the same. 
%%%%%%% remove %%%%%%
 \iffalse
The Wasserstein of two distribution $u, v \in \mathbb{R}^{n \times m}$ can be stated as:
\begin{equation*}
    d_W(u, v) = \underset{\Pi \in \mathbb{R}^{(n \cdot m) \times (n \cdot m)}}{min}< \Pi, \mathbf{C}>
\vspace{-0.3em}
\end{equation*}
where $\Pi$ refers to the transport plan and \mathbf{C} represents the cost of transporting a mass unit from each position.
\fi
 %%%%%%%% end remove %%%%%%
 
Note that Wasserstein distance and Euclidean distance are different measures and even opposite in some cases. Two time sequences can be close in the Wasserstein distance while far away from each other in Euclidean distance. As shown in Figure \ref{fig:intuition},  Wasserstein distance can better reflect human perception. The time series that appear more distinguished to our eyes are captured by larger Wasserstein distances. The example in the upper figure may not be a successful adversarial example under $L_1$ distance as it is distant in the $L_1$ space. However, it is almost imperceptible to human evaluation. In this case, Wasserstein distance is a better measurement of adversarial examples. 

In this work, we study the adversarial attack on time series in the Wasserstein space for the first time. % which better capture the distances. 
Our goal is to generate adversarial examples that have small Wasserstein perturbation so it is more indistinguishable and natural to human, e.g., physician who examines ECG data.
Projected gradient descent attack \cite{madry2017towards}  is a widely-used attack method that applies small steps of maximizing the loss objective iteratively and clipping the values of intermediate results after each step (projection to the $L_p$ norm ball) to ensure that they are in a constrained neighbourhood of the original inputs. Similarly, we propose a Wasserstein PGD method to search for adversarial examples in the Wasserstein space for univariant time series. Wasserstein distance cannot be calculated directly without solving an optimization subproblem and has no closed-form solution in most cases, which limits its applications. At present, there are only two cases that the Wasserstein distance can be directly calculated, one is the case of the dimension of inputs being 1, and the other is the inputs following Gaussian distribution. 
%In the paper\cite{wong2019wasserstein}, they applied Wasserstein PGD on the image classification task. As the projection cannot be directed calculated, they used Projected Sinkhorn Iterations to approximate the projection.
For the univariant time series, we can take advantage of its $1D$ characteristic and use the closed-form Wasserstein distance to apply the projection of intermediate results of each step onto the Wasserstein ball with gradient descent method.

\begin{figure}[htbp]
% \setlength{\abovecaptionskip}{0.cm}
  \centering
  \includegraphics[width=0.5\textwidth]{./fig/step.png}
  \caption{Illustration of the difference between direct projection and two-step projection}
  \label{fig:step}
%   \vspace{-0.4cm}
\end{figure}

%During experiments, we observed that 
The direct projection of intermediate results onto the Wasserstein ball using gradient descent could be time-consuming and converge to a sub-optimal solution. Thus, we further propose a two-step projection method that first projects the intermediate result to an $L_p$ norm ball (one step clipping) and then uses the projected example on the norm ball as the starting point for Wasserstein projection, so that the search in the Wasserstein space is guided and constrained. The intuition of the two-step projection is illustrated in Figure \ref{fig:step}, which will be explained in detail in the Method Section. 

The state-of-the-art defense mechanism to adversarial examples is the certified robustness approach which provides a theoretical guarantee that adversarial examples generated within certain distance bounds can be correctly classified. To better study the nature of Wasserstein adversarial examples, we also investigate how well the existing certified robustness approach work against our proposed Wasserstein attack. As Wasserstein adversarial examples are bounded by Wasserstein distance, the existing and well-known certified defense within Euclidean distance is not applicable \cite{cohen2019certified}. We adapt Wasserstein Smoothing \cite{levine2020wasserstein}, a certified robustness approach to Wasserstein adversarial examples for image data which transfers Wasserstein distance on the image into $L_1$ distance on the transport plan, to univariant time series adversarial examples. From the results, although the defense can achieve some accuracy gain, it still has limitations in many cases and leaves space for developing a stronger certified robustness method to Wasserstein adversarial examples on univariant time series data.

Our Contributions can be summarized as follows:

1) We study adversarial examples in the Wasserstein space for time series data for the first time which better capture the distance and are more natural and imperceptible.

2) We utilize the characteristics of univariant time series data and propose a projected gradient descent attack method 
which efficiently projects (bounds) adversarial examples in the Wasserstein ball.

3) We develop a two-step projection that first projects an adversarial example to an $L_p$ norm ball and then use the projected example as the starting point for Wasserstein projection to overcome the computing bottleneck of direct Wasserstein projection.

4) We empirically evaluate the proposed attack on several Electrocardiogram (ECG) datasets in the health care domain. Extensive results demonstrate that the Wasserstein PGD is powerful and can attack most of the target classifiers with a high attack success rate and yield more imperceptible and natural examples than attacks in the Euclidean space.

5) We evaluate Wasserstein smoothing designed for image data as a baseline certified robustness approach against Wasserstein attack which suggest that there is space for stronger defense mechanisms tailored to time series data.

