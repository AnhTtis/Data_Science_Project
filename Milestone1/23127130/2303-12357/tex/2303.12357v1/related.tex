
\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.1cm}
In this section, we will first introduce  existing adversarial attacks in the image domain and time series domain. Followed by that we will discuss the state-of-the-art defense mechanism: certified robustness.
\vspace{-0.2cm}
\subsection{Adversarial Attack Methods}
\vspace{-0.1cm}
Adversarial examples were first introduced in the image domain \cite{szegedy2013intriguing}. Ian Goodfellow et al \cite{goodfellow2014explaining} proposed a fast Gradient Method to generate adversarial examples by performing a one-step gradient update along the direction of the sign of the gradient at each pixel. This method is simple and fast but cannot minimize perturbation. Other research attempted to iteratively apply FGSM in order to achieve a smaller perturbation such as I-FGSM and Projected Gradient Descent (PGD) \cite{madry2017towards, pWorld}. C\&W \cite{carlini2017towards} is a powerful attack that aims to both minimize the distance and maximizes classification loss  and hence achieves minimal perturbations for the same attack success rate. However, C\&W is relatively time-consuming. 

The above mentioned attack methods are all in the Euclidean space. \cite{wong2019wasserstein} opened up a new direction in adversarial examples in Wasserstein space for the image domain, by developing a procedure for projecting an example onto the Wasserstein ball. As the projection cannot be directed calculated, they used Projected Sinkhorn Iterations to approximate the projection. 

Several works have  adapted adversarial attacks from images to time series data. \cite{TSC} empirically studies the performance of I-FGSM in the time series classification tasks while \cite{MTSR} adopted I-FGSM into the time series regression tasks. \cite{AATS} utilized an adversarial transformation network (ATN) \cite{baluja2017adversarial} on a distilled model to attack various time series classification models. Although Wasserstein distance is a better distance measurement for time series data, no previous work has studied  adversarial examples in Wasserstein space for time series data. 

\vspace{-0.2cm}
\subsection{Certified Robustness}
\vspace{-0.1cm}
Certified robustness is the most powerful defense method to date as it is provable. The main idea is to transform the base classifier into a randomized classifier by applying random noise onto the inputs or classifier, such that the perturbed examples within a certain Euclidean ball are certified to have the same classification as the original example.  PixelDP\cite{PixelDP} utilized the definition of differential privacy (DP) \cite{mcsherry2007mechanism} to prove certified robustness using the Gaussian mechanism through noisy layers in the network. WordDP \cite{wang2021certified} extended the idea into the text domain using the Exponential mechanism. Randomized smoothing \cite{cohen2019certified} generates Gaussian noise on the input and provided a tighter certified bound using the Neyman Pearson lemma\cite{Neyman1992}.

Wasserstein smoothing \cite{levine2020wasserstein} was the first certified robustness method for adversarial images in Wasserstein space. The basic idea is to define a reduced transport plan and transfer Wasserstein distance on the image space to 
$L_1$ norm on the transport plan. Therefore, smoothing in the transport domain can be performed using  existing $L_1$ robustness certification provided by \cite{PixelDP} and transferred back to the Wasserstein space. In this paper, we will adopt Wasserstein smoothing
from the image domain to the univariant time series data and evaluate the performance of our proposed Wasserstein attack against this potential countermeasure. 

