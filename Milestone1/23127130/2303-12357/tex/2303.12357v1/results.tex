\vspace{-0.2cm}
\section{Experiments}
In this section, we will first describe our experimental settings and then evaluate our proposed Wasserstein PGD in terms of the Attack Success Rate (ASR, the fraction of examples that label has been fliped), and comparison with the PGD attack in the Euclidean space. We also demonstrate the effectiveness of the 2-step projection. Finally, we will demonstrate the results of certified robustness to the proposed Wasserstein PGD attack.
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|} 
\toprule
\hline
~ &\multicolumn{4}{l|}{\textbf{Data Description}} \\
\hline
\textbf{Dataset} & \textbf{TrainSize}          & \textbf{TestSize}    & \textbf{Classes} & \textbf{SeqLen}   \\ 
\hline
\textbf{ECG200}  & 100        & 100     &2 &96\\
\hline
\textbf{ECG5000}  & 500        & 4500     &5 &160  \\
\hline
\textbf{ECGFiveDays}  & 23        & 861     &2 &136 \\
\hline
~ &\multicolumn{4}{l|}{\textbf{Model Performance}} \\
\hline
 \textbf{Dataset} & \textbf{MLP}   & \textbf{FCN}  &\textbf{CNN}   &\textbf{ResNet} \\
\hline
\textbf{ECG200} &0.916 &0.9 &0.83 &0.89 \\
\hline
\textbf{ECG5000} &0.931 &0.939 &0.928 &0.934 \\
\hline
\textbf{ECGFiveDays} &0.979 &0.987 &0.885 &0.993 \\
\hline
\end{tabular}
\caption{Summary of datasets}
\label{tab: summary}

\end{table}
\vspace{-0.1cm}

\begin{figure*}[ht]
 \centering
 %%%%%%%% remove %%%%%%
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECG5000-w01.png}
         \label{fig:y equals x}
     \end{subfigure}
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECG200-w01.png}
         \label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECGFD-w01.png}
         \label{fig:five over x}
     \end{subfigure}
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECG5000-l_05.png}
         \caption{ECG5000}
         \label{fig:y equals x}
     \end{subfigure}
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECG200-l_05.png}
         \caption{ECG200}
         \label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECGFD-l_05.png}
         \caption{ECGFiveDays}
         \label{fig:five over x}
     \end{subfigure}
 %%%%%%%% end remove %%%%%%
  \iffalse
    \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECG5000.png}
         \caption{ECG5000}
         \label{fig:y equals x}
     \end{subfigure}
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECG200.png}
         \caption{ECG200}
         \label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/ECGFD.png}
         \caption{ECGFiveDays}
         \label{fig:five over x}
     \end{subfigure}
\fi
\caption{Attack Success Rate under different $\l_{\infty}$ and Wasserstein Bound: The columns represent the three dataset respectively.The first row illustrate under the same Wasserstein distance bound, how the attack success rate change with the increase of the $\l_{\infty}$ bound; The Second row illustrate under the same $\l_{\infty}$ bound, how the attack success rate change with the increase of the Wasserstein distance bound.}
\label{fig:three graphs}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.5em}
\subsection{Experimental Setup}
\vspace{-0.3em}
Our experiments are evaluated on Five benchmark time series classification datasets from the publicly available UCR archive \cite{UCRArchive}. The datasets are selected under the ``ECG" category for ECG based diagnosis tasks, where an adversarial attack is a potential security concern. In this section, we only show the results on three of the five ECG dataset, which are: 1) ECG200 which includes two classes (normal heartbeat and  Myocardial Infarction), and contains 35 half-hour records sampled with the rate of 125 Hz. 2) ECG5000 which is the Beth Israel Deaconess Medical Center (BIDMC) congestive heart failure database, consisting of records of 15 subjects, with severe congestive heart failure. Five labels refes to different levels of heart failure. Records of each individual were recorded in 20 hours, containing two ECG signals, sampled with the rate of 250 Hz. 3) ECGFiveDays which is from a 67-year-old male, including two classes which are two ECG dates. We relegate the full version of the experiments of the other two datasets to the Appendix. 


We adopt and evaluate the target deep learning models from \cite{ismail2019deep} including: Multi-Layer Perceptron (MLP) \cite{gardner1998artificial}, Fully Connected Networks (FCN)\cite{long2015fully}, Convolutional Neural Networks (CNN) \cite{krizhevsky2012imagenet} and Residual Networks (ResNet) \cite{he2016deep}. The detailed information of the ECG datasets and the performance of the target models on each dataset is listed in Table \ref{tab: summary}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-0.2cm}
\subsection{Attack Success Rate}
\vspace{-0.4em}
Our proposed 2-step projection Wasserstein PGD involves a first projection to a $L_{\infty}$ norm-ball and a second projection to a Wasserstein ball. Figure \ref{fig:three graphs} illustrates the impact of these two projections by comparing the ASR under different $L_{\infty}$ and Wasserstein bounds. The first row shows under the same Wasserstein distance bound, how the ASR changes with the increase of the $L_{\infty}$ bound, while the Second row shows under the same $\L_{\infty}$ bound, how the ASR changes with the increase of the Wasserstein distance bound. Each column represents the results of each dataset and each line in each figure corresponds to a target model. We show two models for each dataset and relegate the full version of the experiments to the Appendix. 

%For ECG5000 dataset, 
Under the same radius of Wasserstein ball, the general trend is that ASR first increases with the increase of $L_{\infty}$ bound. This is intuitive as the search space for optimal adversarial examples that satisfy the Wasserstein distance constraint is increasing. It is easier to find an adversarial example that successfully attack the target model and meanwhile satisfy the Wasserstein constraint. However, as the $L_{\infty}$ bound keeps increasing, it does not help anymore and even hurts the performance because the search in the Wasserstein Space is not guided and constrained any more and can be too large for the projection to find a good solution.   Therefore, the ASR stops increasing or starts to decrease.  This decreasing trend is more noticeable in ECG200 and ECGFiveDays datasets, while for ECG5000, the ASR increases to and stays at 1 (for the CNN model). Note that our attack is untargeted attack that aims to flip the label to any class other than the original label rather than the targeted label. Therefore, multi-class classification task (ECG5000) is easier to attack than binary classification tasks (ECG200 and ECGFiveDays). 

\begin{figure}[ht]
\centering
  \includegraphics[width=0.5\textwidth]{./fig/tsne.png}
  \caption{t-sne for ECG5000 (left) and ECG200 (right)}
  \label{fig:tsne}
 \vspace{-0.4cm}
\end{figure}


To further explain the difference between the datasets, we use t-distributed stochastic neighbor embedding (t-SNE), a nonlinear dimensionality reduction technique for high-dimensional visualization in a low-dimensional space \cite{JMLR:v9:vandermaaten08a}, to visualize the datasets. Figure \ref{fig:tsne} shows the 2D t-sne for ECG200 and ECG5000 respectively. Each color represents a class label. We can note that data points of ECG200 is more separable and the class boundary is more clear, while the classes are overlapping for ECG5000 and the boundary is less clear, which makes it easier to attack. This explains why for ECG5000, the ASR increases to 1 and does not decrease.


On the other hand, under the same $L_{\infty}$ bound shown in the bottom row of Figure \ref{fig:three graphs}, larger Wasserstein bound  also renders higher ASR at first due to larger search space. As it keeps increasing, the ASR decreases due to the search space being too large and the ineffectiveness of the search, especially when the radius of Wasserstein ball is greater than the Euclidean ball. Overall, the ASR of ECG5000, ECG200 and ECGFivedays can reach 100\%, 62\% and 74\% respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.4em}
\subsection{Effectiveness of 2-step Projection}
\vspace{-0.4em}

From the perspective of ASR, we compare the 2-step projection with the direct 1-step Wasserstein projection shown as the dotted lines in the top row of Figure \ref{fig:three graphs} under the same attack settings. We observe that 2-step projection can achieve a higher ASR in general and optimal attack success rate when choosing the proper $L_{\infty}$ bound for the first projection.

\begin{figure}[ht]
\vspace{-0.2cm}
  \setlength{\abovecaptionskip}{0.cm}
  \centering
  \includegraphics[width=0.42\textwidth]{./fig/1step-2step.png}
  \caption{Comparison between direct Wasserstein projection (1-step projection) and 2-step projection.}
  \label{fig:1step-2step}
  \vspace{-0.1cm}
\end{figure}

From the perspective of human inspection,  Figure \ref{fig:1step-2step} shows two successful adversarial examples generated by the 2-step projection (purple) and direct  projection (red) respectively in comparison with the original example (blue). Although the Wasserstein perturbation distances of the two adversarial examples are both 0.99, the one that is first norm clipped is more imperceptible to human eyes, which has not only small Wasserstein distance but also bounded by $L_{\infty}$ distance. 
\vspace{-0.2cm}

\subsection{Comparison with $L_{\infty}$ PGD}
\setlength{\abovecaptionskip}{0.cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-0.2cm}


The intuition of developing the Wasserstein PGD attack is to search for more indistinguishable and natural adversarial examples in the Wasserstein space. Therefore, we compare the adversarial examples generated by Wasserstein PGD with those generated by original PGD in the Euclidean space ($L_{\infty}$ PGD). On one hand, We draw the utility comparison from two aspects: 1) Under the same attack success rate, Wasserstein PGD is more natural; and 2) Under the same perturbation scale, Wasserstein PGD has a higher attack success rate. On the other hand, as Wasserstein projection involves gradient descent which will add more time cost in generating adversarial examples, we also compare the average time cost of two attack methods.


\begin{figure}[ht]
\vspace{-0.2cm}
  \centering
  \includegraphics[width=0.45\textwidth]{./fig/compare.png}
  \caption{Comparison between Wasserstein PGD (yellow) and $L_{\infty}$ PGD (green) under the same attack success rate.}
  \label{fig:compare}
 \vspace{-0.1cm}

\end{figure}
\vspace{-0.1cm}
\subsubsection{Under the same attack success rate, Wasserstein PGD is more natural}
Figure \ref{fig:compare} illustrates several comparisons between Wasserstein PGD and $L_{\infty}$ PGD under the same ASR. We selected three examples randomly. For each figure, the blue curve is the original input. The yellow curves represent the perturbation and adversarial example generated by Wasserstein PGD, while the green curves represent the $L_{\infty}$ PGD. Clearly, the perturbation generated from Wasserstein PGD is smaller and more indistinguishable than $L_{\infty}$ PGD. 

\subsubsection{Under the same perturbation scale, Wasserstein PGD has a higher attack success rate.}
Another aspect to show the effectiveness of Wasserstein PGD is to compare the ASR with the original PGD under the same perturbation scale. However, the two attacks are conducted in different spaces. It is unfair to compare the ASR of adversarial examples generated from the Wasserstein ball and the $L_{\infty}$ ball with the same radius, as they represent completely different spaces.
To overcome this challenge, we use the greatest $L_{\infty}$ norm of the Wasserstein examples as the radius of $L_{\infty}$ ball to generate $L_{\infty}$ PGD adversarial examples. For example, the maximum $L_{\infty}$ norm of the Wasserstein adversarial examples with 0.01 Wasserstein distance is 0.2. Then we will search for adversarial examples in the  $L_{\infty}$ ball with the radius equal to 0.2. In this way, we have a fair comparison between the Wasserstein PGD and the original PGD attack. 

Figure \ref{fig:compare2} shows the comparison of ASR in the way we introduced above. The x-axis refers to different attack settings corresponding to different Wasserstein and $L_\infty$ bounds (note that we have two steps of projections, first to an $L_\infty$ norm ball and second to a Wasserstein ball, while original PGD only uses $L_\infty$ projection). The purple and orange lines correspond to the Wasserstein PGD and original PGD respectively. We can note that in most cases, Wasserstein PGD has a higher attack success rate than the original attack.

From these two aspects, we can conclude that for univariant time series data, Wasserstein PGD not only can generate more natural adversarial examples but also can achieve a higher ASR under the same attack scale.  
\begin{figure}[ht]
\vspace{-0.2cm}
  \centering
  \setlength{\abovecaptionskip}{0.cm}
  \includegraphics[width=0.44\textwidth]{./fig/compare2.png}
  \caption{Comparison between Wasserstein PGD and $l_{\infty}$ PGD under the same attack scale.}
  \label{fig:compare2}
%   \vspace{-0.4cm}
\vspace{-2em}
\end{figure}

\begin{figure*}[ht]
 \centering
    \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/time_ECG200_cnn.png}
         \caption{ECG200 CNN}
         \label{fig:ECG200_CNN}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/time_ECG200_fcn.png}
         \caption{ECG200 FCN}
         \label{fig:ECG200_FCN}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/time_ECG5000_cnn.png}
         \caption{ECG5000 CNN}
         \label{fig:ECG5000_CNN}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/time_ECG5000_fcn.png}
         \caption{ECG5000 FCN}
         \label{fig:ECG5000_FCN}
     \end{subfigure}
\caption{The average time cost of generating an adversarial example with Wasserstien PGD (with and without norm bound clipping) and $L_{\infty}$ PGD attack.} 
\label{fig:time}
\vspace{-2em}
\end{figure*}

\subsubsection{Compare the time cost between Wasserstein PGD and $L_{\infty}$ PGD}
Besides comparing the utility of Wasserstein PGD with $L_{\infty}$ PGD, we also evaluate how Wasserstein PGD increases the time cost by comparing the average time cost of generating an adversarial example with $L_{\infty}$ PGD attack, as Wasserstein PGD involves projection into the Wasserstein ball via gradient which will result in more time cost. As the time cost of the Wasserstein projection is largely relative to the size of $L_{\infty}$ ball (the start point of gradient descent) and the size of the Wasserstein ball (the destination), we compare the average time cost according to the ratio between Wasserstein bound and $L_{\infty}$ bound.


As shown in Figure \ref{fig:time}, the red and blue curves are the Wasserstein PGD with and without the first stage of norm clipping, while the black line represents the baseline $L_{\infty}$ PGD attack, whose value is irrelevant to the ratio between Wasserstein and $L_{\infty}$ bound. We show the result on two datasets and two models: ECG200 and ECG5000, with CNN and FCN models. We can note that Wasserstein PGD will result in more time cost than $L_{\infty}$ PGD, especially when the ratio between Wasserstein bound and $L_{\infty}$ bound is large. However, when the ratio approaches 1, this increase in time cost is neglectable. By comparing the red and blue curves we can conclude that when the ratio between the Wasserstein bound and $L_{\infty}$ bound approaches 1, the first stage of norm clipping can effectively guide the search for Wasserstein adversarial examples. However, when the ratio increases( approaches 0), even with the bound of norm clipping, the search also tends to be a random search.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Countermeasure against Wasserstein PGD}
To better study the nature of Wasserstein adversarial examples, we also explored certified robustness approach as a potential defense mechanism for Wasserstein PGD. We consider certified robustness in contrast to other empirical defense methods as it is the most powerful and principled defense method to date. 

We adopt Wasserstein smoothing \cite{levine2020wasserstein} which is originally designed for image data not the univariant time series data.  The basic idea of Wasserstein smoothing is to define a reduced transport plan and map the Wasserstein distance on the input space to the $L_1$ norm on the transport plan. The base classifier is transformed into a smoothed classifier by adding Laplacian noise $Laplace(0, \sigma)^r$ to the reduced transport plan, where $r$ is corresponding to the input dimension. Because of the mapping, smoothing in the transport domain can be performed using existing $L_1$ robustness certification provided by \cite{PixelDP} and mapped back to the Wasserstein Space. The strict form of the certified condition can be stated as:
\begin{theorem}\label{theorem}
For any normalized probability distribution input $x \in \mathbb{R}^{n*m}$ with correct class $c$,  if:
\begin{equation}
    \bar{f_c}(x) \geq e^{2 \sqrt{2}\zeta / \sigma} (1-\bar{f_c}(x)) \label{condition}
\end{equation}
then for any perturbed $\tilde{x}$ that $d_W(x, \tilde{x}) \leq \zeta$, we have 
\begin{equation}
\bar{f_c}(\tilde{x}) \geq \underset{i \neq c}{max} \bar{{f_i}}(\tilde{x})
\end{equation}

The detailed proof and the design of the reduced transport plan can be referred to \cite{levine2020wasserstein}

\end{theorem}




We evaluate how Wasserstein Smoothing empirically works on the proposed Wasserstein PGD attack with two evaluation metrics \cite{wang2021certified}: \textbf{Certified accuracy (CertAcc)} which denotes the
fraction of the clean testing set on which the predictions are correct and also satisfy the certification criteria. 
 Formally, it is defined as:
$ \frac{\sum_{t = 1}^T certifiedCheck(X_t, L, \epsilon)\& corrClass(X_t, L,\epsilon)}{T},$
where {\small $certifiedCheck$} returns 1 if Theorem \ref{theorem} is satisfied and {\small $corrClass$} returns 1 if the classification output is correct. $T$ is the size of the test dataset. \textbf{Conventional accuracy (ConvAcc)} is defined as the fraction of testing set that is correctly classified, $\frac{\sum_{t = 1}^T corrClass(X_t, L,\epsilon)}{T}$, which is a standard metric to evaluate any deep learning systems.

Figure \ref{fig:CertAcc} illustrates how the certified accuracy changes under different Wasserstein certified radius $\zeta$ in Equation \ref{condition}. During the experiments, the Laplace parameter $\sigma$ which controls the noise to the transport plan is set to 0.01 and the soft prediction result is averaged over 100 times of sampling.  As Wasserstein radius increases, the certified accuracy decreases, which means less fraction of examples can satisfy the certification condition in Equation \ref{condition}. When  Wasserstein radius $\zeta$ is set over 0.1, the certified accuracy stops decreasing. We can note from this result that: first,  under a certain Laplacian distribution, the certified radius is limited. Second, to a very small certified radius, for examples less than 0.01, the certified accuracy can achieve 100\%.

\begin{figure}[ht]
\vspace{-0.1cm}
\setlength{\abovecaptionskip}{0.cm}
  \centering
  \includegraphics[width=0.45\textwidth]{./fig/CertAcc.png}
  \caption{Comparison of Certified Accuracy under different Wasserstein radius with $\sigma = 0.01$}
  \label{fig:CertAcc}
\vspace{-0.2cm}
\end{figure}

\begin{figure}[ht]
\vspace{-0.1cm}
\setlength{\abovecaptionskip}{0.cm}
  \centering

  \includegraphics[width=0.45\textwidth]{./fig/ConvAcc.png}
  \caption{Comparison of Conventional Accuracy of successfully attacked adversarial examples. }
  \label{fig:ConvAcc}
\vspace{-0.2cm}

\end{figure}

Figure \ref{fig:ConvAcc} demonstrates whether Wasserstein smoothing can empirically increase the conventional accuracy of adversarial examples generated by Wasserstein PGD. In the experiments, we test on successfully generated adversarial examples, so the baseline accuracy of the dataset is 0. Laplace parameter $\sigma$ is set to 0.1 and the soft prediction result is averaged over 100 times of sampling. The x-axis refers to the Wasserstein attack scales. The two red lines represent the adversarial examples generated from ECG200 under two different settings, $L_{\infty}$ norm set to 0.1 and 0.2, and the two blue lines represent the adversarial examples generated from ECG5000 where $L_{\infty}$ norm set to 0.1 and 0.2. We can note from the figures the following.  First, when the Wasserstein attack scales are small, Wasserstein smoothing can render some accuracy gain. The accuracy decreases with the increase of Wasserstein perturbation scales as expected.  When the Wasserstein distance increase over 0.06, which is also the greatest certified radius, the accuracy gain is limited and does not change anymore. We also randomly select two adversarial examples with Wasserstein perturbations around 0.06. As shown in Figure \ref{fig:cert06}, the adversarial ECG is fairly indistinguishable to human eyes. Yet Wasserstein Smoothing can not provide reasonable defense at this level. %also cannot facilitate the physician to tell the difference.  
Second, with ECG200 and ECG5000 under setting2 ( $L_{\infty}$ norm set to 0.2), the overall accuracy gain is very small for all perturbation scales. 

We can conclude from the results that the existing Wasserstein smoothing has limited success in both certified ratio and conventional accuracy gain. This suggests that there is still space for developing stronger certified robustness method to Wasserstein PGD tailored to time series data instead of using the general transport plan based smoothing designed for image data.

\begin{figure}[ht]
 \centering
    \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/cert_006.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./fig/cert_006_1.png}
     \end{subfigure}
\caption{The comparison between adversarial examples and clean examples at $d_\mathcal{W}$ scale around 0.06.} 
\label{fig:cert06}
\vspace{-1em}
\end{figure}


