%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,submission]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
%\acmDOI{10.1145/1122445.1122456}
%\acmConference[LICS'23]{LICS'23}{Boston, USA}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[LICS'23]{LICS'23}{Boston, USA}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.


\usepackage{stmaryrd}
\usepackage{mathbbol}
\usepackage{times}
%\usepackage{inputenc}

\usepackage{latexsym,amsmath,epsfig,amsthm}
\let\Bbbk\relax
\usepackage{amssymb}
\usepackage{graphics,tabularx}
\usepackage{color,graphicx}
\usepackage{prftree}

\usepackage{mathbbol}
\newcommand\lvar{\mathcal{L}}

\newcommand{\mycomment}[1]{{\color{red}[#1]} }

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\input{userdef}

\newcommand{\leadm}[1]{\xrightarrow{#1}}
\newcommand{\seman}[1]{[\![#1 ]\!]}
\newcommand{\nseman}[3]{\seman{#1}_{#2, #3}^{\lvar}}
\newcommand{\nsemans}[4]{\seman{#1}_{#2, #3}^{#4}}
\newcommand{\updateh}[4]{#1 \langle #2, #3, #4 \rangle}
\newcommand{\rdy}{\mathit{rdy}}
\newcommand{\pblks}{\mathit{pblks}}
\newcommand{\blks}{\mathit{blks}}
\newcommand{\combine}{\mathit{combine}}
\newcommand{\ommit}[1]{}
\newcommand{\ob}{\mathtt{OutBlock}}
\newcommand{\ib}{\mathtt{InBlock}}
\newcommand{\iob}{\mathtt{IOBlock}}
\newcommand{\wb}{\mathtt{WaitBlock}}
\newcommand{\ihistory}{\mathcal{I}}
\newcommand{\IFE}[3]{\textrm{if}\ #1\ \textrm{then}\ #2\ \textrm{else}\ #3}
\newcommand{\spec}[3]{\{#1\}\ #2\ \{#3\}}
\newcommand{\join}[2]{#1\,\textsf{@}\,#2}
\newcommand{\wand}[2]{#1\,\textsf{@-}\,#2}
\newcommand{\wandop}{\,\textsf{@-}\,}
\newcommand{\joinop}{\,\textsf{@}\,}
\renewcommand{\wp}{\mathit{wlp}}
\newcommand{\var}{\mathit{var}}
\newcommand{\wvar}{\mathit{uvar}}
\newcommand{\freev}{\mathit{free}}
\renewcommand{\sp}{\mathit{sp}_{\|}}
\newcommand{\compat}{\operatorname{compat}}
\newcommand{\len}{\operatorname{len}}
\renewcommand{\evo}[3]{\langle \vec{\dot{#1}}=\vec{#2}\& #3\rangle}
\newcommand{\dL}{d$\mathcal{L}$}
\newcommand{\closeb}{\mathit{cl}}
\newcommand{\tracev}{\gamma}
\newcommand{\specs}{\mathit{specs}}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Generalized Hybrid  Hoare Logic}

%\author{Naijun Zhan$^{1}$, Bohua Zhan$^{1}$, Shuling Wang$^{1}$, Dimitar Guelev$^{2}$ and Xiangyu Jin$^{1}$\\
%$^1$Institute of Software, Chinese Academy of Sciences	\\
%$^2$Bulgarian Academy of Sciences}
\author{Naijun Zhan}
\affiliation{
\institution{School of Computer Science, \\Peking University\\
State Key Lab. of Computer Science, ISCAS}
\city{Beijing}
\country{China}
}

\author{Xiangyu Jin}
\affiliation{
\institution{State Key Lab. of Computer Science, ISCAS\\
University of Chinese Academy of Sciences}
\city{Beijing}
\country{China}
}

\author{Bohua Zhan}
\affiliation{
\institution{Huawei Technologies Co., Ltd.}
\city{Beijing}
\country{China}
}


\author{Shuling Wang}
\affiliation{
\institution{National Key Laboratory of Space Integrated Information System,\\ ISCAS}
\city{Beijing}
\country{China}
}

\author{Dimitar Guelev}
\affiliation{
\institution{Institute of Mathematics and Informatics Bulgarian Academy of Sciences}
\city{Sofia}
\country{Bulgaria}
}



% author names and affiliations
% use a multiple column layout for up to three different
% affiliations



% make the title area


% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Deductive verification of hybrid systems (HSs) increasingly attracts more attention in recent years because of its power and scalability, where a powerful specification logic for HSs is the cornerstone. % of deduction-based approaches.
Often, HSs are naturally modelled by concurrent processes that communicate with each other. However, existing specification logics cannot easily handle such models. In this paper, we present a specification logic and proof system for Hybrid Communicating Sequential Processes (HCSP), that extends CSP with ordinary differential equations (ODE) and interrupts to model interactions between continuous and discrete evolution. Because it includes a rich set of algebraic operators, complicated hybrid systems can be easily modelled in an algebra-like compositional way in HCSP. Our logic can be seen as 
a generalization and simplification of existing hybrid Hoare logics (HHL)  based on duration calculus (DC), as well as a conservative extension of existing Hoare logics for concurrent programs. Its assertion logic is the first-order theory of differential equations (FOD), together with 
assertions about traces recording communications, readiness, and continuous evolution.
We prove continuous relative completeness of the logic w.r.t. FOD, as well as discrete relative completeness in the sense  that continuous behaviour can be arbitrarily approximated by discretization. Besides, we discuss how to simplify proofs using the logic by providing a simplified assertion language and  the corresponding inference rules for HCSP, especially those for ODEs and interrupts. Finally,  we  
implement the above logic  
in Isabelle/HOL, and apply it to verify a simplified version of the cruise control system.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Hybrid systems, hybrid Hoare Logic, Hybrid CSP, proof system, relative completeness}




\maketitle

\section{Introduction}

{\em Hybrid systems} (HSs) exhibit combinations of discrete jumps and
continuous evolution. Applications of HSs are everywhere in our daily life, e.g. aircrafts, spacecrafts, industrial automation, high-speed train, and so on. Many of these applications are \emph{safety-critical}. 
How to design correct and reliable complex safety-critical HSs so that people can bet their life on them becomes a grand challenge in computer science and control theory \cite{Wing08}.

There have been a huge bulk of work on formal modeling and verification of HSs, e.g., \cite{Alur:1992,Manna93,Manna93b,Henzinger96,LSVW96,LPY02}, most of which are automata-based. In automata-based approaches,
HSs are modeled as {\em hybrid automata} (HA) ~\cite{Alur:1992,Manna93,Henzinger96},
and verified by computing reachable sets. Unfortunately, as shown in \cite{Henzinger96,HenzingerKPV98}, reachability for most of these systems is undecidable, except for some special 
linear \cite{AD94,LPY02} and non-linear \cite{Gan17} ones. Therefore, in practice, people mainly focus on how to over- and under-approximate reachable sets by using different geometric objects to represent abstract states. \oomit{ e.g. polyhedral \cite{Frehse11}, zonotope \cite{Girard05,Girard08}, ellipsoid \cite{Kurzhanski00},  Taylor model \cite{Xin2014} etc., or by applying numeric computation based SMT solvers \cite{Eggers2012,dReach}, or by (sub) level-set together with optimization  \cite{Stipanovic03,Mitchell05,Mitchell07,xue2018,xue2020}, or by discretization together interval arithmetic analysis \cite{Dang10}, or by homemorphism \cite{Xue16}, and so on.} 
The advantages of automata-based approaches are twofold:  a HA describes the whole behavior of the system to be developed, therefore is very intuitive; and the verification is fully  automatic.  However, their disadvantages are also twofold: 
HA is analogous to state machines, with little support for structured description, and  is thus difficult to model complex systems; moreover,
 existing techniques for computing reachable sets are not scalable, particularly, most of them can only be used to compute reachable sets in bounded time.

Deductive verification presents an alternative way to ensure correctness of HSs. Several formalisms for reasoning about HSs have been proposed, including those based on differential dynamic logic (\dL)   \cite{Platzer08,Platzer}, extended duration calculus~\cite{ChaochenRH92}, and Hybrid Communicating Sequential Processes (HCSP) ~\cite{Jifeng:1994,Zhou:1996,LLQZ10,WZG12,GWZZ13}, and so on. For {\dL}, an initial version of the proof system~\cite{Platzer08} is stated in terms of explicit solutions to ODEs, and is proved to be relatively complete with respect to first-order logic of differential equations (FOD), with the assumption that any valid statements involving ODEs can be proved. The ensuing work~\cite{Platzer12a} gives equivalent discrete versions of ODE rules using Euler approximation. Further work~\cite{Platzer10,Platzer12-cut,PlatzerT20} discusses additional rules, such as differential invariant, differential cut, and differential ghosts for reasoning about ODEs. {\dL} does not provide explicit operators for concurrency and communication, requiring these characteristics of HSs to be encoded within its sequential hybrid programs, meaning that a complicated HSs with communication and parallel composition cannot be specified and reasoned about in an explicit and  compositional way with {\dL}.

Process algebras such as Communicating Sequential Processes (CSP)~\cite{Hoare85} provide a natural compositional way to model systems with concurrency and communication. Extending classical Hoare logic \cite{Hoare69} to CSP has been studied by Apt \emph{et al.}~\cite{Apt80,Apt83} and by Levin and Gries~\cite{LevinG81}. In both works, Hoare triples for input and output statements are essentially arbitrary when reasoning about sequential processes. Then, for each pair of input and output commands in a parallel process, a cooperation test is introduced to relate the global state before and after communication. As with Owicki-Gries' method for shared-memory concurrency \cite{OG76a,OG76b,Owicki76}, these initial proof systems are not compositional, in the sense that there are proof obligations involving every pair of processes that communicate with each other. Moreover, auxiliary variables are usually needed to keep track of progress within each sequential process. The work by Soundararajan~\cite{Soundararajan84} proposes a compositional proof system for CSP. The main idea is to explicitly introduce a trace recording the history of communications, and allow assertions to also depend on traces. For stating the rule for parallel processes, a compatibility condition is defined, characterizing when the records of communications in different traces are consistent with each other.

For modelling HSs, CSP was extended to Hybrid CSP (HCSP) by introducing ordinary differential equations (ODE) to model continuous evolution and interruptions to model interactions between continuous and discrete evolution ~\cite{Jifeng:1994,Zhou:1996}. Because it has a rich set of algebraic operators, complicated HSs can be easily modelled  in an algebra-like compositional way in HCSP.
Like CSP, it is desired to invent a specification logic for HCSP in order to specify and reason about HSs with concurrency and communication in a compositional way. 
There are several attempts to extend Hoare logic to HCSP based on duration calculus (DC) \cite{ZHR91} in the literature.  \cite{LLQZ10} first extended Hoare logic to HCSP, in which postcondition and history formula in terms of DC that specify invariant properties  are given separately, therefore it fails to define compositional rules for communications, parallelism and interruptions. To address this issue, \cite{WZG12} 
proposed an assume-guarantee proof system for HCSP, still based on DC, but it 
cannot handle super-dense computation well. Super-dense computation assumes that 
computer is much faster
than other physical devices and computation time of a control program  is therefore
negligible, although the temporal order of computations is still there. Super-dense computation provides a comfortable abstraction of HSs, and 
is thus commonly adopted in most models of HSs. To solve this problem,  \cite{GWZZ13} proposed another DC-based proof system 
for HCSP by introducing the notion of infinitesimal time to model computation cost of control events, that changes the semantics slightly in a counter-intuitive way.
In a word, DC-based proof systems for HCSP are too complicated to be used in practice easily. In addition, they lack of logical foundations like (relative) completeness etc.

In this paper,  we re-investigate proof theory for HCSP by providing a much simpler proof system with continuous and discrete relative completeness. 
In order to deal with communication and parallelism in a compositional way, inspired by Soundararajan's work  \cite{Soundararajan84} and Hoare and He's work \cite{UTP1998}, we explicitly introduce the notion of  trace. Different from \cite{Soundararajan84} and \cite{UTP1998}, to deal with continuous evolution, in our setting traces record not only the history of communications and readiness of communication events, but also continuous behavior, which are uniformly called \emph{generalized events}.  So, 
unlike existing proof systems for HCSP based on DC, the assertion logic of  our proof system is simply FOD, with assertions about traces. For expressing rules about parallel processes, we define a synchronization operator on traces.
%\textcolor{red}{
Thus, our proof system can be seen as 
 a weakest liberal precondition calculus for sequential processes and a strongest postcondition calculus for parallel processes, together with rules for reasoning about synchronization on traces.%} 
 

Clearly, our proof system  can be seen as 
a generalization and simplification of existing DC-based hybrid Hoare logics in the sense: 
\begin{itemize}
  %\vspace*{-1mm} 
    \item \vspace*{-1mm}  first, discarding DC part in the assertion logic simplifies the proof system very much; 
   \item second, the notions of \emph{generalized event}, \emph{trace} and \emph{trace synchronization} provide  the possibility that parallelism, typically,  
   communication synchronization and time synchronization, can be coped with uniformly in  a compositional way; \\[-3mm]
  \item finally,  super-dense computation is well naturally accounted  by allowing that a trace can 
  contain many discrete events happened at the same instant ordered by  their causal dependency. \vspace*{-2mm} 
\end{itemize}
Our proof system is also essentially a conservative extension of Hoare logic for concurrent programs (including CSP) by allowing continuous events (wait events, the definition will be given in  Section~\ref{sec:OperationalSem}) and by introducing traces and trace synchronization so that 
non-interference in Owicki/Gries's logic \cite{OG76a,OG76b,Owicki76} and cooperativeness in Apt \emph{et al.}â€™s logic \cite{Apt80,Apt83} can be reasoned about explicitly. 
%If we omit the trace part, parallelism is simply specified and reasoned about with logical conjunction. So, rules for communication and parallelism here are much simpler than in DC-based hybrid Hoare logics.


The completeness properties of the proof system are analogous to  continuous and discrete completeness results for differential dynamic logic {\dL} shown in~\cite{Platzer08,Platzer12a}. However, the situation in HCSP is different in several ways. First, for relative completeness with respect to FOD, we need to consider the encoding of the full trace of communications and continuous evolution. Second, for discrete completeness, the semantics for continuous evolution is different in HCSP compared to {\dL}, for termination, in HCSP only the state along the continuous evolution  at the boundary is considered, whereas 
all reachable states along it inside the boundary 
are considered in \dL. This gives rise to  extra difficulties, that we have to address in this paper. Finally, for both continuous and discrete completeness, we need to consider the additional constructs in HCSP, e.g., interruptions and parallelism.
 
The bare version of the proof system, while satisfying good completeness properties, is still tedious to use in practice, as the weakest preconditions consist of several cases even for simple communications.
To simplify proofs in practice, we define on top of the existing assertion language a set of simplified constructs. We demonstrate that, using the defined syntactical sugar, the proof system can be simplified,  and thus verification of HCSP programs can be done in a relatively concise and much simpler manner. 
Finally, we implement the above logic, including the simplified rules, in the proof assistant Isabelle/HOL, and apply it to verify an example of a cruise control system.
 
In summary, our main contributions are as follows:
\begin{itemize}
%\vspace*{-3mm} 
    \item \vspace*{-1mm}  We present a simplified and generalized Hoare logic for HCSP using assertions about traces recording  communications, readiness, and continuous evolution.
    \item We show both continuous and discrete completeness of the proof system. 
     %, with respect to first-order logic with differential equations.
    \item We further simplify the proof system by providing a simplified language of assertions on traces and a set of  new inference rules for HCSP, 
    allowing practical verification to be conducted in a concise manner. 
    %In addition, We  define the for ODEs and prove their soundness and completeness. 
    \item We implement the proof system, including the semantics of HCSP and the soundness of all proof rules, in Isabelle/HOL. 
    %\footnote{All code related to The implementation and case studies can be found at https://github.com/AgHHL/lics2023.git.}. 
    \vspace*{-2mm}
\end{itemize}
 
\subsection{Related Work}
Inspired by the success of Floyd-Hoare logic \cite{Hoare69,Floyd67} in
the verification of sequential programs, several extensions of Floyd-Hoare logic to concurrent programs were proposed in the 1970s- 1980s.\oomit{including Hoare and Zhou's \cite{Hoare72,ZH81,Hoare81}, Owicki and Gries' \cite{OG76a,OG76b,Owicki76}, Apt \emph{et al.}'s \cite{Apt80,Apt83}, Lamport's \cite{Lamport77,Lamport80,Lamport84} and so on.}\oomit{In \cite{Hoare72}, Hoare first investigated how to specify and reason about partial correctness of concurrent programs with shared variables by extending Hoare logic.} Owicki and Gries\oomit{found Hoare's proof system is incomplete as it lacks an inference rule for auxiliary variables, thus they} established a complete proof system for concurrent programs with shared variables\oomit{(called \emph{resource regions})} in \cite{OG76b}, in which  an important notion called \emph{non-interference} was introduced in order to deal with parallelism. Owicki proved in \cite{Owicki76} the completeness of 
the proof system in the sense of Cook \cite{Cook78}. To this end, she introduced two types of auxiliary variables, i.e., \emph{traces} and \emph{clocks}, to show an interference-free property between any two component processes. In another direction, Zhou and Hoare \cite{ZH81,Hoare81}, and Apt \emph{et al.} \cite{Apt80,Apt83} studied proof systems for concurrent programs with message passing (i.e., CSP).  Particularly, the notion of \emph{cooperative}, similar to \emph{interference-free} in \cite{OG76a,OG76b}, was introduced in 
\cite{Apt80}. In \cite{Apt83}, using similar technique to \cite{Owicki76}, Apt proved the completeness of the Hoare logic for CSP. Lamport and Schneider unified Hoare logics for sequential programs and different models of concurrency within a single paradigm, called \emph{generalized Hoare logic}~\cite{Lamport77,Lamport80,Lamport84}. Cousot and Cousot proved the relative completeness of generalized Hoare logic in \cite{CC89}. 

In \cite{Hooman94}, Hooman extended Hoare logic to timed CSP. Extension of Hoare logic to HCSP was first tried by Liu \emph{et al.} \cite{LLQZ10}. They established hybrid Hoare logic (HHL). In HHL, a hybrid Hoare assertion consists of four parts: \emph{pre-} and \emph{post-conditions}, a HCSP program, and a history formula  in term of DC ~\cite{ZHR91} to specify invariants during continuous evolution. A compositional proof system of HHL using assume/guarantee is presented in \cite{WZG12,GWZZ13}, then \cite{GuelevWZ17} shows the relative completeness of the proof system w.r.t.  DC. In \cite{WZZ15}, a theorem prover for HHL was implemented in Isabelle/HOL. However, reliance on 
DC complicates and prevents practical applications of these proof systems, as DC is not able to cope with general continuous behavior of HSs.

In the literature, there are many other proof-theoretic approaches to verification of HSs, e.g., \dL~\cite{Platzer2008,Platzer18}, hybrid  action systems \cite{BackPP00}, and Hybrid Event-B \cite{Abrial2012}. As  mentioned, {\dL} extends dynamic logic \cite{Pratt76} to HSs by allowing modalities over hybrid programs, that extend classical sequential programs with ODEs to model continuous evolution. To deal with more complex behaviour of HSs, several variants of \dL~ were established, e.g., stochastic differential dynamic logic \cite{Platzer} and differential game logic \cite{Platzer15}. \cite{LP16} proposed differential refinement logic to cope with refinement relation among different levels of abstraction for a given HS;  \cite{Garlan14} investigated how to apply \dL~to define architecture of  CPSs. 
Recently, component-based verification methodologies developed in \dL~\cite{Muller2016,MullerMRSP18} introduced composition operators to split verification of systems into more manageable pieces.\oomit{\cite{Lunel19} extended them with parallel composition (true concurrency) using design patterns defined using the primitive constructs of \dL.} A temporal logic for \dL~based on trace semantics is proposed in~\cite{Platzer07-temporal}. 
However, as we argued, \dL\ cannot handle 
communication and concurrency in an explicitly compositional way, although some attempts have been done e.g. \cite{Lunel19}.


The rest of this paper is organized as follows: Sect.~\ref{sec:HCSP} recalls HCSP and Sect.~\ref{sec:OperationalSem} defines its operational semantics;  Sect.~\ref{sec:hoare} defines HHL, its continuous and discrete relative completeness are proved in Sect.~\ref{sec:completeness}\&\ref{sec:discretecomplete}; Sect.~\ref{sec:simplified-rules} discuss how to simplify the proof system, while Sect.~\ref{sec:examples} provides implementation and case studies; Sect.~\ref{sec:Conclusion} concludes this paper. 

All code related to the implementation and case studies can be found at https://github.com/AgHHL/gHHL2024.git. 
Due to space limit, some of the rules and proofs are given in the Appendix.  

\section{HCSP}
\label{sec:HCSP}

Hybrid CSP (HCSP)~\cite{Jifeng:1994,Zhou:1996} is a formal language for describing HSs, which is an extension of CSP  by introducing  timing constructs, interrupts, and ODEs for modelling continuous evolution. Exchanging data among processes is described solely by communications, no shared variable is allowed between different processes in parallel, so each program variable is local to the respective sequential component.

The syntax for HCSP is given as follows:
\[ \vspace*{-1mm}
\begin{array}{lll}  
	c  & ::= & 
	\pskip \mid
	x := e \mid 
	ch?x \mid 
	ch!e \mid 
	c_1 \sqcup c_2 \mid
	c_1; c_2 \mid c^* \mid\\
	& &
	\IFE{B}{c_1}{c_2} \mid 
	\evo{x}{e}{B} \mid \\ 
	 & & \exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i} \\
	pc &::=& c \mid pc_1\|_{cs} pc_2 %\vspace*{-1mm}
\end{array} 
\]
where \sm{c} and \sm{c_i} are sequential processes, and \sm{pc} and 
 \sm{pc_i} are parallel processes; \sm{x} is a variable over reals in a process, \sm{\dot{x}} stands for its derivative w.r.t. time, $\vec{x}$ (resp. $\vec{e}$) is a vector of variables (expressions) and its $i$-th element is denoted by $x_i$ (resp. $e_i$); $ch$ is a channel name, $ch_i*$ is  either an input event $ch_i?x$ or output  event $ch_i!e$, and $I$ is a non-empty set of indices; $B$ and $e$ are Boolean and arithmetic expressions, respectively; $cs$ is a set of channel names. 

The meaning of $\pskip$, assignment, internal choice, sequential composition, and 
conditional statement are as usual. We explain the intuitive meaning of the additional constructs as follows:
\begin{itemize}
	\item $ch?x$ receives a value along the channel $ch$ and assigns it to variable $x$. It may block waiting for the corresponding output to be ready.
	\item $ch!e$ sends the value of $e$ along $ch$. It may block waiting for the corresponding input to be ready. 
	\item  The repetition \sm{c^*} executes $c$ for a nondeterministic finite number of times.
 
    \item \sm{\evo{x}{e}{B}} is a continuous evolution, which evolves continuously according to the differential equation \sm{\vec{\dot{x}}=\vec{e}} as long
	as the \emph{domain} $B$ holds, and terminates whenever $B$ becomes false. In order to guarantee the existence and uniqueness of the solution of any differential equation, we require as usual that 
the right side $\vec{e}$ satisfies the \emph{local Lipschitz condition}.  
	\item %Communication interruption 
	\sm{\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}} behaves like \sm{\evo{x}{e}{B}}, except it is preempted as soon as one of the communication events \sm{ch_i*} takes place, and then is followed by the corresponding \sm{c_i}. Notice that, if the continuous evolution terminates (reaches the boundary of $B$) before a communication in \sm{\{ch_i*\}_{i\in I}} occurs, the process executes \sm{c}. Notice that if $B$ is $\mathsf{true}$ then \sm{c} will never be executed no matter what \sm{c} is.
 
	\item \sm{pc_1\|_{cs} pc_2} behaves as  $pc_1$ and $pc_2$ run independently except that all communications along the set of common channels $cs$ between $pc_1$ and $pc_2$ are synchronized. We assume $pc_1$ and $pc_2$ do not share any variables, nor does the same channel direction (e.g. $ch!$) occur in $pc_1$ and $pc_2$.
\end{itemize}
When there is no confusion in the context, we will use $c$ to represent either sequential or parallel process below. 
The other constructs of HCSP in \cite{Jifeng:1994,Zhou:1996} are definable, e.g., $\pwait\ d$,  external choice, \emph{etc}. 
\oomit{ \sm{\pwait \ d  \pdef  t:=0; \langle \dot{t}=1\& t < d\rangle,}
  \sm{c^* \pdef \mu X. \pskip \sqcup c; X}, 
  and the external choice \sm{\external{i\in I}{ch_i*}{c_i}  \pdef  t:=0; \langle \dot{t}=1\&\textit{true}\rangle \unrhd \external{i\in I}{ch_i*}{c_i}}.} 
 
\section{Operational semantics} \label{sec:OperationalSem}
In this section, we define a big-step semantics for HCSP, and prove that it is equivalent to the existing small-step semantics~\cite{Zhan17}. Both semantics are defined using the new concept of \emph{generalized events}, in order to fit better with the trace-based development later.

We begin by defining some basic notions. A state for a sequential process is a mapping from variable names to real values. A state for a parallel process \sm{pc_1\|_{cs} pc_2} is a pair $(s_1,s_2)$, where $s_1$ is a state for $pc_1$ and $s_2$ is a state for $pc_2$. This enforces the requirement that $pc_1$ and $pc_2$ do not share variables.
A \emph{ready set} is a set of channel directions (of the form \sm{ch*}), indicating that these channel directions are waiting for communication. Two ready sets \sm{\rdy_1} and \sm{\rdy_2} are \emph{compatible}, denoted by \sm{\compat(\rdy_1, \rdy_2)}, if there does not exist a channel $ch$ such that \sm{ch?\in \rdy_1 \wedge ch!\in \rdy_2} or \sm{ch!\in \rdy_1 \wedge ch?\in \rdy_2}. 
 Intuitively, it means input and output along the same channel cannot be both waiting at the same time, that is, as soon as both channel ends are ready, a communication along the channel occurs immediately. This is consistent with the maximal synchronization semantics as in CSP \cite{Hoare85} and Calculus of Communicating Systems (CCS) \cite{Milner80}.

A \emph{generalized event}  defines one step execution of observable behavior for a (sequential or parallel) HCSP process. There are two types of generalized events:
\begin{itemize}
	\item A \emph{communication event} is of the form $\langle ch\triangleright , v\rangle$, where $\triangleright$ is one of $?$, $!$, or nothing, indicating input, output, and synchronized input/output (IO) event, respectively, and $v$ is a real number indicating the transferred value. 
	\item A \emph{continuous event}, also called 
	a \emph{wait event},  is of the form $\langle d, \vec{p}, \rdy\rangle$ representing an evolution of time length $d>0$. Here $\vec{p}$ is a continuous function from $[0,d]$ to states, that is the unique  trajectory of the considered ODE starting with the given initial state,  and $\rdy$ is the set of channels that are waiting for communication during this period. We allow $d=\infty$ to indicate waiting for an infinite amount of time.
\end{itemize}
We will use \sm{s[\vec{x} \mapsto \vec{e}]} to stand for another state same as $s$ except for mapping each $x_i$ to the corresponding $e_i$, \sm{s(e)} (resp. \sm{s(B)} for Boolean expression $B$) for the evaluated value of $e$ (resp. \sm{B}) under \sm{s}. Given a formula $\phi$ (resp. expression $e$), \sm{\phi[\vec{e}/\vec{x}]} (resp. \sm{e[\vec{e}/\vec{x}]}) denotes replacing all  $x_i$ occurring in  $\phi$ (resp. $e$) by  $e_i$ simultaneously. 


\subsection{Trace-based Big-step Semantics of HCSP}
\label{sec:tracesemantics}
A \emph{trace} is an ordered sequence of generalized events  as the result of executing a (sequential or parallel) HCSP process. We denote the empty trace by $\epsilon$, the trace for a deadlocked process by $\delta$, and use the operator $^\chop$ to denote concatenating  two traces.

\paragraph{Trace synchronization:}
Given two traces $tr_1, tr_2$ and a set of shared channels $cs$, we define the relation  to \emph{synchronize} $tr_1$ and $tr_2$  over $cs$ and result in a trace $\textit{tr}$, denoted by  $tr_1 \|_{cs} tr_2 \Downarrow \textit{tr}$. The derivation rules defining synchronization are given in Fig.~\ref{fig:rule-synchronization}. Rule SyncIO defines that when the two parallel traces start with the compatible input and output events along same channel \sm{ch} (that belongs to the common channel set \sm{cs}) with  same value \sm{v}, then a synchronized event $\langle ch, v\rangle$ is produced,  followed by the synchronization of the rest traces. Rule NoSyncIO defines the case when an external communication event occurs on one side. 
Rules SyncEmpty1-3 deal with the cases one side terminates earlier than the other side. As in CSP and CCS, 
 a parallel process terminates only if all subprocesses in parallel terminate. 
Rules SyncWait1-2 define the cases when both sides are wait events, i.e. waiting for a communication or evolving w.r.t. an ODE, then the wait events of the same length will synchronize if they have compatible ready sets.  
\begin{figure*} \centering \vspace*{-2mm}
{\small 
\begin{eqnarray*} 
 &  \prftree[r]{SyncIO}{ch\in cs}{tr_1\|_{cs} tr_2 \Downarrow \textit{tr}}
{\langle ch!,v\rangle^\chop tr_1 \|_{cs} \langle ch?,v\rangle ^\chop tr_2 \Downarrow \langle ch,v\rangle ^\chop \textit{tr}}
\quad
\prftree[r]{NoSyncIO}{ch\notin cs}{tr_1\|_{cs} tr_2 \Downarrow \textit{tr}}
{\langle ch\triangleright,v\rangle^\chop tr_1 \|_{cs} tr_2 \Downarrow \langle ch\triangleright,v\rangle^\chop \textit{tr}} &\\[1mm]
\oomit{\quad
\prftree[r]{SyncEmpty1}{ch\notin cs}{tr_1\|_{cs} \epsilon  \Downarrow \textit{tr}}
{\langle ch\triangleright,v\rangle^\chop tr_1 \|_{cs} \epsilon  \Downarrow \langle ch\triangleright,v\rangle^\chop \textit{tr}} &\\[1mm]}
 &\prftree[r]{SyncEmpty1}{ch\in cs}{ }
{\langle ch\triangleright,v\rangle^\chop tr_1 \|_{cs} \epsilon  \Downarrow \delta}
\quad
\prftree[r]{SyncEmpty2}{tr_1\|_{cs} \epsilon  \Downarrow \textit{tr}}
{\langle d,\vec{p}_1,\rdy_1\rangle^\chop tr_1 \|_{cs} \epsilon  \Downarrow \langle d,\vec{p}_1,\rdy_1\rangle^\chop \textit{tr}} \quad   
\prftree[r]{SyncEmpty3}{\epsilon \|_{\mathit{cs}} \epsilon \Downarrow \epsilon}  & \\[1mm]
& \prftree[r]{SyncWait1 }
{tr_1\|_{cs} tr_2 \Downarrow \textit{tr}}
{\compat(\rdy_1,\rdy_2) \quad d >0 }
{\langle d,\vec{p}_1,\rdy_1\rangle^\chop tr_1 \|_{cs} \langle d,\vec{p}_2,\rdy_2\rangle^\chop tr_2 \Downarrow \langle d, \vec{p}_1\uplus \vec{p}_2,(\rdy_1\cup \rdy_2)-cs\rangle^\chop \textit{tr}} & \\[1mm] 
& \prftree[r]{SyncWait2 }
{\begin{array}{cc}
		d_1>d_2>0 \quad \langle d_1-d_2,\vec{p}_1(\cdot+d_2),\rdy_1 \rangle^\chop tr_1\|_{cs} tr_2\Downarrow \textit{tr} \quad
		\compat(\rdy_1,\rdy_2)
\end{array}}
{\langle d_1,\vec{p}_1,\rdy_1\rangle^\chop tr_1 \|_{cs} \langle d_2,\vec{p}_2,\rdy_2\rangle^\chop tr_2 \Downarrow \langle d_2,\vec{p}_1\uplus \vec{p}_2,(\rdy_1\cup \rdy_2)-cs \rangle^\chop \textit{tr}} & 
\end{eqnarray*} }
\vspace{-4mm} 
\caption{Trace synchronization rules}
\label{fig:rule-synchronization} \vspace*{-1mm}
\end{figure*}


\paragraph{Big-step semantics:}
A big-step semantics for HCSP is presented partly in Fig.~\ref{fig:big-semantics} (the full version is referred to Appendix~\ref{app:full-semantics}). This leads naturally to the trace-based Hoare logic in Sect.~\ref{sec:hoare}. For a sequential process $c$, its semantics is defined as a mapping, denoted by  \sm{(c,s)\Rightarrow (s',\textit{tr})}, which 
means that $c$ carries initial state $s$ to final state $s'$ with resulting trace $\textit{tr}$ \footnote{It can also be defined as a mapping from an initial state and trace to a final state and trace. The two  definitions are equivalent.}. 
\begin{figure*} \centering \vspace*{-4mm}
{\small 
\begin{eqnarray*} 
 &  \prftree[r]{}{(ch!e, s) \Rightarrow (s, \langle ch!, s(e)\rangle)} 
	\quad\prftree[r]{}{d>0}{(ch!e, s) \Rightarrow (s, \langle d, I_s, \{ch!\}\rangle ^\chop \langle ch!, s(e)\rangle)}   %\\[2mm] 
 \quad  \prftree[r]{}{(ch!e, s) \Rightarrow (s, \langle \infty,I_s,\{ch!\}\rangle )} \quad
 \prftree[r]{}{(ch?x, s) \Rightarrow (s[x \mapsto v], \langle ch?, v\rangle) }& \\[1mm] 
  & 
  %\prftree[r]{}{(ch?x, s) \Rightarrow (s[x \mapsto v], \langle ch?, v\rangle) }
   \prftree[r]{}{d>0}{(ch?x, s) \Rightarrow (s[x \mapsto v], \langle d, I_s, \{ch?\} \rangle^\chop \langle ch?, v\rangle)}  \quad \prftree[r]{}{(ch?x,s)\Rightarrow (s, \langle \infty,I_s,\{ch?\})}  \quad \prftree[r]{}{}{(c^*, s) \Rightarrow (s, \epsilon)}
	\quad\prftree[r]{}{(c, s) \Rightarrow (s_1, \textit{tr}_1) \quad (c^*, s_1) \Rightarrow (s_2, \textit{tr}_2 )}{ (c^*, s)  \Rightarrow (s_2, {\textit{tr}_1}^\chop\textit{tr}_2)}
	 & \\[1mm]
	& \prftree[r]{}{
		\begin{array}{cc}
			\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \quad \vec{p}(0)=s_1(\vec{x}) \quad d>0 \\
			\forall t\in[0,d).\,s_1[\vec{x} \mapsto \vec{p}(t)](B) \quad i\in I\quad ch_i*=ch!e\quad (c_i, s_1[\vec{x} \mapsto \vec{p}(d)])\Rightarrow (s_2,\textit{tr})
	\end{array}}
	{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s_1) \Rightarrow (s_2,
		\langle d, \vec{p}, \rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s_1[\vec{x} \mapsto \vec{p}(t)](e)\rangle ^\chop \textit{tr})} & \\[2mm] 
    & \prftree[r]{}{
		\begin{array}{cc}
			\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \\
			\vec{p}(0)=s(\vec{x})\quad \forall t\in[0,d).\,s[\vec{x}\mapsto \vec{p}(t)](B) \quad \neg s[\vec{x}\mapsto \vec{p}(d)](B)
	\end{array}}
	{(\evo{x}{e}{B}, s) \Rightarrow (s[\vec{x}\mapsto \vec{p}(d)], \langle d, \vec{p}, \{\}\rangle) } \quad \prftree[r]{}{(c_1, s_1) \Rightarrow (s_1', tr_1)}{(c_2, s_2) \Rightarrow (s_2', tr_2)}{tr_1\|_{cs}tr_2 \Downarrow \textit{tr}}
{(c_1 \|_{cs} c_2, s_1\uplus s_2) \Rightarrow (s_1'\uplus s_2', \textit{tr})} & 
\end{eqnarray*}
}
\vspace*{-5mm} 
\caption{Selection of rules in big-step operational semantics}
\label{fig:big-semantics} \vspace*{-2mm}
\end{figure*}

\begin{itemize}
	\vspace*{-1mm} 
	\item For output \sm{ch!e}, there are three cases depending on whether the communication occurs immediately, waits for some finite time, or waits indefinitely.  Input \sm{ch?x} is defined similarly.  \sm{I_s} represents a constant mapping from \sm{[0, d]} to the initial state \sm{s}.
	\item $c^*$ can be understood in a standard way. 
	\item The execution of \sm{\evo{x}{e}{B}} produces a trajectory of  \sm{\vec{\dot{x}}=\vec{e}}  with the given initial state, represented as a wait event. \sm{B} must become false at the end of the trajectory, while remaining true before that. During the evolution, the ready set is empty. 

	\item For interruption \sm{\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}}, communications have a chance to interrupt up to and including the time at which the ODE reaches the boundary. %We show the case with output interrupt after evolving for some time. 
	
\item The semantics of parallel composition is defined by the semantics of its components. Given \sm{s_1} for \sm{c_1} and \sm{s_2} for \sm{c_2},  \sm{s_1\uplus s_2} denotes the pair of states \sm{(s_1,s_2)} as a state for \sm{c_1\|_{cs}c_2}.  \vspace*{-2mm} 
\end{itemize}

\paragraph{Example}
A possible trace for \sm{(\pwait\,1; ch!3)} is \sm{tr_1 = \langle 1, [x \mapsto 0], \emptyset\rangle ^\chop
\langle ch!, 3\rangle} (here we use \sm{[x\mapsto 0]} to denote a constant state trajectory where the value of \sm{x} is 0). A possible trace for \sm{ch!3} is \sm{tr_2 = \langle 1, [x \mapsto 0], \{ch!\} \rangle ^\chop \langle ch!, 3\rangle}. A possible trace for \sm{ch?x} is \sm{tr_3 = \langle 1, [x \mapsto 0], \{ch?\} \rangle ^\chop
\langle ch?, 3\rangle}.
Traces \sm{tr_1} and \sm{tr_3} can synchronize with each other, and form the trace \sm{\langle 1, [x \mapsto 0], \{ch?\} \rangle ^\chop \langle ch, 3\rangle}. \oomit{This corresponds to running \sm{(\pwait\,1; ch!3)} and \sm{ch?x} in parallel, resulting in waiting for one time unit and then communicating 3 along channel \sm{ch}.} However, \sm{tr_2} and \sm{tr_3}  \emph{cannot} synchronize with each other, as the ready sets \sm{\{ch!\}} and \sm{\{ch?\}} are not compatible. \oomit{This corresponds to the fact that running \sm{ch!3} and \sm{ch?x} in parallel cannot result in waiting for one time unit and then communicating.} 

\subsection{Equivalent to Small-step Semantics}
We now rephrase the existing small-step semantics using generalized events. Each transition in the small-step semantics is of the form $(c,s)\stackrel{e}{\to}(c',s')$, meaning that starting from process $c$
and state $s$, executing one step yields an event $e$ 
(either $\tau$ or a communication event or a wait event) and ends with process $c'$ and state $s'$. $\tau$ represents an internal discrete event.
The small-step operational semantics is similar 
to the one given in \cite{Zhan17}, the full semantics is in Appendix~\ref{app:full-semantics}.

We use \sm{(c,s) \stackrel{\textit{\textit{tr}}}{\rightarrow^*}(c',s')} to indicate that starting from process \sm{c} and state \sm{s}, a sequence of small-step transitions results in process \sm{c'} and state \sm{s'}, and \sm{\textit{\textit{tr}}} collects the events that occurred in between, ignoring any \sm{\tau} events. Also, we use \sm{\textit{\textit{tr}} \leadsto_r \textit{\textit{tr}}'} to mean \sm{\textit{\textit{tr}}'} can be obtained from \sm{\textit{\textit{tr}}} by combining some of the neighboring events that can be joined together.

The following theorem shows the equivalence between big-step and small-step semantics. \begin{theorem} \label{thm:small-big-semantics}
    \vspace*{-3mm}
    i) \sm{(c,s)\Rightarrow (s',\textit{\textit{tr}})} implies 
	\sm{(c,s) \stackrel{\textit{\textit{tr}}}{\rightarrow^*} (\pskip,s')}.
	%\item 
	ii)  \sm{(c,s)\stackrel{\textit{\textit{tr}}}{\rightarrow^*}(\pskip,s')} implies    \sm{\textit{\textit{tr}}\leadsto_r \textit{\textit{tr}}'$ and $(c,s)\Rightarrow (s',\textit{\textit{tr}}')} for some $\textit{\textit{tr}}'$.
\end{theorem}

\section{Hybrid Hoare logic}\label{sec:hoare}
In this section, we introduce our version of hybrid Hoare logic, still denoted by HHL, including the syntax, semantics and proof system. 

\subsection{Syntax and Semantics}
\subsubsection{Syntax}
We first present the syntax for terms. The language consists of terms of several types. In the proof of relative completeness w.r.t FOD, we will describe how to encode them to FOD.
{\small \[ \vspace*{-2mm}
\begin{array}{rll}
	\textit{val} &:=& x_i ~|~ c ~|~ v + w ~|~ v \cdot w ~|~ \cdots \\
	\textit{time} &:=& d ~|~ \infty ~|~ d_1 + d_2 ~|~ d_1 - d_2 ~|~ \cdots \\
	\textit{vector} &:=& (x_1,\dots,x_n) ~|~ \vec{x} ~|~ \vec{p}(t) \\[1mm]
%\end{array}\]
%\[ \begin{array}{rll}
	\textit{state\_traj} &:=& I_{\vec{x}_0} ~|~ \vec{p}_{\vec{x}_0,\vec{e}} ~|~ \vec{p}(\cdot + d) ~|~ \vec{p}_1 \uplus \vec{p}_2 \\
	\textit{generalized\_event} &:=& \langle ch\triangleright,\mathit{val}\rangle ~|~
	\langle \mathit{time},\mathit{state\_traj},\mathit{rdy}\rangle \\
	\textit{trace} &:=& \epsilon \, | \, \mathit{generalized\_event} \,|\, \tracev \,|\,  \mathit{trace}_1 {^\chop} \textit{trace}_2 
\end{array} %\vspace*{-2mm}
\] }
Here \sm{\mathit{val}} are terms evaluating to real numbers, including process variables \sm{x_i}, constants \sm{c}, as well as arithmetic operations. \sm{\mathit{time}} evaluates to time lengths, either a positive real number or \sm{\infty}. \sm{\mathit{vector}} evaluates to vectors. We use the special symbol \sm{\vec{x}} to denote the vector consisting of all variables in the state of a sequential process in a pre-determined order. Note that this is viewed as an abbreviation, so that substitution for a particular variable \sm{x_i} will replace the corresponding component of \sm{\vec{x}}. \sm{\mathit{state\_traj}} evaluates to solutions of ODEs. Here \sm{I_{\vec{x}_0}} denotes the constant state trajectory with value \sm{\vec{x}_0}, i.e., for any \sm{t}, \sm{I(t) = \vec{x}_0} (by convention, we use \sm{\vec{x}_0} for the initial values of all process variables). \sm{\vec{p}_{\vec{x}_0,\vec{e}}} denotes the trajectory of  \sm{\vec{\dot{x}}=\vec{e}} starting from \sm{\vec{x}_0}. The second subscript \sm{\vec{e}} may be omitted if it is clear from context. \sm{\vec{p}(\cdot + d)} denotes a time shift by \sm{d} units, \sm{\vec{p}_1\uplus \vec{p}_2} denotes merging two state trajectories  for two sequential processes with disjoint sets of process variables, and \sm{\vec{p}(t)} denotes extracting 
the state at time \sm{t} from  the state trajectory $\vec{p}$. The syntax for generalized events and traces are as before. As in Hoare and He's Unifying Theories of Programming (UTP) \cite{UTP1998}, we introduce a system observational  variable, denoted by $\tracev$, to stand for the current trace of the considered process, which never occur in any process syntactically. 

The language of formulas contain the usual constructs for first-order logic. We allow equality checks between two terms of the same type, as well as comparisons via \sm{\{>, \geq, <, \leq \}} between terms evaluating to real numbers. 


\subsubsection{Semantics} 
The terms and formulas are defined over a triple \sm{\langle s, h, \lvar \rangle}, 
where  \sm{s} is a state,
\sm{h} a trace, and \sm{\lvar} a valuation  assigning values to logical variables.
The evaluation of \sm{\textit{val}}, \sm{\textit{time}} and \textit{vector} is defined with respect to  \sm{s} and \sm{\lvar}, denoted by \sm{\seman{\textit{val}}_s^\lvar},  \sm{\seman{\textit{time}}_s^\lvar} and \sm{\seman{\textit{vector}}_s^\lvar} respectively. Their definitions are routine, so we omit them here. 
The evaluation of \sm{\textit{state\_traj}} with respect to \sm{s} and \sm{\lvar}, which returns a function mapping from time to state, is given as follows:
{\small \[ \vspace*{-2mm}
\begin{array}{rcl}
	\seman{I_{\vec{x}_0}}_s^\lvar(t)  &=& s[\vec{x} \mapsto \vec{x}_0] \\
	\seman{\vec{p}_{\vec{x}_0,e}}_s^\lvar(t)  &=&
	s[\vec{x} \mapsto \vec{p}(t)]  \\
	\multicolumn{3}{l}{\qquad\mbox{where $\vec{p}$ is the solution of \sm{\vec{\dot{x}}=\vec{e}} with initial state \sm{s(\vec{x}_0)}}} \\
	\seman{\vec{p}(\cdot + d)}_s^\lvar(t) &=& \seman{\vec{p}}_s^\lvar(t+d)\\
	\seman{\vec{p}_1 \uplus \vec{p}_2} _s^\lvar(t) &=&
	\seman{\vec{p}_1}_s^\lvar(t) \uplus \seman{\vec{p}_2} _s^\lvar(t)
\end{array}
\] }
Given a state \sm{s}, a trace \sm{h} and a valuation \sm{\lvar} of logical variables,
the semantics of trace expressions 
\sm{\nseman{e}{s}{h}}
is defined as follows:
{\small \[ \vspace*{-2mm}
\begin{array}{rcl}
	%\nseman{x_i}{s}{h} &=& s(x_i) \\
	%\nseman{\vec{x}_0}{s}{h} &=& (s(x_1),\dots,s(x_n)) \\
	\nseman{\langle ch\triangleright, val\rangle}{s}{h}
	&=&  \langle ch\triangleright, \seman{\textit{val}}_{s}^{\lvar} \rangle  \\
	\nseman{\langle \textit{time}, \textit{state\_traj}, \textit{rdy} \rangle}{s}{h}	&=&\\
\multicolumn{3}{l}{\qquad	
 \langle \seman{\textit{time}}_{s}^{\lvar}, \seman{\textit{state\_traj}}_{s}^{\lvar}|_{[0, \seman{\textit{time}}_{s}^{\lvar}]}, \textit{rdy} \rangle } \\
	\nseman{\tracev}{s}{h} &=& h\\
	\nseman{\textit{trace}_1{^\chop} \mathit{trace}_2}{s}{h} &=& 
	\nseman{\textit{trace}_1}{s}{h}{^\chop}\nseman{\textit{trace}_2}{s}{h}
\end{array}
\] }
We can see that each trace expression is interpreted to a trace value defined in Sect.~\ref{sec:tracesemantics}. Especially, the state trajectory $\textit{state\_traj}$ in each continuous event 
is restricted to the time interval  $[0, \seman{\textit{time}}_{s}^{\lvar}]$, i.e. $\seman{\textit{state\_traj}}_{s}^{\lvar}|_{[0, \seman{\textit{time}}_{s}^{\lvar}]}$.  Based on the semantics of terms, the semantics of formulas can be defined as usual.

Given a (sequential or parallel) process \sm{c}, we say a Hoare triple is \emph{valid}, denoted \sm{\vDash \spec{P}{c}{Q}}, 
if for all \sm{s_1, h_1} such that \sm{\nseman{P}{s_1}{h_1}} holds, and big-step relation \sm{(c,s_1) \Rightarrow (s_2,h_2)}, then \sm{\nseman{Q}{s_2}{{h_1}^\chop h_2}} holds.  


\subsection{Proof System}
The proof system of HHL consists of three parts: the proof system for FOD\footnote{When we consider the discrete relative completeness, FOD will be replaced by first-order theory of real arithmetic.}, axioms and inference rules for timed traces and readiness, and axioms and inference rules for HCSP constructs. We won't discuss the proof theory for FOD in this paper. 

\subsubsection{Axioms and inference rules for traces and readiness} 
 Here we give a set of inference rules shown in Fig.~\ref{fig:elim-synchronization} for concluding properties of \sm{\textit{tr}}  from those of  \sm{tr_1} and 
 \sm{tr_2}, given a synchronization operation \sm{\mathit{tr_1}\|_{cs}\mathit{tr_2}\Downarrow \mathit{tr}}. 
   We omit obvious symmetric versions of rules. 

\begin{figure*}
{ \centering \small
 \begin{eqnarray*}
&  \prftree[r]{SyncPairE}{\langle ch_1\triangleright_1,v_1\rangle^\chop tr_1 \|_{cs} \langle ch_2\triangleright_2,v_2\rangle^\chop tr_2 \Downarrow \textit{tr}}{ch_1\in cs}{ch_2\in cs}{\exists \textit{tr}'.\, ch_1=ch_2 \wedge v_1 = v_2 \wedge (\triangleright_1,\triangleright_2)\in \{(!,?),(?,!)\}\wedge \textit{tr} = \langle ch,v\rangle^\chop \textit{tr}' \wedge tr_1\|_{cs}tr_2 \Downarrow \textit{tr}'} & \\%[1mm] 
& \prftree[r]{SyncUnpairE1}{\langle ch_1\triangleright_1,v_1\rangle^\chop tr_1 \|_{cs} \langle ch_2\triangleright_2,v_2\rangle^\chop tr_2 \Downarrow \textit{tr}}{ch_1\notin cs}{ch_2\in cs}
{\exists \textit{tr}'.\, \textit{tr}=\langle ch_1\triangleright_1,v_1\rangle^\chop \textit{tr}' \wedge tr_1\|_{cs} \langle ch_2\triangleright_2,v_2\rangle^\chop tr_2 \Downarrow \textit{tr}'} & \\%[1mm] 
&  \prftree[r]{SyncUnpairE2}{\langle ch_1\triangleright_1,v_1\rangle^\chop tr_1 \|_{cs} \langle ch_2\triangleright_2,v_2\rangle^\chop tr_2 \Downarrow \textit{tr}}{ch_1\notin cs}{ch_2\notin cs}
{\begin{array}{ll}
		&(\exists \textit{tr}'.\, \textit{tr}=\langle ch_1\triangleright_1,v_1\rangle^\chop \textit{tr}' \wedge tr_1\|_{cs} \langle ch_2\triangleright_2,v_2\rangle^\chop tr_2 \Downarrow \textit{tr}')\,\vee \\
		&(\exists \textit{tr}'.\, \textit{tr}=\langle ch_2\triangleright_2,v_2\rangle^\chop \textit{tr}' \wedge \langle ch_1\triangleright_1,v_1\rangle^\chop tr_1\|_{cs} tr_2 \Downarrow \textit{tr}')
\end{array}}  & \\%[1mm] 
& \prftree[r]{SyncUnpairE3}{ch\notin cs}{\langle ch\triangleright,v\rangle^\chop tr_1 \|_{cs} \langle d,\vec{p},\rdy \rangle^\chop tr_2 \Downarrow \textit{tr}}{\exists \textit{tr}'.\, \textit{tr} = \langle ch\triangleright,v\rangle^\chop \textit{tr}' \wedge tr_1\|_{cs} \langle d,\vec{p},\rdy\rangle^\chop tr_2 \Downarrow \textit{tr}'} 
\qquad \prftree[r]{SyncUnPairE4}{ch\in cs}{\langle ch\triangleright,v\rangle^\chop tr_1 \|_{cs} \langle d,\vec{p},\rdy \rangle^\chop tr_2 \Downarrow \delta} & \\
%\qquad\qquad 
& \prftree[r]{SyncWaitE1}{\neg \compat(\rdy_1,\rdy_2)}{\langle d_1,\vec{p}_1,\rdy_1\rangle^\chop tr_1 \|_{cs} \langle d_2,\vec{p}_2,\rdy_2\rangle^\chop tr_2 \Downarrow \delta}
\qquad \prftree[r]{SyncWaitE2}{\langle d,\vec{p}_1,\rdy_1\rangle^\chop tr_1 \|_{cs} \langle d,\vec{p}_2,\rdy_2\rangle^\chop tr_2 \Downarrow \textit{tr}}{\compat(\rdy_1,\rdy_2)}
{\exists \textit{tr}'.\, \textit{tr} = \langle d,\vec{p}_1\uplus \vec{p}_2,(\rdy_1\cup \rdy_2)-cs\rangle ^\chop \textit{tr}' \wedge tr_1\|_{cs} tr_2 \Downarrow \textit{tr}'} & \\%[1mm] 
& \prftree[r]{SyncWaitE3}{d_1<d_2}{\langle d_1,\vec{p}_1,\rdy_1\rangle^\chop tr_1 \|_{cs} \langle d_2,\vec{p}_2,\rdy_2\rangle^\chop tr_2 \Downarrow \textit{tr}}{\compat(\rdy_1,\rdy_2)}
{\exists \textit{tr}'.\, \textit{tr} = \langle d_1,\vec{p}_1\uplus \vec{p}_2,(\rdy_1\cup \rdy_2)-cs\rangle ^\chop \textit{tr}' \wedge tr_1\|_{cs} \langle d_2-d_1,\vec{p}_2(\cdot+d_1),\rdy_2\rangle^\chop tr_2 \Downarrow \textit{tr}'} & \\%[1mm] 
&  
\oomit{\quad
\prftree[r]{SyncEmpty1}{\epsilon \|_{\mathit{cs}} \epsilon \Downarrow \epsilon} 
\oomit{ \prftree[r]{SyncEmpty1}{ch\notin cs}{tr_1\|_{cs} \epsilon  \Downarrow \textit{tr}}
{\langle ch\triangleright,v\rangle^\chop tr_1 \|_{cs} \epsilon  \Downarrow \langle ch\triangleright,v\rangle^\chop \textit{tr}}}
\quad 
\prftree[r]{SyncEmpty2}{ch\in cs}{ }
{\langle ch\triangleright,v\rangle^\chop tr_1 \|_{cs} \epsilon  \Downarrow \delta}
  \quad 
\prftree[r]{SyncEmpty3}{tr_1\|_{cs} \epsilon  \Downarrow \textit{tr}}
{\langle d,\vec{p}_1,\rdy_1\rangle^\chop tr_1 \|_{cs} \epsilon  \Downarrow \langle d,\vec{p}_1 \uplus I_{\bot},\rdy_1\rangle^\chop \textit{tr}}
& \\
[1mm]
&}
\prftree[r]{SyncEmptyE1}{\epsilon \|_{cs} \epsilon \Downarrow tr}{tr = \epsilon} \qquad
\prftree[r]{SyncEmptyE2}{\langle d,\vec{p},\rdy\rangle^\chop tr_1 \|_{cs} \epsilon \not\Downarrow tr} \qquad \prftree[r]{SyncEmptyE3}{\langle ch*,v\rangle^\chop tr_1 \|_{cs} \epsilon \not\Downarrow tr}{ch\notin cs\wedge \exists tr'.\, tr=\langle ch*,v\rangle^\chop tr' \wedge tr_1 \|_{cs} \epsilon \Downarrow tr'} 
&
\end{eqnarray*} }
 \vspace*{-4mm} 
\caption{Inference rules for timed traces and readiness}
\label{fig:elim-synchronization}
\vspace*{-5mm}
\end{figure*}


This set of rules can be categorized by the types of initial events on the two sides. For each combination of types of initial events, there is exactly one rule that is applicable, which either produces a synchronization operation where at least one of \sm{tr_1} and \sm{tr_2} is reduced by one event, or produces a deadlock. The initial event has three cases: communication event  where the channel lies or does not lie in \sm{cs}, and continuous event. We only explain several cases because of space limit. If both sides are communication events, where the channel lies in \sm{cs}, then the two events must synchronize with each other, and they have the same channel and value (rule SyncPairE). If both sides are continuous events, then the two ready sets must be compatible (rule SyncWaitE1). Moreover,  if the two durations are equal, they can be synchronized with each other (rule SyncWaitE2); otherwise, the shorter one synchronizes with the initial part of the longer one first (rule SyncWaitE3).

Note that these rules above are essentially same as the ones in Fig.~\ref{fig:rule-synchronization}, except that
the rules in Fig.~\ref{fig:rule-synchronization} 
 compose a synchronized trace for a parallel process from the traces of its component processes, while these  rules above decompose the trace of a parallel process  into the traces for its component processes in order to split a complicated proof obligation into several smaller ones.

\subsubsection{Axioms and inference rules for HCSP constructs}

%We now state the inference rules for HCSP, starting from the %weakest liberal precondition calculus for 
%sequential HCSP processes. 
The axioms and rules for the constructs of HCSP are presented in Fig.~\ref{fig:hoarelogic}. We explain them in sequence below. 

\begin{figure*}
{\small 
\begin{eqnarray*}
  &   \prftree[r]{ Skip }{\spec{Q}{\pskip}{Q}}   \qquad
	\prftree[r]{ Assign }{\spec{Q[e/x]}{x:=e}{Q}} 
	\qquad \prftree[r]{ Output }{\left\{
	 	\begin{array}{ll}
	 		Q[\tracev^\chop \langle ch!, e\rangle/\tracev]\,\wedge \\
	 		\forall d>0.\,Q[\tracev^\chop \langle d, I_{\vec{x}_0}, \{ch!\}\rangle^\chop \langle ch!, e\rangle/\tracev]\,\wedge \\
	 		Q[\tracev^\chop \langle \infty, I_{\vec{x}_0}, \{ch!\}\rangle/\tracev]
	 	\end{array}
	 	\right\}
	 	\ ch!e\ \{Q\}} & \\%[1mm] 
 	 & \prftree[r]{ Input }{
	 	\left\{
	 	\begin{array}{ll}
	 		\forall v.\, Q[v/x, \tracev^\chop \langle ch?, v\rangle/\tracev]\,\wedge \\
	 		\forall d>0.\,\forall v.\, Q[v/x, \tracev^\chop \langle d, I_{\vec{x}_0},\{ch?\}\rangle ^\chop \langle ch?,v\rangle /\tracev]\,\wedge \\
	 		Q[\tracev^\chop\langle \infty, I_{\vec{x}_0}, \{ch?\}\rangle/\tracev]
	 	\end{array}
	 	\right\}
	 	\ ch?x\ \{Q\}} \quad \prftree[r]{ Cont }{
 		\left\{
 		\begin{array}{ll}
 			(\neg B \rightarrow Q)\,\wedge 	\forall d>0.\\
 		\,(\forall t\in[0,d).\,B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]\wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \\
 			 \to Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle/\tracev])
 		\end{array}
 		\right\}\ \evo{x}{e}{B}\ \{Q\}} & \\[1mm] 
 	& \prftree[r]{ Int }{
 		\begin{array}{l}
 			\forall i\in I.\,\textrm{if } ch_i*=ch!e \textrm{ then} ~ \spec{Q_i}{c_i}{R} \textrm{ and }  
 		    P \rightarrow Q_i[\tracev^\chop \langle ch!,e\rangle/\tracev]\,\wedge \\
 			\qquad\qquad P \rightarrow \forall d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \to Q_i[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,e[\vec{p}_{\vec{x}_0}(d)/\vec{x}]\rangle/\tracev] \\
 			\qquad\quad \textrm{elif } ch_i*=ch?x \textrm{ then} ~  \spec{Q_i}{c_i}{R} \textrm{ and }   P \rightarrow \forall v.\, Q_i[v/x, \tracev^\chop \langle ch?,v\rangle/\tracev]\,\wedge \\
 			\qquad\qquad P \rightarrow \forall d>0.\, \forall v.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]  \to 
 			Q_i[\vec{p}_{\vec{x}_0}(d)/\vec{x},v/x,\tracev^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch?,v\rangle/\tracev] \\
 			\spec{P \land \neg B}{c}{R}  \\
 			P\rightarrow (\forall d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]  \wedge \neg B[\vec{p}_{\vec{x}_0}(t)/\vec{x}] ) \to   Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d, \vec{p}_{\vec{x}_0}, \rdy(\cup_{i\in I} ch_i*)\rangle/\tracev]) \land \spec{Q}{c}{R}
 		\end{array}
 	}
 	{ \spec{P}{\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}}{R} } & \\[1mm]
 	&  \prftree[r]{ Par }
 	{\spec{P_1}{c_1}{Q_1}}{\spec{P_2}{c_2}{Q_2}}
 	{\spec{P_1[\epsilon/\tracev] \wedge P_2[\epsilon/\tracev]}
 		{c_1 \|_{cs} c_2}
 		{\exists tr_1,\, tr_2.\, Q_1[tr_1/\tracev] \wedge Q_2[tr_2/\tracev] \wedge tr_1\|_{cs}tr_2 \Downarrow \tracev}} & \\[1mm]
 	%\quad	
   &  
	\prftree[r]{Seq}{\spec{P}{c_1}{Q}}{\spec{Q}{c_2}{R}}
 	    {\spec{P}{c_1; c_2}{R}}	\quad 
 	    \prftree[r]{Cond}{\spec{P\wedge b}{c_1}{Q}}{\spec{P \wedge \neg b}{c_2}{Q}}
 	{\spec{P}{\IFE{b}{c_1}{c_2}}{Q}} \quad %\prftree[r]{Repetition}{\spec{P}{c}{P}}
 %{\spec{P}{c^*}{P}} 
 \quad
 	%\qquad 
 	\prftree[r]{IChoice}{\spec{P_1}{c_1}{Q}}{\spec{P_2}{c_2}{Q}}
 	{\spec{P_1 \wedge P_2}{c_1 \sqcup c_2}{Q}}
 & \\[1mm]	 
 	& \prftree[r]{Rep}{\spec{P}{c}{P}}{\spec{P}{c^*}{P}} \quad \prftree[r]{Conj}{\spec{P}{ c}{Q} }{\spec{P}{ c}{R} }
 	{\spec{P}{ c}{Q \wedge R}}
 	\quad\prftree[r]{Inv}{\freev(P) \cap \var(c) = \emptyset}{\spec{P}{c}{P}}
 	\quad
 	\prftree[r]{Conseq}{P'\rightarrow P}{Q\rightarrow Q'}{\spec{P}{c}{Q}}{\spec{P'}{c}{Q'}} &  \vspace*{-6mm} 
 \vspace*{-6mm} 
 	\end{eqnarray*}} \vspace*{-4mm} 
	\caption{Axioms and inference rules for HCSP constructs}
	\label{fig:hoarelogic}  \vspace*{-4mm} 
\end{figure*}

\begin{itemize}
	 \item
The axioms   for $\pskip$ and assignment, and rules for  sequential composition, conditional  statement and internal choice are as usual. 

\item 
For communication events, we need to consider  when a communication event can happen as it may need to wait for its dual from the environment for 
synchronization, that could be one of  three 
possibilities,
  
see axioms Output and Input. These axioms also provide a way 
to compute the weakest precondition 
w.r.t. a given postcondition. 

\item The axiom for ODE \sm{\evo{x}{e}{B}} considers two cases: when the domain \sm{B} is initially false (and the process terminates immediately), or when the ODE evolves for some positive amount of time \sm{d} (axiom Cont).
In Cont, \sm{\vec{p}_{\vec{x}_0}} is the unique solution of \sm{\vec{\dot{x}}=\vec{e}} starting from \sm{\vec{x}_0}. \oomit{This rule can be directly applied if there is an explicit solution of the ODE. For ODEs without explicit solutions, We will derive more convenient rules based on differential invariants later on.}

\item 
For interrupt (rule Int),  precondition \sm{P} should imply the weakest precondition derived from each of the possibilities. Here \sm{Q_i} is a family of predicates indexed by \sm{i\in I}.

\item  
The rule for repetition is defined with the help of a loop invariant.

\item Moreover, for completeness, several  general rules including invariance, conjunction and consequence are added (rules Inv, Conj and Conseq). 
\end{itemize}

We now turn to the rule for parallel processes (rule Par). Any state \sm{s} of \sm{c_1\|_{cs}c_2} can be written in the form \sm{s_1\uplus s_2}, where \sm{s_1} and \sm{s_2} are states of \sm{c_1} and \sm{c_2}, respectively. Here \sm{P_1} and \sm{Q_1} are predicates on the state for \sm{c_1}, and \sm{P_2} and \sm{Q_2} are predicates on the state for \sm{c_2}. In the postcondition, we require that the trace of the parallel program is a synchronization of the traces of \sm{c_1} and \sm{c_2}.  

For any HCSP process \sm{c}, if \sm{\spec{P}{c}{Q}} is derived by the above inference rules, we write \sm{\vdash \spec{P}{c}{Q}}. The following theorem indicates that the proof systems given in Fig.~\ref{fig:elim-synchronization} and  Fig.~\ref{fig:hoarelogic} are sound (The proofs are given in Appendix~\ref{app:soundness}).
\begin{theorem}[Soundness] \label{thm:soundness} 
	If \sm{\vdash \spec{P}{c}{Q}}, then \sm{\vDash \spec{P}{c}{Q}}. Furthermore, reasoning about 
	\sm{tr_1\|_{cs}tr_2 \Downarrow \textit{tr}} according to the rules of Fig.~\ref{fig:elim-synchronization} is valid according to the rules in Fig.~\ref{fig:rule-synchronization}. 
\end{theorem}

Obviously, \sm{\models \{\top\} ~ {\textbf{while} ~B ~ \textbf{ do } ~S} ~ \{\bot\}} 
if and only if
\sm{\textbf{while} ~B ~ \textbf{do} ~S} does not terminate. As argued in \cite{Cook78}, in order to 
specify termination, an assertion logic should be at least as expressive as Peano arithmetic, which is not 
complete according to G\"{o}del's Incompleteness Theorem \cite{godel1931}. So, the proof system of HHL is 
not complete. Moreover, its validity is not decidable either, even not semi-decidable, as \emph{multiple-path polynomial programs} (MPP), whose termination problem is even not semi-decidable \cite{Manna05},  can be easily modelled with HCSP. 


\begin{theorem}[Incompleteness and Undecidability]
The proof system of HHL is incomplete, and the validity of HHL is undecidable, even not semi-decidable. 
\end{theorem}

Additionally, as we pointed out before, in order to deal with communication and concurrency, we introduce \emph{generalized events}, 
\emph{traces} and \emph{trace synchronization}. Thus, the execution of 
a process may start with some history trace, but the following 
theorem indicates that our proof system can guarantee any execution of a process itself is indeed independent of any history trace, which is in accordance with  the healthiness condition given in UTP \cite{UTP1998}.
\begin{theorem} \label{thm:trace-independence}
If $\vdash \spec{P\wedge \tracev=\epsilon}{c}{Q\wedge \tracev=h} $, then for any trace $h'$ with $\freev(h') \cap \wvar(c) = \emptyset$, we have 
 $\vdash \spec{P\wedge \tracev=h'}{c}{Q\wedge \tracev=h'^\chop h} $, where $\wvar(c)$ returns the variables that are updated  by $c$. %$h'$ is required not to contain these variables. 
\end{theorem}
 

\subsection{Discussion on partial correctness, total correctness, deadlock, livelock and invariants}
\emph{Partial correctness vs total correctness.} 
If we focus on partial correctness, our proof system needs to add  the following inference rule for deadlock: 
\begin{eqnarray*}
& \prftree[r]{Deadlock}{\spec{P}{c}{\tracev=\delta}}{Q \mbox{ is any formula of FOD}}{}{\spec{P}{c}{Q}} & 
\end{eqnarray*}
Rule Deadlock says that  a deadlocked behaviour can imply any property, as it never terminates. 
If we investigate total correctness, we need a rule for variant  as in classical Hoare logic. We will address this issue together with \emph{livelock} for future work. 

\noindent 
\emph{Deadlock and livelock.} \emph{Deadlock} can be handled in our proof system with the rules on trace synchronization and the above rule, but {\em livelock} is not considered. {\em Livelock} could be handled by proving/disproving its existence/absence  like program termination analysis, or 
by allowing recording internal action in traces and checking whether 
there are infinitely many internal actions in a finite time horizon. 
As said above, we will address this issue for future work. 

\noindent 
\emph{Loop invariant and differential invariant.}
Reasoning about repetition needs invariants. Although continuous evolution can be reasoned about by explicitly using  its solution as indicated in rule Cont, differential invariants can 
ease the reasoning very much as obtaining a solution of  a ODE is mathematically  difficult. 
As in classical Hoare logic, invariant generation plays a central role in deductive verification of HSs. 
But in HSs, one has to consider to synthesize \emph{global invariants} (for loops and recursions) and 
local (differential) invariants (for ODEs) simultaneously.  As discussed in \cite{PlatzerClarke08}, synthesizing global invariants can be achieved by combining invariant generation techniques for discrete programs and differential invariant generation techniques for ODEs. 
In the literature, there are various work on differential invariant synthesis for dynamical systems. 
\cite{LiuEmsoft} gave a necessary and sufficient condition for a semi-algebraic set to be a differential invariant of a polynomial system. Based on which, \cite{WangCAV2021} proposed  a very efficient approach for synthesizing semi-algebraic invariants for polynomial dynamical systems by exploiting 
difference of convex programming.   \cite{PlatzerT20} presented  a complete axiomatic system for reasoning about differential invariant based on a similar condition to \cite{LiuEmsoft}. 

Alternatively, reasoning about continuous evolution  can be conducted  by discretization, e.g., \cite{TOSEM20} presented  a set of refinement rules to discretize HCSP, further refined discretized HCSP  to SystemC. 
While, \cite{LP16} presented a refinement logic 
which investigates the inverse direction to reduce verification of discrete systems to verification of hybrid systems.



\section{ \hspace*{-4mm} Continuous relative completeness}\label{sec:completeness}

In this section, we show continuous relative completeness of HHL w.r.t. FOD. This is done in two steps. First, we show that the proof system is complete if all weakest liberal preconditions/strongest postconditions can be expressed in FOD and all valid entailments between predicates can be proved. Second, we %define an encoding of traces in the language of FOD, and 
show that all required predicates can be expressed as formulas of FOD. Following the form of the proof system given above, weakest liberal preconditions are used in the sequential case, and strongest postconditions are used in the parallel case.


\subsection{Weakest Liberal Preconditions and Strongest Postconditions}

For the sequential case, given a process \sm{c} and postcondition \sm{Q}, the weakest liberal precondition \sm{\wp(c,Q)} is a predicate on state and trace pairs, defined as:
{\small \[\wp(c,Q) = \{(s, tr)\,|\,\forall s', tr'. (c, s) \Rightarrow (s', tr') \rightarrow Q(s', tr^\chop tr')\}\]}
Thus, the computation of weakest liberal preconditions is straightforward from the definition of big-step semantics. Most of them correspond directly to the Hoare rules in Fig.~\ref{fig:hoarelogic}. %They are listed in Figure~\ref{fig:wp} in full for reference.
The only $\wp$ rule that does not allow direct computation is that for repetition. Instead it satisfies the following recursive equation:
\[\wp(c^*, Q) = \wp(c; c^*, Q)\]
The recursive  equation is not solvable in general, but it provides a way to approximate $\wp(c^*,Q)$ according to provided invariants for $c^*$. So,  as the verification of programs with classical Hoare logic, invariant generation plays a central role in the verification of HSs with HHL. 
Regarding \sm{\wp}, we have the following result.
\begin{lemma}\label{lem:wp-complete}
	For any sequential process \sm{c},  \sm{\vdash \{\wp(c,Q)\}\ c\ \{Q\}}.
\end{lemma}
Next, we consider the case of parallel processes. Here we make use of strongest postconditions. Given a parallel process \sm{c} w.r.t. a given precondition \sm{P} on the global state, the strongest postcondition \sm{\sp(c,P)} is a predicate on state and trace, such that \sm{\sp(c,P)(s',\textit{tr}')} holds iff  there exists \sm{s} satisfying \sm{P}, such that it is possible to go from \sm{(c,s)} to \sm{(s',\textit{tr}')} under big-step semantics.


The strongest postcondition can be recursively computed for preconditions $P$ in the form of conjunctions of predicates on individual processes. For a single process, it is equivalent to strongest postcondition for sequential processes. For the parallel composition of two processes, { %\color{red}
 define $P_1 \uplus P_2$  by $(P_1\uplus P_2)(s_1\uplus s_2)=P_1(s_1)\wedge P_2(s_2)$,} then we have:
{\small \[ 
\begin{array}{l}
	\sp(c_1\|_{cs} c_2, P_1 \uplus P_2)(s_1\uplus s_2, \textit{tr}) = \\
	(\exists tr_1\ tr_2.\, \sp(c_1,P_1)[tr_1/\textit{tr}] \wedge \sp(c_2,P_2)[tr_2/\textit{tr}] \wedge tr_1\|_{cs}tr_2 \Downarrow \textit{tr})
\end{array} \]}
From this, we get the following lemma:
\begin{lemma}\label{lem:sp-parallel-complete}
	For any parallel process \sm{c}, and precondition \sm{P} in the form of conjunction of predicates on individual processes of \sm{c}, we have \sm{\vdash \{P\}\ c\ \{\sp(c,P)\}}.
\end{lemma}


From Lemma~\ref{lem:wp-complete} and Lemma~\ref{lem:sp-parallel-complete}, we get the following theorem, under the assumption of expressibility of predicates and provability of entailments in the underlying logical system.
\begin{theorem}\label{thm:continuous-complete}
 Suppose all valid \text{FOD} formulas are provable, then if  \sm{\{P\}\ c\ \{Q\}} is valid, then it is provable in the above system.
\end{theorem}


\subsection{Expressing Predicates in FOD}
By Theorem~\ref{thm:continuous-complete}, in order to prove the continuous relative completeness, the only  remained is to  show expressibility of traces and trace
assertions in FOD. 
%which is introduced in~\cite{Platzer08} in the context of {\dL}. 
For  simplicity, we use  \sm{\vec{x}_0 \langle \vec{\dot{x}}=\vec{e} \rangle \vec{x}_1 } to mean  
that starting from vector \sm{\vec{x}_0}, following the differential equation \sm{\vec{\dot{x}}=\vec{e}}, \sm{\vec{x}_1} can be reached.

\subsubsection{Encoding traces}

First, we discuss how to encode traces that appear in the previous sections in FOD. Using the $\mathbb{R}$-G\"odel encoding in \cite{Platzer12a}, it is possible to encode any sequence of real numbers of fixed length as a single real number. The basic idea (for two real numbers) is as follows. Suppose real numbers $a$ and $b$ are written as $a_0.a_1a_2\dots$ and $b_0.b_1b_2\dots$ in binary form, then the pair $(a,b)$ can be represented as $a_0b_0.a_1b_1a_2b_2\dots$. In fact, we can extend this encoding to a sequence \sm{\vec{y}} of real numbers with arbitrary length, by encoding \sm{\vec{y}} as the $\mathbb{R}$-encoding of the pair \sm{(l,y)}, where \sm{l} stores the length of \sm{\vec{y}}, and \sm{y} is the $\mathbb{R}$-G\"odel encoding of \sm{\vec{y}}. From now on, we will make implicit use of this encoding, allowing us to quantify over sequences of real numbers of arbitrary length, and adding to the language the function \sm{\len(x)} for the length of sequence \sm{x}, and \sm{x_i} (with \sm{1\le i\le \len(x)}) for the \sm{i^{th}} component of \sm{x}.

Given an HCSP process, we can fix a mapping from the channel names appearing in the process to natural numbers. Hence a communication event  of the form \sm{\langle ch\triangleright,v\rangle} can be encoded as a real number. Encoding a continuous event 
of the form \sm{\langle d,\vec{p},\rdy\rangle} requires more care. The ready set can be encoded as a natural number since the total number of channels is finite. The main problem is how to encode the state trajectory  \sm{\vec{p}} in the continuous event.
We make the restriction that any state trajectory 
\sm{\vec{p}} appearing in a continuous event of the trace must be either constant or a solution of an ODE appearing in the HCSP process. This restriction is reasonable since any other state trajectory 
cannot possibly appear in the behavior of the process. We number the ODEs appearing in the process as \sm{\langle \vec{\dot{x}}=\vec{e_1}\rangle,\dots,\langle\vec{\dot{x}}=\vec{e_k}\rangle}, and let  \sm{\langle\vec{\dot{x}}=\vec{e_0}\rangle} be the ODE \sm{\langle \vec{\dot{x}}=0\rangle} (for the case of constant state trajectories).

First, we consider the sequential case. Then a state trajectory 
\sm{\vec{p}} in a continuous event  can be encoded as a triple \sm{(d,\vec{p}_0,i)}, where \sm{d} is its duration, \sm{\vec{p}_0} is the initial state, and \sm{0\le i\le k} is the index of the differential equation satisfied by \sm{\vec{p}}. We can then define \sm{\vec{p}_{\varsigma}} for $\varsigma<d$, the state of the state trajectory 
at time $\varsigma$, as the unique state satisfying the FOD formula 
\[ (\vec{x}=\vec{p}_0 \wedge t=0)\langle \vec{\dot{x}}=\vec{e}_i, t'=1 \rangle (\vec{x}=\vec{p}_{\varsigma} \wedge t=\varsigma).  \]
For the parallel case, the state is eventually divided into component states for sequential processes, so that each component state follows one of the ODEs \sm{\vec{\dot{x}}=\vec{e}_i}. Hence, it can be encoded as a binary tree where each leaf node contains a tuple of the form \sm{(d,\vec{p}_0,i)}. For example, if we have a parallel of two sequential processes, with a path starting from state \sm{\vec{p}_0} and following ODE \sm{i} on the left, and starting from state \sm{\vec{q}_0} and following ODE \sm{j} on the right, then this state trajectory  is encoded as \sm{((d,\vec{p}_0,i),(d,\vec{q}_0,j))}.

With this encoding, it is clear that given representations of state trajectories  \sm{\vec{p}_1} and \sm{\vec{p}_2}, the state trajectory  \sm{\vec{p}_1\uplus \vec{p}_2} can be represented. For the purpose of encoding synchronization below, we also need to encode \sm{\vec{p}(\cdot + d')}. With \sm{\vec{p}} given as \sm{(d,\vec{p}_0,i)}, this is simply \sm{(d-d',\vec{p}_{d'},i)}.

Hence, we can encode any general event  as a single real number. Then a trace can also be encoded as a single real number. So, we can also encode operations on traces such as the join operation.


\subsubsection{Encoding synchronization} A key relation that needs to be encoded is the synchronization relation \sm{tr_1\|_{cs} tr_2 \Downarrow \textit{tr}}. We note that each of the synchronization rules (except SyncEmpty) produces an extra general event  in \sm{\textit{tr}}. Hence, the derivation of \sm{tr_1\|_{cs}tr_2 \Downarrow \textit{tr}} consists of exactly \sm{\len(\textit{tr})} steps plus a final SyncEmpty step. We encode the relation by encoding the entire derivation using two sequences of traces \sm{T_1} and \sm{T_2}, intended to represent intermediate traces for \sm{tr_1} and \sm{tr_2}:
{\small \[ \vspace*{-1mm}
\begin{array}{ll}
	tr_1\|_{cs}tr_2\Downarrow \textit{tr} \equiv \exists T_1,\,T_2.
	 \len(T_1) = \len(\textit{tr}_1) + 1 \wedge \,  \\ \  \len(T_2) = \len(\textit{tr}_2) + 1
	\wedge T_{11} = tr_1 \wedge T_{21} = tr_2 \wedge T_{1,k+1} = T_{2,k+1}=\epsilon\, \\
	\qquad  \wedge\forall 1\le i\le k.\,
	\textit{Step}_{cs}(T_{1i},T_{1,i+1},T_{2i},T_{2,i+1},tr[i])
\end{array}
\] }
where \sm{	\textit{Step}_{cs}(tr_1,tr_1',tr_2,tr_2',e)} means one step in the derivation of the synchronization relation, reducing $tr_1$ to $tr_1'$ and $tr_2$ to $tr_2'$, and producing event $e$. It is obtained by encoding the definition of synchronization in Fig.~\ref{fig:rule-synchronization}.


\subsubsection{Encoding the predicates}

We now show that using the above encoding, each of the weakest liberal precondition formulas can be rewritten in the language of FOD. For the sequential case, the only tricky case is encoding the continuous evolution using the method above. As an example, consider the second conjunct of the weakest liberal precondition for ODE:
{\small \[ \vspace*{-1mm}
\begin{array}{ll}
\forall d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}] \wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}]) \to \\
\qquad
Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop\langle d,\vec{p}_{\vec{x}_0},\emptyset\rangle/\tracev].
\end{array}
\] }
Suppose the differential equation \sm{\vec{\dot{x}}=\vec{e}} is numbered as \sm{\vec{\dot{x}}=\vec{e}_i}, then the condition can be written equivalently as follows, unfolding the encoding of \sm{\vec{p}_{\vec{x}_0}} as \sm{(d,\vec{x}_0,i)}:
\[ \vspace*{-1mm}
\begin{array}{l}
\forall d>0.\, (\forall t\in[0,d).\, B[(d,\vec{x}_0,i)_t/\vec{x}] \wedge \neg B[(d,\vec{x}_0,i)_d/\vec{x}])\to \, \\ \qquad
Q[(d,\vec{x}_0,i)_d/\vec{x}, \tracev^\chop\langle d, (d,\vec{x}_0,i),\emptyset\rangle/\tracev].
\end{array}\]
%For recursion, define by induction on natural numbers:
%{\small \[\begin{array}{rl}
%	\wp_0(c,Q) = true, & 
%	\wp_{k+1}(c,Q) = \wp_k(c[c/X], Q).
%\end{array}\] }
%Then we can write:
%$ \wp(\mu X.c,Q) = (\exists k >0.\, \wp_k(c, Q)) $.
For repetition, define by induction on natural numbers:
{\small \[\begin{array}{rl}
	\wp_0(c^*,Q) = true, & 
	\wp_{k+1}(c^*,Q) = \wp_k(c; c^*, Q).
\end{array}\] }
Then we can write:
$ \wp(c^*,Q) = (\exists k >0.\, \wp_k(c^*, Q)) $.

This concludes the case of sequential programs. For the case of parallel programs, the only extra relation is the synchronization relation, whose expressibility is shown in the previous subsection.

\section{Discrete relative completeness}
\label{sec:discretecomplete}
In this section, we prove the discrete relative completeness of our proof system in the sense that all continuous evolution can be approximated by discrete actions with arbitrary precision, which is similar to the discrete relative completeness for \dL\ in \cite{Platzer12a}.


 We say two generalized events  are within distance $\epsilon$ if 
\begin{itemize}
	\item  they are both wait events, of the form \sm{\langle d_1,\vec{p}_1,\rdy_1\rangle} and \sm{\langle d_2,\vec{p}_2,\rdy_2\rangle} respectively, and \sm{|d_1-d_2|<\epsilon}, \sm{\|\vec{p}_1(t)-\vec{p}_2(t)\|<\epsilon} for all \sm{0<t<\min(d_1,d_2)}, and \sm{\rdy_1=\rdy_2}; or
	\item  they are both communication events, of the form \sm{\langle ch_1\triangleright_1,v_1\rangle} and \sm{\langle ch_2\triangleright_2,v_2\rangle}, and \sm{ch_1\triangleright_1=ch_2\triangleright_2} and \sm{|v_1-v_2|<\epsilon}.
\end{itemize}
Two traces \sm{tr_1} and \sm{tr_2} are within distance $\epsilon$, denoted by \sm{d(tr_1, tr_2) < \epsilon}, if they contain the same number of generalized events, and each pair of corresponding generalized events are within $\epsilon$. Two states \sm{s_1} and \sm{s_2} are within distance $\epsilon$, denoted by \sm{d(s_1, s_2) < \epsilon},  if \sm{|s_1(x) - s_2(x)| < \epsilon} for all variables \sm{x}. The \sm{\epsilon$-neighborhood $\mathcal{U}_{\epsilon}(s, \textit{tr})} of a pair \sm{(s, \textit{tr})} of state and trace is defined as:
{\small \[\vspace*{-1mm} \mathcal{U}_{\epsilon}(s, \textit{tr}) \triangleq \{(s', \textit{tr}') ~|~ d(s, s') < \epsilon \wedge  d(\textit{tr},\textit{tr}') < \epsilon\}. \]}
Similarly, we define the $\epsilon$-neighborhood of a state \sm{s}. The $-\epsilon$ neighborhood of a predicate \sm{Q} on pairs of state and trace is the set of pairs for which their $\epsilon$-neighborhoods satisfy \sm{Q}. That is,
{\small \[\vspace*{-1mm} \mathcal{U}_{-\epsilon}(Q) \triangleq \{(s, \textit{tr}) ~|~ \forall (s', \textit{tr}') \in \mathcal{U}_{\epsilon}(s, \textit{tr}).\, Q(s', \textit{tr}')\}. \]}
Similarly, we define  \sm{\mathcal{U}_{-\epsilon}(B)} for a predicate \sm{B} on states only. A predicate \sm{Q} is \emph{open} if for any state \sm{s} and trace \sm{\textit{tr}} satisfying \sm{Q}, there exists $\epsilon>0$ such that \sm{Q(s', \textit{tr}')} holds for all \sm{(s', \textit{tr}') \in \mathcal{U}_{\epsilon}(s, \textit{tr})}.  For discrete relative completeness, we  consider the case for \emph{open} pre- and post-conditions first, then show  the general case in  Theorem~\ref{thm:dicreteRC}.

Next, we define the Euler approximation to the solution \sm{\vec{p}_{\vec{x}_0}} of an ODE \sm{\vec{\dot{x}}=\vec{e}} w.r.t. an initial vector \sm{\vec{x}_0}. Given a step size \sm{h>0}, a \emph{discrete solution} starting at \sm{\vec{x}_0} is a sequence \sm{(\vec{x}_0,\dots,\vec{x}_n,\dots)} with   \sm{\vec{x}_{i+1} = \vec{x}_i + h\cdot \vec{e}(\vec{x}_i)}, for \sm{i=0,1,\ldots}. We define the continuous approximation by joining the discrete points with straight lines. Define a function \sm{\vec{f}_{\vec{x}_0,h}:\mathbb{R}^+\to S} by \sm{\vec{f}_{\vec{x}_0,h}((i+t)h) = \vec{x}_i + (\vec{x}_{i+1}-\vec{x}_i)t} for any integer \sm{i\ge 0} and fractional part \sm{0\le t<1}.

\subsection{Discretization Rule for Continuous Evolution}
We first  define assertions equivalent to those appearing in the rule for ODE
with the assumption that  \sm{B} is \emph{open}. Such assumption will be dropped later when proving the final theorem for discrete completeness. 
The original precondition on the continuous solution is
{\small  \[ \vspace*{-1mm}
\begin{array}{l}
	\forall d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \to \\
	 \qquad 
	Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle/\tracev]
\end{array} 
\tag{CP}\] }
%where \sm{\vec{p}_{\vec{x}_0}} is the unique solution starting from \sm{\vec{x}_0} and following the ODE \sm{\vec{\dot{x}}=\vec{e}}. 
Note that  there is at most one value of \sm{d} for which the precondition holds, which is the maximal duration that  the ODE can continuously evolve subject to \sm{B}. \sm{d} may not exist in case the ODE has infinite time horizon trajectory on which \sm{B} always holds. 

The discrete version of the assertion, without mentioning solutions to ODEs, is as follows:
{\small \[ 
\begin{array}{ll}
	\forall T>0.\, (\forall 0\le t<T.\, \exists \epsilon_0>0.\, \forall 0<\epsilon<\epsilon_0.\,  \\ \qquad \quad
	\exists h_0>0.\, \forall 0<h<h_0.\, \vec{f}_{\vec{x}_0,h}(t)\in \mathcal{U}_{-\epsilon}(B)) \to \\
 	 (\exists \epsilon_0>0.\, \forall 0<\epsilon<\epsilon_0.\, \exists h_0>0.\, \forall 0<h<h_0.\, 
	%\\ \qquad\qquad
	  
	\\ \hspace*{-7mm} \vec{f}_{\vec{x}_0,h}(T) \in \neg \mathcal{U}_{-\epsilon}(B) \to
	(\vec{f}_{\vec{x}_0,h}(T), \tracev^\chop \langle T, \vec{f}_{\vec{x}_0,h}, \emptyset \rangle) \in \mathcal{U}_{-\epsilon}(Q))
	\tag{DP}	
\end{array} \] }
where \sm{\vec{f}_{\vec{x}_0,h}} is the continuous approximation defined above. 
%According to the syntax of HCSP, the domain $B$ is open. Moreover, we assume for now that $Q$ is open. 
{ %\color{red} 
As stated by (DP), the continuous approximation  of the solution at time $t$, i.e. $\vec{f}_{\vec{x}_0,h}(t)$,  is within $B$ and moreover the distance between it  and the boundary of $B$ must be at least $\epsilon$, and if  the continuous approximation  of the solution at exiting  time $T$ is no longer within the $-\epsilon$ neighborhood of $B$, then  the corresponding state and trace pair at time $T$ must be within $Q$ and the distance between it and the boundary of postcondition $Q$ must be at least  $\epsilon$.}

To justify the equivalence between the predicates CP and DP, we first need to estimate the global error of Euler approximations. According to~\cite{Walter98}, we have the following theorem.

\begin{theorem} \label{thm:globalerror}
	Let \sm{\vec{p}(t)} be a solution of the initial value problem \sm{\vec{\dot{x}} = \vec{e}}, \sm{\vec{p}(0)=\vec{x}_0} on the time interval \sm{[0, T]}.
	Let \sm{L} be the Lipschitz constant of the ODE \sm{\vec{\dot{x}}=\vec{e}}, that is, for any compact set \sm{S} of $\mathbb{R}^n$, \sm{\|\vec{e}(\vec{y}_1) - \vec{e}(\vec{y}_2)\| \leq L\|\vec{y}_1 - \vec{y}_2\|} for all \sm{\vec{y}_1, \vec{y}_2 \in S}. Then there exists a step size \sm{h_e > 0} such that for all \sm{0<h\leq h_e} and all \sm{n} with \sm{nh \leq T}, the global discretization error between \sm{\vec{p}(nh)}  and the discrete solution
	\sm{\vec{x}_{n} = \vec{x}_{n-1} + h \cdot \vec{e}(\vec{x}_{n-1})} satisfies 
{\small \[
		\|\vec{p}(nh) - \vec{x}_{n}\| \leq  \frac{h}{2}\max_{\theta \in [0, T]} \left\| \frac{d^2 \vec{p}}{dt^2}(\theta) \right\|\fracN{e^{LT} - 1}{L}.
  \] }
\end{theorem}

Using Theorem~\ref{thm:globalerror}, we can always find a sufficiently small time step $h$ such that the error between the exact solution and the discrete approximations is arbitrarily small. The following theorem is proved in Appendix~\ref{app:completeness}. 
\begin{theorem} \label{thm:CP-DP} 
\sm{\models \text{CP} \leftrightarrow \text{DP}}.    
\end{theorem}

\subsection{Discretization Rule for Continuous Interrupt}
An approximation to the rule 
 for continuous interrupt can be presented similarly. There are three occurrences  of solutions to ODEs in the rule. The third occurrence deals with the case where the ODE exits without performing a communication, which 
has the same form as the rule for continuous evolution, so can be handled in the same way. The other two occurrences deal with
the cases where the ODE is interrupted by an output and input communication, respectively. They can be  handled similarly, so we only show the output case. The corresponding assertion in the weakest precondition is
{\small \[ \hspace*{-2mm}
\begin{array}{l}
	\forall d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \to 
	\ Q[\vec{p}_{\vec{x}_0}(d)/\vec{x},  \\
	 \quad \quad 
	 \tracev^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i* )\rangle^\chop \langle ch!,e[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \rangle/\tracev]
\end{array} \tag{CI}
\] }
%where \sm{\vec{p}_{\vec{x}_0}} is the unique solution starting from \sm{\vec{x}_0} and following  \sm{\vec{\dot{x}}=\vec{e}}. 
The discrete version of the assertion is:
{\small \[ 
\begin{array}{l} 
	\forall T>0.\, (\forall 0\le t<T.\, \exists \epsilon_0>0.\, \forall 0<\epsilon<\epsilon_0.\,  \\
	 \qquad 
	\exists h_0>0.\, \forall 0<h<h_0.\, \vec{f}_{\vec{x}_0,h}(t)\in\mathcal{U}_{-\epsilon}(B) ) \to \\
 \exists \epsilon_0>0.\, \forall 0<\epsilon<\epsilon_0.\,
	\exists h_0>0.\, \forall 0<h<h_0.\,  
  (\vec{f}_{\vec{x}_0,h}(T), 
   \\   
 \hspace*{-6mm} \tracev^\chop \langle T,\vec{f}_{\vec{x}_0,h},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,e[\vec{f}_{\vec{x}_0,h}(T)/\vec{x}] \rangle)  %\\
     %\quad \quad  \quad \quad \quad 
  \in \mathcal{U}_{-\epsilon}(Q) \tag{DI} 
\end{array} \] }
%where $\vec{f}_{\vec{x}_0,h}$ is the continuous approximation. % defined previously. 

The following theorem is proved in Appendix~\ref{app:completeness}.
\begin{theorem} \label{thm:CI-DI} 
\sm{\models \text{CI} \leftrightarrow \text{DI}}.
\end{theorem}


Now we present the discrete relative completeness theorem.
\begin{theorem}[Discrete Relative Completeness] \label{thm:dicreteRC}
	The proof system  presented in Sect.~\ref{sec:hoare}, plus \sm{\text{CP} \leftrightarrow \text{DP}} and \sm{\text{CI} \leftrightarrow \text{DI}}, are complete relative to the discrete fragment, without referring to solutions of differential equations. 	
\end{theorem} 

In the proof of Theorem~\ref{thm:dicreteRC}, we exploit the fact that any set can be represented as the intersection of an open set and a closed set, while a closed set can be represented as the intersection of a sequence of (possibly infinitely many) open sets.

Using the same method as for proving the continuous relative completeness of the full HHL, we can show that the proof system without rules of the continuous operations is relatively complete for discrete HCSP. The statement of the theorem is as follows.
\begin{theorem}[Relative Completeness of the Discrete Fragment]\label{thm:relativeRC-DF}
The counterpart of the proof system corresponding to the discrete HCSP, i.e., without rules $\textrm{SyncUnpairE3}$, $\textrm{SyncUnpairE4}$, $\textrm{SyncWaitE1-3}$, $\textrm{Cont}$ and $\textrm{Int}$ is relatively complete in Cook's sense. 
%, i.e., suppose 
%all valid formulas of first-order theory of real arithmetic are provable. 
\end{theorem}

\section{Simplification}\label{sec:simplified-rules}
\oomit{
In this section, we first introduce a simplified assertion language, which 
allows us to 
simplify the proof rules and proofs. 
Then, we present 
the differential invariant rules adopted from~\cite{Platzer10}  as alternatives of    \text{Cont} and \text{Int} for easing reasoning ODEs. 

\oomit{restate the strongest postcondition and synchronization rules in a simpler form. They will be used in  the case studies in Sect.~\ref{sec:examples}.}

\subsection{Simplified assertions and proof rules}
First, we define assertions corresponding to the result of input and output statements:
{\small
\[ \vspace*{-1mm}
\begin{array}{rll}
	\textsf{in}(\vec{x}_0,ch,v,\rdy) &\triangleq& \tracev = \langle ch?,v \rangle \vee 
	 \exists d>0.\, \tracev = \langle d,I_{\vec{x}_0},\rdy \rangle \\
	 && ^\chop \langle ch?,v\rangle\vee   \tracev = \langle\infty,I_{\vec{x}_0},\rdy\rangle \\

	\textsf{out}(\vec{x}_0,ch,v,\rdy) &\triangleq& \tracev = \langle ch!,v \rangle \vee 
	 \exists d>0.\, \tracev = \langle d,I_{\vec{x}_0},\rdy \rangle \\
	 &&^\chop \langle ch!,v\rangle\vee \tracev = \langle\infty,I_{\vec{x}_0},\rdy\rangle \\
	
	\textsf{IO}(ch,v) &\triangleq& \tracev = \langle ch,v\rangle
\end{array}
\]
}
We abbreviate \sm{\textsf{out}(\vec{x}_0,ch,v,\{ch!\})} as \sm{\textsf{out}(\vec{x}_0,ch,v)} and abbreviate \sm{\textsf{in}(\vec{x}_0,ch,v,\{ch?\})} as \sm{\textsf{in}(\vec{x}_0,ch,v)}.

We further define the result of ODE statements:
\[\vspace*{-1mm} 
\begin{array}{rcl}
    \textsf{traj}(d,\vec{p}_{\vec{x}_0},\rdy) &\triangleq& \tracev = \langle d,\vec{p}_{\vec{x}_0},\rdy\rangle \\

%    \textsf{trajIn}(\vec{p},ch,v,\rdy) &\triangleq& \textit{tr} = \langle ch?,v\rangle \vee 
%    \exists d>0.\, \textit{tr} = \langle d,\vec{p},\rdy\rangle ^\chop \langle ch?,v\rangle \\
    
%    \textsf{trajOut}(\vec{p},ch,e,\rdy) &\triangleq& \textit{tr} = \langle ch!,e(\vec{p}(0))\rangle \vee
%    \exists d>0.\, \textit{tr} = \langle d,\vec{p},\rdy\rangle ^\chop \langle ch!,e(\vec{p}(d))\rangle
\end{array}
\]
%Here $\vec{p}_{\vec{x}_0}$ is a state trajectory with initial value $\vec{x}_0$, we omit it without confusion. \oomit{Note the asymmetry between \sm{\textsf{trajIn}} and \sm{\textsf{trajOut}} (input value is independent of $d$, while output value depends on $d$).}
We abbreviate \sm{\textsf{traj}(d,\vec{p})} for \sm{\textsf{traj}(d,\vec{p},\emptyset)}.

We further define empty and join operations on assertions. The join operator serves a similar function in our theory as the chop operator in DC ~\cite{ZHR91}. On the surface, it may also look similar to separating conjunction in separation logic~\cite{Reynolds02}. However, note that the join operator concerns combination in the temporal rather than the spatial dimension, and the operator is also not commutative.
{\small \begin{eqnarray*} 
 \textsf{emp} & \triangleq & \tracev = \epsilon \\
  P \joinop Q &\triangleq& \exists tr_1,\,tr_2.\, P[tr_1/\tracev]\wedge Q[tr_2/\tracev]\wedge {tr_1}^\chop tr_2 = \tracev
  \end{eqnarray*} }

%\[ Q \wandop P \triangleq \forall \textit{tr}'.\, Q[\textit{tr}'/tr] \rightarrow P[\textit{tr}^\chop \textit{tr}'/tr] \]

% {\small 
% \[ \begin{array}{c}
%  \prftree{
% \spec{\forall v.\, \wand{\textsf{in}(\vec{x}_0,ch,v)}{Q[v/y]}}{ch?y}{Q}} \\[3mm]
% \prftree{
% \spec{\wand{\textsf{out}(\vec{x}_0,ch,e)}{Q}}{ch!e}{Q}}
% \oomit{\\[3mm]
%  \prftree{
%  \begin{array}{ll}
% \{\forall d>0.\, \forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}] \wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \to \\
% \qquad \wand{\textsf{state\_Traj}(d,\vec{p}_{\vec{x}_0})}{Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}]}\}
% \evo{x}{e}{B}\{Q\} \end{array}} }
% \end{array} 
% \]}

\vspace{-4mm}
Using these assertions, we can restate the strongest postcondition rules in a simpler form, e.g. for input, output, ODE statements:
{\small 
\[ \vspace*{-1mm} \begin{array}{c} 
% \prftree[r]{In}{\spec{P}
% 	{ch?y}{\exists v.\, y =  v \wedge
% 		(\join{P[v/y]}{\textsf{in}(\vec{x}_0[v/y],ch,v)})}} 
\prftree[r]{In}{\spec{\vec{x} = \vec{x}_0 \wedge P}
    {ch?y}{\exists v.\, \vec{x}=\vec{x}_0[v/y]\wedge P\joinop \textsf{in}(\vec{x}_0,ch,v)}}
\\[1mm]
\prftree[r]{Out}{\spec{\vec{x} = \vec{x}_0 \wedge P}
	{ch!e}{P \joinop \textsf{out}(\vec{x}_0,ch,e}} \\[1mm] 
\prftree[r]{Cont}{\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]}
{\neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}]}
{\spec{\vec{x} = \vec{x}_0 \wedge P}
	{\evo{x}{e}{B}}{\vec{x} = \vec{p}_{\vec{x}_0}(d) \wedge P \joinop \textsf{traj}(d,\vec{p}_{\vec{x}_0})}}
\end{array}
\]}

Next, we consider reasoning about synchronization using the simplified assertions. The synchronization of two assertions is defined as follows, directly lifting the corresponding definition on traces.
{\small \vspace*{-1mm} \[P\,\|_{cs}\,Q \triangleq \exists tr_1,\, tr_2.\, P[tr_1/\tracev] \wedge Q[tr_2/\tracev] \wedge tr_1\,\|_{cs}\,tr_2 \Downarrow \tracev \]  \vspace*{-1mm} }
The synchronization rules on simplified assertions describe how they can be combined. They are consistent with the rules presented in Fig.~\ref{fig:elim-synchronization}, e.g. for the synchronization case, we have 
{\small \[\vspace*{-1mm} \prftree{ch\in cs}{
	\begin{array}{l}
		(\textsf{in}(\vec{x}_1,ch,v) \joinop P) \|_{cs} (\textsf{out}(\vec{x}_2,ch,w) \joinop Q) \rightarrow \\
		\qquad v = w \wedge \textsf{IO}(ch,v) \joinop (P \|_{cs} Q)
	\end{array}} \] }
	The full list  of such rules is given in Fig.~\ref{fig:elimination_rules} in Appendix~\ref{app:simplify}.  


With the definition of synchronization on assertions, we can simplify the statement of the parallel rule as follows:
{\small \[ \vspace*{-1mm} \prftree
{\spec{P_1}{c_1}{Q_1}}
{\spec{P_2}{c_2}{Q_2}}
{\spec{P_1[\epsilon/\tracev]\wedge P_2[\epsilon/\tracev]}{c_1\,\|_{cs}\,c_2}
	{Q_1\,\|_{cs}\,Q_2}}
\]}

\vspace{-3mm}
Synchronization of assertions also satisfies  the following compositionality property, which will be used in  following proofs:
{\small \[\vspace*{-1mm} \prftree{P \rightarrow P'}{Q \rightarrow Q'}
{P\,\|_{cs}\,Q \rightarrow P'\,\|_{cs}\,Q'}
\] }

\vspace{-3mm}
\subsection{Differential invariant rules}
So far, we have only considered rules that involve explicit solutions of ODEs. Next we define a set of differential invariant rules adopted from~\cite{Platzer10}. %whose soundness has been proved in Isabelle. 
%Before presenting the rules, 
To the end,
we introduce the following notation first. 
%define an assertion for state trajectory satisfying an invariant condition:

\vspace{-3mm}
{\small \[ 
\textsf{trInv}(d,\textit{Inv},\rdy) \triangleq \exists\vec{p}.\, \tracev = \langle d,\vec{p},\rdy\rangle \wedge \forall \tau\in[0,d].\, \textit{Inv}[\vec{p}(\tau)/\vec{x}]
\] }

\vspace{-3mm} \noindent Here \textit{Inv} is a Boolean formula on states, and the assertion states that \textit{Inv} is satisfied along the entire trajectory.

\oomit{
The following differential invariant rule  says that whenever the Lie derivative of an expression \textit{inv} w.r.t. the ODE within domain $B$ is zero, then \textit{inv} keeps invariant. 
%is maintained as an invariant.
\[ \vspace*{-1mm}
\prftree[r]{DiffInv}{B \rightarrow \mathcal{L}_{\vec{e}}(\textit{inv})=0}
{\spec{\textit{inv}=c\wedge P}{\evo{x}{e}{B}}{\textit{inv}=c\wedge P\joinop \textsf{trInv}(d,\textit{inv}=c)}}
\]
here $\mathcal{L}_{\vec{e}}(\textit{inv})$ denotes the Lie derivative of \textit{inv} w.r.t. the vector field $\vec{e}$. 
  %ODE $\vec{\dot{x}}=\vec{e}$. 
%Note this rule is not in the strongest postcondition form. Instead it gives an \emph{overapproximation} to the postcondition. However, 
This rule could be useful when the differential equation cannot be solved exactly, and all we need is to prove some invariant property.
Moreover, we prove both the positive and negative cases for the above invariant rules, by changing the premise to be $\mathcal{L}_{\vec{e}}(\textit{inv}) \geq 0$, and  the conclusion to be $inv \geq c$ or $inv > c$; symmetrically, the premise $\mathcal{L}_{\vec{e}}(\textit{inv}) \leq 0$, with the conclusion  $\textit{inv} \leq c$ or $\textit{inv} < c$.

Likewise, we can prove a version of Darboux equality rule~\cite{PlatzerT20}:
\[ \vspace*{-1mm} 
\prftree[r]{Dbx}{B \rightarrow \mathcal{L}_{\vec{e}}(\textit{inv})=g\cdot \textit{inv}}{\spec{\textit{inv}=0\wedge P}{\evo{x}{e}{B}}{\textit{inv}=0\wedge P\joinop \textsf{trInv}(d,\textit{inv}=0)}}
\]
where $g$ is a continuous function. The intuition is that, when the first Lie derivative of $\textit{inv}$ is a product between a continuous cofactor $g$ and $\textit{inv}$, then its all higher Lie derivatives can also be written as a product between some cofactor and itself. Thus, when $\textit{inv}$ is 0 initially, all its derivatives are 0, $\textit{inv} = 0$ will stay invariant along the evolution. Similarly, we have also proved the positive and negative cases for Darboux inequalities and here will not list them. 

We can also prove the invariant property of ODE with the idea of barrier certificate \cite{DBLP:conf/hybrid/PrajnaJ04}:
{\small \[ \hspace*{-1mm} 
\prftree[r]{Dbarrier}{B \rightarrow \textit{inv}=0 \rightarrow \mathcal{L}_{\vec{e}}(\textit{inv})<0}
{\spec{\textit{inv}\le 0\wedge P}{\evo{x}{e}{B}}{\textit{inv}\le 0\wedge P\joinop \textsf{trInv}(d,\textit{inv}\le 0)}}
\] }
Whenever $\textit{inv}$ reaches $0$ along the trajectory, the negative 
Lie derivatives push $\textit{inv}$ to decrease, thus it can never exceed $0$ within the range of $B$. The case with $\textit{inv} \ge 0 $ is also proved in a similar way.
%We leave the full development of invariant rules for differential equations to future work.

The above differential rules define the sufficient conditions for being an invariant of an ODE, but not necessary.}

The work~\cite{PlatzerT20} proposes the complete version of differential invariant rules with the help of the characterization of higher Lie derivatives. In order to guarantee completeness, \cite{PlatzerT20} proposes the extended term conditions:  $C^{\infty}$ smoothness, existence of  syntactic partial derivative representation, and the most critical one, % proved in \cite{LiuEmsoft}, 
which is, for any term $p$ and an ODE $\dot{x} = e$, there must exist $N>0$ and $N$ terms $g_i$ satisfying this condition such that
the higher Lie derivatives of $p$ along the ODE satisfy $\mathcal{L}^{N}_{\vec{e}}(\textit{p})= \sum_{i=0}^{i=N-1} g_i \cdot \mathcal{L}^{i}_{\vec{e}}(\textit{p})$. As a result of the last condition, all the Lie derivatives  higher than $N$
 can also be written as sums over the Lie derivatives lower than $N$ with appropriate cofactors.  Polynomial terms obviously satisfy the extended term conditions. We adapt the complete invariant rule of \cite{PlatzerT20} to our case and present it below. 
 
 {\small  \vspace*{-2mm}
 \[ \vspace*{-1mm} 
\prftree[r]{}{B \wedge \textit{INV} \wedge \mathcal{L}^{*}_{\vec{e}}(B) \rightarrow \mathcal{L}^{*}_{\vec{e}}(\textit{INV}) \quad B \wedge \neg \textit{INV} \wedge \mathcal{L}^{*}_{-\vec{e}}(B) \rightarrow \mathcal{L}^{*}_{-\vec{e}}(\neg \textit{INV})}{\spec{\textit{INV}\wedge P}{\evo{x}{e}{B}}{\closeb(B) \wedge \closeb(\neg B) \wedge \textit{INV}\wedge P\joinop \textsf{trInv}(d,\textit{INV})}} \]
}

\vspace{-2mm} \noindent where $B$ and $\textit{INV}$ are both semi-analytic formulas, which have the following normal form:
{\small  \[ B, \textit{INV} \triangleq  \bigvee_i (\bigwedge_{j}p_{ij} \geq 0 \wedge \bigwedge_{j}q_{ij} > 0 )\] }

\vspace{-2mm} \noindent with $p_{ij}, q_{ij}$ the extended terms. The Lie derivatives along $\vec{\dot{x}} = \vec{e}$ are:
{\small \[
\begin{array}{ll}
\mathcal{L}^{*}_{\vec{e}}(B), \mathcal{L}^{*}_{\vec{e}}(INV) \triangleq  \bigvee_i (\bigwedge_{j}\mathcal{L}^{*}_{\vec{e}}(p_{ij}) \geq 0 \wedge \bigwedge_{j}\mathcal{L}^{*}_{\vec{e}}(q_{ij}) > 0 )\\
  \mathcal{L}^{*}_{\vec{e}}(q) > 0 \triangleq (q \geq 0) \wedge (q = 0 \rightarrow \mathcal{L}^{}_{\vec{e}}(q) \geq 0) \wedge (q = 0 \wedge \mathcal{L}^{}_{\vec{e}}(q) = 0\\
  \ \rightarrow \mathcal{L}^{2}_{\vec{e}}(q) \geq 0) \wedge \cdots   (q = 0 \wedge \cdots \wedge \mathcal{L}^{N-2}_{\vec{e}}(q) = 0 \rightarrow \mathcal{L}^{N-1}_{\vec{e}}(q) > 0)\\
  \mathcal{L}^{*}_{\vec{e}}(q) \geq  0 \triangleq (\mathcal{L}^{*}_{\vec{e}}(q) > 0) \vee (\bigwedge_{i=0}^{N-1}\mathcal{L}^{i}_{\vec{e}}(q)=0)\\
 \end{array}
\] }
%The left rule defines the complete version of Darboux equality rule. The complete condition is,  $inv$ satisfies the extended term condition, and moreover, all the Lie derivatives less than $N$ are 0 within domain $B$.  This condition holds iff $inv =0$ is the invariant of the ODE. 

\vspace{-2mm} \noindent Known from \cite{PlatzerT20}, the rule is complete for semi-analytic invariant,  in the sense that $\textit{INV}$  is a semi-analytic invariant for the ODE, iff the arithmetic premise holds. The premise states that, for  the positive case  of $\textit{INV}$ (i.e. $q_{ij}$), there must exist some $i$ such that $i < N$ and the $i$-th Lie derivative   is greater than 0 and the lower Lie derivatives than $i$ are 0, and for the non-negative case (i.e. $p_{ij}$), it is weaker that all the Lie derivatives less than $N$ can be 0. Moreover,  the Lie derivatives of $\neg \textit{INV}$ with respect to the backward ODE $\vec{\dot{x}} = -\vec{e}$ have the similar constraint.  At termination, the escaping point must belong to the boundary of domain $B$ (defined by the conjunction of the closures of $B$ and $\neg B$) and also satisfies $\textit{INV}$. We have the  proof in Appendix~\ref{app:simplify} that $\textit{INV}$ is a complete invariant rule for HHL.
 
 For continuous interrupt, the ODE part with explicit solutions can be replaced by differential invariants similarly. %We will not list them here. 
 \oomit{
\begin{theorem} 
\label{theorem:completeODE}
For semi-analytic formulas $INV$ and $Q$, $\spec{\textit{INV}\wedge P}{\evo{x}{e}{B}}{Q \wedge P\joinop \textsf{trInv}(d,\textit{INV})}$ can be derived from the proof system of HHL, iff the premise condition on $INV$ holds, and $\closeb(B) \wedge \closeb(\neg B) \wedge \textit{INV} \rightarrow Q$ holds. 
\end{theorem}}

%{\small
%\begin{figure*}[ht!] 
%\[
%\prftree[r]{}{B \rightarrow \bigwedge_{i=0}^{N-1}\mathcal{L}^{i}_{\overline{e}}(\textit{inv})=0 \quad \mathcal{L}^{N}_{\overline{e}}(\textit{inv})= \sum_{i=0}^{i=N-1} g_i \cdot \mathcal{L}^{i}_{\overline{e}}(\textit{inv})}{\spec{\textit{inv}=0\wedge P}{\evo{x}{e}{B}}{\closeb(B) \wedge \closeb(\neg B) \wedge \textit{inv}=0\wedge P\joinop \textsf{trInv}(d,\textit{inv}=0)}}
%\quad 
%    \prftree[r]{}{B \wedge INV \wedge \mathcal{L}^{*}_{\overline{e}}(B) \rightarrow \mathcal{L}^{*}_{\overline{e}}(INV) \quad B \wedge \neg INV \wedge \mathcal{L}^{*}_{-\overline{e}}(B) \rightarrow \mathcal{L}^{*}_{-\overline{e}}(\neg INV)}{\spec{\textit{INV}\wedge P}{\evo{x}{e}{B}}{\closeb(B) \wedge \closeb(\neg B) \wedge \textit{INV}\wedge P\joinop \textsf{trInv}(d,\textit{INV})}} 
%\]
%\[
%\begin{array}{ll}\mbox{Definitions: } \\
%INV \triangleq  \bigvee_i (\bigwedge_{j}p_{ij} \geq 0 \wedge \bigwedge_{j}q_{ij} > 0 )
%\qquad \mathcal{L}^{*}_{\overline{e}}(INV) \triangleq  \bigvee_i (\bigwedge_{j}\mathcal{L}^{*}_{\overline{e}}(p_{ij}) \geq 0 \wedge \bigwedge_{j}\mathcal{L}^{*}_{\overline{e}}(q_{ij}) > 0 )\\
  %\mathcal{L}^{*}_{\overline{e}}(q) > 0 \triangleq (q \geq 0) \wedge (q = 0 \rightarrow \mathcal{L}^{}_{\overline{e}}(q) > 0) \wedge (q = 0 \wedge \mathcal{L}^{}_{\overline{e}}(q) = 0)\rightarrow \mathcal{L}^{2}_{\overline{e}}(q) > 0) \wedge \cdots \wedge (q = 0 \wedge \cdots \wedge \mathcal{L}^{N-2}_{\overline{e}}(q) = 0) \rightarrow \mathcal{L}^{N-1}_{\overline{e}}(q) > 0)\\
 % \mathcal{L}^{*}_{\overline{e}}(q) \geq  0 \triangleq (\mathcal{L}^{*}_{\overline{e}}(q) > 0) \vee (\bigwedge_{i=0}^{N-1}\mathcal{L}^{i}_{\overline{e}}(q)=0)\\
 
%\end{array}
%\]

%\caption{Complete differential rules for ODE}
%\label{fig:differential_rules}
%\end{figure*}
%}

\oomit{
For the example in Section~\ref{sec:examples}, in addition to the invariant version of \sm{\textsf{traj}}, we will also need invariant versions of \sm{\textsf{in}} and \sm{\textsf{out}}, defined as follows:
\[
\begin{array}{l}
\textsf{in}_\textsf{inv}(Inv,ch,v) \triangleq \exists\vec{x_0}.\, \textsf{in}(\vec{x_0},ch,v)\wedge Inv[\vec{x_0}/\vec{x}] \\
\textsf{out}_\textsf{inv}(Inv,ch,v) \triangleq
\exists\vec{x_0}.\, \textsf{out}(\vec{x_0},ch,v)\wedge Inv[\vec{x_0}/\vec{x}]
\end{array}
\]
We will abbreviate \sm{\textsf{in}_\textsf{inv}(ch,v)} for
\sm{\textsf{in}_\textsf{inv}(\textsf{true},ch,v)} and
\sm{\textsf{out}_\textsf{inv}(ch,v)} for
\sm{\textsf{out}_\textsf{inv}(\textsf{true},ch,v)}.
}
}
In this section, we introduce a simplified assertion language, which 
allows us to 
simplify the proof rules and proofs. 
\subsection{Parameterized Assertions}
Based on assertions of state and trajectory, we introduce assertions parameterized by the starting state. We use $(s,tr)\models P$ to denote state $s$ and trace $tr$ satisfying an assertion $P$. Similarly, we use the notation $(s_0,s,tr)\models Q$ for parameterized assertions $Q$. Some simple assertions and common operations on assertions are shown in Appendix~\ref{app:simplify}:
\ommit{
{\small 
\[ 
\begin{array}{c}
(s_0,s,tr)\models \mathsf{true} \longleftrightarrow \mathit{true} \\

(s_0,s,tr)\models \mathsf{false} \longleftrightarrow \mathit{false} \\

(s_0,s,tr)\models \mathsf{init} \longleftrightarrow s_0=s \wedge tr=\epsilon \\

(s_0,s,tr)\models P\wedge Q \longleftrightarrow (s_0,s,tr)\models P \wedge (s_0,s,tr)\models Q \\

(s_0,s,tr)\models P\vee Q \longleftrightarrow (s_0,s,tr)\models P \vee (s_0,s,tr)\models Q \\

(s_0,s,tr)\models \forall n.\, P(n) \longleftrightarrow \forall n.\, (s_0,s,tr)\models P(n) \\

(s_0,s,tr)\models \exists n.\, P(n) \longleftrightarrow \exists n.\, (s_0,s,tr)\models P(n) \\

(s_0,s,tr)\models P[v:=e] \longleftrightarrow (s_0(v:=e),s,tr)\models P \\
\end{array}
\]
}
}

A path assertion is a predicate on time and state which describes the evolution of state over time. A parameterized path assertion is a predicate on starting state, time and state. We use $(s_0,t,s)\models I$ to express that for starting state $s_0$, time $t$ and state $s$ satisfies path assertion $I$. 
For example, $\mathsf{id\_inv}\triangleq(s=s_0)$ describes a path where the state at any time equals the starting state.
The solution to the ODE $\langle \dot{x}=1\rangle$ satisfies $I=(s=s_0[x:=x+t])$. If the ODE cannot be solved explicitly, but we can find a differential invariant for the ODE. In this case, $I=\mathsf{inv}(s)$, where $\mathsf{inv}$ is the differential invariant.


We define assertions corresponding to input, output, wait and interrupt as in Fig.~\ref{fig:simplified assertion}. The most important definition is that of assertion corresponding to interrupt. Assertions corresponding to input, output, and wait are all its special cases. Besides, we have some useful entailment properties for these parameterized assertions shown in Appendix~\ref{app:simplify}

\begin{figure*}
{\vspace*{-4mm}\small 
\begin{eqnarray*}
&  
\prftree[r]{In1}{(s_0,s,tr)\models P(0,v)}
{(s_0,s,\langle ch?,v\rangle^\chop tr)\models \mathsf{wait\_in}(I,ch,P)}  
\qquad
\prftree[r]{In2}{0<d}{(s_0,s,tr)\models P(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\{ch?\}\rangle^\chop\langle ch?,v\rangle^\chop tr)\models \mathsf{wait\_in}(I,ch,P)}
& \\ 
&
\prftree[r]{Out1}{(s_0,s,tr)\models P(0,v)}
{(s_0,s,\langle ch!,v\rangle^\chop tr)\models \mathsf{wait\_out}(I,ch,P)}
\qquad
\prftree[r]{Out1}{0<d}{(s_0,s,tr)\models P(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\{ch!\}\rangle^\chop\langle ch!,v\rangle^\chop tr)\models \mathsf{wait\_out}(I,ch,P)}
& \\
&
\prftree[r]{Wait1}{e(s_0)>0}{(s_0,s,tr)\models P(e(s_0))}{\forall t\in\{0..e(s_0)\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle e(s_0),p,\emptyset\rangle^\chop tr)\models \mathsf{wait}(I,e,P)} 
\qquad
\prftree[r]{Wait2}{e(s_0)\le 0}{(s_0,s,tr)\models P(0)}
{(s_0,s,tr)\models \mathsf{wait}(I,e,P)}
& \\
&
\prftree[r]{Int1}{e(s_0)>0}{(s_0,s,tr)\models P(e(s_0))}{\forall t\in\{0..e(s_0)\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle e(s_0),p,\rdy(\specs)\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)}
\qquad
\prftree[r]{Int2}{e(s_0)\le 0}{(s_0,s,tr)\models P(0)}
{(s_0,s,tr)\models \mathsf{interrupt}(I,e,P,\specs)} 
& \\
&
\prftree[r]{Int3}{\specs[i] = \langle ch?,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)}
\qquad
\prftree[r]{Int4}{\specs[i] = \langle ch?,Q\rangle}{0<d\le e(s_0)}{(s_0,s,tr)\models Q(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)}
& \\
&
\prftree[r]{Int5}{\specs[i] = \langle ch!,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)}
\qquad
\prftree[r]{Int6}{\specs[i] = \langle ch!,Q\rangle}{0<d\le e(s_0)}{(s_0,s,tr)\models Q(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)}
& \\
&
\prftree[r]{Intinf1}{\specs[i] = \langle ch?,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)}
\qquad
\prftree[r]{Intinf2}{\specs[i] = \langle ch?,Q\rangle}{0<d}{(s_0,s,tr)\models Q(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)}
& \\
&
\prftree[r]{Intinf3}{\specs[i] = \langle ch!,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)} 
\qquad
\prftree[r]{Intinf4}{\specs[i] = \langle ch!,Q\rangle}{0<d}{(s_0,s,tr)\models Q(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)}
&
\vspace*{-6mm} 
\end{eqnarray*}} \vspace*{-4mm} 
\caption{Parameterized assertions for input, output, wait and interrupt}
\label{fig:simplified assertion}  \vspace*{-4mm} 
\end{figure*}
Assertion for input has the form $\mathsf{wait\_in}(I,ch,P)$, where $I$ is a parameterized path assertion, $ch$ is a channel name, and $P$ is a parameterized assertion over delay $d$ and input value $v$. 
The two rules correspond to communicating immediately and communicating after waiting for time d > 0, so the delay given to P is 0 or d with the path satisfing the path assertion I.
\ommit{
{\small
\[ \vspace*{-1mm}
\begin{array}{c}
\prftree{(s_0,s,tr)\models P(0,v)}
{(s_0,s,\langle ch?,v\rangle^\chop tr)\models \mathsf{wait\_in}(I,ch,P)} \vspace{2mm} \\

\prftree{0<d}{(s_0,s,tr)\models P(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\{ch?\}\rangle^\chop\langle ch?,v\rangle^\chop tr)\models \mathsf{wait\_in}(I,ch,P)}
\end{array}
\]}
}

Similarly, the assertion for output with the form $\mathsf{wait\_out}(I,ch,P)$ is defined by two cases. Here $P$ is also parameterized assertion over delay $d$ and output value $v$.
\ommit{
{\small
\[ \vspace*{-1mm}
\begin{array}{c}
\prftree{(s_0,s,tr)\models P(0,v)}
{(s_0,s,\langle ch!,v\rangle^\chop tr)\models \mathsf{wait\_out}(I,ch,P)} \vspace{2mm} \\

\prftree{0<d}{(s_0,s,tr)\models P(d,v)}{\forall t\in\{0..d\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle d,p,\{ch!\}\rangle^\chop\langle ch!,v\rangle^\chop tr)\models \mathsf{wait\_out}(I,ch,P)}
\end{array}
\]}
}
Often it is more convenient to describe the output value explicitly using an expression. We define a special case for it as follows. 
{\small 
\[
\mathsf{wait\_outv}(I,ch,e,P) = \mathsf{wait\_out}(I,ch,\{(d,v)\Rightarrow v=e\wedge P(d)\})
\] 
}
In this definition, $P$ is parameterized over delay $d$ only.

The assertion for wait has the form $\mathsf{wait}(I,e,P)$, where $e$ is an expression specifying the delay in terms of starting state. $P$ is parameterized over delay $d$. 
We consider the different cases with positive delay or zero delay.
\ommit{
{\small
\[\vspace*{-1mm}
\begin{array}{c}
\prftree{e(s_0)>0}{(s_0,s,tr)\models P(e(s_0))}{\forall t\in\{0..e(s_0)\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle e(s_0),p,\emptyset\rangle^\chop tr)\models \mathsf{wait}(I,e,P)} \vspace{2mm} \\

\prftree{e(s_0)\le 0}{(s_0,s,tr)\models P(0)}
{(s_0,s,tr)\models \mathsf{wait}(I,e,P)}
\end{array}
\]}
}

The form of the assertion for interrupt is $\mathsf{interrupt}(I,e,P,\specs)$. Here $e$ specifies the \emph{maximum} waiting time of the interrupt, as an expression over the starting state. $P$ describes the remaining behavior if the waiting stops upon reaching the time computed by $e$. Finally, $\specs$ specifies the list of communications that can happen at any time before $e(s_0)$. It is given by a list of elements of the form $\langle ch!,Q\rangle$ or $\langle ch?,Q\rangle$, where $Q$ is an assertion parameterized by the delay and communicated value, that specifies what happens after the corresponding interrupt is triggered. We use $\rdy(\specs)$ to denote the ready state corresponding to $\specs$. 
\ommit{
{\small
\[\vspace*{-1mm}
\begin{array}{c}
\prftree{e(s_0)>0}{(s_0,s,tr)\models P(e(s_0))}{\forall t\in\{0..e(s_0)\}.\,(s_0,t,p(t))\models I}
{(s_0,s,\langle e(s_0),p,\rdy(\specs)\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)} \vspace{2mm} \\

\prftree{e(s_0)\le 0}{(s_0,s,tr)\models P(0)}
{(s_0,s,tr)\models \mathsf{interrupt}(I,e,P,\specs)} \vspace{2mm} \\

\prftree{\specs[i] = \langle ch?,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)} \vspace{2mm} \\

\prftree{
\begin{array}{c}
     \specs[i] = \langle ch?,Q\rangle \quad 0<d\le e(s_0)\quad(s_0,s,tr)\models Q(d,v)  \\
     \forall t\in\{0..d\}.\,(s_0,t,p(t))\models I
\end{array}
}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)}
\vspace{2mm} \\

\prftree{\specs[i] = \langle ch!,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)} \vspace{2mm} \\

\prftree{
\begin{array}{c}
     \specs[i] = \langle ch!,Q\rangle\quad0<d\le e(s_0)\quad(s_0,s,tr)\models Q(d,v) \\
     \forall t\in\{0..d\}.\,(s_0,t,p(t))\models I 
\end{array}
}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt}(I,e,P,\specs)}
\end{array}
\]}
}
Considering the maximum waiting time may be infinite, we define a separate assertion $\mathsf{interrupt_\infty}(I,\specs)$ given by the corresponding four cases.
\ommit{
{\small
\[\vspace*{-1mm}
\begin{array}{c}
\prftree{\specs[i] = \langle ch?,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)} \vspace{2mm} \\

\prftree{
\begin{array}{c}
     \specs[i] = \langle ch?,Q\rangle\quad 0<d\quad (s_0,s,tr)\models Q(d,v)  \\
     \forall t\in\{0..d\}.\,(s_0,t,p(t))\models I
\end{array}
}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch?,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)}
\vspace{2mm} \\

\prftree{\specs[i] = \langle ch!,Q\rangle}{(s_0,s,tr)\models Q(0,v)}
{(s_0,s,\langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)} \vspace{2mm} \\

\prftree{
\begin{array}{c}
     \specs[i] = \langle ch!,Q\rangle\quad0<d\quad(s_0,s,tr)\models Q(d,v)  \\
     \forall t\in\{0..d\}.\,(s_0,t,p(t))\models I 
\end{array}
}
{(s_0,s,\langle d,p,\rdy(\specs)\rangle^\chop \langle ch!,v\rangle^\chop tr)\models \mathsf{interrupt_\infty}(I,\specs)}
\end{array}
\]}
}
\subsection{Rules for Parameterized Assertions}
Often, we characterize an HCSP program $c$ by stating the Hoare triple for every fixed starting state $s_0$, giving the postcondition $Q$ parameterized by $s_0$. This is contained in the definition of $\mathsf{spec\_of}$.
{\small
\[
\mathsf{spec\_of}(c,Q) \triangleq \forall s_0.\,\models \{s=s_0\land tr=\epsilon \}\,c\,\{Q(s_0,s,tr)\}
\]}
For each HCSP command, we state two forms of Hoare rules: one for the command alone, and one for the case where the command is followed by a subsequent program $c$ as shown in 
as in Fig.~\ref{fig:spec of assertion}

Rules for Input can be interpreted as follows: input $ch?x; c$ has the behavior of first waiting for input along channel $ch$, with state equal to the starting state while waiting ($\mathsf{id\_inv}$). After the input is received, first update the state using $x := v$, then follow the behavior of $c$ as specified by $Q$. In Output rules we used the special case $\mathsf{wait\_outv}$ of $\mathsf{wait\_out}$, which can of course be expanded if necessary.

In Cont rules, we summarize the concept of a parameterized solution to the ODE with open boundary in the predicate:
\begin{itemize}
\item $\mathsf{paramODEsol}(ode, b, p, e)$:
$p(s_0,t)$ is a solution of $ode$ and 
    $e$ maps the starting state to the length of time before the unique solution reaches the boundary $b$.
\item $\mathsf{paramODEsolInf}(ode, p)$:
$p(s_0,t)$ is a solution of $ode$. 
\end{itemize}
The predicate $\mathsf{lipschitz}(ode)$ ensures that there is a unique solution to the ordinary differential equation.

The rule for interrupt can be seen as the combination of rules for ODE, input, and output. Given an interrupt command $\langle ode, \mathsf{true}\propto c\rangle\unrhd \talloblong es$, where each $es[i]$ is of the form $(ch?x\rightarrow c_i)$ or $(ch!e\rightarrow c_i)$, and $p$ is the parameterized solution to $ode$, we say the relation $\mathsf{rel\_specs}(es,p,\specs)$ holds between the interrupts $es$ and $\specs$, if for each $es[i]=(ch?x\rightarrow c_i)$, we have $\mathsf{spec\_of}(c_i,Q_i)$, and 
{\small
\[ 
\specs[i]=\langle ch?, \{(d,v)\Rightarrow Q_i[s:=p(s_0,d)(x := v)]\} \rangle, 
\vspace{-1mm}
\]}
and for each $es[i]=(ch!e\rightarrow c_i)$, we have $\mathsf{spec\_of}(c_i,Q_i)$, and
{\small
\[ \specs[i]=\langle ch!, \{(d,v)\Rightarrow v=e(p(s_0,d)) \wedge Q_i[s:=p(s_0,d)] \} \rangle. 
\vspace{-1mm}
\]}
Since $(\langle ode\& b\propto c\rangle\unrhd \talloblong es);c'$ has the same big step semantics with $\langle ode\& b\propto c;c'\rangle\unrhd \talloblong \mathsf{ext}(es,c')$,
we only introduce one rule for bounded and unbounded interrupt separately.
The function $\mathsf{ext}(es,c)$ expands the execution command for each element in the communication list which is defined as:
{\small
\[
\mathsf{ext}(es,c)[i]=
\begin{cases}
	(ch?x\rightarrow c_i;c), & \text{if } es[i]=
    (ch?x\rightarrow c_i)\\
	(ch!e\rightarrow c_i;c), & \text{if } es[i]=(ch!e\rightarrow c_i) 
\end{cases}
\vspace{-2mm}
\]}
\subsection{Rules for parallel programs}
When dealing with parallel programs, we will give each single process a name, the state, trace and assertions associated with the process will inherit this name. A global state of parallel programs can be viewed as a  partial function from process names to single states. For writing convenience, we have omitted this part of the representation in the article.

Given two parameterized assertions $P,Q$ for parallel programs, the assertion $\mathsf{sync}(chs,P,Q)$ is a parameterized assertion denoting the synchronization:
\[\small
\prftree{
(s_{11},s_{21},tr_1)\models P\quad
(s_{12},s_{22},tr_2)\models Q\quad
tr_1\|_{chs}tr_2 \Downarrow tr
}
{(s_{11}\uplus s_{12}, s_{21}\uplus s_{22}, tr)\models \mathsf{sync}(chs,P,Q)}
\]

The main rule for reasoning about parallel programs derives $\mathsf{spec\_of}$ for parallel composition from $\mathsf{spec\_of}$ for its components:
\[\small
\prftree{\mathsf{spec\_of}(p_1,P_1)}{\mathsf{spec\_of}(p_2,P_2)}
{\mathsf{spec\_of}(p_1\|_{chs}p_2, \mathsf{sync}(chs,P_1,P_2))}
\]
Hence, reasoning about a parallel program consists of the following steps:
\begin{enumerate}
\item Reason about each sequential component using rules for sequential programs, resulting in $\mathsf{spec\_of}$ for each component.
\item Use the above rule to obtain an initial version of $\mathsf{spec\_of}$ for the parallel program. This version consists of $\mathsf{sync}$ assertions.
\item Use synchronization rules in Appendix~\ref{app:simplify} to eliminate each $\mathsf{sync}$ assertion.
\end{enumerate}



\begin{figure*}
{\vspace*{-4mm}\small 
\begin{eqnarray*}
&  
\prftree[r]{SkipSp1}{\mathsf{spec\_of}(\pskip, \mathsf{init})}
\qquad
\prftree[r]{SkipSp2}{\mathsf{spec\_of}(c,Q)}
{\mathsf{spec\_of}(\pskip; c, Q)}
\qquad
\prftree[r]{AssignSp1}{\mathsf{spec\_of}(v := e, \mathsf{init}[v := e])}
\qquad
\prftree[r]{AssignSp2}{\mathsf{spec\_of}(c,Q)}
{\mathsf{spec\_of}(v := e; c, Q[v := e])}
& \\
&
\prftree[r]{InSp1}{\mathsf{spec\_of}(ch?x, \mathsf{wait\_in}(\mathsf{id\_inv}, ch, \{(d,v)\Rightarrow \mathsf{init}[x := v]\}))}
\qquad
\prftree[r]{InSp2}{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(ch?x; c, \mathsf{wait\_in}(\mathsf{id\_inv}, ch, \{(d,v)\Rightarrow Q[x := v]\}))}
& \\
&
\prftree[r]{OutSp1}{\mathsf{spec\_of}(ch!e, \mathsf{wait\_outv}(\mathsf{id\_inv}, ch, e, \{d\Rightarrow \mathsf{init}\}))} \qquad
\prftree[r]{OutSp2}{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(ch!e; c, \mathsf{wait\_outv}(\mathsf{id\_inv}, ch, e, \{d\Rightarrow Q\}))}
& \\
&
\prftree[r]{WaitSp1}{\mathsf{spec\_of}(\mathsf{Wait}(e), \mathsf{wait}(\mathsf{id\_inv}, e, \{d\Rightarrow \mathsf{init}\}))} \qquad
\prftree[r]{WaitSp2}{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(\mathsf{Wait}(e); c, \mathsf{wait}(\mathsf{id\_inv}, e, \{d\Rightarrow Q\}))}
& \\
&
\prftree[r]{ContSp1}{\mathsf{paramODEsol}(ode, b, p, e)}{\mathsf{lipschitz}(ode)}
{\mathsf{spec\_of}(\langle ode\& b\rangle, \mathsf{wait}(
s=p(s_0,t), e, \{d \Rightarrow \mathsf{init}[s:=p(s_0,d)] \}))} 
\quad
\prftree[r]{ContSp2}{\mathsf{paramODEsol}(ode, b, p, e)}{\mathsf{lipschitz}(ode)}{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(\langle ode\& b\rangle; c, \mathsf{wait}(
s=p(s_0,t), e, \{d \Rightarrow Q[s:=p(s_0,d)] \}))}
& \\
&
\prftree[r]{IntSp1}{\mathsf{paramODEsol}(ode,b,p,e)}{\mathsf{lipschitz}(ode)}{\mathsf{rel\_specs}(es,p,\specs)}
{\mathsf{spec\_of}(c,P)}
{\mathsf{spec\_of}(\langle ode\& b\propto c\rangle\unrhd \talloblong es ,
\mathsf{interrupt}(s=p(s_0,t),e,\{d\Rightarrow P[s:=p(s_0,d)]\},\specs)}
& \\
&
\prftree[r]{IntinfSp1}{\mathsf{paramODEsolInf}(ode,p)}{\mathsf{lipschitz}(ode)}{\mathsf{rel\_specs}(es,p,\specs)}
{\mathsf{spec\_of}(\langle ode\& \mathsf{true}\propto c\rangle\unrhd \talloblong es, \mathsf{interrupt_\infty}(s=p(s_0,t), \specs)}
& 
\vspace*{-6mm} 
\end{eqnarray*}} \vspace*{-4mm} 
\caption{Specification Rules with parameterized assertions }
\label{fig:spec of assertion}  \vspace*{-4mm} 
\end{figure*}

\ommit{
\[
\begin{array}{c}
\prftree{\mathsf{spec\_of}(v := e, \mathsf{init}[v := e])} \vspace{2mm} \\
\prftree{\mathsf{spec\_of}(c,Q)}
{\mathsf{spec\_of}(v := e; c, Q[v := e])}
\end{array}
\]
}

\ommit{
\[
\begin{array}{c}
\prftree{\mathsf{spec\_of}(ch?x, \mathsf{wait\_in}(\mathsf{id\_inv}, ch, \{(d,v)\Rightarrow \mathsf{init}[x := v]\}))} \vspace{2mm} \\

\prftree{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(ch?x; c, \mathsf{wait\_in}(\mathsf{id\_inv}, ch, \{(d,v)\Rightarrow Q[x := v]\}))}
\end{array}
\]
}

\ommit{
\[
\begin{array}{c}
\prftree{\mathsf{spec\_of}(ch!e, \mathsf{wait\_outv}(\mathsf{id\_inv}, ch, e, \{d\Rightarrow \mathsf{init}\}))} \vspace{2mm} \\
\prftree{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(ch!e; c, \mathsf{wait\_outv}(\mathsf{id\_inv}, ch, e, \{d\Rightarrow Q\}))}
\end{array}
\]
}

\ommit{
\[
\begin{array}{c}
\prftree{\mathsf{spec\_of}(\mathsf{Wait}(e), \mathsf{wait}(\mathsf{id\_inv}, e, \{d\Rightarrow \mathsf{init}\}))} \vspace{2mm} \\
\prftree{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(\mathsf{Wait}(e); c, \mathsf{wait}(\mathsf{id\_inv}, e, \{d\Rightarrow Q\}))}
\end{array}
\]
}

\oomit{
\[
\begin{array}{c}
\prftree{\mathsf{paramODEsol}(ode, b, p, e)}{\mathsf{lipschitz}(ode)}
{\mathsf{spec\_of}(\langle ode, b\rangle, \mathsf{wait}(
s=p(s_0,t), e, \{d \Rightarrow \mathsf{init}[s:=p(s_0,d)] \}))} \vspace{2mm} \\
\prftree{\mathsf{paramODEsol}(ode, b, p, e)}{\mathsf{lipschitz}(ode)}{\mathsf{spec\_of}(c, Q)}
{\mathsf{spec\_of}(\langle ode, b\rangle; c, \mathsf{wait}(
s=p(s_0,t), e, \{d \Rightarrow Q[s:=p(s_0,d)] \}))}
\end{array}
\]
}




\section{Implementation and Case Studies}\label{sec:examples}

\paragraph{Implementation} The proof system with the above simplification is implemented in Isabelle/HOL. It depends upon the formalization of ODEs in Isabelle/HOL~\cite{ImmlerH12}. The following example is verified using the implementation.

\paragraph{Case study}
We demonstrate how our proof system works in practice by showing a simplified case study of an automatic cruise control system~\cite{DBLP:journals/tcs/XuWZJTZ22}.
In this example we consider an abstract model of the
CCS with two main components: a controller (Control) and a physical plant (Plant), 
The process Plant models the motion of the vehicle. It repeatedly evolve
according to an ODE which is interrupted by sending velocity and position, then receiving the
updated acceleration data. 
\vspace{-1mm}
{\small 
\[
\textit{Plant} \triangleq (\langle\dot{x}=v, \dot{v}=a \&\mathsf{true}\propto \pskip\rangle \unrhd \talloblong[{\textit{chp}!v}\rightarrow {\textit{chp}!x; \textit{chc}?a}])^* 
\] }
The process Control compute and send a proper vehicle acceleration based
on received velocity and position for each fixed period $T$.
\vspace{1mm} 
{\small
\[
\textit{Control} \triangleq (\pwait \ T; \textit{chp}?v; \textit{chp}?x; a:=f(x,v);\textit{chc}!a)^*
\] }
\vspace{-2mm}

The computation of control is based on the concept of Maximum Protection Curve (MPC). An upper limit of velocity $v_\mathit{lim}$ is computed according to current position as follows:
\[
v_\mathit{lim}(x)=
\begin{cases}
	v_\mathit{max}, & \text{if } x_\mathit{obs}-x\ge \frac{v_\mathit{max}^2}{-2a_\mathit{min}}\\
	\sqrt{-2a_\mathit{min}\cdot(x_\mathit{obs}-x)}, & \text{if } 0< x_\mathit{obs}-x<\frac{v_\mathit{max}^2}{-2a_\mathit{min}} \\
	0, & \text{otherwise}
\end{cases}
\]
where $v_\mathit{max}$, $a_\mathit{min}$ and $x_\mathit{obs}$ are constants respectively representing the max speed of vehicle, the braking deceleration of vehicle and the position of obstacle.
Function $f$ is defined based on $v_\mathit{lim}$:
\[\small
f(x,v)=
\begin{cases}
a_\mathit{p}& \text{if } v+a_\mathit{p}\cdot T\le v_\mathit{lim}(x+v\cdot T+\frac{1}{2}\cdot a_\mathit{p}\cdot T^2)\\
0& \text{else if } v\le v_\mathit{lim}(x+v\cdot T)\\
a_\mathit{min}& \text{otherwise}
\end{cases}
\]
where $a_\mathit{p}$ is a constant representing the acceleration and we have $a_\mathit{p}>0$,  $a_\mathit{min}<0$, $v_\mathit{max}>0$ and $T>0$ in our settings. In this example, we want to verify the safety requirement that the vehicle
will never reach the obstacle and the velocity will never exceed the limit speed i.e. $\mathsf{Safe}(s):=x\leq x_\mathit{obs}\wedge v\leq v_\mathit{lim}(x)$.

Based on simplified assertions and corresponding lemmas, we can prove the following two facts:
\[\small
\begin{array}{cc}
  \mathsf{spec\_of}(Plant,\exists n.\, \mathsf{plant\_inv}(n))  \\
   \mathsf{spec\_of}(Control,\exists n.\, \mathsf{control\_inv}(n))  
\end{array}
\]
where $\mathsf{plant\_inv}$ and $\mathsf{control\_inv}$ are defined as follows:
\[\small
\begin{array}{rl}
\mathsf{plant\_inv}(0) =& \mathsf{init} \\
\mathsf{plant\_inv}(n+1) =& \mathsf{wait\_out}(\hat{I},chp,\{(d,\mu)\Rightarrow \mu=v+a\cdot d\wedge\\
   & \quad \mathsf{wait\_outv}(\mathsf{id\_inv},chp,x,\{d_1\Rightarrow\\
   & \quad \quad \mathsf{wait\_in}(\mathsf{id\_inv},chc,\{(d_2,\mu_2)\Rightarrow \\
   & \quad \quad \quad\mathsf{plant\_inv}(n)[a:=\mu_2]\})\})\\
   & \quad\quad [v:=v+a\cdot d,x:=x+v\cdot d+\frac{1}{2}\cdot a\cdot d^2]\})
\end{array}
\]
where $\hat{I}=(s=s0[v:=v+a\cdot t,x:=x+v\cdot t+\frac{1}{2}\cdot a\cdot t^2])$
\[\small
\begin{array}{rl}
\mathsf{control\_inv}(0) =& \mathsf{init} \\
\mathsf{control\_inv}(n+1) =& \mathsf{wait}(\mathsf{id\_inv},T,\{d\Rightarrow \\
&\quad \mathsf{wait\_in}(\mathsf{id\_inv},chp,\{(d_1,\mu_1)\Rightarrow\\
&\quad\quad \mathsf{wait\_in}(\mathsf{id\_inv},chp,\{(d_2,\mu_2)\Rightarrow\\
&\quad\quad\quad \mathsf{wait\_outv}(\mathsf{id\_inv},chc,a,\{d3\Rightarrow \\
&\quad\quad\quad\quad\mathsf{control\_inv}(n)\})\\
&\quad\quad\quad
[a:=f(x,v)][x:=\mu_2]\})\})\\
&\quad\quad[v:=\mu_1]\})
\end{array}
\]

According to the rule for reasoning about parallel programs:
\[
\begin{array}{cc}
     \mathsf{spec\_of}(Plant\|_{\{chp,chc\}}Control,  \\
     \exists n1\ n2. \mathsf{sync}(\{chp,chc\},\mathsf{plant\_inv}(n1),\mathsf{control\_inv}(n2)))
\end{array}
 \]
Moreover, on the use of synchronization of simplified assertions, we can prove that 
\[\small
\begin{array}{cc}
\mathsf{Safe}(s1) \land s_1(a)=f(s_1(x),s_1(v)) \land \\
\mathsf{sync}(\{chp,chc\},\mathsf{plant\_inv} (n1),\mathsf{control\_inv}(n2))(s_1\uplus s_2,s'_1\uplus s'_2,tr) \\
\longrightarrow \mathsf{Safe}(s'_1) \land s'_1(a)=f(s'_1(x),s'_1(v))
\end{array}
\]
Finally, we have the following specification indicating that the safety requirement holds.
\[\small
\begin{array}{cc}
     \mathsf{spec\_of\_gen}(\mathsf{Inv},Plant\|_{\{chp,chc\}}Control,  \\
     \{(s0,s,tr)\Rightarrow \mathsf{Inv}(s)\})
\end{array}
 \]
 where 
 \[
 \mathsf{Inv}(s_1\uplus s_2):=\mathsf{Safe}(s_1) \land s_1(a)=f(s_1(x),s_1(v))\] and  
{\small
\[
\mathsf{spec\_of\_gen}(P,c,Q)\triangleq \forall s_0.\,\ P (s0)\longrightarrow\models \{s=s_0\land tr=\epsilon \}\,c\,\{Q(s_0,s,tr)\}
\]
}

\section{Conclusion} \label{sec:Conclusion}

In this paper, we present a hybrid Hoare logic for reasoning about HCSP processes, which generalizes and simplifies the existing DC-based hybrid Hoare logics,  and prove its soundness, and continuous and discrete relative completeness. We then further discuss how to simply the logic.  
%the logic by a simplified language of assertions and proof rules. Furthermore, we formulate the differential invariant rules for reasoning about ODE without mentioning explicit solutions, and prove the soundness and completeness. 
Finally, we provide an implementation and 
a case study to illustrate the power and scalability of our logic. 
%one example on lunar lander of practical usage of the logic, showing reasoning using invariants and compositional reasoning. 

%Currently, for reasoning about differential equations, the main part of the logic contains only the rule using explicit solutions. While this rule is the strongest possible in theory, it cannot be directly applied if the differential equation cannot be explicitly solved. We then showed how to augment the proof system by additional rules for reasoning about differential equations. However, a complete study of such rules, including their completeness properties, are left for future work. This is studied in the context of differential dynamic logic in~\cite{Platzer10,Platzer12-cut,PlatzerT20}.
For future work, we will consider to specify and verify more properties including livelock and total correctness in HHL. 
%Alternatively, it also deserves to investigate how to incorporate proof rules for discretization of continuous evolution from \cite{TOSEM20} into our logic.
%Besides, a proof assistant for an earlier version of the proof system has been implemented in Isabelle/HOL, and a theorem prover for the current version is still under development. 
%Also, it is a very important future work to give a rigid proof for Theorem~\ref{thm:relativeRC-DF}.

\bibliographystyle{ACM-Reference-Format}
\bibliography{arxiv}

\newpage 
%\begin{appendices}
\appendix
\section{Full versions of operational semantics}\label{app:full-semantics} 

In this appendix, we provide the full definitions of small-step and big-step operational semantics for HCSP, and give detailed proof about their equivalence. 

\subsection{Operational Semantics of HCSP}

Fig.~\ref{fig:full-small-step} and Fig.~\ref{fig:full-big-semantics} list the small-step and big-step operational semantics for HCSP respectively. 

\begin{figure*}
		{\small
		\begin{eqnarray*} &\prftree[r]{AssignS}{(x:=e,s) \xrightarrow{\tau} (\pskip,s[x \mapsto e])}
		\quad
		\prftree[r]{SeqS1}{(c_1,s) \xrightarrow{e} (c_1',s')}{(c_1;c_2, s) \xrightarrow{e} (c_1';c_2,s')}
		\quad
		\prftree[r]{SeqS2}{(\pskip;c, s) \xrightarrow{\tau} (c,s)} 
		\quad
		\prftree[r]{CondS1}{s(b))}{(\IFE{b}{c_1}{c_2}, s) \xrightarrow{\tau} (c,s)}& \\[1mm]
 	& 
		\prftree[r]{CondS2}{\neg s(b)}{(\IFE{b}{c_1}{c_2}, s) \xrightarrow{\tau} (c_2,s)}
		\quad
		\prftree[r]{OutS1}{(ch!e, s) \xrightarrow{\langle ch!,s(e)\rangle} (\pskip, s)}
		\quad
		\prftree[r]{OutS2}{(ch!e, s) \xrightarrow{\langle d,I_s,\{ch!\}\rangle}  (ch!e, s)}
		& \\[1mm]
 	& 
		\prftree[r]{OutS3}{(ch!e, s) \xrightarrow{\langle \infty,I_s,\{ch!\}\rangle} (\pskip, s)} \quad
		\prftree[r]{InS1}{(ch?x, s) \xrightarrow{\langle ch?, v\rangle } (\pskip, s[x\mapsto v])}
		\quad
		\prftree[r]{InS2}{(ch?x, s) \xrightarrow{\langle d,I_s,\{ch?\}\rangle}
		  (ch?x, s)}
		 & \\[1mm]
 	& 			\prftree[r]{InS3}{(ch?x, s) \xrightarrow{\langle \infty,I_s,\{ch?\}\rangle}  (\pskip, s)}
		\quad
		\prftree[r]{IChoiceS1}{(c_1 \sqcup c_2, s) \xrightarrow{\tau} (c_1, s)}
		\quad
		\prftree[r]{IChoiceS2}{(c_1 \sqcup c_2, s) \xrightarrow{\tau} (c_2, s)}
		\quad
		\prftree[r]{RepB}{}
		{(c^*, s) \xrightarrow{\tau}
			(\pskip, s)}& \\[1mm]
 	& 
		%\[ \prftree[r]{EChoice1}{(\external{i\in I}{ch_i*}{Q_i}, s) \stackrel{\langle d,s,\rdy(\cup_{i\in I} ch_i*)\rangle}{\rightarrow} (\external{i\in I}{ch_i*}{Q_i}, s)} 
		%\qquad
		%\prftree[r]{EChoice2}{i\in I}{ch_i*=ch!e}{(\external{i\in I}{ch_i*}{Q_i}, s) \stackrel{\langle ch!, s(e)\rangle}{\rightarrow} (Q_i, s)} \]
		%\[ \prftree[r]{EChoice3}{i\in I}{ch_i*=ch?x}{(\external{i\in I}{ch_i*}{Q_i}, s) \stackrel{\langle ch?, v\rangle}{\rightarrow} (Q_i, s[x\mapsto v])} \qquad
	%	\prftree[r]{EChoice4}{(\external{i\in I}{ch_i*}{Q_i}, s) \stackrel{\langle \infty,s,\rdy(\cup_{i\in I} ch_i*)\rangle}{\rightarrow} (\pskip, s)} \]
		%\[ \prftree[r]{RepS1}{(c^*,s) \xrightarrow{\tau} (\pskip, s)} \qquad
		%\prftree[r]{RepS2}{(c^*,s) \xrightarrow{\tau} (c; c^*, s)} \]
		  \prftree[r]{RepS}
		{ (c,s)\xrightarrow{e} (c',s')  }
		{(c^*, s) \xrightarrow{e} (c';c^*, s')} 
		\quad\prftree[r]{ContS1}
		{\neg s(B)}
		{(\evo{x}{e}{B}, s) \xrightarrow{\tau} (\pskip, s)}
        \quad
        \prftree[r]{ContS2}
		{\begin{array}{cc}
				\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \\
				\vec{p}(0) = s(\vec{x}) \quad \forall t\in[0,d).\,s[\vec{x}\mapsto \vec{p}(t)](B)
		\end{array}}
		{(\evo{x}{e}{B}, s) \xrightarrow{\langle d,\vec{p},\{\}\rangle}  
			(\evo{x}{e}{B}, s[\vec{x} \mapsto \vec{p}(d)])}
		& \\[1mm]
 	&  \prftree[r]{IntS1}
		{\begin{array}{cc}
				\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \\
				\vec{p}(0) = s(\vec{x}) \quad \forall t\in[0,d).\,s[\vec{x}\mapsto \vec{p}(t)](B)
		\end{array}}
		{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s)
		\xrightarrow{\langle d,\vec{p},\rdy(\cup_{i\in I} ch_i*) \rangle} 
			(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s[\vec{x} \mapsto \vec{p}(d)]}
		& \\[1mm]
 	&  \prftree[r]{IntS2}
		{\neg s(B)}
		{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s) \xrightarrow{\tau}
			(c, s)}
		\qquad \prftree[r]{IntS3}
		{i\in I}{ch_i* = ch!e}
		{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s)
		\xrightarrow{\langle ch!,s(e)\rangle}  {(Q_i, s)}}
		& \\[1mm]
 	& 
		\prftree[r]{IntS4}
		{i\in I}{ch_i* = ch?x}
		{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s) \xrightarrow{\langle ch?,v\rangle} {(c_i, s[x\mapsto v])}}
		\qquad \prftree[r]{ParTauS}
{(c_1,s_1) \xrightarrow{\tau} (c_1',s_1')}
{(c_1\|_{cs} c_2,s_1\uplus s_2) \xrightarrow{\tau} (c_1'\|_{cs} c_2, s_1'\uplus s_2)}
& \\[1mm]
 	& 
\prftree[r]{ParDelayS}
{\begin{array}{cc}
		\compat(\rdy_1, \rdy_2) \\
		(c_1,s_1) \xrightarrow{\langle d,\vec{p}_1,\rdy_1\rangle}  (c_1',s_1')
		\quad 
		(c_2,s_2) \xrightarrow{\langle d,\vec{p}_2,\rdy_2\rangle}  (c_2',s_2')
\end{array}}
{(c_1\|_{cs} c_2, s_1\uplus s_2)
\xrightarrow{\langle d,\vec{p}_1\uplus \vec{p}_2,(\rdy_1\cup \rdy_2)-cs\rangle}  (c_1'\|_{cs} c_2', s_1'\uplus s_2')}
\quad
\prftree[r]{ParPairS1}
{ch \in cs} {(c_1,s_1) 
\xrightarrow{\langle ch!, v\rangle} (c_1',s_1')}
{(c_2,s_2) \xrightarrow{\langle ch?, v\rangle}  (c_2',s_2')}
{(c_1\|_{cs} c_2, s_1\uplus s_2) 
\xrightarrow{\langle ch,v\rangle} 
	(c_1'\|_{cs} c_2', s_1'\uplus s_2')}
& \\[1mm]
 	&  
\prftree[r]{ParPairS2}
{ch \in cs} {(c_1,s_1) \xrightarrow{\langle ch?, v\rangle}  (c_1',s_1')}
{(c_2,s_2) \xrightarrow{\langle ch!, v\rangle}  (c_2',s_2')}
{(c_1\|_{cs} c_2, s_1\uplus s_2) \xrightarrow{\langle ch,v\rangle} 
	(c_1'\|_{cs} c_2', s_1'\uplus s_2')}
	\prftree[r]{ParUnpairS1}
{ch \notin cs} {(c_1,s_1) \xrightarrow{\langle ch\triangleright,v\rangle}  (c_1',s_1')}
{(c_1\|_{cs} c_2, s_1\uplus s_2) \xrightarrow{\langle ch\triangleright,v\rangle} 
	(c_1'\|_{cs} c_2, s_1'\uplus s_2)}
& \\[1mm]
 	& 
\prftree[r]{ParUnpairS2}
{ch \notin cs} {(c_2,s_2) \xrightarrow{\langle ch \triangleright,v\rangle}  (c_2',s_2')}
{(c_1\|_{cs} c_2, s_1\uplus s_2) \xrightarrow{\langle ch\triangleright,v\rangle}
	(c_1\|_{cs} c_2', s_1\uplus s_2')}
& 
\end{eqnarray*}
}
\caption{Small-step operational semantics for sequential and parallel processes}
\label{fig:full-small-step}
\end{figure*}

\begin{figure*}
{\small 
\begin{eqnarray*}
& \prftree[r]{SkipB}{(\pskip,s) \Rightarrow (s,\epsilon)} 
	\quad \prftree[r]{AssignB}{(x:=e,s) \Rightarrow (s[x \mapsto e],\epsilon)} 
	\quad \prftree[r]{OutB1}{(ch!e, s) \Rightarrow (s, \langle ch!, s(e)\rangle)}& \\[1mm]
 	& 
	\prftree[r]{OutB2}{(ch!e, s) \Rightarrow (s, \langle d, I_s, \{ch!\}\rangle ^\chop \langle ch!, s(e)\rangle)} \quad \prftree[r]{OutB3}{(ch!e, s) \Rightarrow (s, \langle \infty,I_s,\{ch!\}\rangle )} 
	\quad\prftree[r]{InB1}{(ch?x, s) \Rightarrow (s[x \mapsto v], \langle ch?, v\rangle) } & \\[1mm]
 	& \prftree[r]{InB2}{(ch?x, s) \Rightarrow (s[x \mapsto v], \langle d, I_s, \{ch?\} \rangle^\chop \langle ch?, v\rangle)} 
	\quad\prftree[r]{InB3}{(ch?x,s)\Rightarrow (s, \langle \infty,I_s,\{ch?\})}
	\quad
	\prftree[r]{RepB1}{ }
	{(c^*, s) \Rightarrow (s, \epsilon)} & \\[1mm]
 	& 
	\prftree[r]{RepB2}{ (c, s)  \Rightarrow (s_1, \textit{tr}_1) \quad (c^*, s_1) \Rightarrow (s_2, \textit{tr}_2)  }
	{ (c^*, s) \Rightarrow (s_2, {\textit{tr}_1}^\chop \textit{tr}_2)} 
	\quad \prftree[r]{SeqB}{(c, s_1) \Rightarrow (s_2, tr_1)}{(c_2, s_2) \Rightarrow (s_3, tr_2)}
	{(c_1; c_2, s_1) \Rightarrow (s_3, {tr_1}^\chop tr_2)} 
	\quad
	\prftree[r]{CondB1}{s_1(B)}{(c_1, s_1) \Rightarrow (s_2, \textit{tr})}
	{(\IFE{B}{c_1}{c_2}, s_1) \Rightarrow (s_2, \textit{tr})} & \\[1mm]
 	&  \prftree[r]{CondB2}{\neg s_1(B)}{(c_2, s_1) \Rightarrow (s_2, \textit{tr})}
	{(\IFE{B}{c_1}{c_2}, s_1) \Rightarrow (s_2, \textit{tr})} 
	\quad\prftree[r]{IChoiceB1}{(c_1, s_1) \Rightarrow (s_2, \textit{tr})}
	{(c_1 \sqcup c_2, s_1) \Rightarrow (s_2, \textit{tr})} 
	\quad\prftree[r]{IChoiceB2}{(c_2, s_1) \Rightarrow (s_2, \textit{tr})}
	{(c_1 \sqcup c_2, s_1) \Rightarrow (s_2, \textit{tr})} 
	\quad& \\[1mm]
 	& \prftree[r]{ContB1}{
		\neg B(s)}
	{(\evo{x}{e}{B}, s) \Rightarrow (s, \epsilon)}
	\quad\prftree[r]{ContB2}{
		\begin{array}{cc}
			\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \\
			\vec{p}(0)=s(\vec{x})\quad \forall t\in[0,d).\,s[\vec{x}\mapsto \vec{p}(t)](B) \quad \neg s[\vec{x}\mapsto \vec{p}(d)](B)
	\end{array}}
	{(\evo{x}{e}{B}, s) \Rightarrow (s[\vec{x}\mapsto \vec{p}(d)], \langle d, \vec{p}, \{\}\rangle) }& \\[1mm]
 	& 
	%\[\prftree[r]{EChoiceO1}{i\in I}{ch_i*=ch!e}{(Q_i,s_1)\Rightarrow (s_2,\textit{tr})} {(\external{i\in I}{ch_i*}{Q_i}, s_1) \Rightarrow
%		(s_2, \langle ch!, s(e)\rangle^\chop \textit{tr})}\]
%\[
%	\prftree[r]{EChoiceO2}{i\in I}{ch_i*=ch!e}{(Q_i,s_1)\Rightarrow (s_2,\textit{tr})}
%	{(\external{i\in I}{ch_i*}{Q_i}, s_1) \Rightarrow
%		(s_2, \langle d, s, \rdy(\cup_{i\in I} ch_i*) \rangle ^\chop \langle ch!, s(e)\rangle^\chop \textit{tr})}\]
%	\[\prftree[r]{EChoiceI1}{i\in I}{ch_i*=ch?x}{(Q_i,s_1[x\mapsto v])\Rightarrow (s_2,\textit{tr})} {(\external{i\in I}{ch_i*}{Q_i}, s_1) \Rightarrow
%		(s_2, \langle ch?, v\rangle^\chop \textit{tr})}
%\]
%\[
%	\prftree[r]{EChoiceI2}{i\in I}{ch_i*=ch?x}{(Q_i,s_1[x\mapsto v])\Rightarrow (s_2,\textit{tr})}
%	{(\external{i\in I}{ch_i*}{Q_i}, s_1) \Rightarrow
%		(s_2, \langle d, s, \rdy(\cup_{i\in I} ch_i*) \rangle ^\chop \langle ch?, v\rangle^\chop \textit{tr})}\]
%	\[\prftree[r]{EChoiceW}{(\external{i\in I}{ch_i*}{Q_i}, s)\Rightarrow (s,\langle \infty,s,\rdy(\cup_{i\in I} ch_i*)\rangle)} \]
\prftree[r]{IntB1}{i\in I}{ch_i*=ch!e}{(c_i,s_1) \Rightarrow (s_2,\textit{tr})}
	{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s_1) \Rightarrow
		(s_2, \langle ch!, s_1(e)\rangle^\chop \textit{tr})} 
		\quad
		\prftree[r]{IntB3}{i\in I}{ch_i*=ch?y}{(c_i,s_1[y\mapsto v]) \Rightarrow (s_2,\textit{tr})}
	{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s_1) \Rightarrow
		(s_2, \langle ch?, v\rangle^\chop \textit{tr})}& \\[1mm]
 	& \prftree[r]{IntB2}{
		\begin{array}{cc}
			\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \quad \vec{p}(0)=s_1(\vec{x}) \\
			\forall t\in[0,d).\,s_1[\vec{x}\mapsto \vec{p}(t)](B) \quad i\in I\quad ch_i*=ch!e\quad (c_i,s_1[\vec{x}\mapsto \vec{p}(d)])\Rightarrow (s_2,\textit{tr})
	\end{array}}
	{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s_1) \Rightarrow (s_2,
		\langle d, \vec{p}, \rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s_1[\vec{x}\mapsto \vec{p}(d)](e)\rangle ^\chop \textit{tr})}& \\[1mm]
 	& \prftree[r]{IntB4}{
		\begin{array}{cc}
			\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \quad \vec{p}(0)=s_1(\vec{x}) \\
			\forall t\in[0,d).\,s_1[\vec{x}\mapsto \vec{p}(t)](B) \quad i\in I\quad ch_i*=ch?y\quad (c_i, s_1[\vec{x}\mapsto \vec{p}(d),y\mapsto v])\Rightarrow (s_2,\textit{tr})
	\end{array}}
	{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s_1) \Rightarrow (s_2,
		\langle d, \vec{p}, \rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch?,v\rangle ^\chop \textit{tr})}& \\[1mm]
 	& \prftree[r]{IntB5}{\neg s(B)}{(c,s1) \Rightarrow (s2,tr)
	}{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s1) \Rightarrow (s2, tr)}
\quad
	\prftree[r]{IntB6}{
		\begin{array}{cc}
			\vec{p} \mbox{ is a solution of the ODE $\vec{\dot{x}}=\vec{e}$} \quad\vec{p}(0)=s_1(\vec{x})\\
			 \forall t\in[0,d).\,s_1[\vec{x}\mapsto \vec{p}(t)](B) \quad \neg s_1[\vec{x}\mapsto \vec{p}(d)](B) \quad (c,s_1[\vec{x}\mapsto \vec{p}(d)]) \Rightarrow (s2,tr)
	\end{array}}
	{(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, s_1) \Rightarrow (s2,\langle d,\vec{p},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop tr)}
	& \\[1mm]
 	& 
	\oomit{
	\[ \prftree[r]{RepB1}{(c^*, s) \Rightarrow (s, \epsilon)} 
	\quad\prftree[r]{RepB2}{(c, s_1) \Rightarrow (s_2, tr_1)}{(c^*, s_2) \Rightarrow (s_3, tr_2)}
	{(c^*, s_1) \Rightarrow (s_3, {tr_1}^\chop tr_2)} \] }
	\prftree[r]{ParB}{(c_1, s_1) \Rightarrow (s_1', tr_1)}{(c_2, s_2) \Rightarrow (s_2', tr_2)}{tr_1\|_{cs}tr_2 \Downarrow \textit{tr}}
{(c_1 \|_{cs} c_2, s_1\uplus s_2) \Rightarrow (s_1'\uplus s_2', \textit{tr})} & 
\end{eqnarray*}
}
\caption{Big-step operational semantics}
\label{fig:full-big-semantics}
\end{figure*}
 

\subsection{Equivalence Between Big-step and Small-step Semantics}

Before stating the equivalence between big-step and small-step semantics, we need some preliminary concepts.
The transitive closure of small-step semantics is defined as follows. We use
\sm{(c,s) \stackrel{\textit{tr}}{\rightarrow^*}(c',s')} to indicate that starting from process \sm{c} and state \sm{s}, a sequence of small-step transitions  results in process \sm{c'} and state \sm{s'}, and \sm{\textit{tr}} collects the events that occurred in between, ignoring any \sm{\tau} events. The formal definition is given by the following set of rules. 
\[ \prftree{(c,s)\stackrel{\epsilon}{\rightarrow^*}(c,s)} \]
\[ \prftree{(c,s)\xrightarrow{\tau} (c',s')} {(c',s')\stackrel{\textit{tr}}{\rightarrow^*}(c'',s'')}
{(c,s)\stackrel{\textit{tr}}{\rightarrow^*}(c'',s'')}\]
\[ \prftree{(c,s)\xrightarrow{e} (c',s')}
{(c',s')\stackrel{\textit{tr}}{\rightarrow^*}(c'',s'')}
{(c,s)\stackrel{e^\chop \textit{tr}}{\rightarrow^*}(c'',s'')} \]

The following lemma will be useful later.
\begin{lemma}\label{lem:small-step-split}
	If $(c,s) \xrightarrow{\langle d,\vec{p},\rdy\rangle}  (c',s')$, and $0<d'<d$, then there exists $c''$ and $s''$ such that
	$(c,s) \xrightarrow{\langle d',\vec{p},\rdy\rangle}  (c'',s'')$ and
	$(c'',s'') \xrightarrow{\langle d-d',\vec{p}(\cdot+d'),\rdy\rangle}  (c',s')$.
\end{lemma}
\begin{proof}
	By case analysis on the small-step rule used to derive $(c,s)\xrightarrow{\langle d,\vec{p},\rdy\rangle} (c',s')$.
\end{proof}

The theorem going from big-step to small-step semantics is then stated as follows. For the parallel case, we use a single $\pskip$ to stand for the parallel composition of $\pskip$ programs.
\begin{theorem}[Big-step to Small-step] \label{thm:big2small}
	For any big-step relation \sm{(c,s)\Rightarrow (s',\textit{tr})}, we have the small-step relation
	\sm{(c,s) \stackrel{\textit{tr}}{\rightarrow^*} (\pskip,s')}.
\end{theorem}

\begin{proof}
	First prove the result where $c$ is a sequential program, by induction on the derivation of $(c,s)\Rightarrow (s',tr)$ using big-step semantics. We focus on the operations special to HCSP. The cases for skip, assign, sequence, conditional, internal choice, and repetition are standard.
	
	\begin{itemize}
		\item Input: there are three rules InB1, InB2 and InB3 in big-step semantics. InB1 corresponds to applying InS1, InB2 corresponds to applying InS2 followed by InS1, InB3 corresponds to applying InS3.
		
		\item Output: there are three rules OutB1, OutB2 and OutB3 in big-step semantics. OutB1 corresponds to applying OutS1, OutB2 corresponds to applying OutS2 followed by OutS1, OutB3 corresponds to applying OutS3.
		
		\item Repetition: there are two rules RepB1 and RepB2 in big-step semantics, corresponding to the cases that $c$ executes for zero or more than one times respectively.  
		
		\item Continuous: there are two rules ContB1 and ContB2 in big-step semantics. ContB1 corresponds to applying ContS1, ContB2 corresponds to applying ContS2 followed by ContS1.
		
		\item Interrupt: there are six rules in the big-step semantics. IntB1 corresponds to applying IntS3, IntB2 corresponds to applying IntS1 followed by IntS3, IntB3 corresponds to applying IntS4, IntB4 corresponds to applying IntS1 followed by IntS4. IntB5 corresponds to applying IntS2, IntB6 corresponds to applying IntS1 followed by IntS2.
	\end{itemize}
	
	Next, we prove the result when $c$ is a parallel program. Hence, we assume $(c_1,s_1)\Rightarrow (s_1',tr_1)$, $(c_2,s_2)\Rightarrow (s_2',tr_2)$ and $tr_1\|_{cs}tr_2 \Downarrow tr$. By induction, we have $(c_1,s_1) \stackrel{tr_1}{\rightarrow^*} (\pskip,s_1')$ and $(c_2,s_2) \stackrel{tr_2}{\rightarrow^*} (\pskip,s_2')$. We now induct on the derivation of $tr_1\|_{cs}tr_2 \Downarrow tr$. The cases correspond to the rules in Fig.~\ref{fig:rule-synchronization}.
	\begin{itemize}
		\item SyncIO: We have $ch\in cs$, $tr_1=\langle ch!,v\rangle^\chop tr_1'$, $tr_2=\langle ch?,v\rangle^\chop tr_2'$, $tr=\langle ch,v\rangle^\chop tr'$ and $tr_1'\|_{cs}tr_2'\Downarrow tr'$. From $(c_1,s_1)\stackrel{tr_1}{\rightarrow^*} (\pskip,s_1')$, we obtain $c_1', c_1'', s_1'', s_1'''$ such that
		\[ \begin{array}{l}
		(c_1,s_1)\stackrel{\epsilon}{\rightarrow^*}(c_1',s_1''), \\
		(c_1',s_1'')\xrightarrow{\langle ch!,v\rangle} (c_1'',s_1''') \mbox{ and}\\
		(c_1'',s_1''')\stackrel{tr_1'}{\rightarrow^*}(\pskip, s_1').
		\end{array} \]
		 Likewise, from $(c_2,s_2)\stackrel{tr_2}{\rightarrow^*}(\pskip,s_2')$, we obtain $c_2', c_2'', s_2'', s_2'''$ such that
		\[ \begin{array}{l}
		(c_2,s_2)\stackrel{\epsilon}{\rightarrow^*}(c_2',s_2''), \\
		(c_2',s_2'')\xrightarrow{\langle ch?,v\rangle}  (c_2'',s_2''') \mbox{ and} \\
		(c_2'',s_2''')\stackrel{tr_2'}{\rightarrow^*}(\pskip, s_2').
		\end{array} \]
		Now by applying rule ParTauS repeatedly, rule ParPairS1 and the inductive hypothesis, we obtain
		\[
		\begin{array}{l}
			(c_1\|_{cs}c_2, s_1\uplus s_2) \stackrel{\epsilon}{\rightarrow^*} (c_1'\|_{cs}c_2', s_1''\uplus s_2''), \\
			(c_1'\|_{cs}c_2', s_1''\uplus s_2'') \xrightarrow{\langle ch,v\rangle}  (c_1''\|_{cs}c_2'', s_1'''\uplus s_2'''), \\
			(c_1''\|_{cs}c_2'', s_1'''\uplus s_2''') \stackrel{tr'}{\rightarrow^*} (\pskip\|_{cs}\pskip, s_1'\uplus s_2').
		\end{array}
	 	\]
	 	They combine together to give $(c_1\|_{cs}c_2, s_1\uplus s_2) \stackrel{tr}{\rightarrow^*}(\pskip\|_{cs}\pskip, s_1'\uplus s_2')$, as desired. The other direction SyncIO' is similar, where the small-step rule ParPairS2 is used.
	 	
	 \item NoSyncIO: the proof is similar to the SyncIO case, except we only need to work on the left side. The corresponding small-step rule is ParUnpairS1. 
  %The case NoSyncIO' is similar, where we only need to work on the right side, and the corresponding small-step rule is ParUnpairS2.
	 
	 \item SyncWait1: the proof is similar to the SyncIO case. The corresponding small-step rule is ParDelayS.
	 
	 \item SyncWait2: We have $d_1>d_2$, $tr_1=\langle d_1,\vec{p}_1,\rdy_1\rangle^\chop tr_1'$, $tr_2=\langle d_2,\vec{p}_2,\rdy_2\rangle^\chop tr_2'$, $tr=\langle d_2,\vec{p}_1\uplus \vec{p}_2,(\rdy_1\cup\rdy_2)-cs\rangle^\chop tr'$, $\compat(\rdy_1,\rdy_2)$ and
	 \[ \langle d_1-d_2, \vec{p}_1(\cdot+d_2),\rdy_1\rangle^\chop tr_1 \|_{cs} tr_2 \Downarrow tr \]
	 
	 From $(c,s_1) \stackrel{tr_1}{\rightarrow^*} (\pskip,s_1')$, we obtain $c',c'',s_1'',s_1'''$ such that
	 \[
	 \begin{array}{l}
	 (c,s_1) \stackrel{\epsilon}{\rightarrow^*} (c',s_1''), \\
	 (c',s_1'') \xrightarrow{\langle d_1,\vec{p}_1,\rdy_1\rangle}  (c'',s_1'''), \mbox{and} \\
	 (c'',s_1''') \stackrel{tr_1'}{\rightarrow^*} (\pskip,s_1').
	 \end{array} \]
	 By Lemma~\ref{lem:small-step-split}, we obtain $c''',s''''$ such that
	 \[ \begin{array}{l}
	 (c',s_1'') \xrightarrow{\langle d_2,\vec{p}_1,\rdy_1\rangle}  (c''',s'''') \mbox{and} \\
	 (c''',s'''') \xrightarrow{\langle d_1-d_2,\vec{p}_1(\cdot+d_2),\rdy_1\rangle}  (c'',s_1''').
	 \end{array} \]
	 The rest follows as before, by applying rule ParTauS repeatedly, rule ParDelayS, and the inductive hypothesis. 
  %The symmetric case SyncWait2' is similar.
	\end{itemize}
\end{proof}

To state the theorem going from small-step to big-step semantics, we need to define the concept of \emph{reduction} from one trace to another. We use \sm{\textit{tr} \leadsto_r \textit{tr}'} to mean that \sm{\textit{tr}'} can be obtained from \sm{\textit{tr}} by combining some of the neighboring blocks that can be joined with each other. Note reduction is not unique: there is no obligation to perform all possible joins. The formal definition is as follows:

\[ \prftree[r]{ReduceEmpty}{\epsilon \leadsto_r \epsilon}\]
\[
\prftree[r]{ReduceMerge}{h_1(d_1) = h_2(0)}
{
\begin{array}{ll}
     \langle d_1,\vec{p}_1,\rdy\rangle ^\chop \langle d_2,\vec{p}_2,\rdy\rangle ^\chop \textit{tr}
	\leadsto_r \\
	\qquad\langle d_1+d_2,\vec{p}_1 \cdot \vec{p}_2,\rdy \rangle ^\chop \textit{tr} 
\end{array} }\]
\[ \prftree[r]{ReduceCons}{tr_1 \leadsto_r tr_2}
{e^\chop tr_1 \leadsto_r e^\chop tr_2}
\]
\[
\prftree[r]{ReduceTrans}{tr_1 \leadsto_r tr_2}{tr_2 \leadsto_r tr_3}
{tr_1 \leadsto_r tr_3} \]

A key lemma states that the synchronization of traces respects the reduction relation. More precisely:

\begin{lemma}\label{lem:reduce_sync}
	Given \sm{tr_1\leadsto_r tr_1'}, \sm{tr_2\leadsto_r tr_2'} and \sm{tr_1\|_{cs} tr_2\Downarrow \textit{tr}}, then there exists \sm{\textit{tr}'} such that \sm{\textit{tr}\leadsto_r \textit{tr}'} and \sm{tr_1'\|_{cs} tr_2'\Downarrow \textit{tr}'}.
\end{lemma}

The theorem going from small-step to big-step semantics is as follows.

\begin{theorem}[Small-step to Big-step] \label{thm:small2big}
	For any small-step relation \sm{(c,s)\stackrel{\textit{tr}}{\rightarrow^*}(\pskip,s')}, there exists \sm{\textit{tr}'} such that \sm{\textit{tr}\leadsto_r \textit{tr}'} and \sm{(c,s)\Rightarrow (s',\textit{tr}')}.
\end{theorem}

\begin{proof}
First, we prove the result when $c$ is a sequential program. Induction on the derivation of $(c,s)\stackrel{tr}{\rightarrow^*}(\pskip,s')$ gives three cases. The first case corresponds to $tr=\epsilon,c=\pskip$ and $s=s'$, and the result follows immediately. In the second case, we have $(c,s)\stackrel{\tau}{\to} (c',s')$ and $(c',s')\stackrel{tr}{\to^*} (\pskip,s'')$. From the inductive hypothesis, there exists $tr'$ such that $tr\leadsto_r tr'$ and $(c',s')\Rightarrow (s'',tr')$. It then suffices to show $(c,s)\Rightarrow (s'',tr')$. The proof is by a further induction on the derivation of $(c,s)\stackrel{\tau}{\to}(c',s')$. We omit the details here.

In the third case, we have $(c,s)\stackrel{e}{\to} (c',s')$ and $(c',s')\stackrel{tr}{\to^*} (\pskip,s'')$ for some event $e\neq\tau$. From the inductive hypothesis, there exists $tr'$ such that $tr\leadsto_r tr'$ and $(c',s')\Rightarrow (s'',tr')$, and we need to show there exists some $tr''$ such that $e^\chop tr\leadsto_r tr''$ and $(c,s)\Rightarrow (s'',tr'')$. As in the second case, the proof is by a further induction on the derivation of $(c,s)\stackrel{e}{\to}(c',s')$. In some of the cases where $e$ is a wait block, it is necessary to apply the ReduceMerge rule to merge $e$ with the initial block of $tr$.

Next, we prove the result when $c$ is a parallel program. Again, induction on the derivation of $(c,s) \stackrel{tr}{\rightarrow^*} (\pskip,s')$ results in three cases. In the third case where the first step generates an event $e\neq \tau$, we need to consider each of the small-step rules ParDelayS, ParPairS1, PairPairS2, ParUnpairS1 and ParUnpairS2, making use of Lemma~\ref{lem:reduce_sync}. The details are omitted.
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:small-big-semantics}]
Theorem~\ref{thm:small-big-semantics} can be proved directly from 
Theorem~\ref{thm:big2small} and Theorem~\ref{thm:small2big}.
\end{proof}

\section{Proof of soundness and trace history independence}\label{app:soundness}

In this section, we prove the soundness of the hybrid Hoare logic presented in Section~\ref{sec:hoare}, as stated in 
Theorem~\ref{thm:soundness}. We also prove the trace history independence of the logic, as stated in Theorem~\ref{thm:trace-independence}. 


\begin{proof}[Proof for Theorem~\ref{thm:soundness}]	
	Suppose \sm{(c,s) \Rightarrow (s',h')} and \sm{\nseman{P}{s}{h}}, we need to prove \sm{\nseman{Q}{s'}{h^\chop h'}}. We show this by induction on the structure of program \sm{c}.
	
	\begin{itemize}
		\item Assign: From \sm{\nseman{Q[e/x]}{s}{h}}, it follows \sm{\nseman{Q}{s[x \mapsto e]}{h}}. It is also clear that \sm{Q[e/x]} is the weakest precondition.
		
		\item Output: Assume
		\[\begin{array}{ll}
			Q[\tracev^\chop \langle ch!, e\rangle/\tracev]\,\wedge \\
			\forall d>0.\,Q[\tracev^\chop \langle d, I_{\vec{x}_0}, \{ch!\}\rangle^\chop \langle ch!, e\rangle/\tracev]\,\wedge \\
			Q[\tracev^\chop \langle \infty, I_{\vec{x}_0},\{ch!\}\rangle/\tracev]
		\end{array}\]
		holds in \sm{(s, h)}. The three parts of the conjunction correspond to the three big-step rules for output. Since the first part holds, we get \sm{\nseman{Q}{s}{h^\chop \langle ch!, s(e) \rangle}}, showing \sm{Q} holds after following the semantic rule \sm{\textrm{OutB1}}. Since the second part holds, we get \sm{\nsemans{Q}{s}{h^\chop h'}{\lvar}}, where \sm{h'= \langle d, I_s, \{ch!\}\rangle^\chop \langle ch!, s(e)\rangle}  for any \sm{d>0}, showing \sm{Q} holds after following the second semantic rule \sm{\textrm{OutB2}}. Since the third part holds, we get \sm{\nseman{Q}{s}{h^\chop h'}}, where \sm{h' = \langle \infty,I_s,\{ch!\}\rangle}, showing \sm{Q} holds after following the third semantic rule \sm{\textrm{OutB3}}. The above analysis also shows that the precondition is in fact the weakest liberal precondition.
		
		\item Input: Assume
		\[
		\begin{array}{ll}
			\forall v.\, Q[v/x, \tracev^\chop \langle ch?, v\rangle/\tracev]\,\wedge \\
			\forall d>0.\,\forall v.\, Q[v/x, \tracev^\chop \langle d, I_{\vec{x}_0},\{ch?\}\rangle ^\chop \langle ch?,v\rangle/\tracev]\,\wedge \\
			Q[\tracev^\chop\langle \infty, I_{\vec{x}_0}, \{ch?\}\rangle/\tracev]
		\end{array}
		\]
		holds in \sm{(s, h)}. The three parts of the conjunction correspond to the three big-step rules for input. Since the first part holds, we get \sm{\nsemans{Q}{s[x\mapsto v]}{h^\chop \langle ch?, v \rangle}{\lvar}} for any \sm{v}, showing \sm{Q} holds after following the semantic rule \sm{\textrm{InB1}}. Since the second part holds, we get \sm{\nsemans{Q}{s[x\mapsto v]}{h^\chop h' }{\lvar}}, where \sm{h' =\langle d, I_s, \{ch?\}\rangle^\chop \langle ch?, v \rangle}  for any \sm{d>0} and \sm{v}, showing \sm{Q} holds after following the semantic rule \sm{\textrm{InB2}}. Since the third part holds, we get \sm{\nseman{Q}{s}{h^\chop h'}}, where \sm{h' = \langle \infty,I_s,\{ch?\}\rangle}, showing \sm{Q} holds after following the semantic rule \sm{\textrm{InB3}}. The above analysis also shows that the precondition is in fact the weakest liberal precondition.
		
		\item Sequence: By induction, we have \sm{\vDash\spec{Q}{c_2}{R}}. According to the semantics rule \sm{\textrm{SeqB}}, there must exist \sm{(s_1, h_1)} such that \sm{(c_1,s) \Rightarrow (s_1,h_1)} and \sm{(c_2,s_1) \Rightarrow (s',h_2)} and \sm{h'=h_1 {^\chop} h_2}. The Hoare triple for \sm{c_1} gives \sm{\nseman{Q}{s_1}{h^\chop h_1}}, then the Hoare triple for \sm{c_2} gives
		\sm{\nseman{R}{s'}{(h^\chop h_1)^\chop h_2}}, which is equal to \sm{\nseman{R}{s'}{h^\chop h'}}.
		
		\item The proofs for conditional rule and internal choice are as usual.
		
		\item Repetition: By induction, we have \sm{\vDash \spec{P}{c}{P}}. According to the operational semantics, there are two cases. The first case is \sm{h'=\epsilon} (rule \sm{\textrm{RepB1}}). Then \sm{\nseman{P}{s}{h}} holds directly. In the second case, there exist \sm{s_1, h_1} and \sm{h_2} such that
		\[(c, s) \Rightarrow (s_1, h_1), (c^*, s_1) \Rightarrow (s', h_2) \] 
		and \sm{h'=h_1 {^\chop} h_2} (rule \sm{\textrm{RepB2}}). From the Hoare triple for \sm{c}, we have \sm{\nseman{P}{s_1}{h^\chop h_1}}; then by induction on the number of iterations, we get the \sm{\nseman{P}{s'}{(h^\chop h_1)^\chop h_2}}, which is 
		equal to \sm{\nseman{P}{s'}{h^\chop h'}}.
	
	\oomit{	\item  Recursion: The proof is standard. Assume $P$ holds in $(s, h)$. For recursion, if it terminates, there must be a finite recursive depth. The fact holds for depth 0 trivially.  Suppose the fact holds for depth n, then $\vDash \spec{P}{\mu X. c}{Q}$ holds for depth n. We need to prove the fact holds for depth $n+1$. As $\vDash \spec{P}{\mu X. c}{Q}$, then
		by assumption,  \sm{\vDash \spec{P}{c[\mu X. c/X]}{Q}}  is true, which corresponds to exactly  the $n+1$-th call. According to the semantic rule RecB,  \sm{(c[\mu X. c/X], s) \Rightarrow (s',h')}, then \sm{\nseman{Q}{s'}{h^\chop h'}} is proved by induction. Similarly, we can prove that the weakest liberal precondition defined by unfolding the body is also sound. }
		
		\item Continuous: Assume
		\[  
		\begin{array}{ll}
			(\neg B \rightarrow Q)\,\wedge \\
			\forall d>0.\,(\forall t\in[0,d).\,B[\vec{p}_{\vec{x}_0}(t)/\vec{x}])\wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \to \\
			\qquad Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle/\tracev]
		\end{array}
		\]
		holds in \sm{(s, h)}. There are two parts of the conjunction, corresponding to the evaluation of \sm{B} on \sm{s}.
		If \sm{B} is false in \sm{s}, according to the operational semantics (rule \sm{\textrm{ContB1}}), \sm{s' = s} and \sm{h'} is \sm{\varepsilon}. From the first part of the conjunction, we get \sm{\nseman{Q}{s}{h}}, which is equal to \sm{\nseman{Q}{s'}{h^\chop h'}}, as desired.
		
		Now suppose \sm{B} is true in \sm{s}, according to the operational semantics (rule \sm{\textrm{ContB2}}), suppose \sm{d>0} and the solution \sm{\vec{p}} of the ODE starting from \sm{s} satisfies \sm{\forall t\in[0,d).\,s[\vec{x}\mapsto \vec{p}_{\vec{x}_0}(t)](B)} and \sm{s[\vec{x}\mapsto \vec{p}_{\vec{x}_0}(d)](\neg B)}, then the final state and trace are \sm{s'=s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(d)]}  and \sm{h'= \langle d, \vec{p}, \emptyset\rangle}. From the second part of the conjunction, with \sm{\seman{\vec{p}_{\vec{x}_0}}_{s,h}=\vec{p}}, we get \sm{\nsemans{Q}{s'}{h^\chop \langle d, \vec{p}, \emptyset\rangle}{\lvar}}, as desired. The above analysis also shows that the precondition is in fact the weakest liberal precondition.
		
		\item Interrupt: There are four parts in the precondition of the rule \sm{\textrm{Int}}. The first two parts are for interrupt by output and input communication, respectively. The third part is for the case that violation of \sm{B} holds initially, and the fourth part is for the case that after some time \sm{d>0}, \sm{B} violates. The last two cases will transfer to execute $c$. Here we give the proof for the case of interrupt by output communication. The input case is also similar.
		
		Assume the semantic rule \sm{\textrm{IntB2}} is applied, suppose \sm{d>0} and the solution \sm{\vec{p}} of the ODE starting from \sm{s} satisfies \sm{\forall t\in[0,d).\,s[\vec{x}\mapsto \vec{p}(t)](B)}, and there exists \sm{ch!e \in ch_i*} and \sm{(c_i,s[\vec{x} \mapsto \vec{p}(d)])\Rightarrow (s_2,h_2)}. Then the final state and trace of the interrupt command is
		\[ (s_2, \langle d, \vec{p}, \rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}(d)](e)\rangle ^\chop h_2). \]
		From the assumption 
		{\small
		\[
		\begin{array}{ll}
			P \rightarrow \forall d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \to \\
			\quad Q_i[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle\\
			\quad^\chop \langle ch!,e[\vec{p}_{\vec{x}_0}(d)/\vec{x}]\rangle/\tracev] 
		\end{array}
		\]}
		
		\vspace{-2mm}\noindent plus that \sm{P} holds for \sm{(s, h)}, we have the right side of the entailment holds for \sm{(s, h)}. Then the assumptions on \sm{d} and \sm{\vec{p}} gives
		{\small
		\[Q_i[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,e[\vec{p}_{\vec{x}_0}(d)/\vec{x}]\rangle/\tracev] \]
		}
		
		\vspace{-2mm}\noindent must hold for \sm{(s, h)}. Thus
		\sm{Q_i} must hold for \sm{s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(d)], h_1)} with 
		{\small
		\[ h_1 = h ^\chop \langle d_0,\vec{p},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}(d)](e))\rangle\]
    	}
    	
		\vspace{-2mm}\noindent By the inductive assumption on \sm{c_i}, we have \sm{\nseman{R}{s_2}{h_1 {^\chop} h_2}}, which is 
		\[ \nseman{R}{s_2}{h ^\chop \langle d_0,\vec{p},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}(d)](e)\rangle^\chop h_2} \]
		The above proof shows the case where the output interrupt occurs after time \sm{d>0}. There is another simpler case without waiting time, we omit the details here.
		
		%\item External choice: This can be considered as a simpler variant of continuous interrupt. The only addition is the possibility of waiting for an infinite amount of time. We omit the proof here.
		
		\item Parallel composition: Assume \sm{P_1[\epsilon/\tracev] \wedge P_2[\epsilon/\tracev]} holds in \sm{(s_1\uplus s_2, h)}, 
		and \sm{(c_1 \|_{cs} c_2, s_1\uplus s_2) \Rightarrow (s_1'\uplus s_2', h')}, so that there exist $h_1$ and $h_2$ such that \sm{(c_1, s_1) \Rightarrow (s_1', h_1)$, $(c_2, s_2) \Rightarrow (s_2', h_2)} and \sm{h_1\|_{cs}h_2 \Downarrow h'} hold, according to the semantic rule \sm{\textrm{ParB}}. We need to prove 
		\[ \exists tr_1\, tr_2.\, Q_1[tr_1/\tracev] \wedge Q_2[tr_2/\tracev] \wedge tr_1\|_{cs}tr_2 \Downarrow \tracev \]
		holds for \sm{(s_1'\uplus s_2', h^\chop h')}. From the assumption that \sm{P_1[\epsilon/\tracev] \wedge P_2[\epsilon/\tracev]} holds in \sm{(s_1\uplus s_2, h)}, we have \sm{h=\varepsilon}, \sm{\nseman{P_1}{s_1}{h}} and \sm{\nseman{P_2}{s_2}{h}}. By induction on \sm{c_1} and \sm{c_2}, we get \sm{\nseman{Q_1}{s'_1}{h_1}} and \sm{\nseman{Q_2}{s'_2}{h_2}}. On the other hand, we also have \sm{ \seman{tr_1}_{s'_1}^\lvar = h_1} and
		\sm{\seman{tr_2}_{s'_2}^\lvar = h_2}. 
		
		\ommit{
		We need to prove \sm{\nseman{tr_1\|_{cs}tr_2 \Downarrow \tracev}{s_1'\uplus s_2'}{h'}}, on the premises that \sm{h_1\|_{cs}h_2 \Downarrow h} defined according to Fig.~\ref{fig:rule-synchronization} and \sm{tr_1\|_{cs}tr_2 \Downarrow \tracev} defined according to Fig.~\ref{fig:elim-synchronization}.}
		
		\oomit{
		This can be proved by structural induction on \sm{tr_1} and \sm{tr_2}. We prove two cases here for illustration. If \sm{tr_1} and \sm{tr_2} are beginning with two compatible events, the only rule that can be applied is rule SyncPairE of Fig.~\ref{fig:elim-synchronization}. Then according to rule  SyncPairE, \sm{tr} is \sm{\langle ch,v\rangle^\chop \textit{tr}'}, with \sm{tr'_1\|_{cs}tr'_2 \Downarrow \textit{tr}'} and \sm{ch_1=ch_2=ch \wedge v_1 = v_2 =v \wedge (*_1,*_2)\in \{(!,?),(?,!)\}} for some \sm{ch} and \sm{v}. From the assumption \sm{\seman{tr_1}_{s'_1}^\lvar = h_1} and \sm{\seman{tr_2}_{s'_2}^\lvar = h_2}, \sm{h_1} and \sm{h_2} will be \sm{\langle ch!,v\rangle^\chop h'_1 } and \sm{\langle ch?,v\rangle ^\chop h'_2} for which \sm{h'_1 = \seman{tr'_1}_{s'_1}^\lvar} and \sm{h'_2 = \seman{tr'_2}_{s'_2}^\lvar}. Thus, according to the definition of \sm{\textit{tr}} and \sm{h_1\|_{cs}h_2 \Downarrow h}, the fact  \sm{\nseman{tr_1\|_{cs}tr_2 \Downarrow \textit{tr}}{s_1'\uplus s_2'}{h'}} is proved by induction. If both \sm{tr_1} and \sm{tr_2} are beginning with two external events, the only rule that can be applied is rule SyncUnpairE2 of Fig.~\ref{fig:elim-synchronization}. There are two possibilities for \sm{tr}.\sm{h_1} and \sm{h_2} are also beginning with two corresponding external events, if \sm{h} is produced by taking the left event first. Then it is guaranteed by the reduction of the left event in rule SyncUnpairE2, i.e. 
		\sm{tr = \langle ch_1\triangleright_1, v_1 \rangle ^\chop tr'} for some \sm{tr'} and \sm{tr_1\|_{cs} \langle ch_2\triangleright_2,v_2\rangle^\chop tr_2 \Downarrow \textit{tr}'}.  Thus \sm{\nseman{tr_1\|_{cs}tr_2 \Downarrow \textit{tr}}{s_1'\uplus s_2'}{h'}} is proved by induction. The other case of \sm{h} can be proved also. }
		
		Thus the existence condition holds for \sm{(s_1'\uplus s_2', h')} by taking \sm{tr_1} and \sm{tr_2} to be \sm{h_1, h_2}, respectively. This analysis also shows that the postcondition is in fact the strongest postcondition.
		
		\item The proof for the soundness of the rest rules are  as usual.
		
	%	\item Trace history independence:  Assume $P \wedge tr=h_0$ holds in $(s, h)$. From the assumption that $tr$ does not occur in $P$ freely,  $P$ is not depending on the trace, thus $P$ holds for $(s, \epsilon)$. By induction,  $Q \wedge tr=h_1$ holds for $(s', h')$. As $Q$ is independent on the value of $tr$, and $tr=h_0$ holds for $(s, h)$, we obviously have $Q \wedge tr=h_0 ^\chop h_1$ holds for $(s', h^\chop h')$.
	\end{itemize}
	
We now prove that each rule in Fig.~\ref{fig:elim-synchronization} is valid according to the rules of Fig.~\ref{fig:rule-synchronization}. Selection of the proofs of the rules are given due to similarity. 
\begin{itemize}
\item Rule SyncPairE: Since \sm{ch_1\in cs} and \sm{ch_2\in cs}, the assumption can only be derived from rule SyncIO, and the result follows.

\
item Rule SyncUnpairE1: Since \sm{ch_1\notin cs}, the assumption cannot be derived from rule SyncIO, so only NoSyncIO can be used, and the result follows. 
Derivation for its symmetric counterpart is similar.

\item Rule SyncUnpairE2: the assumption can be derived using  NoSyncIO or its symmetric case. These two cases correspond to the two cases of the disjunction, respectively.

\item Rule SyncUnpairE3: only rule SyncUnpairE1 can be used to derive the assumption, and the result follows. Derivation for its symmetric counterpart   is similar.

\item Rule SyncUnpairE4: the negation of the conclusion cannot be derived using any rule. Note SyncUnpairE1 cannot be used since \sm{ch \in cs}.

\item Rule SyncWaitE1: this rule states that two processes cannot be waiting for two sides of the same communication at the same time. The negation of the conclusion can be derived using only one of SyncWait1, SyncWait2 and its symmetric case. However, all these rules require the condition \sm{\compat(\rdy_1,\rdy_2)}.

\item Rules SyncWaitE2, SyncWaitE3 and its symmetric case: the assumptions of the three rules only be derived from SyncWait1, SyncWait2 and its symmetric case, respectively, so the result follows.

\item Rule SyncEmpE1: the assumption of the rule can be derived only using SyncEmpty3.

\item Rule SyncEmpE2: there is no introduction rule that can derive the negation of the conclusion.

\item Rule SyncEmpE3: the only introduction rule that can derive the assumption is NoSyncIO. This rule requires that \sm{ch \notin cs}, so the result follows. Derivation of its symmetric counterpart   is similar.
\end{itemize}
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:trace-independence}]
We give a proof sketch for this theorem by structural induction on $c$. For all the cases, suppose $\vdash \spec{P\wedge \tracev=\epsilon}{c}{Q\wedge \tracev=h}$ holds, we need to prove $\vdash \spec{P\wedge \tracev=h'}{c}{Q\wedge \tracev=h'^\chop h} $ holds when $\freev(h') \cap \wvar(c) = \emptyset$. Next we use $\equiv$ to represent that two assertions are equivalent. 

\begin{itemize}
    \item Rule Skip: we have $P \equiv Q$ and $h = \epsilon$, the fact holds trivially. 
    
    \item Rule Assign: we have $(P\wedge \tracev=\epsilon) \equiv ((Q\wedge \tracev=h)[e/x])$, then $P\equiv Q[e/x]$ and $h = \epsilon$. By applying rule Assign, $\spec{P\wedge \tracev=h'}{c}{Q\wedge \tracev=h'^\chop h}$  holds when $x$ does not occur in $h'$. This is guaranteed by the restriction $\freev(h') \cap \wvar(c) = \emptyset$. 
    
    \item Rule Output: According to the rule, $P\wedge \tracev=\epsilon$ is equivalent to 
    \[\begin{array}{ll}
			(Q\wedge \tracev=h)[\tracev^\chop \langle ch!, e\rangle/\tracev]\,\wedge \\
			\forall d>0.\,(Q\wedge \tracev=h)[\tracev^\chop \langle d, I_{\vec{x}_0}, \{ch!\}\rangle^\chop \langle ch!, e\rangle/\tracev]\,\wedge \\
			(Q\wedge \tracev=h)[\tracev^\chop \langle \infty, I_{\vec{x}_0},\{ch!\}\rangle/\tracev]
		\end{array}\]
		Then by replacing $h$ by $h'^\chop h$ in the above formula, and denoting the resulting formula as $Pre'$, then we need to prove that $Pre'$ is equivalent to $P \wedge \tracev=h'$. The proof is given below. In fact, there are three cases for the conjunction depending on whether and when communication occurs. If the first case occurs, we have $(P\wedge \tracev=\epsilon) \equiv ((Q\wedge \tracev=h)[\tracev^\chop\langle ch!, e\rangle/\tracev])$, then $P \equiv Q$ and $h \equiv \langle ch!, e\rangle$. Then by applying the same rule for postcondition $Q\wedge \tracev=h'^\chop h$, we get the precondition $P \equiv \tracev=h'$, the fact is proved.
    If the second case occurs, we have  $(P\wedge \tracev=\epsilon) \equiv \forall d>0. ((Q\wedge \tracev=h)[\tracev^\chop \langle d, I_{\vec{x}_0}, \{ch!\}\rangle^\chop \langle ch!, e\rangle/\tracev])$, then  $P \equiv Q$ and $h \equiv \langle d, I_{\vec{x}_0}, \{ch!\}\rangle^\chop \langle ch!, e\rangle$ for any $d>0$. Then by applying the same rule for postcondition $Q\wedge \tracev=h'^\chop h$, we get the precondition $P \equiv \tracev=h'$, the fact is proved.
    The third case can be proved similarly and we omit it. 
    
    \item Rule Input: The proof can be given by combining the proofs for output and assignment. We omit the details here.
    
    \item Rule Cont: According to the rule, $P \wedge \tracev = \epsilon$ is equivalent to 
    \[
    \begin{array}{ll}
 			(\neg B \rightarrow (Q\wedge \tracev=h))\,\wedge \\
 			\forall d>0.\,(\forall t\in[0,d).\,B[\vec{p}_{\vec{x}_0}(t)/\vec{x}])\wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \to \\
 			\qquad (Q\wedge \tracev=h)[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle/\tracev]
 		\end{array}
\]
There are two cases depending on whether $B$ holds or not initially. If $B$ does not hold, we have $h=\epsilon$. By replacing $h$ by $h'^\chop h$, it is equivalent to $P \wedge \tracev=h'$, which completes the proof. Otherwise if $B$ holds, then $h=\langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle$. By replacing 
$h$ by $h'^\chop h$, it is equivalent to $P \wedge \tracev=h'$, as $\vec{x}$ do not occur in $h'$.   

\item Rule Int: There are six cases for the interrupt. Consider the two cases for output interrupt, some $c_i$ executes after the communication occurs. Then by induction, the trace history independence holds for $c_i$: if we have $\spec{P_c \wedge \tracev = h_c}{c_i}{Q\wedge \tracev=h}$  for some $h_c$, then for any $h'$ that the variables of $c_i$ do not occur in, there must be $\spec{P_c \wedge \tracev = h'^\chop h_c}{c_i}{Q\wedge \tracev=h'^\chop h}$. Continuing the proof by considering the communication, we can obtain 
$\spec{P \wedge \tracev = h'}{c}{Q\wedge \tracev=h'^\chop h}$, as $\vec{x}$ does not occur in $h'$.

\item Rule Par: For parallel composition $c_1\|c_2$, the initial traces for both $c_1$ and $c_2$ are always $\epsilon$, so the fact holds trivially. 

\item Rule Seq: By induction, for $c_1$ and $c_2$, we can get the two facts:
$\spec{P\wedge \tracev=h'}{c_1}{Q_m \wedge \tracev=h'^\chop h_m}$ and 
$\spec{Q_m \wedge \tracev=h'^\chop h_m}{c_2}{Q \wedge \tracev=h'^\chop h_m ^\chop h_n}$ such that $h =h_m ^\chop h_n$. The fact holds by applying Rule Seq. 

 

\item  Rules Cond, IChoice, Repetition, Conj, Inv, Conseq can be proved easily by induction.  

\end{itemize}
    
\end{proof}

\section{Continuous and discrete completeness}\label{app:completeness}

In this section, we give details about the proof of continuous and discrete completeness of hybrid Hoare Logic, which are presented in Section~\ref{sec:completeness} and Section~\ref{sec:discretecomplete}, respectively.  

\subsection{Proof of continuous completeness}
\begin{figure*}
{\small 
    \[ \wp(\pskip, Q) = Q \quad \wp(x := e, Q) = Q[e/x] \]	
	\[ \begin{array}{ll} \wp(ch!e, Q) =
		&Q[\tracev^\chop \langle ch!, e\rangle/\tracev]\,\wedge \forall d>0.\,Q[\tracev^\chop \langle d, I_{\vec{x}_0}, \{ch!\}\rangle^\chop \langle ch!, e\rangle/\tracev]\,\wedge  Q[\tracev^\chop \langle \infty,I_{\vec{x}_0},\{ch!\}\rangle/\tracev]
	\end{array}
	\]	
	\[ \begin{array}{ll} \wp(ch?x, Q) =
		&\forall v.\, Q[v/x, \tracev^\chop \langle ch?, v\rangle/\tracev]\,\wedge \forall d>0.\,\forall v.\, Q[v/x, \tracev^\chop \langle d,I_{\vec{x}_0},\{ch?\}\rangle ^\chop \langle ch?,v\rangle/\tracev]\,\wedge  Q[\tracev^\chop \langle \infty,I_{\vec{x}_0},\{ch?\}\rangle/\tracev]
	\end{array}
	\]
	\[ \wp(c1; c2, Q) = \wp(c1, \wp(c2, Q)) \]
	\[ \wp(\IFE{b}{c_1}{c_2}, Q) = \IFE{b}{\wp(c_1,Q)}{\wp(c_2,Q)} \]
	\[ \wp(c_1\sqcup c_2, Q) = \wp(c_1, Q) \wedge \wp(c_2, Q) \]
	\[
	\begin{array}{ll}
		\wp(\evo{x}{e}{B},Q) = &(\neg B \rightarrow Q)\,\wedge  \forall d>0.\,(\forall t\in[0,d).\,B[\vec{p}_{\vec{x}_0}(t)/\vec{x}])\wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \to  Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle/\tracev]
	\end{array}
	\]
	\oomit{
	\[
	\begin{array}{ll}
		&\wp(\external{i\in I}{ch_i*}{c_i}, R) = \\
		&\qquad (\forall i\in I.\, \textrm{if } ch_i*=ch!e \textrm{ then} \\
		&\qquad\qquad\qquad \wp(c_i, R)[\tracev^\chop \langle ch!,e\rangle/\tracev]\,\wedge \\
		&\qquad\qquad\qquad \forall d>0.\, \wp(c_i, R)[\tracev^\chop \langle d,I_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,e\rangle/\tracev] \\
		&\qquad\qquad\quad\ \textrm{elif } ch_i*=ch?x \textrm{ then} \\
		&\qquad\qquad\qquad \forall v.\, \wp(c_i, R)[v/x, \tracev^\chop \langle ch?,v\rangle/\tracev]\,\wedge \\
		&\qquad\qquad\qquad \forall d>0.\, \forall v.\, \wp(c_i, R)[v/x, \tracev^\chop \langle d,I_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch?,v\rangle/\tracev])\,\wedge \\
		&\qquad R[\tracev^\chop\langle \infty,I_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle/\tracev]
	\end{array}
	\]}
	\[
	\begin{array}{ll}
		&\wp(\exempt{\evo{s}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, R) = \\
		&\qquad (\forall i\in I.\, \textrm{if } ch_i*=ch!e \textrm{ then }   \wp(c_i, R)[\tracev^\chop \langle ch!,e\rangle/\tracev]\,\wedge \\
		&\qquad\qquad\qquad \forall d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \to \wp(c_i, R)[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,e[\vec{p}_{\vec{x}_0}(d)/\vec{x}]\rangle/\tracev] \\
		&\qquad\qquad\quad\ \textrm{elif } ch_i*=ch?x \textrm{ then }   \forall v.\, \wp(c_i, R)[v/x, \tracev^\chop \langle ch?,v\rangle /\tracev]\,\wedge \\
		&\qquad\qquad\qquad \forall d>0.\, (\forall t\in[0,d).\,
		B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \to \wp(c_i, R)[\vec{p}_{\vec{x}_0}(d)/\vec{x}, v/x, \tracev^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch?,v\rangle/\tracev])\, \wedge \\
		&\qquad (\neg B \rightarrow \wp(c,R))\, \wedge   \forall d>0.\,(\forall t\in[0,d).\,B[\vec{p}_{\vec{x}_0}(t)/\vec{x}])\wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \to  \wp(c,R)[\vec{p}_{\vec{x}_0}(d)/\vec{x}, \tracev^\chop \langle d, \vec{p}_{\vec{x}_0}, \rdy(\cup_{i\in I} ch_i*)\rangle/\tracev]
	\end{array}
	\]
	\caption{The weakest liberal preconditions for HCSP processes}
	\label{fig:wp}} 
\end{figure*}

The computation of weakest liberal precondition is given in Fig.~\ref{fig:wp}. Justification of this computation is given as part of the soundness proof in Appendix~\ref{app:soundness}. 

\oomit{
We first introduce a lemma stating that the weakest liberal precondition for repetition is able to deduce all valid facts for repetition according to our proof system.
\begin{lemma}
    If $\vDash \spec{p}{ c[\mu X.c/X]}{q}$, then $\spec{\wp(\mu X.c, q)}{\mu X. c}{q} \vdash \spec{p}{ c[\mu X.c/X]}{q}$.
    \label{lemma:recursion}
\end{lemma}
\begin{proof}
    The proof of this lemma can be seen from~\cite{DBLP:journals/toplas/Apt81}. 
\end{proof}
}

We present the proof for Lemma~\ref{lem:wp-complete} stating that the weakest liberal precondition for HCSP is derivable from our proof system.

\begin{proof}[Proof for Lemma~\ref{lem:wp-complete}]
	The proof is by induction on the structure of program \sm{c}. For most statements, the result follows directly by comparing the $\wp$-rule with the corresponding Hoare rule. We explain the more interesting cases in detail.
	
%	For the case of recursion, we wish to prove \sm{\vdash \spec{\wp(\mu X.c, Q)}{\mu X.c}{Q}}. According to Rule recursion, we can prove $\spec{\wp(\mu X.c, Q)}{\mu X.c}{Q} \vdash \spec{\wp(\mu X.c, Q)}{c[\mu X.c/X]}{Q}$ instead. By the soundness proof in Appendix~\ref{app:soundness}, \sm{\vDash \spec{\wp(\mu X.c, Q)}{\mu X.c}{Q}}, then obviously 
%	\[\vDash \spec{\wp(\mu X.c, Q)}{c[\mu X.c/X]}{Q}\]
%	holds according to the semantics of recursion. According to Lemma~\ref{lemma:recursion}, 
%	$\spec{\wp(\mu X.c, Q)}{\mu X.c}{Q} \vdash \spec{\wp(\mu X.c, Q)}{c[\mu X.c/X]}{Q}$ holds, and by applying Rule recursion, \sm{\vdash \spec{\wp(\mu X.c, Q)}{\mu X.c}{Q}} is proved.  
	
	For the case of repetition, we wish to prove \sm{\vdash \{\wp(c^*,Q)\}\ c^*\ \{Q\}}, given the inductive assumption \sm{\vdash \{\wp(c,Q')\}\ c\ \{Q'\}} for any \sm{Q'}. For this, we make use of the following property of \sm{\wp(c^*, Q)}:
	\[ \wp(c^*, Q) \rightarrow Q, \]
	which follows from the equation satisfied by \sm{\wp(c^*,Q)}. This allows us to reduce the goal to proving \sm{\vdash \{\wp(c^*,Q)\}\ c^*\ \{\wp(c^*,Q)\}}, and using the repetition rule, to proving \sm{\vdash \{\wp(c^*,Q)\}\ c\ \{\wp(c^*,Q) \}}.
	
	By the inductive assumption, we have \sm{\vdash \{\wp(c,\wp(c^*,Q))\}\ c\ \{\wp(c^*,Q)\}}. Hence, it suffices to show
	\[ \wp(c^*,Q) \rightarrow \wp(c,\wp(c^*,Q)), \]
	which also follows from the equation satisfied by \sm{\wp(c^*,Q)}.
	
	Next, we consider the case of interrupt. We need to show
	\[ \begin{array}{ll}
	\vdash \{\wp(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, R)\}  \\
	\qquad \exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}\\
	\quad \{R\} \end{array} \]
	given the inductive assumption that \sm{\vdash \{\wp(c_i,R)\}\ c_i\ \{R\}} for any index \sm{i}. Apply the rule Interrupt, with the indexed family of assertions \sm{Q_i} given by \sm{\wp(c_i,R)}. By the inductive hypothesis, each Hoare triple in the assumption is provable. Moreover, each entailment in the assumption holds by the definition of \sm{\wp(\exempt{\evo{x}{e}{B\propto c}}{i\in I}{ch_i*}{c_i}, R)}. This finishes the proof for the interrupt case.
\end{proof}
% that's all folks

 

\begin{proof}[Proof for Lemma~\ref{lem:sp-parallel-complete}]
	The proof is by induction on the structure of $c$. For the base case of sequential processes, this follows from Lemma~\ref{lem:wp-complete} and the definition of $\wp$. For the parallel composition of two processes, this follows from the rule (Par) and the computation of $\sp(c_1\|_{cs}c_2, P_1\uplus P_2)$.
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:continuous-complete}]
	For sequential case, if $\{P\}\ c\ \{Q\}$ is valid, $P\rightarrow \wp(c,P)$ holds, and by assumption provable. Combining with Lemma~\ref{lem:wp-complete}, we have $\{P\}\ c\ \{Q\}$ is provable.  For parallel case, if $\{P\}\ c\ \{Q\}$ is valid, then $\sp(c,P)\rightarrow Q$ holds, and by assumption provable. Combining with Lemma \ref{lem:sp-parallel-complete}, we have $\{P\}\ c\ \{Q\}$ is provable.
\end{proof}

\subsection{Proof of discrete completeness}

First, we state a version of Theorem~\ref{thm:globalerror} that is better suited for the proof below.
\begin{lemma}
	Let $\vec{p}(t)$ be a solution to the initial value problem $\vec{\dot{x}}=\vec{e}$, $\vec{p}(0)=\vec{x}_0$ on the time interval $[0,T]$, and let $L$ be the Lipshitz constant as before. Given any $\epsilon>0$, there exists $h_0>0$ such that for all $0<h<h_0$, the difference between the actual solution $\vec{p}$ and its continuous approximation $\vec{f}_h$ is at most $\epsilon$ on the interval $[0,T]$.
	\label{lem:globalerror}
\end{lemma}
\begin{proof}
	First, from Theorem~\ref{thm:globalerror}, take $h_1$ such that for all $0<h<h_1$ and all $n$ with $nh\le T$, we get
	\begin{equation*}
		\|\vec{p}(nh) - \vec{x}_{n}\| \leq \frac{h}{2}\max_{\theta \in [0, T]} \left\| \frac{d^2 \vec{p}}{dt^2}(\theta) \right\|\fracN{e^{LT} - 1}{L} < \frac{\epsilon}{3}.
	\end{equation*}
	This bounds the difference between the actual solution and the discrete approximation. For the continuous approximation, we further need to consider the intermediate points between $nh$ and $(n+1)h$. Hence, take $t\in(nh, (n+1)h)$, by the mean value theorem, there is a $\theta\in(nh,t)$ such that
	\[ \| \vec{p}(t) - \vec{p}(nh)\| = (t-nh)\cdot \left\|\frac{d \vec{p}}{d t}(\theta)\right\|\]
	Since $\|\frac{d \vec{p}}{d t}(\theta)\|$ is bounded along the path $\vec{p}$, we can take $h_2$ sufficiently small so that for any $0<h<h_2$, $\|\vec{p}(t)-\vec{p}(nh)\|$ is bounded above by $\frac{\epsilon}{3}$. Likewise, since
	\[ \|\vec{f}_h(t) - \vec{x}_n \| = \|\vec{f}_h(t) - \vec{f}_h(nh) \| = (t-nh)\cdot \left\|\frac{d\vec{p}}{d t}(nh)\right\|, \]
    we have any $0<h<h_2$, also $\|\vec{f}_h(t)-\vec{x}_n \|$ is bounded above by $\frac{\epsilon}{3}$. Take $h_0=\min(h_1,h_2)$ and combining, we have for any $0<h<h_0$,
	\[ 
	\begin{array}{lll}
		 &\|\vec{p}(t) - \vec{f}_h(t)\| \\
		&\qquad\le\|\vec{p}(t) - \vec{p}(nh)\| + \|\vec{p}(nh) - \vec{x}_n\| +
		\|\vec{f}_h(t)-\vec{x}_n\| \\
		 &\qquad < \epsilon, 
	\end{array}\]
	as desired.
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:CP-DP}]
(CP) $\rightarrow$ (DP): Assume (CP) is true in state $s$ and trace $\textit{tr}$. This means for any $d>0$, suppose $\forall t\in[0,d).\,  B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ and $\neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}]$ hold, where  $\vec{p}_{\vec{x}_0}$ is the unique solution of $\vec{\dot{s}}=\vec{e}$ with the initial value $\vec{x}_0 = s(\vec{x})$, then\\
$\nseman{Q}{s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(d)]}{\textit{tr}^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle}$ also holds. For ease of presentation, we will abbreviate the latter to  $Q(\vec{p}_{\vec{x}_0}(d),\textit{tr}^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle)$ below.
%, where $\vec{p}_{\vec{x}_0}$ is the unique solution of $\vec{\dot{s}}=\vec{e}$ with the initial value $s(\vec{x})$.

Fix $T>0$ in (DP), there are three cases depending on whether the assumptions $\forall t\in[0,T).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ and $\neg B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ hold or not.

If both assumptions hold, then we get $Q(\vec{p}_{\vec{x}_0}(d),\textit{tr}^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle)$ holds.  From the assumption that $Q$ is open, we can take $\epsilon_1>0$ such that $Q(s',\textit{tr}')$ for all $(s',\textit{tr}')\in \mathcal{U}_{\epsilon_1}(\vec{p}_{\vec{x}_0}(d), \textit{tr}^\chop \langle d,\vec{p}_{\vec{x}_0},\emptyset\rangle)$.

Take $\epsilon_0=\frac{\epsilon_1}{2}$. Then, by Lemma~\ref{lem:globalerror}, there exists $h_0>0$ such that for all $0<h<h_0$, the distance between $\vec{p}_{\vec{x}_0}(t)$ and $\vec{f}_{\vec{x}_0,h}(t)$ is bounded above by $\epsilon_0$ along the interval $t\in[0,T]$. With this choice of $\epsilon_0$ and $h_0$ in the conclusion of (DP), we get for any $\epsilon<\epsilon_0$ and $h<h_0$, the distance between $(\vec{f}_{\vec{x}_0,h}(T),\textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\emptyset\rangle)$ and $(\vec{p}_{\vec{x}_0}(T), \textit{tr}^\chop\langle d,\vec{p}_{\vec{x}_0},\emptyset\rangle)$ is at most $\epsilon_0$. Then, any pair within distance $\epsilon$ of $(\vec{f}_{\vec{x}_0,h}(T),\textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\emptyset\rangle)$ is within distance $\epsilon+\epsilon_0<\epsilon_1$ of $(\vec{p}_{\vec{x}_0}(T), \textit{tr}^\chop\langle d,\vec{p}_{\vec{x}_0},\emptyset\rangle)$, and hence satisfy $Q$. This shows
\[(\vec{f}_{\vec{x}_0,h}(T),\textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\emptyset\rangle)\in\mathcal{U}_{-\epsilon}(Q)\] as desired.

Now, we consider the case where $\forall t\in[0,T).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ does not hold. Intuitively, this corresponds to the case where $T$ is greater than the time length of execution. Choose $t\in[0,T)$ such that $\neg B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$. We claim that the first assumption in (DP) fails for this value of $t$. That is,
\[ 
\begin{array}{l}
	\neg (\exists \epsilon_0>0.\, \forall 0<\epsilon<\epsilon_0.\, \exists h_0>0.\, \forall 0<h<h_0.\, \\
	\qquad \vec{f}_{\vec{x}_0,h}(t)\in \mathcal{U}_{-\epsilon}(B))
\end{array} \]
Suppose otherwise, then take $\epsilon_0$, $\epsilon<\epsilon_0$ and $h_0$ so that for all $0<h<h_0$ the condition $\vec{f}_{\vec{x}_0,h}(t)\in\mathcal{U}_{-\epsilon}(B)$ holds. Take $h$ sufficiently small such that the difference between $\vec{f}_{\vec{x}_0,h}(t)$ and $\vec{p}_{\vec{x}_0}(t)$ is bounded above by $\epsilon$ for all $t\in[0,T)$. Then $\vec{p}_{\vec{x}_0}(t)\in \mathcal{U}_{\epsilon}(\vec{f}_{\vec{x}_0,h}(t))$, so that $\neg B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ and $\vec{f}_{\vec{x}_0,h}(t)\in\mathcal{U}_{-\epsilon}(B)$ together gives a deadlock.

Finally, we consider the case where $\neg B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ does not hold, in other words $\vec{p}_{\vec{x}_0}(T)$ satisfies $B$. Intuitively, this corresponds to the case where $T$ is less than the time length of execution. We claim that for sufficiently small $\epsilon$ and $h$, the condition $\vec{f}_{\vec{x}_0,h}(T)\in \neg\mathcal{U}_{-\epsilon}(B)$ is false, that is $\vec{f}_{\vec{x}_0,h}(T)\in\mathcal{U}_{-\epsilon}(B)$, hence the implication is vacuously true. Since $\vec{p}_{\vec{x}_0}(T)$ satisfies $B$ and $B$ is open, we can take $\epsilon_1>0$ so that $\mathcal{U}_{\epsilon_1}(\vec{p}_{\vec{x}_0}(T))\subseteq B$. Take $\epsilon_0=\frac{\epsilon_1}{2}$, and take $h_0$ so that for any $0<h<h_0$, we have the distance between $\vec{p}_{\vec{x}_0}(T)$ and $\vec{f}_{\vec{x}_0,h}(T)$ is less than $\epsilon_0$. With this choice of $\epsilon_0$ and $h_0$, for any $\epsilon<\epsilon_0$ and $h<h_0$, we know that any state within $\epsilon$ of $\vec{f}_{\vec{x}_0,h}(T)$ is within $\epsilon+\epsilon_0<\epsilon_1$ of $\vec{p}_{\vec{x}_0}(T)$, and hence satisfy $B$. This shows $\vec{f}_{\vec{x}_0,h}(T)\in\mathcal{U}_{-\epsilon}(B)$ as required.

We have now examined all three cases of $T>0$, and so have derived (DP) from (CP).

%The second case is that, $ (\forall t\in[0,T).\,  B(p(t)) \wedge \neg B(p(T))$ does not hold. Then there must exist $t_0 < T$ such that $(\forall t\in[0,t_0).\,  B(p(t)) \wedge \neg B(p(t_0))$ or  $ (\forall t\in[0,T].\,  B(p(t))$ holds. Suppose $(\forall t\in[0,t_0).\,  B(p(t)) \wedge \neg B(p(t_0))$ holds. For any $\epsilon > 0$, according to inequality $(E)$, there must exist $h_0$ such that for any $0<h<h_0$, $\|p(t_0) - f(t_0)\| < \frac{\epsilon}{2}$, i.e. $p(t_0) \in \mathcal{U}_{\frac{\epsilon}{2}}(f(t_0))$. Therefore $f(t_0) \notin \mathcal{U}_{-\epsilon}(B)$ holds from the fact $\neg B(p(t_0))$, and thus $(DP)$ holds from the fact that the premise $(\forall 0\leq t< T.\, f(t)\in \mathcal{U}_{-\epsilon}(B))$ is false. On the other hand, suppose $ (\forall t\in[0,T].\,  B(p(t))$, then there is a $\epsilon_1>0$ such that 
%$\epsilon_1 =  d(p(T), \neg B)$ as $B$ is open. Let $\epsilon_0$ be $\frac{\epsilon_1}{2}$, according to the same arithmetic reasoning, for any $\epsilon < \epsilon_0$, there must exist $h_0$ such that for any $h<h_0$, $f(T) \in \mathcal{U}_{\epsilon}(p(T))$, then $f(T) \in \mathcal{U}_{-\epsilon}(B))$. Thus $(DP)$ holds from the fact that the premise $ (f(T) \in \neg \mathcal{U}_{-\epsilon}(B))$ is false.


%
%The fact $\forall 0\leq t< T.\, f(t)   \in \mathcal{U}_{-\epsilon}(B)$ can be proved similarly. The last is to prove $f(T) \in \neg \mathcal{U}_{-\epsilon}(B) $. From the assumption $\neg B(p(d))$, we have $p(d) \in \neg B$. Fix $\epsilon < \epsilon_0$, then according to the above inequality $(E)$,  
%there exists sufficiently small $h_4$ (with $h_4 <  h_3$) such that $\|f(d) -p(d)\| < \epsilon$, then $f(d) \in \mathcal{U}_{\epsilon}(\neg B)$, plus the fact $\mathcal{U}_{\epsilon}(\neg B) \subseteq \neg \mathcal{U}_{-\epsilon}(B) $, then $f(d) \in \neg \mathcal{U}_{-\epsilon}(B)$ must hold. 

(DP) $\rightarrow$ (CP): Assume (DP) is true in state $s$ and trace $\textit{tr}$. We need to show that (CP) holds in state $s$ and trace $\textit{tr}$. That is, given $d>0$ and a solution $\vec{p}_{\vec{x}_0}$ satisfying $\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ and $\neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}]$, we need to prove $Q(\vec{p}_{\vec{x}_0}(d), \textit{tr}^\chop \langle d, \vec{p}_{\vec{x}_0}, \emptyset\rangle)$.

From (DP), take $T=d$. First, we show that the first assumption in (DP) holds, that is:
\[ 
\begin{array}{l}
	\forall 0\le t<T.\, \exists \epsilon_0>0.\, \forall 0<\epsilon<\epsilon_0.\, \exists h_0>0.\, \\
	\qquad\forall 0<h<h_0.\, \vec{f}_{\vec{x}_0,h}(t)\in \mathcal{U}_{-\epsilon}(B).
\end{array} \]
Fix some $t\in[0,T)$. Then we have $B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ holds, and since $B$ is open, we can take $\epsilon_1$ such that $\mathcal{U}_{\epsilon_1}(\vec{p}_{\vec{x}_0}(t))\subseteq B$. Then take $\epsilon_0=\frac{\epsilon_1}{2}$, and take $h_0$ such that for all $0<h<h_0$, the distance between $\vec{p}_{\vec{x}_0}(t)$ and $\vec{f}_{\vec{x}_0,h}(t)$ is bounded by $\epsilon_0$. Then for any $\epsilon<\epsilon_0$, we know that any state within $\epsilon$ of $\vec{f}_{\vec{x}_0,h}(t)$ is within $\epsilon+\epsilon_0<\epsilon_1$ of $\vec{p}_{\vec{x}_0}(t)$, and hence in $B$. This shows $\vec{f}_{\vec{x}_0,h}(t)\in\mathcal{U}_{-\epsilon}(B)$, as desired. This proves the first assumption in (DP) holds.

Next, we show that for any sufficiently small $\epsilon$, there exists $h_0$ such that for any $0<h<h_0$, the condition $\vec{f}_{\vec{x}_0,h}(T)\in\neg\mathcal{U}_{-\epsilon}(B)$ holds. Given $\epsilon>0$, take $h_0$ such that for any $0<h<h_0$, the distance between $\vec{p}_{\vec{x}_0}(T)$ and $\vec{f}_{\vec{x}_0,h}(T)$ is bounded by $\epsilon$. But from $\neg B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ this implies $\vec{f}_{\vec{x}_0,h}(T)\in\neg\mathcal{U}_{-\epsilon}(B)$ as desired.

From this, we have shown that for any sufficiently small $\epsilon$, there exists $h_0>0$ such that for any $0<h<h_0$, the condition $(\vec{f}_{\vec{x}_0,h}(T),$ $\textit{tr}^\chop \langle T, \vec{f}_{\vec{x}_0,h}, \emptyset \rangle) \in \mathcal{U}_{-\epsilon}(Q)$ holds. Take such $\epsilon_0$ and $\epsilon<\epsilon_0$. Then take $h_1$ such that the distance between $\vec{p}_{\vec{x}_0}(t)$ and $\vec{f}_{\vec{x}_0,h}(t)$ is bounded above by $\epsilon$ along the interval $t\in[0,T]$. Then for $h<\min(h_0,h_1)$, we get that $(\vec{p}_{\vec{x}_0}(T), \textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\emptyset\rangle)$ is within distance $\epsilon$ of $(\vec{f}_{\vec{x}_0,h}(T), $ $\textit{tr}^\chop \langle T, \vec{f}_{\vec{x}_0,h}, \emptyset \rangle)$, and the latter belongs to $\mathcal{U}_{-\epsilon}(Q)$. This implies $(\vec{p}_{\vec{x}_0}(T), \textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\emptyset\rangle)$ satisfies $Q$, as desired. This finishes the proof of (CP) from (DP). 
%From  $(\forall 0\leq t< T.\, f(t)   \in \mathcal{U}_{-\epsilon}(B) \wedge f(T) \in \neg \mathcal{U}_{-\epsilon}(B))$ and $(DP)$, we have the fact $(f(T), \textit{tr}^\chop \langle T, f, \emptyset \rangle)   \in  \mathcal{U}_{-\epsilon}(Q)$.
%there exists  $T$ such that the continuous approximation of $p$ over  $[0, T]$ satisfies $(f(T), \textit{tr}^\chop \langle T, f, \emptyset \rangle)   \in  \mathcal{U}_{-\epsilon}(Q) \wedge \forall 0\leq t< T.\, f(t)   \in \mathcal{U}_{-\epsilon}(B) \wedge f(T) \in \neg \mathcal{U}_{-\epsilon}(B)$. We show that there is a sequence of $f_h(t)$ and $T_h$ that converge to $p(t)$ and $d$ as $h \rightarrow 0$. First assume the sequence of $T_h$ does not converge to $d$, then by contradiction, there must exist $\delta>0$ such that for all $h>0$, $|T_h-d| \geq \delta$. If there is an infinite sequence of $T_h > d$, then consider $f_h(d)$. Then $f_h(d) \in \mathcal{U}_{-\epsilon}(B)$, which means $\mathcal{U}_{\epsilon}(f_h(d)) \in B$. On the other hand, according to the inequality $(E)$, $\|f_h(d) - p(d)\| \rightarrow 0$ when $h \rightarrow 0$. As a result, we derive the inconsistent fact that $p(d) \in  B$. If there is an infinite sequence of $T_h < d$, then consider $p(T_h)$. For any $h$, $p(T_h) \in B$. Plus the fact that $B$ is open, there must exist $\epsilon_1$ such that $\mathcal{U}_{\epsilon_1}(p(T_h)) \subseteq B$. When $h$ is sufficiently small, $\|f_h(T_h) - p(T_h)\| < \frac{\epsilon_1}{2}$. This is contradict to the fact that $f_h(T_h) \in \neg \mathcal{U}_{-\frac{\epsilon_1}{2}}(B) $. Thus $T_h$ must converge to $d$. 
%
%When $T_h$  converges to $d$, 
%Let  $\epsilon \rightarrow 0$, then there exists $h \rightarrow 0$, such that the corresponding sequence of  $f_h(t)$ converges to $p(t)$. This is obtained from the 
%inequality $(E)$ directly. As a special case, $f_h(T)$ converges to 
%$p(T)$. From the fact $(f(T), \textit{tr}^\chop \langle T, f_h, \emptyset \rangle)   \in \mathcal{U}_{-\epsilon}(Q)$ for sufficiently small $h$, the limit $(p(T), \textit{tr}^\chop \langle T, p, \emptyset\rangle) \in \overline{\mathcal{U}_{-\epsilon}(Q)} \subseteq Q$. 
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:CI-DI}]
 (CI) $\rightarrow$ (DI): Assume (CI) is true in state $s$ and trace $\textit{tr}$. Fix $T>0$ in (DI), then there are two cases, depending on whether $\forall t\in[0,T).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ holds or not.

If it holds, then from (CI), with $d=T$, we get

\vspace{-3mm}
{\small
\[ Q(\vec{p}_{\vec{x}_0}(d), \textit{tr}^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(d)](e)\rangle)\]}

\vspace{-2mm}\noindent holds. From the assumption that $Q$ is open, we take $\epsilon_1>0$ such that $Q(s',\textit{tr}')$ for all
$(s',tr')$ in the set

\vspace{-3mm}
{\small
\[ \mathcal{U}_{\epsilon_1}(\vec{p}_{\vec{x}_0}(d), \textit{tr}^\chop\langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(d)](e)\rangle).\]}

\vspace{-2mm}\noindent Since $e$ is expressed in terms of arithmetic operations, it is a continuous function of its argument. Moreover, since the path $\vec{p}_{\vec{x}_0}$ on the interval $[0,T]$ is compact, we get that $e$ is uniformly continuous on a closed neighborhood of the path. Hence, we can take $\epsilon_0>0$ such that $\epsilon_0<\frac{\epsilon_1}{2}$, and $\|\vec{y}_0-\vec{y}_1\|_\infty<\epsilon_0$ implies $|e(\vec{y_0})-e(\vec{y_1})|<\frac{\epsilon_1}{2}$ on the $\epsilon_0$-neighborhood of the path $\vec{p}_{\vec{x}_0}([0,T])$.

For this choice of $\epsilon_0$, there exists $h_0>0$ such that for all $0<h<h_0$, the distance between $\vec{p}_{\vec{x}_0}(t)$ and $\vec{f}_{\vec{x}_0,h}(t)$ is bounded above by $\epsilon_0$ along the interval $t\in[0,T]$. With this choice of $\epsilon_0$ and $h_0$ in the conclusion of (DI), we get for any $\epsilon<\epsilon_0$ and $h<h_0$, the distance between

\vspace{-3mm}
{\small
\[ (\vec{f}_{\vec{x}_0,h}(T), \textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{f}_{\vec{x}_0,h}(T)](e)\rangle) \]}

\vspace{-2mm}\noindent and

\vspace{-3mm}
{\small
\[ (\vec{p}_{\vec{x}_0}(T), \textit{tr}^\chop\langle T,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(T)](e)\rangle) \]}

\vspace{-2mm}\noindent is at most $\frac{\epsilon_1}{2}$. This shows
{\small
\[ 
\begin{array}{ll}
     (\vec{f}_{\vec{x}_0,h}(T), \textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{f}_{\vec{x}_0,h}(T)](e)\rangle)
     \\
     \qquad\qquad\in \mathcal{U}_{-\epsilon}(Q) 
\end{array}\]
}
by the same argument as in the continuous evolution case.

Now suppose the condition $\forall t\in[0,T).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$ does not hold. Intuitively, this corresponds to the case where $T$ is greater than the maximum possible length of execution of the interrupt command. Choose $t\in [0,T)$ such that $\neg B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$. Then the first assumption of (DI) fails for this value of $t$, as shown in the proof of the continuous evolution case. This means (DI) holds vacuously.


{\small
\begin{figure*}[ht!]
\begin{eqnarray*}
&
\prftree{}{\mathsf{sync}(chs,\mathsf{init},\mathsf{init})\Longrightarrow \mathsf{init}}
\qquad
\prftree{ch\in chs}{\mathsf{sync}(chs,\mathsf{init},\mathsf{wait\_in}(I,ch,P))\Longrightarrow \mathsf{false}}
\qquad
\prftree{ch\in chs}{\mathsf{sync}(chs,\mathsf{init},\mathsf{wait\_out}(I,ch,P))\Longrightarrow \mathsf{false}} & \\[1mm]
& 
\prftree{ch\in chs}
{\begin{array}{c}
\mathsf{sync}(chs,\mathsf{wait\_in}(I_1,ch,P),\mathsf{wait\_out}(I_2,ch,Q))(s_0) \Longrightarrow  \exists v.\, \mathsf{sync}(chs,pns_1,pns_2,P(0,v),Q(0,v))(s_0)
\end{array}}
& \\[1mm]
 	& 
  \prftree{ch\in chs}{
	\begin{array}{l}
		\mathsf{sync}(chs,\mathsf{wait}(I_1,e,P),\mathsf{wait\_in}(I_2,ch,Q)) \Longrightarrow 
		\mathsf{wait}(I_1\uplus I_2,e,\{d\Rightarrow sync(chs,P(d),\mathsf{wait\_in}(\mathsf{delay\_inv}(d,I_2),ch,\{(d_1,v_1)\Rightarrow Q(d1+d,v_1)\}))\})
	\end{array}}
& \\[1mm]
&
\prftree{ch_1\notin chs}{ch_2\in chs}
{\begin{array}{c}\mathsf{sync}(chs,\mathsf{wait\_in}(I_1,ch_1,P),
\mathsf{wait\_out}(I_2,ch_2,Q))(s_0) \Longrightarrow \\
\mathsf{wait\_in}(I_1\uplus I_2,ch_1,\{(d,v)\Rightarrow
\mathsf{sync}(chs,P(d,v), 
\mathsf{wait\_out}(\mathsf{delay\_inv}(d,I_2),ch_2,\{(d_2,v_2)\Rightarrow Q(d_2+d,v_2)\}))(s_0)
\end{array}}
& \\[1mm]
&
\prftree[r]{Intinf\_Intinf1}{\compat(\rdy(\specs_1),\rdy(\specs_2))}
{
\begin{array}{c}
\mathsf{sync}(chs,\mathsf{interrupt_\infty}(I_1,\specs_1),\mathsf{interrupt_\infty}(I_2,\specs_2))\Longrightarrow \\
\mathsf{interrupt_\infty}(I_1\uplus I_2,\mathsf{rel1}(\specs1|_{chs^{c}},\mathsf{interrupt_\infty}(I_2,\specs_2)) @ \mathsf{rel2}(\specs2|_{chs^{c}},\mathsf{interrupt_\infty}(I_1,\specs_1)))
\end{array}}
& \\[1mm]
&
\prftree[r]{Intinf\_Intinf2}{\neg\compat(\rdy(\specs_1),\rdy(\specs_2))}
{
\begin{array}{c}
\mathsf{sync}(chs,\mathsf{interrupt_\infty}(I_1,\specs_1),\mathsf{interrupt_\infty}(I_2,\specs_2))\Longrightarrow \\
\mathsf{interrupt}(I_1\uplus I_2,0,\mathsf{comm}(\specs1,\specs2),\mathsf{rel1}(\specs1|_{chs^{c}},\mathsf{interrupt_\infty}(I_2,\specs_2)) @ \mathsf{rel2}(\specs2|_{chs^{c}},\mathsf{interrupt_\infty}(I_1,\specs_1)))
\end{array}}
&\\[1mm]
&
\prftree[r]{Int\_Int1}{\compat(\rdy(\specs_1),\rdy(\specs_2))}
{
\begin{array}{c}
\mathsf{sync}(chs,\mathsf{interrupt}(I_1,e_1,P\specs_1),\mathsf{interrupt}(I_2,e_2,Q,\specs_2))\Longrightarrow \mathsf{interrupt}(I_1\uplus I_2, \min(e_1,e_2),\\
\{d\Rightarrow\mathsf{if}\  e_1>e_2 \land e_1>0 \ \mathsf{then}\  \mathsf{sync}(chs,\mathsf{interrupt}(\mathsf{delay\_inv}(d,I_1),e_1-d,\{d'\Rightarrow P(d+d')\},\mathsf{delay\_specs}(d,\specs1)),Q(d))\\
\qquad
\mathsf{elif}\ e_1<e_2 \land e_2>0 \ \mathsf{then}\ 
\mathsf{sync}(chs,P(d),\mathsf{interrupt}(\mathsf{delay\_inv}(d,I_2),e_2-d,\{d'\Rightarrow Q(d+d')\},\mathsf{delay\_specs}(d,\specs2)))\\
\qquad
\mathsf{else}\ \mathsf{sync}(chs,\mathsf{interrupt}(\mathsf{delay\_inv}(d,I_1),e_1-d,\{d'\Rightarrow P(d+d')\},\mathsf{delay\_specs}(d,\specs1)),Q(d)) \\
\qquad \quad \lor \mathsf{sync}(chs,P(d),\mathsf{interrupt}(\mathsf{delay\_inv}(d,I_2),e_2-d,\{d'\Rightarrow Q(d+d')\},\mathsf{delay\_specs}(d,\specs2)))
\}
\\
\mathsf{rel1}(\specs1|_{chs^{c}},\mathsf{interrupt}(I_2,e_2,Q,\specs_2)) @ \mathsf{rel2}(\specs2|_{chs^{c}},\mathsf{interrupt}(I_1,e_1,P,\specs_1)))
\end{array}}
&\\[1mm]
&
\prftree[r]{Int\_Int2}{\neg\compat(\rdy(\specs_1),\rdy(\specs_2))}
{
\begin{array}{c}
\mathsf{sync}(chs,\mathsf{interrupt}(I_1,e_1,P,\specs_1),\mathsf{interrupt}(I_2,e_2,Q,\specs_2))\Longrightarrow \\
\mathsf{interrupt}(I_1\uplus I_2,0,\\
\mathsf{if} \ e_1>0 \land e_2>0\  \mathsf{then}\ \mathsf{comm}(\specs1,\specs2)\\
\mathsf{elif} \ e_1>0 \land e_2<0\  \mathsf{then}\ \mathsf{comm}(\specs1,\specs2) \lor \mathsf{sync}(chs,\mathsf{interrupt}(I_1,e_1,P,\specs_1),Q(0))\\
\mathsf{elif} \ e_1<0 \land e_2>0\  \mathsf{then}\ \mathsf{comm}(\specs1,\specs2) \lor \mathsf{sync}(chs,P(0),\mathsf{interrupt}(I_2,e_2,Q,\specs_2))\\
\mathsf{else}\ \mathsf{comm}(\specs1,\specs2) \lor \mathsf{sync}(chs,\mathsf{interrupt}(I_1,e_1,P,\specs_1),Q(0)) \lor\mathsf{sync}(chs,P(0),\mathsf{interrupt}(I_2,e_2,Q,\specs_2)),\\
\mathsf{rel1}(\specs1|_{chs^{c}},\mathsf{interrupt}(I_2,e_2,Q,\specs_2)) @ \mathsf{rel2}(\specs2|_{chs^{c}},\mathsf{interrupt}(I_1,e_1,P,\specs_1)))
\end{array}}
&
\end{eqnarray*}
\caption{Elimination rules for simplified assertions}
\label{fig:elimination_rules}
\end{figure*}
}


We have now considered both cases of $T>0$, and so have derived (DI) from (CI).

(DI) $\rightarrow$ (CI): Assume (DI) is true in state $s$ and trace $\textit{tr}$. We need to show that (CI) also holds. That is, given $d>0$ and a solution $\vec{p}_{\vec{x}_0}$ satisfying $\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]$, we need to prove 
\sm{Q(\vec{p}_{\vec{x}_0}(d), \textit{tr}^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(d)](e)\rangle)}.

From (DI), take $T=d$. The assumption in (DI) holds by the same argument as in the continuous evolution case. Therefore, for any sufficiently small $\epsilon$, there exists $h_0>0$ such that for any $0<h<h_0$, the condition

\vspace{-3mm}
{\small
\[ \begin{array}{l}
(\vec{f}_{\vec{x}_0,h}(T), \textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{f}_{\vec{x}_0,h}(T)](e)\rangle) \\
\quad \quad  \in \mathcal{U}_{-\epsilon}(Q) 
\end{array} \]
}

\vspace{-3mm}\noindent holds. Take such $\epsilon_0$ and $\epsilon<\epsilon_0$. Next, take $\epsilon'$ sufficiently small so that $\|\vec{y_0}-\vec{y_1}\|_\infty < \epsilon'$ implies $|e(\vec{y_0})-e(\vec{y_1})|<\epsilon$ on the $\epsilon'$-neighborhood of the path $\vec{p}_x([0,T])$, and take $h_1$ such that the distance between $\vec{p}_{\vec{x}_0}(t)$ and $\vec{f}_{\vec{x}_0,h}(t)$ is bounded above by $\epsilon'$ along the interval $t\in[0,T]$. Then for $h<\min(h_0,h_1)$, we get that \sm{(\vec{p}_{\vec{x}_0}(d), \textit{tr}^\chop \langle d,\vec{p}_{\vec{x}_0},\rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,s[\vec{x} \mapsto \vec{p}_{\vec{x}_0}(d)](e)\rangle)} is within distance $\epsilon$ of 
$(\vec{f}_{\vec{x}_0,h}(T), \textit{tr}^\chop\langle T,\vec{f}_{\vec{x}_0,h}, \rdy(\cup_{i\in I} ch_i*)\rangle^\chop \langle ch!,$
$s[\vec{x} \mapsto \vec{f}_{\vec{x}_0,h}(T)](e) \rangle)$. Since the latter belongs to $\mathcal{U}_{-\epsilon}(Q)$, this implies the former satisfies $Q$, as desired. This finishes the proof of (CI) from (DI).   
\end{proof}



\begin{proof}[Proof for Theorem~\ref{thm:dicreteRC}]
 The hybrid Hoare logic presented in Sect.\ref{sec:hoare} inherits the continuous completeness relative to the first-order theory of differential equations (i.e. FOD). All
that remains to be shown is that we can then prove all those
valid FOD formulas from valid formulas of discrete fragment plus the added formulas $CP \leftrightarrow DP$ and $CI \leftrightarrow DI$. The only question that remains to consider is that,  the restriction that we put when proving $CP \leftrightarrow DP$ and $CI \leftrightarrow DI$: the predicates occurring in the precondition of ODEs (i.e. $Q_1$ and $Q_2$ mentioned in the above proofs) are open, should be removed.

Without loss of generality, we assume all predicates are  first-order formulas of real arithmetic, which can be reduced to the equivalent formula of the following form (denoted by $S$):
\[\bigwedge^m_{i=1}(\bigvee_{j=1}^{l} p_{i,j} > 0\vee \bigvee_{k=1}^{n} q_{i, k} \geq 0)\]

Clearly, $\vee_{j=1}^{l} p_{i,j} > 0$  corresponds to an open basic semi-algebraic set, say $O_i$,  and $\vee_{k=1}^{n} q_{i, k} \geq 0$  corresponds to a closed basic semi-algebraic set, say $C_i$. Being a closed set, $C_i$
is equivalent to $\forall \epsilon_i>0. \, \mathcal{U}_{\epsilon_i}(C_i)$
so $S$ can be reformulated by
\[\bigwedge_{i=1}^{m} \forall \epsilon_i>0.(D_i \vee  \mathcal{U}_{\epsilon_i}(C_i))\] 

For each $D_i \vee  \mathcal{U}_{\epsilon_i}(C_i)$, denoted by $\Phi_i$, it is an open set, or say open predicate. Using the two formulas $CP \leftrightarrow DP$ and $CI \leftrightarrow DI$, 
$\Phi_i$ can be equivalently represented as a discrete formula, denoted by $\mathcal{D}(\Phi_i)$. By $m$ uses of the two formulas, $S$ can be represented by an open predicate, and thus can be derived with respect to the discrete fragment of the logic.  
\end{proof} 

\oomit{
\begin{proof}[Proof for Theorem~\ref{thm:relativeRC-DF}]
 We only need to prove the counter part is equivalent to the one for CSP given in \cite{Apt83}. 
 
 For the direction to prove 
  Apt's proof system's derivability in our proof system, above of all, 
  rule Communication is derivable directly from rule SyncPairE, and rule Formation can be 
  derived from the synchronization rules in our proof system. 
  Regarding rule Parallel, after introducing $tr_1$ for $c_1$, \sm{tr_2} for \sm{c_2} and 
  \sm{tr} for \sm{c_1 \|_{cs} c_2}, according to the premise of 
     rule Paralle, we have \sm{\vdash \{P_1\} \, c_1 \{Q_1\}}, 
     \sm{\vdash \{P_2\} \, c_2 \{Q_2\}}, and the proofs for \sm{\vdash \{P_1\} \, c_1 \{Q_1\}} and  
     \sm{\vdash \{P_2\} \, c_2 \{Q_2\}} cooperative, because 
     \smas \sm{c_1} and \sm{c_2} do not share any variable except for introduced system variables. 
      On the other hand, 
  
  it is easy to prove that in our proof system it can be guaranteed that 
 all decomposed Hoare triples $\{A_i\}{ P_i } \{C_i\}$ are interference-free, because any two of $P_i$ do not share any 
 variables except for the introduced system variables, and similar to the proofs given in \cite{Apt83,Owicki76} it can be 
 shown that any two of $\{A_i\}{ P_i } \{C_i\}$ are interference-free using the introduced variable $\textit{\textit{tr}}$ and $\textit{\textit{tr}}'$. So, the rule $\textbf{Par}$ is derivable by applying the rule \textbf{Parallel} and \textbf{Auxiliary} in . 
 The rest axioms and rules are same as in ours, they are derivable in our proof system obviously. 
 
 For the inverse direction, similar to the proofs given in \cite{Apt83,Owicki76}, after introducing auxiliary variables 
 $\textit{\textit{tr}}$, $\textit{\textit{tr}}'$, $t$ and $t'$ (in  \cite{Apt83,Owicki76} ($t$ and $t'$ correspond to 
  $\textit{clock}$ and $\textit{clock}'$, respectively.), all decomposed Hoare triples in the premise of the rule \textbf{Par} in our proof system are valid, and by \textbf{Parallel} and \textbf{Auxiliary Variables} in 
  Apt's proof system, 
  the \textbf{Par} rule in our proof system is provable. The rest axioms and rules are derivable in Apt's proof system  obviously. 
  This completes the proof. 
\end{proof}}

\oomit{
{\small
\begin{figure*}
	\[ \prftree[r]{Output}{
		\spec{\wand{\textsf{out}(\vec{x}_0,ch,e)}{Q}}{ch!e}{Q}}  \quad \prftree[r]{Input}{
		\spec{\forall v.\, \wand{\textsf{in}(\vec{x}_0,ch,v)}{Q[v/x]}}{ch?x}{Q}} \]
%	\[ \prftree[r]{Wait}{
%		\spec{\wand{\textsf{wait}(\vec{x}_0,d)}{Q}}{\pwait\,d}{Q}} 
	\[ \prftree[r]{Cont}{
	\spec{\forall \vec{x}_2.\, \wand{\textsf{ode}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2)}{Q[\vec{x}_2/\vec{x}]}}
	{\evo{x}{e}{B}}{Q}} \]


%\[\prftree[r]{Cont}{
% \begin{array}{ll}
%\{\forall d>0.\, \forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}] \wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \to \\
%\qquad \wand{\textsf{path}(d,\vec{p}_{\vec{x}_0})}{Q[\vec{p}_{\vec{x}_0}(d)/\vec{x}]}\}
%\evo{x}{e}{B}\{Q\} \end{array}}
%\]

	%\[ \prftree[r]{EChoice}{
	%	\begin{array}{llll}
	%		\forall i\in I.\,\textrm{if } io_i=ch!e \textrm{ then} \\
	%		\qquad\qquad \exists Q.\, \spec{Q}{c_i}{R} \wedge P \Rightarrow
	%		\wand{\textsf{out}(\vec{x},ch,e(\vec{x}),\rdy(\cup_{i\in I} ch_i*))}{Q} \\
	%		\qquad\quad \textrm{elif } io_i=ch?x \textrm{ then} \\
	%		\qquad\qquad \exists Q.\, \spec{Q}{c_i}{R} \wedge P \Rightarrow \forall v.\, \wand{\textsf{in}(\vec{x},ch,v,\rdy(\cup_{i\in I} ch_i*))}{Q[v/x])} \\
	%		P\Rightarrow \textsf{wait}(\vec{x},\infty,\rdy(\cup_{i\in I} ch_i*)) \wandop R
	%	\end{array}
	%}
	%{ \spec{P}{\external{i\in I}{io_i}{c_i}}{R} }
	%\]
	\[ \prftree[r]{Int}{
		\begin{array}{l}
			\forall i\in I.\,\textrm{if } io_i=ch!e \textrm{ then} \\
			\qquad\qquad \exists Q.\, \spec{Q}{c_i}{R} \wedge P \rightarrow 
			\forall \vec{x}_2.\,
			\wand{\textsf{odeout}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,ch,e,\rdy(\cup_{i\in I} ch_i*))}{Q[\vec{x}_2/\vec{x}]} \\
			\qquad\quad \textrm{elif } io_i=ch?x \textrm{ then} \\
			\qquad\qquad \exists Q.\, \spec{Q}{c_i}{R} \wedge P \rightarrow
			\forall \vec{x}_2.\,
			\wand{\textsf{odein}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,ch,x,\rdy(\cup_{i\in I} ch_i*))}{Q[\vec{x}_2/\vec{x}]} \\
			P\rightarrow \forall \vec{x}_2.\,
			\wand{\textsf{ode}(\vec{x},\vec{\dot{x}}=\vec{e},B,\vec{x}_2,\rdy(\cup_{i\in I} ch_i*))}{R[\vec{x}_2/\vec{x}]}
		\end{array}
	}
	{ \spec{P}{\exempt{\evo{x}{e}{B}}{i\in I}{io_i}{c_i}}{R} }
	\]
	\caption{The simplified weakest liberal precondition rules}
	\label{fig:simpwp}
\end{figure*}
}
}

\oomit{
{\small
\begin{figure*}
	\[ \prftree[r]{AssignSp}
	{\spec{ P}
		{x := e}{\exists v. x=v \wedge P[e[v/x]/x] }}
	\]
	\[ \prftree[r]{OutSp}
	{\spec{ P}
		{ch!e}{ \join{P}{\textsf{out}(\vec{x},ch,e)}}}
	\]
	\[ \prftree[r]{InSp}
	{\spec{ P}
		{ch?x}{\exists v.\, x =  v \wedge
			(\join{P[v/x]}{\textsf{in}(\vec{x}[v/x],ch,v)})}}
	\]
	\[ \prftree[r]{ContSp1}{B[\vec{x}_0/\vec{x}]}
	{\spec{\vec{x} = \vec{x}_0 \wedge P}
		{\evo{x}{e}{B}}{\exists \vec{x}_2.\, \vec{x} = \vec{x}_2 \wedge P \joinop \textsf{ode}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2)}}
	\]
	\[ \prftree[r]{ContSp2}{\neg B[\vec{x}_0/\vec{x}]}
	{\spec{\vec{x} = \vec{x}_0 \wedge P}
		{\evo{x}{e}{B}}{ P}} \]
	
\oomit{	\[ \prftree[r]{EChoice}{
		\begin{array}{llll}
			\forall i\in I.\,\textrm{if } ch_i*=ch!e \textrm{ then} \\
			\qquad\qquad \spec{\vec{x}_0 = \vec{x}_0 \wedge (P \joinop \textsf{out}(\vec{x}_0,ch,e(\vec{x}_0),\rdy(\cup_{i\in I} ch_i*)))}{c_i}{Q} \\
			\qquad\quad \textrm{elif } ch_i*=ch?x \textrm{ then} \\
			\qquad\qquad \spec{\exists v.\, \vec{x}_0 = \vec{x}_0[x\mapsto v] \wedge
				(P \joinop \textsf{in}(\vec{x}_0,ch,v,\rdy(\cup_{i\in I} ch_i*)))}{c_i}{Q} \\
			P \joinop \textsf{wait}(\vec{x}_0,\infty,\rdy(\cup_{i\in I} ch_i*)) \Rightarrow Q
		\end{array}
	}
	{ \spec{\vec{x}_0 = \vec{x}_0 \wedge P}{\external{i\in I}{ch_i*}{c_i}}{Q} }
	\] }
   \caption{The simplified strongest postcondition rules}
   \label{fig:simpsp}
\end{figure*}
}
}
\oomit{
\section{Simplified assertions and proof rules}\label{sec:assertions}

\subsection{Basic assertions}

In this appendix, we present further definitions and proof rules in the simplified assertion language. For reference, we repeat the basic assertions on traces given in the main text.
\[ \textsf{emp} \triangleq \textit{tr} = \epsilon \]
\[ \begin{array}{l}
	\textsf{out}(\vec{x}_0,ch,v,\rdy) \triangleq \textit{tr} = \langle ch!,v \rangle \,\vee \\
	\qquad \exists d>0.\, \textit{tr} = \langle d,I_{\vec{x}_0},\rdy \rangle^\chop \langle ch!,v\rangle \,\vee \\
	\qquad \textit{tr} = \langle\infty,I_{\vec{x}_0},\rdy\rangle
\end{array} \]
\[ 
\begin{array}{l}
	\textsf{in}(\vec{x}_0,ch,v,\rdy) \triangleq \textit{tr} = \langle ch?,v \rangle \,\vee \\
	\qquad \exists d>0.\, \textit{tr} = \langle d,I_{\vec{x}_0},\rdy \rangle^\chop \langle ch?,v\rangle \,\vee \\
	\qquad \textit{tr} = \langle\infty,I_{\vec{x}_0},\rdy\rangle
\end{array} \]

\[ \textsf{path}(d,\vec{p},\rdy) \triangleq \textit{tr} = \langle d,\vec{p},\rdy \rangle \]

We make the following abbreviations
\[\begin{array}{l}
\textsf{out}(\vec{x}_0,ch,v) \triangleq 
\textsf{out}(\vec{x}_0,ch,v,\{ch!\}) \\ \textsf{in}(\vec{x}_0,ch,v) \triangleq \textsf{in}(\vec{x}_0,ch,v,\{ch?\}) \\
\textsf{path}(d,\vec{p}) \triangleq \textsf{path}(d,\vec{p},\emptyset)
\end{array}
\]

The assertions involving ODE evolution are:
\begin{align*}
	&\textsf{ode}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,\rdy)
	\triangleq \\
	&\quad (\neg B[\vec{x}_0/\vec{x}] \wedge \textit{tr} = \epsilon \wedge \vec{x}_2 = \vec{x}_0)\, \vee \\
	&\quad \exists d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \wedge \neg B[\vec{p}_{\vec{x}_0}(d)/\vec{x}] \,\wedge \\
	&\qquad\quad \textit{tr} = \langle d,\vec{p}_{\vec{x}_0},\rdy\rangle \wedge \vec{x}_2 = \vec{p}_{\vec{x}_0}(d)
\end{align*}
\begin{align*}
	&\textsf{odein}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,ch,y,\rdy) \triangleq \\
	&\quad (\exists v.\, \textit{tr} = \langle ch?, v \rangle \wedge \vec{x}_2 = \vec{x}_0[v/y])\, \vee \\
	&\quad \exists v.\, \exists d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}]) \,\wedge \\
	&\qquad\quad \textit{tr} = \langle d,\vec{p}_{\vec{x}_0},\rdy\rangle^\chop \langle ch?,v\rangle \wedge \vec{x}_2=\vec{p}(d)[v/y]
\end{align*}
\begin{align*}
	&\textsf{odeout}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,ch,e,\rdy) \triangleq \\
	&\quad (\textit{tr} = \langle ch!, e[\vec{x}_0/\vec{x}] \rangle \wedge \vec{x}_2 = \vec{x}_0)\, \vee \\
	&\quad \exists d>0.\, (\forall t\in[0,d).\, B[\vec{p}_{\vec{x}_0}(t)/\vec{x}] ) \,\wedge \\
	& \qquad\quad \textit{tr} =
	\langle d,\vec{p}_{\vec{x}_0},\rdy\rangle^\chop \langle ch!,e[\vec{p}_{\vec{x}_0}(d) /\vec{x}] )\rangle \wedge \vec{x}_2=\vec{p}_{\vec{x}_0}(d).
\end{align*}

We further define two assertions that represent extending \textsf{path} by an input or output communication. These specify that the trace consists of evolution along a path followed by an immediate communication:
\begin{align*}
	&\textsf{pathOut}(d,\vec{p},ch,e,\rdy) \triangleq \\
	&\quad \mbox{if } d = 0 \mbox{ then } \textit{tr} = \langle ch!, e[\vec{p}_{\vec{x}_0}(0) /\vec{x}] \rangle
	 \\
	&\quad \qquad \mbox{ else } \textit{tr} = \langle d,\vec{p},\rdy \rangle^\chop \langle ch!, e[\vec{p}_{\vec{x}_0}(d) /\vec{x}] \rangle \\
	&\textsf{pathIn}(d,\vec{p},ch,v,\rdy) \triangleq \\
	&\quad \mbox{if } d = 0 \mbox{ then } \textit{tr} = \langle ch?, v \rangle
	\mbox{ else } \textit{tr} = \langle d,\vec{p},\rdy \rangle^\chop \langle ch?, v \rangle
\end{align*}

Also, we make the following abbreviations:
\[\begin{array}{l}
	\oomit{\textsf{wait}(\vec{x}_0,d) \triangleq \textsf{wait}(\vec{x}_0,d,\emptyset) \\}
	\textsf{ode}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2) \triangleq \textsf{ode}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,\emptyset) \\
	\textsf{odein}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,ch,x) \triangleq\\
	\qquad\textsf{odein}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,ch,x,\{ch?\})\\
	\textsf{odeout}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_2,ch,e) \triangleq \\
	\qquad\textsf{odeout}(\vec{x}_0,\vec{\dot{s}}=\vec{e},B,\vec{x}_2,ch,e,\{ch!\})
\end{array}  \]

For reference, we repeat the definition of join and magic wand on assertions:
\[ (\join{P}{Q}) \triangleq \exists tr_1\,tr_2.\, P[tr_1/tr]\wedge Q[tr_2/tr]\wedge \textit{tr} = {tr_1}^\chop tr_2 \]
\[ (\wand{Q}{P}) \triangleq \forall \textit{tr}'.\, Q[\textit{tr}'/tr] \rightarrow P[\textit{tr}^\chop \textit{tr}'/tr] \]

One can easily check many properties of join and magic wand. A selection of which is listed below.
\[ \join{\textsf{emp}}{P} = P \]
\[ \join{P}{\textsf{emp}} = P \]
\[ \join{P}{(\join{Q}{R})} = \join{(\join{P}{Q})}{R} \]
\[ \textsf{emp} \rightarrow \wand{P}{P} \]
\[ Q \rightarrow \wand{P}{(\join{Q}{P})} \]
\[ P \rightarrow Q \longrightarrow \wand{R}{P} \rightarrow \wand{R}{Q} \]


\subsection{Simplified weakest liberal precondition calculus}

With the above definitions, we can simplify the weakest liberal precondition rules stated in Sect.~\ref{sec:hoare}. The rules that can be simplified are given in Fig.~\ref{fig:simpwp}. For interrupt with a fixed configuration of communications, the rules can be stated in simpler form. For example, for the case with one input communication, the rule is:

{\small
\[ \prftree
{\spec{Q}{c}{R}}
{
	\begin{array}{l}
		\left\{\begin{array}{l}
			\forall \vec{x_2}.\,\wand{\textsf{odein}(\vec{x}_0,\vec{\dot{s}}=\vec{e},B,\vec{x}_2,ch,x,\{ch?\})}{Q[\vec{x}_2/\vec{x}]}
			\,\wedge \\
			\forall \vec{x_2}.\,\wand{\textsf{ode}(\vec{x}_0,\vec{\dot{s}}=\vec{e},B,\vec{x}_2,\{ch?\})}{R[\vec{x}_2/\vec{x}]}
		\end{array}
		\right\}\, \\
		\quad \evo{x}{e}{B}\lr (ch?x \to c)\, \\
		\{R\}
	\end{array}}
\]
}

\subsection{strongest postcondition rules}\label{sec:strongest-postcondition}

Similarly, we can state a strongest postcondition calculus for HHL using the simplified assertion language. The rules for sequential processes that can be simplified are shown in Fig.~\ref{fig:simpsp}.

\oomit{
{\small
\[ \prftree
{\spec{P_1}{c_1}{Q_1}}
{\spec{P_2}{c_2}{Q_2}}
{\spec{P_1[\epsilon/\textit{tr}]\wedge P_2[\epsilon/\textit{tr}]}{c_1\|c_2}
	{Q_1 \|_{ch} Q_2}}
\]}

\oomit{
\[ \prftree[r]{Int}{
	\begin{array}{l}
		\forall i\in I.\,\textrm{if } ch_i*=ch!e \textrm{ then} \\
		\qquad\qquad \spec{P \joinop \textsf{odeout}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_0,ch,e, \\ 
		   \quad \quad \quad  \quad \quad  \quad \quad \rdy(\cup_{i\in I} ch_i* ))}{c_i}{Q} \\
		\qquad\quad \textrm{elif } ch_i*=ch?x \textrm{ then} \\
		\qquad\qquad \spec{P \joinop
			\textsf{odein}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_0,ch,x, \\
			 \quad \quad \quad  \quad \quad  \quad \quad \rdy(\cup_{i\in I} ch_i* ))}{c_i}{Q} \\
		P \joinop \textsf{ode}(\vec{x}_0,\vec{\dot{x}}=\vec{e},B,\vec{x}_0,\rdy(\cup_{i\in I} ch_i* )) \Rightarrow Q
	\end{array}
}
{ \spec{\vec{x}=\vec{x}_0\wedge P}{\exempt{\evo{x}{e}{B}}{i\in I}{ch_i*}{c_i}}{Q} }
\]
}}

\oomit{
For example, for external choice with two inputs, we have:
\[ \prftree
{\begin{array}{l}
		\{\exists v.\, \vec{x}_0 = \vec{x}_0[x\mapsto v] \wedge (P \joinop \textsf{in}(\vec{x}_0,ch_1,v,\{ch_1?,ch_2?\}))\}\\
		\quad c_1 \{Q_1\} \\
		\{\exists v.\, \vec{x}_0 = \vec{x}_0[y\mapsto v] \wedge (P \joinop \textsf{in}(\vec{x}_0,ch_2,v,\{ch_1?,ch_2?\}))\}\\
		\quad c_2 \{Q_2\}
\end{array}}
{\{\vec{x}_0 = \vec{x}_0 \wedge P\} ch_1?x\to c_1\lr ch_2?y\to c_2\\
	\{Q_1 \vee Q_2 \}}
\]}

As before, we can obtain simpler forms of rules for interrupt with a fixed configuration of communications. For example, for the case with one input communication, the rule is:
\[ \prftree
{\spec{P \joinop \textsf{odein}(\vec{x}_0,\vec{\dot{s}}=\vec{e},B,\vec{x}_0,ch,x)}{c}{Q}}
{\begin{array}{l}
\{\vec{x}_0 = \vec{x}_0 \wedge P\} ~\evo{x}{e}{B}\lr (ch?x \to c) \\
\{Q \vee (P \joinop \textsf{ode}(\vec{x}_0,\vec{\dot{s}}=\vec{e},B,\vec{x}_0,\{ch?\}))\}
	\end{array}
} 
\]


\subsection{ODE and interrupt: explicit solution rules}


When the ODE appearing in the program can be explicitly solved, we can state rules in terms of explicit solutions.
For example, the rules for ODE making use of explicit solutions is as follows, where $\vec{p}$ is the unique solution to $\evo{x}{e}{B}$ starting from $\vec{x}_0$.


{\small
\[\prftree[r]{ }
{\begin{array}{c}
	d > 0\quad \vec{p}(0)=\vec{x}_0 \quad \forall t\in[0,d).\, B[\vec{p}(t)/\vec{x}] \quad \neg B[\vec{p}(d)/\vec{x}]
\end{array}}
{
	\begin{array}{l}
		\{\vec{x}=\vec{x}_0\wedge P\}~\evo{x}{e}{B}~
		\{\vec{x} = \vec{p}(d) \wedge (P \joinop \textsf{path}(d,\vec{p}))\} 
	\end{array}}
\]
}

\vspace{-3mm}\noindent Likewise, we can state rules for interrupt in terms of explicit solutions. As an example, consider the case with one output communication, where there exists a solution $\vec{p}$ that never exits the boundary (one common case is where \sm{B=\textsf{true}}).

{\small
\[\prftree
{\begin{array}{c}
		\vec{p}(0)=\vec{x}_0 \quad \forall t\ge 0.\, B[\vec{p}(t)/\vec{x}] \\
		\spec{\exists d.\, \vec{x}_0=\vec{p}(d) \wedge (P \joinop \textsf{pathOut}(d,\vec{p},ch,e,\{ch!\}))}{c}{Q}
\end{array}}
{\spec{\vec{x}=\vec{x}_0\wedge P}{\evo{x}{e}{B}\lr (ch!e \to c)}{Q}}
\]
}
}

\oomit{
\section{More Examples}
 
In this section, we provide more examples that have been proved with the theorem prover of HHL implemented in Isabelle/HOL. 

\subsection{Inputs and outputs only}

We begin with simple processes consisting of communications only. For the process $ch!x; ch!y$, the weakest liberal precondition derivation is:
{\small
\begin{align*}
	\{ & \forall v.\, \textsf{in}(\vec{x},ch_1,v) \wandop \\
	&\quad\textsf{out}(\vec{x}[v/x],ch_2,v+1) \wandop Q[v/x] \} \\
	\{ & \forall v.\, \textsf{in}(\vec{x},ch_1,v) \wandop \\
	&\quad\textsf{out}(\vec{x}[v/x],ch_2,x+1) \wandop Q[v/x]) \} \\
	& ch_1?x; \\
	\{ & \textsf{out}(\vec{x},ch_2,x+1) \wandop Q \} \\
	& ch_2!(x+1); \\
	\{ & Q \}
\end{align*}
}
The strongest postcondition derivation is:
{\small
\begin{align*}
	\{ & x = x_0 \wedge P  \} \\
	& ch_1?x; \\
	\{ & \exists v.\, x = v \wedge (P[x_0/x] \joinop \textsf{in}(\vec{x_0},ch_1,v)) \} \\
	& ch_2!(x+1); \\
	\{ & \exists v.\, x = v \wedge (P[x_0/x] \joinop \textsf{in}(\vec{x_0},ch_1,v) \joinop \\
	&\quad\textsf{out}(\vec{x_0}[v/x],ch_2,x+1) \} \\
	\{ & \exists v.\, x = v \wedge (P[x_0/x] \joinop \textsf{in}(\vec{x_0},ch_1,v) \joinop \\
	&\quad\textsf{out}(\vec{x_0}[v/x],ch_2,v+1) \}
\end{align*}
}

\subsection{Output and loops}\label{sec:output-and-loops}

Now consider the process $(x := x + 1; ch!x)^*$. Suppose the initial state has $x=a$. Then, the loop invariant is given by
\[ P \triangleq \exists n.\, x =  a+n \wedge \textsf{countUp}(a,n), \] where \textsf{countUp} is defined as:
\begin{align*}
	\textsf{countUp}(a,0) &= \textsf{emp} \\
	\textsf{countUp}(a,n+1) &= \textsf{out}([x\mapsto a+1],ch,a+1) \joinop \\
	&\quad\textsf{countUp}(a+1,n)
\end{align*}
We defined \textsf{countUp} with recursive call on the right of $\textsf{@}$, for ease of use in the parallel rule later. We can easily check the following property of \textsf{countUp}:
\begin{align*}
 \textsf{countUp}(a,n+1) &= \textsf{countUp}(a,n) \joinop\\
&\quad \textsf{out}([x\mapsto a+n+1], ch, a+n+1) 
\end{align*}
The derivation for one iteration of the loop is:
\begin{align*}
	\{ & \exists n.\, s = [x \mapsto a + n] \wedge \textsf{countUp}(a,n) \} \\
	\{ & s = [x \mapsto a+n] \wedge \textsf{countUp}(a,n) \} \\
	& x := x + 1\\
	\{ & s = [x \mapsto a+n+1] \wedge \textsf{countUp}(a,n) \} \\
	& ch!x \\
	\{ & s = [x \mapsto a+n+1] \wedge \textsf{countUp}(a,n) \joinop \\
	& \quad
	\textsf{out}([x\mapsto a+n+1],ch,a+n+1) \} \\
	\{ & s = [x \mapsto a+n+1] \wedge \textsf{countUp}(a,n+1) \} \\
	\{ & \exists n'.\, s = [x \mapsto a+n'] \wedge \textsf{countUp}(a,n') \}
\end{align*}
The second to last step uses the property of \textsf{countUp} stated above. In the last step, the existence statement is shown by taking $n'=n+1$.

Then, by the repetition rule, we obtain $\spec{P}{(x:=x+1;ch!x)^*}{P}$, the remainder of the derivation is:
\begin{align*}
	\{ & s = [x \mapsto a] \wedge \textit{tr} = \epsilon \} \\
	\{ & \exists n.\, s = [x\mapsto a+n] \wedge \textsf{countUp}(a,n) \} \\
	& (x:=x+1;ch!x)^* \\
	\{ & \exists n.\, s = [x\mapsto a+n] \wedge \textsf{countUp}(a,n) \}
\end{align*}
In the first step, the existence statement is shown by taking $n=0$.

We show a more complex example with a different proof strategy. Consider the process
\[ (x:=x+1; ch!x; x:=x+2)^*.\]
Suppose the initial state has $x=0$. Then a loop invariant is given by the predicate
\[ P \triangleq \exists n.\, s = [x\mapsto 3n] \wedge \textsf{countUp3}(n), \] where $\textsf{countUp3}$ is defined as:
\begin{align*}
	\textsf{countUp3}(0) &= \textsf{emp} \\
	\textsf{countUp3}(n+1) &= \textsf{countUp3}(n) \joinop \\
	& \quad
	\textsf{out}([x\mapsto 3n+1], ch, 3n+1).
\end{align*}
The derivation for one iteration is:
\begin{align*}
	\{ & \exists n.\, s = [x \mapsto 3n] \wedge \textsf{countUp3}(n) \} \\
	\{ & s = [x \mapsto 3n] \wedge \textsf{countUp3}(n) \} \\
	& x := x + 1 \\
	\{ & s = [x \mapsto 3n+1] \wedge \textsf{countUp3}(n) \} \\
	& ch!x \\
	\{ & s = [x \mapsto 3n+1] \wedge \textsf{countUp3}(n) \joinop \\
	& \quad
	\textsf{out}([x\mapsto 3n+1],ch,3n+1) \} \\
	& x := x + 2 \\
	\{ & s = [x \mapsto 3n+3] \wedge \textsf{countUp3}(n) \joinop\\
	& \quad
	\textsf{out}([x\mapsto 3n+1],ch,3n+1) \} \\
	\{ & s = [x \mapsto 3(n+1)] \wedge \textsf{countUp3}(n+1) \} \\
	\{ & \exists n'.\, s = [x \mapsto 3n'] \wedge \textsf{countUp3}(n') \}
\end{align*}
The proof is completed by
\begin{align*}
	\{ & s = [x\mapsto 0] \wedge \textit{tr} = \epsilon \} \\
	\{ & \exists n.\, s = [x \mapsto 3n] \wedge \textsf{countUp3}(n) \} \\
	& (x := x + 1; ch!x; x := x + 2)^* \\
	\{ & \exists n.\, s = [x \mapsto 3n] \wedge \textsf{countUp3}(n) \} 
\end{align*}

\subsection{Input and loops}\label{sec:input-and-loops}

We now consider a process that repeatedly reads from channel $ch$ and add the input values to $x$. The process is given by:
\[ (ch?y; x := x + y)^* \]
To define the invariant of the loop, we first define an assertion parameterized by two real values (initial values of $x$ and $y$), as well as the sequence of input values:
\begin{align*}
	\textsf{receiveAdd}(a,b,[]) &= \textsf{emp} \\
	\textsf{receiveAdd}(a,b,x\cdot xs) &=
	\textsf{in}([x \mapsto a, y\mapsto b],ch,x) \joinop\\
	& \quad \textsf{receiveAdd}(a+x,x,xs)
\end{align*}
We further define $\textsf{last}(a,xs)$ to be the last value of $a\cdot xs$, and $\textsf{sum}(a,xs)$ to be $a+\sum xs$. We check that the following identity holds:
\begin{align*}
	& \textsf{receiveAdd}(a,b,xs\cdot v) =  \textsf{receiveAdd}(a,b,xs) \joinop \\
	& \quad\textsf{in}([x\mapsto \textsf{sum}(a,xs), y\mapsto \textsf{last}(b,xs)],ch,v)
\end{align*}
The invariant is given by:
\begin{align*}
	 &P \triangleq \exists xs.\, s = [x\mapsto \textsf{sum}(a,xs), y\mapsto \textsf{last}(b,xs)]
\wedge \\
& \qquad \quad \textsf{receiveAdd}(a,b,xs) 
\end{align*}
The derivation is as follows:
\begin{align*}
	\{ & \exists xs.\, s = [x\mapsto \textsf{sum}(a,xs), y\mapsto \textsf{last}(b,xs)] \wedge \\
	& \qquad\textsf{receiveAdd}(a,b,xs) \} \\
	\{ & s = [x\mapsto \textsf{sum}(a,xs), y\mapsto \textsf{last}(b,xs)] \wedge \\
	& \qquad\textsf{receiveAdd}(a,b,xs) \} \\
	& ch?y \\
	\{ & \exists v.\, s = [x\mapsto \textsf{sum}(a,xs), y\mapsto v]\,\wedge \textsf{receiveAdd}(a,b,xs) \joinop\\
	& \qquad  \textsf{in}([x\mapsto \textsf{sum}(a,xs), y\mapsto \textsf{last}(b,xs)],ch,v) \} \\
	\{ & s = [x\mapsto \textsf{sum}(a,xs), y\mapsto v]\,\wedge \textsf{receiveAdd}(a,b,xs) \joinop\\
	& \qquad  \textsf{in}([x\mapsto \textsf{sum}(a,xs), y\mapsto \textsf{last}(b,xs)],ch,v) \} \\
	& x := x + y \\
	\{ & s = [x\mapsto \textsf{sum}(a,xs)+v, y\mapsto v]\,\wedge \textsf{receiveAdd}(a,b,xs) \joinop\\
	& \qquad  \textsf{in}([x\mapsto \textsf{sum}(a,xs), y\mapsto \textsf{last}(b,xs)],ch,v) \} \\
	\{ & s = [x\mapsto \textsf{sum}(a,xs\cdot v),
	y\mapsto \textsf{last}(a,xs\cdot v)] \wedge\\
	& \qquad
	\textsf{receiveAdd}(a,b,xs\cdot v) \} \\
	\{ & \exists xs'.\, s = [x\mapsto \textsf{sum}(a,xs'),
	y\mapsto \textsf{last}(a,xs')] \wedge\\
	& \qquad
	\textsf{receiveAdd}(a,b,xs') \}
\end{align*}
The second to last step uses identities for \textsf{sum}, \textsf{last}, and the above stated identity for \textsf{receiveAdd}. The last line takes $xs'=xs\cdot v$.

\subsection{External choice}

We now show an example with external choice and loops. Consider the process
\[ (ch_1?x \to y := y + x \lr ch_2?x \to y := y - x)^* \]
In order to specify an invariant, we need to first define a function \textsf{echoiceEx}, that takes as argument a state, and a list of pairs from $\{ch_1,ch_2\} \times \mathbb{R}$:
\begin{align*}
	&\textsf{echoiceEx}(s,[])  = \textsf{emp} \\
	&\textsf{echoiceEx}(s,(ch_1,v)\cdot \mathit{ins})  = \textsf{in}(s,ch_1,v,\{ch_1?,ch_2?\}) \joinop\\
	& \qquad \textsf{echoiceEx}(s[x\mapsto v, y\mapsto s(y) + v], \mathit{ins}) \\
	&\textsf{echoiceEx}(s,(ch_2,v)\cdot \mathit{ins})  = \textsf{in}(s,ch_2,v,\{ch_1?,ch_2?\}) \joinop\\
	& \qquad \textsf{echoiceEx}(s[x\mapsto v, y\mapsto s(y) - v], \mathit{ins})
\end{align*}
Again, we make the definition with recursive call on the right of $\textsf{@}$ to make the application of parallel rule easier later on.

The state reached starting from $s$ with input $\mathit{ins}$ is given by:
\begin{align*}
	\textsf{lastS}(s,[]) &= s \\
	\textsf{lastS}(s,(ch_1,v)\cdot \mathit{ins}) &=
	\textsf{lastS}(s[x\mapsto v, y\mapsto s(y)+v], \mathit{ins}) \\
	\textsf{lastS}(s,(ch_2,v)\cdot \mathit{ins}) &=
	\textsf{lastS}(s[x\mapsto v, y\mapsto s(y)-v], \mathit{ins})
\end{align*}

As before, we need to state and prove the evaluation of \textsf{echoiceEx} and \textsf{lastS} on $\mathit{ins}\cdot v$. We omit the details here. The final result of the derivation is:
\begin{align*}
	\{ & s = s_0 \wedge \textit{tr} = \epsilon \} \\
	& (ch_1?x \to y := y + x) \lr (ch_2?x \to y := y - x)^* \\
	\{ & \exists \mathit{ins}.\, s = \textsf{lastS}(s_0,\mathit{ins}) \wedge
	\textsf{echoiceEx}(s,\mathit{ins}) \}
\end{align*}


\subsection{Example with loop and parallel}

We demonstrate the above rules by showing a full example with both loop and parallel. Consider the following process:
\[ (x := x + 1; ch!x)^* \| (ch?y)^* \]
The left side has already been analyzed in Section \ref{sec:output-and-loops}. We repeat the result here:
\begin{align*}
	\{ & s = [x \mapsto a] \wedge \textit{tr} = \epsilon \} \\
	& (x:=x+1;ch!x)^* \\
	\{ & \exists n.\, s = [x\mapsto a+n] \wedge \textsf{countUp}(a,n) \}
\end{align*}
where \textsf{countUp} is defined as:
\begin{align*}
	\textsf{countUp}(a,0) &= \textsf{emp} \\
	\textsf{countUp}(a,n+1) &= \textsf{out}([x\mapsto a+1],ch,a+1) \joinop \\
	&\qquad\textsf{countUp}(a+1,n)
\end{align*}

The right side is a simpler version of the example in Section \ref{sec:input-and-loops}, the result is:
\begin{align*}
	\{ & s = [y \mapsto b] \wedge \textit{tr} = \epsilon \} \\
	& (ch?y)^* \\
	\{ & \exists xs.\, s = [y\mapsto \textsf{last}(b,xs)] \wedge \textsf{receive}(a,xs) \}
\end{align*}
where \textsf{receive} is defined as:
\begin{align*}
	\textsf{receive}(b,[]) &= \textsf{emp} \\
	\textsf{receive}(b,x\cdot xs) &= \textsf{in}([y\mapsto b],ch,x) \joinop \textsf{receive}(x,xs)
\end{align*}

To analyze the parallel program, we define the following assertion:
\begin{align*}
	\textsf{countUpIO}(a,0) &= \textsf{emp} \\
	\textsf{countUpIO}(a,n+1) &= \textsf{IO}(ch,a+1) \joinop\\
	&\qquad \textsf{countUpIO}(a+1,n)
\end{align*}

We prove the following by induction on $n$:
\[ \textsf{countUp}(a,n) \|_{ch} \textsf{receive}(b,xs) \rightarrow \textsf{countUpIO}(a,n) \]
\begin{proof}
	For the case $n = 0$, we divide into two cases by whether $xs$ is empty. If $xs$ is empty, then we have $\textsf{emp}\|_{ch}\textsf{emp} \Rightarrow \textsf{emp}$ as desired. Otherwise, we have $xs=x\cdot xs'$, and
	\[ \textsf{emp}\|_{ch} (\textsf{in}([y\mapsto b],ch,x) \joinop \textsf{receive}(x,xs')) \rightarrow \textsf{false} \]
	
	Now consider the induction case, with $n = k + 1$. Again, divide into two cases by whether $xs$ is empty. If $xs$ is empty, then we have
	\[ 
	\begin{array}{l}
		(\textsf{out}([x\mapsto a+1],ch,a+1) \joinop \textsf{countUp}(a+1,k)) \|_{ch} \textsf{emp} \\
		\qquad  \rightarrow\textsf{false}
	\end{array} \]
	So the only remaining case is where $xs=x\cdot xs'$. Then the derivation is:
	\begin{align*}
		& \textsf{countUp}(a,k+1) \|_{ch} \textsf{receive}(b,x\cdot xs') \\
		\rightarrow & (\textsf{out}([x\mapsto a+1],ch,a+1) \joinop \textsf{countUp}(a+1,k)) \|_{ch} \\
		& \qquad(\textsf{in}([y\mapsto b],ch,x) \joinop \textsf{receive}(x,xs')) \\
		\rightarrow & x = a + 1 \wedge \textsf{IO}(ch,a+1) \joinop\\
		& \qquad (\textsf{countUp}(a+1,k) \|_{ch} \textsf{receive}(x,xs')) \\
		\rightarrow & \textsf{IO}(ch,a+1) \joinop \textsf{countUpIO}(a+1,k) \\
		\rightarrow & \textsf{countUpIO}(a,k+1)
	\end{align*}
\end{proof}
The first step expands the definitions. The second step uses one of the rules for synchronization of assertions. The third step uses the inductive hypothesis. The last step uses the definition of \textsf{countUpIO}. This completes the proof.\footnote{We can also obtain information about the list of received values $xs$, and hence the final state of the right process, by keeping the pure assertion $x=a+1$ at each step.}

Applying the parallel rule, we obtain the final result:
\begin{align*}
	\{ & s = [x\mapsto a, y\mapsto b] \wedge \textsf{emp} \} \\
	& (x := x + 1; ch!x)^* \| (ch?y)^* \\
	\{ & \exists b.\, \exists ys.\, s = [x\mapsto b, y\mapsto \textsf{last}(0,ys)] \wedge \textsf{countUpIO}(0) \}
\end{align*} 


\subsection{Example with ODE and loop}

We consider the following process:
\[ (\evo{x}{1}{x < 1}; ch_1!x; ch_2?x)^* \]
This process repeatedly increases $x$ to 1 (does nothing if $x$ is already at least 1), then send the value of $x$ via channel $ch_1$, and receive a new value of $x$ via channel $ch_2$. For the invariant, we define the function $\textsf{left}(a,vs)$, for the behavior of the process when the starting value of $x$ is $a$, and the sequence of values received via $ch_2$ is $vs$.
\begin{align*}
	\textsf{left}(a,[]) &= \textsf{emp} \\
	\textsf{left}(a,v\cdot vs') &= \mbox{if } a < 1 \mbox{ then} \\
	&\qquad \textsf{path}(1-a, [x\mapsto t + a]) \joinop \\
	&\qquad \textsf{out}([x\mapsto 1], ch_1, 1) \joinop
	\textsf{in}([x\mapsto 1], ch_2, v) \joinop \\
	&\qquad\textsf{left}(v,vs') \\
	&\quad \mbox{else } \\
	&\qquad \textsf{out}([x\mapsto a], ch_1, a) \joinop
	\textsf{in}([x\mapsto a], ch_2, v) \joinop \\
	&\qquad\textsf{left}(v,vs')
\end{align*}
The value of $x$ at the end is $\textsf{last}(a,vs)$. We also need to derive the expansion of $\textsf{left}(a,vs\cdot v)$. The details are omitted here. Applying the repetition rule and the above strongest postcondition rules, we get the following:
\begin{align*}
	\{ & s = [x\mapsto a] \wedge \textit{tr} = \epsilon \} \\
	& (\evo{x}{1}{x < 1}; ch_1!x; ch_2?x)^* \\
	\{ & \exists vs.\, [x\mapsto \textsf{last}(a,vs)] \wedge \textsf{left}(a,vs) \}
\end{align*}

Now, we consider its synchronization with a right side, given by:
\[ (ch_1?y; ch_2!(y-1))^* \]
If we assume that $x$ on the left process begins at 0, and $y$ on the right process begins at 1, then the left process behaves in the ``saw-tooth'' pattern, repeatedly increasing from 0 to 1 then suddenly dropping back to 0.

For the invariant on the right, we define the function $\textsf{right}(b,ws)$, for the behavior of the process when the starting value of $y$ is $b$, and the sequence of values received via $ch_1$ is $ws$.
\[
\begin{array}{rl}
	\textsf{right}(b,[])=&\textsf{emp} \\
	\textsf{right}(b,w\cdot ws')=&\textsf{in}([y\mapsto b],ch_1,w) \joinop\\
	&\textsf{out}([y\mapsto w],ch_2,w-1) \joinop \textsf{right}(w,ws')
\end{array}
\]
The value of $y$ at the end is $\textsf{last}(b,ws)$. The result of applying repetition rule and strongest postcondition rules is the following:
\begin{align*}
	\{ & s = [y\mapsto b] \wedge \textit{tr} = \epsilon \} \\
	& (ch_1?y; ch_2!(y-1))^* \\
	\{ & \exists ws.\, s = [y\mapsto \textsf{last}(b,ws)] \wedge \textsf{right}(b,ws) \}
\end{align*}

The final step is to compute the synchronization of the postconditions. The result is given by the following recursive function:
\[
\begin{array}{rl}
	\textsf{tot}(0)=&\textsf{emp} \\
	\textsf{tot}(n + 1)=&\textsf{path}(1,[x\mapsto t, y\mapsto 1]) \joinop
	\textsf{IO}(ch_1,1) \joinop \\
	&\textsf{IO}(ch_2,0) \joinop \textsf{tot}(n)
\end{array}
\]
The computation of synchronization follows the same technique as in Section \ref{sec:synchronize-assertion}. The lemma to be proved by induction is (taking $cs=\{ch_1,ch_2\}$):
\[ \textsf{left}(0,vs) \|_{cs} \textsf{right}(1,ws) = \textsf{tot}(|vs|) \]

We show the interesting part of it below:
\begin{align*}
	& \textsf{left}(0,v\cdot vs) \|_{cs} \textsf{right}(1,w\cdot ws) \\
	\rightarrow & \textsf{path}(1, [x\mapsto t]) \joinop
	\textsf{out}([x\mapsto 1], ch_1, 1) \joinop\\
	&\qquad
	\textsf{in}([x\mapsto 1], ch_2, v) \joinop \textsf{left}(v,vs) \|_{cs} \\
	&\ \textsf{in}([y\mapsto 1],ch_1,w) \joinop
	\textsf{out}([y\mapsto w],ch_2,w-1) \joinop \\
	&\qquad\textsf{right}(w,ws) \\
	\rightarrow & \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop \\
	&\quad (\textsf{out}([x\mapsto 1], ch_1, 1) \joinop
	\textsf{in}([x\mapsto 1], ch_2, v) \joinop \\
	&\qquad\textsf{left}(v,vs) \|_{cs} \\
	&\quad \ \textsf{in}([y\mapsto 1],ch_1,w) \joinop
	\textsf{out}([y\mapsto w],ch_2,w-1) \joinop \\
	&\qquad\textsf{right}(w,ws)) \\ 
	\rightarrow & w = 1 \wedge \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop
	\textsf{IO}(ch_1,1) \joinop \\
	&\quad (\textsf{in}([x\mapsto 1], ch_2, v) \joinop\\
	&\qquad \textsf{left}(v,vs) \|_{cs} 
	\textsf{out}([y\mapsto w],ch_2,0) \joinop \textsf{right}(1,ws)) \\ 
	\rightarrow & w = 1 \wedge v = 0 \wedge \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop\\
	&\qquad
	\textsf{IO}(ch_1,1) \joinop \textsf{IO}(ch_2,0) \\
	&\quad (\textsf{left}(0,vs) \|_{cs} \joinop \textsf{right}(1,ws)) \\ 
	\rightarrow & w = 1 \wedge v = 0 \wedge \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop\\
	&\qquad
	\textsf{IO}(ch_1,1) \joinop \textsf{IO}(ch_2,0) \joinop \textsf{tot}(|vs|) \\
	\rightarrow & \textsf{tot}(|v\cdot vs|) \\
\end{align*}
The first step expands the definition of \textsf{left} and \textsf{right}. The second step uses synchronization of \textsf{path} and \textsf{in}. The third and fourth steps use synchronization between \textsf{in} and \textsf{out}, obtaining the values of $w$ and $v$ in the process. The values are substituted when they are obtained. Finally, in the fifth step, we use the inductive hypothesis, and in the final step use the definition of \textsf{tot}. The final Hoare triple obtained is:
\begin{align*}
	\{ & [x\mapsto 0, y\mapsto 1] \} \\
	& (\evo{x}{1}{x < 1}; ch_1!x; ch_2?x)^* \| (ch_1?y; ch_2!(y-1))^* \\
	\{ & \exists n.\, \textsf{tot}(n) \}
\end{align*}

\subsection{Example with interrupt and loop}

We now consider the process
\[ (\evo{x}{1}{\textsf{true}}\lr (ch_1!x \to \pskip); ch_2?x)^* \]
This process increases $x$ indefinitely, except when output along channel $ch_1$ is possible. In that case, it sends $x$ through $ch_1$ and receives a new value of $x$ through $ch_2$. For the invariant, we define the function $\textsf{ileft}(b,ps)$, where $b$ is the initial value of $x$, and $ps$ is a list of pairs $(d,v)$, where $d$ is the delay before $ch_1$ is able to communicate, and $v$ is the value received on $ch_2$.
\begin{align*}
	\textsf{ileft}(a,[]) &= \textsf{emp} \\
	\textsf{ileft}(a,(d,v) \cdot ps') &=
	\textsf{waitOut}(d,[x\mapsto a+t], ch_1, x, \{ch1?\})  \\
	& \quad\ \joinop\textsf{in}([x\mapsto a+d], ch_2, v) \joinop \textsf{ileft}(v,ps')
\end{align*}

The final value of $x$ is either $b$ (when $ps$ is empty) or the value of $v$ in the last pair in $ps$:
\begin{align*}
	\textsf{last'}(a,[]) &= b \\
	\textsf{last'}(a,(d,v) \cdot ps') &= \textsf{last'}(v,ps')
\end{align*}

Applying the repetition rule and the above strongest postcondition rules, we get the following:
\begin{align*}
	\{ & s = [x\mapsto a] \wedge \textit{tr} = \epsilon \} \\
	& (\evo{x}{1}{\textsf{true}}\lr (ch_1!x \to \pskip); ch_2?x)^* \\
	\{ & \exists ps.\, s = [x\mapsto \textsf{last'}(a,ps)] \wedge \textsf{ileft}(a,ps) ]\}
\end{align*}
}

\oomit{
\subsection{Example with loop and parallel}

We demonstrate the above rules by showing a full example with both loop and parallel. Consider the following process:
\[ (x := x + 1; ch!x)^* \| (ch?y)^* \]
The left side has already been analyzed in Section \ref{sec:output-and-loops}. We repeat the result here:
\begin{align*}
	\{ & s = [x \mapsto a] \wedge \textit{tr} = \epsilon \} \\
	& (x:=x+1;ch!x)^* \\
	\{ & \exists n.\, s = [x\mapsto a+n] \wedge \textsf{countUp}(a,n) \}
\end{align*}
where \textsf{countUp} is defined as:
\begin{align*}
	\textsf{countUp}(a,0) &= \textsf{emp} \\
	\textsf{countUp}(a,n+1) &= \textsf{out}([x\mapsto a+1],ch,a+1) \joinop \\
	&\qquad\textsf{countUp}(a+1,n)
\end{align*}

The right side is a simpler version of the example in Section \ref{sec:input-and-loops}, the result is:
\begin{align*}
	\{ & s = [y \mapsto b] \wedge \textit{tr} = \epsilon \} \\
	& (ch?y)^* \\
	\{ & \exists xs.\, s = [y\mapsto \textsf{last}(b,xs)] \wedge \textsf{receive}(a,xs) \}
\end{align*}
where \textsf{receive} is defined as:
\begin{align*}
	\textsf{receive}(b,[]) &= \textsf{emp} \\
	\textsf{receive}(b,x\cdot xs) &= \textsf{in}([y\mapsto b],ch,x) \joinop \textsf{receive}(x,xs)
\end{align*}

To analyze the parallel program, we define the following assertion:
\begin{align*}
	\textsf{countUpIO}(a,0) &= \textsf{emp} \\
	\textsf{countUpIO}(a,n+1) &= \textsf{IO}(ch,a+1) \joinop\\
	&\qquad \textsf{countUpIO}(a+1,n)
\end{align*}

We prove the following by induction on $n$:
\[ \textsf{countUp}(a,n) \|_{ch} \textsf{receive}(b,xs) \rightarrow \textsf{countUpIO}(a,n) \]
\begin{proof}
	For the case $n = 0$, we divide into two cases by whether $xs$ is empty. If $xs$ is empty, then we have $\textsf{emp}\|_{ch}\textsf{emp} \Rightarrow \textsf{emp}$ as desired. Otherwise, we have $xs=x\cdot xs'$, and
	\[ \textsf{emp}\|_{ch} (\textsf{in}([y\mapsto b],ch,x) \joinop \textsf{receive}(x,xs')) \rightarrow \textsf{false} \]
	
	Now consider the induction case, with $n = k + 1$. Again, divide into two cases by whether $xs$ is empty. If $xs$ is empty, then we have
	\[ 
	\begin{array}{l}
		(\textsf{out}([x\mapsto a+1],ch,a+1) \joinop \textsf{countUp}(a+1,k)) \|_{ch} \textsf{emp} \\
		\qquad  \Rightarrow\textsf{false}
	\end{array} \]
	So the only remaining case is where $xs=x\cdot xs'$. Then the derivation is:
	\begin{align*}
		& \textsf{countUp}(a,k+1) \|_{ch} \textsf{receive}(b,x\cdot xs') \\
		\Rightarrow & (\textsf{out}([x\mapsto a+1],ch,a+1) \joinop \textsf{countUp}(a+1,k)) \|_{ch} \\
		& \qquad(\textsf{in}([y\mapsto b],ch,x) \joinop \textsf{receive}(x,xs')) \\
		\Rightarrow & x = a + 1 \wedge \textsf{IO}(ch,a+1) \joinop\\
		& \qquad (\textsf{countUp}(a+1,k) \|_{ch} \textsf{receive}(x,xs')) \\
		\Rightarrow & \textsf{IO}(ch,a+1) \joinop \textsf{countUpIO}(a+1,k) \\
		\Rightarrow & \textsf{countUpIO}(a,k+1)
	\end{align*}
\end{proof}
The first step expands the definitions. The second step uses one of the rules for synchronization of assertions. The third step uses the inductive hypothesis. The last step uses the definition of \textsf{countUpIO}. This completes the proof.\footnote{We can also obtain information about the list of received values $xs$, and hence the final state of the right process, by keeping the pure assertion $x=a+1$ at each step.}

Applying the parallel rule, we obtain the final result:
\begin{align*}
	\{ & s = [x\mapsto a, y\mapsto b] \wedge \textsf{emp} \} \\
	& (x := x + 1; ch!x)^* \| (ch?y)^* \\
	\{ & \exists b.\, \exists ys.\, s = [x\mapsto b, y\mapsto \textsf{last}(0,ys)] \wedge \textsf{countUpIO}(0) \}
\end{align*} 


\subsection{Example with ODE and loop}

We consider the following process:
\[ (\evo{x}{1}{x < 1}; ch_1!x; ch_2?x)^* \]
This process repeatedly increases $x$ to 1 (does nothing if $x$ is already at least 1), then send the value of $x$ via channel $ch_1$, and receive a new value of $x$ via channel $ch_2$. For the invariant, we define the function $\textsf{left}(a,vs)$, for the behavior of the process when the starting value of $x$ is $a$, and the sequence of values received via $ch_2$ is $vs$.
\begin{align*}
	\textsf{left}(a,[]) &= \textsf{emp} \\
	\textsf{left}(a,v\cdot vs') &= \mbox{if } a < 1 \mbox{ then} \\
	&\qquad \textsf{path}(1-a, [x\mapsto t + a]) \joinop \\
	&\qquad \textsf{out}([x\mapsto 1], ch_1, 1) \joinop
	\textsf{in}([x\mapsto 1], ch_2, v) \joinop \\
	&\qquad\textsf{left}(v,vs') \\
	&\quad \mbox{else } \\
	&\qquad \textsf{out}([x\mapsto a], ch_1, a) \joinop
	\textsf{in}([x\mapsto a], ch_2, v) \joinop \\
	&\qquad\textsf{left}(v,vs')
\end{align*}
The value of $x$ at the end is $\textsf{last}(a,vs)$. We also need to derive the expansion of $\textsf{left}(a,vs\cdot v)$. The details are omitted here. Applying the repetition rule and the above strongest postcondition rules, we get the following:
\begin{align*}
	\{ & s = [x\mapsto a] \wedge \textit{tr} = \epsilon \} \\
	& (\evo{x}{1}{x < 1}; ch_1!x; ch_2?x)^* \\
	\{ & \exists vs.\, [x\mapsto \textsf{last}(a,vs)] \wedge \textsf{left}(a,vs) \}
\end{align*}

Now, we consider its synchronization with a right side, given by:
\[ (ch_1?y; ch_2!(y-1))^* \]
If we assume that $x$ on the left process begins at 0, and $y$ on the right process begins at 1, then the left process behaves in the ``saw-tooth'' pattern, repeatedly increasing from 0 to 1 then suddenly dropping back to 0.

For the invariant on the right, we define the function $\textsf{right}(b,ws)$, for the behavior of the process when the starting value of $y$ is $b$, and the sequence of values received via $ch_1$ is $ws$.
\[
\begin{array}{rl}
	\textsf{right}(b,[])=&\textsf{emp} \\
	\textsf{right}(b,w\cdot ws')=&\textsf{in}([y\mapsto b],ch_1,w) \joinop\\
	&\textsf{out}([y\mapsto w],ch_2,w-1) \joinop \textsf{right}(w,ws')
\end{array}
\]
The value of $y$ at the end is $\textsf{last}(b,ws)$. The result of applying repetition rule and strongest postcondition rules is the following:
\begin{align*}
	\{ & s = [y\mapsto b] \wedge \textit{tr} = \epsilon \} \\
	& (ch_1?y; ch_2!(y-1))^* \\
	\{ & \exists ws.\, s = [y\mapsto \textsf{last}(b,ws)] \wedge \textsf{right}(b,ws) \}
\end{align*}

The final step is to compute the synchronization of the postconditions. The result is given by the following recursive function:
\[
\begin{array}{rl}
	\textsf{tot}(0)=&\textsf{emp} \\
	\textsf{tot}(n + 1)=&\textsf{path}(1,[x\mapsto t, y\mapsto 1]) \joinop
	\textsf{IO}(ch_1,1) \joinop \\
	&\textsf{IO}(ch_2,0) \joinop \textsf{tot}(n)
\end{array}
\]
The computation of synchronization follows the same technique as in Section \ref{sec:synchronize-assertion}. The lemma to be proved by induction is (taking $cs=\{ch_1,ch_2\}$):
\[ \textsf{left}(0,vs) \|_{cs} \textsf{right}(1,ws) = \textsf{tot}(|vs|) \]

We show the interesting part of it below:
\begin{align*}
	& \textsf{left}(0,v\cdot vs) \|_{cs} \textsf{right}(1,w\cdot ws) \\
	\Rightarrow & \textsf{path}(1, [x\mapsto t]) \joinop
	\textsf{out}([x\mapsto 1], ch_1, 1) \joinop\\
	&\qquad
	\textsf{in}([x\mapsto 1], ch_2, v) \joinop \textsf{left}(v,vs) \|_{cs} \\
	&\ \textsf{in}([y\mapsto 1],ch_1,w) \joinop
	\textsf{out}([y\mapsto w],ch_2,w-1) \joinop \\
	&\qquad\textsf{right}(w,ws) \\
	\Rightarrow & \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop \\
	&\quad (\textsf{out}([x\mapsto 1], ch_1, 1) \joinop
	\textsf{in}([x\mapsto 1], ch_2, v) \joinop \\
	&\qquad\textsf{left}(v,vs) \|_{cs} \\
	&\quad \ \textsf{in}([y\mapsto 1],ch_1,w) \joinop
	\textsf{out}([y\mapsto w],ch_2,w-1) \joinop \\
	&\qquad\textsf{right}(w,ws)) \\ 
	\Rightarrow & w = 1 \wedge \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop
	\textsf{IO}(ch_1,1) \joinop \\
	&\quad (\textsf{in}([x\mapsto 1], ch_2, v) \joinop\\
	&\qquad \textsf{left}(v,vs) \|_{cs} 
	\textsf{out}([y\mapsto w],ch_2,0) \joinop \textsf{right}(1,ws)) \\ 
	\Rightarrow & w = 1 \wedge v = 0 \wedge \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop\\
	&\qquad
	\textsf{IO}(ch_1,1) \joinop \textsf{IO}(ch_2,0) \\
	&\quad (\textsf{left}(0,vs) \|_{cs} \joinop \textsf{right}(1,ws)) \\ 
	\Rightarrow & w = 1 \wedge v = 0 \wedge \textsf{path}(1, [x\mapsto t, y\mapsto 1]) \joinop\\
	&\qquad
	\textsf{IO}(ch_1,1) \joinop \textsf{IO}(ch_2,0) \joinop \textsf{tot}(|vs|) \\
	\Rightarrow & \textsf{tot}(|v\cdot vs|) \\
\end{align*}
The first step expands the definition of \textsf{left} and \textsf{right}. The second step uses synchronization of \textsf{path} and \textsf{in}. The third and fourth steps use synchronization between \textsf{in} and \textsf{out}, obtaining the values of $w$ and $v$ in the process. The values are substituted when they are obtained. Finally, in the fifth step, we use the inductive hypothesis, and in the final step use the definition of \textsf{tot}. The final Hoare triple obtained is:
\begin{align*}
	\{ & [x\mapsto 0, y\mapsto 1] \} \\
	& (\evo{x}{1}{x < 1}; ch_1!x; ch_2?x)^* \| (ch_1?y; ch_2!(y-1))^* \\
	\{ & \exists n.\, \textsf{tot}(n) \}
\end{align*}

\subsection{Example with interrupt and loop}

We now consider the process
\[ (\evo{x}{1}{\textsf{true}}\lr (ch_1!x \to \pskip); ch_2?x)^* \]
This process increases $x$ indefinitely, except when output along channel $ch_1$ is possible. In that case, it sends $x$ through $ch_1$ and receives a new value of $x$ through $ch_2$. For the invariant, we define the function $\textsf{ileft}(b,ps)$, where $b$ is the initial value of $x$, and $ps$ is a list of pairs $(d,v)$, where $d$ is the delay before $ch_1$ is able to communicate, and $v$ is the value received on $ch_2$.
\begin{align*}
	\textsf{ileft}(a,[]) &= \textsf{emp} \\
	\textsf{ileft}(a,(d,v) \cdot ps') &=
	\textsf{waitOut}(d,[x\mapsto a+t], ch_1, x, \{ch1?\})  \\
	& \quad\ \joinop\textsf{in}([x\mapsto a+d], ch_2, v) \joinop \textsf{ileft}(v,ps')
\end{align*}

The final value of $x$ is either $b$ (when $ps$ is empty) or the value of $v$ in the last pair in $ps$:
\begin{align*}
	\textsf{last'}(a,[]) &= b \\
	\textsf{last'}(a,(d,v) \cdot ps') &= \textsf{last'}(v,ps')
\end{align*}

Applying the repetition rule and the above strongest postcondition rules, we get the following:
\begin{align*}
	\{ & s = [x\mapsto a] \wedge \textit{tr} = \epsilon \} \\
	& (\evo{x}{1}{\textsf{true}}\lr (ch_1!x \to \pskip); ch_2?x)^* \\
	\{ & \exists ps.\, s = [x\mapsto \textsf{last'}(a,ps)] \wedge \textsf{ileft}(a,ps) ]\}
\end{align*}
}

\section{Simplified assertions and proof rules}
\label{app:simplify}
\subsection{Simple Parameterized Assertions}
{\small 
\[ 
\begin{array}{c}
(s_0,s,tr)\models \mathsf{true} \longleftrightarrow \mathit{true} \\

(s_0,s,tr)\models \mathsf{false} \longleftrightarrow \mathit{false} \\

(s_0,s,tr)\models \mathsf{init} \longleftrightarrow s_0=s \wedge tr=\epsilon \\

(s_0,s,tr)\models P\wedge Q \longleftrightarrow (s_0,s,tr)\models P \wedge (s_0,s,tr)\models Q \\

(s_0,s,tr)\models P\vee Q \longleftrightarrow (s_0,s,tr)\models P \vee (s_0,s,tr)\models Q \\

(s_0,s,tr)\models \forall n.\, P(n) \longleftrightarrow \forall n.\, (s_0,s,tr)\models P(n) \\

(s_0,s,tr)\models \exists n.\, P(n) \longleftrightarrow \exists n.\, (s_0,s,tr)\models P(n) \\

(s_0,s,tr)\models P[v:=e] \longleftrightarrow (s_0(v:=e),s,tr)\models P \\
\end{array}
\]
}

\subsection{Entailment Properties}
Given two assertions $P$ and $Q$ on state and trace, we define the entailment between $P$ and $Q$ as follows:
\[
P\Longrightarrow Q \quad\triangleq\quad \forall s\ tr.\, (s,tr)\models P\longrightarrow (s,tr)\models Q
\]
Likewise, two assertions are equal if they hold on the exactly same state and trace. There are some common entailment rules, for example introduction and elimination rules for conjunction, disjunction, forall and exists. Of special notes are entailments related to monotonicity and substitution of assertions. We state some examples in the following subsections.

The assertions $\mathsf{wait\_in}$, $\mathsf{wait\_out}$, $\mathsf{wait}$, etc. all satisfy monotonicity rules, that reduce entailment relations among similar assertions to entailments on its components. For example, monotonicity of $\mathsf{wait\_in}$ take the following form:
\[
\prftree{\forall d\ v.\, P_1(d,v,s_0) \Longrightarrow P_2(d,v,s_0)}
{\mathsf{wait\_in}(I,ch,P_1,s_0)\Longrightarrow \mathsf{wait\_in}(I,ch,P_2,s_0)}
\]
This rule permits deducing entailment between two $\mathsf{wait\_in}$ assertions that differ only in the ensuing assertion. There are similar rules for changing the path assertion $I$.

In the above rule, the initial state $s_0$ is unchanged from assumption to conclusion. This is true for most assertions, with the important exception of substitution. For example, the monotonicity rule for substitution has the following form:
\[
\prftree{P_1(s_0[v:=e(s_0)]) \Longrightarrow P_2(s_0[v:=e(s_0)])}
{P_1[v:=e](s_0) \Longrightarrow P_2[v:=e](s_0)}
\]
This rule means in order to show an entailment between substitutions $v:=e$ on assertions $P_1$ and $P_2$, acting on initial state $s_0$, it suffices to show an entailment between $P_1$ and $P_2$, acting on initial state $s_0[v:=e(s_0)]$ (updating the value of $v$ to $e(s_0)$ on state $s_0$).
Performing substitution $[v:=e]$, $[f]_r$, or $[f]_e$ on an assertion such as $\mathsf{wait\_in}$ can be reduced to performing the same assertion on its components. This can also be interpreted as performing the syntactical substitution on the expression for the assertion. For example, the entailment rule for $\mathsf{wait\_in}$ is:
\[
\mathsf{wait\_in}(I,ch,P)[v := e](s_0) \Longrightarrow \mathsf{wait\_in}(I[v := e], ch, P[v := e])(s_0)
\]
Note the substitution on $I$ performs the replacement on the variables corresponding to the initial state $s_0$, same as the substitution on $P$.

The assertions $\mathsf{wait\_in}$, $\mathsf{wait\_out}$ and $\mathsf{wait}$ are all special cases of $\mathsf{interrupt}$ or $\mathsf{interrupt_\infty}$. The exact equations are as follows.
\[
\begin{array}{c}
\mathsf{interrupt_\infty}(I,[(ch!,P)]) = \mathsf{wait\_out}(I,ch,P) \\
\mathsf{interrupt_\infty}(I,[(ch?,P)]) = \mathsf{wait\_in}(I,ch,P) \\
\mathsf{interrupt}(I,e,P,[]) = \mathsf{wait}(I,e,P)
\end{array}
\]


\subsection{Elimination Rules}
Fig.~\ref{fig:elimination_rules} lists part of the synchronization rules of simplified assertions. They are consistent with Fig.~\ref{fig:elim-synchronization}. Since wait, input and output assertions are all special cases of Interrupt assertions, we just need the elimination rule between them and we can handle all situations in theory. 
We explain some of the rules in detail.

Rule $\mathsf{Intinf\_Intinf1}$ describes the synchronization of two infinity interrupt assertions in the condition that their rdy set are compatible which means that there is no communication between the two sides and odes on both sides continue to evolve until one of them is interrupted by an external communication. Thus, the synchronization
produces a new infinity interrupt assertion and a complicated list of communications. 
$\specs|_{chs^{c}}$ picks the element of $\specs$ whose channel is not in $chs$ and then forms into a new list.\\
\noindent
For each $\specs1|_{chs^{c}}[i]=\langle ch\triangleright,Q\rangle$
\[
\begin{array}{ll}
     &\mathsf{rel1}(\specs1|_{chs^{c}},\mathsf{interrupt_\infty}(I_2,\specs_2))[i]=  \\
     & \quad \langle ch\triangleright,\{(d,v)\Rightarrow sync(chs,Q(d,v),\\
     & \qquad\mathsf{interrupt_\infty}(\mathsf{delay\_inv}(d,I_2),\mathsf{delay\_specs}(d,\specs_2)))\}\rangle
\end{array}
\]
For each $\specs2|_{chs^{c}}[i]=\langle ch\triangleright,Q\rangle$
\[
\begin{array}{ll}
     &\mathsf{rel2}(\specs2|_{chs^{c}},\mathsf{interrupt_\infty}(I_1,\specs_1))[i]=  \\
     & \quad \langle ch\triangleright,\{(d,v)\Rightarrow sync(chs,\mathsf{interrupt_\infty}(\mathsf{delay\_inv}(d,I_1),\\
     & \qquad\mathsf{delay\_specs}(d,\specs_1)),
      Q(d,v))\}\rangle
\end{array}
\]
For each $\specs[i]=\langle ch\triangleright,Q\rangle$
\[
\mathsf{delay\_specs}(d',\specs)[i]=\langle ch\triangleright,\{(d,v)\Rightarrow Q(d+d',v)\}\rangle
\]

Rule $\mathsf{Intinf\_Intinf2}$ describes the synchronization of two infinity interrupt assertions in the condition that their rdy set are incompatible which means that the interaction between the two sides should occur immediately or one side is interrupted by an external 
produces within 0 delay time. Thus the synchronization
produces an interrupt assertion which has a delay time bound of 0 and the same communication list as in $\mathsf{Intinf\_Intinf1}$. The remaining behaviour should describe all the possible handshake.
$\mathsf{comm}(\specs1,\specs2)$ is a disjunction of $\exists v.\mathsf{sync(chs,P(0,v),Q(0,v))}$ for
all the pair satisfying 
\[ch\in chs \land \specs1[i]=\langle ch?,P\rangle \land \specs2[j]=\langle ch!,Q\rangle\]
or 
\[
ch\in chs \land \specs1[i]=\langle ch!,P\rangle \land \specs2[j]=\langle ch?,Q\rangle\]

Rule $\mathsf{Int\_Int1}$ has the similar idea with $\mathsf{Intinf\_Intinf1}$ while in addition, we need to consider and compare the maximum ode duration between the two sides. \\
\noindent
For each $\specs1|_{chs^{c}}[i]=\langle ch\triangleright,P\rangle$
\[
\begin{array}{ll}
     &\mathsf{rel1}(\specs1|_{chs^{c}},\mathsf{interrupt}(I_2,e_2,Q,\specs_2))[i]=  \\
     & \quad \langle ch\triangleright,\{(d,v)\Rightarrow sync(chs,P(d,v),
      \mathsf{interrupt}(\mathsf{delay\_inv}(d,I_2),e2-d,\\
     &\qquad\{d'\Rightarrow Q(d'+d)\},\mathsf{delay\_specs}(d,\specs_2)))\}\rangle
\end{array}
\]
For each $\specs2|_{chs^{c}}[i]=\langle ch\triangleright,Q\rangle$
\[
\begin{array}{ll}
     &\mathsf{rel2}(\specs2|_{chs^{c}},\mathsf{interrupt}(I_1,e_1,P,\specs_1))[i]=  \\
     & \quad \langle ch\triangleright,\{(d,v)\Rightarrow sync(chs,\mathsf{interrupt}(\mathsf{delay\_inv}(d,I_1),e_1-d,\\
     & \qquad\{d'\Rightarrow P(d'+d)\},\mathsf{delay\_specs}(d,\specs_1)),
      Q(d,v))\}\rangle
\end{array}
\]

Rule $\mathsf{Int\_Int2}$ is similar with $\mathsf{Intinf\_Intinf2}$ while we have take into consideration the situation that the time bound may be 0.
\ommit{
Next we prove the completeness of the differential inavariant rule  in HCSP.

\begin{proof}[Proof for Completeness of Invariants]
 The completeness of the invariant means that, 
$\spec{\textit{INV}\wedge P}{\evo{x}{e}{B}}{ \closeb(B) \wedge \closeb(\neg B) \wedge \textit{INV}\wedge P\joinop \textsf{trInv}(d,\textit{INV})}$, denoted by $ASS$, derives in HHL, iff the premises $B \wedge INV \wedge \mathcal{L}^{*}_{\vec{e}}(B) \rightarrow \mathcal{L}^{*}_{\vec{e}}(INV)$ and $B \wedge \neg INV \wedge \mathcal{L}^{*}_{-\vec{e}}(B) \rightarrow \mathcal{L}^{*}_{-\vec{e}}(\neg INV)$, denoted by $PREs$, hold. $ASS$ 
defines that $INV$ holds initially, and during the execution of ODE, $INV$ is maintained as an invariant, indicated by $\textsf{trInv}(d,\textit{INV})$. Thus, $ASS$ is equivalent to the corresponding specification of~\cite{PlatzerT20}, and we can directly inherit the proof of ~\cite{PlatzerT20} here.  The proof of ~\cite{PlatzerT20} is given based on  the lemmas on continuous existence, uniqueness, and differential adjoints etc. All these lemmas also hold in our case as we require that all ODEs satisfy the local Lipschitz condition. Thus the proof of ~\cite{PlatzerT20} still holds for our case. 

\end{proof}
}

 


\end{document}