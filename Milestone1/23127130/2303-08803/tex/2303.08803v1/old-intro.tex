
% \subsection{Motivation}
Many scientific challenges are solved using multiple codes each with different requirements and use patterns, 
such that a single computational resource may not be
optimal or even sufficient. 
% Workflows that intertwine simulation and machine learning are an illustrative example good example of campaigns that are best distributed across multiple sites.
As an illustrative example, consider ``active learning'' workflows in which machine learning computations are used to determine which simulation tasks to execute~\cite{lookman2019review}.
Simulation tasks may require systems optimized for tightly coupled computations that span multiple nodes---exactly the use case for supercomputers.  % Old detail about task duration - , and active learning workflows maintain a constant stream of such tasks to execute
Machine learning tasks, in contrast, may run best on specialized accelerators and often perform well on a single node---characteristics are not served by supercomputers.
Similar application types (e.g., online post-processing, visualization) are other examples with diverse computing needs between tasks, and we expect more opportunities for ``multi-site computing'' -- which use multiple facilities or clusters within a single facility -- to emerge as cloud computing and hardware continue to diversify. 
However, building multi-site applications is difficult due to challenges with networking and 
co-scheduling of workloads.

% \subsection{Limitations of Prior Approaches}

%\ian{could mention: Condor-G~\cite{frey2002condor} and even MPICH~G~\cite{foster1998grid}.}
Existing workflow systems are certainly capable of being deployed across multiple sites but can be complicated to configure, and spreading them across multiple sites introduces more opportunities for inefficiencies or failure~\cite{frey2002condor, foster1998grid}.
Modern workflow systems typically place services on each site that pull work from a central database (e.g., Fireworks~\cite{jain2015fireworks}, RCT\cite{radical2019}) %Balsam\cite{salim2019balsam}) 
or establish socket connections that a central controller uses to push work to that remote site (e.g., Parsl~\cite{babuji19parsl}).
% \logan{I was considering adding a Balsam cite for databases, and RCT for sockets. Is that right, Kyle?}
% \ian{There is a large literature on how to distribute work among processors. Manager push or worker pull, as cited already, are two (although certainly Fireworks and Parsl are not the first instances). Work stealing is a popular approach: e.g., \cite{rudolph1991simple}. Fully decentralized approaches are also considered, e.g., P2P \cite{milojicic2002peer} and some old work from Globus Labs \cite{ranganathan2002decoupling}.}
% \kyle{Balsam uses database, RCT uses mongo I think}
Both approaches require configuring SSH tunnels or opening ports between sites, which adds complexity and unreliability or is prohibited by security policies of many computing sites.
Sending large data objects or function inputs between sites can also be a bottleneck unless the workflow is specifically configured to use optimized solutions for transferring data across multiple sites.
In short, it is not that multi-site computing is inaccessible or indeed unknown to science~\cite{brunett1998implementing,brace2021stream} %trifan2021intelligent}. 
Rather,
the challenge is that current approaches are inflexible or require significant expertise to implement well.

The other difficulty exacerbated by deploying workflows across multiple sites is scheduling to achieve maximal scientific results for minimal cost.
For example, the time-to-solution versus cost trade-off is evident in the machine learning tasks of active learning campaigns.
Waiting longer between model retraining tasks conserves accelerator resources but may result in lower-value simulations being deployed on the supercomputer.
%Modern active learning algorithms also allow for machine learning tasks to be performed asynchronously with simulation tasks, further complicating scheduling decisions~\cite{kandasamy2018thompson}.
Achieving an optimal retraining frequency requires being reactive to application behavior at runtime (e.g., retraining when a certain number of tasks complete), a behavior that can be hard to express when using workflow systems that require task dependencies to be defined before launch.
Similar examples of tradeoffs and needs for dynamically adjusting workflows at runtime certainly exist for other types of computational campaigns, and the space of strategies for exploiting these tradeoffs is relatively unexplored.
Workflow systems must be made more flexible to be able to express such strategies and realize fully the benefits of multi-site computing.
\ian{I wonder if there is a useful point to be made here about the need for ``reflection'' (still pondering if that is the right word). 
Some applications have a fixed, ``deterministic,'' structure that can, e.g., be expressed as a static DAG. For others, the next step to be undertaken depends on the results of past steps. 
The text somewhat implicitly critiques DAG-based workflow managers. But there are certainly more dynamic systems, although I am blanking on what to suggest right now.
I think that \cite{wozniak2018candle} is relevant.
And this looks relevant too \url{https://doi.org/10.1021/acs.jcim.8b00386}.
}
\logan{That notion of reflection is something that our ensemble steering team hasn't been able to put a finger on either. I think of it as not being able to define the entire DAG at the beginning of a workflow}


% How should our degree of synchronization change as we adjust the computational resources or in light of the cost of starting or stopping resources?
% These questions are still being explored and more data is needed to help shape the development of algorithms to run multi-site campaigns effectively.
% Once developed, tools that simplify implementing optimized policies will become important.
% \greg{Should we say we explore these questions and develop a tool that simplifies the implementation of these policies?}

% \subsection{Key Insights and Contributions}

In this work, we present a collection of tools that are purpose-built to bypass the networking challenges and simplify co-scheduling multi-site computational campaigns. 
Our approach to multi-site computing combines an existing Function-as-a-Service (FaaS) system (\funcx{}~\cite{chard2020funcX}) with a  tool for writing dynamic workflows (\colmena{}), and a new system for directly sending data between steering and worker processes (\proxystore{})  with a data transfer system optimized for moving data between sites (\globus{}~\cite{chard2014globus}).\footnote{\colmena{} is a pseudonym introduced to maintain anonymity for the double-blind review.}
Combined, the tools enable deploying campaigns with minimal network configuration and allow for expressing the complex scheduling policies needed for efficient multi-site execution.
We illustrate the effectiveness of this system by implementing two applications with our toolkit and conventional workflow systems.
We then characterize the efficiency and usability of the system and identify research directions needed to further improve multi-site computation.

% \subsection{Methodology and Artifact Availability}
% % \ryan{This section wants: cite that most relevant and most recent prior works have evaluated their ideas using similar methodology.}\logan{Oops, I did not check the instructions}
% All of our software is open-source and under permissive licenses.
% The full complement of scripts needed to reproduce our results and the raw data used when producing figures will be made available after the review period of this manuscript.
% Our evaluation approach draws upon practices followed by other projects that have evaluated ML-driven simulation processes in similar ways, such as DeepDriveMD~\cite{lee2019deepdrivemd} and Proxima~\cite{zamora2021proxima}. 


% \subsection{Limitations of the Proposed Approach}
% \kyle{Move this somewhere?} \greg{I just removed... doesn't seem necessary}
% Our primary limitation is that this study is focused on a single type of application, active learning, implemented for a specific problem, molecular design.
% Other applications have different computation time and data size characteristics, which will significantly affect the measured efficiency of our tools.
% Consequently, one limitation of our conclusions is that we have yet to fully understand the limitations.
% With the data available, we can conclude that our system will work most efficiently with tasks that complete at a rate slower than once per second and with data transfer times shorter than \num{10}~s (e.g., 1~GB at 100~MB/s), given the latencies observed in our current application.
% Additionally, setup of our system requires configuring \funcx{} and \globus{} endpoints for each site---a cost that must be paid once per site but can be nontrivial.