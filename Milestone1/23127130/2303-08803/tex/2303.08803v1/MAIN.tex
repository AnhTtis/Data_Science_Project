\documentclass[conference]{IEEEtran}

\usepackage{cite}  
\usepackage{siunitx}
\sisetup{detect-all}
\usepackage{xspace}
\usepackage{graphicx}  
\usepackage{multirow}
\usepackage{listings}
\usepackage{balance}
%\usepackage[numbers]{natbib}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}

% Makes citations look better 
\renewcommand{\citepunct}{,\penalty\citepunctpenalty\,}
\renewcommand{\citedash}{--}% optionally

\makeatletter
  \def\sectionautorefname{\S\@gobble}
  \def\subsectionautorefname{\S\@gobble}
  \def\subsubsectionautorefname{\S\@gobble}
  \def\figureautorefname{Fig.}
  \def\subfigureautorefname{Fig.}
  %\def\sectionautorefname{Section}
  %\def\subsectionautorefname{Section}
  %\def\subsubsectionautorefname{Section}
  %\def\figureautorefname{Fig.}
  %\def\subfigureautorefname{Fig.}
\makeatother

% Python style for highlighting
\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{PythonStyle}{
	language=Python,
	backgroundcolor=\color{backgroundColour},
	commentstyle=\color{mGreen},
	keywordstyle=\color{mPurple},
	numberstyle=\tiny\color{mGray},
	stringstyle=\color{mGreen},
	basicstyle=\ttfamily\linespread{0.9}\scriptsize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2,
    frame=lrtb
}
\lstset{style=PythonStyle}


\newif\ifdraft
%\drafttrue
\ifdraft
  \newcommand{\ian}[1]{{\textcolor{red}{ Ian: #1 }}}
  \newcommand{\logan}[1]{{\textcolor{blue}{ Logan: #1 }}}
  \newcommand{\ryan}[1]{{\textcolor{magenta}{ Ryan: #1 }}}
  \newcommand{\greg}[1]{{\textcolor{purple}{ Greg: #1 }}}
  \newcommand{\rajeev}[1]{{\textcolor{orange}{ Rajeev: #1 }}}
  \newcommand{\ganesh}[1]{{\textcolor{green}{ Ganesh: #1 }}}
  \newcommand{\kyle}[1]{{\textcolor{teal}{ Kyle: #1 }}}
  \newcommand{\yadu}[1]{{\textcolor{brown}{ Yadu: #1 }}}
  \newcommand{\nathaniel}[1]{{\textcolor{cyan}{ Nathaniel:  #1 }}}
\else
  \newcommand{\ian}[1]{}
  \newcommand{\logan}[1]{}
  \newcommand{\ryan}[1]{}
  \newcommand{\greg}[1]{}
  \newcommand{\rajeev}[1]{}
  \newcommand{\ganesh}[1]{}
  \newcommand{\kyle}[1]{}
  \newcommand{\yadu}[1]{}
  \newcommand{\nathaniel}[1]{}
\fi

\newcommand{\funcx}{{FuncX}}
\newcommand{\colmena}{{Colmena}}
\newcommand{\globus}{Globus}
\newcommand{\proxystore}{ProxyStore}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}  % Use to force et al.

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
    %\title{Seamless Computational Campaigns across Multiple Computing Clusters with Function-as-a-Service and~Object~Proxies}
%\title{Efficient Computational Campaigns across Heterogeneous Computers with Less Effort through Function-as-a-Service and Object Proxies}
\title{Cloud Services Enable Efficient AI-Guided Simulation Workflows across Heterogeneous Resources}
%\input{authors-ieee}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\input authors-ieee

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows 
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{} %Ward and Pauloski, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
Applications that fuse machine learning and simulation can benefit from the use of multiple computing resources, with, for example, simulation codes running on highly parallel supercomputers and AI training and inference tasks on specialized accelerators.
%Applications that fuse machine learning and simulation may not be best served by a single computing resource.
%Highly parallel simulation codes are best deployed on supercomputers, while AI tasks used to decide which simulations to perform may be best suited to specialized accelerators.
Here, we present our experiences deploying two AI-guided simulation workflows across such heterogeneous systems. % to ensure each application runs on the best resource.
A unique aspect of our approach is our use of cloud-hosted management services to manage challenging aspects of cross-resource authentication and authorization, function-as-a-service (FaaS) function invocation, and data transfer.   
%We simplify deployment by relying on externally maintained cloud services to communicate across resources: Function-as-a-Service (FaaS) for task information and Transfer services for moving task data.
We show that these methods can 
%enhance addition to convenience, our approach 
achieve performance parity with systems that rely on direct connection between resources.
We achieve parity by integrating the FaaS system and data transfer capabilities with a system that passes data by reference among managers and workers, and a user-configurable steering algorithm to hide data transfer latencies.
We anticipate that this ease of use can enable routine use of  heterogeneous resources in computational science.


%Through studying this application, we identify sending large data objects as a key limit in the responsiveness of our workflow and show that we can lower the time needed to act on computations completing to seconds by directly sending data between sites with \globus{}.
%\greg{I rephrased this last sentence to be more general because focusing on data communication feels like leaving out a lot.} \kyle{do we evaluate ease-of-use?} \greg{moved discussion to comment}
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{IEEEkeywords}
Heterogeneous Computing, Function-as-a-Service, Machine Learning, Distributed Systems, Computational Steering
\end{IEEEkeywords}

\section{Introduction}
% \logan{Restructure to go
% ``multi-site computing is awesome... give some examples of why it is important ... but has issues around doing it securely and moving data between,'' ``FaaS systems are emerging as the preferred solution,'' ``a FaaS system is only one part of the equation (using it effectively requires...),'' ``we describe our experiences expanding two types of workflows to multi-site'' }

Scientific applications that employ AI models often require or can benefit from the use of multiple, heterogeneous computing resources. 
%\logan{Too strong of a claim about AI and multi-resources?}
For example, an application that uses AI-based perception to process data from a scientific instrument may engage a remote, more powerful computer for expensive tasks~\cite{vescovi2022linking},
%Some coordinate with instruments that are housed at facilities separate from sufficient resources.
while an AI-guided simulation application may run simulation codes on a conventional high performance computing (HPC) system but perform AI model training and inference on specialized accelerators~\cite{brace2021stream}.
While such multi-resource (or even multi-site) applications are far from new~\cite{smarr1992metacomputing,brunett1998application}, they are arguably becoming more commonplace due to more specialized computer architectures (especially for AI applications), increasingly portable code, and faster and more reliable networks.

The challenges around deploying a single application on multiple resources are well known.
%ensuring security and timely data transfer.
% Existing workflow systems are certainly capable of being deployed across multiple sites but can be complicated to configure, and spreading them across multiple sites introduces more opportunities for inefficiencies or failure~\cite{frey2002condor, foster1998grid}.
Modern workflow systems typically place services on each resource (e.g., pilot jobs) that connect back to a controller through an open port on a central server or via a secure tunnel.
However, this approach requires either maintaining a server or establishing secure tunnels to each resource, introducing deployment complexity and single points of failure.
Hybrid software-as-a-service approaches, as used in Globus~\cite{chard2014globus} and \funcx{}~\cite{chard20funcx}, 
%The FuncX function-as-a-service (FaaS) system 
reduce deployment complexity and increase reliability
%by  approach, an emerging paradigm for remote computing~\cite{spillner2017faaster}, reduces deployment complexity by
by using a cloud-hosted coordination service to manage authentication and respond appropriately to errors.
%FaaS systems are well understood in the context of proprietary clouds, but their application in HPC centers and in federated environments is relatively new. 

% Both approaches require configuring SSH tunnels or opening ports between sites, which adds complexity and unreliability or is prohibited by security policies of many computing sites.
% Function-as-a-service (FaaS) approaches, in which task information is communicated between sites from a central cloud platform\ian{ I don't recognize this as a description of FaaS. You seem to be describing how the original \funcx{} works?}, provide a potential solution to these challenges. FaaS deployments are highly available and robust via deployment on cloud-hosted infrastructure, implement best-practices security frameworks, and can execute diverse programming functions. 
A second challenge in multi-resource deployments is to mitigate the costs of data transfer---a critical need in data-heavy AI applications.
Most multi-resource workflows communicate data between resources via the workflow controller or a shared data store like a database or shared file system.
Complexity arises, however, when data sizes increase and overwhelm the controller or resources differ in their access to shared data transfer mediums.
Passing data by reference and employing peer-to-peer data transfers can reduce strain on the workflow controller (e.g., Brace et al.\cite{brace2021stream}).
% Fireworks~\cite{jain2015fireworks} includes file locations with task results so file transfer can occur separately from the notification that a task is complete.
Separation allows data transfer via more efficient mechanisms (e.g., Parsl~\cite{babuji19parsl} uses Globus).
Subsystems that enable robust and flexible data movement by reference are key to the efficacy of multi-resource workflows.
% The general strategy in most multi-resource workflows (e.g., Brace et al.\cite{brace2021stream}) is to avoid passing large data through the workflow controller, instead passing large data by reference and employing peer-to-peer data transfers.
% For example, file locations are sent along with task results in some workflow engines so the files can be transferred separately from the notification that tasks are completed (e.g., in Fireworks~\cite{jain2015fireworks}).
% The files can then be transferred via more efficient paths or mechanisms (e.g., Parsl uses Globus~\cite{babuji19parsl}).
% % and also directly between resources running each part of a calculation.
% Thus, subsystems for passing data by reference are a crucial component for future systems of AI transfer
% and also another route where deployment complexity often arises.
% 

% Achieving the FaaS model of computing with scientific applications running on HPC offers unique challenges, namely due to the size and frequency of data transfers.
% A conventional FaaS model relies on a third-party broker (e.g., a cloud provider) to communicate tasks, 
% which introduces significant latency and cost when large data are involved.
% Rather, we desire the peer-to-peer communication models (e.g., SSH tunnels, GridFTP) used in existing multi-site applications based on conventional workflow systems.

We describe here our successes in deploying two multi-resource applications (molecular design and surrogate training) with approaches that maximize performance and minimize deployment challenges.
Both applications require using CPU resources to run simulation tasks and GPU resources to achieve timely solutions for AI tasks but vary in the frequency, duration, and data transfer requirements for each type of task. 
Our implementation uses cloud-managed services where possible to ensure reliability and simplify deployment of both secure function execution and data transfer between resources.
Specifically, we chose a federated FaaS platform (\funcx{}) and a system (\proxystore{}) that enables peer-to-peer data transfer via \globus{} with minimal changes to application code.
We first characterize the performance of these tools using a synthetic application and then describe how we achieved performance parity between our cloud-managed solution and one using a conventional workflow system, Parsl. 

% \subsubsection{New text}
% \logan{I think we should dive right in to FaaS here}
% The ``Function-as-a-Service`` (FaaS) model in cloud computing offers idealized characteristics of a subsystem for multi-resource computing. \logan{Is FaaS the right pitch to make here? Or, are we talking about something more specialized?}
% As soon as the functions which are part of a workflow are defined, they can be executed on remote resources with minimal 


% \kyle{We could potentially list the challenges of remote computing and then describe 
% how we address each. E.g., we need to
% 1. securely connect to a remote computer (typically via SSH or ad hoc services)
% 2. execute work on a remote computer (typically via workflow system, pilot job)
% 3. ensure the environment is consistent (typically via modules, containers, conda)
% 4. move data from client to remote computer, and ideally between computers (typically via scp, rsync)
% }

% \subsubsection{Previous Text}\logan{Seems too focused on the workflow systems. Abrupt transition to}


% \ian{I wonder if the framing can be more focused. 
% The title says that the paper is about AI-guided simulation workflows.
% The start of the intro says it is about applications that require heterogeneous resources.
% Later parts of the intro emphasize FaaS.
% It seems that it is a combination of each:
% workflow/FaaS in that the AI-guided simulations involve collections of tasks that evolve dynamically over time;
% FaaS because that is a convenient way to express such collections of tasks.
% Then the data issues arise because when tasks are deployed dynamically at different locations, they need means of exchanging information. 
% }
% \logan{I'm thinking of adding an emphasis on why to care about multi-resource for }



%\ian{Need to be careful how we express state of things. There must be 100s of papers on multi-site applications, dating back at least to the early 1990s~\cite{smarr1992metacomputing,brunett1998application}. But certainly it has not become mainstream, and it was not easy in the early days. So describing new motivation applications, new technologies, and new results is good.}



\section{Related Work}\label{sec:background}
Many capabilities are needed to implement multi-resource computational campaigns.
Specifically, workflow systems must
1) execute tasks on resources acquired from multiple facilities; 
2) exchange large task data between resources;
3) express policies that avoid latencies inherent to orchestrating multiple systems.
%Fortunately, many of the required features are available as part of existing software.
Here, we review the technologies available for each need.


\subsection{Coordinating Remote Execution}

Remote computing has often been performed via SSH connections. % to remote resources. %and local provisioning of resources.
Grid computing introduced remote connectivity and web services to manage remote execution, for example via Globus Grid Resource Allocation Manager (GRAM)~\cite{feller2007gt4}. Recently, facilities such as NERSC and TACC have developed web APIs for submitting and managing batch jobs with Newt~\cite{cholia10newt} and Tapis~\cite{stubbs21tapis}, respectively. These approaches 
add each new task to a global queue, which can result in significant delays due to the need to wait for batch jobs to be scheduled in applications where new tasks are created dynamically.

Many workflow systems have been developed to execute sets of tasks on both local and remote computing resources. 
Systems such as Pegasus~\cite{deelman19pegasus}, Dask~\cite{rocklin2015dask}, Parsl~\cite{babuji19parsl}, Swift~\cite{zhao07swift}, and 
Galaxy~\cite{afgan22galaxy} differ in various ways, but all provide:  
1) a method for describing a workflow;
2) a data model for representing dependencies among tasks (e.g., a DAG); and
3) a runtime system for executing tasks on local and/or remote resources.
% typically via a static specification (e.g., YML), graphical user 
% interface, or directly in a domain specific language or existing 
% programming language. They include .
When dispatching tasks to HPC, 
such systems often employ multi-level scheduling schemes that map individual tasks to workers deployed on HPC resources, an approach that 
%Many systems provide the capability for remote execution,
%either via a central database (e.g., RADICAL-Cybertools~\cite{bala19radical}) via which
%workers can retrieve work or via SSH connections to remote systems (e.g., Parsl and Swift). 
%Remote workers can run multiple tasks per batch job, 
allows dynamically generated tasks
timely access to computing resources~\cite{bala19radical}.


%\logan{This feels like a paragraph we could cut. I tried to get the ``multiple tasks per batch job'' idea into the previous para}
% Many workflow systems leverage the pilot job model to manage execution
% of tasks on resources. In this model, 
% a software agent (called a pilot) is deployed on nodes provisioned
% as part of a batch job. The pilots call back to a central coordinator
% to retrieve tasks to be executed. The advantage of this approach is 
% that workloads can be dynamically assigned to nodes with finer
% granularity than a traditional batch job. Several pilot systems have been
% developed independent from a workflow system that can be used to manage
% remote execution, these include WorkQueue~\cite{albrecht13wq}, RADICAL-Pilot, and Parsl's High
% Throughput Executor. The Flux scheduler applies a similar model, deploying
% hierarchical jobs within jobs while exposing the same batch scheduler 
% interface at each level. 

%\logan{Could use a better transition between workflow systems and this paragraph}

The serverless or FaaS paradigm popularized by hosted cloud platforms such as Amazon Lambda~\cite{amazonlambda},
Google Cloud Functions~\cite{googlecloudfunctions}, and Azure Functions~\cite{azureFunctions} is an attractive model for remote scientific computing as it 
allows large applications to be broken down into smaller, efficient, 
and adaptable functions~\cite{foster2017cloud,spillner2017faaster,malawski2016towards,fox2017conceptualizing,kiar2019serverless}. 
However, the commercial FaaS offerings are proprietary systems
that are restricted to a single cloud provider and do not support
execution of functions on existing cyberinfrastructure. 
%they are not designed
%to support the diversity of research cyberinfrastructure, which use batch schedulers, heterogeneous hardware, differing container technologies, and rigid authentication models.

Open source frameworks (e.g., Apache OpenWhisk~\cite{openwhisk}, Fn~\cite{Fn}, KNIX MicroFunctions~\cite{knix},  Abaco~\cite{garcia2020abaco}) can be used for on-premise deployments.
%and be customized to address unique use cases. 
However, most %systems we surveyed
use Docker and rely on Kubernetes to operate---an assumption that prohibits
use on HPC systems that employ batch schedulers.
Some systems, such as ChainFaaS~\cite{ghaemi2020chainfaas} and DFaaS~\cite{ciavotta2021dfaas}, support distributed function execution on personal computers and edge nodes. 
% However, to the best of our knowledge, these systems do not support remote execution over a federated ecosystem of endpoints on diverse research cyberinfrastructure spanning HPC to the edge.
%To the best of our knowledge, 
\funcx{}~\cite{chard20funcx} is the only system that supports remote execution on a federated ecosystem of endpoints in a diverse research cyberinfrastructure spanning from HPC to edge.
% To address these challenges, we employ \funcx{} to provide a federated FaaS model that enables large-scale remote computing across resources, both centrally and at the edge.

\subsection{Inter-Resource Data Fabrics}

Workflow systems that couple diverse applications in distributed environments require a data fabric which provides consistent access to data regardless of location.
In the tuple space model, originating in Linda~\cite{carriero1994linda}, data producers and consumers use put and get operations on a shared distributed-memory space.
Dataspaces~\cite{dataspaces2017aktas} supports large-scale, dynamic scientific applications via a tuple-space-like model implemented using the Margo and Mercury RPC libraries~\cite{soumagne2013mercury, ross2020mochi}.
WA-Dataspaces~\cite{aktas2017wa} provides predictive prefetching and data staging support in wide area networks.

Multi-site workflows require a secure data fabric accessible by hosts behind different firewalls, features not supported by Linda, Dataspaces, or WA-Dataspaces.
SSH tunnels can enable secure communication but can be cumbersome to establish and fragile to maintain.
Science DMZs~\cite{data2013sciencedmz} provide secure sub-networks that span sites without firewalls but are only available at select computing sites.
SciStream~\cite{chung2022scistream} uses gateway nodes provided by computing sites for fast memory-to-memory data streaming for high throughput science applications.
Cloud services provide higher availability than the aforementioned but add additional network hops (which adds latency), fail to take advantage of high-performance connections between sites, and can be cost prohibitive.

\subsection{AI-Integrated Workflows}
AI-integrated workflows are emerging as a powerful tool across many scientific domains, with many use patterns~\cite{jha2022AIHPC, ejarque2022aiworkflows}.
Following the lexicon of Jha et al.~\cite{jha2022AIHPC}, there are at least ``ML-in-HPC'' applications where AI-based software are used inside conventional HPC applications (e.g., surrogates for expensive computational routines~\cite{atlas2022AtlFast3}) and ``ML-out-HPC'' where machine learning controls the execution of the application (e.g., steering a workflow via active learning~\cite{dunn2019rocketsled, montoya2020camd, lee2019deepdrivemd}).
The large diversity of application types is reflected in the many tools that support integration of AI in existing scientific codes or the creation of entirely new classes of applications.
Most relevant to our work are variants that use AI to determine the next task to execute in a workflow.

Systems for steering AI-guided workflows vary in how they express coordination between the workflow and the intelligence system.
Systems such as Supervisor~\cite{wozniak2018supervisor} and DeepHyper~\cite{balaprakash2018deephyper}
use a model where a single process running a steering algorithm receives results and submits new tasks to the workflow via a queue. 
In libEnsemble~\cite{libEnsemble}, a workflow is expressed as \textit{simulation} tasks that produce data, 
\textit{generator} tasks that produce new tasks,
and \textit{allocator} tasks that determine when to launch each type of task.
Ray~\cite{mortiz2018ray} and Decaf~\cite{yildiz2021decaf} allow a decentralized model where the steering logic is expressed in an agent-based programming model.
%\logan{I'd like to write about PyCOMPS, but need to study it more} \kyle{I dont think we need to include it here, it is basically like Parsl}

%\logan{This seemed not particularly useful}
% Workflow steering systems also vary in how they distribute computations across resources.
% Some are closely integrated with specific workflow systems, such as Rocketsled with Fireworks~\cite{dunn2019rocketsled}.
% \colmena{} integrates with Parsl and \funcx{}. 
% libEnsemble integrates with Balsam~\cite{salim2019balsam} and also uses \funcx{} for multi-site deployments. 
% The variety in programming models and advantages of the underlying workflow engines yield a rich suite of options for scientific applications.

\section{Motivating Applications}

\begin{figure}
    \centering
    \includegraphics[trim=1mm 0mm 1mm 0mm,clip]{figures/design/resource-utilization-and-data.pdf}
    \includegraphics[trim=1mm 0mm 1mm 0mm,clip]{figures/surrogate/resource-utilization-and-data.pdf}
    \caption{Resource utilization for our example applications. We show the number of tasks running and the cumulative data transfered to each resource over time. Data were collected using 20 T4 GPUs and 8 KNL processors, with a workflow based on Parsl without pass-by-reference.}
    \label{fig:apps}
\end{figure}

We study two applications that require CPU resources for simulation tasks and GPU resources for AI tasks. 
Each have different coordination patterns and data transfer requirements, as shown in \autoref{fig:apps}.
The first application, Molecular Design, consistently runs tasks on a GPU and transfers O(10)~GB
per batch of AI tasks.
The second application, Surrogate Fine-tuning, runs AI tasks more sporadically and has an order of magnitude smaller data transfer requirements.
The quality of results in both applications requires completing AI tasks rapidly after sufficient simulation tasks have completed.

Full implementations are available on GitHub (\url{https://github.com/exalearn/multi-site-campaigns}) and
the Materials Data Facility~\cite{ward2023data}.


\subsection{Application: Molecular~Design}\label{sec:moldesign}

This application seeks to identify molecules within a candidate set (here, \num{1115321} molecules from the MOSES dataset~\cite{polykovskiy2020moses}, as collected in the nCov-Group Data Repository~\cite{babuji2020covdata}) that have
desirable properties.
Properties of any individual molecule can be determined via an expensive quantum chemistry simulation, so
we use active learning~\cite{cohn1996active} to select simulations to perform.
Results from previous simulations 
% performed on a small set of molecules selected at random from the dataset
are used to train a machine-learned surrogate model, which is then used to infer scores for the remaining molecules; those scores are used to determine which simulations to be performed.
This process repeats until our computational budget is exhausted.
The application thus performs \textit{simulation}, \textit{training}, and \textit{inference} tasks.

%Our first application uses active learning to design molecular materials. Active learning~\cite{cohn1996active} is a simple form of computational campaign in which simulation tasks are chosen based on the results of previous simulations. Choosing the next simulation requires \emph{training} a surrogate model based on the results of previous simulations and then using the trained model to \emph{infer} the likely outcome of new simulations. 
%Simulation tasks may then be chosen to exp The application thus comprises distinct \textit{simulation}, \textit{training}, and \textit{inference} tasks.

%We describe below the specifics of the \textit{simulation}, \textit{training}, and \textit{inference} tasks used in this application and how we combine them to construct an optimization workflow.

\textbf{Simulation} The simulation task computes a molecule's ionization potential (IP), a key design property for organic electrolytes~\cite{li2020recent}, using tight binding, an inexpensive quantum-mechanical simulation method.
First, RDKit~\cite{landrum2006rdkit} is used to generate an initial 3D geometry for a molecule from its bonding connectivity.
Then, geoMETRIC~\cite{leeping2016geometric} determines the equilibrium geometry/energy for the neutral and charged states using the energies and forces computed with the QCEngine interface~\cite{smith2020qcarchive} of eXtended Tight Binding (xTB)~\cite{bannwarth2020xtb}.
% We chose xTB based on its fast runtime (minutes per molecule), though could easily replace it with a different quantum chemistry code as our workflow is built using QCEngine~\cite{smith2020qcarchive}.
All computations take $\sim$60~s on a CPU and produce 1~MB data.

\textbf{Training} The surrogate model used to predict a molecule's IP given its bonding connectivity comprises an ensemble of eight message-passing neural networks~\cite{stjohn2019mpnnpolymers} (MPNNs).
Each model has the same architecture but is trained on a different, randomly-selected subset of the training data.
Training each model requires 340~s on a GPU and generates 10~MB, and models can be trained in parallel.

\textbf{Inference} The inference tasks use the ensemble of MPNN models from the training task to estimate IPs.
% \ian{I am trying to write this so I understand it, but I am not sure what is being done. Am I correct in understanding that the inference task (1) for each molecule $i$, applies the eight models to compute eight predicted $\textrm{IP}_{ij, j\in\{1..8\}}$, and then computes a score $S_i = \sum_{j\in\{1..8\}}(\textrm{IP}_{ij})$ + $s(\textrm{IP}_{ij})$; (2) selects the $N$ highest (or lowest?) scoring molecules for the next simulation step? What is $N$?}\logan{You are correct. $N$ is equal to the number of molecules we will evaluate per run, typically 256.}
After predicting all molecule IPs with each model,
the molecules are ranked by the Upper Confidence Bound (UCB) of the predictions, which is the sum of the mean and standard deviations of the model predictions.
%Our search space is a set of \num{1115321} molecules from the MOSES molecule collection~\cite{polykovskiy2020moses}, as collected in the nCov-Group Data Repository~\cite{babuji2020covdata}.
Scoring the full dataset per model takes 900~s on a single GPU (1000~inferences/s) and requires transfer of 2.4~GB (model weights, molecule inputs, outputs). 
%\ian{per model or for all models? Perhaps you run eight scoring tasks, each on a separate GPU and each scoring all molecules with a separate model, then combine the 8 sets of scores to determine the scores, then selects the highest scoring?}.

%% LW (31Jan): No longer discussing policy
% Task scheduling is controlled by a few parameters.
% The number of execution slots available for a task queue controls the degree of parallelism; adding more slots than available nodes allows \funcx{} to improve utilization by prefetching tasks to endpoints.
% The frequency with which models are updated trades off the resources spent retraining models vs.\ the timeliness of the information used to select simulation tasks.
% We can also control resource usage vs.\ walltime by changing whether more simulations are started to submit tasks while the models are being updated\ian{I don't understand this sentence}.
% We explore the implications of changing these parameters in \autoref{sec:policy}.

Success is measured by how many molecules we find with high IPs after a certain amount of compute has been expended.

\subsection{Application: Surrogate Fine-tuning}\label{sec:hydronet}

% \ian{I think a sentence or two would help people what is being done here. What is the QM calculation? What is HydroNet? How is this fine tuning? Later, the word ``cluster'' is used, which is presumably relevant, but is not defined. What does ``B3LYP-D3//6-31g level'' mean? Presumably a higher-accuracy method, but higher accuracy than what?}

This application trains a surrogate model for expensive quantum mechanical computations.
Our goal is to produce a model capable of replicating the energies and forces from Density Functional Theory (DFT) calculations on clusters of water surrounding a methane solute.
We first train a model on the energies of many water clusters computed using an approximate method (Thole-Type Models~\cite{fanourgakis2006ttm}, HydroNet~\cite{choudhury2020hydronet} Dataset) and then refine the model with a small number of energies and forces of solvated methane calculated using DFT.
We use an active learning approach similar to the molecular design application to guide the choice of training data: we run the DFT calculations on the structures where the surrogate model is least certain.
Beyond the inference, training, and simulation tasks required by active learning, we also use a \textit{sampling} task to produce new structures.

\textbf{Training}: We train surrogate models using the SchNet architecture~\cite{schutt2018schnet} as implemented in PyTorch-Geometric.
A training task requires an average of 4~minutes on a GPU and transmitting 21~MB.
We train an ensemble of 8 SchNet models where each is trained on a different, randomly-selected subset of the training data.

\textbf{Inference}: An inference task on a batch of 100 structures takes an average of 3.2~s on a GPU and involves transmitting 3~MB  (including all inputs and outputs).

\textbf{Simulation}: Psi4~\cite{smith2020psi4} via the Atomic Simulation Environment (ASE) interface~\cite{larsen2017ase} is used to compute the energy and forces acting on a cluster of atoms using the PBE0 exchange-correlation function and the aug-cc-pvdz basis set.
Each task takes a mean of 360~s on CPU and produces 20~kB.

\textbf{Sampling} Sampling tasks produce new structures using a trained SchNet model.
We create new structures by initializing the temperature of a structure of water-solvated methane to 100K, then running molecular dynamics for a set number of timesteps.
Choosing the number of timesteps involves a tradeoff: too few produces too little diversity in structures, 
too many produces unrealistic structures due the accumulation of model errors over many steps.
Our first sampling tasks use only 20 timesteps because our model is not yet capable of producing realistic dynamics.
We gradually increase to 1000 over the course of the run.
Sampling tasks take between 1 and 3~s on a CPU and require transmitting 3~MB.

% \ian{I gave a name to the second pool, for clarity. But ``predicted'' may be the wrong name? Also: I don't think that the text says what the audit pool is for? Or how big the predicted pool is.}
% \logan{The audit pool is used to inform how long our sampling runs should be. Too short, they produce boring structures. Too long, inaccuracies in the ML model will accumulate and lead to unrealistic structures. We audit the end of each run and control the length to produce structures to produce a balance between boring and unrealistic. The predicted pool is bound}

Our steering agent selects the next simulation to perform from two pools, 
each designed to provide unique structures to add to the training set.
The \textit{audit} pool contains the last structure from each sampling task, which should be the most different from the training set by nature of being farthest along a time-evolution pathway.
The \textit{uncertainity} pool contains the structures for which our models have the least-certain predictions across all structures produced during sampling.
%\logan{I removed mention of the batch active learning strategy, which seemed unnecessary for understanding this manuscript.}
We re-populate the uncertainty pool each time 100 new structures have been sampled by performing inference on each newly-sampled structure with each model in our ensemble and ranking structures based on the variance in predicted energy.

% Then, it selects the structure with the largest variation in energy per atom across the eight models before removing 5\% of the selection pool with the highest correlation to the selected structures\ian{I don't understand this sentence}.
% This selection and pruning process continues until there are no more structures in the selection pool.
We start by training our model ensemble on \num{1720} previously collected structures and continue the active learning until 500 new structures have been added to the set.
Our steering algorithm balances the number of workers devoted to simulation and sampling to maintain a constant number of structures in the audit pool.
The algorithm begins a new training run after 25 new structures are added to the training set.

Success is measured by the accuracy of a model trained on all DFT calculations available after a run using test sets created from data unseen during training.
We produced the test set by running molecular dynamics for 10 structures of solvate methane using DFT to compute energies and forces at starting temperatures of 100K, 300K, and 900K for 32 timesteps.


\section{Technical Approach}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/implementation.pdf}
    \caption{Our implementation of a multi-site workflow.
    A \textbf{Thinker} expressed using \colmena{} controls when tasks are executed by communicating with a \textbf{Task Server} that manages submitting task via \funcx{}. \funcx{} routes task requests through a cloud provider and uses endpoints deployed on multiple resources that communicate with workers deployed on compute nodes. Large data are passed directly between the Thinker and workers using \proxystore{} backed by \globus.}
    \label{fig:overall}
\end{figure}

As detailed in \S\ref{sec:background}, AI-guided applications require systems for remote execution, data transfer, and defining our coordination policy.
We selected tools to achieve several goals: security, performance and scalability, minimial network configuration, robustness, and portability.
% \begin{enumerate}
%     \item Security: Follow best practices to prevent unauthorized execution and data access.
%     %\item Performance: No compromises to scalability
%     \item Performance and scalability: Capable of managing execution over hundreds of nodes while maintaining high utilization for tasks as short as 10 seconds. 
%     \item Reduced network complexity: No need to open ports on any resource or maintain SSH tunnels.
%     \item Robustness: Be resilient to network interruptions
%     \item Portability: No modifications required to application code to change data fabrics.
% \end{enumerate}
These objectives lead us to use \funcx{} to provide secure and scalable
remote execution across resources, 
%as a workflow tool to reduce our networking requirements, 
\proxystore{} to offload data transfer to different data fabrics (e.g., Globus),
and \colmena{} to express AI-guided workflows.
We describe each component below.

\subsection{Cloud-hosted management services}
Our approach centers around the use of cloud-hosted services to manage data and compute across distributed resources. 
This approach enables our applications to communicate with the reliable and accessible cloud-hosted Globus Transfer and \funcx{} services irrespective of data or compute location. 

\subsubsection{Security}
We require a security model that ensures only authenticated and authorized users can execute tasks and access data.
A multi-resource or -site workflow requires a solution that supports different identity management systems and authentication models, stringent security requirements (e.g., two-factor authentication, short authorization lifetimes), the need to use different accounts to access different resources, and resource sharing among groups of users. 
% We therefore require a flexible security model and uniform API to enable secure execution of tasks across resources and institutions.

Globus and \funcx{} fulfill such security requirements through Globus Auth~\cite{tuecke2016globus}, a general identity and access management platform. Globus Auth is widely adopted in the scientific community, implements standard protocols (e.g., OAuth~2), and enables secure delegation so 
workflows (e.g., Colmena) can securely leverage other 
services like \funcx{} and Globus. 
%our task execution system, \funcx{}, and data transfer fabric, Globus. 
% Globus and \funcx{} implement a hybrid deployment model, relying on users or administrators to deploy
% endpoint software (Globus Connect or \funcx{} Agents) on remote resources. These endpoints are 
% securely paired with the cloud and subsequent access is permitted only to authenticated and authorized users.
% \colmena{} users must authenticate with Globus Auth. As part of this process, \colmena{} obtains OAuth tokens
% that allow it to call Globus and \funcx{} on behalf of the users (and therefore enable access to that user's 
% data and compute resources).



\subsubsection{Network simplicity}
Both Globus and \funcx{} implement a hybrid deployment model, relying on users or administrators to deploy lightweight
software (Globus Connect or \funcx{} Agents) on remote resources. 
Communication between cloud and endpoints
use inbound HTTPS for Globus and outbound RabbitMQ TCP sockets for \funcx{}.
These endpoints are 
securely paired with the cloud platforms and
subsequent access is permitted only to authenticated and authorized users. 
Network connections between resources are not required.

\subsubsection{Robustness}
Cloud-hosted management services are highly available and provide high levels of robustness compared to locally-managed services.
For example, both \funcx{} and Globus's services accept and store tasks (and results) even while remote endpoints (or clients) are unavailable so tasks can be resumed when endpoints reconnect to the cloud.

\subsection{\funcx: Federated Function-as-a-Service} % for Cluster Computing}

%\logan{I trimmed this as some of the text is above.}
% Multi-resource workflows need mechanisms for dispatching  computations for execution on disparate resources. 
% Given the diversity of the computational ecosystem, ranging from
% edge devices to high performance compute clusters, it is important
% that this compute fabric abstract execution location and enable portable execution.
% Furthermore, we aim to 
% orchestrate a wide range of tasks, from short running
% inference tasks to long-running, many core training tasks.
% We employ \funcx{} as it addresses these requirements. 

\funcx{} is a federated FaaS platform that combines
a hosted cloud service for registering Python functions and dispatching function invocations
with an ecosystem of user-managed endpoints deployed on arbitrary computing
systems.
%\funcx{} supports execution of Python functions.
% Similar to cloud-hosted FaaS systems (e.g., Amazon Lambda and Google Cloud Functions), 
%\funcx{} allows users to register a function (and, optionally,
% an associated container) with the hosted service. 
Users can invoke functions by passing the function
body and input arguments to the cloud 
service via an implementation of Python's \texttt{concurrent.futures.Executor} interface.  
Unlike traditional FaaS systems, a \funcx{} user also supplies an endpoint ID that determines where 
the function will be executed. 
The \funcx{} endpoint is user-deployed and is responsible for provisioning
resources and managing function execution. The endpoint code can interface
with different schedulers (e.g., Slurm or PBS) to provision resources.
\funcx{} manages serializing
the function body and arguments, sending them to the remote
endpoint, and execution on the remote endpoint. 
% \funcx{} provides a traditional FaaS interface
% in Python and an implementation of Python's \texttt{concurrent.futures.Executor}
% interface that allows for asynchronous invocation of many functions. 

% \funcx{}'s endpoint software is also Python-based and can be installed
% in user space on remote systems. The endpoint includes
% software for interfacing with different schedulers to provision 
% resources and currently supports major HPC schedulers (e.g., 
% Slurm, PBS, LSF) as well as cloud platforms (e.g., AWS, Google Cloud). 
% The endpoint applies a pilot job model, via which a number of worker
% processes are deployed on provisioned nodes. These worker processes
% retrieve work from the Task Server (via the \funcx{} service and
% endpoint) and execute the function on the target system (optionally 
% inside a container). 
% Results are returned as Python objects following the reverse path. 

% Unlike many workflow systems, \funcx{} inverts the communication
% model such that deployments connect out from a remote system to 
% the cloud-hosted service to receive work.
% This avoids cumbersome and 
% fragile SSH connections used when workflow systems ``push'' work
% to remote resources. Furthermore, as a cloud-hosted system, there is little
% infrastructure needed to be deployed and maintained on the remote systems like
% the databases (e.g., MongoDB) or queues relied upon by 
% other workflow systems. 
% For example, 
% other approaches require users to 
% deploy and configure a database (e.g., MongoDB) to queue tasks, 
% which leads to fragile deployments. 

\subsection{\proxystore} %: Decoupling Data Transfer from Workflow Systems}
\label{sec:proxystore-backends}

Multi-resource workflows require a resource-spanning data fabric for data exchange.
Architecting this system element is challenging because capabilities and performance characteristics vary across resources and a single workflow may have many distinct data patterns (e.g., both frequent small transfers and infrequent large transfers).
% Challenges for a data fabric include optimal communication methods changing from resource to resource, computations and the corresponding data traversing many resources before reaching the final destination, and scientific applications varying widely in their data patterns.
% A once-size-fits-all approach is not suitable in this context because optimal communication methods change across resources and a single scientific workflow may have many different data transfer patterns. 
Any data transfer mechanism provided by the compute fabric is typically designed to support the most common use case: e.g.,
% Compute fabrics may provide data transfer mechanisms but have to make trade offs to support the most common use cases.
\funcx{} communicates function inputs and results via the cloud which adds overheads and incurs financial costs, so that payloads are limited to 10~MB.
Decoupling the data fabric from compute fabric can enable greater flexibility in data management. %across workflows.
Here, we use \proxystore{}, a system that enables pass-by-reference functionality in a diverse range of distributed applications without any modifications to application code.

Passing data by reference is key to dissociating data transfer from control flow. %, the decision making and dispatching of computations.
References are small so can be efficiently moved along with function bodies, and
can be passed between many resources without having to incur costs for the movement of the actual data.
That is, the data are only de-referenced once on the target resource regardless of how many intermediate resources through which the reference was passed.
This ensures processes or services responsible for control flow do not become an I/O bottleneck.
% The pass-by-reference paradigm yields many benefits in this context.
% Workflow systems typically have one or a few intermediaries responsible for coordinating the flow of tasks and data between clients and workers (the cloud service in \funcx{}).
% he intermediaries do not perform heavy computation because they are primarily responsible for coordinating or dispatching computations.
% If task inputs and results become too large, the intermediaries can quickly become a bottleneck in the workflow.
% Replacing task inputs or results with a reference to the objects allows data to be communicated in a more efficient manner that bypasses the intermediaries.

% Workflow systems typically have one or a few intermediaries responsible for coordinating the flow of tasks and data between clients and workers.
% In this work, the intermediaries are the \colmena{} Task Server and \funcx{} cloud service and endpoints.
% Intermediaries are not intended to perform heavy computations, so a single Task Server can keep thousands of workers utilized if the task definitions, input data, and results are sufficiently small.
% However, when task inputs or results are large, intermediaries can quickly become a bottleneck in large-scale workflows.
% Thus, a data transfer system is needed that can move data between the source and destination directly, avoiding overloading the intermediaries.

% Beyond reducing communication bottlenecks, workflow systems require a data storage and transfer system that is flexible because the data usage patterns of scientific tasks can vary widely.
%A simulation may only require a few hundred bytes to describe the necessary initial conditions to begin simulating, while machine learning inference tasks need the models as input, which can be hundreds or thousands of megabytes.
%Both simulations and machine learning inference tasks can produce large results.
%or can produce large inference results.
%Subsequently, the type of object store used by the workflow system can have a significant impact on performance or introduce limitations on the user.
%For a campaign running within a single cluster, storing large task inputs in a performant in-memory object store may reduce latency but limits the amount of data stored to the system's available memory.
%Alternatively, a file system may be used when tasks use data too large to store in memory, but a campaign running across multiple sites will not have access to a shared file system, therefore necessitating the use of a file-transfer service or web-based object store.
%As there is no
%Reducing communication bottlenecks is further complicated by the widely varying data patterns across scientific tasks, and there is no one-size-fits-all data storage and transfer system.
%As a result, many workflow systems place hard limits on supported workloads.
%For example, \funcx{} can store data in the cloud, making it accessible between sites, but it limits task input and output sizes to 10~MB to avoid excessive ingress and egress fees. 
%Other workflow systems, such as Ray~\cite{mortiz2018ray}, use in-memory data stores (e.g., Redis or Memcached), which are performant across a large range of object sizes but are challenging for multi-site deployments as compute clusters often prevent the opening of ports. % for security reasons.
% Workarounds, such as SSH tunnels, can resolve these limitations but are fragile, less performant (from both bandwidth and latency perspectives), and do not scale well with the number of systems being used.

%\subsubsection{Design Requirements}

%To address the aforementioned issues inherent to data storage and transfer in workflow systems, \colmena{} decouples the data storage and transfer system from \colmena{} and \funcx{}.
%\colmena{} works in conjunction with \proxystore{}, a standalone Python interface to object stores via transparent object proxies.

% \proxystore{} has a number of design requirements and goals to meet the needs of \colmena{}.
% At its core, \proxystore{} provides the Thinker and workers access to an object store such that data can be shared, while avoiding overheads in the Task Server and endpoints as noted in \ref{fig:overall}.
% To support the wide range of potential uses cases, \proxystore{} must support multiple backend services for storage and transfer (e.g., file system for large objects, in-memory data stores for low latency, data transfer services for multi-site).

Rather than exposing traditional \texttt{get}/\texttt{set} operations, \proxystore{} uses the \emph{proxy} model for seamless pass-by-reference functionality in Python.
This easy-to-use programming paradigm allows users to dynamically change transfer methods without needing to modify task code.
ProxyStore supports many backend object stores and transfer methods to provide efficient transfer of objects between processes located within the same resource or across different resources.

% At its core, \proxystore{} provides a unique unified interface to a wide range of supported object stores that enables efficient data transfer between the \colmena{} Thinker and \funcx{} workers.
% as the interface to the backend store.


%\proxystore{}'s key contribution is that lazy transparent object proxies serve as the interface to the object stores rather than traditional \texttt{get} and \texttt{set} operations.
%The use of lazy object proxies %enables additional goals for \proxystore{} including an
%provides an easy-to-use programming paradigm and amortized communication costs.

%\subsubsection{Lazy Transparent Object Proxies}
Proxies are used to intercept and redefine operations on a \emph{target} object. %~\cite{shapiro1986structure}.
% For example, proxies are commonly used to wrap sensitive objects with access control or provide caching for expensive object operations.
\proxystore{} uses lazy transparent object proxies that behave identically to the target object which is achieved by the proxy forwarding all operations on itself to the target.
%Transparent object proxies are constructed by passing the target object to the constructor of the proxy.
%Given an arbitrary object $v$ wrapped in a proxy $p$, the types of $v$ and $p$ will be equivalent, i.e., \texttt{isinstance(p, type(v))} is True.
Lazy proxies provide just-in-time \emph{resolution} of the target via a \emph{factory} function.
Factories resolve the target when called; a proxy, initialized with a factory, calls the factory and retrieves the target the first time the proxy is accessed.
%This process of the proxy calling the factory to retrieve the target is referred to as \emph{resolving} the proxy and enables just-in-time resolution of target objects.

When the \texttt{proxy()} function is called on a target object, the object is placed in a backend store, a factory
% containing the information needed to
capable of resolving the object from the store is created, and a proxy, initialized with the factory, is returned.
The resulting proxy is the lightweight reference that can be efficiently transmitted.
%to the object that is lightweight and can be efficiently transferred in place of the object.
% Proxies can be used without modifying the functions that use them.
A function that receives a proxy uses the proxy as if it were the target object due to the proxy's transparent nature so no modification of function code is needed.
%The first attempt of the function to use the proxy triggers the factory to resolve the object, after which the proxy behaves as the target object for the rest of its existence.
%Simply, the proxy enables wide area, just-in-time resolution of objects.

%Consider that we have some factory $f$, which when called will retrieve a large object $v$ from an object store, i.e., $f()\rightarrow v$.
%A proxy $p$ can be created with $p=\texttt{Proxy}(f)$.
%Until $p$ is first accessed in some way, $p$ is a lightweight object that can be efficiently transported---essentially acting as a reference to $v$.
%When $p$ is accessed to perform some computation, $p$ will resolve itself by calling $f$ to retrieve $v$ from the store.
%$p$ will continue to behave like $v$, forwarding all operations on itself to $v$, for the remainder of $p$'s lifetime.

% This paradigm is powerful
% % provides a powerful interface to object stores
% in a producer-consumer model.
% Typically, a producer places an object in the store with a key and sends the key to the consumer, then the consumer uses the key to retrieve the object from the store.
% Both the producer and the consumer need to know how to access the object store, and the consumer needs to expect to receive a key in place of the actual object.
% With lazy transparent object proxies, the consumer does not need to know how to access the object store nor need to handle the case where a key is sent in place of the object.
% The only work is required on the side of the producer which creates the initial proxy.
% Because the factory inside the proxy contains all the necessary information to resolve the target object, the proxy can be transferred to any arbitrary Python process and successfully resolve itself (assuming the process has physical access to the object store---e.g., file system or network access).
% \logan{Would you be able to shortly comment about what happens if a proxy is modified on the remote system? One of our reviewers was confused about that last time.}
% \greg{Added below. A bit long.}

% Proxied objects can be modified like any normal Python object once resolved; however, the original object data in the storage is read-only.
% This write-once, read-many approach enables copies of a proxy to be consumed by different processes.
% This paradigm fits the workflow model well, where proxies are used to communicate the inputs and outputs of independent tasks (i.e., proxies are not intended to provide statefulness between processes).

%\subsubsection{Object Store Backends}

%\proxystore{} supports many backend object stores.
%The \texttt{Store} interface exposes \texttt{get(key)}, \texttt{set(object, key)}, \texttt{exists(key)}, \texttt{evict(key)}, and \texttt{proxy(object)} methods.
%Calling \texttt{Store.proxy()} on an object will place the object in the store, create a factory containing the key associated with the object and any additional information needed to access the store, then return a proxy initialized with the factory.
%The returned proxy can be efficiently serialized and passed between workers and will just-in-time resolve itself to the target object when needed.
%The \texttt{Store} interface also provides a cache to avoid unnecessary communication when possible, and \proxystore{} supports optional strict guarantees that the returned object is the most recent version associated with a key. 

In this work, we use the Redis, file system, and \globus{} backends for \proxystore{}.
The Redis backend is ideal when resources exist within a single, fast network.
The file system backend supports scenarios where separate systems have access to a
%in which the Thinker and workers have access to a shared file system but not shared network as can be the case in computing centers that provide users a
shared file system. % or data sizes are too large for in-memory storage.
The \globus{} backend is used for multi-resource applications that lack a shared file system.
%Redis provides a high throughput and low latency object store.
%Storing objects on a share file system is slower but is ideal for workflows that produce objects too large for Redis or that persist between runs and are used by other applications (e.g., ML models).
% Removing below for anonymity
%\globus{} provides an efficient, reliable, and secure method of file transfer between sites and is currently used at hundreds of universities and labs worldwide making it an ideal choice for moving task data between sites.

%New backends can be added to \proxystore{} by implementing the \texttt{get}, \texttt{set}, \texttt{exists}, and \texttt{evict} methods of a \texttt{Store} interface.
%For example, the Redis backend implementation requires only $\sim$40 lines of code, and \proxystore{} will handle the creation of factories and proxies.
%Many computing systems and cloud providers provide object stores optimized for the systems, so the easily extendable \texttt{Store} interface 

% \begin{lstlisting}[style=PythonStyle, label={lst:proxystore}, caption={Example Thinker agent that begins inference over lists of models and input data chunks. The \proxystore{} \texttt{batch\_proxy} method can be used to efficiently proxy all of the models at once, and the resulting proxies can be reused over multiple tasks. \greg{Could this be dropped?}\logan{Probably. Especially since we need to save space.}}, float, floatplacement=bt]
% from hive.thinker import BaseThinker
% from proxystore.store import get_store

% class Thinker(BaseThinker):
%   ...

%   @event_responder(event_name='start_inference'):
%   def inference(self):
%     store = get_store('redis')
%     models = store.batch_proxy(self.models)

%     for chunk in input_data:
%       for model in models:
%         self.queues.send_inputs(model, chunk, ...)
% \end{lstlisting}

% As illustrated in \autoref{fig:overall}, our workflow is coordinated by a ``thinking'' agent (\colmena) that deploys tasks on multiple systems using \funcx. 
% Large function inputs and outputs (e.g., neural networks, quantum chemistry logs) are conveyed between systems by using \proxystore{} backed by \globus.
% In the following sections, we describe the desired features for each component and how the selected software satisfy the requirements.

\subsection{\colmena: Steering Policies as Cooperative Agents}

\colmena{} is a Python library for expressing the steering of dynamic workflows as a collection of interacting agents, which are known collectively as a \textit{Thinker}. %~\cite{colweb}.
The Thinker controls what tasks are performed and how resources are allocated by a workflow system over time based on behavior defined in the agents.
For example, one agent may submit a retraining task after a certain number of simulation tasks complete, and another may submit a new simulation when resources are available.
Agents, each running as a separate Python thread, interact via Python's threading primitives (e.g., the simulation agent may consume tasks from a queue populated from a task scoring agent).
A Thinker communicates task requests to a \textit{Task Server} that employs some compute fabric (e.g., Parsl or \funcx{}) to execute tasks on distributed resources and return results back to the Thinker (\autoref{fig:overall}).
% We use \funcx{} for the Task Server to easily submit workloads across multiple sites.

\colmena{} integrates support for \proxystore{} by automatically creating proxies for objects larger than a user-specified size.
The threshold size and \proxystore{} backend can vary between tasks types,
and users can also proxy objects manually before submitting the proxies to tasks.
%(see \autoref{lst:proxystore} for an example).
The flexibility in how proxies are generated makes it possible to adopt different data fabrics between sites and to cache data needed for a computation ahead of time.
Regardless of whether \colmena{}'s automatic proxying is used or the user manually proxies objects, no changes to user task code or the compute fabric (i.e., \funcx{}) are necessary with \proxystore{}.




\section{Results and Discussion}
\label{sec:results}

We started by evaluating the data fabric in a simplified, synthetic application (\autoref{results:proxystore}),
then evaluated our cloud-managed approach in depth for one of the two applications (\autoref{results:design}).
Finally, we contrast the performance differences between a cloud-hosted approach and a baseline that lacks our advanced data fabric using both motivating applications (\autoref{results:compare}).


\subsection{Computational Resources and Software Configuration}

We use the computational resources of the Argonne Leadership Computing Facility (ALCF) and Computing, Environment, and Life Sciences (CELS) directorate for our experiments.
The Knights Landing nodes of ALCF's Theta supercomputer are used to perform the simulation components of each workflow.
We use a NVIDIA DGX server with 20 T4 GPUs (known as Venti) for AI tasks (i.e., training, inference).
The Venti system is representative of off-site resources because, even though housed in the same building, it exists on a separate network, does not have access to any Theta file systems, and uses a different authentication procedure.
%Each site has a different \funcx{} endpoint.

The \colmena{} Thinker and Task Server reside on a login node of Theta.


\subsection{Workflow Configurations}\label{sec:workflow}

We compare three different workflow  system configurations. 
Our two baselines use Parsl, which requires open ports or a tunnel to each computing resource to communicate task information.
All configurations require creating a Python virtual environment on each computing resource.

\begin{enumerate}
    \item \textbf{Parsl}: Our baseline without \proxystore{}. Requires two ports for Parsl to communicate tasks to, and results from, a remote system.
    \item \textbf{Parsl+Redis}: Our baseline with \proxystore{} using Redis to communicate task data across sites and the file system for local tasks. Requires a third port for Redis in addition to two for Parsl.
    \item \textbf{FuncX+Globus}: Uses FuncX to communicate task instructions, and \proxystore{} with Globus for task data across sites and the file system for local tasks. Requires no open ports besides those used by Globus, which are already configured at most computing centers.
\end{enumerate}

% This node shares a file system with Theta/ThetaGPU, so we may use \proxystore{}'s file backend for this service.
% \globus{} is used to send data from Thinker to workers residing on Lambda. 


\subsection{Evaluating Data Transfer using Synthetic Applications}\label{results:proxystore}

% Passing data through a workflow controller is acceptable for small control messages but problematic for large data consumed or produced by tasks.
% We address this issue by leveraging \proxystore{} to pass large objects by reference and delegate the actual data transfer to peer-to-peer subsystems with faster data transfer rates.
% Our first step is to quantify the performance of different data transfer subsystems.
We first investigate using pass-by-reference to reduce overheads in \colmena{} and \funcx{}. Then we profile \proxystore{} backends to guide our deployment strategies. 

\begin{figure}
    \centering
    \includegraphics[trim=1mm 0.25cm 1mm 0, clip,width=\columnwidth]{figures/proxystore/colmena_overheads.png}
    \caption{
    Median times for different components of the end-to-end execution of a no-op task with \colmena{}.
    The use of \proxystore{} to transfer objects reduces communication costs for both small (10~kB) and large (1~MB) task inputs.
    \proxystore{} can avoid repeated serializations and deserializations of object transmitted through the Task Server and \funcx{} service.
    }
    \label{fig:proxystore-overhead}
\end{figure}


\subsubsection{Avoiding \funcx{} Overheads with \proxystore{}}

We first compare the performance achieved when communicating task inputs with \funcx{} and two \proxystore{} backends: shared file system and Redis.
Specifically, we aim to quantify overhead improvements to the Task Server and \funcx{} by transferring large objects via alternative means through \proxystore{}.

We execute no-op tasks that return no output to measure task overheads. We perform the experiment with 10~kB and 1~MB inputs. We chose the input sizes  based on characteristics of \funcx{}.
\funcx{} stores function arguments and results smaller than 20~kB in an Amazon ElastiCache Redis store and objects greater than 20~kB in Amazon S3. % the cumulative size of objects cannot exceed 10~MB in \funcx{}.
In all experiments, the Thinker and Task Server are located on a Theta login node, and we use a \funcx{} endpoint on Theta which executes tasks on a single Theta KNL node.
We execute 50 tasks and record the time spent in different stages of the task's life cycle.

%\logan{I've joined the mention of the figure to the paragraph explaining what you measured}
%We report the median time to transfer tasks and data between the Thinker, Task Server, and worker as well as the serialization time, time spent on the worker, and overall lifetime of the task, and the results are presented in~\ref{fig:proxystore-overhead}.
We show in \autoref{fig:proxystore-overhead} communication times between the Thinker, Task Server, and worker, as well as the serialization time, time spent on the worker, and overall task lifetime.
\emph{Serialization} time is that spent serializing and deserializing tasks on the Thinker, Task Server, and worker.
When serializing a task, \colmena{} scans for task inputs or outputs with sizes exceeding the \proxystore{} threshold (set to zero for this experiment).
If such large objects are found, the object is proxied and the lightweight proxy is serialized along with the task instead.
Therefore, the serialization time reflects proxying time, which includes time spent communicating objects to Redis or writing objects to disk.
%\logan{does the serialization time include saving the data to disk/redis. be clear}
%Subsequently, the serialization time includes the time to place objects in \proxystore{} and get a proxy in return.
\emph{Time on worker} is the time between the task starting on the worker and the worker returning the completed task;
it includes deserialization of the task, possible resolving of proxies, the execution of the task itself (which in this case is a no-op), and the serialization of the results.
\emph{Task lifetime} is the time between a task being created by the Thinker and the result being received by the Thinker.

Task Server-to-worker communication dominates the overall task lifetime because inputs must go through \funcx{}'s cloud service.
Passing objects via proxies can reduce this cost by 2--3$\times$ for 10~kB inputs and up to 10$\times$ for 1~MB inputs.
% The increase in \funcx{} overheads with larger data sizes is due to more data needing to be transmitted over the Internet and because \funcx{} uses the slower S3 for transient storage of larger data.
%\funcx{} stores the 1~MB inputs in S3 where as the smaller 10~kB inputs are stored in a faster Redis server.

Similar magnitude speedups are found for the communication between the Thinker and Task Server with larger objects.
The Thinker and Task Server communicate via Redis queues so sending small objects (e.g., 10~kB) via \proxystore{}'s Redis backend performs similar to without \proxystore{}, but larger objects see significant gains.
The Task Server, upon receiving a task from the Thinker, deserializes the task to determine the endpoint the task needs to be executed on and then serializes the task again to send to \funcx{}.
ProxyStore avoids additional deserialization and serialization of the input data because the data are replaced by a lightweight proxy.
% This additional deserialization and serialization on the Task Server is negligible for small task inputs but for large task inputs, such as in the 1~MB case, can make a large difference in the Thinker to Task Server communication time.

A workflow using \colmena{} and \funcx{} for data movement will not be able to respond to or initiate tasks with low latency as data sizes increase.
% The key conclusion here is that \colmena{} and \funcx{} overheads scale with data size so
Having an alternative method of object communication that can enable direct exchange of objects between the Thinker and workers is vital to avoid overloading the intermediate systems.
%and building a scalable workflow system.

\subsubsection{\proxystore{} Backends}

\begin{figure*}[ht]
    \centering
    \includegraphics[trim=0.2cm 0.2cm 0.2cm 0.2cm, clip, width=\textwidth]{figures/proxystore/overheads_input_size.png}
    \caption{
    Mean times of components in the lifetime of a \colmena{} task. 
    The Redis, file system, and \globus{} \proxystore{} backends are used to proxy inputs, ranging in size from 10~kB to 100~MB, to no-op tasks.
    The Redis backend provides low latency while the file system backend performs well with large objects ($\sim$100~MB).
    Mean time of a task spent on a worker increases with \globus{} because the task must wait on the web-based data transfer to complete.
    }
    \label{fig:proxystore-input-size}
\end{figure*}

As discussed in \autoref{sec:proxystore-backends}, we employ multiple \proxystore{} backends to support a range of task characteristics and computing environments.
For example, Redis can be faster than file system I/O for certain object sizes, but opening ports for Redis may not be possible in all environments. 
Here we benchmark the ProxyStore backends to guide the deployment of our motivating applications.

% We explore the performance of \proxystore{} in more detail in \autoref{fig:proxystore-input-size}. 
Specifically, we measure components of a task's life cycle with three different \proxystore{} backends---Redis, file system, and \globus{}---across a range of task input sizes, from 10~kB to 100~MB.
We use the same no-op and no-output tasks, and the same Theta KNL node endpoint, as in the prior experiment.
The Redis and file system backends enable intra-site communication, and thus for experiments with those backends we place the Thinker and Task Server on the same Theta login node.
The \globus{} backend is used for inter-site communication, and so for Globus experiments we place the Thinker and Task Server on a login node of a cluster in the the University of Chicago's Research Computing Center.

% These three backend choices cover a wide-range of use cases but exhibit very different performance. % as a result.
% E.g., Redis will typically be faster than file system I/O but opening the ports necessary to use Redis may not be possible in all circumstances. 
% Here we explore the effect that different backends have on the various stages in a task's life cycle in \colmena{}.

Comparing the performance of Redis-backed and file system-backed \proxystore{} in \autoref{fig:proxystore-input-size}, we find the only significant differences to be in serialization time, which includes storing objects in the \proxystore{} backend.
Latencies are much lower when using Redis in the case of smaller objects, but are comparable across Redis and file system backends for larger objects.
The ``time on worker'' is consistent between the Redis and file system backends and across task input sizes.

The serialization time is similar between the file system and \globus{} backends.
%This is because with the \globus{} backend, 
Objects are still written to the shared file system prior to starting a \globus{} transfer operation.
Thus, the serialization performance in both cases is a reflection of the I/O performance of the file system.
The \globus{} backend does incur higher overheads in the time spent on the worker, because a proxy must wait for the \globus{} transfer of its target object to finish before the proxy can be resolved. 
% itself.
This can take up to a few seconds, but the performance is constant with task input size (up to 100~MB), which indicates the communication bottleneck is due to the \globus{} web service latency rather than bandwidth limitations of the Thinker.
%, which is on a 300~Mbps residential internet connection, or endpoint, which has a commercial-grade internet connection.

Overall, there are clear trade offs between backends for \proxystore{}.
%Overall, we find that \proxystore{} provides access to many data storage and communication methods, and switching between backends can be done with a single line of code, enabling workflows to be easily optimized. % their workflows with ease.
A Redis backend is optimal for low latency access to small objects within a single site, whereas a shared file system backend is easier to use and provides good performance for large objects such as ML models.
\globus{} can enable multi-site workloads with low overheads when networking restrictions limit other options, and is competitive with Redis for dataset sizes beyond 10~MB.
All of these choices are accessible by changing a single line of code, enabling workflows to be easily optimized.


\subsection{Assessing System Performance in a Science Application}\label{results:design}

We next implement our science applications using our multi-resource workflow system
and ensure that communication overheads are not bottlenecks in system performance.
An effective steering system must minimize three forms of latency:
(a) between results completing and being available to the Thinker (reaction time),
(b) between results being received and new decisions being made (decision time),
and (c) between decisions made and new computations started (dispatch time).
%These experiments deploy the Thinker on a Theta login node, simulations on Theta KNL compute nodes, and use Lambda for AI tasks.
% \logan{There is also inefficiencies within the workflow execution system for how well it keeps nodes busy given work. I'm concerned that \funcx{} is not performing well for this.} \yadu{Might be smarter to frame this as responsiveness and latency/overheads, and avoid utilization} \logan{Agreed. Low utilization is a FuncX problem and, I'd imagine, a focus of a different paper}

\subsubsection{Reaction Time}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth,trim=0.4cm 0.5cm 0.35cm 0.3cm,clip]{figures/latency-analysis_reaction-time.pdf}
    \caption{Result notification timings for Molecular Design application. Above: Time between when a tasks finishes computing and when the Thinker is notified that the computation has finished. Below: How long the Thinker waits to access the result data.}
    \label{fig:rxntime}
\end{figure}

Reaction time latency has two components: the time to notify the Thinker process of a task and the time to make the result data available. 
Our use of \proxystore{} means that the two processes occur separately.

Notification includes serializing the data, creating a proxy object, and communicating the result proxy to the Thinker.
As shown in \autoref{fig:rxntime}, notification for simulation tasks is faster (median \num{500}~ms) than inference or training as it does not require initializing a \globus{} transfer because the Thinker and simulation worker share a file system.
The inference and training tasks are limited by the latency to start a data transfer between resources, which requires an HTTPS request to \globus{} that takes an average of $\sim$\num{500}~ms.

Data transfer latency---the time taken to access a result---is only greater than 1~s when data are transferred between resources, as when moving between CPU and GPU machines for inference and training.
Transferring between resources requires a Globus transfer, which typically completes in 1--5~s, depending on data transfer node utilization and concurrent transfer limits per user.
Fusing multiple transfers into a single task would be a viable route to avoid the concurrent transfer limit and avoiding this issue.
%Some \globus{} transfers complete while the Thinker is processing results of previous tasks, which is visible in inference data latencies that are sub-\num{500}~ms.

\subsubsection{Decision Time}
%We define the ``decision time'' as the time between when a result is placed in the result queue and the Thinker finishes using it to decide what to do next.
Our steering policy is designed to minimize the points where decision making is time-critical.

The first latency-sensitive decision is to start a new simulation after another completes. 
This decision can be made rapidly because it does not require accessing the result data. 
The median time between a Thinker receiving a completed simulation and sending the next to the Task Server is only \num{5}~ms, which is negligible in comparison to Reaction Time.
% In contrast, the median time until the task is submitted to \funcx{} by the Task Server is \num{194}~ms. 
% While larger, the latency of submitting to \funcx{} is not significant given that it is \num{0.1}\% of the average task runtime and 
% that its 95th percentile is only \num{300}~ms.
% The minimal ($<5$ ms) time for the Thinker to respond shows our use of multi-threaded Python in \colmena{} is not an issue in spite of Python's Global Interpreter Lock. % (GIL).

Decisions based on model training and inference results involve reading the contents of a result, which requires deserializing the message and resolving any proxies.
Responding to model training and inference events each take a median of \num{4}~s; both times are primarily waiting for the data transfer to complete (as in \autoref{fig:rxntime}).
Thus the time to receive data from remote systems is, again, the most important factor in application performance.

\subsubsection{Dispatch Time}
The time for a chosen task to begin executing on a remote system, the \textit{dispatch time}, is the final type of latency we consider.
%Latency in tasks starting on a worker is not hidden by a task backlog occurs three places: 
We hide the dispatch latency in many places with a backlog of queued tasks, but this latency cannot be hidden in three places: when we begin training models, when a batch of inference tasks are submitted after the first model completes training, and when a simulation worker completes.
In each case, we know at least one worker on a resource is available, and we want to provide this worker a new task as fast as possible.

Data transfer across multiple resources is the primary source of latency for inference and training tasks.
%require transferring data across multiple resources, and this is the primary source of latency.
The median latencies for starting the first inference task of a batch, and a training task, are \num{3.8} and \num{2.5}~s, respectively; resolving the data proxy accounts for \num{3.6}~s (95\%) and \num{1.7}~s (67\%) of those times. 
Each of these latencies is small in terms of the average task times; overhead is less than 1\% of runtime for training tasks and less than 10\% inference tasks. 
The inference tasks benefit strongly from the ahead-of-time data transfer and caching provided by ProxyStore, with 12\% of inference proxies resolving in under \num{100}~ms ($<1$\% of the task time).

The latency of the simulation dispatch times is minimal.
The time is dominated by communicating the task request via \funcx{}, which is a median of \num{100}~ms.
The dispatch time is less than 1\% of the total task runtime and, consequently, not our main target for improvement.

\subsubsection{Assessment} The latency of responding to a completed calculation across multiple resources is typically below \num{1}~s but can be as high as \num{10}~s under worst-case conditions.
As illustrated in the previous sections, \colmena{} is alerted of a new result within \num{100}~ms and acting on the result can take up to \num{3}~s if data must be transferred from a remote system.
Starting a new task on a new system is a minimum of \num{100}~ms and increases to several seconds if data must be transferred between sites.

\subsection{Comparing System Performance}\label{results:compare}

We study the performance differences between workflow systems (see \autoref{sec:workflow}) with two metrics: scientific performance and responsiveness of the workflow system. 
%We study both of our applications and use Venti for training and inference tasks, and Theta for simulation and sampling tasks.
 
\subsubsection{Molecular Design}

\begin{figure}
    \centering
    \includegraphics[trim=3mm 0.1cm 3mm 0cm, clip,width=\columnwidth]{figures/parsl-comparision_active-learning-performance.pdf}
    \includegraphics[trim=3mm 0.4cm 3mm 0.3cm, clip,width=\columnwidth]{figures/parsl-comparision_latencies.pdf}
    \caption{Comparison of different implementations of the multi-site active learning pipeline. An implementation using only Parsl, Parsl using \proxystore{} with Redis, and \funcx{} %to transmit requests and 
    using \proxystore{} with 
    \globus{}. (a) Number of top-performing molecules found as a function of simulation time expended.(b) Median time required to reorder a task queue (ML Makespan) and median idle time for CPU workers between simulation tasks. Error bars represent the 40th and 60th percentiles. }
    \label{fig:parslcomparison}
\end{figure}

We compared scientific outcomes by measuring how many molecules with IP $>$ 14 were found after 6 node hours of compute and averaged the value over three runs of each implementation.
Our best effort with open ports, Parsl+Redis, has equivalent outcomes to \funcx{}+\globus{} (140.3 vs.\ 145.0). 
The results are within the margin of error as the Parsl+Redis version varied between 129 and 149 suitable molecules found over 3 runs with different random seeds.
We therefore conclude that direct connections between computing providers are an unnecessary complication to deploying our multi-resource application.

The key responsiveness needed by our application is the time to update the task list given new data and rate at which new simulations are evaluated.
We define the time to update the task list as the time from when the steering policy requests models to be retrained to when the results from all inference tasks are used to reprioritize the task queue.
As shown in \autoref{fig:parslcomparison}a, the \funcx{} implementation completes the ML tasks faster (in 1565~s, on average) than native Parsl (1828~s) and is faster than Parsl+\proxystore{} backed by Redis (1676~s). 
There is a clear advantage to using pass-by-reference, with both \proxystore{}-backed applications outperforming a baseline Parsl, and we find that transferring with Globus yields improved application performance given the large data requirements of inference tasks.

Our other key latency metric is CPU utilization.
CPU nodes are idle between one simulation completing and the next starting, which requires two fast operations: notifying the Thinker of a completion and dispatching the next task.
We find the average idle time between tasks around 500~ms for FuncX and around 100~ms for Parsl with a Redis \proxystore{}.
In both cases, these latencies are small enough to achieve overall CPU utilization of above 99\%.
Utilization can be improved even further improved by submitting at least one more simulation task to execute than there are CPU workers available.

\subsubsection{Surrogate Finetuning}
The surrogate models produced using our cloud-managed workflows are indistinguishable from those produced using a conventional workflow solution.
The Root Mean Squared Deviation (RMSD) of the forces predicted using the models from the FuncX solution are $1.30\pm0.08$ $\mathrm{eV/\text{\AA}}$, compared to $1.47\pm0.09$ $\mathrm{eV/\text{\AA}}$ for Parsl with ProxyStore and $1.36\pm0.07$ $\mathrm{eV/\text{\AA}}$ without (\autoref{fig:surrogate}a). 
Run-to-run variations are larger than variation between the applications. 

While the three approaches show comparable scientific performance, the task overhead when using FuncX and Globus is clearly larger.
We measure this overhead as the time between when a task was created and when the result was read that is not the task running.
When tasks are run on remote GPUs, overhead is dominated by the time to transfer data with Globus (\autoref{fig:surrogate}b).
The second largest component is the data transfer time, which is roughly 2~s per direction and consistent with the results of synthetic experiments (\autoref{fig:proxystore-input-size}).
Transferring to remote sites via Redis is faster, but requires configuring a tunnel between resources.

CPU task overheads are dominated by the time to notify the Thinker of task completion for FuncX and data transfer for Parsl, regardless of task data size.
In contrast, the overhead for Parsl without pass-by-reference is strongly dependent on data size; 820~ms for sampling tasks (3~MB) and 20~ms for simulation tasks (20~kB).
The data transfer times for Parsl with pass-by-reference are consistent for these two task types at 200 and 170~ms for the sampling and simulation, respectively.
We can see the benefit of pass-by-reference for larger messages but also that our application could be accelerated by avoiding the overhead of proxying small messages.

\begin{figure}
    \centering
    \includegraphics[trim=4mm 5mm 4mm 3mm, clip,width=\columnwidth]{figures/surrogate/error-comparison-barh.pdf}
    \includegraphics[trim=3mm 4mm 3mm 1mm, clip,width=\columnwidth]{figures/surrogate/overhead-comparison.pdf}
    \caption{Comparison of (a) Root Mean Squared Deviation (RMSD) in predicted forces on a test set and (b) median overheads per task type for three different workflow systems. (a) Error bars are the standard error of the mean over three tests. The vertical dashed line is the error before fine-tuning. (b) The time spent waiting for result data is shown in gray for the systems where \proxystore{} is used to transfer data separately from task instructions (Redis store with Parsl, Globus store with FuncX).}
    \label{fig:surrogate}
\end{figure}

\subsection{Recommendations}


Our experience leads to several recommendations.
\begin{itemize}
    \item Use pass-by-reference and intelligent steering policies to achieve 10x reductions in application latency.
    \item Transmit data between sites directly for data larger than 10~kB. If messages are smaller than 100~MB and direct connection between resources is feasible, Redis is ideal, otherwise Globus is the best choice.
    \item Employ pass-by-reference with a conventional workflow system (e.g., Parsl) if data are larger than 10~kB, especially if data are reused between tasks.
\end{itemize}

\section{Conclusions}

We have presented experiences deploying two scientific applications across multiple heterogeneous resources.
The applications share a need to run simulation and AI tasks on separate resources, but differ in the amount of data transfer by an order of magnitude and in the frequency of AI tasks being required.
We employed a combination of FuncX and Globus Transfer to pass task instructions and data between resources,
\proxystore{} to allow data transfer via Globus without modifying application code,
and \colmena{} to express steering policies that hide data transfer latencies.
We show that our implementation achieves scientific outputs that are indistinguishable from those produced by implementations that require direct connections between resources.
We hope our demonstration that cloud services simplify deployment and ensure security of AI-Integrated workflows without reducing performance will lower the barrier to more applications being deploy across computational sciences.


% \ian{If this is being submitted to EXSAIS then the summary should say something about AI and extreme scale.}
% We characterized the performance of this system when used to implement two scientific applications and found that:

% \ian{I changed several COMPLEX to SOPHISTICATED.
% In my view, COMPLEX is bad, while SOPHISTICATED is good.}
% \logan{SOPHISTICATED is good!}

% \begin{enumerate}
%     \item 
%     Our system achieves comparable scientific outputs to an alternative that uses direct connections between systems;
%     \item 
%     Sending task data via \globus{} instead of \funcx{} or Redis is best for messages sizes above 100~MB; and
%     \item Resource-cost tradeoffs of a factor of two are accessible by altering campaign steering logic.
% \end{enumerate}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\section*{Acknowledgments}
We thank K.~Hermann, H.~Sprueill, J.~Bilbrey, and S.~Xantheas for help with the surrogate fine-tuning application.
LW, GS, GP, RC, SC, RT, and IF acknowledge support by the ExaLearn Co-design Center of Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration, to develop Colmena and evaluate its performance on HPC.
YB and KC were supported to integrate Parsl with Colmena by NSF Grant 1550588 and the ExaWorks Project within the Exascale Computing Project.
GP, VHS, and KC were supported to develop ProxyStore by NSF Grant 2004894.
This research used resources of the Argonne Leadership Computing Facility (ALCF), a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357, including via the ALCF Data Science Program. It also used resources provided by the University of Chicago's Research Computing Center.

\balance

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, refs}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
