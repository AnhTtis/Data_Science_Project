
@inproceedings{tensor,
author = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
title = {TensorFlow: A System for Large-Scale Machine Learning},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {265–283},
numpages = {19},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@article{chollet2018keras,
  title={Keras: The python deep learning library},
  author={Chollet, Fran{\c{c}}ois and others},
  journal={Astrophysics source code library},
  pages={ascl--1806},
  year={2018}
}

@article{paszke2017pytorch,
  title={Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory},
  journal={PyTorch: Tensors and dynamic neural networks in Python with strong GPU acceleration},
  volume={6},
  number={3},
  pages={67},
  year={2017}
}

@INPROCEEDINGS{sank,
  author={Sankaran, Anush and Aralikatte, Rahul and Mani, Senthil and Khare, Shreya and Panwar, Naveen and Gantayat, Neelamadhav},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER)}, 
  title={DARVIZ: Deep Abstract Representation, Visualization, and Verification of Deep Learning Models}, 
  year={2017},
  volume={},
  number={},
  pages={47-50},
  doi={10.1109/ICSE-NIER.2017.13}}

@INPROCEEDINGS{deepgraph,
  author={Hu, Qiang and Ma, Lei and Zhao, Jianjun},
  booktitle={2018 25th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={DeepGraph: A PyCharm Tool for Visualizing and Understanding Deep Learning Models}, 
  year={2018},
  volume={},
  number={},
  pages={628-632},
  doi={10.1109/APSEC.2018.00079}}
  
@article{nnconsole,
title={Neural Network Console that Accelerates the Use of Deep Learning},
author={Kobayashi, Yoshiyuki},
journal={Medical Imaging and Information Sciences},
volume={37},
number={1},
pages={1-4},
year={2020},
doi={10.11318/mii.37.1}
}

@inproceedings{ide,
author = {Tamilselvam, Srikanth G and Panwar, Naveen and Khare, Shreya and Aralikatte, Rahul and Sankaran, Anush and Mani, Senthil},
title = {A Visual Programming Paradigm for Abstract Deep Learning Model Development},
year = {2019},
isbn = {9781450377164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364183.3364202},
doi = {10.1145/3364183.3364202},
abstract = {Deep learning is one of the fastest growing technologies in computer science with a plethora of applications. But this unprecedented growth has so far been limited to the consumption of deep learning experts. The primary challenge being a steep learning curve for learning the programming libraries and the lack of intuitive systems enabling non-experts to consume deep learning. Towards this goal, we study the effectiveness of a "no-code" paradigm for designing deep learning models. Particularly, a visual drag-and-drop interface is found more efficient when compared with the traditional programming and alternative visual programming paradigms. We conduct user studies of different expertise levels to measure the entry level barrier and the developer load across different programming paradigms. We obtain a System Usability Scale (SUS) of 90 and a NASA Task Load index (TLX) score of 21 for the proposed visual programming compared to 68 and 52, respectively, for the traditional programming methods.},
booktitle = {Proceedings of the 10th Indian Conference on Human-Computer Interaction},
articleno = {16},
numpages = {11},
keywords = {drag-and-drop interface, auto programming, visual programming, deep learning},
location = {Hyderabad, India},
series = {IndiaHCI '19}
}
@inproceedings{gpt,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}}


@misc{pyqt,
  key   = "PyQT",
  note  = "\url{https://riverbankcomputing.com/software/pyqt/}",
}
@misc{pyqtg,
  key   = "PyQTgraph",
  note  = "\url{https://pyqtgraph.readthedocs.io/}",
}

@article{dann,
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran\c{c}ois and Marchand, Mario and Lempitsky, Victor},
title = {Domain-Adversarial Training of Neural Networks},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2096–2030},
numpages = {35},
keywords = {person re-identification, representation learning, domain adaptation, synthetic data, deep learning, sentiment analysis, image classification, neural network}
}
@INPROCEEDINGS{deepvis,
  author={Xie, Chao and Qi, Hua and Ma, Lei and Zhao, Jianjun},
  booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)}, 
  title={DeepVisual: A Visual Programming Tool for Deep Learning Systems}, 
  year={2019},
  volume={},
  number={},
  pages={130-134},
  doi={10.1109/ICPC.2019.00028}}


@article{fabrik,
  author    = {Utsav Garg and
               Viraj Prabhu and
               Deshraj Yadav and
               Ram Ramrakhya and
               Harsh Agrawal and
               Dhruv Batra},
  title     = {Fabrik: An Online Collaborative Neural Network Editor},
  journal   = {CoRR},
  volume    = {abs/1810.11649},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.11649},
  eprinttype = {arXiv},
  eprint    = {1810.11649},
  timestamp = {Sat, 23 Jan 2021 01:11:46 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-11649.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{weka,
author = {Witten, Ian and Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter},
year = {2009},
month = {11},
pages = {10-18},
title = {The WEKA data mining software: An update},
volume = {11},
journal = {SIGKDD Explorations},
doi = {10.1145/1656274.1656278}
}

@misc{digits,
  title = {NVIDIA DIGITS | NVIDIA Developer.},
  howpublished = {\url{https://developer.nvidia.com/digits}},

}

@misc{AETROS,
  title = {AETROS - Enterprise deep learning},
  howpublished = {\url{https://aetros.com/}},
}

@misc{DeepCognition,
  title = {DeepCognition - Become an AI-Powered},
  howpublished = {\url{http://deepcognition.ai/}},
}

@misc{Machine,
  title = {Machine UI},
  howpublished = {\url{https://machineui.co/}},
}



@article{activis,
author = {Kahng, Minsuk and Andrews, Pierre and Kalro, Aditya and Chau, Polo},
year = {2017},
month = {04},
pages = {},
title = {ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models},
volume = {PP},
journal = {IEEE Transactions on Visualization and Computer Graphics},
doi = {10.1109/TVCG.2017.2744718}
}

@inproceedings{dlide,
author = {Tamilselvam, Srikanth G and Panwar, Naveen and Khare, Shreya and Aralikatte, Rahul and Sankaran, Anush and Mani, Senthil},
title = {A Visual Programming Paradigm for Abstract Deep Learning Model Development},
year = {2019},
isbn = {9781450377164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364183.3364202},
doi = {10.1145/3364183.3364202},
abstract = {Deep learning is one of the fastest growing technologies in computer science with a plethora of applications. But this unprecedented growth has so far been limited to the consumption of deep learning experts. The primary challenge being a steep learning curve for learning the programming libraries and the lack of intuitive systems enabling non-experts to consume deep learning. Towards this goal, we study the effectiveness of a "no-code" paradigm for designing deep learning models. Particularly, a visual drag-and-drop interface is found more efficient when compared with the traditional programming and alternative visual programming paradigms. We conduct user studies of different expertise levels to measure the entry level barrier and the developer load across different programming paradigms. We obtain a System Usability Scale (SUS) of 90 and a NASA Task Load index (TLX) score of 21 for the proposed visual programming compared to 68 and 52, respectively, for the traditional programming methods.},
booktitle = {Proceedings of the 10th Indian Conference on Human-Computer Interaction},
articleno = {16},
numpages = {11},
keywords = {auto programming, visual programming, drag-and-drop interface, deep learning},
location = {Hyderabad, India},
series = {IndiaHCI '19}
}
@article{mnist,
  author = {LeCun, Yann and Cortes, Corinna},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}


@inproceedings{gan,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}


@inproceedings{10.1145/2702123.2702509,
author = {Amershi, Saleema and Chickering, Max and Drucker, Steven M. and Lee, Bongshin and Simard, Patrice and Suh, Jina},
title = {ModelTracker: Redesigning Performance Analysis Tools for Machine Learning},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702509},
doi = {10.1145/2702123.2702509},
abstract = {Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present ModelTracker, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with ModelTracker over six months shows ModelTracker is used often and throughout model building. A controlled experiment focusing on ModelTracker's debugging capabilities shows participants prefer ModelTracker over traditional tools without a loss in model performance.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {337–346},
numpages = {10},
keywords = {machine learning, performance analysis, debugging, interactive visualization},
location = {Seoul, Republic of Korea},
}

@inproceedings{umlaut,
author = {Schoop, Eldon and Huang, Forrest and Hartmann, Bjoern},
title = {UMLAUT: Debugging Deep Learning Programs Using Program Structure and Model Behavior},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445538},
doi = {10.1145/3411764.3445538},
abstract = {Training deep neural networks can generate non-descriptive error messages or produce unusual output without any explicit errors at all. While experts rely on tacit knowledge to apply debugging strategies, non-experts lack the experience required to interpret model output and correct Deep Learning (DL) programs. In this work, we identify DL debugging heuristics and strategies used by experts, andIn this work, we categorize the types of errors novices run into when writing ML code, and map them onto opportunities where tools could help. We use them to guide the design of Umlaut. Umlaut checks DL program structure and model behavior against these heuristics; provides human-readable error messages to users; and annotates erroneous model output to facilitate error correction. Umlaut links code, model output, and tutorial-driven error messages in a single interface. We evaluated Umlaut in a study with 15 participants to determine its effectiveness in helping developers find and fix errors in their DL programs. Participants using Umlaut found and fixed significantly more bugs and were able to implement fixes for more bugs compared to a baseline condition.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {310},
numpages = {16},
keywords = {End-User ML, ML Development, ML Debugging},
location = {Yokohama, Japan},

}
