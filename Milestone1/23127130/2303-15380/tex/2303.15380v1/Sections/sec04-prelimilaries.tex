% !TeX root = ../main.tex

\section{Dynamic Personalized Prior}
\vspace{-0.2em}
\label{subsec: prior_intro}

\input{Figures/sec03-overview/alternating_opt.tex}

To build personalized priors, we first capture 4D scans for each subject during dynamic motion. Minimally clothed parametric body models (here, SMPL) are registered to these scans (\secref{subsec: fit_smpl}). Next, detailed avatars are learned for each subject (\secref{subsec: snarf}). We leverage these to alleviate instance ambiguities during close interaction  (\secref{sec: method}). 


\subsection{Parametric Body Model Fitting}
\label{subsec: fit_smpl}
We register SMPL \cite{loper2015smpl} to the individual scans to represent the underlying body and its pose. 
SMPL is defined as a differentiable function 
that maps shape parameters $\beta \in \mathbb{R}^{10}$, pose parameters $\theta \in \mathbb{R}^{72}$ and translation $t \in \mathbb{R}^3$ to a body mesh $\mathcal{M}$ with $6890$ vertices.
Registering the SMPL model is formulated as an energy minimization problem over body shape, pose, and translation parameters:
%
\begin{equation}
    E(\beta, \theta, t)=\lambda_S E_S+\lambda_J E_J+\lambda_\theta E_\theta+\lambda_\beta E_\beta \ ,
    \label{eq: fit_smpl}
\end{equation}
where $E_S$ denotes the bi-directional distances between the SMPL mesh and the corresponding scan, and $E_J$ is a 3D keypoint energy between regressed SMPL 3D joints and 3D keypoints which are obtained via triangulation of 2D keypoints detected in the multi-view RGB images~\cite{cao2017openpose}. 
$E_\theta$ and $E_\beta$ are prior terms to constrain human pose and shape (\cf~\cite{bogo2016simplify}).
Each term is weighted with a corresponding weight $\lambda$. See Sup. Mat. for more details.

To obtain high-quality registrations, the body shape $\beta$ is estimated in advance from a minimally clothed static scan for each subject following \cite{bhatnagar2020ipnet, bhatnagar2020loopreg}. For registering SMPL to clothed scans, we keep $\beta$ fixed in \equref{eq: fit_smpl}. For brevity, we summarize the parameters as $\boldsymbol{\Theta} =  (\beta, \theta, t)$. 



\subsection{Human Avatar Learning}
\label{subsec: snarf}
Neural implicit surfaces can be used to represent articulated human bodies \cite{saito2021scanimate, deng2020nasa, tiwari2021neuralgif, chen2021snarf}. We leverage SNARF~\cite{chen2021snarf} for its generalization to unseen and challenging poses.
%
Following \cite{chen2021snarf} we use two neural fields to model shape and deformation in canonical space: 
\begin{itemize}
  \item \textbf{Shape Field:}  $f_{\sigma}$ is used to predict the occupancy probability $\hat{o}\left(\mathbf{x}_c, \boldsymbol{\Theta}\right)$ of any 3D point $\boldsymbol{x}_c$ in canonical space, where 1 is defined as inside and 0 as outside. The SMPL pose parameters are provided as an input to model pose-dependent deformations. The canonical shape is implicitly defined as the 0.5 level set $\mathcal{C} = \{ \boldsymbol{\ x}_c \ |\ f_{\sigma}(\boldsymbol{x}_c,\boldsymbol{\Theta}) = 0.5 \ \}$.
  \item  \textbf{Deformation Field:} $\mathbf{w}_{\omega}$ denotes a person-specific, canonical deformation field. It transforms the acquired shape to a desired pose $\boldsymbol{\Theta}$ via linear blend skinning (LBS) with learned deformation field.
\end{itemize}
%
\textbf{Correspondence Search.} \label{subsubsec: correspondence} Given a query point sampled in deformed space $\boldsymbol{x}_d$, \cite{chen2021snarf} determines its correspondence $\boldsymbol{x}_c$ in canonical space via iterative root finding such that it satisfies the forward skinning function $\boldsymbol{x}_d=\boldsymbol{d}_{\omega}(\boldsymbol{x_c}, \boldsymbol{\Theta})$.  
 
\noindent\textbf{Training Losses.} The implicit model is then trained via binary cross entropy $\mathcal{L}_{B C E}\left(\hat{o}\left(\mathbf{x}_d,\boldsymbol{\Theta}\right), o^{raw}\left(\mathbf{x}_d \right)\right)$, formed between the predicted occupancy $\hat{o}\left(\mathbf{x}_d, \boldsymbol{\Theta}\right)$ and the ground-truth occupancy $o^{raw}\left(\mathbf{x}_d \right)$ of points $\mathbf{x}_d$ in deformed space.

\noindent\textbf{Mesh Extraction.} We use \textit{Multiresolution IsoSurface Extraction} (MISE) \cite{mescheder2019occupancy} to extract meshes $\mathcal{C}$ from the continuous occupancy fields in canonical space: 
\begin{equation}
    \mathcal{C} = MISE(f_{\sigma}, \boldsymbol{\Theta}).
    \label{eq: MISE}
\end{equation}
The canonical shape can be deformed to posed space via linear blend skinning using the learned deformation field $\mathbf{w}_{\omega}$. For brevity, we denote this deformation as 
\begin{equation}
    \mathcal{D} = LBS(\mathcal{C}, \mathbf{w}_{\omega}, \boldsymbol{\Theta}).
    \label{eq: LBS}
\end{equation}





