% !TeX root = ../main.tex

\section{Conclusion}
\label{sec:coclusion}
In this paper, we propose a method to track and segment 4D scans of multiple people interacting in close range with dynamic physical contact.
To do so we first build a personalized implicit avatar model for each subject and then refine pose and shape network parameters given fused raw scans in an alternating fashion.
%
We further introduce \datasetname, a dataset consisting of close human interaction with high-quality 4D textured scans alongside corresponding multi-view RGB sequences, instance segmentation masks in 2D and 3D, registered parametric body models and vertex-level contact annotations. 
%
We define several vision benchmarks, such as monocular and multi-view human pose estimation and detailed geometry reconstruction conducted on \datasetname.

\noindent\textbf{Limitations.} 
Currently, our method does not model hands or facial expressions explicitly. We see the integration of more expressive human models \eg \cite{shen2023xavatar} as a fruitful future direction. Furthermore, the optimization schema of our method is not very computationally efficient. The optimization can be accelerated remarkably by upgrading the current deformer to a faster version \cite{chen2022fastsnarf}.

\noindent\textbf{Acknowledgments.} We thank Stefan Walter and Dean Bakker for the infrastructure support. We thank Deniz Yildiz and Laura W{\"u}lfroth for the data collection. We also thank all the participants who contribute to \datasetname.

