% !TeX root = ../main.tex
\section{Related Work}
\label{sec:rw}

\input{Figures/sec03-overview/method_overview}

\noindent \textbf{Instance Segmentation.}
Most works that tackle human instance segmentation \cite{Lin2022RobustHV,Sun2022HumanIM,liu2011markerless, liu2013markerless,Seong2022OneTrimapVM} or object detection in general \cite{Girshick2015FastRCNN, Ren2015FasterRCNN, redmon2016yolo, he2017maskrcnn, bolya2019yolact, kirillov2020pointrend,Lei2022Transfiner,Liu2022GlobalSF} are only applicable to the 2D domain.
These methods do not na\"ively transfer to the 3D domain \cite{hou20193dsis}.
%
For 3D instance segmentation, previous work predominantly focuses on scene understanding that does not include humans \cite{dai2017scannet,Engelmann20CVPR}.
Thus, 3D instance segmentation of humans in close interactions is a relatively under-explored task.
Unlike static objects, humans undergo articulated motion, interact dynamically with surroundings, and cannot be represented by simple geometric primitives, which makes human-centric 3D instance segmentation inherently challenging.
%
To address this challenging problem, we create  personalized human avatars of each individual and subsequently leverage them as priors to track and segment closely interacting 3D humans.

\noindent\textbf{Human Pose and Shape Modeling.}
Explicit body models \cite{loper2015smpl,pavlakos2019expressive,Joo2018TotalCapture,xu2020ghum,Anguelov2005SCAPE} are widely used for human modeling in computer vision and computer graphics. Because of their low-dimensional parameter space and fixed topology of the underlying 3D mesh, they are well suited for learning tasks like fitting to RGB images \cite{kocabas2019vibe,kanazawa2018hmr,kolotouros2019spin,Li2021hybrik,sun2021romp,Choutas2020ExPose,pavlakos2019expressive}, RGB-D \cite{Bogo2015DetailedFR,Chen2016RealtimeRO,DoubleFusion}, or sparse point clouds \cite{Loper2014MoSh,AMASS:2019}.
Yet, the fixed 3D topology limits the maximum resolution and the expressive power to represent individual features and clothing.
While there have been efforts to alleviate this \cite{Alldieck2018SMPLclothing,alldieck2019learning, guo2021human, bhatnagar2019mgn,ma2020cape}, recent attention has turned to the use of implicit representations to tackle these limitations.
\cite{saito2021scanimate, deng2020nasa, tiwari2021neuralgif, chen2021snarf} have shown promising results for modeling articulated clothed human bodies in 3D. 
Among these, SNARF \cite{chen2021snarf} achieves state-of-the-art results and shows good generalization to unseen poses. We thus use it as a building block in our method, which - according to our ablations - is the more suitable choice than an explicit body model.

\noindent\textbf{Multi-Person Pose and Shape Estimation.}
Compared to the remarkable progress that has been made in estimating pose and shape of single humans from images or videos \cite{kanazawa2018hmr, joo2020eft, kolotouros2019spin, saito2019pifu, huang2020arch, zheng2021pamir, saito2020pifuhd, xiu2022icon, jiang2022selfrecon,Li2021hybrik, song2020human, guo2023vid2avatar}, not much attention has been paid to multi-person pose and shape estimation for \textit{closely} interacting humans. Multi-person estimators, \eg \cite{kocabas2019vibe, kocabas2021pare, jiang2020coherent, sun2021romp, sun2022bev, mustafa2021multi, dong2019mvpose, dong2021shapeaware, wang2021mvp}, mainly deal with the case where people are far away from each other and do not interact naturally in close range.
While the works of \cite{zhang2021lightweight,zheng2021deepmulticap} show more closely interacting people, the focus lies more on occlusions caused by this scenario and the actual contact between body parts is often limited.
\cite{fieraru2020chi} study closer human interactions in a similar setting to ours, but without textured scans. Their method to disentangle interacting people is fundamentally different from ours and heavily relies on manual annotations, which our method is able to avoid through the designed optimization schema.

\noindent\textbf{Close Human Interaction Datasets.}  There are several contact-related datasets focusing on how humans interact with objects or static scenes \cite{GRAB:2020, PROX:2019, bhatnagar22behave, RICH:2022, fan2023arctic}. None of them considers close interactions between dynamic humans.
Of the datasets containing human-human interactions \cite{mehta2018mopots3d, joo2015panoptic, hu2013k3hi, gemeren2016shakefive2, fieraru2020chi, zheng2021deepmulticap, guo2021expi} the most recent ones with close human interactions are summarized in \tabref{tab: dataset}. 
ShakeFive2\cite{gemeren2016shakefive2} and MuPoTS-3D \cite{mehta2018mopots3d} only provide 3D joint locations as reference data, lacking body shape information. The most related dataset to ours is CHI3D\cite{fieraru2020chi}, which employs a  motion capture system to fit parametric human models of at most one actor at a time. CHI3D only provides contact labels at body region-level and only for 631 frames, whereas we provide vertex-level annotations at more than 6K instances.  
%
Furthermore, CHI3D does not contain textured scans, which are crucial to evaluate surface reconstruction tasks. MultiHuman \cite{zheng2021deepmulticap} provides textured scans of interacting people, but only of 453 static frames and without ground-truth level body model registrations. ExPI \cite{guo2021expi} contains dynamic textured meshes in addition to 3D joint locations, but misses instance masks along with body model registrations and contact information. Moreover, there are only two pairs of dance actors in ExPI \cite{guo2021expi}, thus lacking in subject and clothing diversity.
%
In contrast, our dataset \datasetname encompasses a rich set of data modalities for closely interacting humans.



