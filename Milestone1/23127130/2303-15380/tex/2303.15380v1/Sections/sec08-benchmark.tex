% !TeX root = ../main.tex


\section{Benchmark Baselines}

%%%% Figures %%%
\input{Figures/sec08-benchmark/smpl_baseline.tex}
\input{Figures/sec08-benchmark/mesh_baseline.tex}
%%%%


We define several standard vision benchmarks conducted on the \datasetname dataset. These benchmarks include monocular SMPL estimation, multi-view SMPL estimation, monocular detailed geometry reconstruction and multi-view detailed geometry reconstruction. 
We evaluate several baseline methods on each of these tasks and demonstrate experimentally that our \datasetname dataset is challenging, thus opening many doors for future research.


\subsection{SMPL Estimation}
\label{subsec: smpl_baseline}

\noindent\textbf{Evaluation Protocol.} 
\datasetname provides multi-view RGB sequences with corresponding SMPL body registrations of the interacting people. 
For the SMPL estimation task, we mainly follow the evaluation protocol of AGORA \cite{patel2021agora}. For the monocular setting, MPJPE [mm]
and MVE [mm] are calculated after alignment to the pelvis. The NMJE [mm] and
NMVE [mm] are MPJPE and MVE errors normalized by the F1 score respectively.
%, to penalize misses and false alarms. 
We adopt the Percentage of Correct Depth Relations ($\text{PCDR} ^{0.1}$ [\%]) 
metric that is introduced in \cite{sun2022bev} to evaluate depth reasoning. Contact Distances (CD [mm]) measures the distances between contact correspondences annotated in our dataset.\\
%
\noindent\textbf{Monocular Setting.} We evaluate one top-down method (PARE \cite{kocabas2021pare}) and two  bottom-up methods (ROMP \cite{sun2021romp} and BEV \cite{sun2022bev}) for the monocular SMPL estimation task. From \tabref{tab: smpl_estimation} we see that all methods have a relatively high MPJPE and MVE, demonstrating that current methods are not robust enough when strong human-human occlusion occurs. 
All methods fail to provide the reasonable spatial arrangement and contact relation, as shown in metric CD and \figref{fig: smpl_estimation}. \\
%
\textbf{Multi-View Setting.} Most of the multi-view pose estimation methods still focus only on skeleton estimation without taking body shape into account. We evaluate the open-sourced multi-view SMPL estimation method MVPose\cite{dong2019mvpose} on 4-view and 8-view settings.  
Although in the multi-view setting, MVPose achieves lower MPJPE and MVE, heavy interpenetration, and inaccurate poses in 3D space still exist, especially in the contact area (\cf \figref{fig: smpl_estimation}).  

\subsection{Detailed Geometry Reconstruction}
\label{subsec: mesh_baseline}
%
\noindent\textbf{Evaluation Protocol.} 
\datasetname provides high-quality 4D scans, which can serve as ground truth for the detailed geometry reconstruction task. We apply the same metrics described in \secref{subsec: mesh_eval} to measure the reconstruction accuracy.\\
%
\noindent\textbf{Monocular Setting.} Most of the existing monocular mesh reconstruction methods focus on the single-person scenario without any occlusion. 
We extend two methods for single-person geometry reconstruction PIFuHD \cite{saito2020pifuhd} and ICON \cite{xiu2022icon} to handle the multi-person case. For implementation details please refer to the Supp. Mat.
From \figref{fig: mesh_reconstruction}, we can observe that both methods are not robust against human-human occlusions and fail to produce high-quality reconstructions. 
\tabref{tab:mesh_reconstruction} quantitatively shows that current single-person methods cannot achieve satisfactory reconstructions when directly extended to the challenging multi-person scenario. We believe that \datasetname provides the necessary data to unlock next-generation methods to reconstruct detailed geometry from monocular RGB sequences depicting closely interacting humans.\\
%
\textbf{Multi-View Setting.} We evaluate the method DMC \cite{zheng2021deepmulticap} in both the 4-view and 8-view settings. 
Qualitative results in \figref{fig: mesh_reconstruction} show that although DMC can correctly reconstruct the geometry globally, artifacts still exist, \cf the hands and feet of the person colored in yellow. \tabref{tab:mesh_reconstruction} further highlights the opportunities for improvement on this task. 
