\section{Introduction}

Matrix denoising and factorization play a crucial role in a variety of data science tasks such as matrix sensing, phase retrieval or synchronisation, or matrix completion. The problem consists in reducing the amount of noise or irrelevant information present in a dataset, allowing for more accurate analysis and interpretation of the data, as well as better computational efficiency and modeling by way of dimensionality reduction. The literature on the subject is immense and we refer to \cite{ChenChi, chi2019nonconvex} for recent overviews of applications and theory in various settings and formulations.

In this contribution we focus on the study of gradient-flow for the following statistical formulation for positive definite matrix denoising. We consider a "ground truth" signal $X^* \in \mathbb R^{n \times d}$ with randomly sampled independent entries $X^*_{ij} \sim \mathcal N(0,\frac{1}{n})$ where the dimensions $n,d$ are  such that $\phi = \frac{d}{n}$ is fixed. Then we define the corrupted data matrix $Y \in \mathbb R^{n \times n}$
\begin{equation}\label{model1}
    Y = X^* X^{*T} + \frac{1}{\sqrt{\lambda}} \xi
\end{equation}
where $\xi$ is an additive symmetric random noise with $\xi_{ij} = \xi_{ji} \sim \mathcal N(0,\frac{1}{n})$ and $\lambda$ is (proportional to) the signal-to-noise ratio. The objective is to estimate the ground truth positive semi-definite matrix $X^* X^{*T}$ from the corrupted data matrix $Y$ with a matrix $XX^T$ such that $X \in \mathbb R^{n \times m}$ where $m$ is set from the fixed ratio $\psi = \frac{m}{n}$. Note that we allow $d$ and $m$ to be different. 
The estimator studied in this contribution is given by the gradient flow $X(t)$ ($t$ is  time) for an objective function with regularization parameter $\mu$, defined as 
\begin{equation}
    \mathcal H(X) = \frac{1}{4 d} \norm{Y - X X^{T}}_{F}^{2} + \frac{\mu}{2d} \norm{X}_{F}^{2} 
\end{equation}
where $\norm{ \cdot }_F$ is the Frobenius norm. The initialization of gradient flow is $X(0)= X_0 \in \mathbb R^{n \times m}$ random with i.i.d matrix elements from  
$\mathcal N(0,\frac{1}{n})$.
As a measure of performance we adopt the expected matrix-mean-square-error 
\begin{equation}
\mathbb{E}\mathcal E = \frac{1}{d} \mathbb{E}\norm{X^* X^{*T} - X X^{T}}_{F}^{2} 
\end{equation}
where the expectation is over $\xi$, $X^*$, $X_0$.
Note that the objective function and performance measure are not the same and can be thought of as "training" and "generalization" errors in the language of machine learning.

{\bf  Summary of main contributions}: 
\begin{itemize}
\item
We derive a set of analytical fixed point equations whose solutions allow to compute the full performance curve $t \to \mathbb{E}\mathcal{E}_t$ 
for the extensive-rank and high-dimensional regime where $m, d, n$ all tend to infinity while $\phi, \psi$ are kept fixed (results 1 and 2 in Sec. \ref{sec:results}). Continuous time average behaviour of gradient flow is a proxy for the usual discrete gradient descent algorithm, and has the advantage that it is more amenable to analytical study. The numerical experiments confirm that (a) $\mathcal{E}_t$ concentrates over its expectation; (b) theoretical predictions of gradient flow agree with gradient descent. See Fig. \ref{fig:graph1}.
\item
We further push the analysis of these equations in the time limit $t=+\infty$ and display specific examples where a critical value $\lambda_c$ can be calculated such that: (a) for $\lambda\leq\lambda_c$ the performance error of gradient flow is no better than the one of the null-estimator $X=0$; (b) for $\lambda>\lambda_c$ better estimation is possible; (c) the phase transition between the two regimes is a continuous type phase transition. These results are displayed on Fig. \ref{fig:graph3}.
\item
We analyze the limit $\phi = \psi \to 0$ (after $n, m, d$ have been sent to infinity) and derive a connection with the low-rank setting. It turns out that the matrix-mean-square-error curve (when $t\to +\infty$) tends to the one of the rank-one problem and the phase transition reduces to the well known BBP transition at $\lambda_c=1$. 
\end{itemize}
 
We use tools based on modern results in random matrix theory. Central to our derivations, is the formalism of \emph{linear-pencils}, that initially appeared in \cite{spectra, mingo2017free} and has been further improved recently in the context of neural networks \cite{adlam2020neural, bodin2021model, bodin.2212.06757}. In particular we make use of extensions provided in \cite{bodin.2212.06757} to derive closed-form expressions of non-trivial averages over $\xi$, $X^*$, $X_0$ appearing in traces of complicated "rational" expressions of these random matrices. Although these techniques have not yet always been rigorously proven they have been used successfully in different applications, and the predictions are confirmed by numerical experiments. In addition, we use holomorphic functional calculus for matrices \cite{schwartz1958linear}.

{\bf  Brief review of literature}:
The full time-evolution of gradient flow for the rank-one problem (the so-called spiked Wigner model with $d=m=1$) has been solved and rigorously analyzed in much the same spirit as the present work in \cite{pmlr-v134-bodin21a} with the difference that the spike is constrained to lie on a sphere all along the evolution. 
%The {\it limiting} performance for $t\to +\infty$ is the same as the one where the spherical constraint is relaxed to a ridge term (like in \eqref{model1}) with $\mu= \frac{1}{\lambda}$. 
For the present extensive-rank setting rigorous or even analytical results on the whole time-evolution are scarce. Closely connected to our work is the recent paper \cite{tarmoun2021understanding}. An essential difference however is that in \cite{tarmoun2021understanding} the initialization $X(0)= X_0$ is taken to have eigenvectors aligned with those of $Y$ (this pre-processing can be implemented empirically in practice). Moreover the authors do not carry out the random matrix averages fully analytically. Gradient flow has been studied in a variety of settings more or less related to the present one, see  \cite{gunasekar2017implicit, chou2020gradient, saxe2013exact, mannelli2019passed, BenArous-et-al-2022, Liang-Sen-Sur-2022}.

%In contrast with former work , the matrix $X(t=0)=X_0$ is initialized randomly with $[X_0]_{ij} \sim \mathcal N(0,\frac{1}{n})$ so that the eigenvectors of $X_0 X_0^T$ have not been preprocessed to be aligned with those of $Y$.

Bayesian approaches are quite well understood for the low-rank problem (mainly rank-one). This context is quite different from the present one. To begin with it is not dynamical. One studies the Minimum-Mean-Square-Estimator (MMSE) computed as the conditional expectation of the signal with respect to the Bayesian posterior probability distribution \cite{Montanari-Richard-2014, lelarge2019fundamental, luneau2020high, barbier2019adaptive, miolane2017fundamental, pourkamali2021mismatched, pourkamali2022mismatched, CamilliContucciMingione, barbier2022price}. Bayesian-optimal as well as mismatched estimation settings have been well studied with rigorous results on the mutual information, the MMSE, the cross-entropy, and the problem displays a rich phenomenology of first and higher order phase transitions depending on the nature of the priors. Related dynamics of the Approximate Message Passing (AMP) algorithms is also well understood for these problems \cite{DBLP:conf/isit/LesieurMLKZ17, Lesieur_2017, Montanari2017EstimationOL} . The realm of extensive-rank within such Bayesian and AMP approaches is quite open and very timely \cite{Kabashima_2016, barbier2021statistical, maillard2021perturbative, troiani2022optimal, camilli2022matrix}.

Finally other types of non-dynamical approach belong to the class of spectral methods like Principal Component Analysis (PCA).  The low rank case is covered by \cite{baik2005phase, Pch2004TheLE, benaych2011eigenvalues}. 
For the extensive-rank setting the results are scarce and little is known except for ensembles of rotation invariant signals for which an interesting class of Rotation Invariant Estimators (RIE) has been proposed \cite{bun2017cleaning}.

  
%In this work, we consider a planted signal $X^* \in \mathbb R^{n \times d}$ randomly sampled from an independent entries such that $X^*_{ij} \sim \mathcal N(0,\frac{1}{n})$ where the dimensions $n,d$ are  such that $\phi = \frac{d}{n}$ is fixed. Then we define the a symmetric random noise $\xi$ with $\xi_{ij} = \xi_{ji} \sim \mathcal N(0,\frac{1}{n})$ and the corrupted data matrix $Y \in \mathbb R^{n \times n}$ with signal-to-noise ratio $\lambda$. as follows:
%\begin{equation}
%    Y = X^* X^{*T} + \frac{1}{\sqrt{\lambda}} \xi
%\end{equation}
%The objective of this work is to estimate the planted signal $X^* X^{*T}$ from the corrupted data matrix $Y$ with a matrix $XX^T$ such that $X \in \mathbb R^{n \times m}$ where $m$ is set from the fixed ratio $\psi = \frac{m}{n}$. This estimation is realized by the minimization with the gradient-flow algorithm of an objective function (or the training error) with a regularization parameter $\mu$ defined as follows:
%\begin{equation}
%    \mathcal H(X) = \frac{1}{4 d} \norm{Y - X X^{T}}_{F}^{2} + \frac{\mu}{2d} \norm{X}_{F}^{2}
%\end{equation}
%Where $\norm{ \cdot }_F$ is the Frobenius norm. Further, to measure the performance of the estimation, the matrix mean-square error (or generalization error) $\mathcal E(X) = \frac{1}{d} \norm{X^* X^{*T} - X X^{T}}_{F}^{2}$ is used. In contrast with former work \cite{tarmoun2021understanding}, the matrix $X(t=0)=X_0$ is initialized randomly with $[X_0]_{ij} \sim \mathcal N(0,\frac{1}{n})$ so that the eigenvectors of $X_0 X_0^T$ have not been preprocessed to be aligned with those of $Y$. 




