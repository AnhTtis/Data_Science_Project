@inproceedings{nakkiran2020optimal,
  title     = {Optimal Regularization can Mitigate Double Descent},
  author    = {Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham M and Ma, Tengyu},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@book{potters2020first,
  title     = {A First Course in Random Matrix Theory: For Physicists, Engineers and Data Scientists},
  author    = {Potters, Marc and Bouchaud, Jean-Philippe},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@inproceedings{woodworth2020kernel,
  title        = {Kernel and rich regimes in overparametrized models},
  author       = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle    = {Conference on Learning Theory},
  pages        = {3635--3673},
  year         = {2020},
  organization = {PMLR}
}

@article{hoffmann2022training,
  title   = {Training Compute-Optimal Large Language Models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022}
}

@article{meir1994bias,
  title   = {Bias, variance and the combination of least squares estimators},
  author  = {Meir, Ronny},
  journal = {Advances in neural information processing systems},
  volume  = {7},
  year    = {1994}
}

@article{belkin2020two,
  title     = {Two models of double descent for weak features},
  author    = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal   = {SIAM Journal on Mathematics of Data Science},
  volume    = {2},
  number    = {4},
  pages     = {1167--1180},
  year      = {2020},
  publisher = {SIAM}
}

@article{PhysRevE.98.062120,
  title     = {Path integral approach to random neural networks},
  author    = {Crisanti, A. and Sompolinsky, H.},
  journal   = {Phys. Rev. E},
  volume    = {98},
  issue     = {6},
  pages     = {062120},
  numpages  = {16},
  year      = {2018},
  month     = {Dec},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevE.98.062120},
  url       = {https://link.aps.org/doi/10.1103/PhysRevE.98.062120}
}


@article{PhysRevLett.61.259,
  title     = {Chaos in Random Neural Networks},
  author    = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  journal   = {Phys. Rev. Lett.},
  volume    = {61},
  issue     = {3},
  pages     = {259--262},
  numpages  = {0},
  year      = {1988},
  month     = {Jul},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.61.259},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.61.259}
}


@book{parisi2020theory,
  title     = {Theory of Simple Glasses: Exact Solutions in Infinite Dimensions},
  author    = {Parisi, G. and Urbani, P. and Zamponi, F.},
  isbn      = {9781107191075},
  lccn      = {2019038319},
  url       = {https://books.google.ch/books?id=qkCUxgEACAAJ},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@article{Biroli2018,
  title     = {Out-of-equilibrium dynamical mean-field equations for the perceptron model},
  volume    = {51},
  issn      = {1751-8121},
  url       = {http://dx.doi.org/10.1088/1751-8121/aaa68d},
  doi       = {10.1088/1751-8121/aaa68d},
  number    = {8},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  publisher = {IOP Publishing},
  author    = {Agoritsas, Elisabeth and Biroli, Giulio and Urbani, Pierfrancesco and Zamponi, Francesco},
  year      = {2018},
  month     = {Jan},
  pages     = {085002}
}



@article{Mignacco-et-al2021,
  title     = {Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem},
  author    = {Mignacco, Francesca and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal   = {Machine Learning: Science and Technology},
  year      = {2021},
  publisher = {IOP Publishing}
}

@inproceedings{NEURIPS2020_6c81c83c,
  author    = {Mignacco, Francesca and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov\'{a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {9540--9550},
  publisher = {Curran Associates, Inc.},
  title     = {Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification},
  url       = {https://proceedings.neurips.cc/paper/2020/file/6c81c83c4bd0b58850495f603ab45a93-Paper.pdf},
  volume    = {33},
  year      = {2020}
}


@article{Sarao_Mannelli_2020,
  title     = {Marvels and Pitfalls of the {L}angevin Algorithm in Noisy High-Dimensional Inference},
  volume    = {10},
  issn      = {2160-3308},
  url       = {http://dx.doi.org/10.1103/PhysRevX.10.011057},
  doi       = {10.1103/physrevx.10.011057},
  number    = {1},
  journal   = {Physical Review X},
  publisher = {American Physical Society (APS)},
  author    = {Sarao Mannelli, Stefano and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\`a}, Lenka},
  year      = {2020},
  month     = {Mar}
}


@inproceedings{mannelli2019passed,
  title     = {Passed and Spurious: Descent Algorithms and Local Minima in Spiked Matrix-Tensor Models},
  author    = {Mannelli, Stefano Sarao and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborova, Lenka},
  pages     = {4333--4342},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/mannelli19a/mannelli19a.pdf},
  url       = {http://proceedings.mlr.press/v97/mannelli19a.html},
  abstract  = {In this work we analyse quantitatively the interplay between the loss landscape and performance of descent algorithms in a prototypical inference problem, the spiked matrix-tensor model. We study a loss function that is the negative log-likelihood of the model. We analyse the number of local minima at a fixed distance from the signal/spike with the Kac-Rice formula, and locate trivialization of the landscape at large signal-to-noise ratios. We evaluate analytically the performance of a gradient flow algorithm using integro-differential PDEs as developed in physics of disordered systems for the Langevin dynamics. We analyze the performance of an approximate message passing algorithm estimating the maximum likelihood configuration via its state evolution. We conclude by comparing the above results: while we observe a drastic slow down of the gradient flow dynamics even in the region where the landscape is trivial, both the analyzed algorithms are shown to perform well even in the part of the region of parameters where spurious local minima are present.}
}


@incollection{mannelli2019afraid,
  title     = {Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models},
  author    = {Sarao Mannelli, Stefano and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Zdeborov\'{a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8679--8689},
  year      = {2019},
  publisher = {Curran Associates, Inc.}
}



@article{BADAGA,
  author  = {Ben Arous, G{\'e}rard and Dembo, Amir and Guionnet, Alice},
  year    = {2004},
  month   = {10},
  pages   = {},
  title   = {Cugliandolo-Kurchan equations for dynamics of Spin-Glasses},
  volume  = {136},
  journal = {Probability Theory and Related Fields},
  doi     = {10.1007/s00440-005-0491-y}
}



@article{PhysRevLett.71.173,
  title     = {Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model},
  author    = {Cugliandolo, L. F. and Kurchan, J.},
  journal   = {Phys. Rev. Lett.},
  volume    = {71},
  issue     = {1},
  pages     = {173--176},
  numpages  = {0},
  year      = {1993},
  month     = {Jul},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.71.173},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.71.173}
}



@article{Crisanti1993TheSI,
  title   = {The sphericalp-spin interaction spin-glass model},
  author  = {A. Crisanti and H. Horner and H. Sommers},
  journal = {Zeitschrift f{\"u}r Physik B Condensed Matter},
  year    = {1993},
  volume  = {92},
  pages   = {257-271}
}
@article{bodin2021model,
  title   = {Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model},
  author  = {Bodin, Antoine and Macris, Nicolas},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  year    = {2021}
}

@inproceedings{brown2020language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}


@article{kaplan2020scaling,
  author   = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title    = {Scaling Laws for Neural Language Models},
  journal  = {arXiv},
  volume   = {},
  number   = {},
  pages    = {2001.08361v1},
  year     = {2020},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  location = {},
  keywords = {}
}

 @inproceedings{adlam2020neural,
  title     = {The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization},
  author    = {Adlam, Ben and Pennington, Jeffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {74--84},
  year      = {2020},
  editor    = {Hal Daume III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/adlam20a/adlam20a.pdf},
  url       = { http://proceedings.mlr.press/v119/adlam20a.html },
  abstract  = {Modern deep learning models employ considerably more parameters than required to fit the training data. Whereas conventional statistical wisdom suggests such models should drastically overfit, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a \emph{double descent} curve, in which increasing a model�s capacity causes its test error to first decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has nonmonotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.}
} 

@article{edwards1976eigenvalue,
  title     = {The eigenvalue spectrum of a large symmetric random matrix},
  author    = {Edwards, Samuel F and Jones, Raymund C},
  journal   = {Journal of Physics A: Mathematical and General},
  volume    = {9},
  number    = {10},
  pages     = {1595},
  year      = {1976},
  publisher = {IOP Publishing}
}


@book{hastie01statisticallearning,
  added-at  = {2008-05-16T16:17:42.000+0200},
  address   = {New York, NY, USA},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl    = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords  = {ml statistics},
  publisher = {Springer New York Inc.},
  series    = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title     = {The Elements of Statistical Learning},
  year      = 2001
}



@article{DBLP:journals/corr/ZhangBHRV16,
  author        = {Chiyuan Zhang and
                   Samy Bengio and
                   Moritz Hardt and
                   Benjamin Recht and
                   Oriol Vinyals},
  title         = {Understanding deep learning requires rethinking generalization},
  journal       = {ICLR 2017},
  volume        = {arXiv abs/1611.03530},
  year          = {2016},
  url           = {http://arxiv.org/abs/1611.03530},
  archiveprefix = {arXiv},
  eprint        = {1611.03530},
  timestamp     = {Mon, 13 Aug 2018 16:47:02 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/ZhangBHRV16.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2021multiple,
  title   = {Multiple descent: Design your own generalization curve},
  author  = {Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  year    = {2021}
}

@article{BelkinPNAS2019,
  author  = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year    = {2019},
  month   = {07},
  pages   = {201903070},
  title   = {Reconciling modern machine-learning practice and the classical bias-variance trade-off},
  volume  = {116},
  journal = {Proceedings of the National Academy of Sciences},
  doi     = {10.1073/pnas.1903070116}
}

@inproceedings{pmlr-v80-belkin18a,
  title     = {To Understand Deep Learning We Need to Understand Kernel Learning},
  author    = {Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {541--549},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf},
  url       = {http://proceedings.mlr.press/v80/belkin18a.html},
  abstract  = {Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this "overfitting", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification error or near zero regression error (interpolation) perform very well on test data. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. None of the existing bounds produce non-trivial results for interpolating solutions. We also show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results recently reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Some key phenomena of deep learning are manifested similarly in kernel methods in the modern "overfitted" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable "shallow" kernel methods are better understood.}
}


@inproceedings{pmlr-v89-belkin19a,
  title     = {Does data interpolation contradict statistical optimality?},
  author    = {Belkin, Mikhail and Rakhlin, Alexander and Tsybakov, Alexandre B.},
  booktitle = {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages     = {1611--1619},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume    = {89},
  series    = {Proceedings of Machine Learning Research},
  month     = {16--18 Apr},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v89/belkin19a/belkin19a.pdf},
  url       = {http://proceedings.mlr.press/v89/belkin19a.html},
  abstract  = {We show that classical learning methods interpolating the training data can achieve optimal rates for the problems of nonparametric regression and prediction with square loss.}
}


@article{Belkin_2020,
  title     = {Two Models of Double Descent for Weak Features},
  volume    = {2},
  issn      = {2577-0187},
  url       = {http://dx.doi.org/10.1137/20M1336072},
  doi       = {10.1137/20m1336072},
  number    = {4},
  journal   = {SIAM Journal on Mathematics of Data Science},
  publisher = {Society for Industrial & Applied Mathematics (SIAM)},
  author    = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  year      = {2020},
  month     = {Jan},
  pages     = {1167-1180}
}

@article{bouchaud2002statistical,
  title     = {Statistical properties of stock order books: empirical results and models},
  author    = {Bouchaud, Jean-Philippe and M{\'e}zard, Marc and Potters, Marc},
  journal   = {Quantitative finance},
  volume    = {2},
  number    = {4},
  pages     = {251},
  year      = {2002},
  publisher = {IOP Publishing}
}

@article{Hastie-Montanari-2019,
  author        = {{Hastie}, Trevor and {Montanari}, Andrea and {Rosset}, Saharon and {Tibshirani}, Ryan J.},
  title         = {{Surprises in High-Dimensional Ridgeless Least Squares Interpolation}},
  journal       = {arXiv e-prints},
  keywords      = {Mathematics - Statistics Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
  year          = 2019,
  month         = mar,
  eid           = {arXiv:1903.08560},
  pages         = {arXiv:1903.08560},
  archiveprefix = {arXiv},
  eprint        = {1903.08560},
  primaryclass  = {math.ST},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190308560H},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{bun2017cleaning,
  title     = {Cleaning large correlation matrices: tools from random matrix theory},
  author    = {Bun, Jo{\"e}l and Bouchaud, Jean-Philippe and Potters, Marc},
  journal   = {Physics Reports},
  volume    = {666},
  pages     = {1--109},
  year      = {2017},
  publisher = {Elsevier}
}

@inproceedings{bordelon2020spectrum,
  title        = {Spectrum dependent learning curves in kernel regression and wide neural networks},
  author       = {Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1024--1034},
  year         = {2020},
  organization = {PMLR}
}

@article{ADVANI2020428,
  title    = {High-dimensional dynamics of generalization error in neural networks},
  journal  = {Neural Networks},
  volume   = {132},
  pages    = {428-446},
  year     = {2020},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.022},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608020303117},
  author   = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
  keywords = {Neural networks, Generalization error, Random matrix theory},
  abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.}
}

@book{erdHos2017dynamical,
  title     = {A dynamical approach to random matrix theory},
  author    = {Erd{\H{o}}s, L{\'a}szl{\'o} and Yau, Horng-Tzer},
  volume    = {28},
  year      = {2017},
  publisher = {American Mathematical Soc.}
}

@article{benaych2016lectures,
  title   = {Lectures on the local semicircle law for Wigner matrices},
  author  = {Benaych-Georges, Florent and Knowles, Antti},
  journal = {arXiv preprint arXiv:1601.04055},
  year    = {2016}
}

@article{Sahai-et-al,
  author  = {Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  journal = {IEEE Journal on Selected Areas in Information Theory},
  title   = {Harmless Interpolation of Noisy Data in Regression},
  year    = {2020},
  volume  = {1},
  number  = {1},
  pages   = {67-83},
  doi     = {10.1109/JSAIT.2020.2984716}
}
  
  @article{Thrampoulidis2021,
  author   = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  title    = {{A model of double descent for high-dimensional binary linear classification}},
  journal  = {Information and Inference: A Journal of the IMA},
  year     = {2021},
  month    = {04},
  abstract = {{We consider a model for logistic regression where only a subset of features of size \\$p\\$ is used for training a linear classifier over \\$n\\$ training samples. The classifier is obtained by running gradient descent on logistic loss. For this model, we investigate the dependence of the classification error on the ratio \\$\\kappa =p/n\\$. First, building on known deterministic results on the implicit bias of gradient descent, we uncover a phase-transition phenomenon for the case of Gaussian features: the classification error of the gradient descent solution is the same as that of the maximum-likelihood solution when \\$\\kappa \\&lt;\\kappa \_\\star \\$, and that of the support vector machine when \\$\\kappa\\&gt;\\kappa \_\\star \\$, where \\$\\kappa \_\\star \\$ is a phase-transition threshold. Next, using the convex Gaussian min–max theorem, we sharply characterize the performance of both the maximum-likelihood and the support vector machine solutions. Combining these results, we obtain curves that explicitly characterize the classification error for varying values of \\$\\kappa \\$. The numerical results validate the theoretical predictions and unveil double-descent phenomena that complement similar recent findings in linear regression settings as well as empirical observations in more complex learning scenarios.}},
  issn     = {2049-8772},
  doi      = {10.1093/imaiai/iaab002},
  url      = {https://doi.org/10.1093/imaiai/iaab002},
  note     = {iaab002},
  eprint   = {https://academic.oup.com/imaiai/advance-article-pdf/doi/10.1093/imaiai/iaab002/36872804/iaab002.pdf}
}


@inproceedings{NEURIPS2020_37740d59,
  author    = {Derezinski, Michal and Liang, Feynman T and Mahoney, Michael W},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {5152--5164},
  publisher = {Curran Associates, Inc.},
  title     = {Exact expressions for double descent and implicit regularization via surrogate random design},
  url       = {https://proceedings.neurips.cc/paper/2020/file/37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@article{Spigler_2019,
  title     = {A jamming transition from under- to over-parametrization affects generalization in deep learning},
  volume    = {52},
  issn      = {1751-8121},
  url       = {http://dx.doi.org/10.1088/1751-8121/ab4c8b},
  doi       = {10.1088/1751-8121/ab4c8b},
  number    = {47},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  publisher = {IOP Publishing},
  author    = {Spigler, S and Geiger, M and d'Ascoli, S and Sagun, L and Biroli, G and Wyart, M},
  year      = {2019},
  month     = {Oct},
  pages     = {474001}
}

  
@article{Bartlett30063,
  author    = {Bartlett, Peter L. and Long, Philip M. and Lugosi, G{\'a}bor and Tsigler, Alexander},
  title     = {Benign overfitting in linear regression},
  volume    = {117},
  number    = {48},
  pages     = {30063--30070},
  year      = {2020},
  doi       = {10.1073/pnas.1907378117},
  publisher = {National Academy of Sciences},
  abstract  = {The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lie in an infinite-dimensional space vs. when the data lie in a finite-dimensional space with dimension that grows faster than the sample size.},
  issn      = {0027-8424},
  url       = {https://www.pnas.org/content/117/48/30063},
  eprint    = {https://www.pnas.org/content/117/48/30063.full.pdf},
  journal   = {Proceedings of the National Academy of Sciences}
}

@inproceedings{liao:hal-02971807,
  title       = {{A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent}},
  author      = {Liao, Zhenyu and Couillet, Romain and Mahoney, Michael W},
  url         = {https://hal.archives-ouvertes.fr/hal-02971807},
  booktitle   = {{34th Conference on Neural Information Processing Systems (NeurIPS 2020)}},
  address     = {Vancouver, Canada},
  year        = {2020},
  month       = Dec,
  pdf         = {https://hal.archives-ouvertes.fr/hal-02971807/file/main.pdf},
  hal_id      = {hal-02971807},
  hal_version = {v1}
}

@inproceedings{pmlr-v119-gerace20a,
  title     = {Generalisation error in learning with random features and the hidden manifold model},
  author    = {Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and Mezard, Marc and Zdeborova, Lenka},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {3452--3462},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/gerace20a/gerace20a.pdf},
  url       = {
               http://proceedings.mlr.press/v119/gerace20a.html
               },
  abstract  = {We study generalised linear regression and classification for a synthetically generated dataset encompassing different problems of interest, such as learning with random features, neural networks in the lazy training regime, and the hidden manifold model. We consider the high-dimensional regime and using the replica method from statistical physics, we provide a closed-form expression for the asymptotic generalisation performance in these problems, valid in both the under- and over-parametrised regimes and for a broad choice of generalised linear model loss functions. In particular, we show how to obtain analytically the so-called double descent behaviour for logistic regression with a peak at the interpolation threshold, we illustrate the superiority of orthogonal against random Gaussian projections in learning with random features, and discuss the role played by correlations in the data generated by the hidden manifold model. Beyond the interest in these particular problems, the theoretical formalism introduced in this manuscript provides a path to further extensions to more complex tasks.}
}

@inproceedings{pmlr-v119-d-ascoli20a,
  title     = {Double Trouble in Double Descent: Bias and Variance(s) in the Lazy Regime},
  author    = {D'Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {2280--2290},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/d-ascoli20a/d-ascoli20a.pdf},
  url       = { http://proceedings.mlr.press/v119/d-ascoli20a.html }
} 
 
@article{mei2020generalization,
  author        = {{Mei}, Song and {Montanari}, Andrea},
  title         = {{The generalization error of random features regression: Precise asymptotics and double descent curve}},
  journal       = {arXiv e-prints},
  keywords      = {Mathematics - Statistics Theory, Statistics - Machine Learning, 62J99},
  year          = 2019,
  month         = aug,
  eid           = {arXiv:1908.05355},
  pages         = {arXiv:1908.05355},
  archiveprefix = {arXiv},
  eprint        = {1908.05355},
  primaryclass  = {math.ST},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190805355M},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Geiger_2020,
  doi       = {10.1088/1742-5468/ab633c},
  url       = {https://doi.org/10.1088/1742-5468/ab633c},
  year      = 2020,
  month     = {feb},
  publisher = {{IOP} Publishing},
  volume    = {2020},
  number    = {2},
  pages     = {023401},
  author    = {Mario Geiger and Arthur Jacot and Stefano Spigler and Franck Gabriel and Levent Sagun and St{\'{e}}phane d'Ascoli and Giulio Biroli and Cl{\'{e}}ment Hongler and Matthieu Wyart},
  title     = {Scaling description of generalization with number of parameters in deep learning},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  abstract  = {Supervised deep learning involves the training of neural networks with a large number N of parameters. For large enough N, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as N grows past a certain threshold N*. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with N. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations of the neural net output function f N around its expectation . These affect the generalization error for classification: under natural assumptions, it decays to a plateau value in a power-law fashion  ∼N−1/2. This description breaks down at a so-called jamming transition N  =  N*. At this threshold, we argue that diverges. This result leads to a plausible explanation for the cusp in test error known to occur at N*. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond N*, and averaging their outputs.}
}

@article{loureiro2021capturing,
  title   = {Capturing the learning curves of generic features maps for realistic data sets with a teacher-student model},
  author  = {Loureiro, Bruno and Gerbelot, C{\'e}dric and Cui, Hugo and Goldt, Sebastian and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  journal = {arXiv preprint arXiv:2102.08127},
  year    = {2021}
}



@article{adlam2019random,
  author        = {{Adlam}, Ben and {Levinson}, Jake and {Pennington}, Jeffrey},
  title         = {{A Random Matrix Perspective on Mixtures of Nonlinearities for Deep Learning}},
  journal       = {arXiv e-prints},
  keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
  year          = 2019,
  month         = dec,
  eid           = {arXiv:1912.00827},
  pages         = {arXiv:1912.00827},
  archiveprefix = {arXiv},
  eprint        = {1912.00827},
  primaryclass  = {stat.ML},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191200827A},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{pmlr-v119-jacot20a,
  title     = {Implicit Regularization of Random Feature Models},
  author    = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clement and Gabriel, Franck},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {4631--4640},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/jacot20a/jacot20a.pdf},
  url       = { http://proceedings.mlr.press/v119/jacot20a.html },
  abstract  = {Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with $P$ features, $N$ data points, and a ridge $\lambda$, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an \emph{effective ridge} $\tilde{\lambda}$. We show that $\tilde{\lambda} > \lambda$ and $\tilde{\lambda} \searrow \lambda$ monotonically as $P$ grows, thus revealing the \emph{implicit regularization effect} of finite RF sampling. We then compare the risk (i.e. test error) of the $\tilde{\lambda}$-KRR predictor with the average risk of the $\lambda$-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically find an extremely good agreement between the test errors of the average $\lambda$-RF predictor and $\tilde{\lambda}$-KRR predictor.}
} 

@article{10.1214/19-ECP262,
  author    = {S. Péché},
  title     = {{A note on the Pennington-Worah distribution}},
  volume    = {24},
  journal   = {Electronic Communications in Probability},
  number    = {none},
  publisher = {Institute of Mathematical Statistics and Bernoulli Society},
  pages     = {1 -- 7},
  keywords  = {machine learning, random matrices},
  year      = {2019},
  doi       = {10.1214/19-ECP262},
  url       = {https://doi.org/10.1214/19-ECP262}
}


@article{8180454,
  author  = {J. W. {Helton} and R. R. {Far} and R. {Speicher}},
  journal = {International Mathematics Research Notices},
  title   = {Operator-valued Semicircular Elements: Solving A Quadratic Matrix Equation with Positivity Constraints},
  year    = {2007},
  volume  = {2007},
  number  = {9},
  pages   = {rnm086-rnm086},
  doi     = {10.1093/imrn/rnm086}
}

@book{mingo2017free,
  title     = {Free probability and random matrices},
  author    = {Mingo, James A and Speicher, Roland},
  volume    = {35},
  year      = {2017},
  publisher = {Springer}
}

@article{helton2018applications,
  title     = {Applications of realizations (aka linearizations) to free probability},
  author    = {Helton, J William and Mai, Tobias and Speicher, Roland},
  journal   = {Journal of Functional Analysis},
  volume    = {274},
  number    = {1},
  pages     = {1--79},
  year      = {2018},
  publisher = {Elsevier}
}


@inproceedings{dascoli2020triple,
  author    = {d{'}Ascoli, St\'{e}phane and Sagun, Levent and Biroli, Giulio},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {3058--3069},
  publisher = {Curran Associates, Inc.},
  title     = {Triple descent and the two kinds of overfitting: where and why do they appear?},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{nakkiran2019deep,
  title     = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  author    = {Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@inproceedings{RechtRahimi2007,
  author    = {Rahimi, Ali and Recht, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {J. Platt and D. Koller and Y. Singer and S. Roweis},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Random Features for Large-Scale Kernel Machines},
  url       = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
  volume    = {20},
  year      = {2008}
}

@inproceedings{pennington2017nonlinear,
  author    = {Jeffrey Pennington and
               Pratik Worah},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Nonlinear random matrix theory for deep learning},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {2637--2646},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/PenningtonW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bodin2021rankone,
  title         = {Rank-one matrix estimation: analytic time evolution of gradient descent dynamics},
  author        = {Antoine Bodin and Nicolas Macris},
  journal       = {arXiv e-prints},
  eid           = {arXiv:2105.12257},
  pages         = {arXiv:2105.12257},
  year          = {2021},
  eprint        = {2105.12257},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}



@inproceedings{pmlr-v134-bodin21a,
  title     = {Rank-one matrix estimation: analytic time evolution of gradient descent dynamics},
  author    = {Bodin, Antoine and Macris, Nicolas},
  booktitle = {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages     = {635--678},
  year      = {2021},
  editor    = {Belkin, Mikhail and Kpotufe, Samory},
  volume    = {134},
  series    = {Proceedings of Machine Learning Research},
  month     = {15--19 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v134/bodin21a/bodin21a.pdf},
  url       = {https://proceedings.mlr.press/v134/bodin21a.html},
  abstract  = {We consider a rank-one symmetric matrix corrupted by additive noise. The rank-one matrix is formed by an n-component unknown vector on the sphere of radius $\sqrt{n}$, and we consider the problem of estimating this vector from the corrupted matrix in the high dimensional limit of $n$ large, by gradient descent for a quadratic cost function on the sphere. Explicit formulas for the whole time evolution of the overlap between the estimator and unknown vector, as well as the cost, are rigorously derived. In the long time limit we recover the well known spectral phase transition, as a function of the signal-to-noise ratio. The explicit formulas also allow to point out interesting transient features of the time evolution. Our analysis technique is based on recent progress in random matrix theory and uses local versions of the semi-circle law.}
}

@article{rubio2011spectral,
  title     = {Spectral convergence for a general class of random matrices},
  author    = {Rubio, Francisco and Mestre, Xavier},
  journal   = {Statistics \& probability letters},
  volume    = {81},
  number    = {5},
  pages     = {592--602},
  year      = {2011},
  publisher = {Elsevier}
}

@book{GrobnerBasesBook,
  author    = {Cox, David A. and Little, John and O'Shea, Donal},
  title     = {Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra, 3/e (Undergraduate Texts in Mathematics)},
  year      = {2007},
  isbn      = {0387356509},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg}
}

@inbook{Opper1995,
  author    = {Opper, Manfred},
  title     = {Statistical Mechanics of Generalization},
  year      = {1998},
  isbn      = {0262511029},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  booktitle = {The Handbook of Brain Theory and Neural Networks},
  pages     = {922-925},
  numpages  = {4}
}

@book{engelvandenbroeck2001,
  place     = {Cambridge},
  title     = {Statistical Mechanics of Learning},
  doi       = {10.1017/CBO9781139164542},
  publisher = {Cambridge University Press},
  author    = {Engel, A. and Van den Broeck, C.},
  year      = {2001}
}

@article{spectra,
  author        = {{Rashidi Far}, Reza and {Oraby}, Tamer and {Bryc}, Wlodzimierz and {Speicher}, Roland},
  title         = {{Spectra of large block matrices}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Information Theory, Mathematics - Operator Algebras, H.1.1},
  year          = 2006,
  month         = oct,
  eid           = {cs/0610045},
  pages         = {cs/0610045},
  archiveprefix = {arXiv},
  eprint        = {cs/0610045},
  primaryclass  = {cs.IT},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2006cs.......10045R},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{mezard1987spin,
  title     = {Spin Glass Theory and Beyond},
  author    = {Mezard, M. and Parisi, G. and Virasoro, M.A.},
  isbn      = {9789971501150},
  lccn      = {sls90082516},
  series    = {Lecture Notes in Physics Series},
  url       = {https://books.google.ch/books?id=ZIF9QgAACAAJ},
  year      = {1987},
  publisher = {World Scientific}
}


@article{hendrycks2021unsolved,
  title   = {Unsolved Problems in ML Safety},
  author  = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2109.13916},
  year    = {2021}
}

@inproceedings{power2021grokking,
  title     = {GROKKING: GENERALIZATION BEYOND OVERFITTING ON SMALL ALGORITHMIC DATASETS},
  author    = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  booktitle = {ICLR MATH-AI Workshop},
  year      = {2021}
}

@inproceedings{liao2018dynamics,
  title        = {The dynamics of learning: A random matrix approach},
  author       = {Liao, Zhenyu and Couillet, Romain},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3072--3081},
  year         = {2018},
  organization = {PMLR}
}



@article{WYART1,
  author     = {Mario Geiger and
                Arthur Jacot and
                Stefano Spigler and
                Franck Gabriel and
                Levent Sagun and
                St{\'{e}}phane d'Ascoli and
                Giulio Biroli and
                Cl{\'{e}}ment Hongler and
                Matthieu Wyart},
  title      = {Scaling description of generalization with number of parameters in
                deep learning},
  journal    = {CoRR},
  volume     = {abs/1901.01608},
  year       = {2019},
  url        = {http://arxiv.org/abs/1901.01608},
  eprinttype = {arXiv},
  eprint     = {1901.01608},
  timestamp  = {Sat, 23 Jan 2021 01:20:16 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1901-01608.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{WYART2,
  title     = {A jamming transition from under-to over-parametrization affects generalization in deep learning},
  author    = {Spigler, Stefano and Geiger, Mario and d’Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  volume    = {52},
  number    = {47},
  pages     = {474001},
  year      = {2019},
  publisher = {IOP Publishing}
}

@article{AdvaniSaxeSompolynski,
  title    = {High-dimensional dynamics of generalization error in neural networks},
  journal  = {Neural Networks},
  volume   = {132},
  pages    = {428-446},
  year     = {2020},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.022},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608020303117},
  author   = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
  keywords = {Neural networks, Generalization error, Random matrix theory},
  abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.}
}

@inproceedings{DoubleTrouble,
  title        = {Double trouble in double descent: Bias and variance (s) in the lazy regime},
  author       = {d’Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2280--2290},
  year         = {2020},
  organization = {PMLR}
}

@inproceedings{Gerace-et-al,
  title        = {Generalisation error in learning with random features and the hidden manifold model},
  author       = {Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3452--3462},
  year         = {2020},
  organization = {PMLR}
}

@article{Deng2019,
  author   = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  title    = {{A model of double descent for high-dimensional binary linear classification}},
  journal  = {Information and Inference: A Journal of the IMA},
  volume   = {11},
  number   = {2},
  pages    = {435-495},
  year     = {2021},
  month    = {04},
  abstract = {{We consider a model for logistic regression where only a subset of features of size \\$p\\$ is used for training a linear classifier over \\$n\\$ training samples. The classifier is obtained by running gradient descent on logistic loss. For this model, we investigate the dependence of the classification error on the ratio \\$\\kappa =p/n\\$. First, building on known deterministic results on the implicit bias of gradient descent, we uncover a phase-transition phenomenon for the case of Gaussian features: the classification error of the gradient descent solution is the same as that of the maximum-likelihood solution when \\$\\kappa \\&lt;\\kappa \_\\star \\$, and that of the support vector machine when \\$\\kappa\\&gt;\\kappa \_\\star \\$, where \\$\\kappa \_\\star \\$ is a phase-transition threshold. Next, using the convex Gaussian min–max theorem, we sharply characterize the performance of both the maximum-likelihood and the support vector machine solutions. Combining these results, we obtain curves that explicitly characterize the classification error for varying values of \\$\\kappa \\$. The numerical results validate the theoretical predictions and unveil double-descent phenomena that complement similar recent findings in linear regression settings as well as empirical observations in more complex learning scenarios.}},
  issn     = {2049-8772},
  doi      = {10.1093/imaiai/iaab002},
  url      = {https://doi.org/10.1093/imaiai/iaab002},
  eprint   = {https://academic.oup.com/imaiai/article-pdf/11/2/435/44020681/iaab002.pdf}
}


@inproceedings{Kin-et-Thrampoulidis2020,
  author    = {Kini, Ganesh Ramachandra and Thrampoulidis, Christos},
  booktitle = {2020 IEEE International Symposium on Information Theory (ISIT)},
  title     = {Analytic Study of Double Descent in Binary Classification: The Impact of Loss},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {2527-2532},
  doi       = {10.1109/ISIT44484.2020.9174344}
}

@book{livre-Engel-vandenBroeck,
  title     = {Statistical mechanics of learning},
  author    = {Engel, Andreas and Van den Broeck, Christian},
  year      = {2001},
  publisher = {Cambridge University Press}
}


@article{lu2022equivalence,
  title   = {An Equivalence Principle for the Spectrum of Random Inner-Product Kernel Matrices},
  author  = {Lu, Yue M and Yau, Horng-Tzer},
  journal = {arXiv preprint arXiv:2205.06308},
  year    = {2022}
}

@article{hu2022sharp,
  title   = {Sharp Asymptotics of Kernel Ridge Regression Beyond the Linear Regime},
  author  = {Hu, Hong and Lu, Yue M},
  journal = {arXiv preprint arXiv:2205.06798},
  year    = {2022}
}

@article{misiakiewicz2022spectrum,
  title   = {Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression},
  author  = {Misiakiewicz, Theodor},
  journal = {arXiv preprint arXiv:2204.10425},
  year    = {2022}
}

@article{xiao2022precise,
  title   = {Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression},
  author  = {Xiao, Lechao and Pennington, Jeffrey},
  journal = {arXiv preprint arXiv:2205.14846},
  year    = {2022}
}


@article{dobriban2018high,
  title     = {High-dimensional asymptotics of prediction: Ridge regression and classification},
  author    = {Dobriban, Edgar and Wager, Stefan},
  journal   = {The Annals of Statistics},
  volume    = {46},
  number    = {1},
  pages     = {247--279},
  year      = {2018},
  publisher = {JSTOR}
}

@article{wu2020optimal,
  title   = {On the Optimal Weighted L2 Regularization in Overparameterized Linear Regression},
  author  = {Wu, Denny and Xu, Ji},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {10112--10123},
  year    = {2020}
}

@inproceedings{richards2021asymptotics,
  title        = {Asymptotics of ridge (less) regression under general source condition},
  author       = {Richards, Dominic and Mourtada, Jaouad and Rosasco, Lorenzo},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  pages        = {3889--3897},
  year         = {2021},
  organization = {PMLR}
}

@article{meng2022multiple,
  title   = {Multiple Descent in the Multiple Random Feature Model},
  author  = {Meng, Xuran and Yao, Jianfeng and Cao, Yuan},
  journal = {arXiv preprint arXiv:2208.09897},
  year    = {2022}
}

@article{lin2021causes,
  title   = {What Causes the Test Error? Going Beyond Bias-Variance via ANOVA.},
  author  = {Lin, Licong and Dobriban, Edgar},
  journal = {J. Mach. Learn. Res.},
  volume  = {22},
  pages   = {155--1},
  year    = {2021}
}

@article{adlam2020understanding,
  title   = {Understanding double descent requires a fine-grained bias-variance decomposition},
  author  = {Adlam, Ben and Pennington, Jeffrey},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {11022--11032},
  year    = {2020}
}

@inproceedings{tarmoun2021understanding,
  title        = {Understanding the dynamics of gradient flow in overparameterized linear models},
  author       = {Tarmoun, Salma and Franca, Guilherme and Haeffele, Benjamin D and Vidal, Rene},
  booktitle    = {International Conference on Machine Learning},
  pages        = {10153--10161},
  year         = {2021},
  organization = {PMLR}
}

@article{gunasekar2017implicit,
  title   = {Implicit regularization in matrix factorization},
  author  = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {30},
  year    = {2017}
}


@article{guionnet2002large,
  title     = {Large deviations asymptotics for spherical integrals},
  author    = {Guionnet, Alice and Zeitouni, Ofer},
  journal   = {Journal of functional analysis},
  volume    = {188},
  number    = {2},
  pages     = {461--515},
  year      = {2002},
  publisher = {Elsevier}
}

@book{anderson2010introduction,
  title     = {An introduction to random matrices},
  author    = {Anderson, Greg W and Guionnet, Alice and Zeitouni, Ofer},
  year      = {2010},
  publisher = {Cambridge university press}
}

@article{davidson2001local,
  title   = {Local operator theory, random matrices and Banach spaces},
  author  = {Davidson, Kenneth R and Szarek, Stanislaw J},
  journal = {Handbook of the geometry of Banach spaces},
  volume  = {1},
  number  = {317-366},
  pages   = {131},
  year    = {2001}
}

@book{villani2021topics,
  title     = {Topics in optimal transportation},
  author    = {Villani, C{\'e}dric},
  volume    = {58},
  year      = {2021},
  publisher = {American Mathematical Soc.}
}

@article{guo2005mutual,
  title     = {Mutual information and minimum mean-square error in {G}aussian
               channels},
  author    = {Guo, Dongning and Shamai, Shlomo and Verd{\'u}, Sergio},
  journal   = {IEEE transactions on information theory},
  volume    = {51},
  number    = {4},
  pages     = {1261--1282},
  year      = {2005},
  publisher = {IEEE}
}

@article{matytsin1994large,
  title     = {On the large-{N} limit of the {I}tzykson-{Z}uber integral},
  author    = {Matytsin, A},
  journal   = {Nuclear Physics B},
  volume    = {411},
  number    = {2-3},
  pages     = {805--820},
  year      = {1994},
  publisher = {Elsevier}
}

@article{maillard2021perturbative,
  title     = {Perturbative construction of mean-field equations in
               extensive-rank matrix factorization and denoising},
  author    = {Maillard, Antoine and Krzakala, Florent and M{\'e}zard, Marc
               and Zdeborov{\'a}, Lenka},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  volume    = {2022},
  number    = {8},
  pages     = {083301},
  year      = {2022},
  publisher = {IOP Publishing}
}

@article{biane1997free,
  title     = {On the free convolution with a semi-circular distribution},
  author    = {Biane, Philippe},
  journal   = {Indiana University Mathematics Journal},
  pages     = {705--718},
  year      = {1997},
  publisher = {JSTOR}
}

@article{verdu2010mismatched,
  title     = {Mismatched estimation and relative entropy},
  author    = {Verd{\'u}, Sergio},
  journal   = {IEEE Transactions on Information Theory},
  volume    = {56},
  number    = {8},
  pages     = {3712--3720},
  year      = {2010},
  publisher = {IEEE}
}

@article{voiculescu1993analogues,
  title     = {The analogues of entropy and of {F}isher's information measure
               in free probability theory, I},
  author    = {Voiculescu, Dan},
  journal   = {Communications in mathematical physics},
  volume    = {155},
  number    = {1},
  pages     = {71--92},
  year      = {1993},
  publisher = {Springer}
}

@article{voiculescu1994analogues,
  title     = {The analogues of entropy and of {F}isher's information measure
               in free probability theory, II},
  author    = {Voiculescu, Dan},
  journal   = {Inventiones mathematicae},
  volume    = {118},
  number    = {1},
  pages     = {411--440},
  year      = {1994},
  publisher = {Springer}
}

@article{yoshida2008remarks,
  title     = {Remarks on a semicircular perturbation of the free {F}isher
               information},
  author    = {Yoshida, Hiroaki},
  journal   = {Infinite Dimensional Analysis, Quantum Probability and
               Related Topics},
  volume    = {11},
  number    = {01},
  pages     = {97--108},
  year      = {2008},
  publisher = {World Scientific}
}


@article{voiculescu1997derivative,
  title     = {The derivative of order 1/2 of a free convolution by a
               semicircle distribution},
  author    = {Voiculescu, Dan},
  journal   = {Indiana University Mathematics Journal},
  pages     = {697--703},
  year      = {1997},
  publisher = {JSTOR}
}

@article{bun2016rotational,
  title     = {Rotational invariant estimator for general noisy matrices},
  author    = {Bun, Jo{\"e}l and Allez, Romain and Bouchaud, Jean-Philippe
               and Potters, Marc},
  journal   = {IEEE Transactions on Information Theory},
  volume    = {62},
  number    = {12},
  pages     = {7475--7490},
  year      = {2016},
  publisher = {IEEE}
}

@article{lelarge2019fundamental,
  title     = {Fundamental limits of symmetric low-rank matrix estimation},
  author    = {Lelarge, Marc and Miolane, L{\'e}o},
  journal   = {Probability Theory and Related Fields},
  volume    = {173},
  number    = {3},
  pages     = {859--929},
  year      = {2019},
  publisher = {Springer}
}

@article{dia2016mutual,
  title   = {Mutual information for symmetric rank-one matrix estimation: A
             proof of the replica formula},
  author  = {Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and
             Lesieur, Thibault and Zdeborov{\'a}, Lenka and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {29},
  year    = {2016}
}

@article{benaych2011eigenvalues,
  title     = {The eigenvalues and eigenvectors of finite, low rank
               perturbations of large random matrices},
  author    = {Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal   = {Advances in Mathematics},
  volume    = {227},
  number    = {1},
  pages     = {494--521},
  year      = {2011},
  publisher = {Elsevier}
}

@article{janson2007resultant,
  title   = {Resultant and discriminant of polynomials},
  author  = {Janson, Svante},
  journal = {Notes, September},
  volume  = {22},
  year    = {2007}
}

@article{barbier2021statistical,
  title     = {Statistical limits of dictionary learning: random matrix
               theory and the spectral replica method},
  author    = {Barbier, Jean and Macris, Nicolas},
  journal   = {Physical Review E},
  volume    = {106},
  number    = {2},
  pages     = {024136},
  year      = {2022},
  publisher = {APS}
}

@article{miolane2017fundamental,
  title   = {Fundamental limits of low-rank matrix estimation: the
             non-symmetric case},
  author  = {Miolane, L{\'e}o},
  journal = {arXiv preprint arXiv:1702.00473},
  year    = {2017}
}

@article{korada2009exact,
  title     = {Exact solution of the gauge symmetric p-spin glass model on a
               complete graph},
  author    = {Korada, Satish Babu and Macris, Nicolas},
  journal   = {Journal of Statistical Physics},
  volume    = {136},
  number    = {2},
  pages     = {205--230},
  year      = {2009},
  publisher = {Springer}
}

@inproceedings{lesieur2015mmse,
  title        = {{MMSE} of probabilistic low-rank matrix estimation:
                  {U}niversality with respect to the output channel},
  author       = {Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'a},
                  Lenka},
  booktitle    = {2015 53rd Annual Allerton Conference on Communication,
                  Control, and Computing (Allerton)},
  pages        = {680--687},
  year         = {2015},
  organization = {IEEE}
}

@article{barbier2019adaptive,
  title     = {The adaptive interpolation method: a simple scheme to prove
               replica formulas in {B}ayesian inference},
  author    = {Barbier, Jean and Macris, Nicolas},
  journal   = {Probability theory and related fields},
  volume    = {174},
  number    = {3},
  pages     = {1133--1185},
  year      = {2019},
  publisher = {Springer}
}

@article{barbier2019adaptive-b,
  title     = {The adaptive interpolation method for proving replica
               formulas. {A}pplications to the {C}urie--{W}eiss and {W}igner spike models},
  author    = {Barbier, Jean and Macris, Nicolas},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  volume    = {52},
  number    = {29},
  pages     = {294002},
  year      = {2019},
  publisher = {IOP Publishing}
}

@inproceedings{luneau2020high,
  title        = {High-dimensional rank-one nonsymmetric matrix decomposition:
                  the spherical case},
  author       = {Luneau, Cl{\'e}ment and Macris, Nicolas and Barbier, Jean},
  booktitle    = {2020 IEEE International Symposium on Information Theory
                  (ISIT)},
  pages        = {2646--2651},
  year         = {2020},
  organization = {IEEE}
}

@inproceedings{pourkamali2022mismatched,
  title        = {Mismatched Estimation of Non-Symmetric Rank-One Matrices Under
                  {G}aussian Noise},
  author       = {Pourkamali, Farzad and Macris, Nicolas},
  booktitle    = {2022 IEEE International Symposium on Information Theory
                  (ISIT)},
  pages        = {1288--1293},
  year         = {2022},
  organization = {IEEE}
}

@article{barbier2022price,
  title   = {The price of ignorance: how much does it cost to forget noise
             structure in low-rank matrix estimation?},
  author  = {Barbier, Jean and Hou, TianQi and Mondelli, Marco and
             S{\'a}enz, Manuel},
  journal = {arXiv preprint arXiv:2205.10009},
  year    = {2022}
}

@article{kabashima2016phase,
  title     = {Phase transitions and sample complexity in {B}ayes-optimal
               matrix factorization},
  author    = {Kabashima, Yoshiyuki and Krzakala, Florent and M{\'e}zard,
               Marc and Sakata, Ayaka and Zdeborov{\'a}, Lenka},
  journal   = {IEEE Transactions on information theory},
  volume    = {62},
  number    = {7},
  pages     = {4228--4265},
  year      = {2016},
  publisher = {IEEE}
}

@article{troiani2022optimal,
  title   = {Optimal denoising of rotationally invariant rectangular matrices},
  author  = {Troiani, Emanuele and Erba, Vittorio and Krzakala, Florent
             and Maillard, Antoine and Zdeborov{\'a}, Lenka},
  journal = {arXiv preprint arXiv:2203.07752},
  year    = {2022}
}

@article{husson2022spherical,
  title   = {Spherical Integrals of Sublinear Rank},
  author  = {Husson, Jonathan and Ko, Justin},
  journal = {arXiv preprint arXiv:2208.03642},
  year    = {2022}
}

@book{nica2006lectures,
  title     = {Lectures on the combinatorics of free probability},
  author    = {Nica, Alexandru and Speicher, Roland},
  volume    = {13},
  year      = {2006},
  publisher = {Cambridge University Press}
}


@inproceedings{muller2004random,
  title        = {Random matrices, free probability and the replica method},
  author       = {M{\"u}ller, Ralf R},
  booktitle    = {2004 12th European Signal Processing Conference},
  pages        = {189--196},
  year         = {2004},
  organization = {IEEE}
}

@inproceedings{reeves2018mutual,
  title        = {Mutual information as a function of matrix snr for linear
                  gaussian channels},
  author       = {Reeves, Galen and Pfister, Henry D and Dytso, Alex},
  booktitle    = {2018 IEEE International Symposium on Information Theory
                  (ISIT)},
  pages        = {1754--1758},
  year         = {2018},
  organization = {IEEE}
}


@inproceedings{nica2008free,
  title        = {Free probability extensions and applications},
  author       = {Nica, Alexandru and Speicher, Roland and Tulino, Antonia and
                  Voiculescu, Dan},
  booktitle    = {Proc. BIRS},
  pages        = {3--10},
  year         = {2008},
  organization = {Citeseer}
}

@article{shlyakhtenko2020fractional,
  title   = {Fractional free convolution powers},
  author  = {Shlyakhtenko, Dimitri and Jekel, Terence Tao},
  journal = {arXiv preprint arXiv:2009.01882},
  year    = {2020}
}

@book{hardy1952inequalities,
  title     = {Inequalities},
  author    = {Hardy, Godfrey Harold and Littlewood, John Edensor and
               P{\'o}lya, George and P{\'o}lya, Gy{\"o}rgy and others},
  year      = {1952},
  publisher = {Cambridge university press}
}

@book{sauer2011numerical,
  title     = {Numerical analysis},
  author    = {Sauer, Timothy},
  year      = {2011},
  publisher = {Addison-Wesley Publishing Company}
}

@article{zuber2008large,
  title     = {The large-N limit of matrix integrals over the orthogonal group},
  author    = {Zuber, Jean-Bernard},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  volume    = {41},
  number    = {38},
  pages     = {382001},
  year      = {2008},
  publisher = {IOP Publishing}
}

@article{menon2017complex,
  title   = {The complex {B}urgers’ equation, the {HCIZ} integral and the
             {C}alogero-{M}oser system},
  author  = {Menon, Govind},
  journal = {preprint},
  year    = {2017}
}

@article{guionnet2004first,
  title     = {First order asymptotics of matrix integrals; a rigorous
               approach towards the understanding of matrix models},
  author    = {Guionnet, Alice},
  journal   = {Comm. Math. Phys.},
  volume    = {244},
  number    = {3},
  pages     = {527--569},
  year      = {2004},
  publisher = {Springer}
}

@article{jekel2020elementary,
  title     = {An elementary approach to free entropy theory for convex
               potentials},
  author    = {Jekel, David},
  journal   = {Anal.  PDE},
  volume    = {13},
  number    = {8},
  pages     = {2289--2374},
  year      = {2020},
  publisher = {Mathematical Sciences Publishers}
}

@inproceedings{mairal2009online,
  title     = {Online dictionary learning for sparse coding},
  author    = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro,
               Guillermo},
  booktitle = {Proceedings of the 26th annual international conference on
               machine learning},
  pages     = {689--696},
  year      = {2009}
}

@article{olshausen1996emergence,
  title     = {Emergence of simple-cell receptive field properties by
               learning a sparse code for natural images},
  author    = {Olshausen, Bruno A and Field, David J},
  journal   = {Nature},
  volume    = {381},
  number    = {6583},
  pages     = {607--609},
  year      = {1996},
  publisher = {Nature Publishing Group}
}

@article{olshausen1997sparse,
  title     = {Sparse coding with an overcomplete basis set: A strategy
               employed by {V}1?},
  author    = {Olshausen, Bruno A and Field, David J},
  journal   = {Vision research},
  volume    = {37},
  number    = {23},
  pages     = {3311--3325},
  year      = {1997},
  publisher = {Elsevier}
}

@article{belouchrani1997blind,
  title     = {A blind source separation technique using second-order
               statistics},
  author    = {Belouchrani, Adel and Abed-Meraim, Karim and Cardoso, J-F and
               Moulines, Eric},
  journal   = {IEEE Transactions on signal processing},
  volume    = {45},
  number    = {2},
  pages     = {434--444},
  year      = {1997},
  publisher = {IEEE}
}

@article{candes2011robust,
  title     = {Robust principal component analysis?},
  author    = {Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and
               Wright, John},
  journal   = {Journal of the ACM (JACM)},
  volume    = {58},
  number    = {3},
  pages     = {1--37},
  year      = {2011},
  publisher = {ACM New York, NY, USA}
}

@phdthesis{miolane2019fundamental,
  title  = {Fundamental limits of inference: A statistical physics approach.},
  author = {Miolane, L{\'e}o},
  year   = {2019},
  school = {Ecole normale sup{\'e}rieure-ENS PARIS; Inria Paris}
}

@article{donoho2013phase,
  title     = {The phase transition of matrix recovery from Gaussian
               measurements matches the minimax MSE of matrix denoising},
  author    = {Donoho, David L and Gavish, Matan and Montanari, Andrea},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {110},
  number    = {21},
  pages     = {8405--8410},
  year      = {2013},
  publisher = {National Acad Sciences}
}

@article{barbier2022bayes,
  title   = {Bayes-optimal limits in structured PCA, and how to reach them},
  author  = {Barbier, Jean and Camilli, Francesco and Mondelli, Marco and
             Saenz, Manuel},
  journal = {arXiv preprint arXiv:2210.01237},
  year    = {2022}
}

@inproceedings{pourkamali2021mismatched,
  title        = {Mismatched Estimation of Symmetric Rank-One Matrices Under
                  Gaussian Noise},
  author       = {Pourkamali, Farzad and Macris, Nicolas},
  booktitle    = {International Zurich Seminar on Information and
                  Communication (IZS 2022). Proceedings},
  pages        = {84--88},
  year         = {2022},
  organization = {ETH Zurich}
}

@phdthesis{schmidt2018statistical,
  title  = {Statistical Physics of Sparse and Dense Models in Optimization
            and Inference},
  author = {Schmidt, Hinnerk Christian},
  year   = {2018},
  school = {Universit{\'e} Paris Saclay (COmUE)}
}




@article{voiculescu1991limit,
  title     = {Limit laws for random matrices and free products},
  author    = {Voiculescu, Dan},
  journal   = {Inventiones mathematicae},
  volume    = {104},
  number    = {1},
  pages     = {201--220},
  year      = {1991},
  publisher = {Springer}
}

@article{voiculescu1999analogues,
  title     = {The analogues of entropy and of Fisher's information measure
               in free probability theory: VI. Liberation and mutual free information},
  author    = {Voiculescu, Dan},
  journal   = {Advances in Mathematics},
  volume    = {146},
  number    = {2},
  pages     = {101--166},
  year      = {1999},
  publisher = {Elsevier}
}


@misc{bodin.2212.06757,
  doi       = {10.48550/ARXIV.2212.06757},
  url       = {https://arxiv.org/abs/2212.06757},
  author    = {Bodin, Antoine and Macris, Nicolas},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Gradient flow in the gaussian covariate model: exact solution of learning curves and multiple descent structures},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{chou2020gradient,
  title   = {Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank},
  author  = {Chou, Hung-Hsu and Gieshoff, Carsten and Maly, Johannes and Rauhut, Holger},
  journal = {arXiv preprint arXiv:2011.13772},
  year    = {2020}
}

@article{saxe2013exact,
  title   = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author  = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal = {arXiv preprint arXiv:1312.6120},
  year    = {2013}
}


@article{baik2005phase,
  title   = {Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author  = {J. Baik and G. Ben Arous and S. P{\'e}ch{\'e}},
  journal = {Annals of Probability},
  pages   = {1643},
  year    = {2005}
}

@inproceedings{DBLP:conf/isit/LesieurMLKZ17,
  author    = {Thibault Lesieur and
               L{\'{e}}o Miolane and
               Marc Lelarge and
               Florent Krzakala and
               Lenka Zdeborov{\'{a}}},
  title     = {Statistical and computational phase transitions in spiked tensor estimation},
  booktitle = {2017 {IEEE} International Symposium on Information Theory, {ISIT}
               2017, Aachen, Germany, June 25-30, 2017},
  pages     = {511--515},
  publisher = {{IEEE}},
  year      = {2017},
  url       = {https://doi.org/10.1109/ISIT.2017.8006580},
  doi       = {10.1109/ISIT.2017.8006580},
  timestamp = {Wed, 16 Oct 2019 14:14:48 +0200},
  biburl    = {https://dblp.org/rec/conf/isit/LesieurMLKZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Pch2004TheLE,
  title   = {The largest eigenvalue of small rank perturbations of Hermitian random matrices},
  author  = {P{\'e}ch{\'e}, Sandrine},
  journal = {Probability Theory and Related Fields},
  year    = {2004},
  volume  = {134},
  pages   = {127-173}
}


@book{schwartz1958linear,
  title     = {Linear Operators},
  author    = {Dunford, N and Schwartz, J T},
  publisher = {Wiley  Classics Library},
  year      = {1988}
}

@article{chi2019nonconvex,
  title     = {Nonconvex optimization meets low-rank matrix factorization: An overview},
  author    = {Chi, Yuejie and Lu, Yue M and Chen, Yuxin},
  journal   = {IEEE Transactions on Signal Processing},
  volume    = {67},
  number    = {20},
  pages     = {5239--5269},
  year      = {2019},
  publisher = {IEEE}
}

@article{marchenko1967distribution,
  title     = {Distribution of eigenvalues for some sets of random matrices},
  author    = {Marchenko, Vladimir Alexandrovich and Pastur, Leonid Andreevich},
  journal   = {Matematicheskii Sbornik},
  volume    = {114},
  number    = {4},
  pages     = {507--536},
  year      = {1967},
  publisher = {Russian Academy of Sciences, Steklov Mathematical Institute of Russian~…}
}


@article{camilli2022matrix,
  title   = {Matrix factorization with neural networks},
  author  = {Camilli, Francesco and M{\'e}zard, Marc},
  journal = {arXiv preprint arXiv:2212.02105},
  year    = {2022}
}

@article{ChenChi,
  author  = {Chen, Yudong and Chi, Yuejie},
  journal = {IEEE Signal Processing Magazine},
  title   = {Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation: Recent Theory and Fast Algorithms via Convex and Nonconvex Optimization},
  year    = {2018},
  volume  = {35},
  number  = {4},
  pages   = {14-31},
  doi     = {10.1109/MSP.2018.2821706}
}
  
  @article{Lesieur_2017,
  doi       = {10.1088/1742-5468/aa7284},
  url       = {https://dx.doi.org/10.1088/1742-5468/aa7284},
  year      = {2017},
  month     = {jul},
  publisher = {IOP Publishing and SISSA},
  volume    = {2017},
  number    = {7},
  pages     = {073403},
  author    = {Thibault Lesieur and Florent Krzakala and Lenka Zdeborová},
  title     = {Constrained low-rank matrix estimation: phase transitions, approximate message passing and applications},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment}
}

@inproceedings{Montanari-Richard-2014,
  author    = {Montanari, Andrea and Richard, Emile},
  title     = {A Statistical Model for Tensor PCA},
  year      = {2014},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  pages     = {2897-2905},
  numpages  = {9},
  location  = {Montreal, Canada},
  series    = {NIPS 2014}
}

  @article{Montanari2017EstimationOL,
  title   = {Estimation of low-rank matrices via approximate message passing},
  author  = {Andrea Montanari and Ramji Venkataramanan},
  journal = {The Annals of Statistics},
  year    = {2017}
}

@article{Kabashima_2016,
  doi       = {10.1109/tit.2016.2556702},
  url       = {https://doi.org/10.1109%2Ftit.2016.2556702},
  year      = 2016,
  month     = {jul},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume    = {62},
  number    = {7},
  pages     = {4228--4265},
  author    = {Yoshiyuki Kabashima and Florent Krzakala and Marc Mezard and Ayaka Sakata and Lenka Zdeborova},
  title     = {Phase Transitions and Sample Complexity in Bayes-Optimal Matrix Factorization},
  journal   = {{IEEE} Transactions on Information Theory}
}

@article{CamilliContucciMingione,
  title     = {{An inference problem in a mismatched setting: a spin-glass model with  Mattis interaction}},
  author    = {Francesco Camilli and Pierluigi Contucci and Emanuele Mingione},
  journal   = {SciPost Phys.},
  volume    = {12},
  pages     = {125},
  year      = {2022},
  publisher = {SciPost},
  doi       = {10.21468/SciPostPhys.12.4.125},
  url       = {https://scipost.org/10.21468/SciPostPhys.12.4.125}
}

@misc{BenArous-et-al-2022,
  doi       = {10.48550/ARXIV.2206.04030},
  url       = {https://arxiv.org/abs/2206.04030},
  author    = {Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), Probability (math.PR), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title     = {High-dimensional limit theorems for SGD: Effective dynamics and critical scaling},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Liang-Sen-Sur-2022,
  author    = {Liang, Tengyuan and Sen, Subhabrata and Sur, Pragya},
  keywords  = {Statistics Theory (math.ST), Machine Learning (cs.LG), Probability (math.PR), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {High-dimensional Asymptotics of Langevin Dynamics in Spiked Matrix Models},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


