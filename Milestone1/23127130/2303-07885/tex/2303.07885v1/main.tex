\documentclass[journal,twoside,web]{ieeecolor}
%\overrideIEEEmargins
\let\labelindent\relax 
\let\proof\relax 
\let\endproof\relax 
%
\usepackage{lcsys}
\usepackage{cite}

%
\usepackage{amsmath,amssymb,amsthm,graphics,epsfig,times,enumitem,float,booktabs}
\usepackage{hyperref,stmaryrd,comment,filecontents,bm} 
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=blue, urlcolor=blue,citecolor=blue}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage[dvipsnames]{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{font=small}
\captionsetup[sub]{font=scriptsize,labelfont=small}
\captionsetup{belowskip=-7pt}
%\setlength{\abovedisplayskip}{-1pt}
\setlength{\belowdisplayskip}{4.8pt}

%
\usepackage{times,mathptm}
\usepackage{tikz,tikz-3dplot}
\usetikzlibrary{arrows,shapes,calc}
\usetikzlibrary{calc,intersections,through,backgrounds}
\usetikzlibrary{positioning}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{pgfplotstable}
\usepgfplotslibrary{units}
%  
%% THEOREM, LEMMA ....
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem Statement}
\newtheorem*{problem*}{Problem Statement}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example} 
%  
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\tb}[1]{\textcolor{black}{#1}}
\newcommand{\diag}[1]{\textrm{diag}\{#1\}}
\newcommand{\col}[1]{\textrm{col}\{#1\}}
\newcommand{\row}[1]{\textrm{row}\{#1\}}
\newcommand{\idm}[1]{\mathbf{I}_{#1}}
\newcommand{\zdm}[1]{\mathbf{0}_{#1}}
\newcommand{\odm}[1]{\mathbf{1}_{#1}} 
\newcommand{\cufrac}[2]{\tfrac{\partial #1}{\partial #2}}
%
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\PVR}[1]{\textcolor{Sepia}{#1}}
\newcommand{\AG}[1]{\textcolor{CadetBlue}{#1}}
% 
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{ }{ }

% Symbol Definitions
\def\B{{\mathbb B}}
\def\w{{\widetilde w}}
\def\t{\tau}
\def \tb {\textcolor{blue}}
\def \tr {\textcolor{red}}
\def\inpr#1{\left\langle #1\right\rangle}
\def\Bf{\mathbf}
\def\x{\mathbf{x}}
\def\z{\mathbf{z}}
\def\B{\mathit{B}}
\def\Bs{\mathsf B}
\def\mR{\mathsf R}
\def\R{{\mathbb R}}
\def\u{\mathbf u}
\def\v{\mathbf v}
\def\H{\mathit H}
\def\C{\mathcal C}
\def\T{\mathsf T}
\def\A{\mathbf A}
\def\V{\mathcal V}

\begin{document}
	\title{Optimal Role Assignment for Multi-Player Reach-Avoid Differential Games in 3D Space}
	\author{Abinash Agasti, Puduru Viswanadha Reddy, \IEEEmembership{Member, IEEE}, Bharath Bhikkaji
		\thanks{A. Agasti, P. V. Reddy and B. Bhikkaji are with the Department of Electrical Engineering, Indian Institute of Technology-Madras, Chennai, 600036, India.  
			(e-mail: ee20d201@smail.iitm.ac.in, vishwa@ee.iitm.ac.in, bharath@ee.iitm.ac.in)}} 
	
	\maketitle
	\thispagestyle{empty}
	\begin{abstract} 
		In this article an $n$-pursuer versus $m$-evader reach-avoid differential game in 3D space is studied. A team of evaders wish to \textit{reach} a stationary target while a team of pursuers wish to \textit{avoid} said scenario and attempt to intercept the evading team. The multiplayer scenario is formulated in a differential game framework. This article provides an optimal solution for the particular case of $n=m=1$ and extends it to a general scenario of $n\geq m$ via an optimal role assignment algorithm. The algorithm is based on a linear programming based method and the consequent solution is proven to be a saddle point trajectory in feedback for the differential game.
		
	\end{abstract}
	
	\begin{IEEEkeywords}Reach-Avoid Differential Games; Linear Programming; Task assignment
	\end{IEEEkeywords}
	
	\section{Introduction} Several autonomous systems are built to cooperate in groups to complete tasks in hostile environments. Differential game theory \cite{isaacs_1965} offers the ideal framework to model and analyze optimal behavior in these conflictual circumstances. Representative examples include a team of robots rescuing civilians in a hazardous scenario or a group of prey fleeing from predators towards a haven. These require two termination criteria to capture the possibility of either team winning. Such situations can be modelled as a reach-avoid differential game (RADG). Reach-avoid games were first analyzed in a differential game framework to study reachability problems in adversarial scenarios \cite{margellos_2011}. These earlier attempts \cite{tomlin_2005,tomlin_2014} were based on numerical techniques to circumvent the difficulty of obtaining an analytical solution satisfying the Isaacs condition. However, numerical methods often succumb to the curse of dimensionality as the number of agents increases, making a real-time implementation infeasible. This further led to attempts to obtain analytical saddle-point solution in state feedback form ensuring the robustness of a player's strategies against deviation from the optimal strategies by the players \cite{yan_2d1a_2019,yan_guarding_2022,garcia_2020}. \par 
	This article considers a multiplayer reach-avoid differential game (MRADG) which requires a two-layered solution. The first layer assigns pursuers to capture evaders. Following this assignment, the second layer deals with designing saddle-point strategies for each player. The optimal assignment problem has been discussed in the existing literature \cite{tomlin_2017,tomlin_2018,yan_assignment_2020,yan_matching_2022,weintraub_2020} through a matching-based strategy to allot an evader to the best-suited pursuer. 
	However, these approaches either provide open-loop solutions or approximate solutions to allow for computational efficiency.	Garcia et al. \cite{garcia_2021} on the other hand, provide state-feedback optimal strategies satisfying the Isaacs condition, by a codesign of optimal assignment and optimal guidance strategies. But, the assignment strategy is determined by searching over all feasible assignments and hence a computationally inefficient real-time solution. \par 
	Thus, there is a lack of literature that provides an optimal assignment scheme such that the resulting guidance strategies satisfy the Isaacs condition while ensuring that the assignment scheme is computationally efficient. This article attempts to bridge this gap by considering an MRADG in 3D space and providing a linear programming(LP)-based assignment scheme to obtain a saddle-point solution for the differential game. Moreover, the reach-avoid game considered in 3D space is a generalization of a game considered in the literature \cite{garcia_2020}. Unlike \cite{garcia_2020}, we consider multiple pursuers and multiple evaders with unequal speeds. Also, as opposed to the majority of multiplayer games in literature considering all pursuers superior to all evaders, we allow a subset of evaders to be superior to a subset of pursuers, thus enabling a more realistic scenario. \par 
	The paper is organized as follows. The problem formulation of the MRADG is stated in Section \ref{sec:prelim}. Section \ref{sec:1v1} considers the particular case of one pursuer against one evader. Using this result, an assignment technique is proposed in Section \ref{sec:multiplayer} solve the MRADG analytically. Section \ref{sec:num} provides a few numerical illustrations, and finally conclusions are drawn in Section \ref{sec:conc} along with citing some future directions.
	
	
	%%%
	%%%
	%%%
	\section{Preliminaries and Problem Formulation}
	\label{sec:prelim}
	We consider a multi-player reach-avoid differential game  (MRADG) consisting of $n$ pursuers and $m$ evaders. A player in the evading team is denoted by $E_i$, $i\in M:=\{1,\cdots,m\}$, and one in the pursuing team by $P_j$, $j\in N:=\{1,\cdots,n\}$. The players are assumed to be holonomic, and interact in  3-dimensional Euclidean space. The evaders aim to reach a stationary target, while the pursuers desire to prevent this outcome. We assume (without loss of generality) that  the target is located at the origin. The position vector or state of $E_i$ and $P_j$ are denoted respectively by $\x_{E_i} :=(x_{E_i} ,y_{E_i} ,z_{E_i} )\in \mathbb{R}^3$ and $\x_{P_j} :=(x_{P_j} ,y_{P_j} ,z_{P_j} )\in \mathbb{R}^3$  for   $i\in M$  and $j\in N$. The global state space of the differential game is denoted by $\x:=(\x_E,\x_P)\in \mathbb R^{3(m+n)}$, where $\x_E=\{\x_{E_i}:i\in M\}$ and $\x_P=\{\x_{P_j}:j\in N\}$.  We denote the controls of the players $E_i$ and $P_j$ respectively by $\u_{E_i}=(u_{x_i},u_{y_i},u_{z_i})$  and $\v_{P_j}=(v_{x_j},v_{y_j},v_{z_j})$  for $i\in M$ and $j\in N$. The joint team controls for the evaders and pursuers are denoted by $\u:=\{\u_{E_i}:i\in M\}$ and $\v:=\{\v_{P_j}:j\in N\}$ respectively. We assume that players $E_i$ and $P_j$ move with constant speeds $U_i> 0$ and $V_j> 0$ respectively. There is no further assumption on the speeds of the players, and hence, the speed ratios $\alpha_{ij}=U_i/V_j$ can take any positive real value for all $i\in M$ and $j\in N$. Consequently, the admissible control sets of $E_i$ and $P_j$  are given respectively by $\{\u_{E_i}\in \mathbb R^3:||\u_{E_i}|| =U_i \}$ and $\{\v_{P_j}\in \mathbb R^3:||\v_{P_j}|| =V_j \}$. The players have simple motion dynamics given by
	% 
	\begin{equation}
		\begin{aligned}
			&\dot{x}_{E_i}(t) =u_{x_i}(t),~	\dot{y}_{E_i}(t)=u_{y_i}(t), ~	\dot{z}_{E_i}(t)=u_{z_i}(t), \\
			&\dot{x}_{P_j}(t)=v_{x_j}(t),~ \dot{y}_{P_j}(t)=v_{y_j}(t),~\dot{z}_{P_j}(t)=v_{z_j}(t)
			\label{eq:multi_dynamics}
		\end{aligned} 
	\end{equation}
	with  initial positions  
	$\x_{E_i}(0)=(x_{E_{i_0}},y_{E_{i_0}},z_{E_{i_0}})$ and    $\x_{P_j}(0)=(x_{P_{j_0}},y_{P_{j_0}},z_{P_{j_0}})$ for $i\in M$ and $j\in N$.  The global initial state of the system is denoted by $\x_0\in \mathbb R^{3(m+n)}$.
	\par
	The MRADG considered in this paper is characterized by two termination criteria: either all the evaders are captured by the pursuer team or at least one of the evaders reaches the origin. We assume point capture, that is, an evader is said to have been captured when its state vector coincides with that of a pursuer, and an evader is said to have reached the target when its state vector coincides with the origin. After capturing an evader, the speed of the pursuer drops to zero, and both the pursuer and the evader cease to remain active in the game. To represent the terminal criteria, we first define binary variables $\{\mu_{ij}\in \{0,1\},~i\in M,~j\in N\}$, where $\mu_{ij}=1$ when $P_j$ is assigned to capture $E_i$, and $0$ otherwise. 
	
	
	Consequently, the termination set for the game is given by
	\begin{equation}
		\T=\T_E\cup\T_P, \label{eq:termination}
	\end{equation}
	where
	\begin{equation}
		\begin{aligned}
			\T_E=\{ \x\in\R^{3(m+n)}~\big|~\exists i\in M\ \text{ such that  }
			\vert\vert\x_{E_i} \vert\vert=0\},
		\end{aligned}
	\end{equation}
	represents the game outcome when  atleast one of the evaders reaches the target, and
	\begin{equation}
		\begin{aligned}
			\T_P=\{ \x\in\R^{3(m+n)}~\big|~\forall i\in M\ \exists j\in N, 	\text{ such that}& \\
			\mu_{ij}=1 \text{ and }	\vert\vert \x_{P_j}-\x_{E_i} \vert\vert=0&\},
		\end{aligned}
	\end{equation}
	represents the outcome when all the evaders are captured by the pursuer team. Further, the associated termination time is calculated as $t_f:=\inf\{t\in\R^+: \x(t)\in\T\}$.  As there are two outcomes in MRADG,  
	Game of Kind (GoK) needs to be solved to partition the global state space into winning regions for the pursuer  and evader teams. Let $\B:\R^{3(m+n)}\shortrightarrow\R$ denote the Barrier function whose zero level set constructs the barrier surface of the MRADG given by
	\begin{equation}
		\Bs(\x):=\{\x~|~\B(\x)=0\}, \label{eq:barrier_surface}
	\end{equation}
	which  partitions $\R^{3(m+n)}$ into the following two sets
	\begin{equation}
		\mR_P:=\{\x ~|~\B(\x)>0 \}, \quad  \mR_E:=\{\x ~|~\B(\x)<0\}, \label{eq:partition_sets}
	\end{equation}
	where $\mR_P$ and $\mR_E$ denote winning regions for the pursuer the evaders teams respectively. The optimal strategies of the players within their winning regions are obtained by solving the Game of Degree (GoD). Let the subscript $f$ denote a value at terminal time. Starting in the region $\mR_E$, consider the cost function given by 
	\begin{equation}
		J(\u(.),\v(.);\x_0)=-\sum\limits_{i\in\tilde{M}}\min\limits_{j\in \tilde{N}_i}\vert\vert\x_{P_{j_f}}\vert\vert, \label{eq:multi_Ecost}
	\end{equation}
	where $\tilde{M}=\{i\in M:\vert\vert \x_{E_{i_f}}\vert\vert =0\}$, $\tilde{N}_i=\{j\in N:\mu_{ij}=1\}$. 
	%	\textcolor{Brown}{This cost accumulates the individual costs corresponding to all the evaders $E_i$ reaching the target at the terminal time. The negative sign conventionally allows the pursuers to be the maximizing team for the cost in \eqref{eq:multi_Ecost}. This convention is also applied in the cost considered when starting in the region $\mR_P$, which is given by}
	This cost accumulates the individual costs corresponding to all the evaders $E_i$ reaching the target at the terminal time. The individual cost is in turn given by the closest distance of all pursuers assigned to $E_i$, from the target. The negative sign is a conventional choice to allow for the pursuers to be the maximizing team for the cost in \eqref{eq:multi_Ecost} as well as in the cost considered starting in the region $\mR_P$, given by 
	\begin{equation}
		J(\u(.),\v(.);\x_0)=\sum\limits_{i\in M}\vert\vert\x_{E_{i_f}}\vert\vert. \label{eq:multi_Pcost}
	\end{equation}
	This cost function again represents an accumulation of individual costs corresponding to the terminal distance of each evader from the target.
	The optimal payoff in this game, referred to as the Value of the game, is defined as
	\begin{equation}
		\V(\x_0):=\min\limits_{\u(.)}\max\limits_{\v(.)}J(\u(.),\v(.);\x_0) \label{eq:valfun}
	\end{equation} 
	subject to \eqref{eq:multi_dynamics} and \eqref{eq:termination}, where $\u(.)$ and $\v(.)$ are the teams' state feedback strategies. 
	
	\begin{problem*}
		Clearly, a solution of a  MRADG consists of two steps. The first step involves an assignment problem, which is combinatorial in nature, and the second step involves computation of the barrier function and value of the game. In this paper, we seek to solve this co-design problem through a computationally efficient assignment scheme. Then, we aim to provide an analytical characterization of the barrier function and the value function, to obtain optimal strategies of the players in feedback form.
	\end{problem*}
	%
	
	We state the following preliminary result which will be used throughout the reminder of the paper.
	%
	\begin{theorem}
		Consider the MRADG described by  \eqref{eq:multi_dynamics},    \eqref{eq:multi_Ecost} and \eqref{eq:multi_Pcost}. The optimal headings are constant and the optimal trajectories are straight lines. \label{theorem:prelim}
	\end{theorem}
	%
	\begin{proof}
		As the cost functions  \eqref{eq:multi_Ecost} and \eqref{eq:multi_Pcost} are of terminal type, the  Hamiltonian associated with the saddle-point problem in \eqref{eq:valfun}
		is written as 
		\begin{equation}
			\begin{aligned}
				\H &=\sum\limits_{i\in M}\langle \lambda_{E_i},\dot\x_{E_i}\rangle+\sum\limits_{j\in N} \langle\lambda_{P_j},\dot\x_{P_j}\rangle\\
				&=\sum\limits_{i\in M} \langle \lambda_{E_i},\u_i\rangle+\sum\limits_{j\in N} \langle \lambda_{P_j},\v_j\rangle,
			\end{aligned}
		\end{equation}
		where  $\lambda=(\lambda_{E_1},...,\lambda_{E_m},\lambda_{P_1},...,\lambda_{P_n})^T\in\R^{3(m+n)}$ denotes the costate vector. As the Hamiltonian is independent of $\x$   the costate dynamics is obtained as $\dot\lambda=-\frac{\partial}{\partial \x}\H=0$. So, the costate vector remain constant under optimal play. The saddle-point controls must satisfy $\min_{\u}\max_{\v}H=0$ and thus depend on the costates. As the costates are constant, this implies that the optimal controls also remain constant.  Consequently, the optimal trajectories are straight lines. 
	\end{proof}
	%%%%
	% 
	%%%%
	\section{1 versus 1 Differential Game}
	\label{sec:1v1}
	In this section, we will analyze a RADG with only one pursuer and one evader, which we will refer to as the \texttt{1v1} RADG for brevity. To keep the notation consistent with MRADG we label the evader and pursuer as $E_i$ and $P_j$ respectively. The computation of the Barrier function involves finding the Apollonius sphere, that is the locus of all the points which can be reached by the players at the same time. The desired locus is given by $\frac{\vert\vert \x-\x_{P_j}\vert\vert^2}{v_{P_j}^2}=\frac{\vert\vert \x-\x_{E_i}\vert\vert^2}{v_{E_i}^2} \Rightarrow \alpha_{ij}^2\vert\vert \x-\x_{P_j}\vert\vert^2=\vert\vert \x-\x_{E_i}\vert\vert^2$.  Upon simplification, the Apollonius sphere is calculated as
	\begin{equation}
		(x-x_{c_{ij}})^2+(y-y_{c_{ij}})^2+(z-z_{c_{ij}})^2=r_{c_{ij}}^2, \label{eq:apsphere_E}
	\end{equation}
	where $x_{c_{ij}}=\frac{x_{E_i}-\alpha_{ij}^2x_{P_j}}{1-\alpha_{ij}^2}$, $y_{c_{ij}}=\frac{y_{E_i}-\alpha_{ij}^2y_{P_j}}{1-\alpha_{ij}^2}$, $z_{c_{ij}}=\frac{z_{E_i}-\alpha_{ij}^2z_{P_j}}{1-\alpha_{ij}^2}$, and $r_{c_{ij}}=\frac{\alpha_{ij}}{1-\alpha_{ij}^2}||\x_{P_j}-\x_{E_i}||$. The next result is concerned with the computation of the
	Barrier function.
	\begin{lemma}
		The   function $\B_{ij}(\x):\R^6\shortrightarrow\R$ defined by 
		\begin{equation}
			\B_{ij}(\x)=R_{E_i}^2-\alpha_{ij}^2R_{P_j}^2, \label{eq:1v1_barrier}
		\end{equation}
		where $R_{E_i}=\vert\vert x_{E_i}\vert\vert$, and $R_{P_j}=\vert\vert x_{P_j}\vert\vert$, 
		is a Barrier function for the \texttt{1v1} RADG. 
	\end{lemma}
	\begin{proof}
		If $\B_{ij}(\x)>0$ for $\x\in\R^6$, then $R_{E_i}^2>\alpha_{ij}^2R_{P_j}^2 \Rightarrow R_{E_i}^2/U_i^2>{R_{P_j}^2}/{V_j^2}$. The pursuer can thus reach the origin faster than the evader, hence this condition characterizes the pursuer winning region $\mR_P$. On the other hand, if $\B_{ij}(\x)<0$, then the evader can reach the origin faster than the pursuer, hence this condition characterizes the evader winning region $\mR_E$. If $\B_{ij}(\x)=0$, both players can reach the origin simultaneously. The barrier surface given by \eqref{eq:barrier_surface} characterizes this tie situation.
	\end{proof}
	Using the Barrier function \eqref{eq:1v1_barrier}, we provide optimal strategies of the players in their respective winning
	regions in the next two results.  
	\begin{theorem}
		Consider the \texttt{1v1} RADG with $\x\in\mR_P$ and $\alpha_{ij}<1$. The Value function is $\C^1$ and it is the solution of the Hamilton-Jacobi-Isaacs (HJI) partial differential equation. The Value function is given by 
		\begin{equation}
			\V_{ij}(\x)=R_{c_{ij}}-r_{c_{ij}}, \label{eq:value_Pout}
		\end{equation}
		where $R_{c_{ij}}=\vert\vert \x_{c_{ij}} \vert\vert$, with $\x_{c_{ij}}=(x_{c_{ij}},y_{c_{ij}},z_{c_{ij}})$,  %$\sqrt{x_{c_{ij}}^2+y_{c_{ij}}^2+z_{c_{ij}}^2}$, 
		and $x_{c_{ij}},\ y_{c_{ij}},\ z_{c_{ij}}$, and $r_{c_{ij}}$ are as in \eqref{eq:apsphere_E}. \label{theorem:1v1_varspeed}
	\end{theorem}
	\begin{proof}
		Since $\x\in\mR_P$ and $\alpha_{ij}<1$, the evader is located inside the Apollonius sphere with the target lying outside it. As both the players reach the Apollonius sphere at the same time, considering the cost specified by \eqref{eq:multi_Pcost}, the evader must choose the point on the sphere that minimizes its distance from the origin. The coordinates of this point can be determined as the solution of the following  equality constrained optimization problem  
		\begin{equation}
			\begin{aligned}
				\min\ &x^2+y^2+z^2,\\ \label{eq:opt_sphere}
				\text{subject to } (x-x_{c_{ij}}&)^2+(y-y_{c_{ij}})^2+(z-z_{c_{ij}})^2=r_{c_{ij}}^2.
			\end{aligned}
		\end{equation}
		Taking the Lagrange multiplier associated with the equality constraint as $\delta$, the first order necessary condition is obtained as  $
		x+\delta(x-x_{c_{ij}})=0$, $
		y+\delta(y-y_{c_{ij}})=0$, and $ 
		z+\delta(z-z_{c_{ij}})=0$.
		The desired optimal point is obtained as $I=(x^*,y^*,z^*)=\frac{\delta}{1+\delta}(x_{c_{ij}},y_{c_{ij}},z_{c_{ij}})$. As $I$ must lie on  the Apollonius sphere, it satisfies a quadratic equation given by $(x_{c_{ij}}^2+y_{c_{ij}}^2+z_{c_{ij}}^2)\left(1-\frac{\delta}{1+\delta}\right)^2=r_{c_{ij}}^2$. Solving for $\delta$, we get $1-\tfrac{\delta}{1+\delta}=1\mp\tfrac{r_{c_{ij}}}{R_{c_{ij}}}$. 
		One solution corresponds to the point on the sphere farthest from the origin, while the other solution provides the candidate interception point, given by:
		\begin{equation}
			I=(x^*,y^*,z^*)=\left( 1-\tfrac{r_{c_{ij}}}{R_{c_{ij}}}\right) (x_{c_{ij}},y_{c_{ij}},z_{c_{ij}}).
		\end{equation}
		The payoff of the game when the evader and the pursuer choose to head towards the interception point (as a result of Theorem \ref{theorem:prelim}) yields a guess of the Value function as $\V_{ij}(\x)=||I||=R_{c_{ij}}-r_{c_{ij}}$. It is now imperative to verify that the candidate Value function satisfies the HJI equation. The partial derivatives of $\V_{ij}$ can be written as follows: 
		\begin{equation}
			\begin{aligned}
				\begin{bmatrix}\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}\end{bmatrix}&=\tfrac{1}{1-\alpha_{ij}^2}\left(\tfrac{\x_{c_{ij}}}{R_{c_{ij}}} -\tfrac{\alpha_{ij}^2}{1-\alpha_{ij}^2}\tfrac{\x_{E_i}-\x_{P_j}}{r_{c_{ij}}}\right), \\
				\begin{bmatrix}\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}&\cufrac{\V_{ij}}{z_{P_j}}\end{bmatrix}&=\tfrac{\alpha_{ij}^2}{1-\alpha_{ij}^2}\left(-\tfrac{\x_{c_{ij}}}{R_{c_{ij}}} +\tfrac{1}{1-\alpha_{ij}^2}\tfrac{\x_{E_i}-\x_{P_j}}{r_{c_{ij}}}\right).
			\end{aligned}
			\label{eq:gradV_compact}
		\end{equation}
		%
		The HJI equation associated with the saddle-point problem \eqref{eq:valfun} is given by
		%
		\begin{multline}
			\min\limits_{\u_i}\max\limits_{\v_j}\left(\cufrac{\V_{ij}}{x_{E_i}} u_{x_i}+\cufrac{\V_{ij}}{y_{E_i}}u_{y_i}+\cufrac{\V_{ij}}{z_{E_i}}u_{z_i}\right. \\
			\left. +\cufrac{\V_{ij}}{x_{P_j}}v_{x_j}+\cufrac{\V_{ij}}{y_{P_j}}v_{y_j}+\cufrac{\V_{ij}}{z_{P_j}}v_{z_j}\right)=0. \label{eq:ME1}
		\end{multline}
		%
		The optimal controls in the feedback form are then obtained as  
		\begin{equation}
			\begin{aligned}
				\u_i^*&=-\tfrac{U_i}{\rho_{E_i}}\begin{bmatrix}
					\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}
				\end{bmatrix},\\  \v_j^*&=\tfrac{V_j}{\rho_{P_j}}\begin{bmatrix}
					\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}& \cufrac{\V_{ij}}{z_{P_j}}
				\end{bmatrix},\label{eq:opt_con}
			\end{aligned}
		\end{equation}
		where $\rho_{E_i}=\sqrt{\cufrac{\V_{ij}}{x_{E_i}}^2+\cufrac{\V_{ij}}{y_{E_i}}^2+\cufrac{\V_{ij}}{z_{E_i}}^2}$ and $\rho_{P_j}=\sqrt{\cufrac{\V_{ij}}{x_{P_j}}^2+\cufrac{\V_{ij}}{y_{P_j}}^2+\cufrac{\V_{ij}}{z_{P_j}}^2}$. Substituting the optimal controls \eqref{eq:opt_con} in the HJI equation \eqref{eq:ME1}, we get
		\begin{equation}
			-\alpha_{ij}\rho_{E_i}+\rho_{P_j}=0. \label{eq:ME2}
		\end{equation}
		Next, we verify if the candidate Value function \eqref{eq:value_Pout}
		indeed satisfies the equation \eqref{eq:ME2}. Using \eqref{eq:gradV_compact}, we get
		%
		\begin{align*}
			\alpha_{ij}^2\rho_{E_i}^2&=\alpha_{ij}^2\left( \cufrac{\V_{ij}}{x_{E_i}}^2+\cufrac{\V_{ij}}{y_{E_i}}^2+\cufrac{\V_{ij}}{z_{E_i}}^2\right) \\ 
			&=\tfrac{\alpha_{ij}^2}{(1-\alpha_{ij}^2)^2}\left[1-\tfrac{2\alpha_{ij}^2}{1-\alpha_{ij}^2}\tfrac{\langle \x_{c_{ij}},\x_{EP} \rangle}{R_{c_{ij}}r_{c_{ij}}}+\alpha_{ij}^2 \right],  \\
			\rho_{P_j}^2&=\cufrac{\V_{ij}}{x_{P_j}}^2+\cufrac{\V_{ij}}{y_{P_j}}^2+\cufrac{\V_{ij}}{z_{P_j}}^2 \\
			&=\tfrac{\alpha_{ij}^4}{(1-\alpha_{ij}^2)^2}\left[1-\tfrac{2}{1-\alpha_{ij}^2}\tfrac{\langle \x_{c_{ij}},\x_{EP} \rangle}{R_{c_{ij}}r_{c_{ij}}}+\tfrac{1}{\alpha_{ij}^2} \right] 
			= \alpha_{ij}^2\rho_{E_i}^2.
		\end{align*} 
		The function defined by \eqref{eq:value_Pout} satisfies the HJI equation and is, therefore, the Value function of the differential game. The saddle-point strategies of the players in feedback form are obtained as \eqref{eq:opt_con}.
	\end{proof}
	\begin{theorem}Consider the \texttt{1v1} RADG with $\x\in\mR_E$ and $\alpha_{ij}<1$. The Value function is $\C^1$ and it is the solution of the Hamilton-Jacobi-Isaacs (HJI) partial differential equation. The Value function is given by  
		\begin{equation}
			\V_{ij}(\x)=R_{P_{j}}-\frac{R_{E_{i}}}{\alpha_{ij}}, \label{eq:valueE}
		\end{equation}
		where $R_{P_{j}}=\vert\vert \x_{P_j} \vert\vert$, and $R_{E_{i}}=\vert\vert \x_{E_i} \vert\vert$.
		\label{theorem:1v1_varspeed_E}
	\end{theorem}
	\begin{proof}
		Since $\x\in\mR_E$ and $\alpha_{ij}<1$, the pursuer is located inside the Apollonius sphere with the target lying outside it. Considering the cost specified in \eqref{eq:multi_Ecost}, as the target lies in the dominance region of the evader, both the players must head straight to the target (as a result of \ref{theorem:prelim}). In such a scenario, the time taken by the evader to reach origin is given by $t_{E_i}=\tfrac{\sqrt{x_{E_i}^2+y_{E_i}^2+z_{E_i}^2}}{U_i}=\tfrac{R_{E_i}}{U_i}$.
		%	\begin{equation}
			%		t_{E_i}=\frac{\sqrt{x_{E_i}^2+y_{E_i}^2+z_{E_i}^2}}{U_i}=\frac{R_{E_i}}{U_i}
			%	\end{equation}
		In this time, $P_j$ covers a distance of $V_jt_{E_i}$ along the direction towards the origin. Hence, the terminal location of $P_j$ is given by 		$\x_{P_{j_f}}=\x_{P_j}-V_jt_{E_i}\tfrac{\x_{P_j}}{R_{P_j}}$, and the payoff is given by this distance $||\x_{P_{j_f}}||$. This payoff now forms a guess for the Value function given by $\V_{ij}(\x)=||\x_{P_{j_f}}||$ which equals the expression proposed in \eqref{eq:valueE}.
		\begin{comment}
		\begin{equation}
			\begin{aligned}
				\V(\x)&=\vert\vert\x_{P_{j_f}}\vert\vert=\left\vert\left\vert\x_{P_j}-V_jt_{E_i}\frac{\x_{P_j}}{R_{P_j}} \right\vert\right\vert\\
				&=\left[\vert\vert \x_{P_j} \vert\vert^2 -2\frac{R_{E_i}}{\alpha_{ij}}\left\langle \x_{P_j},\frac{\x_{P_j}}{R_{P_j}}\right\rangle+\frac{R_{E_i}^2}{\alpha_{ij}^2}\left\vert\left\vert \frac{\x_{P_j}}{\alpha_{ij}} \right\vert\right\vert^2 \right]^{1/2}\\
				&=R_{P_j}-\frac{R_{E_i}}{\alpha}.
			\end{aligned}
			\label{eq:value_guess_E}
		\end{equation}
		\end{comment}
		The partial derivatives of the Value function guess are given by
		\begin{comment}
			\begin{equation}
				\begin{aligned}
					V_{x_{E_i}}=-\frac{1}{\alpha}\frac{x_{E_i}}{R_{E_i}} \qquad V_{x_{P_j}}=\frac{x_{P_j}}{R_{P_j}}\\
					V_{y_{E_i}}=-\frac{1}{\alpha}\frac{y_{E_i}}{R_{E_i}} \qquad V_{y_{P_j}}=\frac{y_{P_j}}{R_{P_j}}\\
					V_{z_{E_i}}=-\frac{1}{\alpha}\frac{z_{E_i}}{R_{E_i}} \qquad V_{z_{P_j}}=\frac{z_{P_j}}{R_{P_j}}
				\end{aligned}
			\end{equation}
		\end{comment}
		\begin{equation}
			\begin{aligned}
				\begin{bmatrix}\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}\end{bmatrix}&=-\tfrac{1}{\alpha_{ij}}\tfrac{\x_{E_i}}{R_{E_i}},\\
				\begin{bmatrix}\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}&\cufrac{\V_{ij}}{z_{P_j}}\end{bmatrix}&=\tfrac{\x_{P_j}}{R_{P_j}}.
			\end{aligned}
		\end{equation}
		It remains to be checked if the proposed Value function \eqref{eq:valueE} satisfies the HJI equation which can be written as
	\begin{multline}
		\min\limits_{\u_i}\max\limits_{\v_j}\left(\cufrac{\V_{ij}}{x_{E_i}} u_{x_i}+\cufrac{\V_{ij}}{y_{E_i}}u_{y_i}+\cufrac{\V_{ij}}{z_{E_i}}u_{z_i}\right. \\
		\left. +\cufrac{\V_{ij}}{x_{P_j}}v_{x_j}+\cufrac{\V_{ij}}{y_{P_j}}v_{y_j}+\cufrac{\V_{ij}}{z_{P_j}}v_{z_j}\right)=0. \label{eq:ME1E}
	\end{multline}
		Thus, the optimal controls in terms of the Value function can be written as 
		\begin{equation}
			\begin{aligned}
				\u_i^*&=-\tfrac{U_i}{\rho_{E_i}}\begin{bmatrix}
					\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}
				\end{bmatrix},\\  \v_j^*&=\tfrac{V_j}{\rho_{P_j}}\begin{bmatrix}
					\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}& \cufrac{\V_{ij}}{z_{P_j}}
				\end{bmatrix}. \label{eq:opt_conE}
			\end{aligned}
		\end{equation}
	where $\rho_{E_i}=\sqrt{\cufrac{\V_{ij}}{x_{E_i}}^2+\cufrac{\V_{ij}}{y_{E_i}}^2+\cufrac{\V_{ij}}{z_{E_i}}^2}$ and $\rho_{P_j}=\sqrt{\cufrac{\V_{ij}}{x_{P_j}}^2+\cufrac{\V_{ij}}{y_{P_j}}^2+\cufrac{\V_{ij}}{z_{P_j}}^2}$. Substituting the optimal controls \eqref{eq:opt_conE} in the HJI equation \eqref{eq:ME1E}, we get
		\begin{equation}
			-\alpha_{ij}\rho_{E_i}+\rho_{P_j}=0. \label{eq:ME2E}
		\end{equation}
		Plugging in the Value function proposed in \eqref{eq:valueE} into \eqref{eq:ME2E}, the LHS is determined as
		\begin{equation}
			\begin{aligned}
				%&-\alpha_{ij}\rho_{E_i}+\rho_{P_j}\\
				% &=-\alpha_{ij}\sqrt{\V_{x_{E_i}}^2+\V_{y_{E_i}}^2+\V_{z_{E_i}}^2}+\sqrt{\V_{x_{P_j}}^2+\V_{y_{P_j}}^2+\V_{z_{P_j}}^2}\\
				&=-\alpha_{ij}\sqrt{\tfrac{1}{\alpha_{ij}^2}\left[\left(\tfrac{x_{E_i}}{R_{E_i}} \right)^2+\left( \tfrac{y_{E_i}}{R_{E_i}}\right)^2+\left( \tfrac{z_{E_i}}{R_{E_i}}\right)^2 \right] }\\
				&\qquad \qquad +\sqrt{\left(\tfrac{x_{P_j}}{R_{P_j}} \right)^2+\left( \tfrac{y_{P_j}}{R_{P_j}}\right)^2+\left( \tfrac{z_{P_j}}{R_{P_j}}\right)^2}\\
				&=-\alpha_{ij}\tfrac{1}{\alpha_{ij}}+1=0.
			\end{aligned}
		\end{equation}
	The function defined by \eqref{eq:valueE} satisfies the HJI equation and is, therefore, the Value function of the differential game. The saddle-point strategies of the players in feedback form are obtained as \eqref{eq:opt_conE}.
	\end{proof}
	\begin{remark}
		Theorems \ref{theorem:1v1_varspeed} and \ref{theorem:1v1_varspeed_E} generalize the analysis of the \texttt{1v1} 3D RADG presented in \cite{garcia_2020}. The previous work considered a specific case with $\alpha_{ij}=1$ and thus a plane for interception point calculations instead of an Apollonius sphere.
	\end{remark}
	%%%%
	\section{Multiplayer Differential Game}
	\label{sec:multiplayer}
	In this section, we analyze the MRADG using the results obtained from the previous section. Specifically, we solve the co-design problem by providing an optimal assignment scheme and an analytical characterization of the Barrier and the Value of the game in the puruser team's winning region. To this end, we have the following assumptions on players' interactions.
	\begin{assumption} 
		\begin{enumerate}[label=(\roman*)]
			\item \label{itm:assumitem1}  A pursuer can capture at most one evader, and an evader can be pursued by at most one pursuer.
			\item \label{itm:assumitem2} The pursuing team is atleast as large as the evading team, that is, $n\geq m$.
			%
			\item \label{itm:assumitem3} The pursuers commit to their assignments throughout the duration of the game.
		\end{enumerate}
		\label{assum:interactions}
	\end{assumption} 
	%
	Item \ref{itm:assumitem1} is a crucial assumption in our paper. Using this, we show that there exists a linear-programming based optimal assignment scheme for matching the pursuers with   evaders. Item \ref{itm:assumitem2} is a natural consequence of Item \ref{itm:assumitem1}, because if $n<m$ then, by Item \ref{itm:assumitem1}, at least $m-n$ evaders can reach the target, thus making the game outcome (evader team wins) trivial. 
	Item \ref{itm:assumitem3} implies that the pursuers strictly commit to their assignments throughout the game i.e. $\mu_{ij}(t)=\mu_{ij}(0)$. 
	Note that we do not make the assumption that all pursuers are faster than all evaders, as is often assumed in the existing literature.
	\par	
	Next, following Assumption \ref{assum:interactions} Item \ref{itm:assumitem1} we consider the set of all assignments with \texttt{1v1} matchings as follows:
	\begin{definition} \label{def:aval}
		The set of all \emph{feasible} assignments is denoted by
		\begin{align}
			\Sigma:=\big\{\bm{\mu}\in\{0,1\}^{m\times n}~\big|~
			\bm\mu.\bm 1_n=\bm1_m,~ \bm\mu^T.\bm 1_m\leq \bm1_n \big\}, \label{eq:feasibleset}
		\end{align}
		where $\bm1_k$ denotes a column one vector of size $k$. Denote $\mu_{ij}$ as the element in $i^{th}$ row and $j^{th}$ column of $\bm\mu$. Associated with every $\bm\mu\in\Sigma$, we define 
		\begin{equation}
			A_\mu:=\big\{ ij\in M\times N~\big|~\mu_{ij}=1 \big\}.
		\end{equation}
		%		indicating the pursuer assigned to each evader by the assignment $\bm\mu$. 
		Further, we denote the set of all \emph{feasible probabilistic} assignments by
		\begin{align}
			\Gamma:=\big\{\bm{\gamma}\in[0,1]^{m\times n}~\big|~\bm\gamma.\bm 1_n=\bm1_m,~ \bm\gamma^T.\bm 1_m\leq \bm1_n \big\}. \label{eq:feasiblepset}
		\end{align}
		Denote $\bm\gamma_{ij}$ as the element in $i^{th}$ row and $j^{th}$ column of $\bm\gamma$. We further define the payoff received by the pursuer $P_j$ when matched with an evader $E_i$  as 
		\begin{align*}
			a_{ij}(\x_{E_i},\x_{P_j})= \begin{cases} \mathcal V_{ij}(\x_{E_i},\x_{P_j}),&  B_{ij}(\x_{E_i},\x_{P_j})\geq 0,~ \alpha_{ij}\leq1  \\ -L,&  \text{otherwise}. \end{cases}
		\end{align*}
		If an evader $E_i$ is assigned to a pursuer $P_j$ at least as fast as the evader, and $(\x_{E_i},\x_{P_j})$ lies outside the evader winning region of the \texttt{1v1} RADG induced by the two players, then the pursuer receives a payoff equal to the Value of the game obtained from \eqref{eq:value_Pout} for $\alpha_{ij}<1$ and from \cite{garcia_2020} for $\alpha_{ij}=1$. In any other case, the pursuer incurs a cost equal to $L>0$. \par
		Under a feasible assignment $\bm\mu\in\Sigma$, as per the definition \eqref{eq:feasibleset}, there can only be one pursuer who is matched to an evader $E_i$. The contribution of this evader's capture to the pursuer team can be written as 
		\begin{align}
			\Psi_i((\x_{E_i},\x_P),\bm{\mu}):=\sum_{j\in N} a_{ij}(\x_{E_i},\x_{P_j})\mu_{ij}.
			\label{eq:payoffcontribution}
		\end{align}
		Now, the team payoff of the pursuers under the assignment $\bm\mu$ is given by 
		\begin{equation}
			\Psi(\x,\bm\mu):=\sum\limits_{i\in M}\Psi_i\left((\x_{E_i},\x_{P_j}),\bm\mu\right).
		\end{equation}
		Note that $a_{ij}$ and $\V_{ij}$ are used further in the text with the implicit assumption of dependence on $\x_{E_i}$ and $\x_{P_j}$. Let $\hat N_i=\{j\in N:\B_{ij}\geq0,\ \alpha_{ij}\leq0\}$, using which we define the maximum possible payoff generated by the pursuer team by
		\begin{align}
			L^\star:=\sum_{i\in M}~ \max_{j\in \hat N_i} \mathcal V_{ij}(\x_{E_i},\x_{P_j}). \label{eq:lowerboundcost}
		\end{align}
	\end{definition}
	%
	\begin{remark}
		In \eqref{eq:lowerboundcost}, $\max_{j\in \hat{N}_i} \mathcal V_{ij}(\x_{E_i},\x_{P_j})$ denotes the best payoff generated by a pursuer from among all the pursuers (faster than $E_i$) which can capture $E_i$.  Then $L^\star$ denotes the best payoff that a pursuer team could generate if every evader is assigned to a pursuer who can generate the best possible payoff. However, such an assignment need not be feasible.
	\end{remark} 
	\begin{comment}
		\begin{theorem}
			The linear programming problem defined by
			\begin{equation}
				\bm\gamma^\star:=\argmax\limits_{\bm\gamma\in\Gamma} \Psi(\x,\bm\gamma)
				\label{eq:lpsolution}
			\end{equation}
			satisfies $\bm\gamma^\star\in\Sigma$, that is $\bm\gamma^\star$ is a feasible assignment. Further, if $L>L^\star$, then the optimal assignment $\bm\gamma^\star$ ensures that the maximum number of evaders are captured.
		\end{theorem}
		\begin{proof}
			A linear program of the form \eqref{eq:lpsolution} has been shown to have integer solutions \cite{shapley_1971}, thus resulting in $\bm\gamma^\star\in\Sigma$. Assume there exists an assignment $\bm\mu\in\Sigma$ that allows for the capture of $m_\mu$ evaders. We claim that if $L>L^\star$, the optimal assignment $\bm\gamma^\star$ allows for the capture of at least $m_\mu$ evaders. Assume to the contrary that the optimal assignment results in the capture of $m_{\bm\gamma}$ evaders with $m_{\bm\gamma}<m_\mu$. Define $A_\mu:=\{ij\in M\times N:\mu_{ij}=1\}$ and $A_\gamma:=\{ij\in M\times N:\bm\gamma_{ij}^\star=1\}$. Note that, by the feasibility of the assignments $\bm\mu$ and $\bm\gamma$, we have $|A_\mu|=m=|A_\gamma|$. Further, let $A_\mu^C:=\{ij\in A_\mu:a_{ij}\geq0\}$ and $A_\gamma^C:=\{ij\in A_\gamma:a_{ij}\geq0\}$. Then, the assignment $\bm\mu$ yields a payoff $\Psi(\x,\bm\mu)=\sum\limits_{ij\in A_\mu}a_{ij}$, and the assignment $\bm\gamma^\star$ yields a payoff $\Psi(\x,\bm\gamma^\star)=\sum\limits_{ij\in A_\gamma}a_{ij}$. Now, consider 
			\begin{align*}
				&\Psi(\x,\bm\mu)-\Psi(\x,\bm\gamma^\star)=\sum\limits_{ij\in A_\mu}a_{ij}-\sum\limits_{ij\in A_\gamma}a_{ij}\\
				&=\Bigg[ \sum\limits_{ij\in A_\mu^C}a_{ij}+\sum\limits_{ij\in A_\mu\setminus A_\mu^C}a_{ij} \Bigg] -\Bigg[ \sum\limits_{ij\in A_\gamma^C}a_{ij}+\sum\limits_{ij\in A_\gamma\setminus A_\gamma^C}a_{ij} \Bigg] \\
				&=\Bigg[ \sum\limits_{ij\in A_\mu^C}a_{ij}-(m-m_\mu)L\Bigg] -\Bigg[ \sum\limits_{ij\in A_\gamma^C}a_{ij}-(m-m_{\bm\gamma})L\Bigg] \\
				&=\Bigg[ \sum\limits_{ij\in A_\mu^C}a_{ij}-\sum\limits_{ij\in A_\gamma^C}a_{ij} \Bigg]+(m_\mu-m_{\bm\gamma})L
			\end{align*}
			By the definition of $L^\star$, we have that $\sum\limits_{ij\in A_\mu^C}a_{ij}\leq L^\star\ \forall\bm\mu\in\Sigma$, and since $\bm\gamma^\star\in\Sigma$, we have $\sum\limits_{ij\in A_\mu^C}a_{ij}-\sum\limits_{ij\in A_\gamma^C}a_{ij}\geq -L^\star$. And, by assumption $m_{\bm\gamma}<m_\mu$ and both are integers, thus $m_\mu-m_{\bm\gamma}\geq1$. Hence, we have that $\Psi(\x,\bm\mu)-\Psi(\x,\bm\gamma^\star)>0$ which is a contradiction to the fact that $\bm\gamma^\star$ optimizes the LP given by \eqref{eq:lpsolution}. Further, since the chosen $\bm\mu\in\Sigma$ is arbitrary, $m_{\bm\gamma}\geq m_\mu\ \forall\bm\mu\in\Sigma$.
		\end{proof}
	\end{comment}
	\begin{theorem}
		The linear programming problem defined by
		\begin{equation}
			\bm\gamma^\star:=\argmax\limits_{\bm\gamma\in\Gamma} \Psi(\x,\bm\gamma)
			\label{eq:lpsolution}
		\end{equation}
		satisfies $\bm\gamma^\star\in\Sigma$, that is $\bm\gamma^\star$ is a feasible assignment. Further, if $L>L^\star$, then the optimal assignment $\bm\gamma^\star$ ensures that the least number of evaders reach the target without being intercepted.
		\label{thm:lp}
	\end{theorem}
	\begin{proof}
		A linear program of the form \eqref{eq:lpsolution} has been shown to have integer solutions \cite{shapley_1971,dantzig_1963}, thus resulting in $\bm\gamma^\star\in\Sigma$. Assume there exists an assignment $\bm\mu\in\Sigma$ that allows $m_\mu$ evaders to reach the target without interception. We claim that if $L>L^\star$, the optimal assignment $\bm\gamma^\star$ allows at most $m_\mu$ evaders to reach the target. Assume to the contrary that the optimal assignment allows $m_{\bm\gamma}$ evaders to reach the target with $m_{\bm\gamma}>m_\mu$. 
		%Define $A_\mu:=\{ij\in M\times N:\mu_{ij}=1\}$ and $A_{\gamma^\star}:=\{ij\in M\times N:\bm\gamma_{ij}^\star=1\}$. Further, let
		Define $A_\mu^C:=\{ij\in A_\mu:a_{ij}\geq0\}$ and $A_{\gamma^\star}^C:=\{ij\in A_{\gamma^\star}:a_{ij}\geq0\}$. Then, the assignments $\bm\mu$ and $\bm\gamma^\star$ yield payoffs $\Psi(\x,\bm\mu)=\sum\limits_{ij\in A_\mu}a_{ij}$ and $\Psi(\x,\bm\gamma^\star)=\sum\limits_{ij\in A_{\gamma^\star}}a_{ij}$ for the pursuer team respectively. Now, consider 
		\begin{align*}
			&\Psi(\x,\bm\mu)-\Psi(\x,\bm\gamma^\star)=\sum\limits_{ij\in A_\mu}a_{ij}-\sum\limits_{ij\in A_{\gamma^\star}}a_{ij}\\
			&=\Big( \sum\limits_{ij\in A_\mu^C}a_{ij}+\sum\limits_{ij\in A_\mu\setminus A_\mu^C}a_{ij} \Big) -\Big( \sum\limits_{ij\in A_{\gamma^\star}^C}a_{ij}+\sum\limits_{ij\in A_{\gamma^\star}\setminus A_{\gamma^\star}^C}a_{ij} \Big) \\
			%&=\Bigg[ \sum\limits_{ij\in A_\mu^C}a_{ij}-m_\mu L\Bigg] -\Bigg[ \sum\limits_{ij\in A_{\gamma^\star}^C}a_{ij}-m_{\bm\gamma} L\Bigg] \\
			&=\Big( \sum\limits_{ij\in A_\mu^C}a_{ij}-\sum\limits_{ij\in A_{\gamma^\star}^C}a_{ij} \Big)+(m_{\bm\gamma}-m_\mu)L
		\end{align*}
		By the definition of $L^\star$, we have that $0\leq\sum\limits_{ij\in A_\mu^C}a_{ij}\leq L^\star\ \forall\bm\mu\in\Sigma$, and since $\bm\gamma^\star\in\Sigma$, we have $\sum\limits_{ij\in A_\mu^C}a_{ij}-\sum\limits_{ij\in A_{\gamma^\star}^C}a_{ij}\geq -L^\star$. By assumption $m_{\bm\gamma}>m_\mu$, and thus $m_{\bm\gamma}-m_\mu\geq1$ as $m_{\bm\gamma}$ and $m_\mu$ are integers. Hence, we have that $\Psi(\x,\bm\mu)-\Psi(\x,\bm\gamma^\star)>0$ which is a contradiction to the fact that $\bm\gamma^\star$ optimizes the LP given by \eqref{eq:lpsolution}. Further, since the chosen $\bm\mu\in\Sigma$ is arbitrary, $m_{\bm\gamma}\geq m_\mu\ \forall\bm\mu\in\Sigma$.
	\end{proof}
	\begin{remark}
		By the nature of the formulation of the linear program in \eqref{eq:lpsolution}, the optimal assignment is agnostic between the result of the game. Furthermore, if the cost incurred by a pursuer on failing to capture its evader, $L$, is small enough, it has been shown that the optimal assignment might allow more evaders to reach the target even if there exists an assignment allowing for less evaders to win. And, $L^\star$ proves to be a conservative lower bound for $L$.
		% Finally, it is evident that using a linear programming based method allows for polynomial time algorithms and thus makes the role assignment process computationally quite efficient. 	
		%The linear program \eqref{eq:lpsolution} that forms the basis for the assignment in the MRADG is indifferent of whether the pursuers or the evaders win. In both the cases the optimal solution is such that the resulting assignment tries to minimize the number of evaders that can reach the target without being captured. And, to ensure this, it is imperative that the cost on each pursuer failing to capture its assigned evader has to be large enough. And $L^\star$ proves to be a conservative lower bound for this cost. It is evident in the proof that if $L$ is small enough then the optimal assignment might end up with assigning a pursuer to an evader it cannot capture despite there existing assignments where all evaders can be captured. Further, this assignment scheme does not restrict all pursuers to be strictly superior to all evaders as has been usually done in the literature. And, finally this formulation provides a computationally efficient way to assign players in real-time.
	\end{remark}
	Next, using the optimal assignment \eqref{eq:lpsolution} we provide the Barrier function for the MRADG in the following theorem.
	\begin{theorem} Let Assumption \ref{assum:interactions} hold and $L>L^\star$.
		The function $\B(\x):\R^{3(m+n)}\shortrightarrow\R$ defined by 
		\begin{equation}
			B(\x)=\min\limits_{i\in M}\Psi_i((\x_{E_i},\x_P),\bm{\gamma}^\star), \label{eq:nm_barrier}
		\end{equation}
		is a Barrier function for the MRADG, where $\gamma^\star\in \Sigma$ satisfies \eqref{eq:lpsolution}. Further, the Barrier function partitions the global state space into pursuer and evader winning regions as in \eqref{eq:partition_sets}. 
		\label{thm:multibarrier}
	\end{theorem}
	\begin{proof}
		Consider $\x\in\R^{3(m+n)}$ and $\B(\x)>0$. We claim that $a_{ij}>0,\ \forall\ ij\in A_{\bm\gamma^\star}$. Assume to the contrary that there exists $\bar{i}\bar{j}\in A_{\bm\gamma^\star}$ such that $a_{\bar i\bar j}\leq0$. By Definition \ref{def:aval}, it is possible either if $a_{\bar i\bar j}=-L$ or $a_{\bar i\bar j}=0$ resulting in $\Psi_i((\x_{E_i},\x_P),\bm\gamma^\star)=\sum_{j\in N}a_{\bar i j}\bm\gamma_{\bar i j}^\star\leq0$. Thus, $\B(\x)\leq0$, resulting in a contradiction. Hence, every pursuer assigned to an evader can intercept it before the target is reached. Further, the constraint $\sum_{j\in N}\bm\gamma^\star.\bm 1_n=1_m$ ensures that every evader has been assigned to some pursuer. By the previous two arguments, it is evident that the pursuing team wins, and $\B(\x)>0$ characterizes the winning region for the pursuers $\mR_P$.\par 
		Now, consider the case when $\B(\x)<0$. There exists $\bar i\in M$ such that $\Psi_i((\x_{E_i},\x_P),\bm\gamma^\star)=\sum_{j\in N}a_{\bar i j}\bm\gamma^\star_{\bar ij}<0$. This is again possible only if $a_{\bar i\bar j}<0$ for some $\bar j\in N$ with $\bar i\bar j\in A_{\bm\gamma^\star}$. But, this implies that at least one evader reaches the target before interception with the assignment $\bm\gamma^\star$. Therefore, using Theorem \ref{thm:lp}, every $\bm\mu\in\Sigma$ allows at least one evader to reach the target. Thus, $\B(\x)<0$ characterizes the winning region of the evaders. \par
		Finally, consider the case when $\B(\x)=0$. This implies that $\forall ij\in A_{\gamma^\star}$, we have $a_{ij}\geq0$ and there is at least one $\bar i\bar j\in A_{\bm\gamma^\star}$ such that $a_{\bar i\bar j}=0$. A particular $a_{\bar i\bar j}$ can be zero only if the Value of the associated \texttt{1v1} game is zero, which is only possible when both $P_{\bar j}$ intercepts $E_{\bar i}$ precisely at the target. This corresponds to a tie situation, and hence characterizes the barrier surface. 
	\end{proof}
	Having solved the GoK for MRADG, starting from any point $\x\in \R^{3(n+m)}$ in the global state space it is possible to determine which team wins the game. Next, in the pursuer team's winning region, the optimal strategies of the pursuers  are provided by the solution of a Game of Degree as formulated in the following theorem.
	\begin{theorem} Let Assumption \ref{assum:interactions} hold and $L>L^\star$.
		Consider the  MRADG   for $\x\in\mR_P$. The Value function is $C^1$ (except at the dispersal surfaces i.e., there are multiple optima for the LP in \eqref{eq:lpsolution}) and it is the solution of the Hamilton-Jacobi-Isaacs(HJI) partial differential equation. The Value function is given by 
		\begin{equation}
			\V(\x)= \Psi(\x,\gamma^\star), \label{eq:multi_value}
		\end{equation}
		where $\gamma^\star\in \Sigma$ satisfies \eqref{eq:lpsolution}. The saddle-point strategies of all the players are obtained from \eqref{eq:opt_con}. \label{theorem:multivalue}
	\end{theorem}
	\begin{proof}
		Since $\x\in\mR_P$, we have $\B(\x)>0$. This results in $a_{ij}>0\ \forall ij\in A_{\gamma^\star}$ from Theorem \ref{thm:multibarrier}. But, by Definition \ref{def:aval}, $a_{ij}>0$ only when $a_{ij}=\V_{ij}$. Thus, the proposed Value function \eqref{eq:multi_value} can be written as 
		\begin{equation}
			\begin{aligned}
				\V(\x)=\Psi(\x,\gamma^\star)=\sum\limits_{ij\in A_{\gamma^\star}}a_{ij}=\sum\limits_{ij\in A_{\gamma^\star}}\V_{ij}. \label{eq:multi_value_simplify}
			\end{aligned}
		\end{equation}
		The proposed Value of the multiplayer game is hence posed as a sum of the individual pairwise games dictated by the optimal assignment. We must now verify that the proposed Value function \eqref{eq:multi_value} satisfies the HJI equation:
		\begin{equation*}
			\begin{aligned}
				&\min\limits_{\u}\max\limits_{\v}\langle \nabla \V(\x),f(\x,\u,\v)\rangle\\
				%			&=\min\limits_{\u(.)}\max\limits_{\v(.)}\langle \nabla V(\x),[\u\ \v]^T\rangle\\
				&=\min\limits_{\u}\max\limits_{\v}\sum\limits_{ij\in A_{\gamma^\star}}\Big\langle\Big[\tfrac{\partial \V_{ij}}{\partial\x_{E_i}}\ \tfrac{\partial\V_{ij}}{\partial\x_{P_j}}\Big],[\u_i\ \v_j] \Big\rangle \\
				&=\sum\limits_{ij\in A_{\gamma^\star}} \min\limits_{\u_i}\max\limits_{\v_j}\left[\cufrac{\V_{ij}}{x_{E_i}} u_{x_i}+\cufrac{\V_{ij}}{y_{E_i}}u_{y_i}\right. \\
				&\left.+\cufrac{\V_{ij}}{z_{E_i}}u_{z_i}+\cufrac{\V_{ij}}{x_{P_j}}v_{x_j}+\cufrac{\V_{ij}}{y_{P_j}}v_{y_j}+\cufrac{\V_{ij}}{z_{P_j}}v_{z_j}\right]=0.\\
			\end{aligned}
		\end{equation*}
		The separability of the expression in terms of the individual controls of all the players allows the interchange of the summation with the minmax operator. Hence, the Value function proposed in \eqref{eq:multi_value} satisfies the HJI equation and provides the solution to the MRADG. The closed loop optimal controls can also be obtained from the Value function as given in \eqref{eq:opt_con}.\par 
		Finally, the singular surface defined by $\Psi(\x,\bm\gamma_1^\star)=...=\Psi(\x,\bm\gamma_k^\star)$ forms the dispersal surface, where $\bm\gamma_1^\star,...,\bm\gamma_k^\star$solve the LP in \eqref{eq:lpsolution}. In such a scenario, each team has multiple equally optimal strategies. To move away from the dispersal surface, each team can choose an assignment scheme randomly from the available optimal options $\{\bm\gamma_1^\star,...,\bm\gamma_k^\star\}$ in the instant the game begins. After an infinitesimal amount of time, the state moves out of the dispersal surface and the optimal strategies become fixed. However, this initial choice may favor one team and disadvantage their opponents.
	\end{proof}
	\begin{remark}
	In the existing literature for MRADG, either computationally efficient solutions have been proposed which were numerical or approximate, or optimal solutions have been determined analytically at the cost of a factorial time complexity. However, by formulating the assignment game as a linear program which has well known polynomial time algorithms, this computational bottleneck is eliminated. Furthermore, this assigment scheme is used to characterize both the barrier surface and the Value function. This results in a state-feedback saddle point strategy for every player which can be implemented in any real-time system without imposing a computational burder. 
	%which is optimal and further demonstrate the computational efficiency of the algorithm over a standard brute force method in Example \ref{ex:1} Section \ref{sec:num}, thus not having to choose between efficiency and optimality.  
	\end{remark}
	%In a differential game, a dispersal surface is a locus of points in the game space allowing multiple equally optimal paths to the termination set. In the particular MRADG, a dispersal surface exists if there are multiple assignments attaining the maximum of the optimization problem \eqref{eq:lpsolution}. When such a scenario arises, the pursuing team has multiple equally optimal ways to assign evaders amongst themselves. To move away from the dispersal surface, each player can play an instantaneous mixed strategy as the game begins by choosing from one of the available optimal options with an equal probability. After making this move for an infinitesimal amount of time, as the state is no more on the dispersal surface, each player has a determined strategy. However, the inital choice can incur an advantage for one team and a disadvantage for its adversary. A detailed example has been discussed in Section \ref{sec:num}.
	%%%%% 
	\section{Numerical Illustrations}
	\label{sec:num}
	We consider a few representative MRADG examples to illustrate our solution process. The computations were done in MATLAB 2022b on a workstation PC with a Core i9-13900K processor with a memory of 128GB.
	\begin{example}
		\label{ex:1}
		First, we illustrate the efficiency of the linear program-based assignment algorithm by using the built-in \texttt{linprog} function using the dual-simplex algorithm. We compare our results with a brute-force algorithm that goes over all $max(m,n)!$ possible assignments; see Table \ref{tab:Table1}. Note that, although the simplex algorithm has an exponential time complexity, it can significantly outperform a brute-force method in most real-world cases, as can be seen in Table \ref{tab:Table1}. This table shows the average time required to compute assignments via brute-force and linear programming (in seconds) for different values of $n$ and $m$. The last column shows the required computational time for coordinate and distance calculations (in milliseconds).
		\par
		%	First, we illustrate the efficiency of the linear program based assignment algorithm by using the inbuilt \texttt{linprog} function with the dual-simplex algorithm. These results are compared with a brute force algorithm going over all the $max(m,n)!$ possible assignments. Note that even though the simplex algorithm has an exponential time complexity, in most real-world cases it performs significantly better than a brute force method which can be seen in the table \ref{tab:Table1}. This table shows the average time required to compute assignments via brute-force and linear programming (in seconds) for different values of $n$ and $m$. The last column shows the required computational time for coordinates and distance calculations (in miliseconds). Evidently, an LP based assignment is far superior to the brute force method for optimal assignment which forms the bottleneck of the whole computational load 
		\begin{table}[h]
			\caption{Average computation time for various multiplayer cases}
			\label{tab:Table1}
			\centering
			{\setlength{\tabcolsep}{5pt} 
				\renewcommand{\arraystretch}{1} { 
					\begin{tabular}{| m{4em} | m{5em} | m{5em} | m{5em} |}
						\hline 
						Player number ($n,m$) & Brute Force Assignment(s) & LP-Based Assignment(s) &Coordinates/ Distance(ms) \\ 
						\toprule
						3,3 & 0.0105 & 0.0018 & 0.0074\\
						\hline
						%5,4 & 0.0529 & 0.0021 & 0.0087\\
						%\hline 
						7,5 & 0.0761 & 0.0023 & 0.0106 \\
						\hline
						%8,8 & 0.0946 & 0.0033 & 0.0161 \\
						%\hline 
						%9,6 & 0.1478 & 0.0023 & 0.0148\\
						%\hline
						10,8 & 0.9792 & 0.0022 & 0.0172\\
						\hline
						11,7 & 11.0177 & 0.0019 & 0.0176\\
						\hline 
						12,10 & 287.7911 & 0.0020 & 0.0220\\
						\hline 
						20,15 & -- & 0.0020 & 0.0328 \\
						\hline 
						50,40 & -- & 0.0043 & 0.0886\\
						\hline 
						100,100 & -- & 0.0223 & 0.1964\\
						\bottomrule
					\end{tabular}
			} }
		\end{table} 
	\end{example}
	\begin{example}
		Consider a MRADG with the origin as the target with 3 pursuers and 3 evaders. The pursuers are situated at  $\x_{P_1}=[-6.77,-2.95,0.01],\ \x_{P_2}=[-3.34,-3.96,-3.33]$ and $\x_{P_3}=[4.76,-13.35,-0.61]$, while the evaders are situated at $\x_{E_1}=[4.92,-7.91,4.43],\ \x_{E_2}=[-8.07,2.73,-5.91]$ and $\x_{E_3}=[-6.73,-10.65,-12.49]$. The speeds of the pursuers are $V_{P_1}=1.71,\ V_{P_2}=2.23$ and $V_{P_3}=2.28$, while those of the evaders are $U_{E_1}=1.69,\ U_{E_2}=1.01$ and $U_{E_3}=1.84$. The pursuing team wins in this particular case and the optimal assignment is given by $E_1-P_2,\ E_2-P_1,\ E_3-P_3$. The resulting Value of the game is given by $18.63$. The optimal trajectories for all the agents can be seen in figure \ref{fig:optimal}. Note that in this setting there is a subset of pursuers who are not superior to all the evaders, namely $P_1$ is slower than $E_3$. Furthermore, $E_1$ cannot be captured by $P_3$  and $E_3$ cannot be captured by $P_1$. 
		This particular example also serves as an illustration for the sensitivity of the assignment to the value of $L$. If $L>L^\star=23.84$, then the optimal assignment is as given above. However, for values of $L\leq L^\star$ there is no guarantee on the nature of the optimal assignment. If for example, $L=1$, then the optimal assignment provided by the LP in \eqref{eq:lpsolution} allots $E_1-P_3,\ E_2-P_1,\ E_3-P_2$. Despite obtaining a payoff larger than the Value of the game, the assignment fails as $E_1$ and $E_2$ to pursuers who cannot capture them. Thus, $L$ must be large enough to ensure that the pursuer team goal is kept as a priority. Here, we also illustrate the robustness of the optimal state feedback strategies. Supposing that the evaders decided not to follow the obtained optimal strategies and instead decided to head straight to the origin with the pursuers sticking to their optimal strategies, then the ensuing nonoptimal play results in a higher Value of $20.26$ for the game favoring the pursuers. These trajectories are shown in Figure \ref{fig:nonoptimal}.
	\end{example}
	\begin{figure}
		\centering
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[scale=.5]{FigureOPT_new.pdf}		
			\caption{Optimal Play}
			\label{fig:optimal}
		\end{subfigure}  
		\begin{subfigure}[h]{0.24\textwidth}
			\centering 
			\includegraphics[scale=.5]{FigureNONOPT_new.pdf}
			\caption{Nonoptimal Play}
			\label{fig:nonoptimal}
		\end{subfigure}
		\caption{Trajectories of a 3v3 Reach-Avoid Game under nonoptimal play by Evaders}
		\label{fig:num_ill}
	\end{figure}
		\begin{example}
		Now, consider a reach-avoid game with $3$ pursuers and $2$ evaders with $\x_{P_1}=[1,0,0],\ \x_{P_2}=[1,0,0.5],\ \x_{P_3}=[1,0,-0.5],\ \x_{E_1}=[0.75,1,0]$ and $\x_{E_2}=[0.75,-1,0]$. All the evaders are assumed to have a speed of 0.5 units while the pursuers have unit speed. Amongst the $6$ feasible assignments, $4$ achieve the optimal Value and hence the state belongs to a dispersal surface. Thus at the beginning of the game, there are $4$ equally optimal solutions for the players given by the assignments $[E_1-P_3,\ E_2-P_1]$, $[E_1-P_2,\ E_2-P_1]$, $[E_1-P_1,\ E_2-P_3]$ and $[E_1-P_1,\ E_2-P_2]$ resulting in a Value of $1.5398$. Hence, this state lies on the dispersal surface discussed in the proof of Theorem \ref{theorem:multivalue}. The optimal trajectories in each of these cases are shown in Figure \ref{fig:dispersal}.
	\end{example} 
	\begin{figure}[h]  
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_1.pdf}
			\label{fig:dispersal1}
		\end{subfigure} \hspace{20pt}
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_2.pdf}
			\label{fig:dispersal2}
		\end{subfigure} \\
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_3.pdf}
			\label{fig:dispersal3}
		\end{subfigure} \hspace{20pt}
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_4.pdf}
			\label{fig:dispersal4}
		\end{subfigure} 
		\caption{Optimal assignments for dispersal surface}
		\label{fig:dispersal} 
	\end{figure}
	%%%%%
	\section{Conclusion and Future Work}
	\label{sec:conc}
	The paper discusses a reach-avoid differential game in 3D space with $n$ pursuers and $m$ evaders. An LP-based optimal task assignment algorithm is used to provide saddle point equilibrium trajectories for all players satisfying the HJI partial differential equation. Currently, the method assigns a maximum of one pursuer to one evader, but assigning multiple pursuers to one evader can improve the Value of the game for the pursuing team by restricting the evader's reachable space. Another key assumption in this work enforces strict commitment to the assignment strategy across time. Relaxing this assumption allows switching assignments which may lead to a better Value for the pursuing team. Future work can thus relax the assumptions of the current work to analyze more realistic scenarios. 
	%`The general case of $N$ pursuers $M$ evaders differential game is addressed for a reach-avoid differential game in 3D space. A linear programming based optimal assignment algorithm is used to provide an optimal task assignment for the pursuing team and the resulting solution satisfies the HJI partial differential equation and hence provides the saddle point equilibrium trajectories for all the players. The current method that is used allots a maximum of one pursuer to one evader. However, it has been seen in literature that the addition of more pursuers in pursuit of a single evader restricts the allowable space for the evader and hence improves the Value of the game for the pursuing team. Thus, one particular direction in which the current work can be extended is by considering the assignment of multiple pursuers to one evader. Another possible extension is to use a receeding horizon approach to compute the assignments. This would allow for a dynamic computation of the optimal assignment in intervals to allow for switching roles in the midst of the game. 
	
	\bibliographystyle{IEEEtran}
	\bibliography{CDC2023}
	
\end{document}
