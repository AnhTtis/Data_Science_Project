\section{State Evolution: Proof of Proposition \ref{prop:state_evolution}}
\label{sec:ProofSE}


In this section we prove Proposition~\ref{prop:state_evolution}, following the appendix of \cite{ams20}. Throughout, we denote by $\bG^{(k)}\in (\bbR^N)^{\otimes k}$, $k\ge 2$ a sequence of standard Gaussian tensors. For $S_k$ the symmetric group on $k$ elements we also write 
\begin{equation}
\label{eq:Ak-def}
    \bA^{(k)}=\frac{1}{N^{(k-1)/2} }\Gamma^{(k)}
    \diamond 
    \sum_{\pi\in S^k}(\bG^{(k)})^{\pi}
\end{equation}
for the rescaled tensors with entries
\begin{equation}
\label{eq:A-defn}
     \bA^{(k)}_{i_1,\dots,i_k}
     =
     \frac{1}{N^{(k-1)/2} }
     \gamma_{s(i_1),\dots,s(i_k)} 
     \sum_{\pi\in S_k}\bG^{(k)}_{i_{\pi(1)},\dots,i_{\pi(k)}}
     \,.
\end{equation}
For a symmetric tensor $\bA^{(k)}\in(\bbR^N)^{\otimes k}$ and $\bT\in(\bbR^N)^{\otimes (k-1)}$, we denote by  $\bA^{(k)}\{\bT\}\in\bbR^N$ the vector with components
\begin{equation}
\label{eq:TensorTensor}
    \bA^{(k)}\{\bT\}_i = \frac{1}{(k-1)!}\sum_{1\le i_1,\cdots,i_{k-1}\le N}A^{(k)}_{i,i_1,\cdots,i_{k-1}}T_{i_1\dots i_{k-1}}.
\end{equation}
For $\bu\in\bbR^N$ we denote by $\bA^{(k)}\{\bu\}=\bA^{(k)}\{\bu^{\otimes (k-1)}\}$ the vector with entries
%
\begin{equation}
\label{eq:vector}
\bA^{(k)}\{\bu\}_i = \frac{1}{(k-1)!}\sum_{1\le i_1,\cdots,i_{k-1}\le N}A^{(k)}_{i,i_1,\cdots,i_{k-1}}u_{i_1}\cdots u_{i_{k-1}}.
\end{equation}
Note that for $\bA^{(k)}$ as in \eqref{eq:Ak-def}, one has 
\[
\bA^{(k)}\{\bu\}=\nabla H_{N,k}(\bu)
\]
where $H_{N,k}$ denotes the part of $H_N$ of total degree $k$.


For $\bu,\bv\in\bbR^N$ we recall from Subsection~\ref{subsec:notation} the notations 
\begin{align*}
    \< \bv\>_N&=N^{-1}\sum_{i\le N} v_i,
    \\
    \< \bu,\bv\>_N
    &= 
    N^{-1}\sum_{i\le N}u_iv_i
    =
    \langle\vec\lambda, \vR(\bu,\bv)\rangle,
    \\
    \|\bu\|_{N}&= \<\bu,\bu\>_N^{1/2}=\sqrt{\sum_s \lambda_s R_s(\bu,\bu)}.
\end{align*}


Given functions $f_{t,s}:\bbR^{t+1}\to\bbR$ of $t+1$
variables for each $s\in\sS$, and $\bv^0,\bv^1,\dots,\bv^t\in\bbR^{N}$, we define $f_t(\bv^0,\bv^1,\dots,\bv^t)\in\mathbb R^N$ component-wise via
%
\begin{align}
%
    f_t(\bv^0,\bv^1,\dots,\bv^t)_i=f_{t,s(i)}(v^0_i,\dots,v^t_i).
%
\end{align}
%
Finally, for a sequence of vectors $\bw^0,\bw^1,\dots$, we write $\bw^{\le t} = (\bw^0,\bw^1,\dots,\bw^t)$.


To deduce the state evolution result for mixed tensors, we analyze a slightly more general iteration where each homogenous
$k$-tensor is tracked separately, while restricting ourselves to the case where the mixture $\xi$ has finitely many components: $\gamma_{s_1,\dots,s_k} = 0$ for all $(s_1,\dots,s_k)\in \sS^k$ for all $k \ge D +1$ for some fixed $D \ge 2$. We then proceed by an approximation argument to extend the convergence to the general case $D = \infty$.

We begin by introducing the Gaussian process that captures the asymptotic behavior of AMP. 
Define $\xi^k$ to be the degree $k$ part of $\xi$, and 
\[
    \xi^{k,s}=\frac{1}{\lambda_s}\partial_{x_s}\xi^k(x_1,\dots,x_r)
\]
the degree $k-1$ part of $\xi^s$. 

An AMP iteration is specified by Lipschitz functions $f_{t,s}:\bbR^{2(t+1)}\to \bbR$ for each $(t,s)\in\naturals\times \sS$.\footnote{The unusual factor $2$ in the exponent comes from the external randomness vectors $\be^1,\dots,\be^t$.}  
For each iteration $t$, the state of the algorithm is given by vectors
$\bw^t\in \bbR^N$, and $\bz^{k,t}\in \bbR^N$, with $k\in\{2,\dots,D\}$. 
Moreover for each $t$, there is also an external randomness vector $\be^t\in\bbR^N$ with independent coordinates $e^t_i\sim \mu_{t,s(i)}$ from deterministic probability distributions $\big(\mu_{t,s}\big)_{t\geq 0,s\in \sS}$ with finite $L^2$ norm.
We now start to define the AMP iteration steps (the definition finishes at \eqref{eq:recursive-covariance}). A single step is given by
%
\begin{align}
\label{eq:AMP-def1}
    \AMP_t
    \lt(\bw^0,\dots,\bw^t
    ;
    \be^0,\dots,\be^t\rt
    )_{k}
    &\equiv
    \bA^{(k)}
    \{
    \f_t
    \}
    -
    \sum_{t'\leq t}
    d_{t,t',k}
    \diamond 
    \f_{t'-1}
    \, ,
    \\
\label{eq:bff}
    \f_t &= f_t(\bw^0,\dots,\bw^t;\be^0,\dots,\be^t),
    \\
\label{eq:AMP-def2}
    d_{t,t',k,s} 
    &\equiv 
    \sum_{s'\in\sS}
    \partial_{x_{s'}}\xi^{k,s}
    \lt(\lt( 
    \E\lt[ 
    F_{t,s} F_{t'-1,s}
    \rt]
    \rt)_{s\in\sS}
    \rt) 
    \times
    \E\lt[ 
    \frac{\partial_{W^{t'}_{s'}} F_{t,s'}}{\partial W^{t'}_{s'}}
    \rt]
    \, 
    ,
    \\
\label{eq:F-def}
    F_{t,s'}
    &\equiv
    f_{t,s'}\big(W^0_{s'},W^1_{s'},\dots, W^t_{s'};E^0_{s'},\dots,E^t_{s'}\big)
    .
%
\end{align}
A general multi-species tensor AMP algorithm then takes the form:
\begin{align}
\label{eq:TensorAMP}
    \bw^t=\sum_{2\leq k\leq D} \bz^{k,t}\, ,\quad \quad
    \bz^{k,t+1}
    =
    \AMP_t
    \lt(\bw^0,\dots,\bw^t;\be^0,\dots,\be^t\rt)_k\, .
\end{align}


For the right-hand side of \eqref{eq:F-def} to make sense, we must define for each $t\geq 0$ and $s\in \sS$ a distribution over sequences
$(W^0_s,\dots,W^t_s;E^0_s,\dots,E^t_s)$. The latter variables $E^{t'}_s\sim \mu_{t',s}$ are simply taken independent of each other and all other variables. The construction of the $W$ variables is recursive across $t$ as follows. 
For each $2\leq k\leq D$ and $s\in\sS$, we let $U^{k,0}_s\sim \nu_{k,s}$
and construct a centered Gaussian process
\[
    (U^{k,1}_s,U^{k,1}_s,\dots,U^{k,t}_s)
\]
which is independent of $U^{k,0}_s$. The variables $U^{k,t}_s$ and $U^{k',t'}_{s'}$ are independent unless $(k,s)=(k',s')$. It remains to specify the covariance of $(U^{k,t}_s)_{1\leq t\le T}$ which is given recursively by:
\begin{equation}
\label{eq:recursive-covariance}
\begin{aligned}
    \E\big[U^{k,t+1}_s U^{k,t'+1}_{s}\big]
    & = 
    \xi^{k,s}(\Sigma^{k,t,t'})\,;
    \\
    \Sigma^{k,t,t'}_{s}
    &=
    \bbE\lt[
    f_{t,s}(W^0_s,\dots,W^t_s;E^0_s,\dots,E^t_s)
    f_{t',s}(W^0_s,\dots,W^{t'}_s;E^0_s,\dots,E^{t'}_s)
    \rt],\quad \forall s\in \sS
    \\
    W^t_s & \equiv  \sum_{2\leq k\leq D} U^{k,t}_s\, 
    .
\end{aligned}
\end{equation}





%
The main result, an extension of Proposition \ref{prop:state_evolution}, follows. Below we use $\bbW_2$ to denote the Wasserstein-$2$ distance between probability measures on Euclidean space in any dimension. We say a function $\psi:\bbR^d\to\bbR$ is \textbf{pseudo-Lipschitz} if 
\[
    |\psi(\bw)-\psi(\by)|
    \leq 
    C(1+\|\bw\|+\|\by\|)
    \cdot 
    \|\bw-\by\|
    ,\quad\forall \bw,\by\in\bbR^d.
\]

\begin{theorem}[State Evolution for AMP]
\label{thm:mixedAMP}
Let $\{\bG^{(k)}\}_{k\ge 2}$ be independent standard Gaussian tensors with $\bG^{(k)}\in(\bbR^N)^{\otimes k}$, and define $\bA^{(k)}$ as in \eqref{eq:A-defn}. Fix a sequence of Lipschitz functions $f_{t,s}:\bbR^{k+1}\to\bbR$.
 Let $\bz^{2,0},\cdots \bz^{D,0}\in\bbR^N$ be deterministic vectors and $\bw^0 =\sum_{2\leq k\leq D} \bz^{k,0}$.
Assume that for each $s\in\sS$, the empirical distribution of the vectors 
\[
    (z_i^{2,0},\cdots z_i^{D,0}),\quad i\in \cI_s
\]
converges in $\bbW_2(\bbR^{D-1})$ distance to the law of the vector $(U^{k,0}_s)_{2\le k\le D}$. 

Let $\bw^{t}, \bz^{k,t}$, $t\ge 1$ be given by the \emph{tensor AMP} iteration. Then, for all $s\in\sS$ and $T\ge 1$ and for any pseudo-Lipschitz functions $\psi:\bbR^{D \times T}\to \bbR$ and $\wt\psi:\bbR^T\to\bbR$, we have
%
\begin{align}
\label{eq:z-U-convergence}
    \plim_{N\to\infty}\frac{1}{N_s}\sum_{i\in\cI_s}\psi \Big((z_i^{k,t})_{ k\le D,t\le T}\Big) 
    &= \E\lt[\psi\big((U^{k,t}_s)_{2\leq k\leq D,t\leq T}\big)\rt]\, 
    ;
    \\
\label{eq:w-W-convergence}
    \plim_{N\to\infty}\frac{1}{N_s}\sum_{i\in\cI_s}\wt\psi \Big((w_i^{t})_{t\le T}\Big) 
    &= \E\lt[\wt\psi\big((W^{t}_s)_{t\leq T}\big)\rt]\,
    .
%
\end{align}
\end{theorem}
%

Note that \eqref{eq:w-W-convergence} (which concerns the actual AMP iterates $\bw^t$) is a special case of \eqref{eq:z-U-convergence} (which is more convenient to prove). Indeed one can take $\psi((z^{k,t})_{k\le D,t\le T}=\wt\psi\lt(\sum_{k\le D}z^{k,t}\rt)_{t\le T}$. 
In the special case that $c_k =0$ for all $k\ge D+1$, Proposition \ref{prop:state_evolution} follows immediately from Theorem~\ref{thm:mixedAMP} by baking the contribution of $\bh$ explicitly into $f_t$ (since we require $k\geq 2$ above). Proposition \ref{prop:state_evolution} for non-polynomial $\xi$ follows by a standard approximation argument outlined at the end of Subsection~\ref{subsec:further-def-app}.
For the remainder of this Appendix we thus focus on establishing \eqref{eq:z-U-convergence}.

\subsection{Further Definitions}
\label{subsec:further-def-app}

We define the notations
%
\begin{align*}
%
    \bW_t
    &=
    \big[
    \,\bw^0~|~\bw^1~|\;\cdots\;|~\bw^t
    \big]\, ,
    \\
    \bE_t
    &=
    \big[
    \,\be^0~|~\be^1~|\;\cdots\;|~\be^t
    \big]\, ,
    \\
    \bZ_{k,t}
    &=
    \big[
    \,\bz^{k,0}~|~\bz^{k,1}~|\,\cdots\;|~\bz^{k,t}\,
    \big].
\end{align*}
%
Given a $N\times (t+1)$ matrix such as $\bW_t$, and a tensor $\bA^{(k)}\in(\bbR^{N})^{\otimes k}$, we write 
$\bA^{(k)}\{\bW_t\}$ for the $N\times (t+1)$ matrix with columns $\bA^{(k)}\{\bw^0\}$, \dots, $\bA^{(k)}\{\bw^t\}$:
%
\begin{align*}
%
\bA^{(k)}\{\bW_t\}&=\Big[\bA^{(k)}\{\bw^0\}\Big|\bA^{(k)}\{\bw^1\}\Big|\;\cdots\;\Big|\bA^{(k)}\{\bw^t\}\Big]\, .
%
\end{align*}
%
We will write $\f_t=f_t(\W_t,\bE_t)=f_t(\bw^0,\dots,\bw^t,\be^0,\dots,\be^t)$ and also set 
%
\begin{align}
\label{eq:Ydef1}
    \by_{k,t+1}(\ZZ_{k,t})
    &=
    \A_k\lt\{\f_t\rt\} 
    = 
    \bz^{k,t+1}
    +
    \sum_{t_1\leq t} 
    d_{t,t_1,k}
    \diamond 
    \f_{t_1-1}\, ,
    \\
\label{eq:Ydef2}
    \bY_{k,t} 
    &= 
    [\by_{k,1}|\;\cdots\;|\by_{k,t}]
    \, ,
    \\
\nonumber
    \by_{t}(\ZZ_{k,t})&=\sum_{2\leq k\leq D}  \by_{k,t}(\ZZ_{k,t})\, . 
\end{align}
We also define an associated $(t+1)\times (t+1)$ Gram matrix $\bG_{\xi^{k,s}}=\bG_{\xi^{k,s},t}$ via 
\begin{equation}
\label{eq:bG-def}
    (\bG_{\xi^{k,s}})_{t_1,t_2}
    =
    \xi^{k,s}\big(\vR( 
    \f_{t_1},\f_{t_2})\big).
\end{equation}
The dependence of $\bG_{\xi^{k,s}}$ will often be suppressed.
We define a second Gram matrix 
$\bGG_{\xi^{k,s}}$, also of size $(t+1)\times (t+1)$, via (recall \eqref{eq:zeta-defn})
\begin{align}
\label{eq:bGG-def}
    (\bGG_{\xi^{k,s}})_{t_1,t_2}
    &=
    \zeta^{k,s}\lt(\vR\big( \f_{t_1},\f_{t_2}\big)\rt)
    \\
\nonumber
    &=
    \sum_{s'\in\sS}
    \partial_{x_{s'}}\xi^{k,s}
    \lt(\vR\big( \f_{t_1}, \f_{t_2}\big)\rt).
\end{align}
Finally, we let $\cF_t$ denote the $\sigma$-algebra generated by all iterates up to time $t$:
%
\begin{equation}
\label{eq:cFt-defn}
%
    \cF_t
    =
    \sigma\big(\{\bz_{k,t_1}\}_{k\le D,t_1\le t}\big)
    =
    \sigma\big(\{\bz_{k,t_1},\bw^{t_1},\f_{t_1}\}_{k\le D,t_1\le t}\big)\, .
%
\end{equation}

Throughout the proof of state evolution we make the following simplifying assumptions:

\begin{assumption}
\label{as:degree-D}
    $\xi$ is a degree $D$ polynomial with all coefficients $\gamma_{s_1,\dots,s_k}$ for $2\leq k\leq D$ strictly positive.
\end{assumption}

\begin{assumption}
\label{as:well-conditioned}
    Each matrix $\bG_{\xi^{k,s},t}$ is well-conditioned, i.e. 
    \[
    C^{-1}\le \sigma_{\min}(\bG_{\xi^{k,s},t})\le \sigma_{\max}(\bG_{\xi^{k,s},t})\le C
    \]
    for all $t\le T$. Here $\bG_{\xi^{k,s},t}$ is defined based on iterates that will appear in Theorems~\ref{thm:SELAMP} and \ref{lem:ampequalslamp}. The same holds for $\cL_{k,t}$ as defined in \eqref{eq:cL-k-t}.
\end{assumption}



It is a standard argument that to establish Proposition~\ref{prop:state_evolution}, it suffices to do so under the above assumptions. The reason is that one can always slightly perturb both $\xi$ and the non-linearities $f_{t,s}$ to ensure the assumptions hold. Then suitable continuity properties suffice to transfer all asymptotic guarantees. We refer the reader to \cite[Appendices A.8 and A.9]{ams20} for the arguments in the single-species case, still in the generality of mixed tensors. (In the more common setting $D=2$ of just a random matrix this step is also common for state evolution proofs, see e.g. \cite[Section 4.2.1]{javanmard2013state}.) The corresponding extension in our setting is completely analogous and omitted.




\subsection{Preliminary Lemmas}


The next lemma has several parts. All are elementary Gaussian calculations so their proofs are omitted.

\begin{lemma}
\label{lem:4}
For any deterministic $\bu,\bv\in \bbR^N$ and $\bA^{(k)}$ defined by \eqref{eq:A-defn} we have:
%
\begin{enumerate}
    \item Letting $g_0\sim\normal(0,1)$ be independent of $\bg\sim\normal(0,\id_N)$, we have
        \begin{align}
            \bA^{(k)}\{\bu\}\ed
            \sum_{s\in\sS}
            \bg_s
            \sqrt{\xi^{k,s}(\vR(\bu,\bu))}
            +  
            \frac{g_0}{\sqrt N}
            \sum_{s\in\sS}
            \bu_s
            \sqrt{
                \sum_{s'\in\sS}
                \partial_{x_{s'}} \xi^{k,s}
                \Big(
                \vR(\bu,\bu)
                \Big)
            }
            \, .
        \end{align}
    %
    \item Let $g_0,g_1,\dots,g_r\sim\normal(0,1)$ be independent. We have (jointly across $s\in\sS$)
    \begin{align}
        \sqrt{\lambda_s N} R_s(\bv,\bA^{(k)}\{\bu\})
        \ed 
        \sqrt{\xi^{k,s}(\vR(\bu,\bu))\cdot \vR(\bv,\bv)}g_s
        +  
        \sqrt{
            \sum_{s'\in\sS}
            \partial_{x_{s'}} \xi^{k,s}
            \Big(
            \vR(\bu,\bu)
            \Big)
        }
        R_s(\bu,\bv) \, g_0\, .
    \end{align}
    %
    \item For $s\in\sS$:
     \[
        R\lt(\bA^{(k)}\{\bu\},\bA^{(k)}\{\bv\}\rt)_s\simeq \xi^{k,s}\lt(\vR( \bu,\bv)\rt).
    \]
    \item For a deterministic symmetric tensor $\bT\in(\bbR^N)^{\otimes k-1}$, the vector
    $\bA^{(k)}\{\bT\}$ is centered Gaussian. Its covariance is given by
    %
    \begin{align*}
      \E\big[\bA^{(k)}\{\bT\}_i\bA^{(k)}\{\bT\}_j\big] 
      &= 
      \langle \xi^{k,s(i)}\diamond \bT,\, \bT\rangle_N
      \cdot 1\{i=j\} 
      \\
      &
      \quad
      +\frac{k(k-1)}{N^{k-1}}\,
      \sum_{i_1,\dots,i_{k-2}=1}^N
      \gamma_{i,i_1,\dots,i_{k-2}}
      \gamma_{j,i_1,\dots,i_{k-2}}
      T_{i,i_1,\dots,i_{k-2}}
      T_{j,i_1,\dots,i_{k-2}}\, . 
    \end{align*}
%
    \item Let $\bP\in\bbR^{N\times N}$ be the orthogonal projection onto a (deterministic) subspace $S\subseteq\bbR^N$ with $d=\dim(S)=O(1)$. Then
    \[
        \|\bP\bG^{(k)}\{\bu\} - \bG^{(k)}\{\bu\}\|_2 /\|\bG^{(k)}\{\bu\}\|_2\simeq 0.
    \]
\end{enumerate}
\end{lemma}
%



We next develop a formula for the conditional expectation of a Gaussian tensor $\bA^{(k)}$ given
a collection of linear observations.
We set $\bD$ to be the $t\times t\times t$ tensor with entries $D_{ijk}=1$ if $i=j=k$ and $D_{ijk}=0$ otherwise.
%
\begin{lemma}
\label{lem:symregression}
Recalling \eqref{eq:cFt-defn}, let $\E\{\bA^{(k)}|\cF_t\}$.
Equivalently $\E\{\bA^{(k)}|\cF_t\}$ is the conditional expectation of $\bA^{(k)}$ given the linear-in-$\bA^{(k)}$ observations
%
\begin{align}
%
\bA^{(k)}\{\f_{t'}\}=\by_{k,t'+1} \, \;\;\; \mbox{ for $s\in\{0,\dots,t-1\}$.} \label{eq:LinearConstraint}
%
\end{align}
%
Then we have for $i_1,i_2,\dots,i_k\leq n$,  
%
\begin{align}
%
\label{eq:SymmRegression}
    \E[\bA^{(k)}|\cF_t]_{i_1,i_2,\dots,i_k}= 
    \sum_{j=1}^k \sum_{0\leq r,s\leq t-1} (\hZZ_{k,t})_{i_j,s}\cdot (\bG^{-1}_{\xi^{k,s},t-1})_{s,r}\cdot (\f_{r,i_1}\cdots \f_{r,i_{j-1}}\f_{r,i_{j+1}}\cdots \f_{r,i_{k}})\, .
%
\end{align}
%
Here, the matrix $\hZZ_{k,t}\in\bbR^{N\times t}$ is defined as the solution of a system of linear equations as follows.
Define the linear operator $\cT_{k,t}:\bbR^{N\times t}\to\bbR^{N\times t}$ by letting, for $i\leq N$, $0\leq t_3\leq t-1$:
%
\begin{align}
\label{eq:Tdef}
    [\cT_{k,t}(\bZ)]_{i,t_3}&= \sum_{j=1}^N \sum_{0\leq t_1,t_2\leq t-1} (\f_{t_2})_{i} (\f_{t_2})_{j}
    \lt(
    (\bG_{\xi^{k,s(i)},t-1}^{-1})_{t_2,t_1} (\bGG_{\xi^{k,s(i)},t-1})_{t_2,t_3} 
    \rt)
    \diamond
    (\bZ)_{j,t_1}\, ,
%
\end{align}
%
Then $\hZZ_{k,t}$ is the unique solution of the following linear equation (with $\bY_{k,t}$ defined as per \eqref{eq:Ydef1})
%
\begin{align}
%
\hZZ_{k,t}+\cT_{k,t}(\hZZ_{k,t}) =\bY_{k,t}.\label{eq:ZZ-eq}
%
\end{align}

(Here, $\hZZ_{k,t} = [\hat{\bz}_{k,0},\cdots,\hat{\bz}_{k,t-1}]$ and $\bY_{k,t} = [\hat{\by}_{k,1},\cdots,\hat{\by}_{k,t}]$ have dimensions $N \times t$.)
%
\end{lemma}
%
%\am{Again, not clear the time index for $\bY_{k,t}$ is correct.}
%\ms{Right, changed}

The above formulas for $\E[\bA^{(k)}|\cF_t]$ and $\cT_{k,t}$ are rather complicated. In \cite[Appendix A]{ams20} the reader may find helpful tensor network diagrams for the single-species case. Unfortunately it is less clear how to draw a corresponding tensor network with multiple species.


\begin{proof}[Proof of Lemma \ref{lem:symregression}]
Let $\cV_{k,t}$ be the affine space of symmetric tensors satisfying the constraint \eqref{eq:LinearConstraint}.
The conditional expectation $\E[\bA^{(k)}|\cF_t]$ is the tensor with minimum weighted Frobenius norm $\|\cdot\|_{F,\xi^k}$ in the affine space $\cV_{k,t}$, given by
\begin{equation}
\label{eq:Gammak-inner-product}
    \|\bA\|_{F,\xi^k}^2
    =
    \langle 
    (\Gamma^{(k)})^{-1}\diamond \bA
    ,
    (\Gamma^{(k)})^{-1}\diamond\bA
    \rangle.
\end{equation}
Here $(\Gamma^{(k)})^{-1}$ is the entry-wise inverse of $\Gamma^{(k)}$, which exists by Assumption~\ref{as:degree-D}.

By Lagrange multipliers, there exist vectors $\bm_1,\dots,\bm_t\in\bbR^N$ such that $\E[\bA^{(k)}|\cF_t]=\hbA^{(k)}$ equals
%
\begin{align}
%
    \hbA^{(k)}_t \equiv\Gamma^{(k)}\diamond
    \sum_{t'=0}^{t-1}\sum_{j=1}^k \underbrace{\f_{t'}
    \otimes 
    \cdots 
    \otimes 
    \f_{t'}}_{\mbox{$j-1$ times}}\otimes\bm_{t'} 
    \otimes
    \underbrace{\f_{t'}\otimes \cdots \otimes \f_{t'}}_{\mbox{$k-j$ times}}\, .
%
\end{align}
%


Also by Lagrange multipliers, if a tensor $\hbA^{(k)}$ of this form (for some choice of vectors $\bm_1,\dots,\bm_t$) satisfies the constraints $\hbA^{(k)}\{\f_{t'}\}=\by_{k,t'+1}$
for $s< t$, then such a tensor is unique, and corresponds to $\E[\bA^{(k)}|\cF_t]$.
Without loss of generality, we write 
%
\begin{align}
%
\bm_r =  \sum_{t'=0}^{t-1}(\bG^{-1}_{\xi^{k,s},t-1})_{r,t'}\hbz_{t'}\, , \;\;\;\; \hZZ_{k,t}= [\hbz_1|\;\cdots\;|\hbz_t]\, .
%
\end{align}
%
By direct calculation we obtain that for each $i\in [N]$,
%
\begin{align}
\label{eq:SumT}
    \big(\hbA^{(k)}_t\{\f_{t_1}\}\big)_i &= \sum_{t_2=0}^{t-1} (\bG_{\xi^{k,s(i)},t-1})_{t_1,t_2}(\bm_{t_2})_i
    + 
    \sum_{t_2=0}^{t-1} (\bGG_{\xi^{k,s(i)},t-1})_{t_1,t_2}\<\f_{t_1},\bm_{t_2}\>(\f_{t_2})_i
    \\
    &= 
    (\hbz_{t_1})_i
    +
    \sum_{t_2=0}^{t-1} (\bGG_{\xi^{k,s(i)},t-1})_{t_1,t_2}\<\f_{t_1},\bm_{t_2}\>(\f_{t_2})_i
    \, .
%
\end{align}


We next stack these vectors as columns of an $N\times t$ matrix. The first term yields $\hZZ_{k,t}$. Moreover the second term
coincides with $\cT_{k,t}(\hZZ_{k,t})$ as following by rearranging the order of sums in \eqref{eq:SumT}. Hence
%
\begin{align}
%
    \big[\hbA^{(k)}_t\{\f_{0}\},\cdots,\hbA^{(k)}_t\{\f_{t-1}\}\big] 
    & =  
    \hZZ_{k,t} + \cT_{k,t}(\hZZ_{k,t})\, .
%
\end{align}
%
This in turn implies that the equation determining $\hZZ_{k,t}$ takes the form \eqref{eq:ZZ-eq}.
\end{proof}



\subsection{Long AMP}
\label{sec:LAMP}

As an intermediate step towards proving Theorem \ref{thm:mixedAMP}, we introduce a new iteration that we call Long AMP (LAMP), following 
\cite{berthier2019state}. This iteration is less compact but simpler to analyze. For each $k\le D$, let  
$\cS_{k,t}\subseteq (\bbR^N)^{\otimes k}$ be the linear subspace of tensors $\bT$ that are symmetric and such that
$\bT\{\f_{t_1}\}=0$ for all $t_1<t$.   We denote by $\cP_t^{\perp}(\bA^{(k)})$ the projection of $\bA^{(k)}$ 
onto $\cS_{k,t}$, in the inner product space \eqref{eq:Gammak-inner-product} corresponding to $\Gamma^{(k)}$. 
We then define the LAMP mapping 
%
\begin{align}
\label{eq:LAMP1}
    \LAMP_t\lt(\bv^{\le t}\rt)_{k}
    &\equiv
    \cP_t^{\perp} (\bA^{(k)})
    \{\f_t\}
    +
    \sum_{0\leq t_1\leq t}  
    h_{t,t_1-1,k}
    \diamond 
    \qq^{k,t_1},
    \\
\label{eq:LAMP2}
    h_{t,t_1,k,s}
    &\equiv 
    \sum_{0\le t_2\le t-1}
    \big[\bG_{\xi^{k,s},t-1}^{-1}\big]_{t_1,t_2}
    \big[\bG_{\xi^{k,s},t}\big]_{t_2,t}
    ,~~~ h_{t,-1,k}=0. 
\end{align}
%
Here we use similar notations $\f_t = f_t(\VV_t;\bE_t)$ and $\bG_{\xi^{k,s},t}$ as before (recall \eqref{eq:bG-def}), and take the vectors $\be^t$ as before. However the quantities $\f_t,\bG_{\xi^{k,s},t}$ are now different: they are computed using the vectors $\bv^0,\dots,\bv^t$ using the recursion:
\begin{align}
    \bv^t=\sum_{2\leq k\leq D} \qq^{k,t}\, ,\;\;\;\;\;\;\;
    \qq^{k,t+1}=\LAMP_t\lt(\bv^{\le t}\rt)_{k}\, . 
\end{align}


%
Following \cite{berthier2019state,ams20}, we first establish state evolution for LAMP (under the non-degeneracy Assumption~\ref{as:degree-D}), and then deduce the result for the original AMP.
In analyzing LAMP we use notations analogous to the ones introduced for AMP. In particular:
%
\begin{align}
    \VV_{t}
    &=
    [\bv_{1}|\bv_{2}|\dots|\bv_{t}]
    \\
    \bQ_{k,t}
    &=
    [
    \qq_{k,1}^{\otimes k}|
    \qq_{k,2}^{\otimes k}|
    \dots|
    \qq_{k,t}^{\otimes k}
    ].
\end{align}
%


\subsection{State Evolution for Long AMP}
 
%
\begin{theorem}
\label{thm:SELAMP}
  Under the assumptions of Theorem \ref{thm:mixedAMP}, let $\qq^{2,0},\cdots \qq^{D,0}\in\bbR^N$
be deterministic vectors and $\bv^0 =\sum_{2\leq k\leq D} \bq^{k,0}$.
Assume that the uniform empirical distribution of the $N$ vectors $\{(q_i^{2,0},\cdots, q_i^{D,0})\}_{i\leq N}$ converges 
in $\bbW_2$ distance to the law of the vector $(U^{k,0})_{2\le k\le D}$.

Further we assume there is a constant $C<\infty$ such that for all $t\le T$:
%
\begin{itemize}
%
\item[$(i)$] The matrices $\bG_{\xi^{k,s},t}= \bG_{\xi^{k,s},t}(\VV)$ are uniformly well-conditioned as guaranteed by Assumption~\ref{as:well-conditioned}.
%
\item[$(ii)$] Let the linear operator $\cT_{k,t}:\bbR^{N\times t}\to\bbR^{N\times t}$ be defined as per \eqref{eq:Tdef}, with $\bG_{\xi^{k,s},t} = \bG_{\xi^{k,s},t}(\VV,\bE)$,
and  $\f_t=f_t(\VV,\bE)$, and define 
\begin{equation}
\label{eq:cL-k-t}
\cL_{k,t} = {\boldsymbol 1}+\cT_{k,t}.
\end{equation}
Then $C^{-1}\le \sigma_{\min}(\cL_{k,t})\le \sigma_{\max}(\cL_{k,t})\le C$.
%
\end{itemize}

Then the following statements hold for any $t\le T$ and sufficiently large $N$:
%
\begin{enumerate}[label=(\alph*)]
\item Correct conditional law: 
%
\begin{equation}\label{eq:conditional}
%
\qq^{k,t+1}|_{\mathcal F_t}\ed \E[\qq^{k,t+1}|\mathcal F_t] +  \cP_t^{\perp}(\tbA^{(k)}) \{\f_t\}\, .
%
\end{equation}
%
where $\tbA^{(k)}$ is a symmetric tensor distributed identically to $\bA^{(k)}$ and independent of everything else,  and
$\cP_{t}^{\perp}$ is the projection onto the subspace $\cS_{k,t}$ defined in Section \ref{sec:LAMP}.
Further 
\begin{equation}
\label{eq:CondExp}
    \E[\qq^{k,t+1}|\mathcal F_t]
    = 
    \sum_{s\in\sS}
    \sum_{0\leq t_1\leq t}  
    h_{t,t_1-1,k,s}
    \qq^{k,t_1}_s\, .
\end{equation}
%
Moreover, the vectors $(\qq^{k,t+1})_{2\leq k\leq D}$ are conditionally independent given $\mathcal F_t$. 
%
\item Approximate isometry: we have
 %   
\begin{align}
\label{eq:c1}
    R_s(\qq^{k,t_1+1},\qq^{k,t_2+1})
    &\simeq 
    \xi^{k,s}\lt(\vR( \f_{t_1},\f_{t_2})\rt)\, ,
    \\
\label{eq:c3}
    R_s(\bv^{t_1+1},\bv^{t_2+1})
    &\simeq 
    \xi^{s}\big(\vR( \f_{t_1},\f_{t_2})\big). 
\end{align}
%
Moreover, both sides converge in probability to constants as $N\to\infty$, and for $k_1\neq k_2$ and any $(t_1,t_2)$ and $s\in\sS$,    
\begin{equation}
\label{eq:c2}
    R_s(\qq^{k_1,t_1},\qq^{k_2,t_2})\simeq 0.
\end{equation}
%
\item  State evolution: for each $s\in\sS$ and any pseudo-Lipschitz function 
$\psi:\bbR^{D \times 2(t+1)}\to \bbR$, we have
%
\begin{align}
\label{eq:ConvergenceLAMP}
    \plim_{N\to\infty}
    \frac{1}{N_s}
    \sum_{i\in\cI_s}
    \psi\big((q_i^{k,t'})_{ k\le D,t'\le t}; (e^t_i)_{t'\leq t}\big) 
    = 
    \E\big\{\psi\big((U^{k,t'}_s)_{2\leq k\leq D,t'\leq t}; 
    (E^{t'}_s)_{t'\leq t}
    \big)\big\}\, .
%
\end{align}
%
where $(U^{k,t}_s)_{k\le D,1\le t\le T}$ is the centered Gaussian process defined in the statement of  Theorem \ref{thm:mixedAMP}.
\end{enumerate}
\end{theorem}
% %
% %
In the next subsection, we will prove these statements by induction on $t$. The crucial point we exploit is the representation $(a)$. We emphasize that the iteration number $t$ is bounded as $N\to\infty$; therefore all numerical quantities not depending on $N$ (but possibly on $t$) will be treated as constants. 


\subsection{Proof of Theorem~\ref{thm:SELAMP}}

The proof will be by induction over $t$. The base case is clear, so we focus on the inductive step. We assume the statements above for $t-1$ and prove them for $t$. 

\subsubsection{Proof of $(a)$}

Note that $\cP_t^{\perp}(\bA^{(k)})$ is by construction independent of $\cF_t$, and therefore we can replace $\bA^{(k)}$
by a fresh independent matrix in \eqref{eq:LAMP1}, whence \eqref{eq:conditional} follows. The equality \eqref{eq:CondExp} holds by definition of the iteration.


\subsubsection{Proof of $(b)$: Approximate isometry}

We will repeatedly apply Lemma~\ref{lem:4}. We start with  \eqref{eq:c1}. As we are inducting on $t$, we may limit ourselves to considering overlaps
$\vR( \qq^{k,t+1},\qq^{k,t_1+1})$, for $t_1\le t$. 

Define the tensor $\Gamma^{(k),\nabla}\in (\bbR^{\sS}_{\geq 0})^{\otimes (k-1)}$ by
\begin{equation}
\label{eq:Gamma-nabla-def}
    \Gamma^{(k),\nabla}_{s_1,\dots,s_{k-1}}
    =
    \sqrt{
    k
    \sum_{s\in\sS}
    \lambda_s
    \big(\Gamma^{(k)}_{s,s_1,\dots,s_{k-1}}\big)^2
    }.
\end{equation}
We choose 
\[
    (\f_{t}^{\otimes k-1})_{\parallel}
    \in
    {\rm span}\lt(\f_{t_1}^{\otimes k-1}\rt)_{t_1< t}
\]
such that
\[
    \Gamma^{(k),\nabla}\diamond \big(\f_{t}^{\otimes k-1}\big)_{\parallel}
\]
is the orthogonal projection of $\Gamma^{(k),\nabla}\diamond \f_{t}^{\otimes k-1}$ onto 
\[
    {\rm span}\lt(\Gamma^{(k),\nabla}\diamond\f_{t_1}^{\otimes k-1}\rt)_{t_1< t}
\]
and also set
\[
    (\f_{t}^{\otimes k-1})_{\perp}=\f_{t}^{\otimes k-1}-(\f_{t}^{\otimes k-1})_{\parallel}.
\]

We will use (and soon after, prove) the following lemma.
\begin{lemma}
\label{lem:LemmaPerp}
For all $t_1\leq t_1$, we have
\begin{equation}
\label{eq:LemmaPerp2}
    \cP_t^{\perp} (\tbA^{(k)})\{(\f_t^{\otimes k-1})_{\perp}\} \simeq
    \tbA^{(k)}\{(\f_t^{\otimes k-1})_{\perp}\}\, .
\end{equation}
\end{lemma}
%
For $t_1\le t-1$, using Lemma~\ref{lem:4}, point 2 implies
\begin{align*}
    \vR(\qq^{k,t+1},\qq^{k,t_1+1}) 
    \simeq 
    \vR\big(\E[\qq^{k,t+1}|\cF_t],\, \qq^{k,t_1+1}\big)
\end{align*}
%
We next use the formula in $(a)$ for $\E[\qq^{k,t+1}|\cF_t]$ together with the expression in \eqref{eq:LAMP1}. For each $s\in\sS$:
\begin{align}
\nonumber
    R\big(\mathbb E[\qq^{k,t+1}|\mathcal F_t],\qq^{k,t_1+1}\big)_s
    &\simeq
    R\Bigg(
    \sum_{0\leq t_2,t_3\leq t-1}
    \qq^{k,t_3+1}(\bG_{\xi^{k,s},t-1}^{-1})_{t_3,t_2}
    \,
    \xi^{k,s}\big(\f_{t_2},\f_{t}\big)
    ,\, \qq^{k,t_1+1}
    \Bigg)_s
    \\
\nonumber
    &= 
     \sum_{0\leq t_2,t_3\leq t-1}
     R\big(\qq^{k,t_3+1},\qq^{k,t_1+1}\big)_s
     \,
     (\bG_{\xi^{k,s},t-1}^{-1})_{t_3,t_2} 
     \,
     \xi^{k,s}\big(\f_{t_2},\f_{t}\big)
    \\
\label{eq:3rd-ineq}
    &\simeq
    \sum_{0\leq t_2,t_3\leq t-1} 
    (\bG_{\xi^{k,s},t-1})_{t_3,t_1}
    \,
    (\bG_{\xi^{k,s},t-1}^{-1})_{t_3,t_2} 
    \,
    \xi^{k,s}\big(\f_{t_2},\f_{t}\big)
    \\
\nonumber
    &=
    \big(
    \bG_{\xi^{k,s},t-1}
    \times 
    \bG_{\xi^{k,s},t-1}^{-1}
    \times
    \bG_{\xi^{k,s},t-1}
    \big)_{t_1,t}
    \\
\nonumber
    &=
    (\bG_{\xi^{k,s},t-1})_{t_1,t}.
%
\end{align}
Here \eqref{eq:3rd-ineq} comes from the induction hypothesis \eqref{eq:c1} (and the symmetry of the matrix $\bG_{\xi^{k,s},t-1}$ is used to obtain the next line). We next prove that \eqref{eq:c1} holds for $t_1=t$. We have by definition of the projections that
%
\begin{align*}
\cP_t^{\perp} (\tbA^{(k)})\{\f_t\}=\cP_t^{\perp} (\tbA^{(k)})\{(\f_t^{\otimes k-1})_{\perp}\} \, ,
\end{align*}
%
where the right-hand side is defined according to \eqref{eq:TensorTensor}.
%
Using \eqref{eq:LemmaPerp2} from Lemma~\ref{lem:LemmaPerp} as well as point~4 of Lemma~\ref{lem:4}, we have
%
\begin{align}
\label{eq:PAnorm}
    R\lt(
    \cP_t^{\perp} (\tbA^{(k)})\{\f_t\}
    \,,
    \cP_t^{\perp} (\tbA^{(k)})\{\f_t\}
    \rt)_s
    \simeq 
    \xi^{(k,s)}\lt(
    R\big(
    (\f_{t}^{\otimes k-1})_{\perp}
    \,,
    (\f_{t}^{\otimes k-1})_{\perp}
    \big)
    \rt).
\end{align}
%
Next, using \eqref{eq:LemmaPerp2} and Lemma~\ref{lem:4} (point 2), we obtain that for all $s\in\sS$
%
\begin{equation}
\label{eq:ApproxOrth}
    R\big(\cP_t^{\perp} (\tbA^{(k)})\{\f_t\},\E[\qq^{k,t+1}|\cF_t]\big)_s \simeq 0\, .
\end{equation}
%
Moreover we recall that by the expression for $\E[\qq^{k,t+1}|\cF_t]$ from part $(a)$,
%
\begin{align}
%
    R\lt(
    \E[\qq^{k,t+1}|\mathcal F_t]
    \,,
    \E[\qq^{k,t+1}|\mathcal F_t]
    \rt)
    \simeq
    \xi^{k,s}
    \lt(
    \vR(
    (\f_t^{\otimes k-1})_{\parallel}
    \,,
    (\f_t^{\otimes k-1})_{\parallel}
    )
    \rt).
%
\end{align}
%
The formula for linear regression implies
%
\begin{align}
\label{eq:lin-reg-1}
%
    (\f_{t}^{\otimes k-1})_{\parallel}
    & =
    \sum_{0\leq t_1\leq t-1} \alpha_{t_1,t} \Gamma^{(k,s)}\diamond \f_{t_1}^{\otimes k-1},
    \\
    \alpha_{t_1,t}
    &=
    \sum_{0\leq t_2\le t-1} 
    (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_2}
    \< 
    \Gamma^{(k,s)}\diamond \f_{t_2}^{\otimes k-1},
    \Gamma^{(k,s)}\diamond \f_{t}^{\otimes k-1}
    \>_N
    \\
    &=
    \sum_{0\leq t_2\le t-1} 
    (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_2}
   (\bG_{\xi^{k,s},t}^{-1})_{t_2,t}
    \, .
%
 \end{align}
%

By part $(b)$ of the inductive step, for $1\leq t_1,t_2\leq t-1$ we have
\[
    \xi^{k,s}\big(\vR( \f_{t_2},\f_{t_1})\big)
    \simeq 
    R_s( \qq_{k,t_2+1},\qq_{k,t_1+1})
    \,.
\]
In particular the formulas \eqref{eq:LAMP2} and \eqref{eq:lin-reg-1} have asymptotically the same coefficients, and the overlap structure between the summands is identical. It follows that
\begin{align}
\label{eq:CEnorm}
    R\lt(
    \E[\qq^{k,t+1}|\mathcal F_t],
    \,,
    \E[\qq^{k,t+1}|\mathcal F_t]
    \rt)
    \simeq
    R\lt(
    (\f_{t}^{\otimes k-1})_{\parallel}
    \,,
    (\f_{t}^{\otimes k-1})_{\parallel}
    \rt).
\end{align}
%
Using together Eqs.~\eqref{eq:PAnorm}, \eqref{eq:ApproxOrth}, and \eqref{eq:CEnorm}, we get
%
\begin{align*}
%
    R\lt(\qq^{k,t+1},\qq^{k,t+1}\rt)
    &\simeq 
    R\lt(\E[\qq^{k,t+1}|\mathcal F_t]
    \,,
    \E[\qq^{k,t+1}|\mathcal F_t]
    \rt)
    + 
    R\lt(
    (\f_t^{\otimes k-1})_{\perp}
    \,,
    (\f_t^{\otimes k-1})_{\perp}
    \rt)
    \\
    &\simeq
    R\lt(
    (\f_t^{\otimes k-1})_{\perp}
    \,,
    (\f_t^{\otimes k-1})_{\perp}
    \rt)
    \\
    &=
    \xi^{k,s}\lt(
    \vR( \f_t,\f_t)
    \rt)
    \,.
%
 \end{align*}
%
This establishes \eqref{eq:c1}.

Next consider \eqref{eq:c2}, i.e., approximate orthogonality of $\qq^{k,r}$ and $\qq^{p',r}$ for $k\neq p'.$ This follows easily from the representation
in point $(a)$ which, together with Lemma~\ref{lem:4}, inductively implies that the iterates $\qq^{s,k}$ for different $k$ are approximately orthogonal.
Finally, \eqref{eq:c3}  follows directly from \eqref{eq:c1} and \eqref{eq:c2}.
We now prove Lemma~\ref{lem:LemmaPerp}.

\begin{proof}[Proof of Lemma~\ref{lem:LemmaPerp}]
For convenience we write $\tbA=\tbA^{(k)}$. By Lagrange multipliers, there exist vectors
    $(\btheta_{t_1})_{t_1 \le t-1}$ in $\bbR^N$ such that $\cP_t^{\perp} (\tbA) = \tbA - \bQ$, where 
\begin{align*}
  \bQ = 
  \big(\Gamma^{(k)}\big)^{\odot 2}
  \diamond
  \frac{(k-1)!}{N^{k-1}}\sum_{t_1=0}^{t-1} \sum_{j=1}^k  \underbrace{\f_{t_1}\otimes \cdots \otimes \f_{t_1}}_{\mbox{$j-1$ times}}\otimes \btheta_{t_1} \otimes \underbrace{\f_{t_1}\otimes \cdots \otimes \f_{t_1}}_{\mbox{$k-j$ times}}.
\end{align*}
%
The vectors $(\btheta_{t_1})_{t_1 \le t-1}$ are determined by the equations
% $\cP_t^{\perp} (\tbA) \{\f_{t_1}\} = 0$
$\bQ \{\f_{t_1}\}=\tbA \{\f_{t_1}\}$
for all $t_1\le t-1$.
This expands (for each $t_1\leq t-1$) to
%
\begin{align*}
  %
  \sum_{t_2\leq t-1} 
  (\bG_{\xi^{k,s},t-1})_{t_1,t_2}
  \diamond 
  \btheta_{t_2}
  +
%   (k-1)
  \sum_{t_2\leq t-1}
  (\bGG_{\xi^{k,s}})_{t_1,t_2}
  \diamond
  \<\f_{t_1},\btheta_{t_2}\>_N \f_{t_2}
  = 
  \tbA\{\f_{t_1}\}\, .
  %
\end{align*}
%
Recall that we assume each $\bG_{\xi^{k,s},t-1}$ is well conditioned with high probability. Thus we can multiply the system of $t$ equations above by $\bG_{\xi^{k,s},t-1}^{-1}$ in the coordinates $\cI_s$ for each $s\in\sS$. For each $t_3\leq t-1$, we obtain:
%
\begin{align}
\label{eq:LambdaEq}
  \btheta_{t_3}
  +
  \sum_{t_1,t_2<t}
  \lt(
  (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_3} (\bGG_{\xi^{k,s},t-1})_{t_1,t_2}
  \rt)
  \diamond
  \<\f_{t_1},\btheta_{t_2}\>_N \f_{t_2}=
  \sum_{t_1<t}
  (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_3}
  \diamond
  \tbA\{\f_{t_1}\}\, . 
  %
\end{align}
%
Switching $t_3$ to $t_1$, we find
%
\begin{align}
\nonumber
  %
  \btheta_{t_1} &=   \btheta^0_{t_1}+ \btheta^{\parallel}_{t_1}\, ,
  \\
\label{eq:theta0}
  \btheta^0_{t_1}
  &\equiv
  \sum_{t_2<t}
  (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_2}\diamond \tbA\{\f_{t_2}\} \, ,
  \\
\nonumber
  \btheta^{\parallel}_{t_1}
  &\in 
  \spn\lt((\f_{t_2,s})_{t_2<t,s\in\sS}\rt).
  %
\end{align}
%
We claim that $\|\btheta^{\parallel}_{t_1}\|_N\simeq 0$, i.e.,
$\btheta_{t_1}\simeq   \btheta^0_{t_1}$. Indeed, let $\bTheta\in\bbR^{N\times t}$ be the matrix with columns
$(\btheta_{t_2})_{t_2<t}$, and $\bTheta^0$ the matrix with columns $(\btheta^0_{t_2})_{t_2<t}$.
Then \eqref{eq:LambdaEq} can be written as
%
\begin{align*}
  %
  \cL_{k,t}^{\sT}(\bTheta) =\bTheta^0\, .
  %
\end{align*}
%
Here we recall $\cL_{k,t}=\bfone+\cT_{k,t}$ and $\cT_{k,t}\in\bbR^{Nt\times Nt}$ is defined in \eqref{eq:Tdef}.
Substituting the decomposition 
$\bTheta = \bTheta^0+\bTheta^{\parallel}$ in the above, we obtain
%
\begin{align*}
  %
  \cL_{k,t}^{\sT}(\bTheta^{\parallel}) =-\cT^{\sT}_{k,t}(\bTheta^0)\, .
  %
\end{align*}
%
Recall that $\cL_{k,t}$ is well conditioned by Assumption~\ref{as:well-conditioned}. Therefore it remains to prove 
\begin{equation}
\label{eq:key-lamp-computation}
    \cT^{\sT}_{k,t}(\bTheta^0)\stackrel{?}{\simeq} 0.
\end{equation}
%
Let $\bc_0,\cdots,\bc_{t-1} \in \bbR^N$ be the columns of $\cT^{\sT}_{k,t}(\bTheta^0)$. We first note that for all $t_1\leq t-1$ and $s\in\sS$, 
\[
    \bc_{t_1,s} \in \spn((\f_{t_2,s})_{t_2<t}).
\]
Moreover the Gram matrix 
\[
    \bG_{1,t-1,s}=\lt(R_s(\f_{t_1},\f_{t_2})\rt)_{t_1,t_2<t}
\]
is well conditioned for each $s\in\sS$. Therefore it is sufficient to check that $R_s(\f_{t_1},\bc_{t_4})\simeq 0$ for each $t_1,t_4<t$ and $s\in\sS$. Plugging in the definition \eqref{eq:Tdef}, it remains to check
%
\begin{align*}
  \sum_{t_2,t_3<t}
  \lt\<
  \f_{t_1}
  ,
  \,
  (\bG_{\xi^{k,s},t-1}^{-1})_{t_2,t_3} 
  (\bGG_{\xi^{k,s},t-1})_{t_4,t_3}
  \diamond
  \btheta^0_{t_2}
  \rt\>_N 
  R_s(\f_{t_1},\f_{t_3})
  \stackrel{?}{\simeq} 
  0\, 
  ,
  \quad
  \forall~ 0\leq t_1,t_4\leq t-1\,.
\end{align*}
%
Finally, this last claim follows by substituting the definition \eqref{eq:theta0} of $\btheta^0_{t_2}$, and using the fact that  
\[
    R_s(\f_{t_1},\tbA\{\f_{t_3}\})\simeq 0,\quad
    \forall~ t_1,t_3\le t,\,s\in\sS
\]
which follows from Lemma \ref{lem:4}. Thus \eqref{eq:key-lamp-computation} is established.



We are now ready to prove Lemma~\ref{lem:LemmaPerp}. 
First note that 
\begin{equation}
\label{eq:god}
     \tbA\{(\f_t^{\otimes k-1})_{\perp}\} 
    -
    \cP_t^{\perp} 
    (\tbA)
    \{(\f_t^{\otimes k-1})_{\perp}\} 
    = 
    \bQ\{(\f_t^{\otimes k-1})_{\perp}\}
\end{equation}
decomposes into two types of terms based on the definition of $\bQ$ above. Recalling \eqref{eq:Gamma-nabla-def}, the first involves
\[
    \lt\langle 
    \Gamma^{(k),\nabla}
    \diamond 
    (\f_{t_1}^{\otimes k-1})_{\perp}
    ,
    \Gamma^{(k),\nabla}
    \diamond 
    (\f_t^{\otimes k-1})_{\perp}
    \rt\rangle_N
    \btheta_{t_1}
\]
for $t_1\leq t-1$, which vanishes by the definition of $(\f_t^{\otimes k-1})_{\perp}$. The other terms take the form
\[
    \lt\langle 
    \Gamma^{(k),\nabla}
    \diamond 
    (\btheta_{t_1}\otimes \f_{t_1}^{\otimes k-2})
    ,\,
    \Gamma^{(k),\nabla}
    \diamond 
    (\f_t^{\otimes k-1})_{\perp}
    \rt\rangle_N
    ~
    \f_{t_1}.
\]
In particular, this means that to prove \eqref{eq:god} vanishes, suffices to show
\[
    R\lt(
    \bQ\{(\f_t^{\otimes k-1})_{\perp}\}
    ,\,
    \f_{t_2}
    \rt)
    =0
\]
for all $t_2\leq t$.


Note that by construction,
\[
   (\f_t^{\otimes k-1})_{\perp}
   =
   \sum_{t_1\leq  t} b_{t_1} \f_{t_1}^{\otimes k-1}
    \,.
\]
By the well-conditioning assumption, the $b_{t_1}$ are bounded. Therefore it suffices to show that 
\[
    R\lt(
    \bQ\{\f_{t_2}^{\otimes k-1}\}
    ,\,
    \f_{t_1}
    \rt)
    \stackrel{?}{\simeq} 
    0,
    \quad 
    \forall\, t_1\leq t-1,\,t_2\leq t.
\]
Finally note that each term in the left-hand side includes an overlap $R_s(\btheta_{t_1},\f_{t_2})$. However these all vanish:
\[
    R_s(\btheta_{t_1},\f_{t_2})
    \simeq
    0.
\]
This is because we can substitute $\btheta_{t_1}$ with $\btheta_{t_1}^0$ as defined in \eqref{eq:theta0} and use the fact that $\vR(\tbA\{\f_{t_3}\},\f_t)\simeq 0$ which follows from Lemma \ref{lem:4}. This completes the proof.
\end{proof}


\subsubsection{Proof of $(c)$}

Recall that the process $(U^{k,t}_s)_{t\ge 1}$ is Gaussian by construction, and independent of $U^{k,0}_s$. Define 
\begin{align*}
    C_{t_1,t_2,s} &= \E\big[U^{k,t_1}_s U^{k,t_2}_s\big]\,;
    \\
    \bC_{\le t,s} &= (C_{t_1,t_2,s})_{t_1,t_2\le t}\,.
\end{align*}
We then have
%
\begin{equation} 
\label{eq:wt-alpha}
\begin{aligned}
    \E[U^{k,t+1}_s| U^{k,0}_s,\dots,U^{k,t}_s]
    &=
    \sum_{t_1=1}^t 
    \wt\alpha_{t_1,s} U^{k,t_1}
    \, ;
    \\
    \wt\alpha_{t_1,s}
    & \equiv
    \sum_{t_2=1}^t
    (\bC^{-1}_{\le t,s})_{t_1,t_2}C_{t_2,t+1,s}
    \, .
\end{aligned}
\end{equation} 
Here in writing $(\bC^{-1}_{\le t,s})_{t_1,t_2}$, we view $\bC_{\le t,s}$ as a $(t+1)\times (t+1)$ matrix for each $s\in\sS$.
%

On the other hand, from point $(a)$, we know that
%
\begin{equation} 
\label{eq:alpha}
\begin{aligned}
    \E[\qq^{k,t+1}_{s}|\mathcal F_t]
    & = 
    \sum_{1\leq t_1\leq t}  
    \alpha_{t_1,s} 
    \qq^{t_1,k}_s\, ;
    \\
    \alpha_{t_1,s}
    & \equiv
    \sum_{t_2=1}^t
    (\bG^{-1}_{\xi^{k,s},t-1})_{t_1-1,t_2-1} 
    (\bG_{\xi^{k,s},t})_{t_2-1,t}
    \, .
\end{aligned}
\end{equation} 
%
Moreover the induction hypothesis of \eqref{eq:ConvergenceLAMP} implies that for $t_1,t_2 \le t$,
%
\begin{equation}
\label{eq:augment-e-for-W2}
    (\bG_{\xi^{k,s},t})_{t_1,t_2} 
    \simeq 
    \bbE\lt[
    \xi^{k,s}
    \lt(
    f_{t_1}(W^0_s,\dots,W^{t_1}_s;E^0_s,\dots,E^{t_1}_s), f_{t}(W^0_s,\dots,W^{t_2}_s;E^0_s,\dots,E^{t_2}_s)\}
    \rt)
    \rt]
    \, .
\end{equation}
%
(Recall that by definition $W^t_s \equiv \sum_{k\le D} U^{k,t}_s$, while $\f_t=f_t(\VV_t;\bE_t)$ here.) 


Therefore, from the definition of the process $(U^{k,t}_s)_{t\ge 0}$,
\[
    (\bG_{\xi^{k,s},t})_{t_1,t_2} \simeq C_{t_1+1,t_2+1,s}
    ,
    \quad\quad \forall t_1,t_2\le t.
\]
Recalling that $\bG_{\xi^{k,s},t}$ is well conditioned, we find (recall \eqref{eq:wt-alpha},\eqref{eq:alpha}):
\[
    \alpha_{t_1,s}\simeq \wt\alpha_{t_1,s}.
\]
Therefore we also have 
%
\begin{align*}
    \E[\qq^{k,t+1}|\mathcal F_t] - \sum_{t_1=1}^t \wt\alpha_{t_1}\diamond \qq^{k,t_1}
    &=
    \sum_{t_1=1}^t (\alpha_{t_1}-\wt\alpha_{t_1}) \diamond \qq^{k,t_1}
    \\
    &\simeq 
    0.
%
\end{align*}
%
Moreover, Lemma~\ref{lem:4} (point 4) shows that $\cP^{\perp}_t(\tbA^{(k)})\{\f_t\}\simeq\tbA^{(k)}\{(\f_{t}^{\otimes k-1})_{\perp}\}$ 
has entries which are approximately independent Gaussian with variance 
\[
    \sigma^2_{t,s}\equiv 
    \lt(
    \Gamma^{(k),\nabla}
    \diamond
    (\f_{t}^{\otimes k-1})_{\perp}
    ,\,
    \Gamma^{(k),\nabla}
    \diamond
    (\f_{t}^{\otimes k-1})_{\perp}
    \rt)
\]
on coordinates $i\in\cI_s$, even conditionally on $\cF_t$. 
Therefore
%
\begin{align}
\label{eq:ReprSE}
  %
  \qq^{k,t+1} &\ed \sum_{t_1=1}^t \wt\alpha_{t_1} \diamond\qq^{k,t_1} +\sigma_t \diamond \bg + \bferr^{k,t+1}\, ,
  %
\end{align}
%
where $\|\bferr\|_N\simeq 0$ and $\bg\sim\normal(\bfzero,\id_N)$ is independent of everything else.
It now remains to verify that this agrees with the desired covariance.
As proved in the previous point, for any $t_1\le t$,
%
\begin{align*}
  %
  R\lt(\qq^{k,t+1} ,\qq^{k,t'+1}\rt)_s
  &\simeq 
  \xi^{k,s}\lt(
  \f_t,\f_{t'}
  \rt)
  \\
  &\simeq
  \E\big[ 
  U^{k,t+1}_s U^{k,t'+1}_s
  \big]\, .
%
\end{align*}
%
Therefore, in order to  prove \eqref{eq:ConvergenceLAMP}, it is sufficient to consider $\psi:\bbR^{D \times (t+1)}\to\bbR$  Lipschitz.
Using the representation \eqref{eq:ReprSE}, and focusing for simplicity on a single $k$, we get
%
\begin{align*}
  %
  \frac{1}{N_s}
  \sum_{i\in\cI_s}
  \psi\big(\qq_i^{k,\le t}, q_i^{k,t+1}
  ;
  \be^{\leq t}_i
  \big) 
  &\simeq 
  \frac{1}{N_s}
  \sum_{i\in\cI_s}
  \psi\lt(
      \qq_i^{k,\le t},
      \sum_{s=1}^t 
      \wt\alpha_s \qq^{k,s} 
      +
      \sigma_tg_i
      ;
    \be^{\leq t}_i
  \rt)
  \\
  &\simeq 
  \frac{1}{N_s}
  \sum_{i\in\cI_s}
  \bbE^{g\sim\cN(0,1)}
  \psi\lt(
  \qq_i^{k,\le t},
  \sum_{s=1}^t 
  \wt\alpha_s 
  \qq^{k,s} 
  +
  \sigma_t g
  ;
  \be^{\leq t}_i
  \rt)
  \, .
  %
\end{align*}
%
The second equality above follows by Gaussian concentration. Applying induction hypothesis now implies \eqref{eq:ConvergenceLAMP}, except that $\be^{t+1}$ is not present. 
However since $\be^{t+1}_i$ and $E^{t+1}_s$ have the same law and are both independent of the past, $\bbW_2$ convergence immediately transfers (this is the only step of the proof that involves the vectors $\be^t$ at all). This completes the proof.


\subsection{Asymptotic Equivalence of AMP and Long AMP}

Here we show that AMP and LAMP produce approximately the same iterates. 
%
\begin{lemma}
\label{lem:ampequalslamp}
Let $\{\bG^{(k)}\}_{k\le D}$ be standard Gaussian tensors, and $\bA^{(k)} = \Gamma^{(k)}\diamond \bG^{(k)}$ for $k\ge 2$. Consider the corresponding AMP
iterates $\ZZ_{t}\equiv (\bz^{k,t_1})_{k\le D,t_1\le t}$ and LAMP iterates $\bQ_{t}\equiv (\qq^{k,t_1})_{k\le D,t_1\le t}$,
from the same initialization $\ZZ_0=\bQ_0$ satisfying the assumptions of Theorem \ref{thm:mixedAMP}
and Theorem \ref{thm:SELAMP}.

Let $\f_t = f_t(\VV_t;\bE_t)$, $t\ge 0$ be the nonlinearities applied to LAMP iterates.
% and $(\bG_{k,t}(\VV))_{r,s}=\<\f_t,\f_{t'}\>^k$ be the corresponding Gram matrices.
Further assume that there exists a constant $C<\infty$ such that, for all $t\le T$,
%
\begin{itemize}
%
\item[$(i)$] The LAMP Gram matrices $\bG_{k,t} = \bG_{k,t}$ are
  well-conditioned as guaranteed by Assumption~\ref{as:well-conditioned}, i.e., 
 \[
    C^{-1}\le \sigma_{\min}(\bG_{k,t})\le \sigma_{\max}(\bG_{k,t})\le C,
    \quad\quad
    \forall k\le D,~t\le T\,.
\]
%
\item[$(ii)$] Let the linear operator $\cT_{k,t}:\bbR^{N\times t}\to\bbR^{N\times t}$ be defined as per \eqref{eq:Tdef}, with $\bG_{k,t} = \bG_{k,t}(\VV)$,
and $\f_t=f_t(\VV,\bE_t)$, and define $\cL_{k,t} = \bone+\cT_{k,t}$. Then 
\[
    C^{-1}\le \sigma_{\min}(\cL_{k,t})\le \sigma_{\max}(\cL_{k,t})\le C.
\]
%
\end{itemize}
%
Then, for any $t\le T$, we have
%
\begin{align}
%
\|\ZZ_{t} - \bQ_{t}\|_N\simeq 0 \, .
%
\end{align}
%
\end{lemma}
%
\begin{proof}
Throughout the proof we will suppress $\bE_t$ and simply write $f_t(\bW_t)$ or $f_t(\bV_t)$ to distinguish AMP and LAMP iterates, and analogously for
$\bG_{k,t}(\bW_t)$ or $\bG_{k,t}(\bV_t)$.
The proof is by induction over the iteration number, so we will assume it to hold at iteration $t$, 
and prove it for iteration  $t+1$. We prove the induction step by establishing the following two facts for each $2\leq k\leq D$:
%
\begin{align}
%
\big\|\AMP_{t+1}(\ZZ_{t})_k -\AMP_{t+1}(\bQ_{t})_k \big\|_N&\simeq 0 \, ,\label{eq:LAMPapprox1}\\
%
\big\|\AMP_{t+1}(\bQ_{t})_k -\LAMP_{t+1}(\bQ_{t})_k \big\|_N &\simeq 0\, . \label{eq:LAMPapprox2}
%
\end{align}

Let us first consider the claim \eqref{eq:LAMPapprox1}, and note that
%
\begin{align*}
    \AMP_{t+1}(\ZZ_{t})_k -\AMP_{t+1}(\bQ_{t})_k  &= \bA^{(k)}\{f_t(\bW_t)\}-\bA^{(k)}\{f_t(\bV_t)\} 
    \\
    &\quad - 
    \sum_{t_1\leq t} 
    d_{t,t_1,k}
    \diamond
    \big(f_{t_1-1}(\bW_{t_1-1})-f_{t_1-1}(\bV_{t_1-1})\big)\, ,
\end{align*}
%
where we wrote $d_{t,t_1,k,s}$ for the coefficients of \eqref{eq:AMP-def2}, with AMP iterates replaced by LAMP iterates. 
We then have 
%
\begin{align*}
%
    \big\|\AMP_{t+1}(\ZZ_{t})_k -\AMP_{t+1}(\bQ_{t})_k \big\|_N 
    &\le 
    D_{1,t}+D_{2,t}\, ;
    \\
    D_{1,t} 
    & \equiv 
    \big\| \bA^{(k)}\{f_t(\bW_t)\}-\bA^{(k)}\{f_t(\bV_t)\}\big\|_N\, ,
    \\
    D_{2,t} 
    &\equiv 
    \sum_{t_1\leq t,~s\in\sS} 
    |d_{t,t_1,k,s}| 
    \cdot 
    \big\|f_{t_1-1,s}(\bW_{t_1-1})- f_{t_1-1,s}(\bV_{t_1-1})\big\|_N\, .
%
\end{align*}
%
Notice that, by the induction assumption (and recalling that each $f_{t,s}$ is Lipschitz continuous and acts component-wise): 
%
\begin{equation}
\label{eq:InductionF}
%
    \big\|f_t(\bW_t)-f_t(\bV_t)\big\|_N
    \le C_T
    \sum_{t_1\le t,~k\le D}\|\bw^{k,t_1}-\bv^{k,t_1}\|_N 
    \simeq 
    0
    \, .
%
\end{equation}
%
Further, for any tensor $\bT\in(\bbR^{N})^{\otimes k}$, and any vectors $\bv_1,bv_2\in\bbR^N$,
%
\begin{align}
%
\|\bT\{\bv_1\}-\bT\{\bv_{2}\}\|_N\le (N^{\frac{k-2}{2}}\|\bT\|_{\op})  (\|\bv_1\|_N+\|\bv_2\|_N)^{k-2} \|\bv_1-\bv_2\|_N
%
\end{align}
%
Using Lemma~\ref{lem:4}, this implies that the following bound holds with high probability for a constant $C$:
%
\begin{align*}
    D_{1,t} 
    &\le C  
    (\|f_t(\bW_t)\|_N+\|f_t(\bV_t)\|_N)^{k-2} \|f_t(\bW_t)-f_t(\bV_t)\|_N\\
    & \le 
    C  (2\|f_t(\bV_t)\|_N+\|f_t(\bW_t) -f_t(\bV_t)\|_N)^{k-2} \|f_t(\bW_t) -f_t(\bV_t)\|_N
    \\
    &\simeq 0.
\end{align*}
%
The last step follows from \eqref{eq:InductionF} and Theorem~\ref{thm:SELAMP}, which implies (recall each $f_{t,s}$ is Lipschitz) that
$\|f_t(\bV_t)\|_N\le C$ with probability $1-o(1)$. Notice that the same argument implies $\|f_t(\bW_t)\|_N \le C$ with high probability.

Similarly, $D_{2,t}\simeq 0$ follows since $\|f_{t_1-1}(\bW_{t_1-1})- f_{t_1-1}(\bV_{t_1-1})\|_N\simeq 0$ and $|d_{t,t_1,k,s}|\le C_T$ by construction, thus yielding \eqref{eq:LAMPapprox1}.


We now prove \eqref{eq:LAMPapprox2}. Comparing \eqref{eq:AMP-def2} and \eqref{eq:LAMP1}, with
$\cP_t^{\parallel} = \bfone-\cP_t^{\perp}$ we find
%
\begin{align}
\label{eq:AMP-LAMP}
    \AMP_{t+1}(\bQ_{t})_k 
    -
    \LAMP_{t+1}(\bQ_{t})_k 
    &= 
    \cP_t^{\parallel} (\bA^{(k)})\{f_t(\bV_t)\}
    -
    \ons_{k,t+1}
    -
    \sum_{0\leq t_1\leq t-1}  
    h_{t,t_1,k}\diamond \qq^{k,t_1+1}\, ,
    \\
\nonumber
    \ons_{k,t+1} 
    &= 
    \sum_{t_1\leq t} 
    d_{t,t_1,k}\diamond f_{t_1-1}(\bV_{t_1-1})
%
\end{align}
%
Note that $\cP_t^{\parallel} (\bA^{(k)})=\E\lt[\bA^{(k)}|\cF_t\rt]$, where $\cF_t$ is the $\sigma$-algebra generated by 
$\{\bq ^{k,s}\}_{t_1\le t, k\le D}$. Equivalently, this is the conditional expectation of $\bA^{(k)}$ given the linear constraints
%
\begin{align}
%
\A^{(k)}\{f_{t_1}(\bV_{t_1})\}&=\by_{k,t_1+1}\, ,\;\;\;\;\;\mbox{ for } t_1\in \{0,\dots, t-1\}\, ,
%
\end{align}
%
Also notice that, by the induction hypothesis, and the definition of $\by_{k,t_1}$, \eqref{eq:Ydef1}, we have for all $t_1\le t$, 
%
\begin{equation}
\label{eq:Y-ons}
    \by_{k,t_1} \simeq  \qq^{k,t_1}+\ons_{k,t_1}\, .
%
\end{equation}
%
Lemma~\ref{lem:symregression} implies that $\cP_t^{\parallel} (\bA^{(k)})$ takes the form of \eqref{eq:SymmRegression} for a suitable matrix
$\hZZ_{k,t}\in\bbR^{N\times t}$. 
The key claim is that
%
\begin{equation}
\label{eq:ZequalsQ}
    \hZZ_{k,t} \simeq \bQ_t\, .
%
\end{equation}
%
In order to establish this claim, we show that,  under the inductive hypothesis,
\begin{equation}
\label{eq:1+TQ=Y}
    (\bfone+\cT_{k,t})\bQ_t\simeq \bY_{k,t}.
\end{equation}
Since $\cL_{k,t}=\bfone +\cT_{k,t}$ is well-conditioned by assumption, the combination of \eqref{eq:ZZ-eq} and \eqref{eq:1+TQ=Y} implies $\hZZ_{k,t}\simeq \bQ_t$. By \eqref{eq:Y-ons}, in order to prove \eqref{eq:1+TQ=Y}, it is sufficient to show that 
\begin{equation}
\label{eq:this-claim}
    \cT_t\bQ_t
    \simeq 
    \ONS_{k,t}
    \equiv 
    [\ons_{k,1}|\cdots|\ons_{k,t}].
\end{equation}

In order to prove \eqref{eq:this-claim}, we use Theorem~\ref{thm:SELAMP}. Recall that 
\begin{align*}
    C_{t_1,t_2,s} &= \E\{U^{k,t_1}_s U^{k,t_2}_s\}\,,
    \\
    W^{t_1}_s &= \sum_{2\leq k\leq D} U^{k,t_1}_s\,,
    \\
    \bC_{\le t} &= (C_{t_1,t_2,s})_{t_1,t_2\le t}\,.
\end{align*}
(The value $2\leq k\leq D$ is implicitly fixed in the definition of $\bC_{\leq t}$.) By Theorem~\ref{thm:SELAMP}, 
\[
    C_{t_1+1,t_2+1} \simeq \<\qq^{k,t_1+1},\qq^{k,t_2+1}\> 
    \simeq 
    (\bG_{\xi^{k,s},t}(\bV))_{t_1,t_2}
    ,
    \quad\forall~t_1,t_2\le t.
\]
This implies for any $0 \le t_1\le t-1$ and $s\in\sS$,
%
\begin{align}
\nonumber
    \sum_{t_2=0}^{t-1}
    (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_2}
    R\lt(\qq^{k,t_2+1},f_{t-1}(\VV_{t-1})\rt)_s
    &\simeq 
    \sum_{t_2=0}^{t-1} 
    (\bC_{\le t,s}^{-1})_{t_1+1,t_2+1} 
    \E\lt[
    U^{k,t_2+1}_s f_{t-1,s}(W^0,\dots,W^{t-1})
    \rt] 
    \\
\label{eq:Stein}
    &= 
    \E\lt[
        \frac{\partial f_{t-1,s}}{\partial W^{t_1+1}_s} (W^0_s,\dots,W^{t-1}_s)
    \rt] 
    \bfone_{t_1\le t-2}\, .
%
\end{align}
%
Indeed, Gaussian integration by parts yields the latter expression. Combining \eqref{eq:Stein} with the definition \eqref{eq:AMP-def2} will now allow us to conclude $\cT_{k,t}\bQ_t\simeq \ONS_{k,t}$ as desired. Indeed for each $s\in\sS$ we have
\begin{align*}
    \big[\cT_{k,t}\bQ_t\big]_{t,s} 
    &= 
    \sum_{t_1=0}^{t-1} 
    (\bGG_{\xi^{k,s},t-1})_{t_1,t-1} 
    \f_{t_1,s}
    \Big(
    \sum_{t_2=0}^{t-1} 
    (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_2} 
    \,
    R_s(\qq^{k,t_2+1}, \f_{t-1})
    \Big)
    \\
    &\simeq 
    \sum_{t_1=0}^{t-2}
    \zeta^{k,s}
    \big(
    \vR(\f_{t_1}, \f_{t-1})
    \big)
    \f_{t_1,s}
    \cdot 
    \E\lt[
    \frac{\partial f_{t-1,s}}{\partial W^{t_1+1}_s} 
    (W^0_s,\dots,W^{t-1}_s)
    \rt]
    \\
    &= 
    \ons_{k,t}.
\end{align*}
Having established \eqref{eq:ZequalsQ}, we now use the formula \eqref{eq:SymmRegression} for $\cP^{\parallel}_t(\bA^{(k)})=\E\big[\bA^{(k)}|\cF_t\big]$. The result is:
%
\begin{align}
\label{eq:AParallel}
    \cP^{\parallel}_{t}(\bA^{(k)})\{\f_t\}
    &\simeq 
    \sum_{t_1\leq t}
    \big(\alpha_{t_1}
    \diamond
    \qq^{k,t_1}
    +
    \beta_{t_1}
    \diamond
    \f_{t_1}\big)\, ;
    \\
\nonumber
     \alpha_{t_1,s}
     &= 
     \sum_{0\leq t_2\le t-1} 
     (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_2} 
     \,
     \xi^{k,s}\big(
     \vR(
     f_{t_2}(\VV_{t_2}),f_{t}(\VV_t)
     )
     \big)
     \, ,
     \\
\nonumber
    \beta_{t_1,s} 
    &= 
    \Big(
    \sum_{0\le t_2\leq t-1} 
    (\bG_{\xi^{k,s},t-1}^{-1})_{t_1,t_2} 
    \,
    R_s( \qq^{k,t_2},\f_{t} )
    \Big) 
    \zeta^{k,s}\big(
    \vR( \f_{t_1},\f_{t} )
    \big).
%
\end{align}
On the other hand, using again \eqref{eq:Stein} gives
%
\begin{align*}
    \sum_{t_1\leq t}\beta_{t_1}\diamond \f_{t_1} 
    &\simeq  
    \sum_{t_1 \le t-1}  d_{t,t_1,k}\diamond \f_{t_1-1}  
    =
    \ons_{k,t+1},
\\
    \sum_{t_1\leq t}
    \alpha_{t_1}
    \diamond
    \qq^{k,t_1} 
    &\simeq 
    \sum_{0\leq t_1\leq t-1}  
    h_{t,t_1,k}
    \diamond
    \qq^{k,t_1+1}.
\end{align*}
%
We conclude from \eqref{eq:AMP-LAMP} that $\|\AMP_{t+1}(\bQ_{t})_k -\LAMP_{t+1}(\bQ_{t})_k  \|_N\simeq 0$. This concludes the proof.
\end{proof}

