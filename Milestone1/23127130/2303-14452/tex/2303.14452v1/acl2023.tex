% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


%%%%%%%


\usepackage{microtype}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{array}
\newcolumntype{L}{>{$}l<{$}}
\newcolumntype{C}{>{$}c<{$}}
\newcolumntype{R}{>{$}r<{$}}
\newcommand{\nm}[1]{\textnormal{#1}}


\newcommand{\model}[1]{{\texttt{COFFEE}}}


\newcommand{\ys}[1]{{\color{blue}{\small\bf\sf [Yixuan: #1]}}}
\newcommand{\fzh}[1]{{\color{orange}{\small\bf\sf [FZH: #1]}}}
\newcommand{\mz}[1]{{\color{teal}{\small\bf\sf [MZ: #1]}}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \usepackage{times}
% \usepackage{latexsym}
% \renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
% \usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

% Standard package includes
% \usepackage{times}
% \usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{multirow}
\usepackage{array, tabularx, caption, boldline}
\usepackage{graphicx}
\usepackage{cellspace}
\usepackage{algpseudocode}  
\usepackage{amsmath}
\usepackage{multicol}  
\usepackage{multirow} 
\usepackage{flexisym}
\usepackage{graphicx}  %Required
\usepackage{array}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%\newcommand\BibTeX{B\textsc{ib}\TeX}
\usepackage{makecell}
% Standard package includes
\usepackage{latexsym}
%\usepackage[table]{xcolor}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
%\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
%\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{array, tabularx, caption, boldline}
\usepackage{graphicx}
\usepackage{cellspace}
\usepackage{algpseudocode}  
\usepackage{amsmath}
\usepackage{multicol}  
\usepackage{multirow} 
\usepackage{flexisym}
\usepackage{graphicx}  %Required
\usepackage{array}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{tabularx}
\makeatletter
\def\hlinewd#1{%
\noalign{\ifnum0=`}\fi\hrule \@height #1 %
\futurelet\reserved@a\@xhline}
\makeatother


%\usepackage{algorithm}  
\usepackage{algpseudocode}  
\usepackage{amsmath}
\usepackage{multicol}  
\usepackage{multirow} 
\usepackage{graphicx}  %Required
\usepackage{array}
%\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash}m{#1}}
%\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{textcomp}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{xcolor,colortbl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%%%% End TACL-instructions-specific macro block
%%%%




\title{\model{}: A Contrastive Oracle-Free Framework for Event Extraction}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Meiru Zhang \\
  \texttt{mz468@cam.ac.uk} \\\And
  Yixuan Su \\
  University of Cambridge \\
  \texttt{ys484@cam.ac.uk} \\\And 
  Zaiqiao Meng \\
  University of Glasgow \\
  \texttt{zaiqiao.meng.glasgow.ac.uk} \\\And
  Zihao Fu \\
  University of Cambridge \\
  \texttt{zf268@cam.ac.uk} \\\And 
  Nigel Collier \\
  University of Cambridge \\
  \texttt{nhc30@cam.ac.uk} \\}

\author{Meiru Zhang$^{\spadesuit}$\ \ \ \  \textbf{Yixuan Su}$^{\spadesuit}$\ \ \ \   \textbf{Zaiqiao Meng}$^{\spadesuit \diamondsuit}$\\ \textbf{Zihao Fu}$^\spadesuit$\ \ \ \  \textbf{Nigel Collier}$^\spadesuit$ \\
$^\spadesuit$Language Technology Lab, University of Cambridge \\
$^\diamondsuit$Department of Computing Science, University of Glasgow \\
 \texttt{$^\spadesuit$\{mz468, ys484, zf268, nhc30\}@cam.ac.uk} \\
 \texttt{$^\diamondsuit$zaiqiao.meng@glasgow.ac.uk}
 }


\begin{document}
\maketitle
\begin{abstract}
Event extraction is a complex information extraction task that involves extracting events from unstructured text. Prior classification-based methods require comprehensive entity annotations for joint training, while newer generation-based methods rely on heuristic templates containing oracle information such as event type, which is often unavailable in real-world scenarios. In this study, we consider a more realistic setting of this task, namely the Oracle-Free Event Extraction (OFEE) task, where only the input context is given without any oracle information, including event type, event ontology and trigger word. To solve this task, we propose a new framework, called \model{}, which extracts the events solely based on the document context without referring to any oracle information. In particular, a contrastive selection model is introduced in \model{} to rectify the generated triggers and handle multi-event instances. The proposed \model{} outperforms state-of-the-art approaches under the oracle-free setting of the event extraction task, as evaluated on a public event extraction benchmark ACE05.

\end{abstract}

\section{Introduction}

% Paragraph 1, introduce the task and its significance/approach categorization (classification/generative) 
% cite on event extraction description
Event extraction task aims to identify events and their arguments from the given textual input context~\cite{nguyen2016joint, wadden2019entity, yang2019exploring}. Conventionally, this task can be decomposed into four sub-tasks~\cite{nguyen2016joint}: (i) detecting the trigger word that most directly describes the event; (ii) event type classification for defining its event-specific attributes; (iii) argument identification and (iv) argument classification that maps the argument entities to the corresponding role attributes based on the event structure of each event type, namely event schema. For instance, Figure \ref{fig:example} shows the input context of an event extraction example that contains two events: a \texttt{TRANSPORT} event triggered by the 
trigger word `went' and an \texttt{ATTACK} event triggered by the trigger word `killed', where \texttt{TRANSPORT} and \texttt{ATTACK} are two event types. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/example.pdf}
    \caption{An event extraction example that contains two events: \texttt{TRANSPORT} and \texttt{ATTACK}. In the \texttt{TRANSPORT} event, `\underline{\textit{\textcolor{blue}{went}}}' is the trigger word, and `home' is the `Destination' argument. In the \texttt{ATTACK} event, `\underline{\textit{\textcolor{teal}{killed}}}' is the trigger word while `father-in-law' and `home' are the `Agent' and `Place' arguments respectively.}
    \label{fig:example}
\end{figure}

% Paragraph 2, state that one of the challenges in this task, which is a pipeline based approach over three sub-tasks (trigger, event type and argument classification), which usually need to involve some addition oracle information (i.e. event type). Exemplify some existing approaches that use some types of oracle information (i.e. DEGREE that use event type). Also state the maybe not a realistic setting, as event type may not easy to determine.

% 强调motivation: fully end-to-end without oracle
 Many prior studies formulate event extraction as a token-level classification problem that extract event triggers and arguments using sequence tagging models based on tailor-designed neural networks~\citep{nguyen2016joint,liu2018gru, li2019biomedical,yang2019exploring, wadden2019entity, huang2020biomedical,lin2020joint,van2021cross}. However, such methods cannot leverage rich label semantics as the target outputs are represented in class numbers. Recently, with advances in generative pre-trained language models, several generation-based approaches~\citep{hsu2022degree, huang2022multilingual, li2021document, zhang2021contrastive} have been applied to solve this task, which transform event extraction task into a conditional generation task. By utilizing the autoregressive generation nature of generative pre-trained language models (e.g. BART-Gen \citep{li2021document}, DEGREE \citep{hsu2022degree}) and some manual prompts, it becomes possible to harness the semantics of labels and conduct both entity extraction and classification in an autoregressive manner simultaneously.%The encoder-decoder based structure can leverage the label semantics and perform extraction and classification of entities simultaneously in an auto-regressive manner.

% Paragraph 3, state our task setting and objectives, and why our setting is more realistic.
% why generation-based methods were proposed -> existing drawbacks

% explain generative approach (one-sentence)
% drawbacks of generative approach --- heuristic, expensive, low generalization ability (domain specific), 
While impressive results are reported, we identify two major limitations of the current generation-based event extraction methods. Firstly, most of these methods rely on 
heuristic templates and extensive human knowledge engineering. According to the experiments conducted by \cite{hsu2022degree}, a slight change in the template might lead to significant performance changes, thus raises the issue of using sub-optimal templates. Secondly, most of these generation-based approaches still rely on certain oracle information, such as event type and event schema, which requires extensive manual annotations. For example, the inference process of the DEGREE model \citep{hsu2022degree} needs manually designed event-specific templates to be provided for each example and iterate over all event types. On the other hand, Text2Event \citep{lu2021text2event} also constrains the generation with manually designed templates, which still require event schema to be given. However, it is unrealistic for a real inference system to automatically obtain these oracle information (e.g. event type and event schema in the above examples). Hence, in this paper, we solve the Oracle-Free Event Extraction (OFEE) task where only the input context is given.

%Secondly, the event-specific templates limits the generalization of these models as the template is domain-specific. Transferring patterns from one domain to another requires a lot of work and is highly dependent on the expression form of text.

% Paragraph 4, how do we address the task (generative approach+re-ranking), what are the basic results and conclusions.
In this study, we propose a novel \textbf{C}ontrastive \textbf{O}racle-\textbf{F}ree \textbf{F}ramework for \textbf{E}vent \textbf{E}xtraction (\model{}), which addresses the event extraction task without using any oracle information. Our \model{} consists of two parts, namely, a generator that performs the extraction of events and a selector that aims to refine the generated results. The generator of our \model{} generates both the trigger word candidates and event arguments, where the shared generator allows for cross-task knowledge sharing between these sub-tasks. The selector of our \model{} learns to re-rank and select the trigger word candidates to obtain more accurate event trigger words, which is inspired by \citep{su2021few}. One challenge of sentence-level event extraction is that a sentence may contain different types of event records (e.g. the example in Figure \ref{fig:example}), and specific event templates can help the model to identify and extract events in a targeted manner. Prior approaches to tackling this challenge have necessitated either multi-label tagging \citep{ramponi2020biomedical, lin2020joint}, event-specific templates \citep{hsu2022degree}, or multi-turn generation techniques \citep{du2020qaevent,li2020qaevent}. In contrast, our proposed model can concurrently generate and select multiple event candidates, encompassing both the event trigger and its associated type, thereby effectively addressing the aforementioned challenge.

%\fzh{How our coffee model can solve the oracle or template problem. We should explicitly show that our specific design can somehow alleviate the oracle or template problems mentioned in paragraph 3.} 

% Paragraph 5, list contributions.
The contribution of this work is as follows:
\begin{itemize}
    \item We highlight the challenge of the current event extraction task setting and introduce the oracle-free setting of this task that requires the model to produce the structural event without using oracle information beyond the context.
    \item We propose \model{}, a novel \textbf{C}ontrastive \textbf{O}racle-\textbf{F}ree \textbf{F}ramework for \textbf{E}vent \textbf{E}xtraction which generates structural event information from context without using any oracle information.
    \item We conduct experiments on the Automatic Content Extraction (ACE) corpus under an oracle-free setting and showed that the template-based methods heavily depend on the extra oracle information. Our \model{} model empirically outperforms other methods under the oracle-free setting.
\end{itemize}

\begin{figure*}
\centering
  \includegraphics[width=1.0\linewidth]{figures/overview.png}
  \caption{Overview of our proposed \model{} framework.}
  \label{fig:framework}
\end{figure*}


\section{Task Definition}
Conventionally, the event extraction task entails the following terminologies \cite{nguyen2016joint, liu2020qaevent, paolini2021structure}.
\begin{itemize}
    \item \emph{Input Context}: The input document consisting multiple sentences within which one or multiple events are described. Let $\textbf{t}$ denotes the given input context, which is a sequence of tokens $\{t_1, t_2, \cdots, t_n\}$.
    \item \emph{Trigger Word}: The main word that most clearly expresses the occurrence of an event (e.g. words `went' and `killed' in Figure \ref{fig:example}).
    \item \emph{Event Type}: Event type defines the semantic structure of a particular event. Note that one single trigger word might associate with multiple different event types. (e.g. \texttt{TRANSPORT} and \texttt{ATTACK} in Figure \ref{fig:example})
    \item \emph{Event Argument}: Event arguments identify entities involved in the events and the roles of these entities based on their relationship with event triggers. (e.g. `home' is the Place argument of the \texttt{ATTACK} event and Destination argument of the \texttt{TRANSPORT} event in Figure \ref{fig:example})
\end{itemize}
Given the input context, the event extraction task aims to identify and classify the event triggers and the arguments associated with the event. Although argument roles are event specific, in a realistic scenario, either the gold trigger words or event type information should not be given during the event arguments extraction. However, as we discussed earlier, most of the existing generation-based approaches address this task by providing some manual prompts (e.g., trigger words or event types) to reduce the difficulty of this task. To offer a more practical scenario for this task, we focus on the Oracle-Free Event Extraction (OFEE) task, providing only the input context during inference. In this task, our goal is to infer the event type and arguments directly without relying on any pre-defined triggers or types. This task is a much more challenging task than traditional event extraction.


% oracle-free framework event-extraction
\section{Methodology}\label{sec:model}
% The key distinction is that our study focuses on a more challenging and realistic environment: only context  is provided. In this configuration, the context  serves as the input to \model{}, and its final outputs are the elements of all events mentioned in the context . In contrast to other generation-based works, event description, manually generated template, and set of trigger example are not required.

% Place figures and tables in the paper near where they are first discussed.
% Note that MIT Press disallows figures and tables on the first page.
To address the OFEE task, we propose \model{}, which is a novel contrastive framework for event extraction without using any oracle information. As shown in Figure \ref{fig:framework}, our \model{} framework contains two main components namely, \texttt{Generator} and \texttt{Selector}. The \texttt{generator} in our framework can be any decoder-based transformer models, such as \citep{lewis2020bart, raffel2020exploring, xue2021mt5}. Previous research \citep{cohen2019beam} has shown that the beam search does not always generate the best ones, the scores of generated candidates might not be a good ranking indicator for the event triggers. Therefore, our \model{} trains a \texttt{selector}, which is an encoder-based ranking model (Section \ref{subsec:selector}), to re-rank the generated trigger word candidates. \model{} can be separated into two stages, namely \emph{Trigger Word Prediction} and \emph{Argument Prediction}. Both the \texttt{generator} and \texttt{selector} are finetuned during the trigger word prediction stage, and the \texttt{generator} will be shared and finetuned further in the arguments extraction stage. In particular, the \texttt{generator} will be trained to generate trigger word candidates given the input context, and then the \texttt{selector} is trained with the trigger word candidates generated by the \texttt{generator} for further re-ranking. During the argument prediction stage, the top trigger words after re-ranking are selected and forwarded into the trained \texttt{generator} to generate event arguments. The details of our proposed \texttt{generator} and \texttt{selector} are as follows.

% share information among different event types
\subsection{Generator}
\label{subsec:generator}
The generator $G$ of \model{} is trained for both the trigger word prediction and argument prediction. In this study, T5-base model \citep{raffel2020exploring} is employed as the backbone model for generator. For the trigger word prediction, the generator takes the context as input, $\textbf{x}^{trg}=\textbf{t}=\{t_1, t_2, \cdots, t_n\}$, and outputs trigger word and event type $\textbf{y}^{trg}_i$ for the $i$-th event in the context. If there are multiple events involved in the context, the same context will be inputted but target outputs are different event candidates. The generator is trained to learn that there could be multiple target output for the same context. 

For the argument prediction of $i$-th event, the trigger word is concatenated with the input context $\textbf{x}^{arg}_i=[\textbf{t}:\textbf{y}^{trg}_i]=\{t_1, t_2, \cdots, t_n, \textbf{y}^{trg}_i\}$ and ground truth output is a string of entities with their argument roles $\textbf{y}_i^{arg}=\{a_i^1, a_i^2, \cdots, a_i^m\}$, where the $a_i^j$ is the candidate argument for the $j$-th role type in the $i$-th target event. In line with X-GEAR \cite{huang2022multilingual}, we insert placeholders before and after the target entity to be generated, as illustrated in the left half of the Figure \ref{fig:framework}. To address the challenge of multiple arguments within a single role, we separate the entities within a placeholder pair using the delimiter $[and]$. 

During the inference stage, we apply beam search to generate a list of trigger outputs for each input context, along with their corresponding beam scores, which are later utilized for re-ranking. In other words, given context $\textbf{t}$, the generator outputs a list of triggers $C=\{\hat{\textbf{y}}^{trg}_{1}, \hat{\textbf{y}}^{trg}_{2},\cdots, \hat{\textbf{y}}^{trg}_{l}\}$ with the beam score $\{b_1, b_2, \cdots, b_l\}$, where $l$ is the number of candidates generated, and $\hat{\textbf{y}}^{trg}{i}$ represents the predicted trigger candidate for an event within the input context $\textbf{t}$. A \texttt{selector} is employed to further refine the trigger candidates and obtain the final trigger predictions.

%%
% The model maximizes the conditional probability of generating target sequence $\textbf{y}=\{y_1, y_2, \cdots, y_m\}$:
% \begin{equation}\label{eq:generator}
%     P (\textbf{y}|\textbf{x}) = \prod^m_{i=1} p(y_i|y_1,y_2, \cdots,y_{i-1}, \textbf{x}).
% \end{equation}
%%

\subsection{Selector}
\label{subsec:selector}
The selector model $f$ takes the generated trigger-event candidates $C$ and original context $t$ as input and predicts a ranking score for each candidate $\hat{\textbf{y}}^{trg}_{i}$ in $C$. During training, the ground truth trigger-events $\textbf{y}^{trg}_{i}$ are used as the positive anchors and negative samples are the other incorrect candidates generated. The selector $f$ contrastively learns to separate the embeddings of positive and negative samples apart. The selector will re-rank the candidates based on the pair-wise similarity between each candidate. Instead of using direct measurements such as cosine similarity, the similarity score is defined as $f(t,\hat{\textbf{y}}^{trg}_{i})$. The concatenation of context and candidates allows the model to capture the semantic relevance between context and trigger candidates. We select the final $|K|$ triggers $\tilde{K}=\{\tilde{\textbf{y}}^{trg}_{1} \tilde{\textbf{y}}^{trg}_{2}, \cdots, \tilde{\textbf{y}}^{trg}_{|K|}\}$, which satisfies that $\forall \tilde{\textbf{y}}^{trg}_{i}$,
\begin{equation}
\small
\alpha\cdot\sigma(f(t, \tilde{\textbf{y}}^{trg}_{i}))+(1-\alpha)\cdot\sigma (b_i) > thr, 
\end{equation}
where $thr$ and $\alpha$ are hyper-parameters specifying the threshold for filtering triggers and the weight for combining the beam score with the ranking score, respectively; $\sigma$ denotes the softmax function; $b_i$ represents the beam score of the candidate.

The RoBERTa model \cite{liu2019roberta} is employed to construct the trigger candidate selector. The score $f(t, \hat{\textbf{y}}^{trg}_{i})$ is computed using a linear layer that accepts as input the average embeddings of $RoBERTa([t:\hat{\textbf{y}}^{trg}_{i}])$ and outputs a floating-point number. The selector is trained to minimize the hinge loss, defined as
\begin{equation}
    \small
    L_f = \sum_{j=1}^k \max\{0, 1 - f (t, \textbf{y}^{trg}_j) + f(t, \hat{\textbf{y}}^{trg}_{j})\},
\end{equation}
where $\hat{\textbf{y}}^{trg}_{j} \in C, \hat{\textbf{y}}^{trg}_{j} \not\in \textbf{y}^{trg}$ and $k$ represents the number of negatives sampled from $C$. By considering the implicit correlation between the context and generated candidates, the selector enhances trigger extraction and positively impacts the performance of argument extraction.

Upon training $f$ on the training set and fine-tuning hyper-parameters on the development set, the final predicted trigger-event candidates $K$ in the test set can be inferred. The final $K$ trigger words $\tilde{\textbf{y}}^{trg}_i$ chosen by the selector are then concatenated with the context for argument extraction. Specifically, given the input $\tilde{x}^{arg}_i=[\textbf{t}:\tilde{\textbf{y}}^{trg}_i]=\{t_1, t_2, \cdots, t_n, \tilde{\textbf{y}}^{trg}_i\}$, the \texttt{generator} $G$ will generate $\tilde{\textbf{y}}_i^{arg}=\{\tilde{a}_i^1, \tilde{a}_i^2, \cdots, \tilde{a}_i^m\}$ for the $i$-th predicted event.


\section{Experiments}
\subsection{Dataset}\label{sec:dataset}
In this work, we evaluate our \model{} based on a public event extraction benchmark ACE05~\cite{walker2005ace}, which consists of 599 English documents, 33 event types and 22 argument roles. The split and pre-processing of this dataset follows the previous work \citep{wadden2019entity} and \citep{lin2020joint}, resulting in two variants \textbf{ACE05-E} and \textbf{ACE05-E+}. Detailed split and statistics of this benchmark can be found in Figure \ref{ACE05dataset}. 

\begin{table}
\centering
\resizebox{.47\textwidth}{!}{
\begin{tabular}{lllllll}
\toprule
\small
&&\multicolumn{2}{c}{\textbf{ACE05-E}}&\multicolumn{2}{c}{\textbf{ACE05-E+}}\\
\cmidrule(lr){3-4}
\cmidrule(lr){5-6}
 & \# sent & \# triggers & \# args & \# triggers & \# args \\
\hline

train & 17172 & 4202 & 4859 & 4419 & 6607 \\
val & 923 & 450 & 605 & 468 & 759 \\
test & 832 & 403 & 576 & 424 & 689 \\
\bottomrule
\end{tabular}
}
\caption{\label{ACE05dataset}The statistics of our used datasets.}
\end{table}

\subsection{Evaluation Metrics}
The assessment of trigger identification, event type classification, argument identification, and argument role classification tasks employs F1-score, following previous studies~\cite{zhang2019extracting, wadden2019entity}. For a trigger classification prediction to be deemed correct, the trigger word and event type must be accurately predicted. Correct identification of arguments necessitates accurate prediction of both the event type and argument entity. A correct argument role classification requires both correct argument identification and correct prediction of the role type. Specifically, a predicted event type $\tilde{e}$ and argument $\tilde{a}$ of role type $\tilde{r}$ are considered correct when $(\tilde{a}, \tilde{r}, \tilde{e}) = (a, r, e)$.

\subsection{Baselines}\label{sec:baseline}
To validate the effectiveness of our proposed method, we compared our \model{} with the following baselines: 
\begin{itemize}
    \item  \textbf{OneIE} \cite{lin2020joint} is a joint neural model that simultaneously extracts entities and relations using a dynamic relation graph.
    \item \textbf{Text2Event} \cite{lu2021text2event} is a sequence-to-structure controlled generation model with constrained decoding for event extraction. It focuses on the structured generation and can generate event schema directly to form event records.
    \item \textbf{BART-Gen} \cite{li2021document} is designed for document-level event extraction that can deal with the long-distance dependence issue and co-reference problem. Constrained generation is applied for argument extraction that requires event-specific templates.
    \item \textbf{DEGREE} \cite{hsu2022degree} is a generative event extraction approach that highly relies on the designed template. This approach provides both pipelined implementation and an end-to-end approach.
    \item \textbf{TANL} \cite{paolini2021structure} is a model that embeds target output into the context sentence, and the model achieves the entity mention extraction by so-called augmented translation. 
\end{itemize}

\subsection{Implementation}
The implementation details, including the model architecture, hyperparameters, and training procedures, can be found in the Appendix section. We provide a comprehensive overview of these aspects to ensure reproducibility and facilitate future research based on our work. Please refer to the Appendix for a thorough understanding of our model's implementation.

\section{Results}
% extractive / classification based
% ONE-IE
\begin{table*}[t]
    \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
	 \resizebox{.95\textwidth}{!}{
	\begin{tabular}{clcccccccc}
		\hlinewd{0.75pt}
	    %&&&&&&&&&\\
	    &\multirow{2}{*}{\textbf{Model}}&\multicolumn{4}{c}{ACE-05E}&\multicolumn{4}{c}{ACE-05E+}\\
	    \cmidrule(lr){3-6}
	    \cmidrule(lr){7-10}
	    &&Trig I&Trig C&Arg I&Arg C&Trig I&Trig C&Arg I&Arg C\\\hline
            &OneIE&76.83&73.05&57.26&54.31&77.31&74.01&56.66&54.29\\\hline
	    &Text2Event$^\natural$ &-&69.2&-&49.8&-&71.8&-&54.4\\
	    &BARTGen$^\natural$ &74.36&71.13&55.22&53.71&-&-&66.62&64.28\\
	    &DEGREE$^{\natural\diamondsuit\flat}$ &74.57&70.96&56.03&53.41&74.90&70.30&55.74&53.61\\
	    \hlinewd{0.75pt}
	    \multirow{6}{*}{\rotatebox[origin=c]{90}{Oracle Free}}&Text2Event &73.49&68.60&51.24&49.32&72.73&68.30&52.48&50.35\\
	    &BARTGen &70.96&66.59&48.47&46.36&-&-&51.43&47.49\\
	    &DEGREE &43.64&2.18$^\dagger$&28.54&25.99&54.32&2.26$^\dagger$&30.09&28.79\\
	    &TANL&\textbf{81.10}&\textbf{77.09}&55.28&52.16&\textbf{80.28}&\textbf{76.03}&54.56&52.57\\
	    \cline{2-10}
	    &\textbf{COFFEE}&\underline{79.61}&\underline{75.73}&\textbf{59.88}&\textbf{55.43}&\underline{78.28}&\underline{74.70}&\underline{56.87}&\underline{54.11}\\
	    &\makecell[c]{\textit{+TANL}}&\textbf{81.10}&\textbf{77.09}&\underline{58.74}&\underline{55.24}&\textbf{80.28}&\textbf{76.03}&\textbf{59.78}&\textbf{57.06}\\
		\hlinewd{0.75pt}
	\end{tabular}
	}
    \caption{Performance comparison between \model{} and SOTA generation-based approaches in OFEE setting. $^\dagger$ The trigger classification F1 of DEGREE is nearly zero because the model cannot exclude the negative samples constructed without template.$^\natural$, $^\diamondsuit$, and $^\flat$ denote the model requires manually designed template, example keywords, and event description, respectively. The highest results are in \textbf{bold} and the second highest results are \underline{underlined}}
    	\vspace{-1.5mm}
\label{tb:mainresult}
\end{table*}


\subsection{OFEE performance}
% describe the table content
As described in Section \ref{sec:baseline}, several baseline approaches employ distinct template formats. To compare the performance of our \model{} framework with these methods under the OFEE setting, we have implemented the following adaptations to these baseline approaches:
\begin{itemize}
    \item \textbf{Text2Event} relies on a complex constrained decoding mechanism that depends on the event schema. For the oracle-free settings, we utilize the default decoding of the T5 model to generate results.
    \item \textbf{BART-Gen} adopts a constrained generation mechanism, which necessitates the use of templates. We removed the template and the constrained-decoding, thereby enabling the model to function. The trigger extraction performance of BART-Gen is not reported in our study due to an implementation error stemming from different preprocessing methods, which prevented us from applying this approach to the ACE-05E+ dataset. Consequently, we depend on the ground truth triggers for argument extraction in this instance.
    \item \textbf{DEGREE} model is designed to generate 'invalid' instances during both the training and inference phases, wherein event-specific knowledge is combined with context even if no such event is mentioned in the context. We eliminated these event-specific templates, leaving only the context sentence as input.
\end{itemize}

As presented in Table \ref{tb:mainresult}, we compare methods based on F1 scores for the four sub-tasks described in \ref{sec:dataset}, which include trigger identification, trigger classification, argument identification, and argument classification. We observe the following:

\begin{itemize}
    \item Firstly, it is essential to emphasize that the oracle-free setting represents a more challenging scenario. Upon removal of all oracle information, generation-based baselines relying on templates exhibit a varied degree of performance decline on both datasets. DEGREE is unable to filter out the 'invalid' events, leading to an almost zero trigger classification precision. This reveals that the information leaked in the template contributes to the performance of these models.
    \item Our proposed \model{} surpasses the classification-based approach OneIE and the generation-based approaches Text2Event, BARTGen, and DEGREE in both the presence and absence of oracle information across all four metrics. This demonstrates that our \model{} can effectively leverage the input context to extract event frames.
    \item In comparison to TANL, our \model{} achieves competitive results in trigger extraction. One potential explanation for TANL's higher performance is the difference in model settings. Specifically, we employ a threshold-based method to determine the number of events in each context. Although this approach may introduce slightly more errors, our model possesses robust argument extraction capabilities and attains superior performance in argument extraction with these extracted triggers. Moreover, our framework can be integrated with other methods to further enhance extraction performance. We conducted additional experiments to evaluate the argument extraction results using triggers extracted from TANL and discovered that our generator outperformed TANL. These findings corroborate the effectiveness of the shared \texttt{generator}.

\end{itemize}
    %\item The results show a considerable performance gap between our model and these generation baselines, with our model achieving superior performance in argument extraction, even when none of the extra knowledge is employed.

\subsection{Ablation study}

We conducted an ablation study on the threshold and weight parameters to demonstrate the effectiveness of our \emph{selector} and the influence of \emph{threshold} and \emph{weight} parameters in \model{}.

\begin{figure}[thb]
\centering
\includegraphics[width=\columnwidth]{figures/ace05_t5_roberta_threshold.pdf}
\caption{Effect of threshold in \model{} framework. Colors indicate the sub-task evaluation metric.}
\label{fig:ablation_threshold}
\end{figure}

The \emph{threshold} serves as a benchmark for the candidate selection process. As the \emph{threshold} increases, fewer candidates are selected, but with a higher degree of accuracy. Conversely, a too high \emph{threshold} could filter out some of the correct candidates, resulting in a performance decrease. Figure \ref{fig:ablation_threshold} shows that the optimal \emph{threshold} value is 0.2, which results in the best performance for the system.

\begin{figure}[thb]
\centering
\includegraphics[width=\columnwidth]{figures/ace05_t5_roberta_weight.pdf}
\caption{The influence of the weight $\alpha$ on performance. Colors indicate the sub-task evaluation metric.}
\label{fig:ablation_weight}
\end{figure}

The \emph{weight} represents the ratio of combining ranking score and generation score. When the weight is set to \emph{0}, the ranking score is entirely disregarded, and the generation score becomes the sole factor in candidate selection. In contrast, a weight of \emph{1} signifies that the ranking score is given full consideration, while the generation score is overlooked. As depicted in Figure \ref{fig:ablation_weight}, with a fixed threshold, an optimal weight ($\alpha = 0.4$) exists, which yields the best extraction performance. The initial improvement in F1 score with increasing weight suggests that the ranking score can effectively refine the results of the beam search. However, as the weight continues to increase, the ranking scores themselves exhibit significant variation, leading to a corresponding fluctuation in the probability formed by the softmax function. As the final probability becomes increasingly reliant on the ranker probability, fewer candidates are selected at the same threshold, resulting in a decline in performance.

% \subsection{Error Analysis}
% Error analysis 
% • Missing role: when the model cannot extract an event due to errors that occur before the event layer. 
% • Missing trigger: when the model cannot extract an event because it cannot detect the trigger. 
% • Incorrect event class: when the textual span of a trigger is correctly detected but classified to a wrong class. 
% • Missing entity: similar to ‘missing trigger’, we count the number of missing events due to undetected entity. 
% • Missing argument: when an event trigger is correctly detected, but its arguments are missing. 
% F1 on subset of documents with E events?


\subsection{Qualitative Case Analysis}
\begin{table*}[t]
    \centering  % 表居中
    \renewcommand{\arraystretch}{1.2}
\resizebox{.95\textwidth}{!}{
\begin{tabular}{p{0.2\textwidth}|p{0.4\textwidth}|p{0.6\textwidth}}
\toprule
  \multicolumn{3}{c}{\textbf{Example 1}}\\\hline
  \textbf{Context} & \multicolumn{2}{p{\textwidth}}{Kommersant business daily joined in , declaring in a furious front - page headline : " The United States is demanding that Russia , France and Germany pay for the Iraqi war .} \\\hline
  \multirow[t]{2}{*}{\textbf{Reference}} 
  & \textbf{E1}: [Transaction_Transfer-Money] pay  & \textbf{Args}: [Giver] Germany \\
  & \textbf{E2}: [Conflict_Attack] war & \textbf{Args}: [Place] Iraqi \\\hline
  \multirow[t]{2}{*}{\textbf{TANL + COFFEE}} 
  & \textbf{E1}: & \textbf{Args}: \\
  & \textbf{E2}: [Conflict_Attack] war & \textbf{Args}: [Place] Iraq \\\hdashline
  \multirow[t]{2}{*}{\textbf{\model{} w/o Ranker}} 
  & \textbf{E1}:  & \textbf{Args}: \\
  & \textbf{E2}: [Conflict_Attack] war & \textbf{Args}: [Place] Iraq \\\hdashline
  \multirow[t]{2}{*}{\textbf{\model{}}} 
  &\colorbox{pink}{ \textbf{E1}: [Transaction_Transfer-Money] pay}  & \colorbox{pink}{\textbf{Args}: [Giver] Germany} \\
  & \textbf{E2}: [Conflict_Attack] war & \textbf{Args}: [Place] Iraq \\

  \midrule\midrule
  \multicolumn{3}{c}{\textbf{Example 2}}\\\hline
  \textbf{Context} & \multicolumn{2}{p{\textwidth}}{Welch specifically is seeking performance evaluations , correspondence between his estranged wife and partners while she worked at the law firm 's office in London , and documents related to her prospects of becoming a partner .} \\\hline
  \multirow[t]{2}{*}{\textbf{Reference}} 
  & \textbf{E1}: [Contact_Phone-Write] correspondence & \textbf{Args}: [Entity] partners; [Place] office \\
  & \textbf{E2}: [Personnel_Start-Position] becoming & \textbf{Args}: [Entity] firm \\\hline
  \multirow[t]{2}{*}{\textbf{TANL + COFFEE}}
  & \textbf{E1}: [Contact_Phone-Write] correspondence & \textbf{Args}: [Entity] partners; [Place] office \\
  & \textbf{E2}: & \textbf{Args}: \\\hdashline  
  \multirow[t]{2}{*}{\textbf{\model{} w/o Ranker}} 
  & \textbf{E1}: & \textbf{Args}: \\
  & \textbf{E2}: [Personnel_Start-Position] becoming & \textbf{Args}: \\\hdashline
  \multirow[t]{2}{*}{\textbf{\model{}}} 
  & \textbf{E1}: [Contact_Phone-Write] correspondence & \textbf{Args}: [Entity] partners; [Place] office \\
  & \colorbox{pink}{\textbf{E2}: [Personnel_Start-Position] becoming} & \textbf{Args}: \\

\bottomrule
\end{tabular}
}
\caption{Event extraction examples from the test set using \model{}, \model{} without ranking and TANL+\model{}. The trigger and arguments that are missed in baseline results but captured by \model{} are \colorbox{pink}{highlighted}. It is evident that \model{} is generally more effective in detecting the events.}


\label{tb:case_study}
\end{table*}

In order to demonstrate the ability of our model to select event candidates, we analyzed the results of two instances selected from the test set. To provide a baseline for comparison, we selected \model{} without ranking and TANL due to its high performance. As shown in Table \ref{tb:case_study}, our proposed model successfully extracted the missing events that were not detected by the baselines, and the re-ranking mechanism enabled the model to select more accurate candidates.

Specifically, only \model{} successfully predicted all the events within the context. In Example 1, both TANL and \model{} without ranking failed to extract \textbf{E1}, which is triggered by "pay". This suggests that the baselines may have difficulty in recognizing complex event triggers. In this case, there is not a specific amount of money to be paid, but a mentioning of cost. In Example 2, TANL failed to extract \textbf{E2}, which is triggered by "becoming", and \model{} without ranking failed to extract \textbf{E1}. This highlights the inability of the baselines to consistently identify events and their corresponding arguments. In contrast, our \model{} successfully identified the events and extracted the target arguments, showcasing its superior performance.

Comparing \model{} with and without ranking, we can conclude that re-ranking in the selector is crucial. In both examples, \model{} without ranking failed to detect all events. This is because the beam search score of the candidates differs significantly even though both candidates are the correct target. The re-ranking can increase the probability of the second candidate and thus be selected under the selected threshold. This demonstrates that the integration of a ranker in the selector stage not only improves the accuracy of event detection but also allows for better adaptation to various instances.

Overall, these examples demonstrate the improvement in event extraction offered by our \emph{selector}. It allows the framework to re-rank and select the correct event candidates for multi-event instances, outperforming the baselines and establishing our model as a more effective and reliable solution for event extraction tasks.

% explain why
% from the passage perspective?
% from the data perspective?
% check the results on single events


\section{Related Work}

\subsection{Event Extraction}
% pipelined classifiers
% joint inference
% neural networks
% The most related work to our appraoch is that, 
% another related work
% there does exist some prior work on xxx
% 

Early event extraction research primarily relied on rule-based methods involving hand-written patterns to identify event triggers and arguments in text \citep{li2013joint, kai2015improving}. Supervised machine learning techniques became popular, with various feature-based classification models employed \citep{hsi2016leveraging}. However, these methods faced limitations due to manual feature engineering and the need for large annotated datasets. Researchers then turned to deep learning approaches, utilizing convolutional neural networks (CNNs) \citep{chen2015event, nguyen2015event, bjorne2018biomedical, yang2019exploring}, recurrent neural networks (RNNs) \citep{nguyen2016joint}, and Tree-LSTM \citep{li2019biomedical} for event extraction, which automatically learned relevant features and improved performance.

The introduction of pretrained language models revolutionized event extraction. Fine-tuning these models achieved state-of-the-art performance across various benchmarks \citep{lin2020joint, ramponi2020biomedical, wadden2019entity, yang2021document}. These models captured deep contextual information and benefited from knowledge transfer, enhancing performance with limited annotated data. Some studies framed event extraction as a multi-turn question answering task \citep{du2020qaevent, li2020qaevent, liu2020qaevent, zhou2021role}, while others approached it as a sequence-to-sequence generation task \citep{hsu2022degree, lu2021text2event, li2021document}. Although effective, these methods heavily relied on manually designed prompts and templates, except for Text2Event \citep{paolini2021structure}, which depended solely on context information. In this study, we propose an oracle-free event extraction task and focus on event extraction via generation without templates.


\subsection{Post-Generation Ranking}
Post-generation re-ranking is usually applied in two-stage systems, that is generation and reanking, to re-score the output from the first stage by training an additional re-ranking module. This technique has been widely used in neural translation and summarization. For example, \citet{ng2019facebook, yee2019simple} re-score and select the best hypotheses using Noisy Channel Modeling to improve translation quality. \citet{zhong2020extractive} formulate the summarization as text matching and re-ranks the summary candidates based on similarity score. \citet{liu2021simcls} introduce an additional scoring model with contrastive training to predict the score of generated summaries. Both of the two methods utilize a margin-based ranking loss that initialize candidates with orders. For the event trigger selection, we assume that the beam score is not a reliable indicator and consequently treat the candidates equally. \citet{su2021few} use a contrastive re-ranking module with hinge loss to select prototypes for table-to-text generation. To the best of our knowledge, our work is the first to focus on enhancing the oracle-free generation-based event extraction models using re-ranking.


\section{Conclusion}
In this work, we studied a more realistic setting of the event extraction task, namely the oracle-free event extraction, where no additional information beyond the context  is required for the event inference. We proposed a generation-based event extraction framework called \model{} to address this task. Our \model{} introduces a selector to improve trigger extraction performance by re-ranking and automatically determines the number of events in a context . We also investigated how dependent the current generation-based models are on extra knowledge, including designed event-specific templates, event trigger keywords and event descriptions. Our results show that this reliance on the template and human-designed trigger sets is unnecessary. A pure oracle-free model applied directly can perform very well on general event extraction. In the future, we would like to investigate extending the sentence-level event extraction to document-level and exploring the zero-shot setting to meet the situation where unseen events emerge.


\section{Limitations}
In this section, we discuss the limitations of our study, which should be taken into account when interpreting the results and building upon our work in future research.
\begin{itemize}
\item Our approach primarily focuses on English text, potentially limiting its applicability to other languages. Future work could explore the adaptation of the model for multilingual or cross-lingual event extraction tasks.
\item In this study, we concentrate on sentence-level event extraction, which does not consider document context information. Future research could investigate document-level extraction and the incorporation of cross-sentence relationships.
\item The training dataset employed in our study is relatively small and may not encompass all possible event types or scenarios. Utilizing a larger and more diverse dataset could potentially enhance the model's performance and generalizability.
\item We introduced a ranking module to improve trigger generation; however, due to the two-stage inference, the framework is susceptible to error propagation. If the trigger is not identified in the first stage, the arguments cannot be extracted.
\end{itemize}

In conclusion, although our study yields promising results, we recognize the limitations outlined above. Addressing these limitations in future work may lead to further advancements in event extraction performance and expand the applicability of our approach.

\section{Ethics Statement}
In the preparation and submission of this research paper, we affirm that our work adheres to the highest ethical standards and is devoid of any ethical issues. The study presented in the manuscript was conducted in a manner that respects the principles of academic integrity, transparency, and fairness.



% Entries for the entire Anthology, followed by custom entries
\bibliography{acl2023}
\bibliographystyle{acl_natbib}

\clearpage
\appendix

\section{Appendices}
\label{sec:appendix}

\subsection{Implementation Details}
The training of the pipeline requires two stages. In the first stage, we train the generation model by fine-tuning a pre-trained T5-base model \cite{raffel2020exploring} (i.e., the generator of our \model{}) to make use of the strong natural language understanding ability. T5 \cite{raffel2020exploring} is used based on our empirical results. The T5 model and tokenization implementation are done with HuggingFace's Transformers library \cite{wolf2019huggingface}. We use 1 GeForce RTX3090 with a batch size of 8; the optimizer is AdamW \cite{adamw} with a learning rate of 0.0001 decaying at a rate of 0.00001. The maximum input/output sequence length is 650 and 200, respectively. The number of fine-tuning epochs is set to 60 while saving the best model based on the performance on the validation set. Training of trigger extraction and argument extraction is done simultaneously, and the two tasks are distinguished by using different prefixes (\texttt{TriggerExtract} and \texttt{ArgumentExtract} respectively). For the re-ranking model, we fine-tune a pre-trained RoBERTa-base model \cite{liu2019roberta} that is also implemented using Transformers library and AdamW optimizer with a batch size of 8. The maximum input/output length is set to 512 and 200, respectively. The number of negative candidates is set to 5, and training starts with a learning rate of 0.005.

During inference, the maximum sequence length is the same as training, which is 200. For trigger extraction, we employ beam search with 10 beams and set the number of returns to 10 for the training of the ranker. The generated candidates are then input to the Selector to be re-ranked and filtered by the pre-defined hyper-parameter \textit{threshold}. The final set of trigger words is then input back to the generator together with the context to generate arguments through greedy search. We use \textit{regular expression} to extract the entity and role from the generated sentence. The results of all experiments are expressed in Table \ref{tb:mainresult}.

\end{document}
