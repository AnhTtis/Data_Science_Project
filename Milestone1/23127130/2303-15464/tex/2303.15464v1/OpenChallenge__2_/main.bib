@inproceedings{costa2003entropic,
  title={Entropic graphs for manifold learning},
  author={Costa, Jose A and Hero, AO},
  booktitle={The Thrity-Seventh Asilomar Conference on Signals, Systems \& Computers, 2003},
  volume={1},
  pages={316--320},
  year={2003},
  organization={IEEE}
}
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{dhar2021survey,
  title={A survey of on-device machine learning: An algorithms and learning theory perspective},
  author={Dhar, Sauptik and Guo, Junyao and Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
  journal={ACM Transactions on Internet of Things},
  volume={2},
  number={3},
  pages={1--49},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{li2020train,
  title={Train big, then compress: Rethinking model size for efficient training and inference of transformers},
  author={Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joey},
  booktitle={International Conference on machine learning},
  pages={5958--5968},
  year={2020},
  organization={PMLR}
}
@article{bengio2021flow,
  title={Flow network based generative models for non-iterative diverse candidate generation},
  author={Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27381--27394},
  year={2021}
}
@article{goyal2022inductive,
  title={Inductive biases for deep learning of higher-level cognition},
  author={Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2266},
  pages={20210068},
  year={2022},
  publisher={The Royal Society}
}
@article{bengio2017consciousness,
  title={The consciousness prior},
  author={Bengio, Yoshua},
  journal={arXiv preprint arXiv:1709.08568},
  year={2017}
}
@article{xia2021causal,
  title={The causal-neural connection: Expressiveness, learnability, and inference},
  author={Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10823--10836},
  year={2021}
}
@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017},
  publisher={JMLR. org}
}
@article{zhu2020global,
  title={The global optimization geometry of shallow linear neural networks},
  author={Zhu, Zhihui and Soudry, Daniel and Eldar, Yonina C and Wakin, Michael B},
  journal={Journal of Mathematical Imaging and Vision},
  volume={62},
  number={3},
  pages={279--292},
  year={2020},
  publisher={Springer}
}

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

@article{10.1162/neco.1997.9.1.1,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Flat Minima}",
    journal = {Neural Computation},
    volume = {9},
    number = {1},
    pages = {1-42},
    year = {1997},
    month = {01},
    abstract = "{We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a “flat” minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to “simple” networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a “good” weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and “optimal brain surgeon/optimal brain damage.”}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.1.1},
    url = {https://doi.org/10.1162/neco.1997.9.1.1},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/1/1/813385/neco.1997.9.1.1.pdf},
}

@incollection{vapnik1971uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, Vladimir N and Chervonenkis, A Ya},
  booktitle={Measures of complexity},
  pages={11--30},
  year={1971},
  publisher={Springer}
}

@article{blumer1989learnability,
  title={Learnability and the Vapnik-Chervonenkis dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM (JACM)},
  volume={36},
  number={4},
  pages={929--965},
  year={1989},
  publisher={ACM New York, NY, USA}
}

@inproceedings{harvey2017nearly,
  title={Nearly-tight VC-dimension bounds for piecewise linear neural networks},
  author={Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  booktitle={Conference on learning theory},
  pages={1064--1068},
  year={2017},
  organization={PMLR}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{bartlett2019nearly,
  title={Nearly-tight {VC}-dimension and pseudodimension bounds for piecewise linear neural networks},
  author={Bartlett, Peter L and Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={2285--2301},
  year={2019},
  publisher={JMLR. org}
}

@article{ben2006analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}

@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}


@article{gulrajani2020search,
  title={In search of lost domain generalization},
  author={Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:2007.01434},
  year={2020}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@inproceedings{li2017deeper,
  title={Deeper, broader and artier domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5542--5550},
  year={2017}
}

@inproceedings{ghifary2015domain,
  title={Domain generalization for object recognition with multi-task autoencoders},
  author={Ghifary, Muhammad and Kleijn, W Bastiaan and Zhang, Mengjie and Balduzzi, David},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2551--2559},
  year={2015}
}

@inproceedings{fang2013unbiased,
  title={Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias},
  author={Fang, Chen and Xu, Ye and Rockmore, Daniel N},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1657--1664},
  year={2013}
}

@inproceedings{beery2018recognition,
  title={Recognition in terra incognita},
  author={Beery, Sara and Van Horn, Grant and Perona, Pietro},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={456--473},
  year={2018}
}

@inproceedings{peng2019moment,
  title={Moment matching for multi-source domain adaptation},
  author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1406--1415},
  year={2019}
}

@inproceedings{venkateswara2017deep,
  title={Deep hashing network for unsupervised domain adaptation},
  author={Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5018--5027},
  year={2017}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@inproceedings{zhang2022low,
  title={Low-Precision Stochastic Gradient Langevin Dynamics},
  author={Zhang, Ruqi and Wilson, Andrew Gordon and De Sa, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={26624--26644},
  year={2022},
  organization={PMLR}
}
@article{metel2022variants,
  title={Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments},
  author={Metel, Michael R},
  journal={arXiv preprint arXiv:2211.04655},
  year={2022}
}
@article{cacciola2023convergence,
  title={On the Convergence of Stochastic Gradient Descent in Low-precision Number Formats},
  author={Cacciola, Matteo and Frangioni, Antonio and Asgharian, Masoud and Ghaffari, Alireza and Nia, Vahid Partovi},
  journal={arXiv preprint arXiv:2301.01651},
  year={2023}
}
@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}
@article{ghaffari2022integer,
  title={Is Integer Arithmetic Enough for Deep Learning Training?},
  author={Ghaffari, Alireza and Tahaei, Marzieh S and Tayaranian, Mohammadreza and Asgharian, Masoud and Nia, Vahid Partovi},
  journal={arXiv preprint arXiv:2207.08822},
  year={2022}
}
@inproceedings{zhang2019bridging,
  title={Bridging theory and algorithm for domain adaptation},
  author={Zhang, Yuchen and Liu, Tianle and Long, Mingsheng and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={7404--7413},
  year={2019},
  organization={PMLR}
}

@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International Conference on Machine Learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}

@inproceedings{christie2018functional,
  title={Functional map of the world},
  author={Christie, Gordon and Fendley, Neil and Wilson, James and Mukherjee, Ryan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6172--6180},
  year={2018}
}

@article{bandi2018detection,
  title={From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge},
  author={Bandi, Peter and Geessink, Oscar and Manson, Quirine and Van Dijk, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and Bejnordi, Babak Ehteshami and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and others},
  journal={IEEE transactions on medical imaging},
  volume={38},
  number={2},
  pages={550--560},
  year={2018},
  publisher={IEEE}
}


@article{degrave2021ai,
  title={AI for radiographic COVID-19 detection selects shortcuts over signal},
  author={DeGrave, Alex J and Janizek, Joseph D and Lee, Su-In},
  journal={Nature Machine Intelligence},
  volume={3},
  number={7},
  pages={610--619},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{blitzer2006domain,
  title={Domain adaptation with structural correspondence learning},
  author={Blitzer, John and McDonald, Ryan and Pereira, Fernando},
  booktitle={Proceedings of the 2006 conference on empirical methods in natural language processing},
  pages={120--128},
  year={2006}
} 
@inproceedings{saenko2010adapting,
  title={Adapting visual category models to new domains},
  author={Saenko, Kate and Kulis, Brian and Fritz, Mario and Darrell, Trevor},
  booktitle={European conference on computer vision},
  pages={213--226},
  year={2010},
  organization={Springer}
}

@inproceedings{lehner20223d,
  title={3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection},
  author={Lehner, Alexander and Gasperini, Stefano and Marcos-Ramiro, Alvaro and Schmidt, Michael and Mahani, Mohammad-Ali Nikouei and Navab, Nassir and Busam, Benjamin and Tombari, Federico},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17295--17304},
  year={2022}
}


@inproceedings{rosenfeld2020risks,
  title={The Risks of Invariant Risk Minimization},
  author={Rosenfeld, Elan and Ravikumar, Pradeep Kumar and Risteski, Andrej},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{tachet2020domain,
  title={Domain adaptation with conditional distribution matching and generalized label shift},
  author={Tachet des Combes, Remi and Zhao, Han and Wang, Yu-Xiang and Gordon, Geoffrey J},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19276--19289},
  year={2020}
}

@inproceedings{zhao2019learning,
  title={On learning invariant representations for domain adaptation},
  author={Zhao, Han and Des Combes, Remi Tachet and Zhang, Kun and Gordon, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={7523--7532},
  year={2019},
  organization={PMLR}
}

@inproceedings{kamath2021does,
  title={Does invariant risk minimization capture invariance?},
  author={Kamath, Pritish and Tangella, Akilesh and Sutherland, Danica and Srebro, Nathan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4069--4077},
  year={2021},
  organization={PMLR}
}

@inproceedings{acuna2021f,
  title={f-domain adversarial learning: Theory and algorithms},
  author={Acuna, David and Zhang, Guojun and Law, Marc T and Fidler, Sanja},
  booktitle={International Conference on Machine Learning},
  pages={66--75},
  year={2021},
  organization={PMLR}
}

@article{zhu2021understanding,
  title={Understanding the Generalization Benefit of Model Invariance from a Data Perspective},
  author={Zhu, Sicheng and An, Bang and Huang, Furong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4328--4341},
  year={2021}
}

@article{littlestone1986relating,
  title={Relating data compression and learnability},
  author={Littlestone, Nick},
  year={1986},
  publisher={Citeseer}
}

@article{haghifam2021towards,
  title={Towards a unified information-theoretic framework for generalization},
  author={Haghifam, Mahdi and Dziugaite, Gintare Karolina and Moran, Shay and Roy, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26370--26381},
  year={2021}
}

@article{ashtiani2018nearly,
  title={Nearly tight sample complexity bounds for learning mixtures of {G}aussians via sample compression schemes},
  author={Ashtiani, Hassan and Ben-David, Shai and Harvey, Nicholas and Liaw, Christopher and Mehrabian, Abbas and Plan, Yaniv},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{lotfipac,
  title={PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization},
  author={Lotfi, Sanae and Finzi, Marc Anton and Kapoor, Sanyam and Potapczynski, Andres and Goldblum, Micah and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}


@inproceedings{mcallester1998some,
  title={Some pac-bayesian theorems},
  author={McAllester, David A},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={230--234},
  year={1998}
}

@inproceedings{neyshabur2018role,
  title={The role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{koltchinskii2001rademacher,
  title={Rademacher penalties and structural risk minimization},
  author={Koltchinskii, Vladimir},
  journal={IEEE Transactions on Information Theory},
  volume={47},
  number={5},
  pages={1902--1914},
  year={2001},
  publisher={IEEE}
}


@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984},
  publisher={Acm New York, NY, USA}
}




@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}



@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  pages={5827--5837},
  year={2019},
  organization={PMLR}
}


@article{neyshabur2018towards,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}


@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{chatterjee2020coherent,
  title={Coherent gradients: An approach to understanding generalization in gradient descent-based optimization},
  author={Chatterjee, Satrajit},
  journal={arXiv preprint arXiv:2002.10657},
  year={2020}
}





% double descent
@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}
@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}
@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={949--986},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}
@article{wyner2017explaining,
  title={Explaining the success of adaboost and random forests as interpolating classifiers},
  author={Wyner, Abraham J and Olson, Matthew and Bleich, Justin and Mease, David},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={1558--1590},
  year={2017},
  publisher={JMLR. org}
}
@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}
@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}
@inproceedings{chang2021provable,
  title={Provable benefits of overparameterization in model compression: From double descent to pruning neural networks},
  author={Chang, Xiangyu and Li, Yingcong and Oymak, Samet and Thrampoulidis, Christos},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={6974--6983},
  year={2021}
}








@inproceedings{reuther2019survey,
  title={Survey and benchmarking of machine learning accelerators},
  author={Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
  booktitle={2019 IEEE high performance extreme computing conference (HPEC)},
  pages={1--9},
  year={2019},
  organization={IEEE}
}
@inproceedings{steinhardt2016memory,
  title={Memory, communication, and statistical queries},
  author={Steinhardt, Jacob and Valiant, Gregory and Wager, Stefan},
  booktitle={Conference on Learning Theory},
  pages={1490--1516},
  year={2016},
  organization={PMLR}
}

@article{levina2004maximum,
  title={Maximum likelihood estimation of intrinsic dimension},
  author={Levina, Elizaveta and Bickel, Peter},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}


@article{friedman2000additive,
  title={Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  journal={The annals of statistics},
  volume={28},
  number={2},
  pages={337--407},
  year={2000},
  publisher={Institute of Mathematical Statistics}
}
@article{blalock2020state,
  title={What is the state of neural network pruning?},
  author={Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
  journal={Proceedings of machine learning and systems},
  volume={2},
  pages={129--146},
  year={2020}
}
@article{mackay2005comments,
  title={Comments on’maximum likelihood estimation of intrinsic dimension’by E. Levina and P. Bickel (2005)},
  author={MacKay, David JC and Ghahramani, Zoubin},
  journal={The Inference Group Website, Cavendish Laboratory, Cambridge University},
  year={2005}
}
@article{blier2018description,
  title={The description length of deep learning models},
  author={Blier, L{\'e}onard and Ollivier, Yann},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{grant2022predicting,
  title={Predicting generalization with degrees of freedom in neural networks},
  author={Grant, Erin and Wu, Yan},
  booktitle={ICML 2022 2nd AI for Science Workshop},
  year={2022}
}
@article{dherin2022neural,
  title={Why neural networks find simple solutions: the many regularizers of geometric complexity},
  author={Dherin, Benoit and Munn, Michael and Rosca, Mihaela and Barrett, David GT},
  journal={arXiv preprint arXiv:2209.13083},
  year={2022}
}
@article{gao2016degrees,
  title={Degrees of freedom in deep neural networks},
  author={Gao, Tianxiang and Jojic, Vladimir},
  journal={arXiv preprint arXiv:1603.09260},
  year={2016}
}

@article{ji1993generalization,
  title={Generalization error and the expected network complexity},
  author={Ji, Chuanyi},
  journal={Advances in Neural Information Processing Systems},
  volume={6},
  year={1993}
}
@article{zou2007degrees,
  title={On the “degrees of freedom” of the lasso},
  author={Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  journal={The Annals of Statistics},
  volume={35},
  number={5},
  pages={2173--2192},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

@article{efron2004estimation,
  title={The estimation of prediction error: covariance penalties and cross-validation},
  author={Efron, Bradley},
  journal={Journal of the American Statistical Association},
  volume={99},
  number={467},
  pages={619--632},
  year={2004},
  publisher={Taylor \& Francis}
}
@article{ye1998measuring,
  title={On measuring and correcting the effects of data mining and model selection},
  author={Ye, Jianming},
  journal={Journal of the American Statistical Association},
  volume={93},
  number={441},
  pages={120--131},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{wang2022,
	title={{NITI: Training Integer Neural Networks Using Integer-Only Arithmetic}},
	author={Wang, Maolin and Rasoulinezhad, Seyedramin and Leong, Philip HW and So, Hayden K-H},
	journal={IEEE Transactions on Parallel and Distributed Systems},
	volume={33},
	number={11},
	pages={3249--3261},
	year={2022},
	publisher={IEEE}
}

@inproceedings{zhu2017,
	title={{Trained Ternary Quantization}},
	author={Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J},
	booktitle={ICLR},
	year={2017}
}

@inproceedings{rastegari2016,
	title={{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}},
	author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	booktitle={European Conference on Computer Vision},
	pages={525--542},
	year={2016},
	organization={Springer}
}

@InProceedings{courbariaux2015,
	title={{BinaryConnect: Training Deep Neural Networks with binary weights during propagations}},
	author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	booktitle = {NeurIPS},
	year={2015}
}

@InProceedings{jacob2018,
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	title = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
	booktitle = {CVPR},
	year = {2018}
}

@article{zhou2016,
	title={{DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients}},
	author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
	journal={arXiv preprint arXiv:1606.06160},
	year={2016}
}

@inproceedings{banner2018,
	author = {Banner, Ron and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
	booktitle = {NeurIPS},
	title = {Scalable methods for 8-bit training of neural networks},
	year = {2018}
}

@inproceedings{wu2018,
	title={{Training and Inference with Integers in Deep Neural Networks}},
	author={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},
	booktitle={ICLR},
	year={2018}
}

@inproceedings{chen2017,
	title={{FxpNet: Training a deep convolutional neural network in fixed-point representation}},
	author={Chen, Xi and Hu, Xiaolin and Zhou, Hucheng and Xu, Ningyi},
	booktitle={IJCNN},
	pages={2494--2501},
	year={2017},
	organization={IEEE}
}

@inproceedings{das2018,
	title={{Mixed Precision Training of Convolutional Neural Networks using Integer Operations}},
	author={Das, Dipankar and Mellempudi, Naveen and Mudigere, Dheevatsa and Kalamkar, Dhiraj and Avancha, Sasikanth and Banerjee, Kunal and Sridharan, Srinivas and Vaidyanathan, Karthik and Kaul, Bharat and Georganas, Evangelos and 
	Heinecke, Alexander and Dubey, Pradeep and 
	Corbal, Jesus and Shustrov, Nikita and Dubtsov, Roma and Fomenko, Evarist and Pirogov, Vadim},
	booktitle={ICLR},
	year={2018}
}

@article{polyak1987,
	title={Introduction to optimization},
	author={Polyak, Boris T},
	journal={Optimization Software Inc., Publications Division, New York},
	year={1987}
}

@book{bertsekas1999,
	title={{Nonlinear Programming}},
	author={Bertsekas, Dimitri P},
	year={1999},
	publisher={Athena Scientific}
}

@article{polyak,
	title={{Some methods of speeding up the convergence of iteration methods}},
	author={B. T. Polyak},
	journal={USSR Computational Mathematics and Mathematical Physics},
	volume={4},
	number={5},
	pages={1--17},
	year={1964},
	publisher={}
}

@article{nesterov,
	title={{A Method for Solving a Convex Programming Problem with Convergence Rate $O(1/k^2)$}},
	author={Nesterov, Y. E.},
	journal={Soviet Mathematics Doklady},
	volume={27},
	number={2},
	pages={372--376},
	year={1983},
	publisher={}
}

@article{xia2022,
	title={On the influence of roundoff errors on the convergence of the gradient descent method with low-precision floating-point computation},
	author={Xia, Lu and Massei, Stefano and Hochstenbach, Michiel and Koren, Barry},
	journal={arXiv preprint arXiv:2202.12276},
	year={2022}
}

@InProceedings{Danilova,
author="Danilova, Marina
and Kulakova, Anastasiia
and Polyak, Boris",
editor="Bohner, Martin
and Siegmund, Stefan
and {\v{S}}imon Hilscher, Roman
and Stehl{\'i}k, Petr",
title="Non-monotone Behavior of the Heavy Ball Method",
booktitle="Difference Equations and Discrete Dynamical Systems with Applications",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="213--230",
isbn="978-3-030-35502-9"
}

@inproceedings{
reddi2018,
title={On the Convergence of Adam and Beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015}
}

@article{Keskar2017ImprovingGP,
  title={Improving Generalization Performance by Switching from Adam to SGD},
  author={Nitish Shirish Keskar and Richard Socher},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.07628}
}

@inproceedings{ZhangKVKRKS20,
  title = {Why are Adaptive Methods Good for Attention Models?},
  author = {Jingzhao Zhang and Sai Praneeth Karimireddy and Andreas Veit and Seungyeon Kim and Sashank J. Reddi and Sanjiv Kumar and Suvrit Sra},
  year = {2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/b05b57f6add810d3b7490866d74c0053-Abstract.html},
  researchr = {https://researchr.org/publication/ZhangKVKRKS20},
  cites = {0},
  citedby = {0},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
  editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria-Florina Balcan and Hsuan-Tien Lin},
}

@inproceedings{wilson2017,
author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4151--4161},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{lars,
  title={Large Batch Training of Convolutional Networks},
  author={Yang You and Igor Gitman and Boris Ginsburg},
  journal={arXiv: Computer Vision and Pattern Recognition},
  year={2017}
}

@inproceedings{lamb,
  author    = {Yang You and
               Jing Li and
               Sashank J. Reddi and
               Jonathan Hseu and
               Sanjiv Kumar and
               Srinadh Bhojanapalli and
               Xiaodan Song and
               James Demmel and
               Kurt Keutzer and
               Cho{-}Jui Hsieh},
  title     = {Large Batch Optimization for Deep Learning: Training {BERT} in 76
               minutes},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=Syx4wnEtvH},
  timestamp = {Sat, 17 Dec 2022 01:15:29 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/YouLRHKBSDKH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{goyal2017,
  author    = {Priya Goyal and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick and
               Pieter Noordhuis and
               Lukasz Wesolowski and
               Aapo Kyrola and
               Andrew Tulloch and
               Yangqing Jia and
               Kaiming He},
  title     = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal   = {CoRR},
  volume    = {abs/1706.02677},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02677},
  eprinttype = {arXiv},
  eprint    = {1706.02677},
  timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalDGNWKTJH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Andrychowicz2016LearningTL,
  title={Learning to learn by gradient descent by gradient descent},
  author={Marcin Andrychowicz and Misha Denil and Sergio Gomez Colmenarejo and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
  booktitle={NIPS},
  year={2016}
}
@article{velo2022,
  author    = {Luke Metz and
               James Harrison and
               C. Daniel Freeman and
               Amil Merchant and
               Lucas Beyer and
               James Bradbury and
               Naman Agrawal and
               Ben Poole and
               Igor Mordatch and
               Adam Roberts and
               Jascha Sohl{-}Dickstein},
  title     = {VeLO: Training Versatile Learned Optimizers by Scaling Up},
  journal   = {CoRR},
  volume    = {abs/2211.09760},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.09760},
  doi       = {10.48550/arXiv.2211.09760},
  eprinttype = {arXiv},
  eprint    = {2211.09760},
  timestamp = {Thu, 24 Nov 2022 15:52:33 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-09760.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pope2021intrinsic,
  title={The intrinsic dimension of images and its impact on learning},
  author={Pope, Phillip and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2104.08894},
  year={2021}
}

@article{ansuini2019intrinsic,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{shaham2018provable,
  title={Provable approximation properties for deep neural networks},
  author={Shaham, Uri and Cloninger, Alexander and Coifman, Ronald R},
  journal={Applied and Computational Harmonic Analysis},
  volume={44},
  number={3},
  pages={537--557},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{imaizumi2019deep,
  title={Deep neural networks learn non-smooth functions effectively},
  author={Imaizumi, Masaaki and Fukumizu, Kenji},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={869--878},
  year={2019},
  organization={PMLR}
}

@article{imaizumi2022advantage,
  title={Advantage of deep neural networks for estimating functions with singularity on hypersurfaces},
  author={Imaizumi, Masaaki and Fukumizu, Kenji},
  journal={Journal of Machine Learning Research},
  volume={23},
  pages={1--54},
  year={2022}
}

@article{nakada2020adaptive,
  title={Adaptive approximation and generalization of deep neural network with intrinsic dimensionality},
  author={Nakada, Ryumei and Imaizumi, Masaaki},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={7018--7055},
  year={2020},
  publisher={JMLRORG}
}

@article{birdal2021intrinsic,
  title={Intrinsic dimension, persistent homology and generalization in neural networks},
  author={Birdal, Tolga and Lou, Aaron and Guibas, Leonidas J and Simsekli, Umut},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6776--6789},
  year={2021}
}

