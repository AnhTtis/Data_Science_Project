\section{Experiment}
\subsection{Image classification}
\vspace{-2mm}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{5pt}
    \scalebox{0.8}{\input{tables/imagenet}}
    \caption{\label{tab:imagenet}
    \textbf{Performance of models trained on ImageNet-1K.} The throughputs are measured on an A100 GPU (PyTorch 1.13.0 and CUDA 11.7.1) with TF32 (TensorFloat-32), and on a 2080Ti (PyTorch 1.8.1 and CUDA 10.2) with FP32. The batch size for throughput benchmarking is initially set as 128 and is reduced until the GPU can host. The better results of ``Channel First" and ``Channel Last"  memory layouts are reported. }
    \vspace{-9mm}
\end{table*}

\myPara{Setup}
For the image classification task, ImageNet-1K \cite{imagenet_cvpr, imagenet_ijcv} is one of the most commonly-used benchmarks, which contains around 1.3 million images in the training set and 50 thousand images in the validation set. To fairly compare with the widely-used baselines, \eg, Swin \cite{swin} and ConvNeXt \cite{convnext}, we mainly follow the training hyper-parameters from DeiT \cite{deit} without distillation. Specifically, the models are trained by AdamW \cite{adamw} optimizer with a learning rate $lr = 0.001 \times \mathrm{batch size} / 1024$ ($lr=4e-3$ and $\mathrm{batch size} = 4096$ are used in this paper the same as ConvNeXt). Following DeiT, data augmentation includes standard random resized crop, horizontal flip, RandAugment \cite{randaugment}, Mixup \cite{mixup}, CutMix \cite{cutmix}, Random Erasing \cite{random_erasing} and color jitter. For regularization, label smoothing \cite{inception_v3}, stochastic depth \cite{stochastic_depth}, and weight decay are adopted. Like ConvNeXt, we also use LayerScale \cite{layerscale}, a technique to help train deep models. Our code is based on PyTorch \cite{pytorch} and timm \cite{rw2019timm}. 




\begin{table}[t]
\begin{center}
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l | c c c c c }
\whline
\multirow{2}{*}{\makecell[c]{Model}} &  \multirow{2}{*}{\makecell[c]{Params \\ (M)}} & \multirow{2}{*}{\makecell[c]{MACs \\ (G)}}  & \multicolumn{2}{c}{Throughput (img/s)} & \multirow{2}{*}{\makecell[c]{Top-1 \\ (\%)}} \\
~ & ~ & ~ & Train & Infer  & ~  \\
\whline
DeiT-S \cite{deit} & 22 & 4.6 & 276 & 784 & 79.8 \\ 
MetaNeXt-Attn & 22 & 4.6 & 288 & 816 & 3.9  \\ 
ConvNeXt-S (\textit{iso.}) \cite{convnext} & 22 & 4.3 & 270 & 879 &  79.7 \\
\modelname{}-S (\textit{iso.}) & 22 & 4.2 & 310 & 998 & 79.7 \\
\whline
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{\textbf{Comparison among ViT, isotropic ConvNeXt and \modelname{}.}  MetaNeXt-Attn is instantiated from MetaNeXt with token mixer of self-attention \cite{transformer}. The throughputs are measured on 2080Ti (PyTorch 1.8.1 and CUDA 10.2) with FP32. The batch size for throughput benchmarking is initially set as 128 and is reduced until the GPU can host. The better results of ``Channel First" and ``Channel Last"  memory layouts are reported.}
\label{tab:iso}
\vspace{-9mm}
\end{table}

\myPara{Results} 
We compare \modelname{} with various state-of-the-art models, including attention-based and convolution-based models. As can be seen in Table \ref{tab:imagenet}, \modelname{} achieves highly competitive performance as well as enjoys higher speed. 
\modelname{} consistently enjoys better accuracy-speed trade-off than ConvNeXt \cite{convnext}.
For example, \modelname{}-T not only surpasses ConvNeXt-T by 0.2\%, but also enjoys $1.6 \times$/$1.2 \times $ training/inference throughputs on A100 than ConvNeXts, similar to those of ResNet-50. That is to say, \modelname{}-T enjoys both ResNet-50's speed and ConvNeXt-T's accuracy. 
Moreover,  following Swin and ConvNeXt, we also finetuned the \modelname{}-B trained at the resolution of $224 \times 224$ to $384 \times 384$ for 30 epochs.  We can see that \modelname{}-B obtains higher train and inference throughputs than ConvNeXt-B while keeping competitive accuracy.


It is observed that the speed improvement is much more significant for the lightweight model size, and the improvement gradually becomes smaller when the model size scales up.  The reason is that  computation complexity of depthwise and Inception depthwise convolutions are linear to channel number, \ie, $\mathcal{O}(C)$ where $C$ is channel number. For MLPs, their computation complexity is
$\mathcal{O}(C^2)$. For larger models (larger $C$), its computation is further dominated by MLPs. By only improving depthwise convolution, the speed improvement becomes smaller when the model is larger. 


Besides the 4-stage framework \cite{vgg, resnet, swin}, another notable one is ViT-style \cite{vit} isotropic architecture which has only one stage. To match the parameters and MACs of DeiT-S, we construct \modelname{}-S (\textit{iso.}) following ConvNeXt-S (\textit{iso.}) \cite{convnext}. Specifically, we set the embedding dimension as 384 and the block number as 18. Besides, we build a model called MetaNeXt-Attn which is instantiated from MetaNeXt block by specifying self-attention as token mixer. The aim of this model is to investigate whether it is possible to merge two residual sub-blocks of the Transformer block into a single one. The experiment results are shown in Table \ref{tab:iso}. It can be seen that \modelname{} can also perform well with the isotropic architecture, demonstrating \modelname{} exhibits good generalization across different frameworks. It is worth noting that MetaNeXt-Attn could not be trained to converge and only achieved an accuracy of 3.9\%. This result suggests that, unlike the token mixer in MetaFormer, the token mixer in MetaNeXt cannot be too complex. If it is, the model may not be trainable.





\begin{table}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{6pt}
\input{tables/ade_uper}
\caption{\textbf{Performance of semantic segmentation with UperNet \cite{upernet} on ADE20K~\cite{ade20k} validation set.} Images are cropped to $512 \times 512$ for training. The MACs are measured with input size of $512 \times 2048$. The FPS are benchamrked on 2080Ti.}
\label{tab:upernet}
\vspace{-6mm}
\end{table}

\begin{table}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\input{tables/ade_fpn}
\caption{\textbf{Performance of semantic segmentation with Semantic FPN \cite{fpn} on ADE20K~\cite{ade20k} validation set.} Images are cropped to $512 \times 512$ for training. The MACs are measured with input size of $512 \times 512$. The FPS are benchamrked on 2080Ti.}
\label{tab:fpn}
\vspace{-9mm}
\end{table}

\begin{table*}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{9pt}
\scalebox{1.0}{\input{tables/ablation}}
\caption{\textbf{Ablation for \modelname{} on ImageNet-1K classification benchmark.} \modelname{}-T is utilized as the baseline for the ablation study. Top-1 accuracy on the validation set is reported. The throughputs are measured on an A100 GPU (PyTorch 1.13.0 and CUDA 11.7.1) with TF32 and batch size of 128.
}
\label{tab:ablation}
\vspace{-9mm}
\end{table*}


\subsection{Semantic segmentation}
\myPara{Setup} 
ADE20K~\cite{ade20k}, a commonly used scene parsing benchmark, is used to evaluate our models on semantic segmentation task. ADE20K includes 150 fine-grained semantic categories, containing twenty thousand and two thousand images in the training set and validation set, respectively.
The checkpoints trained on ImageNet-1K \cite{imagenet_cvpr} at the resolution of $224^2$ are utilized to initialize the backbones. Following Swin \cite{swin} and ConvNeXt \cite{convnext}, we firstly evaluate \modelname{} with UperNet \cite{upernet}. The models are trained with AdamW \cite{adamw} optimizer with learning rate of 6e-5 and batch size of 16 for 160K iterations. Following PVT \cite{pvt} and PoolFormer \cite{metaformer}, \modelname{} is also evaluated with Semantic FPN \cite{fpn}. 
In common practices~\cite{fpn,chen2017deeplab}, the batch size is 16 for the setting of 80K iterations. Following PoolFormer \cite{metaformer}, we increase the batch size to 32 and decrease the iterations to 40K to speed up training.  AdamW \cite{adam, adamw}  is adopted with a learning rate of 2e-4 and a polynomial decay schedule of 0.9 power. Our code is based on PyTorch \cite{pytorch} and mmsegmentation \cite{mmseg2020}.

\myPara{Results}
For segmentation with UpNet \cite{upernet}, the results are shown in Table \ref{tab:upernet}. As can be seen, 
\modelname{} consistently outperforms Swin \cite{swin} and ConvNeXt \cite{convnext} for different model sizes. In the method of Semantic FPN \cite{fpn} as shown in Table \ref{tab:fpn}, \modelname{} significantly surpasses other backbones, like PVT \cite{pvt} and PoolFormer \cite{metaformer}. These results show that \modelname{} also has a high potential for dense prediction tasks.   





\subsection{Ablation studies}
We conduct ablation studies on ImageNet-1K \cite{imagenet_cvpr, imagenet_ijcv} using \modelname{}-T as baseline from the following aspects.


\myPara{Branch} Inception depthwise convolution includes four branches, three convolutional ones, and identity mapping. When removing any branch of horizontal or vertical band kernel, performance significantly drops from 82.3\% to 81.9\%, demonstrating the importance of these two branches. This is because these two branches with band kernels can enlarge the receptive field of the model. For the branch of small square kernel size of $3\times 3$, removing it can still achieve up to 82.0\% top-1 accuracy and bring higher throughput. This inspires us that if we attach more importance to the model speed, the simple version of \modelname{} without the square kernel of $3\times 3$ can be adopted.  For the band kernel, Inception v3 mostly equips them in a sequential way. We find that this assembling method can also obtain similar performance and even a little speed up the model. A possible reason is that PyTorch/CUDA may have optimized sequential convolutions well, and we only implement the parallel branches at a high level (see Algorithm \ref{alg:code}). We believe the parallel method will be faster when it is optimized better. Thus, parallel method for the band kernels is adopted by default. 


\myPara{Band kernel size} It is found the performance can be improved from kernel size 7 to 11, but it drops when the band kernel size increases to 13. This phenomenon may result from the optimization  and can be solved by methods like structural re-parameterization \cite{repvgg, replknet}. For simplicity, we  set the  kernel size as 11 by default except for atto size.

\myPara{Convolution branch ratio} When the ratio increases from $1/8$ to $1/4$, performance improvement can not be observed. Ma \etal \cite{shufflenet_v2} also point out that it is not necessary for all channels to conduct convolution. But when the ratio decreases to $1/16$, it brings a serious performance drop. This is because a smaller ratio would limit the degree of token mixing, resulting in performance drop. We thus set the convolution branch ratio as $1/8$ by default except for atto size.

\myPara{Normalization} When replacing the Batch Normalization \cite{batch_norm} with Layer Normalization \cite{layer_norm}, the performance improvement improve by 0.1\% but suffer throughput drop in both training and inference. Since this paper focuses on efficiency, we adopt Batch Normalization for \modelname{}.






