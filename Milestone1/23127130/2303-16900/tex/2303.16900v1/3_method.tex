\section{Method}
\vspace{-1mm}
%%%%%%%%% Algorithm
\begin{algorithm}[t]
\caption{Inception Depthwise Convolution (PyTorch-like Code)}
\label{alg:code}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
}
\begin{lstlisting}[language=python]
import torch.nn as nn

class InceptionDWConv2d(nn.Module):
    def __init__(self, in_channels, square_kernel_size=3, band_kernel_size=11, branch_ratio=1/8):
        super().__init__()
        
        gc = int(in_channels * branch_ratio) # channel number of a convolution branch
        
        self.dwconv_hw = nn.Conv2d(gc, gc, square_kernel_size, padding=square_kernel_size//2, groups=gc)
        
        self.dwconv_w = nn.Conv2d(gc, gc, kernel_size=(1, band_kernel_size), padding=(0, band_kernel_size//2), groups=gc)
        
        self.dwconv_h = nn.Conv2d(gc, gc, kernel_size=(band_kernel_size, 1), padding=(band_kernel_size//2, 0), groups=gc)
        
        self.split_indexes = (gc, gc, gc, in_channels - 3 * gc)
        
    def forward(self, x):
        # B, C, H, W = x.shape
        x_hw, x_w, x_h, x_id = torch.split(x, self.split_indexes, dim=1)
        
        return torch.cat(
            (self.dwconv_hw(x_hw), 
            self.dwconv_w(x_w), 
            self.dwconv_h(x_h), 
            x_id), 
            dim=1)
\end{lstlisting}
\end{algorithm}
\subsection{MetaNeXt}
\myPara{Formulation of MetaNeXt Block}
ConvNeXt \cite{convnext} is a modern CNN model with simple architecture. For each ConvNeXt block, the input $X$ is first processed by a depthwise convolutioin to propagate information along spatial dimensions. We follow MetaFormer \cite{metaformer} to abstract the depthwise convolution as a \textit{token mixer} which is responsible for spatial information interaction. Accordingly, as shown in the second subfigure in Figure \ref{fig:block}, the ConvNeXt  is abstracted as \textit{MetaNeXt} block. Formally, in a MetaNeXt block, its  input $X$ is firstly processed as

\begin{equation}
    X' = \mathrm{TokenMixer}(X)
\end{equation}
where $X, X' \in \mathbb{R}^{B \times C \times H \times W}$ with $B$, $C$, $H$ and $W$ denoting batch size, channel number, height and width, respectively. 
Then the output from the token mixer is normalized,
\begin{equation}
    Y = \mathrm{Norm}(X')
\end{equation}
After normalization \cite{batch_norm, layer_norm}, the resulting features are inputted into an MLP module consisting of two fully-connected layers with an activation function sandwiched between them, the same as feed-forward network in Transformer \cite{transformer}. The two fully-connected layers can also be implemented by $1 \times 1$ convolutions. Also, shortcut connection \cite{resnet, highway} is adopted. This process can be expressed by
\begin{equation}
     Y = \mathrm{Conv}_{1 \times 1}^{rC\rightarrow C}\{\sigma[\mathrm{Conv}_{1 \times 1}^{C \rightarrow rC}(Y)]\} + X,
\end{equation}
where $\mathrm{Conv}_{k \times k}^{C_i \rightarrow C_o}$ means convolution with kernel size of $k \times k$, input channels of $C_i$ and output channels of $C_o$; $r$ is the expansion ratio and $\sigma$ denotes activation function. 

\myPara{Comparison to MetaFormer block} As shown in Figure \ref{fig:block}, it can be found that MetaNeXt block shares similar modules with MetaFormer block \cite{metaformer}, \eg~token mixer and MLP. Nevertheless, a critical differentiation between the two models lies in the number of shortcut connections \cite{resnet, highway}. MetaNeXt block implements a single shortcut connection, whereas the MetaFormer block incorporates two, one for the token mixer and the other for the MLP.
From this aspect, MetaNeXt block can be regarded as a result of merging two residual sub-blocks from MetaFormer, thereby simplifying the overall architecture.
As a result, the MetaNeXt architecture exhibits a higher speed compared to MetaFormer. 
However, this simpler design comes with a limitation: the token mixer component in MetaNeXt cannot be complicated (\eg, Attention) as shown in our experiments (Table \ref{tab:iso}).

\myPara{Instantiation to ConvNeXt} As shown in Figure \ref{fig:block}, in ConvNeXt, the token mixer is simply implemented by a depthwise convolution,
\begin{equation}
    X' = \mathrm{TokenMixer}(X) = \mathrm{DWConv}_{k \times k}^{C\rightarrow C}(X)
\end{equation}
where $\mathrm{DWConv}_{k \times k}^{C \rightarrow C}$ denotes depthwise convolution with kernel size of $k \times k$. In ConvNeXt, $k$ is set as 7 by default.


\subsection{Inception depthwise convolution}
\myPara{Formulation}
As illustrated in Figure  \ref{fig:first_figure}, conventional depthwise convolution with large kernel size significantly impedes model speed.
Firstly, inspired by ShuffleNetV2 \cite{shufflenet_v2}, we find processing partial channels is also enough for single depthwise convolution layer as shown in our preliminary experiments in Appendix \ref{sec:pre_exp}. Thus, we leave partial channels unchanged and denote them as a branch of identity mapping. For the processing channels, we propose to decompose the depthwise operations with Inception style \cite{inception_v1, inception_v3, inception_v4}. 
Inception \cite{inception_v1} utilizes several branches of small kernels (\eg~$3 \times 3$) and large kernels (\eg~$5 \times 5$). Similarly, we adopt $3 \times 3$ as one of our branches but avoid the large square kernels because of their slow practical speed. Instead, large kernel $k_h \times k_w$ is decomposed as $1 \times k_w$ and $k_h \times 1$ inspired by Inception v3 \cite{inception_v3}. 

Specifically, for input $X$, we split it into four groups along the channel dimension,
\begin{equation}
\begin{split}
X_\mathrm{hw}, X_\mathrm{w}, X_\mathrm{h}, X_\mathrm{id} &= \mathrm{Split}(X) \\
&= X_{:, :g}, X_{: g:2g}, X_{: 2g:3g}, X_{: 3g:} 
\end{split}
\end{equation}
where $g$ is the channel numbers of convolution branches. We can set a ratio $r_g$ to determine the branch channel numbers by $g = r_g C$. Next, the splitting inputs are fed into different parallel branches,
\begin{equation}
\begin{split}
X'_\mathrm{hw} &= \mathrm{DWConv}_{k_s \times k_s}^{g\rightarrow g}g(X_\mathrm{hw}), \\
X'_\mathrm{w} &= \mathrm{DWConv}_{1\times k_b}^{g\rightarrow g}g(X_\mathrm{w}), \\
X'_\mathrm{h} &= \mathrm{DWConv}_{k_b\times 1}^{g\rightarrow g}g(X_\mathrm{h}), \\
X'_\mathrm{id} &= X_\mathrm{id}. \\
\end{split}
\end{equation}
where $k_s$ denotes  the small square kernel size set as 3 by default;   $k_b$ represents the band kernel size set as 11 by default. Finally, the outputs from each branch are concatenated,
\begin{equation} \label{eq1}
X' = \mathrm{Concat}(X'_\mathrm{hw}, X'_\mathrm{w}, X'_\mathrm{h}, X'_\mathrm{id}).
\end{equation}
The illustration of \modelname{} block is shown in Figure \ref{fig:block}. Moreover,  its  PyTorch \cite{pytorch} code is summarized  in Algorithm \ref{alg:code}.




\myPara{Complexity} The complexity of three types of convolution, \ie, conventional, depthwise, and Inception depthwise convolution is shown in Table \ref{tab:complexity}. As can be seen, Incetion depthwise convolution is much more efficient than the other two types of convolution in terms of parameter numbers of FLOPs. Inception depthwise convolution consumes parameters and FLOPs linear to both channel and kernel size. The comparison of depthwise and Inception depthwise convolutions regarding FLOPs is also clearly shown in Figure \ref{fig:two_types}. 


\begin{table}
\begin{center}
\small
\begin{tabular}{l|c|c}
\whline
Conv. type &  Params & FLOPs \\
\whline
Conventional conv. & $k^2C^2$ & $2k^2C^2HW$\\
Depthwise conv. & $k^2C$ & $2k^2CHW$\\
Inception dep. conv. & $(2k+9)C/8$ & $(2k+9)CHW/4$ \\
\whline
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{\textbf{Complexity of different types of convolution.} For simplicity, assume input and output channels are the same, and the bias term is omitted. $k$, $C$, $H$ and $W$ denote kernel size, channel number, height and width, respectively. The parameters and FLOPs of conventional convolution and depthwise convolution are quadratic to kernel size $k$. In contrast, Inception depthwise convolution is linear to $k$.}
\label{tab:complexity}
\end{table}


\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{figures/two_types_of_conv.pdf}
\end{center}
\vspace{-2mm}
\caption{\textbf{Comparison of FLOPs between depthwise convolution and Inception depthwise convolution.} Inception depthwise convolution is much more efficient than depthwise convolution as kernel size increases.}
\label{fig:two_types}
\end{figure}


\subsection{\modelname{}}
Based on InceptionNeXt block, we can build a series of models named InceptionNeXt. Since ConvNeXt \cite{convnext} is the our main comparing baseline, we mainly follow it to build models with several sizes. Specifically, similar to ResNet \cite{resnet} and ConvNeXt, \modelname{} also adopts 4-stage framework.  The same as ConvNeXt, the numbers of 4 stages are [3, 3, 9, 3] for small size and [3, 3, 27, 3] for base size. 
We adopt Batch Normalization since this paper emphasizes speed.  Another difference with ConvNeXt is that \modelname{} uses an MLP ratio of 3 in stage 4 and moves the saved parameters to the classifier, which can help reduce a few FLOPs (\eg~3\% for base size). The detailed model configurations are reported  in Table \ref{tab:model}.


%%%%%%%%% Table: Model Configurations
\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{2pt}
\input{tables/models}
\caption{\textbf{ Configurations of \modelname{} models.} \modelname{} has similar model configurations to Swin \cite{swin} and ConvNeXt \cite{convnext}. 
}
\label{tab:model}
\end{table}