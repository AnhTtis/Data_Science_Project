\section{Conclusion and future work}
In this work, we propose an effective and efficient CNN InceptionNeXt architecture that enjoys a better trade-off between the practical speed and the performance than previous network architectures. 
InceptionNeXt decomposes large-kernel depthwise convolution along channel dimension into four parallel branches, including identity mapping, a small square kernel, and two orthogonal band kernels. All these four branches are much more  computationally efficient than a large-kernel depthwise convolution in practice, and can also work together to have a  large spatial receptive field for good performance. Extensive experimental results demonstrate the superior performance and also the high practical efficiency  of InceptionNeXt. 

We also notice the speed-up ratios of \modelname{} in inference is smaller than that during training.  
In the future, we will dig into the different speed-up ratios during training and inference, and hope to find a way to further improve the speed-up ratio of InceptionNeXt over previous network architectures in inference. 