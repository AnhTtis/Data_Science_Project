\begin{table}[h]
\begin{center}
\small
% \setlength{\tabcolsep}{2pt}
\begin{tabular}{l | c c c }
\whline
\multirow{2}{*}{\makecell[c]{Model}} &  \multirow{2}{*}{\makecell[c]{Params \\ (M)}} & \multirow{2}{*}{\makecell[c]{MACs \\ (G)}}  & \multirow{2}{*}{\makecell[c]{Top-1 \\ (\%)}} \\
~ & ~ & ~ &  \\
\whline
DeiT-S \cite{deit} & 22 & 4.6 & \textbf{79.8} \\ 
MetaNeXt-Attn & 22 & 4.6 & 3.9  \\ 
ConvNeXt-S (\textit{iso.}) \cite{convnext} & 22 & 4.3 & 79.7 \\
\modelname{}-S (\textit{iso.}) & 22 & 4.2 & 79.7 \\
\hline
DeiT-B \cite{deit} & 87 & 17.6 & 81.8 \\
ConvNeXt-S (\textit{iso.}) \cite{convnext} & 87 & 16.9 & 82.0 \\
\modelname{}-S (\textit{iso.}) & 86 & 16.8 & \textbf{82.1}  \\
\whline
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{\textbf{Comparison among ViT, isotropic ConvNeXt and \modelname{}.}  MetaNeXt-Attn is instantiated from MetaNeXt with token mixer of self-attention \cite{transformer}.}
\label{tab:iso}
\end{table}

\begin{table*}[h]
    \centering
    \setlength{\tabcolsep}{5pt}
    \input{tables/imagenet}
    \caption{\label{tab:imagenet}
    \textbf{Performance of models trained on ImageNet-1K.} The throughputs are measured on an A100 GPU with batch size of 128 and full precision (FP32). Our environment is PyTorch 1.13.0 and NVIDIA CUDA 11.7.1. The better results of ``Channel First" and ``Channel Last"  memory layouts are reported. The numbers in \textcolor{gray}{gray} color are reported by ConvNeXt \cite{convnext}. In our environment, ConvNeXt achieves much higher throughput than the values reported in the paper \cite{convnext}.}
\end{table*}

\section{Experiment}
%%%%%%%%% Table: ADE20K uper
\begin{table}[t]
\centering
\setlength{\tabcolsep}{3.5pt}
\input{tables/ade_uper}
\caption{\textbf{Performance of Semantic segmentation with UperNet \cite{upernet} on ADE20K~\cite{ade20k} validation set.} Images are cropped to $512 \times 512$ for training. The MACs are measured with input size of $512 \times 2048$. }
\label{tab:upernet}
\normalsize
\end{table}

%%%%%%%%% Table: ADE20K
\begin{table}[h!]
\small
\centering
\setlength{\tabcolsep}{3.5pt}
\input{tables/ade_fpn}
\caption{\textbf{Performance of Semantic segmentation with Semantic FPN \cite{fpn} on ADE20K~\cite{ade20k} validation set.} Images are cropped to $512 \times 512$ for training. The MACs are measured with input size of $512 \times 512$. }
\label{tab:fpn}
\normalsize
\end{table}


\subsection{Image classification}
\myPara{Setup}
For the image classification task, ImageNet-1K \cite{imagenet_cvpr, imagenet_ijcv} is one of the most commonly-used benchmarks, which contains around 1.3 million images in the training set and 50 thousand images in the validation set. To fairly compared with the widely-used baselines, \eg  Swin \cite{swin} and ConvNeXt \cite{convnext}, we mainly follow the training hyper-parameters from DeiT \cite{deit} without distillation. Specifically, the models are trained by AdamW \cite{adamw} optimizer with a learning rate $lr = 0.001 \times \mathrm{batch size} / 1024$ ($lr=4e-3$ and $\mathrm{batch size} = 4096$ are used in this paper the same as ConvNeXt). Following DeiT, data augmentation includes standard random resized crop, horizontal flip, RandAugment \cite{randaugment}, Mixup \cite{mixup}, CutMix \cite{cutmix}, Random Erasing \cite{random_erasing} and color jitter. For regularization, label smoothing \cite{inception_v3}, stochastic depth \cite{stochastic_depth}, and weight decay are adopted. Like ConvNeXt, we also use LayerScale \cite{layerscale}, a technique to help train deep models. Our code is based on PyTroch \cite{pytorch} and timm \cite{rw2019timm} libraries. 


\myPara{Results} 
We compare \modelname{} with various state-of-the-art models, including attention-based and convolution-based models. As can be seen in Table \ref{tab:imagenet}, \modelname{} achieves highly competitive performance as well as enjoys higher speed. With similar model sizes and MACs, \modelname{} consistently outperforms ConvNeXt in terms of top-1 accuracy, and also exhibits higher throughput. For example, \modelname{}-T not only surpasses ConvNeXt-T by 0.2\%, but also enjoys $1.6 \times$/$1.2 \times $ training/inference throughputs than ConvNeXts, similar to those of ResNet-50. That is to say, \modelname{}-T enjoys both ResNet-50's speed and ConvNeXt-T's accuracy. 
Moreover,  following Swin and ConvNeXt, we also finetuned the model trained at the resolution of $224 \times 224$ to $384 \times 384$ for 30 epochs.  We can see that \modelname{} still obtains promising performance similar to that of ConvNeXt.


Besides the 4-stage framework \cite{vgg, resnet, swin}, another notable one is ViT-style \cite{vit} isotropic architecture which has only one stage. To match the parameters and MACs of DeiT, we construct \modelname{} (\textit{iso.}) following ConvNeXt \cite{convnext}. Specifically, for the small/base model, we set the embedding dimension as 384/768 and the block number as 18/18. Besides, we build a model called MetaNeXt-Attn which is instantiated from MetaNeXt block by specifying self-attention as token mixer. The aim of this model is to investigate whether it is possible to merge two residual sub-blocks of the Transformer block into a single one. The experiment results are shown in Table \ref{tab:iso}. It can be seen that \modelname{} can also perform well with the isotropic architecture, demonstrating \modelname{} exhibits good generalization across different frameworks. It is worth noting that MetaNeXt-Attn could not be trained to converge and only achieved an accuracy of 3.9\%. This result suggests that, unlike the token mixer in MetaFormer, the token mixer in MetaNeXt cannot be too complex. If it is, the model may not be trainable.


%%%%%%%%% Table: Ablation
\begin{table*}[h]
\centering
\setlength{\tabcolsep}{2pt}
\scalebox{1.0}{\input{tables/ablation}}
\caption{\textbf{Ablation for \modelname{} on ImageNet-1K classification benchmark.} \modelname{}-T is utilized as the baseline for the ablation study. Top-1 accuracy on the validation set is reported. 
}
\label{tab:ablation}
\normalsize
\end{table*}


\subsection{Semantic segmentation}
\myPara{Setup} 
ADE20K~\cite{ade20k}, one of the commonly used scene parsing benchmarks, is used to evaluate our models on semantic segmentation task. ADE20K includes 150 fine-grained semantic categories, containing twenty thousand and two thousand images in the training set and validation set, respectively.
The checkpoints trained on ImageNet-1K \cite{imagenet_cvpr} at the resolution of $224^2$ are utilized to initialize the backbones. Following Swin \cite{swin} and ConvNeXt \cite{convnext}, we firstly evaluate \modelname{} with UperNet \cite{upernet}. The models are trained with AdamW \cite{adamw} optimizer with learning rate of 6e-5 and batch size of 16 for 160K iterations. Following PVT \cite{pvt} and PoolFormer \cite{metaformer}, \modelname{} is also evaluated with Semantic FPN \cite{fpn}. 
In common practices~\cite{fpn,chen2017deeplab}, for the setting of 80K iterations, the batch size is 16. Following PoolFormer \cite{metaformer}, we increase the batch size to 32 and decrease the iterations to 40K, to speed up training. The AdamW \cite{adam, adamw} optimized is adopted with a learning rate of 2e-4 and a polynomial decay schedule of 0.9 power. Our code is based on PyTorch \cite{pytorch} and mmsegmentation library \cite{mmseg2020}.

\myPara{Results}
For segmentation with UpNet \cite{upernet}, the results are shown in Table \ref{tab:upernet}. As can be seen, 
\modelname{} consistently outperforms Swin \cite{swin} and ConvNeXt \cite{convnext} for different model sizes. On the setting of Semantic FPN \cite{fpn} as shown in Table \ref{tab:fpn}, \modelname{} significantly surpasses other backbones, like PVT \cite{pvt} and PoolFormer \cite{metaformer}. These results show that \modelname{} also has a high potential for dense prediction tasks.   





\subsection{Ablation studies}
We conduct ablation studies on ImageNet-1K \cite{imagenet_cvpr, imagenet_ijcv} using \modelname{}-T as baseline from the following aspects.


\myPara{Branch} Inception depthwise convolution includes four branches, three convolutional ones, and identity mapping. When removing any branch of horizontal or vertical band kernel, performance significantly drops from 82.3\% to 81.9\%, demonstrating the importance of these two branches. This is because these two branches with band kernels can enlarge the receptive field of the model. For the branch of small square kernel size of $3\times 3$, removing it can also achieve up to 82.0\% top-1 accuracy and bring higher throughput. This inspires us that if we attach more importance to the model speed, the simple version of \modelname{} without the square kernel of $3\times 3$ can be adopted.  For the band kernel, Inception v3 mostly equips them in a sequential way. We find that this assembling method can also obtain similar performance and even a little speed up the model. A possible reason is that PyTorch/CUDA may have optimized sequential convolutions well, and we only implement the parallel branches at a high level (see Algorithm \ref{alg:code}). We believe the parallel method will be faster when it is optimized better. Thus, parallel method for the band kernels is adopted by default. 


\myPara{Band kernel size} It is found the performance can be improved from kernel size 7 to 11, but it drops when the band kernel size increases to 13. This phenomenon may result from the optimization difficulty and can be solved by methods like structural re-parameterization \cite{repvgg, replknet}. For simplicity, we just set the band kernel size as 11 by default. 

\myPara{Convolution branch ratio} When the ratio increases from $1/8$ to $1/4$, performance improvement can not be observed. Ma \etal \cite{shufflenet_v2} also point out that it is not necessary for all channels to conduct convolution. But when the ratio decreases to $1/16$, it brings a serious performance drop. It is because a smaller ratio would limit the degree of token mixing, resulting in performance drop. Thus, we set the convolution branch ratio as $1/8$ by default.







