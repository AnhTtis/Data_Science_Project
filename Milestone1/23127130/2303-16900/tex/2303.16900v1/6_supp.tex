\section{Preliminary experiments based on ConvNeXt-T}
\label{sec:pre_exp}

We conducted preliminary experiments based on ConvNeXt-T and the results are shown in Table \ref{tab:pre_exp}. Firstly, the kernel size of depthwise convolution is reduced from $7 \times 7$ to $3 \times 3$. Compared to the model with kernel size of $7 \times 7$, the one with kernel size of $3 \times 3$ has $1.4 \times$ training throughput, but it suffers a significant performance drop from 82.1\% to 81.5\%. Next, inspired by ShuffleNet V2 \cite{shufflenet_v2}, we only feed partial input channels into depthwise convolution while the remaining ones keep unchanged. The number of processed input channels is controlled by a ratio. It is found that when the ratio is reduced from 1 to $1/4$, the training throughput can be further improved while the performance almost maintains. For inference throughput, significant improvement can not be observed. A possible reason is that our current code is implemented at API level and has not been optimized at low level. 




\section{Hyper-parameters}
\subsection{ImageNet-1K image classification}
On ImageNet-1K \cite{imagenet_cvpr, imagenet_ijcv} classification benchmark, we adopt the hyper-parameters shown in Table \ref{tab:hyperparameter} to train \modelname{} at the input resolution of $224^2$ and fine-tune it at $384^2$. Our code is implemented by PyTorch \cite{pytorch} based on timm library \cite{rw2019timm}.

\subsection{Semantic segmentation}
For ADE20K \cite{ade20k} semantic segmentation, we utilize ConvNeXt as the backbone with UpNet \cite{upernet} following the configs of Swin \cite{swin}, and FPN \cite{fpn} following the configs of PVT \cite{pvt} and PoolFormer \cite{metaformer}. The backbone is initialized by checkpoints pre-trained on ImageNet-1K at the resolution of $224^2$. The peak stochastic depth rates of the \modelname{} backbone are shown in Table \ref{tab:hyperparameter_ade}. Our implementation is based on PyTorch \cite{pytorch} and mmsegmentation library \cite{mmseg2020}.


\section{Qualitative results}
Grad-CAM \cite{gradcam} is employed to visualize the activation maps of different models trained on ImageNet-1K, including RSB-ResNet-50 \cite{resnet, resnetsb}, Swin-T \cite{swin}, ConvNeXt-T \cite{convnext} and our \modelname{}-T. The results are shown in Figure \ref{fig:grad_cam}. Compared with other models, \modelname{}-T locates key parts more accurately with smaller activation areas.

\begin{table}[h]
\centering
\input{tables/hyperparameters}
\caption{\textbf{Hyper-parameters of \modelname{} on ImageNet-1K image classification.}
\label{tab:hyperparameter}
}
\end{table}



\begin{table}[h]
\centering
\input{tables/hyperparameters_ade20k}
\caption{\textbf{Stochasic depth rate of \modelname{} backbone with UperNet and FPN for ADE20K semantic segmentation.}
\label{tab:hyperparameter_ade}
}
\end{table}



\begin{table*}[h]
\centering
\input{tables/pre_exp}
\caption{\textbf{Preliminary experiments based on ConvNeXt-T. } Convolution ratio means the ratio of channels to be processed by depthwise convolution while the other channels keep unchanged. Throughputs are measured on an A100 GPU with batch size of 128 and full precision (FP32). * The result is reported in ConvNeXt paper \cite{convnext}.
\label{tab:pre_exp}
}
\end{table*}




\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n07873807/ILSVRC2012_val_00016614_224.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n01530575/ILSVRC2012_val_00047878_224.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n02123045/ILSVRC2012_val_00043014_224.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n03495258/ILSVRC2012_val_00006832_224.JPEG}
    \end{subfigure}    
    % \hspace{0.1in}
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n07873807/ILSVRC2012_val_00016614_resnet.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n01530575/ILSVRC2012_val_00047878_resnet.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n02123045/ILSVRC2012_val_00043014_resnet.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n03495258/ILSVRC2012_val_00006832_resnet.JPEG}
    \end{subfigure}  
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n07873807/ILSVRC2012_val_00016614_swin.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n01530575/ILSVRC2012_val_00047878_swin.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n02123045/ILSVRC2012_val_00043014_swin.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n03495258/ILSVRC2012_val_00006832_swin.JPEG}
    \end{subfigure}  
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n07873807/ILSVRC2012_val_00016614_convnext.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n01530575/ILSVRC2012_val_00047878_convnext.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n02123045/ILSVRC2012_val_00043014_convnext.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n03495258/ILSVRC2012_val_00006832_convnext.JPEG}
    \end{subfigure}  
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n07873807/ILSVRC2012_val_00016614_inceptionnext.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n01530575/ILSVRC2012_val_00047878_inceptionnext.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n02123045/ILSVRC2012_val_00043014_inceptionnext.JPEG}
        \includegraphics[width=1\textwidth]{figures/qualitative_results/n03495258/ILSVRC2012_val_00006832_inceptionnext.JPEG}
    \end{subfigure}  
    \begin{center}
    	 ~~~~~~~Input\qquad \qquad \quad RSB-ResNet-50 \cite{resnet, resnetsb} \qquad  Swin-T \cite{deit} \qquad ~~~~ ConvNeXt-T \cite{convnext} \qquad ~~ \modelname{}-T
    \end{center}  
    \caption{
        \label{fig:grad_cam} Grad-CAM \cite{gradcam} activation maps of different models trained on ImageNet-1K. The visualized images are from the validation set of ImageNet-1K. 
    }
\end{figure*}

