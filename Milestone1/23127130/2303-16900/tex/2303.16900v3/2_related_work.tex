\section{Related work}
\vspace{-1mm}
\subsection{Transformer \vs CNN}
\vspace{-1mm}
Transformer \cite{transformer} was introduced in 2017 for NLP tasks because of its parallel training and also better performance than  LSTM. 
Then many famous NLP models are built on Transformer, including GPT series \cite{gpt1, gpt2, gpt3, ouyang2022training}, BERT \cite{bert}, T5 \cite{raffel2020exploring}, and OPT \cite{zhang2022opt}. For the application of the Transformer in vision tasks, Vision Transformer (ViT) is definitely the seminal work, showing that Transformer can achieve impressive performance after large-scale supervised training. Follow-up works \cite{deit, t2t, pvt, tnt, pvtv2, ren2022shunted, ren2023sg} like Swin \cite{swin} continually improve model performance, achieving new state-of-the-art on various vision tasks. These results seem to tell us ``Attention is all you need" \cite{transformer}. 

But it is not that simple. ViT variants like DeiT usually adopt modern training procedures including various advanced techniques of data augmentation \cite{randaugment, autoaugment, mixup, cutmix, random_erasing}, regularization \cite{inception_v3, stochastic_depth} and optimizers \cite{adam, adamw}. Wightman \etal find that with similar training procedures, the performance of ResNet can be largely improved. Besides, Yu \etal \cite{metaformer} argue that the general architecture instead of attention plays a key role in model performance. Han \etal \cite{han2021connection} find by replacing attention in Swin with regular or dynamic depthwise convolution, the model can also obtain comparable performance. ConvNeXt \cite{convnext}, a remarkable work, modernizes ResNet into an advanced version with some designs from ViTs, and the resulting models consistently outperform Swin \cite{swin}. Other works like RepLKNet \cite{replknet}, VAN \cite{van}, FocalNets \cite{focalnet}, HorNet \cite{rao2022hornet}, SLKNet \cite{more_convnets}, ConvFormer \cite{metaformer_baselines}, Conv2Former \cite{hou2022conv2former}, and InternImage \cite{internimage} constantly improve performance of CNNs. 
Despite the high performance obtained, 
these models neglect efficiency, exhibiting lower speed than ConvNeXt. Actually, ConvNeXt is also not an efficient model compared with ResNet. We argue that CNN models should keep the original advantage of efficiency. Thus, in this paper, we aim to improve the model efficiency of CNNs while maintaining high performance.




\subsection{Convolution with large kernels.}
Well-known works, like AlexNet \cite{alexnet} and Inception v1 \cite{inception_v1} already utilize large kernels up to $11 \times 11$ and $7\times 7$, respectively. To improve the efficiency of large kernels, VGG \cite{vgg} proposes to heavily stack $3\times 3$ convolutions while Inception v3 \cite{inception_v3} factorizes $k \times k$ convolution into $1 \times k$ and $k \times 1$ staking sequentially. For depthwise convolution, MixConv \cite{tan2019mixconv} splits kernels into several groups from $3\times 3$ to $k \times k$. Besides, Peng \etal find that large kernels are important for semantic segmentation and they decompose large kernels similar to Inception v3 \cite{inception_v3}. Witnessing the success of Transformer in vision tasks \cite{vit, pvt, swin}, large-kernel convolution is more emphasized since it can offer a large receptive field to imitate attention \cite{han2021connection, convnext}. For example, ConvNeXt adopts kernel size of $7 \times 7 $ for depthwise convolution by default.
To employ larger kernels, RepLKNet \cite{replknet} proposes to utilize structural re-parameterization techniques \cite{zagoruyko2017diracnets, repvgg} to scale up kernel size to $31 \times 31$; VAN \cite{van} sequentially stacks large-kernel depth-wise convolution (DW-Conv) and depth-wise dilation convolution to obtain $21 \times 21$ receptive filed; FocalNets \cite{focalnet} employ a gating mechanism to fuse multi-level features from stacking depthwise convolutions; 
SegNeXt \cite{guo2022segnext} learns multi-scale features by multiple branches of staking $1\times k$ and $k \times 1$. 
Recently, SLaK \cite{more_convnets} factorizes large kernel $k \times k$ into two small non-square kernels ($k \times s$ and $s \times k$ with $s < k$). Unlike these works, we do not aim to scale up larger kernels. Instead, we target efficiency and decompose large kernels in a simple and speed-friendly way while keeping comparable performance.
