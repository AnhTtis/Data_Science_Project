\section{Introduction}

\begin{figure}[t]
\vspace{-6mm}
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/inceptionnext_first_figure.pdf}
\end{center}
\vspace{-7mm}
\caption{\textbf{Trade-off between accuracy and training throughput.} All models are trained under the  DeiT training hyperparameters \cite{deit, swin, convnext, resnetsb}. The training throughput is measured on an A100 GPU with batch size of 128. ConvNeXt-T/k$n$ means variants with depthwise convolution kernel size of $n \times n$. \textbf{\modelname{}-T enjoys both ResNet-50's speed and ConvNeXt-T's accuracy.}  }
\label{fig:first_figure}
\vspace{-4mm}
\end{figure}



\begin{figure*}[htp]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/InceptionNet_block.pdf}
\end{center}
\vspace{-6mm}
\caption{\textbf{Block illustration of MetaFormer, MetaNext, ConvNeXt and \modelname{}.} Similar to MetaFormer block \cite{metaformer}, MetaNeXt is a general block abstracted from ConvNeXt \cite{convnext}. 
MetaNeXt can be regarded as a simpler version obtained from MetaFormer by merging two residual sub-blocks into one. 
It is worth noting that the token mixer used in MetaNeXt cannot be too complex (\eg,~self-attention \cite{transformer}) or it may fail to train to converge.
By specifying the token mixer as depthwise convolution or Inception depthwise convolution, the model is instantiated as ConvNeXt or InceptionNeXt block. Compared with ConvNeXt, InceptionNeXt is more efficient because it decomposes expensive large-kernel depthwise convolution into four efficient parallel branches.}
\label{fig:block}
\vspace{-5mm}
\end{figure*}


Reviewing the history of deep learning \cite{lecun2015deep}, Convolutional Neural Networks (CNNs) \cite{lecun1989backpropagation, lecun1998gradient} are definitely the most popular models in computer vision. The watershed moment arrived in 2012 when AlexNet \cite{alexnet} claimed victory in the ImageNet contest, ushering in a new era for CNNs in computer vision~\cite{alexnet, imagenet_cvpr, imagenet_ijcv}. Since then, a myriad of influential CNNs has emerged like    Network In Network \cite{lin2013network}, VGG \cite{vgg}, Inception Nets \cite{inception_v1}, ResNe(X)t \cite{resnet, resnext}, DenseNet \cite{huang2017densely} and other efficient models \cite{howard2017mobilenets, sandler2018mobilenetv2, zhang2018shufflenet, tan2019efficientnet, tan2021efficientnetv2}. 

 
Motivated by the great achievement of Transformer in NLP, researchers attempt to integrate its modules or blocks into vision CNN models \cite{wang2018non, detr, huang2019ccnet, bello2019attention}, \eg,~the representative works like Non-local Neural Networks \cite{wang2018non} and DETR \cite{detr}, or even make self-attention as stand-alone primitive \cite{ramachandran2019stand, zhao2020exploring}. Moreover, inspired by the language generative pre-training \cite{gpt1}, Image GPT (iGPT) \cite{imageGPT} treats pixels as tokens and adopts pure Transformer for visual self-supervised learning. However, iGPT faces limitations in handling high-resolution images due to computational costs~\citep{imageGPT}. The breakthrough came with Vision Transformer (ViT)~\citep{vit}, which treats  image patches as tokens, leverages a pure Transformer as the backbone, and has demonstrated remarkable performance in image classification after large-scale supervised image pre-training. 


Apparently, the success of ViT \cite{vit} further ignites the enthusiasm for Transformer's application in computer vision. Many ViT variants \cite{deit, t2t, pvt, swin, cswin, focal_transformer, li2022mvitv2}, like DeiT \cite{deit} and Swin \cite{swin}, are proposed and have achieved remarkable performance across a wide range of vision tasks. The superior performance of ViT-like models over traditional CNNs (\eg,~Swin-T's 81.2\% \vs~ResNet-50's 76.1\% on ImageNet \cite{swin, resnet, imagenet_cvpr, imagenet_ijcv}) leads many researchers to believe that Transformers will eventually replace CNNs and dominate the field of computer vision.  


It is time for CNN to fight back. With advanced training techniques in DeiT \cite{deit} and Swin \cite{swin}, the work of ``ResNet strikes back"~\cite{resnetsb} shows that the performance of ResNet-50 can rise by 2.3\%, up to 78.4\%. Further, ConvNeXt \cite{convnext} demonstrates that with modern modules like GELU~\cite{gelu} activation and large kernel size similar to attention window size \cite{swin}, CNN models can consistently outperform Swin Transformer \cite{swin} in various settings and tasks. ConvNeXt is not alone: More and more works have shown similar observations \cite{replknet, van, focalnet, rao2022hornet, more_convnets, metaformer_baselines, internimage, hou2022conv2former}, like RepLKNet \cite{replknet} and SLaK \cite{more_convnets}. Among these modern CNN models, the common key feature is the large receptive field that is usually achieved by depthwise convolution \cite{mamalet2012simplifying, chollet2017xception} with large kernel size (\eg,~$7\times7$).


However, despite its small FLOPs, depthwise convolution is actually an ``expensive" operator because it brings high memory access costs and can be a bottleneck on powerful computing devices, like GPUs \cite{shufflenet_v2}. 
Moreover, as observed in \cite{replknet}, larger kernel sizes lead to significantly lower speeds. 
 As shown in Figure \ref{fig:first_figure}, the ConvNeXt-T with a default $7\times7$ kernel size is $1.4 \times$ slower than that with small kernel size of $3\times3$, and is $1.8 \times $  slower than ResNet-50, although they have similar FLOPs. However, using a smaller kernel size limits the receptive field, which can result in performance degradation. For example, ConvNeXt-T/k3 suffers a performance drop of $0.6\%$ top-1 accuracy on the ImageNet-1K dataset when compared to ConvNeXt-T/k7, where k$n$ denotes a kernel size of $n \times n$.


This poses a challenging problem: How to speed up large-kernel CNNs while preserving their performance? In this paper, we aim to address this issue by building upon ConvNeXt as our baseline and improving the depthwise convolution module.  Through our preliminary experiments based on ConvNeXt (see Table \ref{tab:pre_exp}),
we find that not all input channels need to undergo the computationally expensive depthwise convolution operation \cite{shufflenet_v2}. Accordingly, we propose to leave some channels unaltered and process only a portion of the channels with the depthwise convolution operation. 
Next, we propose to decompose large kernel of depthwise convolution into several groups of small kernels in Inception style \cite{inception_v1, inception_v3, inception_v4}. Specifically, for the processing channels, $1/3$ of channels are conducted with kernel of $3\times3$,  another $1/3$ are with $1\times k$, and the remaining $1/3$ are with $k \times 1$. With this new simple and cheap operator, termed as ``\textit{Inception depthwise convolution}", our built model \textit{\modelname{}} achieves a much better trade-off between accuracy and speed. For example, as shown in Figure \ref{fig:first_figure}, \modelname{}-T achieves higher accuracy than ConvNeXt-T while enjoying $1.6 \times$ speedup of training throughput similar to ResNet-50.


The contributions of this paper are two-fold. Firstly, we identify the speed bottleneck of ConvNeXt as shown in Figure \ref{fig:first_figure}. To solve this speed bottleneck while keeping accuracy, we propose Inception depthwise convolution which decomposes the expensive depthwise convolution into three convolution branches with small kernel sizes as well as a branch of identity mapping. Secondly, extensive experiments on image classification and semantic segmentation show a better speed-accuracy trade-off of our model \modelname{}  than  ConvNeXt. We hope that \modelname{} can serve as a new CNN baseline to speed up the research of neural architecture design.

 


