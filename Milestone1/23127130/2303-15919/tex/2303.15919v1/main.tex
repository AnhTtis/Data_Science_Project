\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress, sort}{natbib}
% before loading neurips_2022

% ready for submission
%\usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[preprint]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{wrapfig}

\usepackage{pgfplots}
\usepackage{tikz}

\hypersetup{
    colorlinks,
    citecolor=green,
    linkcolor=red
}

\definecolor{QPblue}{RGB}{24,78,119}
\definecolor{liblue}{rgb}{0.655,0.835,1.0}
\definecolor{ligreen}{rgb}{0.686,1.0,0.655}
\definecolor{lired}{rgb}{0.961,0.306,0.259}
\definecolor{liyellow}{rgb}{0.988,0.961,0.573}

\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{bm}

\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

\newcommand{\given}{\:\vert\:}
\newcommand{\matr}[1]{\mathrm{\textbf{#1}}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\origin}{\overline{\bm{0}}}
\newcommand{\transp}[3]{\mathrm{PT}^K_{{#1}\rightarrow{#2}}\left({#3}\right)}
\newcommand{\logmap}[2]{\log^K_{#1}(#2)}
\newcommand{\expmap}[2]{\exp^K_{#1}(#2)}
\newcommand{\lprod}[2]{\langle #1,#2\rangle_{\mathcal{L}}}
\newcommand{\lnorm}[1]{||#1||_{\mathcal{L}}}
\newcommand{\tangentsp}[1]{\mathcal{T}_{#1}\mathbb{L}_K^n}
\newcommand{\tangentspN}[2]{\mathcal{T}_{#1}\mathbb{L}_K^{#2}}

\title{Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks}
%\title{Towards Hyperbolic Convolutional Neural Networks in Computer Vision}
%\title{Towards Hyperbolic Geometry in Computer Vision}
%\title{Hyperbolic CNN Framework for Computer Vision}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Ahmad Bdeir\thanks{denotes equal contribution} \\
  %Department of Computer Science\\
  University of Hildesheim\\
  %Pittsburgh, PA 15213 \\
  \texttt{bdeira@uni-hildesheim.de} \\
  % examples of more authors
  \And
  Kristian Schwethelm$^*$\\
  % Affiliation \\
  University of Hildesheim \\
  \texttt{schwethelm@uni-hildesheim.de} \\
  \AND
  Niels Landwehr \\
  % Affiliation \\
  University of Hildesheim \\
  \texttt{landwehr@uni-hildesheim.de} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
   Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current methods in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, the first fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR). Experimentation on standard vision tasks demonstrates the effectiveness of our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic settings. Overall, we aim to pave the way for future research in hyperbolic computer vision by offering a new paradigm for interpreting and analyzing visual data. Our code is publicly available at \url{https://github.com/kschwethelm/HyperbolicCV}.
\end{abstract}


\section{Introduction}

As machine learning models become more complex and datasets grow larger, the ability to accurately represent and manipulate data becomes increasingly important. Traditionally, Euclidean geometry has been used to represent data in machine learning as it offers a convenient, intuition-friendly environment with a defined vector space and stable closed-form formulas. However, recent research has shown that hyperbolic geometry may be better suited for capturing hierarchical structures in data. Specifically, the negative curvature of hyperbolic manifolds allows for distances to scale exponentially with respect to the radius, which matches the exponential scaling of tree distances between graph nodes \cite{hyperbolic_trees}. This can then better retain graph information while preventing spatial distortion. Additionally, even the natural spatial representations in the human brain have shown to exhibit a hyperbolic geometry \cite{zhang2023hippocampal}. 

Leveraging this better representative capacity, Hyperbolic Neural Networks (HNNs) have demonstrated superior performance compared to Euclidean models in many natural language processing (NLP) and graph embedding tasks \cite{hnn-survey}. However, hierarchical structures are not limited to textual and graph-structured data but can also be found in images. The notion of hierarchy in images is particularly established through the concept of part-whole relationships within object representations and classes. In addition, \citet{khrulkov} have found high hyperbolicity in image datasets; where the hyperbolicity measure represents the degree of innate hierarchy between semantic embeddings and can highlight the potential benefits of using HNNs.

In light of these findings, recent works have begun integrating hyperbolic geometry into vision architectures. Specifically, they rely on the Poincaré ball and the Lorentz model as descriptors of hyperbolic space, and formalize hyperbolic translations of Euclidean machine learning operations. Currently, most HNN components are only available in the Poincaré ball as it supports the gyrovector space with basic vector operations. However, due to its hard numerical constraint, the Poincaré ball is much more susceptible to numerical instability than the Lorentz model \cite{mishne-et-al-2022}. Moreover, HNNs in computer vision have been limited to a mixture of Euclidean backbones and hyperbolic task heads due to the lack of required operations and the high computation cost entailed. Until now, fully hyperbolic architectures have mainly been designed for NLP and graph applications \cite{hnn-survey}.

In this work, we propose the first fully hyperbolic framework for vision tasks. We generalize the ubiquitous convolutional neural network (CNN) architecture to the Lorentz model and present novel hyperbolic formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR). Our methodology is general, and we show that our components can be easily integrated into existing architectures. This is done through a series of experiments based on direct translations of Euclidean architectures. Our contributions then become three-fold:

\begin{enumerate}
  \item We extend vision HNNs to the fully hyperbolic setting by proposing the first fully hyperbolic CNN framework dubbed HCNN.
  \item We bridge the gap between HNNs in the Lorentz model and the Poincaré ball by providing novel Lorentzian formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR), which can also be applied outside computer vision.
  \item We empirically demonstrate the effectiveness of HCNN in experiments on standard vision tasks, including image classification and generation.
 \end{enumerate}

 We believe our contributions provide a foundation for the development of more powerful HNNs that can better represent the complex hierarchical structures found in data.

\section{Related works}

\paragraph{Hyperbolic image embeddings} Previous research on HNNs has mainly focused on combining Euclidean backbones and hyperbolic embeddings. This approach involves projecting Euclidean features onto the hyperbolic space in the task heads and designing loss functions based on hyperbolic geometry. Such simple hybrid architectures have been proven effective in various computer vision tasks like recognition \cite{khrulkov, liu2020zeroShot, guo-et-al-2022}, segmentation \cite{hsu20203dImg, atigh-et-al-2022}, reconstruction/generation \cite{mathieu-et-al-2019, nagano-et-al-2019, ovinnikov-et-al-2019, lazcano-et-al-2021, qu-zou-2022}, and metric learning \cite{ermolov-et-al-2022,yue2023contrastive}. However, \citet{guo-et-al-2022} have shown that learning a mixture of Euclidean and hyperbolic features can exacerbate gradient vanishing, and it remains unclear whether these hybrid models can fully exploit the properties of hyperbolic geometry. In contrast, our HCNN naturally learns latent hyperbolic feature representations in every layer, mitigating these issues. We also forgo the typically used Poincaré ball in favor of the Lorentz model, as it is considered to offer better properties for stability and optimization \cite{mishne-et-al-2022}.

\paragraph{Fully hyperbolic neural networks} Designing fully hyperbolic neural networks requires generalizing Euclidean network components to hyperbolic geometry. Notably, \citet{ganea-et-al-2018} and \citet{shimizu-et-al-2020} utilized the Poincaré ball with the gyrovector space and generalized various layers, including the fully-connected, convolutional, and attention layers, as well as operations like split, concatenation, and multinomial logistic regression (MLR). Researchers have also designed components in the Lorentz model \cite{chami-et-al-2019,fan-et-al-2021,chen2021,qu-zou-2022}, but some crucial components are still missing, including the standard convolutional layer and MLR. Following the hyperbolic layer definitions, fully hyperbolic neural networks have dominated various tasks in natural language processing (NLP) and graph applications \cite{hnn-survey}. However, no fully hyperbolic architecture has yet been utilized in computer vision. Our work provides formulations for missing components in the Lorentz model, allowing for the first fully hyperbolic vision CNNs. %Additionally, we propose the missing components for the Lorentz model, closing the gap with the Poincaré ball.

\paragraph{Normalization in HNNs} Standard normalization layers of Euclidean neural networks have found little application in HNNs. To the best of our knowledge, there is only a single hyperbolic normalization layer, i.e., the general Riemannian batch normalization \cite{lou-et-al-2020}. However, this method is not ideal due to the slow iterative computation of the Fréchet mean and the arbitrary re-scaling operation that is not based on hyperbolic geometry. We propose an efficient batch normalization algorithm founded in the Lorentz model, which utilizes the Lorentzian centroid \cite{law-et-al-2019} and a mathematically motivated re-scaling operation.

\paragraph{Numerical stability of HNNs} The exponential growth of the Lorentz model's volume with respect to the radius can introduce numerical instability and rounding errors in floating-point arithmetic. This requires many works to rely on 64-bit precision at the cost of higher memory and runtime requirements. To mitigate this, researchers have introduced feature clipping and Euclidean reparameterizations \cite{mishne-et-al-2022,guo-et-al-2022,mathieu-et-al-2019}. We adopt these approaches in HCNN, allowing us to run under float32 arithmetic and reducing the computational cost.

\section{Background}

This section shortly summarizes the mathematical background of hyperbolic geometry \cite{cannon-1997,ratcliffe-2006}. The $n$-dimensional hyperbolic space $\mathbb{H}^n_K$ is a smooth Riemannian manifold ($\mathcal{M}^n, \mathfrak{g}_{\vect{x}}^K$) with constant negative curvature $K<0$, where $\mathcal{M}^n$ and $\mathfrak{g}_{\vect{x}}^K$ represent the manifold and the Riemannian metric, respectively. There are multiple isometrically equivalent models describing hyperbolic geometry. We build our HCNN upon the Lorentz model because of its numerical stability and simple exponential/logarithmic maps and distance functions. Additionally, we use the Poincaré Ball as a baseline. Both hyperbolic models provide closed-form formulae for standard Riemannian operations, including distance measures, exponential/logarithmic maps, and parallel transportation. They are detailed in Appendix \ref{apdx:hyp_geometry}. 

\begin{wrapfigure}{r}{4.1cm}
\includegraphics[width=4.1cm]{figs/LP_proj.pdf}
\caption{Hyperbolic models.}\label{fig:hyp_mod}
\end{wrapfigure} 

\paragraph{Lorentz model} The $n$-dimensional Lorentz model $\mathbb{L}^n_K = (\mathcal{L}^n, \mathfrak{g}_{\vect{x}}^K)$ models hyperbolic geometry on the upper sheet of a two-sheeted hyperboloid $\mathcal{L}^n$, with origin $\origin = [\sqrt{-1/K}, 0, \cdots, 0]^T$ and embedded in $(n+1)$-dimensional Minkowski space. Based on the Riemannian metric $\mathfrak{g}_{\vect{x}}^K = \mathrm{diag}(-1, 1, \dots, 1)$, the manifold is defined as
\begin{gather}\label{equ:lor_points}
	\mathcal{L}^n := \{\vect{x}\in \mathbb{R}^{n+1}~|~\langle \vect{x}, \vect{x}\rangle_{\mathcal{L}}= \frac{1}{K},~x_t>0\},
\end{gather}
with the Lorentzian inner product
\begin{equation}\label{equ:lor_inner}
	\langle \vect{x}, \vect{y}\rangle_{\mathcal{L}} := -x_ty_t+\vect{x}_s^T\vect{y}_s = \vect{x}^T\mathrm{diag}(-1,1,\cdots,1)\vect{y}.
\end{equation}

When describing points in the Lorentz model, we inherit the terminology of special relativity and call the first dimension the \textit{time component} $x_t$ and the remaining dimensions the \textit{space component} $\vect{x}_s$, such that $\vect{x}\in\mathbb{L}^n_K=[x_t, \vect{x_s}]^T$ and $x_t = \sqrt{||\vect{x}_s||^2-1/K}$.

\paragraph{Poincaré ball} The n-dimensional Poincaré ball $\mathbb{B}^n_K = (\mathcal{B}^n, \mathfrak{g}_{\vect{x}}^K)$ is defined by the Riemannian metric $\mathfrak{g}_{\vect{x}}^K = (\lambda^K_{\vect{x}})^2\matr{I}_n$, where $\lambda_{\vect{x}}^K=2(1+K||\vect{x}||^2)^{-1}$. It describes the hyperbolic space by an open ball of radius $\sqrt{-1/K}$ as follows

\begin{equation}
	\mathcal{B}^n = \{\vect{x} \in\mathbb{R}^n~|~-K||\vect{x}||^2<1\}.
\end{equation}

\section{Fully Hyperbolic CNN (HCNN)}

Our HCNN framework aims to give way to building vision models that can effectively extract features in hyperbolic spaces, fully leveraging the advantages of hyperbolic geometry. For this, we generalize Euclidean CNN components to the Lorentz model, yielding one-to-one replacements that can be integrated into existing architectures. In the following, we first define the cornerstone of HCNNs, i.e., the Lorentz convolutional layer, including its transposed variant. Then, we introduce the Lorentz batch normalization algorithm and the MLR classifier. Finally, we generalize the residual connection and non-linear activation.

\subsection{Lorentz convolutional layer}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=.8\textwidth]{figs/HConv.pdf}
%	\caption[Illustration of the 2-dimensional Lorentz convolutional Layer]{Illustration of the 2-dimensional Lorentz convolutional layer with stride $\delta=1$, kernel size $\tilde{h}, \tilde{w} = 2$, two filters, and input features $\vect{x}_i \in \mathbb{L}^2_K \subset \mathbb{R}^3$. Time components are colored in blue and space components in gray. The Lorentz convolutional layer can be implemented by following steps: (1) Extract feature sets from the input feature map for all receptive fields using the unfolding operation, (2) apply Lorentz direct concatenation to all feature sets, (3) transform all concatenated features using the Lorentz fully-connected layer yielding the output features, and (4) order output features to the correct position in the output feature map.}
%	\label{fig:HConv}
%\end{figure}

\paragraph{Hyperbolic feature maps} The convolutional layer applies vector operations to an input feature map containing the activations of the previous layer. In Euclidean space, arbitrary numerical values can be combined to form a vector. Therefore, it is not required to strictly determine which feature map dimension holds feature vectors. However, in the Lorentz model, not all possible value combinations represent a point that can be processed with hyperbolic operations ($\mathbb{L}_K^n\subset\mathbb{R}^{n+1}$). We propose using channel-last feature map representations throughout the model and adding the time component as an additional channel dimension. This defines a feature map as an ordered set of $n$-dimensional hyperbolic vectors, where every spatial position contains a vector that can be combined with its neighbors. Additionally, it offers a nice interpretation where an image is an ordered set of color vectors, each describing a pixel.

\paragraph{Formalization of the convolutional layer} We define the convolutional layer as an affine transformation between a linearized kernel and a concatenation of the values in its receptive field, following \citet{shimizu-et-al-2020}. Then, we generalize this definition by replacing the Euclidean operators with their hyperbolic counterparts in the Lorentz model.

Given an input feature map $\matr{x} = \{\vect{x}_{h,w} \in \mathbb{L}^n_K\}_{h,w=1}^{H,W}$ as an ordered set of $n$-dimensional hyperbolic feature vectors each describing image pixels, the features within the receptive field of the kernel $\matr{K} \in \mathbb{R}^{m \times n \times \tilde{H} \times \tilde{W}}$ are $\{\vect{x}_{{h'+\delta \tilde{h}},{w'+\delta \tilde{w}}} \in \mathbb{L}^n_K\}_{\tilde{h},\tilde{w}=1}^{\tilde{H},\tilde{W}}$, where $(h', w')$ denotes the starting position and $\delta$ is the stride parameter. Now, we define the Lorentz convolutional layer as

\begin{equation}
	\vect{y}_{h,w} = \text{LFC}(\text{HCat}(\{\vect{x}_{{h'+\delta \tilde{h}},{w'+\delta \tilde{w}}}\}_{\tilde{h},\tilde{w}=1}^{\tilde{H},\tilde{W}}\})),
\end{equation}

where HCat denotes the concatenation of hyperbolic vectors and LFC denotes a Lorentz fully-connected layer performing the affine transformation and parameterizing the kernel and bias, respectively. Additionally, we propose padding with origin vectors, the analog of zero vectors in hyperbolic space. 

Our formulation generalizes arbitrary dimensional convolutional layers with little modification to the 2-dimensional case presented here. However, it remains essential to predetermine the structure of hyperbolic feature maps.

\paragraph{Extension to the transposed setting} The transposed convolutional layer is usually used in encoder-decoder architectures for up-sampling. It is straightforward to generalize to hyperbolic space, as it can be defined using a standard convolutional layer. A convolutional layer carries out a transposed convolution when the correct local connectivity is established by inserting zeros at certain positions. Specifically, when stride $s>1$, then $s-1$ zero vectors are inserted between the features. The input feature map is always implicitly padded on each side. We refer to \citet{trConv} for illustrations. Under this relationship, the Lorentz transposed convolutional layer is a Lorentz convolutional layer with changed connectivity through origin padding.

\subsection{Lorentz batch normalization}

Given a batch $\mathcal{B}$ of $m$ features $\vect{x}_i$, the traditional batch normalization algorithm \cite{bnorm} calculates the mean $\vect{\mu}_{\mathcal{B}} = \frac{1}{m}\sum_{i=1}^{m}\vect{x}_i$ and variance $\vect{\sigma}^2_{\mathcal{B}} = \frac{1}{m}\sum_{i=1}^{m}(\vect{x}_i-\vect{\mu}_{\mathcal{B}})^2$ across the batch dimension. Then, the features are re-scaled and re-centered using a parameterized variance $\vect{\gamma}$ and mean $\vect{\beta}$ as follows

\begin{equation}
	\text{BN}(\vect{x}_i) = \vect{\gamma} \odot \frac{\vect{x}_i - \vect{\mu}_{\mathcal{B}}}{\sqrt{\vect{\sigma}^2_{\mathcal{B}}}} + \vect{\beta}.
\end{equation}

At test time, running estimates approximate the batch statistics. They are calculated iteratively during training: $\vect{\mu}_t = (1-\eta)\vect{\mu}_{t-1} + \eta\vect{\mu}_{\mathcal{B}}$ and $\vect{\sigma}^2_t = (1-\eta)\vect{\sigma}^2_{t-1} + \eta\vect{\sigma}^2_{\mathcal{B}}$, with $\eta$ and $t$ denoting momentum and the current iteration, respectively. We generalize batch normalization to the Lorentz model using the Lorentzian centroid and the parallel transport operation for re-centering, and the Fréchet variance and straight geodesics at the origin's tangent space for re-scaling.

\paragraph{Re-centering} To re-center hyperbolic features, it is necessary to compute a notion of mean. Usually, the Fréchet mean is used \cite{lou-et-al-2020}, which minimizes the expected squared distance between a set of points in a metric space \citep{normal_distr}. Generally, the Fréchet mean must be solved iteratively, massively slowing down training. To this end, we propose to use the centroid with respect to the squared Lorentzian distance, which can be calculated efficiently in closed form. The weighted Lorentzian centroid, which solves $\min_{\vect{\mu} \in \mathbb{L}^n_K}\sum_{i=1}^{m}\nu_i d^2_{\mathcal{L}}(\vect{x}_i, \vect{\mu})$, with $\vect{x}_i \in \mathbb{L}^n_K$ and $\nu_i\geq0, \sum_{i=1}^{m}\nu_i > 0$, is given by

\begin{equation}
	\vect{\mu} = \frac{\sum_{i=1}^{m}\nu_i\vect{x}_i}{\sqrt{-K}\left|||\sum_{i=1}^{m}\nu_i\vect{x}_i||_{\mathcal{L}}\right|}.
\end{equation}

In batch normalization, the mean is not weighted, which gives $\nu_{i} = \frac{1}{m}$. Now, we shift the features from the batch's mean $\vect{\mu}_{\mathcal{B}}$ to the parameterized mean $\vect{\beta}$ using the parallel transport operation $\transp{\vect{\mu}_{\mathcal{B}}}{\vect{\beta}}{\vect{x}}$. Parallel transport does not change the variance, as it is defined to preserve the distance between all points. Finally, the running estimate is updated iteratively using the weighted centroid with $\nu_1=(1-\eta)$ and $\nu_2=\eta$. %Algorithm \ref{alg:bnorm} summarizes the proposed Lorentz batch normalization without variance.

\paragraph{Re-scaling} For re-scaling, we rely on the Fréchet variance $\sigma^2 \in \mathbb{R}^+$, which can be defined as the expected squared Lorentzian distance between a point $\vect{x}_i$ and the mean $\vect{\mu}$, given by $\sigma^2 = \frac{1}{m}\sum_{i=1}^{m}d^2(\vect{x}_i, \vect{\mu})$. To re-scale the batch, features must be moved along the geodesics connecting them to their centroid, which is generally infeasible to compute. However, geodesics intersecting the origin are very simple, as they can be represented by straight lines in tangent space $\tangentsp{\origin}$. This is reflected by the equality between the distance of a point to the origin and the length of its corresponding tangent vector ($d_{\mathcal{L}}(\vect{x},\origin) = ||\logmap{\origin}{\vect{x}}||$). Using this property, we propose to re-scale features by first parallel transporting them towards the origin $\transp{\mu_{\mathcal{B}}}{\origin}{\vect{x}}$, making the origin the new mean. Then, a simple multiplication re-scales the features in tangent space. Finally, parallel transporting to the parameterized mean completes the algorithm and yields the normalized features. The final algorithm of Lorentz batch normalization can be formalized as

\begin{equation}
	\text{LBN}(\vect{x}) = \transp{\origin}{\vect{\beta}}{{\gamma}\cdot \frac{\transp{\vect{\mu}_B}{\origin}{\vect{x}}}{{\sqrt{{\sigma}^2_{\mathcal{B}}}}}}.
\end{equation}

In our definition of hyperbolic feature maps, the centroid must be computed across multiple dimensions. For example, given a feature map $\{\vect{x}_{h,w} \in \mathbb{L}^n_K\}_{h,w=1}^{H,W}$ for all $m$ instances, we first compute the centroid of all hyperbolic vectors per instance and then the centroid across the batch dimension, i.e., the centroid of instance centroids. Similarly, the Fréchet variance is computed for separate instances and then averaged across the batch dimension as it is in $\mathbb{R}^+$. Furthermore, the running estimate of the Fréchet variance is computed using a standard running average.

%Maybe include discussion on the less expressive scalar variance (different regularization on variance -> On average not a square, but a circle)?? And there is also the closeness to the Euclidean bnorm

%\begin{remark}
Notice that in Lorentz batch normalization, the features must be parallel transported twice, which is computationally slower than directly transporting to the new mean, as in \cite{lou-et-al-2020}. However, transporting towards and from the origin are special cases that can be computed efficiently, reducing the overhead significantly. That is why our algorithm still obtains a similar runtime to methods using a single parallel transport operation.
%\end{remark}

%\subsection{Efficient convolutional block}

%Computing multiple time components leads to unnecessary rounding errors \cite{mishne-et-al-2022}. Maybe we can get a mathematical formulation to combine Conv+Bnorm+Activation.

% ambient space = space surrounding a mathematical object, e.g., a hyperplane.

\subsection{Lorentz MLR classifier}

In this section, we consider the problem of classifying instances that are represented in the Lorentz model. A standard method for multi-class classification is multinomial logistic regression (MLR). Inspired by the generalization of MLR to the Poincaré ball \cite{ganea-et-al-2018,shimizu-et-al-2020} based on the distance to margin hyperplanes, we derive a formulation in the Lorentz model.

\paragraph{Hyperplane in the Lorentz model} Analogous to Euclidean space, hyperbolic hyperplanes split the manifold into two half-spaces, which can then be used to separate instances into classes. The hyperplane in the Lorentz model is defined by a geodesic that results from the intersection of an n-dimensional hyperplane with the hyperboloid in the ambient space $\mathbb{R}^{n+1}$ \cite{cho-et-al-2019}. Specifically, for $\vect{p}\in \mathbb{L}^n_K$ and $\vect{w}\in \mathcal{T}_{\vect{p}}\mathbb{L}^n_K$, the hyperplane passing through $\vect{p}$ and perpendicular to $\vect{w}$ is given by

    \begin{align}\label{eq:hyperplane_def}
    	H_{\vect{w},\vect{p}} = \{\vect{x}\in\mathbb{L}^n_K\given\langle \vect{w},\vect{x}\rangle_{\mathcal{L}}=0\}.
    \end{align}

This formulation comes with the non-convex optimization condition $\lprod{\vect{w}}{\vect{w}}>0$, which is undesirable in machine learning. To eliminate this condition, we use the Euclidean reparameterization of \citet{mishne-et-al-2022}, which we extend to include the curvature $K$ in Appendix \ref{apdx:proof_hyperplane}. In short, $\vect{w}$ is parameterized by a vector $\vect{\overline{z}}\in \tangentsp{\origin} = [0,a{\vect{z}/}{||\vect{z}||}]$, where $a \in \mathbb{R}$ and $\vect{z}\in \mathbb{R}^n$. As $\vect{w}\in\tangentsp{\vect{p}}$, $\vect{\overline{z}}$ is parallel transported to $\vect{p}$, which gives

\begin{equation}\label{eq:w_parametr}
    \vect{w}:= \transp{\origin}{\vect{p}}{\overline{\vect{z}}} = [\sinh(\sqrt{-K}a)||\vect{z}||, \cosh(\sqrt{-K}a)\vect{z}]. 
\end{equation}

Inserting Eq. \ref{eq:w_parametr} into Eq. \ref{eq:hyperplane_def}, the formula of the Lorentz hyperplane becomes

\begin{equation}\label{eq:hyppp}
	\tilde{H}_{\vect{z},a} = \{\vect{x}\in \mathbb{L}^n_K~|~ \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t = 0\}.
\end{equation}

Finally, we need the distance to the hyperplane to quantify the model's confidence. It is formulated by the following theorem, proven in Appendix \ref{apdx:proof_dist_hyperpl}. %A closed-form solution for the unit Lorentz model was proven by \citet{cho-et-al-2019}. However, in Appendix \ref{apdx:proof_dist_hyperpl} we extend the solution to the general case, i.e., $K<0$, and employ our reparameterization. As a result, we express the distance to the Lorentz hyperplane as

\begin{theorem} \label{theorem:disthyp}
Given $a\in\mathbb{R}$, $\vect{z}\in\mathbb{R}^n$, the minimum hyperbolic distance from a point $\vect{x}\in\mathbb{L}^n_K$ to the hyperplane $\tilde{H}_{\vect{z},a}$ defined in Eq. \ref{eq:hyppp} is given by
\begin{equation}\label{equ:dist_hyperplane}
    d_{\mathcal{L}}(\vect{x}, \tilde{H}_{\vect{z},a}) = \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t}{\sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2}}\right)\right|.
\end{equation}
\end{theorem}

\paragraph{MLR in the Lorentz model} \citet{mlr} formulated the logits of the Euclidean MLR classifier using the distance from instances to hyperplanes describing the class regions. Specifically, given input $\vect{x}\in\mathbb{R}^n$ and $C$ classes, the output probability of class $c\in\{1,...,C\}$ can be expressed as

\begin{equation}\label{eq:mlrEuclTop}
    p(y=c\given\vect{x}) \propto \exp(v_{\vect{w}_c}(\vect{x})),~~v_{\vect{w}_c}(\vect{x})=\text{sign}(\langle\vect{w}_c,\vect{x}\rangle)||\vect{w}_c||d(\vect{x}, H_{\vect{w}_c}),~~\vect{w}_c\in\mathbb{R}^n,
\end{equation}

where $H_{\vect{w}_c}$ is the decision hyperplane of class $c$. 

We define the Lorentz MLR without loss of generality by inserting the Lorentzian counterparts into Eq. \ref{eq:mlrEuclTop}. The MLR logits are given in the following theorem, proven in Appendix \ref{apdx:proof_lMLR}.

%in Eq. \ref{eq:w_parametr}, Eq. \ref{eq:hyppp}, and Theorem \ref{theorem:disthyp}. %Now, the logits are computed for $\vect{x}\in\mathbb{L}^n_K$ as follows

\begin{theorem}\label{theorem:lorentzMLR}
Given parameters $a_c\in\mathbb{R}$, $\vect{z}_c\in\mathbb{R}^n$, the Lorentz MLR's output logit corresponding to class $c$ and input $\vect{x}\in\mathbb{L}^n_K$ is given by

\begin{equation}\label{eq:logits}
    v_{\vect{z}_c, a_c}(\vect{x})=\frac{1}{\sqrt{-K}}~\mathrm{sign}(\alpha)\beta\left|\sinh^{-1}\left(\sqrt{-K}\frac{\alpha}{\beta}\right)\right|,
\end{equation}

with 

\begin{equation*}
    \alpha = \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a),
\end{equation*} 
\begin{equation*}
    \beta = \sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2}.
\end{equation*}
\end{theorem}

\subsection{Lorentz residual connection and activation}

\paragraph{Residual connection} The residual connection is a crucial component when designing deep CNNs. As vector addition is ill-defined in the Lorentz model, we add the vector's space components and concatenate a corresponding time component. This is possible as a point $\vect{x}\in\mathbb{L}^n_K$ can be defined by an arbitrary space component $\vect{x}_s \in \mathbb{R}^n$ and a time component $x_t = \sqrt{||\vect{x}_s||^2-1/K}$. Our method is straightforward and provides the best empirical performance compared to other viable methods we implemented, i.e., tangent space addition \cite{nickel-kiela-2018}, parallel transport addition \cite{chami-et-al-2019}, Möbius addition (after projecting to the Poincaré ball) \cite{ganea-et-al-2018}, and fully-connected layer addition \cite{chen2021}.

\paragraph{Non-linear activation} Prior works use non-linear activation in tangent space \cite{fan-et-al-2021}, which weakens the model's stability due to frequent logarithmic and exponential maps. We propose to simplify the operation for the Lorentz model by applying the activation function to the space component and concatenating a time component. For example, the Lorentz ReLU activation is given by

\begin{equation}
	\vect{y} = \left[\begin{array}{c}
		\sqrt{||\text{ReLU}(\vect{x}_s)||^2-1/K}\\
		\text{ReLU}(\vect{x}_s)
	\end{array}\right].
\end{equation}

This operation can be interpreted similarly to Euclidean activations that break linearity with heuristic mathematical projections. 

%\begin{remark} When designing HNN components, we aim to leverage the properties of hyperbolic geometry. However, some heuristics, e.g., non-linear activation, have limited mathematical interpretation under hyperbolic geometry. Current research uses the tangent space, but the simpler approach in the Lorentz model is to operate on the space component analogous to \cite{qu-zou-2022}. This is possible as a point $\vect{x}\in\mathbb{L}^n_K$ can be defined by an arbitrary space component $\vect{x}_s \in \mathbb{R}^n$ and a time component $x_t = \sqrt{||\vect{x}_s||^2-1/K}$. Avoiding the tangent space strengthens the model's stability, as no logarithmic and exponential maps are needed. Additionally, this simple mathematical formulation can be easily transferred to other methods, offering a good first baseline where missing components would otherwise hinder the development of HNNs. However, we strongly encourage future research to further explore the properties of hyperbolic geometry and design HNNs that exploit their full potential.
%\end{remark}

\section{Experiments}

We evaluate HCNN models on image classification and generation tasks and compare them against Euclidean and hybrid HNN counterparts. To ensure a fair comparison, in every task, we directly translate a Euclidean baseline to the hyperbolic setting by using hyperbolic modules as one-to-one replacements. All experiments are implemented in PyTorch \citep{pytorch}, and we optimize hyperbolic models using adaptive Riemannian optimizers \cite{riemadam} provided by Geoopt \citep{geoopt2020kochurov}, with floating-point precision set to 32 bits. We provide detailed experimental configurations and results in Appendix \ref{apdx:exp_details}. To facilitate reproducibility and further exploration, we make the code of our experiments publicly available\footnote{\url{https://github.com/kschwethelm/HyperbolicCV}}.

%As a general purpose backbone we build a fully hyperbolic Lorentz ResNet. However, for generation we employ a vanilla convolutional variational autoencoder (VAE) similar to \cite{ghosh-et-al-2019}.

%Overall, our results confirm a certain hyperbolic structure in image datasets, which can be leveraged by explicitly including hyperbolic geometry in neural networks. However, depending on the task and dataset, the improvement can differ. Furthermore, we show that it is advantageous to generalize all layers to hyperbolic space and design a fully hyperbolic neural network instead of only learning hyperbolic embeddings. Overall, the results validate the effectiveness of our proposed fully hyperbolic CNN framework.

\subsection{Image classification}

\paragraph{Experimental setup} In this experiment, we evaluate the performance of HNNs on standard image classification tasks using ResNet-18 \cite{resnet} and three benchmark datasets: CIFAR-10 \cite{cifar}, CIFAR-100 \cite{cifar}, and Tiny-ImageNet \cite{tinyimagenet}. We test two hybrid HNNs, which retain the Euclidean model for embedding images, project the final embeddings onto hyperbolic space, and apply a hyperbolic classification layer. The first hybrid HNN employs the state-of-the-art Poincaré MLR classifier \cite{shimizu-et-al-2020}, while the second model employs our novel Lorentz MLR. Additionally, we must use feature clipping on embeddings to stop gradient issues that arise from the hybrid architecture \cite{guo-et-al-2022}. Furthermore, we create an HCNN model by replacing all Euclidean components in the ResNet architecture with our proposed modules. Finally, for all models, we adopt the training procedure and hyperparameters of \citet{resnetHyp}.

%compare the performance of a Lorentz ResNet based on our HCNN framework against the standard Euclidean ResNet and two hybrid models. For the hybrid models, we employ the Poincaré MLR of \citet{shimizu-et-al-2020} and our proposed Lorentz MLR, respectively. To address gradient issues in hybrid HNNs, we use the feature clipping method of \citet{guo-et-al-2022}, and tune the 

%Following the architecture of prior hybrid HNNs

\paragraph{Main results} Table \ref{tab:classification} summarizes the classification accuracies on all datasets. Our HCNN-ResNet achieves competitive results compared to the Euclidean baseline, even though the hyperparameters are optimized for the Euclidean model. This suggests that much performance can still be gained through optimized architectures and novel hyperbolic training procedures. For example, Euclidean ResNet has greatly benefited from a tighter training regime, with CIFAR-100 performance increasing from 74.84\% for the original ResNet-110 \cite{zagoruyko2017wide} to 77.72\% we obtained for the much smaller ResNet-18.

%For example, in Section \ref{sec:ablations}, we find that HCNNs are superior in a low-dimensional setting.

Comparing performance between HNNs, we find that both Lorentz HNNs exhibit superior stability and performance compared to the Poincaré baseline. Remarkably, our hybrid Lorentzian model is the first HNN to outperform the Euclidean baseline in standard image classification. These findings highlight the potential of our HCNN framework in improving image classification tasks and suggest that the Lorentz model has superior properties for HNNs than the Poincaré ball. % and suggest that hyperbolic geometry can provide useful insights for designing neural networks.

\begin{table}[h]
  \caption{Classification accuracy (\%) of manifold ResNets. We report mean and standard deviation across five runs. The best performance is highlighted in bold (higher is better).}
  \label{tab:classification}
  \centering
  \begin{adjustbox}{width=.74\linewidth,center}
  \begin{tabular}{lccc}
    \toprule
       & CIFAR-10 & CIFAR-100 & Tiny-ImageNet \\
    \midrule
    Euclidean & $\bm{95.14_{\pm0.12}}$ & $77.72_{\pm0.15}$ & $65.09_{ \pm0.12}$   \\
    \midrule
    Hybrid Poincaré & $95.04_{\pm0.13}$ & $77.19_{\pm0.50}$ & $64.93_{\pm0.38}$ \\
    Hybrid Lorentz (Ours) & ${94.98_{\pm0.12}}$ & $\bm{78.03_{\pm0.21}}$ & $\bm{65.63_{\pm0.10}}$ \\
    \midrule
    %Ours + Chen \cite{chen2021} & $\mathbb{L}$ &  & $77.35\pm$ &\\
    HCNN Lorentz (Ours) & $94.93_{\pm0.11}$ & $77.52_{\pm0.10}$ & $64.97_{\pm0.21}$ \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

%\paragraph{Adversarial robustness}

% \subsection{Few-shot learning}

% \paragraph{Experimental setup} We adopt the few-shot learning setting of \citet{khrulkov} and design hyperbolic ProtoNets \cite{protonet} with a ResNet-18 backbone. In the Lorentz model, prototypes are computed by the Lorentzian centroid and distance. Performance is evaluated for the 1-shot and 5-shot setting on CUB \cite{cub} and Mini-ImageNet \cite{imagenet} datasets. See table \ref{tab:fewshot}.

% \paragraph{Main results}


% \begin{table}[h]
%   \caption{Few-shot classification accuracy on CUB and Mini-ImageNet. We report 95\% confidence interval of a single run. The best performance is highlighted in bold (higher is better).}
%   \label{tab:fewshot}
%   \begin{adjustbox}{width=\columnwidth,center}
%   \begin{tabular}{lccccc}
%     \toprule
%     & & \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{Mini-ImageNet} \\
%     \cmidrule(r){3-4}\cmidrule(r){5-6}
%       & Manifold & 1-Shot 5-Way & 5-Shot 5-Way & 1-Shot 5-Way & 5-Shot 5-Way \\
%     \midrule
%     ProtoNet \cite{protonet} & $\mathbb{R}$ & - & - & $57.88 \pm 0.21$  & $75.75 \pm 0.15$ \\
%     \midrule
%     \citet{khrulkov} & $\mathbb{R}+\mathbb{B}$ & $72.86 \pm 0.22$ & $85.69 \pm 0.13$ & $59.47 \pm 0.20$ & $76.84 \pm 0.14$\\
%     %Gao \cite{gao-et-al-2021} & $\mathbb{R}+\mathbb{B}$ &  \\
%     \citet{fang_2021_ICCV} & $\mathbb{R}+\mathbb{B}$ & $74.46 \pm 0.22$ & $89.28 \pm 0.11$ & $61.04 \pm 0.21$ & $77.01 \pm 0.15$\\
%     \citet{guo-et-al-2022} & $\mathbb{R}+\mathbb{B}$ &  \\
%     Ours (Prototypes) & $\mathbb{R}+\mathbb{L}$ & & & &\\
%     \midrule
%     %Ours + Chen \cite{chen2021} & $\mathbb{L}$ & \\
%     Ours (HCNN) & $\mathbb{L}$ & \\
%     \bottomrule
%   \end{tabular}
%   \end{adjustbox}
% \end{table}

\subsection{Image generation}

\paragraph{Experimental setup} Variational autoencoders (VAEs) \cite{kingma, rezende} have been widely adopted in HNN research to model latent embeddings in hyperbolic spaces \cite{nagano-et-al-2019, mathieu-et-al-2019, ovinnikov-et-al-2019, hsu20203dImg}. However, these works rely on hybrid architectures and mainly focus on learning strong embeddings. In this experiment, we extend the hyperbolic VAE to the fully hyperbolic setting using our proposed HCNN framework and, for the first time, evaluate its performance on image generation using the standard Fréchet Inception Distance (FID) metric \cite{FID}. Building on the experimental settings of \citet{ghosh-et-al-2019}, we test vanilla VAEs and assess generative performance on CIFAR-10 \cite{cifar}, CIFAR-100 \cite{cifar}, and CelebA \cite{celeba} datasets. We compare our HCNN-VAE against the Euclidean and two hybrid models. Like most previous works, the hybrid models only include a hyperbolic latent distribution and no hyperbolic layers. Furthermore, we employ the wrapped normal distributions in the Lorentz model \cite{nagano-et-al-2019} and the Poincaré ball \cite{mathieu-et-al-2019}, respectively.

%Try Mathieu with additional poincare fc layer for hyp geometry. 

\paragraph{Main results} The results in Table \ref{tab:results_gen} show that our HCNN-VAE outperforms all baselines. Also, the hybrid models improve performance over the Euclidean model, indicating that learning the latent embeddings in hyperbolic spaces is beneficial. However, our HCNN model is better at leveraging the advantages of hyperbolic geometry due to its fully hyperbolic architecture. These results suggest that our method is a promising approach for modeling latent structures in image data and has the potential to generate high-quality images when employing more complex architectures.

\begin{table}[h]
    \caption{Reconstruction and generation FID of manifold VAEs. We report mean and standard deviation across five runs. The best performance is highlighted in bold (lower is better).}
    \label{tab:results_gen}
	\begin{adjustbox}{width=\columnwidth,center}
	\begin{tabular}{lcccccc}
		\toprule
		& \multicolumn{2}{c}{{CIFAR-10}} &\multicolumn{2}{c}{{CIFAR-100}} & \multicolumn{2}{c}{{CelebA}}\\
        \cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
		 & {Rec. FID} & {Gen. FID} & {Rec. FID} & {Gen. FID} & {Rec. FID} & {Gen. FID}\\
    \midrule
		Euclidean & $61.21_{\pm 0.72}$ & $92.40_{ \pm 0.80}$ & $63.81_{\pm0.47}$ & $103.54_{\pm0.84}$ & $54.80_{\pm0.29}$ & $79.25_{\pm0.89}$\\ \midrule
        Hybrid Poincaré & $59.85_{\pm0.50}$ & $90.13_{\pm0.77}$ & $62.64_{\pm0.43}$ & \bm{$98.19_{\pm0.57}$} & $54.62_{\pm0.61}$ & $81.30_{\pm0.56}$\\
		Hybrid Lorentz & $59.29_{\pm0.47}$ & $90.91_{\pm0.84}$ & $62.14_{\pm0.35}$ & {$98.34_{\pm0.62}$} & $54.64_{\pm0.34}$ & $82.78_{\pm0.93}$\\
        \midrule
        %Ours + Chen \cite{chen2021} & $\mathbb{L}$ & 68.5 & 97.7 & {-} & {-} & - & -\\
		HCNN Lorentz (Ours) & \bm{$57.83_{\pm 0.44}$} & \bm{$89.23_{\pm0.80}$} & \bm{$61.91_{\pm 0.59}$} & $101.09_{\pm0.92}$ & \bm{$54.13_{\pm0.63}$} & \bm{$78.28_{\pm1.01}$}\\
		\bottomrule
	\end{tabular}%
    \end{adjustbox}
\end{table}

\subsection{Ablation studies}\label{sec:ablations}

In this section, we conduct ablation studies to obtain additional insights into HCNN. Unless otherwise stated, all ablation experiments consider image classification on CIFAR-100 and no Poincaré HNN.

\begin{wraptable}{r}{0.4\textwidth}
	\footnotesize
	\centering
	\caption{Runtime improvement using PyTorch's \textit{compile} function on ResNet-18. Duration of a training epoch in seconds.}
	\label{tab:runtime}
	\begin{adjustbox}{width=0.4\textwidth,center}
	\begin{tabular}{@{\extracolsep{4pt}}lrrr@{}}
		\toprule
		& Eucl. & Hybrid & HCNN\\
		\midrule
		Default & \textbf{7.4}  & 9.2 & 103.3 \\
		Compiled & \textbf{7.1} & 8.0 & 62.1 \\
		\midrule
		Difference & -4.1\% &  -13.0\% & \textbf{-39.9\%}\\
		\bottomrule
	\end{tabular}
\end{adjustbox}
\end{wraptable} 

\paragraph{Runtime} Currently, two major drawbacks of HNNs are relatively high runtime and memory requirements. This is partly due to custom pythonic implementations of hyperbolic network components introducing significant computational overhead. To study this, we use PyTorch's \textit{compile} function, which automatically builds a more efficient computation graph. We compare the runtime of our Lorentz ResNets with the Euclidean baseline under compiled and default settings in Table \ref{tab:runtime}. The results show that hybrid HNNs only add little overhead compared to the significantly slower HCNN. This makes scaling HCNNs challenging and requires special attention in future works. However, we also see that hyperbolic models gain much more performance from the automatic compilation than the Euclidean model. This indicates greater room for improvement in terms of implementation optimizations. 

\paragraph{Curvature} The curvature $K$ of hyperbolic space is an additional hyperparameter in HNNs. We investigate the effect of $K$ in Lorentzian ResNets by evaluating classification accuracy with fixed curvatures from 0.2 to 1.4, and a learnable curvature initialized with 1.0. The results are shown in Figure \ref{fig:ablation}. While in the fully hyperbolic model, a curvature deviating from $K=-1$ deteriorates accuracy, the hybrid model is stable against fixed changes in curvature. Furthermore, learning $K$ does not show to be effective in our experimental setting.

\paragraph{Low embedding dimension} HNNs have shown to be most effective in lower-dimensional spaces. To this end, we reduce the dimensionality of the final ResNet block and evaluate classification accuracy. The results in Figure \ref{fig:ablation} verify the effectiveness of hyperbolic spaces with low dimensions, where our HCNN can leverage this property best. This suggests that HCNNs offer great opportunities for designing smaller models with fewer parameters.

\input{ablation_plot}

\section{Conclusion}

In this work, we proposed HCNN, a generalization of the convolutional neural network that learns latent feature representations in hyperbolic spaces. To this end, we formalized the necessary modules in the Lorentz model and derived novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. We demonstrated that ResNet and VAE models based on our HCNN framework achieve competitive performance on standard vision tasks compared to Euclidean and hybrid baselines. Additionally, we showed that using the Lorentz model in HNNs leads to better stability and performance than the Poincaré ball. 

However, HCNNs are still in their early stages, and it remains to be seen to which extent they can replace Euclidean networks as they introduce mathematical complexity and computational overhead. Moreover, our HCNN framework relies on generalizations of networks that were designed for Euclidean geometry and might not fully capture the unique properties of hyperbolic geometry. Further research is needed to fully understand the properties of HCNNs and address open questions such as optimization, scalability, and performance on other machine learning problems. We hope our work will inspire future research and development in this exciting and rapidly evolving field.

%In this work, our Lorentz convolutional layer adopts operations from state-of-the-art Lorentz HNNs, i.e., the Lorentz direct concatenation \cite{qu-zou-2022} and the Lorentz fully-connected layer of \citet{chen2021}. Both operations are crucial for performance and should follow improvements of future works.

%Overall, our work highlights the potential benefits of utilizing hyperbolic geometry in machine learning, particularly in computer vision. We hope that our proposed HCNN framework will inspire future research and development in this exciting and rapidly evolving field.

\begin{ack}

This work was performed on the HoreKa supercomputer funded by the
Ministry of Science, Research and the Arts Baden-Württemberg and by
the Federal Ministry of Education and Research. Ahmad Bdeir was funded by the European Union's Horizon 2020 research and innovation programme under the SustInAfrica grant agreement No 861924. Kristian Schwethelm was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project number 225197905.

%Use unnumbered first level headings for the acknowledgments. All acknowledgments
%go at the end of the paper before the list of references. Moreover, you are required to declare
%funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
%More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.


%Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}


{
\small

\bibliographystyle{abbrvnat}
\bibliography{references}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Checklist}


% %%% BEGIN INSTRUCTIONS %%%
% The checklist follows the references.  Please
% read the checklist guidelines carefully for information on how to answer these
% questions.  For each question, change the default \answerTODO{} to \answerYes{},
% \answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
% justification to your answer}, either by referencing the appropriate section of
% your paper or providing a brief inline description.  For example:
% \begin{itemize}
%   \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
%   \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
%   \item Did you include the license to the code and datasets? \answerNA{}
% \end{itemize}
% Please do not modify the questions and only use the provided macros for your
% answers.  Note that the Checklist section does not count towards the page
% limit.  In your paper, please delete this instructions block and only keep the
% Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%


% \begin{enumerate}


% \item For all authors...
% \begin{enumerate}
%   \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \answerTODO{}
%   \item Did you describe the limitations of your work?
%     \answerTODO{}
%   \item Did you discuss any potential negative societal impacts of your work?
%     \answerTODO{}
%   \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
%     \answerTODO{}
% \end{enumerate}


% \item If you are including theoretical results...
% \begin{enumerate}
%   \item Did you state the full set of assumptions of all theoretical results?
%     \answerTODO{}
%         \item Did you include complete proofs of all theoretical results?
%     \answerTODO{}
% \end{enumerate}


% \item If you ran experiments...
% \begin{enumerate}
%   \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
%     \answerTODO{}
%   \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
%     \answerTODO{}
%         \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
%     \answerTODO{}
%         \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
%     \answerTODO{}
% \end{enumerate}


% \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
% \begin{enumerate}
%   \item If your work uses existing assets, did you cite the creators?
%     \answerTODO{}
%   \item Did you mention the license of the assets?
%     \answerTODO{}
%   \item Did you include any new assets either in the supplemental material or as a URL?
%     \answerTODO{}
%   \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
%     \answerTODO{}
%   \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
%     \answerTODO{}
% \end{enumerate}


% \item If you used crowdsourcing or conducted research with human subjects...
% \begin{enumerate}
%   \item Did you include the full text of instructions given to participants and screenshots, if applicable?
%     \answerTODO{}
%   \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
%     \answerTODO{}
%   \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
%     \answerTODO{}
% \end{enumerate}


% \end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\appendix

\section{Operations in hyperbolic geometry}\label{apdx:hyp_geometry}

\subsection{Lorentz model}\label{apdx:lorentz_model}

\begin{figure}[h]
	\centering
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=.9\textwidth]{figs/hyp-distance.pdf}
		\caption{Geodesic}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=.9\textwidth]{figs/hyp-tangent.pdf}
		\caption{Tangent space}
	\end{subfigure} 
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=.9\textwidth]{figs/hyp-parallel.pdf}
		\caption{Parallel Transport}
	\end{subfigure}
	\hfill
	\caption[Illustrations of geometrical operations in the 2-dimensional Lorentz model]{Illustrations of geometrical operations in the 2-dimensional Lorentz model. (a) The shortest distance between two points is represented by the connecting geodesic (red line). (b) The red line gets projected onto the tangent space of the origin resulting in the green line. (c) The green line gets parallel transported to the tangent space of the origin.}
	\label{fig:LorentzOps}
\end{figure}

\begin{figure}[h]
	\centering
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=.65\textwidth]{figs/LRot.pdf}
		\caption{Lorentz rotation}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=.65\textwidth]{figs/LBoost.pdf}
		\caption{Lorentz boost}
	\end{subfigure}
	\hfill
	\caption{Illustration of the Lorentz transformations in the 2-dimensional Lorentz model.}
	\label{fig:LorentzTransformations}
\end{figure}

In this section, we describe essential geometrical operations in the Lorentz model. Most of these operations are defined for all Riemannian manifolds and thus introduced for the general case first. However, the closed-form formulae are only given for the Lorentz model. We also provide visual illustrations in Figure \ref{fig:LorentzOps}.

\paragraph{Distance}

Distance is defined as the length of the shortest path between a pair of points on a surface. While in Euclidean geometry, this is a straight line, in hyperbolic space, the shortest path is represented by a curved geodesic generalizing the notion of a straight line. In the Lorentz model, the distance is inherited from Minkowski space. Let $\vect{x}, \vect{y} \in \mathbb{L}^n_K$ denote two points in the Lorentz model. Then, the length of the connecting geodesic and, thereby, the distance is given by
\begin{equation}
	d_{\mathcal{L}}(\vect{x},\vect{y}) = \frac{1}{\sqrt{-K}}\cosh^{-1}(K\langle \vect{x},\vect{y}\rangle_{\mathcal{L}}),
\end{equation}
and the squared distance \cite{law-et-al-2019} by
\begin{equation}
	d^2_{\mathcal{L}}(\vect{x},\vect{y}) = ||\vect{x}-\vect{y}||_{\mathcal{L}}^2 = \frac{2}{K}-2\lprod{\vect{x}}{\vect{y}}.
\end{equation}

When calculating the distance of any point $\vect{x} \in \mathbb{L}^n_K$ to the origin $\origin$, the equations can be simplified to

\begin{equation}
	d_{\mathcal{L}}(\vect{x},\origin) = ||\logmap{\origin}{\vect{x}}||,
\end{equation}

\begin{equation}
	d_{\mathcal{L}}^2(\vect{x},\origin) = \frac{2}{K} (1+\sqrt{-K}x_t).
\end{equation}

\paragraph{Tangent space}

The space around each point $\vect{x}$ on a differentiable manifold $\mathcal{M}$ can be linearly approximated by the tangent space $\mathcal{T}_{\vect{x}}\mathcal{M}$. It is a first-order approximation bridging the gap to Euclidean space. This helps performing Euclidean operations, but it introduces an approximation error, which generally increases with the distance from the reference point. Let $\vect{x} \in \mathbb{L}^n_K$, then the tangent space at point $\vect{x}$ can be expressed as
\begin{equation}
	\mathcal{T}_{\vect{x}}\mathbb{L}_K^n := \{\vect{y}\in \mathbb{R}^{n+1}~|~\langle \vect{y}, \vect{x}\rangle_{\mathcal{L}}=0\}.
\end{equation}

\paragraph{Exponential and logarithmic maps}

Exponential and logarithmic maps are mappings between the manifold $\mathcal{M}$ and the tangent space $\mathcal{T}_{\vect{x}}\mathcal{M}$ with $\vect{x}\in \mathcal{M}$. The exponential map $\expmap{\vect{x}}{\vect{z}}: \tangentsp{\vect{x}} \rightarrow \mathbb{L}^n_K$ maps a tangent vector $\vect{z} \in \tangentsp{\vect{x}}$ on the Lorentz manifold by

\begin{gather}
	\exp_{\vect{x}}^K(\vect{z}) = \cosh(\alpha) \vect{x} + \sinh(\alpha)\frac{\vect{z}}{\alpha}, \text{ with  }
	\alpha = \sqrt{-K}||\vect{z}||_{\mathcal{L}},~~
	||\vect{z}||_{\mathcal{L}}=\sqrt{{\langle \vect{z},\vect{z}\rangle}_{\mathcal{L}}}.
\end{gather}

The logarithmic map is the inverse mapping and maps a vector $\vect{y}\in\mathbb{L}^n_K$ to the tangent space of $\vect{x}$ by
\begin{gather}
	\log_{\vect{x}}^K(\vect{y}) = \frac{\cosh^{-1}(\beta)}{\sqrt{\beta^2-1}}\cdot (\vect{y}-\beta \vect{x}), \text{ with  }
	\beta=K\langle \vect{x},\vect{y}\rangle_{\mathcal{L}}.
\end{gather}

In the special case of working with the tangent space at the origin $\origin$, the exponential map simplifies to

\begin{align}
	%\overline{\vect{z}}&=[0,\vect{z}]\\
	%\alpha &= \sqrt{-K}||\vect{z}||_{\mathcal{L}}\\
	%	&= \sqrt{-K}  \sqrt{{\langle \vect{z},\vect{z}\rangle}_{\mathcal{L}}}\\
	%	&= \sqrt{-K}  ||z||\\
	%\exp_{\overline{0}}^K(\vect{z}) &= \cosh(\alpha) \overline{0} + \sinh(\alpha)\frac{\vect{z}}{\alpha}\\
	%&= \left[\cosh(\alpha) \frac{1}{\sqrt{-K}},~\sinh(\alpha)\frac{\vect{z}}{\alpha}\right]\\
	\exp_{\origin}^K(\vect{z}) = \frac{1}{\sqrt{-K}}\left[\cosh(\sqrt{-K}  ||\vect{z}||),~\sinh(\sqrt{-K}  ||\vect{z}||)\frac{\vect{z}}{ ||\vect{z}||}\right].
\end{align}

%and the logarithmic map simplifies to

%\begin{align}
	%\beta&=K\langle \overline{0},\vect{y}\rangle_{\mathcal{L}} = \sqrt{-K} y_t\\
	%\log_{\overline{0}}^K(\vect{y}) &= \frac{\cosh^{-1}(\beta)}{\sqrt{\beta^2-1}} (\vect{y}-\beta \overline{0})\\
	%&= \frac{\cosh^{-1}(\sqrt{-K} y_t)}{\sqrt{\sqrt{-K} y_t^2-1}} (\vect{y}-\sqrt{-K} y_t \overline{0})\\
 %	&= \frac{\cosh^{-1}(\sqrt{-K} y_t)}{\sqrt{\sqrt{-K} y_t^2-1}}\cdot (\vect{y}-[1,0,\dots,0]^T)\\
%	\log_{\origin}^K(\vect{y})&= \frac{\sqrt{-K}d_{\mathcal{L}}(\overline{0},\vect{y})}{\sqrt{\sqrt{-K} y_t^2-1}} (\vect{y}-[1,0,\dots,0]^T)
%\end{align}

\paragraph{Parallel transport}

The parallel transport operation $\transp{\vect{x}}{\vect{y}}{\vect{v}}$ maps a vector $\vect{v} \in \mathcal{T}_{\vect{x}}\mathcal{M}$ from the tangent space of $\vect{x} \in \mathcal{M}$ to the tangent space of $\vect{y} \in \mathcal{M}$. It preserves the local geometry around the reference point by moving the points along the geodesic connecting $\vect{x}$ and $\vect{y}$. %Additionally, it preserves the hyperbolic distances between all points. 
The formula for the Lorentz model is given by
% Preserves distances to the reference point.

\begin{align}
	\transp{\vect{x}}{\vect{y}}{\vect{v}} &= \vect{v} - \frac{\lprod{\logmap{\vect{x}}{\vect{y}}}{\vect{v}}}{d_{\mathcal{L}}(\vect{x},\vect{y})}(\logmap{\vect{x}}{\vect{y}}+ \logmap{\vect{y}}{\vect{x}})\\
	&= \vect{v} + \frac{\lprod{\vect{y}}{\vect{v}}}{\frac{1}{-K}-\lprod{\vect{x}}{\vect{y}}}(\vect{x}+\vect{y}).
\end{align}

%In the special case of $\vect{x}=\origin$ the equation simplifies to

%\begin{align}
%	\transp{\origin}{\vect{y}}{\vect{v}} &= ...
%\end{align}

%In the special case of $\vect{y}=\origin$ the equation simplifies to

%\begin{align}
%	\transp{\vect{x}}{\origin}{\vect{v}} &= ...
%\end{align}

\paragraph{Lorentzian centroid \cite{law-et-al-2019}} The weighted centroid with respect to the squared Lorentzian distance, which solves $\min_{\vect{\mu} \in \mathbb{L}^n_K}\sum_{i=1}^{m}\nu_i d^2_{\mathcal{L}}(\vect{x}_i, \vect{\mu})$, with $\vect{x}_i \in \mathbb{L}^n_K$ and $\nu_i\geq0, \sum_{i=1}^{m}\nu_i > 0$, is given by

\begin{equation}
	\vect{\mu} = \frac{\sum_{i=1}^{m}\nu_i\vect{x}_i}{\sqrt{-K}\left|||\sum_{i=1}^{m}\nu_i\vect{x}_i||_{\mathcal{L}}\right|}.
\end{equation}

\paragraph{Lorentz transformations}

The set of linear transformations in the Lorentz model are called Lorentz transformations. A transformation matrix $\matr{A}^{(n+1)\times(n+1)}$ that linearly maps $\mathbb{R}^{n+1} \rightarrow \mathbb{R}^{n+1}$ is called Lorentz transformation if and only if $\lprod{\matr{A}\vect{x}}{\matr{A}\vect{y}} = \lprod{\vect{x}}{\vect{y}}~\forall~\vect{x},\vect{y} \in \mathbb{R}^{n+1}$. The set of matrices forms an orthogonal group $\bm{O}(1,n)$ called the Lorentz group. As the Lorentz model only uses the upper sheet of the two-sheeted hyperboloid, the transformations under consideration here lie within the positive Lorentz group $\bm{O}^+(1,n) = \{\matr{A}\in \bm{O}(1,n): a_{11}>0\}$, preserving the sign of the time component $x_t$ of $\vect{x}\in\mathbb{L}^n_K$. Specifically, here, the Lorentz transformations can be formulated as 

\begin{equation}
	\bm{O}^+(1,n) = \{\matr{A} \in\mathbb{R}^{(n+1)\times (n+1)}~|~\forall \vect{x} \in \mathbb{L}^n_K: \lprod{\matr{A}\vect{x}}{\matr{A}\vect{x}} = \frac{1}{K}, (\matr{A}\vect{x})_0>0)\}.
\end{equation}

Any Lorentz transformation can be decomposed into a Lorentz rotation and Lorentz boost by polar decomposition $\matr{A} = \matr{R}\matr{B}$ \cite{moretti-2002}. The former rotates points around the time axis, using matrices given by

\begin{equation}
	\matr{R} = \left[\begin{array}{cc}
		1 & \vect{0}^T\\ \vect{0} & \tilde{\matr{R}}
\end{array}\right],
\end{equation}

where $\vect{0}$ is a zero vector, $\tilde{\matr{R}}^T\tilde{\matr{R}} = \matr{I}$, and $\det(\tilde{\matr{R}})=1$. This shows that the Lorentz rotations for the upper sheet lie in a special orthogonal subgroup $\bm{SO}^+(1,n)$ preserving the orientation, while $\tilde{\matr{R}} \in \bm{SO}(n)$. On the other side, the Lorentz boost moves points along the spatial axis given a velocity $\vect{v}\in\mathbb{R}^n, ||\vect{v}||<1$ without rotating them along the time axis. Formally, the boost matrices are given by

\begin{equation}
	\matr{B} = \left[\begin{array}{cc}
		\gamma & -\gamma \vect{v}^T\\
		-\gamma \vect{v} & \matr{I}+\frac{\gamma^2}{1+\gamma}\vect{v}\vect{v}^T
	\end{array}
	\right],
\end{equation}

with $\gamma = \frac{1}{\sqrt{1-||\vect{v}||^2}}$. See Figure \ref{fig:LorentzTransformations} for illustrations of the Lorentz rotation and Lorentz boost.

\paragraph{Lorentz fully-connected layer} Recently, \citet{chen2021} showed that the linear transformations performed in the tangent space \cite{ganea-et-al-2018,nickel-kiela-2018} can not apply all Lorentz transformations but only a special rotation and no boost. They proposed a direct method in pseudo-hyperbolic space\footnote{\citet{chen2021} note that their general formula is not fully hyperbolic, but a relaxation in implementation, while the input and output are still guaranteed to lie in the Lorentz model.}, which can apply all Lorentz transformations. Specifically, let $\vect{x}\in\mathbb{L}^n_K$ denote the input vector and $\matr{W}\in \mathbb{R}^{m\times n+1}$, $\vect{v}\in \mathbb{R}^{n+1}$ the weight parameters, then the transformation matrix is given by

\begin{equation}\label{eq:lolin}
	f_{\vect{x}}(\matr{M}) = f_{\vect{x}}\left(\left[\begin{array}{c}
		\vect{v}^T\\
		\matr{W}
	\end{array}\right]\right) =  \left[\begin{array}{c}
		\frac{\sqrt{||\matr{W}\vect{x}||^2-1/K}}{\vect{v}^T\vect{x}}\vect{v}^T\\
		\matr{W}
	\end{array}
	\right].
\end{equation}

Adding other components of fully-connected layers, including normalization, the final definition of the proposed Lorentz fully-connected layer becomes

\begin{equation}
	\vect{y} = \left[\begin{array}{c}
		\sqrt{||\phi(\matr{W}\vect{x}, \vect{v})||^2-1/K}\\
		\phi(\matr{W}\vect{x}, \vect{v})
	\end{array}
	\right],
\end{equation}

with operation function

\begin{equation}
	\phi(\matr{W}\vect{x},\vect{v}) = \lambda\sigma(\vect{v}^T\vect{x}+\vect{b}')\frac{\matr{W}\psi(\vect{x})+\vect{b}}{||\matr{W}\psi(\vect{x})+\vect{b}||},
\end{equation}

where $\lambda>0$ is a learnable scaling parameter and $\vect{b}\in\mathbb{R}^n,\psi,\sigma$ denote the bias, activation, and sigmoid function, respectively. 

In this work, we simplify the layer definition by removing the internal normalization, as we use batch normalization. This gives following formula for the Lorentz fully connected layer

\begin{equation}
	\vect{y} =  \text{LFC}(\vect{x}) = \left[\begin{array}{c}
		\sqrt{||\psi(\matr{W}\vect{x} + \vect{b})||^2-1/K}\\
		\psi(\matr{W}\vect{x} + \vect{b})
	\end{array}
	\right].
\end{equation}

\paragraph{Lorentz direct concatenation \cite{qu-zou-2022}} Given a set of hyperbolic points $\{\vect{x}_i \in \mathbb{L}^n_K\}_{i=1}^N$, the Lorentz direct concatenation is given by

\begin{equation}
	\vect{y} = \mathrm{HCat}(\{\vect{x}_i\}_{i=1}^N) = \left[\sqrt{\sum_{i=1}^{N}x^2_{i_t} + \frac{N-1}{K}}, \vect{x}^T_{1_s},\dots, \vect{x}^T_{N_s}\right]^T,
\end{equation}

where $\vect{y}  \in \mathbb{L}^{nN}_K \subset \mathbb{R}^{nN+1}$.

\subsection{Poincaré ball} 

The n-dimensional Poincaré ball $\mathbb{B}^n_K = (\mathcal{B}^n, \mathfrak{g}_{\vect{x}}^K)$ is defined by the Riemannian metric $\mathfrak{g}_{\vect{x}}^K = (\lambda^K_{\vect{x}})^2\matr{I}_n$, where $\lambda_{\vect{x}}^K=2(1+K||\vect{x}||^2)^{-1}$ and by the open ball

\begin{equation}
	\mathcal{B}^n = \{\vect{x} \in\mathbb{R}^n~|~-K||\vect{x}||^2<1\}.
\end{equation}

%The distance function is given by

%\begin{equation}
%    d_{\mathcal{B}}(\vect{x},\vect{y}) = \frac{1}{\sqrt{-K}} \cosh^{-1}\left(1+\frac{2|\vect{x}-\vect{y}|^2}{(1-|\vect{x}|^2)(1-|\vect{y}|^2)}\right).
%\end{equation}

%TODO: A FEW POINCARE OPERATIONS

%\paragraph{Distance} 

\subsection{Mapping between models} 

Because of the isometry between models of hyperbolic geometry, points in the Lorentz model can be mapped to the Poincaré ball by the following diffeomorphism

\begin{equation}
	p_{\mathbb{L}^n_K\rightarrow\mathbb{B}^n_K}(\vect{x}) = \frac{\vect{x_s}}{x_t + \frac{1}{\sqrt{-K}}}.
\end{equation}

%TODO: ALSO SHOW THE OTHER DIRECTION

\section{Proofs}

\subsection{Proofs for Lorentz hyperplane}\label{apdx:proof_hyperplane}

This section contains the proof for the Euclidean reparameterization of the Lorentz hyperplane proposed by \citet{mishne-et-al-2022}. Unfortunately, the authors only provided proof for the unit Lorentz model, i.e., assuming a curvature of $K=-1$. However, in general, the curvature can be different $K<0$. That is why we reproduce their proof for the general case.

\paragraph{Proof for Eq. \ref{eq:w_parametr}}

Let $a\in\mathbb{R}$, $\vect{z}\in \mathbb{R}^n$, and $\vect{\overline{z}}\in \tangentsp{\origin} = [0,a{\vect{z}/}{||\vect{z}||}]$. Then, \citet{mishne-et-al-2022} parameterize a point in the Lorentz model as follows

\begin{align}
	\vect{p}\in\mathbb{L}^n_K &:= \exp_{\origin}\left(a\frac{\vect{z}}{||\vect{z}||}\right)\\
	&= \left[\frac{1}{\sqrt{-K}}\cosh(\alpha),~\sinh(\alpha)\frac{a\frac{\vect{z}}{||\vect{z}||}}{\alpha}\right].
\end{align}

Now, with $\alpha = \sqrt{-K}||a\frac{\vect{z}}{||\vect{z}||}||_{\mathcal{L}} = \sqrt{-K} a$ we get

\begin{align}
	\vect{p} &= \left[\cosh(\sqrt{-K} a) \frac{1}{\sqrt{-K}},~\sinh(\sqrt{-K} a)\frac{a\frac{\vect{z}}{||\vect{z}||}}{\sqrt{-K} a}\right]\\
	&= \frac{1}{\sqrt{-K}}\left[\cosh(\sqrt{-K} a),~\sinh(\sqrt{-K} a)\frac{\vect{z}}{||\vect{z}||}\right].
\end{align}

This definition gets used to reparameterize the hyperplane parameter $\vect{w}$ as follows

\begin{align*}
	\vect{w} &:= \transp{\origin}{\vect{p}}{\overline{\vect{z}}}\\
	&= \overline{\vect{z}} + \frac{\lprod{\vect{p}}{\overline{\vect{z}}}}{\frac{1}{-K}-\lprod{\origin}{\vect{p}}}(\origin+\vect{p})\\
	&= [0,\vect{z}]^T + \frac{\lprod{\vect{p}}{[0,\vect{z}]^T}}{\frac{1}{-K}-\lprod{\origin}{\vect{p}}}(\origin+\vect{p})\\
	&= [0,\vect{z}]^T + \frac{\frac{1}{\sqrt{-K}}\sinh(\sqrt{-K}a)||\vect{z}||}{\frac{1}{-K}+\frac{1}{-K}\cosh(\sqrt{-K}a)} \cdot \frac{1}{\sqrt{-K}}\left[1 + \cosh(\sqrt{-K}a),~\sinh(\sqrt{-K}a)\frac{\vect{z}}{||\vect{z}||}\right]\\
	&= [0,\vect{z}]^T + \frac{\sinh(\sqrt{-K}a)||\vect{z}||}{1+\cosh(\sqrt{-K}a)} \cdot \left[1 + \cosh(\sqrt{-K}a),~\sinh(\sqrt{-K}a)\frac{\vect{z}}{||\vect{z}||}\right]\\
	&= \left[\sinh(\sqrt{-K}a)||\vect{z}||,~\vect{z}+\frac{\sinh^2(\sqrt{-K}a)}{1+\cosh(\sqrt{-K}a)}\vect{z}\right]\\
	&= \left[\sinh(\sqrt{-K}a)||\vect{z}||,~\vect{z}+\frac{\cosh^2(\sqrt{-K}a)-1}{1+\cosh(\sqrt{-K}a)}\vect{z}\right]\\
	&= [\sinh(\sqrt{-K}a)||\vect{z}||,~\cosh(\sqrt{-K}a)\vect{z}].
\end{align*}

\paragraph{Proof for Eq. \ref{eq:hyppp}} 

After inserting Eq. \ref{eq:w_parametr} into Eq. \ref{eq:hyperplane_def} and solving the inner product, the hyperplane definition becomes

\begin{equation}
	\tilde{H}_{\vect{z},a} = \{\vect{x}\in \mathbb{L}^n_K~|~ \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t = 0\}.
\end{equation}

\subsection{Proof for distance to Lorentz hyperplane}\label{apdx:proof_dist_hyperpl}

%\paragraph{Proof for Eq. \ref{equ:dist_hyperplane}} 
\paragraph{Proof for Theorem \ref{theorem:disthyp}}
To proof the distance to a hyperplane in the Lorentz model, we follow the approach of \citet{cho-et-al-2019} and utilize the hyperbolic reflection. The idea is, that a hyperplane defines a reflection that interchanges two half-spaces. Therefore, the distance from a point $\vect{x}\in\mathbb{L}^n_K$ to the hyperplane $H_{\vect{w},\vect{p}}$ can be calculated by halving the distance to its reflection in the hyperplane $\vect{x}\rightarrow\vect{y}_{\vect{w}}$

\begin{equation}\label{eq:dist1}
    d_{\mathcal{L}}(\vect{x},H_{\vect{w},\vect{p}}) = \frac{1}{2}d_{\mathcal{L}}(\vect{x},\vect{y}_{\vect{w}}).
\end{equation}

The hyperbolic reflection is well-known in the literature \cite{reflection} and can be formulated as

\begin{equation}\label{eq:reflection}
    \vect{y}_{\vect{w}} = \vect{x}+\frac{2\lprod{\vect{w}}{\vect{x}}\vect{w}}{\lprod{\vect{w}}{\vect{w}}},
\end{equation}

where $\vect{w}$ is the perpendicular vector to the hyperplane and $\lprod{\vect{w}}{\vect{w}}>0$. Now, inserting Equation \ref{eq:reflection} into Equation \ref{eq:dist1} we can compute the distance to the hyperplane as follows

\begin{align*}
    d_{\mathcal{L}}(\vect{x},H_{\vect{w},\vect{p}}) &= \frac{1}{2\sqrt{-K}}\cosh^{-1}(K\lprod{\vect{x}}{\vect{y}_{\vect{w}}})\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}(K\lprod{\vect{x}}{\vect{x}+\frac{2\lprod{\vect{w}}{\vect{x}}\vect{w}}{\lprod{\vect{w}}{\vect{w}}}})\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}(2K\lprod{\vect{x}}{\vect{x}} + K \lprod{\vect{x}}{\frac{\lprod{\vect{w}}{\vect{x}}\vect{w}}{\lprod{\vect{w}}{\vect{w}}}})\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}\left(K\frac{1}{K} + 2K \left(\frac{\lprod{\vect{w}}{\vect{x}}}{\sqrt{\lprod{\vect{w}}{\vect{w}}}}\right)^2\right)\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}\left(1 + 2 \left(\sqrt{-K}\frac{\lprod{\vect{w}}{\vect{x}}}{\sqrt{\lprod{\vect{w}}{\vect{w}}}}\right)^2\right)\\[5pt]
    &= \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\lprod{\vect{w}}{\vect{x}}}{\lnorm{\vect{w}}}\right)\right|,
\end{align*}
which gives the final formula:
\begin{equation}\label{eq:stdDistHyp}
    d_{\mathcal{L}}(\vect{x},H_{\vect{w},\vect{p}}) = \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\lprod{\vect{w}}{\vect{x}}}{\lnorm{\vect{w}}}\right)\right|
\end{equation}


Comparing Eq. \ref{eq:stdDistHyp} to the formula of \citet{cho-et-al-2019} shows that the distance formula of hyperplanes in the unit Lorentz model can be extended easily to the general case by inserting the curvature parameter $K$ at two places.

Finally, defining $\vect{w}$ with the aforementioned reparameterization

\begin{equation}\label{eq:wrepara}
    \vect{w} := \transp{\origin}{\vect{p}}{\overline{\vect{z}}} = [\sinh(\sqrt{-K}a)||\vect{z}||, \cosh(\sqrt{-K}a)\vect{z}],
\end{equation}

and solving the inner products, gives our final distance formula

\begin{align}
    d_{\mathcal{L}}(\vect{x}, \tilde{H}_{\vect{z},a}) = \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t}{\sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2}}\right)\right|.
\end{align}

\subsection{Proof for logits in the Lorentz MLR classifier}\label{apdx:proof_lMLR}

\paragraph{Proof for Theorem \ref{theorem:lorentzMLR}}

Following \citet{mlr}, given input $\vect{x}\in\mathbb{R}^n$ and $C$ classes, the Euclidean MLR logits of class $c\in\{1,...,C\}$ can be expressed as

\begin{equation}\label{eq:mlrEucl}
    v_{\vect{w}_c}(\vect{x})=\text{sign}(\langle\vect{w}_c,\vect{x}\rangle)||\vect{w}_c||d(x, H_{\vect{w}_c}),~~\vect{w}_c\in\mathbb{R}^n,
\end{equation}

where $H_{\vect{w}_c}$ is the decision hyperplane of class $c$.

Replacing the Euclidean operations with their counterparts in the Lorentz model yields logits for $\vect{x}\in\mathbb{L}^n_K$ as follows

\begin{equation}\label{eq:logitsHypHyp}
    v_{\vect{w}_c, \vect{p}_c}(\vect{x})=\text{sign}(\lprod{\vect{w}_c}{\vect{x}})||\vect{w}_c||_{\mathcal{L}}d_{\mathcal{L}}(x, H_{\vect{w}_c,\vect{p}_c}),
\end{equation}

with $\vect{w}_c\in\tangentsp{\vect{p}_c}, \vect{p}_c\in\mathbb{L}^n_K$, and $\lprod{\vect{w}_c}{\vect{w}_c}>0$.

Inserting Eq. \ref{eq:stdDistHyp} into Eq. \ref{eq:logitsHypHyp} gives a general formula without our reparameterization

\begin{equation}
    v_{\vect{w}_c, \vect{p}_c}(\vect{x})=\frac{1}{\sqrt{-K}}~\text{sign}(\lprod{\vect{w}_c}{\vect{x}})||\vect{w}_c||_{\mathcal{L}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\lprod{\vect{w}_c}{\vect{x}}}{\lnorm{\vect{w}_c}}\right)\right|.
\end{equation}

Now, we reparameterize $\vect{w}$ with Eq. \ref{eq:wrepara} again, which gives

\begin{align}
    \alpha := \lprod{\vect{w}_c}{\vect{x}} = \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a),
\end{align}

\begin{align}
    \beta := \lnorm{\vect{w}_c} = \sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2},
\end{align}

with $a\in\mathbb{R}$ and $\vect{z}\in\mathbb{R}^n$. Finally, we obtain the equation in Theorem \ref{theorem:lorentzMLR}:

\begin{equation}
    v_{\vect{z}_c, a_c}(\vect{x})=\frac{1}{\sqrt{-K}}~\mathrm{sign}(\alpha)\beta\left|\sinh^{-1}\left(\sqrt{-K}\frac{\alpha}{\beta}\right)\right|.
\end{equation}

\section{Additional experimental details}\label{apdx:exp_details}

\subsection{Classification}

\paragraph{Datasets} For classification, we employ the benchmark datasets CIFAR-10 \cite{cifar}, CIFAR-100 \cite{cifar}, and Tiny-ImageNet \cite{tinyimagenet}. The CIFAR-10 and CIFAR-100 datasets each contain 60,000 $32\times 32$ colored images from 10 and 100 different classes, respectively. We use the dataset split implemented in PyTorch, which includes 50,000 training images and 10,000 testing images. Tiny-ImageNet is a small subset of the ImageNet \cite{imagenet} dataset, with 100,000 images of 200 classes downsized to $64 \times 64$ images. Here, we use the official validation split for testing our models.

\paragraph{Settings} 

Table \ref{tab:hyperparameters_cl} summarizes the hyperparameters we adopt from \citet{resnetHyp} to train all ResNet classification models. Additionally, we use standard data augmentation methods in training, i.e., random mirroring and cropping. Regarding the feature clipping in hybrid HNNs, we tune the feature clipping parameter $r$ between 1.0 and 5.0 and find that, for most experiments, the best feature clipping parameter is $r=1$. Only the Lorentz hybrid ResNet performs best with $r=4$ on Tiny-ImageNet, and $r=2$ on CIFAR-100 with lower embedding dimensions (see ablation experiment). Overall, we observe that the hybrid Lorentz ResNet has fewer gradient issues, allowing for higher clipping values.

\begin{table}[h]
	\centering
	\caption{Summary of hyperparameters used in training classification models.}
	\label{tab:hyperparameters_cl}
	\begin{tabular}{lr}
        \toprule
        Hyperparameter & Value\\
        \midrule
		{Epochs} & 200\\
		{Batch Size} & 128\\
		{Learning Rate (LR)} & 1e-1\\
        {Drop LR epochs} & 60, 120, 160\\
        {Drop LR gamma} & 0.2\\
		{Weight Decay} & 5e-4\\
		{Optimizer} & (Riemannian)SGD\\
		{Floating point Precision} & 32 bit\\
  		{GPU Type} & RTX A5000\\
        {Hyperbolic curvature $K$} & -1\\
		%{Seeds (5 runs)} & 1,2,3,4,5\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Generation}

\paragraph{Datasets} For image generation, we use the aforementioned CIFAR-10 \cite{cifar} and CIFAR-100 \cite{cifar} datasets again. Additionally, we employ the CelebA \cite{celeba} dataset, which includes colored $64\times64$ images of human faces. Here, we use the PyTorch implementation, containing 162,770 training images, 19,867 validation images, and 19,962 testing images.

\paragraph{Settings} For hyperbolic and Euclidean models, we use the same architecture (see Table \ref{tab:architecture}) and training hyperparameters (see Table \ref{tab:hyperparameters_gen}). We employ a vanilla VAE similar to \citet{ghosh-et-al-2019} as the baseline Euclidean architecture (E-VAE). For the hybrid model, we replace the latent distribution of the E-VAE with the hyperbolic wrapped normal distribution in the Lorentz model \cite{nagano-et-al-2019} and the Poincaré ball \cite{mathieu-et-al-2019}, respectively. Replacing all layers with our proposed hyperbolic counterparts yields the fully hyperbolic model. Furthermore, we set the curvature $K$ for the Lorentz model to $-1$ and for the Poincaré ball to $-0.1$. 

We evaluate the VAEs by employing two versions of the FID using the PyTorch implementation of \citet{Seitzer2020FID}:

\begin{enumerate}
	\item The \textit{reconstruction FID} gives a lower bound on the generation quality. It is calculated by comparing test images with reconstructed validation images. As the CIFAR datasets have no official validation set, we exclude a fixed random portion of 10,000 images from the training set.
	\item The \textit{generation FID} measures the generation quality by comparing random generations from the models' latent space with the test set.
\end{enumerate}

\begin{table}[h]
	\centering
	\caption{Vanilla VAE architecture employed in all image generation experiments. Convolutional layers have a kernel size of $3\times 3$ and transposed convolutional layers of $4\times 4$. $s$ and $p$ denote stride and zero padding, respectively. The MLR in the Euclidean model is mimicked by a $1\times1$ convolutional layer.}
	\label{tab:architecture}
	\begin{tabular}{lrr}
        \toprule
		Layer & CIFAR-10/100 & {CelebA}\\
		\midrule
		{\textsc{Encoder:}}  & &  \\
		$\rightarrow$ \textsc{Proj}$_{\mathbb{R}^n\rightarrow\mathbb{L}^n_K}$ & 32$\times$32$\times$3 & 64$\times$64$\times$3  \\
		$\rightarrow$ \textsc{Conv}$_{64,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU}  & 16$\times$16$\times$64  & 32$\times$32$\times$64\\
		$\rightarrow$ \textsc{Conv}$_{128,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 8$\times$8$\times$128& 16$\times$16$\times$128 \\
		$\rightarrow$ \textsc{Conv}$_{256,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 4$\times$4$\times$256& 8$\times$8$\times$256\\
		$\rightarrow$ \textsc{Conv}$_{512,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 2$\times$2$\times$512& 4$\times$4$\times$512\\
		$\rightarrow$ \textsc{Flatten}  & 2048 & 8192\\
		$\rightarrow$ \textsc{FC-Mean}$_{d}$  & 128 & 64 \\
		$\rightarrow$ \textsc{FC-Var}$_{d}$ $\rightarrow$ \textsc{Softplus} & 128 & 64 \\
		\midrule
		{\textsc{Decoder:}}   & &   \\
		$\rightarrow$ \textsc{Sample}  & 128 & 64  \\
		$\rightarrow$ \textsc{FC}$_{32768}$ $\rightarrow$ BN $\rightarrow$ \textsc{ReLU} & 32768 & 32768 \\
		$\rightarrow$ \textsc{Reshape}  & 8$\times$8$\times$512 & 8$\times$8$\times$512 \\
		$\rightarrow$ \textsc{ConvTr}$_{256,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 16$\times$16$\times$256 & 16$\times$16$\times$256\\
		$\rightarrow$ \textsc{ConvTr}$_{128,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 32$\times$32$\times$128& 32$\times$32$\times$128 \\
		$\rightarrow$ \textsc{ConvTr}$_{64,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & - & 64$\times$64$\times$64\\
		$\rightarrow$  \textsc{Conv}$_{64,s1,p1}$ & 32$\times$32$\times$64 & 64$\times$64$\times$64 \\
		$\rightarrow$  \textsc{MLR} & 32$\times$32$\times$3 & 64$\times$64$\times$3 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{Summary of hyperparameters used in training image generation models.}
	\label{tab:hyperparameters_gen}
	\begin{tabular}{lrr}
        \toprule
		Hyperparameter & CIFAR-10/100 &\textsc{CelebA}\\
		\midrule
		{Epochs}  & 100 & 70\\
		{Batch Size}  & 100 & 100\\
		{Learning Rate}  & 5e-4 & 5e-4\\
		{Weight Decay} & 0 & 0\\
		{KL Loss Weight}  & 0.024 & 0.09\\
		{Optimizer}  & (Riem)Adam & (Riem)Adam\\
		{Floating point Precision} & 32 bit & 32 bit\\
		%\textsc{\#GPUs} & 1 & 1 & 1\\
		{GPU Type} & RTX A5000 & RTX A5000\\
		%\textsc{Seeds (5 runs)} & 1,2,3,4,5 & 1,2,3,4,5 & 1,2,3,4,5\\
		\bottomrule
	\end{tabular}
\end{table}

%\section{Additional experiments}

%\subsection{Classification}

%\paragraph{Comparison feature clipping} Try different $r$ and show how it affects both hybrid models...

%\paragraph{Residual connection} Show performance with different residual connections...

%\subsection{Generation}

%\paragraph{Qualitative comparison of generations} 

%Show sample generations and reconstructions

%\paragraph{Qualitative comparison of latent embeddings}

%Compare latent embeddings of different models (MNIST)

\end{document}
