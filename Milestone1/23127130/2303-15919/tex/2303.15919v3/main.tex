	
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

\iclrfinalcopy

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{wrapfig}

\usepackage{pgfplots}
\usepackage{tikz}

\definecolor{QPblue}{RGB}{24,78,119}
\definecolor{liblue}{rgb}{0.655,0.835,1.0}
\definecolor{ligreen}{rgb}{0.686,1.0,0.655}
\definecolor{lired}{rgb}{0.961,0.306,0.259}
\definecolor{liyellow}{rgb}{0.988,0.961,0.573}
\definecolor{lilila}{HTML}{E0D4E7}    
\definecolor{Wowgreen}{HTML}{009052}   

\hypersetup{
    colorlinks,
    citecolor=green,
    linkcolor=red,
    urlcolor=magenta
}

\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{bm}

\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

\newcommand{\given}{\:\vert\:}
\newcommand{\matr}[1]{\mathrm{\textbf{#1}}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\origin}{\overline{\bm{0}}}
\newcommand{\transp}[3]{\mathrm{PT}^K_{{#1}\rightarrow{#2}}\left({#3}\right)}
\newcommand{\logmap}[2]{\log^K_{#1}(#2)}
\newcommand{\expmap}[2]{\exp^K_{#1}\left(#2\right)}
\newcommand{\lprod}[2]{\langle #1,#2\rangle_{\mathcal{L}}}
\newcommand{\lnorm}[1]{||#1||_{\mathcal{L}}}
\newcommand{\tangentsp}[1]{\mathcal{T}_{#1}\mathbb{L}_K^n}
\newcommand{\tangentspN}[2]{\mathcal{T}_{#1}\mathbb{L}_K^{#2}}

\title{Fully Hyperbolic Convolutional\\ Neural Networks for Computer Vision}

\author{Ahmad Bdeir\thanks{Equal contribution.}\hspace*{2mm}, Kristian Schwethelm$^*$ \& Niels Landwehr\\
Data Science Department\\
University of Hildesheim\\
\texttt{\{bdeira, schwethelm, landwehr\}@uni-hildesheim.de}\\
}


\begin{document}


\maketitle

\begin{abstract}
   Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current HNNs in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, a fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. {Experiments on standard vision tasks demonstrate the promising performance of our HCNN framework in both hybrid and fully hyperbolic settings.} Overall, we believe our contributions provide a foundation for developing more powerful HNNs that can better represent complex structures found in image data. Our code is publicly available at \textcolor{magenta}{\url{https://github.com/kschwethelm/HyperbolicCV}}.
\end{abstract}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\section{Introduction}

Representation learning is a fundamental aspect of deep neural networks, as obtaining an optimal representation of the input data is crucial. {While Euclidean geometry has been the traditional choice for representing data due to its intuitive properties, recent research has highlighted the advantages of using hyperbolic geometry as a geometric prior for the feature space of neural networks. Given the exponentially increasing distance to the origin, hyperbolic spaces can be thought of as continuous versions of trees that naturally model tree-like structures, like hierarchies or taxonomies, without spatial distortion and information loss \citep{nickel-kiela-2018, hyperbolic_trees}. This is compelling since hierarchies are ubiquitous in knowledge representation \citep{Noy_Hafner_1997}, and even the natural spatial representations in the human brain exhibit a hyperbolic geometry \citep{zhang2023hippocampal}.}

{Leveraging this better representative capacity, hyperbolic neural networks (HNNs) have demonstrated increased performance over Euclidean models in many natural language processing (NLP) and graph embedding tasks \citep{hnn-survey}. However, hierarchical structures have also been shown to exist in images. Mathematically, \citet{khrulkov} have found high $\delta$-hyperbolicity in the final embeddings of image datasets, where the hyperbolicity quantifies the degree of inherent tree-structure. Extending their measurement to the whole model reveals high hyperbolicity in intermediate embeddings as well (see Appendix \ref{apdx:hyperbolicity}). Intuitively, hierarchies that emerge within and across images can be demonstrated on the level of object localization and object class relationships. A straightforward example of the latter is animal classification hierarchy, where species is the lowest tier, preceded by genus, family, order, etc. Similarly, on a localization level, humans are one example: the nose, eyes, and mouth are positioned on the face, which is a part of the head, and, ultimately, a part of the body. This tree-like localization forms the basis of part-whole relationships and is strongly believed to be how we parse visual scenes \citep{Biederman1987RecognitionbycomponentsAT, HINTON1979231, KAHNEMAN1992175}.}

\begin{figure}[t]
    \centering
    \includegraphics[width=.88\textwidth]{figs/overview.pdf}
    \caption{In contrast to hybrid HNNs that use a Euclidean CNN for feature extraction, our HCNN learns features in hyperbolic spaces in every layer, fully leveraging the benefits of hyperbolic geometry. This leads to better image representations and performance.}
    \label{fig:overview}
\end{figure}

{In light of these findings, recent works have begun integrating hyperbolic geometry into vision architectures \citep{mettes2023hyperbolic, fang2023hyperbolic}}. Specifically, they rely on the Poincaré ball and the Lorentz model as descriptors of hyperbolic space and formalize hyperbolic translations of neural network layers. This is challenging due to ill-defined hyperbolic analogs of, e.g., addition, multiplication, and statistical measures. Currently, most HNN components are only available in the Poincaré ball as it supports the gyrovector space with basic vector operations. However, due to its hard numerical constraint, the Poincaré ball is more susceptible to numerical instability than the Lorentz model \citep{mishne-et-al-2022}, which motivates introducing the missing layers for the Lorentz model. Moreover, HNNs in computer vision have been limited to hybrid architectures that might not fully leverage the advantages of hyperbolic geometry as they rely on Euclidean encoders to learn hyperbolic representations. Until now, hyperbolic encoder architectures are missing in computer vision, although prevalent in NLP and graph applications \citep{hnn-survey}.

In this work, we present HCNN, a fully hyperbolic framework for vision tasks that can be used to design hyperbolic encoder models. We generalize the ubiquitous convolutional neural network (CNN) architecture to the Lorentz model, extend hyperbolic convolutional layers to 2D, and present novel hyperbolic formulations of batch normalization and multinomial logistic regression. Our methodology is general, and we show that our components can be easily integrated into existing architectures. Our contributions then become three-fold:

\begin{enumerate}
  \item We propose hybrid (HECNN) and fully hyperbolic (HCNN) convolutional neural network encoders for image data, introducing the fully hyperbolic setting in computer vision.
  \item We provide missing Lorentzian formulations of the 2D convolutional layer, batch normalization, and multinomial logistic regression.
  \item We empirically demonstrate the performance potential of deeper hyperbolic integrations in experiments on standard vision tasks, including image classification and generation.
 \end{enumerate}

\section{Related work}

\paragraph{Hyperbolic image embeddings} Previous research on HNNs in computer vision has mainly focused on combining Euclidean encoders and hyperbolic embeddings. This approach involves projecting Euclidean embeddings onto the hyperbolic space in the task heads and designing task-related objective functions based on hyperbolic geometry. Such simple hybrid architectures have been proven effective in various vision tasks like recognition \citep{yu2022skin,khrulkov, liu2020zeroShot, guo-et-al-2022}, segmentation \citep{hsu20203dImg, atigh-et-al-2022}, reconstruction/generation \citep{mathieu-et-al-2019, nagano-et-al-2019, ovinnikov-et-al-2019, qu-zou-2022}, and metric learning \citep{ermolov-et-al-2022,yan2021metric,yue2023contrastive}. However, there remains the discussion of whether the single application of hyperbolic geometry in the decoder can fully leverage the present hierarchical information. In contrast, HE/HCNN also learns latent hyperbolic feature representations in the encoder, potentially magnifying these benefits. We also forgo the typically used Poincaré ball in favor of the Lorentz model, as it offers better stability and optimization \citep{mishne-et-al-2022}. {For a complete overview of vision HNNs and motivations, refer to \citep{mettes2023hyperbolic, fang2023hyperbolic}.}

\paragraph{Fully hyperbolic neural networks} Designing fully hyperbolic neural networks requires generalizing Euclidean network components to hyperbolic geometry. Notably, \citet{ganea-et-al-2018} and \citet{shimizu-et-al-2020} utilized the Poincaré ball and the gyrovector space to generalize various layers, including fully-connected, convolutional, and attention layers, as well as operations like split, concatenation, and multinomial logistic regression (MLR). Researchers have also designed components in the Lorentz model \citep{nickel-kiela-2018,fan-et-al-2021,chen2021,qu-zou-2022}, but crucial components for vision, like the standard convolutional layer and the MLR classifier, are still missing. Among the hyperbolic layer definitions, fully hyperbolic neural networks have been built for various tasks in NLP and graph applications \citep{hnn-survey}. However, no hyperbolic encoder architecture has yet been utilized in computer vision. Our work provides formulations for missing components in the Lorentz model, allowing for hyperbolic CNN vision encoders. Concurrently, \cite{vanspengler2023poincare} proposed a fully hyperbolic Poincaré CNN.

\paragraph{Normalization in HNNs} There are few attempts at translating standard normalization layers to the hyperbolic setting. To the best of our knowledge, there is only a single viable normalization layer for HNNs, i.e., the general Riemannian batch normalization \citep{lou-et-al-2020}. However, this method is not ideal due to the slow iterative computation of the Fréchet mean and the arbitrary re-scaling operation that is not based on hyperbolic geometry. The concurrent work on Poincaré CNN \citep{vanspengler2023poincare} only solved the first issue by using the Poincaré midpoint. In contrast, we propose an efficient batch normalization algorithm founded in the Lorentz model, which utilizes the Lorentzian centroid \citep{law-et-al-2019} and a mathematically motivated re-scaling operation.

\paragraph{Numerical stability of HNNs} The exponential growth of the Lorentz model's volume with respect to the radius can introduce numerical instability and rounding errors in floating-point arithmetic. This requires many works to rely on 64-bit precision at the cost of higher memory and runtime requirements. To mitigate this, researchers have introduced feature clipping and Euclidean reparameterizations \citep{mishne-et-al-2022,guo-et-al-2022,mathieu-et-al-2019}. We adopt these approaches to run under 32-bit floating point arithmetic and reduce computational cost.

\section{Background}

This section summarizes the mathematical background of hyperbolic geometry \citep{cannon-1997,ratcliffe-2006}. {The $n$-dimensional hyperbolic space $\mathbb{H}^n_K$ is a Riemannian manifold ($\mathcal{M}^n, \mathfrak{g}_{\vect{x}}^K$) with constant negative curvature $K<0$, where $\mathcal{M}^n$ and $\mathfrak{g}_{\vect{x}}^K$ represent the manifold and the Riemannian metric, respectively.} There are isometrically equivalent models of hyperbolic geometry. We employ the Lorentz model because of its numerical stability and its simple exponential/logarithmic maps and distance functions. Additionally, we use the Poincaré ball for baseline implementations. Both hyperbolic models provide closed-form formulae for manifold operations, including distance measures, exponential/logarithmic maps, and parallel transportation. They are detailed in Appendix \ref{apdx:hyp_geometry}. 

\begin{wrapfigure}{r}{5cm}
\centering
\includegraphics[width=3.7cm]{figs/LP_proj.pdf}
\caption{Comparison of Lorentz and Poincaré model.}\label{fig:hyp_mod}
\end{wrapfigure} 


\paragraph{Lorentz model} The $n$-dimensional Lorentz model $\mathbb{L}^n_K = (\mathcal{L}^n, \mathfrak{g}_{\vect{x}}^K)$ models hyperbolic geometry on the upper sheet of a two-sheeted hyperboloid $\mathcal{L}^n$, with origin $\origin = [\sqrt{-1/K}, 0, \cdots, 0]^T$ and embedded in $(n+1)$-dimensional Minkowski space (see Figure \ref{fig:hyp_mod}). Based on the Riemannian metric $\mathfrak{g}_{\vect{x}}^K = \mathrm{diag}(-1, 1, \dots, 1)$, the manifold is defined as
\begin{gather}\label{equ:lor_points}
	\mathcal{L}^n := \{\vect{x}\in \mathbb{R}^{n+1}~|~\langle \vect{x}, \vect{x}\rangle_{\mathcal{L}}= \frac{1}{K},~x_t>0\},
\end{gather}
with the Lorentzian inner product
\begin{equation}\label{equ:lor_inner}
	\langle \vect{x}, \vect{y}\rangle_{\mathcal{L}} := -x_ty_t+\vect{x}_s^T\vect{y}_s = \vect{x}^T\mathrm{diag}(-1,1,\cdots,1)\vect{y}.
\end{equation}

When describing points in the Lorentz model, we inherit the terminology of special relativity and call the first dimension the \textit{time component} $x_t$ and the remaining dimensions the \textit{space component} $\vect{x}_s$, such that $\vect{x}\in\mathbb{L}^n_K=[x_t, \vect{x_s}]^T$ and $x_t = \sqrt{||\vect{x}_s||^2-1/K}$. %For a comparison with the Poincare model see Appendix \ref{apdx:diff_hypers}.

\section{Fully hyperbolic CNN (HCNN)}

We aim to give way to building vision models that can fully leverage the advantages of hyperbolic geometry by learning features in hyperbolic spaces. For this, we generalize Euclidean CNN components to the Lorentz model, yielding one-to-one replacements that can be integrated into existing architectures. In the following, we first define the cornerstone of HCNNs, i.e., the Lorentz convolutional layer, including its transposed variant. Then, we introduce the Lorentz batch normalization algorithm and the MLR classifier. Finally, we generalize the residual connection and non-linear activation.

\subsection{Lorentz convolutional layer}

\paragraph{Hyperbolic feature maps} The convolutional layer applies vector operations to an input feature map containing the activations of the previous layer. In Euclidean space, arbitrary numerical values can be combined to form a vector. However, in the Lorentz model, not all possible value combinations represent a point that can be processed with hyperbolic operations ($\mathbb{L}_K^n\subset\mathbb{R}^{n+1}$). 

We propose using channel-last feature map representations throughout HCNNs and adding the Lorentz model's time component as an additional channel dimension. This defines a hyperbolic feature map as an ordered set of $n$-dimensional hyperbolic vectors, where every spatial position contains a vector that can be combined with its neighbors. Additionally, it offers a nice interpretation where an image is an ordered set of color vectors, each describing a pixel.

\paragraph{Formalization of the convolutional layer} We define the convolutional layer as a matrix multiplication between a linearized kernel and a concatenation of the values in its receptive field, following \citet{shimizu-et-al-2020}. Then, we generalize this definition by replacing the Euclidean operators with their hyperbolic counterparts in the Lorentz model.

Given a hyperbolic input feature map $\matr{x} = \{\vect{x}_{h,w} \in \mathbb{L}^n_K\}_{h,w=1}^{H,W}$ as an ordered set of $n$-dimensional hyperbolic feature vectors, each describing image pixels, the features within the receptive field of the kernel $\matr{K} \in \mathbb{R}^{m \times n \times \tilde{H} \times \tilde{W}}$ are $\{\vect{x}_{{h'+\delta \tilde{h}},{w'+\delta \tilde{w}}} \in \mathbb{L}^n_K\}_{\tilde{h},\tilde{w}=1}^{\tilde{H},\tilde{W}}$, where $(h', w')$ denotes the starting position and $\delta$ is the stride parameter. Now, we define the Lorentz convolutional layer as

\begin{equation}
	\vect{y}_{h,w} = \text{LFC}(\text{HCat}(\{\vect{x}_{{h'+\delta \tilde{h}},{w'+\delta \tilde{w}}} \in \mathbb{L}^n_K\}_{\tilde{h},\tilde{w}=1}^{\tilde{H},\tilde{W}}\})),
\end{equation}

where HCat denotes the concatenation of hyperbolic vectors, and LFC denotes a Lorentz fully-connected layer performing the affine transformation and parameterizing the kernel and bias, respectively {(see Appendix \ref{apdx:hyp_geometry})}. Additionally, we implement padding using origin vectors, the analog of zero vectors in hyperbolic space. The LFC layer is similar to \citet{chen2021} but does not use normalization as it is done through the hyperbolic batch normalization formulated below.

\paragraph{Extension to the transposed setting} The transposed convolutional layer is usually used in encoder-decoder architectures for up-sampling. A convolutional layer carries out a transposed convolution when the correct local connectivity is established by inserting zeros at certain positions. Specifically, when stride $s>1$, then $s-1$ zero vectors are inserted between the features. We refer to \citet{trConv} for illustrations. Under this relationship, the Lorentz transposed convolutional layer is a Lorentz convolutional layer with changed connectivity through origin padding.

\subsection{Lorentz batch normalization}

Given a batch $\mathcal{B}$ of $m$ features $\vect{x}_i$, the traditional batch normalization algorithm \citep{bnorm} calculates the mean $\vect{\mu}_{\mathcal{B}} = \frac{1}{m}\sum_{i=1}^{m}\vect{x}_i$ and variance $\vect{\sigma}^2_{\mathcal{B}} = \frac{1}{m}\sum_{i=1}^{m}(\vect{x}_i-\vect{\mu}_{\mathcal{B}})^2$ across the batch dimension. Then, the features are \textit{re-scaled} and \textit{re-centered} using a parameterized variance $\vect{\gamma}$ and mean $\vect{\beta}$ as follows

\begin{equation}
	\text{BN}(\vect{x}_i) = \vect{\gamma} \odot \frac{\vect{x}_i - \vect{\mu}_{\mathcal{B}}}{\sqrt{\vect{\sigma}^2_{\mathcal{B}}}+\epsilon} + \vect{\beta}.
\end{equation}

At test time, running estimates approximate the batch statistics. They are calculated iteratively during training: $\vect{\mu}_t = (1-\eta)\vect{\mu}_{t-1} + \eta\vect{\mu}_{\mathcal{B}}$ and $\vect{\sigma}^2_t = (1-\eta)\vect{\sigma}^2_{t-1} + \eta\vect{\sigma}^2_{\mathcal{B}}$, with $\eta$ and $t$ denoting momentum and the current iteration, respectively. We generalize batch normalization to the Lorentz model using the Lorentzian centroid and the parallel transport operation for re-centering, and the Fréchet variance and straight geodesics at the origin's tangent space for re-scaling.

\paragraph{Re-centering} To re-center hyperbolic features, it is necessary to compute a notion of mean. Usually, the Fréchet mean is used \citep{lou-et-al-2020}, which minimizes the expected squared distance between a set of points in a metric space \citep{normal_distr}. Generally, the Fréchet mean must be solved iteratively, massively slowing down training. To this end, we propose to use the centroid with respect to the squared Lorentzian distance, which can be calculated efficiently in closed form \citep{law-et-al-2019}. The weighted Lorentzian centroid, which solves $\min_{\vect{\mu} \in \mathbb{L}^n_K}\sum_{i=1}^{m}\nu_i d^2_{\mathcal{L}}(\vect{x}_i, \vect{\mu})$, with $\vect{x}_i \in \mathbb{L}^n_K$ and $\nu_i\geq0, \sum_{i=1}^{m}\nu_i > 0$, is given by

\begin{equation}
	\vect{\mu} = \frac{\sum_{i=1}^{m}\nu_i\vect{x}_i}{\sqrt{-K}\left|||\sum_{i=1}^{m}\nu_i\vect{x}_i||_{\mathcal{L}}\right|}.
\end{equation}

In batch normalization, the mean is not weighted, which gives $\nu_{i} = \frac{1}{m}$. Now, we shift the features from the batch's mean $\vect{\mu}_{\mathcal{B}}$ to the parameterized mean $\vect{\beta}$ using the parallel transport operation $\transp{\vect{\mu}_{\mathcal{B}}}{\vect{\beta}}{\vect{x}}$. Parallel transport does not change the variance, as it is defined to preserve the distance between all points. Finally, the running estimate is updated iteratively using the weighted centroid with $\nu_1=(1-\eta)$ and $\nu_2=\eta$.

\paragraph{Re-scaling} For re-scaling, we rely on the Fréchet variance $\sigma^2 \in \mathbb{R}^+$, defined as the expected squared Lorentzian distance between a point $\vect{x}_i$ and the mean $\vect{\mu}$, and given by $\sigma^2 = \frac{1}{m}\sum_{i=1}^{m}d_\mathcal{L}^2(\vect{x}_i, \vect{\mu})$ \citep{kobler-et-al-2022}. In order to re-scale the batch, features must be moved along the geodesics connecting them to their centroid, which is generally infeasible to compute. However, geodesics intersecting the origin are very simple, as they can be represented by straight lines in tangent space $\tangentsp{\origin}$. This is reflected by the equality between the distance of a point to the origin and the length of its corresponding tangent vector ($d_{\mathcal{L}}(\vect{x},\origin) = ||\logmap{\origin}{\vect{x}}||$). Using this property, we propose to re-scale features by first parallel transporting them towards the origin %$\transp{\mu_{\mathcal{B}}}{\origin}{\vect{x}}$
{$\transp{\vect{\mu}_B}{\origin}{\logmap{\vect{\mu}_{\mathcal{B}}}{\vect{x}}}$}, making the origin the new centroid and straightening the relevant geodesics. Then, a simple multiplication re-scales the features in tangent space. Finally, parallel transporting to $\vect{\beta}\in\mathbb{L}^n_K$ completes the algorithm and yields the normalized features. The final algorithm is formalized as

\begin{equation}
	\text{LBN}(\vect{x}) = \expmap{\vect{\beta}}{\transp{\origin}{\vect{\beta}}{{\gamma}\cdot \frac{\transp{\vect{\mu}_B}{\origin}{\logmap{\vect{\mu}_{\mathcal{B}}}{\vect{x}}}}{{\sqrt{{\sigma}^2_{\mathcal{B}}}+\epsilon}}}}.
\end{equation}

% In our definition of hyperbolic feature maps, the centroid must be computed across multiple dimensions. For example, given a feature map $\{\vect{x}_{h,w} \in \mathbb{L}^n_K\}_{h,w=1}^{H,W}$ for all $m$ instances, we first compute the centroid of all hyperbolic vectors per instance and then the centroid across the batch dimension. 

\subsection{Lorentz MLR classifier}

In this section, we consider the problem of classifying instances that are represented in the Lorentz model. A standard method for multi-class classification is multinomial logistic regression (MLR). Inspired by the generalization of MLR to the Poincaré ball \citep{ganea-et-al-2018,shimizu-et-al-2020} based on the distance to margin hyperplanes, we derive a formulation in the Lorentz model.

% ambient space = space surrounding a mathematical object, e.g., a hyperplane.

\paragraph{Hyperplane in the Lorentz model} Analogous to Euclidean space, hyperbolic hyperplanes split the manifold into two half-spaces, which can then be used to separate instances into classes. The hyperplane in the Lorentz model is defined by a geodesic that results from the intersection of an $n$-dimensional hyperplane with the hyperboloid in the ambient space $\mathbb{R}^{n+1}$ \citep{cho-et-al-2019}. Specifically, for $\vect{p}\in \mathbb{L}^n_K$ and $\vect{w}\in \mathcal{T}_{\vect{p}}\mathbb{L}^n_K$, the hyperplane passing through $\vect{p}$ and perpendicular to $\vect{w}$ is given by

    \begin{align}\label{eq:hyperplane_def}
    	H_{\vect{w},\vect{p}} = \{\vect{x}\in\mathbb{L}^n_K\given\langle \vect{w},\vect{x}\rangle_{\mathcal{L}}=0\}.
    \end{align}

This formulation comes with the non-convex optimization condition $\lprod{\vect{w}}{\vect{w}}>0$, which is undesirable in machine learning. To eliminate this condition, we use the Euclidean reparameterization of \citet{mishne-et-al-2022}, which we extend to include the curvature parameter $K$ in Appendix \ref{apdx:proof_hyperplane}. In short, $\vect{w}$ is parameterized by a vector $\vect{\overline{z}}\in \tangentsp{\origin} = [0,a{\vect{z}/}{||\vect{z}||}]$, where $a \in \mathbb{R}$ and $\vect{z}\in \mathbb{R}^n$. As $\vect{w}\in\tangentsp{\vect{p}}$, $\vect{\overline{z}}$ is parallel transported to $\vect{p}$, which gives

\begin{equation}\label{eq:w_parametr}
    \vect{w}:= \transp{\origin}{\vect{p}}{\overline{\vect{z}}} = [\sinh(\sqrt{-K}a)||\vect{z}||, \cosh(\sqrt{-K}a)\vect{z}]. 
\end{equation}

Inserting Eq. \ref{eq:w_parametr} into Eq. \ref{eq:hyperplane_def}, the formula of the Lorentz hyperplane becomes

\begin{equation}\label{eq:hyppp}
	\tilde{H}_{\vect{z},a} = \{\vect{x}\in \mathbb{L}^n_K~|~ \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t = 0\},
\end{equation}

where $a$ and $\vect{z}$ represent the distance and orientation to the origin, respectively.

Finally, we need the distance to the hyperplane to quantify the model's confidence. It is formulated by the following theorem, proven in Appendix \ref{apdx:proof_dist_hyperpl}.

\begin{theorem} \label{theorem:disthyp}
Given $a\in\mathbb{R}$ and $\vect{z}\in\mathbb{R}^n$, the minimum hyperbolic distance from a point $\vect{x}\in\mathbb{L}^n_K$ to the hyperplane $\tilde{H}_{\vect{z},a}$ defined in Eq. \ref{eq:hyppp} is given by
\begin{equation}\label{equ:dist_hyperplane}
    d_{\mathcal{L}}(\vect{x}, \tilde{H}_{\vect{z},a}) = \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t}{\sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2}}\right)\right|.
\end{equation}
\end{theorem}

\paragraph{MLR in the Lorentz model} \citet{mlr} formulated the logits of the Euclidean MLR classifier using the distance from instances to hyperplanes describing the class regions. Specifically, given input $\vect{x}\in\mathbb{R}^n$ and $C$ classes, the output probability of class $c\in\{1,...,C\}$ can be expressed as

\begin{equation}\label{eq:mlrEuclTop}
    p(y=c\given\vect{x}) \propto \exp(v_{\vect{w}_c}(\vect{x})),~~v_{\vect{w}_c}(\vect{x})=\text{sign}(\langle\vect{w}_c,\vect{x}\rangle)||\vect{w}_c||d(\vect{x}, H_{\vect{w}_c}),~~\vect{w}_c\in\mathbb{R}^n,
\end{equation}

where $H_{\vect{w}_c}$ is the decision hyperplane of class $c$. 

We define the Lorentz MLR without loss of generality by inserting the Lorentzian counterparts into Eq. \ref{eq:mlrEuclTop}. This yields logits given by the following theorem, proven in Appendix \ref{apdx:proof_lMLR}.

\begin{theorem}\label{theorem:lorentzMLR}
Given parameters $a_c\in\mathbb{R}$ and $\vect{z}_c\in\mathbb{R}^n$, the Lorentz MLR's output logit corresponding to class $c$ and input $\vect{x}\in\mathbb{L}^n_K$ is given by

\begin{equation}\label{eq:logits}
    v_{\vect{z}_c, a_c}(\vect{x})=\frac{1}{\sqrt{-K}}~\mathrm{sign}(\alpha)\beta\left|\sinh^{-1}\left(\sqrt{-K}\frac{\alpha}{\beta}\right)\right|,
\end{equation}

\begin{equation*}
    \alpha = \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a),
\end{equation*} 
\begin{equation*}
    \beta = \sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2}.
\end{equation*}
\end{theorem}

\subsection{Lorentz residual connection and activation}\label{sec:method:resAct}

\paragraph{Residual connection} The residual connection is a crucial component when designing deep CNNs. As vector addition is ill-defined in the Lorentz model, we add the vector's space components and concatenate a corresponding time component. This is possible as a point $\vect{x}\in\mathbb{L}^n_K$ can be defined by an arbitrary space component $\vect{x}_s \in \mathbb{R}^n$ and a time component $x_t = \sqrt{||\vect{x}_s||^2-1/K}$. Our method is straightforward and provides the best empirical performance compared to other viable methods for addition we implemented, i.e., tangent space addition \citep{nickel-kiela-2018}, parallel transport addition \citep{chami-et-al-2019}, Möbius addition (after projecting to the Poincaré ball) \citep{ganea-et-al-2018}, and fully-connected layer addition \citep{chen2021}.

\paragraph{Non-linear activation} Prior works use non-linear activation in tangent space \citep{fan-et-al-2021}, which weakens the model's stability due to frequent logarithmic and exponential maps. We propose a simpler operation for the Lorentz model by applying the activation function to the space component and concatenating a time component. For example, the Lorentz ReLU activation is given by

\begin{equation}
	\vect{y} = \left[\begin{array}{c}
		\sqrt{||\text{ReLU}(\vect{x}_s)||^2-1/K}\\
		\text{ReLU}(\vect{x}_s)
	\end{array}\right].
\end{equation}

%This operation can be interpreted similarly to Euclidean activations that break linearity with heuristic mathematical projections. 

\section{Experiments}

We evaluate hyperbolic models on image classification and generation tasks and compare them against Euclidean and hybrid HNN counterparts from the literature. To ensure a fair comparison, in every task, we directly translate a Euclidean baseline to the hyperbolic setting by using hyperbolic modules as one-to-one replacements. All experiments are implemented in PyTorch \citep{pytorch}, and we optimize hyperbolic models using adaptive Riemannian optimizers \citep{riemadam} provided by Geoopt \citep{geoopt2020kochurov}, with floating-point precision set to 32 bits. We provide detailed experimental configurations in Appendix \ref{apdx:exp_details} and ablation experiments in Appendix \ref{apdx:add_exp}.

\subsection{Image classification}

\paragraph{Experimental setup} We evaluate image classification performance using ResNet-18 \citep{resnet} and three datasets: CIFAR-10 \citep{cifar}, CIFAR-100 \citep{cifar}, and Tiny-ImageNet \citep{tinyimagenet}. All these datasets exhibit hierarchical class relations and high hyperbolicity (low $\delta_{rel}$), making the use of hyperbolic models well-motivated.

For the HCNN, we replace all components in the ResNet architecture with our proposed Lorentz modules. Additionally, we experiment with a novel hybrid approach (HECNN), where we employ our Lorentz decoder and replace only the ResNet encoder blocks with the highest hyperbolicity ($\delta_{rel}<0.2$), i.e., blocks 1 and 3 (see Appendix \ref{apdx:hyperbolicity}). To establish hyperbolic baselines we follow the literature \citep{atigh-et-al-2022, guo-et-al-2022} and implement hybrid HNNs with a Euclidean encoder and a hyperbolic output layer (using both the Poincaré MLR \citep{shimizu-et-al-2020} and our novel Lorentz MLR). Additionally, we report classification results for the concurrently developed fully hyperbolic Poincaré ResNet \citep{vanspengler2023poincare}. For all models, we adopt the training procedure and hyperparameters of \cite{resnetHyp}, which have been optimized for Euclidean CNNs and yield a strong Euclidean ResNet baseline. 

\paragraph{Main results} Table \ref{tab:classification} shows that hyperbolic models using the Lorentz model achieve the highest accuracy across all datasets, outperforming both the Euclidean and Poincaré baselines. In contrast, the Poincaré HNNs are consistently worse than the Euclidean baseline, aligning with the results of \citet{guo-et-al-2022}. Notably, only in the case of CIFAR-10, all models exhibit equal performance, which is expected due to the dataset's simplicity. We also notice that the hybrid encoder model outperforms the fully hyperbolic model, indicating that not all parts of the model benefit from hyperbolic geometry. Overall, our findings suggest that the Lorentz model is better suited for HNNs than the Poincaré ball. This may be attributed to the better numerical stability causing fewer inaccuracies \citep{mishne-et-al-2022}. Furthermore, we achieve a notable improvement (of up to 1.5\%) in the accuracy of current HNNs. This shows the potential of using our HCNN components in advancing HNNs.

\begin{table}[h]
  \caption{Classification accuracy (\%) of ResNet-18 models. We estimate the mean and standard deviation from five runs. The best performance is highlighted in bold (higher is better).}
  \label{tab:classification}
   \begin{adjustbox}{width=0.87\textwidth, center}

  \begin{tabular}{lccc}
    \toprule
       & CIFAR-10 & CIFAR-100 & Tiny-ImageNet \\
       & $(\delta_{rel} = 0.26)$ & $(\delta_{rel} = 0.23)$ & $(\delta_{rel} = 0.20)$ \\
    \midrule
    Euclidean \citep{resnet} & ${95.14_{\pm0.12}}$ & $77.72_{\pm0.15}$ & $65.19_{ \pm0.12}$   \\
    \midrule
    Hybrid Poincaré \citep{guo-et-al-2022} & $95.04_{\pm0.13}$ & $77.19_{\pm0.50}$ & $64.93_{\pm0.38}$ \\
    Hybrid Lorentz (Ours) & ${94.98_{\pm0.12}}$ & ${78.03_{\pm0.21}}$ & ${65.63_{\pm0.10}}$ \\
    \midrule
    Poincaré ResNet \citep{vanspengler2023poincare} & $94.51_{\pm0.15}$ & $76.60_{\pm0.32}$ &$62.01_{\pm0.56}$  \\
    HECNN Lorentz (Ours) & $\bm{95.16_{\pm0.11}}$ & $\bm{78.76_{\pm0.24}}$ & $\bm{65.96_{\pm0.18}}$ \\
    HCNN Lorentz (Ours) & ${95.14_{\pm0.08}}$ & ${78.07_{\pm0.17}}$ & ${65.71_{\pm0.13}}$ \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

\begin{table}[t]
    \caption{Classification accuracy (\%) after performing FGSM and PGD attacks on CIFAR-100. We estimate the mean and standard deviation from attacking five trained models (higher is better).}
    \label{tab:attack}
	\begin{adjustbox}{width=\columnwidth,center}
	\begin{tabular}{lcccccc}
		\toprule
		& \multicolumn{3}{c}{{FGSM}} & \multicolumn{3}{c}{{PGD}}\\
        \cmidrule(r){2-4}\cmidrule(r){5-7}
		Max. perturbation $\epsilon$  & {0.8/255} & {1.6/255} & {3.2/255} & {0.8/255} & {1.6/255} & {3.2/255}\\
    \midrule
		Euclidean \citep{resnet} & $65.70_{\pm 0.28}$ & $54.98_{ \pm 0.39}$ & $39.97_{\pm 0.43}$ & $64.43_{\pm 0.29}$ & $49.76_{\pm 0.42}$ & $26.30_{\pm 0.40}$\\ \midrule
        Hybrid Poincaré \citep{guo-et-al-2022} & $64.68_{\pm 0.40}$ & $53.32_{\pm 0.60}$ & $37.52_{\pm 0.50}$ & $63.43_{\pm 0.44}$ & $48.41_{\pm 0.60}$ & $23.78_{\pm 0.75}$\\
		Hybrid Lorentz (Ours) & $65.27_{\pm 0.52}$ & $53.82_{\pm 0.49}$ & $40.53_{\pm 0.31}$ & $64.15_{\pm 0.53}$ & $49.05_{\pm 0.68}$ & $27.17_{\pm 0.40}$\\
          \midrule	
        HECNN Lorentz (Ours) & $66.13_{\pm 0.41}$ & $55.71_{\pm 0.43}$ & $42.76_{\pm 0.37}$ & $65.01_{\pm 0.49}$ & $50.82_{\pm 0.37}$ &         $30.34_{\pm 0.22}$\\
		HCNN Lorentz (Ours) & \bm{$66.47_{\pm 0.27}$} & \bm{$57.14_{\pm 0.30}$} & \bm{$43.51_{\pm 0.35}$} & $\bm{65.04_{\pm 0.28}}$ & \bm{$52.25_{\pm 0.34}$} & \bm{$31.77_{\pm 0.55}$}\\
		\bottomrule
	\end{tabular}%
    \end{adjustbox}
\end{table}

\paragraph{Adversarial robustness} Prior works have demonstrated the robustness of hyperbolic models against adversarial attacks \citep{yue2023contrastive, guo-et-al-2022}. We expect better performance for HCNNs/HECNNs due to the bigger effect fully hyperbolic models have on the embedding space as can be seen in Figure \ref{fig:embed}. We believe the benefit could come from the increased inter-class separation afforded by the distance metric which allows for greater slack in the object classification. To study this, we employ the trained models and attack them using FGSM \citep{fgsm} and PGD \citep{pgd} with different perturbations. The results in Table \ref{tab:attack} show that our HCNN is more robust, achieving up to 5\% higher accuracy. In addition, and contrary to \citet{guo-et-al-2022}, we observe that hybrid decoder HNNs can be more susceptible to adversarial attacks than Euclidean models. 

\begin{wrapfigure}{r}{0.56\textwidth}
    \input{embed_plot}
\end{wrapfigure}

\paragraph{Low embedding dimensionality} HNNs have shown to be most effective for low-dimensional embeddings \citep{hnn-survey}. To this end, we reduce the dimensionality of the final ResNet block and the embeddings and evaluate classification accuracy on CIFAR-100. 

The results in Figure \ref{fig:embed} verify the effectiveness of hyperbolic spaces with low dimensions, where all HNNs outperform the Euclidean models. However, our HCNN and HECNN can leverage this advantage best, suggesting that hyperbolic encoders offer great opportunities for dimensionality reduction and designing smaller models with fewer parameters. The high performance of HECNN is unexpected as we hypothesized the fully hyperbolic model to perform best. This implies that hybrid encoder HNNs might make better use of the combined characteristics of both Euclidean and hyperbolic spaces.

\subsection{Image generation}\label{sec:generation}

\paragraph{Experimental setup} Variational autoencoders (VAEs) \citep{kingma, rezende} have been widely adopted in HNN research to model latent embeddings in hyperbolic spaces \citep{nagano-et-al-2019, mathieu-et-al-2019, ovinnikov-et-al-2019, hsu20203dImg}. HNNs have shown to generate more expressive embeddings under lower dimensionalities which would make them a good fit for VAEs. In this experiment, we extend the hyperbolic VAE to the fully hyperbolic setting using our proposed HCNN framework and, for the first time, evaluate its performance on image generation using the standard Fréchet Inception Distance (FID) metric \citep{FID}. 

Building on the experimental setting of \citet{ghosh-et-al-2019}, we test vanilla VAEs and assess generative performance on CIFAR-10 \citep{cifar}, CIFAR-100 \citep{cifar}, and CelebA \citep{celeba} datasets. We compare our HCNN-VAE against the Euclidean and two hybrid models. Following prior works, the hybrid models only include a latent hyperbolic distribution and no hyperbolic layers. Specifically, we employ the wrapped normal distributions in the Lorentz model \citep{nagano-et-al-2019} and the Poincaré ball \citep{mathieu-et-al-2019}, respectively.

\paragraph{Main results} The results in Table \ref{tab:results_gen} show that our HCNN-VAE outperforms all baselines. Likewise, the hybrid models improve performance over the Euclidean model, indicating that learning the latent embeddings in hyperbolic spaces is beneficial. This is likely due to the higher representation capacity of the hyperbolic space, which is crucial in low dimensional settings. However, our HCNN is better at leveraging the advantages of hyperbolic geometry due to its fully hyperbolic architecture. These results suggest that our method is a promising approach for generation and for modeling latent structures in image data.

\begin{table}[h]
    \caption{Reconstruction and generation FID of manifold VAEs across five runs (lower is better).}
    \label{tab:results_gen}
	\begin{adjustbox}{width=\columnwidth,center}
	\begin{tabular}{lcccccc}
		\toprule
		& \multicolumn{2}{c}{{CIFAR-10}} &\multicolumn{2}{c}{{CIFAR-100}} & \multicolumn{2}{c}{{CelebA}}\\
        \cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
		 & {Rec. FID} & {Gen. FID} & {Rec. FID} & {Gen. FID} & {Rec. FID} & {Gen. FID}\\
    \midrule
		Euclidean & $61.21_{\pm 0.72}$ & $92.40_{ \pm 0.80}$ & $63.81_{\pm0.47}$ & $103.54_{\pm0.84}$ & $54.80_{\pm0.29}$ & $79.25_{\pm0.89}$\\ \midrule
        Hybrid Poincaré \citep{mathieu-et-al-2019} & $59.85_{\pm0.50}$ & $90.13_{\pm0.77}$ & $62.64_{\pm0.43}$ & \bm{$98.19_{\pm0.57}$} & $54.62_{\pm0.61}$ & $81.30_{\pm0.56}$\\
		Hybrid Lorentz \citep{nagano-et-al-2019} & $59.29_{\pm0.47}$ & $90.91_{\pm0.84}$ & $62.14_{\pm0.35}$ & {$98.34_{\pm0.62}$} & $54.64_{\pm0.34}$ & $82.78_{\pm0.93}$\\
        \midrule
		HCNN Lorentz (Ours) & \bm{$57.78_{\pm 0.56}$} & \bm{$89.20_{\pm0.85}$} & \bm{$61.44_{\pm 0.64}$} & $100.27_{\pm0.84}$ & \bm{$54.17_{\pm0.66}$} & \bm{$78.11_{\pm0.95}$}\\
		\bottomrule
	\end{tabular}%
    \end{adjustbox}
\end{table}


\paragraph{Analysis of latent embeddings} The latent embedding space is a crucial component of VAEs as it influences how the data's features are encoded and used for generating the output. We visually analyze the distribution of latent embeddings inferred by the VAEs. For this, the models are retrained on the MNIST \citep{mnist} dataset with an embedding dimension $d_E=2$. Then, the images of the training dataset are passed through the encoder and visualized as shown in Figure \ref{fig:VAEembeds}.

We observe the formation of differently shaped clusters that correlate with the ground truth labels. While the embeddings of the Euclidean and hybrid models form many clusters that direct towards the origin, the HCNN-VAE obtains rather curved clusters that maintain a similar distance from the origin. The structures within the HCNN's latent space can be interpreted as hierarchies where the distance to the origin represents hierarchical levels. {As these structures cannot be found for the hybrid model, our results suggest that hybrid HNNs using only a single hyperbolic layer have little impact on the model's Euclidean characteristics. Conversely, our fully hyperbolic architecture significantly impacts how features are represented and learned, directing the model toward tree-like structures.}

\begin{figure}[t]
	\centering
	\hfill
	\begin{subfigure}[b]{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/E-VAE-embed-MNIST.pdf}
		\caption{Euclidean (81.02)}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/EL-VAE-embed-MNIST.pdf}
		\caption{Hybrid Lorentz (80.03)}
	\end{subfigure} 
	\hfill
	\begin{subfigure}[b]{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/L-VAE-embed-MNIST.pdf}
		\caption{HCNN (74.87)}
	\end{subfigure}
	\hfill
	\caption{Embeddings of MNIST dataset in 2D latent space of VAEs (with gen. FID). Colors represent golden labels and Lorentz embeddings are projected onto the Poincaré ball for better visualization.}
	\label{fig:VAEembeds}
\end{figure}

\section{Conclusion}

In this work, we proposed HCNN, a generalization of the convolutional neural network that learns latent feature representations in hyperbolic spaces. To this end, we formalized the necessary modules in the Lorentz model, deriving novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. We empirically demonstrated that ResNet and VAE models based on our hyperbolic framework achieve better performance on standard vision tasks than Euclidean and hybrid decoder baselines, especially in adversarial and lower dimensional settings. Additionally, we showed that using the Lorentz model in HNNs leads to better stability and performance than the Poincaré ball. 

However, hyperbolic CNNs are still in their early stages and introduce mathematical complexity and computational overhead. For this, we explored HECNN models with the benefit of targeting only specific parts of the encoder, allowing for faster runtimes and larger models. Moreover, our framework currently relies on generalizations of neural network layers that were designed for Euclidean geometry and might not fully capture the unique properties of hyperbolic geometry. Further research is needed to fully understand the properties of HCNNs and address open questions such as optimization, scalability, and performance on other deep learning problems. We hope our work will inspire future research and development in this exciting and rapidly evolving field.

\subsubsection*{Acknowledgments}

This work was performed on the HoreKa supercomputer funded by the Ministry of Science, Research and the Arts Baden-Württemberg and by the Federal Ministry of Education and Research. Ahmad Bdeir was funded by the European Union's Horizon 2020 research and innovation programme under the SustInAfrica grant agreement No 861924. Kristian Schwethelm was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project number 225197905.

{
\bibliographystyle{iclr2024_conference}
\bibliography{references}
}

\clearpage

\appendix

\section{Operations in hyperbolic geometry}\label{apdx:hyp_geometry}

\subsection{Lorentz and Poincare}\label{apdx:diff_hypers}

\paragraph{Poincaré ball} The n-dimensional Poincaré ball $\mathbb{B}^n_K = (\mathcal{B}^n, \mathfrak{g}_{\vect{x}}^K)$ is defined by $\mathcal{B}^n = \{\vect{x} \in\mathbb{R}^n~|~-K||\vect{x}||^2<1\}$ and the Riemannian metric $\mathfrak{g}_{\vect{x}}^K = (\lambda^K_{\vect{x}})^2\matr{I}_n$, where $\lambda_{\vect{x}}^K=2(1+K||\vect{x}||^2)^{-1}$. It describes the hyperbolic space by an open ball of radius $\sqrt{-1/K}$, see Figure \ref{fig:hyp_mod}.

\subsection{Lorentz model}\label{apdx:lorentz_model}

\begin{figure}[h]
	\centering
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=.9\textwidth]{figs/hyp-distance.pdf}
		\caption{Geodesic}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=.9\textwidth]{figs/hyp-tangent.pdf}
		\caption{Tangent space}
	\end{subfigure} 
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=.9\textwidth]{figs/hyp-parallel.pdf}
		\caption{Parallel Transport}
	\end{subfigure}
	\hfill
	\caption[Illustrations of geometrical operations in the 2-dimensional Lorentz model]{Illustrations of geometrical operations in the 2-dimensional Lorentz model. (a) The shortest distance between two points is represented by the connecting geodesic (red line). (b) The red line gets projected onto the tangent space of the origin resulting in the green line. (c) The green line gets parallel transported to the tangent space of the origin.}
	\label{fig:LorentzOps}
\end{figure}

\begin{figure}[h]
	\centering
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{figs/LRot.pdf}
		\caption{Lorentz rotation}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{figs/LBoost.pdf}
		\caption{Lorentz boost}
	\end{subfigure}
	\hfill
	\caption{Illustration of the Lorentz transformations in the 2-dimensional Lorentz model.}
	\label{fig:LorentzTransformations}
\end{figure}

In this section, we describe essential geometrical operations in the Lorentz model. Most of these operations are defined for all Riemannian manifolds and thus introduced for the general case first. However, the closed-form formulae are only given for the Lorentz model. We also provide visual illustrations in Figure \ref{fig:LorentzOps}.

\paragraph{Distance}

Distance is defined as the length of the shortest path between a pair of points on a surface. While in Euclidean geometry, this is a straight line, in hyperbolic space, the shortest path is represented by a curved geodesic generalizing the notion of a straight line. In the Lorentz model, the distance is inherited from Minkowski space. Let $\vect{x}, \vect{y} \in \mathbb{L}^n_K$ denote two points in the Lorentz model. Then, the length of the connecting geodesic and, thereby, the distance is given by
\begin{equation}
	d_{\mathcal{L}}(\vect{x},\vect{y}) = \frac{1}{\sqrt{-K}}\cosh^{-1}(K\langle \vect{x},\vect{y}\rangle_{\mathcal{L}}),
\end{equation}
and the squared distance \citep{law-et-al-2019} by
\begin{equation}
	d^2_{\mathcal{L}}(\vect{x},\vect{y}) = ||\vect{x}-\vect{y}||_{\mathcal{L}}^2 = \frac{2}{K}-2\lprod{\vect{x}}{\vect{y}}.
\end{equation}

When calculating the distance of any point $\vect{x} \in \mathbb{L}^n_K$ to the origin $\origin$, the equations can be simplified to

\begin{equation}
	d_{\mathcal{L}}(\vect{x},\origin) = ||\logmap{\origin}{\vect{x}}||,
\end{equation}

\begin{equation}
	d_{\mathcal{L}}^2(\vect{x},\origin) = \frac{2}{K} (1+\sqrt{-K}x_t).
\end{equation}

\paragraph{Tangent space}

The space around each point $\vect{x}$ on a differentiable manifold $\mathcal{M}$ can be linearly approximated by the tangent space $\mathcal{T}_{\vect{x}}\mathcal{M}$. It is a first-order approximation bridging the gap to Euclidean space. This helps performing Euclidean operations, but it introduces an approximation error, which generally increases with the distance from the reference point. Let $\vect{x} \in \mathbb{L}^n_K$, then the tangent space at point $\vect{x}$ can be expressed as
\begin{equation}
	\mathcal{T}_{\vect{x}}\mathbb{L}_K^n := \{\vect{y}\in \mathbb{R}^{n+1}~|~\langle \vect{y}, \vect{x}\rangle_{\mathcal{L}}=0\}.
\end{equation}

\paragraph{Exponential and logarithmic maps}

Exponential and logarithmic maps are mappings between the manifold $\mathcal{M}$ and the tangent space $\mathcal{T}_{\vect{x}}\mathcal{M}$ with $\vect{x}\in \mathcal{M}$. The exponential map $\expmap{\vect{x}}{\vect{z}}: \tangentsp{\vect{x}} \rightarrow \mathbb{L}^n_K$ maps a tangent vector $\vect{z} \in \tangentsp{\vect{x}}$ on the Lorentz manifold by

\begin{gather}
	\exp_{\vect{x}}^K(\vect{z}) = \cosh(\alpha) \vect{x} + \sinh(\alpha)\frac{\vect{z}}{\alpha}, \text{ with  }
	\alpha = \sqrt{-K}||\vect{z}||_{\mathcal{L}},~~
	||\vect{z}||_{\mathcal{L}}=\sqrt{{\langle \vect{z},\vect{z}\rangle}_{\mathcal{L}}}.
\end{gather}

The logarithmic map is the inverse mapping and maps a vector $\vect{y}\in\mathbb{L}^n_K$ to the tangent space of $\vect{x}$ by
\begin{gather}
	\log_{\vect{x}}^K(\vect{y}) = \frac{\cosh^{-1}(\beta)}{\sqrt{\beta^2-1}}\cdot (\vect{y}-\beta \vect{x}), \text{ with  }
	\beta=K\langle \vect{x},\vect{y}\rangle_{\mathcal{L}}.
\end{gather}

In the special case of working with the tangent space at the origin $\origin$, the exponential map simplifies to

\begin{align}
	\exp_{\origin}^K(\vect{z}) = \frac{1}{\sqrt{-K}}\left[\cosh(\sqrt{-K}  ||\vect{z}||),~\sinh(\sqrt{-K}  ||\vect{z}||)\frac{\vect{z}}{ ||\vect{z}||}\right].
\end{align}

\paragraph{Parallel transport}

The parallel transport operation $\transp{\vect{x}}{\vect{y}}{\vect{v}}$ maps a vector $\vect{v} \in \mathcal{T}_{\vect{x}}\mathcal{M}$ from the tangent space of $\vect{x} \in \mathcal{M}$ to the tangent space of $\vect{y} \in \mathcal{M}$. It preserves the local geometry around the reference point by moving the points along the geodesic connecting $\vect{x}$ and $\vect{y}$. 
The formula for the Lorentz model is given by

\begin{align}
	\transp{\vect{x}}{\vect{y}}{\vect{v}} &= \vect{v} - \frac{\lprod{\logmap{\vect{x}}{\vect{y}}}{\vect{v}}}{d_{\mathcal{L}}(\vect{x},\vect{y})}(\logmap{\vect{x}}{\vect{y}}+ \logmap{\vect{y}}{\vect{x}})\\
	&= \vect{v} + \frac{\lprod{\vect{y}}{\vect{v}}}{\frac{1}{-K}-\lprod{\vect{x}}{\vect{y}}}(\vect{x}+\vect{y}).
\end{align}

\paragraph{Lorentzian centroid \citep{law-et-al-2019}} The weighted centroid with respect to the squared Lorentzian distance, which solves $\min_{\vect{\mu} \in \mathbb{L}^n_K}\sum_{i=1}^{m}\nu_i d^2_{\mathcal{L}}(\vect{x}_i, \vect{\mu})$, with $\vect{x}_i \in \mathbb{L}^n_K$ and $\nu_i\geq0, \sum_{i=1}^{m}\nu_i > 0$, is given by

\begin{equation}
	\vect{\mu} = \frac{\sum_{i=1}^{m}\nu_i\vect{x}_i}{\sqrt{-K}\left|||\sum_{i=1}^{m}\nu_i\vect{x}_i||_{\mathcal{L}}\right|}.
\end{equation}

{\paragraph{Lorentz average pooling} The average pooling layer is implemented by computing the Lorentzian centroid of all hyperbolic features within the receptive field.}

\paragraph{Lorentz transformations}

The set of linear transformations in the Lorentz model are called Lorentz transformations. A transformation matrix $\matr{A}^{(n+1)\times(n+1)}$ that linearly maps $\mathbb{R}^{n+1} \rightarrow \mathbb{R}^{n+1}$ is called Lorentz transformation if and only if $\lprod{\matr{A}\vect{x}}{\matr{A}\vect{y}} = \lprod{\vect{x}}{\vect{y}}~\forall~\vect{x},\vect{y} \in \mathbb{R}^{n+1}$. The set of matrices forms an orthogonal group $\bm{O}(1,n)$ called the Lorentz group. As the Lorentz model only uses the upper sheet of the two-sheeted hyperboloid, the transformations under consideration here lie within the positive Lorentz group $\bm{O}^+(1,n) = \{\matr{A}\in \bm{O}(1,n): a_{11}>0\}$, preserving the sign of the time component $x_t$ of $\vect{x}\in\mathbb{L}^n_K$. Specifically, here, the Lorentz transformations can be formulated as 

\begin{equation}
	\bm{O}^+(1,n) = \{\matr{A} \in\mathbb{R}^{(n+1)\times (n+1)}~|~\forall \vect{x} \in \mathbb{L}^n_K: \lprod{\matr{A}\vect{x}}{\matr{A}\vect{x}} = \frac{1}{K}, (\matr{A}\vect{x})_0>0)\}.
\end{equation}

Any Lorentz transformation can be decomposed into a Lorentz rotation and Lorentz boost by polar decomposition $\matr{A} = \matr{R}\matr{B}$ \citep{moretti-2002}. The former rotates points around the time axis, using matrices given by

\begin{equation}
	\matr{R} = \left[\begin{array}{cc}
		1 & \vect{0}^T\\ \vect{0} & \tilde{\matr{R}}
\end{array}\right],
\end{equation}

where $\vect{0}$ is a zero vector, $\tilde{\matr{R}}^T\tilde{\matr{R}} = \matr{I}$, and $\det(\tilde{\matr{R}})=1$. This shows that the Lorentz rotations for the upper sheet lie in a special orthogonal subgroup $\bm{SO}^+(1,n)$ preserving the orientation, while $\tilde{\matr{R}} \in \bm{SO}(n)$. On the other side, the Lorentz boost moves points along the spatial axis given a velocity $\vect{v}\in\mathbb{R}^n, ||\vect{v}||<1$ without rotating them along the time axis. Formally, the boost matrices are given by

\begin{equation}
	\matr{B} = \left[\begin{array}{cc}
		\gamma & -\gamma \vect{v}^T\\
		-\gamma \vect{v} & \matr{I}+\frac{\gamma^2}{1+\gamma}\vect{v}\vect{v}^T
	\end{array}
	\right],
\end{equation}

with $\gamma = \frac{1}{\sqrt{1-||\vect{v}||^2}}$. See Figure \ref{fig:LorentzTransformations} for illustrations of the Lorentz rotation and Lorentz boost.

\paragraph{Lorentz fully-connected layer} Recently, \citet{chen2021} showed that the linear transformations performed in the tangent space \citep{ganea-et-al-2018,nickel-kiela-2018} can not apply all Lorentz transformations but only a special rotation and no boost. They proposed a direct method in pseudo-hyperbolic space\footnote{\citet{chen2021} note that their general formula is not fully hyperbolic, but a relaxation in implementation, while the input and output are still guaranteed to lie in the Lorentz model.}, which can apply all Lorentz transformations. Specifically, let $\vect{x}\in\mathbb{L}^n_K$ denote the input vector and $\matr{W}\in \mathbb{R}^{m\times n+1}$, $\vect{v}\in \mathbb{R}^{n+1}$ the weight parameters, then the transformation matrix is given by

\begin{equation}\label{eq:lolin}
	f_{\vect{x}}(\matr{M}) = f_{\vect{x}}\left(\left[\begin{array}{c}
		\vect{v}^T\\
		\matr{W}
	\end{array}\right]\right) =  \left[\begin{array}{c}
		\frac{\sqrt{||\matr{W}\vect{x}||^2-1/K}}{\vect{v}^T\vect{x}}\vect{v}^T\\
		\matr{W}
	\end{array}
	\right].
\end{equation}

Adding other components of fully-connected layers, including normalization, the final definition of the proposed Lorentz fully-connected layer becomes

\begin{equation}
	\vect{y} = \left[\begin{array}{c}
		\sqrt{||\phi(\matr{W}\vect{x}, \vect{v})||^2-1/K}\\
		\phi(\matr{W}\vect{x}, \vect{v})
	\end{array}
	\right],
\end{equation}

with operation function

\begin{equation}
	\phi(\matr{W}\vect{x},\vect{v}) = \lambda\sigma(\vect{v}^T\vect{x}+\vect{b}')\frac{\matr{W}\psi(\vect{x})+\vect{b}}{||\matr{W}\psi(\vect{x})+\vect{b}||},
\end{equation}

where $\lambda>0$ is a learnable scaling parameter and $\vect{b}\in\mathbb{R}^n,\psi,\sigma$ denote the bias, activation, and sigmoid function, respectively. 

In this work, we simplify the layer definition by removing the internal normalization, as we use batch normalization. This gives following formula for the Lorentz fully connected layer

\begin{equation}
	\vect{y} =  \text{LFC}(\vect{x}) = \left[\begin{array}{c}
		\sqrt{||\psi(\matr{W}\vect{x} + \vect{b})||^2-1/K}\\
		\psi(\matr{W}\vect{x} + \vect{b})
	\end{array}
	\right].
\end{equation}

\paragraph{Lorentz direct concatenation \citep{qu-zou-2022}} Given a set of hyperbolic points $\{\vect{x}_i \in \mathbb{L}^n_K\}_{i=1}^N$, the Lorentz direct concatenation is given by

\begin{equation}
	\vect{y} = \mathrm{HCat}(\{\vect{x}_i\}_{i=1}^N) = \left[\sqrt{\sum_{i=1}^{N}x^2_{i_t} + \frac{N-1}{K}}, \vect{x}^T_{1_s},\dots, \vect{x}^T_{N_s}\right]^T,
\end{equation}

where $\vect{y}  \in \mathbb{L}^{nN}_K \subset \mathbb{R}^{nN+1}$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.55\textwidth]{figs/wrapped_normal_expmap.pdf}
	\caption{Growth of the space component's norm $||\vect{z}_s||$ after applying the exponential map to an  $n$-dimensional vector $\bm{1}_n = (1,...,1) \in \tangentsp{\origin}$ with curvature $K=-1$. The y-axis is scaled logarithmically.}
	\label{fig:space_wrapped}
\end{figure}

\paragraph{Wrapped normal distribution} \citet{nagano-et-al-2019} proposed a wrapped normal distribution in the Lorentz model, which offers efficient sampling, great flexibility, and a closed-form density formulation. It can be constructed as follows:

\begin{enumerate}
	\item Sample a Euclidean vector $\vect{\tilde{v}}$ from the Normal distribution $\mathcal{N}(\vect{0},\vect{\Sigma})$.
	\item Assume the sampled vector lies in the tangent space of the Lorentz model's origin $\vect{v} = [0, \vect{\tilde{v}}] \in \tangentsp{\origin}$.
	\item Parallel transport $\vect{v}$ from the tangent space of the origin to the tangent space of a new mean $\vect{\mu} \in \mathbb{L}^n_K$, yielding a tangent vector $\vect{u} \in \tangentsp{\vect{\mu}}$.
	\item Map $\vect{u}$ to $\mathbb{L}^n_K$ by applying the exponential map, yielding the final sample $\vect{z} \in \mathbb{L}^n_K$.
\end{enumerate}

The distribution is parameterized by a Euclidean variance $\vect{\Sigma} \in \mathbb{R}^{n\times n}$ and a hyperbolic mean $\vect{\mu} \in \mathbb{L}^n_K$.

This method has shown to work well in hybrid HNN settings. However, in our fully hyperbolic VAE, high Euclidean variances destabilize the model. This is because, usually, the VAE's prior is set to a standard normal distribution with unit variance $\vect{\tilde{v}}\sim \mathcal{N}(\vect{0}, \matr{I})$. However, for high dimensional spaces, this leads to large values after the exponential map. That is why we propose to scale the prior variance as follows.

Let $\vect{v}\in \tangentsp{\origin}$ denote a vector in the tangent space of the origin. Then the space component of the hyperbolic vector $\vect{z}\in \mathbb{L}_K^n$ resulting from the exponential map is given by

\begin{equation}
	\vect{z}_s = (\expmap{\origin}{\vect{v}})_s = \frac{1}{\sqrt{-K}}\sinh(\sqrt{-K}||\vect{v}||)\frac{\vect{v}}{||\vect{v}||}.
\end{equation}

This shows that the norm of the space component depends on the sinh function, which grows approximately exponentially with the norm of the tangent vector ($||\vect{z}_s||~= \frac{1}{\sqrt{-K}}\sinh(\sqrt{-K}||\vect{v}||)$). The norm of the space component is important as it gets used to calculate the time component $z_t = \sqrt{||\vect{z}_s||^2-1/K}$, and it indicates how large the values of the hyperbolic points are. Now, assume an n-dimensional vector $\bm{1}_n = (1,...,1) \in \tangentsp{\origin}$, resembling the diagonal of the covariance matrix. Applying the exponential map to such a vector leads to fast-growing values with respect to the dimensionality $n$ because the norm of the tangent vector increases with $n$:

\begin{equation}
	||\bm{1}_n||~= \sqrt{\sum_{i=1}^{n}1^2} = \sqrt{n}.
\end{equation}

To work against this, we propose to clip the norm of the prior variance as follows

\begin{equation}\label{eq:scaling}
    \vect{\sigma}^2 = \begin{cases}
        \frac{s}{\sqrt{n}}:~~\mathrm{if}~\sqrt{n}>s\\
        \vect{\sigma}^2:~~~\mathrm{otherwise}
    \end{cases},
\end{equation}

where $s$ parameterizes the resulting norm. This achieves a clipped time component with respect to the dimensionality of $\vect{1}_n$ (see Figure \ref{fig:space_wrapped}). Furthermore, it offers nice interpretability using the Fréchet variance. As the distribution has zero mean, the Fréchet variance is given by the distance to the origin, which can be calculated by the norm of the tangent vector. This shows that this method controls the Fréchet variance. In practice, we empirically found $s = 2.5$ to be a good value.

Additionally, to prevent the HCNN-VAE from predicting relatively high variances, the scaling in Eq. \ref{eq:scaling} is applied. In this case, the Fréchet variance is not predefined, as usually $\vect{\sigma}^2 \neq \bm{1}_n$. However, it introduces a scaling operation resembling the variance scaling of the prior.

\subsection{Mapping between models} 

Because of the isometry between models of hyperbolic geometry, points in the Lorentz model can be mapped to the Poincaré ball by the following diffeomorphism

\begin{equation}
	p_{\mathbb{L}^n_K\rightarrow\mathbb{B}^n_K}(\vect{x}) = \frac{\vect{x_s}}{x_t + \frac{1}{\sqrt{-K}}}.
\end{equation}

\section{Proofs}

\subsection{Proofs for Lorentz hyperplane}\label{apdx:proof_hyperplane}

This section contains the proof for the Euclidean reparameterization of the Lorentz hyperplane proposed by \citet{mishne-et-al-2022}. Unfortunately, the authors only provided proof for the unit Lorentz model, i.e., assuming a curvature of $K=-1$. However, in general, the curvature can be different as $K<0$. That is why we reproduce their proof for the general case.

\paragraph{Proof for Eq. \ref{eq:w_parametr}}

Let $a\in\mathbb{R}$, $\vect{z}\in \mathbb{R}^n$, and $\vect{\overline{z}}\in \tangentsp{\origin} = [0,a{\vect{z}/}{||\vect{z}||}]$. Then, \citet{mishne-et-al-2022} parameterize a point in the Lorentz model as follows

\begin{align}
	\vect{p}\in\mathbb{L}^n_K &:= \exp_{\origin}\left(a\frac{\vect{z}}{||\vect{z}||}\right)\\
	&= \left[\frac{1}{\sqrt{-K}}\cosh(\alpha),~\sinh(\alpha)\frac{a\frac{\vect{z}}{||\vect{z}||}}{\alpha}\right].
\end{align}

Now, with $\alpha = \sqrt{-K}||a\frac{\vect{z}}{||\vect{z}||}||_{\mathcal{L}} = \sqrt{-K} a$ we get

\begin{align}
	\vect{p} &= \left[\cosh(\sqrt{-K} a) \frac{1}{\sqrt{-K}},~\sinh(\sqrt{-K} a)\frac{a\frac{\vect{z}}{||\vect{z}||}}{\sqrt{-K} a}\right]\\
	&= \frac{1}{\sqrt{-K}}\left[\cosh(\sqrt{-K} a),~\sinh(\sqrt{-K} a)\frac{\vect{z}}{||\vect{z}||}\right].
\end{align}

This definition gets used to reparameterize the hyperplane parameter $\vect{w}$ as follows

\begin{align*}
	\vect{w} &:= \transp{\origin}{\vect{p}}{\overline{\vect{z}}}\\
	&= \overline{\vect{z}} + \frac{\lprod{\vect{p}}{\overline{\vect{z}}}}{\frac{1}{-K}-\lprod{\origin}{\vect{p}}}(\origin+\vect{p})\\
	&= [0,\vect{z}]^T + \frac{\lprod{\vect{p}}{[0,\vect{z}]^T}}{\frac{1}{-K}-\lprod{\origin}{\vect{p}}}(\origin+\vect{p})\\
	&= [0,\vect{z}]^T + \frac{\frac{1}{\sqrt{-K}}\sinh(\sqrt{-K}a)||\vect{z}||}{\frac{1}{-K}+\frac{1}{-K}\cosh(\sqrt{-K}a)} \cdot \frac{1}{\sqrt{-K}}\left[1 + \cosh(\sqrt{-K}a),~\sinh(\sqrt{-K}a)\frac{\vect{z}}{||\vect{z}||}\right]\\
	&= [0,\vect{z}]^T + \frac{\sinh(\sqrt{-K}a)||\vect{z}||}{1+\cosh(\sqrt{-K}a)} \cdot \left[1 + \cosh(\sqrt{-K}a),~\sinh(\sqrt{-K}a)\frac{\vect{z}}{||\vect{z}||}\right]\\
	&= \left[\sinh(\sqrt{-K}a)||\vect{z}||,~\vect{z}+\frac{\sinh^2(\sqrt{-K}a)}{1+\cosh(\sqrt{-K}a)}\vect{z}\right]\\
	&= \left[\sinh(\sqrt{-K}a)||\vect{z}||,~\vect{z}+\frac{\cosh^2(\sqrt{-K}a)-1}{1+\cosh(\sqrt{-K}a)}\vect{z}\right]\\
	&= [\sinh(\sqrt{-K}a)||\vect{z}||,~\cosh(\sqrt{-K}a)\vect{z}].
\end{align*}

\paragraph{Proof for Eq. \ref{eq:hyppp}} 

After inserting Eq. \ref{eq:w_parametr} into Eq. \ref{eq:hyperplane_def} and solving the inner product, the hyperplane definition becomes

\begin{equation}
	\tilde{H}_{\vect{z},a} = \{\vect{x}\in \mathbb{L}^n_K~|~ \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t = 0\}.
\end{equation}

\subsection{Proof for distance to Lorentz hyperplane}\label{apdx:proof_dist_hyperpl}

\paragraph{Proof for Theorem \ref{theorem:disthyp}}
To proof the distance of a point to hyperplanes in the Lorentz model, we follow the approach of \citet{cho-et-al-2019} and utilize the hyperbolic reflection. The idea is, that a hyperplane defines a reflection that interchanges two half-spaces. Therefore, the distance from a point $\vect{x}\in\mathbb{L}^n_K$ to the hyperplane $H_{\vect{w},\vect{p}}$ can be calculated by halving the distance to its reflection in the hyperplane $\vect{x}\rightarrow\vect{y}_{\vect{w}}$

\begin{equation}\label{eq:dist1}
    d_{\mathcal{L}}(\vect{x},H_{\vect{w},\vect{p}}) = \frac{1}{2}d_{\mathcal{L}}(\vect{x},\vect{y}_{\vect{w}}).
\end{equation}

The hyperbolic reflection is well-known in the literature \citep{reflection} and can be formulated as

\begin{equation}\label{eq:reflection}
    \vect{y}_{\vect{w}} = \vect{x}+\frac{2\lprod{\vect{w}}{\vect{x}}\vect{w}}{\lprod{\vect{w}}{\vect{w}}},
\end{equation}

where $\vect{w}$ is the perpendicular vector to the hyperplane and $\lprod{\vect{w}}{\vect{w}}>0$. Now, inserting Eq. \ref{eq:reflection} into Eq. \ref{eq:dist1} we can compute the distance to the hyperplane as follows

\begin{align*}
    d_{\mathcal{L}}(\vect{x},H_{\vect{w},\vect{p}}) &= \frac{1}{2\sqrt{-K}}\cosh^{-1}(K\lprod{\vect{x}}{\vect{y}_{\vect{w}}})\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}(K\lprod{\vect{x}}{\vect{x}+\frac{2\lprod{\vect{w}}{\vect{x}}\vect{w}}{\lprod{\vect{w}}{\vect{w}}}})\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}(2K\lprod{\vect{x}}{\vect{x}} + K \lprod{\vect{x}}{\frac{\lprod{\vect{w}}{\vect{x}}\vect{w}}{\lprod{\vect{w}}{\vect{w}}}})\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}\left(K\frac{1}{K} + 2K \left(\frac{\lprod{\vect{w}}{\vect{x}}}{\sqrt{\lprod{\vect{w}}{\vect{w}}}}\right)^2\right)\\[5pt]
    &= \frac{1}{2\sqrt{-K}}\cosh^{-1}\left(1 + 2 \left(\sqrt{-K}\frac{\lprod{\vect{w}}{\vect{x}}}{\sqrt{\lprod{\vect{w}}{\vect{w}}}}\right)^2\right)\\[5pt]
    &= \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\lprod{\vect{w}}{\vect{x}}}{\lnorm{\vect{w}}}\right)\right|,
\end{align*}
which gives the final formula:
\begin{equation}\label{eq:stdDistHyp}
    d_{\mathcal{L}}(\vect{x},H_{\vect{w},\vect{p}}) = \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\lprod{\vect{w}}{\vect{x}}}{\lnorm{\vect{w}}}\right)\right|.
\end{equation}


Comparing Eq. \ref{eq:stdDistHyp} to the equation of \citet{cho-et-al-2019} shows that the distance formula for hyperplanes in the unit Lorentz model can be extended easily to the general case by inserting the curvature parameter $K$ at two places.

Finally, defining $\vect{w}$ with the aforementioned reparameterization

\begin{equation}\label{eq:wrepara}
    \vect{w} := \transp{\origin}{\vect{p}}{\overline{\vect{z}}} = [\sinh(\sqrt{-K}a)||\vect{z}||, \cosh(\sqrt{-K}a)\vect{z}],
\end{equation}

and solving the inner products, gives our final distance formula

\begin{align}
    d_{\mathcal{L}}(\vect{x}, \tilde{H}_{\vect{z},a}) = \frac{1}{\sqrt{-K}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a)~||\vect{z}||~x_t}{\sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2}}\right)\right|.
\end{align}

\subsection{Proof for logits in the Lorentz MLR classifier}\label{apdx:proof_lMLR}

\paragraph{Proof for Theorem \ref{theorem:lorentzMLR}}

Following \citet{mlr}, given input $\vect{x}\in\mathbb{R}^n$ and $C$ classes, the Euclidean MLR logits of class $c\in\{1,...,C\}$ can be expressed as

\begin{equation}\label{eq:mlrEucl}
    v_{\vect{w}_c}(\vect{x})=\text{sign}(\langle\vect{w}_c,\vect{x}\rangle)||\vect{w}_c||d(x, H_{\vect{w}_c}),~~\vect{w}_c\in\mathbb{R}^n,
\end{equation}

where $H_{\vect{w}_c}$ is the decision hyperplane of class $c$.

Replacing the Euclidean operations with their counterparts in the Lorentz model yields logits of class $c$ for $\vect{x}\in\mathbb{L}^n_K$ as follows

\begin{equation}\label{eq:logitsHypHyp}
    v_{\vect{w}_c, \vect{p}_c}(\vect{x})=\text{sign}(\lprod{\vect{w}_c}{\vect{x}})||\vect{w}_c||_{\mathcal{L}}d_{\mathcal{L}}(x, H_{\vect{w}_c,\vect{p}_c}),
\end{equation}

with $\vect{w}_c\in\tangentsp{\vect{p}_c}, \vect{p}_c\in\mathbb{L}^n_K$, and $\lprod{\vect{w}_c}{\vect{w}_c}>0$.

Inserting Eq. \ref{eq:stdDistHyp} into Eq. \ref{eq:logitsHypHyp} gives a general formula without our reparameterization

\begin{equation}
    v_{\vect{w}_c, \vect{p}_c}(\vect{x})=\frac{1}{\sqrt{-K}}~\text{sign}(\lprod{\vect{w}_c}{\vect{x}})||\vect{w}_c||_{\mathcal{L}}\left|\sinh^{-1}\left(\sqrt{-K}\frac{\lprod{\vect{w}_c}{\vect{x}}}{\lnorm{\vect{w}_c}}\right)\right|.
\end{equation}

Now, we reparameterize $\vect{w}$ with Eq. \ref{eq:wrepara} again, which gives

\begin{align}
    \alpha := \lprod{\vect{w}_c}{\vect{x}} = \cosh(\sqrt{-K}a)\langle \vect{z}, \vect{x}_s\rangle-\sinh(\sqrt{-K}a),
\end{align}

\begin{align}
    \beta := \lnorm{\vect{w}_c} = \sqrt{||\cosh(\sqrt{-K}a)\vect{z}||^2-(\sinh(\sqrt{-K}a)||\vect{z}||)^2},
\end{align}

with $a\in\mathbb{R}$ and $\vect{z}\in\mathbb{R}^n$. Finally, we obtain the equation in Theorem \ref{theorem:lorentzMLR}:

\begin{equation}
    v_{\vect{z}_c, a_c}(\vect{x})=\frac{1}{\sqrt{-K}}~\mathrm{sign}(\alpha)\beta\left|\sinh^{-1}\left(\sqrt{-K}\frac{\alpha}{\beta}\right)\right|.
\end{equation}

\section{Additional experimental details}\label{apdx:exp_details}

\subsection{Classification}\label{apdx:exp_details_class}

\paragraph{Datasets} For classification, we employ the benchmark datasets CIFAR-10 \citep{cifar}, CIFAR-100 \citep{cifar}, and Tiny-ImageNet \citep{tinyimagenet}. The CIFAR-10 and CIFAR-100 datasets each contain 60,000 $32\times 32$ colored images from 10 and 100 different classes, respectively. We use the dataset split implemented in PyTorch, which includes 50,000 training images and 10,000 testing images. Tiny-ImageNet is a small subset of the ImageNet \citep{imagenet} dataset, with 100,000 images of 200 classes downsized to $64 \times 64$. Here, we use the official validation split for testing our models.

\paragraph{Settings} 

Table \ref{tab:hyperparameters_cl} summarizes the hyperparameters we adopt from \citet{resnetHyp} to train all classification models. Additionally, we use standard data augmentation methods in training, i.e., random mirroring and cropping. Regarding the feature clipping in hybrid HNNs, we tune the feature clipping parameter $r$ between 0.8 and 5.0 and find that, for most experiments, the best feature clipping parameter is $r=1$. Only the Lorentz hybrid ResNet performs best with $r=4$ on Tiny-ImageNet, and $r=2$ on CIFAR-100 with lower embedding dimensions. Overall, we observe that the hybrid Lorentz ResNet has fewer gradient issues, allowing for higher clipping values. The HCNN-ResNet does not need tuning of any additional hyperparameters.

\begin{table}[h]
	\centering
	\caption{Summary of hyperparameters used in training classification models.}
	\label{tab:hyperparameters_cl}
	\begin{tabular}{lr}
        \toprule
        Hyperparameter & Value\\
        \midrule
		{Epochs} & 200\\
		{Batch size} & 128\\
		{Learning rate (LR)} & 1e-1\\
        {Drop LR epochs} & 60, 120, 160\\
        {Drop LR gamma} & 0.2\\
		{Weight decay} & 5e-4\\
		{Optimizer} & (Riemannian)SGD\\
		{Floating point precision} & 32 bit\\
  		{GPU type} & RTX A5000\\
        {Num. GPUs} & 1 or 2\\
        {Hyperbolic curvature $K$} & $-1$\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Generation}

\paragraph{Datasets} For image generation, we use the aforementioned CIFAR-10 \citep{cifar} and CIFAR-100 \citep{cifar} datasets again. Additionally, we employ the CelebA \citep{celeba} dataset, which includes colored $64\times64$ images of human faces. Here, we use the PyTorch implementation, containing 162,770 training images, 19,867 validation images, and 19,962 testing images.

\paragraph{Settings} For hyperbolic and Euclidean models, we use the same architecture (see Table \ref{tab:architecture}) and training hyperparameters (see Table \ref{tab:hyperparameters_gen}). We employ a vanilla VAE similar to \citet{ghosh-et-al-2019} as the baseline Euclidean architecture (E-VAE). For the hybrid model, we replace the latent distribution of the E-VAE with the hyperbolic wrapped normal distribution in the Lorentz model \citep{nagano-et-al-2019} and the Poincaré ball \citep{mathieu-et-al-2019}, respectively. Replacing all layers with our proposed hyperbolic counterparts yields the fully hyperbolic model. Here, we include the variance scaling mentioned in Section \ref{apdx:lorentz_model}, as otherwise training fails with NaN errors. Furthermore, we set the curvature $K$ for the Lorentz model to $-1$ and for the Poincaré ball to $-0.1$. 

We evaluate the VAEs by employing two versions of the FID \citep{FID} implemented by \citet{Seitzer2020FID}:

\begin{enumerate}
	\item The \textit{reconstruction FID} gives a lower bound on the generation quality. It is calculated by comparing test images with reconstructed validation images. As the CIFAR datasets have no official validation set, we exclude a fixed random portion of 10,000 images from the training set.
	\item The \textit{generation FID} measures the generation quality by comparing random generations from the models' latent space with the test set.
\end{enumerate}

\begin{table}[h]
	\centering
	\caption{Vanilla VAE architecture employed in all image generation experiments. Convolutional layers have a kernel size of $3\times 3$ and transposed convolutional layers of $4\times 4$. $s$ and $p$ denote stride and zero padding, respectively. The MLR in the Euclidean model is mimicked by a $1\times1$ convolutional layer.}
	\label{tab:architecture}
	\begin{tabular}{lrr}
        \toprule
		Layer & CIFAR-10/100 & {CelebA}\\
		\midrule
		{\textsc{Encoder:}}  & &  \\
		$\rightarrow$ \textsc{Proj}$_{\mathbb{R}^n\rightarrow\mathbb{L}^n_K}$ & 32$\times$32$\times$3 & 64$\times$64$\times$3  \\
		$\rightarrow$ \textsc{Conv}$_{64,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU}  & 16$\times$16$\times$64  & 32$\times$32$\times$64\\
		$\rightarrow$ \textsc{Conv}$_{128,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 8$\times$8$\times$128& 16$\times$16$\times$128 \\
		$\rightarrow$ \textsc{Conv}$_{256,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 4$\times$4$\times$256& 8$\times$8$\times$256\\
		$\rightarrow$ \textsc{Conv}$_{512,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 2$\times$2$\times$512& 4$\times$4$\times$512\\
		$\rightarrow$ \textsc{Flatten}  & 2048 & 8192\\
		$\rightarrow$ \textsc{FC-Mean}$_{d}$  & 128 & 64 \\
		$\rightarrow$ \textsc{FC-Var}$_{d}$ $\rightarrow$ \textsc{Softplus} & 128 & 64 \\
		\midrule
		{\textsc{Decoder:}}   & &   \\
		$\rightarrow$ \textsc{Sample}  & 128 & 64  \\
		$\rightarrow$ \textsc{FC}$_{32768}$ $\rightarrow$ BN $\rightarrow$ \textsc{ReLU} & 32768 & 32768 \\
		$\rightarrow$ \textsc{Reshape}  & 8$\times$8$\times$512 & 8$\times$8$\times$512 \\
		$\rightarrow$ \textsc{ConvTr}$_{256,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 16$\times$16$\times$256 & 16$\times$16$\times$256\\
		$\rightarrow$ \textsc{ConvTr}$_{128,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & 32$\times$32$\times$128& 32$\times$32$\times$128 \\
		$\rightarrow$ \textsc{ConvTr}$_{64,s2,p1}$ $\rightarrow$ BN $\rightarrow$  \textsc{ReLU} & - & 64$\times$64$\times$64\\
		$\rightarrow$  \textsc{Conv}$_{64,s1,p1}$ & 32$\times$32$\times$64 & 64$\times$64$\times$64 \\
		$\rightarrow$  \textsc{MLR} & 32$\times$32$\times$3 & 64$\times$64$\times$3 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{Summary of hyperparameters used in training image generation models.}
	\label{tab:hyperparameters_gen}
	\begin{tabular}{lrrr}
        \toprule
		Hyperparameter & MNIST & CIFAR-10/100 &\textsc{CelebA}\\
		\midrule
		{Epochs}  & 100 & 100 & 70\\
		{Batch size}  & 100 & 100 & 100\\
		{Learning rate}  & 5e-4 & 5e-4 & 5e-4\\
		{Weight decay} & 0 & 0 & 0\\
		{KL loss weight} & 0.312 & 0.024 & 0.09\\
		{Optimizer}  & (Riem)Adam & (Riem)Adam & (Riem)Adam\\
		{Floating point precision} & 32 bit & 32 bit & 32 bit\\
		{GPU type} & RTX A5000 & RTX A5000 & RTX A5000\\
        {Num. GPUs} & 1 & 1 & 2\\
		\bottomrule
	\end{tabular}
\end{table}

\pagebreak

\section{Ablation experiments}\label{apdx:add_exp}

\subsection{Hyperbolicity} \label{apdx:hyperbolicity}

\begin{wraptable}{r}{0.35\textwidth}
	\centering
	\caption{$\delta_{rel}$ for intermediate embeddings of ResNet-18 trained on CIFAR-100.}
	\label{tab:hyperbolicity_test}
	\begin{adjustbox}{width=0.25\textwidth,center}
	\begin{tabular}{@{\extracolsep{4pt}}lr@{}}
        \toprule
        Encoder Section & $\delta_{rel}$\\
        \midrule
		{Initial Conv.} & 0.26\\
		{Block 1} & 0.19\\
		{Block 2} & 0.23\\
        {Block 3} & 0.18\\
        {Block 4} & 0.21\\
		\bottomrule
	\end{tabular}
 \end{adjustbox}
\end{wraptable}

Motivating the use of HNNs by measuring the $\delta$-hyperbolicity of visual datasets was proposed by \cite{khrulkov}. The idea is to first generate image embeddings from a vision model and then quantify the degree of inherent tree-structure. This is achieved by considering $\delta$-slim triangles and determining the minimal value that satisfies the triangle inequality using the Gromov product. The lower $\delta\geq0$ is, the higher the hyperbolicity of the dataset. Usually, the scale-invariant value $\delta_{rel}$ is reported. For further insights refer to \cite{khrulkov}.

While \cite{khrulkov} studies the hyperbolicity of the final embeddings, we extend the perspective to intermediate embeddings from the encoder, aiming to provide additional motivation for fully hyperbolic models. For this, we first train a Euclidean ResNet-18 classifier using the settings detailed in Appendix \ref{apdx:exp_details_class}. Then, we run the model on the test dataset and extract the embeddings after each ResNet block to assess their $\delta$-hyperbolicity. The results in Table \ref{tab:hyperbolicity_test} show that all intermediate embeddings exhibit a high degree of hyperbolicity. Furthermore, we observe a difference in hyperbolicity between blocks. This motivates hybrid hyperbolic encoders (HECNNs) with hyperbolic layer only used where hyperbolicity is the highest.

\subsection{HCNN components}

In this section, we perform ablation studies to obtain additional insights into our proposed HCNN components. All ablation experiments consider image classification on CIFAR-100 using ResNet-18. We estimate the mean and standard deviation from five runs, and the best performance is highlighted in bold.

\begin{wraptable}{r}{0.5\textwidth}
	\footnotesize
	\centering
	\caption{Runtime improvement using PyTorch's \textit{compile} function on ResNet-18. Duration of a training epoch in seconds.}
	\label{tab:runtime}
	\begin{adjustbox}{width=0.5\textwidth,center}
	\begin{tabular}{@{\extracolsep{4pt}}lrrrr@{}}
		\toprule
		& Euclidean & Hybrid L. & HCNN & HECNN\\
		\midrule
		Default & \textbf{7.4}  & 9.2 & 103.3 & 65.2 \\
		Compiled & \textbf{7.1} & 8.0 & 62.1 & 42.5 \\
		\midrule
		Difference & -4.1\% &  -13.0\% & \textbf{-39.9\%}& -35.8\%\\
		\bottomrule
	\end{tabular}
\end{adjustbox}
\end{wraptable} 

\paragraph{Runtime} Currently, two major drawbacks of HNNs are relatively high runtime and memory requirements. This is partly due to custom Pythonic implementations of hyperbolic network components introducing significant computational overhead. To study the overhead in practice and assess the efficiency of our implementations, we use PyTorch's \textit{compile} function, which automatically builds a more efficient computation graph. We compare the runtime of our Lorentz ResNets with the Euclidean baseline under compiled and default settings in Table \ref{tab:runtime} using an RTX 4090 GPU. 

The results show that hybrid HNNs only add little overhead compared to the significantly slower HCNN. This makes scaling HCNNs challenging and requires special attention in future works. However, we also see that hyperbolic models gain much more performance from the automatic compilation than the Euclidean model. This indicates greater room for improvement in terms of implementation optimizations.

\begin{wraptable}{r}{0.5\textwidth}
	\footnotesize
	\centering
	\caption{Normalization ablation.}
	\label{tab:abl_bnorm}
	\begin{tabular}{@{\extracolsep{4pt}}lr@{}}
		\toprule
		Normalization & Accuracy (\%)\\
		\midrule
		None & ${42.24_{\pm3.86}}$\\
        \midrule
        {Space bnorm} & {${77.48_{\pm0.25}}$}\\
        Tangent bnorm & NaN\\
        Riemannian bnorm \citep{lou-et-al-2020} & NaN\\
		LFC norm \citep{chen2021} & ${1.00_{\pm0.00}}$\\
		\midrule
        LBN + LFC norm \citep{chen2021} & ${76.98_{\pm0.18}}$\\
		LBN & $\bm{78.07_{\pm0.21}}$\\
		\bottomrule
	\end{tabular}
\end{wraptable} 

\paragraph{Batch normalization} We investigate the effectiveness of our proposed Lorentz batch normalization (LBN) compared to other possible methods. Specifically, we train HCNN-ResNets with different normalization methods and compare them against a model without normalization. The results are shown in Table \ref{tab:abl_bnorm}. 

Firstly, the batch normalization in tangent space and the Riemannian batch normalization \citep{lou-et-al-2020} lead to infinite loss within the first few training iterations. This could be caused by the float32 precision used in this work. Additionally, the normalization within the Lorentz fully-connected layer (LFC) proposed by \citet{chen2021} (see Section \ref{apdx:lorentz_model}) inhibits learning when not combined with our LBN. 

Conversely, using LBN alone improves convergence speed and accuracy significantly, getting approximately 36\% higher accuracy than the non-normalized model after 200 epochs. Combining LBN and the LFC normalization leads to worse accuracy and runtime, validating our modified LFC (see Section \ref{apdx:lorentz_model}). Overall, this experiment shows the effectiveness of our LBN and suggests that, currently, there is no viable alternative for HNNs using the Lorentz model. {However, a naive implementation operating on the hyperbolic space component can serve as a good initial baseline, although ignoring the properties of hyperbolic geometry.}

\begin{wraptable}{r}{0.4\textwidth}
	\footnotesize
	\centering
	\caption{Activation ablation.}
	\label{tab:abl_act}
	\begin{tabular}{@{\extracolsep{4pt}}lr@{}}
		\toprule
		Activation & Accuracy (\%)\\
		\midrule
		None & ${52.85_{\pm0.41}}$\\
		Tangent ReLU & ${77.43_{\pm0.45}}$\\
		\midrule
		Lorentz ReLU & $\bm{78.07_{\pm0.21}}$\\
		\bottomrule
	\end{tabular}
\end{wraptable} 

\paragraph{Non-linear activation} In this work, we propose a method for applying standard activation functions to points in the Lorentz model that is more stable and efficient than the usual tangent space activations. Here, we quantify the improvement by comparing HCNN-ResNets using different ReLU applications. Furthermore, we consider a model without activation functions, as the LFC is non-linear already and might not need such functions. 

The results in Table \ref{tab:abl_act} show that non-linear activations improve accuracy significantly and are therefore needed in HCNNs. Furthermore, compared to Tangent ReLU, our Lorentz ReLU increases the average accuracy by 0.64\%, decreases the runtime of a training epoch by about 16.7\%, and is more consistent overall.

\paragraph{Residual connection} In this experiment, we compare the effect of different approaches for residual connections on performance. As mentioned in Section \ref{sec:method:resAct}, vector addition is ill-defined in the Lorentz model. As alternatives, we test tangent space addition \citep{nickel-kiela-2018}, parallel transport (PT) addition \citep{chami-et-al-2019}, Möbius addition (after projecting to the Poincaré ball) \citep{ganea-et-al-2018}, fully-connected (FC) layer addition \citep{chen2021}, and our proposed space component addition. The results are shown in Table \ref{tab:abl_residual}.

In training, we observe that PT and Möbius addition make the model very unstable, causing an early failure in the training process. This might be because of the float32 precision again. The tangent space addition performs relatively well, but the needed exponential and logarithmic maps add computational overhead ($\approx$ 12\% higher runtime per training epoch) and some instability that hampers learning. The FC addition and our proposed space addition are very similar and perform best. However, our method is simpler and therefore preferred.

\begin{table}[h]
	\footnotesize
	\centering
	\caption{Residual connection ablation.}
	\label{tab:abl_residual}
	\begin{tabular}{@{\extracolsep{4pt}}lr@{}}
		\toprule
		Residual connection & Accuracy (\%)\\
		\midrule
        Tangent addition \citep{nickel-kiela-2018} & ${77.73_{\pm0.32}}$\\
		PT addition \citep{chami-et-al-2019} & NaN\\
        Möbius addition \citep{ganea-et-al-2018} & NaN\\
        FC addition \citep{chen2021} & ${77.93_{\pm0.25}}$\\
		\midrule
		Space addition & $\bm{78.07_{\pm0.21}}$\\
		\bottomrule
	\end{tabular}
\end{table} 

\paragraph{Initialization} \citet{chen2021} proposed initializing Lorentz fully-connected layers with the uniform distribution $\mathcal{U}(-0.02, 0.02)$. As the LFC is the backbone of our Lorentz convolutional layer, we test the uniform initialization in HCNNs and compare it to the standard Kaiming initialization \citep{kaiming_init} used in most Euclidean CNNs. For this, we employ the same ResNet architecture and initialize Lorentz convolutional layers with these two methods. 

The results in Table \ref{tab:abl_init} show that the Kaiming initialization is preferred in HCNNs, leading to 0.55\% higher accuracy.

\begin{table}[h]
	\footnotesize
	\centering
	\caption{Initialization ablation.}
	\label{tab:abl_init}
	\begin{tabular}{@{\extracolsep{4pt}}lr@{}}
		\toprule
		Initialization & Accuracy (\%)\\
		\midrule
        $\mathcal{U}(-0.02, 0.02)$ \citep{chen2021} & ${77.52_{\pm0.10}}$\\
		\midrule
		Kaiming \citep{kaiming_init} & $\bm{78.07_{\pm0.21}}$\\
		\bottomrule
	\end{tabular}
\end{table} 

\end{document}
