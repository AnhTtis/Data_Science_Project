{
    "arxiv_id": "2303.15805",
    "paper_title": "StarNet: Style-Aware 3D Point Cloud Generation",
    "authors": [
        "Yunfan Zhang",
        "Hao Wang",
        "Guosheng Lin",
        "Vun Chan Hua Nicholas",
        "Zhiqi Shen",
        "Chunyan Miao"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "eess.IV"
    ],
    "abstract": "This paper investigates an open research task of reconstructing and generating 3D point clouds. Most existing works of 3D generative models directly take the Gaussian prior as input for the decoder to generate 3D point clouds, which fail to learn disentangled latent codes, leading noisy interpolated results. Most of the GAN-based models fail to discriminate the local geometries, resulting in the point clouds generated not evenly distributed at the object surface, hence degrading the point cloud generation quality. Moreover, prevailing methods adopt computation-intensive frameworks, such as flow-based models and Markov chains, which take plenty of time and resources in the training phase. To resolve these limitations, this paper proposes a unified style-aware network architecture combining both point-wise distance loss and adversarial loss, StarNet which is able to reconstruct and generate high-fidelity and even 3D point clouds using a mapping network that can effectively disentangle the Gaussian prior from input's high-level attributes in the mapped latent space to generate realistic interpolated objects. Experimental results demonstrate that our framework achieves comparable state-of-the-art performance on various metrics in the point cloud reconstruction and generation tasks, but is more lightweight in model size, requires much fewer parameters and less time for model training.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15805v1"
    ],
    "publication_venue": null
}