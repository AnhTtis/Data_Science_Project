\section{Theoretical Model for First- and Second-Hand Evaluation }\label{sec:background}

Our study was primarily informed by a bidirectional model of first- and second-hand evaluation strategies \autocite{barzilai2020dealing} while also considering one additional feature of the Critical Online Resource Evaluation (CORE) framework \autocite{forzani2022does}. The bidirectional model of first- and second-hand evaluation strategies is based on the content-source-integration model \autocite{stadtler2014content} that suggests that readers employ two levels of strategies to resolve scientific conflicts. Readers can evaluate information validity to determine whether the presented claims are accurate (i.e., using first-hand evaluation). However, readers do not always have sufficient knowledge to make the appropriate judgment. In this case, they can resolve the conflict by turning to second-hand evaluation to determine whether source of information is trustworthy. 

In their bidirectional model of first- and second-hand evaluation strategies, \autocite{barzilai2020dealing} specify three first-hand evaluation strategies to judge the validity of scientific claims (i.e., distinguish accurate claims from inaccurate ones). These strategies are knowledge-based validation, discourse-based validation, and corroboration. When using knowledge-based validation, readers rely on their prior knowledge and beliefs about the topic, whereas when using discourse-based validation, readers use various discourse features, such as argument structure and quality to make judgements about the credibility of content. Corroboration, a third strategy for validating content, is when readers compare text content to what other resources state about a given issue. 

The term second-hand evaluation \autocites{barzilai2020dealing, stadtler2014content} refers to sourcing strategies that readers employ to judge a source's trustworthiness. Readers can make inferences and evaluate a source’s expertise, benevolence, and integrity \autocite{hendriks2015measuring} by relying on available information about the source. According to the model, readers use first- and second-hand evaluation strategies reciprocally. That is, content quality judgments inform source trustworthiness judgments and vice versa. In addition to the first and second-hand evaluation strategies, readers can also evaluate context as part of the credibility evaluation process, which \textcite{forzani2022does} refers to as tertiary evaluation. According to \textcite{forzani2022does}, context refers to the temporal, social, and political setting of the text (e.g., social media texts read in specific crowdsourcing platforms). In this study, we considered the effects of platform context, while also incorporating a slightly broader interpretation of context by considering contextual attributes of the readers evaluating the texts (i.e., adult readers representing different nationalities).   

\subsection{First-Hand Evaluation} 
In this study, we focused on credibility judgments of health-related social media posts to determine the extent to which readers rely on their prior beliefs (i.e., knowledge-based validation) and information presented in the posts (i.e., discourse-based validation) and sourcing. Next we define relevant concepts and outline previous research associated with first- and second-hand evaluation; first-hand evaluation involves knowledge-based and discourse-based validation while second-hand evaluation focuses on sourcing.  We also present research suggesting contextual attributes, such as a reader’s cultural background, can affect adults’ credibility evaluations in social media contexts.  

\subsubsection{Knowledge-Based Validation}

Prior knowledge and beliefs are internal resources that readers can use to quickly evaluate the accuracy of information \autocite{richter2017comprehension}. When readers evaluate information using their prior beliefs, they seem to favor belief-consistent information over belief-inconsistent information \autocites{abendroth2020mere, mccrudden2016differences, van2016attitude}. As such, readers attend to evidence that is consistent with their beliefs and resist evidence that contradicts their beliefs. This text-belief consistency effect is similar to confirmation bias \autocite{karimi2023thinking} - the term used in the psychological literature to refer to seeking or interpreting evidence in accordance with a person's existing beliefs, expectations, or hypotheses \autocite{nickerson1998confirmation}. However, a confirmation bias occurs when a reader starts with a particular view of a particular issue and then actively searches for additional information that upholds that view.  In contrast, the text-belief consistency effect occurs during the comprehension process \autocite{karimi2023thinking}, when a reader encounters information and, often unconsciously, attends to text that is consistent with their beliefs while also passing by text that contradicts their beliefs.

The text-belief consistency effect in credibility evaluation has been shown in different contexts, including isolated arguments with no indicators of context \textcite{mccrudden2016differences}, and arguments presented in the context of social media posts \autocite{wertgen2023source}. For example, \textcite{mccrudden2016differences} asked 72 high school students to evaluate eight isolated arguments about climate change. Four arguments claimed that humans affect climate change, and four other arguments claimed the opposite. Half of the arguments were strong, and half were weak. It was found that students evaluated belief-consistent arguments more favorably than belief-inconsistent arguments. However, arguments were not evaluated solely on the basis of their belief-consistency. Namely, students evaluated strong arguments more favorably than weak arguments. Importantly, further analysis showed that students’ evaluations of strong belief-inconsistent arguments and weak belief-consistent arguments did not differ. 

In a second study, \textcite{wertgen2023source} examined the text-belief consistency effect when asking university students to evaluate short messages, resembling Twitter posts, on four socio-scientific controversial topics (see Experiment 1). For each topic, the messages represented opposing positions. In addition to the position, \textcite{wertgen2023source} also manipulated the source-message consistency. In the consistent source-message condition, the source (e.g., World Wildlife Fund) presented the expected position (humans cause climate change). In the inconsistent source-message condition, the source presented an unexpected position (natural causes for climate change). Participants read belief-consistent messages faster and judged them more plausible than belief-inconsistent messages. The same pattern was found for source-message consistency. When the sources presented expected positions in the social media messages, the messages were read faster and rated slightly more plausible (or more likely to be acceptable in a certain situation); when the sources presented unexpected  positions, the messages were read more slowly and rated slightly less plausible. Overall, the use of prior beliefs as a resource could be especially applicable when people initially browse short social media messages using quick, superficial processing \autocite{metzger2013credibility} before they stop to evaluate the information more strategically.  

\subsubsection{Discourse-Based Validation}
According to \textcite{barzilai2020dealing}, evaluating arguments presented in a text is an essential component of discourse-based validation. Discursive elements include the argument’s structure and coherence as well as the quality of evidence provided. Arguments may have a rhetorical structure or a dialogic structure. A rhetorical argument includes a claim (or assertion) with an accompanying justification, which differs from a dialogic argument or debate, which includes an opposition between two or more assertions \autocite{kuhn1991skills}. A simple rhetorical argument contains one claim with only one instance of justifying support, which is called evidence \autocite{zarefsky2019practice} or data \autocite{toulmin2003uses}. According to \textcite{toulmin2003uses}, an argument can also include other components, such as a warrant, which is an often unstated but implicit inference that connects the evidence to the claim. For example, empirical research (used as evidence) entitles us to conclude that vaccines are safe (the claim) because we trust that the research has followed scientific standards (the warrant). Social media posts with restricted lengths, such as those found on Twitter, are typically structured as simple rhetorical arguments, making them a relevant context for examining how adults attend to the coherence of a single claim, evidence, and warrant to evaluate the credibility of a simple argument.

In addition to judging the coherence of the parts of an argument, readers can also attend to the quality of evidence in an argument. There are several different types of evidence \autocites{HornikxJ.M.A2005Aroe, kuhn1991skills, zarefsky2019practice}; these types reflect how the evidence was produced and vary in terms of their quality \autocite{ClarkA.Chinn2014ECaE}. Four types of evidence include evidence produced from research, testimonies, anecdotes, and social consensus \autocite{zarefsky2019practice}. Evidence resulting from rigorous research processes, such as experiments, statistical analyses, and meta-analyses, can offer causal and statistical evidence. A second type of evidence is testimonial evidence \autocite{zarefsky2019practice} or expert evidence \autocite{HornikxJ.M.A2005Aroe}. Testimony, which can consist of either a fact or an opinion, is a statement made by some source.  It is reasonable for an author to rely on testimonial evidence from a qualified source to support one’s claim when an author does not have direct knowledge of the topic but can trust the expertise of others. 

Anecdotal evidence, such as personal experience, relies on a narrative way of knowing \autocite{hinyard2007using} while social consensus refers to widely agreed-upon facts, shared value judgments, shared historical understandings, and previously established claims \autocite{zarefsky2019practice}. Anecdotal evidence and social consensus, two types of evidence that are not necessarily supported by accurate information (e.g., research evidence or expert testimony), are common in health communication, especially in social media contexts \autocite{suarez2021prevalence}. Thus, social media posts are an authentic context for determining if adults can evaluate the extent to which evidence supporting certain claims provides accurate and inaccurate information.

Because social media contain an enormous number of short messages competing for readers’ attention, the persuasiveness of evidence is crucial for authors to craft their arguments in ways that are both efficient and effective. When examining the relative persuasiveness of different evidence types (e.g., anecdotal versus statistical evidence), results have been contradictory. In one narrative literature review, \textcite{HornikxJ.M.A2005Aroe} reviewed twelve studies that compared the persuasiveness of anecdotal evidence and statistical evidence. Six of those studies showed that statistical evidence was more persuasive than anecdotal evidence, one study showed that anecdotal evidence was more persuasive, and five studies showed no difference between the two types of evidence.  Elsewhere, a meta-analysis of 15 studies concerning health communication campaigns \autocite{zebregs2015differential} showed that statistical evidence had a stronger influence on beliefs and attitudes than anecdotal evidence, but that anecdotal evidence had a stronger influence on intention, which relates more closely to affective responses. Notably, a recent review found that presenting anecdotal evidence and personal experiences was one of the twelve identified persuasive techniques used in communicating online health misinformation \autocite{peng2022persuasive}.

When evaluating evidence, readers may struggle to differentiate evidence in terms of quality, especially when judging the quality of different types of research evidence.  For example, \textcite{list2021examining} found that even though undergraduate students (\textit{N} = 82) considered anecdotal evidence considerably less convincing than other types of evidence, they struggled to differentiate between certain type of research evidence that imply causation and observational and correlational evidence (that do not imply causation). This is especially concerning because, on social media, discussions of health issues often include causal claims or generalizations that may or may not be supported by appropriate causal evidence.


\subsection{Second-Hand Evaluation}
The trustworthiness of a source refers to readers' perceptions of positive characteristics on the part of that source, which allow them to accept the source’s message \autocites{fogg2002persuasive, ohanian1990construction}. Evaluation of source trustworthiness is essential, particularly when readers evaluate scientific information, including health information,  about which they do not have specialized knowledge \autocite{hendriks2015measuring}. \textcite{hendriks2015measuring} found three dimensions that laypersons rely on when deciding whether they trust an expert. These dimensions are expertise, integrity, and benevolence. A source’s expertise can be evaluated by exploring that source’s credentials, such as degrees, professional achievements, affiliations, and other indicators of competence, such as awards and publications \autocites{rolin2020trust, braaten2018task}. Furthermore, ‘integrity’ refers to honesty, objectivity, and acting according to scientific and professional standards, whereas ‘benevolence’ refers to goodwill and intentions toward others and society \autocite{hendriks2015measuring}.		

It is widely acknowledged that information about a source's expertise plays a pivotal role in credibility evaluation of social media messages and beyond \autocites{meinert2022expertise, LIN2016264, stadtler2014content}. For example, \textcite{hirvonen2018cognitive} have studied cognitive authority in online health-related information-sharing processes. They found that authority-related cues, such as user information, the authority's own experience, education and background, were important when girls and young women evaluated the information credibility of their peers in an online forum related to health topics. However, evaluation of the source's trustworthiness can also be based on symbols of authority and fruits of success rather than on perceived expertise \autocite{shieber2015testimony}.
For example, one interview study, all 18 interviewed participants (ages 18 to 30) viewed celebrities as trustworthy sources of online information \autocite{djafarova2017exploring}. 

In other research, \textcite{LIN2016264} studied the credibility evaluation of mock Twitter posts that discussed health risks. In the study, 696 undergraduates participated in a quasi-experiment in which the following aspects of Twitter posts were manipulated: author information, originality of the tweets (original or retweeted), and bandwagon cues (i.e, replies or not). The authors were manipulated by authority and whether the identifying information was provided. The Twitter pages were owned by either the Center for Disease Control and Preventions (CDC), a college student, or a stranger without any identifying information. After viewing the pages, students were asked to evaluate the source's credibility according to three criteria: competence, goodwill, and trustworthiness. The researchers determined that participants viewed authority cues as the most credible compared to the other cues (identity, original or retweeted, bandwagon cues). The CDC was evaluated as more credible than the student and the stranger. The bandwagon cues also had an effect, such that the student and the stranger were evaluated as more credible when they retweeted each other. Interestingly, the CDC twitter page without any retweets was perceived as the most credible in all evaluated aspects.

Some studies have examined other features of the source, such as gender and ethnicity. \textcite{spence2013intercultural} investigated intercultural differences in responding to health messages specifically in the context of Facebook. In their study, participants from the Caucasian and African American communities (\textit{N} = 200\textit{)} were evaluated to determine their response efficacy and behavioral intentions after being exposed to a social media page with an avatar. The African American page owner communicated either high or low ethnic identity (that is, including a greater or lesser amount of cues on their page to promote their ethnic identity) while encouraging participants to read a story about heart disease. African American participants perceived both ethnic authors as more competent, caring, and trustworthy than Caucasian participants did. While the perceived credibility of the author was not measured, African American participants indicated stronger intention to change their dietary behavior after seeing a health message delivered by an African American avatar with high ethnic identification as compared to one with low ethnic identification.

\textcite{armstrong2009blogs} examined the role of gender in credibility evaluation of blogs. They found that gender influenced how undergraduates at a large university in southeastern United States perceived credibility. The blog posts with male authors were seen as more credible than posts with female authors. Similar results regarding credibility and gender have also been attained with newscasters \autocite{weibel2008gender}. More recently, \textcite{groggel2019race} investigated how people perceived both gender and ethnicity as well as physical attractiveness as a cue indicating trustworthiness when evaluating Twitter accounts. In the study, Amazon Mechanical Turk workers evaluated the trustworthiness of 816 Twitter profiles.  Results indicated that attractiveness was positively associated with trust. Furthermore, White Twitter accounts were evaluated as more trustworthy than Black accounts. Also, Black male and both Black and White female accounts were viewed as less trustworthy than White male accounts. White workers also evaluated White accounts as trustworthy.

\subsection{Contextual Attributes of Readers}
At least two studies have found that cultural context or a reader's membership in a particular social media network had an effect on adults’ credibility perceptions of social media posts. \textcite{yang2013microblog} compared credibility perceptions of microblog posts among U.S. and Chinese participants in two social media networks; the posts varied in terms of author’s gender, name style, profile image, location, and degree of network overlap with the reader. They identified several key differences in how users from each country critically consumed microblog content in the two networks, including that Chinese respondents considered information from microblogs to be more credible than U.S. respondents. However, the researchers explained “it wasn’t clear whether this difference was due to inherent differences in credibility quality of microblog information in the two cultural contexts, or to the different way that people perceive the credibility of microblog information and/or other information sources.” (p. 581)  

In another study of 746 adults from 76 countries, \textcite{mohd2016correlation} used chi-square analyses to correlate tweet credibility evaluations and demographic attributes (gender, age, education, and location) in original, binary, and categorical setting. Location data were partitioned into original setting (Asia, Europe, South America, North America, and Africa), binary setting (Eastern hemisphere, Western hemisphere) and categorical setting (Asia-Pacific, Americas, Europe, Africa). They found a combination of readers' education background and their geo-location had a significant correlation with credibility judgments but gender and age did not. Further, only location was significantly correlated at all levels of data partitioning. Finally, \textcite{shariff2020review} conducted a literature review of credibility perceptions of online information and concluded that most studies have not paid sufficient attention to how and why readers carry out their credibility judgements of social media posts. Mohd Shariff called for more research in credibility perceptions that attends to demographic attributes and other participant characteristics (e.g., membership in particular groups or social media platforms).

Considering all of these findings, in this study, we focused on credibility judgements of health-related social media posts to determine to what extent adult readers rely on their prior beliefs (i.e., knowledge based validation), evidence types presented in the posts (i.e., discourse-based evaluation) and source characteristics (expertise, gender, and ethnicity) after controlling for the topic of health-related posts.  We also examined whether crowdsourcing platform membership played a role in readers’ credibility evaluations given that the nationalities of members across the two platforms did not overlap.