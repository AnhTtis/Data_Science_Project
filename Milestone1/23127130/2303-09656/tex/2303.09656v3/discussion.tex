\newpage
\section{Discussion}\label{sec:discussion}

This study sought to understand how adult readers evaluate the credibility of short health-related social media posts that vary in source characteristics and argument quality. Our design of the social media posts included some unique features. First, we chose topics on which contradictory claims, including accurate and inaccurate informationspread on the internet, by relying on the thorough investigation of a medical professor who has collected scientific evidence to combat the common misinformation on health issues \autocite{knuuti2020kauppatavarana}. Second, this study was the first to manipulate evidence type (i.e., research evidence, testimony, consensus, and personal experience) in social media posts showing that the quality of evidence had a minor effect on adult readers’ credibility evaluations. Another unique feature of this study was that we recruited participants from two different crowdsourcing platforms to have a culturally broad sample, which had interesting results. First, we discuss the findings in terms of first-and second-hand evaluation (i.e., content and source evaluation), after which we discuss effects of the crowdsourcing platform as a context for evaluating social media posts. Finally, we discuss how these findings provide important contributions to theoretical models and methodologies for studying credibility evaluation and consider limitations of the study.    

In terms of first-hand evaluation strategies, participants' prior beliefs mattered more regarding their credibility judgments than the quality of the evidence presented in the post. The more consistent the participants' prior beliefs were with the claim of the post, the more credible the post was evaluated as being. This association was found in credibility judgments of both accurate and inaccurate posts, and it is in line with previous research \autocites{abendroth2020mere, mccrudden2016differences}. Relying on one's prior beliefs in credibility evaluation is, however, a double-edged sword. When a person has accurate prior beliefs, they can be effectively used to disregard misinformation. In contrast, when a person has false prior beliefs, relying on them in credibility evaluation is unwise, as this strategy may do nothing but strengthen one's false beliefs. Therefore, it is essential that, in our increasingly complex knowledge society, laypeople acknowledge the limits of their prior knowledge and beliefs when consuming online information \autocite{osborne2022science}. This becomes even more important when comprehending health information in social media contexts, given the level of domain knowledge required to understand complicated medical terminology and evidence that misinformation on major public health issues is very common on social media platforms (\cite{suarez2021prevalence}; see also \cite{vosoughi2018spread}).  


Readers' prior beliefs are a quick internal resource that they can easily access and use in their credibility evaluations. In contrast, using discourse-based validation strategies, such as evaluating the strength of the argument, may require more processing effort. However, it is still somewhat surprising that the evidence type had relatively small effect (\textit{d}s 0.02–0.28) on participants' evaluation of accurate social media posts in which all four evidence types (i.e., research, testimony, consensus, and personal experience) were represented. It is understandable that research and testimony had a similar effect on credibility evaluation because the testimonies used in our posts relied on authorities (e.g., European Food Safety) or experts (e.g., a doctor), but it is surprising that the posts relying on consensus or personal experience were valued rather similarly to the posts that included research evidence or expert testimony.  

It is true that findings in previous studies by \textcite{list2021examining}  and \textcite{HornikxJ.M.A2005Aroe} indicate that research-based evidence is highly valued among readers. However, in these studies participants evaluated evidence in more traditional contexts, such as newspaper stories \autocite{list2021examining}, not social media. Our study uniquely examined evidence as part of social media posts, suggesting that research-based evidence may be less valued in the social media context. One reason may be that narrative ways of knowing, such as personal experience, are more persuasive in social media. Instead of seeking research-based evidence for decision-making, people may turn to other consumers’ or influencers’ experiences they can relate to \autocite{feng2021expert}. Such behavior may also reflect in credibility evaluation of the social media posts. Another explanation is that many students leave high school unable to understand, evaluate, or write arguments, and especially scientific arguments \autocites{duschl2002supporting, takao2003assessment, larson2009improving}   Strategies for comprehending and evaluating arguments are not explicitly taught in school and are rarely emphasized in university curricula \autocite{osborne2010arguing} and deficits in aspects of scientific reasoning are pervasive among university students  \autocite{britt2014scientific}. Thus, more research is needed to understand if increased exposure to arguments, and explicit instruction and practice in the skills of argumentation, has an effect on adults’ discourse-based credibility evaluation skills, particularly in social media contexts.  

In terms of second-hand evaluation strategies, we manipulated the source's expertise, gender, and ethnicity. Our results suggest that participants focused primarily on a source's expertise when evaluating the credibility of social media posts. In fact, a source's gender and ethnicity were not associated at all with participants' credibility judgments. Our findings about the role of source expertise in credibility evaluation are in line with previous research (e.g., \textcite{LIN2016264}). Participants evaluated the expert sources (i.e., the professor of medicine and nurse education practitioner) as being more credible than the laypersons (i.e., the lifestyle blogger and parent) almost regardless of the presented evidence (Figure \ref{fig:evidencesource}). This could be explained based on default trust: the default trust occurs when there is no reason to doubt the argument of the source. This kind of trust is typically based on inductive reasoning \autocite{shieber2015testimony}. For example, trust in the professor's argument is based on the experience that professors typically provide accurate information. Trust in epistemic authority can also be based on monitoring for the trappings of competence, such as professional titles. This kind of monitoring is more about the symbols of authority than substance \autocite{shieber2015testimony}. 
 
In our study, a source’s gender and ethnicity did not predict participants' credibility judgments. This is in contradiction to some previous work \autocite{armstrong2009blogs, groggel2019race}. In these previous studies, the focus was more on the source's gender and/or ethnicity as such. For example, when examining the gender effects on perceived credibility of blog authors, the content of the blog was kept constant \autocite{armstrong2009blogs}. In our study, the expertise of the source and content of the post were varied, which might explain why gender and ethnicity of the source did not predict participants’ credibility judgments. Thus, it is possible that this variation overrode the effect of the source’s gender and ethnicity.


It is worth noting that the credibility evaluation of social media posts is cognitively demanding as readers must keep several content and source features in their minds when processing and judging the posts. In this light, our results align with the limited capacity model of mediated message processing \autocite{lang2000limited}. The model suggests that due to limited cognitive capacity, people cannot process all aspects of messages they receive, and thus, readers select and process only some features of messages. Furthermore, building on this assumption, the dual processing model of credibility assessment emphasizes that readers’ motivation and ability determine whether and to what degree readers will evaluate the credibility of online information \autocite{metzger2015psychological}. Our credibility evaluation task did not have high personal relevance for the participants, crowdworkers. Thus, crowdworkers’ motivation to deeply consider a broader range of source and content features may have been low, leading to using cognitively and temporally cost-effective evaluation strategies. For example, crowdworkers may have based their credibility evaluations mainly on their prior beliefs and source expertise, neglecting the evaluation of the quality of evidence. We assume that readers’ behavior would likely be somewhat different if they were engaged in seeking information they need to make important health-related decisions (cf. \cite{sperber2010epistemic}). Thus, the results of this study should be interpreted carefully. 

Regarding contextual attributes of the crowdworkers themselves, because there appeared to be no overlap between the self-reported nationalities of study participants in the Prolific and Toloka platforms, we explored the possibility that cultural background (i.e., groups of individuals with nationalities associated with each platform) played a role in predicting the quality of adults’ credibility evaluation practices. Notably, we found that the crowdsourcing platform in which participants completed the credibility evaluation tasks had a significant effect on their performance. Toloka crowdworkers, representing nationalities including Russia, Eastern Europe, the Middle East, Keyna, and India, tended to evaluate the credibility of the inaccurate posts, in particular, higher than Prolific crowdworkers, most of whom lived in the United Kingdom, United States, Canada, and Western and Southern Europe. One reason behind these results might be that adults living in open Westernized countries, like the participants in Prolific, may be exposed to more opportunities to critically question the quality of information sources while participants in Toloka were living in countries like Russia and Turkey with governments that are more likely to repress public forms of criticism and adopt greater restrictions targeting social media use (cf.\cite{tufekci2014social}).Thus, Toloka participants may have less practice in negotiating alternative viewpoints and distinguishing between accurate and inaccurate information in social media posts. 

\subsection{ Implications for Credibility Evaluation Theory and Research}

Findings from this study have implications for theory and research involving the study of credibility evaluation. First, our results affirm the bi-directional model of first- and second-hand evaluation strategies \autocite{barzilai2020dealing} by showing that both content and source related issues were associated with reader’s credibility judgments of social media posts. In addition, by accounting for multiple factors (i.e., variations in content and evidence type, variations in the source’s expertise, gender and ethnicity, and differences in crowdsourcing platform membership) at the same time, rather than assigning participants to predefined conditions involving fewer factors, this study provides a richer and more complex understanding of the order of importance each factor can play in adults’ overall credibility evaluations of social media posts. For example, while we learned that adults’ rely on their prior beliefs and source expertise more than other source features, such as gender and ethnicity, to evaluate overall credibility, we found that evidence quality doesn’t matter as much as we would hope when adult readers consume information about controversial health issues on social media. Our study also provides more conceptual clarity in how variations in these factors can be operationalized and incorporated into the design of authentic, critical online reading tasks for adults.

These findings also complexify bi-directional theories of credibility evaluation \autocites{barzilai2020dealing, richter2017comprehension, stadtler2014content} by hinting at how readers may attend to contextual attributes of the text as a third tier of credibility evaluation \autocite{forzani2022does}. That is, as readers are expected to make inferences about the social and political contexts of health-related social media posts (e.g., unvetted information often paired with testimonials and endorsements by strangers), it is possible that their reasoning may need to extend beyond the use of explicit content and source features to evaluate the quality of unstated warrants in these contexts. Of course, the design of our study does not allow us to explore this possibility. To understand how participants employ first- or second-hand evaluation strategies or if there is a need for additional tiers of strategy use, methods (e.g., think-aloud, open-ended questions) allowing participants to explain their reasoning behind their credibility judgments are needed. Furthermore, more controlled experiments could clarify the nature of the reciprocal relationship between content and source evaluation, or explore the role of additional reader attributes needed to make inferences about information in complicated globally networked social media contexts.

Finally, by broadening our conceptualization of context beyond text and source attributes to explore the effect of one contextual attribute of the participating readers (i.e., crowdsourcing platform membership), we considered the idea that significant differences in credibility evaluations may potentially reflect the different demographic compositions of the participants in the Prolific and Toloka platforms. Unfortunately, because many different nationalities were represented within each platform, it is impossible to know more about how geographic location or nationality might have affected the credibility evaluations of participating crowdworkers in our study. However, the idea of including nationality, and other demographic attributes of readers (i.e., such as gender, ethnicity, and level of education) in future conceptualizations of credibility frameworks seems worthy of additional study.  

Findings from this study also have methodological implications for research in credibility evaluation. Our full factorial design with five independent variables, each having multiple levels, allowed us to study numerous factors within the same experiment, constrained only by the number of unique variations that can be generated. Our methodology also enabled a comprehensive comparison of the influence that each considered factor had on the dependent variable. By employing a cumulative link mixed model (special form of multiple regression) to assess the impact of individual categories within each factor, the resulting coefficients and calculated effect sizes offer a means to rank these factors based on their relative impact. Our study also underscores the potential of applying software to generate and ensure that all unique combinations of social media posts have been created. Key to this approach was the well-established research in combinatorial software testing \autocite{kuhn2009combinatorial}, which involves the automatic generation of unique test cases from a model of all factors and their levels. Crowdsourcing has emerged as a natural methodological choice in modern online combinatorial experiments that demand scalable and on-demand human subject participation. In our study, this approach enables the rapid testing of hundreds of unique variations. Additionally, leveraging multiple platforms promotes cultural and global diversity in samples, as different online human subject pools have distinct participant bases \autocite{douglas2023data}. Achieving full cultural representativeness in study samples will require further efforts of course, and one must recognize the self-selection bias inherent in these subject pools. Nevertheless, platforms like Prolific, Panels \& Samples (by Qualtrics), and Google Surveys are now widely recognized for capturing high-quality human opinions. They are widely adopted by multinational corporations like Meta, institutions such as The World Bank, and behavioral researchers from prestigious institutions, including Stanford and the University of Oxford.

\subsection{Limitations} 

As with all studies, this study has limitations that must be considered when interpreting the results. First, we only investigated two first-hand evaluation strategies (i.e., discourse-based validation and knowledge-based validation), excluding corroboration from our inspection. As corroboration is a vital strategy to understand the consensus in the scientific community \autocite{osborne2022science}, future studies could seek to understand how and in what conditions readers corroborate information when they encounter health information in social media. Furthermore, we did not manipulate the bandwagon features typical of social media messages, such as likes and shares, even though they have been shown to affect the credibility evaluation of such messages \autocite{LIN2016264}. However, we would like to note that the current design already resulted in a large number of different messages (480 unique combinations for accurate posts and 360 unique combinations for inaccurate posts).

Second, we used well-known organizations (e.g., WHO) and non-named experts (e.g., doctor, personal trainer) in the testimonial evidence. Consequently, the strength of the evidence in the testimony and research categories was partly overlapping. In future studies, the second-hand sources referred to in testimonial evidence should be better aligned.

Third, due to the platforms' functionalities, the posts were delivered to the different crowdsourcing platforms slightly differently. However, we attempted to minimize the potential effects by controlling for the platform used in our analysis. However, using two crowdsourcing platforms resulted in a more diverse sample compared to using only one platform. It is worth mentioning that the crowdsourcing platform had an effect on perceived credibility. The crowdworkers of Toloka tended to evaluate the credibility of the inaccurate posts, in particular, higher than the crowdworkers of Prolific. Future studies should consider potential effects when using different crowdsourcing platforms, especially when generalizing findings.

Fourth, using crowd-sourcing platforms for data collection has some downsides. For example, workers may complete the tasks as quickly as possible to maximize their income which is a potential threat to the validity of all studies performed using crowd-sourcing \autocite{gadiraju2015understanding}. We combated this threat by using filters in both crowd-sourcing platforms, that is, recruiting only the top-performing workers for our study.

Fifth, we used a single item to measure the perceived credibility of the social media posts, limiting the estimation of instrument reliability. However, according to \textcite{fuchs2009using}, single-item scale may be advantageous to avoid participants resenting their being asked to respond to similar items, in particular when items are administered multiple times. 



