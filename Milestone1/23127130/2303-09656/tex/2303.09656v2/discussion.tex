\section{Discussion}\label{sec:discussion}

This study sought to understand how adult readers evaluate the credibility of short health-related social media posts that vary in source characteristics and the argument quality. Our design of the social media posts included some unique features. First, we chose topics on which accurate and inaccurate claims spread on the internet by relying on the thorough investigation of a medical professor who has collected scientific evidence to combat the common misinformation on health issues \citep{knuuti2020kauppatavarana}. Second, this study was the first to manipulate evidence type (i.e., research evidence, testimony, consensus, and personal experience) in social media posts showing that the quality of evidence had a minor effect on adult readers’ credibility evaluations. Another unique feature of this study was that we recruited participants from the two different crowdsourcing platforms to have a culturally broad sample. This solution also revealed that the choice of the increasingly common crowdsourcing platforms in data collection might have some effects on the results. First, we discuss the findings in terms of first-and second-hand evaluation (i.e., content and source evaluation) strategies, after which we consider  limitations of the study. 

In terms of first-hand evaluation strategies, participants' prior beliefs mattered more regarding their credibility judgments than the quality of the evidence presented in the post. The more consistent the participants' prior beliefs were with the claim of the post, the more credible the post was evaluated as being. This association was found in credibility judgments of both accurate and inaccurate posts, and it is  in line with previous research \citep{abendroth2020mere, mccrudden2016differences}. Relying on one's prior beliefs in credibility evaluation is, however, a double-edged sword. When person has accurate prior beliefs, they can be effectively used to disregard misinformation. In contrast, when a person has false prior beliefs, relying on them in credibility evaluation is unwise, as this strategy may do nothing but strengthen one's false beliefs. Therefore, it is essential that, in our increasingly complex knowledge society, laypeople acknowledge the limits of their prior knowledge and beliefs when consuming online information \citep{osborne2022science}.

Readers' prior beliefs are a quick internal resource that they can easily access and use in their credibility evaluations. In contrast, evaluating the evidence in relation to the presented claim may require more processing effort. However, it is still somewhat surprising that the evidence type had hardly an effect (\textit{d}s < 0.20) on participants' evaluation of accurate social media posts in which all four evidence types (i.e., research, testimony, consensus, and personal experience) were represented. It is understandable that research and testimony had similar effect on credibility evaluation because the testimonies used in our posts relied on authorities (e.g., European Food Safety) or experts (e.g., a doctor), but it is astonishing that the posts relying on consensus or personal experience were valued similarly to the posts that included research evidence or expert testimony. 

Our findings contradict the previous findings of \cite{list2021examining}  and \cite{HornikxJ.M.A2005Aroe}. In the former study, participants evaluated evidence in the contexts of brief newspaper stories. Unlike these previous studies, our study uniquely examined evidence as part of social media posts, suggesting that research-based evidence may not be so highly valued in the social media context. One reason for this may be that narrative ways of knowing, such as personal experience or consensus, are more persuasive in social media.

In terms of second-hand evaluation strategies, we manipulated the source's expertise, gender, and ethnicity. Our results suggest that participants emphasise a source's expertise when evaluating the credibility of social media posts. In fact,  source's gender and ethnicity were not associated at all with participants' credibility judgments. 

Our findings about the role of source's expertise in credibility evaluation are in line with previous research (e.g. \citealp{LIN2016264}). Participants evaluated the expert sources (i.e., the professor of medicine and nurse education practitioner) as being more credible than the laypersons (i.e. the lifestyle blogger and parent) almost regardless of the presented evidence (Figure 2). This could be explained based on default trust: the default trust occurs when there is no reason to doubt the argument of the source. This kind of trust is typically based on inductive reasoning \citep{shieber2015testimony}. For example, trust in the professor's argument is based on the experience that professors typically provide accurate information. Trust in epistemic authority can also be based on monitoring for the trappings of competence, such as professional titles. This kind of monitoring is more about the symbols of authority than substance \citep{shieber2015testimony}. 

In our study, the gender and ethnicity of the source did not predict participants' credibility judgments. This is in contradiction to some previous work \citep{armstrong2009blogs, groggel2019race}. In these previous studies, the focus has been more on the source's gender and/or ethnicity as such. For example, when examining the gender effects on perceived credibility of blog authors, the content of the blog was kept constant \citep{armstrong2009blogs}. In our study, the expertise of the source and content of the post were varied, which might explain why gender and ethnicity of the source did not predict participants’ credibility judgments. Thus, it is possible that this variation overrode the effect of the source’s gender and ethnicity.

It is worth noting that the credibility evaluation of social media posts is cognitively demanding as readers must keep several content and source features in their minds when processing and judging the posts. In this light, our results align with the limited capacity model of mediated message processing \citep{lang2000limited}. The model suggests that due to limited cognitive capacity, people cannot process all aspects of messages they receive, and thus, readers select and process only some features of messages. Furthermore, building on this assumption, the dual processing model of credibility assessment emphasizes that readers’ motivation and ability determine whether and to what degree readers will evaluate the credibility of online information \citep{metzger2015psychological}. Our credibility evaluation task did not have high personal relevance for the participants, crowdworkers. Thus, crowdworkers’ motivation to deeply consider a broader range of source and content features may have been low, leading to using cognitively and temporally cost-effective evaluation strategies. For example, crowdworkers may have based their credibility evaluations mainly on their prior beliefs and source expertise, neglecting the evaluation of the quality of evidence. We assume that readers’ behavior would likely be somewhat different if they were engaged in seeking information they need to make important health-related decisions (cf. \cite{sperber2010epistemic}). Thus, the results of this study need to be interpreted carefully. 

To conclude, our results supported the bi-directional model of the first and second-hand evaluation strategies \citep{barzilai2020dealing} by showing that both first- and second hand evaluation strategies (i.e., content evaluation and source evaluation) contributed to adult readers’ perceived credibility of social media posts. However, more controlled experiments are needed to examine the nature of the reciprocal relationship between content and source evaluation strategies.

\subsection{Limitations} 

As with all studies, this study has limitations that must be considered when interpreting the results. First, our study focused only on the effects of the content of the message (evidence type) and specific source features (expertise, gender, and ethnicity). We did not manipulate the bandwagon features typical of social media messages, such as likes and shares, even though they have been shown to affect the credibility evaluation of such messages \citep{LIN2016264}. Furthermore, we did not include any explicit hints about the integrity or benevolence of the source. However, we would like to note that the current design  resulted in a large number of different messages (480 unique combinations for accurate posts and 360 unique combinations for inaccurate posts).

Second, due to the platforms' functionalities, the messages were delivered to the different crowdsourcing platforms slightly differently. However, we attempted to minimize the potential effects by controlling for the platform used in our analysis. There were also some benefits of using two crowdsourcing platforms. Namely, our sample was more diverse as compared to one derived using only one specific platform. It is worth mentioning that the crowdsourcing platform had an effect on perceived credibility. The crowdworkers of Toloka tended to evaluate the credibility of the inaccurate posts, in particular, higher than the crowdworkers of Profilic. The future studies should consider potential effects when using different crowdsourcing platforms, especially when generalizing findings.

Third, using crowd-sourcing platforms for data collection has some downsides. For example, workers may complete the tasks as quickly as possible to maximise their income which is a potential threat to the validity of all studies performed using crowd-sourcing \citep{gadiraju2015understanding}. We combated this threat by using filters in both crowd-sourcing platforms, that is, recruiting only the top-performing workers for our tasks.

Fourth, we used a single item to measure the perceived credibility of the social media posts, limiting the estimation of instrument reliability. However, according to \cite{fuchs2009using}, single-item scale may be advantageous to avoid participants being resent of responding repetitively to similar items, in particular when items are administered multiple times. 


%Fourth, we used a single item to measure the perceived credibility of the constructed social media posts, limiting the estimation of instrument reliability. However, our single item is relatively clear and unambiguous, as advocated by \cite{fuchs2009using, bergkvist2007predictive}. The item was also presented simultaneously while viewing the post, removing any memory biases.



