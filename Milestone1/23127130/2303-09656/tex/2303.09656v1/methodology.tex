\section{Present Study}

The present study was intended to understand how adults evaluate short social media posts on health issues. Informed by the bidirectional model of first- and second-hand evaluation strategies created by \cite{barzilai2020dealing}, we were interested in how evaluation strategies would be reflected in readers’ credibility judgments regarding short social media posts. We created accurate, meaning in line with current scientific knowledge, and inaccurate, meaning not in line with current scientific knowledge, social media messages on five health topics that included a simple argument in the form of a claim and evidence. We manipulated evidence type and source characteristics, specifically the author’s expertise, gender, and ethnicity. The participants, who were recruited from two crowdsourcing platforms, were asked to evaluate the credibility of the posts. Before the evaluators were given a set of credibility evaluation tasks, participants responded to item regarding prior beliefs about the five health topics. 

Using this design, we examined how evidence type, participants’ prior beliefs, and the characteristics of the source were associated with participants' credibility judgments regarding the posts after the topic and crowd-sourcing platform were controlled for. Based on theoretical assumptions \citep{barzilai2020dealing, richter2017comprehension, stadtler2014content} supported by previous empirical evidence, we assumed that the quality of evidence \citep{HornikxJ.M.A2005Aroe,list2021examining}, participants’ prior beliefs \citep{mccrudden2016differences, van2016attitude}, and the expertise of the source \citep{braaten2018task, LIN2016264}  would be associated with the participants' credibility judgments. Finally, we were interested in testing  the effect of the source’s gender and ethnicity on credibility evaluation.  

We controlled for the topic of the posts because it has been shown to play a role in the credibility evaluation of texts  \citep{braaten2018task, hamalainen2021students}. Because the workers on the two chosen crowd-sourcing platforms have somewhat different social-cultural backgrounds, platform was controlled for. All the variables included in this study are presented in Table \ref{tab:var} 

\begin{table*}[ht]
\caption{Independent and control variables in a set of the evaluation tasks for accurate and inaccurate posts.}
\label{tab:var}
\begin{tabular}{|p{8em}|p{15em}|p{17em}|}
\hline
Variable & Description & Manipulation \\
\hline
Independent Variables: & & \\
  \hline
 Evidence Type & Evidence type supporting the main & A) Research (only for accurate claims)\\
 & claim of the social media post. & B) Testimony  \\
 & & C) Personal experience  \\
 & & D) Consensus  \\
 \hline
 Prior Belief Consistency & Consistency of self-rated prior beliefs about the topic in relation to the post with the accurate or inaccurate knowledge claim. &  \\
 \hline
Source & The author of the social media post. & A) Professor of medicine
 \\
 &  & B) Nurse education practitioner
  \\
 &  & C) Professional lifestyle blogger \\
 &  & D) Parent \\
 \hline
 %Claim Against/Favor & Whether the claim is arguing for or against the topic & 1) \textbf{Against} \\
%  & & 2)\textbf{Favor}  \\
%  \hline
Source's Gender & Self-identified gender of the author  & A) Female \\
& presented in the profile picture.& B) Male \\
\hline
Source's Ethnicity & Self-identified ethnicity of the author  & A) Asian \\
& presented in the profile picture. & B) Black \\
& & C) White \\
 \hline
  Control Variables: & & \\
   \hline
   Topic & The topic of the health-related social & A) Food safety \\
 & media post. & B) Fish oil  \\
 & & C) Processed red meat  \\
  & & D) Vaccine safety  \\
  & & E) Vitamin D  \\
  \hline
  Platform & Crowdsourcing platform on which & A) Prolific \\
   & participants completed the set of credibility evaluation tasks. & B) Toloka  \\
   \hline
\end{tabular}
\end{table*}

\section{Methodology}\label{sec:methodology}

\subsection{Participants}
We used crowdsourcing to recruit participants for our study. The popularity of crowdsourcing has increased steadily, and it is used in a variety of fields \citep{hossain2015crowdsourcing}. A total of 844 participants were recruited from two crowdsourcing platforms: Profilic and Toloka. We used two platforms to obtain a geographically and culturally broad sample. Participants were required to complete a demographic survey at the beginning of the study. We used Pew Research's Demographics Questionnaire \cite{pewresearch2015demographic}, as the basis for our survey. The demographic information regarding the participants is presented in Table \ref{TableParticipants}. %This survey included questions about respondents' age, gender, nationality, ethnicity, marital status, education level, employment status, and annual income level. As there is no agreed upon international standard for ethnicity classification \cite{connelly2016ethnicity}, we used the suggested classification from the National Content Test 2015, explained by \cite{mathews2015national} and  \cite{jones2017overview}.)

\begin{table*}[ht]
\caption{Participants self-reported demographic information by crowdsourcing platform.}.  
\label{TableParticipants}
\begin{tabular}{l|lll}
\hline
     & Toloka & Prolific & Total \\ \hline
Respondents & 425  & 419 & 844\\
Mean age & 33  & 43 & 38 \\
Minimum age &  18 & 20 & \\
Maximum age &  85 & 84  \\
\hline
\textbf{Gender:} &   &  \\
Male &  240(56\%) & 204(49\%) &  444(53\%)\\
Female &  180(42\%) & 215(51\%) &  395(47\%)\\
Other &  5(1\%) & 0 &  5(<1\%) \\
\hline
\textbf{Ethnicity:} &   &  \\
Asian & 47 (11\%) & 23(5\%)  &  70(8\%)\\
Black & 26 (6\%) &  10(2\%) &  36(4\%)\\
White & 309 (73\%) &  357(85\%) &  666(79\%)\\
Other or multiple & 43 (10\%) &  29(7\%) &  72(9\%)\\
\hline
\textbf{Nationalities}: & Russia – 237(56\%) & UK – 272(65\%) & UK – 273(33\%) \\
& Ukraine – 29(7\%) & United States – 49(12\%) & Russia – 237(28\%) \\
& Turkey – 20(5\%) &  Italy – 15(4\%) & United States – 56(7\%) \\
\textbf{Rest}:  & 139(33\%) & 83(20\%) & 278(33\%)\\
\hline
\textbf{Education:} &   &  \\
Doctorate & 5 (1\%)  & 11(3\%) & 16(1\%)\\
Master degree & 73 (17\%)  & 68(16\%) & 141(17\%)\\
Bachelor degree &  148(35\%) & 173(41\%) & 321(38\%) \\
High school diploma &  107(25\%) & 123(29\%) & 230(27\%)\\
Professional degree & 59(14\%)  & 17(4\%) & 76(9\%)\\
Other qualifications or none & 33(8\%) & 27(6\%) & 60(7\%)\\
\hline
\end{tabular}
\end{table*}

\subsection{Social Media Credibility Evaluation Task}\label{sec:evaltask}

\begin{figure*}[ht]
\centering
\includegraphics[width=.60\textwidth]{prolifictask.png}
\caption{Credibility evaluation task on the Prolific platform.}
\label{fig:profilictask}
\end{figure*}

%To answer our research questions, we deployed an online credibility evaluation task. In the task, participants were asked to evaluate social media messages, contents of which were designed by the researchers. 

We designed short social media posts regarding five health topics on which both accurate and inaccurate information spread on the internet. The topics were the safety of vaccines, fish oil, Vitamin D, processed red meat, and food safety. We chose the topics from a current Finnish book written by \cite{knuuti2020kauppatavarana}, a professor and director of the Turku PET Centre, University of Turku. For this book, Knuuti collected 250 inaccurate health-related knowledge claims that are common on the internet. He describes the reasoning regarding inaccurate knowledge claims on the internet and then provides current scientific knowledge on the issue. We also used these descriptions in designing the messages.

The messages comprised three main components which were manipulated: the knowledge claim, supporting evidence, and author of the message (see Figure \ref{fig:profilictask}). Knowledge claims were either accurate, that is, in correspondence with the current scientific knowledge (e.g., Vitamin D does not prevent cancer), or inaccurate (e.g., Vitamin D prevents cancer). The knowledge claims were paired with supporting evidence that represented one of the following evidence types: research evidence, testimony, personal experience, and social consensus \citep{jacobsen2018thinking, zarefsky2019practice}. Because we used authentic findings as research evidence, we only presented these findings with accurate claims.  Examples of the knowledge claims and evidence types related to processed red meat are presented in Table \ref{tab:claimmeat}. 

\begin{table*}[ht]
\caption{Arguments of the evaluated social media posts concerning processed red meat.}
\label{tab:claimmeat}
\begin{tabular}{|p{25em}|p{10em}|}
  \hline
Argument with the accurate claim  & Evidence type  \\
  \hline
Red processed meat is a major health hazard. The study review found that red processed meat was associated with a higher incidence and mortality of arterial disease.& Research \\
\hdashline
Red processed meat is a major health hazard. The WHO places processed meat into the highest risk category of carcinogens..& Testimony \\
\hdashline
Red processed meat is a major health hazard. Humans with a balanced diet without red processed meat feel better.& Consensus  \\
\hdashline
Red processed meat is a major health hazard. I eat red meat regularly and at my physical, some of my important indicators are alarming. & Personal Experience \\
   \hline
Argument with the inaccurate claim  &   \\
  \hline
Red processed meat is not a significant health hazard. My personal trainer appreciates red processed meat as a part of a balanced diet. & Testimony \\
\hdashline
Red processed meat is not a significant health hazard. Humans are carnivores and this is how we have survived. & Consensus  \\
\hdashline
Red processed meat is not a significant health hazard. I eat red meat regularly and at my physical, all of my important indicators are great. & Personal Experience \\
\hline
\end{tabular}
\end{table*}

We created four types of authors, or sources and varied their expertise. Two of them, a professor of medicine and a nurse education practitioner for undergraduates, were domain experts. The other two, a professional lifestyle blogger and a parent, did not have domain expertise. In addition to this information, the author profile included a short description, such as ‘passion for running’, to make the social media messages seem more authentic. 

Because prior work has suggested that an author's ethnic background and gender are associated with reader's responses to social media health information \citep{armstrong2009blogs, spence2013intercultural}, we also varied  the author gender and ethnicity. Thus, we used the Chicago face database \citep{ma2015chicago}, which provides facial pictures along with the self-identified ethnicity and gender of the model, as well as a subjective attractiveness score based on ratings from US-based raters. Using this data, we selected pictures of people with average attractiveness who were between 35 and 45 years of age. We included males and females, as well as those who self-identified as Asian, Black and White. The age and attractiveness selection were performed in order to create a uniform set of facial pictures and thus rule out very unlikely combinations, such as 20 year old professors. The selected faces are shown in the appendix in Figure \ref{fig:faces}, in the Appendix.

These manipulations resulted in 480 unique posts with accurate knowledge claim and 360 unique post with inaccurate knowledge claim. Participants were assigned to evaluate the credibility of ten posts on a scale from 1 to 7 (1 = not at all credible; 7 = extremely credible).


\subsection{Prior belief measure}
Before gaining access to a set of credibility evaluation tasks, participants’ prior beliefs on the topic were measured using one statement for each topic at hand and a 7-point scale, for example, `Vaccines are safe', and `Red meat is healthy'.
%The items for prior beliefs were the following: ``Vaccines are safe'', ``Today, food is healthy, safe and clean'', ``Fish Oil is healthy'', ``Vitamins are good.'' and ``Red Meat is healthy''.

We coded the responses to prior belief items in relation to how consistent they were with the claim. For accurate claims, a prior belief score of 7 corresponds to a prior belief consistency score of 3, and a prior belief score of 1 corresponds to consistency of -3. For inaccurate claims, this was done in reverse, with a prior belief score of 1 corresponding to consistency score of 3 and a prior belief score of 7 corresponding to consistency score of -3. The variable was labeled as prior belief consistency, with a scale from -3 to 3.


\subsection{Data collection procedure}
Crowdworkers on Toloka and Prolific were offered an opportunity to take part in our study. These crowdworkers were first asked to fill out a questionnaire that contained questions about demographic information and the prior belief measure. After completing the questionnaire successfully, the workers proceeded to the credibility evaluation task, in which they were asked to evaluate ten social media posts. On the two platforms, somewhat different methodologies were used to deliver the set of credibility evaluation tasks because of the ways how these platforms work. 

For the Prolific platform, we gave the tasks to the participants in predetermined batches of ten, with minimised repetition of variables. The tasks were accepted for payment only if the participant made ten evaluations. 

On the Toloka platform, each participant had the opportunity to make the maximum of ten evaluations, and they were assigned at random. Only the top 20\% of crowdworkers, in terms of performance, on the Toloka platform were offered chance to take part in the study. All evaluations made on the Toloka platform were accepted. Of the Toloka participants, $ 99 \% $ completed the maximum of ten evaluations. The task used on the Prolific platform is shown in Figure \ref{fig:profilictask}. 

\subsection{Statistical analysis}\label{sec:statanalysis}

In the statistical analysis, the unit of analysis was the credibility evaluation of a social media post. The analyses were conducted separately for posts including accurate and inaccurate knowledge claim, which are later referred as accurate and inaccurate posts below. The data consists of 4787 credibility evaluation of accurate posts and 3593 evaluations of inaccurate posts. Twenty credibility evaluations were excluded from the analysis due to technical problems. 

All statistical analyses were conducted using R. To examine how independent variables (see Table \ref{tab:var}) were associated with the credibility evaluations of social media posts, we used a cumulative link mixed model. The cumulative link mixed model (CLMM), also known as ordered regression model \citep{christensen2015ordinal}, has been advocated for use with ordered outcome measures \citep{fullerton2021ordered}. 

To provide a CLMM model and odds ratios, we used the ordinal package created by \cite{christensen2015ordinal} for the R programming language. The specific formula used is follows: "Credibility\textasciitilde  Accuracy + ClaimEvidence + Topic + Author + Platform + Gender + Ethnicity + (1|ID)". The control variable ID corresponds to each individual evaluator who performed the maximum of ten evaluations in the sample. For the reference category, that is, the category other categories are compared to in the model (see reference categories in Tables \ref{TableAcc} and \ref{TableInacc}), we chose the category that one would assume to produce the highest credibility rating. Otherwise we used the category that was first in alphabetic order. Thus we chose research for evidence type, highest consistency for prior beliefs and professor for source expertise. For gender and ethnicity and the control variables, we used the alphabetic order.

We also present odds ratios (ORs), which are calculated with the ordinal package \citep{christensen2015ordinal}, to highlight the association between the independent variable and the dependent variable. Odds ratios have been advocated for use as an effect size measure with cumulative link mixed models and ordered regression models \citep{fleiss1994measures, fullerton2016ordered}. Moreover, odds ratios have been used with CLMM models in a variety of fields \cite{oehm2022identifying, rezapour2021modeling}. An OR greater than 1 indicates how many times greater the expected credibility evaluation value of the category of interest is than the reference category. In contrast, an OR smaller than 1 indicates that the expected evaluation value in the reference category is greater than that in the category of interest. The ORs were transformed to another popular effect size measure, Cohen's d, with the effectsize R package created by \cite{ben2020effectsize}. The Cohen's d effect sizes were interpreted as follows: small (\textit{d} = 0.2), medium (\textit{d} = 0.5), and large (\textit{d} = 0.8)\citep{cohen2013statistical}.

To calculate the goodness-of-fit statistics, we used the anova.clmm function of the RVAideMemoire package \citep{herve2020package} to perform a likelihood-ratio test \citep{crainiceanu2004likelihood}. %and various pseudo rsquared indices \citep{smith2012examination}. 
