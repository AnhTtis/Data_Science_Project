\section{Discussion}\label{sec:discussion}

This study sought to understand how adult readers evaluate the credibility of short media posts that vary in terms of source characteristics and the quality of argument quality. We were also interested in how adults prior beliefs of the topic of the post affect their evaluation. First, we will discuss the results in terms of first-and second-hand strategies after which we will discuss the limitations of the study. 

In terms of first-hand evaluation strategies, participants' prior beliefs mattered more regarding their credibility judgments than the quality of the evidence presented in the post. The more consistent the participants' prior beliefs were with the claim of the post, the more credible the post was evaluated as being. This association was found in credibility judgments of both accurate and inaccurate posts, and it is  in line with previous research \citep{abendroth2020mere, mccrudden2016differences}. Relying on one's prior beliefs in credibility evaluation is, however, a double-edged sword. When person has accurate prior beliefs, they can be effectively used to disregard misinformation. In contrast, when a person has false prior beliefs, relying on them in credibility evaluation is unwise, as this strategy may do nothing but strengthen one's false beliefs. Therefore, it is essential that, in our increasingly complex knowledge society, laypeople  acknowledge the limits of their prior knowledge and beliefs when consuming online information \citep{osborne2022science}.

Readers' prior beliefs are a quick internal resource that they can easily access and use in their credibility evaluations. In contrast, evaluating the evidence in relation to the presented claim may require more processing effort %(xxx).
However, it is still somewhat surprising  that  the evidence type did not have any effect (\textit{d} < 0.20) on participants' evaluation of accurate social media posts in which all four evidence types (i.e., research, testimony, consensus, and personal experience) were represented. It is understandable that research and testimony had similar effect on credibility evaluation because the testimonies used in our posts relied on authorities (e.g., European Food Safety) or experts (e.g., a doctor), but it is astonishing that the posts relying on consensus or personal experience were valued similarly to the posts that included research evidence or expert testimony. 

Our findings contradict the previous findings of \cite{list2021examining}  and \cite{HornikxJ.M.A2005Aroe}. In the former study, participants evaluated evidence in the contexts of brief newspaper stories. Unlike these previous studies, our study uniquely examined evidence as part of social media posts, suggesting that research-based evidence may not be so highly valued in the social media context. One reason for this may be narrative ways of knowing, such as personal experience or consensus, may be more persuasive in the context of social media. 

It is worth noting that the credibility evaluation task did not have high personal relevance for the participants. Readers' behavior would likely be somewhat different if they were seeking information they need to make important health-related decisions (cf. \citealp{sperber2010epistemic}).

In terms of second-hand evaluation strategies, we manipulated the source's expertise, gender, and ethnicity. Our results suggest that participants emphasise a source's expertise when evaluating the credibility of social media posts. In fact, a source's gender and ethnicity were not associated at all with participants' credibility judgments. 

Our findings about the role of source's expertise in credibility evaluation are in line with previous research (e.g. \citealp{LIN2016264}). Participants evaluated the expert sources (i.e., the professor of medicine and nurse education practitioner) as being more credible than the laypersons (i.e. the lifestyle blogger and parent) almost regardless of the presented evidence (Figure 2). This could be explained based on default trust: the default trust occurs when there is no reason to doubt the argument of the source. This kind of trust is typically based on inductive reasoning \citep{shieber2015testimony}. For example, trust in the professor's argument is based on the experience that professors typically provide accurate information. Trust in epistemic authority can also be based on monitoring for the trappings of competence, such as professional titles. This kind of monitoring is more about the symbols of authority than substance \citep{shieber2015testimony}. Monitoring symbols is a peripheral route to acquiring information of the source. \cite{cacioppo1986central} claim that when argument scrutiny is reduced shallow monitoring and peripheral cues are emphasised. 

Readers should typically evaluate information validity as well; otherwise, they could fall into blind belief. Even though the source may appear to be reliable based on that source's competency and expertise, it is not necessarily honest \citep{sinatra2020evaluating}. A trustworthy source must be both competent and benevolent. The author must to possess accurate information, and intend to share it.

In our study, the gender and ethnicity of the source did not predict participants' credibility judgments. This is in contradiction to some previous work \citep{armstrong2009blogs, groggel2019race}. In these previous studies, the focus has been more on the source's gender and/or ethnicity as such. For example, when examining the gender effects on perceived credibility of blog authors, the content of the blog was kept constant \citep{armstrong2009blogs}. Our results suggest that when the expertise of the source and content of the post are varied, this seem to override the effect of the source's gender and ethnicity.  


\subsection{Limitations} 

As with all studies, this study has limitations that must be considered when interpreting the results. First, our study focused only on the effects of the content of the message (evidence type) and specific source features (expertise, gender, and ethnicity). We did not manipulate the bandwagon features typical of social media messages, such as likes and shares, even though they have been shown to affect the credibility evaluation of such messages (\textbf{\cite{LIN2016264}}). Furthermore, we did not include any explicit hints about the integrity or benevolence of the source. However, we would like to note that the current design  resulted in a large number of different messages (480 unique combinations for accurate posts and 360 unique combinations for inaccurate posts).

Second, due to the platforms' functionalities, the messages were delivered to the different crowdsourcing platforms slightly differently. However, we attempted to minimize the potential effects by controlling for the platform used in our analysis. There were also some benefits of using two crowdsourcing platforms. Namely, our sample was more diverse as compared to one derived using only one specific platform.

Third, using crowd-sourcing platforms for data collection has some downsides. For example, workers may complete the tasks as quickly as possible to maximise their income which is a potential threat to the validity of all studies performed using crowd-sourcing \citep{gadiraju2015understanding}. We combated this threat by using filters in both crowd-sourcing platforms, that is, recruiting only the top-performing workers for our tasks.



