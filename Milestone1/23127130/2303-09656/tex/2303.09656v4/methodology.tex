\section{Methodology}\label{sec:methodology}

\begin{table*}[!htb]
\caption{Independent and control variables in a set of the evaluation tasks for accurate and inaccurate posts.}
\label{tab:var}
\begin{tabular}{|p{8em}|p{15em}|p{17em}|}
\hline
Variable & Description & Manipulation \\
\hline
Independent Variables: & & \\
  \hline
 Evidence Type\textsuperscript{1} & Evidence type supporting the main & A) Research (only for accurate claims)\\
 & claim of the social media post. & B) Testimony  \\
 & & C) Personal experience  \\
 & & D) Consensus  \\
 \hline
 Prior Belief Consistency\textsuperscript{2} & Consistency of self-rated prior beliefs about the topic in relation to the post with the accurate or inaccurate knowledge claim. &  \\
 \hline
Source\textsuperscript{3} & The author of the social media post. & A) Professor of medicine
 \\
 &  & B) Nurse education practitioner
  \\
 &  & C) Professional lifestyle blogger \\
 &  & D) Parent \\
 \hline
 %Claim Against/Favor & Whether the claim is arguing for or against the topic & 1) \textbf{Against} \\
%  & & 2)\textbf{Favor}  \\
%  \hline
Source's Gender\textsuperscript{3} & Self-identified gender of the author  & A) Female \\
& presented in the profile picture.& B) Male \\
\hline
Source's Ethnicity\textsuperscript{3} & Self-identified ethnicity of the author  & A) Asian \\
& presented in the profile picture. & B) Black \\
& & C) White \\
 \hline
 Platform\textsuperscript{4} & Crowdsourcing platform on which & A) Prolific \\
   & participants completed the set of credibility evaluation tasks. & B) Toloka  \\
   \hline
  Control Variable: & & \\
   \hline
   Topic & The topic of the health-related social & A) Fish oil \\
 & media post. & B) Food healthiness  \\
 & & C) Processed red meat  \\
  & & D) Vaccine safety  \\
  & & E) Vitamin D  \\
  \hline
\multicolumn{3}{l}{\multirow{4}{*}{}} \\
\multicolumn{3}{l}{\textsuperscript{1}First-hand evaluation: Discourse-based validation, \textsuperscript{2}First-hand evaluation: Knowledge-based validation,}                 \\
\multicolumn{3}{l}{\textsuperscript{3}Second-hand evaluation,    \textsuperscript{4}Contextual attributes.}              \\   
\end{tabular}
\end{table*}

The present study was intended to understand how adults evaluate short social media posts on health issues. We created accurate (i.e., in line with current scientific knowledge) and inaccurate (i.e., not in line with current scientific knowledge) social media posts on five health topics. Then, we used a computerized algorithm that systematically manipulated evidence type in the posts and source characteristics (the author’s expertise, gender, and ethnicity) to generate 840 unique combinations of the posts. Each unique post was copied 10 times to generate a pool of 8400 posts. The participants, recruited from two crowdsourcing platforms, were asked to evaluate the credibility of ten different posts selected from the larger sample and delivered to each participant via the platform. We examined how evidence type, participants’ prior beliefs, source characteristics of the post and crowdsourcing platform membership were associated with participants' credibility judgments on the posts after controlling for the topic of the posts.

Based on theoretical assumptions \autocites{barzilai2020dealing, richter2017comprehension, stadtler2014content} supported by previous empirical evidence, we assumed that the quality of evidence, participants’ prior beliefs, and expertise of the source would be associated with participants' credibility judgments.  First, we expected that the more consistent the participants’ prior beliefs were with the content in the post, the higher the participants would evaluate the credibility of the post \autocites{mccrudden2016differences, wertgen2023source}.  Second, we expected that posts with claims supported by research-based evidence would be evaluated as more credible than posts that contained evidence from testimony, personal experience, or consensus  \autocites{HornikxJ.M.A2005Aroe,list2021examining}. Third, we expected that posts by sources with relevant domain expertise would be evaluated more credible than posts by sources without domain expertise \autocites{braaten2018task, LIN2016264}. In addition, we were interested in testing the effect of the source’s gender and ethnicity on credibility judgements as well as exploring whether membership in a certain crowdsourcing platform would be associated with participants' credibility judgments. In our analyses, we controlled for the topic of the posts because it has been shown to play a role in the credibility evaluation of texts  \autocites{braaten2018task, hamalainen2021students}. 

To study these associations, we ran an online quasi-experiment with an incomplete repeated measures design \autocite{shaughnessy2012repeated}. That is, there was no control group, and a single rater rated a maximum of 10 posts out of the 840 unique generated posts. The factors were controlled independently, with practice effects controlled with a completely random order in the Toloka platform, and with balancing in the Prolific platform. The final dataset (8380 posts) consisted of 10 ratings of the 820 unique posts from different raters, and 9 ratings for 20 of the unique posts. All the variables included in this study are presented in Table \ref{tab:var}. 

\subsection{Participants}
We used crowdsourcing to recruit participants for our study. The popularity of crowdsourcing has increased steadily, and it has been used in a variety of fields \autocite{hossain2015crowdsourcing}. A total of 844 participants were recruited from two crowdsourcing platforms: Prolific and Toloka. According to \textcite{chapkovski2023conducting}, Toloka provides access to some populations that can be hard to access through Prolific or mTurk. While the MTurk population is mostly based in the US and India \autocite{difallah2018demographics}, and Prolific users reside mostly in the UK and US \autocite{peer2017beyond}, Toloka’s users mostly reside in Russia and other former Soviet countries. Thus, we used the Prolific and Toloka platforms to obtain a geographically and culturally broad sample. 

The Prolific platform is available for participants who are at least 18 years old from most OECD countries, with the exception of Turkey, Lithuania, Colombia and Costa Rica where Prolific is not available (\autocite{profilic2023participants}). Prolific is also available to participants in South Africa and participants who are not from OECD supported countries can still participate if they live in a supported country. In September 2023, according to Prolific's audience checker tool \autocite{profilic2023audience}, approximately 58\% of its users identified as female, while 42\% identified as male. Regarding age, 47\% reported being under 30 years old, 48\% were between the ages of 30 and 59, and 5\% were 60 or older. Prolific crowdworkers are primarily recruited by word-of-mouth through social media (e.g., Facebook, Twitter, Reddit) and flyers distributed on university campuses. All participants are paid rewards when their submissions are approved. 

According to its website, the Toloka platform collects data from crowdworkers who live in more than 100 different countries, with a “large concentration of participants coming from Pakistan, Kenya, Brazil, Turkey, India, Egypt, the Philippines, and the United States,” and “about half of Tolokers are native speakers of English, Urdu, Arabic, Russian, or Spanish” \autocite{toloka2023global}. The majority of Toloka workers are between 20–30 years of age; they are equally divided between members of the middle class and working class, and 62\% of its members identify as male. Toloka acquires its users at least through social media presence and advertising. %ANY INFORMATION ABOUT HOW RECRUITED?

Participants were required to complete a demographic survey at the beginning of the study. We used Pew Research's Demographics Questionnaire \autocite{pewresearch2015demographic}, as the basis for our survey.As shown in Table \ref{TableParticipants}, the mean age of respondents was higher in the Prolific platform (\textit{M }= 43) compared to Toloka (\textit{M} = 33) and there were more male respondents in Toloka (56\%) than in Prolific (49\%). In terms of ethnicity, a large percentage of respondents in both platforms identified as White (73\% in Toloka and 85\% in Prolific), fewer identified as Asian (11\% in Toloka and 5\% in Prolific) or representing multiple ethnicities (10\% in Toloka and 7\% in Prolific) while the smallest number of respondents in either platform identified as Black (6\% in Toloka and 2\% in Prolific). Education levels were fairly similar across the two platforms, but notably, there was no apparent overlap in the nationalities of respondents across the two platforms. In Toloka, more than half (56\%) of respondents identified as Russian, while the remaining respondents lived in regions in Eastern Europe, the Middle East, Kenya, or India. In contrast, more than three-quarters (77\%) of respondents in Prolific were from either the United Kingdom (65\%) or the United States (12\%), and the remaining respondents were from either Canada or other countries in Western and Southern Europe. 


\subsection{Generation of  Credibility Evaluation Items for Social Media Posts}\label{sec:evaltask}
\subsubsection{Designing Posts}
We designed short social media posts regarding five health topics on which contradictory information, including accurate and inaccurate claims, had been spread on the internet. In designing the social media posts, we consulted a current Finnish book written by \textcite{knuuti2020kauppatavarana}, a professor and director of the Turku PET Centre, University of Turku. For this book, Knuuti collected 250 common health-related knowledge claims spreading on the internet and analyzed them in light of current scientific knowledge. The chosen topics were the safety of vaccines, Vitamin D, processed red meat, fish oil, and food healthiness. There are several false claims about vaccines on the internet, one of which is that the aluminum in vaccines is dangerous. Also, excessive claims about the benefits of Vitamin D, such as preventing cancer, have been presented. Furthermore, research on the health effects of red processed meat has been misinterpreted by neglecting the potential harms. Finally, there are contradictory claims about the benefits of fish oil and food healthiness. We used research presented in the book to design the research-based social media posts. The sources provided in the book are presented in Table \ref{tab:knuutireferences} of Appendix. 

Each post comprised three main components which were systematically manipulated with a computer algorithm: the knowledge claim, supporting evidence, and author of the post (see Figure \ref{fig:profilictask}). Knowledge claims were either accurate, that is, in correspondence with the current scientific knowledge (e.g., Vitamin D does not prevent cancer), or inaccurate (e.g., Vitamin D prevents cancer). The knowledge claims were paired with supporting evidence that represented one of the following evidence types: research evidence, testimony, personal experience, and social consensus \autocites{jacobsen2018thinking, zarefsky2019practice}. Because we used authentic findings as research evidence, we only presented these findings with accurate claims. Examples of the knowledge claims and evidence types related to processed red meat are presented in Table \ref{tab:claimmeat}. 

\begin{table*}[!htb]
\caption{Participants self-reported demographic information by crowdsourcing platform.}.  
\label{TableParticipants}
\begin{tabular}{l|lll}
\hline
     & Toloka & Prolific & Total \\ \hline
Respondents & 425  & 419 & 844\\
Mean age & 33  & 43 & 38 \\
Minimum age &  18 & 20 & \\
Maximum age &  85 & 84  &\\
25 or under & 107 (25\%) & 25 (6\%)& 132 (16\%)\\
26-35 & 139 (33\%) & 127 (30\%) & 266 (32\%)\\
36-45 & 108 (25\%) & 100 (24\%)& 208 (25\%)\\
46-55 & 36 (8\%)& 83 (20\%)& 119 (14\%)\\
over 55 & 35 (8\%) & 84 (20\%)& 119 (14\%)\\
\hline
\textbf{Gender:} &   &  \\
Male &  240 (56\%) & 204 (49\%) &  444 (53\%)\\
Female &  180 (42\%) & 215 (51\%) &  395 (47\%)\\
Other &  5 (1\%) & 0 &  5 (<1\%) \\
\hline
\textbf{Ethnicity:} &   &  \\
Asian & 47 (11\%) & 23 (5\%)  &  70 (8\%)\\
Black & 26 (6\%) &  10 (2\%) &  36 (4\%)\\
White & 309 (73\%) &  357 (85\%) &  666 (79\%)\\
Other or multiple & 43 (10\%) &  29 (7\%) &  72 (9\%)\\
\hline
\textbf{Nationalities}: & Russia – 247 (56\%) & UK – 272 (65\%) & UK – 273 (32\%) \\
& Ukraine – 27 (7\%) & United States – 49 (12\%) & Russia – 247 (29\%) \\
& Turkey – 21 (5\%) &  Italy – 15 (4\%) & United States – 56 (7\%) \\
& Belarus - 20 (5\%)& Spain - 11 (3\%) & Ukraine - 27 (3\%) \\
& Pakistan - 15 (4\%)& Poland - 11 (3\%) & Turkey - 23 (3\%)\\
& India - 15 (4\%)& Canada - 11 (3\%)& Belarus - 20 (2\%)\\
& Kenya - 13 (3\%) & Portugal - 9 (2\%) & India - 15 (2\%) \\
& Other - 67 (16\%) & Other - 41 (10\%) & Other - 183 (22\%)\\
\hline
\textbf{Education:} &   &  \\
Doctorate & 5 (1\%)  & 11 (3\%) & 16 (1\%)\\
Master degree & 73 (17\%)  & 68 (16\%) & 141 (17\%)\\
Bachelor degree &  148 (35\%) & 173 (41\%) & 321 (38\%) \\
High school diploma &  107 (25\%) & 123 (29\%) & 230 (27\%)\\
Professional degree & 59 (14\%)  & 17 (4\%) & 76 (9\%)\\
Other qualifications or none & 33 (8\%) & 27 (6\%) & 60 (7\%)\\
\hline
\end{tabular}
\end{table*}


\begin{table*}[ht]
\caption{Arguments of the evaluated social media posts concerning processed red meat.}
\label{tab:claimmeat}
\begin{tabular}{|p{25em}|p{10em}|}
  \hline
Argument with the accurate claim  & Evidence type  \\
  \hline
Red processed meat is a major health hazard. The study review found that red processed meat was associated with a higher incidence and mortality of arterial disease.& Research \\
\hdashline
Red processed meat is a major health hazard. The WHO places processed meat into the highest risk category of carcinogens..& Testimony \\
\hdashline
Red processed meat is a major health hazard. Humans with a balanced diet without red processed meat feel better.& Consensus  \\
\hdashline
Red processed meat is a major health hazard. I eat red meat regularly and at my physical, some of my important indicators are alarming. & Personal Experience \\
   \hline
Argument with the inaccurate claim  &   \\
  \hline
Red processed meat is not a significant health hazard. My personal trainer appreciates red processed meat as a part of a balanced diet. & Testimony \\
\hdashline
Red processed meat is not a significant health hazard. Humans are carnivores and this is how we have survived. & Consensus  \\
\hdashline
Red processed meat is not a significant health hazard. I eat red meat regularly and at my physical, all of my important indicators are great. & Personal Experience \\
\hline
\end{tabular}
\end{table*}

\begin{figure*}[ht]
\centering
\caption{Example of the credibility evaluation task item on the Prolific platform.}
\includegraphics[width=.60\textwidth]{prolifictask.png}
\label{fig:profilictask}
\end{figure*}


\begin{figure*}[ht]
\centering
\caption{Illustration of algorithmic process\textsuperscript{1} used to generate 840 unique combinations of social media posts.}
\includegraphics[width=.99\textwidth]{generation.png}
    \bigskip
    \begin{minipage}{\textwidth}
        \captionsetup{labelformat=empty}
        \raisebox{1.5ex}{\llap{\textbf{}}}:\newline
        Note: \textsuperscript{1} Rows 1-3 represent steps in the computerized process of selecting a unique combination of source features (source, gender, and ethnicity [i.e., Professor who is female and Black] paired with a profile picture matching those features. Rows 4-6 represent steps in generating content of the social media post associated with that source by selecting a topic (Vitamin D), claim type (accurate), and evidence type (research).\newline
        \textsuperscript{2} The Research type of evidence was only available for accurate claims.
    \end{minipage}
\label{fig:generation}
\end{figure*}

We created four types of authors (or sources) and varied their expertise. Two sources, a professor of medicine and a nurse education practitioner for undergraduates, were experts in the health domain. The other two sources, a professional lifestyle blogger and a parent, did not have domain expertise. In addition to this information, the author’s profile included a short description, such as ‘passion for running’, to make the social media messages seem more authentic. 

Because some prior work has suggested that an author's ethnic background and gender may be associated with readers' responses to social media health information \autocites{armstrong2009blogs, spence2013intercultural}, we also varied the author gender and ethnicity. To do this, we used the Chicago face database \autocite{ma2015chicago}, which provides facial pictures along with the self-identified ethnicity and gender, as well as a subjective attractiveness score based on ratings from US-based raters. Using this data, we selected pictures of people with average attractiveness who were between 35 and 45 years of age. We included males and females, as well as those who self-identified as Asian, Black and White. The age and attractiveness selection were performed in order to create a uniform set of facial pictures and thus rule out very unlikely combinations, such as 20 year old professors. The selected profile pictures are shown in Appendix (Figure \ref{fig:faces}).

\subsubsection{Full Factorial Design}

Next, unique combinations of sourcing features (author expertise, gender, ethnicity, and profile picture) and content features (topic, claim type, and evidence type) were generated by applying mathematical principles of combinatorics \autocite{kuhn2009combinatorial} to a full factorial design to study all possible combinations of the different levels of factors being considered. Traditional experimental designs in credibility evaluation often avoid full factorial design because the total number of all combinations can escalate quickly, surpassing what is feasible to study - hence, the complexities we touch upon in the paper title. For instance, an experiment may examine two factors: source expertise of a post and evidence backing the post. If each factor has two levels (e.g., source expertise includes 1) Professor and 2) Blogger, and evidence type comprises 1) Research and 2) Personal experience), this results in four unique combinations (2x2). If there are three levels for each factor, then nine (3x3) combinations arise. Introducing a third factor with three potential values, such as source ethnicity, balloons the number of combinations to 27 (3x3x3). Number of factors and levels renders full factorial design unfeasible in studies of credibility evaluation. 

In our study, we implemented a full factorial design that tested 840 unique combinations. With such a vast number of combinations, we encountered two main challenges. The first was the effort of creating 840 distinct social media posts and ensuring each combination was created just once. The second challenge was how to conduct the experiment with this multitude of posts. We addressed the first issue using an algorithm and software that automatically generated the required 840 social media posts, ensuring all combinations were produced. This resulted in 480 unique posts with accurate knowledge claims and 360 unique posts with inaccurate knowledge claims. For the second challenge, we employed two crowdsourcing platforms, from which large number participants could easily be recruited.  To conduct the experiment, we used an incomplete repeated measures design since expecting one person to evaluate all 840 posts would be unrealistic. As a result, each participant only assessed 10 unique posts.

Figure \ref{fig:generation} illustrates how social media posts were generated with this computerized procedure by using the credibility evaluation task item presented in Figure 1 as an example. A combination of ethnicity, gender, and profession was used to select the profile picture (i.e., Black female professor) associated with the social media post, which was generated using a combination of the topic (Vitamin D), accuracy of claim (accurate), and evidence type (research). 

Although we conceptualized perceived credibility as a multidimensional construct, reflecting the idea of first- and second hand evaluation \autocites{barzilai2020dealing, stadtler2014content}, we measured participants’ credibility judgments with a single item  (cf. \cite{hanimann2023believing}). Participants were assigned to evaluate the credibility of ten posts on a single item scale from 1 to 7 (1 = not at all credible; 7 = extremely credible). The single item was used because we sought to understand how different content- and source-related issues affected participants’ overall evaluations of the social media posts’ credibility. Single-item scale may be advantageous because participants may resent being asked questions that appear to be repetitive \autocite{fuchs2009using}. In this study, the unit of analysis was the credibility evaluation of one post. 


\subsection{Prior Belief Measure}
Before gaining access to the credibility evaluation task, participants’ prior beliefs on the topic were measured using one statement for each topic at hand and a 7-point scale. The statements were worded, for example, ‘Vaccines are safe', and `Red meat is healthy'. We coded the responses to prior belief items in relation to how consistent they were with the claim. For accurate claims, a prior belief score of 7 corresponded to a prior belief consistency score of 3, and a prior belief score of 1 corresponded to consistency of -3. For inaccurate claims, this was done in reverse, with a prior belief score of 1 corresponding to consistency score of 3 and a prior belief score of 7 corresponding to consistency score of -3. For example, if the participant responded with 1 on the item “Vaccines are safe,”, it was coded as -3 for the accurate claim. In the case of an inaccurate claim, the participant’s response 1 was coded as 3. The variable was labeled as prior belief consistency, with a scale from -3 to 3.


\subsection{Data Collection Procedure}
Prior work has established cultural differences in credibility evaluation of online reviews \autocite{venturebeat2022how}, as well as how people use and trust health information in online contexts \autocites{khosrowjerdi2020national, song2016trusting}. We opted for two crowdsourcing platforms, with a wide variety of nationalities in the userbase. Crowdworkers on Toloka and Prolific were offered an opportunity to take part in our study. Participants were first asked to fill out a questionnaire on demographic information and the prior belief measure. After completing the questionnaire successfully, the participants proceeded to the credibility evaluation task, in which they were asked to evaluate the credibility of ten social media posts. We assigned only ten credibility evaluation items to each participant to ensure that they could focus sufficiently on each item. In addition, with this procedure we could recruit a considerable number of participants which assured that they represented diverse backgrounds. 

On the two platforms, somewhat different methodologies were used to deliver the set of credibility evaluation items to raters because of how these platforms worked. On the Prolific platform, we gave the tasks to participants in predetermined batches of ten, with minimized repetition of the manipulated aspects. In principle, ten posts could have some repetitive elements. The same topic, evidence type, source, and profile picture could have been shown to the participant in a maximum of three posts. However, the exact same post was not shown twice to the same participant. On the Toloka platform, each participant had an opportunity to make ten evaluations that were assigned to them at random. This was done by randomizing the order of posts that were given to the crowdworkers on a first-come first-serve basis. However, similar to Prolific, the exact same post was not shown twice to the same participant. Thus the items were manipulated independently of each other. Only the top 20 \% performing crowdworkers on the Toloka platform were offered the chance to participate in the study. Crowdworkers’ performance was calculated by their success in completing tasks on the platform (i.e., the acceptance rate of their previous work at the platform). All evaluations made on the Toloka platform were accepted. Of the Toloka participants, $ 99 \% $ completed the maximum of ten evaluations. The task used on the Prolific platform is shown in Figure \ref{fig:profilictask}.

\subsection{Statistical Analysis}\label{sec:statanalysis}

In the statistical analysis, the unit of analysis was the credibility evaluation of one social media post. The analyses were conducted separately for posts including accurate and inaccurate knowledge claims, which are later referred to as accurate and inaccurate posts below. The data consisted of 4787 credibility evaluations of accurate posts and 3593 evaluations of inaccurate posts. Twenty credibility evaluations were excluded from the analysis due to technical problems.  

All statistical analyses were conducted using R. To examine how independent variables (see Table \ref{tab:var}) were associated with the credibility evaluations of social media posts, we used a cumulative link mixed model. The cumulative link mixed model (CLMM), also known as ordered regression model \autocite{christensen2015ordinal}, has been advocated for use with ordered outcome measures \autocite{fullerton2021ordered}. 

To provide a CLMM model and odds ratios, we used the ordinal package created by \textcite{christensen2015ordinal} for the R programming language. The specific formula used is follows: "Credibility\textasciitilde  Accuracy + ClaimEvidence + Topic + Author + Platform + Gender + Ethnicity + (1|ID)". The control variable ID corresponds to each individual evaluator who performed a maximum of ten evaluations in the sample. For the reference category, that is, the category other categories are compared to in the model (see reference categories in Tables \ref{TableAcc} and \ref{TableInacc}), we chose the category that one would assume to produce the highest credibility rating. Otherwise we used the category that was first in alphabetical order. Thus, for evidence type, we chose research; for prior beliefs, we chose highest consistency, and for source expertise, we chose professor. For gender, ethnicity, platform, and topic, we used alphabetic order.

We also present odds ratios (ORs), which are calculated with the ordinal package \autocite{christensen2015ordinal}, to highlight the association between the independent variable and the dependent variable. Odds ratios have been advocated for use as an effect size measure with cumulative link mixed models and ordered regression models \autocites{fleiss1994measures, fullerton2016ordered}. Moreover, odds ratios have been used with CLMM models in a variety of fields \autocites{oehm2022identifying, rezapour2021modeling}. An OR greater than 1 indicates how many times greater the expected credibility evaluation value of the category of interest is than the reference category. In contrast, an OR smaller than 1 indicates that the expected evaluation value in the reference category is greater than that in the category of interest. The ORs were transformed to another popular effect size measure, Cohen's d, with the effectsize R package created by \textcite{ben2020effectsize}. The Cohen's d effect sizes were interpreted as follows: small (\textit{d} = 0.2), medium (\textit{d} = 0.5), and large (\textit{d} = 0.8)\autocite{cohen2013statistical}. To calculate the goodness-of-fit statistics, we used the anova.clmm function of the RVAideMemoire package \autocite{herve2020package} to perform a likelihood-ratio test \autocite{crainiceanu2004likelihood}. %and various pseudo rsquared indices \citep{smith2012examination}. 


