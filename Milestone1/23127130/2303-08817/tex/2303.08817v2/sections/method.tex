\section{Methodology}
\label{sec:method}
\begin{figure*}
	\centering
	\includegraphics[width=0.98\linewidth]{./figures/overview.pdf}
	\caption{An overview of DeepMIM. DeepMIM applies deep supervision on intermediate features during pre-training. Each lightweight decoder is composed of 4 Transformer blocks. DeepMIM is compatible with many MIM models across a range of architectures and reconstruction targets. Using the hybrid target generator (a pre-trained MIM model) is an optional choice.}
	\label{fig:method}
\end{figure*}

As shown in~\cref{fig:method}, DeepMIM adopts an encoder-multi-decoder architecture to perform the mask-and-predict task for ViT pretraining. 
For concreteness, we use MAE~\cite{he2022masked} to illustrate our underlying approach. 
Nevertheless, DeepMIM can be applied to various masked image modeling (MIM) frameworks---we describe how at the end of this section.


\noindent\textbf{Encoder and multi-level features.} 
We adopt ViT-B with 12 Transformer blocks as the encoder $h_{\theta}$. 
Following ViT~\cite{dosovitskiy2020image}, we divide an input image $\boldsymbol{x}$ into regular non-overlapping patches. 
Then, similar to MAE~\cite{he2022masked}, we randomly mask a high proportion of patches, yielding a masked image $\widetilde{\boldsymbol{x}}$. 
We then feed $\widetilde{\boldsymbol{x}}$ into the encoder $h_{\theta}$ to produce multi-level features.

\noindent\textbf{Multi-decoder.} 
Let $g_{\xi}$ represent the default decoder attached to the last (12$^{\mathrm{th}}$) block. 
In addition to the last Transformer block, decoders are also attached to intermediate blocks. 
For ViT-B, we append three extra decoders (denoted $g_{\xi}^{1}$, $g_{\xi}^{2}$ and $g_{\xi}^{3}$) onto the \sixth, \eighth and \tenth Transformer block of the encoder $h_{\theta}$ to facilitate deep supervision. 
Each decoder is an independent 4-layer Transformer with encoded visible patches (from the last block or intermediate blocks) and masked tokens as input. Thanks to the lightweight decoders, the overall training cost of DeepMIM is slightly higher than that of MAE, i.e., \methodName and MAE take 115 and 108 training hours respectively under a 1600-epoch schedule on 32$\times$NVIDIA V100 GPUs.
We use $\boldsymbol{p}$ and $\boldsymbol{p_i}$ to denote the reconstruction prediction by $g_{\xi}$ and $g_{\xi}^{i}$, respectively.


\noindent\textbf{Progressive hybrid targets (optional).} 
The features produced by the shallower layers of the ViT are less discriminative. 
It may be beyond the capacity of these intermediate features to reconstruct the targets that are too complicated, $i.e.$, raw pixels. 
As a by-product of MIM task, a pre-trained MAE is able to recover masked images.  
Though MAE produces fuzzy reconstruction results\footnote{Image inpainting is not the objective of MAE.}, we propose to use these reconstructed images as appropriate reconstruction targets to ease the learning of the intermediate blocks. We name a pre-trained MAE as a hybrid target generator, as illustrated in Figure~\ref{fig:method}. Concretely, given a masked image $\widetilde{\boldsymbol{x}}$, we feed it into a pre-trained MAE to generate a reconstructed image, denoted $\hat{\boldsymbol{x}}$. 
The hybrid target $\boldsymbol{t}$ is generated by blending the raw image $\boldsymbol{x}$ and the reconstructed image $\hat{\boldsymbol{x}}$ with a blending ratio $\alpha$:
\begin{equation}
\label{eq:blend}
    \boldsymbol{t} = \alpha \boldsymbol{x} + (1-\alpha)\hat{\boldsymbol{x}}.
\end{equation}
Let $\boldsymbol{t}_i$ represent the reconstruction target of the decoder $g_{\xi}^{i}$. We set $\alpha$ as $0$, $1/3$ and $2/3$ for $g_{\xi}^{1}$, $g_{\xi}^{2}$ and $g_{\xi}^{3}$, respectively. 

Note that the hybrid targets are optional. Even though using hybrid targets improves fine-tuning performance, there is still additional computational overhead. We suggest to use hybrid targets only if there is an off-the-shelf hybrid target generator (a pre-trained MIM model). We use DeepMIM$^\dagger$ to denote our framework with hybrid targets. For DeepMIM (without hybrid targets), we set $\alpha=1$ for all decoders.


\noindent\textbf{Training objective.}
The overall loss is the sum of the $M+1$ $\ell$-2 reconstruction losses produced by the $M$ extra decoders and the primary decoder:
\begin{equation}
    	\mathcal{L} = \sum_{i=1}^{M} \|\mathcal{M}(\boldsymbol{t}_i)-\boldsymbol{p}_i\|_2^2 + \|\mathcal{M}(\boldsymbol{x})-\boldsymbol{p}\|_2^2, 
\label{eq:loss}
\end{equation}
where $\mathcal{M}(\cdot)$ denotes the operation that extracts the targets of the masked patches, and $M$ is the number of extra decoders. $M=3$ for ViT-B.


\noindent\textbf{\methodName is compatible with a range of MIM models.} 
\methodName can be applied to a range of MIM models with different reconstruction targets, $\textit{e.g.}$, RGB pixels~\cite{he2022masked,xie2022simmim}, discrete tokens~\cite{bao2021beit}, histograms of oriented gradients~\cite{wei2022masked}, CLIP features~\cite{hou2022milan} and DINO features~\cite{caron2021emerging}. The implementation consists of four steps:
1) utilize an off-the-shelf MIM model to produce reconstructed signals for masked images (optional);
2) generate a collection of hybrid targets by integrating raw signals and reconstructed signals with a given blending ratio (optional);
3) append extra decoders to the intermediate Transformer blocks;
4) train the MIM model equipped with \methodName through the loss defined in Eq.~\ref{eq:loss}.
