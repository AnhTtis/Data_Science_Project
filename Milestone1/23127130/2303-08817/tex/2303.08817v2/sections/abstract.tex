\begin{abstract}
Deep supervision, which involves extra supervisions to the intermediate features of a neural network, was widely used in image classification in the early deep learning era since it significantly reduces the training difficulty and eases the optimization like avoiding gradient vanish over the vanilla training. Nevertheless, with the emergence of normalization techniques and residual connection, deep supervision in image classification was gradually phased out. In this paper, we revisit deep supervision for masked image modeling (MIM) that pre-trains a Vision Transformer (ViT) via a mask-and-predict scheme. Experimentally, we find that deep supervision drives the shallower layers to learn more meaningful representations, accelerates model convergence, and expands attention diversities. Our approach, called \methodName, significantly boosts the representation capability of each layer. In addition, \methodName is compatible with many MIM models across a range of reconstruction targets. For instance, using ViT-B, \methodName on MAE achieves 84.2 top-1 accuracy on ImageNet, outperforming MAE by +0.6.
By combining \methodName with a stronger tokenizer CLIP, our model achieves state-of-the-art performance on various downstream tasks, including image classification (85.6 top-1 accuracy on ImageNet-1K, outperforming MAE-CLIP by +0.8), object detection (52.8 AP$^{\text{box}}$ on COCO) and semantic segmentation (53.1 mIoU on ADE20K). Code and models are available at \url{https://github.com/OliverRensu/DeepMIM}.


\if
Masked image modeling (MIM), which couples the mask-and-predict task with an encoder-decoder architecture, shows considerable potential for ViT pre-training.
A range of prior work has investigated refinements to MIM relating to the design of appropriate reconstruction targets.
In this work, we study an orthogonal dimension of MIM---where to apply the reconstruction loss.
We discover that features produced by shallower Transformer blocks exhibit predictive power for reconstruction, motivating the application of deep supervision on intermediate features during pre-training.
Rather than using the same reconstruction targets as the last block, we generate different levels of hybrid targets, that are less challenging to reconstruct as targets for less-discriminative intermediate features.
Our approach, called \methodName, significantly boosts the representation capability of each block.
In addition, \methodName is compatible with many MIM models across a range of architectures and reconstruction targets.
For instance, using ViT-B, \methodName on MAE achieves 84.2 top-1 accuracy on ImageNet, outperforming MAE by +0.6 while reducing training epochs by half.
By combining \methodName with a stronger tokenizer CLIP, our model achieves state-of-the-art performance on various downstream tasks, including image classification (85.6 top-1 accuracy on ImageNet-1K outperforms by MAE-CLIP by 0.8\%), object detection (52.8 AP$^{\text{box}}$ on COCO) and semantic segmentation (53.1 mIoU on ADE20K).
\fi
\end{abstract}