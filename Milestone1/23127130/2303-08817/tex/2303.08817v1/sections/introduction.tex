\section{Introduction}
\begin{figure}
\centering
    \includegraphics[width=1.0\linewidth]{./figures/tesear/teaser_acc.pdf}
    \caption{\methodName improves a broad range of MIM models.
    We highlight the improvements of top-1 accuracy on ImageNet-1K achieved by combining \methodName with various reconstruction targets (\textit{e.g.}, MAE-Pixels, MAE-DINO and MAE-CLIP). All models are pre-trained for 1600 epochs except those shown on the far right which are the best-performing models under the 800-epoch or 1600-epoch pre-training schedule.}
    
    \label{fig:tesear_acc}
\end{figure}
In the early deep learning era, deep supervision, which involves extra supervisions to the intermediate features of a neural network, has been widely adopted to a number of computer vision tasks, e.g., image classification~\cite{szegedy2015going,lee2015deeply} and edge detection~\cite{xie2015holistically}. Almost a decade ago, GoogLeNet~\cite{szegedy2015going} won the ILSVRC14 competition. A key element of GoogLeNet was its use of additional losses on intermediate features, motivated by the goal of ensuring gradients are propagated back through the network effectively, and to improve regularisation and discrimination in the network lower layers. However, deep supervision in image classification has fallen out of favour with the emergence of batch normalization~\cite{ioffe2015batch} and residual connections~\cite{he2016deep} which appear to have substantially mitigated issues relating to vanishing gradients. Experimental results presented in \cref{tab:supvsmim} also suggest that deep supervision offers limited value for supervised ViT on image classification.


In this paper, we revisit deep supervision for masked image modeling (MIM)~\cite{he2022masked,dong2021peco,wei2022masked,bao2021beit}, a self-supervised pre-training strategy for Vision Transformer~\cite{dosovitskiy2020image} (ViT). MIM is a simple but effective idea that trains a ViT by requiring it to predict masked inputs. Typically, MIM models adopt an encoder-decoder architecture with a masking scheme applied on image patches: the encoder (\textit{i.e.}, ViT) generates the latent representations from the last Transformer block, while the decoder predicts the reconstruction targets of the masked patches. When transferring the MIM pre-trained model to downstream tasks, the encoder is retained while the decoder is dropped. As a result, the decoder implicitly deepens the network during the pre-training stage, making the shallower layers of the encoder receives weaker informative feedback from the supervision signal. 



\begin{table}[t]
\centering
\begin{tabular}{l|c|c}
\toprule
Model & Deep Supervision & IN Top-1 (\%) \\
\midrule
\multicolumn{3}{c}{\textit{Supervised learning}} \\
\midrule
Supervised Cls.  & &    81.2       \\
Supervised Cls. &   $\checkmark$  &  80.1 (\red{-1.1})    \\
Supervised Cls.$^\ddagger$ &   $\checkmark$  &  80.6 (\red{-0.6})    \\
\midrule
\multicolumn{3}{c}{\textit{Self-supervised pre-training}} \\
\midrule
MAE  &   &    82.6     \\
DeepMIM-MAE  &   $\checkmark$ &    83.4 (\textcolor{OliveGreen}{+0.8})    \\
DeepMIM$^\dagger$-MAE     &   $\checkmark$    &  83.6 (\textcolor{OliveGreen}{+1.0}) \\
\bottomrule
\end{tabular}
\caption{
\textbf{Deep supervision benefits self-supervised pretraining.}
We compare the effect of deep supervision during supervised learning and self-supervised pre-training, respectively, and report top-1 accuracy on ImageNet (IN).
The experiment adopts a DeiT-style~\cite{deit} training paradigm to optimize ViT-B on supervised image classification. We pre-train MAE and DeepMIM on MAE for 300 epochs on ViT-B.
$\ddagger$: that hyper-parameters are carefully tuned.
DeepMIM$^{\dagger}$: DeepMIM with hybrid targets (optional).
}
\label{tab:supvsmim}
\end{table}

To drive the shallower layers learn more meaningful representations, we present DeepMIM, a ViT pre-training framework with deep supervision applied to MIM pretext task: we pre-train an MAE (on ViT-B) with three extra lightweight decoders appended to the outputs of the shallow blocks (the \sixth, \eighth and \tenth blocks) of the encoder (ViT-B). Then ViT-B is finetuned on ImageNet-1K~\cite{deng2009imagenet}, following common practice~\cite{he2022masked}. 
Surprisingly, a substantial improvement (+0.8 top-1 accuracy) over the baseline (ViT-B pre-trained via the original MAE) is observed in \cref{tab:supvsmim}, highlighting the potential benefits of deep supervision in MIM pre-training.

To investigate the benefits of introducing deep supervision into MIM pre-training, we comprehensively study several factors including 1) reconstruction loss of MAE and \methodName; 2) CKA (centered kernel alignment)~\cite{kornblith2019similarity} similarities between features produced by the intermediate layers and features produced by the last layer in DeepMIM; 3) attention diversities of different attention heads. In \cref{fig:tesear_loss}, we find that DeepMIM applied to MAE attains lower
reconstruction losses across various layers in comparison with MAE. Xie \textit{et al.}~\cite{xie2022data} also reveal that the lower the reconstruction loss on the validation set is, the better performance of MIM is. In addition, we also observe that CKA scores between the last and shallow layers of \methodName always surpass those of MAE (\cref{fig:cka}) and DeepMIM learns more diverse attention heads than MAE (\cref{fig:similarity}).

Recent MIM works have studied the question: \textit{what} are appropriate reconstruction targets? Proposals have included discrete tokens~\cite{bao2021beit}, RGB pixels~\cite{he2022masked,xie2022simmim}, histograms of oriented gradients~\cite{wei2022masked} and CLIP features~\cite{peng2022beit,hou2022milan}. Our work dedicates to studying an orthogonal component of MIM pre-training: \textit{where} should the reconstruction loss be applied?  Therefore, our DeepMIM is compatible with a broad range of encode-decoder MIM models as shown in \cref{fig:tesear_acc}. Incorporating DeepMIM into various MIM models including MAE, and several other MAE variants with different reconstruction targets yield consistent improvements over the non-\methodName baselines. With parallel small decoders, \methodName only brings slightly more computation costs. The contributions of this work can be summarized as follows:
\begin{itemize}
    \item We revisit the deep supervision for MIM pre-training. In contrast to previous MIM works exploring \textit{what} form appropriate reconstruction targets should take, we focus on an orthogonal direction: \textit{where} to apply the reconstruction loss. We throughly investigate the benefits of introducing deep supervision into MIM pre-training and find that it results in lower reconstruction loss, more diverse heads, and more powerful representation capability of the shallower layers.
    \item We present an optional module termed hybrid target generator, which further boosts the performance but involves extra computational overhead. A comparison with DeepMIM-MAE and MAE is shown in \cref{tab:supvsmim}.
    
    \item DeepMIM is complementary to most existing MIM works. Extensive experiments demonstrate that MIM models equipped with \methodName significantly outperform their non-\methodName counterparts. For instance, using ViT-B, MAE with \methodName achieves 84.2 top-1 accuracy on ImageNet, outperforming MAE by +0.6 top-1 accuracy. By using \methodName with a strong reconstruction tokenizer, CLIP, we achieve state-of-the-art performance on various downstream tasks including image classification (85.6 top-1 accuracy on ImageNet-1K), object detection (52.8 AP$^{\text{box}}$ on COCO) and semantic segmentation (53.1 mIoU on ADE20K).
\end{itemize}

