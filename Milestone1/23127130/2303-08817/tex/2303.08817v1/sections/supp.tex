In this supplementary material, we first provide further implementation details (\cref{sec:implementation}).
We then describe additional ablation studies (\cref{sec:ablations}).

\section{More Implementation Details}
\label{sec:implementation}
% \noindent\textbf{Pre-training on ImageNet-1K.} \fawe{Batch size. LR and hyper-parameters for different models. Add more if we did not mention in the main paper.}

\noindent\textbf{Pretraining.} We follow the same training recipe of MAE~\cite{he2022masked} and use a batch size of 4096 and a learning rate of $lr = base\_lr \times \text{BatchSize}/256$, where $base\_lr=$1.5e-4. We adopt a cosine decay schedule with a warm-up for 40 epochs.  We use AdamW~\cite{loshchilov2018decoupled} with a weight decay of 0.05 as the optimizer. For data augmentations, we use random resized cropping and random horizontal flipping. The mask ratio is set to 75\% by default. 


\noindent\textbf{Finetuning on ImageNet-1K.} ImageNet-1K~\cite{deng2009imagenet} contains 1.28M images for training and 50K images for validation. We follow the same recipe of MAE~\cite{he2022masked} on ImageNet-1K except for the learning rate and layer decay. We adopt AdamW as the optimizer and finetune the pre-trained model for 100 epochs. The learning rate is set to 2e-4 for DeepMIM on MAE-Pixel~\cite{he2022masked}/HOG~\cite{dalal2005histograms}/DINO~\cite{caron2021emerging}/Codebook~\cite{dong2021peco} and 4e-4 for DeepMIM on MAE-CLIP. We adopt a cosine decay schedule with a warm-up for 5 epochs. The layer-wise learning rate decay is set to 0.65 for 300-epoch pre-trained model and 0.6 for 800-epoch pre-trained model. The batch size is set to 1024. For data augmentation and regularization, we adopt drop path with a drop path rate of 0.1, RandAugment, label smoothing of 0.1, Mixup of 0.8, CutMix of 1.0.

\noindent\textbf{Finetuning on COCO Objection Detection and Instance Segmentation.} COCO~\cite{lin2014microsoft} contains 118K images for training and 5K images for validation. Following\cite{he2022masked}, we adopt DeepMIM pre-trained ViT as the backbone and Mask-RCNN~\cite{he2017mask} as the framework. For all DeepMIM pre-trained models, we use AdamW as the optimizer with a weight decay of 0.1. The learning rate, the layer-wise learning rate decay and the batch size is set to 1e-4, 0.75 and 16, respectively. The input resolution is 1024$\times$1024. We do not use multi-scale testing.

\noindent\textbf{Finetuning on ADK20K Semantic Segmentation.} ADE20K~\cite{ade20k} is a widely used dataset for semantic segmentation with 150 classes and 20K images for training and 2K images for validation. We use DeepMIM pre-trained ViT as the backbone and UperNet~\cite{xiao2018unified} as the segmentation model. We use AdamW as the optimizer with a weight decay of 0.05. We set drop path rate to 0.1, batch size to 16. For DeepMIM on MAE, the learning rate is set to 1e-4, the layer-wise learning rate decay is set to 0.65. For DeepMIM on MAE-CLIP, the learning rate is set to 5e-5, the layer-wise learning rate decay is set to 0.9. The input resolution is 512$\times$512. We do not use multi-scale testing.

\noindent\textbf{Pre-training and Finetuning on Kinetics-400 Video Classification.} DeepMIM on VideoMAE~\cite{tong2022videomae} is pre-trained for 800 epochs and finetuned for 75 epochs on Kinetics-400 video classification dataset. The optimizer, data augmentation and hyper-parameters are the same as when pre-training on ImageNet. We use a batch size of 1024 during pre-training. During finetuning, we use AdamW with a weight decay of 0.05 as the optimizer. The learning rate is set to 1e-3. We adopt a cosine decay schedule with a warm-up for 5 epochs. The layer-wise learning rate decay is set to 0.75. For data augmentation and regularization, we adopt drop path with a drop path rate of 0.1, RandAugment, label smoothing of 0.1, Mixup of 0.8, CutMix of 1.0. We follow SlowFast~\cite{feichtenhofer2019slowfast} and take 5 clips $\times$ 3 crops for inference.

\section{More Ablation Studies}
\label{sec:ablations}
\begin{table}[t]
	\centering
	\begin{tabular}{lcl}
		\toprule
		Method    & Masking Strategy & Top-1 Acc. \\
		\midrule
		MAE  & Random~\cite{he2022masked} & 82.6 \\
		DeepMIM  & Random~\cite{he2022masked} & 83.6 (+1.0) \\
		\midrule
		MAE  & Semantic-guided~\cite{li2022semmae} & 82.8 \\
		DeepMIM  & Semantic-guided~\cite{li2022semmae} & 83.8 (+1.0) \\
		\midrule
		MAE  & Semantic-aware~\cite{hou2022milan} & 82.9 \\
		DeepMIM  & Semantic-aware~\cite{hou2022milan}  & 83.8 (+0.9) \\
		\bottomrule
	\end{tabular}
		\caption{DeepMIM is compatible with different masking strategies. The results are evaluated on ImageNet-1K finetuning.}
	\label{tab:masking}
\end{table}
\noindent\textbf{DeepMIM is Compatible with Different Masking Strategies.}
DeepMIM is compatible not only with different reconstruction targets at output level, but also with different masking strategies at input level. As shown in Table~\ref{tab:masking}, DeepMIM consistently improves various MIM models with different masking strategies including random masking~\cite{he2022masked}, semantic-guided masking~\cite{li2022semmae} and semantic-aware masking~\cite{hou2022milan}.