\section{Experiments}
\noindent\textbf{Implementation Details.}
We adopt ViT-B/16 as our backbone with input size of $224\times 224$. 
The ImageNet-1K image classification dataset is used for both pre-training and fine-tuning~\cite{deng2009imagenet}. 
We apply our \methodName on MAE with RGB pixels~\cite{he2022masked} and CLIP features~\cite{radford2021learning}. 
For comparison with state-of-the-art methods, we pre-train \methodName for 300 and 1600 epochs.
% We pre-train ViT-B/16 on ImageNet-1K training set for 300 and 800 epochs. 
We follow the same training recipe of MAE~\cite{he2022masked}, more details can be found in supplementary materials. We adopt the hybrid target generator (a pre-trained MIM model) if the reconstruction targets are pixels and detach it when using other reconstruction targets, since there are lots of off-the-shelf pre-trained MIM models with pixels as the reconstruction targets (e.g. from Huggingface). We use DeepMIM$^\dagger$ to indicate \methodName with hybrid targets.



\subsection{Main Results}

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{lccccc}
		\toprule
		Method & Epochs    & Target & Top-1 \\
		\midrule
		\multicolumn{4}{c}{\textit{ViT-B}} \\
		BeiT~\cite{bao2021beit}& 300 & DALL-E& 83.2  \\
		CIM~\cite{fang2022corrupted}& 300 & Pixel& 83.3 \\
		CAE~\cite{chen2022context}& 300  & Momentum & 83.6 \\
		MaskFeat~\cite{wei2022masked} & 300 & HOG & 83.6 \\
		iBoT~\cite{zhou2021ibot}       &  400 & Momentum  & 83.8  \\
		PeCo~\cite{dong2021peco} & 300  & Codebook & 84.1  \\
		\midrule
		MAE~\cite{he2022masked}   & 300 & Pixel & 82.6  \\
  \methodName-MAE & 300   & Pixel & 83.4 \\
		DeepMIM$^\dagger$-MAE & 300   & Pixel & 83.6 \\
		\midrule
		MAE-CLIP$^\star$~\cite{he2022masked}   & 300 & CLIP& 83.8  \\
		\methodName-MAE-CLIP & 300  & CLIP & 84.8 \\
		\midrule
		\midrule
		\multicolumn{4}{c}{\textit{ViT-B + longer pre-training}} \\
		DINO~\cite{caron2021emerging}    & 1600  & Momentum & 82.8 \\
		BEiT~\cite{bao2021beit}          & 800  & DALL-E & 83.2 \\
		SimMIM~\cite{xie2022simmim}      & 800  & Pixel   & 83.8 \\
		SIM~\cite{tao2022siamese}        & 1600  & Momentum & 83.8 \\
		CAE~\cite{chen2022context}& 1600  & Momentum & 83.8 \\
		MaskFeat~\cite{wei2022masked}    & 1600  & HOG & 84.0  \\
		iBOT~\cite{zhou2021ibot}         & 1600  & Momentum & 84.0 \\
		PeCo~\cite{dong2021peco}         & 800  & Codebook & 84.5 \\
		%CMAE~\cite{huang2022contrastive}  & 1600  & Momentum & 84.7 \\
		\midrule
		MAE~\cite{he2022masked}         & 1600 & Pixel & 83.6 \\
  		DeepMIM-MAE & 1600   & Pixel & 84.0 \\
		DeepMIM$^\dagger$-MAE & 1600   & Pixel & 84.2 \\
		\midrule
		MAE-CLIP$^\star$ & 1600  & CLIP & 84.8 \\
		DeepMIM-MAE-CLIP & 1600  & CLIP & 85.6 \\
		\bottomrule
	\end{tabular}
 \vspace{-3mm}
	\caption{
 Comparison with previous self-supervised pre-training methods on ViT-B/16. 
 We report top-1 accuracy on ImageNet-1K. 
 $\star$: reproduced result. MAE-CLIP: an MAE variant with CLIP features as targets.}
  \vspace{-1mm}
	\label{tab:ImageNet-1k}
\end{table}

\noindent\textbf{Image Classification.} 
In~\cref{tab:ImageNet-1k}, we compare the fine-tuning results of various self-supervised pre-training methods on ImageNet-1K. 
We report the top-1 accuracy on the ImageNet validation set. 
Under a 300-epoch pre-training schedule, DeepMIM$^\dagger$-MAE (83.6) outperforms the MAE baseline (82.6) by +1.0. 
With a more powerful tokenizer like CLIP, \methodName-MAE-CLIP achieves 84.8 top-1 accuracy, outperforming the baseline of MAE-CLIP by +1.0. 

When conducting a longer pre-training schedule (1600 epochs), DeepMIM$^\dagger$-MAE (84.2) surpasses MAE (83.6) by +0.6. \methodName-MAE-CLIP (85.6) outperforms MAE-CLIP (84.8) by +0.8, achieving a new state-of-the-art.



\noindent\textbf{Object Detection.}
Following common practice, we use the COCO~\cite{lin2014microsoft} benchmark to evaluate the transferability of \methodName to the object detection task.
We employ \methodName pre-trained ViT-B/16 as the backbone and Mask R-CNN~\cite{he2017mask} as the detector.
We report AP$^{\text{box}}$ AP$^{\text{mask}}$ in~\cref{tab:coco_det}. 
DeepMIM$^\dagger$-MAE outperforms the MAE baseline by +1.3 AP$^{\text{box}}$ and +0.3 AP$^{\text{mask}}$. \methodName with MAE-CLIP achieves state-of-the-art performance of 52.8 AP$^{\text{box}}$ and 46.0 AP$^{\text{mask}}$.
\begin{table}
		\centering
		\setlength{\tabcolsep}{8pt}
		\begin{tabular}{l|c|c|c}
			\toprule
			Method & Epochs  & AP$^{\text{box}}$ & AP$^{\text{mask}}$ \\
			\midrule
			PeCo~\cite{dong2021peco} & 800 & 44.9 & 40.4 \\
			MoCo-v3~\cite{chen2021mocov3} & 300  & 47.9 & 42.7 \\
			SIM~\cite{tao2022siamese} & 1600 & 49.1 & 43.8 \\
			BEiT~\cite{bao2021beit}& 800 &  49.8 & 44.4 \\
			iBOT~\cite{zhou2021ibot} & 1600 & 51.2 & 44.2\\
			CAE~\cite{chen2022context} & 1600 & 50.1 & 44.0 \\
			MAE~\cite{he2022masked}  & 1600 & 50.3 & 44.9 \\
   \midrule
%   			DeepMIM-MAE & 1600 & XXX & XXX \\
			DeepMIM$^\dagger$-MAE & 1600 & 51.6 & 45.2  \\
			\methodName-MAE-CLIP & 1600 & 52.8 & 46.0  \\
			\bottomrule
			
		\end{tabular}
   \vspace{-3mm}
		\caption{COCO~\cite{lin2014microsoft} object detection and instance segmentation using Mask R-CNN~\cite{he2017mask}. 
		%$^\ddagger$ means using Cascade Mask R-CNN~\cite{cai2019cascade}.
		}
		\label{tab:coco_det}
\end{table}



\begin{table}[t]
	\centering
	\renewcommand\arraystretch{1.1}
		\begin{tabular}{l|c|c}
			\toprule
			Method & Epochs & mIoU\\
			\midrule
			CIM~\cite{fang2022corrupted} & 300 &  43.5 \\
			CAE~\cite{chen2022context} & 1600&  44.0 \\
			BEiT~\cite{bao2021beit} & 800  & 47.1\\
			MoCo-v3~\cite{chen2021mocov3} & 300 & 47.3 \\
			DINO~\cite{caron2021emerging} & 400& 47.2 \\
			PeCo~\cite{dong2021peco} & 800 & 48.5\\
			iBOT~\cite{zhou2021ibot} & 1600 & 50.0\\
			%CMAE~\cite{huang2022contrastive} & 1600  & 50.1 \\
			MAE~\cite{he2022masked} & 1600 &48.1 \\
                \midrule
%                DeepMIM-MAE & 1600 &XXX \\
			DeepMIM$^\dagger$-MAE & 1600 &49.5 \\
			\methodName-MAE-CLIP & 1600 & 53.1 \\
			\bottomrule
		\end{tabular}
  \vspace{-3mm}
		\caption{ADE20K~\cite{ade20k} semantic segmentation using UperNet~\cite{xiao2018unified}}
  \vspace{-3mm}
		\label{tab:seg}
	\end{table}
	
\noindent\textbf{Semantic Segmentation.}
We also transfer the \methodName pre-trained ViT-B/16 to semantic segmentation on the ADE20K~\cite{ade20k} benchmark.
We use UperNet~\cite{xiao2018unified} for a fair comparison with previous methods.
We report mean intersection over union (mIoU) for each model in~\cref{tab:seg}.
DeepMIM$^\dagger$-MAE outperforms its counterpart by +1.4, and \methodName-MAE-CLIP achieves the state-of-the-art performance of 53.1 mIoU.


\noindent\textbf{Video Classification.}
This transfer experiment is conducted on Kinetics-400~\cite{kay2017kinetics} benchmark using ViT-B. 
We apply DeepMIM to VideoMAE~\cite{tong2022videomae}, and report top-1 accuracy on Kinetics-400 validation set. 
We follow the same pre-training and fine-tuning settings of VideoMAE and pre-train \methodName-VideoMAE for 800 epochs for a fair comparison with VideoMAE. 
The results are reported in~\cref{tab:k400}.
\methodName-VideoMAE improves over the baseline VideoMAE by +1.2.

\noindent\textbf{Robustness Evaluation on Out-of-domain Datasets.}
We evaluate \methodName-MAE on three out-of-domain datasets~\cite{imageneta,imagenetr,imagenetc}: ImageNet-A (natural adversarial examples), ImageNet-R (semantic shifts), and ImageNet-C (image corruptions). We report top-1 accuracy on ImageNet-A/R and mCE error on ImageNet-C. The results are shown in \cref{tab:robust}, our \methodName significantly outperforms MAE baseline by large margins.


% add pretrain epoch
\begin{table}[t!]
	\centering
	\begin{tabular}{l|c|c}
		\toprule
		Method &Pre-Data  & Top-1 Acc. \\
		\midrule
		
		NL I3D~\cite{nonlocal} &  \multirow{4}{*}{ImageNet-1K}   & 77.3   \\
		
		TANet~\cite{tanet} &   & 79.3 \\
		TDN$_{En}$~\cite{tdn} &  & 79.4    \\
		Video Swin~\cite{liu2021video}&  &  80.6   \\
		\midrule
		TimeSformer~\cite{timesformer} & \multirow{2}{*}{ImageNet-21K} & 78.3 \\
		Motionformer~\cite{motionformer} &     & 80.2 \\
		%Video Swin~\cite{liu2021video}  &  & 80.6  \\
		\midrule
		VideoMAE~\cite{tong2022videomae} &  \multirow{2}{*}{Kinetics-400}  & 80.0  \\
		%Ours on MAE & ViT-B & ImageNet-1K & 16 & 180$\times$5$\times$3 & 87 & 81.4 \\
		\methodName-VideoMAE  &&  81.2 \\
		\bottomrule
	\end{tabular}
 \vspace{-3mm}
	\caption{Kinetics-400~\cite{kay2017kinetics} video classification. }
  \vspace{-2mm}
	\label{tab:k400}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{l|c|c|c}
\toprule
Method& IN-A$\uparrow$ &IN-R$\uparrow$&IN-C $\downarrow$ \\
\midrule
DeiT~\cite{deit} &  25.8      & 45.4 & 36.8\\
%Swin~\cite{liu2021Swin}&     &  & \\
MAE~\cite{he2022masked}  &   33.6  &50.0& 37.8     \\
\methodName-MAE &   52.0  &64.5 &  33.0    \\
\bottomrule
\end{tabular}
\vspace{-3mm}
\caption{Robustness evaluation on out-of-domain datasets.}
\vspace{-2mm}
\label{tab:robust}
\end{table}

\subsection{Analysis and Ablation Study}
For all ablation studies, we adopt \methodName-MAE with a 300-epoch pre-training schedule and a 100-epoch fine-tuning schedule on ImageNet-1K (reporting top-1 accuracy). 
We employ ViT-B as the backbone.

\begin{table}[t]
	\centering
	\begin{tabular}{c|c|c}
		\toprule
		Deep Supervision    & Hybrid Target   & Top-1 Acc. \\
		\midrule
		   &          &   82.6  \\
		 \checkmark &          &   83.4  \\
		 \checkmark  &     \checkmark      &   83.6  \\
		\bottomrule
	\end{tabular}
 \vspace{-3mm}
	\caption{Ablation study on the effectiveness of deep supervision and hybrid targets.}
 \vspace{-3mm}
	\label{tab:component}
\end{table}

\noindent\textbf{The effectiveness of deep supervision and progressive hybrid targets.}
\methodName presents two techniques:
1) appending extra decoders to the intermediate blocks of the encoder to enable deep supervision for MIM pre-training;
2) utilizing progressive hybrid targets as the reconstruction targets for intermediate features. 
\cref{tab:component} demonstrates the effectiveness of each proposed technique.

\noindent\textbf{Deep Supervision.} 
\methodName appends extra decoders onto the intermediate blocks of the encoder.
Here we explore: 1) where to apply deep supervision; 2) how many blocks should be involved.
We do not use progressive hybrid targets in this study for efficiency.
The results are shown in~\cref{tab:stages}.
We compare several variants with MAE baseline, which uses a single decoder on the output of the last Transformer block.
We conclude that: 1) Regardless of how many blocks are used for deep supervision, all variations outperform the baseline.
2) Deep supervision should be applied to the appropriate blocks.
For instance, the variant with \{3$^{\mathrm{rd}}$, 6$^{\mathrm{th}}$, 9$^{\mathrm{th}}$, 12$^{\mathrm{th}}$\} blocks involved deep supervision performs worse than that involving \{6$^{\mathrm{th}}$, 9$^{\mathrm{th}}$, 12$^{\mathrm{th}}$\} blocks.
3) The configuration which involves \{6$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$, 12$^{\mathrm{th}}$\} blocks yields the best result, outperforming the MAE baseline by +0.8.



\begin{table}[]
	\centering
	\begin{tabular}{c|c|c}
		\toprule
		Intermediate Block    & Last Block (12$^{\mathrm{th}}$)     & Top-1 Acc. \\
		\midrule
		    -     &  \checkmark         &   82.6  \\
		\midrule
		  6$^{\mathrm{th}}$ & \checkmark      &  82.8  \\
		8$^{\mathrm{th}}$  &\checkmark      & 82.9\\
		9$^{\mathrm{th}}$ & \checkmark      &  82.9   \\
		\midrule
		6$^{\mathrm{th}}$, 9$^{\mathrm{th}}$ & \checkmark  &  83.1   \\
		  8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$      & \checkmark  &  83.1 \\
		\midrule
		3$^{\mathrm{th}}$, 6$^{\mathrm{th}}$, 9$^{\mathrm{th}}$& \checkmark &  83.0   \\
		4$^{\mathrm{th}}$, 6$^{\mathrm{th}}$, 8$^{\mathrm{th}}$ & \checkmark &   83.2  \\
		6$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$ & \checkmark&  83.4  \\
		\bottomrule
	\end{tabular}
  \vspace{-3mm}
	\caption{Study of deep supervision using \methodName-MAE. First row is an MAE baseline.}
 \vspace{-2mm}
	\label{tab:stages}
\end{table}

\noindent\textbf{Progressive hybrid targets.} 
In~\cref{sec:method}, we propose to generate hybrid targets for different intermediate blocks by blending raw signals and the reconstructed signals with a blending ratio of $\alpha$ that controls the ratio between them (see~\cref{eq:blend}). 
Here we study using different blending ratios for the configuration that involves deep supervision on the \{4$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$, 12$^{\mathrm{th}}$\} blocks in~\cref{tab:genraw}.
We observe a slight improvement for the configuration where the targets of the \{4$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$\} blocks are pure reconstructed signals. 
Adopting progressive hybrid targets ($\alpha=0,1/3,2/3,1$ for the 6$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$ and 12$^{\mathrm{th}}$ blocks) yields the best performance, achieving 83.6\% top-1 accuracy.




\begin{table}[t]
	\centering
	\begin{tabular}{c|cccc|c}
		\toprule
		\multirow{2}{*}{Config ID}  & \multicolumn{4}{c|}{Blending Ratio ($\alpha$)} & \multirow{2}{*}{Top-1 Acc.} \\
		 &   6$^{\mathrm{th}}$    &8$^{\mathrm{th}}$ &10$^{\mathrm{th}}$     & 12$^{\mathrm{th}}$      &    \\
		\midrule
		1 & 1 & 1 & 1 & 1 & 83.5 \\
		2 & 0 & 0 & 0 & 1 & 83.4 \\
		3 & 1/2 & 1/2 & 1/2 & 1/2 & 83.4\\
		4 & 0 & 1/3 & 2/3 & 1& 83.6 \\
		\bottomrule
	\end{tabular}
 \vspace{-3mm}
	\caption{Study of progressive hybrid targets. We vary the blending ratio $\alpha$ of the \{6$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$, 12$^{\mathrm{th}}$\} blocks.}
  \vspace{-2mm}
	\label{tab:genraw}
\end{table}

\noindent\textbf{Shared decoder or independent decoder.}
By default, \methodName appends independent decoders at intermediate blocks.
We now examine the possibility of employing a single shared decoder for different blocks as shown in~\cref{tab:shared}.
Using a shared decoder significantly degrades the performance, possibly because the feature distributions of different blocks vary significantly from one to another.



\begin{table}[]
	\centering
	\begin{tabular}{c|c|c}
		\toprule
		 Block  & Decoder   & Top-1 Acc. \\
		\midrule
		%1 & 12 & &   82.6  \\
		%\midrule
		 6$^{\mathrm{th}}$, 12$^{\mathrm{th}}$ & Shared     &  82.1  \\
		 6$^{\mathrm{th}}$, 12$^{\mathrm{th}}$ & Independent &    82.9  \\
		\midrule
		6$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$, 12$^{\mathrm{th}}$ &Shared   &  81.4   \\
		6$^{\mathrm{th}}$, 8$^{\mathrm{th}}$, 10$^{\mathrm{th}}$, 12$^{\mathrm{th}}$ & Independent &    83.4   \\
		 %6+8+10+12$\dagger$ & &  83.6  \\
		\bottomrule
	\end{tabular}
 \vspace{-3mm}
	\caption{Comparison of shared or Independent decoder in deep supervision. All supervision is the raw patches except without hybrid targets.}
  \vspace{-2mm}
	\label{tab:shared}
\end{table}


\noindent\textbf{\methodName with different reconstruction targets.}
Differently from reconstruction targets in natural language processing with rich semantics, reconstruction targets in computer vision are low-level pixels~\cite{he2022masked,xie2022simmim}. 
Recent works~\cite{ren2023tinymim,he2022masked,dong2021peco} have explored more semantic reconstruction targets, \textit{e.g.}, CLIP features~\cite{radford2021learning}.
We apply \methodName to different MAE variants whose targets are RGB pixels~\cite{he2022masked}, HOG~\cite{wei2022masked}, DINO features~\cite{caron2021emerging}, discrete tokens generated by perceptual codebook~\cite{dong2021peco} and CLIP features~\cite{radford2021learning}. 
As shown in~\cref{tab:target}, without bells and whistles, \methodName consistently outperforms its counterpart in each comparison.

\noindent\textbf{Training Complexity.} Compared with MAE (on ViT-B), \methodName adopts four lightweight decoders and DeepMIM$^\dagger$ extra utilizes a hybrid target generator (a pre-trained MIM model). In contrast to the original MAE decoder, which uses eight Transformer blocks, each lightweight decoder in our DeepMIM is made up of four blocks. Besides, we suggest to use the hybrid target generator only if a pre-trained MIM model is available. Therefore, The extra cost of DeepMIM$^\dagger$ over DeepMIM is the inference of generating hybrid targets. The pre-training cost of \methodName and \methodName$^\dagger$ are 115 hours and 119 hours respectively, which are slightly higher than 108 hours of MAE under a 1600-epoch pre-training schedule on 32$\times$NVIDIA V100 GPUs.




\begin{table}[]
	\centering
	\begin{tabular}{l|c|c}
		\toprule
		Method    & Target & Top-1 Acc. \\
		\midrule
		MAE  & Pixel & 82.6 \\
		DeepMIM  & Pixel & 83.4 (\textcolor{OliveGreen}{+0.8}) \\
		\midrule
		MAE  & HOG~\cite{wei2022masked} & 83.4 \\
		DeepMIM  & HOG~\cite{wei2022masked} & 83.9 (\textcolor{OliveGreen}{+0.5}) \\
		\midrule
		MAE  & DINO Features~\cite{caron2021emerging} & 83.8 \\
		DeepMIM  & DINO Features~\cite{caron2021emerging} & 84.4 (\textcolor{OliveGreen}{+0.6}) \\
		\midrule
		MAE & Perceptual Codebook~\cite{dong2021peco} & 83.7 \\
		DeepMIM & Perceptual
		Codebook~\cite{dong2021peco} & 84.3 (\textcolor{OliveGreen}{+0.6}) \\
		\midrule
		MAE & CLIP Features~\cite{radford2021learning} & 83.8 \\
		DeepMIM & CLIP Features~\cite{radford2021learning} & 84.8 (\textcolor{OliveGreen}{+1.0}) \\
		\bottomrule
	\end{tabular}
   \vspace{-3mm}
	\caption{Comparison of our method with MAE models with different reconstruction targets. All models are pre-trained for 300 epochs. Our methods bring consistent improvements. }
  \vspace{-2mm}
  %\vspace{-5mm}
	\label{tab:target}
\end{table}



