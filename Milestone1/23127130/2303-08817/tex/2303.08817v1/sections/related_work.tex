\section{Related Work}
\noindent\textbf{Self-supervised learning.}
Self-supervised pretraining aims to induce the model to learn transferable representations via a proxy task that does not require labels.
One approach that has seen widespread adoption is to employ a contrastive loss, modeling the similarity between different views of the same image and the dissimilarity between different images~\cite{wu2018unsupervised,chen2020simple,he2020momentum,clip,xie2021propagate,wu2018unsupervised,wang2021dense}. 
% 
Recently, inspired by the success of BERT~\cite{devlin2018bert} in natural language processing with Masked Language Modeling, masking approaches (particularly Masked Image Modeling) have become a popular alternative for computer vision pretraining. 


\noindent\textbf{Masked image modeling.}
Masked Image Modeling operates by removing a portion of the input image before it is passed to the model and training the model to reconstruct the missing content.
In the same spirit as masked language modeling, MAE~\cite{he2022masked} and SimMIM~\cite{xie2022simmim} employ raw pixels as the targets for reconstruction.
In contrast to the tokens employed as reconstruction targets in natural language processing (which exhibit rich semantics), the pixels in computer vision convey a relatively low-level signal and typically exhibit considerable redundancy.  
To inject more semantics into target tokens, several proposals for the design of an appropriate tokenizer have been put forward. 
BeiT~\cite{bao2021beit} and PeCo~\cite{dong2021peco} employ VQVAE~\cite{ramesh2021zero} to predict discrete visual vocabularies.
MaskFeat~\cite{wei2022masked} employ HOG (local gradient features) as a lightweight tokenizer with minimal parameters. 
iBOT~\cite{zhou2021ibot} and data2vec~\cite{baevski2022data2vec} use the exponential moving average of the model as their tokenizer, inspired by contrastive learning approaches. 
Our method is compatible with previous methods and brings consistent improvements when used in combination with them.


\noindent\textbf{Deep supervision.}
Deep Supervision~\cite{wang2015training,lee2015deeply,zhang2022contrastive,li2022comprehensive} was studied in the past as a tool to assist with training deep nets that could potentially mitigate gradient vanishing/exploding~\cite{hochreiter2001gradient} problems.
GoogleNet~\cite{szegedy2015going} introduces two extra supervision on intermediate layers. 
DSN~\cite{wang2015training} propose to design auxiliary supervision branches at certain intermediate layers.  
With the emergence of batch normalization~\cite{ioffe2015batch} and residual learning~\cite{he2016deep}, gradient vanishing/exploding issues appear to be less prevalent. 
It may be for this reason that deep supervision has received less interest recently.
Differently from prior work, we revisit deep supervision in the context of self-supervised learning and masked image modeling and demonstrate its value in this setting.
