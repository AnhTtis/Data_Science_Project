\section{Discussion}
\label{sec:discuss}
We compare MAE to MAE equipped with \methodName (\methodName-MAE) with respect to the loss, layer similarity and capability of shallower blocks.
We adopt ViT-B for both methods. 
Both pre-training and fine-tuning are conducted on ImageNet-1K training set. 
Unless otherwise specified, we adopt a 300-epoch pre-training schedule.


\begin{figure}
    \includegraphics[width=1.0\linewidth]{./figures/tesear/tesear_Loss.pdf}
    \vspace{-4mm}
    \caption{Comparison of the training loss (left) and the validation loss (right) of the MAE and our \methodName-MAE.
    We show only the reconstruction loss from the last block of the \methodName-MAE.}
    \vspace{-2mm}
    \label{fig:tesear_loss}
\end{figure}
\noindent\textbf{Training and validation loss.}
In \cref{fig:tesear_loss}, we plot the training loss and the validation loss of MAE and DeepMIM-MAE on ImageNet-1K, respectively. 
For a fair comparison, we only show the reconstruction loss associated with the last block of the \methodName-MAE.
We observe that \methodName-MAE lowers the training loss across different training regimes (300 and 800 epochs). 
A recent work~\cite{xie2022data} shows that validation loss in pre-training is a good indicator for how well a model will perform during fine-tuning on downstream tasks. 
% To verify the generalization capability,
For completeness, we also report validation loss, showing the identical phenomenon---\methodName-MAE achieves a lower loss than MAE.

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.4\textwidth]{figures/cka_self.pdf}
    \end{tabular}
    \vspace{-3mm}
    \caption{We use CKA~\cite{kornblith2019similarity} to evaluate the correspondences between the feature from the last block and the features from the intermediate blocks. Blue: MAE; red: DeepMIM-MAE.}
  \label{fig:cka}
\end{figure}

\noindent\textbf{Feature similarity.} 
We employ centered kernel alignment (CKA)~\cite{kornblith2019similarity} to identify correspondences between features produced by the last block and features produced by the intermediate blocks. 
The comparison between MAE and \methodName-MAE is shown in~\cref{fig:cka}.
From the first block to the penultimate block, the CKA score of \methodName-MAE always surpasses that of MAE, suggesting that the features from the intermediate blocks of \methodName-MAE are more discriminative.



\noindent\textbf{Cross feature similarity.}
We use CKA to calculate similarities between features from the \eighth block of MAE and features from all blocks of \methodName-MAE as shown in~\cref{fig:cross_cka_a}. 
We also plot a reversed version in~\cref{fig:cross_cka_b}.
The MAE's intermediate (\eighth block) features exhibit greatest alignment with \methodName-MAE's shallower block features (3$^{\mathrm{th}}$ and 4$^{\mathrm{th}}$).
In contrast, the intermediate (\eighth block) features of \methodName-MAE align more closely with features from the deeper blocks (9$^{\mathrm{th}}$ and \tenth) of MAE. 
This study also indicates that DeepMIM significantly strengths the discriminative power of the features from the shallower blocks. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/cka_mae_8.pdf}
         \caption{CKA similarity between the 8$^{\mathrm{th}}$ layer of MAE and all layers of DeepMIM.}
         \label{fig:cross_cka_a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/cka_ours_8.pdf}
         \caption{CKA similarity between the 8$^{\mathrm{th}}$ layer of DeepMIM and all layers of MAE.}
          \label{fig:cross_cka_b}
     \end{subfigure}
     \vspace{-3mm}
    \caption{Cross feature similarities evaluated by CKA.
    }
  \label{fig:cross_cka}
\end{figure}


\noindent\textbf{Attention Head Diversity.}
\begin{figure}
\begin{tabular}{cc}
\hspace{-2mm}
     \includegraphics[width=0.48\linewidth]{./figures/mae_head.pdf}& 
     \hspace{-3mm}\includegraphics[width=0.48\linewidth]{./figures/deepmim_head.pdf} \\
\end{tabular}
    \caption{Comparison of the head cosine similarity of the MAE (left) and our \methodName-MAE (right) across different layers. Large dots denote the averaged similarities.}
    \label{fig:similarity}
\end{figure}
We calculate the cosine similarities between different attention heads to explore the head diversities. According to~\cite{xie2022revealing}, more diverse heads indicate a stronger capacity for representation. As shown in \cref{fig:similarity}, we plot the cosine similarities of different attention heads across various layers of MAE and \methodName. Our DeepMIM yields more diverse heads in comparison with MAE.

\noindent\textbf{Fine-tuning the last-K blocks.}
To further assess the quality of the features from the shallower blocks, we freeze a subset of shallow blocks and fine-tune the remaining ones. 
For this experiment, we report top-1 accuracy on ImageNet.
As shown in~\cref{fig:partial}, when the number of trainable blocks is varied from 1 (only the last block is trainable) to 12 (all blocks are trainable), \methodName-MAE consistently outperforms MAE by a significant margin.

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.4\textwidth]{figures/partial_k.pdf}
    \end{tabular}
    \vspace{-3mm}
    \caption{When varying the number of trainable blocks from 1 to 12, DeepMIM-MAE consistently outperforms MAE by apparent margins.}
  \label{fig:partial}
\end{figure}

\noindent\textbf{Randomly initialising the last-K blocks when fine-tuning.}
For this experiment, we first randomly initialize the last-K blocks of a pre-trained ViT-B, then the ViT-B is fine-tuned on ImageNet in an end-to-end manner.
\cref{fig:rand} shows a comparison between MAE and \methodName-MAE.
We observe that \methodName-MAE consistently outperforms MAE in each case, demonstrating that good representations at shallow blocks are advantageous for the learning of deeper blocks particularly when they are randomly initialized.


\begin{figure}[t]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.4\textwidth]{figures/rand_k.pdf}
    \end{tabular}
    \vspace{-3mm}
    \caption{End-to-End finetune on ImageNet-1K. 
    Start k layers are pretrained by MAE/Ours, and rest layers are randomly initialized. }
    \vspace{-3mm}
    %\samuel{I dont' understand the second sentence here - could you rephrase?}}
  \label{fig:rand}
\end{figure}