\section{Related Work}

\label{Sec:relatedwork}


\begin{table*}
\centering
\setlength{\tabcolsep}{0.3 em} 
{\renewcommand{\arraystretch}{1.0}
\begin{tabular}{l|c|c|c|c|c|c|c|c} 
\hline
{Dataset} & {Hours} & {Subjects} & \begin{tabular}[c]{@{}c@{}}{Captured}\\{Environment}\end{tabular}& {Acquisition} & \begin{tabular}[c]{@{}c@{}}{Music}\\{Genres}\end{tabular} & \begin{tabular}[c]{@{}c@{}}{Dance}\\{Styles}\end{tabular} & \begin{tabular}[c]{@{}c@{}}{Group}\\{ Dance}\end{tabular} & \begin{tabular}[c]{@{}c@{}}{Ground}\\{Truth}\end{tabular} \\ 
\hline
Dance Melody~\cite{tang2018_dancemelody} & 1.57 & n/a & Lab-control & MoCap & limited & limited & no & 3D Joints \\ 

DanceNet~\cite{zhuang2020_music2dance} & 0.96 & 2 & Lab-control & MoCap & limited & limited & no & 3D Joints \\ 

YT-Dance3D~\cite{sun2020_deepdance} & 5 & n/a & In-the-wild & Fully-automatic & rich & rich & no & 3D Joints \\ 

Dancing2Music~\cite{lee2019_dancing2music} & 71 & n/a & In-the-wild & Fully-automatic & rich & limited & no & 2D Joints \\ 

DanceRevolution~\cite{Dance_Revolution} & 12 & n/a & In-the-wild & Fully-automatic & rich & limited & no & 2D Joints \\ 

PMSD~\cite{perez2021_transflower} & 3.84 & 2 & Lab-control & MoCap &  varied  & limited & no & 3D Joints \\ 

ShaderMotionVR~\cite{perez2021_transflower} & 10.2 & 11 & In-the-wild & VR~Tracking & varied & rich & no & 3D Joints \\ 

AIST++~\cite{li2021AIST++} & 5.2 & 30 & Lab-control & Fully-automatic & limited & varied & no & 3D Mesh \\ 

PhantomDance~\cite{li2022_phantomdance} & 9.6 & 100+ & In-the-wild & Artistic & rich & rich & no & 3D Mesh \\ 
\hline \hline
\textbf{\textsf{AIOZ-GDANCE}} (ours) & \textbf{16.7} & \textbf{4000+} & \textbf{In-the-wild} & \textbf{Semi-automatic} & \textbf{rich} & \textbf{rich} & \textbf{yes} & \textbf{3D Mesh} \\
\hline
\end{tabular}}

\caption{Comparison between music-to-dance datasets.}

\label{tab:datasetCompare} 
\end{table*}



\textbf{Music-Motion Datasets.} Early efforts to create music-motion dataset focus on using motion capture system. Specifically, the authors of~\cite{tang2018_dancemelody} use MoCap to record 3D skeletons from dancers to establish a music-to-dance dataset with four dancing types. It is challenging to collect a large dataset using the MoCap method since it is costly to hire performing dancers and equip the required devices. Notice the limitations of MoCap, the authors in~\cite{lee2019_dancing2music,Dance_Revolution} propose a music-to-dance dataset by crawling internet videos and use OpenPose~\cite{openpose1} to obtain 2D skeleton ground truths. The authors in~\cite{perez2021_transflower} propose a dataset using MoCap and Virtual Reality data. Instead of generating dance ground truths as human skeletons, AIST++ dataset~\cite{li2021AIST++,sun2019deepAISTorigin} obtains 3D motion by fitting SMPL model~\cite{SMPL:2015} from multi-view videos. Unlike~\cite{li2021AIST++} the work in~\cite{li2022_phantomdance} proposes a large-scale music-to-dance dataset from in-the-wild videos. Nevertheless, existing datasets only focus on the single-dance scenario. Therefore, the group motion and the interaction between dancers are not exploited in these datasets. We compare different music-to-dance datasets in Table~\ref{tab:datasetCompare}.

\textbf{Audio-driven Motion Generation.}
Generating natural and realistic human motion from audio is a challenging problem~\cite{joshi2021extensive}. A classical approach is based on the motion graph constructed from a largely captured motion database~\cite{motion_graph1}. To synthesize novel motion, different motion segments are combined by optimizing the transition cost along the path of the graph~\cite{motion_graph2}. Other works apply music-motion similarity matching constraints to further ensure the consistency between motion and music \cite{motion_graph3,motion_graph4,motion_graph5,motion_graph6,moion_graph7,thangstyle}. In recent years, several progresses have been made in the field of music-to-dance motion generation using CNN~\cite{zhuang2020_music2dance, sun2020_deepdance,ye2020_choreonet,ahn2020_autoregressive}, RNN~\cite{tang2018_dancemelody, sun2020_deepdance, alemi2017_groovenet, Dance_Revolution}, GCN~\cite{ferreira2021_learn2dance_gcn, ren2020_ssl_gcn}, GAN~\cite{sun2020_deepdance,lee2019_dancing2music}, or Transformer~\cite{siyao2022_bailando, li2021AIST++, li2022_phantomdance, perez2021_transflower,kim2022brandnew_dance}. Typically, these music-to-dance methods are conditioned on multimodal inputs 
and then generate the future sequence of human poses.
Despite the potential to generate realistic dancing motion from the music, these methods lack the ability to produce coherent movements between multiple dancers altogether. 

\textbf{Multi-person Motion Prediction.} Learning and forecasting the behavior between multiple people has been a longstanding problem~\cite{stergiou2019analyzing,khaire2022deep,nguyen2021graph}. Alahi~\etal~\cite{alahi2014socially} jointly reasons the trajectories of multiple pedestrians and forecasts their destinations in a scene by using Markov chain model. The authors in~\cite{adeli2020socially} combine the visual context of the scene and social interactions between multiple persons to forecast their future motion. Recently, the Multi-Range Transformers proposed by~\cite{wang2021multi} can predict the movements of more than 10 people for social interaction groups. 
Difference from the motion prediction task that uses motion or visual inputs, we aim to generate human dancing motion from the input music.
