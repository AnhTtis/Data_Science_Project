\section{Introduction}

Dancing is an important part of human culture and remains one of the most expressive physical art and communication forms~\cite{FINK2021351, Visualization-folk-dance}. With the rapid development of digital social media platforms, creating dancing videos has gained significant attention from social communities. As the consequence, millions of dancing videos are created and watched daily on online platforms. Recently, studies of how to create natural dancing motion from music have attracted great attention in the research community~\cite{bisig2022generative}. The outcome of dancing generation techniques can be applied to various applications such as animation~\cite{li2021AIST++}, virtual idol~\cite{perez2021_transflower}, metaverse~\cite{survey-dancing-deep-metaverse}, or in dance education~\cite{physic-perform, shi2021application}.


Although there is some progress towards synthesizing realistic dancing motion from music in recent literature~\cite{li2021AIST++,perez2021_transflower,siyao2022_bailando,kim2022brandnew_dance} creating natural 3D dancing motions from the input audio remains an open problem~\cite{li2021AIST++}. This is mainly due to \textit{(i)} the complex structure and the non-linear correlation between continuous human motion and the accompanying music audio, \textit{(ii)} the variety in the repertoire of dancing motions for an expressive  choreography performance, \textit{(iii)} the difficulty in generating long motion sequences, and \textit{(iv)} the complication of capturing the correspondences between the dancing motion and audio such as dancing styles or music rhythms. Furthermore, recent works focus on generating dancing motion for solo dancer~\cite{Dance_Revolution,yalta2019_weaklyrnn, li2021AIST++, ferreira2021_learn2dance_gcn} while producing dancing motion for a group of dancers have not been well-investigated by the community yet. 

Compared to the single dance generation task, the group dance generation poses a more challenging problem~\cite{eisenmann2011creating}. In practice, group dance may contain complicated and different choreographies between attended dancers while also retaining the relationship between the motion and the input music rhythmically. Furthermore, group dance has the communication between dancers through physical contact, hence performing a correlation between motion series in a group is essential and challenging. 
Currently, most of the music-to-dance datasets~\cite{li2021AIST++, perez2021_transflower, tang2018_dancemelody, zhuang2020_music2dance, sun2020_deepdance, li2022_phantomdance} only contain solo dance videos. Hence, they are not able to capture essential aspects that only occurred in group dance scenarios, including multiple-person motions, synchronization, and interaction between dancers.

Since learning to synthesize dancing motion from music is a highly challenging task, one of the primary requirements is the availability of large-scale datasets. Some works employ professional choreographers to obtain music-dance datasets under a highly complex Motion Capture (MoCap) system~\cite{chen2021_choreomaster, ye2020_choreonet, tang2018_dancemelody, li2022_phantomdance}. Although the captured motions are accurate, it is challenging to scale up and increase the diversity of the data with several dance styles and music genres.
To overcome the limitation of the MoCap system, another line of works leverage existing pose estimation algorithm to generate the pseudo-ground truths for in-the-wild dancing videos~\cite{sun2020_deepdance, Dance_Revolution,lee2019_dancing2music}. However, these aforementioned datasets are designed originally for the single motion generation task and provide only paired music and single dancing motion~\cite{li2021AIST++, li2022_phantomdance, perez2021_transflower}, thus they cannot be applied to facilitate generating multiple motions within a group of dancers.




Motivated by these shortcomings, this paper introduces \textsf{AIOZ-GDANCE}, a new large-scale dataset to advance the study of group choreography. Unlike existing choreography datasets that only supports single dancer, our dataset consists of group dance videos. As in Figure~\ref{fig:IntroVis}, our dataset has multiple input modalities  (i.e., video frames, audio) and multiple 3D human mesh ground truths. To annotate the dataset, we introduce a semi-automatic method with humans in the loop to ensure the data quality. 
Using the new dataset, we propose the first strong baseline for group dance generation that can jointly generate multiple dancing motions expressively and coherently. 

Our contributions are summarised as follows:
\begin{itemize}
\item We introduce \textsf{AIOZ-GDANCE}, a new large-scale dataset for group dance generation. To our best knowledge, \textsf{AIOZ-GDANCE} is the \textit{largest} audio-driven group dance dataset.
\item Based on our new dataset, we propose a new method, namely GDanceR, to efficiently generate group dancing motion from the input audio.
\end{itemize}
