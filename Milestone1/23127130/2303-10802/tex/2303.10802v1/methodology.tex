\section{Methodology}
\label{sec:methodology}
\begin{figure*}[t]
    \centering
    % \begin{tikzpicture}
    %     \input{iccv2023AuthorKit/images/architecture}
    % \end{tikzpicture}
    \includegraphics[width=0.85\linewidth]{images/architecture-v9.pdf}
    \caption{Our proposed  \textbf{PASS} consists of three classifiers trained in a round-robin fashion, with one  classifier (\(h_{\gamma_{1}}\) in red) being trained and the other peer classifiers (\(h_{\gamma_{2},\gamma_{3}}\) in green) being used for selecting samples. 
    The training process begins with a warm-up of all classifiers, followed by the sample selection stage. During the selection stage, the peer classifiers calculate the prediction agreement using cosine similarity between their posterior distributions, followed by the Otsu's thresholding~\cite{otsu1979threshold} to automatically find the threshold \(t\) to select the clean $\mathcal{D}_{clean}$ and noisy $\mathcal{D}_{noisy}$ sets. In the training stage, we follow the noisy-label robust training algorithm.}
    % \gustavo{I think we should call the sets clean and noisy instead of reliable and unreliable.  Typo: "non-reliable" should be "unreliable"}.\arpit{updated}}
    % \gustavo{why sampling stage instead of selection stage?  rename unreliable to unreliable? In the sampling stage, why do you have the green trapezoid?}
    % \gustavo{I'd replace the whole bottom part by a simpler "noisy label robust algorithm"}\arpit{updated}
    \label{fig:architecture}
\end{figure*}
% As formerly mentioned, various algorithms have been developed to address the issue of noisy labeling, with a focus on sample selection~\cite{kim2021fine, feng2021ssr, wei2020combating, han2018co, jiang2018mentornet}. Among these, the small-loss approach~\cite{jiang2018mentornet} has gained widespread recognition. However, we propose replacing the small-loss hypothesis with the peer-grading criterion~\cite{artstein2008inter}. \cuong{I think this adds redundency to the paper. We already mention this in the introduction. Here, just go straight-away to explain the method.}
% Our approach involves decision-making based on the level of agreement or disagreement among peer networks, in contrast to the conventional approach that analyzes loss during training. Our algorithm is composed of utilizing one base classifier for training, while keeping the remaining classifiers as peer classifiers at a given time, in a manner similar to round-robin selection. The selection of samples is carried out by assessing the agreement or concordance of posterior probabilities between the peer classifiers. This methodology can be applied to numerous robust classification methods.
\subsection{Problem Definition}\label{problem_definition}

Formally, we define the instance space as \(\mathcal{X}\) and their respective label space as \(\mathcal{Y}\). The training set is represented by \(\Tilde{\mathcal{D}} = \{(\mathbf{x}_{i}, \hat{\mathbf{y}}_{i}) \}^{n}_{i=1}\), where \(\mathbf{x}_{i} \in \mathcal{X} \subseteq \mathbb{R}^{d}\) 
% \cuong{I suggest to replace by \(\mathbb{R}^{d}\) since we do not constrain ourselves to colour images only.} 
represents an instance and \(\hat{\mathbf{y}}_{i} \in \mathcal{Y} = \{ \hat{\mathbf{y}} \in \{0,1\}^{C}: \pmb{1}^{\top} 
\hat{\mathbf{y}} = 1 \}\) denotes the \(C\)-dimensional one-hot vector representation of the corresponding noisy label. 
%Total number of instances \(|\Tilde{D}|\) is \(n\).  
In the conventional classification problem, \(\Tilde{\mathcal{D}}\) is used to train a classifier $h_{\gamma}: \mathcal{X} \to \Delta_{C-1}$, parameterized by $\gamma \in \Gamma \subseteq \mathbb{R}^{D}$ with $\Delta_{C - 1}$ representing the \((C - 1)\)-dimensional probability simplex. In noisy label learning, the noisy label data \(\Tilde{\mathcal{D}}\) is exploited to 
obtain a model $h_{\gamma}$ that can accurately predict the clean labels of samples in a testing set. 
% \cuong{By the way, could we make the notations easily to follow. For example, bold lower case letter denotes vector.}

\subsection{Reliability based Sample Selection}

To enhance the lucidity of our explanation, we commence with delineating our methodology for sample selection. 
%Our sampling approach PASS differs from traditional methods such as the small-loss hypothesis~\cite{han2018co, jiang2018mentornet, li2020dividemix} .
As shown in~\cref{fig:architecture}, our proposed PASS requires multiple classifiers (\(h_{\gamma}\)) to employ the peer-agreement to select reliable samples.
% \cuong{Could we make a figure similar to co-teaching or something alike, then our proposed approach with three classifiers. The current figure looks nice, but lacks the comparison/difference between ours and existing method.}
%for our hypothesis class \(\mathcal{H}\). 
It is noteworthy that all classifiers consistently rotate  between the roles of peers and  training classifier. It is also important to say that we have randomly initialized the classifiers to reduce the chances of confirmation bias~\cite{li2020dividemix}. Another important note is that our sample selection approach can be easily integrated into various noise-label robust algorithms, as we will demonstrate in~\cref{sec:experiments}.


The output of the $k$-th classifier, denoted by \(h_{\gamma_{k}} (\mathbf{x_{i}})\), represents the probability of $\mathbf{y}_{i}$ given $\mathbf{x}_{i}$. To estimate the predictive probability agreement between the peer classifiers (\(h_{\gamma_{l}} \) and \(h_{\gamma_{m}} \)), we compute the cosine similarity for each data point $\mathbf{x}_{i}$, as follows: 
% \sout{The cosine similarity \(\mathbf{s}_i\) between the predictive probability vectors of two classifiers \(h_{\gamma_{l}}\) and \(h_{\gamma_{l}}\) consists of the \emph{predictive probability agreement} that is defined by:}
\begin{equation}\label{eq:cosine_sim}
\begin{aligned}[b]
%&\operatorname{cosine} \operatorname{similarity}\left(p_1\left(y_i \mid x_i\right), p_2\left(y_i \mid x_i\right)\right)\\
& \mathbf{s}_{i} = \frac{h_{\gamma_{l}}^{\top} \left(\mathbf{x}_{i}\right) h_{\gamma_{m}}\left(\mathbf{x}_{i}\right)}{\norm{h_{\gamma_{l}}\left(\mathbf{x}_{i}\right)} \, \norm{h_{\gamma_{m}}\left(\mathbf{x}_{i}\right)}}.
\end{aligned}
\end{equation}
% We then calculate the mean cosine similarity between the posterior probability vectors of all possible pairs of classifiers for each data point $x_i$, denoted by $\bar{s}_i$ \cuong{I thought that we just needed to cluster \(s_{i}\) into reliable and unreliable for the other classifier. I do not understand the purpose of calculating the mean of \(s\)?}. 
% Let $\mathbf{s}$ denotes the vector containing the cosine similarities for all data points in $\Tilde{\mathcal{D}}$, i.e., $\mathbf{s} = [\bar{s}_1, \bar{s}_2, \dots, \bar{s}_n]$. It denotes the probability agreement between the peer-classifiers.
% \begin{figure*}[htbp!]
%         \centering
%         % \begin{tikzpicture}
%         %     \input{iccv2023AuthorKit/images/architecture}
%         % \end{tikzpicture}
%         \includegraphics[width=\linewidth]{iccv2023AuthorKit/images/empirical_graphs_v13.pdf}
%         \caption{The figures presented above demonstrate a comparative analysis of the effectiveness of selecting clean or noisy samples, with reference to three metrics: F1-Score(a), Precision(b), and Ratio of data classified as clean(c). The comparison is made between PASS (grey) with small-loss approach~\cite{jiang2018mentornet} (pink), FINE~\cite{kim2021fine} (yellow) (all on base model DivideMix~\cite{li2020dividemix}), implemented on the CIFAR-100 dataset\cite{krizhevsky2009learning} at \(0.5\) IDN noise rate, as described in Xia et al.~\cite{xia2020part}.
%         % \gustavo{For SSR, how is it possible that it recalls a large proportion of the labelled samples, and precision stays very high?  Shouldn't precision go down as the amount of false positives increase?}
%         % \arpit{Removed smoothness of curves}
%         % \gustavo{Change graph (s) to Ratio of data classified as clean}\arpit{updated}\gustavo{add more results like this in the supp material.}
%         Additional empirical analysis is mentioned in~\cref{sec:appendix_empirical}.}
%         \label{fig:empirical_graph}
% \end{figure*}



\begin{figure*}
    \centering
    % \begin{minipage}[b]{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{iccv2023AuthorKit/images/graph_f1.pdf}
    %     % \caption{F1 Score}
    %     \label{fig:f1}
    % \end{minipage}
    % \begin{minipage}[b]{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{iccv2023AuthorKit/images/graph_precision.pdf}
    %     % \caption{Precision}
    %     \label{fig:precision}
    % \end{minipage}
    % \begin{minipage}[b]{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{iccv2023AuthorKit/images/graph_clean-1.pdf}
    %     % \caption{Ratio of data classified as clean}
    %     \label{fig:ratio}
    % \end{minipage}
    \hspace{-5em}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{iccv2023AuthorKit/images/graph_f1.pdf}
        \begin{tikzpicture}
            \pgfplotstableread[col sep=comma, header=true]{csv/f1.tex} \myTable

            \begin{axis}[
                height = 0.6\linewidth,
                width = 0.8\linewidth,
                % title={MAML - Omniglot - validation},
                % title style={font=\small},
                xlabel={\textnumero~of epochs},
                xlabel style={font=\footnotesize},
                xticklabel style = {font=\footnotesize},
                % xmin=40,
                % xmax=300,
                % restrict x to domain=0:43,
                % xtick={15, 30, 45},
                ylabel={F1-score},
                ylabel style={font=\footnotesize, yshift=-0.5em},
                yticklabel style = {font=\footnotesize},
                % ymin=35,
                % ymax=45,
                % restrict x to domain=0:300,
                scale only axis
            ]
                \addplot[mark=none, MidnightBlue, thick, solid] table[x={epochs}, y={FINE}]{\myTable};
                \addplot[mark=none, BurntOrange, thick, densely dashed] table[x={epochs}, y={SmallLoss}]{\myTable};
                \addplot[mark=none, BrickRed, solid, thick, dashdotted] table[x={epochs}, y={PASS}]{\myTable};
            \end{axis}
        \end{tikzpicture}
        \caption{F1 Score}
        \label{fig:f1}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{iccv2023AuthorKit/images/graph_precision.pdf}
        \begin{tikzpicture}
            \pgfplotstableread[col sep=comma, header=true]{csv/precision.tex} \myTable

            \begin{axis}[
                height = 0.6\linewidth,
                width = 0.8\linewidth,
                % title={MAML - Omniglot - validation},
                % title style={font=\small},
                xlabel={\textnumero~of epochs},
                xlabel style={font=\footnotesize},
                xticklabel style = {font=\footnotesize},
                % xmin=40,
                % xmax=300,
                % restrict x to domain=0:43,
                % xtick={15, 30, 45},
                ylabel={Precision},
                ylabel style={font=\footnotesize, yshift=-0.5em},
                yticklabel style = {font=\footnotesize},
                % ymin=35,
                % ymax=45,
                % restrict x to domain=0:300,
                scale only axis
            ]
                \addplot[mark=none, MidnightBlue, thick, solid] table[x={epochs}, y={FINE}]{\myTable};
                \addplot[mark=none, BurntOrange, thick, densely dashed] table[x={epochs}, y={SmallLoss}]{\myTable};
                \addplot[mark=none, BrickRed, solid, thick, dashdotted] table[x={epochs}, y={PASS}]{\myTable};
            \end{axis}
        \end{tikzpicture}
        \caption{Precision}
        \label{fig:precision}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{iccv2023AuthorKit/images/graph_clean-1.pdf}
        \begin{tikzpicture}
            \pgfplotstableread[col sep=comma, header=true]{csv/clean.tex} \myTable

            \begin{axis}[
                height = 0.6\linewidth,
                width = 0.8\linewidth,
                % title={MAML - Omniglot - validation},
                % title style={font=\small},
                xlabel={\textnumero~of epochs},
                xlabel style={font=\footnotesize},
                xticklabel style = {font=\footnotesize},
                % xmin=40,
                % xmax=300,
                % restrict x to domain=0:43,
                % xtick={15, 30, 45},
                ylabel={Ratio of clean data},
                ylabel style={font=\footnotesize, yshift=-0.5em},
                yticklabel style = {font=\footnotesize},
                % ymin=35,
                % ymax=45,
                % restrict x to domain=0:300,
                legend entries={FINE, Small loss, PASS},
                legend style={draw=none, font=\scriptsize},
                legend image post style={scale=1.},
                legend cell align={left},
                legend pos=north east,
                scale only axis
            ]
                \addplot[mark=none, MidnightBlue, thick, solid] table[x={epochs}, y={FINE}]{\myTable};
                \addplot[mark=none, BurntOrange, thick, densely dashed] table[x={epochs}, y={SmallLoss}]{\myTable};
                \addplot[mark=none, BrickRed, solid, thick, dashdotted] table[x={epochs}, y={PASS}]{\myTable};

                % plot the ideal noise ratio
                \addplot[mark=none, style={densely dashed}] coordinates {
                    (50, 0.5)
                    (300, 0.5)
                };
                \node[above, color=Black, rotate=0, ] at (80, 0.5) {\scriptsize{ideal ratio}};
            \end{axis}
        \end{tikzpicture}
        \caption{Ratio of data classified as clean}
        \label{fig:ratio}
    \end{subfigure}
    \caption{The figures presented above demonstrate a comparative analysis of the effectiveness of selecting clean or noisy samples, with reference to three metrics: \protect\subref{fig:f1}~F1-score, \subref{fig:precision}~precision, and \subref{fig:ratio}~ratio of data classified as clean. The comparison is made between PASS (red) with small-loss approach~\cite{jiang2018mentornet} (yellow), FINE~\cite{kim2021fine} (blue) (all on base model DivideMix~\cite{li2020dividemix}), implemented on the CIFAR-100 dataset\cite{krizhevsky2009learning} at \(0.5\) IDN noise rate, as described in~\cite{xia2020part}.
%         % \gustavo{For SSR, how is it possible that it recalls a large proportion of the labelled samples, and precision stays very high?  Shouldn't precision go down as the amount of false positives increase?}
%         % \arpit{Removed smoothness of curves}
%         % \gustavo{Change graph (s) to Ratio of data classified as clean}\arpit{updated}\gustavo{add more results like this in the supp material.}
        Additional empirical analysis is mentioned in~\cref{sec:appendix_empirical}.}
        \label{fig:empirical_graph}
\end{figure*}








% \begin{tikzpicture}
% \begin{axis}[
%     xlabel=Epoch,
%     ylabel=Accuracy,
%     legend style={at={(0.5,-0.2)},anchor=north},
%     xmin=50, xmax=300,
%     ymin=0, ymax=1.1,
%     ytick={0,0.2,0.4,0.6,0.8,1},
%     xtick={50,100,150,200,250,300},
%     grid=major,
%     grid style={dashed,gray!30},
% ]

% \addplot[color=gray, solid, thick] table[x=epoch,y=PASS,col sep=comma, row sep =crcr] {iccv2023AuthorKit/images/empirical_results.csv};
% \addplot[color=golden, densely dotted, thick] table[x=epoch,y=FINE,col sep=comma, row sep =crcr] {iccv2023AuthorKit/images/empirical_results.csv};
% \addplot[color=pink, dashed, thick] table[x=epoch,y=Small-Loss,col sep=comma, row sep=crcr] {iccv2023AuthorKit/images/empirical_results.csv};

% \legend{PASS, FINE, Small-Loss}
% \end{axis}
% \end{tikzpicture}
% \begin{remark}


% \begin{tikzpicture}
% \begin{axis}[
%     xlabel=Epoch,
%     ylabel=Accuracy,
%     legend style={at={(0.5,-0.2)},anchor=north},
%     xmin=50, xmax=300,
%     ymin=0, ymax=1.1,
%     ytick={0,0.2,0.4,0.6,0.8,1},
%     xtick={50,100,150,200,250,300},
%     grid=major,
%     grid style={dashed,gray!30},
% ]

% \addplot[color=gray, solid, thick] table[x index=0,y index=2,col sep=comma] {empirical_results.csv};
% \addplot[color=golden, densely dotted, thick] table[x index=0,y index=1,col sep=comma] {empirical_results.csv};
% \addplot[color=pink, dashed, thick] table[x index=0,y index=3,col sep=comma] {empirical_results.csv};

% \legend{PASS, FINE, Small-Loss}
% \end{axis}
% \end{tikzpicture}


\hypothesis{noisy label data samples are likely to have small predictive probability agreement   values in~\eqref{eq:cosine_sim}. %diverge from the assumed distribution, consequently causing a lack of consensus among peer classifiers regarding their classification. 
The reason being that the different influence of noisy label data samples on predictive probabilities can result in disparate outcomes between peer classifiers.  % \gustavo{not sure what is meant by common prior - same model architecture? similar initialisation?}\arpit{updated},
On the other hand, clean data are likely to have high predictive probability agreement in~\eqref{eq:cosine_sim}.  According to the theory of peer classifier agreement~\cite{he2022towards, steinhardt2016avoiding,ramesh2022peer}, it is advisable that clean data should be selected based on high peer classifier agreement, but noisy data should be cautiously selected based on low peer classifier agreement.}\\
% \vspace{-0.1em}
% \end{remark}
Leveraging the aforementioned remark, we partition the training set $\Tilde{\mathcal{D}}$ into the clean set $\Tilde{\mathcal{D}}_{\text{clean}}$ and noisy set  $\Tilde{\mathcal{D}}_{\text{noisy}}$ using the cosine similarity in~\eqref{eq:cosine_sim}. This partition can be achieved by any thresholding or clustering algorithm, with the clean set comprising data points exhibiting high cosine similarity values, and the noisy set comprising data points exhibiting low cosine similarity values. In our case, we utilize the global thresholding technique known as the Otsu's algorithm~\cite{otsu1979threshold}. When compared with other clustering and thresholding algorithms (K-Means~\cite{ikotun2022k} and Gaussian Mixture Model (GMM)~\cite{reynolds2009gaussian}), Otsu's algorithm~\cite{otsu1979threshold} is advantageous as it provide PASS with the best performance, as shown in~\cref{sec:ablation}.
% Otsu's thresholding~\cite{otsu1979threshold}  stands out as a notably straight-forward and advantageous global thresholding approach in instances. 
%The rationale for utilizing this algorithm over other clustering algorithms are expounded in the~\cref{sec:ablation}.
% \gustavo{can we provide a short summary for the decision in favor of Otsu's?}.
% \paragraph{Otsu's algorithm:} 
% \cuong{I suggest to remove the part about Otsu's algorithm since it adds redundency to the paper} 
This thresholding algorithm automatically estimate an optimal threshold ($t$) to divide the data samples into two classes, namely, clean ($\mathbf{s_i} \geq t$) and noisy data ($\mathbf{s_i} < t$).
% \sout{We use Otsu's thresholding~\cite{otsu1979threshold} for sample selection in noisy label robust training algorithms for categorising clean and noisy data sets using cosine similarity~\eqref{eq:cosine_sim}. The automatic estimation of the optimal threshold ($t$), divides 
% data samples into two classes, namely, clean ($\mathbf{s_i} \geq t$) and noisy data ($\mathbf{s_i} < t$).} 
Please see~\cref{sec:appendix_otsu} for more details about Otsu's algorithm.
Once the clean and noisy samples have been selected, we employ a noisy label robust training algorithm from the literature. Our employed algorithms to test the efficacy of our proposed approach of sampling are described in~\cref{sec:experiments}. Our training procedure is succinctly described in~\cref{algorithm:Proposed_Algo} and visually portrayed in~\cref{fig:architecture}.


\subsection{Empirical Analysis of Sample Selection}
\label{subsec:sample_selection}
To empirically analyse PASS, we focus on the challenging IDN synthetic noise at \(50\%\) noise rate~\cite{xia2020part} on CIFAR-100~\cite{krizhevsky2009learning}.
\cref{fig:empirical_graph} shows three plots to measure the performance of the classification  of clean samples, namely: (a) F1 Score, (b) Precision, and (c) Ratio of data classified as clean.
We use these plots to compare our PASS against the small-loss hypothesis~\cite{li2020dividemix}, and FINE~\cite{kim2021fine} (all utilizing  DivideMix~\cite{li2020dividemix} as the noisy label robust training algorithm). We have only considered the methods of sample selection, and have not incorporated the methods that involve sample relabeling within this analysis.
% \rafa{Should you add more details about how you defined this experiment in terms of the data you are using? Or perhaps this is a well-known graph for the community?}

\cref{fig:ratio} displays the proportion of data classified as clean (by the model). 
It is evident that the small-loss~\cite{li2020dividemix} hypothesis and FINE~\cite{kim2021fine} consistently yields a ratio of around $0.70 - 0.65$ during the training process, while our approach maintains a ratio of around $0.50-0.45$. 
As we are aware from the setup, the optimal rate (ideal ratio) should be $\approx$ $0.50$. This indicates that our approach is more capable of identifying the correct proportion of noisy label samples for the IDN at \(50\%\) on CIFAR-100. Further results in~\cref{sec:appendix_empirical} show that our method can identify the correct proportion of noisy label samples for other IDN settings \((20\%, 40\%)\).
% \gustavo{COMMENT: if we have more results in the appendix, here is the place to say that "further results in the appendix show that our method can identify the correct proportion of noisy label samples for other IDN settings (20\%, 40\%)".}. 
However, that proportion alone does not ensure that the clean samples are accurately selected. 
Hence, we have also calculated the F1 score (\cref{fig:f1}) and precision (\cref{fig:precision}), which show superior results by our approach. 
More specifically, \cref{fig:f1} shows that our strategy exhibits a consistently superior F1 score compared to other approaches, achieving the final result of $0.87$, which is better than other approaches, such as small-loss and FINE~\cite{kim2021fine} with similar result of $0.75$. 
%Under the aforementioned noise settings, both models (small-loss and FINE) perform similarly.
Another important comparison measure is precision. PASS shows very high precision of more than $0.96$, whilst small-loss~\cite{li2020dividemix} and FINE~\cite{kim2021fine} show much smaller precision values of around $0.72$.
% \rafa{I'd say that the message of this sentence slightly contrast with the arguments you've added to section Discussions -> } 
This empirical analysis suggests that our method is more efficacious at correctly identifying positive and negative samples from the training set than other competing approaches. 



%Furthermore, our approach also outperforms the other mentioned strategy in terms of the precision score in~\cref{fig:empirical_graph}(b), where PASS and SSR score settles at around $0.96$ and $0.94$ respectively, while the small-loss and FINE approach only manages to achieve around $0.72$. 
%This means that our proposed method is much more effective at identifying true reliable samples.
% \gustavo{I think we can show curves for some of the feature-based methods, like FINE?  and KNN-based (also feature-based), like SSR?}






\begin{algorithm}[htbp!]
    \caption{Sample selection and training of noise-robust classifiers 
    % \gustavo{the round-robin training of the three classifiers is not shown here. Replace reliable and unreliable by clean and noisy.}\arpit{updated}
    % \gustavo{The algorithm is not being cited in the text.}\arpit{Updated}
    }
    \label{algorithm:Proposed_Algo}
    \begin{algorithmic}[1]
        \Procedure{PASS}{$\Tilde{\mathcal{D}}, \Psi, E$}
            % long comments to describe input arguments
            \LComment{\(\Tilde{\mathcal{D}}=\{(\mathbf{x}_i, \hat{\mathbf{y}}_i)\}_{i=1}^{n}\): noisy-labeled dataset}
            \LComment{\(E\): total number of epochs}
            \LComment{\(\Psi\): training algorithm to use, e.g., DivideMix}
            % \State \(h_{n}(Y | X) \gets\) \Call{Warmup}{$\Tilde{\mathcal{D}}$} \Comment{Warm-up training of \(n \in I\) number of classifiers clean-label classifiers on noisy dataset}
            \For{\(e = 1:E\)}
                \LComment{Select clean/noisy samples for classifier \(h_{\gamma_{1}}\)}
                % \LComment{\gustavo{you need to give a set of peers, right? Not just the $k^{th}$ one.}}
                \State \(\mathcal{L}_{j}, \mathcal{U}_{j} \gets \) \Call{Sample-Selection}{$ \Tilde{\mathcal{D}}, h_{\gamma_{2}}, h_{\gamma_{3}}$}
                % \LComment{Apply Gaussian mixture model on loss values and filter out clean and noisy with a threshold on the likelihood} 
                \LComment{\(\mathcal{L}_j\) are clean data}
                \LComment{\(\mathcal{U}_j\) are noisy data}
                % \LComment{What is $q(.)$?}
                \State \(\mathsf{L}_{(j)}^{\mathrm{(\Psi)}} \gets\) \Call{ Loss}{$ \mathcal{L}_{j}, \mathcal{U}_{j}, h_{\gamma_{1}}$}
                % \LComment{\cuong{this is really unfair. I believe that you should select samples for all classifiers, then update them at the same time, not one by one like this.}}
                % \LComment{\gustavo{what is \Call{\(\Psi\) Loss}? Shouldn't it only be \Call{Loss}{$ \mathcal{L}_{j}, \mathcal{U}_{j}, h_{\gamma_{1}}$}}\arpit{updated}}
                %\Comment{Calculate training loss using given \(\Psi\)}
                \State Update %model parameters by minimizing the calculated loss 
                \(\Psi\)
                % \State Repeat by selecting \(h_{\gamma_{2}}\) and peers \(h_{\{\gamma_{1}}, h_{\gamma_{3}\}}\)
                % \State Repeat by selecting \(h_{\gamma_{3}}\) and peers \(h_{\{\gamma_{1}}, h_{\gamma_{2}\}}\)
                % \Statex
                \State Repeat for \(h_{\gamma_{2}}\) and \(h_{\gamma_{3}}\)
              \EndFor
            % \State \Return \(q_{i}(Y|X)\) %\Comment{clean-label classifier}
            \State \Return \(h_{\gamma_{1}}, h_{\gamma_{2}}, h_{\gamma_{3}}\)
        \EndProcedure
        
        \Statex
        
        \Function{Sample-Selection}{$\Tilde{\mathcal{D}}, h_{\gamma_{j}},h_{\gamma_{k}}$}
            \State \(\mathbf{s} \gets \pmb{0}\) \Comment{Vector to store cosine similarity}
            \For{each each \((\mathbf{x}_{i}, \hat{\mathbf{y}}_{i})\) \text{ in } \(\Tilde{\mathcal{D}}\)}
                \State \(\mathbf{s}_{i} \gets \) \Call{Cosine-Similarity}{$h_{\gamma_{j}}(\mathbf{x}_{i}), h_{\gamma_{k}}(\mathbf{x}_{i})$}
            \EndFor
            % \State $\mathcal{P}(Y | X) \gets$ \Call{CalculatePosterior}{$\Tilde{\mathcal{D}}, h_{\{\gamma_{2},\gamma_{3}\}}(Y | X)$}
            % \LComment{Posterior probability of all peer classifiers}
            % \State $\mathbf{s} \gets$ \Call{CalculateCosineSimilarity}{$\mathcal{P}(Y | X)$}
            % \LComment{Cosine similarity between posteriors}
            % \LComment{Apply Otsu algorithm on \(\mathbf{s}\) to  find threshold t}
            \State $t \gets$ \Call{Otsu}{$\mathbf{s}$} \Comment{Find thresholding with Otsu}
            % \LComment{\gustavo{you need to say that you'll do that for all samples in $\mathcal{D}$}}
            \State \(\mathcal{D}_{\text{clean}} \gets \varnothing\), \(\mathcal{D}_{\text{noisy}} \gets \varnothing\)
            \For{ each \((\mathbf{x}_{i}, \hat{\mathbf{y}}_{i})\) \text{ in } \(\Tilde{\mathcal{D}}\)}
                \If{\(\mathbf{s}_i \geq t\)} \Comment{High agreement \(\to\) clean}
                    \State $\mathcal{D}_{\mathrm{clean}} \gets \mathcal{D}_{\text{clean}} \cup {(x_i, \hat{y}_i)} $
                \Else \Comment{Low agreement \(\to\) noisy}
                    \State $\mathcal{D}_{\mathrm{noisy}} \gets \mathcal{D}_{\mathrm{noisy}} \cup {(x_i, \hat{y}_i)}$
                \EndIf
            \EndFor
            \State \Return $\mathcal{D}_{\mathrm{clean}}, \mathcal{D}_{\mathrm{noisy}}$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
