\section{Experiments} \label{sec:experiments}
This section presents the outcomes of an extensive experimental investigation on the benchmark dataset CIFAR-100~\cite{krizhevsky2009learning}, with various IDN~\cite{xia2020part} noise rates, as well as on several real-world datasets, such as CIFAR-N~\cite{wei2022learning}, Animal-10N~\cite{song2019selfie}, Red Mini-Imagenet from CNWL~\cite{jiang2020beyond}, Clothing1M~\cite{xiao2015learning}, Mini-Webvision~\cite{li2017webvision} and Imagenet~\cite{deng2009imagenet}. We prioritize small-loss based models but also use others for experiments to showcase the adaptability of PASS. 
The datasets mentioned before are thoroughly explained in~\cref{subsec:dataset}. Additionally, all models and their corresponding parameters are described in~\cref{subsec:implementation}. These analytical procedures and the results obtained from our methodology are assessed with several SOTA models in both IDN benchmarks and real-world datasets, as described in~\cref{subsec:baseline}. 



\subsection{Dataset}\label{subsec:dataset}
%CIFAR10/100
 In the CIFAR-100~\cite{krizhevsky2009learning} dataset there are \(50,000\) training images and \(10,000\) testing images with each image having a size of \(32 \times 32 \times 3\) pixels and \(100\) class-balanced categories. This dataset do not possess label noise by default, so we follow Xia et al.'s~\cite{xia2020part} configuration by introducing IDN with noise rates of \(\{0.2, 0.3, 0.4, 0.45\text{ and } 0.5\}.\)

%CIFARN
CIFAR-N~\cite{wei2022learning} is created by relabeling CIFAR-10N/CIFAR-100N~\cite{wei2022learning} dataset with Amazon Mechanical Turk (M-Turk)~\cite{ipeirotis2010quality}  labeling of the original CIFAR-10/CIFAR-100~\cite{krizhevsky2009learning} datasets.
%which involves annotations from human workers. 
The CIFAR-10N dataset includes five distinct noise rate options, from which we have selected the \textit{"worst"} version
% \rafa{is there a better word for describing it? (e.g., the most challenging (or is this normal for the community?)} 
(noise rate of \(40.21\)\%). In the CIFAR-100N dataset, we considered \textit{fine} labels with overall noise level of \(40.20\)\%.

%Animal-10N
Animal-10N dataset is proposed by Song et al.~\cite{song2019selfie}, and is a real-world dataset that includes \(10\) animal categories, with \(5\) pairs of animals having similar appearances, \textit{such as chimpanzee and orangutan}. The dataset has an estimated label noise rate of \(8\%\), and it comprises \(50,000\) training images and \(10,000\) test images. Data augmentation is not utilized in the dataset, making it consistent with the setup proposed in Song et al.'s work~\cite{song2019selfie}.
 
 %Redmini-Imagenet
Red Mini-Imagenet from CNWL~\cite{jiang2020beyond} is a real-world dataset, which is primarily established for scrutinizing the impact of label noise rates on image categorization models. This dataset includes \(600\) images from  ImageNet~\cite{deng2009imagenet} in each of its \(100\) categories. To ensure an equitable comparison, the images have been resized to \(32 \times 32 \times 3\) pixels. In addition, it incorporates images with noise rates ranging from \(0\% \text{ to } 80\%\). We concentrated on noise rates of \(40\%, 60\%, \text{ and } 80\%\) to maintain consistency with existing literature~\cite{Garg_2023_WACV, xu2021faster, yao2021instance, cordeiro2021propmix}.

%Clothing1M
Clothing1M~\cite{xiao2015learning} is a real dataset consisting of \(1\) million training images of clothes from \(14\) distinct online shopping website categories. There is an estimated \(38.5\%\) noise level in this dataset's labels, which are derived from surrounding text. To ensure comparability, we employ the downsized images of size \(256 \times 256\) pixels, as per the prevalent format in the previous works~\cite{Garg_2023_WACV, Iscen_2022_CVPR, han2019deep, li2020dividemix, cordeiro2021propmix}. This dataset includes \(50k, 14k, \text{ and } 10k\) manually authenticated training, validation, and testing samples, respectively. We exclude the clean training and validation sets during training. We only use the clean testing set for evaluation, following the literature~\cite{Garg_2023_WACV, li2020dividemix, nishi2021augmentation, cordeiro2021propmix}.

%mini-Webvision
Mini-WebVision~\cite{li2020dividemix} consists of \(65,944\) internet-collected images from the initial \(50\) categories of the Webvision dataset~\cite{li2017webvision}, with images reduced to \(256 \times 256\) pixels. For validation, the equivalent \(50\) categories from the ILSVRC12~\cite{deng2009imagenet} dataset are employed. 
%These original dataset are picked for assessment in this investigation to compare the performance of PASS (ours) with SOTA models in handling noisy label learning.


\subsection{Implementation}\label{subsec:implementation}
In this study, all methods are implemented utilizing the PyTorch framework and executed on the NVIDIA RTX 3090 computing platform. Baseline models are selected based on their accuracy and compatibility with the dataset under consideration. For the CIFAR-100~\cite{krizhevsky2009learning} (IDN~\cite{xia2020part}), we use the InstanceGM~\cite{Garg_2023_WACV} and DivideMix~\cite{li2020dividemix} models, both of which demonstrated high levels of performance. Additionally, the DivideMix~\cite{li2020dividemix} model was utilized for the CIFAR-N dataset~\cite{wei2022learning}, while the Animal-10N dataset~\cite{song2019selfie} was trained using the SSR~\cite{feng2021ssr} model. In the case of the Red Mini-Imagenet dataset~\cite{jiang2020beyond}, we employed a hybrid approach using FaMUS~\cite{xu2021faster} with two evaluation versions, one with and one without DINO self-supervision~\cite{caron2021emerging}. Moreover, the Clothing1M dataset~\cite{xiao2015learning} employed the AugDesc~\cite{nishi2021augmentation} model. The Mini-Webvision dataset~\cite{li2020dividemix} was trained using the C2D~\cite{zheltonozhskii2022contrast} and validation is performed on Imagenet~\cite{krizhevsky2012imagenet}. Unless otherwise stated, default hyperparameters and network architectures are as specified in the original papers. %are utilized during training.

\subsection{Comparisons on Benchmarks}\label{subsec:baseline}
In~\cref{subsubsec:idn_benchmark}, we perform a comparison study on the IDN benchmarks, and in~\cref{subsubsec:real-world}, we evaluate our method on real-world noisy label benchmarks.


\subsubsection{IDN Benchmark}\label{subsubsec:idn_benchmark}
\begin{table}[t]
        \caption{Test accuracy (\%) on CIFAR-100~\cite{krizhevsky2009learning} subject to various IDN noise rates~\cite{xia2020part}. The results were obtained from~\cite{Garg_2023_WACV}, wherein the base model (\(*\)) results are denoted in \textit{italics}. \textbf{PASS} represents our approach with mentioned baselines.}
        \label{tab:cifar}
        \centering
        \addtolength{\tabcolsep}{-0.2em}
        \begin{tabular}{l c c c c c}
        \toprule
        & \textbf{0.20} & \textbf{0.30} & \textbf{0.40} & \textbf{0.45} & \textbf{0.50} \\
        \midrule
        CE~\cite{yao2021instance}  & 30.42 & 24.15 & 21.45 & 15.23  & 14.42\\
        % Mixup~\cite{zhang2017mixup}  & 32.92 & 29.76 & 25.92 & 23.13 & 21.31\\
        % Forward~\cite{patrini2017making}  & 36.38 & 33.17 & 26.75 & 21.93 & 19.27\\
        % T-Revision~\cite{xia2019anchor}  & 37.24 & 36.54 & 27.23 & 25.53 & 22.54\\
        % Reweight~\cite{liu2015classification}  & 36.73 & 31.91 & 28.39 & 24.12 & 20.23\\
        PTD-R-V~\cite{xia2020part}  & 65.33 & 64.56\ & 59.73 & \_ & 56.80 \\
        % Decoupling~\cite{malach2017decoupling}  & 36.53 & 30.93 & 27.85 & 23.81 & 19.59\\
        Co-teaching~\cite{han2018co}  & 37.96 & 33.43 & 28.04 & 25.60 & 23.97 \\
        MentorNet~\cite{jiang2018mentornet}  & 38.91 & 34.23 & 31.89 & 27.53 & 24.15\\
        %CausalNL~\cite{yao2021instance}  & 41.47 & 40.98 & 34.02 & 33.34 & 32.13\\
        % HOC~\cite{zhu2021clusterability}  & 68.82 & \_ & 62.29 & \_ & \_\\
        %CAL~\cite{Zhu_2021_CVPR}  & 69.11 & \_ & 63.17 & \_ & \_\\
        kMEIDTM~\cite{cheng2022instance}  & 69.16 & 66.76 & 63.46 & \_ & 59.18\\
        \midrule
        DivideMix*~\cite{li2020dividemix}  & \textit{77.07} & \textit{76.33} & \textit{70.80} & \textit{57.78}  & \textit{58.61}\\
        \rowcolor{Gray!25} \textbf{DivideMix-PASS}  & \textbf{77.41} & \textbf{76.58} & \textbf{75.07} & \textbf{72.91}  & \textbf{72.27}\\
        \midrule
        InstanceGM*~\cite{Garg_2023_WACV}  & \textit{79.69} & \textit{79.21} & \textit{78.47} & \textit{77.49}  & \textit{77.19}\\
        \rowcolor{Gray!25} \textbf{InstanceGM-PASS}  & \textbf{81.02} & \textbf{80.33} & \textbf{79.28} & \textbf{78.69}  & \textbf{78.26}\\
        \bottomrule
        \end{tabular}
\end{table} 
In \cref{tab:cifar}, a comparative analysis is presented showcasing the performance of the proposed PASS against various SOTA techniques on the CIFAR-100~\cite{krizhevsky2009learning} IDN benchmark~\cite{xia2020part}. Notably, our method demonstrates significant improvements on this dataset across various IDN noise rates ranging from \(20\%\) to \(50\%\), when employing  InstanceGM~\cite{Garg_2023_WACV} and DivideMix~\cite{li2020dividemix} models as baselines. As the baseline models represent the primary reference for PASS, it is critical to compare the performance of postulated PASS method to these models to showcase the efficacy and adaptability of our work. Compared to current SOTA methods on this benchmark (InstanceGM~\cite{Garg_2023_WACV} and DivideMix~\cite{li2020dividemix}), our approach outperforms these models by approximately between \(1.2\%\) to \(14\%\) at \(0.50\) noise rate.

\subsubsection{Real-World noisy label benchmarks}\label{subsubsec:real-world}
In~\cref{table:RedMini,table:Animal10N,table:cifar100N,table:clothing1M,table:miniWebvision}, we showcase the results of our proposed method on  CIFAR-N~\cite{wei2022learning}, Animal-10N~\cite{song2019selfie}, Red Mini-Imagenet~\cite{jiang2020beyond}, Clothing1M~\cite{xiao2015learning}, Mini-Webvision~\cite{li2020dividemix} and Imagenet~\cite{deng2009imagenet}. Overall, our approach demonstrates superior performance or competitiveness with the current SOTA models employing various baselines for both large-scale web-crawled dataset and small-scale human-annotated noisy dataset. Additionally, our method exhibits a high degree of flexibility and can be easily integrated into existing models. 

\begin{table}
    \centering
    \caption{Test accuracy (\%) on CIFAR-N~\cite{wei2022learning}, where results of other models are from~\cite{wei2022learning}. The \textbf{PASS} base model  is DivideMix~\cite{li2020dividemix} (\(*\) with results in \textit{italics}).}
    \label{table:cifar100N}
    \begin{tabular}{l c c}
        \toprule
        \bfseries Method & \bfseries CIFAR10N-W & \bfseries CIFAR100N-F  \\
        \midrule
        CE~\cite{liu2022robust} & 77.69 & 55.50\\
        CAL~\cite{zhu2021clusterability} & 85.36 & 61.73\\
        ELR~\cite{liu2020early} & 91.09 & 66.72\\
        SOP+~\cite{liu2022robust} & 93.24 & 67.81\\
        %ProMix  & \textbf{96.16} & \textbf{73.39}\\
        \midrule
        DivideMix~\cite{li2020dividemix} & \textit{92.56} & \textit{71.13}\\
        \rowcolor{Gray!25} \textbf{DivideMix-PASS}  & \textbf{94.02} & \textbf{72.03} \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]
        \centering
        \caption{Test accuracy (\%) of various approaches on Animal-10N~\cite{song2019selfie} with baseline (\(*\) and outcomes in \textit{italics}) SSR~\cite{feng2021ssr}. The other results are  from~\cite{feng2021ssr}. \textbf{PASS} represents our approach with baseline SSR~\cite{feng2021ssr}.}
        \label{table:Animal10N}
        \begin{tabular}{l c}
            \toprule
            \bfseries Method & \bfseries Test Accuracy (\%) \\
            \midrule
            CE~\cite{zhang2021learning} & 79.4 \\
            SELFIE~\cite{song2019selfie} & 81.8 \\
            PLC~\cite{zhang2021learning} & 83.4 \\
            Nested-CE~\cite{chen2021boosting} & 84.1 \\
            Jigsaw-ViT~\cite{chen2023jigsaw} & \textbf{89.0}\\
            \midrule
            SSR*~\cite{feng2021ssr} & \textit{88.5}\\
            \rowcolor{Gray!25} \textbf{SSR-PASS} & \textbf{89.0} \\
            \bottomrule
        \end{tabular}
    \end{table}
\cref{table:cifar100N,table:Animal10N} present the results obtained by PASS with their corresponding baselines on CIFAR-N~\cite{wei2022learning} and Animal-10N~\cite{song2019selfie}, respectively. 
It is noteworthy that the results are shown to improve all baselines, exhibiting competitive performance across the entire dataset. 
\cref{table:RedMini} reports the results on Red Mini-Imagenet~\cite{xu2021faster} using our PASS method with baseline model FaMUS~\cite{xu2021faster} in two different setups: 1) without pre-training (upper section of the table), and 2) with self-supervised (SS) pre-training (lower section of the table). The SS pre-training relies on DINO~\cite{caron2021emerging} using the unlabeled Red Mini-Imagenet dataset to ensure a fair comparison with InstanceGM~\cite{Garg_2023_WACV} and PropMix~\cite{cordeiro2021propmix}, which also incorporate a similar SS pre-training technique. The results demonstrate that PASS can effectively improve performance, by achieving SOTA outcomes on Red Mini-Imagenet~\cite{xu2021faster}.
% \gustavo{COMMENT: please be careful to show the tables in correct order.  Here, we should have Table 2, not Table 4.} 
\begin{table}[t]
    \centering
    \caption{
    Test accuracy (\%) on Red Mini-Imagenet (CNWL)~\cite{jiang2020beyond}. Additional model results are  from~\cite{Garg_2023_WACV}. We show \textbf{PASS} (ours) using baseline FaMUS~\cite{xu2021faster} (\(*\) and results in \textit{italics}) without and with self-supervision (SS)~\cite{caron2021emerging}.
    }
    \label{table:RedMini}
    \begin{tabular}{l p{3em} p{3em} p{3em} l}
        \toprule
        \multirow{2}{*}{\bfseries Method} & \multicolumn{3}{c}{\bfseries Noise rate} \\
        \cmidrule{2-4}
         & \bfseries 0.4 & \bfseries 0.6 & \bfseries 0.8 \\
        \midrule
        CE~\cite{xu2021faster} &  42.70 & 37.30 & 29.76 \\
        MixUp~\cite{zhang2017mixup} &  46.40 & 40.58 & 33.58 \\
        DivideMix~\cite{li2020dividemix} & 46.72 & 43.14 & 34.50 \\
        MentorMix~\cite{jiang2020beyond} &  47.14 & 43.80 & 33.46 \\
        InstanceGM~\cite{Garg_2023_WACV} &  52.24 & 47.96 & 39.62 \\
        \midrule
        FaMUS*~\cite{xu2021faster} & \textit{51.42} &  \textit{45.10} & \textit{35.50}\\
        \rowcolor{Gray!25} \textbf{FaMUS-PASS}  &  \textbf{53.40} & \textbf{48.04} & \textbf{40.08} %   \textbf{52.06} \textbf{47.96} old acc
        \\
        \midrule
        \midrule
        \multicolumn{4}{l}{\bfseries With self-supervised learning}\\
        \midrule
        % DivideMix~\cite{} & & & &  \\
        PropMix~\cite{cordeiro2021propmix} & 56.22 & 52.84 & 43.42\\
        InstanceGM-SS~\cite{Garg_2023_WACV} & 56.37 & 53.21 & 44.03\\
        \rowcolor{Gray!25} \textbf{FaMUS-SS-PASS}  & \textbf{56.48} & \textbf{53.53} & \textbf{44.32} \\ % \textbf{71.31} & \textbf{69.21} & \textbf{66.20} 59.46  for 0.8
        \bottomrule
    \end{tabular}
\end{table}

AugDesc~\cite{nishi2021augmentation} is utilized as a baseline for Clothing1M~\cite{xiao2015learning}, where two different training setups named as AugDesc-WAW~\cite{nishi2021augmentation} and AugDesc-SAW~\cite{nishi2021augmentation} are employed as shown in~\cref{table:clothing1M}.
% \gustavo{COMMENT: Should be Table 3, not Table 5...}. 
PASS is found to be easily adaptable to both the versions, delivering highly competitive results in comparison to the existing methods. Furthermore, \cref{table:miniWebvision} 
% \gustavo{COMMENT: Should be Tables 4,5,6, not Table 2,3,6...} 
presents the results obtained by PASS on Mini-Webvision~\cite{li2017webvision} and Imagenet~\cite{deng2009imagenet}.
It is noteworthy that the results are shown to improve all baselines, exhibiting competitive performance across the entire dataset. 

\begin{table}[htbp!]
        \centering
        \caption{Test accuracy (\%) of competing strategies on Clothing1M~\cite{xiao2015learning}. 
        In the experiments only noisy labels are used for training. The base model used is AugDesc~\cite{nishi2021augmentation} with results in \textit{italics}. \textbf{PASS} results are within 1\% of the best result in this dataset.}
        % \gustavo{what does it mean to be in top 1\%?}\arpit{updated}.
        \label{table:clothing1M}
        \begin{tabular}{l c}
            \toprule
            \bfseries Method & \bfseries Test Accuracy (\%) \\
            \midrule
            DivideMix~\cite{li2020dividemix} & 74.76\\
            Nested-CoTeaching~\cite{chen2021boosting} & 74.90\\
            MLC~\cite{zheng2021meta} & \textbf{75.78}\\
            % FASTEN~\cite{kye2022learning} & \textbf{77.83}\\
            \midrule
            AugDesc-WAW*~\cite{nishi2021augmentation} & \textit{74.72}\\
            \rowcolor{Gray!25} \textbf{AugDesc-WAW-PASS}  & 74.81 \\
            AugDesc-SAW*~\cite{nishi2021augmentation} & \textit{75.11}\\
            \rowcolor{Gray!25} \textbf{AugDesc-SAW-PASS}  & 75.13 \\
            \bottomrule
        \end{tabular}
\end{table}


\begin{table}[htbp!]
        \caption{Test accuracy (\%) on Mini-Webvision~\cite{li2020dividemix} and validation on Imagenet~\cite{deng2009imagenet}. Base model is Contrast-to-Divide(C2D)~\cite{zheltonozhskii2022contrast} represented by \(*\) with results in \textit{italics}, whilst \textbf{C2D-PASS} is our proposed results.
        }
        \label{table:miniWebvision}
        \centering
        
        \begin{tabular}{l c c c c }
        \toprule
        \multirow{2}{*}{\bfseries Dataset} & \multicolumn{2}{c}{\bfseries Mini-Webvision} & \multicolumn{2}{c}{\bfseries Imagenet} \\ 
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} \\
        \midrule
        DivideMix~\cite{li2020dividemix}  & 77.32 & 91.64 & 75.20 & 91.64 \\
        SSR~\cite{feng2021ssr} & \textbf{80.92} & \textbf{92.80} & 75.76 & 91.76\\
        \midrule
        C2D*~\cite{zheltonozhskii2022contrast} & \textit{79.42} & \textit{92.32} & \textit{78.57} & \textit{93.04}\\
        \rowcolor{Gray!25} \textbf{C2D-PASS}  & 80.50 & 92.78 & \textbf{79.32} & \textbf{93.20}  \\
        \bottomrule
        \end{tabular}
\end{table} 


% \gustavo{Please add a table with training time and memory need for training for previous methods and previous methods + PASS.  This is certainly a question to be asked, so we'd better this here.}\arpit{updated}

    
