\section{Related Work}
\label{sec:related_work}

\textbf{Noisy label Learning:} DNNs have been demonstrated to be highly effective in fitting randomly labeled training data, which may, however, result in overfitting on a noisy label dataset~\cite{zhang2017understanding}. Consequently, when presented with clean label testing data, these networks exhibits poor generalization\cite{arpit2017closer, zhang2021learning_}. To address this challenge, numerous studies have explored supervised learning under the label noise setting, leading to the development of various techniques, such as robust loss functions~\cite{ma2020normalized}, sample selection~\cite{li2020dividemix, jiang2018mentornet, kim2021fine, han2018co}, robust regularization~\cite{jenni2018deep, wei2021open, menon2019can}, and robust architecture~\cite{xiao2015learning, cheng2020weakly, han2018masking, kong2021resolving}. Our work primarily aims to address the issue of noisy label DNNs, with a particular emphasis on sample selection methods. We provide a brief review of the methods proposed in the literature that fall within this category.

To maximize the utility of the entire dataset during training, including the noisy label subset, recent methods have incorporated semi-supervised training~\cite{li2020dividemix, liu2020early, Garg_2023_WACV} techniques, such as MixMatch~\cite{berthelot2019mixmatch}. This involves treating the clean subset as labelled and the noisy subset as unlabeled. However, to accomplish this, a sample selection stage is required to classify samples into clean or noisy. While this approach is well-motivated and often effective, it is vulnerable to cumulative errors from the selection process, particularly when there are numerous unreliable samples in the training set~\cite{song2022learning, zheltonozhskii2022contrast}. Therefore, sample selection methods often employ multiple clean label sample classifiers to enhance their robustness against such cumulative errors~\cite{kim2021fine, feng2021ssr}. In most cases, distinguishing between hard-to-classify clean and noisy label samples poses a significant challenge. This issue is highlighted by recent research~\cite{song2022learning, kim2021fine}, which note that model performance can significantly deteriorate due to sampling errors~\cite{kim2021fine}.

\textbf{Sample Selection:} The process of selecting samples falls into two categories, namely loss-based sampling~\cite{jiang2018mentornet} and feature-based sampling~\cite{feng2021ssr}. Loss-based sample selection~\cite{li2020dividemix, zheltonozhskii2022contrast, sachdeva2021evidentialmix} involves the application of the small loss hypothesis, which presupposes that noisy data tends to incur a high loss due to the model's difficulty in correctly classifying such data. However, these methods require the tuning of a small-loss threshold to enable the selection of training samples. Such challenge motivated the development of complex strategies, such as the filtration of samples using small loss over a number of training epochs~\cite{cordeiro2021propmix}.  
In feature-based sampling~\cite{kim2021fine, feng2021ssr, wu2020topological}, both clean and noisy samples are distinguished by leveraging features. For instance, Chen et al.~\cite{feng2021ssr} and Wu et al.~\cite{wu2020topological} employed KNN to identify clean samples, whereas FINE~\cite{kim2021fine} utilized eigen vectors for this purpose.

 %\gustavo{SSR selects features based on KNN, right?  Not based on small loss.  This is more based on features.}

Moreover, there are other sampling techniques, like the one by
Le Bras et al.~\cite{le2020adversarial} that proposed an adversarial filtering-based approach to eliminate spurious artifacts in a dataset.
Li et al.~\cite{li2019repair} suggested a resampling procedure to reduce representation bias by learning a weight distribution that favors difficult instances for a given feature representation. 
Nevertheless, these two techniques rely solely on theoretical claims or necessitate meticulous fine-tuning of complex hyperparameters to accommodate the type and magnitude of noise present, which leads to significant performance degradation when incorrect selections are made~\cite{song2022learning}.

% \gustavo{not all methods in this paragraph are small-loss approaches...}

% \textbf{Measures of Agreement with Validity and Reliability:} 
% \cuong{If you want to highlight each paragraph, please make the whole section consistent.}
\textbf{Data reliability:} 
The reliability of data is paramount for developing a computational model~\cite{craggs2004two} or supporting an empirical claim~\cite{artstein2008inter}. 
Over the years, research has focused on diverse ways of selecting reliable samples from unreliable datasets based on peer effects and social networks~\cite{newman2018network}. 
In an unreliable data environment, peer-based sample selection has emerged as a promising approach for training models and selecting high-confidence samples~\cite{steinhardt2016avoiding}. This method involves utilizing a group of models to collectively judge and select samples to train the model~\cite{dagan1995committee, malach2017decoupling}. 
This technique aims to improve model performance in scenarios involving unreliable data while also mitigating the influence of confirmation bias~\cite{chen2023bias}. 
Peer-based sample selection has the potential to enhance the accuracy and reliability of learning systems in situations where data quality is uncertain~\cite{kaiser2022blind}. 
When different models produce consistent results, it indicates that they have a similar understanding of the categories and can be expected to perform consistently~\cite{craggs2004two, kuncheva2002theoretical}. 
Notwithstanding, it is imperative to consider that attaining agreement does not invariably guarantee validity; nevertheless, it is probable that they would agree on reliable samples to a greater extent~\cite{artstein2008inter, kuncheva2002theoretical, ramesh2022peer, kaiser2022blind}. 
Whilst our work draws inspiration from the aforementioned approaches, our fundamental aim is to address the issue of noisy label image classification through the application of a novel sample selection.
