

\subsection{Empirical Analysis on Sample Selection}\label{subsec:sample_selection}
% \cuong{Could we move this section into the ablation studies since it is slightly abrupt without properly introducing datasets, models and other settings.}
\input{main/main_empirical_comparison}

In this section, we conduct an empirical analysis of the PASS algorithm. Detailed in \cref{subsec:empirical_subsec_cifar}, the analysis compares the effectiveness of PASS against small loss~\cite{li2020dividemix} and FINE~\cite{kim2021fine} approaches on the CIFAR100~\cite{krizhevsky2009learning}, specifically at \(0.5, 0.4, \text{ and } 0.2\) IDN~\cite{xia2020part}. The comparison focuses on metrics F1 score, precision, and the ratio of clean samples, employing DivideMix~\cite{li2020dividemix} as the base model for PASS. Similarly,~\cref{subsec:empirical_subsec_clothing} extends this analysis to the Clothing1M~\cite{xiao2015learning}, a real-world dataset. Here, the comparison is between AugDesc~\cite{nishi2021augmentation}, and PASS with baseline AugDesc~\cite{nishi2021augmentation}.

\subsubsection{Analysis of PASS Performance at Various Noise Rates on CIFAR100}\label{subsec:empirical_subsec_cifar}
\paragraph{IDN setting at high noise rate (e.g., 50\%)} To empirically analyse PASS, we focus on the challenging IDN synthetic noise at \(50\%\) noise rate~\cite{xia2020part} on CIFAR-100~\cite{krizhevsky2009learning}.
\cref{fig:empirical_graph} shows three graphs to measure the performance of the clean sample classification, namely: \cref{fig:f1} shows \emph{F1 score}, \cref{fig:precision} shows \emph{precision}, and \cref{fig:ratio} shows the \emph{ratio of the data classified as clean}.
We use these graphs to compare our PASS against the small-loss hypothesis~\cite{li2020dividemix}, and feature-based approach~\cite{kim2021fine} (all using DivideMix~\cite{li2020dividemix} as the robust noisy-label training algorithm). We have only considered the methods of sample selection and have not incorporated the methods that involve sample relabeling within this analysis~\cite{feng2021ssr}. 
% \gustavo{please add citation to a sample relabeling method}.

\cref{fig:ratio} shows the proportion of data classified as clean (by the model). 
It is evident that the small-loss~\cite{li2020dividemix} hypothesis and the feature-based approach~\cite{kim2021fine} consistently yield a ratio of around $0.70 - 0.65$ during the training process, while our approach maintains a ratio of around $0.50-0.45$. 
As we know from the setup, the optimal rate (ideal ratio) should be $\approx$ $0.50$. This indicates that our approach is more capable of identifying the correct proportion of noisy-label samples for the IDN at \(50\%\) on CIFAR-100. 
However, that proportion alone does not ensure that clean samples are accurately selected. 
Therefore, we also calculated the F1 score (\cref{fig:f1}) and the precision (\cref{fig:precision}), both of which show superior results using our approach. 
More specifically, \cref{fig:f1} shows that our strategy exhibits a consistently higher F1 score compared to other approaches, achieving the final result of $0.87$, which is better than other approaches, such as small-loss and feature-based~\cite{kim2021fine} that present a similar result of $0.75$. Another important comparison measure is precision. PASS shows very high precisions of more than $0.96$, while small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} show much lower precision values around $0.72$. This empirical analysis suggests that our method is more efficacious at correctly identifying positive and negative samples from the training set than other competing approaches.

\paragraph{IDN settings at low and intermediate noise rates} (e.g., \(20\% \text{ and } 40\%\)) We further extended our empirical analysis to include other challenging IDN noise cases~\cite{xia2020part} at rates of $40\%$ and $20\%$, as shown in \cref{fig:0.4_supp,fig:0.2_supp} on CIFAR-100~\cite{krizhevsky2009learning} respectively. These plots compare our PASS (using DivideMix~\cite{li2020dividemix}) against the small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} approaches by measuring the 
classification performance of clean samples based on (a) F1 score, (b) precision, and (c) ratio of data classified as clean. 

\input{main/supp_empirical_comparison}

 From \cref{fig:0.4_supp,fig:0.2_supp}, it is clear that as training evolves, PASS gets closer to the ideal proportion of clean-label samples available for training than the small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} approaches, suggesting that our approach
is more capable of identifying the correct proportion of noisy-label samples for the IDN noise. 
This proportion alone does not imply accuracy. Therefore, we also provide graphs with F1 and precision scores, which help to highlight the advantages of using our peer agreement for sample selection. 
More specifically, \cref{fig:0.4_supp,fig:0.2_supp} show that our strategy exhibits a consistently superior F1 score compared to other approaches for noise rates $40\%$ (\cref{fig:0.4_f1}) and $20\%$ (\cref{fig:0.2_f1}). PASS achieves a final result of $0.92$, which directly reflects the improvement in the performance of PASS when compared to small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} with similar results of $0.8-0.85$ at noise rate $40\%$. Whilst feature-based~\cite{kim2021fine} and PASS are very competitive in F1 score for noise rate $20\%$ with a value around $0.94$, small-loss~\cite{li2020dividemix} stays around $0.89$.
PASS shows an outstanding precision higher than $0.98$, while small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} show much smaller precision values of around $0.8$ for noise rate $40\%$ (\cref{fig:0.4_precision}). Moreover, all methods are very competitive in precision at a low noise rate of $20\%$ (\cref{fig:0.2_precision}). Our empirical analysis shows that our method outperforms other competing approaches in correctly identifying positive and negative samples from the training set across all noise levels.


\subsubsection{Empirical Insights on PASS using Clothing1M}\label{subsec:empirical_subsec_clothing}

Although Clothing1M~\cite{xiao2015learning} offers a clean validation set, we did not incorporate it into our training process. However, we used this clean validation set to assess and compare the effectiveness of PASS and baseline AugDesc~\cite{nishi2021augmentation}. For AugDesc training with and without PASS, we have used the \emph{DM-AugDesc-WS-WAW} version of training, as mentioned in AugDesc~\cite{nishi2021augmentation}. As mentioned in \cref{table:clothing1M},  
%\rafa{This refernce is broken in the text -> }~\cref{table:Clothing1M} 
our results are competitive with the existing model. Although both baseline methods are competitive, PASS is still capable of outperforming based on: (\subref{fig:cloth_f1}) F1, (\subref{fig:cloth_precision}) precision, and (\subref{fig:cloth_ratio}) the ratio of clean data in \cref{fig:clothing_supp}.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \begin{tikzpicture}
            \pgfplotstableread[col sep=comma, header=true]{clothing1m_csv/f1.tex} \myTable
            \begin{axis}[
                height = 0.6\linewidth,
                width = 0.8\linewidth,
                xlabel={\textnumero~of epochs},
                xlabel style={font=\scriptsize},
                xticklabel style = {font=\scriptsize},
                ylabel={F1-score},
                ylabel style={font=\scriptsize, yshift=-0.5em},
                yticklabel style = {font=\scriptsize},
                legend entries={AugDesc, AugDesc-PASS},
                legend style={draw=none, font=\scriptsize},
                legend image post style={scale=0.5},
                legend cell align={left},
                legend pos=south east,
                scale only axis
            ]
                \addplot[mark=none, BurntOrange, thick, densely dashed] table[x={epochs}, y={SmallLoss}]{\myTable};
                \addplot[mark=none, MidnightBlue, solid, thick, solid] table[x={epochs}, y={PASS}]{\myTable};
            \end{axis}
        \end{tikzpicture}
        \caption{F1 Score}
        \label{fig:cloth_f1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering

        \begin{tikzpicture}
            \pgfplotstableread[col sep=comma, header=true]{clothing1m_csv/precision_.tex} \myTable

            \begin{axis}[
                height = 0.6\linewidth,
                width = 0.8\linewidth,
                xlabel={\textnumero~of epochs},
                xlabel style={font=\scriptsize},
                xticklabel style = {font=\scriptsize},
                ylabel={Precision},
                ylabel style={font=\scriptsize, yshift=-0.5em},
                yticklabel style = {font=\scriptsize},
                scale only axis
            ]
                \addplot[mark=none, BurntOrange, thick, densely dashed] table[x={epochs}, y={SmallLoss}]{\myTable};
                \addplot[mark=none, MidnightBlue, solid, thick, solid] table[x={epochs}, y={PASS}]{\myTable};
            \end{axis}
        \end{tikzpicture}
        \caption{Precision}
        \label{fig:cloth_precision}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering

        \begin{tikzpicture}
            \pgfplotstableread[col sep=comma, header=true]{clothing1m_csv/clean.tex} \myTable
            \begin{axis}[
                height = 0.6\linewidth,
                width = 0.8\linewidth,
                xlabel={\textnumero~of epochs},
                xlabel style={font=\scriptsize},
                xticklabel style = {font=\scriptsize},
                ylabel={Ratio of clean data},
                ylabel style={font=\scriptsize, yshift=-0.5em},
                yticklabel style = {font=\scriptsize},
                scale only axis
            ]
                \addplot[mark=none, BurntOrange, thick, densely dashed] table[x={epochs}, y={SmallLoss}]{\myTable};
                \addplot[mark=none, MidnightBlue, solid, thick, solid] table[x={epochs}, y={PASS}]{\myTable};
            \end{axis}
        \end{tikzpicture}
        \caption{Ratio of clean classified data}
        \label{fig:cloth_ratio}
    \end{subfigure}
    \caption{Graphs to compare the effectiveness of selecting clean or noisy samples, regarding three metrics: (a) F1-score, (b) precision, and (c) ratio of data classified as clean.
    The comparison is made between our approach AugDesc-PASS (solid \textcolor{MidnightBlue}{blue}) with baseline AugDesc approach~\cite{nishi2021augmentation} (dashed \textcolor{BurntOrange}{yellow}) on Clothing1M. We have used DM-AugDesc-WS-WAW version of training as mentioned in AugDesc~\cite{nishi2021augmentation}.}\label{fig:clothing_supp}
\end{figure*}