\section{Experiments} \label{sec:experiments}

This section presents an extensive experimental evaluation of PASS. We present the datasets, implementation details, and results over several LNL benchmarks, followed by an ablation study.
\input{tables/cifar_table_1}

\subsection{Datasets}\label{subsec:dataset}

The experiments are performed on many common datasets in LNL, including CIFAR-100~\cite{krizhevsky2009learning}, CIFAR-N~\cite{wei2022learning}, Animal-10N~\cite{song2019selfie}, Red mini-ImageNet~\cite{jiang2020beyond}, Clothing-1M~\cite{xiao2015learning} and mini-WebVision~\cite{li2020dividemix}.
%CIFAR10/100
\paragraph{CIFAR-100} The dataset consists of \(50,000\) training images and \(10,000\) testing images with each image having a size of \(32 \times 32 \times 3\) pixels, distributed evenly into 100 categories. This dataset does not possess label noise by default, so we follow the \emph{part-dependent label noise} setting~\cite{xia2020part} to simulate various IDN noise rates: \(\{0.2, 0.3, 0.4, 0.45, 0.5\}.\)

%CIFARN
\paragraph{CIFAR-10N and CIFAR-100N} The datasets are created by relabelling both the original CIFAR-10 and CIFAR-100~\cite{wei2022learning} datasets using the Amazon Mechanical Turk (M-Turk) labelling service.
%which involves annotations from human workers. 
The CIFAR-10N dataset includes five distinct noise rate options, from which we have selected the \say{\emph{worst}} version (noise rate of \(40.21\)\%). In the CIFAR-100N dataset, we considered \say{\emph{fine}} labels with an overall noise level of \(40.20\)\%.

%Animal-10N
\paragraph{Animal-10N} This is a real-world dataset including \(10\) animal categories, with \(5\) pairs of animals sharing similar appearances, such as \emph{chimpanzee} and \emph{orangutan}. The dataset has an estimated label noise rate of \(8\%\), and it comprises of \(50,000\) training images and \(10,000\) test images. In the experiments, we do not perform data augmentation to be consistent with the standard setup~\cite{song2019selfie} for a fair evaluation.
 
 %Redmini-ImageNet
\paragraph{Red mini-ImageNet} The dataset is a subset of the real-world CNWL dataset, which is mainly established to examine the impact of label noise rates on image classification. This dataset includes 100 categories where each categories consists of \(600\) colour images. To ensure an equitable comparison to previous studies, all images have been resized to 32\(\times\)32 pixel\({}^{2}\). There are various noise rates ranging from \(0\% \text{ to } 80\%\). We focused on the noise rates of \(40\%, 60\%, \text{ and } 80\%\) to maintain consistency with the existing literature~\cite{Garg_2023_WACV, xu2021faster}.
% \input{main/train_loss_dm_pass}
%Clothing1M
\paragraph{Clothing1M} This is also a real-world dataset consisting of 1 million training images collected from \(14\) distinct online shopping website categories. There is an estimated \(38.5\%\) noise level in this dataset's labels, which are derived from the surrounding text. To ensure comparability, we used downsized images to \(256 \times 256\) pixel\(^{2}\), as per the prevalent format in previous works~\cite{Garg_2023_WACV, li2020dividemix}. There are \(50,000, 14,000, \text{ and } 10,000\) manually authenticated training, validation, and testing samples, respectively. We excluded clean training and validation sets during training. We only use the clean test set for evaluation, following the literature~\cite{Garg_2023_WACV, li2020dividemix}.

\input{tables/cifarN_table_2}
\input{tables/animal10N_table_3}
% \input{main/train_label_unlabel}
%mini-Webvision
\paragraph{Mini-WebVision} The dataset consists of \(65,944\) colour images taken from the initial \(50\) categories of the WebVision dataset~\cite{li2017webvision}, with images reduced to \(256 \times 256\) pixels. In the experiments, we follow the standard benchmark by evaluating on the clean validation sets of both mini-WebVision and the equivalent \(50\) categories from the ImageNet dataset~\cite{deng2009imagenet}.


\subsection{Implementation}\label{subsec:implementation}

All methods are implemented in the PyTorch framework and executed on the NVIDIA RTX 3090 GPU computing platform. Baseline models are selected based on their accuracy and compatibility with the dataset under consideration. For CIFAR-100, the InstanceGM~\cite{Garg_2023_WACV} and DivideMix~\cite{li2020dividemix} models are used because both have demonstrated to be highly accurate. For CIFAR-N, the DivideMix~\cite{li2020dividemix} model is used. For Animal-10N, SSR~\cite{feng2021ssr} is selected as the base model. For Red mini-ImageNet, a hybrid approach using FaMUS~\cite{xu2021faster} with two evaluation versions, one with and one without DINO self-supervision~\cite{caron2021emerging} is employed. For Clothing-1M, AugDesc~\cite{nishi2021augmentation} model is used. For mini-WebVision, C2D~\cite{zheltonozhskii2022contrast} is employed as the base model. Unless otherwise stated, default hyperparameters and network architectures are as specified in their corresponding papers.

% \begin{table*}[htbp!]
%     \small
%     \centering
%     \caption{Test accuracy (\%) on (a) CIFAR-N~\cite{wei2022learning}, where results of other models are from~\cite{wei2022learning}. The \textbf{PASS} base model  is DivideMix~\cite{li2020dividemix} (\(*\) with results in \textit{italics}). (b) Test accuracy (\%) of various approaches on Animal-10N~\cite{song2019selfie} with baseline (\(*\) and outcomes in \textit{italics}) SSR~\cite{feng2021ssr}. The other results are  from~\cite{feng2021ssr}. \textbf{PASS} represents our approach with baseline SSR~\cite{feng2021ssr}.} \label{table:2}
%     \begin{subtable}[t]{0.5\linewidth}
%         \centering
%         \begin{tabular*}{\linewidth}{@{\extracolsep{0.5pt}}lcc}
%             \toprule
%             \textbf{CIFAR-N} & \small{\textbf{10N-W}} & \small{\textbf{100N-F}}  \\
%             \midrule
%             CE~\cite{liu2022robust} & 77.69 & 55.50\\
%             CAL~\cite{zhu2021clusterability} & 85.36 & 61.73\\
%             ELR~\cite{liu2020early} & 91.09 & 66.72\\
%             SOP+~\cite{liu2022robust} & 93.24 & 67.81\\
%             \midrule
%             DivideMix*~\cite{li2020dividemix} & \textit{92.56} & \textit{71.13}\\
%             \rowcolor{Gray!25} \textbf{DivideMix-PASS}  & \textbf{94.02} & \textbf{72.03} \\
%             \bottomrule
%         \end{tabular*}
%         \caption{CIFAR-N}\label{table:cifar100N}
%     \end{subtable}%
%     \hfill
%     \begin{subtable}[t]{0.45\linewidth}
%         \centering
%         \begin{tabular*}{\linewidth}{@{\extracolsep{0.5pt}}lc}
%             \toprule
%             \textbf{Animal-10N} & \textbf{Accuracy (\%)} \\
%             \midrule
%             CE~\cite{zhang2021learning} & 79.4 \\
%             SELFIE~\cite{song2019selfie} & 81.8 \\
%             PLC~\cite{zhang2021learning} & 83.4 \\
%             Nested-CE~\cite{chen2021boosting} & 84.1 \\
%             Jigsaw-ViT~\cite{chen2023jigsaw} & \textbf{89.0}\\
%             \midrule
%             SSR*~\cite{feng2021ssr} & \textit{88.5}\\
%             \rowcolor{Gray!25} \textbf{SSR-PASS} & \textbf{89.0} \\
%             \bottomrule
%         \end{tabular*}
%         \caption{Animal-10N}
%         \label{table:Animal10N}
%     \end{subtable}
% \end{table*}


\subsection{Comparisons on Benchmarks}\label{subsec:baseline}
In this section, we perform a comparison study on IDN benchmarks and real-world noisy-label benchmarks.

\subsubsection{IDN Benchmark}\label{subsubsec:idn_benchmark}
 
In \cref{tab:cifar}, a comparative analysis is presented showcasing the performance of the proposed method, PASS, against various SOTA techniques on the CIFAR-100 IDN benchmark~\cite{xia2020part}.
% In particular, PASS demonstrates significant improvements in this dataset across various IDN noise rates ranging from \(20\%\) to \(50\%\), when employing  InstanceGM~\cite{Garg_2023_WACV} and DivideMix~\cite{li2020dividemix} models as baselines. As baseline models represent the primary reference for PASS, it is critical to compare the performance of the postulated PASS method with these models to showcase the efficacy and adaptability of our work. Compared to current SOTA methods on this benchmark (InstanceGM~\cite{Garg_2023_WACV} and DivideMix~\cite{li2020dividemix}),
In particular, PASS outperforms these models by approximately between \(1.2\%\) to \(14\%\) at \(0.50\) noise rate. 
% We also provide the training loss over epochs in~\cref{fig:lossComparison}.\gustavo{can we remove this training loss from here? I'm not sure why we need it.}

\subsubsection{Real-world noisy-label benchmarks}\label{subsubsec:real-world}
In~\cref{table:cifar100N,table:Animal10N,table:RedMini,table:clothing1M,table:miniWebvision}, we showcase the results of our proposed method on CIFAR-N~\cite{wei2022learning}, Animal-10N~\cite{song2019selfie}, Red mini-ImageNet~\cite{jiang2020beyond}, Clothing1M~\cite{xiao2015learning}, mini-WebVision~\cite{li2020dividemix} and ImageNet~\cite{krizhevsky2012imagenet}. Overall, PASS demonstrates superior performance or competitiveness with current SOTA models.
% employing various baselines for both a large-scale web-crawled dataset and a small-scale human-annotated noisy dataset.
The results also show that PASS exhibits a high degree of flexibility and can be easily integrated into existing LNL models. 


\input{tables/cnwl_table_4}

\input{tables/clothing1m_table_5}

% \begin{table*}[ht]
%     \small
%     \centering
%     \caption{Test accuracy (\%) on mini-WebVision~\cite{li2020dividemix} and validation on ImageNet~\cite{krizhevsky2012imagenet}. Base model is Contrast-to-Divide(C2D)~\cite{zheltonozhskii2022contrast} represented by \(*\) with results in \textit{italics}, whilst \textbf{C2D-PASS} is our proposed results.}\label{table:miniWebvision}
%     \begin{tabular}{l c c c c }
%         \toprule
%         \textbf{Dataset} & \textbf{mini-WebVision Top-1} & \textbf{mini-WebVision Top-5} & \textbf{ImageNet Top-1} & \textbf{ImageNet Top-5} \\
%         \midrule
%         DivideMix~\cite{li2020dividemix}  & 77.32 & 91.64 & 75.20 & 91.64 \\
%         SSR~\cite{feng2021ssr} & \textbf{80.92} & \textbf{92.80} & 75.76 & 91.76\\
%         \midrule
%          FINE*~\cite{kim2021fine} & \textit{75.24} & \textit{90.28} & \textit{70.08} & \textit{89.71}\\
%          \rowcolor{Gray!25} \textbf{FINE-PASS}*~\cite{zheltonozhskii2022contrast} & 77.01 & 91.42 & 72.78 & 90.21\\
%         \midrule
%         C2D*~\cite{zheltonozhskii2022contrast} & \textit{79.42} & \textit{92.32} & \textit{78.57} & \textit{93.04}\\
%         \rowcolor{Gray!25} \textbf{C2D-PASS}  & 80.50 & 92.78 & \textbf{79.32} & \textbf{93.20}  \\
%         \bottomrule
%     \end{tabular}
% \end{table*} 


\input{tables/redMini_table_6}

In more detail, \cref{table:cifar100N,table:Animal10N} present the results obtained by PASS with their corresponding baselines in CIFAR-N~\cite{wei2022learning} and Animal-10N~\cite{song2019selfie}, respectively. 
It is noteworthy that the results from PASS are shown to improve all baselines, exhibiting competitive performance across both datasets. 

\cref{table:RedMini} reports the results on Red mini-ImageNet~\cite{xu2021faster} using our PASS method with baseline model FaMUS~\cite{xu2021faster} in two different setups: 1) without pretraining (upper section of the table), and 2) with self-supervised (SS) pre-training (lower section of the table). SS pre-training relies on DINO~\cite{caron2021emerging} using the unlabelled Red mini-ImageNet dataset to ensure a fair comparison with InstanceGM~\cite{Garg_2023_WACV}. The results demonstrate that PASS can effectively improve performance and achieve SOTA outcomes on Red mini-ImageNet~\cite{xu2021faster}.
 

\input{tables/ablation_cifar100_table_7}



\cref{table:clothing1M} shows the result on Clothing-1M, where two different training setups named AugDesc-WAW~\cite{nishi2021augmentation} and AugDesc-SAW~\cite{nishi2021augmentation} are used. PASS is found to be easily adaptable to both versions, delivering highly competitive results compared to the existing methods.

Furthermore, \cref{table:miniWebvision} presents the results obtained by PASS on mini-WebVision~\cite{li2017webvision} and ImageNet~\cite{krizhevsky2012imagenet}. In particular, the results are shown to improve all baselines and exhibit competitive performance across the entire dataset. 

\section{Empirical Analysis}
\subsection{Ablation Study on Clustering Algorithms}\label{sec:ablation}
This section present our ablation study on different algorithms that cluster the peer agreement in \cref{eq:cosine_sim} to partition the training dataset into a clean and a noisy subsets. The ablation study is conducted on CIFAR-100~\cite{krizhevsky2009learning} in IDN settings~\cite{xia2020part} with a noise rate of \(0.5\). Two other clustering algorithms, namely K-Means and GMM, are considered in this study with results shown in \cref{tab:ablation_cifar}. Overall, the performance of K-Means and GMM are lower than Otsu's algorithm, which could be attributed to their nature: K-Means and GMM are optimally local (depending on initialisation and stopping criteria), while Otsu's algorithm is a global one due to its exhaustive search. It is worth noting that using GMM and K-Means offers improvements of approximately \(2\%\) accuracy w.r.t. the baseline method, DivideMix~\cite{li2020dividemix}. However, using these clustering techniques can still restrict the classification accuracy since Otsu's thresholding~\cite{otsu1979threshold} enables a further improvement in accuracy of approximately \(6\%\).

\subsection{Computational Time}
\label{sec:computational_time}
We show a training time comparison between various base models~\cite{li2020dividemix, Garg_2023_WACV, feng2021ssr, xu2021faster, nishi2021augmentation, zheltonozhskii2022contrast} and their PASS variants in \cref{table:computation}. Overall, PASS has an overhead due to the usage of multiple classifiers compared to the corresponding baselines. However, this aspect of PASS is mitigated by its satisfactory performance in terms of running time, particularly when executed using half-precision, which stands favorably against its baselines.

\input{tables/ablation_time_table_8}

\subsection{Statistical Hypothesis Testing on Models' Performances}
We perform a statistical hypothesis testing to determine if the integration of PASS into other SOTA methods is effective. Our study compares three models (i.e., DivideMix~\cite{li2020dividemix}, InstanceGM~\cite{Garg_2023_WACV}, and DivideMix-PASS (ours)) on ten datasets (i.e., CIFAR-100~\cite{krizhevsky2009learning} with noise rates of \(0.2, 0.3, 0.4, 0.45, \text{ and } 0.5\), Red mini-ImageNet~\cite{jiang2020beyond} at \(0.4, 0.6, \text{ and } 0.8\) noise rates, Clothing 1M~\cite{xiao2015learning}, and Animal-10N~\cite{song2019selfie}) using one metric, standard accuracy. Generally, a hypothesis testing consists of:
\begin{itemize}
    \item a \emph{null} hypothesis denoting that all means of models' performance are equal, and
    \item an \emph{alternative} hypothesis denoting that at least one of the models performs differently.
\end{itemize}
The conclusion of such a hypothesis testing, of course, holds statistically under a certain significant level (usually 0.05).

One straight approach to compare the performance of several models on many datasets is ANOVA (analysis of variance). However, ANOVA assumes that data follows a normal distribution, which might not hold in our case. Hence, we employ the Friedman test -- a non-parametric hypothesis testing -- as an alternative one.

% We performed the non-parametric test~\cite{statistical2006demsar} on models DivideMix~\cite{li2020dividemix}, InstanceGM~\cite{Garg_2023_WACV}, and DivideMix-PASS (ours),  to assess the differences in their performance across various datasets and noise rates. The datasets used in this analysis included CIFAR-100~\cite{krizhevsky2009learning} with noise rates of \(0.2, 0.3, 0.4, 0.45, \text{ and } 0.5\), Red mini-ImageNet~\cite{jiang2020beyond} at \(0.4, 0.6, \text{ and } 0.8\) noise rates, Clothing 1M~\cite{xiao2015learning}, and Animal-10N~\cite{song2019selfie}. 

The Friedman test with a significance level of \(0.1\), yielded a test statistic of \(16.20\) and a p-value of \(3.035 \times 10^{-4}\), leading us to reject the null hypothesis that all methods perform equally well. This suggests that at least one of the methods significantly differs from the others in terms of performance. 
 % \cuong{We should briefly explain the intuition of each test. Mentioning only their names reduces the self-contain-ness of the paper.}
 \input{tables/statistical}
To further understand these differences, we applied the post-hoc Nemenyi test. The test results showed significant differences between some of the methods. The Critical Difference (CD) value was calculated to be approximately \(1.15\). Based on this value, the methods whose average ranks differ by at least this CD value are considered significantly different at the \(0.1\) confidence level. Our analysis indicates that PASS is significantly different from both DivideMix and InstanceGM, as denoted by the Nemenyi test p-values (\(0.001\) against both). However, there is no significant difference between DivideMix and InstanceGM, as their comparison yields a p-value of \(0.109268\), which is above our threshold for significance. This comprehensive statistical analysis illustrates (\cref{fig:stats_cd_diagram}) the comparative effectiveness of these methods in handling various types and degrees of noise in datasets, affirming that DivideMix-PASS (ours) exhibits a statistically significant improvement over the other methods under study.

\input{main/empirical_analysis}