\section{Related Work}
\label{sec:related_work}
\paragraph{\textbf{Learning with noisy-label (LNL)}} DNNs have been demonstrated to be highly effective in fitting randomly-labelled training data, which may result in overfitting~\cite{cordeiro2023longremix}. Consequently, when presented with clean-label testing data, these networks exhibit poor generalisation~\cite{li2020dividemix}. To address this challenge, numerous studies have explored supervised learning in a label noise setting, leading to the development of various techniques, such as robust loss functions~\cite{xiao2015learning}, sample selection~\cite{li2020dividemix}, robust regularisation~\cite{xiao2015learning}, and robust architectures~\cite{song2022learning}. Our work primarily aims at addressing the issue of noisy-label learning for DNNs, with a particular emphasis on sample selection methods. We provide a brief review of the methods proposed in the literature that fall into this category. To maximise the utility of the entire dataset during training, including the noisy-label subset, recent methods have incorporated semisupervised training~\cite{li2020dividemix, Garg_2023_WACV} techniques, such as MixMatch~\cite{berthelot2019mixmatch}. This involves treating the clean subset as labelled and the noisy subset as unlabelled. However, to accomplish this, a sample selection stage is required to classify samples as clean or noisy. Although this approach is well-motivated and often effective, it is vulnerable to cumulative errors from the selection process, particularly when there are numerous unreliable samples in the training set~\cite{song2022learning}. Therefore, sample selection methods often employ multiple clean-label sample classifiers to enhance their robustness against such cumulative errors~\cite{kim2021fine, feng2021ssr}. In most cases, distinguishing between hard-to-classify clean and noisy-label samples poses a significant challenge. This issue is highlighted by recent research~\cite{song2022learning, kim2021fine}, which notes that model performance can significantly deteriorate due to sampling errors~\cite{kim2021fine}.

\paragraph{\textbf{Sample selection}} The sample selection process falls into two categories, namely loss-based sampling~\cite{jiang2018mentornet} and feature-based sampling~\cite{feng2021ssr}. Loss-based sample selection~\cite{li2020dividemix, zheltonozhskii2022contrast} involves the application of the small-loss trick, which hypothesises that noisy data tend to incur a high loss due to the model's difficulty in correctly classifying such data. However, these methods require the adjustment of a small-loss threshold to enable the selection of training samples. Such a challenge motivated the development of complex strategies, such as the filtration of samples using a small-loss over a number of training epochs~\cite{li2020dividemix}.  
In feature-based selection~\cite{kim2021fine, feng2021ssr}, clean and noisy samples are classified using features extracted from the input data. For example, clean samples can be identified by KNN \cite{feng2021ssr, wu2020topological}, or distance to eigenvectors~\cite{kim2021fine}. Others follow sampling-based technique that employs an adversarial filtering-based approach to eliminate spurious artifacts in a dataset~\cite{le2020adversarial}. Furthermore, resampling procedures which learns a weight distribution to favor difficult instances for a given feature representation~\cite{li2019repair} is suggested to reduce representation bias. However, these two techniques rely solely on theoretical claims or require meticulous fine-tuning of complex hyperparameters to accommodate the type and magnitude of the noise present, leading to significant performance degradation when incorrect selections are made~\cite{song2022learning}.


\paragraph{\textbf{Data reliability}}
The reliability of data is crucial to develop a computational model or to support an empirical claim~\cite{he2022towards}. Over the years, research has focused on various ways to select reliable samples from unreliable datasets based on peer effects and social networks~\cite{newman2018network}. 
In an unreliable data environment, peer-based sample selection has emerged as a promising approach to train models and select high-confidence samples~\cite{song2022learning}. This method involves using a group of models to collectively judge and select samples to train the model~\cite{malach2017decoupling}. 
This technique aims to improve the performance of the model in scenarios that involve unreliable data, while also mitigating the influence of confirmation bias~\cite{song2022learning}. 
Peer-based sample selection has the potential to enhance the accuracy and reliability of learning systems in situations where data quality is uncertain~\cite{ramesh2022peer}. 
When different models produce consistent results, it indicates that they have a similar understanding of categories and can be expected to perform consistently~\cite{ramesh2022peer}. 
However, it is imperative to consider that attaining agreement does not invariably guarantee validity; nevertheless, it is probable that they would agree on reliable samples to a greater extent~\cite{ramesh2022peer}. 
Although our work draws inspiration from the aforementioned approaches, our fundamental aim is to address the issue of noisy-label image classification through the application of a novel sample selection.
