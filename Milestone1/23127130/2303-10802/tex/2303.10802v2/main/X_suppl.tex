\clearpage
\setcounter{page}{1}
\section{appendices}


\subsection{Empirical Analysis}\label{sec:appendix_empirical}
\subsubsection{Comparative Analysis of selecting samples}

As discussed in Section 3.3, we extended our empirical analysis to include other challenging IDN noise cases~\cite{xia2020part} at rates of $40\%$ and $20\%$, as shown in \cref{fig:0.4_supp,fig:0.2_supp} on CIFAR-100~\cite{krizhevsky2009learning} respectively. These plots compare our PASS (using DivideMix~\cite{li2020dividemix}) against the small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} approaches by measuring the 
classification performance of clean samples based on (a) F1 Score, (b) Precision, and (c) Ratio of data classified as clean. 

\input{sec/supp_empirical_comparison}

 From \cref{fig:0.4_supp,fig:0.2_supp}, it is clear that as training evolves, PASS gets closer to the ideal proportion of clean-label samples available for training than the small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} approaches, suggesting that our approach
is more capable of identifying the correct proportion of noisy-label samples for the IDN noise. 
This proportion alone does not imply accuracy. Therefore, we also provide graphs with F1 and precision scores, which help to highlight the advantages of using our peer agreement for sample selection. 
More specifically, \cref{fig:0.4_supp,fig:0.2_supp} show that our strategy exhibits a consistently superior F1 score compared to other approaches for noise rates $40\%$ (\cref{fig:0.4_f1}) and $20\%$ (\cref{fig:0.2_f1}). PASS achieves a final result of $0.92$, which directly reflects the improvement in the performance of PASS when compared to small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} with similar results of $0.8-0.85$ at noise rate $40\%$. Whilst feature-based~\cite{kim2021fine} and PASS are very competitive in F1 score for noise rate $20\%$ with a value around $0.94$, small-loss~\cite{li2020dividemix} stays around $0.89$.
PASS shows an outstanding precision higher than $0.98$, while small-loss~\cite{li2020dividemix} and feature-based~\cite{kim2021fine} show much smaller precision values of around $0.8$ for noise rate $40\%$ (\cref{fig:0.4_precision}). Moreover, all methods are very competitive in precision at a low noise rate of $20\%$ (\cref{fig:0.2_precision}). Our empirical analysis shows that our method outperforms other competing approaches in correctly identifying positive and negative samples from the training set across all noise levels.


\subsubsection{Clothing1M Analysis}

Although Clothing1M~\cite{xiao2015learning} offers a clean validation set, we did not incorporate it into our training process. However, we used this clean validation set to assess and compare the effectiveness of PASS and baseline AugDesc~\cite{nishi2021augmentation}.


For AugDesc training with and without PASS, we have used the \emph{DM-AugDesc-WS-WAW} version of training, as mentioned in AugDesc~\cite{nishi2021augmentation}. As mentioned in \cref{table:clothing1M},  
%\rafa{This refernce is broken in the text -> }~\cref{table:Clothing1M} 
our results are competitive with the existing model. Although both baseline methods are competitive, PASS is still able to outperform based on: \subref{fig:cloth_f1} F1, \subref{fig:cloth_precision} precision, and \subref{fig:cloth_ratio} the ratio of clean data in \cref{fig:clothing_supp}.




\subsection{Otsu}\label{sec:appendix_otsu}

Otsu's algorithm~\cite{otsu1979threshold} aims to estimate the threshold that partitions data samples by maximising the between-class variance and minimising the within-class variance. Otsu's thresholding~\cite{otsu1979threshold}  stands out as a notably straight-forward and advantageous global thresholding approach. 
The Otsu's~\cite{otsu1979threshold} formula for finding the optimal threshold \(t^{*}\) is the following: %maximises the between 
\begin{equation}\label{eq:Otsu}
    t^* = \arg\max_{t}( \sigma_B^2(t) ),
\end{equation}
where $t$ is the threshold value, $\sigma_B^2(t)$ is the between-class variance for threshold $t$, computed as 
$$\sigma_B^2(t) = w_1(t)w_2(t)(\mu_1(t) - \mu_2(t))^2,$$
with $w_1(t)$ and $w_2(t)$ representing the weights of the clean and noisy classes (calculated as fractions of the data on each side of the threshold), and $\mu_1(t),\mu_2(t)$ representing the mean value of of samples in the clean and noisy  classes. The effectiveness of Otsu is compared to that of other approaches in \cref{tab:ablation_cifar}.



\subsection{Computational Time}
\label{sec:computational_time}

We show a training time comparison between various base models~\cite{li2020dividemix, Garg_2023_WACV, feng2021ssr, xu2021faster, nishi2021augmentation, zheltonozhskii2022contrast} and their PASS variants in \cref{table:computation}.

\begin{table}[th]
    \centering
    \caption{Training time (in hours) of the base models and base models with \textbf{PASS} (ours).}
    \label{table:computation}
    \scalebox{0.85}{
    \begin{tabular}{l l r r}
        \toprule
        \textbf{Models} & \textbf{Dataset} & \textbf{Base} & \textbf{PASS} \\
        \midrule
        DivideMix~\cite{li2020dividemix} & CIFAR-100  & 7.5 & 9.8 \\
        InstanceGM~\cite{yao2021instance} & CIFAR-100 & 31.2 & 34.0\\
        SSR~\cite{feng2021ssr} & Animal-10N & 6.5 & 9.8\\
        FaMUS~\cite{xu2021faster} & Red mini-Imagenet & 12.0 & 14.2 \\
        FINE~\cite{kim2021fine} & Clothing1M & 30.3 & 34.1\\ 
        AugDesc~\cite{nishi2021augmentation} & Clothing1M & 29.6 & 30.1\\
        FINE~\cite{kim2021fine} & Mini-Webvision & 41.5 & 44.7\\
        C2D~\cite{zheltonozhskii2022contrast} & Mini-Webvision & 42.2 & 44.1\\
        \bottomrule
    \end{tabular}
    }
\end{table}
\
