\section{Conclusion}
\label{sec:conclusion}
This article proposed a new peer-agreement-based sample selection technique, PASS, for noisy-label learning
to improve the performance of robust classifiers.
We show that PASS can be easily integrated into SOTA noisy-label learning methods~\cite{li2020dividemix, Garg_2023_WACV, xu2021faster, feng2021ssr, zheltonozhskii2022contrast, nishi2021augmentation}
to improve their classification accuracy results on several noisy-label learning benchmarks, including CIFAR-100~\cite{krizhevsky2009learning}, Red mini-ImageNet from CNWL~\cite{xu2021faster}, Animal-10N~\cite{song2019selfie}, CIFAR-N~\cite{wei2022learning}, Clothing1M~\cite{xiao2015learning}, mini-Webvision~\cite{li2017webvision}, and Imagenet~\cite{krizhevsky2012imagenet}. It consistently outperforms existing methods in most cases. 
Our proposed approach has the potential to create a positive societal impact by mitigating biases in resolving noisy-labelled data. 
The slight increase in training time, between \((2\%)\) and \((10\%)\) as detailed in~\cref{sec:computational_time}, is a small investment for the gains in accuracy and reliability of the model. 
Furthermore, in addition to the gains listed above, our strategic design choice enables a richer, and more nuanced understanding of the data. 
Looking ahead, we plan to refine and enhance PASS's efficiency through methods such as dimensionality reduction and early stopping, alongside the adoption of mixed precision training. The integration of these techniques will not only streamline PASS's performance, but also significantly broaden its applicability and effectiveness in diverse scenarios, solidifying its position as a state-of-the-art tool in the field.
% Finally, we intend to conduct a thorough analysis of PASS to uncover the theoretical insights of the methodology. 

%Our proposed PASS approach has exhibited exceptional performance for noisy-label classification. As experiment section shows, the integration of PASS into many SOTA noisy-label learning methods~\cite{Garg_2023_WACV, li2020dividemix, feng2021ssr, xu2021faster, nishi2021augmentation, zheltonozhskii2022contrast} has produced notable improvements in accuracy across multiple benchmarks~\cite{krizhevsky2009learning, wei2022learning, song2019selfie, xu2021faster, li2017webvision}, and has shown that our approach possesses remarkable adaptability to a diverse range of methods.
% A limitation of PASS is the need for $3$ classifiers, which require $2\%$ to $10\%$ additional training time, as shown in \cref{sec:computational_time}. We plan to work on techniques to reduce training time with methods such as dimensionality reduction~\cite{mackiewicz1993principal} or early stopping~\cite{liu2020early} or using mixed precision training in the future. 
