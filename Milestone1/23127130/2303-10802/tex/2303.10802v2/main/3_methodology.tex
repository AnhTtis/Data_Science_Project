\section{Methodology}\label{sec:methodology}
% \input{tables/notation}
\input{final_figures/fig2}
\subsection{Problem definition}\label{problem_definition}
Formally, we define the instance space as \(\mathcal{X}\) and their respective label space as \(\mathcal{Y}\). The training set is represented by $\Tilde{{D}} = \{(\mathbf{x}_{i}, \hat{\mathbf{y}}_{i}) \}^{n}_{i=1}$, where $\mathbf{x}_{i} \in \mathcal{X} \subseteq \mathbb{R}^{d}$ represents an instance, and $\hat{\mathbf{y}}_{i} \in \mathcal{Y} = \{ \hat{\mathbf{y}}: \hat{\mathbf{y}} \in \{0,1\}^{C} \wedge \pmb{1}^{\top} 
\hat{\mathbf{y}} = 1 \}$, denotes the $C$-dimensional one-hot vector representation of the corresponding noisy-label. 
In the conventional classification problem, $\Tilde{{D}}$ is used to train a classifier $h_{\gamma}: {X} \to \Delta_{C-1}$, parameterised by $\gamma \in \Gamma \subseteq \mathbb{R}^{\abs{\gamma}}$ with $\Delta_{C - 1}$ representing the \((C - 1)\)-dimensional probability simplex. In noisy-label learning, noisy-label data $\Tilde{{D}}$ are exploited to
obtain a model $h_{\gamma}$ that can accurately predict the clean-label of samples in a test set. 
\input{main/algorithm}

\subsection{Reliability based sample selection}
To enhance the lucidity of our explanation, we begin by delineating our methodology for sample selection. As shown in~\cref{fig:architecture}, our proposed method, PASS, requires at least three classifiers: \(\{h_{\gamma_{k}}\}_{k = 1}^{3}\), to select reliable samples via peer agreement. In particular, all classifiers consistently rotate between the roles of peers and training classifiers. It is also important to note that we have randomly initialised the classifiers to reduce the chances of confirmation bias~\cite{li2020dividemix}. Another important note is that our sample selection approach can be easily integrated into various models in LNL, as we demonstrate in \cref{sec:experiments}.

The output of the $k$-th classifier, denoted by \(h_{\gamma_{k}} (\mathbf{x}_{i})\), represents the probability of $\mathbf{y}_{i}$ given $\mathbf{x}_{i}$. The predictive probability agreement between two peer classifiers: \(h_{\gamma_{l}} \) and \(h_{\gamma_{m}} \), on a data point \(\mathbf{x}_{i}\) is defined as the cosine similarity of the two predictions made by the two moels: 
\begin{equation}\label{eq:cosine_sim}
    \mathbf{s}_{i} = \operatorname{agreement} \left( h_{\gamma_{l}}, h_{\gamma_{m}} \vert \mathbf{x}_{i} \right) = \frac{h_{\gamma_{l}}(\mathbf{x}_{i})^{\top}  \, h_{\gamma_{m}}(\mathbf{x}_{i})}{\norm{h_{\gamma_{l}}(\mathbf{x}_{i}) } \, \norm{h_{\gamma_{m}}(\mathbf{x}_{i}) }},
\end{equation}
where \(\norm{.}\) denotes the Frobenius norm.


\input{final_figures/fig3}

\begin{remark}
\label{thm:remark}
    Noisy-label samples will likely have small predictive probability agreement values in \cref{eq:cosine_sim}, as empirically shown in~\cref{fig:cosinehypothesis}. The reason is that the different influences of noisy-label samples on predictive probabilities can result in disparate outcomes between peer classifiers (see~\cref{fig:cosinehypothesis} - orange). Moreover, clean data is likely to have high predictive probability agreement (see~\cref{fig:cosinehypothesis} - blue). According to many studies in peer classifier agreement~\cite{ramesh2022peer}, it is recommended to select clean data based on a high peer classifier agreement, but noisy data should be selected cautiously based on low peer classifier agreement.
\end{remark}
 

Leveraging the remark above, we partition the training set \(\Tilde{{D}}\) into a clean set \(\Tilde{{D}}_{\text{clean}}\) and a noisy set  \(\Tilde{{D}}_{\text{noisy}}\) based on the value of the cosine similarity in \cref{eq:cosine_sim}. This partition can be achieved by any thresholding algorithm, with the clean set comprising data points exhibiting high cosine similarity values and the noisy set comprising data points exhibiting low cosine similarity values. In our case, we use the global thresholding technique known as Otsu's algorithm \cite{otsu1979threshold}. Compared to other clustering and thresholding algorithms (K-Means and the Gaussian Mixture Model (GMM)), Otsu's algorithm \cite{otsu1979threshold} is advantageous, as it can find the optimal clustering, and hence, provides PASS with the best performance, as shown in the ablation studies under \cref{sec:ablation}. This thresholding algorithm automatically estimates an optimal threshold $t$ to divide the data samples into two classes, namely \emph{clean} ($\mathbf{s}_{i} \geq t$ or most likely agreed) and \emph{noisy} data ($\mathbf{s}_{i} < t$ or most unlikely agreed). Further detailed explanation of  Otsu's algorithm \cite{otsu1979threshold} can be found in~\cref{sec:appendix_otsu}. 

Once clean and noisy samples have been selected, we employ  noisy-label learning training algorithms from the literature (\cite{Garg_2023_WACV,li2020dividemix,feng2021ssr,xu2021faster,nishi2021augmentation,zheltonozhskii2022contrast}). These algorithms are employed to test the efficacy of our proposed sampling approach in experiments. Our training procedure is succinctly described in~\cref{procedure:Proposed_Procedure} and visually portrayed in~\cref{fig:architecture}.


\subsection{Otsu's algorithm}\label{sec:appendix_otsu}

Otsu's algorithm~\cite{otsu1979threshold} aims to estimate the threshold that partitions data samples by maximising the between-class variance and minimising the within-class variance. The Otsu's thresholding stands out as a notably straight-forward and advantageous global thresholding approach. The Otsu's formula for finding the optimal threshold \(t^{*}\) is the following: %maximises the between 
\begin{equation}\label{eq:Otsu}
    t^* = \arg\max_{t}( \sigma_B^2(t) ),
\end{equation}
where $t$ is the threshold value, $\sigma_B^2(t)$ is the between-class variance for threshold $t$, computed as 
$$\sigma_B^2(t) = w_1(t)w_2(t)(\mu_1(t) - \mu_2(t))^2,$$
with $w_1(t)$ and $w_2(t)$ representing the weights of the clean and noisy classes (calculated as fractions of the data on each side of the threshold), and $\mu_1(t),\mu_2(t)$ representing the mean values of cosine similarity in the clean and noisy classes, respectively. The effectiveness of the Otsu's clustering is compared other clustering approaches in \cref{sec:ablation}.

% \subsection{Theoretical Analysis}\label{subsec:theoritical_proof}

% In this section, we analyse~\cref{thm:remark}.

% \begin{definition}[Optimal Classifier]
% An optimal classifier $h_{\gamma}^*$ is defined as the classifier that minimizes the expected loss over the data distribution $\mathcal{D}$:
% \[ h_{\gamma}^* := \arg \min _\gamma \mathbb{E}_{\mathcal{D}}[\ell(h_{\gamma}(X), Y)]. \]
% \end{definition}

% \begin{definition}[Loss Function Bounds]
% The loss function $\ell(\cdot)$ is bounded by $\ell_{\min}$ and $\ell_{\max}$ which represent the minimum and maximum loss values respectively.
% \end{definition}

% \begin{lemma}[Variance under clean-labels]
% Given the loss function constraints, the variance of the loss under clean-labels is zero:
% \[ \operatorname{var}_{\mathcal{D}}(\ell(h_{\gamma}^*(X), Y)) = 0. \]
% \end{lemma}

% \begin{proof}
% The proof is trivial since the loss incurred for each instance is constant, $\ell(h_{\gamma}^*(X), Y) = \ell_{\min}$ \arpit{(close to \(0\))}, leading to a zero variance in ideal cases or close to zero.
% \end{proof}

% \begin{theorem}\label{theorem:1}[Variance under noisy-labels]
% Introducing a noise rate $\varepsilon$, the variance of the loss function under noisy-labels is given by:
% \[ \operatorname{var}_{\widetilde{\mathcal{D}}}[\ell(h_{\gamma}^*(X), \tilde{Y})] = \varepsilon (1 - \varepsilon) (\ell_{\max} - \ell_{\min})^2. \]
% \end{theorem}

% \begin{proof}
% The expected loss under the noisy-label distribution $\widetilde{\mathcal{D}}$ is a weighted average of the loss bounds:
% \[ \mathbb{E}_{\widetilde{\mathcal{D}}}[\ell(h_{\mathcal{D}}^*(X), \tilde{Y})] = \varepsilon \ell_{\max} + (1 - \varepsilon) \ell_{\min}. \]

% The variance is thus the expectation of the squared deviations from this expected loss:
% \begin{equation*}
% \scalebox{0.9}{$
% \begin{aligned}[b]
% \operatorname{var}_{\widetilde{\mathcal{D}}}[\ell(h_{\mathcal{D}}^*(X), \tilde{Y})] &= \varepsilon (\ell_{\max} - \mathbb{E}[\ell])^2 + (1 - \varepsilon) (\ell_{\min} - \mathbb{E}[\ell])^2 \\
% &= \varepsilon (1 - \varepsilon) (\ell_{\max} - \ell_{\min})^2,
% \end{aligned}
% $}
% \end{equation*}
% where $\mathbb{E}[\ell]$ denotes the expected loss under the noisy distribution $\widetilde{\mathcal{D}}$. This concludes that the clean-label approximates the true distribution with low variance, while the wrong label approximates the true distribution but with larger variance.
% \end{proof}

% \cuong{The variance analysis above provides a clear understanding of the variance of the loss evaluated on the noisy-label training set. However, it is unclear why \(\ell(h^{*}(x), y) = \ell_{\mathrm{min}}\) for clean samples and \(\ell(h^{*}(x), y) = \ell_{\mathrm{max}}\) for noisy samples. To my understanding, the assumptions made in the above analysis should be written as follows:
% \begin{itemize}
%     \item The loss function used is a 0-1 loss.
%     \item \(h^{*}(x)\) is a perfect classifier, which means \(\ell(h^{*}(x), y) = 0\) for clean samples and 1 for noisy samples.
% \end{itemize}
% Could this be verified if it is the case?
% }
% \arpit{\(h^*\) minimises the expected loss over the distribution D it will be close to 0 but not exactly zero, l refers to any loss function like CE}

% \cuong{Please elaborate equation at line 323 further. The reason I am asking is because according to your assumption of \(h^{*}\), \(\ell(h^{*}(x), y) = \ell_{\mathrm{min}}\) for clean samples and \(\ell(h^{*}(x), y) = \ell_{\mathrm{max}}\). Then, the result in \cref{theorem:1} is trivial. Imagine we have noise rate \(\epsilon\), then according to that assumption, we would have \(\epsilon n\) noisy samples and \((1 - \epsilon)n\) clean samples. It will immediately lead to the result. Thus, I do not understand the purpose of such a \emph{Bayes optimal} classifier used in here.}

% \begin{lemma}\label{lemma:2}(Cosine Similarity and Variance)
% The cosine similarity between the predictive probability vectors of any two classifiers \( h_{\gamma_l} \) and \( h_{\gamma_m} \) for an instance \( \mathbf{x}_i \) is inversely proportional to the variance of the predictive probability vectors.
% \end{lemma}
% \begin{proof}
% Let \( \mathbf{p}_l = h_{\gamma_l}(\mathbf{x}_i) \) and \( \mathbf{p}_m = h_{\gamma_m}(\mathbf{x}_i) \) be the predictive probability vectors for classifiers \( h_{\gamma_l} \) and \( h_{\gamma_m} \), respectively. The cosine similarity in ~\cref{eq:cosine_sim} could be rewritten as 
% \begin{equation*}
%     \cos(\theta) = \frac{\mathbf{p}_l^{\top} \mathbf{p}_m}{\|\mathbf{p}_l\| \|\mathbf{p}_m\|} 
% \end{equation*}
% , where \( \theta \) is the angle between these vectors.

% Consider the variance of a probability vector \( \mathbf{p} \), given by \( \operatorname{var}_{\tilde{\mathcal{D}}}(\mathbf{p}) = \mathbb{E}_{\tilde{\mathcal{D}}}[(\mathbf{p} - \mathbb{E}_{\tilde{\mathcal{D}}}[\mathbf{p}])^2] \) \cuong{Why is the definition of the variance changed?}. High variance in \( \mathbf{p}_l \) and \( \mathbf{p}_m \) suggests less alignment, i.e. lower \( \cos(\theta) \), while low variance indicates higher alignment, i.e. higher \( \cos(\theta) \)~\cite{ertoz2003finding}. Formally, the covariance matrix \( \Sigma_{lm} \) between \( \mathbf{p}_l \) and \( \mathbf{p}_m \) can be used to express cosine similarity as:
% \[
% \cos(\theta) = \frac{\sum_{i=1}^{n} \Sigma_{lm}[i, i]}{\sqrt{\sum_{i=1}^{n} \operatorname{var}(\mathbf{p}_l[i])} \sqrt{\sum_{i=1}^{n} \operatorname{var}(\mathbf{p}_m[i])}}
% \]
% demonstrating the inverse relationship between cosine similarity and variance.

% \cuong{Please explain the notations, e.g., \(n\) and \(\operatorname{var}(\mathbf{p}_{l}[i])\). Also, saying that variance in the denominator does not clearly explain the relation. Also, please provide details how to come up from the definition of cosine similarity to this expression. In addition, please address the counter-example I mentioned below.}

% \textit{Therefore, the cosine similarity between the predictive probability vectors of two classifiers for an instance is inversely proportional to the variance of these vectors, as shown by their covariance and variance properties.}
% \end{proof}
% % \begin{proof}
% % Let \( \mathbf{p}_l = h_{\gamma_l}(\mathbf{x}_i) \) and \( \mathbf{p}_m = h_{\gamma_m}(\mathbf{x}_i) \) be the predictive probability vectors from classifiers \( h_{\gamma_l} \) and \( h_{\gamma_m} \) for an instance \( \mathbf{x}_i \). The cosine similarity, denoted as \(\mathbf{s}_{i}\), is defined as the normalized dot product of these vectors as mentioned in~\cref{eq:cosine_sim}. To relate this to the variance of the predictive probability vectors, we first define the variance of a probability vector \( \mathbf{p} \) over a discrete set of outcomes \( \mathcal{C} \) as follows:

% % \begin{equation}
% % \operatorname{var}_{\tilde{\mathcal{D}}}(\mathbf{p}) = \sum_{c \in \mathcal{C}} p(c) \left( \mu - p(c) \right)^2,
% % \end{equation}
% % where \( \mu \) is the mean probability across all outcomes in \( \mathcal{C} \). Considering the variance of the vectors \( \mathbf{p}_l \) and \( \mathbf{p}_m \), a lower variance implies a higher concentration around their mean probabilities, leading to a higher alignment in their directional vectors. Conversely, a higher variance implies a more dispersed set of probabilities, leading to a lower alignment. Therefore, there exists an inverse relationship between the cosine similarity \( \mathbf{s}_i \) and the variance of the probability vectors \( \operatorname{var}(\mathbf{p}_l) \) and \( \operatorname{var}(\mathbf{p}_m) \).
% % \end{proof}

% \todo{The proof of \cref{lemma:2} is more convincing if there is a formulation of the cosine similarity \(\mathbf{x}_{i}\) in terms of the variance of predictive probability vectors. In the current form, it is unclear about the validity of the proof. In addition, there is one counter-example:
% \begin{itemize}
%     \item \(\mathbf{p}_{1} = \begin{bmatrix} 0.1 & 0.2 & 0.7\end{bmatrix}^{\top}\) and \(\mathbf{p}_{2} = \begin{bmatrix}0.2 & 0.1 & 0.7\end{bmatrix}^{\top}\): \(s = 0.98\) and \(\operatorname{var}(\mathbf{p}_{1}) = \operatorname{var}(\mathbf{p}_{2}) = 0.1031\)
%     \item \(\mathbf{p}_{1} = \begin{bmatrix}0.1 & 0.2 & 0.7\end{bmatrix}^{\top}\) and \(\mathbf{p}_{2} = \begin{bmatrix}0.2 & 0.7 & 0.1\end{bmatrix}^{\top}\): \(s = 0.43\) and \(\operatorname{var}(\mathbf{p}_{1}) = \operatorname{var}(\mathbf{p}_{2}) = 0.1031\)
% \end{itemize}
% In the above case, the variance is the same, but the cosine similarity is different. Or, is there any assumptions on \(\mathbf{p}\) that I missed?

% Besides, since sum of probability is 1, does that also mean \(\mu = \nicefrac{1}{C}\)?
% }
% \arpit{Assumption: In high-dimensional settings, the cosine similarity of predictive probability vectors from two classifiers for a given instance primarily reflects dimensional alignment, not the general spread (variance). This relationship depends on specific conditions like fixed mean or limited range in probability vectors and certain distribution patterns.\citeyear{ertoz2003finding}, yes \(\mu=\nicefrac{1}{C}\) considering unfirm case here}
% \cuong{It is still unclear for me. Could you formulate what you explain above in  a proper way?}\arpit{rewrote this part, in more variance terms}
% \begin{corollary}
% Given a data instance \( \mathbf{x} \) and an ensemble of classifiers \( \{h_{\gamma_k}\}_{k=1}^K \), 
% where each classifier \( h_{\gamma_k}: X \to \Delta_{C-1} \) acts as an unbiased estimator of the true label distribution \( P(Y | X) \), 
% it follows from~\cref{theorem:1} that the variance in the predictive probability vectors for instances with clean-labels will be lower than that for instances with noisy-labels. Formally, if we denote the predictive probability vector of classifier \( h_{\gamma_k} \) for \( \mathbf{x} \) as \( \mathbf{p}_k(\mathbf{x}) \) and the variance of these vectors as \( \operatorname{var}_{\tilde{\mathcal{D}}}(\mathbf{p}_k(\mathbf{x})) \), we have:
% \begin{equation}
% \operatorname{var}_{\tilde{\mathcal{D}}}(\mathbf{p}_k(\mathbf{x}_{\text{clean}})) < \operatorname{var}_{\tilde{\mathcal{D}}}(\mathbf{p}_k(\mathbf{x}_{\text{noisy}})),
% \end{equation}
% where \( \mathbf{x}_{\text{clean}} \) and \( \mathbf{x}_{\text{noisy}} \) represent instances with clean and noisy-labels, respectively. This inequality is a direct consequence of the established facts that the variance of the loss under clean-labels is zero (or near zero), thus similarity is high as discussed in~\cref{lemma:2}. In contrast, variance increases on label noise, so cosine similarity is low.
% \end{corollary}

% \cuong{The corollary is unclear since the variance in \cref{theorem:1} is induced by the perfect classifier, \(h^{*}(.)\) but not \(h_{\gamma}\). In addition, it is hard to comprehend the consequence of cosine similarity in \cref{lemma:2} to lead to the conclusion in the corollary.}
% \arpit{updated lemma 2 so it could be aligned with this corollary}

% \cuong{\cref{lemma:2} is still unclear, where as the part about the relation between the variation of \(h^{*}(.)\) and \(h(.)\) has not been explained properly.}
% Such high similarity of clean-label samples and low similarity of noisy-label samples between models is the main point being explored by \textbf{PASS}, as exemplified in~\cref{fig:cosinehypothesis} and proposed in~\cref{thm:remark}.

% %\gustavo{From what I understand of this proof, you assume that for clean-label samples, your K classifiers will approximate the true label distribution, while if you have noisy-label, then the K classifiers may approximate different distributions.  Maybe it's better to talk about variance in this case, where the clean-label approximates the true distribution with low variance, while the wrong label approximates the true distribution, but with larger variance.  Then you need to prove (or find a proof) that shows that the similarity between predictions for low variance distributions is larger than for high variance distributions. This looks like something you can find in statistics books.}

% % \begin{lemma}[Predictive Probability Agreement]
% % For a given dataset with clean and noisy-labels, let \( \mathbf{X} \) be the feature space and \( \mathbf{Y} \) be the label space. Consider a set of classifiers \( \{h_{\gamma_k}\}_{k=1}^K \) trained on \( \mathbf{X} \times \mathbf{Y} \) such that each classifier \( h_{\gamma_k} : \mathbf{X} \rightarrow \mathbf{Y} \) is an estimator of the true label distribution conditioned on the features \( P(\mathbf{Y} | \mathbf{X}) \). Define the predictive probability vector for a sample \( \mathbf{x}_i \) from classifier \( h_{\gamma_k} \) as \( \mathbf{p}_{\gamma_k}(\mathbf{x}_i) = h_{\gamma_k}(\mathbf{x}_i) \). Assume that for clean data samples, the classifiers are unbiased estimators of the true probability vector, and for noisy samples, the classifiers are biased estimators due to label noise.

% % The cosine similarity of predictive probability vectors for clean data samples \( (\mathbf{s}_{i,\text{clean}}) \) is statistically higher than that for noisy data samples \( (\mathbf{s}_{i,\text{noisy}}) \).
% % \end{lemma}

% % \begin{proof}
% % Let \( \mathbf{x}_i \) be a clean data sample and \( \mathbf{x}_j \) be a noisy data sample. Under the assumption of unbiased estimation for clean data, we have:
% % \[ E[\mathbf{p}_{\gamma_l}(\mathbf{x}_i)] = E[\mathbf{p}_{\gamma_m}(\mathbf{x}_i)] = \mathbf{p}(\mathbf{x}_i), \forall l, m \in \{1, \dots, K\}, \]
% % where \( \mathbf{p}(\mathbf{x}_i) \) is the true predictive probability vector for the clean sample \( \mathbf{x}_i \).

% % For noisy data, the expectation does not hold:
% % \[ E[\mathbf{p}_{\gamma_l}(\mathbf{x}_j)] \neq E[\mathbf{p}_{\gamma_m}(\mathbf{x}_j)], \text{ for some } l \neq m. \]

% % Given that cosine similarity for two predictive probability vectors \( \mathbf{p}_{\gamma_l}(\mathbf{x}_i) \) and \( \mathbf{p}_{\gamma_m}(\mathbf{x}_i) \) is defined as:
% % \[ \mathbf{s}_i = \frac{\mathbf{p}_{\gamma_l}(\mathbf{x}_i)^\top \mathbf{p}_{\gamma_m}(\mathbf{x}_i)}{\|\mathbf{p}_{\gamma_l}(\mathbf{x}_i)\| \|\mathbf{p}_{\gamma_m}(\mathbf{x}_i)\|}, \]
% % where \( \|\cdot\| \) denotes the Euclidean norm.

% % For clean data, \( \mathbf{s}_{i,\text{clean}} \) can be expressed as:
% % \[ \mathbf{s}_{i,\text{clean}} = \frac{\mathbf{p}(\mathbf{x}_i)^\top \mathbf{p}(\mathbf{x}_i) + \epsilon_{lm}}{\|\mathbf{p}(\mathbf{x}_i) + \delta_{l}\| \|\mathbf{p}(\mathbf{x}_i) + \delta_{m}\|}, \]
% % where \( \epsilon_{lm} \) represents the small noise due to estimator variance, and \( \delta_l, \delta_m \) represent small perturbations due to random effects or model imperfections.

% % For noisy data, \( \mathbf{s}_{i,\text{noisy}} \) is expected to be:
% % \[ \mathbf{s}_{i,\text{noisy}} < \mathbf{s}_{i,\text{clean}}, \]
% % because the difference in expected probability vectors due to label noise leads to an increased likelihood of \( \epsilon_{lm} \) being larger and the vectors \( \mathbf{p}_{\gamma_l}(\mathbf{x}_j) \) and \( \mathbf{p}_{\gamma_m}(\mathbf{x}_j) \) diverging.

% % The Central Limit Theorem further supports this by suggesting that, with a sufficiently large number of classifiers, the distribution of the predictive probability vectors' cosine similarity will converge to a normal distribution centered around the true cosine similarity value for clean data and a different, likely lower mean for noisy data.

% % Hence, \( \mathbf{s}_{i,\text{noisy}} \) is statistically lower than \( \mathbf{s}_{i,\text{clean}} \), which concludes the proof.
% % \end{proof}

