\section{Benchmarks for embedded StateDB in HLF}\label{sec4}

\subsection{Performance benchmark evaluation configuration}

Performance evaluations of HLF with the new embedded StateDBs were carried out using the Hyperledger Caliper tool\cite{l35}, which offers universal benchmarks for several blockchain platforms, including Hyperledger Fabric.

This utility supports the capture of basic blockchain performance \cite{l45} metrics. Due to the large number of metrics to compare, in order to simplify for better visualization in our work we will consider only write transactions throughput.

The performance benchmarks are based on code samples from \cite{l36}.

The configuration of the peer machine on which the performance benchmarks were run is: OS: Ubuntu 20.04.4 LTS, CPU(s): 4, RAM: 16, Total SSD memory: 60 GB, Max. bandwidth (read | write): 30 MB/s | 30 MB/s, Max. IOPS (read | write): 2000 | 2000, CPU family: 6, Model: 106, Model name: Intel Xeon Processor (Icelake), Thread(s) per core: 2, Core(s) per socket: 2, Socket(s): 1.

FixedAsset chaincode methods \cite{l38} were developed in Golang to evaluate the performance of various interactions with StateDB.

\subsection{Create-Asset performance benchmark} \label{4c}

The scenario under study is aimed at writing to StateDB. The write performance benchmark consists of Submit calls to the CreateAsset method of the FixedAsset chaincode. We'll refer to such calls as transactions. This chaincode was deployed in independent HLF networks per each considered StateDB: RocksDB, bboltdb or BadgerDB.

Each transaction writes one key-value pair to the StateDB. It lasts 5 minutes for each client for each type of transaction: 100 bytes / 1000 bytes / 4000 bytes / 8000 bytes / 16000 bytes / 24000 bytes / 32000 bytes / 64000 bytes. 

Fig.\ref{pic305} compares the throughput of HLF transactions for embedded StateDBs.
It shows that all StateDB compete with each other almost on an equal footing. However, StateDB BadgerDB has a slight TPS edge for all types of transactions.
Additionally, for a transaction size of 64KB, BadgerDB's TPS is much greater than goleveldb's.
In turn, RocksDB showed equal performance with goleveldb and achieved a clear advantage for a transaction size of 64KB. Similar to this, bbolt only demonstrated a marginal benefit over goleveldb for transactions of 32KB and 64KB in size.

We  can therefore say that BadgerDB demonstrated a higher TPS rate in writing than other StateDBs.

%The RAM utilization graph (Fig. \ref{pic308})demonstrates the increasing memory usage of RocksDB and BadgerDB. The speed of writing to the disk of the peer was higher for goleveldb and bbolt (Fig.\ref{pic309}).


\begin{figure}[htbp]
\centerline{\includegraphics[width = 9cm, height = 2.7cm]{figures/create-assetTPS.png}}
\caption{Create-Asset performance benchmark results (transactions per second rate).}
\label{pic305}
\end{figure}
%
%\begin{figure}[htbp]
%\centerline{\includegraphics[width = 9cm, height = 2.7cm]{figures/create-assetRAM.png}}
%\caption{Create-Asset performance benchmark results (average RAM usage rate).}
%\label{pic308}
%\end{figure}
%
%\begin{figure}[htbp]
%\centerline{\includegraphics[width = 9cm, height = 2.7cm]{figures/create-assetTotalDiskWrite.png}}
%\caption{Create-Asset performance benchmark results (average Disk Writes per second rate).}
%\label{pic309}
%\end{figure}


