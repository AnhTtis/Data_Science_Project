\section{Preliminaries}
\label{sec:preliminaries}

% \subsection{Acronyms/Initialisms}
\input{sections/acronym}

\subsection{Notation}

% % \urg{Fill in the notation}

% \old{The $n$-dimensional real Euclidean space is $\R^n$. The set of natural numbers is $\N$, and its subset of natural numbers between $1$ and $N$ is $1..N$. The set of $n$-multi-indices is $\N^n$. The degree of a multi-index $\alpha \in \N^n$ is $\abs{\alpha} = \sum_{i=1}^n \alpha_i$. 
% A monomial is a term $x^\alpha = \prod_{i=1}^n x_i^{\alpha_i}$ with degree $\deg{x^\alpha} = \abs{\alpha}$. A polynomial $p(x) \in \R[x]$ {is a linear combination of monomials} %may be uniquely represented in terms of multi-indices $\alpha$ and coefficients $p_\alpha$ as 
% $p(x) = \sum_{\alpha \in S} p_\alpha x^\alpha$ {with finite support ${S \subset} \N^n$ and degree $\deg(p) = \max_{\alpha \in S}|\alpha|$}. The degree of a vector of polynomials ($f \in (\R[x])^N$) is the maximum degree of its coordinates ($\deg f = \max_{i\in 1..N} \deg f_i$). The vector space of polynomials of degree at most $d$ is 
% $\R[x]_{\leq d}$ and its dimension is $\binom{n+d}{d}$.
% %The set of \ac{SOS} polynomials is $\Sigma[x],$ and its degree-$2d$-bounded subset is $\Sigma[x]_{\leq 2d} \subset \R[x]_{\leq 2d}$. 
% The \acf{SOC} (or Lorentz cone)  is ${\mathbb{L}^n = \{(y, s) \in \R^n \times \R : \ s \geq \norm{y}_2\}}$, where $\norm{{y}}_2 = ({y}_1^2 + \ldots + {y}_n^2)^{1/2}$ denotes the Euclidean norm.}

The real Euclidean space with $n$ dimensions is $\R^n$. The set of natural numbers is $\N$, the subset of natural numbers between $a$ and $b$ is $a..b \subset \N$, and the set of $n$-dimensional multi-indices is $\N^n$. The degree of a multi-index $\alpha \in \N^n$ is $\abs{\alpha} = \sum_{i=1}^n \alpha_i$. A monomial $x^\alpha = \prod_{i=1}^n x_i^{\alpha_i}$ has degree $\deg{x^\alpha} = \abs{\alpha} = \sum_{i=1}^n \alpha_i$. A polynomial $p(x) \in \R[x]$ is a linear combination of monomials
$p(x) = \sum_{\alpha \in S} p_\alpha x^\alpha$ {with finite support ${S \subset} \N^n$ and degree $\deg(p) = \max_{\alpha \in S}|\alpha|$}. The set $\R[x]_{\leq d}$of polynomials with degree at most $d$ forms a vector space of dimension is $\binom{n+d}{d}$. The $n$-dimensional \ac{SOC} is ${\mathbb{L}^n = \{(y, s) \in \R^n \times \R : \ s \geq \norm{y}_2\}}$, where $\norm{{y}}_2 = ({y}_1^2 + \ldots + {y}_n^2)^{1/2}$ is the Euclidean norm.

The topological dual of a Banach space ${B}$ is ${B}^*$. Given a topological space $X$, the set of all continuous functions over $X$ is $C(X)$, and the subcone of nonnegative continuous functions is $C_+(X) \subset C(X)$. The subset of $C(X)$ that is $k$-times continuously differentiable is $C^k(X)$. The cone of nonnegative Borel measures supported in $X$ is $\Mp{X}$. The space of signed Borel measures supported in $X$ is $\mathcal{M}(X) = \Mp{X} - \Mp{X}$. The sets $C(X)$ and $\mathcal{M}(X)$ are topological duals when $X$ is compact with a duality product $\inp{\cdot}{\cdot}$ via Lebesgue integration: for $f \in C(X), \ \mu \in \mathcal{M}(X), \inp{f}{\mu} = \int_{X} f(x) d \mu(x)$. The duality product over $C(X)$ and $\mathcal{M}(X)$ induces a duality pairing between $C_+(X)$ and $\mathcal{M}_+(X)$. We will slightly abuse notation to extend this duality product to Borel measurable functions $f$ as $\inp{f}{\mu} = \int_X f(x) d\mu(x)$.



The indicator function of $A \subseteq X$ is $I_A: X \rightarrow \{0, 1\}$ with value $1$ exactly on $A$. %, and has the values $I_A(x)=0$ for $x \not\in A$ and $I_A(x)=1$ for $x \in A$. 
The measure of $A \subset X$ with respect to $\mu \in \Mp{X}$ is $\mu(A) = \inp{I_A}{\mu}$, and the mass of $\mu$ is $\mu(X) = \inp{1}{\mu}$. The measure $\mu$ is a probability measure if $\mu(X) = 1$, under which $(X,\mu)$ is a probability space). 
% The mass of a measure $\mu \in \Mp{X}$ is $\mu(X) = \inp{1}{\mu}$, and $\mu$ is a probability measure if $\mu(X) = 1$ {(then $(X,\mu)$ is a probability space)}. 

The pushforward of a measure $\mu \in \Mp{X}$ along a function ${V}: X \rightarrow Y$ is ${V}_\# \mu \in \Mp{Y}$ such that $\forall {\varphi} \in C(Y), \ \inp{{\varphi}}{{V}_\# \mu} = \inp{{\varphi\circ V}}{\mu}$. 

If moreover $\mu$ is a probability measure, then $V$ is called a \ac{RV} and one can define the expected value $\mathbb{E}[V] = \inp{V}{\mu}$ and probability $\mu(V \in A) = \inp{I_A}{V_\#\mu}$.
The support of a measure $\mu$ {(resp. \ac{RV} $V$)} is the set of all points $x$ in which every open neighborhood $N_x$ of $x$ obeys $\mu(N_x) > 0$ {(resp. $V_\#\mu(N_x)>0$)}.
The Dirac probability $\delta_{\overline{x}}$ supported at ${\overline{x} \in X}$ is such that $\inp{f}{\delta_{\overline{x}}} = f({\overline{x}})$ for all $f \in C(X)$. For every $\mu \in \Mp{X}$ and $\nu \in \Mp{Y}$, the product $\mu \otimes \nu$ is the unique measure satisfying $\forall A \subseteq X, \ B \subseteq Y: \ (\mu \otimes \nu)(A \times B) = \mu(A) \nu(B)$. 
% The pushforward of a map $Q: \rw{S}\rightarrow Y$ along a measure $\mu\rev{(\rw{s})}$ is $Q_\# \mu\rev{(y)}$, which satisfies 
% $\forall f \in C(Y): \ \inp{f(y)}{Q_\# \mu(y)} = \inp{f(Q(\rw{s}))}{\mu(\rw{s})}$.

The operator $\wedge$ {(resp. $\vee$)} will denote the {min. (resp. max.)} of two quantities as $a \wedge b = \min(a,b)$ {(resp. $a \vee b = \max(a,b)$)}. Given a linear operator $\Lie: X \rightarrow Y$, the adjoint of $\Lie: X \rightarrow Y$  is $\Lie^\dagger: Y^* \rightarrow X^*$.
% The adjoint of a linear operator $\Lie: X \rightarrow Y$ is $\Lie^\dagger: Y^* \rightarrow X^*$.}
% For any two measures $\mu, \nu \in \Mp{X}$, t
The measure $\nu \in \Mp{X}$ is absolutely continuous to $\mu \in \Mp{X}$ ($\nu \ll \mu$) if $\forall A \subseteq X: \ \mu(A) = 0 \implies \nu(A) = 0$. {If} $\nu \ll \mu$, {then} there exists a nonnegative density $\rho: X \rightarrow \R^+$ such that $\forall f \in C(X): \inp{f(x)}{\nu(x)} = \inp{f(x)\rho(x)}{\mu(x)}$. This nonnegative density is also called the Radon-Nikodym derivative $\rho = \frac{d \nu}{d\mu}$. The measure $\mu$ dominates $\nu$ ($\nu \leq \mu$) if $\forall A \subseteq X: \nu(A) \leq \mu(A)$. Domination $\nu \leq \mu$ will occur if  $\nu \ll \mu$ and $\frac{d \nu}{d\mu} \leq 1$.

\subsection{Probability Tail Bounds and Value-at-Risk}
Define {a univariate \ac{RV} $V : (\Omega,\mathbb{P}) \rightarrow \mathbb{R}$ }% $\xi(\omega) \in \Mp{\R}$ as a univariate probability measure 
with finite first and second moments {($\abs{\mathbb{E}[V]} < \infty$, $\mathbb{E}[V^2] < \infty$, note that $V^\alpha$ is a \ac{RV} for any $\alpha \in \N$).} %($\inp{1}{\xi} = 1$, $\abs{\inp{\omega}{\xi}} < \infty,$ $\inp{\omega^2}{\xi} < \infty$). 
We will define the $\epsilon$-\ac{VAR} of {$V$} %$\xi$ 
for $\epsilon \in [0, 1]$ as 
%\begin{align}
%    &VaR_\epsilon(\xi) = \sup\left\{
%    \lambda \in \R \; | \; \xi([\lambda,\infty)) \geq \epsilon
%    \right\}. \label{eq:var_center}
%\end{align}
\begin{align}
    &\mathrm{VaR}_\epsilon(V) = \sup\left\{
    \lambda \in \R \; | \; \mathbb{P}(V \geq \lambda) \geq \epsilon
    \right\}. \label{eq:var_center}
\end{align}

Equation \eqref{eq:var_center} defines $\mathrm{VaR}_\epsilon({V})$ as the $(1-\epsilon)$ quantile statistic of ${V}$.
We now review two methods to upper-bound the \ac{VAR}: Concentration bounds and \ac{CVAR}.

\subsubsection{Concentration Bounds/Minimax}

This approach uses worst-case (minimax) bounds on the \ac{VAR} given the first and second moments of {$V$} \cite{dupacova1987minimax}. Letting %$\sigma^2 = \inp{\omega^2}{\xi} - \inp{\omega}{\xi}^2$ be the variance of the $\xi$, the Cantelli \cite{cantelli1929sui} \ac{VAR} upper-bound is
{$\sigma^2 = \mathbb{E}[V^2] - \mathbb{E}[V]^2$} be the variance of {$V$}, the Cantelli \cite{cantelli1929sui} \ac{VAR} upper-bound is
\begin{subequations}
\label{eq:tail_bounds}
\begin{align}
    \mathrm{VaR}_\epsilon({V}) &\leq \sigma \sqrt{(1/\epsilon)- 1} + {\mathbb{E}[V] = \mathrm{VC}_\epsilon(V)}. \label{eq:var_cant}\\    \intertext{
    {When $V$} is unimodal and $\epsilon \leq 1/6$, the sharper \ac{VP} bound may be applied as in \cite{vysochanskij1980justification}}
    \mathrm{VaR}_\epsilon({V}) &\leq \sigma \sqrt{4/(9\epsilon) - 1} + {\mathbb{E}[V] = \mathrm{VP}_\epsilon(V)}. \label{eq:var_vp}
\end{align}
\end{subequations}


% \begin{subequations}
% \label{eq:tail_bounds}
% \begin{align}
% \intertext{The Cantelli bound  for \ac{VAR} is \cite{cantelli1929sui}} 
%     VaR_\epsilon(\xi) &\leq \sigma \sqrt{1/(\epsilon)- 1}+\inp{\omega}{\xi} = VaR_\epsilon^{cant}(\xi). \label{eq:var_cant}\\    \intertext{
%     The \ac{VP} bound for the \ac{VAR} is \cite{vysochanskij1980justification}}
%     VaR_\epsilon(\xi) &\leq \sigma \sqrt{4/(9\epsilon) - 1} +\inp{\omega}{\xi}= VaR_\epsilon^{VP}(\xi). \label{eq:var_vp}
% \end{align}
% \end{subequations}
\subsubsection{Conditional Value-at-Risk / Expected Shortfall}

\begin{defn}
\label{defn:cvar}
The \ac{CVAR} is the mean value of ${V}$ such that ${V}$ is greater than or equal to the \ac{VAR} \cite[Equation 3]{rockafellar2002conditional}:
%\begin{align}
%    CVaR_\epsilon(\xi) = \frac{1}{\epsilon} \int_{\omega \geq VaR_\epsilon(\xi)} \omega d\xi(\omega). \label{eq:cvar}
%\end{align}
\begin{align}
    \mathrm{ES}_\epsilon(V) = \frac{1}{\epsilon} \, \mathbb{E}\left[I_{V \geq \mathrm{VaR}_\epsilon(V)} \, V\right]. \label{eq:cvar}
\end{align}

\end{defn}

\begin{rmk}
    A consequence of \eqref{eq:cvar} is {$\mathrm{ES}_\epsilon(V) \geq \mathrm{VaR}_\epsilon(V)$ for all \ac{RV} $V$} and probability values $\epsilon \in (0, 1]$. {Indeed, one has $\mathbb{E}[I_{V \geq \mathrm{VaR}_\epsilon(V)} \, V] \geq \mathrm{VaR}_\epsilon(V) \, \mathbb{E}[I_{V \geq \mathrm{VaR}_\epsilon(V)}] = \mathrm{VaR}_\epsilon(V) \, \mathbb{P}(V \geq \mathrm{VaR}_\epsilon(V)) \geq \epsilon \, \mathrm{VaR}_\epsilon(V)$ by definition of the \ac{VAR}.}
\end{rmk}

We now list other definitions for the \ac{CVAR}.

\begin{lem}[Equations 4 and 5 of \cite{rockafellar2002conditional}] {Defining the positive part $f_+ = f\vee 0$ of a function $f$, the} \ac{CVAR} is the solution to the parametric problem
%\begin{align}
%    CVaR_\epsilon(\xi) = \min_{\alpha \in \R} \alpha + \frac{1}{\epsilon} \int_{\omega > \alpha} (\omega-\alpha) d\xi(\omega). \label{eq:cvar_param}
%\end{align}
\begin{align}
    \mathrm{ES}_\epsilon(V) = \min \left\{ \lambda + \frac{1}{\epsilon} \mathbb{E}\left[(V-\lambda)_+\right] \; \middle| \; \lambda \in \mathbb{R} \right\}. \label{eq:cvar_param}
\end{align}

\end{lem}

\begin{lem}[Equation 5.5 of \cite{follmer2010convex}] {Denoting $\psi = V_\#\mathbb{P}$ as the probability law of $V$ (equivalently, one can write $V \sim \psi$) and $\mathrm{id}_\mathbb{R} = \mathbb{R} \ni s \mapsto s$ as the identity map, the} \ac{CVAR} is the solution to the following optimization program in measures: \label{lem:cvar_abscont}
\begin{subequations}
\label{eq:cvar_abscont}
%\begin{align}
%    CVaR_\epsilon(\xi) = & \sup_{\psi \in \Mp{\R}} \inp{\omega}{\psi} \\
%    & \psi \ll \xi \label{eq:cvar_abscont_def}\\
%    & \frac{d\psi}{d\xi} \leq 1/\epsilon \label{eq:cvar_radon_nikodym} \\
%    & \inp{1}{\psi} = 1. \label{eq:cvar_prob}
%\end{align}
\begin{align}
    \mathrm{ES}_\epsilon(V) = & \sup_{\nu \in \Mp{\R}} \inp{\mathrm{id}_\mathbb{R}}{\nu} \label{eq:cvar_abscont_obj}\\
    & \nu \ll \psi \label{eq:cvar_abscont_def}\\
    & \frac{d\nu}{d\psi} \leq 1/\epsilon \label{eq:cvar_radon_nikodym} \\
    & \inp{1}{\nu} = 1. \label{eq:cvar_prob}
\end{align}

    \end{subequations}
\end{lem}

% {Matt: what is $q$, what is $\nu$?}
The objective in \eqref{eq:cvar_abscont_obj} is the mean of $\nu$.
Equation \eqref{eq:cvar_abscont_def} imposes that ${\nu}$ is absolutely continuous with respect to ${\psi}$. {From equation \eqref{eq:cvar_radon_nikodym}} ${\nu}$ possesses a Radon-Nikodym derivative  that has value $\leq 1/\epsilon$ at {any realization}. Equation \eqref{eq:cvar_prob} enforces that ${\nu}$ is a probability measure. 

\begin{rmk}
    The equation in \eqref{eq:cvar_abscont} is modified from \cite{follmer2010convex} to possess a supremization objective and to explicitly include the mass constraint \eqref{eq:cvar_prob}.
\end{rmk}

\begin{rmk}
When $\mathrm{VaR}_\epsilon({V})$ is not an atom of ${\psi}$, an analytical expression may be developed for ${\nu}$ solving \eqref{eq:cvar_abscont} as:
%\begin{align}
%    \psi^*: \ \frac{d\psi^*}{d\xi}(q) = \begin{cases} 1/\epsilon & q \geq VaR_{\epsilon}(\xi) \\ 0 & \text{Else}.\end{cases}
%\end{align}
\begin{align}
    \nu^* = (1/\epsilon) \, I_{[\mathrm{VaR}_\epsilon(V),\infty)} \; \psi. \label{eq:nu_explicit}
\end{align}

Similar principles may be used to derive ${\nu^*}$ when ${\psi}$ has atomic components, but this may lead to splitting an atom.
\end{rmk}

Figure \ref{fig:es_bell} summarizes this subsection, with an example where $\psi$ is the unit normal distribution $V$ ($\mathbb{E}[V] = 0$, $\mathbb{E}[V^2] = 1$).  The blue curve is the probability density of $V$. The black area has a mass of $\epsilon = 0.1$, and the left edge of the black area is $\textrm{VaR}_{0.1}(V) = 1.2819$. The red curve is $\epsilon \nu^* = I_{[\mathrm{VaR}_\epsilon(V),\infty)} \; \psi$ from \eqref{eq:nu_explicit}. The green dotted line is the $\textrm{ES}_{0.1}(V) = 1.7550$. The \ac{VP} and Cantelli bounds are $\textrm{VP}_{0.1}(V)=1.8559$ and $\textrm{VC}_{0.1}(V) = 3$ respectively.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/es_bell_higlight.pdf}
    \caption{\ac{VAR} and \ac{CVAR} of a unit normal distribution at $\epsilon = 0.1$}
    \label{fig:es_bell}
\end{figure}

\subsection{Stochastic Processes and Occupation Measures}
\label{sec:prelim_sde}

Let $\{\mu_t\} \in \Mp{X}$ be a time-indexed sequence of probability distributions. Define $\mathcal{T}_\tau$ as a time-shifting (Feller) semigroup operator acting as $\mathcal{T}_\tau \mu_t= \mu_{t+\tau}$. The \textit{generator} $\Lie_\tau$ of a stochastic processes associated with the distributions $\{\mu_t\}$ is a linear operator satisfying (for any test function $v(t, x) \in C([0, T] \times X)$ and in the domain of $\Lie$):
\begin{equation}
    \Lie_\tau v= \lim_{\tau' \rightarrow \tau} (\inp{v(t+\tau', x)}{\mu_{t+\tau'}}- \inp{v(t,x)}{\mu_t})/\tau'. \label{eq:generator_lim}
\end{equation}

A discrete-time Markov stochastic process with parameter distribution $\xi(\lambda) \in \Mp{\Lambda}$ and time-step $\Delta t> 0$ has a law and associated generator of 
\begin{align}
    x[t+\Delta t] &=  f(t, x[t], \lambda[t]), \qquad \lambda[t] \sim \xi \\
    \Lie_{\Delta t} v &= \left(\int_{\Lambda}v(t+\Delta t, f(t, x, \lambda)) d \xi(\lambda) - v(t, x)\right)/\Delta t. \label{eq:generator_discrete}
\end{align}
The domain of $\Lie_{\Delta t}$ in \eqref{eq:generator_discrete} is $\cs = C([0, T] \times X)$.

\ito \acp{SDE} are the unique class of continuous-time stochastic processes that are nonanticipative, have independent increments, and possess continuous sample paths \cite{oksendal2003stochastic}. Every \ito \ac{SDE} has a definition in terms of a (possibly nonunique) drift function $f$, a diffusion function $g$, and an $n$-dimensional Wiener process $W$ as 
\begin{equation}
\label{eq:sde}
    dx = f(t, x) dt + g(t, x) d{W}.
\end{equation}

This paper will involve stochastic trajectories evolving in a compact set $X$, starting within an initial set $X_0$ at time $0$. Letting $\tau_X$ be a stopping time (\ac{RV}) associated with the first touch of the boundary $\partial X$, the strong \ac{SDE} solution of \eqref{eq:sde} in times $t \in [0, T]$ starting at an initial point $x(0)\in X_0$ is
\begin{equation}
\label{eq:sde_sol}
    x(t) = x(0) + \int_{t=0}^{\tau_X \wedge T} f(t, x) dt + \int_{t=0}^{\tau_X \wedge T} g(t, x) d{W}.
\end{equation}

Strong solutions of \eqref{eq:sde_sol} (sequence of state-probability-distributions $\{\mu_t\}$ describing $x(t)$) are unique if there exists $C, D>0$ such that the following Lipschitz and Growth conditions hold for all $(t, x, x') \in [0, T] \times X^2$ \cite{oksendal2003stochastic}:
 \begin{align}
    D \norm{x-x'}_2 &\geq \norm{f(t,x)-f(t,x')}_2 + \norm{g(t, x)-g(t,x')}_2  \nonumber \\
     C (1+\norm{x}_2) & \geq \norm{f(t,x)}_2 + \norm{g(t, x)}_2. \label{eq:lip_growth}
 \end{align}
These Lipschitz and Growth conditions will be satisfied if $X$ is compact and $(f, g)$ are locally Lipschitz. The generator associated to \eqref{eq:sde} with domain $C^{1, 2}([0, T] \times X)$ is
\begin{equation}
\label{eq:lie}
    \Lie v(t, x) = \partial_t v + f(t, x) \cdot \nabla_x v + \frac{1}{2} g(t, x)^T \left(\nabla^2_{xx}v\right)  g(t, x).
\end{equation}
% We note that L\'{e}vy processes are Feller continuous-time stochastic processes that are nonanticipative and have independent increments
In the rest of this paper, we denote the domain of the generator $\Lie$ by $\cs$, and will refer to stochastic processes by their generators ($x(t)$ or $\{\mu_t\}$ satisfy the stochastic process of $\Lie$ in time $T \wedge \tau_X$).


Let $\mu_0 \in \Mp{X_0}$ be an initial distribution, $t' \in [0, T]$ be a terminal time, and $\tau = t' \wedge \tau_X$ be an associated stopping time. The occupation measure $\mu \in \Mp{[0, T] \times X}$ and stopping measure $\mu_\tau \in \Mp{[0, T] \times X}$ of the stochastic processes $\Lie$ w.r.t. $\mu_0$ is $\forall A \subseteq [0, T], B \subseteq X$:
\begin{subequations}
\begin{align}
    \label{eq:avg_free_occ}
    \mu(A \times B) &= \int_{X_0} \int_{t=0}^\tau  I_{A \times B}\left(t, x(t \mid x_0)\right) dt \, d\mu_0(x_0) \\
    \label{eq:avg_free_stop}
    \mu_\tau(A \times B) &= \int_{X_0} I_{A \times B}\left(\tau, x(\tau \mid x_0)\right) \, d\mu_0(x_0).
\end{align}
\end{subequations}

The measures $(\mu_0, \mu, \mu_\tau)$ together obey a martingale relation \cite{rogers2000diffusions}:
\begin{align}
\label{eq:dynkin_strong}
    \inp{v}{\mu_\tau} &= \inp{v(0, x)}{\mu_0(x)} + \inp{\Lie v}{\mu} & & \forall v \in \cs,
\end{align}
which can be equivalently expressed in shorthand form as
\begin{align}
    \mu_\tau &= \delta_{0} \otimes \mu_0 + \Lie^\dagger \mu. \label{eq:dynkin}
\end{align}

The martingale relation in \eqref{eq:dynkin} is known as Dynkin's formula \cite{dynkin1965markov} when $\Lie$ is the generator for \iac{SDE}. A \textit{relaxed occupation measure} is a tuple $(\mu_0, \mu, \mu_\tau)$ satisfying \eqref{eq:dynkin} with $\inp{1}{\mu_0} = 1$.

An optimal stopping problem to maximize the expectation of the reward $p(x)$ along trajectories of $\Lie$ is $P^* = \sup_{t' \in [0, T]} \inp{p(x)}{\mu_{t'}(x)}$. 
This stopping problem may be upper-bounded by an infinite-dimensional \ac{LP} in measures
\begin{subequations}
\label{eq:peak_meas}
\begin{align}
p^* = & \ \sup \quad \inp{p}{\mu_\tau} \label{eq:peak_meas_obj} \\
    & \mu_\tau = \delta_0 \otimes\mu_0 + \Lie^\dagger \mu \label{eq:peak_meas_flow}\\
    & \inp{1}{\mu_0} = 1 \label{eq:peak_meas_prob}\\
    & \mu, \mu_\tau \in \Mp{[0, T] \times X}.  \notag
\end{align}
\end{subequations}

The upper-bound is tight $(p^* = P^*)$ when $p$ is continuous, $[0, T] \times X$ is compact, and closure conditions hold on the generator $\Lie$ \cite{cho2002linear} (to be reviewed in Section \eqref{sec:assum}). \ac{SDE} trajectories under Lipschitz and Growth \eqref{eq:lip_growth} conditions will satisfy these requirements.

% % \urg{Rephrase this in terms of generators, and give some examples}
% {\color{teal}

% % Let $(\Omega, \mathcal{F}, \mathcal{P})$ be a probability space with time-indexed filtration $\mathcal{F}_t$ \urg{Matt: do we really need to mention $\mathcal{F}_t$? At this point we have not introduced the notion of $\sigma$-algebra, not to mention a filtration of $\sigma$-algebras...},  

% % Let $X \subset \R^n$  be a compact set, and {$W$ be an} $n$-dimensional Wiener process.
% % An \ito \ac{SDE} with a drift function $f$ and diffusion function $g$ is \cite{gardiner2009stochastic}
% % Let $x(t)$ follow an It\^{o} SDE where $w$ represents brownian motion (Wiener process),
% \begin{equation}
% \label{eq:sde}
%     dx = f(t, x) dt + g(t, x) d{W}.
% \end{equation}

% % In this paper, trajectories will start from an initial set $X_0 \subseteq X$ and will remain within $X$ in times $t \in [0, T]$ by virtue of stopping at the boundary $\partial X$.
% % Define $\tau_X$ as a stopping time (random variable) corresponding to the time at which the process \eqref{eq:sde} starting from $X_0$ first touches the boundary $\partial X$ for the first time. A process of \eqref{eq:sde} starting from an initial condition $x(0) \in X_0$ in times $t \in [0, T]$ is
% % \begin{equation}
% % \label{eq:sde_sol}
% %     x(t) = x(0) + \int_{t=0}^{\tau_X \wedge T} f(t, x) dt + \int_{t=0}^{\tau_X \wedge T} g(t, x) d{W}.
% % \end{equation}
 
%  % Strong solutions of \eqref{eq:sde_sol} are unique
%  % \urg{Matt: unique up to the realization of $w$, right? Here $x(t)$ is still a \ac{RV}, even when $x(0)$ is deterministic.} 
% %  if there exists finite constants $C, D > 0$ such that for all $(t, x, x') \in [0, T] \times X^2$,  the following Lipschitz and Growth conditions hold \cite{oksendal2003stochastic}:
% %  \begin{align}
% %     D \norm{x-x'}_2 &\geq \norm{f(t,x)-f(t,x')}_2 + \norm{g(t, x)-g(t,x')}_2  \nonumber \\
% %      C (1+\norm{x}_2) & \geq \norm{f(t,x)}_2 + \norm{g(t, x)}_2. \label{eq:lip_growth}
% %  \end{align}
% % The Lipschitz and Growth conditions will hold if $(f,g)$ are locally Lipschitz and the set $X$ is compact.
% % Distributions of the densities of \eqref{eq:sde_sol} may be computed by solving a Fokker-Planck equation with absorbing boundary conditions on $\partial X$ \cite{furth1917einige, hwang2014fokker}.
 
% % The generator $\Lie$ associated with the \ac{SDE} is a linear operator that satisfies $\forall v(t,x) \in C^2([0, T] \times X)$ \cite{oksendal2003stochastic}:
% % %  For every function $v \in C^2([0, T] \times X)$,  in \eqref{eq:sde} satisfies,
% % \begin{equation}
% % \label{eq:lie}
% %     \Lie v(t, x) = \partial_t v + f(t, x) \cdot \nabla_x v + \frac{1}{2} g(t, x)^T \left(\nabla^2_{xx}v\right)  g(t, x).
% % \end{equation}
 
% % The $\nabla_{xx}^2 v$ term arises from the \ito Lemma. Let $\tau$ be a stopping time adapted to the given filtration, defined by $\tau = \tau_X \wedge T$. The occupation {and stopping} measure{s} $\mu {,\mu_\tau} \in \Mp{[0, T] \times X}$ corresponding to the stopping time $\tau$, initial distribution $\mu_0 \in \Mp{X_0}$, and dynamics \eqref{eq:sde} {are,} $\forall A \subseteq [0, T], \  B \subseteq X$:
% % \begin{subequations}
% % \begin{equation}
% %     \label{eq:avg_free_occ}
% %     \mu(A \times B) = \int_{X_0} \int_{t=0}^\tau  I_{A \times B}\left(t, x(t \mid x_0)\right) dt \, d\mu_0(x_0).
% % \end{equation}
% % \begin{equation}
% %     \label{eq:avg_free_stop}
% %     \mu_\tau(A \times B) = \int_{X_0} I_{A \times B}\left(\tau, x(\tau \mid x_0)\right) \, d\mu_0(x_0).
% % \end{equation}
% % \end{subequations}

% The initial measure $\mu_0 \in \Mp{X_0}$, the occupation measure $\mu$ from  \eqref{eq:avg_free_occ}, and the terminal measure $\mu_\tau \in \Mp{[0, T] \times X}$ defined by following the \ac{SDE} \eqref{eq:sde} from initial conditions $x_0 \sim \mu_0$ until the stopping time $\tau$, are all related by Dynkin's formula \cite{dynkin1965markov}
% \begin{align}
% \label{eq:dynkin_strong}
%     \inp{v}{\mu_\tau} &= \inp{v(0, x)}{\mu_0(x)} + \inp{\Lie v}{\mu} & & \forall v \in C^2.
% \end{align}

% Dynkin's formula is an \ac{SDE} generalization of the Liouville equation for \acp{ODE}. Equation \eqref{eq:dynkin_strong} may be equivalently written in weak form as
% \begin{align}
%     \mu_\tau &= \delta_{0} \otimes \mu_0 + \Lie^\dagger \mu. \label{eq:dynkin}
% \end{align}
 
 
% An expectation-maximizing optimal stopping problem for the \ac{SDE} in \eqref{eq:sde} with a reward function $p$ in the region $[0, T] \times X$, when starting at the initial condition $x(0)\sim\mu_0 \in \Mp{X_0}$, is $P^* = \sup \E_{\mu_0}[p(x(\tau))]$. The work in \cite{cho2002linear} presents an infinite-dimensional \ac{LP} in measures to solve this stopping problem
% \begin{subequations}
% \label{eq:peak_meas}
% \begin{align}
% p^* = & \ \sup \quad \inp{p}{\mu_\tau} \label{eq:peak_meas_obj} \\
%     & \mu_\tau = \delta_0 \otimes\mu_0 + \Lie^\dagger \mu \label{eq:peak_meas_flow}\\
%     & \inp{1}{\mu_0} = 1 \label{eq:peak_meas_prob}\\
%     & \mu, \mu_\tau \in \Mp{[0, T] \times X}  \notag\\
%     & \mu_0 \in \Mp{X_0}. \notag
% \end{align}
% \end{subequations}

% Any $\mu$ that is part of a feasible solution $(\mu, \mu_0, \mu_\tau)$ for \eqref{eq:peak_meas} will be referred to as a \textit{relaxed} occupation measure. Program \eqref{eq:peak_meas} satisfies $p^* \geq P^*$, and tightness $(p^*=P^*)$ is achieved under the assumptions of  Lipschitz continuity and Growth \eqref{eq:lip_growth}, compactness of $[0, T] \times X$, and continuity of $p$.
% }


 
% Compare \acp{ODE} and \acp{SDE}. \ito integral, \itos Lemma.

% Possibly talk about the Fokker-Planck equation.

% \subsection{Occupation Measures and Peak Estimation}

% Definition of an Occupation measure for an \ac{SDE}

% Dynkin's rule using \itos lemma. We need to be consistent about generators and the factor of 1/2 in \itos Lemma.

% Presentation of peak-expectation program


% % \subsection{Sum of Squares}
% % \urg{Fill in \ac{SOS} background for proofs of polynomial nonnegativity}

% % Putinar Psatz \cite{putinar1993compact}


% Scherer Psatz for Matrices \cite{scherer2006matrix}

