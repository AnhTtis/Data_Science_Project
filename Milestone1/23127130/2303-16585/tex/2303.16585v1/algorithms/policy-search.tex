\begin{algorithm}[t!]
	\caption{Policy-Search Deep Hedging with Orthogonal Neural Networks}
	\label{algorithm:policy-search-deep-hedging}
	\begin{algorithmic}
		\STATE {\bfseries input} Policy QNN $\pi$.
		\STATE {\bfseries hyperparameters} Number of episodes per training step $N$.
		\STATE Initialize policy QNN with parameters $\boldsymbol{\phi}$.
		\WHILE{True}
		\FOR{episode $i=1$ {\bfseries to} $N$}
		\FOR{time-step $t=0$ {\bfseries to} $T$}
		\STATE Compute action $a^i_t: =\pi^{\boldsymbol{\phi}_t}_t(s^i_t)$. 
		\STATE Take action $a^i_t$ and receive total reward $r^i_t: = r_t(s_t^i,a_t^i)$.
		\ENDFOR
		\STATE Compute total cumulative return
		$\widetilde{R}_0^i := \sum_{t=0}^T r^i_t$.
		\ENDFOR
		\STATE Update policy parameters $\boldsymbol{\phi}$ with gradient descent to minimize:
		$$\widetilde{\mathcal{L}}(\boldsymbol{\phi}) := \frac{1}{\lambda} \log\frac{1}{N}\sum_{i=1}^{N} \exp\big(-\lambda \widetilde{R}_0^i \big)$$
		\ENDWHILE
		\STATE {\bfseries output} Policy parameters $\boldsymbol{\phi}$.
	\end{algorithmic}
\end{algorithm}