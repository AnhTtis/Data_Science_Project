\begin{algorithm}[H]
	\caption{Distributional Actor-Critic Deep Hedging  with Compound Neural Networks}
	\label{algorithm:distributional-deep-hedging}
	\begin{algorithmic}
		\STATE {\bfseries input} Policy QNN $\pi$, Value QNN $v$.
		\STATE {\bfseries hyperparameters} Number of episodes per training step $N$.
		\STATE Initialize policy and value QNNs with parameters $\{\boldsymbol{\phi}_{t}\}_{t=0}^{T}, \{\boldsymbol{\omega}_{t}\}_{t=0}^{T}$.
		\WHILE{True}
		\FOR{episode $i=1$ {\bfseries to} $N$}
		\FOR{time-step $t=0$ {\bfseries to} $T$}
		\STATE Compute action $a^i_t:=\text{\normalfont Tr}[O^{\mathbf{a}}_t\rho^{\boldsymbol{\phi}_t}_t(\mathbf{a}|s_t^i) ]$. 
		\STATE Take action $a^i_t$ and receive total reward $r^i_t := r_t^+ - r_t^-$.
		\ENDFOR
		\FOR{time-step $t=T$ {\bfseries to} $0$}
		\STATE Compute total cumulative return: 
		$\widetilde{R}_t^i = \sum_{t'=t}^T r^i_{t'}$.
		\STATE Compute Hamming-weight $k^{i}_t:= |s_T^i|-|s_t^i|$.
		\ENDFOR
		\ENDFOR
		\STATE Update value parameters $\boldsymbol{\omega}$ with gradient descent to minimize
		$$\widetilde{\mathcal{L}}(\boldsymbol{\omega}) =  \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{T} \big( \text{\normalfont Tr}[O^{\mathbf{z}}_t \rho^{\boldsymbol{\omega}_t}_t(\mathbf{z}|s^i_t,k^{i}_t+1))] - \exp(-\lambda\widetilde{R}_t^i) \big)^2.$$
		\STATE Update policy parameters $\boldsymbol{\phi}$ with gradient descent to minimize
		$$\widetilde{\mathcal{L}}(\boldsymbol{\phi}) =  \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{T}\frac{1}{\lambda}\exp( -\lambda r_t^i) \times \text{\normalfont Tr}[O^{\mathbf{z}}_{t+1}\rho^{\boldsymbol{\omega}_{t+1}}_t(\mathbf{z}|s^i_{t+1}) ] \big). $$
		\ENDWHILE
		\STATE {\bfseries output} Policy parameters $\boldsymbol{\phi}$.
	\end{algorithmic}
\end{algorithm}