
%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
\documentclass[sigconf]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}




\newcommand \change[1]{{\textcolor{red}{#1}}}
\newcommand \del[1]{\textcolor{red}{\sout{#1}}}
\newcommand \todo[1]{{\textcolor{green}{#1}}}

\newcommand \revision[1]{{\textcolor{black}{#1}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
% \copyrightyear{2023}
% \acmYear{2023}
% \acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI '23]{CHI' 23: The ACM CHI Conference on Human Factors in Computing Systems}{April 23 --- April 28, 2023 }{Hamburg, Germany}
\acmBooktitle{CHI '23: The ACM CHI Conference on Human Factors in Computing Systems, April 23 --- April 28, 2023, Hamburg, Germany}
\acmPrice{15.00}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{rightsretained} 
\acmConference[CHI '23]{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}{April 23--28, 2023}{Hamburg, Germany}
\acmBooktitle{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23), April 23--28, 2023, Hamburg, Germany}\acmDOI{10.1145/3544548.3581008}
\acmISBN{978-1-4503-9421-5/23/04}


\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[]{Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}

%% Towards Designing and Recognizing 
% Enabling Concomitant Hand-to-Face Gesture Sensing for Voice Interaction

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Zisu Li}
\authornote{indicates equal contribution.}
\authornote{This work was conducted when Zisu Li was a research intern at Tsinghua University.}
\email{zlihe@connect.ust.hk}
\orcid{0000-0001-8825-0191}
\affiliation{
  \institution{Tsinghua University}
  \country{Beijing, China}
}
\affiliation{
  \institution{The Hong Kong University of Science and Technology}
  \country{Hong Kong SAR, China}
}

\author{Chen Liang}
\authornotemark[1]
\email{liang-c19@mails.tsinghua.edu.cn}
\orcid{0000-0003-0579-2716}
\affiliation{
  \institution{Tsinghua University}
  \country{Beijing, China}
}

\author{Yuntao Wang}
\authornote{indicates the corresponding author.}
\email{yuntaowang@tsinghua.edu.cn}
\orcid{0000-0002-4249-8893}
\affiliation{
  \institution{Tsinghua University}
  \country{Beijing, China}
}

\author{Yue Qin}
\email{qiny19@mails.tsinghua.edu.cn}
\orcid{0000-0003-1351-5284}
\affiliation{
  \institution{Tsinghua University}
  \country{Beijing, China}
}

\author{Chun Yu}
\email{chunyu@tsinghua.edu.cn}
\orcid{0000-0003-2591-7993}
\affiliation{
  \institution{Tsinghua University}
  \country{Beijing, China}
}

\author{Yukang Yan}
\email{yukangy@andrew.cmu.edu}
\orcid{0000-0001-7515-3755}
\affiliation{
  \institution{Tsinghua University}
  \country{Beijing, China}
}

\author{Mingming Fan}
\email{mingmingfan@ust.hk}
\orcid{0000-0002-0356-4712}
\affiliation{
 % \institution{Computational Media and Arts Thrust}
  \institution{The Hong Kong University of Science and Technology (Guangzhou)}
  \city{Guangzhou}
  \country{China}
  }
\affiliation{
  \institution{The Hong Kong University of Science and Technology}
  \city{Hong Kong SAR}
  \country{China}
}

\author{Yuanchun Shi}
\email{shiyc@tsinghua.edu.cn}
\orcid{0000-0003-2273-6927}
\affiliation{
  \institution{Tsinghua University}
  \city{Beijing}
  \country{China}
}
\affiliation{
  \institution{Qinghai University}
  \city{Xining}
  \country{China}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Li and Liang, et al.}
%%
%% The abstract is a short summary of the work to be presented in the
%% article.
%, to indicate or convey certain intentions,

\begin{abstract}
Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\% for recognizing 3 gestures and 91.5\% for recognizing 8 gestures \revision{(excluding the "empty" gesture)}, proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios. 

%% Old
% Voice interaction with computing devices has been rated as a promising technique which can be used in a wide range of tasks. Unfortunately, the modality control in voice interaction is still a challenging problem due to the implicitness of modality information in speech and the restricted NLP techniques. In this paper, we proposed \projectName{}, a novel sensing technique allowing the user to use hand-to-face gestures to enhance modality control in voice interaction. With \projectName{}, when a user perform a hand-to-face gesture and speak simultaneously, the acoustic feature of his speech influenced by his gesture serves as a channel for gesture recognition and the speech semantics is passed to the application indicated by the recognized gesture as parameters. For example, a user can keep his hand on his specific face region(e.g., cheek) to activate certain function and speaking (e.g., consult the weather to Siri without wake-up, sending the speech to Mom). We explore the design space of \projectName{} gestures and categorized them in different dimensions (face region, touch point, open or closed hands). Then we leverage heterogeneous sensor data from multiple commodity devices to recognize \projectName{} gestures. Our detailed quantitative analysis shed light on the recognition capability of the different sensor combinations including acoustic and movement sensors of earbuds, smart watch and smart ring over gestures with different characteristics. [We achieved xxx data.] In our evaluation and application, we provided guidelines for the design of the recognition system of the combined use of voice and \projectName{} on cross-device interaction.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003138.10003141</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile devices</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003128.10011755</concept_id>
       <concept_desc>Human-centered computing~Gestural input</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile devices}
\ccsdesc[500]{Human-centered computing~Gestural input}

% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{hand gestures, acoustic sensing, sensor fusion}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/teaser.png}
  \caption{A typical usage scenario enabled by voice-accompanying hand-to-face (VAHF) gestures. (a) The user wants to know how to make egg fried rice. (b) The user can perform different VAHF gestures to redirect their voice input to different targets (e.g., asking Siri, searching on Google with the transcribed text, and sending a voice message to mum). (c) The user performs a "phone call" gesture and speaks simultaneously. (d) The smart devices recognize the user's intention through the performed VAHF gesture and simulate sending a voice message to the user's mum.}
  \Description{This figure contains a typical usage scenario enabled by voice-accompanying hand-to-face (VAHF) gestures. (a) The user wants to know how to make egg-fried rice. (b) The user can perform different VAHF gestures to redirect their voice input to different targets (e.g., asking Siri, searching on Google with the transcribed text, and sending a voice message to mum). (c) The user performs a "phone call" gesture and speaks simultaneously. (d) The smart devices recognize the user's intention through the performed VAHF gesture and simulate sending a voice message to the user's mum. }
  \label{fig:teaser}
\end{teaserfigure}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\input{sections/1-introduction}
\input{sections/2-related_work}
\input{sections/3-study1}
\input{sections/4-technical_overview}
\input{sections/5-evaluation}
\input{sections/6-application}
\input{sections/7-discussion}
\input{sections/8-conclusion}


%% why not .tex




%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This work is supported by the Natural Science Foundation of China (NSFC) under Grant No. 62132010 and No. 62002198, Tsinghua University Initiative Scientific Research Program, Beijing Key Lab of Networked Multimedia, and Institute for Artificial Intelligence, Tsinghua University.
\end{acks}

\balance{}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
