\section{Introduction}
% 1. Voice interaction for wearable devices is common, e.g., earphones. However, interactions solely rely on voice is not convenient with two major reasons. 1) requirement of the wake-up keyword. 2) Function navigation in a voice menu requires multiple rounds of communications. 

% 2. Adding gesture to the voice input is a promising approach to rich the functionality. 

Voice input has become a natural and always-available interaction modality for wearable devices such as earphones and smartwatches. However, the modality control (e.g., the wake-up state) in voice interaction is still a challenging problem due to the implicitness of modality information in speech and the restricted NLP techniques. People have to repeat the hotword to switch to the modality or the target device actively, which introduce extra burdens for the interaction. Thus, researchers have been seeking supplementary input methods as parallel input channels to assist voice interaction \revision{\cite{10.1145/3351276,Yan-UIST-2019,10.1145/3313831.3376479}}.

%(e.g., PrivateTalk \cite{Yan-UIST-2019}).
% However, interactions solely rely on voice is not convenient for the following two primary reasons. Wake-up keywords were required to activate the voice interface. Navigation in a voice menu requires multiple rounds of voice communications. Solving these issues can simplify the interaction flow and improve the user experience. Therefore, researchers are actively seeking supplementary solutions to facilitating voice interaction for wearable devices. 

% \revision{In the aspect of interaction efficiency, the combination of voice and gesture input can overcome the typical computer interaction trade-off: ease V.S. expressiveness} \revision{ since speech and gestural commands are easy to execute while retaining a large command vocabulary \cite{10.1145/307710.307730}}. \revision{In the aspect of linguistics }, p
People tend to perform body gestures accompanying their voice for better expression of certain emotions or intentions during the conversation \revision{\cite{10.1007/978-3-030-05792-3_15}}. In such a case, we defined the gesture, which is performed simultaneously with the speech, as a voice-accompanying gesture. Analogously, in the voice interaction with smart devices, the use of voice-accompanying gestures could provide parallel information that expands the input channel bandwidth \revision{\cite{Katsamanis2017,10.1145/3313831.3376479,VoGe2017}}, simplifies the voice interface flow \revision{\cite{10.1145/3351276,10.1145/3411764.3445687,Yan-UIST-2019}}, and make voice interaction more convenient \revision{\cite{10.1145/3411764.3445687,Yan-UIST-2019,10.1145/3313831.3376479}}. \revision{The underlying logic why voice-accompanying gestures have been prevalent and widely researched is due to the physiological nature that the voice channel and the gesture channel are highly independent and complementary \cite{10.1145/3313831.3376479} (e.g., performing a gesture as parallel input information while not repressing the voice expressivity).} For instance, the user can define a specific voice-accompanying gesture (e.g., covering the mouth) instead of repetitively saying the wake-up keyword to keep the voice interface active. The user can also define multiple gestures to represent the redirection of voice input to different input modalities (e.g., ignored, interrupted, transcribed, or raw audio input), target devices (e.g., whether should the TV or the smartphone accept the input), and shortcuts (e.g., binding certain UI operation flows with the gesture).

% todo: R1 suggests enhancing the introduction by emphasizing what makes voice accompanying hand-to-face gestures more suitable than other input methods.
% other methods: raise-to-speak, proxitalk, xxx
% VAHF 的好处 - voice accompanying - parallel input channel, hand-to-face - natural, related to voice
%\revision{Why we chose hand-to-face gesture motivated by XXX proven to be natural and more related to the speec}

In this paper, we investigated the feasibility of using voice-accompanying hand-to-face (VAHF) gestures as parallel channels to improve the traditional voice interaction flow. Specifically, we aim at designing VAHF gestures and recognizing them with an acoustic-based cross-device sensing method. We targeted hand-to-face gestures as the instantiation of voice-accompanying gestures because they \revision{have been proven to be natural, expressive (e.g., various landmarks on the face to yield a large gesture space), and more related to the speech by existing researches \cite{earbuddy,Yan-UIST-2019,46-facesight}}. Moreover, hand-to-face gestures yielded significant features in voice propagation, which is beneficial for acoustic sensing.


 
% In this paper, we aim at designing para-linguistic hand-to-face (PLHF) gestures and recognizing them with a novel cross-device sensing method.

To understand the design space of VAHF gestures, we conducted a user-centric gesture elicitation study with a total proposal of 15 gestures from end-users. Then we narrowed down the gesture set from 15 to 8 gestures with better usability and the least ambiguity. 
% not only usability and ambiguity, but also sematically related

As for the sensing schemes, the underlying principle is that when the user speaks, each VAHF gesture creates a unique acoustic propagation path from the mouth to the set of microphones on wearable devices. Therefore, our method can recognize the hand-to-face gesture using the acoustic features of each unique propagation path. Further, we incorporated an ultrasound channel and an IMU channel to provide supplementary sensing information and enhance gesture recognition. For the ultrasound channel, the smartwatch served as an ultrasound source and all microphones captured such signals from different positions to indicate position-aware features. For the IMU channel, the IMU on the ring can convey the attitude and motion of the user's hand. We also investigated the fusion mechanism among different devices and channels.

To evaluate our technique, we first built a cross-device VAHF dataset consisting of 1800 samples of 8 VAHF gestures \revision{along with one "empty" gesture (meaning not performing a VAHF gesture)}. Then we conducted 1) a two-factorial evaluation regarding sensor combination and model selection, 2) an extensive evaluation of a reduced gesture set, and 3) an ablation study for the optimal model to validate the computation feasibility and the applicability of our technique. Results showed our model achieved high recognition accuracy of 97.3\% for 3 + 1\revision{(empty)} gestures and 91.5\% for 8+1\revision{(empty)} gestures recognition on our cross-device VAHF dataset. Quantitative analysis also sheds light on the recognition capability of each sensor channel and its different combinations.

At the end of the paper, we discuss real-life application scenarios to demonstrate the applicability of VAHF gestures as well as provide general design implications for VAHF gesture-enhanced voice interaction.








In summary, the contributions of this paper are as follows:
\begin{itemize}
    \item We conducted a comprehensive study to elicit the gesture space of VAHF gestures and proposed a gesture set with better usability, better social acceptance, less fatigue, and less ambiguity. 
    
    \item We propose a novel sensor-fusion technique for VAHF gesture recognition which is supported by cross-device sensors. Our quantitative analysis sheds light on the recognition capability of the different sensor combinations over VAHF gestures with different characteristics.
    \item We demonstrate a set of use cases of our gesture recognition technique that outline new opportunities for VAHF gestures to benefit voice interaction.
\end{itemize}
































