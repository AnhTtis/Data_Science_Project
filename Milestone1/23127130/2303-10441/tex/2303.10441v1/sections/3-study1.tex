\section{Designing Voice-Accompanying Hand-to-Face Gestures}
To thoroughly understand and explore the gesture space of voice-accompanying gestures, we conducted a user-centric gesture elicitation study to elicit gesture design from end users. Subsequently, we conducted a hierarchical analysis process to narrow down the gesture space from 15 gestures to 8 gestures which are easy to perform, easy to memorize, and with less ambiguity.  \revision{The design of this study was in line with the previous gesture elicitation work \cite{10.1145/3461778.3462004,Lu-handtohand,earbuddy} consisting the typical phases of gesture proposal, gesture evaluation, and gesture set refinement.}

%Therefore, we generated empirical categories and collected users' subjective perception in different dimensions on the chosen gestures to identify a subset of the most preferable gestures. 

%我们通过brainstorming的方式先选出了一个general hand-to-mouth gesture set，之后通过经验性分类和主观测评最终从15个gesture中筛选出8个gestures。





\begin{table}
  \vspace{-0.3cm}
  \centering
  \caption{Text description and empirical categorization for all the 15 gestures in our gesture set. Each gesture was empirically categorized in three dimensions: contact position, contact type, and occlusion state. Contact position is represented in 5 levels: ear (E), mouth (M), chin(CN), cheek(CK), and none(N). Contact type is represented in 3 levels: finger(F), palm(P), and hand segments(HS). The occlusion state on the sound propagation path for the human voice to ears is represented in 3 levels: hardly(H), partially(P), and completely(C).  }
  \Description{This table contains text description and empirical categorization for all the 15 gestures in our gesture set. Each gesture was empirically categorized in three dimensions: contact position, contact type, and occlusion state. Contact position is represented in 5 levels: ear (E), mouth (M), chin(CN), cheek(CK), and none(N). Contact type is represented in 3 levels: finger(F), palm(P), and hand segments(HS). The occlusion state on the sound propagation path for the human voice to ears is represented in 3 levels: hardly(H), partially(P), and completely(C). }
  ~\label{tab:gestures}
    \vspace{-0.3cm}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{l|c|c|c|c|c}
    \toprule
    Index & Gesture & Contact Position & Contact Type & Occlusion State & \revision{Semantics} \\
    \midrule
    1  & pinch ear rim & E &  F & H & \revision{Earphone Manipulation}\\
    \midrule
    2  & thinking face gesture & M, CN & HS & H & \revision{Thinking, Querying}\\
    \midrule
    3  & support cheek with fist  &  M, CK & P & P & \revision{Thinking, Resting}\\
    \midrule
    4 & non-contact cover mouth with palm   &  N &	P &	P & \revision{Directional Speech, Whisper}\\
    \midrule
    5 & support cheek with palm  &  M, CK & P & P & \revision{Thinking, Concentrating}\\
    \midrule
    6  & cover mouth with fist  & M & HS & C & \revision{Interphone, Messaging}\\
    \midrule
    7  & cover ear with arched palm & E & P & C & \revision{Hearing, Phone Call}\\
    \midrule
    8  & hold up palm beside nose and mouth & M, CK& P& C & \revision{Directional Speech, Block}\\
    \midrule
    9  & touch earphone with index finger &  E &  F & H & \revision{Earphone Manipulation}\\
    \midrule
    10  & touch top ear rim  &  E &  F & H & \revision{Earphone Manipulation}\\
    \midrule
    11  & touch vocal cord   &  N&F& H & \revision{Voice Distortion}\\
    \midrule
    12  & cover mouth with palm & M, CK, CN & P & C & \revision{Silence, Whisper}\\
    \midrule
    13  & shushing gesture   & M& F& H & \revision{Silence, Interruption}\\
    \midrule
    14  & touch the back of ear rim &  E &  F & H & \revision{Hearing, Attention}\\
    \midrule
    15  & calling gesture &  M, CK, CN, E& HS & P & \revision{Communication, Phone Call}\\
    \bottomrule
  \end{tabular}
  }
  \vspace{0.2cm}
\end{table}

\subsection{Voice-accompanying Hand-to-Face Gesture Proposal}
We conducted a brainstorming gesture proposal study to understand users' preference on VAHF gestures and derive a gesture set with general agreements. 

\subsubsection{Participants, Brainstorming Design, and Procedure}
%participant
We recruited 10 participants (4 female, all right-handed) from the local campus, with an average age of 21.3 (from 18 to 27, SD=2.4). Their familiarity score of wearable devices and voice interaction was 3.3\/5 (SD=1.2). The whole study took about 1 hour and each participant received 15\$ for compensation. 

%design
The purpose of this study is to encourage participants to brainstorm as many voice-accompanying hand-to-face (VAHF) gestures as they could without considering the sensing feasibility \revision{and together work out a usable VAHF gesture set with common agreements}. To achieve this point, we do not restrict the gestures to specific application scenarios or tasks, which maintained their focus on the physical nature of performing different VAHF gestures. The only constraint we imposed on the design was that the gestures should be static and durable to meet the nature of "voice-accompanying". The whole study consisted of 4 stages: 1) icebreaking and introduction, 2) individual thinking, 3) individual proposal, and 4) group discussion.

%  The only constraint we imposed on the design was that the gestures should imply para-linguistic semantics or have been used in current voice interaction.

%procedure
After a short icebreaking procedure where all the participants introduced themselves and familiarized themselves with each other, the experimenter acknowledged the participants of the purpose and the procedure for the study as well as the definition of VAHF gestures to the participants. Then the participants went through an individual thinking process for 10 minutes where participants worked separately to \revision{come} up with as many gestures as possible and wrote them down on a notebook \revision{as detailed as possible (e.g., encouraging them to write down the motivations, semantics, and potential usages of the gestures other than simply the descriptions).} After that, each participant was asked to verbalize their proposal \revision{(including the gesture descriptions, motivations, semantics, and potential usages)} and perform the proposed gestures by hand orderly. They could also sketch and show their ideas on a public whiteboard. Participants then came up with a group discussion where one could either show the pros and cons of the others' proposal or generate new gestures from the others' inspiration. The discussion ended until all participants worked out a final gesture set with the consistent agreement. \revision{The whole brainstorming process was hosted by two experimenters - one guiding the experiment while the other taking notes of the key points presented by the participants.}

% The brainstorm session, lasting for about an hour, began with a short icebreaking procedure where all the participants introduced themselves and familiarized with each other. The researcher who conducted the brainstorming session also asked about the state of familiarity of wearable devices and voice interaction to analyse whether people with different using experience could have different ideas about para-linguistic gestures. Then the researcher described the purpose and the procedure for the brainstorming session, as well as the definition of para-linguistic gestures to the participants and encouraged them to brainstorm as many para-linguistic gestures as they could, without considering the sensing feasibility; However, we do not describe the specific application scenario or tasks from which to base their ideas. This choice in procedure was to keep the participants' focus on the physical nature of performing different hand-to-face gestures. With more general guidelines, the participants could concentrate more on generating more gestures without evaluation of them at this stage. The only constraint we imposed on the design was that the gestures should imply para-linguistic semantics or have been used in current voice interaction. Then we conducted peer interviews with another set of users to further supplement the gesture set by the same brainstorming  procedure until the gestures that users can think of were already included in our gesture set.

% To record the gestures proposed, the researcher would provide the white board to participants to help them sketch and show their ideas. An individual thinking period was conducted before the group talking session first for 10 minutes where participants work separately to comp up with as many gestures as possible. Then the researcher asked participants to take turns to verbalize their thought and perform the proposed gestures by hands. After group talking session, participants were asked to spend the rest of the time working together to create new ideas, adding to those they thought of individually. The researcher presented questions to spark new lines of thought whenever the group had trouble brainstorming new ideas. We ended our brainstorming session until the gestures thay users can think of were already included in our gesture set.


%We first introduced the definition of para-linguistic gestures to the participants and encouraged them to come up with as many gestures as possible. After giving the participants ten minutes for thinking, we asked them to take turns to clarify their gestures and demonstrate gestures with their hands. We also asked the participants to verbalize their thought process during their speeches. In order to dispel the participants' worries, we did not allow evaluation of gestures at this stage. Then we conducted peer interviews with another set of users to further supplement the gesture set by brainstorming until the gestures that users can think of were already included in our gesture set. (!xxx)

% 语义 - 拟物化，用户倾向于迁移使用智能设备的经验

\subsubsection{Results and Discussion}
Fig. \ref{fig:gestures} and the "Gesture" column of Table \ref{tab:gestures} illustrated the 15 VAHF gestures and their text descriptions proposed by users in the brainstorming study. \revision{The "Semantics" column summarized some of the typical semantics of each gesture from participants' quotes. An interesting finding is that participants tended to design the gestures in a mimetic and semantic-based manner, borrowing the inspirations from their daily activities and usages of smart devices. For example, touching gestures on the ear (gestures 1, 9, and 10 in Table \ref{tab:gestures}, same as below) was regarded as the metaphor of earphone manipulations related to voice interaction (N=9), which came from their experiences of using wireless earbuds (e.g., triggering the voice assistant and controlling the volume and the progress). Participants also presented their potential usages, such as waking up Siri (P2), making a voice memo (P3), and sending a voice message to a specific person (P8). Similarly, participants described gestures 6, 7, and 15 as "the imitation of using certain devices" (N=10). \textit{"Holding up the fist in front of the mouth is a cool gesture. It is just like sending an instant command with an interphone"} (P4). \textit{"Covering the ear with the arched palm is like you are holding the phone while the 'calling' gesture is like you are imitating an old-fashioned telephone. I would prefer the former one because it is easy to perform and seems more natural to others"} (P10).}

\revision{In addition to the above gestures related to device usage, some gestures were proposed for their prevalence in daily communication and social expression. Participants (N=10) showed their will to transfer these gestures to the interaction with voice assistance. For example, the "shushing" gesture (gesture 13) and the "flaring ear" gesture (gesture 14) were proposed because they were frequently used in daily dialog. \textit{"Shushing has the meaning of silence and interruption. We can also use it to interrupt the conversation with the voice assistant" (P3).} \textit{"The 'flaring ear' gesture means 'pardon' or shows attention to the speaker. I guess it would be nice to assign this gesture to functions with similar meanings" (P1).} Gestures 4, 6, 7, and 12 were mouth-related gestures proposed by the participants, with the general meanings of special speech, lowered volume, whisper, and silence. The gestures were distinguished by different ways of covering the mouth. \textit{"When I hold up the palm on one side of the mouth, I probably want to speak to the one on the other side directionally. However, when I cover my mouth, the meaning could be totally different" (P3).} Similarly, participants designed three face-related gestures (gestures 2, 3, and 5), indicating thinking, querying, resting, or concentrating, yet with slightly different implications. \textit{"It would be wonderful the voice assistant could respond to my 'thinking face' gesture by querying my words on the searching engine"} (P2). Exceptionally, P5 proposed a "touch vocal cord" gesture (gesture 11) with a unique position. "The vocal cord affects the timbre, meaning to 'make a different sound' (P5)."}

\revision{Although the semantics of each gesture seemed clear to individuals, we found some conflicts in the group discussion stage. For example, regarding the "cover mouth with fist" gesture (gesture 6), most participants showed approval of the "interphone" metaphor while some participants (P2, P5) thought it should be with the semantics of "silence" and "secrete". Some participants also mentioned the meanings and preferences of certain gestures might vary under different cultural backgrounds, especially for the gestures with social functionalities.}







% 1	pinch the ear rim	E	F	H
% 2	thinking face gesture	M, CN	HS	H
% 3	support cheek with fist	M, CK	HS	P
% 4	non-contact cover mouth with palm	N	P	P
% 5	support cheek with palm	M, CK	P	P
% 6	cover mouth with fist	M	HS	C
% 7	cover ear with palm	E	P	C
% 8	cover nose and mouth with palm	M, CK	P	P
% 9	touch the earphone with index finger	E	F	H
% 10	touch the top ear rim	E	F	H
% 11	touch the vocal cord	N	F	H
% 12	cover mouth with palm	M, CK, CN	P	C
% 13	shushing gesture	M	F	H
% 14	touch the back of ear rim with finger tip	E	F	H
% 15	calling gesture	M, CK, CN, E	HS	P
\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{figures/gesture_list2.png}
    \caption{Drafts illustrating each gesture in the gesture set: 1) pinch the ear rim, 2) thinking face gesture, 3) support cheek with fist, 4)non-contact cover mouth with palm, 5)support cheek with palm, 6) cover mouth with fist 7) cover ear with arched palm, 8) hold up palm beside nose and mouth,  9) touch the earphone with index finger, 10) touch the top ear rim, 11) touch the vocal cord, 12) cover mouth with palm, 13) shushing gesture, 14) touch back of ear rim with fingers, 15) calling gesture. The mean±s.d. of users’ subjective scores (1-7, the higher the better) on Usability (U), Social Acceptance (S), Fatigue (F) and Disambiguity (D) is shown on the bottom of each draft. Scores of gestures in our final gesture set are highlighted in orange. }
    \Description{This figure contains each gesture in the gesture set: 1) pinch the ear rim, 2) thinking face gesture, 3) support cheek with fist, 4)non-contact cover mouth with palm, 5)support cheek with palm, 6) cover mouth with fist 7) cover ear with arched palm, 8) hold up palm beside nose and mouth,  9) touch the earphone with index finger, 10) touch the top ear rim, 11) touch the vocal cord, 12) cover mouth with palm, 13) shushing gesture, 14) touch back of ear rim with fingers, 15) calling gesture. The mean±s.d. of users’ subjective scores (1-7, the higher, the better) on Usability (U), Social Acceptance (S), Fatigue (F) and Disambiguity (D) is shown on the bottom of each draft. Scores of gestures in our final gesture set are highlighted in orange. }
    \label{fig:gestures}
\end{figure*}


\subsection{Optimizing VAHF Gesture Set}
To derive the final user-defined gesture set from all gestures proposed by all participants, we collated the gestures and asked participants to perform all the gestures, and conducted subjective ratings from 4 dimensions. We resolved repeatability between gestures by empirical categories, which intuitively characterized the similarity between gestures from 3 dimensions. We chose one gesture from each category to a subset of the most preferable gestures.


\subsubsection{Subjective Evaluation}
After deriving a gesture set with 15 gestures, we sought to find out which gesture is most suited for voice interactions, especially in social acceptance and using fatigue. We recruited 25 participants (10 male and 15 female) for our subjective evaluation, with an average age of 21(from 19 to 32, SD = 2.1). All of the participants were right-handed. Each participant performed all gestures three times using their right hand. The order of the gestures was pre-determined to counterbalance ordering effects. For each gesture, the experimenter would show an example video of this gesture to ensure the participant could perform the gesture correctly. The participant then followed the instructions provided on a laptop screen to perform gestures. After performing the gesture three times, the participant was asked to rate the gesture according to the following four criteria along a 7-point Likert scale (1: strongly disagree to 7: strongly agree), and the results are shown in Fig. \ref{fig:gestures}:
\begin{itemize}
    \item \textbf{Usability} measured ergonomics to reflect the comfort of the gesture. The participants are required to consider the gesture not only in stationary conditions (e.g., sitting) but also under moving conditions (e.g., running). The higher the score, the easier the gesture is to perform.
    \item \textbf{Social Acceptance} measures if the user will feel uncomfortable or embarrassed, or if performing the gesture will disturb others in public settings. The higher the score, the more acceptable the gesture is in social environments.
    \item \textbf{Disambiguity} measures the difficulty of confusing the gesture with daily hand movements or with other gestures. The higher the score, the less ambiguous the gesture is.
    \item \textbf{Fatigue} measures the physiological burden to perform the gesture. The higher the score, the less fatigue the gesture is to perform.
    
\end{itemize}


\subsubsection{Design Principles and Finalized Gesture Set}
In order to eliminate the design consistency and gestures with signal similarity to derive gestures that can be naturally performed and quickly remembered, we categorized all the gestures to propose the most representative one in each category. Considering the propagation path of the human voice around the head, we \revision{identify three structural properties to represent the proposed gesture set}, which are illustrated in Table \ref{tab:gestures}:
\begin{itemize}
    \item \textbf{Contact Position:} Due to the different contact positions of the fingers, the microphone mounted on the ring can receive different sounds. Because the mouth is the source of the sound, the closer the finger touches the sound source, the louder the sound will be picked up by the microphone on the ring. In all gestures, the contact positions of the fingers are the mouth(M), the cheek(CK), the chin(CN), and the ear(E).
    \item \textbf{Contact Type:} Different contact types, specifically divided into fingers(F), palms(P), and hand segments(HS) by which hands used to contact the face region, have clear distinctions in morphology which can be distinguished easily by users without ambiguity. Furthermore, different contact types will form a unique structure on the face to affect the collection of the earphones' feed-forward microphones. 
    \item \textbf{Occlusion State:} The occlusion state, which is separated in 3 levels(hardly(H), partially(P), and Completely(C), will produce different sounds by affecting the propagation path from the human voice to the ears. For example, gesture 7 (cover ear with arched palm) and gesture 12 (cover mouth with palm) shown in Fig. \ref{fig:gestures}, which 'completely' occlude the receiver (ears) and the transmitter (mouth) of the sound the propagation path in the air, will cause a loss of high-frequency sound.
\end{itemize}

% structural property
\revision{We combined the subjective evaluation results shown on Fig. \ref{fig:gestures} (in 4 dimensions: usability, social acceptance, disambiguity, and fatigue) with the gesture set optimization process. The process was based on first grouping gestures with the same structural property combinations from the above three dimensions and chose one gesture according to the subjective scores to represent each category.} From the categorization results, we found that gestures belonging to the E-F-H category include gestures 1, 9, 10, and 14. This type of gesture with fingers to contact the ear region and have similarity in signal. Moreover, E-F-H gestures are also commonly used to interact with earphones. We chose gesture 1 to represent E-F-H gestures in our final subset of VAHF gestures according to subjective ratings; Gestures 3,5 belong to M,CK-P-P gestures. \revision{Considering the operating region of gesture 4 is also near mouth and cheek(M, CK) although it does not actually contact the face and the gesture 4's other two dimensions are the same P(Contact Type)-P(Occulasion State) with gesture 3, 5, we grouped gestures 3,4,5 into one category and chose gesture 5 to represent this category.} Gestures 11 and 13 are omitted due to their lower social acceptance \revision{(e.g., lower than 3.5)}. \revision{And the remaining gestures (2,6,7,8,12,15) are kept in the gesture set due to their specificity in the three dimensions and higher subjective scores.} 
The gesture selection process resulted in 8 gestures. We checked the subjective scores of each dimension of the eight gestures selected again and found they are all above 4.4 and have a comprehensively higher score over others, which proves that our gesture selection is subjectively reasonable and practical for users. 

The above gesture selection process filtered out the following 8 gestures: gesture 1 (E-F-H), gesture 2 (M,CN-HS-H), gesture 5 (M,CK-P-P), gesture 6 (M-HS-C), gesture 7 (E-P-C), gesture 8 (M,CK-P-C), gesture 12 (M,CK,CN-P-C), gesture 15 (M,CK,CN,E-HS-P) where the indexes were consistent with Fig. \ref{fig:gestures}. These gestures constituted our final gesture set. 

























