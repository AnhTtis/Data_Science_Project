\section{Recognizing Voice-Accompanying Hand-to-Face Gestures with Cross-Device Sensing}

In this section, we introduce the design considerations and the technical details of our cross-device sensing method to recognize VAHF gestures. We explain the implementation considerations regarding device and channel selection. Then we present individual sensing models for vocal, ultrasonic, and IMU channels. Finally, we clarify the sensor combination and fusion strategies for real-world deployment.

\subsection{Considerations and Technical Overview}

We first clarify the considerations of our implementation before going into the technical details. To recognize VAHF gestures, we chose 3 types of commercial wearable devices - wireless ANC earbuds, smartwatches, and smart rings - as the sensor nodes in consideration of real-life deployment. Each wireless ANC earbud consists of an inner microphone and an outer microphone for noise canceling. The smartwatch is equipped with a microphone and a speaker which is capable to play sounds at 22.5 KHz and the ring is equipped with a microphone and an IMU. We chose the microphones and the IMUs as the sensor candidates in consideration of the computation efficiency for the always-availability (e.g., raise-to-speak technique on an Apple watch). These sensors are widely equipped on the aforementioned commercial wearable devices (earbuds, watch, and ring).

An illustration of the entire system is shown in Figure \ref{fig:algorithm}. The sensing system consists of three independent models: vocal model, ultrasonic model, and IMU model. Each channel takes the corresponding preprocessed signal from the devices and outputs the feature vectors, which are fused and fed to the classifier layers to output the prediction logits.

% Each of the above devices is equipped with an inertial measurement unit (IMU) and at least one microphone. each earbud - 2 microphones.  The smart watch 
% is also equipped with a speaker which is capable to play sounds at 22.5 KHz(?). xxx, computational efficient, xx

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/algorithms.png}
    \caption{The sensing algorithm pipeline.}
    \Description{This figure contains the sensing system that consists of three independent models: vocal model, ultrasonic model, and IMU model. Each channel takes the corresponding preprocessed signal from the devices and outputs the feature vectors, which are fused and fed to the classifier layers to output the prediction logits.}
    \label{fig:algorithm}
\end{figure*}



% 我们的目标是识别语音交互中的hand-to-face手势，为了部署的可行性（能耗、性能、形态），我们选取了现有几种普及的商用可穿戴硬件设备 - 耳机、智能手表、智能戒指 - 作为可选的传感节点，并选取设备中两种常用的低功耗（computational feasibility）传感器 - 麦克风和IMU - 进行传感。

% 系统流水线如xx图，

\subsection{Recognizing VAHF Gestures with Single-Modality Solutions}

% Vocal, Ultrasonic, IMU

To facilitate efficient recognition of VAHF gestures, we first build three individual sensing models involving three independent channels of features - vocal features, ultrasonic features, and IMU features. Each channel of features serves as individual input of recognition in different dimensions.

\subsubsection{Vocal Model}

Performing hand-to-face gestures while speaking leads to changes in the acoustic property including amplitude, frequency response, and reverberation for the received signal. For example, an "hold up the palm beside nose and mouth" gesture may impede the direct transmission of sound to the left earbud's microphone, resulting in a lower amplitude and decay in high frequency in the corresponding channel. Since we focus on the difference in the acoustic property among the distributed microphones, we first figured out the reference channel. Typically, we chose the inner microphone as the reference channel when inner and outer microphones were simultaneously used and the outer microphone of the right earbud when inner channels were disabled.

%你这段是哪里的？我去看看 对就是内外耳 forward是外 back是内

Similar to prior work, we processes the audio data for classification using mel spectrum \cite{7952132}. Given a set of audio segments from all channels $[a_1,\cdots,a_n;a_{ref}]$ with the sample rate of 16 KHz, we convert each segment into the frequency domain by first applying the short-time Fourier transform then adopting a mel-scale transform with 128 mel filterbanks, after which we pad or trunk each spectrum in the temporal axis with zeros into $128 \times 250$ ($\approx 3$ seconds) and acquire $n+1$ maps $[m_1,\cdots,m_n;m_{ref}]$. Then we subtract the reference 
map $m_{ref}$ from all the monitored map $m_{i}$ to acquire the channel-wise difference in mel spectrum $[m_1 - m_{ref}, \cdots, m_n-m_{ref};m_{ref}]$. Finally, we concatenate all the maps in the first axis into a single input frame that can be fed into a deep-learning classification model. 

A MobileNet V3 Large \cite{Howard_2019_ICCV} model pretrained on ImageNet \cite{russakovsky2015imagenet} is used as the backbone network for feature extraction. The input frame is fed to the feature extractor layers of the pretrained model to generate a 1-D feature series $f_{spec}$. Such a model is chosen in consideration of the balance between computational complexity and performance \cite{Howard_2019_ICCV}. Benefitting from the well-designed network structure and the large parameter space with good initialization, MobileNet V3 Large has the potential in capturing fine-grained textural and geometry features from the concatenated spectrum map. 

Despite the direct use of the neural network on the mel frequency map, we extracted two additional sets of statistics features - transient signal amplitude and pair-wise similarity among Mel-frequency spectrum coefficients (MFCC) series - as classifier input, which is inspired by PrivateTalk's \cite{Yan-UIST-2019} solution in dealing with channel difference and delay between audio segments. For the transient amplitude feature, we use a sliding window with the size and stride of 200 to compute the amplitude series for each segment, after which we pad or trunk each series to a fixed length of 250. Then we concatenate all the amplitude series into a 1-D feature series $f_{amp}$. For pair-wise MFCC similarity, we first compute the MFCC series for each audio channel, then resample each MFCC map in the temporal domain into 20-frame segments with a stride of 10. For each pair of segment series, we compute their similarity using dynamic time warping (DTW) \cite{berndt1994using}. We acquired the pair-wise similarity feature vector $f_{MFCC}$ by concatenating all the above ${1\over2}n(n+1)$ simularity values.

After getting $f_{spec}$, $f_{amp}$, and $f_{MFCC}$, we concatenate them into a 1-dimensional vocal feature $f_{vol}$, which can either be used in an individual recognition model or be combined with other features. For an individual recognition model, $f_{vol}$ is fed into a multi-layer perceptron (MLP) classifier to predict the performed gesture.

% from the original audio segments
% statistics: signal amplitude, pair-wise distance DTW Inspired by PrivateTalk \cite{},

\subsubsection{Ultrasonic Model}

When the user performs a hand-to-face gesture, with his hand reaching different position on the face, the relative positions among the wrist, the finger, and the ears are temporally changing, thus yielding salient positional features. To facilitate such features, we devised an embedded ultrasonic sensing component, where the speaker on the smart watch works as an active source transmitting a 17.5 KHz - 22.5 KHz linear chirp signal which is captured by the microphones on the target devices. Such a design is inspired by the theory of Frequency Modulated Continuous Wave (FMCW) \cite{mao2016cat}, which is widely used in radar and indoor positioning systems to acquire positional tracking information. The sensing principals can be formalized as a typical linear chirp based FMCW. Let $x_0(t) = A_0 cos(2 \pi f_0 t + \pi {B \over T} t^2)$ be the source signal and $x_i(t) = A_1 cos(2 \pi f_0 (t-t_0) + \pi {B \over T} (t-t_0)^2)$ be the signal received by the $i^{th}$ device, where $B=f_1 - f_0$ is the bandwidth and $T$ is the period of the chirp. We first compute the correlation $x_0(t) x_i(t)$ and then pass the result to a low-pass filter to acquire the low-frequency component:

\begin{equation}
\begin{aligned}
    LPF(x_0(t)x_i(t)) = {1\over2} A_0 A_1 cos(2 \pi f_0 t_0 + \pi {B \over T} (2t_0 t - t_0^2))
\end{aligned}
\end{equation}

Note that $LPF(x_0(t)x_i(t))$ is a cosine function form of $t$ with an amplitude of ${1\over2} A_0 A_1$ and a frequency of $2{B \over T}t_0$, where the amplitude indicates the decay of the signal transmission and the frequency is proportional to the delay $t_0$ of the received signal. So we first compute the spectrum of $LPF(x_0(t)x_i(t))$ using short time Fourier transform (STFT). We extracted the following two features based on the spectrum: 1) the image feature $f_{spec}$ of the spectrum using a pretrained MobileNet V3 network and 2) the amplitude and frequency series $f_{stats}$ (which is flattened into a 1-D vector) derived from the spectrum. Finally, we concatenate $f_{spec}$ and $f_{stats}$ into $f_{ultra}$, which can be either fed into a downstream classifier or combined with other features as mentioned above.

\subsubsection{IMU Model}

Devices worn on the user's hand, such as a watch and ring, help to capture the movement and attitude of the user's moving hand, thus beneficial for recognizing hand-to-mouth gestures. In our setting, we choose to mount a 9-axis wireless IMU on the ring as previous work \cite{10.1145/3478114} did. The IMU reports 3-axis acceleration, 3-axis angular velocity, and the quaternion at 200 Hz. For gesture recognition, we adopted a fixed window of 400 frames (or 2 seconds), concatenating the acceleration, angular velocity, and quaternion series into a 4000-length vector. Then we used a 3-layer MLP with the structure of $Dropout(0.5) \rightarrow Linear(4000,512) \rightarrow ReLU \rightarrow Linear(512,512) \rightarrow ReLU \rightarrow Linear(512,9)$
for classification. 

\subsection{Sensor Combination and Fusion Strategies}

In consideration of the real-world deployment, we first figure out the reasonable device and sensor combinations. The devices include 1) left earbud (LE) with inner and outer microphones ($m_{l,i}$, $m_{l,o}$), 2) right earbud (RE) with inner and outer microphones ($m_{r,i}$, $m_{r,o}$), 3) watch with a microphone ($m_w$), and 4) ring with a microphone ($m_r$) and an IMU. Considering earbuds are most commonly used, we chose them as the primary device, which would work in different forms including two-side, one-side (wearing one earbud), and outer-only (for ones without active noise canceling). The introduction of the watch could be beneficial in providing an active ultrasound source as well as a hand-mounted microphone. Last, a ring device with an IMU and a microphone could track the movement of the hand and finger as well as provide a finger-mounted microphone. Based on the above observation, we devised four typical settings as follows for investigation: 1) single earbud (RE); 2) two earbuds (LE+RE); 3) two earbuds + watch (LE+RE+W); and 4) all devices (LE+RE+W+R).

%$m_{l,o}$, $m_{r,o}$, $m_w$, $m_r$

For settings 3) and 4), since the active ultrasound source and the IMU enable the ultrasonic model and the IMU model, a fusion method is required to fuse different recognition models from different channels. We investigated two fusion strategies: 1) logit-level fusion and 2) feature-level fusion. 

Let $F_{v}$, $F_{u}$, and $F_{i}$ be the feature extractor network of vocal, ultrasonic, and IMU models respectively and $C_{v}$, $C_{u}$, and $C_{i}$ be the corresponding multilayer classifier that outputs the logits. For logit-level fusion, the output logits are computed as 
\begin{equation}
    logits = a\cdot C_v(F_v(x_v)) + b\cdot C_u(F_u(x_u)) + c\cdot C_i(F_i(x_i))
\end{equation}
where $a$, $b$, and $c$ are learnable weight parameters ($x_v$, $x_u$, and $x_i$ are the corresponding channels of input). For feature-level fusion, the output logits are computed as 
\begin{equation}
    logits = C_{fuse}([F_v(x_v), F_u(x_u), F_i(x_i)])
\end{equation}
, where [*, *, *] refers to concatenation and $C_{fuse}$ is another MLP classifier that takes the concatenated features as input and outputs the logits.



% two fusion strategies:

% logit-level fusion

% feature-level fusion

% 4 different fusion strategies:

% 1) voting; 2) hierarchical; 3) feature-level fusion; 4) logit-level fusion; 5) + pretrained 

% \subsection{Towards Multi-Objective Optimization for xxxx}

% different targets:

% 1) optimizing recognition method
% 2) optimizing gesture set
% 3) optimizing form factor

%input/output
%不同的channels 
%device groups
%model

