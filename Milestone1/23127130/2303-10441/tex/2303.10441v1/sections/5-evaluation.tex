\section{Evaluation}
In this section, we conducted a systematic evaluation on the cross-device sensing method illustrated in the previous section. We first built a cross-device VAHF dataset consisting of \revision{10} users $\times$ 20 sentences $\times$ (8+1) gestures $=$ \revision{1800} samples. Then we evaluated our cross-device sensing method on the dataset in the dimensions of sensor combination, model selection, gesture reduction, and model ablation.

\subsection{Participants and Apparatus}
We recruited \revision{10} participants (\revision{6} female and 4 male) with an average age of  \revision{21.2 (from 19 to 28, SD=2.64)} and all participants were right-handed. All participants were recruited via emails and websites in our organization. 
We used a pair of earbuds with microphones, a smart watch with a motion sensor and a microphone, and a smart ring with a motion sensor and a microphone. The data of the microphones and motion sensors were fetched synchronously by a data collection thread. 

% 21 (from 19 to 28, SD=2.1)

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/hardware.png}
    \caption{The apparatus of data collection. (a) Hardware overview. (b) A ring with an IMU and a microphone. (c) An earphone with a feed-forward microphone and a feed-back microphone. (d) A smartwatch with a microphone and a speaker. (e) A Zoom H6 recorder with six audio input channels.}
    \Description{This figure contains the apparatus of data collection. (a) Hardware overview. (b) A ring with an IMU and a microphone. (c) An earphone with a feed-forward microphone and a feed-back microphone. (d) A smartwatch with a microphone and a speaker. (e) A Zoom H6 recorder with six audio input channels.}
    \label{fig:hardware}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{figures/experiment.png}
    \caption{(a) Experiment setup. (b) A gesture example.}
    \Description{This figure contains the experiment setup and a gesture example. (a) The user was sitting at the table wearing all the devices. (b) The user performed a "cover mouth" gesture to record a data sample.}
    \label{fig:exp}
\end{figure}

\subsubsection{Microphones} We used the Sony WF-1000XM3 wireless noise-canceling headphone~\footnote{https://www.sony.com/electronics/truly-wireless/wf-1000xm3} and ZOOM H6 Handy recorder~\footnote{https://zoomcorp.com/en/us/handheld-recorders/handheld-recorders/h6-audio-recorder/} in this paper. We used four one-channel TRS audio cables to connect to the feed-forward and feed-back microphones
with headphones and a two-channel TRS audio cable to connect to the watch and the ring respectively. The ZOOM H6 audio recorder can record these six channels of timely synchronized audio data to a TF storage card (32 GB). The audio sampling rate was set to 48 KHz. To remain the same acoustic characteristic, we kept all the hardware in its original position in the earphone. The battery was run out of power to disable the on-chip software including the active noise canceling.

We use the MI Watch~\footnote{https://www.mi.com/global/mi-watch-lite} with 3-axis accelerometer and 3-axis gyroscope at 100Hz. The data is kept locally on watch and would be pulled after each round of the experiment. 

%超声波要不要写在这里
%这里也要插个图

\subsubsection{Inertial Measurement Unit}
We used a ring embedded with a wireless BMI-055 9-axis Inertial Measurement Unit (IMU) module, as shown in Figure \ref{fig:hardware}. The IMU data (3-axis acceleration, 3-axis gyroscope data, 3-axis geomagnetic data, and 3-axis Euler angle, current system time) is transmitted to a PC with a Bluetooth module at 200Hz (460800 baud rate).


\subsection{Data Collection}
We collected gesture samples from \revision{10} participants. The data collection entailed recording voice, ultrasound, and motion data while participants performed VAHF gestures corresponding to Section 3 and speak with daily voice commands. Each data collection study lasted about 60 minutes. Initially, participants were asked to read and sign consent forms. They were then shown instruction slides explaining the overall procedure of the data collection session and videos of VAHF gesture set from Section 3. Then we instructed participants to put on the earbuds, the smartwatch, and the ring properly, helping them to adjust the wearing until they felt comfortable with the devices. 
%文本不知道应该写在哪里,是不是要在sec3里面也写一下%. 

Each participant was required to perform 15 gestures and record 10 voice commands for each gesture (150 gesture samples in total). For each gesture, the participant was shown a slide with the gesture's name, the 10 voice commands, and the posture (sitting or standing). The order of the gestures and the posture condition were randomly picked to remove the order effect. The 10 voice commands were randomly picked from the daily Siri voice commands\footnote{https://support.apple.com/siri}, as shown in Table \ref{tab:commands}.

\begin{table}
  \vspace{-0.3cm}
  \centering
  \caption{The English translations of the voice commands used in data collection. The participants read the commands in Chinese. These voice commands were picked from Apple Siri's tutorial.}
  \Description{This table contains the English translations of the voice commands used in data collection. The participants read the commands in Chinese. These voice commands were picked from Apple Siri's tutorial. }
  ~\label{tab:commands}
    \vspace{-0.3cm}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{l|l|l|l}
    \toprule
    Index & Voice Command & Index & Voice Command\\
    \midrule
    1  & Text Mom. & 11  & Turn the temperature up to 24 degrees.\\
    \midrule
    2  & Read my messages. & 12 & Show the photos taken today.\\
    \midrule
    3  & Who is calling? & 13 & Find the popular restaurants nearby. \\
    \midrule
    4 & Set an alarm for eight o'clock. & 14 & What is the latest movie?\\
    \midrule
    5 & Pay with Apple Pay. & 15 & How to take a holiday on National Day?\\
    \midrule
    6  & Transfer 20 yuan to Amy. & 16 & Buy train tickets to Beijing. \\
    \midrule
    7  & Remind me to pick up the clothes. & 17 & How is the weather today?  \\
    \midrule
    8  & What is my plan today? & 18 & Open Voice Memos. \\
    \midrule
    9  & Play my favorite song. & 19  & How to go to the nearest metro station? \\
    \midrule
    10  & Turn on the living room lights. & 20 & Countdown 20 minutes. \\
    \bottomrule
  \end{tabular}
  }
\end{table}

During the recording of the 10 gesture samples of each gesture, the experimenter first turned on the recording of the IMU ring, the watch's ultrasound, and the recorder. Then the participant clapped his or her hands to provide a synchronous signal used for the synchronization of different sensors. 
%没写第一个tick
For each gesture sample, the experimenter first pressed a key on the PC to label a tick and record the system time, which was used for gesture sample segmentation, and then signaled the participant to perform the gesture and read the corresponding voice command while keeping the gesture. This process was repeated 10 times until the participant finished all 10 gesture samples. After that, the experimenter turned off the recording. 


%Once the experimenter pressed the start buttons for IMU ring, the watch's ultrasound, and the recorder, the participant would clap his or her hands to [provide a synchronous signal used for xxx] synchronize the different sensors[]. Then the experimenter would press the space on PC to indicate the participant to perform the desired gesture and record the current system time. [After the gesture prompt and execution,]x the participant [first performed and kept the desired gesture, and then read the corresponding voice command on the slide.] would keep the gesture and simultaneously read the corresponding voice command on the slide. The experimenter would press the space to label a tick on PC after the voice command and the participant would repeat the gesture and read the next voice command. The ticks were used for gesture sample segmentation. The time window between every space was labeled as a gesture segment. The labels were used to create the dataset and perform evaluations.

%用户在看到实验员按下按键之后，开始做手势，并在把手放在脸附近的时候开始读语音指令

 
 
\subsection{Data Preprocessing}
%synchronize
%segment
%label
%sample划分
%超声和人声的分离
Our data preprocessing process consisted of four steps: channel synchronization, segmentation, voice activation detection (VAD), and vocal-ultra sound separation. Below we illustrate the implementation details. 

\subsubsection{Channel Synchronization} The synchronization between audio channels was achieved by the audio card in ZOOM H6. To synchronize the audio channels and the IMU channel, we required the user to clap their hand to provide a signal for alignment before starting data collection for each session. Then we located such a clapping peak in both the audio channels and the IMU channel to acquire the relative time shift. The peak in the audio channels and the IMU channel was detected by finding the first local maximum in the amplitude spectrogram and the acceleration spectrogram, respectively.

\subsubsection{Audio Segmentation and VAD} After aligning all the channels, we segment the audio data which includes 10 voice commands for each. This was achieved by simply separating a piece of audio using the keystroke points annotated by the participants during recording. After getting the coarse segmentation, we further ran a VAD algorithm \cite{chang2006voice} to remove the silent period at the two ends in each segment.

%We need to segment audio data which includes 10 voice commands for each. After the gesture session starts, the user would clap his or her hands, and the applause would generate a peak in the amplitude spectrogram which is a second earlier than when the experimenter press the key to record the first system time point in average. So we took a second after the applause time as the starting point, and then use the remaining ten system time points to segment the audio. 


\subsubsection{Separation of Ultrasound and Vocals}
%butterworth highpass/lowpass
%1/2/3
We used a Butterworth~\footnote{https://en.wikipedia.org/wiki/Butterworth\textunderscore{}filter} highpass filter with 17500Hz cutoff frequency to separate the ultrasound and vocal from the audio data. 


\subsection{Evaluation Design}

The evaluation consists of three sessions. In the first session, we conducted a two-factorial evaluation to analyze the recognition performance with regard to sensor combination and model selection. For sensor combination, corresponding to Section 4.3, we investigated five settings: 1) single (right) earbud with inner and outer microphones (RE, 2 audio channels), 2) two earbuds with inner and outer microphones (LE+RE, 4 audio channels), 3) two earbuds with outer microphones + watch (LE+RE+W, 3 audio channels), 4) all devices without the earbuds' inner channels (ALL-4ch, 4 audio channels), and 5) all devices with all channels (ALL-6ch, 6 audio channels). For model selection, we investigated the following six models: 1) vocal only (V), 2) ultrasound only (U), 3) IMU only (I), 4) vocal + ultrasound(V+U), 5) vocal + ultrasound + IMU with logit-level fusion (ALL-L), and 6) vocal + ultrasound + IMU with feature-level fusion (ALL-F). It is worth mentioning that the above two factors are correlated. The ultrasound channel would be activated unless the watch is used. Similarly, the IMU channel would be activated when the ring is used. Other factors, including the network structure, hyperparameters (max training epoch=100, dropout=0.5), and optimization strategies, are strictly controlled. We adopt three optimization strategies - pretraining, dropout, and \revision{warm-up} to improve the performance and training robustness of our model. For pretraining, we initialized the MobileNet V3's parameters with the one pretrained on ImageNet \cite{russakovsky2015imagenet}. For dropout, we added a dropout layer with a probability of 0.5 after the input layer to alleviate overfitting during training. For \revision{warm-up}, we adopted a \revision{warm-up} and weight decay strategy on the learning rate using the following piecewise function: if $n \leq 10$, then \revision{$lr(n)=0.1 \times n \times lr(0)$}, else $lr(n)=0.97^{n-10}\times lr(0)$, where $n$ is the training epoch and $lr(n)$ is the learning rate of the $n^{th}$ epoch.


In the second session, we conducted an extensive evaluation on a reduced gesture set to analyze the optimal performance and usability of each sensor combination for practical deployment. The reduced gesture set contains three signature gestures - cover mouth with palm (G1), cover ear with arched palm (G2), and hold up the palm beside nose and mouth (G3) - which received high preference scores from the previous user study and intuitively had significant effects on the acoustic propagation. For each sensor combination, we chose the optimal model, as acquired above, to compute the classification accuracies of each gesture and all three gestures (\{G1, E\}, \{G2, E\}, \{G3, E\}, and \{G1, G2, G3, E\}, where E refers to the empty gesture). All the evaluation settings were consistent with the first session. Such an evaluation helps to ground the applicability value of the minimal functionality under different hardware settings.


In the last session, we conducted an ablation study for the optimal model to analyze the effects of the optimization strategies in the model design: 1) pretraining, 2) dropout, and 3) warm-up. After getting the optimal model above, we ran the model in the same setting except disabling 1) all the three optimizations, 2) pretraining, 3) dropout, and 4) \revision{warm-up} to acquire the recognition accuracy in these 4 ablation settings. Such a study helped to validate the effectiveness of our model design.

All the above evaluations were conducted with leave-one-user-out cross-validation. \revision{For all the numerical comparisons, we reported the results along with the Wilcoxon Signed-Rank test to indicate the significance.}

% MODEL 4 PARTS: VOICE ULTRA BOOSTING IMU boosting
%优化目标：1.整体的正确率 2. 大类的可分的gestures 3.gesture的选取 3.有无boosting的 sensor-fusion/cross-device的组合 

\subsection{Results}

\begin{table}
  \vspace{-0.3cm}
  \centering
  \caption{Evaluation results regarding different sensor combinations and model selection. The notions in the table are consistent with Section 5.4. The numbers in the table indicate recognition accuracies in \% with standard deviations.}
  \Description{This table contains evaluation results regarding different sensor combinations and model selection. The notions in the table are consistent with Section 5.4. The numbers in the table indicate recognition accuracies in percent with standard deviations. }
  ~\label{tab:eval}
    \vspace{-0.3cm}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{|l|p{40pt}|p{40pt}|p{40pt}|p{40pt}|p{40pt}|p{40pt}|}
    \toprule
     & V & U & I & V+U & ALL-L & ALL-F \\
    \midrule
    RE & 39.5(6.3) & - & - & - & - & -\\
    \midrule
    LE+RE & 70.3(10.0) & - & - & - & - & -\\
    \midrule
    LE+RE+W & 84.2(12.0) & 52.4(11.5) & - & 85.8(13.2) & - & -\\
    \midrule
    ALL-4ch & 89.9(10.5) & 66.7(13.6) & 49.0(5.3) & 89.8(10.3) & 90.8(10.2) & \textbf{91.5(8.9)}\\
    \midrule
    ALL-6ch & 90.0(10.9) & 70.9(14.5) & 49.0(5.3) & 89.2(11.4) & 90.7(9.9) & 90.9(9.4)\\
    \bottomrule
  \end{tabular}
  }
  \vspace{-0.2cm}
\end{table}



Table \ref{tab:eval} showed the results of the recognition performance regarding sensor combination and model selection. 

For vocal-only models, we observed a constant increase in recognition accuracy as more sensor nodes were introduced (e.g., from 39.5\% with a single earbud to 90.0\% with all the sensors, \revision{$Z=-2.81, p < 0.05$}). However, the difference between ALL-4ch and ALL-6ch was not significant, meaning when multiple devices were used, the introduction of the earbuds' inner channels brings limited information for the vocal channel. For ultrasonic-only models, the performance increased from 52.5\% to 70.9\% \revision{($Z=-2.81, p < 0.05$)} as the ring microphone and the earbuds' inner microphones were added. Notably, the independent use of the ultrasonic channels has its unique advantage of not relying on the vocal feature so that the model can still work well in scenarios such as noisy environments and whispering. The IMU model achieved an accuracy of 49.0\%, meaning the IMU could provide complementary information on hand and finger movement, though far from practical as an individual model. 

As for the sensor fusion models, we notice the vocal+ultra model had a performance increase over the vocal-only model with fewer input channels (LE+RE+W, \revision{84.2\% V.S. 85.8\%, $Z=-1.64, p=0.1$}), while it had no increase for ALL-4ch \revision{(89.9\% V.S. 89.8\%, $Z = -0.18, p = 0.86$)} and had a decrease for ALL-6ch \revision{(90.0\% V.S. 89.2\%, $Z = -0.98, p = 0.33$)}. This is probably because the vocal-only model with multiple channels (e.g., 6 channels) is a strong baseline, and combining it with an inferior model would introduce additional noise. Regarding all-channel fusion, we found feature-level fusion slightly outperformed logit-level fusion in accuracy \revision{(91.5\% V.S. 90.8\%, $Z = -0.98, p = 0.33$)}, probably due to the larger parameter space. We also observed a slight performance decrease for fusion models when adding the inner channels of the earbuds to ALL-4ch \revision{(91.5\% V.S. 90.9\%, $Z = -0.36, p = 0.72$)}, although the difference was not significant. The optimal model (all-channel feature-level fusion for ALL-4ch) achieved a 9-class recognition accuracy of 91.5\%\revision{, which significantly outperformed the vocal-only model ($Z=-1.96, p < 0.05$) with the same channels.}

% probably because of the redundancy of the inner channels


\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{figures/confusion_matrix.png}
    \caption{The confusion matrix of different models: vocal-only, ultra-only, IMU-only, and feature-level fusion. 0-9 represent the following gestures respectively: 0 - pinch the ear rim, 1 - calling gesture, 2 - support cheek with palm, 3 - cover mouth with palm, 4 - cover ear with arched palm, 5 - thinking face gesture, 6 - hold up the palm beside nose and mouth, 7 - cover mouth with fist, and 8 - empty gesture.}
    \Description{This figure contains the confusion matrix of different models: vocal-only, ultra-only, IMU-only, and feature-level fusion. 0-9 represent the following gestures respectively: 0 - pinch the ear rim, 1 - calling gesture, 2 - support cheek with palm, 3 - cover mouth with palm, 4 - cover ear with arched palm, 5 - thinking face gesture, 6 - hold up the palm beside nose and mouth, 7 - cover mouth with fist, and 8 - empty gesture. }
    \label{fig:confusion}
\end{figure}

To ground a better understanding of how each channel (vocal, ultrasound, and IMU) contributed to the recognition, we analyzed the confusion matrix of four models (vocal-only, ultra-only, IMU-only, and feature-level fusion) under ALL-4ch, as shown in Figure \ref{fig:confusion}. This result was understandable because for the gestures with larger confusion, we could easily figure out their similarity based on semantics. For example, gesture pairs $(0,4)$ and $(3,7)$ yield larger confusion for vocal and ultrasound models, where we observed similar touch positions for each pair of gestures (ear for $(0,4)$ and mouth for $(3,7)$). Gesture 1 confuses with gestures 3 and 7 in the ultrasound model probably due to a similar hand position, though it yields less confusion for the vocal model probably due to different occlusion levels (gestures 3 and 7 yield greater occlusion) that may influence the frequency response of the human voice. 

Results on the reduced gesture set were shown in Table \ref{tab:eval_reduced}. Since ALL-4ch achieved higher recognition accuracy than ALL-6ch in the fusion model (e.g., 91.5\% \revision{V.S.} 90.9\%), we dropped ALL-6ch in this table. We had the following observations: 1) Using one earbud with inner and outer microphones (RE), which is a severely restricted setting, could achieve a narrowly applicable accuracy of over $80\%$ for recognizing a specific single gesture (82.3\% for G1 and 83.0\% for G2) while it performed worse in recognizing other gesture (e.g., G3) or multiple gestures, which is understandable due to limited sensing information. 2) Using a pair of earbuds (LE+RE) could significantly boost the performance, with promising accuracies of 87.3\% for recognizing all three gestures and 98.8\% for recognizing G2, indicating the high applicability of such compact hardware form. 3) Additional hardware including a watch and ring brought the feasibility of fusing more input channels (e.g., ultrasound), which constantly improved the performance to a highly robust one (e.g., 97.3\% for recognizing all three gestures and 100\% for recognizing G2 and G3) and meanwhile lifting the distinguishable gesture space (e.g., from 3 gestures to 8 gestures, see Table \ref{tab:eval}) with high applicability (e.g., 91.5\% for simultaneously recognizing 8 gestures). The above results showed a leap over previous work with similar interaction modality (e.g., PrivateTalk \cite{Yan-UIST-2019}), revealing the feasibility of broadened gesture space (e.g., recognizing 8 gestures simultaneously) and the effectiveness of multi-device sensing.


\begin{table}
  \vspace{-0.3cm}
  \centering
  \caption{Recognition accuracy on the reduced gesture set. G1: cover mouth with palm, G2: cover ear with arched palm, and G3: hold up the palm beside nose and mouth.}
  \Description{This table contains the recognition accuracy on the reduced gesture set. G1: cover mouth with palm, G2: cover ear with arched palm, and G3: hold up the palm beside nose and mouth. }
  ~\label{tab:eval_reduced}
    \vspace{-0.3cm}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{|l|p{40pt}|p{40pt}|p{40pt}|p{40pt}|}
    \toprule
     & RE & LE+RE & LE+RE+W & ALL-4ch \\
    \midrule
    G1 & 82.3(11.4) & 92.1(12.8) & 97.8(6.7) & 95.7(7.3) \\
    \midrule
    G2 & 83.0(13.0) & 98.8(1.9) & 97.9(6.3) & 100.0(0.0) \\
    \midrule
    G3 & 75.5(26.7) & 90.6(9.0) & 94.2(8.4) & 100.0(0.0) \\
    \midrule
    G1+G2+G3 & 64.4(10.6) & 87.3(8.9) & 91.3(11.3) & 97.3(4.5)\\
    \bottomrule
  \end{tabular}
  }
\end{table}



\begin{table}
  \centering
  \caption{Results of the ablation study. The numbers in the table indicate recognition accuracies in \% with standard deviations.}
  \Description{This table contains the results of the ablation study. The numbers in the table indicate recognition accuracies in \% with standard deviations. }
  ~\label{tab:ablation}
    \vspace{-0.3cm}
    
    \resizebox{0.5\columnwidth}{!}{
    \begin{tabular}{lc}
    \toprule
    Techniques & Accuracy \\
    \midrule
    No Optimization & 75.8(12.2) \\
    No Pretraining & 77.2(12.8) \\
    No Dropout & 91.2(7.8) \\
    No \revision{warm-up} & 87.8(11.1) \\
    \midrule
    Full Model & 91.5(8.9) \\
    \bottomrule
  \end{tabular}
  }
  \vspace{-0.3cm}
\end{table}

%  for single column
% \begin{table}
% \parbox{.62\linewidth}{
% \centering
% \caption{Recognition accuracy on the reduced gesture set. G1: cover mouth with palm, G2: cover ear with arched palm, and G3: hold up the palm beside nose and mouth.}~\label{tab:eval_reduced}
% \begin{tabular}{|l|p{40}|p{40}|p{40}|p{40}|}
%     \toprule
%      & RE & LE+RE & LE+RE+W & ALL-4ch \\
%     \midrule
%     G1 & 82.3(11.4) & 92.1(12.8) & 97.8(6.7) & 95.7(7.3) \\
%     \midrule
%     G2 & 83.0(13.0) & 98.8(1.9) & 97.9(6.3) & 100.0(0.0) \\
%     \midrule
%     G3 & 75.5(26.7) & 90.6(9.0) & 94.2(8.4) & 100.0(0.0) \\
%     \midrule
%     G1+G2+G3 & 64.4(10.6) & 87.3(8.9) & 91.3(11.3) & 97.3(4.5)\\
%     \bottomrule
%   \end{tabular}
% }
% \hfill
% \parbox{.33\linewidth}{
% \centering
% \caption{Results of the ablation study. The numbers in the table indicates recognition accuracies in \% with standard deviations.}~\label{tab:ablation}
% \begin{tabular}{lc}
%     \toprule
%     Techniques & Accuracy \\
%     \midrule
%     No Optimization & 75.8(12.2) \\
%     No Pretraining & 77.2(12.8) \\
%     No Dropout & 91.2(7.8) \\
%     No \revision{Warm-Up} & 87.8(11.1) \\
%     \midrule
%     Full Model & 91.5(8.9) \\
%     \bottomrule
%   \end{tabular}
% }
% \end{table}

The results of the ablation study are shown in Table \ref{tab:ablation}. We found disabling pretraining, dropout, and \revision{warm-up} caused different levels of performance degradation. Disabling pretraining caused the most significant decrease in performance ($-14.3\%$, \revision{$Z=-2.81, p < 0.05$}), which is probably because the feature extractor network (MobileNet V3) with pretraining on large-scale datasets could better extract different levels of image features. Meanwhile, disabling dropout caused slight decrease of $0.3\%$ \revision{($Z = -0.18, p = 0.86$), which was not significant,} and disabling \revision{warm-up} caused a decrease of $3.7\%$ \revision{($Z = -2.67, p < 0.05$)}. The introduction of \revision{warm-up} and dropout aims to optimize the training procedure (e.g., alleviating overfitting) and improve the robustness of the model. Compared with the raw model with no optimization, our model achieved a significant increase of $15.7\%$ \revision{($Z=-2.81, p < 0.05$)}, showing the superiority of all the optimization techniques. 