\section{Related Work}
In this section, we presented related work in three aspects: enhancing voice interaction with parallel gestures, hand-to-face interaction, and cross-device sensing for hand gestures.

%First, we review previous research to enhance voice interaction with parallel gestures. We then summarize the combination of the use of voice and hand gesture. Finally, we introduce different sensing technologies for \projectName{} gestures. 

%Gestures as pl channel to enhance voice interaction
%PL gesture design
%cross device sensing for hand gestures

% Para-language, which includes gestures, facial expressions, interfactional synchrony, eye contact, use of space, touching, aspects of voice modification, and silence are shown to play a crucial role in human interaction \cite{pennycook1985actions}.


\subsection{Enhancing Voice Interaction with Parallel Gestures}
Performing body gestures parallel to voice commands has been a prevalent method to convey certain intentions or information during the voice interaction. People tended to use gestures of different body segments, such as head gestures\cite{1374748,9053124,10.1145/1088463.1088470,10.1145/3313831.3376479}, gaze\cite{4115628, 10.1145/2043674.2043723}, hand gestures \cite{Yan-UIST-2019}, and facial expressions \cite{10.1145/3313831.3376810}, to provide supplementary information obliged for voice interaction. The purpose of introducing of certain parallel body gestures to the voice interface typically included: indicating the wakeup state \cite{10.1145/3411764.3445687,Yan-UIST-2019,10.1145/3351276}, serving as the control (or trigger) signal \cite{earbuddy,10.1145/3313831.3376810}, and passing scene-related context information \cite{10.1145/3313831.3376479,10.1145/3379337.3415588}. For example, Yan et al. \cite{10.1145/3313831.3376810} proposed frowning, a facial expression of para-language, to implement interrupting the responses during voice interactions between human and smart devices. Qin et al. \cite{10.1145/3411764.3445687} leveraged the speech features when user raise the microphone embeded in devices to close to mouth to facilitate wake-up free techniques. WorldGaze \cite{10.1145/3313831.3376479} used commodity a smartphone to recognize the real-world head-gaze location (e.g., certain buildings or objects) of a user to provide the voice agent with supplementary scene-related information for better comprehension.
% Researchers have proved the importance and benefits when introducing para-language to enhance voice interaction. For example, Nomoto et al. \cite{nomoto2011anger} applied para-linguistic cues named dialog-features such as turn-taking and backchannel feedback to recognize anger in conversational interactive situations more robustly. Pennycook et al. \cite{pennycook1985actions} examined the importance of para-language (kinesics, proxemics, and paraverbal features) in communication and proposed some suggestions to facilitate students' acquisition of para-language. 

% In addition to leveraging para-language to enhance the machines' understanding of semantics or user behaviour in voice interaction, researchers also attempted to use para-language as active input with the aim of realizing a faster and more natural way of voice interaction. For example, Yan et al. \cite{10.1145/3313831.3376810} proposed frowning, a facial expression of para-language, to implement interrupting the responses during voice interactions between human and smart devices. Qin et al. \cite{10.1145/3411764.3445687} leveraged the speech features when user raise the microphone embeded in devices to close to mouth to facilitate wake-up free techniques. Yan et al. \cite{Yan-UIST-2019} introduced an interaction technique which allow users to activate voice input by performing the Hand-On-Mouth gesture during speaking. 

Our work was also within the framework of using parallel gestures as active input to enhance voice interaction, which was most related to but achieved a leap over PrivateTalk \cite{Yan-UIST-2019}, which allowed users to activate voice input by performing a hand-on-mouth gesture during speaking. Compared with existing work \cite{10.1145/3313831.3376810,10.1145/3411764.3445687,Yan-UIST-2019} where a specific gesture (e.g., bringing the phone to the mouth\cite{10.1145/3411764.3445687}) was designed and recognized for specific functionality (e.g., interrupting the conversation or activating the voice assistance), our multiple VAHF gesture recognition on multi-modal wearable devices could effectively broaden the input channel of actions as parallel information with the potential of supporting a larger interaction space, such as defining multiple shortcuts. 


% actions as paralinguistic information to enhance xxx interaction. Unlike previous work (you can compare PrivateTalk and ProxiMic here), most of the previous work is to realize single action recognition, such as PrivateTalk, ProxiMic , RaiseToSpeek, so only a single modal control or switching (such as waking up a voice assistant) is supported, and our fusion perception based on multi-modal wearable devices can effectively broaden the input channel of actions as paralinguistic information, so it has the potential to support more Large interaction space, such as Shortcut bindings, parameter input, etc.

% Inspired by the benefits of introducing para-linguistic features to facilitate a natural and fast voice interaction, we 





% Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a closeto-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded
% device close to the mouth and speaks directly to the device without wake-up phrases or button presses.

%写的挺好的
%xi
% propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. 


% papers:
% \cite{1374748} Fujie, A conversation robot that recognizes user's head gestures and uses its results as para-linguistic information is developed. Conversation robots using head gestures as para-linguistic information
% \cite{4115628} Fujie, using gaze as para-linguistic feature to implement conversation robots
% \cite{10.1145/2043674.2043723}Lv, use eye-tracking to recognize attitudes
% \cite{9053124},author=Vadiraj, use head gestures to identify speakers when more than one speakers are narrating a story.
% \cite{pennycook1985actions}author={Pennycook}, This article examines the importance of paralanguage (kinesics, proxemics, and paraverbal features) in communication. Gestures, facial expressions, interfactional synchrony, eye contact, use of space, touching, aspects of voice modification, and silence are shown to play a crucial role in human interaction and to be highly culture-specific. The implications of this broad paradigm of communication are discussed with respect to language development, and it is suggested that paralanguage be included as a primary facet of communicative competence. Finally, the importance of awareness of paralanguage in the classroom is discussed, and a number of suggestions are made to facilitate students' acquisition of paralanguage.

% \cite{furui2003recent}author={Furui}, analyse intonation as para-linguistic information to enhance speech recognition

% \cite{nomoto2011anger}author:Nomoto, To recognize anger more robustly, we apply other para-linguistic cues named dialog-features which are seen in conversational interactive situations between two speakers such as turn-taking and backchannel feedback.

% \cite{1318446},  author=Fujie, In this paper, we present a recognition method of attitudes by prosodic information, and a recognition method of head gestures. In the former method, in order to recognize two attitudes, such as "positive" and "negative", F0 pattern and phoneme alignment are introduced as features. Experiment results show that these methods are sufficient to recognize the user attitude as para-linguistic information. Finally, we show a prototype spoken dialogue system using para-linguistic information and how these sorts of information contribute to efficient conversation.

% \cite{10.1145/1088463.1088470}, author = {Morency}, predict para-linguistic head gestures by recognizing contextual dialogue


% %use para-language as input for voice interaction
% \cite{10.1145/3313831.3376810}, author = {Yan} We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. 

% \cite{Yan-UIST-2019}author = Yan, We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. 

% \cite{10.1145/3411764.3445687} author = {Qin}, Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a closeto-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded
% device close to the mouth and speaks directly to the device without wake-up phrases or button presses.




\subsection{Hand-to-Face Interaction}
%validation/priority
%design space
Gestures involving hand and face have been demonstrated as a natural and easy-to-use way to input commands. Prior research has proposed the validation of the inherent unobtrusiveness, subtlety and social acceptability\cite{HWDs} of hand-to-face interaction. Design space of hand-to-face gestures has been explored by prior research. For example, different face regions such as the ear \cite{EarTouch}, cheek \cite{cheek,HWDs} or nose \cite{nose} were demonstrated to have the viability of hand-to-face input. Mahmoud et al. \cite{hand-over-gesture_book} proves that people prefer lower face regions to upper regions, especially chin, mouth and lower cheeks, for naturalistic interaction. Weng et al. \cite{46-facesight} proposed recognition of hand-to-face gestures for AR glasses. Serrano et al. \cite{HWDs} provided a set of guidelines for developing effective Hand-to-Face interaction techniques and found that the cheek is the most promising area on the face. Miniaturizing obfuscating, screening, camouflaging and re-purposing have been purposed by Lee et al.\cite{Acceptable} as the strategies of the design of socially acceptable hand-to-face gestures. We refer to some principles (e.g. lower face region, social acceptance, etc.) from previous work mentioned above to design our subjective evaluation process. Different from previous work, our design space of hand-to-face gestures are generated from real-life conversations, which has more para-linguistic features resulting in performing more easily and naturally. 

Combining voice input with hand gestures can provide rich information to enable convenient and expressive multi-modal interaction~\cite{Katsamanis2017}. "Put that there" was the first multi-model interaction method introducing the pointing gesture to indicate the location mentioned in the voice command~\cite{Bolt1980}. Bourguet et al. studied the temporal synchronization between speech (Japanese) and hand pointing gestures during multi-modal interaction~\cite{Bourguet1998}. Sauras-Perez et al. \cite{VoGe2017} proposed a human vehicle interaction system based on voice and pointing gestures that enables the user making spontaneous decisions over the route and communicate them to the car. Closest to our work, Yan et al. \cite{Yan-UIST-2019} proposed to enable hand-to-mouth gesture interaction as a wake-up action for voice interfaces. However, the combination of the hand-to-face gestures and voice interaction in previous work is limited to few types of gestures and the fixed usage patterns. In our work, we discussed more types of gestures which can be used in versatile \revision{scenarios} of voice interaction.










\subsection{Cross-Device Sensing for Hand Gestures}
%先单音频、单imu，再写cross-device，还是一起写？
Free-form hand gesture sensing is key to enabling rich interaction space taking advantage of the expressiveness of human hand. Previous literature has investigated sensing solutions for free-form hand gestures with different sensors including cameras (RGB \cite{17-interface,19-fish-eye,30-Generate-hands-tracking,55-handsee}, IR \cite{3-IR-touch-on-phone,46-facesight}, and depth \cite{1-depth,2-depth}), EMG sensors \cite{1-EMG,2-EMG}, capacitive sensors \cite{1-capacitive}, millimeter-wave radar \cite{10.1145/2897824.2925953,10.1145/2984511.2984565,7907299}, acoustic sensors \cite{Wang2019,gupta2012soundwave,Ono2013,Liu2019}, and inertial sensors \cite{7-arm,31-ViBand,59-Smartwatch-Based,62-gesture-with-acc}. Among these sensors, the camera is most widely researched since it has the strongest sensing capability to capture pixel-wise image data, based on which many computer vision models have been developed for fine-grained hand sensing such as detecting hand keypoints and recovering the hand pose. However, vision-based hand gesture sensing often requires externally-mounted camera and heavy computation (e.g., using a GPU), which prevents the practical use in mobile and pervasive scenarios. Similarly, sensing methods based on EMG sensors \cite{1-EMG,2-EMG}, capacitive sensors \cite{1-capacitive}, and millimeter-wave radar \cite{10.1145/2897824.2925953,10.1145/2984511.2984565,7907299} require additional wearing on the human body, making them far from practical deployment. In our work, we focused on the latter two - microphones and inertial sensors - because they are computational efficient and largely equipped on commodity devices such as smartphones, earbuds, smartwatches, and smart rings. Below we presented related work on acoustic- and inertial-based hand gesture sensing. 

\subsubsection{Acoustic sensing}

The principle of acoustic sensing for hand gestures is to measure how specific hand gesture influences the propagation of active (e.g., active ultrasound) or passive (e.g., human voice) sound sources or makes a sound. Based on the presence of an active sound source, it can be categorized into active acoustic sensing and passive acoustic sensing.

Active acoustic sensing methods~\cite{Wang2019} has been widely explored for gesture recognition including in-air hand gesture~\cite{Yuzhou-reflectrack, gupta2012soundwave}, face orientation~\cite{yuntao-faceori}, ambient activity~\cite{Mesaros2010}, touch gesture on everyday objects or surfaces~\cite{Ono2013, Liu2019}, finger tracking~\cite{Yun2017}, silent speech interface~\cite{Gao2020, Zhang2020}, cough monitoring~\cite{wang-hearcough} or sleep activity recognition~\cite{Ken-sleep}. For example, Touch \& Activate~\cite{Ono2013} enabled touch interface on everyday objects by measuring the acoustic frequency response of the object and the touch gesture.
Strata~\cite{Yun2017} enabled 2-dimensional finger position tracking using the reflective impulse audio signal on commodity smartphones. Ando et al. \cite{Ando2017} modeled the transfer function, or the propagation path of the sound, and used it in gesture recognition. These methods recognized the acoustic echo or propagation features caused by the gestures using a speaker and one or more microphones. Doppler effect was also frequently used for sensing subtle hand gestures involving relative movements~\cite{gupta2012soundwave,Liu2019}. EchoWhisper ~\cite{Gao2020} also leveraged the Doppler shift of reflection for near-ultrasound sound waves caused by the mouth and tongue movements to interpret the speech and build a silent speech interface.


Passive acoustic sensing recognizes sound activities using merely microphones~\cite{Wu2020, Laput2018,Harrison2012,Harrison2011,Xiao-2014-Toffee, Mesaros2010}. For instance, Toffee~\cite{Xiao-2014-Toffee} enabled an ad-hoc touch interface on a table around the device using time of arrival correction. TapSense~\cite{Harrison2011} enhanced finger interaction on touch screens by detecting the unique sound features of fingertip, pad, knuckle, and nail. Acoustic Barcodes~\cite{Harrison2012} was an identifying tag that used notches to produce sound when dragged across, which can be recognized by a microphone for information retrieval or triggering interactive functions. Daily activities or ambient environment (e.g., taking a bus) can be detected based on features of the sound collected by a single microphone~\cite{Mesaros2010} or a microphone array. Ubicoustics \cite{Laput2018} proposed a plug-and-use sound recognition pipeline for general-purpose activity recognition. Wu et al.~\cite{Wu2020} further extended the environment acoustic event detection using an end-to-end system for self-supervised learning of events labeled through one-shot interaction.

In our work, we took advantage of both passive and active acoustic sensing. To be more specific, passive acoustic sensing recognized the hand's influence on the features of the accompanying speech, including frequency response, amplitude, etc. Active acoustic sensing helped to determine relative position-based features among different devices. The two channels can provide supplementary capability in VAHF gesture sensing.



\subsubsection{Inertial Sensing}
Inertial sensors, commonly integrated into commodity devices, are efficient in detecting motion- and attitude-related hand/finger gestures. For instance, a number of previous works \cite{7-arm,31-ViBand,59-Smartwatch-Based,62-gesture-with-acc,10.1145/3478114,10.1145/3569463} used acceleration and rotation with wrist-worn inertial sensors to recognize hand gesture. Serendipity \cite{Serendipity} recognized five fine-grained gestures based on the IMU in off-the-shelf smartwatches. Mo-Bi \cite{Mo-Bi2016}  used a smartphone and two accelerometer-embedded wrist-worn devices for each hand to collect the hand-posture data and developed the implicit hand-posture recognition software. Leveraging inertial sensors integrated into smartwatches, Float \cite{SunkeFloat2017} recognized wrist-to-finger gestures to enhance one-hand and smartwatch interaction. Gu et al. \cite{Gu-IMU-RING-TYPING} enabled one-finger typing with an index-finger-worn IMU ring by detecting hand-to-surface touching events and rotation angles. Lu et al. \cite{Lu-handtohand} studied the sensing capability of dual wrist-worn devices and analyzed cross-device features for more accurate gesture inference. 

Inspired by these works, we incorporated an IMU ring into our sensing system to capture the motion features of VAHF gestures. 

\subsubsection{Sensor Fusion Methods for Hand Gestures}
Previous research has explored cross-device sensor fusion methods to enhance 
the recognition capability of different types of hand gestures, especially when the sensing capability of different sensors are complementary for recognizing different features. Sensor fusion methods included homogeneous fusion and heterogeneous fusion. Homogeneous fusion aimed to add more homogeneous sensor nodes into the sensing system to capture fine-grained information. For example, fusing camera captures from different views \cite{moon2020interhand2} is a classical and effective solution to reduce 3D reconstruction or detection error which is also widely used in generating high-quality machine annotations. For acoustic sensing, adding more microphones to the scene achieved more fine-grained acoustic measurements, which is beneficial to various sensing purposes such as 2D localization \cite{Yun2017} and gesture classification \cite{Wang2019}.
%特别是当sensors对于传感某类手势的不同特征的能力互补的时候

The other type is heterogeneous fusion, where different types of sensors are combined to merge their strengths \cite{UWB-and-Doppler,EMG-and-camera,RFID,EMG-and-IMU-for-stroke,Kinect-and-IMU}. For example, Li et al. \cite{UWB-and-Doppler} presented a hierarchical sensor fusion approach for human micro-gesture recognition by combining an Ultra Wide Band (UWB) Doppler radar and wearable pressure sensors. Ceolini et al. \cite{EMG-and-camera} presented a sensor fusion framework that integrates complementary systems: the electromyography (EMG) signal from muscles and visual information. Ceolini et al. \cite{EMG-and-camera2019} also investigated the fusion of EMG and a camera on limited computational resources of mobile phones to detect gestures. A more typical scene is fusing an IMU with a camera \cite{10.1145/3332165.3347947}, where the IMU detects the subtle contact signal and the camera senses the global hand state. Acoustico \cite{10.1145/3379337.3415901} fused acoustic and IMU signal for 2D tap position localization based on the TDOA of the tap sound's two propagation paths.

In our work, we adopted both homogeneous fusion and heterogeneous fusion strategies. The former aimed to probe more measurement nodes into the sensing space, while the latter aimed to capture different types of features from different channels.




