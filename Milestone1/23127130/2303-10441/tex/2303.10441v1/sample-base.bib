
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

@inproceedings{Yan-UIST-2019,
author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing},
booktitle = {UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3332165.3347950},
isbn = {9781450368162},
keywords = {Hand gesture,Voice input},
pages = {1013--1020},
title = {{PrivateTalk: Activating voice input with hand-on-mouth gesture detected by bluetooth earphones}},
year = {2019}
}

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@Article{Abril07,
  author        = "Patricia S. Abril and Robert Plant",
  title         = "The patent holder's dilemma: Buy, sell, or troll?",
  journal       = "Communications of the ACM",
  volume        = "50",
  number        = "1",
  month         = jan,
  year          = "2007",
  pages         = "36--44",
  doi           = "10.1145/1188913.1188915",
  url           = "http://doi.acm.org/10.1145/1219092.1219093",
  note          = "",
}

@Article{Cohen07,
  author        = "Sarah Cohen and Werner Nutt and Yehoshua Sagic",
  title         = "Deciding equivalances among conjunctive aggregate queries",
  journal       = JACM,
  articleno     = 5,
  numpages      = 50,
  volume        = 54,
  number        = 2,
  month         = apr,
  year          = 2007,
  doi           = "10.1145/1219092.1219093",
  url           = "http://doi.acm.org/10.1145/1219092.1219093",
  acmid         = 1219093,
}


@periodical{JCohen96,
  key =          "Cohen",
  editor =       "Jacques Cohen",
  title =        "Special issue: Digital Libraries",
  journal =      CACM,
  volume =       "39",
  number =       "11",
  month =        nov,
  year =         "1996",
}


@Book{Kosiur01,
  author =       "David Kosiur",
  title =        "Understanding Policy-Based Networking",
  publisher =    "Wiley",
  year =         "2001",
  address =      "New York, NY",
  edition =      "2nd.",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Harel79,
  author =       "David Harel",
  year =         "1979",
  title =        "First-Order Dynamic Logic",
  series =       "Lecture Notes in Computer Science",
  volume =       "68",
  address =      "New York, NY",
  publisher =    "Springer-Verlag",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09237-4",
  editor =       "",
  number =       "",
  month =        "",
  note =         "",
}


@Inbook{Editor00,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book one",
  subtitle =     "The book subtitle",
  series =       "The name of the series one",
  year =         "2007",
  volume =       "9",
  address =      "Chicago",
  edition =      "1st.",
  publisher =    "University of Chicago Press",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09456-9",
  chapter =      "",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}

%
@InBook{Editor00a,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book two",
  subtitle =     "The book subtitle",
  series =       "The name of the series two",
  year =         "2008",
  address =      "Chicago",
  edition =      "2nd.",
  publisher =    "University of Chicago Press",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09456-9",
  volume =       "",
  chapter =      "100",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Spector90,
  author =       "Asad Z. Spector",
  title =        "Achieving application requirements",
  booktitle =    "Distributed Systems",
  publisher =    "ACM Press",
  address =      "New York, NY",
  year =         "1990",
  edition =      "2nd.",
  chapter =      "",
  editor =       "Sape Mullender",
  pages =        "19--33",
  doi =          "10.1145/90417.90738",
  url =          "http://doi.acm.org/10.1145/90417.90738",
  volume =       "",
  number =       "",
  series =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Douglass98,
  author =       "Bruce P. Douglass and David Harel and Mark B. Trakhtenbrot",
  title =        "Statecarts in use: structured analysis and object-orientation",
  series =       "Lecture Notes in Computer Science",
  booktitle =    "Lectures on Embedded Systems",
  publisher =    "Springer-Verlag",
  address =      "London",
  volume =       "1494",
  year =         "1998",
  chapter =      "",
  editor =       "Grzegorz Rozenberg and Frits W. Vaandrager",
  pages =        "368--394",
  doi =          "10.1007/3-540-65193-4_29",
  url =          "http://dx.doi.org/10.1007/3-540-65193-4_29",
  edition =      "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


@Book{Knuth97,
  author =       "Donald E. Knuth",
  title =        "The Art of Computer Programming, Vol. 1: Fundamental Algorithms (3rd. ed.)",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  year =         "1997",
  address =      "",
  edition =      "",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Knuth98,
  author =       "Donald E. Knuth",
  year =         "1998",
  title =        "The Art of Computer Programming",
  series =       "Fundamental Algorithms",
  volume =       "1",
  edition =      "3rd",
  address =      "",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  doi =          "",
  url =          "",
  editor =       "",
  number =       "",
  month =        "",
  note =         "(book)",
}

@misc{ wiki:transfer,
    author = "{Wikipedia contributors}",
    title = "Transfer function --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Transfer_function&oldid=982071949}",
    note = "[Online; accessed 21-October-2020]"
}

@inproceedings{HWDs,
author = {Serrano, Marcos and Ens, Barrett M. and Irani, Pourang P.},
title = {Exploring the Use of Hand-to-Face Input for Interacting with Head-Worn Displays},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556984},
doi = {10.1145/2556288.2556984},
abstract = {We propose the use of Hand-to-Face input, a method to interact with head-worn displays
(HWDs) that involves contact with the face. We explore Hand-to-Face interaction to
find suitable techniques for common mobile tasks. We evaluate this form of interaction
with document navigation tasks and examine its social acceptability. In a first study,
users identify the cheek and forehead as predominant areas for interaction and agree
on gestures for tasks involving continuous input, such as document navigation. These
results guide the design of several Hand-to-Face navigation techniques and reveal
that gestures performed on the cheek are more efficient and less tiring than interactions
directly on the HWD. Initial results on the social acceptability of Hand-to-Face input
allow us to further refine our design choices, and reveal unforeseen results: some
gestures are considered culturally inappropriate and gender plays a role in selection
of specific Hand-to-Face interactions. From our overall results, we provide a set
of guidelines for developing effective Hand-to-Face interaction techniques.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3181â€“3190},
numpages = {10},
keywords = {body interaction, mobile interfaces, hwd, head-worn display, input techniques, hmd},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@InProceedings{hand-over-gesture_book,
author="Mahmoud, Marwa
and Robinson, Peter",
editor="D'Mello, Sidney
and Graesser, Arthur
and Schuller, Bj{\"o}rn
and Martin, Jean-Claude",
title="Interpreting Hand-Over-Face Gestures",
booktitle="Affective Computing and Intelligent Interaction",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="248--255",
abstract="People often hold their hands near their faces as a gesture in natural conversation, which can interfere with affective inference from facial expressions. However, these gestures are valuable as an additional channel for multi-modal inference. We analyse hand-over-face gestures in a corpus of naturalistic labelled expressions and propose the use of those gestures as a novel affect cue for automatic inference of cognitive mental states. We define three hand cues for encoding hand-over-face gestures, namely hand shape, hand action and facial region occluded, serving as a first step in automating the interpretation process.",
isbn="978-3-642-24571-8"
}








%Inbook{Knuth97,
%  author =       "Donald E. Knuth",
%  title =        "The Art of Computer Programming",
%  booktitle =    "the booktitle",
%  edition =      "3",
%  volume =       "1",
%  year =         "1997",
%  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
%  editor =       "",
%  number =       "",
%  series =       "Fundamental Algorithms",
%  type =         "",
%  chapter =      "",
%  pages =        "",
%  address =      "",
%  month =        "",
%  note =         "(inbook)",
%}

%INBOOK{DK:73-inbook-full,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (inbook w series)",
%   volume = 1,
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   edition = "Second",
%   month = "10~" # jan,
%   year = "1973",
%   type = "Section",
%   chapter = "1.2",
%   pages = "10--119",
%   note = "Full INBOOK entry (w series)",
%}

%INcollection{DK:74-incoll,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1974",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor",
%}

%INcollection{DK:75-incollws,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll w series)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1975",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor and series",
%}


@incollection{GM05,
Author= "Dan Geiger and Christopher Meek",
Title= "Structured Variational Inference Procedures and their Realizations (as incol)",
Year= 2005,
Booktitle="Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics, {\rm The Barbados}",
Publisher="The Society for Artificial Intelligence and Statistics",
Month= jan,
Editors= "Z. Ghahramani and R. Cowell"
}

@Inproceedings{Smith10,
  author =       "Stan W. Smith",
  title =        "An experiment in bibliographic mark-up: Parsing metadata for XML export",
  booktitle =    "Proceedings of the 3rd. annual workshop on Librarians and Computers",
  series =       "LAC '10",
  editor =       "Reginald N. Smythe and Alexander Noble",
  volume =       "3",
  year =         "2010",
  publisher =    "Paparazzi Press",
  address =      "Milan Italy",
  pages =        "422--431",
  doi =          "99.9999/woot07-S422",
  url =          "http://dx.doi.org/99.0000/woot07-S422",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Inproceedings{VanGundy07,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2007,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '07",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    {Paper 7},
  numpages =     9,
}

@Inproceedings{VanGundy08,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2008,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '08",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    7,
  numpages =     2,
  pages =        "99-100",
}

@Inproceedings{VanGundy09,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2009,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '09",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  pages =        "90--100",
}

@Inproceedings{Andler79,
  author =       "Sten Andler",
  title =        "Predicate Path expressions",
  booktitle =    "Proceedings of the 6th. ACM SIGACT-SIGPLAN symposium on Principles of Programming Languages",
  series =       "POPL '79",
  year =         "1979",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "226--236",
  doi =          "10.1145/567752.567774",
  url =          "http://doi.acm.org/10.1145/567752.567774",
  editor =       "",
  volume =       "",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Techreport{Harel78,
  author =       "David Harel",
  year =         "1978",
  title =        "LOGICS of Programs: AXIOMATICS and DESCRIPTIVE POWER",
  institution =  "Massachusetts Institute of Technology",
  type =         "MIT Research Lab Technical Report",
  number =       "TR-200",
  address =      "Cambridge, MA",
  month =        "",
  note =         "",
}

@MASTERSTHESIS{anisi03,
author = {David A. Anisi},
title = {Optimal Motion Control of a Ground Vehicle},
school = {Royal Institute of Technology (KTH), Stockholm, Sweden},
intitution = {FOI-R-0961-SE, Swedish Defence Research Agency (FOI)},
year = {2003},
}


@Phdthesis{Clarkson85,
  author =       "Kenneth L. Clarkson",
  year =         "1985",
  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
  school =       "Stanford University",
  address =      "Palo Alto, CA",
  note =         "UMI Order Number: AAT 8506171",
  type =         "",
  month =        "",
}


@online{Thornburg01,
  author =       "Harry Thornburg",
  year =         "2001",
  title =        "Introduction to Bayesian Statistics",
  url =          "http://ccrma.stanford.edu/~jos/bayes/bayes.html",
  month =        mar,
  lastaccessed = "March 2, 2005",
}


@online{Ablamowicz07,
  author =       "Rafal Ablamowicz and Bertfried Fauser",
  year =         "2007",
  title =        "CLIFFORD: a Maple 11 Package for Clifford Algebra Computations, version 11",
  url =          "http://math.tntech.edu/rafal/cliff11/index.html",
  lastaccessed = "February 28, 2008",
}


@misc{Poker06,
  author =       "Poker-Edge.Com",
  year =         "2006",
  month =        mar,
  title =        "Stats and Analysis",
  lastaccessed = "June 7, 2006",
  url =          "http://www.poker-edge.com/stats.php",
}

@misc{Obama08,
  author        = "Barack Obama",
  year          = "2008",
  title         = "A more perfect union",
  howpublished  = "Video",
  day           = "5",
  url           = "http://video.google.com/videoplay?docid=6528042696351994555",
  month         = mar,
  lastaccessed  = "March 21, 2008",
  note          =  "",
}

@misc{JoeScientist001,
  author =       "Joseph Scientist",
  year =         "2009",
  title =        "The fountain of youth",
  note =         "Patent No. 12345, Filed July 1st., 2008, Issued Aug. 9th., 2009",
  url =          "",
  howpublished = "",
  month =        aug,
  lastaccessed = "",
}


@Inproceedings{Novak03,
  author =       "Dave Novak",
  title =        "Solder man",
  booktitle =    "ACM SIGGRAPH 2003 Video Review on Animation theater Program: Part I - Vol. 145 (July 27--27, 2003)",
  year =         "2003",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "4",
  month =        "March 21, 2008",
  doi =          "99.9999/woot07-S422",
  url =          "http://video.google.com/videoplay?docid=6528042696351994555",
  note =         "",
  howpublished = "Video",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  organization = "",
}


@article{Lee05,
  author =       "Newton Lee",
  year =         "2005",
  title =        "Interview with Bill Kinder: January 13, 2005",
  journal =      "Comput. Entertain.",
  eid =          "4",
  volume =       "3",
  number =       "1",
  month =        "Jan.-March",
  doi =          "10.1145/1057270.1057278",
  url =          "http://doi.acm.org/10.1145/1057270.1057278",
  howpublished = "Video",
  note =         "",
}

@article{rous08,
  author =       "Bernard Rous",
  year =         "2008",
  title =        "The Enabling of Digital Libraries",
  journal =      "Digital Libraries",
  volume =       "12",
  number =       "3",
  month =        jul,
  articleno =    "Article~5",
  doi =          "",
  url =          "",
  howpublished = "",
  note =         "To appear",
}

@article{384253,
 author = {Werneck,, Renato and Setubal,, Jo\~{a}o and da Conceic\~{a}o,, Arlindo},
 title = {(old) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = {5},
 year = {2000},
 issn = {1084-6654},
 pages = {11},
 doi = {http://doi.acm.org/10.1145/351827.384253},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


@article{Werneck:2000:FMC:351827.384253,
 author = {Werneck, Renato and Setubal, Jo\~{a}o and da Conceic\~{a}o, Arlindo},
 title = {(new) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = 5,
 month = dec,
 year = 2000,
 issn = {1084-6654},
 articleno = 11,
 url = {http://portal.acm.org/citation.cfm?id=351827.384253},
 doi = {10.1145/351827.384253},
 acmid = 384253,
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(old) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 doi = {http://dx.doi.org/10.1016/j.inffus.2009.01.002},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 }

@article{Conti:2009:DDS:1555009.1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(new) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 month = oct,
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=1555009.1555162},
 doi = {10.1016/j.inffus.2009.01.002},
 acmid = {1555162},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Clone detection, Distributed protocol, Securing data fusion, Wireless sensor networks},
}

@inproceedings{Li:2008:PUC:1358628.1358946,
 author = {Li, Cheng-Lun and Buyuktur, Ayse G. and Hutchful, David K. and Sant, Natasha B. and Nainwal, Satyendra K.},
 title = {Portalis: using competitive online interactions to support aid initiatives for the homeless},
 booktitle = {CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 pages = {3873--3878},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=1358628.1358946},
 doi = {10.1145/1358628.1358946},
 acmid = {1358946},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cscw, distributed knowledge acquisition, incentive design, online games, recommender systems, reputation systems, user studies, virtual community},
}

@book{Hollis:1999:VBD:519964,
 author = {Hollis, Billy S.},
 title = {Visual Basic 6: Design, Specification, and Objects with Other},
 year = {1999},
 isbn = {0130850845},
 edition = {1st},
 publisher = {Prentice Hall PTR},
 address = {Upper Saddle River, NJ, USA},
 }


@book{Goossens:1999:LWC:553897,
 author = {Goossens, Michel and Rahtz, S. P. and Moore, Ross and Sutor, Robert S.},
 title = {The  Latex Web Companion: Integrating TEX, HTML, and XML},
 year = {1999},
 isbn = {0201433117},
 edition = {1st},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
 }

% need to test genres for errant isbn output

% techreport
@techreport{897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

@techreport{Buss:1987:VTB:897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

% whole proceedings

@proceedings{Czerwinski:2008:1358628,
 author = {},
 note = {General Chair-Czerwinski, Mary and General Chair-Lund, Arnie and Program Chair-Tan, Desney},
 title = {CHI '08: CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 order_no = {608085},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

% phdthesis

@phdthesis{Clarkson:1985:ACP:911891,
 author = {Clarkson, Kenneth Lee},
 advisor = {Yao, Andrew C.},
 title = {Algorithms for Closest-Point Problems (Computational Geometry)},
 year = {1985},
 note = {AAT 8506171},
 school = {Stanford University},
 address = {Stanford, CA, USA},
 }
% school is being picked up -- but not publisher (which is OK)
% Also -- the title is NOT being output in italics !!! Arrrrgh! - I fixed it. :-)


%%% compare with 'old'
%%% atsign-Phdthesis{Clarkson85,
%%%  author =       "Kenneth L. Clarkson",
%%%  year =         "1985",
%%%  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
%%%  school =       "Stanford University",
%%%  address =      "Palo Alto, CA",
%%%  note =         "UMI Order Number: AAT 8506171",
%%%  type =         "",
%%%  month =        "",
%%%}

% A bibliography
@Article{1984:1040142,
 key = {{$\!\!$}},
 journal = {SIGCOMM Comput. Commun. Rev.},
 year = {1984},
 issn = {0146-4833},
 volume = {13-14},
 number = {5-1},
 issue_date = {January/April 1984},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


% grinder
@inproceedings{2004:ITE:1009386.1010128,
 key = {IEEE},
 title = {IEEE TCSC Executive Committee},
 booktitle = {Proceedings of the IEEE International Conference on Web Services},
 series = {ICWS '04},
 year = {2004},
 isbn = {0-7695-2167-3},
 pages = {21--22},
 url = {http://dx.doi.org/10.1109/ICWS.2004.64},
 doi = {http://dx.doi.org/10.1109/ICWS.2004.64},
 acmid = {1010128},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

% div book
@book{Mullender:1993:DS:302430,
 editor = {Mullender, Sape},
 title = {Distributed systems (2nd Ed.)},
 year = {1993},
 isbn = {0-201-62427-3},
 publisher = {ACM Press/Addison-Wesley Publishing Co.},
 address = {New York, NY, USA},
 }

% master thesis (as techreport and thesis)

@techreport{Petrie:1986:NAD:899644,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 publisher = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }

@MASTERSTHESIS{Petrie:1986:NAD:12345,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 school = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }




@BOOK{book-minimal,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   publisher = "Addison-Wesley",
   year = "1981",
}

% incollection (has an editor, title, and possibly a booktitle)
@INcollection{KA:2001,
 author = {Kong, Wei-Chang},
 Title = {The implementation of electronic commerce in SMEs in Singapore (as Incoll)},
 booktitle = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}


% with bibfield 'type' before chapter (note no editor)
@INBOOK{KAGM:2001,
 author = {Kong, Wei-Chang},
 type = {Name of Chapter:},
 chapter = {The implementation of electronic commerce in SMEs in Singapore (Inbook-w-chap-w-type)},
 title = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

%%% Notes! This is because the atsign-INBOOK citation type specifies EITHER
%%% editor or author, but not both. In my experiments with the harvard/dcu
%%% bibtex style (and presumably this applies to other styles too), bibtex
%%% ignores the editor information if author information exists in an
%%% atsign-INBOOK entry. atsign-INCOLLECTION is far more commonly used in my references,
%%% and in the absence of an editor I believe most bibtex styles will just
%%% ommit the editor from the reference - the chapter information will not
%%% end up in the in-text citation as you suggest it should be but at least
%%% there is a place to put the editor if necessary.



% was 'Inbook' -- changed to incollection - (editor is different to author) - need to tell Asad to codify as such.
@incollection{Kong:2002:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {Chapter 9},
  booktitle =   {E-commerce and cultural values (Incoll-w-text (chap 9) 'title')},
  year =        {2002},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}

% incol when the chapter is 'text' - due to presence of editor (different to author)
@incollection{Kong:2003:IEC:887006.887011,
 author = {Kong, Wei-Chang},
 title = {The implementation of electronic commerce in SMEs in Singapore (Incoll)},
 booktitle = {E-commerce and cultural values},
 editor = {Thanasankit, Theerasak},
 year = {2003},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

% ------ test
%incollection{Kong:2003:IEC:887006.887010,
% author = {Kong, Wei-Chang},
% chapter = {The implementation of electronic commerce in SMEs in Singapore (Incoll-text-in-chap)},
% booktitle = {booktitle E-commerce and cultural values},
% title =   {The title},
% editor = {Thanasankit, Theerasak},
% year = {2003},
% isbn = {1-59140-056-2},
% pages = {51--74},
% numpages = {24},
% url = {http://portal.acm.org/citation.cfm?id=887006.887010},
% acmid = {887010},
% publisher = {IGI Publishing},
% address = {Hershey, PA, USA},
%}


% ---------





% Need inbook with num in chapter

% and inbook with number in chapter
@InBook{Kong:2004:IEC:123456.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values - (InBook-num-in-chap)},
  chapter =     {9},
  year =        {2004},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}


% and inbook with text in chapter
@Inbook{Kong:2005:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-text-in-chap)},
  chapter =     {The implementation of electronic commerce in SMEs in Singapore},
  year =        {2005},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter:},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and inbook with a num and type field
@Inbook{Kong:2006:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-num chap)},
  chapter =     {22},
  year =        {2006},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter (in type field)},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and incol coz we have a BLANK chapter - due to presence of editor
%atIncollection{Kong:2006:IEC:887006.887011,
%  author =     {Kong, Wei-Chang},
%  editor =     {Theerasak Thanasankit},
%  title =      "The title"
%  booktitle =  {E-commerce and cultural values (Incol-coz-blank-chap)},
%  year =       {2006},
%  address =    {Hershey, PA, USA},
%  publisher =  {IGI Publishing},
%  url =        {http://portal.acm.org/citation.cfm?id=887006.887010},
%  type =       {Type!},
%  chapter =    {},
%  pages =      {51--74},
%  numpages =   {24},
%  acmid =      {887010},
%  isbn =       {1-59140-056-2},
%  number =     "",
%  month =      "",
%  note =       "",
%}

@article{SaeediMEJ10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi},
            title = {A library-based synthesis methodology for reversible logic},
            journal = {Microelectron. J.},
            volume = {41},
            number = {4},
            month = apr,
            year = {2010},
            pages = {185--194},
}

@ARTICLE{SaeediJETC10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi and Zahra Sasanian},
            title = {Synthesis of Reversible Circuit Using Cycle-Based Approach},
            journal = {J. Emerg. Technol. Comput. Syst.},
            volume = {6},
            number = {4},
            month = dec,
            year = {2010}
            }

% Asad's new version
@article{Kirschmer:2010:AEI:1958016.1958018,
 author = {Kirschmer, Markus and Voight, John},
 title = {Algorithmic Enumeration of Ideal Classes for Quaternion Orders},
 journal = {SIAM J. Comput.},
 issue_date = {January 2010},
 volume = {39},
 number = {5},
 month = jan,
 year = {2010},
 issn = {0097-5397},
 pages = {1714--1747},
 numpages = {34},
 url = {http://dx.doi.org/10.1137/080734467},
 doi = {https://doi.org/10.1137/080734467},
 acmid = {1958018},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {ideal classes, maximal orders, number theory, quaternion algebras},
}


% incol due to presence of booktitle
@incollection{Hoare:1972:CIN:1243380.1243382,
 author = {Hoare, C. A. R.},
 title = {Chapter II: Notes on data structuring},
 booktitle = {Structured programming (incoll)},
 editor = {Dahl, O. J. and Dijkstra, E. W. and Hoare, C. A. R.},
 year = {1972},
 isbn = {0-12-200550-3},
 pages = {83--174},
 numpages = {92},
 url = {http://portal.acm.org/citation.cfm?id=1243380.1243382},
 acmid = {1243382},
 publisher = {Academic Press Ltd.},
 address = {London, UK, UK},
}

% incol due to presence of booktitle
@incollection{Lee:1978:TQA:800025.1198348,
 author = {Lee, Jan},
 title = {Transcript of question and answer session},
 booktitle = {History of programming languages I (incoll)},
 editor = {Wexelblat, Richard L.},
 year = {1981},
 isbn = {0-12-745040-8},
 pages = {68--71},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800025.1198348},
 doi = {http://doi.acm.org/10.1145/800025.1198348},
 acmid = {1198348},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Dijkstra:1979:GSC:1241515.1241518,
 author = {Dijkstra, E.},
 title = {Go to statement considered harmful},
 booktitle = {Classics in software engineering (incoll)},
 year = {1979},
 isbn = {0-917072-14-6},
 pages = {27--33},
 numpages = {7},
 url = {http://portal.acm.org/citation.cfm?id=1241515.1241518},
 acmid = {1241518},
 publisher = {Yourdon Press},
 address = {Upper Saddle River, NJ, USA},
}

% incol due to booktitle
@incollection{Wenzel:1992:TVA:146022.146089,
 author = {Wenzel, Elizabeth M.},
 title = {Three-dimensional virtual acoustic displays},
 booktitle = {Multimedia interface design (incoll)},
 year = {1992},
 isbn = {0-201-54981-6},
 pages = {257--288},
 numpages = {32},
 url = {http://portal.acm.org/citation.cfm?id=146022.146089},
 doi = {10.1145/146022.146089},
 acmid = {146089},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Mumford:1987:MES:54905.54911,
 author = {Mumford, E.},
 title = {Managerial expert systems and organizational change: some critical research issues},
 booktitle = {Critical issues in information systems research (incoll)},
 year = {1987},
 isbn = {0-471-91281-6},
 pages = {135--155},
 numpages = {21},
 url = {http://portal.acm.org/citation.cfm?id=54905.54911},
 acmid = {54911},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

@book{McCracken:1990:SSC:575315,
 author = {McCracken, Daniel D. and Golden, Donald G.},
 title = {Simplified Structured COBOL with Microsoft/MicroFocus COBOL},
 year = {1990},
 isbn = {0471514071},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

% Let's include Boris / BBeeton entries  (multi-volume works)

@book {MR781537,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {III}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Pseudodifferential operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {viii+525},
      ISBN = {3-540-13828-5},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781536 (87d:35002a)},
MRREVIEWER = {Min You Qi},
}

@book {MR781536,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {IV}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Fourier integral operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {vii+352},
      ISBN = {3-540-13829-3},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781537 (87d:35002b)},
MRREVIEWER = {Min You Qi},
}

%%%%%%%%%%%%%%%%%%%%%% Start of Aptara sample bib entries

% acmsmall-sam.bib
@InProceedings{Adya-01,
  author        = {A. Adya and P. Bahl and J. Padhye and A.Wolman and L. Zhou},
  title         = {A multi-radio unification protocol for {IEEE} 802.11 wireless networks},
  booktitle     = {Proceedings of the IEEE 1st International Conference on Broadnets Networks (BroadNets'04)},
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "210--217"
}

@article{Akyildiz-01,
  author        = {I. F. Akyildiz and W. Su and Y. Sankarasubramaniam and E. Cayirci},
  title         = {Wireless Sensor Networks: A Survey},
  journal       = {Comm. ACM},
  volume        = 38,
  number        = "4",
  year          = {2002},
  pages         = "393--422"
}

@article{Akyildiz-02,
  author        = {I. F. Akyildiz and T. Melodia and K. R. Chowdhury},
  title         = {A Survey on Wireless Multimedia Sensor Networks},
  journal       = {Computer Netw.},
  volume        = 51,
  number        = "4",
  year          = {2007},
  pages         = "921--960"
}

@InProceedings{Bahl-02,
  author        = {P. Bahl and R. Chancre and J. Dungeon},
  title         = {{SSCH}: Slotted Seeded Channel Hopping for Capacity Improvement in {IEEE} 802.11 Ad-Hoc Wireless Networks},
  booktitle     = {Proceeding of the 10th International Conference on Mobile Computing and Networking (MobiCom'04)},
  publisher     = "ACM",
  address       = "New York, NY",
  year          = {2004},
  pages         = "112--117"
}

@misc{CROSSBOW,
  key       = {CROSSBOW},
  title     = {{XBOW} Sensor Motes Specifications},
  note      = {http://www.xbow.com},
  year      = 2008
}

@article{Culler-01,
  author        = {D. Culler and D. Estrin and M. Srivastava},
  title         = {Overview of Sensor Networks},
  journal       = {IEEE Comput.},
  volume        = 37,
  number        = "8 (Special Issue on Sensor Networks)",
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "41--49"
}

@misc{Harvard-01,
    key         = {Harvard CodeBlue},
    title       = {{CodeBlue}: Sensor Networks for Medical Care},
    note        = {http://www.eecs.harvard.edu/mdw/ proj/codeblue/},
    year        = 2008
}

@InProceedings{Natarajan-01,
    author      = {A. Natarajan and M. Motani and B. de Silva and K. Yap and K. C. Chua},
    title       = {Investigating Network Architectures for Body Sensor Networks},
    booktitle   = {Network Architectures},
    editor      = {G. Whitcomb and P. Neece},
    publisher   = "Keleuven Press",
    address     = "Dayton, OH",
    year        = {2007},
    pages       = "322--328",
    eprint      = "960935712",
    primaryclass = "cs",
}

@techreport{Tzamaloukas-01,
  author        = {A. Tzamaloukas and J. J. Garcia-Luna-Aceves},
  title         = {Channel-Hopping Multiple Access},
  number =        {I-CA2301},
  institution =   {Department of Computer Science, University of California},
  address =       {Berkeley, CA},
  year          = {2000}
}

@BOOK{Zhou-06,
  author        = {G. Zhou and J. Lu and C.-Y. Wan and M. D. Yarvis and J. A. Stankovic},
  title         = {Body Sensor Networks},
  publisher     = "MIT Press",
  address       = "Cambridge, MA",
  year          = {2008}
}

@mastersthesis{ko94,
author = "Jacob Kornerup",
title = "Mapping Powerlists onto Hypercubes",
school = "The University of Texas at Austin",
note = "(In preparation)",
year = "1994"}
%month = "dec",}

@PhdThesis{gerndt:89,
  author =       "Michael Gerndt",
  title =        "Automatic Parallelization for Distributed-Memory
                  Multiprocessing Systems",
  school =       "University of Bonn",
  year =         1989,
  address =      "Bonn, Germany",
  month =        dec
}

@article{6:1:1,
author = "J. E. {Archer, Jr.} and R. Conway and F. B. Schneider",
title = "User recovery and reversal in interactive systems",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "1",
month = jan,
year = 1984,
pages = "1--19"}

@article{7:1:137,
author = "D. D. Dunlop and V. R. Basili",
title = "Generalizing specifications for uniformly implemented loops",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "1",
month = jan,
year = 1985,
pages = "137--158"}

@article{7:2:183,
author = "J. Heering and P. Klint",
title = "Towards monolingual programming environments",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "2",
month = apr,
year = 1985,
pages = "183--213"}

@book{knuth:texbook,
author = "Donald E. Knuth",
title = "The {\TeX{}book}",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1984}

@article{6:3:380,
author = "E. Korach and D.  Rotem and N. Santoro",
title = "Distributed algorithms for finding centers and medians in networks",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "3",
month = jul,
year = 1984,
pages = "380--401"}

@book{Lamport:LaTeX,
author = "Leslie Lamport",
title = "\it {\LaTeX}: A Document Preparation System",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1986}

@article{7:3:359,
author = "F. Nielson",
title = "Program transformations in a denotational setting",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "3",
month = jul,
year = 1985,
pages = "359--379"}

%testing
@BOOK{test,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   volume = 2,
   series = "The Art of Computer Programming",
   publisher = "Addison-Wesley",
   address = "Reading, MA",
   edition = "2nd",
   month = "10~" # jan,
   year = "1981",
}

@inproceedings{reid:scribe,
author = "Brian K. Reid",
title = "A high-level approach to computer document formatting",
booktitle = "Proceedings of the 7th Annual Symposium on Principles of
  Programming Languages",
month = jan,
year = 1980,
publisher = "ACM",
address = "New York",
pages = "24--31"}

@article{Zhou:2010:MMS:1721695.1721705,
 author = {Zhou, Gang and Wu, Yafeng and Yan, Ting and He, Tian and Huang, Chengdu and Stankovic, John A. and Abdelzaher, Tarek F.},
 title = {A multifrequency MAC specially designed for wireless sensor network applications},
 journal = {ACM Trans. Embed. Comput. Syst.},
 issue_date = {March 2010},
 volume = 9,
 number = 4,
 month = {April},
 year = 2010,
 issn = {1539-9087},
 pages = {39:1--39:41},
 articleno = 39,
 numpages = 41,
 url = {http://doi.acm.org/10.1145/1721695.1721705},
 doi = {10.1145/1721695.1721705},
 acmid = 1721705,
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Wireless sensor networks, media access control, multi-channel, radio interference, time synchronization},
}


@online{TUGInstmem,
  key =          {TUG},
  year  =        2017,
  title =        "Institutional members of the {\TeX} Users Group",
  url =          "http://wwtug.org/instmem.html",
  lastaccessed = "May 27, 2017",
}

@online{CTANacmart,
  author =    {Boris Veytsman},
  title =  {acmart---{C}lass for typesetting publications of {ACM}},
  url =    {http://www.ctan.org/pkg/acmart},
  lastaccessed = {May 27, 2017}
  }

@ARTICLE{bowman:reasoning,
    author = {Bowman, Mic and Debray, Saumya K. and Peterson, Larry L.},
    title = {Reasoning About Naming Systems},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {795-825},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161471},
}

@ARTICLE{braams:babel,
    author = {Braams, Johannes},
    title = {Babel, a Multilingual Style-Option System for Use with LaTeX's Standard Document Styles},
    journal = {TUGboat},
    volume = {12},
    number = {2},
    pages = {291-301},
    month = {June},
    year = {1991},
}

@INPROCEEDINGS{clark:pct,
  AUTHOR = "Malcolm Clark",
  TITLE = "Post Congress Tristesse",
  BOOKTITLE = "TeX90 Conference Proceedings",
  PAGES = "84-89",
  ORGANIZATION = "TeX Users Group",
  MONTH = "March",
  YEAR = {1991}
}

@ARTICLE{herlihy:methodology,
    author = {Herlihy, Maurice},
    title = {A Methodology for Implementing Highly Concurrent Data Objects},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {745-770},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161469},
}

@BOOK{salas:calculus,
  AUTHOR = "S.L. Salas and Einar Hille",
  TITLE = "Calculus: One and Several Variable",
  PUBLISHER = "John Wiley and Sons",
  ADDRESS = "New York",
  YEAR = "1978"
}

@MANUAL{Fear05,
  title =        {Publication quality tables in {\LaTeX}},
  author =       {Simon Fear},
  month =        {April},
  year =         2005,
  note =         {\url{http://www.ctan.org/pkg/booktabs}}
}

@Manual{Amsthm15,
  title =        {Using the amsthm Package},
  organization = {American Mathematical Society},
  month =        {April},
  year =         2015,
  note =         {\url{http://www.ctan.org/pkg/amsthm}}
}

@ArtifactSoftware{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
}

@ArtifactDataset{UMassCitations,
 author    =  {Sam Anzaroot and Andrew McCallum},
 title     =  {{UMass} Citation Field Extraction Dataset},
 year      = 2013,
 url       =
    {http://www.iesl.cs.umass.edu/data/data-umasscitationfield},
 lastaccessed = {May 27, 2019}
}

@Eprint{Bornmann2019,
       author = {Bornmann, Lutz and Wray, K. Brad and Haunschild,
                  Robin},
        title = {Citation concept analysis {(CCA)}---A new form of
                  citation analysis revealing the usefulness of
                  concepts for other researchers illustrated by two
                  exemplary case studies including classic books by
                  {Thomas S.~Kuhn} and {Karl R.~Popper}},
     keywords = {Computer Science - Digital Libraries},
         year = 2019,
        month = "May",
          eid = {arXiv:1905.12410},
archivePrefix = {arXiv},
       eprint = {1905.12410},
 primaryClass = {cs.DL},
}

@Eprint{AnzarootPBM14,
  author    = {Sam Anzaroot and
               Alexandre Passos and
               David Belanger and
               Andrew McCallum},
  title     = {Learning Soft Linear Constraints with Application to
                  Citation Field Extraction},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1403.1349},
}

@inproceedings{Hagerup1993,
title        = {Maintaining Discrete Probability Distributions Optimally},
author       = {Hagerup, Torben and Mehlhorn, Kurt and Munro, J. Ian},
booktitle    = {Proceedings of the 20th International Colloquium on Automata, Languages and Programming},
series       = {Lecture Notes in Computer Science},
volume       = {700},
pages        = {253--264},
year         = {1993},
publisher    = {Springer-Verlag},
address      = {Berlin},
}

@inproceedings{EarTouch,
author = {Kikuchi, Takashi and Sugiura, Yuta and Masai, Katsutoshi and Sugimoto, Maki and Thomas, Bruce H.},
title = {EarTouch: Turning the Ear into an Input Surface},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098538},
doi = {10.1145/3098279.3098538},
abstract = {In this paper, we propose EarTouch, a new sensing technology for ear-based input for
controlling applications by slightly pulling the ear and detecting the deformation
by an enhanced earphone device. It is envisioned that EarTouch will enable control
of applications such as music players, navigation systems, and calendars as an "eyes-free"
interface. As for the operation of EarTouch, the shape deformation of the ear is measured
by optical sensors. Deformation of the skin caused by touching the ear with the fingers
is recognized by attaching optical sensors to the earphone and measuring the distance
from the earphone to the skin inside the ear. EarTouch supports recognition of multiple
gestures by applying a support vector machine (SVM). EarTouch was validated through
a set of user studies.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {27},
numpages = {6},
keywords = {earphone, skin deformation, photo reflective sensor},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{Acceptable,
author = {Lee, DoYoung and Lee, Youryang and Shin, Yonghwan and Oakley, Ian},
title = {Designing Socially Acceptable Hand-to-Face Input},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242642},
doi = {10.1145/3242587.3242642},
abstract = {Wearable head-mounted displays combine rich graphical output with an impoverished
input space. Hand-to-face gestures have been proposed as a way to add input expressivity
while keeping control movements unobtrusive. To better understand how to design such
techniques, we describe an elicitation study conducted in a busy public space in which
pairs of users were asked to generate unobtrusive, socially acceptable hand-to-face
input actions. Based on the results, we describe five design strategies: miniaturizing,
obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies
in two hand-to-face input prototypes, one based on touches to the ear and the other
based on touches of the thumbnail to the chin or cheek. Performance assessments characterize
time and error rates with these devices. The paper closes with a validation study
in which pairs of users experience the prototypes in a public setting and we gather
data on the social acceptability of the designs and reflect on the effectiveness of
the different strategies.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {711â€“723},
numpages = {13},
keywords = {social acceptability, user elicitation, hand-to-face input, head mounted display, augmented reality},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{Nose,
author = {Lee, Juyoung and Yeo, Hui-Shyong and Dhuliawala, Murtaza and Akano, Jedidiah and Shimizu, Junichi and Starner, Thad and Quigley, Aaron and Woo, Woontack and Kunze, Kai},
title = {Itchy Nose: Discreet Gesture Interaction Using EOG Sensors in Smart Eyewear},
year = {2017},
isbn = {9781450351881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123021.3123060},
doi = {10.1145/3123021.3123060},
abstract = {We propose a sensing technique for detecting finger movements on the nose, using EOG
sensors embedded in the frame of a pair of eyeglasses. Eyeglasses wearers can use
their fingers to exert different types of movement on the nose, such as flicking,
pushing or rubbing. These subtle gestures can be used to control a wearable computer
without calling attention to the user in public. We present two user studies where
we test recognition accuracy for these movements.},
booktitle = {Proceedings of the 2017 ACM International Symposium on Wearable Computers},
pages = {94â€“97},
numpages = {4},
keywords = {nose gesture, EOG, subtle interaction, smart eyeglasses, face gesture, wearable computer, smart eyewear},
location = {Maui, Hawaii},
series = {ISWC '17}
}

@inproceedings{cheek,
author = {Yamashita, Koki and Kikuchi, Takashi and Masai, Katsutoshi and Sugimoto, Maki and Thomas, Bruce H. and Sugiura, Yuta},
title = {CheekInput: Turning Your Cheek into an Input Surface by Embedded Optical Sensors on a Head-Mounted Display},
year = {2017},
isbn = {9781450355483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139131.3139146},
doi = {10.1145/3139131.3139146},
abstract = {In this paper, we propose a novel technology called "CheekInput" with a head-mounted
display (HMD) that senses touch gestures by detecting skin deformation. We attached
multiple photo-reflective sensors onto the bottom front frame of the HMD. Since these
sensors measure the distance between the frame and cheeks, our system is able to detect
the deformation of a cheek when the skin surface is touched by fingers. Our system
uses a Support Vector Machine to determine the gestures: pushing face up and down,
left and right. We combined these 4 directional gestures for each cheek to extend
16 possible gestures. To evaluate the accuracy of the gesture detection, we conducted
a user study. The results revealed that CheekInput achieved 80.45 % recognition accuracy
when gestures were made by touching both cheeks with both hands, and 74.58 % when
by touching both cheeks with one hand.},
booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
articleno = {19},
numpages = {8},
keywords = {OST-HMD, skin interface, photo-reflective sensor},
location = {Gothenburg, Sweden},
series = {VRST '17}
}

@article{hand-over-face-classify,
author = {Mahmoud, Marwa and Baltru\v{s}aitis, Tadas and Robinson, Peter},
title = {Automatic Analysis of Naturalistic Hand-Over-Face Gestures},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2946796},
doi = {10.1145/2946796},
abstract = {One of the main factors that limit the accuracy of facial analysis systems is hand
occlusion. As the face becomes occluded, facial features are lost, corrupted, or erroneously
detected. Hand-over-face occlusions are considered not only very common but also very
challenging to handle. However, there is empirical evidence that some of these hand-over-face
gestures serve as cues for recognition of cognitive mental states. In this article,
we present an analysis of automatic detection and classification of hand-over-face
gestures. We detect hand-over-face occlusions and classify hand-over-face gesture
descriptors in videos of natural expressions using multi-modal fusion of different
state-of-the-art spatial and spatio-temporal features. We show experimentally that
we can successfully detect face occlusions with an accuracy of 83%. We also demonstrate
that we can classify gesture descriptors (hand shape, hand action, and facial region
occluded) significantly better than a na\"{\i}ve baseline. Our detailed quantitative analysis
sheds some light on the challenges of automatic classification of hand-over-face gestures
in natural expressions.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {19},
numpages = {18},
keywords = {Hand-over-face occlusions, face touches, facial landmarks, hand gestures, space-time interest points, histograms of oriented gradient}
}

@inbook{Katsamanis2017,
author = {Katsamanis, Athanasios and Pitsikalis, Vassilis and Theodorakis, Stavros and Maragos, Petros},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1},
isbn = {9781970001679},
pages = {449--487},
publisher = {Association for Computing Machinery and Morgan {\&} Claypool},
title = {{Multimodal Gesture Recognition}},
url = {https://doi.org/10.1145/3015783.3015796},
year = {2017}
}

@article{Bolt1980,
address = {New York, NY, USA},
author = {Bolt, Richard A},
doi = {10.1145/965105.807503},
issn = {0097-8930},
journal = {SIGGRAPH Comput. Graph.},
keywords = { Gesture, Graphics, Graphics interface, Man-machine interfaces, Space sensing, Spatial data management, Voice input,Speech input},
number = {3},
pages = {262--270},
publisher = {Association for Computing Machinery},
title = {{â€œPut-That-Thereâ€: Voice and Gesture at the Graphics Interface}},
url = {https://doi.org/10.1145/965105.807503},
volume = {14},
year = {1980}
}

@inproceedings{Bourguet1998,
address = {New York, NY, USA},
author = {Bourguet, Marie-Luce and Ando, Akio},
booktitle = {CHI 98 Conference Summary on Human Factors in Computing Systems},
doi = {10.1145/286498.286726},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Synchronization of speech and hand gestures during multimodal human-computer interaction.pdf:pdf},
isbn = {1581130287},
keywords = {hand gestures,multimodal interaction,predictive model,speech recognition,synchrony},
pages = {241--242},
publisher = {Association for Computing Machinery},
series = {CHI '98},
title = {{Synchronization of Speech and Hand Gestures during Multimodal Human-Computer Interaction}},
url = {https://doi.org/10.1145/286498.286726},
year = {1998}
}

@inproceedings{VoGe2017,
author = {Sauras-Perez, Pablo and Gil, Andrea and {Singh Gill}, Jasprit and Pisu, Pierluigi and Taiber, Joachim},
booktitle = {WCXâ„¢ 17: SAE World Congress Experience},
doi = {https://doi.org/10.4271/2017-01-0068},
issn = {0148-7191},
publisher = {SAE International},
title = {{VoGe: A Voice and Gesture System for Interacting with Autonomous Cars}},
url = {https://doi.org/10.4271/2017-01-0068},
year = {2017}
}
@article{Williams2020,
author = {Williams, A S and Garcia, J and Ortega, F},
doi = {10.1109/TVCG.2020.3023566},
issn = {1941-0506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Two dimensional displays;Augmented reality;Proposals;Task analysis;Timing;Resists;Speech recognition},
pages = {1},
title = {{Understanding Multimodal User Gesture and Speech Behavior for Object Manipulation in Augmented Reality Using Elicitation}},
year = {2020}
}

@article{Wang2019,
author = {Wang, Z and Hou, Y and Jiang, K and Dou, W and Zhang, C and Huang, Z and Guo, Y},
doi = {10.1109/ACCESS.2019.2933987},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {feature extraction;gesture recognition;ubiquitous computing;hand tracking;ultrasonic hand gesture recognition system;ultrasonic signal;smartphone;active sonar sensing system;hand movements;state-of-the-art hand gesture applications;dynamic gesture recognition;Acoustics;Sensors;Gesture recognition;Microphones;Feature extraction;Sonar;Tracking;Doppler effect;hand gesture recognition;smartphone;ultrasonic signal},
pages = {111897--111922},
title = {{Hand Gesture Recognition Based on Active Ultrasonic Sensing of Smartphone: A Survey}},
volume = {7},
year = {2019}
}

@inproceedings{gupta2012soundwave,
author = {Gupta, Sidhant and Morris, Dan and Patel, Shwetak N. and Tan, Desney},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2207676.2208331},
isbn = {9781450310154},
keywords = {Doppler,In-air gesture sensing,Interaction technique},
pages = {1911--1914},
title = {{SoundWave: Using the Doppler effect to sense gestures}},
year = {2012}
}

@article{Jongseo1999,
author = {{Jongseo Sohn} and {Nam Soo Kim} and {Wonyong Sung}},
doi = {10.1109/97.736233},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/A Statistical Model-Based Voice Activity Detection.pdf:pdf},
issn = {1558-2361},
journal = {IEEE Signal Processing Letters},
keywords = {Discrete Fourier transforms,Frequency estimation,HMM,Hidden Markov models,Light rail systems,Maximum likelihood estimation,Parameter estimation,SNR,Signal to noise ratio,Speech enhancement,Statistics,Testing,VAD,decision-directed parameter estimation method,first-order Markov process modeling,hang-over scheme,hidden Markov models,likelihood ratio test,low signal-to-noise ratio,parameter estimation,robust voice activity detector,simulation results,speech coding,speech occurrences,statistical model-based,variable rate codes,variable-rate speech coding,vehicular noise environment,voice activity detection},
number = {1},
pages = {1--3},
title = {{A statistical model-based voice activity detection}},
volume = {6},
year = {1999}
}
@article{Ramirez2004,
author = {Ram{\'{i}}rez, Javier and Segura, Jos{\'{e}} C. and Ben{\'{i}}tez, Carmen and {De la Torre}, {\'{A}}ngel and Rubio, Antonio},
doi = {10.1016/j.specom.2003.10.002},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Efficient voice activity detection algorithmsusing long-term speech information.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Long-term spectral divergence,Long-term spectral envelope,Speech enhancement,Speech recognition,Speech/non-speech detection},
number = {3-4},
pages = {271--287},
title = {{Efficient voice activity detection algorithms using long-term speech information}},
volume = {42},
year = {2004}
}

@article{Ashbrook2016,
doi = {10.1145/2935334.2935389},
file = {:C$\backslash$:/Users/Osea/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashbrook et al. - 2016 - Bitey An exploration of tooth click gestures for hands-free user interface control.pdf:pdf},
isbn = {9781450344081},
journal = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2016},
keywords = {Audio interfaces,Bio-acoustics,Gestures,Subtle interfaces,Tooth input,Wearable computing},
pages = {158--169},
title = {{Bitey: An exploration of tooth click gestures for hands-free user interface control}},
year = {2016}
}

@inproceedings{Vega2019,
address = {New York, NY, USA},
author = {{Vega G{\'{a}}lvez}, Tom{\'{a}}s and Sapkota, Shardul and Dancu, Alexandru and Maes, Pattie},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3290607.3312925},
file = {:C$\backslash$:/Users/Osea/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maes - 2019 - Byte . it Discreet Teeth Gestures for Mobile Device Interaction(2).pdf:pdf},
isbn = {9781450359719},
keywords = {discreet interfaces,hands-free interaction,microgestures,teeth gestures},
pages = {1--6},
publisher = {Association for Computing Machinery},
series = {CHI EA '19},
title = {{Byte.It: Discreet Teeth Gestures for Mobile Device Interaction}},
url = {https://doi.org/10.1145/3290607.3312925},
year = {2019}
}

@article{Foggia2015,
author = {Foggia, Pasquale and Petkov, Nicolai and Saggese, Alessia and Strisciuglio, Nicola and Vento, Mario},
doi = {10.1016/j.patrec.2015.06.026},
file = {:C$\backslash$:/Users/Osea/Downloads/1-s2.0-S0167865515001981-main.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Audio surveillance,Bag of words,Event detection},
pages = {22--28},
publisher = {Elsevier Ltd.},
title = {{Reliable detection of audio events in highly noisy environments}},
url = {http://dx.doi.org/10.1016/j.patrec.2015.06.026},
volume = {65},
year = {2015}
}
@article{Mesaros2010,
author = {Mesaros, Annamaria and Heittola, Toni and Eronen, Antti and Virtanen, Tuomas},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/ACOUSTIC EVENT DETECTION IN REAL LIFE RECORDINGS.pdf:pdf},
issn = {22195491},
journal = {European Signal Processing Conference},
pages = {1267--1271},
publisher = {IEEE},
title = {{Acoustic event detection in real life recordings}},
year = {2010}
}

@article{Ono2013,
author = {Ono, Makoto and Shizuki, Buntarou and Tanaka, Jiro},
doi = {10.1145/2501988.2501989},
file = {:C$\backslash$:/Users/Osea/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ono, Shizuki, Tanaka - 2013 - Touch {\&} activate adding interactivity to existing objects using active acoustic sensing.pdf:pdf},
isbn = {9781450322683},
journal = {Proceedings of the 26th annual ACM symposium on User interface software and technology},
pages = {31--40},
title = {{Touch {\&} activate: adding interactivity to existing objects using active acoustic sensing}},
url = {http://dl.acm.org/citation.cfm?id=2501989},
year = {2013}
}

@inproceedings{LissermannCHI13,
address = {New York, NY, USA},
author = {Lissermann, Roman and Huber, Jochen and Hadjakos, Aristotelis and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
doi = {10.1145/2468356.2468592},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/EarPut.pdf:pdf},
isbn = {9781450319522},
keywords = { device augmentation, ear-based interaction, experiment, eyes-free, mobile interaction, touch,multi-touch},
pages = {1323--1328},
publisher = {Association for Computing Machinery},
series = {CHI EA '13},
title = {{EarPut: Augmenting behind-the-Ear Devices for Ear-Based Interaction}},
url = {https://doi.org/10.1145/2468356.2468592},
year = {2013}
}

@inproceedings{Kikuchi17,
address = {New York, NY, USA},
author = {Kikuchi, Takashi and Sugiura, Yuta and Masai, Katsutoshi and Sugimoto, Maki and Thomas, Bruce H},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
doi = {10.1145/3098279.3098538},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/EarTouch - Turning the Ear into an Input Surface.pdf:pdf},
isbn = {9781450350754},
keywords = { earphone, photo reflective sensor,skin deformation},
publisher = {Association for Computing Machinery},
series = {MobileHCI '17},
title = {{EarTouch: Turning the Ear into an Input Surface}},
url = {https://doi.org/10.1145/3098279.3098538},
year = {2017}
}

@inproceedings{Ku-CHI2020,
author = {Ku, Pin-Sung and Shao, Qijia and Wu, Te-Yen and Gong, Jun and Zhu, Ziyan and Zhou, Xia and Yang, Xing-Dong},
title = {ThreadSense: Locating Touch on an Extremely Thin Interactive Thread},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376779},
doi = {10.1145/3313831.3376779},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1â€“12},
numpages = {12},
keywords = {impedance sensing, thread, fabric, touch input},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Sauras2017,
author = {Sauras-Perez, Pablo and Gil, Andrea and {Singh Gill}, Jasprit and Pisu, Pierluigi and Taiber, Joachim},
booktitle = {WCXâ„¢ 17: SAE World Congress Experience},
doi = {https://doi.org/10.4271/2017-01-0068},
issn = {0148-7191},
publisher = {SAE International},
title = {{VoGe: A Voice and Gesture System for Interacting with Autonomous Cars}},
url = {https://doi.org/10.4271/2017-01-0068},
year = {2017}
}


@article{Yun2017,
author = {Yun, Sangki and Chen, Yi Chao and Zheng, Huihunag and Qiu, Lili and Mao, Wenguang},
doi = {10.1145/3081333.3081356},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Strata - Fine-Grained Acoustic-based Device-Free Tracking.pdf:pdf},
isbn = {9781450349284},
journal = {MobiSys 2017 - Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
keywords = {Acoustic tracking,Channel impulse response,Gesture recognition},
pages = {15--28},
title = {{Strata: Fine-grained acoustic-based device-free tracking}},
year = {2017}
}

@article{Liu2019,
author = {Liu, W and Shen, W and Li, B and Wang, L},
doi = {10.1109/ACCESS.2018.2886279},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Toward Device-Free Micro-Gesture Tracking via.pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Acoustics;Tracking;Bandwidth;Microphones;Smart phones;RF signals;Device free;finger gesture tracking;Fourier fitting;Doppler-shift},
pages = {1084--1094},
title = {{Toward Device-Free Micro-Gesture Tracking via Accurate Acoustic Doppler-Shift Detection}},
volume = {7},
year = {2019}
}


@article{Gao2020,
address = {New York, NY, USA},
author = {Gao, Yang and Jin, Yincheng and Li, Jiyang and Choi, Seokmin and Jin, Zhanpeng},
doi = {10.1145/3411830},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
keywords = { Acoustic, echo, silent speech,smartphone},
number = {3},
publisher = {Association for Computing Machinery},
title = {{EchoWhisper: Exploring an Acoustic-Based Silent Speech Interface for Smartphone Users}},
url = {https://doi.org/10.1145/3411830},
volume = {4},
year = {2020}
}


@article{Zhang2020,
author = {Zhang, Yongzhao and Huang, Wei Hsiang and Yang, Chih Yun and Wang, Wen Ping and Chen, Yi Chao and You, Chuang Wen and Huang, Da Yuan and Xue, Guangtao and Yu, Jiadi},
doi = {10.1145/3381008},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Zhang et al. - 2020 - Endophasia Utilizing Acoustic-Based Imaging for I.pdf:pdf},
issn = {24749567},
journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
keywords = {Acoustic-based imaging,Mobile devices,Silent command},
number = {1},
title = {{Endophasia: Utilizing acoustic-based imaging for issuing contact-free silent speech commands}},
volume = {4},
year = {2020}
}

@article{Wu2020,
author = {Wu, Jason and Harrison, Chris and Bigham, Jeffrey P. and Laput, Gierad},
doi = {10.1145/3313831.3376875},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Automatic Class Discovery and One-Shot.pdf:pdf},
isbn = {9781450367080},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {acoustic activity recognition,automatic class discovery},
title = {{Automated Class Discovery and One-Shot Interactions for Acoustic Activity Recognition}},
year = {2020}
}

@article{Laput2018,
author = {Laput, Gierad and Ahuja, Karan and Goel, Mayank and Harrison, Chris},
doi = {10.1145/3242587.3242609},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Ubicoustics - Plug-and-Play Acoustic Activity Recognition.pdf:pdf},
isbn = {9781450359481},
journal = {UIST 2018 - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
keywords = {IoT,Smart environments,Ubiquitous sensing},
pages = {213--224},
title = {{Ubicoustics: Plug-and-play acoustic activity recognition}},
year = {2018}
}

@inproceedings{Laput2015,
address = {New York, NY, USA},
author = {Laput, Gierad and Brockmeyer, Eric and Hudson, Scott E and Harrison, Chris},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
doi = {10.1145/2702123.2702414},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Acoustruments - Passive, Acoustically-Driven, Interactive.pdf:pdf},
isbn = {9781450331456},
keywords = {acoustic sensing,fabrication,mechanisms and controls,mobile and handheld devices},
pages = {2161--2170},
publisher = {Association for Computing Machinery},
series = {CHI '15},
title = {{Acoustruments: Passive, Acoustically-Driven, Interactive Controls for Handheld Devices}},
url = {https://doi.org/10.1145/2702123.2702414},
year = {2015}
}

@article{Harrison2012,
abstract = {We present acoustic barcodes, structured patterns of physical notches that, when swiped with e.g., a fingernail, produce a complex sound that can be resolved to a binary ID. A single, inexpensive contact microphone attached to a surface or object is used to capture the waveform. We present our method for decoding sounds into IDs, which handles variations in swipe velocity and other factors. Acoustic barcodes could be used for information retrieval or to triggering interactive functions. They are passive, durable and inexpensive to produce. Further, they can be applied to a wide range of materials and objects, including plastic, wood, glass and stone. We conclude with several example applications that highlight the utility of our approach, and a user study that explores its feasibility. Copyright 2012 ACM.},
author = {Harrison, Chris and Xiao, Robert and Hudson, Scott E.},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/Acoustic Barcodes.pdf:pdf},
isbn = {9781450315807},
journal = {UIST'12 - Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
keywords = {Classification,ID,Identification,Interaction techniques,Location,Markers,Microphones,Sound,Tags,Ubiquitous and pervasive computing,Vibration},
pages = {563--567},
title = {{Acoustic barcodes: Passive, durable and inexpensive notched identification tags}},
year = {2012}
}

@inproceedings{Harrison2011,
address = {New York, NY, USA},
author = {Harrison, Chris and Schwarz, Julia and Hudson, Scott E},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2047196.2047279},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/tapsense.pdf:pdf},
isbn = {9781450307161},
keywords = {acoustic classification,collaborative,finger,input,interactive surfaces,multi-user,pens,stylus,tabletop computing,tangibles,tools,touchscreen},
pages = {627--636},
publisher = {Association for Computing Machinery},
series = {UIST '11},
title = {{TapSense: Enhancing Finger Interaction on Touch Surfaces}},
url = {https://doi.org/10.1145/2047196.2047279},
year = {2011}
}

@inproceedings{Xiao-2014-Toffee,
address = {New York, NY, USA},
author = {Xiao, Robert and Lew, Greg and Marsanico, James and Hariharan, Divya and Hudson, Scott and Harrison, Chris},
booktitle = {Proceedings of the 16th International Conference on Human-computer Interaction with Mobile Devices {\&}{\#}38; Services},
doi = {10.1145/2628363.2628383},
file = {:C$\backslash$:/Users/Osea/Desktop/hand-to-mouth/ref/ToffeeCMU.pdf:pdf},
isbn = {978-1-4503-3004-6},
keywords = {adi,around device interaction,everyday surfaces,interfaces everywhere,tdoa,time difference of arrival,time of flight correlation,toa,tof,touch interaction,vibro-acoustic sensors},
pages = {67--76},
publisher = {ACM},
series = {MobileHCI '14},
title = {{Toffee: Enabling Ad Hoc, Around-device Interaction with Acoustic Time-of-arrival Correlation}},
url = {http://doi.acm.org/10.1145/2628363.2628383},
year = {2014}
}

@article{Ando2017,
author = {Ando, Toshiyuki and Kubo, Yuki and Shizuki, Buntarou and Takahashi, Shin},
doi = {10.1145/3126594.3126649},
file = {:C$\backslash$:/Users/Osea/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ando et al. - 2017 - CanalSense Face-related movement recognition system based on sensing air pressure in ear canals.pdf:pdf},
isbn = {9781450349819},
journal = {UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
keywords = {Barometer,Earphones,Eyes-free,Facial movement,Hands-free,Head movement,Jaw movement,Mouth movement,Outer ear interface,Wearable computing},
pages = {679--689},
title = {{CanalSense: Face-related movement recognition system based on sensing air pressure in ear canals}},
year = {2017}
}

@inproceedings{Mo-Bi2016,
author = {Kim, Han-Jong and Cha, Seijin and Park, Richard C. and Nam, Tek-Jin and Lee, Woohun and Lee, Geehyuk},
title = {Mo-Bi: Contextual Mobile Interfaces through Bimanual Posture Sensing with Wrist-Worn Devices},
year = {2016},
isbn = {9788968487910},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
url = {https://doi.org/10.17210/hcik.2016.01.94},
doi = {10.17210/hcik.2016.01.94},
abstract = {Using multiple sensor-embedded devices enables detection of usage contexts that extend
new interaction opportunities (e.g., a gesture and posture composite interface). In
this study, we explored a design space of contextual interactions using a mobile device
and two wrist-worn devices, suggesting Mo-Bi, the bimanual posture-based mobile interface
system. Mo-Bi recognizes a user's hand posture compositely utilizing the spatial configurations
of the three devices. It provides the contextually appropriate interface layout or
application function according to the inference based on bimanual posture information.
We suggested four interface scenarios of Mo-Bi that are applicable for common use
applications in mobile context. A technical evaluation shows the feasibility of the
system, and users found that Mo-Bi enhanced physical comfort and stability, providing
interfaces that fit their context without increasing interaction steps for the adaptive
interface.},
booktitle = {Proceedings of HCI Korea},
pages = {94â€“99},
numpages = {6},
keywords = {wrist-worn device, Hand posture, wearable device, context inference, adaptive interface},
location = {Jeongseon, Republic of Korea},
series = {HCIK '16}
}

@inproceedings{yuntao-faceori,
author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun},
title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517698},
doi = {10.1145/3491102.3517698},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {290},
numpages = {12},
keywords = {Acoustic ranging, earphone, head orientation, head pose estimation.},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{Yuzhou-reflectrack,
author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun},
title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474805},
doi = {10.1145/3472749.3474805},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1050â€“1062},
numpages = {13},
keywords = {Acoustic tracking, FMCW, smartphones, sound reflection},
location = {Virtual Event, USA},
series = {UIST '21}
}

@INPROCEEDINGS{Ken-sleep,
  author={Christofferson, Kenneth and Chen, Xuyang and Wang, Zeyu and Mariakakis, Alex and Wang, Yuntao},
  booktitle={2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, 
  title={Sleep Sound Classification Using ANC-Enabled Earbuds}, 
  year={2022},
  volume={},
  number={},
  pages={397-402},
  doi={10.1109/PerComWorkshops53856.2022.9767394}}
  
@article{wang-hearcough,
title = {HearCough: Enabling continuous cough event detection on edge computing hearables},
journal = {Methods},
volume = {205},
pages = {53-62},
year = {2022},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1046202322001165},
author = {Yuntao Wang and Xiyuxing Zhang and Jay M. Chakalasiya and Xuhai Xu and Yu Jiang and Yuang Li and Shwetak Patel and Yuanchun Shi},
keywords = {Continuous cough monitoring, Active noise cancellation hearables, Health monitoring, Edge computing, Deep learning, On-chip Machine Learning}}
}

@inbook{SunkeFloat2017,
author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun},
title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026027},
abstract = {Touch interaction on smartwatches suffers from the awkwardness of having to use two
hands and the "fat finger" problem. We present Float, a wrist-to-finger input approach
that enables one-handed and touch-free target selection on smartwatches with high
efficiency and precision using only commercially-available built-in sensors. With
Float, a user tilts the wrist to point and performs an in-air finger tap to click.
To realize Float, we first explore the appropriate motion space for wrist tilt and
determine the clicking action (finger tap) through a user-elicitation study. We combine
the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger
taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that
using just one hand, Float allows users to acquire targets with size ranging from
2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct
touch in both stationary (&gt;98.9%) and walking (&gt;71.5%) contexts.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {692â€“704},
numpages = {13}
}

@inbook{Serendipity,
author = {Wen, Hongyi and Ramos Rojas, Julian and Dey, Anind K.},
title = {Serendipity: Finger Gesture Recognition Using an Off-the-Shelf Smartwatch},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858466},
abstract = {Previous work on muscle activity sensing has leveraged specialized sensors such as
electromyography and force sensitive resistors. While these sensors show great potential
for detecting finger/hand gestures, they require additional hardware that adds to
the cost and user discomfort. Past research has utilized sensors on commercial devices,
focusing on recognizing gross hand gestures. In this work we present Serendipity,
a new technique for recognizing unremarkable and fine-motor finger gestures using
integrated motion sensors (accelerometer and gyroscope) in off-the-shelf smartwatches.
Our system demonstrates the potential to distinguish 5 fine-motor gestures like pinching,
tapping and rubbing fingers with an average f1-score of 87%. Our work is the first
to explore the feasibility of using solely motion sensors on everyday wearable devices
to detect fine-grained gestures. This promising technology can be deployed today on
current smartwatches and has the potential to be applied to cross-device interactions,
or as a tool for research in fields involving finger and hand motion.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3847â€“3851},
numpages = {5}
}

@inproceedings{3D-dataset,
author = {Sudharsan, Bharath and Sundaram, Dineshkumar and Breslin, John G. and Ali, Muhammad Intizar},
title = {Avoid Touching Your Face: A Hand-to-Face 3D Motion Dataset (COVID-Away) and Trained Models for Smartwatches},
year = {2020},
isbn = {9781450388207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423423.3423433},
doi = {10.1145/3423423.3423433},
abstract = {World Health Organisation (WHO) advises that humans must try to avoid touching their
eye, nose and mouth, which is an effective way to stop the spread of viral diseases.
This has become even more prominent with the widespread coronavirus (COVID-19), resulting
in a global pandemic. However, we humans on average touch our face (eye, nose and
mouth) 10-20 times an hour [22] [12], which is often the primary source [15] of getting
infected by a variety of viral infections including seasonal Influenza, Coronavirus,
Swine flu, Ebola virus, etc. Touching our face all day long is a quirk of human nature
[13] and it is extremely difficult to train people to avoid touching their face. However,
wearable devices and technology can help to continuously monitor our movements and
trigger a timely event reminding people to avoid touching their face. In this work,
we have collected a hand-to-face multi-sensor 3D motion dataset and named it COVID-away
dataset. Using our dataset, we trained models that can continuously monitor human
arm/hand movement using a wearable device and trigger a timely notification (e.g.
vibration) to warn the device users when their hands are moved (unintentionally) towards
their face. Our trained COVID-away models can be easily integrated into an app for
smartwatches or fitness bands. Our evaluation shows that the Minimum Covariance Determinant
(MCD) model produces the highest F1-score (0.93) using just the smartwatchâ€™s accelerometer
data (39 features). Both the dataset and trained models are openly available on the
Web at https://github.com/bharathsudharsan/COVID-away.},
booktitle = {10th International Conference on the Internet of Things Companion},
articleno = {7},
numpages = {9},
keywords = {3D Motion Dataset, CNN Edge Deployment., COVID-away Models, Edge Analytics, Intelligent Smartwatches, Post-training Quantization},
location = {Malm\"{o}, Sweden},
series = {IoT '20 Companion}
}

@article{Gu-IMU-RING-TYPING,
author = {Gu, Yizheng and Yu, Chun and Li, Zhipeng and Li, Zhaoheng and Wei, Xiaoying and Shi, Yuanchun},
title = {QwertyRing: Text Entry on Physical Surfaces Using a Ring},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432204},
doi = {10.1145/3432204},
abstract = {The software keyboard is widely used on digital devices such as smartphones, computers,
and tablets. The software keyboard operates via touch, which is efficient, convenient,
and familiar to users. However, some emerging technology devices such as AR/VR headsets
and smart TVs do not support touch-based text entry. In this paper, we present QwertyRing,
a technique that supports text entry on physical surfaces using an IMU (Inertial Measurement
Unit) ring. Users wear the ring on the middle phalanx of the index finger and type
on any desk-like surface, as if there is a QWERTY keyboard on the surface. While typing,
users do not focus on monitoring the hand motions. They receive text feedback on a
separate screen, e.g., an AR/VR headset or a digital device display, such as a computer
monitor. The basic idea of QwertyRing is to detect touch events and predict users'
desired words by the orientation of the IMU ring. We evaluate the performance of QwertyRing
through a five-day user study. Participants achieved a speed of 13.74 WPM in the first
40 minutes and reached 20.59 WPM at the end. The speed outperforms other ring-based
techniques [24, 30, 45, 68] and is 86.48% of the speed of typing on a smartphone with
an index finger. The results show that QwertyRing enables efficient touch-based text
entry on physical surfaces.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {128},
numpages = {29},
keywords = {smart ring, touch input, text entry}
}

@inbook{7-arm,
author = {Buschek, Daniel and Roppelt, Bianka and Alt, Florian},
title = {Extending Keyboard Shortcuts with Arm and Wrist Rotation Gestures},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173595},
abstract = {We propose and evaluate a novel interaction technique to enhance physical keyboard
shortcuts with arm and wrist rotation gestures, performed during keypresses: rolling
the wrist, rotating the arm/wrist, and lifting it. This extends the set of shortcuts
from key combinations (e.g. ctrl + v) to combinations of key(s) and gesture (e.g.
v + roll left) and enables continuous control. We implement this approach for isolated
single keypresses, using inertial sensors of a smartwatch. We investigate key aspects
in three studies: 1) rotation flexibility per keystroke finger, 2) rotation control,
and 3) user-defined gesture shortcuts. As a use case, we employ our technique in a
painting application and assess user experience. Overall, results show that arm and
wrist rotations during keystrokes can be used for interaction, yet challenges remain
for integration into practical applications. We discuss recommendations for applications
and ideas for future research.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1â€“12},
numpages = {12}
}

@inproceedings{31-ViBand,
author = {Laput, Gierad and Xiao, Robert and Harrison, Chris},
title = {ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers},
year = {2016},
isbn = {9781450341899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984511.2984582},
doi = {10.1145/2984511.2984582},
abstract = {Smartwatches and wearables are unique in that they reside on the body, presenting
great potential for always-available input and interaction. Their position on the
wrist makes them ideal for capturing bio-acoustic signals. We developed a custom smartwatch
kernel that boosts the sampling rate of a smartwatch's existing accelerometer to 4
kHz. Using this new source of high-fidelity data, we uncovered a wide range of applications.
For example, we can use bio-acoustic data to classify hand gestures such as flicks,
claps, scratches, and taps, which combine with on-device motion tracking to create
a wide range of expressive input modalities. Bio-acoustic sensing can also detect
the vibrations of grasped mechanical or motor-powered objects, enabling passive object
recognition that can augment everyday experiences with context-aware functionality.
Finally, we can generate structured vibrations using a transducer, and show that data
can be transmitted through the human body. Overall, our contributions unlock user
interface techniques that previously relied on special-purpose and/or cumbersome instrumentation,
making such interactions considerably more feasible for inclusion in future consumer
devices.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {321â€“333},
numpages = {13},
keywords = {object detection, gestures, vibro-tags, wearables},
location = {Tokyo, Japan},
series = {UIST '16}
}

@article{59-Smartwatch-Based,
author = {Vu, Tran Huy and Misra, Archan and Roy, Quentin and Wei, Kenny Choo Tsu and Lee, Youngki},
title = {Smartwatch-Based Early Gesture Detection 8 Trajectory Tracking for Interactive Gesture-Driven Applications},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191771},
doi = {10.1145/3191771},
abstract = {The paper explores the possibility of using wrist-worn devices (specifically, a smartwatch)
to accurately track the hand movement and gestures for a new class of immersive, interactive
gesture-driven applications. These interactive applications need two special features:
(a) the ability to identify gestures from a continuous stream of sensor data early--i.e.,
even before the gesture is complete, and (b) the ability to precisely track the hand's
trajectory, even though the underlying inertial sensor data is noisy. We develop a
new approach that tackles these requirements by first building a HMM-based gesture
recognition framework that does not need an explicit segmentation step, and then using
a per-gesture trajectory tracking solution that tracks the hand movement only during
these predefined gestures. Using an elaborate setup that allows us to realistically
study the table-tennis related hand movements of users, we show that our approach
works: (a) it can achieve 95% stroke recognition accuracy. Within 50% of gesture,
it can achieve a recall value of 92% for 10 novice users and 93% for 15 experienced
users from a continuous sensor stream; (b) it can track hand movement during such
strokeplay with a median accuracy of 6.2 cm.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {39},
numpages = {27},
keywords = {wearable devices, low-latency, gesture recognition, VR, immersive applications, hand tracking}
}

@inproceedings{62-gesture-with-acc,
author = {Ward, Jamie A. and Lukowicz, Paul and Tr\"{o}ster, Gerhard},
title = {Gesture Spotting Using Wrist Worn Microphone and 3-Axis Accelerometer},
year = {2005},
isbn = {1595933042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1107548.1107578},
doi = {10.1145/1107548.1107578},
abstract = {We perform continuous activity recognition using only two wrist-worn sensors - a 3-axis
accelerometer and a microphone. We build on the intuitive notion that two very different
sensors are unlikely to agree in classification of a false activity. By comparing
imperfect, jumping window classifications from each of these sensors, we are able
discern activities of interest from null or uninteresting activities. Where one sensor
alone is unable to perform such partitioning, using comparison we are able to report
good overall system performance of up to 70% accuracy. In presenting these results,
we attempt to give a more-in depth visualization of the errors than can be gathered
from confusion matrices alone.},
booktitle = {Proceedings of the 2005 Joint Conference on Smart Objects and Ambient Intelligence: Innovative Context-Aware Services: Usages and Technologies},
pages = {99â€“104},
numpages = {6},
location = {Grenoble, France},
series = {sOc-EUSAI '05}
}

@article{Lu-handtohand,
author = {Lu, Yiqin and Huang, Bingjian and Yu, Chun and Liu, Guahong and Shi, Yuanchun},
title = {Designing and Evaluating Hand-to-Hand Gestures with Dual Commodity Wrist-Worn Devices},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3380984},
doi = {10.1145/3380984},
abstract = {Hand gestures provide a natural and easy-to-use way to input commands. However, few
works have studied the design space of bimanual hand gestures or attempted to infer
gestures that involve devices on both hands. We explore the design space of hand-to-hand
gestures, a group of gestures that are performed by touching one hand with the other
hand. Hand-to-hand gestures are easy to perform and provide haptic feedback on both
hands. Moreover, hand-to-hand gestures generate simultaneous vibration on two hands
that can be sensed by dual off-the-shelf wrist-worn devices. In this work, we derive
a hand-to-hand gesture vocabulary with subjective ratings from users and select gesture
sets for real-life scenarios. We also take advantage of devices on both wrists to
demonstrate their gesture-sensing capability. Our results show that the recognition
accuracy for fourteen gestures is 94.6% when the user is stationary, and the accuracy
for five gestures is 98.4% or 96.3% when the user is walking or running, respectively.
This is significantly more accurate than a single device worn on either wrist. Our
further evaluation also validates that users can easily remember hand-to-hand gestures
and use our technique to invoke commands in real-life contexts.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {20},
numpages = {27},
keywords = {hand gesture, locomotion, wearable device, motion correlation}
}

@inbook{17-interface,
author = {Gustafson, Sean and Bierwirth, Daniel and Baudisch, Patrick},
title = {Imaginary Interfaces: Spatial Interaction with Empty Hands and without Visual Feedback},
year = {2010},
isbn = {9781450302715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866029.1866033},
abstract = {Screen-less wearable devices allow for the smallest form factor and thus the maximum
mobility. However, current screen-less devices only support buttons and gestures.
Pointing is not supported because users have nothing to point at. However, we challenge
the notion that spatial interaction requires a screen and propose a method for bringing
spatial interaction to screen-less devices.We present Imaginary Interfaces, screen-less
devices that allow users to perform spatial interaction with empty hands and without
visual feedback. Unlike projection-based solutions, such as Sixth Sense, all visual
"feedback" takes place in the user's imagination. Users define the origin of an imaginary
space by forming an L-shaped coordinate cross with their non-dominant hand. Users
then point and draw with their dominant hand in the resulting space.With three user
studies we investigate the question: To what extent can users interact spatially with
a user interface that exists only in their imagination? Participants created simple
drawings, annotated existing drawings, and pointed at locations described in imaginary
space. Our findings suggest that users' visual short-term memory can, in part, replace
the feedback conventionally displayed on a screen.},
booktitle = {Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {3â€“12},
numpages = {10}
}

@INPROCEEDINGS{19-fish-eye,  author={Kashiwagi, Naoaki and Sugiura, Yuta and Miyata, Natsuki and Tada, Mitsunori and Sugimoto, Maki and Saito, Hideo},  booktitle={2017 IEEE Winter Applications of Computer Vision Workshops (WACVW)},   title={Measuring Grasp Posture Using an Embedded Camera},   year={2017},  volume={},  number={},  pages={42-47},  doi={10.1109/WACVW.2017.14}}

@InProceedings{30-Generate-hands-tracking,
author = {Mueller, Franziska and Bernard, Florian and Sotnychenko, Oleksandr and Mehta, Dushyant and Sridhar, Srinath and Casas, Dan and Theobalt, Christian},
title = {GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@inbook{55-handsee,
author = {Yu, Chun and Wei, Xiaoying and Vachher, Shubh and Qin, Yue and Liang, Chen and Weng, Yueting and Gu, Yizheng and Shi, Yuanchun},
title = {HandSee: Enabling Full Hand Interaction on Smartphone with Front Camera-Based Stereo Vision},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300935},
abstract = {We present HandSee, a novel sensing technique that can capture the state and movement
of the user's hands touching or gripping a smartphone. We place a right angle prism
mirror on the front camera to achieve a stereo vision of the scene above the touchscreen
surface. We develop a pipeline to extract the depth image of hands from a monocular
RGB image, which consists of three components: a stereo matching algorithm to estimate
the pixel-wise depth of the scene, a CNN-based online calibration algorithm to detect
hand skin, and a merging algorithm that outputs the depth image of the hands. Building
on the output, a substantial set of valuable interaction information, such as fingers'
3D location, gripping posture, and finger identity can be recognized concurrently.
Due to this unique sensing ability, HandSee enables a variety of novel interaction
techniques and expands the design space for full hand interaction on smartphones.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1â€“13},
numpages = {13}
}

@inproceedings{3-IR-touch-on-phone,
author = {Butler, Alex and Izadi, Shahram and Hodges, Steve},
title = {SideSight: Multi-"touch" Interaction around Small Devices},
year = {2008},
isbn = {9781595939753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449715.1449746},
doi = {10.1145/1449715.1449746},
abstract = {Interacting with mobile devices using touch can lead to fingers occluding valuable
screen real estate. For the smallest devices, the idea of using a touch-enabled display
is almost wholly impractical. In this paper we investigate sensing user touch around
small screens like these. We describe a prototype device with infra-red (IR) proximity
sensors embedded along each side and capable of detecting the presence and position
of fingers in the adjacent regions. When this device is rested on a flat surface,
such as a table or desk, the user can carry out single and multi-touch gestures using
the space around the device. This gives a larger input space than would otherwise
be possible which may be used in conjunction with or instead of on-display touch input.
Following a detailed description of our prototype, we discuss some of the interactions
it affords.},
booktitle = {Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology},
pages = {201â€“204},
numpages = {4},
keywords = {multi-touch, proximity sensing, mobile device interaction, novel hardware},
location = {Monterey, CA, USA},
series = {UIST '08}
}

@inproceedings{46-facesight,
  title={FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision},
  author={Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021}
}

@inproceedings{1-depth,
author = {Gustafson, Sean and Holz, Christian and Baudisch, Patrick},
title = {Imaginary Phone: Learning Imaginary Interfaces by Transferring Spatial Memory from a Familiar Device},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047233},
doi = {10.1145/2047196.2047233},
abstract = {We propose a method for learning how to use an imaginary interface (i.e., a spatial
non-visual interface) that we call "transfer learning". By using a physical device
(e.g. an iPhone) a user inadvertently learns the interface and can then transfer that
knowledge to an imaginary interface. We illustrate this concept with our Imaginary
Phone prototype. With it users interact by mimicking the use of a physical iPhone
by tapping and sliding on their empty non-dominant hand without visual feedback. Pointing
on the hand is tracked using a depth camera and touch events are sent wirelessly to
an actual iPhone, where they invoke the corresponding actions. Our prototype allows
the user to perform everyday task such as picking up a phone call or launching the
timer app and setting an alarm. Imaginary Phone thereby serves as a shortcut that
frees users from the necessity of retrieving the actual physical device. We present
two user studies that validate the three assumptions underlying the transfer learning
method. (1) Users build up spatial memory automatically while using a physical device:
participants knew the correct location of 68% of their own iPhone home screen apps
by heart. (2) Spatial memory transfers from a physical to an imaginary inter-face:
participants recalled 61% of their home screen apps when recalling app location on
the palm of their hand. (3) Palm interaction is precise enough to operate a typical
mobile phone: Participants could reliably acquire 0.95cm wide iPhone targets on their
palm-sufficiently large to operate any iPhone standard widget.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {283â€“292},
numpages = {10},
keywords = {mobile, memory, screen-less, non-visual, spatial memory, imaginary interface, wearable, touch},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{2-depth,
  title={Gunslinger: Subtle arms-down mid-air interaction},
  author={Liu, Mingyu and Nancel, Mathieu and Vogel, Daniel},
  booktitle={Proceedings of the 28th Annual ACM Symposium on User Interface Software \& Technology},
  pages={63--71},
  year={2015}
}

@article{1-ppg,
author = {Zhang, Yu and Gu, Tao and Luo, Chu and Kostakos, Vassilis and Seneviratne, Aruna},
title = {FinDroidHR: Smartwatch Gesture Input with Optical Heartrate Monitor},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191788},
doi = {10.1145/3191788},
abstract = {We present FinDroidHR, a novel gesture input technique for off-the-shelf smartwatches.
Our technique is designed to detect 10 hand gestures on the hand wearing a smartwatch.
The technique is enabled by analysing features of the Photoplethysmography (PPG) signal
that optical heart-rate sensors capture. In a study with 20 participants, we show
that FinDroidHR achieves 90.55% accuracy and 90.73% recall. Our work is the first
study to explore the feasibility of using optical sensors on the off-the-shelf wearable
devices to recognise gestures. Without requiring bespoke hardware, FinDroidHR can
be readily used on existing smartwatches.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {56},
numpages = {42},
keywords = {Gesture Interaction, Mobile Sensors, Wearable Computing, Machine Learning}
}

@inproceedings{1-EMG,
  title={EMPress: practical hand gesture classification with wrist-mounted EMG and pressure sensing},
  author={McIntosh, Jess and McNeill, Charlie and Fraser, Mike and Kerber, Frederic and L{\"o}chtefeld, Markus and Kr{\"u}ger, Antonio},
  booktitle={Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
  pages={2332--2342},
  year={2016}
}

@inproceedings{2-EMG,
  title={NeuroPose: 3D Hand Pose Tracking using EMG Wearables},
  author={Liu, Yilin and Zhang, Shijia and Gowda, Mahanth},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1471--1482},
  year={2021}
}

@inproceedings{1-capacitive,
  title={Recognition of grip-patterns by using capacitive touch sensors},
  author={Chang, Wook and Kim, Kee Eung and Lee, Hyunjeong and Cho, Joon Kee and Soh, Byung Seok and Shim, Jung Hyun and Yang, Gyunghye and Cho, Sung-Jung and Park, Joonah},
  booktitle={2006 IEEE International Symposium on Industrial Electronics},
  volume={4},
  pages={2936--2941},
  year={2006},
  organization={IEEE}
}

@ARTICLE{UWB-and-Doppler,
  author={Li, Haobo and Liang, Xiangpeng and Shrestha, Aman and Liu, Yuchi and Heidari, Hadi and Le Kernec, Julien and Fioranelli, Francesco},
  journal={IEEE Journal of Electromagnetics, RF and Microwaves in Medicine and Biology}, 
  title={Hierarchical Sensor Fusion for Micro-Gesture Recognition With Pressure Sensor Array and Radar}, 
  year={2020},
  volume={4},
  number={3},
  pages={225-232},
  doi={10.1109/JERM.2019.2949456}}

@article{EMG-and-camera,
  title={Hand-gesture recognition based on EMG and event-based camera sensor fusion: A benchmark in neuromorphic computing},
  author={Ceolini, Enea and Frenkel, Charlotte and Shrestha, Sumit Bam and Taverni, Gemma and Khacef, Lyes and Payvand, Melika and Donati, Elisa},
  journal={Frontiers in Neuroscience},
  volume={14},
  pages={637},
  year={2020},
  publisher={Frontiers}
}

@inproceedings{EMG-and-camera2019,
  title={Sensor fusion using EMG and vision for hand gesture classification in mobile applications},
  author={Ceolini, Enea and Taverni, Gemma and Khacef, Lyes and Payvand, Melika and Donati, Elisa},
  booktitle={2019 IEEE Biomedical Circuits and Systems Conference (BioCAS)},
  pages={1--4},
  year={2019},
  organization={IEEE}
}

@inproceedings{RFID,
author = {Gillian, Nicholas and Pfenninger, Sara and Russell, Spencer and Paradiso, Joseph A.},
title = {Gestures Everywhere: A Multimodal Sensor Fusion and Analysis Framework for Pervasive Displays},
year = {2014},
isbn = {9781450329521},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611009.2611032},
doi = {10.1145/2611009.2611032},
abstract = {Gestures Everywhere is a dynamic framework for multimodal sensor fusion, pervasive
analytics and gesture recognition. Our framework aggregates the real-time data from
approximately 100 sensors that include RFID readers, depth cameras and RGB cameras
distributed across 30 interactive displays that are located in key public areas of
the MIT Media Lab. Gestures Everywhere fuses the multimodal sensor data using radial
basis function particle filters and performs real-time analysis on the aggregated
data. This includes key spatio-temporal properties such as presence, location and
identity; in addition to higher-level analysis including social clustering and gesture
recognition. We describe the algorithms and architecture of our system and discuss
the lessons learned from the systems deployment.},
booktitle = {Proceedings of The International Symposium on Pervasive Displays},
pages = {98â€“103},
numpages = {6},
location = {Copenhagen, Denmark},
series = {PerDis '14}
}

@INPROCEEDINGS{EMG-and-IMU-for-stroke,  author={Colli-Alfaro, J. Guillermo and Ibrahim, Anas and Trejos, Ana Luisa},  booktitle={2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR)},   title={Design of User-Independent Hand Gesture Recognition Using Multilayer Perceptron Networks and Sensor Fusion Techniques},   year={2019},  volume={},  number={},  pages={1103-1108},  doi={10.1109/ICORR.2019.8779533}}

@inproceedings{Kinect-and-IMU,
  title={Monitoring intake gestures using sensor fusion (microsoft kinect and inertial sensors) for smart home tele-rehab setting},
  author={Hondori, Hossein Mousavi and Khademi, Maryam and Lopes, Cristina V},
  booktitle={2012 1st Annual IEEE Healthcare Innovation Conference},
  year={2012}
}

@inproceedings{rgb+depth+radar,
  title={Multi-sensor system for driver's hand-gesture recognition},
  author={Molchanov, Pavlo and Gupta, Shalini and Kim, Kihwan and Pulli, Kari},
  booktitle={2015 11th IEEE international conference and workshops on automatic face and gesture recognition (FG)},
  volume={1},
  pages={1--8},
  year={2015},
  organization={IEEE}
}

@article{doi:10.1080/15459620802003896,
author = { Mark   Nicas  and  Daniel   Best },
title = {A Study Quantifying the Hand-to-Face Contact Rate and Its Potential Application to Predicting Respiratory Tract Infection},
journal = {Journal of Occupational and Environmental Hygiene},
volume = {5},
number = {6},
pages = {347-352},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1080/15459620802003896},
    note ={PMID: 18357546},

URL = { 
        https://doi.org/10.1080/15459620802003896
    
},
eprint = { 
        https://doi.org/10.1080/15459620802003896
    
}

}

@inbook{earbuddy,
author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.},
title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376836},
abstract = {Past research regarding on-body interaction typically requires custom sensors, limiting
their scalability and generalizability. We propose EarBuddy, a real-time system that
leverages the microphone in commercial wireless earbuds to detect tapping and sliding
gestures near the face and ears. We develop a design space to generate 27 valid gestures
and conducted a user study (N=16) to select the eight gestures that were optimal for
both human preference and microphone detectability. We collected a dataset on those
eight gestures (N=20) and trained deep learning models for gesture detection and classification.
Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user
study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can
facilitate novel interaction and that users feel very positively about the system.
EarBuddy provides a new eyes-free, socially acceptable input method that is compatible
with commercial wireless earbuds and has the potential for scalability and generalizability},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1â€“14},
numpages = {14}
}

@INPROCEEDINGS{1374748,  author={Fujie, S. and Ejiri, Y. and Nakajima, K. and Matsusaka, Y. and Kobayashi, T.},  booktitle={RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)},   title={A conversation robot using head gesture recognition as para-linguistic information},   year={2004},  volume={},  number={},  pages={159-164},  doi={10.1109/ROMAN.2004.1374748}}

@INPROCEEDINGS{4115628,  author={Fujie, Shinya and Yamahata, Toshihiko and Kobayashi, Tetsunori},  booktitle={2006 6th IEEE-RAS International Conference on Humanoid Robots},   title={Conversation Robot with the Function of Gaze Recognition},   year={2006},  volume={},  number={},  pages={364-369},  doi={10.1109/ICHR.2006.321298}}

@inproceedings{10.1145/2043674.2043723,
author = {Lv, Yanpeng and Wang, Shangfei and Shen, Peijia},
title = {A Real-Time Attitude Recognition by Eye-Tracking},
year = {2011},
isbn = {9781450309189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2043674.2043723},
doi = {10.1145/2043674.2043723},
abstract = {Attitude, which is the outward manifestation of human psychology information, is inevitable in everyday situation. Attitude recognition by artificial intelligence has become a hotspot and produced promising results in recent years. In this paper, we propose an attitude recognition method based on eye-tracking. We first improve the eye location algorithm in OpenCV and assume that the trajectory of eyes-center is the same with that of head. Then using eye location method, the center of eyes is calculated. By analysis of displacement of eyes' centers in each two successive frames, sub-result is obtained. The final result is determined by voting algorithm. Experimental results indicate that this method could recognize positive and negative attitudes in real time. We applied our method to "KeJia", an intelligent service robot and it worked well.},
booktitle = {Proceedings of the Third International Conference on Internet Multimedia Computing and Service},
pages = {170â€“173},
numpages = {4},
keywords = {eye location, attitude recognition, positive, negative},
location = {Chengdu, China},
series = {ICIMCS '11}
}

@INPROCEEDINGS{9053124,  author={Vadiraj, Sanjeev Kadagathur and Rao M .V., Achuth and Ghosh, Prasanta Kumar},  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Automatic Identification of Speakers From Head Gestures in a Narration},   year={2020},  volume={},  number={},  pages={6314-6318},  doi={10.1109/ICASSP40776.2020.9053124}}


@misc{dettmers20158bit,
    title={8-Bit Approximations for Parallelism in Deep Learning},
    author={Tim Dettmers},
    year={2015},
    eprint={1511.04561},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@misc{courbariaux2014training,
    title={Training deep neural networks with low precision multiplications},
    author={Matthieu Courbariaux and Yoshua Bengio and Jean-Pierre David},
    year={2014},
    eprint={1412.7024},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{birem2014dreamcam,
title = "DreamCam: A modular FPGA-based smart camera architecture",
journal = "Journal of Systems Architecture",
volume = "60",
number = "6",
pages = "519-527",
year = "2014",
issn = "1383-7621",
doi = "https://doi.org/10.1016/j.sysarc.2014.01.006",
url = "http://www.sciencedirect.com/science/article/pii/S1383762114000228",
author = "Merwan Birem and FranÃ§ois Berry",
keywords = "Smart camera, Image processing, Interest points, VHDL, Harris and Stephen algorithm, Field Programmable Gate Array (FPGA), Hardware implementation, Real-time system",
abstract = "DreamCam is a modular smart camera constructed with the use of an FPGA like main processing board. The core of the camera is an Altera Cyclone-III associated with a CMOS imager and six private Ram blocks. The main novel feature of our work consists in proposing a new smart camera architecture and several modules (IP) to efficiently extract and sort the visual features in real time. In this paper, extraction is performed by a Harris and Stephen filtering associated with customized modules. These modules extract, select and sort visual features in real-time. As a result, DreamCam (with such a configuration) provides a description of each visual feature in the form of its position and the grey-level template around it."
}

@article{pennycook1985actions,
  title={Actions speak louder than words: Paralanguage, communication, and education},
  author={Pennycook, Alastair},
  journal={Tesol Quarterly},
  volume={19},
  number={2},
  pages={259--282},
  year={1985},
  publisher={Wiley Online Library}
}

@inproceedings{furui2003recent,
  title={Recent advances in spontaneous speech recognition and understanding},
  author={Furui, Sadaoki},
  booktitle={ISCA \& IEEE workshop on spontaneous speech processing and recognition},
  year={2003}
}

@inproceedings{nomoto2011anger,
  title={Anger recognition in spoken dialog using linguistic and para-linguistic information},
  author={Nomoto, Narichika and Tamoto, Masafumi and Masataki, Hirokazu and Yoshioka, Osamu and Takahashi, Satoshi},
  booktitle={Twelfth Annual Conference of the International Speech Communication Association},
  year={2011}
}

@INPROCEEDINGS{1318446,  author={Fujie, S. and Ejiri, Y. and Matsusaka, Y. and Kikuchi, H. and Kobayashi, T.},  booktitle={2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)},   title={Recognition of para-linguistic information and its application to spoken dialogue system},   year={2003},  volume={},  number={},  pages={231-236},  doi={10.1109/ASRU.2003.1318446}}

@inproceedings{10.1145/1088463.1088470,
author = {Morency, Louis-Philippe and Sidner, Candace and Lee, Christopher and Darrell, Trevor},
title = {Contextual Recognition of Head Gestures},
year = {2005},
isbn = {1595930280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1088463.1088470},
doi = {10.1145/1088463.1088470},
abstract = {Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people. We investigate how dialog context from an embodied conversational agent (ECA) can improve visual recognition of user gestures. We present a recognition framework which (1) extracts contextual features from an ECA's dialog manager, (2) computes a prediction of head nod and head shakes, and (3) integrates the contextual predictions with the visual observation of a vision-based head gesture recognizer. We found a subset of lexical, punctuation and timing features that are easily available in most ECA architectures and can be used to learn how to predict user feedback. Using a discriminative approach to contextual prediction and multi-modal integration, we were able to improve the performance of head gesture detection even when the topic of the test set was significantly different than the training set.},
booktitle = {Proceedings of the 7th International Conference on Multimodal Interfaces},
pages = {18â€“24},
numpages = {7},
keywords = {human-computer interaction, head gestures, dialog context, embodied conversational agent, context-based recognition},
location = {Torento, Italy},
series = {ICMI '05}
}

@inbook{10.1145/3313831.3376810,
author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun},
title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376810},
abstract = {In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1â€“14},
numpages = {14}
}

@inproceedings{10.1145/3411764.3445687,
author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun},
title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445687},
doi = {10.1145/3411764.3445687},
abstract = { Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1\% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68&nbsp;KB memory size, which can run at 352&nbsp;fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {8},
numpages = {12},
keywords = {activity recognition, sensing technique, voice input},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3478114,
author = {Liang, Chen and Yu, Chun and Qin, Yue and Wang, Yuntao and Shi, Yuanchun},
title = {DualRing: Enabling Subtle and Expressive Hand Interaction with Dual IMU Rings},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478114},
doi = {10.1145/3478114},
abstract = {We present DualRing, a novel ring-form input device that can capture the state and movement of the user's hand and fingers. With two IMU rings attached to the user's thumb and index finger, DualRing can sense not only the absolute hand gesture relative to the ground but also the relative pose and movement among hand segments. To enable natural thumb-to-finger interaction, we develop a high-frequency AC circuit for on-body contact detection. Based on the sensing information of DualRing, we outline the interaction space and divide it into three sub-spaces: within-hand interaction, hand-to-surface interaction, and hand-to-object interaction. By analyzing the accuracy and performance of our system, we demonstrate the informational advantage of DualRing in sensing comprehensive hand gestures compared with single-ring-based solutions. Through the user study, we discovered the interaction space enabled by DualRing is favored by users for its usability, efficiency, and novelty.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {115},
numpages = {27},
keywords = {ring form device, hand interaction, hand gesture sensing}
}

@article{10.1145/3569463,
author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun},
title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings},
year = {2022},
issue_date = {Dec 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569463},
doi = {10.1145/3569463},
abstract = {We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM ($68.3\%$ of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {dec},
articleno = {170},
numpages = {30},
keywords = {ring form device, hand interaction, hand gesture sensing}
}

@INPROCEEDINGS{7952132,  author={Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={CNN architectures for large-scale audio classification},   year={2017},  volume={},  number={},  pages={131-135},  doi={10.1109/ICASSP.2017.7952132}}

@InProceedings{Howard_2019_ICCV,
author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
title = {Searching for MobileNetV3},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{mao2016cat,
  title={Cat: high-precision acoustic motion tracking},
  author={Mao, Wenguang and He, Jian and Qiu, Lili},
  booktitle={Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking},
  pages={69--81},
  year={2016}
}

@inproceedings{berndt1994using,
  title={Using dynamic time warping to find patterns in time series.},
  author={Berndt, Donald J and Clifford, James},
  booktitle={KDD workshop},
  volume={10},
  number={16},
  pages={359--370},
  year={1994},
  organization={Seattle, WA, USA:}
}

@article{chang2006voice,
  title={Voice activity detection based on multiple statistical models},
  author={Chang, Joon-Hyuk and Kim, Nam Soo and Mitra, Sanjit K},
  journal={IEEE Transactions on Signal Processing},
  volume={54},
  number={6},
  pages={1965--1976},
  year={2006},
  publisher={IEEE}
}

 @article{10.1145/2897824.2925953,
author = {Lien, Jaime and Gillian, Nicholas and Karagozler, M. Emre and Amihood, Patrick and Schwesig, Carsten and Olson, Erik and Raja, Hakim and Poupyrev, Ivan},
title = {Soli: Ubiquitous Gesture Sensing with Millimeter Wave Radar},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925953},
doi = {10.1145/2897824.2925953},
abstract = {This paper presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solid-state radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {142},
numpages = {19},
keywords = {gestures, RF, interaction, radar, sensors}
}

@inproceedings{10.1145/2984511.2984565,
author = {Wang, Saiwen and Song, Jie and Lien, Jaime and Poupyrev, Ivan and Hilliges, Otmar},
title = {Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum},
year = {2016},
isbn = {9781450341899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984511.2984565},
doi = {10.1145/2984511.2984565},
abstract = {This paper proposes a novel machine learning architecture, specifically designed for radio-frequency based gesture recognition. We focus on high-frequency (60]GHz), short-range radar based sensing, in particular Google's Soli sensor. The signal has unique properties such as resolving motion at a very fine level and allowing for segmentation in range and velocity spaces rather than image space. This enables recognition of new types of inputs but poses significant difficulties for the design of input recognition algorithms. The proposed algorithm is capable of detecting a rich set of dynamic gestures and can resolve small motions of fingers in fine detail. Our technique is based on an end-to-end trained combination of deep convolutional and recurrent neural networks. The algorithm achieves high recognition rates (avg 87\%) on a challenging set of 11 dynamic gestures and generalizes well across 10 users. The proposed model runs on commodity hardware at 140 Hz (CPU only).},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {851â€“860},
numpages = {10},
keywords = {deep learning, gesture recognition, wearables, radar sensing},
location = {Tokyo, Japan},
series = {UIST '16}
}

@ARTICLE{7907299,  author={Gu, Changzhan and Lien, Jaime},  journal={IEEE Sensors Letters},   title={A Two-Tone Radar Sensor for Concurrent Detection of Absolute Distance and Relative Movement for Gesture Sensing},   year={2017},  volume={1},  number={3},  pages={1-4},  doi={10.1109/LSENS.2017.2696520}}


@inproceedings{moon2020interhand2,
  title={Interhand2. 6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image},
  author={Moon, Gyeongsik and Yu, Shoou-I and Wen, He and Shiratori, Takaaki and Lee, Kyoung Mu},
  booktitle={European Conference on Computer Vision},
  pages={548--564},
  year={2020},
  organization={Springer}
}

@inproceedings{10.1145/3332165.3347947,
author = {Gu, Yizheng and Yu, Chun and Li, Zhipeng and Li, Weiqi and Xu, Shuchang and Wei, Xiaoying and Shi, Yuanchun},
title = {Accurate and Low-Latency Sensing of Touch Contact on Any Surface with Finger-Worn IMU Sensor},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347947},
doi = {10.1145/3332165.3347947},
abstract = {Head-mounted Mixed Reality (MR) systems enable touch inÂ­teraction on any physical surface. However, optical methods (i.e., with cameras on the headset) have difficulty in determinÂ­ing the touch contact accurately. We show that a finger ring with Inertial Measurement Unit (IMU) can substantially imÂ­prove the accuracy of contact sensing from 84.74\% to 98.61\% (f1 score), with a low latency of 10 ms. We tested different ring wearing positions and tapping postures (e.g., with different fingers and parts). Results show that an IMU-based ring worn on the proximal phalanx of the index finger can accurately sense touch contact of most usable tapping postures. ParticiÂ­pants preferred wearing a ring for better user experience. Our approach can be used in combination with the optical touch sensing to provide robust and low-latency contact detection.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {1059â€“1070},
numpages = {12},
keywords = {smart ring, head-mounted display, touch interaction, mixed reality},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inbook{10.1145/3379337.3415901,
author = {Gong, Jun and Gupta, Aakar and Benko, Hrvoje},
title = {Acustico: Surface Tap Detection and Localization Using Wrist-Based Acoustic TDOA Sensing},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415901},
abstract = {In this paper, we present Acustico, a passive acoustic sensing approach that enables tap detection and 2D tap localization on uninstrumented surfaces using a wrist-worn device. Our technique uses a novel application of acoustic time differences of arrival (TDOA) analysis. We adopt a sensor fusion approach by taking both 'surface waves' (i.e., vibrations through surface) and 'sound waves' (i.e., vibrations through air) into analysis to improve sensing resolution. We carefully design a sensor configuration to meet the constraints of a wristband form factor. We built a wristband prototype with four acoustic sensors, two accelerometers and two microphones. Through a 20-participant study, we evaluated the performance of our proposed sensing technique for tap detection and localization. Results show that our system reliably detects taps with an F1-score of 0.9987 across different environmental noises and yields high localization accuracies with root-mean-square-errors of 7.6mm (X-axis) and 4.6mm (Y-axis) across different surfaces and tapping techniques.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {406â€“419},
numpages = {14}
}

@article{10.1145/3351276,
author = {Yang, Zhican and Yu, Chun and Zheng, Fengshi and Shi, Yuanchun},
title = {ProxiTalk: Activate Speech Input by Bringing Smartphone to the Mouth},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351276},
doi = {10.1145/3351276},
abstract = {Speech input, such as voice assistant and voice message, is an attractive interaction option for mobile users today. However, despite its popularity, there is a use limitation for smartphone speech input: users need to press a button or say a wake word to activate it before use, which is not very convenient. To address it, we match the motion that brings the phone to mouth with the user's intention to use voice input. In this paper, we present ProxiTalk, an interaction technique that allows users to enable smartphone speech input by simply moving it close to their mouths. We study how users use ProxiTalk and systematically investigate the recognition abilities of various data sources (e.g., using a front camera to detect facial features, using two microphones to estimate the distance between phone and mouth). Results show that it is feasible to utilize the smartphone's built-in sensors and instruments to detect ProxiTalk use and classify gestures. An evaluation study shows that users can quickly acquire ProxiTalk and are willing to use it. In conclusion, our work provides the empirical support that ProxiTalk is a practical and promising option to enable smartphone speech input, which coexists with current trigger mechanisms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {118},
numpages = {25},
keywords = {inertial sensors, activity recognition, smartphone, mobile interaction, voice input}
}

@inproceedings{10.1145/3313831.3376479,
author = {Mayer, Sven and Laput, Gierad and Harrison, Chris},
title = {Enhancing Mobile Voice Assistants with WorldGaze},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376479},
doi = {10.1145/3313831.3376479},
abstract = {Contemporary voice assistants require that objects of inter-est be specified in spoken commands. Of course, users are often looking directly at the object or place of interestâ€¯? fine-grained, contextual information that is currently unused. We present WorldGaze, a software-only method for smartphones that provides the real-world gaze location of a user that voice agents can utilize for rapid, natural, and precise interactions. We achieve this by simultaneously opening the front and rear cameras of a smartphone. The front-facing camera is used to track the head in 3D, including estimating its direction vector. As the geometry of the front and back cameras are fixed and known, we can raycast the head vector into the 3D world scene as captured by the rear-facing camera. This allows the user to intuitively define an object or region of interest using their head gaze. We started our investigations with a qualitative exploration of competing methods, before developing a functional, real-time implementation. We conclude with an evaluation that shows WorldGaze can be quick and accurate, opening new multimodal gaze+voice interactions for mobile voice agents.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1â€“10},
numpages = {10},
keywords = {mobile interaction, worldgaze, interaction techniques},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3379337.3415588,
author = {Ahuja, Karan and Kong, Andy and Goel, Mayank and Harrison, Chris},
title = {Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415588},
doi = {10.1145/3379337.3415588},
abstract = {Future homes and offices will feature increasingly dense ecosystems of IoT devices, such as smart lighting, speakers, and domestic appliances. Voice input is a natural candidate for interacting with out-of-reach and often small devices that lack full-sized physical interfaces. However, at present, voice agents generally require wake-words and device names in order to specify the target of a spoken command (e.g., 'Hey Alexa, kitchen lights to full bright-ness'). In this research, we explore whether speech alone can be used as a directional communication channel, in much the same way visual gaze specifies a focus. Instead of a device's microphones simply receiving and processing spoken commands, we suggest they also infer the Direction of Voice (DoV). Our approach innately enables voice commands with addressability (i.e., devices know if a command was directed at them) in a natural and rapid manner. We quantify the accuracy of our implementation across users, rooms, spoken phrases, and other key factors that affect performance and usability. Taken together, we believe our DoV approach demonstrates feasibility and the promise of making distributed voice interactions much more intuitive and fluid.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1121â€“1131},
numpages = {11},
keywords = {addressability, speaker orientation, voice interfaces},
location = {Virtual Event, USA},
series = {UIST '20}
}

@article{10.1145/307710.307730,
author = {Billinghurst, Mark},
title = {Put That Where? Voice and Gesture at the Graphics Interface},
year = {1998},
issue_date = {Nov. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {0097-8930},
url = {https://doi.org/10.1145/307710.307730},
doi = {10.1145/307710.307730},
abstract = {A person stands in front of a large projection screen on which is shown a checked floor. They say, "Make a table," and a wooden table appears in the middle of the floor."On the table, place a vase," they gesture using a fist relative to palm of their other hand to show the relative location of the vase on the table. A vase appears at the correct location."Next to the table place a chair." A chair appears to the right of the table."Rotate it like this," while rotating their hand causes the chair to turn towards the table."View the scene from this direction," they say while pointing one hand towards the palm of the other. The scene rotates to match their hand orientation.In a matter of moments, a simple scene has been created using natural speech and gesture. The interface of the future? Not at all; Koons, Thorisson and Bolt demonstrated this work in 1992 [23]. Although research such as this has shown the value of combining speech and gesture at the interface, most computer graphics are still being developed with tools no more intuitive than a mouse and keyboard. This need not be the case. Current speech and gesture technologies make multimodal interfaces with combined voice and gesture input easily achievable. There are several commercial versions of continuous dictation software currently available, while tablets and pens are widely supported in graphics applications. However, having this capability doesn't mean that voice and gesture should be added to every modeling package in a haphazard manner. There are numerous issues that must be addressed in order to develop an intuitive interface that uses the strengths of both input modalities.In this article we describe motivations for adding voice and gesture to graphical applications, review previous work showing different ways these modalities may be used and outline some general interface guidelines. Finally, we give an overview of promising areas for future research. Our motivation for writing this is to spur developers to build compelling interfaces that will make speech and gesture as common on the desktop as the keyboard and mouse.},
journal = {SIGGRAPH Comput. Graph.},
month = {nov},
pages = {60â€“63},
numpages = {4}
}

@InProceedings{10.1007/978-3-030-05792-3_15,
author="Sapi{\'{n}}ski, Tomasz
and Kami{\'{n}}ska, Dorota
and Pelikant, Adam
and Ozcinar, Cagri
and Avots, Egils
and Anbarjafari, Gholamreza",
editor="Zhang, Zhaoxiang
and Suter, David
and Tian, Yingli
and Branzan Albu, Alexandra
and Sid{\`e}re, Nicolas
and Jair Escalante, Hugo",
title="Multimodal Database of Emotional Speech, Video and Gestures",
booktitle="Pattern Recognition and Information Forensics",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="153--163",
abstract="People express emotions through different modalities. Integration of verbal and non-verbal communication channels creates a system in which the message is easier to understand. Expanding the focus to several expression forms can facilitate research on emotion recognition as well as human-machine interaction. In this article, the authors present a Polish emotional database composed of three modalities: facial expressions, body movement and gestures, and speech. The corpora contains recordings registered in studio conditions, acted out by 16 professional actors (8 male and 8 female). The data is labeled with six basic emotions categories, according to Ekman's emotion categories. To check the quality of performance, all recordings are evaluated by experts and volunteers. The database is available to academic community and might be useful in the study on audio-visual emotion recognition.",
isbn="978-3-030-05792-3"
}

@inproceedings{10.1145/3461778.3462004,
author = {Chen, Victor and Xu, Xuhai and Li, Richard and Shi, Yuanchun and Patel, Shwetak and Wang, Yuntao},
title = {Understanding the Design Space of Mouth Microgestures},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462004},
doi = {10.1145/3461778.3462004},
abstract = {As wearable devices move toward the face (i.e. smart earbuds, glasses), there is an increasing need to facilitate intuitive interactions with these devices. Current sensing techniques can already detect many mouth-based gestures; however, usersâ€™ preferences of these gestures are not fully understood. In this paper, we investigate the design space and usability of mouth-based microgestures. We first conducted brainstorming sessions (N=16) and compiled an extensive set of 86 user-defined gestures. Then, with an online survey (N=50), we assessed the physical and mental demand of our gesture set and identified a subset of 14 gestures that can be performed easily and naturally. Finally, we conducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to various daily smartphone operations under a sitting and walking context. From these studies, we develop a taxonomy for mouth gestures, finalize a practical gesture set for common applications, and provide design guidelines for future mouth-based gesture interactions.},
booktitle = {Designing Interactive Systems Conference 2021},
pages = {1068â€“1081},
numpages = {14},
keywords = {user-designed gestures, design space, Mouth microgesture, interaction techniques},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{zhang2018shufflenet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6848--6856},
  year={2018}
}
