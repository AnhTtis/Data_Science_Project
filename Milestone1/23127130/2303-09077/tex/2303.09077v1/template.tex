\documentclass{article}


\usepackage{arxiv}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }


\title{Towards the Understanding of Receptivity and Affect in EMAs using Physiological based Machine Learning Method: Analysis of Receptivity and Affect}


\author{
 Zachary D King \\
  Department of Electrical \\and Computer Engineering\\
  Rice University\\
  Houston, TX, 77005 \\
   \\
  %% examples of more authors
   \And
Han Yu \\
  Department of Electrical \\and Computer Engineering\\
  Rice University\\
  Houston, TX, 77005 \\
   \\
  \And
 Thomas Vaessen \\
  Center For Contextual Psychiatry\\
  KU Leuven\\
  Leuven, Belgium \\
  \\
  \And
 Iniz Myin-Germeys \\
  Center For Contextual Psychiatry\\
  KU Leuven\\
  Leuven, Belgium \\
  \\
  \And
 Akane Sano \\
  Department of Electrical \\and Computer Engineering\\
  Rice University\\
  Houston, TX, 77005 \\
}

\begin{document}
\maketitle
\begin{abstract}
As mobile health (mHealth) studies become increasingly productive due to the advancements in wearable and mobile sensor technology, our ability to monitor and model human behavior will be constrained by participant receptivity. Many health constructs are dependent on subjective responses, and without such responses, researchers are left with little to no ground truth to accompany our ever-growing biobehavioral data. We examine the factors that affect participantsâ€™ responsiveness to ecological momentary assessments (EMA) in a 10-day wearable and EMA-based affect sensing mHealth study. We study the physiological relationships indicative of receptivity and affect while also analyzing the interaction between the two constructs. We collected the data from 45 healthy participants wearing two devices measuring electrodermal activity, acceleration, electrocardiography, and skin temperature while answering 10 EMAs a day containing questions related to perceived mood. Due to the nature of our constructs, we can only obtain ground truth measures for both affect and receptivity during a response. Therefore, we utilized unsupervised and supervised learning methods to infer affect when a participant did not respond. Our unsupervised method used k-means clustering to find the relationship between physiological relationships and responsiveness then inferred the emotional state during non-responses. For the supervised learning method, we primarily used Random Forest (RF) and Neural Networks (NN) to predict affect of unlabeled data points. Based on our findings we showed that using a receptivity model to trigger EMAs will decrease the reported negative affect by more than 3 points or 0.29 standard deviation using our psychological instrument scored between 13 and 91. The findings also showed a bimodal distribution of our predicted affect during non-responses. Our results showed a clear relationship between affect and receptivity. This relationship can affect the efficacy of a mHealth study, particularly those studies that employ a learning algorithm to trigger EMAs. Therefore, we propose a smart trigger that promotes EMA and JITI receptivity without influencing affect during sampled time points as future work.
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
Mobile health (mHealth) technologies continue to grow within the healthcare sector and are imperative in the precision medicine initiative. mHealth can provide beneficial interactions between healthcare providers and patients outside clinical settings. An engaged and responsive user base in mHealth systems is vital in maximizing the knowledge that researchers and providers acquire. Mental health research is particularly dependent on active users because investigators rely on participant survey responses to establish ground truth. Researchers can only adequately interpret the relationships between physiology and psychological state with a population receptive to sensors and surveys. Without highly receptive participants in these mHealth studies, it is challenging to evaluate health construct effectively. Therefore, researchers need to promote responsiveness during their interaction with participants. 

Here we discuss two forms of interaction between participants and mHealth systems in this paper: Ecological Momentary Assessments (EMA) and Just-in-Time Adaptive Interventions (JITAI). EMAs are used to gather in-situ data from users in real-time without reminders. EMAs are commonly used in mHealth studies as they allow a researcher to prompt subjects regularly throughout the day \cite{stone1994ecological}. In the case of mHealth studies focusing on psychological states, EMAs enable users to report their momentary symptoms or context in a natural environment, often using smartphones due to their accessibility. JITAI is a method that allows investigators to send interventions at opportune times to encourage healthy behaviors \cite{morris2005embedded}. Researchers' pursuit of improving the efficiency of these interactions is not new. However, it is needed because the consequence of ineffectual interactions is significant. Several authors have proposed changing the complexity of the questionnaires \cite{king2019micro, ponnada2017microinteraction}, reducing the frequency of the questionnaires \cite{eisele2020effects}, incentivizing based on the momentary response, and using machine learning (ML) models to predict the most receptive times for EMAs and JITAIs \cite{mishra2021detecting, nahum2018just}. These methods have drawbacks, including ethical concerns and diminished datasets. Moreover, there needs to be additional research into ML models' potential harms and biases when dictating the interaction between participants and a mHealth system. We first consider why participants would not respond to an intervention or survey to understand the potential biases better.

Ho et al. \cite{ho2005using} describe 11 factors that influence a person's interuptability (willingness to follow through if notified or interrupted); they include those based on context (social engagement, current activities, future schedule, and emotional state) and the message (frequency, complexity, modality, and utility). Current implementations of systems that attempt to reduce interuptability mainly focus on being context-aware rather than dynamically altering the survey/intervention. The factors employed will influence a model built to minimize these burdens and determine the moments for a higher likelihood of response. Logically, a study intended to analyze emotional state that uses a model to increase the chance for a response will unintentionally influence a participant's emotional state during that response. This relationship between mood and engagement is well documented in the literature. Past research has shown how Positive and Negative affect can influence participation in activities of daily living (ADL) \cite{eckenrode1984impact} and the negative relationship between students' emotional state and academic achievement \cite{duchesne2008trajectories, valiente2012linking}. Demonstrating this relationship is difficult due to the lack of reported affect during non-responses.

This paper will investigate the relationship between receptivity and affect in mental health-related mHealth studies. Furthermore, we will discuss a possible way of improving receptivity in mHealth studies while promoting a diverse dataset regarding the health construct of interest. To do so, we need to infer the emotional state when there is no response using machine learning algorithms. Changing any protocol for initiating messages, for either an EMA or JITAI, there is an inherited added bias by controlling the times a participant is sampled. And by promoting diversity of responses, we would also be introducing bias. However, intervention-based mHealth systems depend on responses collected during negative behaviors or emotions so that JITAIs can be context-aware. Throughout this paper, we will continue to address "bias", our definition of this relates to the effect that changing the protocol can have on a participant's reported emotional state. 

Our contributions to this paper are the following:

\begin{itemize}
    

 \item Propose models for identifying receptive time points and predicting stress.
 \item Analyze the relationship between participant EMA receptivity and affect in a 10-day wearable and EMA-based affect sensing study (N = 45) in the following ways. Explore a way to reveal the relationship between the physiology of response and non-response
 \begin{itemize}
     
 
\item Analyze the relationship using supervised and unsupervised models.
\item Analyze the relationship between affect and response-related features like response time.
\end{itemize}
\item Propose a way to improve user engagement while promoting diverse EMA data collection.
\end{itemize}

In the following sections, we will discuss the methods and framework for analyzing the relationship between receptivity and affect. We will also explore the possible biases that ML models for receptivity can have on the outcome of a study and how those biases can affect the reliability of a study. Lastly, we will discuss the design and implementation of machine learning models dedicated to improving receptivity in mobile or ubiquitous computing settings while diversifying data collection from the study participants. The ideas we propose in this paper can be applied to any application that involves an interaction between a machine and a user. As that connection between machine and user becomes more frequent, it is imperative that we not only optimize the information we have gathered but how we gather it.

\section{Related Work}

In this section, we will discuss three critical topics represented in our paper. We first discuss how researchers attempt to improve participant receptivity in mHealth studies by altering study protocol and using ML models to improve the likelihood of response. As stated in the previous section, emotion plays a role in a user's receptivity. Therefore, we will discuss how emotional states influence participants' engagement in studies across multiple scopes. Finally, to determine the relationship between affect and receptivity, we must predict affect during non-responses. Accordingly, we will discuss mobile and sensor-based methods for inferring affect.

\subsection{Improving Receptivity in mHealth studies}

Reducing the complexity of psychological tools is common; one example is the Perceived Stress Scale (PSS) \cite{cohen1983global}. Initially, this scale was a 14-item instrument but was reduced to a 10-item question set, as 4 of the questions were identified as poor performing \cite{cohen1988perceived}. The authors also proposed a 4-item instrument, demonstrating that the measurement was "adequately reliable" compared to the more reliable 10-item set. Where the 4-item PSS is advantageous for providing "brief measures of perceptions of stress" \cite{cohen1988perceived} and is well suited for online/mobile assessments \cite{vallejo2018determining}. Likewise, the Positive and Negative Affect Schedule (PANAS) instrument was reduced to a 20-item scale \cite{watson1988development} and then to a short form 10-item scale \cite{crawford2004positive} (similar to the psychological instrument used in our study).

A less general version of reducing the complexity of an EMA is from Intille et al. \cite{intille2016muema}. They proposed the micro-EMA, which allowed the user to be prompted more frequently by reducing the complexity of the EMA (i.e., reducing the question set). Intille et al. found that compliance for a standard EMA dropped as the 4-week study went on, while compliance for the micro-EMA remained steady throughout the study \cite{intille2016muema}. The limitation of the micro-EMA is the loss of knowledge gained per query. Many constructs in mental healthcare are complex, and the instruments are lengthy. However, these instruments are the gold standard for measuring psychological constructs. Therefore, it is difficult for researchers to argue using these less complex instruments compared to the more well-defined and researched questionnaires. King et al. proposed a method for determining a micro-EMA by reducing the complexity of a previously described psychological instrument \cite{king2019micro} The proposed framework would analyze user responses and the inter and intra-relationship between questions and physiology. The overall goal of the proposed framework was to find a subset of questions that would best relate to the original question set. While this method alleviates some of the issues mentioned using these less complex EMAs, it still assumes that the reduced question set is representative of the psychological construct. The idea of altering the EMAs themselves is a promising method for improving compliance; however, the main issue is that we will need to validate these new instruments to be accepted by the psychology community.

Other researchers have attempted to improve receptivity by reducing sampling frequency rather than the complexity of the EMA. Wen et al. analyzed multiple studies that used EMA focusing on children and adolescents, finding several factors influencing compliance, including frequency \cite{wen2017compliance} The authors found a decline in compliance rate as frequency increased in clinical and nonclinical studies. On the other hand, \cite{jones2019compliance} conducted similar research on substance users and found little to no variation in compliance based on sampling frequency. These conflicting results indicate that multiple factors influence compliance rates, including frequency, complexity, and population.

The more common approach to increasing compliance is by using incentives. An analysis was done considering 11 studies, both with incentives and without, promoting a health benefit (dentist visits, immunizations, weight loss, etc.) \cite{schouwenburg1995trait}. Of the 11 studies, 1 study did not have higher compliance rates (-1\%) when offering incentives; the remaining 10 had considerable improvement in compliance ranging from 9\% to 35\% increases. Harari et al. found that students were more likely to be compliant with passive sensing and active logging when there was a monetary/device incentive compared to an incentive of course credit \cite{harari2017evaluation}. Financial incentives can be expensive for longitudinal studies and can be seen as exploitative, especially when dealing with vulnerable populations.

Some researchers have tried increasing compliance by changing how they interact with the subjects. Mishra et al. used machine learning models built from previously collected data to improve the receptivity of a JITAI by contacting users at points where they are more likely to be receptive \cite{mishra2021detecting} The authors used contextual features like location and physical activity to predict whether or not a user would respond to the JITAI. They tested a static model using only the previously collected data and an adaptive model, which expanded on the static model using information gathered from the user. The study showed a difference of over 38\% in receptivity between the static model and the control, which distributed the intervention using a set schedule. Mishra et al. built a model for predicting the optimal time to send an EMA \cite{mishra2017investigating}. Their results demonstrated that a model built from contextual cues like activity, audio, conversation, and location could significantly outperform a baseline model (prediction based on the proportion of responded EMAs). Researchers have also shown that contextual cues, including location \cite{ pielot2017beyond, morrison2017effect}, personality traits \cite{kunzler2019exploring, mehrotra2016my}, physical activity \cite{kunzler2019exploring, mehrotra2015designing}, and time of day \cite{bidargaddi2018prompt}, have influenced a participant's willingness to respond to regular surveys.

Nahum et al. discussed the critical components, motivations, and design of JITAIs\cite{nahum2018just}. One of the topics discussed in this publication is adherence and retention to JITAIs, specifically how proximal and distal outcomes related to adherence and retention can be used to prevent poor adherence or receptivity. The authors suggest using behavioral (are participants using the interventions?), cognitive (what is the participant perception of the intervention?), and affective (do participants trust the intervention?) outcomes to assess the intervention. These outcomes can guide and personalize the timing and type of intervention by using reinforcement learning to represent the model's reward. Researchers have used this method to improve interventions in mHealth systems aimed at weight loss and improving physical activity. Several researchers have implemented systems where interventions were assessed and updated based on proximal outcomes (e.g., steps after the intervention, intensive exercise, goals reached, etc.) \cite{wang2021reinforcement, yom2017encouraging}. The issue is that a proximal outcome is not clear for certain health constructs. While physical activity can be seen directly after an intervention, measuring the proximal outcomes of more abstract constructs would be more difficult, particularly in mental health.

These prior studies lack further analysis of the model's effect on reported emotional state; as we mentioned previously, emotional state is one of the factors of interruption burden. Therefore, a model that determines when an EMA is sent should query subjects at points based on their emotions. To avoid biased responses, it is imperative that the models must not affect the response. If so, they need to consider the participant's affect when triggering an EMA or JITAI. In the following section we look at this relationship between affect and receptivity discussed in prior work.

\subsection{Relationship between Affect and Engagement}

Clark et al. \cite{clark1988mood} describe how Positive and Negative affect can influence participation in activities of daily living (ADL). Using statistical tests, the authors analyzed the relationship between reported negative and positive affect with engagement in social activities (parties, eating/drinking, shopping, conversation, etc.). Their results show differences in expected mean across many social activities with reported positive affect having more significance in differentiating the two groups. Similarly, research has also demonstrated a negative relationship between students' emotional state and academic achievement \cite{duchesne2008trajectories, valiente2012linking}. While none of these studies demonstrate the relationship between affect and EMA receptivity during mHealth studies, they all demonstrate the effect of emotional state on a participant's general ability to engage in normal ADL.

\subsection{Mobile Sensor-base Affect Inference}

The field of emotion recognition is well documented and encompasses multiple modalities, including the subject's voice, facial expressions, and physiological data. Wearable sensors are one of the modalities used for in-wild studies, mostly due to the privacy concerns when using video and audio recordings of subjects and those around the subject \cite{alharbi2018can}. There have been many wearable signals utilized for emotion recognition: Electrocardiography (ECG) \cite{ha2021wistress, he2017emotion, hu2018scai, rattanyu2010emotion}, Photoplethysmogram (PPG) \cite{nalepa2019analysis}, Galvanic Skin Response (GSR) \cite{huynh2021stressnas, nalepa2019analysis, ragot2017emotion, sano2013stress, zhao2018emotionsense}, Electroencephalography (EEG), Respiration (RESP) \cite{bari2020automated, he2017emotion}, Body Temperature (BT) \cite{huynh2021stressnas}, Accelerometer (ACC) \cite{bari2020automated, huynh2021stressnas, saganowski2020emotion}, and phone data (GPS, ACC, Activity, Call, and Text logs) \cite{rashid2020predicting}. Although ACC on its own is not necessarily a physiological sensor since it does not measure body functions. The types of emotions related to a signal and the viability of recognition vary from sensor to sensor. Negative affect \cite{rattanyu2010emotion}, Positive affect \cite{rattanyu2010emotion}, general emotion \cite{he2017emotion}, stress \cite{ha2021wistress, huynh2021stressnas, king2019micro, sano2013stress}, and happiness/sadness/fear/anger \cite{zhao2018emotionsense}. There is still no accepted best sensor suite for affect inference, but most researchers would agree that multimodal analysis is the best route for inferring emotional state \cite{dzedzickis2020human}.
The sensor suites used in real-world applications of affect inference are restricted based on accessibility, wearability, and portability; therefore, most researchers are limited to wearable and phone sensors. EDA, ACC, ECG, RESP, and phone data have all been utilized by researchers, with most using a relatively expansive sensor suite. Modeling techniques utilized by these researchers also vary, although some sensors, particularly those collected via phone data, would be considered contextual data rather than time-series data. Machine learning algorithms employed by researchers include Random Forest (RF) \cite{ha2021wistress, huynh2021stressnas, zhao2018emotionsense}, Support Vector Machine (SVM) \cite{he2017emotion, hu2018scai, zhao2018emotionsense}, Logistic Regression (LR), K-nearest neighbors (kNN) \cite{huynh2021stressnas}, Neural Network (LSTM, RNN, CNN, etc.) \cite{huynh2021stressnas, zhao2018emotionsense}, Naive Bayes (NB) \cite{zhao2018emotionsense}, and many more \cite{zhang2020emotion}.

\section{Methods}

\subsection{Data Collection}
The study included 45 healthy adult participants in Leuven, Belgium. The average age of the subjects was 24 years old and 85\% of the subjects were female.

The participants wore a sensor suite and responded to 10 EMAs daily that were spaced out roughly 90 minutes apart. The sensor suite in Figure ~\ref{fig:sensor} includes a chest patch with two electrodes for gathering Electrocardiography (ECG) at 256 Hz. and a wristband for electrodermal activity (EDA) at 256 Hz, skin temperature (ST) at 1 Hz. and acceleration (ACC) at 32 Hz. Participants were allowed to remove the device while they slept and were asked to remove the devices while they bathed or participated in rigorous activity. The sensors had battery life that surpassed the duration of the study and data was recorded on the device in an SD card.

\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth]{images/Figure_1.jpg}
\end{center}
\caption{Left: chest patch for gathering electrocardiogram (ECG) and acceleration (ACC). Right: wristband for gathering electrodermal
activity (EDA), skin temperature and ACC.}
\label{fig:sensor}
\vspace{-20pt}
\end{figure}

As the term EMA can be ambiguous, we reference our protocol and the participant's perceived emotional state when we refer to an EMA. EMAs are initiated via text message and the subject has a specific amount of time to respond to the survey attached to the text message before it closes. The EMAs were available in three languages English, Belgium, and French. The study lasted around 10 days, although some participants enrolled for longer with a max of 12 days, although they did not receive EMAs after their 10th day of enrollment. The EMAs contained items from the Positive and Negative affect schedule (PANAS) as well as mood questions from \cite{myin2001emotional}. The questions and the distribution of responses can be seen in Figure ~\ref{fig:EM_State}.

In total, there were 13 questions (Figure ~\ref{fig:EM_State}). Responses were between 1 and 7, with nine negative and four positive questions. Participants were given 50 cents for each EMA they responded to. Participants responded to the notification on average in 20.9 seconds and a median response time of 8.7 seconds. There were no responses after 306 seconds of a notification.

\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth]{images/Figure_2.jpg}
\caption{Question Set; Includes the 13 questions we use to distinguish affect with their mean, standard deviation and correlation to the final affect score. These 13 questions can be split into Positive Affect (PA) and Negative Affect (NA)}
\label{fig:EM_State}
\end{figure}

\subsection{EMA Analysis}

Scoring our question set was done by adding the numerical interpretation of the nine negative responses to the reverse (1 is 7 and 7 is 1) of the positive questions. The range of possible scores is between 13 and 91, with higher scores relating to more negative emotions. Figure ~\ref{fig:3} shows a box plot of each participant's responses. On average, participants responded with a 4.5 on positive affect (PA) questions and a 1.8 on negative affect (NA) related questions. This disparity in affect intensity was consistent with past research \cite{myin2001emotional}.

\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth,trim={4cm 0 4cm 0}]{images/Figure_3.jpg}
\caption{Box plot of perceived emotional state, min is 13 (Negative) max is 91 (Positive). The average perceived Emotional State is 26.42 which is denoted by the blue horizontal line.}
\label{fig:3}
\end{figure}

There were 3885 notifications sent to the 45 participants and 3066 responses for a receptivity rate of just under 79\%. Most studies say that the quality receptivity rate is at 80\%. The range of response time was between 0.5 seconds and 306 seconds. The reason for this low response time is that participants were allowed 90 seconds to begin the survey, after which the survey would no longer be accessible. This restriction makes it challenging to relate response times to participant affect, as other researchers have done.


For each mood question, we calculated the correlation with response time (time taken for participants to respond to EMAs) using Spearman's correlation coefficient (cannot assume the response times are normally distributed) on the initial 3066 responses. Since the surveys were repeated, we calculated the correlation for each participant and then averaged the coefficient across all questions. By doing so, we found that none of the mood responses were strongly correlated with time to respond. Across each question, we did not obtain a correlation coefficient greater than 0.03 (all correlations indicated significant confidence p < 0.05). This low correlation coefficient would indicate that the participant's mood had little to do with how long it took the subject to respond to a question. Although considering the limit we put on response time, this relationship might be difficult to assume.

\subsection{Framework}

In the following sections, we will discuss the sequential methodology of handling the raw signal data and eventually model our two constructs, responsiveness and affect. This framework is shown in Figure ~\ref{fig:4} and discussed in the following section. We began by processing our four sets of time series data, ST, ECG, EDA, and ACC. Once we have processed the data, we segmented it and attached labels to each segment based on conditions explained in a later section. Next, we built the model, tested multiple machine-learning algorithms, and verified the results using several statistical techniques. The flow defining the structure from input of raw data to our results can be summed up into 4 modules: (1) defining our input data (sections Time Series Processing and Segmentation), (2) defining our ground truth (Class Labels Section), (3) modeling (Responsiveness and Affect Model Design Section) and finally (4) cross-validation (Model Evaluation Section).


\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth]{images/Figure_4.jpg}
\caption{Methodology used from the raw signals to our evaluation of the relationship between Affect and Receptivity}
\label{fig:4}
\end{figure}

\subsubsection{Time Series Processing}

We began by extracting all the data from the four time series data sets. See features computed for the four sets of the data in Table ~\ref{tab:features} We used interquartile range (IQR = Q3 - Q1) to process ST to remove outliers. We used biosppy \cite{biosppy} for ECG to process the data and extract R peaks. Biosppy uses a Band pass filter with frequencies at 3 Hz and 45 Hz, a sampling rate of 256, and the Hamilton segmentation algorithm to extract R peaks. We then validated the R peaks using an algorithm from Hovesepian et al., this algorithm uses the criterion beat difference (CBD) based on the Maximum Expected Difference (MED) for a beat and the Minimal Artifact Difference (MAD) \cite{hovsepian2015cstress}. We then used HRVanalysis to extract heart rate and heart-rate variability (HRV) features such as NN20 and RMSSD \cite{pichot2016hrvanalysis}. We also obtained some frequency and geometric-based features. For EDA, we used the method proposed by \cite{taylor2015automatic} to process and extract both statistical and wavelet features. Finally, for ACC, we smoothed the signal by using a 4th order 10Hz low pass Butterworth filter and obtaining an average, then we used a package from \cite{sensormotion} to extract step features. The features we extracted and information on how those features are calculated can be seen in Table ~\ref{tab:features}


\begin{table}
\centering  
\resizebox{0.95\linewidth}{!}{\begin{tabular}{p{0.162\linewidth} | p{0.3\linewidth} | p{0.5\linewidth}}
\hline
\textbf{Signal} &  \textbf{Features} & \textbf{Description}\\
\hline
\textbf{Skin} \textbf{Temperature} \textbf{(ST)} & mean, median, mode, minimum, range, root mean square, zero cross, Kurtosis, skew, IQR 25\textsuperscript{th} and 75\textsuperscript{th} Percentile &Zero-cross here is based on the number of times ST crosses over the mean ST. Kurtosis measures the extremity of the data in the segment and skew is the measure of asymmetry.  \\
\hline
\textbf{Electrocardiogram} \textbf{(ECG)} & mean, median, mode, minimum, range, root mean square, zero cross Kurtosis, skew, IQR 25\textsuperscript{th} and 75\textsuperscript{th} Percentile RMSSD, CVSD, CVNNI SDNN, NNI50, NNI20, PNNI50, PNNI50, low frequency (lf), very low frequency (vlf), high frequency (hf), high/low frequency ratio (hf/lf) &  NN (N-N or R-R interval) indicates time between heart beats. NNI20/50 refers to the number of successive intervals that differ by more than 20 or 50 ms. P indicates the proportion of NNI20/50 in the segment. RMSSD is the root mean square of successive differences between heartbeats. CVNNI and CVSD are the coefficients of variation (sdnn/mean) and (rmssd/mean) respectively. Our frequency domain features are based on how much of the signal lies between 0.003 to 0.04 Hz (vlf),  0.04 to 0.15 Hz (lf), 0.15 to 0.40 Hz (hf)\\
\hline
\textbf{Electrodermal} \textbf{Activity} \textbf{(EDA)} & \textbf{Wavelet:} max, mean, std, median, above zero (1 second and half second wavelet)
\textbf{Raw:} amplitude, max, min, mean
\textbf{Filtered:} amplitude, max, min, average  & A 1-second and a half-second window were used for wavelet features. Features were calculated for both the first and second derivatives of each window size.
\\
\hline
\end{tabular}}
\caption{Features from our 3 raw sources as well as definitions for those features that are less commonly used.}
\label{tab:features}
\end{table}

\subsubsection{Segmentation}

We segmented the data into 1-minute windows with a 30-second overlap. We then calculated statistical features for each of the sensors, excluding steps. For each of these windows, we calculated historic features. To do so, we elongated each of the windows by 5, 30, and 60 minutes, then extracted the features with the extended window size (i.e., for each 1-minute window, we have not only the features from the 1 minute but also the features going back to these 4 time frames).

\subsubsection{Receptivity Labels}

Labels for receptivity were gathered based on whether it was within a specific time of the scheduled notification and whether or not the user responded. By expanding the window of labeled data, we can increase the size of our labeled dataset. Although, as this window increases, so does the distance between some of our time points and the corresponding label. We tested windows that are 5, 30, 60, and 120 minutes long. For instance, for the 5-minute window, if an EMA was sent at 12:00, the segments that fall between 11:55 and 12:00 would be labeled "responded" if they did respond and "no response" if they did not. We applied the same method for the affect labels, see Figure ~\ref{fig:5} We ultimately chose 30-minute windows due to the balance between the size of the training set and the labeled points being relatively close in terms of time to the actual response (or non-response).

\subsubsection{Affect Label Thresholds}

While many psychological instruments have predefined categorical representations for their scores, allowing researchers to separate the construct in a meaningful way, our chosen instrument is not as well documented. For this reason, we constructed several different thresholds for converting the affect score into binary or categorical classes for negative and positive emotion. One such threshold we tested was a generalized mean (mean of affect score across all participants) and used that score as the cutoff. Although participants rarely indicated high negative emotion, this can be seen in Figure ~\ref{fig:3} Participants' average and median reported affect were under 26, meaning that on average, the participant responded to each question with a relatively low 2 (on a scale between 1 and 7 where 1 indicates high positive emotion and 7 shows high negative emotion).

\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth]{images/Figure_5.jpg}
\caption{Representation of our labeled segments for the 5-minute window.}
\label{fig:5}
\end{figure}

Similarly to the generalized mean, we also looked at a threshold based on a personalized mean. While these methods may seem logical, they do not represent positive and negative affect very well. These methods assume that a subject is equally likely to be in a negative emotional state at any time compared to a neutral or positive emotional state. Hence, we focused on techniques where the cutoff was more logical including splitting the scores based on the 3rd quartile (32), a personalized 3rd quartile threshold, splitting the scores into multiple classes (binning responses into six groups 13-26, 27-39, 40-52, 53-65, 66-78, and 79+), and regression (using the exact score as our ground truth).

While there is no correct answer to which threshold is best, we will focus on two thresholds based on our experience. The first is a personalized 3rd quartile; many of the binary labels we generated using these thresholds made little to no sense regarding the threshold position and distribution of affect labels (we assume more positive vs. negative). The personalized 3rd quartile gave us a reasonable allocation of affect labels (roughly 75-25 negative-positive split). The second label we use is the actual score, and then we implement a version of regression for each of our ML algorithms.

\subsubsection{Responsiveness and Affect Model Design}

We started by testing several models that showed promise in similar studies, including Random Forest (RF), and Support Vector Machine (SVM) \cite{mishra2017investigating}. A Neural Network (NN) and a baseline model were included in the tested models. The baseline model was used to determine how models differ from random, while the Neural Network was introduced as a possible improvement on existing model implementations. These models were used for both receptivity and affect models. 

The one thing absent in our discussion thus far is what emotional states look like when a subject does not respond to an EMA. To better understand the impact a receptivity model could have on reported perceived emotional states and interventions. We designed models dedicated to predicting both responsiveness and affect. While we cannot determine the participant's emotional state during times when they did not respond to an EMA, we can use the output of an emotional state model as a reference. Model validity is essential for establishing confidence in our predictions and eventually demonstrating the relationship between affect and responsiveness. We will primarily use personalized models to ensure model validity while filtering predicted values based on model uncertainty. By inferring affect scores during non-responses, we can analyze the relationship between reported perceived emotional state and predicted responsiveness.

\subsubsection{Model Design and Hyperparameter Tuning}

In order to optimize our personalized model, we selected hyperparameters using grid search method for each participant, explicitly using the GridSearchCV method defined in scikit-learn. This method uses an exhaustive search method (i.e., testing each user-defined parameter permutation). Using a training and validation set we selected the parameters and then applied the optimal model to our test set. 

Our NN model was structured to use three densely connected layers using a ReLu activation function at each layer. The output dimension of each layer was 256, 128, and 64, and the output layer was a densely connected layer with two output dimensions. The reasoning for an output layer of two is to define a confidence interval for our regression model.

The baseline model was built by predicting randomly based on the distribution of the class labels in the training set (i.e., if 10\% of the labels are non-responses and 90\% are responses, the model would predict non-responses 10\% of the time). We can determine expected outputs for this model; our True Positive Rate should equal Pr(response in the training set) $\times$ Pr(response in the test set). The more evenly the class labels are distributed the worse the model will perform. A similar method was applied using the categorical class labels, and for regression, we used a normal sampling method with the mean and standard deviation based on the training set class labels.
As there are more labeled responses compared to non-responses, we considered this imbalance in the model, weighting the classes based on the distribution in our training set. All the models were built using python package scikit learn \cite{scikit-learn} or Tensorflow \cite{abadi2016tensorflow}.

In order to determine the relationship between affect and receptivity, we have to use predictions to infer the emotional state of our participants during non-responses. And since affect is a complex and difficult to predict construct, we need a method for filtering our predictions based on some level of confidence. For this purpose we will be introducing a method for calculating uncertainty for regression using a neural network.   

\subsection{Model Uncertainty}

Determining a value for confidence for a regression model is difficult compared to binary or categorical model. Our binary classification model uses the probability of the predicted class label as our determinant for uncertainty. We used the least confidence uncertainty for the binary and categorical models, calculated by the difference between our most confident prediction and absolute confidence. When the output of a model is binary or categorical, we can easily calculate the probability of each label and, therefore, the uncertainty. This is not as simple to calculate for regression models because the output is a continuous value (prediction) rather than a probability distribution. We can utilize a custom loss function in our neural network to estimate epistemic and aleatoric uncertainty for our regression model, where epistemic uncertainty is based on our ability to predict our class labels with the data available (affected by lack of knowledge or data), and the aleatoric uncertainty is affected by randomness that is unknown or unmeasured in the model \cite{der2009aleatory}. The loss function is derived from the Mean Square Error (MSE) calculation and the maximum likelihood of a normal Gaussian distribution \cite{valdenegro2022deeper}. The loss function is shown in equation ~\ref{eg:sig}, which is based on our two output values $\mu(x)$ and $ln(\sigma(x))$. Rather than a single predicted output, ours will be two dimensional, the first position will be  $\mu(x)$ the predicted affect, and the second position will contain $ln(\sigma(x))$ the predicted variance (the log allows us to take the exponent to ensure a positive value for sigma). Both $\mu$ and $\sigma$ are functions of our training set x. The numerator of this equation is identical to the MSE loss function, where $\mu(x)$ is the predicted output of our model. Unlike the MSE loss function, we will continuously update not only our predicted output $\mu$, but predicted variance $\sigma$. The sigma output of our model is based on error, the sigma value will increase to account for higher error and decrease to account for lower error. This $\sigma$ value can then be used as an uncertainty or error metric. While it is still a predicted value, it should align with how confident the model is in the $\sigma(x)$ output. The results section will demonstrate the relationship between uncertainty and sigma.

\begin{equation}\label{eg:sig}
\bigg( \frac{1}{n} \bigg) \sum_{i=1}^{n} \Bigg( 2 \ln{\sigma(x_i)} + \bigg( \frac{y_i - \mu(x_i)}{\sigma(x_i)} \bigg)^2 \Bigg)
\end{equation}

\subsection{Model Evaluation}

For cross-validation, we used a personalized random train-test split cross-validation method. We randomly split into our training and testing set using the response label (whether they responded to the EMA or not) to stratify the split. Since our response labels are unbalanced, we want to ensure that our training, validation, and test sets have a relatively even number of responses and non-responses. For the purpose of fairness, we excluded 3 participants from our responsiveness results. These participants had a single non-response during the entirety of the study; as there is a single non-response we cannot split our training, validation, and testing set fairly.

We first normalized the training and test set independently of one another based on the participant. In total, we had around 230 features derived from the sensor signals. We initially selected significant features based on the correlation between the signal and the class label represented by whether or not they responded to the EMA. But after further consideration, we decided to reduce our feature set using Principal Component Analysis (PCA). PCA is a dimensionality reduction technique representing our original dataset using a smaller group of principal components. Our implemented PCA was set so that the number of produced components explained 99\% of the variance (48 features). This method was used for each model, excluding the Random Forest model, where we used the original normalized data as input. Using the models we defined in the previous subsection, we determined the relationship between affect and receptivity. Once we made predictions for our test set, we looked at the agreement between receptivity and predicted affect using true labels when available (i.e., time points where we had responses we have true labels for receptivity and affect). We also analyzed the difference in the perceived emotional state during predicted non-responses and responses by looking at the average emotional state during these two response states.

\subsection{Analyses of the Relationship between Affect and Receptivity}

We conducted two different analyses to understand the relationship between affect and receptivity better:
\begin{enumerate}
     

\item In order to infer emotional state during non-responses, we clustered the physiological data and then examined the makeup of the clusters. By doing so we can assume emotional state of different clusters and of unlabeled data points.
\item For EMAs the participants did not respond to, we used the model described in the previous section to infer the emotional state at the time of a non-response. With these newly predicted affect scores, we can analyze the differences in the emotional state during a response and non-response.
\end{enumerate}

\subsubsection{Cluster Evaluation}

The data we used as input for our clustering method are the most significant features (based on correlation) when predicting receptivity. To find the optimal clustering method, we tested several clusters and both K-means and Hierarchical clustering. We then calculated the silhouette score across all clusters using receptivity as our ground truth and selected our best-performing combination of clusters. Based on the cluster distribution, we analyzed the difference in the perceived emotional state of the participants. We calculated the average NA and PA of responses in each cluster for each participant and then the difference in the emotional state between the clusters using repeated measure ANOVA. These results will give us a sense of participants' perceived emotional state during non-responses. We also investigated differences in receptivity in two clusters using the chi-square test.

\subsubsection{Analysis of Receptivity and Affect Relationship}

Ideally, we would show the interaction between affect and receptivity with the data collected. Still, since nonresponses do not have a corresponding affect score, we designed and implemented our models for receptivity and emotional state. Using the output of the affect model, we analyzed the distribution of affect scores during responses and non-responses. Using predicted affect during non-responses and truly perceived affect during responses, we also looked at the difference in variance between the two groups using an ANOVA test. These measures will demonstrate the relationship and agreeableness of our two constructs. Based on our receptivity model, we can estimate the difference in the reported perceived emotional state between our true findings and predicted affect during time points that would initiate an EMA.

\section{Results}

In the following section, we will discuss the results of our study, particularly the methods of evaluation that were discussed in the previous section.

\subsection{Analysis of Features}

Before using PCA, we calculated repeated ANOVA tests to determine how well the features differ between the response class labels. The features that we found to be the most significant were ECG low frequency (1 min, momentary F(2,54) = 6.7, p<.001) and very low frequency features (1 min, momentary F(2,54) = 4.7 p=.02 and 60 minutes, F(2,54) = 4.1 p<.001), EDA mean F(2,54) = 10.2 (p<.001) and median F(2,54) = 15.4 (p<.001), NNI (5 and 60 mins), pNNI50 (30 mins, F(2,54) = 11.2 p<.001), and max (F(2,54) = 6.3 p<.001), min F(2,54) = 3.6 (p=.009), and absolute max (F(2,54) = 6.6 p=.002) of the first and second derivatives for EDA. For the most part. ECG and EDA related features were best at differentiating between responses and non-responses.

\subsection{Model Evaluation}

\subsubsection{Analyzing Uncertainty in Affect model}

Figure ~\ref{fig:6a} helps visualize the relationship between the sigma value we calculated and the uncertainty. Uncertainty should follow a pattern where class labels that are more represented in the training set should have lower uncertainty. Conversely, values less represented in the dataset should have larger uncertainty. As you can see, Figure ~\ref{fig:6a} sigma values are smaller when the reported emotional state is more positive. If you recall Figure ~\ref{fig:3}, most of our responses are relatively low, with few participants reporting an affect score greater than 40.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/Figure_6a.jpg}
  \caption{}
  \label{fig:6a}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=\textwidth]{images/Figure_6b.jpg}
\caption{}
\label{fig:6b}
\end{subfigure}
\caption{Box plot showing Sigma value for True Labels (a) and Predicted Error (b)}
\end{figure}

Figure ~\ref{fig:6b} shows the relationship between sigma and testing error. This is expected and can be seen as the model uncertainty. When our prediction is far from the ground truth we expect uncertainty to be larger than predictions with smaller errors. Based on these two figures we can say that the sigma value we calculated is related in some way to uncertainty. Based on Figure ~\ref{fig:6b}, we can see that the majority of responses with an affect score less than 39 have a sigma of less than 6. Therefore, we have chosen 6 as our cutoff for uncertainty. This cutoff will filter out many of the predictions that are more likely to have higher error, since we cannot look at error during non-responses as we have no affect label.

\subsection{Responsiveness and Affect Analyses}

After processing, cleaning, and filtering out segments with confounding values, we were left with 1368 responses with usable physiological data. As our class labels were expanded to include segments 30 minutes before the point of response, we ended up with 13477 data points for determining affect and 17254 data points for predicting response (cannot use non-response in our affect models).

\subsubsection{Cluster Analysis}

We chose to cluster our data into 2 clusters, and found that the distribution of responsiveness was fairly different between clusters. The number of clusters was based on silhouette scores, based on the "elbow rule" of silhouette scores, we chose 2 clusters for the physiological data. Cluster 0 contained a higher density of responses, with just under 15\% non-responses. While cluster 1 had a higher density of non-responses of just over 21\%. We first analyzed the difference in overall affect score, where we found the average reported affect score in cluster 1 to be more than 3 points higher than the average reported affect in cluster 0 (repeated measure ANOVA, F value 23.16, p<.001). Then we also found the distribution of receptivity was different between the two clusters using the Chi-square test of independence (test-statistic = 898.8, p<.001). These results indicate distinctions between response and affect across the cluster labels. Considering that the cluster with a higher density of non-responses also had a higher average affect score (higher scores indicate more intense negative emotions or lower positive emotions), we can assume that there was a relationship between subject receptivity and reported affect. Figure ~\ref{fig:7} is a scatter plot of the difference between perceived positive affect (PA)/negative affect (NA) and the cluster for each participant. The results show that participant perceived emotion was more negative in terms of lower PA and higher NA in cluster 1 compared to their perceived emotional state in cluster 0. As we stated earlier, cluster 1 contains a higher percentage of non-responses compared to cluster 0, indicating cluster 1 is a better representation of a non-response. Therefore, it would appear that there is a relationship between negative perceived emotional state and receptivity. Using the cluster labels as groups we calculate the f-score using an ANOVA test of each of the features. The features that separated the two clusters were mostly calculated from the ECG signal, including minimum heart rate, low/very low-frequency, mean heart rate, CVNNI, CVSD, high frequency, and maximum heart rate (in order of f-score). Features obtained from EDA, ACC, and body temperature did not return significant p-values when calculating the f score.

\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth]{images/Figure_7.jpg}
\caption{ Each point represents a participant where the x axis denotes the difference between average NA of cluster 0 and 1, while the y axis represents the difference between average PA of clusters 0 and 1.}
\label{fig:7}
\end{figure}

\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth]{images/Figure_8.jpg}
\caption{Distribution of responses and non-response in each cluster; Distribution of negative affect intensity (high>36) in each cluster.}
\label{fig:8}
\end{figure}

\subsubsection{Model Results}

As we mentioned previously, our regression model included a $\sigma$ value representing uncertainty. The $\sigma$ value allows us to filter our results based on the model confidence. Based on the results presented in Figures ~\ref{fig:6a} and ~\ref{fig:6b}, we selected our cutoff for confidence to be 6.  Figure ~\ref{fig:9} shows the cumulative distribution of reported affect scores for responses and predicted affect scores for responses and non-responses. Based on this figure there is a clear difference between predicted affect during non-responses and our true affect scores. While this could be a model error, we also predicted affect scores during these responses and found that our model consistently predicts smaller affect values. Table ~\ref{tab:2} also shows the results of our regression models. Based on these results, there was little difference between the Random Forest and Neural Network models, although we use the Neural Network models to demonstrate the relationship between affect and receptivity in the following section. By using the NN models, we can more confidently analyze this relationship by filtering out more confounding data points based on the predicted sigma value.  

\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth,trim={3cm 0 3cm 0}]{images/Figure_9.jpg}
\caption{Cumulative distribution of predicted and actual affect scores for responses and non-responses.}
\label{fig:9}
\end{figure}

\begin{table}

\centering  
\resizebox{0.95\linewidth}{!}{\begin{tabular}{|c||c|c|c|c||c|}
    \hline
    
    & \multicolumn{4}{c||}{Receptivity} &  \multicolumn{1}{c|}{Affect}\\
\hline 
Model & ACC & Precision & Recall & F1 & RMSE  \\
    \hline\hline

Baseline  &   0.73 (0.001) & 0.83 (0.002) & 0.84 (0.002) & 0.83 (0.002) &   11.1 (4.3)      \\
    \hline
NN   &       0.84 (0.19) & 0.82 (0.006) & 0.85 (0.1) & 0.86 (0.2) &  7.3 (2.7)               \\

\hline
RF   &  0.83 (0.11) & 0.82 (0.15) & 0.94 (0.1) & 0.87 (0.12) &  7.5 (3.1)       \\
\hline
\end{tabular}}
\caption{Model results for predicting Receptivity (Binary) and Affect (Regression). Average evaluation metric across participants followed by the standard deviation in parenthesis.}
 \label{tab:2}
\end{table}

\subsection{Relationship and Analysis between Receptivity and Affect}
There was a fair amount of agreement between our affect and our binary response model with a Cohen's Kappa score of 0.33 and a correlation of 0.44. When our model predicted a response, 77\% of those responses were during times when the affect model predicted positive affect. While only 69\% of the predicted non-responses reported positive affect. This indicates that the predicted response is negatively related to affect (i.e., responses are associated with positive affect while non-responses are associated with negative affect). The reason determining the relationship between our constructs is important is because this bias can and as we show, will affect the overall outcome of a study. For instance, the average predicted affect score for times that we predicted as low likelihood for a response was a full 1.5 standard deviations (1.35) or 2.01 score more than the average predicted affect for points predicted to be of high likelihood for a response. When observing just the segments where we misclassified a response (i.e., we have a true affect, but we are using predicted response), we found that the average affect score dipped slightly from 26.1 (predicted non-response) to 25 (predicted response). This difference in affect between responses and non-responses is evidence that our receptivity model is indirectly based on affect. The standard deviation of the affect score also decreases from 11.1 (true labels) to 9.8 (true affect and predicted response) during responses. This demonstrates that an mHealth study implementing a receptivity trigger based purely on likelihood to respond (model that triggers EMAs and JITAIs based on likelihood to respond) will bias the subject's response. In this case, the model would initiate an EMA or JITI during times of more positive emotions, therefore, decreasing the overall affect score for the EMA and possibly sending the JITI during times where the intended construct is not being met. Since our ability to predict binary affect is limited with this dataset, we believe that using the affect regression and ground truth labels for responses will return the most realistic representation of affect during non-responses. The average predicted affect score for a non-response was 30.9 (11.2) and the average affect score for a response was 29.3 (10.7) (True) and 27.7 (8.9) (Predicted). Predicted affect scores during non-responses are larger than both reported and predicted affect scores during responses. And given that our average testing error was -1.6, we could also assume that predicted affect during these non-responses could be more negative than true predictions. The distribution of these scores can be seen in Figure ~\ref{fig:10}

\begin{figure}
 \centering
\includegraphics[width=0.85\textwidth]{images/Figure_10.jpg}
\caption{Distribution of predicted and True Affect scores for responses and non-responses. Density is specific to the Response and non-Response.}
\label{fig:10}
\end{figure}

\section{Discussion}

In this section, we will discuss the outcome of our study, particularly the relationship between emotional state and receptivity, what that means, how it affects our results, and how we might implement a receptivity model that removes this bias. We will also mention the limitations of this work and study.

\subsection{Principal Results}

This work aimed to understand how machine learning models, used to improve subject receptivity, can affect the outcome of a study. While we focus on the emotional state in this work, we feel as if there are many health constructs and outcomes that can be affected by these receptivity models. Improving receptivity is not a new concept, but in the realm of mHealth, it is an emerging problem. The factors influencing study adherence have been analyzed and discussed in depth in past research. One such scope is in medication adherence. Researchers have found many factors that affect medication adherence, from social, therapy, patient, or disease-related \cite{gast2019medication}. But few have looked at the momentary factors that affect adherence to medication or a health construct, and few have had the ability to without wearable sensors and momentary assessments.

Based on Figure ~\ref{fig:10}, all three groups' affect scores peaked at around 20-25; this is probably due to a large number of reported affect scores in this range. But non-responses have a second peak at an affect score of 40. This bimodal distribution could indicate that our distribution is affected by two or more factors. Some non-responses may not be because of their affect, but perhaps their daily life activities (seeing a movie, spending time with family, taking a shower, etc.). In contrast, the second peak would indicate that negative affect is related to non-responses. These results are consistent with our clustering method as well. While it is difficult to determine whether there is a bimodal distribution among the affect of our non-responses, we can see a clear difference in the affect scores of our two clusters.

By incorporating more psychological and environmental cues (personality traits, working hours, etc.), we can better understand what to expect from our subjects regarding responsiveness and affect. Incorporating uncertainty as part of the decision-making process would allow us to see a consistent sampling rate between different emotional states. And then, as the model has time to develop, we can incorporate expected affect so that if we continue to see a disproportionate amount of responses with a certain affect, we can interrupt the model. When comparing our receptivity modeling to other researchers, our model performance is limited due to the lack of contextual cues. Introducing features like location, phone use, activity, and others would help us predict emotional state and receptivity.

\subsection{Limitations}

Our current method assumes the perceived emotional state reported by participants during the study is representative of the participant's average affect. Our study sent 10 EMAs a day and receptivity was nearly 75\%, allowing us to comprehensively understand a participant's affect compared to studies that may be more conservative in the frequency of sending EMAs. This could be difficult for other researchers to implement as the frequency and complexity of the EMA is fairly burdensome. While we believe this relationship will transfer over to other studies and uses, the number of participants was relatively low (N = 45). This may be specific to our cohort, but previous studies analyzing medication adherence indicate that the effect of emotional state on receptivity is common among multiple populations

The data gathered in our study was limited to physiological features and user-defined responses. While the physiological features make up a large portion of what researchers consider important for predicting psychological constructs, the dataset lacks in sampling contextual data. Certain contextual information is imperative for recognizing emotion and improving EMA response rates that cannot be obtained using physiology, like social context. Social context can help infer the participant's emotional state and their willingness to respond to an EMA or JITAI.

Another aspect that this study and paper do not address is burnout. As this was only a ten-day study, it is hard to see the effect of the EMAs on the participants and in turn how our model might affect participantsâ€™ adherence to a study as a whole. While increasing responsiveness is good, if it negatively affects the engagement of the subject over time, it needs to be incorporated into the final model. To do so, we intend to look at how our EMAs affect participant response rates over time, using perceived emotional responses, sensor data, and situational information. Ideally, this will allow for us to see the rate at which responses decay, the causes and how we might combat burnout.

The significant limitation of this paper is that the emotional state during non-responses, for this dataset and all datasets, is unknown. We attempt to reduce this limitation by focusing on uncertainty. Nevertheless, the predicted affect is only as good as our models. The only way to curtail this limitation is by improving the affect models. While some may argue that the quality of our models need to be stronger to claim that there is a relationship between affect and receptivity, the effects of emotional state on engagement in social and daily-life activities are well documented and consistent with our conclusion.

\subsection{Comparison with Prior Work}
While our results showed promise for a model dedicated to predicting response, we also showed the biases in a model like this. Ideally, we would want a receptivity model completely independent of emotion. Otherwise, we are influencing the subject's responses. Based on our results, a response model used to issue EMAs would send them at times of more positive affect. For this reason, a responsiveness model needs to include more factors than just if the subject is likely to respond. Furthermore, the accuracy of the response might be less consequential once we add other components to the model, like promoting a diverse set of emotional states using uncertainty, finding points of engagement, and varying the protocol for sending EMAs. Some of these components will negatively affect a responsiveness model's performance, but the opportunity for knowledge gained can be greater than in a traditional EMA protocol.

Our findings suggest an intelligent trigger that initiates EMAs or JITAIs based on the likelihood to respond and the intended health construct. The models discussed in this paper have mostly proposed single-objective optimization functions that try to optimize based on whether the model thinks a user will respond to an EMA. We propose a multi-objective optimization function for triggering EMAs and JITAIs based on the likelihood to respond and an active-learning measurement of the health construct. This multi-objective function would base the timing of the EMAs on two separate objectives, the likelihood of user response and the ability of our model to determine the psychological construct. This measurement for a psychological construct could be uncertainty (e.g., least confidence, entropy). By initiating EMAs or JITAIs based on these two objectives, we can obtain an expected response that is more diverse in terms of affect. We must look at several factors when verifying that our proposed model is less biased. We need to compare our dataset to our initial assessments and determine if we are significantly affecting the perceived emotions of our participants. Additionally, as the model would be based on a combination of receptivity and affect, we would expect a more diverse range of emotional responses from our subjects because the trigger would initiate a more various perceived emotional state.


\section{Conclusion}

This paper presents the possibilities for bias in machine learning models to trigger surveys and interventions for participants in mHealth studies. Our results show a clear relationship between emotional state and user receptivity. By designing an mHealth study using a "trigger" to improve participant response, it is imperative to consider some biases that may arise, in this case, affect. Participants were more likely to respond to an EMA during positive emotional states. If we were to constrain those EMAs to times when they are more likely to respond, we would further be biasing our participants' recorded emotional state. While this may not be a significant problem for less responsive populations, for the general population, this could change researchers' perception of the participant's perceived emotional state. In this paper, we did not examine other constructs that might be a factor of receptivity because affect is the focal point of this study. We are collecting both subjective and physiological data for this purpose. While this may be broad, it could be applied to any construct, particularly the intended construct of an mHealth study.

The pitfall of any mHealth study, particularly those involving psychological concepts, is the dependency on subjective user responses. The sampling rate of subjective responses will always be less than that of our physiological sensors and even some contextual cues. As our feature set becomes more and more comprehensive, our labeled data remains relatively sparse. Our proposed trigger would likely increase receptivity less than others, as we consider more than just the likelihood to respond. However, the importance of even a minimal increase in a user's adherence or engagement to a study can drastically improve researchersâ€™ understanding of the health construct. We hope that the work we have presented in this paper can be used to enhance further our communication and ability to gain knowledge from subjects.





\bibliographystyle{unsrt}  

\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
% \begin{thebibliography}{1}

% \bibitem{kour2014real}
% George Kour and Raid Saabne.
% \newblock Real-time segmentation of on-line handwritten arabic script.
% \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
%   International Conference on}, pages 417--422. IEEE, 2014.

% \bibitem{kour2014fast}
% George Kour and Raid Saabne.
% \newblock Fast classification of handwritten on-line arabic characters.
% \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
%   International Conference of}, pages 312--318. IEEE, 2014.

% \bibitem{hadash2018estimate}
% Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
%   Jacovi.
% \newblock Estimate and replace: A novel approach to integrating deep neural
%   networks with existing applications.
% \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
